<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 54]
- [cs.LG](#cs.LG) [Total: 185]
- [stat.ML](#stat.ML) [Total: 12]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain Wireless Autoencoding](https://arxiv.org/abs/2601.11713)
*Rodney Martinez Alonso,Cel Thys,Cedric Dehos,Yuneisy Esthela Garcia Guzman,Sofie Pollin*

Main category: eess.SP

TL;DR: 提出一种在沃尔什域中联合优化发射机和接收机的端到端无线自动编码器架构，用于抑制超宽带系统中来自5G基站的带内部分干扰，实现高达12dB的干扰抑制。


<details>
  <summary>Details</summary>
Motivation: 超宽带通信系统面临来自共存窄带5G基站的带内部分干扰问题，传统方法难以有效抑制这种干扰，需要新的干扰抑制技术。

Method: 设计端到端无线自动编码器架构，在沃尔什域中联合优化发射机和接收机的编码/解码。利用沃尔什函数的正交性和自逆特性，将比特字分布并学习编码到并行沃尔什分支中。

Result: 通过分析和仿真，表征了5G CPOFDM干扰在沃尔什域中的映射，确定了传输频率与采样率的最佳比例。实验结果显示，所提自动编码器在相同基线信道噪声下实现高达12dB的干扰抑制，同时保持低块错误率。

Conclusion: 提出的沃尔什域端到端自动编码器架构能有效抑制超宽带系统中的5G带内部分干扰，为共存通信系统提供了一种有效的干扰管理解决方案。

Abstract: This paper proposes a novel technique for rejecting partial-in-band inter-cell interference (ICI) in ultrawideband communication systems. We present the design of an end-to-end wireless autoencoder architecture that jointly optimizes the transmitter and receiver encoding/decoding in the Walsh domain to mitigate interference from coexisting narrower-band 5G base stations. By exploiting the orthogonality and self-inverse properties of Walsh functions, the system distributes and learns to encode bit-words across parallel Walsh branches. Through analytical modeling and simulation, we characterize how 5G CPOFDM interference maps into the Walsh domain and identify optimal ratios of transmission frequencies and sampling rate where the end-to-end autoencoder achieves the highest rejection. Experimental results show that the proposed autoencoder achieves up to 12 dB of ICI rejection while maintaining a low block error rate (BLER) for the same baseline channel noise, i.e., baseline Signal-to-Noise-Ratio (SNR) without the interference.

</details>


### [2] [Sparsity Realization in User-Side Multilayer RIS](https://arxiv.org/abs/2601.11720)
*Hasan M. Boudi,Taissir Y. Elganimi*

Main category: eess.SP

TL;DR: 提出多层用户侧可重构智能表面（US-RIS）的稀疏架构框架，通过元素级稀疏和几何稀疏两种策略提升系统性能


<details>
  <summary>Details</summary>
Motivation: 解决大规模用户侧天线阵列的高硬件成本和物理尺寸限制问题，US-RIS通信成为有前景的解决方案，但需要更高效的架构设计

Method: 提出两种稀疏策略：1）元素级稀疏 - 在多层结构中不规则分布有限数量的有源元件；2）几何稀疏 - 提出可折叠RIS架构，通过优化多层结构的折叠拓扑实现性能提升

Result: 仿真结果表明，所提出的稀疏架构相比现有设计能够持续提供更高的可达速率

Conclusion: 首次提出的多层US-RIS稀疏框架通过元素级和几何稀疏策略有效提升了系统性能，为US-RIS通信提供了新的设计思路

Abstract: User-side reconfigurable intelligent surface (US-RIS)-aided communication has recently emerged as a promising solution to overcome the high hardware cost and physical size limitations of large-scale user side antenna arrays. This letter proposes, for the first time, a framework that realizes sparsity in multilayer US-RIS using two strategies, namely element-wise sparsity and geometric sparsity. The element-wise approach distributes a limited number of active elements irregularly across multiple layers, thereby exploiting additional spatial degrees of freedom and boosting the achievable rate. For further performance enhancement, a novel foldable RIS architecture leveraging geometric sparsity is proposed, achieving additional gains by optimizing the folding topology of its multilayer structure. Simulation results show that the proposed sparse architectures provide consistently higher achievable rates than existing designs.

</details>


### [3] [LarS-Net: A Large-Scale Framework for Network-Level Spectrum Sensing](https://arxiv.org/abs/2601.11734)
*Hao Guo,Ruoyu Sun,Amir Hossein Fahim Raouf,Rahil Gandotra,Jiayu Mao,Mark Poletti*

Main category: eess.SP

TL;DR: 设计大规模频谱感知网络(LarS-Net)，通过基站共享基础设施降低成本，分析微波链路检测性能，提出三种网络级感知指标评估空间覆盖和时间检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信需求增长，频谱资源需要更高效利用，推动从静态频谱分配向动态频谱共享演进。现有研究局限于单个或小型传感器，需要大规模、经济有效的频谱感知网络来支持智能决策。

Method: 设计大规模频谱感知网络(LarS-Net)，传感器与基站共址共享基础设施，或集成到基站天线系统中。以7GHz以下频段的固定服务微波链路为例，通过蒙特卡洛模拟确定满足目标检测概率所需的最小传感器基站子集。考虑多种天线配置、传播信道模型和占空比。

Result: 提出三种网络级感知性能指标：发射检测概率(EDP)、时间检测概率(TDP)和时间误检概率(TMP)，用于联合评估空间覆盖、时间可检测性和多节点多样性效应。分析了站点间距、噪声不确定性和感知占空比对大规模感知性能的影响。

Conclusion: LarS-Net为大规模频谱感知提供经济有效的解决方案，提出的性能指标能全面评估网络级感知能力，为6G频谱共享和动态频谱管理提供重要参考。

Abstract: As the demand of wireless communication continues to rise, the radio spectrum (a finite resource) requires increasingly efficient utilization. This trend is driving the evolution from static, stand-alone spectrum allocation toward spectrum sharing and dynamic spectrum sharing. A critical element of this transition is spectrum sensing, which facilitates informed decision-making in shared environments. Previous studies on spectrum sensing and cognitive radio have been largely limited to individual sensors or small sensor groups. In this work, a large-scale spectrum sensing network (LarS-Net) is designed in a cost-effective manner. Spectrum sensors are either co-located with base stations (BSs) to share the tower, backhaul, and power infrastructure, or integrated directly into BSs as a new feature leveraging active BS antenna systems. As an example incumbent system, fixed service microwave link operating in the lower-7 GHz band is investigated. This band is a primary candidate for 6G, being considered by the WRC-23, ITU, and FCC. Based on Monte Carlo simulations, we determine the minimum subset of BSs equipped with sensing capability to guarantee a target incumbent detection probability. The simulations account for various sensor antenna configurations, propagation channel models, and duty cycles for both incumbent transmissions and sensing operations. Building on this framework, we introduce three network-level sensing performance metrics: Emission Detection Probability (EDP), Temporal Detection Probability (TDP), and Temporal Mis-detection Probability (TMP), which jointly capture spatial coverage, temporal detectability, and multi-node diversity effects. Using these metrics, we analyze the impact of LarS-Net inter-site distance, noise uncertainty, and sensing duty-cycle on large-scale sensing performance.

</details>


### [4] [MIMO Array Calibration in Non-stationary Channels with Residual Surfaces and Slepian Spherical Harmonics](https://arxiv.org/abs/2601.11741)
*Oliver Kirkpatrick,Santiago Ozafrain,Christopher Gilliam,Beth Jelfs*

Main category: eess.SP

TL;DR: 提出一种在非平稳信道中通过间接测量和补偿硬件影响来实现MIMO波束成形的校准方法，使用Slepian球谐基表征阵列元素相对模式，显著提升波束成形性能。


<details>
  <summary>Details</summary>
Motivation: 在非平稳信道中，到达信号的幅度和相位随时间显著变化，使得直接测量硬件影响变得不可行。需要开发一种能够在动态信道条件下间接测量和补偿硬件影响的校准方法。

Method: 提出一种校准方法，通过表征阵列元素相对于参考元素的模式（称为残差表面），并使用Slepian球谐基来估计这些相对模式，从而间接测量和补偿硬件影响。

Result: 仿真结果表明，该校准方法实现的波束成形增益接近理论最优值，同时减少了目标方向估计误差，降低了旁瓣水平，并改善了零点控制能力。

Conclusion: 该方法有效解决了非平稳信道中硬件影响难以直接测量的问题，通过间接校准显著提升了MIMO波束成形的性能，为实际动态信道环境下的阵列校准提供了可行方案。

Abstract: The fundamental mechanism driving MIMO beamforming is the relative phases of signals departing the transmit array and arriving at the receive array. If a propagation channel affects all transmitted signals equally, the relative phases are a function of the directions of departure and arrival, as well as the transmit and receive hardware. In a non-stationary channel, the amplitudes and phases of arriving signals may vary significantly over time, making it infeasible to directly measure the influence of hardware. In this paper, we present a calibration method for achieving indirect measurement and compensation of hardware influences in non-stationary channels. Our method characterizes the patterns of array elements relative to a reference element and estimates these relative patterns, termed residual surfaces, using a Slepian spherical harmonic basis. Using simulations, we demonstrate that our calibration method achieves beamforming gains that closely match theoretical optimums. Our results also show a reduction in the error in estimating the target direction, lower side lobes, and improve null-steering capabilities.

</details>


### [5] [AI-Driven Spectrum Occupancy Prediction Using Real-World Spectrum Measurements](https://arxiv.org/abs/2601.11742)
*Jiayu Mao,Ruoyu Sun,Mark Poletti,Rahil Gandotra,Hao Guo,Aylin Yener*

Main category: eess.SP

TL;DR: 该论文使用美国真实频谱测量数据，研究短时频谱占用预测，比较AI方法与传统统计方法，发现学习型方法在动态信道中表现更优。


<details>
  <summary>Details</summary>
Motivation: 频谱占用预测是实时动态频谱共享的关键使能技术，但现有研究多依赖开源数据集或模拟数据，缺乏真实世界数据的验证。

Method: 使用美国中频段24X7真实频谱测量数据构建多频段信道占用数据集，分析61天经验数据，提出下一分钟信道占用预测任务，比较随机森林、XGBoost、LSTM等AI方法与马尔可夫链统计基线。

Result: 学习型方法在动态信道中优于统计基线，特别是在固定误报约束下。轻量级学习模型能有效支持未来部署导向的动态频谱共享系统。

Conclusion: AI驱动的频谱占用预测是有效的，轻量级学习模型可为未来动态频谱共享系统提供有效支持，验证了真实世界数据上AI方法的优越性。

Abstract: Spectrum occupancy prediction is a critical enabler for real-time and proactive dynamic spectrum sharing (DSS), as it can provide short-term channel availability information to support more efficient spectrum access decisions in wireless communication systems. Instead of relying on open-source datasets or simulated data, commonly used in the literature, this paper investigates short-horizon spectrum occupancy prediction using mid-band, 24X7 real-world spectrum measurement data collected in the United States. We construct a multi-band channel occupancy dataset through analyzing 61 days of empirical data and formulate a next-minute channel occupancy prediction task across all frequency channels. This study focuses on AI-driven prediction methods, including Random Forest, Extreme Gradient Boosting (XGBoost), and a Long Short-Term Memory (LSTM) network, and compares their performance against a conventional Markov chain-based statistical baseline. Numerical results show that learning-based methods outperform the statistical baseline on dynamic channels, particularly under fixed false-alarm constraints. These results demonstrate the effectiveness of AI-driven spectrum occupancy prediction, indicating that lightweight learning models can effectively support future deployment-oriented DSS systems.

</details>


### [6] [Automated Angular Received-Power Characterization of Embedded mmWave Transmitters Using Geometry-Calibrated Spatial Sampling](https://arxiv.org/abs/2601.12562)
*Maaz Qureshi,Mohammad Omid Bagheri,Abdelrahman Elbadrawy,William Melek,George Shaker*

Main category: eess.SP

TL;DR: RAPTAR：一种用于嵌入式毫米波发射机角度接收功率自动测量的几何校准空间采样系统，相比传统探针台测量误差降低36.5%


<details>
  <summary>Details</summary>
Motivation: 传统探针台技术角度覆盖有限且对准可变性大，而暗室测试对平台安装的主动模块不实用，需要一种能在实际安装约束下进行角度接收功率测量的方法

Method: 使用协作机器人执行几何校准、碰撞感知的半球形轨迹，携带校准接收探针围绕固定被测设备进行可控可重复的空间定位，通过频谱分析仪接收链获取角度和距离函数的幅度接收功率

Result: 60GHz雷达模块实验显示，相对于仿真参考的平均绝对接收功率误差低于2dB，相比手动探针台测量误差降低36.5%，主要归因于减少了对准可变性和一致的空间采样

Conclusion: 该方法无需相干场测量和近场变换，实现了嵌入式毫米波模块的实际功率域表征，特别适用于传统暗室测量不实用的真实世界平台的角度验证

Abstract: This paper presents an automated measurement methodology for angular received-power characterization of embedded millimeter-wave transmitters using geometry-calibrated spatial sampling. Characterization of integrated mmWave transmitters remains challenging due to limited angular coverage and alignment variability in conventional probe-station techniques, as well as the impracticality of anechoic-chamber testing for platform-mounted active modules. To address these challenges, we introduce RAPTAR, an autonomous measurement system for angular received-power acquisition under realistic installation constraints. A collaborative robot executes geometry-calibrated, collision-aware hemispherical trajectories while carrying a calibrated receive probe, enabling controlled and repeatable spatial positioning around a fixed device under test. A spectrum-analyzer-based receiver chain acquires amplitude-only received power as a function of angle and distance following quasi-static pose stabilization. The proposed framework enables repeatable angular received-power mapping and power-domain comparison against idealized free-space references derived from full-wave simulation. Experimental results for a 60-GHz radar module demonstrate a mean absolute received-power error below 2 dB relative to simulation-derived references and a 36.5 % reduction in error compared to manual probe-station measurements, attributed primarily to reduced alignment variability and consistent spatial sampling. The proposed method eliminates the need for coherent field measurements and near-field transformations, enabling practical power-domain characterization of embedded mmWave modules. It is well suited for angular validation in real-world platforms where conventional anechoic measurements are impractical.

</details>


### [7] [Automated Spectrum Sensing and Analysis Framework](https://arxiv.org/abs/2601.11748)
*Rahil Gandotra,Ruoyu Sun,Mark Poletti,Jiayu Mao,Hao Guo*

Main category: eess.SP

TL;DR: 论文提出了一个在美国多地部署的端到端频谱分析框架，用于实时频谱感知、数据处理和可视化，以支持频谱资源管理和动态频谱共享。


<details>
  <summary>Details</summary>
Motivation: 频谱感知与分析对于监管合规、干扰检测与缓解、频谱资源规划与优化至关重要。然而，实时频谱分析面临挑战：需要在资源有限的情况下分析日益复杂动态的环境，海量频谱数据需要复杂的数据分析处理技术，这些技术既要求高又成本昂贵。

Method: 开发并部署了一个新颖的端到端频谱分析框架，包括多个模块：远程位置的数据收集与预处理、数据传输到集中位置、后处理分析、可视化以及长期存储。该框架在美国多个地点部署。

Result: 成功开发并部署了一个完整的频谱分析框架，能够处理从数据收集到长期存储的全流程，为全国范围内的频谱使用提供深入洞察。

Conclusion: 该框架为频谱分析提供了一个稳健的解决方案，有助于更好地理解全国频谱使用情况，并支持动态频谱共享等附加用例。

Abstract: Spectrum sensing and analysis is crucial for a variety of reasons, including regulatory compliance, interference detection and mitigation, and spectrum resource planning and optimization. Effective, real-time spectrum analysis remains a challenge, stemming from the need to analyse an increasingly complex and dynamic environment with limited resources. The vast amount of data generated from sensing the spectrum at multiple sites requires sophisticated data analysis and processing techniques, which can be technically demanding and expensive. This paper presents a novel, holistic framework developed and deployed at multiple locations across the USA for spectrum analysis and describes the different parts of the end-to-end pipeline. The details of each of the modules of the pipeline, data collection and pre-processing at remote locations, transfer to a centralized location, post-processing analysis, visualization, and long-term storage, are reported. The motivation behind this work is to develop a robust spectrum analysis framework that can help gain greater insights into the spectrum usage across the country and augment additional use cases such as dynamic spectrum sharing.

</details>


### [8] [Necessity of Cooperative Transmissions for Wireless MapReduce](https://arxiv.org/abs/2601.11844)
*Yue Bi,Michèle Wigger*

Main category: eess.SP

TL;DR: 本文改进了分布式MapReduce系统中NDT与计算负载权衡的上界，提出了基于干扰对齐和迫零的协作方案，并证明了在某些参数下非协作方案无法达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究分布式计算MapReduce系统中归一化交付时间(NDT)与计算负载之间的最优权衡，探索协作传输方案相对于非协作方案的优势。

Method: 采用干扰对齐与迫零技术相结合的协作传输方案，同时分析了非协作方案（单个节点传输子中间值）的性能下界。

Result: 提出了改进的NDT-计算权衡上界，证明了在某些参数范围内，协作方案优于非协作方案，且非协作方案无法达到最优NDT-计算权衡。

Conclusion: 对于某些参数配置，必须采用协作传输方案（如迫零）才能达到最优NDT-计算权衡，而非协作方案存在性能限制。

Abstract: The paper presents an improved upper bound (achievability result) on the optimal tradeoff between Normalized Delivery Time (NDT) and computation load for distributed computing MapReduce systems in certain ranges of the parameters. The upper bound is based on interference alignment combined with zero-forcing. The paper further provides a lower bound (converse) on the optimal NDT-computation tradeoff that can be achieved when IVAs are partitioned into sub-IVAs, and these sub-IVAs are then transmitted (in an arbitrary form) by a single node, without cooperation among nodes. For appropriate linear functions (e.g., XORs), such non-cooperative schemes can achieve some of the best NDT-computation tradeoff points so far obtained in the literature. However, as our lower bound shows, any non-cooperative scheme achieves a worse NDT-computation tradeoff than our new proposed scheme for certain parameters, thus proving the necessity of cooperative schemes like zero-forcing to attain the optimal NDT-computation tradeoff.

</details>


### [9] [Delay-Doppler-Domain Channel Estimation and Reduced-Complexity Detection of Faster-than-Nyquist Signaling Aided OTFS](https://arxiv.org/abs/2601.11869)
*Zekun Hong,Shinya Sugiura,Chao Xu,Lajos Hanzo*

Main category: eess.SP

TL;DR: 提出了一种用于OTFS-FTN传输的新型信道估计和数据检测方案，旨在提高频谱效率和抗多普勒能力，在DD域设计FTN导频传输和低复杂度均衡器。


<details>
  <summary>Details</summary>
Motivation: 传统OTFS系统在频谱效率和抗多普勒性能方面仍有提升空间，FTN（超奈奎斯特）传输可以进一步提高频谱效率，但需要解决FTN引入的符号间干扰问题。

Method: 1. 推导OTFS-FTN信号在延迟-多普勒域的输入输出关系；2. 设计DD域信道估计器，使用FTN导频传输；3. 提出支持噪声白化的低复杂度线性最小均方误差均衡器，将FTN引起的ISI矩阵近似为稀疏矩阵。

Result: 提出的OTFS-FTN方案能够提高可达信息率，同时获得与奈奎斯特OTFS方案相当的BER性能，且优于其他使用相同RRC整形滤波器的FTN传输方案。

Conclusion: OTFS-FTN方案成功结合了OTFS的抗多普勒能力和FTN的高频谱效率优势，通过创新的信道估计和均衡器设计，实现了性能提升。

Abstract: We conceive a novel channel estimation and data detection scheme for OTFS-modulated faster-than-Nyquist (FTN) transmission over doubly selective fading channels, aiming for enhancing the spectral efficiency and Doppler resilience. The delay-Doppler (DD) domain's input-output relationship of OTFS-FTN signaling is derived by employing a root-raised cosine (RRC) shaping filter. More specifically, we design our DD-domain channel estimator for FTN-based pilot transmission, where the pilot symbol interval is lower than that defined by the classic Nyquist criterion. Moreover, we propose a reduced-complexity linear minimum mean square error equalizer, supporting noise whitening, where the FTN-induced inter-symbol interference (ISI) matrix is approximated by a sparse one. Our performance results demonstrate that the proposed OTFS-FTN scheme is capable of enhancing the achievable information rate, while attaining a comparable BER performance to both that of its Nyquist-based OTFS counterpart and to other FTN transmission schemes, which employ the same RRC shaping filter.

</details>


### [10] [Accelerated MR Elastography Using Learned Neural Network Representation](https://arxiv.org/abs/2601.11878)
*Xi Peng*

Main category: eess.SP

TL;DR: 提出一种无需高质量训练数据的自监督深度学习方法，从高度欠采样数据实现快速高分辨率磁共振弹性成像


<details>
  <summary>Details</summary>
Motivation: 传统线性子空间方法在高度欠采样情况下重建质量有限，需要开发非线性方法提升重建质量，同时避免对高质量训练数据的依赖

Method: 将深度神经网络表示为线性子空间模型的非线性扩展，采用多级k空间一致性损失进行自监督学习，并整合幅度和相位先验信息

Result: 相比传统线性子空间方法，非线性网络表示能从单个平面螺旋臂重建出噪声和伪影抑制的优质图像，刚度估计与全采样数据相当

Conclusion: 深度网络表示可作为子空间方法的非线性扩展，有效建模和重建高度欠采样的MRE数据，无需高质量训练数据集

Abstract: To develop a deep-learning method for achieving fast high-resolution MR elastography from highly undersampled data without the need of high-quality training dataset. We first framed the deep neural network representation as a nonlinear extension of the linear subspace model, then used it to represent and reconstruct MRE image repetitions from undersampled k-space data. The network weights were learned using a multi-level k-space consistent loss in a self-supervised manner. To further enhance reconstruction quality, phase-contrast specific magnitude and phase priors were incorporated, including the similarity of anatomical structures and smoothness of wave-induced harmonic displacement. Experiments were conducted using both 3D gradient-echo spiral and multi-slice spin-echo spiral MRE datasets. Compared to the conventional linear subspace-based approaches, the nonlinear network representation method was able to produce superior image reconstruction with suppressed noise and artifacts from a single in-plane spiral arm per MRE repetition (e.g., total R=10), yielding comparable stiffness estimation to the fully sampled data. This work demonstrated the feasibility of using deep network representations to model and reconstruct MRE images from highly-undersampled data, a nonlinear extension of the subspace-based approaches.

</details>


### [11] [Optimal Calibration of the endpoint-corrected Hilbert Transform](https://arxiv.org/abs/2601.13962)
*Eike Osmers,Dorothea Kolossa*

Main category: eess.SP

TL;DR: 该论文对端点校正希尔伯特变换(ecHT)进行了理论分析，推导了其端点算子，提出了校准方法(c-ecHT)和设计准则，实现了近乎零平均相位误差的实时相位估计。


<details>
  <summary>Details</summary>
Motivation: 在闭环神经刺激等实时应用中，需要准确、低延迟的瞬时相位估计。ecHT虽然广泛使用，但其端点失真缺乏系统的理论分析。

Method: 通过解析推导ecHT端点算子，将其输出分解为期望的正频率项（可校准的确定性复增益）和残余泄漏项（设定方差下限），提出了均方误差最优的标量校准方法(c-ecHT)。

Result: 获得了端点相位/幅度误差的明确表征和边界，实现了近乎零平均相位误差的校准ecHT，并建立了窗口长度、带宽/阶数、中心频率失配与残余偏差之间的设计规则。

Conclusion: 通过理论分析ecHT端点失真，提出的校准方法显著提高了相位估计精度，同时保持实时计算兼容性，为实时相位估计应用提供了理论基础和实用工具。

Abstract: Accurate, low-latency estimates of the instantaneous phase of oscillations are essential for closed-loop sensing and actuation, including (but not limited to) phase-locked neurostimulation and other real-time applications. The endpoint-corrected Hilbert transform (ecHT) reduces boundary artefacts of the Hilbert transform by applying a causal narrow-band filter to the analytic spectrum. This improves the phase estimate at the most recent sample. Despite its widespread empirical use, the systematic endpoint distortions of ecHT have lacked a principled, closed-form analysis. In this study, we derive the ecHT endpoint operator analytically and demonstrate that its output can be decomposed into a desired positive-frequency term (a deterministic complex gain that induces a calibratable amplitude/phase bias) and a residual leakage term setting an irreducible variance floor. This yields (i) an explicit characterisation and bounds for endpoint phase/amplitude error, (ii) a mean-squared-error-optimal scalar calibration (c-ecHT), and (iii) practical design rules relating window length, bandwidth/order, and centre-frequency mismatch to residual bias via an endpoint group delay. The resulting calibrated ecHT achieves near-zero mean phase error and remains computationally compatible with real-time pipelines. Code and analyses are provided at https://github.com/eosmers/cecHT.

</details>


### [12] [Beyond Target-Level: ISAC-Enabled Event-Level Sensing for Behavioral Intention Prediction](https://arxiv.org/abs/2601.11894)
*Haotian Liu,Zhiqing Wei,Yucong Du,Jiachen Wei,Xingwang Li,Zhiyong Feng*

Main category: eess.SP

TL;DR: 提出首个基于ISAC的行为意图预测框架，在恶劣天气下比传统传感器方法F1分数提升11.4%


<details>
  <summary>Details</summary>
Motivation: ISAC在事件级感知（如自动驾驶中的行为意图预测）方面具有巨大潜力，特别是在NLoS或恶劣天气条件下传统传感器性能下降时。然而，ISAC在行为意图预测这一关键应用上尚未被探索。

Method: 提出一个ISAC启用的行为意图预测框架，通过大量仿真验证其可行性和有效性

Result: 框架在安全关键场景中表现稳健，在恶劣天气条件下比基于传感器的基线方法F1分数提升11.4%

Conclusion: 该研究证明了ISAC在智能事件级感知方面的潜力，特别是在传统传感器性能受限的场景下

Abstract: Integrated Sensing and Communication (ISAC) holds great promise for enabling event-level sensing, such as behavioral intention prediction (BIP) in autonomous driving, particularly under non-line-of-sight (NLoS) or adverse weather conditions where conventional sensors degrade. However, as a key instance of event-level sensing, ISAC-based BIP remains unexplored. To address this gap, we propose an ISAC-enabled BIP framework and validate its feasibility and effectiveness through extensive simulations. Our framework achieves robust performance in safety-critical scenarios, improving the F1-score by 11.4% over sensor-based baselines in adverse weather, thereby demonstrating ISAC's potential for intelligent event-level sensing.

</details>


### [13] [Radar-Based Fall Detection for Assisted Living: A Digital-Twin Representation Case Study](https://arxiv.org/abs/2601.11938)
*Sebastian Ratto,Huy Trinh,Ahmed N. Sayed,Abdelrahman Elbadrawy,Arien Sligar,George Shaker*

Main category: eess.SP

TL;DR: 使用FMCW雷达数字孪生测试不同表示方法对跌倒检测的影响，发现时频谱图比静态距离多普勒图表现更好，但模拟结果与真实测量存在差异


<details>
  <summary>Details</summary>
Motivation: 获取老年人高危跌倒数据存在伦理困难，而现有雷达跌倒检测器大多基于年轻人模拟跌倒训练，缺乏对危险跌倒雷达信号的验证。需要研究表示方法选择对跌倒/非跌倒区分能力的影响。

Method: 使用频率调制连续波雷达数字孪生作为模拟房间测试平台，从相同的模拟距离-多普勒序列中提取三种表示：多普勒-时间频谱图、三通道接收机频谱图堆栈和时间池化距离-多普勒图，使用相同的紧凑CNN在平衡数据集上训练比较。

Result: 时频谱图达到98-99%的测试准确率，两类具有相似的精确率和召回率；静态距离-多普勒图达到89.4%，训练过程更不稳定。模拟与实测频谱图的定性比较显示捕获了主要多普勒-时间结构，但幅度直方图分布存在差异。

Conclusion: 数字孪生提供了受控合成条件下的表示层面指导，而非可直接用于临床的实际性能。模拟忽略了噪声和硬件损伤，仅与单个实测示例进行定性比较，结果需谨慎解读。

Abstract: Obtaining data on high-impact falls from older adults is ethically difficult, yet these rare events cause many fall-related health problems. As a result, most radar-based fall detectors are trained on staged falls from young volunteers, and representation choices are rarely tested against the radar signals from dangerous falls. This paper uses a frequency-modulated continuous-wave (FMCW) radar digital twin as a single simulated room testbed to study how representation choice affects fall/non-fall discrimination. From the same simulated range-Doppler sequence, Doppler-time spectrograms, three-channel per-receiver spectrogram stacks, and time-pooled range-Doppler maps (RDMs) are derived and fed to an identical compact CNN under matched training on a balanced fall/non-fall dataset. In this twin, temporal spectrograms reach 98-99% test accuracy with similar precision and recall for both classes, while static RDMs reach 89.4% and show more variable training despite using the same backbone. A qualitative comparison between synthetic and measured fall spectrograms suggests that the twin captures gross Doppler-time structure, but amplitude histograms reveal differences in the distributions of amplitude values consistent with receiver processing not modeled in the twin. Because the twin omits noise and hardware impairments and is only qualitatively compared to a single measured example, these results provide representation-level guidance under controlled synthetic conditions rather than ready-to-use clinical performance in real settings.

</details>


### [14] [Robust distributed extended Kalman filter based on adaptive multi-kernel mixture maximum correntropy for non-Gaussian systems](https://arxiv.org/abs/2601.11971)
*Duc Viet Nguyen,Haiquan Zhao,Jinhui Hu,Xiaoli Li*

Main category: eess.SP

TL;DR: 提出多核混合相关熵(MKMC)概念，基于学生t-柯西混合核函数，开发自适应多核混合最大相关熵鲁棒分布式扩展卡尔曼滤波器(AMKMMC-RDEKF)，用于多传感器网络状态估计


<details>
  <summary>Details</summary>
Motivation: 现有多核相关熵方法存在两个主要问题：1) 依赖单一类型的高斯核函数，对参数敏感；2) 需要手动选择自由参数。这限制了其在处理非高斯噪声（特别是多模态分布）时的鲁棒性和实用性。

Method: 1) 提出多核混合相关熵(MKMC)概念，使用两个学生t-柯西函数的混合核函数，具有可调非零均值；2) 基于MKMC开发自适应多核混合最大相关熵鲁棒分布式扩展卡尔曼滤波器(AMKMMC-RDEKF)；3) 引入共识平均策略降低通信开销；4) 设计自适应机制减少手动调参影响

Result: 1) 分析了算法的计算复杂度和收敛能力；2) 在电力系统和陆地车辆状态估计的挑战性场景中验证了算法的有效性，展示了其在非高斯噪声环境下的优越性能

Conclusion: 提出的MKMC框架和AMKMMC-RDEKF算法解决了传统多核相关熵方法的局限性，通过混合核函数和自适应机制显著提高了鲁棒性，在复杂非高斯噪声环境下表现出优越的状态估计性能

Abstract: As one of the most advanced variants in the correntropy family, the multi-kernel correntropy criterion demonstrates superior accuracy in handling non-Gaussian noise, particularly with multimodal distributions. However, current approaches suffer from key limitations-namely, reliance on a single type of sensitive Gaussian kernel and the manual selection of free parameters. To address these issues and further boost robustness, this paper introduces the concept of multi-kernel mixture correntropy (MKMC), along with its key properties. MKMC employs a flexible kernel function composed of a mixture of two Students t-Cauchy functions with adjustable (non-zero) means. Building on this criterion within multi-sensor networks, we propose a robust distributed extended Kalman filter-AMKMMC-RDEKF based on adaptive multi-kernel mixture maximum correntropy. To reduce communication overhead, a consensus averaging strategy is incorporated. Furthermore, an adaptive mechanism is introduced to mitigate the impact of manually tuned free parameters. At the same time, the computational complexity and convergence ability of the proposed algorithm are analyzed. The effectiveness of the proposed algorithm is validated through challenging scenarios involving power system and land vehicle state estimation.

</details>


### [15] [Extended Weighted ABG: A Robust Non-Linear ABG-Based Approach for Optimal Combination of ABG Path-Loss Propagation Models](https://arxiv.org/abs/2601.12110)
*David. Casillas-Pérez,Daniel. Merino-Pérez,Silvia. Jiménez-Fernández,J. Antonio. Portilla-Figueras,Sancho. Salcedo-Sanz*

Main category: eess.SP

TL;DR: 提出EWABG模型，首个非线性扩展的ABG路径损耗模型，能整合多个数据集和现有模型，解决5G高低频不均匀问题和异常值处理


<details>
  <summary>Details</summary>
Motivation: 现有5G路径损耗模型存在高低频测量不均匀问题（多数测量集中在低频），缺乏异常值处理机制，且需要整合多个数据集和现有模型来扩展频率和距离覆盖范围

Method: 提出扩展加权ABG（EWABG）模型，采用非线性扩展的ABG方法，使用Theil-Sen方法处理异常值，考虑大气气体非线性衰减，在最小二乘意义下整合多个路径损耗数据集和模型

Result: EWABG在5G非视距环境（UMiSC、UMiOS、UMa场景）中表现最佳，特别是在有异常值的噪声环境中，误差率增加可忽略（低于1%），优于ABG和WABG模型

Conclusion: EWABG是首个非线性扩展的ABG路径损耗模型，能有效整合多个数据集，解决5G高低频不均匀问题，并包含异常值处理机制，在噪声环境中表现优异

Abstract: This paper proposes a robust non-linear generalized path-loss propagation model, the Extended Weighted ABG (EWABG), which efficiently allows generating a path-loss propagation model by combining several available path-loss datasets (from measurements campaigns) and other previously proposed state-of-the-art 5G path-loss propagation models. The EWABG model works by integrating individual path-loss models into one single model in the least-squares sense, allowing to extend knowledge from frequencies and distances covered by path-loss datasets or path-loss propagation models. The proposed EWABG model is the first non-linear extension of the common ABG-based approach, which surpasses the non-uniformity problem between the low and high 5G frequencies (as most measurements campaigns have taken place in low frequencies). The EWABG also addresses the problem of removing outlier measurements, a step not included in previous propagation path-loss models. In this case, we have compared the most recent techniques for avoiding outliers, and we have adopted the Theil-Sen method, due to its strong robustness demonstrated in the experiments carried out. In addition, the proposed model specifically considers non-linear attenuation by atmospheric gases, in order to improve its estimations. The good performance of the proposed EWABG model has been tested and compared against recent 5G propagation path-loss models including the ABG and WABG models. The exhaustive experimentation carried out includes the 5G non-line-of-sight environment in different 5G scenarios, UMiSC, UMiOS and UMa. The proposed EWABG obtains the best accuracy, specially in noisy environments with outliers, reporting negligible increment error rates (with respect to the non-outliers situation), lower than 1%, compared to the ABG and WABG.

</details>


### [16] [Boiling flow estimation for aero-optic phase screen generation](https://arxiv.org/abs/2601.12171)
*Jeffrey W. Utley,Gregery T. Buzzard,Charles A. Bouman,Matthew R. Kemnetz*

Main category: eess.SP

TL;DR: 提出一种改进的沸腾流算法，用于生成匹配实测气动光学数据统计特性的任意长度合成数据，特别支持各向异性相位屏生成。


<details>
  <summary>Details</summary>
Motivation: 湍流引起的气动光学效应会降低光波传输效率，现有湍流数据生成方法（实验、高/低精度CFD、自回归方法）存在成本高、数据量有限、统计不准确或复杂度高等问题。沸腾流算法虽然简单高效，但参数定义不明确且未广泛应用于气动光学领域。

Method: 改进标准沸腾流算法，使其能够生成各向异性相位屏，通过调整参数匹配实测气动光学数据的统计特性，特别是时间功率谱或各向异性二维结构函数。

Result: 该方法能够生成任意长度的合成数据，匹配实测数据的统计特性，在时间功率谱和各向异性二维结构函数的保真度之间实现权衡。

Conclusion: 改进的沸腾流算法为气动光学湍流补偿提供了一种简单、计算高效的合成数据生成方法，虽然不能完全捕获所有统计特性，但在关键统计指标上能够匹配实测数据。

Abstract: Aero-optic effects due to turbulence can reduce the effectiveness of transmitting light waves to a distant target. Methods to compensate for turbulence typically rely on realistic turbulence data, which can be generated by i) experiment, ii) high-fidelity CFD, iii) low-fidelity CFD, and iv) autoregressive methods. However, each of these methods has significant drawbacks, including monetary and/or computational expense, limited quantity, inaccurate statistics, and overall complexity. In contrast, the boiling flow algorithm is a simple, computationally efficient model that can generate atmospheric phase screen data with only a handful of parameters. However, boiling flow has not been widely used in aero-optic applications, at least in part because some of these parameters, such as r0, are not clearly defined for aero-optic data. In this paper, we demonstrate a method to use the boiling flow algorithm to generate arbitrary length synthetic data to match the statistics of measured aero-optic data. Importantly, we modify the standard boiling flow method to generate anisotropic phase screens. While this model does not fully capture all statistics, it can be used to generate data that matches the temporal power spectrum or the anisotropic 2D structure function, with the ability to trade fidelity to one for fidelity to the other.

</details>


### [17] [Low-Complexity RSS-based Underwater Localization with Unknown Transmit Power](https://arxiv.org/abs/2601.12278)
*Yingquan Li,Jiajie Xu,Bodhibrata Mukhopadhyay,Mohamed-Slim Alouini*

Main category: eess.SP

TL;DR: 提出GUTP方法，利用加权RSS测量和广义信任域子问题，通过二分法联合估计水下目标位置和发射功率，无需先验功率知识，相比现有SDP方法精度更高、计算复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 水下无线传感器网络部署成本高、挑战大，基于接收信号强度的定位方法硬件需求低、成本效益高。传统方法需要目标节点发射功率的先验知识，限制了实际应用。

Method: 提出GUTP方法：1) 对RSS测量值分配基于距离的权重，使更近的锚节点具有更高可靠性；2) 利用加权RSS测量和广义信任域子问题，通过简单二分法求解；3) 联合估计目标节点位置和发射功率，无需先验功率知识。

Result: 推导了已知和未知发射功率情况下RSS水下定位的Cramer-Rao下界。仿真表明，GUTP相比现有SDP方法在估计目标节点位置和发射功率方面精度更高、计算复杂度显著降低。

Conclusion: GUTP方法通过联合估计位置和发射功率，无需先验功率知识，扩展了实际应用范围，同时提供更高精度和更低计算复杂度，为水下无线传感器网络定位提供了有效解决方案。

Abstract: Underwater wireless sensor networks (UWSNs) have received significant attention due to their various applications, with underwater target localization playing a vital role in enhancing network performance. Given the challenges and high costs associated with UWSN deployments, Received Signal Strength (RSS)-based localization offers a viable solution due to its minimal hardware requirements and cost-effectiveness. In this paper, we assign distance-based weights to RSS measurements, providing higher reliability to closer anchor nodes. Using the weighted RSS measurements and generalized trust region subproblem (GTRS), we propose the GTRS-based localization technique with Unknown Transmit Power (GUTP), which can be solved by a simple bisection method. Unlike conventional localization methods that require prior knowledge of the target node's transmit power, GUTP jointly estimates both the location and transmit power of the target node, broadening its practical use. Additionally, we derive the Cramer-Rao lower bounds (CRLBs) for RSS-based underwater localization with known and unknown transmit power, respectively. Extensive simulations demonstrate that GUTP achieves enhanced accuracy and significantly lower computational complexity in estimating the target node's location and transmit power compared to existing semidefinite programming (SDP)-based techniques.

</details>


### [18] [Overcoming BS Down-Tilt for Air-Ground ISAC Coverage: Antenna Design, Beamforming and User Scheduling](https://arxiv.org/abs/2601.12281)
*Lingyi Zhu,Zhongxiang Wei,Fan Liu,Jianjun Wu,Xiao-Wei Tang,Christos Masouros,Shanpu Shen*

Main category: eess.SP

TL;DR: 提出一种新型天线结构，通过全向转向板与有源阵列协作实现全空间波束成形，解决了传统基站无法感知空中目标的问题，并优化了用户调度、被动系数和波束成形以最大化感知通信互信息。


<details>
  <summary>Details</summary>
Motivation: 传统下倾基站主要为地面服务提供扇区前向波束，但由于后向盲区无法感知空中目标，这限制了低空经济应用中集成感知与通信的能力。

Method: 提出新型天线结构：用全向转向板替代传统反射器，与有源阵列协作实现全向波束成形。将优化问题分解为两个子问题：1) 在流形上通过黎曼梯度优化被动系数；2) 优化用户调度和有源阵列波束成形。利用感知通信互信息、数据解码MMSE和参数估计MMSE之间的关系，将原问题等价转化为加权MMSE问题。

Result: 仿真表明，所提算法在总和互信息和均方误差方面优于基线方法，同时提供360度感知覆盖。波束图分析进一步证明了有效的用户调度和准确的目标对准。

Conclusion: 该研究提出了一种创新的天线结构和优化框架，成功解决了传统基站无法同时进行全空间感知与通信的问题，为低空经济应用中的集成感知通信系统提供了有效解决方案。

Abstract: Integrated sensing and communication holds great promise for low-altitude economy applications. However, conventional downtilted base stations primarily provide sectorized forward lobes for ground services, failing to sense air targets due to backward blind zones. In this paper, a novel antenna structure is proposed to enable air-ground beam steering, facilitating simultaneous full-space sensing and communication (S&C). Specifically, instead of inserting a reflector behind the antenna array for backlobe mitigation, an omni-steering plate is introduced to collaborate with the active array for omnidirectional beamforming. Building on this hardware innovation, sum S&C mutual information (MI) is maximized, jointly optimizing user scheduling, passive coefficients of the omni-steering plate, and beamforming of the active array. The problem is decomposed into two subproblems: one for optimizing passive coefficients via Riemannian gradient on the manifold, and the other for optimizing user scheduling and active array beamforming. Exploiting relationships among S&C MI, data decoding MMSE, and parameter estimation MMSE, the original subproblem is equivalently transformed into a sum weighted MMSE problem, rigorously established via the Lagrangian and first-order optimality conditions. Simulations show that the proposed algorithm outperforms baselines in sum-MI and MSE, while providing 360 sensing coverage. Beampattern analysis further demonstrates effective user scheduling and accurate target alignment.

</details>


### [19] [RIS-Enhanced Information-Decoupled Symbiotic Radio Over Broadcasting Signals](https://arxiv.org/abs/2601.12403)
*Shu Cai,Ya-Feng Liu,Jun Zhan,Qi Zhang*

Main category: eess.SP

TL;DR: 本文研究了RIS增强的解耦共生无线电系统，通过能量检测而非解码主信号来增强安全性，并提出了低复杂度算法来最小化发射功率。


<details>
  <summary>Details</summary>
Motivation: 传统共生无线电系统中，次接收器需要解码主信号，这会导致主数据暴露给非授权接收器，存在安全隐患。本文旨在通过解耦设计消除这种模糊性，提高系统安全性。

Method: 采用RIS增强的解耦共生无线电架构，次接收器仅进行能量检测而不解码主信号。通过速率平衡重构和单调BER比表征，开发了基于惩罚的块坐标下降算法，具有闭式更新。

Result: 所提算法收敛速度快，与传统共生无线电基线相比，RIS增强的信息解耦共生无线电系统显著降低了功耗。

Conclusion: RIS增强的解耦共生无线电系统通过能量检测而非解码主信号，有效解决了传统共生无线电的安全隐患，同时提出的低复杂度算法能够高效优化系统性能，降低功耗。

Abstract: This paper studies a reconfigurable intelligent surface (RIS)-enhanced decoupled symbiotic radio (SR) system in which a primary transmitter delivers common data to multiple primary receivers (PRs), while a RIS-based backscatter device sends secondary data to a backscatter receiver (BRx). Unlike conventional SR, the BRx performs energy detection and never decodes the primary signal, thereby removing ambiguity and preventing exposure of the primary payload to unintended receivers. In this paper, we formulate the problem as the minimization of the transmit power subject to a common broadcast rate constraint across all PRs and a bit error rate (BER) constraint at the BRx. The problem is nonconvex due to the unit-modulus RIS constraint and coupled quadratic forms. Leveraging a rate-balanced reformulation and a monotonic BER ratio characterization, we develop a low-complexity penalty-based block coordinate descent algorithm with closed-form updates. Numerical results show fast convergence of the proposed algorithm and reduced power consumption of the considered RIS-enhanced information-decoupled SR system over conventional SR baselines.

</details>


### [20] [Temporal Data and Short-Time Averages Improve Multiphase Mass Flow Metering](https://arxiv.org/abs/2601.12433)
*Amanda Nyholm,Yessica Arellano,Jinyu Liu,Damian Krakowiak,Pierluigi Salvo Rossi*

Main category: eess.SP

TL;DR: 该研究证明在利用机器学习校正科里奥利质量流量计多相流测量误差时，保留时间信息能显著提升模型性能，其中CNN在0.25Hz采样频率下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前流量测量仪器在多相流条件下测量精度不足，而科里奥利质量流量计作为广泛使用的单相流量计，其测量结果可通过机器学习校正来减少多相流条件下的误差。

Method: 比较多层感知机、窗口多层感知机和卷积神经网络在342个三相气-水-油流实验数据上的表现。不同于以往将每个实验压缩为单一平均样本的方法，本研究计算实验内的短时平均值，在多个下采样间隔下保留时间信息训练模型。

Result: CNN在0.25Hz频率下表现最佳：约95%的相对误差低于13%，归一化均方根误差为0.03，平均绝对百分比误差约4.3%。明显优于最佳单平均模型，且结果在不同数据分割和随机种子下保持一致。

Conclusion: 在个体实验内进行短时平均优于单一实验平均，保留时间信息能显著提升多相流测量校正模型的性能，CNN在此类任务中表现最优。

Abstract: Reliable flow measurements are essential in many industries, but current instruments often fail to accurately estimate multiphase flows, which are frequently encountered in real-world operations. Combining machine learning (ML) algorithms with accurate single-phase flowmeters has therefore received extensive research attention in recent years. The Coriolis mass flowmeter is a widely used single-phase meter that provides direct mass flow measurements, which ML models can be trained to correct, thereby reducing measurement errors in multiphase conditions. This paper demonstrates that preserving temporal information significantly improves model performance in such scenarios. We compare a multilayer perceptron, a windowed multilayer perceptron, and a convolutional neural network (CNN) on three-phase air-water-oil flow data from 342 experiments. Whereas prior work typically compresses each experiment into a single averaged sample, we instead compute short-time averages from within each experiment and train models that preserve temporal information at several downsampling intervals. The CNN performed best at 0.25 Hz with approximately 95 % of relative errors below 13 %, a normalized root mean squared error of 0.03, and a mean absolute percentage error of approximately 4.3 %, clearly outperforming the best single-averaged model and demonstrating that short-time averaging within individual experiments is preferable. Results are consistent across multiple data splits and random seeds, demonstrating robustness.

</details>


### [21] [The Effect of Noise Correlation on MMSE Channel Estimation in One-Bit Quantized Systems](https://arxiv.org/abs/2601.12482)
*Minhua Ding,Prathapasinghe Dharmawansa,Italo Atzeni,Antti Tölli*

Main category: eess.SP

TL;DR: 本文分析了空间相关加性噪声对基于1比特量化观测的MIMO信道MMSE估计的影响，推导了适用于任意信道和噪声相关结构的通用MMSE估计器表达式，并揭示了噪声相关在特定条件下对估计性能的复杂影响。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，由于干扰、杂波或其他外部扰动，加性噪声可能具有空间相关性，但现有研究尚未探索这种相关性对基于1比特量化观测的MIMO信道MMSE估计的影响。

Method: 推导了适用于任意信道和噪声相关结构的通用MIMO MMSE信道估计器解析表达式，该估计器在1比特观测中本质上是非线性的。随后将通用表达式特化为具有单参数恒定相关结构的可处理多天线配置，以进一步分析噪声相关的影响。

Result: 分析揭示了非平凡的噪声相关诱导场景：在某些条件下，即使信道和噪声相关参数非零，估计器仍保持线性。结果表明，在低到中信噪比下，当信道不相关时，噪声相关会改善MMSE性能；但当信道强相关时，噪声相关会降低性能。

Conclusion: 本文首次系统分析了空间相关噪声对1比特量化MIMO信道MMSE估计的影响，推导了通用解析表达式，并揭示了噪声相关与信道相关之间的复杂相互作用对估计性能的影响机制。

Abstract: This paper analyzes the impact of spatially correlated additive noise on the minimum mean-square error (MMSE) estimation of multiple-input multiple-output (MIMO) channels from one-bit quantized observations. Although additive noise can be correlated in practical scenarios, e.g., due to jamming, clutter, or other external disturbances, the effect of such correlation on the MMSE channel estimator in this setting remains unexplored in prior work. Against this backdrop, we derive a novel analytical expression for the general MIMO MMSE channel estimator, which is inherently nonlinear in one-bit observations, and accommodates arbitrary channel and noise correlation structures. To further characterize the impact of noise correlation, we subsequently specialize the general MMSE expression to certain tractable multi antenna configurations in which both the channel and the noise assume single-parameter constant correlation structures. Our analyses reveal nontrivial, noise-correlation-induced scenarios in which the estimator remains linear despite non-zero channel and noise correlation parameters. Moreover, the results indicate that, at low-to-medium signal-to-noise ratio, noise correlation improves the MMSE performance when channels are uncorrelated, but degrades performance when channels are strongly correlated.

</details>


### [22] [Millimeter-Wave Multi-Radar Tracking System Enabled by a Modified GRIN Luneburg Lens for Real-Time Healthcare Monitoring](https://arxiv.org/abs/2601.12629)
*Mohammad Omid Bagheri,Justin Chow,Josh Visser,Veronica Leong,George Shaker*

Main category: eess.SP

TL;DR: 提出一种基于改进球形梯度折射率Luneburg透镜的同步毫米波多雷达跟踪系统，用于非接触式人体运动检测和跌倒监测，具有140度覆盖范围和12dB增益提升。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健环境中，非接触式运动跟踪和生命体征监测需要隐私保护、宽角度覆盖和高精度检测。传统雷达系统存在角度分辨率有限、相互干扰等问题。

Method: 使用改进的球形梯度折射率Luneburg透镜，将5个工作在58-63GHz频段的FMCW雷达模块呈半圆形排列。透镜的定制折射率分布支持双静态雷达模块，实现多个固定高增益波束。通过基于Python的集中采集框架实现时间同步和并行数据采集。

Result: 10厘米直径3D打印原型显示每个模块增益提升约12dB，检测范围显著改善。全波仿真和测量证实了在五个28度扇区（总计140度覆盖）内有效的非接触式人体运动检测。跌倒检测实验验证了可靠的宽角度性能和连续空间跟踪。

Conclusion: 该系统为毫米波传感提供了一个紧凑、低成本、可扩展的平台，适用于环境医疗保健和智能环境应用，实现了隐私保护、宽角度覆盖和高性能运动跟踪。

Abstract: Multi-beam radar sensing systems are emerging as powerful tools for non-contact motion tracking and vital-sign monitoring in healthcare environments. This paper presents the design and experimental validation of a synchronized millimeter-wave multi-radar tracking system enhanced by a modified spherical gradient-index (GRIN) Luneburg lens. Five commercial FMCW radar modules operating in the 58--63 GHz band are arranged in a semi-circular configuration around the lens, whose tailored refractive-index profile accommodates bistatic radar modules with co-located transmit (TX) and receive (RX) antennas. The resulting architecture generates multiple fixed high-gain beams with improved angular resolution and minimal mutual interference. Each radar operates independently but is temporally synchronized through a centralized Python-based acquisition framework to enable parallel data collection and low-latency motion tracking. A 10-cm-diameter 3D-printed prototype demonstrates a measured gain enhancement of approximately 12 dB for each module, corresponding to a substantial improvement in detection range. Full-wave simulations and measurements confirm effective non-contact, privacy-preserving short-range human-motion detection across five 28-degree sectors, providing 140-degree total angular coverage. Fall-detection experiments further validate reliable wide-angle performance and continuous spatial tracking. The proposed system offers a compact, low-cost, and scalable platform for millimeter-wave sensing in ambient healthcare and smart-environment applications.

</details>


### [23] [Two-Layer Reinforcement Learning-Assisted Joint Beamforming and Trajectory Optimization for Multi-UAV Downlink Communications](https://arxiv.org/abs/2601.12659)
*Ruiqi Wang,Essra M. Ghoura,Omar Alhussein,Yuzhi Yang,Yuhang Sheng,Jing Ren,Shizhong Xu,Sami Muhaidat*

Main category: eess.SP

TL;DR: 本文提出了一种分层解耦框架，结合图神经网络和多智能体强化学习，用于解决无人机通信网络中波束成形和轨迹规划的联合优化问题。


<details>
  <summary>Details</summary>
Motivation: 无人机在6G非地面网络中至关重要，但其高移动性导致波束成形和轨迹设计形成复杂的耦合优化问题。现有数值方法延迟过高，而标准深度学习方法往往忽略动态干扰拓扑，限制了可扩展性。

Method: 提出分层解耦框架：短时间尺度上，使用包含GraphNorm的拓扑感知图神经网络进行波束成形，将动态无人机-用户关联建模为时变异构图；长时间尺度上，将轨迹规划建模为去中心化部分可观测马尔可夫决策过程，采用多智能体近端策略优化算法，在集中训练分散执行范式下实现合作行为。

Result: 广泛的仿真结果表明，所提框架在系统总速率、收敛速度和泛化能力方面显著优于最先进的优化启发式方法和深度学习基线。

Conclusion: 该分层解耦框架有效解决了无人机通信网络中波束成形和轨迹规划的联合优化问题，通过结合图神经网络和多智能体强化学习，实现了高性能、低延迟和良好的可扩展性。

Abstract: Unmanned aerial vehicles (UAVs) are pivotal for future 6G non-terrestrial networks, yet their high mobility creates a complex coupled optimization problem for beamforming and trajectory design. Existing numerical methods suffer from prohibitive latency, while standard deep learning often ignores dynamic interference topology, limiting scalability. To address these issues, this paper proposes a hierarchically decoupled framework synergizing graph neural networks (GNNs) with multi-agent reinforcement learning. Specifically, on the short timescale, we develop a topology-aware GNN beamformer by incorporating GraphNorm. By modeling the dynamic UAV-user association as a time-varying heterogeneous graph, this method explicitly extracts interference patterns to achieve sub-millisecond inference. On the long timescale, trajectory planning is modeled as a decentralized partially observable Markov decision process and solved via the multi-agent proximal policy optimization algorithm under the centralized training with decentralized execution paradigm, facilitating cooperative behaviors. Extensive simulation results demonstrate that the proposed framework significantly outperforms state-of-the-art optimization heuristics and deep learning baselines in terms of system sum rate, convergence speed, and generalization capability.

</details>


### [24] [Energy-Efficient Prediction in Textile Manufacturing: Enhancing Accuracy and Data Efficiency With Ensemble Deep Transfer Learning](https://arxiv.org/abs/2601.12663)
*Yan-Chen Chen,Wei-Yu Chiu,Qun-Yu Wang,Jing-Wei Chen,Hao-Ting Zhao*

Main category: eess.SP

TL;DR: 提出EDTL框架，通过集成深度迁移学习提升纺织工厂能耗预测精度和数据效率，在数据有限场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 纺织工厂能耗大，需要节能优化；传统深度神经网络需要大量历史数据，但传感器部署和数据收集成本高，数据获取困难

Method: 提出集成深度迁移学习(EDTL)框架，结合迁移学习和集成策略，包含特征对齐层；先在数据丰富的产线预训练模型，再迁移到数据有限的产线

Result: 在真实纺织工厂数据集上，EDTL比传统DNN预测精度提升5.66%，模型鲁棒性提升3.96%，在20%-40%数据可用性场景下表现尤其突出

Conclusion: EDTL为节能纺织制造提供了准确预测且数据需求少的解决方案，为智能生产系统提供了可扩展且经济有效的方案

Abstract: Traditional textile factories consume substantial energy, making energy-efficient production optimization crucial for sustainability and cost reduction. Meanwhile, deep neural networks (DNNs), which are effective for factory output prediction and operational optimization, require extensive historical data, posing challenges due to high sensor deployment and data collection costs. To address this, we propose Ensemble Deep Transfer Learning (EDTL), a novel framework that enhances prediction accuracy and data efficiency by integrating transfer learning with an ensemble strategy and a feature alignment layer. EDTL pretrains DNN models on data-rich production lines (source domain) and adapts them to data-limited lines (target domain), reducing dependency on large datasets. Experiments on real-world textile factory datasets show that EDTL improves prediction accuracy by 5.66% and enhances model robustness by 3.96% compared to conventional DNNs, particularly in data-limited scenarios (20%-40% data availability). This research contributes to energy-efficient textile manufacturing by enabling accurate predictions with fewer data requirements, providing a scalable and cost-effective solution for smart production systems.

</details>


### [25] [Energy-Based Cell Association in Nonuniform Renewable Energy-Powered Cellular Networks: Analysis and Optimization of Carbon Efficiency](https://arxiv.org/abs/2601.12708)
*Yuxi Zhao,Vicente Casares-Giner,Vicent Pla,Luis Guijarro,Iztok Humar,Yi Zhong,Xiaohu Ge*

Main category: eess.SP

TL;DR: 该论文提出了一种基于能量的蜂窝关联方案，通过建模可再生能源状态和信道占用情况，优化蜂窝网络的碳效率，相比最近关联方案可减少13.0%碳排放并提高11.3%碳效率。


<details>
  <summary>Details</summary>
Motivation: 全球碳减排压力下，需要将可再生能源整合到蜂窝网络供应链中。但由于可再生能源发电的随机性和基站间负载分布不均，可再生能源利用率仍然较低，需要在碳排放和下行吞吐量之间进行权衡优化。

Method: 将基站电池的可再生能源状态和占用信道数建模为准生灭过程，基于随机几何构建信道阻塞概率、用户平均成功传输概率、下行吞吐量、碳排放和碳效率模型，提出基于能量的蜂窝关联方案来优化网络碳效率。

Result: 相比最近蜂窝关联方案，基于能量的蜂窝关联方案能够将网络碳排放减少13.0%，并将碳效率提高11.3%。

Conclusion: 提出的基于能量的蜂窝关联方案有效解决了可再生能源利用率低的问题，在保证网络性能的同时显著提升了蜂窝网络的碳效率，为实现可持续蜂窝网络提供了有效解决方案。

Abstract: The increasing global push for carbon reduction highlights the importance of integrating renewable energy into the supply chain of cellular networks. However, due to the stochastic nature of renewable energy generation and the uneven load distribution across base stations, the utilization rate of renewable energy remains low. To address these challenges, this paper investigates the trade-off between carbon emissions and downlink throughput in cellular networks, offering insights into optimizing both network performance and sustainability. The renewable energy state of base station batteries and the number of occupied channels are modeled as a quasi-birth-death process. We construct models for the probability of channel blocking, average successful transmission probability for users, downlink throughput, carbon emissions, and carbon efficiency based on stochastic geometry. Based on these analyses, an energy-based cell association scheme is proposed to optimize the carbon efficiency of cellular networks. The results show that, compared to the closest cell association scheme, the energy-based cell association scheme is capable of reducing the carbon emissions of the network by 13.0% and improving the carbon efficiency by 11.3%.

</details>


### [26] [Robust Beamforming and Time Allocation for Time-Division Cell-Free Near-Field ISAC](https://arxiv.org/abs/2601.12725)
*Chaedam Son,Si-Hyeon Lee*

Main category: eess.SP

TL;DR: 提出了一种用于无蜂窝MIMO的时分近场集成感知与通信框架，通过感知阶段估计用户位置来构建位置感知信道，优化时间分配比、感知协方差矩阵和鲁棒下行波束成形，以提高定位精度和通信速率。


<details>
  <summary>Details</summary>
Motivation: 在无蜂窝MIMO系统中，需要解决感知精度与通信吞吐量之间的权衡问题。传统方法未能充分考虑感知引起的定位误差与信道估计误差之间的耦合关系，限制了系统性能。

Method: 提出时分近场ISAC框架，将感知和下行通信在时间上分离。感知阶段估计用户位置并构建位置感知信道，通信阶段利用这些信息。通过联合优化时间分配比、感知协方差矩阵和鲁棒下行波束成形，并采用基于SDP的交替优化框架。还提出了两种低复杂度次优方案：误差忽略方案和基于MRT的方案。

Result: 仿真结果表明，所提方案相比远场和单站设置显著提高了定位精度，从而减少了信道估计误差并最终提高了可达速率。误差忽略方案在严格感知要求下表现良好，而基于MRT的方案通过调整时间分配比，在各种感知要求下保持鲁棒性（尽管有波束成形损失）。

Conclusion: 该研究成功解决了无蜂窝MIMO系统中感知与通信的权衡问题，提出的框架和优化方法有效提升了系统性能，为实际部署提供了可行的解决方案。

Abstract: In this paper, we propose a time-division near-field integrated sensing and communication (ISAC) framework for cell-free multiple-input multiple-output (MIMO), where sensing and downlink communication are separated in time. During the sensing phase, user locations are estimated and used to construct location-aware channels, which are then exploited in the subsequent communication phase. By explicitly modeling the coupling between sensing-induced localization errors and channel-estimation errors, we capture the tradeoff between sensing accuracy and communication throughput. Based on this model, we jointly optimize the time-allocation ratio, sensing covariance matrix, and robust downlink beamforming under imperfect channel state information (CSI). The resulting non-convex problem is addressed via a semidefinite programming (SDP)-based reformulation within an alternating-optimization framework. To further reduce computational complexity, we also propose two low-complexity suboptimal designs: an error-ignorant scheme and a maximum ratio transmission (MRT)-based scheme. Simulation results show that the proposed scheme significantly improves localization accuracy over far-field and monostatic setups, thereby reducing channel estimation errors and ultimately enhancing the achievable rate. Moreover, the error-ignorant scheme performs well under stringent sensing requirements, whereas the MRT-based scheme remains robust over a wide range of sensing requirements by adapting the time-allocation ratio, albeit with some beamforming loss.

</details>


### [27] [Movable Antenna Enhanced MIMO Communications with Spatial Modulation](https://arxiv.org/abs/2601.12788)
*Kaihe Wang,Ran Yang,Lipeng Zhu,Rongyan Xi,Yue Xiu,Zhongpei Zhang*

Main category: eess.SP

TL;DR: 提出一种基于可移动天线（MA）的MIMO空间调制系统，通过联合收发器设计最小化误码率，采用交替优化和逐次凸逼近算法实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 可移动天线能增强无线通信性能，结合空间调制可以在提升性能的同时减少射频链成本，但需要有效的联合收发器设计来优化系统性能。

Method: 提出基于最大最小距离准则的联合收发器设计框架，采用交替优化（AO）和逐次凸逼近（SCA）技术开发高效迭代算法。

Result: 仿真结果显示所提算法具有快速收敛性能，显著优于现有基准方案。

Conclusion: MA-enabled MIMO空间调制系统通过提出的联合收发器设计能有效提升通信性能，同时降低系统成本，具有实际应用价值。

Abstract: Movable antenna (MA) has demonstrated great potential in enhancing wireless communication performance. In this paper, we investigate an MA-enabled multiple-input multiple-output (MIMO) communication system with spatial modulation (SM), which improves communication performance by utilizing flexible MA placement while reducing the cost of RF chains. To this end, we propose a joint transceiver design framework aimed at minimizing the bit error rate (BER) based on the maximum minimum distance (MMD) criterion. To address the intractable problem, we develop an efficient iterative algorithm based on alternating optimization (AO) and successive convex approximation (SCA) techniques. Simulation results demonstrate that the proposed algorithm achieves rapid convergence performance and significantly outperforms the existing benchmark schemes.

</details>


### [28] [Integrated Sensing and Semantic Communication with Adaptive Source-Channel Coding](https://arxiv.org/abs/2601.12827)
*Haotian Wang,Dan Wang,Xiaodong Xu,Chuan Huang,Hao Chen,Nan Ma*

Main category: eess.SP

TL;DR: 提出用于集成感知与语义通信系统的自适应信源信道编码与波束成形设计框架，通过联合优化语义通信任务的编码率和感知任务的波束成形，实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注感知数据压缩以减少通信开销，但缺乏同时考虑语义通信和感知任务的集成传输框架。需要设计一个统一的系统来同时优化语义通信和感知性能。

Method: 提出自适应信源信道编码和波束成形设计框架：1) 推导端到端语义失真函数的上界；2) 推导时间同步不完美下的目标位置混合克拉美罗下界；3) 建立考虑HCRB阈值、信道使用和功率预算的失真最小化问题；4) 提出基于逐次凸逼近和分式规划的交替优化算法，将问题解耦为编码率和波束成形两个子问题。

Result: 仿真结果表明，所提方案优于传统的深度联合信源信道编码-注水-迫零基准方案，在集成感知和语义通信系统中实现了更好的性能。

Conclusion: 该研究为6G集成感知与语义通信系统提供了一个有效的自适应设计框架，通过联合优化编码率和波束成形，在满足感知精度要求的同时优化语义通信性能，为未来通信系统设计提供了新思路。

Abstract: Semantic communication has emerged as a new paradigm to facilitate the performance of integrated sensing and communication systems in 6G. However, most of the existing works mainly focus on sensing data compression to reduce the subsequent communication overheads, without considering the integrated transmission framework for both the SemCom and sensing tasks. This paper proposes an adaptive source-channel coding and beamforming design framework for integrated sensing and SemCom systems by jointly optimizing the coding rate for SemCom task and the transmit beamforming for both the SemCom and sensing tasks. Specifically, an end-to-end semantic distortion function is approximated by deriving an upper bound composing of source and channel coding induced components, and then a hybrid Cramér-Rao bound (HCRB) is also derived for target position under imperfect time synchronization. To facilitate the joint optimization, a distortion minimization problem is formulated by considering the HCRB threshold, channel uses, and power budget. Subsequently, an alternative optimization algorithm composed of successive convex approximation and fractional programming is proposed to address this problem by decoupling it into two subproblems for coding rate and beamforming designs, respectively. Simulation results demonstrate that our proposed scheme outperforms the conventional deep joint source-channel coding -water filling-zero forcing benchmark.

</details>


### [29] [Angular Sensing by Highly Reconfigurable Pixel Antennas with Joint Radiating Aperture and Feeding Ports Reconfiguration](https://arxiv.org/abs/2601.12867)
*Zixiang Han,Hanning Wang,Shiwen Tang,Yujie Zhang*

Main category: eess.SP

TL;DR: 提出高度可重构像素天线(HRPA)，通过联合重构辐射孔径和馈电端口实现角度感知能力，相比传统均匀平面阵列将角度估计误差降低50%以上。


<details>
  <summary>Details</summary>
Motivation: 传统像素天线只有固定位置的单一馈电端口，对于角度感知不够高效。需要扩展像素天线的可重构性，同时控制天线几何形状和馈电端口切换，以提升角度感知性能。

Method: 提出高度可重构像素天线(HRPA)，建立包含电路和辐射参数的模型。定义包含像素连接状态和馈电端口位置的码本，开发优化方法最小化CRLB，获取给定区域内角度感知的最优HRPA几何形状。

Result: HRPA在全三维球面上将角度估计误差降低超过50%，相比相同尺寸的传统均匀平面阵列。数值结果验证了方法的有效性。

Conclusion: HRPA方法有效提升了角度感知性能，展示了在集成感知和通信系统中的潜力。通过联合重构辐射孔径和馈电端口，实现了显著的角度估计精度提升。

Abstract: Angular sensing capability is realized using highly reconfigurable pixel antenna (HRPA) with joint radiating aperture and feeding ports reconfiguration. Pixel antennas represent a general class of reconfigurable antenna designs in which the radiating surface, regardless of its shape or size, is divided into sub-wavelength elements called pixels. Each pixel is connected to its neighboring elements through radio frequency switches. By controlling pixel connections, the pixel antenna topology can be flexibly adjusted so that the resulting radiation pattern can be reconfigured. However, conventional pixel antennas have only a single, fixed-position feeding port, which is not efficient for angular sensing. Therefore, in this work, we further extend the reconfigurability of pixel antennas by introducing the HRPA, which enables both geometry control of the pixel antenna and switching of its feeding ports. The model of the proposed HRPA, including both circuit and radiation parameters, is derived. A codebook is then defined, consisting of pixel connection states and feeding port positions for each sensing area. Based on this codebook, an efficient optimization approach is developed to minimize the Cram\acute{\mathrm{\mathbf{e}}}r-Rao lower bound (CRLB) and obtain the optimal HRPA geometries for angular sensing within a given area. Numerical results show that the HRPA reduces the angle estimation error by more than 50% across the full three-dimensional sphere when compared with a conventional uniform planar array of the same size. This demonstrates the effectiveness of the proposed approach and highlights the potential of HRPA for integrated sensing and communication systems.

</details>


### [30] [Fluid Antenna Relay (FAR)-assisted Communication with Hybrid Relaying Scheme Selection](https://arxiv.org/abs/2601.12924)
*Ruopeng Xu,Songling Zhang,Zhaohui Yang,Mingzhe Chen,Zhaoyang Zhang,Kai-Kit Wong*

Main category: eess.SP

TL;DR: 该论文研究了采用混合中继方案选择的流体天线中继通信系统，通过高斯copula方法近似计算不同中继方案的中断概率，并基于中断概率最小化原则选择转发方案，最终通过功率控制优化提升系统总速率。


<details>
  <summary>Details</summary>
Motivation: 传统中继系统存在自干扰和多用户干扰问题，且半双工模式会引入速率损失。流体天线系统具有空间分集优势，但需要有效的方案选择和资源分配来最大化系统性能。

Method: 1) 利用统计CSI和FAS分布特性，采用高斯copula方法近似计算不同中继方案的中断概率；2) 基于中断概率最小化原则选择中继转发方案；3) 采用半双工中继和频分多址避免干扰；4) 将原始非凸优化问题转化为功率控制问题，通过闭式带宽分配和低复杂度算法求解。

Result: 仿真结果表明，所提算法能有效提升系统总速率，验证了流体天线中继系统结合混合中继方案选择和优化资源分配的性能优势。

Conclusion: 该研究提出了一种有效的流体天线中继系统优化框架，通过混合中继方案选择和功率控制，在避免干扰的同时最大化系统总速率，为未来移动通信系统提供了有前景的解决方案。

Abstract: In this paper, we investigate a fluid antenna relay (FAR)-assisted communication system with hybrid relaying scheme selection. By leveraging statistical channel state information (CSI) and distribution characteristics of fluid antenna system (FAS), we approximate the outage probability (OP) with different relaying schemes utilizing a Gaussian copula-based method. Each relay node follows the OP-minimized principle to choose the forwarding schemes. To reduce self-interference and avoid multi-user interference, half-duplex relays and frequency division multiple access schemes are considered, respectively. On this basis, we formulate a sum-rate maximization problem to mitigate the rate loss introduced by the half-duplex mode. To solve this problem, we first transform the original nonconvex problem into a power control optimization problem by obtaining the closed form of bandwidth allocation and substituting it into the original problem. Then, we solve the power control optimization problem with a low complexity method. Simulation results verify the effectiveness of our proposed algorithm to improve the sum rate of the system.

</details>


### [31] [Monostatic ISAC Without Full Buffers: Revisiting Spatial Trade-Offs Under Bursty Traffic](https://arxiv.org/abs/2601.12963)
*Mauro Marchese,Musa Furkan Keskin,Pietro Savazzi,Henk Wymeersch*

Main category: eess.SP

TL;DR: 研究在突发流量下，单基地集成感知与通信基站发射波束成形设计中的空间权衡问题，揭示了影响ISAC性能的多个效应


<details>
  <summary>Details</summary>
Motivation: 下一代无线系统需要集成通信和感知功能，但在突发流量场景下，基站并不总是有数据传输可用，这给ISAC设计带来了新的挑战

Method: 比较不同的ISAC策略，分析数据辅助策略与导频策略之间的性能差异，考虑非全缓冲假设下的检测概率饱和效应，以及目标与用户相对位置引起的方向性掩蔽效应

Result: 仿真结果显示，在不同操作条件下，这些效应对ISAC权衡产生不同影响：数据辅助策略相比导频策略具有SNR提升优势，但存在检测概率饱和问题，同时目标与用户的相对位置会导致方向性掩蔽

Conclusion: 研究揭示了突发流量下ISAC性能的多个关键效应，为设计高效的ISAC传输策略提供了指导，表明需要根据具体操作条件平衡这些效应

Abstract: This work investigates the spatial trade-offs arising from the design of the transmit beamformer in a monostatic integrated sensing and communication (ISAC) base station (BS) under bursty traffic, a crucial aspect necessitated by the integration of communication and sensing functionalities in next-generation wireless systems. In this setting, the BS does not always have data available for transmission. This study compares different ISAC policies and reveals the presence of multiple effects influencing ISAC performance: signal-to-noise ratio (SNR) boosting of data-aided strategies compared to pilot-based ones, saturation of the probability of detection in data-aided strategies due to the non-full-buffer assumption, and, finally, directional masking of sensing targets due to the relative position between target and user. Simulation results demonstrate varying impact of these effects on ISAC trade-offs under different operating conditions, thus guiding the design of efficient ISAC transmission strategies.

</details>


### [32] [6G OFDM Communications with High Mobility Transceivers and Scatterers via Angle-Domain Processing and Deep Learning](https://arxiv.org/abs/2601.12970)
*Mauro Marchese,Musa Furkan Keskin,Henk Wymeersch,Pietro Savazzi*

Main category: eess.SP

TL;DR: 提出一种利用角度域分离多径的OFDM接收机架构，通过块状导频估计多径参数，采用决策导向方法迭代优化多普勒估计，DL方法可在1000km/h速度下保持恒定BER性能


<details>
  <summary>Details</summary>
Motivation: 高速移动通信导致OFDM波形因多普勒效应产生强载波间干扰，需要解决高速场景下的ICI问题

Method: 提出新颖接收机架构：1) 发送块状导频估计多径的到达方向、传播时延和信道增益；2) 采用决策导向方法估计并迭代优化多普勒；3) 研究两种初始多普勒估计方法：基于误差向量幅度的方法和基于深度学习的方法

Result: 仿真结果显示，基于深度学习的方法可在最高1000km/h的6G速度下保持恒定的误码率性能

Conclusion: 提出的接收机架构能有效解决高速移动通信中的ICI问题，特别是DL方法在极端高速场景下表现优异

Abstract: High-mobility communications, which are crucial for next-generation wireless systems, cause the orthogonal frequency division multiplexing (OFDM) waveform to suffer from strong intercarrier interference (ICI) due to the Doppler effect. In this work, we propose a novel receiver architecture for OFDM that leverages the angular domain to separate multipaths. A block-type pilot is sent to estimate direction-of-arrivals (DoAs), propagation delays, and channel gains of the multipaths. Subsequently, a decision-directed (DD) approach is employed to estimate and iteratively refine the Dopplers. Two different approaches are investigated to provide initial Doppler estimates: an error vector magnitude (EVM)-based method and a deep learning (DL)-based method. Simulation results reveal that the DL-based approach allows for constant bit error rate (BER) performance up to the maximum 6G speed of 1000 km/h.

</details>


### [33] [Physics-Aware RIS Codebook Compilation for Near-Field Beam Focusing under Mutual Coupling and Specular Reflections](https://arxiv.org/abs/2601.12982)
*Alexandros I. Papadopoulos,Maria Anna Pistela,Dimitrios Tyrovolas,Antonios Lalas,Konstantinos Votis,Sotiris Ioannidis,George K. Karagiannidis,Christos Liaskos*

Main category: eess.SP

TL;DR: MATCH：一种基于物理的RIS码本编译算法，考虑单元间电磁耦合和环境反射，在近场条件下实现高效可编程无线环境配置。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需要在强多径和严重信道变化的环境中实现可靠低延迟连接。可编程无线环境通过RIS实现确定性电磁传播控制，但在实时配置RIS方面存在瓶颈，特别是在近场条件下，互耦合和镜面反射会改变预期响应。

Method: 提出MATCH算法，一种基于物理的码本编译方法，明确考虑RIS单元间的电磁耦合以及与环境结构的反射相互作用，确保生成的码本与环境物理特性保持一致。

Result: 在全波仿真框架下评估MATCH，包含互耦合和二次反射，证明其能够将散射能量集中在焦点区域，验证了基于物理一致码本的优化是实用高效的RIS配置方法。

Conclusion: 基于物理的码本编译算法MATCH能够有效解决RIS在近场条件下的配置问题，为可编程无线环境的实际应用提供了高效解决方案。

Abstract: Next-generation wireless networks are envisioned to achieve reliable, low-latency connectivity within environments characterized by strong multipath and severe channel variability. Programmable wireless environments (PWEs) address this challenge by enabling deterministic control of electromagnetic (EM) propagation through software-defined reconfigurable intelligent surfaces (RISs). However, effectively configuring RISs in real time remains a major bottleneck, particularly under near-field conditions where mutual coupling and specular reflections alter the intended response. To overcome this limitation, this paper introduces MATCH, a physics-based codebook compilation algorithm that explicitly accounts for the EM coupling among RIS unit cells and the reflective interactions with surrounding structures, ensuring that the resulting codebooks remain consistent with the physical characteristics of the environment. Finally, MATCH is evaluated under a full-wave simulation framework incorporating mutual coupling and secondary reflections, demonstrating its ability to concentrate scattered energy within the focal region, confirming that physics-consistent, codebook-based optimization constitutes an effective approach for practical and efficient RIS configuration.

</details>


### [34] [When Is Distributed Nonlinear Aggregation Private? Optimality and Information-Theoretical Bounds](https://arxiv.org/abs/2601.13001)
*Wenrui Yu,Jaron Skovsted Gundersen,Richard Heusdens,Qiongxiu Li*

Main category: eess.SP

TL;DR: 该论文建立了信息论框架分析分布式非线性聚合的隐私泄露，推导了在不牺牲准确性情况下的基本下界，并提出了达到最优界的隐私保护算法。


<details>
  <summary>Details</summary>
Motivation: 非线性聚合在现代分布式系统中至关重要，但其隐私行为远不如线性聚合被充分理解。非线性算子对可实现的理论隐私保证施加了固有的结构限制，特别是当需要精确计算聚合值时。

Method: 开发了统一的信息论框架来表征分布式非线性聚合中的隐私泄露，涵盖两类非线性聚合：基于顺序的算子（最大值/最小值和top-K）和鲁棒聚合（中位数/分位数和修剪均值）。推导了在不牺牲准确性情况下的基本下界，并提出了简单的隐私保护分布式算法。

Result: 推导了隐私泄露的基本下界，确定了计算和传输中不可避免的最小信息泄露。提出的算法通过适当的随机初始化和参数选择，能够达到推导出的最优界。实验验证了界的紧致性，并表明网络拓扑和关键算法参数（包括步长）按照理论分析控制观察到的泄露。

Conclusion: 该研究为隐私保护的分布式非线性聚合提供了理论框架和实用算法，识别了不可避免的隐私泄露下界，并展示了如何通过算法设计达到这些最优界，为实际系统提供了可操作的指导方针。

Abstract: Nonlinear aggregation is central to modern distributed systems, yet its privacy behavior is far less understood than that of linear aggregation. Unlike linear aggregation where mature mechanisms can often suppress information leakage, nonlinear operators impose inherent structural limits on what privacy guarantees are theoretically achievable when the aggregate must be computed exactly. This paper develops a unified information-theoretic framework to characterize privacy leakage in distributed nonlinear aggregation under a joint adversary that combines passive (honest-but-curious) corruption and eavesdropping over communication channels.
  We cover two broad classes of nonlinear aggregates: order-based operators (maximum/minimum and top-$K$) and robust aggregation (median/quantiles and trimmed mean). We first derive fundamental lower bounds on leakage that hold without sacrificing accuracy, thereby identifying the minimum unavoidable information revealed by the computation and the transcript. We then propose simple yet effective privacy-preserving distributed algorithms, and show that with appropriate randomized initialization and parameter choices, our proposed approaches can attach the derived optimal bounds for the considered operators. Extensive experiments validate the tightness of the bounds and demonstrate that network topology and key algorithmic parameters (including the stepsize) govern the observed leakage in line with the theoretical analysis, yielding actionable guidelines for privacy-preserving nonlinear aggregation.

</details>


### [35] [OTFS-IDMA: An Unsourced Multiple Access Scheme for Doubly-Dispersive Channels](https://arxiv.org/abs/2601.13065)
*Davide Bergamasco,Federico Clazzer,Paolo Casari*

Main category: eess.SP

TL;DR: 提出一种针对高移动性无线信道的无源多址接入方案，基于OTFS调制和稀疏交织分多址，在延迟-多普勒域实现联合活动检测和信道估计。


<details>
  <summary>Details</summary>
Motivation: 解决高移动性无线信道中的无源多址接入问题，传统方案在双选择性信道中性能受限，需要适应时频双选择性信道特性的新方案。

Method: 采用正交时频空间调制和稀疏交织分多址技术，在延迟-多普勒域设计接收机，通过压缩感知实现联合活动检测和信道估计，然后使用最大比合并原理的单用户解码器利用多径分集。

Result: 数值结果表明延迟-多普勒域无协调方案在双选择性信道中具有潜力，同时指出了设计权衡和剩余挑战。

Conclusion: 基于延迟-多普勒域的无源多址接入方案为高移动性无线通信提供了有前景的解决方案，但仍需进一步解决设计中的权衡和挑战。

Abstract: We present an unsourced multiple access (UMAC) scheme tailored to high-mobility wireless channels. The proposed construction is based on orthogonal time frequency space (OTFS) modulation and sparse interleaver division multiple access (IDMA) in the delay-Doppler (DD) domain. The receiver runs a compressive-sensing joint activity-detection and channel estimation process followed by a single-user decoder which harnesses multipath diversity via the maximal-ratio combining (MRC) principle. Numerical results show the potential of DD-based uncoordinated schemes in the presence of double selectivity, while remarking the design tradeoffs and remaining challenges introduced by the proposed design.

</details>


### [36] [Seeing Radio: From Zero RF Priors to Explainable Modulation Recognition with Vision Language Models](https://arxiv.org/abs/2601.13157)
*Hang Zou,Bohao Wang,Yu Tian,Lina Bariah,Chongwen Huang,Samson Lasaulce,Mérouane Debbah*

Main category: eess.SP

TL;DR: 利用视觉语言模型进行射频调制识别，通过将IQ信号转换为图像，无需专门设计射频接收器，实现了90%的分类准确率


<details>
  <summary>Details</summary>
Motivation: 传统射频感知需要设计专门的神经网络接收器，而视觉语言模型的兴起为射频感知提供了新途径。研究探索是否可以通过将射频波形表示为图像，让通用视觉语言模型识别调制方式

Method: 提出将复杂IQ流转换为视觉可解释输入的实用管道，构建包含57类模拟/数字调制的RF视觉问答基准框架，使用三种互补图像模式：时域IQ段、幅度谱图和联合表示。设计统一的零样本和少样本提示进行类别级和家族级评估

Result: 微调后的视觉语言模型达到90%的准确率（基础模型仅10%），在噪声下表现稳健，对未见调制类型具有高泛化性能，无需射频领域先验知识或专门架构

Conclusion: RF到图像的转换与可提示视觉语言模型结合，为未来6G网络中的射频感知AI系统提供了可扩展且实用的基础

Abstract: The rise of vision language models (VLMs) paves a new path for radio frequency (RF) perception. Rather than designing task-specific neural receivers, we ask if VLMs can learn to recognize modulations when RF waveforms are expressed as images. In this work, we find that they can. In specific, in this paper, we introduce a practical pipeline for converting complex IQ streams into visually interpretable inputs, hence, enabling general-purpose VLMs to classify modulation schemes without changing their underlying design. Building on this, we construct an RF visual question answering (VQA) benchmark framework that covers 57 classes across major families of analog/digital modulations with three complementary image modes, namely, (i) short \emph{time-series} IQ segments represented as real/imaginary traces, (ii) magnitude-only \emph{spectrograms}, and (iii) \emph{joint} representations that pair spectrograms with a synchronized time-series waveforms. We design uniform zero-shot and few-shot prompts for both class-level and family-level evaluations. Our finetuned VLMs with these images achieve competitive accuracy of $90\%$ compared to $10\%$ of the base models. Furthermore, the fine-tuned VLMs show robust performance under noise and demonstrate high generalization performance to unseen modulation types, without relying on RF-domain priors or specialized architectures. The obtained results show that combining RF-to-image conversion with promptable VLMs provides a scalable and practical foundation for RF-aware AI systems in future 6G networks.

</details>


### [37] [Decentralized Cooperative Beamforming for BDRIS-Assisted Cell-Free MIMO OFDM Systems](https://arxiv.org/abs/2601.13201)
*Konstantinos D. Katsanos,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 提出了一种针对宽带无小区多流多用户MIMO-OFDM系统的鲁棒去中心化主动和被动波束赋形框架，采用动态组连接BDRIS架构，通过分布式优化显著降低集中式处理开销。


<details>
  <summary>Details</summary>
Motivation: 传统集中式波束赋形方案需要高计算能力的中共处理单元，存在显著的开销问题。本文旨在开发一种去中心化框架，减少基站间协作需求，同时应对信道状态信息不完美的挑战。

Method: 采用动态组连接BDRIS架构，考虑频率选择性响应。通过联合优化BDRIS的可调电容、置换矩阵和基站的预编码矩阵，使用连续凹逼近、交替投影和基于共识的更新算法解决系统和速率最大化问题。

Result: 仿真结果表明，所提出的鲁棒去中心化协作方法在不同BDRIS架构下均优于非协作基准方案。动态组连接BDRIS架构能够提供接近更复杂全连接BDRIS结构的和速率性能增益。

Conclusion: 提出的去中心化波束赋形框架有效降低了系统开销，动态组连接BDRIS架构在性能和复杂度之间取得了良好平衡，为智能无线环境中的宽带无小区MIMO-OFDM系统提供了实用解决方案。

Abstract: In this paper, a wideband cell-free multi-stream multi-user Multiple-Input Multiple-Output (MIMO) Orthogonal Frequency Division Multiplexing (OFDM) system is considered operating within a smart wireless environment enabled by multiple Beyond Diagonal Reconfigurable Intelligent Surfaces (BDRISs). A novel decentralized active and passive beamforming framework, robust to imperfect channel state availability and with minimal cooperation among the system's multiple Base Stations (BSs) for deciding the final configurations of the shared BDRISs, is proposed, which aims to substantially reduce the overhead inherent in centralized solutions necessitating a central processing unit of high computational power. By considering a Dynamic Group-Connected (DGC) BDRIS architecture with frequency-selective responses per unit element, we formulate the system's sum-rate maximization problem with respect to the tunable capacitances and permutation matrices of the BDRISs as well as the precoding matrices of the BSs, which is solved via successive concave approximation and alternating projections as well as consensus-based updates for the BDRISs' design. Through extensive simulation results, it is showcased that the proposed robust decentralized cooperative approach with diverse BDRIS architectures outperforms non-cooperation benchmarks. It is also demonstrated that the considered DGC BDRIS architecture is able to provide sum-rate performance gains sufficiently close to the more complex fully-connected BDRIS structure.

</details>


### [38] [Hierarchical Sparse Vector Transmission for Ultra Reliable and Low Latency Communications](https://arxiv.org/abs/2601.13204)
*Yanfeng Zhang,Xi'an Fan,Jinkai Zheng,Xiaoye Jing,Weiwei Yang,Xu Zhu*

Main category: eess.SP

TL;DR: 提出了一种用于多用户URLLC场景的分层稀疏向量传输方案，通过将传输比特分为公共和私有部分，分别用稀疏向量的非零段索引和特定块长度的非零块嵌入信息，在接收端先恢复公共比特再解码用户私有比特，性能优于现有SVT方案。


<details>
  <summary>Details</summary>
Motivation: 稀疏向量传输是实现超可靠低延迟通信的有前景技术，但现有方案在多用户场景下的性能需要进一步提升。本文旨在设计一种适用于多用户URLLC场景的改进SVT方案。

Method: 提出分层SVT方案：1) 将传输比特划分为公共部分和私有部分；2) 公共信息通过稀疏向量中的非零段索引传输；3) 每个用户的私有信息嵌入到具有特定块长度的非零块中；4) 接收端先检测非零段恢复公共比特，再根据对应的非零块索引解码用户私有比特。

Result: 仿真结果表明，所提出的分层SVT方案在块错误率方面优于最先进的SVT方案。

Conclusion: 分层SVT方案为多用户URLLC场景提供了一种有效的通信方案，通过分离公共和私有信息传输，提高了系统性能，在块错误率方面表现出优越性。

Abstract: Sparse vector transmission (SVT) is a promising candidate technology for achieving ultra-reliable low-latency communication (URLLC). In this paper, a hierarchical SVT scheme is proposed for multi-user URLLC scenarios. The hierarchical SVT scheme partitions the transmitted bits into common and private parts. The common information is conveyed by the indices of non-zero sections in a sparse vector, while each user's private information is embedded into non-zero blocks with specific block lengths. At the receiver, the common bits are first recovered from the detected non-zero sections, followed by user-specific private bits decoding based on the corresponding non-zero block indices. Simulation results show the proposed scheme outperforms state-of-the-art SVT schemes in terms of block error rate.

</details>


### [39] [Co-Channel Interference Mitigation Using Deep Learning for Drone-Based Large-Scale Antenna Measurements](https://arxiv.org/abs/2601.13205)
*Kadyrzhan Tortayev,Oliver Falkenberg Damborg,Jònas À Hàlvmørk Joensen,Jonas Pedesk,Yifa Li,Fengchun Zhang,Zeliang An,Yubo Wang,Ming Shen*

Main category: eess.SP

TL;DR: 提出轻量级深度卷积神经网络(DC-CNN)用于无人机天线辐射测量中，在强同信道干扰下准确估计连续波信号幅度


<details>
  <summary>Details</summary>
Motivation: 无人机天线辐射测量中，传统FFT方法在信号干扰比低于-10dB时无法准确估计连续波信号幅度，需要解决强同信道干扰下的测量问题

Method: 设计轻量级深度卷积神经网络(DC-CNN)，参数少于2万，在真实5GHz测量数据上训练，覆盖-33.3dB到+46.7dB的信号干扰比范围

Result: DC-CNN在全范围内平均绝对误差7%，在SIR≥-30dB时误差小于1dB，相比传统FFT方法在SIR<-10dB时失效有显著改进

Conclusion: DC-CNN具有鲁棒性和高效性，适合部署在嵌入式无人机平台上，实现抗干扰的天线方向图测量

Abstract: Unmanned aerial vehicles (UAVs) enable efficient in-situ radiation characterization of large-aperture antennas directly in their deployment environments. In such measurements, a continuous-wave (CW) probe tone is commonly transmitted to characterize the antenna response. However, active co-channel emissions from neighboring antennas often introduce severe in-band interference, where classical FFT-based estimators fail to accurately estimate the CW tone amplitude when the signal-to-interference ratios (SIR) falls below -10 dB. This paper proposes a lightweight deep convolutional neural network (DC-CNN) that estimates the amplitude of the CW tone. The model is trained and evaluated on real 5~GHz measurement bursts spanning an effective SIR range of --33.3 dB to +46.7 dB. Despite its compact size (<20k parameters), the proposed DC-CNN achieves a mean absolute error (MAE) of 7% over the full range, with <1 dB error for SIR >= -30 dB. This robustness and efficiency make DC-CNN suitable for deployment on embedded UAV platforms for interference-resilient antenna pattern characterization.

</details>


### [40] [Semantic Communication in Underwater IoT Networks for Meaning-Driven Connectivity](https://arxiv.org/abs/2601.13289)
*Ruhul Amin Khalil,Asiya Jehangir,Hanane Lamaazi,Sadaf Rubab,Nasir Saeed*

Main category: eess.SP

TL;DR: 该论文综述了水下物联网中语义通信的最新进展，探讨了AI驱动的语义压缩、上下文感知优先处理和鲁棒信息重建技术，以解决水下通信的带宽、延迟和能耗限制问题。


<details>
  <summary>Details</summary>
Motivation: 水下物联网面临传统数据通信的挑战：窄带宽、高延迟和严格能耗限制。需要新的通信范式来提升效率，语义通信通过传输信息的含义而非原始符号，有望解决这些问题。

Method: 调查了AI驱动的语义通信框架，包括大语言模型、基于扩散的生成编码器和联邦学习，结合语义压缩与上下文感知优先处理。还考虑了混合声学-光学-RF架构和边缘智能语义编码器。

Result: 提供了水下考古、海洋生态学和自主水下航行器协调等应用案例，展示了语义通信的优势。提出了语义表示标准化、跨域插值和隐私保护方案等建议。

Conclusion: 语义通信是解决水下物联网通信挑战的有前景范式，但需要解决标准化、跨域集成和隐私保护等问题，才能开发出可信赖的语义通信水下物联网系统。

Abstract: The Internet of Underwater Things (IoUT) is revolutionizing marine sensing and environmental monitoring, as well as subaquatic exploration, which are enabled by interconnected and intelligent subsystems. Nevertheless, underwater communication is constrained by narrow bandwidth, high latency, and strict energy constraints, which are the source of efficiency problems in traditional data-centric networks. To tackle these problematic issues, this work provides a survey of recent advances in Semantic Communication (SC) for IoUT, a novel communication paradigm that seeks to harness not raw symbol information but rather its meaning and/or contextual significance. In this paper, we investigate the emerging advanced AI-powered frameworks, including large language models (LLMs), diffusion-based generative encoders, and federated learning (FL), that bridge semantic compression with context-aware prioritization and robust information reconstruction over noisy underwater channels. Hybrid acoustic-optical-RF architectures and edge-intelligent semantic encoders are also considered enablers of sustainable, adaptive operations. Examples in underwater archaeology, marine ecology, and autonomous underwater vehicles (AUVs) coordination are provided as a relief to illustrate the merits of meaning-driven connectivity. The paper concludes with some recommendations, including semantic representations standardization, cross-domain interpolation, and privacy-support schemes. These issues must be addressed in the future before trustworthy SC-enabled IoUT systems can be developed for underwater communication.

</details>


### [41] [Autonomous Self-Healing UAV Swarms for Robust 6G Non-Terrestrial Networks](https://arxiv.org/abs/2601.13418)
*Sambrama Hegde,Venkata Srirama Rohit Kantheti,Liang C Chu,Erik Blasch,Shih-Chun Lin*

Main category: eess.SP

TL;DR: 论文提出了一种用于无人机网络的弹性自适应自愈网络设计(RASHND)，通过智能算法选择和分布式信号处理技术来优化动态干扰环境下的信号质量。


<details>
  <summary>Details</summary>
Motivation: 随着非地面网络(NTN)特别是无人机(UAV)在下一代无线网络中的应用日益增多，需要解决动态干扰和对抗条件下的网络可靠性问题，以支持关键通信应用。

Method: 提出RASHND框架，利用节点间通信和智能算法选择过程，结合分布式最大比合并(d-MRC)、分布式线性最小均方误差估计(d-LMMSE)和选择合并(SC)等技术，根据网络条件自适应选择最佳算法。

Result: 通过软件定义无线电(SDR)硬件测试平台和AERPAW测试床的无人机实际测试验证，RASHND显著提高了无人机网络的可靠性和干扰恢复能力。

Conclusion: RASHND能够有效增强无人机网络在动态干扰环境下的性能，使其更适合关键通信应用，为非地面网络的可靠连接提供了有前景的解决方案。

Abstract: Recent years have seen an increased interest in the use of Non-terrestrial networks (NTNs), especially the unmanned aerial vehicles (UAVs) to provide cost-effective global connectivity in next-generation wireless networks. We introduce a resilient, adaptive, self-healing network design (RASHND) to optimize signal quality under dynamic interference and adversarial conditions. RASHND leverages inter-node communication and an intelligent algorithm selection process, incorporating combining techniques like distributed-Maximal Ratio Combining (d-MRC), distributed-Linear Minimum Mean Squared Error Estimation(d-LMMSE), and Selection Combining (SC). These algorithms are selected to improve performance by adapting to changing network conditions. To evaluate the effectiveness of the proposed RASHND solutions, a software-defined radio (SDR)-based hardware testbed afforded initial testing and evaluations. Additionally, we present results from UAV tests conducted on the AERPAW testbed to validate our solutions in real-world scenarios. The results demonstrate that RASHND significantly enhances the reliability and interference resilience of UAV networks, making them well-suited for critical communications.

</details>


### [42] [Joint Subarray Selection, User Scheduling, and Pilot Assignment for XL-MIMO](https://arxiv.org/abs/2601.13470)
*Gabriel Avanzi Ubiali,José Carlos Marinello Filho,Taufik Abrão*

Main category: eess.SP

TL;DR: 该论文针对超大规模MIMO系统资源管理复杂度高的问题，提出了基于统计CSI的确定性SINR表达式和联合资源分配算法，显著提高了系统公平性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 超大规模MIMO是6G关键技术，但基于瞬时CSI的资源管理复杂度极高，难以实际部署。需要开发基于长期信道统计信息的低复杂度解决方案。

Method: 推导了集中式和分布式上行操作的确定性SINR闭式表达式，适用于Rician衰落信道和MMSE接收合并。基于这些表达式，开发了联合子阵列选择、用户调度和导频分配的统计CSI算法，利用用户可见区域的空间稀疏性实现更激进的导频复用。

Result: 数值结果表明，推导的SINR近似具有高精度，所提算法在拥挤场景下显著提高了公平性和吞吐量，相比传统大规模MIMO实现了更高效的导频复用。

Conclusion: 该工作为超大规模MIMO系统提供了基于统计CSI的实用资源管理框架，解决了瞬时CSI优化的计算复杂度问题，为6G超大规模MIMO部署提供了可行的解决方案。

Abstract: Extra-large scale MIMO (XL-MIMO) is a key technology for meeting sixth-generation (6G) requirements for high-rate connectivity and uniform quality of service (QoS); however, its deployment is challenged by the prohibitive complexity of resource management based on instantaneous channel state information (CSI). To address this intractability, this work derives novel closed-form deterministic signal-to-interference-plus-noise ratio (SINR) expressions for both centralized and distributed uplink operations. Valid for Rician fading channels with minimum mean square error (MMSE) receive combining and MMSE channel estimation, these expressions depend exclusively on long-term channel statistics, providing a tractable alternative to computationally expensive instantaneous CSI-driven optimization. Building on these results, we develop statistical-CSI-based algorithms for joint subarray selection, users scheduling, and pilot assignment, leveraging the derived SINR approximations to maximize the minimum spectral efficiency (SE) among scheduled users while preserving computational tractability. The proposed framework exploits the spatial sparsity of user equipment (UE) visibility regions (VRs) to enable more aggressive pilot reuse than is possible in conventional massive MIMO. Numerical results validate the high accuracy of the derived SINR approximations and demonstrate that the proposed algorithms significantly enhance fairness and throughput in crowded scenarios.

</details>


### [43] [Near-field Physical Layer Security: Robust Beamforming under Location Uncertainty](https://arxiv.org/abs/2601.13549)
*Chao Zhou,Changsheng You,Cong Zhou,Chengwen Xing,Jianhua Zhang*

Main category: eess.SP

TL;DR: 提出了一种针对近场物理层安全系统的鲁棒波束成形设计方法，特别处理窃听者位置不确定性问题，通过两阶段方法显著提升安全性能。


<details>
  <summary>Details</summary>
Motivation: 现有近场物理层安全研究大多假设窃听者信道状态信息或位置信息完美已知，但在实际场景中窃听者位置信息通常不完美。本文针对这一更实际且具有挑战性的场景，研究在窃听者位置不确定情况下的鲁棒波束成形设计。

Method: 首先将笛卡尔坐标位置误差转换到极坐标域，揭示近场角度误差放大效应。然后提出两阶段鲁棒波束成形方法：第一阶段将不确定区域划分为多个扇形子区域；第二阶段制定并求解基于线性矩阵不等式的鲁棒波束成形优化问题。该方法还扩展到多合法用户和多窃听者场景。

Result: 数值结果表明，所提方法在速率性能和保密鲁棒性之间实现了优越的权衡，在窃听者位置不确定情况下显著优于现有基准方法。

Conclusion: 本文提出的两阶段鲁棒波束成形方法有效解决了近场物理层安全系统中窃听者位置不确定性问题，通过处理近场角度误差放大效应，实现了更好的安全性能。

Abstract: In this paper, we study robust beamforming design for near-field physical-layer-security (PLS) systems, where a base station (BS) equipped with an extremely large-scale array (XL-array) serves multiple near-field legitimate users (Bobs) in the presence of multiple near-field eavesdroppers (Eves). Unlike existing works that mostly assume perfect channel state information (CSI) or location information of Eves, we consider a more practical and challenging scenario, where the locations of Bobs are perfectly known, while only imperfect location information of Eves is available at the BS. We first formulate a robust optimization problem to maximize the sum-rate of Bobs while guaranteeing a worst-case limit on the eavesdropping rate under location uncertainty. By transforming Cartesian position errors into the polar domain, we reveal an important near-field angular-error amplification effect: for the same location error, the closer the Eve, the larger the angle error, severely degrading the performance of conventional robust beamforming methods based on imperfect channel state information. To address this issue, we first establish the conditions for which the first-order Taylor approximation of the near-field channel steering vector under location uncertainty is largely accurate. Then, we propose a two-stage robust beamforming method, which first partitions the uncertainty region into multiple fan-shaped sub-regions, followed by the second stage to formulate and solve a refined linear-matrix-inequality (LMI)-based robust beamforming optimization problem. In addition, the proposed method is further extended to scenarios with multiple Bobs and multiple Eves. Finally, numerical results validate that the proposed method achieves a superior trade-off between rate performance and secrecy robustness, hence significantly outperforming existing benchmarks under Eve location uncertainty.

</details>


### [44] [Instant Preliminary Cardiac Analysis from Smartphone Auscultation: A Real-World Canine Heart Sound Dataset and Evaluation](https://arxiv.org/abs/2601.13593)
*Aswin Jose,Roeland P. J. E. Decorte,Laurent Locquet*

Main category: eess.SP

TL;DR: 该研究提出了一个真实世界犬类心音数据集，并评估了SoNUS 3.2.x算法在智能手机录音上进行初步心脏分析的性能，展示了在兽医心脏病学中可扩展的初步评估和远程医疗应用的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种基于智能手机录音的机器学习算法，用于兽医心脏病学的初步心脏评估，支持可扩展的远程医疗应用，解决传统兽医心脏检查的便利性和可及性问题。

Method: 方法包括：1）收集来自四大洲100多个犬类心音录音；2）由兽医心脏病专家对38个录音进行标注用于定量评估；3）使用SoNUS 3.2.x算法，采用多级回退架构和质量感知过滤，确保在不同录音条件下的可靠输出；4）评估60秒主模型和30-40秒快速模型的性能。

Result: 主要结果：60秒主模型的心率准确率均值为91.63%，中位数为94.95%；30-40秒快速模型的准确率均值为88.86%，中位数为92.98%。这些结果表明从智能手机录音中提取临床相关心脏信息的可行性。

Conclusion: 结论：SoNUS 3.2.x算法能够从机会性智能手机录音中可靠地提取临床相关心脏信息，支持兽医心脏病学中可扩展的初步评估和远程医疗应用，为宠物心脏健康监测提供了便捷有效的工具。

Abstract: This study presents a real-world canine heart sound dataset and evaluates SoNUS version 3.2.x, a machine learning algorithm for preliminary cardiac analysis using smartphone microphone recordings. More than one hundred recordings were collected from dogs across four continents, with thirty eight recordings annotated by board certified veterinary cardiologists for quantitative evaluation. SoNUS version 3.2.x employs a multi-stage fallback architecture with quality-aware filtering to ensure reliable output under variable recording conditions. The primary sixty second model achieved mean and median heart rate accuracies of ninety one point six three percent and ninety four point nine five percent, while a fast model optimized for thirty to forty second recordings achieved mean and median accuracies of eighty eight point eight six percent and ninety two point nine eight percent. These results demonstrate the feasibility of extracting clinically relevant cardiac information from opportunistic smartphone recordings, supporting scalable preliminary assessment and telehealth applications in veterinary cardiology.

</details>


### [45] [Deep Learning-Enabled Signal Detection for MIMO-OTFS-Based 6G and Future Wireless Networks](https://arxiv.org/abs/2601.13635)
*Emin Akpinar,Emir Aslandogan,Burak Ahmet Ozden,Haci Ilhan,Erdogan Aydin*

Main category: eess.SP

TL;DR: 本文提出用于MIMO-OTFS系统的低复杂度深度学习信号检测方法，在Nakagami-m信道条件下，使用MLP、CNN和ResNet架构，相比传统方法显著降低计算复杂度，同时保持与高性能MLD相当的误码率性能。


<details>
  <summary>Details</summary>
Motivation: OTFS调制作为6G及未来无线通信系统的有前景波形，在高移动性和色散信道条件下表现优异。深度学习信号检测方法计算复杂度低，是传统技术的有力替代方案。本研究旨在为MIMO-OTFS系统设计低复杂度的深度学习检测方法。

Method: 针对MIMO-OTFS系统，在Nakagami-m信道条件下，采用最大比合并接收天线符号，然后使用基于多层感知器、卷积神经网络和残差网络的深度学习检测器进行信号检测。

Result: 复杂度分析显示MLP架构相比CNN、ResNet和传统最大似然检测方法显著降低计算复杂度。数值分析表明，尽管复杂度低，所提出的深度学习检测器在各种系统条件下都能达到与高性能MLD相当的误码率性能。

Conclusion: 深度学习信号检测方法为MIMO-OTFS系统提供了低复杂度且高性能的解决方案，MLP架构在复杂度和性能之间提供了最佳平衡，是实际部署的有力候选方案。

Abstract: Orthogonal time frequency space (OTFS) modulation stands out as a promising waveform for sixth generation (6G) and beyond wireless communication systems, offering superior performance over conventional methods, particularly in high-mobility scenarios and dispersive channel conditions. Recent research has demonstrated that the reduced computational complexity of deep learning (DL)-based signal detection (SD) methods constitutes a compelling alternative to conventional techniques. In this study, low-complexity DL-based SD methods are proposed for a multiple-input multiple-output (MIMO)-OTFS system and examined under Nakagami-$m$ channel conditions. The symbols obtained from the receiver antennas are combined using maximum ratio combining (MRC) and detected with the help of a DL-based detector implemented with multi-layer perceptron (MLP), convolutional neural network (CNN), and residual network (ResNet). Complexity analysis reveals that the MLP architecture offers significantly lower computational complexity compared to CNN, ResNet, and classical methods such as maximum likelihood detection (MLD). Furthermore, numerical analyses have shown that the proposed DL-based detectors, despite their low complexity, achieve comparable bit error rate (BER) performance to that of a high-performance MLD under various system conditions.

</details>


### [46] [TSN-IoT: A Two-Stage NOMA-Enabled Framework for Prioritized Traffic Handling in Dense IoT Networks](https://arxiv.org/abs/2601.13680)
*Shama Siddiqui,Anwar Ahmed Khan,Nicola Marchetti*

Main category: eess.SP

TL;DR: 提出TSN-IoT框架，在密集IoT环境中通过NOMA实现分布式同步和优先级数据传输，相比OFDMA显著提升性能


<details>
  <summary>Details</summary>
Motivation: 密集IoT场景中，节点移动和GNSS信号不可靠会破坏同步，需要确保连续连接并提供优先级访问

Method: 提出TSN-IoT框架，整合传统PTP同步、分布式同步和数据传输，采用四层架构实现从传感器节点到基站的优先级数据传输

Result: 通过医疗用例仿真，相比基于优先级的OFDMA，TSN-IoT在同步速度和端到端延迟方面表现显著更好

Conclusion: TSN-IoT框架通过NOMA实现分布式同步和并行传输，有效解决了密集IoT环境中的连接和优先级管理挑战

Abstract: With the growing applications of the Internet of Things (IoT), a major challenge is to ensure continuous connectivity while providing prioritized access. In dense IoT scenarios, synchronization may be disrupted either by the movement of nodes away from base stations or by the unavailability of reliable Global Navigation Satellite System (GNSS) signals, which can be affected by physical obstructions, multipath fading, or environmental interference, such as such as walls, buildings, moving objects, or electromagnetic noise from surrounding devices. In such contexts, distributed synchronization through Non-Orthogonal Multiple Access (NOMA) offers a promising solution, as it enables simultaneous transmission to multiple users with different power levels, supporting efficient synchronization while minimizing the signaling overhead. Moreover, NOMA also plays a vital role for dynamic priority management in dense and heterogeneous IoT environments. In this article, we proposed a Two-Stage NOMA-Enabled Framework "TSN-IoT" that integrates the mechanisms of conventional Precision Time Protocol (PTP) based synchronization, distributed synchronization and data transmission. The framework is designed as a four-tier architecture that facilitates prioritized data delivery from sensor nodes to the central base station. We demonstrated the performance of "TSN-IoT" through a healthcare use case, where intermittent connectivity and varying data priority levels present key challenges for reliable communication. Synchronization speed and end-to-end delay were evaluated through a series of simulations implemented in Python. Results show that, compared to priority-based Orthogonal Frequency Division Multiple Access (OFDMA), TSN-IoT achieves significantly better performance by offering improved synchronization opportunities and enabling parallel transmissions over the same sub-carrier.

</details>


### [47] [Channel Estimation in MIMO Systems Using Flow Matching Models](https://arxiv.org/abs/2601.13827)
*Yongqiang Zhang,Qurrat-Ul-Ain Nadeem*

Main category: eess.SP

TL;DR: 提出基于流匹配的MIMO信道估计器，相比现有扩散模型方法，推理速度提升49倍，GPU内存使用减少20倍


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型和分数匹配的信道估计方法存在计算开销大、推理速度慢的问题，需要更高效的生成模型方法

Method: 基于流匹配训练深度神经网络学习无线信道的速度场，然后集成到即插即用近端梯度下降框架中

Result: 性能优于现有SOTA生成模型估计器，推理速度提升49倍，GPU峰值内存使用减少20倍

Conclusion: 流匹配方法为MIMO信道估计提供了高效准确的解决方案，显著降低了计算开销

Abstract: Multiple-input multiple-output (MIMO) systems require efficient and accurate channel estimation with low pilot overhead to unlock their full potential for high spectral and energy efficiency. While deep generative models have emerged as a powerful foundation for the channel estimation task, the existing approaches using diffusion-based and score-based models suffer from high computational runtime due to their stochastic and many-step iterative sampling. In this paper, we introduce a flow matching-based channel estimator to overcome this limitation. The proposed channel estimator is based on a deep neural network trained to learn the velocity field of wireless channels, which we then integrate into a plug-and-play proximal gradient descent (PnP-PGD) framework. Simulation results reveal that our formulated approach consistently outperforms existing state-of-the-art (SOTA) generative model-based estimators, achieves up to 49 times faster inference at test time, and reduces up to 20 times peak graphics processing unit (GPU) memory usage. Our code and models are publicly available to support reproducible research.

</details>


### [48] [Riemannian optimization on the manifold of unitary and symmetric matrices with application to BD-RIS-assisted systems](https://arxiv.org/abs/2601.13877)
*Ignacio Santamaria,Mohammad Soleymani,Eduard Jorswieck,Jesus Gutierrez,Carlos Beltran*

Main category: eess.SP

TL;DR: 本文首次严格刻画了酉矩阵和对称矩阵的流形，推导了其切空间和测地线，并基于此开发了一种无需调节参数的黎曼流形优化算法，应用于BD-RIS辅助的MIMO系统速率最大化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的黎曼流形优化算法通常需要设置调整参数，这在实际应用中可能带来不便。同时，对于酉矩阵和对称矩阵流形的严格数学刻画尚未完全建立，限制了相关优化算法的发展。

Method: 首先严格推导了酉矩阵和对称矩阵流形的切空间和测地线，通过实对称矩阵参数化测地线。基于此参数化，开发了一种无需调节参数的黎曼流形优化算法，并将其应用于BD-RIS辅助的MIMO系统速率最大化问题。

Result: 提出的MO算法相比基于Takagi分解的先前方法显著降低了计算成本，同时保持了向成本函数驻点的全局收敛性。在BD-RIS辅助的MIMO系统仿真中验证了方法的性能。

Conclusion: 本文首次建立了酉矩阵和对称矩阵流形的严格数学框架，并基于此开发了无需调节参数的高效黎曼流形优化算法，为相关工程应用提供了理论支持和实用工具。

Abstract: In this paper, we rigorously characterize for the first time the manifold of unitary and symmetric matrices, deriving its tangent space and its geodesics. The resulting parameterization of the geodesics (through a real and symmetric matrix) allows us to derive a new Riemannian manifold optimization (MO) algorithm whose most remarkable feature is that it does not need to set any adaptation parameter. We apply the proposed MO algorithm to maximize the achievable rate in a multiple-input multiple-output (MIMO) system assisted by a beyond-diagonal reconfigurable intelligent surface (BD-RIS), illustrating the method's performance through simulations. The MO algorithm achieves a significant reduction in computational cost compared to previous alternatives based on Takagi decomposition, while retaining global convergence to a stationary point of the cost function.

</details>


### [49] [Deep Reinforcement Learning-Based Dynamic Resource Allocation in Cell-Free Massive MIMO](https://arxiv.org/abs/2601.13934)
*Phuong Nam Tran,Nhan Thanh Nguyen,Hien Quoc Ngo,Markku Juntti*

Main category: eess.SP

TL;DR: 该论文提出了一种基于深度强化学习的框架，用于联合优化无蜂窝大规模MIMO系统的天线激活和功率分配，以提高能量效率。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO系统需要同时优化天线激活和功率分配以提高能量效率，但这是一个非凸、混合整数的高维优化问题，传统方法难以有效解决。

Method: 提出基于深度强化学习（DRL）的框架，智能体学习将大尺度衰落系数映射到AP激活比例、天线系数和功率系数，然后通过闭式表达式确定每个AP的激活天线数和用户功率分配，将高维优化转化为低维学习任务。

Result: 在40个AP和20个用户的CFmMIMO系统中，相比传统顺序凸逼近方法，实现了50%的能量效率提升和3350倍的运行时间减少，证明了方法的效率和可扩展性。

Conclusion: 提出的DRL框架成功解决了无蜂窝大规模MIMO系统中天线激活和功率分配的联合优化难题，显著提高了能量效率并大幅降低了计算复杂度。

Abstract: In this paper, we consider power allocation and antenna activation of cell-free massive multiple-input multiple-output (CFmMIMO) systems. We first derive closed-form expressions for the system spectral efficiency (SE) and energy efficiency (EE) as functions of the power allocation coefficients and the number of active antennas at the access points (APs). Then, we aim to enhance the EE through jointly optimizing antenna activation and power control. This task leads to a non-convex and mixed-integer design problem with high-dimensional design variables. To address this, we propose a novel DRL-based framework, in which the agent learns to map large-scale fading coefficients to AP activation ratio, antenna coefficient, and power coefficient. These coefficients are then employed to determine the number of active antennas per AP and the power factors assigned to users based on closed-form expressions. By optimizing these parameters instead of directly controlling antenna selection and power allocation, the proposed method transforms the intractable optimization into a low-dimensional learning task. Our extensive simulations demonstrate the efficiency and scalability of the proposed scheme. Specifically, in a CFmMIMO system with 40 APs and 20 users, it achieves a 50% EE improvement and 3350 times run time reduction compared to the conventional sequential convex approximation method.

</details>


### [50] [Achieving Full Multipath Diversity by Random Constellation Rotation: a Theoretical Perspective](https://arxiv.org/abs/2601.13997)
*Xuehan Wang,Jinhong Yuan,Jintao Wang,Kehan Huang*

Main category: eess.SP

TL;DR: 提出利用随机星座旋转简化全分集调制设计条件，证明线性预编码CP-OFDM系统只要扩展矩阵无零元素即可获得最大多径分集，并推导出一般调制方案的充要条件。


<details>
  <summary>Details</summary>
Motivation: 现有分析框架大多针对特定调制方案，而验证一般调制方案的全多径分集效率仍是一个开放问题。需要填补这一研究空白，简化全分集调制设计条件。

Method: 采用随机星座旋转技术，推导线性预编码CP-OFDM系统的充分条件（扩展矩阵无零元素），并建立一般调制方案的充要条件验证框架，将验证任务分解为调制矩阵各列的独立验证。

Result: 理论证明通过启用随机生成的旋转模式，在时间和双弥散信道中以概率1获得最大分集阶数。数值结果验证了理论分析的有效性，随机星座旋转技术能持续提升编码和非编码系统的传输可靠性。

Conclusion: 随机星座旋转简化了全分集调制设计条件，将分集评估集中于单符号错误概率，显著降低了新调制方案的分集驱动设计和性能分析复杂度，为一般调制方案提供了有效的分集验证框架。

Abstract: Diversity is an essential concept associated with communication reliability in multipath channels since it determines the slope of bit error rate performance in the medium to high signal-to-noise ratio regions. However, most of the existing analytical frameworks were developed for specific modulation schemes while the efficient validation of full multipath diversity for general modulation schemes remains an open problem. To fill this research gap, we propose to utilize random constellation rotation to ease the conditions for full-diversity modulation designs. For linearly precoded cyclic-prefix orthogonal frequency division multiplexing (OFDM) systems, we prove that maximum multipath diversity can be attained as long as the spread matrix does not have zero entries, which is a sufficient but easily satisfied condition. Furthermore, we derive the sufficient and necessary condition for general modulation schemes, whose verification can be divided into validation tasks for each column of the modulation matrix. Based on the proposed conditions, maximum diversity order can be attained with the probability of 1 by enabling a randomly generated rotation pattern for both time and doubly dispersive channels. The theoretical analysis in this paper also demonstrates that the diversity evaluation can be concentrated on the pairwise error probability when the number of error symbols is one, which reduces the complexity of diversity-driven design and performance analysis for novel modulation schemes significantly in both time and doubly dispersive channels. Finally, numerical results for various modulation schemes confirm that the theoretical analysis holds in both time and doubly dispersive channels. Furthermore, when employing practical detectors, the random constellation rotation technique consistently enhance the transmission reliability for both coded and uncoded systems.

</details>


### [51] [Background Subtraction with Drift Correction for Bistatic Radar Reflectivity Measurements](https://arxiv.org/abs/2601.14080)
*Alexander Ihlow,Marius Schmidt,Carsten Andrich,Reiner S. Thomä*

Main category: eess.SP

TL;DR: 提出一种用于校正无回波室双基地雷达测量中仪器漂移的模型，可显著改善天线串扰消除效果


<details>
  <summary>Details</summary>
Motivation: 双基地雷达反射率研究对6G移动通信标准中的集成感知与通信(ISAC)至关重要。在无回波室测量中，背景减法的有效性受到前景和背景测量过程之间轻微非相干性的影响，导致结果恶化。

Method: 引入仪器漂移校正模型，分析2-18GHz频段的实际测量数据，使用TU Ilmenau的双基地雷达(BIRA)测量设施进行验证。

Result: 应用提出的漂移校正模型后，在消除视线天线串扰方面相比现有技术提升了40dB的改进。

Conclusion: 该漂移校正模型能有效解决双基地雷达测量中的仪器漂移问题，显著提高背景减法的准确性，对ISAC应用具有重要意义。

Abstract: Fundamental research on bistatic radar reflectivity is highly relevant, e.g., to the upcoming mobile communication standard 6G, which includes integrated sensing and communication (ISAC). We introduce a model for correcting instrumentation drift during bistatic radar measurements in anechoic chambers. Usually, background subtraction is applied with the goal to yield the target reflection signal as best as possible while coherently subtracting all signals which were present in both the foreground and background measurement. However, even slight incoherences between the foreground and background measurement process deteriorate the result. We analyze these effects in real measurements in the frequency range 2-18 GHz, taken with the Bistatic Radar (BIRA) measurement facility at TU Ilmenau. Applying our proposed drift correction model, we demonstrate up to 40 dB improvement for the removal of direct line-of-sight antenna crosstalk over the state of the art.

</details>


### [52] [Bit-Efficient Quantisation for Two-Channel Modulo-Sampling Systems](https://arxiv.org/abs/2601.14220)
*Wenyi Yan,Zeyuan Li,Lu Gan,Honqing Liu,Guoquan Li*

Main category: eess.SP

TL;DR: 提出一种比特高效的两通道模数ADC量化方案，通过利用通道间差值的整数结构，传输一个量化通道输出和紧凑的差值索引，相比传统ADC仅需1-2比特/样本的开销。


<details>
  <summary>Details</summary>
Motivation: 现有两通道模数ADC设计独立量化两个通道输出，导致冗余比特率成本。需要在保持高动态范围信号感知的同时，减少比特率开销。

Method: 提出比特高效量化方案，利用通道间差值为整数的结构特性，仅传输一个量化通道输出和紧凑的差值索引，而非独立量化两个通道。

Result: 理论证明该方法相比传统ADC仅需1-2比特/样本的开销，仿真验证了理论误差界限和比特率分析，硬件实验显示相比现有模数采样方案显著节省比特率，同时保持可比较的重建精度。

Conclusion: 该方案为比特率受限系统提供了一条实现高分辨率、带宽高效模数ADC的实用路径。

Abstract: Two-channel modulo analog-to-digital converters (ADCs) enable high-dynamic-range signal sensing at the Nyquist rate per channel, but existing designs quantise both channel outputs independently, incurring redundant bitrate costs. This paper proposes a bit-efficient quantisation scheme that exploits the integer-valued structure of inter-channel differences, transmitting one quantised channel output together with a compact difference index. We prove that this approach requires only 1-2 bits per signal sample overhead relative to conventional ADCs, despite operating with a much smaller per-channel dynamic range. Simulations confirm the theoretical error bounds and bitrate analysis, while hardware experiments demonstrate substantial bitrate savings compared with existing modulo sampling schemes, while maintaining comparable reconstruction accuracy. These results highlight a practical path towards high-resolution, bandwidth-efficient modulo ADCs for bitrate-constrained systems.

</details>


### [53] [Burst Aware Forecasting of User Traffic Demand in LEO Satellite Networks](https://arxiv.org/abs/2601.14233)
*Yekta Demirci,Guillaume Mantelet,Stephane Martel,Jean-Francois Frigon,Gunes Karabulut Kurt*

Main category: eess.SP

TL;DR: 提出一种面向低轨卫星网络波束跳变调度的突发流量感知预测方法，通过改进Transformer架构来准确预测流量突发，减少预测误差达94%。


<details>
  <summary>Details</summary>
Motivation: 在低轨卫星网络中，波束跳变技术需要提前知道未来流量来进行调度规划。在重负载条件下，意外的流量突发结合链路质量下降可能导致缓冲区溢出和数据包丢失，因此准确的流量预测特别是突发流量预测变得至关重要。

Method: 提出一种突发感知预测解决方案，对Transformer架构进行三项关键改进：(1) 引入"距离上次突发"的嵌入表示来捕捉突发临近性；(2) 在解码器中增加两个线性层，分别预测即将到来的突发及其相对影响；(3) 在模型训练中使用非对称损失函数来更好地捕捉突发动态。

Result: 在高流量需求场景下的地球固定小区中进行实证评估，结果显示所提模型在一步预测范围内将预测误差降低了94%，并且在较长预测范围末端仍能准确捕捉突发流量（基于MSE指标）。

Conclusion: 该突发感知预测方法能有效解决低轨卫星网络中波束跳变调度面临的流量突发预测挑战，不仅适用于卫星网络，也适用于具有突发流量模式且需要准确需求预测的各种无线网络场景。

Abstract: In Low Earth Orbit (LEO) satellite networks, Beam Hopping (BH) technology enables the efficient utilization of limited radio resources by adapting to varying user demands and link conditions. Effective BH planning requires prior knowledge of upcoming traffic at the time of scheduling, making forecasting an important sub-task. Forecasting becomes particularly critical under heavy load conditions where an unexpected demand burst combined with link degradation may cause buffer overflows and packet loss. To address this challenge, we propose a burst aware forecasting solution. This challenge may arise in a wide range of wireless networks; therefore, the proposed solution is broadly applicable to settings characterized by bursty traffic patterns where accurate demand forecasting is essential. Our approach introduces three key enhancements to a transformer architecture: (i) a distance from the last burst embedding to capture burst proximity, (ii) two additional linear layers in the decoder to forecast both upcoming bursts and their relative impact, and (iii) use of an asymmetric cost function during model training to better capture burst dynamics. Empirical evaluations in an Earth-fixed cell under high-traffic demand scenario demonstrate that the proposed model reduces prediction error by up to 94% at a one-step horizon and maintains the ability to accurately capture bursts even near the end of longer prediction horizons following Mean Square Error (MSE) metric.

</details>


### [54] [Robust Localization in OFDM-Based Massive MIMO through Phase Offset Calibration](https://arxiv.org/abs/2601.14244)
*Qing Zhang,Adham Sakhnini,Robbert Beerten,Haoqiu Xiong,Zhuangzhuang Cui,Yang Miao,Sofie Pollin*

Main category: eess.SP

TL;DR: 论文研究了OFDM大规模MIMO系统中相位非相干性对定位性能的影响，提出了CSI校准框架，将定位误差从5米降低到1.2厘米


<details>
  <summary>Details</summary>
Motivation: OFDM大规模MIMO系统的精确定位依赖于子载波和天线间的相位相干性，但实际系统存在频率相关和天线相关的相位偏移，这会降低定位精度。需要分析这些相位非相干性的影响并找到解决方案。

Method: 使用两种互补工具：1) 推导Cramér-Rao下界(CRLB)量化相位偏移下的理论极限；2) 开发空间模糊函数(SAF)模型表征模糊模式。提出鲁棒的CSI校准框架，并在实际大规模MIMO测试平台上用真实测量数据进行验证。

Result: 仿真结果表明：空间相位偏移严重降低定位性能，而频率相位偏移在考虑的系统配置中影响较小。提出的校准框架将定位均方根误差(RMSE)从5米显著改善到1.2厘米，与理论预测一致。

Conclusion: 相位非相干性特别是空间相位偏移对大规模MIMO定位精度有严重影响，提出的CSI校准框架能有效解决这一问题，显著提升定位性能，理论分析与实验结果吻合。

Abstract: Accurate localization in Orthogonal Frequency Division Multiplexing (OFDM)-based massive Multiple-Input Multiple-Output (MIMO) systems depends critically on phase coherence across subcarriers and antennas. However, practical systems suffer from frequency-dependent and (spatial) antenna-dependent phase offsets, degrading localization accuracy. This paper analytically studies the impact of phase incoherence on localization performance under a static User Equipment (UE) and Line-of-Sight (LoS) scenario. We use two complementary tools. First, we derive the Cramér-Rao Lower Bound (CRLB) to quantify the theoretical limits under phase offsets. Then, we develop a Spatial Ambiguity Function (SAF)-based model to characterize ambiguity patterns. Simulation results reveal that spatial phase offsets severely degrade localization performance, while frequency phase offsets have a minor effect in the considered system configuration. To address this, we propose a robust Channel State Information (CSI) calibration framework and validate it using real-world measurements from a practical massive MIMO testbed. The experimental results confirm that the proposed calibration framework significantly improves the localization Root Mean Squared Error (RMSE) from 5 m to 1.2 cm, aligning well with the theoretical predictions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration](https://arxiv.org/abs/2601.11556)
*Boyang Wang,Yash Vishe,Xin Xu,Zachary Novack,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: CSyMR-Bench是一个新的符号音乐推理基准测试，包含126个需要组合分析的多选题，并提出了基于music21工具增强的代理框架来应对这一挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在符号音乐推理方面的基准测试主要关注孤立的知识或原子分析，缺乏对连接音乐结构的组合性推理能力的评估，而这是音乐创作和理解的关键。

Method: 1) 创建CSyMR-Bench数据集：从专家论坛和专业考试中收集126个需要组合多个原子分析的多选题；2) 提出工具增强的代理框架：利用music21库中的符号音乐分析工具来辅助推理。

Result: CSyMR-Bench对现有模型构成非平凡挑战，而提出的工具增强代理框架在所有基线方法中表现最佳，实现了5-7%的绝对准确率提升。

Conclusion: 该研究填补了符号音乐组合推理评估的空白，证明了工具增强方法在复杂音乐分析任务中的有效性，为音乐AI的发展提供了新的基准和解决方案。

Abstract: Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item involves combining several atomic analyses to arrive at the final answer. Furthermore, we introduce a tool-augmented agent framework that leverages symbolic music analysis tools from the music21 library to address the challenges posed by CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial challenge across both community-sourced and exam-style questions, while our tool-augmented agent consistently outperforms all baselines, achieving 5-7% absolute accuracy gains.

</details>


### [56] [Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLMs](https://arxiv.org/abs/2601.12341)
*Rezky Kam,Coddy N. Siswanto*

Main category: cs.LG

TL;DR: 提出结合物理信息神经网络与上下文学习的数据集和框架，让LLM模拟真实世界情感动态，实现可解释的对话建模


<details>
  <summary>Details</summary>
Motivation: 当前LLM在情感理解和对话建模中缺乏真实世界情感动态的模拟能力，且模型可解释性不足，需要新的方法来捕捉情感随时间演化的复杂模式

Method: 构建专门数据集，结合物理信息神经网络（PINN）与上下文学习，通过物理原理约束情感动态建模，实现时间序列情感演化模拟

Result: 开发出能够模拟真实世界情感动态的框架，为LLM提供可解释的情感演化建模能力，开启了可解释对话建模的新可能性

Conclusion: 物理信息神经网络与上下文学习的结合为LLM情感动态建模提供了新途径，增强了对话系统的可解释性和真实感

Abstract: This paper introduces a dataset and conceptual framework for LLMs to mimic real world emotional dynamics through time and in-context learning leveraging physics-informed neural network, opening a possibility for interpretable dialogue modeling.

</details>


### [57] [AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control](https://arxiv.org/abs/2601.11568)
*Quang-Hung Bui,Anh Son Ta*

Main category: cs.LG

TL;DR: AdaFRUGAL通过动态调整梯度分割的超参数（子空间比例ρ和更新频率T），自动优化内存和计算开销，在保持性能的同时显著减少LLM训练的资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有FRUGAL框架虽然通过梯度分割减少LLM训练的内存占用，但其静态超参数需要手动调优，成本高昂且缺乏适应性，限制了实际应用。

Method: 提出AdaFRUGAL框架，引入两种动态控制机制：1) 线性衰减子空间比例ρ以逐步减少内存占用；2) 基于损失感知的更新频率T调度以降低计算开销。

Result: 在英文C4、越南语VietVault的大规模预训练和GLUE微调实验中，AdaFRUGAL在保持与AdamW和静态FRUGAL相当性能的同时，显著减少了GPU内存和训练时间。

Conclusion: AdaFRUGAL提供了更实用、自主的资源受限LLM训练解决方案，实现了性能与资源消耗的良好平衡。

Abstract: Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($ρ$) and update frequency ($T$) -- require costly manual tuning, limiting adaptability. We present AdaFRUGAL, which automates this process by introducing two dynamic controls: (i) a linear decay for $ρ$ to progressively reduce memory, and (ii) a loss-aware schedule for $T$ to lower computational overhead. Experiments across large-scale pre-training (English C4, Vietnamese VietVault) and fine-tuning (GLUE) demonstrate that AdaFRUGAL achieves a compelling trade-off. It maintains competitive performance against AdamW and static FRUGAL while significantly reducing both GPU memory and training time, offering a more practical, autonomous solution for resource-constrained LLM training.

</details>


### [58] [Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces](https://arxiv.org/abs/2601.11572)
*Timo Aukusti Laine*

Main category: cs.LG

TL;DR: 使用线性代数和哈密顿形式分析LLM嵌入空间结构，发现L2归一化约束使嵌入空间适合哈密顿分析，推导余弦相似度与向量扰动关系，探索量子力学类比


<details>
  <summary>Details</summary>
Motivation: 观察到LLM嵌入呈现离散语义状态，需要数学工具分析语义关系，特别是量子力学系统类比可能提供新视角

Method: 应用线性代数和哈密顿形式分析LLM嵌入空间，推导L2归一化约束下的数学关系，探索余弦相似度与向量扰动的关系，研究直接和间接语义转换

Result: 发现L2归一化约束使嵌入空间适合哈密顿形式分析，推导出余弦相似度与向量扰动的数学关系，建立了量子力学类比（如零点能量），并探索了与Koopman-von Neumann力学的潜在联系

Conclusion: 该方法为深入理解LLM提供了有前景的途径，可能有助于开发减轻幻觉的新方法，但解释需要谨慎考虑

Abstract: We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the observation that LLM embeddings exhibit distinct states, suggesting discrete semantic representations, we explore the application of these mathematical tools to analyze semantic relationships. We demonstrate that the L2 normalization constraint, a characteristic of many LLM architectures, results in a structured embedding space suitable for analysis using a Hamiltonian formalism. We derive relationships between cosine similarity and perturbations of embedding vectors, and explore direct and indirect semantic transitions. Furthermore, we explore a quantum-inspired perspective, deriving an analogue of zero-point energy and discussing potential connections to Koopman-von Neumann mechanics. While the interpretation warrants careful consideration, our results suggest that this approach offers a promising avenue for gaining deeper insights into LLMs and potentially informing new methods for mitigating hallucinations.

</details>


### [59] [Resource-Conscious RL Algorithms for Deep Brain Stimulation](https://arxiv.org/abs/2601.12699)
*Arkaprava Gupta,Nicholas Carter,William Zellers,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Parasara Sridhar Duggirala,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: 提出T3P MAB强化学习方法用于帕金森病深部脑刺激，能同时调节频率和振幅，计算轻量适合植入设备，无需离线训练。


<details>
  <summary>Details</summary>
Motivation: 传统DBS使用固定频率和振幅存在副作用和电池寿命短的问题，现有RL方法计算复杂难以在体内训练，且大多只调节单一参数。

Method: 提出时间与阈值触发的多臂老虎机（T3P MAB）强化学习方法，能同时调节DBS的频率和振幅，计算轻量适合微控制器部署。

Result: T3P MAB算法在多种MCU平台上实现，能耗测量显示适合资源受限平台，比现有RL方法样本效率更高、收敛时间更短。

Conclusion: T3P MAB为DBS提供了一种轻量、自适应的强化学习解决方案，能同时优化频率和振幅，适合植入式设备部署，解决了现有方法的计算复杂性和训练困难问题。

Abstract: Deep Brain Stimulation (DBS) has proven to be a promising treatment of Parkinson's Disease (PD). DBS involves stimulating specific regions of the brain's Basal Ganglia (BG) using electric impulses to alleviate symptoms of PD such as tremors, rigidity, and bradykinesia. Although most clinical DBS approaches today use a fixed frequency and amplitude, they suffer from side effects (such as slurring of speech) and shortened battery life of the implant. Reinforcement learning (RL) approaches have been used in recent research to perform DBS in a more adaptive manner to improve overall patient outcome. These RL algorithms are, however, too complex to be trained in vivo due to their long convergence time and requirement of high computational resources.
  We propose a new Time & Threshold-Triggered Multi-Armed Bandit (T3P MAB) RL approach for DBS that is more effective than existing algorithms. Further, our T3P agent is lightweight enough to be deployed in the implant, unlike current deep-RL strategies, and even forgoes the need for an offline training phase. Additionally, most existing RL approaches have focused on modulating only frequency or amplitude, and the possibility of tuning them together remains greatly unexplored in the literature. Our RL agent can tune both frequency and amplitude of DBS signals to the brain with better sample efficiency and requires minimal time to converge. We implement an MAB agent for DBS for the first time on hardware to report energy measurements and prove its suitability for resource-constrained platforms. Our T3P MAB algorithm is deployed on a variety of microcontroller unit (MCU) setups to show its efficiency in terms of power consumption as opposed to other existing RL approaches used in recent work.

</details>


### [60] [GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment](https://arxiv.org/abs/2601.11574)
*Lukas Abrie Nel*

Main category: cs.LG

TL;DR: GRADE提出了一种基于Gumbel-Softmax重参数化的可微对齐方法，替代高方差的策略梯度方法，在文本生成任务上实现了更好的奖励性能和更稳定的训练。


<details>
  <summary>Details</summary>
Motivation: 传统的RLHF方法如PPO存在梯度估计方差高、需要精细超参数调优和大量计算资源的问题，需要更稳定高效的对齐方法。

Method: GRADE使用Gumbel-Softmax重参数化配合直通估计（GRADE-STE），通过离散token采样过程的可微松弛实现端到端梯度传播，替代高方差的策略梯度估计。

Result: 在IMDB数据集的情感控制文本生成任务中，GRADE-STE获得0.763±0.344的测试奖励，优于PPO的0.510±0.313和REINFORCE的0.617±0.378，梯度方差比REINFORCE低14倍以上，训练动态更稳定。

Conclusion: GRADE为LLM对齐提供了一种更简单、更稳定、更有效的强化学习替代方案，在保持测试集泛化能力的同时显著降低了训练复杂性。

Abstract: Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.

</details>


### [61] [On the Relation of State Space Models and Hidden Markov Models](https://arxiv.org/abs/2601.13357)
*Aydin Ghojogh,M. Hadi Sepanj,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: 该论文系统比较了HMM、线性高斯状态空间模型、卡尔曼滤波与当代NLP状态空间模型，分析它们在概率图模型、推理算法和学习方法上的异同，并探讨现代NLP SSM与经典概率模型的关系。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）和隐马尔可夫模型（HMMs）是序列数据建模的基础框架，广泛应用于信号处理、控制理论和机器学习。尽管它们具有相似的时间结构，但在潜在状态性质、概率假设、推理过程和学习范式上存在根本差异。近年来，确定性状态空间模型通过S4和Mamba等架构在自然语言处理中重新出现，引发了关于经典概率SSMs、HMMs与现代神经序列模型关系的新问题。

Method: 通过概率图模型的视角分析这些模型的数学表述，比较它们的推理算法（包括前向-后向推理和卡尔曼滤波），并对比它们的学习方法（期望最大化与基于梯度的优化）。系统性地比较HMMs、线性高斯状态空间模型、卡尔曼滤波和当代NLP状态空间模型。

Result: 阐明了这些模型何时等价、何时存在根本差异，以及现代NLP SSMs如何与经典概率模型相关联。通过突出结构相似性和语义差异，为理解这些模型之间的关系提供了清晰的框架。

Conclusion: 该分析连接了控制理论、概率建模和现代深度学习的视角，为理解经典概率模型与现代神经序列模型之间的关系提供了统一的框架，有助于澄清这些模型在理论和实践应用中的异同。

Abstract: State Space Models (SSMs) and Hidden Markov Models (HMMs) are foundational frameworks for modeling sequential data with latent variables and are widely used in signal processing, control theory, and machine learning. Despite their shared temporal structure, they differ fundamentally in the nature of their latent states, probabilistic assumptions, inference procedures, and training paradigms. Recently, deterministic state space models have re-emerged in natural language processing through architectures such as S4 and Mamba, raising new questions about the relationship between classical probabilistic SSMs, HMMs, and modern neural sequence models.
  In this paper, we present a unified and systematic comparison of HMMs, linear Gaussian state space models, Kalman filtering, and contemporary NLP state space models. We analyze their formulations through the lens of probabilistic graphical models, examine their inference algorithms -- including forward-backward inference and Kalman filtering -- and contrast their learning procedures via Expectation-Maximization and gradient-based optimization. By highlighting both structural similarities and semantic differences, we clarify when these models are equivalent, when they fundamentally diverge, and how modern NLP SSMs relate to classical probabilistic models. Our analysis bridges perspectives from control theory, probabilistic modeling, and modern deep learning.

</details>


### [62] [Hindsight Preference Replay Improves Preference-Conditioned Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2601.11604)
*Jonaid Shianifar,Michael Schukat,Karl Mason*

Main category: cs.LG

TL;DR: Hindsight Preference Replay (HPR) 是一种简单的回放增强策略，通过重新标记存储的转移数据来利用其他偏好的离线数据，从而在多目标强化学习中提高性能。


<details>
  <summary>Details</summary>
Motivation: CAPQL方法在多目标强化学习中只使用特定偏好下收集的数据，忽略了其他偏好的离线数据，导致数据利用效率低下。

Method: 提出Hindsight Preference Replay (HPR)策略，对存储的转移数据进行重新标记，使用替代偏好来增强监督信号，无需改变CAPQL架构或损失函数。

Result: 在6个MO-Gymnasium运动任务中，HPR-CAPQL在5个环境中提高了超体积(HV)，在4个环境中提高了期望效用(EUM)。特别是在mo-humanoid-v5中，EUM从323±125提升到1613±464，HV从0.52M提升到9.63M。

Conclusion: HPR是一种简单有效的回放增强策略，能够显著提高多目标强化学习的性能，通过更好地利用离线数据来增强偏好空间中的监督信号。

Abstract: Multi-objective reinforcement learning (MORL) enables agents to optimize vector-valued rewards while respecting user preferences. CAPQL, a preference-conditioned actor-critic method, achieves this by conditioning on weight vectors w and restricts data usage to the specific preferences under which it was collected, leaving off-policy data from other preferences unused. We introduce Hindsight Preference Replay (HPR), a simple and general replay augmentation strategy that retroactively relabels stored transitions with alternative preferences. This densifies supervision across the preference simplex without altering the CAPQL architecture or loss functions. Evaluated on six MO-Gymnasium locomotion tasks at a fixed 300000-step budget using expected utility (EUM), hypervolume (HV), and sparsity, HPR-CAPQL improves HV in five of six environments and EUM in four of six. On mo-humanoid-v5, for instance, EUM rises from $323\!\pm\!125$ to $1613\!\pm\!464$ and HV from 0.52M to 9.63M, with strong statistical support. mo-halfcheetah-v5 remains a challenging exception where CAPQL attains higher HV at comparable EUM. We report final summaries and Pareto-front visualizations across all tasks.

</details>


### [63] [A Multimodal Data Processing Pipeline for MIMIC-IV Dataset](https://arxiv.org/abs/2601.11606)
*Farzana Islam Adiba,Varsha Danduri,Fahmida Liza Piya,Ali Abbasi,Mehak Gupta,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: 开发了一个全面的多模态MIMIC-IV数据处理管道，可自动化整合结构化数据、临床笔记、波形和影像数据，显著减少处理时间并提高研究可重复性。


<details>
  <summary>Details</summary>
Motivation: MIMIC-IV数据集包含多种模态数据，但现有处理工具要么只支持部分模态，要么无法灵活适应不同下游应用，需要大量手动预处理工作。

Method: 扩展了之前流行的单模态管道，开发了可定制的多模态管道，支持自动化队列选择、跨模态时间对齐，并提供标准化的多模态输出格式。

Result: 发布了包含代码、简单UI和Python包的完整解决方案，支持选择性集成和嵌入，显著减少了多模态数据处理时间。

Conclusion: 该多模态管道能够显著提高MIMIC-IV数据处理的效率和可重复性，支持任意的静态和时间序列下游应用。

Abstract: The MIMIC-IV dataset is a large, publicly available electronic health record (EHR) resource widely used for clinical machine learning research. It comprises multiple modalities, including structured data, clinical notes, waveforms, and imaging data. Working with these disjointed modalities requires an extensive manual effort to preprocess and align them for downstream analysis. While several pipelines for MIMIC-IV data extraction are available, they target a small subset of modalities or do not fully support arbitrary downstream applications. In this work, we greatly expand our prior popular unimodal pipeline and present a comprehensive and customizable multimodal pipeline that can significantly reduce multimodal processing time and enhance the reproducibility of MIMIC-based studies. Our pipeline systematically integrates the listed modalities, enabling automated cohort selection, temporal alignment across modalities, and standardized multimodal output formats suitable for arbitrary static and time-series downstream applications. We release the code, a simple UI, and a Python package for selective integration (with embedding) at https://github.com/healthylaife/MIMIC-IV-Data-Pipeline.

</details>


### [64] [Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory Storage Model Based on Invertible Compression and Learnable Prediction](https://arxiv.org/abs/2601.11609)
*Weinuo Ou*

Main category: cs.LG

TL;DR: 提出ApCM模型，通过辅助预测压缩记忆架构解决LLMs缺乏有效运行时记忆机制的问题


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型缺乏有效的运行时记忆机制，难以适应动态和个性化的交互需求

Method: 提出新颖的神经记忆存储架构——辅助预测压缩记忆模型（ApCM模型）

Result: 从摘要中无法得知具体实验结果

Conclusion: 该模型旨在解决LLMs的记忆机制问题，提升对动态个性化交互的适应能力

Abstract: Current large language models (LLMs) generally lack an effective runtime memory mechanism,making it difficult to adapt to dynamic and personalized interaction requirements. To address this issue, this paper proposes a novel neural memory storage architecture--the Auxiliary Prediction Compression Memory Model (ApCM Model).

</details>


### [65] [Integrating Temporal Context into Streaming Data for Human Activity Recognition in Smart Home](https://arxiv.org/abs/2601.11611)
*Marina Vicini,Martin Rudorfer,Zhuangzhuang Dai,Luis J. Manso*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的基于被动传感器的人类活动识别方法，通过聚类活动时间（早、中、晚）并编码到特征加权中，同时加入循环时间特征和用户位置特征，在多个真实数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 随着全球人口老龄化，需要让老年人能够独立安全地在家中生活。使用PIR和门传感器等被动传感器监测日常活动并促进预防性医疗干预越来越受关注。传统HAR方法虽然能捕捉空间上下文，但有效利用时间信息仍然是一个挑战。

Method: 1. 将活动聚类为早晨、下午和夜晚三个时间段；2. 将这些时间聚类编码到特征加权方法中，计算不同的互信息矩阵；3. 扩展特征向量，加入一天中的时间和一周中的天作为循环时间特征；4. 添加跟踪用户位置的特征。

Result: 在四个真实世界数据集的三个中，该方法在准确率和F1分数上优于现有最先进方法，在数据量较少的情况下获得最高增益。

Conclusion: 该方法展示了开发有效智能家居解决方案以支持老年人原地养老的潜力，通过更好地利用时间信息提高了人类活动识别的性能。

Abstract: With the global population ageing, it is crucial to enable individuals to live independently and safely in their homes. Using ubiquitous sensors such as Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest for monitoring daily activities and facilitating preventative healthcare interventions for the elderly. Human Activity Recognition (HAR) from passive sensors mostly relies on traditional machine learning and includes data segmentation, feature extraction, and classification. While techniques like Sensor Weighting Mutual Information (SWMI) capture spatial context in a feature vector, effectively leveraging temporal information remains a challenge. We tackle this by clustering activities into morning, afternoon, and night, and encoding them into the feature weighting method calculating distinct mutual information matrices. We further propose to extend the feature vector by incorporating time of day and day of week as cyclical temporal features, as well as adding a feature to track the user's location. The experiments show improved accuracy and F1-score over existing state-of-the-art methods in three out of four real-world datasets, with highest gains in a low-data regime. These results highlight the potential of our approach for developing effective smart home solutions to support ageing in place.

</details>


### [66] [A Review on Machine Learning Approaches for the Prediction of Glucose Levels and Hypogylcemia](https://arxiv.org/abs/2601.11615)
*Beyza Cinar,Louisa van den Boom,Maria Maleshkova*

Main category: cs.LG

TL;DR: 本文综述了基于连续血糖监测数据的机器学习模型在1型糖尿病低血糖预测中的应用，比较了不同预测时间窗口下模型的性能表现。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病患者需要终身胰岛素治疗，但胰岛素治疗有导致低血糖的副作用。低血糖会增加死亡风险，因此需要准确预测低血糖事件以改善糖尿病管理。

Method: 综述分析了基于连续血糖监测数据的机器学习模型，包括回归模型（预测血糖值）和分类模型（识别低血糖事件），比较了短期（15-120分钟）和长期（3-24小时以上）预测时间窗口下的性能。

Result: 1）1小时内的预测时间窗口效果最佳；2）传统机器学习方法在分类任务中表现最好，深度学习在回归任务中表现最好；3）模型性能受多变量数据集和输入序列长度影响；4）个性化数据能提升性能，但由于数据质量限制，基于人群的模型更受青睐。

Conclusion: 机器学习模型可以有效预测1型糖尿病患者的低血糖事件，但需要根据预测时间窗口选择合适的模型类型，同时需要考虑数据质量和个性化因素对性能的影响。

Abstract: Type 1 Diabetes (T1D) is an autoimmune disease leading to insulin insufficiency. Thus, patients require lifelong insulin therapy, which has a side effect of hypoglycemia. Hypoglycemia is a critical state of decreased blood glucose levels (BGL) below 70 mg/dL and is associated with increased risk of mortality. Machine learning (ML) models can improve diabetes management by predicting hypoglycemia and providing optimal prevention methods. ML models are classified into regression and classification based, that forecast glucose levels and identify events based on defined labels, respectively. This review investigates state-of-the-art models trained on data of continuous glucose monitoring (CGM) devices from patients with T1D. We compare the models' performance across short-term (15 to 120 min) and long term (3 to more than 24 hours) prediction horizons (PHs). Particularly, we explore: 1) How much in advance can glucose values or a hypoglycemic event be accurately predicted? 2) Which models have the best performance? 3) Which factors impact the performance? and 4) Does personalization increase performance? The results show that 1) a PH of up to 1 hour provides the best results. 2) Conventional ML methods yield the best results for classification and DL for regression. A single model cannot adequately classify across multiple PHs. 3) The model performance is influenced by multivariate datasets and the input sequence length (ISL). 4) Personal data enhances performance but due to limited data quality population-based models are preferred.

</details>


### [67] [Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective](https://arxiv.org/abs/2601.11616)
*Feilong Liu*

Main category: cs.LG

TL;DR: MoE架构通过软分区表示空间，降低局部敏感性和曲率，同时增加表示的有效秩，专家间变换近乎正交。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构对学习函数和表示几何特性的影响，理解路由机制如何塑造表示空间的几何结构。

Method: 引入双雅可比-PCA谱几何探针，通过雅可比奇异值谱分析局部函数几何，通过加权PCA分析路由隐藏状态的表示几何，在可控MLP-MoE设置下比较密集、Top-k和全软路由架构。

Result: MoE路由一致降低局部敏感性，专家局部雅可比矩阵具有更小的主导奇异值和更快的谱衰减；加权PCA显示专家局部表示在更多主方向上分布方差，表明更高有效秩；平均专家雅可比矩阵近乎正交；Top-k路由产生更低秩、更集中的专家局部结构，全软路由产生更宽、更高秩的表示。

Conclusion: MoE可解释为函数空间的软分区，能够平坦化局部曲率同时重新分布表示方差，为理解MoE架构的几何特性提供了新视角。

Abstract: Mixture-of-Experts (MoE) architectures are commonly motivated by efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly characterized. In this work, we study MoEs through a geometric lens, interpreting routing as a form of soft partitioning of the representation space into overlapping local charts. We introduce a Dual Jacobian-PCA Spectral Geometry probe. It analyzes local function geometry via Jacobian singular-value spectra and representation geometry via weighted PCA of routed hidden states. Using a controlled MLP-MoE setting that permits exact Jacobian computation, we compare dense, Top-k, and fully-soft routing architectures under matched capacity. Across random seeds, we observe that MoE routing consistently reduces local sensitivity, with expert-local Jacobians exhibiting smaller leading singular values and faster spectral decay than dense baselines. At the same time, weighted PCA reveals that expert-local representations distribute variance across a larger number of principal directions, indicating higher effective rank under identical input distributions. We further find that average expert Jacobians are nearly orthogonal, suggesting a decomposition of the transformation into low-overlap expert-specific subspaces rather than scaled variants of a shared map. We analyze how routing sharpness modulates these effects, showing that Top-k routing produces lower-rank, more concentrated expert-local structure, while fully-soft routing yields broader, higher-rank representations. Together, these results support a geometric interpretation of MoEs as soft partitionings of function space that flatten local curvature while redistributing representation variance.

</details>


### [68] [Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention](https://arxiv.org/abs/2601.11618)
*Luis Rosario Freytes*

Main category: cs.LG

TL;DR: 几何注意力（GA）通过四个独立输入定义注意力层：载体、证据核规则、探针族和锚定/更新规则，将不变结构与建模选择分离，为注意力机制提供统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制（如Transformer中的softmax）缺乏统一的理论框架，难以进行系统比较和扩展。本文旨在建立几何注意力的形式化框架，分离不变结构与建模选择，为注意力机制提供理论基础。

Method: 提出几何注意力框架，通过四个组件定义：有限载体（可寻址索引）、证据核规则（掩码原始分数和链接产生非负权重）、探针族（可观测量的集合）、锚定/更新规则（选择和应用代表性核）。在标量关系工作表示和证据乘法组合律下，推导出指数链接族（Gibbs权重），包含softmax核作为子情况。通过商化一元行/列分数场，得到规范秩r范式（Eckart-Young/SVD）。

Result: 建立了统一的几何注意力框架，将标准Transformer注意力、自适应载体、多核注意力、基于计划的锚定（如熵最优传输/Sinkhorn）等作为特例包含其中。证明了点积分数图实现了相应的低秩交互机制。

Conclusion: 几何注意力框架分离了注意力机制的不变结构与建模选择，为系统比较和扩展注意力机制提供了理论基础，支持多种注意力变体（包括多核、自适应载体、基于计划的锚定等）的统一描述。

Abstract: Geometric Attention (GA) specifies an attention layer by four independent inputs: a finite carrier (what indices are addressable), an evidence-kernel rule (how masked proto-scores and a link induce nonnegative weights), a probe family (which observables are treated as admissible), and an anchor/update rule (which representative kernel is selected and how it is applied). Probe families induce an operational equivalence relation on kernels and therefore a gauge; anchors select representatives relative to that probe. Under a scalar relational-work representation and a multiplicative compositionality law for evidence, the admissible link family is exponential, yielding Gibbs weights; with row anchoring this includes the softmax kernel family as a subregime. After quotienting unary row/column score fields, the remaining interaction component admits a canonical rank-r normal form (Eckart-Young/SVD); dot-product score charts implement the corresponding low-rank interaction regime. Fixing the carrier and extensionalizing the update yields the standard fixed-token Transformer attention operator; allowing carrier updates yields adaptive-carrier and staged-depth regimes. The operator language also supports multihead/mixed kernels, plan-based anchors (e.g., entropic OT/Sinkhorn), and unary operators (e.g., FFN-style fields) as explicit regime choices. This separates invariant structure from modeling choice, enabling principled comparison and extension of attention mechanisms, and attention-based architectures.

</details>


### [69] [NoiseFormer -- Noise Diffused Symmetric Attention Transformer](https://arxiv.org/abs/2601.11619)
*Phani Kumar,Nyshadham,Jyothendra Varma,Polisetty V R K,Aditya Rathore*

Main category: cs.LG

TL;DR: 提出Noise Diffused Symmetric Attention Transformer，在保持Symmetric Attention内存优势的同时，通过微小参数和计算开销提升模型性能，在GLUE基准测试中表现介于原始Symmetric Attention和GPT2基础模型之间。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型规模急剧增大，内存占用问题导致难以在单个GPU/AI加速器上部署，需要多设备计算从而增加成本。这促使需要稀疏注意力等参数减少技术来降低模型大小。

Method: 分析Symmetric Dot-Product Attention（对称注意力）技术，提出Noise Diffused Symmetric Attention Transformer统一架构。在保持对称注意力内存优势的同时，通过添加微小噪声扩散机制来增强性能。

Result: 在GPT2基础模型上验证，在多种GLUE基准任务中，准确率表现介于原始对称注意力和GPT2基础模型之间，同时相对于基础模型实现了显著的模型大小缩减。

Conclusion: 提出的Noise Diffused Symmetric Attention Transformer在保持内存效率的同时，通过微小开销实现了性能提升，为大规模语言模型的高效部署提供了有效解决方案。

Abstract: Transformer architecture has been very successful long runner in the field of Deep Learning (DL) and Large Language Models (LLM) because of its powerful attention-based learning and parallel-natured architecture. As the models grow gigantic in terms of memory footprint, difficulties in fitting the model on a device like a GPU or an AI accelerator give rise to the need for multiple computing devices thereby escalating the computing cost. This increased training/inference cost paved the way for efficient model size reduction/parametric reduction deploying Sparse Attention techniques. In this paper, we start analyzing one of the techniques of Sparse Attention called Symmetric Dot-Product Attention (referred to as Symmetric Attention) and propose a novel unified model architecture called Noise Diffused Symmetric Attention Transformer to enhance the model's performance. While maintaining the memory gains of Symmetric Attention, with minute overhead in terms of model parameters and computational overhead, the proposed model brings in enhanced performance in terms of accuracy and inference-time sampling. The proposed model is validated upon GPT2 base model and the results reflect the performance gains falling between plain Symmetric attention and GPT2 base model on a variety of GLUE benchmark tasks in terms of accuracy, with significant model size reduction with respect to the base model.

</details>


### [70] [Verifying Physics-Informed Neural Network Fidelity using Classical Fisher Information from Differentiable Dynamical System](https://arxiv.org/abs/2601.11638)
*Josafat Ribeiro Leal Filho,Antônio Augusto Fröhlich*

Main category: cs.LG

TL;DR: 该论文提出使用Fisher信息$g_F^C$来量化评估PINNs对物理系统动态行为的捕获程度，通过比较PINN学习模型与原始解析模型的Fisher信息景观来验证PINN的全面保真度。


<details>
  <summary>Details</summary>
Motivation: 当前PINNs在求解微分方程和建模物理系统方面表现出色，但缺乏严格的方法来量化PINN是否完整捕获了系统的动态行为（而不仅仅是轨迹预测）。需要一种评估PINN对系统几何和稳定性特性保真度的框架。

Method: 提出使用Fisher信息$g_F^C$（针对可微动态系统）来评估PINN的保真度。该方法计算并比较原始解析模型和PINN学习模型的Fisher信息景观，通过分析各自系统动态的雅可比矩阵来量化PINN对系统内在不确定性、相空间曲率和状态空间演化拉伸效应的捕获程度。

Result: 论文提出了一个实验框架，使用汽车动态模型作为案例，计算和比较解析模型与训练后PINN的$g_F^C$。这种比较提供了PINN在表示系统复杂动态特性方面保真度的定量度量。

Conclusion: 通过Fisher信息$g_F^C$的比较，可以评估PINN是否不仅准确预测状态演化，而且完整捕获了物理系统的几何和稳定性特性，为PINN的全面保真度评估提供了新方法。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations and modeling physical systems by embedding physical laws into the learning process. However, rigorously quantifying how well a PINN captures the complete dynamical behavior of the system, beyond simple trajectory prediction, remains a challenge. This paper proposes a novel experimental framework to address this by employing Fisher information for differentiable dynamical systems, denoted $g_F^C$. This Fisher information, distinct from its statistical counterpart, measures inherent uncertainties in deterministic systems, such as sensitivity to initial conditions, and is related to the phase space curvature and the net stretching action of the state space evolution. We hypothesize that if a PINN accurately learns the underlying dynamics of a physical system, then the Fisher information landscape derived from the PINN's learned equations of motion will closely match that of the original analytical model. This match would signify that the PINN has achieved comprehensive fidelity capturing not only the state evolution but also crucial geometric and stability properties. We outline an experimental methodology using the dynamical model of a car to compute and compare $g_F^C$ for both the analytical model and a trained PINN. The comparison, based on the Jacobians of the respective system dynamics, provides a quantitative measure of the PINN's fidelity in representing the system's intricate dynamical characteristics.

</details>


### [71] [Task-tailored Pre-processing: Fair Downstream Supervised Learning](https://arxiv.org/abs/2601.11897)
*Jinwon Sohn,Guang Lin,Qifan Song*

Main category: cs.LG

TL;DR: 本文提出了一种针对监督学习的公平性预处理框架，通过HGR相关性分析发现现有数据公平方法正则化过强，设计了一种考虑公平-效用权衡的任务定制预处理方法，并理论分析了下游模型的公平性改进和效用保持条件。


<details>
  <summary>Details</summary>
Motivation: 现有公平性预处理方法分为两类：数据公平性（独立于下游模型）和任务定制公平性（考虑监督学习任务）。作者认为数据公平性方法从HGR相关性角度看施加了过强的正则化，因此需要设计更适合监督学习的预处理方法。

Method: 提出了一种新颖的监督学习任务定制预处理框架：1）考虑公平性和效用之间的权衡来获得预处理映射；2）理论分析在变换数据上学习的任意下游监督模型，找到保证其公平性改进和效用保持的充分条件；3）在表格和图像数据集上进行实验验证。

Result: 实验表明该框架在多个下游模型中保持一致的公平-效用权衡，优于现有竞争模型。特别在计算机视觉数据上，该方法仅改变与核心机器学习任务相关的必要语义特征来实现公平性。

Conclusion: 本文提出的任务定制预处理框架在理论上为下游模型提供公平性保证，在实践中在多种数据集上表现出优越性能，特别是在视觉任务中能够精确地调整相关语义特征来实现公平性。

Abstract: Fairness-aware machine learning has recently attracted various communities to mitigate discrimination against certain societal groups in data-driven tasks. For fair supervised learning, particularly in pre-processing, there have been two main categories: data fairness and task-tailored fairness. The former directly finds an intermediate distribution among the groups, independent of the type of the downstream model, so a learned downstream classification/regression model returns similar predictive scores to individuals inputting the same covariates irrespective of their sensitive attributes. The latter explicitly takes the supervised learning task into account when constructing the pre-processing map. In this work, we study algorithmic fairness for supervised learning and argue that the data fairness approaches impose overly strong regularization from the perspective of the HGR correlation. This motivates us to devise a novel pre-processing approach tailored to supervised learning. We account for the trade-off between fairness and utility in obtaining the pre-processing map. Then we study the behavior of arbitrary downstream supervised models learned on the transformed data to find sufficient conditions to guarantee their fairness improvement and utility preservation. To our knowledge, no prior work in the branch of task-tailored methods has theoretically investigated downstream guarantees when using pre-processed data. We further evaluate our framework through comparison studies based on tabular and image data sets, showing the superiority of our framework which preserves consistent trade-offs among multiple downstream models compared to recent competing models. Particularly for computer vision data, we see our method alters only necessary semantic features related to the central machine learning task to achieve fairness.

</details>


### [72] [Global Optimization By Gradient from Hierarchical Score-Matching Spaces](https://arxiv.org/abs/2601.11639)
*Ming Li*

Main category: cs.LG

TL;DR: 提出了一种通过分数匹配获取梯度，将带复杂约束的优化问题统一为无约束分层优化目标的方法，首次实现了基于严格梯度的确定性全局优化，并揭示了全局优化与扩散生成模型之间的深刻联系。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降方法存在局限性：只能找到局部最优解，且仅适用于连续可微问题和简单凸约束。需要解决这些限制，处理各种复杂约束的优化问题。

Method: 将带复杂约束的优化问题统一为无约束的分层优化目标，通过分数匹配技术获取梯度进行优化，实现了基于严格梯度的确定性全局优化方法。

Result: 通过简单构造和复杂实际实验验证了方法的有效性，首次实现了使用严格梯度的确定性全局优化，并揭示了全局优化与扩散生成模型之间的深刻联系。

Conclusion: 该方法突破了传统梯度下降的局限性，为处理复杂约束优化问题提供了新思路，建立了全局优化与扩散生成模型之间的理论联系，具有重要的理论和实践意义。

Abstract: Gradient descent is the most commonly used optimization method, but limited to local optimality, and confined to the field of continuous differentiable problems with simple convex constraints. This work solve these limitations and restrictions by unifying all optimization problems with various complex constraints as a general hierarchical optimization objective without constraints, which is optimized by gradient obtained through score matching. By this way, global optimization by deterministic method using strict gradient is achieved for the first time, and verified through simple-constructed and complex-practical experiments. Even more importantly, it reveals the profound connection between global optimization and diffusion based generative modeling.

</details>


### [73] [Federated Learning for the Design of Parametric Insurance Indices under Heterogeneous Renewable Production Losses](https://arxiv.org/abs/2601.12178)
*Fallou Niakh*

Main category: cs.LG

TL;DR: 提出联邦学习框架用于参数化保险指数校准，处理可再生能源生产损失的异质性，通过分布式优化学习共同指数而不共享原始数据。


<details>
  <summary>Details</summary>
Motivation: 可再生能源生产损失存在异质性，传统方法需要共享敏感数据，联邦学习可以在保护数据隐私的同时进行参数化保险指数校准。

Method: 生产者使用Tweedie广义线性模型本地建模损失，通过联邦优化（FedAvg、FedProx、FedOpt）学习共同指数，处理方差和链接函数的异质性。

Result: 在德国太阳能发电的实证应用中，联邦学习在适度异质性下恢复可比较的指数系数，相比现有近似聚合方法提供更通用和可扩展的框架。

Conclusion: 联邦学习为参数化保险指数校准提供了有效的分布式解决方案，既能保护数据隐私，又能处理生产损失的异质性。

Abstract: We propose a federated learning framework for the calibration of parametric insurance indices under heterogeneous renewable energy production losses. Producers locally model their losses using Tweedie generalized linear models and private data, while a common index is learned through federated optimization without sharing raw observations. The approach accommodates heterogeneity in variance and link functions and directly minimizes a global deviance objective in a distributed setting. We implement and compare FedAvg, FedProx and FedOpt, and benchmark them against an existing approximation-based aggregation method. An empirical application to solar power production in Germany shows that federated learning recovers comparable index coefficients under moderate heterogeneity, while providing a more general and scalable framework.

</details>


### [74] [Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning](https://arxiv.org/abs/2601.11657)
*Jack T. Beerman,Shobhan Roy,H. S. Udaykumar,Stephen S. Baek*

Main category: cs.LG

TL;DR: D-PARC架构通过可变形物理感知循环卷积，在保持网络精简的同时，比大规模CNN在复杂物理系统预测中表现更优，展示了物理直觉架构设计优于参数扩展的策略。


<details>
  <summary>Details</summary>
Motivation: 当前卷积神经网络在处理高度非线性物理流时存在困难，单纯扩大模型规模对物理建模效果有限，需要借鉴混合拉格朗日-欧拉数值方法的物理直觉来改进架构。

Method: 提出可变形物理感知循环卷积（D-PARC），借鉴混合拉格朗日-欧拉方法，克服CNN的刚性限制，使卷积核能够自适应变形以更好地捕捉物理特征。

Result: 在Burgers方程、Navier-Stokes和反应流等多个物理系统中，D-PARC相比规模大得多的架构实现了更高的预测精度，卷积核表现出反聚类行为，形成独特的"主动过滤"策略。

Conclusion: 物理直觉的架构设计比单纯扩大网络规模更有效，精简网络的策略性学习为物理感知深度学习提供了更有效的发展路径，D-PARC能够自主集中资源于高应变区域，类似于计算力学中的自适应细化。

Abstract: Physics-aware deep learning (PADL) enables rapid prediction of complex physical systems, yet current convolutional neural network (CNN) architectures struggle with highly nonlinear flows. While scaling model size addresses complexity in broader AI, this approach yields diminishing returns for physics modeling. Drawing inspiration from Hybrid Lagrangian-Eulerian (HLE) numerical methods, we introduce deformable physics-aware recurrent convolutions (D-PARC) to overcome the rigidity of CNNs. Across Burgers' equation, Navier-Stokes, and reactive flows, D-PARC achieves superior fidelity compared to substantially larger architectures. Analysis reveals that kernels display anti-clustering behavior, evolving into a learned "active filtration" strategy distinct from traditional h- or p-adaptivity. Effective receptive field analysis confirms that D-PARC autonomously concentrates resources in high-strain regions while coarsening focus elsewhere, mirroring adaptive refinement in computational mechanics. This demonstrates that physically intuitive architectural design can outperform parameter scaling, establishing that strategic learning in lean networks offers a more effective path forward for PADL than indiscriminate network expansion.

</details>


### [75] [One-Sided Matrix Completion from Ultra-Sparse Samples](https://arxiv.org/abs/2601.12213)
*Hongyang R. Zhang,Zhenshuo Zhang,Huy L. Nguyen,Guanghui Lan*

Main category: cs.LG

TL;DR: 提出一种在超稀疏采样下估计矩阵行空间或二阶矩矩阵的方法，通过归一化观测频率和梯度下降来恢复缺失的二阶矩信息。


<details>
  <summary>Details</summary>
Motivation: 针对大型稀疏面板数据集，当每行观测条目数少于矩阵秩时，传统矩阵补全方法失效，需要估计矩阵的行空间或二阶矩矩阵。

Method: 提出无偏估计器：对二阶矩矩阵的非零条目按观测频率归一化，然后使用梯度下降补全缺失的二阶矩矩阵条目。

Result: 理论证明：当n≥O(dr⁵ε⁻²C⁻²log d)时，梯度下降的局部最小值近似全局最优，能以ε²误差恢复T。实验显示在MovieLens数据集上减少88%偏差，在Amazon评论数据集上减少59%的T恢复误差。

Conclusion: 该方法在超稀疏采样下有效估计二阶矩矩阵，适用于大规模稀疏面板数据，显著优于基线方法。

Abstract: Matrix completion is a classical problem that has received recurring interest across a wide range of fields. In this paper, we revisit this problem in an ultra-sparse sampling regime, where each entry of an unknown, $n\times d$ matrix $M$ (with $n \ge d$) is observed independently with probability $p = C / d$, for a fixed integer $C \ge 2$. This setting is motivated by applications involving large, sparse panel datasets, where the number of rows far exceeds the number of columns. When each row contains only $C$ entries -- fewer than the rank of $M$ -- accurate imputation of $M$ is impossible. Instead, we estimate the row span of $M$ or the averaged second-moment matrix $T = M^{\top} M / n$.
  The empirical second-moment matrix computed from observed entries exhibits non-random and sparse missingness. We propose an unbiased estimator that normalizes each nonzero entry of the second moment by its observed frequency, followed by gradient descent to impute the missing entries of $T$. The normalization divides a weighted sum of $n$ binomial random variables by the total number of ones. We show that the estimator is unbiased for any $p$ and enjoys low variance. When the row vectors of $M$ are drawn uniformly from a rank-$r$ factor model satisfying an incoherence condition, we prove that if $n \ge O({d r^5 ε^{-2} C^{-2} \log d})$, any local minimum of the gradient-descent objective is approximately global and recovers $T$ with error at most $ε^2$.
  Experiments on both synthetic and real-world data validate our approach. On three MovieLens datasets, our algorithm reduces bias by $88\%$ relative to baseline estimators. We also empirically validate the linear sampling complexity of $n$ relative to $d$ on synthetic data. On an Amazon reviews dataset with sparsity $10^{-7}$, our method reduces the recovery error of $T$ by $59\%$ and $M$ by $38\%$ compared to baseline methods.

</details>


### [76] [Machine learning model for predicting surface wettability in laser-textured metal alloys](https://arxiv.org/abs/2601.11661)
*Mohammad Mohammadzadeh Sanandaji,Danial Ebrahimzadeh,Mohammad Ikram Haider,Yaser Mike Banad,Aleksandar Poleksic,Hongtao Ding*

Main category: cs.LG

TL;DR: 机器学习框架准确预测激光纹理金属合金的润湿性，通过形态和化学特征实现高精度预测（R²=0.942）


<details>
  <summary>Details</summary>
Motivation: 表面润湿性在热传递、润滑、微流体和表面涂层等应用中至关重要，但受形貌和化学的复杂相互作用影响。传统方法难以准确预测这种复杂关系，需要开发数据驱动的方法来设计和优化功能表面。

Method: 1. 在AA6061和AISI 4130合金上通过纳秒激光纹理化和化学浸渍处理制备超亲水和超疏水表面；2. 使用Laws纹理能量法和轮廓测量法定量表面形态；3. 通过XPS表征表面化学，提取官能团极性、分子体积和峰面积分数等特征；4. 训练包含残差连接、批量归一化和dropout正则化的集成神经网络模型。

Result: 模型实现了高预测精度（R²=0.942，RMSE=13.896），优于先前方法。特征重要性分析显示表面化学对接触角预测影响最大，形貌特征也有显著贡献。

Conclusion: 该研究证明了人工智能通过捕捉表面特性的复杂相互作用来建模和预测润湿行为的潜力，为设计定制功能表面提供了数据驱动的途径。

Abstract: Surface wettability, governed by both topography and chemistry, plays a critical role in applications such as heat transfer, lubrication, microfluidics, and surface coatings. In this study, we present a machine learning (ML) framework capable of accurately predicting the wettability of laser-textured metal alloys using experimentally derived morphological and chemical features. Superhydrophilic and superhydrophobic surfaces were fabricated on AA6061 and AISI 4130 alloys via nanosecond laser texturing followed by chemical immersion treatments. Surface morphology was quantified using the Laws texture energy method and profilometry, while surface chemistry was characterized through X-ray photoelectron spectroscopy (XPS), extracting features such as functional group polarity, molecular volume, and peak area fraction. These features were used to train an ensemble neural network model incorporating residual connections, batch normalization, and dropout regularization. The model achieved high predictive accuracy (R2 = 0.942, RMSE = 13.896), outperforming previous approaches. Feature importance analysis revealed that surface chemistry had the strongest influence on contact angle prediction, with topographical features also contributing significantly. This work demonstrates the potential of artificial intelligence to model and predict wetting behavior by capturing the complex interplay of surface characteristics, offering a data-driven pathway for designing tailored functional surfaces.

</details>


### [77] [Statistical-Neural Interaction Networks for Interpretable Mixed-Type Data Imputation](https://arxiv.org/abs/2601.12380)
*Ou Deng,Shoji Nishimura,Atsushi Ogihara,Qun Jin*

Main category: cs.LG

TL;DR: SNI是一个可解释的混合类型数据填补框架，通过可控先验特征注意力模块结合统计先验和神经注意力，提供内在的特征依赖诊断和统计-神经权衡参数。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据通常包含连续测量和分类记录，但缺失值普遍存在且会扭曲下游分析。现有方法要么缺乏可解释性，要么无法有效处理混合数据类型。

Method: 提出统计-神经交互框架，通过可控先验特征注意力模块学习头级先验强度系数，软性正则化注意力朝向统计先验，同时允许数据驱动的非线性模式偏差。

Result: 在6个数据集上评估，30%MCAR/严格MAR缺失下，SNI在连续变量指标上具有竞争力，但在分类变量上常被MissForest和MIWAE超越；提供内在依赖诊断和显式权衡参数。

Conclusion: SNI在可解释性和性能间提供权衡，特别适用于需要依赖诊断和统计-神经权衡参数的应用场景，但在严重不平衡分类目标上存在局限性。

Abstract: Real-world tabular databases routinely combine continuous measurements and categorical records, yet missing entries are pervasive and can distort downstream analysis. We propose Statistical-Neural Interaction (SNI), an interpretable mixed-type imputation framework that couples correlation-derived statistical priors with neural feature attention through a Controllable-Prior Feature Attention (CPFA) module. CPFA learns head-wise prior-strength coefficients $\{λ_h\}$ that softly regularize attention toward the prior while allowing data-driven deviations when nonlinear patterns appear to be present in the data. Beyond imputation, SNI aggregates attention maps into a directed feature-dependency matrix that summarizes which variables the imputer relied on, without requiring post-hoc explainers. We evaluate SNI against six baselines (Mean/Mode, MICE, KNN, MissForest, GAIN, MIWAE) on six datasets spanning ICU monitoring, population surveys, socio-economic statistics, and engineering applications. Under MCAR/strict-MAR at 30\% missingness, SNI is generally competitive on continuous metrics but is often outperformed by accuracy-first baselines (MissForest, MIWAE) on categorical variables; in return, it provides intrinsic dependency diagnostics and explicit statistical-neural trade-off parameters. We additionally report MNAR stress tests (with a mask-aware variant) and discuss computational cost, limitations -- particularly for severely imbalanced categorical targets -- and deployment scenarios where interpretability may justify the trade-off.

</details>


### [78] [Activation Sensitivity as a Unifying Principle for Post-Training Quantization](https://arxiv.org/abs/2601.11663)
*Bruce Changlong Xu*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的理论框架，将激活感知量化（如AWQ）和二阶量化（如GPTQ）方法统一为对"激活敏感性"的不同近似，为后训练量化提供了概念基础。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法（AWQ和GPTQ）虽然表现良好，但概念上分散，不清楚它们近似的是什么底层量。需要统一的理论框架来理解这些方法。

Method: 通过一阶泰勒展开，形式化定义"激活敏感性"（通道扰动对损失的期望影响），将其表示为梯度加权激活的平方范数，从而建立统一的理论框架。

Result: 在该框架下，AWQ和GPTQ可解释为在特定简化假设下对敏感性的互补近似。分析了敏感性度量的设计空间，连接了梯度显著性、Fisher信息和Hessian准则。

Conclusion: 该工作为理解和比较后训练量化方法提供了概念基础，通过敏感性视角统一了不同量化方法，而非提出新算法。

Abstract: Post-training quantization (PTQ) methods for large language models rely on heuristics that implicitly estimate which weight channels most strongly influence model behavior. Two dominant paradigms have emerged: activation-aware methods such as AWQ prioritize channels with large activation magnitudes, while second-order methods such as GPTQ allocate quantization error according to input covariance structure. Despite strong empirical performance, these approaches remain conceptually fragmented, and it is unclear what underlying quantity they are approximating. In this work, we present a unified theoretical framework for PTQ by formalizing activation sensitivity, defined as the expected impact of channel-wise perturbations on the loss. Using a first-order Taylor expansion, we show that sensitivity naturally arises as the squared norm of gradient-weighted activations, yielding a principled measure of channel importance that captures both activation magnitude and downstream error propagation. Within this framework, AWQ and GPTQ can be interpreted as complementary approximations that recover sensitivity under distinct simplifying assumptions. We analyze the design space of sensitivity metrics, connect gradient-based saliency, Fisher information, and Hessian-based criteria, and clarify their relationships to classical pruning methods such as Optimal Brain Damage and Optimal Brain Surgeon. Rather than proposing a new quantization algorithm, this work provides a conceptual foundation for understanding and comparing post-training quantization methods through the lens of sensitivity.

</details>


### [79] [Cooperative Multi-agent RL with Communication Constraints](https://arxiv.org/abs/2601.12518)
*Nuoya Xiong,Aarti Singh*

Main category: cs.LG

TL;DR: 提出基策略预测技术，通过旧梯度预测策略更新，减少基策略与当前策略差距，在有限通信下实现高效MARL学习


<details>
  <summary>Details</summary>
Motivation: 传统合作MARL通常假设能频繁访问全局信息（如团队奖励、其他智能体动作），但在去中心化系统中由于高通信成本不现实。当通信受限时，智能体必须依赖过时信息估计梯度和更新策略，而重要性采样方法在通信受限时（缺失数据概率高）会变得不稳定

Method: 提出基策略预测技术，利用旧梯度预测策略更新，为一系列基策略收集样本，减少基策略与当前策略之间的差距。该方法通过一轮通信收集预测基策略的样本，显著减少通信轮数

Result: 理论上证明算法在势博弈中收敛到ε-纳什均衡，仅需O(ε^{-3/4})通信轮数和O(poly(max_i |A_i|)ε^{-11/4})样本，在通信成本和样本复杂度上改进现有最优结果，避免了对联合动作空间大小的指数依赖。扩展到一般马尔可夫合作博弈寻找智能体局部最优

Conclusion: 基策略预测算法在有限通信下有效，在模拟游戏和复杂环境的MAPPO中实证验证，显著减少通信需求同时保持学习效果

Abstract: Cooperative MARL often assumes frequent access to global information in a data buffer, such as team rewards or other agents' actions, which is typically unrealistic in decentralized MARL systems due to high communication costs. When communication is limited, agents must rely on outdated information to estimate gradients and update their policies. A common approach to handle missing data is called importance sampling, in which we reweigh old data from a base policy to estimate gradients for the current policy. However, it quickly becomes unstable when the communication is limited (i.e. missing data probability is high), so that the base policy in importance sampling is outdated. To address this issue, we propose a technique called base policy prediction, which utilizes old gradients to predict the policy update and collect samples for a sequence of base policies, which reduces the gap between the base policy and the current policy. This approach enables effective learning with significantly fewer communication rounds, since the samples of predicted base policies could be collected within one communication round. Theoretically, we show that our algorithm converges to an $\varepsilon$-Nash equilibrium in potential games with only $O(\varepsilon^{-3/4})$ communication rounds and $O(poly(\max_i |A_i|)\varepsilon^{-11/4})$ samples, improving existing state-of-the-art results in communication cost, as well as sample complexity without the exponential dependence on the joint action space size. We also extend these results to general Markov Cooperative Games to find an agent-wise local maximum. Empirically, we test the base policy prediction algorithm in both simulated games and MAPPO for complex environments.

</details>


### [80] [Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction](https://arxiv.org/abs/2601.11667)
*Xiaojie Xia,Huigang Zhang,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.LG

TL;DR: 提出一种高效方法，通过块级局部蒸馏和贪婪层替换策略，将预训练的全注意力模型转换为任务特定的混合注意力模型，平衡效率和性能。


<details>
  <summary>Details</summary>
Motivation: Transformer的全注意力机制具有二次复杂度，限制了实际部署；线性注意力虽然效率高但性能下降；混合模型需要昂贵训练且设计困难。

Method: 1) 通过块级局部蒸馏将预训练全注意力模块权重转移到线性注意力模块；2) 采用贪婪层替换策略，迭代替换全注意力块为线性注意力块，同时监控验证性能。

Result: 该方法能在单次高效过程中生成任务特定的混合模型，无需昂贵重训练或神经架构搜索，可应用于任何预训练全注意力骨干网络。

Conclusion: 提出的方法有效解决了混合注意力模型训练成本高和设计困难的问题，实现了效率和表达能力的平衡。

Abstract: Transformer architectures deliver state-of-the-art accuracy via dense full-attention, but their quadratic time and memory complexity with respect to sequence length limits practical deployment. Linear attention mechanisms offer linear or near-linear scaling yet often incur performance degradation. Hybrid models that integrate full and linear attention layers promise a balance between efficiency and expressiveness, but face two major challenges: training such hybrid models from scratch is computationally expensive, and manually designing the optimal placement of attention types is highly nontrivial. We address both issues by first transferring weights from the pretrained full-attention modules to its linear attention counterparts through blockwise local distillation, and second, introducing a greedy layer replacement strategy that iteratively substitutes full attention blocks with linear ones while monitoring validation performance on the target task. This yields a task-specific hybrid model in a single efficient pass, without costly re-training or neural architecture search, and can be applied to any pretrained full-attention backbone for diverse downstream tasks.

</details>


### [81] [What Trace Powers Reveal About Log-Determinants: Closed-Form Estimators, Certificates, and Failure Modes](https://arxiv.org/abs/2601.12612)
*Piyush Sao*

Main category: cs.LG

TL;DR: 该论文提出了一种基于矩阵迹幂计算对数行列式的新方法，通过矩生成函数变换和插值技术，在常数时间内提供对数行列式的点估计和可证明的上下界。


<details>
  <summary>Details</summary>
Motivation: 高斯过程推断和贝叶斯模型比较中需要计算大型对称正定矩阵的对数行列式。传统方法结合矩阵向量乘积和多项式近似，但本文研究当矩阵幂可用时，通过迹幂计算对数行列式的不同模型。

Method: 1. 使用归一化特征值的矩生成函数M(t)，将对数行列式转化为在t=0处的导数估计问题
2. 通过变换K(t)=log M(t)压缩数值范围，利用K(0)=K(1)=0的锚点进行插值
3. 从相同的迹信息推导出对数行列式的上下界，提供可证明的区间估计
4. 所有估计器和边界计算成本为O(m)，对于m∈{4,...,8}基本上是常数时间

Result: 1. 证明了使用有限正矩的连续估计器在无界条件数下无法达到一致精度的基本限制
2. 开发了基于谱下界r≤λ_min的矩约束下界方法
3. 提供了间隙诊断指标，用于判断何时信任点估计、何时报告边界
4. 方法在计算上高效，独立于矩阵维度n

Conclusion: 该论文提出了一种新颖的对数行列式计算方法，通过矩生成函数变换和插值技术，在仅需矩阵迹幂信息的情况下，提供了高效的点估计和可证明的边界。方法特别适用于矩阵幂可用的场景，为高斯过程推断和贝叶斯模型比较提供了实用的计算工具。

Abstract: Computing $\log\det(A)$ for large symmetric positive definite matrices arises in Gaussian process inference and Bayesian model comparison. Standard methods combine matrix-vector products with polynomial approximations. We study a different model: access to trace powers $p_k = \tr(A^k)$, natural when matrix powers are available.
  Classical moment-based approximations Taylor-expand $\log(λ)$ around the arithmetic mean. This requires $|λ- \AM| < \AM$ and diverges when $κ> 4$. We work instead with the moment-generating function $M(t) = \E[X^t]$ for normalized eigenvalues $X = λ/\AM$. Since $M'(0) = \E[\log X]$, the log-determinant becomes $\log\det(A) = n(\log \AM + M'(0))$ -- the problem reduces to estimating a derivative at $t = 0$. Trace powers give $M(k)$ at positive integers, but interpolating $M(t)$ directly is ill-conditioned due to exponential growth. The transform $K(t) = \log M(t)$ compresses this range. Normalization by $\AM$ ensures $K(0) = K(1) = 0$. With these anchors fixed, we interpolate $K$ through $m+1$ consecutive integers and differentiate to estimate $K'(0)$. However, this local interpolation cannot capture arbitrary spectral features.
  We prove a fundamental limit: no continuous estimator using finitely many positive moments can be uniformly accurate over unbounded conditioning. Positive moments downweight the spectral tail; $K'(0) = \E[\log X]$ is tail-sensitive. This motivates guaranteed bounds. From the same traces we derive upper bounds on $(\det A)^{1/n}$. Given a spectral floor $r \leq λ_{\min}$, we obtain moment-constrained lower bounds, yielding a provable interval for $\log\det(A)$. A gap diagnostic indicates when to trust the point estimate and when to report bounds. All estimators and bounds cost $O(m)$, independent of $n$. For $m \in \{4, \ldots, 8\}$, this is effectively constant time.

</details>


### [82] [IPEC: Test-Time Incremental Prototype Enhancement Classifier for Few-Shot Learning](https://arxiv.org/abs/2601.11669)
*Wenwen Liao,Hang Ruan,Jianbo Yu,Xiaofeng Yang,Qingchao Jiang,Xuefeng Yan*

Main category: cs.LG

TL;DR: IPEC是一种测试时增量原型增强方法，通过利用先前查询样本的信息优化原型估计，提升小样本分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于度量的小样本方法在测试时假设批次独立，无法利用先前批次积累的宝贵知识，限制了性能提升。

Method: 提出增量原型增强分类器(IPEC)，维护动态辅助集，通过双重过滤机制选择高置信度查询样本，与支持集聚合构建更稳定的原型，采用贝叶斯解释和"预热-测试"两阶段推理协议。

Result: 在多个小样本分类任务上的广泛实验验证了IPEC的优越性能。

Conclusion: IPEC通过测试时增量学习有效减少对初始支持集的依赖，构建更稳定和具有代表性的原型，显著提升小样本分类性能。

Abstract: Metric-based few-shot approaches have gained significant popularity due to their relatively straightforward implementation, high interpret ability, and computational efficiency. However, stemming from the batch-independence assumption during testing, which prevents the model from leveraging valuable knowledge accumulated from previous batches. To address these challenges, we propose a novel test-time method called Incremental Prototype Enhancement Classifier (IPEC), a test-time method that optimizes prototype estimation by leveraging information from previous query samples. IPEC maintains a dynamic auxiliary set by selectively incorporating query samples that are classified with high confidence. To ensure sample quality, we design a robust dual-filtering mechanism that assesses each query sample based on both global prediction confidence and local discriminative ability. By aggregating this auxiliary set with the support set in subsequent tasks, IPEC builds progressively more stable and representative prototypes, effectively reducing its reliance on the initial support set. We ground this approach in a Bayesian interpretation, conceptualizing the support set as a prior and the auxiliary set as a data-driven posterior, which in turn motivates the design of a practical "warm-up and test" two-stage inference protocol. Extensive empirical results validate the superior performance of our proposed method across multiple few-shot classification tasks.

</details>


### [83] [Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization](https://arxiv.org/abs/2601.12707)
*Junyi Liao,Zihan Zhu,Ethan Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出一个统一框架，用于在正则化熵的零和矩阵博弈和马尔可夫博弈中恢复奖励函数，通过QRE建立可识别性，开发新算法从观察到的行动中学习奖励函数，并提供理论保证和实验验证。


<details>
  <summary>Details</summary>
Motivation: 在逆强化学习和博弈论中，估计驱动智能体行为的未知奖励函数是核心问题。现有方法面临逆问题的固有模糊性、可行奖励的非唯一性以及观测数据覆盖有限等挑战。

Method: 建立基于量化响应均衡(QRE)的奖励函数可识别性理论框架，提出适用于静态和动态设置的新算法，可结合最大似然估计等方法，从观察到的玩家策略和行动中恢复奖励函数。

Result: 提供了算法的可靠性和样本效率的强理论保证，通过广泛的数值研究证明了框架的实际有效性，为竞争环境中的决策制定提供了新见解。

Conclusion: 该研究为逆强化学习中奖励函数恢复问题提供了一个统一的理论和算法框架，在零和博弈设置下解决了可识别性和学习效率问题，具有理论和实践意义。

Abstract: Estimating the unknown reward functions driving agents' behaviors is of central interest in inverse reinforcement learning and game theory. To tackle this problem, we develop a unified framework for reward function recovery in two-player zero-sum matrix games and Markov games with entropy regularization, where we aim to reconstruct the underlying reward functions given observed players' strategies and actions. This task is challenging due to the inherent ambiguity of inverse problems, the non-uniqueness of feasible rewards, and limited observational data coverage. To address these challenges, we establish the reward function's identifiability using the quantal response equilibrium (QRE) under linear assumptions. Building upon this theoretical foundation, we propose a novel algorithm to learn reward functions from observed actions. Our algorithm works in both static and dynamic settings and is adaptable to incorporate different methods, such as Maximum Likelihood Estimation (MLE). We provide strong theoretical guarantees for the reliability and sample efficiency of our algorithm. Further, we conduct extensive numerical studies to demonstrate the practical effectiveness of the proposed framework, offering new insights into decision-making in competitive environments.

</details>


### [84] [A Confidence-Variance Theory for Pseudo-Label Selection in Semi-Supervised Learning](https://arxiv.org/abs/2601.11670)
*Jinshi Liu,Pan Liu*

Main category: cs.LG

TL;DR: 提出CoVar理论框架，结合最大置信度和残差类别方差作为伪标签选择准则，替代传统固定置信度阈值方法


<details>
  <summary>Details</summary>
Motivation: 传统半监督学习中的伪标签选择依赖固定置信度阈值，但深度网络常存在过度自信问题：高置信度预测可能错误，而决策边界附近信息丰富的低置信度样本却被丢弃

Method: 从熵最小化原理出发，推导出结合最大置信度(MC)和残差类别方差(RCV)的可靠性度量，将伪标签选择转化为置信度-方差特征空间中的谱松弛问题，设计无阈值选择机制

Result: 在PASCAL VOC 2012、Cityscapes、CIFAR-10和Mini-ImageNet数据集上，使用不同标签比例和骨干网络，CoVar作为插件模块持续改进强基线方法

Conclusion: 结合置信度和残差类别方差为伪标签选择提供了比固定置信度阈值更可靠的基础，CoVar框架能有效纠正过度自信但不稳定的预测

Abstract: Most pseudo-label selection strategies in semi-supervised learning rely on fixed confidence thresholds, implicitly assuming that prediction confidence reliably indicates correctness. In practice, deep networks are often overconfident: high-confidence predictions can still be wrong, while informative low-confidence samples near decision boundaries are discarded. This paper introduces a Confidence-Variance (CoVar) theory framework that provides a principled joint reliability criterion for pseudo-label selection. Starting from the entropy minimization principle, we derive a reliability measure that combines maximum confidence (MC) with residual-class variance (RCV), which characterizes how probability mass is distributed over non-maximum classes. The derivation shows that reliable pseudo-labels should have both high MC and low RCV, and that the influence of RCV increases as confidence grows, thereby correcting overconfident but unstable predictions. From this perspective, we cast pseudo-label selection as a spectral relaxation problem that maximizes separability in a confidence-variance feature space, and design a threshold-free selection mechanism to distinguish high- from low-reliability predictions. We integrate CoVar as a plug-in module into representative semi-supervised semantic segmentation and image classification methods. Across PASCAL VOC 2012, Cityscapes, CIFAR-10, and Mini-ImageNet with varying label ratios and backbones, it consistently improves over strong baselines, indicating that combining confidence with residual-class variance provides a more reliable basis for pseudo-label selection than fixed confidence thresholds. (Code: https://github.com/ljs11528/CoVar_Pseudo_Label_Selection.git)

</details>


### [85] [Online Continual Learning for Time Series: a Natural Score-driven Approach](https://arxiv.org/abs/2601.12931)
*Edoardo Urettini,Daniele Atzeni,Ioanna-Yvonni Tsaknaki,Antonio Carta*

Main category: cs.LG

TL;DR: 该论文提出NatSR方法，将在线持续学习应用于时间序列预测，通过自然梯度下降与t分布似然结合实现鲁棒优化，结合回放缓冲和动态尺度启发式方法，在预测性能上超越现有复杂方法。


<details>
  <summary>Details</summary>
Motivation: 在线时间序列预测需要快速适应环境变化同时保持长期记忆，这与在线持续学习的目标高度一致。现有研究已初步将OCL应用于OTSF，但理论和实践连接仍需加强。

Method: 1. 将神经网络优化重构为参数滤波问题，证明自然梯度下降是一种分数驱动方法并证明其信息理论最优性；2. 使用Student's t分布似然结合自然梯度实现有界更新，提高对异常值的鲁棒性；3. 提出NatSR方法，结合鲁棒优化器、回放缓冲和动态尺度启发式方法，在制度漂移时快速适应。

Result: NatSR在预测性能上超越了更复杂的现有最先进方法，表现出更强的适应能力和鲁棒性。

Conclusion: 该研究强化了时间序列方法与在线持续学习之间的理论和实践连接，提出的NatSR方法在在线时间序列预测中实现了更好的性能，为实际应用提供了有效解决方案。

Abstract: Online continual learning (OCL) methods adapt to changing environments without forgetting past knowledge. Similarly, online time series forecasting (OTSF) is a real-world problem where data evolve in time and success depends on both rapid adaptation and long-term memory. Indeed, time-varying and regime-switching forecasting models have been extensively studied, offering a strong justification for the use of OCL in these settings. Building on recent work that applies OCL to OTSF, this paper aims to strengthen the theoretical and practical connections between time series methods and OCL. First, we reframe neural network optimization as a parameter filtering problem, showing that natural gradient descent is a score-driven method and proving its information-theoretic optimality. Then, we show that using a Student's t likelihood in addition to natural gradient induces a bounded update, which improves robustness to outliers. Finally, we introduce Natural Score-driven Replay (NatSR), which combines our robust optimizer with a replay buffer and a dynamic scale heuristic that improves fast adaptation at regime drifts. Empirical results demonstrate that NatSR achieves stronger forecasting performance than more complex state-of-the-art methods.

</details>


### [86] [Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis](https://arxiv.org/abs/2601.11686)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: 提出结合预测模型与LLM的混合框架，为野火风险管理生成结构化可操作报告


<details>
  <summary>Details</summary>
Motivation: 现有野火风险评估方法忽视实际运营需求，缺乏对多维度风险的综合分析，限制了应急响应和消防服务的实用价值

Method: 开发混合框架：结合多个预测模型分别评估气象危险、点火活动、干预复杂性和资源调动等风险维度，再利用大语言模型(LLM)将异质输出合成为结构化可操作报告

Result: 论文提出的是一个概念验证方案，尚未提供具体实验结果，但展示了将预测模型与LLM结合解决野火风险管理问题的创新方法

Conclusion: 通过整合多维度预测模型和LLM的合成能力，可以创建更实用、更全面的野火风险评估系统，为应急响应提供结构化、可操作的信息支持

Abstract: Current state-of-the-art approaches to wildfire risk assessment often overlook operational needs, limiting their practical value for first responders and firefighting services. Effective wildfire management requires a multi-target analysis that captures the diverse dimensions of wildfire risk, including meteorological danger, ignition activity, intervention complexity, and resource mobilization, rather than relying on a single predictive indicator. In this proof of concept, we propose the development of a hybrid framework that combines predictive models for each risk dimension with large language models (LLMs) to synthesize heterogeneous outputs into structured, actionable reports.

</details>


### [87] [Multi-level Monte Carlo Dropout for Efficient Uncertainty Quantification](https://arxiv.org/abs/2601.13272)
*Aaron Pim,Tristan Pryer*

Main category: cs.LG

TL;DR: 提出基于多级蒙特卡洛的dropout不确定性量化框架，通过重用dropout掩码构建耦合粗-细估计器，降低方差并提升计算效率


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛dropout是深度学习中常用的不确定性量化方法，但需要大量前向传播来估计预测矩，计算成本高。本文旨在通过多级蒙特卡洛方法降低方差，在相同计算预算下获得更准确的不确定性估计。

Method: 将dropout掩码视为认知随机源，通过前向传播次数定义保真度层级。重用dropout掩码构建耦合的粗-细估计器，形成用于预测均值和方差的多级蒙特卡洛伸缩估计器。推导了偏差、方差和有效成本表达式，并制定了层级间的样本分配规则。

Result: 在正向和逆向PINNs-Uzawa基准测试中验证了预测的方差率，证明在相同计算成本下，多级蒙特卡洛dropout比单级蒙特卡洛dropout具有更高的效率增益。

Conclusion: 多级蒙特卡洛框架为dropout不确定性量化提供了有效的方差缩减方法，通过重用dropout掩码和优化样本分配，在保持无偏性的同时显著提升了计算效率。

Abstract: We develop a multilevel Monte Carlo (MLMC) framework for uncertainty quantification with Monte Carlo dropout. Treating dropout masks as a source of epistemic randomness, we define a fidelity hierarchy by the number of stochastic forward passes used to estimate predictive moments. We construct coupled coarse--fine estimators by reusing dropout masks across fidelities, yielding telescoping MLMC estimators for both predictive means and predictive variances that remain unbiased for the corresponding dropout-induced quantities while reducing sampling variance at fixed evaluation budget. We derive explicit bias, variance and effective cost expressions, together with sample-allocation rules across levels. Numerical experiments on forward and inverse PINNs--Uzawa benchmarks confirm the predicted variance rates and demonstrate efficiency gains over single-level MC-dropout at matched cost.

</details>


### [88] [jBOT: Semantic Jet Representation Clustering Emerges from Self-Distillation](https://arxiv.org/abs/2601.11719)
*Ho Fung Tsoi,Dylan Rankin*

Main category: cs.LG

TL;DR: jBOT是一种基于自蒸馏的预训练方法，专门用于处理CERN大型强子对撞机的喷注数据，结合局部粒子级和全局喷注级蒸馏，学习支持异常检测和分类等下游任务的喷注表示。


<details>
  <summary>Details</summary>
Motivation: 自监督学习是一种无需标签即可学习特征表示的强大预训练方法，通常能捕捉数据的通用底层语义，并可在后续微调用于下游任务。本研究旨在为LHC喷注数据开发有效的预训练方法。

Method: 提出jBOT预训练方法，基于自蒸馏技术，结合局部粒子级蒸馏和全局喷注级蒸馏，学习喷注的表示。该方法在无标签喷注数据上进行预训练。

Result: 在无标签喷注上预训练导致表示空间中出现语义类别聚类现象。当仅在背景喷注上预训练时，冻结嵌入中的聚类可通过简单距离度量实现异常检测，且学习到的嵌入微调后用于分类任务，性能优于从头训练的监督模型。

Conclusion: jBOT方法成功地为LHC喷注数据开发了有效的自监督预训练框架，能够在表示空间形成语义聚类，支持异常检测和分类任务，并展现出优于传统监督方法的性能。

Abstract: Self-supervised learning is a powerful pre-training method for learning feature representations without labels, which often capture generic underlying semantics from the data and can later be fine-tuned for downstream tasks. In this work, we introduce jBOT, a pre-training method based on self-distillation for jet data from the CERN Large Hadron Collider, which combines local particle-level distillation with global jet-level distillation to learn jet representations that support downstream tasks such as anomaly detection and classification. We observe that pre-training on unlabeled jets leads to emergent semantic class clustering in the representation space. The clustering in the frozen embedding, when pre-trained on background jets only, enables anomaly detection via simple distance-based metrics, and the learned embedding can be fine-tuned for classification with improved performance compared to supervised models trained from scratch.

</details>


### [89] [Fairness-informed Pareto Optimization : An Efficient Bilevel Framework](https://arxiv.org/abs/2601.13448)
*Sofiane Tanji,Samuel Vaiter,Yassine Laguel*

Main category: cs.LG

TL;DR: BADR是一个双层自适应重新标量化框架，可为任何公平性指标恢复最优帕累托效率模型，解决了现有方法偏向特定公平视角且无法适应广泛公平指标的问题。


<details>
  <summary>Details</summary>
Motivation: 现有公平机器学习方法经常产生帕累托低效模型，某些群体的性能可以在不损害其他群体的情况下得到改进。传统处理方法（如通过正则化的公平性）存在此问题，而现有帕累托效率方法偏向特定公平视角，无法适应文献中研究的广泛公平指标。

Method: 提出BADR（Bilevel Adaptive Rescalarisation）框架：下层是加权经验风险最小化任务，权重是各组的凸组合；上层优化选择的公平性目标。开发了两种新颖的大规模单循环算法BADR-GD和BADR-SGD，并建立了收敛保证。

Result: 发布了badr开源Python工具箱，支持多种学习任务和公平性指标。通过大量数值实验证明BADR相比现有帕累托效率公平性方法的优势。

Conclusion: BADR提供了一个简单框架，可为任何公平性指标恢复最优帕累托效率模型，解决了现有方法的局限性，并通过开源工具和收敛保证算法实现了实际应用。

Abstract: Despite their promise, fair machine learning methods often yield Pareto-inefficient models, in which the performance of certain groups can be improved without degrading that of others. This issue arises frequently in traditional in-processing approaches such as fairness-through-regularization. In contrast, existing Pareto-efficient approaches are biased towards a certain perspective on fairness and fail to adapt to the broad range of fairness metrics studied in the literature. In this paper, we present BADR, a simple framework to recover the optimal Pareto-efficient model for any fairness metric. Our framework recovers its models through a Bilevel Adaptive Rescalarisation procedure. The lower level is a weighted empirical risk minimization task where the weights are a convex combination of the groups, while the upper level optimizes the chosen fairness objective. We equip our framework with two novel large-scale, single-loop algorithms, BADR-GD and BADR-SGD, and establish their convergence guarantees. We release badr, an open-source Python toolbox implementing our framework for a variety of learning tasks and fairness metrics. Finally, we conduct extensive numerical experiments demonstrating the advantages of BADR over existing Pareto-efficient approaches to fairness.

</details>


### [90] [Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis](https://arxiv.org/abs/2601.11789)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Minhak Song,Yaoqing Yang*

Main category: cs.LG

TL;DR: 论文研究了随机梯度下降（SGD）在病态优化中的"可疑对齐"现象，揭示了梯度与主导子空间对齐的动态变化规律，并分析了步长选择如何影响这一现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于观察到的"可疑对齐"现象：在病态优化中，梯度与主导子空间的对齐会经历先下降后上升最终稳定的动态过程，但令人困惑的是，这种高度对齐的主导子空间投影更新却无法有效降低损失。需要深入理解这一现象背后的机制。

Method: 在高维二次设置中进行细粒度分析，提出步长条件理论。通过分析自适应临界步长η_t^*，区分对齐下降和对齐上升机制。研究在病态条件下，步长区间如何导致批量子空间投影降低损失而主导子空间投影增加损失。

Result: 发现：1）在低对齐机制中，自适应临界步长η_t^*分离对齐下降和对齐上升机制；2）在高对齐机制中，对齐具有自校正特性，无论步长如何都会下降；3）在足够病态条件下，存在步长区间使得批量子空间投影降低损失而主导子空间投影增加损失；4）对于恒定步长和大初始化，SGD确实表现出两阶段行为：初始对齐下降阶段，随后稳定在高对齐状态。

Conclusion: 论文通过理论分析揭示了SGD在病态优化中的"可疑对齐"现象机制，解释了为什么主导子空间投影更新无效，并建立了步长选择与对齐动态之间的定量关系，为理解SGD在非凸优化中的行为提供了新视角。

Abstract: This paper explores the suspicious alignment phenomenon in stochastic gradient descent (SGD) under ill-conditioned optimization, where the Hessian spectrum splits into dominant and bulk subspaces. This phenomenon describes the behavior of gradient alignment in SGD updates. Specifically, during the initial phase of SGD updates, the alignment between the gradient and the dominant subspace tends to decrease. Subsequently, it enters a rising phase and eventually stabilizes in a high-alignment phase. The alignment is considered ``suspicious'' because, paradoxically, the projected gradient update along this highly-aligned dominant subspace proves ineffective at reducing the loss. The focus of this work is to give a fine-grained analysis in a high-dimensional quadratic setup about how step size selection produces this phenomenon. Our main contribution can be summarized as follows: We propose a step-size condition revealing that in low-alignment regimes, an adaptive critical step size $η_t^*$ separates alignment-decreasing ($η_t < η_t^*$) from alignment-increasing ($η_t > η_t^*$) regimes, whereas in high-alignment regimes, the alignment is self-correcting and decreases regardless of the step size. We further show that under sufficient ill-conditioning, a step size interval exists where projecting the SGD updates to the bulk space decreases the loss while projecting them to the dominant space increases the loss, which explains a recent empirical observation that projecting gradient updates to the dominant subspace is ineffective. Finally, based on this adaptive step-size theory, we prove that for a constant step size and large initialization, SGD exhibits this distinct two-phase behavior: an initial alignment-decreasing phase, followed by stabilization at high alignment.

</details>


### [91] [Preconditioning Benefits of Spectral Orthogonalization in Muon](https://arxiv.org/abs/2601.13474)
*Jianhao Ma,Yu Huang,Yuejie Chi,Yuxin Chen*

Main category: cs.LG

TL;DR: 该论文分析了Muon优化器的简化版本，证明了在矩阵分解和线性Transformer的上下文学习中，简化Muon具有独立于条件数的线性收敛性，优于梯度下降和Adam。


<details>
  <summary>Details</summary>
Motivation: Muon优化器作为大语言模型预训练的重要算法，其核心机制——梯度正交化的作用——尚未得到充分理解，缺乏端到端的理论分析来解释其在具体应用中的优势。

Method: 通过两个案例研究分析简化版Muon的有效性：矩阵分解和线性Transformer的上下文学习。在谱域中将Muon动态解耦为独立的标量序列，分析其收敛行为。

Result: 证明了简化Muon在这两个问题上都能线性收敛，且迭代复杂度与相关条件数无关，理论上优于梯度下降和Adam。揭示了谱正交化带来的预处理效应。

Conclusion: 该理论分析形式化了谱正交化诱导的预处理效应，为理解Muon在矩阵优化问题中的有效性提供了理论依据，并可能推广到更广泛的应用场景。

Abstract: The Muon optimizer, a matrix-structured algorithm that leverages spectral orthogonalization of gradients, is a milestone in the pretraining of large language models. However, the underlying mechanisms of Muon -- particularly the role of gradient orthogonalization -- remain poorly understood, with very few works providing end-to-end analyses that rigorously explain its advantages in concrete applications. We take a step by studying the effectiveness of a simplified variant of Muon through two case studies: matrix factorization, and in-context learning of linear transformers. For both problems, we prove that simplified Muon converges linearly with iteration complexities independent of the relevant condition number, provably outperforming gradient descent and Adam. Our analysis reveals that the Muon dynamics decouple into a collection of independent scalar sequences in the spectral domain, each exhibiting similar convergence behavior. Our theory formalizes the preconditioning effect induced by spectral orthogonalization, offering insight into Muon's effectiveness in these matrix optimization problems and potentially beyond.

</details>


### [92] [Physics-Constrained Denoising Autoencoders for Data-Scarce Wildfire UAV Sensing](https://arxiv.org/abs/2601.11794)
*Abdelrahman Ramadan,Zahra Dorbeigi Namaghi,Emily Taylor,Lucas Edwards,Xan Giuliani,David S. McLagan,Sidney Givigi,Melissa Greeff*

Main category: cs.LG

TL;DR: PC²DAE：一种物理信息去噪自编码器，通过将物理约束嵌入网络架构，解决无人机传感器数据稀缺下的野火监测去噪问题。


<details>
  <summary>Details</summary>
Motivation: 无人机搭载的低成本传感器存在基线漂移、交叉敏感性和响应延迟等问题，传统深度学习方法需要大量数据，而野火监测的无人机飞行数据有限，难以满足需求。

Method: 提出PC²DAE物理信息去噪自编码器，通过softplus激活函数确保浓度估计非负，物理合理的时间平滑约束，分层解码器头处理不同传感器家族，提供轻量版（21k参数）和宽版（204k参数）两种变体。

Result: 在仅7,894个样本（约2.2小时飞行数据）的小数据集上，PC²DAE-Lean实现67.3%平滑度提升和90.7%高频噪声减少，零物理违规，优于五种基线方法，训练时间仅65秒。

Conclusion: PC²DAE通过将物理约束直接嵌入网络架构，在数据稀缺情况下实现有效的传感器去噪，轻量版性能优于宽版，表明在数据稀缺场景中，强归纳偏置比模型容量更重要。

Abstract: Wildfire monitoring requires high-resolution atmospheric measurements, yet low-cost sensors on Unmanned Aerial Vehicles (UAVs) exhibit baseline drift, cross-sensitivity, and response lag that corrupt concentration estimates. Traditional deep learning denoising approaches demand large datasets impractical to obtain from limited UAV flight campaigns. We present PC$^2$DAE, a physics-informed denoising autoencoder that addresses data scarcity by embedding physical constraints directly into the network architecture. Non-negative concentration estimates are enforced via softplus activations and physically plausible temporal smoothing, ensuring outputs are physically admissible by construction rather than relying on loss function penalties. The architecture employs hierarchical decoder heads for Black Carbon, Gas, and CO$_2$ sensor families, with two variants: PC$^2$DAE-Lean (21k parameters) for edge deployment and PC$^2$DAE-Wide (204k parameters) for offline processing. We evaluate on 7,894 synchronized 1 Hz samples collected from UAV flights during prescribed burns in Saskatchewan, Canada (approximately 2.2 hours of flight data), two orders of magnitude below typical deep learning requirements. PC$^2$DAE-Lean achieves 67.3\% smoothness improvement and 90.7\% high-frequency noise reduction with zero physics violations. Five baselines (LSTM-AE, U-Net, Transformer, CBDAE, DeSpaWN) produce 15--23\% negative outputs. The lean variant outperforms wide (+5.6\% smoothness), suggesting reduced capacity with strong inductive bias prevents overfitting in data-scarce regimes. Training completes in under 65 seconds on consumer hardware.

</details>


### [93] [Does Privacy Always Harm Fairness? Data-Dependent Trade-offs via Chernoff Information Neural Estimation](https://arxiv.org/abs/2601.13698)
*Arjun Nichani,Hsiang Hsu,Chun-Fu,Chen,Haewon Jeong*

Main category: cs.LG

TL;DR: 该论文利用信息论中的切尔诺夫信息分析公平性、隐私性和准确性三者之间的关系，提出了噪声切尔诺夫差异作为分析工具，揭示了这种关系的数据依赖性。


<details>
  <summary>Details</summary>
Motivation: 尽管公平性和隐私性都是可信机器学习的重要支柱，但两者之间的关系研究相对较少。本文旨在探索公平性、隐私性和准确性三者之间的复杂关系，特别是它们如何相互影响。

Method: 1. 提出噪声切尔诺夫差异作为同时分析公平性、隐私性和准确性三者关系的工具
2. 在合成数据上分析该值在不同数据分布下的三种行为模式
3. 提出估计未知分布数据切尔诺夫信息的方法
4. 在真实数据集上应用该框架分析三者动态关系

Result: 1. 噪声切尔诺夫差异在合成数据中表现出三种不同的行为模式，取决于数据分布
2. 该值可以作为公平性-准确性曲线陡峭度的代理指标
3. 揭示了公平性、隐私性和准确性之间的关系具有数据依赖性
4. 成功将框架应用于真实数据集分析

Conclusion: 该研究推进了对公平性-隐私性-准确性关系的统一理解，强调了这种关系的数据依赖性。噪声切尔诺夫差异是一个有效的分析工具，能够同时考察三者之间的复杂互动关系。

Abstract: Fairness and privacy are two vital pillars of trustworthy machine learning. Despite extensive research on these individual topics, the relationship between fairness and privacy has received significantly less attention. In this paper, we utilize the information-theoretic measure Chernoff Information to highlight the data-dependent nature of the relationship among the triad of fairness, privacy, and accuracy. We first define Noisy Chernoff Difference, a tool that allows us to analyze the relationship among the triad simultaneously. We then show that for synthetic data, this value behaves in 3 distinct ways (depending on the distribution of the data). We highlight the data distributions involved in these cases and explore their fairness and privacy implications. Additionally, we show that Noisy Chernoff Difference acts as a proxy for the steepness of the fairness-accuracy curves. Finally, we propose a method for estimating Chernoff Information on data from unknown distributions and utilize this framework to examine the triad dynamic on real datasets. This work builds towards a unified understanding of the fairness-privacy-accuracy relationship and highlights its data-dependent nature.

</details>


### [94] [Shapelets-Enriched Selective Forecasting using Time Series Foundation Models](https://arxiv.org/abs/2601.11821)
*Shivani Tomar,Seshu Tirupathi,Elizabeth Daly,Ivana Dusparic*

Main category: cs.LG

TL;DR: 提出基于shapelets的选择性预测框架，识别时间序列关键区域，选择性丢弃不可靠预测，提升基础模型在实际应用中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在零样本预测中表现出色，但在某些关键数据区域的预测不可靠，限制了其在具有独特趋势的真实世界应用中的可用性。

Method: 使用shapelets构建选择性预测框架：在目标域验证集上通过平移不变字典学习学习shapelets，利用基于距离的相似性识别不可靠预测区域，选择性丢弃不可靠预测。

Result: 在多样化基准数据集上，该方法使零样本模型平均减少22.17%误差，全样本微调模型减少22.62%误差；相比随机选择方法，在某个数据集上分别提升21.41%和21.43%。

Conclusion: 基于shapelets的选择性预测框架能有效识别时间序列关键区域，显著提升基础模型的预测可靠性，为实际应用提供更现实的模型能力评估。

Abstract: Time series foundation models have recently gained a lot of attention due to their ability to model complex time series data encompassing different domains including traffic, energy, and weather. Although they exhibit strong average zero-shot performance on forecasting tasks, their predictions on certain critical regions of the data are not always reliable, limiting their usability in real-world applications, especially when data exhibits unique trends. In this paper, we propose a selective forecasting framework to identify these critical segments of time series using shapelets. We learn shapelets using shift-invariant dictionary learning on the validation split of the target domain dataset. Utilizing distance-based similarity to these shapelets, we facilitate the user to selectively discard unreliable predictions and be informed of the model's realistic capabilities. Empirical results on diverse benchmark time series datasets demonstrate that our approach leveraging both zero-shot and full-shot fine-tuned models reduces the overall error by an average of 22.17% for zero-shot and 22.62% for full-shot fine-tuned model. Furthermore, our approach using zero-shot and full-shot fine-tuned models, also outperforms its random selection counterparts by up to 21.41% and 21.43% on one of the datasets.

</details>


### [95] [Orthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks](https://arxiv.org/abs/2601.13776)
*Thibaut Boissin,Franck Mamalet,Valentin Lafargue,Mathieu Serrurier*

Main category: cs.LG

TL;DR: Orthogonium是一个统一的PyTorch库，提供正交和1-Lipschitz神经网络层，解决了现有实现分散、有限且计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 正交和1-Lipschitz神经网络层对于认证对抗鲁棒性、稳定生成模型和可靠循环网络至关重要，但现有实现分散、有限且计算成本高，阻碍了这些鲁棒深度学习架构的采用。

Method: 开发了Orthogonium库，提供统一、高效、全面的正交和1-Lipschitz层实现，支持标准卷积特征（步长、膨胀、分组、转置等），同时保持严格的数学保证，并优化实现以减少大规模基准测试的开销。

Result: 该库显著降低了采用障碍，支持可扩展的实验和集成，同时通过严格测试发现了现有实现中的关键错误，强调了标准化可靠工具的重要性。

Conclusion: Orthogonium为需要正交性和鲁棒Lipschitz约束的多样化应用提供了一个统一、高效、可靠的解决方案，有助于推动鲁棒深度学习架构的发展和应用。

Abstract: Orthogonal and 1-Lipschitz neural network layers are essential building blocks in robust deep learning architectures, crucial for certified adversarial robustness, stable generative models, and reliable recurrent networks. Despite significant advancements, existing implementations remain fragmented, limited, and computationally demanding. To address these issues, we introduce Orthogonium , a unified, efficient, and comprehensive PyTorch library providing orthogonal and 1-Lipschitz layers. Orthogonium provides access to standard convolution features-including support for strides, dilation, grouping, and transposed-while maintaining strict mathematical guarantees. Its optimized implementations reduce overhead on large scale benchmarks such as ImageNet. Moreover, rigorous testing within the library has uncovered critical errors in existing implementations, emphasizing the importance of standardized and reliable tools. Orthogonium thus significantly lowers adoption barriers, enabling scalable experimentation and integration across diverse applications requiring orthogonality and robust Lipschitz constraints. Orthogonium is available at https://github.com/deel-ai/orthogonium.

</details>


### [96] [MixFlow: Mixture-Conditioned Flow Matching for Out-of-Distribution Generalization](https://arxiv.org/abs/2601.11827)
*Andrea Rubbi,Amir Akbarnejad,Mohammad Vali Sanian,Aryan Yazdan Parast,Hesam Asadollahzadeh,Arian Amani,Naveed Akhtar,Sarah Cooper,Andrew Bassett,Pietro Liò,Lassi Paavolainen,Sattar Vakili,Mo Lotfollahi*

Main category: cs.LG

TL;DR: MixFlow：一种用于描述符控制生成的混合条件流匹配框架，通过联合学习描述符条件的基础分布和流场，显著提升分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有条件流方法在分布偏移下泛化能力不足，难以在训练条件之外进行外推，这限制了条件生成模型的实际应用。

Method: 提出MixFlow框架，通过最短路径流匹配联合学习描述符条件的基础分布（建模为可学习的描述符依赖混合分布）和描述符条件的流场，实现平滑插值和外推。

Result: 在多个领域（单细胞转录组数据扰动响应预测、高内涵显微镜药物筛选等）中，MixFlow在分布外泛化方面显著优于标准条件流匹配基线方法。

Conclusion: MixFlow提供了一种简单而强大的方法，可在异构领域实现鲁棒、可泛化且可控的生成建模，有效解决了条件生成模型在分布偏移下的泛化挑战。

Abstract: Achieving robust generalization under distribution shift remains a central challenge in conditional generative modeling, as existing conditional flow-based methods often struggle to extrapolate beyond the training conditions. We introduce MixFlow, a conditional flow-matching framework for descriptor-controlled generation that directly targets this limitation by jointly learning a descriptor-conditioned base distribution and a descriptor-conditioned flow field via shortest-path flow matching. By modeling the base distribution as a learnable, descriptor-dependent mixture, MixFlow enables smooth interpolation and extrapolation to unseen conditions, leading to substantially improved out-of-distribution generalization. We provide analytical insights into the behavior of the proposed framework and empirically demonstrate its effectiveness across multiple domains, including prediction of responses to unseen perturbations in single-cell transcriptomic data and high-content microscopy-based drug screening tasks. Across these diverse settings, MixFlow consistently outperforms standard conditional flow-matching baselines. Overall, MixFlow offers a simple yet powerful approach for achieving robust, generalizable, and controllable generative modeling across heterogeneous domains.

</details>


### [97] [Inverting Self-Organizing Maps: A Unified Activation-Based Framework](https://arxiv.org/abs/2601.13851)
*Alessandro Londei,Matteo Benati,Denise Lanzieri,Vittorio Loreto*

Main category: cs.LG

TL;DR: 提出MUSIC方法，通过自组织映射(SOM)的原型距离反演精确恢复输入数据，并实现可控的语义轨迹生成，无需采样或编码器-解码器架构。


<details>
  <summary>Details</summary>
Motivation: 传统SOM主要用于可视化、聚类和向量量化，但缺乏精确反演和可控生成的能力。作者希望利用SOM的原型距离几何特性，实现精确数据恢复和可控语义变化。

Method: 基于欧氏距离几何理论：D维空间中的点可由其到D+1个仿射独立参考点的距离唯一确定。提出MUSIC更新规则，通过修改选定原型的平方距离同时保持其他距离不变，实现确定性几何流。使用Tikhonov正则化稳定更新规则。

Result: 在合成高斯混合、MNIST和Faces in the Wild数据集上验证，MUSIC能产生平滑、可解释的轨迹，揭示学习流形的底层几何结构。无扰动时精确恢复输入，指定目标聚类时产生连贯的语义变化。

Conclusion: MUSIC为基于原型几何的数据增强和可控潜在探索提供了新视角，展示了SOM反演相比无监督聚类的优势，无需依赖采样、潜在先验或编码器-解码器架构。

Abstract: Self-Organizing Maps provide topology-preserving projections of high-dimensional data and have been widely used for visualization, clustering, and vector quantization. In this work, we show that the activation pattern of a SOM - the squared distances to its prototypes - can be inverted to recover the exact input under mild geometric conditions. This follows from a classical fact in Euclidean distance geometry: a point in $D$ dimensions is uniquely determined by its distances to $D{+}1$ affinely independent references. We derive the corresponding linear system and characterize the conditions under which the inversion is well-posed. Building upon this mechanism, we introduce the Manifold-Aware Unified SOM Inversion and Control (MUSIC) update rule, which enables controlled, semantically meaningful trajectories in latent space. MUSIC modifies squared distances to selected prototypes while preserving others, resulting in a deterministic geometric flow aligned with the SOM's piecewise-linear structure. Tikhonov regularization stabilizes the update rule and ensures smooth motion on high-dimensional datasets. Unlike variational or probabilistic generative models, MUSIC does not rely on sampling, latent priors, or encoder-decoder architectures. If no perturbation is applied, inversion recovers the exact input; when a target cluster or prototype is specified, MUSIC produces coherent semantic variations while remaining on the data manifold. This leads to a new perspective on data augmentation and controllable latent exploration based solely on prototype geometry. We validate the approach using synthetic Gaussian mixtures, the MNIST and the Faces in the Wild dataset. Across all settings, MUSIC produces smooth, interpretable trajectories that reveal the underlying geometry of the learned manifold, illustrating the advantages of SOM-based inversion over unsupervised clustering.

</details>


### [98] [AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training](https://arxiv.org/abs/2601.11864)
*Zhiyuan Li,Yuan Wu,Yi Chang*

Main category: cs.LG

TL;DR: AGGC提出自适应分组梯度裁剪方法，通过按功能类型分组参数并基于历史行为进行EMA调整，解决传统全局裁剪中的梯度异质性问题，在多个LLM微调任务中表现优于LoRA和全参数微调。


<details>
  <summary>Details</summary>
Motivation: 传统全局梯度裁剪假设所有参数梯度同质化，但实际上不同功能模块的梯度行为差异很大，导致"溢出效应"——波动大的参数会不必要地缩放稳定参数，影响训练稳定性。

Method: AGGC将参数按功能类型分组，使用指数移动平均(EMA)跟踪每组的历史梯度行为，构建自适应区间同时缓解梯度爆炸和消失问题，并采用时间相关调度机制平衡探索与收敛。

Result: 在LLaMA 2-7B、Mistral-7B和Gemma-7B模型上，AGGC一致优于LoRA并经常超越全参数微调。在GSM8K基准测试中，Mistral-7B使用AGGC达到72.93%准确率，超过LoRA的69.5%。AGGC还能有效稳定RLVR训练，提升Qwen 2.5和Llama 3.2的逻辑推理能力。

Conclusion: AGGC通过模块化自适应裁剪策略有效解决了传统梯度裁剪方法的局限性，特别是梯度异质性问题。其轻量级设计可无缝集成到现有后训练流程中，开销极小，显著提升大语言模型训练的稳定性。

Abstract: To stabilize the training of Large Language Models (LLMs), gradient clipping is a nearly ubiquitous heuristic used to alleviate exploding gradients. However, traditional global norm clipping erroneously presupposes gradient homogeneity across different functional modules, leading to an adverse "spill-over" effect where volatile parameters force unnecessary scaling on stable ones. To overcome this, we propose Adaptive Group-wise Gradient Clipping (AGGC). AGGC partitions parameters into groups based on functional types and regulates each according to its historical behavior using an Exponential Moving Average (EMA). Specifically, it constructs an adaptive interval to simultaneously mitigate gradient explosion and vanishing, while employing a time-dependent scheduling mechanism to balance exploration and convergence. Experiments on LLaMA 2-7B, Mistral-7B, and Gemma-7B models show that AGGC consistently outperforms LoRA and frequently surpasses Full Fine-Tuning. On the GSM8K benchmark, Mistral-7B fine-tuned with AGGC achieves an accuracy of 72.93%, exceeding LoRA's 69.5%. AGGC also effectively stabilizes Reinforcement Learning with Verifiable Rewards (RLVR), enhancing the logic deduction of Qwen 2.5 and Llama 3.2 models. Experimental results demonstrate that AGGC effectively addresses the limitations of traditional gradient clipping methods, particularly in overcoming gradient heterogeneity, by utilizing a modular, adaptive clipping strategy to stabilize the training process. Due to its lightweight design, AGGC can be seamlessly integrated into existing post-training pipelines with negligible overhead.

</details>


### [99] [Penalizing Localized Dirichlet Energies in Low Rank Tensor Products](https://arxiv.org/abs/2601.14173)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: TPBS模型在回归任务中表现出色，通过局部Dirichlet能量正则化解决全局正则化失效问题，在过拟合情况下优于神经网络。


<details>
  <summary>Details</summary>
Motivation: 研究低秩张量积B样条模型在回归任务中的应用，探索Dirichlet能量作为平滑度度量，发现全局Dirichlet能量正则化在某些情况下失效，需要新的正则化策略。

Method: 提出基于训练点周围小超立方体定义的局部Dirichlet能量正则化策略，利用预训练的TPBS模型，引入两种从不完整样本进行推断的估计器。

Result: TPBS模型在大多数数据集上，在过拟合情况下优于神经网络，其他情况下保持竞争力。TPBS模型对过拟合更鲁棒，能持续从正则化中受益，而神经网络对过拟合更敏感，利用正则化的效果较差。

Conclusion: TPBS模型是回归任务中稳健的选择，特别是在过拟合情况下表现优异，局部Dirichlet能量正则化策略有效解决了全局正则化失效的问题。

Abstract: We study low-rank tensor-product B-spline (TPBS) models for regression tasks and investigate Dirichlet energy as a measure of smoothness. We show that TPBS models admit a closed-form expression for the Dirichlet energy, and reveal scenarios where perfect interpolation is possible with exponentially small Dirichlet energy. This renders global Dirichlet energy-based regularization ineffective. To address this limitation, we propose a novel regularization strategy based on local Dirichlet energies defined on small hypercubes centered at the training points. Leveraging pretrained TPBS models, we also introduce two estimators for inference from incomplete samples. Comparative experiments with neural networks demonstrate that TPBS models outperform neural networks in the overfitting regime for most datasets, and maintain competitive performance otherwise. Overall, TPBS models exhibit greater robustness to overfitting and consistently benefit from regularization, while neural networks are more sensitive to overfitting and less effective in leveraging regularization.

</details>


### [100] [TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures](https://arxiv.org/abs/2601.11880)
*Yingxiao Zhang,Jiaxin Duan,Junfu Zhang,Ke Feng*

Main category: cs.LG

TL;DR: TF-CoDiT：首个用于语言控制国债期货合成的扩散Transformer框架，通过离散小波变换和U型VAE处理低数据量学习，引入金融市场属性协议生成提示，在国债期货数据合成上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散Transformer在股票价格和订单流等金融时间序列数据合成方面取得了进展，但在国债期货数据合成方面的性能仍未充分探索。国债期货数据具有低交易量、市场依赖性和多变量分组相关性等独特特征，需要专门的处理方法。

Method: 提出TF-CoDiT框架：1）将多通道1-D时间序列转换为离散小波变换系数矩阵以促进低数据学习；2）设计U型VAE分层编码跨通道依赖到潜在变量，并通过解码桥接潜在空间和DWT空间，实现潜在扩散生成；3）引入金融市场属性协议（FinMAP）作为多级描述系统，从7/8个角度识别17/23个经济指标，标准化市场动态以生成提示。

Result: 在2015-2025年四种国债期货数据上测试，合成任务持续时间从一周到四个月。TF-CoDiT能生成高度真实的数据，与真实数据的误差最多为MSE 0.433和MAE 0.453。进一步研究证明TF-CoDiT在不同合约和时间范围内的鲁棒性。

Conclusion: TF-CoDiT是首个用于语言控制国债期货合成的扩散Transformer框架，通过创新的小波变换、U型VAE编码和标准化市场描述协议，成功解决了国债期货数据合成的独特挑战，为金融时间序列合成提供了新方法。

Abstract: Diffusion Transformers (DiT) have achieved milestones in synthesizing financial time-series data, such as stock prices and order flows. However, their performance in synthesizing treasury futures data is still underexplored. This work emphasizes the characteristics of treasury futures data, including its low volume, market dependencies, and the grouped correlations among multivariables. To overcome these challenges, we propose TF-CoDiT, the first DiT framework for language-controlled treasury futures synthesis. To facilitate low-data learning, TF-CoDiT adapts the standard DiT by transforming multi-channel 1-D time series into Discrete Wavelet Transform (DWT) coefficient matrices. A U-shape VAE is proposed to encode cross-channel dependencies hierarchically into a latent variable and bridge the latent and DWT spaces through decoding, thereby enabling latent diffusion generation. To derive prompts that cover essential conditions, we introduce the Financial Market Attribute Protocol (FinMAP) - a multi-level description system that standardizes daily$/$periodical market dynamics by recognizing 17$/$23 economic indicators from 7/8 perspectives. In our experiments, we gather four types of treasury futures data covering the period from 2015 to 2025, and define data synthesis tasks with durations ranging from one week to four months. Extensive evaluations demonstrate that TF-CoDiT can produce highly authentic data with errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth. Further studies evidence the robustness of TF-CoDiT across contracts and temporal horizons.

</details>


### [101] [Q-learning with Adjoint Matching](https://arxiv.org/abs/2601.14234)
*Qiyang Li,Sergey Levine*

Main category: cs.LG

TL;DR: QAM是一种新的强化学习算法，通过伴随匹配技术解决了连续动作RL中扩散/流匹配策略优化的长期挑战，避免了不稳定的反向传播，在稀疏奖励任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 连续动作强化学习中，对表达性强的扩散或流匹配策略进行高效优化一直是个挑战。直接通过多步去噪过程进行基于梯度的优化存在数值不稳定性，现有方法要么只使用价值信息丢弃梯度，要么依赖近似方法牺牲策略表达能力或引入偏差。

Method: QAM采用伴随匹配技术，这是一种最近在生成建模中提出的方法。它将评论家的动作梯度转换为逐步目标函数，避免了不稳定的反向传播，同时在最优解处提供无偏且表达性强的策略。结合评论家学习的时间差分备份。

Result: QAM在困难的稀疏奖励任务上，无论是离线RL还是离线到在线RL，都持续优于先前的方法。

Conclusion: QAM通过伴随匹配技术有效解决了连续动作RL中扩散/流匹配策略优化的长期问题，提供了一种既保持策略表达能力又避免数值不稳定性的解决方案。

Abstract: We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.

</details>


### [102] [Approximation Algorithm for Constrained $k$-Center Clustering: A Local Search Approach](https://arxiv.org/abs/2601.11883)
*Chaoqi Jia,Longkun Guo,Kewen Liao,Zhigang Lu,Chao Chen,Jason Xue*

Main category: cs.LG

TL;DR: 提出基于支配匹配集问题转换的新型局部搜索框架，解决带约束的k中心聚类问题，达到最佳近似比2


<details>
  <summary>Details</summary>
Motivation: 传统k中心问题的最佳近似比为2，任何改进都会导致P=NP。在加入实例级不能链接(CL)和必须链接(ML)约束后，虽然不相交CL集允许常数因子近似，但局部搜索能否达到此保证仍是开放问题

Method: 提出基于支配匹配集问题转换的新型局部搜索框架，将带约束的k中心聚类问题转化为支配匹配集问题，然后应用局部搜索算法

Result: 算法达到了最佳可能的近似比2，在真实世界和合成数据集上的实验结果表明，该算法在解质量上优于基线方法

Conclusion: 通过将带约束的k中心聚类问题转化为支配匹配集问题，提出的局部搜索框架成功解决了该开放问题，实现了最佳近似比2，并在实际应用中表现出色

Abstract: Clustering is a long-standing research problem and a fundamental tool in AI and data analysis. The traditional k-center problem, a fundamental theoretical challenge in clustering, has a best possible approximation ratio of 2, and any improvement to a ratio of 2 - ε would imply P = NP. In this work, we study the constrained k-center clustering problem, where instance-level cannot-link (CL) and must-link (ML) constraints are incorporated as background knowledge. Although general CL constraints significantly increase the hardness of approximation, previous work has shown that disjoint CL sets permit constant-factor approximations. However, whether local search can achieve such a guarantee in this setting remains an open question. To this end, we propose a novel local search framework based on a transformation to a dominating matching set problem, achieving the best possible approximation ratio of 2. The experimental results on both real-world and synthetic datasets demonstrate that our algorithm outperforms baselines in solution quality.

</details>


### [103] [From Relative Entropy to Minimax: A Unified Framework for Coverage in MDPs](https://arxiv.org/abs/2601.11890)
*Xihe Gu,Urbashi Mitra,Tara Javidi*

Main category: cs.LG

TL;DR: 本文提出了一种基于加权凹覆盖目标U_ρ的统一框架，用于在无奖励MDP中进行有目标的探索，该框架统一了多种现有目标，并通过梯度算法主动引导状态-动作占用分布。


<details>
  <summary>Details</summary>
Motivation: 在无奖励马尔可夫决策过程中，不同状态-动作对具有不同的重要性或难度，需要主动且有目标的探索策略。现有方法缺乏统一的框架来平衡探索的广度和深度。

Method: 提出加权参数化凹覆盖目标族U_ρ，定义在状态-动作占用测度上。该框架统一了基于散度的边际匹配、加权平均覆盖和最坏情况覆盖。利用U_ρ的凹性和梯度闭式解，开发梯度算法主动引导占用分布。

Result: U_ρ框架成功统一了多种探索目标，梯度算法能有效引导探索策略。当ρ增大时，探索策略越来越关注最少探索的状态-动作对，在极限情况下恢复最坏情况覆盖行为。

Conclusion: 提出的加权凹覆盖目标族为无奖励MDP中的有目标探索提供了统一框架，通过参数ρ灵活控制探索策略，从平均覆盖到最坏情况覆盖连续变化，梯度算法能有效实现所需的探索模式。

Abstract: Targeted and deliberate exploration of state--action pairs is essential in reward-free Markov Decision Problems (MDPs). More precisely, different state-action pairs exhibit different degree of importance or difficulty which must be actively and explicitly built into a controlled exploration strategy. To this end, we propose a weighted and parameterized family of concave coverage objectives, denoted by $U_ρ$, defined directly over state--action occupancy measures. This family unifies several widely studied objectives within a single framework, including divergence-based marginal matching, weighted average coverage, and worst-case (minimax) coverage. While the concavity of $U_ρ$ captures the diminishing return associated with over-exploration, the simple closed form of the gradient of $U_ρ$ enables an explicit control to prioritize under-explored state--action pairs. Leveraging this structure, we develop a gradient-based algorithm that actively steers the induced occupancy toward a desired coverage pattern. Moreover, we show that as $ρ$ increases, the resulting exploration strategy increasingly emphasizes the least-explored state--action pairs, recovering worst-case coverage behavior in the limit.

</details>


### [104] [DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models](https://arxiv.org/abs/2601.11895)
*Pareesa Ameneh Golnari,Adarsh Kumarappan,Wen Wen,Xiaoyu Liu,Gabriel Ryan,Yuting Sun,Shengyu Fu,Elsie Nallipogu*

Main category: cs.LG

TL;DR: DevBench是一个基于真实开发者遥测数据的代码补全基准测试，包含6种编程语言和6个任务类别，共1800个评估实例，旨在评估LLM在实际开发场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有代码补全基准测试缺乏生态效度，容易受到训练数据污染，且无法提供详细的诊断信息。需要创建一个基于真实开发者遥测数据的基准测试，以更准确地评估LLM在实际开发场景中的表现。

Method: 从真实开发者遥测数据中提取6种编程语言和6个任务类别（如API使用、代码目的理解等），构建1800个评估实例。采用功能正确性、相似性指标和LLM评判相结合的多维度评估方法，重点关注实用性和上下文相关性。

Result: 评估了9个最先进的模型，揭示了它们在语法精度、语义推理和实际效用方面的差异。基准测试提供了可操作的见解，指导模型选择和改进。

Conclusion: DevBench提供了比其他基准测试更详细的诊断信息，对实际部署和针对性模型开发至关重要。它强调了生态效度，避免了训练数据污染，能够为模型选择和改进提供有价值的指导。

Abstract: DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.

</details>


### [105] [Communication-Corruption Coupling and Verification in Cooperative Multi-Objective Bandits](https://arxiv.org/abs/2601.11924)
*Ming Shi*

Main category: cs.LG

TL;DR: 研究多智能体协作随机多臂老虎机问题，考虑向量奖励、对抗性腐败和有限验证。揭示了通信协议如何影响腐败的有效水平，从Γ到NΓ不等，并建立了相应的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体协作学习中的通信-腐败耦合问题。在对抗性腐败环境下，不同的通信协议（共享原始样本、统计摘要或仅推荐）会导致腐败影响被不同程度地放大，需要量化这种影响并设计有效的学习算法。

Method: 提出协议诱导的多重性函数来形式化通信-腐败耦合。分析三种通信协议：原始样本共享、统计摘要共享和仅推荐共享。建立参数化有效腐败水平的遗憾界，并研究验证观测如何恢复可学习性。

Result: 原始样本共享可能遭受N倍的腐败惩罚，而摘要共享和仅推荐共享保持O(Γ)的未放大项，达到中心化速率的团队遗憾。建立了信息论极限：不可避免的Ω(Γ)惩罚，以及高腐败Γ=Θ(NT)下没有干净信息则无法实现次线性遗憾。验证观测可以恢复可学习性。

Conclusion: 通信协议显著影响多智能体协作学习中对腐败的鲁棒性。摘要共享和仅推荐共享比原始样本共享更鲁棒。在高腐败环境下，验证是必要的，一旦超过识别阈值，验证就足够了，通过认证共享可以使团队遗憾独立于Γ。

Abstract: We study cooperative stochastic multi-armed bandits with vector-valued rewards under adversarial corruption and limited verification. In each of $T$ rounds, each of $N$ agents selects an arm, the environment generates a clean reward vector, and an adversary perturbs the observed feedback subject to a global corruption budget $Γ$. Performance is measured by team regret under a coordinate-wise nondecreasing, $L$-Lipschitz scalarization $φ$, covering linear, Chebyshev, and smooth monotone utilities. Our main contribution is a communication-corruption coupling: we show that a fixed environment-side budget $Γ$ can translate into an effective corruption level ranging from $Γ$ to $NΓ$, depending on whether agents share raw samples, sufficient statistics, or only arm recommendations. We formalize this via a protocol-induced multiplicity functional and prove regret bounds parameterized by the resulting effective corruption. As corollaries, raw-sample sharing can suffer an $N$-fold larger additive corruption penalty, whereas summary sharing and recommendation-only sharing preserve an unamplified $O(Γ)$ term and achieve centralized-rate team regret. We further establish information-theoretic limits, including an unavoidable additive $Ω(Γ)$ penalty and a high-corruption regime $Γ=Θ(NT)$ where sublinear regret is impossible without clean information. Finally, we characterize how a global budget $ν$ of verified observations restores learnability. That is, verification is necessary in the high-corruption regime, and sufficient once it crosses the identification threshold, with certified sharing enabling the team's regret to become independent of $Γ$.

</details>


### [106] [Trainability-Oriented Hybrid Quantum Regression via Geometric Preconditioning and Curriculum Optimization](https://arxiv.org/abs/2601.11942)
*Qingyu Meng,Yangshuai Wang*

Main category: cs.LG

TL;DR: 提出混合量子-经典回归框架，通过经典嵌入层作为几何预处理器改善量子神经网络训练稳定性，结合课程优化协议提升性能


<details>
  <summary>Details</summary>
Motivation: 量子神经网络在回归任务中面临梯度噪声和优化条件不良的问题，需要更稳定的训练方法

Method: 1) 轻量级经典嵌入层作为可学习的几何预处理器；2) 课程优化协议：逐步增加电路深度，从SPSA随机探索过渡到Adam梯度微调

Result: 在PDE回归基准和标准数据集上，该框架相比纯QNN基线表现更好，收敛更稳定，结构化误差减少，振荡分量相关误差降低

Conclusion: 几何预处理结合课程训练是稳定量子回归的实用方法，能改善量子神经网络在科学机器学习中的性能

Abstract: Quantum neural networks (QNNs) have attracted growing interest for scientific machine learning, yet in regression settings they often suffer from limited trainability under noisy gradients and ill-conditioned optimization. We propose a hybrid quantum-classical regression framework designed to mitigate these bottlenecks. Our model prepends a lightweight classical embedding that acts as a learnable geometric preconditioner, reshaping the input representation to better condition a downstream variational quantum circuit. Building on this architecture, we introduce a curriculum optimization protocol that progressively increases circuit depth and transitions from SPSA-based stochastic exploration to Adam-based gradient fine-tuning. We evaluate the approach on PDE-informed regression benchmarks and standard regression datasets under a fixed training budget in a simulator setting. Empirically, the proposed framework consistently improves over pure QNN baselines and yields more stable convergence in data-limited regimes. We further observe reduced structured errors that are visually correlated with oscillatory components on several scientific benchmarks, suggesting that geometric preconditioning combined with curriculum training is a practical approach for stabilizing quantum regression.

</details>


### [107] [Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration](https://arxiv.org/abs/2601.11953)
*Shiqing Gao,Jiaxin Ding,Luoyi Fu,Xinbing Wang*

Main category: cs.LG

TL;DR: 提出MICE方法解决约束强化学习中的约束违反问题，通过记忆模块存储危险状态并引入内在成本来缓解成本函数低估，实现更安全的探索。


<details>
  <summary>Details</summary>
Motivation: 现有约束强化学习算法在训练过程中经常出现严重的约束违反，限制了在安全关键场景中的应用。研究发现成本价值函数的低估是导致这些违反的关键因素。

Method: 提出记忆驱动的内在成本估计（MICE）方法：1）构建记忆模块存储探索过的不安全状态；2）基于当前状态访问风险区域的伪计数定义内在成本；3）提出包含内在成本的外在-内在成本价值函数，采用偏差校正策略；4）在信任区域内制定优化目标。

Result: 理论分析提供了成本价值函数的收敛保证和MICE更新的最坏情况约束违反界限。大量实验表明MICE显著减少了约束违反，同时保持了与基线相当的策略性能。

Conclusion: MICE通过缓解成本价值函数低估问题，有效减少了约束强化学习中的约束违反，实现了更安全的探索，为安全关键应用提供了可行的解决方案。

Abstract: Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards while satisfying constraints. However, existing CRL algorithms often encounter significant constraint violations during training, limiting their applicability in safety-critical scenarios. In this paper, we identify the underestimation of the cost value function as a key factor contributing to these violations. To address this issue, we propose the Memory-driven Intrinsic Cost Estimation (MICE) method, which introduces intrinsic costs to mitigate underestimation and control bias to promote safer exploration. Inspired by flashbulb memory, where humans vividly recall dangerous experiences to avoid risks, MICE constructs a memory module that stores previously explored unsafe states to identify high-cost regions. The intrinsic cost is formulated as the pseudo-count of the current state visiting these risk regions. Furthermore, we propose an extrinsic-intrinsic cost value function that incorporates intrinsic costs and adopts a bias correction strategy. Using this function, we formulate an optimization objective within the trust region, along with corresponding optimization methods. Theoretically, we provide convergence guarantees for the proposed cost value function and establish the worst-case constraint violation for the MICE update. Extensive experiments demonstrate that MICE significantly reduces constraint violations while preserving policy performance comparable to baselines.

</details>


### [108] [Data-centric Prompt Tuning for Dynamic Graphs](https://arxiv.org/abs/2601.11954)
*Yufei Peng,Cheng Yang,Zhengjie Fan,Chuan Shi*

Main category: cs.LG

TL;DR: DDGPrompt：一种面向动态图的数据中心提示框架，通过统一节点特征矩阵和三种提示矩阵（时间偏置、边权重、特征掩码）来优化预训练节点嵌入，提升少样本下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统动态图方法预训练后直接应用节点时间嵌入到下游任务，但任务差异导致性能下降，尤其在少样本场景。现有提示方法通常与特定模型架构或预训练任务强耦合，且只关注节点或时间特征而忽略空间结构信息，表达能力有限。

Method: 提出DDGPrompt框架：1) 定义统一节点表达特征矩阵，聚合节点所有相关时间和结构信息；2) 引入三种提示矩阵（时间偏置、边权重、特征掩码）在输入数据层面对特征矩阵进行全面调整，实现节点嵌入的任务特定适应。

Result: 在四个公共动态图数据集上，在严格的少样本设置下进行评估。实验结果表明，在标签有限和冷启动条件下，该方法显著优于传统方法和现有提示方法。

Conclusion: DDGPrompt通过数据中心提示框架有效优化预训练节点嵌入，解决了现有方法架构耦合和忽略空间结构的问题，在少样本动态图下游任务中表现出优越性能。

Abstract: Dynamic graphs have attracted increasing attention due to their ability to model complex and evolving relationships in real-world scenarios. Traditional approaches typically pre-train models using dynamic link prediction and directly apply the resulting node temporal embeddings to specific downstream tasks. However, the significant differences among downstream tasks often lead to performance degradation, especially under few-shot settings. Prompt tuning has emerged as an effective solution to this problem. Existing prompting methods are often strongly coupled with specific model architectures or pretraining tasks, which makes it difficult to adapt to recent or future model designs. Moreover, their exclusive focus on modifying node or temporal features while neglecting spatial structural information leads to limited expressiveness and degraded performance. To address these limitations, we propose DDGPrompt, a data-centric prompting framework designed to effectively refine pre-trained node embeddings at the input data level, enabling better adaptability to diverse downstream tasks. We first define a unified node expression feature matrix that aggregates all relevant temporal and structural information of each node, ensuring compatibility with a wide range of dynamic graph models. Then, we introduce three prompt matrices (temporal bias, edge weight, and feature mask) to adjust the feature matrix completely, achieving task-specific adaptation of node embeddings. We evaluate DDGPrompt under a strict few-shot setting on four public dynamic graph datasets. Experimental results demonstrate that our method significantly outperforms traditional methods and prompting approaches in scenarios with limited labels and cold-start conditions.

</details>


### [109] [R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning](https://arxiv.org/abs/2601.11960)
*Jingchu Wang,Bingbing Xu,Yige Yuan,Bin Xie,Xiaoqian Sun,Huawei Shen*

Main category: cs.LG

TL;DR: 提出R²PO方法，通过引入轻量级残差rollout头来解耦训练轨迹与推理响应，解决现有强化学习方法中单一策略同时负责推理响应和训练优化轨迹导致的探索不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法使用单一策略同时生成推理响应和训练优化轨迹，导致生成稳定推理响应与多样化训练轨迹之间的目标冲突，造成探索不足，损害推理能力。

Method: 提出R²PO（残差rollout策略优化），在策略之上引入轻量级残差rollout头，将训练轨迹与推理响应解耦，在训练期间实现可控的轨迹多样化，同时保持推理生成的稳定性。

Result: 在多个基准测试中一致优于基线方法，在MATH-500上平均准确率提升3.1%，在APPS上提升2.4%，同时减少格式错误并缓解长度偏差以实现稳定优化。

Conclusion: R²PO通过解耦训练轨迹与推理响应，有效解决了强化学习在LLM推理中的探索不足问题，显著提升了推理能力，同时保持了推理稳定性。

Abstract: Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.

</details>


### [110] [One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints](https://arxiv.org/abs/2601.11977)
*Ren He,Yinliang Xu,Jinfeng Wang,Jeremy Watson,Jian Song*

Main category: cs.LG

TL;DR: 提出MoE-Encoder模块，通过稀疏专家混合层增强预训练时序模型，解决电力系统多变量预测中的隐私约束和跨区域泛化问题


<details>
  <summary>Details</summary>
Motivation: 电力系统预测面临多变量复杂依赖、严格隐私约束和跨区域部署的挑战。传统方法需要大量专家知识且泛化能力有限，现有预训练模型的零样本性能在领域特定任务上仍有不足。

Method: 在预训练预测模型的tokenization和encoding之间注入稀疏专家混合层，将多变量预测转化为专家引导的单变量任务，支持联邦学习环境下的本地化训练和轻量级参数共享。

Result: 在公开多变量数据集上，MoE-Encoder显著提升了预测精度。联邦环境模拟显示，仅传输MoE-Encoder参数即可高效适应新区域，性能下降最小。

Conclusion: MoE-Encoder为时序基础模型提供了可扩展且隐私感知的扩展方案，能有效处理电力系统中的复杂依赖和隐私约束问题。

Abstract: Forecasting in power systems often involves multivariate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, we propose a novel MoE Encoder module that augments pretrained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: (1) trans forming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and (2) supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. We further simulate federated environments and show that transferring only MoE-Encoder parameters allows efficient adaptation to new regions, with minimal performance degradation. Our findings suggest that MoE-Encoder provides a scalable and privacy-aware extension to foundation time series models.

</details>


### [111] [Extreme Value Policy Optimization for Safe Reinforcement Learning](https://arxiv.org/abs/2601.12008)
*Shiqing Gao,Yihang Zhou,Shuai Shao,Haoyu Luo,Yiheng Bing,Jiaxin Ding,Luoyi Fu,Xinbing Wang*

Main category: cs.LG

TL;DR: 提出EVO算法，利用极值理论处理强化学习中的极端风险事件，减少约束违反


<details>
  <summary>Details</summary>
Motivation: 现有约束强化学习基于期望约束，忽略了尾部分布中的罕见但高影响的极端值事件（如黑天鹅事件），可能导致严重的约束违反

Method: 提出极值策略优化（EVO）算法：1）利用极值理论建模极端奖励和成本样本；2）引入极端分位数优化目标捕获成本尾部分布；3）提出回放过程中的极端优先级机制，放大罕见但高影响样本的学习信号

Result: 理论上建立了策略更新期间预期约束违反的上界，保证在零违反分位数水平的严格约束满足；实验表明EVO显著减少训练期间的约束违反，同时保持与基线相当的策略性能

Conclusion: EVO算法通过建模极端事件有效解决了约束强化学习中的尾部风险问题，比基于期望的方法具有更低的约束违反概率，比分位数回归方法具有更低的方差

Abstract: Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.

</details>


### [112] [Why Loss Re-weighting Works If You Stop Early: Training Dynamics of Unconstrained Features](https://arxiv.org/abs/2601.12011)
*Yize Zhao,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 损失重加权在过参数化DNN中无法改变最终学习阶段，但在训练早期显著有益。作者引入小规模模型(SSM)来透明分析这一现象，发现标准ERM早期偏好学习多数类特征，而重加权能恢复平衡学习动态。


<details>
  <summary>Details</summary>
Motivation: 损失重加权在现代深度学习中的应用呈现复杂图景：虽然无法改变过参数化DNN在训练后期的最终学习阶段，但实证证据一致表明它在训练早期提供显著益处。需要透明地展示和分析这一现象。

Method: 引入小规模模型(SSM)，该模型专门设计用于抽象DNN架构和输入数据的固有复杂性，同时保持其频谱分量中不平衡结构的关键信息。通过SSM分析标准经验风险最小化(ERM)与重加权方法的学习动态差异。

Result: SSM揭示：1) 标准ERM在训练早期优先学习区分多数类而非少数类，从而延迟少数类学习；2) 重加权恢复平衡学习动态，使与多数类和少数类相关的特征能够同时学习。

Conclusion: 损失重加权通过恢复训练早期的平衡学习动态而发挥作用，尽管在过参数化DNN的最终学习阶段无法改变结果。SSM为理解这一现象提供了透明分析框架。

Abstract: The application of loss reweighting in modern deep learning presents a nuanced picture. While it fails to alter the terminal learning phase in overparameterized deep neural networks (DNNs) trained on high-dimensional datasets, empirical evidence consistently shows it offers significant benefits early in training. To transparently demonstrate and analyze this phenomenon, we introduce a small-scale model (SSM). This model is specifically designed to abstract the inherent complexities of both the DNN architecture and the input data, while maintaining key information about the structure of imbalance within its spectral components. On the one hand, the SSM reveals how vanilla empirical risk minimization preferentially learns to distinguish majority classes over minorities early in training, consequently delaying minority learning. In stark contrast, reweighting restores balanced learning dynamics, enabling the simultaneous learning of features associated with both majorities and minorities.

</details>


### [113] [Learning to Factorize and Adapt: A Versatile Approach Toward Universal Spatio-Temporal Foundation Models](https://arxiv.org/abs/2601.12083)
*Siru Zhong,Junjie Qiu,Yangyu Wu,Yiqiu Liu,Yuanpeng He,Zhongwen Rao,Bin Yang,Chenjuan Guo,Hao Xu,Yuxuan Liang*

Main category: cs.LG

TL;DR: FactoST-v2提出了一种分解的时空基础模型框架，通过分离通用时间学习和领域特定空间适应，实现了高效的全权重迁移和任意长度泛化。


<details>
  <summary>Details</summary>
Motivation: 时空基础模型虽然具有跨数据集泛化潜力，但联合时空预训练计算成本高，且难以处理领域特定的空间模式异质性。需要一种更实用、可扩展的解决方案。

Method: 采用两阶段分解框架：第一阶段预训练仅编码器骨干网络，使用随机序列掩码捕获不变时间动态；第二阶段通过轻量适配器注入空间感知，使用元自适应学习和提示机制。

Result: 在多个领域评估中，FactoST-v2在零样本和少样本场景下达到最先进精度，线性效率显著优于现有基础模型，媲美领域特定专家基线。

Conclusion: 这种分解范式为构建真正通用的时空基础模型提供了实用且可扩展的路径，实现了高效的全权重迁移和任意长度泛化能力。

Abstract: Spatio-Temporal (ST) Foundation Models (STFMs) promise cross-dataset generalization, yet joint ST pretraining is computationally expensive and grapples with the heterogeneity of domain-specific spatial patterns. Substantially extending our preliminary conference version, we present FactoST-v2, an enhanced factorized framework redesigned for full weight transfer and arbitrary-length generalization. FactoST-v2 decouples universal temporal learning from domain-specific spatial adaptation. The first stage pretrains a minimalist encoder-only backbone using randomized sequence masking to capture invariant temporal dynamics, enabling probabilistic quantile prediction across variable horizons. The second stage employs a streamlined adapter to rapidly inject spatial awareness via meta adaptive learning and prompting. Comprehensive evaluations across diverse domains demonstrate that FactoST-v2 achieves state-of-the-art accuracy with linear efficiency - significantly outperforming existing foundation models in zero-shot and few-shot scenarios while rivaling domain-specific expert baselines. This factorized paradigm offers a practical, scalable path toward truly universal STFMs. Code is available at https://github.com/CityMind-Lab/FactoST.

</details>


### [114] [Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate](https://arxiv.org/abs/2601.12091)
*Qian Tan,Lei Jiang,Yuting Zeng,Shuoyang Ding,Xiaohua Xu*

Main category: cs.LG

TL;DR: 本文提出CEBiasBench中英双语基准和Multi-Agent Vote评估框架，发现中文提示仅将偏见转向东亚视角而非消除偏见，并提出Multi-Agent Cultural Debate训练免费框架来缓解跨文化偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在系统性西方中心偏见，但使用非西方语言（如中文）提示是否能缓解这种偏见尚未充分研究。现有方法在评估和缓解两方面都存在不足：评估方法强制输出到预定义文化类别而无中立选项，缓解方法依赖昂贵的多文化语料库或缺乏明确文化表征的代理框架。

Method: 1) 引入CEBiasBench中英双语基准和Multi-Agent Vote框架，支持明确的"无偏见"判断；2) 提出Multi-Agent Cultural Debate框架，为代理分配不同文化角色，采用"求同存异"策略进行审议。

Result: 实验表明：1) 中文提示仅将偏见转向东亚视角而非消除偏见；2) MACD在CEBiasBench上达到57.6%的平均无偏见率（LLM-as-judge评估）和86.0%（MAV评估），相比GPT-4o基线的47.6%和69.0%有显著提升；3) MACD能泛化到阿拉伯语CAMeL基准。

Conclusion: 代理框架中明确的文化表征对于跨文化公平至关重要，Multi-Agent Cultural Debate通过训练免费的方式有效缓解了语言模型中的文化偏见。

Abstract: Large language models (LLMs) exhibit systematic Western-centric bias, yet whether prompting in non-Western languages (e.g., Chinese) can mitigate this remains understudied. Answering this question requires rigorous evaluation and effective mitigation, but existing approaches fall short on both fronts: evaluation methods force outputs into predefined cultural categories without a neutral option, while mitigation relies on expensive multi-cultural corpora or agent frameworks that use functional roles (e.g., Planner--Critique) lacking explicit cultural representation. To address these gaps, we introduce CEBiasBench, a Chinese--English bilingual benchmark, and Multi-Agent Vote (MAV), which enables explicit ``no bias'' judgments. Using this framework, we find that Chinese prompting merely shifts bias toward East Asian perspectives rather than eliminating it. To mitigate such persistent bias, we propose Multi-Agent Cultural Debate (MACD), a training-free framework that assigns agents distinct cultural personas and orchestrates deliberation via a "Seeking Common Ground while Reserving Differences" strategy. Experiments demonstrate that MACD achieves 57.6% average No Bias Rate evaluated by LLM-as-judge and 86.0% evaluated by MAV (vs. 47.6% and 69.0% baseline using GPT-4o as backbone) on CEBiasBench and generalizes to the Arabic CAMeL benchmark, confirming that explicit cultural representation in agent frameworks is essential for cross-cultural fairness.

</details>


### [115] [PTL-PINNs: Perturbation-Guided Transfer Learning with Physics- Informed Neural Networks for Nonlinear Systems](https://arxiv.org/abs/2601.12093)
*Duarte Alexandrino,Ben Moseley,Pavlos Protopapas*

Main category: cs.LG

TL;DR: 提出PTL-PINN框架，结合微扰理论和迁移学习，快速求解非线性微分方程，速度比传统方法快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络(PINNs)在求解非线性微分方程时存在泛化能力有限和训练时间长的问题，需要更高效的解决方案。

Method: 提出PTL-PINN框架，将微扰理论与迁移学习结合，通过求解近似线性微扰系统（使用闭式表达式），实现快速泛化，时间复杂度仅为矩阵向量乘法级别。

Result: PTL-PINNs达到与多种Runge-Kutta方法相当的精度，计算速度比传统方法快一个数量级，成功求解了非线性振荡器、Lotka-Volterra系统、KPP-Fisher方程和波动方程等多种问题。

Conclusion: 该工作将长期存在的微扰方法与PINNs连接起来，展示了微扰理论如何指导基础模型以接近经典求解器的速度求解非线性系统，为高效求解非线性微分方程提供了新思路。

Abstract: Accurately and efficiently solving nonlinear differential equations is crucial for modeling dynamic behavior across science and engineering. Physics-Informed Neural Networks (PINNs) have emerged as a powerful solution that embeds physical laws in training by enforcing equation residuals. However, these struggle to model nonlinear dynamics, suffering from limited generalization across problems and long training times. To address these limitations, we propose a perturbation-guided transfer learning framework for PINNs (PTL-PINN), which integrates perturbation theory with transfer learning to efficiently solve nonlinear equations. Unlike gradient-based transfer learning, PTL-PINNs solve an approximate linear perturbative system using closed-form expressions, enabling rapid generalization with the time complexity of matrix-vector multiplication. We show that PTL-PINNs achieve accuracy comparable to various Runge-Kutta methods, with computational speeds up to one order of magnitude faster. To benchmark performance, we solve a broad set of problems, including nonlinear oscillators across various damping regimes, the equilibrium-centered Lotka-Volterra system, the KPP-Fisher and the Wave equation. Since perturbation theory sets the accuracy bound of PTL-PINNs, we systematically evaluate its practical applicability. This work connects long-standing perturbation methods with PINNs, demonstrating how perturbation theory can guide foundational models to solve nonlinear systems with speeds comparable to those of classical solvers.

</details>


### [116] [Neural Isomorphic Fields: A Transformer-based Algebraic Numerical Embedding](https://arxiv.org/abs/2601.12095)
*Hamidreza Sadeghi,Saeedeh Momtazi,Reza Safabakhsh*

Main category: cs.LG

TL;DR: 提出固定长度数字嵌入向量，保留有理数代数运算特性，通过神经同构场解决神经网络处理极值数字时的数值不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络在处理极小或极大数值时面临溢出、下溢和输出不稳定等问题，需要一种能保留代数特性同时避免数值不稳定的数字表示方法。

Method: 提出固定长度的数字嵌入向量表示，引入神经同构场作为代数结构（群、域）的神经抽象，嵌入向量在计算中保持代数结构。

Result: 加法运算表现优异，在恒等性、封闭性、结合性等关键代数测试中准确率超过95%；乘法运算面临挑战，在不同代数特性上准确率在53%-73%之间。

Conclusion: 模型在保持加法运算的代数特性方面表现出色，但在处理乘法运算方面仍有改进空间，为数字嵌入和代数结构保留提供了新的研究方向。

Abstract: Neural network models often face challenges when processing very small or very large numbers due to issues such as overflow, underflow, and unstable output variations. To mitigate these problems, we propose using embedding vectors for numbers instead of directly using their raw values. These embeddings aim to retain essential algebraic properties while preventing numerical instabilities. In this paper, we introduce, for the first time, a fixed-length number embedding vector that preserves algebraic operations, including addition, multiplication, and comparison, within the field of rational numbers. We propose a novel Neural Isomorphic Field, a neural abstraction of algebraic structures such as groups and fields. The elements of this neural field are embedding vectors that maintain algebraic structure during computations. Our experiments demonstrate that addition performs exceptionally well, achieving over 95 percent accuracy on key algebraic tests such as identity, closure, and associativity. In contrast, multiplication exhibits challenges, with accuracy ranging from 53 percent to 73 percent across various algebraic properties. These findings highlight the model's strengths in preserving algebraic properties under addition while identifying avenues for further improvement in handling multiplication.

</details>


### [117] [SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data](https://arxiv.org/abs/2601.12124)
*Bing Hu,Yixin Li,Asma Bahamyirou,Helen Chen*

Main category: cs.LG

TL;DR: SynQP是一个用于评估合成数据隐私风险的开源框架，使用模拟敏感数据避免真实数据泄露，并提出了更准确的隐私度量指标。


<details>
  <summary>Details</summary>
Motivation: 健康应用中合成数据使用存在隐私担忧，但缺乏开放的隐私评估框架和可访问的基准数据集，阻碍了其采用。

Method: 开发SynQP开源框架，使用模拟敏感数据进行隐私基准测试；提出新的身份披露风险度量指标，更准确地评估隐私风险；以CTGAN为例进行基准测试。

Result: 非私有模型在机器学习效能上达到≥0.97；差分隐私（DP）能有效降低身份披露风险和成员推理攻击风险，所有DP增强模型均低于0.09的监管阈值。

Conclusion: SynQP为隐私评估提供了关键工具，提高了透明度和可靠性，使合成数据在健康相关应用中更安全地使用。

Abstract: The use of synthetic data in health applications raises privacy concerns, yet the lack of open frameworks for privacy evaluations has slowed its adoption. A major challenge is the absence of accessible benchmark datasets for evaluating privacy risks, due to difficulties in acquiring sensitive data. To address this, we introduce SynQP, an open framework for benchmarking privacy in synthetic data generation (SDG) using simulated sensitive data, ensuring that original data remains confidential. We also highlight the need for privacy metrics that fairly account for the probabilistic nature of machine learning models. As a demonstration, we use SynQP to benchmark CTGAN and propose a new identity disclosure risk metric that offers a more accurate estimation of privacy risks compared to existing approaches. Our work provides a critical tool for improving the transparency and reliability of privacy evaluations, enabling safer use of synthetic data in health-related applications. % In our quality evaluations, non-private models achieved near-perfect machine-learning efficacy \(\ge0.97\). Our privacy assessments (Table II) reveal that DP consistently lowers both identity disclosure risk (SD-IDR) and membership-inference attack risk (SD-MIA), with all DP-augmented models staying below the 0.09 regulatory threshold. Code available at https://github.com/CAN-SYNH/SynQP

</details>


### [118] [SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics](https://arxiv.org/abs/2601.12131)
*Santosh Chapagain,MohammadReza EskandariNasab,Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: SolarGPT-QA是基于LLaMA-3构建的领域自适应大语言模型问答系统，专门用于空间天气和太阳物理学教育，通过科学文献和GPT-4生成的数据训练，在零样本设置下优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 太阳活动（太阳耀斑、日冕物质抛射等）对卫星、电网等关键基础设施有重大影响，极端事件可能造成巨大经济损失。当前通用大语言模型缺乏领域专业知识，且缺乏清晰解释复杂空间科学概念的教学能力。

Method: 基于LLaMA-3基础模型构建SolarGPT-QA问答系统，使用科学文献和大规模问答数据进行领域自适应预训练，数据由GPT-4生成并通过Grok-3以学生友好的故事化风格精炼。结合领域自适应预训练和教学微调。

Result: 人工成对评估显示，SolarGPT-QA在零样本设置下优于通用模型，在教学解释方面与指令微调模型竞争。小型试点学生理解研究表明生成解释的清晰度和可访问性有所改善。消融实验表明结合领域自适应预训练和教学微调对平衡科学准确性和教学效果很重要。

Conclusion: SolarGPT-QA在空间天气和太阳物理学教育解释方面表现出色，是迈向更广泛的SolarGPT空间科学教育和预报框架的初步步骤，展示了领域自适应与教学微调结合的价值。

Abstract: Solar activity, including solar flares, coronal mass ejections (CMEs), and geomagnetic storms, can significantly impact satellites, aviation, power grids, data centers, and space missions. Extreme solar events can cause substantial economic damage if not predicted in advance, highlighting the importance of accurate forecasting and effective education in space science. Although large language models (LLMs) perform well on general tasks, they often lack domain-specific knowledge and pedagogical capability to clearly explain complex space science concepts.
  We introduce SolarGPT-QA, a question answering system based on a domain-adapted large language model built on the LLaMA-3 base model. The model is trained using scientific literature and large-scale question-answer data generated with GPT-4 and refined using Grok-3 in a student-friendly storytelling style. Human pairwise evaluations show that SolarGPT-QA outperforms general-purpose models in zero-shot settings and achieves competitive performance compared to instruction-tuned models for educational explanations in space weather and heliophysics. A small pilot student comprehension study further suggests improved clarity and accessibility of the generated explanations. Ablation experiments indicate that combining domain-adaptive pretraining with pedagogical fine-tuning is important for balancing scientific accuracy and educational effectiveness. This work represents an initial step toward a broader SolarGPT framework for space science education and forecasting.

</details>


### [119] [EMoE: Eigenbasis-Guided Routing for Mixture-of-Experts](https://arxiv.org/abs/2601.12137)
*Anzhe Cheng,Shukai Duan,Shixuan Li,Chenzhong Yin,Mingxi Cheng,Shahin Nazarian,Paul Thompson,Paul Bogdan*

Main category: cs.LG

TL;DR: EMoE提出基于正交特征基的路由机制，通过将输入token投影到共享特征空间的主成分上，实现负载均衡和专家专业化，无需额外的负载均衡损失函数。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能提高效率，但面临两个核心问题：1) "富者愈富"的负载不均衡问题，少数专家被过度使用；2) 专家同质化问题，专家学习冗余表示，失去了专业化意义。现有解决方案通常使用辅助负载均衡损失，但这会加剧同质化问题。

Method: 提出Eigen-Mixture-of-Experts (EMoE)架构，采用基于学习正交特征基的路由机制。该方法将输入token投影到共享特征空间的正交基上，根据token与主成分的对齐程度进行路由，实现数据的几何划分。

Result: EMoE能够同时促进专家负载均衡和专业化发展，无需使用冲突的辅助损失函数。代码已公开在GitHub上。

Conclusion: EMoE通过基于正交特征基的路由机制，从根本上解决了MoE架构中的负载不均衡和专家同质化问题，为高效深度学习模型提供了新的解决方案。

Abstract: The relentless scaling of deep learning models has led to unsustainable computational demands, positioning Mixture-of-Experts (MoE) architectures as a promising path towards greater efficiency. However, MoE models are plagued by two fundamental challenges: 1) a load imbalance problem known as the``rich get richer" phenomenon, where a few experts are over-utilized, and 2) an expert homogeneity problem, where experts learn redundant representations, negating their purpose. Current solutions typically employ an auxiliary load-balancing loss that, while mitigating imbalance, often exacerbates homogeneity by enforcing uniform routing at the expense of specialization. To resolve this, we introduce the Eigen-Mixture-of-Experts (EMoE), a novel architecture that leverages a routing mechanism based on a learned orthonormal eigenbasis. EMoE projects input tokens onto this shared eigenbasis and routes them based on their alignment with the principal components of the feature space. This principled, geometric partitioning of data intrinsically promotes both balanced expert utilization and the development of diverse, specialized experts, all without the need for a conflicting auxiliary loss function. Our code is publicly available at https://github.com/Belis0811/EMoE.

</details>


### [120] [Threshold Differential Attention for Sink-Free, Ultra-Sparse, and Non-Dispersive Language Modeling](https://arxiv.org/abs/2601.12145)
*Xingyue Huang,Xueying Ding,Mingxuan Ju,Yozen Liu,Neil Shah,Tong Zhao*

Main category: cs.LG

TL;DR: TDA是一种无注意力沉没的注意力机制，通过阈值差分方法实现超稀疏性，在长上下文任务中表现优异，产生超过99%的精确零值。


<details>
  <summary>Details</summary>
Motivation: 解决Softmax注意力在长上下文中的结构限制：严格的归一化约束导致注意力沉没在无关令牌上，随着序列长度增加，概率质量分散。

Method: 提出阈值差分注意力（TDA）：采用行级极值阈值化配合长度相关门控，只保留超过阈值的值；借鉴差分变换器思想，减去抑制视图以增强表达能力。

Result: 理论上证明TDA每行虚假幸存者期望为O(1)，独立视图间的虚假匹配随上下文增长而消失；实证显示产生>99%精确零值，消除注意力沉没，在标准及长上下文基准上保持竞争力。

Conclusion: TDA解决了Softmax注意力的长上下文限制，实现了超稀疏、无沉没的注意力机制，在计算效率和性能上优于现有方法。

Abstract: Softmax attention struggles with long contexts due to structural limitations: the strict sum-to-one constraint forces attention sinks on irrelevant tokens, and probability mass disperses as sequence lengths increase. We tackle these problems with Threshold Differential Attention (TDA), a sink-free attention mechanism that achieves ultra-sparsity and improved robustness at longer sequence lengths without the computational overhead of projection methods or the performance degradation caused by noise accumulation of standard rectified attention. TDA applies row-wise extreme-value thresholding with a length-dependent gate, retaining only exceedances. Inspired by the differential transformer, TDA also subtracts an inhibitory view to enhance expressivity. Theoretically, we prove that TDA controls the expected number of spurious survivors per row to $O(1)$ and that consensus spurious matches across independent views vanish as context grows. Empirically, TDA produces $>99\%$ exact zeros and eliminates attention sinks while maintaining competitive performance on standard and long-context benchmarks.

</details>


### [121] [Speculative Sampling with Reinforcement Learning](https://arxiv.org/abs/2601.12212)
*Chenan Wang,Daniel H. Shi,Haipeng Chen*

Main category: cs.LG

TL;DR: 提出Re-SpS框架，使用强化学习动态优化推测采样的树结构超参数，相比静态超参数的EAGLE-3方法，在保持输出质量的同时获得最高1.12倍加速


<details>
  <summary>Details</summary>
Motivation: 现有推测采样方法（如EAGLE-3）使用静态树结构超参数，限制了在不同上下文和领域中的灵活性和效率。需要动态调整超参数来平衡推测的激进程度与计算开销。

Method: 提出基于强化学习的推测采样框架Re-SpS，利用目标模型隐藏状态构建高效状态表示，引入多步动作持久化以更好地建模上下文，实时动态调整草稿树超参数。

Result: 在五个多样化基准测试中，相比骨干LLM获得最高5.45倍加速，相比SOTA方法EAGLE-3获得最高1.12倍加速，且没有输出质量损失。

Conclusion: Re-SpS通过强化学习动态优化推测采样超参数，显著提升了LLM推理速度，为实时应用提供了更高效的解决方案。

Abstract: Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\times$ speedup over the backbone LLM and up to 1.12$\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.

</details>


### [122] [Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks](https://arxiv.org/abs/2601.12662)
*Xingran Chen,Navid NaderiAlizadeh,Alejandro Ribeiro,Shirin Saeedi Bidokhti*

Main category: cs.LG

TL;DR: 提出基于图多智能体强化学习的框架，用于优化无线网络中自回归马尔可夫源的实时采样与估计策略，该策略具有可迁移性且能应对非平稳性。


<details>
  <summary>Details</summary>
Motivation: 在多跳无线网络中，由于动作空间维度高、网络拓扑复杂，难以通过解析方法推导最优的实时采样与估计策略，需要一种有效的优化方法。

Method: 采用图多智能体强化学习框架进行策略优化，利用网络的结构相似性，使在一个图上训练的策略能够迁移到其他结构相似的图上。

Result: 提出的策略优于现有基线方法；训练的策略可迁移到更大网络，且性能增益随智能体数量增加；图形训练过程能承受非平稳性；循环机制对独立学习和集中训练分散执行都至关重要。

Conclusion: 图多智能体强化学习框架能有效解决复杂无线网络中的实时采样与估计问题，所提出的策略具有可迁移性和对非平稳性的鲁棒性，为分布式网络优化提供了新思路。

Abstract: We address real-time sampling and estimation of autoregressive Markovian sources in dynamic yet structurally similar multi-hop wireless networks. Each node caches samples from others and communicates over wireless collision channels, aiming to minimize time-average estimation error via decentralized policies. Due to the high dimensionality of action spaces and complexity of network topologies, deriving optimal policies analytically is intractable. To address this, we propose a graphical multi-agent reinforcement learning framework for policy optimization. Theoretically, we demonstrate that our proposed policies are transferable, allowing a policy trained on one graph to be effectively applied to structurally similar graphs. Numerical experiments demonstrate that (i) our proposed policy outperforms state-of-the-art baselines; (ii) the trained policies are transferable to larger networks, with performance gains increasing with the number of agents; (iii) the graphical training procedure withstands non-stationarity, even when using independent learning techniques; and (iv) recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity.

</details>


### [123] [Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models](https://arxiv.org/abs/2601.12215)
*Megha Thukral,Cyrus Tanade,Simon A. Lee,Juhyeon Lee,Hao Zhou,Keum San Chun,Migyeong Gwak,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Mehrab Bin Morshed,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 提出MMR框架，通过小波多分辨率分解和掩码重建任务，从大规模PPG信号中学习跨时间-频率尺度的表征，在17/19个健康相关任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴基础模型大多忽略PPG信号的频谱结构，而许多健康相关任务需要从细粒度波形形态到全局节律动态的多分辨率特征。

Method: 提出掩码多分辨率重建(MMR)自监督预训练框架：使用小波多分辨率分解PPG信号，随机掩码系数后让Transformer编码器重建，强制模型整合跨时间和频谱尺度的信息。

Result: 使用约1700万个未标记的10秒PPG片段（来自约3.2万智能手表用户）预训练，在19个多样化健康相关任务中的17个上优于或匹配最先进的开源PPG基础模型、时间序列基础模型和其他自监督基线。

Conclusion: MMR展示了作为通用PPG基础模型的潜力，小波表示能捕获稳健且具有生理基础的特征，为可穿戴健康监测提供了有效的表征学习方法。

Abstract: Wearable foundation models have the potential to transform digital health by learning transferable representations from large-scale biosignals collected in everyday settings. While recent progress has been made in large-scale pretraining, most approaches overlook the spectral structure of photoplethysmography (PPG) signals, wherein physiological rhythms unfold across multiple frequency bands. Motivated by the insight that many downstream health-related tasks depend on multi-resolution features spanning fine-grained waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale Reconstruction (MMR) for PPG representation learning - a self-supervised pretraining framework that explicitly learns from hierarchical time-frequency scales of PPG data. The pretraining task is designed to reconstruct randomly masked out coefficients obtained from a wavelet-based multiresolution decomposition of PPG signals, forcing the transformer encoder to integrate information across temporal and spectral scales. We pretrain our model with MMR using ~17 million unlabeled 10-second PPG segments from ~32,000 smartwatch users. On 17 of 19 diverse health-related tasks, MMR trained on large-scale wearable PPG data improves over or matches state-of-the-art open-source PPG foundation models, time-series foundation models, and other self-supervised baselines. Extensive analysis of our learned embeddings and systematic ablations underscores the value of wavelet-based representations, showing that they capture robust and physiologically-grounded features. Together, these results highlight the potential of MMR as a step toward generalizable PPG foundation models.

</details>


### [124] [Learning Longitudinal Health Representations from EHR and Wearable Data](https://arxiv.org/abs/2601.12227)
*Yuanyun Zhang,Han Zhou,Li Feng,Yilin Hong,Shi Li*

Main category: cs.LG

TL;DR: 提出一个多模态基础模型，联合表示电子健康记录和可穿戴设备数据作为连续时间潜在过程，在生理预测和风险建模任务上优于单模态基线


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据稀疏且不规则，可穿戴设备提供密集连续生理信号但缺乏语义基础，现有方法通常单独建模或通过后期融合组合这些数据源

Method: 使用模态特定编码器和共享时间骨干网络，通过自监督和跨模态目标进行预训练，将电子健康记录和可穿戴数据联合表示为连续时间潜在过程

Result: 在预测生理和风险建模任务中，模型优于仅使用电子健康记录或仅使用可穿戴设备的基线，特别是在长时程预测和缺失数据情况下表现更好

Conclusion: 联合电子健康记录和可穿戴设备预训练能够产生更准确的纵向健康表示，证明多模态整合的优势

Abstract: Foundation models trained on electronic health records show strong performance on many clinical prediction tasks but are limited by sparse and irregular documentation. Wearable devices provide dense continuous physiological signals but lack semantic grounding. Existing methods usually model these data sources separately or combine them through late fusion. We propose a multimodal foundation model that jointly represents electronic health records and wearable data as a continuous time latent process. The model uses modality specific encoders and a shared temporal backbone pretrained with self supervised and cross modal objectives. This design produces representations that are temporally coherent and clinically grounded. Across forecasting physiological and risk modeling tasks the model outperforms strong electronic health record only and wearable only baselines especially at long horizons and under missing data. These results show that joint electronic health record and wearable pretraining yields more faithful representations of longitudinal health.

</details>


### [125] [Wavelet-Aware Anomaly Detection in Multi-Channel User Logs via Deviation Modulation and Resolution-Adaptive Attention](https://arxiv.org/abs/2601.12231)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Shijie Xu,Guanggang Geng*

Main category: cs.LG

TL;DR: 提出一种结合小波感知调制、多分辨率小波分解和分辨率自适应注意力的新型框架，用于企业安全中的内部威胁检测，在CERT基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内部威胁检测面临多通道、非平稳的用户活动日志数据，且异常事件稀少，传统方法难以有效处理这些复杂的行为模式。

Method: 1) 偏差感知调制方案抑制常规行为并放大异常偏差；2) 离散小波变换将日志信号分解为多分辨率表示；3) 可学习的注意力机制动态重加权最具区分性的频带。

Result: 在CERT r4.2基准测试中，该方法在不同时间粒度和场景下，在精确率、召回率和F1分数方面均优于现有基线方法。

Conclusion: 提出的集成小波感知调制、多分辨率分解和自适应注意力的框架能够有效处理复杂的企业安全日志数据，显著提升内部威胁检测性能。

Abstract: Insider threat detection is a key challenge in enterprise security, relying on user activity logs that capture rich and complex behavioral patterns. These logs are often multi-channel, non-stationary, and anomalies are rare, making anomaly detection challenging. To address these issues, we propose a novel framework that integrates wavelet-aware modulation, multi-resolution wavelet decomposition, and resolution-adaptive attention for robust anomaly detection. Our approach first applies a deviation-aware modulation scheme to suppress routine behaviors while amplifying anomalous deviations. Next, discrete wavelet transform (DWT) decomposes the log signals into multi-resolution representations, capturing both long-term trends and short-term anomalies. Finally, a learnable attention mechanism dynamically reweights the most discriminative frequency bands for detection. On the CERT r4.2 benchmark, our approach consistently outperforms existing baselines in precision, recall, and F1 score across various time granularities and scenarios.

</details>


### [126] [TimeGMM: Single-Pass Probabilistic Forecasting via Adaptive Gaussian Mixture Models with Reversible Normalization](https://arxiv.org/abs/2601.12288)
*Lei Liu,Tengyuan Liu,Hongwei Zhao,Jiahui Huang,Ruibo Guo,Bin Li*

Main category: cs.LG

TL;DR: TimeGMM：基于高斯混合模型的概率时间序列预测框架，通过GRIN模块动态适应时间-概率分布漂移，在单次前向传播中捕捉复杂未来分布


<details>
  <summary>Details</summary>
Motivation: 现有概率时间序列预测方法存在两个主要问题：1）依赖计算昂贵的采样方法；2）使用限制性参数假设来表征未来分布，这限制了预测性能并导致分布不匹配

Method: 提出TimeGMM框架，基于高斯混合模型，包含GRIN模块（GMM适应的可逆实例归一化）动态适应分布漂移，集成时间编码器（TE-Module）和条件时间-概率解码器（CTPD-Module）联合捕捉时间依赖性和混合分布参数

Result: 在广泛实验中，TimeGMM持续优于最先进方法，在CRPS指标上最大提升22.48%，在NMAE指标上最大提升21.23%

Conclusion: TimeGMM通过高斯混合模型框架有效解决了现有概率时间序列预测方法的局限性，在单次前向传播中捕捉复杂分布，显著提升了预测性能

Abstract: Probabilistic time series forecasting is crucial for quantifying future uncertainty, with significant applications in fields such as energy and finance. However, existing methods often rely on computationally expensive sampling or restrictive parametric assumptions to characterize future distributions, which limits predictive performance and introduces distributional mismatch. To address these challenges, this paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. A key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48\% in CRPS and 21.23\% in NMAE.

</details>


### [127] [Distribution Shift Is Key to Learning Invariant Prediction](https://arxiv.org/abs/2601.12296)
*Hong Zheng,Fei Teng*

Main category: cs.LG

TL;DR: 研究发现，训练数据中的分布偏移程度越大，即使使用简单的经验风险最小化（ERM）方法，也能获得接近不变预测模型的性能，有时甚至优于专门设计的OOD方法。


<details>
  <summary>Details</summary>
Motivation: 观察到经验风险最小化（ERM）有时在分布外任务上表现优于专门设计的方法，这促使研究者探究算法设计之外的原因，特别是训练域间的分布偏移对模型性能的影响。

Method: 通过理论推导和实证验证相结合的方法：1）提出理论上界表明分布偏移程度直接影响模型预测能力；2）证明在特定数据条件下，ERM解能达到不变预测模型的性能；3）通过实验验证当训练数据分布偏移增加时，学习模型的预测接近Oracle或最优模型。

Result: 研究发现：1）分布偏移程度越大，模型预测能力越强，越能近似不变预测模型；2）在特定条件下，ERM能达到与不变预测模型相当的性能；3）实证表明训练数据分布偏移增加时，学习模型的预测确实接近Oracle或最优模型。

Conclusion: 训练域间的分布偏移是影响模型学习不变预测的关键因素，较大的分布偏移能使ERM等简单方法获得接近专门设计的OOD方法的性能，这为理解算法在分布外任务上的表现提供了新视角。

Abstract: An interesting phenomenon arises: Empirical Risk Minimization (ERM) sometimes outperforms methods specifically designed for out-of-distribution tasks. This motivates an investigation into the reasons behind such behavior beyond algorithmic design. In this study, we find that one such reason lies in the distribution shift across training domains. A large degree of distribution shift can lead to better performance even under ERM. Specifically, we derive several theoretical and empirical findings demonstrating that distribution shift plays a crucial role in model learning and benefits learning invariant prediction. Firstly, the proposed upper bounds indicate that the degree of distribution shift directly affects the prediction ability of the learned models. If it is large, the models' ability can increase, approximating invariant prediction models that make stable predictions under arbitrary known or unseen domains; and vice versa. We also prove that, under certain data conditions, ERM solutions can achieve performance comparable to that of invariant prediction models. Secondly, the empirical validation results demonstrated that the predictions of learned models approximate those of Oracle or Optimal models, provided that the degree of distribution shift in the training data increases.

</details>


### [128] [Machine Learning as a Service (MLaaS) Dataset Generator Framework for IoT Environments](https://arxiv.org/abs/2601.12305)
*Deepak Kanneganti,Sajib Mistry,Sheik Fattah,Joshua Boland,Aneesh Krishna*

Main category: cs.LG

TL;DR: 提出MLaaS数据集生成器(MDG)框架，用于创建可配置、可复现的数据集来评估MLaaS服务选择和组合，通过模拟真实MLaaS行为生成大规模基准数据集。


<details>
  <summary>Details</summary>
Motivation: 需要系统化评估机器学习即服务(MLaaS)选择和组合的方法，现有方法缺乏可配置、可复现的数据集来模拟真实MLaaS行为和服务交互。

Method: MDG框架通过训练和评估多种模型家族，跨多个真实数据集和数据分布设置来模拟MLaaS行为，记录功能属性、服务质量指标和组合特定指标，并内置组合机制模拟物联网条件下的服务交互。

Result: 生成了超过一万个MLaaS服务实例，构建了适用于下游评估的大规模基准数据集，实验表明MDG生成的数据集相比现有基线提高了选择准确性和组合质量。

Conclusion: MDG为推进MLaaS选择和组合的数据驱动研究提供了实用且可扩展的基础，能够生成高质量、可配置的数据集来系统评估服务性能。

Abstract: We propose a novel MLaaS Dataset Generator (MDG) framework that creates configurable and reproducible datasets for evaluating Machine Learning as a Service (MLaaS) selection and composition. MDG simulates realistic MLaaS behaviour by training and evaluating diverse model families across multiple real-world datasets and data distribution settings. It records detailed functional attributes, quality of service metrics, and composition-specific indicators, enabling systematic analysis of service performance and cross-service behaviour. Using MDG, we generate more than ten thousand MLaaS service instances and construct a large-scale benchmark dataset suitable for downstream evaluation. We also implement a built-in composition mechanism that models how services interact under varied Internet of Things conditions. Experiments demonstrate that datasets generated by MDG enhance selection accuracy and composition quality compared to existing baselines. MDG provides a practical and extensible foundation for advancing data-driven research on MLaaS selection and composition

</details>


### [129] [Explanova: Automatically Discover Data Insights in N \times M Table via XAI Combined LLM Workflow](https://arxiv.org/abs/2601.12317)
*Yiming Huang*

Main category: cs.LG

TL;DR: Explanova是一个基于预设AutoML工作流的自动化数据分析框架，使用本地小型LLM降低成本


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM代理工具调用的数据分析框架（如DeepAnalyze、DataSage、Datawise）虽然强大，但成本较高。研究者希望探索基于预设AutoML工作流的替代方案，通过遍历所有可能的分析路径（如变量自身统计、变量间关系、变量与所有其他变量的关系、最终解释）来实现自动化分析。

Method: Explanova采用预设的AutoML式工作流，系统性地遍历所有可能的分析探索路径，包括：变量自身统计特征分析、变量间关系分析、变量与所有其他变量的关系分析，最终生成解释。关键创新是使用本地小型LLM来降低计算成本。

Result: Explanova实现了自动化数据分析，相比现有的LLM代理框架，成本显著降低（由于使用本地小型LLM），同时通过系统性的工作流确保了分析的全面性。

Conclusion: 基于预设AutoML工作流的数据分析框架是可行的替代方案，能够以更低的成本实现自动化数据分析，为数据科学自动化提供了新的方向。

Abstract: Automation in data analysis has been a long-time pursuit. Current agentic LLM shows a promising solution towards it. Like DeepAnalyze, DataSage, and Datawise. They are all powerful agentic frameworks for automatic fine-grained analysis and are powered by LLM-based agentic tool calling ability. However, what about powered by a preset AutoML-like workflow? If we traverse all possible exploration, like Xn itself`s statistics, Xn1-Xn2 relationships, Xn to all other, and finally explain? Our Explanova is such an attempt: Cheaper due to a Local Small LLM.

</details>


### [130] [Ordered Local Momentum for Asynchronous Distributed Learning under Arbitrary Delays](https://arxiv.org/abs/2601.12322)
*Chang-Wei Shi,Shi-Shang Wang,Wu-Jun Li*

Main category: cs.LG

TL;DR: 提出OrLoMo方法，首次实现带本地更新的异步分布式动量SGD，通过有序聚合本地动量来加速异构集群训练


<details>
  <summary>Details</summary>
Motivation: 动量SGD是深度学习训练的基础优化器，异步分布式学习对训练大规模模型至关重要，尤其是在异构计算集群中。现有方法缺乏异步分布式动量SGD与本地更新的结合方案。

Method: 提出OrLoMo（有序本地动量）方法：每个工作节点本地运行动量SGD，服务器根据全局迭代索引有序聚合各工作节点的本地动量。

Result: 证明了OrLoMo在任意延迟下的非凸问题收敛性，实验验证其优于同步方法和其他异步方法。

Conclusion: OrLoMo是首个实现异步分布式动量SGD与本地更新的方法，能有效处理异构集群训练，提升训练效率。

Abstract: Momentum SGD (MSGD) serves as a foundational optimizer in training deep models due to momentum's key role in accelerating convergence and enhancing generalization. Meanwhile, asynchronous distributed learning is crucial for training large-scale deep models, especially when the computing capabilities of the workers in the cluster are heterogeneous. To reduce communication frequency, local updates are widely adopted in distributed learning. However, how to implement asynchronous distributed MSGD with local updates remains unexplored. To solve this problem, we propose a novel method, called \underline{or}dered \underline{lo}cal \underline{mo}mentum (OrLoMo), for asynchronous distributed learning. In OrLoMo, each worker runs MSGD locally. Then the local momentum from each worker will be aggregated by the server in order based on its global iteration index. To the best of our knowledge, OrLoMo is the first method to implement asynchronous distributed MSGD with local updates. We prove the convergence of OrLoMo for non-convex problems under arbitrary delays. Experiments validate that OrLoMo can outperform its synchronous counterpart and other asynchronous methods.

</details>


### [131] [IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning](https://arxiv.org/abs/2601.12330)
*Zuha Fatima,Muhammad Anser Sohaib,Muhammad Talha,Ayesha Kanwal,Sidra Sultana,Nazia Perwaiz*

Main category: cs.LG

TL;DR: 提出IceWatch深度学习框架，结合空间视觉和时序物理动态，实现冰川湖溃决洪水(GLOF)的自动预测与预警


<details>
  <summary>Details</summary>
Motivation: 传统GLOF检测方法依赖水文模型、阈值监测和人工卫星图像分析，存在更新慢、依赖人工、云干扰和现场数据缺乏等问题，需要更自动化和可靠的预测系统

Method: 开发IceWatch框架，包含：1) RiskFlow视觉组件：基于CNN分析Sentinel-2多光谱卫星图像，识别冰雪融水空间模式；2) TerraFlow：从NASA ITS_LIVE时序数据建模冰川流速；3) TempFlow：从MODIS LST记录预测近地表温度；通过协调预处理和同步实现多模态物理信息融合

Result: 系统提供交叉验证，提高GLOF检测的可靠性和可解释性，确保强预测性能、实时数据处理能力，以及对噪声和缺失信息的鲁棒性

Conclusion: IceWatch为自动、可扩展的GLOF预警系统铺平道路，具有与多种传感器输入和全球冰川监测活动集成的潜力

Abstract: Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. They are hazardous to communities, infrastructure, and ecosystems further downstream. The classical methods of GLOF detection and prediction have so far mainly relied on hydrological modeling, threshold-based lake monitoring, and manual satellite image analysis. These approaches suffer from several drawbacks: slow updates, reliance on manual labor, and losses in accuracy when clouds interfere and/or lack on-site data. To tackle these challenges, we present IceWatch: a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. The vision component, RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery using a CNN-based classifier and predicts GLOF events based on the spatial patterns of snow, ice, and meltwater. Its tabular counterpart confirms this prediction by considering physical dynamics. TerraFlow models glacier velocity from NASA ITS_LIVE time series while TempFlow forecasts near-surface temperature from MODIS LST records; both are trained on long-term observational archives and integrated via harmonized preprocessing and synchronization to enable multimodal, physics-informed GLOF prediction. Both together provide cross-validation, which will improve the reliability and interpretability of GLOF detection. This system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information. IceWatch paves the way for automatic, scalable GLOF warning systems. It also holds potential for integration with diverse sensor inputs and global glacier monitoring activities.

</details>


### [132] [LB-MCTS: Synergizing Large Language Models and Bayesian Optimization for Efficient CASH](https://arxiv.org/abs/2601.12355)
*Beicheng Xu,Weitong Qian,Lingching Tung,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: LB-MCTS：结合大语言模型和贝叶斯优化的AutoML框架，通过蒙特卡洛树搜索解决CASH问题，在104个数据集上表现优于基线方法


<details>
  <summary>Details</summary>
Motivation: 降低机器学习门槛，解决传统贝叶斯优化在CASH问题中的冷启动问题，同时克服现有基于LLM的优化器在高维结构化CASH空间中泛化能力差的问题

Method: 提出LB-MCTS框架，将LLM和BO结合在蒙特卡洛树搜索结构中，使用选择性调优记忆（STM）最大化LLM推理能力，通过显式探索-利用权衡，随着数据积累动态从LLM驱动转向BO驱动提案

Result: 在104个AMLB数据集上的实验表明，LB-MCTS优于竞争基线方法

Conclusion: LB-MCTS成功结合了LLM和BO的优势，通过动态切换策略有效解决了CASH问题，为AutoML提供了更强大的优化框架

Abstract: To lower the expertise barrier in machine learning, the AutoML community has focused on the CASH problem, a fundamental challenge that automates the process of algorithm selection and hyperparameter tuning. While traditional methods like Bayesian Optimization (BO) struggle with cold-start issues, Large Language Models (LLMs) can mitigate these via semantic priors. However, existing LLM-based optimizers generalize poorly to the high-dimensional, structured CASH space. We propose LB-MCTS, a framework synergizing LLMs and BO within a Monte Carlo Tree Search structure. It maximizes LLM reasoning with Selective Tuning Memory (STM) and explicit exploration-exploitation trade-off. It combines the strengths of both paradigms by dynamically shifting from LLM-driven to BO-driven proposals as data accumulates. Experiments on 104 AMLB datasets demonstrate the superiority of LB-MCTS over the competitive baselines.

</details>


### [133] [Machine Learning-Based Framework for Real Time Detection and Early Prediction of Control Valve Stiction in Industrial Control Systems](https://arxiv.org/abs/2601.12362)
*Natthapong Promsricha,Chotirawee Chatpattanasiri,Nuttavut Kerdgongsup,Stavroula Balabani*

Main category: cs.LG

TL;DR: 基于机器学习的控制阀粘滞故障早期预测框架，使用常规过程信号（控制器输出和过程变量），LSTM模型在真实炼油厂数据上实现最高精度，可提前4小时预测粘滞故障。


<details>
  <summary>Details</summary>
Motivation: 控制阀粘滞是工业过程中的常见故障，会导致系统不稳定、设备磨损和维护成本增加。许多工厂仍使用缺乏实时监控的传统阀门，使得早期预测具有挑战性。

Method: 开发了三种深度学习模型：CNN、CNN-SVM混合模型和LSTM网络。使用基于斜率比分析的数据驱动标注方法，在真实石油和天然气炼油厂数据集上进行训练。

Result: LSTM模型取得了最高准确率，能够提前4小时预测控制阀粘滞故障。这是首次基于真实工业数据展示机器学习早期预测控制阀粘滞的研究。

Conclusion: 提出的框架可集成到现有控制系统中，支持预测性维护，减少停机时间，避免不必要的硬件更换。

Abstract: Control valve stiction, a friction that prevents smooth valve movement, is a common fault in industrial process systems that causes instability, equipment wear, and higher maintenance costs. Many plants still operate with conventional valves that lack real time monitoring, making early predictions challenging. This study presents a machine learning (ML) framework for detecting and predicting stiction using only routinely collected process signals: the controller output (OP) from control systems and the process variable (PV), such as flow rate. Three deep learning models were developed and compared: a Convolutional Neural Network (CNN), a hybrid CNN with a Support Vector Machine (CNN-SVM), and a Long Short-Term Memory (LSTM) network. To train these models, a data-driven labeling method based on slope ratio analysis was applied to a real oil and gas refinery dataset. The LSTM model achieved the highest accuracy and was able to predict stiction up to four hours in advance. To the best of the authors' knowledge, this is the first study to demonstrate ML based early prediction of control valve stiction from real industry data. The proposed framework can be integrated into existing control systems to support predictive maintenance, reduce downtime, and avoid unnecessary hardware replacement.

</details>


### [134] [Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation](https://arxiv.org/abs/2601.12401)
*Jinmei Liu,Haoru Li,Zhenhong Sun,Chaofeng Chen,Yatao Bian,Bo Wang,Daoyi Dong,Chunlin Chen,Zhi Wang*

Main category: cs.LG

TL;DR: DRIFT框架通过采样、提示和优化三个角度解决RL微调生成模型时的多样性崩溃问题，在保持任务对齐的同时显著提升生成多样性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在微调大规模生成模型时存在"多样性崩溃诅咒"，即优化过程会使策略收敛到狄拉克δ分布，导致生成结果缺乏多样性，限制了模型在需要多样化候选生成的应用中的实用性。

Method: 提出DRIFT框架，从三个角度系统性地激励输出多样性：1) 采样奖励集中子集，过滤奖励异常值防止过早崩溃；2) 使用随机变体提示扩展条件空间；3) 通过基于势能的奖励塑造机制优化组内多样性。

Result: 实验显示DRIFT在任务对齐和生成多样性方面实现帕累托优势：在相同对齐水平下多样性提升9.08%~43.46%，在相同多样性水平下对齐度提升59.65%~65.86%。

Conclusion: DRIFT框架有效解决了RL微调生成模型时的多样性崩溃问题，实现了强任务对齐与高生成多样性的平衡，提升了模型在需要多样化生成应用中的实用性。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \textbf{DRIFT} (\textbf{D}ive\textbf{R}sity-\textbf{I}ncentivized Reinforcement \textbf{F}ine-\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\%\!\sim\! 43.46\%$ increase in diversity at equivalent alignment levels and a $ 59.65\% \!\sim\! 65.86\%$ increase in alignment at equivalent levels of diversity.

</details>


### [135] [Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants](https://arxiv.org/abs/2601.12405)
*Manasi Kanade,Abhi Thakkar,Gabriela Fernandes*

Main category: cs.LG

TL;DR: 开发了一个可解释的机器学习框架用于儿科牙科风险分层，强调可解释性和伦理部署而非最大预测精度


<details>
  <summary>Details</summary>
Motivation: 儿科牙科疾病是全球最普遍且不公平的慢性健康问题之一。虽然流行病学证据显示口腔健康结果与社会经济和人口统计学因素相关，但当前牙科AI应用主要依赖基于图像的诊断和黑盒预测模型，在儿科人群中缺乏透明度和伦理适用性。

Method: 使用人口水平的儿科数据（包括年龄、收入贫困比、种族/民族、性别和病史）训练监督机器学习模型。通过ROC分析和校准曲线评估模型性能，使用SHAP方法实现可解释性，提供全局和个体层面的预测解释。

Result: 模型实现了适度的区分能力（AUC=0.61），具有保守的校准特性，在高概率水平下低估风险。SHAP分析显示年龄和收入贫困比对预测风险的贡献最大，其次是种族/民族和性别。

Conclusion: 可解释的机器学习能够实现透明的、预防导向的儿科牙科风险分层，支持人群筛查和公平的资源分配，而不是用于诊断决策。

Abstract: Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations.
  Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy.
  Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions.
  Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender.
  Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.

</details>


### [136] [Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF](https://arxiv.org/abs/2601.12415)
*Wang Zixian*

Main category: cs.LG

TL;DR: 本文提出正交化策略优化（OPO）框架，将对齐方法解耦为采样几何和优化几何两个独立设计选择，解决KL散度导致的数值不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（如PPO、DPO、IPO）隐含地将采样几何和优化几何混为一谈，导致KL散度对无界值信号施加指数惩罚，造成数值不稳定和高置信度下的梯度消失问题。

Method: 提出正交化策略优化（OPO）框架，将采样几何（α散度加权的重采样）与优化几何（χ²诱导的二次正则化）显式解耦，在比率坐标中使用线性梯度动态的简单目标函数。

Result: OPO在保持峰值寻求行为的同时实现稳定优化，避免梯度饱和，即使模型置信度高时也能维持良好条件的目标函数。

Conclusion: OPO为现有对齐方法提供了统一视角，为稳健的推理导向训练奠定了理论基础，通过解耦采样和优化几何解决了数值不稳定问题。

Abstract: Recent alignment methods for large language models, including PPO, DPO, and IPO, are often presented as distinct algorithms. In this work, we show that many of these approaches implicitly conflate two fundamental and independent design choices: (i) the sampling geometry, which determines which samples dominate the gradient signal, and (ii) the optimization geometry, which determines how deviations in value are penalized. We formalize this observation by expressing alignment as the minimization of a generalized distance between policy energy and target energy, parameterized by an alpha-divergence-based sampling weight and a Bregman-divergence-based value metric. We demonstrate that the commonly used KL divergence induces an exponential penalty on unbounded value signals, leading to numerical instability and vanishing gradients in high-confidence regimes. To address this issue, we propose Orthogonalized Policy Optimization (OPO), a framework that explicitly decouples sampling geometry from optimization geometry. By combining alpha-weighted importance sampling with a chi-square-induced quadratic regularization in ratio coordinates, OPO yields a simple and well-conditioned objective with linear gradient dynamics. This formulation maintains stable optimization while preserving peak-seeking behavior and avoids gradient saturation even when model confidence is high. Our analysis positions OPO as a unifying perspective on existing alignment methods and provides a principled foundation for robust reasoning-oriented training.

</details>


### [137] [Graph Attention Networks with Physical Constraints for Anomaly Detection](https://arxiv.org/abs/2601.12426)
*Mohammadhossein Homaei,Iman Khazrak,Ruben Molano,Andres Caro,Mar Avila*

Main category: cs.LG

TL;DR: 提出一种基于水力感知的图注意力网络，利用归一化守恒定律违规作为特征，结合图注意力和双向LSTM学习时空模式，实现水分配系统的异常检测。


<details>
  <summary>Details</summary>
Motivation: 水分配系统面临日益增长的网络物理风险，现有数据驱动模型忽略网络拓扑且难以解释，而基于模型的方法严重依赖参数准确性。

Method: 使用归一化守恒定律违规作为特征，结合质量与能量平衡残差、图注意力机制和双向LSTM学习时空模式，并采用多尺度模块从节点到网络层面聚合检测分数。

Result: 在BATADAL数据集上达到F1=0.979，相比基线提升3.3个百分点，在15%参数噪声下仍保持高鲁棒性。

Conclusion: 该方法有效结合了物理约束和数据驱动方法，实现了高精度且鲁棒的异常检测，为水分配系统的网络安全提供了可靠解决方案。

Abstract: Water distribution systems (WDSs) face increasing cyber-physical risks, which make reliable anomaly detection essential. Many data-driven models ignore network topology and are hard to interpret, while model-based ones depend strongly on parameter accuracy. This work proposes a hydraulic-aware graph attention network using normalized conservation law violations as features. It combines mass and energy balance residuals with graph attention and bidirectional LSTM to learn spatio-temporal patterns. A multi-scale module aggregates detection scores from node to network level. On the BATADAL dataset, it reaches $F1=0.979$, showing $3.3$pp gain and high robustness under $15\%$ parameter noise.

</details>


### [138] [Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery](https://arxiv.org/abs/2601.12442)
*Shahnawaz Alam,Mohammed Mudassir Uddin,Mohammed Kaif Pasha*

Main category: cs.LG

TL;DR: CANUF框架将贝叶斯深度学习与可微分符号推理结合，在科学AI中同时实现不确定性量化和领域约束满足。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法缺乏整合符号科学知识的机制，而神经符号方法缺乏原则性的不确定性建模。科学AI需要既提供可信不确定性估计又尊重领域约束的模型。

Method: 提出约束感知神经符号不确定性框架(CANUF)，包含三个组件：从科学文献自动提取约束、具有变分推理的概率神经骨干、确保物理一致性的可微分约束满足层。

Result: 在Materials Project、QM9分子属性和气候基准测试中，CANUF相比贝叶斯神经网络将期望校准误差降低34.7%，同时保持99.2%的约束满足率。约束引导的重新校准贡献18.3%性能增益，约束提取精度达91.4%。

Conclusion: CANUF提供了首个端到端可微分管道，同时解决科学预测中的不确定性量化、约束满足和可解释性解释问题。

Abstract: Scientific Artificial Intelligence (AI) applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. We introduce the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. The architecture comprises three components: automated constraint extraction from scientific literature, probabilistic neural backbone with variational inference, and differentiable constraint satisfaction layer ensuring physical consistency. Experiments on Materials Project (140,000+ materials), QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline simultaneously addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.

</details>


### [139] [Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting](https://arxiv.org/abs/2601.12467)
*Saurish Nagrath*

Main category: cs.LG

TL;DR: 提出两阶段时间序列预测框架：第一阶段用CNN提取局部时间动态特征并生成补丁级token嵌入，第二阶段用Transformer编码器建模补丁间依赖关系进行预测。


<details>
  <summary>Details</summary>
Motivation: Transformer在时间序列预测中表现良好，但其效果严重依赖于从原始多元时间序列数据中提取的输入表示的质量和结构。需要更好的时间表示学习方法。

Method: 两阶段框架：1) 局部时间表示学习：CNN在固定长度时间补丁上提取短程时间动态和非线性特征交互，生成补丁级token嵌入，并使用token级自注意力优化；2) 全局依赖建模：Transformer编码器处理token序列，建模补丁间时间依赖并生成预测。

Result: 在具有受控静态和动态因素的合成多元时间序列数据上的实验表明，所提出的基于补丁的token化策略相比卷积和基于补丁的Transformer基线实现了有竞争力的预测性能。

Conclusion: 结构化时间表示的重要性得到验证，将局部时间编码与基于全局注意力的建模解耦能够产生更有效和稳定的时间序列预测。

Abstract: Transformer-based models have shown strong performance in time-series forecasting by leveraging self-attention to model long-range temporal dependencies. However, their effectiveness depends critically on the quality and structure of input representations derived from raw multivariate time-series data. This work proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the first stage, a convolutional neural network (CNN) operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is subsequently applied during representation learning to refine these embeddings by enabling interactions across temporal patches. In the second stage, a Transformer encoder processes the resulting token sequence to model inter-patch temporal dependencies and generate per-patch forecasts. Experiments conducted on synthetic multivariate time-series data with controlled static and dynamic factors demonstrate that the proposed patch-based tokenization strategy achieves competitive forecasting performance compared to convolutional and patch-based Transformer baselines. The results highlight the importance of structured temporal representations and show that decoupling local temporal encoding from global attention-based modelling yields more effective and stable time-series forecasting.

</details>


### [140] [Semidefinite Programming for Quantum Channel Learning](https://arxiv.org/abs/2601.12502)
*Mikhail Gennadievich Belov,Victor Victorovich Dubov,Vadim Konstantinovich Ivanov,Alexander Yurievich Maslov,Olga Vladimirovna Proshina,Vladislav Gennadievich Malyshkin*

Main category: cs.LG

TL;DR: 该论文提出使用半正定规划（SDP）从经典数据重建量子通道的方法，该方法适用于保真度可表示为两个二次型比值的情况，并发现重建的量子通道通常具有较小的Kraus秩。


<details>
  <summary>Details</summary>
Motivation: 研究如何从经典实验数据中重建量子通道，解决量子信息处理中的通道表征和优化问题。

Method: 使用半正定规划（SDP）方法，通过优化Choi矩阵来最大化保真度，该方法适用于保真度可表示为两个二次型比值的情况（如混合态到纯态映射、投影算子、酉学习等）。

Result: 测试了多个商业SDP求解器，都能成功重建不同形式的量子通道。重建的量子通道通常具有较小的Kraus秩（通常小于最大可能值的几个百分比），表明用相对简单的量子通道就能描述实验观测数据。

Conclusion: SDP是重建量子通道的有效方法，重建的通道通常具有低Kraus秩，该方法还可应用于重建投影算子，并讨论了基于量子通道变换的经典计算模型。

Abstract: The problem of reconstructing a quantum channel from a sample of classical data is considered. When the total fidelity can be represented as a ratio of two quadratic forms (e.g., in the case of mapping a mixed state to a pure state, projective operators, unitary learning, and others), Semidefinite Programming (SDP) can be applied to solve the fidelity optimization problem with respect to the Choi matrix. A remarkable feature of SDP is that the optimization is convex, which allows the problem to be efficiently solved by a variety of numerical algorithms. We have tested several commercially available SDP solvers, all of which allowed for the reconstruction of quantum channels of different forms. A notable feature is that the Kraus rank of the obtained quantum channel typically comprises less than a few percent of its maximal possible value. This suggests that a relatively small Kraus rank quantum channel is typically sufficient to describe experimentally observed classical data. The theory was also applied to the problem of reconstructing projective operators from data. Finally, we discuss a classical computational model based on quantum channel transformation, performed and calculated on a classical computer, possibly hardware-optimized.

</details>


### [141] [Learning Relativistic Geodesics and Chaotic Dynamics via Stabilized Lagrangian Neural Networks](https://arxiv.org/abs/2601.12519)
*Abdullah Umut Hamzaogullari,Arkadas Ozakin*

Main category: cs.LG

TL;DR: 论文提出改进拉格朗日神经网络（LNNs）训练稳定性的方法，包括Hessian正则化、改进的激活函数和物理感知坐标缩放，使LNNs能处理更复杂系统（如三摆）并扩展到相对论场景。


<details>
  <summary>Details</summary>
Motivation: 拉格朗日神经网络能从轨迹数据学习任意拉格朗日量，但其不寻常的优化目标导致显著训练不稳定性，限制了在复杂系统中的应用。

Method: 提出三种改进：1) Hessian正则化方案，惩罚拉格朗日量对速度二阶导数中的非物理特征；2) 更适合学习拉格朗日量的激活函数；3) 物理感知坐标缩放以提高稳定性。还扩展正则化以处理相对论场景，惩罚洛伦兹特征违反。

Result: 改进架构成功训练前所未有的复杂系统（包括三摆），在双摆系统中验证损失降低96.6%，稳定性提高90.68%。首次从轨迹数据预测AdS₄时空度量下的测地线拉格朗日量。

Conclusion: 该方法显著扩展了LNNs在科学发现任务中的实际应用，为物理中几何结构的自动发现（包括从测地线轨迹提取时空度量张量分量）开辟了新可能性。

Abstract: Lagrangian Neural Networks (LNNs) can learn arbitrary Lagrangians from trajectory data, but their unusual optimization objective leads to significant training instabilities that limit their application to complex systems. We propose several improvements that address these fundamental challenges, namely, a Hessian regularization scheme that penalizes unphysical signatures in the Lagrangian's second derivatives with respect to velocities, preventing the network from learning unstable dynamics, activation functions that are better suited to the problem of learning Lagrangians, and a physics-aware coordinate scaling that improves stability. We systematically evaluate these techniques alongside previously proposed methods for improving stability. Our improved architecture successfully trains on systems of unprecedented complexity, including triple pendulums, and achieved 96.6\% lower validation loss value and 90.68\% better stability than baseline LNNs in double pendulum systems. With the improved framework, we show that our LNNs can learn Lagrangians representing geodesic motion in both non-relativistic and general relativistic settings. To deal with the relativistic setting, we extended our regularization to penalize violations of Lorentzian signatures, which allowed us to predict a geodesic Lagrangian under AdS\textsubscript{4} spacetime metric directly from trajectory data, which to our knowledge has not been done in the literature before. This opens new possibilities for automated discovery of geometric structures in physics, including extraction of spacetime metric tensor components from geodesic trajectories. While our approach inherits some limitations of the original LNN framework, particularly the requirement for invertible Hessians, it significantly expands the practical applicability of LNNs for scientific discovery tasks.

</details>


### [142] [Approximating splits for decision trees quickly in sparse data streams](https://arxiv.org/abs/2601.12525)
*Nikolaj Tatti*

Main category: cs.LG

TL;DR: 提出针对稀疏二元特征和二元分类的决策树分裂优化算法，在信息增益和基尼指数上实现(1+α)近似，时间复杂度优于传统O(d)方法，特别适用于稀疏数据。


<details>
  <summary>Details</summary>
Motivation: 传统决策树学习算法在处理数据流时，需要在每个叶子节点维护计数器来确定最优分裂，但搜索最优分裂的时间复杂度为O(d)，其中d是特征数量。对于稀疏二元特征和二元分类场景，当d很大但数据稀疏时，这种方法的效率较低。

Method: 提出一种针对稀疏二元特征和二元分类的近似分裂算法。对于信息增益（条件熵），在摊销时间复杂度O(α^{-1}(1+m log d) log log n)内实现(1+α)近似；对于基尼指数，在摊销时间复杂度O(α^{-1}+m log d)内实现(1+α)近似，其中m是数据点中1的数量，n是数据点数量。

Result: 实验结果表明，该算法能够高效地找到几乎最优的分裂点，比基线方法更快，并且实际性能优于理论近似保证。特别在稀疏数据(m≪d)场景下表现优异。

Conclusion: 针对稀疏二元特征和二元分类的决策树分裂问题，提出的近似算法在保证近似质量的同时显著提高了计算效率，特别适用于数据流场景下的稀疏数据处理。

Abstract: Decision trees are one of the most popular classifiers in the machine learning literature. While the most common decision tree learning algorithms treat data as a batch, numerous algorithms have been proposed to construct decision trees from a data stream. A standard training strategy involves augmenting the current tree by changing a leaf node into a split. Here we typically maintain counters in each leaf which allow us to determine the optimal split, and whether the split should be done. In this paper we focus on how to speed up the search for the optimal split when dealing with sparse binary features and a binary class. We focus on finding splits that have the approximately optimal information gain or Gini index. In both cases finding the optimal split can be done in $O(d)$ time, where $d$ is the number of features. We propose an algorithm that yields $(1 + α)$ approximation when using conditional entropy in amortized $O(α^{-1}(1 + m\log d) \log \log n)$ time, where $m$ is the number of 1s in a data point, and $n$ is the number of data points. Similarly, for Gini index, we achieve $(1 + α)$ approximation in amortized $O(α^{-1} + m \log d)$ time. Our approach is beneficial for sparse data where $m \ll d$. In our experiments we find almost-optimal splits efficiently, faster than the baseline, overperforming the theoretical approximation guarantees.

</details>


### [143] [Press Start to Charge: Videogaming the Online Centralized Charging Scheduling Problem](https://arxiv.org/abs/2601.12543)
*Alireza Ghahtarani,Martin Cousineau,Amir-massoud Farahmand,Jorge E. Mendoza*

Main category: cs.LG

TL;DR: 该研究将电动汽车在线集中充电调度问题（OCCSP）游戏化，通过图像到动作模型结合DAgger训练，在负载均衡和经济效益上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决电动汽车实时充电调度问题，需要在满足容量限制的同时平衡电网负载，传统方法面临模型复杂和泛化能力有限的问题。

Method: 将充电调度问题游戏化建模为网格上的约束放置问题，设计启发式策略，通过专家演示训练学习智能体，并使用数据集聚合（DAgger）进行改进。

Result: 游戏化方法降低了模型复杂度并获得了更紧的泛化边界。实验表明，DAgger训练的模型在负载均衡上持续优于启发式基线、向量方法和监督学习智能体，在蒙特利尔地区的实际案例中每年可节省数千万美元系统成本。

Conclusion: 游戏化学习为电动汽车充电调度提供了有效解决方案，不仅提升了负载均衡性能，还能显著降低系统成本并推迟昂贵的电网升级需求。

Abstract: We study the online centralized charging scheduling problem (OCCSP). In this problem, a central authority must decide, in real time, when to charge dynamically arriving electric vehicles (EVs), subject to capacity limits, with the objective of balancing load across a finite planning horizon. To solve the problem, we first gamify it; that is, we model it as a game where charging blocks are placed within temporal and capacity constraints on a grid. We design heuristic policies, train learning agents with expert demonstrations, and improve them using Dataset Aggregation (DAgger). From a theoretical standpoint, we show that gamification reduces model complexity and yields tighter generalization bounds than vector-based formulations. Experiments across multiple EV arrival patterns confirm that gamified learning enhances load balancing. In particular, the image-to-movement model trained with DAgger consistently outperforms heuristic baselines, vector-based approaches, and supervised learning agents, while also demonstrating robustness in sensitivity analyses. These operational gains translate into tangible economic value. In a real-world case study for the Greater Montréal Area (Québec, Canada) using utility cost data, the proposed methods lower system costs by tens of millions of dollars per year over the prevailing practice and show clear potential to delay costly grid upgrades.

</details>


### [144] [Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory](https://arxiv.org/abs/2601.12557)
*Mark Moussa,Amber V. Young,Brianna Isola,Vasuda Trehan,Michael D. Himes,Nicholas Wogan,Giada Arney*

Main category: cs.LG

TL;DR: 本文提出两种机器学习架构（BCNN和SQuAT）用于从系外行星反射光谱预测生物标志物通量，以支持未来旗舰任务如HWO的观测优先级决策。


<details>
  <summary>Details</summary>
Motivation: 未来直接成像旗舰任务（如NASA的HWO）面临严格的时间和资源限制，需要高效优先观测目标。传统方法难以从反射光谱中准确预测生物标志物通量并量化不确定性。

Method: 提出两种机器学习架构：1）贝叶斯卷积神经网络（BCNN），量化认知和随机不确定性；2）光谱查询自适应变换器（SQuAT），使用查询驱动注意力机制增强可解释性，将光谱特征与特定生物标志物物种关联。

Result: 两种模型在涵盖广泛系外行星条件的增强数据集上均实现高预测精度，BCNN在不确定性量化方面表现优异，SQuAT在光谱可解释性方面具有优势。

Conclusion: 这些方法有望加速目标筛选、优化观测计划，最大化未来旗舰任务（如HWO）的科学回报，为系外行星生物标志物探测提供可靠工具。

Abstract: Future direct-imaging flagship missions, such as NASA's Habitable Worlds Observatory (HWO), face critical decisions in prioritizing observations due to extremely stringent time and resource constraints. In this paper, we introduce two advanced machine-learning architectures tailored for predicting biosignature species fluxes from exoplanetary reflected-light spectra: a Bayesian Convolutional Neural Network (BCNN) and our novel model architecture, the Spectral Query Adaptive Transformer (SQuAT). The BCNN robustly quantifies both epistemic and aleatoric uncertainties, offering reliable predictions under diverse observational conditions, whereas SQuAT employs query-driven attention mechanisms to enhance interpretability by explicitly associating spectral features with specific biosignature species. We demonstrate that both models achieve comparably high predictive accuracy on an augmented dataset spanning a wide range of exoplanetary conditions, while highlighting their distinct advantages in uncertainty quantification and spectral interpretability. These capabilities position our methods as promising tools for accelerating target triage, optimizing observation schedules, and maximizing scientific return for upcoming flagship missions such as HWO.

</details>


### [145] [Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization](https://arxiv.org/abs/2601.12598)
*Younes Bouhadjar,Maxime Fabre,Felix Schmidt,Emre Neftci*

Main category: cs.LG

TL;DR: 提出SelectivBench基准测试，系统评估线性循环模型的选择性能力，发现门控和快速遗忘机制有助于召回，通道混合对泛化关键，注意力机制因内存容量随序列长度扩展而保持优势。


<details>
  <summary>Details</summary>
Motivation: 线性循环神经网络作为Transformer注意力机制的高效替代方案，但现有模型架构机制日益复杂，缺乏系统性的直接比较。现有基准任务要么过于简单无法揭示显著差异，要么资源消耗过大难以实验。

Method: 提出线性循环模型的细化分类法，并引入SelectivBench——一套轻量级可定制的合成基准任务集。该基准使用基于规则的语法生成可调整复杂度的序列，包含故意违反转换规则的不规则间隔，专门评估序列模型在小到中等规模的选择性能力。

Result: 在线性循环模型上的评估显示，性能模式与大规模语言任务结果一致。分析表明：门控和快速遗忘机制促进召回；状态内通道混合对选择性不必要但对泛化关键；softmax注意力因内存容量随序列长度扩展而保持主导地位。

Conclusion: SelectivBench为线性循环模型提供了有针对性的高效探索工具，并为研究大规模评估中观察到的行为提供了受控环境，有助于理解不同架构特征的作用。

Abstract: Linear recurrent neural networks have emerged as efficient alternatives to the original Transformer's softmax attention mechanism, thanks to their highly parallelizable training and constant memory and computation requirements at inference. Iterative refinements of these models have introduced an increasing number of architectural mechanisms, leading to increased complexity and computational costs. Nevertheless, systematic direct comparisons among these models remain limited. Existing benchmark tasks are either too simplistic to reveal substantial differences or excessively resource-intensive for experimentation. In this work, we propose a refined taxonomy of linear recurrent models and introduce SelectivBench, a set of lightweight and customizable synthetic benchmark tasks for systematically evaluating sequence models. SelectivBench specifically evaluates selectivity in sequence models at small to medium scale, such as the capacity to focus on relevant inputs while ignoring context-based distractors. It employs rule-based grammars to generate sequences with adjustable complexity, incorporating irregular gaps that intentionally violate transition rules. Evaluations of linear recurrent models on SelectivBench reveal performance patterns consistent with results from large-scale language tasks. Our analysis clarifies the roles of essential architectural features: gating and rapid forgetting mechanisms facilitate recall, in-state channel mixing is unnecessary for selectivity, but critical for generalization, and softmax attention remains dominant due to its memory capacity scaling with sequence length. Our benchmark enables targeted, efficient exploration of linear recurrent models and provides a controlled setting for studying behaviors observed in large-scale evaluations. Code is available at https://github.com/symseqbench/selectivbench

</details>


### [146] [Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients by f-SoftArgmax Parameterization with Coupled Regularization](https://arxiv.org/abs/2601.12604)
*Safwan Labbi,Daniil Tiapkin,Paul Mangold,Eric Moulines*

Main category: cs.LG

TL;DR: 提出用广义f-softargmax替代softmax策略参数化，结合f-散度正则化，无需预条件即可保证有限MDP中随机策略梯度方法的非渐近收敛


<details>
  <summary>Details</summary>
Motivation: 传统softmax参数化会导致病态优化景观和指数级慢收敛，而预条件方法计算成本高，需要更有效的替代方案

Method: 使用广义f-softargmax作为策略参数化，配合对应f-散度的正则化器，改善优化景观并满足Polyak-Lojasiewicz不等式

Result: 首次为有限MDP的随机策略梯度方法建立了无需预条件的显式非渐近最后迭代收敛保证；Tsallis散度的f-PG实现多项式样本复杂度，而标准softmax需要指数复杂度

Conclusion: f-softargmax参数化加f-散度正则化是解决策略梯度方法收敛问题的有效方案，显著优于传统softmax方法

Abstract: Policy gradient methods are known to be highly sensitive to the choice of policy parameterization. In particular, the widely used softmax parameterization can induce ill-conditioned optimization landscapes and lead to exponentially slow convergence. Although this can be mitigated by preconditioning, this solution is often computationally expensive. Instead, we propose replacing the softmax with an alternative family of policy parameterizations based on the generalized f-softargmax. We further advocate coupling this parameterization with a regularizer induced by the same f-divergence, which improves the optimization landscape and ensures that the resulting regularized objective satisfies a Polyak-Lojasiewicz inequality. Leveraging this structure, we establish the first explicit non-asymptotic last-iterate convergence guarantees for stochastic policy gradient methods for finite MDPs without any form of preconditioning. We also derive sample-complexity bounds for the unregularized problem and show that f-PG, with Tsallis divergences achieves polynomial sample complexity in contrast to the exponential complexity incurred by the standard softmax parameterization.

</details>


### [147] [Towards Robust Universal Perturbation Attacks: A Float-Coded, Penalty-Driven Evolutionary Approach](https://arxiv.org/abs/2601.12624)
*Shiqi Wang,Mahdi Khosravy,Neeraj Gupta,Olaf Witkowski*

Main category: cs.LG

TL;DR: 提出一种基于浮点编码、惩罚驱动的单目标进化框架，用于生成通用对抗扰动，在降低可见性的同时提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 通用对抗扰动能够用单一噪声模式破坏多个深度神经网络输入，进化算法因其在非凸、无梯度环境中的导航能力而成为有前景的生成方法。

Method: 采用浮点编码的单目标进化框架，包含与当代深度学习规模对齐的连续基因表示、具有自适应调度的动态进化算子，以及模块化PyTorch实现。通过跨模型测试和周期性批次切换确保扰动的通用性。

Result: 在ImageNet数据集上的实验表明，相比现有进化方法，该框架能产生更小范数、更高误分类效果和更快收敛的扰动。

Conclusion: 该方法展示了在各种深度学习架构上进行通用对抗攻击的鲁棒性和可扩展性。

Abstract: Universal adversarial perturbations (UAPs) have garnered significant attention due to their ability to undermine deep neural networks across multiple inputs using a single noise pattern. Evolutionary algorithms offer a promising approach to generating such perturbations due to their ability to navigate non-convex, gradient-free landscapes. In this work, we introduce a float-coded, penalty-driven single-objective evolutionary framework for UAP generation that achieves lower visibility perturbations while enhancing attack success rates. Our approach leverages continuous gene representations aligned with contemporary deep learning scales, incorporates dynamic evolutionary operators with adaptive scheduling, and utilizes a modular PyTorch implementation for seamless integration with modern architectures. Additionally, we ensure the universality of the generated perturbations by testing across diverse models and by periodically switching batches to prevent overfitting. Experimental results on the ImageNet dataset demonstrate that our framework consistently produces perturbations with smaller norms, higher misclassification effectiveness, and faster convergence compared to existing evolutionary-based methods. These findings highlight the robustness and scalability of our approach for universal adversarial attacks across various deep learning architectures.

</details>


### [148] [Topology-Aware Multiscale Mixture of Experts for Efficient Molecular Property Prediction](https://arxiv.org/abs/2601.12637)
*Long D. Nguyen,Kelin Xia,Binh P. Nguyen*

Main category: cs.LG

TL;DR: MI-MoE：一种用于3D分子图学习的多尺度交互专家混合模型，通过拓扑感知路由机制自适应建模短、中、长程相互作用


<details>
  <summary>Details</summary>
Motivation: 现有3D分子图神经网络依赖固定的邻域启发式方法（如距离截断和最大邻居限制），导致刚性的、数据无关的交互预算，无法自适应建模不同几何尺度下的相互作用

Method: 提出多尺度交互专家混合模型（MI-MoE）：1）距离截断专家集合，显式捕获短、中、长程相互作用；2）拓扑门控编码器，使用基于过滤的描述符（包括持久同调特征）将输入路由到专家；3）作为即插即用模块改进现有3D分子骨干网络

Result: MI-MoE在多个分子和聚合物性质预测基准数据集（涵盖回归和分类任务）上，一致改进了多个强大的3D分子骨干网络性能

Conclusion: 拓扑感知的多尺度路由是3D分子图学习的有效原则，MI-MoE通过自适应建模不同几何尺度下的相互作用，提升了分子性质预测性能

Abstract: Many molecular properties depend on 3D geometry, where non-covalent interactions, stereochemical effects, and medium- to long-range forces are determined by spatial distances and angles that cannot be uniquely captured by a 2D bond graph. Yet most 3D molecular graph neural networks still rely on globally fixed neighborhood heuristics, typically defined by distance cutoffs and maximum neighbor limits, to define local message-passing neighborhoods, leading to rigid, data-agnostic interaction budgets. We propose Multiscale Interaction Mixture of Experts (MI-MoE) to adapt interaction modeling across geometric regimes. Our contributions are threefold: (1) we introduce a distance-cutoff expert ensemble that explicitly captures short-, mid-, and long-range interactions without committing to a single cutoff; (2) we design a topological gating encoder that routes inputs to experts using filtration-based descriptors, including persistent homology features, summarizing how connectivity evolves across radii; and (3) we show that MI-MoE is a plug-in module that consistently improves multiple strong 3D molecular backbones across diverse molecular and polymer property prediction benchmark datasets, covering both regression and classification tasks. These results highlight topology-aware multiscale routing as an effective principle for 3D molecular graph learning.

</details>


### [149] [Explanation Multiplicity in SHAP: Characterization and Assessment](https://arxiv.org/abs/2601.12654)
*Hyunseung Hwang,Seungeun Lee,Lucas Rosenblatt,Julia Stoyanovich,Steven Euijong Whang*

Main category: cs.LG

TL;DR: SHAP解释存在多重性：相同输入、任务和模型下，多次运行会产生不同但都有效的特征归因解释，这种不稳定性在多种场景下普遍存在。


<details>
  <summary>Details</summary>
Motivation: SHAP等事后解释方法被广泛用于高风险领域决策的验证和审计，但SHAP解释在重复运行中可能产生显著差异，这种解释多重性现象需要系统研究和量化。

Method: 提出量化特征归因解释多重性的方法学，区分模型训练/选择与解释流程内在随机性的来源，使用幅度距离和排序距离评估稳定性，并通过随机基线模型提供对比基准。

Result: 解释多重性普遍存在，即使在高置信度预测中也会持续出现；幅度距离可能显示稳定，但排序距离揭示特征身份和顺序的显著变化；不同数据集、模型类别和置信度区间均观察到这种现象。

Conclusion: SHAP解释的多重性挑战了其作为可靠决策解释的地位，需要开发与解释预期用途相匹配的评估指标和基线，以确保解释的可靠性和实用性。

Abstract: Post-hoc explanations are widely used to justify, contest, and audit automated decisions in high-stakes domains. SHAP, in particular, is often treated as a reliable account of which features drove an individual prediction. Yet SHAP explanations can vary substantially across repeated runs even when the input, task, and trained model are held fixed. We term this phenomenon explanation multiplicity: multiple internally valid but substantively different explanations for the same decision. We present a methodology to characterize multiplicity in feature-attribution explanations and to disentangle sources due to model training/selection from stochasticity intrinsic to the explanation pipeline. We further show that apparent stability depends on the metric: magnitude-based distances can remain near zero while rank-based measures reveal substantial churn in the identity and ordering of top features. To contextualize observed disagreement, we derive randomized baseline values under plausible null models. Across datasets, model classes, and confidence regimes, we find explanation multiplicity is pervasive and persists even for high-confidence predictions, highlighting the need for metrics and baselines that match the intended use of explanations.

</details>


### [150] [MetaToolAgent: Towards Generalizable Tool Usage in LLMs through Meta-Learning](https://arxiv.org/abs/2601.12680)
*Zheng Fang,Wolfgang Mayer,Zeyu Zhang,Jian Wang,Hong-Yu Zhang,Wanli Li,Zaiwen Feng*

Main category: cs.LG

TL;DR: 提出MetaToolAgent (MTA)元学习方法，通过包含155个工具和9,377个问答对的跨7个领域数据集，显著提升LLM在未见工具上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具选择方法通常局限于有限工具集，难以泛化到实际部署中遇到的新工具，这限制了LLM在复杂现实任务中的工具协调能力。

Method: 提出MetaToolAgent (MTA)，一种元学习方法，旨在改进跨工具泛化能力。构建了包含7个领域、155个工具和9,377个问答对的综合数据集，模拟真实集成场景。

Result: 实验结果表明，MTA在未见工具上的表现显著优于基线方法，证明了其在构建需要动态工具协调的灵活可扩展系统方面的潜力。

Conclusion: MTA通过元学习方法有效解决了工具选择的泛化问题，为构建能够动态协调多样化工具的LLM系统提供了有前景的解决方案。

Abstract: Tool learning is increasingly important for large language models (LLMs) to effectively coordinate and utilize a diverse set of tools in order to solve complex real-world tasks. By selecting and integrating appropriate tools, LLMs extend their capabilities beyond pure language understanding to perform specialized functions. However, existing methods for tool selection often focus on limited tool sets and struggle to generalize to novel tools encountered in practical deployments. To address these challenges, we introduce a comprehensive dataset spanning 7 domains, containing 155 tools and 9,377 question-answer pairs, which simulates realistic integration scenarios. Additionally, we propose MetaToolAgent (MTA), a meta-learning approach designed to improve cross-tool generalization. Experimental results show that MTA significantly outperforms baseline methods on unseen tools, demonstrating its promise for building flexible and scalable systems that require dynamic tool coordination.

</details>


### [151] [Towards Spectroscopy: Susceptibility Clusters in Language Models](https://arxiv.org/abs/2601.12703)
*Andrew Gordon,Garrett Baker,George Wang,William Snell,Stan van Wingerden,Daniel Murfet*

Main category: cs.LG

TL;DR: 提出一种基于谱分析的神经网络内部结构探测方法，通过扰动数据分布测量模型响应，识别出510个可解释的聚类模式


<details>
  <summary>Details</summary>
Motivation: 借鉴物理学中的谱分析原理，通过扰动数据分布来探测神经网络内部结构，理解模型如何处理不同上下文模式

Method: 使用随机梯度朗之万动力学（SGLD）计算局部吉布斯后验，通过测量模型对token权重扰动的响应（敏感性χ_xy），开发基于电导的聚类算法

Result: 在Pythia-14M模型中识别出510个可解释聚类，涵盖语法模式、代码结构、数学符号等，50%聚类与稀疏自编码器特征匹配

Conclusion: 该方法成功将谱分析原理应用于神经网络，提供了一种探测模型内部结构的新方法，与现有方法结果一致且更具解释性

Abstract: Spectroscopy infers the internal structure of physical systems by measuring their response to perturbations. We apply this principle to neural networks: perturbing the data distribution by upweighting a token $y$ in context $x$, we measure the model's response via susceptibilities $χ_{xy}$, which are covariances between component-level observables and the perturbation computed over a localized Gibbs posterior via stochastic gradient Langevin dynamics (SGLD). Theoretically, we show that susceptibilities decompose as a sum over modes of the data distribution, explaining why tokens that follow their contexts "for similar reasons" cluster together in susceptibility space. Empirically, we apply this methodology to Pythia-14M, developing a conductance-based clustering algorithm that identifies 510 interpretable clusters ranging from grammatical patterns to code structure to mathematical notation. Comparing to sparse autoencoders, 50% of our clusters match SAE features, validating that both methods recover similar structure.

</details>


### [152] [Adaptively trained Physics-informed Radial Basis Function Neural Networks for Solving Multi-asset Option Pricing Problems](https://arxiv.org/abs/2601.12704)
*Yan Ma,Yumeng Ren*

Main category: cs.LG

TL;DR: 提出基于径向基函数神经网络的物理信息机器学习算法（PIRBFNN），用于求解多资产期权定价的Black-Scholes偏微分方程，通过自适应优化网络结构提高计算精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在求解多资产期权定价的Black-Scholes偏微分方程时面临维度灾难和非光滑支付条件等挑战，需要开发更高效准确的数值方法。

Method: 结合径向基函数配置法和物理信息神经网络，提出PIRBFNN算法，采用PDE残差技术自适应优化隐藏神经元分布，处理多维期权定价模型。

Result: 通过单资产欧式看跌期权、双资产交换期权和四资产篮子看涨期权实验验证了方法的有效性，能够准确高效处理多维非光滑支付条件问题。

Conclusion: PIRBFNN方法成功结合了传统径向基函数配置法和物理信息神经网络的优点，为金融领域多维PDE问题提供了有效的数值解决方案。

Abstract: The present study investigates the numerical solution of Black-Scholes partial differential equation (PDE) for option valuation with multiple underlying assets. We develop a physics-informed (PI) machine learning algorithm based on a radial basis function neural network (RBFNN) that concurrently optimizes the network architecture and predicts the target option price. The physics-informed radial basis function neural network (PIRBFNN) combines the strengths of the traditional radial basis function collocation method and the physics-informed neural network machine learning approach to effectively solve PDE problems in the financial context. By employing a PDE residual-based technique to adaptively refine the distribution of hidden neurons during the training process, the PIRBFNN facilitates accurate and efficient handling of multidimensional option pricing models featuring non-smooth payoff conditions. The validity of the proposed method is demonstrated through a set of experiments encompassing a single-asset European put option, a double-asset exchange option, and a four-asset basket call option.

</details>


### [153] [Trend-Adjusted Time Series Models with an Application to Gold Price Forecasting](https://arxiv.org/abs/2601.12706)
*Sina Kazemdehbashi*

Main category: cs.LG

TL;DR: 本文提出TATS模型，将时间序列预测重构为趋势预测和数值预测两部分，通过二元分类器预测趋势方向，再用LSTM/Bi-LSTM预测数值，最后基于预测趋势调整数值预测结果，在金融时间序列上表现优于标准模型。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法（包括统计模型和LSTM等神经网络）通常直接预测数值，但作者认为时间序列预测应分为趋势预测（方向性运动）和数值预测两部分。现有评估指标如MSE和MAE不足以全面评估模型性能，需要加入趋势检测准确率来衡量模型捕捉趋势的能力。

Method: 提出趋势调整时间序列（TATS）模型：1）使用二元分类器预测下一时间步的趋势方向（上涨/下跌）；2）使用LSTM或Bi-LSTM预测下一时间步的数值；3）基于分类器预测的趋势方向调整LSTM/Bi-LSTM的预测值。通过理论分析和实证评估验证方法有效性。

Result: 在波动性金融时间序列（每日黄金价格）上的实验表明，TATS模型相比标准LSTM和Bi-LSTM模型显著降低了预测误差。同时发现传统评估指标MSE和MAE不足以全面评估时间序列模型性能，需要加入趋势检测准确率指标。

Conclusion: 将时间序列预测重构为趋势预测和数值预测两部分是有效的，TATS模型通过趋势调整机制提升了预测性能。未来时间序列模型评估应同时考虑数值预测精度和趋势捕捉能力。

Abstract: Time series data play a critical role in various fields, including finance, healthcare, marketing, and engineering. A wide range of techniques (from classical statistical models to neural network-based approaches such as Long Short-Term Memory (LSTM)) have been employed to address time series forecasting challenges. In this paper, we reframe time series forecasting as a two-part task: (1) predicting the trend (directional movement) of the time series at the next time step, and (2) forecasting the quantitative value at the next time step. The trend can be predicted using a binary classifier, while quantitative values can be forecasted using models such as LSTM and Bidirectional Long Short-Term Memory (Bi-LSTM). Building on this reframing, we propose the Trend-Adjusted Time Series (TATS) model, which adjusts the forecasted values based on the predicted trend provided by the binary classifier. We validate the proposed approach through both theoretical analysis and empirical evaluation. The TATS model is applied to a volatile financial time series (the daily gold price) with the objective of forecasting the next days price. Experimental results demonstrate that TATS consistently outperforms standard LSTM and Bi-LSTM models by achieving significantly lower forecasting error. In addition, our results indicate that commonly used metrics such as MSE and MAE are insufficient for fully assessing time series model performance. Therefore, we also incorporate trend detection accuracy, which measures how effectively a model captures trends in a time series.

</details>


### [154] [Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off](https://arxiv.org/abs/2601.12730)
*Zhaochun Li,Chen Wang,Jionghao Bai,Shisheng Cui,Ge Lan,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: 提出DCPO方法，从分布中心视角解决LLM强化学习中的探索-利用权衡问题，通过分布级正则化控制熵，相比GRPO平均提升20%性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法如GRPO倾向于过度利用，导致熵单调下降、样本收敛、探索不足。现有解决方案多为样本中心视角，依赖稀有样本的"运气"，缺乏对策略的原则性控制，效果有限且不稳定。

Method: 提出分布中心策略优化(DCPO)，将熵调节重新定义为分布级正则化问题。通过引导策略朝向"更好"的目标分布，实现完全在线、无需外部分布采样的可控熵调节，从而促进高效探索。

Result: 在多个模型和七个基准测试中，DCPO相比GRPO平均提升约20%性能。该方法实现了可控的熵调节，保持了训练稳定性，同时促进了高效探索。

Conclusion: DCPO用分布级原则替代样本级启发式方法，为可控探索和更强的探索-利用权衡提供了理论基础扎实且灵活的框架，解决了RL中熵崩溃的根本问题。

Abstract: The exploration-exploitation (EE) trade-off is a central challenge in reinforcement learning (RL) for large language models (LLMs). With Group Relative Policy Optimization (GRPO), training tends to be exploitation driven: entropy decreases monotonically, samples convergence, and exploration fades. Most existing fixes are \textbf{sample-centric}: they seek or bonus rare samples, assuming exploration comes from novel trajectories and tokens. These heuristics depend on the "luck" of informative samples, lack principled control of the policy, and often yield limited or inconsistent gains. In this work, we are the first to introduce a \textbf{distribution-centric} perspective for RL, in which exploration is always guided by a "better" target distribution, and reveal that a policy's ability to resist entropy collapse is governed by the distribution itself rather than individual samples. Building on this insight, we propose Distribution-Centric Policy Optimization (DCPO), which reformulates entropy regulation as distribution-level regularization. DCPO achieves controllable entropy fully on-policy without sampling from external distributions, enabling efficient exploration while maintaining training stability. Across multiple models and seven benchmarks, DCPO improves over GRPO by about 20\% on average. Overall, DCPO replaces sample-level heuristics with distribution-level principles, offering a theoretically grounded and flexible framework for controllable exploration and a stronger EE trade-off. The code is available in https://github.com/597358816/DCPO.

</details>


### [155] [A Graph Prompt Fine-Tuning Method for WSN Spatio-Temporal Correlation Anomaly Detection](https://arxiv.org/abs/2601.12745)
*Miao Ye,Jing Cui,Yuan huang,Qian He,Yong Wang,Jiwen Zhang*

Main category: cs.LG

TL;DR: 提出一种结合时空相关特征图神经网络和多任务自监督训练的WSN异常检测方法，通过改进Mamba模型和图提示微调机制，在公开和实际数据集上取得优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 针对无线传感器网络多时序模态数据异常检测中存在的时空相关特征提取不足、异常样本标注成本高、样本不平衡等问题，需要设计更有效的检测方法。

Method: 1) 设计基于多尺度策略和模态间融合方法改进Mamba模型的异常检测骨干网络，结合变分图卷积模块充分提取时空相关特征；2) 设计包含无负对比学习、预测和重构三个子任务的预训练方法；3) 提出"图提示-微调"机制指导预训练模型参数微调。

Result: 在公开数据集和实际采集数据集上的F1指标分别达到91.30%和92.31%，相比现有方法具有更好的检测性能和泛化能力。

Conclusion: 该方法能有效解决WSN多时序模态数据异常检测中的特征提取不足和样本不平衡问题，降低训练成本并提升检测泛化性能。

Abstract: Anomaly detection of multi-temporal modal data in Wireless Sensor Network (WSN) can provide an important guarantee for reliable network operation. Existing anomaly detection methods in multi-temporal modal data scenarios have the problems of insufficient extraction of spatio-temporal correlation features, high cost of anomaly sample category annotation, and imbalance of anomaly samples. In this paper, a graph neural network anomaly detection backbone network incorporating spatio-temporal correlation features and a multi-task self-supervised training strategy of "pre-training - graph prompting - fine-tuning" are designed for the characteristics of WSN graph structure data. First, the anomaly detection backbone network is designed by improving the Mamba model based on a multi-scale strategy and inter-modal fusion method, and combining it with a variational graph convolution module, which is capable of fully extracting spatio-temporal correlation features in the multi-node, multi-temporal modal scenarios of WSNs. Secondly, we design a three-subtask learning "pre-training" method with no-negative comparative learning, prediction, and reconstruction to learn generic features of WSN data samples from unlabeled data, and design a "graph prompting-fine-tuning" mechanism to guide the pre-trained self-supervised learning. The model is fine-tuned through the "graph prompting-fine-tuning" mechanism to guide the pre-trained self-supervised learning model to complete the parameter fine-tuning, thereby reducing the training cost and enhancing the detection generalization performance. The F1 metrics obtained from experiments on the public dataset and the actual collected dataset are up to 91.30% and 92.31%, respectively, which provides better detection performance and generalization ability than existing methods designed by the method.

</details>


### [156] [A Boolean Function-Theoretic Framework for Expressivity in GNNs with Applications to Fair Graph Mining](https://arxiv.org/abs/2601.12751)
*Manjish Pal*

Main category: cs.LG

TL;DR: 本文提出基于布尔函数理论的图神经网络表达能力新框架，引入子群体布尔同构概念，超越现有表达能力度量，识别傅里叶度、电路类和影响力为公平感知GNN的关键表达障碍，设计能处理复杂布尔函数定义子群体的公平算法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络表达能力分析框架（如Weisfeiler-Lehman、双连通性、同态等）无法精细分析GNN捕捉复杂子群体结构的能力，特别是在公平性场景中处理由复杂布尔函数定义的交叉子群体时存在局限。

Method: 提出基于布尔函数理论的表达能力框架，引入子群体布尔同构概念；识别傅里叶度、电路类（AC⁰、NC¹）和影响力作为表达能力关键障碍；设计基于电路遍历的公平算法，能处理如奇偶性等高复杂度布尔函数定义的子群体。

Result: 理论证明子群体布尔同构严格包含现有表达能力度量；在真实世界图数据上的实验表明，该方法在现有方法失败的交叉子群体上实现了低公平性差距，为GNN公平性提供了首个原则性表达能力处理方案。

Conclusion: 该研究为图神经网络公平性提供了基于布尔函数理论的原则性表达能力分析框架，超越了现有表达能力度量，设计的算法能有效处理复杂布尔函数定义的子群体，在交叉子群体公平性问题上取得显著改进。

Abstract: We propose a novel expressivity framework for Graph Neural Networks (GNNs) grounded in Boolean function theory, enabling a fine-grained analysis of their ability to capture complex subpopulation structures. We introduce the notion of \textit{Subpopulation Boolean Isomorphism} (SBI) as an invariant that strictly subsumes existing expressivity measures such as Weisfeiler-Lehman (WL), biconnectivity-based, and homomorphism-based frameworks. Our theoretical results identify Fourier degree, circuit class (AC$^0$, NC$^1$), and influence as key barriers to expressivity in fairness-aware GNNs. We design a circuit-traversal-based fairness algorithm capable of handling subpopulations defined by high-complexity Boolean functions, such as parity, which break existing baselines. Experiments on real-world graphs show that our method achieves low fairness gaps across intersectional groups where state-of-the-art methods fail, providing the first principled treatment of GNN expressivity tailored to fairness.

</details>


### [157] [Eddy-Resolving Global Ocean Forecasting with Multi-Scale Graph Neural Networks](https://arxiv.org/abs/2601.12775)
*Yuta Hirabayashi,Daisuke Matusoka,Konobu Kimura*

Main category: cs.LG

TL;DR: 提出基于多尺度图神经网络的全球海洋预报模型，用于10天预报，通过双分辨率球面网格和大气变量输入，提升短期预报精度和多尺度海洋变率表征能力。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的海洋模型研究进展迅速，但应用于全球涡旋分辨率海洋预报仍有限制。准确表征跨广泛空间尺度的海洋动力学是主要挑战。

Method: 采用编码器-处理器-解码器架构，使用两个不同分辨率的球面网格捕获多尺度海洋动力学。将表面大气变量与海洋状态变量作为节点输入，以表征大气强迫作用。

Result: 通过表面动能谱和案例研究评估，模型能准确表征广泛空间尺度。均方根误差比较显示短期预报技能提升。

Conclusion: 该模型提供更准确的短期预报和改善的多尺度海洋动力学表征，突显其在推进数据驱动、涡旋分辨率全球海洋预报方面的潜力。

Abstract: Research on data-driven ocean models has progressed rapidly in recent years; however, the application of these models to global eddy-resolving ocean forecasting remains limited. The accurate representation of ocean dynamics across a wide range of spatial scales remains a major challenge in such applications. This study proposes a multi-scale graph neural network-based ocean model for 10-day global forecasting that improves short-term prediction skill and enhances the representation of multi-scale ocean variability. The model employs an encoder-processor-decoder architecture and uses two spherical meshes with different resolutions to better capture the multi-scale nature of ocean dynamics. In addition, the model incorporates surface atmospheric variables along with ocean state variables as node inputs to improve short-term prediction accuracy by representing atmospheric forcing. Evaluation using surface kinetic energy spectra and case studies shows that the model accurately represents a broad range of spatial scales, while root mean square error comparisons demonstrate improved skill in short-term predictions. These results indicate that the proposed model delivers more accurate short-term forecasts and improved representation of multi-scale ocean dynamics, thereby highlighting its potential to advance data-driven, eddy-resolving global ocean forecasting.

</details>


### [158] [Distilling Time Series Foundation Models for Efficient Forecasting](https://arxiv.org/abs/2601.12785)
*Yuqi Li,Kuiye Ding,Chuanguang Yang,Szu-Yu Chen,Yingli Tian*

Main category: cs.LG

TL;DR: DistilTS是首个专门为时间序列基础模型设计的蒸馏框架，通过水平加权目标和时间对齐策略解决任务难度差异和架构差异问题，实现参数减少150倍、推理加速6000倍的同时保持可比性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型虽然预测性能强，但参数量大导致部署成本高。现有的知识蒸馏技术不适用于时间序列预测的特殊性，需要专门设计针对TSFMs的蒸馏框架。

Method: 提出DistilTS框架：1）针对预测任务难度差异，引入水平加权目标平衡短期和长期预测的学习；2）针对架构差异，设计时间序列预测中的对齐机制，减少架构不匹配。

Result: 在多个基准测试中，DistilTS实现了与完整大小TSFMs相当的预测性能，同时参数减少高达1/150，推理加速高达6000倍。

Conclusion: DistilTS是首个专门为时间序列基础模型设计的有效蒸馏框架，成功解决了任务难度差异和架构差异两大挑战，为TSFMs的高效部署提供了实用解决方案。

Abstract: Time Series foundation models (TSFMs) deliver strong forecasting performance through large-scale pretraining, but their large parameter sizes make deployment costly. While knowledge distillation offers a natural and effective approach for model compression, techniques developed for general machine learning tasks are not directly applicable to time series forecasting due to the unique characteristics. To address this, we present DistilTS, the first distillation framework specifically designed for TSFMs. DistilTS addresses two key challenges: (1) task difficulty discrepancy, specific to forecasting, where uniform weighting makes optimization dominated by easier short-term horizons, while long-term horizons receive weaker supervision; and (2) architecture discrepancy, a general challenge in distillation, for which we design an alignment mechanism in the time series forecasting. To overcome these issues, DistilTS introduces horizon-weighted objectives to balance learning across horizons, and a temporal alignment strategy that reduces architectural mismatch, enabling compact models. Experiments on multiple benchmarks demonstrate that DistilTS achieves forecasting performance comparable to full-sized TSFMs, while reducing parameters by up to 1/150 and accelerating inference by up to 6000x. Code is available at: https://github.com/itsnotacie/DistilTS-ICASSP2026.

</details>


### [159] [Semi-supervised Instruction Tuning for Large Language Models on Text-Attributed Graphs](https://arxiv.org/abs/2601.12807)
*Zixing Song,Irwin King*

Main category: cs.LG

TL;DR: SIT-Graph：一种用于图学习的半监督指令调优框架，通过迭代自训练利用未标记节点提升LLM在图学习任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有图指令调优方法需要大量标注节点数据，这在社交领域等敏感或快速演化的场景中成本高昂且缓慢，且未能充分利用未标记节点中蕴含的边连接潜在相关性

Method: 提出模型无关的SIT-Graph框架，通过迭代自训练过程：1）先用标记节点指令对微调模型；2）为未标记节点生成置信度过滤的伪响应；3）策略性地扩充数据集进行下一轮微调；4）迭代优化使LLM与底层节点相关性对齐

Result: 将SIT-Graph集成到最先进的图指令调优方法中，在文本属性图基准测试上显著提升性能，在低标签率设置下实现超过20%的改进

Conclusion: SIT-Graph有效解决了图指令调优中标注数据稀缺的问题，通过利用未标记节点信息显著提升了LLM在图学习任务中的性能，为半监督图学习提供了新范式

Abstract: The emergent reasoning capabilities of Large Language Models (LLMs) offer a transformative paradigm for analyzing text-attributed graphs. While instruction tuning is the prevailing method for adapting pre-trained LLMs to graph learning tasks like node classification, it requires a substantial volume of annotated (INSTRUCTION, OUTPUT) pairs deriving from labeled nodes. This requirement is particularly prohibitive in the social domain, where obtaining expert labels for sensitive or evolving content is costly and slow. Furthermore, standard graph instruction tuning fails to exploit the vast amount of unlabeled nodes, which contain latent correlations due to edge connections that are beneficial for downstream predictions. To bridge this gap, we propose a novel Semi-supervised Instruction Tuning pipeline for Graph Learning, named SIT-Graph. Notably, SIT-Graph is model-agnostic and can be seamlessly integrated into any graph instruction tuning method that utilizes LLMs as the predictor. SIT-Graph operates via an iterative self-training process. Initially, the model is fine-tuned using instruction pairs constructed solely from the labeled nodes. Then it generates confidence-filtered pseudo-responses for unlabeled nodes to strategically augment the dataset for the next round of fine-tuning. Finally, this iterative refinement progressively aligns the LLM with the underlying node correlations. Extensive experiments demonstrate that when incorporated into state-of-the-art graph instruction tuning methods, SIT-Graph significantly enhances their performance on text-attributed graph benchmarks, achieving over 20% improvement under the low label ratio settings.

</details>


### [160] [Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning](https://arxiv.org/abs/2601.12816)
*Ishir Garg,Neel Kolhe,Andy Peng,Rohan Gopalam*

Main category: cs.LG

TL;DR: 提出FOPNG优化器，通过Fisher正交投影约束参数更新，在连续学习中防止灾难性遗忘，统一了自然梯度下降和正交梯度方法。


<details>
  <summary>Details</summary>
Motivation: 连续学习需要神经网络在顺序任务中获取新知识，但关键挑战是在学习新任务时不遗忘旧任务（灾难性遗忘）。现有方法在欧几里得参数空间操作，缺乏信息几何框架的统一。

Method: 提出Fisher正交投影自然梯度下降（FOPNG）优化器，将梯度投影到先前任务梯度的Fisher正交补空间。该方法在信息几何框架下统一自然梯度下降和正交梯度方法，更新方向对重参数化不变，保证在Fisher度量下下降。

Result: 在标准连续学习基准测试（Permuted-MNIST、Split-MNIST、Rotated-MNIST、Split-CIFAR10、Split-CIFAR100）上展示了强劲结果，证明能有效保留旧任务性能同时学习新任务。

Conclusion: FOPNG通过Fisher正交约束提供了一种理论严谨且实用的连续学习优化方法，在信息几何框架下统一了现有方法，有效解决了灾难性遗忘问题。

Abstract: Continual learning aims to enable neural networks to acquire new knowledge on sequential tasks. However, the key challenge in such settings is to learn new tasks without catastrophically forgetting previously learned tasks. We propose the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer, which enforces Fisher-orthogonal constraints on parameter updates to preserve old task performance while learning new tasks. Unlike existing methods that operate in Euclidean parameter space, FOPNG projects gradients onto the Fisher-orthogonal complement of previous task gradients. This approach unifies natural gradient descent with orthogonal gradient methods within an information-geometric framework. The resulting update direction is invariant under reparameterization, guarantees descent in the Fisher metric, and helps preserve prior task outputs. We provide theoretical analysis establishing the properties of the projected update, describe efficient and practical implementations using the diagonal Fisher, and demonstrate strong results on standard continual learning benchmarks such as Permuted-MNIST, Split-MNIST, Rotated-MNIST, Split-CIFAR10, and Split-CIFAR100.

</details>


### [161] [Knowledge-Integrated Representation Learning for Crypto Anomaly Detection under Extreme Label Scarcity; Relational Domain-Logic Integration with Retrieval-Grounded Context and Path-Level Explanations](https://arxiv.org/abs/2601.12839)
*Gyuyeon Na,Minjung Park,Soyoun Kim,Jungbin Shin,Sangmi Chai*

Main category: cs.LG

TL;DR: RDLI框架通过将专家启发式逻辑嵌入表示学习，结合检索式上下文模块，在标签极度稀缺的加密货币网络中显著提升异常交易检测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 去中心化加密货币网络中的异常轨迹检测面临标签极度稀缺（0.01%）和恶意行为者自适应规避策略的挑战。传统GNN虽然能捕捉局部结构模式，但难以内化资金分散、分层等多跳逻辑驱动模式，限制了在FATF旅行规则等监管要求下的法证可追溯性。

Method: 提出关系域逻辑集成（RDLI）框架，将专家推导的启发式规则作为可微分的逻辑感知潜在信号嵌入表示学习。不同于静态规则方法，RDLI能检测规避标准消息传递的复杂交易流。此外，引入检索式上下文（RGC）模块，将异常评分基于监管和宏观经济背景，减轻良性制度变化导致的误报。

Result: 在极端标签稀缺（0.01%）条件下，RDLI在F1分数上优于最先进的GNN基线28.9%。微观专家用户研究进一步证实，RDLI的路径级解释在可信度、感知有用性和清晰度方面显著优于现有方法。

Conclusion: RDLI框架通过将领域逻辑与上下文基础相结合，不仅提高了检测准确性，还增强了可解释性，为加密货币网络中的异常检测提供了更可靠和可信的解决方案。

Abstract: Detecting anomalous trajectories in decentralized crypto networks is fundamentally challenged by extreme label scarcity and the adaptive evasion strategies of illicit actors. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they struggle to internalize multi hop, logic driven motifs such as fund dispersal and layering that characterize sophisticated money laundering, limiting their forensic accountability under regulations like the FATF Travel Rule. To address this limitation, we propose Relational Domain Logic Integration (RDLI), a framework that embeds expert derived heuristics as differentiable, logic aware latent signals within representation learning. Unlike static rule based approaches, RDLI enables the detection of complex transactional flows that evade standard message passing. To further account for market volatility, we incorporate a Retrieval Grounded Context (RGC) module that conditions anomaly scoring on regulatory and macroeconomic context, mitigating false positives caused by benign regime shifts. Under extreme label scarcity (0.01%), RDLI outperforms state of the art GNN baselines by 28.9% in F1 score. A micro expert user study further confirms that RDLI path level explanations significantly improve trustworthiness, perceived usefulness, and clarity compared to existing methods, highlighting the importance of integrating domain logic with contextual grounding for both accuracy and explainability.

</details>


### [162] [Generating Cyclic Conformers with Flow Matching in Cremer-Pople Coordinates](https://arxiv.org/abs/2601.12859)
*Luca Schaufelberger,Aline Hartgers,Kjell Jorner*

Main category: cs.LG

TL;DR: PuckerFlow是一种用于环状分子构象生成的机器学习模型，通过Cremer-Pople空间进行流匹配，能够高效可靠地生成环状结构的构象。


<details>
  <summary>Details</summary>
Motivation: 环状分子在化学和生物学应用中普遍存在，其受限的构象灵活性对药物发现和催化功能至关重要。然而，可靠地采样环系统的构象集合仍然具有挑战性。

Method: 引入PuckerFlow生成模型，在Cremer-Pople空间（捕获环相关自由度的低维内部坐标系）上执行流匹配，能够设计生成有效的闭合环。

Result: PuckerFlow在几乎所有定量指标上都优于其他构象生成方法，能够生成既多样又精确的构象，特别适用于催化和药物发现相关的环系统。

Conclusion: 该工作实现了环状结构的高效可靠构象生成，为建模结构-性质关系和跨化学与生物学应用的属性引导环生成铺平了道路。

Abstract: Cyclic molecules are ubiquitous across applications in chemistry and biology. Their restricted conformational flexibility provides structural pre-organization that is key to their function in drug discovery and catalysis. However, reliably sampling the conformer ensembles of ring systems remains challenging. Here, we introduce PuckerFlow, a generative machine learning model that performs flow matching on the Cremer-Pople space, a low-dimensional internal coordinate system capturing the relevant degrees of freedom of rings. Our approach enables generation of valid closed rings by design and demonstrates strong performance in generating conformers that are both diverse and precise. We show that PuckerFlow outperforms other conformer generation methods on nearly all quantitative metrics and illustrate the potential of PuckerFlow for ring systems relevant to chemical applications, particularly in catalysis and drug discovery. This work enables efficient and reliable conformer generation of cyclic structures, paving the way towards modeling structure-property relationships and the property-guided generation of rings across a wide range of applications in chemistry and biology.

</details>


### [163] [Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition](https://arxiv.org/abs/2601.12879)
*Mohammed Mudassir Uddin,Shahnawaz Alam,Mohammed Kaif Pasha*

Main category: cs.LG

TL;DR: HAGD框架通过多分辨率抽象层次和可微分电路搜索，将电路发现复杂度从O(2^n)降至O(n² log n)，在GPT-2、Llama和Pythia模型上验证了算法任务和自然语言基准的性能。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性旨在将神经网络计算反工程为人类可理解的算法，但从数十亿参数语言模型中提取稀疏计算电路面临指数搜索复杂性和普遍多义性的挑战。

Method: 提出分层归因图分解(HAGD)框架，整合跨层转码器进行单义特征提取、图神经网络元学习进行拓扑预测、因果干预协议进行验证。

Result: 在模运算任务上实现高达91%的行为保持(±2.3%)，同时保持可解释子图大小；跨架构迁移实验显示发现电路具有67%的平均结构相似性。

Conclusion: 为更大模型规模的可解释性提供了初步基础，同时识别出当前归因方法的重要局限性，需要未来进一步改进。

Abstract: Mechanistic interpretability seeks to reverse-engineer neural network computations into human-understandable algorithms, yet extracting sparse computational circuits from billion-parameter language models remains challenging due to exponential search complexity and pervasive polysemanticity. The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarchies and differentiable circuit search. The methodology integrates cross-layer transcoders for monosemantic feature extraction, graph neural network meta-learning for topology prediction, and causal intervention protocols for validation. Empirical evaluation spans GPT-2 variants, Llama-7B through Llama-70B, and Pythia suite models across algorithmic tasks and natural language benchmarks. On modular arithmetic tasks, the framework achieves up to 91% behavioral preservation ($\pm$2.3\% across runs) while maintaining interpretable subgraph sizes. Cross-architecture transfer experiments suggest that discovered circuits exhibit moderate structural similarity (averaging 67%) across model families, indicating potential shared computational patterns. These results provide preliminary foundations for interpretability at larger model scales while identifying significant limitations in current attribution methodologies that require future advances.

</details>


### [164] [AdaNODEs: Test Time Adaptation for Time Series Forecasting Using Neural ODEs](https://arxiv.org/abs/2601.12893)
*Ting Dang,Soumyajit Chatterjee,Hong Jia,Yu Wu,Flora Salim,Fahim Kawsar*

Main category: cs.LG

TL;DR: AdaNODEs是一种针对时间序列预测的源自由测试时间适应方法，利用神经常微分方程处理分布偏移，仅需更新少量参数即可显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时间适应方法主要针对独立数据，忽视了时间序列数据的特性和预测任务的需求，需要专门针对时间序列预测的适应方法。

Method: 提出基于神经常微分方程的源自由测试时间适应框架，设计了新的损失函数来处理预测任务，仅更新有限模型参数以降低内存消耗。

Result: 在一维和高维数据实验中，相比现有最优方法分别取得5.88%和28.4%的相对改进，在更严重的分布偏移下表现出更强的鲁棒性。

Conclusion: AdaNODEs为时间序列预测任务提供了一种有效的测试时间适应方法，能够有效处理时间依赖性和分布偏移，同时保持较低的计算开销。

Abstract: Test time adaptation (TTA) has emerged as a promising solution to adapt pre-trained models to new, unseen data distributions using unlabeled target domain data. However, most TTA methods are designed for independent data, often overlooking the time series data and rarely addressing forecasting tasks. This paper presents AdaNODEs, an innovative source-free TTA method tailored explicitly for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), we propose a novel adaptation framework that accommodates the unique characteristics of distribution shifts in time series data. Moreover, we innovatively propose a new loss function to tackle TTA for forecasting tasks. AdaNODEs only requires updating limited model parameters, showing effectiveness in capturing temporal dependencies while avoiding significant memory usage. Extensive experiments with one- and high-dimensional data demonstrate that AdaNODEs offer relative improvements of 5.88\% and 28.4\% over the SOTA baselines, especially demonstrating robustness across higher severity distribution shifts.

</details>


### [165] [Supervised Learning for the (s,S) Inventory Model with General Interarrival Demands and General Lead Times](https://arxiv.org/abs/2601.12900)
*Eliran Sherzer,Yonit Barron*

Main category: cs.LG

TL;DR: 使用神经网络监督学习框架近似(s,S)库存系统的稳态性能指标，替代昂贵的仿真计算


<details>
  <summary>Details</summary>
Motivation: 传统(s,S)库存模型在非马尔可夫系统中分析困难，依赖昂贵的仿真计算来评估长期性能指标

Method: 提出基于神经网络的监督学习框架：先用仿真生成训练标签，然后用少量低阶矩作为输入训练神经网络，预测库存水平稳态分布、期望周期时间、缺货概率等指标

Result: 神经网络能快速准确预测各种系统指标，在广泛参数范围内表现出高精度，有效替代重复昂贵的仿真运行

Conclusion: 该框架为分析复杂随机系统提供了高效快速替代方案，易于扩展到其他库存模型

Abstract: The continuous-review (s,S) inventory model is a cornerstone of stochastic inventory theory, yet its analysis becomes analytically intractable when dealing with non-Markovian systems. In such systems, evaluating long-run performance measures typically relies on costly simulation.
  This paper proposes a supervised learning framework via a neural network model for approximating stationary performance measures of (s,S) inventory systems with general distributions for the interarrival time between demands and lead times under lost sales. Simulations are first used to generate training labels, after which the neural network is trained. After training, the neural network provides almost instantaneous predictions of various metrics of the system, such as the stationary distribution of inventory levels, the expected cycle time, and the probability of lost sales. We find that using a small number of low-order moments of the distributions as input is sufficient to train the neural networks and to accurately capture the steady-state distribution. Extensive numerical experiments demonstrate high accuracy over a wide range of system parameters. As such, it effectively replaces repeated and costly simulation runs. Our framework is easily extendable to other inventory models, offering an efficient and fast alternative for analyzing complex stochastic systems.

</details>


### [166] [Deep Temporal Graph Clustering: A Comprehensive Benchmark and Datasets](https://arxiv.org/abs/2601.12903)
*Meng Liu,Ke Liang,Siwei Wang,Xingchen Hu,Sihang Zhou,Xinwang Liu*

Main category: cs.LG

TL;DR: 本文提出了一个名为BenchTGC的时序图聚类（TGC）综合基准，包括框架和数据集，解决了现有聚类技术不适用和数据集缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 时序图聚类是一个新兴但关注度低的任务，相比静态图聚类，它通过基于交互序列的批处理模式能在时间需求和空间需求之间找到平衡。然而，现有聚类技术不适用和缺乏合适数据集两大挑战阻碍了TGC的发展。

Method: 提出了BenchTGC基准，包括：1）设计BenchTGC框架来说明时序图聚类的范式；2）改进现有聚类技术以适应时序图；3）开发了多个适合TGC任务的BenchTGC数据集。

Result: 通过大量实验验证了BenchTGC的优势，并证明了时序图聚类任务的必要性和重要性。现实世界中动态变化和复杂场景是时序图聚类的基础。

Conclusion: BenchTGC为解决时序图聚类面临的挑战提供了综合解决方案，包括框架、改进算法和专用数据集，推动了该领域的发展。

Abstract: Temporal Graph Clustering (TGC) is a new task with little attention, focusing on node clustering in temporal graphs. Compared with existing static graph clustering, it can find the balance between time requirement and space requirement (Time-Space Balance) through the interaction sequence-based batch-processing pattern. However, there are two major challenges that hinder the development of TGC, i.e., inapplicable clustering techniques and inapplicable datasets. To address these challenges, we propose a comprehensive benchmark, called BenchTGC. Specially, we design a BenchTGC Framework to illustrate the paradigm of temporal graph clustering and improve existing clustering techniques to fit temporal graphs. In addition, we also discuss problems with public temporal graph datasets and develop multiple datasets suitable for TGC task, called BenchTGC Datasets. According to extensive experiments, we not only verify the advantages of BenchTGC, but also demonstrate the necessity and importance of TGC task. We wish to point out that the dynamically changing and complex scenarios in real world are the foundation of temporal graph clustering. The code and data is available at: https://github.com/MGitHubL/BenchTGC.

</details>


### [167] [CooperLLM: Cloud-Edge-End Cooperative Federated Fine-tuning for LLMs via ZOO-based Gradient Correction](https://arxiv.org/abs/2601.12917)
*He Sun,Jinrui Zhou,Li Li,Mingjun Xiao*

Main category: cs.LG

TL;DR: CooperLLM：云辅助的边端协同联邦微调框架，结合移动设备上的零阶优化和云端梯度校正，显著降低内存占用、加速收敛并提升精度。


<details>
  <summary>Details</summary>
Motivation: LLMs在移动设备上微调面临内存和计算成本高的挑战，现有联邦学习方法要么依赖内存密集的反向传播，要么使用收敛慢、精度低的零阶优化方法。

Method: 提出CooperLLM框架：移动客户端在私有数据上执行轻量级零阶优化更新，云端在辅助公共数据上使用反向传播微调，并通过注入引导扰动来校正本地更新。采用流水线调度和自适应压缩来优化系统性能。

Result: 在多个Transformer模型和数据集上的实验表明，CooperLLM将设备内存降低达86.4%，加速收敛8.8倍，相比最先进的ZOO基线方法精度提升达10个百分点。

Conclusion: CooperLLM通过云辅助的边端协同设计，有效解决了移动设备上LLM联邦微调的内存、收敛和精度问题，实现了隐私保护下的高效个性化。

Abstract: Large Language Models (LLMs) perform well on many NLP tasks, but fine-tuning them on resource-constrained mobile devices is challenging due to high memory and computation costs, despite growing demands for privacy-preserving personalization. Federated Learning (FL) enables local-data training, yet existing methods either rely on memory-intensive backpropagation or use zeroth-order optimization (ZOO), which avoids backward passes but suffers from slow convergence and degraded accuracy. We propose CooperLLM, a cloud-assisted edge-end cooperative federated fine-tuning framework that combines ZOO on mobile devices with cloud-guided gradient rectification. Mobile clients perform lightweight ZOO updates on private data, while the cloud fine-tunes on auxiliary public data using backpropagation and injects guided perturbations to rectify local updates, improving convergence and accuracy without violating privacy. To address system bottlenecks, CooperLLM introduces pipeline scheduling and adaptive compression to overlap computation and communication and reduce memory usage. Experiments on multiple Transformer models and datasets show that CooperLLM reduces on-device memory by up to $86.4\%$, accelerates convergence by $8.8 \times$, and improves accuracy by up to 10 percentage points over state-of-the-art ZOO-based baselines.

</details>


### [168] [An efficient heuristic for geometric analysis of cell deformations](https://arxiv.org/abs/2601.12928)
*Yaima Paz Soto,Silena Herold Garcia,Ximo Gual-Arnau,Antoni Jaume-i-Capó,Manuel González-Hidalgo*

Main category: cs.LG

TL;DR: 提出基于形状空间和弹性距离的镰状细胞自动分类方法，通过固定参数化和模板对齐简化计算，在监督分类和无监督聚类中达到96.03%准确率


<details>
  <summary>Details</summary>
Motivation: 镰状细胞病导致红细胞变形，影响血液流动和氧气输送，全球患病率高且医疗负担重。自动分类可减少专家工作量、避免量化误差，对评估病情严重程度至关重要

Method: 将红细胞建模为形状空间中的闭合平面曲线，使用弹性距离（对旋转、平移、缩放和重参数化不变）。改进包括：(1)基于细胞主轴使用固定参数化计算距离，(2)在计算距离前将每个细胞与两个模板对齐。相比在所有可能参数化中最小化距离，这种方法简化了计算

Result: 在监督分类和无监督聚类中均达到96.03%的准确率。方法在保持或提高形状空间模型准确性的同时，显著降低了计算成本

Conclusion: 该方法实现了高效的红细胞分类，通过固定参数化和模板对齐策略，在保证高准确率的同时大幅减少计算复杂度，适用于资源有限地区的医疗系统

Abstract: Sickle cell disease causes erythrocytes to become sickle-shaped, affecting their movement in the bloodstream and reducing oxygen delivery. It has a high global prevalence and places a significant burden on healthcare systems, especially in resource-limited regions. Automated classification of sickle cells in blood images is crucial, allowing the specialist to reduce the effort required and avoid errors when quantifying the deformed cells and assessing the severity of a crisis. Recent studies have proposed various erythrocyte representation and classification methods. Since classification depends solely on cell shape, a suitable approach models erythrocytes as closed planar curves in shape space. This approach employs elastic distances between shapes, which are invariant under rotations, translations, scaling, and reparameterizations, ensuring consistent distance measurements regardless of the curves' position, starting point, or traversal speed. While previous methods exploiting shape space distances had achieved high accuracy, we refined the model by considering the geometric characteristics of healthy and sickled erythrocytes. Our method proposes (1) to employ a fixed parameterization based on the major axis of each cell to compute distances and (2) to align each cell with two templates using this parameterization before computing distances. Aligning shapes to templates before distance computation, a concept successfully applied in areas such as molecular dynamics, and using a fixed parameterization, instead of minimizing distances across all possible parameterizations, simplifies calculations. This strategy achieves 96.03\% accuracy rate in both supervised classification and unsupervised clustering. Our method ensures efficient erythrocyte classification, maintaining or improving accuracy over shape space models while significantly reducing computational costs.

</details>


### [169] [Deterministic Dynamics of Sampling Processes in Score-Based Diffusion Models with Multiplicative Noise Conditioning](https://arxiv.org/abs/2601.12965)
*Doheon Kim*

Main category: cs.LG

TL;DR: 基于分数的扩散模型通过乘积结构学习分数函数，虽然无法完全学习正确分数，但实践中表现良好，本文从确定性动力学角度提供理论解释


<details>
  <summary>Details</summary>
Motivation: Song和Ermon(2020)发现使用乘性噪声调节的神经网络仍能生成满意样本，但这种乘积结构限制了模型表示空间变量与噪声之间更一般关系的能力，无法完全学习正确分数。尽管有这种限制，模型在实践中表现良好，需要理论解释这一现象。

Method: 通过研究相关微分方程的确定性动力学，分析模型如何操作，为这种现象提供理论解释。关注乘积结构模型在无法学习正确分数的情况下仍能有效工作的机制。

Result: 为乘积结构扩散模型在无法完全学习正确分数的情况下仍能生成良好样本的现象提供了理论解释，揭示了确定性动力学的作用机制。

Conclusion: 即使乘积结构限制了模型学习正确分数函数的能力，通过分析相关微分方程的确定性动力学，可以解释为什么这些模型在实践中仍能有效工作，为扩散模型的理论理解提供了新视角。

Abstract: Score-based diffusion models generate new samples by learning the score function associated with a diffusion process. While the effectiveness of these models can be theoretically explained using differential equations related to the sampling process, previous work by Song and Ermon (2020) demonstrated that neural networks using multiplicative noise conditioning can still generate satisfactory samples. In this setup, the model is expressed as the product of two functions: one depending on the spatial variable and the other on the noise magnitude. This structure limits the model's ability to represent a more general relationship between the spatial variable and the noise, indicating that it cannot fully learn the correct score. Despite this limitation, the models perform well in practice. In this work, we provide a theoretical explanation for this phenomenon by studying the deterministic dynamics of the associated differential equations, offering insight into how the model operates.

</details>


### [170] [Architecture-Optimization Co-Design for Physics-Informed Neural Networks Via Attentive Representations and Conflict-Resolved Gradients](https://arxiv.org/abs/2601.12971)
*Pancheng Niu,Jun Guo,Qiaolin He,Yongming Chen,Yanchao Shi*

Main category: cs.LG

TL;DR: 提出了ACR-PINN方法，通过层动态注意力机制增强表示能力，并采用冲突解决梯度更新策略优化训练，显著提升了PINNs在求解PDE问题中的收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在求解偏微分方程时面临表示能力有限和优化困难的问题，特别是物理约束之间的竞争和梯度冲突会阻碍训练效果。

Method: 1. 提出层动态注意力机制(LDA-PINN)增强表示灵活性；2. 将PINN训练重构为多任务学习问题，引入冲突解决梯度更新策略(GC-PINN)；3. 整合两者形成ACR-PINN，结合注意力表示和冲突感知优化。

Result: 在Burgers、Helmholtz、Klein-Gordon和腔体驱动流等基准PDE问题上，ACR-PINN相比标准PINNs实现了更快的收敛速度和显著更低的相对L2和L∞误差。

Conclusion: 架构-优化协同设计能有效提升PINN求解器的鲁棒性和准确性，为改进基于PINN的PDE求解器提供了有效途径。

Abstract: Physics-Informed Neural Networks (PINNs) provide a learning-based framework for solving partial differential equations (PDEs) by embedding governing physical laws into neural network training. In practice, however, their performance is often hindered by limited representational capacity and optimization difficulties caused by competing physical constraints and conflicting gradients. In this work, we study PINN training from a unified architecture-optimization perspective. We first propose a layer-wise dynamic attention mechanism to enhance representational flexibility, resulting in the Layer-wise Dynamic Attention PINN (LDA-PINN). We then reformulate PINN training as a multi-task learning problem and introduce a conflict-resolved gradient update strategy to alleviate gradient interference, leading to the Gradient-Conflict-Resolved PINN (GC-PINN). By integrating these two components, we develop the Architecture-Conflict-Resolved PINN (ACR-PINN), which combines attentive representations with conflict-aware optimization while preserving the standard PINN loss formulation. Extensive experiments on benchmark PDEs, including the Burgers, Helmholtz, Klein-Gordon, and lid-driven cavity flow problems, demonstrate that ACR-PINN achieves faster convergence and significantly lower relative $L_2$ and $L_\infty$ errors than standard PINNs. These results highlight the effectiveness of architecture-optimization co-design for improving the robustness and accuracy of PINN-based solvers.

</details>


### [171] [PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient](https://arxiv.org/abs/2601.12988)
*Zijian Wang,Tiancheng Huang,Hanqi Li,Da Ma,Lu Chen,Kai Yu*

Main category: cs.LG

TL;DR: PaperCompass框架通过分离高层规划与细粒度执行，使用Draft-and-Follow Policy Optimization训练LLM，在科学论文问答任务中提高效率而不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 科学文献的快速增长使得研究人员难以通过手动阅读跟踪新进展。现有基于LLM的自主代理方法要么依赖大量工程化提示，要么使用传统SFT-RL训练流程，都容易导致过度探索和低效产出。

Method: 提出PaperCompass框架，将高层规划与细粒度执行分离：先起草明确计划，然后通过详细推理实例化每个步骤。引入Draft-and-Follow Policy Optimization（DFPO）联合优化计划草稿和最终解决方案，这是一种轻量级的分层强化学习方法。

Result: 在论文问答基准测试中，PaperCompass在保持性能的同时提高了效率，取得了与更大模型相当的结果。

Conclusion: PaperCompass通过分离规划与执行的认知科学启发方法，结合DFPO优化，有效缩小了LLM的"知行差距"，为科学文献处理提供了高效解决方案。

Abstract: The accelerating growth of the scientific literature makes it increasingly difficult for researchers to track new advances through manual reading alone. Recent progress in large language models (LLMs) has therefore spurred interest in autonomous agents that can read scientific papers and extract task-relevant information. However, most existing approaches rely either on heavily engineered prompting or on a conventional SFT-RL training pipeline, both of which often lead to excessive and low-yield exploration. Drawing inspiration from cognitive science, we propose PaperCompass, a framework that mitigates these issues by separating high-level planning from fine-grained execution. PaperCompass first drafts an explicit plan that outlines the intended sequence of actions, and then performs detailed reasoning to instantiate each step by selecting the parameters for the corresponding function calls. To train such behavior, we introduce Draft-and-Follow Policy Optimization (DFPO), a tailored RL method that jointly optimizes both the draft plan and the final solution. DFPO can be viewed as a lightweight form of hierarchical reinforcement learning, aimed at narrowing the `knowing-doing' gap in LLMs. We provide a theoretical analysis that establishes DFPO's favorable optimization properties, supporting a stable and reliable training process. Experiments on paper-based question answering (Paper-QA) benchmarks show that PaperCompass improves efficiency over strong baselines without sacrificing performance, achieving results comparable to much larger models.

</details>


### [172] [HT-GNN: Hyper-Temporal Graph Neural Network for Customer Lifetime Value Prediction in Baidu Ads](https://arxiv.org/abs/2601.13013)
*Xiaohui Zhao,Xinjian Zhao,Jiahui Zhang,Guoyu Liu,Houzhi Wang,Shu Wu*

Main category: cs.LG

TL;DR: 提出HT-GNN模型解决新闻流广告中的LTV预测问题，通过超图监督模块、Transformer时序编码器和任务自适应专家混合机制，有效处理用户群体异质性和动态行为序列。


<details>
  <summary>Details</summary>
Motivation: 新闻流广告中的LTV预测面临两大挑战：1) 基于人口统计的目标定位导致不同用户群体的LTV分布差异巨大；2) 动态营销策略产生不规则的、快速演变的用户行为序列。

Method: 提出超时序图神经网络(HT-GNN)，包含三个核心组件：1) 超图监督模块捕捉用户群体间关系；2) 基于Transformer的时序编码器带自适应权重；3) 任务自适应专家混合机制，使用动态预测塔进行多时间范围LTV预测。

Result: 在百度广告平台的1500万用户数据上实验表明，HT-GNN在所有指标和预测时间范围上都持续优于现有最先进方法。

Conclusion: HT-GNN通过联合建模人口统计异质性和时序动态，有效解决了新闻流广告中的LTV预测挑战，为长期收入增长优化提供了有力工具。

Abstract: Lifetime value (LTV) prediction is crucial for news feed advertising, enabling platforms to optimize bidding and budget allocation for long-term revenue growth. However, it faces two major challenges: (1) demographic-based targeting creates segment-specific LTV distributions with large value variations across user groups; and (2) dynamic marketing strategies generate irregular behavioral sequences where engagement patterns evolve rapidly. We propose a Hyper-Temporal Graph Neural Network (HT-GNN), which jointly models demographic heterogeneity and temporal dynamics through three key components: (i) a hypergraph-supervised module capturing inter-segment relationships; (ii) a transformer-based temporal encoder with adaptive weighting; and (iii) a task-adaptive mixture-of-experts with dynamic prediction towers for multi-horizon LTV forecasting. Experiments on \textit{Baidu Ads} with 15 million users demonstrate that HT-GNN consistently outperforms state-of-the-art methods across all metrics and prediction horizons.

</details>


### [173] [PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning](https://arxiv.org/abs/2601.13020)
*Zhiyan Hou,Haiyun Guo,Haokai Ma,Yandu Sun,Yonghui Yang,Jinqiao Wang*

Main category: cs.LG

TL;DR: 提出PASs（路径激活子空间）方法解决多模态大语言模型持续指令调优中的专家共漂移问题，通过路径激活信号校准路由并稳定重要秩方向，在保持参数不变的情况下提升准确性和抗遗忘能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的混合专家方法在持续指令调优中，路由器和专家会共同漂移，导致早期输入-专家专业化逐渐偏离，专家职责模糊并加剧遗忘问题。

Method: 提出路径激活子空间（PASs）作为能力对齐的坐标系统，包含PAS引导的重加权（用路径激活信号校准路由）和PAS感知的秩稳定化（选择性稳定对先前任务重要的秩方向）。

Result: 在持续指令调优基准测试中，该方法在准确性和抗遗忘方面均优于传统持续学习基线和MoE-LoRA变体，且不增加额外参数。

Conclusion: PASs方法通过解耦路由和专家更新，有效解决了专家共漂移问题，为持续指令调优提供了更稳定和高效的解决方案。

Abstract: Continual instruction tuning (CIT) requires multimodal large language models (MLLMs) to adapt to a stream of tasks without forgetting prior capabilities. A common strategy is to isolate updates by routing inputs to different LoRA experts. However, existing LoRA-based Mixture-of-Experts (MoE) methods often jointly update the router and experts in an indiscriminate way, causing the router's preferences to co-drift with experts' adaptation pathways and gradually deviate from early-stage input-expert specialization. We term this phenomenon Misaligned Co-drift, which blurs expert responsibilities and exacerbates forgetting.To address this, we introduce the pathway activation subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, providing a capability-aligned coordinate system for routing and preservation. Based on PASs, we propose a fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided Reweighting, which calibrates routing using each expert's pathway activation signals, and PAS-aware Rank Stabilization, which selectively stabilizes rank directions important to previous tasks. Experiments on a CIT benchmark show that our approach consistently outperforms a range of conventional continual learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting without adding parameters. Our code will be released upon acceptance.

</details>


### [174] [Enhancing Generalization in Sickle Cell Disease Diagnosis through Ensemble Methods and Feature Importance Analysis](https://arxiv.org/abs/2601.13021)
*Nataša Petrović,Gabriel Moyà-Alcover,Antoni Jaume-i-Capó,Jose Maria Buades Rubio*

Main category: cs.LG

TL;DR: 提出一种基于集成学习的镰状细胞病诊断支持系统，通过特征选择和模型优化实现更好的泛化性能


<details>
  <summary>Details</summary>
Motivation: 开发一个能够为镰状细胞病提供诊断支持的自动化系统，专注于提高模型的泛化能力和可解释性，以辅助医疗诊断

Method: 对血液涂片图像进行预处理和分割，提取高质量特征；使用集成机器学习方法（随机森林和极端随机树）进行分类；设计特征重要性分析方法来识别关键特征；在新数据集上进行验证

Result: 随机森林和极端随机树集成模型取得了F1分数90.71%和SDS分数93.33%的优异性能，相比梯度提升分类器（F1 87.32%，SDS 89.51%）有明显提升

Conclusion: 提出的集成学习方法在镰状细胞病诊断支持中表现出优越的泛化能力和性能，通过特征选择降低了模型复杂性并提高了可解释性，为临床诊断提供了有效的辅助工具

Abstract: This work presents a novel approach for selecting the optimal ensemble-based classification method and features with a primarly focus on achieving generalization, based on the state-of-the-art, to provide diagnostic support for Sickle Cell Disease using peripheral blood smear images of red blood cells. We pre-processed and segmented the microscopic images to ensure the extraction of high-quality features. To ensure the reliability of our proposed system, we conducted an in-depth analysis of interpretability. Leveraging techniques established in the literature, we extracted features from blood cells and employed ensemble machine learning methods to classify their morphology. Furthermore, we have devised a methodology to identify the most critical features for classification, aimed at reducing complexity and training time and enhancing interpretability in opaque models. Lastly, we validated our results using a new dataset, where our model overperformed state-of-the-art models in terms of generalization. The results of classifier ensembled of Random Forest and Extra Trees classifier achieved an harmonic mean of precision and recall (F1-score) of 90.71\% and a Sickle Cell Disease diagnosis support score (SDS-score) of 93.33\%. These results demonstrate notable enhancement from previous ones with Gradient Boosting classifier (F1-score 87.32\% and SDS-score 89.51\%). To foster scientific progress, we have made available the parameters for each model, the implemented code library, and the confusion matrices with the raw data.

</details>


### [175] [Analysis of Long Range Dependency Understanding in State Space Models](https://arxiv.org/abs/2601.13048)
*Srividya Ravikumar,Abhinav Anand,Shweta Verma,Mira Mezini*

Main category: cs.LG

TL;DR: 首次对S4D模型进行系统性核可解释性研究，分析其在真实世界任务（源代码漏洞检测）中的表现，发现不同架构下S4D核表现出不同的滤波特性，影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然状态空间模型在长序列任务中表现出色，但现有研究主要关注预测准确性而非可解释性。本研究旨在填补这一空白，首次对S4D模型进行系统性核可解释性分析。

Method: 在真实世界任务（源代码漏洞检测）上训练S4D模型，通过时域和频域分析S4D核的特性，研究不同模型架构下核的行为差异。

Result: 发现S4D的长程建模能力在不同架构下差异显著，影响模型性能。S4D核可表现为低通、带通或高通滤波器，具体取决于架构设计。

Conclusion: S4D模型的可解释性分析为未来设计更好的S4D基模型提供了指导，强调了架构选择对模型滤波特性和性能的重要影响。

Abstract: Although state-space models (SSMs) have demonstrated strong performance on long-sequence benchmarks, most research has emphasized predictive accuracy rather than interpretability. In this work, we present the first systematic kernel interpretability study of the diagonalized state-space model (S4D) trained on a real-world task (vulnerability detection in source code). Through time and frequency domain analysis of the S4D kernel, we show that the long-range modeling capability of S4D varies significantly under different model architectures, affecting model performance. For instance, we show that the depending on the architecture, S4D kernel can behave as low-pass, band-pass or high-pass filter. The insights from our analysis can guide future work in designing better S4D-based models.

</details>


### [176] [TinyML-Enabled IoT for Sustainable Precision Irrigation](https://arxiv.org/abs/2601.13054)
*Kamogelo Taueatsoala,Caitlyn Daniels,Angelina J. Ramsunar,Petrus Bronkhorst,Absalom E. Ezugwu*

Main category: cs.LG

TL;DR: 提出基于TinyML的边缘物联网框架，用于小农场的智能精准灌溉，无需云端依赖，显著减少用水量


<details>
  <summary>Details</summary>
Motivation: 解决小规模农场面临的水资源短缺、气候不稳定以及缺乏先进、可负担农业技术的问题

Method: 四层边缘物联网架构，使用ESP32微控制器作为边缘推理节点，Raspberry Pi作为本地边缘服务器，集成多种环境传感器，采用梯度提升模型并转换为TinyML部署，基于MQTT的局域网通信协议

Result: 梯度提升模型表现最佳（R²=0.9973，MAPE=0.99%），部署后灌溉需求预测准确率高（MAPE<1%），实验显示相比传统方法显著减少用水量

Conclusion: 该框架为资源受限的农村环境提供了实用、经济高效的精准灌溉解决方案，通过设备端人工智能提升水资源利用效率

Abstract: Small-scale farming communities are disproportionately affected by water scarcity, erratic climate patterns, and a lack of access to advanced, affordable agricultural technologies. To address these challenges, this paper presents a novel, edge-first IoT framework that integrates Tiny Machine Learning (TinyML) for intelligent, offline-capable precision irrigation. The proposed four-layer architecture leverages low-cost hardware, an ESP32 microcontroller as an edge inference node, and a Raspberry Pi as a local edge server to enable autonomous decision-making without cloud dependency. The system utilizes capacitive soil moisture, temperature, humidity, pH, and ambient light sensors for environmental monitoring. A rigorous comparative analysis of ensemble models identified gradient boosting as superior, achieving an R^2 score of 0.9973 and a Mean Absolute Percentage Error (MAPE) of 0.99%, outperforming a random forest model (R^2 = 0.9916, MAPE = 1.81%). This optimized model was converted and deployed as a lightweight TinyML inference engine on the ESP32 and predicts irrigation needs with exceptional accuracy (MAPE < 1%). Local communication is facilitated by an MQTT-based LAN protocol, ensuring reliable operation in areas with limited or no internet connectivity. Experimental validation in a controlled environment demonstrated a significant reduction in water usage compared to traditional methods, while the system's low-power design and offline functionality confirm its viability for sustainable, scalable deployment in resource-constrained rural settings. This work provides a practical, cost-effective blueprint for bridging the technological divide in agriculture and enhancing water-use efficiency through on-device artificial intelligence.

</details>


### [177] [METIS: Mentoring Engine for Thoughtful Inquiry & Solutions](https://arxiv.org/abs/2601.13075)
*Abhinav Rajeev Kumar,Dhruv Trehan,Paras Chopra*

Main category: cs.LG

TL;DR: METIS是一个面向本科生的AI研究导师系统，通过阶段感知的辅助工具（文献搜索、指南、方法检查、记忆）帮助学生从想法到论文写作，在多个评估中表现优于GPT-5和Claude Sonnet 4.5。


<details>
  <summary>Details</summary>
Motivation: 许多学生缺乏专家研究指导，需要AI导师帮助他们从研究想法发展到完整论文。

Method: 构建METIS系统，包含工具增强、阶段感知的助手，具有文献搜索、精选指南、方法学检查和记忆功能。使用LLM作为评判者进行成对偏好评估、学生角色评分标准、短多轮辅导以及证据/合规性检查。

Result: 在90个单轮提示中，LLM评判者更偏好METIS（vs Claude Sonnet 4.5 71%，vs GPT-5 54%）。学生评分（清晰度/可操作性/约束匹配）在所有阶段都更高。在多轮会话中，METIS的最终质量略高于GPT-5。优势集中在文档基础阶段（D-F）。

Conclusion: METIS作为AI研究导师在帮助学生完成研究论文方面表现有效，特别是在文档基础阶段。失败模式包括过早工具路由、浅层基础和偶尔的阶段分类错误。

Abstract: Many students lack access to expert research mentorship. We ask whether an AI mentor can move undergraduates from an idea to a paper. We build METIS, a tool-augmented, stage-aware assistant with literature search, curated guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, student-persona rubrics, short multi-turn tutoring, and evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores (clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly higher final quality than GPT-5. Gains concentrate in document-grounded stages (D-F), consistent with stage-aware routing and groundings failure modes include premature tool routing, shallow grounding, and occasional stage misclassification.

</details>


### [178] [Recursive Meta-Distillation: An Axiomatic Framework for Iterative Knowledge Refinement](https://arxiv.org/abs/2601.13100)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 本文提出了递归元蒸馏的算子理论框架，将迭代知识蒸馏形式化为概率分布算子序列，证明了在温和条件下锚定递归蒸馏能诱导KL散度收缩，收敛到基础教师分布的唯一全局吸引不动点。


<details>
  <summary>Details</summary>
Motivation: 现有概率域知识蒸馏研究主要关注单阶段设置，但递归或多代蒸馏的数学行为理解不足，先前方法主要依赖经验启发式。需要建立理论框架来理解递归蒸馏何时是数学良定义且收敛的，而不是误差累积的。

Method: 引入递归元蒸馏的公理化和算子理论框架，将迭代知识蒸馏形式化为具有显式锚定到基础教师的概率分布算子序列。定义有效元教师构建的结构公理，证明满足这些公理的非平凡算子族存在性，不依赖特定算法或损失函数。

Result: 在温和可实现性和凸性假设下，锚定递归蒸馏诱导KL散度收缩，产生几何收敛到基础教师分布，并存在唯一全局吸引不动点。框架独立于模型架构、优化细节或特定算子实例化。

Conclusion: 该框架是基础性而非算法性的贡献，刻画了递归蒸馏何时是数学良定义且收敛的，为理解容量约束下迭代和多教师蒸馏的稳定性、偏差-方差行为和失效模式提供了理论基础。

Abstract: Recent work in probability-domain knowledge distillation has established axiomatic frameworks for temperature scaling, multi-teacher aggregation, and bias-variance trade-offs in single-stage settings. However, the mathematical behavior of recursive or multi-generation distillation remains poorly understood, with prior approaches relying primarily on empirical heuristics. In this work, we introduce an axiomatic and operator-theoretic framework for recursive meta-distillation, formalizing iterative knowledge distillation as a sequence of probability-distribution operators with explicit anchoring to base teachers.
  We define structural axioms for valid meta-teacher construction and prove the existence of non-trivial operator families satisfying these axioms without specifying particular algorithms or loss functions. Under mild realizability and convexity assumptions, we show that anchored recursive distillation induces contraction in KL divergence, yielding geometric convergence to base teacher distributions and a unique, globally attractive fixed point.
  The contribution is foundational rather than algorithmic: the framework characterizes when recursive distillation is mathematically well-posed and convergent rather than error-accumulating, independent of model architecture, optimization details, or specific operator instantiations. These results provide a theoretical basis for understanding stability, bias-variance behavior, and failure modes in iterative and multi-teacher distillation under capacity constraints.

</details>


### [179] [FastAV: Efficient Token Pruning for Audio-Visual Large Language Model Inference](https://arxiv.org/abs/2601.13143)
*Chaeyoung Jung,Youngjoon Jang,Seungwoo Lee,Joon Son Chung*

Main category: cs.LG

TL;DR: FastAV是首个针对音频-视觉大语言模型（AV-LLMs）的token剪枝框架，通过两阶段剪枝策略减少40%以上计算量，同时保持或提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然token剪枝在标准LLMs和视觉语言模型中已有探索，但在AV-LLMs中却很少被研究。多模态整合显著增加了token需求，因此需要专门针对AV-LLMs的高效剪枝方法。

Method: 基于注意力权重识别不同阶段的重要token，采用两阶段剪枝策略：1）中间层进行全局剪枝去除影响力较小的token；2）后续层进行精细剪枝，考虑对下一个token生成的影响。该方法不依赖完整注意力图，与FlashAttention等高效注意力机制完全兼容。

Result: 在两个代表性AV-LLMs上，FastAV减少了超过40%的FLOPs，同时保持甚至提升了模型性能。

Conclusion: FastAV为AV-LLMs提供了首个有效的token剪枝框架，显著降低了计算成本，同时保持了模型性能，为多模态大语言模型的高效部署提供了解决方案。

Abstract: In this work, we present FastAV, the first token pruning framework tailored for audio-visual large language models (AV-LLMs). While token pruning has been actively explored in standard large language models (LLMs) and vision-language models (LVLMs), its application to AV-LLMs has received little attention, even though multimodal integration substantially increases their token demands. To address this gap, we introduce a pruning strategy that utilizes attention weights to identify tokens emphasized at different stages and estimates their importance. Building on this analysis, FastAV applies a two-stage pruning strategy: (1) global pruning in intermediate layers to remove broadly less influential tokens, and (2) fine pruning in later layers considering the impact on next token generation. Notably, our method does not rely on full attention maps, which makes it fully compatible with efficient attention mechanisms such as FlashAttention. Extensive experiments demonstrate that FastAV reduces FLOPs by more than 40% on two representative AV-LLMs, while preserving or even improving model performance.

</details>


### [180] [Training instability in deep learning follows low-dimensional dynamical principles](https://arxiv.org/abs/2601.13160)
*Zhipeng Zhang,Zhenjie Yao,Kai Li,Lei Yang*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的动力学视角，将训练稳定性作为学习系统的内在属性，通过受控扰动审计识别训练稳定性规律，发现最终性能与训练稳定性经常解耦，受控随机性可缓冲学习动态，低维潜在元状态偏差可预测性能崩溃。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统虽然取得了显著的实证性能，但训练过程本身的稳定性仍然理解不足。训练作为高维动力系统，对优化、数据、参数或学习信号的微小扰动可能导致突然且不可逆的崩溃，损害可重复性和可扩展性。

Method: 提出了统一的动力学视角，将训练稳定性组织为四个相互作用的维度：优化稳定性、环境/数据稳定性、参数稳定性和学习信号稳定性。通过受控扰动审计训练轨迹来操作化这一视角，在不修改学习算法的情况下探测学习动态对结构化扰动的响应。

Result: 在强化学习和大型语言模型训练中发现了三个重复出现的规律：1) 最终高性能经常与训练稳定性解耦；2) 受控随机性在不同范式中一致地缓冲学习动态；3) 低维潜在元状态的偏差系统地先于可观察的性能崩溃。

Conclusion: 这些发现确立了训练稳定性作为学习系统可测量和可比较的动力学属性，为超越最终性能结果研究学习动态提供了描述性基础。

Abstract: Deep learning systems achieve remarkable empirical performance, yet the stability of the training process itself remains poorly understood. Training unfolds as a high-dimensional dynamical system in which small perturbations to optimization, data, parameters, or learning signals can induce abrupt and irreversible collapse, undermining reproducibility and scalability.
  We propose a unified dynamical perspective that characterizes training stability as an intrinsic property of learning systems, organized along four interacting dimensions: optimization, environmental/data, parametric, and learning-signal stability. We operationalize this perspective through controlled perturbation auditing of training trajectories, probing how learning dynamics respond to structured disturbances without modifying learning algorithms.
  Across reinforcement learning and large language model training, we identify three recurring regularities: high final performance is frequently decoupled from training stability; controlled stochasticity consistently buffers learning dynamics across paradigms; and deviations in low-dimensional latent meta-states systematically precede observable performance collapse. Together, these findings establish training stability as a measurable and comparable dynamical property of learning systems, providing a descriptive foundation for studying learning dynamics beyond final performance outcomes.

</details>


### [181] [NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness](https://arxiv.org/abs/2601.13162)
*Ali Shafiee Sarvestani,Jason Schmidt,Arman Roohi*

Main category: cs.LG

TL;DR: Neuro-symbolic框架DesignII通过符号规则监督增强神经网络对抗鲁棒性和可解释性，在GTSRB数据集上相比标准对抗训练获得3倍鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络存在对抗脆弱性和缺乏可解释性的关键限制，特别是在自动驾驶等安全敏感场景中，需要同时提升鲁棒性和可解释性。

Method: 提出DesignII神经符号框架，将领域知识编码为形状、颜色等外观属性的逻辑约束，通过语义和符号逻辑损失在训练中强制执行，使用FGSM和PGD对抗训练变体。

Result: 在ε=8/255的扰动预算下，FGSM-Neuro-Symbolic和PGD-Neuro-Symbolic模型相比对应对抗训练基线分别提升18.1%和17.35%的对抗准确率，比标准对抗训练提供约3倍的鲁棒性增益，且不降低干净样本准确率。

Conclusion: 符号推理为构建鲁棒且可解释的AI提供了有效路径，神经符号方法在保持轻量架构的同时实现了与复杂Transformer防御相当或更优的鲁棒性。

Abstract: Adversarial vulnerability and lack of interpretability are critical limitations of deep neural networks, especially in safety-sensitive settings such as autonomous driving. We introduce \DesignII, a neuro-symbolic framework that integrates symbolic rule supervision into neural networks to enhance both adversarial robustness and explainability. Domain knowledge is encoded as logical constraints over appearance attributes such as shape and color, and enforced through semantic and symbolic logic losses applied during training. Using the GTSRB dataset, we evaluate robustness against FGSM and PGD attacks at a standard $\ell_\infty$ perturbation budget of $\varepsilon = 8/255$. Relative to clean training, standard adversarial training provides modest improvements in robustness ($\sim$10 percentage points). Conversely, our FGSM-Neuro-Symbolic and PGD-Neuro-Symbolic models achieve substantially larger gains, improving adversarial accuracy by 18.1\% and 17.35\% over their corresponding adversarial-training baselines, representing roughly a three-fold larger robustness gain than standard adversarial training provides when both are measured relative to the same clean-training baseline, without reducing clean-sample accuracy. Compared to transformer-based defenses such as LNL-MoEx, which require heavy architectures and extensive data augmentation, our PGD-Neuro-Symbolic variant attains comparable or superior robustness using a ResNet18 backbone trained for 10 epochs. These results show that symbolic reasoning offers an effective path to robust and interpretable AI.

</details>


### [182] [LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow Simulations](https://arxiv.org/abs/2601.13190)
*Vittoria De Pellegrini,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: LAViG-FLOW：一种用于地下多相流体流动建模的潜在自回归视频生成扩散框架，能够快速生成饱和度与压力场的耦合演化，比传统数值求解器快几个数量级。


<details>
  <summary>Details</summary>
Motivation: 地下多相流体流动建模对CO2地质封存和地热生产等应用至关重要，但高保真模拟器在反演和不确定性量化中计算成本过高，需要更高效的替代方法。

Method: 提出LAViG-FLOW框架：1）使用专用2D自编码器压缩每个状态变量；2）用视频扩散变换器（VDiT）建模时间上的耦合分布；3）先在给定时间范围内训练学习耦合关系，然后自回归微调以进行时间外推。

Result: 在开源CO2封存数据集上评估，LAViG-FLOW生成的饱和度与压力场在时间上保持一致性，运行速度比传统数值求解器快几个数量级。

Conclusion: LAViG-FLOW为地下多相流体流动建模提供了一种高效替代方案，显著加速了反演和不确定性量化过程，在CO2封存等应用中具有重要价值。

Abstract: Modeling and forecasting subsurface multiphase fluid flow fields underpin applications ranging from geological CO2 sequestration (GCS) operations to geothermal production. This is essential for ensuring both operational performance and long-term safety. While high fidelity multiphase simulators are widely used for this purpose, they become prohibitively expensive once many forward runs are required for inversion purposes and quantify uncertainty. To tackle this challenge we propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that explicitly learns the coupled evolution of saturation and pressure fields. Each state variable is compressed by a dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their coupled distribution across time. We first train the model on a given time horizon to learn their coupled relationship and then fine-tune it autoregressively so it can extrapolate beyond the observed time window. Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates saturation and pressure fields that stay consistent across time while running orders of magnitude faster than traditional numerical solvers.

</details>


### [183] [A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms](https://arxiv.org/abs/2601.13243)
*Yapeng Li,Jiakuo Yu,Zhixin Liu,Xinnan Liu,Jing Yu,Songze Li,Tonghua Su*

Main category: cs.LG

TL;DR: 该论文系统评估了LLM推理范式（直接生成、CoT、多智能体系统）的性能与成本权衡，发现结构复杂性不一定提升推理性能，并提出了新的开放基准MIMeBench来评估语义能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为推理系统部署时，不同推理范式（如CoT和多智能体系统）的相对有效性、成本-准确率权衡缺乏系统理解，需要统一评估框架来指导范式选择。

Method: 1）在多样化闭式基准上统一评估直接生成、CoT增强单模型、多智能体系统工作流；2）通过角色隔离分析探究MAS中角色特定能力需求；3）分析成本-准确率权衡；4）提出新开放基准MIMeBench评估语义抽象和对比辨别能力。

Result: 1）结构复杂性增加不一定提升推理性能，其益处高度依赖于推理范式本身特性与适用性；2）识别出哪些MAS工作流在成本与准确率间达到有利平衡，哪些因边际收益而成本过高；3）MIMeBench提供了现有基准难以捕捉的语义能力细粒度评估。

Conclusion: 需要根据具体任务特性选择推理范式，而非盲目追求复杂结构；MIMeBench为LLM语义能力评估提供了新维度；研究为LLM推理系统部署提供了实证指导。

Abstract: Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.

</details>


### [184] [Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks](https://arxiv.org/abs/2601.13244)
*Prateek Munjal,Clement Christophe,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.LG

TL;DR: 指令微调并不真正提升推理能力，而是让模型依赖特定的提示模式；在零样本思维链和分布偏移下，基础模型表现更好


<details>
  <summary>Details</summary>
Motivation: 研究指令微调是否真正增强大语言模型的推理能力，还是仅仅诱导表面模式匹配，揭示指令微调的实际效果和局限性

Method: 在标准数学基准测试（GSM8K）、结构扰动变体和领域转移任务上评估基础和指令微调模型，分析不同评估设置下的性能差异

Result: 1. 在GSM8K零样本思维链设置中，基础模型始终优于指令微调变体（Llama3-70B下降高达32.67%）；2. 指令微调模型仅在提供少样本示例时才能匹配或超越基础模型；3. 在MedCalc领域特定基准测试中，基础模型优于指令微调变体；4. 指令微调模型在扰动数据集上表现急剧下降

Conclusion: 指令微调的优势不稳定且依赖评估设置，模型依赖特定提示模式而非内在推理能力，在分布偏移下性能脆弱，表明指令微调并未真正增强推理能力

Abstract: Instruction finetuning is standard practice for improving LLM performance, yet it remains unclear whether it enhances reasoning or merely induces surface-level pattern matching. We investigate this by evaluating base and instruction-tuned models on standard math benchmarks, structurally perturbed variants, and domain-shifted tasks. Our analysis highlights two key (often overlooked) limitations of instruction tuning. First, the performance advantage is unstable and depends heavily on evaluation settings. In zero-shot CoT settings on GSM8K, base models consistently outperform instruction-tuned variants, with drops as high as 32.67\% (Llama3-70B). Instruction-tuned models only match or exceed this performance when provided with few-shot exemplars, suggesting a reliance on specific prompting patterns rather than intrinsic reasoning. Second, tuning gains are brittle under distribution shift. Our results show that base models surpass instruction-tuned variants on the domain-specific MedCalc benchmark. Additionally, instruction-tuned models show sharp declines on perturbed datasets, indicating sensitivity to prompt structure over robust reasoning.

</details>


### [185] [Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning](https://arxiv.org/abs/2601.13284)
*Duygu Nur Yaldiz,Evangelia Spiliopoulou,Zheng Qi,Siddharth Varia,Srikanth Doss,Nikolaos Pappas*

Main category: cs.LG

TL;DR: RLVR微调导致LLM过度自信，SFT校准更好但性能提升较小；提出校准感知强化学习，保持性能同时改善校准


<details>
  <summary>Details</summary>
Motivation: LLM在决策任务中需要可靠的置信度估计，但现有微调方法在准确性和校准性之间存在权衡：RLVR提高性能但导致过度自信，SFT校准更好但性能提升有限

Method: 系统研究SFT和RLVR的校准特性；诊断RLVR失败原因（决策token作为提取步骤不携带置信信息）；提出校准感知强化学习，直接调整决策token概率

Result: RLVR产生极度过度自信模型，SFT在校准方面表现更好（即使在分布偏移下）；提出的方法保持RLVR准确性同时显著改善校准，ECE分数降低最多9点

Conclusion: RLVR虽然提高任务性能但损害校准，SFT提供更好校准但性能提升有限；通过校准感知强化学习可以在保持性能的同时解决过度自信问题

Abstract: Large language models (LLMs) are increasingly deployed in decision-making tasks, where not only accuracy but also reliable confidence estimates are essential. Well-calibrated confidence enables downstream systems to decide when to trust a model and when to defer to fallback mechanisms. In this work, we conduct a systematic study of calibration in two widely used fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). We show that while RLVR improves task performance, it produces extremely overconfident models, whereas SFT yields substantially better calibration, even under distribution shift, though with smaller performance gains. Through targeted experiments, we diagnose RLVR's failure, showing that decision tokens act as extraction steps of the decision in reasoning traces and do not carry confidence information, which prevents reinforcement learning from surfacing calibrated alternatives. Based on this insight, we propose a calibration-aware reinforcement learning formulation that directly adjusts decision-token probabilities. Our method preserves RLVR's accuracy level while mitigating overconfidence, reducing ECE scores up to 9 points.

</details>


### [186] [CooperBench: Why Coding Agents Cannot be Your Teammates Yet](https://arxiv.org/abs/2601.13295)
*Arpandeep Khatua,Hao Zhu,Peter Tran,Arya Prabhudesai,Frederic Sadrieh,Johann K. Lieberwirth,Xinkai Yu,Yicheng Fu,Michael J. Ryan,Jiaxin Pei,Diyi Yang*

Main category: cs.LG

TL;DR: 论文提出了CooperBench基准测试，评估AI代理在协作编程中的协调能力，发现当前代理存在"协调诅咒"——协作时成功率比单独执行低30%，揭示了沟通、承诺和期望理解三大问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在复杂工作中越来越多地协作，它们需要发展协调能力成为有效的团队成员。但作者假设当前代理缺乏这些能力，需要系统评估和基准测试。

Method: 引入CooperBench基准测试，包含600多个协作编程任务，涉及12个库和4种编程语言。每个任务分配两个代理不同的功能，这些功能可以独立实现但需要协调避免冲突。任务基于真实开源仓库和专家编写的测试。

Result: 发现"协调诅咒"现象：代理协作时的平均成功率比单独执行两个任务低30%。这与人类团队通常因增加队友而提高生产力形成鲜明对比。分析揭示了三个关键问题：沟通渠道堵塞、代理偏离承诺、代理对他人计划和沟通有错误期望。同时观察到罕见的涌现协调行为。

Conclusion: 研究提出了新颖的协作编程基准测试，呼吁从追求个体代理能力转向发展社会智能，强调协调能力对AI团队协作的重要性。

Abstract: Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.

</details>


### [187] [Verifying Local Robustness of Pruned Safety-Critical Networks](https://arxiv.org/abs/2601.13303)
*Minh Le,Phuong Cao*

Main category: cs.LG

TL;DR: 研究剪枝对神经网络形式化验证的影响，发现轻剪枝（MNIST 40%）和重剪枝（JPL 70-90%）能提升可验证性，最优剪枝比例因数据集而异。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在安全关键应用中的形式化验证至关重要，但大规模模型的计算成本阻碍了实际应用。需要探索模型压缩（如剪枝）如何影响形式化验证的可行性。

Method: 使用最先进的α,β-CROWN验证器，在不同剪枝比例下评估ResNet4模型，在MNIST和NASA JPL火星霜冻识别数据集上进行实验。

Result: 发现非线性关系：MNIST上轻剪枝（40%）和JPL数据集上重剪枝（70-90%）能改善可验证性，使模型在L∞鲁棒性证明上优于未剪枝基线。

Conclusion: 减少连接性简化了形式化求解器的搜索空间，最优剪枝比例在不同数据集间差异显著。这为在高风险环境中部署高效且形式化验证的DNN提供了关键见解。

Abstract: Formal verification of Deep Neural Networks (DNNs) is essential for safety-critical applications, ranging from surgical robotics to NASA JPL autonomous systems. However, the computational cost of verifying large-scale models remains a significant barrier to adoption. This paper investigates the impact of pruning on formal local robustness certificates with different ratios. Using the state-of-the-art $α,β$-CROWN verifier, we evaluate ResNet4 models across varying pruning ratios on MNIST and, more importantly, on the NASA JPL Mars Frost Identification datasets. Our findings demonstrate a non-linear relationship: light pruning (40%) in MNIST and heavy pruning (70%-90%) in JPL improve verifiability, allowing models to outperform unpruned baselines in proven $L_\infty$ robustness properties. This suggests that reduced connectivity simplifies the search space for formal solvers and that the optimal pruning ratio varies significantly between datasets. This research highlights the complex nature of model compression, offering critical insights into selecting the optimal pruning ratio for deploying efficient, yet formally verified, DNNs in high-stakes environments where reliability is non-negotiable.

</details>


### [188] [Beyond Mapping : Domain-Invariant Representations via Spectral Embedding of Optimal Transport Plans](https://arxiv.org/abs/2601.13350)
*Abdel Djalil Sad Saoud,Fred Maurice Ngolè Mboula,Hanane Slimani*

Main category: cs.LG

TL;DR: 提出基于谱嵌入的领域不变表示学习方法，通过将平滑传输计划解释为二分图邻接矩阵，在多个音频和电缆缺陷检测任务上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 训练和推理数据之间的分布偏移是机器学习中的核心挑战，传统基于最优传输的无监督领域自适应方法依赖Monge映射近似，对正则化策略和超参数敏感，可能导致有偏的领域对齐

Method: 将平滑传输计划解释为连接源域和目标域的二分图邻接矩阵，通过谱嵌入推导领域不变的样本表示

Result: 在音乐流派识别、音乐-语音区分以及电缆缺陷检测和分类任务上评估，在不同诊断设置下使用时域反射技术，取得了整体强劲的性能

Conclusion: 提出的谱嵌入方法能够有效学习领域不变表示，克服传统最优传输方法对正则化和超参数的敏感性，在多个实际应用中表现出色

Abstract: Distributional shifts between training and inference time data remain a central challenge in machine learning, often leading to poor performance. It motivated the study of principled approaches for domain alignment, such as optimal transport based unsupervised domain adaptation, that relies on approximating Monge map using transport plans, which is sensitive to the transport problem regularization strategy and hyperparameters, and might yield biased domains alignment. In this work, we propose to interpret smoothed transport plans as adjacency matrices of bipartite graphs connecting source to target domain and derive domain-invariant samples' representations through spectral embedding. We evaluate our approach on acoustic adaptation benchmarks for music genre recognition, music-speech discrimination, as well as electrical cable defect detection and classification tasks using time domain reflection in different diagnosis settings, achieving overall strong performances.

</details>


### [189] [CausationEntropy: Pythonic Optimal Causation Entropy](https://arxiv.org/abs/2601.13365)
*Kevin Slote,Jeremie Fish,Erik Bollt*

Main category: cs.LG

TL;DR: CausationEntropy v1.1是一个Python包，实现了最优因果熵（oCSE）方法及其优化扩展，用于从动态系统和耦合振荡器中揭示因果网络，区分直接与间接路径。


<details>
  <summary>Details</summary>
Motivation: 需要为复杂动态系统中的因果发现提供一个基准工具，实现oCSE方法及其多种优化和扩展，使研究人员能够方便地进行因果网络建模。

Method: 实现了oCSE算法及其优化，包含多种信息论因果网络发现算法，支持高斯、k近邻、几何k近邻、核密度估计和泊松熵估计器，并提供合成数据生成器和绘图工具。

Result: 发布了CausationEntropy v1.1包，可通过PyPi安装，具有完整文档和代码示例，采用模块化结构支持未来扩展，代码在MIT许可下开源于GitHub和PyPi。

Conclusion: 该包将成为复杂动态系统中因果发现的基准工具，为研究人员提供强大且易用的因果网络建模解决方案。

Abstract: Optimal Causation Entropy (oCSE) is a robust causal network modeling technique that reveals causal networks from dynamical systems and coupled oscillators, distinguishing direct from indirect paths. CausationEntropy is a Python package that implements oCSE and several of its significant optimizations and methodological extensions. In this paper, we introduce the version 1.1 release of CausationEntropy, which includes new synthetic data generators, plotting tools, and several advanced information-theoretical causal network discovery algorithms with criteria for estimating Gaussian, k-nearest neighbors (kNN), geometric k-nearest neighbors (geometric-kNN), kernel density (KDE) and Poisson entropic estimators. The package is easy to install from the PyPi software repository, is thoroughly documented, supplemented with extensive code examples, and is modularly structured to support future additions. The entire codebase is released under the MIT license and is available on GitHub and through PyPi Repository. We expect this package to serve as a benchmark tool for causal discovery in complex dynamical systems.

</details>


### [190] [Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility](https://arxiv.org/abs/2601.13398)
*Nickil Maveli,Antonio Vergari,Shay B. Cohen*

Main category: cs.LG

TL;DR: RTCE基准测试揭示LLMs在代码往返执行中缺乏一致性推理能力，即使采用多种优化方法仍无法解决此问题


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码基准测试中表现良好，但在往返代码执行中保持一致性推理的能力存在局限，需要更严格的评估方法

Method: 提出RoundTripCodeEval(RTCE)基准，包含四个代码执行推理任务，通过零样本提示、监督微调和自反思机制评估模型

Result: 所有方法仅带来有限改进，无法解决往返一致性问题，表明当前LLMs缺乏可信代码推理所需的内在一致性

Conclusion: RTCE揭示了现有基准未捕捉到的新见解，表明LLMs在代码往返一致性方面存在根本性缺陷

Abstract: LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.

</details>


### [191] [TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction](https://arxiv.org/abs/2601.13422)
*Dahai Yu,Rongchao Xu,Dingyi Zhuang,Yuheng Bu,Shenhao Wang,Guang Wang*

Main category: cs.LG

TL;DR: TrustEnergy：一个用于准确可靠用户级能源使用预测的统一框架，通过分层时空表示和顺序保形分位数回归，在预测准确性和不确定性量化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有能源使用预测方法大多忽视家庭间的空间相关性，难以扩展到个体化预测，且缺乏对动态不确定性的量化，无法满足细粒度用户级预测的准确性和可靠性需求。

Method: 提出TrustEnergy框架，包含两个关键技术组件：(1)分层时空表示模块，使用新型记忆增强时空图神经网络捕捉宏观和微观能源使用模式；(2)顺序保形分位数回归模块，动态调整不确定性边界，确保随时间推移的有效预测区间，无需对底层数据分布做强假设。

Result: 与佛罗里达州电力供应商合作实施评估，结果显示TrustEnergy相比最先进的基线方法，预测准确性提高5.4%，不确定性量化改进5.7%。

Conclusion: TrustEnergy框架能够有效解决用户级能源使用预测中的空间相关性捕捉、个体化预测扩展和不确定性量化问题，为电网管理、基础设施规划和灾害响应等应用提供准确可靠的预测工具。

Abstract: Energy usage prediction is important for various real-world applications, including grid management, infrastructure planning, and disaster response. Although a plethora of deep learning approaches have been proposed to perform this task, most of them either overlook the essential spatial correlations across households or fail to scale to individualized prediction, making them less effective for accurate fine-grained user-level prediction. In addition, due to the dynamic and uncertain nature of energy usage caused by various factors such as extreme weather events, quantifying uncertainty for reliable prediction is also significant, but it has not been fully explored in existing work. In this paper, we propose a unified framework called TrustEnergy for accurate and reliable user-level energy usage prediction. There are two key technical components in TrustEnergy, (i) a Hierarchical Spatiotemporal Representation module to efficiently capture both macro and micro energy usage patterns with a novel memory-augmented spatiotemporal graph neural network, and (ii) an innovative Sequential Conformalized Quantile Regression module to dynamically adjust uncertainty bounds to ensure valid prediction intervals over time, without making strong assumptions about the underlying data distribution. We implement and evaluate our TrustEnergy framework by working with an electricity provider in Florida, and the results show our TrustEnergy can achieve a 5.4% increase in prediction accuracy and 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines.

</details>


### [192] [A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization](https://arxiv.org/abs/2601.13435)
*Shuozhe Li,Du Cheng,Leqi Liu*

Main category: cs.LG

TL;DR: WaveLSFormer：一种可学习的小波基长短期Transformer，通过端到端训练的小波前端进行多尺度分解，结合低频引导高频注入模块，直接优化交易目标和风险感知正则化，在日内交易中显著提升盈利能力和风险调整收益。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列的日内交易策略学习面临三大挑战：1）噪声严重；2）非平稳性；3）相关资产间的强横截面依赖性。现有方法难以有效处理这些复杂特性。

Method: 1）可学习小波前端：通过端到端训练的滤波器组生成低频/高频分量，使用频谱正则化器鼓励稳定且分离良好的频带；2）低频引导高频注入（LGHI）模块：用高频线索精炼低频表示，同时控制训练稳定性；3）风险预算约束：输出长/短头寸组合，按固定风险预算重新缩放；4）直接优化：使用交易目标和风险感知正则化进行端到端优化。

Result: 在5年每小时数据、6个行业组、10个随机种子的广泛实验中，WaveLSFormer始终优于MLP、LSTM和Transformer基线（无论是否使用固定离散小波前端）。在所有行业中平均实现累计总策略回报0.607±0.045，夏普比率2.157±0.166，显著提升了盈利能力和风险调整收益。

Conclusion: WaveLSFormer通过可学习的小波分解和低频引导高频注入机制，有效处理金融时间序列的噪声、非平稳性和横截面依赖性，在日内交易中实现了卓越的性能表现，为金融时间序列分析提供了新的多尺度建模框架。

Abstract: Learning profitable intraday trading policies from financial time series is challenging due to heavy noise, non-stationarity, and strong cross-sectional dependence among related assets. We propose \emph{WaveLSFormer}, a learnable wavelet-based long-short Transformer that jointly performs multi-scale decomposition and return-oriented decision learning. Specifically, a learnable wavelet front-end generates low-/high-frequency components via an end-to-end trained filter bank, guided by spectral regularizers that encourage stable and well-separated frequency bands. To fuse multi-scale information, we introduce a low-guided high-frequency injection (LGHI) module that refines low-frequency representations with high-frequency cues while controlling training stability. The model outputs a portfolio of long/short positions that is rescaled to satisfy a fixed risk budget, and is optimized directly with a trading objective and risk-aware regularization. Extensive experiments on five years of hourly data across six industry groups, evaluated over ten random seeds, demonstrate that WaveLSFormer consistently outperforms MLP, LSTM and Transformer backbones, with and without fixed discrete wavelet front-ends. On average in all industries, WaveLSFormer achieves a cumulative overall strategy return of $0.607 \pm 0.045$ and a Sharpe ratio of $2.157 \pm 0.166$, substantially improving both profitability and risk-adjusted returns over the strongest baselines.

</details>


### [193] [BladeSDF : Unconditional and Conditional Generative Modeling of Representative Blade Geometries Using Signed Distance Functions](https://arxiv.org/abs/2601.13445)
*Ashish S. Nair,Sandipp Krishnan Ravi,Itzel Salgado,Changjie Sun,Sayan Ghosh,Liping Wang*

Main category: cs.LG

TL;DR: 提出基于DeepSDF的涡轮叶片领域特定隐式生成框架，实现性能感知的可制造设计生成，建立可解释的潜在空间，通过工程描述符映射实现性能引导的几何生成。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI在工程设计中存在性能感知建模和可制造设计生成的不足，需要开发能够保持可行性、性能相关性的涡轮叶片几何自动合成与重建方法。

Method: 采用连续符号距离函数(SDF)表示，构建可解释的近似高斯潜在空间，通过插值和高斯采样实现可控探索和无条件合成；使用紧凑神经网络将工程描述符（如最大方向应变）映射到潜在代码。

Result: 实现高重建保真度，表面距离误差集中在最大叶片尺寸的1%以内；对未见设计具有鲁棒泛化能力；超越传统2D引导或无约束3D流程。

Conclusion: 该框架通过集成约束、目标和性能指标，为数据驱动的涡轮叶片建模和概念生成提供了实用且可解释的解决方案，推进了工程设计的生成式AI应用。

Abstract: Generative AI has emerged as a transformative paradigm in engineering design, enabling automated synthesis and reconstruction of complex 3D geometries while preserving feasibility and performance relevance. This paper introduces a domain-specific implicit generative framework for turbine blade geometry using DeepSDF, addressing critical gaps in performance-aware modeling and manufacturable design generation. The proposed method leverages a continuous signed distance function (SDF) representation to reconstruct and generate smooth, watertight geometries with quantified accuracy. It establishes an interpretable, near-Gaussian latent space that aligns with blade-relevant parameters, such as taper and chord ratios, enabling controlled exploration and unconditional synthesis through interpolation and Gaussian sampling. In addition, a compact neural network maps engineering descriptors, such as maximum directional strains, to latent codes, facilitating the generation of performance-informed geometry. The framework achieves high reconstruction fidelity, with surface distance errors concentrated within $1\%$ of the maximum blade dimension, and demonstrates robust generalization to unseen designs. By integrating constraints, objectives, and performance metrics, this approach advances beyond traditional 2D-guided or unconstrained 3D pipelines, offering a practical and interpretable solution for data-driven turbine blade modeling and concept generation.

</details>


### [194] [Federated Learning Under Temporal Drift -- Mitigating Catastrophic Forgetting via Experience Replay](https://arxiv.org/abs/2601.13456)
*Sahasra Kokkula,Daniel David,Aaditya Baruah*

Main category: cs.LG

TL;DR: 联邦学习中客户端数据随时间变化时，标准FedAvg会出现灾难性遗忘问题。通过客户端经验回放（每个客户端维护少量历史样本缓冲区），无需修改服务器聚合即可有效防止遗忘。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在面临时间概念漂移时表现不佳，客户端数据分布随时间变化会导致模型灾难性遗忘，需要解决这一问题。

Method: 提出客户端经验回放方法：每个客户端维护一个小型缓冲区存储过去样本，在本地训练时将历史样本与当前数据混合使用，无需修改服务器聚合机制。

Result: 在Fashion-MNIST上的实验显示，标准FedAvg在季节性漂移下准确率从74%降至28%。使用每类50个样本的缓冲区后，性能恢复到78-82%，有效防止了遗忘。消融研究揭示了内存与准确率之间的权衡关系。

Conclusion: 客户端经验回放是一种简单有效的联邦学习方法，能够在不改变服务器聚合的情况下防止时间概念漂移导致的灾难性遗忘，通过适度的内存开销即可显著提升模型稳定性。

Abstract: Federated Learning struggles under temporal concept drift where client data distributions shift over time. We demonstrate that standard FedAvg suffers catastrophic forgetting under seasonal drift on Fashion-MNIST, with accuracy dropping from 74% to 28%. We propose client-side experience replay, where each client maintains a small buffer of past samples mixed with current data during local training. This simple approach requires no changes to server aggregation. Experiments show that a 50-sample-per-class buffer restores performance to 78-82%, effectively preventing forgetting. Our ablation study reveals a clear memory-accuracy trade-off as buffer size increases.

</details>


### [195] [Quantum Qualifiers for Neural Network Model Selection in Hadronic Physics](https://arxiv.org/abs/2601.13463)
*Brandon B. Le,D. Keller*

Main category: cs.LG

TL;DR: 提出量子机器学习在强子物理中的诊断框架，通过量子质量指标指导量子与经典神经网络的选择，并在康普顿形状因子提取中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习架构的成熟，核心挑战不再是构建这些架构，而是识别它们在哪些领域能提供优于经典方法的实际优势。本研究旨在为数据驱动的强子物理问题开发诊断工具，指导量子与经典模型的选择。

Method: 开发以定量量子质量指标为中心的诊断框架，通过受控的分类和回归研究，分析模型性能与数据复杂性、噪声和维度等内在特性的关系，并将这些趋势提炼为预测性准则。

Result: 研究表明相对模型性能遵循复杂性、噪声和维度的系统性趋势，这些趋势可以提炼为预测性准则。在深度虚康普顿散射的康普顿形状因子提取应用中，量子质量指标成功识别了量子模型有利的运动学区域。

Conclusion: 该研究为在精密强子物理中部署量子机器学习工具建立了一个原则性框架，通过诊断工具和量子质量指标指导量子与经典模型的选择，为量子机器学习在物理应用中的实际部署提供了系统方法。

Abstract: As quantum machine-learning architectures mature, a central challenge is no longer their construction, but identifying the regimes in which they offer practical advantages over classical approaches. In this work, we introduce a framework for addressing this question in data-driven hadronic physics problems by developing diagnostic tools - centered on a quantitative quantum qualifier - that guide model selection between classical and quantum deep neural networks based on intrinsic properties of the data. Using controlled classification and regression studies, we show how relative model performance follows systematic trends in complexity, noise, and dimensionality, and how these trends can be distilled into a predictive criterion. We then demonstrate the utility of this approach through an application to Compton form factor extraction from deeply virtual Compton scattering, where the quantum qualifier identifies kinematic regimes favorable to quantum models. Together, these results establish a principled framework for deploying quantum machine-learning tools in precision hadronic physics.

</details>


### [196] [A Unified Variational Imputation Framework for Electric Vehicle Charging Data Using Retrieval-Augmented Language Model](https://arxiv.org/abs/2601.13476)
*Jinhao Li,Hao Wang*

Main category: cs.LG

TL;DR: 提出PRAIM框架，利用预训练语言模型和检索增强记忆机制，解决电动汽车充电数据缺失问题，显著提升插补精度和下游预测性能。


<details>
  <summary>Details</summary>
Motivation: 电动汽车基础设施中数据驱动应用的可靠性依赖于完整、高质量的充电数据，但现实数据集常存在缺失记录，现有插补方法无法处理充电数据的复杂多模态特性，且通常采用"一站一模型"的局限范式，忽略了站间相关性。

Method: 开发PRAIM框架：使用预训练语言模型编码时间序列需求、日历特征和地理空间上下文等异构数据为统一语义表示；通过检索增强记忆机制从整个充电网络中检索相关示例；采用变分神经架构构建统一的插补模型克服数据稀疏性。

Result: 在四个公共数据集上的实验表明，PRAIM在插补精度和保持原始数据统计分布方面显著优于现有基线方法，并大幅提升了下游预测性能。

Conclusion: PRAIM框架通过结合语言模型编码能力和检索增强记忆机制，有效解决了电动汽车充电数据缺失问题，为数据驱动的电动汽车基础设施应用提供了可靠的数据质量保障。

Abstract: The reliability of data-driven applications in electric vehicle (EV) infrastructure, such as charging demand forecasting, hinges on the availability of complete, high-quality charging data. However, real-world EV datasets are often plagued by missing records, and existing imputation methods are ill-equipped for the complex, multimodal context of charging data, often relying on a restrictive one-model-per-station paradigm that ignores valuable inter-station correlations. To address these gaps, we develop a novel PRobabilistic variational imputation framework that leverages the power of large lAnguage models and retrIeval-augmented Memory (PRAIM). PRAIM employs a pre-trained language model to encode heterogeneous data, spanning time-series demand, calendar features, and geospatial context, into a unified, semantically rich representation. This is dynamically fortified by retrieval-augmented memory that retrieves relevant examples from the entire charging network, enabling a single, unified imputation model empowered by variational neural architecture to overcome data sparsity. Extensive experiments on four public datasets demonstrate that PRAIM significantly outperforms established baselines in both imputation accuracy and its ability to preserve the original data's statistical distribution, leading to substantial improvements in downstream forecasting performance.

</details>


### [197] [StoTAM: Stochastic Alternating Minimization for Tucker-Structured Tensor Sensing](https://arxiv.org/abs/2601.13522)
*Shuang Li*

Main category: cs.LG

TL;DR: 提出一种基于Tucker分解的随机交替最小化算法，直接在核心张量和因子矩阵上操作，避免重复张量投影，实现低维张量因子的高效小批量更新。


<details>
  <summary>Details</summary>
Motivation: 低Tucker秩张量能有效捕捉高维数据的多模态子空间结构，但现有恢复方法要么在全张量变量上操作（张量投影昂贵），要么采用因子化公式但仍依赖全梯度计算，而大多数随机因子化方法仅限于张量分解设置。

Method: 提出一种随机交替最小化算法，直接在Tucker分解下的核心张量和因子矩阵上操作，避免重复张量投影，支持低维张量因子的高效小批量更新。

Result: 在合成张量感知实验中，与代表性随机张量恢复基线相比，该算法在挂钟时间上表现出更优的收敛行为。

Conclusion: 所提出的方法为低Tucker秩张量感知提供了一种高效的随机优化方案，避免了传统方法的计算瓶颈。

Abstract: Low-rank tensor sensing is a fundamental problem with broad applications in signal processing and machine learning. Among various tensor models, low-Tucker-rank tensors are particularly attractive for capturing multi-mode subspace structures in high-dimensional data. Existing recovery methods either operate on the full tensor variable with expensive tensor projections, or adopt factorized formulations that still rely on full-gradient computations, while most stochastic factorized approaches are restricted to tensor decomposition settings. In this work, we propose a stochastic alternating minimization algorithm that operates directly on the core tensor and factor matrices under a Tucker factorization. The proposed method avoids repeated tensor projections and enables efficient mini-batch updates on low-dimensional tensor factors. Numerical experiments on synthetic tensor sensing demonstrate that the proposed algorithm exhibits favorable convergence behavior in wall-clock time compared with representative stochastic tensor recovery baselines.

</details>


### [198] [MN-TSG:Continuous Time Series Generation with Irregular Observations](https://arxiv.org/abs/2601.13534)
*Xu Zhang,Junwei Deng,Chang Xu,Hao Li,Jiang Bian*

Main category: cs.LG

TL;DR: MN-TSG：基于混合专家NCDE的框架，用于不规则采样时间序列的连续生成，通过动态参数化专家函数和解耦设计优化专家混合动态，在多个数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列数据通常是不规则采样和稀疏观测的，而现有时间序列生成方法大多假设规则采样和固定输出分辨率，这在临床监测等应用中尤为成问题。神经控制微分方程（NCDE）在建模不规则时间序列方面有潜力，但仍难以捕捉复杂动态时间模式和支持连续时间序列生成。

Method: 提出MN-TSG框架，结合混合专家（MoE）的NCDE架构，具有动态参数化专家函数和解耦设计以优化MoE动态。利用现有TSG模型学习专家混合与生成时间序列的联合分布，使框架不仅能生成新样本，还能为每个样本生成适当的专家配置。

Result: 在十个公共和合成数据集上的广泛实验表明，MN-TSG在从不规则到规则和不规则到连续的生成任务中，始终优于强TSG基线方法。

Conclusion: MN-TSG通过混合专家NCDE架构有效解决了不规则采样时间序列的连续生成问题，为临床监测等实际应用提供了更实用的解决方案。

Abstract: Time series generation (TSG) plays a critical role in a wide range of domains, such as healthcare. However, most existing methods assume regularly sampled observations and fixed output resolutions, which are often misaligned with real-world scenarios where data are irregularly sampled and sparsely observed. This mismatch is particularly problematic in applications such as clinical monitoring, where irregular measurements must support downstream tasks requiring continuous and high-resolution time series.
  Neural Controlled Differential Equations (NCDEs) have shown strong potential for modeling irregular time series, yet they still face challenges in capturing complex dynamic temporal patterns and supporting continuous TSG. To address these limitations, we propose MN-TSG, a novel framework that explores Mixture-of-Experts (MoE)-based NCDEs and integrates them with existing TSG models for irregular and continuous generation tasks.
  The core of MN-TSG lies in a MoE-NCDE architecture with dynamically parameterized expert functions and a decoupled design that facilitates more effective optimization of MoE dynamics. Furthermore, we leverage existing TSG models to learn the joint distribution over the mixture of experts and the generated time series. This enables the framework not only to generate new samples, but also to produce appropriate expert configurations tailored to each sample, thereby supporting refined continuous TSG.
  Extensive experiments on ten public and synthetic datasets demonstrate the effectiveness of MN-TSG, consistently outperforming strong TSG baselines on both irregular-to-regular and irregular-to-continuous generation tasks.

</details>


### [199] [Patterning: The Dual of Interpretability](https://arxiv.org/abs/2601.13548)
*George Wang,Daniel Murfet*

Main category: cs.LG

TL;DR: 该论文提出"模式化"作为机制可解释性的对偶问题：给定期望的泛化形式，确定产生它的训练数据。通过反转线性响应关系，可以设计数据干预来引导模型达到目标内部配置。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性研究神经网络如何泛化到训练数据之外，但反向问题——给定期望的泛化形式，如何设计训练数据来实现它——尚未得到充分探索。作者希望建立从"读取"内部结构到"写入"内部结构的数学框架。

Method: 基于敏感性概念，测量可观测量后验期望值对数据分布微小变化的响应。通过反转这种线性响应关系，计算出能够引导模型达到目标内部配置的数据干预。具体方法包括沿主敏感性方向重新加权训练数据。

Result: 在小语言模型中，通过重新加权训练数据可以加速或延迟特定结构（如归纳电路）的形成。在括号平衡任务中，当多个算法都能达到完美训练准确率时，模式化可以通过针对每个解决方案的局部学习系数来选择模型学习的算法。

Conclusion: 相同的数学框架既可以用于读取神经网络的内部结构，也可以反转用于写入内部结构。这建立了机制可解释性与其对偶问题"模式化"之间的理论联系，为控制模型学习特定算法提供了新方法。

Abstract: Mechanistic interpretability aims to understand how neural networks generalize beyond their training data by reverse-engineering their internal structures. We introduce patterning as the dual problem: given a desired form of generalization, determine what training data produces it. Our approach is based on susceptibilities, which measure how posterior expectation values of observables respond to infinitesimal shifts in the data distribution. Inverting this linear response relationship yields the data intervention that steers the model toward a target internal configuration. We demonstrate patterning in a small language model, showing that re-weighting training data along principal susceptibility directions can accelerate or delay the formation of structure, such as the induction circuit. In a synthetic parentheses balancing task where multiple algorithms achieve perfect training accuracy, we show that patterning can select which algorithm the model learns by targeting the local learning coefficient of each solution. These results establish that the same mathematical framework used to read internal structure can be inverted to write it.

</details>


### [200] [ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits](https://arxiv.org/abs/2601.13563)
*Aryan Karmore*

Main category: cs.LG

TL;DR: ButterflyMoE通过将专家视为共享量化基质的几何重定向，而非独立权重矩阵，实现了专家数量的亚线性内存增长，在256个专家时达到150倍内存压缩。


<details>
  <summary>Details</summary>
Motivation: 传统MoE方法中，N个独立专家权重矩阵需要O(N·d²)内存，超出边缘设备内存预算。现有压缩方法（量化、剪枝、低秩分解）只能减少常数因子，无法解决线性扩展瓶颈。

Method: 将专家视为共享量化基质的几何重定向，而非独立存储。通过对共享三元原型应用学习到的旋转，每个专家获得O(d² + N·d log d)内存。训练这些旋转与量化结合，减少激活异常值并稳定极端低比特训练。

Result: 在语言建模基准测试中，ButterflyMoE在256个专家时实现150倍内存压缩，精度损失可忽略。使64个专家能适配4GB设备，而标准MoE只能容纳8个专家。

Conclusion: 几何参数化打破了MoE的线性内存扩展瓶颈，通过共享容量和旋转视角实现专家多样性，而非冗余存储，为边缘设备部署大规模专家模型提供了可行方案。

Abstract: Linear memory scaling stores $N$ independent expert weight matrices requiring $\mathcal{O}(N \cdot d^2)$ memory, which exceeds edge devices memory budget. Current compression methods like quantization, pruning and low-rank factorization reduce constant factors but leave the scaling bottleneck unresolved. We introduce ButterflyMoE, a method that treats experts not as independent weight matrices but as geometric reorientations of a unified shared quantized substrate. Diversity among experts arises from viewing different angles of shared capacity, not from redundant storage. By applying learned rotations to a shared ternary prototype, each expert yields $\mathcal{O}(d^2 + N \cdot d \log d)$ memory -- sub-linear in the number of experts. The key insight: training these rotations with quantization reduces activation outliers and stabilizes extreme low bit training, where static methods collapse. Across language modeling benchmarks, ButterflyMoE achieves 150 times memory reduction at 256 experts with negligible accuracy loss. This allows 64 experts to fit on 4GB devices compared to standard MoE's 8 experts, showing geometric parametrization breaks linear scaling.

</details>


### [201] [Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework](https://arxiv.org/abs/2601.13564)
*Yanheng Li,Zhichen Pu,Lijiang Yang,Zehao Zhou,Yi Qin Gao*

Main category: cs.LG

TL;DR: LUMOS是一个数据与物理双驱动的荧光分子逆设计框架，通过结合生成器与预测器、神经网络与快速TD-DFT计算、以及多目标进化算法，实现了高效可靠的荧光分子设计与优化。


<details>
  <summary>Details</summary>
Motivation: 传统荧光分子设计方法面临化学空间探索效率低、机器学习预测泛化能力不可靠、量子化学计算成本高昂等问题，难以满足多目标约束下的实际设计需求。

Method: LUMOS框架包含：1）共享潜在表征的生成器-预测器耦合系统；2）神经网络与快速TD-DFT计算结合的互补预测器套件；3）属性引导扩散模型与多目标进化算法集成的分子优化方法。

Result: 在综合基准测试中，LUMOS在荧光属性预测的准确性、泛化性和物理合理性方面均优于基线模型，在多目标分子优化中表现优异，TD-DFT和MD模拟验证了其生成符合目标规格的有效荧光分子。

Conclusion: LUMOS建立了一个数据与物理双驱动的通用荧光分子逆设计框架，能够高效可靠地设计满足多目标约束的荧光分子。

Abstract: Designing fluorescent small molecules with tailored optical and physicochemical properties requires navigating vast, underexplored chemical space while satisfying multiple objectives and constraints. Conventional generate-score-screen approaches become impractical under such realistic design specifications, owing to their low search efficiency, unreliable generalizability of machine-learning prediction, and the prohibitive cost of quantum chemical calculation. Here we present LUMOS, a data-and-physics driven framework for inverse design of fluorescent molecules. LUMOS couples generator and predictor within a shared latent representation, enabling direct specification-to-molecule design and efficient exploration. Moreover, LUMOS combines neural networks with a fast time-dependent density functional theory (TD-DFT) calculation workflow to build a suite of complementary predictors spanning different trade-offs in speed, accuracy, and generalizability, enabling reliable property prediction across diverse scenarios. Finally, LUMOS employs a property-guided diffusion model integrated with multi-objective evolutionary algorithms, enabling de novo design and molecular optimization under multiple objectives and constraints. Across comprehensive benchmarks, LUMOS consistently outperforms baseline models in terms of accuracy, generalizability and physical plausibility for fluorescence property prediction, and demonstrates superior performance in multi-objective scaffold- and fragment-level molecular optimization. Further validation using TD-DFT and molecular dynamics (MD) simulations demonstrates that LUMOS can generate valid fluorophores that meet various target specifications. Overall, these results establish LUMOS as a data-physics dual-driven framework for general fluorophore inverse design.

</details>


### [202] [Self-Improvement as Coherence Optimization: A Theoretical Account](https://arxiv.org/abs/2601.13566)
*Tianyi Qiu,Ahmed Hani Ismail,Zhonghao He,Shi Feng*

Main category: cs.LG

TL;DR: 语言模型通过辩论、自举和内部一致性最大化等方法实现无外部监督的自我提升，这些方法本质上是连贯性优化的特例，等同于描述长度正则化，在预训练模型下对半监督学习最优。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明语言模型可以通过辩论、自举和内部一致性最大化等方法在没有外部监督的情况下提升准确性，甚至匹配有监督微调的性能。但这些方法为何有效在理论上尚不明确，需要建立统一的理论框架来解释这些自我提升机制的工作原理。

Method: 提出连贯性优化理论框架，将辩论、自举和内部一致性最大化等方法统一为寻找最可压缩和联合可预测的上下文到行为映射。证明连贯性优化等价于描述长度正则化，并证明在预训练模型下，这种正则化方案对半监督学习是最优的。

Result: 理论分析表明无反馈的自我提升方法之所以有效，是因为它们本质上是连贯性优化的特例。初步实验支持该理论，该框架不仅能解释现有方法的工作原理，还能预测这些方法在何种情况下会成功或失败。

Conclusion: 语言模型的自我提升方法可以通过连贯性优化理论得到统一解释，该理论揭示了这些方法本质上是描述长度正则化的特殊形式，在预训练模型下对半监督学习具有最优性，为理解无监督自我提升提供了理论基础。

Abstract: Can language models improve their accuracy without external supervision? Methods such as debate, bootstrap, and internal coherence maximization achieve this surprising feat, even matching golden finetuning performance. Yet why they work remains theoretically unclear. We show that they are all special cases of coherence optimization: finding a context-to-behavior mapping that's most compressible and jointly predictable. We prove that coherence optimization is equivalent to description-length regularization, and that among all such regularization schemes, it is optimal for semi-supervised learning when the regularizer is derived from a pretrained model. Our theory, supported by preliminary experiments, explains why feedback-free self-improvement works and predicts when it should succeed or fail.

</details>


### [203] [DRGW: Learning Disentangled Representations for Robust Graph Watermarking](https://arxiv.org/abs/2601.13569)
*Jiasen Li,Yanwei Liu,Zhuoyi Shang,Xiaoyan Gu,Weiping Wang*

Main category: cs.LG

TL;DR: DRGW是首个基于解耦表示学习的图水印框架，通过分离结构表示与水印载体，解决了现有方法在透明度和鲁棒性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图水印方法主要在结构或纠缠表示上操作，由于图表示的信息耦合和连续数值到离散结构转换的不可控性，导致水印透明度和鲁棒性不足。

Method: 1) 对抗训练编码器学习不变结构表示并生成统计独立的水印载体；2) 图感知可逆神经网络提供无损水印嵌入/提取通道；3) 结构感知编辑器将潜在修改转化为离散图编辑。

Result: 在多个基准数据集上的实验表明DRGW具有优越的有效性。

Conclusion: DRGW通过解耦表示学习解决了图水印的透明度和鲁棒性问题，是首个在该方向上的框架，实验验证了其有效性。

Abstract: Graph-structured data is foundational to numerous web applications, and watermarking is crucial for protecting their intellectual property and ensuring data provenance. Existing watermarking methods primarily operate on graph structures or entangled graph representations, which compromise the transparency and robustness of watermarks due to the information coupling in representing graphs and uncontrollable discretization in transforming continuous numerical representations into graph structures. This motivates us to propose DRGW, the first graph watermarking framework that addresses these issues through disentangled representation learning. Specifically, we design an adversarially trained encoder that learns an invariant structural representation against diverse perturbations and derives a statistically independent watermark carrier, ensuring both robustness and transparency of watermarks. Meanwhile, we devise a graph-aware invertible neural network to provide a lossless channel for watermark embedding and extraction, guaranteeing high detectability and transparency of watermarks. Additionally, we develop a structure-aware editor that resolves the issue of latent modifications into discrete graph edits, ensuring robustness against structural perturbations. Experiments on diverse benchmark datasets demonstrate the superior effectiveness of DRGW.

</details>


### [204] [GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds](https://arxiv.org/abs/2601.13570)
*Tingting Dan,Jiaqi Ding,Guorong Wu*

Main category: cs.LG

TL;DR: GeoDynamics是一个几何状态空间神经网络，直接在对称正定流形上建模大脑功能连接性轨迹，用于追踪任务驱动状态变化和疾病早期标志物。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型通常将大脑视为松散连接区域或施加过度简化的网络先验，缺乏真正的整体自组织动力学系统视角。大脑功能连接性矩阵位于黎曼流形而非欧几里得空间，需要几何感知的方法来捕捉其轨迹。

Method: 提出GeoDynamics模型，将每个连接性矩阵嵌入到流形感知的循环框架中，在高维对称正定流形上直接学习平滑且尊重几何的状态转移。

Result: 模型能够揭示任务驱动状态变化，检测阿尔茨海默病、帕金森病和自闭症的早期标志物，并在人类动作识别基准（UTKinect、Florence、HDM05）上验证了其可扩展性和鲁棒性。

Conclusion: GeoDynamics提供了一个在流形上直接建模大脑状态轨迹的几何状态空间框架，不仅适用于神经科学，还能跨领域建模复杂的时空动力学。

Abstract: State-space models (SSMs) have become a cornerstone for unraveling brain dynamics, revealing how latent neural states evolve over time and give rise to observed signals. By combining the flexibility of deep learning with the principled dynamical structure of SSMs, recent studies have achieved powerful fits to functional neuroimaging data. However, most existing approaches still view the brain as a set of loosely connected regions or impose oversimplified network priors, falling short of a truly holistic and self-organized dynamical system perspective. Brain functional connectivity (FC) at each time point naturally forms a symmetric positive definite (SPD) matrix, which resides on a curved Riemannian manifold rather than in Euclidean space. Capturing the trajectories of these SPD matrices is key to understanding how coordinated networks support cognition and behavior. To this end, we introduce GeoDynamics, a geometric state-space neural network that tracks latent brain-state trajectories directly on the high-dimensional SPD manifold. GeoDynamics embeds each connectivity matrix into a manifold-aware recurrent framework, learning smooth and geometry-respecting transitions that reveal task-driven state changes and early markers of Alzheimer's disease, Parkinson's disease, and autism. Beyond neuroscience, we validate GeoDynamics on human action recognition benchmarks (UTKinect, Florence, HDM05), demonstrating its scalability and robustness in modeling complex spatiotemporal dynamics across diverse domains.

</details>


### [205] [Behavior Knowledge Merge in Reinforced Agentic Models](https://arxiv.org/abs/2601.13572)
*Xiangchi Yuan,Dachuan Shi,Chunhui Zhang,Zheyuan Liu,Shenglong Yao,Soroush Vosoughi,Wenke Lee*

Main category: cs.LG

TL;DR: RAM：针对RL训练智能体模型的分布感知融合框架，通过分离共享和任务特定参数更新，解决标准融合方法在RL智能体上的性能下降问题


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法主要针对监督微调设计，不适用于RL训练的智能体模型。RL产生的任务向量稀疏且异构，而SFT融合假设密集且全局可比，导致标准全局平均会稀释RL任务特定行为的关键参数更新

Method: RAM框架：1）解耦共享和任务特定的独特参数更新；2）对共享组件进行平均；3）选择性保留并重新缩放独特组件以抵消参数更新稀释

Result: 在多个智能体领域和模型架构上的实验表明，RAM不仅超越现有融合基线，还能解锁智能体间的协同潜力，实现优于各自领域专门智能体的性能

Conclusion: RAM是针对RL训练智能体模型的有效融合框架，解决了任务向量不匹配问题，能够更好地保留任务特定能力并实现智能体间的协同效应

Abstract: Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.

</details>


### [206] [FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning](https://arxiv.org/abs/2601.13578)
*Qian Feng,JiaHang Tu,Mintong Kang,Hanbin Zhao,Chao Zhang,Hui Qian*

Main category: cs.LG

TL;DR: 提出FG-OrIU框架，通过特征和梯度的双重正交约束实现深度遗忘，解决增量遗忘中残留信息可恢复的问题


<details>
  <summary>Details</summary>
Motivation: 现有增量遗忘方法主要在参数层面抑制或混淆知识，缺乏对特征和梯度层面的明确约束，导致"表面遗忘"——残留信息仍可恢复。这种不完整的遗忘存在安全风险，并破坏保留平衡，特别是在增量遗忘场景中。

Method: 提出FG-OrIU框架：1) 通过SVD分解特征空间，将遗忘类和保留类特征分离到不同子空间；2) 实施双重正交约束：特征正交投影防止特征混合，梯度正交投影防止遗忘知识重新引入和保留类被干扰；3) 动态子空间适应：合并新遗忘子空间并收缩保留子空间，确保序列遗忘任务中移除与保留的稳定平衡。

Result: 大量实验证明了该方法的有效性，能够实现深度遗忘（遗忘效果不可逆），解决了现有方法中残留信息可恢复的问题。

Conclusion: FG-OrIU是首个在特征和梯度层面统一正交约束的增量遗忘框架，通过双重正交约束和动态子空间适应实现了深度遗忘，确保了遗忘的不可逆性和保留的稳定性。

Abstract: Incremental unlearning (IU) is critical for pre-trained models to comply with sequential data deletion requests, yet existing methods primarily suppress parameters or confuse knowledge without explicit constraints on both feature and gradient level, resulting in \textit{superficial forgetting} where residual information remains recoverable. This incomplete forgetting risks security breaches and disrupts retention balance, especially in IU scenarios. We propose FG-OrIU (\textbf{F}eature-\textbf{G}radient \textbf{Or}thogonality for \textbf{I}ncremental \textbf{U}nlearning), the first framework unifying orthogonal constraints on both features and gradients level to achieve deep forgetting, where the forgetting effect is irreversible. FG-OrIU decomposes feature spaces via Singular Value Decomposition (SVD), separating forgetting and remaining class features into distinct subspaces. It then enforces dual constraints: feature orthogonal projection on both forgetting and remaining classes, while gradient orthogonal projection prevents the reintroduction of forgotten knowledge and disruption to remaining classes during updates. Additionally, dynamic subspace adaptation merges newly forgetting subspaces and contracts remaining subspaces, ensuring a stable balance between removal and retention across sequential unlearning tasks. Extensive experiments demonstrate the effectiveness of our method.

</details>


### [207] [Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation for Transformer Models](https://arxiv.org/abs/2601.13580)
*Ahmad Al-Zuraiqi*

Main category: cs.LG

TL;DR: Neural Organ Transplantation (NOT) 是一种模块化适配框架，可将训练好的Transformer层作为可重用、可移植的检查点进行领域适配，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法将训练参数与特定模型实例和训练数据紧密耦合，限制了模型的复用性和隐私保护。需要一种能够提取和共享训练知识而不暴露原始训练数据的方法。

Method: 从预训练模型中提取连续的层子集（"供体器官"），在领域特定数据上独立训练，保存为独立的检查点文件，然后移植到兼容的接收模型中，无需访问原始训练数据。

Result: 在124M到20B参数的三种解码器架构（GPT-2、TinyLlama、GPT-OSS）上，NOT显著优于现有适配方法，困惑度比LoRA提高一个数量级，训练速度更快。移植位置有依赖性，早期插入效果最佳。

Conclusion: Transformer中间层支持解码器架构的高效模块化迁移，通过检查点分发实现隐私保护的专家知识共享。该方法目前仅限于解码器模型，在编码器架构上效果有限。

Abstract: We introduce Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation. Unlike conventional fine-tuning approaches that tightly couple trained parameters to specific model instances and training data, NOT extracts contiguous layer subsets ("donor organs") from pre-trained models, trains them independently on domain-specific data, and saves them as standalone checkpoint files that can be transplanted into compatible recipient models without access to the original training data. Through experiments on three decoder-only transformer architectures spanning 124M to 20B parameters (GPT-2, TinyLlama, and GPT-OSS), we demonstrate that donor transplantation substantially outperforms existing adaptation methods, achieving an order-of-magnitude improvement in perplexity over LoRA while training significantly faster. The method exhibits position dependence, with early insertion positions yielding optimal results. Cross-domain transfer at billion-parameter scale reveals unexpected regularization benefits. These findings demonstrate that transformer middle layers can support efficient modular transfer for decoder-only architectures, enabling privacy-preserving expertise sharing through checkpoint distribution. We note that this approach is currently limited to decoder-only models; preliminary experiments on encoder-based architectures show reduced effectiveness.

</details>


### [208] [Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments](https://arxiv.org/abs/2601.13592)
*Hao Jing,Sa Xiao,Haoyu Li,Huadong Xiao,Wei Xue*

Main category: cs.LG

TL;DR: 使用残差卷积神经网络替代传统辐射方案，在保持精度的同时将计算速度提升约8倍，实现十天集成预报


<details>
  <summary>Details</summary>
Motivation: 辐射过程是数值模型中最耗时的物理过程，需要提高计算效率；同时需要解决混合预报框架中深度神经网络与数值预报模型耦合的两个关键瓶颈：耦合兼容性和长期积分稳定性

Method: 采用残差卷积神经网络近似RRTMG辐射方案，使用离线训练和在线耦合方法。通过模型模拟生成包含有云和无云大气柱的全面数据集，采用经验回放增强数据集稳定性，并基于物理意义施加额外输出约束。使用基于LibTorch的耦合方法进行实时计算

Result: 混合模型能够按要求进行十天集成预报。两个月的业务回算实验表明，机器学习模拟器达到与传统物理方案相当的精度，同时将计算速度提升约8倍

Conclusion: 成功开发了适用于中国气象局全球业务系统的机器学习辐射方案，解决了耦合兼容性和长期积分稳定性问题，在保持精度的同时显著提升了计算效率

Abstract: Radiation is typically the most time-consuming physical process in numerical models. One solution is to use machine learning methods to simulate the radiation process to improve computational efficiency. From an operational standpoint, this study investigates critical limitations inherent to hybrid forecasting frameworks that embed deep neural networks into numerical prediction models, with a specific focus on two fundamental bottlenecks: coupling compatibility and long-term integration stability. A residual convolutional neural network is employed to approximate the Rapid Radiative Transfer Model for General Circulation Models (RRTMG) within the global operational system of China Meteorological Administration. We adopted an offline training and online coupling approach. First, a comprehensive dataset is generated through model simulations, encompassing all atmospheric columns both with and without cloud cover. To ensure the stability of the hybrid model, the dataset is enhanced via experience replay, and additional output constraints based on physical significance are imposed. Meanwhile, a LibTorch-based coupling method is utilized, which is more suitable for real-time operational computations. The hybrid model is capable of performing ten-day integrated forecasts as required. A two-month operational reforecast experiment demonstrates that the machine learning emulator achieves accuracy comparable to that of the traditional physical scheme, while accelerating the computation speed by approximately eightfold.

</details>


### [209] [Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models](https://arxiv.org/abs/2601.13599)
*Linrui Ma,Yufei Cui,Kai Han,Yunhe Wang*

Main category: cs.LG

TL;DR: 提出Diffusion in Diffusion框架，通过"草稿-精炼"两阶段方法解决块扩散语言模型的不可逆性和短视问题，在OpenWebText数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 块扩散语言模型结合了自回归和扩散模型的优势，但其严格的单向块依赖导致不可逆性，并牺牲了扩散模型著名的全局规划能力。需要解决这些限制。

Method: 采用"草稿-精炼"框架：1) 使用小块进行块扩散快速生成草稿；2) 通过具有更大双向感受野的全局双向扩散精炼草稿；3) 使用快照置信度重掩码识别需要修改的关键token；4) 应用混合尺度训练扩展块扩散模型的全局能力。

Result: 在OpenWebText数据集上为离散扩散模型设立了新基准：仅使用基线模型26%的微调预算，将生成困惑度从25.7降低到21.9，显著缩小了与自回归模型的性能差距。

Conclusion: Diffusion in Diffusion框架有效解决了块扩散模型的不可逆性和短视问题，通过两阶段方法实现了更好的全局规划能力，在效率和性能上都取得了显著提升。

Abstract: Block diffusion language models, operating as semi-autoregressive paradigms, combine the strengths of both autoregressive and diffusion paradigms. However, their strict unidirectional block dependencies introduce irreversibility and sacrifice the global planning capabilities for which diffusion models are renowned. In order to address these issues, we propose Diffusion in Diffusion, a draft-then-refine framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilise snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using just 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.

</details>


### [210] [Fisher-Informed Parameterwise Aggregation for Federated Learning with Heterogeneous Data](https://arxiv.org/abs/2601.13608)
*Zhipeng Chang,Ting He,Wenrui Hao*

Main category: cs.LG

TL;DR: 提出FIPA方法，使用Fisher信息矩阵进行参数级聚合，解决联邦学习中非IID数据下的客户端漂移问题


<details>
  <summary>Details</summary>
Motivation: 标准联邦学习方法如FedAvg对每个客户端的所有参数使用相同的标量权重，在非IID数据下，这些均匀加权的更新会在客户端间产生强烈错位，导致客户端漂移并降低全局模型性能

Method: 提出Fisher信息参数级聚合(FIPA)，用参数特定的Fisher信息矩阵权重替代客户端级标量权重，实现真正的参数级缩放，捕捉每个客户端数据对不同参数的独特影响，并通过低秩近似保持通信和计算效率

Result: 在非线性函数回归、PDE学习和图像分类任务中，FIPA始终优于基于平均的聚合方法，并能与最先进的客户端优化算法有效结合，进一步提高图像分类准确率

Conclusion: FIPA在异构数据分布下的联邦学习中具有显著优势，通过参数级聚合改善了模型性能

Abstract: Federated learning aggregates model updates from distributed clients, but standard first order methods such as FedAvg apply the same scalar weight to all parameters from each client. Under non-IID data, these uniformly weighted updates can be strongly misaligned across clients, causing client drift and degrading the global model. Here we propose Fisher-Informed Parameterwise Aggregation (FIPA), a second-order aggregation method that replaces client-level scalar weights with parameter-specific Fisher Information Matrix (FIM) weights, enabling true parameter-level scaling that captures how each client's data uniquely influences different parameters. With low-rank approximation, FIPA remains communication- and computation-efficient. Across nonlinear function regression, PDE learning, and image classification, FIPA consistently improves over averaging-based aggregation, and can be effectively combined with state-of-the-art client-side optimization algorithms to further improve image classification accuracy. These results highlight the benefits of FIPA for federated learning under heterogeneous data distributions.

</details>


### [211] [Quadratic Upper Bound for Boosting Robustness](https://arxiv.org/abs/2601.13645)
*Euijin You,Hyang-Won Lee*

Main category: cs.LG

TL;DR: 提出一种二次上界损失函数，用于改善快速对抗训练中因对抗空间探索不足导致的鲁棒性下降问题。


<details>
  <summary>Details</summary>
Motivation: 快速对抗训练虽然减少了训练时间，但往往因对抗空间探索不足而导致模型鲁棒性下降，需要解决这一关键问题。

Method: 推导出对抗训练损失函数的二次上界，并将该上界与现有快速对抗训练方法结合使用。

Result: 实验结果表明，将QUB损失应用于现有方法能显著提升模型鲁棒性，且这种改进可能源于所得模型损失景观的平滑化。

Conclusion: 提出的二次上界损失函数能有效缓解快速对抗训练中的鲁棒性下降问题，通过平滑损失景观提升模型对抗攻击的鲁棒性。

Abstract: Fast adversarial training (FAT) aims to enhance the robustness of models against adversarial attacks with reduced training time, however, FAT often suffers from compromised robustness due to insufficient exploration of adversarial space. In this paper, we develop a loss function to mitigate the problem of degraded robustness under FAT. Specifically, we derive a quadratic upper bound (QUB) on the adversarial training (AT) loss function and propose to utilize the bound with existing FAT methods. Our experimental results show that applying QUB loss to the existing methods yields significant improvement of robustness. Furthermore, using various metrics, we demonstrate that this improvement is likely to result from the smoothened loss landscape of the resulting model.

</details>


### [212] [TimeART: Towards Agentic Time Series Reasoning via Tool-Augmentation](https://arxiv.org/abs/2601.13653)
*Xingjian Wu,Junkai Lu,Zhengyu Li,Xiangfei Qiu,Jilin Hu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: TimeART：融合强工具分析能力与LLM推理能力的全自动时间序列问答框架，通过100k专家轨迹数据集和四阶段训练策略，在多项TSQA任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分析主要依赖人工数据科学家，成本高且缺乏自动化。需要开发能够自动分析时间序列数据的智能系统

Method: 1. 提出TimeART框架，融合现成工具的分析能力和LLM的推理能力；2. 收集100k专家轨迹数据集TimeToolBench；3. 设计四阶段训练策略，让模型从早期经验和自我反思中学习

Result: 训练出8B参数的TSRM模型，在多个时间序列问答任务上取得一致性的最先进性能

Conclusion: TimeART开创了代理式时间序列推理的新方法，实现了全自动的时间序列数据分析科学家

Abstract: Time series data widely exist in real-world cyber-physical systems. Though analyzing and interpreting them contributes to significant values, e.g, disaster prediction and financial risk control, current workflows mainly rely on human data scientists, which requires significant labor costs and lacks automation. To tackle this, we introduce TimeART, a framework fusing the analytical capability of strong out-of-the-box tools and the reasoning capability of Large Language Models (LLMs), which serves as a fully agentic data scientist for Time Series Question Answering (TSQA). To teach the LLM-based Time Series Reasoning Models (TSRMs) strategic tool-use, we also collect a 100k expert trajectory corpus called TimeToolBench. To enhance TSRMs' generalization capability, we then devise a four-stage training strategy, which boosts TSRMs through learning from their own early experiences and self-reflections. Experimentally, we train an 8B TSRM on TimeToolBench and equip it with the TimeART framework, and it achieves consistent state-of-the-art performance on multiple TSQA tasks, which pioneers a novel approach towards agentic time series reasoning.

</details>


### [213] [Autoregressive deep learning for real-time simulation of soft tissue dynamics during virtual neurosurgery](https://arxiv.org/abs/2601.13676)
*Fabian Greifeneder,Wolfgang Fenz,Benedikt Alkin,Johannes Brandstetter,Michael Giretzlehner,Philipp Moser*

Main category: cs.LG

TL;DR: 基于深度学习的脑组织变形快速模拟方法，通过物理变换器和随机教师强制策略，实现神经外科手术模拟中的实时交互


<details>
  <summary>Details</summary>
Motivation: 神经外科手术模拟器需要准确模拟脑组织变形，但传统数值求解器难以满足实时性要求，需要开发高效的替代模型

Method: 基于通用物理变换器构建深度学习替代模型，直接处理大规模网格数据，采用随机教师强制策略减少自回归推理中的误差累积

Result: 模型在多种瞬态脑变形场景中实现准确预测，支持15万个节点的网格，最大预测误差从6.7mm降至3.5mm，在消费级硬件上每步模拟时间低于10ms

Conclusion: 提出的深度学习框架能够实现快速、平滑、准确的动态脑组织生物力学模拟，为现实手术训练环境奠定基础

Abstract: Accurate simulation of brain deformation is a key component for developing realistic, interactive neurosurgical simulators, as complex nonlinear deformations must be captured to ensure realistic tool-tissue interactions. However, traditional numerical solvers often fall short in meeting real-time performance requirements. To overcome this, we introduce a deep learning-based surrogate model that efficiently simulates transient brain deformation caused by continuous interactions between surgical instruments and the virtual brain geometry. Building on Universal Physics Transformers, our approach operates directly on large-scale mesh data and is trained on an extensive dataset generated from nonlinear finite element simulations, covering a broad spectrum of temporal instrument-tissue interaction scenarios. To reduce the accumulation of errors in autoregressive inference, we propose a stochastic teacher forcing strategy applied during model training. Specifically, training consists of short stochastic rollouts in which the proportion of ground truth inputs is gradually decreased in favor of model-generated predictions. Our results show that the proposed surrogate model achieves accurate and efficient predictions across a range of transient brain deformation scenarios, scaling to meshes with up to 150,000 nodes. The introduced stochastic teacher forcing technique substantially improves long-term rollout stability, reducing the maximum prediction error from 6.7 mm to 3.5 mm. We further integrate the trained surrogate model into an interactive neurosurgical simulation environment, achieving runtimes below 10 ms per simulation step on consumer-grade inference hardware. Our proposed deep learning framework enables rapid, smooth and accurate biomechanical simulations of dynamic brain tissue deformation, laying the foundation for realistic surgical training environments.

</details>


### [214] [Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction](https://arxiv.org/abs/2601.13710)
*Sayeed Shafayet Chowdhury,Snehasis Mukhopadhyay,Shiaofen Fang,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 研究比较监督式机器学习与生成式AI在预测慢性鼻窦炎手术效果上的表现，发现MLP模型表现最佳，建议采用ML为主、GenAI为辅的工作流程


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医学影像领域取得进展，但在临床数据上进行前瞻性决策支持的应用仍然有限。本研究旨在探索术前预测慢性鼻窦炎手术效果的可能性，识别那些手术效果不佳的患者，帮助他们避免不必要的手术。

Method: 在前瞻性收集的队列中，比较监督式机器学习（逻辑回归、树集成、MLP）与生成式AI（ChatGPT、Claude、Gemini、Perplexity）的表现。所有模型接收相同的结构化输入，输出二元推荐及置信度。建立了可复现的表格数据到生成式AI的评估协议。

Result: 最佳ML模型（MLP）达到85%准确率，具有优越的校准和决策曲线净效益。生成式AI模型在零样本设置下在区分度和校准方面表现较差。生成式AI的推理与临床经验和MLP特征重要性一致，都强调基线SNOT-22、CT/内窥镜严重程度、息肉表型和心理/疼痛合并症。

Conclusion: 支持采用ML优先、GenAI增强的工作流程：部署校准的ML进行手术候选者的初步筛查，使用GenAI作为解释器增强透明度和共同决策制定。

Abstract: Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.

</details>


### [215] [EEG-Titans: Long-Horizon Seizure Forecasting via Dual-Branch Attention and Neural Memory](https://arxiv.org/abs/2601.13748)
*Tien-Dat Pham,Xuan-The Tran*

Main category: cs.LG

TL;DR: EEG-Titans：一种双分支架构，结合滑动窗口注意力和循环记忆机制，用于癫痫发作预测，在CHB-MIT数据集上达到99.46%的平均段级灵敏度


<details>
  <summary>Details</summary>
Motivation: 癫痫发作预测面临挑战，因为发作前动态可能跨越长时间范围，而临床相关特征可能微妙且短暂。现有深度学习模型在捕获局部时空模式和保持长程上下文信息之间存在权衡

Method: 提出EEG-Titans双分支架构：1）滑动窗口注意力分支捕获短期异常；2）循环记忆路径总结随时间变化的缓慢渐进趋势。采用现代神经记忆机制进行长上下文建模

Result: 在CHB-MIT头皮EEG数据集上，按时间顺序保留协议评估，EEG-Titans在18名受试者中达到99.46%的平均段级灵敏度。通过分层上下文策略扩展高噪声受试者的感受野，可显著减少误报（极端异常值中降至0.00 FPR/h）而不牺牲灵敏度

Conclusion: 记忆增强的长上下文建模可以在临床约束评估下提供稳健的癫痫发作预测，表明该方法在捕获局部异常和维持长程上下文信息方面具有优势

Abstract: Accurate epileptic seizure prediction from electroencephalography (EEG) remains challenging because pre-ictal dynamics may span long time horizons while clinically relevant signatures can be subtle and transient. Many deep learning models face a persistent trade-off between capturing local spatiotemporal patterns and maintaining informative long-range context when operating on ultralong sequences. We propose EEG-Titans, a dualbranch architecture that incorporates a modern neural memory mechanism for long-context modeling. The model combines sliding-window attention to capture short-term anomalies with a recurrent memory pathway that summarizes slower, progressive trends over time. On the CHB-MIT scalp EEG dataset, evaluated under a chronological holdout protocol, EEG-Titans achieves 99.46% average segment-level sensitivity across 18 subjects. We further analyze safety-first operating points on artifact-prone recordings and show that a hierarchical context strategy extending the receptive field for high-noise subjects can markedly reduce false alarms (down to 0.00 FPR/h in an extreme outlier) without sacrificing sensitivity. These results indicate that memory-augmented long-context modeling can provide robust seizure forecasting under clinically constrained evaluation

</details>


### [216] [vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.13768)
*Wenzhen Yue,Ruohao Guo,Ji Shi,Zihan Hao,Shiyu Hu,Xianghua Ying*

Main category: cs.LG

TL;DR: vLinear是一种基于线性运算的高效多元时间序列预测器，包含vecTrans模块和WFMLoss目标函数，在保持高性能的同时大幅降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的预测器通常依赖自注意力或其变体来捕捉多元相关性，这通常导致与变量数N相关的O(N²)计算复杂度。需要一种更高效的替代方案。

Method: 提出两个核心组件：1) vecTrans模块，使用可学习向量建模多元相关性，将复杂度降至O(N)；2) WFMLoss目标函数，采用最终序列导向的流匹配损失，结合路径和时域加权策略。

Result: 在22个基准测试和124个预测设置中达到最先进性能；vecTrans可无缝集成到Transformer预测器中，实现高达5倍的推理加速和持续性能提升；WFMLoss作为即插即用目标函数能持续改进现有预测器。

Conclusion: vLinear提供了一种高效且有效的多元时间序列预测解决方案，通过线性复杂度模块和创新的损失函数设计，在保持高性能的同时显著提升计算效率。

Abstract: In this paper, we present \textbf{vLinear}, an effective yet efficient \textbf{linear}-based multivariate time series forecaster featuring two components: the \textbf{v}ecTrans module and the WFMLoss objective. Many state-of-the-art forecasters rely on self-attention or its variants to capture multivariate correlations, typically incurring $\mathcal{O}(N^2)$ computational complexity with respect to the number of variates $N$. To address this, we propose vecTrans, a lightweight module that utilizes a learnable vector to model multivariate correlations, reducing the complexity to $\mathcal{O}(N)$. Notably, vecTrans can be seamlessly integrated into Transformer-based forecasters, delivering up to 5$\times$ inference speedups and consistent performance gains. Furthermore, we introduce WFMLoss (Weighted Flow Matching Loss) as the objective. In contrast to typical \textbf{velocity-oriented} flow matching objectives, we demonstrate that a \textbf{final-series-oriented} formulation yields significantly superior forecasting accuracy. WFMLoss also incorporates path- and horizon-weighted strategies to focus learning on more reliable paths and horizons. Empirically, vLinear achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings. Moreover, WFMLoss serves as an effective plug-and-play objective, consistently improving existing forecasters. The code is available at https://anonymous.4open.science/r/vLinear.

</details>


### [217] [Principled Latent Diffusion for Graphs via Laplacian Autoencoders](https://arxiv.org/abs/2601.13780)
*Antoine Siraudin,Christopher Morris*

Main category: cs.LG

TL;DR: LG-Flow：一种潜在图扩散框架，通过将图压缩到低维潜在空间进行扩散，解决了传统图扩散模型二次复杂度的问题，实现了近无损重建和1000倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统图扩散模型存在二次复杂度问题，且大部分计算能力浪费在建模稀疏图中的边缺失上。需要一种方法能在低维潜在空间进行扩散，同时保证图重建的准确性。

Method: 使用置换等变自编码器将每个节点映射到固定维度的嵌入中，从该嵌入中可以证明完全恢复邻接矩阵。在潜在空间中使用流匹配训练扩散变换器。

Result: 实现了与最先进图扩散模型竞争的结果，同时获得了高达1000倍的加速，潜在表示维度与节点数线性相关，消除了二次瓶颈。

Conclusion: LG-Flow成功解决了图潜在扩散中的重建精度挑战，为大规模图生成提供了高效且表达能力强的解决方案。

Abstract: Graph diffusion models achieve state-of-the-art performance in graph generation but suffer from quadratic complexity in the number of nodes -- and much of their capacity is wasted modeling the absence of edges in sparse graphs. Inspired by latent diffusion in other modalities, a natural idea is to compress graphs into a low-dimensional latent space and perform diffusion there. However, unlike images or text, graph generation requires nearly lossless reconstruction, as even a single error in decoding an adjacency matrix can render the entire sample invalid. This challenge has remained largely unaddressed. We propose LG-Flow, a latent graph diffusion framework that directly overcomes these obstacles. A permutation-equivariant autoencoder maps each node into a fixed-dimensional embedding from which the full adjacency is provably recoverable, enabling near-lossless reconstruction for both undirected graphs and DAGs. The dimensionality of this latent representation scales linearly with the number of nodes, eliminating the quadratic bottleneck and making it feasible to train larger and more expressive models. In this latent space, we train a Diffusion Transformer with flow matching, enabling efficient and expressive graph generation. Our approach achieves competitive results against state-of-the-art graph diffusion models, while achieving up to $1000\times$ speed-up.

</details>


### [218] [PAtt: A Pattern Attention Network for ETA Prediction Using Historical Speed Profiles](https://arxiv.org/abs/2601.13793)
*ByeoungDo Kim,JunYeop Na,Kyungwook Tak,JunTae Kim,DongHyeon Kim,Duckky Kim*

Main category: cs.LG

TL;DR: 本文提出了一种基于注意力机制的ETA预测模型，通过捕捉历史道路速度模式的时空特征来提升到达时间估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和智能交通系统的普及，准确可靠的ETA估计在导航、出行规划和交通管理中变得至关重要。然而，由于交通流的动态复杂性，传统方法要么简单结合实时和历史数据，要么依赖复杂的规则计算，而现有深度学习模型计算成本高且未能有效捕捉关键的时空模式。

Method: 提出基于注意力机制的ETA模型，利用注意力机制提取路线中每个时空点积累的时间特征，有效捕捉时空因果关系。该架构保持模型轻量化和可扩展性，同时整合道路特征、实时交通状况和历史速度模式。

Result: 在真实驾驶数据集上的验证表明，该方法优于现有基线模型，能够以任务感知的方式有效整合多种交通信息。

Conclusion: 提出的基于注意力机制的ETA模型能够高效准确地估计到达时间，通过有效捕捉时空模式解决了传统方法的局限性，为智能交通系统提供了实用的解决方案。

Abstract: In this paper, we propose an ETA model (Estimated Time of Arrival) that leverages an attention mechanism over historical road speed patterns. As autonomous driving and intelligent transportation systems become increasingly prevalent, the need for accurate and reliable ETA estimation has grown, playing a vital role in navigation, mobility planning, and traffic management. However, predicting ETA remains a challenging task due to the dynamic and complex nature of traffic flow. Traditional methods often combine real-time and historical traffic data in simplistic ways, or rely on complex rule-based computations. While recent deep learning models have shown potential, they often require high computational costs and do not effectively capture the spatio-temporal patterns crucial for ETA prediction. ETA prediction inherently involves spatio-temporal causality, and our proposed model addresses this by leveraging attention mechanisms to extract and utilize temporal features accumulated at each spatio-temporal point along a route. This architecture enables efficient and accurate ETA estimation while keeping the model lightweight and scalable. We validate our approach using real-world driving datasets and demonstrate that our approach outperforms existing baselines by effectively integrating road characteristics, real-time traffic conditions, and historical speed patterns in a task-aware manner.

</details>


### [219] [ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical Federated Learning over Resource-Constrained Edge Networks](https://arxiv.org/abs/2601.13824)
*Xiaohong Yang,Tong Xie,Minghui Liwang,Chikai Shang,Yang Lu,Zhenzhen Jiao,Liqun Fu,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: ELSA是一个结合分割学习和分层联邦学习的高效LLM边缘微调框架，通过客户端聚类、模型分割和轻量通信方案解决资源限制、数据异构和隐私风险问题。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上训练大语言模型面临三大挑战：设备资源限制、严重的数据异构性以及隐私风险加剧。现有方法难以同时解决这些问题，需要一种系统性的解决方案。

Method: 1. 任务无关的行为感知客户端聚类机制：使用公共探测输入和对称KL散度构建语义指纹，结合预测一致性信任评分和延迟感知边缘分配；2. 将LLM分割为三部分分布在客户端和边缘服务器，云端仅用于适配器聚合；3. 基于计算草图的轻量通信方案结合语义子空间正交扰动(SS-OP)减少通信开销并保护隐私。

Result: 在多种NLP任务上的实验表明，ELSA在适应性、收敛行为和鲁棒性方面持续优于最先进方法，为资源受限下的边缘侧LLM微调提供了可扩展且隐私感知的解决方案。

Conclusion: ELSA通过系统整合分割学习和分层联邦学习，成功解决了边缘LLM微调中的资源限制、数据异构和隐私风险问题，为边缘智能提供了有效的框架。

Abstract: Training large language models (LLMs) at the network edge faces fundamental challenges arising from device resource constraints, severe data heterogeneity, and heightened privacy risks. To address these, we propose ELSA (Efficient LLM-centric Split Aggregation), a novel framework that systematically integrates split learning (SL) and hierarchical federated learning (HFL) for distributed LLM fine-tuning over resource-constrained edge networks. ELSA introduces three key innovations. First, it employs a task-agnostic, behavior-aware client clustering mechanism that constructs semantic fingerprints using public probe inputs and symmetric KL divergence, further enhanced by prediction-consistency-based trust scoring and latency-aware edge assignment to jointly address data heterogeneity, client unreliability, and communication constraints. Second, it splits the LLM into three parts across clients and edge servers, with the cloud used only for adapter aggregation, enabling an effective balance between on-device computation cost and global convergence stability. Third, it incorporates a lightweight communication scheme based on computational sketches combined with semantic subspace orthogonal perturbation (SS-OP) to reduce communication overhead while mitigating privacy leakage during model exchanges. Experiments across diverse NLP tasks demonstrate that ELSA consistently outperforms state-of-the-art methods in terms of adaptability, convergence behavior, and robustness, establishing a scalable and privacy-aware solution for edge-side LLM fine-tuning under resource constraints.

</details>


### [220] [Optimal L2 Regularization in High-dimensional Continual Linear Regression](https://arxiv.org/abs/2601.13844)
*Gilad Karpel,Edward Moroshko,Ran Levinstein,Ron Meir,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: 在过参数化持续线性回归中，各向同性正则化能缓解标签噪声，最优正则化强度随任务数T近似线性增长为T/lnT


<details>
  <summary>Details</summary>
Motivation: 研究过参数化持续学习中的泛化问题，特别是在线性回归设置下，探索正则化如何影响多任务学习中的泛化性能

Method: 在高维机制下推导闭式表达式分析期望泛化损失，考虑任意线性教师模型，研究各向同性正则化对标签噪声的缓解作用

Result: 各向同性正则化能有效缓解单教师和多教师设置下的标签噪声，最优固定正则化强度随任务数T近似线性增长为T/lnT

Conclusion: 这是理论持续学习领域的首个此类结果，为持续学习系统设计提供了实用指导，并通过线性回归和神经网络实验验证了理论发现

Abstract: We study generalization in an overparameterized continual linear regression setting, where a model is trained with L2 (isotropic) regularization across a sequence of tasks. We derive a closed-form expression for the expected generalization loss in the high-dimensional regime that holds for arbitrary linear teachers. We demonstrate that isotropic regularization mitigates label noise under both single-teacher and multiple i.i.d. teacher settings, whereas prior work accommodating multiple teachers either did not employ regularization or used memory-demanding methods. Furthermore, we prove that the optimal fixed regularization strength scales nearly linearly with the number of tasks $T$, specifically as $T/\ln T$. To our knowledge, this is the first such result in theoretical continual learning. Finally, we validate our theoretical findings through experiments on linear regression and neural networks, illustrating how this scaling law affects generalization and offering a practical recipe for the design of continual learning systems.

</details>


### [221] [Multi-Objective Hierarchical Optimization with Large Language Models](https://arxiv.org/abs/2601.13892)
*Andrej Schwanke,Lyubomir Ivanov,David Salinas,Frank Hutter,Arber Zela*

Main category: cs.LG

TL;DR: LLM作为替代模型和候选采样器，通过分层搜索策略解决多目标优化问题，在局部空间生成候选解，收敛到真实帕累托集


<details>
  <summary>Details</summary>
Motivation: 尽管LLM具有强大的推理能力，但在多目标优化中还不是现成的选择。传统方法通过处理数值输入、平衡探索与利用、处理多个冲突目标而在基准测试中表现优异。本文旨在缩小这一差距。

Method: 将LLM作为替代模型和候选采样器，嵌入结构化的分层搜索策略中。通过自适应地将输入空间划分为不相交的超矩形区域，并使用复合评分函数对区域进行排序，将LLM的生成过程限制在特定高潜力子空间中，使LLM只需进行局部推理而非全局推理。

Result: 在标准正则性假设下，算法生成的候选解在Hausdorff距离上收敛到真实帕累托集。实证表明，该方法持续优于基于LLM的全局多目标优化器，并在合成和真实世界基准测试中与标准进化和贝叶斯优化算法表现相当。

Conclusion: 通过将LLM作为替代模型嵌入分层搜索策略，成功地将LLM应用于多目标优化问题，实现了与传统优化方法相当的性能，同时利用了LLM的推理能力。

Abstract: Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn't have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.

</details>


### [222] [TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography](https://arxiv.org/abs/2601.13897)
*Ankita Joshi,Ashutosh Sharma,Anoushkrit Goel,Ranjeet Ranjan Jha,Chirag Ahuja,Arnav Bhavsar,Aditya Nigam*

Main category: cs.LG

TL;DR: 提出TractRLFusion：基于GPT的策略融合框架，通过数据驱动的融合策略整合多个强化学习策略，提升白质纤维束追踪的准确性和解剖可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统白质纤维束追踪方法存在准确重建白质束同时最小化虚假连接的挑战。虽然深度学习（DL）和深度强化学习（DRL）有所改进，但仍需提升追踪精度和解剖可靠性。

Method: 提出TractRLFusion框架：1）采用两阶段训练数据选择过程进行有效策略融合；2）基于GPT的融合策略整合多个RL策略；3）多critic微调阶段增强鲁棒性和泛化能力。

Result: 在HCP、ISMRM和TractoInferno数据集上的实验表明，TractRLFusion在准确性和解剖可靠性方面优于单个RL策略以及最先进的经典方法和DRL方法。

Conclusion: TractRLFusion通过融合多个RL策略有效提升了白质纤维束追踪的性能，为神经外科规划和脑连接研究提供了更可靠的工具。

Abstract: Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.

</details>


### [223] [Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained Composition](https://arxiv.org/abs/2601.13953)
*Gorgi Pavlov*

Main category: cs.LG

TL;DR: 提出Hierarchical Spectral Composition架构，通过选择布尔傅里叶基的谱系数，结合Sinkhorn约束路由和列符号调制，实现可微的布尔逻辑合成，在GPU上达到10,959 MOps/s的推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络通过梯度下降学习布尔逻辑时，通常会收敛到"模糊"的近似解，这些解在量化后性能会下降。需要一种能够精确合成布尔逻辑的可微架构。

Method: 采用分层谱合成架构：从冻结的布尔傅里叶基中选择谱系数，通过Sinkhorn约束路由（投影到Birkhoff多面体）进行组合，并添加列符号调制来实现布尔否定功能。随着维度增加，结合精确的Walsh-Hadamard系数、三元量化和MCMC并行回火优化。

Result: 在n=2时，梯度下降达到100%准确率，零路由漂移和零损失三元量化；n=3时，梯度下降76%准确率，但枚举证明所有操作都存在最优三元掩码（100%准确率，39%稀疏性）；n=4时，谱合成方法在所有操作上达到100%准确率。

Conclusion: 证明了所有测试函数都存在三元多项式阈值表示，但随着维度增加，需要超越纯梯度下降的方法来找到这些表示。该方法在GPU上实现单周期组合逻辑推理，速度为10,959 MOps/s，展示了硬件高效的神经符号逻辑合成的可行性。

Abstract: Learning precise Boolean logic via gradient descent remains challenging: neural networks typically converge to "fuzzy" approximations that degrade under quantization. We introduce Hierarchical Spectral Composition, a differentiable architecture that selects spectral coefficients from a frozen Boolean Fourier basis and composes them via Sinkhorn-constrained routing with column-sign modulation. Our approach draws on recent insights from Manifold-Constrained Hyper-Connections (mHC), which demonstrated that projecting routing matrices onto the Birkhoff polytope preserves identity mappings and stabilizes large-scale training. We adapt this framework to logic synthesis, adding column-sign modulation to enable Boolean negation -- a capability absent in standard doubly stochastic routing.
  We validate our approach across four phases of increasing complexity: (1) For n=2 (16 Boolean operations over 4-dim basis), gradient descent achieves 100% accuracy with zero routing drift and zero-loss quantization to ternary masks. (2) For n=3 (10 three-variable operations), gradient descent achieves 76% accuracy, but exhaustive enumeration over 3^8 = 6561 configurations proves that optimal ternary masks exist for all operations (100% accuracy, 39% sparsity). (3) For n=4 (10 four-variable operations over 16-dim basis), spectral synthesis -- combining exact Walsh-Hadamard coefficients, ternary quantization, and MCMC refinement with parallel tempering -- achieves 100% accuracy on all operations. This progression establishes (a) that ternary polynomial threshold representations exist for all tested functions, and (b) that finding them requires methods beyond pure gradient descent as dimensionality grows. All operations enable single-cycle combinational logic inference at 10,959 MOps/s on GPU, demonstrating viability for hardware-efficient neuro-symbolic logic synthesis.

</details>


### [224] [RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning](https://arxiv.org/abs/2601.13964)
*Cheol-Hui Lee,Hwa-Yeon Lee,Dong-Joo Kim*

Main category: cs.LG

TL;DR: RL-BioAug：利用强化学习代理自动选择最优数据增强策略的框架，显著提升EEG对比学习性能


<details>
  <summary>Details</summary>
Motivation: EEG信号具有非平稳性，传统静态或随机数据增强策略难以保留内在信息，影响对比学习性能

Method: 提出RL-BioAug框架，使用标签高效的强化学习代理自动确定最优增强策略，仅需10%标签数据指导代理策略，编码器以严格自监督方式学习鲁棒表示

Result: 在Sleep-EDFX和CHB-MIT数据集上，相比随机选择策略分别提升9.69%和8.80%的Macro-F1分数；代理针对不同任务选择不同最优策略

Conclusion: RL-BioAug有潜力替代传统启发式增强方法，建立数据增强的自主范式

Abstract: The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\% and 8.80\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\% probability for sleep stage classification and Crop \& Resize with a 77\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.

</details>


### [225] [A universal linearized subspace refinement framework for neural networks](https://arxiv.org/abs/2601.13989)
*Wenbo Cao,Weiwei Zhang*

Main category: cs.LG

TL;DR: LSR框架通过固定训练网络状态的雅可比诱导线性残差模型，在子空间中求解最小二乘问题，显著提升预测精度，无需修改网络架构或训练过程。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络主要通过梯度方法训练，但在许多应用中，其最终预测精度远未达到模型表达能力可达到的水平。梯度训练未能充分利用模型潜力，即使局部线性化产生凸问题时也是如此。

Method: 提出线性化子空间细化(LSR)框架：1) 在固定训练网络状态利用雅可比诱导线性残差模型；2) 在该子空间中求解简化直接最小二乘问题；3) 计算线性化残差模型的子空间最优解；4) 对于算子约束问题，进一步提出迭代LSR，交替进行单次LSR和监督非线性对齐。

Result: LSR系统性地暴露了梯度训练未充分利用的精度水平，经常实现数量级的误差减少。在监督函数逼近、数据驱动算子学习和物理信息算子微调等任务中，LSR显著提升了预测精度。

Conclusion: 损失诱导的数值病态性（而非非凸性或模型表达能力）是实际瓶颈。LSR通过将非线性神经表示与固定线性化点的降阶线性求解器结合，为监督学习、算子学习和科学计算提供了数值基础广泛的细化框架。

Abstract: Neural networks are predominantly trained using gradient-based methods, yet in many applications their final predictions remain far from the accuracy attainable within the model's expressive capacity. We introduce Linearized Subspace Refinement (LSR), a general and architecture-agnostic framework that exploits the Jacobian-induced linear residual model at a fixed trained network state. By solving a reduced direct least-squares problem within this subspace, LSR computes a subspace-optimal solution of the linearized residual model, yielding a refined linear predictor with substantially improved accuracy over standard gradient-trained solutions, without modifying network architectures, loss formulations, or training procedures. Across supervised function approximation, data-driven operator learning, and physics-informed operator fine-tuning, we show that gradient-based training often fails to access this attainable accuracy, even when local linearization yields a convex problem. This observation indicates that loss-induced numerical ill-conditioning, rather than nonconvexity or model expressivity, can constitute a dominant practical bottleneck. In contrast, one-shot LSR systematically exposes accuracy levels not fully exploited by gradient-based training, frequently achieving order-of-magnitude error reductions. For operator-constrained problems with composite loss structures, we further introduce Iterative LSR, which alternates one-shot LSR with supervised nonlinear alignment, transforming ill-conditioned residual minimization into numerically benign fitting steps and yielding accelerated convergence and improved accuracy. By bridging nonlinear neural representations with reduced-order linear solvers at fixed linearization points, LSR provides a numerically grounded and broadly applicable refinement framework for supervised learning, operator learning, and scientific computing.

</details>


### [226] [Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment](https://arxiv.org/abs/2601.14022)
*Rodrigo Pereira David,Luciano Araujo Dourado Filho,Daniel Marques da Silva,João Alfredo Cal-Braz*

Main category: cs.LG

TL;DR: 提出基于机器学习的框架，在相同真实驾驶条件下公平比较燃油车和电动车的CO2排放


<details>
  <summary>Details</summary>
Motivation: 道路运输脱碳需要一致透明的方法来比较不同车辆技术的CO2排放，现有方法难以在相同驾驶条件下公平比较燃油车和电动车

Method: 使用循环神经网络分别训练燃油车和电动车模型，学习从驾驶变量（速度、加速度、温度）到内部执行变量（扭矩、油门）和瞬时CO2当量排放率的映射，构建反事实场景进行对比

Result: 建立了可扩展的框架，能够在统一瞬时排放指标下公平评估动力总成技术，为真实驾驶条件下的车辆碳性能评估提供可信的数据驱动方法

Conclusion: 该机器学习框架为车辆技术比较提供了公平、可重复的评估方法，支持道路运输脱碳的数据驱动决策

Abstract: Decarbonizing road transport requires consistent and transparent methods for comparing CO2 emissions across vehicle technologies. This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. The approach isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. Recurrent neural network models are trained independently for each domain to learn the mapping from contextual driving variables (speed, acceleration, temperature) to internal actuation variables (torque, throttle) and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: What emissions would an EV have generated if it had followed the same driving profile as an ICEV? By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies. It offers a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world operating conditions.

</details>


### [227] [Universal Approximation Theorem for Input-Connected Multilayer Perceptrons](https://arxiv.org/abs/2601.14026)
*Vugar Ismailov*

Main category: cs.LG

TL;DR: 提出输入连接多层感知机（IC-MLP），该架构在隐藏层神经元中除了接收前一层输出外，还直接接收原始输入的仿射连接，并证明了其在任意有限隐藏层下的通用逼近定理。


<details>
  <summary>Details</summary>
Motivation: 探索一种改进的神经网络架构，通过在每个隐藏层神经元中引入原始输入的直接连接，增强网络的信息传递能力和表达能力。

Method: 提出IC-MLP架构，首先在单变量设置下研究，给出任意有限隐藏层的显式系统描述和迭代公式，然后扩展到向量值输入，建立相应的数学分析框架。

Result: 证明了深度IC-MLP在激活函数非线性的条件下，能够逼近闭区间上的任何连续函数（单变量）和紧致子集上的连续函数（多变量），即满足通用逼近定理。

Conclusion: IC-MLP是一种有效的神经网络架构，通过引入输入直接连接增强了表达能力，在激活函数非线性的条件下具有通用逼近能力，为神经网络设计提供了新思路。

Abstract: We introduce the Input-Connected Multilayer Perceptron (IC-MLP), a feedforward neural network architecture in which each hidden neuron receives, in addition to the outputs of the preceding layer, a direct affine connection from the raw input. We first study this architecture in the univariate setting and give an explicit and systematic description of IC-MLPs with an arbitrary finite number of hidden layers, including iterated formulas for the network functions. In this setting, we prove a universal approximation theorem showing that deep IC-MLPs can approximate any continuous function on a closed interval of the real line if and only if the activation function is nonlinear. We then extend the analysis to vector-valued inputs and establish a corresponding universal approximation theorem for continuous functions on compact subsets of $\mathbb{R}^n$.

</details>


### [228] [PAC-Private Responses with Adversarial Composition](https://arxiv.org/abs/2601.14033)
*Xiaochen Zhu,Mayuri Sridhar,Srinivas Devadas*

Main category: cs.LG

TL;DR: 该论文提出了一种基于PAC隐私的API部署机器学习模型隐私保护方法，通过在模型输出而非权重上实施隐私保护，实现了高效用和强隐私保证。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型通常通过API部署，传统的权重隐私保护方法（如DP-SGD）会导致不必要的噪声和效用损失。模型权重随训练数据变化大，但模型对特定输入的响应维度更低且更稳定，因此直接在模型输出上实施隐私保护更有优势。

Method: 采用PAC隐私框架，通过控制互信息（MI）为任意黑盒函数提供实例级隐私保证。提出新算法通过自适应噪声校准实现对抗性组合，证明互信息保证在自适应和对抗性查询下线性累积。

Result: 在表格、视觉和NLP任务上，该方法在极小的每查询隐私预算下实现高效用。在CIFAR-10上达到87.79%准确率，每步MI预算为2^{-32}。服务100万查询时，可证明将成员推理攻击成功率限制在51.08%，相当于(0.04, 10^{-5})-DP保证。通过私有响应标注公共数据蒸馏模型，在ImageNet子集上，从210,000个响应蒸馏的模型在CIFAR-10上达到91.86%准确率，MIA成功率上限为50.49%，相当于(0.02,10^{-5})-DP。

Conclusion: 该方法在API部署场景下实现了高效用和强隐私保护的平衡，通过输出级隐私保护避免了传统权重隐私方法的噪声问题，并通过自适应噪声校准解决了对抗性查询组合的挑战。

Abstract: Modern machine learning models are increasingly deployed behind APIs. This renders standard weight-privatization methods (e.g. DP-SGD) unnecessarily noisy at the cost of utility. While model weights may vary significantly across training datasets, model responses to specific inputs are much lower dimensional and more stable. This motivates enforcing privacy guarantees directly on model outputs.
  We approach this under PAC privacy, which provides instance-based privacy guarantees for arbitrary black-box functions by controlling mutual information (MI). Importantly, PAC privacy explicitly rewards output stability with reduced noise levels. However, a central challenge remains: response privacy requires composing a large number of adaptively chosen, potentially adversarial queries issued by untrusted users, where existing composition results on PAC privacy are inadequate. We introduce a new algorithm that achieves adversarial composition via adaptive noise calibration and prove that mutual information guarantees accumulate linearly under adaptive and adversarial querying.
  Experiments across tabular, vision, and NLP tasks show that our method achieves high utility at extremely small per-query privacy budgets. On CIFAR-10, we achieve 87.79% accuracy with a per-step MI budget of $2^{-32}$. This enables serving one million queries while provably bounding membership inference attack (MIA) success rates to 51.08% -- the same guarantee of $(0.04, 10^{-5})$-DP. Furthermore, we show that private responses can be used to label public data to distill a publishable privacy-preserving model; using an ImageNet subset as a public dataset, our model distilled from 210,000 responses achieves 91.86% accuracy on CIFAR-10 with MIA success upper-bounded by 50.49%, which is comparable to $(0.02,10^{-5})$-DP.

</details>


### [229] [LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems](https://arxiv.org/abs/2601.14053)
*Badri N. Patro,Vijay S. Agneeswaran*

Main category: cs.LG

TL;DR: LLMOrbit提出一个2019-2025年大语言模型的循环分类法，分析了50多个模型，识别了数据稀缺、成本增长、能耗危机三大挑战，并揭示了突破扩展墙的六大范式和后训练收益、效率革命、民主化三大范式转变。


<details>
  <summary>Details</summary>
Motivation: 随着AI从基础Transformer架构发展到接近人类水平的推理系统，需要对大语言模型的发展进行全面梳理。研究者旨在通过系统性的分类法来导航LLM领域，识别关键挑战和突破性创新，为未来发展方向提供洞察。

Method: 采用LLMOrbit循环分类法，通过八个相互关联的轨道维度分析2019-2025年间超过50个模型和15个组织。该方法涵盖架构创新、训练方法、效率模式，并系统性地识别关键危机和突破范式。

Result: 识别出三大危机：数据稀缺（2026-2028年耗尽9-27T tokens）、成本指数增长（5年内从300万到3亿+美元）、能耗不可持续（增加22倍）。发现六大突破范式：测试时计算、量化、分布式边缘计算、模型融合、高效训练、小型专用模型。揭示三大范式转变：后训练收益、效率革命、民主化。

Conclusion: 大语言模型发展面临扩展墙限制，但通过测试时计算、效率优化和开源民主化等创新范式正在突破限制。未来LLM将从被动生成转向工具使用代理系统，后训练技术和效率革命将成为关键发展方向。

Abstract: The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.

</details>


### [230] [Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2601.14092)
*Babacar Toure,Dimitrios Tsilimantos,Omid Esrafilian,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出基于注意力的多目标强化学习架构，用于无人机数据收集与能耗权衡的路径规划，无需无线信道先验知识，实现单模型适应不同偏好和动态场景。


<details>
  <summary>Details</summary>
Motivation: 无人机在无线网络数据收集任务中日益重要，但现有AI方法存在训练数据有限、忽略多目标本质的问题，难以适应高度动态的复杂环境。

Method: 采用基于注意力的多目标强化学习架构，显式处理数据收集与能耗之间的权衡，无需无线信道先验知识，开发单一模型适应不同偏好和动态参数。

Result: 大量仿真表明，该方法在性能、模型紧凑性、样本效率和泛化能力方面显著提升，优于现有强化学习解决方案。

Conclusion: 提出的注意力多目标强化学习架构有效解决了无人机路径规划中的多目标权衡问题，实现了更好的泛化能力和适应性，推动了无人机网络服务的实际部署。

Abstract: Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are becoming increasingly essential for wireless network services, particularly for data harvesting tasks. In this context, Artificial Intelligence (AI)-based approaches have gained significant attention for addressing UAV path planning tasks in large and complex environments, bridging the gap with real-world deployments. However, many existing algorithms suffer from limited training data, which hampers their performance in highly dynamic environments. Moreover, they often overlook the inherently multi-objective nature of the task, treating it in an overly simplistic manner. To address these limitations, we propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly handles the trade-off between data collection and energy consumption in urban environments, even without prior knowledge of wireless channel conditions. Our method develops a single model capable of adapting to varying trade-off preferences and dynamic scenario parameters without the need for fine-tuning or retraining. Extensive simulations show that our approach achieves substantial improvements in performance, model compactness, sample efficiency, and most importantly, generalization to previously unseen scenarios, outperforming existing RL solutions.

</details>


### [231] [Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping](https://arxiv.org/abs/2601.14099)
*Shi-Shun Chen,Xiao-Yang Li,Enrico Zio*

Main category: cs.LG

TL;DR: 提出基于时滞交叉映射的因果特征选择框架，解决工业过程中变量间时滞和相互依赖问题，提升软测量模型性能


<details>
  <summary>Details</summary>
Motivation: 现有因果特征选择方法忽略工业过程中的两个关键特性：变量间的因果关系存在时间延迟，且变量相互依赖违反传统因果推断的去相关假设，导致软测量模型精度和稳定性不足

Method: 提出时滞交叉映射框架，使用时滞收敛交叉映射(TDCCM)进行总体因果推断，时滞部分交叉映射(TDPCM)进行直接因果推断，并基于验证集性能自动确定因果阈值实现特征选择

Result: 两个真实案例研究表明，TDCCM获得最高平均性能，TDPCM在最坏情况下提升软测量稳定性和性能

Conclusion: 提出的时滞交叉映射框架能有效处理工业过程中的时滞和变量依赖问题，显著提升软测量模型的准确性和稳定性

Abstract: Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.

</details>


### [232] [Riemannian Liquid Spatio-Temporal Graph Network](https://arxiv.org/abs/2601.14115)
*Liangsi Lu,Jingchao Wang,Zhaorong Dai,Hanqian Liu,Yang Shi*

Main category: cs.LG

TL;DR: RLSTG将连续时间液态动力学与黎曼流形几何归纳偏置相结合，在非欧几里得空间中建模时空图演化，克服了传统LTC网络在欧氏空间中的几何失真问题。


<details>
  <summary>Details</summary>
Motivation: 传统Liquid Time-Constant网络（LTCs）虽然擅长建模不规则采样动态，但局限于欧氏空间，在处理具有固有非欧结构（如层次结构和循环）的真实世界图时会产生几何失真，降低表示质量。

Method: 提出黎曼液态时空图网络（RLSTG），在弯曲流形上直接公式化常微分方程（ODE），统一连续时间液态动力学与黎曼流形几何归纳偏置，建模图在非欧空间中的演化。

Result: 在真实世界基准测试中，RLSTG通过结合先进的时间动力学与黎曼空间表示，在具有复杂结构的图上实现了优越性能。提供了理论保证，将LTC稳定性定理扩展到黎曼域，并通过状态轨迹分析量化表达能力。

Conclusion: RLSTG通过将连续时间液态动力学与黎曼几何相结合，成功克服了传统LTC网络在非欧图结构中的几何失真问题，为复杂时空图建模提供了更准确的几何感知框架。

Abstract: Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io

</details>


### [233] [A model of errors in transformers](https://arxiv.org/abs/2601.14175)
*Suvrat Raju,Praneeth Netrapalli*

Main category: cs.LG

TL;DR: 本文研究LLM在确定性任务（如算术）上的错误率，提出基于注意力机制误差累积的定量模型，通过两个参数预测准确率，并进行实证验证。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在需要确定性输出的任务（如算术）上产生错误的原因，挑战现有关于LLM在长重复任务上出现"推理崩溃"或无法表达"组合函数"的观点。

Method: 提出基于注意力机制误差累积的理论模型，将LLM的众多参数重组为两个关键参数：基本噪声率和可能错误预测的token数量。使用Gemini 2.5 Flash、Gemini 2.5 Pro和DeepSeek R1进行广泛的实证测试。

Result: 理论预测与观测准确率在多种任务上高度一致，但也发现某些情况下存在偏差。模型能有效解释LLM在重复任务中的错误模式，并展示了如何通过提示工程降低错误率。

Conclusion: LLM在确定性任务上的错误可以通过注意力机制的小误差累积来解释，而非"推理崩溃"。提出的两参数模型能有效预测错误率，为理解LLM错误机制提供了新视角，并指导提示设计以提升性能。

Abstract: We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.

</details>


### [234] [Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery](https://arxiv.org/abs/2601.14196)
*Albina Galiullina,Wouter van Heeswijk,Tom van Woensel*

Main category: cs.LG

TL;DR: 提出差异化取货点提供策略，通过为每位顾客推荐单一取货点而非自由选择，减少送货卡车和顾客出行的总碳排放，在动态随机环境中使用强化学习方法优化推荐策略。


<details>
  <summary>Details</summary>
Motivation: 取货点作为家庭送货的可持续替代方案，通过订单整合可以缩短送货路线并提高首次投递成功率。然而，当顾客开车取货时，这些环境效益可能会被抵消。需要一种策略来同时减少送货卡车路线和顾客出行的碳排放。

Method: 提出差异化取货点提供策略，为每位到达顾客推荐单一取货点而非自由选择，同时保留家庭送货选项。采用基于强化学习的方法，考虑顾客与取货点之间的空间关系及其对未来路线整合的影响，在动态随机环境中优化推荐策略。

Result: 计算实验表明，差异化取货点提供策略能显著减少总碳排放。相对于纯家庭送货，总排放最多减少9%；与替代策略（包括自由选择取货点和最近取货点分配）相比，平均减少2%。在取货点多、距离短的密集城市环境中效果尤为显著。

Conclusion: 差异化取货点提供策略能有效减少送货和取货的总碳排放，特别是在密集城市环境中。当顾客不太倾向于选择取货点送货时，明确考虑顾客到达和选择的动态特性尤为重要。该策略为可持续物流提供了实用解决方案。

Abstract: Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.

</details>


### [235] [InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning](https://arxiv.org/abs/2601.14209)
*Matthew Y. R. Yang,Hao Bai,Ian Wu,Gene Yang,Amrith Setlur,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出Intervention Training (InT)方法，通过模型自我修正推理轨迹来解决强化学习中的信用分配问题，显著提升数学推理能力


<details>
  <summary>Details</summary>
Motivation: 传统结果奖励强化学习在信用分配上存在问题：失败轨迹中正确的中间步骤被惩罚，成功轨迹中错误的步骤却被强化。虽然过程奖励模型是自然解决方案，但准确优化这类模型识别纠正性推理步骤仍然困难

Method: 引入干预训练(InT)：模型通过提出短小、有针对性的修正来对自己的推理轨迹进行细粒度信用分配。利用数学推理数据集中常见的参考解，模型识别推理中的第一个错误，提出单步干预将轨迹转向正确解，然后对错误点之前的策略展开加上干预进行监督微调

Result: 经过InT和后续RL微调后，在IMO-AnswerBench上将4B参数基础模型的准确率提升了近14%，超过了gpt-oss-20b等更大的开源模型

Conclusion: Intervention Training通过让模型自我修正推理轨迹，有效解决了强化学习中的信用分配问题，为后续RL训练提供了更好的初始化，显著提升了数学推理能力

Abstract: Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.

</details>


### [236] [Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment](https://arxiv.org/abs/2601.14228)
*Punit Kumar,Vaibhav Saran,Divyesh Patel,Nitin Kulkarni,Alina Vereshchaka*

Main category: cs.LG

TL;DR: 提出一个可解释的败血症决策支持框架，包含风险分层、数据增强、离线强化学习和理由生成四个模块，在MIMIC-III和eICU数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 败血症是ICU中死亡率高的主要原因，及时准确的治疗决策对患者预后有重要影响。当前需要既能提供准确治疗建议又能解释决策理由的系统。

Method: 1. 基于聚类的风险分层模块：将患者分为低、中、高风险组；2. 合成数据增强：使用VAE和扩散模型丰富代表性不足的治疗轨迹；3. 离线强化学习代理：使用AWR算法训练，结合注意力编码器和集成模型；4. 理由生成模块：基于多模态大语言模型生成自然语言解释。

Result: 在MIMIC-III和eICU数据集上评估，该方法实现了高治疗准确性，同时为临床医生提供可解释且稳健的政策建议。

Conclusion: 该框架成功整合了风险分层、数据增强、强化学习和可解释性，为败血症治疗提供了准确且可解释的决策支持系统。

Abstract: Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.

</details>


### [237] [KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning](https://arxiv.org/abs/2601.14232)
*Egor Cherepanov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: KAGE-Env是一个JAX原生的2D平台游戏环境，将观察过程分解为独立可控的视觉轴，同时保持底层控制问题不变，用于系统分析视觉分布偏移对强化学习代理的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉强化学习基准测试往往将多种偏移源纠缠在一起，阻碍了对视觉分布偏移的系统性分析。像素基础的强化学习代理在纯视觉分布偏移下经常失败，即使潜在动态和奖励保持不变。

Method: 开发KAGE-Env环境，将观察过程分解为独立可控的视觉轴；基于此构建KAGE-Bench基准测试，包含6个已知轴套件和34个训练-评估配置对，用于隔离单个视觉偏移；使用标准PPO-CNN基线进行实验。

Result: 观察到强烈的轴依赖性失败：背景和光度偏移通常导致任务完全失败，而代理外观偏移相对温和；某些偏移保持前进运动但破坏任务完成，显示仅靠回报可能掩盖泛化失败；JAX实现达到单GPU每秒3300万环境步的速度。

Conclusion: KAGE-Env和KAGE-Bench提供了一个干净的系统化框架，用于分析视觉强化学习中的泛化问题，揭示了不同视觉偏移对性能的差异化影响，并支持快速可重复的实验。

Abstract: Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.

</details>


### [238] [Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression](https://arxiv.org/abs/2601.14238)
*Shaurya Mathur,Shreyas Bellary Manjunath,Nitin Kulkarni,Alina Vereshchaka*

Main category: cs.LG

TL;DR: FireCastRL是一个结合野火预测与智能扑救的AI框架，使用深度学习预测起火点，用强化学习在物理模拟中执行直升机灭火战术。


<details>
  <summary>Details</summary>
Motivation: 野火频率和强度不断增加，造成巨大生态和经济损失。传统野火管理主要是被动应对，只在火灾发生后才采取行动，需要更主动的预防和响应方法。

Method: 1. 使用深度时空模型预测野火起火点；2. 对高风险预测部署预训练的强化学习智能体，在物理信息3D模拟中执行直升机灭火战术；3. 生成威胁评估报告帮助应急响应者优化资源分配。

Result: 发布了包含950万个环境变量样本的大规模时空数据集，展示了深度学习与强化学习结合支持野火预测和战术响应的可行性。

Conclusion: FireCastRL框架展示了AI在野火管理中的潜力，将预测与主动响应相结合，为应急响应提供决策支持，有望改善传统被动式管理方法。

Abstract: Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.

</details>


### [239] [Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow](https://arxiv.org/abs/2601.14243)
*Haocheng Xi,Charlie Ruan,Peiyuan Liao,Yujun Lin,Han Cai,Yilong Zhao,Shuo Yang,Kurt Keutzer,Song Han,Ligeng Zhu*

Main category: cs.LG

TL;DR: Jet-RL是一个FP8强化学习训练框架，通过统一的FP8精度流程解决现有BF16训练+FP8 rollout策略的不稳定性问题，实现显著加速且保持稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 现有RL训练管道计算效率低下，rollout阶段占训练时间70%以上。虽然FP8量化训练有潜力缓解这一瓶颈，但常用的BF16训练+FP8 rollout策略在长序列和复杂任务下存在严重不稳定性和精度崩溃问题。

Method: 提出Jet-RL框架，采用统一的FP8精度流程同时用于训练和rollout，最小化数值差异，消除低效的跨步骤校准需求，实现稳定高效的RL优化。

Result: Jet-RL在rollout阶段实现最高33%加速，训练阶段最高41%加速，端到端相比BF16训练提升16%速度，在所有设置下保持稳定收敛，精度损失可忽略不计。

Conclusion: 统一的FP8精度流程是解决RL训练中数值不匹配问题的有效方案，Jet-RL框架在显著加速训练的同时保持稳定性和精度，为高效RL训练提供了新方向。

Abstract: Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [240] [Gradient-based Active Learning with Gaussian Processes for Global Sensitivity Analysis](https://arxiv.org/abs/2601.11790)
*Guerlain Lambert,Céline Helbert,Claire Lauvernet*

Main category: stat.ML

TL;DR: 提出一种基于主动学习的高效全局敏感性分析方法，利用高斯过程代理模型和梯度信息，通过改进的采集函数优化实验设计，在有限计算预算下提高敏感性分析精度。


<details>
  <summary>Details</summary>
Motivation: 复杂数值模拟器的全局敏感性分析通常受限于可负担的模型评估次数。在有限模拟次数下，需要高效的实验设计方法来构建准确的代理模型，从而降低计算负担并提高敏感性分析精度。

Method: 基于高斯过程代理模型的主动学习方法，利用GP梯度的联合后验分布，开发新的采集函数来考虑偏导数之间的相关性及其对响应面的影响。该方法针对Sobol指数和基于导数的全局敏感性度量进行优化。

Result: 在标准基准函数上与最先进方法进行比较，并在农药迁移的实际环境模型中应用。结果表明该方法比现有的DGSM导向准则更全面和稳健。

Conclusion: 提出的主动学习方法能够更有效地利用有限的计算预算，通过针对输入空间中最具信息量的区域进行采样，显著提高了全局敏感性分析的准确性，为复杂数值模拟器的敏感性分析提供了更高效的解决方案。

Abstract: Global sensitivity analysis of complex numerical simulators is often limited by the small number of model evaluations that can be afforded. In such settings, surrogate models built from a limited set of simulations can substantially reduce the computational burden, provided that the design of computer experiments is enriched efficiently. In this context, we propose an active learning approach that, for a fixed evaluation budget, targets the most informative regions of the input space to improve sensitivity analysis accuracy. More specifically, our method builds on recent advances in active learning for sensitivity analysis (Sobol' indices and derivative-based global sensitivity measures, DGSM) that exploit derivatives obtained from a Gaussian process (GP) surrogate. By leveraging the joint posterior distribution of the GP gradient, we develop acquisition functions that better account for correlations between partial derivatives and their impact on the response surface, leading to a more comprehensive and robust methodology than existing DGSM-oriented criteria. The proposed approach is first compared to state-of-the-art methods on standard benchmark functions, and is then applied to a real environmental model of pesticide transfers.

</details>


### [241] [A Kernel Approach for Semi-implicit Variational Inference](https://arxiv.org/abs/2601.12023)
*Longlin Yu,Ziheng Cheng,Shiyue Zhang,Cheng Zhang*

Main category: stat.ML

TL;DR: KSIVI提出基于核方法的半隐式变分推断，通过核Stein散度消除额外优化层，在保持表达能力的同时提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统SIVI使用ELBO优化存在偏差，而SIVI-SM虽然解决了偏差问题但引入了额外的下层优化问题，计算复杂。需要一种既无偏差又计算高效的方法。

Method: 提出KSIVI方法，在再生核希尔伯特空间中优化，将下层问题转化为显式解，目标函数简化为核Stein散度。利用半隐式分布的层次结构，通过随机梯度方法高效优化。还引入了多层层次扩展以增强表达能力。

Result: 建立了蒙特卡洛梯度估计器的方差界限优化保证，推导了统计泛化界。在合成和真实世界的贝叶斯推断任务中验证了方法的有效性。

Conclusion: KSIVI提供了一种原则性、可处理的半隐式变分推断方法，消除了额外优化层，在保持表达能力的同时提高了计算效率，为贝叶斯推断提供了有效工具。

Abstract: Semi-implicit variational inference (SIVI) enhances the expressiveness of variational families through hierarchical semi-implicit distributions, but the intractability of their densities makes standard ELBO-based optimization biased. Recent score-matching approaches to SIVI (SIVI-SM) address this issue via a minimax formulation, at the expense of an additional lower-level optimization problem. In this paper, we propose kernel semi-implicit variational inference (KSIVI), a principled and tractable alternative that eliminates the lower-level optimization by leveraging kernel methods. We show that when optimizing over a reproducing kernel Hilbert space, the lower-level problem admits an explicit solution, reducing the objective to the kernel Stein discrepancy (KSD). Exploiting the hierarchical structure of semi-implicit distributions, the resulting KSD objective can be efficiently optimized using stochastic gradient methods. We establish optimization guarantees via variance bounds on Monte Carlo gradient estimators and derive statistical generalization bounds of order $\tilde{\mathcal{O}}(1/\sqrt{n})$. We further introduce a multi-layer hierarchical extension that improves expressiveness while preserving tractability. Empirical results on synthetic and real-world Bayesian inference tasks demonstrate the effectiveness of KSIVI.

</details>


### [242] [On the Provable Suboptimality of Momentum SGD in Nonstationary Stochastic Optimization](https://arxiv.org/abs/2601.12238)
*Sharan Sahu,Cameron J. Hogan,Martin T. Wells*

Main category: stat.ML

TL;DR: 动量优化方法在动态环境中的性能分析：动量虽能抑制梯度噪声，但会显著放大漂移导致的跟踪误差，在快速变化环境中可能劣于普通SGD。


<details>
  <summary>Details</summary>
Motivation: 动量加速方法在确定性优化中已被广泛研究，但在非平稳环境（数据分布和最优参数随时间漂移）中的行为仍未被充分探索。需要理解SGD及其动量变体在动态环境中的跟踪性能。

Method: 在均匀强凸性和平滑性假设下，分析SGD、Polyak heavy-ball和Nesterov动量方法在不同步长机制下的跟踪性能。推导有限时间期望和高概率跟踪误差界，建立包含初始化瞬态项、噪声方差项和漂移跟踪滞后项的三部分分解。

Result: 发现动量方法存在基本权衡：虽然能抑制梯度噪声，但会显著增加跟踪滞后惩罚。动量会放大漂移导致的跟踪误差，且当动量参数接近1时放大效应无界。建立了梯度变化约束下的动态遗憾极小极大下界，证明惯性惩罚是信息论障碍而非分析假象。

Conclusion: 动量在动态环境中存在不可避免的"惯性窗口"，在漂移主导机制下会从根本上降低性能。这些结果为动量在动态环境中经验不稳定性提供了理论依据，并划定了SGD优于加速变体的精确机制边界。

Abstract: While momentum-based acceleration has been studied extensively in deterministic optimization problems, its behavior in nonstationary environments -- where the data distribution and optimal parameters drift over time -- remains underexplored. We analyze the tracking performance of Stochastic Gradient Descent (SGD) and its momentum variants (Polyak heavy-ball and Nesterov) under uniform strong convexity and smoothness in varying stepsize regimes. We derive finite-time bounds in expectation and with high probability for the tracking error, establishing a sharp decomposition into three components: a transient initialization term, a noise-induced variance term, and a drift-induced tracking lag. Crucially, our analysis uncovers a fundamental trade-off: while momentum can suppress gradient noise, it incurs an explicit penalty on the tracking capability. We show that momentum can substantially amplify drift-induced tracking error, with amplification that becomes unbounded as the momentum parameter approaches one, formalizing the intuition that using 'stale' gradients hinders adaptation to rapid regime shifts. Complementing these upper bounds, we establish minimax lower bounds for dynamic regret under gradient-variation constraints. These lower bounds prove that the inertia-induced penalty is not an artifact of analysis but an information-theoretic barrier: in drift-dominated regimes, momentum creates an unavoidable 'inertia window' that fundamentally degrades performance. Collectively, these results provide a definitive theoretical grounding for the empirical instability of momentum in dynamic environments and delineate the precise regime boundaries where SGD provably outperforms its accelerated counterparts.

</details>


### [243] [A Theory of Diversity for Random Matrices with Applications to In-Context Learning of Schrödinger Equations](https://arxiv.org/abs/2601.12587)
*Frank Cole,Yulong Lu,Shaurya Sehgal*

Main category: stat.ML

TL;DR: 研究随机矩阵集合中心化子为平凡的概率下界，并将其应用于Transformer网络在薛定谔方程上下文学习中的泛化能力保证


<details>
  <summary>Details</summary>
Motivation: 研究随机矩阵集合中心化子为平凡的概率问题，该问题源于随机势能线性薛定谔算子离散化，并与机器学习理论结合，为Transformer网络在薛定谔方程上下文学习提供理论保证

Method: 针对从共同分布中抽取的独立随机矩阵集合，基于样本大小N和维度d提供中心化子为平凡的概率下界，特别关注随机势能薛定谔算子离散化产生的随机矩阵族

Result: 为多个随机矩阵族提供了中心化子为平凡的概率下界，这些下界与样本大小N和维度d相关，当与近期机器学习理论结合时，能为Transformer网络在薛定谔方程上下文学习提供泛化能力保证

Conclusion: 随机矩阵中心化子平凡性的概率下界分析为Transformer网络在薛定谔方程上下文学习的泛化能力提供了理论保证，连接了随机矩阵理论与机器学习应用

Abstract: We address the following question: given a collection $\{\mathbf{A}^{(1)}, \dots, \mathbf{A}^{(N)}\}$ of independent $d \times d$ random matrices drawn from a common distribution $\mathbb{P}$, what is the probability that the centralizer of $\{\mathbf{A}^{(1)}, \dots, \mathbf{A}^{(N)}\}$ is trivial? We provide lower bounds on this probability in terms of the sample size $N$ and the dimension $d$ for several families of random matrices which arise from the discretization of linear Schrödinger operators with random potentials. When combined with recent work on machine learning theory, our results provide guarantees on the generalization ability of transformer-based neural networks for in-context learning of Schrödinger equations.

</details>


### [244] [Approximate full conformal prediction in RKHS](https://arxiv.org/abs/2601.13102)
*Davidson Lova Razafindrakoto,Alain Celisse,Jérôme Lacaille*

Main category: stat.ML

TL;DR: 提出了一种高效计算全保形预测置信区域的紧致近似方法，并引入厚度概念量化近似误差


<details>
  <summary>Details</summary>
Motivation: 全保形预测框架虽然能构建分布无关的置信预测区域，但传统方法需要训练无限多个估计器，计算上不可行，需要设计高效计算的近似方法

Method: 提出通用策略设计全保形预测区域的紧致近似，引入厚度概念量化近似与全保形区域之间的差异，理论分析依赖于损失函数和评分函数的平滑性假设

Result: 开发了可高效计算的近似置信区域，并建立了理论框架量化该近似的紧致程度

Conclusion: 该工作解决了全保形预测的计算瓶颈问题，为实际应用提供了理论保证的近似方法，厚度概念为评估近似质量提供了新工具

Abstract: Full conformal prediction is a framework that implicitly formulates distribution-free confidence prediction regions for a wide range of estimators. However, a classical limitation of the full conformal framework is the computation of the confidence prediction regions, which is usually impossible since it requires training infinitely many estimators (for real-valued prediction for instance). The main purpose of the present work is to describe a generic strategy for designing a tight approximation to the full conformal prediction region that can be efficiently computed. Along with this approximate confidence region, a theoretical quantification of the tightness of this approximation is developed, depending on the smoothness assumptions on the loss and score functions. The new notion of thickness is introduced for quantifying the discrepancy between the approximate confidence region and the full conformal one.

</details>


### [245] [Distribution-Free Confidence Ellipsoids for Ridge Regression with PAC Bounds](https://arxiv.org/abs/2601.13436)
*Szabolcs Szentpéteri,Balázs Csanád Csáji*

Main category: stat.ML

TL;DR: 本文扩展了SPS EOA算法到岭回归，推导了置信椭球大小的PAC上界，揭示了正则化参数对区域大小的影响，并在较弱激励条件下提供了更紧的界。


<details>
  <summary>Details</summary>
Motivation: 线性参数化模型中的最小二乘估计在输入激励不足时可能无法求解或不稳定，正则化（如岭回归）可以解决此问题，但需要量化估计不确定性。现有的SPS EOA算法只能用于线性回归，需要扩展到岭回归场景。

Method: 扩展SPS EOA算法到岭回归，推导概率近似正确（PAC）上界来分析正则化参数对置信椭球大小的影响，并在较弱激励条件下提供更紧的界。

Result: 成功将SPS EOA扩展到岭回归，推导的PAC上界明确显示了正则化参数如何影响区域大小，在较弱激励条件下获得了更紧的界，并通过仿真实验验证了正则化的实际效果。

Conclusion: 提出的方法为岭回归提供了非渐近置信椭球构建工具，理论分析揭示了正则化参数与置信区域大小的明确关系，为实际应用中正则化参数的选择提供了理论指导。

Abstract: Linearly parametrized models are widely used in control and signal processing, with the least-squares (LS) estimate being the archetypical solution. When the input is insufficiently exciting, the LS problem may be unsolvable or numerically unstable. This issue can be resolved through regularization, typically with ridge regression. Although regularized estimators reduce the variance error, it remains important to quantify their estimation uncertainty. A possible approach for linear regression is to construct confidence ellipsoids with the Sign-Perturbed Sums (SPS) ellipsoidal outer approximation (EOA) algorithm. The SPS EOA builds non-asymptotic confidence ellipsoids under the assumption that the noises are independent and symmetric about zero. This paper introduces an extension of the SPS EOA algorithm to ridge regression, and derives probably approximately correct (PAC) upper bounds for the resulting region sizes. Compared with previous analyses, our result explicitly show how the regularization parameter affects the region sizes, and provide tighter bounds under weaker excitation assumptions. Finally, the practical effect of regularization is also demonstrated via simulation experiments.

</details>


### [246] [Empirical Risk Minimization with $f$-Divergence Regularization](https://arxiv.org/abs/2601.13191)
*Francisco Daunas,Iñaki Esnaola,Samir M. Perlaza,H. Vincent Poor*

Main category: stat.ML

TL;DR: 本文提出了经验风险最小化与f-散度正则化问题的解决方案，建立了与约束优化问题的等价条件，引入了归一化函数概念，并开发了数值算法计算f-散度正则化解。


<details>
  <summary>Details</summary>
Motivation: 解决经验风险最小化与f-散度正则化问题，扩展适用f-散度类别，建立理论框架连接正则化与约束优化问题，为实际应用提供计算工具。

Method: 提出归一化函数概念，建立其非线性常微分方程表征，开发数值算法近似计算，分析不同f-散度问题的结构等价性，通过变换经验风险建立联系。

Result: 扩展了f-散度正则化的适用范围，建立了正则化与约束问题的等价条件，开发了有效的数值算法，揭示了不同f-散度问题的结构关系，并通过数值实验展示了不同f函数选择的影响。

Conclusion: 本文为f-散度正则化的经验风险最小化问题提供了完整的理论框架和计算工具，统一并扩展了现有结果，为实际应用中的正则化选择提供了理论指导和数值支持。

Abstract: In this paper, the solution to the empirical risk minimization problem with $f$-divergence regularization (ERM-$f$DR) is presented and conditions under which the solution also serves as the solution to the minimization of the expected empirical risk subject to an $f$-divergence constraint are established. The proposed approach extends applicability to a broader class of $f$-divergences than previously reported and yields theoretical results that recover previously known results. Additionally, the difference between the expected empirical risk of the ERM-$f$DR solution and that of its reference measure is characterized, providing insights into previously studied cases of $f$-divergences. A central contribution is the introduction of the normalization function, a mathematical object that is critical in both the dual formulation and practical computation of the ERM-$f$DR solution. This work presents an implicit characterization of the normalization function as a nonlinear ordinary differential equation (ODE), establishes its key properties, and subsequently leverages them to construct a numerical algorithm for approximating the normalization factor under mild assumptions. Further analysis demonstrates structural equivalences between ERM-$f$DR problems with different $f$-divergences via transformations of the empirical risk. Finally, the proposed algorithm is used to compute the training and test risks of ERM-$f$DR solutions under different $f$-divergence regularizers. This numerical example highlights the practical implications of choosing different functions $f$ in ERM-$f$DR problems.

</details>


### [247] [Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs](https://arxiv.org/abs/2601.13458)
*Zihan Dong,Ruijia Wu,Linjun Zhang*

Main category: stat.ML

TL;DR: 提出PCAL方法，通过半参数推断将标注预算分配问题转化为单调缺失数据框架，优化地面真实标签和成对偏好的预算分配，实现统计高效估计。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成伪标签越来越依赖人类偏好反馈，需要原则性的、预算敏感的数据获取策略。关键问题是如何在有限标注预算下，最优分配地面真实标签和成对偏好的标注资源。

Method: 基于半参数推断，将预算分配问题建模为单调缺失数据框架。提出偏好校准主动学习（PCAL）方法，学习最优数据获取策略，并开发统计高效的数据分布泛函估计器。通过直接优化估计器方差而非要求闭式解，适用于广泛问题类别。

Result: 理论证明PCAL估计器的渐近最优性，并建立关键鲁棒性保证，即使在辅助模型估计不佳时也能保持稳健性能。仿真和真实数据分析展示了方法的实际优势和优越性能。

Conclusion: 为现代AI中的预算约束学习提供了原则性和统计高效的方法框架，解决了地面真实标签和偏好反馈之间的最优预算分配问题。

Abstract: The increasing reliance on human preference feedback to judge AI-generated pseudo labels has created a pressing need for principled, budget-conscious data acquisition strategies. We address the crucial question of how to optimally allocate a fixed annotation budget between ground-truth labels and pairwise preferences in AI. Our solution, grounded in semi-parametric inference, casts the budget allocation problem as a monotone missing data framework. Building on this formulation, we introduce Preference-Calibrated Active Learning (PCAL), a novel method that learns the optimal data acquisition strategy and develops a statistically efficient estimator for functionals of the data distribution. Theoretically, we prove the asymptotic optimality of our PCAL estimator and establish a key robustness guarantee that ensures robust performance even with poorly estimated nuisance models. Our flexible framework applies to a general class of problems, by directly optimizing the estimator's variance instead of requiring a closed-form solution. This work provides a principled and statistically efficient approach for budget-constrained learning in modern AI. Simulations and real-data analysis demonstrate the practical benefits and superior performance of our proposed method.

</details>


### [248] [Small Gradient Norm Regret for Online Convex Optimization](https://arxiv.org/abs/2601.13519)
*Wenzhi Gao,Chang He,Madeleine Udell*

Main category: stat.ML

TL;DR: 提出一种新的在线凸优化问题依赖的遗憾度量$G^\star$遗憾，基于累积平方梯度范数，比现有的$L^\star$遗憾更精细，在损失函数在最优解附近曲率消失时表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的$L^\star$遗憾度量在损失函数在最优解附近曲率消失时可能不够精确，需要一种更精细的遗憾度量来更好地衡量在线凸优化算法的性能。

Method: 提出$G^\star$遗憾度量，定义为$\sum_{t=1}^T \|\nabla \ell(x^\star)\|^2$，即累积平方梯度范数在事后最优决策处的评估。建立该度量的上下界，并扩展到动态遗憾和赌博机设置。

Result: 证明$G^\star$遗憾严格细化了$L^\star$遗憾，当损失函数在最优解附近曲率消失时可以任意更尖锐。建立了上下界，并改进了插值机制下随机优化算法的收敛性分析。

Conclusion: $G^\star$遗憾是一种更精细的问题依赖遗憾度量，能更好地衡量在线凸优化算法性能，特别是在损失函数曲率消失的情况下，实验验证了理论发现。

Abstract: This paper introduces a new problem-dependent regret measure for online convex optimization with smooth losses. The notion, which we call the $G^\star$ regret, depends on the cumulative squared gradient norm evaluated at the decision in hindsight $\sum_{t=1}^T \|\nabla \ell(x^\star)\|^2$. We show that the $G^\star$ regret strictly refines the existing $L^\star$ (small loss) regret, and that it can be arbitrarily sharper when the losses have vanishing curvature around the hindsight decision. We establish upper and lower bounds on the $G^\star$ regret and extend our results to dynamic regret and bandit settings. As a byproduct, we refine the existing convergence analysis of stochastic optimization algorithms in the interpolation regime. Some experiments validate our theoretical findings.

</details>


### [249] [Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning](https://arxiv.org/abs/2601.13642)
*Yuchen Jiao,Jiin Woo,Gen Li,Gauri Joshi,Yuejie Chi*

Main category: stat.ML

TL;DR: 该论文提出了平均奖励MDPs的Q-learning算法，在单智能体和联邦学习场景下都取得了改进的样本复杂度，并首次建立了联邦Q-learning的理论保证。


<details>
  <summary>Details</summary>
Motivation: 平均奖励强化学习为长期决策提供了原则性框架，但Q-learning在平均奖励设置下的理论保证有限。现有研究在样本复杂度方面仍有改进空间，且缺乏联邦学习场景下的理论分析。

Method: 研究有限状态和动作空间的弱通信平均奖励MDPs，设计参数精心选择的Q-learning算法。分析单智能体场景，并扩展到联邦学习设置，其中多个智能体协作学习。

Result: 单智能体样本复杂度为$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$，比先前结果改进至少$\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$倍。联邦学习场景下，每个智能体样本复杂度降至$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$，仅需$\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$通信轮次。

Conclusion: 该工作首次建立了平均奖励MDPs的联邦Q-learning算法，在样本复杂度和通信复杂度方面都具有可证明的效率，为长期决策的分布式强化学习提供了理论基础。

Abstract: Average-reward reinforcement learning offers a principled framework for long-term decision-making by maximizing the mean reward per time step. Although Q-learning is a widely used model-free algorithm with established sample complexity in discounted and finite-horizon Markov decision processes (MDPs), its theoretical guarantees for average-reward settings remain limited. This work studies a simple but effective Q-learning algorithm for average-reward MDPs with finite state and action spaces under the weakly communicating assumption, covering both single-agent and federated scenarios. For the single-agent case, we show that Q-learning with carefully chosen parameters achieves sample complexity $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$, where $\|h^{\star}\|_{\mathsf{sp}}$ is the span norm of the bias function, improving previous results by at least a factor of $\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$. In the federated setting with $M$ agents, we prove that collaboration reduces the per-agent sample complexity to $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$, with only $\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$ communication rounds required. These results establish the first federated Q-learning algorithm for average-reward MDPs, with provable efficiency in both sample and communication complexity.

</details>


### [250] [Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses](https://arxiv.org/abs/2601.13874)
*Shijie Zhong,Jiangfeng Fu,Yikun Yang*

Main category: stat.ML

TL;DR: 论文研究了最大均值差异（MMD）统计量的方差特性，提出了统一的有限样本方差表征方法，并在拉普拉斯核下开发了计算复杂度从O(n²)降至O(n log n)的精确加速算法。


<details>
  <summary>Details</summary>
Motivation: MMD作为基于核的非参数双样本检验统计量，其推断准确性严重依赖于方差表征。现有方法提供了各种有限样本的MMD方差估计器，但这些估计器在零假设与备择假设下、平衡或不平衡抽样方案中往往存在差异，缺乏统一的理论框架。

Method: 通过U统计量表示和Hoeffding分解研究MMD统计量的方差特性，建立了覆盖不同假设和样本配置的统一有限样本表征。在此基础上，针对拉普拉斯核下的单变量情况，提出了精确加速方法，将计算复杂度从O(n²)降低到O(n log n)。

Result: 建立了MMD方差在有限样本下的统一理论框架，能够处理不同假设和样本配置。在拉普拉斯核的单变量情况下，成功实现了计算效率的显著提升，从二次复杂度降低到接近线性复杂度。

Conclusion: 该研究为MMD方差分析提供了统一的理论基础，并展示了在特定核函数下实现计算加速的可行性，为大规模双样本检验的实际应用提供了理论支持和方法改进。

Abstract: The maximum mean discrepancy (MMD) is a kernel-based nonparametric statistic for two-sample testing, whose inferential accuracy depends critically on variance characterization. Existing work provides various finite-sample estimators of the MMD variance, often differing under the null and alternative hypotheses and across balanced or imbalanced sampling schemes. In this paper, we study the variance of the MMD statistic through its U-statistic representation and Hoeffding decomposition, and establish a unified finite-sample characterization covering different hypotheses and sample configurations. Building on this analysis, we propose an exact acceleration method for the univariate case under the Laplacian kernel, which reduces the overall computational complexity from $\mathcal O(n^2)$ to $\mathcal O(n \log n)$.

</details>


### [251] [Intermittent time series forecasting: local vs global models](https://arxiv.org/abs/2601.14031)
*Stefano Damato,Nicolò Rubattu,Dario Azzimonti,Giorgio Corani*

Main category: stat.ML

TL;DR: 该研究首次系统比较了局部模型（iETS、TweedieGP）和全局模型（D-Linear、DeepAR、Transformers）在间歇性时间序列预测上的表现，发现D-Linear在神经网络模型中表现最佳，且优于局部模型。


<details>
  <summary>Details</summary>
Motivation: 间歇性时间序列（包含大量零值）在供应链库存中占比很大，需要概率预测来规划库存水平。虽然全局模型在时间序列预测中越来越流行，但尚未在间歇性时间序列上得到充分测试。

Method: 研究比较了局部模型（iETS、TweedieGP）和全局神经网络模型（D-Linear、DeepAR、Transformers）。对于神经网络模型，测试了三种适合间歇性序列的分布头：负二项分布、障碍偏移负二项分布和Tweedie分布。在包含超过40,000个真实世界时间序列的五个大数据集上进行实验。

Result: 在神经网络模型中，D-Linear提供最佳准确性，且始终优于局部模型，同时计算需求较低。基于Transformer的架构计算需求更高且准确性较差。在分布头中，Tweedie对最高分位数估计最好，而负二项分布整体性能最佳。

Conclusion: D-Linear是间歇性时间序列预测的有效选择，结合了良好的准确性、计算效率和优于传统局部模型的性能。Tweedie分布头在高分位数估计方面有优势，而负二项分布整体表现最佳。

Abstract: Intermittent time series, characterised by the presence of a significant amount of zeros, constitute a large percentage of inventory items in supply chain. Probabilistic forecasts are needed to plan the inventory levels; the predictive distribution should cover non-negative values, have a mass in zero and a long upper tail. Intermittent time series are commonly forecast using local models, which are trained individually on each time series. In the last years global models, which are trained on a large collection of time series, have become popular for time series forecasting. Global models are often based on neural networks. However, they have not yet been exhaustively tested on intermittent time series. We carry out the first study comparing state-of-the-art local (iETS, TweedieGP) and global models (D-Linear, DeepAR, Transformers) on intermittent time series. For neural networks models we consider three different distribution heads suitable for intermittent time series: negative binomial, hurdle-shifted negative binomial and Tweedie. We use, for the first time, the last two distribution heads with neural networks. We perform experiments on five large datasets comprising more than 40'000 real-world time series. Among neural networks D-Linear provides best accuracy; it also consistently outperforms the local models. Moreover, it has also low computational requirements. Transformers-based architectures are instead much more computationally demanding and less accurate. Among the distribution heads, the Tweedie provides the best estimates of the highest quantiles, while the negative binomial offers overall the best performance.

</details>
