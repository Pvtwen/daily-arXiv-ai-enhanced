<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 22]
- [cs.LG](#cs.LG) [Total: 82]
- [stat.ML](#stat.ML) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Interference Mitigation using U-Net Autoencoder based system](https://arxiv.org/abs/2512.13844)
*Hiten Prakash Kothari,R. Michael Buehrer*

Main category: eess.SP

TL;DR: 提出基于U-Net的自编码器框架，用于抑制通信信号中的噪声和多种干扰源，在低中SIR下优于传统方法


<details>
  <summary>Details</summary>
Motivation: 通信信号常受到噪声和各种干扰源的影响，传统干扰抑制方法在复杂干扰场景下效果有限，需要更灵活有效的解决方案

Method: 采用U-Net架构的自编码器框架，利用多尺度特征提取和跳跃连接，在保留精细时间结构的同时抑制干扰成分

Result: 在低中SIR情况下持续优于传统干扰消除方法，在高SIR下保持竞争力，在载波偏移和有色噪声等模型失配条件下表现良好

Conclusion: 多尺度神经网络架构为多种干扰类型提供了灵活有效的干扰抑制平台，展示了深度学习在通信信号处理中的潜力

Abstract: This paper proposes a U-Net-based autoencoder framework for mitigating interference in communication signals corrupted by noise and diverse interference sources. The approach targets scenarios involving both signal-plus-noise and signal-plus-interference-plus-noise mixtures, including sinusoidal interferers, LFM chirps, QPSK interferers with different sampling rates, and modulated interference such as QAM. The U-Net architecture leverages multiscale feature extraction and skip connections to preserve fine-grained temporal structure while suppressing interference components. Performance is evaluated using bit error rate and compared against conventional cancellation methods. Results show that the proposed method consistently outperforms traditional techniques in low- and mid-SIR regimes, while remaining competitive at high SIRs. Additional experiments examine the autoencoder's behavior under model mismatch conditions such as carrier offset and colored noise. The study demonstrates that multiscale neural architectures provide a flexible and effective platform for interference mitigation across a wide range of interference types.

</details>


### [2] [Simultaneous and Proportional Finger Motion Decoding Using Spatial Features from High-Density Surface Electromyography](https://arxiv.org/abs/2512.13870)
*Ricardo Gonçalves Molinari,Leonardo Abdala Elias*

Main category: eess.SP

TL;DR: MLD-BFM方法利用HD sEMG的空间信息，显著提高了连续手指运动解码精度，MLP+MLD-BFM组合达到最佳性能（R²vw=86.68%）


<details>
  <summary>Details</summary>
Motivation: 恢复自然直观的手部功能需要同时比例控制多个自由度，现有方法在利用高密度表面肌电信号的空间信息方面存在不足

Method: 提出多通道线性描述符块场方法（MLD-BFM），从EDC和FDS肌肉记录HD sEMG信号，提取区域特异性空间特征（Σ、Φ、Ω），与时间域特征和降维方法对比

Result: MLD-BFM在所有模型中均获得最高R²vw值，MLP+MLD-BFM组合表现最佳（86.68%±0.33），中指和无名指解码精度高于拇指

Conclusion: MLD-BFM提高了连续手指运动解码精度，证明了利用HD sEMG空间丰富性的重要性，为设计鲁棒、实时、响应式肌电接口提供指导

Abstract: Restoring natural and intuitive hand function requires simultaneous and proportional control (SPC) of multiple degrees of freedom (DoFs). This study systematically evaluated the multichannel linear descriptors-based block field method (MLD-BFM) for continuous decoding of five finger-joint DoFs by leveraging the rich spatial information of high-density surface electromyography (HD sEMG). Twenty-one healthy participants performed dynamic sinusoidal finger movements while HD sEMG signals were recorded from the \textit{extensor digitorum communis} (EDC) and \textit{flexor digitorum superficialis} (FDS) muscles. MLD-BFM extracted region-specific spatial features, including effective field strength ($Σ$), field-strength variation rate ($Φ$), and spatial complexity ($Ω$). Model performance was optimized (block size: $2 \times 2$; window: 0.15 s) and compared with conventional time-domain features and dimensionality reduction approaches when applied to multi-output regression models. MLD-BFM consistently achieved the highest $\mathrm{R}^2_{\mathrm{vw}}$ values across all models. The multilayer perceptron (MLP) combined with MLD-BFM yielded the best performance ($\mathrm{R}^2_{\mathrm{vw}} = 86.68\% \pm 0.33$). Time-domain features also showed strong predictive capability and were statistically comparable to MLD-BFM in some models, whereas dimensionality reduction techniques exhibited lower accuracy. Decoding accuracy was higher for the middle and ring fingers than for the thumb. Overall, MLD-BFM improved continuous finger movement decoding accuracy, underscoring the importance of taking advantage of the spatial richness of HD sEMG. These findings suggest that spatially structured features enhance SPC and provide practical guidance for designing robust, real-time, and responsive myoelectric interfaces.

</details>


### [3] [Pipeline Stage Resolved Timing Characterization of FPGA and ASIC Implementations of a RISC V Processor](https://arxiv.org/abs/2512.13866)
*Mostafa Darvishi*

Main category: eess.SP

TL;DR: 论文提出了一种流水线阶段解析的时序分析方法，用于比较32位RISC-V处理器在20nm FPGA和7nm FinFET ASIC上的时序特性差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解异构实现技术（FPGA和ASIC）在微架构层面的时序行为差异，为处理器设计提供跨平台时序收敛的指导。

Method: 提出统一分析框架，将时序路径分解为逻辑、布线和时钟组件，映射到明确定义的流水线阶段转换，使用静态时序分析和统计表征方法。

Result: 研究发现：虽然两种实现的关键路径都集中在EX到MEM流水线转换，但时序机制根本不同。FPGA时序受布线寄生效应和布局依赖的变异性主导，而ASIC时序主要由组合逻辑深度和可预测的工艺/电压/温度变化决定。

Conclusion: 流水线阶段解析分析能有效识别平台特定的瓶颈，为FPGA和ASIC实现的可预测时序收敛提供设计指导，揭示了可编程和定制化架构之间时序差异的结构性根源。

Abstract: This paper presents a pipeline stage resolved timing characterization of a 32-bit RISC V processor implemented on a 20 nm FPGA and a 7 nm FinFET ASIC platform. A unified analysis framework is introduced that decomposes timing paths into logic, routing, and clocking components and maps them to well-defined pipeline stage transitions. This approach enables systematic comparison of timing behavior across heterogeneous implementation technologies at a microarchitectural level. Using static timing analysis and statistical characterization, the study shows that although both implementations exhibit dominant critical paths in the EX to MEM pipeline transition, their underlying timing mechanisms differ fundamentally. FPGA timing is dominated by routing parasitics and placement dependent variability, resulting in wide slack distributions and sensitivity to routing topology. In contrast, ASIC timing is governed primarily by combinational logic depth and predictable parametric variation across process, voltage, and temperature corners, yielding narrow and stable timing distributions. The results provide quantitative insight into the structural origins of timing divergence between programmable and custom fabrics and demonstrate the effectiveness of pipeline stage resolved analysis for identifying platform specific bottlenecks. Based on these findings, the paper derives design implications for achieving predictable timing closure in processor architectures targeting both FPGA and ASIC implementations.

</details>


### [4] [Fundamental Limits of Localization with Fluid Antenna Systems: A Fisher Information Analysis](https://arxiv.org/abs/2512.13941)
*Abdelhamid Salem,Kai-Kit Wong,Hyundong Shin,Yangyang Zhang*

Main category: eess.SP

TL;DR: 本文研究了流体天线系统(FAS)中定位的基本极限，使用费舍尔信息理论框架推导了定位误差下界，并提出了最优端口选择策略来提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 随着未来无线网络对高精度定位的需求日益增长，需要探索流体天线系统在定位方面的潜力。传统天线系统的定位能力有限，而FAS通过合成孔径效应可能提供更好的定位性能。

Method: 采用费舍尔信息理论框架，建立了统一的模型来量化从到达时间(ToA)和到达角(AoA)测量中提取的定位信息。推导了等效费舍尔信息矩阵(EFIM)和定位误差下界(PEB)的闭式表达式，并提出了基于贪心算法和凸松弛的最优端口选择策略。

Result: 数值结果表明，提出的端口选择方案相比随机激活能显著收紧定位误差下界，证实了FAS在高精度定位方面的强大潜力。

Conclusion: FAS能够实现高精度定位，研究结果为未来无线网络中FAS辅助的定位系统提供了理论分析和实用设计指导。

Abstract: In this letter, we investigate the fundamental limits of localization in fluid antenna systems (FAS) utilizing a Fisher-information-theoretic framework. We develop a unified model to quantify the localization information extractable from time-of-arrival (ToA) and angle-of-arrival (AoA) measurements, explicitly capturing the synthetic aperture effects induced by FAS. Closed-form expressions are derived for the equivalent Fisher information matrix (EFIM) and the corresponding positioning error bound (PEB) in both user-side and base-station (BS)-side FAS configurations. Also, we propose optimal port-selection strategies based on greedy algorithms and convex relaxation to maximize the information gain under a constrained number of activated ports. Numerical results demonstrate that the proposed port-selection schemes can substantially tighten the PEB compared with random activation, thereby confirming the strong potential of FAS to enable high-precision localization. These results offer analytical insights and practical design guidelines for FAS-aided positioning in future-generation wireless networks

</details>


### [5] [Hierarchical Deep Reinforcement Learning for Robust Access in Cognitive IoT Networks under Smart Jamming Attacks](https://arxiv.org/abs/2512.14013)
*Nadia Abdolkhani,Walaa Hamouda*

Main category: eess.SP

TL;DR: 提出分层深度确定性策略梯度(H-DDPG)框架，解决认知物联网中次级用户在能量约束和智能干扰下的动态频谱接入问题


<details>
  <summary>Details</summary>
Motivation: 认知物联网网络中，次级用户面临能量约束和智能干扰器的对抗性干扰，需要同时满足能量和干扰约束，决策过程复杂（多层级决策结构和混合动作空间）

Method: 提出分层深度确定性策略梯度(H-DDPG)框架，将决策过程分解为三个层级：高层策略决定传输或能量收集模式，中层策略选择信道，低层执行器输出连续功率水平；干扰器建模为使用离散DDPG变体的强化学习智能体

Result: 仿真结果表明，H-DDPG方法优于传统的扁平强化学习基线方法

Conclusion: 提出的分层强化学习框架能有效解决认知物联网中复杂的动态频谱接入问题，处理混合动作空间和多层级决策挑战

Abstract: In this paper, we address the challenge of dynamic spectrum access in a cognitive Internet of Things (CIoT) network where a secondary user (SU) operates under both energy constraints and adversarial interference from a smart jammer. The SU coexists with primary users (PUs) and must ensure that its transmissions do not exceed a predefined interference threshold on licensed channels. At each time slot, the SU must jointly determine whether to transmit or harvest energy, which channel to access, and the appropriate transmit power while satisfying energy and interference constraints. Meanwhile, a smart jammer actively selects a channel to disrupt, aiming to degrade the SU's communication performance. This setting presents a significant challenge due to its multi-level decision structure and hybrid action space, which combines both discrete and continuous decisions. To tackle this, we propose a novel Hierarchical Deep Deterministic Policy Gradient (H-DDPG) framework that decomposes the decision-making process into three levels: the high-level policy determines the mode (transmit or harvest), the mid-level policy selects the channel, and the low-level actor outputs a continuous power level. Concurrently, the jammer is modeled as a reinforcement learning agent that learns an adaptive channel jamming strategy using a discrete variant of DDPG. Simulation results show that our H-DDPG approach outperforms conventional flat reinforcement learning baselines.

</details>


### [6] [Cooperative Caching Towards Efficient Spectrum Utilization in Cognitive-IoT Networks](https://arxiv.org/abs/2512.14029)
*Nadia Abdolkhani,Walaa Hamouda*

Main category: eess.SP

TL;DR: 本文提出了一种基于深度强化学习的认知物联网网络联合协作缓存与频谱接入协调方法，通过缓存主用户内容并为其提供服务，实现互惠共赢，显著降低延迟并提升网络性能。


<details>
  <summary>Details</summary>
Motivation: 认知物联网网络中，随着无线需求增长，高效频谱共享变得至关重要。传统认知无线电方法中CIoT代理需要为主用户腾出频谱，而中继技术仅支持频谱共享，无法有效降低延迟。需要一种新方法来重新定义频谱共享，将数据推向网络边缘。

Method: 提出基于深度强化学习的框架，联合优化缓存策略和频谱接入。CIoT代理与主用户协作，缓存主用户内容并为其提供服务，将数据推向网络边缘，减少检索距离。该方法在动态频谱接入场景下实现协作优化。

Result: 仿真结果表明，该方法在降低延迟、提高CIoT和主用户缓存命中率、增强网络吞吐量方面优于其他方法。展示了DRL引导的缓存在动态频谱接入场景中的协作优势。

Conclusion: 该方法重新定义了频谱共享，为CIoT网络设计提供了新视角，展示了在资源受限条件下通过DRL引导的缓存提升CIoT性能的潜力，强调了协作相对于传统频谱接入方法的优势。

Abstract: In cognitive Internet of Things (CIoT) networks, efficient spectrum sharing is essential to address increasing wireless demands. This paper presents a novel deep reinforcement learning (DRL)-based approach for joint cooperative caching and spectrum access coordination in CIoT networks, enabling the CIoT agents to collaborate with primary users (PUs) by caching PU content and serving their requests, fostering mutual benefits. The proposed DRL framework jointly optimizes caching policy and spectrum access under challenging conditions. Unlike traditional cognitive radio (CR) methods, where CIoT agents vacate the spectrum for PUs, or relaying techniques, which merely support spectrum sharing, caching brings data closer to the edge, reducing latency by minimizing retrieval distance. Simulations demonstrate that our approach outperforms others in lowering latency, increasing CIoT and PU cache hit rates, and enhancing network throughput. This approach redefines spectrum sharing, offering a fresh perspective on CIoT network design and illustrating the potential of DRL-guided caching to highlight the benefits of collaboration over dynamic spectrum access scenarios, elevating CIoT performance under constrained resources.

</details>


### [7] [Cooperative Rotatable IRSs for Wireless Communications: Joint Beamforming and Orientation Optimization](https://arxiv.org/abs/2512.14037)
*Qiaoyan Peng,Qingqing Wu,Guangji Chen,Wen Chen,Shanpu Shen,Shaodan Ma*

Main category: eess.SP

TL;DR: 该论文研究了双可旋转智能反射面(IRS)系统，通过联合优化基站波束成形、IRS被动波束成形和旋转角度来最大化接收信噪比，在LOS信道下推导出闭式最优旋转解，在莱斯信道下提出AO-PSO算法，验证了可旋转IRS相比固定IRS的性能优势。


<details>
  <summary>Details</summary>
Motivation: 传统固定IRS系统缺乏方向调整自由度，限制了信道塑造能力。可旋转IRS引入新的自由度，能够自适应调整反射面方向，从而更好地优化无线传播环境，提高通信性能。

Method: 1. 在LOS信道下，采用粒子群优化(PSO)确定IRS旋转，并推导出二维部署下的闭式最优旋转表达式；2. 在一般莱斯衰落信道下，提出交替优化与PSO结合的AO-PSO算法，联合优化基站波束成形、IRS被动波束成形和旋转角度。

Result: 数值结果表明：1. 可旋转IRS相比固定IRS方案获得显著性能增益；2. 在给定足够总IRS元素数时，双可旋转IRS系统优于单可旋转IRS系统。

Conclusion: 可旋转IRS为无线通信系统提供了新的优化维度，通过联合设计旋转角度和波束成形能够显著提升系统性能，特别是在多IRS协作场景下具有明显优势。

Abstract: Rotatable intelligent reflecting surfaces (IRSs) introduce a new degree of freedom (DoF) for shaping wireless propagation by adaptively adjusting the orientation of IRSs. This paper considers an angle-dependent reflection model in a wireless communication system aided by two rotatable IRSs. Specifically, we study the joint design of the base station transmit beamforming, as well as the cooperative passive beamforming and orientation of the two IRSs, to maximize the received signal-to-noise ratio (SNR). Under the light-of-sight (LoS) channels, we first develop a particle swarm optimization (PSO) based method to determine the IRS rotation and derive an optimal rotation in a closed-form expression for a two-dimensional IRS deployment. Then, we extend the design to the general Rician fading channels by proposing an efficient alternating optimization and PSO (AO-PSO) algorithm. Numerical results validate the substantial gains achieved by the IRS rotation over fixed-IRS schemes and also demonstrate the superior performance of the double rotatable IRSs over a single rotatable IRS given a sufficient total number of IRS elements.

</details>


### [8] [Hybrid Iterative Detection for OTFS: Interplay between Local L-MMSE and Global Message Passing](https://arxiv.org/abs/2512.14116)
*Ruohai Yang,Shuangyang Li,Han Yu,Zhiqiang Wei,Kai Wan,Giuseppe Caire*

Main category: eess.SP

TL;DR: 提出一种结合L-MMSE和消息传递的混合低复杂度迭代检测框架，用于OTFS系统，通过DD域预编码实现结构化信道矩阵，在保证性能的同时降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统OTFS检测算法（如线性均衡器和消息传递方法）在高移动性无线通信中存在噪声增强问题，或在复杂双选择性信道（特别是存在分数延迟和多普勒频移时）下失效。

Method: 提出混合低复杂度迭代检测框架：1) 使用新的延迟-多普勒域预编码器（DDCP）使有效信道矩阵呈现结构化形式（局部密集块稀疏连接）；2) 对密集块应用低维L-MMSE估计；3) 利用消息传递处理稀疏块间连接。

Result: 1) 复杂度分析显示计算成本低于全尺寸L-MMSE检测；2) 收敛性能证实快速可靠收敛；3) 误码率性能在各种信道条件下显著优于现有方法，多径场景下接近匹配滤波器界（MFB）。

Conclusion: 提出的混合消息传递检测框架为OTFS系统提供了高效、低复杂度且接近最优的检测方案，特别适用于高移动性无线通信场景。

Abstract: Orthogonal time frequency space (OTFS) modulation has emerged as a robust solution for high-mobility wireless communications. However, conventional detection algorithms, such as linear equalizers and message passing (MP) methods, either suffer from noise enhancement or fail under complex doubly-selective channels, especially in the presence of fractional delay and Doppler shifts. In this paper, we propose a hybrid low-complexity iterative detection framework that combines linear minimum mean square error (L-MMSE) estimation with MP-based probabilistic inference. The key idea is to apply a new delay-Doppler (DD) commutation precoder (DDCP) to the DD domain signal vector, such that the resulting effective channel matrix exhibits a structured form with several locally dense blocks that are sparsely inter-connected. This precoding structure enables a hybrid iterative detection strategy, where a low-dimensional L-MMSE estimation is applied to the dense blocks, while MP is utilized to exploit the sparse inter-block connections. Furthermore, we provide a detailed complexity analysis, which shows that the proposed scheme incurs lower computational cost compared to the full-size L-MMSE detection. The simulation results of convergence performance confirm that the proposed hybrid MP detection achieves fast and reliable convergence with controlled complexity. In terms of error performance, simulation results demonstrate that our scheme achieves significantly better bit error rate (BER) under various channel conditions. Particularly in multipath scenarios, the BER performance of the proposed method closely approaches the matched filter bound (MFB), indicating its near-optimal error performance.

</details>


### [9] [Antenna Coding Optimization Based on Pixel Antennas for MIMO Wireless Power Transfer with DC Combining](https://arxiv.org/abs/2512.14135)
*Yijun Chen,Shanpu Shen,Tianrui Qiao,Hongyu Li,Jun Qian,Ross Murch*

Main category: eess.SP

TL;DR: 本文研究基于像素天线的天线编码作为增强MIMO无线功率传输系统的新自由度，通过联合优化天线编码和发射波束成形，在完美CSI下最大化输出直流功率，相比固定天线配置系统可获得高达15dB的平均输出直流功率增益。


<details>
  <summary>Details</summary>
Motivation: 探索天线编码作为增强MIMO无线功率传输系统的新自由度，像素天线与流体天线系统概念相关，进一步推广了辐射模式可重构性，旨在提高无线功率传输效率。

Method: 引入波束空间信道模型展示天线编码器实现的可重构辐射模式；采用交替优化方法，结合拟牛顿法处理发射波束成形设计，使用带热启动的连续穷举布尔优化方法处理天线编码设计。

Result: 仿真结果表明，提出的基于像素天线的MIMO WPT系统相比传统固定天线配置系统，在平均输出直流功率上可获得高达15dB的增益。

Conclusion: 像素天线通过天线编码技术显著提升了MIMO无线功率传输系统的效率，验证了像素天线在增强WPT性能方面的巨大潜力。

Abstract: This paper investigates antenna coding based on pixel antennas as a new degree of freedom for enhancing multiple-input multiple-output (MIMO) wireless power transfer (WPT) systems. Antenna coding is closely related to the Fluid Antenna System (FAS) concept and further generalizes the radiation pattern reconfigurability. We first introduce a beamspace channel model to demonstrate reconfigurable radiation patterns enabled by antenna coders. By jointly optimizing the antenna coding and transmit beamforming with perfect channel state information (CSI), we exploit gains from antenna coding, transmit beamforming, and rectenna nonlinearity to maximize the output DC power. We adopt an alternating optimization approach with the quasi-Newton method and Successive Exhaustive Boolean Optimization (SEBO) method with warm-start to handle the transmit beamforming design and antenna coding design respectively. Finally, simulation results show that the proposed MIMO WPT system with pixel antennas achieves up to 15 dB gain in average output DC power compared with a conventional system with fixed antenna configuration, highlighting the potential of pixel antennas for boosting the WPT efficiency.

</details>


### [10] [Rethinking Gaussian-Windowed Wavelets for Damping Identification](https://arxiv.org/abs/2512.14205)
*Hadi M. Daniali,Martin v. Mohrenschildt*

Main category: eess.SP

TL;DR: 本文挑战了模态分析中基于高斯小波（如Morlet和Gabor）的阻尼估计传统方法，提出数据驱动的包络优化框架，发现三角窗和Welch窗在中高信噪比下优于高斯小波方法，而Blackman滤波在低信噪比和密集模态下更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 挑战模态分析中普遍使用高斯小波（Morlet、Gabor）进行阻尼估计的传统做法，认为这种方法很少受到质疑，需要探索更优的包络基阻尼估计器。

Method: 提出数据驱动框架，利用已知真实包络的合成脉冲响应优化包络形状和参数，系统探索包络基阻尼估计器，并与频域方法（LSRF、pLSCF、PP、Yoshida方法）进行基准测试。

Result: 三角窗和Welch窗在中高信噪比下表现优于或与高斯小波方法相当；Blackman滤波在低信噪比和密集模态情况下表现出更强的鲁棒性；频域方法中LSRF在极低信噪比下最可靠，而非高斯优化包络估计器在信噪比提高时表现优异。

Conclusion: 高斯小波并非模态分析中阻尼估计的最佳选择，数据驱动的包络优化方法能提供更优性能，特别是在不同信噪比和模态密度条件下，应根据具体应用场景选择合适的估计器。

Abstract: In modal analysis, the prevalent use of Gaussian-based wavelets (such as Morlet and Gabor) for damping estimation is rarely questioned. In this study, we challenge this conventional approach by systematically exploring envelope-based damping estimators and proposing a data-driven framework that optimizes the shape and parameters of the envelope utilizing synthetic impulse responses with known ground-truth envelopes. The performance of the resulting estimators is benchmarked across a range of scenarios and compared against frequency-domain damping estimation methods, including Least Squares Rational Function (LSRF), poly-reference Least Squares Complex Frequency-Domain (pLSCF), peak picking (PP), and the Yoshida method. Our findings indicate that Triangle and Welch windows consistently outperform or are on par with Gaussian wavelet methods in contexts of moderate to high signal-to-noise ratios (SNR). In contrast, Blackman filtering demonstrates superior robustness under low SNR conditions and scenarios involving closely spaced modes. Among the frequency-domain methods assessed, LSRF shows the most reliability at very low SNR; however, the non-Gaussian optimized envelope estimators perform exceptionally well as the SNR improves.

</details>


### [11] [Graph Signal Denoising Using Regularization by Denoising and Its Parameter Estimation](https://arxiv.org/abs/2512.14213)
*Hayate Kojima,Hiroshi Higashi,Yuichi Tanaka*

Main category: eess.SP

TL;DR: 提出一种基于去噪正则化(RED)的可解释图信号去噪方法，通过深度算法展开进行参数估计，在合成和真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将图像处理中的RED技术扩展到图信号去噪领域，利用RED的优势（显式使用去噪器、梯度易计算）来解决图信号去噪问题，特别是针对图神经网络等复杂去噪器。

Method: 1) 将RED技术适配到图信号去噪；2) 证明多种图信号去噪器（包括图神经网络）满足RED条件；3) 从图滤波器角度分析RED有效性；4) 提出基于深度算法展开的监督和无监督参数估计方法。

Result: 在合成和真实数据集上的去噪实验表明，所提方法在均方误差上优于现有图信号去噪方法，特别是在无监督设置下增强了算法适用性。

Conclusion: 成功将RED技术扩展到图信号去噪领域，提供了一种可解释且有效的去噪框架，通过深度算法展开的参数估计方法进一步提升了算法实用性。

Abstract: In this paper, we propose an interpretable denoising method for graph signals using regularization by denoising (RED). RED is a technique developed for image restoration that uses an efficient (and sometimes black-box) denoiser in the regularization term of the optimization problem. By using RED, optimization problems can be designed with the explicit use of the denoiser, and the gradient of the regularization term can be easily computed under mild conditions. We adapt RED for denoising of graph signals beyond image processing. We show that many graph signal denoisers, including graph neural networks, theoretically or practically satisfy the conditions for RED. We also study the effectiveness of RED from a graph filter perspective. Furthermore, we propose supervised and unsupervised parameter estimation methods based on deep algorithm unrolling. These methods aim to enhance the algorithm applicability, particularly in the unsupervised setting. Denoising experiments for synthetic and real-world datasets show that our proposed method improves signal denoising accuracy in mean squared error compared to existing graph signal denoising methods.

</details>


### [12] [Robust Design for Multi-Antenna LEO Satellite Communications with Fractional Delay and Doppler Shifts: An RSMA-OTFS Approach](https://arxiv.org/abs/2512.14287)
*Yunnuo Xu,Yumeng Zhang,Yijie Mao,Bruno Clerck,Yun Hee Kim,Yujun Li*

Main category: eess.SP

TL;DR: RSMA-OTFS结合方案提升LEO卫星通信系统在高速移动环境下的多用户传输性能，通过OTFS稳定时变信道表征和RSMA增强干扰抑制能力。


<details>
  <summary>Details</summary>
Motivation: LEO卫星通信系统面临卫星高速移动带来的挑战，难以获取可靠的瞬时信道状态信息，导致多用户传输性能下降。需要解决时变信道表征和干扰抑制问题。

Method: 提出RSMA-OTFS集成方案：1) OTFS在延迟-多普勒域稳定表征时变信道；2) RSMA提供干扰抑制和CSIT不完美的鲁棒性；3) 建立RSMA-OTFS的跨域输入输出关系；4) 设计基于加权最小均方误差的交替优化算法最大化遍历和速率。

Result: 仿真结果表明：1) 实际传播效应（非正交矩形脉冲成形、分数延迟、多普勒频移、不完美CSIT）会显著降低性能；2) RSMA-OTFS方案在不同用户部署和CSIT质量下均表现出更高的遍历和速率和对CSIT不确定性的鲁棒性。

Conclusion: RSMA-OTFS为LEO卫星通信系统提供了一种全面的解决方案，能够有效应对高速移动环境下的信道时变性和干扰问题，提升多用户传输性能。

Abstract: Low-Earth-orbit (LEO) satellite communication systems face challenges due to high satellite mobility, which hinders the reliable acquisition of instantaneous channel state information at the transmitter (CSIT) and subsequently degrades multi-user transmission performance. This paper investigates a downlink multi-user multi-antenna system, and tackles the above challenges by introducing orthogonal time frequency space (OTFS) modulation and rate-splitting multiple access (RSMA) transmission. Specifically, OTFS enables stable characterization of time-varying channels by representing them in the delay-Doppler domain. However, realistic propagation introduces various inter-symbol and inter-user interference due to non-orthogonal yet practical rectangular pulse shaping, fractional delays, Doppler shifts, and imperfect (statistical) CSIT. In this context, RSMA offers promising robustness for interference mitigation and CSIT imperfections, and hence is integrated with OTFS to provide a comprehensive solution. A compact cross-domain input-output relationship for RSMA-OTFS is established, and an ergodic sum-rate maximization problem is formulated and solved using a weighted minimum mean-square-error based alternating optimization algorithm that does not depend on channel sparsity. Simulation results reveal that the considered practical propagation effects significantly degrade performance if unaddressed. Furthermore, the RSMA-OTFS scheme demonstrates improved ergodic sum-rate and robustness against CSIT uncertainty across various user deployments and CSIT qualities.

</details>


### [13] [User Localization and Channel Estimation for Pinching-Antenna Systems (PASS)](https://arxiv.org/abs/2512.14351)
*Xiaoxia Xu,Xidong Mu,Yuanwei Liu,Hong Xing,Arumugam Nallanathan*

Main category: eess.SP

TL;DR: 提出了一种用于夹持天线系统(PASS)的新型用户定位和信道估计框架，通过将天线分组为子阵列来协同估计用户/散射体位置并重建信道，考虑了单波导和多波导结构，开发了基于正交匹配追踪的几何一致性定位算法。


<details>
  <summary>Details</summary>
Motivation: 夹持天线系统需要有效的用户定位和信道估计方法，现有方法在几何多样性和角度模糊性方面存在限制，特别是在多波导场景中需要更精确的定位能力。

Method: 提出单波导(SW)和多波导(MW)两种结构：SW采用交替激活的子阵列，MW在每个波导上部署一个子阵列实现并发测量。开发了基于正交匹配追踪的几何一致性定位(OMP-GCL)算法，利用子阵列间几何关系和压缩感知进行精确估计，并扩展到3D场景。

Result: 理论分析表明：增加几何多样性可提高估计精度；SW在180°半空间内几何多样性有限且存在角度模糊，MW可实现全空间观测并减少开销。数值结果显示MW仅用三个波导就能在2D和3D场景中分别实现厘米级和分米级定位精度。

Conclusion: 提出的PASS框架和OMP-GCL算法有效解决了夹持天线系统中的用户定位和信道估计问题，多波导结构相比单波导具有更好的几何多样性、全空间观测能力和更低的开销，在2D和3D场景中均表现出优异的定位性能。

Abstract: This letter proposes a novel user localization and channel estimation framework for pinching-antenna systems (PASS), where pinching antennas are grouped into subarrays on each waveguide to cooperatively estimate user/scatterer locations, thus reconstructing channels. Both single-waveguide (SW) and multi-waveguide (MW) structures are considered. SW consists of multiple alternatingly activated subarrays, while MW deploys one subarray on each waveguide to enable concurrent subarray measurements. For the 2D scenarios with a fixed user/scatter height, an orthogonal matching pursuit-based geometry-consistent localization (OMP-GCL) algorithm is proposed, which leverages inter-subarray geometric relationships and compressed sensing for precise estimation. Theoretical analysis on Cramér-Rao lower bound (CRLB) demonstrates that: 1) The estimation accuracy can be improved by increasing the geometric diversity through multi-subarray deployment; and 2) SW provides a limited geometric diversity within a $180^\circ$ half space and leads to angle ambiguity, while MW enables full-space observations and reduces overheads. The OMP-GCL algorithm is further extended to 3D scenarios, where user and scatter heights are also estimated. Numerical results validate the theoretical analysis, and verify that MW achieves centimeter- and decimeter-level localization accuracy in 2D and 3D scenarios with only three waveguides.

</details>


### [14] [Sparse OFDM Design for Interference and Ambiguity Mitigation in Multi-Static ISAC](https://arxiv.org/abs/2512.14357)
*Navid Amani,Priyanka Maity,Musa Furkan Keskin,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出一种基于OFDM波形设计的随机稀疏资源分配方案，用于6G多基站感知框架，同时减轻基站间干扰和感知模糊性。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要将雷达式感知能力集成到通信基础设施中，多基站感知框架面临基站间干扰和感知模糊性的挑战。

Method: 采用半双工基站作为发射或接收节点，提出基于OFDM波形设计的随机稀疏资源分配方案，通过时频域受控不规则性解决稀疏信号引起的模糊性，并优化子载波间距以抵抗符号间干扰和载波间干扰。

Result: 仿真结果表明，所提设计在存在多个目标和杂波的情况下具有有效性和鲁棒性，且噪声基底升高可忽略不计。

Conclusion: 该随机稀疏资源分配方案能够有效解决6G多基站感知中的干扰和模糊性问题，为集成感知与通信提供了可行方案。

Abstract: The sixth-generation (6G) wireless networks promises the integration of radar-like sensing capabilities into communication infrastructure. In this paper, we investigate a multi-static sensing framework where half-duplex base stations (BSs) are assigned as either transmitter or sensing receiver nodes. We propose a randomized sparse resource allocation scheme based on orthogonal frequency division multiplexing (OFDM) waveform design tailored for the multi-static scenario to simultaneously mitigate inter-BS interference (IBI) and sensing ambiguities. The waveform design also ensures robustness against inter-symbol interference (ISI) and intercarrier interference (ICI) via a judicious choice of subcarrier spacing according to the deployment of BSs. The potential ambiguity caused by sparse signaling is addressed through controlled irregularity in both time and frequency domains, with a negligible noise floor elevation. Simulation results demonstrate the effectiveness and resilience of the proposed design in the presence of multiple targets and clutter.

</details>


### [15] [Pragmatic Earth-Fixed Beam Management for 3GPP NTN Common Signaling in LEO Satellites](https://arxiv.org/abs/2512.14368)
*Xavier Artiga,Màrius Caus,Ana Pérez-Neira*

Main category: eess.SP

TL;DR: 提出一种设计波束覆盖布局和波束跳变照明模式的方法，用于LEO卫星高效广播3GPP NTN公共信令，在满足用户SINR阈值的同时最小化覆盖整个区域所需的时间资源。


<details>
  <summary>Details</summary>
Motivation: LEO卫星的EIRP（等效全向辐射功率）有限，需要高效的方法来广播3GPP NTN公共信令到大的覆盖区域。传统方法需要大量波束，资源消耗大，需要优化设计以减少波束数量并提高效率。

Method: 提出系统化设计方法，包括：(1) 地球固定网格波束布局设计；(2) 波束赋形向量和波束功率分配；(3) 波束跳变模式；(4) 3GPP公共信令的空间、时间和频率资源分配。提出两种主要波束布局方案：基于低交叉水平的相控阵波束和基于加宽波束的方案。

Result: 数值评估显示两种方案性能相似，但优化交叉水平的相控阵波束效果最佳。对于评估的系统，波束数量从1723减少到451个，结合适当的波束跳变模式和调度方案，实现了100%的覆盖率，在3GPP最严格的20ms公共信令周期下，公共信令效率达到80.6%。

Conclusion: 提出的方法能显著减少LEO卫星广播3GPP NTN公共信令所需的波束数量，提高资源利用效率，在满足覆盖和SINR要求的同时实现高信令效率，为EIRP受限的LEO卫星系统提供了实用的设计解决方案。

Abstract: This work proposes a pragmatic method for the design of beam footprint layouts and beam hopping illumination patterns to efficiently broadcast 3GPP NTN common signaling to large coverage areas using EIRP-limited LEO satellites. This method minimizes the time resources required to sweep over the whole coverage while ensuring that the signal-to-interference-plus-noise ratio received by users is above a given threshold. It discusses the design of: (i) an Earth-fixed grid of beam layouts; (ii) beamforming vectors and beam power allocation; (iii) beam hopping patterns and (iv) space, time and frequency resource allocation of 3GPP common signaling. Two main beam layout solutions are proposed to significantly reduce the number of beams required to illuminate the coverage area: one based on phased array beams with low beam crossover levels and the other on widened beams. A numerical evaluation using practical system parameters showed that both solutions perform similarly, but that the best result is obtained with phased arrays beams with optimized beam cross over levels. Indeed, for the system evaluated, they allowed reducing the total number of beams from 1723 to 451, which combined with a proper beam hopping pattern and scheduling scheme allowed obtaining a coverage ratio of 100% and a common signaling efficiency (i.e. number of slots carrying common signaling over total number of slots) up to 80.6% for the most stringent common signaling periodicity of 20 ms considered by 3GPP.

</details>


### [16] [Terahertz Signal Coverage Enhancement in Hall Scenarios Based on Single-Hop and Dual-Hop Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2512.14394)
*Ben Chen,Zhangdui Zhong,Ke Guan,Danping He,Yiran Wang,Jianwen Ding,Qi Luo*

Main category: eess.SP

TL;DR: 该论文提出了一种将天线阵列RIS模型集成到射线追踪仿真平台的方法，用于评估RIS在室内太赫兹通信中的覆盖增强效果，重点关注单跳和双跳RIS配置在不同部署方案下的性能。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信虽然能提供超高数据速率，但其固有的高自由空间路径损耗严重限制了覆盖范围。为了扩展太赫兹网络的实际部署，需要解决覆盖范围受限这一关键挑战。可重构智能表面（RIS）能够动态操控电磁波传播，为增强太赫兹覆盖提供了解决方案。

Method: 将基于天线阵列的RIS模型集成到射线追踪仿真平台中，以室内大厅为代表性案例研究，评估单跳和双跳RIS配置在各种部署方案下对室内信号覆盖的增强效果。

Result: 开发了一个评估框架，为优化RIS辅助的室内太赫兹通信和覆盖估计提供了有价值的见解和设计参考。该框架能够分析不同RIS配置对信号覆盖的增强效果。

Conclusion: RIS技术能有效增强太赫兹通信的覆盖范围，特别是在室内环境中。通过合理的RIS部署配置（单跳或双跳），可以显著改善信号覆盖，为太赫兹网络的实际部署提供技术支持。

Abstract: Terahertz (THz) communication offers ultra-high data rates and has emerged as a promising technology for future wireless networks. However, the inherently high free-space path loss of THz waves significantly limits the coverage range of THz communication systems. Therefore, extending the effective coverage area is a key challenge for the practical deployment of THz networks. Reconfigurable intelligent surfaces (RIS), which can dynamically manipulate electromagnetic wave propagation, provide a solution to enhance THz coverage. To investigate multi-RIS deployment scenarios, this work integrates an antenna array-based RIS model into the ray-tracing simulation platform. Using an indoor hall as a representative case study, the enhancement effects of single-hop and dual-hop RIS configurations on indoor signal coverage are evaluated under various deployment schemes. The developed framework offers valuable insights and design references for optimizing RIS-assisted indoor THz communication and coverage estimation.

</details>


### [17] [Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components](https://arxiv.org/abs/2512.14426)
*Simon Steuernagel,Marcus Baum*

Main category: eess.SP

TL;DR: 提出一种确定性闭式椭圆扩展目标跟踪器，通过解耦运动学、方向和轴长，减少近似需求，性能优于现有算法，达到采样方法的精度


<details>
  <summary>Details</summary>
Motivation: 扩展目标跟踪需要同时估计目标的物理范围和运动参数，传统方法通常需要多个近似处理。现有算法在精度和计算效率方面存在局限，需要一种既能保持高精度又具有计算效率的解决方案

Method: 提出确定性闭式椭圆扩展目标跟踪器，将状态分量（运动学、方向、轴长）解耦处理，减少各分量估计器所需的近似。同时引入批处理变体以提高计算效率

Result: 算法性能优于现有算法，达到采样方法的精度水平。批处理变体在保持高精度的同时实现了高效计算，在仿真研究和真实汽车雷达数据评估中均表现出色

Conclusion: 通过解耦状态分量的方法，成功开发出既精确又高效的扩展目标跟踪器，在仿真和真实数据中验证了其优越性能，为扩展目标跟踪提供了有效的解决方案

Abstract: Extended object tracking involves estimating both the physical extent and kinematic parameters of a target object, where typically multiple measurements are observed per time step. In this article, we propose a deterministic closed-form elliptical extended object tracker, based on decoupling of the kinematics, orientation, and axis lengths. By disregarding potential correlations between these state components, fewer approximations are required for the individual estimators than for an overall joint solution. The resulting algorithm outperforms existing algorithms, reaching the accuracy of sampling-based procedures. Additionally, a batch-based variant is introduced, yielding highly efficient computation while outperforming all comparable state-of-the-art algorithms. This is validated both by a simulation study using common models from literature, as well as an extensive quantitative evaluation on real automotive radar data.

</details>


### [18] [Chirp Delay-Doppler Domain Modulation Based Joint Communication and Radar for Autonomous Vehicles](https://arxiv.org/abs/2512.14432)
*Zhuoran Li,Zhen Gao,Sheng Chen,Dusit Niyato,Zhaocheng Wan,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 提出一种感知导向的联合通信与毫米波雷达范式，通过DD-QAM调制和4D参数估计实现智能车辆协作


<details>
  <summary>Details</summary>
Motivation: 促进智能车辆之间的协作，解决传统系统在动态环境中通信与感知的协同问题

Method: 1. 基于啁啾波形的延迟-多普勒正交幅度调制(DD-QAM)；2. 扩展卡尔曼滤波器进行4D参数估计；3. 双补偿解调与跟踪方案

Result: 仿真结果验证了方法的可行性和优越性能，在自动驾驶领域取得显著进展

Conclusion: 提出的感知导向联合通信雷达范式有效提升了智能车辆的协作能力，为自动驾驶系统提供了创新解决方案

Abstract: This paper introduces a sensing-centric joint communication and millimeter-wave radar paradigm to facilitate collaboration among intelligent vehicles.
  We first propose a chirp waveform-based delay-Doppler quadrature amplitude modulation (DD-QAM) that modulates data across delay, Doppler, and amplitude dimensions.
  Building upon this modulation scheme, we derive its achievable rate to quantify the communication performance.
  We then introduce an extended Kalman filter-based scheme for four-dimensional (4D) parameter estimation in dynamic environments, enabling the active vehicles to accurately estimate orientation and tangential-velocity beyond traditional 4D radar systems.
  Furthermore, in terms of communication, we propose a dual-compensation-based demodulation and tracking scheme that allows the passive vehicles to effectively demodulate data without compromising their sensing functions.
  Simulation results underscore the feasibility and superior performance of our proposed methods, marking a significant advancement in the field of autonomous vehicles.
  Simulation codes are provided to reproduce the results in this paper: \href{https://github.com/LiZhuoRan0/2026-IEEE-TWC-ChirpDelayDopplerModulationISAC}{https://github.com/LiZhuoRan0}.

</details>


### [19] [Relaying Signal When Monitoring Traffic: Double Use of Aerial Vehicles Towards Intelligent Low-Altitude Networking](https://arxiv.org/abs/2512.14436)
*Jiahui Liang,Wenlihan Lu,Tianyi Liu,Kang Kang,Guixin Pan,Liuqing Yang,Xinhu Zheng,Shijian Gao*

Main category: eess.SP

TL;DR: 提出一种无人机双重用途策略，将监控和中继切换功能统一到单个高效流程中，降低通信中断概率同时保持监控性能


<details>
  <summary>Details</summary>
Motivation: 智能低空网络中，将监控任务集成到通信无人机中会消耗资源并增加通信链路切换延迟，需要解决这一挑战

Method: 基于集成感知与通信框架，通过主动切换网络协调多角色无人机，融合空地和地面车辆的多视角感知数据，开发轻量级车辆检测模块和两阶段训练程序

Result: 在200 Mbps要求下通信中断概率降低近10%，监控性能不受影响，即使缺少多个无人机仍保持高弹性（86%可达速率），优于传统地面切换方案

Conclusion: 提出的集成方法有效统一了监控和中继切换功能，提高了低空网络的效率和弹性，代码已开源

Abstract: In intelligent low-altitude networks, integrating monitoring tasks into communication unmanned aerial vehicles (UAVs) can consume resources and increase handoff latency for communication links. To address this challenge, we propose a strategy that enables a "double use" of UAVs, unifying the monitoring and relay handoff functions into a single, efficient process. Our scheme, guided by an integrated sensing and communication framework, coordinates these multi-role UAVs through a proactive handoff network that fuses multi-view sensory data from aerial and ground vehicles. A lightweight vehicle inspection module and a two-stage training procedure are developed to ensure monitoring accuracy and collaborative efficiency. Simulation results demonstrate the effectiveness of this integrated approach: it reduces communication outage probability by nearly 10% at a 200 Mbps requirement without compromising monitoring performance and maintains high resilience (86% achievable rate) even in the absence of multiple UAVs, outperforming traditional ground-based handoff schemes. Our code is available at the https://github.com/Jiahui-L/UAP.

</details>


### [20] [Hybrid Cognitive IoT with Cooperative Caching and SWIPT-EH: A Hierarchical Reinforcement Learning Framework](https://arxiv.org/abs/2512.14488)
*Nadia Abdolkhani,Walaa Hamouda*

Main category: eess.SP

TL;DR: 提出基于SAC算法的分层深度强化学习框架，用于优化具有SWIPT能量收集和协作缓存的混合认知物联网网络，联合优化能量收集、频谱接入、功率分配和缓存决策。


<details>
  <summary>Details</summary>
Motivation: 现有分层DRL方法主要关注频谱接入或功率控制，缺乏对能量收集、混合接入协调、功率分配和缓存的联合优化。认知物联网网络需要同时考虑能量、干扰和存储约束，以最大化吞吐量和缓存命中率，最小化传输延迟。

Method: 提出三层分层SAC（H-SAC）代理框架：高层策略调整时间切换因子，中层策略管理频谱接入协调和缓存共享，低层策略决定发射功率和缓存动作。将混合离散-连续动作空间分解为模块化子问题，提高可扩展性和收敛性。

Result: 仿真结果表明，所提出的分层SAC方法显著优于基准和贪婪策略，在平均总速率、延迟、缓存命中率和能量效率方面表现更好，即使在信道衰落和不确定条件下也能保持良好性能。

Conclusion: 提出的分层SAC框架能够有效解决认知物联网网络中能量收集、频谱接入、功率分配和缓存的联合优化问题，通过模块化分解提高了可扩展性和收敛性，为复杂网络优化提供了有效解决方案。

Abstract: This paper proposes a hierarchical deep reinforcement learning (DRL) framework based on the soft actor-critic (SAC) algorithm for hybrid underlay-overlay cognitive Internet of Things (CIoT) networks with simultaneous wireless information and power transfer (SWIPT)-energy harvesting (EH) and cooperative caching. Unlike prior hierarchical DRL approaches that focus primarily on spectrum access or power control, our work jointly optimizes EH, hybrid access coordination, power allocation, and caching in a unified framework. The joint optimization problem is formulated as a weighted-sum multi-objective task, designed to maximize throughput and cache hit ratio while simultaneously minimizing transmission delay. In the proposed model, CIoT agents jointly optimize EH and data transmission using a learnable time switching (TS) factor. They also coordinate spectrum access under hybrid overlay-underlay paradigms and make power control and cache placement decisions while considering energy, interference, and storage constraints. Specifically, in this work, cooperative caching is used to enable overlay access, while power control is used for underlay access. A novel three-level hierarchical SAC (H-SAC) agent decomposes the mixed discrete-continuous action space into modular subproblems, improving scalability and convergence over flat DRL methods. The high-level policy adjusts the TS factor, the mid-level policy manages spectrum access coordination and cache sharing, and the low-level policy decides transmit power and caching actions for both the CIoT agent and PU content. Simulation results show that the proposed hierarchical SAC approach significantly outperforms benchmark and greedy strategies. It achieves better performance in terms of average sum rate, delay, cache hit ratio, and energy efficiency, even under channel fading and uncertain conditions.

</details>


### [21] [Fusion of Cellular ISAC and Passive RF Sensing for UAV Detection and Tracking](https://arxiv.org/abs/2512.14608)
*Cole Dickerson,Sean Kearney,Sultan Manjur,Ismail Guvenc,Sevgi Gurbuz,Ali Gurbuz,Ozgur Ozdemir,Mihail Sichitiu*

Main category: eess.SP

TL;DR: 该论文提出了一种融合被动RF传感器和Ku波段雷达的多模态无人机检测与跟踪系统，通过卡尔曼滤波器整合异步2D RF和3D雷达观测，在真实世界实验中验证了系统在室外环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在民用和关键基础设施空域的快速增长，需要能够在多样化环境和传感条件下可靠工作的检测与跟踪系统。现有单一传感器系统在不同几何、距离和视线条件下存在局限性。

Method: 系统融合了Keysight N6841A RF被动传感器网络和Fortem TrueView R20 Ku波段雷达（16.3 GHz）的测量数据。采用卡尔曼滤波器，使用恒定速度运动模型，整合异步的2D RF观测和3D雷达观测。

Result: 真实世界实验在NSF AERPAW测试平台进行，结果表明雷达和RF传感在不同几何、距离和视线条件下具有互补优势。融合系统抑制了单传感器的大误差，提高了精度，增加了跟踪覆盖范围而不降低性能。

Conclusion: 多模态、面向ISAC（集成传感与通信）的传感方法在室外环境中对无人机鲁棒跟踪是有效的，展示了异构传感器融合在实际应用中的价值。

Abstract: The rapid growth of unmanned aerial vehicles (UAVs) in civilian and critical-infrastructure airspace has created a need for reliable detection and tracking systems that operate under diverse environmental and sensing conditions. This paper presents a UAV detection and tracking system that fuses measurements from a network of passive Keysight N6841A RF sensors and a Ku-band Fortem TrueView R20 radar operating in the FR3 spectrum (16.3 GHz) as an ISAC proxy. Real-world experiments at the NSF AERPAW testbed demonstrate that radar and RF sensing provide complementary strengths under varying geometric, range, and line-of-sight conditions. A Kalman filter using a constant-velocity motion model integrates the asynchronous 2D RF and 3D radar observations, suppressing large standalone errors, improving accuracy over individual modalities, and increasing tracking coverage without degrading performance. These results demonstrate the effectiveness of multi-modal, ISAC-oriented sensing for robust UAV tracking in outdoor environments.

</details>


### [22] [Tunable Gaussian Pulse for Delay-Doppler ISAC](https://arxiv.org/abs/2512.14637)
*Bruno Felipe Costa,Anup Mishra,Israel Leyva-Mayorga,Taufik Abrão,Petar Popovski*

Main category: eess.SP

TL;DR: 本文提出可调高斯脉冲(TGP)用于延迟-多普勒域ISAC系统，通过三个参数(γ,αc,βc)实现感知精度与通信容量的灵活权衡，在保持90%最大容量的同时达到接近RRC脉冲的感知精度。


<details>
  <summary>Details</summary>
Motivation: 传统DD域脉冲设计要么是通信中心化的，要么是静态的，无法适应非平稳信道和多样化应用需求。高移动性下OFDM系统存在严重载波间干扰，需要更鲁棒的DD域ISAC解决方案。

Method: 提出可调高斯脉冲(TGP)，这是一种DD域原生的、解析可调的脉冲形状，通过三个参数控制：纵横比γ、啁啾率αc和相位耦合βc。在感知侧推导CRLB闭式解，在通信侧分析参数对干扰结构的影响。

Result: TGP能够实现从Sinc脉冲的高容量到RRC脉冲的高精度之间的灵活权衡区域。特别地，TGP在保持超过90% Sinc最大容量的同时，达到了接近RRC脉冲的感知精度。

Conclusion: TGP提供了一种动态可调的DD域脉冲设计框架，能够在感知精度和通信容量之间实现平衡，解决了传统静态脉冲设计的局限性，为高移动性ISAC系统提供了更优的解决方案。

Abstract: Integrated sensing and communication (ISAC) for next-generation networks targets robust operation under high mobility and high Doppler spread, leading to severe inter-carrier interference (ICI) in systems based on orthogonal frequency-division multiplexing (OFDM) waveforms. Delay--Doppler (DD)-domain ISAC offers a more robust foundation under high mobility, but it requires a suitable DD-domain pulse-shaping filter. The prevailing DD pulse designs are either communication-centric or static, which limits adaptation to non-stationary channels and diverse application demands. To address this limitation, this paper introduces the tunable Gaussian pulse (TGP), a DD-native, analytically tunable pulse shape parameterized by its aspect ratio \( γ\), chirp rate \( α_c \), and phase coupling \( β_c \). On the sensing side, we derive closed-form Cramér--Rao lower bounds (CRLBs) that map \( (γ,α_c,β_c) \) to fundamental delay and Doppler precision. On the communications side, we show that \( α_c \) and \( β_c \) reshape off-diagonal covariance, and thus inter-symbol interference (ISI), without changing received power, isolating capacity effects to interference structure rather than power loss. A comprehensive trade-off analysis demonstrates that the TGP spans a flexible operational region from the high capacity of the Sinc pulse to the high precision of the root raised cosine (RRC) pulse. Notably, TGP attains near-RRC sensing precision while retaining over \( 90\% \) of Sinc's maximum capacity, achieving a balanced operating region that is not attainable by conventional static pulse designs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Physics-Guided Deep Learning for Heat Pump Stress Detection: A Comprehensive Analysis on When2Heat Dataset](https://arxiv.org/abs/2512.13696)
*Md Shahabub Alam,Md Asifuzzaman Jishan,Ayan Kumar Ghosh*

Main category: cs.LG

TL;DR: 本文提出了一种物理引导的深度神经网络方法，用于热泵应力分类，在When2Heat数据集上实现了78.1%的测试准确率，相比基线方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 热泵系统在现代节能建筑中至关重要，但由于复杂的热力学相互作用和有限的真实世界数据，其运行应力检测仍然具有挑战性。

Method: 采用物理引导的深度神经网络方法，包括物理引导的特征选择和类别定义，使用5个隐藏层的深度神经网络架构，并采用双重正则化策略。

Result: 模型在测试集上达到78.1%的准确率，验证集上达到78.5%的准确率，相比浅层网络提升5.0%，相比有限特征集提升4.0%，相比单一正则化策略提升2.0%。

Conclusion: 提出的系统为热泵应力检测提供了生产就绪的解决方案，通过全面的消融研究验证了物理引导特征选择、可变阈值和跨国能源模式分析的有效性。

Abstract: Heat pump systems are critical components in modern energy-efficient buildings, yet their operational stress detection remains challenging due to complex thermodynamic interactions and limited real-world data. This paper presents a novel Physics-Guided Deep Neural Network (PG-DNN) approach for heat pump stress classification using the When2Heat dataset, containing 131,483 samples with 656 features across 26 European countries. The methodology integrates physics-guided feature selection and class definition with a deep neural network architecture featuring 5 hidden layers and dual regularization strategies. The model achieves 78.1\% test accuracy and 78.5% validation accuracy, demonstrating significant improvements over baseline approaches: +5.0% over shallow networks, +4.0% over limited feature sets, and +2.0% over single regularization strategies. Comprehensive ablation studies validate the effectiveness of physics-guided feature selection, variable thresholding for realistic class distribution, and cross-country energy pattern analysis. The proposed system provides a production-ready solution for heat pump stress detection with 181,348 parameters and 720 seconds training time on AMD Ryzen 9 7950X with RTX 4080 hardware.

</details>


### [24] [Scaling and Transferability of Annealing Strategies in Large Language Model Training](https://arxiv.org/abs/2512.13705)
*Siqi Wang,Zhengyu Chen,Teng Xiao,Zheqi Lv,Jinluan Yang,Xunliang Cai,Jingang Wang,Xiaomeng Li*

Main category: cs.LG

TL;DR: 该研究探索了大语言模型训练中学习率退火策略的可迁移性，提出了改进的WSD调度器预测框架，证明小模型可作为大模型训练动态的可靠代理，无需大量超参数搜索。


<details>
  <summary>Details</summary>
Motivation: 学习率调度对大语言模型训练至关重要，但不同模型配置下的最优退火策略难以确定，需要理解退火动态的可迁移性以减少昂贵的超参数搜索。

Method: 研究学习率退火动态的可迁移性，改进Warmup-Steady-Decay调度器的预测框架，纳入训练步数、最大学习率和退火行为等参数，建立更高效的学习率调度优化方法。

Result: 验证了最优退火比率遵循一致模式，可在不同训练配置间迁移，小模型可作为大模型训练动态的可靠代理，在Dense和MoE模型上均得到验证。

Conclusion: 该工作为选择最优退火策略提供了实用指导，无需大量超参数搜索，证明了训练动态的可迁移性，有助于更高效地优化大语言模型训练。

Abstract: Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.

</details>


### [25] [gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation](https://arxiv.org/abs/2512.14658)
*Alban Puech,Matteo Mazzonelli,Celia Cintas,Tamara R. Govindasamy,Mangaliso Mngomezulu,Jonas Weiss,Matteo Baù,Anna Varbella,François Mirallès,Kibaek Kim,Le Xie,Hendrik F. Hamann,Etienne Vos,Thomas Brunschwiler*

Main category: cs.LG

TL;DR: gridfm-datakit-v1是一个用于生成真实多样电力潮流和最优潮流数据集的Python库，解决了现有工具在场景多样性、运行限制外泛化和成本函数固定等问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有电力系统数据集和库面临三个主要挑战：1) 缺乏真实的随机负荷和拓扑扰动，限制了场景多样性；2) PF数据集仅限于OPF可行点，阻碍了ML求解器对违反运行限制情况的泛化；3) OPF数据集使用固定的发电机成本函数，限制了跨不同成本的泛化能力。

Method: 通过结合真实世界负荷剖面的全局缩放与局部噪声，支持任意N-k拓扑扰动来创建多样且真实的数据集；生成超出运行限制的PF样本；产生具有变化发电机成本的OPF数据；并能高效扩展到大型电网（最多10,000个节点）。

Result: 开发了gridfm-datakit-v1库，解决了现有工具的局限性，提供了与OPFData、OPF-Learn、PGLearn和PFΔ的比较，并在GitHub上开源（Apache 2.0许可），可通过pip安装。

Conclusion: gridfm-datakit-v1为训练机器学习电力系统求解器提供了更全面、真实和多样的数据集生成工具，解决了现有数据集的三个主要限制，有助于提升ML求解器的泛化能力。

Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.

</details>


### [26] [Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training](https://arxiv.org/abs/2512.13706)
*John Graham Reynolds*

Main category: cs.LG

TL;DR: 研究发现在数学推理微调中，大语言模型会出现灾难性遗忘，但通过混合训练策略可以完全消除遗忘，同时保持数学性能


<details>
  <summary>Details</summary>
Motivation: 当微调大语言模型进行数学推理等专门任务时，模型会出现灾难性遗忘，失去先前学习的能力。本研究旨在探索如何解决这个问题

Method: 使用Flan-T5-Base模型，在DeepMind Mathematics数据集上进行微调，测量在MultiNLI上的遗忘情况。提出混合训练策略，在训练中交错使用数学和NLI示例，并系统探索从1:1到15:1的混合比例

Result: 纯数学训练将数学准确率从3.1%提升到12.0%，但导致NLI准确率从81.0%崩溃到16.5%（下降64.5个百分点）。混合训练完全消除了灾难性遗忘：1:1比例达到12.0%数学准确率（与纯数学训练相当），同时保持86.2%的NLI准确率。即使最小NLI暴露（6.2%）也能提供有效正则化

Conclusion: 专业化不需要以遗忘通用能力为代价，混合训练策略可以完全消除灾难性遗忘，同时保持专门任务的性能。这对于扩展到更大模型具有重要意义，混合训练可能带来超越遗忘预防的额外好处

Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.

</details>


### [27] [Variational Physics-Informed Ansatz for Reconstructing Hidden Interaction Networks from Steady States](https://arxiv.org/abs/2512.13708)
*Kaiming Luo*

Main category: cs.LG

TL;DR: VPIA方法从异构稳态数据中推断复杂系统的相互作用结构，无需时间轨迹或导数估计，通过变分物理信息表示和自然梯度优化恢复有向、加权和多体耦合。


<details>
  <summary>Details</summary>
Motivation: 现有重构方法难以处理非线性、异构和高阶耦合的系统，特别是在只能观测到稳态数据的情况下。需要一种能够直接从异构稳态数据推断一般相互作用算子的方法。

Method: 提出变分物理信息表示（VPIA），将动力学的稳态约束嵌入可微变分表示中，通过最小化物理导出的稳态残差来重构底层耦合，无需时间轨迹、导数估计或监督。结合残差采样和自然梯度优化实现大规模高阶网络的可扩展学习。

Result: 在多种非线性系统中，VPIA能够准确恢复有向、加权和多体结构，即使在存在显著噪声的情况下也能稳定工作，为仅能获得快照观测的复杂相互作用网络推断提供了统一鲁棒框架。

Conclusion: VPIA提供了一个物理约束的推断框架，能够从异构稳态数据中准确恢复复杂相互作用网络的结构，解决了现有方法在非线性、异构和高阶耦合系统重构中的局限性。

Abstract: The interaction structure of a complex dynamical system governs its collective behavior, yet existing reconstruction methods struggle with nonlinear, heterogeneous, and higher-order couplings, especially when only steady states are observable. We propose a Variational Physics-Informed Ansatz (VPIA) that infers general interaction operators directly from heterogeneous steady-state data. VPIA embeds the steady-state constraints of the dynamics into a differentiable variational representation and reconstructs the underlying couplings by minimizing a physics-derived steady-state residual, without requiring temporal trajectories, derivative estimation, or supervision. Residual sampling combined with natural-gradient optimization enables scalable learning of large and higher-order networks. Across diverse nonlinear systems, VPIA accurately recovers directed, weighted, and multi-body structures under substantial noise, providing a unified and robust framework for physics-constrained inference of complex interaction networks in settings where only snapshot observations are available.

</details>


### [28] [Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables](https://arxiv.org/abs/2512.13710)
*Edwin Oluoch Awino,Denis Machanda*

Main category: cs.LG

TL;DR: 该研究结合SAR影像与环境水文数据，使用机器学习模型评估肯尼亚Nyando河流域的洪水易发性，发现随机森林模型表现最佳，识别出维多利亚湖附近的Kano平原为最高风险区。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球最具破坏性的自然灾害之一，对生态系统、基础设施和人类生计构成严重威胁。本研究旨在开发一种结合SAR影像和环境水文数据的洪水易发性评估方法，特别是在数据有限的地区，为灾害风险减少、土地利用规划和预警系统开发提供支持。

Method: 研究结合Sentinel-1双极化SAR影像（2024年5月洪水事件）与环境水文数据，生成二元洪水清单作为训练数据。整合六个条件因子（坡度、高程、坡向、土地利用/土地覆盖、土壤类型、距河流距离），训练四种监督分类器：逻辑回归、分类回归树、支持向量机和随机森林。使用准确率、Cohen's Kappa和ROC分析评估模型性能。

Result: 随机森林模型表现最佳（准确率=0.762；Kappa=0.480），优于其他三种模型。基于RF的易发性地图显示，维多利亚湖附近的低洼Kano平原具有最高的洪水脆弱性，这与历史洪水记录和2024年5月洪水事件的影响一致。

Conclusion: 研究证明了在数据有限地区结合SAR数据和集成机器学习方法进行洪水易发性制图的价值。生成的地图为灾害风险减少、土地利用规划和预警系统开发提供了重要见解，特别是在类似Nyando河流域的易发洪水地区。

Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.

</details>


### [29] [Dropout Neural Network Training Viewed from a Percolation Perspective](https://arxiv.org/abs/2512.13853)
*Finley Devlin,Jaron Sanders*

Main category: cs.LG

TL;DR: 本文研究了深度神经网络中dropout训练时的渗流现象及其影响，发现dropout可能导致网络输入输出路径中断，从而影响训练效果。


<details>
  <summary>Details</summary>
Motivation: 研究dropout正则化方法中随机移除连接的过程与统计物理中渗流模型的相似性，探究当dropout移除足够多连接导致网络输入输出路径中断时对神经网络预测能力的影响。

Method: 提出新的渗流模型来模拟神经网络中的dropout，分析网络拓扑结构与路径问题之间的关系，从理论上证明dropout中存在渗流效应，并研究无偏置和有偏置神经网络中的训练崩溃问题。

Result: 理论分析表明dropout确实存在渗流效应，这种效应可能导致无偏置神经网络在dropout训练时出现崩溃，并且启发式地论证这种崩溃可能扩展到有偏置的神经网络。

Conclusion: dropout训练中的渗流效应是一个重要现象，当移除的连接足够多时可能导致网络输入输出路径中断，从而影响神经网络的训练效果和预测能力，这一发现对理解dropout机制和设计更有效的正则化方法具有重要意义。

Abstract: In this work, we investigate the existence and effect of percolation in training deep Neural Networks (NNs) with dropout. Dropout methods are regularisation techniques for training NNs, first introduced by G. Hinton et al. (2012). These methods temporarily remove connections in the NN, randomly at each stage of training, and update the remaining subnetwork with Stochastic Gradient Descent (SGD). The process of removing connections from a network at random is similar to percolation, a paradigm model of statistical physics.
  If dropout were to remove enough connections such that there is no path between the input and output of the NN, then the NN could not make predictions informed by the data. We study new percolation models that mimic dropout in NNs and characterise the relationship between network topology and this path problem. The theory shows the existence of a percolative effect in dropout. We also show that this percolative effect can cause a breakdown when training NNs without biases with dropout; and we argue heuristically that this breakdown extends to NNs with biases.

</details>


### [30] [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)
*Aadya Goel,Mayuri Sridhar*

Main category: cs.LG

TL;DR: 本文提出Hessian Reassignment方法，用于文档分类模型的类别级遗忘，通过两步法高效移除目标类别训练数据的影响，在保持准确性的同时大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在高效移除特定训练数据对模型的影响，而无需完全重新训练。虽然LLMs的遗忘研究已有进展，但文档分类模型的遗忘研究相对不足，特别是类别级遗忘。

Method: 提出Hessian Reassignment方法：1）单次影响式更新，通过共轭梯度求解Hessian-vector系统，减去目标类别所有训练点的贡献；2）通过Top-1分类强制执行决策空间保证，而非随机重新分类已删除类别样本。

Result: 在标准文本基准测试中，Hessian Reassignment在保持接近完整重新训练（不含目标类别）准确性的同时，运行速度提升了数个数量级。同时，在移除类别上持续降低了成员推断优势。

Conclusion: Hessian Reassignment为文档分类中的高效类别遗忘提供了一条实用且原则性的路径，实现了性能与效率的良好平衡。

Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.

</details>


### [31] [Understanding the Gain from Data Filtering in Multimodal Contrastive Learning](https://arxiv.org/abs/2512.14230)
*Divyansh Pareek,Sewoong Oh,Simon S. Du*

Main category: cs.LG

TL;DR: 论文通过理论分析证明，在多模态表示学习中，使用预训练模型进行数据筛选（教师筛选）相比不筛选能显著降低学习误差，特别是在数据匹配质量较低的情况下。


<details>
  <summary>Details</summary>
Motivation: 现代多模态表示学习依赖互联网规模的数据集，但大量原始网络数据质量较低，数据筛选成为训练流程中的关键步骤。虽然基于预训练模型的筛选方法在实践中取得了成功，但其理论原理尚不明确，需要从理论角度解释其有效性。

Method: 采用标准的双模态数据生成模型，在线性对比学习框架下进行理论分析。定义η∈(0,1]为n个配对样本中模态正确匹配的比例，分别推导了不使用筛选和使用教师筛选两种情况下学习误差的上界和下界。

Result: 理论结果表明：1) 不使用筛选时，误差上下界为1/(η√n)；2) 使用教师筛选时，在η较大时误差上界为1/√(ηn)，在η较小时误差上界为1/√n。这表明教师筛选能显著降低学习误差，特别是在数据匹配质量较差的情况下。

Conclusion: 该研究从理论上证明了基于预训练模型的数据筛选在多模态表示学习中的有效性，为实践中广泛使用的数据筛选方法提供了理论依据，特别是在处理低质量网络数据时具有重要价值。

Abstract: The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $η\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\frac{1}{η\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\frac{1}{\sqrt{ηn}}$ in the large $η$ regime, and by $\frac{1}{\sqrt{n}}$ in the small $η$ regime.

</details>


### [32] [Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data](https://arxiv.org/abs/2512.13712)
*Eric Guo*

Main category: cs.LG

TL;DR: 开发机器学习框架整合废水监测、气象和空气质量数据预测美国RSV住院率，发现废水RSV水平是最强预测因子，并揭示原住民和高海拔地区住院率更高。


<details>
  <summary>Details</summary>
Motivation: RSV是儿童住院的主要原因，其暴发受环境条件强烈影响。需要整合多源数据来预测RSV住院率，以便及时进行公共卫生干预和资源分配。

Method: 整合每周住院率、废水RSV水平、每日气象测量和空气污染物浓度数据。使用CART、随机森林和提升等分类模型，预测RSV住院率分为低风险、警报和流行三个等级。

Result: 废水RSV水平是最强预测因子，其次是温度、臭氧水平和比湿度等气象和空气质量变量。发现原住民和阿拉斯加原住民RSV住院率显著更高，高海拔地区住院率也持续较高。

Conclusion: 结合环境和社区监测数据对预测RSV暴发具有重要价值。开发了交互式R Shiny仪表板，使模型更易访问和实用，支持用户探索各州RSV风险水平、可视化关键预测因子影响并生成预测。

Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.

</details>


### [33] [Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean](https://arxiv.org/abs/2512.14686)
*Chuan He*

Main category: cs.LG

TL;DR: 该论文研究了在重尾噪声（尾指数α∈(0,2]）下随机一阶方法的梯度裁剪技术，通过分析偏差-方差权衡，为全范围尾指数建立了统一的复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 现有随机优化研究主要关注轻尾噪声或有限均值（α∈(1,2]）的重尾噪声，当α接近1时复杂度趋于无穷。实践中常出现更重的噪声（包括无限均值情况），需要研究α∈(0,2]全范围的复杂度保证。

Method: 通过分析梯度裁剪中的偏差-方差权衡，提出新颖的理论框架。该方法可控制噪声尾部的对称性度量，将经典轻尾噪声分析与重尾情况结合，为裁剪后的随机一阶方法建立复杂度保证。

Result: 当噪声尾部对称性受控时，裁剪的随机一阶方法在任意尾指数α∈(0,2]的重尾噪声下都能获得改进的复杂度保证，覆盖了从有限方差到无限均值的全范围噪声情况。

Conclusion: 梯度裁剪技术通过偏差-方差权衡分析，能够有效处理全范围重尾噪声，为随机一阶方法提供了统一的复杂度理论框架，数值实验验证了理论发现。

Abstract: Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $α$ of the noise. Nonetheless, existing complexity results often cover only the case $α\in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $α$ approaches $1$. This paper tackles the general case of noise with tail index $α\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $α\in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.

</details>


### [34] [Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints](https://arxiv.org/abs/2512.13717)
*Ekaterina Sysoykova,Bernhard Anzengruber-Tanase,Michael Haslgrubler,Philipp Seidl,Alois Ferscha*

Main category: cs.LG

TL;DR: 提出两阶段联邦少样本学习框架，用于个性化EEG癫痫检测，解决数据稀缺、分布不均和隐私限制问题。


<details>
  <summary>Details</summary>
Motivation: 临床实践中EEG数据稀缺、分布在不同机构且受隐私法规限制，难以构建集中式AI癫痫检测模型。

Method: 两阶段方法：第一阶段通过联邦学习在非IID模拟医院站点上微调BIOT模型；第二阶段使用仅5个标记EEG片段进行联邦少样本个性化适应。

Result: 联邦微调平衡准确率0.43（集中式0.52）；FFSL阶段客户特定模型平均平衡准确率0.77，Cohen's kappa 0.62，加权F1 0.73。

Conclusion: FFSL能够在现实数据可用性和隐私约束下支持有效的患者自适应癫痫检测。

Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.

</details>


### [35] [Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce](https://arxiv.org/abs/2512.13726)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.LG

TL;DR: 该论文提出将时间约束下的推荐列表优化建模为马尔可夫决策过程，使用强化学习在用户偏好和时间预算之间进行平衡，相比传统上下文赌博机方法在严格时间预算下表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统推荐任务忽略了用户有限的时间预算这一关键资源约束。在移动购物等场景中，用户需要花费时间评估推荐商品，高相关性的商品可能评估成本也高，无法在用户时间预算内完成评估，从而影响用户参与度。

Method: 1) 将时间约束下的推荐列表优化统一建模为具有预算感知效用的马尔可夫决策过程；2) 开发模拟框架研究重排序数据上的策略行为；3) 使用强化学习算法（包括在线策略和离线策略控制）同时学习用户偏好和时间预算模式。

Result: 实验使用阿里巴巴个性化重排序数据集，在电子商务场景中进行推荐列表优化。实证结果表明，在线策略和离线策略控制在严格时间预算下相比传统上下文赌博机方法能够提高性能。

Conclusion: 强化学习方法能够有效平衡项目相关性和评估成本，在用户时间预算约束下生成具有更高参与潜力的推荐，为解决资源受限的推荐问题提供了有前景的方向。

Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.

</details>


### [36] [RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing](https://arxiv.org/abs/2512.13727)
*Yuhan Tang,Kangxin Cui,Jung Ho Park,Yibo Zhao,Xuan Jiang,Haoze He,Dingyi Zhuang,Shenhao Wang,Jiangbo Yu,Haris Koutsopoulos,Jinhua Zhao*

Main category: cs.LG

TL;DR: 提出RAST-MoE框架，使用混合专家自注意力编码器解决网约车平台的延迟匹配问题，在真实Uber数据上显著降低匹配和接驾延迟。


<details>
  <summary>Details</summary>
Motivation: 网约车平台需要在高度不确定的供需条件下平衡乘客等待时间和系统效率。现有方法往往过度简化交通动态或使用浅层编码器，无法捕捉复杂的时空模式。

Method: 提出Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE)框架，将自适应延迟匹配形式化为机制感知MDP，配备自注意力混合专家编码器。使用物理信息拥堵代理实现高效模拟，自适应奖励方案防止病态策略。

Result: 在真实Uber轨迹数据（旧金山）上，仅用1200万参数，总奖励提升超过13%，平均匹配延迟降低10%，接驾延迟降低15%。在未见需求机制下表现稳健，训练稳定。

Conclusion: 混合专家增强的强化学习在处理复杂时空动态的大规模决策中具有巨大潜力，RAST-MoE框架在网约车延迟匹配问题上表现出色。

Abstract: Ride-hailing platforms face the challenge of balancing passenger waiting times with overall system efficiency under highly uncertain supply-demand conditions. Adaptive delayed matching creates a trade-off between matching and pickup delays by deciding whether to assign drivers immediately or batch requests. Since outcomes accumulate over long horizons with stochastic dynamics, reinforcement learning (RL) is a suitable framework. However, existing approaches often oversimplify traffic dynamics or use shallow encoders that miss complex spatiotemporal patterns.
  We introduce the Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE), which formalizes adaptive delayed matching as a regime-aware MDP equipped with a self-attention MoE encoder. Unlike monolithic networks, our experts specialize automatically, improving representation capacity while maintaining computational efficiency. A physics-informed congestion surrogate preserves realistic density-speed feedback, enabling millions of efficient rollouts, while an adaptive reward scheme guards against pathological strategies.
  With only 12M parameters, our framework outperforms strong baselines. On real-world Uber trajectory data (San Francisco), it improves total reward by over 13%, reducing average matching and pickup delays by 10% and 15% respectively. It demonstrates robustness across unseen demand regimes and stable training. These findings highlight the potential of MoE-enhanced RL for large-scale decision-making with complex spatiotemporal dynamics.

</details>


### [37] [CurvaDion: Curvature-Adaptive Distributed Orthonormalization](https://arxiv.org/abs/2512.13728)
*Bhavesh Kumar,Roger Jin,Jeffrey Quesnelle*

Main category: cs.LG

TL;DR: CurvaDion通过动量变化检测高曲率区域，仅在需要时进行梯度同步，在160M到1.3B参数模型中实现99%通信减少，同时保持收敛性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型分布式训练中，梯度同步成为关键瓶颈。现有方法如Dion虽然减少了每步通信，但无论优化地形如何都同步，而实际训练中不同区域对同步的需求差异很大：平坦区域梯度相似，频繁同步冗余；高曲率区域则需要协调防止发散。

Method: 提出CurvaDion方法，使用相对最大动量变化(RMMC)检测需要同步的高曲率区域。RMMC利用优化过程中已计算的动量动态作为方向曲率的计算可行代理，每层仅增加O(d)操作。该方法建立了RMMC与损失曲率之间的理论联系。

Result: 在160M到1.3B参数的模型中，CurvaDion实现了99%的通信减少，同时匹配基线收敛性能。

Conclusion: CurvaDion通过智能检测高曲率区域，仅在必要时进行梯度同步，显著减少了大规模语言模型分布式训练中的通信开销，同时保持了训练效果。

Abstract: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.

</details>


### [38] [Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution](https://arxiv.org/abs/2512.13729)
*Jacob Schnell,Aditya Makkar,Gunadi Gani,Aniket Srinivasan Ashok,Darren Lo,Mike Optis,Alexander Wong,Yuhao Chen*

Main category: cs.LG

TL;DR: 本文提出了一种用于风数据超分辨率的扩散模型WindDM，通过新颖的复合分类器自由引导（CCFG）方法处理多通道输入，在保持成本效益的同时实现了高精度重建。


<details>
  <summary>Details</summary>
Motivation: 高分辨率、高精度的风数据对于天气建模、涡轮机布局优化等应用至关重要，但获取成本高昂。传统方法难以同时实现成本效益和准确性，而现有深度学习方法在处理风数据（通常有10个以上输入通道）时面临挑战。

Method: 提出了复合分类器自由引导（CCFG）方法，这是对标准分类器自由引导（CFG）的泛化，能够更好地处理多个条件输入。将CCFG集成到预训练的扩散模型中，开发了WindDM模型用于工业级风动力学重建。

Result: CCFG在风超分辨率任务中比标准CFG产生更高保真度的输出。WindDM在深度学习模型中实现了最先进的重建质量，同时成本比传统方法低1000倍。

Conclusion: CCFG方法有效解决了风数据超分辨率中多条件输入的挑战，WindDM模型在成本效益和准确性之间取得了良好平衡，为工业级风数据重建提供了实用解决方案。

Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.

</details>


### [39] [PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion](https://arxiv.org/abs/2512.13732)
*Weijie Yang,Xun Zhang*

Main category: cs.LG

TL;DR: PIS是一个基于集合条件扩散的物理反演框架，能够在任意稀疏、不规则观测下稳定求解PDE约束的参数反演问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: PDE约束的物理参数反演在稀疏、不规则观测下本质上是病态的，现有深度学习和算子学习方法在极端稀疏条件下失效，缺乏鲁棒性和不确定性量化。

Method: 提出物理反演求解器(PIS)：基于集合条件扩散框架，使用Set Transformer编码器处理任意数量和几何的观测，采用余弦退火稀疏课程实现鲁棒性，并辅以信息论分析。

Result: 在Darcy流、Helmholtz波场反演和Hooke定律结构健康监测三个挑战性问题上，PIS在极端稀疏观测(0.29%观测率)下保持稳定准确，反演误差降低12.28%-88.73%，并能生成校准的后验样本。

Conclusion: PIS是一个强大、通用且对稀疏性具有独特韧性的物理反演解决方案，能够在任意和严重欠采样观测下可靠工作。

Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.

</details>


### [40] [Low-Rank Compression of Language Models via Differentiable Rank Selection](https://arxiv.org/abs/2512.13733)
*Sidhant Sundrani,Francesco Tudisco,Pasquale Minervini*

Main category: cs.LG

TL;DR: 提出LLRC方法，通过梯度学习掩码权重来选择奇异值，在无需微调的情况下优化大语言模型的低秩压缩


<details>
  <summary>Details</summary>
Motivation: 现有低秩压缩方法在层间秩选择上存在局限：启发式方法搜索空间有限，梯度方法性能不佳且需要后压缩微调。需要一种无需微调就能优化压缩率和下游任务准确性的方法

Method: LLRC（Learning to Low-Rank Compress）：基于梯度的方法，直接学习选择奇异值的掩码权重。使用校准数据集训练掩码权重，在减少奇异值的同时最小化中间激活与原始模型的差异

Result: 在Llama-2-13B模型20%压缩率下，LLRC在MMLU、BoolQ和OpenbookQA上分别比STRS方法提升12%、3.5%和4.4%。在无需微调的情况下，优于SVD-LLM和LLM-Pruner的变体，且与需要微调的LLM-Pruner性能相当

Conclusion: LLRC提供了一种有效的无需微调的低秩压缩方法，通过直接学习奇异值选择掩码，在保持性能的同时实现高效压缩，为大型语言模型压缩提供了新思路

Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.

</details>


### [41] [Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation](https://arxiv.org/abs/2512.13734)
*Haochen Yuan,Yang Zhang,Xiang He,Quan Z. Sheng,Zhongjie Wang*

Main category: cs.LG

TL;DR: 提出一个基于参数高效微调（PEFT）的联邦推荐框架，通过减少需要传输的嵌入参数来降低通信开销，同时提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着云边协同的发展，推荐服务在分布式环境中训练。联邦推荐通过共享模型参数而非原始数据来保护隐私，但海量项目嵌入参数导致通信效率低下。现有研究主要关注提升联邦推荐模型效率，却忽视了嵌入参数开销问题。

Method: 提出一个基于参数高效微调（PEFT）的联邦推荐训练框架，采用轻量级插件式设计，可无缝集成到现有联邦推荐方法中。除了整合常见的PEFT技术（如LoRA和基于哈希的编码），还探索使用残差量化变分自编码器（RQ-VAE）作为新的PEFT策略。

Result: 在多种联邦推荐模型骨干和数据集上的大量实验表明，该框架显著降低了通信开销，同时提高了推荐准确性。

Conclusion: 提出的基于PEFT的联邦推荐框架有效解决了嵌入参数传输的通信效率问题，为联邦推荐系统提供了一种高效、轻量级的解决方案。

Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.

</details>


### [42] [DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series](https://arxiv.org/abs/2512.13735)
*Xuechun Liu,Heli Sun,Xuecheng Wu,Ruichen Cao,Yunyun Shi,Dingkang Yang,Haoran Li*

Main category: cs.LG

TL;DR: DARTs是一个用于高维多变量时间序列异常检测的双路径框架，通过短路径和长路径分别捕捉短期和长期时空依赖，并结合窗口感知软融合机制来过滤噪声。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低维场景下能识别明显异常模式，但在处理高维噪声时间序列时，难以鲁棒地捕捉长距离时空依赖关系。

Method: 提出DARTs双路径框架：短路径包含多视图稀疏图学习器和扩散多关系图单元，捕捉高噪声下的短期时空模式；长路径使用多尺度时空图构造器建模长期动态；最后通过窗口感知时空软融合机制过滤噪声并整合异常模式。

Result: 在主流数据集上的大量定性和定量实验表明，DARTs具有优越性和鲁棒性。消融研究验证了各组件设计的关键作用。

Conclusion: DARTs能有效处理高维噪声时间序列中的异常检测问题，通过双路径设计和软融合机制实现了对复杂异常模式的准确识别和定位。

Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.

</details>


### [43] [TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection](https://arxiv.org/abs/2512.13736)
*Li-Xuan Zhao,Chen-Yang Xu,Wen-Qiang Li,Bo Wang,Rong-Xing Wei,Qing-Hao Menga*

Main category: cs.LG

TL;DR: 提出TF-MCL模型，通过时间-频率融合和多域交叉损失改进对比学习，用于基于EEG信号的抑郁症检测，在两个公开数据集上显著超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于EEG的抑郁症检测方法过度依赖标签，而传统对比学习方法未能充分表征EEG信号的时频分布特征，获取低语义数据表示的能力不足。

Method: 提出TF-MCL模型：1）使用融合映射头生成时频混合表示，将时频域信息重新映射到融合域；2）通过优化多域交叉损失函数，重构时频域和融合域中的表示分布。

Result: 在MODMA和PRED+CT公开数据集上评估，准确率分别比现有SOTA方法提升5.87%和9.96%。

Conclusion: TF-MCL模型通过时频融合和多域交叉损失有效增强了对比学习在EEG抑郁症检测中的表现，解决了传统方法对标签的过度依赖和时频特征表征不足的问题。

Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.

</details>


### [44] [The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models](https://arxiv.org/abs/2512.13741)
*Md. Hasib Ur Rahman*

Main category: cs.LG

TL;DR: 提出Laminar Flow Hypothesis，认为良性输入在LLM潜在空间产生平滑过渡，而对抗性提示引发语义湍流，可通过层间余弦速度方差检测，实现轻量级越狱检测和安全架构诊断。


<details>
  <summary>Details</summary>
Motivation: 当前LLM防御策略依赖计算昂贵的外部分类器或脆弱的词法过滤器，忽略了模型推理过程的内在动态。需要更有效、轻量的越狱检测方法。

Method: 引入Laminar Flow Hypothesis，提出零指标度量：层间余弦速度方差，用于量化语义湍流。通过实验评估不同小型语言模型在对抗攻击下的行为差异。

Result: RLHF对齐的Qwen2-1.5B在攻击下湍流增加75.4%，验证内部冲突假设；Gemma-2B湍流减少22.0%，显示低熵"反射式"拒绝机制。语义湍流可作为轻量实时越狱检测器。

Conclusion: 语义湍流不仅是轻量级实时越狱检测器，还是非侵入式诊断工具，可分类黑盒模型的安全架构，为LLM安全提供新视角。

Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.

</details>


### [45] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 本文比较了在数据稀缺环境下金融新闻情感分类的嵌入方法，发现预训练嵌入在数据量不足时效果有限，验证集过小会导致过拟合，建议采用少样本学习等替代方案。


<details>
  <summary>Details</summary>
Motivation: 金融情感分析对市场理解很重要，但标准NLP方法在小数据集上表现不佳。本研究旨在评估在资源受限环境下，基于嵌入的方法在金融新闻情感分类中的效果。

Method: 比较了Word2Vec、GloVe和句子转换器的嵌入表示，结合梯度提升算法，在手动标注的新闻标题数据集上进行评估。通过验证集和测试集性能对比分析模型表现。

Result: 实验结果显示验证集和测试集性能存在显著差距，模型表现甚至不如简单基线方法。预训练嵌入在数据量低于临界阈值时收益递减，小验证集导致模型选择过程中的过拟合。

Conclusion: 嵌入质量本身无法解决情感分类中的数据稀缺问题。对于资源有限的实践者，当标注样本稀缺时，应考虑少样本学习、数据增强或词典增强的混合方法等替代方案。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [46] [MIDUS: Memory-Infused Depth Up-Scaling](https://arxiv.org/abs/2512.13751)
*Taero Kim,Hoyoon Byun,Youngjun Choi,Sungrae Park,Kyungwoo Song*

Main category: cs.LG

TL;DR: MIDUS提出了一种新的深度扩展方法，用头级记忆层替代传统的FFN复制，通过为每个注意力头分配独立记忆库，在保持高效参数的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度扩展方法（DUS）通过复制层并持续预训练来增加模型容量，但依赖前馈网络（FFN）限制了效率和性能提升。需要一种更高效的深度扩展方法，能够在不显著增加参数和推理成本的情况下提升模型能力。

Method: MIDUS用头级记忆层（HML）替代FFN复制，为每个注意力头分配独立记忆库，支持头级检索并将信息注入后续层，同时保持头级功能结构。结合稀疏记忆访问和头级表示，并包含高效的头级值分解模块。

Result: 在持续预训练实验中，MIDUS相比强基线DUS展现出稳健的性能提升，同时保持高效的参数占用。该方法缓解了传统效率-性能权衡问题。

Conclusion: MIDUS通过头级记忆设计，成为传统FFN复制进行深度扩展的有力且资源高效的替代方案，为大规模语言模型扩展提供了新方向。

Abstract: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.

</details>


### [47] [Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention](https://arxiv.org/abs/2512.13758)
*Léo Hein,Giovanni de Nunzio,Giovanni Chierchia,Aurélie Pirayre,Laurent Najman*

Main category: cs.LG

TL;DR: 提出HDA-STGNN模型，利用速度数据和道路属性进行全网交通量估计，无需依赖传感器数据


<details>
  <summary>Details</summary>
Motivation: 现有交通量估计方法要么依赖传感器数据进行预测，要么需要邻近传感器数据进行空间插补，在传感器稀缺的城市中应用受限。而车辆速度数据和静态道路属性更广泛可用，能够覆盖大多数城市道路网络。

Method: 提出混合定向注意力时空图神经网络(HDA-STGNN)，这是一个归纳式深度学习框架，利用速度剖面、静态道路属性和道路网络拓扑结构来预测网络中所有路段的日交通量剖面。

Result: 通过广泛的消融研究证明了模型能够捕捉复杂的时空依赖性，并突出了拓扑信息对于在不依赖推理时交通量数据的情况下进行准确全网交通量估计的价值。

Conclusion: HDA-STGNN框架能够有效解决全网交通量估计问题，利用广泛可用的速度数据和道路属性，在传感器稀缺的城市中具有更好的适用性。

Abstract: Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.

</details>


### [48] [Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training](https://arxiv.org/abs/2512.13770)
*Huaiyuan Xiao,Fadi Dornaika,Jingjun Bi*

Main category: cs.LG

TL;DR: MV-SupGCN：一种半监督图卷积网络模型，通过联合损失函数、多图构建方法和对比学习与伪标签的统一框架，有效整合多视图互补信息，提升特征表示性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCN的多视图学习方法未能充分利用不同视图间的互补信息，导致特征表示次优和性能受限。需要一种能够更好整合多视图结构信息、提高模型泛化能力的解决方案。

Method: 1) 设计联合损失函数：结合交叉熵损失和监督对比损失，减少类内方差并增大类间可分性；2) 多图构建方法：在每个视图上结合KNN和半监督图构建，增强数据结构表示的鲁棒性；3) 统一框架：整合对比学习（强制多视图嵌入一致性）和伪标签（为损失函数提供额外监督），有效利用未标记数据。

Result: 在多个基准测试中，MV-SupGCN始终超越最先进方法，验证了所提集成方法的有效性。

Conclusion: MV-SupGCN通过整合互补组件（联合损失、多图构建、对比学习与伪标签框架），有效提升了多视图学习的性能，为复杂多视图数据建模提供了强大解决方案。

Abstract: The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN

</details>


### [49] [AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts](https://arxiv.org/abs/2512.14461)
*Niklas Grieger,Jannik Raskob,Siamak Mehrkanoon,Stephan Bialonski*

Main category: cs.LG

TL;DR: AnySleep是一个深度神经网络模型，能够使用任意EEG或EOG数据在可调时间分辨率下进行睡眠分期，在跨中心数据上达到最先进性能，并支持亚30秒时间尺度的分析。


<details>
  <summary>Details</summary>
Motivation: 传统睡眠分期需要手动评分，劳动密集且受限于30秒固定时间窗口、电极配置差异和中心间变异性，这阻碍了多中心睡眠研究的协调和新生物标志物的发现。

Method: 开发AnySleep深度神经网络模型，使用超过19,000个夜间记录（来自21个数据集，近200,000小时EEG/EOG数据）进行训练和验证，支持任意EEG/EOG通道配置和可调时间分辨率。

Result: 模型在30秒时间窗口达到或超越现有基线性能；多通道性能更佳，但单通道（仅EEG或仅EOG）仍表现良好；在亚30秒时间尺度能捕捉短暂觉醒，并改进对年龄、性别和睡眠呼吸暂停等特征的预测。

Conclusion: AnySleep模型公开可用，能够促进异质电极设置的大规模研究，加速睡眠中新生物标志物的发现，为睡眠研究和临床护理提供灵活、强大的自动化分期工具。

Abstract: Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.

</details>


### [50] [Constrained Policy Optimization via Sampling-Based Weight-Space Projection](https://arxiv.org/abs/2512.13788)
*Shengfan Cao,Francesco Borrelli*

Main category: cs.LG

TL;DR: SCPO是一种基于采样的权重空间投影方法，用于约束策略学习，通过轨迹采样和光滑性边界构建局部安全区域，使用凸SOCP投影梯度更新，确保从安全初始化开始的所有中间策略都保持安全。


<details>
  <summary>Details</summary>
Motivation: 安全关键学习需要在不离开安全操作区域的情况下改进性能。研究约束策略学习，其中模型参数必须满足未知的、基于rollout的安全约束。现有方法通常需要约束函数的梯度信息，这在实践中可能不可用。

Method: 提出SCPO方法：1）通过轨迹采样和光滑性边界构建局部安全区域；2）使用凸二阶锥规划（SOCP）将梯度更新投影到安全区域；3）不需要约束函数的梯度信息；4）在有稳定备份策略的约束控制设置中，确保闭环稳定性并实现超出保守备份的安全适应。

Result: 1）提供安全归纳保证：从任何安全初始化开始，所有中间策略在可行投影下保持安全；2）在有害监督回归和具有恶意专家的约束双积分器任务中，方法始终拒绝不安全更新，在整个训练过程中保持可行性，并实现有意义的原始目标改进。

Conclusion: SCPO提供了一种无需约束函数梯度的安全策略学习方法，通过权重空间投影确保训练过程中的安全性，在约束控制任务中表现出色，能够安全地适应超出保守备份策略的范围。

Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.

</details>


### [51] [Synthetic Electrogram Generation with Variational Autoencoders for ECGI](https://arxiv.org/abs/2512.14537)
*Miriam Gutiérrez Fernández,Karen López-Linares,Carlos Fambuena Santos,María S. Guillem,Andreu M. Climent,Óscar Barquero Pérez*

Main category: cs.LG

TL;DR: 使用变分自编码器生成合成心房电图以解决配对体表电位-心内电图数据稀缺问题，提升非侵入性电生理成像性能


<details>
  <summary>Details</summary>
Motivation: 心房颤动是最常见的心律失常，其临床评估需要准确表征心房电活动。虽然非侵入性心电图成像结合深度学习在从体表电位估计心内电图方面显示出潜力，但进展受到配对体表电位-心内电图数据集有限的阻碍。

Method: 研究使用变分自编码器生成合成多通道心房电图。提出了两种模型：窦性心律特定VAE和类别条件VAE（在窦性心律和房颤信号上训练）。通过形态学、频谱和分布相似性指标评估生成的心电图。

Result: 窦性心律特定VAE在模拟心电图上实现了更高的保真度，而类别条件VAE能够实现节律特定生成，但以降低窦性重建质量为代价。作为概念验证，生成的心电图用于下游非侵入性心电图重建任务的数据增强，适度增强提高了估计性能。

Conclusion: 这些结果证明了基于VAE的生成建模在缓解数据稀缺和增强基于深度学习的电生理成像流程方面的潜力。

Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.

</details>


### [52] [EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models](https://arxiv.org/abs/2512.13806)
*Siegfried Ludwig,Stylianos Bakas,Konstantinos Barmpas,Georgios Zoumpourlis,Dimitrios A. Adamos,Nikolaos Laskaris,Yannis Panagakis,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 提出D3方法，通过弱监督训练分离EEG信号中的潜在脑活动成分，防止隐藏过拟合，提高模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在EEG解码中声称达到最先进精度，但在实际应用中推广有限，表明存在隐藏的过拟合问题。需要一种方法能够分离脑活动的真实成分与伪迹，提高模型泛化能力。

Method: 提出解耦解码分解(D3)方法，通过预测输入窗口在试验序列中的位置来分离EEG信号的潜在成分。采用具有完全独立子网络的新模型架构，实现严格可解释性。建立特征解释范式对比不同数据集上的成分激活模式。

Result: D3方法能可靠分离运动想象数据中的脑活动潜在成分。在下游分类器中使用适当成分子集可防止任务相关伪迹引起的隐藏过拟合。线性可分离的潜在空间支持有效的少样本学习，在睡眠阶段分类中表现良好。

Conclusion: D3方法能够区分脑活动的真实成分与虚假特征，避免隐藏过拟合问题，仅需少量标记数据即可良好泛化到实际应用。为神经科学研究提供了分离个体脑过程、发现未知动态的工具。

Abstract: Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.

</details>


### [53] [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出CTVP框架，通过语义轨道分析验证不可信代码生成模型，检测后门注入，无需直接执行潜在恶意代码


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地生成代码且人类监督减少，后门注入和恶意行为成为关键问题，需要可靠的AI控制框架

Method: Cross-Trace Verification Protocol (CTVP)：通过分析模型对语义等价程序变换的执行轨迹预测的一致性模式，检测行为异常；引入Adversarial Robustness Quotient (ARQ)量化验证计算成本

Result: 理论分析建立信息论边界，证明不可博弈性——由于基本空间复杂度约束，对手无法通过训练改进；验证成本随轨道大小呈指数增长

Conclusion: 语义轨道分析为代码生成任务提供了可扩展、理论基础的AI控制方法

Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.

</details>


### [54] [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)
*Shicheng Liu,Siyuan Xu,Wenjie Qiu,Hangfan Zhang,Minghui Zhu*

Main category: cs.LG

TL;DR: 提出一种通过修正原因来改进RLHF的方法：首先识别导致不满意响应的训练数据，然后通过反学习这些数据来改进模型表现。


<details>
  <summary>Details</summary>
Motivation: 人类在改进不满意结果时通常会寻找并修正其原因，这种策略能否应用于改进语言模型的对齐训练？RLHF调优后的模型仍可能产生不满意响应，需要一种方法来改进这些响应。

Method: 方法分为两部分：1）事后解释方法，通过约束组合优化问题识别导致不满意响应的训练数据；2）反学习方法，通过反学习这些训练数据来改进不满意响应，同时不显著影响其他满意响应。

Result: 实验结果表明，该方法能够有效改进RLHF的表现。

Conclusion: 将人类寻找并修正原因的改进策略应用于RLHF是可行的，提出的方法能够识别并修正导致不满意响应的训练数据，从而改进语言模型的对齐效果。

Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.

</details>


### [55] [Topologically-Stabilized Graph Neural Networks: Empirical Robustness Across Domains](https://arxiv.org/abs/2512.13852)
*Jelena Losic*

Main category: cs.LG

TL;DR: 提出结合持久同调特征与稳定性正则化的图神经网络框架，增强对结构扰动的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 图神经网络已成为图表示学习的标准方法，但对结构扰动仍然脆弱，需要提高鲁棒性

Method: 基于持久同调稳定性定理，将GIN架构与从持久性图像提取的多尺度拓扑特征相结合，并通过Hiraoka-Kusano启发的稳定性约束进行正则化

Result: 在六个涵盖生化、社交和协作网络的数据集上，该方法在边扰动下表现出卓越的鲁棒性，同时保持有竞争力的准确率，扰动下性能下降最小（大多数数据集0-4%）

Conclusion: 这项工作提供了一个理论上有基础、经验上验证的鲁棒图学习方法，与拓扑正则化的最新进展保持一致

Abstract: Graph Neural Networks (GNNs) have become the standard for graph representation learning but remain vulnerable to structural perturbations. We propose a novel framework that integrates persistent homology features with stability regularization to enhance robustness. Building on the stability theorems of persistent homology \cite{cohen2007stability}, our method combines GIN architectures with multi-scale topological features extracted from persistence images, enforced by Hiraoka-Kusano-inspired stability constraints. Across six diverse datasets spanning biochemical, social, and collaboration networks , our approach demonstrates exceptional robustness to edge perturbations while maintaining competitive accuracy. Notably, we observe minimal performance degradation (0-4\% on most datasets) under perturbation, significantly outperforming baseline stability. Our work provides both a theoretically-grounded and empirically-validated approach to robust graph learning that aligns with recent advances in topological regularization

</details>


### [56] [Measuring Uncertainty Calibration](https://arxiv.org/abs/2512.13872)
*Kamil Ciosek,Nicolò Felicioni,Sina Ghiassian,Juan Elenter Litwin,Francesco Tonolini,David Gustaffson,Eva Garcia Martin,Carmen Barcena Gonzales,Raphaëlle Bertrand-Lalo*

Main category: cs.LG

TL;DR: 提出两种估计二元分类器L1校准误差的方法：1) 对有界变差校准函数的分类器提供上界；2) 提出修改分类器的方法，使其校准误差可高效上界估计且不影响性能


<details>
  <summary>Details</summary>
Motivation: 从有限数据集估计二元分类器L1校准误差的问题需要非渐近、分布无关的实用方法，现有方法存在限制性假设或计算效率问题

Method: 1) 对有界变差校准函数的分类器提供理论上的上界；2) 提出修改分类器的方法，使其校准误差可高效上界估计，无需限制性假设且保持分类性能

Result: 所有结果都是非渐近且分布无关的，提出的方法可在实际数据集上运行，计算开销适中，为实践中的校准误差测量提供实用建议

Conclusion: 提供了两种实用的校准误差估计方法，可在实际应用中高效实施，为二元分类器的校准评估提供了实用的理论框架和操作指南

Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.

</details>


### [57] [Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880)
*Geofrey Owino,Bernard Shibwabo*

Main category: cs.LG

TL;DR: 提出一个结合去噪自编码器、卷积分词器和Transformer编码器的婴儿哭声分类系统，采用联邦学习进行隐私保护训练，实现噪声鲁棒、通信高效的边缘部署。


<details>
  <summary>Details</summary>
Motivation: 婴儿哭声分类有助于早期评估婴儿需求，但现有方案面临隐私问题（音频数据）、背景噪声敏感性和跨记录环境的领域偏移等部署限制。

Method: 端到端婴儿哭声分析管道，集成去噪自编码器(DAE)、卷积分词器和Transformer编码器，采用通信高效的联邦学习训练。系统执行设备端去噪、自适应分割、事后校准和基于能量的分布外(OOD)弃权。联邦训练采用带8位适配器delta的正则化控制变量更新和安全聚合。

Result: 在Baby Chillanto和Donate-a-Cry数据集（带ESC-50噪声叠加）上，模型达到宏F1分数0.938、AUC 0.962、预期校准误差(ECE) 0.032，每轮客户端上传从约36-42 MB减少到3.3 MB。在NVIDIA Jetson Nano上的实时边缘推理达到每1秒频谱图帧96毫秒。

Conclusion: 该研究展示了实现隐私保护、噪声鲁棒和通信高效的婴儿哭声分类的实用路径，适合联邦部署。

Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.

</details>


### [58] [OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction](https://arxiv.org/abs/2512.13886)
*Mohammad Mozaffari,Samuel Kushnir,Maryam Mehri Dehnavi,Amir Yazdanbakhsh*

Main category: cs.LG

TL;DR: OPTIMA是一种实用的单次训练后剪枝方法，通过将层权重重构转化为独立的行式二次规划问题，在保持准确性的同时实现大规模剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有训练后剪枝方法面临权衡：简单启发式方法速度快但准确率下降，而联合优化方法准确率高但计算不可行。需要一种平衡准确性和可扩展性的实用方法。

Method: 将掩码选择后的层权重重构转化为独立的行式二次规划问题，这些QP共享层Hessian矩阵。实现加速器友好的QP求解器，每层累积一个Hessian并并行求解多个小QP。

Result: OPTIMA在多个LLM家族和稀疏度下一致提升零样本性能，最高带来3.97%的绝对准确率提升。在NVIDIA H100上，40小时内完成80亿参数transformer的端到端剪枝，峰值内存60GB。

Conclusion: OPTIMA为单次训练后剪枝设定了新的准确率-效率权衡标准，实现了实用的大规模模型剪枝而不需要微调。

Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.

</details>


### [59] [Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs](https://arxiv.org/abs/2512.13898)
*Rachit Bansal,Aston Zhang,Rishabh Tiwari,Lovish Madaan,Sai Surya Duvvuri,Devvrit Khatri,David Brandfonbrener,David Alvarez-Melis,Prajjwal Bhargava,Mihir Sanjay Kale,Samy Jelassi*

Main category: cs.LG

TL;DR: 论文指出当前长上下文LLM存在"分数稀释"问题，静态自注意力机制限制了推理时计算策略的效果，提出通过针对特定上下文的梯度更新来替代生成更多思考标记，显著提升长上下文任务性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM已能处理数百万token的长上下文，但经验表明它们无法可靠利用这么长的文本。同时，推理时计算（如生成思考标记）虽然能提升多步推理任务性能，但在长上下文任务中效果迅速衰减。需要解决长上下文下推理策略失效的问题。

Method: 提出一种简单方法：在给定上下文上进行有针对性的梯度更新，而非生成更多思考标记。这种方法理论上能克服静态自注意力的限制，通过少量上下文特定训练来更有效地利用推理计算资源。

Result: 该方法在不同模型和长上下文基准测试中带来一致的大幅性能提升。在Qwen3-4B模型上，LongBench-v2和ZeroScrolls基准测试子集的平均性能分别提升12.6和14.1个百分点。

Conclusion: 对于长上下文任务，少量上下文特定训练比当前推理时扩展策略（如生成更多思考标记）是更好的推理计算利用方式。这一发现具有实际应用价值。

Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.

</details>


### [60] [Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America](https://arxiv.org/abs/2512.13910)
*Matheus Corrêa Domingos,Valdivino Alexandre de Santiago Júnior,Juliana Aparecida Anochi,Elcio Hideiti Shiguemori,Luísa Mirelle Costa dos Santos,Hércules Carlos dos Santos Pereira,André Estevam Costa Oliveira*

Main category: cs.LG

TL;DR: 该研究比较了传统机器学习、深度学习与动态模型在南美洲降水预报中的表现，发现LSTM模型在强降水预测上最准确，而传统动态模型BAM表现最差，证实了深度学习在气候预报中的可行性。


<details>
  <summary>Details</summary>
Motivation: 降水预报对社会至关重要，但传统动态模型复杂且计算成本高。虽然AI技术已被用于气象预报，但缺乏对纯数据驱动方法在降水预报中可行性的广泛研究。本研究旨在填补这一空白，系统评估不同机器学习方法在南美洲降水预报中的表现。

Method: 研究比较了传统机器学习方法（随机森林、XGBoost）和深度学习方法（1D CNN、LSTM、GRU），并以巴西全球大气模型（BAM）作为传统动态建模方法的代表。所有模型用于预测南美洲2019年全年的降水情况，并使用可解释AI技术分析模型行为。

Result: LSTM模型表现出最强的预测性能，特别是在强降水预测上最准确。传统动态模型BAM表现最差。XGBoost在计算成本方面表现较好，虽然精度略有损失但延迟更低。深度学习模型整体优于传统方法。

Conclusion: 研究证实了深度学习模型在气候预报中的可行性，强化了全球主要气象和气候预报中心采用AI技术的趋势。LSTM在精度方面表现最佳，而XGBoost在成本敏感场景中提供了良好的平衡。

Abstract: Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.

</details>


### [61] [Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations](https://arxiv.org/abs/2512.13913)
*Patrick Egenlauf,Iva Březinová,Sabine Andergassen,Miriam Klopotek*

Main category: cs.LG

TL;DR: 论文研究量子多体系统动力学中二粒子约化密度矩阵（2RDM）方法的局限性，发现神经网络ODE仅在二粒子与三粒子关联强相关时能准确模拟，否则需要记忆依赖的闭合方案。


<details>
  <summary>Details</summary>
Motivation: 研究非平衡量子多体系统中关联快速建立的动力学描述问题。传统波函数方法计算复杂度指数增长，平均场方法忽略关键关联，而TD2RDM方法通过闭合BBGKY层级需要三粒子累积量重构，但忽略记忆效应的时域重构泛函的有效性在不同动力学区域尚不明确。

Method: 使用神经网络ODE模型在精确2RDM数据上训练（无降维），尝试仅从二粒子累积量信息重现动力学。分析二粒子与三粒子累积量之间的皮尔逊相关性，评估模型在不同参数区域的表现。

Result: 神经网络ODE仅在二粒子与三粒子累积量强相关区域能准确模拟动力学；在反相关或不相关区域失败，表明简单的时域二粒子累积量泛函无法捕捉演化。三粒子关联建立的时间平均幅度是成功的主要预测指标：中等关联建立时，神经网络ODE和现有TD2RDM重构都准确；强关联时系统失效。

Conclusion: 强关联区域需要三粒子累积量重构中的记忆依赖核。神经网络ODE可作为模型无关的诊断工具，映射累积量展开方法的适用区域，指导非局部闭合方案开发。从有限数据学习高维RDM动力学为快速数据驱动的关联量子物质模拟开辟新途径。

Abstract: Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.

</details>


### [62] [Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics](https://arxiv.org/abs/2512.13919)
*Eugenio Varetti,Matteo Torzoni,Marco Tezzele,Andrea Manzoni*

Main category: cs.LG

TL;DR: 本文提出一种自适应数字孪生框架，通过贝叶斯更新在线学习状态转移概率，增强土木工程中数字孪生的价值实现


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生在土木工程中的应用缺乏适应性，无法有效处理物理与虚拟域之间的动态交互，限制了其价值实现

Method: 使用概率图模型表示数字孪生，通过动态贝叶斯网络建模物理与虚拟域的双向交互，将状态转移概率视为具有共轭先验的随机变量，实现分层在线学习，并通过强化学习求解参数化马尔可夫决策过程

Result: 提出的自适应数字孪生框架具有增强的个性化、更高的鲁棒性和改进的成本效益，在铁路桥梁结构健康监测和维护规划的案例研究中得到验证

Conclusion: 自适应方法能够显著提升数字孪生在土木工程中的价值实现，通过在线学习状态转移动态和精确策略更新，为基础设施监测和维护提供更有效的解决方案

Abstract: This work shows how adaptivity can enhance value realization of digital twins in civil engineering. We focus on adapting the state transition models within digital twins represented through probabilistic graphical models. The bi-directional interaction between the physical and virtual domains is modeled using dynamic Bayesian networks. By treating state transition probabilities as random variables endowed with conjugate priors, we enable hierarchical online learning of transition dynamics from a state to another through effortless Bayesian updates. We provide the mathematical framework to account for a larger class of distributions with respect to the current literature. To compute dynamic policies with precision updates we solve parametric Markov decision processes through reinforcement learning. The proposed adaptive digital twin framework enjoys enhanced personalization, increased robustness, and improved cost-effectiveness. We assess our approach on a case study involving structural health monitoring and maintenance planning of a railway bridge.

</details>


### [63] [Sliding Window Recurrences for Sequence Models](https://arxiv.org/abs/2512.13921)
*Dragos Secrieru,Garyk Brixi,Yoshua Bengio,Taiji Suzuki,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 提出Phalanx层作为窗口注意力或线性循环的替代方案，在10-40亿参数模型中实现10-40%速度提升，同时保持困惑度不变


<details>
  <summary>Details</summary>
Motivation: 多混合架构在语言建模中具有更好的质量和性能，但需要与GPU内存层次结构对齐的算法来优化性能

Method: 引入滑动窗口循环（SWR）的分层分解框架，将循环截断为硬件对齐的窗口，减少跨warp通信，开发Phalanx层作为窗口注意力或线性循环的替代方案

Result: 在10-40亿参数的多混合模型中，Phalanx在4K到32K上下文长度上比优化的Transformer快10-40%，同时保持相同的困惑度

Conclusion: Phalanx层作为多混合架构的有效组件，在保持模型质量的同时显著提升了推理速度

Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.

</details>


### [64] [A Complete Guide to Spherical Equivariant Graph Transformers](https://arxiv.org/abs/2512.13927)
*Sophia Tang*

Main category: cs.LG

TL;DR: 本文是关于球面等变图神经网络（EGNNs）的完整指南，介绍了如何构建在三维分子系统中保持旋转对称性的神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 在三维分子和生物分子系统中，预测必须尊重物理固有的旋转对称性。传统神经网络无法保证在输入旋转时预测以物理上有意义的方式变化，因此需要开发能够保持SO(3)旋转群对称性的等变模型。

Method: 基于群表示论和球谐函数，使用球张量表示节点和边特征，通过张量积、Clebsch-Gordan分解构建SO(3)-等变核，并在此基础上构建Tensor Field Network和SE(3)-Transformer架构，实现等变消息传递和注意力机制。

Result: 开发了完整的球面等变建模理论框架，提供了从数学推导到代码实现的全面指南，使研究人员能够理解和实现球面EGNNs用于化学、分子性质预测、蛋白质结构建模和生成建模等应用。

Conclusion: 球面等变图神经网络为三维分子系统提供了保持物理对称性的原则性框架，通过系统化的数学基础和架构设计，为相关领域的研究和应用提供了重要工具。

Abstract: Spherical equivariant graph neural networks (EGNNs) provide a principled framework for learning on three-dimensional molecular and biomolecular systems, where predictions must respect the rotational symmetries inherent in physics. These models extend traditional message-passing GNNs and Transformers by representing node and edge features as spherical tensors that transform under irreducible representations of the rotation group SO(3), ensuring that predictions change in physically meaningful ways under rotations of the input. This guide develops a complete, intuitive foundation for spherical equivariant modeling - from group representations and spherical harmonics, to tensor products, Clebsch-Gordan decomposition, and the construction of SO(3)-equivariant kernels. Building on this foundation, we construct the Tensor Field Network and SE(3)-Transformer architectures and explain how they perform equivariant message-passing and attention on geometric graphs. Through clear mathematical derivations and annotated code excerpts, this guide serves as a self-contained introduction for researchers and learners seeking to understand or implement spherical EGNNs for applications in chemistry, molecular property prediction, protein structure modeling, and generative modeling.

</details>


### [65] [Informing Acquisition Functions via Foundation Models for Molecular Discovery](https://arxiv.org/abs/2512.13935)
*Qi Chen,Fabio Ramos,Alán Aspuru-Guzik,Florian Shkurti*

Main category: cs.LG

TL;DR: 提出一种免似然贝叶斯优化方法，直接利用LLM和化学基础模型的先验知识指导分子发现，通过树状空间划分和MCTS提高搜索效率，结合粗粒度聚类提升大规模候选集的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在分子发现中面临低数据量、先验知识不足和候选空间巨大的挑战，而现有LLM和化学基础模型虽然提供丰富先验，但高维特征、上下文学习成本高以及深度贝叶斯代理模型计算负担阻碍了其充分利用。

Method: 提出免似然贝叶斯优化方法：1) 绕过显式代理建模，直接利用LLM和化学基础模型的先验指导采集函数；2) 学习分子搜索空间的树状划分，结合局部采集函数，通过蒙特卡洛树搜索高效选择候选分子；3) 引入粗粒度LLM聚类，将采集函数评估限制在统计上具有更高属性值的簇中，提升大规模候选集的可扩展性。

Result: 通过大量实验和消融研究，证明所提方法在LLM引导的分子发现贝叶斯优化中显著提高了可扩展性、鲁棒性和样本效率。

Conclusion: 该方法成功解决了传统贝叶斯优化在分子发现中的局限性，通过直接利用LLM先验、树状空间划分和聚类策略，实现了更高效、可扩展的分子优化过程。

Abstract: Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.

</details>


### [66] [Pattern-Guided Diffusion Models](https://arxiv.org/abs/2512.13945)
*Vivian Lin,Kuk Jin Jang,Wenwen Si,Insup Lee*

Main category: cs.LG

TL;DR: PGDM利用时间数据中的模式进行预测，通过原型分析提取模式并指导扩散模型，在视觉场测量和动作捕捉预测中性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在多元时间序列预测中很少考虑数据中的重复结构或模式，导致预测可能不符合已知模式分布。

Method: PGDM首先使用原型分析提取时间数据中的模式，估计最可能的下一个模式，然后用这个模式估计指导扩散模型的预测。还引入了基于原型分析的不确定性量化技术，并根据模式估计的不确定性动态调整指导强度。

Result: 在两个应用场景（视觉场测量和动作捕捉帧预测）中，模式指导使PGDM性能分别提升高达40.67%/56.26%和14.12%/14.10%，并且PGDM比基线方法分别提升高达65.58%/84.83%和93.64%/92.55%。

Conclusion: PGDM通过利用时间数据中的固有模式进行指导，能够做出更符合已知模式的现实预测，在时间序列预测任务中表现出显著优势。

Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.

</details>


### [67] [A Single Architecture for Representing Invariance Under Any Space Group](https://arxiv.org/abs/2512.13989)
*Cindy Y. Zhang,Elif Ertekin,Peter Orbanz,Ryan P. Adams*

Main category: cs.LG

TL;DR: 提出一种能自动适应任意空间群对称性的单一机器学习架构，通过构建对称适应的傅里叶基来实现权重共享和零样本学习


<details>
  <summary>Details</summary>
Motivation: 传统方法需要为每个对称群设计专门的架构，限制了可扩展性和知识迁移。对于材料科学中的230个三维空间群，这一问题尤为突出，需要更通用的解决方案。

Method: 通过显式表征群操作对傅里叶系数的约束，构建对称适应的傅里叶基，并将这些约束编码到神经网络层中，实现跨不同空间群的权重共享。

Result: 该方法在材料属性预测任务中表现出竞争力，并能进行零样本学习以泛化到未见过的空间群。

Conclusion: 提出的单一架构能够自动适应任意空间群对称性，克服了传统方法需要为每个对称群设计专门架构的限制，提高了可扩展性和知识迁移能力。

Abstract: Incorporating known symmetries in data into machine learning models has consistently improved predictive accuracy, robustness, and generalization. However, achieving exact invariance to specific symmetries typically requires designing bespoke architectures for each group of symmetries, limiting scalability and preventing knowledge transfer across related symmetries. In the case of the space groups, symmetries critical to modeling crystalline solids in materials science and condensed matter physics, this challenge is particularly salient as there are 230 such groups in three dimensions. In this work we present a new approach to such crystallographic symmetries by developing a single machine learning architecture that is capable of adapting its weights automatically to enforce invariance to any input space group. Our approach is based on constructing symmetry-adapted Fourier bases through an explicit characterization of constraints that group operations impose on Fourier coefficients. Encoding these constraints into a neural network layer enables weight sharing across different space groups, allowing the model to leverage structural similarities between groups and overcome data sparsity when limited measurements are available for specific groups. We demonstrate the effectiveness of this approach in achieving competitive performance on material property prediction tasks and performing zero-shot learning to generalize to unseen groups.

</details>


### [68] [Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation](https://arxiv.org/abs/2512.14011)
*Yue Wan,Jiayi Yuan,Zhiwei Feng,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该论文构建了一个精心整理的MHC-II抗原表位数据集，提出了三个机器学习任务，并建立了多尺度评估框架来推进计算免疫治疗研究。


<details>
  <summary>Details</summary>
Motivation: MHC-II抗原表位在免疫治疗中至关重要，但与MHC-I相比，其研究面临更多挑战：结合特异性复杂、基序模式模糊、数据集更小且标准化程度低。需要更好的数据集和建模方法来推进计算免疫治疗。

Method: 1. 从IEDB和其他公共来源构建精心整理的MHC-II数据集，包括肽段-MHC-II数据集和新型抗原-MHC-II数据集；2. 提出三个机器学习任务：肽段结合、肽段呈递和抗原呈递；3. 建立多尺度评估框架来基准测试现有模型；4. 使用模块化框架对各种建模设计进行全面分析。

Result: 创建了一个扩展且标准化的MHC-II数据集，包含更丰富的生物学背景。通过多尺度评估框架对现有模型进行了基准测试，并分析了各种建模设计，为MHC-II抗原呈递途径的机器学习研究提供了系统框架。

Conclusion: 这项工作为推进计算免疫治疗提供了宝贵资源，为未来ML指导的表位发现和免疫反应预测建模研究奠定了基础，有助于解决MHC-II研究的现有挑战。

Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.

</details>


### [69] [EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment](https://arxiv.org/abs/2512.14019)
*Juseung Yun,Sunwoo Yu,Sumin Ha,Jonghyun Kim,Janghyeon Lee,Jongseong Jang,Soonyoung Lee*

Main category: cs.LG

TL;DR: EXAONE Path 2.5是一个病理学基础模型，通过联合建模组织学、基因组学、表观遗传学和转录组学等多模态数据，生成更全面的肿瘤生物学患者表征，在病理学基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 癌症进展涉及多个生物层面的相互作用，特别是超越形态学层面的分子层面信息，这些信息对仅基于图像的模型是不可见的。为了捕捉更广泛的生物学景观，需要开发能够整合多模态数据的模型。

Method: 模型包含三个关键组件：1) 支持异质模态间全对比学习的多模态SigLIP损失；2) 保留WSI空间结构和组织片段拓扑的片段感知旋转位置编码(F-RoPE)；3) 为WSI和RNA-seq提供生物学基础嵌入的领域专业化内部基础模型。

Result: 在内部真实世界临床数据集和包含80个任务的Patho-Bench基准测试中，EXAONE Path 2.5与六个领先的病理学基础模型相比，表现出高数据和参数效率，在Patho-Bench上达到最先进水平，在内部临床环境中表现出最高的适应性。

Conclusion: 研究结果强调了生物学信息多模态设计的价值，并突显了整合基因型到表型建模在下一代精准肿瘤学中的潜力。

Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.

</details>


### [70] [Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks](https://arxiv.org/abs/2512.14023)
*Yong Fang,Na Li,Hangguan Shan,Eryun Liu,Xinyu Li,Wei Ni,Er-Ping Li*

Main category: cs.LG

TL;DR: HSMGNN提出了一种混合欧几里得-黎曼几何框架的图神经网络，用于多元时间序列预测，通过双空间投影和自适应距离计算，在三个基准数据集上实现了最高13.8%的精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列预测方法通常在欧几里得或黎曼空间中建模，难以捕捉真实数据中多样化的几何结构和复杂的时空依赖关系。

Method: 提出HSMGNN模型：1) SCS嵌入将输入投影到欧几里得和黎曼双空间；2) ADB层通过可训练记忆机制降低黎曼距离计算成本；3) FGCN通过可学习融合算子集成双空间特征进行预测。

Result: 在三个基准数据集上的实验表明，HSMGNN相比最先进的基线方法，在预测精度上实现了最高13.8%的提升。

Conclusion: HSMGNN首次利用混合几何表示进行多元时间序列预测，能够更全面、更具表达力地建模几何特性，为复杂时空依赖的捕捉提供了有效解决方案。

Abstract: Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.

</details>


### [71] [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Feiping Nie,Junyu Gao,Xuelong Li*

Main category: cs.LG

TL;DR: FusAD是一个统一的时间序列分析框架，通过自适应时频融合和去噪机制，在分类、预测和异常检测等多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分析面临三大挑战：1）缺乏高效、多任务兼容且可泛化的统一框架；2）现有方法通常针对单一任务或特定数据类型；3）现实数据常受噪声、复杂频率成分和多尺度动态模式影响，难以进行稳健的特征提取。

Method: FusAD采用自适应时频融合机制，结合傅里叶变换和小波变换来捕获全局-局部和多尺度动态特征；包含自适应去噪机制，自动感知和过滤各类噪声；集成通用信息融合和解码结构，结合掩码预训练来促进多粒度表示的学习和迁移。

Result: 在主流时间序列基准测试中，FusAD在分类、预测和异常检测任务上始终优于最先进模型，同时保持高效率和可扩展性。

Conclusion: FusAD通过统一的时频融合和自适应去噪框架，有效解决了多任务时间序列分析的挑战，为复杂环境下的稳健特征提取提供了有效解决方案。

Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.

</details>


### [72] [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080)
*Wentao Guo,Mayank Mishra,Xinle Cheng,Ion Stoica,Tri Dao*

Main category: cs.LG

TL;DR: SonicMoE：通过内存高效算法、GPU内核优化和令牌舍入技术，解决细粒度MoE模型的内存和计算效率问题，显著提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型趋向于高专家粒度（小专家维度）和高稀疏性（恒定激活专家数），这虽然提高了每FLOP的模型质量，但带来了激活内存增加、硬件效率降低以及分组GEMM内核填充浪费计算的问题。

Method: 1. 提出内存高效算法，最小化MoE前向/反向传播的激活缓存；2. 设计重叠内存IO与计算的GPU内核；3. 提出"令牌舍入"方法，减少分组GEMM内核的填充浪费。

Result: SonicMoE减少45%激活内存，在Hopper GPU上相比ScatterMoE的BF16 MoE内核实现1.86倍计算吞吐提升。64个H100 GPU上达到2130亿令牌/天的训练吞吐，与96个H100上的ScatterMoE性能相当。高稀疏设置下，令牌舍入算法相比传统top-K路由额外带来1.16倍加速。

Conclusion: SonicMoE通过系统级优化有效解决了细粒度、高稀疏MoE模型的效率和内存问题，显著提升了训练性能，相关内核已开源以促进MoE模型训练加速。

Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

</details>


### [73] [Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization](https://arxiv.org/abs/2512.14086)
*Boyuan Yao,Dingcheng Luo,Lianghao Cao,Nikola Kovachki,Thomas O'Leary-Roseberry,Omar Ghattas*

Main category: cs.LG

TL;DR: 本文提出了导数信息傅里叶神经算子（DIFNOs）的逼近理论和高效训练方法，用于PDE约束优化。DIFNO通过同时最小化输出和Fréchet导数误差来学习高保真算子的响应和灵敏度，在样本复杂度上优于传统FNO。


<details>
  <summary>Details</summary>
Motivation: 传统傅里叶神经算子（FNOs）作为代理模型在PDE约束优化中可能不够准确，因为准确的优化需要准确的导数信息。需要开发既能准确预测算子响应又能准确预测其灵敏度的代理模型。

Method: 提出导数信息傅里叶神经算子（DIFNOs），通过联合最小化输出和Fréchet导数样本来训练。开发了使用维度缩减和多分辨率技术的高效训练方案，显著降低内存和计算成本。建立了FNO及其Fréchet导数在紧集上的同时通用逼近理论。

Result: 理论证明了FNOs在导数信息算子学习和PDE约束优化中的能力。数值实验表明DIFNOs在非线性扩散-反应、Helmholtz和Navier-Stokes方程上，在算子学习和无限维PDE约束逆问题求解中具有优越的样本复杂度，能以少量训练样本实现高精度。

Conclusion: DIFNOs作为代理模型在PDE约束优化中优于传统FNOs，能够准确模拟高保真算子的响应和灵敏度，并通过高效训练方法实现低样本复杂度下的高精度。

Abstract: We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fréchet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fréchet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fréchet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fréchet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.

</details>


### [74] [Arithmetic-Intensity-Aware Quantization](https://arxiv.org/abs/2512.14090)
*Taig Singh,Shreshth Rajan,Nikhil Iyer*

Main category: cs.LG

TL;DR: AIQ是一种后训练混合精度量化框架，通过逐层选择比特宽度来最大化算术强度，同时最小化精度损失，从而解决内存带宽限制问题。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络越来越受内存限制，推理吞吐量受DRAM带宽而非计算能力限制。需要一种方法在保持精度的同时提高算术强度，从而提升推理性能。

Method: AIQ是一种后训练量化方法，使用搜索算法在逐层量化方案上优化，最小化算术强度和精度的加权损失。通过混合精度量化，为每层选择最优比特宽度。

Result: 在ResNet-20/CIFAR-10上，AIQ将算术强度提高约50%，测试精度保持在1个百分点内，优于全局统一量化。在内存受限的MobileNetV2上，AIQ配置提供1.66倍吞吐量提升，精度损失在1个百分点内。

Conclusion: AIQ能有效提高内存受限神经网络的推理吞吐量，通过算术强度感知的混合精度量化，在保持精度的同时显著提升性能，且自然地对大层进行更激进的量化。

Abstract: As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.

</details>


### [75] [Cornserve: Efficiently Serving Any-to-Any Multimodal Models](https://arxiv.org/abs/2512.14098)
*Jeff J. Ma,Jae-Won Chung,Jisang Ahn,Yizhuo Liang,Akshay Jajoo,Myungjin Lee,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Cornserve是一个针对Any-to-Any多模态模型的高效在线服务系统，通过自动规划模型部署和分布式执行，显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: Any-to-Any多模态模型（接受和生成文本、图像、视频、音频等多种模态数据）在在线服务时面临请求类型、计算路径和计算规模异质性的挑战，现有解决方案难以高效处理这种复杂性。

Method: 1) 允许开发者描述包含多模态编码器、LLM、扩散变换器等异质组件的计算图；2) 规划器基于模型和工作负载特征自动优化部署计划，包括是否及如何将模型分解为更小组件；3) 分布式运行时按计划执行模型，高效处理异质性。

Result: Cornserve能够高效服务多样化的Any-to-Any模型和工作负载，相比现有解决方案，吞吐量提升最高达3.81倍，尾部延迟降低最高达5.79倍。

Conclusion: Cornserve通过自动化的部署规划和高效的分布式执行，成功解决了Any-to-Any多模态模型在线服务的异质性问题，为这类新兴模型提供了有效的服务系统解决方案。

Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.

</details>


### [76] [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)
*Chunjin Jian,Xinhua Zhu*

Main category: cs.LG

TL;DR: 提出S-GRPO方法，使用逻辑相似性奖励机制替代传统奖励建模，通过形式逻辑一致性引导模型对齐人类偏好，避免模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法依赖奖励模型的质量和稳定性，而传统奖励建模存在启发式估计的局限性。需要更可靠的方法来确保大语言模型与人类价值观的对齐。

Method: 提出逻辑相似性奖励机制，基于形式逻辑一致性而非启发式奖励估计。引入S-GRPO（GRPO的监督变体），结合监督组件，联合优化生成项、KL散度正则化和基于标签的目标函数。

Result: S-GRPO在性能和鲁棒性上均优于标准监督微调（SFT），并能扩展现有的偏好学习框架（如GRPO和DPO），提供更灵活、任务自适应的对齐训练方法。

Conclusion: 逻辑相似性奖励机制和S-GRPO框架为RLHF提供了更可靠的对齐方法，通过形式逻辑一致性提升模型对齐效果，避免模型崩溃问题。

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

</details>


### [77] [PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario](https://arxiv.org/abs/2512.14150)
*Zhijie Zhong,Zhiwen Yu,Pengyu Li,Jianming Lv,C. L. Philip Chen,Min Chen*

Main category: cs.LG

TL;DR: PathFinder提出了一种新颖的无线电路径损耗预测架构，通过解耦特征编码主动建模建筑物和发射器，使用掩码引导低秩注意力机制，并引入发射器导向的混合策略，在单发射器到多发射器的分布偏移场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的无线电路径损耗预测方法存在三个主要问题：1）被动环境建模，忽略了发射器和关键环境特征；2）过度关注单发射器场景，而现实世界多为多发射器场景；3）过度关注分布内性能，忽视了分布偏移挑战，特别是在训练/测试环境建筑密度或发射器配置不同的情况下。

Method: 提出PathFinder架构，通过解耦特征编码主动建模建筑物和发射器，集成掩码引导低秩注意力机制独立关注接收器和建筑区域，并引入发射器导向的混合策略进行鲁棒训练。还创建了新的单到多发射器RPP基准来评估外推性能。

Result: 实验结果表明PathFinder显著优于现有最先进方法，特别是在具有挑战性的多发射器场景中，在单发射器训练后多发射器测试的分布偏移情况下表现出色。

Conclusion: PathFinder通过主动环境建模、专门的多发射器处理机制和鲁棒训练策略，有效解决了当前RPP方法在现实多发射器场景和分布偏移下的局限性，为5G网络优化和物联网应用提供了更实用的解决方案。

Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.

</details>


### [78] [On Improving Deep Active Learning with Formal Verification](https://arxiv.org/abs/2512.14170)
*Jonathan Spiegelman,Guy Amir,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出通过形式验证生成对抗样本来增强深度主动学习，相比传统梯度攻击方法能更有效提升模型泛化性能


<details>
  <summary>Details</summary>
Motivation: 深度主动学习旨在减少神经网络训练中的标注成本，但现有方法主要关注选择哪些样本进行标注。本文探索通过添加违反鲁棒性约束的对抗样本来增强训练数据，进一步提升数据效率

Method: 提出使用形式验证生成对抗样本，而非传统的基于梯度的攻击方法，并将此方法扩展到多种现代深度主动学习技术中，同时提出一种新技术

Result: 形式验证生成的对抗样本相比标准梯度攻击方法贡献显著更大，在多个标准基准测试中显著提升了模型的泛化能力

Conclusion: 通过形式验证生成对抗样本来增强训练数据是提升深度主动学习性能的有效方法，能够显著提高模型泛化能力

Abstract: Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.

</details>


### [79] [Optimizing the Adversarial Perturbation with a Momentum-based Adaptive Matrix](https://arxiv.org/abs/2512.14188)
*Wei Tao,Sheng Long,Xin Liu,Wei Li,Qing Tao*

Main category: cs.LG

TL;DR: 论文提出AdaMI攻击方法，使用基于动量的自适应矩阵优化对抗样本，解决了MI-FGSM的非收敛问题，提高了对抗样本的迁移性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的对抗攻击方法（如PGD、MI-FGSM）都使用符号函数来缩放扰动，从优化理论角度看存在多个理论问题。作者发现PGD实际上是投影梯度法的特定重构，且当使用包含累积梯度的传统自适应矩阵时，PGD就变成了AdaGrad。这一分析启发了新方法的设计。

Method: 提出AdaMI攻击方法，使用基于动量的自适应矩阵来优化扰动。该方法被证明在凸问题上能达到最优收敛，解决了MI-FGSM的非收敛问题，确保了优化过程的稳定性。

Result: 实验表明，提出的基于动量的自适应矩阵可以作为通用且有效的技术，在不同网络架构上显著提升对抗样本的迁移性，同时保持更好的稳定性和不可感知性。

Conclusion: AdaMI通过引入动量自适应矩阵，不仅解决了现有方法的理论缺陷，还在实践中显著提升了对抗攻击的效果，为对抗样本生成提供了更稳定和有效的优化框架。

Abstract: Generating adversarial examples (AEs) can be formulated as an optimization problem. Among various optimization-based attacks, the gradient-based PGD and the momentum-based MI-FGSM have garnered considerable interest. However, all these attacks use the sign function to scale their perturbations, which raises several theoretical concerns from the point of view of optimization. In this paper, we first reveal that PGD is actually a specific reformulation of the projected gradient method using only the current gradient to determine its step-size. Further, we show that when we utilize a conventional adaptive matrix with the accumulated gradients to scale the perturbation, PGD becomes AdaGrad. Motivated by this analysis, we present a novel momentum-based attack AdaMI, in which the perturbation is optimized with an interesting momentum-based adaptive matrix. AdaMI is proved to attain optimal convergence for convex problems, indicating that it addresses the non-convergence issue of MI-FGSM, thereby ensuring stability of the optimization process. The experiments demonstrate that the proposed momentum-based adaptive matrix can serve as a general and effective technique to boost adversarial transferability over the state-of-the-art methods across different networks while maintaining better stability and imperceptibility.

</details>


### [80] [Random-Bridges as Stochastic Transports for Generative Models](https://arxiv.org/abs/2512.14190)
*Stefano Goria,Levent A. Mengütürk,Murat C. Mengütürk,Berkan Sesen*

Main category: cs.LG

TL;DR: 提出使用随机桥（random-bridges）作为生成模型中的随机传输方法，能在较少步骤内生成高质量样本，计算成本低，适合高速生成任务。


<details>
  <summary>Details</summary>
Motivation: 将随机桥（随机过程在固定时间点被约束到目标分布）应用于生成建模领域，作为两个概率分布之间的随机传输方法。

Method: 从一般概率陈述出发，推导出特定的表示形式，用于学习和模拟算法的信息处理。基于高斯随机桥构建具体实现。

Result: 相比传统方法，在显著减少步骤的情况下生成高质量样本，同时获得有竞争力的Fréchet inception distance分数。

Conclusion: 提出的框架计算成本低，适合高速生成任务，随机桥可根据驱动过程展示马尔可夫或非马尔可夫、连续、不连续或混合模式。

Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.

</details>


### [81] [Understanding and Improving Hyperbolic Deep Reinforcement Learning](https://arxiv.org/abs/2512.14202)
*Timo Klein,Thomas Lang,Andrii Shkabrii,Alexander Sturm,Kevin Sidak,Lukas Miklautz,Claudia Plant,Yllka Velaj,Sebastian Tschiatschek*

Main category: cs.LG

TL;DR: Hyper++：一种稳定的双曲强化学习智能体，通过改进双曲特征空间的优化方法，解决了传统双曲RL训练中的不稳定性问题，在ProcGen和Atari-5任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 双曲特征空间能更好地捕捉复杂RL环境中的层次和关系结构，但由于RL的非平稳性，双曲空间的优化面临挑战，特别是大范数嵌入会导致梯度训练不稳定。

Method: 提出Hyper++双曲PPO智能体，包含三个核心组件：1）使用分类价值损失而非回归的稳定评论家训练；2）特征正则化保证有界范数同时避免裁剪的维度诅咒；3）使用更优化友好的双曲网络层公式。

Result: 在ProcGen实验中，Hyper++保证了稳定学习，优于先前的双曲智能体，并减少约30%的墙钟时间。在Atari-5的Double DQN中，Hyper++显著优于欧几里得和双曲基线方法。

Conclusion: 通过分析双曲几何中的梯度特性并设计相应的稳定化组件，Hyper++成功解决了双曲RL训练中的优化挑战，为利用双曲空间进行强化学习提供了有效的解决方案。

Abstract: The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .

</details>


### [82] [Estimating problem difficulty without ground truth using Large Language Model comparisons](https://arxiv.org/abs/2512.14220)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.LG

TL;DR: 提出LLM compare方法，通过大语言模型进行成对难度比较并计算Bradley-Terry分数，以评估超出人类和当前LLM能力范围的分布外问题的难度。


<details>
  <summary>Details</summary>
Motivation: 当前基于人工校准或性能评分的问题难度评估方法无法扩展到分布外问题（人类和LLM目前无法解决的问题），因为这些方法不可扩展、耗时且依赖真实标签。

Method: 使用LLM进行成对难度比较，然后基于比较结果计算Bradley-Terry分数。该方法在三个正交维度（构建方式、规模、依赖性）上占据理想象限，是首个连续动态、模型无关且不依赖真实标签的难度评估方法。

Result: 1) 提出概念框架定位现有方法；2) LLM compare与人工标注高度一致（Pearson r ≥ 0.80，n=1876）；3) 对幻觉具有鲁棒性（10%噪声注入下Pearson相关性下降小于6%）。

Conclusion: LLM compare是评估分布外问题难度的有效方法，能够替代耗时的人工标注和合成数据生成，对课程设计、模型评估和AI辅助研究构思有重要推动作用。

Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.

</details>


### [83] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 提出一种从数据中学习反应-扩散系统的物理一致性方法，确保学习模型满足质量守恒和拟正性等物理约束，并证明其理论收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型学习反应-扩散系统时往往忽略物理约束，导致模型可能违反基本物理定律（如质量守恒、非负性），需要开发既能从数据学习又能保证物理一致性的方法。

Method: 基于正则化框架，提出系统修改参数化反应项的技术，使其固有地满足质量守恒和拟正性；扩展理论结果到物理一致的反应-扩散系统学习，证明收敛性并提供拟正函数逼近结果。

Result: 开发出能保证物理一致性的反应-扩散系统学习方法，确保学习模型满足非负性和物理原理；理论证明学习问题解收敛到正则化最小化解，即使施加守恒律和拟正性约束。

Conclusion: 该方法推进了可解释、可靠的数据驱动反应-扩散模型发展，确保模型与基本物理定律一致，为物理约束下的系统辨识提供了系统化框架。

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [84] [Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning](https://arxiv.org/abs/2512.14241)
*Salvatore Romano,Marco Grassia,Giuseppe Mangioni*

Main category: cs.LG

TL;DR: 该论文提出了一种名为RGM的新方法，用于评估图生成模型，克服了传统MMD指标的局限性，并对GRAN和EDGE两种先进模型进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 当前图生成模型评估主要依赖最大均值差异(MMD)指标，这种方法存在局限性，无法充分评估生成图的结构特性，需要更有效的评估方法。

Method: 提出RGM（Representation-aware Graph-generation Model evaluation）评估方法，使用几何深度学习模型在专门设计的合成和真实图数据集上进行图分类任务，比较不同图生成模型的性能。

Result: 对GRAN和EDGE两种先进图生成模型的评估显示，虽然两者都能生成具有某些拓扑特性的图，但在保持区分不同图域的结构特征方面存在显著局限性。

Conclusion: MMD作为图生成模型评估指标存在不足，需要开发更有效的评估方法，RGM为未来研究提供了替代方案，强调需要更好地保留图的结构特征。

Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.

</details>


### [85] [FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting](https://arxiv.org/abs/2512.14253)
*Xingjian Wu,Hanyin Cheng,Xiangfei Qiu,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: FLAME是一个极轻量级的时间序列基础模型家族，支持确定性和概率性预测，通过生成式概率建模确保效率和鲁棒性，在零样本预测任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型往往在效率和准确性之间需要权衡，特别是对于概率性预测任务。需要开发一个既轻量又强大的基础模型，能够同时支持确定性和概率性预测，并在零样本场景下表现优异。

Method: FLAME采用Legendre Memory增强泛化能力，通过变体LegT和LegS在编码和解码阶段捕获数据内在归纳偏置，实现高效长程推理。使用归一化流作为预测头，以生成方式建模任意复杂分布。

Result: 在TSFM-Bench和ProbTS等基准测试中，FLAME在确定性和概率性预测任务上均展现出一致的SOTA零样本性能，证明了其高效性和准确性。

Conclusion: FLAME成功构建了一个极轻量级的时间序列基础模型，通过创新的Legendre Memory架构和归一化流预测头，在保持高效率的同时实现了卓越的预测性能，为零样本时间序列预测提供了有效解决方案。

Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.

</details>


### [86] [Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization](https://arxiv.org/abs/2512.14263)
*Nick Leenders,Thomas Quadt,Boris Cule,Roy Lindelauf,Herman Monsuur,Joost van Oijen,Mark Voskuijl*

Main category: cs.LG

TL;DR: 提出基于决策树的解释性贝叶斯优化模型，替代传统高斯过程，能处理分类和连续数据，在尖峰函数上表现更优


<details>
  <summary>Details</summary>
Motivation: 当前基于高斯过程的偏好贝叶斯优化方法存在解释性差、难以处理分类数据、计算复杂度高等问题，限制了实际应用

Method: 引入基于决策树的解释性代理模型，能够处理分类和连续数据，适用于大规模数据集

Result: 在八个尖峰优化函数上，新模型在尖峰函数上优于GP方法，在非尖峰函数上性能略低；在真实寿司数据集上能学习个人偏好；初步研究表明历史偏好数据可加速新用户优化

Conclusion: 决策树模型为偏好贝叶斯优化提供了可解释、可扩展的替代方案，特别适用于尖峰函数和包含分类数据的实际应用

Abstract: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.

</details>


### [87] [Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits](https://arxiv.org/abs/2512.14338)
*Michael Murray,Tenzin Chan,Kedar Karhadker,Christopher J. Hillar*

Main category: cs.LG

TL;DR: Hopfield网络能够通过小样本学习图同构类，其学习过程具有向范数效率解的隐式偏置，这驱动了在群结构数据下近似不变性的涌现。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在处理对称性数据时的不变性如何从训练中涌现，特别是在经典Hopfield网络中探索这一现象，以理解图同构类学习的基本机制。

Method: 使用经典Hopfield网络，通过最小化能量流（MEF）的梯度下降方法，研究网络如何从小随机样本中推断图的完整同构类，分析参数在不变子空间中的收敛行为。

Result: 发现图同构类可以在三维不变子空间中表示；梯度下降的范数效率偏置支撑了学习同构类的多项式样本复杂度界限；多种学习规则下参数都向不变子空间收敛。

Conclusion: Hopfield网络中的泛化机制统一于学习过程中的范数效率偏置，这种偏置在群结构数据下驱动了近似不变性的涌现，为理解神经网络中不变性的学习提供了理论框架。

Abstract: Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

</details>


### [88] [Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis](https://arxiv.org/abs/2512.14361)
*Nicholas Tagliapietra,Katharina Ensinger,Christoph Zimmer,Osman Mian*

Main category: cs.LG

TL;DR: CaDyT是一种用于动态系统因果发现的新方法，通过基于差异的因果模型和精确高斯过程推理来建模连续时间动力学，在规则和不规则采样数据上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统按照其潜在因果关系在连续时间中演化，但现有方法要么离散化时间导致不规则采样数据性能差，要么忽略底层因果关系，需要一种能同时解决这两个挑战的方法。

Method: CaDyT基于差异因果模型，使用精确高斯过程推理建模连续时间动力学，通过算法马尔可夫条件和最小描述长度原则指导的贪婪搜索来识别因果结构。

Result: 实验表明CaDyT在规则和不规则采样数据上都优于现有方法，发现的因果网络更接近真实的底层动力学。

Conclusion: CaDyT通过结合连续时间建模和因果发现，为动态系统的因果分析提供了更准确的方法，特别是在处理不规则采样数据时表现出色。

Abstract: Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.

</details>


### [89] [Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries](https://arxiv.org/abs/2512.14388)
*Baobao Song,Shiva Raj Pokhrel,Athanasios V. Vasilakos,Tianqing Zhu,Gang Li*

Main category: cs.LG

TL;DR: 首个基于提升量子差分隐私的黑盒隐私审计框架，用于量子机器学习模型，通过量子金丝雀检测记忆化并量化隐私泄露


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在处理敏感数据时存在记忆化个体记录的风险，而现有的量子差分隐私机制缺乏对部署模型的实证验证工具

Method: 基于提升量子差分隐私，使用量子金丝雀（战略性偏移编码的量子态）检测记忆化，建立金丝雀偏移与迹距离界限的数学联系，推导隐私预算消耗的经验下界

Result: 在模拟和物理量子硬件上的全面评估表明，该框架能有效测量QML模型的实际隐私损失，实现量子机器学习系统的鲁棒隐私验证

Conclusion: 该框架填补了理论保证与实际隐私验证之间的关键空白，为量子机器学习系统提供了首个黑盒隐私审计解决方案

Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.

</details>


### [90] [RePo: Language Models with Context Re-Positioning](https://arxiv.org/abs/2512.14391)
*Huayang Li,Tianyu Zhao,Richard Sproat*

Main category: cs.LG

TL;DR: RePo提出了一种新的上下文重定位机制，通过可微分模块动态分配token位置来捕获上下文依赖关系，减少外部认知负荷，提升LLM在噪声上下文、结构化数据和长上下文任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM架构采用固定线性或常数位置索引，这种无信息结构增加了外部认知负荷，消耗了本应用于深度推理和注意力分配的有限工作记忆容量。

Method: 提出RePo机制，使用可微分模块f_φ为token分配位置来捕获上下文依赖关系，而不是依赖预定义的整数范围。在OLMo-2 1B骨干网络上进行持续预训练。

Result: RePo显著提升了在噪声上下文、结构化数据和长上下文任务上的性能，同时在一般短上下文任务上保持竞争力。分析显示RePo能更好地关注远距离相关信息，在密集非线性空间中分配位置，并捕获输入上下文的内在结构。

Conclusion: RePo通过减少外部认知负荷，提供了一种更有效的上下文表示方法，能够改善LLM在复杂上下文场景下的推理能力。

Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.

</details>


### [91] [SuperWing: a comprehensive transonic wing dataset for data-driven aerodynamic design](https://arxiv.org/abs/2512.14397)
*Yunjia Yang,Weishao Tang,Mengxin Liu,Nils Thuerey,Yufei Zhang,Haixin Chen*

Main category: cs.LG

TL;DR: SuperWing是一个包含4,239个参数化机翼几何形状和28,856个RANS流场解的开源数据集，用于训练可泛化的三维机翼空气动力学机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 现有三维机翼数据集稀缺且多样性有限，限制了机器学习代理模型在空气动力学设计中的通用性和泛化能力。

Method: 使用简化的几何参数化方法生成机翼形状，包含翼展方向的翼型变化、扭转和上反角，覆盖广泛的马赫数和攻角范围，通过RANS模拟获得流场数据。

Result: 基于SuperWing训练的最先进Transformer模型能够准确预测表面流动，在保留样本上达到2.5阻力计数误差，并在DLR-F6和NASA CRM等复杂基准机翼上表现出强大的零样本泛化能力。

Conclusion: SuperWing数据集具有足够的多样性和实用性，能够支持开发可泛化的三维机翼空气动力学预测模型，有望加速空气动力学设计过程。

Abstract: Machine-learning surrogate models have shown promise in accelerating aerodynamic design, yet progress toward generalizable predictors for three-dimensional wings has been limited by the scarcity and restricted diversity of existing datasets. Here, we present SuperWing, a comprehensive open dataset of transonic swept-wing aerodynamics comprising 4,239 parameterized wing geometries and 28,856 Reynolds-averaged Navier-Stokes flow field solutions. The wing shapes in the dataset are generated using a simplified yet expressive geometry parameterization that incorporates spanwise variations in airfoil shape, twist, and dihedral, allowing for an enhanced diversity without relying on perturbations of a baseline wing. All shapes are simulated under a broad range of Mach numbers and angles of attack covering the typical flight envelope. To demonstrate the dataset's utility, we benchmark two state-of-the-art Transformers that accurately predict surface flow and achieve a 2.5 drag-count error on held-out samples. Models pretrained on SuperWing further exhibit strong zero-shot generalization to complex benchmark wings such as DLR-F6 and NASA CRM, underscoring the dataset's diversity and potential for practical usage.

</details>


### [92] [GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion](https://arxiv.org/abs/2512.14400)
*Fangzhou Lin,Guoshun He,Zhenyu Guo,Zhe Huang,Jinsong Tao*

Main category: cs.LG

TL;DR: GRAFT：一种改进的电网负荷预测模型，通过文本引导融合和多源文本干预，在澳大利亚五个州的统一基准测试中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 电力负荷同时受到天气、日历节奏、突发事件和政策等多种外生因素的多时间尺度影响，需要更好的电网感知预测和多源文本干预支持。

Method: 改进STanHOP模型，严格对齐每日汇总的新闻、社交媒体和政策文本与半小时负荷数据，通过交叉注意力实现文本引导融合到特定时间位置，并提供即插即用的外部记忆接口。

Result: 在澳大利亚五个州2019-2021年的统一基准测试中，GRAFT在小时、日和月三个时间尺度上显著优于强基线方法，达到或超过最先进水平，在事件驱动场景中表现稳健。

Conclusion: GRAFT通过文本引导融合和多源文本干预有效提升了电网负荷预测性能，其注意力机制支持时间定位和源级解释，发布的基准和工具促进了标准化评估和可复现性。

Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.

</details>


### [93] [Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space](https://arxiv.org/abs/2512.14418)
*Dejun Hu,Zhiming Li,Jia-Rui Shen,Jia-Ning Tu,Zi-Hao Ye,Junliang Zhang*

Main category: cs.LG

TL;DR: 提出双轴表示-完全收敛学习(RCCL)策略，通过FD25数据集实现有机分子的近完全组合覆盖，使图神经网络在分子性质预测中达到约1.0 kcal/mol的MAE误差


<details>
  <summary>Details</summary>
Motivation: 化学空间极其庞大(10^30-10^60分子)，现有机器学习模型能否在这种规模的空间中实现收敛学习仍是一个开放的科学问题。需要建立系统性的框架来确保模型能够覆盖完整的化学空间并实现可泛化的学习。

Method: 提出双轴表示-完全收敛学习(RCCL)策略：1) 基于现代价键理论的图卷积网络(GCN)编码局部价态环境；2) 无桥图(NBG)编码环/笼拓扑结构。基于此框架构建FD25数据集，系统覆盖13,302个局部价态单元和165,726个环/笼拓扑，实现H/C/N/O/F有机分子的近完全组合覆盖。

Result: 在FD25数据集上训练的图神经网络表现出表示-完全收敛学习能力，在外部基准测试中整体预测误差约为1.0 kcal/mol MAE，显示出强大的分布外泛化能力。

Conclusion: 建立了分子表示、结构完整性和模型泛化之间的定量联系，为可解释、可迁移和数据高效的分子智能奠定了基础，解决了化学空间中收敛学习的核心挑战。

Abstract: Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.

</details>


### [94] [Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF](https://arxiv.org/abs/2512.14444)
*Akira Takeshima,Kenta Shiraishi,Atsushi Okazaki,Tadashi Tsuyuki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: ClimaX-LETKF是首个纯数据驱动的机器学习集合天气预报系统，通过同化NCEP观测数据实现多年稳定运行，研究发现RTPP比RTPS更适合ML模型，且ML模型恢复大气场吸引子的能力弱于NWP模型。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习天气预报（MLWP）在融合真实观测或集合预报方面研究有限，需要开发独立于数值天气预报（NWP）模型的纯数据驱动集合预报系统，以推进MLWP的实际应用。

Method: 提出ClimaX-LETKF系统，基于局部集合变换卡尔曼滤波（LETKF）框架，同化NCEP ADP全球高空和地面天气观测数据，比较RTPP（松弛到先验扰动）和RTPS（松弛到先验扩展）两种扰动松弛方法。

Result: 系统能多年稳定运行，RTPP比RTPS在MLWP模型中表现更稳定准确（与NWP模型相反），实验显示MLWP模型恢复大气场吸引子的能力弱于NWP模型。

Conclusion: 这项工作为增强MLWP集合预报系统提供了重要见解，代表了向实际应用迈出的重要一步，揭示了MLWP与NWP在扰动松弛方法上的不同特性。

Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.

</details>


### [95] [Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics](https://arxiv.org/abs/2512.14471)
*Additi Pandey,Liang Wei,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: Kinetic-Mamba：基于Mamba的神经算子框架，用于准确预测燃烧模拟中的化学动力学演化，通过多种模型变体实现高效的时间建模和守恒约束。


<details>
  <summary>Details</summary>
Motivation: 准确的化学动力学建模对燃烧模拟至关重要，传统方法难以高效处理复杂反应路径和热化学状态演化，需要结合神经算子的表达能力和Mamba架构的时间建模效率。

Method: 提出Kinetic-Mamba框架，包含三种模型：(1)独立Mamba模型预测热化学状态变量时间演化；(2)约束Mamba模型强制质量守恒；(3)基于温度区间的双Mamba模型架构。还开发了潜在空间变体，在降维空间中演化动力学并重构到物理流形。使用时间分解和递归预测策略评估。

Result: 在Syngas和GRI-Mech 3.0反应机理上的计算实验表明，该框架仅使用状态变量初始条件就能高保真地预测复杂动力学行为，并具有良好的外推能力。

Conclusion: Kinetic-Mamba框架成功整合了神经算子的表达能力和Mamba架构的时间建模效率，为燃烧模拟中的化学动力学建模提供了准确、高效的解决方案，在保持高精度的同时确保了物理一致性。

Abstract: Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.

</details>


### [96] [Improving Slow Transfer Predictions: Generative Methods Compared](https://arxiv.org/abs/2512.14522)
*Jacob Taegon Kim,Alex Sim,Kesheng Wu,Jinoh Kim*

Main category: cs.LG

TL;DR: 该研究探讨了在科学计算网络中通过数据增强方法解决类别不平衡问题，以提高机器学习模型对数据传输性能的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在科学计算网络中，早期预测数据传输性能对于识别潜在缓慢传输和优化网络使用至关重要。然而，机器学习模型在此应用中的预测能力受到类别不平衡问题的限制，需要有效的方法来解决这一瓶颈。

Method: 研究分析和比较了多种数据增强策略，包括传统的过采样方法和生成技术（如CTGAN），同时调整训练数据集中的类别不平衡比例来评估对模型性能的影响。

Result: 研究发现，虽然数据增强可能改善性能，但随着不平衡比例的增加，性能提升并不显著。即使是最先进的CTGAN技术，也没有比简单的分层采样方法带来显著改进。

Conclusion: 在科学计算网络的性能预测任务中，解决类别不平衡问题时，先进的数据增强技术（如CTGAN）并不比简单的分层采样方法有显著优势，特别是在高不平衡比例情况下。

Abstract: Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.

</details>


### [97] [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559)
*Emmanuel C. Chukwu,Rianne M. Schouten,Monique Tabak,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 该论文批评现有时间序列反事实解释方法在临床推荐场景中的不足，指出它们过于关注最小化输入扰动而忽略了因果合理性和时间连贯性，并呼吁开发更实用、用户中心的反事实方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分类的反事实解释方法主要基于静态数据假设，专注于生成最小输入扰动来改变模型预测。然而在临床推荐场景中，干预措施随时间展开，必须具有因果合理性和时间连贯性，现有方法存在根本性不足。

Method: 作者通过论证分析现有方法的局限性，特别是时间盲点和用户中心考虑不足的问题。对几种最先进的时间序列反事实方法进行了鲁棒性分析，展示生成的反事实对随机噪声高度敏感。

Result: 分析显示现有方法生成的反事实对随机噪声高度敏感，在真实临床环境中可靠性有限，因为微小测量变异不可避免。这凸显了现有方法在实际应用中的局限性。

Conclusion: 呼吁开发超越仅改变预测而不考虑可行性的方法和评估框架，强调需要可操作、目的驱动的干预措施，这些措施在真实世界环境中对用户是可行的。

Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.

</details>


### [98] [Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection](https://arxiv.org/abs/2512.14563)
*Tejaswani Dash,Gautam Datla,Anudeep Vurity,Tazeem Ahmad,Mohd Adnan,Saima Rafi,Saisha Patro,Saina Patro*

Main category: cs.LG

TL;DR: 提出Residual GRU with Multi-Head Self-Attention模型，在UCI心脏病数据集上达到0.861准确率，优于传统方法和现代深度学习基线，适合资源受限的医疗环境部署。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要可靠高效的预测工具。传统方法依赖人工特征和临床经验，机器学习方法可提高可重复性但难以在嘈杂异构的临床数据中泛化。

Method: 提出紧凑的深度学习架构，整合残差双向门控循环单元进行特征列序列建模、通道重加权块、以及带可学习分类标记的多头自注意力池化来捕获全局上下文。

Result: 在UCI心脏病数据集上，模型达到准确率0.861、宏F1 0.860、ROC-AUC 0.908、PR-AUC 0.904，优于所有基线方法。消融研究确认了残差循环、通道门控和注意力池化的各自贡献。

Conclusion: 轻量级混合循环和注意力架构在临床风险预测中提供了准确性和效率的良好平衡，支持在资源受限的医疗环境中部署。

Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.

</details>


### [99] [Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs](https://arxiv.org/abs/2512.14596)
*Youngkyu Lee,Francesc Levrero Florencio,Jay Pathak,George Em Karniadakis*

Main category: cs.LG

TL;DR: Geo-DeepONet：一种几何感知的深度算子网络，结合有限元离散化的域信息，可在任意非结构化网格上进行算子学习而无需重新训练，并用于构建几何感知的混合预条件迭代求解器。


<details>
  <summary>Details</summary>
Motivation: 传统迭代求解器对参数偏微分方程的收敛行为高度依赖于域和离散化方式，先前提出的混合求解器在训练未见过的几何形状上表现不佳，需要解决几何泛化问题。

Method: 提出Geo-DeepONet几何感知深度算子网络，从有限元离散化中提取域信息，结合传统松弛方案和Krylov子空间算法，构建几何感知的混合预条件迭代求解器。

Result: 通过在不同非结构化域上的参数偏微分方程数值实验，证明了所提混合求解器在多个实际应用中具有增强的鲁棒性和效率。

Conclusion: Geo-DeepONet能够实现跨任意非结构化网格的准确算子学习，无需重新训练，基于此构建的几何感知混合求解器显著提升了求解参数偏微分方程的鲁棒性和效率。

Abstract: The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.

</details>


### [100] [Hierarchical Persistence Velocity for Network Anomaly Detection: Theory and Applications to Cryptocurrency Markets](https://arxiv.org/abs/2512.14615)
*Omid Khormali*

Main category: cs.LG

TL;DR: 提出了一种新的拓扑数据分析方法OW-HNPV，用于检测时变网络中的异常，通过测量拓扑特征的"速度"（出现和消失的速率）来改进加密货币异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要测量累积拓扑存在，缺乏对拓扑特征动态变化速率的关注。需要一种能自动降噪、数学稳定且适用于不同特征类型网络的方法来检测动态网络中的结构异常。

Method: 提出重叠加权分层归一化持久性速度（OW-HNPV），首次引入持久性图的速度视角，测量特征出现和消失的速率，通过基于重叠的加权自动降低噪声影响。

Result: 在以太坊交易网络（2017年5月-2018年5月）的实验中，OW-HNPV在加密货币异常检测方面表现优异，相比基线模型在7天价格变动预测中实现了高达10.4%的AUC提升。在中长期预测（4-7天）中优于现有方法。

Conclusion: 建模拓扑速度对于检测动态网络中的结构异常至关重要，OW-HNPV提供了最一致和稳定的性能，证明了速度视角在拓扑数据分析中的价值。

Abstract: We introduce the Overlap-Weighted Hierarchical Normalized Persistence Velocity (OW-HNPV), a novel topological data analysis method for detecting anomalies in time-varying networks. Unlike existing methods that measure cumulative topological presence, we introduce the first velocity-based perspective on persistence diagrams, measuring the rate at which features appear and disappear, automatically downweighting noise through overlap-based weighting. We also prove that OW-HNPV is mathematically stable. It behaves in a controlled, predictable way, even when comparing persistence diagrams from networks with different feature types. Applied to Ethereum transaction networks (May 2017-May 2018), OW-HNPV demonstrates superior performance for cryptocurrency anomaly detection, achieving up to 10.4% AUC gain over baseline models for 7-day price movement predictions. Compared with established methods, including Vector of Averaged Bettis (VAB), persistence landscapes, and persistence images, velocity-based summaries excel at medium- to long-range forecasting (4-7 days), with OW-HNPV providing the most consistent and stable performance across prediction horizons. Our results show that modeling topological velocity is crucial for detecting structural anomalies in dynamic networks.

</details>


### [101] [Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes](https://arxiv.org/abs/2512.14617)
*Alessandro Trapasso,Luca Iocchi,Fabio Patrizi*

Main category: cs.LG

TL;DR: QR-MAX：首个基于模型的NMRDP算法，通过奖励机分解马尔可夫转移与非马尔可夫奖励，实现多项式样本复杂度的PAC收敛，并扩展到连续状态空间。


<details>
  <summary>Details</summary>
Motivation: 许多实际决策问题涉及依赖于整个系统历史的任务，而不仅仅是达到具有期望属性的状态。马尔可夫强化学习方法不适用于此类任务，而具有非马尔可夫奖励决策过程（NMRDP）的RL虽然能处理时间依赖任务，但长期以来缺乏关于（近）最优性和样本效率的形式保证。

Method: 提出QR-MAX算法，通过奖励机将马尔可夫转移学习与非马尔可夫奖励处理进行因子分解。这是首个利用这种分解实现多项式样本复杂度PAC收敛的离散动作NMRDP模型基RL算法。然后扩展到连续状态空间的Bucket-QR-MAX，使用SimHash离散化器保持相同的因子分解结构。

Result: 在复杂度递增的环境上与最先进的模型基RL方法进行实验比较，显示样本效率显著提高，寻找最优策略的鲁棒性增强。

Conclusion: QR-MAX解决了NMRDP中长期缺乏形式保证的问题，通过创新的因子分解方法实现了多项式样本复杂度的PAC收敛，并在连续状态空间中保持了高效稳定的学习性能。

Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

</details>


### [102] [ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning](https://arxiv.org/abs/2512.14619)
*Chaohao Yuan,Zhenjie Song,Ercan Engin Kuruoglu,Kangfei Zhao,Yang Liu,Deli Zhao,Hong Cheng,Yu Rong*

Main category: cs.LG

TL;DR: ParaFormer提出PageRank增强的注意力机制来解决图Transformer中的过平滑问题，在节点分类和图分类任务上取得一致性能提升


<details>
  <summary>Details</summary>
Motivation: 研究发现图Transformer中的全局注意力机制存在严重的过平滑问题，导致节点表示变得不可区分，这种过平滑效应甚至比GNNs更强

Method: 提出PageRank Transformer (ParaFormer)，采用PageRank增强的注意力模块来模拟深度Transformer的行为，作为自适应滤波器缓解过平滑

Result: 在11个数据集（从数千到数百万节点）的节点分类和图分类任务上，ParaFormer都取得了稳定的性能提升

Conclusion: ParaFormer通过PageRank增强的注意力机制有效缓解了图Transformer中的过平滑问题，在各种规模的数据集上都表现出优越性能

Abstract: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

</details>


### [103] [Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks](https://arxiv.org/abs/2512.14675)
*Rae Chipera,Jenny Du,Irene Tsapara*

Main category: cs.LG

TL;DR: 研究发现非光滑激活函数（如混沌、随机、分形函数）在储层计算中不仅保持回声状态特性，而且在收敛速度和谱半径容限方面优于传统光滑函数，其中康托函数表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 当前储层计算主要依赖光滑、全局Lipschitz连续的激活函数，限制了在国防、灾害响应和药物建模等需要极端条件下鲁棒操作的领域的应用。需要研究非光滑激活函数的潜力。

Method: 系统研究非光滑激活函数（包括混沌、随机和分形变体）在回声状态网络中的应用，通过36,610个储层配置的全面参数扫描，并引入量化激活函数的理论框架，定义退化回声状态特性(d-ESP)。

Result: 多个非光滑函数不仅保持回声状态特性，而且在收敛速度和谱半径容限方面优于传统光滑激活函数。康托函数能将ESP一致性行为保持到谱半径ρ~10，比光滑函数的典型界限高一个数量级，收敛速度比tanh和ReLU快2.6倍。

Conclusion: 预处理拓扑而非连续性本身决定稳定性：单调、压缩的预处理能在不同尺度上保持ESP，而分散或不连续的预处理会引发急剧失效。虽然研究结果挑战了储层计算中激活函数设计的假设，但某些分形函数优异性能的机制仍无法解释，表明对激活函数几何特性如何影响储层动态的理解存在根本性空白。

Abstract: Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.

</details>


### [104] [Early Warning Index for Patient Deteriorations in Hospitals](https://arxiv.org/abs/2512.14683)
*Dimitris Bertsimas,Yu Ma,Kimberly Villalobos Carballo,Gagan Singh,Michal Laskowski,Jeff Mather,Dan Kombert,Howard Haronian*

Main category: cs.LG

TL;DR: 开发了一个多模态机器学习框架EWI，通过整合临床和运营数据预测ICU入院、急救团队派遣和死亡风险，并在医院仪表板中部署为分诊工具。


<details>
  <summary>Details</summary>
Motivation: 医院缺乏自动化系统来利用日益增长的异构临床和运营数据有效预测关键事件。早期识别有恶化风险的患者对患者护理质量监测和医生护理管理都至关重要，但将不同数据流转化为准确且可解释的风险评估面临数据格式不一致的挑战。

Method: 开发了多模态机器学习框架EWI，采用人机协同流程：临床医生帮助确定警报阈值和解释模型输出，使用SHAP可解释性方法突出显示驱动每个患者风险的临床和运营因素。从结构化和非结构化电子健康记录数据中自动提取特征。

Result: 在美国一家大型医院的18,633名独特患者数据集上，EWI实现了C统计量0.796。目前作为分诊工具用于主动管理高风险患者，将患者分为三个风险等级。

Conclusion: 该方法通过自动对不同风险水平的患者进行分类，为医生节省了宝贵时间，使他们能够专注于患者护理。通过进一步确定特定的风险驱动因素，为护理人员调度和关键资源分配提供数据支持的调整，从而避免下游并发症并改善整体患者流程。

Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [105] [One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing](https://arxiv.org/abs/2512.13892)
*Albert Dorador*

Main category: stat.ML

TL;DR: 提出一种基于确定性最优排列的特征重要性评估方法，替代传统的随机排列，提高计算效率和稳定性，并引入系统性变量重要性用于模型压力测试。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的特征贡献评估对于信任、透明度和监管合规至关重要，尤其是在黑盒模型场景下。传统排列方法依赖重复随机排列，存在计算开销大和随机不稳定的问题。

Method: 用单一确定性最优排列替代多次随机排列，保留排列重要性核心原理的同时实现非随机、更快、更稳定的评估。扩展提出系统性变量重要性，考虑特征相关性进行模型压力测试。

Result: 在近200个场景（包括真实家庭金融和信用风险应用）中验证，在样本量小、高维、低信噪比等挑战性场景下表现出改进的偏差-方差权衡和准确性。通过两个真实案例展示了系统性变量重要性在审计模型对受保护属性依赖方面的应用。

Conclusion: 确定性排列方法为特征重要性评估提供了更高效稳定的解决方案，系统性变量重要性框架为模型公平性和系统性风险评估提供了原则性且计算高效的方法，特别适用于监管合规和模型审计。

Abstract: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.

</details>


### [106] [Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics](https://arxiv.org/abs/2512.13997)
*Aaron Wei,Milad Jalali,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 该论文解决了传统MMD双样本检验中要求等样本量的限制，通过推广U统计量理论，提出了适用于不等样本量的MMD检验方法，保留了所有数据以提高检验功效。


<details>
  <summary>Details</summary>
Motivation: 现有基于最大均值差异（MMD）的双样本检验方法通常假设两个分布具有相等的样本量，这在实际应用中需要丢弃有价值的数据，降低了检验功效。论文旨在解决这一长期存在的限制。

Method: 通过扩展广义U统计量理论，将其应用于常规MMD估计量，获得了不等样本量下MMD估计量的渐近分布新特征（特别是在先前部分结果要求的比例范围之外）。该推广还提供了优化不等样本量MMD检验功效的新准则。

Result: 提出的方法保留了所有可用数据，增强了检验准确性和在实际场景中的适用性。同时给出了MMD估计量方差的更清晰特征，揭示了即使在非零MMD情况下也可能出现退化估计量的可能性（尽管在常见情况下不会发生）。

Conclusion: 该研究解决了MMD双样本检验中不等样本量的长期限制，通过理论扩展提供了更实用的检验方法，同时深化了对MMD估计量统计特性的理解。

Abstract: Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.

</details>


### [107] [On the Hardness of Conditional Independence Testing In Practice](https://arxiv.org/abs/2512.14000)
*Zheng He,Roman Pogodin,Yazhe Li,Namrata Deka,Arthur Gretton,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 本文分析了条件独立性检验（KCI测试）的实际表现，发现条件均值嵌入估计误差对Type-I错误有重要影响，而条件核的选择对检验功效至关重要但会膨胀Type-I错误。


<details>
  <summary>Details</summary>
Motivation: 条件独立性检验在机器学习和统计学中至关重要，但Shah和Peters（2020）证明不存在普遍有限样本有效的检验。然而，这一理论结果似乎不能解释实践中常见CI检验的失败现象，因此需要深入分析具体检验方法（如KCI）的实际表现因素。

Method: 研究基于核的条件独立性（KCI）检验方法，分析其实际行为的主要影响因素。特别关注条件均值嵌入估计误差对Type-I错误的作用，以及条件核选择对检验功效和Type-I错误的影响。指出广义协方差度量（GCM）是KCI检验的一个特例。

Result: 识别出KCI检验实际表现的两个关键因素：1）条件均值嵌入估计误差是Type-I错误的主要来源；2）条件核的选择对检验功效至关重要，但会倾向于膨胀Type-I错误。这些发现解释了实践中CI检验失败的原因。

Conclusion: 条件独立性检验的实际失败不仅源于理论上的不可能性，更与具体实现中的技术因素密切相关。条件均值嵌入估计误差和条件核选择是影响KCI检验性能的关键因素，这为改进实际CI检验提供了重要见解。

Abstract: Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on "hiding" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.

</details>


### [108] [Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms](https://arxiv.org/abs/2512.14221)
*Jiarong Fan,Juhyun Park. Thi Phuong Thuy Vo,Nicolas Brunel*

Main category: stat.ML

TL;DR: 提出一种处理缺失协变量的保形预测方法，通过预插补-掩码-校正框架，在保证边际覆盖的同时实现掩码条件有效性，显著减少预测区间宽度。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测在缺失协变量时无法保证覆盖性，而不同缺失模式导致的异质性使得掩码条件覆盖比边际覆盖更理想。需要开发能处理缺失值并保证覆盖性的方法。

Method: 提出预插补-掩码-校正框架：1) 对校准数据集进行分布插补（多重插补）；2) 使用重加权保形预测程序校正预测集。该方法与标准插补流程兼容，并推导出两种算法。

Result: 方法在合成和真实数据集上评估，相比标准掩码条件有效方法，在保持目标保证的同时显著减少了预测区间宽度。证明了方法的近似边际有效性和掩码条件有效性。

Conclusion: 该方法为处理缺失协变量的不确定性量化提供了有效解决方案，通过创新的预插补-掩码-校正框架，在保证覆盖性的同时提高了预测效率。

Abstract: Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.

</details>


### [109] [Improving the Accuracy of Amortized Model Comparison with Self-Consistency](https://arxiv.org/abs/2512.14308)
*Šimon Kucharský,Aayush Mishra,Daniel Habermann,Stefan T. Radev,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: SC训练能提高摊销贝叶斯模型比较的鲁棒性，特别是在模型错误设定情况下，基于参数后验的方法优于直接近似模型证据的方法。


<details>
  <summary>Details</summary>
Motivation: 摊销贝叶斯推断(ABI)在模型错误设定时表现不稳定，特别是在模型比较场景中，当观测数据超出训练分布时，神经代理可能产生不可预测的行为。需要解决ABI在模型错误设定下的鲁棒性问题。

Method: 研究自洽性(SC)如何改进四种不同概念的摊销模型比较方法，通过两个合成和两个真实世界案例研究，比较基于参数后验估计边际似然的方法与直接近似模型证据的方法。

Result: 基于参数后验估计边际似然的方法始终优于直接近似模型证据或后验模型概率的方法。当似然函数可用时，SC训练即使在严重模型错误设定下也能提高鲁棒性；但对于无法访问解析似然的方法，SC的好处有限且不一致。

Conclusion: 建议实用的摊销贝叶斯模型比较指导：优先选择基于参数后验的方法，并在经验数据集上使用SC训练来减轻模型错误设定下的外推偏差。

Abstract: Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.

</details>


### [110] [Continual Learning at the Edge: An Agnostic IIoT Architecture](https://arxiv.org/abs/2512.14311)
*Pablo García-Santaclara,Bruno Fernández-Castro,Rebeca P. Díaz-Redondo,Carlos Calvo-Moa,Henar Mariño-Bodelón*

Main category: stat.ML

TL;DR: 提出一种在边缘计算场景中应用增量学习的方法，用于工业制造系统的实时质量控制，解决传统机器学习在动态数据环境中的不适应问题。


<details>
  <summary>Details</summary>
Motivation: 互联网设备指数增长给传统集中式计算系统带来延迟和带宽限制挑战，边缘计算虽能解决部分问题，但传统机器学习算法不适合边缘计算系统中动态连续到达的数据特性。

Method: 将增量学习理念应用于工业边缘计算场景，采用持续学习方法减少灾难性遗忘的影响，为制造系统提供实时质量控制解决方案。

Result: 该方法为边缘计算环境下的工业应用提供了高效有效的解决方案，能够处理动态连续数据并保持模型性能。

Conclusion: 在边缘计算场景中应用增量学习是解决工业制造实时质量控制的有效途径，能够应对动态数据环境并减少灾难性遗忘问题。

Abstract: The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.

</details>


### [111] [From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification](https://arxiv.org/abs/2512.14404)
*Hangjun Cho,Fabio V. G. Amaral,Andrei A. Klishin,Cassio M. Oishi,Steven L. Brunton*

Main category: stat.ML

TL;DR: 论文提出了一种基于分数的字典选择方法，用于改进SINDy类型算法中的稀疏回归，通过理论分析和数值实验验证了该方法在动力系统识别中的有效性。


<details>
  <summary>Details</summary>
Motivation: 针对字典基稀疏回归（特别是STLS算法）在实际应用中缺乏实用指导的问题，作者希望提供一种基于分数的字典选择策略，以改进SINDy类型算法的数据驱动建模能力。

Method: 提出了分数引导的字典选择方法，该方法利用投影重构误差（称为分数）和字典项之间的互相关性来指导字典选择。首先对分数和字典选择策略进行理论分析，然后在原始和弱SINDy框架下验证，最后通过常微分方程和偏微分方程的数值实验进行验证。

Result: 数值实验表明，基于分数的筛选方法能够有效提高动力系统识别的准确性和可解释性，特别是在某些情况下能够增强数据驱动发现控制方程的鲁棒性。

Conclusion: 分数引导的字典选择方法为SINDy用户提供了实用的指导，能够通过更精确地细化字典来增强数据驱动建模的鲁棒性和准确性，特别是在动力系统识别应用中。

Abstract: In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the $\ell_0$ sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations.

</details>


### [112] [LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts](https://arxiv.org/abs/2512.14604)
*Prasanjit Dubey,Aritra Guha,Zhengyi Zhou,Qiong Wu,Xiaoming Huo,Paromita Dubey*

Main category: stat.ML

TL;DR: 提出LLmFPCA-detect框架，结合LLM文本嵌入与功能数据分析，用于稀疏纵向文本数据的聚类和异常检测


<details>
  <summary>Details</summary>
Motivation: 稀疏纵向文本数据（如客户评论、社交媒体帖子、电子病历）具有巨大潜力但缺乏专门方法，数据噪声大、异质性强且易含异常，检测关键模式具有挑战性

Method: 1) 使用LLM提示将文本嵌入到应用特定的数值空间；2) 在数值空间中进行稀疏多元功能主成分分析(mFPCA)恢复总体特征；3) 结合静态协变量进行数据分割、无监督异常检测和推断；4) 利用LLM进行动态关键词分析

Result: 在亚马逊客户评论轨迹和维基百科讨论页评论流两个公开数据集上验证，跨领域实用性强，优于现有最先进基线方法，并能提升现有预测管道的性能

Conclusion: LLmFPCA-detect为稀疏纵向文本数据分析提供了灵活框架，结合LLM和功能数据分析的优势，有效解决聚类、异常检测和模式推断问题，具有实际应用价值

Abstract: Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.

</details>
