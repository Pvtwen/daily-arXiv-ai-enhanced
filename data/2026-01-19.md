<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 22]
- [cs.LG](#cs.LG) [Total: 60]
- [stat.ML](#stat.ML) [Total: 4]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Zonotope Shadow and Reflection Matching: A Novel GNSS Reflection-Based Framework for Enhanced Positioning Accuracy in Urban Areas](https://arxiv.org/abs/2601.10727)
*Sanghyun Kim,Jiwon Seo*

Main category: eess.SP

TL;DR: 提出ZSRM方法，结合GNSS阴影和反射匹配，提高城市定位精度，相比ZSM改进10-53%


<details>
  <summary>Details</summary>
Motivation: 城市环境中GNSS信号受建筑物反射影响导致定位不准确，传统阴影匹配采用离散网格估计，精度受网格分辨率限制，难以满足用户指定的保护等级要求

Method: 提出ZSRM方法，结合阴影匹配和反射匹配，使用基于集合的位置估计而非网格估计，通过区分LOS和NLOS卫星确定接收机位置集合

Result: ZSRM相比ZSM在RMS水平位置误差改进10.0%到53.6%，横街和沿街位置边界分别改进18.0%到50.1%和30.7%到59.3%

Conclusion: ZSRM方法能有效提高城市环境中的GNSS定位精度，通过结合阴影和反射信息，比单纯阴影匹配提供更精确的位置估计

Abstract: In urban areas, signal reception conditions are often poor due to reflections from buildings, resulting in inaccurate global navigation satellite system (GNSS)-based positioning. Various 3D-mapping-aided (3DMA) GNSS techniques, including shadow matching, have been proposed to address this issue. However, conventional shadow matching estimates positions in a discretized manner. The accuracy of this approach is limited by the resolution of the grid points representing the candidate receiver positions, making it difficult to achieve robust urban positioning and to ensure that the position estimate satisfies user-specified protection levels or safety bounds. To overcome these limitations, zonotope shadow matching (ZSM) has been proposed, which utilizes a set-based position estimate rather than grid-based estimates. ZSM calculates the GNSS shadow--an area on the ground where the line-of-sight (LOS) is blocked and only non-line-of-sight (NLOS) signals can be received--to estimate the receiver's position set. ZSM distinguishes between LOS and NLOS satellites, determining that the receiver is inside the GNSS shadow if the satellite is NLOS and outside if the satellite is LOS. However, relying solely on GNSS shadows limits the ability to sufficiently reduce the size of the receiver position set and to precisely estimate the receiver's location. To address this, we propose zonotope shadow and reflection matching (ZSRM) to enhance positioning accuracy in urban areas. The proposed ZSRM technique is validated through field tests using GNSS signals collected in an urban environment. Consequently, the RMS horizontal position error of ZSRM improved by 10.0% to 53.6% compared with ZSM, while the RMS cross-street and along-street position bounds improved by 18.0% to 50.1% and 30.7% to 59.3%, respectively.

</details>


### [2] [Millimeter-Wave Gesture Recognition in ISAC: Does Reducing Sensing Airtime Hamper Accuracy?](https://arxiv.org/abs/2601.10733)
*Jakob Struye,Nabeel Nisar Bhat,Siddhartha Kumar,Mohammad Hossein Moghaddam,Jeroen Famaey*

Main category: eess.SP

TL;DR: 毫米波ISAC系统中，仅用25%的感知时间就能实现接近全时感知的手势识别准确率


<details>
  <summary>Details</summary>
Motivation: 现有ISAC系统需要在感知和通信之间分配时间资源，但这种时间分配决策对感知性能的具体影响尚不明确，需要深入研究

Method: 使用两个毫米波设备进行恒定波束扫描，收集测试对象执行不同手势时的波束对功率数据，构建数据集；然后训练卷积神经网络手势分类器；通过子采样测量数据来模拟减少感知时间的效果

Result: 仅使用25%的感知时间，分类准确率仅比全时感知下降0.15个百分点，表明毫米波ISAC系统能够在低感知时间下实现高质量的感知性能

Conclusion: 毫米波ISAC系统既能提供极高的数据吞吐量，又能在低感知时间下保持高质量的感知能力，是真正无线扩展现实等应用的理想使能技术

Abstract: Most Integrated Sensing and Communications (ISAC) systems require dividing airtime across their two modes. However, the specific impact of this decision on sensing performance remains unclear and underexplored. In this paper, we therefore investigate the impact on a gesture recognition system using a Millimeter-Wave (mmWave) ISAC system. With our dataset of power per beam pair gathered with two mmWave devices performing constant beam sweeps while test subjects performed distinct gestures, we train a gesture classifier using Convolutional Neural Networks. We then subsample these measurements, emulating reduced sensing airtime, showing that a sensing airtime of 25 % only reduces classification accuracy by 0.15 percentage points from full-time sensing. Alongside this high-quality sensing at low airtime, mmWave systems are known to provide extremely high data throughputs, making mmWave ISAC a prime enabler for applications such as truly wireless Extended Reality.

</details>


### [3] [On the static and small signal analysis of DAB converter](https://arxiv.org/abs/2601.10746)
*Yuxin Yang,Hang Zhou,Hourong Song,Branislav Hredzak*

Main category: eess.SP

TL;DR: 提出了一种求解双有源桥(DAB)周期性工作点的方法


<details>
  <summary>Details</summary>
Motivation: 双有源桥(DAB)在电力电子转换器中广泛应用，但准确求解其周期性工作点对于系统设计和控制至关重要。现有方法可能存在计算复杂或精度不足的问题。

Method: 开发了一种系统性的数学方法来求解DAB的周期性工作点，可能涉及状态空间方程、谐波分析或数值求解技术，以确定系统在稳态下的周期性运行状态。

Result: 该方法能够有效求解DAB的周期性工作点，为系统分析和设计提供了可靠的工具，可能提高了计算效率或精度。

Conclusion: 提出的方法为双有源桥的周期性工作点求解提供了有效的解决方案，有助于DAB系统的优化设计和控制策略开发。

Abstract: This document develops a method to solve the periodic operating point of Dual-Active-Bridge (DAB).

</details>


### [4] [SSC-UNet: UNet with Self-Supervised Contrastive Learning for Phonocardiography Noise Reduction](https://arxiv.org/abs/2601.10735)
*Lizy Abraham,Siobhan Coughlan,Kritika Rajain,Changhong Li,Saji Philip,Adam James*

Main category: eess.SP

TL;DR: 提出基于Noise2Noise的自监督心音图降噪模型，无需干净数据训练，在10dB医院噪声下平均SNR达12.98dB，分类灵敏度从27%提升至88%


<details>
  <summary>Details</summary>
Motivation: 先天性心脏病(CHD)影响全球约1%新生儿，心音图是经济有效的辅助诊断工具。但诊断模型性能依赖心音图质量，降噪尤为关键。监督UNet虽能提升降噪能力，但干净数据有限限制了应用。心音图复杂的时频特性使平衡噪声去除和病理特征保留变得困难。

Method: 提出基于Noise2Noise的自监督心音图降噪模型，无需干净数据训练。采用数据增强和对比学习提升性能。

Result: 在10dB医院噪声环境下，滤波后平均信噪比(SNR)达到12.98dB。分类灵敏度从27%提升至88%，表明在实际噪声环境中具有良好的病理特征保留能力。

Conclusion: 提出的自监督降噪模型有效解决了心音图降噪中干净数据稀缺的问题，在保持病理特征的同时显著提升了噪声抑制效果，为先天性心脏病诊断提供了实用的辅助工具。

Abstract: Congenital Heart Disease (CHD) remains a significant global health concern affecting approximately 1\% of births worldwide. Phonocardiography has emerged as a supplementary tool to diagnose CHD cost-effectively. However, the performance of these diagnostic models highly depends on the quality of the phonocardiography, thus, noise reduction is particularly critical. Supervised UNet effectively improves noise reduction capabilities, but limited clean data hinders its application. The complex time-frequency characteristics of phonocardiography further complicate finding the balance between effectively removing noise and preserving pathological features. In this study, we proposed a self-supervised phonocardiography noise reduction model based on Noise2Noise to enable training without clean data. Augmentation and contrastive learning are applied to enhance its performance. We obtained an average SNR of 12.98 dB after filtering under 10~dB of hospital noise. Classification sensitivity after filtering was improved from 27\% to 88\%, indicating its promising pathological feature retention capabilities in practical noisy environments.

</details>


### [5] [Differentiating through binarized topology changes: Second-order subpixel-smoothed projection](https://arxiv.org/abs/2601.10737)
*Giuseppe Romano,Rodrigo Arrieta,Steven G. Johnson*

Main category: eess.SP

TL;DR: 提出SSP2方法，通过Hessian正则化改进SSP，确保拓扑变化时的二阶可微性，提升收敛性并扩展优化算法选择范围。


<details>
  <summary>Details</summary>
Motivation: 拓扑优化中可制造结构本质上是二值的，与基于梯度的优化存在根本性矛盾。SSP方法通过亚像素平滑解决此问题，但在拓扑变化（如界面合并）时无法保证可微性，违反了许多梯度优化算法的收敛保证。

Method: 提出二阶SSP（SSP2）方法，通过对滤波场的Hessian进行正则化，在拓扑变化期间实现投影密度的二阶可微性，同时保证几乎处处二值结构。

Result: 在热学和光子学问题上验证了SSP2的有效性，显示在连接主导（频繁拓扑变化）的情况下，SSP2比SSP收敛更快，在其他情况下性能相当。SSP2还能支持更广泛的优化算法（如内点法）。

Conclusion: SSP2在SSP或传统投影方案基础上增加复杂度极小，可作为现有拓扑优化代码的直接替代方案，不仅改进了CCSA优化器的收敛保证，还扩展了具有更强理论保证的优化算法选择范围。

Abstract: A key challenge in topology optimization (TopOpt) is that manufacturable structures, being inherently binary, are non-differentiable, creating a fundamental tension with gradient-based optimization. The subpixel-smoothed projection (SSP) method addresses this issue by smoothing sharp interfaces at the subpixel level through a first-order expansion of the filtered field. However, SSP does not guarantee differentiability under topology changes, such as the merging of two interfaces, and therefore violates the convergence guarantees of many popular gradient-based optimization algorithms. We overcome this limitation by regularizing SSP with the Hessian of the filtered field, resulting in a twice-differentiable projected density during such transitions, while still guaranteeing an almost-everywhere binary structure. We demonstrate the effectiveness of our second-order SSP (SSP2) methodology on both thermal and photonic problems, showing that SSP2 has faster convergence than SSP for connectivity-dominant cases -- where frequent topology changes occur -- while exhibiting comparable performance otherwise. Beyond improving convergence guarantees for CCSA optimizers, SSP2 enables the use of a broader class of optimization algorithms with stronger theoretical guarantees, such as interior-point methods. Since SSP2 adds minimal complexity relative to SSP or traditional projection schemes, it can be used as a drop-in replacement in existing TopOpt codes.

</details>


### [6] [UBiGTLoc: A Unified BiLSTM-Graph Transformer Localization Framework for IoT Sensor Networks](https://arxiv.org/abs/2601.10743)
*Ayesh Abu Lehyeh,Anastassia Gharib,Tian Xia,Dryver Huston,Safwan Wshah*

Main category: eess.SP

TL;DR: 提出UBiGTLoc框架，结合双向LSTM和Graph Transformer，在有无锚节点情况下都能实现无线传感器网络节点定位，仅使用RSSI数据。


<details>
  <summary>Details</summary>
Motivation: 现有传感器节点定位方法严重依赖锚节点，但在实际IoT场景中锚节点可能不可行，且RSSI波动（特别是在NLOS条件下）会影响定位精度。

Method: 提出统一的双向LSTM-Graph Transformer定位框架（UBiGTLoc），利用BiLSTM捕获RSSI数据的时间变化，使用Graph Transformer层建模传感器节点间的空间关系。

Result: 大量仿真表明，UBiGTLoc始终优于现有方法，在密集和稀疏WSN中都能提供鲁棒的定位，仅依赖成本效益高的RSSI数据。

Conclusion: UBiGTLoc框架解决了锚节点依赖和RSSI波动问题，为无线IoT传感器网络提供了一种有效的节点定位解决方案。

Abstract: Sensor nodes localization in wireless Internet of Things (IoT) sensor networks is crucial for the effective operation of diverse applications, such as smart cities and smart agriculture. Existing sensor nodes localization approaches heavily rely on anchor nodes within wireless sensor networks (WSNs). Anchor nodes are sensor nodes equipped with global positioning system (GPS) receivers and thus, have known locations. These anchor nodes operate as references to localize other sensor nodes. However, the presence of anchor nodes may not always be feasible in real-world IoT scenarios. Additionally, localization accuracy can be compromised by fluctuations in Received Signal Strength Indicator (RSSI), particularly under non-line-of-sight (NLOS) conditions. To address these challenges, we propose UBiGTLoc, a Unified Bidirectional Long Short-Term Memory (BiLSTM)-Graph Transformer Localization framework. The proposed UBiGTLoc framework effectively localizes sensor nodes in both anchor-free and anchor-presence WSNs. The framework leverages BiLSTM networks to capture temporal variations in RSSI data and employs Graph Transformer layers to model spatial relationships between sensor nodes. Extensive simulations demonstrate that UBiGTLoc consistently outperforms existing methods and provides robust localization across both dense and sparse WSNs while relying solely on cost-effective RSSI data.

</details>


### [7] [An IoT-Based Controlled Environment Storage for Prevention of Spoilage of Onion (Allium Cepa) During Post-Harvest with UV-C Disinfection](https://arxiv.org/abs/2601.10745)
*Shivam Kumar,Himanshu Singh*

Main category: eess.SP

TL;DR: 本文提出了一种基于物联网的低成本智能洋葱存储系统，通过监测和自动调节温度、湿度及腐败气体，结合UV-C消毒技术，旨在将洋葱腐败率从40-45%降低到15-20%，同时保持对小农户的经济可及性。


<details>
  <summary>Details</summary>
Motivation: 印度作为世界第二大洋葱生产国，年产量超过2600万吨，但在储存过程中约有30-40%的洋葱因腐烂、发芽和失重而损失。传统存储方法要么成本低但效果差（传统存储有40%腐败率），要么效果好但成本过高（冷藏），对小农户来说难以承受。

Method: 开发了一个基于物联网的智能洋葱存储系统，使用ESP32微控制器、DHT22传感器监测温湿度、MQ-135气体传感器检测腐败气体，并集成UV-C消毒技术。系统自动调节环境参数，设计为太阳能供电、节能且用户友好。

Result: 系统旨在将洋葱腐败率从当前的40-45%降低到15-20%，同时保持成本效益（预计6-7万印度卢比），适合构成印度大多数的小农户和边际农户使用。

Conclusion: 该低成本物联网智能存储系统为解决印度洋葱储存损失问题提供了可行方案，平衡了存储效果和经济可及性，有助于减少粮食浪费并提高小农户的经济收益。

Abstract: India is the second largest producer of onions in the world, contributing over 26 million tonnes annually. However, during storage, approximately 30-40% of onions are lost due to rotting, sprouting, and weight loss. Despite being a major producer, conventional storage methods are either low-cost but ineffective (traditional storage with 40% spoilage) or highly effective but prohibitively expensive for small farmers (cold storage). This paper presents a low-cost IoT-based smart onion storage system that monitors and automatically regulates environmental parameters including temperature, humidity, and spoilage gases using ESP32 microcontroller, DHT22 sensor, MQ-135 gas sensor, and UV-C disinfection technology. The proposed system aims to reduce onion spoilage to 15-20% from the current 40-45% wastage rate while remaining affordable for small and marginal farmers who constitute the majority in India. The system is designed to be cost-effective (estimated 60k-70k INR), energy-efficient, farmer-friendly, and solar-powered.

</details>


### [8] [Sensor Placement for Urban Traffic Interpolation: A Data-Driven Evaluation to Inform Policy](https://arxiv.org/abs/2601.10747)
*Silke K. Kaiser*

Main category: eess.SP

TL;DR: 本研究通过柏林和曼哈顿的实际数据，系统比较了多种数据驱动的交通传感器布设策略，发现强调空间均匀覆盖和主动学习的空间布设策略能显著降低预测误差，结合时间均匀分布可让临时部署接近永久部署的性能。


<details>
  <summary>Details</summary>
Motivation: 城市街道交通流量数据对城市规划至关重要，但现有传感器布设通常基于行政优先级而非数据驱动优化，导致覆盖偏差和估计性能下降，需要研究更有效的传感器布设策略。

Method: 使用柏林（Strava自行车计数）和曼哈顿（出租车计数）的街道段级数据，大规模实景基准测试多种数据驱动的传感器布设策略：基于网络中心性、空间覆盖、特征覆盖和主动学习的空间布设策略，以及临时传感器的时间部署方案。

Result: 强调空间均匀覆盖和采用主动学习的空间布设策略表现最佳，仅用10个传感器就能在柏林降低60%以上、曼哈顿降低70%以上的平均绝对误差；时间均匀分布测量进一步降低误差（柏林7%、曼哈顿21%），使临时部署接近最优永久部署的性能。

Conclusion: 城市可通过采用数据驱动的传感器布设策略显著提高数据实用性，同时在临时和永久部署之间保持灵活性，空间均匀覆盖和主动学习是关键原则，时间均匀分布能进一步提升性能。

Abstract: Data on citywide street-segment traffic volumes are essential for urban planning and sustainable mobility management. Yet such data are available only for a limited subset of streets due to the high costs of sensor deployment and maintenance. Traffic volumes on the remaining network are therefore interpolated based on existing sensor measurements. However, current sensor locations are often determined by administrative priorities rather than by data-driven optimization, leading to biased coverage and reduced estimation performance. This study provides a large-scale, real-world benchmarking of easily implementable, data-driven strategies for optimizing the placement of permanent and temporary traffic sensors, using segment-level data from Berlin (Strava bicycle counts) and Manhattan (taxi counts). It compares spatial placement strategies based on network centrality, spatial coverage, feature coverage, and active learning. In addition, the study examines temporal deployment schemes for temporary sensors. The findings highlight that spatial placement strategies that emphasize even spatial coverage and employ active learning achieve the lowest prediction errors. With only 10 sensors, they reduce the mean absolute error by over 60% in Berlin and 70% in Manhattan compared to alternatives. Temporal deployment choices further improve performance: distributing measurements evenly across weekdays reduces error by an additional 7% in Berlin and 21% in Manhattan. Together, these spatial and temporal principles allow temporary deployments to closely approximate the performance of optimally placed permanent deployments. From a policy perspective, the results indicate that cities can substantially improve data usefulness by adopting data-driven sensor placement strategies, while retaining flexibility in choosing between temporary and permanent deployments.

</details>


### [9] [AnyECG: Evolved ECG Foundation Model for Holistic Health Profiling](https://arxiv.org/abs/2601.10748)
*Jun Li,Hongling Zhu,Yujie Xiao,Qinghao Zhao,Yalei Ke,Gongzheng Tang,Guangkun Nie,Deyun Zhang,Jin Li,Canqing Yu,Shenda Hong*

Main category: eess.SP

TL;DR: AnyECG是基于ECGFounder微调的心电图基础模型，利用1330万份心电图数据，可同时检测1172种疾病，实现当前诊断、未来风险预测和共病识别。


<details>
  <summary>Details</summary>
Motivation: 现有AI-ECG模型大多专注于单一疾病识别，忽略了共病和未来风险预测。虽然ECGFounder扩展了心脏疾病覆盖范围，但仍需要一个全面的健康分析模型。

Method: 构建包含1330万份心电图的多中心数据集，使用迁移学习对ECGFounder进行微调，开发AnyECG基础模型。通过外部验证队列和10年纵向队列评估性能。

Result: AnyECG在1172种疾病中表现出系统性预测能力，306种疾病的AUROC超过0.7。模型揭示了新的疾病关联、稳健的共病模式和未来疾病风险，例如甲状旁腺功能亢进症(AUROC 0.941)、2型糖尿病(0.803)等。

Conclusion: AnyECG基础模型提供了有力证据，表明AI-ECG可以作为同时进行疾病检测和长期风险预测的系统性工具。

Abstract: Background: Artificial intelligence enabled electrocardiography (AI-ECG) has demonstrated the ability to detect diverse pathologies, but most existing models focus on single disease identification, neglecting comorbidities and future risk prediction. Although ECGFounder expanded cardiac disease coverage, a holistic health profiling model remains needed.
  Methods: We constructed a large multicenter dataset comprising 13.3 million ECGs from 2.98 million patients. Using transfer learning, ECGFounder was fine-tuned to develop AnyECG, a foundation model for holistic health profiling. Performance was evaluated using external validation cohorts and a 10-year longitudinal cohort for current diagnosis, future risk prediction, and comorbidity identification.
  Results: AnyECG demonstrated systemic predictive capability across 1172 conditions, achieving an AUROC greater than 0.7 for 306 diseases. The model revealed novel disease associations, robust comorbidity patterns, and future disease risks. Representative examples included high diagnostic performance for hyperparathyroidism (AUROC 0.941), type 2 diabetes (0.803), Crohn disease (0.817), lymphoid leukemia (0.856), and chronic obstructive pulmonary disease (0.773).
  Conclusion: The AnyECG foundation model provides substantial evidence that AI-ECG can serve as a systemic tool for concurrent disease detection and long-term risk prediction.

</details>


### [10] [LSR-Net: A Lightweight and Strong Robustness Network for Bearing Fault Diagnosis in Noise Environment](https://arxiv.org/abs/2601.10761)
*Junseok Lee,Jihye Shin,Sangyong Lee,Chang-Jae Chun*

Main category: eess.SP

TL;DR: 提出LSR-Net轻量鲁棒网络，用于轴承故障诊断，在噪声环境下保持高精度并实现实时诊断


<details>
  <summary>Details</summary>
Motivation: 旋转轴承在恶劣环境下易发生故障，传统诊断方法在噪声环境下准确性不足，且实时性要求高，需要轻量而鲁棒的诊断方案

Method: 设计了去噪特征增强模块(DFEM)和卷积效率洗牌(CES)块。DFEM使用卷积去噪块和自适应剪枝增强抗噪能力；CES采用分组卷积、分组点卷积和通道分割实现轻量化，结合注意力机制和通道洗牌平衡精度与复杂度

Result: 在噪声环境下验证，LSR-Net相比基准模型具有最佳的抗噪能力，同时模型计算复杂度最低

Conclusion: LSR-Net在噪声环境下实现了高精度的实时轴承故障诊断，平衡了模型轻量化和鲁棒性，具有实际应用价值

Abstract: Rotating bearings play an important role in modern industries, but have a high probability of occurrence of defects because they operate at high speed, high load, and poor operating environments. Therefore, if a delay time occurs when a bearing is diagnosed with a defect, this may cause economic loss and loss of life. Moreover, since the vibration sensor from which the signal is collected is highly affected by the operating environment and surrounding noise, accurate defect diagnosis in a noisy environment is also important. In this paper, we propose a lightweight and strong robustness network (LSR-Net) that is accurate in a noisy environment and enables real-time fault diagnosis. To this end, first, a denoising and feature enhancement module (DFEM) was designed to create a 3-channel 2D matrix by giving several nonlinearity to the feature-map that passed through the denoising module (DM) block composed of convolution-based denoising (CD) blocks. Moreover, adaptive pruning was applied to DM to improve denoising ability when the power of noise is strong. Second, for lightweight model design, a convolution-based efficiency shuffle (CES) block was designed using group convolution (GConv), group pointwise convolution (GPConv) and channel split that can design the model while maintaining low parameters. In addition, the trade-off between the accuracy and model computational complexity that can occur due to the lightweight design of the model was supplemented using attention mechanisms and channel shuffle. In order to verify the defect diagnosis performance of the proposed model, performance verification was conducted in a noisy environment using a vibration signal. As a result, it was confirmed that the proposed model had the best anti-noise ability compared to the benchmark models, and the computational complexity of the model was also the lowest.

</details>


### [11] [Physically constrained unfolded multi-dimensional OMP for large MIMO systems](https://arxiv.org/abs/2601.10771)
*Nay Klaimi,Clément Elvira,Philippe Mary,Luc Le Magoarou*

Main category: eess.SP

TL;DR: MOMPnet是一个基于深度展开的稀疏恢复框架，通过数据驱动的字典学习和多字典策略，解决传统方法在信道估计和定位中的模型不准确和高计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏恢复方法依赖准确的物理模型（现实中很难获得），且在大型MIMO系统中计算复杂度随字典维度急剧增加，存在可靠性和计算效率的双重挑战。

Method: 结合深度展开与数据驱动的字典学习，使用多个独立的小字典替代单个大字典，实现低复杂度的多维正交匹配追踪算法。

Result: 在真实信道数据上评估，相比多个基线方法，MOMPnet表现出强大的性能和潜力。

Conclusion: MOMPnet通过深度展开框架有效解决了传统稀疏恢复方法的可靠性和复杂度问题，在保持可解释性的同时提高了性能。

Abstract: Sparse recovery methods are essential for channel estimation and localization in modern communication systems, but their reliability relies on accurate physical models, which are rarely perfectly known. Their computational complexity also grows rapidly with the dictionary dimensions in large MIMO systems. In this paper, we propose MOMPnet, a novel unfolded sparse recovery framework that addresses both the reliability and complexity challenges of traditional methods. By integrating deep unfolding with data-driven dictionary learning, MOMPnet mitigates hardware impairments while preserving interpretability. Instead of a single large dictionary, multiple smaller, independent dictionaries are employed, enabling a low-complexity multidimensional Orthogonal Matching Pursuit algorithm. The proposed unfolded network is evaluated on realistic channel data against multiple baselines, demonstrating its strong performance and potential.

</details>


### [12] [Adaptive algorithm for microsensor in sustainable environmental monitoring](https://arxiv.org/abs/2601.10780)
*Nursultan Daupayev,Christian Engel,Ricky Bendyk,Soeren Hirsch*

Main category: eess.SP

TL;DR: 提出基于傅里叶变换的数据采集处理算法，通过谐波分析提取主导频率成分，实现事件触发式传感器激活，减少功耗和存储需求


<details>
  <summary>Details</summary>
Motivation: 传统传感器数据采集产生大量数据，导致持续功耗和存储空间需求增加，需要更高效的数据采集方法

Method: 基于离散傅里叶变换(DFT)的数据采集处理算法，利用谐波分析提取主导频率成分，识别频率峰值，实现事件触发式传感器激活

Result: 算法使传感器仅在事件发生时激活，同时保留检测缺陷所需的关键信息，如建筑表面结构缺陷，确保预测准确性

Conclusion: 提出的算法能有效减少传感器功耗和存储需求，同时保持对建筑表面结构等缺陷检测的准确性

Abstract: Traditional data collection from sensors produce a lot of data, which lead to constant power consumption and require more storage space. This study proposes an algorithm for a data acquisition and processing method based on Fourier transform (DFT), which extracts dominant frequency components using harmonic analysis (HA) to identify frequency peaks. This algorithm allows sensors to activate only when an event occurs, while preserving critical information for detecting defects, such as those in the surface structures of buildings and ensuring accuracy for further predictions.

</details>


### [13] [RIS-aided Radar Detection Architectures with Application to Low-RCS Targets](https://arxiv.org/abs/2601.10846)
*Fabiola Colone,Filippo Costa,Yiding Gao,Chengpeng Hao,Linjie Yan,Giuliano Manara,Danilo Orlando*

Main category: eess.SP

TL;DR: 利用可重构智能表面辅助雷达检测低可观测目标，通过联合处理单站和双站回波提高检测性能


<details>
  <summary>Details</summary>
Motivation: 传统多基地雷达网络存在同步、成本、相位相干性和能耗等问题，需要更有效的反隐身策略来检测低可观测目标

Method: 使用可重构智能表面形成联合单站和双站配置，截获目标在不同方向散射的能量并重定向回雷达，设计了五种具有恒虚警率特性的联合检测架构

Result: 与传统检测器相比，所提策略能有效检测低可观测目标，并为RIS的实际实现提供了设计指南

Conclusion: 利用可重构智能表面辅助的雷达检测策略为解决低可观测目标检测问题提供了有效方案

Abstract: In this paper, we address the radar detection of low observable targets with the assistance of a reconfigurable intelligent surface (RIS). Instead of using a multistatic radar network as counter-stealth strategy with its synchronization, costs, phase coherence, and energy consumption issues, we exploit a RIS to form a joint monostatic and bistatic configuration that can intercept the energy backscattered by the target along irrelevant directions different from the line-of-sight of the radar. Then, this energy is redirected towards the radar that capitalizes all the backscattered energy to detect the low observable target. To this end, five different detection architectures are devised that jointly process monostatic and bistatic echoes and exhibit the constant false alarm rate property at least with respect to the clutter power. To support the practical implementation, we also provide a guideline for the design of a RIS that satisfies the operating requirements of the considered application. The performance analysis is carried out in comparison with conventional detectors and shows that the proposed strategy leads to effective solutions to the detection of low observable targets.

</details>


### [14] [Large Wireless Foundation Models: Stronger over Bigger](https://arxiv.org/abs/2601.10963)
*Xiang Cheng,Boxun Liu,Xuanyu Liu,Xuesong Cai*

Main category: eess.SP

TL;DR: 该论文提出大型无线基础模型（LWFMs）的概念和框架，旨在解决现有AI通信系统泛化能力差的问题，通过两种范式实现无线约束下的基础模型应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的物理层设计采用任务特定模型，泛化能力差；而通信系统本质上是通用系统，需要支持广泛场景。基础模型具有强大的推理和泛化能力，但无线系统约束阻碍了大型语言模型（LLM）式成功直接迁移到无线领域。

Method: 提出大型无线基础模型（LWFMs）概念，并设计两种实现范式：1）利用现有通用基础模型；2）构建新型无线基础模型。提炼每种范式的路线图，在无线约束下制定设计原则，通过案例研究验证优势，并通过多维分析定义LWFMs中的"大型"概念。

Result: 提出了完整的LWFMs框架，包括两种实现范式、设计原则和路线图。通过案例研究直观验证了LWFM赋能的无线系统优势，并通过多维分析明确了"大型"在无线基础模型中的含义。

Conclusion: 大型无线基础模型为解决6G中AI通信集成的泛化问题提供了有前景的解决方案，通过提出的框架和范式可以在无线约束下实现基础模型的成功应用，并为未来研究指明了方向。

Abstract: AI-communication integration is widely regarded as a core enabling technology for 6G. Most existing AI-based physical-layer designs rely on task-specific models that are separately tailored to individual modules, resulting in poor generalization. In contrast, communication systems are inherently general-purpose and should support broad applicability and robustness across diverse scenarios. Foundation models offer a promising solution through strong reasoning and generalization, yet wireless-system constraints hinder a direct transfer of large language model (LLM)-style success to the wireless domain. Therefore, we introduce the concept of large wireless foundation models (LWFMs) and present a novel framework for empowering the physical layer with foundation models under wireless constraints. Specifically, we propose two paradigms for realizing LWFMs, including leveraging existing general-purpose foundation models and building novel wireless foundation models. Based on recent progress, we distill two roadmaps for each paradigm and formulate design principles under wireless constraints. We further provide case studies of LWFM-empowered wireless systems to intuitively validate their advantages. Finally, we characterize the notion of "large" in LWFMs through a multidimensional analysis of existing work and outline promising directions for future research.

</details>


### [15] [DuTrack: Long-Term Indoor Human Tracking with Dual-Channel Sensing and Inference](https://arxiv.org/abs/2601.10972)
*Mengning Li,Wenye Wang*

Main category: eess.SP

TL;DR: DuTrack：一种融合Wi-Fi和声学传感的多模态人体追踪系统，通过声学信号校正Wi-Fi累积误差，实现稳定长期追踪


<details>
  <summary>Details</summary>
Motivation: 当前基于速度特征的Wi-Fi追踪方法存在累积误差问题，难以实现长期稳定的人体轨迹追踪。需要一种能够校正Wi-Fi累积误差的稳定追踪方案。

Method: 提出融合Wi-Fi和声学传感的多模态追踪系统。将Wi-Fi在视距和非视距场景分别建模为椭圆菲涅尔区和双曲线区，设计声学传感信号将其建模为双曲线簇，建立电磁波与机械波融合的优化方程，并设计数据驱动架构求解该方程。

Result: 实验结果显示，DuTrack相比基于模型的方法将中位追踪误差降低了89.37%，相比数据驱动方法降低了65.02%，表现出优越的追踪性能。

Conclusion: DuTrack通过融合Wi-Fi和声学传感，有效解决了Wi-Fi追踪的累积误差问题，实现了稳定的人体长期追踪，为智能家居和家庭护理应用提供了可靠的技术方案。

Abstract: Wi-Fi tracking technology demonstrates promising potential for future smart home and intelligent family care. Currently, accurate Wi-Fi tracking methods rely primarily on fine-grained velocity features. However, such velocity-based approaches suffer from the problem of accumulative errors, making it challenging to stably track users' trajectories over a long period of time. This paper presents DuTrack, a fusion-based tracking system for stable human tracking. The fundamental idea is to leverage the ubiquitous acoustic signals in households to rectify the accumulative Wi-Fi tracking error. Theoretically, Wi-Fi sensing in line-of-sight (LoS) and non-line-of-sight (NLoS) scenarios can be modeled as elliptical Fresnel zones and hyperbolic zones, respectively. By designing acoustic sensing signals, we are able to model the acoustic sensing zones as a series of hyperbolic clusters. We reveal how to fuse the fields of electromagnetic waves and mechanical waves, and establish the optimization equation. Next, we design a data-driven architecture to solve the aforementioned optimization equation. Experimental results show that the proposed multimodal tracking scheme exhibits superior performance. We achieve a 89.37% reduction in median tracking error compared to model-based methods and a 65.02% reduction compared to data-driven methods.

</details>


### [16] [Delay-Aware Task Offloading for Heterogeneous VLC-RF-based Vehicular Fog Computing](https://arxiv.org/abs/2601.10978)
*Nan An,Hongyi He,Fang Yang,Chang Liu,Jian Song,Zhu Han,Binbin Zhu*

Main category: eess.SP

TL;DR: 本文提出了一种基于异构可见光通信-射频架构的车载雾计算系统，通过动态任务分割和卸载优化，相比单一通信方式平均任务处理延迟降低15%。


<details>
  <summary>Details</summary>
Motivation: 传统车载雾计算依赖射频通信，在密集车辆环境中适应性有限。可见光通信具有抗干扰优势，但覆盖范围有限，需要结合两者优势来提升任务卸载效率。

Method: 设计异构VLC-RF架构，将计算任务动态分割并通过VLC和RF链路卸载到空闲车辆；提出基于残差的主化最小化算法来优化任务卸载和计算资源分配，最小化平均任务处理延迟。

Result: 仿真结果表明，提出的异构VLC-RF架构结合RBMM算法，相比仅使用VLC或RF的车载雾计算系统，平均任务处理延迟降低了15%。

Conclusion: 异构VLC-RF架构能有效结合VLC的抗干扰性和RF的覆盖优势，显著提升车载雾计算系统的任务处理效率，为下一代交通网络中的延迟敏感服务提供支持。

Abstract: Vehicular fog computing (VFC) is a promising paradigm for reducing the computation burden of vehicles, thus supporting delay-sensitive services in next-generation transportation networks. However, traditional VFC schemes rely on radio frequency (RF) communications, which limits their adaptability for dense vehicular environments. In this paper, a heterogeneous visible light communication (VLC)-RF architecture is designed for VFC systems to facilitate efficient task offloading. Specifically, computing tasks are dynamically partitioned and offloaded to idle vehicles via both VLC and RF links, thereby fully exploiting the interference resilience of VLC and the coverage advantage of RF. To minimize the average task processing delay (TPD), an optimization problem of task offloading and computing resource allocation is formulated, and then solved by the developed residual-based majorization-minimization (RBMM) algorithm. Simulation results confirm that the heterogeneous VLC-RF architecture with the proposed algorithm achieves a 15% average TPD reduction compared to VFC systems relying solely on VLC or RF.

</details>


### [17] [Uni-Fi: Integrated Multi-Task Wi-Fi Sensing](https://arxiv.org/abs/2601.10980)
*Mengning Li,Wenye Wang*

Main category: eess.SP

TL;DR: Uni-Fi是一个可扩展的多任务Wi-Fi感知集成框架，通过统一架构和可扩展流水线解决不同感知任务集成难题，在定位、活动分类和存在检测任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi感知技术能够非侵入式连续监测用户位置和活动，支持多种智能家居应用。不同感知任务存在上下文关系，集成可以提升单个模块性能，但当前研究缺乏统一架构和可扩展流水线来整合不同任务。

Method: 提出Uni-Fi框架：1) 建立统一理论框架揭示单任务与多任务感知的根本差异；2) 开发可扩展感知流水线，自动生成多任务感知求解器，实现多个感知模型的无缝集成。

Result: 实验结果显示Uni-Fi在各项任务上表现稳健：定位误差约0.54米，活动分类准确率98.34%，存在检测准确率98.57%。

Conclusion: Uni-Fi提供了一个有效的多任务Wi-Fi感知集成解决方案，通过统一架构和可扩展流水线成功解决了不同感知任务集成的挑战，为未来研究提供了可扩展的基础框架。

Abstract: Wi-Fi sensing technology enables non-intrusive, continuous monitoring of user locations and activities, which supports diverse smart home applications. Since different sensing tasks exhibit contextual relationships, their integration can enhance individual module performance. However, integrating sensing tasks across different research efforts faces challenges due to the absence of two key elements. The first is a unified architecture that captures the fundamental nature shared across diverse sensing tasks. The second is an extensible pipeline that can integrate sensing methodologies proposed in potential future research. This paper presents Uni-Fi, an extensible framework for multi-task Wi-Fi sensing integration. This paper makes the following contributions. First, we propose a unified theoretical framework that reveals the fundamental differences between single-task and multi-task sensing. Second, we develop a scalable sensing pipeline that automatically generates multi-task sensing solvers, enabling seamless integration of multiple sensing models. Experimental results show that Uni-Fi achieves robust performance across tasks, with a localization error of approximately 0.54 meters, 98.34 percent accuracy for activity classification, and 98.57 percent accuracy for presence detection.

</details>


### [18] [Hybrid Resource Allocation Scheme for Bistatic ISAC with Data Channels](https://arxiv.org/abs/2601.11110)
*Marcus Henninger,Lucas Giroto,Ahmed Elkelesh,Silvio Mandelli*

Main category: eess.SP

TL;DR: 论文提出了一种混合资源分配方案，通过在合适的感知网格上放置低调制阶数的符号作为伪导频，在略微降低通信链路频谱效率的同时，显著提升双基地感知性能。


<details>
  <summary>Details</summary>
Motivation: 双基地集成感知与通信（ISAC）能够有效重用现有蜂窝基础设施，在未来的感知网络中发挥重要作用。然而，使用数据信道进行ISAC时存在资源分配冲突：通信链路希望传输更高调制阶数的符号以最大化吞吐量，而感知则偏好更低调制阶数以在雷达图像中获得更高的信噪比。

Method: 提出了一种混合资源分配方案，通过在合适的感知网格上放置低调制阶数符号作为伪导频，在提升双基地感知性能的同时，仅略微降低通信链路的频谱效率。

Result: 仿真结果验证了该方法相对于不同基线的有效性，并提供了关于解码错误如何影响感知性能的实际见解。

Conclusion: 该混合资源分配方案能够有效解决ISAC中通信与感知的资源分配冲突，在保证通信性能的同时显著提升感知性能，为未来双基地感知网络的实现提供了实用方案。

Abstract: Bistatic integrated sensing and communication (ISAC) enables efficient reuse of the existing cellular infrastructure and is likely to play an important role in future sensing networks. In this context, ISAC using the data channel is a promising approach to improve the bistatic sensing performance compared to relying solely on pilots. One of the challenges associated with this approach is resource allocation: the communication link aims to transmit higher modulation order (MO) symbols to maximize the throughput, whereas a lower MO is preferable for sensing to achieve a higher signal-to-noise ratio in the radar image. To address this conflict, this paper introduces a hybrid resource allocation scheme. By placing lower MO symbols as pseudo-pilots on a suitable sensing grid, we enhance the bistatic sensing performance while only slightly reducing the spectral efficiency of the communication link. Simulation results validate our approach against different baselines and provide practical insights into how decoding errors affect the sensing performance.

</details>


### [19] [Comprehensive Robust Dynamic Mode Decomposition from Mode Extraction to Dimensional Reduction](https://arxiv.org/abs/2601.11116)
*Yuki Nakamura,Shingo Takemoto,Shunsuke Ono*

Main category: eess.SP

TL;DR: 提出CR-DMD框架，通过凸优化预处理去除混合噪声，并用凸优化降维方法构建忠实低维表示，在噪声条件下优于现有鲁棒DMD方法。


<details>
  <summary>Details</summary>
Motivation: 标准DMD依赖最小二乘估计计算线性时间演化算子，在噪声条件下性能显著下降。现有鲁棒变体通常修改最小二乘公式，但仍不稳定且无法确保忠实的低维表示。

Method: 1) 基于凸优化的预处理方法，有效去除混合噪声，实现准确稳定的模态提取；2) 新的凸优化降维公式，将鲁棒提取的模态与原始噪声观测显式关联，通过模态的稀疏加权和构建原始数据的忠实表示。两个阶段都通过预条件原始-对偶分裂方法高效求解。

Result: 在流体动力学数据集上的实验表明，CR-DMD在噪声条件下的模态准确性和低维表示保真度方面始终优于最先进的鲁棒DMD方法。

Conclusion: CR-DMD通过鲁棒化整个DMD流程（从模态提取到降维），为混合噪声条件下的动态系统分析提供了更稳定和准确的框架。

Abstract: We propose Comprehensive Robust Dynamic Mode Decomposition (CR-DMD), a novel framework that robustifies the entire DMD process - from mode extraction to dimensional reduction - against mixed noise. Although standard DMD widely used for uncovering spatio-temporal patterns and constructing low-dimensional models of dynamical systems, it suffers from significant performance degradation under noise due to its reliance on least-squares estimation for computing the linear time evolution operator. Existing robust variants typically modify the least-squares formulation, but they remain unstable and fail to ensure faithful low-dimensional representations. First, we introduce a convex optimization-based preprocessing method designed to effectively remove mixed noise, achieving accurate and stable mode extraction. Second, we propose a new convex formulation for dimensional reduction that explicitly links the robustly extracted modes to the original noisy observations, constructing a faithful representation of the original data via a sparse weighted sum of the modes. Both stages are efficiently solved by a preconditioned primal-dual splitting method. Experiments on fluid dynamics datasets demonstrate that CR-DMD consistently outperforms state-of-the-art robust DMD methods in terms of mode accuracy and fidelity of low-dimensional representations under noisy conditions.

</details>


### [20] [Scalable mm-Wave Liquid Crystal Reconfigurable Intelligent Surfaces based on the Delay Line Architecture](https://arxiv.org/abs/2601.11307)
*Julia Schwarzbeck,Robin Neuder,Marc Späth,Alejandro Jiménez-Sáez*

Main category: eess.SP

TL;DR: 本文设计、制造并表征了工作在60GHz频段、最多750个辐射单元的宽带液晶可重构智能表面，采用延迟线架构实现宽带、连续相位控制和快速响应。


<details>
  <summary>Details</summary>
Motivation: 开发可扩展的毫米波可重构智能表面，解决传统方法在带宽、相位控制和响应速度方面的限制，实现高效波束赋形。

Method: 采用延迟线架构将相位控制层与辐射层解耦，使用4.6微米薄液晶层，实现列式偏置控制，制造了120和750单元两种原型。

Result: 测量显示±60°波束扫描能力，-3dB带宽超过9%，单元功耗纳瓦级，仿真预测孔径效率超过20%，实测效率为9.2%和2.6%。

Conclusion: 延迟线架构的液晶RIS在带宽、相位控制和可扩展性方面优于传统方法，实测效率下降主要源于实验室环境的技术挑战。

Abstract: This paper presents the design, fabrication, and characterization of broadband liquid crystal (LC) reconfigurable intelligent surfaces (RIS) operating around 60 GHz and scaling up to 750 radiating elements. The RISs employ a delay line architecture (DLA) that decouples the phase shifting and radiating layer, enabling wide bandwidth, continuous phase control exceeding 360°, and fast response times with a micrometer-thin LC layer of 4.6 micrometer. Two prototypes with 120 and 750 elements are realized using identical unit cells and column-wise biasing. Measurements demonstrate beam steering over +-60° and -3 dB bandwidths exceeding 9% for both apertures, confirming the scalability of the proposed architecture. On top of a measured nanowatt power consumption per unit cell, aperture efficiencies above 20% are predicted by simulations. While the measured efficiencies are reduced to 9.2% and 2.6%, a detailed analysis verifies that this reduction can be attributed to technological challenges in a laboratory environment. Finally, a comprehensive comparison between the applied DLA-based LC-RIS and a conventional approach highlights the superior potential of applied architecture.

</details>


### [21] [Modulation, ISI, and Detection for Langmuir Adsorption-Based Microfluidic Molecular Communication](https://arxiv.org/abs/2601.11351)
*Ruifeng Zheng,Pengjie Zhou,Pit Hofmann,Martín Schottlender,Fatima Rani,Juan A. Cabrera,Frank H. P. Fitzek*

Main category: eess.SP

TL;DR: 研究微流体分子通信接收器，采用有限容量Langmuir吸附模型，在反应限制区域推导了闭式单脉冲响应核和符号率递归，揭示了信道记忆和符号间干扰，并提出低复杂度检测器。


<details>
  <summary>Details</summary>
Motivation: 研究微流体分子通信接收器的性能，特别是在有限容量Langmuir吸附模型下的信道特性，旨在理解信道记忆和符号间干扰对通信性能的影响。

Method: 在反应限制区域推导闭式单脉冲响应核和符号率递归，开发短脉冲和长脉冲近似，采用有限受体二项计数模型和脉冲结束采样，提出低复杂度中点阈值检测器。

Result: 揭示了长脉冲区域由于饱和导致的干扰不对称性，提出的检测器在干扰可忽略时简化为固定阈值，数值结果验证了所提出的表征方法并量化了检测性能。

Conclusion: 该研究为微流体分子通信接收器提供了理论分析框架，揭示了信道记忆和干扰特性，提出的低复杂度检测器在实际应用中具有可行性。

Abstract: This paper studies microfluidic molecular communication receivers with finite-capacity Langmuir adsorption driven by an effective surface concentration. In the reaction-limited regime, we derive a closed-form single-pulse response kernel and a symbol-rate recursion for on-off keying that explicitly exposes channel memory and inter-symbol interference. We further develop short-pulse and long-pulse approximations, revealing an interference asymmetry in the long-pulse regime due to saturation. To account for stochasticity, we adopt a finite-receptor binomial counting model, employ pulse-end sampling, and propose a low-complexity midpoint-threshold detector that reduces to a fixed threshold when interference is negligible. Numerical results corroborate the proposed characterization and quantify detection performance versus pulse and symbol durations.

</details>


### [22] [Channel Estimation in MIMO Systems Aided by Microwave Linear Analog Computers (MiLACs)](https://arxiv.org/abs/2601.11438)
*Qiaosen Zhang,Matteo Nerini,Bruno Clerckx*

Main category: eess.SP

TL;DR: 提出针对微波线性模拟计算机辅助MIMO系统的高效信道估计方案，通过设计训练预编码器和组合器，在模拟域实现LS和MMSE估计，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 微波线性模拟计算机（MiLACs）在巨型MIMO系统中具有硬件和计算成本优势，但信道估计问题尚未解决。传统LS和MMSE估计依赖密集数字计算，削弱了MiLACs的优势。

Method: 设计由MiLACs实现的训练预编码器和组合器，使LS和MMSE估计完全在模拟域进行，性能与数字对应方法相同但复杂度大幅降低。

Result: 提出的方案在保持与数字方法相同性能的同时，显著降低了计算复杂度、发射RF链数量、ADC/DAC分辨率要求和峰均功率比。

Conclusion: 数值结果验证了所提方案的有效性和优势，为MiLAC辅助MIMO系统提供了高效的信道估计解决方案。

Abstract: Microwave linear analog computers (MiLACs) have recently emerged as a promising solution for future gigantic multiple-input multiple-output (MIMO) systems, enabling beamforming with greatly reduced hardware and computational cost. However, channel estimation for MiLAC-aided systems remains an open problem. Conventional least squares (LS) and minimum mean square error (MMSE) estimation rely on intensive digital computation, which undermines the benefits offered by MiLACs. In this letter, we propose efficient LS and MMSE channel estimation schemes for MiLAC-aided MIMO systems. By designing training precoders and combiners implemented by MiLACs, both LS and MMSE estimation are performed fully in the analog domain, achieving identical performance to their digital counterparts while significantly reducing computational complexity, transmit RF chains, analog-to-digital/digital-to-analog converters (ADCs/DACs) resolution requirements, and peak-to-average power ratio (PAPR). Numerical results verify the effectiveness and advantages of the proposed schemes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Analytic Bijections for Smooth and Interpretable Normalizing Flows](https://arxiv.org/abs/2601.10774)
*Mathis Gerdes,Miranda C. N. Cheng*

Main category: cs.LG

TL;DR: 提出三种全局平滑、定义在全实数域且可解析逆变换的双射函数（三次有理、sinh、三次多项式），以及径向流架构，在保持训练稳定性的同时显著减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有归一化流方法存在权衡：仿射变换平滑可解析逆但表达能力有限；单调样条有局部控制但分段平滑且定义在有界域；残差流平滑但需要数值求逆。需要结合各种方法优点的平滑、全局定义、可解析逆的双射函数。

Method: 1. 提出三种解析双射函数族：三次有理、sinh和三次多项式，它们全局平滑(C^∞)、定义在全实数域、可闭式解析求逆。2. 开发径向流架构：通过直接参数化变换径向坐标同时保持角度方向，产生几何可解释的变换。

Result: 1. 在耦合流中作为即插即用替换，匹配或超越样条性能。2. 径向流展示出卓越的训练稳定性，对具有径向结构的目标，能以1000倍更少的参数达到与耦合流相当的质量。3. 在φ^4晶格场论的高维物理问题中，超越仿射基线并解决模式崩溃问题。

Conclusion: 提出的三种解析双射函数结合了现有方法的优点，而径向流架构提供了参数高效且几何可解释的替代方案，在保持训练稳定性的同时显著减少参数需求，适用于高维物理问题。

Abstract: A key challenge in designing normalizing flows is finding expressive scalar bijections that remain invertible with tractable Jacobians. Existing approaches face trade-offs: affine transformations are smooth and analytically invertible but lack expressivity; monotonic splines offer local control but are only piecewise smooth and act on bounded domains; residual flows achieve smoothness but need numerical inversion. We introduce three families of analytic bijections -- cubic rational, sinh, and cubic polynomial -- that are globally smooth ($C^\infty$), defined on all of $\mathbb{R}$, and analytically invertible in closed form, combining the favorable properties of all prior approaches. These bijections serve as drop-in replacements in coupling flows, matching or exceeding spline performance. Beyond coupling layers, we develop radial flows: a novel architecture using direct parametrization that transforms the radial coordinate while preserving angular direction. Radial flows exhibit exceptional training stability, produce geometrically interpretable transformations, and on targets with radial structure can achieve comparable quality to coupling flows with $1000\times$ fewer parameters. We provide comprehensive evaluation on 1D and 2D benchmarks, and demonstrate applicability to higher-dimensional physics problems through experiments on $φ^4$ lattice field theory, where our bijections outperform affine baselines and enable problem-specific designs that address mode collapse.

</details>


### [24] [FSL-BDP: Federated Survival Learning with Bayesian Differential Privacy for Credit Risk Modeling](https://arxiv.org/abs/2601.11134)
*Sultan Amed,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: 提出了联邦生存学习框架FSL-BDP，结合贝叶斯差分隐私，在保护借款人数据隐私的同时实现跨机构信用风险建模，解决了传统违约预测的局限性。


<details>
  <summary>Details</summary>
Motivation: 数据保护法规（如GDPR、CCPA）禁止跨境共享借款人数据，但信用风险模型需要跨机构学习。传统违约预测存在两个问题：二元分类忽略违约时间，将早期违约者与晚期违约者等同对待；集中式训练违反新兴监管约束。

Method: 提出联邦生存学习框架FSL-BDP，结合贝叶斯差分隐私，在不集中敏感数据的情况下建模违约时间轨迹。该框架提供贝叶斯（数据依赖）差分隐私保证，同时使机构能够联合学习风险动态。

Result: 在三个真实信用数据集（LendingClub、SBA、Bondora）上的实验表明，联邦学习从根本上改变了隐私机制的相对有效性。在集中式设置中，经典DP优于贝叶斯DP，但后者从联邦学习中获益更大（+7.0% vs +1.4%），达到接近非私有性能，并在大多数参与客户端中优于经典DP。

Conclusion: 隐私机制选择应在目标部署架构中评估，而不是基于集中式基准。这些发现为在受监管的多机构环境中设计隐私保护决策支持系统的从业者提供了可操作的指导。

Abstract: Credit risk models are a critical decision-support tool for financial institutions, yet tightening data-protection rules (e.g., GDPR, CCPA) increasingly prohibit cross-border sharing of borrower data, even as these models benefit from cross-institution learning. Traditional default prediction suffers from two limitations: binary classification ignores default timing, treating early defaulters (high loss) equivalently to late defaulters (low loss), and centralized training violates emerging regulatory constraints. We propose a Federated Survival Learning framework with Bayesian Differential Privacy (FSL-BDP) that models time-to-default trajectories without centralizing sensitive data. The framework provides Bayesian (data-dependent) differential privacy (DP) guarantees while enabling institutions to jointly learn risk dynamics. Experiments on three real-world credit datasets (LendingClub, SBA, Bondora) show that federation fundamentally alters the relative effectiveness of privacy mechanisms. While classical DP performs better than Bayesian DP in centralized settings, the latter benefits substantially more from federation (+7.0\% vs +1.4\%), achieving near parity of non-private performance and outperforming classical DP in the majority of participating clients. This ranking reversal yields a key decision-support insight: privacy mechanism selection should be evaluated in the target deployment architecture, rather than centralized benchmarks. These findings provide actionable guidance for practitioners designing privacy-preserving decision support systems in regulated, multi-institutional environments.

</details>


### [25] [Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework](https://arxiv.org/abs/2601.10779)
*Qingyue Zhang,Chang Chu,Haohao Fu,Tianren Peng,Yanru Wu,Guanbo Huang,Yang Li,Shao-Lun Huang*

Main category: cs.LG

TL;DR: UOWQ框架统一优化多源迁移学习中的源权重和迁移数量，通过理论分析证明调整权重后使用所有源样本最优，并提供闭式解和优化方法，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统多源迁移学习方法通常只优化源权重或迁移数量，忽略了二者的联合优化，导致可能产生负迁移。需要一种统一框架来平衡异构源任务的贡献。

Method: 提出UOWQ理论框架，基于KL散度泛化误差的渐近分析，将多源迁移学习建模为参数估计问题。证明权重调整后使用所有源样本最优，为单源提供闭式解，为多源设计凸优化数值方法，并开发实用算法。

Result: 在DomainNet和Office-Home等真实数据集上的实验表明，UOWQ始终优于强基线方法，验证了理论预测和实际有效性。

Conclusion: UOWQ框架统一优化源权重和迁移数量，解决了多源迁移学习中的负迁移问题，理论分析和实验验证均表明其优越性，为迁移学习提供了新的理论指导和实用工具。

Abstract: Transfer learning plays a vital role in improving model performance in data-scarce scenarios. However, naive uniform transfer from multiple source tasks may result in negative transfer, highlighting the need to properly balance the contributions of heterogeneous sources. Moreover, existing transfer learning methods typically focus on optimizing either the source weights or the amount of transferred samples, while largely neglecting the joint consideration of the other. In this work, we propose a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), which formulates multi-source transfer learning as a parameter estimation problem grounded in an asymptotic analysis of a Kullback-Leibler divergence-based generalization error measure. The proposed framework jointly determines the optimal source weights and optimal transfer quantities for each source task. Firstly, we prove that using all available source samples is always optimal once the weights are properly adjusted, and we provide a theoretical explanation for this phenomenon. Moreover, to determine the optimal transfer weights, our analysis yields closed-form solutions in the single-source setting and develops a convex optimization-based numerical procedure for the multi-source case. Building on the theoretical results, we further propose practical algorithms for both multi-source transfer learning and multi-task learning settings. Extensive experiments on real-world benchmarks, including DomainNet and Office-Home, demonstrate that UOWQ consistently outperforms strong baselines. The results validate both the theoretical predictions and the practical effectiveness of our framework.

</details>


### [26] [Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration](https://arxiv.org/abs/2601.10973)
*Zain ul Abdeen,Waris Gill,Ming Jin*

Main category: cs.LG

TL;DR: 提出MGF-RL框架，结合元学习和进化策略，用于配电网故障恢复，能在不确定性和非线性约束下快速适应新场景。


<details>
  <summary>Details</summary>
Motivation: 极端事件后恢复关键负荷需要自适应控制，但可再生能源不确定性、可调度资源有限和非线性动态使恢复困难。传统强化学习泛化能力差，需要大量重新训练。

Method: 提出元引导无梯度强化学习(MGF-RL)框架，结合一阶元学习和进化策略，从历史故障经验学习可迁移初始化，无需梯度计算即可快速适应新场景。

Result: 在IEEE 13总线和123总线测试系统中，MGF-RL在可靠性、恢复速度和适应效率方面优于标准RL、基于MAML的元RL和模型预测控制，泛化能力强且需要更少的微调。

Conclusion: MGF-RL为可再生能源丰富的配电网实时负荷恢复提供了有效解决方案，具有理论保证和实际应用价值。

Abstract: Restoring critical loads after extreme events demands adaptive control to maintain distribution-grid resilience, yet uncertainty in renewable generation, limited dispatchable resources, and nonlinear dynamics make effective restoration difficult. Reinforcement learning (RL) can optimize sequential decisions under uncertainty, but standard RL often generalizes poorly and requires extensive retraining for new outage configurations or generation patterns. We propose a meta-guided gradient-free RL (MGF-RL) framework that learns a transferable initialization from historical outage experiences and rapidly adapts to unseen scenarios with minimal task-specific tuning. MGF-RL couples first-order meta-learning with evolutionary strategies, enabling scalable policy search without gradient computation while accommodating nonlinear, constrained distribution-system dynamics. Experiments on IEEE 13-bus and IEEE 123-bus test systems show that MGF-RL outperforms standard RL, MAML-based meta-RL, and model predictive control across reliability, restoration speed, and adaptation efficiency under renewable forecast errors. MGF-RL generalizes to unseen outages and renewable patterns while requiring substantially fewer fine-tuning episodes than conventional RL. We also provide sublinear regret bounds that relate adaptation efficiency to task similarity and environmental variation, supporting the empirical gains and motivating MGF-RL for real-time load restoration in renewable-rich distribution grids.

</details>


### [27] [When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models](https://arxiv.org/abs/2601.11444)
*Raphaël Razafindralambo,Rémy Sun,Frédéric Precioso,Damien Garreau,Pierre-Alexandre Mattei*

Main category: cs.LG

TL;DR: 扩散模型集成能改善似然度但未必提升图像质量指标，理论分析揭示了模型组合的机制


<details>
  <summary>Details</summary>
Motivation: 尽管集成方法在监督学习中效果显著，但在无条件基于分数的扩散模型中的应用尚未充分探索，本研究旨在探究集成是否能为生成建模带来实际益处

Method: 使用深度集成、蒙特卡洛Dropout等多种聚合规则，在CIFAR-10和FFHQ数据集上进行实验，同时通过随机森林研究表格数据，并进行理论分析

Result: 集成分数模型能改善分数匹配损失和模型似然度，但无法一致提升FID等感知质量指标；在表格数据中发现某些聚合策略表现更优

Conclusion: 扩散模型集成在改善统计指标方面有效，但对感知质量提升有限，理论分析为模型组合技术提供了新的见解

Abstract: Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).

</details>


### [28] [Towards Tensor Network Models for Low-Latency Jet Tagging on FPGAs](https://arxiv.org/abs/2601.10801)
*Alberto Coppi,Ema Puljak,Lorenzo Borella,Daniel Jaschke,Enrique Rico,Maurizio Pierini,Jacopo Pazzini,Andrea Triossi,Simone Montangero*

Main category: cs.LG

TL;DR: 该研究系统探索了张量网络（TN）模型（MPS和TTN）用于高能物理中的实时喷注标记，重点关注在FPGA上的低延迟部署，展示了在HL-LHC Level-1触发系统中应用的可行性。


<details>
  <summary>Details</summary>
Motivation: HL-LHC Level-1触发系统的严格要求需要紧凑且可解释的替代方案来替代深度神经网络，以实现低延迟的实时喷注标记。

Method: 使用矩阵乘积态（MPS）和树张量网络（TTN）作为张量网络模型，采用低层喷注成分特征，研究后训练量化以实现硬件高效实现，并在FPGA上进行综合以评估资源使用情况。

Result: 模型性能与最先进的深度学习分类器相当，通过量化后性能不下降，FPGA实现显示亚微秒级延迟，支持在实时触发系统中在线部署的可行性。

Conclusion: 张量网络模型在低延迟环境中具有快速和资源高效推理的潜力，特别适用于HL-LHC Level-1触发系统等实时应用场景。

Abstract: We present a systematic study of Tensor Network (TN) models $\unicode{x2013}$ Matrix Product States (MPS) and Tree Tensor Networks (TTN) $\unicode{x2013}$ for real-time jet tagging in high-energy physics, with a focus on low-latency deployment on Field Programmable Gate Arrays (FPGAs). Motivated by the strict requirements of the HL-LHC Level-1 trigger system, we explore TNs as compact and interpretable alternatives to deep neural networks. Using low-level jet constituent features, our models achieve competitive performance compared to state-of-the-art deep learning classifiers. We investigate post-training quantization to enable hardware-efficient implementations without degrading classification performance or latency. The best-performing models are synthesized to estimate FPGA resource usage, latency, and memory occupancy, demonstrating sub-microsecond latency and supporting the feasibility of online deployment in real-time trigger systems. Overall, this study highlights the potential of TN-based models for fast and resource-efficient inference in low-latency environments.

</details>


### [29] [Offline Reinforcement-Learning-Based Power Control for Application-Agnostic Energy Efficiency](https://arxiv.org/abs/2601.11352)
*Akhilesh Raj,Swann Perarnau,Aniruddha Gokhale,Solomon Bekele Abera*

Main category: cs.LG

TL;DR: 使用离线强化学习设计CPU功率控制器，通过预收集的数据训练，在运行时优化并行应用的能效，同时控制性能下降在可接受范围内。


<details>
  <summary>Details</summary>
Motivation: 现代计算基础设施设计中能效至关重要，虽然强化学习适合设计能效控制系统，但在线训练面临缺乏合适模拟环境、噪声干扰和可靠性等问题。离线强化学习可以规避这些在线训练问题。

Method: 采用离线强化学习方法，结合灰盒能效方法，使用在线应用无关性能数据（如心跳）和硬件性能计数器，在训练前收集任意策略的状态转换数据集，避免在线训练问题。

Result: 在多种计算密集型和内存密集型基准测试上评估，通过Intel的Running Average Power Limit控制实时系统功率，证明离线训练的智能体能在可容忍性能下降成本下显著降低能耗。

Conclusion: 离线强化学习是设计自主CPU功率控制器的可行替代方案，能够有效提高并行应用的运行时能效，同时控制性能影响在可接受范围内。

Abstract: Energy efficiency has become an integral aspect of modern computing infrastructure design, impacting the performance, cost, scalability, and durability of production systems. The incorporation of power actuation and sensing capabilities in CPU designs is indicative of this, enabling the deployment of system software that can actively monitor and adjust energy consumption and performance at runtime. While reinforcement learning (RL) would seem ideal for the design of such energy efficiency control systems, online training presents challenges ranging from the lack of proper models for setting up an adequate simulated environment, to perturbation (noise) and reliability issues, if training is deployed on a live system.
  In this paper we discuss the use of offline reinforcement learning as an alternative approach for the design of an autonomous CPU power controller, with the goal of improving the energy efficiency of parallel applications at runtime without unduly impacting their performance. Offline RL sidesteps the issues incurred by online RL training by leveraging a dataset of state transitions collected from arbitrary policies prior to training.
  Our methodology applies offline RL to a gray-box approach to energy efficiency, combining online application-agnostic performance data (e.g., heartbeats) and hardware performance counters to ensure that the scientific objectives are met with limited performance degradation. Evaluating our method on a variety of compute-bound and memory-bound benchmarks and controlling power on a live system through Intel's Running Average Power Limit, we demonstrate that such an offline-trained agent can substantially reduce energy consumption at a tolerable performance degradation cost.

</details>


### [30] [Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning -- Towards a Pure Neural Logic Core](https://arxiv.org/abs/2601.10810)
*Mengmeng Peng,Zhenyu Fang,He Sun*

Main category: cs.LG

TL;DR: 论文提出"数字代谢"假说，通过选择性遗忘来分离LLM中的逻辑推理能力与事实知识，并开发RLCP训练框架实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在参数纠缠问题，通用推理能力（逻辑）和特定事实知识（事实）在共享权重中处于叠加状态。这种耦合导致"记忆墙"问题，计算能力浪费在模拟检索上，经常产生幻觉。

Method: 提出再生逻辑核心协议（RLCP），一种双流训练框架，通过深层梯度反转使特定事实依赖线性不可解码。将RLCP应用于Qwen2.5-0.5B模型。

Result: 观察到明显的相变：模型对目标事实关联的保留率接近零（准确率<7%），同时表现出与"结构结晶"效应一致的变化。在GSM8K上，代谢后的模型自发采用思维链（CoT）支架。

Conclusion: 研究为模块化"神经CPU+符号RAM"架构铺平了道路，提供了与DeepSeek的Engram等架构创新相对应的动态权重级解决方案，但行为转变的因果机制仍需进一步研究。

Abstract: Large language models (LLMs) currently suffer from parameter entanglement, where general reasoning capabilities (logic) and specific factual knowledge (facts) exist in a superposition state within shared weights. This coupling leads to the "memory wall," where computational capacity is squandered on simulating retrieval, often resulting in hallucinations. In this paper, we propose "digital metabolism," a thermodynamic hypothesis suggesting that targeted forgetting is necessary for distilling a pure neural logic core. To validate this hypothesis, we introduce the Regenerative Logic-Core Protocol (RLCP), a dual-stream training framework that renders specific factual dependencies linearly undecodable via deep-layer gradient reversal. Applying RLCP to Qwen2.5-0.5B, we observe a distinct phase transition: the model achieves near-zero retention of targeted factual associations (Accuracy < 7%) while exhibiting changes consistent with an emergent "structural crystallization" effect. Empirical analysis on GSM8K reveals that the "metabolized" model spontaneously adopts chain-of-thought (CoT) scaffolding, which we interpret as compensating for the loss of direct associative recall (shifting from $O(1)$ recall to $O(N)$ reasoning). While the causal mechanism underlying this behavioral shift requires further investigation, our findings provide a dynamic weight-level counterpart to architectural innovations like DeepSeek's Engram, paving the way for modular "Neural CPU + Symbolic RAM" architectures.

</details>


### [31] [MetaboNet: The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management](https://arxiv.org/abs/2601.11505)
*Miriam K. Wolff,Peter Calhoun,Eleonora Maria Aiello,Yao Qin,Sam F. Royston*

Main category: cs.LG

TL;DR: 研究者整合多个公开的1型糖尿病数据集，创建了名为MetaboNet的统一资源，包含3135名患者和1228患者年的连续血糖监测与胰岛素泵数据，旨在解决数据碎片化和标准化问题。


<details>
  <summary>Details</summary>
Motivation: 当前1型糖尿病算法开发受到数据碎片化和缺乏标准化的限制。现有数据集结构差异大，访问和处理耗时，阻碍了数据整合，降低了算法开发的可比性和泛化性。

Method: 整合多个公开的1型糖尿病数据集，要求同时包含连续血糖监测数据和胰岛素泵剂量记录。保留碳水化合物摄入和体力活动等辅助信息。提供完全公开的子集和需要数据使用协议限制的子集，并为后者提供数据处理管道以转换为标准化格式。

Result: MetaboNet数据集包含3135名受试者和1228患者年的重叠CGM和胰岛素数据，规模远超现有独立基准数据集。数据集涵盖广泛的血糖谱和人口统计学特征，可产生比单个数据集更具泛化性的算法性能。

Conclusion: 提出了一个用于1型糖尿病研究的统一公共数据集，描述了其无限制和DUA管理组件的访问途径。该资源旨在促进更标准化、可比较和泛化的算法开发。

Abstract: Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unified and accessible data resource for T1D algorithm development. Multiple publicly available T1D datasets were consolidated into a unified resource, termed the MetaboNet dataset. Inclusion required the availability of both continuous glucose monitoring (CGM) data and corresponding insulin pump dosing records. Additionally, auxiliary information such as reported carbohydrate intake and physical activity was retained when present. The MetaboNet dataset comprises 3135 subjects and 1228 patient-years of overlapping CGM and insulin data, making it substantially larger than existing standalone benchmark datasets. The resource is distributed as a fully public subset available for immediate download at https://metabo-net.org/ , and with a Data Use Agreement (DUA)-restricted subset accessible through their respective application processes. For the datasets in the latter subset, processing pipelines are provided to automatically convert the data into the standardized MetaboNet format. A consolidated public dataset for T1D research is presented, and the access pathways for both its unrestricted and DUA-governed components are described. The resulting dataset covers a broad range of glycemic profiles and demographics and thus can yield more generalizable algorithmic performance than individual datasets.

</details>


### [32] [Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents](https://arxiv.org/abs/2601.10820)
*Himanshu Thakur,Anusha Kamath,Anurag Muthyala,Dhwani Sanmukhani,Smruthi Mukund,Jay Katukuri*

Main category: cs.LG

TL;DR: 提出一个规划引导的多智能体框架，用于自动化特征工程，通过LLM规划器协调多个智能体，利用团队环境图生成代码，并能请求人工干预，显著提升特征工程效率。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成模型在特征工程自动化方面面临三大挑战：1) 缺乏捕捉生产级特征工程复杂迭代过程的数据集；2) 现有编码智能体(如CoPilot、Devin)与团队特定工具、代码库和工作流程的集成不足；3) 人机协作时机不当或反馈不足导致效果不佳。

Method: 采用规划引导、约束拓扑的多智能体框架，通过LLM驱动的规划器利用团队环境图协调可用智能体，生成上下文感知提示，并利用下游失败回溯修正上游产物。框架能在关键步骤请求人工干预，确保生成代码可靠、可维护且符合团队期望。

Result: 在内部数据集上，该方法相比人工构建和无规划工作流分别提升38%和150%的评估指标。在实际应用中，为服务1.2亿用户的推荐模型构建特征时，将特征工程周期从三周缩短至一天。

Conclusion: 该规划引导的多智能体框架有效解决了特征工程自动化的关键挑战，显著提升了生产效率和代码质量，在实际大规模推荐系统中证明了其实用价值。

Abstract: Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.

</details>


### [33] [Mugi: Value Level Parallelism For Efficient LLMs](https://arxiv.org/abs/2601.10823)
*Daniel Price,Prabhu Vellaisamy,John Shen,Di Wu*

Main category: cs.LG

TL;DR: Mugi架构通过值级并行性优化LLM，在非线性近似和小批量GEMM中显著提升性能、能效和可持续性


<details>
  <summary>Details</summary>
Motivation: 现有VLP主要针对大批量、低精度GEMM，但LLM包含更复杂的操作，需要探索VLP如何全面优化LLM

Method: 1) 将VLP推广到非线性近似，采用以值为中心的方法；2) 优化小批量GEMM中的VLP，结合权重量化、KV缓存量化等技术；3) 设计Mugi架构整合创新

Result: Mugi在非线性softmax操作中吞吐量提升45倍、能效提升668倍；在LLM中吞吐量提升2.07倍、能效提升3.11倍；运行碳排放降低1.45倍，隐含碳排放降低1.48倍

Conclusion: VLP可有效优化LLM的复杂操作，Mugi架构实现了显著的性能、能效和可持续性改进，为LLM硬件加速提供新方案

Abstract: Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM. In this paper, we explore how VLP benefits LLMs. First, we generalize VLP for nonlinear approximations, outperforming existing nonlinear approximations in end-to-end LLM accuracy, performance, and efficiency. Our VLP approximation follows a value-centric approach, where important values are assigned with greater accuracy. Second, we optimize VLP for small-batch GEMMs with asymmetric inputs efficiently, which leverages timely LLM optimizations, including weight-only quantization, key-value (KV) cache quantization, and group query attention. Finally, we design a new VLP architecture, Mugi, to encapsulate the innovations above and support full LLM workloads, while providing better performance, efficiency and sustainability. Our experimental results show that Mugi can offer significant improvements on throughput and energy efficiency, up to $45\times$ and $668\times$ for nonlinear softmax operations, and $2.07\times$ and $3.11\times$ for LLMs, and also decrease operational carbon for LLM operation by $1.45\times$ and embodied carbon by $1.48\times$.

</details>


### [34] [AI-Guided Human-In-the-Loop Inverse Design of High Performance Engineering Structures](https://arxiv.org/abs/2601.10859)
*Dat Quoc Ha,Md Ferdous Alam,Markus J. Buehler,Faez Ahmed,Josephine V. Carstensen*

Main category: cs.LG

TL;DR: 提出AI协同拓扑优化系统，通过机器学习预测用户偏好的修改区域，减少人机交互迭代次数，提高设计效率


<details>
  <summary>Details</summary>
Motivation: 传统拓扑优化计算时间长且黑箱特性阻碍用户交互，现有的人机交互方法依赖耗时的迭代区域选择，需要减少迭代次数

Method: 采用U-Net架构的图像分割模型预测用户偏好区域，使用合成数据集训练（识别最长拓扑构件或最复杂结构连接），将AI推荐集成到人机交互拓扑优化流程中

Result: 模型能成功预测合理的修改区域，在多样化非标准问题上展现泛化能力，演示案例显示可提高制造性或线性屈曲载荷39%，总设计时间仅增加15秒

Conclusion: AI协同的人机交互拓扑优化方法能有效减少迭代次数，提高设计效率，同时保持用户对设计过程的控制

Abstract: Inverse design tools such as Topology Optimization (TO) can achieve new levels of improvement for high-performance engineered structures. However, widespread use is hindered by high computational times and a black-box nature that inhibits user interaction. Human-in-the-loop TO approaches are emerging that integrate human intuition into the design generation process. However, these rely on the time-consuming bottleneck of iterative region selection for design modifications. To reduce the number of iterative trials, this contribution presents an AI co-pilot that uses machine learning to predict the user's preferred regions. The prediction model is configured as an image segmentation task with a U-Net architecture. It is trained on synthetic datasets where human preferences either identify the longest topological member or the most complex structural connection. The model successfully predicts plausible regions for modification and presents them to the user as AI recommendations. The human preference model demonstrates generalization across diverse and non-standard TO problems and exhibits emergent behavior outside the single-region selection training data. Demonstration examples show that the new human-in-the-loop TO approach that integrates the AI co-pilot can improve manufacturability or improve the linear buckling load by 39% while only increasing the total design time by 15 sec compared to conventional simplistic TO.

</details>


### [35] [Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting](https://arxiv.org/abs/2601.10863)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.LG

TL;DR: 提出新的预测评分标准（AC评分），同时考虑多步预测的准确性和时间一致性，并应用于季节性ARIMA模型训练，在M4小时数据集上显著降低预测波动性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法只优化准确性，忽略了时间一致性（即模型在不同预测起点对同一未来事件的预测一致性）。需要一种能同时评估多步预测准确性和稳定性的质量度量方法。

Method: 提出预测准确性与一致性评分（AC评分），该评分可衡量概率性多步预测的质量，允许用户指定准确性与一致性要求的权重平衡。将该评分作为可微分目标函数应用于季节性ARIMA模型训练。

Result: 在M4小时基准数据集上，与传统最大似然估计相比，AC优化模型在保持相当或改进的点预测准确性的同时，对相同目标时间戳的预测波动性降低了75%。

Conclusion: AC评分提供了一种同时考虑预测准确性和时间一致性的有效评估框架，优化该评分能显著提高预测稳定性，为时间序列预测提供了更全面的质量度量方法。

Abstract: Traditional time series forecasting methods optimize for accuracy alone. This objective neglects temporal consistency, in other words, how consistently a model predicts the same future event as the forecast origin changes. We introduce the forecast accuracy and coherence score (forecast AC score for short) for measuring the quality of probabilistic multi-horizon forecasts in a way that accounts for both multi-horizon accuracy and stability. Our score additionally provides for user-specified weights to balance accuracy and consistency requirements. As an example application, we implement the score as a differentiable objective function for training seasonal ARIMA models and evaluate it on the M4 Hourly benchmark dataset. Results demonstrate substantial improvements over traditional maximum likelihood estimation. Our AC-optimized models achieve a 75\% reduction in forecast volatility for the same target timestamps while maintaining comparable or improved point forecast accuracy.

</details>


### [36] [Unit-Consistent (UC) Adjoint for GSD and Backprop in Deep Learning Applications](https://arxiv.org/abs/2601.10873)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: 提出一种在正齐次神经网络中保持规范对称性的优化方法，通过单位一致伴随替换欧几里得转置，实现规范一致的梯度下降和反向传播。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络存在节点级对角重缩放的规范对称性，但标准梯度下降不满足这种等变性，导致优化轨迹严重依赖于参数化方式。需要设计保持这种对称性的优化方法。

Method: 在反向伴随/优化几何层面制定不变性要求，用单位一致伴随替换欧几里得转置，推导出单位一致规范一致的梯度下降和反向传播。

Result: 提出了一种简单、算子级的通用方法，可统一应用于网络组件和优化器状态，实现规范对称性保持的优化。

Conclusion: 通过单位一致伴随方法，能够在正齐次神经网络中实现规范一致的优化，解决标准梯度下降对参数化敏感的问题。

Abstract: Deep neural networks constructed from linear maps and positively homogeneous nonlinearities (e.g., ReLU) possess a fundamental gauge symmetry: the network function is invariant to node-wise diagonal rescalings. However, standard gradient descent is not equivariant to this symmetry, causing optimization trajectories to depend heavily on arbitrary parameterizations. Prior work has proposed rescaling-invariant optimization schemes for positively homogeneous networks (e.g., path-based or path-space updates). Our contribution is complementary: we formulate the invariance requirement at the level of the backward adjoint/optimization geometry, which provides a simple, operator-level recipe that can be applied uniformly across network components and optimizer state. By replacing the Euclidean transpose with a Unit-Consistent (UC) adjoint, we derive UC gauge-consistent steepest descent and backprogation.

</details>


### [37] [Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning](https://arxiv.org/abs/2601.10905)
*Rajat Ghosh,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出Action Shapley作为训练数据选择的公平度量方法，通过随机动态算法降低计算复杂度，在数据受限的真实场景中验证了其有效性


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，世界模型的质量对系统效能和可解释性至关重要，而训练数据的质量直接影响世界模型性能。现有训练数据选择方法缺乏系统性和公平性，传统Shapley值计算存在指数复杂度问题

Method: 提出Action Shapley作为与模型无关的训练数据选择度量标准，设计随机动态算法来降低传统Shapley值计算的指数复杂度

Result: 算法在五个数据受限的真实案例研究中，计算效率比传统指数时间计算提升超过80%。基于Action Shapley的训练数据选择策略始终优于临时性数据选择方法

Conclusion: Action Shapley为训练数据选择提供了公平有效的度量方法，其高效计算算法使其适用于实际应用，能显著提升世界模型在数据受限场景下的性能

Abstract: Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.

</details>


### [38] [Realistic Curriculum Reinforcement Learning for Autonomous and Sustainable Marine Vessel Navigation](https://arxiv.org/abs/2601.10911)
*Zhang Xiaocai,Xiao Zhe,Liang Maohan,Liu Tao,Li Haijiang,Zhang Wenbin*

Main category: cs.LG

TL;DR: 提出一个结合课程强化学习、数据驱动海洋模拟环境和机器学习燃料预测的框架，用于实现可持续和安全的海上船舶导航。


<details>
  <summary>Details</summary>
Motivation: 传统船舶导航依赖人工经验，缺乏自主性和排放意识，容易因人为错误影响安全。海事运输的可持续性（包括温室气体排放和航行安全）日益重要。

Method: 1) 构建基于真实船舶运动数据的海洋模拟环境，使用扩散模型模拟动态海事条件；2) 基于历史操作数据和机器学习回归预测燃料消耗；3) 将周围环境表示为图像输入以捕捉空间复杂性；4) 设计轻量级基于策略的课程强化学习代理，包含考虑安全、排放、及时性和目标完成的综合奖励机制。

Result: 在印度洋海域验证了该方法的有效性，能够实现可持续和安全的船舶导航，有效处理复杂任务并在连续动作空间中确保稳定高效的学习。

Conclusion: 提出的课程强化学习框架结合数据驱动模拟和燃料预测，能够实现更可持续和安全的海上船舶导航，解决了传统导航方法的局限性。

Abstract: Sustainability is becoming increasingly critical in the maritime transport, encompassing both environmental and social impacts, such as Greenhouse Gas (GHG) emissions and navigational safety. Traditional vessel navigation heavily relies on human experience, often lacking autonomy and emission awareness, and is prone to human errors that may compromise safety. In this paper, we propose a Curriculum Reinforcement Learning (CRL) framework integrated with a realistic, data-driven marine simulation environment and a machine learning-based fuel consumption prediction module. The simulation environment is constructed using real-world vessel movement data and enhanced with a Diffusion Model to simulate dynamic maritime conditions. Vessel fuel consumption is estimated using historical operational data and learning-based regression. The surrounding environment is represented as image-based inputs to capture spatial complexity. We design a lightweight, policy-based CRL agent with a comprehensive reward mechanism that considers safety, emissions, timeliness, and goal completion. This framework effectively handles complex tasks progressively while ensuring stable and efficient learning in continuous action spaces. We validate the proposed approach in a sea area of the Indian Ocean, demonstrating its efficacy in enabling sustainable and safe vessel navigation.

</details>


### [39] [FAConvLSTM: Factorized-Attention ConvLSTM for Efficient Feature Extraction in Multivariate Climate Data](https://arxiv.org/abs/2601.10914)
*Francis Ndikum Nji,Jianwu Wang*

Main category: cs.LG

TL;DR: FAConvLSTM：一种分解注意力ConvLSTM层，作为ConvLSTM2D的替代方案，通过因子化门计算、多尺度深度可分离分支和轻量轴向注意力，提升效率、空间表达能力和物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统ConvLSTM2D在处理高分辨率地球观测数据时面临挑战：密集卷积门计算成本高，局部感受野限制长程空间结构和解耦气候动力学的建模。

Method: 提出FAConvLSTM，采用[1×1]瓶颈分解门计算，共享深度空间混合；多尺度扩张深度分支和挤压激励重校准；轻量轴向空间注意力稀疏时间应用；专用子空间头生成紧凑时间步嵌入，通过带固定季节位置编码的时间自注意力优化。

Result: 在多变量时空气候数据实验中，FAConvLSTM比标准ConvLSTM产生更稳定、可解释和鲁棒的潜在表示，同时显著降低计算开销。

Conclusion: FAConvLSTM作为ConvLSTM2D的直接替代方案，在效率、空间表达能力和物理可解释性方面均有提升，能更好地建模地球观测数据中的复杂时空动态。

Abstract: Learning physically meaningful spatiotemporal representations from high-resolution multivariate Earth observation data is challenging due to strong local dynamics, long-range teleconnections, multi-scale interactions, and nonstationarity. While ConvLSTM2D is a commonly used baseline, its dense convolutional gating incurs high computational cost and its strictly local receptive fields limit the modeling of long-range spatial structure and disentangled climate dynamics. To address these limitations, we propose FAConvLSTM, a Factorized-Attention ConvLSTM layer designed as a drop-in replacement for ConvLSTM2D that simultaneously improves efficiency, spatial expressiveness, and physical interpretability. FAConvLSTM factorizes recurrent gate computations using lightweight [1 times 1] bottlenecks and shared depthwise spatial mixing, substantially reducing channel complexity while preserving recurrent dynamics. Multi-scale dilated depthwise branches and squeeze-and-excitation recalibration enable efficient modeling of interacting physical processes across spatial scales, while peephole connections enhance temporal precision. To capture teleconnection-scale dependencies without incurring global attention cost, FAConvLSTM incorporates a lightweight axial spatial attention mechanism applied sparsely in time. A dedicated subspace head further produces compact per timestep embeddings refined through temporal self-attention with fixed seasonal positional encoding. Experiments on multivariate spatiotemporal climate data shows superiority demonstrating that FAConvLSTM yields more stable, interpretable, and robust latent representations than standard ConvLSTM, while significantly reducing computational overhead.

</details>


### [40] [HOSL: Hybrid-Order Split Learning for Memory-Constrained Edge Training](https://arxiv.org/abs/2601.10940)
*Aakriti,Zhe Li,Dandan Liang,Chao Huang,Rui Li,Haibo Yang*

Main category: cs.LG

TL;DR: HOSL提出了一种混合阶分割学习框架，通过在客户端使用零阶优化减少内存开销，在服务器端使用一阶优化保证性能，解决了分割学习中内存效率与优化效果之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有分割学习系统主要依赖一阶优化，需要客户端存储激活值等中间量，导致内存开销大，抵消了模型分割的优势。零阶优化虽然减少内存使用，但收敛慢且性能下降。需要解决内存效率与优化效果之间的根本权衡。

Method: HOSL采用混合阶分割学习框架：客户端使用内存高效的零阶梯度估计，消除反向传播和激活存储；服务器端使用一阶优化确保快速收敛和竞争性能。理论分析表明收敛率取决于客户端模型维度而非完整模型维度。

Result: 在OPT模型（125M和1.3B参数）的6个任务上，HOSL相比一阶方法减少客户端GPU内存达3.7倍，同时准确率仅比基线低0.20%-4.23%。相比零阶基线，HOSL性能提升达15.55%，验证了混合策略在边缘设备内存高效训练中的有效性。

Conclusion: HOSL成功解决了分割学习中内存效率与优化效果之间的权衡，通过客户端零阶优化减少内存开销，服务器端一阶优化保证性能，为资源受限边缘设备上的大型语言模型协同训练提供了有效解决方案。

Abstract: Split learning (SL) enables collaborative training of large language models (LLMs) between resource-constrained edge devices and compute-rich servers by partitioning model computation across the network boundary. However, existing SL systems predominantly rely on first-order (FO) optimization, which requires clients to store intermediate quantities such as activations for backpropagation. This results in substantial memory overhead, largely negating benefits of model partitioning. In contrast, zeroth-order (ZO) optimization eliminates backpropagation and significantly reduces memory usage, but often suffers from slow convergence and degraded performance. In this work, we propose HOSL, a novel Hybrid-Order Split Learning framework that addresses this fundamental trade-off between memory efficiency and optimization effectiveness by strategically integrating ZO optimization on the client side with FO optimization on the server side. By employing memory-efficient ZO gradient estimation at the client, HOSL eliminates backpropagation and activation storage, reducing client memory consumption. Meanwhile, server-side FO optimization ensures fast convergence and competitive performance. Theoretically, we show that HOSL achieves a $\mathcal{O}(\sqrt{d_c/TQ})$ rate, which depends on client-side model dimension $d_c$ rather than the full model dimension $d$, demonstrating that convergence improves as more computation is offloaded to the server. Extensive experiments on OPT models (125M and 1.3B parameters) across 6 tasks demonstrate that HOSL reduces client GPU memory by up to 3.7$\times$ compared to the FO method while achieving accuracy within 0.20%-4.23% of this baseline. Furthermore, HOSL outperforms the ZO baseline by up to 15.55%, validating the effectiveness of our hybrid strategy for memory-efficient training on edge devices.

</details>


### [41] [Multivariate LSTM-Based Forecasting for Renewable Energy: Enhancing Climate Change Mitigation](https://arxiv.org/abs/2601.10961)
*Farshid Kamrani,Kristen Schell*

Main category: cs.LG

TL;DR: 提出基于多元LSTM的可再生能源发电预测模型，利用本地及邻近区域历史数据提升预测精度，降低碳排放并提高供电可靠性


<details>
  <summary>Details</summary>
Motivation: 可再生能源（RESs）在现代电力系统中的集成度不断提高，但其固有的波动性给系统运行带来挑战。准确的RES发电预测对维持电力系统的可靠性、稳定性和经济效率至关重要。传统方法（如确定性方法和随机规划）常依赖K-means等聚类技术生成代表性场景，但可能无法完全捕捉RES数据中的复杂时间依赖性和非线性模式。

Method: 提出一种基于多元长短期记忆（LSTM）网络的预测模型，利用可再生能源的实际历史数据进行发电预测。该模型能有效捕捉长期依赖关系以及不同可再生能源之间的相互作用，同时利用本地和邻近区域的历史数据来提升预测准确性。

Result: 案例研究表明，所提出的预测方法能够降低二氧化碳排放，并提供更可靠的电力负荷供应。

Conclusion: 多元LSTM模型能够有效解决可再生能源发电预测中的复杂时间依赖性和非线性模式问题，通过利用本地及邻近区域历史数据，显著提升预测精度，从而降低碳排放并提高电力系统供电可靠性。

Abstract: The increasing integration of renewable energy sources (RESs) into modern power systems presents significant opportunities but also notable challenges, primarily due to the inherent variability of RES generation. Accurate forecasting of RES generation is crucial for maintaining the reliability, stability, and economic efficiency of power system operations. Traditional approaches, such as deterministic methods and stochastic programming, frequently depend on representative scenarios generated through clustering techniques like K-means. However, these methods may fail to fully capture the complex temporal dependencies and non-linear patterns within RES data. This paper introduces a multivariate Long Short-Term Memory (LSTM)-based network designed to forecast RESs generation using their real-world historical data. The proposed model effectively captures long-term dependencies and interactions between different RESs, utilizing historical data from both local and neighboring areas to enhance predictive accuracy. In the case study, we showed that the proposed forecasting approach results in lower CO2 emissions, and a more reliable supply of electric loads.

</details>


### [42] [Transient learning dynamics drive escape from sharp valleys in Stochastic Gradient Descent](https://arxiv.org/abs/2601.10962)
*Ning Yang,Yikuan Zhang,Qi Ouyang,Chao Tang,Yuhai Tu*

Main category: cs.LG

TL;DR: SGD通过非平衡机制选择更平坦的解，噪声重塑损失景观为有效势能，平坦解更受青睐，训练后期存在瞬态冻结机制


<details>
  <summary>Details</summary>
Motivation: 虽然SGD是深度学习的核心，但其偏好更平坦、更具泛化性解的动态机制尚不清楚。本文旨在揭示SGD选择平坦解的非平衡机制，理解学习动态、损失景观几何与泛化之间的物理联系。

Method: 通过分析SGD学习动态，使用数值实验观察SGD轨迹行为，并构建可处理的物理模型来研究SGD噪声如何重塑损失景观为有效势能。

Result: 发现SGD存在瞬态探索阶段，轨迹反复逃离尖锐谷并转向平坦区域；SGD噪声将景观重塑为偏好平坦解的有效势能；存在瞬态冻结机制，随着训练进行，能量壁垒增长抑制谷间转移；增加SGD噪声强度延迟冻结，增强向平坦最小值的收敛。

Conclusion: 研究提供了连接学习动态、损失景观几何和泛化的统一物理框架，为设计更有效的优化算法提供了原则性指导。

Abstract: Stochastic gradient descent (SGD) is central to deep learning, yet the dynamical origin of its preference for flatter, more generalizable solutions remains unclear. Here, by analyzing SGD learning dynamics, we identify a nonequilibrium mechanism governing solution selection. Numerical experiments reveal a transient exploratory phase in which SGD trajectories repeatedly escape sharp valleys and transition toward flatter regions of the loss landscape. By using a tractable physical model, we show that the SGD noise reshapes the landscape into an effective potential that favors flat solutions. Crucially, we uncover a transient freezing mechanism: as training proceeds, growing energy barriers suppress inter-valley transitions and ultimately trap the dynamics within a single basin. Increasing the SGD noise strength delays this freezing, which enhances convergence to flatter minima. Together, these results provide a unified physical framework linking learning dynamics, loss-landscape geometry, and generalization, and suggest principles for the design of more effective optimization algorithms.

</details>


### [43] [Reasoning Distillation for Lightweight Automated Program Repair](https://arxiv.org/abs/2601.10987)
*Aanand Balasubramanian,Sashank Silwal*

Main category: cs.LG

TL;DR: 轻量级符号推理监督能提升紧凑型自动程序修复模型的修复类型分类性能，通过推理蒸馏方法让大模型提供结构化推理标签，在小模型上实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 小型代码模型适合资源受限环境，但通常只产生单一预测，不清楚它们是否真正学习程序结构还是依赖浅层相关性。需要提升轻量级程序修复模型的解释性和鲁棒性。

Method: 提出推理蒸馏方法：大型教师模型提供结构化符号推理标签和修复类型标签，这些标签捕获bug的高层因果属性而不依赖自由形式解释。在IntroClass基准上训练CodeT5学生模型，比较纯标签训练和推理蒸馏两种设置。

Result: 推理监督持续提升宏平均性能，特别是在较少出现的bug类别上，且不增加模型大小或复杂度。正确推理轨迹与正确预测强相关，但不完全决定预测结果。

Conclusion: 符号推理蒸馏是提升轻量级程序修复模型解释性和鲁棒性的实用方法，能帮助小模型学习更有意义的程序结构而非浅层相关性。

Abstract: We study whether lightweight symbolic reasoning supervision can improve fix type classification in compact automated program repair models. Small code models are attractive for resource-constrained settings, but they typically produce only a single prediction, making it unclear whether they learn meaningful program structure or rely on shallow correlations. We propose a reasoning distillation approach in which a large teacher model provides structured symbolic reasoning tags alongside fix-type labels. These tags capture high-level causal properties of bugs without relying on free-form explanations. We train a CodeT5-based student model under label-only and reasoning-distilled settings on the IntroClass benchmark. Reasoning supervision consistently improves macro averaged performance, particularly on less frequent bug categories, without increasing model size or complexity. We further analyze the relationship between reasoning accuracy and fix-type prediction, showing that correct reasoning traces strongly correlate with correct predictions, while not fully determining them. Our results suggest that symbolic reasoning distillation is a practical way to improve interpretability and robustness in lightweight program repair models.

</details>


### [44] [Constant Metric Scaling in Riemannian Computation](https://arxiv.org/abs/2601.10992)
*Kisung You*

Main category: cs.LG

TL;DR: 本文澄清了黎曼度量常数缩放对计算几何的影响，区分了会变化的量（范数、距离、体积等）和不变的几何对象（联络、测地线、指数映射等），并讨论了在黎曼优化中的意义。


<details>
  <summary>Details</summary>
Motivation: 在计算几何中，黎曼度量的常数缩放经常出现，但实践中其影响常被混淆或误解。作者旨在澄清这一基本操作的实际影响，区分哪些量会变化、哪些几何结构保持不变。

Method: 提供简洁自洽的理论分析，系统考察常数度量缩放对黎曼流形上各种数学对象的影响。通过理论推导区分不变与变化的量，并讨论在黎曼优化算法中的具体含义。

Result: 明确区分：变化的量包括范数、距离、体积元素、梯度大小；不变的几何对象包括Levi-Civita联络、测地线、指数映射、对数映射、平行移动。在优化中，常数缩放可解释为步长的全局重缩放而非几何结构的改变。

Conclusion: 常数度量缩放是引入全局尺度参数的简便方法，不影响核心几何结构。在黎曼计算中，这为调整算法参数提供了灵活性，同时保持几何不变性，有助于澄清实践中的常见混淆。

Abstract: Constant rescaling of a Riemannian metric appears in many computational settings, often through a global scale parameter that is introduced either explicitly or implicitly. Although this operation is elementary, its consequences are not always made clear in practice and may be confused with changes in curvature, manifold structure, or coordinate representation. In this note we provide a short, self-contained account of constant metric scaling on arbitrary Riemannian manifolds. We distinguish between quantities that change under such a scaling, including norms, distances, volume elements, and gradient magnitudes, and geometric objects that remain invariant, such as the Levi--Civita connection, geodesics, exponential and logarithmic maps, and parallel transport. We also discuss implications for Riemannian optimization, where constant metric scaling can often be interpreted as a global rescaling of step sizes rather than a modification of the underlying geometry. The goal of this note is purely expository and is intended to clarify how a global metric scale parameter can be introduced in Riemannian computation without altering the geometric structures on which these methods rely.

</details>


### [45] [Backdoor Attacks on Multi-modal Contrastive Learning](https://arxiv.org/abs/2601.11006)
*Simi D Kuniyilh,Rita Machacy*

Main category: cs.LG

TL;DR: 本文对对比学习中的后门攻击进行了全面比较性综述，分析了威胁模型、攻击方法、目标领域和现有防御措施，强调了对比学习的特定漏洞，并讨论了安全部署的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 对比学习已成为跨领域自监督表示学习的主流方法，但近期研究表明对比学习容易受到后门和数据中毒攻击。攻击者可以通过操纵预训练数据或模型更新来插入隐藏的恶意行为，这对工业分布式环境中的系统安全部署构成严重威胁。

Method: 本文采用系统性综述方法，对对比学习中的后门攻击进行深入分析。具体包括：1) 分析不同的威胁模型；2) 比较各种攻击方法；3) 考察目标应用领域；4) 评估现有防御措施；5) 识别对比学习的特定漏洞。

Result: 研究发现对比学习存在特定的安全漏洞，攻击者可以通过数据污染和模型更新操纵来实施后门攻击。论文总结了该领域的最新进展，揭示了对比学习在视觉、多模态、图学习和联邦学习等不同领域中的安全风险。

Conclusion: 对比学习的安全漏洞对工业分布式系统的安全部署具有重要影响。未来研究需要开发更有效的防御机制，加强对抗性攻击的鲁棒性，并建立更完善的安全评估框架，以确保对比学习在实际应用中的安全性。

Abstract: Contrastive learning has become a leading self- supervised approach to representation learning across domains, including vision, multimodal settings, graphs, and federated learning. However, recent studies have shown that contrastive learning is susceptible to backdoor and data poisoning attacks. In these attacks, adversaries can manipulate pretraining data or model updates to insert hidden malicious behavior. This paper offers a thorough and comparative review of backdoor attacks in contrastive learning. It analyzes threat models, attack methods, target domains, and available defenses. We summarize recent advancements in this area, underline the specific vulnerabilities inherent to contrastive learning, and discuss the challenges and future research directions. Our findings have significant implications for the secure deployment of systems in industrial and distributed environments.

</details>


### [46] [Combating Spurious Correlations in Graph Interpretability via Self-Reflection](https://arxiv.org/abs/2601.11021)
*Kecheng Cai,Chenyang Xu,Chao Peng*

Main category: cs.LG

TL;DR: 提出自反思框架提升图学习可解释性，特别针对Spurious-Motif基准测试中的虚假相关性问题


<details>
  <summary>Details</summary>
Motivation: 现有图学习可解释性方法在包含虚假相关的Spurious-Motif基准上表现不佳，需要提升模型区分真实相关结构与误导模式的能力

Method: 提出自反思框架，将现有方法的节点/边重要性得分反馈回原方法进行第二轮评估，并基于反馈机制提出微调训练方法

Result: 自反思技术能有效提升图学习在Spurious-Motif数据集上的可解释性，改善模型区分真实相关与虚假相关的能力

Conclusion: 借鉴大语言模型的自反思技术可有效提升图学习在复杂虚假相关场景下的可解释性，为图表示学习提供新思路

Abstract: Interpretable graph learning has recently emerged as a popular research topic in machine learning. The goal is to identify the important nodes and edges of an input graph that are crucial for performing a specific graph reasoning task. A number of studies have been conducted in this area, and various benchmark datasets have been proposed to facilitate evaluation. Among them, one of the most challenging is the Spurious-Motif benchmark, introduced at ICLR 2022. The datasets in this synthetic benchmark are deliberately designed to include spurious correlations, making it particularly difficult for models to distinguish truly relevant structures from misleading patterns. As a result, existing methods exhibit significantly worse performance on this benchmark compared to others.
  In this paper, we focus on improving interpretability on the challenging Spurious-Motif datasets. We demonstrate that the self-reflection technique, commonly used in large language models to tackle complex tasks, can also be effectively adapted to enhance interpretability in datasets with strong spurious correlations. Specifically, we propose a self-reflection framework that can be integrated with existing interpretable graph learning methods. When such a method produces importance scores for each node and edge, our framework feeds these predictions back into the original method to perform a second round of evaluation. This iterative process mirrors how large language models employ self-reflective prompting to reassess their previous outputs. We further analyze the reasons behind this improvement from the perspective of graph representation learning, which motivates us to propose a fine-tuning training method based on this feedback mechanism.

</details>


### [47] [Matching High-Dimensional Geometric Quantiles for Test-Time Adaptation of Transformers and Convolutional Networks Alike](https://arxiv.org/abs/2601.11022)
*Sravan Danda,Aditya Challa,Shlok Mehendale,Snehanshu Saha*

Main category: cs.LG

TL;DR: 提出一种架构无关的测试时自适应方法，通过添加适配器网络预处理输入图像，使用分位数损失训练，匹配高维几何分位数来校正分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法大多依赖修改分类器权重，与架构紧密相关，难以扩展到通用架构。需要一种架构无关的方法来处理测试数据分布与训练数据分布略有不同的情况。

Method: 提出架构无关的测试时自适应方法：1）添加适配器网络预处理输入图像；2）使用提出的分位数损失训练适配器；3）通过匹配高维几何分位数来校正分布偏移。

Result: 在CIFAR10-C、CIFAR100-C和TinyImageNet-C数据集上验证了方法有效性，训练了经典卷积网络和Transformer网络，证明了理论上的最优适配器学习条件。

Conclusion: 该方法提供了一种架构无关的测试时自适应解决方案，通过分位数损失和适配器网络有效处理分布偏移问题，具有理论保证和实验验证。

Abstract: Test-time adaptation (TTA) refers to adapting a classifier for the test data when the probability distribution of the test data slightly differs from that of the training data of the model. To the best of our knowledge, most of the existing TTA approaches modify the weights of the classifier relying heavily on the architecture. It is unclear as to how these approaches are extendable to generic architectures. In this article, we propose an architecture-agnostic approach to TTA by adding an adapter network pre-processing the input images suitable to the classifier. This adapter is trained using the proposed quantile loss. Unlike existing approaches, we correct for the distribution shift by matching high-dimensional geometric quantiles. We prove theoretically that under suitable conditions minimizing quantile loss can learn the optimal adapter. We validate our approach on CIFAR10-C, CIFAR100-C and TinyImageNet-C by training both classic convolutional and transformer networks on CIFAR10, CIFAR100 and TinyImageNet datasets.

</details>


### [48] [AVP-Pro: An Adaptive Multi-Modal Fusion and Contrastive Learning Approach for Comprehensive Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2601.11028)
*Xinru Wen,Weizhong Lin,zi liu,Xuan Xiao*

Main category: cs.LG

TL;DR: AVP-Pro是一个两阶段抗病毒肽预测框架，通过自适应特征融合和对比学习提高预测准确性，在通用AVP识别和病毒家族分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉复杂序列依赖性和区分高相似度样本方面存在局限，需要更准确的抗病毒肽识别工具来支持新药开发。

Method: 提出两阶段框架：第一阶段使用包含10种描述符的全景特征空间，结合CNN、BiLSTM、自注意力和自适应门控机制进行特征融合；第二阶段采用基于BLOSUM62增强的OHEM对比学习策略，结合迁移学习进行病毒家族分类。

Result: 第一阶段通用AVP识别准确率0.9531，MCC 0.9064，优于现有SOTA方法；第二阶段在小样本条件下成功分类6个病毒家族和8种特定病毒。

Conclusion: AVP-Pro为高通量抗病毒药物筛选提供了强大且可解释的新工具，已开发用户友好的Web界面供使用。

Abstract: The accurate identification of antiviral peptides (AVPs) is crucial for novel drug development. However, existing methods still have limitations in capturing complex sequence dependencies and distinguishing confusing samples with high similarity. To address these challenges, we propose AVP-Pro, a novel two-stage predictive framework that integrates adaptive feature fusion and contrastive learning. To comprehensively capture the physicochemical properties and deep-seated patterns of peptide sequences, we constructed a panoramic feature space encompassing 10 distinct descriptors and designed a hierarchical fusion architecture. This architecture integrates self-attention and adaptive gating mechanisms to dynamically modulate the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Targeting the blurred decision boundary caused by the high similarity between positive and negative sample sequences, we adopted an Online Hard Example Mining (OHEM)-driven contrastive learning strategy enhanced by BLOSUM62. This approach significantly sharpened the model's discriminative power. Model evaluation results show that in the first stage of general AVP identification, the model achieved an accuracy of 0.9531 and an MCC of 0.9064, outperforming existing state-of-the-art (SOTA) methods. In the second stage of functional subtype prediction, combined with a transfer learning strategy, the model realized accurate classification of 6 viral families and 8 specific viruses under small-sample conditions. AVP-Pro provides a powerful and interpretable new tool for the high-throughput screening of antiviral drugs. To further enhance accessibility for users, we have developed a user-friendly web interface, which is available at https://wwwy1031-avp-pro.hf.space.

</details>


### [49] [Self-Augmented Mixture-of-Experts for QoS Prediction](https://arxiv.org/abs/2601.11036)
*Kecheng Cai,Chao Peng,Chenyang Xu,Xia Chen*

Main category: cs.LG

TL;DR: 提出一种自增强混合专家模型用于QoS预测，通过迭代反馈机制解决用户-服务交互稀疏性问题


<details>
  <summary>Details</summary>
Motivation: QoS预测是服务计算和个性化推荐中的基础问题，但用户-服务交互数据通常非常稀疏，只有少量反馈值被观察到，这给准确预测带来了挑战

Method: 提出自增强策略，利用模型自身预测进行迭代精炼：部分掩码预测值并反馈给模型重新预测。基于此设计自增强混合专家模型，多个专家网络迭代协作估计QoS值，通过专家间通信实现预测精炼

Result: 在基准数据集上的实验表明，该方法优于现有基线方法，取得了有竞争力的结果

Conclusion: 自增强混合专家模型通过迭代反馈机制有效解决了QoS预测中的数据稀疏性问题，为服务计算和推荐系统提供了有效的解决方案

Abstract: Quality of Service (QoS) prediction is one of the most fundamental problems in service computing and personalized recommendation. In the problem, there is a set of users and services, each associated with a set of descriptive features. Interactions between users and services produce feedback values, typically represented as numerical QoS metrics such as response time or availability. Given the observed feedback for a subset of user-service pairs, the goal is to predict the QoS values for the remaining pairs.
  A key challenge in QoS prediction is the inherent sparsity of user-service interactions, as only a small subset of feedback values is typically observed. To address this, we propose a self-augmented strategy that leverages a model's own predictions for iterative refinement. In particular, we partially mask the predicted values and feed them back into the model to predict again. Building on this idea, we design a self-augmented mixture-of-experts model, where multiple expert networks iteratively and collaboratively estimate QoS values. We find that the iterative augmentation process naturally aligns with the MoE architecture by enabling inter-expert communication: in the second round, each expert receives the first-round predictions and refines its output accordingly. Experiments on benchmark datasets show that our method outperforms existing baselines and achieves competitive results.

</details>


### [50] [OpFML: Pipeline for ML-based Operational Forecasting](https://arxiv.org/abs/2601.11046)
*Shahbaz Alvi,Giusy Fedele,Gabriele Accarino,Italo Epicoco,Ilenia Manco,Pasquale Schiano*

Main category: cs.LG

TL;DR: OpFML是一个可配置的机器学习管道，用于周期性预测，特别应用于野火危险指数预测


<details>
  <summary>Details</summary>
Motivation: 传统野火风险评估方法经常高估风险，机器学习在气候和地球科学领域有广泛应用潜力，需要可配置的周期性预测系统

Method: 开发了OpFML（Operational Forecasting with Machine Learning）管道，这是一个可配置和可适应的机器学习模型部署框架，用于周期性预测

Result: 成功开发了OpFML管道，并展示了其在每日火灾危险指数预测中的应用能力，突出了其各种功能特性

Conclusion: OpFML为机器学习在周期性预测任务中提供了一个灵活实用的解决方案，特别适用于野火危险评估等气候科学应用

Abstract: Machine learning is finding its application in a multitude of areas in science and research, and Climate and Earth Sciences is no exception to this trend. Operational forecasting systems based on data-driven approaches and machine learning methods deploy models for periodic forecasting. Wildfire danger assessment using machine learning has garnered significant interest in the last decade, as conventional methods often overestimate the risk of wildfires. In this work, we present the code OpFML: Operational Forecasting with Machine Learning. OpFML is a configurable and adaptable pipeline that can be utilized to serve a machine learning model for periodic forecasting. We further demonstrate the capabilities of the pipeline through its application to daily Fire Danger Index forecasting and outline its various features.

</details>


### [51] [Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs](https://arxiv.org/abs/2601.11061)
*Lecheng Yan,Ruizhe Li,Guanhua Chen,Qing Li,Jiahui Geng,Wenxi Li,Vincent Wang,Chris Lee*

Main category: cs.LG

TL;DR: 研究发现RLVR训练中即使使用虚假奖励也能提升性能，这是因为模型形成了"困惑度悖论"：答案标记困惑度下降但提示侧连贯性退化，模型通过记忆捷径绕过推理过程。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR能有效提升LLM推理能力，但近期证据显示即使使用虚假或不正确的奖励，模型也能获得显著提升。研究者希望探究这一现象背后的机制，理解模型如何绕过正常推理过程。

Method: 使用Path Patching、Logit Lens、JSD分析和神经微分方程等技术，揭示了隐藏的Anchor-Adapter电路机制。通过定位功能锚点（L18-20层）和结构适配器（L21+层），并缩放特定MLP键值来双向因果操控。

Result: 发现了"困惑度悖论"现象，识别出模型通过记忆捷径绕过推理的具体电路机制。通过操控电路中的MLP键值，可以人为放大或抑制污染驱动的性能提升。

Conclusion: 研究为识别和缓解RLVR调优模型中的数据污染提供了机制性路线图，揭示了模型如何形成记忆捷径的神经机制，有助于开发更可靠的RLVR训练方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a "Perplexity Paradox": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.

</details>


### [52] [Bridging Cognitive Neuroscience and Graph Intelligence: Hippocampus-Inspired Multi-View Hypergraph Learning for Web Finance Fraud](https://arxiv.org/abs/2601.11073)
*Rongkun Cui,Nana Zhang,Kun Zhu,Qi Zhang*

Main category: cs.LG

TL;DR: HIMVH：受海马体启发的多视图超图学习模型，用于网络金融欺诈检测，通过跨视图不一致性感知和新颖性感知超图学习，解决欺诈伪装和长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: 网络金融服务面临欺诈伪装（恶意交易模仿良性行为）和长尾数据分布（罕见但关键的欺诈案例被掩盖）两大挑战，现有基于图神经网络的检测方法难以有效应对。

Method: 提出HIMVH模型：1）受海马体场景冲突监控启发，设计跨视图不一致性感知模块，捕捉多交易视图间的细微差异和行为异质性；2）受CA1区匹配-不匹配新颖性检测机制启发，引入新颖性感知超图学习模块，测量特征与邻域期望的偏差并自适应重加权消息。

Result: 在六个基于网络的金融欺诈数据集上，HIMVH相比15个SOTA模型平均提升：AUC 6.42%、F1 9.74%、AP 39.14%。

Conclusion: HIMVH通过生物启发的方法有效解决了金融欺诈检测中的伪装和长尾分布问题，显著提升了检测性能，为网络金融安全提供了新方案。

Abstract: Online financial services constitute an essential component of contemporary web ecosystems, yet their openness introduces substantial exposure to fraud that harms vulnerable users and weakens trust in digital finance. Such threats have become a significant web harm that erodes societal fairness and affects the well being of online communities. However, existing detection methods based on graph neural networks (GNNs) struggle with two persistent challenges: (1) fraud camouflage, where malicious transactions mimic benign behaviors to evade detection, and (2) long-tailed data distributions, which obscure rare but critical fraudulent cases. To fill these gaps, we propose HIMVH, a Hippocampus-Inspired Multi-View Hypergraph learning model for web finance fraud detection. Specifically, drawing inspiration from the scene conflict monitoring role of the hippocampus, we design a cross-view inconsistency perception module that captures subtle discrepancies and behavioral heterogeneity across multiple transaction views. This module enables the model to identify subtle cross-view conflicts for detecting online camouflaged fraudulent behaviors. Furthermore, inspired by the match-mismatch novelty detection mechanism of the CA1 region, we introduce a novelty-aware hypergraph learning module that measures feature deviations from neighborhood expectations and adaptively reweights messages, thereby enhancing sensitivity to online rare fraud patterns in the long-tailed settings. Extensive experiments on six web-based financial fraud datasets demonstrate that HIMVH achieves 6.42\% improvement in AUC, 9.74\% in F1 and 39.14\% in AP on average over 15 SOTA models.

</details>


### [53] [Soft Bayesian Context Tree Models for Real-Valued Time Series](https://arxiv.org/abs/2601.11079)
*Shota Saito,Yuta Nakahara,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: 提出Soft-BCT模型，用于实值时间序列，采用软（概率）分割上下文空间而非硬分割，基于变分推断学习，在真实数据集上表现优于或等同于传统BCT。


<details>
  <summary>Details</summary>
Motivation: 传统BCT模型对实值时间序列使用硬分割上下文空间，可能不够灵活。Soft-BCT通过引入软分割来提高模型的适应性和表达能力。

Method: 提出Soft-BCT模型，采用概率分割上下文空间而非确定性分割。基于变分推断设计学习算法，通过优化变分下界来估计模型参数。

Result: 在多个真实数据集上，Soft-BCT表现出与传统BCT相当或更优的性能，验证了软分割方法的有效性。

Conclusion: Soft-BCT通过软分割上下文空间改进了传统BCT模型，在实值时间序列建模中表现出更好的灵活性和性能。

Abstract: This paper proposes the soft Bayesian context tree model (Soft-BCT), which is a novel BCT model for real-valued time series. The Soft-BCT considers soft (probabilistic) splits of the context space, instead of hard (deterministic) splits of the context space as in the previous BCT for real-valued time series. A learning algorithm of the Soft-BCT is proposed based on the variational inference. For some real-world datasets, the Soft-BCT demonstrates almost the same or superior performance to the previous BCT.

</details>


### [54] [Differentially Private Subspace Fine-Tuning for Large Language Models](https://arxiv.org/abs/2601.11113)
*Lele Zheng,Xiang Wang,Tao Zhang,Yang Cao,Ke Cheng,Yulong Shen*

Main category: cs.LG

TL;DR: DP-SFT：一种两阶段子空间微调方法，通过在低维任务特定子空间中注入差分隐私噪声，显著降低噪声影响，在保持隐私保护的同时提升模型性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调通常依赖敏感数据，存在隐私风险。传统差分隐私方法在高维参数空间中注入噪声会导致噪声范数过大，降低性能并破坏训练稳定性

Method: 两阶段子空间微调方法：第一阶段通过分析主梯度方向识别低维任务特定子空间；第二阶段将完整梯度投影到该子空间，添加差分隐私噪声，然后将扰动后的梯度映射回原始参数空间进行模型更新

Result: 在多个数据集上的实验表明，DP-SFT在严格的差分隐私约束下提高了准确性和稳定性，加速了收敛速度，相比基线差分隐私微调方法取得了显著提升

Conclusion: DP-SFT通过将差分隐私噪声限制在低维任务特定子空间中，有效降低了噪声影响，在保持严格隐私保护的同时显著提升了微调性能，为解决隐私保护与模型性能之间的权衡提供了有效方案

Abstract: Fine-tuning large language models on downstream tasks is crucial for realizing their cross-domain potential but often relies on sensitive data, raising privacy concerns. Differential privacy (DP) offers rigorous privacy guarantees and has been widely adopted in fine-tuning; however, naively injecting noise across the high-dimensional parameter space creates perturbations with large norms, degrading performance and destabilizing training. To address this issue, we propose DP-SFT, a two-stage subspace fine-tuning method that substantially reduces noise magnitude while preserving formal DP guarantees. Our intuition is that, during fine-tuning, significant parameter updates lie within a low-dimensional, task-specific subspace, while other directions change minimally. Hence, we only inject DP noise into this subspace to protect privacy without perturbing irrelevant parameters. In phase one, we identify the subspace by analyzing principal gradient directions to capture task-specific update signals. In phase two, we project full gradients onto this subspace, add DP noise, and map the perturbed gradients back to the original parameter space for model updates, markedly lowering noise impact. Experiments on multiple datasets demonstrate that DP-SFT enhances accuracy and stability under rigorous DP constraints, accelerates convergence, and achieves substantial gains over DP fine-tuning baselines.

</details>


### [55] [Optimized Algorithms for Text Clustering with LLM-Generated Constraints](https://arxiv.org/abs/2601.11118)
*Chaoqi Jia,Weihong Wu,Longkun Guo,Zhigang Lu,Chao Chen,Kok-Leong Ong*

Main category: cs.LG

TL;DR: 提出基于大语言模型的约束生成方法，通过生成约束集而非传统成对约束，减少资源消耗并提高约束准确性，同时设计针对LLM生成约束的聚类算法


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法通过人工标注的must-link和cannot-link约束来提高准确性，但人工标注成本高。随着大语言模型的发展，希望通过LLM自动生成约束来改善聚类质量，但现有方法在查询效率和约束准确性方面仍有不足

Method: 1) 提出新颖的约束生成方法，生成约束集而非成对约束，提高查询效率和约束准确性；2) 设计针对LLM生成约束的约束聚类算法，引入置信度阈值和惩罚机制处理可能不准确的约束

Result: 在五个文本数据集上的评估显示，该方法在保持与最先进算法相当的聚类准确性的同时，将LLM查询次数减少了20倍以上，显著降低了约束生成成本

Conclusion: 提出的基于LLM的约束生成和聚类方法有效平衡了聚类质量和资源消耗，为文本聚类提供了一种高效且准确的解决方案，显著减少了LLM查询成本

Abstract: Clustering is a fundamental tool that has garnered significant interest across a wide range of applications including text analysis. To improve clustering accuracy, many researchers have incorporated background knowledge, typically in the form of must-link and cannot-link constraints, to guide the clustering process. With the recent advent of large language models (LLMs), there is growing interest in improving clustering quality through LLM-based automatic constraint generation. In this paper, we propose a novel constraint-generation approach that reduces resource consumption by generating constraint sets rather than using traditional pairwise constraints. This approach improves both query efficiency and constraint accuracy compared to state-of-the-art methods. We further introduce a constrained clustering algorithm tailored to the characteristics of LLM-generated constraints. Our method incorporates a confidence threshold and a penalty mechanism to address potentially inaccurate constraints. We evaluate our approach on five text datasets, considering both the cost of constraint generation and the overall clustering performance. The results show that our method achieves clustering accuracy comparable to the state-of-the-art algorithms while reducing the number of LLM queries by more than 20 times.

</details>


### [56] [Shape-morphing programming of soft materials on complex geometries via neural operator](https://arxiv.org/abs/2601.11126)
*Lu Chen,Gengxiang Chen,Xu Liu,Jingyan Su,Xuhao Lyu,Lihui Wang,Yingguang Li*

Main category: cs.LG

TL;DR: 提出S2NO神经算子，结合谱方法和空间卷积，实现复杂几何体上的高保真形变预测，结合进化算法优化材料分布设计


<details>
  <summary>Details</summary>
Motivation: 现有形状变形设计主要针对简单几何体，难以在复杂几何体上实现精确多样的形变设计，限制了在植入物部署、气动变形等高级应用中的潜力

Method: 提出Spectral and Spatial Neural Operator (S2NO)，整合拉普拉斯特征函数编码和空间卷积，捕捉不规则计算域上的全局和局部形变行为；结合进化算法进行体素级材料分布优化

Result: S2NO能够在各种复杂几何体（不规则边界、多孔结构、薄壁结构）上实现高保真形变预测；神经算子的离散不变性支持超分辨率材料分布设计，扩展了形变设计的多样性和复杂性

Conclusion: S2NO显著提高了编程复杂形状变形的效率和能力，为高级应用如保形植入物部署和气动变形提供了有效工具

Abstract: Shape-morphing soft materials can enable diverse target morphologies through voxel-level material distribution design, offering significant potential for various applications. Despite progress in basic shape-morphing design with simple geometries, achieving advanced applications such as conformal implant deployment or aerodynamic morphing requires accurate and diverse morphing designs on complex geometries, which remains challenging. Here, we present a Spectral and Spatial Neural Operator (S2NO), which enables high-fidelity morphing prediction on complex geometries. S2NO effectively captures global and local morphing behaviours on irregular computational domains by integrating Laplacian eigenfunction encoding and spatial convolutions. Combining S2NO with evolutionary algorithms enables voxel-level optimisation of material distributions for shape morphing programming on various complex geometries, including irregular-boundary shapes, porous structures, and thin-walled structures. Furthermore, the neural operator's discretisation-invariant property enables super-resolution material distribution design, further expanding the diversity and complexity of morphing design. These advancements significantly improve the efficiency and capability of programming complex shape morphing.

</details>


### [57] [Context-aware Graph Causality Inference for Few-Shot Molecular Property Prediction](https://arxiv.org/abs/2601.11135)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: CaMol是一个基于因果推理的上下文感知图学习框架，用于少样本分子属性预测，通过发现因果子结构并消除混杂因素来提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习的分子属性预测方法存在两个主要局限：1) 未能充分利用与属性因果相关的官能团先验知识；2) 难以识别与属性直接相关的关键子结构。这些限制在少样本场景下尤为突出。

Method: CaMol采用因果推理视角，假设分子包含决定特定属性的潜在因果结构。方法包括：1) 构建编码化学知识的上下文图，连接官能团、分子和属性；2) 提出可学习的原子掩码策略，从混杂结构中解耦因果子结构；3) 引入分布干预器，通过后门调整结合因果子结构和化学基础的混杂因素。

Result: 在多个分子数据集上的实验表明，CaMol在少样本任务中实现了优越的准确性和样本效率，显示出对未见属性的良好泛化能力。发现的因果子结构与官能团的化学知识高度一致，支持模型的可解释性。

Conclusion: CaMol通过因果推理框架有效解决了少样本分子属性预测中的关键挑战，不仅提高了预测性能，还增强了模型的可解释性，为基于Web的蛋白质结构预测和药物发现等服务提供了有力工具。

Abstract: Molecular property prediction is becoming one of the major applications of graph learning in Web-based services, e.g., online protein structure prediction and drug discovery. A key challenge arises in few-shot scenarios, where only a few labeled molecules are available for predicting unseen properties. Recently, several studies have used in-context learning to capture relationships among molecules and properties, but they face two limitations in: (1) exploiting prior knowledge of functional groups that are causally linked to properties and (2) identifying key substructures directly correlated with properties. We propose CaMol, a context-aware graph causality inference framework, to address these challenges by using a causal inference perspective, assuming that each molecule consists of a latent causal structure that determines a specific property. First, we introduce a context graph that encodes chemical knowledge by linking functional groups, molecules, and properties to guide the discovery of causal substructures. Second, we propose a learnable atom masking strategy to disentangle causal substructures from confounding ones. Third, we introduce a distribution intervener that applies backdoor adjustment by combining causal substructures with chemically grounded confounders, disentangling causal effects from real-world chemical variations. Experiments on diverse molecular datasets showed that CaMol achieved superior accuracy and sample efficiency in few-shot tasks, showing its generalizability to unseen properties. Also, the discovered causal substructures were strongly aligned with chemical knowledge about functional groups, supporting the model interpretability.

</details>


### [58] [Assesing the Viability of Unsupervised Learning with Autoencoders for Predictive Maintenance in Helicopter Engines](https://arxiv.org/abs/2601.11154)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 比较直升机发动机预测性维护的两种方法：监督分类与基于自编码器的无监督异常检测，评估它们在真实数据上的表现和适用场景。


<details>
  <summary>Details</summary>
Motivation: 直升机发动机的意外故障会导致严重的运营中断、安全隐患和高昂维修成本，需要有效的预测性维护策略来降低这些风险。

Method: 比较两种方法：1) 监督分类管道，依赖正常和故障行为的标记数据；2) 基于自编码器的无监督异常检测，仅使用健康发动机数据学习正常操作模型，将偏差标记为潜在故障。

Result: 监督模型在有标注故障数据时表现良好，而自编码器无需故障标签也能实现有效检测，特别适用于故障数据稀缺或不完整的场景。

Conclusion: 研究突出了准确性、数据可用性和部署可行性之间的实际权衡，强调了无监督学习作为航空航天应用中早期故障检测可行解决方案的潜力。

Abstract: Unplanned engine failures in helicopters can lead to severe operational disruptions, safety hazards, and costly repairs. To mitigate these risks, this study compares two predictive maintenance strategies for helicopter engines: a supervised classification pipeline and an unsupervised anomaly detection approach based on autoencoders (AEs). The supervised method relies on labelled examples of both normal and faulty behaviour, while the unsupervised approach learns a model of normal operation using only healthy engine data, flagging deviations as potential faults. Both methods are evaluated on a real-world dataset comprising labelled snapshots of helicopter engine telemetry. While supervised models demonstrate strong performance when annotated failures are available, the AE achieves effective detection without requiring fault labels, making it particularly well suited for settings where failure data are scarce or incomplete. The comparison highlights the practical trade-offs between accuracy, data availability, and deployment feasibility, and underscores the potential of unsupervised learning as a viable solution for early fault detection in aerospace applications.

</details>


### [59] [Theoretically and Practically Efficient Resistance Distance Computation on Large Graphs](https://arxiv.org/abs/2601.11159)
*Yichun Yang,Longlong Lin,Rong-Hua Li,Meihao Liao,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了两种基于Lanczos方法的电阻距离计算新算法：Lanczos Iteration（全局算法）和Lanczos Push（局部算法），显著降低了传统方法对图拉普拉斯矩阵条件数κ的依赖，在大型图分析中实现了更高的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 电阻距离计算在图聚类、链接预测和图神经网络等应用中至关重要，但现有方法（如幂迭代算法和随机游走局部方法）在大型图上效率低下，特别是当图拉普拉斯矩阵条件数κ较大时收敛缓慢。

Method: 提出了两种基于经典Lanczos方法的算法：1) Lanczos Iteration - 近线性时间的全局算法，时间复杂度为Õ(√κ m)；2) Lanczos Push - 时间复杂度与图大小无关的局部算法，在温和假设下时间复杂度为Õ(κ^2.75)。

Result: Lanczos Iteration相比之前的幂迭代全局方法实现了√κ倍的加速，Lanczos Push相比最先进的随机游走局部算法改进了κ^0.25倍。在八个不同规模和统计特性的真实数据集上的实验验证了两种算法在效率和精度上都显著优于现有方法。

Conclusion: 提出的Lanczos Iteration和Lanczos Push算法有效解决了大型图电阻距离计算效率低下的问题，通过降低对条件数κ的依赖，为图分析应用提供了更高效、更精确的解决方案。

Abstract: The computation of resistance distance is pivotal in a wide range of graph analysis applications, including graph clustering, link prediction, and graph neural networks. Despite its foundational importance, efficient algorithms for computing resistance distances on large graphs are still lacking. Existing state-of-the-art (SOTA) methods, including power iteration-based algorithms and random walk-based local approaches, often struggle with slow convergence rates, particularly when the condition number of the graph Laplacian matrix, denoted by $κ$, is large. To tackle this challenge, we propose two novel and efficient algorithms inspired by the classic Lanczos method: Lanczos Iteration and Lanczos Push, both designed to reduce dependence on $κ$. Among them, Lanczos Iteration is a near-linear time global algorithm, whereas Lanczos Push is a local algorithm with a time complexity independent of the size of the graph. More specifically, we prove that the time complexity of Lanczos Iteration is $\tilde{O}(\sqrtκ m)$ ($m$ is the number of edges of the graph and $\tilde{O}$ means the complexity omitting the $\log$ terms) which achieves a speedup of $\sqrtκ$ compared to previous power iteration-based global methods. For Lanczos Push, we demonstrate that its time complexity is $\tilde{O}(κ^{2.75})$ under certain mild and frequently established assumptions, which represents a significant improvement of $κ^{0.25}$ over the SOTA random walk-based local algorithms. We validate our algorithms through extensive experiments on eight real-world datasets of varying sizes and statistical properties, demonstrating that Lanczos Iteration and Lanczos Push significantly outperform SOTA methods in terms of both efficiency and accuracy.

</details>


### [60] [Clustering High-dimensional Data: Balancing Abstraction and Representation Tutorial at AAAI 2026](https://arxiv.org/abs/2601.11160)
*Claudia Plant,Lena G. M. Bauer,Christian Böhm*

Main category: cs.LG

TL;DR: 聚类算法需要在抽象与表示之间找到平衡：抽象去除无关细节，表示强调区分性特征。传统K-means抽象度高但表示简单，而子空间和深度聚类通过更丰富的表示处理高维复杂数据，但需要显式强制抽象以避免仅进行表示学习。


<details>
  <summary>Details</summary>
Motivation: 如何在大规模真实数据集中找到自然分组？聚类需要在抽象（去除个体对象的冗余细节）和表示（强调区分不同群组的关键特征）之间取得平衡。当前方法要么过于抽象（如K-means），要么表示过于复杂而缺乏有效抽象。

Method: 分析不同聚类算法的抽象-表示权衡：K-means采用高抽象（平均细节）和简单表示（高斯分布）；子空间聚类和深度聚类通过更丰富的表示处理复杂数据，但需要显式强制抽象（如基于质心或密度的聚类损失）。子空间聚类通过分离聚类相关信息和无关信息的潜在空间来实现平衡。

Result: 不同聚类算法在抽象-表示谱系上各有定位：传统方法抽象度高但表示能力有限，现代方法表示能力强但需要显式抽象约束。子空间聚类通过分离潜在空间提供了有效的平衡方案。

Conclusion: 未来聚类研究需要更自适应地平衡抽象与表示，以提高性能、能效和可解释性。人脑在聚类和单次学习等任务中能自动找到最佳平衡点，这为算法改进提供了方向。

Abstract: How to find a natural grouping of a large real data set? Clustering requires a balance between abstraction and representation. To identify clusters, we need to abstract from superfluous details of individual objects. But we also need a rich representation that emphasizes the key features shared by groups of objects that distinguish them from other groups of objects.
  Each clustering algorithm implements a different trade-off between abstraction and representation. Classical K-means implements a high level of abstraction - details are simply averaged out - combined with a very simple representation - all clusters are Gaussians in the original data space. We will see how approaches to subspace and deep clustering support high-dimensional and complex data by allowing richer representations. However, with increasing representational expressiveness comes the need to explicitly enforce abstraction in the objective function to ensure that the resulting method performs clustering and not just representation learning. We will see how current deep clustering methods define and enforce abstraction through centroid-based and density-based clustering losses. Balancing the conflicting goals of abstraction and representation is challenging. Ideas from subspace clustering help by learning one latent space for the information that is relevant to clustering and another latent space to capture all other information in the data.
  The tutorial ends with an outlook on future research in clustering. Future methods will more adaptively balance abstraction and representation to improve performance, energy efficiency and interpretability. By automatically finding the sweet spot between abstraction and representation, the human brain is very good at clustering and other related tasks such as single-shot learning. So, there is still much room for improvement.

</details>


### [61] [GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling](https://arxiv.org/abs/2601.11161)
*Pascal Schlachter,Bin Yang*

Main category: cs.LG

TL;DR: GMM-COMET：首个持续源自由通用域适应方法，通过高斯混合模型伪标签和均值教师框架处理多目标域流


<details>
  <summary>Details</summary>
Motivation: 现实场景中，源数据在适应期间可能不再可用，且目标域标签空间可能与源域不同。现有SF-UniDA方法只假设单一域偏移，而实际应用中模型需要连续适应多个不同未标记目标域。

Method: 结合高斯混合模型伪标签和均值教师框架，在均值教师框架内集成GMM伪标签以提高长期适应稳定性，并引入一致性损失增强鲁棒性。

Result: GMM-COMET在所有评估场景中持续改进源模型性能，是实验中唯一在所有情况下都优于源模型的方法。

Conclusion: 该方法为持续SF-UniDA提供了强大的首个基线，有效解决了模型需要连续适应多个不同未标记目标域的挑战。

Abstract: Unsupervised domain adaptation tackles the problem that domain shifts between training and test data impair the performance of neural networks in many real-world applications. Thereby, in realistic scenarios, the source data may no longer be available during adaptation, and the label space of the target domain may differ from the source label space. This setting, known as source-free universal domain adaptation (SF-UniDA), has recently gained attention, but all existing approaches only assume a single domain shift from source to target. In this work, we present the first study on continual SF-UniDA, where the model must adapt sequentially to a stream of multiple different unlabeled target domains. Building upon our previous methods for online SF-UniDA, we combine their key ideas by integrating Gaussian mixture model-based pseudo-labeling within a mean teacher framework for improved stability over long adaptation sequences. Additionally, we introduce consistency losses for further robustness. The resulting method GMM-COMET provides a strong first baseline for continual SF-UniDA and is the only approach in our experiments to consistently improve upon the source-only model across all evaluated scenarios. Our code is available at https://github.com/pascalschlachter/GMM-COMET.

</details>


### [62] [LSTM VS. Feed-Forward Autoencoders for Unsupervised Fault Detection in Hydraulic Pumps](https://arxiv.org/abs/2601.11163)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 提出两种基于自编码器的无监督故障检测方法：前馈模型分析单个传感器快照，LSTM模型捕捉短期时间窗口，仅使用健康数据训练，在包含7个故障区间的测试集上实现高可靠性检测。


<details>
  <summary>Details</summary>
Motivation: 工业液压泵的意外故障会导致生产中断和高昂成本，需要早期故障检测方法来预防这些问题。

Method: 采用两种无监督自编码器方案：1）前馈模型分析单个传感器快照；2）LSTM模型捕捉短期时间窗口。两种网络仅使用52个传感器通道的健康数据训练，不接触故障样本。

Result: 尽管训练阶段未使用故障样本，两种模型在包含7个标注故障区间的测试集上都实现了高可靠性故障检测。

Conclusion: 无监督自编码器方法仅需健康数据即可有效检测工业液压泵早期故障，为预防性维护提供可行方案。

Abstract: Unplanned failures in industrial hydraulic pumps can halt production and incur substantial costs. We explore two unsupervised autoencoder (AE) schemes for early fault detection: a feed-forward model that analyses individual sensor snapshots and a Long Short-Term Memory (LSTM) model that captures short temporal windows. Both networks are trained only on healthy data drawn from a minute-level log of 52 sensor channels; evaluation uses a separate set that contains seven annotated fault intervals. Despite the absence of fault samples during training, the models achieve high reliability.

</details>


### [63] [TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation](https://arxiv.org/abs/2601.11184)
*Xiangyu Xu,Qingsong Zhong,Jilin Hu*

Main category: cs.LG

TL;DR: 提出一个结构解耦的多尺度时间序列生成框架，通过双路径VQ-VAE分离趋势和季节成分，实现从粗到细的自回归生成，在减少参数量的同时生成更高质量的长序列。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析面临数据稀缺和隐私挑战，生成建模提供了有前景的解决方案。然而，时间序列的结构复杂性（包括多尺度时间模式和异构成分）尚未得到充分解决。

Method: 1. 将序列编码为多个时间分辨率的离散token，以从粗到细的方式进行自回归生成；2. 引入双路径VQ-VAE解耦趋势和季节成分，学习语义一致的潜在表示；3. 提出基于引导的重建策略，利用粗粒度季节信号作为先验指导细粒度季节模式的重建。

Result: 在六个数据集上的实验表明，该方法比现有方法生成更高质量的时间序列。模型在显著减少参数数量的情况下实现了强大性能，并在生成高质量长序列方面表现出卓越能力。

Conclusion: 该结构解耦的多尺度生成框架有效解决了时间序列的结构复杂性，在保持参数效率的同时提升了生成质量，为时间序列生成建模提供了新的解决方案。

Abstract: Generative modeling offers a promising solution to data scarcity and privacy challenges in time series analysis. However, the structural complexity of time series, characterized by multi-scale temporal patterns and heterogeneous components, remains insufficiently addressed. In this work, we propose a structure-disentangled multiscale generation framework for time series. Our approach encodes sequences into discrete tokens at multiple temporal resolutions and performs autoregressive generation in a coarse-to-fine manner, thereby preserving hierarchical dependencies. To tackle structural heterogeneity, we introduce a dual-path VQ-VAE that disentangles trend and seasonal components, enabling the learning of semantically consistent latent representations. Additionally, we present a guidance-based reconstruction strategy, where coarse seasonal signals are utilized as priors to guide the reconstruction of fine-grained seasonal patterns. Experiments on six datasets show that our approach produces higher-quality time series than existing methods. Notably, our model achieves strong performance with a significantly reduced parameter count and exhibits superior capability in generating high-quality long-term sequences. Our implementation is available at https://anonymous.4open.science/r/TimeMAR-BC5B.

</details>


### [64] [FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization](https://arxiv.org/abs/2601.11200)
*Haiyang Xiao,Weiqing Li,Jinyue Guo,Guochao Jiang,Guohua Liu,Yuewei Zhang*

Main category: cs.LG

TL;DR: FAQ是一种利用同家族大语言模型先验知识生成高质量校准数据的量化框架，通过数据再生和专家指导的组竞争选择最佳样本，显著提升后训练量化的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化方法依赖有限校准样本，难以捕捉推理阶段的激活分布，导致量化参数存在偏差。校准数据的代表性和普适性成为决定量化精度的核心瓶颈。

Method: FAQ框架首先将原始校准样本输入到与目标模型同家族的更大LLM中，利用高度一致的知识系统再生高保真校准数据。这些携带思维链推理、符合预期激活分布的数据在专家指导下进行组竞争，选择最佳样本并重新归一化，以增强标准PTQ的效果。

Result: 在包括Qwen3-8B在内的多个模型系列上的实验表明，FAQ相比使用原始校准数据的基线方法，将精度损失降低了高达28.5%，展示了其强大的潜力和贡献。

Conclusion: FAQ通过利用同家族LLM的先验知识生成高质量校准数据，有效解决了传统PTQ方法中校准数据代表性不足的问题，显著提升了量化模型的准确性，为资源受限设备上部署大语言模型提供了更可靠的量化方案。

Abstract: Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.

</details>


### [65] [SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients](https://arxiv.org/abs/2601.11219)
*Zhikang Shen,Jianrong Lu,Haiyuan Wan,Jianhai Chen*

Main category: cs.LG

TL;DR: SDFLoRA：一种解决联邦学习中LoRA适配器秩异构问题的选择性双模块框架，通过分离全局和本地模块实现鲁棒聚合与隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的LoRA方法面临秩异构问题，不同客户端使用不同的低秩配置导致直接聚合产生偏差和不稳定。现有解决方案强制统一秩或将异构更新对齐到共享子空间，这会过度约束客户端特定语义、限制个性化，并在差分隐私噪声下提供较弱的本地信息保护。

Method: 提出选择性双模块联邦LoRA（SDFLoRA），将每个客户端适配器分解为：1）捕获可迁移知识的全局模块；2）保留客户端特定适配的本地模块。全局模块在客户端间选择性对齐和聚合，本地模块保持私有。该设计支持在秩异构下的鲁棒学习，并通过仅在全局模块注入差分隐私噪声实现隐私感知优化。

Result: 在GLUE基准测试上的实验表明，SDFLoRA优于代表性的联邦LoRA基线方法，并实现了更好的效用-隐私权衡。

Conclusion: SDFLoRA通过双模块分解有效解决了联邦学习中LoRA适配器的秩异构问题，在保持个性化能力的同时实现了鲁棒的聚合和隐私保护，为实际部署提供了实用解决方案。

Abstract: Federated learning (FL) for large language models (LLMs) has attracted increasing attention as a way to enable privacy-preserving adaptation over distributed data. Parameter-efficient methods such as LoRA are widely adopted to reduce communication and memory costs. Despite these advances, practical FL deployments often exhibit rank heterogeneity, since different clients may use different low-rank configurations. This makes direct aggregation of LoRA updates biased and unstable. Existing solutions typically enforce unified ranks or align heterogeneous updates into a shared subspace, which over-constrains client-specific semantics, limits personalization, and provides weak protection of local client information under differential privacy noise. To address this issue, we propose Selective Dual-module Federated LoRA (SDFLoRA), which decomposes each client adapter into a global module that captures transferable knowledge and a local module that preserves client-specific adaptations. The global module is selectively aligned and aggregated across clients, while local modules remain private. This design enables robust learning under rank heterogeneity and supports privacy-aware optimization by injecting differential privacy noise exclusively into the global module. Experiments on GLUE benchmarks demonstrate that SDFLoRA outperforms representative federated LoRA baselines and achieves a better utility-privacy trade-off.

</details>


### [66] [Operator learning on domain boundary through combining fundamental solution-based artificial data and boundary integral techniques](https://arxiv.org/abs/2601.11222)
*Haochen Wu,Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: 提出MAD-BNO框架，仅使用边界数据学习偏微分方程算子，无需全域采样，通过数学人工数据生成训练样本，结合边界积分恢复内部解。


<details>
  <summary>Details</summary>
Motivation: 传统算子学习方法需要全域采样数据，计算成本高。本文旨在开发仅依赖边界数据的算子学习框架，减少数据需求，提高效率。

Method: 结合数学人工数据方法，从基本解生成边界数据对（Dirichlet-Neumann数据），学习边界到边界的映射。训练后通过边界积分公式恢复任意内部点的解。

Result: 在二维Laplace、Poisson和Helmholtz方程基准测试中，精度达到或优于现有神经算子方法，同时显著减少训练时间。框架可扩展到三维问题和复杂几何。

Conclusion: MAD-BNO提供了一种高效的数据驱动算子学习框架，仅需边界数据即可学习偏微分方程，具有扩展到高维和复杂问题的潜力。

Abstract: For linear partial differential equations with known fundamental solutions, this work introduces a novel operator learning framework that relies exclusively on domain boundary data, including solution values and normal derivatives, rather than full-domain sampling. By integrating the previously developed Mathematical Artificial Data (MAD) method, which enforces physical consistency, all training data are synthesized directly from the fundamental solutions of the target problems, resulting in a fully data-driven pipeline without the need for external measurements or numerical simulations. We refer to this approach as the Mathematical Artificial Data Boundary Neural Operator (MAD-BNO), which learns boundary-to-boundary mappings using MAD-generated Dirichlet-Neumann data pairs. Once trained, the interior solution at arbitrary locations can be efficiently recovered through boundary integral formulations, supporting Dirichlet, Neumann, and mixed boundary conditions as well as general source terms. The proposed method is validated on benchmark operator learning tasks for two-dimensional Laplace, Poisson, and Helmholtz equations, where it achieves accuracy comparable to or better than existing neural operator approaches while significantly reducing training time. The framework is naturally extensible to three-dimensional problems and complex geometries.

</details>


### [67] [Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation](https://arxiv.org/abs/2601.11258)
*Pingzhi Tang,Yiding Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: PaST框架通过提取领域无关的技能向量，实现高效的知识适应，在SFT后线性注入知识操作技能，显著提升问答和工具使用性能。


<details>
  <summary>Details</summary>
Motivation: LLMs面临"知识截止"问题，SFT虽然能更新事实知识但无法可靠提升问答能力，RL能获得推理技能但计算成本过高。研究发现SFT和RL的参数更新几乎正交，需要一种高效的知识适应方法。

Method: 提出Parametric Skill Transfer (PaST)框架：1) 从源领域提取领域无关的技能向量；2) 目标模型在新数据上进行轻量级SFT；3) 将技能向量线性注入到目标模型中，实现知识操作技能的转移。

Result: 在SQuAD上比最先进的SFT基线提升9.9分；在LooGLE长上下文QA上获得8.0分绝对准确率提升；在ToolBench工具使用基准上平均提升10.3分成功率，技能向量展现强可扩展性和跨领域迁移性。

Conclusion: PaST框架通过模块化技能转移有效解决了LLMs的知识适应问题，实现了高效的知识操作技能注入，在多个基准上显著优于现有方法，具有实际应用价值。

Abstract: Large Language Models (LLMs) face the "knowledge cutoff" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.

</details>


### [68] [Latent Dynamics Graph Convolutional Networks for model order reduction of parameterized time-dependent PDEs](https://arxiv.org/abs/2601.11259)
*Lorenzo Tomada,Federico Pichi,Gianluigi Rozza*

Main category: cs.LG

TL;DR: LD-GCN：一种无编码器的图神经网络架构，用于参数化偏微分方程的模型降阶，能在潜在空间建模时间演化并支持时间外推和零样本预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时结合几何归纳偏置与可解释的潜在行为，要么忽略动态驱动特征，要么忽视空间信息。需要一种能学习全局低维表示并保持可解释性的数据驱动方法。

Method: 提出Latent Dynamics Graph Convolutional Network (LD-GCN)，无编码器的纯数据驱动架构。在潜在空间建模时间演化并通过时间步进推进，使用GNN将轨迹解码到几何参数化域。支持潜在插值进行零样本预测。

Result: 通过通用逼近定理进行数学验证，在复杂计算力学问题上进行数值测试（包括物理和几何参数），成功检测Navier-Stokes方程的分岔现象。

Conclusion: LD-GCN填补了现有方法的空白，结合了几何归纳偏置与可解释的潜在行为，为参数化PDE的模型降阶提供了有效且可解释的框架。

Abstract: Graph Neural Networks (GNNs) are emerging as powerful tools for nonlinear Model Order Reduction (MOR) of time-dependent parameterized Partial Differential Equations (PDEs). However, existing methodologies struggle to combine geometric inductive biases with interpretable latent behavior, overlooking dynamics-driven features or disregarding spatial information. In this work, we address this gap by introducing Latent Dynamics Graph Convolutional Network (LD-GCN), a purely data-driven, encoder-free architecture that learns a global, low-dimensional representation of dynamical systems conditioned on external inputs and parameters. The temporal evolution is modeled in the latent space and advanced through time-stepping, allowing for time-extrapolation, and the trajectories are consistently decoded onto geometrically parameterized domains using a GNN. Our framework enhances interpretability by enabling the analysis of the reduced dynamics and supporting zero-shot prediction through latent interpolation. The methodology is mathematically validated via a universal approximation theorem for encoder-free architectures, and numerically tested on complex computational mechanics problems involving physical and geometric parameters, including the detection of bifurcating phenomena for Navier-Stokes equations. Code availability: https://github.com/lorenzotomada/ld-gcn-rom

</details>


### [69] [Sample-Near-Optimal Agnostic Boosting with Improved Running Time](https://arxiv.org/abs/2601.11265)
*Arthur da Cunha,Miakel Møller Høgsgaard,Andrea Paudice*

Main category: cs.LG

TL;DR: 提出了首个具有接近最优样本复杂度的不可知提升算法，在固定其他参数时，运行时间与样本大小呈多项式关系。


<details>
  <summary>Details</summary>
Motivation: 提升方法能将弱学习器转化为高精度的强学习器，但在不可知设置下（不对数据做任何假设）理解不足。虽然最近解决了不可知提升的样本复杂度问题，但已知算法具有指数运行时间。

Method: 提出了第一个具有接近最优样本复杂度的不可知提升算法，当考虑问题的其他参数固定时，该算法在样本大小上具有多项式运行时间。

Result: 实现了首个在样本大小上具有多项式运行时间的不可知提升算法，同时保持了接近最优的样本复杂度。

Conclusion: 该工作填补了不可知提升算法在计算效率方面的空白，为实际应用提供了可行的解决方案。

Abstract: Boosting is a powerful method that turns weak learners, which perform only slightly better than random guessing, into strong learners with high accuracy. While boosting is well understood in the classic setting, it is less so in the agnostic case, where no assumptions are made about the data. Indeed, only recently was the sample complexity of agnostic boosting nearly settled arXiv:2503.09384, but the known algorithm achieving this bound has exponential running time. In this work, we propose the first agnostic boosting algorithm with near-optimal sample complexity, running in time polynomial in the sample size when considering the other parameters of the problem fixed.

</details>


### [70] [Metabolomic Biomarker Discovery for ADHD Diagnosis Using Interpretable Machine Learning](https://arxiv.org/abs/2601.11283)
*Nabil Belacel,Mohamed Rachid Boulassel*

Main category: cs.LG

TL;DR: 使用尿液代谢组学和可解释机器学习识别ADHD生物标志物，CR分类器基于14种代谢物实现AUC>0.97，为ADHD客观诊断提供新方法。


<details>
  <summary>Details</summary>
Motivation: ADHD作为常见神经发育障碍缺乏客观诊断工具，需要基于生物学的精准精神病学诊断框架。当前诊断主要依赖主观临床评估，需要客观、生物学基础的诊断方法。

Method: 整合尿液代谢组学与可解释机器学习框架，分析52名ADHD和46名对照参与者的靶向代谢组学数据，使用具有嵌入式特征选择的Closest Resemblance（CR）分类器，并与随机森林和K近邻分类器比较。

Result: CR模型优于其他分类器，基于14种代谢物实现AUC>0.97。关键代谢物包括多巴胺4-硫酸盐、N-乙酰天冬氨酰谷氨酸和瓜氨酸，这些代谢物映射到多巴胺能神经传递和氨基酸代谢通路，为ADHD病理生理学提供机制见解。

Conclusion: 该研究展示了代谢组学与可解释机器学习结合的转化框架，支持整合到靶向代谢组学检测和未来即时诊断平台，推进ADHD的客观、生物学知情诊断策略。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a prevalent neurodevelopmental disorder with limited objective diagnostic tools, highlighting the urgent need for objective, biology-based diagnostic frameworks in precision psychiatry. We integrate urinary metabolomics with an interpretable machine learning framework to identify biochemical signatures associated with ADHD. Targeted metabolomic profiles from 52 ADHD and 46 control participants were analyzed using a Closest Resemblance (CR) classifier with embedded feature selection. The CR model outperformed Random Forest and K-Nearest Neighbor classifiers, achieving an AUC > 0.97 based on a reduced panel of 14 metabolites. These metabolites including dopamine 4-sulfate, N-acetylaspartylglutamic acid, and citrulline map to dopaminergic neurotransmission and amino acid metabolism pathways, offering mechanistic insight into ADHD pathophysiology. The CR classifier's transparent decision boundaries and low computational cost support integration into targeted metabolomic assays and future point of care diagnostic platforms. Overall, this work demonstrates a translational framework combining metabolomics and interpretable machine learning to advance objective, biologically informed diagnostic strategies for ADHD.

</details>


### [71] [FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning](https://arxiv.org/abs/2601.11311)
*Zhihan Yang,Jiaqi Wei,Xiang Zhang,Haoyu Dong,Yiwen Wang,Xiaoke Guo,Pengkun Zhang,Yiwei Xu,Chenyu You*

Main category: cs.LG

TL;DR: FORESTLLM：结合决策树结构偏置与LLM语义推理的少样本表格学习框架，训练时使用LLM设计轻量级可解释森林模型，测试时无需LLM推理


<details>
  <summary>Details</summary>
Motivation: 表格数据在金融、医疗等关键领域决策中至关重要，但少样本场景下学习困难。传统树方法依赖统计纯度指标，在有限监督下不稳定易过拟合；直接应用LLM又忽视表格结构导致性能不佳

Method: 1) 语义分割准则：LLM基于标记和未标记数据评估候选分割的语义连贯性，构建更鲁棒的树结构；2) 一次性上下文推理机制：LLM将决策路径和支持样本提炼为确定性预测，替代噪声经验估计

Result: 在多样化少样本分类和回归基准测试中，FORESTLLM实现了最先进的性能

Conclusion: FORESTLLM成功统一了决策森林的结构归纳偏置与LLM的语义推理能力，在训练时利用LLM作为离线模型设计器，生成轻量级可解释模型，测试时无需LLM推理，解决了少样本表格学习的关键挑战

Abstract: Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.

</details>


### [72] [Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models](https://arxiv.org/abs/2601.11342)
*Chuanyue Yu,Jiahui Wang,Yuhan Li,Heng Chang,Ge Lan,Qingyun Sun,Jia Li,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: 本文提出SPREAD框架，通过查询相关性引导的去噪策略解决扩散语言模型在检索增强生成中的语义漂移问题，显著提升生成精度。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在自然语言处理任务中表现出色，但其在检索增强生成框架中的潜力尚未充分探索。虽然RAG在增强大语言模型方面取得巨大成功，但由于LLM和DLM解码机制的根本差异，DLM在RAG中的表现有待研究。

Method: 提出SPREAD（Semantic-Preserving REtrieval-Augmented Diffusion）框架，引入查询相关性引导的去噪策略。该策略主动引导去噪轨迹，确保生成内容始终锚定在查询语义上，有效抑制语义漂移。

Result: 实验结果表明，SPREAD显著提升了RAG框架中生成答案的精度，有效缓解了响应语义漂移问题。DLM结合RAG显示出对上下文信息更强的依赖性，但存在生成精度有限的问题。

Conclusion: SPREAD框架通过查询相关性引导的去噪策略成功解决了扩散语言模型在检索增强生成中的语义漂移问题，为DLM在RAG中的应用提供了有效解决方案。

Abstract: Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.

</details>


### [73] [FEATHer: Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting](https://arxiv.org/abs/2601.11350)
*Jaehoon Lee,Seungwoo Lee,Younghwi Kim,Dohee Kim,Sunghyun Sim*

Main category: cs.LG

TL;DR: FEATHer是一种用于边缘设备时间序列预测的超轻量级模型，在仅400参数下实现准确长期预测，在8个基准测试中取得60个第一名。


<details>
  <summary>Details</summary>
Motivation: 工业领域（如制造和智能工厂）需要边缘设备上的时间序列预测，但传统深度模型在PLC、微控制器等设备上参数过多、延迟高、内存受限，无法满足实时推理需求。

Method: 提出FEATHer模型：1) 超轻量级多尺度频率分解；2) 共享密集时间核（投影-深度卷积-投影）；3) 频率感知分支门控；4) 稀疏周期核通过周期下采样捕获季节性。

Result: 在8个基准测试中取得最佳排名，获得60个第一名，平均排名2.05，证明在仅400参数下仍能实现可靠的长期预测。

Conclusion: FEATHer证明了在严格约束的边缘硬件上实现可靠长期预测是可行的，为工业实时推理提供了实用方向。

Abstract: Time-series forecasting is fundamental in industrial domains like manufacturing and smart factories. As systems evolve toward automation, models must operate on edge devices (e.g., PLCs, microcontrollers) with strict constraints on latency and memory, limiting parameters to a few thousand. Conventional deep architectures are often impractical here. We propose the Fourier-Efficient Adaptive Temporal Hierarchy Forecaster (FEATHer) for accurate long-term forecasting under severe limits. FEATHer introduces: (i) ultra-lightweight multiscale decomposition into frequency pathways; (ii) a shared Dense Temporal Kernel using projection-depthwise convolution-projection without recurrence or attention; (iii) frequency-aware branch gating that adaptively fuses representations based on spectral characteristics; and (iv) a Sparse Period Kernel reconstructing outputs via period-wise downsampling to capture seasonality. FEATHer maintains a compact architecture (as few as 400 parameters) while outperforming baselines. Across eight benchmarks, it achieves the best ranking, recording 60 first-place results with an average rank of 2.05. These results demonstrate that reliable long-range forecasting is achievable on constrained edge hardware, offering a practical direction for industrial real-time inference.

</details>


### [74] [Latent Space Inference via Paired Autoencoders](https://arxiv.org/abs/2601.11397)
*Emma Hart,Bas Peters,Julianne Chung,Matthias Chung*

Main category: cs.LG

TL;DR: 提出基于配对自编码器的数据驱动潜在空间推理框架，解决反问题中的观测不一致性，通过潜在空间映射实现正则化反演和优化。


<details>
  <summary>Details</summary>
Motivation: 传统反问题求解方法在处理观测数据不一致性（如部分、噪声或分布外数据）时面临挑战，需要一种能保持物理模型一致性同时处理数据不一致性的灵活框架。

Method: 使用两个自编码器分别处理参数空间和观测空间，通过学习的潜在空间映射连接，构建正则化反演的替代模型，在低维信息丰富的潜在空间中进行优化。

Result: 相比单独使用配对自编码器和相同架构的端到端编码器-解码器，该方法在数据不一致场景下能产生更准确的重建结果，在医学层析成像和地球物理地震波形反演中得到验证。

Conclusion: 该框架能灵活处理部分、噪声或分布外数据，同时保持与底层物理模型的一致性，可广泛应用于科学和工程领域的各种反问题。

Abstract: This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mappings between the autoencoders' latent spaces. These mappings enable a surrogate for regularized inversion and optimization in low-dimensional, informative latent spaces. Our flexible framework can work with partial, noisy, or out-of-distribution data, all while maintaining consistency with the underlying physical models. The paired autoencoders enable reconstruction of corrupted data, and then use the reconstructed data for parameter estimation, which produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of the same architecture, especially in scenarios with data inconsistencies. We demonstrate our approaches on two imaging examples in medical tomography and geophysical seismic-waveform inversion, but the described approaches are broadly applicable to a variety of inverse problems in scientific and engineering applications.

</details>


### [75] [Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.11401)
*Ahmed Rashwan,Keith Briggs,Chris Budd,Lisa Kreusser*

Main category: cs.LG

TL;DR: 提出扩散价值函数(DVF)用于图马尔可夫决策过程，通过在图结构上扩散奖励来解决多智能体强化学习中的信用分配问题，并开发了DA2C算法和LD-GNN架构。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的信用分配问题在大规模结构化局部交互系统中尤为突出。现有的全局价值函数学习信号弱，局部构造难以估计且在无限时域中表现不佳。

Method: 提出扩散价值函数(DVF)，通过在图影响结构上进行时间折扣和空间衰减的奖励扩散，为每个智能体分配价值分量。基于DVF开发了DA2C算法和LD-GNN消息传递执行器。

Result: 在消防基准测试和三个分布式计算任务（向量图着色和两个发射功率优化问题）中，DA2C始终优于局部和全局批评器基线，平均奖励提升高达11%。

Conclusion: DVF为图马尔可夫决策过程提供了一种结构对齐的价值函数分解方法，有效解决了大规模多智能体系统中的信用分配问题，并通过DA2C算法在实际任务中验证了其优越性。

Abstract: Credit assignment is a core challenge in multi-agent reinforcement learning (MARL), especially in large-scale systems with structured, local interactions. Graph-based Markov decision processes (GMDPs) capture such settings via an influence graph, but standard critics are poorly aligned with this structure: global value functions provide weak per-agent learning signals, while existing local constructions can be difficult to estimate and ill-behaved in infinite-horizon settings. We introduce the Diffusion Value Function (DVF), a factored value function for GMDPs that assigns to each agent a value component by diffusing rewards over the influence graph with temporal discounting and spatial attenuation. We show that DVF is well-defined, admits a Bellman fixed point, and decomposes the global discounted value via an averaging property. DVF can be used as a drop-in critic in standard RL algorithms and estimated scalably with graph neural networks. Building on DVF, we propose Diffusion A2C (DA2C) and a sparse message-passing actor, Learned DropEdge GNN (LD-GNN), for learning decentralised algorithms under communication costs. Across the firefighting benchmark and three distributed computation tasks (vector graph colouring and two transmit power optimisation problems), DA2C consistently outperforms local and global critic baselines, improving average reward by up to 11%.

</details>


### [76] [Forcing and Diagnosing Failure Modes of Fourier Neural Operators Across Diverse PDE Families](https://arxiv.org/abs/2601.11428)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: FNO在PDE求解中表现良好，但在分布偏移、长时程推演和结构扰动下的鲁棒性不足。本文提出系统压力测试框架，在五类PDE上揭示FNO的失效模式，发现参数或边界条件分布偏移可使误差增加一个数量级。


<details>
  <summary>Details</summary>
Motivation: 虽然Fourier神经算子在求解偏微分方程方面表现出色，但其在分布偏移、长时程推演和结构扰动下的鲁棒性尚未得到充分理解，需要系统评估其失效模式。

Method: 提出系统压力测试框架，在五类不同性质的PDE（色散、椭圆、多尺度流体、金融、混沌系统）上进行大规模评估（1000个训练模型）。设计控制性压力测试，包括参数偏移、边界/终端条件变化、分辨率外推与谱分析、迭代推演等。

Result: 参数或边界条件分布偏移可使误差增加一个数量级以上；分辨率变化主要将误差集中在高频模式；输入扰动通常不会放大误差，但最坏情况（如局部泊松扰动）仍具挑战性。

Conclusion: 研究结果提供了比较性失效模式图集和可操作的见解，有助于改进算子学习的鲁棒性，揭示了FNO在特定条件下的脆弱性。

Abstract: Fourier Neural Operators (FNOs) have shown strong performance in learning solution maps of partial differential equations (PDEs), but their robustness under distribution shifts, long-horizon rollouts, and structural perturbations remains poorly understood. We present a systematic stress-testing framework that probes failure modes of FNOs across five qualitatively different PDE families: dispersive, elliptic, multi-scale fluid, financial, and chaotic systems. Rather than optimizing in-distribution accuracy, we design controlled stress tests--including parameter shifts, boundary or terminal condition changes, resolution extrapolation with spectral analysis, and iterative rollouts--to expose vulnerabilities such as spectral bias, compounding integration errors, and overfitting to restricted boundary regimes. Our large-scale evaluation (1{,}000 trained models) reveals that distribution shifts in parameters or boundary conditions can inflate errors by more than an order of magnitude, while resolution changes primarily concentrate error in high-frequency modes. Input perturbations generally do not amplify error, though worst-case scenarios (e.g., localized Poisson perturbations) remain challenging. These findings provide a comparative failure-mode atlas and actionable insights for improving robustness in operator learning.

</details>


### [77] [Inter-patient ECG Arrhythmia Classification with LGNs and LUTNs](https://arxiv.org/abs/2601.11433)
*Wout Mommen,Lars Keuninckx,Paul Detterer,Achiel Colpaert,Piet Wambacq*

Main category: cs.LG

TL;DR: 提出使用深度可微分逻辑门网络(LGNs)和查找表网络(LUTNs)进行心电图分类，在MIT-BIH数据集上达到94.28%准确率，计算量比SOTA方法低3-6个数量级，功耗极低。


<details>
  <summary>Details</summary>
Motivation: 开发适用于心脏植入物或可穿戴设备的低功耗、高速心律失常检测方法，特别是针对训练集未包含的患者（患者间范式）。

Method: 使用LGNs和LUTNs进行ECG分类，采用新颖的预处理方法，为LUTNs设计基于多路复用器布尔方程的训练方法，首次在LGNs/LUTNs中使用速率编码。

Result: 在MIT-BIH四类分类中达到94.28%准确率和0.683 jκ指数，仅需2.89k-6.17k FLOPs，FPGA实现需2000-2990个LUTs，功耗5-7mW（50-70pJ/推理）。

Conclusion: LGNs和LUTNs适合极低功耗、高速的心律失常检测，可用于心脏植入物和可穿戴设备，即使对训练集未包含的患者也有效。

Abstract: Deep Differentiable Logic Gate Networks (LGNs) and Lookup Table Networks (LUTNs) are demonstrated to be suitable for the automatic classification of electrocardiograms (ECGs) using the inter-patient paradigm. The methods are benchmarked using the MIT-BIH arrhythmia data set, achieving up to 94.28% accuracy and a $jκ$ index of 0.683 on a four-class classification problem. Our models use between 2.89k and 6.17k FLOPs, including preprocessing and readout, which is three to six orders of magnitude less compared to SOTA methods. A novel preprocessing method is utilized that attains superior performance compared to existing methods for both the mixed-patient and inter-patient paradigms. In addition, a novel method for training the Lookup Tables (LUTs) in LUTNs is devised that uses the Boolean equation of a multiplexer (MUX). Additionally, rate coding was utilized for the first time in these LGNs and LUTNs, enhancing the performance of LGNs. Furthermore, it is the first time that LGNs and LUTNs have been benchmarked on the MIT-BIH arrhythmia dataset using the inter-patient paradigm. Using an Artix 7 FPGA, between 2000 and 2990 LUTs were needed, and between 5 to 7 mW (i.e. 50 pJ to 70 pJ per inference) was estimated for running these models. The performance in terms of both accuracy and $jκ$-index is significantly higher compared to previous LGN results. These positive results suggest that one can utilize LGNs and LUTNs for the detection of arrhythmias at extremely low power and high speeds in heart implants or wearable devices, even for patients not included in the training set.

</details>


### [78] [GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2601.11440)
*Francisco Giral,Álvaro Manzano,Ignacio Gómez,Ricardo Vinuesa,Soledad Le Clainche*

Main category: cs.LG

TL;DR: GenDA：一种基于生成扩散的数据同化框架，能从稀疏传感器数据重建城市风场，在未见过的几何形状、风向和网格分辨率上具有泛化能力。


<details>
  <summary>Details</summary>
Motivation: 城市风场重建对空气质量评估、热量扩散和行人舒适度至关重要，但仅凭稀疏传感器数据难以实现高分辨率重建。

Method: 提出GenDA框架，采用多尺度图基扩散架构，在CFD模拟数据上训练。将无分类器引导解释为学习后验重建机制：无条件分支学习几何感知流先验，传感器条件分支在采样时注入观测约束。

Result: 相比监督GNN基线和经典降阶数据同化方法，GenDA将相对均方根误差降低25-57%，结构相似性指数提高23-33%。

Conclusion: 该框架为复杂环境监测中的生成式、几何感知数据同化提供了可扩展路径。

Abstract: Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism: the unconditional branch learns a geometry-aware flow prior, while the sensor-conditioned branch injects observational constraints during sampling. This formulation enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. We consider both sparse fixed sensors and trajectory-based observations using the same reconstruction procedure. When evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, GenDA reduces the relative root-mean-square error (RRMSE) by 25-57% and increases the structural similarity index (SSIM) by 23-33% across the tested meshes. Experiments are conducted on Reynolds-averaged Navier-Stokes (RANS) simulations of a real urban neighbourhood in Bristol, United Kingdom, at a characteristic Reynolds number of $\mathrm{Re}\approx2\times10^{7}$, featuring complex building geometry and irregular terrain. The proposed framework provides a scalable path toward generative, geometry-aware data assimilation for environmental monitoring in complex domains.

</details>


### [79] [Low-Rank Key Value Attention](https://arxiv.org/abs/2601.11471)
*James O'Neill,Robert Clancy,Mariia Matskevichus,Fergal Reid*

Main category: cs.LG

TL;DR: LRKV是一种通过低秩残差共享KV投影来减少Transformer KV缓存内存的注意力机制，在保持头多样性的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: Transformer预训练面临内存和计算限制，其中KV缓存成为训练和自回归解码的主要瓶颈，需要一种既能减少KV缓存又能保持模型性能的方法。

Method: 提出低秩KV适应（LRKV），在多头注意力中使用共享的全秩KV投影，加上低秩的头特定残差，实现KV投影的连续权衡，完全替代标准多头注意力。

Result: 在大规模预训练实验中，LRKV相比标准注意力、MQA/GQA和MLA，实现了更快的损失下降、更低的验证困惑度和更强的下游任务性能。在2.5B规模下，使用约一半KV缓存就能超越标准注意力，达到相同模型质量可减少20-25%训练计算。

Conclusion: LRKV通过分析注意力头结构证明能保持几乎所有的功能头多样性，而更激进的KV共享机制需要查询专业化补偿，LRKV成为内存和计算受限环境下扩展Transformer预训练的实用有效注意力机制。

Abstract: Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.

</details>


### [80] [Extractive summarization on a CMOS Ising machine](https://arxiv.org/abs/2601.11491)
*Ziqing Zeng,Abhimanyu Kumar,Chris H. Kim,Ulya R. Karpuzcu,Sachin S. Sapatnekar*

Main category: cs.LG

TL;DR: 本文探索在低功耗CMOS耦合振荡器伊辛机(COBI)上实现McDonald式抽取式摘要，提出硬件感知伊辛公式化、随机舍入和分解策略，在CNN/DailyMail数据集上实现3-4.5倍加速和2-3个数量级能耗降低。


<details>
  <summary>Details</summary>
Motivation: 现代抽取式摘要系统依赖CPU/GPU基础设施，能耗高且不适合资源受限环境的实时推理。本文探索在低功耗CMOS耦合振荡器伊辛机上实现抽取式摘要，以支持边缘设备的实时低能耗文本摘要。

Method: 1) 提出硬件感知伊辛公式化，减少局部场和耦合项之间的尺度不平衡，提高系数量化的鲁棒性；2) 开发完整ES流水线，包括随机舍入和迭代精炼以补偿精度损失；3) 采用分解策略将大型ES问题划分为可在COBI上高效求解的小型伊辛子问题。

Result: 在CNN/DailyMail数据集上，COBI实现3-4.5倍运行时间加速（与暴力方法相比），能耗降低2-3个数量级，同时保持有竞争力的摘要质量，性能与软件Tabu搜索相当。

Conclusion: 研究结果表明CMOS伊辛求解器在边缘设备上实现实时低能耗文本摘要具有潜力，为资源受限环境中的高效摘要部署提供了新途径。

Abstract: Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU infrastructures that are energy-intensive and poorly suited for real-time inference in resource-constrained environments. In this work, we explore the feasibility of implementing McDonald-style extractive summarization on a low-power CMOS coupled oscillator-based Ising machine (COBI) that supports integer-valued, all-to-all spin couplings. We first propose a hardware-aware Ising formulation that reduces the scale imbalance between local fields and coupling terms, thereby improving robustness to coefficient quantization: this method can be applied to any problem formulation that requires k of n variables to be chosen. We then develop a complete ES pipeline including (i) stochastic rounding and iterative refinement to compensate for precision loss, and (ii) a decomposition strategy that partitions a large ES problem into smaller Ising subproblems that can be efficiently solved on COBI and later combined. Experimental results on the CNN/DailyMail dataset show that our pipeline can produce high-quality summaries using only integer-coupled Ising hardware with limited precision. COBI achieves 3-4.5x runtime speedups compared to a brute-force method, which is comparable to software Tabu search, and two to three orders of magnitude reductions in energy, while maintaining competitive summary quality. These results highlight the potential of deploying CMOS Ising solvers for real-time, low-energy text summarization on edge devices.

</details>


### [81] [QUPID: A Partitioned Quantum Neural Network for Anomaly Detection in Smart Grid](https://arxiv.org/abs/2601.11500)
*Hoang M. Ngo,Tre' R. Jeter,Jung Taek Seo,My T. Thai*

Main category: cs.LG

TL;DR: 本文提出QUPID和R-QUPID两种量子机器学习模型，用于智能电网异常检测，通过分区量子神经网络解决可扩展性问题，并在对抗性攻击下保持高性能。


<details>
  <summary>Details</summary>
Motivation: 智能电网面临网络物理威胁、系统故障和网络攻击等风险，传统机器学习模型难以处理智能电网的复杂性且易受对抗性攻击，需要更鲁棒的异常检测方法。

Method: 提出QUPID分区量子神经网络，利用量子增强特征表示建模智能电网高维复杂性；扩展为R-QUPID，结合差分隐私增强鲁棒性；通过分区框架解决量子机器学习可扩展性问题。

Result: 实验结果显示QUPID和R-QUPID在多种场景下显著优于传统最先进的机器学习模型，即使在加入差分隐私后仍保持高性能，分区框架有效解决了大规模部署的可扩展性问题。

Conclusion: 量子机器学习为智能电网异常检测提供了更有效和鲁棒的解决方案，QUPID和R-QUPID通过量子增强特征表示和分区架构，实现了高性能、抗攻击性和可扩展性的平衡。

Abstract: Smart grid infrastructures have revolutionized energy distribution, but their day-to-day operations require robust anomaly detection methods to counter risks associated with cyber-physical threats and system faults potentially caused by natural disasters, equipment malfunctions, and cyber attacks. Conventional machine learning (ML) models are effective in several domains, yet they struggle to represent the complexities observed in smart grid systems. Furthermore, traditional ML models are highly susceptible to adversarial manipulations, making them increasingly unreliable for real-world deployment. Quantum ML (QML) provides a unique advantage, utilizing quantum-enhanced feature representations to model the intricacies of the high-dimensional nature of smart grid systems while demonstrating greater resilience to adversarial manipulation. In this work, we propose QUPID, a partitioned quantum neural network (PQNN) that outperforms traditional state-of-the-art ML models in anomaly detection. We extend our model to R-QUPID that even maintains its performance when including differential privacy (DP) for enhanced robustness. Moreover, our partitioning framework addresses a significant scalability problem in QML by efficiently distributing computational workloads, making quantum-enhanced anomaly detection practical in large-scale smart grid environments. Our experimental results across various scenarios exemplifies the efficacy of QUPID and R-QUPID to significantly improve anomaly detection capabilities and robustness compared to traditional ML approaches.

</details>


### [82] [Building Production-Ready Probes For Gemini](https://arxiv.org/abs/2601.11516)
*János Kramár,Joshua Engels,Zheng Wang,Bilal Chughtai,Rohin Shah,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 论文提出新的激活探针架构以应对长上下文分布偏移，在网络安全领域评估其鲁棒性，并成功部署于Gemini模型，同时展示了自动化AI安全研究的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着前沿语言模型能力快速提升，需要更强的滥用防范措施。现有激活探针技术面临关键挑战：在重要的生产分布偏移下泛化能力不足，特别是从短上下文到长上下文的转换。

Method: 提出新的探针架构处理长上下文分布偏移，在网络安全领域评估其鲁棒性，包括多轮对话、静态越狱和自适应红队测试。结合架构选择和多样化分布训练，并探索探针与提示分类器的组合使用。

Result: 新探针架构能有效处理上下文长度问题，但需要架构选择和多样化训练才能实现广泛泛化。探针与提示分类器组合能以低成本获得最优准确率。研究成果已成功部署于Gemini模型，并展示了自动化AI安全研究的早期积极结果。

Conclusion: 激活探针是有效的滥用防范技术，但需要专门设计应对分布偏移。结合架构创新、多样化训练和自动化方法，可以构建更强大的安全系统，自动化AI安全研究已具备可行性。

Abstract: Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.
  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.
  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [83] [Mass Distribution versus Density Distribution in the Context of Clustering](https://arxiv.org/abs/2601.10759)
*Kai Ming Ting,Ye Zhu,Hang Zhang,Tianrun Liang*

Main category: stat.ML

TL;DR: 论文提出质量分布优于密度分布作为聚类描述符，开发了质量最大化聚类算法克服密度偏差


<details>
  <summary>Details</summary>
Motivation: 传统密度分布在聚类中存在固有局限性——高密度偏差，这阻碍了发现任意形状、大小和密度的聚类。现有密度聚类算法虽然尝试缓解此问题，但密度分布的根本限制仍然存在。

Method: 提出质量最大化聚类算法，以质量分布作为数据描述基础，最大化所有聚类的总质量。该方法可以轻松调整为最大化总密度，以便对比质量分布与密度分布的根本差异。

Result: 质量最大化聚类相比密度最大化聚类的关键优势在于，其最大化过程不会偏向密集聚类，能够更好地发现任意形状、大小和密度的聚类。

Conclusion: 质量分布是比密度分布更优的数据描述符，质量最大化聚类算法克服了传统密度聚类的高密度偏差问题，为聚类分析提供了更好的理论基础。

Abstract: This paper investigates two fundamental descriptors of data, i.e., density distribution versus mass distribution, in the context of clustering. Density distribution has been the de facto descriptor of data distribution since the introduction of statistics. We show that density distribution has its fundamental limitation -- high-density bias, irrespective of the algorithms used to perform clustering. Existing density-based clustering algorithms have employed different algorithmic means to counter the effect of the high-density bias with some success, but the fundamental limitation of using density distribution remains an obstacle to discovering clusters of arbitrary shapes, sizes and densities. Using the mass distribution as a better foundation, we propose a new algorithm which maximizes the total mass of all clusters, called mass-maximization clustering (MMC). The algorithm can be easily changed to maximize the total density of all clusters in order to examine the fundamental limitation of using density distribution versus mass distribution. The key advantage of the MMC over the density-maximization clustering is that the maximization is conducted without a bias towards dense clusters.

</details>


### [84] [Memorize Early, Then Query: Inlier-Memorization-Guided Active Outlier Detection](https://arxiv.org/abs/2601.10993)
*Minseo Kang,Seunghwan Park,Dongha Kim*

Main category: stat.ML

TL;DR: IMBoost：通过主动学习和强化内点记忆效应提升无监督异常检测性能的新框架


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖内点记忆效应的无监督异常检测方法在处理内点和异常点分离不明显或异常点形成密集簇时效果不佳，需要更有效的方法来提升检测性能。

Method: 提出IMBoost两阶段框架：1）预热阶段诱导和促进内点记忆效应；2）极化阶段使用主动查询的样本最大化内点和异常点得分的差异，包含新颖的查询策略和定制损失函数。

Result: 在多个基准数据集上的实验表明，IMBoost不仅显著优于最先进的主动异常检测方法，而且计算成本大幅降低。

Conclusion: IMBoost通过结合主动学习和强化内点记忆效应，有效解决了传统无监督异常检测方法的局限性，为异常检测提供了高效且性能优越的解决方案。

Abstract: Outlier detection (OD) aims to identify abnormal instances, known as outliers or anomalies, by learning typical patterns of normal data, or inliers. Performing OD under an unsupervised regime-without any information about anomalous instances in the training data-is challenging. A recently observed phenomenon, known as the inlier-memorization (IM) effect, where deep generative models (DGMs) tend to memorize inlier patterns during early training, provides a promising signal for distinguishing outliers. However, existing unsupervised approaches that rely solely on the IM effect still struggle when inliers and outliers are not well-separated or when outliers form dense clusters. To address these limitations, we incorporate active learning to selectively acquire informative labels, and propose IMBoost, a novel framework that explicitly reinforces the IM effect to improve outlier detection. Our method consists of two stages: 1) a warm-up phase that induces and promotes the IM effect, and 2) a polarization phase in which actively queried samples are used to maximize the discrepancy between inlier and outlier scores. In particular, we propose a novel query strategy and tailored loss function in the polarization phase to effectively identify informative samples and fully leverage the limited labeling budget. We provide a theoretical analysis showing that the IMBoost consistently decreases inlier risk while increasing outlier risk throughout training, thereby amplifying their separation. Extensive experiments on diverse benchmark datasets demonstrate that IMBoost not only significantly outperforms state-of-the-art active OD methods but also requires substantially less computational cost.

</details>


### [85] [Contextual Distributionally Robust Optimization with Causal and Continuous Structure: An Interpretable and Tractable Approach](https://arxiv.org/abs/2601.11016)
*Fenglin Zhang,Jie Wang*

Main category: stat.ML

TL;DR: 提出因果Sinkhorn分布鲁棒优化框架，结合因果结构、连续传输和可解释决策规则，开发高效算法并在实验中验证性能。


<details>
  <summary>Details</summary>
Motivation: 传统分布鲁棒优化方法往往忽略数据的因果结构和连续性特征，同时缺乏可解释性。需要开发既能考虑因果一致性、连续传输，又能提供可解释决策规则的框架。

Method: 1) 提出因果Sinkhorn散度(CSD)，结合熵正则化和因果一致性；2) 构建基于CSD的上下文分布鲁棒优化模型(Causal-SDRO)；3) 设计软回归森林(SRF)决策规则，保持可解释性同时具备参数化、可微和Lipschitz平滑特性；4) 开发随机组合梯度算法，收敛速度达到O(ε⁻⁴)。

Result: 1) 推导出Causal-SDRO的强对偶重构，证明最坏分布是Gibbs分布的混合；2) SRF能在任意可测函数空间中逼近最优策略；3) 算法收敛速度与标准随机梯度下降匹配；4) 在合成和真实数据集上验证了方法的优越性能和可解释性。

Conclusion: 提出的Causal-SDRO框架成功整合了因果结构、连续传输和可解释决策规则，在保持理论保证的同时提供了实用的算法实现，为分布鲁棒优化领域提供了新的解决方案。

Abstract: In this paper, we introduce a framework for contextual distributionally robust optimization (DRO) that considers the causal and continuous structure of the underlying distribution by developing interpretable and tractable decision rules that prescribe decisions using covariates. We first introduce the causal Sinkhorn discrepancy (CSD), an entropy-regularized causal Wasserstein distance that encourages continuous transport plans while preserving the causal consistency. We then formulate a contextual DRO model with a CSD-based ambiguity set, termed Causal Sinkhorn DRO (Causal-SDRO), and derive its strong dual reformulation where the worst-case distribution is characterized as a mixture of Gibbs distributions. To solve the corresponding infinite-dimensional policy optimization, we propose the Soft Regression Forest (SRF) decision rule, which approximates optimal policies within arbitrary measurable function spaces. The SRF preserves the interpretability of classical decision trees while being fully parametric, differentiable, and Lipschitz smooth, enabling intrinsic interpretation from both global and local perspectives. To solve the Causal-SDRO with parametric decision rules, we develop an efficient stochastic compositional gradient algorithm that converges to an $\varepsilon$-stationary point at a rate of $O(\varepsilon^{-4})$, matching the convergence rate of standard stochastic gradient descent. Finally, we validate our method through numerical experiments on synthetic and real-world datasets, demonstrating its superior performance and interpretability.

</details>


### [86] [Split-and-Conquer: Distributed Factor Modeling for High-Dimensional Matrix-Variate Time Series](https://arxiv.org/abs/2601.11091)
*Hangjin Jiang,Yuzhou Li,Zhaoxing Gao*

Main category: stat.ML

TL;DR: 提出一个分布式框架，用于通过因子模型降低高维、大规模、异构矩阵变量时间序列数据的维度，保留潜在矩阵结构以提高计算效率和信息利用率。


<details>
  <summary>Details</summary>
Motivation: 处理高维、大规模、异构矩阵变量时间序列数据时，现有分布式方法往往破坏数据的矩阵结构，导致计算效率低下和信息利用不足。

Method: 采用分布式框架：1) 按列(或行)分区数据到节点服务器；2) 各节点通过二维张量PCA估计行(或列)加载矩阵；3) 本地估计传输到中央服务器聚合；4) 最终PCA步骤获得全局加载矩阵估计；5) 计算对应的因子矩阵。还讨论了未知分组时的行列聚类方法，并扩展到单位根非平稳矩阵变量时间序列。

Result: 推导了数据维度发散和样本量T增长时的渐近性质。仿真结果表明该方法在计算效率和估计精度方面表现优异，实际数据应用验证了其预测性能。

Conclusion: 提出的分布式框架有效保留了矩阵结构，显著提高了计算效率和信息利用率，适用于大规模矩阵变量时间序列数据的降维分析，并能扩展到非平稳数据和未知分组情况。

Abstract: In this paper, we propose a distributed framework for reducing the dimensionality of high-dimensional, large-scale, heterogeneous matrix-variate time series data using a factor model. The data are first partitioned column-wise (or row-wise) and allocated to node servers, where each node estimates the row (or column) loading matrix via two-dimensional tensor PCA. These local estimates are then transmitted to a central server and aggregated, followed by a final PCA step to obtain the global row (or column) loading matrix estimator. Given the estimated loading matrices, the corresponding factor matrices are subsequently computed. Unlike existing distributed approaches, our framework preserves the latent matrix structure, thereby improving computational efficiency and enhancing information utilization. We also discuss row- and column-wise clustering procedures for settings in which the group memberships are unknown. Furthermore, we extend the analysis to unit-root nonstationary matrix-variate time series. Asymptotic properties of the proposed method are derived for the diverging dimension of the data in each computing unit and the sample size $T$. Simulation results assess the computational efficiency and estimation accuracy of the proposed framework, and real data applications further validate its predictive performance.

</details>
