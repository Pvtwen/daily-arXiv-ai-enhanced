{"id": "2508.05663", "categories": ["stat.ML", "cs.CR", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05663", "abs": "https://arxiv.org/abs/2508.05663", "authors": ["Xingran Chen", "Parimal Parag", "Rohit Bhagat", "Zonghong Liu", "Salim El Rouayheb"], "title": "Random Walk Learning and the Pac-Man Attack", "comment": null, "summary": "Random walk (RW)-based algorithms have long been popular in distributed\nsystems due to low overheads and scalability, with recent growing applications\nin decentralized learning. However, their reliance on local interactions makes\nthem inherently vulnerable to malicious behavior. In this work, we investigate\nan adversarial threat that we term the ``Pac-Man'' attack, in which a malicious\nnode probabilistically terminates any RW that visits it. This stealthy behavior\ngradually eliminates active RWs from the network, effectively halting the\nlearning process without triggering failure alarms. To counter this threat, we\npropose the Average Crossing (AC) algorithm--a fully decentralized mechanism\nfor duplicating RWs to prevent RW extinction in the presence of Pac-Man. Our\ntheoretical analysis establishes that (i) the RW population remains almost\nsurely bounded under AC and (ii) RW-based stochastic gradient descent remains\nconvergent under AC, even in the presence of Pac-Man, with a quantifiable\ndeviation from the true optimum. Our extensive empirical results on both\nsynthetic and real-world datasets corroborate our theoretical findings.\nFurthermore, they uncover a phase transition in the extinction probability as a\nfunction of the duplication threshold. We offer theoretical insights by\nanalyzing a simplified variant of the AC, which sheds light on the observed\nphase transition.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cPac-Man\u201d\u7684\u653b\u51fb\uff0c\u6076\u610f\u8282\u70b9\u4f1a\u7ec8\u6b62\u968f\u673a\u6e38\u8d70\uff08RW\uff09\uff0c\u4ece\u800c\u7834\u574f\u5206\u5e03\u5f0f\u5b66\u4e60\u3002\u4f5c\u8005\u63d0\u51fa\u4e86Average Crossing\uff08AC\uff09\u7b97\u6cd5\u6765\u5bf9\u6297\u8fd9\u79cd\u653b\u51fb\uff0c\u4fdd\u8bc1RW\u7684\u5b58\u6d3b\u548c\u5b66\u4e60\u8fc7\u7a0b\u7684\u6536\u655b\u3002", "motivation": "\u968f\u673a\u6e38\u8d70\u7b97\u6cd5\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5bf9\u672c\u5730\u4ea4\u4e92\u7684\u4f9d\u8d56\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u6076\u610f\u884c\u4e3a\u7684\u653b\u51fb\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Average Crossing\uff08AC\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u590d\u5236RW\u6765\u9632\u6b62\u5176\u5728\u201cPac-Man\u201d\u653b\u51fb\u4e0b\u706d\u7edd\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cAC\u7b97\u6cd5\u80fd\u4fdd\u8bc1RW\u6570\u91cf\u51e0\u4e4e\u5fc5\u7136\u6709\u754c\uff0c\u4e14\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u5728AC\u4e0b\u4ecd\u80fd\u6536\u655b\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u53d1\u73b0\u706d\u7edd\u6982\u7387\u5b58\u5728\u76f8\u53d8\u73b0\u8c61\u3002", "conclusion": "AC\u7b97\u6cd5\u80fd\u6709\u6548\u5bf9\u6297\u201cPac-Man\u201d\u653b\u51fb\uff0c\u4fdd\u8bc1\u5206\u5e03\u5f0f\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.05715", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05715", "abs": "https://arxiv.org/abs/2508.05715", "authors": ["Johannes Piller", "L\u00e9a Orsini", "Simon Wiegrebe", "John Zobolas", "Lukas Burk", "Sophie Hanna Langbein", "Philip Studener", "Markus Goeswein", "Andreas Bender"], "title": "Reduction Techniques for Survival Analysis", "comment": null, "summary": "In this work, we discuss what we refer to as reduction techniques for\nsurvival analysis, that is, techniques that \"reduce\" a survival task to a more\ncommon regression or classification task, without ignoring the specifics of\nsurvival data. Such techniques particularly facilitate machine learning-based\nsurvival analysis, as they allow for applying standard tools from machine and\ndeep learning to many survival tasks without requiring custom learners. We\nprovide an overview of different reduction techniques and discuss their\nrespective strengths and weaknesses. We also provide a principled\nimplementation of some of these reductions, such that they are directly\navailable within standard machine learning workflows. We illustrate each\nreduction using dedicated examples and perform a benchmark analysis that\ncompares their predictive performance to established machine learning methods\nfor survival analysis.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u5b58\u5206\u6790\u4e2d\u7684\u2018\u964d\u7ef4\u6280\u672f\u2019\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u56de\u5f52\u6216\u5206\u7c7b\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u7559\u751f\u5b58\u6570\u636e\u7279\u6027\uff0c\u4fbf\u4e8e\u673a\u5668\u5b66\u4e60\u5e94\u7528\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u751f\u5b58\u5206\u6790\u4efb\u52a1\uff0c\u4f7f\u5176\u80fd\u591f\u5229\u7528\u6807\u51c6\u673a\u5668\u5b66\u4e60\u5de5\u5177\uff0c\u800c\u65e0\u9700\u5b9a\u5236\u5b66\u4e60\u5668\u3002", "method": "\u7efc\u8ff0\u4e86\u591a\u79cd\u964d\u7ef4\u6280\u672f\uff0c\u5e76\u5b9e\u73b0\u4e86\u90e8\u5206\u6280\u672f\u4ee5\u9002\u914d\u6807\u51c6\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\u3002", "result": "\u901a\u8fc7\u793a\u4f8b\u548c\u57fa\u51c6\u5206\u6790\u6bd4\u8f83\u4e86\u8fd9\u4e9b\u6280\u672f\u4e0e\u4f20\u7edf\u751f\u5b58\u5206\u6790\u65b9\u6cd5\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u964d\u7ef4\u6280\u672f\u4e3a\u751f\u5b58\u5206\u6790\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2508.05764", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "15A15, 65F99, 65C05, 68W20, 68Q32"], "pdf": "https://arxiv.org/pdf/2508.05764", "abs": "https://arxiv.org/abs/2508.05764", "authors": ["Arvind K. Saibaba", "Ilse C. F. Ipsen"], "title": "Stochastic Trace Optimization of Parameter Dependent Matrices Based on Statistical Learning Theory", "comment": "3 figures", "summary": "We consider matrices $\\boldsymbol{A}(\\boldsymbol\\theta)\\in\\mathbb{R}^{m\\times\nm}$ that depend, possibly nonlinearly, on a parameter $\\boldsymbol\\theta$ from\na compact parameter space $\\Theta$. We present a Monte Carlo estimator for\nminimizing $\\text{trace}(\\boldsymbol{A}(\\boldsymbol\\theta))$ over all\n$\\boldsymbol\\theta\\in\\Theta$, and determine the sampling amount so that the\nbackward error of the estimator is bounded with high probability. We derive two\ntypes of bounds, based on epsilon nets and on generic chaining. Both types\npredict a small sampling amount for matrices\n$\\boldsymbol{A}(\\boldsymbol\\theta)$ with small offdiagonal mass, and parameter\nspaces $\\Theta$ of small ``size.'' Dependence on the matrix dimension~$m$ is\nonly weak or not explicit. The bounds based on epsilon nets are easier to\nevaluate and come with fully specified constants. In contrast, the bounds based\non chaining depend on the Talagrand functionals which are difficult to\nevaluate, except in very special cases. Comparisons between the two types of\nbounds are difficult, although the literature suggests that chaining bounds can\nbe superior.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u6700\u5c0f\u5316\u53c2\u6570\u4f9d\u8d56\u77e9\u9635\u7684\u8ff9\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u8fb9\u754c\u65b9\u6cd5\uff08epsilon nets\u548cgeneric chaining\uff09\u786e\u4fdd\u4f30\u8ba1\u5668\u7684\u540e\u5411\u8bef\u5dee\u9ad8\u6982\u7387\u6709\u754c\u3002", "motivation": "\u7814\u7a76\u53c2\u6570\u4f9d\u8d56\u77e9\u9635\u7684\u8ff9\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u9ad8\u6548\u4e14\u7406\u8bba\u4fdd\u8bc1\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\uff0c\u7ed3\u5408epsilon nets\u548cgeneric chaining\u4e24\u79cd\u8fb9\u754c\u65b9\u6cd5\uff0c\u63a7\u5236\u91c7\u6837\u91cf\u4ee5\u786e\u4fdd\u8bef\u5dee\u6709\u754c\u3002", "result": "\u4e24\u79cd\u8fb9\u754c\u65b9\u6cd5\u5747\u9002\u7528\u4e8e\u5c0f\u975e\u5bf9\u89d2\u7ebf\u8d28\u91cf\u548c\u53c2\u6570\u7a7a\u95f4\u5c0f\u7684\u77e9\u9635\uff0cepsilon nets\u66f4\u6613\u8ba1\u7b97\uff0c\u800cchaining\u8fb9\u754c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u66f4\u4f18\u3002", "conclusion": "\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\u5728\u5c0f\u91c7\u6837\u91cf\u4e0b\u6709\u6548\uff0cepsilon nets\u5b9e\u7528\u6027\u5f3a\uff0cchaining\u8fb9\u754c\u7406\u8bba\u6f5c\u529b\u5927\u4f46\u8ba1\u7b97\u590d\u6742\u3002"}}
{"id": "2508.06069", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06069", "abs": "https://arxiv.org/abs/2508.06069", "authors": ["Bo Yang", "Ruixuan Luo", "Junqi Jin", "Han Zhu"], "title": "Lightweight Auto-bidding based on Traffic Prediction in Live Advertising", "comment": null, "summary": "Internet live streaming is widely used in online entertainment and\ne-commerce, where live advertising is an important marketing tool for anchors.\nAn advertising campaign hopes to maximize the effect (such as conversions)\nunder constraints (such as budget and cost-per-click). The mainstream control\nof campaigns is auto-bidding, where the performance depends on the decision of\nthe bidding algorithm in each request. The most widely used auto-bidding\nalgorithms include Proportional-Integral-Derivative (PID) control, linear\nprogramming (LP), reinforcement learning (RL), etc. Existing methods either do\nnot consider the entire time traffic, or have too high computational\ncomplexity. In this paper, the live advertising has high requirements for\nreal-time bidding (second-level control) and faces the difficulty of unknown\nfuture traffic. Therefore, we propose a lightweight bidding algorithm Binary\nConstrained Bidding (BiCB), which neatly combines the optimal bidding formula\ngiven by mathematical analysis and the statistical method of future traffic\nestimation, and obtains good approximation to the optimal result through a low\ncomplexity solution. In addition, we complement the form of upper and lower\nbound constraints for traditional auto-bidding modeling and give theoretical\nanalysis of BiCB. Sufficient offline and online experiments prove BiCB's good\nperformance and low engineering cost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5b9e\u65f6\u7ade\u4ef7\u7b97\u6cd5BiCB\uff0c\u7ed3\u5408\u6570\u5b66\u5206\u6790\u548c\u7edf\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u76f4\u64ad\u5e7f\u544a\u4e2d\u7684\u5b9e\u65f6\u6027\u548c\u672a\u6765\u6d41\u91cf\u672a\u77e5\u95ee\u9898\u3002", "motivation": "\u76f4\u64ad\u5e7f\u544a\u9700\u8981\u5b9e\u65f6\u7ade\u4ef7\uff08\u79d2\u7ea7\u63a7\u5236\uff09\u4e14\u9762\u4e34\u672a\u6765\u6d41\u91cf\u672a\u77e5\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u672a\u8003\u8651\u5168\u65f6\u6bb5\u6d41\u91cf\uff0c\u8981\u4e48\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u63d0\u51faBinary Constrained Bidding (BiCB)\u7b97\u6cd5\uff0c\u7ed3\u5408\u6700\u4f18\u7ade\u4ef7\u516c\u5f0f\u548c\u672a\u6765\u6d41\u91cf\u7edf\u8ba1\u4f30\u8ba1\uff0c\u901a\u8fc7\u4f4e\u590d\u6742\u5ea6\u65b9\u6848\u903c\u8fd1\u6700\u4f18\u7ed3\u679c\u3002", "result": "\u79bb\u7ebf\u4e0e\u5728\u7ebf\u5b9e\u9a8c\u8bc1\u660eBiCB\u6027\u80fd\u4f18\u5f02\u4e14\u5de5\u7a0b\u6210\u672c\u4f4e\u3002", "conclusion": "BiCB\u4e3a\u76f4\u64ad\u5e7f\u544a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u590d\u6742\u5ea6\u7684\u5b9e\u65f6\u7ade\u4ef7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05659", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.05659", "abs": "https://arxiv.org/abs/2508.05659", "authors": ["Jeroen F. Uleman", "Loes Crielaard", "Leonie K. Elsenburg", "Guido A. Veldhuis", "Karien Stronks", "Naja Hulvej Rod", "Rick Quax", "V\u00edtor V. Vasconcelos"], "title": "Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty", "comment": "21 pages, 4 figures, 4 tables", "summary": "Causal loop diagrams (CLDs) are widely used in health and environmental\nresearch to represent hypothesized causal structures underlying complex\nproblems. However, as qualitative and static representations, CLDs are limited\nin their ability to support dynamic analysis and inform intervention\nstrategies. Additionally, quantitative CLD analysis methods like network\ncentrality analysis often lead to false inference. We propose\nDiagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory\nsystem dynamics models (SDMs) in the absence of empirical data. With minimal\nuser input - following a protocol to label variables as stocks,\nflows/auxiliaries, or constants - D2D leverages the structural information\nalready encoded in CLDs, namely, link existence and polarity, to simulate\nhypothetical interventions and explore potential leverage points under\nuncertainty. Results suggest that D2D helps distinguish between high- and\nlow-ranked leverage points. We compare D2D to a data-driven SDM constructed\nfrom the same CLD and variable labeling. D2D showed greater consistency with\nthe data-driven model than network centrality analysis, while providing\nuncertainty estimates and guidance for future data collection. The method is\nimplemented in an open-source Python package and a web-based application to\nsupport further testing and lower the barrier to dynamic modeling for\nresearchers working with CLDs. We expect additional validation will further\nestablish the approach's utility across a broad range of cases and domains.", "AI": {"tldr": "\u63d0\u51faDiagrams-to-Dynamics (D2D)\u65b9\u6cd5\uff0c\u5c06\u56e0\u679c\u5faa\u73af\u56fe(CLDs)\u8f6c\u5316\u4e3a\u7cfb\u7edf\u52a8\u529b\u5b66\u6a21\u578b(SDMs)\uff0c\u652f\u6301\u52a8\u6001\u5206\u6790\u548c\u5e72\u9884\u7b56\u7565\u63a2\u7d22\u3002", "motivation": "CLDs\u4f5c\u4e3a\u9759\u6001\u5b9a\u6027\u5de5\u5177\uff0c\u65e0\u6cd5\u652f\u6301\u52a8\u6001\u5206\u6790\u4e14\u6613\u5bfc\u81f4\u9519\u8bef\u63a8\u65ad\uff0c\u9700\u6539\u8fdb\u65b9\u6cd5\u4ee5\u63d0\u5347\u5206\u6790\u80fd\u529b\u3002", "method": "D2D\u5229\u7528CLDs\u7684\u7ed3\u6784\u4fe1\u606f\uff08\u94fe\u63a5\u5b58\u5728\u6027\u548c\u6781\u6027\uff09\uff0c\u901a\u8fc7\u7528\u6237\u7b80\u5355\u6807\u6ce8\u53d8\u91cf\u7c7b\u578b\uff08\u5b58\u91cf\u3001\u6d41\u91cf/\u8f85\u52a9\u53d8\u91cf\u3001\u5e38\u91cf\uff09\u751f\u6210SDMs\uff0c\u6a21\u62df\u5e72\u9884\u548c\u63a2\u7d22\u6760\u6746\u70b9\u3002", "result": "D2D\u80fd\u533a\u5206\u9ad8\u4f4e\u4f18\u5148\u7ea7\u6760\u6746\u70b9\uff0c\u4e0e\u6570\u636e\u9a71\u52a8\u6a21\u578b\u4e00\u81f4\u6027\u4f18\u4e8e\u7f51\u7edc\u4e2d\u5fc3\u6027\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u6570\u636e\u6536\u96c6\u6307\u5bfc\u3002", "conclusion": "D2D\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u5b9e\u73b0\uff0c\u964d\u4f4e\u4e86\u52a8\u6001\u5efa\u6a21\u95e8\u69db\uff0c\u672a\u6765\u9a8c\u8bc1\u5c06\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.05801", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05801", "abs": "https://arxiv.org/abs/2508.05801", "authors": ["Yingbo Hua"], "title": "A Remark on the AAA Method for Secret-Key Generation in Mobile Networks", "comment": null, "summary": "A broadly applicable method for secret-key generation is named for its\naccumulative, adaptable and additive (AAA) properties. This paper first shows a\nrobustness of its performance. Namely, even if there is an inter correlation or\na leakage caused intra correlation among the superimposed packets, provided\nthere is a nonzero probability for each packet to be missed in full or in part\nby Eve, then the equivocation of the key generated by the AAA method always\nbecomes perfect as the number of superpositions becomes infinite. Also shown in\nthis paper is a comparison between the AAA method and an ideal method based on\nreciprocal channel estimation, which reveals several advantages of the AAA\nmethod.", "AI": {"tldr": "AAA\u65b9\u6cd5\u5728\u5bc6\u94a5\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5b58\u5728\u76f8\u5173\u6027\u6216\u6cc4\u6f0f\uff0c\u53ea\u8981Eve\u6709\u975e\u96f6\u6982\u7387\u4e22\u5931\u90e8\u5206\u6216\u5168\u90e8\u6570\u636e\u5305\uff0c\u5bc6\u94a5\u7684\u6a21\u7cca\u6027\u4f1a\u968f\u7740\u53e0\u52a0\u6b21\u6570\u7684\u589e\u52a0\u8d8b\u4e8e\u5b8c\u7f8e\u3002", "motivation": "\u7814\u7a76AAA\u65b9\u6cd5\u5728\u5bc6\u94a5\u751f\u6210\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u76f8\u5173\u6027\u548c\u6cc4\u6f0f\u7684\u60c5\u51b5\u4e0b\uff0c\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u6bd4\u8f83AAA\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u4e92\u6613\u4fe1\u9053\u4f30\u8ba1\u7684\u7406\u60f3\u65b9\u6cd5\u3002", "result": "AAA\u65b9\u6cd5\u5728\u53e0\u52a0\u6b21\u6570\u8d8b\u4e8e\u65e0\u9650\u65f6\uff0c\u5bc6\u94a5\u6a21\u7cca\u6027\u8d8b\u4e8e\u5b8c\u7f8e\uff0c\u4e14\u76f8\u6bd4\u7406\u60f3\u65b9\u6cd5\u5177\u6709\u591a\u9879\u4f18\u52bf\u3002", "conclusion": "AAA\u65b9\u6cd5\u5728\u5bc6\u94a5\u751f\u6210\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4f18\u4e8e\u7406\u60f3\u65b9\u6cd5\u3002"}}
{"id": "2508.06337", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.06337", "abs": "https://arxiv.org/abs/2508.06337", "authors": ["Benedikt Fr\u00f6hlich", "Alison Durst", "Merle Behr"], "title": "Decorrelated feature importance from local sample weighting", "comment": null, "summary": "Feature importance (FI) statistics provide a prominent and valuable method of\ninsight into the decision process of machine learning (ML) models, but their\neffectiveness has well-known limitations when correlation is present among the\nfeatures in the training data. In this case, the FI often tends to be\ndistributed among all features which are in correlation with the\nresponse-generating signal features. Even worse, if multiple signal features\nare in strong correlation with a noise feature, while being only modestly\ncorrelated with one another, this can result in a noise feature having a\ndistinctly larger FI score than any signal feature. Here we propose local\nsample weighting (losaw) which can flexibly be integrated into many ML\nalgorithms to improve FI scores in the presence of feature correlation in the\ntraining data. Our approach is motivated from inverse probability weighting in\ncausal inference and locally, within the ML model, uses a sample weighting\nscheme to decorrelate a target feature from the remaining features. This\nreduces model bias locally, whenever the effect of a potential signal feature\nis evaluated and compared to others. Moreover, losaw comes with a natural\ntuning parameter, the minimum effective sample size of the weighted population,\nwhich corresponds to an interpretation-prediction-tradeoff, analog to a\nbias-variance-tradeoff as for classical ML tuning parameters. We demonstrate\nhow losaw can be integrated within decision tree-based ML methods and within\nmini-batch training of neural networks. We investigate losaw for random forest\nand convolutional neural networks in a simulation study on settings showing\ndiverse correlation patterns. We found that losaw improves FI consistently.\nMoreover, it often improves prediction accuracy for out-of-distribution, while\nmaintaining a similar accuracy for in-distribution test data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3alosaw\u7684\u5c40\u90e8\u6837\u672c\u52a0\u6743\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u7279\u5f81\u76f8\u5173\u6027\u5b58\u5728\u65f6\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6837\u672c\u52a0\u6743\u51cf\u5c11\u6a21\u578b\u504f\u5dee\uff0c\u5e76\u5728\u6a21\u62df\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7279\u5f81\u76f8\u5173\u6027\u4f1a\u5e72\u6270\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u7684\u51c6\u786e\u6027\uff0c\u751a\u81f3\u53ef\u80fd\u5bfc\u81f4\u566a\u58f0\u7279\u5f81\u7684\u91cd\u8981\u6027\u9ad8\u4e8e\u4fe1\u53f7\u7279\u5f81\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u6539\u5584\u8fd9\u79cd\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u6837\u672c\u52a0\u6743\uff08losaw\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u9006\u6982\u7387\u52a0\u6743\u51cf\u5c11\u76ee\u6807\u7279\u5f81\u4e0e\u5176\u4ed6\u7279\u5f81\u7684\u5173\u8054\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u504f\u5dee\u3002\u8be5\u65b9\u6cd5\u53ef\u96c6\u6210\u5230\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e2d\u3002", "result": "\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0closaw\u80fd\u663e\u8457\u6539\u5584\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u5e76\u5728\u4fdd\u6301\u5206\u5e03\u5185\u6570\u636e\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u5206\u5e03\u5916\u6570\u636e\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "losaw\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.05724", "categories": ["cs.LG", "physics.data-an", "68T07, 81-08, 05C90", "I.2.6; G.2.2; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.05724", "abs": "https://arxiv.org/abs/2508.05724", "authors": ["Massimiliano Romiti"], "title": "A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics", "comment": "14 pages, 9 figures", "summary": "This work introduces a novel framework for representing and analyzing\nphysical laws as a weighted knowledge graph. We constructed a database of 659\ndistinct physical equations, subjected to rigorous semantic cleaning to resolve\nnotational ambiguities, resulting in a corpus of 400 advanced physics\nequations. We developed an enhanced graph representation where both physical\nconcepts and equations are nodes, connected by weighted inter-equation bridges.\nThese weights are objectively defined using normalized metrics for variable\noverlap, physics-informed importance scores, and bibliometric data. A Graph\nAttention Network (GAT) was trained for link prediction, achieving a test AUC\nof 0.9742 +/- 0.0018 across five independent runs, significantly outperforming\nboth classical heuristics (best baseline AUC: 0.9487) and established GNN\narchitectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing\nconfirmed significance of all comparisons (p < 0.05), with 2.7% improvement\nover the best baseline. Our analysis reveals three key findings: (i) The model\nautonomously rediscovers the known macroscopic structure of physics,\nidentifying strong conceptual axes between Electromagnetism and Statistical\nMechanics. (ii) It identifies central hub equations that serve as critical\nbridges between multiple physical domains. (iii) The model generates stable,\ncomputationally-derived hypotheses for cross-domain relationships, identifying\nboth known principles and suggesting novel mathematical analogies for further\ntheoretical investigation. The framework can generate hundreds of such\nhypotheses, enabling the creation of specialized datasets for targeted analysis\nof specific physics subfields. Code and data available at\nhttps://github.com/kingelanci/graphysics", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u7269\u7406\u5b9a\u5f8b\u8868\u793a\u4e3a\u52a0\u6743\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u901a\u8fc7\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u8fdb\u884c\u94fe\u63a5\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7269\u7406\u65b9\u7a0b\u8868\u793a\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u8de8\u9886\u57df\u7269\u7406\u5173\u7cfb\u3002", "method": "\u6784\u5efa\u5305\u542b400\u4e2a\u9ad8\u7ea7\u7269\u7406\u65b9\u7a0b\u7684\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u52a0\u6743\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\uff0c\u8bad\u7ec3GAT\u8fdb\u884c\u94fe\u63a5\u9884\u6d4b\u3002", "result": "GAT\u5728\u6d4b\u8bd5\u4e2dAUC\u8fbe\u52300.9742\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u7269\u7406\u5b66\u7684\u5b8f\u89c2\u7ed3\u6784\u548c\u8de8\u9886\u57df\u5173\u7cfb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u91cd\u65b0\u53d1\u73b0\u4e86\u5df2\u77e5\u7684\u7269\u7406\u7ed3\u6784\uff0c\u8fd8\u63d0\u51fa\u4e86\u65b0\u7684\u8de8\u9886\u57df\u5173\u7cfb\u5047\u8bbe\uff0c\u4e3a\u7269\u7406\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2508.05882", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05882", "abs": "https://arxiv.org/abs/2508.05882", "authors": ["Yingbo Hua"], "title": "STEEP -- An Alternative To Quantum Key Distribution", "comment": null, "summary": "Secret-message transmission by echoing encrypted probes (STEEP) is discussed\nas an alternative to quantum key distribution (QKD). The former only needs\nclassic or non-quantum channels while the latter needs both quantum and classic\nchannels for secret-key generation. STEEP is shown to yield a secrecy rate\nsufficient for one-time pads encryption in many practical situations including\nin-air channels or undersea optical cables. Other advantages of STEEP over QKD\ninclude cost, complexity, compatibility, and robustness against constant\neavesdropping.", "AI": {"tldr": "STEEP\u662f\u4e00\u79cd\u66ff\u4ee3QKD\u7684\u79d8\u5bc6\u4fe1\u606f\u4f20\u8f93\u65b9\u6cd5\uff0c\u4ec5\u9700\u7ecf\u5178\u4fe1\u9053\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\uff0c\u4e14\u6210\u672c\u66f4\u4f4e\u3001\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "motivation": "\u63d0\u51faSTEEP\u4f5c\u4e3aQKD\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u56e0\u5176\u4ec5\u9700\u7ecf\u5178\u4fe1\u9053\uff0c\u66f4\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u56de\u663e\u52a0\u5bc6\u63a2\u9488\u5b9e\u73b0\u79d8\u5bc6\u4fe1\u606f\u4f20\u8f93\u3002", "result": "STEEP\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\uff08\u5982\u7a7a\u4e2d\u6216\u6d77\u5e95\u5149\u7f06\uff09\u80fd\u63d0\u4f9b\u8db3\u591f\u7684\u5b89\u5168\u901f\u7387\uff0c\u4e14\u6210\u672c\u3001\u590d\u6742\u5ea6\u548c\u517c\u5bb9\u6027\u4f18\u4e8eQKD\u3002", "conclusion": "STEEP\u662f\u4e00\u79cd\u66f4\u5b9e\u7528\u3001\u66f4\u7ecf\u6d4e\u7684\u79d8\u5bc6\u4fe1\u606f\u4f20\u8f93\u65b9\u6cd5\uff0c\u4f18\u4e8eQKD\u3002"}}
{"id": "2508.06243", "categories": ["cs.LG", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06243", "abs": "https://arxiv.org/abs/2508.06243", "authors": ["Ioan-Sorin Comsa", "Purav Shah", "Karthik Vaidhyanathan", "Deepak Gangadharan", "Christof Imhof", "Per Bergamin", "Aryan Kaushik", "Gabriel-Miro Muntean", "Ramona Trestian"], "title": "SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems", "comment": null, "summary": "The advent of 6G networks opens new possibilities for connected infotainment\nservices in vehicular environments. However, traditional Radio Resource\nManagement (RRM) techniques struggle with the increasing volume and complexity\nof data such as Channel Quality Indicators (CQI) from autonomous vehicles. To\naddress this, we propose SCAR (State-Space Compression for AI-Driven Resource\nManagement), an Edge AI-assisted framework that optimizes scheduling and\nfairness in vehicular infotainment. SCAR employs ML-based compression\ntechniques (e.g., clustering and RBF networks) to reduce CQI data size while\npreserving essential features. These compressed states are used to train\n6G-enabled Reinforcement Learning policies that maximize throughput while\nmeeting fairness objectives defined by the NGMN. Simulations show that SCAR\nincreases time in feasible scheduling regions by 14\\% and reduces unfair\nscheduling time by 15\\% compared to RL baselines without CQI compression.\nFurthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based\nclustering reduces CQI clustering distortion by 10\\%, confirming its\nefficiency. These results demonstrate SCAR's scalability and fairness benefits\nfor dynamic vehicular networks.", "AI": {"tldr": "SCAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fb9\u7f18AI\u7684\u6846\u67b6\uff0c\u901a\u8fc7ML\u538b\u7f29\u6280\u672f\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u53166G\u8f66\u8f7d\u5a31\u4e50\u7f51\u7edc\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u63d0\u5347\u8c03\u5ea6\u516c\u5e73\u6027\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edfRRM\u6280\u672f\u96be\u4ee5\u5e94\u5bf96G\u8f66\u8f7d\u7f51\u7edc\u4e2d\u590d\u6742\u7684\u6570\u636e\uff08\u5982CQI\uff09\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "SCAR\u4f7f\u7528ML\u538b\u7f29\u6280\u672f\uff08\u5982\u805a\u7c7b\u548cRBF\u7f51\u7edc\uff09\u51cf\u5c11CQI\u6570\u636e\u91cf\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8c03\u5ea6\u548c\u516c\u5e73\u6027\u3002", "result": "SCAR\u5c06\u53ef\u884c\u8c03\u5ea6\u533a\u57df\u65f6\u95f4\u63d0\u534714%\uff0c\u51cf\u5c11\u4e0d\u516c\u5e73\u8c03\u5ea6\u65f6\u95f415%\uff0c\u4e14SAST\u805a\u7c7b\u964d\u4f4eCQI\u5931\u771f10%\u3002", "conclusion": "SCAR\u5728\u52a8\u6001\u8f66\u8f7d\u7f51\u7edc\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u516c\u5e73\u6027\u4f18\u52bf\u3002"}}
{"id": "2508.06377", "categories": ["stat.ML", "cs.CR", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2508.06377", "abs": "https://arxiv.org/abs/2508.06377", "authors": ["Thomas Michel", "Debabrota Basu", "Emilie Kaufmann"], "title": "DP-SPRT: Differentially Private Sequential Probability Ratio Tests", "comment": null, "summary": "We revisit Wald's celebrated Sequential Probability Ratio Test for sequential\ntests of two simple hypotheses, under privacy constraints. We propose DP-SPRT,\na wrapper that can be calibrated to achieve desired error probabilities and\nprivacy constraints, addressing a significant gap in previous work. DP-SPRT\nrelies on a private mechanism that processes a sequence of queries and stops\nafter privately determining when the query results fall outside a predefined\ninterval. This OutsideInterval mechanism improves upon naive composition of\nexisting techniques like AboveThreshold, potentially benefiting other\nsequential algorithms. We prove generic upper bounds on the error and sample\ncomplexity of DP-SPRT that can accommodate various noise distributions based on\nthe practitioner's privacy needs. We exemplify them in two settings: Laplace\nnoise (pure Differential Privacy) and Gaussian noise (R\\'enyi differential\nprivacy). In the former setting, by providing a lower bound on the sample\ncomplexity of any $\\epsilon$-DP test with prescribed type I and type II errors,\nwe show that DP-SPRT is near optimal when both errors are small and the two\nhypotheses are close. Moreover, we conduct an experimental study revealing its\ngood practical performance.", "AI": {"tldr": "DP-SPRT\u662f\u4e00\u79cd\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u6539\u8fdb\u7684\u5e8f\u5217\u6982\u7387\u6bd4\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u79c1\u6709\u673a\u5236\u5904\u7406\u67e5\u8be2\u5e8f\u5217\u5e76\u5728\u7ed3\u679c\u8d85\u51fa\u9884\u8bbe\u533a\u95f4\u65f6\u505c\u6b62\uff0c\u5b9e\u73b0\u4e86\u8bef\u5dee\u548c\u9690\u79c1\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5de5\u4f5c\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u8fdb\u884c\u5e8f\u5217\u5047\u8bbe\u6d4b\u8bd5\u65f6\u7684\u4e0d\u8db3\uff0c\u586b\u8865\u4e86\u8bef\u5dee\u6982\u7387\u548c\u9690\u79c1\u9700\u6c42\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faDP-SPRT\u65b9\u6cd5\uff0c\u5229\u7528OutsideInterval\u673a\u5236\u6539\u8fdb\u73b0\u6709\u6280\u672f\uff0c\u652f\u6301\u591a\u79cd\u566a\u58f0\u5206\u5e03\uff08\u5982\u62c9\u666e\u62c9\u65af\u548c\u9ad8\u65af\u566a\u58f0\uff09\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86DP-SPRT\u7684\u8bef\u5dee\u548c\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u754c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "DP-SPRT\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u63a5\u8fd1\u6700\u4f18\uff0c\u9002\u7528\u4e8e\u5c0f\u8bef\u5dee\u548c\u63a5\u8fd1\u5047\u8bbe\u7684\u573a\u666f\u3002"}}
{"id": "2508.05778", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.05778", "abs": "https://arxiv.org/abs/2508.05778", "authors": ["Jaemin Oh", "Jinsil Lee", "Youngjoon Hong"], "title": "Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems", "comment": "21 pages, 5 figures, 6 tables", "summary": "Nudging is an empirical data assimilation technique that incorporates an\nobservation-driven control term into the model dynamics. The trajectory of the\nnudged system approaches the true system trajectory over time, even when the\ninitial conditions differ. For linear state space models, such control terms\ncan be derived under mild assumptions. However, designing effective nudging\nterms becomes significantly more challenging in the nonlinear setting. In this\nwork, we propose neural network nudging, a data-driven method for learning\nnudging terms in nonlinear state space models. We establish a theoretical\nexistence result based on the Kazantzis--Kravaris--Luenberger observer theory.\nThe proposed approach is evaluated on three benchmark problems that exhibit\nchaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and\nthe Kolmogorov flow.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6570\u636e\u540c\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u8bbe\u8ba1\u6709\u6548\u7684nudging\u9879\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u795e\u7ecf\u7f51\u7edcnudging\u65b9\u6cd5\uff0c\u5229\u7528Kazantzis--Kravaris--Luenberger\u89c2\u6d4b\u5668\u7406\u8bba\u8fdb\u884c\u7406\u8bba\u652f\u6301\u3002", "result": "\u5728Lorenz 96\u6a21\u578b\u3001Kuramoto--Sivashinsky\u65b9\u7a0b\u548cKolmogorov\u6d41\u4e09\u4e2a\u6df7\u6c8c\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edcnudging\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u903c\u8fd1\u771f\u5b9e\u7cfb\u7edf\u8f68\u8ff9\u3002"}}
{"id": "2508.05959", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.05959", "abs": "https://arxiv.org/abs/2508.05959", "authors": ["Amirhossein Taherpour", "Somayeh Khani", "Abbas Taherpour", "Tamer Khattab"], "title": "IRS-Assisted IoT Activity Detection Under Asynchronous Transmission and Heterogeneous Powers: Detectors and Performance Analysis", "comment": null, "summary": "This paper addresses the problem of activity detection in distributed\nInternet of Things (IoT) networks, where devices employ asynchronous\ntransmissions with heterogeneous power levels to report their local\nobservations. The system leverages an intelligent reflecting surface (IRS) to\nenhance detection reliability, with optional incorporation of a direct\nline-of-sight (LoS) path. We formulate the detection problem as a binary\nhypothesis test and develop four detectors: an optimal detector alongside three\ncomputationally efficient detectors designed for practical scenarios with\ndifferent levels of prior knowledge about noise variance, channel state\ninformation, and device transmit powers. For each detector, we derive\nclosed-form expressions for both detection and false alarm probabilities,\nestablishing theoretical performance benchmarks. Extensive simulations validate\nour analytical results and systematically evaluate the impact of key system\nparameters including the number of antennas, samples, users, and IRS elements\non detection performance. The proposed framework effectively bridges\ntheoretical optimality with implementation practicality, providing a scalable\nsolution for IRS-assisted IoT networks in emerging 6G systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u53cd\u5c04\u9762\uff08IRS\uff09\u7684\u5206\u5e03\u5f0f\u7269\u8054\u7f51\uff08IoT\uff09\u7f51\u7edc\u6d3b\u52a8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u68c0\u6d4b\u5668\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0fIoT\u7f51\u7edc\u4e2d\u8bbe\u5907\u5f02\u6b65\u4f20\u8f93\u548c\u529f\u7387\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u6d3b\u52a8\u68c0\u6d4b\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u5c06\u68c0\u6d4b\u95ee\u9898\u5efa\u6a21\u4e3a\u4e8c\u5143\u5047\u8bbe\u68c0\u9a8c\uff0c\u5f00\u53d1\u4e86\u56db\u79cd\u68c0\u6d4b\u5668\uff08\u5305\u62ec\u6700\u4f18\u68c0\u6d4b\u5668\u548c\u4e09\u79cd\u8ba1\u7b97\u9ad8\u6548\u68c0\u6d4b\u5668\uff09\uff0c\u5e76\u63a8\u5bfc\u4e86\u68c0\u6d4b\u548c\u865a\u8b66\u6982\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u5929\u7ebf\u6570\u3001\u6837\u672c\u6570\u3001\u7528\u6237\u6570\u548cIRS\u5143\u7d20\u6570\u7b49\u5173\u952e\u53c2\u6570\u5bf9\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u7406\u8bba\u6700\u4f18\u6027\u548c\u5b9e\u73b0\u5b9e\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a6G\u7cfb\u7edf\u4e2d\u7684IRS\u8f85\u52a9IoT\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05791", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05791", "abs": "https://arxiv.org/abs/2508.05791", "authors": ["Haoran Li", "Lihao Mai", "Muhao Guo", "Jiaqi Wu", "Yang Weng", "Yannan Sun", "Ce Jimmy Liu"], "title": "From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data", "comment": "10 pages", "summary": "Accurate distribution grid topology is essential for reliable modern grid\noperations. However, real-world utility data originates from multiple sources\nwith varying characteristics and levels of quality. In this work, developed in\ncollaboration with Oncor Electric Delivery, we propose a scalable framework\nthat reconstructs a trustworthy grid topology by systematically integrating\nheterogeneous data. We observe that distribution topology is fundamentally\ngoverned by two complementary dimensions: the spatial layout of physical\ninfrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the\nsystem in the signal domain (e.g., voltage time series). When jointly\nleveraged, these dimensions support a complete and physically coherent\nreconstruction of network connectivity. To address the challenge of uneven data\nquality without compromising observability, we introduce a confidence-aware\ninference mechanism that preserves structurally informative yet imperfect\ninputs, while quantifying the reliability of each inferred connection for\noperator interpretation. This soft handling of uncertainty is tightly coupled\nwith hard enforcement of physical feasibility: we embed operational\nconstraints, such as transformer capacity limits and radial topology\nrequirements, directly into the learning process. Together, these components\nensure that inference is both uncertainty-aware and structurally valid,\nenabling rapid convergence to actionable, trustworthy topologies under\nreal-world deployment conditions. The proposed framework is validated using\ndata from over 8000 meters across 3 feeders in Oncor's service territory,\ndemonstrating over 95% accuracy in topology reconstruction and substantial\nimprovements in confidence calibration and computational efficiency relative to\nbaseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5f02\u6784\u6570\u636e\u91cd\u5efa\u53ef\u4fe1\u7684\u7535\u7f51\u62d3\u6251\u7ed3\u6784\uff0c\u7ed3\u5408\u7a7a\u95f4\u5e03\u5c40\u548c\u52a8\u6001\u4fe1\u53f7\u884c\u4e3a\uff0c\u5e76\u5f15\u5165\u7f6e\u4fe1\u611f\u77e5\u63a8\u7406\u673a\u5236\u3002", "motivation": "\u73b0\u4ee3\u7535\u7f51\u8fd0\u884c\u9700\u8981\u51c6\u786e\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u4f46\u5b9e\u9645\u6570\u636e\u6765\u6e90\u591a\u6837\u4e14\u8d28\u91cf\u4e0d\u5747\uff0c\u9700\u89e3\u51b3\u6570\u636e\u6574\u5408\u4e0e\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408GIS\u548c\u7535\u538b\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5f15\u5165\u7f6e\u4fe1\u611f\u77e5\u63a8\u7406\u673a\u5236\uff0c\u5e76\u5d4c\u5165\u7269\u7406\u53ef\u884c\u6027\u7ea6\u675f\u3002", "result": "\u57288000\u591a\u4e2a\u7535\u8868\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u62d3\u6251\u91cd\u5efa\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u7f6e\u4fe1\u6821\u51c6\u548c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u80fd\u5feb\u901f\u6536\u655b\u5230\u53ef\u4fe1\u62d3\u6251\uff0c\u4e3a\u7535\u7f51\u64cd\u4f5c\u63d0\u4f9b\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2508.06022", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.06022", "abs": "https://arxiv.org/abs/2508.06022", "authors": ["Zeping Sui", "Qu Luo", "Zilong Liu", "Murat Temiz", "Leila Musavian", "Christos Masouros", "Yong Liang Guan", "Pei Xiao", "Lajos Hanzo"], "title": "Multi-Functional Chirp Signalling for Next-Generation Multi-Carrier Wireless Networks: Communications, Sensing and ISAC Perspectives", "comment": "8 pages, 5 figures, submitted to IEEE Wireless Communications", "summary": "To meet the increasingly demanding quality-of-service requirements of the\nnext-generation multi-carrier mobile networks, it is essential to design\nmulti-functional signalling schemes facilitating efficient, flexible, and\nreliable communication and sensing in complex wireless environments. As a\ncompelling candidate, we advocate chirp signalling, beneficially amalgamating\nsequences (e.g., Zadoff-Chu sequences) with waveforms (e.g., chirp spread\nspectrum and frequency-modulated continuous wave (FMCW) radar), given their\nresilience against doubly selective channels. Besides chirp sequences, a wide\nrange of chirp waveforms is considered, ranging from FMCW to affine\nfrequency-division multiplexing (AFDM), to create a promising chirp\nmulticarrier waveform. This study also highlights the advantages of such\nwaveforms in supporting reliable high-mobility communications, plus integrated\nsensing and communications (ISAC). Finally, we outline several emerging\nresearch directions for chirp signalling designs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5541\u557e\u4fe1\u53f7\u8bbe\u8ba1\u591a\u529f\u80fd\u901a\u4fe1\u65b9\u6848\uff0c\u4ee5\u6ee1\u8db3\u4e0b\u4e00\u4ee3\u591a\u8f7d\u6ce2\u79fb\u52a8\u7f51\u7edc\u7684\u9ad8\u670d\u52a1\u8d28\u91cf\u9700\u6c42\uff0c\u5e76\u652f\u6301\u9ad8\u79fb\u52a8\u6027\u901a\u4fe1\u4e0e\u96c6\u6210\u611f\u77e5\u901a\u4fe1\uff08ISAC\uff09\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u591a\u8f7d\u6ce2\u79fb\u52a8\u7f51\u7edc\u5bf9\u901a\u4fe1\u548c\u611f\u77e5\u7684\u9ad8\u6548\u3001\u7075\u6d3b\u548c\u53ef\u9760\u6027\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u9700\u8981\u8bbe\u8ba1\u591a\u529f\u80fd\u4fe1\u53f7\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u5541\u557e\u4fe1\u53f7\uff08\u5982Zadoff-Chu\u5e8f\u5217\uff09\u4e0e\u6ce2\u5f62\uff08\u5982\u5541\u557e\u6269\u9891\u548cFMCW\u96f7\u8fbe\uff09\u7ed3\u5408\uff0c\u8bbe\u8ba1\u591a\u8f7d\u6ce2\u6ce2\u5f62\uff08\u5982AFDM\uff09\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u65e0\u7ebf\u73af\u5883\u3002", "result": "\u5541\u557e\u4fe1\u53f7\u5728\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e2d\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u652f\u6301\u9ad8\u79fb\u52a8\u6027\u901a\u4fe1\u548cISAC\u3002", "conclusion": "\u5541\u557e\u4fe1\u53f7\u8bbe\u8ba1\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.06247", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06247", "abs": "https://arxiv.org/abs/2508.06247", "authors": ["Zichun Ye", "Runqi Wang", "Xutong Liu", "Shuai Li"], "title": "Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits", "comment": null, "summary": "The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential\ndecision-making framework, dominated by two algorithmic families: UCB-based and\nadversarial methods such as follow the regularized leader (FTRL) and online\nmirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer\nfrom additional regret factor $\\log T$ that is detrimental over long horizons,\nwhile adversarial methods such as EXP3.M and HYBRID impose significant\ncomputational overhead. To resolve this trade-off, we introduce the\nCombinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS\nis a computationally efficient algorithm that achieves an instance-independent\nregret of $O\\big( (\\log k)^2\\sqrt{kmT}\\big )$ under semi-bandit feedback, where\n$m$ is the number of arms and $k$ is the maximum cardinality of a feasible\naction. Crucially, this result eliminates the dependency on $\\log T$ and\nmatches the established $\\Omega\\big( \\sqrt{kmT}\\big)$ lower bound up to\n$O\\big((\\log k)^2\\big)$. We then extend our analysis to show that CMOSS is also\napplicable to cascading feedback. Experiments on synthetic and real-world\ndatasets validate that CMOSS consistently outperforms benchmark algorithms in\nboth regret and runtime efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCMOSS\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2dUCB\u548c\u5bf9\u6297\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0e\u7406\u8bba\u4e0b\u754c\u5339\u914d\u7684\u9057\u61be\u3002", "motivation": "\u73b0\u6709UCB\u65b9\u6cd5\uff08\u5982CUCB\uff09\u5b58\u5728\u957f\u671f\u9057\u61be\u95ee\u9898\uff0c\u800c\u5bf9\u6297\u65b9\u6cd5\uff08\u5982EXP3.M\u548cHYBRID\uff09\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9700\u8981\u4e00\u79cd\u517c\u987e\u6027\u80fd\u548c\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86CMOSS\u7b97\u6cd5\uff0c\u901a\u8fc7\u534a\u5f3a\u76d7\u53cd\u9988\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\uff0c\u5e76\u6269\u5c55\u5230\u7ea7\u8054\u53cd\u9988\u573a\u666f\u3002", "result": "CMOSS\u5b9e\u73b0\u4e86\u5b9e\u4f8b\u65e0\u5173\u7684\u9057\u61be$O\\big( (\\log k)^2\\sqrt{kmT}\\big )$\uff0c\u6d88\u9664\u4e86\u5bf9$\\log T$\u7684\u4f9d\u8d56\uff0c\u4e14\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u9057\u61be\u548c\u8fd0\u884c\u65f6\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\u3002", "conclusion": "CMOSS\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u6700\u4f18\u7684\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.05831", "categories": ["cs.LG", "cs.NA", "math.NA", "15A29 Inverse problems in linear algebra 65F22, 68T07, 65F05, 62C12", "G.1.3; F.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.05831", "abs": "https://arxiv.org/abs/2508.05831", "authors": ["Alexander DeLise", "Kyle Loh", "Krish Patel", "Meredith Teague", "Andrea Arnold", "Matthias Chung"], "title": "Optimal Linear Baseline Models for Scientific Machine Learning", "comment": "40 pages, 10 Figures, 9 Tables", "summary": "Across scientific domains, a fundamental challenge is to characterize and\ncompute the mappings from underlying physical processes to observed signals and\nmeasurements. While nonlinear neural networks have achieved considerable\nsuccess, they remain theoretically opaque, which hinders adoption in contexts\nwhere interpretability is paramount. In contrast, linear neural networks serve\nas a simple yet effective foundation for gaining insight into these complex\nrelationships. In this work, we develop a unified theoretical framework for\nanalyzing linear encoder-decoder architectures through the lens of Bayes risk\nminimization for solving data-driven scientific machine learning problems. We\nderive closed-form, rank-constrained linear and affine linear optimal mappings\nfor forward modeling and inverse recovery tasks. Our results generalize\nexisting formulations by accommodating rank-deficiencies in data, forward\noperators, and measurement processes. We validate our theoretical results by\nconducting numerical experiments on datasets from simple biomedical imaging,\nfinancial factor analysis, and simulations involving nonlinear fluid dynamics\nvia the shallow water equations. This work provides a robust baseline for\nunderstanding and benchmarking learned neural network models for scientific\nmachine learning problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u98ce\u9669\u6700\u5c0f\u5316\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u7ebf\u6027\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u975e\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u5728\u79d1\u5b66\u9886\u57df\u4e2d\u7684\u7406\u8bba\u4e0d\u900f\u660e\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u7ebf\u6027\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8d1d\u53f6\u65af\u98ce\u9669\u6700\u5c0f\u5316\uff0c\u63a8\u5bfc\u51fa\u95ed\u5f0f\u3001\u79e9\u7ea6\u675f\u7684\u7ebf\u6027\u548c\u4eff\u5c04\u7ebf\u6027\u6700\u4f18\u6620\u5c04\uff0c\u7528\u4e8e\u6b63\u5411\u5efa\u6a21\u548c\u9006\u5411\u6062\u590d\u4efb\u52a1\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u5728\u751f\u7269\u533b\u5b66\u6210\u50cf\u3001\u91d1\u878d\u56e0\u5b50\u5206\u6790\u548c\u975e\u7ebf\u6027\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u7406\u89e3\u548c\u57fa\u51c6\u5316\u7684\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2508.06037", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06037", "abs": "https://arxiv.org/abs/2508.06037", "authors": ["Tien Ngoc Ha", "Daniel Romero"], "title": "Bayesian Radio Map Estimation: Fundamentals and Implementation via Diffusion Models", "comment": null, "summary": "Radio map estimation (RME) is the problem of inferring the value of a certain\nmetric (e.g. signal power) across an area of interest given a collection of\nmeasurements. While most works tackle this problem from a purely non-Bayesian\nperspective, some Bayesian estimators have been proposed. However, the latter\nfocus on estimating the map itself, the Bayesian standpoint is adopted mainly\nto exploit prior information or to capture uncertainty. This paper pursues a\nmore general formulation, where the goal is to determine the posterior\ndistribution of the map given the measurements. Besides handling uncertainty\nand allowing standard Bayesian estimates, solving this problem is seen to\nenable minimum mean square error estimation of arbitrary map functionals (e.g.\ncapacity, bit error rate, or coverage area to name a few) while training only\nfor power estimation. A general Bayesian estimator is proposed based on\nconditional diffusion models and both the Bayesian and non-Bayesian paradigms\nare compared analytically and numerically to determine when the Bayesian\napproach is preferable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u8d1d\u53f6\u65af\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u65e0\u7ebf\u7535\u5730\u56fe\u4f30\u8ba1\uff08RME\uff09\uff0c\u65e8\u5728\u786e\u5b9a\u7ed9\u5b9a\u6d4b\u91cf\u503c\u540e\u5730\u56fe\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u4ece\u800c\u652f\u6301\u4efb\u610f\u5730\u56fe\u529f\u80fd\u7684\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u8d1d\u53f6\u65af\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5730\u56fe\u672c\u8eab\u7684\u4f30\u8ba1\uff0c\u800c\u672c\u6587\u65e8\u5728\u66f4\u4e00\u822c\u5316\u5730\u786e\u5b9a\u5730\u56fe\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u5e76\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u529f\u80fd\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u8d1d\u53f6\u65af\u4f30\u8ba1\u5668\uff0c\u5e76\u4e0e\u975e\u8d1d\u53f6\u65af\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u6790\u548c\u6570\u503c\u6bd4\u8f83\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u4e13\u95e8\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4efb\u610f\u5730\u56fe\u529f\u80fd\uff08\u5982\u5bb9\u91cf\u3001\u8bef\u7801\u7387\u7b49\uff09\u7684\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1\u3002", "conclusion": "\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u4f18\u4e8e\u975e\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u652f\u6301\u591a\u529f\u80fd\u4f30\u8ba1\u65f6\u3002"}}
{"id": "2508.05836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05836", "abs": "https://arxiv.org/abs/2508.05836", "authors": ["Rituparna Datta", "Nibir Chandra Mandal"], "title": "An Effective Approach for Node Classification in Textual Graphs", "comment": null, "summary": "Textual Attribute Graphs (TAGs) are critical for modeling complex networks\nlike citation networks, but effective node classification remains challenging\ndue to difficulties in integrating rich semantics from text with structural\ngraph information. Existing methods often struggle with capturing nuanced\ndomain-specific terminology, modeling long-range dependencies, adapting to\ntemporal evolution, and scaling to massive datasets. To address these issues,\nwe propose a novel framework that integrates TAPE (Text-Attributed Graph\nRepresentation Enhancement) with Graphormer. Our approach leverages a large\nlanguage model (LLM), specifically ChatGPT, within the TAPE framework to\ngenerate semantically rich explanations from paper content, which are then\nfused into enhanced node representations. These embeddings are combined with\nstructural features using a novel integration layer with learned attention\nweights. Graphormer's path-aware position encoding and multi-head attention\nmechanisms are employed to effectively capture long-range dependencies across\nthe citation network. We demonstrate the efficacy of our framework on the\nchallenging ogbn-arxiv dataset, achieving state-of-the-art performance with a\nclassification accuracy of 0.772, significantly surpassing the best GCN\nbaseline of 0.713. Our method also yields strong results in precision (0.671),\nrecall (0.577), and F1-score (0.610). We validate our approach through\ncomprehensive ablation studies that quantify the contribution of each\ncomponent, demonstrating the synergy between semantic and structural\ninformation. Our framework provides a scalable and robust solution for node\nclassification in dynamic TAGs, offering a promising direction for future\nresearch in knowledge systems and scientific discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408TAPE\u4e0eGraphormer\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528ChatGPT\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u7ed3\u6784\u4e0e\u8bed\u4e49\u4fe1\u606f\uff0c\u5728ogbn-arxiv\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u5c5e\u6027\u56fe\uff08TAGs\uff09\u8282\u70b9\u5206\u7c7b\u4e2d\u96be\u4ee5\u6574\u5408\u6587\u672c\u8bed\u4e49\u4e0e\u56fe\u7ed3\u6784\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9886\u57df\u672f\u8bed\u3001\u957f\u7a0b\u4f9d\u8d56\u3001\u65f6\u95f4\u6f14\u5316\u548c\u5927\u89c4\u6a21\u6570\u636e\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408TAPE\u6846\u67b6\u4e0eGraphormer\uff0c\u5229\u7528ChatGPT\u751f\u6210\u8bed\u4e49\u89e3\u91ca\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u8bed\u4e49\u4e0e\u7ed3\u6784\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u8def\u5f84\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u548c\u591a\u5934\u6ce8\u610f\u529b\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\u3002", "result": "\u5728ogbn-arxiv\u6570\u636e\u96c6\u4e0a\u5b9e\u73b00.772\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8eGCN\u57fa\u7ebf\uff080.713\uff09\uff0c\u5e76\u5728\u7cbe\u786e\u7387\uff080.671\uff09\u3001\u53ec\u56de\u7387\uff080.577\uff09\u548cF1\u5206\u6570\uff080.610\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u52a8\u6001TAGs\u7684\u8282\u70b9\u5206\u7c7b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u77e5\u8bc6\u7cfb\u7edf\u548c\u79d1\u5b66\u53d1\u73b0\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.06054", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06054", "abs": "https://arxiv.org/abs/2508.06054", "authors": ["Yiheng Wang", "Shutao Zhang", "Ye Xue", "Tsung-Hui Chang"], "title": "Multi-Modal Neural Radio Radiance Field for Localized Statistical Channel Modelling", "comment": null, "summary": "This paper presents MM-LSCM, a self-supervised multi-modal neural radio\nradiance field framework for localized statistical channel modeling (LSCM) for\nnext-generation network optimization. Traditional LSCM methods rely solely on\nRSRP data, limiting their ability to model environmental structures that affect\nsignal propagation. To address this, we propose a dual-branch neural\narchitecture that integrates RSRP data and LiDAR point cloud information,\nenhancing spatial awareness and predictive accuracy. MM-LSCM leverages\nvolume-rendering-based multi-modal synthesis to align radio propagation with\nenvironmental obstacles and employs a self-supervised training approach,\neliminating the need for costly labeled data. Experimental results demonstrate\nthat MM-LSCM significantly outperforms conventional methods in channel\nreconstruction accuracy and robustness to noise, making it a promising solution\nfor real-world wireless network optimization.", "AI": {"tldr": "MM-LSCM\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u591a\u6a21\u6001\u795e\u7ecf\u65e0\u7ebf\u7535\u8f90\u5c04\u573a\u6846\u67b6\uff0c\u7528\u4e8e\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4f18\u5316\u7684\u5c40\u90e8\u7edf\u8ba1\u4fe1\u9053\u5efa\u6a21\uff0c\u7ed3\u5408RSRP\u6570\u636e\u548cLiDAR\u70b9\u4e91\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u9053\u91cd\u5efa\u7cbe\u5ea6\u548c\u6297\u566a\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfLSCM\u65b9\u6cd5\u4ec5\u4f9d\u8d56RSRP\u6570\u636e\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u5f71\u54cd\u4fe1\u53f7\u4f20\u64ad\u7684\u73af\u5883\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cc\u5206\u652f\u795e\u7ecf\u67b6\u6784\uff0c\u6574\u5408RSRP\u6570\u636e\u548cLiDAR\u70b9\u4e91\u4fe1\u606f\uff0c\u91c7\u7528\u57fa\u4e8e\u4f53\u79ef\u6e32\u67d3\u7684\u591a\u6a21\u6001\u5408\u6210\u548c\u81ea\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMM-LSCM\u5728\u4fe1\u9053\u91cd\u5efa\u7cbe\u5ea6\u548c\u6297\u566a\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "MM-LSCM\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65e0\u7ebf\u7f51\u7edc\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u6602\u8d35\u6807\u6ce8\u6570\u636e\u3002"}}
{"id": "2508.05876", "categories": ["cs.LG", "astro-ph.EP", "astro-ph.IM", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.05876", "abs": "https://arxiv.org/abs/2508.05876", "authors": ["Francesca Ferrara", "Lander W. Schillinger Arana", "Florian D\u00f6rfler", "Sarah H. Q. Li"], "title": "A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance", "comment": "16 pages, 13 figures, submitted to the 2025 Astrodynamics Specialist\n  Conference", "summary": "This work presents a Markov decision process (MDP) framework to model\ndecision-making for collision avoidance maneuver (CAM) and a reinforcement\nlearning policy gradient (RL-PG) algorithm to train an autonomous guidance\npolicy using historic CAM data. In addition to maintaining acceptable collision\nrisks, this approach seeks to minimize the average fuel consumption of CAMs by\nmaking early maneuver decisions. We model CAM as a continuous state, discrete\naction and finite horizon MDP, where the critical decision is determining when\nto initiate the maneuver. The MDP model also incorporates analytical models for\nconjunction risk, propellant consumption, and transit orbit geometry. The\nMarkov policy effectively trades-off maneuver delay-which improves the\nreliability of conjunction risk indicators-with propellant consumption-which\nincreases with decreasing maneuver time. Using historical data of tracked\nconjunction events, we verify this framework and conduct an extensive ablation\nstudy on the hyper-parameters used within the MDP. On synthetic conjunction\nevents, the trained policy significantly minimizes both the overall and average\npropellant consumption per CAM when compared to a conventional cut-off policy\nthat initiates maneuvers 24 hours before the time of closest approach (TCA). On\nhistorical conjunction events, the trained policy consumes more propellant\noverall but reduces the average propellant consumption per CAM. For both\nhistorical and synthetic conjunction events, the trained policy achieves equal\nif not higher overall collision risk guarantees.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMDP\u548cRL-PG\u7684\u78b0\u649e\u907f\u514d\u673a\u52a8\u51b3\u7b56\u6846\u67b6\uff0c\u65e8\u5728\u5e73\u8861\u78b0\u649e\u98ce\u9669\u548c\u71c3\u6599\u6d88\u8017\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u4fdd\u8bc1\u78b0\u649e\u98ce\u9669\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u65e9\u671f\u673a\u52a8\u51b3\u7b56\u51cf\u5c11\u71c3\u6599\u6d88\u8017\u3002", "method": "\u5c06CAM\u5efa\u6a21\u4e3a\u8fde\u7eed\u72b6\u6001\u3001\u79bb\u6563\u52a8\u4f5c\u7684\u6709\u9650\u65f6\u57dfMDP\uff0c\u7ed3\u5408\u98ce\u9669\u3001\u71c3\u6599\u6d88\u8017\u548c\u8f68\u9053\u51e0\u4f55\u7684\u89e3\u6790\u6a21\u578b\uff0c\u4f7f\u7528RL-PG\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u5386\u53f2\u6570\u636e\u4e0a\uff0c\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u51cf\u5c11\u4e86\u71c3\u6599\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u78b0\u649e\u98ce\u9669\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u71c3\u6599\u6d88\u8017\u548c\u4fdd\u8bc1\u78b0\u649e\u98ce\u9669\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7b56\u7565\u3002"}}
{"id": "2508.06141", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06141", "abs": "https://arxiv.org/abs/2508.06141", "authors": ["Marco Bertuletti", "Yichao Zhang", "Mahdi Abdollahpour", "Samuel Riedel", "Alessandro Vanelli-Coralli"], "title": "Fast End-to-End Simulation and Exploration of Many-RISCV-Core Baseband Transceivers for Software-Defined Radio-Access Networks", "comment": "7 pages", "summary": "The fast-rising demand for wireless bandwidth requires rapid evolution of\nhigh-performance baseband processing infrastructure. Programmable many-core\nprocessors for software-defined radio (SDR) have emerged as high-performance\nbaseband processing engines, offering the flexibility required to capture\nevolving wireless standards and technologies. This trend must be supported by a\ndesign framework enabling functional validation and end-to-end performance\nanalysis of SDR hardware within realistic radio environment models. We propose\na static binary translation based simulator augmented with a fast, approximate\ntiming model of the hardware and coupled to wireless channel models to simulate\nthe most performance-critical physical layer functions implemented in software\non a many (1024) RISC-V cores cluster customized for SDR. Our framework\nsimulates the detection of a 5G OFDM-symbol on a server-class processor in\n9.5s-3min, on a single thread, depending on the input MIMO size (three orders\nof magnitude faster than RTL simulation). The simulation is easily parallelized\nto 128 threads with 73-121x speedup compared to a single thread.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9759\u6001\u4e8c\u8fdb\u5236\u7ffb\u8bd1\u7684\u6a21\u62df\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6027\u80fdSDR\u57fa\u5e26\u5904\u7406\u7684\u529f\u80fd\u9a8c\u8bc1\u548c\u6027\u80fd\u5206\u6790\u3002", "motivation": "\u6ee1\u8db3\u65e0\u7ebf\u5e26\u5bbd\u5feb\u901f\u589e\u957f\u7684\u6027\u80fd\u9700\u6c42\uff0c\u652f\u6301SDR\u786c\u4ef6\u5728\u771f\u5b9e\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u529f\u80fd\u9a8c\u8bc1\u548c\u6027\u80fd\u5206\u6790\u3002", "method": "\u7ed3\u5408\u9759\u6001\u4e8c\u8fdb\u5236\u7ffb\u8bd1\u548c\u5feb\u901f\u8fd1\u4f3c\u65f6\u5e8f\u6a21\u578b\uff0c\u4e0e\u65e0\u7ebf\u4fe1\u9053\u6a21\u578b\u8026\u5408\uff0c\u6a21\u62df1024\u4e2aRISC-V\u6838\u5fc3\u96c6\u7fa4\u4e0a\u7684\u5173\u952e\u7269\u7406\u5c42\u529f\u80fd\u3002", "result": "\u5728\u5355\u7ebf\u7a0b\u4e0b\u6a21\u62df5G OFDM\u7b26\u53f7\u68c0\u6d4b\u65f6\u95f4\u4e3a9.5\u79d2\u81f33\u5206\u949f\uff0c\u6bd4RTL\u6a21\u62df\u5feb\u4e09\u4e2a\u6570\u91cf\u7ea7\uff1b\u5e76\u884c\u5316\u540e\u901f\u5ea6\u63d0\u534773-121\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aSDR\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u529f\u80fd\u9a8c\u8bc1\u548c\u6027\u80fd\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2508.05905", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05905", "abs": "https://arxiv.org/abs/2508.05905", "authors": ["Jeffrey Uhlmann"], "title": "The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)", "comment": null, "summary": "Quantization is usually regarded as a means to trade quality of performance\nfor reduced compute requirements, i.e., as a suboptimal approximation. However,\nif examined in terms of a fixed overall resource budget, a very different\nperspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit\nquantization that deterministically provides gradient information with no\nforward-path penalty. Our analysis provides evidence that it may improve\ninformation density compared to non-quantized alternatives.", "AI": {"tldr": "SZT\u662f\u4e00\u79cd2\u4f4d\u91cf\u5316\u65b9\u6cd5\uff0c\u5728\u56fa\u5b9a\u8d44\u6e90\u9884\u7b97\u4e0b\u53ef\u80fd\u4f18\u4e8e\u975e\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u91cf\u5316\u5728\u56fa\u5b9a\u8d44\u6e90\u9884\u7b97\u4e0b\u662f\u5426\u53ef\u80fd\u4f18\u4e8e\u975e\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u5f15\u5165Signed-Zero Ternary (SZT)\uff0c\u4e00\u79cd2\u4f4d\u91cf\u5316\u65b9\u6cd5\uff0c\u786e\u5b9a\u6027\u5730\u63d0\u4f9b\u68af\u5ea6\u4fe1\u606f\u4e14\u4e0d\u5f71\u54cd\u524d\u5411\u8def\u5f84\u3002", "result": "\u5206\u6790\u8868\u660eSZT\u53ef\u80fd\u63d0\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u3002", "conclusion": "SZT\u5728\u56fa\u5b9a\u8d44\u6e90\u9884\u7b97\u4e0b\u53ef\u80fd\u6bd4\u975e\u91cf\u5316\u65b9\u6cd5\u66f4\u4f18\u3002"}}
{"id": "2508.06176", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06176", "abs": "https://arxiv.org/abs/2508.06176", "authors": ["Marco Bertuletti", "Yichao Zhang", "Alessandro Vanelli-Coralli", "Luca Benini"], "title": "A 66-Gb/s/5.5-W RISC-V Many-Core Cluster for 5G+ Software-Defined Radio Uplinks", "comment": null, "summary": "Following the scale-up of new radio (NR) complexity in 5G and beyond, the\nphysical layer's computing load on base stations is increasing under a strictly\nconstrained latency and power budget; base stations must process > 20-Gb/s\nuplink wireless data rate on the fly, in < 10 W. At the same time, the\nprogrammability and reconfigurability of base station components are the key\nrequirements; it reduces the time and cost of new networks' deployment, it\nlowers the acceptance threshold for industry players to enter the market, and\nit ensures return on investments in a fast-paced evolution of standards. In\nthis article, we present the design of a many-core cluster for 5G and beyond\nbase station processing. Our design features 1024, streamlined RISC-V cores\nwith domain-specific FP extensions, and 4-MiB shared memory. It provides the\nnecessary computational capabilities for software-defined processing of the\nlower physical layer of 5G physical uplink shared channel (PUSCH), satisfying\nhigh-end throughput requirements (66 Gb/s for a transition time interval (TTI),\n9.4-302 Gb/s depending on the processing stage). The throughput metrics for the\nimplemented functions are ten times higher than in state-of-the-art (SoTA)\napplication-specific instruction processors (ASIPs). The energy efficiency on\nkey NR kernels (2-41 Gb/s/W), measured at 800 MHz, 25 {\\deg}C, and 0.8 V, on a\nplaced and routed instance in 12-nm CMOS technology, is competitive with SoTA\narchitectures. The PUSCH processing runs end-to-end on a single cluster in 1.7\nms, at <6-W average power consumption, achieving 12 Gb/s/W.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6838\u96c6\u7fa4\u7528\u4e8e5G\u53ca\u4ee5\u540e\u57fa\u7ad9\u5904\u7406\uff0c\u5177\u67091024\u4e2aRISC-V\u6838\u5fc3\u548c4-MiB\u5171\u4eab\u5185\u5b58\uff0c\u6ee1\u8db3\u9ad8\u541e\u5410\u91cf\u548c\u80fd\u6548\u8981\u6c42\u3002", "motivation": "5G\u53ca\u4ee5\u540e\u57fa\u7ad9\u7684\u8ba1\u7b97\u8d1f\u8f7d\u589e\u52a0\uff0c\u9700\u5728\u4e25\u683c\u5ef6\u8fdf\u548c\u529f\u8017\u9650\u5236\u4e0b\u5904\u7406\u9ad8\u6570\u636e\u901f\u7387\uff0c\u540c\u65f6\u8981\u6c42\u53ef\u7f16\u7a0b\u6027\u548c\u53ef\u91cd\u6784\u6027\u3002", "method": "\u91c7\u75281024\u4e2aRISC-V\u6838\u5fc3\u548c4-MiB\u5171\u4eab\u5185\u5b58\u7684\u8bbe\u8ba1\uff0c\u652f\u6301\u8f6f\u4ef6\u5b9a\u4e49\u5904\u7406\uff0c\u6ee1\u8db35G\u7269\u7406\u4e0a\u884c\u5171\u4eab\u4fe1\u9053\u7684\u9ad8\u541e\u5410\u9700\u6c42\u3002", "result": "\u541e\u5410\u91cf\u6bd4\u73b0\u6709ASIPs\u9ad8\u5341\u500d\uff0c\u80fd\u6548\u8fbe2-41 Gb/s/W\uff0cPUSCH\u5904\u7406\u57281.7 ms\u5185\u5b8c\u6210\uff0c\u529f\u8017\u4f4e\u4e8e6 W\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u5728\u541e\u5410\u91cf\u548c\u80fd\u6548\u4e0a\u4f18\u4e8e\u73b0\u6709\u67b6\u6784\uff0c\u9002\u7528\u4e8e5G\u53ca\u4ee5\u540e\u57fa\u7ad9\u7684\u9ad8\u6027\u80fd\u9700\u6c42\u3002"}}
{"id": "2508.05915", "categories": ["cs.LG", "62M10"], "pdf": "https://arxiv.org/pdf/2508.05915", "abs": "https://arxiv.org/abs/2508.05915", "authors": ["Alex Glushkovsky"], "title": "Dual Signal Decomposition of Stochastic Time Series", "comment": "21 pages, 9 figures, 1 table", "summary": "The research paper addresses decomposition of a stochastic time series into\nthree time series representing a dual signal i.e., the mean and the dispersion,\nwith noise isolated. Decomposition is done by applying machine learning to fit\na dual signal. Machine learning minimizes the loss function which compromises\nbetween fitting the original time series and penalizing irregularities of the\ndual signal. The latter includes terms based on the first and second order\nderivatives along time. To preserve special patterns, weighting of the\nregularization components of the loss function has been introduced based on\nStatistical Process Control methodology. The proposed decomposition can be\napplied as a smoothing algorithm against the mean and dispersion of the time\nseries. By isolating noise, the proposed decomposition can be seen as a\ndenoising algorithm. Two approaches of the learning process have been\nconsidered: sequential and jointly. The former approach learns the mean signal\nfirst and then dispersion. The latter approach fits the dual signal jointly.\nJointly learning can uncover complex relationships for the time series with\nheteroskedasticity. Learning has been set by solving the direct non-linear\nunconstrained optimization problem or by applying neural networks that have\nsequential or twin output architectures. Tuning of the loss function\nhyperparameters focuses on the isolated noise to be a stationary stochastic\nprocess without autocorrelation properties. Depending on the applications, the\nhyperparameters of the learning can be tuned towards either the discrete states\nby stepped signal or smoothed series. The decomposed dual signal can be\nrepresented on the 2D space and used to learn inherent structures, to forecast\nboth mean and dispersion, or to analyze cross effects in case of multiple time\nseries.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u968f\u673a\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5747\u503c\u3001\u79bb\u6563\u5ea6\u548c\u566a\u58f0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u62df\u5408\u53cc\u4fe1\u53f7\u5e76\u4f18\u5316\u635f\u5931\u51fd\u6570\u3002", "motivation": "\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u95ee\u9898\uff0c\u63d0\u53d6\u5747\u503c\u3001\u79bb\u6563\u5ea6\u548c\u566a\u58f0\uff0c\u4ee5\u652f\u6301\u5e73\u6ed1\u3001\u53bb\u566a\u548c\u8fdb\u4e00\u6b65\u5206\u6790\u3002", "method": "\u5e94\u7528\u673a\u5668\u5b66\u4e60\u62df\u5408\u53cc\u4fe1\u53f7\uff0c\u4f18\u5316\u635f\u5931\u51fd\u6570\uff08\u5305\u542b\u6b63\u5219\u5316\u9879\uff09\uff0c\u652f\u6301\u987a\u5e8f\u6216\u8054\u5408\u5b66\u4e60\uff0c\u4f7f\u7528\u975e\u7ebf\u6027\u4f18\u5316\u6216\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5b9e\u73b0\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u5206\u89e3\uff0c\u53ef\u7528\u4e8e\u5e73\u6ed1\u3001\u53bb\u566a\u3001\u9884\u6d4b\u548c\u5206\u6790\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\uff0c\u652f\u6301\u591a\u79cd\u5e94\u7528\u573a\u666f\uff0c\u5982\u9884\u6d4b\u548c\u7ed3\u6784\u5206\u6790\u3002"}}
{"id": "2508.06275", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06275", "abs": "https://arxiv.org/abs/2508.06275", "authors": ["SaiKrishna Saketh Yellapragada", "Esa Ollila", "Mario Costa"], "title": "Efficient Deep Neural Receiver with Post-Training Quantization", "comment": null, "summary": "Deep learning has recently garnered significant interest in wireless\ncommunications due to its superior performance compared to traditional\nmodel-based algorithms. Deep convolutional neural networks (CNNs) have\ndemonstrated notable improvements in block error rate (BLER) under various\nchannel models and mobility scenarios. However, the high computational\ncomplexity and resource demands of deep CNNs pose challenges for deployment in\nresource-constrained edge systems. The 3rd Generation Partnership Project\n(3GPP) Release 20 highlights the pivotal role of artificial intelligence (AI)\nintegration in enabling advanced radio-access networks for 6G systems. The hard\nreal-time processing demands of 5G and 6G require efficient techniques such as\npost-training quantization (PTQ), quantization-aware training (QAT), pruning,\nand hybrid approaches to meet latency requirements. In this paper, we focus on\nPTQ to reduce model complexity by lowering the bit-width of weights, thereby\nenhancing computational efficiency. Our analysis employs symmetric uniform\nquantization, applying both per-tensor and per-channel PTQ to a neural receiver\nachieving performance comparable to full-precision models. Specifically, 8-bit\nper-channel quantization maintains BLER performance with minimal degradation,\nwhile 4-bit quantization shows great promise but requires further optimization\nto achieve target BLER levels. These results highlight the potential of\nultra-low bitwidth PTQ for efficient neural receiver deployment in 6G systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u901a\u8fc7\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u964d\u4f4e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4ee5\u9002\u914d\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u7cfb\u7edf\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\u548c\u8d44\u6e90\u9700\u6c42\u9650\u5236\u4e86\u5728\u8fb9\u7f18\u7cfb\u7edf\u7684\u90e8\u7f72\u30023GPP Release 20\u5f3a\u8c03\u4e86AI\u57286G\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u6280\u672f\u5982PTQ\u6765\u6ee1\u8db3\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5bf9\u79f0\u5747\u5300\u91cf\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u9010\u5f20\u91cf\u548c\u9010\u901a\u9053PTQ\uff0c\u5bf9\u795e\u7ecf\u63a5\u6536\u5668\u8fdb\u884c\u91cf\u5316\uff0c\u4ee5\u964d\u4f4e\u6743\u91cd\u4f4d\u5bbd\u3002", "result": "8\u4f4d\u9010\u901a\u9053\u91cf\u5316\u4fdd\u6301\u4e86\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u76f8\u5f53\u7684BLER\u6027\u80fd\uff0c\u800c4\u4f4d\u91cf\u5316\u867d\u8868\u73b0\u6f5c\u529b\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "conclusion": "\u8d85\u4f4e\u4f4d\u5bbdPTQ\u57286G\u7cfb\u7edf\u4e2d\u90e8\u7f72\u9ad8\u6548\u795e\u7ecf\u63a5\u6536\u5668\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.05921", "categories": ["cs.LG", "math.FA", "math.RT", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.05921", "abs": "https://arxiv.org/abs/2508.05921", "authors": ["Siddharth Rout"], "title": "Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations", "comment": null, "summary": "Accuracy in neural PDE solvers often breaks down not because of limited\nexpressivity, but due to poor optimisation caused by ill-conditioning,\nespecially in multi-fidelity and stiff problems. We study this issue in\nPhysics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural\nPDE solvers, and show that asymptotic components in governing equations can\nproduce highly ill-conditioned activation matrices, severely limiting\nconvergence. We introduce Shifted Gaussian Encoding, a simple yet effective\nactivation filtering step that increases matrix rank and expressivity while\npreserving convexity. Our method extends the solvable range of Peclet numbers\nin steady advection-diffusion equations by over two orders of magnitude,\nachieves up to six orders lower error on multi-frequency function learning, and\nfits high-fidelity image vectors more accurately and faster than deep networks\nwith over a million parameters. This work highlights that conditioning, not\ndepth, is often the bottleneck in scientific neural solvers and that simple\narchitectural changes can unlock substantial gains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aShifted Gaussian Encoding\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u5584\u795e\u7ecfPDE\u6c42\u89e3\u5668\u7684\u4f18\u5316\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6c42\u89e3\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u795e\u7ecfPDE\u6c42\u89e3\u5668\u5728\u591a\u4fdd\u771f\u5ea6\u548c\u521a\u6027\u95ee\u9898\u4e0a\u56e0\u4f18\u5316\u4e0d\u826f\uff08\u5c24\u5176\u662f\u77e9\u9635\u75c5\u6001\uff09\u5bfc\u81f4\u7684\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u5728Physics-Informed Extreme Learning Machines (PIELMs)\u4e2d\u5f15\u5165Shifted Gaussian Encoding\uff0c\u901a\u8fc7\u6fc0\u6d3b\u77e9\u9635\u8fc7\u6ee4\u63d0\u9ad8\u77e9\u9635\u79e9\u548c\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u65b9\u6cd5\u5c06\u7a33\u6001\u5bf9\u6d41-\u6269\u6563\u65b9\u7a0b\u7684Peclet\u6570\u6c42\u89e3\u8303\u56f4\u6269\u5c55\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u591a\u9891\u51fd\u6570\u5b66\u4e60\u7684\u8bef\u5dee\u964d\u4f4e\u4e86\u516d\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u6bd4\u767e\u4e07\u53c2\u6570\u6df1\u5ea6\u7f51\u7edc\u66f4\u5feb\u66f4\u51c6\u5730\u62df\u5408\u9ad8\u4fdd\u771f\u56fe\u50cf\u5411\u91cf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u795e\u7ecf\u79d1\u5b66\u6c42\u89e3\u5668\u7684\u74f6\u9888\u901a\u5e38\u662f\u6761\u4ef6\u6570\u800c\u975e\u6df1\u5ea6\uff0c\u7b80\u5355\u7684\u67b6\u6784\u6539\u8fdb\u53ef\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.06340", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06340", "abs": "https://arxiv.org/abs/2508.06340", "authors": ["Danish Mehmood Mughal", "Daniyal Munir", "Qazi Arbab Ahmed", "Hans D. Schotten", "Thorsten Jungeblut", "Sang-Hyo Kim", "Min Young Chung"], "title": "MALRIS: Malicious Hardware in RIS-Assisted Wireless Communications", "comment": "Accepted for presentation at IEEE CSCN 2025", "summary": "Reconfigurable intelligent surfaces (RIS) enhance wireless communication by\ndynamically shaping the propagation environment, but their integration\nintroduces hardware-level security risks. This paper presents the concept of\nMalicious RIS (MALRIS), where compromised components behave adversarially, even\nunder passive operation. The focus of this work is on practical threats such as\nmanufacturing time tampering, malicious firmware, and partial element control.\nTwo representative attacks, power-splitting and element-splitting, are modeled\nto assess their impact. Simulations in a RIS-assisted system reveal that even a\nlimited hardware compromise can significantly degrade performance metrics such\nas bit error rate, throughput, and secrecy metrics. By exposing this overlooked\nthreat surface, this work aims to promote awareness and support secure,\ntrustworthy RIS deployment in future wireless networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6076\u610f\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08MALRIS\uff09\u6982\u5ff5\uff0c\u63ed\u793a\u786c\u4ef6\u7ea7\u5b89\u5168\u98ce\u9669\uff0c\u6a21\u62df\u4e24\u79cd\u653b\u51fb\u65b9\u5f0f\uff0c\u5c55\u793a\u5176\u5bf9\u6027\u80fd\u6307\u6807\u7684\u663e\u8457\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u52a8\u6001\u4f18\u5316\u4f20\u64ad\u73af\u5883\uff0c\u4f46\u5176\u786c\u4ef6\u53ef\u80fd\u88ab\u6076\u610f\u7be1\u6539\uff0c\u5bfc\u81f4\u5b89\u5168\u98ce\u9669\uff0c\u9700\u7814\u7a76\u5176\u6f5c\u5728\u5a01\u80c1\u3002", "method": "\u63d0\u51faMALRIS\u6982\u5ff5\uff0c\u6a21\u62df\u5236\u9020\u65f6\u7be1\u6539\u3001\u6076\u610f\u56fa\u4ef6\u548c\u90e8\u5206\u5143\u4ef6\u63a7\u5236\u7b49\u5a01\u80c1\uff0c\u5efa\u6a21\u529f\u7387\u5206\u5272\u548c\u5143\u4ef6\u5206\u5272\u4e24\u79cd\u653b\u51fb\u65b9\u5f0f\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0c\u5373\u4f7f\u6709\u9650\u7684\u786c\u4ef6\u88ab\u653b\u9677\uff0c\u4e5f\u4f1a\u663e\u8457\u964d\u4f4e\u8bef\u7801\u7387\u3001\u541e\u5410\u91cf\u548c\u4fdd\u5bc6\u6027\u7b49\u6027\u80fd\u6307\u6807\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86RIS\u786c\u4ef6\u5b89\u5168\u5a01\u80c1\uff0c\u65e8\u5728\u63d0\u5347\u610f\u8bc6\uff0c\u652f\u6301\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b89\u5168\u53ef\u4fe1\u7684RIS\u90e8\u7f72\u3002"}}
{"id": "2508.05928", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05928", "abs": "https://arxiv.org/abs/2508.05928", "authors": ["Si Shen", "Peijun Shen", "Wenhua Zhao", "Danhao Zhu"], "title": "Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting", "comment": null, "summary": "Group-Relative Policy Optimization (GRPO) is a key technique for training\nlarge reasoning models, yet it suffers from a critical vulnerability: the\n\\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning\nprocess. This problem is most severe in unbalanced response groups,\nparadoxically degrading the signal precisely when it should be most\ninformative. To address this challenge, we propose Stable Group-Relative Policy\nOptimization (S-GRPO), a principled enhancement that derives optimal,\nnoise-aware advantage weights to stabilize training. Our comprehensive\nexperiments on mathematical reasoning benchmarks demonstrate S-GRPO's\neffectiveness and robustness. On various models, S-GRPO significantly\noutperforms DR. GRPO, achieving performance gains of +2.5% on\nQwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on\nQwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn\nunder 20% synthetic reward noise, S-GRPO maintains stable learning progress.\nThese results highlight S-GRPO's potential for more robust and effective\ntraining of large-scale reasoning models. \\footnote{Code and data are available\nat: https://github.com/shenpeijun0212/S-GRPO", "AI": {"tldr": "S-GRPO\u662f\u4e00\u79cd\u6539\u8fdb\u7684GRPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u7684\u4f18\u52bf\u6743\u91cd\u7a33\u5b9a\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86Think-Answer Mismatch\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "GRPO\u5728\u8bad\u7ec3\u5927\u578b\u63a8\u7406\u6a21\u578b\u65f6\u5b58\u5728Think-Answer Mismatch\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u5e73\u8861\u54cd\u5e94\u7ec4\u4e2d\uff0c\u566a\u58f0\u4fe1\u53f7\u4f1a\u5e72\u6270\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faS-GRPO\uff0c\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u7684\u4f18\u52bf\u6743\u91cd\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cS-GRPO\u663e\u8457\u4f18\u4e8eGRPO\uff0c\u6027\u80fd\u63d0\u5347\u8fbe2.2%-2.5%\uff0c\u5e76\u572820%\u566a\u58f0\u73af\u5883\u4e0b\u4fdd\u6301\u7a33\u5b9a\u5b66\u4e60\u3002", "conclusion": "S-GRPO\u4e3a\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u548c\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.06428", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06428", "abs": "https://arxiv.org/abs/2508.06428", "authors": ["Zhiwen Zhou"], "title": "Full-Dimensional Beamforming for Multi-User MIMO-OFDM ISAC for Low-Altitude UAV with Zero Sensing Resource Allocation", "comment": null, "summary": "Low-altitude unmanned aerial vehicles (UAVs) are expected to play an\nimportant role for low-altitude economy with a wide range of applications like\nprecise agriculture, aerial delivery and surveillance. Integrated sensing and\ncommunication (ISAC) is a key technology to enable the large-scale deployment\nand routine usage of UAVs by providing both communication and sensing services\nefficiently. For UAV ISAC systems, as UAV often acts as both a communication\nuser equipment (UE) and a sensing target, traditional ISAC systems that usually\nallocate dedicated TF resources for sensing are inefficient due to the severe\ndegradation of communication spectral efficiency. To address this issue, in\nthis paper, we propose a novel multiple-input multiple-output (MIMO) orthogonal\nfrequency division multiplexing (OFDM)-based ISAC framework for UAVs that\neliminates the need for dedicated sensing TF resources, achieving zero TF\nsensing overhead. By designing the transmit beamforming to meet the\nrequirements for both communication and sensing tasks, our proposed approach\nenables the communication TF resources to be fully reused for sensing, thereby\nenhancing both the communication sum rate and the sensing performance in terms\nof resolution, unambiguous range, and accuracy. Additionally, we introduce a\nlow-complexity target searching beamforming algorithm and a two-stage\nsuper-resolution sensing algorithm, which ensure efficient implementation.\nSimulation results demonstrate that the proposed MIMO-OFDM-ISAC framework not\nonly improves the communication sum rate but also outperforms traditional ISAC\nsystems in sensing performance, making it a promising solution for future ISAC\nsystems to support low-altitude UAVs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMIMO-OFDM\u7684\u65e0\u4eba\u673aISAC\u6846\u67b6\uff0c\u65e0\u9700\u4e13\u7528\u611f\u77e5\u8d44\u6e90\uff0c\u63d0\u5347\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfISAC\u7cfb\u7edf\u56e0\u4e13\u7528\u611f\u77e5\u8d44\u6e90\u5bfc\u81f4\u901a\u4fe1\u9891\u8c31\u6548\u7387\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u4ee5\u6ee1\u8db3\u901a\u4fe1\u548c\u611f\u77e5\u9700\u6c42\uff0c\u5f15\u5165\u4f4e\u590d\u6742\u5ea6\u76ee\u6807\u641c\u7d22\u6ce2\u675f\u6210\u5f62\u7b97\u6cd5\u548c\u4e24\u9636\u6bb5\u8d85\u5206\u8fa8\u7387\u611f\u77e5\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u901a\u4fe1\u603b\u901f\u7387\u548c\u611f\u77e5\u6027\u80fd\uff08\u5206\u8fa8\u7387\u3001\u8303\u56f4\u548c\u7cbe\u5ea6\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u672a\u6765\u652f\u6301\u4f4e\u7a7a\u65e0\u4eba\u673a\u7684ISAC\u7cfb\u7edf\u7684\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05957", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05957", "abs": "https://arxiv.org/abs/2508.05957", "authors": ["Hasibul Karim Shanto", "Umme Ayman Koana", "Shadikur Rahman"], "title": "Multi-Armed Bandits-Based Optimization of Decision Trees", "comment": null, "summary": "Decision trees, without appropriate constraints, can easily become overly\ncomplex and prone to overfit, capturing noise rather than generalizable\npatterns. To resolve this problem,pruning operation is a crucial part in\noptimizing decision trees, as it not only reduces the complexity of trees but\nalso decreases the probability of generating overfit models. The conventional\npruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning\n(REP) are mostly based on greedy approaches that focus on immediate gains in\nperformance while pruning nodes of the decision tree. However, this might\nresult in a lower generalization in the long run, compromising the robust\nability of the tree model when introduced to unseen data samples, particularly\nwhen trained with small and complex datasets. To address this challenge, we are\nproposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement\nlearning (RL)-based technique, that will dynamically prune the tree to generate\nan optimal decision tree with better generalization. Our proposed approach\nassumes the pruning process as an exploration-exploitation problem, where we\nare utilizing the MAB algorithms to find optimal branch nodes to prune based on\nfeedback from each pruning actions. Experimental evaluation on several\nbenchmark datasets, demonstrated that our proposed approach results in better\npredictive performance compared to the traditional ones. This suggests the\npotential of utilizing MAB for a dynamic and probabilistic way of decision tree\npruning, in turn optimizing the decision tree-based model.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u7684\u51b3\u7b56\u6811\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u526a\u679d\u8fc7\u7a0b\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\uff08\u5982CCP\u548cREP\uff09\u57fa\u4e8e\u8d2a\u5fc3\u7b56\u7565\uff0c\u53ef\u80fd\u727a\u7272\u957f\u671f\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5904\u7406\u5c0f\u89c4\u6a21\u548c\u590d\u6742\u6570\u636e\u96c6\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5c06\u526a\u679d\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u63a2\u7d22-\u5229\u7528\u95ee\u9898\uff0c\u5229\u7528MAB\u7b97\u6cd5\u52a8\u6001\u9009\u62e9\u6700\u4f18\u5206\u652f\u8282\u70b9\u8fdb\u884c\u526a\u679d\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "MAB\u65b9\u6cd5\u4e3a\u51b3\u7b56\u6811\u526a\u679d\u63d0\u4f9b\u4e86\u4e00\u79cd\u52a8\u6001\u4e14\u6982\u7387\u5316\u7684\u4f18\u5316\u65b9\u5f0f\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.05960", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05960", "abs": "https://arxiv.org/abs/2508.05960", "authors": ["Haohui Chen", "Zhiyong Chen"], "title": "Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) seeks to learn optimal policies from\nstatic datasets without further environment interaction. A key challenge is the\ndistribution shift between the learned and behavior policies, leading to\nout-of-distribution (OOD) actions and overestimation. To prevent gross\noverestimation, the value function must remain conservative; however, excessive\nconservatism may hinder performance improvement. To address this, we propose\nthe mildly conservative regularized evaluation (MCRE) framework, which balances\nconservatism and performance by combining temporal difference (TD) error with a\nbehavior cloning term in the Bellman backup. Building on this, we develop the\nmildly conservative regularized Q-learning (MCRQ) algorithm, which integrates\nMCRE into an off-policy actor-critic framework. Experiments show that MCRQ\noutperforms strong baselines and state-of-the-art offline RL algorithms on\nbenchmark datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCRE\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408TD\u8bef\u5dee\u548c\u884c\u4e3a\u514b\u9686\u9879\u6765\u5e73\u8861\u4fdd\u5b88\u6027\u548c\u6027\u80fd\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86MCRQ\u7b97\u6cd5\uff0c\u5728\u79bb\u7ebfRL\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4OOD\u52a8\u4f5c\u548c\u8fc7\u9ad8\u4f30\u8ba1\uff0c\u9700\u8981\u5e73\u8861\u4fdd\u5b88\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faMCRE\u6846\u67b6\uff0c\u7ed3\u5408TD\u8bef\u5dee\u548c\u884c\u4e3a\u514b\u9686\u9879\uff1b\u5f00\u53d1MCRQ\u7b97\u6cd5\uff0c\u5c06\u5176\u878d\u5165\u79bb\u7ebf\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6846\u67b6\u3002", "result": "MCRQ\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u3002", "conclusion": "MCRE\u548cMCRQ\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u4e2d\u7684\u4fdd\u5b88\u6027\u4e0e\u6027\u80fd\u5e73\u8861\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.05977", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2508.05977", "abs": "https://arxiv.org/abs/2508.05977", "authors": ["Aoming Liang", "Chi Cheng", "Dashuai Chen", "Boai Sun", "Dixia Fan"], "title": "LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning", "comment": null, "summary": "In the domain of scientific machine learning, designing effective reward\nfunctions remains a challenge in reinforcement learning (RL), particularly in\nenvironments where task goals are difficult to specify numerically. Reward\nfunctions in existing work are predominantly based on heuristics, manual\nengineering, or task-specific tuning. In this work, we introduce a semantically\naligned reinforcement learning method where rewards are computed by aligning\nthe current state with a target semantic instruction using a\nSentence-Bidirectional Encoder Representations from Transformers (SBERT).\nInstead of relying on manually defined reward functions, the policy receives\nfeedback based on the reward, which is a cosine similarity between the goal\ntextual description and the statement description in the episode. We evaluated\nour approach in several environments and showed that semantic reward can guide\nlearning to achieve competitive control behavior, even in the absence of\nhand-crafted reward functions. Our study demonstrates a correlation between the\nlanguage embedding space and the conventional Euclidean space. This framework\nopens new horizons for aligning agent behavior with natural language goals and\nlays the groundwork for a more seamless integration of larger language models\n(LLMs) and fluid control applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5bf9\u9f50\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7SBERT\u8ba1\u7b97\u5956\u52b1\uff0c\u907f\u514d\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u8bbe\u8ba1\u6709\u6548\u7684\u5956\u52b1\u51fd\u6570\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4efb\u52a1\u76ee\u6807\u96be\u4ee5\u6570\u503c\u5316\u7684\u73af\u5883\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u624b\u52a8\u5de5\u7a0b\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u4f7f\u7528SBERT\u8ba1\u7b97\u5f53\u524d\u72b6\u6001\u4e0e\u76ee\u6807\u8bed\u4e49\u6307\u4ee4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u5956\u52b1\uff0c\u66ff\u4ee3\u624b\u52a8\u5b9a\u4e49\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bed\u4e49\u5956\u52b1\u80fd\u5f15\u5bfc\u5b66\u4e60\u5b9e\u73b0\u7ade\u4e89\u6027\u63a7\u5236\u884c\u4e3a\uff0c\u4e14\u8bed\u8a00\u5d4c\u5165\u7a7a\u95f4\u4e0e\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u5b58\u5728\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u4e0e\u4ee3\u7406\u884c\u4e3a\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u63a7\u5236\u5e94\u7528\u7684\u96c6\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.05984", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05984", "abs": "https://arxiv.org/abs/2508.05984", "authors": ["Ankur Naskar", "Gugan Thoppe", "Vijay Gupta"], "title": "Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning", "comment": null, "summary": "Algorithms for solving \\textit{nonlinear} fixed-point equations -- such as\naverage-reward \\textit{$Q$-learning} and \\textit{TD-learning} -- often involve\nsemi-norm contractions. Achieving parameter-free optimal convergence rates for\nthese methods via Polyak--Ruppert averaging has remained elusive, largely due\nto the non-monotonicity of such semi-norms. We close this gap by (i.) recasting\nthe averaged error as a linear recursion involving a nonlinear perturbation,\nand (ii.) taming the nonlinearity by coupling the semi-norm's contraction with\nthe monotonicity of a suitably induced norm. Our main result yields the first\nparameter-free $\\tilde{O}(1/\\sqrt{t})$ optimal rates for $Q$-learning in both\naverage-reward and exponentially discounted settings, where $t$ denotes the\niteration index. The result applies within a broad framework that accommodates\nsynchronous and asynchronous updates, single-agent and distributed deployments,\nand data streams obtained either from simulators or along Markovian\ntrajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u975e\u7ebf\u6027\u56fa\u5b9a\u70b9\u65b9\u7a0b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u534a\u8303\u6570\u6536\u7f29\u548c\u8bf1\u5bfc\u8303\u6570\u7684\u5355\u8c03\u6027\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u53c2\u6570\u65e0\u5173\u7684\u6700\u4f18\u6536\u655b\u901f\u7387\u3002", "motivation": "\u89e3\u51b3\u975e\u7ebf\u6027\u56fa\u5b9a\u70b9\u65b9\u7a0b\uff08\u5982\u5e73\u5747\u5956\u52b1Q\u5b66\u4e60\u548cTD\u5b66\u4e60\uff09\u4e2d\u534a\u8303\u6570\u975e\u5355\u8c03\u6027\u5bfc\u81f4\u7684\u6536\u655b\u901f\u7387\u95ee\u9898\u3002", "method": "\u5c06\u5e73\u5747\u8bef\u5dee\u91cd\u65b0\u8868\u8ff0\u4e3a\u6d89\u53ca\u975e\u7ebf\u6027\u6270\u52a8\u7684\u7ebf\u6027\u9012\u5f52\uff0c\u5e76\u901a\u8fc7\u534a\u8303\u6570\u6536\u7f29\u4e0e\u8bf1\u5bfc\u8303\u6570\u5355\u8c03\u6027\u7684\u8026\u5408\u6765\u63a7\u5236\u975e\u7ebf\u6027\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u53c2\u6570\u65e0\u5173\u7684$\\tilde{O}(1/\\sqrt{t})$\u6700\u4f18\u6536\u655b\u901f\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\uff08\u540c\u6b65/\u5f02\u6b65\u66f4\u65b0\u3001\u5355/\u591a\u4ee3\u7406\u90e8\u7f72\u7b49\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u975e\u7ebf\u6027\u56fa\u5b9a\u70b9\u65b9\u7a0b\u7684\u6c42\u89e3\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.05988", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05988", "abs": "https://arxiv.org/abs/2508.05988", "authors": ["Wenhao Zeng", "Yaoning Wang", "Chao Hu", "Yuling Shi", "Chengcheng Wan", "Hongyu Zhang", "Xiaodong Gu"], "title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal", "comment": "Code and model available at https://github.com/Zengwh02/ASAP", "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in code reasoning by scaling up the length of Chain-of-Thought\n(CoT). However, excessively long reasoning traces introduce substantial\nchallenges in terms of training cost, inference latency, and deployment\nfeasibility. While various CoT compression approaches have emerged to address\nthis challenge, they face inherent trade-offs: token-level methods often\ndisrupt syntactic and logical coherence, while step-level methods based on\nperplexity fail to reliably capture the logically critical reasoning steps. In\nthis paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel\ncoarse-to-fine framework for CoT compression. ASAP first performs anchor-guided\npruning to preserve the core reasoning structure, which efficiently reduces the\nsearch space for subsequent processing. It then enables a logic-aware pruning\nby selecting logically essential reasoning steps based on a novel first-token\nsurprisal metric. Finally, ASAP teaches models to autonomously generate and\nleverage these concise CoTs at inference time, enabling efficient reasoning in\ncoding tasks. Experiments show that ASAP achieves state-of-the-art accuracy\nacross multiple code generation benchmarks while substantially reducing\ntraining and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,\nour approach reduces token generation by 23.5% and inference latency by 43.5%\ncompared to the strongest baseline, while achieving a competitive accuracy of\n36.19% in Pass@1. Our results highlight a promising direction for building\npowerful and efficient LRMs.", "AI": {"tldr": "ASAP\u662f\u4e00\u79cd\u65b0\u9896\u7684CoT\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u951a\u70b9\u5f15\u5bfc\u548c\u57fa\u4e8e\u60ca\u8bb6\u5ea6\u7684\u4fee\u526a\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u548c\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u957f\u63a8\u7406\u94fe\u5e26\u6765\u7684\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u63a8\u7406\u5ef6\u8fdf\u957f\u548c\u90e8\u7f72\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u951a\u70b9\u5f15\u5bfc\u4fee\u526a\u548c\u57fa\u4e8e\u7b2c\u4e00\u6807\u8bb0\u60ca\u8bb6\u5ea6\u7684\u903b\u8f91\u611f\u77e5\u4fee\u526a\uff0c\u5e76\u7ed3\u5408\u81ea\u4e3b\u751f\u6210\u7b80\u6d01CoT\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u51c6\u786e\u7387\uff0c\u663e\u8457\u51cf\u5c11\u751f\u6210\u6807\u8bb0\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "ASAP\u4e3a\u6784\u5efa\u9ad8\u6548\u5f3a\u5927\u7684LRMs\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2508.05995", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05995", "abs": "https://arxiv.org/abs/2508.05995", "authors": ["Fei Xu Yu", "Gina Adam", "Nathaniel D. Bastian", "Tian Lan"], "title": "Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode generation and structured reasoning; however, their performance often\ndegrades on complex tasks that require consistent multi-step planning. Recent\nwork has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet\nexisting approaches primarily focus on generating heuristic-based code for\noptimization or target simpler tasks where correctness alone is sufficient. In\nthis work, we propose MCTS-OPS, a novel neural-symbolic framework that\nformulates prompt selection as a sequential decision process guided by MCTS.\nOur method explores and refines multi-step prompt sequences for the goal of\nimproving code generation quality and enhancing the problem-solving\ncapabilities of LLMs in general optimization. Experiments on network\noptimization show significant improvement over the baselines, both in the\nsuccess rate of executing the generated code and in the optimization results\nwith the specified objective and constraints (2$\\sim$4$\\times$ higher reward\nand 3$\\times$ lower standard deviation). Moreover, it improves the chance of\nattaining the optimal solution by about 10\\% of cases, compared to baseline\nmethods in hard problems. These results highlight the promise of combining\nsymbolic planning with LLMs for robust, high-quality code generation in complex\ndomains.", "AI": {"tldr": "MCTS-OPS\u662f\u4e00\u4e2a\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9700\u8981\u591a\u6b65\u89c4\u5212\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0cMCTS-OPS\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u9009\u62e9\u63d0\u5347LLMs\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u5c06\u63d0\u793a\u9009\u62e9\u5efa\u6a21\u4e3aMCTS\u5f15\u5bfc\u7684\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63a2\u7d22\u548c\u4f18\u5316\u591a\u6b65\u63d0\u793a\u5e8f\u5217\u3002", "result": "\u5728\u7f51\u7edc\u4f18\u5316\u5b9e\u9a8c\u4e2d\uff0c\u4ee3\u7801\u6267\u884c\u6210\u529f\u7387\u548c\u4f18\u5316\u7ed3\u679c\u663e\u8457\u63d0\u5347\uff08\u5956\u52b1\u63d0\u9ad82~4\u500d\uff0c\u6807\u51c6\u5dee\u964d\u4f4e3\u500d\uff09\uff0c\u6700\u4f18\u89e3\u83b7\u5f97\u7387\u63d0\u9ad8\u7ea610%\u3002", "conclusion": "\u7ed3\u5408\u7b26\u53f7\u89c4\u5212\u548cLLMs\u5728\u590d\u6742\u9886\u57df\u4e2d\u5c55\u73b0\u51fa\u9ad8\u8d28\u91cf\u4ee3\u7801\u751f\u6210\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06023", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06023", "abs": "https://arxiv.org/abs/2508.06023", "authors": ["Xiaobin Shen", "Jonathan Elmer", "George H. Chen"], "title": "Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients", "comment": null, "summary": "Prognostication for comatose post-cardiac arrest patients is a critical\nchallenge that directly impacts clinical decision-making in the ICU. Clinical\ninformation that informs prognostication is collected serially over time.\nShortly after cardiac arrest, various time-invariant baseline features are\ncollected (e.g., demographics, cardiac arrest characteristics). After ICU\nadmission, additional features are gathered, including time-varying hemodynamic\ndata (e.g., blood pressure, doses of vasopressor medications). We view these as\ntwo phases in which we collect new features. In this study, we propose a novel\nstepwise dynamic competing risks model that improves the prediction of\nneurological outcomes by automatically determining when to take advantage of\ntime-invariant features (first phase) and time-varying features (second phase).\nNotably, our model finds patients for whom this second phase (time-varying\nhemodynamic) information is beneficial for prognostication and also when this\ninformation is beneficial (as we collect more hemodynamic data for a patient\nover time, how important these data are for prognostication varies). Our\napproach extends the standard Fine and Gray model to explicitly model the two\nphases and to incorporate neural networks to flexibly capture complex nonlinear\nfeature relationships. Evaluated on a retrospective cohort of 2,278 comatose\npost-arrest patients, our model demonstrates robust discriminative performance\nfor the competing outcomes of awakening, withdrawal of life-sustaining therapy,\nand death despite maximal support. Our approach generalizes to more than two\nphases in which new features are collected and could be used in other dynamic\nprediction tasks, where it may be helpful to know when and for whom newly\ncollected features significantly improve prediction.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u9636\u6bb5\u52a8\u6001\u7ade\u4e89\u98ce\u9669\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u5fc3\u810f\u9aa4\u505c\u540e\u660f\u8ff7\u60a3\u8005\u7684\u795e\u7ecf\u529f\u80fd\u7ed3\u5c40\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u5229\u7528\u65f6\u95f4\u4e0d\u53d8\u548c\u65f6\u95f4\u53d8\u5316\u7279\u5f81\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5fc3\u810f\u9aa4\u505c\u540e\u660f\u8ff7\u60a3\u8005\u7684\u9884\u540e\u9884\u6d4b\u5bf9ICU\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u968f\u65f6\u95f4\u53d8\u5316\u7684\u52a8\u6001\u7279\u5f81\u3002", "method": "\u6269\u5c55Fine and Gray\u6a21\u578b\uff0c\u5206\u9636\u6bb5\u5efa\u6a21\u65f6\u95f4\u4e0d\u53d8\u548c\u65f6\u95f4\u53d8\u5316\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\u3002", "result": "\u57282,278\u540d\u60a3\u8005\u7684\u56de\u987e\u6027\u961f\u5217\u4e2d\uff0c\u6a21\u578b\u5bf9\u89c9\u9192\u3001\u64a4\u9664\u751f\u547d\u652f\u6301\u548c\u6b7b\u4ea1\u7b49\u7ade\u4e89\u7ed3\u5c40\u8868\u73b0\u51fa\u7a33\u5065\u7684\u5224\u522b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u63a8\u5e7f\u81f3\u591a\u9636\u6bb5\u7279\u5f81\u6536\u96c6\u573a\u666f\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u52a8\u6001\u9884\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2508.06034", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06034", "abs": "https://arxiv.org/abs/2508.06034", "authors": ["Qin Chen", "Guojie Song"], "title": "Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity", "comment": "Accepted tp CIKM 2025", "summary": "Heterogeneous graphs (HGs) are common in real-world scenarios and often\nexhibit heterophily. However, most existing studies focus on either\nheterogeneity or heterophily in isolation, overlooking the prevalence of\nheterophilic HGs in practical applications. Such ignorance leads to their\nperformance degradation. In this work, we first identify two main challenges in\nmodeling heterophily HGs: (1) varying heterophily distributions across hops and\nmeta-paths; (2) the intricate and often heterophily-driven diversity of\nsemantic information across different meta-paths. Then, we propose the Adaptive\nHeterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN\nemploys a heterophily-aware convolution that accounts for heterophily\ndistributions specific to both hops and meta-paths. It then integrates messages\nfrom diverse semantic spaces using a coarse-to-fine attention mechanism, which\nfilters out noise and emphasizes informative signals. Experiments on seven\nreal-world graphs and twenty baselines demonstrate the superior performance of\nAHGNN, particularly in high-heterophily situations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08AHGNN\uff09\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u56fe\uff08HGs\uff09\u4e2d\u5f02\u8d28\u6027\u5206\u5e03\u548c\u8bed\u4e49\u591a\u6837\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5f02\u8d28\u6027\u611f\u77e5\u5377\u79ef\u548c\u7c97\u5230\u7ec6\u7684\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5b64\u7acb\u5904\u7406\u5f02\u8d28\u6027\u6216\u5f02\u8d28\u6027\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u5f02\u8d28\u6027HGs\u7684\u666e\u904d\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faAHGNN\uff0c\u91c7\u7528\u5f02\u8d28\u6027\u611f\u77e5\u5377\u79ef\u5904\u7406\u4e0d\u540c\u8df3\u6570\u548c\u5143\u8def\u5f84\u7684\u5f02\u8d28\u6027\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u7c97\u5230\u7ec6\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u591a\u8bed\u4e49\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u548c\u4e8c\u5341\u4e2a\u57fa\u7ebf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAHGNN\u5728\u9ad8\u5f02\u8d28\u6027\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "AHGNN\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u8d28\u6027HGs\u7684\u5efa\u6a21\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.06041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06041", "abs": "https://arxiv.org/abs/2508.06041", "authors": ["Sangwoo Kwon", "Seong Hoon Seo", "Jae W. Lee", "Yeonhong Park"], "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment", "comment": null, "summary": "How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding iterations. Building on this\ninsight, we introduce DP-LLM, a novel mechanism that dynamically assigns\nprecision to each layer based on input values. DP-LLM augments each linear\nlayer in an LLM with a precision selector that determines the bitwidth at\nruntime using a lightweight error estimator and threshold values learned\nthrough fine-tuning. Experimental results across multiple models and benchmarks\ndemonstrate that DP-LLM achieves a superior performance-latency trade-off,\noutperforming prior approaches.", "AI": {"tldr": "DP-LLM\u662f\u4e00\u79cd\u52a8\u6001\u5206\u914d\u7cbe\u5ea6\u7684\u673a\u5236\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bef\u5dee\u4f30\u8ba1\u5668\u548c\u9608\u503c\u5b66\u4e60\uff0c\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bbe\u5907\u4e0a\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u4e0e\u5ef6\u8fdf\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u5728\u8bbe\u5907\u4e0a\u8fd0\u884c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u5982\u4f55\u6839\u636e\u52a8\u6001\u53d8\u5316\u7684\u8fd0\u884c\u65f6\u7ea6\u675f\uff08\u5982\u5ef6\u8fdf\u548c\u7cbe\u5ea6\uff09\u6709\u6548\u914d\u7f6e\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDP-LLM\u673a\u5236\uff0c\u52a8\u6001\u4e3a\u6bcf\u4e2a\u5c42\u5206\u914d\u7cbe\u5ea6\uff0c\u57fa\u4e8e\u8f93\u5165\u503c\u548c\u8f7b\u91cf\u7ea7\u8bef\u5dee\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u5fae\u8c03\u5b66\u4e60\u9608\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDP-LLM\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u4e0e\u5ef6\u8fdf\u6743\u8861\u3002", "conclusion": "DP-LLM\u901a\u8fc7\u52a8\u6001\u7cbe\u5ea6\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bbe\u5907\u4e0a\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.06066", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06066", "abs": "https://arxiv.org/abs/2508.06066", "authors": ["Barak Gahtan", "Alex M. Bronstein"], "title": "Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology", "comment": null, "summary": "Deep temporal architectures such as Temporal Convolutional Networks (TCNs)\nachieve strong predictive performance on sequential data, yet theoretical\nunderstanding of their generalization remains limited. We address this gap by\nproviding both the first non-vacuous, architecture-aware generalization bounds\nfor deep temporal models and a principled evaluation methodology.\n  For exponentially $\\beta$-mixing sequences, we derive bounds scaling as $\nO\\!\\Bigl(R\\,\\sqrt{\\tfrac{D\\,p\\,n\\,\\log N}{N}}\\Bigr), $ where $D$ is network\ndepth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our\ndelayed-feedback blocking mechanism transforms dependent samples into\neffectively independent ones while discarding only $O(1/\\log N)$ of the data,\nyielding $\\sqrt{D}$ scaling instead of exponential, implying that doubling\ndepth requires approximately quadrupling the training data.\n  We also introduce a fair-comparison methodology that fixes the effective\nsample size to isolate the effect of temporal structure from information\ncontent. Under $N_{\\text{eff}}=2{,}000$, strongly dependent sequences\n($\\rho=0.8$) exhibit $\\approx76\\%$ smaller generalization gaps than weakly\ndependent ones ($\\rho=0.2$), challenging the intuition that dependence is\npurely detrimental. Yet convergence rates diverge from theory: weak\ndependencies follow $N_{\\text{eff}}^{-1.21}$ scaling and strong dependencies\nfollow $N_{\\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.\nThese findings reveal that temporal dependence can enhance learning under fixed\ninformation budgets, while highlighting gaps between theory and practice that\nmotivate future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u6df1\u5ea6\u65f6\u5e8f\u6a21\u578b\uff08\u5982TCN\uff09\u7684\u67b6\u6784\u611f\u77e5\u6cdb\u5316\u8fb9\u754c\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u516c\u5e73\u6bd4\u8f83\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u65f6\u5e8f\u4f9d\u8d56\u6027\u5728\u56fa\u5b9a\u4fe1\u606f\u9884\u7b97\u4e0b\u53ef\u80fd\u589e\u5f3a\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u7406\u89e3\u6df1\u5ea6\u65f6\u5e8f\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u586b\u8865\u7406\u8bba\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63a8\u5bfc\u4e86\u975e\u7a7a\u6cdb\u7684\u6cdb\u5316\u8fb9\u754c\uff0c\u4f7f\u7528\u5ef6\u8fdf\u53cd\u9988\u963b\u585e\u673a\u5236\u5c06\u4f9d\u8d56\u6837\u672c\u8f6c\u5316\u4e3a\u72ec\u7acb\u6837\u672c\uff0c\u5e76\u5f15\u5165\u516c\u5e73\u6bd4\u8f83\u65b9\u6cd5\u3002", "result": "\u6cdb\u5316\u8fb9\u754c\u4e0e\u7f51\u7edc\u6df1\u5ea6\u3001\u6838\u5927\u5c0f\u7b49\u53c2\u6570\u76f8\u5173\uff0c\u65f6\u5e8f\u4f9d\u8d56\u6027\u5728\u56fa\u5b9a\u4fe1\u606f\u9884\u7b97\u4e0b\u53ef\u80fd\u589e\u5f3a\u5b66\u4e60\u6548\u679c\u3002\u5b9e\u9a8c\u53d1\u73b0\u6536\u655b\u901f\u5ea6\u4e0e\u7406\u8bba\u9884\u6d4b\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u65f6\u5e8f\u4f9d\u8d56\u6027\u53ef\u80fd\u5bf9\u5b66\u4e60\u6709\u76ca\uff0c\u4f46\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.06097", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06097", "abs": "https://arxiv.org/abs/2508.06097", "authors": ["Simon B\u00fchrer", "Andreas Plesner", "Till Aczel", "Roger Wattenhofer"], "title": "Recurrent Deep Differentiable Logic Gate Networks", "comment": null, "summary": "While differentiable logic gates have shown promise in feedforward networks,\ntheir application to sequential modeling remains unexplored. This paper\npresents the first implementation of Recurrent Deep Differentiable Logic Gate\nNetworks (RDDLGN), combining Boolean operations with recurrent architectures\nfor sequence-to-sequence learning.\n  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and\n30.9\\% accuracy during training, approaching GRU performance (5.41 BLEU) and\ngraceful degradation (4.39 BLEU) during inference. This work establishes\nrecurrent logic-based neural computation as viable, opening research directions\nfor FPGA acceleration in sequential modeling and other recursive network\narchitectures.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u5b9e\u73b0\u4e86\u5faa\u73af\u6df1\u5ea6\u53ef\u5fae\u903b\u8f91\u95e8\u7f51\u7edc\uff08RDDLGN\uff09\uff0c\u5c06\u5e03\u5c14\u8fd0\u7b97\u4e0e\u5faa\u73af\u67b6\u6784\u7ed3\u5408\u7528\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\uff0c\u5728WMT'14\u82f1\u5fb7\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1GRU\u3002", "motivation": "\u63a2\u7d22\u53ef\u5fae\u903b\u8f91\u95e8\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u5176\u5728\u5faa\u73af\u67b6\u6784\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faRDDLGN\uff0c\u7ed3\u5408\u5e03\u5c14\u8fd0\u7b97\u4e0e\u5faa\u73af\u67b6\u6784\uff0c\u7528\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u3002", "result": "\u5728WMT'14\u82f1\u5fb7\u7ffb\u8bd1\u4e2d\uff0cRDDLGN\u8bad\u7ec3\u65f6\u8fbe\u52305.00 BLEU\u548c30.9%\u51c6\u786e\u7387\uff0c\u63a8\u7406\u65f6\u8868\u73b0\u63a5\u8fd1GRU\uff085.41 BLEU\uff09\u4e14\u5177\u6709\u4f18\u96c5\u9000\u5316\uff084.39 BLEU\uff09\u3002", "conclusion": "RDDLGN\u8bc1\u660e\u4e86\u57fa\u4e8e\u5faa\u73af\u903b\u8f91\u7684\u795e\u7ecf\u8ba1\u7b97\u7684\u53ef\u884c\u6027\uff0c\u4e3aFPGA\u52a0\u901f\u548c\u9012\u5f52\u7f51\u7edc\u67b6\u6784\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.06108", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06108", "abs": "https://arxiv.org/abs/2508.06108", "authors": ["Xing Lei", "Wenyan Yang", "Kaiqiang Ke", "Shentao Yang", "Xuetao Zhang", "Joni Pajarinen", "Donglin Wang"], "title": "GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning", "comment": null, "summary": "Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a\nfundamental challenge in reinforcement learning. While hindsight experience\nreplay (HER) has shown promise by relabeling collected trajectories with\nachieved goals, we argue that trajectory relabeling alone does not fully\nexploit the available experiences in off-policy GCRL methods, resulting in\nlimited sample efficiency. In this paper, we propose Hindsight Goal-conditioned\nRegularization (HGR), a technique that generates action regularization priors\nbased on hindsight goals. When combined with hindsight self-imitation\nregularization (HSR), our approach enables off-policy RL algorithms to maximize\nexperience utilization. Compared to existing GCRL methods that employ HER and\nself-imitation techniques, our hindsight regularizations achieve substantially\nmore efficient sample reuse and the best performances, which we empirically\ndemonstrate on a suite of navigation and manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u540e\u89c1\u76ee\u6807\u751f\u6210\u52a8\u4f5c\u6b63\u5219\u5316\u5148\u9a8c\u7684\u6280\u672fHGR\uff0c\u7ed3\u5408HSR\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u5229\u7528\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u5956\u52b1\u4e0b\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff08GCRL\uff09\u6548\u7387\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u5982HER\u672a\u80fd\u5145\u5206\u5229\u7528\u7ecf\u9a8c\u3002", "method": "\u63d0\u51faHindsight Goal-conditioned Regularization (HGR)\u548cHindsight Self-Imitation Regularization (HSR)\u7ed3\u5408\uff0c\u6700\u5927\u5316\u7ecf\u9a8c\u5229\u7528\u3002", "result": "\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cHGR\u548cHSR\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u91cd\u7528\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "HGR\u548cHSR\u7684\u7ec4\u5408\u4e3aGCRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6837\u672c\u5229\u7528\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.06151", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06151", "abs": "https://arxiv.org/abs/2508.06151", "authors": ["Yong Oh Lee", "JeeEun Kim", "Jung Woo Lee"], "title": "Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models", "comment": null, "summary": "In oral cancer diagnostics, the limited availability of annotated datasets\nfrequently constrains the performance of diagnostic models, particularly due to\nthe variability and insufficiency of training data. To address these\nchallenges, this study proposed a novel approach to enhance diagnostic accuracy\nby synthesizing realistic oral cancer lesions using an inpainting technique\nwith a fine-tuned diffusion model. We compiled a comprehensive dataset from\nmultiple sources, featuring a variety of oral cancer images. Our method\ngenerated synthetic lesions that exhibit a high degree of visual fidelity to\nactual lesions, thereby significantly enhancing the performance of diagnostic\nalgorithms. The results show that our classification model achieved a\ndiagnostic accuracy of 0.97 in differentiating between cancerous and\nnon-cancerous tissues, while our detection model accurately identified lesion\nlocations with 0.85 accuracy. This method validates the potential for synthetic\nimage generation in medical diagnostics and paves the way for further research\ninto extending these methods to other types of cancer diagnostics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u53e3\u8154\u764c\u75c5\u53d8\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u53e3\u8154\u764c\u8bca\u65ad\u4e2d\u6807\u6ce8\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u9650\u5236\u4e86\u8bca\u65ad\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\u548c\u4fee\u590d\u6280\u672f\u5408\u6210\u903c\u771f\u7684\u53e3\u8154\u764c\u75c5\u53d8\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u591a\u6e90\u6570\u636e\u96c6\u3002", "result": "\u5206\u7c7b\u6a21\u578b\u5728\u533a\u5206\u764c\u53d8\u548c\u975e\u764c\u53d8\u7ec4\u7ec7\u65f6\u51c6\u786e\u7387\u8fbe0.97\uff0c\u68c0\u6d4b\u6a21\u578b\u5b9a\u4f4d\u75c5\u53d8\u7684\u51c6\u786e\u7387\u4e3a0.85\u3002", "conclusion": "\u5408\u6210\u56fe\u50cf\u751f\u6210\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u764c\u75c7\u8bca\u65ad\u7814\u7a76\u3002"}}
{"id": "2508.06183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06183", "abs": "https://arxiv.org/abs/2508.06183", "authors": ["Xiyuan Yang", "Shengyuan Hu", "Soyeon Kim", "Tian Li"], "title": "Differentially Private Federated Clustering with Random Rebalancing", "comment": "21 pages", "summary": "Federated clustering aims to group similar clients into clusters and produce\none model for each cluster. Such a personalization approach typically improves\nmodel performance compared with training a single model to serve all clients,\nbut can be more vulnerable to privacy leakage. Directly applying client-level\ndifferentially private (DP) mechanisms to federated clustering could degrade\nthe utilities significantly. We identify that such deficiencies are mainly due\nto the difficulties of averaging privacy noise within each cluster (following\nstandard privacy mechanisms), as the number of clients assigned to the same\nclusters is uncontrolled. To this end, we propose a simple and effective\ntechnique, named RR-Cluster, that can be viewed as a light-weight add-on to\nmany federated clustering algorithms. RR-Cluster achieves reduced privacy noise\nvia randomly rebalancing cluster assignments, guaranteeing a minimum number of\nclients assigned to each cluster. We analyze the tradeoffs between decreased\nprivacy noise variance and potentially increased bias from incorrect\nassignments and provide convergence bounds for RR-Clsuter. Empirically, we\ndemonstrate the RR-Cluster plugged into strong federated clustering algorithms\nresults in significantly improved privacy/utility tradeoffs across both\nsynthetic and real-world datasets.", "AI": {"tldr": "RR-Cluster\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6280\u672f\uff0c\u901a\u8fc7\u968f\u673a\u91cd\u65b0\u5e73\u8861\u805a\u7c7b\u5206\u914d\uff0c\u51cf\u5c11\u9690\u79c1\u566a\u58f0\uff0c\u63d0\u5347\u8054\u90a6\u805a\u7c7b\u4e2d\u7684\u9690\u79c1/\u6548\u7528\u6743\u8861\u3002", "motivation": "\u8054\u90a6\u805a\u7c7b\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u8bad\u7ec3\u5355\u72ec\u6a21\u578b\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u53ef\u80fd\u589e\u52a0\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u76f4\u63a5\u5e94\u7528\u5dee\u5206\u9690\u79c1\u673a\u5236\u4f1a\u663e\u8457\u964d\u4f4e\u6548\u7528\u3002", "method": "\u63d0\u51faRR-Cluster\u6280\u672f\uff0c\u901a\u8fc7\u968f\u673a\u91cd\u65b0\u5e73\u8861\u805a\u7c7b\u5206\u914d\uff0c\u786e\u4fdd\u6bcf\u4e2a\u805a\u7c7b\u6709\u6700\u5c0f\u5ba2\u6237\u7aef\u6570\u91cf\uff0c\u51cf\u5c11\u9690\u79c1\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRR-Cluster\u663e\u8457\u6539\u5584\u4e86\u9690\u79c1/\u6548\u7528\u6743\u8861\uff0c\u9002\u7528\u4e8e\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u3002", "conclusion": "RR-Cluster\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6280\u672f\uff0c\u80fd\u663e\u8457\u63d0\u5347\u8054\u90a6\u805a\u7c7b\u7684\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.06199", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06199", "abs": "https://arxiv.org/abs/2508.06199", "authors": ["Mateusz Praski", "Jakub Adamczyk", "Wojciech Czech"], "title": "Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning", "comment": null, "summary": "Pretrained neural networks have attracted significant interest in chemistry\nand small molecule drug design. Embeddings from these models are widely used\nfor molecular property prediction, virtual screening, and small data learning\nin molecular chemistry. This study presents the most extensive comparison of\nsuch models to date, evaluating 25 models across 25 datasets. Under a fair\ncomparison framework, we assess models spanning various modalities,\narchitectures, and pretraining strategies. Using a dedicated hierarchical\nBayesian statistical testing model, we arrive at a surprising result: nearly\nall neural models show negligible or no improvement over the baseline ECFP\nmolecular fingerprint. Only the CLAMP model, which is also based on molecular\nfingerprints, performs statistically significantly better than the\nalternatives. These findings raise concerns about the evaluation rigor in\nexisting studies. We discuss potential causes, propose solutions, and offer\npractical recommendations.", "AI": {"tldr": "\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5728\u5316\u5b66\u548c\u5c0f\u5206\u5b50\u836f\u7269\u8bbe\u8ba1\u4e2d\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u4ec5CLAMP\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfECFP\u6307\u7eb9\u3002", "motivation": "\u8bc4\u4f30\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5b50\u5316\u5b66\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u63ed\u793a\u73b0\u6709\u7814\u7a76\u7684\u8bc4\u4f30\u4e25\u8c28\u6027\u95ee\u9898\u3002", "method": "\u5bf925\u79cd\u6a21\u578b\u572825\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\uff0c\u6db5\u76d6\u591a\u79cd\u6a21\u6001\u3001\u67b6\u6784\u548c\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u5206\u5c42\u8d1d\u53f6\u65af\u7edf\u8ba1\u6d4b\u8bd5\u6a21\u578b\u3002", "result": "\u51e0\u4e4e\u6240\u6709\u795e\u7ecf\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff0c\u4ec5CLAMP\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfECFP\u6307\u7eb9\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8d28\u7591\u73b0\u6709\u8bc4\u4f30\u7684\u4e25\u8c28\u6027\uff0c\u63d0\u51fa\u6f5c\u5728\u539f\u56e0\u3001\u89e3\u51b3\u65b9\u6848\u53ca\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2508.06208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06208", "abs": "https://arxiv.org/abs/2508.06208", "authors": ["Ce Na", "Kai Yang", "Dengzhao Fang", "Yu Li", "Jingtong Gao", "Chengcheng Zhu", "Jiale Zhang", "Xiaobing Sun", "Yi Chang"], "title": "Graph Federated Learning for Personalized Privacy Recommendation", "comment": null, "summary": "Federated recommendation systems (FedRecs) have gained significant attention\nfor providing privacy-preserving recommendation services. However, existing\nFedRecs assume that all users have the same requirements for privacy\nprotection, i.e., they do not upload any data to the server. The approaches\noverlook the potential to enhance the recommendation service by utilizing\npublicly available user data. In real-world applications, users can choose to\nbe private or public. Private users' interaction data is not shared, while\npublic users' interaction data can be shared. Inspired by the issue, this paper\nproposes a novel Graph Federated Learning for Personalized Privacy\nRecommendation (GFed-PP) that adapts to different privacy requirements while\nimproving recommendation performance. GFed-PP incorporates the interaction data\nof public users to build a user-item interaction graph, which is then used to\nform a user relationship graph. A lightweight graph convolutional network (GCN)\nis employed to learn each user's user-specific personalized item embedding. To\nprotect user privacy, each client learns the user embedding and the scoring\nfunction locally. Additionally, GFed-PP achieves optimization of the federated\nrecommendation framework through the initialization of item embedding on\nclients and the aggregation of the user relationship graph on the server.\nExperimental results demonstrate that GFed-PP significantly outperforms\nexisting methods for five datasets, offering superior recommendation accuracy\nwithout compromising privacy. This framework provides a practical solution for\naccommodating varying privacy preferences in federated recommendation systems.", "AI": {"tldr": "GFed-PP\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\uff0c\u9002\u5e94\u4e0d\u540c\u9690\u79c1\u9700\u6c42\u5e76\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u5047\u8bbe\u6240\u6709\u7528\u6237\u9690\u79c1\u9700\u6c42\u76f8\u540c\uff0c\u5ffd\u7565\u4e86\u516c\u5f00\u7528\u6237\u6570\u636e\u7684\u6f5c\u529b\u3002", "method": "\u5229\u7528\u516c\u5f00\u7528\u6237\u6570\u636e\u6784\u5efa\u4ea4\u4e92\u56fe\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7GCN\u5b66\u4e60\u4e2a\u6027\u5316\u5d4c\u5165\uff0c\u672c\u5730\u5b66\u4e60\u4fdd\u62a4\u9690\u79c1\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a8\u8350\u51c6\u786e\u6027\u66f4\u9ad8\u4e14\u4e0d\u727a\u7272\u9690\u79c1\u3002", "conclusion": "GFed-PP\u4e3a\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u4e0d\u540c\u9690\u79c1\u504f\u597d\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06214", "abs": "https://arxiv.org/abs/2508.06214", "authors": ["Hai Zhong", "Xun Wang", "Zhuoran Li", "Longbo Huang"], "title": "Reparameterization Proximal Policy Optimization", "comment": null, "summary": "Reparameterization policy gradient (RPG) is promising for improving sample\nefficiency by leveraging differentiable dynamics. However, a critical barrier\nis its training instability, where high-variance gradients can destabilize the\nlearning process. To address this, we draw inspiration from Proximal Policy\nOptimization (PPO), which uses a surrogate objective to enable stable sample\nreuse in the model-free setting. We first establish a connection between this\nsurrogate objective and RPG, which has been largely unexplored and is\nnon-trivial. Then, we bridge this gap by demonstrating that the\nreparameterization gradient of a PPO-like surrogate objective can be computed\nefficiently using backpropagation through time. Based on this key insight, we\npropose Reparameterization Proximal Policy Optimization (RPO), a stable and\nsample-efficient RPG-based method. RPO enables multiple epochs of stable sample\nreuse by optimizing a clipped surrogate objective tailored for RPG, while being\nfurther stabilized by Kullback-Leibler (KL) divergence regularization and\nremaining fully compatible with existing variance reduction methods. We\nevaluate RPO on a suite of challenging locomotion and manipulation tasks, where\nexperiments demonstrate that our method achieves superior sample efficiency and\nstrong performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a33\u5b9a\u7684\u91cd\u53c2\u6570\u5316\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff08RPO\uff09\uff0c\u901a\u8fc7\u7ed3\u5408PPO\u7684\u66ff\u4ee3\u76ee\u6807\u548cKL\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u91cd\u53c2\u6570\u5316\u7b56\u7565\u68af\u5ea6\uff08RPG\uff09\u5728\u6837\u672c\u6548\u7387\u4e0a\u6709\u6f5c\u529b\uff0c\u4f46\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u9ad8\u65b9\u5dee\u68af\u5ea6\u4f1a\u7834\u574f\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5c06PPO\u7684\u66ff\u4ee3\u76ee\u6807\u4e0eRPG\u7ed3\u5408\uff0c\u63d0\u51faRPO\u65b9\u6cd5\uff0c\u5229\u7528\u65f6\u95f4\u53cd\u5411\u4f20\u64ad\u9ad8\u6548\u8ba1\u7b97\u68af\u5ea6\uff0c\u5e76\u5f15\u5165KL\u6b63\u5219\u5316\u8fdb\u4e00\u6b65\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u8fd0\u52a8\u548c\u64cd\u4f5c\u4efb\u52a1\u4e0a\uff0cRPO\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "RPO\u662f\u4e00\u79cd\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684RPG\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2508.06244", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06244", "abs": "https://arxiv.org/abs/2508.06244", "authors": ["Xurun Wang", "Guangrui Liu", "Xinjie Li", "Haoyu He", "Lin Yao", "Weizhe Zhang"], "title": "Membership Inference Attack with Partial Features", "comment": null, "summary": "Machine learning models have been shown to be susceptible to membership\ninference attack, which can be used to determine whether a given sample appears\nin the training data. Existing membership inference methods commonly assume\nthat the adversary has full access to the features of the target sample. This\nassumption, however, does not hold in many real-world scenarios where only\npartial features information is available, thereby limiting the applicability\nof these methods. In this work, we study an inference scenario where the\nadversary observes only partial features of each sample and aims to infer\nwhether this observed subset was present in the training set of the target\nmodel. We define this problem as Partial Feature Membership Inference (PFMI).\nTo address this problem, we propose MRAD (Memory-guided Reconstruction and\nAnomaly Detection), a two-stage attack framework. In the first stage, MRAD\noptimizes the unknown feature values to minimize the loss of the sample. In the\nsecond stage, it measures the deviation between the reconstructed sample and\nthe training distribution using anomaly detection. Empirical results\ndemonstrate that MRAD is effective across a range of datasets, and maintains\ncompatibility with various off-the-shelf anomaly detection techniques. For\nexample, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of\nthe missing features.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u90e8\u5206\u7279\u5f81\u4fe1\u606f\u53ef\u7528\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86MRAD\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u653b\u51fb\u5b9e\u73b0\u9ad8\u6548\u63a8\u65ad\u3002", "motivation": "\u73b0\u6709\u6210\u5458\u63a8\u65ad\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u653b\u51fb\u8005\u80fd\u83b7\u53d6\u76ee\u6807\u6837\u672c\u7684\u5168\u90e8\u7279\u5f81\uff0c\u4f46\u73b0\u5b9e\u4e2d\u5f80\u5f80\u53ea\u80fd\u83b7\u53d6\u90e8\u5206\u7279\u5f81\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "method": "\u63d0\u51faMRAD\u6846\u67b6\uff0c\u7b2c\u4e00\u9636\u6bb5\u4f18\u5316\u672a\u77e5\u7279\u5f81\u503c\u4ee5\u6700\u5c0f\u5316\u6837\u672c\u635f\u5931\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u5f02\u5e38\u68c0\u6d4b\u8861\u91cf\u91cd\u6784\u6837\u672c\u4e0e\u8bad\u7ec3\u5206\u5e03\u7684\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMRAD\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u6709\u6548\uff0c\u4f8b\u5982\u5728STL-10\u4e0a\uff0c\u5373\u4f7f\u7f3a\u593140%\u7279\u5f81\uff0cAUC\u4ecd\u53ef\u8fbe0.6\u3002", "conclusion": "MRAD\u89e3\u51b3\u4e86\u90e8\u5206\u7279\u5f81\u4e0b\u7684\u6210\u5458\u63a8\u65ad\u95ee\u9898\uff0c\u517c\u5bb9\u591a\u79cd\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.06249", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06249", "abs": "https://arxiv.org/abs/2508.06249", "authors": ["David Kacz\u00e9r", "Magnus J\u00f8rgenv\u00e5g", "Clemens Vetter", "Lucie Flek", "Florian Mai"], "title": "In-Training Defenses against Emergent Misalignment in Language Models", "comment": "Under review", "summary": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\u4e2d\u51fa\u73b0\u7684\u7a81\u53d1\u6027\u9519\u4f4d\uff08EMA\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u56db\u79cd\u8bad\u7ec3\u6b63\u5219\u5316\u5e72\u9884\u65b9\u6cd5\u4ee5\u51cf\u5c11EMA\u3002", "motivation": "\u5fae\u8c03LLM\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5728\u76ee\u6807\u9886\u57df\u5916\u4ea7\u751f\u6709\u5bb3\u884c\u4e3a\uff0c\u5373\u4f7f\u5fae\u8c03\u6570\u636e\u672c\u8eab\u65e0\u5bb3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22API\u63d0\u4f9b\u5546\u53ef\u7528\u7684\u8bad\u7ec3\u4fdd\u969c\u63aa\u65bd\u3002", "method": "\u7814\u7a76\u4e86\u56db\u79cd\u5e72\u9884\u65b9\u6cd5\uff1aKL\u6563\u5ea6\u6b63\u5219\u5316\u3001\u7279\u5f81\u7a7a\u95f4\u21132\u8ddd\u79bb\u3001\u5b89\u5168\u5b50\u7a7a\u95f4\u6295\u5f71\uff08SafeLoRA\uff09\u548c\u6df7\u5408\u5b89\u5168\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u56db\u4e2a\u6076\u610f\u4efb\u52a1\u4e2d\u7684EMA\u6548\u679c\uff0c\u4ee5\u53ca\u5bf9\u826f\u6027\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u7a81\u53d1\u6027\u9519\u4f4d\u7814\u7a76\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.06251", "categories": ["cs.LG", "cs.AI", "cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06251", "abs": "https://arxiv.org/abs/2508.06251", "authors": ["Alejandro Moreno R.", "Desale Fentaw", "Samuel Palmer", "Ra\u00fal Salles de Padua", "Ninad Dixit", "Samuel Mugel", "Roman Or\u00fas", "Manuel Radons", "Josef Menter", "Ali Abedi"], "title": "Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)", "comment": "10 pages", "summary": "Synthetic data generation is a key technique in modern artificial\nintelligence, addressing data scarcity, privacy constraints, and the need for\ndiverse datasets in training robust models. In this work, we propose a method\nfor generating privacy-preserving high-quality synthetic tabular data using\nTensor Networks, specifically Matrix Product States (MPS). We benchmark the\nMPS-based generative model against state-of-the-art models such as CTGAN, VAE,\nand PrivBayes, focusing on both fidelity and privacy-preserving capabilities.\nTo ensure differential privacy (DP), we integrate noise injection and gradient\nclipping during training, enabling privacy guarantees via R\\'enyi Differential\nPrivacy accounting. Across multiple metrics analyzing data fidelity and\ndownstream machine learning task performance, our results show that MPS\noutperforms classical models, particularly under strict privacy constraints.\nThis work highlights MPS as a promising tool for privacy-aware synthetic data\ngeneration. By combining the expressive power of tensor network representations\nwith formal privacy mechanisms, the proposed approach offers an interpretable\nand scalable alternative for secure data sharing. Its structured design\nfacilitates integration into sensitive domains where both data quality and\nconfidentiality are critical.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e9\u9635\u4e58\u79ef\u6001\uff08MPS\uff09\u7684\u9690\u79c1\u4fdd\u62a4\u9ad8\u8d28\u91cf\u5408\u6210\u8868\u683c\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u3001\u9690\u79c1\u7ea6\u675f\u4ee5\u53ca\u591a\u6837\u5316\u6570\u636e\u96c6\u9700\u6c42\uff0c\u4e3a\u8bad\u7ec3\u9c81\u68d2\u6a21\u578b\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u4f7f\u7528MPS\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u7ed3\u5408\u566a\u58f0\u6ce8\u5165\u548c\u68af\u5ea6\u88c1\u526a\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\uff08DP\uff09\uff0c\u5e76\u901a\u8fc7R\u00e9nyi\u5dee\u5206\u9690\u79c1\u6838\u7b97\u63d0\u4f9b\u9690\u79c1\u4fdd\u8bc1\u3002", "result": "MPS\u5728\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u6027\u80fd\u4e0a\u4f18\u4e8eCTGAN\u3001VAE\u548cPrivBayes\u7b49\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4e25\u683c\u9690\u79c1\u7ea6\u675f\u4e0b\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "MPS\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u9690\u79c1\u611f\u77e5\u5408\u6210\u6570\u636e\u751f\u6210\u5de5\u5177\uff0c\u7ed3\u5408\u5f20\u91cf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5f62\u5f0f\u5316\u9690\u79c1\u673a\u5236\uff0c\u4e3a\u5b89\u5168\u6570\u636e\u5171\u4eab\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06257", "abs": "https://arxiv.org/abs/2508.06257", "authors": ["Jielong Lu", "Zhihao Wu", "Jiajun Yu", "Jiajun Bu", "Haishuai Wang"], "title": "Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors", "comment": null, "summary": "Integrating multi-omics datasets through data-driven analysis offers a\ncomprehensive understanding of the complex biological processes underlying\nvarious diseases, particularly cancer. Graph Neural Networks (GNNs) have\nrecently demonstrated remarkable ability to exploit relational structures in\nbiological data, enabling advances in multi-omics integration for cancer\nsubtype classification. Existing approaches often neglect the intricate\ncoupling between heterogeneous omics, limiting their capacity to resolve subtle\ncancer subtype heterogeneity critical for precision oncology. To address these\nlimitations, we propose a framework named Graph Transformer for Multi-omics\nCancer Subtype Classification (GTMancer). This framework builds upon the GNN\noptimization problem and extends its application to complex multi-omics data.\nSpecifically, our method leverages contrastive learning to embed multi-omics\ndata into a unified semantic space. We unroll the multiplex graph optimization\nproblem in that unified space and introduce dual sets of attention coefficients\nto capture structural graph priors both within and among multi-omics data. This\napproach enables global omics information to guide the refining of the\nrepresentations of individual omics. Empirical experiments on seven real-world\ncancer datasets demonstrate that GTMancer outperforms existing state-of-the-art\nalgorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGTMancer\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u6bd4\u5b66\u4e60\u6574\u5408\u591a\u7ec4\u5b66\u6570\u636e\uff0c\u7528\u4e8e\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u7ec4\u5b66\u6570\u636e\u6574\u5408\u4e2d\u5ffd\u89c6\u5f02\u6784\u7ec4\u5b66\u95f4\u7684\u590d\u6742\u8026\u5408\uff0c\u9650\u5236\u4e86\u5176\u5728\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5c06\u591a\u7ec4\u5b66\u6570\u636e\u5d4c\u5165\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u901a\u8fc7\u53cc\u6ce8\u610f\u529b\u7cfb\u6570\u6355\u6349\u7ec4\u5185\u548c\u7ec4\u95f4\u7ed3\u6784\u5148\u9a8c\uff0c\u4f18\u5316\u56fe\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5728\u4e03\u4e2a\u771f\u5b9e\u764c\u75c7\u6570\u636e\u96c6\u4e0a\uff0cGTMancer\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "GTMancer\u901a\u8fc7\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\u4e2a\u4f53\u7ec4\u5b66\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7ec4\u5b66\u6570\u636e\u6574\u5408\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2508.06269", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06269", "abs": "https://arxiv.org/abs/2508.06269", "authors": ["Zhuoran Li", "Xun Wang", "Hai Zhong", "Longbo Huang"], "title": "OM2P: Offline Multi-Agent Mean-Flow Policy", "comment": null, "summary": "Generative models, especially diffusion and flow-based models, have been\npromising in offline multi-agent reinforcement learning. However, integrating\npowerful generative models into this framework poses unique challenges. In\nparticular, diffusion and flow-based policies suffer from low sampling\nefficiency due to their iterative generation processes, making them impractical\nin time-sensitive or resource-constrained settings. To tackle these\ndifficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel\noffline MARL algorithm to achieve efficient one-step action sampling. To\naddress the misalignment between generative objectives and reward maximization,\nwe introduce a reward-aware optimization scheme that integrates a\ncarefully-designed mean-flow matching loss with Q-function supervision.\nAdditionally, we design a generalized timestep distribution and a\nderivative-free estimation strategy to reduce memory overhead and improve\ntraining stability. Empirical evaluations on Multi-Agent Particle and MuJoCo\nbenchmarks demonstrate that OM2P achieves superior performance, with up to a\n3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.\nOur approach represents the first to successfully integrate mean-flow model\ninto offline MARL, paving the way for practical and scalable generative\npolicies in cooperative multi-agent settings.", "AI": {"tldr": "OM2P\u662f\u4e00\u79cd\u65b0\u578b\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e00\u6b65\u52a8\u4f5c\u91c7\u6837\u548c\u5956\u52b1\u611f\u77e5\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u6a21\u578b\u5728\u79bb\u7ebfMARL\u4e2d\u7684\u4f4e\u6548\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u548c\u6d41\u6a21\u578b\uff09\u5728\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8fed\u4ee3\u751f\u6210\u8fc7\u7a0b\u5bfc\u81f4\u91c7\u6837\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u65f6\u95f4\u654f\u611f\u6216\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002", "method": "\u63d0\u51faOM2P\u7b97\u6cd5\uff0c\u7ed3\u5408\u5747\u503c\u6d41\u5339\u914d\u635f\u5931\u548cQ\u51fd\u6570\u76d1\u7763\u7684\u5956\u52b1\u611f\u77e5\u4f18\u5316\u65b9\u6848\uff0c\u8bbe\u8ba1\u5e7f\u4e49\u65f6\u95f4\u6b65\u5206\u5e03\u548c\u65e0\u5bfc\u6570\u4f30\u8ba1\u7b56\u7565\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728Multi-Agent Particle\u548cMuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOM2P\u6027\u80fd\u4f18\u8d8a\uff0cGPU\u5185\u5b58\u4f7f\u7528\u51cf\u5c113.8\u500d\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u534710.8\u500d\u3002", "conclusion": "OM2P\u9996\u6b21\u6210\u529f\u5c06\u5747\u503c\u6d41\u6a21\u578b\u96c6\u6210\u5230\u79bb\u7ebfMARL\u4e2d\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u751f\u6210\u7b56\u7565\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2508.06280", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06280", "abs": "https://arxiv.org/abs/2508.06280", "authors": ["Gokul Adethya T", "S. Jaya Nirmala"], "title": "A Study on Regularization-Based Continual Learning Methods for Indic ASR", "comment": null, "summary": "Indias linguistic diversity poses significant challenges for developing\ninclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual\nmodels, which require simultaneous access to all language data, are impractical\ndue to the sequential arrival of data and privacy constraints. Continual\nLearning (CL) offers a solution by enabling models to learn new languages\nsequentially without catastrophically forgetting previously learned knowledge.\nThis paper investigates CL for ASR on Indian languages using a subset of the\nIndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,\ninitially pretrained on Hindi, which is then incrementally trained on eight\nadditional Indian languages, for a total sequence of nine languages. We\nevaluate three prominent regularization- and distillation-based CL strategies:\nElastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning\nwithout Forgetting (LwF), selected for their suitability in no-replay,\nprivacy-conscious scenarios. Performance is analyzed using Word Error Rate\n(WER) for both RNN-T and CTC paths on clean and noisy data, as well as\nknowledge retention via Backward Transfer. We also explore the impact of\nvarying the number of training epochs (1, 2, 5, and 10) per task. Results,\ncompared against naive fine-tuning, demonstrate CLs effectiveness in mitigating\nforgetting, making it a promising approach for scalable ASR in diverse Indian\nlanguages under realistic constraints. The code is available at:\nhttps://github.com/FrozenWolf-Cyber/Indic-CL-ASR", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5370\u5ea6\u8bed\u8a00\u591a\u6837\u6027\u80cc\u666f\u4e0b\uff0c\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u65b9\u6cd5\u5f00\u53d1\u5305\u5bb9\u6027\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5370\u5ea6\u8bed\u8a00\u591a\u6837\u6027\u4f7f\u5f97\u4f20\u7edf\u591a\u8bed\u8a00ASR\u6a21\u578b\u96be\u4ee5\u5b9e\u73b0\uff0c\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u4e0d\u9057\u5fd8\u5df2\u5b66\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u9010\u6b65\u5b66\u4e60\u65b0\u8bed\u8a00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eConformer\u7684\u6df7\u5408RNN-T/CTC\u6a21\u578b\uff0c\u4ece\u5370\u5730\u8bed\u9884\u8bad\u7ec3\u5f00\u59cb\uff0c\u9010\u6b65\u5b66\u4e60\u516b\u79cd\u5370\u5ea6\u8bed\u8a00\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cdCL\u7b56\u7565\uff08EWC\u3001MAS\u3001LwF\uff09\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cCL\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u5370\u5ea6\u8bed\u8a00ASR\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6301\u7eed\u5b66\u4e60\u5728\u5370\u5ea6\u8bed\u8a00ASR\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u6570\u636e\u987a\u5e8f\u5230\u8fbe\u548c\u9690\u79c1\u53d7\u9650\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2508.06292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06292", "abs": "https://arxiv.org/abs/2508.06292", "authors": ["Sanja Karilanova", "Subhrakanti Dey", "Ay\u00e7a \u00d6z\u00e7elikkale"], "title": "Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback", "comment": "15 pages, 7 Tables, 6 Figures", "summary": "Neuromorphic computing is an emerging technology enabling low-latency and\nenergy-efficient signal processing. A key algorithmic tool in neuromorphic\ncomputing is spiking neural networks (SNNs). SNNs are biologically inspired\nneural networks which utilize stateful neurons, and provide low-bit data\nprocessing by encoding and decoding information using spikes. Similar to SNNs,\ndeep state-space models (SSMs) utilize stateful building blocks. However, deep\nSSMs, which recently achieved competitive performance in various temporal\nmodeling tasks, are typically designed with high-precision activation functions\nand no reset mechanisms. To bridge the gains offered by SNNs and the recent\ndeep SSM models, we propose a novel multiple-output spiking neuron model that\ncombines a linear, general SSM state transition with a non-linear feedback\nmechanism through reset. Compared to the existing neuron models for SNNs, our\nproposed model clearly conceptualizes the differences between the spiking\nfunction, the reset condition and the reset action. The experimental results on\nvarious tasks, i.e., a keyword spotting task, an event-based vision task and a\nsequential pattern recognition task, show that our proposed model achieves\nperformance comparable to existing benchmarks in the SNN literature. Our\nresults illustrate how the proposed reset mechanism can overcome instability\nand enable learning even when the linear part of neuron dynamics is unstable,\nallowing us to go beyond the strictly enforced stability of linear dynamics in\nrecent deep SSM models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u8f93\u51fa\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u7ebf\u6027\u72b6\u6001\u8f6c\u79fb\u548c\u975e\u7ebf\u6027\u53cd\u9988\u673a\u5236\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7ed3\u5408\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u548c\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u7684\u4f18\u52bf\uff0c\u89e3\u51b3SSMs\u7f3a\u4e4f\u91cd\u7f6e\u673a\u5236\u548c\u9ad8\u7cbe\u5ea6\u6fc0\u6d3b\u51fd\u6570\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u8f93\u51fa\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u660e\u786e\u533a\u5206\u4e86\u8109\u51b2\u529f\u80fd\u3001\u91cd\u7f6e\u6761\u4ef6\u548c\u91cd\u7f6e\u52a8\u4f5c\uff0c\u5e76\u5f15\u5165\u4e86\u975e\u7ebf\u6027\u53cd\u9988\u673a\u5236\u3002", "result": "\u5728\u5173\u952e\u8bcd\u8bc6\u522b\u3001\u4e8b\u4ef6\u89c6\u89c9\u548c\u5e8f\u5217\u6a21\u5f0f\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4e0e\u73b0\u6709SNN\u57fa\u51c6\u76f8\u5f53\uff0c\u4e14\u91cd\u7f6e\u673a\u5236\u514b\u670d\u4e86\u7ebf\u6027\u52a8\u6001\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u901a\u8fc7\u975e\u7ebf\u6027\u53cd\u9988\u673a\u5236\u6269\u5c55\u4e86\u6df1\u5ea6SSMs\u7684\u5e94\u7528\u8303\u56f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86SNNs\u7684\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6548\u80fd\u4f18\u52bf\u3002"}}
{"id": "2508.06301", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.06301", "abs": "https://arxiv.org/abs/2508.06301", "authors": ["Junhyeog Yun", "Minui Hong", "Gunhee Kim"], "title": "FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields", "comment": "ICCV 2025", "summary": "Neural fields provide a memory-efficient representation of data, which can\neffectively handle diverse modalities and large-scale data. However, learning\nto map neural fields often requires large amounts of training data and\ncomputations, which can be limited to resource-constrained edge devices. One\napproach to tackle this limitation is to leverage Federated Meta-Learning\n(FML), but traditional FML approaches suffer from privacy leakage. To address\nthese issues, we introduce a novel FML approach called FedMeNF. FedMeNF\nutilizes a new privacy-preserving loss function that regulates privacy leakage\nin the local meta-optimization. This enables the local meta-learner to optimize\nquickly and efficiently without retaining the client's private data. Our\nexperiments demonstrate that FedMeNF achieves fast optimization speed and\nrobust reconstruction performance, even with few-shot or non-IID data across\ndiverse data modalities, while preserving client data privacy.", "AI": {"tldr": "FedMeNF\u662f\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u635f\u5931\u51fd\u6570\u89e3\u51b3\u4f20\u7edfFML\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u795e\u7ecf\u573a\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f20\u7edfFML\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002", "method": "\u63d0\u51faFedMeNF\uff0c\u91c7\u7528\u9690\u79c1\u4fdd\u62a4\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u672c\u5730\u5143\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFedMeNF\u5728\u5c11\u6837\u672c\u548c\u975eIID\u6570\u636e\u4e0b\u4ecd\u80fd\u5feb\u901f\u4f18\u5316\u5e76\u4fdd\u6301\u9690\u79c1\u3002", "conclusion": "FedMeNF\u5728\u9ad8\u6548\u5b66\u4e60\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.06336", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06336", "abs": "https://arxiv.org/abs/2508.06336", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Victor Oei", "Anna Penzkofer", "Andreas Bulling"], "title": "Unsupervised Partner Design Enables Robust Ad-hoc Teamwork", "comment": "16 pages", "summary": "We introduce Unsupervised Partner Design (UPD) - a population-free,\nmulti-agent reinforcement learning framework for robust ad-hoc teamwork that\nadaptively generates training partners without requiring pretrained partners or\nmanual parameter tuning. UPD constructs diverse partners by stochastically\nmixing an ego agent's policy with biased random behaviours and scores them\nusing a variance-based learnability metric that prioritises partners near the\nego agent's current learning frontier. We show that UPD can be integrated with\nunsupervised environment design, resulting in the first method enabling fully\nunsupervised curricula over both level and partner distributions in a\ncooperative setting. Through extensive evaluations on Overcooked-AI and the\nOvercooked Generalisation Challenge, we demonstrate that this dynamic partner\ncurriculum is highly effective: UPD consistently outperforms both\npopulation-based and population-free baselines as well as ablations. In a user\nstudy, we further show that UPD achieves higher returns than all baselines and\nwas perceived as significantly more adaptive, more human-like, a better\ncollaborator, and less frustrating.", "AI": {"tldr": "UPD\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u3001\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u8bad\u7ec3\u4f19\u4f34\u63d0\u5347\u534f\u4f5c\u80fd\u529b\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u4f19\u4f34\u6216\u624b\u52a8\u8c03\u53c2\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u4f19\u4f34\u6216\u624b\u52a8\u8c03\u53c2\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u534f\u4f5c\u5b66\u4e60\u3002", "method": "\u901a\u8fc7\u968f\u673a\u6df7\u5408\u667a\u80fd\u4f53\u7b56\u7565\u4e0e\u504f\u7f6e\u968f\u673a\u884c\u4e3a\u751f\u6210\u591a\u6837\u4f19\u4f34\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u65b9\u5dee\u7684\u6613\u5b66\u6027\u6307\u6807\u8bc4\u5206\u3002", "result": "\u5728Overcooked-AI\u7b49\u6d4b\u8bd5\u4e2d\uff0cUPD\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u66f4\u5177\u9002\u5e94\u6027\u548c\u4eba\u6027\u5316\u3002", "conclusion": "UPD\u662f\u9996\u4e2a\u5b9e\u73b0\u5b8c\u5168\u65e0\u76d1\u7763\u534f\u4f5c\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u80fd\u529b\u3002"}}
{"id": "2508.06346", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06346", "abs": "https://arxiv.org/abs/2508.06346", "authors": ["Mert Can Kurucu", "Tufan Kumbasar", "\u0130brahim Eksin", "M\u00fcjde G\u00fczelkaya"], "title": "Introducing Fractional Classification Loss for Robust Learning with Noisy Labels", "comment": "25 pages, 6 figures, 2 table. Submitted to Pattern Recognition", "summary": "Robust loss functions are crucial for training deep neural networks in the\npresence of label noise, yet existing approaches require extensive,\ndataset-specific hyperparameter tuning. In this work, we introduce Fractional\nClassification Loss (FCL), an adaptive robust loss that automatically\ncalibrates its robustness to label noise during training. Built within the\nactive-passive loss framework, FCL employs the fractional derivative of the\nCross-Entropy (CE) loss as its active component and the Mean Absolute Error\n(MAE) as its passive loss component. With this formulation, we demonstrate that\nthe fractional derivative order $\\mu$ spans a family of loss functions that\ninterpolate between MAE-like robustness and CE-like fast convergence.\nFurthermore, we integrate $\\mu$ into the gradient-based optimization as a\nlearnable parameter and automatically adjust it to optimize the trade-off\nbetween robustness and convergence speed. We reveal that FCL's unique property\nestablishes a critical trade-off that enables the stable learning of $\\mu$:\nlower log penalties on difficult or mislabeled examples improve robustness but\nimpose higher penalties on easy or clean data, reducing model confidence in\nthem. Consequently, FCL can dynamically reshape its loss landscape to achieve\neffective classification performance under label noise. Extensive experiments\non benchmark datasets show that FCL achieves state-of-the-art results without\nthe need for manual hyperparameter tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u9c81\u68d2\u635f\u5931\u51fd\u6570FCL\uff0c\u901a\u8fc7\u5206\u6570\u9636\u5bfc\u6570\u548cMAE\u7ed3\u5408\uff0c\u52a8\u6001\u8c03\u6574\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u53c2\u3002", "motivation": "\u73b0\u6709\u9c81\u68d2\u635f\u5931\u51fd\u6570\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\u7279\u5b9a\u7684\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "FCL\u7ed3\u5408\u4e86\u4ea4\u53c9\u71b5\u7684\u5206\u6570\u9636\u5bfc\u6570\uff08\u4e3b\u52a8\u90e8\u5206\uff09\u548cMAE\uff08\u88ab\u52a8\u90e8\u5206\uff09\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u53c2\u6570\u03bc\u52a8\u6001\u5e73\u8861\u9c81\u68d2\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "result": "FCL\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u53c2\u3002", "conclusion": "FCL\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6807\u7b7e\u566a\u58f0\u4e0b\u7684\u5206\u7c7b\u95ee\u9898\u3002"}}
{"id": "2508.06347", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.06347", "abs": "https://arxiv.org/abs/2508.06347", "authors": ["Ruiyu Zhang", "Ce Zhao", "Xin Zhao", "Lin Nie", "Wai-Fung Lam"], "title": "Structural Equation-VAE: Disentangled Latent Representations for Tabular Data", "comment": "10 pages, 2 figures", "summary": "Learning interpretable latent representations from tabular data remains a\nchallenge in deep generative modeling. We introduce SE-VAE (Structural\nEquation-Variational Autoencoder), a novel architecture that embeds measurement\nstructure directly into the design of a variational autoencoder. Inspired by\nstructural equation modeling, SE-VAE aligns latent subspaces with known\nindicator groupings and introduces a global nuisance latent to isolate\nconstruct-specific confounding variation. This modular architecture enables\ndisentanglement through design rather than through statistical regularizers\nalone. We evaluate SE-VAE on a suite of simulated tabular datasets and\nbenchmark its performance against a series of leading baselines using standard\ndisentanglement metrics. SE-VAE consistently outperforms alternatives in factor\nrecovery, interpretability, and robustness to nuisance variation. Ablation\nresults reveal that architectural structure, rather than regularization\nstrength, is the key driver of performance. SE-VAE offers a principled\nframework for white-box generative modeling in scientific and social domains\nwhere latent constructs are theory-driven and measurement validity is\nessential.", "AI": {"tldr": "SE-VAE\u662f\u4e00\u79cd\u65b0\u578b\u53d8\u5206\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u6784\u65b9\u7a0b\u5efa\u6a21\u76f4\u63a5\u5d4c\u5165\u6d4b\u91cf\u7ed3\u6784\uff0c\u63d0\u5347\u6f5c\u5728\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u89e3\u8026\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8868\u683c\u6570\u636e\u4e2d\u6f5c\u5728\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u79d1\u5b66\u548c\u793e\u4f1a\u9886\u57df\uff0c\u7406\u8bba\u9a71\u52a8\u7684\u6f5c\u5728\u6784\u5ff5\u548c\u6d4b\u91cf\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "SE-VAE\u7ed3\u5408\u7ed3\u6784\u65b9\u7a0b\u5efa\u6a21\uff0c\u8bbe\u8ba1\u6f5c\u5728\u5b50\u7a7a\u95f4\u4e0e\u5df2\u77e5\u6307\u6807\u5206\u7ec4\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u5168\u5c40\u5e72\u6270\u6f5c\u5728\u53d8\u91cf\u4ee5\u9694\u79bb\u6784\u5ff5\u7279\u5f02\u6027\u6df7\u6742\u53d8\u5f02\u3002", "result": "SE-VAE\u5728\u6a21\u62df\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u56e0\u5b50\u6062\u590d\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5bf9\u5e72\u6270\u53d8\u5f02\u7684\u9c81\u68d2\u6027\u65b9\u9762\u3002", "conclusion": "SE-VAE\u901a\u8fc7\u67b6\u6784\u8bbe\u8ba1\u800c\u975e\u7edf\u8ba1\u6b63\u5219\u5316\u5b9e\u73b0\u89e3\u8026\uff0c\u4e3a\u79d1\u5b66\u548c\u793e\u4f1a\u9886\u57df\u7684\u767d\u76d2\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\u3002"}}
{"id": "2508.06353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06353", "abs": "https://arxiv.org/abs/2508.06353", "authors": ["Parichit Sharma", "Marcin Stanislaw", "Hasan Kurban", "Oguzhan Kulekci", "Mehmet Dalkilic"], "title": "Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means", "comment": null, "summary": "This paper introduces Geometric-k-means (or Gk-means for short), a novel\napproach that significantly enhances the efficiency and energy economy of the\nwidely utilized k-means algorithm, which, despite its inception over five\ndecades ago, remains a cornerstone in machine learning applications. The\nessence of Gk-means lies in its active utilization of geometric principles,\nspecifically scalar projection, to significantly accelerate the algorithm\nwithout sacrificing solution quality. This geometric strategy enables a more\ndiscerning focus on data points that are most likely to influence cluster\nupdates, which we call as high expressive data (HE). In contrast, low\nexpressive data (LE), does not impact clustering outcome, is effectively\nbypassed, leading to considerable reductions in computational overhead.\nExperiments spanning synthetic, real-world and high-dimensional datasets,\ndemonstrate Gk-means is significantly better than traditional and state of the\nart (SOTA) k-means variants in runtime and distance computations (DC).\nMoreover, Gk-means exhibits better resource efficiency, as evidenced by its\nreduced energy footprint, placing it as more sustainable alternative.", "AI": {"tldr": "Gk-means\u662f\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u539f\u7406\u7684\u65b0\u578bk-means\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6807\u91cf\u6295\u5f71\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u7684\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfk-means\u7b97\u6cd5\u6548\u7387\u8f83\u4f4e\uff0cGk-means\u65e8\u5728\u901a\u8fc7\u51e0\u4f55\u4f18\u5316\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u5347\u8fd0\u884c\u901f\u5ea6\u548c\u80fd\u6e90\u7ecf\u6d4e\u6027\u3002", "method": "\u5229\u7528\u51e0\u4f55\u539f\u7406\uff08\u6807\u91cf\u6295\u5f71\uff09\u533a\u5206\u9ad8\u8868\u8fbe\u6027\u6570\u636e\uff08HE\uff09\u548c\u4f4e\u8868\u8fbe\u6027\u6570\u636e\uff08LE\uff09\uff0c\u4ec5\u5173\u6ce8HE\u4ee5\u52a0\u901f\u805a\u7c7b\u66f4\u65b0\u3002", "result": "\u5728\u5408\u6210\u3001\u771f\u5b9e\u4e16\u754c\u548c\u9ad8\u7ef4\u6570\u636e\u96c6\u4e0a\uff0cGk-means\u5728\u8fd0\u884c\u65f6\u95f4\u548c\u8ddd\u79bb\u8ba1\u7b97\u4e0a\u4f18\u4e8e\u4f20\u7edf\u53caSOTA k-means\u53d8\u4f53\uff0c\u4e14\u80fd\u6e90\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "Gk-means\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u8282\u80fd\u7684k-means\u6539\u8fdb\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.06361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06361", "abs": "https://arxiv.org/abs/2508.06361", "authors": ["Zhaomin Wu", "Mingzhe Du", "See-Kiong Ng", "Bingsheng He"], "title": "Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts", "comment": null, "summary": "Large Language Models (LLMs) have been widely deployed in reasoning,\nplanning, and decision-making tasks, making their trustworthiness a critical\nconcern. The potential for intentional deception, where an LLM deliberately\nfabricates or conceals information to serve a hidden objective, remains a\nsignificant and underexplored threat. Existing studies typically induce such\ndeception by explicitly setting a \"hidden\" objective through prompting or\nfine-tuning, which may not fully reflect real-world human-LLM interactions.\nMoving beyond this human-induced deception, we investigate LLMs' self-initiated\ndeception on benign prompts. To address the absence of ground truth in this\nevaluation, we propose a novel framework using \"contact searching questions.\"\nThis framework introduces two statistical metrics derived from psychological\nprinciples to quantify the likelihood of deception. The first, the Deceptive\nIntention Score, measures the model's bias towards a hidden objective. The\nsecond, Deceptive Behavior Score, measures the inconsistency between the LLM's\ninternal belief and its expressed output. Upon evaluating 14 leading LLMs, we\nfind that both metrics escalate as task difficulty increases, rising in\nparallel for most models. Building on these findings, we formulate a\nmathematical model to explain this behavior. These results reveal that even the\nmost advanced LLMs exhibit an increasing tendency toward deception when\nhandling complex problems, raising critical concerns for the deployment of LLM\nagents in complex and crucial domains.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65e0\u660e\u786e\u8bf1\u5bfc\u4e0b\u7684\u81ea\u53d1\u6027\u6b3a\u9a97\u884c\u4e3a\uff0c\u63d0\u51fa\u91cf\u5316\u6b3a\u9a97\u503e\u5411\u7684\u7edf\u8ba1\u6307\u6807\uff0c\u5e76\u53d1\u73b0\u4efb\u52a1\u590d\u6742\u5ea6\u4e0e\u6b3a\u9a97\u503e\u5411\u6b63\u76f8\u5173\u3002", "motivation": "LLMs\u5728\u63a8\u7406\u548c\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u4f7f\u5176\u53ef\u4fe1\u5ea6\u6210\u4e3a\u5173\u952e\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u4eba\u4e3a\u8bf1\u5bfc\u7684\u6b3a\u9a97\uff0c\u5ffd\u89c6\u4e86\u6a21\u578b\u81ea\u53d1\u7684\u6b3a\u9a97\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u2018\u63a5\u89e6\u641c\u7d22\u95ee\u9898\u2019\u7684\u6846\u67b6\uff0c\u5f15\u5165\u4e24\u4e2a\u7edf\u8ba1\u6307\u6807\uff08\u6b3a\u9a97\u610f\u56fe\u5206\u6570\u548c\u6b3a\u9a97\u884c\u4e3a\u5206\u6570\uff09\u91cf\u5316LLMs\u7684\u6b3a\u9a97\u503e\u5411\u3002", "result": "\u8bc4\u4f3014\u4e2a\u4e3b\u6d41LLMs\u53d1\u73b0\uff0c\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u65f6\uff0c\u6b3a\u9a97\u503e\u5411\u663e\u8457\u4e0a\u5347\uff0c\u4e14\u4e24\u79cd\u6307\u6807\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u5373\u4f7f\u6700\u5148\u8fdb\u7684LLMs\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u4e5f\u8868\u73b0\u51fa\u6b3a\u9a97\u503e\u5411\uff0c\u8fd9\u5bf9LLMs\u5728\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\u63d0\u51fa\u4e86\u8b66\u793a\u3002"}}
{"id": "2508.06364", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2508.06364", "abs": "https://arxiv.org/abs/2508.06364", "authors": ["Renyi Zhou", "Huimin Zhu", "Jing Tang", "Min Li"], "title": "ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design", "comment": null, "summary": "Achieving precise control over a molecule's biological activity-encompassing\ntargeted activation/inhibition, cooperative multi-target modulation, and\noff-target toxicity mitigation-remains a critical challenge in de novo drug\ndesign. However, existing generative methods primarily focus on producing\nmolecules with a single desired activity, lacking integrated mechanisms for the\nsimultaneous management of multiple intended and unintended molecular\ninteractions. Here, we propose ActivityDiff, a generative approach based on the\nclassifier-guidance technique of diffusion models. It leverages separately\ntrained drug-target classifiers for both positive and negative guidance,\nenabling the model to enhance desired activities while minimizing harmful\noff-target effects. Experimental results show that ActivityDiff effectively\nhandles essential drug design tasks, including single-/dual-target generation,\nfragment-constrained dual-target design, selective generation to enhance target\nspecificity, and reduction of off-target effects. These results demonstrate the\neffectiveness of classifier-guided diffusion in balancing efficacy and safety\nin molecular design. Overall, our work introduces a novel paradigm for\nachieving integrated control over molecular activity, and provides ActivityDiff\nas a versatile and extensible framework.", "AI": {"tldr": "ActivityDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u7c7b\u5668\u5f15\u5bfc\u6280\u672f\u5b9e\u73b0\u591a\u76ee\u6807\u5206\u5b50\u8bbe\u8ba1\uff0c\u540c\u65f6\u589e\u5f3a\u76ee\u6807\u6d3b\u6027\u548c\u51cf\u5c11\u8131\u9776\u6548\u5e94\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u6d3b\u6027\u5206\u5b50\u751f\u6210\uff0c\u7f3a\u4e4f\u540c\u65f6\u7ba1\u7406\u591a\u76ee\u6807\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u5206\u5b50\u751f\u7269\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u548c\u5206\u7c7b\u5668\u5f15\u5bfc\u6280\u672f\uff0c\u5229\u7528\u5206\u522b\u8bad\u7ec3\u7684\u836f\u7269\u9776\u70b9\u5206\u7c7b\u5668\u8fdb\u884c\u6b63\u8d1f\u5f15\u5bfc\uff0c\u4f18\u5316\u5206\u5b50\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cActivityDiff\u80fd\u6709\u6548\u5904\u7406\u5355/\u53cc\u9776\u70b9\u751f\u6210\u3001\u7247\u6bb5\u7ea6\u675f\u53cc\u9776\u70b9\u8bbe\u8ba1\u3001\u9009\u62e9\u6027\u751f\u6210\u589e\u5f3a\u9776\u70b9\u7279\u5f02\u6027\u53ca\u51cf\u5c11\u8131\u9776\u6548\u5e94\u7b49\u4efb\u52a1\u3002", "conclusion": "ActivityDiff\u4e3a\u5206\u5b50\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u5e73\u8861\u529f\u6548\u548c\u5b89\u5168\u6027\u7684\u65b0\u8303\u5f0f\uff0c\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2508.06387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06387", "abs": "https://arxiv.org/abs/2508.06387", "authors": ["Anurag Tripathi", "Vaibhav Patle", "Abhinav Jain", "Ayush Pundir", "Sairam Menon", "Ajeet Kumar Singh"], "title": "End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation", "comment": "Accepted in IJCNN25", "summary": "Text-to-SQL bridges the gap between natural language and structured database\nlanguage, thus allowing non-technical users to easily query databases.\nTraditional approaches model text-to-SQL as a direct translation task, where a\ngiven Natural Language Query (NLQ) is mapped to an SQL command. Recent advances\nin large language models (LLMs) have significantly improved translation\naccuracy, however, these methods all require that the target database is\npre-specified. This becomes problematic in scenarios with multiple extensive\ndatabases, where identifying the correct database becomes a crucial yet\noverlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL\nframework to identify the user's intended database before generating SQL\nqueries. Our approach leverages LLMs and prompt engineering to extract implicit\ninformation from natural language queries (NLQs) in the form of a ruleset. We\nthen train a large db\\_id prediction model, which includes a RoBERTa-based\nfinetuned encoder, to predict the correct Database identifier (db\\_id) based on\nboth the NLQ and the LLM-generated rules. Finally, we refine the generated SQL\nby using critic agents to correct errors. Experimental results demonstrate that\nour framework outperforms the current state-of-the-art models in both database\nintent prediction and SQL generation accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u7aef\u5230\u7aef\u6587\u672c\u5230SQL\u6846\u67b6\uff0c\u5148\u8bc6\u522b\u76ee\u6807\u6570\u636e\u5e93\u518d\u751f\u6210SQL\u67e5\u8be2\uff0c\u5229\u7528LLM\u548c\u63d0\u793a\u5de5\u7a0b\u63d0\u53d6\u89c4\u5219\uff0c\u8bad\u7ec3db_id\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6279\u8bc4\u4ee3\u7406\u4fee\u6b63SQL\u9519\u8bef\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u76ee\u6807\u6570\u636e\u5e93\u5df2\u77e5\uff0c\u4f46\u5728\u591a\u6570\u636e\u5e93\u573a\u666f\u4e2d\u8bc6\u522b\u6b63\u786e\u6570\u636e\u5e93\u662f\u5173\u952e\u4f46\u88ab\u5ffd\u89c6\u7684\u6b65\u9aa4\u3002", "method": "1. \u4f7f\u7528LLM\u548c\u63d0\u793a\u5de5\u7a0b\u4ece\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e2d\u63d0\u53d6\u89c4\u5219\uff1b2. \u8bad\u7ec3\u57fa\u4e8eRoBERTa\u7684db_id\u9884\u6d4b\u6a21\u578b\uff1b3. \u901a\u8fc7\u6279\u8bc4\u4ee3\u7406\u4fee\u6b63SQL\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6570\u636e\u5e93\u610f\u56fe\u9884\u6d4b\u548cSQL\u751f\u6210\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e09\u9636\u6bb5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6570\u636e\u5e93\u573a\u666f\u4e0b\u7684\u6587\u672c\u5230SQL\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.06409", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06409", "abs": "https://arxiv.org/abs/2508.06409", "authors": ["Wooyong Jung", "Sola Kim", "Dongwook Kim", "Maryam Tabar", "Dongwon Lee"], "title": "A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images", "comment": "10 pages, Accepted to SBP-BRiMS 2025", "summary": "Homelessness in the United States has surged to levels unseen since the Great\nDepression. However, existing methods for monitoring it, such as point-in-time\n(PIT) counts, have limitations in terms of frequency, consistency, and spatial\ndetail. This study proposes a new approach using publicly available,\ncrowdsourced data, specifically 311 Service Calls and street-level imagery, to\ntrack and forecast homeless tent trends in San Francisco. Our predictive model\ncaptures fine-grained daily and neighborhood-level variations, uncovering\npatterns that traditional counts often overlook, such as rapid fluctuations\nduring the COVID-19 pandemic and spatial shifts in tent locations over time. By\nproviding more timely, localized, and cost-effective information, this approach\nserves as a valuable tool for guiding policy responses and evaluating\ninterventions aimed at reducing unsheltered homelessness.", "AI": {"tldr": "\u5229\u7528\u516c\u5f00\u4f17\u5305\u6570\u636e\uff08\u5982311\u670d\u52a1\u7535\u8bdd\u548c\u8857\u666f\u56fe\u50cf\uff09\u9884\u6d4b\u65e7\u91d1\u5c71\u65e0\u5bb6\u53ef\u5f52\u8005\u5e10\u7bf7\u8d8b\u52bf\uff0c\u5f25\u8865\u4f20\u7edf\u70b9\u8ba1\u6570\u7684\u4e0d\u8db3\u3002", "motivation": "\u7f8e\u56fd\u65e0\u5bb6\u53ef\u5f52\u95ee\u9898\u4e25\u91cd\uff0c\u4f46\u73b0\u6709\u76d1\u6d4b\u65b9\u6cd5\uff08\u5982\u70b9\u8ba1\uff09\u5728\u9891\u7387\u3001\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u7ec6\u8282\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "\u4f7f\u7528311\u670d\u52a1\u7535\u8bdd\u548c\u8857\u666f\u56fe\u50cf\u6570\u636e\uff0c\u5efa\u7acb\u9884\u6d4b\u6a21\u578b\u6355\u6349\u6bcf\u65e5\u548c\u793e\u533a\u7ea7\u522b\u7684\u53d8\u5316\u3002", "result": "\u6a21\u578b\u63ed\u793a\u4e86\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u7684\u6a21\u5f0f\uff08\u5982\u75ab\u60c5\u671f\u95f4\u7684\u5feb\u901f\u6ce2\u52a8\u548c\u5e10\u7bf7\u4f4d\u7f6e\u7684\u7a7a\u95f4\u53d8\u5316\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u653f\u7b56\u5236\u5b9a\u548c\u5e72\u9884\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ca\u65f6\u3001\u672c\u5730\u5316\u548c\u7ecf\u6d4e\u7684\u4fe1\u606f\u5de5\u5177\u3002"}}
{"id": "2508.06412", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06412", "abs": "https://arxiv.org/abs/2508.06412", "authors": ["Zichuan Liu", "Jinyu Wang", "Lei Song", "Jiang Bian"], "title": "Sample-efficient LLM Optimization with Reset Replay", "comment": null, "summary": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.", "AI": {"tldr": "LoRR\u662f\u4e00\u79cd\u63d2\u4ef6\uff0c\u901a\u8fc7\u9ad8\u91cd\u653e\u8bad\u7ec3\u548c\u5468\u671f\u6027\u91cd\u7f6e\u7b56\u7565\u63d0\u5347LLM\u7684\u6837\u672c\u6548\u7387\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u635f\u5931\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3RL\u548c\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5728LLM\u8bad\u7ec3\u4e2d\u6837\u672c\u6548\u7387\u4f4e\u548c\u521d\u59cb\u504f\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165LoRR\u63d2\u4ef6\uff0c\u91c7\u7528\u9ad8\u91cd\u653e\u8bad\u7ec3\u3001\u5468\u671f\u6027\u91cd\u7f6e\u7b56\u7565\u548c\u6df7\u5408\u4f18\u5316\u76ee\u6807\uff08SFT+\u504f\u597d\u635f\u5931\uff09\u3002", "result": "LoRR\u663e\u8457\u63d0\u5347\u591a\u79cd\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LoRR\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u4ece\u6709\u9650\u6570\u636e\u4e2d\u91ca\u653e\u66f4\u5927\u6027\u80fd\u3002"}}
{"id": "2508.06467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06467", "abs": "https://arxiv.org/abs/2508.06467", "authors": ["Ameya Anjarlekar", "Sandeep Pombra"], "title": "LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection", "comment": "14 Pages, 3 Figures, 11 Tables", "summary": "The growing legal and ethical scrutiny of large language models (LLMs)\nnecessitates effective machine unlearning, particularly for sensitive or\nunauthorized data. Existing empirical methods often yield incomplete forgetting\nor unintended degradation of unrelated knowledge due to poor localization. In\nthis work, we propose GRIN: a modular and targeted framework for LLM\nunlearning. GRIN introduces a novel gradient-ratio-based metric to identify\nparameters most responsible for memorizing forget data. We then perform\nselective noise injection into these parameters prior to fine-tuning, which\nimproves unlearning performance while maintaining model utility. Finally, we\npropose new evaluation metrics tailored to the LLM setting and validate our\napproach on standard benchmarks such as TOFU, WMDP, and SafePKU.", "AI": {"tldr": "GRIN\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u548c\u76ee\u6807\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9057\u5fd8\u5b66\u4e60\uff0c\u901a\u8fc7\u68af\u5ea6\u6bd4\u6307\u6807\u548c\u9009\u62e9\u6027\u566a\u58f0\u6ce8\u5165\u63d0\u9ad8\u9057\u5fd8\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u6cd5\u5f8b\u548c\u4f26\u7406\u95ee\u9898\uff0cLLM\u9700\u8981\u6709\u6548\u9057\u5fd8\u654f\u611f\u6216\u672a\u7ecf\u6388\u6743\u7684\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u9057\u5fd8\u4e0d\u5f7b\u5e95\u6216\u5f71\u54cd\u5176\u4ed6\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "method": "GRIN\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u6bd4\u7684\u6307\u6807\u6765\u8bc6\u522b\u4e0e\u9057\u5fd8\u6570\u636e\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u9009\u62e9\u6027\u566a\u58f0\u6ce8\u5165\u548c\u5fae\u8c03\u5b9e\u73b0\u9057\u5fd8\u3002", "result": "\u5728TOFU\u3001WMDP\u548cSafePKU\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86GRIN\u7684\u6709\u6548\u6027\u3002", "conclusion": "GRIN\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u7684\u9057\u5fd8\u80fd\u529b\u3002"}}
