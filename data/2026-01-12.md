<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [cs.LG](#cs.LG) [Total: 51]
- [stat.ML](#stat.ML) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Channel Selected Stratified Nested Cross Validation for Clinically Relevant EEG Based Parkinsons Disease Detection](https://arxiv.org/abs/2601.05276)
*Nicholas R. Rasmussen,Rodrigue Rizk,Longwei Wang,Arun Singh,KC Santosh*

Main category: eess.SP

TL;DR: 提出一个用于帕金森病早期检测的EEG分析统一评估框架，采用嵌套交叉验证和三种保障措施来避免数据泄露，在三个独立数据集上达到80.6%准确率


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期检测是临床神经科学的关键挑战，现有机器学习方法存在患者级数据泄露等缺陷，导致性能估计偏高并限制临床转化

Method: 提出基于嵌套交叉验证的统一评估框架，包含三种保障措施：患者级分层消除受试者重叠、多层窗口化处理异质EEG记录、内循环通道选择实现无信息泄露的特征降维

Result: 在三个独立数据集上，使用该框架训练的卷积神经网络达到80.6%准确率，在保留群体块测试中表现出最先进性能，与其他文献方法相当

Conclusion: 嵌套交叉验证是防止偏差的必要保障，也是选择患者级决策最相关信息的原则性方法，为其他生物医学信号分析领域提供了可复现的基础

Abstract: The early detection of Parkinsons disease remains a critical challenge in clinical neuroscience, with electroencephalography offering a noninvasive and scalable pathway toward population level screening. While machine learning has shown promise in this domain, many reported results suffer from methodological flaws, most notably patient level data leakage, inflating performance estimates and limiting clinical translation. To address these modeling pitfalls, we propose a unified evaluation framework grounded in nested cross validation and incorporating three complementary safeguards: (i) patient level stratification to eliminate subject overlap and ensure unbiased generalization, (ii) multi layered windowing to harmonize heterogeneous EEG recordings while preserving temporal dynamics, and (iii) inner loop channel selection to enable principled feature reduction without information leakage. Applied across three independent datasets with a heterogeneous number of channels, a convolutional neural network trained under this framework achieved 80.6% accuracy and demonstrated state of the art performance under held out population block testing, comparable to other methods in the literature. This performance underscores the necessity of nested cross validation as a safeguard against bias and as a principled means of selecting the most relevant information for patient level decisions, providing a reproducible foundation that can extend to other biomedical signal analysis domains.

</details>


### [2] [Discrete Mode Decomposition Meets Shapley Value: Robust Signal Prediction in Tactile Internet](https://arxiv.org/abs/2601.05323)
*Mohammad Ali Vahedifar,Qi Zhang*

Main category: eess.SP

TL;DR: 提出一种结合离散模式分解(DMD)和沙普利模式值(SMV)的预测框架，用于触觉信号预测，以应对触觉互联网中的延迟和丢包挑战。


<details>
  <summary>Details</summary>
Motivation: 触觉互联网需要超低延迟和高可靠性，但可变延迟和数据包丢失对保持沉浸式触觉通信构成重大挑战。

Method: 提出预测框架，整合离散模式分解(DMD)和沙普利模式值(SMV)。DMD将触觉信号分解为可解释的内在模式，SMV评估每个模式对预测准确性的贡献。结合Transformer架构实现高效预测。

Result: DMD+SMV结合Transformer显著优于基线方法：1样本预测准确率达98.9%，100样本预测准确率达92.5%；推理延迟极低，分别为0.056毫秒和2毫秒。

Conclusion: 该框架有潜力缓解触觉互联网的严格延迟和可靠性要求，而不影响性能，突显了其在触觉互联网系统中实际部署的可行性。

Abstract: Tactile Internet (TI) requires ultra-low latency and high reliability to ensure stability and transparency in touch-enabled teleoperation. However, variable delays and packet loss present significant challenges to maintaining immersive haptic communication. To address this, we propose a predictive framework that integrates Discrete Mode Decomposition (DMD) with Shapley Mode Value (SMV) for accurate and timely haptic signal prediction. DMD decomposes haptic signals into interpretable intrinsic modes, while SMV evaluates each mode's contribution to prediction accuracy, which is well-aligned with the goal-oriented semantic communication. Integrating SMV with DMD further accelerates inference, enabling efficient communication and smooth teleoperation even under adverse network conditions.
  Extensive experiments show that DMD+SMV, combined with a Transformer architecture, outperforms baseline methods significantly. It achieves 98.9% accuracy for 1-sample prediction and 92.5% for 100-sample prediction, as well as extremely low inference latency: 0.056 ms and 2 ms, respectively. These results demonstrate that the proposed framework has strong potential to ease the stringent latency and reliability requirements of TI without compromising performance, highlighting its feasibility for real-world deployment in TI systems.

</details>


### [3] [SPARK: Sparse Parametric Antenna Representation using Kernels](https://arxiv.org/abs/2601.05440)
*William Bjorndahl,Mark O'Hair,Ben Zoghi,Joseph Camp*

Main category: eess.SP

TL;DR: SPARK是一种免训练的压缩模型，将天线/RIS辐射模式分解为平滑全局基函数和稀疏局部波瓣，实现紧凑的参数化表示，用于可扩展的波束管理。


<details>
  <summary>Details</summary>
Motivation: 随着天线数量、用户和子带增加，CSI获取和反馈开销成为瓶颈。实际系统依赖码本进行波束管理，但硬件感知操作需要天线/RIS响应的显式表示，而高保真测量模式维度高、处理成本大。

Method: SPARK将辐射模式分解为平滑全局基（3D模式使用低阶球谐函数，RIS 1D方位角切割使用傅里叶级数）和稀疏局部波瓣（使用各向异性高斯核）。这是一种免训练的压缩模型。

Result: 在AERPAW测试床和公共RIS数据集上，SPARK分别实现了2.8倍和10.4倍的重建MSE降低。仿真显示，在固定上行链路预算下，使用紧凑模式描述和稀疏路径描述符可获得12.65%的平均上行链路吞吐量增益。

Conclusion: SPARK将密集辐射模式转换为紧凑的参数化模型，支持可扩展的硬件感知波束管理，解决了天线/RIS系统扩展时的CSI反馈瓶颈问题。

Abstract: Channel state information (CSI) acquisition and feedback overhead grows with the number of antennas, users, and reported subbands. This growth becomes a bottleneck for many antenna and reconfigurable intelligent surface (RIS) systems as arrays and user densities scale. Practical CSI feedback and beam management rely on codebooks, where beams are selected via indices rather than explicitly transmitting radiation patterns. Hardware-aware operation requires an explicit representation of the measured antenna/RIS response, yet high-fidelity measured patterns are high-dimensional and costly to handle. We present SPARK (Sparse Parametric Antenna Representation using Kernels), a training-free compression model that decomposes patterns into a smooth global base and sparse localized lobes. For 3D patterns, SPARK uses low-order spherical harmonics for global directivity and anisotropic Gaussian kernels for localized features. For RIS 1D azimuth cuts, it uses a Fourier-series base with 1D Gaussians. On patterns from the AERPAW testbed and a public RIS dataset, SPARK achieves up to 2.8$\times$ and 10.4$\times$ reductions in reconstruction MSE over baselines, respectively. Simulation shows that amortizing a compact pattern description and reporting sparse path descriptors can produce 12.65% mean uplink goodput gain under a fixed uplink budget. Overall, SPARK turns dense patterns into compact, parametric models for scalable, hardware-aware beam management.

</details>


### [4] [Deformation-Aware Observation Modeling for Radar-Based Human Sensing via 3D Scan-Depth Sequence Fusion](https://arxiv.org/abs/2601.05676)
*Guangqi Shi,Kimitaka Sumi,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 提出融合3D扫描与深度相机的表面变形感知雷达观测模型，提升呼吸监测的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 呼吸引起的人体表面非刚性变形影响电磁波散射，会降低雷达测量的鲁棒性，需要更精确的观测模型

Method: 融合静态高分辨率3D扫描与动态深度相机数据，使用相干点漂移算法进行非刚性配准，采用物理光学近似计算电磁散射，重建中频雷达信号

Result: 在低信号质量条件下，相比仅使用深度序列的模型，提出的模型表现出更高鲁棒性，位移波形相关系数达0.943和0.887，优于对比模型的0.868和0.796

Conclusion: 传感器融合的变形感知观测模型能真实再现雷达观测，为雷达测量变化解释提供物理基础见解

Abstract: Non-contact radar-based human sensing is often interpreted using simplified motion assumptions. However, respiration induces non-rigid surface deformation of the human body that impacts electromagnetic wave scattering and can degrade the robustness of measurements. To address this, we propose a surface-deformation-aware observation model for radar-based human sensing that fuses static high-resolution three-dimensional scanner measurements with temporal depth camera data to represent time-varying human surface geometry. Non-rigid registration using the coherent point drift algorithm is employed to align a static template with dynamic depth frames. Frame-wise electromagnetic scattering is subsequently computed using the physical optics approximation, allowing the reconstruction of intermediate-frequency radar signals that emulate radar observations. Validation against experimental radar data demonstrated that the proposed model exhibited greater robustness than a depth-sequence-only model under low-signal-quality conditions involving complex surface dynamics and multiple reflective sites. For two participants, the proposed model achieved higher Pearson correlation coefficients of 0.943 and 0.887 between model-derived and experimentally measured displacement waveforms, compared with 0.868 and 0.796 for the depth-sequence-only model. Furthermore, in a favorable case characterized by a single relatively-stationary reflective site, the proposed method achieved a correlation coefficient of 0.789 between model-derived and experimentally measured in-phase-quadrature magnitude variations. These results suggest that our sensor-fusion-based deformation-aware observation modeling can realistically reproduce radar observations and provide physically grounded insights into the interpretation of radar measurement variations.

</details>


### [5] [A Novel Deep Learning-Based Coarse-to-Fine Frame Synchronization Method for OTFS Systems](https://arxiv.org/abs/2601.05920)
*Meiwen Men,Tao Zhou,Kaifeng Bao,Zhiyang Guo,Yongning Qi,Liu Liu,Bo Ai*

Main category: eess.SP

TL;DR: 提出基于粗到细深度残差网络的低复杂度OTFS同步方法，利用延迟-时间域导频的周期性特征，通过两阶段策略实现高效同步


<details>
  <summary>Details</summary>
Motivation: OTFS调制在高移动性场景中具有优势，但传统同步算法性能有限，需要低复杂度且准确的同步方法

Method: 采用粗到细的深度残差网络架构，将同步问题转化为分层分类问题：第一阶段缩小搜索空间，第二阶段精确定位符号定时偏移

Result: 在包含多种信道模型和随机STO的综合仿真数据集上验证，相比传统基准方法，在低信噪比和高移动性场景下实现了鲁棒的信号起始检测和更高的精度

Conclusion: 提出的基于深度学习的同步方法在保持高估计精度的同时显著降低了计算复杂度，特别适用于高移动性OTFS系统

Abstract: Orthogonal time frequency space (OTFS) modulation is a robust candidate waveform for future wireless systems, particularly in high-mobility scenarios, as it effectively mitigates the impact of rapidly time-varying channels by mapping symbols in the delay-Doppler (DD) domain. However, accurate frame synchronization in OTFS systems remains a challenge due to the performance limitations of conventional algorithms. To address this, we propose a low-complexity synchronization method based on a coarse-to-fine deep residual network (ResNet) architecture. Unlike traditional approaches relying on high-overhead preamble structures, our method exploits the intrinsic periodic features of OTFS pilots in the delay-time (DT) domain to formulate synchronization as a hierarchical classification problem. Specifically, the proposed architecture employs a two-stage strategy to first narrow the search space and then pinpoint the precise symbol timing offset (STO), thereby significantly reducing computational complexity while maintaining high estimation accuracy. We construct a comprehensive simulation dataset incorporating diverse channel models and randomized STO to validate the method. Extensive simulation results demonstrate that the proposed method achieves robust signal start detection and superior accuracy compared to conventional benchmarks, particularly in low signal-to-noise ratio (SNR) regimes and high-mobility scenarios.

</details>


### [6] [Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world](https://arxiv.org/abs/2601.05923)
*E. Middell,L. Carlton,S. Moradi,T. Codina,T. Fischer,J. Cutler,S. Kelley,J. Behrendt,T. Dissanayake,N. Harmening,M. A. Yücel,D. A. Boas,A. von Lühmann*

Main category: eess.SP

TL;DR: Cedalion是一个基于Python的开源框架，旨在统一fNIRS和DOT数据的模型驱动与数据驱动分析，提供可重复、可扩展的标准化工作流。


<details>
  <summary>Details</summary>
Motivation: 当前fNIRS和DOT分析工具分散在不同平台，限制了可重复性、互操作性和与现代机器学习工作流的集成，需要统一的解决方案。

Method: 开发基于Python的开源框架，集成前向建模、光极配准、信号处理、GLM分析、DOT图像重建和机器学习方法，遵循SNIRF和BIDS标准，支持容器化工作流和Jupyter笔记本。

Result: 创建了Cedalion框架，提供七个可执行笔记本演示核心功能，实现了可重复、可扩展、云就绪和ML就绪的fNIRS/DOT工作流。

Conclusion: Cedalion为实验室和真实世界神经影像提供了开放、透明、社区可扩展的基础，支持可重复、可扩展的fNIRS/DOT分析工作流。

Abstract: Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.

</details>


### [7] [Curving Beam Reflections: Model and Experimental Validation](https://arxiv.org/abs/2601.05998)
*Caroline Jane Spindel,Edward Knightly*

Main category: eess.SP

TL;DR: 提出了首个几何框架，能够准确预测任意凸形亚太赫兹弯曲光束在任意反射面上的反射行为，通过勒让德变换而非传统光线反射方法，实现了毫米级精度的预测。


<details>
  <summary>Details</summary>
Motivation: 弯曲光束是未来毫米波到亚太赫兹网络中绕过障碍物的有前景方法，但缺乏对任意表面反射的通用预测模型。传统的光线反射方法（镜像轨迹）在一般情况下失效。

Method: 引入几何框架，将光束分解为切线族，证明该过程等价于勒让德变换。这种方法能够处理任意形状、大小和位置的反射面，同时保留波传播的物理本质。

Result: 通过有限元模拟和空中实验验证，模型在预测反射时达到毫米级精度。为弯曲光束通信和传感系统提供了基础。

Conclusion: 该模型为未来弯曲光束通信和传感系统奠定了基础，能够设计反射弯曲链路和弯曲雷达路径，解决了传统光线反射方法在弯曲光束反射预测中的局限性。

Abstract: Curving beams are a promising new method for bypassing obstacles in future millimeter-wave to sub-terahertz (sub-THz) networks but lack a general predictive model for their reflections from arbitrary surfaces. We show that, unfortunately, attempting to "mirror" the incident beam trajectory across the normal of the reflector, as in ray optics, fails in general. Thus, we introduce the first geometric framework capable of modeling the reflections of arbitrary convex sub-THz curving beams from general reflectors with experimental verification. Rather than "mirroring" the trajectory, we decompose the beam into a family of tangents and demonstrate that this process is equivalent to the Legendre transform. This approach allows us to accurately account for reflectors of any shape, size, and position while preserving the underlying physics of wave propagation. Our model is validated through finite element method simulations and over-the-air experiments, demonstrating millimeter-scale accuracy in predicting reflections. Our model provides a foundation for future curving beam communication and sensing systems, enabling the design of reflected curved links and curving radar paths.

</details>


### [8] [Cooperative Differential GNSS Positioning: Estimators and Bounds](https://arxiv.org/abs/2601.06012)
*Helena Calatrava,Daniel Medina,Pau Closas*

Main category: eess.SP

TL;DR: 该论文研究如何通过大规模用户合作来减轻传统DGNSS系统中参考站噪声的影响，提出了合作DGNSS（C-DGNSS）和合作RTK（C-RTK）的统一估计框架，并分析了网络规模、卫星几何和参考站噪声对估计性能的影响。


<details>
  <summary>Details</summary>
Motivation: 在DGNSS定位中，用户与参考站之间的差分测量虽然抑制了共模误差，但引入了参考站噪声，这在使用混合质量参考基础设施时成为显著限制因素。论文旨在探索用户合作如何减轻这种噪声影响。

Method: 开发了合作DGNSS（C-DGNSS）和合作RTK（C-RTK）定位的统一估计框架，推导了费舍尔信息矩阵的参数化表达式，作为网络规模、卫星几何和参考站噪声的函数，从而实现对估计性能的理论分析。

Result: 理论分析确定了合作可以渐进恢复理想（无噪声）参考站DGNSS精度的机制，仿真验证了这些理论发现，表明大规模用户合作能有效减轻参考站噪声的影响。

Conclusion: 大规模用户合作能够有效减轻传统DGNSS系统中参考站噪声的影响，在特定条件下可以渐进恢复理想参考站的定位精度，为混合质量参考基础设施下的高精度定位提供了新途径。

Abstract: In Differential GNSS (DGNSS) positioning, differencing measurements between a user and a reference station suppresses common-mode errors but also introduces reference-station noise, which fundamentally limits accuracy. This limitation is minor for high-grade stations but becomes significant when using reference infrastructure of mixed quality. This paper investigates how large-scale user cooperation can mitigate the impact of reference-station noise in conventional (non-cooperative) DGNSS systems. We develop a unified estimation framework for cooperative DGNSS (C-DGNSS) and cooperative real-time kinematic (C-RTK) positioning, and derive parameterized expressions for their Fisher information matrices as functions of network size, satellite geometry, and reference-station noise. This formulation enables theoretical analysis of estimation performance, identifying regimes where cooperation asymptotically restores the accuracy of DGNSS with an ideal (noise-free) reference. Simulations validate these theoretical findings.

</details>


### [9] [Rapid Adaptation of SpO2 Estimation to Wearable Devices via Transfer Learning on Low-Sampling-Rate PPG](https://arxiv.org/abs/2509.12515)
*Zequan Liang,Ruoyu Zhang,Wei Shao,krishna Karthik,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun*

Main category: eess.SP

TL;DR: 提出基于迁移学习的框架，使用25Hz双通道PPG信号在可穿戴设备上实现低功耗血氧饱和度估计，无需临床校准


<details>
  <summary>Details</summary>
Motivation: 传统血氧饱和度估计方法依赖复杂的临床校准，不适合低功耗可穿戴应用，需要开发无需校准的快速适应方法

Method: 使用迁移学习框架：先在公共临床数据集上预训练带自注意力的BiLSTM模型，然后用可穿戴设备收集的数据和FDA批准的参考脉搏血氧仪数据进行微调

Result: 在公共数据集上MAE为2.967%，私有数据集上为2.624%，显著优于传统校准和非迁移机器学习基线；25Hz PPG相比100Hz降低40%功耗；瞬时预测MAE为3.284%

Conclusion: 该方法实现了在可穿戴设备上快速适应准确、低功耗的血氧监测，无需临床校准，有效捕捉快速波动

Abstract: Blood oxygen saturation (SpO2) is a vital marker for healthcare monitoring. Traditional SpO2 estimation methods often rely on complex clinical calibration, making them unsuitable for low-power, wearable applications. In this paper, we propose a transfer learning-based framework for the rapid adaptation of SpO2 estimation to energy-efficient wearable devices using low-sampling-rate (25Hz) dual-channel photoplethysmography (PPG). We first pretrain a bidirectional Long Short-Term Memory (BiLSTM) model with self-attention on a public clinical dataset, then fine-tune it using data collected from our wearable We-Be band and an FDA-approved reference pulse oximeter. Experimental results show that our approach achieves a mean absolute error (MAE) of 2.967% on the public dataset and 2.624% on the private dataset, significantly outperforming traditional calibration and non-transferred machine learning baselines. Moreover, using 25Hz PPG reduces power consumption by 40% compared to 100Hz, excluding baseline draw. Our method also attains an MAE of 3.284% in instantaneous SpO2 prediction, effectively capturing rapid fluctuations. These results demonstrate the rapid adaptation of accurate, low-power SpO2 monitoring on wearable devices without the need for clinical calibration.

</details>


### [10] [Generalizable Blood Pressure Estimation from Multi-Wavelength PPG Using Curriculum-Adversarial Learning](https://arxiv.org/abs/2509.12518)
*Zequan Liang,Ruoyu Zhang,Wei Shao,Mahdi Pirayesh Shirazi Nejad,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun*

Main category: eess.SP

TL;DR: 提出基于课程对抗学习的通用血压估计框架，结合多波长PPG数据，在严格受试者级数据划分下实现准确血压预测


<details>
  <summary>Details</summary>
Motivation: 准确且可泛化的血压估计对心血管疾病早期检测和管理至关重要，现有方法在受试者级泛化方面存在挑战

Method: 采用课程对抗学习框架：1) 课程学习从高血压分类过渡到血压回归；2) 领域对抗训练混淆受试者身份以学习受试者不变特征；3) 多通道融合利用多波长PPG互补信息

Result: 在四波长PPG数据集上，严格受试者级划分下获得SBP MAE 14.2mmHg，DBP MAE 6.4mmHg，多通道融合优于单通道模型，消融研究验证课程和对抗组件的有效性

Conclusion: 多波长PPG的互补信息与课程对抗策略相结合，为准确稳健的血压估计提供了有前景的解决方案，具有临床应用的潜力

Abstract: Accurate and generalizable blood pressure (BP) estimation is vital for the early detection and management of cardiovascular diseases. In this study, we enforce subject-level data splitting on a public multi-wavelength photoplethysmography (PPG) dataset and propose a generalizable BP estimation framework based on curriculum-adversarial learning. Our approach combines curriculum learning, which transitions from hypertension classification to BP regression, with domain-adversarial training that confuses subject identity to encourage the learning of subject-invariant features. Experiments show that multi-channel fusion consistently outperforms single-channel models. On the four-wavelength PPG dataset, our method achieves strong performance under strict subject-level splitting, with mean absolute errors (MAE) of 14.2mmHg for systolic blood pressure (SBP) and 6.4mmHg for diastolic blood pressure (DBP). Additionally, ablation studies validate the effectiveness of both the curriculum and adversarial components. These results highlight the potential of leveraging complementary information in multi-wavelength PPG and curriculum-adversarial strategies for accurate and robust BP estimation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [MoEBlaze: Breaking the Memory Wall for Efficient MoE Training on Modern GPUs](https://arxiv.org/abs/2601.05296)
*Jiyuan Zhang,Yining Liu,Siqi Yan,Lisen Deng,Jennifer Cao,Shuqi Yang,Min Ni,Bi Xue,Shen Li*

Main category: cs.LG

TL;DR: MoEBlaze是一个内存高效的MoE训练框架，通过协同设计的系统方法解决MoE架构中的内存瓶颈问题，实现4倍加速和50%内存节省。


<details>
  <summary>Details</summary>
Motivation: 现代大规模MoE架构中普遍存在的"内存墙"瓶颈被显著放大。MoE固有的架构稀疏性导致稀疏算术计算，同时引入大量激活内存开销，包括大型令牌路由缓冲区和需要物化缓冲的中间张量。这种内存压力限制了GPU上可容纳的最大批处理大小和序列长度，并导致过多的数据移动，阻碍性能和高效模型扩展。

Method: MoEBlaze采用协同设计的系统方法：(1) 端到端的令牌分发和MoE训练方法，使用优化的数据结构消除中间缓冲区和激活物化；(2) 协同设计的内核与智能激活检查点，在减少内存占用的同时实现更好的性能。

Result: MoEBlaze相比现有MoE框架可实现超过4倍的加速和超过50%的内存节省。

Conclusion: MoEBlaze通过协同设计的系统方法有效解决了MoE训练中的内存瓶颈问题，显著提升了训练效率和可扩展性。

Abstract: The pervasive "memory wall" bottleneck is significantly amplified in modern large-scale Mixture-of-Experts (MoE) architectures. MoE's inherent architectural sparsity leads to sparse arithmetic compute and also introduces substantial activation memory overheads -- driven by large token routing buffers and the need to materialize and buffer intermediate tensors. This memory pressure limits the maximum batch size and sequence length that can fit on GPUs, and also results in excessive data movements that hinders performance and efficient model scaling. We present MoEBlaze, a memory-efficient MoE training framework that addresses these issues through a co-designed system approach: (i) an end-to-end token dispatch and MoE training method with optimized data structures to eliminate intermediate buffers and activation materializing, and (ii) co-designed kernels with smart activation checkpoint to mitigate memory footprint while simultaneously achieving better performance. We demonstrate that MoEBlaze can achieve over 4x speedups and over 50% memory savings compared to existing MoE frameworks.

</details>


### [12] [TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning](https://arxiv.org/abs/2601.05300)
*Susmit Das*

Main category: cs.LG

TL;DR: TIME框架让对话模型能够根据上下文和时间线索进行简短、即时的推理，而非总是显示冗长的全局推理轨迹，大幅减少推理token并提升时间感知能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的推理设计存在三个主要问题：1）总是显示冗长的全局推理轨迹，成本高昂且影响可审计性；2）一旦开始输出内容就无法重新触发显式推理；3）对话模型缺乏时间感知能力，无法区分不同时间间隔的回复。

Method: 提出TIME框架，通过引入ISO 8601时间标签、表示静默间隔的"tick turns"和可出现在回复中任何位置的简短<think>块。采用四阶段课程训练，包括小规模、最大多样性的全批次对齐步骤，训练Qwen3模型进行简短、即时的推理爆发。

Result: 在4B到32B不同规模的模型上，TIME在TIMEBench基准测试中（涵盖时间顺序、间隔常识、异常检测和连续性）的表现均优于基础Qwen3模型，同时将推理token减少约一个数量级。

Conclusion: TIME框架成功实现了上下文敏感的时间感知推理，显著提升了对话模型的时间理解能力，同时大幅降低了推理成本，为更高效、更自然的对话系统提供了新方向。

Abstract: Reasoning oriented large language models often expose explicit "thinking" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at https://github.com/The-Coherence-Initiative/TIME and TIMEBench is available at https://github.com/The-Coherence-Initiative/TIMEBench

</details>


### [13] [Ontology Neural Networks for Topologically Conditioned Constraint Satisfaction](https://arxiv.org/abs/2601.05304)
*Jaehong Oh*

Main category: cs.LG

TL;DR: 提出增强的神经符号推理框架，结合拓扑条件与梯度稳定机制，在保持语义连贯性的同时满足物理和逻辑约束，在20节点问题中实现95%约束满足率和显著能量降低。


<details>
  <summary>Details</summary>
Motivation: 神经符号推理系统在保持语义连贯性的同时满足物理和逻辑约束方面面临根本性挑战。基于先前Ontology Neural Networks的工作，需要开发能够整合拓扑信息并稳定梯度优化的增强框架。

Method: 集成拓扑条件与梯度稳定机制：1) 使用Forman-Ricci曲率捕捉图拓扑结构；2) 采用Deep Delta Learning实现约束投影中的稳定秩一扰动；3) 应用协方差矩阵自适应进化策略进行参数优化。

Result: 实验评估显示：平均能量降低至1.15（基线为11.68），约束满足任务成功率95%。框架表现出种子无关收敛性，在20节点问题中具有优雅的扩展行为。

Conclusion: 拓扑结构可以在不牺牲可解释性或计算效率的情况下指导基于梯度的优化，该框架为神经符号推理提供了有效的解决方案。

Abstract: Neuro-symbolic reasoning systems face fundamental challenges in maintaining semantic coherence while satisfying physical and logical constraints. Building upon our previous work on Ontology Neural Networks, we present an enhanced framework that integrates topological conditioning with gradient stabilization mechanisms. The approach employs Forman-Ricci curvature to capture graph topology, Deep Delta Learning for stable rank-one perturbations during constraint projection, and Covariance Matrix Adaptation Evolution Strategy for parameter optimization. Experimental evaluation across multiple problem sizes demonstrates that the method achieves mean energy reduction to 1.15 compared to baseline values of 11.68, with 95 percent success rate in constraint satisfaction tasks. The framework exhibits seed-independent convergence and graceful scaling behavior up to twenty-node problems, suggesting that topological structure can inform gradient-based optimization without sacrificing interpretability or computational efficiency.

</details>


### [14] [When the Server Steps In: Calibrated Updates for Fair Federated Learning](https://arxiv.org/abs/2601.05352)
*Tianrun Yu,Kaixiang Zhao,Cheng Zhang,Anjun Gao,Yueyang Quan,Zhuqing Liu,Minghong Fang*

Main category: cs.LG

TL;DR: EquFL是一种新颖的服务器端去偏方法，通过生成校准更新来减少联邦学习中的偏见，无需修改客户端训练协议。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然具有分布式学习的优势，但在确保不同人口群体间的公平性方面面临挑战。现有的公平性去偏方法要么需要修改客户端训练协议，要么缺乏聚合策略的灵活性。

Method: EquFL是一种服务器端去偏方法，服务器在接收客户端模型更新后生成单个校准更新，然后将该校准更新与聚合的客户端更新结合，生成减少偏见的调整后全局模型。

Result: 理论上证明EquFL能收敛到FedAvg达到的最优全局模型，并有效减少训练轮次中的公平性损失。实证表明EquFL显著减轻了系统中的偏见。

Conclusion: EquFL提供了一种有效且灵活的服务器端去偏方法，无需修改客户端协议，在联邦学习中实现了更好的公平性。

Abstract: Federated learning (FL) has emerged as a transformative distributed learning paradigm, enabling multiple clients to collaboratively train a global model under the coordination of a central server without sharing their raw training data. While FL offers notable advantages, it faces critical challenges in ensuring fairness across diverse demographic groups. To address these fairness concerns, various fairness-aware debiasing methods have been proposed. However, many of these approaches either require modifications to clients' training protocols or lack flexibility in their aggregation strategies. In this work, we address these limitations by introducing EquFL, a novel server-side debiasing method designed to mitigate bias in FL systems. EquFL operates by allowing the server to generate a single calibrated update after receiving model updates from the clients. This calibrated update is then integrated with the aggregated client updates to produce an adjusted global model that reduces bias. Theoretically, we establish that EquFL converges to the optimal global model achieved by FedAvg and effectively reduces fairness loss over training rounds. Empirically, we demonstrate that EquFL significantly mitigates bias within the system, showcasing its practical effectiveness.

</details>


### [15] [GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting](https://arxiv.org/abs/2601.05353)
*Shovito Barua Soumma,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GlyRAG：基于LLM的上下文感知检索增强框架，仅使用CGM数据实现血糖预测，无需额外传感器，显著提升预测准确性和临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有血糖预测模型要么忽略上下文信息，要么依赖难以大规模收集的额外传感器。LLM在时间序列预测中展现出潜力，但在糖尿病护理中作为上下文提取代理的作用尚未充分探索。

Method: 提出GlyRAG框架：1）使用LLM作为上下文提取代理生成临床总结；2）语言模型嵌入总结并与基于补丁的血糖表示融合；3）检索模块在嵌入空间识别相似历史事件；4）通过交叉注意力整合案例类比进行预测。

Result: 在两个T1D队列上评估：1）比SOTA方法降低39% RMSE，比基线进一步降低1.7%；2）85%预测位于安全区域；3）预测血糖异常事件提升51%准确率。

Conclusion: LLM驱动的上下文提取和检索机制能够仅基于CGM数据显著提升长期血糖预测的准确性和临床可靠性，为未来糖尿病管理的智能决策支持工具奠定基础。

Abstract: Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a multimodal transformer architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.

</details>


### [16] [The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection](https://arxiv.org/abs/2601.05371)
*Md Shafiqul Islam,Shakti Prasad Padhy,Douglas Allaire,Raymundo Arróyave*

Main category: cs.LG

TL;DR: 提出基于核几何的贝叶斯优化框架，通过多维缩放将离散核库嵌入连续欧氏流形，实现高效的核选择，在合成基准、时间序列和增材制造案例中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高斯过程回归的性能严重依赖于协方差核的选择，但核选择是概率建模中最具挑战性和计算成本最高的步骤之一，需要一种更高效的核搜索方法。

Method: 基于核几何的贝叶斯优化框架：1）使用期望散度距离度量高斯过程先验之间的差异；2）通过多维缩放将离散核库嵌入连续欧氏流形；3）将核组合作为输入空间，对数边际似然作为目标函数，MDS坐标作为特征化表示。

Result: 在合成基准测试、真实世界时间序列数据集和增材制造案例研究中，该方法在预测精度和不确定性校准方面优于基线方法，包括基于大型语言模型的搜索方法。

Conclusion: 该框架为核搜索建立了可重用的概率几何结构，对高斯过程建模和深度核学习具有直接相关性，提供了一种高效稳定的核选择方法。

Abstract: Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.

</details>


### [17] [Inverting Non-Injective Functions with Twin Neural Network Regression](https://arxiv.org/abs/2601.05378)
*Sebastian J. Wetzel*

Main category: cs.LG

TL;DR: 提出了一种使用孪生神经网络回归和k近邻搜索的确定性框架，用于求解非单射函数的逆问题，即使输入输出维度不匹配也能找到首选解。


<details>
  <summary>Details</summary>
Motivation: 非单射函数不可逆，但可以通过限制到局部单射的子域来实现可逆性。即使维度不匹配，通常也能从多个可能解中选择一个首选解。需要一种系统方法来处理非单射函数的逆问题。

Method: 结合孪生神经网络回归和k近邻搜索的确定性框架。孪生神经网络训练用于预测已知输入变量x^anchor的调整量，以估计未知x^new，对应目标变量从y^anchor到y^new的变化。通过k近邻搜索找到给定目标变量的输入参数。

Result: 该方法成功反演了玩具问题和机器人手臂控制中的非单射函数，这些函数既可以是数据定义的，也可以是数学公式已知的。

Conclusion: 孪生神经网络回归与k近邻搜索相结合，为求解非单射函数的逆问题提供了一个有效的确定性框架，能够处理数据定义和数学公式描述的函数。

Abstract: Non-injective functions are not invertible. However, non-injective functions can be restricted to sub-domains on which they are locally injective and surjective and thus invertible if the dimensionality between input and output spaces are the same. Further, even if the dimensionalities do not match it is often possible to choose a preferred solution from many possible solutions. Twin neural network regression is naturally capable of incorporating these properties to invert non-injective functions. Twin neural network regression is trained to predict adjustments to well known input variables $\mathbf{x}^{\text{anchor}}$ to obtain an estimate for an unknown $\mathbf{x}^{\text{new}}$ under a change of the target variable from $\mathbf{y}^{\text{anchor}}$ to $\mathbf{y}^{\text{new}}$. In combination with k-nearest neighbor search, I propose a deterministic framework that finds input parameters to a given target variable of non-injective functions. The method is demonstrated by inverting non-injective functions describing toy problems and robot arm control that are a) defined by data or b) known as mathematical formula.

</details>


### [18] [Poisson Hyperplane Processes with Rectified Linear Units](https://arxiv.org/abs/2601.05586)
*Shufei Ge,Shijia Wang,Lloyd Elliott*

Main category: cs.LG

TL;DR: 该论文建立了泊松超平面过程与两层ReLU神经网络之间的理论联系，提出了一种基于PHP的可扩展神经网络模型，并通过退火顺序蒙特卡洛算法进行贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 虽然ReLU神经网络在各种分类和回归任务中表现出色，但研究者希望建立更理论化的连接，探索泊松超平面过程作为两层ReLU神经网络的概率表示，以提供更好的理论基础和可扩展性。

Method: 1. 建立泊松超平面过程与两层ReLU神经网络的数学联系；2. 证明具有高斯先验的PHP是两层ReLU神经网络的替代概率表示；3. 通过分解命题实现PHP构建的神经网络的大规模问题可扩展性；4. 提出退火顺序蒙特卡洛算法进行贝叶斯推断。

Result: 数值实验表明，提出的方法在性能上优于经典的两层ReLU神经网络，证明了PHP表示的有效性和优越性。

Conclusion: 泊松超平面过程为两层ReLU神经网络提供了坚实的理论基础和概率表示框架，提出的可扩展模型和贝叶斯推断算法在实际应用中表现出更好的性能。

Abstract: Neural networks have shown state-of-the-art performances in various classification and regression tasks. Rectified linear units (ReLU) are often used as activation functions for the hidden layers in a neural network model. In this article, we establish the connection between the Poisson hyperplane processes (PHP) and two-layer ReLU neural networks. We show that the PHP with a Gaussian prior is an alternative probabilistic representation to a two-layer ReLU neural network. In addition, we show that a two-layer neural network constructed by PHP is scalable to large-scale problems via the decomposition propositions. Finally, we propose an annealed sequential Monte Carlo algorithm for Bayesian inference. Our numerical experiments demonstrate that our proposed method outperforms the classic two-layer ReLU neural network. The implementation of our proposed model is available at https://github.com/ShufeiGe/Pois_Relu.git.

</details>


### [19] [Imitation Learning for Combinatorial Optimisation under Uncertainty](https://arxiv.org/abs/2601.05383)
*Prakash Gawas,Antoine Legrain,Louis-Martin Rousseau*

Main category: cs.LG

TL;DR: 该论文提出了模仿学习在组合优化中的专家分类框架，包含不确定性处理、最优性水平和交互模式三个维度，并开发了支持多专家查询的广义DAgger算法，在动态医生分配问题上验证了随机专家优于确定性专家。


<details>
  <summary>Details</summary>
Motivation: 模仿学习为大规模组合优化问题提供了数据驱动的解决方案框架，但现有研究对专家构造缺乏统一框架，无法系统分析不同专家类型的建模假设、计算特性和学习性能影响。

Method: 提出了组合优化中模仿学习专家的三维分类法：1)不确定性处理（近视、确定性、全信息、两阶段随机、多阶段随机）；2)最优性水平（任务最优vs近似）；3)交互模式（一次性监督到迭代交互）。基于此开发了支持多专家查询、专家聚合和灵活交互策略的广义DAgger算法。

Result: 在动态医生-患者分配问题上的实验表明：从随机专家学习的策略始终优于从确定性或全信息专家学习的策略；交互式学习能用更少的专家演示获得更高质量的解；当随机优化计算困难时，聚合的确定性专家提供了有效替代方案。

Conclusion: 该研究为组合优化中的模仿学习提供了系统的专家分类框架，证明了随机专家和交互式学习的优势，为实际应用中专家选择提供了指导，特别是在计算资源受限时聚合确定性专家可作为有效替代。

Abstract: Imitation learning (IL) provides a data-driven framework for approximating policies for large-scale combinatorial optimisation problems formulated as sequential decision problems (SDPs), where exact solution methods are computationally intractable. A central but underexplored aspect of IL in this context is the role of the \emph{expert} that generates training demonstrations. Existing studies employ a wide range of expert constructions, yet lack a unifying framework to characterise their modelling assumptions, computational properties, and impact on learning performance.
  This paper introduces a systematic taxonomy of experts for IL in combinatorial optimisation under uncertainty. Experts are classified along three dimensions: (i) their treatment of uncertainty, including myopic, deterministic, full-information, two-stage stochastic, and multi-stage stochastic formulations; (ii) their level of optimality, distinguishing task-optimal and approximate experts; and (iii) their interaction mode with the learner, ranging from one-shot supervision to iterative, interactive schemes. Building on this taxonomy, we propose a generalised Dataset Aggregation (DAgger) algorithm that supports multiple expert queries, expert aggregation, and flexible interaction strategies.
  The proposed framework is evaluated on a dynamic physician-to-patient assignment problem with stochastic arrivals and capacity constraints. Computational experiments compare learning outcomes across expert types and interaction regimes. The results show that policies learned from stochastic experts consistently outperform those learned from deterministic or full-information experts, while interactive learning improves solution quality using fewer expert demonstrations. Aggregated deterministic experts provide an effective alternative when stochastic optimisation becomes computationally challenging.

</details>


### [20] [Auditing Fairness under Model Updates: Fundamental Complexity and Property-Preserving Updates](https://arxiv.org/abs/2601.05909)
*Ayoub Ajarra,Debabrota Basu*

Main category: cs.LG

TL;DR: 提出一个通用框架用于在模型自适应更新下进行PAC审计，针对群体公平性等审计目标，引入SP维度来刻画可允许战略更新的复杂度


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器学习模型会自适应更新，这改变了底层模型类别但可能保持某些审计属性不变，需要研究在这种变化下如何进行可靠的公平性审计

Method: 提出基于经验属性优化（EPO）oracle的通用PAC审计框架，针对统计公平性引入SP维度这一组合度量来刻画可允许战略更新的复杂度

Result: 建立了分布无关的审计边界，由SP维度表征，展示了框架可扩展到预测误差和鲁棒风险等其他审计目标

Conclusion: 该框架为模型自适应更新下的公平性审计提供了理论基础和实用方法，SP维度为理解战略更新的复杂度提供了新视角

Abstract: As machine learning models become increasingly embedded in societal infrastructure, auditing them for bias is of growing importance. However, in real-world deployments, auditing is complicated by the fact that model owners may adaptively update their models in response to changing environments, such as financial markets. These updates can alter the underlying model class while preserving certain properties of interest, raising fundamental questions about what can be reliably audited under such shifts.
  In this work, we study group fairness auditing under arbitrary updates. We consider general shifts that modify the pre-audit model class while maintaining invariance of the audited property. Our goals are two-fold: (i) to characterize the information complexity of allowable updates, by identifying which strategic changes preserve the property under audit; and (ii) to efficiently estimate auditing properties, such as group fairness, using a minimal number of labeled samples.
  We propose a generic framework for PAC auditing based on an Empirical Property Optimization (EPO) oracle. For statistical parity, we establish distribution-free auditing bounds characterized by the SP dimension, a novel combinatorial measure that captures the complexity of admissible strategic updates. Finally, we demonstrate that our framework naturally extends to other auditing objectives, including prediction error and robust risk.

</details>


### [21] [DynaSTy: A Framework for SpatioTemporal Node Attribute Prediction in Dynamic Graphs](https://arxiv.org/abs/2601.05391)
*Namrata Banerji,Tanya Berger-Wolf*

Main category: cs.LG

TL;DR: 提出一种动态图上的端到端时空模型，通过可适应注意力偏置处理动态邻接矩阵，实现多步节点属性预测


<details>
  <summary>Details</summary>
Motivation: 现有时空图神经网络通常假设静态邻接矩阵，无法有效处理动态图（如金融网络、生物网络、社交系统）中节点属性的多步预测问题

Method: 基于Transformer的端到端模型，将动态邻接矩阵作为可适应注意力偏置注入，采用掩码节点-时间预训练目标、计划采样和水平加权损失来缓解长期预测中的误差累积

Result: 在RMSE和MAE指标上持续优于强基线方法，能够处理跨样本变化的动态图（如不同受试者的大脑网络、不同情境的金融系统）

Conclusion: 提出的动态边偏置时空模型能够有效处理动态图上的多步节点属性预测，在多种实际应用场景中表现出优越性能

Abstract: Accurate multistep forecasting of node-level attributes on dynamic graphs is critical for applications ranging from financial trust networks to biological networks. Existing spatiotemporal graph neural networks typically assume a static adjacency matrix. In this work, we propose an end-to-end dynamic edge-biased spatiotemporal model that ingests a multi-dimensional timeseries of node attributes and a timeseries of adjacency matrices, to predict multiple future steps of node attributes. At each time step, our transformer-based model injects the given adjacency as an adaptable attention bias, allowing the model to focus on relevant neighbors as the graph evolves. We further deploy a masked node-time pretraining objective that primes the encoder to reconstruct missing features, and train with scheduled sampling and a horizon-weighted loss to mitigate compounding error over long horizons. Unlike prior work, our model accommodates dynamic graphs that vary across input samples, enabling forecasting in multi-system settings such as brain networks across different subjects, financial systems in different contexts, or evolving social systems. Empirical results demonstrate that our method consistently outperforms strong baselines on Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).

</details>


### [22] [Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.05407)
*Minwoo Cho,Batuhan Altundas,Matthew Gombolay*

Main category: cs.LG

TL;DR: HINT是一个用于多智能体强化学习的知识蒸馏框架，通过分层交互教师机制解决传统KD在MARL中的瓶颈问题，在复杂合作任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏在多智能体强化学习中面临三个主要瓶颈：1) 在复杂领域中难以合成高性能教学策略；2) 教师需要在分布外状态进行推理；3) 分散学生与集中教师的观测空间不匹配。

Method: 提出HINT框架，采用分层强化学习构建可扩展的高性能教师策略，引入伪离策略RL使教师能够同时利用教师和学生经验进行更新以改善OOD适应，并应用基于性能的过滤保留结果相关的指导以减少观测不匹配。

Result: 在FireCommander资源分配和MARINE战术战斗等挑战性合作领域进行评估，HINT显著优于基线方法，成功率提升60%到165%。

Conclusion: HINT通过分层交互教师机制有效解决了MARL中知识蒸馏的关键瓶颈，为复杂多智能体系统的加速训练提供了有效解决方案。

Abstract: Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.

</details>


### [23] [Efficient Inference for Noisy LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2601.05420)
*Yiqun T Chen,Sizhu Lu,Sijia Li,Moran Guo,Shengyi Li*

Main category: cs.LG

TL;DR: 本文系统研究LLM作为评估者时的偏差校正方法，比较测量误差校正和预测驱动推断两种方法，推导高效影响函数并分析方差特性。


<details>
  <summary>Details</summary>
Motivation: LLM作为自动评估者存在系统性非随机误差，需要有效的偏差校正方法来提高评估的准确性。

Method: 基于半参数效率理论，推导高效影响函数基础的高效估计器，统一测量误差校正和预测驱动推断两类方法，分析方差特性。

Result: 理论分析表明预测驱动推断方法在某些条件下能达到比测量误差校正更小的渐近方差，仿真和实际数据验证了理论结果。

Conclusion: 系统比较了LLM评估偏差校正方法，提供了理论框架和实现工具，为实际应用中的方法选择提供指导。

Abstract: Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.

</details>


### [24] [Prediction of Fault Slip Tendency in CO${_2}$ Storage using Data-space Inversion](https://arxiv.org/abs/2601.05431)
*Xiaowen He,Su Jiang,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 使用变分自编码器（VAE）的数据空间反演（DSI）框架，预测CO₂封存项目中的压力、应力、应变场和断层滑移倾向，无需生成后验地质模型。


<details>
  <summary>Details</summary>
Motivation: 在涉及断层的耦合流动-地质力学问题中，传统的基于模型的历史匹配方法难以应用，需要一种更有效的方法来准确评估断层滑移潜力。

Method: 开发VAE-DSI框架：1）生成O(1000)个先验地质模型；2）使用GEOS进行耦合流动-地质力学模拟；3）训练具有堆叠卷积LSTM层的VAE，将压力、应变、有效正应力和剪应力场表示为潜变量；4）利用监测井的压力和应变观测数据进行DSI后验预测。

Result: 对合成真实模型的后验结果表明，DSI-VAE框架能够准确预测压力、应变、应力场和断层滑移倾向，同时显著减少了关键地质力学和断层参数的不确定性。

Conclusion: VAE-DSI框架为CO₂封存项目中的断层滑移风险评估提供了一种高效准确的方法，避免了传统后验地质模型生成的复杂性，直接利用先验模拟结果和观测数据进行预测。

Abstract: Accurately assessing the potential for fault slip is essential in many subsurface operations. Conventional model-based history matching methods, which entail the generation of posterior geomodels calibrated to observed data, can be challenging to apply in coupled flow-geomechanics problems with faults. In this work, we implement a variational autoencoder (VAE)-based data-space inversion (DSI) framework to predict pressure, stress and strain fields, and fault slip tendency, in CO${_2}$ storage projects. The main computations required by the DSI workflow entail the simulation of O(1000) prior geomodels. The posterior distributions for quantities of interest are then inferred directly from prior simulation results and observed data, without the need to generate posterior geomodels. The model used here involves a synthetic 3D system with two faults. Realizations of heterogeneous permeability and porosity fields are generated using geostatistical software, and uncertain geomechanical and fault parameters are sampled for each realization from prior distributions. Coupled flow-geomechanics simulations for these geomodels are conducted using GEOS. A VAE with stacked convolutional long short-term memory layers is trained, using the prior simulation results, to represent pressure, strain, effective normal stress and shear stress fields in terms of latent variables. The VAE parameterization is used with DSI for posterior predictions, with monitoring wells providing observed pressure and strain data. Posterior results for synthetic true models demonstrate that the DSI-VAE framework gives accurate predictions for pressure, strain, and stress fields and for fault slip tendency. The framework is also shown to reduce uncertainty in key geomechanical and fault parameters.

</details>


### [25] [RingSQL: Generating Synthetic Data with Schema-Independent Templates for Text-to-SQL Reasoning Models](https://arxiv.org/abs/2601.05451)
*Marko Sterbentz,Kevin Cushing,Cameron Barrie,Kristian J. Hammond*

Main category: cs.LG

TL;DR: RingSQL是一个结合模式无关查询模板和LLM自然语言问题转述的混合数据生成框架，用于解决文本到SQL训练数据稀缺问题，在六个基准测试中平均准确率提升2.3%。


<details>
  <summary>Details</summary>
Motivation: 文本到SQL系统的进展受限于高质量训练数据的稀缺性。手动创建数据成本高，现有合成方法在可靠性和可扩展性之间存在权衡：基于模板的方法能确保SQL正确但需要特定模式模板，而基于LLM的生成方法易于扩展但缺乏质量和正确性保证。

Method: RingSQL采用混合数据生成框架，结合模式无关的查询模板与基于LLM的自然语言问题转述。这种方法在保持跨不同模式SQL正确性的同时，提供了广泛的语言多样性。

Result: 实验表明，使用RingSQL生成的数据训练的模型，在六个文本到SQL基准测试中，相比使用其他合成数据训练的模型，平均准确率提升了2.3%。

Conclusion: RingSQL框架通过结合模板方法的正确性保证和LLM方法的可扩展性，有效解决了文本到SQL训练数据生成的质量与规模平衡问题，显著提升了模型性能。

Abstract: Recent advances in text-to-SQL systems have been driven by larger models and improved datasets, yet progress is still limited by the scarcity of high-quality training data. Manual data creation is expensive, and existing synthetic methods trade off reliability and scalability. Template-based approaches ensure correct SQL but require schema-specific templates, while LLM-based generation scales easily but lacks quality and correctness guarantees. We introduce RingSQL, a hybrid data generation framework that combines schema-independent query templates with LLM-based paraphrasing of natural language questions. This approach preserves SQL correctness across diverse schemas while providing broad linguistic variety. In our experiments, we find that models trained using data produced by RingSQL achieve an average gain in accuracy of +2.3% across six text-to-SQL benchmarks when compared to models trained on other synthetic data. We make our code available at https://github.com/nu-c3lab/RingSQL.

</details>


### [26] [Efficient Differentiable Causal Discovery via Reliable Super-Structure Learning](https://arxiv.org/abs/2601.05474)
*Pingchuan Ma,Qixin Zhang,Shuai Wang,Dacheng Tao*

Main category: cs.LG

TL;DR: ALVGL是一种新颖的可微因果发现增强方法，通过稀疏低秩分解学习数据精度矩阵，构建包含真实因果图的超结构，从而提高优化效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的可微因果发现方法在处理高维数据或存在潜在混杂因素的数据时面临挑战，包括搜索空间巨大、目标函数复杂以及图论约束难以处理。虽然利用超结构指导优化过程受到关注，但学习适当粒度的超结构在不同设置下效率低下。

Method: ALVGL采用稀疏低秩分解学习数据精度矩阵，设计ADMM优化过程识别与底层因果结构最相关的成分，构建可证明包含真实因果图的超结构，然后用该超结构初始化标准可微因果发现方法以缩小搜索空间。

Result: 在合成和真实数据集上的广泛实验表明，ALVGL不仅达到了最先进的准确性，还显著提高了优化效率，在包含高斯和非高斯设置、有无未测量混杂因素的各种结构因果模型中均表现出色。

Conclusion: ALVGL为可微因果发现提供了一个可靠有效的解决方案，通过构建合适的超结构来指导优化过程，显著改善了现有方法在高维和复杂场景下的性能。

Abstract: Recently, differentiable causal discovery has emerged as a promising approach to improve the accuracy and efficiency of existing methods. However, when applied to high-dimensional data or data with latent confounders, these methods, often based on off-the-shelf continuous optimization algorithms, struggle with the vast search space, the complexity of the objective function, and the nontrivial nature of graph-theoretical constraints. As a result, there has been a surge of interest in leveraging super-structures to guide the optimization process. Nonetheless, learning an appropriate super-structure at the right level of granularity, and doing so efficiently across various settings, presents significant challenges.
  In this paper, we propose ALVGL, a novel and general enhancement to the differentiable causal discovery pipeline. ALVGL employs a sparse and low-rank decomposition to learn the precision matrix of the data. We design an ADMM procedure to optimize this decomposition, identifying components in the precision matrix that are most relevant to the underlying causal structure. These components are then combined to construct a super-structure that is provably a superset of the true causal graph. This super-structure is used to initialize a standard differentiable causal discovery method with a more focused search space, thereby improving both optimization efficiency and accuracy.
  We demonstrate the versatility of ALVGL by instantiating it across a range of structural causal models, including both Gaussian and non-Gaussian settings, with and without unmeasured confounders. Extensive experiments on synthetic and real-world datasets show that ALVGL not only achieves state-of-the-art accuracy but also significantly improves optimization efficiency, making it a reliable and effective solution for differentiable causal discovery.

</details>


### [27] [MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization](https://arxiv.org/abs/2601.05475)
*Jiefu Ou,Sapana Chaudhary,Kaj Bostrom,Nathaniel Weir,Shuai Zhang,Huzefa Rangwala,George Karypis*

Main category: cs.LG

TL;DR: MaxCode：基于最大奖励强化学习框架的统一搜索方法，通过执行反馈和自然语言诊断提升LLM代码优化能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用编码任务中表现出色，但在代码优化方面面临两大挑战：1）编写优化代码（如高性能CUDA内核和竞赛级CPU代码）需要系统、算法和特定语言的专业知识；2）需要解释性能指标（如时间和设备利用率），而不仅仅是二进制正确性。

Method: 提出MaxCode框架，统一现有搜索方法于最大奖励强化学习框架下。通过自然语言批判模型将原始执行反馈转换为诊断洞察，提供更丰富的输入；训练生成式奖励到目标模型，使用rollout中的动作值对潜在解决方案进行重新排序，改善搜索探索。

Result: 在KernelBench（CUDA）和PIE（C++）优化基准测试中，MaxCode相比基线方法在优化代码性能方面有显著提升，绝对加速值相对改进20.3%，相对加速排名改进10.1%。

Conclusion: MaxCode通过统一的强化学习框架和增强的观察空间，有效提升了LLM在代码优化任务中的性能，为解决复杂代码优化问题提供了有效方法。

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.

</details>


### [28] [Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection](https://arxiv.org/abs/2601.05501)
*Feihu Jin,Ying Tan*

Main category: cs.LG

TL;DR: 提出Hi-ZFO分层优化框架，结合一阶梯度精确性和零阶方法探索性，通过分层重要性分析自适应分配优化策略，提升LLM微调效果并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 标准一阶优化方法容易使LLM微调陷入尖锐、泛化性差的局部最小值，而零阶方法虽然探索性强但收敛慢且方差大，特别是在生成任务中输出空间巨大导致估计方差显著放大。

Method: 提出Hi-ZFO分层混合优化框架：1) 通过层重要性分析自适应划分模型；2) 对关键层使用精确的一阶梯度更新；3) 对不敏感层使用零阶优化，引入"有益随机性"帮助逃离局部最小值；4) 零阶方法不仅是内存节省替代，而是作为探索机制。

Result: 在多种生成、数学和代码推理任务上验证，Hi-ZFO始终获得优越性能，同时显著减少训练时间，证明分层混合优化对LLM微调的有效性。

Conclusion: 分层混合优化框架Hi-ZFO成功结合了一阶梯度的精确性和零阶方法的探索能力，为LLM微调提供了一种高效且有效的优化策略。

Abstract: Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \textbf{Hi-ZFO} (\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of "beneficial stochasticity" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.

</details>


### [29] [Over-Searching in Search-Augmented Large Language Models](https://arxiv.org/abs/2601.05503)
*Roy Xie,Deepak Gopinath,David Qiu,Dong Lin,Haitian Sun,Saloni Potdar,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 该论文系统评估了搜索增强LLM中的过度搜索问题，提出了TPC指标来衡量性能-成本权衡，并发布了OverSearchQA数据集来促进高效搜索增强LLM的研究。


<details>
  <summary>Details</summary>
Motivation: 搜索增强的大型语言模型在知识密集型任务中表现出色，但存在过度搜索问题——即使搜索不会提高响应质量也会不必要地调用搜索工具，这导致计算效率低下和因引入无关上下文而产生的幻觉。

Method: 1) 在多维度上系统评估过度搜索，包括查询类型、模型类别、检索条件和多轮对话；2) 引入Tokens Per Correctness (TPC)指标来量化过度搜索；3) 在查询和检索层面研究缓解方法；4) 发布OverSearchQA数据集。

Result: 研究发现：1) 搜索通常能提高可回答查询的准确性，但会损害不可回答查询的弃权能力；2) 过度搜索在复杂推理模型和深度研究系统中更明显，受噪声检索加剧，并在多轮对话中累积；3) 检索证据的组成很关键，负面证据的存在能改善弃权能力。

Conclusion: 该研究系统揭示了搜索增强LLM中的过度搜索问题，提出了量化指标和缓解方法，为开发更高效的搜索增强LLM提供了重要见解和基准数据集。

Abstract: Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.

</details>


### [30] [Toward an Integrated Cross-Urban Accident Prevention System: A Multi-Task Spatial-Temporal Learning Framework for Urban Safety Management](https://arxiv.org/abs/2601.05521)
*Jiayu Fang,Zhiqi Shao,Haoning Xi,Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: 提出MLA-STNet统一系统，通过多任务学习解决跨城市事故风险预测问题，整合STG-MA和STS-MA模块，在纽约和芝加哥数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 城市事故数据存在异质性、报告不一致、稀疏、周期性、噪声等问题，加上碎片化治理和不兼容的报告标准，阻碍了跨城市事故预防框架的建立。

Method: 提出MLA-STNet系统，包含两个互补模块：STG-MA抑制时空波动并增强长期依赖；STS-MA通过共享参数设计缓解跨城市异质性，同时保留个体语义表示空间。

Result: 在纽约和芝加哥数据集上进行75个实验，在全天和高频事故时段两种场景下，MLA-STNet相比SOTA基线实现RMSE降低6%、Recall提高8%、MAP提高5%，在50%输入噪声下性能变化小于1%。

Conclusion: MLA-STNet有效统一了异构城市数据，构建了可扩展、鲁棒且可解释的跨城市事故预防系统，为协调的数据驱动城市安全管理铺平了道路。

Abstract: The development of a cross-city accident prevention system is particularly challenging due to the heterogeneity, inconsistent reporting, and inherently clustered, sparse, cyclical, and noisy nature of urban accident data. These intrinsic data properties, combined with fragmented governance and incompatible reporting standards, have long hindered the creation of an integrated, cross-city accident prevention framework. To address this gap, we propose the Mamba Local-ttention Spatial-Temporal Network MLA-STNet, a unified system that formulates accident risk prediction as a multi-task learning problem across multiple cities. MLA-STNet integrates two complementary modules: (i)the Spatio-Temporal Geographical Mamba-Attention (STG-MA), which suppresses unstable spatio-temporal fluctuations and strengthens long-range temporal dependencies; and (ii) the Spatio-Temporal Semantic Mamba-Attention (STS-MA), which mitigates cross-city heterogeneity through a shared-parameter design that jointly trains all cities while preserving individual semantic representation spaces. We validate the proposed framework through 75 experiments under two forecasting scenarios, full-day and high-frequency accident periods, using real-world datasets from New York City and Chicago. Compared with the state-of-the-art baselines, MLA-STNet achieves up to 6% lower RMSE, 8% higher Recall, and 5% higher MAP, while maintaining less than 1% performance variation under 50% input noise. These results demonstrate that MLA-STNet effectively unifies heterogeneous urban datasets within a scalable, robust, and interpretable Cross-City Accident Prevention System, paving the way for coordinated and data-driven urban safety management.

</details>


### [31] [DeMa: Dual-Path Delay-Aware Mamba for Efficient Multivariate Time Series Analysis](https://arxiv.org/abs/2601.05527)
*Rui An,Haohao Qu,Wenqi Fan,Xuequn Shang,Qing Li*

Main category: cs.LG

TL;DR: DeMa：一种双路径延迟感知Mamba骨干网络，通过分离时间动态和变量交互，在保持线性复杂度的同时提升多元时间序列分析性能


<details>
  <summary>Details</summary>
Motivation: Transformer在多元时间序列分析中存在二次计算复杂度和高内存开销问题，而现有Mamba模型直接应用于MTS存在三个关键限制：缺乏显式跨变量建模、难以解耦时间动态与变量交互、对潜在时滞交互效应建模不足

Method: 提出DeMa双路径延迟感知Mamba骨干网络：1）将MTS分解为时间动态和变量交互；2）时间路径使用Mamba-SSD模块捕获单序列长程动态，支持并行计算；3）变量路径使用Mamba-DALA模块集成延迟感知线性注意力建模跨变量依赖

Result: 在五个代表性任务（长短期预测、数据补全、异常检测、序列分类）上实现最先进性能，同时保持卓越的计算效率

Conclusion: DeMa在保持Mamba线性复杂度优势的同时，显著提升了多元时间序列建模的适用性，为大规模长期MTS分析提供了高效解决方案

Abstract: Accurate and efficient multivariate time series (MTS) analysis is increasingly critical for a wide range of intelligent applications. Within this realm, Transformers have emerged as the predominant architecture due to their strong ability to capture pairwise dependencies. However, Transformer-based models suffer from quadratic computational complexity and high memory overhead, limiting their scalability and practical deployment in long-term and large-scale MTS modeling. Recently, Mamba has emerged as a promising linear-time alternative with high expressiveness. Nevertheless, directly applying vanilla Mamba to MTS remains suboptimal due to three key limitations: (i) the lack of explicit cross-variate modeling, (ii) difficulty in disentangling the entangled intra-series temporal dynamics and inter-series interactions, and (iii) insufficient modeling of latent time-lag interaction effects. These issues constrain its effectiveness across diverse MTS tasks. To address these challenges, we propose DeMa, a dual-path delay-aware Mamba backbone. DeMa preserves Mamba's linear-complexity advantage while substantially improving its suitability for MTS settings. Specifically, DeMa introduces three key innovations: (i) it decomposes the MTS into intra-series temporal dynamics and inter-series interactions; (ii) it develops a temporal path with a Mamba-SSD module to capture long-range dynamics within each individual series, enabling series-independent, parallel computation; and (iii) it designs a variate path with a Mamba-DALA module that integrates delay-aware linear attention to model cross-variate dependencies. Extensive experiments on five representative tasks, long- and short-term forecasting, data imputation, anomaly detection, and series classification, demonstrate that DeMa achieves state-of-the-art performance while delivering remarkable computational efficiency.

</details>


### [32] [Scalable Heterogeneous Graph Learning via Heterogeneous-aware Orthogonal Prototype Experts](https://arxiv.org/abs/2601.05537)
*Wei Zhou,Hong Huang,Ruize Shi,Bang Liu*

Main category: cs.LG

TL;DR: 论文提出HOPE框架解决异质图神经网络中的线性投影瓶颈问题，通过原型专家混合机制改善长尾分布下的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有HGNNs主要改进编码器，但解码/投影阶段仍使用单一共享线性头，无法处理异质图中的上下文多样性和长尾分布问题，导致过拟合中心节点、忽略尾部节点。

Method: 提出HOPE框架：1）基于可学习原型的路由机制，根据相似度将实例分配给专家；2）专家正交化促进多样性并防止专家坍缩；3）作为即插即用模块替换标准预测头。

Result: 在四个真实数据集上的实验表明，HOPE能持续提升多种SOTA HGNN骨干网络的性能，且额外开销很小。

Conclusion: HOPE有效解决了异质图神经网络中的线性投影瓶颈问题，通过原型专家混合机制适应自然长尾分布，提升模型在多样化上下文中的表现。

Abstract: Heterogeneous Graph Neural Networks(HGNNs) have advanced mainly through better encoders, yet their decoding/projection stage still relies on a single shared linear head, assuming it can map rich node embeddings to labels. We call this the Linear Projection Bottleneck: in heterogeneous graphs, contextual diversity and long-tail shifts make a global head miss fine semantics, overfit hub nodes, and underserve tail nodes. While Mixture-of-Experts(MoE) could help, naively applying it clashes with structural imbalance and risks expert collapse. We propose a Heterogeneous-aware Orthogonal Prototype Experts framework named HOPE, a plug-and-play replacement for the standard prediction head. HOPE uses learnable prototype-based routing to assign instances to experts by similarity, letting expert usage follow the natural long-tail distribution, and adds expert orthogonalization to encourage diversity and prevent collapse. Experiments on four real datasets show consistent gains across SOTA HGNN backbones with minimal overhead.

</details>


### [33] [Buffered AUC maximization for scoring systems via mixed-integer optimization](https://arxiv.org/abs/2601.05544)
*Moe Shiina,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: 提出基于混合整数线性优化的评分系统构建方法，直接最大化缓冲AUC来提升分类性能


<details>
  <summary>Details</summary>
Motivation: 现有评分系统构建方法未直接优化AUC这一关键评估指标，需要开发能直接最大化AUC的优化框架

Method: 建立混合整数线性优化模型，直接最大化缓冲AUC作为AUC的最紧凹下界，并加入组稀疏约束限制问题数量

Result: 在公开真实数据集上的实验表明，该方法相比基于正则化和逐步回归的基线方法能构建出具有更高AUC值的评分系统

Conclusion: 该研究推动了混合整数优化技术在开发高可解释性分类模型方面的进展

Abstract: A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies have used mixed-integer optimization (MIO) techniques to develop scoring systems for binary classification; however, they have not focused on directly maximizing AUC (i.e., area under the receiver operating characteristic curve), even though AUC is recognized as an essential evaluation metric for scoring systems. Our goal herein is to establish an effective MIO framework for constructing scoring systems that directly maximize the buffered AUC (bAUC) as the tightest concave lower bound on AUC. Our optimization model is formulated as a mixed-integer linear optimization (MILO) problem that maximizes bAUC subject to a group sparsity constraint for limiting the number of questions in the scoring system. Computational experiments using publicly available real-world datasets demonstrate that our MILO method can build scoring systems with superior AUC values compared to the baseline methods based on regularization and stepwise regression. This research contributes to the advancement of MIO techniques for developing highly interpretable classification models.

</details>


### [34] [Learn to Evolve: Self-supervised Neural JKO Operator for Wasserstein Gradient Flow](https://arxiv.org/abs/2601.05583)
*Xue Feng,Li Wang,Deanna Needell,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出自监督学习方法学习JKO解算子，无需数值求解JKO轨迹，通过交替学习算子和生成轨迹来高效计算Wasserstein梯度流


<details>
  <summary>Details</summary>
Motivation: JKO方案为计算Wasserstein梯度流提供了稳定的变分框架，但实际应用受到重复求解JKO子问题的高计算成本限制

Method: 提出Learn-to-Evolve算法，联合学习JKO算子及其诱导轨迹，通过轨迹生成和算子更新交替进行，仅需少量初始密度进行训练

Result: 数值实验表明该方法在各种能量函数和初始条件下具有准确性、稳定性和鲁棒性，生成的数据逐渐逼近真实JKO轨迹

Conclusion: 该方法通过自监督学习和数据增强策略，显著提高了学习算子的泛化能力，为高效计算Wasserstein梯度流提供了新途径

Abstract: The Jordan-Kinderlehrer-Otto (JKO) scheme provides a stable variational framework for computing Wasserstein gradient flows, but its practical use is often limited by the high computational cost of repeatedly solving the JKO subproblems. We propose a self-supervised approach for learning a JKO solution operator without requiring numerical solutions of any JKO trajectories. The learned operator maps an input density directly to the minimizer of the corresponding JKO subproblem, and can be iteratively applied to efficiently generate the gradient-flow evolution. A key challenge is that only a number of initial densities are typically available for training. To address this, we introduce a Learn-to-Evolve algorithm that jointly learns the JKO operator and its induced trajectories by alternating between trajectory generation and operator updates. As training progresses, the generated data increasingly approximates true JKO trajectories. Meanwhile, this Learn-to-Evolve strategy serves as a natural form of data augmentation, significantly enhancing the generalization ability of the learned operator. Numerical experiments demonstrate the accuracy, stability, and robustness of the proposed method across various choices of energies and initial conditions.

</details>


### [35] [PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning](https://arxiv.org/abs/2601.05593)
*Jingcheng Hu,Yinmin Zhang,Shijie Shang,Xiaobo Yang,Yue Peng,Zhewei Huang,Hebin Zhou,Xin Wu,Jie Cheng,Fanqi Wan,Xiangwen Kong,Chengyuan Yao,Kaiwen Yan,Ailin Huang,Hongyu Zhou,Qi Han,Zheng Ge,Daxin Jiang,Xiangyu Zhang,Heung-Yeung Shum*

Main category: cs.LG

TL;DR: PaCoRe是一个训练和推理框架，通过并行探索和消息传递机制，使语言模型能够大幅扩展测试时计算量，突破传统顺序推理的上下文窗口限制，在数学推理等任务上超越前沿系统。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型存在核心限制：无法在固定上下文窗口下大幅扩展测试时计算量，只能进行顺序推理。这限制了模型在复杂任务上的表现，特别是在需要大量计算推理的领域如数学问题求解。

Method: 提出并行协调推理框架：1）采用消息传递架构进行多轮并行探索；2）每轮启动多个并行推理轨迹；3）将发现压缩为上下文有界的消息；4）合成这些消息指导下一轮并最终生成答案；5）使用大规模基于结果的强化学习进行端到端训练。

Result: PaCoRe在多个领域带来显著改进：在数学推理上表现突出，一个8B参数模型在HMMT 2025上达到94.5%，超越GPT-5的93.2%，有效测试时计算量扩展到约200万token而不超过上下文限制。

Conclusion: PaCoRe成功突破了语言模型测试时计算量扩展的限制，通过并行协调推理实现了多轮大规模计算，在数学推理等复杂任务上超越了前沿系统，为后续研究提供了开源模型、训练数据和完整推理管道。

Abstract: We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.

</details>


### [36] [Good Allocations from Bad Estimates](https://arxiv.org/abs/2601.05597)
*Sílvia Casacuberta,Moritz Hardt*

Main category: cs.LG

TL;DR: 本文提出了一种比传统CATE估计更高效的样本分配方法，仅需O(M/ε)样本即可达到与CATE相同的总体治疗效果，而传统方法需要O(M/ε²)样本


<details>
  <summary>Details</summary>
Motivation: 条件平均治疗效果(CATE)估计是异质人群治疗分配的金标准，但需要大量样本(O(M/ε²))来准确估计所有治疗效果。本研究旨在探索治疗分配是否比治疗效果估计需要更少的样本，以实现更高效的资源利用

Method: 提出了一种基于粗粒度估计的治疗分配算法，关键洞察是：对于接近最优的治疗分配，粗略的估计就足够了。算法还利用了预算灵活性进一步降低样本复杂度

Result: 算法仅需O(M/ε)样本即可达到与CATE相同的总体治疗效果，比传统方法减少了一个数量级。在多个真实世界RCT数据集上的评估显示，该算法能以极少的样本找到接近最优的治疗分配方案

Conclusion: 研究强调了治疗效果估计与治疗分配之间的根本区别：后者需要的样本量远少于前者。粗粒度估计足以实现接近最优的治疗分配，这为资源受限环境下的精准医疗提供了更高效的解决方案

Abstract: Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $ε> 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/ε^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $ε$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/ε)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.

</details>


### [37] [Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR](https://arxiv.org/abs/2601.05607)
*Zijun Min,Bingshuai Liu,Ante Wang,Long Zhang,Anxiang Zeng,Haibo Zhang,Jinsong Su*

Main category: cs.LG

TL;DR: DHPO提出了一种动态混合策略优化方法，结合了GRPO的token级重要性比率和GSPO的序列级重要性比率，通过分支特定裁剪策略稳定训练，在数学推理任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法在粒度上各有优缺点：GRPO使用token级重要性比率能保留细粒度信用分配但方差高不稳定；GSPO使用序列级重要性比率匹配序列级奖励但牺牲了token级信用分配。需要结合两者优势。

Method: 提出动态混合策略优化(DHPO)，在单一裁剪代理目标中结合token级和序列级重要性比率。探索了平均混合和熵引导混合两种变体，并采用分支特定裁剪策略分别约束两个分支的重要性比率，防止异常值主导更新。

Result: 在七个具有挑战性的数学推理基准测试中，对Qwen3系列的密集模型和MoE模型进行实验，DHPO始终优于GRPO和GSPO。

Conclusion: DHPO成功结合了GRPO和GSPO的优势，通过动态混合token级和序列级重要性比率以及分支特定裁剪策略，在保持稳定性的同时实现了更好的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.

</details>


### [38] [PiXTime: A Model for Federated Time Series Forecasting with Heterogeneous Data Structures Across Nodes](https://arxiv.org/abs/2601.05613)
*Yiming Zhou,Mingyue Cheng,Hao Wang,Enhong Chen*

Main category: cs.LG

TL;DR: PiXTime：面向联邦学习的时间序列预测模型，通过个性化补丁嵌入和全局变量表解决多粒度异构变量问题


<details>
  <summary>Details</summary>
Motivation: 时间序列数据价值高但难以跨节点共享，不同节点的采样标准导致时间粒度和变量集存在差异，阻碍了传统联邦学习的应用

Method: 采用个性化补丁嵌入将节点特定粒度的时间序列映射到统一维度的token序列；使用全局变量表对齐跨节点的变量类别语义；基于Transformer的共享模型捕获任意数量变量的辅助序列表示，并通过交叉注意力增强目标序列预测

Result: 在联邦学习设置中实现了最先进的性能，并在八个广泛使用的真实世界传统基准测试中表现出优越性能

Conclusion: PiXTime能够有效处理多粒度和异构变量集的时间序列预测问题，在联邦学习环境中具有显著优势

Abstract: Time series are highly valuable and rarely shareable across nodes, making federated learning a promising paradigm to leverage distributed temporal data. However, different sampling standards lead to diverse time granularities and variable sets across nodes, hindering classical federated learning. We propose PiXTime, a novel time series forecasting model designed for federated learning that enables effective prediction across nodes with multi-granularity and heterogeneous variable sets. PiXTime employs a personalized Patch Embedding to map node-specific granularity time series into token sequences of a unified dimension for processing by a subsequent shared model, and uses a global VE Table to align variable category semantics across nodes, thereby enhancing cross-node transferability. With a transformer-based shared model, PiXTime captures representations of auxiliary series with arbitrary numbers of variables and uses cross-attention to enhance the prediction of the target series. Experiments show PiXTime achieves state-of-the-art performance in federated settings and demonstrates superior performance on eight widely used real-world traditional benchmarks.

</details>


### [39] [Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks](https://arxiv.org/abs/2601.05616)
*ShaoZhen Liu,Xinting Huang,Houwen Peng,Xin Chen,Xinyang Song,Qi Li,Zhenan Sun*

Main category: cs.LG

TL;DR: 论文提出两阶段训练框架，通过自生成长链思维数据增强模型自我修正能力，在数学推理任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖强化学习框架，忽视了监督微调方法在激活模型内在推理能力方面的潜力。需要探索更高效的训练方法来提升大语言模型在复杂数学推理任务上的表现。

Method: 1. 第一阶段：采用多轮对话策略引导模型生成包含验证、回溯、子目标分解和逆向推理的思维链数据，通过预定义规则筛选高质量样本进行监督微调。
2. 第二阶段：使用难度感知拒绝采样机制动态优化数据分布，增强模型处理复杂问题的能力。该方法生成的推理链长度扩展超过4倍。

Result: 在GSM8K和MATH500等数学基准测试上表现提升，微调模型在AIME24等竞赛级问题上取得显著改进。证明监督微调能有效激活模型内在推理能力，为复杂任务优化提供资源高效路径。

Conclusion: 该两阶段训练框架通过自生成长链思维数据增强模型自我修正能力，展示了监督微调在激活大语言模型内在推理潜力方面的有效性，为复杂推理任务优化提供了可扩展且资源高效的解决方案。

Abstract: In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.

</details>


### [40] [Continual Learning of Achieving Forgetting-free and Positive Knowledge Transfer](https://arxiv.org/abs/2601.05623)
*Zhi Wang,Zhongbin Wu,Yanni Li,Bing Liu,Guangxi Li,Yuping Wang*

Main category: cs.LG

TL;DR: 提出ETCL方法解决持续学习中的正向知识迁移问题，通过任务特定掩码、梯度对齐和双目标优化实现无遗忘和正向前后向知识迁移


<details>
  <summary>Details</summary>
Motivation: 现有持续学习研究主要关注缓解灾难性遗忘，但理想的持续学习代理还应促进正向前向和反向知识迁移，即利用先前任务知识学习新任务（FKT）以及用新任务知识改进先前任务性能（BKT）

Method: 提出增强任务持续学习（ETCL）方法：1）学习任务特定二进制掩码为每个任务隔离稀疏子网络；2）新任务学习开始时，将新任务梯度与先前最相似任务的子网络梯度对齐以确保正向FKT；3）使用双目标优化策略和正交梯度投影方法，仅更新先前相似任务分类层权重以实现正向BKT；4）理论估计负向FKT和BKT的边界，并提出在线任务相似性检测策略

Result: 在不相似、相似和混合任务序列上，ETCL显著优于强基线方法，实现了无遗忘和正向知识迁移

Conclusion: ETCL方法通过建模持续学习为优化问题，结合任务特定掩码、梯度对齐和双目标优化，有效解决了灾难性遗忘问题并促进了正向前向和反向知识迁移，为持续学习提供了更全面的解决方案

Abstract: Existing research on continual learning (CL) of a sequence of tasks focuses mainly on dealing with catastrophic forgetting (CF) to balance the learning plasticity of new tasks and the memory stability of old tasks. However, an ideal CL agent should not only be able to overcome CF, but also encourage positive forward and backward knowledge transfer (KT), i.e., using the learned knowledge from previous tasks for the new task learning (namely FKT), and improving the previous tasks' performance with the knowledge of the new task (namely BKT). To this end, this paper first models CL as an optimization problem in which each sequential learning task aims to achieve its optimal performance under the constraint that both FKT and BKT should be positive. It then proposes a novel Enhanced Task Continual Learning (ETCL) method, which achieves forgetting-free and positive KT. Furthermore, the bounds that can lead to negative FKT and BKT are estimated theoretically. Based on the bounds, a new strategy for online task similarity detection is also proposed to facilitate positive KT. To overcome CF, ETCL learns a set of task-specific binary masks to isolate a sparse sub-network for each task while preserving the performance of a dense network for the task. At the beginning of a new task learning, ETCL tries to align the new task's gradient with that of the sub-network of the previous most similar task to ensure positive FKT. By using a new bi-objective optimization strategy and an orthogonal gradient projection method, ETCL updates only the weights of previous similar tasks at the classification layer to achieve positive BKT. Extensive evaluations demonstrate that the proposed ETCL markedly outperforms strong baselines on dissimilar, similar, and mixed task sequences.

</details>


### [41] [Transformer Is Inherently a Causal Learner](https://arxiv.org/abs/2601.05647)
*Xinyue Wang,Stephen Wang,Biwei Huang*

Main category: cs.LG

TL;DR: Transformer在自回归训练中自然学习到时延因果结构，其梯度敏感性可直接恢复因果图，无需显式因果目标


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在自回归训练中是否自然学习到因果结构，以及如何利用这种特性进行因果发现，避免传统方法需要显式因果约束的局限性

Method: 提出基于聚合梯度归因的因果结构提取方法，利用Transformer输出对过去输入的梯度敏感性来恢复底层因果图

Result: 在非线性动态、长期依赖和非平稳系统等挑战性场景中，该方法显著超越现有因果发现算法，特别是在数据异质性增加时表现更优，展现出随数据量和异质性增加而提升的扩展潜力

Conclusion: Transformer自回归训练自然编码时延因果结构，为因果发现提供了新范式，同时为基座模型通过因果视角获得可解释性和增强奠定了基础

Abstract: We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.

</details>


### [42] [From Global to Local: Cluster-Aware Learning for Wi-Fi Fingerprinting Indoor Localisation](https://arxiv.org/abs/2601.05650)
*Miguel Matey-Sanz,Joaquín Torres-Sospedra,Joaquín Huerta,Sergio Trilles*

Main category: cs.LG

TL;DR: 提出基于聚类的Wi-Fi指纹定位方法，通过空间或无线电特征对指纹数据集进行分组，在定位阶段仅使用相关聚类进行预测，减少定位误差。


<details>
  <summary>Details</summary>
Motivation: 传统Wi-Fi指纹定位面临数据集规模与异质性、RSSI信号波动、大型多楼层环境模糊性等挑战，导致定位精度下降，特别是全局模型未考虑结构约束时。

Method: 采用聚类方法在定位前结构化指纹数据集，基于空间或无线电特征进行分组，可在建筑或楼层级别应用聚类。定位阶段基于最强接入点的聚类估计程序将未见指纹分配到最相关聚类，仅在选定聚类内进行定位。

Result: 在三个公共数据集和多个机器学习模型上评估，结果显示定位误差持续减少，特别是在建筑级策略下，但代价是降低了楼层检测精度。

Conclusion: 通过聚类显式结构化数据集是扩展室内定位的有效灵活方法，能显著提升定位精度，尽管在楼层识别方面有所权衡。

Abstract: Wi-Fi fingerprinting remains one of the most practical solutions for indoor positioning, however, its performance is often limited by the size and heterogeneity of fingerprint datasets, strong Received Signal Strength Indicator variability, and the ambiguity introduced in large and multi-floor environments. These factors significantly degrade localisation accuracy, particularly when global models are applied without considering structural constraints. This paper introduces a clustering-based method that structures the fingerprint dataset prior to localisation. Fingerprints are grouped using either spatial or radio features, and clustering can be applied at the building or floor level. In the localisation phase, a clustering estimation procedure based on the strongest access points assigns unseen fingerprints to the most relevant cluster. Localisation is then performed only within the selected clusters, allowing learning models to operate on reduced and more coherent subsets of data. The effectiveness of the method is evaluated on three public datasets and several machine learning models. Results show a consistent reduction in localisation errors, particularly under building-level strategies, but at the cost of reducing the floor detection accuracy. These results demonstrate that explicitly structuring datasets through clustering is an effective and flexible approach for scalable indoor positioning.

</details>


### [43] [Do Sparse Autoencoders Identify Reasoning Features in Language Models?](https://arxiv.org/abs/2601.05679)
*George Ma,Zhongyuan Liang,Irene Y. Chen,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 研究发现，通过对比激活方法识别的稀疏自编码器特征主要捕捉的是推理的语言相关性特征，而非真正的推理计算过程。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证稀疏自编码器是否真正识别了大语言模型中的推理特征，而不是仅仅捕捉到与推理相关的表面语言模式。

Method: 采用证伪导向框架，结合因果标记注入实验和LLM引导的证伪方法，测试特征激活是否反映推理过程还是表面语言相关性。在20个配置中跨多个模型家族、层次和推理数据集进行分析。

Result: 1. 59%-94%的特征对标记级干预高度敏感，注入少量特征相关标记即可激活，表明依赖词汇伪影；2. 剩余特征通过LLM引导证伪发现，没有特征满足真正推理行为的标准；3. 引导这些特征对基准性能产生最小变化或轻微退化。

Conclusion: 通过对比方法识别的SAE特征主要捕捉推理的语言相关性特征，而非底层推理计算本身，对当前特征解释方法提出了质疑。

Abstract: We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that identified reasoning features are highly sensitive to token-level interventions. Injecting a small number of feature-associated tokens into non-reasoning text is sufficient to elicit strong activation for 59% to 94% of features, indicating reliance on lexical artifacts. For the remaining features that are not explained by simple token triggers, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields minimal changes or slight degradations in benchmark performance. Together, these results suggest that SAE features identified by contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves.

</details>


### [44] [AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces](https://arxiv.org/abs/2601.05680)
*Yeonsang Shin,Insoo Kim,Bongkeun Kim,Keonwoo Bae,Bohyung Han*

Main category: cs.LG

TL;DR: AGDC提出统一框架，联合建模离散和连续值，解决Transformer自回归模型在生成高精度混合序列时的精度限制问题，特别针对半导体电路设计等需要高精度的领域。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的自回归模型在数据生成方面表现出色，但受限于离散化token的依赖，难以高精度表示连续值。现有离散化方法在处理混合离散-连续序列（特别是半导体电路设计等高精度领域）时存在可扩展性限制，精度损失可能导致功能失效。

Method: 提出AGDC统一框架，采用混合方法：对离散值使用分类预测，对连续值使用基于扩散的建模。包含两个关键技术组件：1) 基于MLP的序列结束(EOS)logit调整机制，根据序列上下文动态调整EOS token的logits；2) 集成到损失函数中的长度正则化项。同时提出ContLayNet大规模基准数据集。

Result: 在半导体布局(ContLayNet)、图形布局和SVG数据集上的实验表明，AGDC在生成高保真混合向量表示方面优于基于离散化和固定模式的基线方法，实现了跨领域可扩展的高精度生成。

Conclusion: AGDC通过联合建模离散和连续值，解决了自回归模型在高精度混合序列生成中的限制，为半导体电路设计等高精度领域提供了有效的解决方案，并在多个领域展示了优越性能。

Abstract: Transformer-based autoregressive models excel in data generation but are inherently constrained by their reliance on discretized tokens, which limits their ability to represent continuous values with high precision. We analyze the scalability limitations of existing discretization-based approaches for generating hybrid discrete-continuous sequences, particularly in high-precision domains such as semiconductor circuit designs, where precision loss can lead to functional failure. To address the challenge, we propose AGDC, a novel unified framework that jointly models discrete and continuous values for variable-length sequences. AGDC employs a hybrid approach that combines categorical prediction for discrete values with diffusion-based modeling for continuous values, incorporating two key technical components: an end-of-sequence (EOS) logit adjustment mechanism that uses an MLP to dynamically adjust EOS token logits based on sequence context, and a length regularization term integrated into the loss function. Additionally, we present ContLayNet, a large-scale benchmark comprising 334K high-precision semiconductor layout samples with specialized evaluation metrics that capture functional correctness where precision errors significantly impact performance. Experiments on semiconductor layouts (ContLayNet), graphic layouts, and SVGs demonstrate AGDC's superior performance in generating high-fidelity hybrid vector representations compared to discretization-based and fixed-schema baselines, achieving scalable high-precision generation across diverse domains.

</details>


### [45] [FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching](https://arxiv.org/abs/2601.05684)
*Hongyaoxing Gul,Lijuan Hu,Shuzi Niu,Fangfang Liu*

Main category: cs.LG

TL;DR: FLRQ是一种灵活的低秩量化方法，通过快速识别最优秩并聚合实现最小存储组合，在量化质量和算法效率上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有低秩PTQ方法需要昂贵的微调来确定不同数据和层的最佳秩，且SVD低秩近似计算开销大，未能充分利用大模型的潜力。

Method: 提出FLRQ框架，包含两个核心组件：1) R1-FLR：使用R1-Sketch和高斯投影快速低秩近似，实现异常值感知的逐层秩提取；2) BLC：通过迭代方法在缩放和裁剪策略下最小化低秩量化误差。

Result: FLRQ在综合实验中表现出强大的有效性和鲁棒性，在量化质量和算法效率方面都达到了最先进的性能。

Conclusion: FLRQ通过灵活的低秩量化方法，解决了传统低秩PTQ需要昂贵微调和计算开销大的问题，为大模型的高效量化提供了有效解决方案。

Abstract: Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.

</details>


### [46] [mHC-lite: You Don't Need 20 Sinkhorn-Knopp Iterations](https://arxiv.org/abs/2601.05732)
*Yongyi Yang,Jianyang Gao*

Main category: cs.LG

TL;DR: mHC-lite通过将双随机矩阵重构为置换矩阵的凸组合，解决了mHC中Sinkhorn-Knopp归一化的近似误差和工程实现复杂性问题，保证了精确的双随机性并提高了训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有mHC方法存在两个主要问题：1) 有限次Sinkhorn-Knopp迭代无法保证精确的双随机性，近似误差会随网络深度累积影响稳定性；2) 高效的SK实现需要专门的CUDA内核，工程门槛高且可移植性差。

Method: 基于Birkhoff-von Neumann定理，提出mHC-lite方法，将双随机矩阵显式构造为置换矩阵的凸组合。这种方法通过构造保证精确的双随机性，且仅需原生矩阵运算即可实现。

Result: 大量实验表明，mHC-lite在性能上匹配或超越mHC，同时使用简单实现即可获得更高的训练吞吐量，并消除了HC和mHC中观察到的残差不稳定性问题。

Conclusion: mHC-lite提供了一种简单有效的解决方案，既保证了双随机矩阵的精确性，又降低了工程实现复杂度，在保持性能的同时提高了训练稳定性和效率。

Abstract: Hyper-Connections (HC) generalizes residual connections by introducing dynamic residual matrices that mix information across multiple residual streams, accelerating convergence in deep neural networks. However, unconstrained residual matrices can compromise training stability. To address this, DeepSeek's Manifold-Constrained Hyper-Connections (mHC) approximately projects these matrices onto the Birkhoff polytope via iterative Sinkhorn--Knopp (SK) normalization. We identify two limitations of this approach: (i) finite SK iterations do not guarantee exact doubly stochasticity, leaving an approximation gap that can accumulate through network depth and undermine stability; (ii) efficient SK implementation requires highly specialized CUDA kernels, raising engineering barriers and reducing portability. Motivated by the Birkhoff--von Neumann theorem, we propose mHC-lite, a simple reparameterization that explicitly constructs doubly stochastic matrices as convex combinations of permutation matrices. This approach guarantees exact doubly stochasticity by construction and can be implemented using only native matrix operations. Extensive experiments demonstrate that mHC-lite matches or exceeds mHC in performance while achieving higher training throughput with a naive implementation and eliminating the residual instabilities observed in both HC and mHC. The code is publicly available at https://github.com/FFTYYY/mhc-lite.

</details>


### [47] [Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms](https://arxiv.org/abs/2601.05759)
*Turkan Simge Ispak,Salih Tileylioglu,Erdem Akagunduz*

Main category: cs.LG

TL;DR: 该研究将P波检测重构为自监督异常检测任务，通过492种VAE配置实验发现：注意力机制优于跳跃连接，在近源区域（0-40km）AUC达0.91，更适合地震预警应用。


<details>
  <summary>Details</summary>
Motivation: 强震动记录中的P波检测面临高噪声、标记数据有限和复杂波形特征的挑战，传统方法效果有限，需要探索自监督学习方案。

Method: 将P波到达检测重构为自监督异常检测任务，通过492种变分自编码器（VAE）配置的网格搜索，比较跳跃连接和注意力机制对重建保真度与异常判别权衡的影响。

Result: 跳跃连接虽能最小化重建误差（MAE约0.0012），但会导致"过度泛化"，重建噪声而掩盖检测信号；注意力机制优先考虑全局上下文而非局部细节，获得最高检测性能（AUC 0.875），在0-40km近源范围AUC达0.91。

Conclusion: 偏向全局上下文而非像素级完美重建的架构约束对于鲁棒的自监督P波检测至关重要，注意力机制VAE在近源区域表现出色，非常适合即时预警应用。

Abstract: Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce "overgeneralization", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.

</details>


### [48] [Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer](https://arxiv.org/abs/2601.05770)
*Yifan Zhang,Wei Bi,Kechi Zhang,Dongming Jin,Jie Fu,Zhi Jin*

Main category: cs.LG

TL;DR: 提出Discrete Transformer架构，通过功能解耦和温度退火采样，从连续表示中提取可读程序，实现无演示算法发现


<details>
  <summary>Details</summary>
Motivation: Transformer在算法提取中存在特征叠加问题，连续表示与离散符号逻辑之间存在鸿沟，阻碍从训练模型中提取可执行程序

Method: 设计Discrete Transformer架构，强制功能解耦：数值注意力负责信息路由，数值MLP负责元素级算术运算，结合温度退火采样

Result: 性能与RNN基线相当，扩展到连续变量领域，退火过程显示从探索到利用的相变，可通过归纳偏置精细控制合成程序

Conclusion: Discrete Transformer为无演示算法发现提供了稳健框架，为Transformer可解释性提供了严格途径

Abstract: Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.

</details>


### [49] [Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning](https://arxiv.org/abs/2601.05792)
*Manel Gil-Sorribes,Júlia Vilalta-Mor,Isaac Filella-Mercè,Robert Soliva,Álvaro Ciudad,Víctor Guallar,Alexis Molina*

Main category: cs.LG

TL;DR: Tensor-DTI是一个用于药物-靶点相互作用预测的对比学习框架，整合了分子图、蛋白质语言模型和结合位点预测的多模态嵌入，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有药物-靶点相互作用预测模型通常依赖单模态预定义分子描述符或序列嵌入，表征能力有限，需要更全面的多模态信息整合来提高预测准确性。

Method: 采用对比学习框架和孪生双编码器架构，整合分子图、蛋白质语言模型和结合位点预测的多模态嵌入，捕捉化学和结构相互作用特征，区分相互作用与非相互作用对。

Result: 在多个DTI基准测试中优于现有序列和基于图的方法；在大规模CDK2化学库推理实验中，即使训练时排除CDK2，仍能产生化学合理的命中分布；在富集研究中与Glide对接和Boltz-2共折叠器保持竞争力。

Conclusion: 整合多模态信息与对比学习目标能显著提高相互作用预测准确性，为虚拟筛选提供更可解释和可靠性感知的模型，并可扩展到蛋白质-RNA和肽-蛋白质相互作用预测。

Abstract: Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.

</details>


### [50] [Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers](https://arxiv.org/abs/2601.05807)
*Mohamed Amine Hallam,Kuo-Kun Tseng*

Main category: cs.LG

TL;DR: 研究位置编码融合机制对Transformer性能的影响，发现融合策略在长序列任务中显著影响性能，而在短序列中影响可忽略


<details>
  <summary>Details</summary>
Motivation: 大多数现有研究专注于设计新的位置编码，而忽略了位置信息如何与词嵌入融合的机制。本文旨在探究融合机制本身是否影响性能，特别是在长序列场景下

Method: 在相同Transformer架构、数据划分和随机种子下，对比三种经典融合策略：逐元素加法、拼接投影和标量门控融合。在三个文本分类数据集上进行实验，涵盖短、中、长序列。进行配对种子分析和跨数据集比较，验证结果的稳定性。在ArXiv数据集上测试融合策略在不同位置编码家族中的泛化能力，并探索轻量级卷积门控机制

Result: 融合选择对短文本影响可忽略，但在长文档上产生一致的性能提升。配对种子分析确认这些提升是结构性的而非随机波动。可学习的融合机制在不同位置编码家族中都能泛化。轻量级卷积门控机制在长文档上表现出色

Conclusion: 位置编码融合是长序列Transformer中一个重要的设计选择，应被视为明确的建模决策而非固定默认设置

Abstract: Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.

</details>


### [51] [Learning Reconstructive Embeddings in Reproducing Kernel Hilbert Spaces via the Representer Theorem](https://arxiv.org/abs/2601.05811)
*Enrique Feito-Casares,Francisco M. Melgarejo-Meseguer,José-Luis Rojo-Álvarez*

Main category: cs.LG

TL;DR: 该论文提出了一种基于RKHS的表示学习方法，通过自表示重构和核对齐将高维数据的几何结构转移到低维嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 随着对揭示高维数据潜在结构的表示学习方法的兴趣日益增长，本文旨在开发新的重构式流形学习算法，利用再生核希尔伯特空间（RKHS）的理论框架。

Method: 1. 在RKHS中通过优化表示定理的向量形式，将每个观测重构为其他样本的线性组合；2. 使用可分离的算子值核扩展到向量值数据；3. 通过核对齐任务将数据投影到低维潜在空间，使其Gram矩阵匹配高维重构核。

Result: 在模拟数据集（同心圆和瑞士卷）和真实数据集（癌症分子活性和物联网网络入侵）上的数值实验提供了该方法的实际有效性证据。

Conclusion: 该方法通过利用和调整核学习理论的已知结果，扩展了自然数据中常见的自表示属性，为基于RKHS的表示学习提供了有效的算法框架。

Abstract: Motivated by the growing interest in representation learning approaches that uncover the latent structure of high-dimensional data, this work proposes new algorithms for reconstruction-based manifold learning within Reproducing-Kernel Hilbert Spaces (RKHS). Each observation is first reconstructed as a linear combination of the other samples in the RKHS, by optimizing a vector form of the Representer Theorem for their autorepresentation property. A separable operator-valued kernel extends the formulation to vector-valued data while retaining the simplicity of a single scalar similarity function. A subsequent kernel-alignment task projects the data into a lower-dimensional latent space whose Gram matrix aims to match the high-dimensional reconstruction kernel, thus transferring the auto-reconstruction geometry of the RKHS to the embedding. Therefore, the proposed algorithms represent an extended approach to the autorepresentation property, exhibited by many natural data, by using and adapting well-known results of Kernel Learning Theory. Numerical experiments on both simulated (concentric circles and swiss-roll) and real (cancer molecular activity and IoT network intrusions) datasets provide empirical evidence of the practical effectiveness of the proposed approach.

</details>


### [52] [Detecting Autism Spectrum Disorder with Deep Eye Movement Features](https://arxiv.org/abs/2601.05812)
*Zhanpei Huang,Taochen chen,Fangqing Gu,Yiqun Zhang*

Main category: cs.LG

TL;DR: 提出DSTS框架，利用眼动数据的离散短时序列特性进行自闭症谱系障碍检测，优于传统机器学习与深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 眼动数据作为非侵入性诊断工具，具有离散性和短时依赖特性，能反映自闭症患者的细微行为模式。然而，基于Transformer的全局注意力机制对此类数据效果有限，需要更适合局部时序模式的架构。

Method: 设计了离散短时序列建模框架，包含类别感知表示和失衡感知机制，以高效捕捉眼动数据的细微复杂模式，区分自闭症与正常发育个体。

Result: 在多个眼动数据集上的实验表明，DSTS框架优于传统机器学习技术和更复杂的深度学习模型。

Conclusion: 针对眼动数据的离散短时特性设计的专门建模框架能更有效地捕捉自闭症相关模式，为自闭症检测提供了更优的非侵入性诊断工具。

Abstract: Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by deficits in social communication and behavioral patterns. Eye movement data offers a non-invasive diagnostic tool for ASD detection, as it is inherently discrete and exhibits short-term temporal dependencies, reflecting localized gaze focus between fixation points. These characteristics enable the data to provide deeper insights into subtle behavioral markers, distinguishing ASD-related patterns from typical development. Eye movement signals mainly contain short-term and localized dependencies. However, despite the widespread application of stacked attention layers in Transformer-based models for capturing long-range dependencies, our experimental results indicate that this approach yields only limited benefits when applied to eye movement data. This may be because discrete fixation points and short-term dependencies in gaze focus reduce the utility of global attention mechanisms, making them less efficient than architectures focusing on local temporal patterns. To efficiently capture subtle and complex eye movement patterns, distinguishing ASD from typically developing (TD) individuals, a discrete short-term sequential (DSTS) modeling framework is designed with Class-aware Representation and Imbalance-aware Mechanisms. Through extensive experiments on several eye movement datasets, DSTS outperforms both traditional machine learning techniques and more sophisticated deep learning models.

</details>


### [53] [A Dual Pipeline Machine Learning Framework for Automated Multi Class Sleep Disorder Screening Using Hybrid Resampling and Ensemble Learning](https://arxiv.org/abs/2601.05814)
*Md Sultanul Islam Ovi,Muhsina Tarannum Munfa,Miftahul Alam Adib,Syed Sabbir Hasan*

Main category: cs.LG

TL;DR: 提出双管道机器学习框架用于睡眠障碍筛查，结合统计和包装器方法，在Sleep Health and Lifestyle数据集上达到98.67%准确率，推理延迟低于400毫秒。


<details>
  <summary>Details</summary>
Motivation: 临床睡眠研究资源密集且难以规模化，需要开发准确、高效的自动化筛查方法，以进行人群水平的睡眠障碍风险分层。

Method: 提出双管道机器学习框架：统计管道使用互信息和线性判别分析处理线性可分性；包装器管道使用Boruta特征选择和自编码器进行非线性表示学习。采用SMOTETomek混合重采样策略处理类别不平衡。

Result: Extra Trees和K最近邻算法达到98.67%准确率，优于相同数据集上的现有基准。Wilcoxon符号秩检验显示改进具有统计显著性，推理延迟保持在400毫秒以下。

Conclusion: 双管道设计支持准确高效的自动化睡眠障碍筛查，可用于非侵入性风险分层，具有临床实用价值。

Abstract: Accurate classification of sleep disorders, particularly insomnia and sleep apnea, is important for reducing long term health risks and improving patient quality of life. However, clinical sleep studies are resource intensive and are difficult to scale for population level screening. This paper presents a Dual Pipeline Machine Learning Framework for multi class sleep disorder screening using the Sleep Health and Lifestyle dataset. The framework consists of two parallel processing streams: a statistical pipeline that targets linear separability using Mutual Information and Linear Discriminant Analysis, and a wrapper based pipeline that applies Boruta feature selection with an autoencoder for non linear representation learning. To address class imbalance, we use the hybrid SMOTETomek resampling strategy. In experiments, Extra Trees and K Nearest Neighbors achieved an accuracy of 98.67%, outperforming recent baselines on the same dataset. Statistical testing using the Wilcoxon Signed Rank Test indicates that the improvement over baseline configurations is significant, and inference latency remains below 400 milliseconds. These results suggest that the proposed dual pipeline design supports accurate and efficient automated screening for non invasive sleep disorder risk stratification.

</details>


### [54] [A New Family of Poisson Non-negative Matrix Factorization Methods Using the Shifted Log Link](https://arxiv.org/abs/2601.05845)
*Eric Weine,Peter Carbonetto,Rafael A. Irizarry,Matthew Stephens*

Main category: cs.LG

TL;DR: 提出带移位对数链接函数的泊松非负矩阵分解，通过单一调参在加性和乘性组合之间灵活转换，改进计数数据的可解释性分解。


<details>
  <summary>Details</summary>
Motivation: 传统泊松NMF假设分解的"部分"是加性组合，这在某些场景下不自然。需要更灵活的模型来适应不同数据特性，提高分解结果的可解释性。

Method: 引入带移位对数链接函数的泊松NMF，通过单一调参控制组合方式从加性到乘性的连续变化。提出最大似然估计算法，并为大型稀疏数据集设计计算高效的近似方法。

Result: 在多个真实数据集上验证了新方法的有效性。结果表明链接函数的选择对分解结果有实质性影响，移位对数链接在某些情况下相比标准加性链接能提高可解释性。

Conclusion: 移位对数链接为泊松NMF提供了从加性到乘性组合的灵活建模框架，能根据数据特性调整分解方式，提升结果的可解释性，特别适用于大型稀疏数据集。

Abstract: Poisson non-negative matrix factorization (NMF) is a widely used method to find interpretable "parts-based" decompositions of count data. While many variants of Poisson NMF exist, existing methods assume that the "parts" in the decomposition combine additively. This assumption may be natural in some settings, but not in others. Here we introduce Poisson NMF with the shifted-log link function to relax this assumption. The shifted-log link function has a single tuning parameter, and as this parameter varies the model changes from assuming that parts combine additively (i.e., standard Poisson NMF) to assuming that parts combine more multiplicatively. We provide an algorithm to fit this model by maximum likelihood, and also an approximation that substantially reduces computation time for large, sparse datasets (computations scale with the number of non-zero entries in the data matrix). We illustrate these new methods on a variety of real datasets. Our examples show how the choice of link function in Poisson NMF can substantively impact the results, and how in some settings the use of a shifted-log link function may improve interpretability compared with the standard, additive link.

</details>


### [55] [IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck](https://arxiv.org/abs/2601.05870)
*Huilin Deng,Hongchen Luo,Yue Zhu,Long Li,Zhuoyue Chen,Xinghao Zhao,Ming Li,Jihai Zhang,Mengchang Wang,Yang Cao,Yu Kang*

Main category: cs.LG

TL;DR: 提出IIB-LPO方法，通过信息瓶颈原理在潜在空间进行策略优化，解决LLM推理中的探索崩溃问题，在数学推理基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在探索崩溃问题，随机rollout的语义同质性导致模型陷入狭窄的过优化行为。全局熵正则化容易导致奖励黑客攻击和无意义冗长，局部token选择性更新则受限于预训练模型的强归纳偏置。

Method: 提出IIB-LPO方法，将探索从token分布的统计扰动转移到推理轨迹的拓扑分支。在高熵状态触发潜在分支以多样化推理路径，并利用信息瓶颈原理作为轨迹过滤器和自奖励机制，确保简洁且信息丰富的探索。

Result: 在四个数学推理基准测试中，IIB-LPO实现了最先进的性能，在准确率上比先前方法高出5.3%，在多样性指标上高出7.4%。

Conclusion: IIB-LPO通过信息瓶颈驱动的潜在策略优化，有效解决了LLM推理中的探索崩溃问题，实现了更优的性能和多样性。

Abstract: Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.

</details>


### [56] [GlueNN: gluing patchwise analytic solutions with neural networks](https://arxiv.org/abs/2601.05889)
*Doyoung Kim,Donghee Lee,Hye-Sung Lee,Jiheon Lee,Jaeok Yi*

Main category: cs.LG

TL;DR: 提出一种学习框架，将渐近解析解的积分常数提升为尺度相关函数，通过约束这些系数函数来学习全局有效解，避免传统匹配方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 在物理和工程中，复杂微分方程常具有强尺度依赖性，难以获得精确解。传统方法将区域划分为多个区块并简化方程，然后通过匹配边界构造全局解，但这种方法可能在匹配边界附近失效，因为近似形式在该区域可能不成立。

Method: 提出一种学习框架，将渐近解析解的积分常数提升为尺度相关函数（系数函数）。通过在整个域上约束这些系数函数满足原始微分方程，网络学习到一个全局有效的解，平滑地在渐近区域之间插值，无需任意边界匹配。

Result: 在化学动力学和宇宙学的代表性问题上展示了该框架的有效性，能够准确再现全局解，并优于传统的匹配程序。

Conclusion: 该方法提供了一种新的解决方案，通过将渐近解析解的积分常数参数化为尺度相关函数，并利用机器学习约束这些函数，能够获得全局有效的解，避免了传统匹配方法的局限性，在复杂微分方程求解中具有应用潜力。

Abstract: In many problems in physics and engineering, one encounters complicated differential equations with strongly scale-dependent terms for which exact analytical or numerical solutions are not available. A common strategy is to divide the domain into several regions (patches) and simplify the equation in each region. When approximate analytic solutions can be obtained in each patch, they are then matched at the interfaces to construct a global solution. However, this patching procedure can fail to reproduce the correct solution, since the approximate forms may break down near the matching boundaries. In this work, we propose a learning framework in which the integration constants of asymptotic analytic solutions are promoted to scale-dependent functions. By constraining these coefficient functions with the original differential equation over the domain, the network learns a globally valid solution that smoothly interpolates between asymptotic regimes, eliminating the need for arbitrary boundary matching. We demonstrate the effectiveness of this framework in representative problems from chemical kinetics and cosmology, where it accurately reproduces global solutions and outperforms conventional matching procedures.

</details>


### [57] [Distilling Lightweight Domain Experts from Large ML Models by Identifying Relevant Subspaces](https://arxiv.org/abs/2601.05913)
*Pattarawat Chormai,Ali Hashemi,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: SubDistill是一种新的知识蒸馏算法，专注于仅蒸馏教师模型中与特定子任务相关的组件，在计算资源有限的环境中表现优于现有层间蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 实际应用中通常只需要蒸馏少数类别及其相关中间概念，但现有蒸馏方法很少明确关注相关子任务，这导致了效率低下和资源浪费的问题。

Method: SubDistill算法改进了数值特性，在每个层仅蒸馏教师模型的相关组件，而不是整个模型，从而专注于特定子任务的知识转移。

Result: 在CIFAR-100和ImageNet数据集上，使用卷积和Transformer模型的实验表明，SubDistill在代表性子任务集上优于现有层间蒸馏技术，且可解释AI分析显示学生模型更接近原始教师模型的决策结构。

Conclusion: SubDistill提供了一种高效的知识蒸馏方法，专注于相关子任务，在保持性能的同时减少了计算需求，为资源受限环境中的模型部署提供了实用解决方案。

Abstract: Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing distillation methods explicitly focus on the relevant subtask. To address this gap, we introduce 'SubDistill', a new distillation algorithm with improved numerical properties that only distills the relevant components of the teacher model at each layer. Experiments on CIFAR-100 and ImageNet with Convolutional and Transformer models demonstrate that SubDistill outperforms existing layer-wise distillation techniques on a representative set of subtasks. Our benchmark evaluations are complemented by Explainable AI analyses showing that our distilled student models more closely match the decision structure of the original teacher model.

</details>


### [58] [Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics](https://arxiv.org/abs/2601.05929)
*Sidney Shapiro,Burhanuddin Panvelwala*

Main category: cs.LG

TL;DR: 该研究评估了Meta开发的Prophet预测框架在可重复性方面的优势，通过对比ARIMA和随机森林，展示了Prophet在可解释性、标准化工作流程和可访问性方面的平衡，为Python研究环境提供了可重复预测的实用参考框架。


<details>
  <summary>Details</summary>
Motivation: 预测研究与实践中的可重复性问题持续存在，特别是在商业和金融分析等高风险决策领域。传统方法需要大量手动调参且难以在专有环境中复制，机器学习方法虽然灵活但存在可解释性和随机训练过程的问题。需要找到平衡可解释性、标准化和可访问性的解决方案。

Method: 研究评估Prophet框架作为可重复性解决方案，通过公开可用的金融和零售数据集，在受控且完全文档化的实验设计中，将Prophet的性能和可解释性与多种ARIMA规格（自动选择、手动指定和季节性变体）以及随机森林进行多模型比较。

Result: Prophet在可重复性方面表现出优势，其加法结构、开源实现和标准化工作流程有助于透明和可复制的预测实践。通过具体的Python示例展示了Prophet如何促进高效预测工作流程并与分析管道集成。

Conclusion: Prophet作为可重复研究的方法构建块，支持验证、可审计性和方法严谨性。该研究为研究人员和从业者提供了在Python研究工作流程中进行可重复预测的实用参考框架，强调了Prophet在平衡可解释性、标准化和可访问性方面的价值。

Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.

</details>


### [59] [On the Robustness of Age for Learning-Based Wireless Scheduling in Unknown Environments](https://arxiv.org/abs/2601.05956)
*Juaren Steiger,Bin Li*

Main category: cs.LG

TL;DR: 提出使用虚拟队列中最老数据包的年龄（head-of-line age）替代虚拟队列长度来设计学习调度策略，提高在信道条件突变时的系统稳定性


<details>
  <summary>Details</summary>
Motivation: 传统基于虚拟队列长度的约束组合多臂老虎机算法在信道条件突变时可能导致约束不可行，虚拟队列长度无限增长，系统不稳定

Method: 设计基于学习的调度策略，使用虚拟队列中最老数据包的年龄（head-of-line age）替代虚拟队列长度，提高对信道突变的鲁棒性

Result: 在独立同分布网络条件下性能与现有最优算法相当，在信道条件突变时系统保持稳定并能快速从约束不可行期恢复

Conclusion: 使用head-of-line age比虚拟队列长度更鲁棒，能有效处理无线网络中信道条件突变导致的约束不可行问题

Abstract: The constrained combinatorial multi-armed bandit model has been widely employed to solve problems in wireless networking and related areas, including the problem of wireless scheduling for throughput optimization under unknown channel conditions. Most work in this area uses an algorithm design strategy that combines a bandit learning algorithm with the virtual queue technique to track the throughput constraint violation. These algorithms seek to minimize the virtual queue length in their algorithm design. However, in networks where channel conditions change abruptly, the resulting constraints may become infeasible, leading to unbounded growth in virtual queue lengths. In this paper, we make the key observation that the dynamics of the head-of-line age, i.e. the age of the oldest packet in the virtual queue, make it more robust when used in algorithm design compared to the virtual queue length. We therefore design a learning-based scheduling policy that uses the head-of-line age in place of the virtual queue length. We show that our policy matches state-of-the-art performance under i.i.d. network conditions. Crucially, we also show that the system remains stable even under abrupt changes in channel conditions and can rapidly recover from periods of constraint infeasibility.

</details>


### [60] [Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks](https://arxiv.org/abs/2601.05984)
*Sahibzada Saadoon Hammad,Joaquín Huerta Guijarro,Francisco Ramos,Michael Gould Carlson,Sergio Trilles Oliver*

Main category: cs.LG

TL;DR: 提出基於社區興趣(CoI)的IoT感測器網路異常偵測框架，透過融合時空相似性分組感測器，使用自動編碼器進行異常偵測，減少計算開銷並分析模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: IoT設備快速部署形成大規模感測網路，需要有效組織異質性感測器並進行異常偵測。CoI範式提供將具有相似操作和環境特徵的設備分組的潛力，以減少計算成本並分析模型跨網路泛化能力。

Method: 1) 使用融合相似性矩陣將感測器分組為社區：包含Spearman係數的時序相關性、高斯距離衰減的空間鄰近性、高程相似性；2) 為每個社區選擇基於最佳輪廓係數的代表站；3) 使用三種自動編碼器架構(BiLSTM、LSTM、MLP)進行訓練，採用貝葉斯超參數優化和擴展窗口交叉驗證；4) 在正常溫度模式上訓練模型，透過重建誤差分析偵測異常。

Result: 實驗結果顯示在所評估配置中具有穩健的社區內性能，同時觀察到跨社區的變化。整體結果支持基於社區的模型共享在減少計算開銷方面的適用性，並能分析模型在IoT感測器網路間的泛化能力。

Conclusion: 提出的CoI基異常偵測框架有效組織異質IoT感測網路，透過社區分組減少計算成本，同時提供分析模型跨網路泛化能力的方法，為大規模感測網路監控提供實用解決方案。

Abstract: The rapid deployment of Internet of Things (IoT) devices has led to large-scale sensor networks that monitor environmental and urban phenomena in real time. Communities of Interest (CoIs) provide a promising paradigm for organising heterogeneous IoT sensor networks by grouping devices with similar operational and environmental characteristics. This work presents an anomaly detection framework based on the CoI paradigm by grouping sensors into communities using a fused similarity matrix that incorporates temporal correlations via Spearman coefficients, spatial proximity using Gaussian distance decay, and elevation similarities. For each community, representative stations based on the best silhouette are selected and three autoencoder architectures (BiLSTM, LSTM, and MLP) are trained using Bayesian hyperparameter optimization with expanding window cross-validation and tested on stations from the same cluster and the best representative stations of other clusters. The models are trained on normal temperature patterns of the data and anomalies are detected through reconstruction error analysis. Experimental results show a robust within-community performance across the evaluated configurations, while variations across communities are observed. Overall, the results support the applicability of community-based model sharing in reducing computational overhead and to analyse model generalisability across IoT sensor networks.

</details>


### [61] [LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection](https://arxiv.org/abs/2601.06016)
*Þór Sverrisson,Steinn Guðmundsson*

Main category: cs.LG

TL;DR: LookAroundNet是一种基于Transformer的癫痫检测模型，通过利用更宽的时间窗口（包括感兴趣段前后的EEG信号）来建模癫痫活动，在多种临床环境和数据集上表现出色，具有良好的泛化能力和临床部署可行性。


<details>
  <summary>Details</summary>
Motivation: 由于癫痫动态在不同患者、记录条件和临床环境中的巨大变异性，自动癫痫检测仍然具有挑战性。临床医生在解读EEG记录时会使用周围上下文信息，但现有方法通常只关注局部片段。

Method: 提出LookAroundNet，一种基于Transformer的癫痫检测器，使用更宽的EEG时间窗口（包括感兴趣段前后的信号）来建模癫痫活动。该方法在多个EEG数据集上进行评估，包括公开数据集和大型专有家庭EEG记录，采用模型集成策略。

Result: LookAroundNet在多个数据集上表现出强大的性能，能够很好地泛化到未见过的记录条件，计算成本适合实际临床部署。结果表明扩展的时间上下文、增加训练数据多样性和模型集成是提高性能的关键因素。

Conclusion: 这项工作通过引入扩展的时间上下文和多样化的训练数据，推动了自动癫痫检测模型向临床可行解决方案的发展，为实际临床应用提供了有前景的方法。

Abstract: Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [62] [Machine learning assisted state prediction of misspecified linear dynamical system via modal reduction](https://arxiv.org/abs/2601.05297)
*Rohan Vitthal Thorat,Rajdip Nayek*

Main category: stat.ML

TL;DR: 提出一个用于高维有限元结构动力系统的模型形式误差估计与修正框架，通过高斯过程潜在力模型在模态域非参数化表征误差，结合贝叶斯滤波和神经网络映射，实现跨不同网格离散化的模型修正。


<details>
  <summary>Details</summary>
Motivation: 参数化模型常因几何、材料、阻尼或边界条件的简化而忽略关键物理效应，产生模型形式误差，损害数字孪生预测精度。需要一种能处理高维系统、考虑不确定性的误差估计与修正方法。

Method: 1. 使用高斯过程潜在力模型在降阶模态域非参数化表征模型形式误差；2. 采用线性贝叶斯滤波联合估计系统状态和误差；3. 将有限元系统投影到模态基上；4. 使用网格不变神经网络将模态状态映射到误差估计，实现跨不同网格离散化的模型修正。

Result: 在五种模型形式误差场景（梁理论错误、阻尼误设、边界条件误设、未建模材料非线性、局部损伤）中验证，该方法在未见激励下显著减少了位移和旋转预测误差。

Conclusion: 该框架为数字孪生在固有建模不确定性下保持准确性提供了潜在途径，通过数据驱动的误差表征和计算高效的修正方法，提升了结构动力系统的预测精度。

Abstract: Accurate prediction of structural dynamics is imperative for preserving digital twin fidelity throughout operational lifetimes. Parametric models with fixed nominal parameters often omit critical physical effects due to simplifications in geometry, material behavior, damping, or boundary conditions, resulting in model form errors (MFEs) that impair predictive accuracy. This work introduces a comprehensive framework for MFE estimation and correction in high-dimensional finite element (FE) based structural dynamical systems. The Gaussian Process Latent Force Model (GPLFM) represents discrepancies non-parametrically in the reduced modal domain, allowing a flexible data-driven characterization of unmodeled dynamics. A linear Bayesian filtering approach jointly estimates system states and discrepancies, incorporating epistemic and aleatoric uncertainties. To ensure computational tractability, the FE system is projected onto a reduced modal basis, and a mesh-invariant neural network maps modal states to discrepancy estimates, permitting model rectification across different FE discretizations without retraining. Validation is undertaken across five MFE scenarios-including incorrect beam theory, damping misspecification, misspecified boundary condition, unmodeled material nonlinearity, and local damage demonstrating the surrogate model's substantial reduction of displacement and rotation prediction errors under unseen excitations. The proposed methodology offers a potential means to uphold digital twin accuracy amid inherent modeling uncertainties.

</details>


### [63] [A Bayesian Generative Modeling Approach for Arbitrary Conditional Inference](https://arxiv.org/abs/2601.05355)
*Qiao Liu,Wing Hung Wong*

Main category: stat.ML

TL;DR: 提出贝叶斯生成模型（BGM）框架，实现无需重新训练即可进行任意条件推断的通用引擎


<details>
  <summary>Details</summary>
Motivation: 现代数据分析需要灵活的任意条件推断P(X_B|X_A)，但现有方法受限于固定的条件结构，无法在训练后执行新的条件推断

Method: 通过迭代贝叶斯更新算法学习X的生成模型，模型参数和潜在变量更新直至收敛，训练后无需重新训练即可获得任意条件分布

Result: BGM在预测性能上表现优越，具有良好校准的预测区间，单个学习模型可作为具有不确定性量化的条件预测通用引擎

Conclusion: BGM框架结合AI捕捉复杂变量关系的能力和贝叶斯原理，为现代数据科学应用提供了有前景的通用条件推断框架

Abstract: Modern data analysis increasingly requires flexible conditional inference P(X_B | X_A) where (X_A, X_B) is an arbitrary partition of observed variable X. Existing conditional inference methods lack this flexibility as they are tied to a fixed conditioning structure and cannot perform new conditional inference once trained. To solve this, we propose a Bayesian generative modeling (BGM) approach for arbitrary conditional inference without retraining. BGM learns a generative model of X through an iterative Bayesian updating algorithm where model parameters and latent variables are updated until convergence. Once trained, any conditional distribution can be obtained without retraining. Empirically, BGM achieves superior prediction performance with well calibrated predictive intervals, demonstrating that a single learned model can serve as a universal engine for conditional prediction with uncertainty quantification. We provide theoretical guarantees for the convergence of the stochastic iterative algorithm, statistical consistency and conditional-risk bounds. The proposed BGM framework leverages the power of AI to capture complex relationships among variables while adhering to Bayesian principles, emerging as a promising framework for advancing various applications in modern data science. The code for BGM is freely available at https://github.com/liuq-lab/bayesgm.

</details>


### [64] [A brief note on learning problem with global perspectives](https://arxiv.org/abs/2601.05441)
*Getachew K. Befekadu*

Main category: stat.ML

TL;DR: 论文提出了一种动态优化的委托-代理学习框架，其中代理具有全局视角，而委托方则通过聚合信息解决高层优化问题。


<details>
  <summary>Details</summary>
Motivation: 研究在委托-代理设置下的学习问题，其中代理能够基于委托方共享的聚合信息获得全局视角，而委托方需要解决高层优化问题，同时考虑代理在样本外预测表现和私有数据集。

Method: 提出一个动态优化的委托-代理学习框架，代理具有全局视角能力，委托方通过经验似然估计器在条件矩限制模型下解决高层优化问题，同时整合代理的预测表现和私有数据信息。

Result: 提出了一个连贯的数学论证来刻画该抽象委托-代理学习框架背后的学习过程，但承认仍有一些概念和理论问题需要解决。

Conclusion: 论文建立了一个委托-代理学习框架的数学基础，为理解代理具有全局视角的动态优化学习过程提供了必要论证，但该框架仍需进一步完善。

Abstract: This brief note considers the problem of learning with dynamic-optimizing principal-agent setting, in which the agents are allowed to have global perspectives about the learning process, i.e., the ability to view things according to their relative importances or in their true relations based-on some aggregated information shared by the principal. Whereas, the principal, which is exerting an influence on the learning process of the agents in the aggregation, is primarily tasked to solve a high-level optimization problem posed as an empirical-likelihood estimator under conditional moment restrictions model that also accounts information about the agents' predictive performances on out-of-samples as well as a set of private datasets available only to the principal. In particular, we present a coherent mathematical argument which is necessary for characterizing the learning process behind this abstract principal-agent learning framework, although we acknowledge that there are a few conceptual and theoretical issues still need to be addressed.

</details>


### [65] [Multi-task Modeling for Engineering Applications with Sparse Data](https://arxiv.org/abs/2601.05910)
*Yigitcan Comlek,R. Murali Krishnan,Sandipp Krishnan Ravi,Amin Moghaddas,Rafael Giorjao,Michael Eff,Anirban Samaddar,Nesar S. Ramachandra,Sandeep Madireddy,Liping Wang*

Main category: stat.ML

TL;DR: 提出一个多任务高斯过程框架，用于处理工程系统中的多源多保真度数据，解决数据稀疏性和任务相关性变化问题


<details>
  <summary>Details</summary>
Motivation: 现代工程和科学工作流需要在相关任务和保真度级别上进行同时预测，其中高保真度数据稀缺且昂贵，而低保真度数据更丰富。需要解决数据稀疏性和不同任务相关性带来的挑战。

Method: 引入一个专门针对多源多保真度数据工程系统的多任务高斯过程框架，利用跨输出和保真度级别的任务间关系来改进预测性能并降低计算成本。

Result: 该框架在三个代表性场景中得到验证：Forrester函数基准测试、3D椭球空隙建模和摩擦搅拌焊接。通过量化和利用任务间关系，该框架在这些领域表现出良好的预测性能。

Conclusion: 提出的MTGP框架为具有显著计算和实验成本的领域提供了一个稳健且可扩展的预测建模解决方案，支持明智决策和高效资源利用。

Abstract: Modern engineering and scientific workflows often require simultaneous predictions across related tasks and fidelity levels, where high-fidelity data is scarce and expensive, while low-fidelity data is more abundant. This paper introduces an Multi-Task Gaussian Processes (MTGP) framework tailored for engineering systems characterized by multi-source, multi-fidelity data, addressing challenges of data sparsity and varying task correlations. The proposed framework leverages inter-task relationships across outputs and fidelity levels to improve predictive performance and reduce computational costs. The framework is validated across three representative scenarios: Forrester function benchmark, 3D ellipsoidal void modeling, and friction-stir welding. By quantifying and leveraging inter-task relationships, the proposed MTGP framework offers a robust and scalable solution for predictive modeling in domains with significant computational and experimental costs, supporting informed decision-making and efficient resource utilization.

</details>


### [66] [Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem](https://arxiv.org/abs/2601.06009)
*Sunia Tanweer,Firas A. Khasawneh*

Main category: stat.ML

TL;DR: 提出基于经典游程理论的单时间序列扩散过程检测框架，通过游程计数与二次变差的标度律区分随机扩散与确定性系统


<details>
  <summary>Details</summary>
Motivation: 现有方法（如熵或递归分析）主观性强，需要开发理论保证的方法来区分随机扩散过程与确定性信号，仅使用单离散时间序列

Method: 基于连续半鞅的游程和穿越定理，关联幅度≥ε的游程数N_ε与过程的二次变差[X]_T，构建数据驱动的扩散检验：比较经验游程计数与理论期望，通过比率K(ε)和对数-对数斜率偏差测量ε^{-2}律进行分类

Result: 该方法在典型随机系统、周期/混沌映射、加性白噪声系统以及随机Duffing系统上验证有效，能可靠区分扩散过程与确定性系统

Conclusion: 提出了一种非参数、模型无关的通用框架，仅依赖连续半鞅的小尺度结构，为单时间序列的随机扩散检测提供了理论保证的方法

Abstract: We develop a practical framework for distinguishing diffusive stochastic processes from deterministic signals using only a single discrete time series. Our approach is based on classical excursion and crossing theorems for continuous semimartingales, which correlates number $N_\varepsilon$ of excursions of magnitude at least $\varepsilon$ with the quadratic variation $[X]_T$ of the process. The scaling law holds universally for all continuous semimartingales with finite quadratic variation, including general Ito diffusions with nonlinear or state-dependent volatility, but fails sharply for deterministic systems -- thereby providing a theoretically-certfied method of distinguishing between these dynamics, as opposed to the subjective entropy or recurrence based state of the art methods. We construct a robust data-driven diffusion test. The method compares the empirical excursion counts against the theoretical expectation. The resulting ratio $K(\varepsilon)=N_{\varepsilon}^{\mathrm{emp}}/N_{\varepsilon}^{\mathrm{theory}}$ is then summarized by a log-log slope deviation measuring the $\varepsilon^{-2}$ law that provides a classification into diffusion-like or not. We demonstrate the method on canonical stochastic systems, some periodic and chaotic maps and systems with additive white noise, as well as the stochastic Duffing system. The approach is nonparametric, model-free, and relies only on the universal small-scale structure of continuous semimartingales.

</details>


### [67] [Manifold limit for the training of shallow graph convolutional neural networks](https://arxiv.org/abs/2601.06025)
*Johanna Tengler,Christoph Brune,José A. Iglesias*

Main category: stat.ML

TL;DR: 研究浅层图卷积神经网络在流形采样点云上的离散到连续一致性，证明正则化经验风险泛函的Γ收敛和全局极小值收敛，形式化网络训练的网格和样本独立性。


<details>
  <summary>Details</summary>
Motivation: 图卷积神经网络在流形采样数据上的训练需要理论保证，特别是在不同分辨率图上的收敛性。现有研究缺乏对训练过程离散到连续一致性的形式化分析，无法确保网格和样本独立性。

Method: 从泛函分析视角，将图信号视为流形上函数的空间离散化。选择弱紧致参数空间（单位球积），对输出权重和偏置施加Sobolev正则性。离散参数空间继承谱衰减特性，并采用适应图拉普拉斯谱窗的频率截断。证明正则化经验风险泛函的Γ收敛及其全局极小值收敛。

Result: 证明了在适当假设下，正则化经验风险泛函Γ收敛，其全局极小值在参数测度弱收敛和函数在紧集上一致收敛的意义下收敛。这为浅层GCNN的训练提供了网格和样本独立性的形式化保证。

Conclusion: 该研究为浅层图卷积神经网络在流形采样数据上的训练提供了严格的离散到连续收敛理论框架，形式化了网格和样本独立性，为实际应用中的多分辨率训练提供了理论依据。

Abstract: We study the discrete-to-continuum consistency of the training of shallow graph convolutional neural networks (GCNNs) on proximity graphs of sampled point clouds under a manifold assumption. Graph convolution is defined spectrally via the graph Laplacian, whose low-frequency spectrum approximates that of the Laplace-Beltrami operator of the underlying smooth manifold, and shallow GCNNs of possibly infinite width are linear functionals on the space of measures on the parameter space. From this functional-analytic perspective, graph signals are seen as spatial discretizations of functions on the manifold, which leads to a natural notion of training data consistent across graph resolutions. To enable convergence results, the continuum parameter space is chosen as a weakly compact product of unit balls, with Sobolev regularity imposed on the output weight and bias, but not on the convolutional parameter. The corresponding discrete parameter spaces inherit the corresponding spectral decay, and are additionally restricted by a frequency cutoff adapted to the informative spectral window of the graph Laplacians. Under these assumptions, we prove $Γ$-convergence of regularized empirical risk minimization functionals and corresponding convergence of their global minimizers, in the sense of weak convergence of the parameter measures and uniform convergence of the functions over compact sets. This provides a formalization of mesh and sample independence for the training of such networks.

</details>
