<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 33]
- [cs.LG](#cs.LG) [Total: 141]
- [stat.ML](#stat.ML) [Total: 11]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Internodal Distance Distributions for Static and Mobile Nodes in 2D/3D Wireless Networks](https://arxiv.org/abs/2511.21864)
*Nicholas Vaiopoulos,Alexander Vavoulas,Harilaos G. Sandalidis,Konstantinos K. Delibasis,Dimitris Varoutas*

Main category: eess.SP

TL;DR: 该论文提出了一个统一的解析框架，用于分析2D和3D无线网络中节点间距离分布，节点被限制在同心圆或球形区域内，考虑了静态和移动节点的四种部署场景。


<details>
  <summary>Details</summary>
Motivation: 无线网络中节点间距离分布对于网络性能分析至关重要，但现有研究缺乏对节点被限制在同心区域且包含移动性影响的统一分析框架。

Method: 提出了一个统一的解析框架，考虑了四种部署场景（静态-静态、静态-移动、移动-静态、移动-移动），推导了节点间距离的概率密度函数闭式表达式，并采用Beta分布近似和蒙特卡洛模拟验证。

Result: 为所有四种部署场景推导出了闭式解析表达式，包括等半径情况的特殊处理，通过Beta分布近似和蒙特卡洛模拟验证了分析结果的准确性和有效性。

Conclusion: 该研究提供了一个全面的解析框架，能够准确分析受限区域内无线网络的节点间距离分布，为网络性能评估和优化提供了理论基础。

Abstract: This letter presents a unified analytical framework for internodal distance distributions in 2D and 3D wireless networks, with nodes confined to concentric circular or spherical regions. Four deployment scenarios are considered, covering all combinations of static (uniform) and mobile (random waypoint-based) nodes. For each scenario, closed-form expressions for the internodal distance probability density functions are derived, incorporating both geometric constraints and spatial effects introduced by mobility. Equal-radius cases are also addressed. Beta-distribution approximations and Monte Carlo simulations demonstrate the accuracy and validity of the analytical results.

</details>


### [2] [Joint Estimation of Sea State and Vessel Parameters Using a Mass-Spring-Damper Equivalence Model](https://arxiv.org/abs/2511.21997)
*Ranjeet K. Tiwari,Daniel Sgarioto,Peter Graham,Alexei Skvortsov,Sanjeev Arulampalam,Damith C. Ranasinghe*

Main category: eess.SP

TL;DR: 提出一种无需先验传递函数知识的实时海况估计方法，通过伪质量-弹簧-阻尼器建模，联合估计海况和船舶参数。


<details>
  <summary>Details</summary>
Motivation: 传统海况估计方法依赖准确的波-船传递函数，但这些函数可能无法获取或随时间变化。需要一种无需先验传递函数知识的方法来实时估计海况。

Method: 使用伪质量-弹簧-阻尼器建模波-船系统，建立动态模型，将波浪激励递归建模为时变输入。推导统计一致的过程噪声协方差，实现平方根容积卡尔曼滤波器进行传感器数据融合。

Result: 通过蒙特卡洛仿真和高保真验证模拟器数据证实，估计的波浪谱与假设完全传递函数知识的方法相匹配。

Conclusion: 该方法能够在不依赖先验传递函数知识的情况下，有效联合估计海况和船舶参数，为船舶建造和海上安全应用提供了实用的实时海况估计方案。

Abstract: Real-time sea state estimation is vital for applications like shipbuilding and maritime safety. Traditional methods rely on accurate wave-vessel transfer functions to estimate wave spectra from onboard sensors. In contrast, our approach jointly estimates sea state and vessel parameters without needing prior transfer function knowledge, which may be unavailable or variable. We model the wave-vessel system using pseudo mass-spring-dampers and develop a dynamic model for the system. This method allows for recursive modeling of wave excitation as a time-varying input, relaxing prior works' assumption of a constant input. We derive statistically consistent process noise covariance and implement a square root cubature Kalman filter for sensor data fusion. Further, we derive the Posterior Cramer-Rao lower bound to evaluate estimator performance. Extensive Monte Carlo simulations and data from a high-fidelity validated simulator confirm that the estimated wave spectrum matches methods assuming complete transfer function knowledge.

</details>


### [3] [CUNEC: A Path Loss Model for Urban Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2511.22041)
*Thomas Choi,Yuning Zhang,Issei Kanno,Masaaki Ito,Andreas F. Molisch*

Main category: eess.SP

TL;DR: CUNEC：针对城市非平稳环境的细胞大规模MIMO路径损耗模型，考虑空间非平稳性、AP/UE相关性及城市特有传播现象，相比传统模型显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 传统路径损耗模型无法捕捉密集城市环境中真实传播的复杂性，特别是在细胞大规模MIMO系统中，需要更准确的模型来评估和优化系统性能。

Method: CUNEC模型通过街道顺序分割AP-UE路径，将路径损耗建模为城市几何的随机函数，并整合空间相关的阴影衰落。参数从大规模射线追踪数据推导，并在纽约的射线追踪和洛杉矶的实际信道测量中验证。

Result: 相比传统的alpha-beta模型，CUNEC在考虑的城市传播场景中显著提高了精度。同时发布了包含3万多个AP位置和128个UE位置的开源数据集。

Conclusion: CUNEC为城市非平稳环境中的细胞大规模MIMO系统提供了更准确的路径损耗建模方法，有助于系统评估和优化，开源数据集支持可重复研究和未来系统开发。

Abstract: Accurate path loss (PL) modeling is essential for evaluating and optimizing cell-free massive MIMO systems, especially in dense urban environments where traditional models fail to capture the complexity of real-world propagation. This paper introduces CUNEC (Cell-free massive MIMO for Urban Non-stationary Environments with Correlations, a novel PL model that accounts for spatial non-stationarity, inter-access point (AP)/user equipment (UE) correlations, and urban-specific propagation phenomena such as corner diffraction and street canyon waveguiding.bCUNEC segments AP-UE paths by street order, models PL as a stochastic function of urban geometry, and integrates spatially correlated shadowing. The parameters are derived from large-scale ray tracing and validated against both additional ray tracing in New York, NY and real-world channel measurements in Los Angeles, CA. Compared to the conventional alpha-beta model, CUNEC significantly improves accuracy in the considered urban propagation scenarios. An open-source dataset comprising over 30,000 AP locations and 128 UE positions is also released to support reproducible research and future system development.

</details>


### [4] [Bistatic Passive Tracking via CSI Power](https://arxiv.org/abs/2511.22144)
*Zhongqin Wang,J. Andrew Zhang,Kai Wu,Kuangda Chen,Min Xu,Y. Jay Guo*

Main category: eess.SP

TL;DR: PowerSense：基于CSI功率的实时被动跟踪框架，在ISAC应用中通过自共轭操作消除相位偏移，利用级联FFT提取特征，结合EKF实现目标跟踪和微多普勒特征提取


<details>
  <summary>Details</summary>
Motivation: ISAC应用中由于有限信号带宽、稀疏天线阵列和双基地收发器部署中的时钟不同步，准确的目标跟踪仍然具有挑战性。CSI相位虽然携带细粒度信息，但其时变失真需要复杂处理，在跟踪过程中会引入额外干扰。

Method: 1. 通过自共轭操作去除所有CSI相位偏移，得到相位无关的CSI功率；2. 应用级联FFT提取延迟、到达角(AoA)和多普勒特征；3. 进行目标检测和异常值去除；4. 通过扩展卡尔曼滤波器(EKF)进行连续轨迹估计；5. 利用估计的目标位置提取细化的微多普勒特征。

Result: 使用3.1 GHz LTE和5 GHz WiFi双基地信号（20 MHz带宽）进行室内实验，实现了0.4米的中值跟踪误差，无需任何预部署系统校准，同时提取出清晰明确的被跟踪目标的微多普勒特征。

Conclusion: PowerSense证明了仅使用CSI功率即可实现准确的目标跟踪和细粒度运动感知，避免了CSI相位处理的复杂性，为ISAC应用提供了一种实用的实时被动跟踪解决方案。

Abstract: Accurate object tracking in Integrated Sensing and Communication (ISAC) applications remains challenging due to limited signal bandwidth, sparse antenna arrays, and clock asynchronism inherent in bistatic transceiver deployments. This paper proposes PowerSense, a real-time passive tracking framework that operates directly on the power of Channel State Information (CSI). Although CSI phase carries fine-grained information, its time-varying distortions require complex processing, which introduces additional interference during tracking. Instead, this work solely relies on CSI power for accurate sensing. We first remove all CSI phase offsets through a self-conjugate operation, yielding phase-independent CSI power. A cascaded Fast Fourier Transform (FFT) is then applied to extract delay, angle-of-arrival (AoA), and Doppler features, followed by object detection, outlier removal, and continuous trajectory estimation via an Extended Kalman Filter (EKF). To enable fine-grained motion sensing, the estimated target positions are further used to extract refined micro-Doppler signatures. Using 3.1 GHz LTE and 5 GHz WiFi bistatic signals with a 20 MHz bandwidth, our indoor experiments achieve a median tracking error of 0.4 m, without performing any pre-deployment system calibration, while simultaneously extracting clear and unambiguous micro-Doppler signatures of the tracked target.

</details>


### [5] [A Model and Data Dual-driven Approach for Multitargets Detection under Mainlobe Jamming](https://arxiv.org/abs/2511.22201)
*Ruohai Guo,Jiang Zhu,Chengjie Yu,Zhigang Wang,Ning Zhang,Fengzhong Qu,Min Gong*

Main category: eess.SP

TL;DR: 提出基于扩散模型和数据双驱动的DMDD方法，用于多目标估计检测和结构化干扰抑制，在雷达系统中应对主瓣干扰挑战。


<details>
  <summary>Details</summary>
Motivation: 现代雷达系统在面对主瓣干扰时，目标检测和参数估计面临重大挑战，需要有效的方法来抑制结构化干扰并准确检测多目标。

Method: 采用扩散模型和数据双驱动方法：1) 通过基于分数的扩散过程对干扰先验建模，从纯干扰数据学习分数；2) 目标信号在距离空间稀疏，通过稀疏贝叶斯学习框架建模，使用期望最大化算法更新超参数；3) 构建单一扩散过程处理干扰，直接后验推理估计目标状态。

Result: 数值实验表明该方法在结构化干扰场景中有效，相比现有方法实现了更优的目标检测性能。

Conclusion: DMDD方法能够有效抑制结构化干扰并准确检测多目标，为雷达系统应对主瓣干扰提供了有效的解决方案。

Abstract: In modern radar systems, target detection and parameter estimation face significant challenges when confronted with mainlobe jamming. This paper presents a Diffusion-based Model and Data Dual-driven (DMDD) approach to estimate and detect multitargets and suppress structured jamming. In DMDD, the jamming prior is modeled through a score-based diffusion process with its score learned from the pure jamming data, enabling posterior sampling without requiring detailed knowledge of jamming. Meanwhile, the target signal is usually sparse in the range space, which can be modeled via a sparse Bayesian learning (SBL) framework, and hyperparameter is updated through the expectation-maximization (EM) algorithm. A single diffusion process is constructed for the jamming, while the state of targets are estimated through direct posterior inference, enhancing computational efficiency. The noise variance is also estimated through EM algorithm. Numerical experiments demonstrate the effectiveness of the proposed method in structured jamming scenarios. The proposed DMDD algorithm achieves superior target detection performance, compared with existing methods.

</details>


### [6] [Foundation Model for Intelligent Wireless Communications](https://arxiv.org/abs/2511.22222)
*Boxun Liu,Xuanyu Liu,Shijian Gao,Xiang Cheng,Liuqing Yang*

Main category: eess.SP

TL;DR: WiFo-2是一个革命性的无线基础模型，采用稀疏专家混合架构，在116亿CSI点的大规模数据集上预训练，在零样本和下游任务中表现出色，并通过硬件原型验证了实际部署可行性。


<details>
  <summary>Details</summary>
Motivation: 当前智能无线系统面临窄域、数据饥渴的AI模型限制，阻碍了频谱效率和可靠性的提升。需要突破这一约束，开发能够处理异构数据和任务的基础模型。

Method: 提出WiFo-2无线基础模型，采用稀疏专家混合架构，在116亿信道状态信息点的大规模多样化数据集上进行预训练，学习深度可泛化的信道知识。

Result: WiFo-2展现出卓越的零样本能力，在未见配置上不仅匹配而且超越了任务特定基线的全样本性能，同时在八个关键下游任务中通过微调实现优异表现。硬件原型验证了实际部署可行性。

Conclusion: WiFo-2代表了AI无线系统的范式转变，通过基础模型架构实现了前所未有的性能，为下一代智能无线系统的发展铺平了道路。

Abstract: The evolution toward intelligent next-generation wireless systems promises unprecedented spectral efficiency and reliability but is hindered by a paradigm of narrow and data-hungry AI models. Breaking from this constraint, this work introduces WiFo-2, a revolutionary wireless foundation model that establishes a new state of the art for extensive channel state information (CSI)-based tasks. Uniquely architected as a sparse mixture of experts, WiFo-2 effectively manages heterogeneous data and tasks while enabling highly efficient inference. It is pretrained on a massive and diverse dataset of 11.6 billion CSI points, which enables the acquisition of profound and generalizable channel knowledge. WiFo-2 demonstrates remarkable zero-shot capabilities, not only matching but surpassing the full-shot performance of task-specific baselines on unseen configurations, all while providing reliable confidence estimates. Furthermore, the model achieves exceptional performance on eight key downstream tasks with minimal fine-tuning. A functional hardware prototype demonstrates its real-world deployment feasibility and significant system gains, highlighting WiFo-2's superiority and paving the way for a paradigm shift in AI-based wireless systems.

</details>


### [7] [Data-Efficient Motor Condition Monitoring with Time Series Foundation Models](https://arxiv.org/abs/2511.23177)
*Deyu Li,Xinyuan Liao,Shaowei Chen,Shuai Zhao*

Main category: eess.SP

TL;DR: 该论文提出了一种基于时间序列基础模型（MOMENT和Mantis）的电机状态监测框架，通过迁移预训练学到的通用特征来解决故障标签稀疏和类别不平衡问题，显著减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 电机状态监测对系统可靠性至关重要，但数据驱动的诊断方法面临故障标签稀疏和严重类别不平衡的问题，限制了在实际应用中的有效性。

Method: 利用两个时间序列基础模型（MOMENT和Mantis）在预训练阶段学到的通用特征，通过迁移学习将大规模预训练获得的时间表示应用到电机故障诊断中。

Result: MOMENT仅使用1%的训练数据就达到传统深度学习模型近两倍的性能；Mantis在相同数据比例下超越最先进基线22%，达到90%的准确率。

Conclusion: 时间序列基础模型在故障诊断中表现出强大的泛化能力和数据效率，为智能电机状态监测的可扩展和自适应框架提供了新思路。

Abstract: Motor condition monitoring is essential for ensuring system reliability and preventing catastrophic failures. However, data-driven diagnostic methods often suffer from sparse fault labels and severe class imbalance, which limit their effectiveness in real-world applications. This paper proposes a motor condition monitoring framework that leverages the general features learned during pre-training of two time series foundation models, MOMENT and Mantis, to address these challenges. By transferring broad temporal representations from large-scale pre-training, the proposed approach significantly reduces dependence on labeled data while maintaining high diagnostic accuracy. Experimental results show that MOMENT achieves nearly twice the performance of conventional deep learning models using only 1\% of the training data, whereas Mantis surpasses state-of-the-art baselines by 22\%, reaching 90\% accuracy with the same data ratio. These results demonstrate the strong generalization and data efficiency of time series foundation models in fault diagnosis, providing new insights into scalable and adaptive frameworks for intelligent motor condition monitoring.

</details>


### [8] [Pinching-Antenna Systems-Assisted SWIPT: A Rate-Energy Trade-off Perspective](https://arxiv.org/abs/2511.22224)
*Qi Yang,Kai Liu,Jingjing Zhao,Kaiquan Cai,Xidong Mu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 本文研究了PASS辅助的SWIPT系统中的速率-能量权衡问题，考虑了单用户和多用户场景，提出了相应的波束成形优化算法，并比较了多种多址接入方案。


<details>
  <summary>Details</summary>
Motivation: 研究PASS辅助的SWIPT系统中的速率-能量权衡问题，旨在通过优化波束成形来同时最大化数据传输速率和能量收集效率，解决传统固定位置天线系统的局限性。

Method: 1) 单用户场景：提出两阶段算法，先用SCA方法最小化大尺度路径损耗，再用相位对齐微调方法。2) 多用户场景：考虑FDMA、TDMA和NOMA三种多址方案，采用ε-约束法将MOOP转换为SOOP，然后结合PSO和凸优化方法分别解决波束成形和资源分配问题。

Result: 仿真结果表明：i) PASS相比传统固定位置天线系统在波束成形方面能获得显著更优的速率-能量区域；ii) 在多用户场景中，利用时分切换特性的TDMA方案优于NOMA和FDMA。

Conclusion: PASS系统在SWIPT应用中具有显著优势，能够有效提升速率-能量权衡性能，特别是在多用户场景下，TDMA方案通过时间切换特性展现出最佳性能。

Abstract: This paper investigates the rate-energy trade-off for pinching-antenna systems (PASS)-assisted simultaneous wireless information and power transfer (SWIPT) systems. Both the single information user (IU)/energy user (EU) and multiple IUs/EUs scenarios are considered.1) For the single IU/EU scenario, a pinching beamforming optimization problem is formulated for simultaneously maximizing data rate and harvested energy. To tackle this problem, a two-stage algorithm is proposed. Specifically, the successive convex approximation (SCA) method is first invoked for minimizing the large-scale path loss, which is followed by the fine-tuning method for the phase alignment. 2) For the multiple IUs/EUs scenario, three multiple access schemes are considered, i.e., frequency division multiple access (FDMA), time division multiple access (TDMA), and non-orthogonal multiple access (NOMA). The corresponding multi-objective optimization problem (MOOP) that simultaneously maximizes the minimum data rate and minimum harvested energy is formulated for ensuring users' fairness. To address this problem, we adopt the $ε$-constraint method to first convert the intractable MOOPs to single-objective optimization problems (SOOPs). Then, for the SOOP under each multiple access protocol, the particle swarm optimization (PSO) and convex optimization methods are adopted for solving the pinching beamforming and resource allocation problems, respectively. Simulation results unveil that: i) PASS can achieve a significantly superior rate-energy region compared to conventional fixed-position antenna systems for pinching beamforming; and ii) by exploiting the time-switching feature, TDMA can outperform both NOMA and FDMA for the multiple IUs/EUs scenario.

</details>


### [9] [NOMA Assisted Downlink Power Allocation in Pinching Antenna Systems Using Convolutional Neural Network](https://arxiv.org/abs/2511.22328)
*Saeed Mohammadzadeh,Kanapathippillai Cumanan,Zhiguo Ding*

Main category: eess.SP

TL;DR: 提出基于卷积神经网络的夹持天线系统NOMA优化方法，通过天线布局和功率分配提升系统性能和公平性


<details>
  <summary>Details</summary>
Motivation: 传统天线系统在非正交多址接入中面临天线布局和功率分配的复杂优化问题，需要高效且可扩展的解决方案

Method: 采用两阶段优化策略：用户感知初始化+梯度优化确定天线位置；最大最小公平功率分配+准线性规划；CNN学习信道条件与最优功率系数的非线性映射

Result: 仿真结果表明，所提CNN方法在PA系统中显著提升和速率和用户公平性，同时降低计算复杂度

Conclusion: 基于CNN的PA-NOMA系统优化方法能有效解决天线布局和功率分配问题，提供可扩展且自适应的解决方案

Abstract: In this paper, we consider a flexible-antenna architecture, referred to as a pinching-antenna (PA) system, in which multiple PAs realized by activating small dielectric particles along a dielectric waveguide are jointly employed to serve a single-antenna user. We investigate antenna placement and power allocation optimization in PA-assisted non-orthogonal multiple access (NOMA) systems using a convolutional neural network (CNN). An optimization strategy is developed to determine the PA locations that maximize achievable NOMA performance while satisfying physical and spatial constraints. The proposed method adopts a two-stage structure, combining a user-aware initialization with a gradient-based refinement, enabling near-optimal performance with significantly lower computational cost. A max-min fairness formulation is introduced for power allocation to balance the power budget among users with varying channel strengths, solved efficiently via quasi-linear programming and bisection search. Finally, a CNN-based learning framework is employed to capture the nonlinear mapping between channel conditions and the corresponding optimal power coefficients. This framework can infer near-optimal power allocations for unseen network configurations without retraining, offering scalability and adaptability. Simulation results show that the proposed CNN-based NOMA approach for PA systems improves sum rate and user fairness while reducing computational complexity.

</details>


### [10] [A Bio-Inspired Whisker Sensor toward Underwater Flow Sensing in Darkness and Turbidity](https://arxiv.org/abs/2511.22353)
*Zheyi Hang,Denghan Xiong,Pengo Xie,Huan Hu*

Main category: eess.SP

TL;DR: 提出一种仿生胡须传感器，用于水下流场感知，具有高灵敏度、线性响应、良好稳定性和简化封装，适用于无人水下航行器导航和环境监测。


<details>
  <summary>Details</summary>
Motivation: 现有水下流场传感器存在响应慢、检测阈值高、方向辨别能力有限、封装复杂和长期稳定性差等问题，特别是在浑浊和杂乱水域中导航和目标感知时表现不佳。传统应变片精度有限，掺杂硅传感器检测高度有限，都存在可扩展性、恶劣水环境下鲁棒性和校准复杂性等挑战。

Method: 采用仿生设计，模仿海豹胡须结构，将高灵敏度硅应变片嵌入柔性PDMS基底中，实现高灵敏度和简化封装。传感器通过测量力-电阻响应来感知水流。

Result: 传感器表现出线性力-电阻响应，检测极限为0.27 mN，经过10,000次加载循环后仍保持稳定，偏移漂移小于2%。在水下偶极子测试中显示出频率匹配特性，具有清晰的纵向和横向空间响应模式。

Conclusion: 该仿生胡须传感器为水下流场感知提供了一种鲁棒且可扩展的解决方案，特别适用于无人水下航行器平台的实际部署，在浑浊和杂乱水域中具有良好应用前景。

Abstract: Underwater flow sensing is critical for unmanned underwater vehicles (UUVs) and environmental monitoring, yet existing sensors often suffer from low responsiveness, high detection thresholds, limited directional discrimination, complex packaging, and poor long-term stability, especially for navigation and target perception in turbid and cluttered waters. Previous solutions based on traditional strain gauges with limited detection accuracy or doped silicon sensors with limited detection height have shown feasibility but still face challenges in scalability, robustness under harsh aquatic conditions, and calibration complexity. This work presents a bio-inspired whisker sensor that provides a balanced solution by embedding high-gauge-factor silicon strain gauges into a flexible PDMS base, mimicking seal whiskers to offer both high sensitivity and simplified packaging. The device exhibits a linear force-resistance response with a limit of detection of 0.27 mN, maintains stability after 10,000 loading cycles, and shows minimal offset drift of less than 2 percent. It also demonstrates frequency matching in underwater dipole tests with clear longitudinal and transverse spatial response patterns. These results indicate a robust and scalable route for underwater flow sensing on UUV platforms in practical deployments.

</details>


### [11] [Adaptive Dual-Windowing Strategies for Multi-Target Detection in OFDM ISAC](https://arxiv.org/abs/2511.22458)
*Ali Al Khansa,Youssef Bahannis*

Main category: eess.SP

TL;DR: 提出一种用于OFDM ISAC系统的双窗周期图算法，通过互补窗口和自适应检测策略，在保持高性能的同时降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在OFDM ISAC系统中，多目标检测需要在旁瓣抑制和分辨率之间取得平衡。现有方法使用单一固定窗口和预定义检测策略，无法动态适应不同场景需求。

Method: 提出双窗周期图算法：使用两个互补窗口（一个优化分辨率，一个优化旁瓣抑制），先应用低复杂度检测算法（如BSTC），通过决策机制比较结果；结果一致时使用分辨率优化估计，否则触发高性能算法（如CSTC）解决模糊性。

Result: 数值结果表明，该方法在保持高性能的同时显著降低了计算复杂度，特别是在高信噪比（SNR）条件下效果更明显。

Conclusion: 提出的自适应双窗方法有效解决了现有固定策略的局限性，动态平衡了检测性能和计算复杂度，为OFDM ISAC系统提供了更优的解决方案。

Abstract: In Orthogonal Frequency Division Multiplexing (OFDM) Integrated Sensing and Communication (ISAC) systems, a key challenge is balancing sidelobe attenuation and resolution for multi-target detection scenarios. While windowing functions are typically used to manage this trade-off, state-of-the-art methods rely on a single, fixed window followed by a predefined detection strategy (e.g., Binary Successive Target Cancellation (BSTC) (low complexity) or Coherent Successive Target Cancellation (CSTC) (high performance)). This paper proposes a novel dual-window periodogram-based algorithm that leverages two complementary windows: one optimized for resolution and the other for sidelobe suppression. Then, a low-complexity detection algorithm (e.g., BSTC) is applied to both, and a decision mechanism compares the outputs. When results align, the resolution-optimized estimates are directly used; otherwise, high performance algorithm (e.g., CSTC) is triggered to resolve ambiguities. This adaptive approach dynamically balances the detection performance and the complexity, addressing limitations in existing fixed strategies. The Numerical results confirm that the proposed method achieves high performance while reducing the complexity, especially at a high Signal to Noise Ratio (SNR).

</details>


### [12] [Learning to Count Targets from Dual-Window: A CNN Approach for OFDM ISAC](https://arxiv.org/abs/2511.22473)
*Ali Al Khansa*

Main category: eess.SP

TL;DR: 提出一种新的CNN方法，使用两个加窗的二维距离-多普勒周期图，通过融合分辨率优化和旁瓣抑制优化的互补视图来提升ISAC-OFDM系统中的目标数量估计精度。


<details>
  <summary>Details</summary>
Motivation: 在ISAC-OFDM系统中，现有CNN方法在密集场景下估计目标数量的精度会下降。主要原因是经典的分辨率-旁瓣衰减权衡问题，当目标较弱或间距较近时性能受限。虽然加窗技术可以调节这种权衡，但传统方法通常使用静态窗口选择。

Method: 提出一种新的CNN方法，使用两个不同加窗的二维距离-多普勒周期图：一个窗口优化分辨率，另一个窗口优化旁瓣抑制。模型学习融合这两种互补视图，自动判断每种视图在何时最有信息价值，从而显式地处理分辨率-旁瓣衰减权衡问题。

Result: 数值实验表明，该方法相比单窗口CNN基线取得了持续的性能提升，在目标密度增加时具有更好的扩展性，并且在不同噪声水平下表现出更强的鲁棒性。

Conclusion: 通过让CNN模型学习融合分辨率优化和旁瓣抑制优化的互补加窗视图，可以有效解决ISAC-OFDM系统中目标数量估计的分辨率-旁瓣衰减权衡问题，提升密集场景下的估计性能。

Abstract: Integrated Sensing and Communication (ISAC) with Orthogonal Frequency Division Multiplexing (OFDM) waveforms is a key enabler for next-generation wireless systems. Recent studies show that Convolutional Neural Networks (CNNs) can estimate the number of targets from two-dimensional (2D) range-Doppler periodogram maps, yet accuracy often degrades as scenes become denser. One significant factor is the classical resolution-sidelobe attenuation trade-off, which limits performance when targets are weak or closely spaced. While windowing is routinely applied to shape this trade-off, the choice is typically static. This paper proposes a new CNN method that uses two windowed range-Doppler periodograms and learns to fuse complementary views: one window optimized for resolution and one window optimized for sidelobe suppression. The design explicitly targets the resolution-sidelobe attenuation trade-off by exposing the model to complementary windowed maps and letting it learn when each is most informative. Numerical experiments show consistent gains over single-window CNN baselines, with better scaling in target density and greater robustness across different noise levels.

</details>


### [13] [Enabling Full-Duplex LEO Satellite Systems with Non-Reciprocal BD-RIS-Assisted Beamforming](https://arxiv.org/abs/2511.22525)
*Ziang Liu,Wonjae Shin,Bruno Clerckx*

Main category: eess.SP

TL;DR: 提出一种采用非互易性对角可重构智能表面(NR-BD-RIS)的全双工LEO卫星系统，通过非互易阻抗网络打破信道互易性，实现多波束同时服务，显著提升上下行和速率性能。


<details>
  <summary>Details</summary>
Motivation: LEO卫星通信面临高速移动带来的波束快速对准挑战，大天线阵列受限于功耗尺寸重量难以实现。未来非地面网络需要高谱效，全双工系统成为必要选择。

Method: 提出FD LEO卫星系统，采用NR-BD-RIS和多发多收天线。NR-BD-RIS通过非互易阻抗网络打破信道互易性，支持多波束方向。采用时分调度框架，每个时隙同时服务多个上下行地面设备。

Result: 数值结果表明，NR-BD-RIS在单用户和多用户场景下，上下行和速率性能显著优于传统BD-RIS和对角RIS。NR-BD-RIS需要更少的重配置频率，更易于实际部署。

Conclusion: NR-BD-RIS为LEO卫星全双工通信提供了一种高效解决方案，通过打破信道互易性实现多波束同时服务，在性能和实用性方面均优于现有RIS方案。

Abstract: Low Earth orbit (LEO) satellite is a promising technology for providing low-latency, high-data-rate, and wide-coverage communication services. However, the high mobility of LEO satellites necessitates fast and accurate beam steering to continuously serve ground devices. While large antenna arrays can address these challenges, strict constraints on power, size, and weight make such solutions difficult to implement. Furthermore, future non-terrestrial networks (NTNs) require high spectral efficiency, which motivates the adoption of in-band full-duplex (FD) systems. {To overcome these challenges, we propose an FD LEO satellite system, where the non-reciprocal beyond-diagonal reconfigurable intelligent surfaces (NR-BD-RIS) and multiple transmit and receive antennas are attached to the LEO satellite. The NR-BD-RIS reflects the downlink (DL) and uplink (UL) signals by passive beamforming.} By incorporating non-reciprocal components into the impedance network of RIS, the NR-BD-RIS breaks channel reciprocity, facilitating simultaneous support for multiple beam directions. To cover a wide coverage, we propose a time-sharing scheduling framework where, in each time slot, the NR-BD-RIS simultaneously serves multiple DL and multiple UL ground devices. An optimization problem is defined to maximize the weighted sum-rate over the entire scheduling period. Numerical results demonstrate that the proposed NR-BD-RIS significantly performs better than both conventional BD-RIS and diagonal RIS (D-RIS) regarding DL and UL sum-rate performance under both single-user (SU) and multiple-user (MU) cases. Additionally, the NR-BD-RIS requires less frequent reconfiguration compared to other two types of RIS, making it more practical for implementation.

</details>


### [14] [Interference and Multipath Resilient ToA Estimation](https://arxiv.org/abs/2511.22629)
*António Barros,Christoph Studer*

Main category: eess.SP

TL;DR: 提出一种计算高效、抗多径和强干扰的到达时间估计算法，结合自适应空间滤波和自动微分技术，无需模型阶数估计即可超分辨率估计首径到达时间。


<details>
  <summary>Details</summary>
Motivation: 传统到达时间估计方法在多径传播和强干扰环境下性能受限，需要解决计算复杂度高、需要模型阶数估计等问题。

Method: 利用多接收天线，结合自适应空间滤波和自动微分技术，实现低计算复杂度的超分辨率首径到达时间估计，无需模型阶数估计。

Result: 通过射线追踪室内传播信道仿真，相比传统相关基到达时间估计方法和子空间技术（如JADE），性能有显著提升。

Conclusion: 提出的算法在多径和强干扰环境下具有计算高效、性能优越的特点，为到达时间估计提供了新的解决方案。

Abstract: We present a computationally-efficient algorithm for time-of-arrival (ToA) estimation that is robust under multipath propagation and strong interference. Our algorithm leverages multiple receive antennas to combine adaptive spatial filtering with autodifferentiation in order to super-resolve the tap of the first-arriving path at low computational complexity and without requiring model-order estimation. We use simulations with ray-traced indoor propagation channels to demonstrate significant performance improvements over conventional correlation-based ToA estimation methods and subspace techniques such as JADE.

</details>


### [15] [Strategies to Minimize Out-of-Distribution Effects in Data-Driven MRS Quantification](https://arxiv.org/abs/2511.23135)
*Julian P. Merkofer,Antonia Kaiser,Anouk Schrantee,Oliver J. Gurney-Champion,Ruud J. G. van Sloun*

Main category: eess.SP

TL;DR: 本研究系统比较了磁共振波谱代谢物定量中的数据驱动与模型驱动策略，重点关注对分布外效应的鲁棒性，以及准确性、鲁棒性和泛化性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 磁共振波谱代谢物定量中，需要评估数据驱动方法在分布外情况下的可靠性，并找到平衡准确性、鲁棒性和泛化性的最佳策略。

Method: 使用三种不同策略训练神经网络：监督回归、自监督学习和测试时适应，并与模型驱动拟合工具进行比较。实验结合大规模模拟数据和7T人体脑部活体光谱数据。

Result: 监督学习在训练分布内表现良好，但在分布外时性能显著下降；测试时适应对分布外效应更具弹性；自监督学习表现居中。活体实验中所有方法方差较大，重叠代谢物和基线变异性仍是挑战。

Conclusion: 数据驱动方法在MRS代谢物定量中可取得良好性能，但其可靠性取决于对训练分布和分布外效应的谨慎考虑。当无法预测目标分布条件时，测试时适应策略能确保定量、数据和模型之间的一致性。

Abstract: This study systematically compared data-driven and model-based strategies for metabolite quantification in magnetic resonance spectroscopy (MRS), focusing on resilience to out-of-distribution (OoD) effects and the balance between accuracy, robustness, and generalizability. A neural network designed for MRS quantification was trained using three distinct strategies: supervised regression, self-supervised learning, and test-time adaptation. These were compared against model-based fitting tools. Experiments combined large-scale simulated data, designed to probe metabolite concentration extrapolation and signal variability, with 1H single-voxel 7T in-vivo human brain spectra. In simulations, supervised learning achieved high accuracy for spectra similar to those in the training distribution, but showed marked degradation when extrapolated beyond the training distribution. Test-time adaptation proved more resilient to OoD effects, while self-supervised learning achieved intermediate performance. In-vivo experiments showed larger variance across the methods (data-driven and model-based) due to domain shift. Across all strategies, overlapping metabolites and baseline variability remained persistent challenges. While strong performance can be achieved by data-driven methods for MRS metabolite quantification, their reliability is contingent on careful consideration of the training distribution and potential OoD effects. When such conditions in the target distribution cannot be anticipated, test-time adaptation strategies ensure consistency between the quantification, the data, and the model, enabling reliable data-driven MRS pipelines.

</details>


### [16] [Advances in electromagnetic techniques for subsurface infrastructure detection: A comprehensive review of methods, challenges, and innovations](https://arxiv.org/abs/2511.22673)
*Arasti Afrasiabi,Farough Rahimzadeh,Alireza Keshavarzi*

Main category: eess.SP

TL;DR: 本文综述了埋地基础设施非侵入式探测方法（ERT、IRT、磁力测量）的现状，重点讨论了数据融合技术和数学估计器在提高探测精度方面的应用。


<details>
  <summary>Details</summary>
Motivation: 埋地基础设施探测面临深度估计困难、不同地下物体难以区分等挑战，需要综合多种非侵入式探测技术来提高检测准确性和可靠性。

Method: 系统综述了三种主要非侵入式探测方法：电阻率层析成像（ERT）、红外热成像（IRT）和磁力测量，并分析了数据融合技术（多传感器集成）和数学估计器（卡尔曼滤波器、粒子滤波器等）的应用。

Result: ERT和IRT在次表面成像方面具有独特优势，磁力测量适用于铁磁性目标探测；数据融合技术能显著克服单一方法的局限性，数学估计器能有效降低噪声并提高测量精度。

Conclusion: 未来研究需要优化传感器性能、改进融合算法、探索实时数据处理的混合模型，以进一步提升埋地基础设施探测的准确性和实用性。

Abstract: This review paper explores the state-of-the-art in non-intrusive methods for detecting and characterising buried infrastructure, focusing on Electrical Resistivity Tomography (ERT), Infrared Thermography (IRT), and magnetometry, along with data fusion techniques and mathematical estimators. ERT and IRT offer distinct advantages in subsurface imaging, while magnetometry provides omnidirectional measurements ideal for detecting ferrous targets. Despite these benefits, each method has inherent limitations, such as challenges in depth estimation and difficulties in distinguishing between various subsurface objects. The integration of multiple sensing techniques through data fusion approaches has shown significant promise in overcoming these limitations and improving detection accuracy. Additionally, mathematical estimators, including Kalman filters and particle filters, play a crucial role in reducing noise and enhancing the precision of geophysical surveys. This review discusses the strengths, limitations, and future research needs of these techniques, offering a comprehensive understanding of their current and potential applications in buried infrastructure detection. The paper concludes by emphasising the importance of optimising sensor performance, refining fusion algorithms, and exploring hybrid models for real-time data processing in future research.

</details>


### [17] [Rethinking Signaling Design for ISAC: From Pilot-Based to Payload-Based Sensing](https://arxiv.org/abs/2511.22703)
*Yunxin Li,Ying Zhang,Christos Masouros,Sofie Pollin,Fan Liu*

Main category: eess.SP

TL;DR: 本文综述了ISAC信号设计从导频辅助感知到数据载荷方法的范式转变，重点探讨如何在现有5G NR框架内实现这些技术，并通过V2I案例展示实际可行性。


<details>
  <summary>Details</summary>
Motivation: ISAC作为6G网络的关键使能技术，其信号设计面临从传统导频辅助感知向更高效数据载荷方法转变的需求。现有5G NR结构为ISAC实现提供了基础框架，但需要探索如何充分利用通信资源实现感知功能。

Method: 首先重用5G NR的导频和参考信号进行感知，然后扩展到更先进的数据载荷集成方法，包括新颖的星座整形、调制基和脉冲整形滤波器。通过V2I网络案例研究展示参考信号和载荷回波的综合利用。

Result: 研究展示了从稀疏导频资源扩展到完整通信帧进行感知的机会与权衡，强调星座特性和调制选择直接影响感知性能。V2I案例表明综合利用参考信号和载荷回波可以减少信令开销，实现主动波束管理和切换。

Conclusion: ISAC信号设计正经历从导频辅助到数据载荷方法的范式转变，现有5G NR结构为实现这一转变提供了可行框架。综合利用通信资源不仅能提升感知性能，还能减少信令开销，为6G网络中的ISAC应用奠定基础。

Abstract: Integrated Sensing and Communications (ISAC) is emerging as a key enabler for 6G networks, with signaling design at the core of its evolution. This paper reviews the paradigm shift of ISAC signaling designs from pilot-aided sensing to data payload-based approaches, with a particular focus on how these techniques can be realized within existing 5G NR structures. We commence with the reuse of pilots and reference signals that exploit existing 5G New Radio (NR) structures for sensing. Then, we extend to more advanced approaches that integrate the data payload through novel constellation shaping, modulation bases, and pulse shaping filters. We highlight the opportunities and tradeoffs that arise when extending sensing from sparse pilot and reference signal resources to the full communication frame, emphasizing how constellation properties and modulation choices directly determine sensing performance. To illustrate practical feasibility, a case study on sensing-assisted NR Vehicle-to-Infrastructure (V2I) networks demonstrates how exploiting both reference signals and payload echoes can reduce signaling overhead and enable proactive beam management and handover.

</details>


### [18] [FPGA-Enabled Modulo ADC with x100 Dynamic-Range Expansion: Hardware Design and Performance Evaluation](https://arxiv.org/abs/2511.22752)
*Zeyuan Li,Wenyi Yan,Lu Gan,Guoquan Li,Hongqing Liu*

Main category: eess.SP

TL;DR: 提出基于FPGA的模数转换器平台，通过折叠输入信号实现高动态范围信号采集，避免传统ADC的削波问题


<details>
  <summary>Details</summary>
Motivation: 传统ADC无法捕获高动态范围信号（容易削波），需要一种能处理HDR信号的模数转换解决方案

Method: 采用FPGA平台实现模数ADC，集成精密模拟前端和200MHz FPGA控制回路，包含多比特更新和数字欠补偿校准，确保稳定折叠和精确反馈生成

Result: 在400kHz带宽内实现超过100倍的动态范围扩展，同时保持与传统ADC相当的保真度；SoC式实现支持板上实时恢复和先进重建算法基准测试

Conclusion: 该平台为HDR信号采集和评估提供了紧凑实用的框架，支持系统性能评估和算法基准测试

Abstract: Conventional analog-to-digital converters (ADCs) fail to capture high-dynamic-range (HDR) signals due to clipping. Modulo ADCs circumvent this limitation by folding the input prior to quantization and algorithmically reconstructing the original waveform. This work presents a field-programmable gate array (FPGA)-based modulo ADC platform for systematic HDR performance evaluation. The mixed-signal architecture integrates a precision analog front end with a 200-MHz FPGA control loop that incorporates multi-bit updates and digital under-compensation calibration, ensuring stable folding and accurate feedback generation. The system achieves more than a hundred-fold dynamic-range expansion within a 400-kHz bandwidth while maintaining fidelity comparable to that of a conventional ADC. A system-on-chip (SoC)-like implementation enables on-board real-time recovery and supports benchmarking of state-of-the-art reconstruction algorithms, providing a compact and practical framework for HDR signal acquisition and evaluation.

</details>


### [19] [Moduli Selection in Robust Chinese Remainder Theorem: Closed-Form Solutions and Layered Design](https://arxiv.org/abs/2511.22757)
*Wenyi Yan,Lu Gan,Hongqing Liu,Shaoqing Hu*

Main category: eess.SP

TL;DR: 该论文研究了鲁棒中国剩余定理中的模数选择问题，针对有界误差扰动的情况，提出了小模数数量的精确解和斐波那契启发的分层构造，建立了RCRT模数设计的一般理论。


<details>
  <summary>Details</summary>
Motivation: 研究鲁棒中国剩余定理中的模数选择这一基本问题，解决在存在有界误差扰动的情况下如何选择模数以最大化鲁棒性，这对于各种应用如亚奈奎斯特采样、相位解缠、模数ADC等具有重要意义。

Method: 1) 针对小模数数量（L=2,3,4）在动态范围和模数约束下获得最大化鲁棒性边界的精确解；2) 引入斐波那契启发的分层构造（针对L=2），产生K个鲁棒解码层；3) 分析鲁棒性和范围在层间的演化；4) 提供闭式表达式估计常见数据和噪声模型下的成功概率。

Result: 获得了小模数情况下的精确解，提出了分层构造实现可预测的误差容忍度与动态范围权衡，建立了鲁棒模数设计的一般理论框架，为多种应用提供了理论基础。

Conclusion: 该研究建立了RCRT模数设计的一般理论，补充了先前的算法工作，强调了鲁棒模数设计在多样化信息处理领域的广泛相关性，为亚奈奎斯特采样、相位解缠、模数ADC等应用提供了有前景的解决方案。

Abstract: We study the fundamental problem of \emph{moduli selection} in the Robust Chinese Remainder Theorem (RCRT), where each residue may be perturbed by a bounded error. Consider $L$ moduli of the form $m_i = Γ_i m$ ($1 \le i \le L$), where $Γ_i$ are pairwise coprime integers and $m \in \mathbb{R}^+$ is a common scaling factor. For small $L$ ($L = 2, 3, 4$), we obtain exact solutions that maximize the robustness margin under dynamic-range and modulus-bound constraints. We also introduce a Fibonacci-inspired \emph{layered} construction (for $L = 2$) that produces exactly $K$ robust decoding layers, enabling predictable trade-offs between error tolerance and dynamic range. We further analyze how robustness and range evolve across layers and provide a closed-form expression to estimate the success probability under common data and noise models. The results are promising for various applications, such as sub-Nyquist sampling, phase unwrapping, range estimation, modulo analog-to-digital converters (ADCs), and robust residue-number-system (RNS)-based accelerators for deep learning. Our framework thus establishes a general theory of moduli design for RCRT, complementing prior algorithmic work and underscoring the broad relevance of robust moduli design across diverse information-processing domains.

</details>


### [20] [Embodied Intelligent Wireless (EIW): Synesthesia of Machines Empowered Wireless Communications](https://arxiv.org/abs/2511.22845)
*Xiang Cheng,Weibo Wen,Haotian Zhang,Boxun Liu,Zonghui Yang,Jianan Zhang,Xuesong Cai*

Main category: eess.SP

TL;DR: 论文提出了一种名为"具身智能无线通信(EIW)"的新范式，将通信节点重新定义为能够主动感知、适应和与环境交互的智能实体，以应对6G及未来通信系统的需求。


<details>
  <summary>Details</summary>
Motivation: 现有AI原生无线通信设计主要采用静态、模块化的AI替代方案，无法满足未来无线网络对动态环境的持续感知、适应和交互需求。需要一种新的通信范式来弥合这一差距。

Method: 提出EIW范式，基于观察-决策-行动框架：多维观察实现环境全面感知，统一决策模块以可解释方式协调多个无线代理，行动使代理对环境产生实际影响。引入无线世界模型以及自我更新和进化机制作为使能技术。

Result: 通过仿真展示了EIW范式及其使能技术在塑造无线通信节点设计方面的优势，验证了其作为未来通信系统智能实体的可行性。

Conclusion: EIW将未来通信系统重新定义为能够与环境持续交互和共同进化的智能实体，而非被动数据管道，为6G及未来通信系统提供了新的设计方向。

Abstract: The evolution toward the sixth-generation (6G) and beyond mobile communication systems is marked by a fundamental shift from merely connecting devices to enabling pervasive and embodied intelligence. While recent advances in artificial intelligence (AI)-native wireless communication designs have achieved remarkable progress, the prevailing paradigm remains limited to static, modular AI substitutions. This approach fails to meet the core requirements of future wireless networks: the ability to continuously perceive, adapt to, and interact with the dynamic wireless environment. To bridge this gap, this paper introduces Embodied Intelligent Wireless (EIW), a novel communication paradigm inspired by embodied intelligence, which redefines the communication node as an active, environment-aware, and evolving entity. EIW is built around an observation-decision-action paradigm, comprising: multi-dimensional observations for comprehensive awareness of the environment and system states, a unified decision module for orchestrating multiple wireless agents in an interpretable manner, and actions where wireless agents exert practical effects on both the environment and communication systems. Furthermore, two enabling technologies, wireless world models as well as self-update and self-evolution mechanisms, are introduced to support training efficiency improvement, counterfactual evaluation, and better adaptation. Unlike existing communication systems, EIW envisions future communication systems not as passive data pipelines, but as intelligent entities that continuously interact with and co-evolve alongside their environments. Finally, through simulations, we showcase the advantages of the proposed EIW paradigm and its enabling techniques in shaping the design of wireless communication nodes.

</details>


### [21] [RIS-Assisted Physical Layer Security: Artificial Noise-Driven Optimization and Measurements](https://arxiv.org/abs/2511.22910)
*Ahmet Muaz Aktas,Sefa Kayraklik,Sultangali Arzykulov,Galymzhan Nauryzbayev,Ibrahim Hokelek,Ali Gorcin*

Main category: eess.SP

TL;DR: 本文研究了一种基于人工噪声的RIS辅助安全通信系统，通过分割RIS并优化相位配置，在保证合法用户通信的同时将人工噪声导向窃听者，显著提升了保密容量。


<details>
  <summary>Details</summary>
Motivation: 可重构智能表面（RIS）在下一代无线通信网络中具有提供信号覆盖、能效、可靠通信和物理层安全（PLS）的潜力。然而，如何有效利用RIS增强物理层安全，特别是在存在窃听者的情况下，是一个重要研究问题。本文旨在通过人工噪声（AN）驱动的方法，利用RIS增强通信系统的安全性。

Method: 提出一种AN驱动的RIS辅助安全通信系统，将RIS分割为两个部分：第一部分配置为将通信信号导向合法用户（Bob），第二部分配置为将人工噪声导向窃听者（Eve）。开发了迭代和基于离散傅里叶变换的算法进行RIS相位优化。同时优化通信信号和人工噪声之间的功率分配，以最大化保密容量并限制Eve的信道容量。

Result: 通过仿真和基于软件定义无线电的测试台实验评估了所提出的PLS框架。结果表明，该方法在保密容量方面取得了显著改进，验证了AN驱动的RIS辅助PLS在实际部署中的潜力。

Conclusion: AN驱动的RIS辅助物理层安全框架能够有效提升无线通信系统的安全性。通过智能分割RIS并优化相位配置和功率分配，可以在保证合法用户通信质量的同时，有效抑制窃听者的信道容量。该方法具有实际部署的潜力，为下一代无线通信网络的安全增强提供了新思路。

Abstract: Reconfigurable intelligent surface (RIS) has emerged as a key enabler for providing signal coverage, energy efficiency, reliable communication, and physical layer security (PLS) in next-generation wireless communication networks. This paper investigates an artificial noise (AN)-driven RIS-assisted secure communication system. The RIS is partitioned into two segments, where the first segment is configured to direct the communication signal (CS) toward the legitimate user (Bob), and the other one is configured to steer the AN toward the eavesdropper (Eve). To this end, iterative and discrete Fourier transform-based algorithms are developed for practical RIS phase shift optimization. The power allocation between the CS and the AN signals is optimized in such a way that the secrecy capacity (SC) is maximized while limiting Eve's channel capacity. The proposed PLS framework is evaluated through both simulations and software defined radio based testbed experiments. The results demonstrate promising improvements in the SC, highlighting the potential of AN-driven RIS-assisted PLS for practical deployments.

</details>


### [22] [RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN](https://arxiv.org/abs/2511.22933)
*Onur Salan,Burak Çırağ,Onur Sever,İbrahim Hökelek,Ali Görçin,Hakan Ali Çırpan*

Main category: eess.SP

TL;DR: 提出基于检索增强生成（RAG）的大型语言模型（ReLLM）驱动的动态无线资源管理框架，用于O-RAN启发的6G网络，通过双智能体设计实现网络切片自适应控制，减少不必要的LLM推理调用。


<details>
  <summary>Details</summary>
Motivation: 人工智能在无线通信领域的进展对资源管理具有重要意义。传统方法难以有效处理网络切片动态资源分配和SLA保障问题，需要更智能的自适应控制方案。

Method: 提出ReLLM框架，包含两个专用智能体：一个负责主动检测SLA违规（持续监控切片性能指标），另一个负责动态重新分配物理资源块（当SLA违规概率超过阈值时）。利用历史数据和实时网络数据进行自适应控制。

Result: 在基于开源OpenAirInterface仿真器的端到端O-RAN测试平台上验证，ReLLM方法保持低优先级切片接近零的丢包率，同时满足高优先级切片的可接受延迟性能，提高了可靠性和SLA合规性。

Conclusion: ReLLM驱动的设计提高了计算和能源效率，减少了不必要的LLM推理调用，证实了其在真实O-RAN测试平台中的实用性，对未来6G网络具有潜在适用性。

Abstract: Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management. In this paper, a Retrieval-Augmented Generation (RAG)-empowered Large Language Model (ReLLM)-driven dynamic radio resource management framework for Open Radio Access Network (O-RAN) inspired 6G networks is proposed. The introduced methodology leverages the ReLLM framework to interpret both historical and real-time network data, enabling adaptive control of network slices. The ReLLM is founded on two specialized agents, one is responsible for proactively detecting service level agreement (SLA) violations by continuously monitoring and estimating slice-specific performance metrics, and the other one is responsible for dynamically reallocating physical resource blocks when the SLA violation probability exceeds a pre-defined threshold. The primary objective of this dual-agent design is to minimize unnecessary LLM inference calls while satisfying the SLA requirements of the slices, thereby improving computational and energy efficiency. The proposed ReLLM framework is implemented and validated on an end-to-end O-RAN testbed built upon open-source OpenAirInterface emulators. The experimental results demonstrate that the LLM approach with its reduced token consumption feature maintains a near-zero drop ratio for the low-priority slice while simultaneously satisfying acceptable latency performance for the high-priority slice. The ReLLM-driven design improves reliability and SLA compliance, confirming its practicality for real-world O-RAN testbeds and its potential applicability to future 6G networks.

</details>


### [23] [DoA Estimation with Sparse Arrays: Effects of Antenna Element Patterns and Nonidealities](https://arxiv.org/abs/2511.23028)
*Niko Lindvall,Mikko Heino,Robin Rajamäki,Mikko Valkama,Visa Koivunen*

Main category: eess.SP

TL;DR: 研究定向天线单元复杂增益模式和非理想性对波达方向估计的影响，比较稀疏阵列和均匀线性阵列，发现稀疏阵列结合定向天线可显著提升DoA估计精度和覆盖距离。


<details>
  <summary>Details</summary>
Motivation: 研究天线单元方向性增益模式和非理想因素对波达方向估计性能的影响，特别是在稀疏阵列配置下的表现，以提升方向估计精度和系统灵敏度。

Method: 使用电磁仿真工具精确建模贴片天线和Vivaldi天线单元的电磁行为，包括互耦效应；比较稀疏阵列和经典均匀线性阵列；采用MUSIC算法估计两个信号源的波达方向。

Result: 稀疏阵列配置结合定向天线单元相比均匀全向阵列（8个单元）可将平均方向估计误差降低90%以上；在固定角度RMSE下，阵列灵敏度提升使单向覆盖距离增加4-15倍；最佳性能由稀疏阵列配合贴片或Vivaldi天线分别在100°和120°视场下获得。

Conclusion: 定向天线单元和稀疏阵列配置能显著提升波达方向估计性能，特别是在多源场景下，为实际系统设计提供了重要指导。

Abstract: This paper studies the effects of directional antenna element complex gain patterns and nonidealities in direction of arrival (DoA) estimation. We compare sparse arrays and classical uniform linear arrays, harnessing EM simulation tools to accurately model the electromagnetic behavior of both patch and Vivaldi antenna element including mutual coupling effects. We show that with sparse array configurations, the performance impacts are significant in terms of DoA estimation accuracy and operable SNR ranges. Specifically, in the scenarios considered, both the usage of directional antenna elements and a sparse array result in over 90% reduction in average direction finding error, compared to a uniform omnidirectional array with the same number of elements (in this case eight), when estimating the directions of two sources using the MUSIC algorithm. For a fixed angular RMSE, the improvements in array sensitivity are shown to yield a 4 to 15-fold increase in one-way coverage distance (assuming free-space path loss). Among the studied options, the best performance was obtained using sparse arrays with either patch or Vivaldi elements for field of views of 100$^\circ$ or 120$^\circ$, respectively.

</details>


### [24] [Harnessing Chaotic Signals for Wireless Information and Power Transfer](https://arxiv.org/abs/2511.23049)
*Priyadarshi Mukherjee,Constantinos Psomas,Ioannis Krikidis*

Main category: eess.SP

TL;DR: 该论文探讨了混沌信号在无线能量传输(WPT)和同时无线信息与功率传输(SWIPT)中的应用，特别针对6G通信系统的自持续网络需求，提出了基于差分混沌键控(DCSK)的接收器架构和波形设计。


<details>
  <summary>Details</summary>
Motivation: 随着6G通信标准的到来，自持续无线网络变得日益重要。混沌信号因其固有的随机性和对初始条件的高度敏感性，在无线能量传输中表现出优越性能，这为6G系统中的大规模无线设备和传感器提供了理想的能量传输解决方案。

Method: 1. 表征多维混沌信号的广义WPT性能，以Lorenz和Henon混沌系统为例；2. 提出适用于增强能量收集的DCSK-based WPT接收器架构；3. 为多天线SWIPT架构设计DCSK-based发射波形，并研究速率-能量权衡的影响。

Result: 实验观察表明，混沌信号在WPT性能上优于现有基准方案。论文系统性地分析了混沌信号在WPT和SWIPT中的应用潜力，为6G通信系统的自持续网络提供了理论基础和技术方案。

Conclusion: 混沌信号在无线能量传输和同时信息功率传输中具有显著优势，特别是在6G通信系统的自持续网络背景下。提出的DCSK-based架构和波形设计为未来大规模无线设备的高效能量收集提供了有前景的解决方案。

Abstract: Chaotic dynamical systems have attracted considerable attention due to their inherent randomness and high sensitivity to initial conditions, which makes them ideal for secure wireless communications. Beyond security, these same characteristics also make chaotic signals particularly effective for wireless power transfer (WPT) applications. On the other hand, connectivity along with self-sustainability are the two cornerstones of the upcoming sixth generation (6G) standard for radio communications. Consequently, with the massive increase in wireless devices and sensors, the concept of self-sustainable wireless networks is becoming more relevant. The aspect of WPT to the widely spread wireless devices and simultaneous wireless information and power transfer (SWIPT) among these devices will play a crucial role in the 6G communication systems. In this context, it has been experimentally observed that chaotic signals result in better WPT performance as compared to the existing benchmark schemes. Hence, in this paper, we characterize the generalized WPT performance of the multi-dimensional chaotic signals and present the use case of the Lorenz and the Henon chaotic systems. Moreover, we provide a novel differential chaos shift keying (DCSK)-based WPT receiver architecture ideal for enhanced energy harvesting (EH). Furthermore, we propose DCSK-based transmit waveform designs for multi-antenna SWIPT architectures and investigate the impact of the rate-energy trade-off. Our goal is to explore these aspects of the chaotic signals and discuss their relevance in the context of both WPT and SWIPT.

</details>


### [25] [What If They Took the Shot? A Hierarchical Bayesian Framework for Counterfactual Expected Goals](https://arxiv.org/abs/2511.23072)
*Mikayil Mahmudlu,Oktay Karakuş,Hasan Arkadaş*

Main category: eess.SP

TL;DR: 提出分层贝叶斯框架，整合专家知识量化球员特定效应，改进传统xG模型将所有球员视为相同射手的局限


<details>
  <summary>Details</summary>
Motivation: 传统预期进球（xG）模型将所有球员视为相同射手，忽略了球员个人射门能力的差异，无法准确评估球员特定效应

Method: 使用分层贝叶斯逻辑回归，结合StatsBomb 2015-16赛季9,970次射门数据和Football Manager 2017评分，通过信息先验稳定球员级估计

Result: 模型减少后验不确定性，外部验证R²达0.833；识别出球员特化类型（一对一、远射、第一触球）；发现表现不佳球员的潜在能力；支持反事实分析

Conclusion: 该框架为球员评估、招募和战术规划提供不确定性感知工具，适用于个人技能和情境因素共同影响表现的领域

Abstract: This study develops a hierarchical Bayesian framework that integrates expert domain knowledge to quantify player-specific effects in expected goals (xG) estimation, addressing a limitation of standard models that treat all players as identical finishers. Using 9,970 shots from StatsBomb's 2015-16 data and Football Manager 2017 ratings, we combine Bayesian logistic regression with informed priors to stabilise player-level estimates, especially for players with few shots. The hierarchical model reduces posterior uncertainty relative to weak priors and achieves strong external validity: hierarchical and baseline predictions correlate at R2 = 0.75, while an XGBoost benchmark validated against StatsBomb xG reaches R2 = 0.833. The model uncovers interpretable specialisation profiles, including one-on-one finishing (Aguero, Suarez, Belotti, Immobile, Martial), long-range shooting (Pogba), and first-touch execution (Insigne, Salah, Gameiro). It also identifies latent ability in underperforming players such as Immobile and Belotti. The framework supports counterfactual "what-if" analysis by reallocating shots between players under identical contexts. Case studies show that Sansone would generate +2.2 xG from Berardi's chances, driven largely by high-pressure situations, while Vardy-Giroud substitutions reveal strong asymmetry: replacing Vardy with Giroud results in a large decline (about -7 xG), whereas the reverse substitution has only a small effect (about -1 xG). This work provides an uncertainty-aware tool for player evaluation, recruitment, and tactical planning, and offers a general approach for domains where individual skill and contextual factors jointly shape performance.

</details>


### [26] [Physical Layer Security with Artificial Noise in MIMO Pinching-Antenna Systems](https://arxiv.org/abs/2511.23079)
*Pigi P. Papanikolaou,Dimitrios Bozanis,Sotiris A. Tegos,Panagiotis D. Diamantoulakis,Panagiotis Sarigiannidis,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 提出一种用于Pinching天线系统的AN辅助波束成形框架，通过联合优化信息波束、AN协方差和PA位置来最大化保密速率，适用于完美和不完美CSI场景。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中安全成为关键性能指标，传统MIMO系统存在严重路径损耗且易受附近窃听者攻击。Pinching天线系统通过可重构天线沿低损耗介质波导布置，能够增强信道条件并动态缓解安全威胁。

Method: 提出AN辅助波束成形框架，联合优化信息波束、AN协方差和PA位置。针对窃听者信道，考虑完美和不完美CSI两种情况。对于不完美CSI，将位置误差通过雅可比映射到椭圆信道不确定性集合。针对单波导场景推导闭式解，针对多波导多用户场景开发DNN辅助联合优化器。

Result: 数值结果表明，所提方案在单用户和多用户设置下，无论是完美还是不完美CSI场景，均能持续提升保密速率，优于PAS基线方法。

Conclusion: 提出的AN辅助波束成形框架有效解决了PAS系统中的安全问题，通过联合优化天线位置和波束成形策略，在多种场景下显著提升了保密性能。

Abstract: As next-generation wireless networks emerge, security is becoming a critical performance metric. However, conventional multiple-input-multiple-output (MIMO) systems often suffer from severe path loss and are vulnerable to nearby eavesdroppers due to their fixed-antenna configurations. Pinching-antenna systems (PASs) offer a promising alternative, leveraging reconfigurable pinching antennas (PAs) positioned along low-loss dielectric waveguides to enhance channel conditions and dynamically mitigate security threats. In this paper, we propose an artificial noise (AN)-aided beamforming framework for the PAS downlink that maximizes the secrecy rate (SR) by jointly optimizing the information beams, the AN covariance, and the PA positions. We examine both perfect and imperfect channel state information (CSI) for the eavesdropper's channel. For the latter, location errors are mapped via a Jacobian into an ellipsoidal channel uncertainty set to accurately formulate the problem. We derive a closed-form solution for the single-waveguide scenario, yielding the optimal PA location and an information/AN power-splitting rule. For multiple waveguides and users, we develop a deep neural network (DNN)-aided joint optimizer that outputs beams, AN, and PA placements. Numerical results demonstrate that the proposed scheme improves SR consistently over PAS baselines in single- and multi-user settings under both perfect and imperfect CSI.

</details>


### [27] [Joint Optimization of Pilot Length, Pilot Assignment, and Power Allocation for Cell-free MIMO Systems with Graph Neural Networks](https://arxiv.org/abs/2511.23128)
*Yao Peng,Tingting Liu,Chenyang Yang*

Main category: eess.SP

TL;DR: 提出基于图神经网络的用户中心无小区多天线系统联合优化方法，通过优化导频长度、导频分配和功率分配来最大化净频谱效率，解决了传统方法中固定导频长度导致的导频开销与污染平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在用户中心无小区多天线系统中，导频污染严重降低频谱效率。现有工作假设固定导频长度来联合优化导频分配和功率分配，但无法平衡导频开销与污染之间的权衡。为了最大化净频谱效率，需要联合优化导频长度、导频分配和功率分配。

Method: 采用深度学习方法，设计尺寸可泛化的图神经网络。针对导频长度可变导致导频分配矩阵大小未知的挑战，引入特征增强解决一对一映射问题，设计污染感知注意力机制提升学习性能。考虑到导频分配和功率分配分别依赖于大尺度和小尺度信道，开发了双时间尺度GNN框架，同时设计了单时间尺度GNN以减少推理时间。

Result: 仿真结果表明，所设计的GNN在净频谱效率、训练复杂度和推理时间方面优于现有方法，并且能够在不同问题规模和信道条件下良好泛化。

Conclusion: 提出的基于图神经网络的联合优化框架有效解决了用户中心无小区系统中导频污染问题，通过联合优化导频长度、分配和功率分配，显著提升了净频谱效率，同时具有良好的泛化能力和计算效率。

Abstract: In user-centric cell-free multi-antenna systems, pilot contamination degrades spectral efficiency (SE) severely. To mitigate pilot contamination, existing works jointly optimize pilot assignment and power allocation by assuming fixed pilot length, which fail to balance pilot overhead against the contamination. To maximize net-SE, we jointly optimize pilot length, pilot assignment, and power allocation with deep learning. Since the pilot length is a variable, the size of pilot assignment matrix is unknown during the optimization. To cope with the challenge, we design size-generalizable graph neural networks (GNNs). We prove that pilot assignment policy is a one-to-many mapping, and improperly designed GNNs cannot learn the optimal policy. We tackle this issue by introducing feature enhancement. To improve learning performance, we design a contamination-aware attention mechanism for the GNNs. Given that pilot assignment and power allocation respectively depend on large- and small-scale channels, we develop a dual-timescale GNN framework to explore the potential. To reduce inference time, a single-timescale GNN is also designed. Simulation results show that the designed GNNs outperform existing methods in terms of net-SE, training complexity, and inference time, and can be well generalized across problem scales and channels.

</details>


### [28] [Near-Field Channel Estimation and Joint Angle-Range Recovery in XL-MIMO Systems: A Gridless Super-Resolution Approach](https://arxiv.org/abs/2511.23187)
*Feng Xi,Dehui Yang*

Main category: eess.SP

TL;DR: 提出一种无需码本的近场信道估计网格化超分辨率框架，通过二阶近似和提升技术将非凸问题转化为凸优化，实现精确的联合角度-距离估计。


<details>
  <summary>Details</summary>
Motivation: 现有XL-MIMO近场信道估计方法通常联合离散化角度和距离参数，导致极域码本过大，且存在基失配问题。需要一种无需显式码本构建、避免二维网格搜索的精确估计方法。

Method: 1) 采用球面波导向矢量的二阶近似，将近场信道表示为未知波形调制的复指数叠加；2) 证明波形位于公共离散啁啾率子空间；3) 利用提升技术将非凸问题转化为正则化原子范数最小化的凸优化问题；4) 从凸解获得网格化角度估计和闭式粗距离估计；5) 在精确球面模型下使用梯度非线性最小二乘进行细化。

Result: 方法避免了基失配和穷举二维网格搜索，在稀疏多径场景下，导频开销随阵列规模亚线性增长，实现了精确的信道重建和用户定位。仿真验证了在典型近场场景下的准确性能。

Conclusion: 提出了一种创新的网格化近场信道估计框架，通过利用信道结构特性和凸优化技术，实现了高效精确的联合角度-距离估计，为XL-MIMO系统提供了实用的近场信道估计解决方案。

Abstract: Existing near-field channel estimation methods for extremely large-scale MIMO (XL-MIMO) typically discretize angle and range parameters jointly, resulting in large polar-domain codebooks. This paper proposes a novel framework that formulates near-field channel estimation as a gridless super-resolution problem, eliminating the need for explicitly constructed codebooks. By employing a second-order approximation of spherical-wave steering vectors, the near-field channel is represented as a superposition of complex exponentials modulated by unknown waveforms. We demonstrate that these waveforms lie tightly in a common discrete chirp rate (DCR) subspace, with a dimension that scales as $Θ(\sqrt{N})$ for an $N$-element array. By leveraging this structure and applying a lifting technique, we reformulate the non-convex problem as a convex program using regularized atomic norm minimization, which admits an equivalent semidefinite program. From the solution to the convex program, we obtain gridless angle estimates and derive closed-form coarse range estimates, followed by refinement under the exact spherical model using gradient-based nonlinear least squares. The proposed method avoids basis mismatch and exhaustive two-dimensional grid searches while enabling accurate joint angle-range estimation with pilot budgets that scale sublinearly with array size in sparse multipath regimes. Simulations demonstrate accurate channel reconstruction and user localization across representative near-field scenarios.

</details>


### [29] [A Framework for Statistical Geometric Channel Model for ISAC Systems](https://arxiv.org/abs/2511.23201)
*Ali Waqar Azim,Ahmad Bazzi,Theodore S. Rappaport,Marwa Chafii*

Main category: eess.SP

TL;DR: 提出针对双基地系统的几何统计ISAC信道模型，将信道分解为目标信道和背景信道，扩展TR38.901标准，保持通信性能同时支持感知评估


<details>
  <summary>Details</summary>
Motivation: 现有的通信信道模型（如TR38.901）主要针对通信优化，缺乏对感知功能的支持。随着集成感知与通信（ISAC）技术的发展，需要能够同时评估通信和感知性能的统一信道模型，特别是在双基地系统中。

Method: 提出双分量模型：1）目标信道（包含所有由感知目标产生的多径分量，参数化目标的雷达截面和散射点）；2）背景信道（包含所有不与感知目标交互的传播路径）。采用混合聚类方法扩展TR38.901，整合时空一致的确定性聚类和随机聚类，保持信道互易性和绝对时延对齐。

Result: 在UMa、UMi和InF场景下的广泛仿真表明，该模型在通信性能（误码率、信道容量）上与TR38.901标准保持同等水平，同时支持感知性能评估（目标测距误差、检测概率的ROC曲线）。

Conclusion: 该框架为双基地ISAC系统提供了一个统一的几何统计信道模型，既能保持通信性能，又能支持感知功能评估，填补了现有信道模型在ISAC应用中的空白。

Abstract: This paper proposes a comprehensive framework for a geometry-based statistical model for integrated sensing and communication (ISAC) tailored for bistatic systems. Our dual-component model decomposes the ISAC channel into a target channel encompassing all multipath components produced by a sensing target parameterized by the target's radar cross-section and scattering points, and a background channel comprising all other propagation paths that do not interact with the sensing target. The framework extends TR38.901 via a hybrid clustering approach, integrating spatiotemporally consistent deterministic clusters with stochastic clusters to preserve channel reciprocity and absolute delay alignment for sensing parameter estimation. Extensive simulations across urban macro, urban micro, and indoor factory scenarios demonstrate that the model maintains communication performance parity with the standard TR38.901, validated through bit-error rate analysis obtained via simulated and measured ISAC channels and channel capacity assessment, while enabling sensing performance evaluation, such as target ranging error for localization and receiver operating characteristic curves for detection probability.

</details>


### [30] [Robust HRRP Recognition under Interrupted Sampling Repeater Jamming using a Prior Jamming Information-Guided Network](https://arxiv.org/abs/2511.23256)
*Guozheng Sun,Lei Wang,Yanhao Wang,Jie Wang,Yimin Liu*

Main category: eess.SP

TL;DR: 提出一种基于先验干扰信息的稳健HRRP识别方法，通过点扩散函数建模ISRJ干扰引起的HRRP失真，利用先验引导的特征交互模块和混合损失函数增强模型判别能力。


<details>
  <summary>Details</summary>
Motivation: 在电子对抗（ECM）环境下，特别是面对主流的中断采样转发干扰（ISRJ），高分辨率距离像（HRRP）往往遭受严重的特征失真，这使得雷达自动目标识别（RATR）面临重大挑战。

Method: 引入点扩散函数（PSF）作为先验信息来建模ISRJ引起的HRRP失真；设计识别网络，通过先验引导的特征交互模块和混合损失函数来利用这一先验信息，增强模型的判别能力。

Result: 模拟和实测数据实验表明，该方法在面临未见过的干扰参数时，始终优于最先进的方法，并展现出更强的泛化能力。

Conclusion: 通过利用先验干扰信息，该方法能够学习不同干扰参数下失真HRRP中的不变特征，有效提升电子对抗环境下雷达自动目标识别的鲁棒性。

Abstract: Radar automatic target recognition (RATR) based on high-resolution range profile (HRRP) has attracted increasing attention due to its ability to capture fine-grained structural features. However, recognizing targets under electronic countermeasures (ECM), especially the mainstream interrupted-sampling repeater jamming (ISRJ), remains a significant challenge, as HRRPs often suffer from serious feature distortion. To address this, we propose a robust HRRP recognition method guided by prior jamming information. Specifically, we introduce a point spread function (PSF) as prior information to model the HRRP distortion induced by ISRJ. Based on this, we design a recognition network that leverages this prior through a prior-guided feature interaction module and a hybrid loss function to enhance the model's discriminative capability. With the aid of prior information, the model can learn invariant features within distorted HRRP under different jamming parameters. Both the simulated and measured-data experiments demonstrate that our method consistently outperforms state-of-the-art approaches and exhibits stronger generalization capabilities when facing unseen jamming parameters.

</details>


### [31] [Hierarchical Feature Integration for Multi-Signal Automatic Modulation Recognition](https://arxiv.org/abs/2511.23258)
*Yunpeng Qu,Yazhou Sun,Bingyu Hui,Jian Wang*

Main category: eess.SP

TL;DR: 提出HIFI-YOLO框架，用于多信号联合检测和调制识别，解决实际信道中多信号叠加问题


<details>
  <summary>Details</summary>
Motivation: 现有自动调制识别研究主要关注单信号识别，忽略了实际信道中多信号叠加现象以及信号检测预处理步骤。RF信号易受噪声干扰且频谱变化显著，需要解决多信号联合检测与识别问题。

Method: 提出分层特征集成(HIFI)-YOLO框架，通过独特的分层特征集成设计，增强不同模块的特征表示能力。构建大规模AMR数据集，包含多种数字和模拟调制方案，模拟实际传播条件下多信号共存或重叠场景。

Result: 在构建的数据集上进行大量实验，证明HIFI-YOLO在多信号检测和调制识别联合方法中表现出优异性能。

Conclusion: HIFI-YOLO框架通过分层特征集成有效提升了多信号联合检测和调制识别性能，为实际无线通信系统中的信号处理提供了有效解决方案。

Abstract: Automatic modulation recognition (AMR) is a crucial step in wireless communication systems, which identifies the modulation scheme from detected signals to provide key information for further processing. However, previous work has mainly focused on the identification of a single signal, overlooking the phenomenon of multiple signal superposition in practical channels and the signal detection procedures that must be conducted beforehand. Considering the susceptibility of radio frequency (RF) signals to noise interference and significant spectral variations, we propose a novel Hierarchical Feature Integration (HIFI)-YOLO framework for multi-signal joint detection and modulation recognition. Our HIFI-YOLO framework, with its unique design of hierarchical feature integration, effectively enhances the representation capability of features in different modules, thereby improving detection performance. We construct a large-scale AMR dataset specifically tailored for scenarios of the coexistence or overlapping of multiple signals transmitted through channels with realistic propagation conditions, consisting of diverse digital and analog modulation schemes. Extensive experiments on our dataset demonstrate the excellent performance of HIFI-YOLO in multi-signal detection and modulation recognition as a joint approach.

</details>


### [32] [Compensation of correlated autoregressive clock jitter in arrays of Analog-to-Digital Converters](https://arxiv.org/abs/2511.23351)
*Daniele Gerosa,Lauri Anttila,Thomas Eriksson*

Main category: eess.SP

TL;DR: 提出基于VAR(1)模型的ADC阵列时钟抖动联合跟踪与补偿方法，使用导频音卡尔曼平滑器显著降低抖动失真


<details>
  <summary>Details</summary>
Motivation: 现代通信系统中ADC的保真度受限于采样时钟抖动，传统标量模型将抖动视为独立高斯噪声，忽略了实际ADC中的时间相关性和MIMO ADC中的空间互相关性

Method: 将抖动建模为一阶向量自回归过程(VAR(1))，提出基于导频音的卡尔曼平滑器来联合跟踪和补偿ADC阵列中的随机互相关时序误差

Result: 仿真结果表明，该方法在各种场景下都能显著降低抖动引起的失真

Conclusion: 通过将抖动建模为VAR(1)过程并使用导频音卡尔曼平滑器，能够有效跟踪和补偿ADC阵列中的互相关时序误差，提高ADC系统的保真度

Abstract: In modern communication systems, the fidelity of analog-to-digital converters (ADCs) is limited by sampling clock jitter, i.e., small random timing deviations that undermine ideal sampling. Traditional scalar models often treat jitter as independent Gaussian noise, which makes it essentially untrackable, whereas real ADCs also exhibit temporally correlated (spectrally colored) imperfections. Moreover, spatial cross-correlations between channels in multiple-input multiple-output (MIMO) ADCs are commonly neglected. This paper addresses the joint tracking and compensation of random, cross-correlated timing errors in ADC arrays by modeling jitter as a coupled vector autoregressive process of order one (VAR(1)). We propose a pilot-tone-based Kalman smoother to track and compensate the jitter, and simulations demonstrate substantial reductions in jitter-induced distortion across diverse scenarios.

</details>


### [33] [EMF-Compliant Power Control in Cell-Free Massive MIMO: Model-Based and Data-Driven Approaches](https://arxiv.org/abs/2511.23357)
*Sergi Liesegang,Stefano Buzzi*

Main category: eess.SP

TL;DR: 该论文研究了在电磁场暴露限制下，用户中心的无蜂窝大规模MIMO系统中的功率控制问题，提出了模型驱动和数据驱动两种方法，在满足EMF约束的同时优化最小数据速率。


<details>
  <summary>Details</summary>
Motivation: 随着无线数据网络的快速增长，电磁污染问题和电磁场暴露限制的遵守变得日益重要。论文旨在解决在EMF约束下，用户中心的无蜂窝大规模MIMO系统中的功率控制问题。

Method: 提出了模型驱动和数据驱动两种方法。模型驱动方法中，下行链路采用连续凸优化和对数-求和-指数近似，上行链路使用常规技术。数据驱动方法则探索了端到端架构和深度展开技术。

Result: 数值结果表明，提出的模型驱动方法能有效满足EMF约束同时保证良好性能；数据驱动方法能够紧密逼近模型驱动方法的性能，但计算复杂度显著降低。

Conclusion: 该论文成功解决了EMF约束下的功率控制问题，提出的模型驱动和数据驱动方法都能在满足电磁场暴露限制的同时优化系统性能，为实际部署提供了有效解决方案。

Abstract: The impressive growth of wireless data networks has recently led to increased attention to the issue of electromagnetic pollution and the fulfillment of electromagnetic field (EMF) exposure limits. This paper tackles the problem of power control in user-centric cell-free massive multiple-input-multiple-output (CF-mMIMO) systems under EMF constraints. Specifically, the power allocation maximizing the minimum data rate across users is derived for both the uplink and the downlink. To solve such optimization problems, two approaches are proposed, i.e., model-based and data-driven. The proposed model-based solutions for the downlink utilize successive convex optimization and the log-sum-exp approximation for the minimum of a discrete set, whereas ordinary techniques are employed for the uplink. With regard to data-driven solutions, solutions based on both end-to-end architectures and deep unfolding techniques are explored. Extensive numerical results confirm that the proposed model-based solutions effectively fulfill the EMF constraints while ensuring very good performance; moreover, the results show that the proposed data-driven approaches can tightly approximate the performance of model-based solutions but with much lower computational complexity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Artificial intelligence for methane detection: from continuous monitoring to verified mitigation](https://arxiv.org/abs/2511.21777)
*Anna Allen,Gonzalo Mateo-Garcia,Itziar Irakulis-Loitxate,Manuel Montesino-San Martin,Marc Watine,James Requeima,Javier Gorroño,Cynthia Randles,Tharwat Mokalled,Luis Guanter,Richard E. Turner,Claudio Cifarelli,Manfredi Caltagirone*

Main category: cs.LG

TL;DR: MARS-S2L机器学习模型利用多光谱卫星图像检测甲烷排放，每两天提供高分辨率检测，能在697个未见站点识别78%的羽流，已向20个国家发出1015次通知，实现可验证的甲烷减排。


<details>
  <summary>Details</summary>
Motivation: 甲烷是强效温室气体，少数大型点源排放占比过高，针对这些站点减排潜力巨大。目前大规模检测和归因甲烷排放仍具挑战，需要有效方法通知资产所有者。

Method: 开发MARS-S2L机器学习模型，基于超过80,000张手动标注的多光谱卫星图像训练，能够每两天提供高分辨率甲烷排放检测，实现设施级别的排放归因。

Result: 在697个未见测试站点上，模型以8%的误报率检测到78%的甲烷羽流。已向20个国家发出1,015次通知，成功验证并永久减少了6个持续排放源，包括利比亚一个先前未知的站点。

Conclusion: MARS-S2L展示了从卫星检测到可量化甲烷减排的可扩展路径，为大规模甲烷排放监测和减排提供了有效工具。

Abstract: Methane is a potent greenhouse gas, responsible for roughly 30\% of warming since pre-industrial times. A small number of large point sources account for a disproportionate share of emissions, creating an opportunity for substantial reductions by targeting relatively few sites. Detection and attribution of large emissions at scale for notification to asset owners remains challenging. Here, we introduce MARS-S2L, a machine learning model that detects methane emissions in publicly available multispectral satellite imagery. Trained on a manually curated dataset of over 80,000 images, the model provides high-resolution detections every two days, enabling facility-level attribution and identifying 78\% of plumes with an 8\% false positive rate at 697 previously unseen sites. Deployed operationally, MARS-S2L has issued 1,015 notifications to stakeholders in 20 countries, enabling verified, permanent mitigation of six persistent emitters, including a previously unknown site in Libya. These results demonstrate a scalable pathway from satellite detection to quantifiable methane mitigation.

</details>


### [35] [Test Time Training for AC Power Flow Surrogates via Physics and Operational Constraint Refinement](https://arxiv.org/abs/2511.22343)
*Panteleimon Dogoulis,Mohammad Iman Alizadeh,Sylvain Kubler,Maxime Cordy*

Main category: cs.LG

TL;DR: 论文提出了一种物理信息测试时训练（PI-TTT）框架，通过推理时强制实施交流潮流等式和运行约束，提升基于机器学习的潮流计算代理模型的准确性和可行性。


<details>
  <summary>Details</summary>
Motivation: 基于机器学习的潮流计算虽然比传统数值方法有计算优势，但通常难以保持完全的物理一致性。需要一种方法在保持计算效率的同时，确保物理约束的满足。

Method: 提出物理信息测试时训练（PI-TTT）框架，在推理时通过少量梯度更新对代理模型输出进行轻量级自监督精炼，直接强制执行交流潮流等式和运行约束，无需标记数据即可适应未见运行条件。

Result: 在IEEE 14、118、300节点系统和PEGASE 1354节点网络上的实验表明，PI-TTT将潮流残差和运行约束违反降低了1-2个数量级，同时保持了机器学习模型的计算优势。

Conclusion: PI-TTT提供了快速、准确且物理可靠的预测，代表了电力系统分析中可扩展且物理一致学习的有前景方向。

Abstract: Power Flow (PF) calculation based on machine learning (ML) techniques offer significant computational advantages over traditional numerical methods but often struggle to maintain full physical consistency. This paper introduces a physics-informed test-time training (PI-TTT) framework that enhances the accuracy and feasibility of ML-based PF surrogates by enforcing AC power flow equalities and operational constraints directly at inference time. The proposed method performs a lightweight self-supervised refinement of the surrogate outputs through few gradient-based updates, enabling local adaptation to unseen operating conditions without requiring labeled data. Extensive experiments on the IEEE 14-, 118-, and 300-bus systems and the PEGASE 1354-bus network show that PI-TTT reduces power flow residuals and operational constraint violations by one to two orders of magnitude compared with purely ML-based models, while preserving their computational advantage. The results demonstrate that PI-TTT provides fast, accurate, and physically reliable predictions, representing a promising direction for scalable and physics-consistent learning in power system analysis.

</details>


### [36] [Physics-Informed Spiking Neural Networks via Conservative Flux Quantization](https://arxiv.org/abs/2511.21784)
*Chi Zhang,Lin Wang*

Main category: cs.LG

TL;DR: 提出物理信息脉冲神经网络(PISNN)框架，通过保守泄漏积分发放(C-LIF)神经元和保守通量量化(CFQ)策略，实现严格物理守恒和长期泛化，为边缘设备提供高效物理预测。


<details>
  <summary>Details</summary>
Motivation: 边缘设备需要实时、物理一致的预测，但现有物理信息神经网络(PINNs)能耗高且难以严格保证物理守恒定律。脉冲神经网络(SNNs)适合边缘计算，但简单转换会降低物理保真度。

Method: 提出PISNN框架：1) 设计C-LIF神经元，其动力学结构保证局部质量守恒；2) 提出CFQ策略，将神经脉冲重新定义为物理通量的离散包，学习时不变物理演化算子。

Result: 在1D热方程和2D拉普拉斯方程等基准测试中表现出色，准确模拟系统动力学，同时通过设计保持完美的质量守恒，优于传统PINNs。

Conclusion: 建立了科学计算严谨性与神经形态工程效率融合的稳健框架，为智能系统复杂、长期、节能的物理预测铺平道路。

Abstract: Real-time, physically-consistent predictions on low-power edge devices is critical for the next generation embodied AI systems, yet it remains a major challenge. Physics-Informed Neural Networks (PINNs) combine data-driven learning with physics-based constraints to ensure the model's predictions are with underlying physical principles.However, PINNs are energy-intensive and struggle to strictly enforce physical conservation laws. Brain-inspired spiking neural networks (SNNs) have emerged as a promising solution for edge computing and real-time processing. However, naively converting PINNs to SNNs degrades physical fidelity and fails to address long-term generalization issues. To this end, this paper introduce a novel Physics-Informed Spiking Neural Network (PISNN) framework. Importantly, to ensure strict physical conservation, we design the Conservative Leaky Integrate-and-Fire (C-LIF) neuron, whose dynamics structurally guarantee local mass preservation. To achieve robust temporal generalization, we introduce a novel Conservative Flux Quantization (CFQ) strategy, which redefines neural spikes as discrete packets of physical flux. Our CFQ learns a time-invariant physical evolution operator, enabling the PISNN to become a general-purpose solver -- conservative-by-construction. Extensive experiments show that our PISNN excels on diverse benchmarks. For both the canonical 1D heat equation and the more challenging 2D Laplace's Equation, it accurately simulates the system dynamics while maintaining perfect mass conservation by design -- a feat that is challenging for conventional PINNs. This work establishes a robust framework for fusing the rigor of scientific computing with the efficiency of neuromorphic engineering, paving the way for complex, long-term, and energy-efficient physics predictions for intelligent systems.

</details>


### [37] [Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions](https://arxiv.org/abs/2511.22406)
*Roland Stolz,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: 本文提出在动作约束强化学习中，使用截断正态分布进行策略梯度方法时，需要准确估计熵、对数概率等关键特性，并提出高效的数值近似方法和采样策略。


<details>
  <summary>Details</summary>
Motivation: 现有动作约束强化学习方法面临策略更新效果差、计算效率低和运行时不可预测等挑战。虽然最近工作提出使用截断正态分布，但在复杂约束下计算熵、对数概率及其梯度变得不可行，现有近似方法严重降低性能。

Method: 提出高效的数值近似方法来准确估计截断分布下的熵、对数概率及其梯度，同时提供截断策略分布的高效采样策略。

Result: 在三个基准环境上进行验证，结果表明使用准确估计能带来显著的性能提升。

Conclusion: 在动作约束强化学习中，准确估计截断分布的关键特性对性能至关重要，提出的数值近似和采样策略能有效解决现有方法的局限性。

Abstract: In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.

</details>


### [38] [Dynamical Implicit Neural Representations](https://arxiv.org/abs/2511.21787)
*Yesom Park,Kelvin Kan,Thomas Flynn,Yi Huang,Shinjae Yoo,Stanley Osher,Xihaier Luo*

Main category: cs.LG

TL;DR: DINR将隐式神经表示建模为连续时间动力系统而非离散层堆叠，通过连续特征演化缓解谱偏置，提升高频细节表达能力，在图像表示、场重建和数据压缩中表现优于传统静态INR。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INR）在建模复杂视觉和几何信号方面具有强大能力，但谱偏置问题限制了其捕捉高频细节的能力。现有解决方案效果有限，需要新的建模框架来从根本上解决这一问题。

Method: 提出动态隐式神经表示（DINR），将特征演化视为连续时间动力系统而非离散层堆叠。通过基于Rademacher复杂度和神经正切核的理论分析，证明DINR能增强表达能力并改善训练动态。通过正则化底层动态的复杂度来平衡表达能力和泛化能力。

Result: 在图像表示、场重建和数据压缩的广泛实验中，DINR相比传统静态INR展现出更稳定的收敛性、更高的信号保真度和更强的泛化能力。

Conclusion: DINR通过将INR建模为连续时间动力系统，有效缓解了谱偏置问题，提供了更丰富、更自适应的频率表示，为隐式神经表示提供了新的建模框架。

Abstract: Implicit Neural Representations (INRs) provide a powerful continuous framework for modeling complex visual and geometric signals, but spectral bias remains a fundamental challenge, limiting their ability to capture high-frequency details. Orthogonal to existing remedy strategies, we introduce Dynamical Implicit Neural Representations (DINR), a new INR modeling framework that treats feature evolution as a continuous-time dynamical system rather than a discrete stack of layers. This dynamical formulation mitigates spectral bias by enabling richer, more adaptive frequency representations through continuous feature evolution. Theoretical analysis based on Rademacher complexity and the Neural Tangent Kernel demonstrates that DINR enhances expressivity and improves training dynamics. Moreover, regularizing the complexity of the underlying dynamics provides a principled way to balance expressivity and generalization. Extensive experiments on image representation, field reconstruction, and data compression confirm that DINR delivers more stable convergence, higher signal fidelity, and stronger generalization than conventional static INRs.

</details>


### [39] [Automated Discovery of Laser Dicing Processes with Bayesian Optimization for Semiconductor Manufacturing](https://arxiv.org/abs/2511.23141)
*David Leeftink,Roman Doll,Heleen Visserman,Marco Post,Faysal Boughorbel,Max Hinne,Marcel van Gerven*

Main category: cs.LG

TL;DR: 首次实现工业激光切割系统的自动化工艺发现，通过贝叶斯优化和双保真度策略，在硅晶圆上自动找到生产就绪的激光切割参数，匹配或超越专家基准


<details>
  <summary>Details</summary>
Motivation: 半导体晶圆激光切割是微电子制造的关键步骤，传统上需要数周专家时间来为新材料调整工艺参数，平衡切割速度、分离质量和材料完整性

Method: 将问题建模为高维约束多目标贝叶斯优化任务，采用顺序双保真度策略最小化昂贵的破坏性芯片强度评估，在工业LASER1205切割系统上实现自动化工艺发现

Result: 在裸硅和产品晶圆上，该方法自主提供可行配置，在生产速度、芯片强度和结构完整性方面匹配或超越专家基准，仅需技术人员操作；后验验证显示可获得多种具有不同权衡的可行解

Conclusion: 专家对发现工艺的进一步优化可在保持芯片强度和结构完整性的同时提高生产速度，超越纯手动或纯自动化方法，为工业激光切割工艺开发提供高效自动化解决方案

Abstract: Laser dicing of semiconductor wafers is a critical step in microelectronic manufacturing, where multiple sequential laser passes precisely separate individual dies from the wafer. Adapting this complex sequential process to new wafer materials typically requires weeks of expert effort to balance process speed, separation quality, and material integrity. We present the first automated discovery of production-ready laser dicing processes on an industrial LASER1205 dicing system. We formulate the problem as a high-dimensional, constrained multi-objective Bayesian optimization task, and introduce a sequential two-level fidelity strategy to minimize expensive destructive die-strength evaluations. On bare silicon and product wafers, our method autonomously delivers feasible configurations that match or exceed expert baselines in production speed, die strength, and structural integrity, using only technician-level operation. Post-hoc validation of different weight configurations of the utility functions reveals that multiple feasible solutions with qualitatively different trade-offs can be obtained from the final surrogate model. Expert-refinement of the discovered process can further improve production speed while preserving die strength and structural integrity, surpassing purely manual or automated methods.

</details>


### [40] [Multiclass threshold-based classification and model evaluation](https://arxiv.org/abs/2511.21794)
*Edoardo Legnaro,Sabrina Guastavino,Francesco Marchetti*

Main category: cs.LG

TL;DR: 提出基于阈值的多类分类框架，用多维阈值替代标准argmax规则，通过阈值调优提升分类性能，并引入基于ROC云的多类ROC分析。


<details>
  <summary>Details</summary>
Motivation: 标准多类分类使用softmax输出的argmax规则，缺乏像二分类那样的阈值调优机制。本文旨在将二分类中的阈值优化思想扩展到多类场景，提升任意训练网络的预测能力。

Method: 将softmax输出的概率解释转换为多维单纯形上的几何解释，引入多维阈值进行分类决策。提出后验阈值调优方法，并基于ROC云进行多类ROC分析，使用DFP分数总结性能。

Result: 多维阈值调优在不同网络和数据集上均能提升分类性能。基于ROC云的DFP分数提供了比传统OvR曲线更一致的多类性能评估方法。

Conclusion: 提出的阈值框架为多类分类提供了有效的后处理优化手段，通过阈值调优可进一步提升网络性能，且ROC云分析为多类评估提供了新视角。

Abstract: In this paper, we introduce a threshold-based framework for multiclass classification that generalizes the standard argmax rule. This is done by replacing the probabilistic interpretation of softmax outputs with a geometric one on the multidimensional simplex, where the classification depends on a multidimensional threshold. This change of perspective enables for any trained classification network an \textit{a posteriori} optimization of the classification score by means of threshold tuning, as usually carried out in the binary setting, thus allowing for a further refinement of the prediction capability of any network. Our experiments show indeed that multidimensional threshold tuning yields performance improvements across various networks and datasets. Moreover, we derive a multiclass ROC analysis based on \emph{ROC clouds} -- the attainable (FPR,TPR) operating points induced by a single multiclass threshold -- and summarize them via a \emph{Distance From Point} (DFP) score to $(0,1)$. This yields a coherent alternative to standard One-vs-Rest (OvR) curves and aligns with the observed tuning gains.

</details>


### [41] [The Double-Edged Nature of the Rashomon Set for Trustworthy Machine Learning](https://arxiv.org/abs/2511.21799)
*Ethan Hsu,Harry Chen,Chudi Zhong,Lesia Semenova*

Main category: cs.LG

TL;DR: Rashomon集合（多个近似最优模型）在可信机器学习中展现出双重角色：一方面通过多样性增强鲁棒性，另一方面增加隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 现实机器学习流程通常产生多个近似最优模型（Rashomon集合），而非单一模型。需要研究这种多重性如何影响机器学习系统的可信度，特别是在鲁棒性和隐私之间的权衡。

Method: 通过理论分析和稀疏决策树、线性模型的实证研究，分析Rashomon集合在个体模型层面和集合层面的特性，包括稀疏可解释模型的隐私保护能力、对抗攻击脆弱性，以及集合多样性带来的反应式鲁棒性。

Result: 稀疏可解释模型能保护隐私但对对抗攻击脆弱；Rashomon集合的多样性提供反应式鲁棒性（攻击破坏一个模型时其他模型仍准确）和小分布偏移下的稳定性；但集合多样性也增加信息泄露风险，披露更多近似最优模型为攻击者提供更丰富的训练数据视图。

Conclusion: Rashomon集合在可信机器学习中具有双重角色：既是增强鲁棒性的资源，也是增加隐私泄露风险的因素。需要理解这种鲁棒性-隐私权衡，以更好地设计可信的机器学习系统。

Abstract: Real-world machine learning (ML) pipelines rarely produce a single model; instead, they produce a Rashomon set of many near-optimal ones. We show that this multiplicity reshapes key aspects of trustworthiness. At the individual-model level, sparse interpretable models tend to preserve privacy but are fragile to adversarial attacks. In contrast, the diversity within a large Rashomon set enables reactive robustness: even when an attack breaks one model, others often remain accurate. Rashomon sets are also stable under small distribution shifts. However, this same diversity increases information leakage, as disclosing more near-optimal models provides an attacker with progressively richer views of the training data. Through theoretical analysis and empirical studies of sparse decision trees and linear models, we characterize this robustness-privacy trade-off and highlight the dual role of Rashomon sets as both a resource and a risk for trustworthy ML.

</details>


### [42] [Unsupervised Anomaly Detection for Smart IoT Devices: Performance and Resource Comparison](https://arxiv.org/abs/2511.21842)
*Md. Sad Abdullah Sami,Mushfiquzzaman Abid*

Main category: cs.LG

TL;DR: 该研究比较了两种无监督异常检测方法（Isolation Forest和One-Class SVM）在IoT环境中的性能，发现Isolation Forest在检测精度和计算效率方面均优于OC-SVM，更适合资源受限的IoT边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 随着IoT设备在各行业的快速部署，虽然提升了运营效率，但也增加了网络安全漏洞。传统的基于签名的异常检测系统难以识别新兴和零日威胁，因此需要研究更有效的无监督异常检测技术来保护IoT环境。

Method: 使用TON_IoT温控器数据集，对两种无监督异常检测技术（Isolation Forest和One-Class SVM）进行综合评估。评估包括标准性能指标（准确率、精确率、召回率、F1分数）和关键资源利用指标（推理时间、模型大小、峰值RAM使用量）。

Result: Isolation Forest在所有评估指标上都优于One-Class SVM：获得更高的检测准确率、更好的精确率和召回率，以及显著更好的F1分数。此外，Isolation Forest展现出更优的计算足迹，推理时间更短、模型更小、RAM使用更少。

Conclusion: Isolation Forest在高维和不平衡的IoT环境中表现出更强的鲁棒性，其优越的计算效率使其更适合在资源受限的IoT边缘设备上进行实时异常检测部署，具有实际应用可行性。

Abstract: The rapid expansion of Internet of Things (IoT) deployments across diverse sectors has significantly enhanced operational efficiency, yet concurrently elevated cybersecurity vulnerabilities due to increased exposure to cyber threats. Given the limitations of traditional signature-based Anomaly Detection Systems (ADS) in identifying emerging and zero-day threats, this study investigates the effectiveness of two unsupervised anomaly detection techniques, Isolation Forest (IF) and One-Class Support Vector Machine (OC-SVM), using the TON_IoT thermostat dataset. A comprehensive evaluation was performed based on standard metrics (accuracy, precision, recall, and F1-score) alongside critical resource utilization metrics such as inference time, model size, and peak RAM usage. Experimental results revealed that IF consistently outperformed OC-SVM, achieving higher detection accuracy, superior precision, and recall, along with a significantly better F1-score. Furthermore, Isolation Forest demonstrated a markedly superior computational footprint, making it more suitable for deployment on resource-constrained IoT edge devices. These findings underscore Isolation Forest's robustness in high-dimensional and imbalanced IoT environments and highlight its practical viability for real-time anomaly detection.

</details>


### [43] [Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics](https://arxiv.org/abs/2511.21848)
*Eric Leonardis,Akira Nagamori,Ayesha Thanawalla,Yuanjia Yang,Joshua Park,Hutton Saunders,Eiman Azim,Talmo Pereira*

Main category: cs.LG

TL;DR: 该研究开发了一个用于行为驱动仿真的通用平台，通过模仿学习框架在物理模拟环境中实现小鼠前肢抓取任务，发现添加能量和速度的自然约束能更好地预测真实EMG信号。


<details>
  <summary>Details</summary>
Motivation: 为了理解大脑如何有效控制身体，需要建模体现控制的传感器运动转换。研究旨在开发一个高保真行为动力学、生物力学和神经回路架构的通用仿真平台。

Method: 开发了一个从神经科学实验室获取运动学数据并创建生物力学模型重现自然运动的流程。采用模仿学习框架在模拟物理环境中执行灵巧的前肢抓取任务，使用JAX和Mujoco-MJX进行GPU加速训练。

Result: 小鼠手臂模型训练速度超过每秒100万步。添加能量和速度的自然约束导致模拟的肌肉骨骼活动能更好地预测真实的EMG信号。

Conclusion: 能量和控制约束对于建模肌肉骨骼运动控制至关重要，该工作为理解体现控制提供了重要证据。

Abstract: The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high fidelity behavioral dynamics, biomechanics, and neural circuit architectures underlying embodied control. We present a pipeline for taking kinematics data from the neuroscience lab and creating a pipeline for recapitulating those natural movements in a biomechanical model. We implement a imitation learning framework to perform a dexterous forelimb reaching task with a musculoskeletal model in a simulated physics environment. The mouse arm model is currently training at faster than 1 million training steps per second due to GPU acceleration with JAX and Mujoco-MJX. We present results that indicate that adding naturalistic constraints on energy and velocity lead to simulated musculoskeletal activity that better predict real EMG signals. This work provides evidence to suggest that energy and control constraints are critical to modeling musculoskeletal motor control.

</details>


### [44] [Lightweight ML-Based Air Quality Prediction for IoT and Embedded Applications](https://arxiv.org/abs/2511.21857)
*Md. Sad Abdullah Sami,Mushfiquzzaman Abid*

Main category: cs.LG

TL;DR: 研究比较了完整版和轻量版XGBoost回归模型在预测CO和NO2浓度方面的表现，发现完整版精度更高，但轻量版在计算资源方面优势明显，适合物联网等资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估不同版本的XGBoost模型在空气质量预测中的表现，特别关注在资源受限环境（如物联网设备）中部署简化模型的可行性。

Method: 使用AirQualityUCI数据集（一年城市环境数据），评估完整版和轻量版XGBoost回归模型。采用MAE、RMSE、MBE、R2等预测精度指标，以及推理时间、模型大小、峰值RAM使用等资源指标进行综合评估。

Result: 完整版XGBoost模型对两种污染物都取得了更好的预测精度，而轻量版模型虽然精度稍低，但在推理时间、模型存储需求等计算资源方面优势显著。

Conclusion: 轻量版XGBoost模型可以在不显著牺牲预测质量的前提下，为资源受限环境提供可行的解决方案，特别适合物联网和嵌入式应用中的实时空气质量监测。

Abstract: This study investigates the effectiveness and efficiency of two variants of the XGBoost regression model, the full-capacity and lightweight (tiny) versions, for predicting the concentrations of carbon monoxide (CO) and nitrogen dioxide (NO2). Using the AirQualityUCI dataset collected over one year in an urban environment, we conducted a comprehensive evaluation based on widely accepted metrics, including Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Mean Bias Error (MBE), and the coefficient of determination (R2). In addition, we assessed resource-oriented metrics such as inference time, model size, and peak RAM usage. The full XGBoost model achieved superior predictive accuracy for both pollutants, while the tiny model, though slightly less precise, offered substantial computational benefits with significantly reduced inference time and model storage requirements. These results demonstrate the feasibility of deploying simplified models in resource-constrained environments without compromising predictive quality. This makes the tiny XGBoost model suitable for real-time air-quality monitoring in IoT and embedded applications.

</details>


### [45] [Representative Action Selection for Large Action Space: From Bandits to MDPs](https://arxiv.org/abs/2511.22104)
*Quan Zhou,Shie Mannor*

Main category: cs.LG

TL;DR: 提出一种从大型动作空间中选择代表性子集的方法，使强化学习在组合决策问题中更高效


<details>
  <summary>Details</summary>
Motivation: 在库存管理和推荐系统等应用中，动作空间极大，直接在整个空间上学习不可行。需要找到一个小型代表性动作子集，使每个环境都包含近似最优动作。

Method: 将元多臂赌博机的方法扩展到马尔可夫决策过程，在放松的非中心化次高斯过程模型下，证明现有算法能达到与使用完整动作空间相当的性能。

Result: 理论证明该方法在更宽松的环境异质性假设下，仍能实现与完整动作空间相当的性能，为大规模组合决策提供计算和样本高效的解决方案。

Conclusion: 该方法成功解决了从大型动作空间选择代表性子集的问题，使强化学习在组合决策应用中更高效，扩展了元学习在复杂决策问题中的应用范围。

Abstract: We study the problem of selecting a small, representative action subset from an extremely large action space shared across a family of reinforcement learning (RL) environments -- a fundamental challenge in applications like inventory management and recommendation systems, where direct learning over the entire space is intractable. Our goal is to identify a fixed subset of actions that, for every environment in the family, contains a near-optimal action, thereby enabling efficient learning without exhaustively evaluating all actions.
  This work extends our prior results for meta-bandits to the more general setting of Markov Decision Processes (MDPs). We prove that our existing algorithm achieves performance comparable to using the full action space. This theoretical guarantee is established under a relaxed, non-centered sub-Gaussian process model, which accommodates greater environmental heterogeneity. Consequently, our approach provides a computationally and sample-efficient solution for large-scale combinatorial decision-making under uncertainty.

</details>


### [46] [Towards a Foundation Model for Partial Differential Equations Across Physics Domains](https://arxiv.org/abs/2511.21861)
*Eduardo Soares,Emilio Vital Brazil,Victor Shirasuna,Breno W. S. R. de Carvalho,Cristiano Malossi*

Main category: cs.LG

TL;DR: PDE-FM是一个用于物理信息机器学习的模块化基础模型，通过统一空间、频谱和时间推理来处理异质偏微分方程系统，在多个物理领域达到最先进精度。


<details>
  <summary>Details</summary>
Motivation: 当前任务特定的神经算子方法需要针对不同物理系统进行专门设计和训练，缺乏统一的、可迁移的模型来处理多样化的偏微分方程系统。需要开发一个能够跨不同物理领域泛化的基础模型。

Method: PDE-FM结合了空间-频谱标记化、物理感知条件化和基于Mamba的状态空间骨干网络，配备算子理论解码器。模型在多样的PDE数据集上进行一次性预训练，无需架构或数据特定修改即可迁移到新的物理体系。

Result: 在The Well基准测试的12个2D和3D数据集上（涵盖流体动力学、辐射、弹性和天体物理现象），PDE-FM在6个领域达到最先进精度，相对于先前的算子学习基线将平均VRMSE降低了46%。模型展示了强大的跨物理泛化能力，在湍流和辐射系统中表现优异，同时在线性和稳态体系中保持强劲性能。

Conclusion: 大规模跨多样物理过程的预训练能够产生可迁移的动态表示，这标志着向统一的多物理模拟和科学发现基础级代理模型迈出了一步。PDE-FM展示了基础模型在物理信息机器学习中的潜力。

Abstract: We present PDE-FM, a modular foundation model for physics-informed machine learning that unifies spatial, spectral, and temporal reasoning across heterogeneous partial differential equation (PDE) systems. PDE-FM combines spatial-spectral tokenization, physics-aware conditioning, and a Mamba-based state-space backbone with an operator-theoretic decoder, enabling scalable and data-efficient modeling of complex physical dynamics. In contrast to task-specific neural operators, PDE-FM is pretrained once on diverse PDE datasets and can be transferred to new physical regimes without architectural or data-specific modifications. Evaluated on twelve 2D and 3D datasets from The Well benchmark - spanning hydrodynamic, radiative, elastic, and astrophysical phenomena - PDE-FM achieves state-of-the-art accuracy in six domains, reducing mean VRMSE by 46% relative to prior operator-learning baselines. The model demonstrates robust cross-physics generalization, excelling in turbulent and radiative systems while maintaining strong performance in linear and steady-state regimes. These results suggest that large-scale pretraining across diverse physical processes can yield transferable representations of dynamics, marking a step toward unified, foundation-level surrogates for multi-physics simulation and scientific discovery.

</details>


### [47] [Probabilistic Digital Twin for Misspecified Structural Dynamical Systems via Latent Force Modeling and Bayesian Neural Networks](https://arxiv.org/abs/2511.22133)
*Sahil Kashyap,Rajdip Nayek*

Main category: cs.LG

TL;DR: 提出一个概率数字孪生框架，用于处理物理模型错误指定的动态系统响应预测，结合高斯过程潜在力模型和贝叶斯神经网络进行端到端不确定性感知推理


<details>
  <summary>Details</summary>
Motivation: 动态系统中存在模型形式错误（MFEs）时，传统方法难以准确预测系统响应。需要开发能够处理模型错误指定、并能系统传播诊断到预测不确定性的可信数字孪生框架

Method: 1) 诊断阶段：将模型形式错误视为线性动态系统的潜在输入力，使用高斯过程潜在力模型（GPLFM）从传感器测量中联合估计系统状态和MFEs；2) 使用贝叶斯神经网络（BNN）从后验样本中学习系统状态到MFEs的概率非线性映射；3) 预测阶段：利用该映射生成伪测量，通过卡尔曼滤波进行状态预测

Result: 在四个非线性示例上验证：单自由度振荡器、多自由度系统、Bouc-Wen迟滞系统和Silverbox实验数据集。框架展示了预测准确性，并对模型错误指定具有鲁棒性

Conclusion: 该概率数字孪生框架能够系统地从诊断传播不确定性到预测，是实现可信数字孪生的关键能力，在模型错误指定的动态系统中表现出良好的预测性能和鲁棒性

Abstract: This work presents a probabilistic digital twin framework for response prediction in dynamical systems governed by misspecified physics. The approach integrates Gaussian Process Latent Force Models (GPLFM) and Bayesian Neural Networks (BNNs) to enable end-to-end uncertainty-aware inference and prediction. In the diagnosis phase, model-form errors (MFEs) are treated as latent input forces to a nominal linear dynamical system and jointly estimated with system states using GPLFM from sensor measurements. A BNN is then trained on posterior samples to learn a probabilistic nonlinear mapping from system states to MFEs, while capturing diagnostic uncertainty. For prognosis, this mapping is used to generate pseudo-measurements, enabling state prediction via Kalman filtering. The framework allows for systematic propagation of uncertainty from diagnosis to prediction, a key capability for trustworthy digital twins. The framework is demonstrated using four nonlinear examples: a single degree of freedom (DOF) oscillator, a multi-DOF system, and two established benchmarks -- the Bouc-Wen hysteretic system and the Silverbox experimental dataset -- highlighting its predictive accuracy and robustness to model misspecification.

</details>


### [48] [Closed-Loop Transformers: Autoregressive Modeling as Iterative Latent Equilibrium](https://arxiv.org/abs/2511.21882)
*Akbar Anbar Jafari,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 论文提出Equilibrium Transformers (EqT)，通过闭环预测原则让模型在生成每个token前迭代优化隐表示，解决传统自回归Transformer的开环瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前自回归Transformer采用开环操作：每个隐藏状态通过单次前向传播计算且从不修正，导致错误在序列中传播。这限制了模型在长程推理、事实一致性和多步规划方面的能力。

Method: 引入闭环预测原则，要求模型在生成每个token前迭代优化隐表示直至达到自洽平衡。具体实现为Equilibrium Transformers (EqT)，在标准Transformer层中增加Equilibrium Refinement Module，通过梯度下降最小化学到的能量函数来优化隐空间表示。

Result: 在二进制奇偶性任务上，EqT平均提升3.28%，在标准Transformer接近随机性能的困难序列上提升达到8.07%。理论证明EqT在隐能量模型中执行近似MAP推理，具有线性收敛保证。

Conclusion: 闭环平衡机制可能解决自回归Transformer的承诺瓶颈，正如注意力机制解决了循环网络的序列瓶颈一样，代表了向更强大语言模型发展的基础性一步。

Abstract: Contemporary autoregressive transformers operate in open loop: each hidden state is computed in a single forward pass and never revised, causing errors to propagate uncorrected through the sequence. We identify this open-loop bottleneck as a fundamental architectural limitation underlying well-documented failures in long-range reasoning, factual consistency, and multi-step planning. To address this limitation, we introduce the closed-loop prediction principle, which requires that models iteratively refine latent representations until reaching a self-consistent equilibrium before committing to each token. We instantiate this principle as Equilibrium Transformers (EqT), which augment standard transformer layers with an Equilibrium Refinement Module that minimizes a learned energy function via gradient descent in latent space. The energy function enforces bidirectional prediction consistency, episodic memory coherence, and output confidence, all computed without external supervision. Theoretically, we prove that EqT performs approximate MAP inference in a latent energy-based model, establish linear convergence guarantees, and show that refinement improves predictions precisely on hard instances where one-shot inference is suboptimal. The framework unifies deep equilibrium models, diffusion language models, and test-time training as special cases. Preliminary experiments on the binary parity task demonstrate +3.28% average improvement on challenging sequences, with gains reaching +8.07% where standard transformers approach random performance, validating that the benefit of deliberation scales with task difficulty. Just as attention mechanisms resolved the sequential bottleneck of recurrent networks, we propose that closed-loop equilibrium may resolve the commitment bottleneck of open-loop autoregression, representing a foundational step toward language models.

</details>


### [49] [A Trainable Centrality Framework for Modern Data](https://arxiv.org/abs/2511.22959)
*Minh Duc Vu,Mingshuo Liu,Doudou Zhou*

Main category: cs.LG

TL;DR: FUSE是一个神经中心性框架，通过全局头（基于距离比较学习无锚点中心性分数）和局部头（通过去噪分数匹配近似平滑对数密度势）的组合，在任意表示上计算数据点的中心性，单个参数可在两种信号间插值。


<details>
  <summary>Details</summary>
Motivation: 传统深度概念在高维空间中计算昂贵且不稳定，难以扩展到非欧几里得数据，需要一种能处理任意表示形式的通用中心性测量方法。

Method: FUSE框架包含两个组件：全局头通过成对距离比较学习无锚点中心性分数；局部头通过去噪分数匹配近似平滑对数密度势。通过0到1之间的单一参数在两种校准信号间插值，实现不同视角的中心性计算。

Result: 在合成分布、真实图像、时间序列和文本数据以及标准异常检测基准测试中，FUSE能够恢复有意义的经典排序，揭示多尺度几何结构，在保持简单高效的同时与强经典基线达到竞争性性能。

Conclusion: FUSE提供了一个灵活高效的神经中心性框架，能够处理各种数据类型和表示形式，为高维和非欧几里得数据的中心性测量提供了有效的解决方案。

Abstract: Measuring how central or typical a data point is underpins robust estimation, ranking, and outlier detection, but classical depth notions become expensive and unstable in high dimensions and are hard to extend beyond Euclidean data. We introduce Fused Unified centrality Score Estimation (FUSE), a neural centrality framework that operates on top of arbitrary representations. FUSE combines a global head, trained from pairwise distance-based comparisons to learn an anchor-free centrality score, with a local head, trained by denoising score matching to approximate a smoothed log-density potential. A single parameter between 0 and 1 interpolates between these calibrated signals, yielding depth-like centrality from different views via one forward pass. Across synthetic distributions, real images, time series, and text data, and standard outlier detection benchmarks, FUSE recovers meaningful classical ordering, reveals multi-scale geometric structures, and attains competitive performance with strong classical baselines while remaining simple and efficient.

</details>


### [50] [Physically Interpretable Representation Learning with Gaussian Mixture Variational AutoEncoder (GM-VAE)](https://arxiv.org/abs/2511.21883)
*Tiffany Fan,Murray Cutforth,Marta D'Elia,Alexandre Cortiella,Alireza Doostan,Eric Darve*

Main category: cs.LG

TL;DR: 提出GM-VAE框架，结合EM训练策略和谱可解释性度量，从高维科学数据中提取紧凑、物理可解释的表示，在湍流和反应流系统中实现稳定训练和物理一致的聚类。


<details>
  <summary>Details</summary>
Motivation: 从高维科学数据中提取紧凑且物理可解释的表示是一个持续挑战，因为物理系统具有复杂的非线性结构。传统VAE在联合优化重建和聚类时存在训练不稳定性问题。

Method: 提出高斯混合变分自编码器(GM-VAE)框架，集成期望最大化(EM)训练方案和块坐标下降策略，交替执行期望步和最大化步。引入基于图拉普拉斯平滑度的谱可解释性度量来评估学习表示。

Result: 在表面反应ODE、Navier-Stokes尾流和实验激光诱导燃烧纹影图像等数据集上验证，GM-VAE产生平滑的物理一致流形和准确的机制聚类，为湍流和反应流系统提供鲁棒的数据驱动解释工具。

Conclusion: GM-VAE框架通过EM训练策略和定量可解释性度量，能够从复杂科学数据中提取物理可解释的表示，为高维物理系统的数据驱动分析提供了有效方法。

Abstract: Extracting compact, physically interpretable representations from high-dimensional scientific data is a persistent challenge due to the complex, nonlinear structures inherent in physical systems. We propose a Gaussian Mixture Variational Autoencoder (GM-VAE) framework designed to address this by integrating an Expectation-Maximization (EM)-inspired training scheme with a novel spectral interpretability metric. Unlike conventional VAEs that jointly optimize reconstruction and clustering (often leading to training instability), our method utilizes a block-coordinate descent strategy, alternating between expectation and maximization steps. This approach stabilizes training and naturally aligns latent clusters with distinct physical regimes. To objectively evaluate the learned representations, we introduce a quantitative metric based on graph-Laplacian smoothness, which measures the coherence of physical quantities across the latent manifold. We demonstrate the efficacy of this framework on datasets of increasing complexity: surface reaction ODEs, Navier-Stokes wake flows, and experimental laser-induced combustion Schlieren images. The results show that our GM-VAE yields smooth, physically consistent manifolds and accurate regime clustering, offering a robust data-driven tool for interpreting turbulent and reactive flow systems.

</details>


### [51] [Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory](https://arxiv.org/abs/2511.23083)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 论文发现高容量核Hopfield网络的"优化脊"对应于统计流形上的"稳定性边缘"，这是Fisher信息矩阵奇异的关键边界，将学习动态与容量通过最小描述长度原理统一起来。


<details>
  <summary>Details</summary>
Motivation: 高容量核Hopfield网络表现出极端稳定的"优化脊"，虽然之前与"谱集中"相关，但其起源仍然不明确。需要深入理解这一现象的本质。

Method: 在统计流形上分析网络动态，揭示"优化脊"对应于"稳定性边缘"——Fisher信息矩阵奇异的关键边界。将欧几里得空间中的力对抗解释为黎曼空间中的"对偶平衡"。

Result: 发现"优化脊"实际上是统计流形上的"稳定性边缘"，在这个边界上Fisher信息矩阵变得奇异。表观欧几里得力对抗是黎曼空间中"对偶平衡"的表现。

Conclusion: 通过最小描述长度原理统一了学习动态和容量，为自组织临界性提供了几何理论框架，揭示了高容量核Hopfield网络优化脊的深层几何本质。

Abstract: High-capacity kernel Hopfield networks exhibit a "Ridge of Optimization" characterized by extreme stability. While previously linked to "Spectral Concentration," its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the "Edge of Stability," a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.

</details>


### [52] [Exploring Fusion Strategies for Multimodal Vision-Language Systems](https://arxiv.org/abs/2511.21889)
*Regan Willis,Jason Bakos*

Main category: cs.LG

TL;DR: 该研究探讨了多模态机器学习中数据融合策略的权衡，通过BERT与视觉网络（MobileNetV2和ViT）的混合框架，比较了早期、中期和晚期融合在准确性和延迟方面的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态机器学习模型需要融合多个输入数据流以提高决策准确性，但融合策略（早期、中期或晚期融合）需要在准确性和延迟之间进行权衡，因为不同的融合阶段会影响模型的性能和推理速度。

Method: 使用BERT与视觉网络（MobileNetV2和ViT）的混合框架，为每个视觉网络提出了三种模型：晚期融合、中期融合和早期融合。在CMU MOSI数据集上评估模型准确性，并在NVIDIA Jetson Orin AGX上基准测试延迟。

Result: 实验结果显示，晚期融合获得最高准确性，而早期融合提供最低推理延迟。模型架构中较早的数据融合会导致更快的推理时间，但以准确性为代价。

Conclusion: 在多模态机器学习中，数据融合策略需要在准确性和延迟之间进行权衡：晚期融合提供最佳准确性，早期融合提供最快推理速度。应用需求应决定选择哪种融合策略。

Abstract: Modern machine learning models often combine multiple input streams of data to more accurately capture the information that informs their decisions. In multimodal machine learning, choosing the strategy for fusing data together requires careful consideration of the application's accuracy and latency requirements, as fusing the data at earlier or later stages in the model architecture can lead to performance changes in accuracy and latency. To demonstrate this tradeoff, we investigate different fusion strategies using a hybrid BERT and vision network framework that integrates image and text data. We explore two different vision networks: MobileNetV2 and ViT. We propose three models for each vision network, which fuse data at late, intermediate, and early stages in the architecture. We evaluate the proposed models on the CMU MOSI dataset and benchmark their latency on an NVIDIA Jetson Orin AGX. Our experimental results demonstrate that while late fusion yields the highest accuracy, early fusion offers the lowest inference latency. We describe the three proposed model architectures and discuss the accuracy and latency tradeoffs, concluding that data fusion earlier in the model architecture results in faster inference times at the cost of accuracy.

</details>


### [53] [Breaking the Illusion: Consensus-Based Generative Mitigation of Adversarial Illusions in Multi-Modal Embeddings](https://arxiv.org/abs/2511.21893)
*Fatemeh Akbarian,Anahita Baninajjar,Yingyi Zhang,Ananth Balashankar,Amir Aminifar*

Main category: cs.LG

TL;DR: 提出对抗幻觉的防御机制：通过生成模型重构被攻击的输入，结合生成采样和共识聚合，显著降低攻击成功率并提升跨模态对齐


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型虽然能在共享嵌入空间中对齐图像、文本等模态，但仍易受对抗幻觉攻击，其中不可察觉的扰动会破坏跨模态对齐并误导下游任务

Method: 提出任务无关的缓解机制：使用生成模型（如VAE）从攻击者的扰动输入中重构原始输入以保持自然对齐；进一步采用生成采样策略结合基于共识的聚合方案来增强防御效果

Result: 在先进多模态编码器上的实验显示，该方法将幻觉攻击成功率大幅降低至接近零，在未扰动和扰动输入设置下分别将跨模态对齐提升4%（42到46）和11%（32到43）

Conclusion: 该方法为对抗幻觉攻击提供了有效且模型无关的防御方案，显著增强了多模态基础模型的鲁棒性

Abstract: Multi-modal foundation models align images, text, and other modalities in a shared embedding space but remain vulnerable to adversarial illusions (Zhang et al., 2025), where imperceptible perturbations disrupt cross-modal alignment and mislead downstream tasks. To counteract the effects of adversarial illusions, we propose a task-agnostic mitigation mechanism that reconstructs the input from the attacker's perturbed input through generative models, e.g., Variational Autoencoders (VAEs), to maintain natural alignment. To further enhance our proposed defense mechanism, we adopt a generative sampling strategy combined with a consensus-based aggregation scheme over the outcomes of the generated samples. Our experiments on the state-of-the-art multi-modal encoders show that our approach substantially reduces the illusion attack success rates to near-zero and improves cross-modal alignment by 4% (42 to 46) and 11% (32 to 43) in unperturbed and perturbed input settings respectively, providing an effective and model-agnostic defense against adversarial illusions.

</details>


### [54] [Towards Understanding Transformers in Learning Random Walks](https://arxiv.org/abs/2511.23239)
*Wei Shi,Yuan Cao*

Main category: cs.LG

TL;DR: 本文从理论上分析了单层Transformer在预测随机游走任务中的能力和可解释性，证明了梯度下降训练后能达到最优精度，并揭示了注意力机制作为标记选择器、值矩阵执行概率转移的机制。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在序列数据处理中表现出色但缺乏理论解释。本文旨在从理论上理解Transformer的能力和可解释性，特别是在学习经典统计模型（圆上随机游走）方面的表现。

Method: 研究单层Transformer模型在预测圆上随机游走任务中的表现。通过理论分析证明，经过梯度下降训练后，模型能达到最优预测精度。分析训练后模型的注意力机制如何作为标记选择器关注父状态，以及值矩阵如何执行一步概率转移。

Result: 理论证明单层Transformer在梯度下降训练后能实现随机游走预测的最优精度。训练后的模型具有可解释性：softmax注意力作为标记选择器关注直接父状态，值矩阵基于父状态执行一步概率转移来预测下一个状态的位置。同时识别了理论未覆盖的边缘情况确实是失败案例。

Conclusion: 本文为Transformer在序列预测任务中的成功提供了理论解释，揭示了其内部工作机制。同时发现小初始化梯度下降在某些简单任务中可能失败或难以收敛到良好解，这为理解Transformer的局限性提供了见解。

Abstract: Transformers have proven highly effective across various applications, especially in handling sequential data such as natural languages and time series. However, transformer models often lack clear interpretability, and the success of transformers has not been well understood in theory. In this paper, we study the capability and interpretability of transformers in learning a family of classic statistical models, namely random walks on circles. We theoretically demonstrate that, after training with gradient descent, a one-layer transformer model can achieve optimal accuracy in predicting random walks. Importantly, our analysis reveals that the trained model is interpretable: the trained softmax attention serves as a token selector, focusing on the direct parent state; subsequently, the value matrix executes a one-step probability transition to predict the location of the next state based on this parent state. We also show that certain edge cases not covered by our theory are indeed failure cases, demonstrating that our theoretical conditions are tight. By investigating these success and failure cases, it is revealed that gradient descent with small initialization may fail or struggle to converge to a good solution in certain simple tasks even beyond random walks. Experiments are conducted to support our theoretical findings.

</details>


### [55] [Beyond Atoms: Evaluating Electron Density Representation for 3D Molecular Learning](https://arxiv.org/abs/2511.21900)
*Patricia Suriana,Joshua A. Rackers,Ewa M. Nowara,Pedro O. Pinheiro,John M. Nicoloudis,Vishnu Sresht*

Main category: cs.LG

TL;DR: 比较三种体素化输入（原子类型、原始电子密度、密度梯度）在3D CNN中的表现，发现电子密度在低数据量下对蛋白质-配体结合亲和力预测更有效，在量子性质预测中优于原子表示


<details>
  <summary>Details</summary>
Motivation: 传统的3D分子性质预测模型主要基于原子表示，可能忽略细微的物理信息。电子密度图作为X射线晶体学和冷冻电镜的直接输出，提供了连续、物理基础更强的替代方案

Method: 使用三种体素化输入类型（原子类型、原始电子密度、密度梯度大小）在3D卷积神经网络中进行比较，应用于两个分子任务：蛋白质-配体结合亲和力预测（PDBbind）和量子性质预测（QM9）

Result: 在PDBbind任务中，全数据量下所有表示表现相似，但在低数据量下，基于密度的输入优于原子类型；在QM9任务中，即使输入密度来自较低级别方法，基于密度的输入在大规模下仍优于原子表示

Conclusion: 电子密度输入在不同任务和数据量下具有优势：在亲和力预测中提高数据效率，在量子性质建模中提高准确性，突显了密度衍生输入的任务和机制依赖性优势

Abstract: Machine learning models for 3D molecular property prediction typically rely on atom-based representations, which may overlook subtle physical information. Electron density maps -- the direct output of X-ray crystallography and cryo-electron microscopy -- offer a continuous, physically grounded alternative. We compare three voxel-based input types for 3D convolutional neural networks (CNNs): atom types, raw electron density, and density gradient magnitude, across two molecular tasks -- protein-ligand binding affinity prediction (PDBbind) and quantum property prediction (QM9). We focus on voxel-based CNNs because electron density is inherently volumetric, and voxel grids provide the most natural representation for both experimental and computed densities. On PDBbind, all representations perform similarly with full data, but in low-data regimes, density-based inputs outperform atom types, while a shape-based baseline performs comparably -- suggesting that spatial occupancy dominates this task. On QM9, where labels are derived from Density Functional Theory (DFT) but input densities from a lower-level method (XTB), density-based inputs still outperform atom-based ones at scale, reflecting the rich structural and electronic information encoded in density. Overall, these results highlight the task- and regime-dependent strengths of density-derived inputs, improving data efficiency in affinity prediction and accuracy in quantum property modeling.

</details>


### [56] [Quantized-Tinyllava: a new multimodal foundation model enables efficient split learning](https://arxiv.org/abs/2511.23402)
*Jiajun Guo,Xin Luo,Jie Liu*

Main category: cs.LG

TL;DR: 提出一种结合学习型数据压缩的多模态模型分割学习方法，通过将模型嵌入压缩为低比特整数来大幅降低传输成本，同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 分割学习虽然能解决数据隐私问题，但高网络通信成本（特别是对于需要传输大量高维数据的大型基础模型）一直是主要障碍

Method: 提出新的多模态模型结构，结合学习型数据压缩方法，将模型嵌入压缩为低比特整数，同时基于熵编码理论确定最优离散表示级别数量

Result: 该方法能大幅减少分割学习中的传输成本，同时保持模型性能

Conclusion: 通过结合学习型数据压缩和熵编码理论，有效解决了分割学习中的高通信成本问题，为隐私保护下的分布式模型训练提供了高效解决方案

Abstract: Split learning is well known as a method for resolving data privacy concerns by training a model on distributed devices, thereby avoiding data sharing that raises privacy issues. However, high network communication costs are always an impediment to split learning, especially for large foundation models that require transmitting large amounts of high-dimensional data. To resolve this issue, we present a new multimodal model structure that incorporates a learning-based data compression method, which compresses model embeddings into low-bit integers while preserving the model's performance, greatly reducing the transmission costs between partitions. We then determine the optimal number of discrete representation levels based on a solid theoretical foundation from entropy coding.

</details>


### [57] [Multi-Modal Machine Learning for Early Trust Prediction in Human-AI Interaction Using Face Image and GSR Bio Signals](https://arxiv.org/abs/2511.21908)
*Hamid Shamszare,Avishek Choudhury*

Main category: cs.LG

TL;DR: 多模态机器学习框架结合面部表情和皮肤电反应数据，在模拟ADHD移动健康场景中预测用户对AI或人类建议的早期信任度，实现了高精度预测。


<details>
  <summary>Details</summary>
Motivation: 预测人类对AI系统的信任对于安全整合AI决策支持工具至关重要，特别是在医疗健康领域。在心理健康应用中，信任校准不当会影响诊断和治疗结果。

Method: 提出多模态机器学习框架，结合图像和皮肤电反应数据。面部视频数据使用OpenCV提取帧，通过预训练transformer模型提取情感特征；GSR信号分解为紧张性和阶段性成分。定义两个时间窗口：早期检测窗口（决策前6-3秒）和邻近检测窗口（决策前3-0秒）。分别使用图像、GSR和多模态特征进行信任预测，通过多模态堆叠集成最佳单模态模型。

Result: 多模态堆叠框架在早期检测窗口达到准确率0.83、F1分数0.88、ROC-AUC 0.87；在邻近检测窗口达到准确率0.75、F1分数0.82、ROC-AUC 0.66。结合面部和生理线索显著提高了预测性能。

Conclusion: 生物信号可作为实时、客观的用户信任标记，使自适应AI系统能够动态调整响应以维持校准的信任，这在心理健康应用中至关重要。

Abstract: Predicting human trust in AI systems is crucial for safe integration of AI-based decision support tools, especially in healthcare. This study proposes a multi-modal machine learning framework that combines image and galvanic skin response (GSR) data to predict early user trust in AI- or human-generated recommendations in a simulated ADHD mHealth context. Facial video data were processed using OpenCV for frame extraction and transferred learning with a pre-trained transformer model to derive emotional features. Concurrently, GSR signals were decomposed into tonic and phasic components to capture physiological arousal patterns. Two temporal windows were defined for trust prediction: the Early Detection Window (6 to 3 seconds before decision-making) and the Proximal Detection Window (3 to 0 seconds before decision-making). For each window, trust prediction was conducted separately using image-based, GSR-based, and multimodal (image + GSR) features. Each modality was analyzed using machine learning algorithms, and the top-performing unimodal models were integrated through a multimodal stacking ensemble for final prediction. Experimental results showed that combining facial and physiological cues significantly improved prediction performance. The multimodal stacking framework achieved an accuracy of 0.83, F1-score of 0.88, and ROC-AUC of 0.87 in the Early Detection Window, and an accuracy of 0.75, F1-score of 0.82, and ROC-AUC of 0.66 in the Proximal Detection Window. These results demonstrate the potential of bio signals as real-time, objective markers of user trust, enabling adaptive AI systems that dynamically adjust their responses to maintain calibrated trust which is a critical capability in mental health applications where mis-calibrated trust can affect diagnostic and treatment outcomes.

</details>


### [58] [Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation](https://arxiv.org/abs/2511.23440)
*Bernhard Klein,Falk Selker,Hendrik Borras,Sophie Steger,Franz Pernkopf,Holger Fröning*

Main category: cs.LG

TL;DR: PFP-BNNs通过高斯分布假设实现高效贝叶斯推理，在ARM嵌入式CPU上实现端到端部署，相比传统SVI获得高达4200倍加速，同时保持准确性和不确定性估计能力。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在安全关键应用中因不确定性处理能力有限而受限，贝叶斯神经网络(BNN)能提供概率估计但计算成本过高。需要开发高效BNN部署方案以在资源受限系统上实现可靠的不确定性估计。

Method: 提出概率前向传播(PFP)方法，假设权重和激活值服从高斯分布，实现完全解析的不确定性传播，用单次确定性前向传播替代采样。使用TVM深度学习编译器构建高斯传播算子库，结合手动和自动调优策略，实现端到端的PFP-BNN训练、编译、优化和部署流程。

Result: PFP在计算效率上持续优于SVI，小批量处理时加速高达4200倍。在Dirty-MNIST数据集上，PFP-BNNs在准确性、不确定性估计和OOD检测方面与SVI-BNNs相当，同时大幅降低计算成本。

Conclusion: 结合贝叶斯近似与代码生成技术，PFP方法为资源受限系统上的高效BNN部署提供了可行方案，展示了在嵌入式设备上实现可靠不确定性估计的潜力。

Abstract: Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.

</details>


### [59] [Exploring Dynamic Properties of Backdoor Training Through Information Bottleneck](https://arxiv.org/abs/2511.21923)
*Xinyu Liu,Xu Zhang,Can Chen,Ren Wang*

Main category: cs.LG

TL;DR: 该论文通过信息瓶颈原理分析后门数据对神经网络训练动态的影响，发现后门攻击会创建独特的互信息特征，揭示了视觉明显攻击在信息论层面可能更隐蔽的悖论，并提出基于动态的隐蔽性度量方法。


<details>
  <summary>Details</summary>
Motivation: 理解后门数据如何影响神经网络训练动态是一个复杂且未被充分探索的挑战。现有研究缺乏对后门数据在学习过程中影响的深入分析，特别是在目标类别与其他干净类别之间的不同行为。

Method: 利用信息瓶颈原理结合内部表示的聚类分析，研究后门攻击在训练不同阶段创建的独特互信息特征。基于这些洞察，提出一种新颖的基于动态的隐蔽性度量方法，量化攻击在模型层面的整合程度。

Result: 发现后门攻击会创建独特的互信息特征，这些特征随训练阶段演变且因攻击机制而异。揭示了一个令人惊讶的权衡：视觉明显的攻击（如BadNets）在信息论视角下可能具有更高的隐蔽性，比许多视觉不可感知的攻击更无缝地整合到模型中。

Conclusion: 该研究为理解和评估后门威胁提供了新的维度，提出的动态隐蔽性度量方法在多个数据集和攻击类型上得到验证，有助于更全面地评估后门攻击的隐蔽性和有效性。

Abstract: Understanding how backdoor data influences neural network training dynamics remains a complex and underexplored challenge. In this paper, we present a rigorous analysis of the impact of backdoor data on the learning process, with a particular focus on the distinct behaviors between the target class and other clean classes. Leveraging the Information Bottleneck (IB) principle connected with clustering of internal representation, We find that backdoor attacks create unique mutual information (MI) signatures, which evolve across training phases and differ based on the attack mechanism. Our analysis uncovers a surprising trade-off: visually conspicuous attacks like BadNets can achieve high stealthiness from an information-theoretic perspective, integrating more seamlessly into the model than many visually imperceptible attacks. Building on these insights, we propose a novel, dynamics-based stealthiness metric that quantifies an attack's integration at the model level. We validate our findings and the proposed metric across multiple datasets and diverse attack types, offering a new dimension for understanding and evaluating backdoor threats. Our code is available in: https://github.com/XinyuLiu71/Information_Bottleneck_Backdoor.git.

</details>


### [60] [Provable Benefits of Sinusoidal Activation for Modular Addition](https://arxiv.org/abs/2511.23443)
*Tianlong Huang,Zhiyuan Li*

Main category: cs.LG

TL;DR: 本文研究激活函数在学习模加法中的作用，发现正弦激活的MLP仅需宽度2即可精确实现任意固定长度的模加法，而ReLU网络需要宽度随长度线性增长，且无法同时拟合不同长度。


<details>
  <summary>Details</summary>
Motivation: 探究不同激活函数（特别是正弦和ReLU）在模加法学习任务中的表达能力差异，以及它们对泛化性能的影响。

Method: 理论分析：建立正弦MLP和ReLU网络在模加法任务上的表达能力对比；提出正弦网络的Natarajan维度泛化界；推导过参数化机制下宽度无关的基于间隔的泛化理论；实验验证。

Result: 正弦网络仅需宽度2即可精确实现任意固定长度的模加法，而ReLU网络需要宽度随长度线性增长；正弦网络具有近乎最优的样本复杂度；实验显示正弦网络在不同机制下都比ReLU网络泛化更好，且表现出强大的长度外推能力。

Conclusion: 正弦激活函数在模加法学习中具有显著优势，不仅表达能力更强，而且泛化性能更好，为神经网络架构设计提供了新的启示。

Abstract: This paper studies the role of activation functions in learning modular addition with two-layer neural networks. We first establish a sharp expressivity gap: sine MLPs admit width-$2$ exact realizations for any fixed length $m$ and, with bias, width-$2$ exact realizations uniformly over all lengths. In contrast, the width of ReLU networks must scale linearly with $m$ to interpolate, and they cannot simultaneously fit two lengths with different residues modulo $p$. We then provide a novel Natarajan-dimension generalization bound for sine networks, yielding nearly optimal sample complexity $\widetilde{\mathcal{O}}(p)$ for ERM over constant-width sine networks. We also derive width-independent, margin-based generalization for sine networks in the overparametrized regime and validate it. Empirically, sine networks generalize consistently better than ReLU networks across regimes and exhibit strong length extrapolation.

</details>


### [61] [Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs](https://arxiv.org/abs/2511.21928)
*Yifan Zhou,Sachin Grover,Mohamed El Mistiri,Kamalesh Kalirathnam,Pratyush Kerhalkar,Swaroop Mishra,Neelesh Kumar,Sanket Gaurav,Oya Aran,Heni Ben Amor*

Main category: cs.LG

TL;DR: ProPS是一种新颖的强化学习方法，将大型语言模型置于策略优化循环的核心，直接基于奖励反馈和自然语言输入提出策略更新，统一了数值和语义推理。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励信号，无法利用现实任务中丰富的语义知识。相比之下，人类学习能有效结合数值反馈与语言、先验知识和常识。需要一种能统一数值和语义推理的框架。

Method: 提出Prompted Policy Search (ProPS)方法，将大型语言模型置于策略优化循环的中心，直接基于奖励反馈和自然语言输入（如目标、领域知识、策略提示）提出策略更新。LLM在上下文中执行数值优化，整合语义信号以指导更明智的探索。

Result: 在15个Gymnasium任务（经典控制、Atari游戏、MuJoCo环境）上评估，与7种广泛采用的RL算法（如PPO、SAC、TRPO）比较。ProPS在15个任务中的8个上优于所有基线，当提供领域知识时表现出显著优势。

Conclusion: ProPS展示了统一语义和数值推理的潜力，能实现更透明、可泛化且与人类对齐的强化学习。LLM能够在上下文中执行数值优化，语义信号的整合能带来更明智的探索和样本高效的学习。

Abstract: Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augment existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop-directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, domain knowledge, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across fifteen Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on eight out of fifteen tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned RL.

</details>


### [62] [Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment](https://arxiv.org/abs/2511.21931)
*Henry Salgado,Meagan Kendall,Martine Ceberio*

Main category: cs.LG

TL;DR: 提出一个简单高效框架，通过比较数据本身特征重要性排序与模型解释排序，评估机器学习模型是否与数据底层结构对齐


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法主要关注解释模型行为，但缺乏评估模型是否真正反映数据底层结构的方法。需要一种能判断"模型是否说出了数据所说内容"的评估框架

Method: 借鉴Rubin潜在结果框架，量化每个特征在二分类任务中对结果组别的分离强度，估计特征对结果的影响。建立数据驱动的特征重要性基线，并与模型解释进行对比

Result: 开发了一个可解释、模型无关的方法来评估模型-数据对齐度。通过数据本身特征排序与模型解释排序的比较，为实践者提供模型是否反映数据结构的评估工具

Conclusion: 该方法为评估机器学习模型与数据结构的对齐提供了简单有效的框架，超越了传统描述性统计，能够更准确地判断模型是否真正捕捉了数据中的因果关系

Abstract: In this work, we propose a simple and computationally efficient framework to evaluate whether machine learning models align with the structure of the data they learn from; that is, whether \textit{the model says what the data says}. Unlike existing interpretability methods that focus exclusively on explaining model behavior, our approach establishes a baseline derived directly from the data itself. Drawing inspiration from Rubin's Potential Outcomes Framework, we quantify how strongly each feature separates the two outcome groups in a binary classification task, moving beyond traditional descriptive statistics to estimate each feature's effect on the outcome. By comparing these data-derived feature rankings against model-based explanations, we provide practitioners with an interpretable and model-agnostic method to assess model--data alignment.

</details>


### [63] [Modeling Quantum Autoencoder Trainable Kernel for IoT Anomaly Detection](https://arxiv.org/abs/2511.21932)
*Swathi Chandrasekhar,Shiva Raj Pokhrel,Swati Kumari,Navneet Singh*

Main category: cs.LG

TL;DR: 提出量子自编码器(QAE)框架，结合量子支持向量分类(QSVC)进行物联网入侵检测，在NISQ设备上展示实际量子优势


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法难以应对物联网流量高维复杂性，深度学习存在计算瓶颈，无法实时大规模部署

Method: 使用量子自编码器压缩网络流量为判别性潜在表示，结合量子支持向量分类进行入侵检测

Result: 在三个数据集上评估，在理想模拟器和IBM量子硬件上实现更高准确率，展示当前NISQ设备的实际量子优势

Conclusion: 量子机器学习是解决现实网络安全挑战的可行且硬件就绪方案，适度去极化噪声可作为隐式正则化提升泛化能力

Abstract: Escalating cyber threats and the high-dimensional complexity of IoT traffic have outpaced classical anomaly detection methods. While deep learning offers improvements, computational bottlenecks limit real-time deployment at scale. We present a quantum autoencoder (QAE) framework that compresses network traffic into discriminative latent representations and employs quantum support vector classification (QSVC) for intrusion detection. Evaluated on three datasets, our approach achieves improved accuracy on ideal simulators and on the IBM Quantum hardware demonstrating practical quantum advantage on current NISQ devices. Crucially, moderate depolarizing noise acts as implicit regularization, stabilizing training and enhancing generalization. This work establishes quantum machine learning as a viable, hardware-ready solution for real-world cybersecurity challenges.

</details>


### [64] [Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation](https://arxiv.org/abs/2511.21934)
*Tao Zhe,Huazhen Fang,Kunpeng Liu,Qian Lou,Tamzidul Hoque,Dongjie Wang*

Main category: cs.LG

TL;DR: 提出一种异构多智能体强化学习框架，用于自动化特征变换，通过共享批评器和多头注意力机制解决动态特征空间和智能体协作问题。


<details>
  <summary>Details</summary>
Motivation: 特征变换对于结构化数据下游任务性能提升至关重要，但现有自动化方法依赖启发式或穷举搜索，效率低下。虽然强化学习改进了传统方法，但仍存在动态特征扩展导致的不稳定性以及智能体间协作不足的问题。

Method: 提出异构多智能体强化学习框架，包含三种异构智能体，分为两类：特征选择智能体和操作选择智能体。采用共享批评器机制促进智能体间通信，使用多头注意力机制处理动态扩展的特征空间，并引入状态编码技术稳定学习过程。

Result: 通过大量实验验证了模型在有效性、效率、鲁棒性和可解释性方面的优越性能。

Conclusion: 该异构多智能体强化学习框架成功解决了自动化特征变换中的动态特征空间和智能体协作问题，为结构化数据的特征工程提供了更高效、稳定的解决方案。

Abstract: Feature transformation enhances downstream task performance by generating informative features through mathematical feature crossing. Despite the advancements in deep learning, feature transformation remains essential for structured data, where deep models often struggle to capture complex feature interactions. Prior literature on automated feature transformation has achieved success but often relies on heuristics or exhaustive searches, leading to inefficient and time-consuming processes. Recent works employ reinforcement learning (RL) to enhance traditional approaches through a more effective trial-and-error way. However, two limitations remain: 1) Dynamic feature expansion during the transformation process, which causes instability and increases the learning complexity for RL agents; 2) Insufficient cooperation and communication between agents, which results in suboptimal feature crossing operations and degraded model performance. To address them, we propose a novel heterogeneous multi-agent RL framework to enable cooperative and scalable feature transformation. The framework comprises three heterogeneous agents, grouped into two types, each designed to select essential features and operations for feature crossing. To enhance communication among these agents, we implement a shared critic mechanism that facilitates information exchange during feature transformation. To handle the dynamically expanding feature space, we tailor multi-head attention-based feature agents to select suitable features for feature crossing. Additionally, we introduce a state encoding technique during the optimization process to stabilize and enhance the learning dynamics of the RL agents, resulting in more robust and reliable transformation policies. Finally, we conduct extensive experiments to validate the effectiveness, efficiency, robustness, and interpretability of our model.

</details>


### [65] [Breaking Algorithmic Collusion in Human-AI Ecosystems](https://arxiv.org/abs/2511.21935)
*Natalie Collina,Eshwar Ram Arunachaleswaran,Meena Jagadeesan*

Main category: cs.LG

TL;DR: 研究混合人机生态系统中算法合谋的脆弱性，发现即使单个人类不采用AI定价策略也能破坏合谋，多个人类参与会进一步压低价格


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在生态系统中与人类和其他AI频繁交互，需要研究这种混合生态系统中算法合谋的稳定性，特别是当人类不采用AI策略时对价格的影响

Method: 使用重复定价博弈的理论框架，AI代理采用均衡策略，而人类采用无遗憾策略，分析人类"背叛"AI策略对合谋稳定性的影响

Result: 单个人类背叛就能破坏合谋并压低价格，多个人类背叛会使价格更接近竞争水平；当AI意识到人类背叛时，合谋性质会发生变化

Conclusion: 算法合谋在混合人机生态系统中可能是脆弱的，人类参与会破坏AI之间的价格合谋，但具体取决于AI是否意识到人类背叛行为

Abstract: AI agents are increasingly deployed in ecosystems where they repeatedly interact not only with each other but also with humans. In this work, we study these human-AI ecosystems from a theoretical perspective, focusing on the classical framework of repeated pricing games. In our stylized model, the AI agents play equilibrium strategies, and one or more humans manually perform the pricing task instead of adopting an AI agent, thereby defecting to a no-regret strategy. Motivated by how populations of AI agents can sustain supracompetitive prices, we investigate whether high prices persist under such defections. Our main finding is that even a single human defection can destabilize collusion and drive down prices, and multiple defections push prices even closer to competitive levels. We further show how the nature of collusion changes under defection-aware AI agents. Taken together, our results characterize when algorithmic collusion is fragile--and when it persists--in mixed ecosystems of AI agents and humans.

</details>


### [66] [Deep Learning Architectures for Code-Modulated Visual Evoked Potentials Detection](https://arxiv.org/abs/2511.21940)
*Kiran Nair,Hubert Cecotti*

Main category: cs.LG

TL;DR: 该研究提出多种深度学习架构用于C-VEP脑机接口解码，其中多类孪生网络在单次试验中达到96.89%的平均准确率，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 基于C-VEP的非侵入式脑机接口需要高度鲁棒的解码方法来应对EEG信号的时间变异性和会话依赖性噪声，传统方法在这方面存在局限。

Method: 提出并评估了多种深度学习架构：用于63位m序列重构和分类的卷积神经网络，基于相似性解码的孪生网络，以及CCA基线方法。使用地球移动距离和约束地球移动距离进行距离解码，并采用时间数据增强技术。

Result: 深度学习模型显著优于传统方法，基于地球移动距离的解码对延迟变化具有更好的鲁棒性。时间数据增强提高了跨会话泛化能力，多类孪生网络达到96.89%的平均准确率。

Conclusion: 数据驱动的深度学习架构在单次试验C-VEP解码中表现出色，为自适应非侵入式脑机接口系统提供了可靠解决方案，展示了深度学习在脑机接口解码中的潜力。

Abstract: Non-invasive Brain-Computer Interfaces (BCIs) based on Code-Modulated Visual Evoked Potentials (C-VEPs) require highly robust decoding methods to address temporal variability and session-dependent noise in EEG signals. This study proposes and evaluates several deep learning architectures, including convolutional neural networks (CNNs) for 63-bit m-sequence reconstruction and classification, and Siamese networks for similarity-based decoding, alongside canonical correlation analysis (CCA) baselines. EEG data were recorded from 13 healthy adults under single-target flicker stimulation. The proposed deep models significantly outperformed traditional approaches, with distance-based decoding using Earth Mover's Distance (EMD) and constrained EMD showing greater robustness to latency variations than Euclidean and Mahalanobis metrics. Temporal data augmentation with small shifts further improved generalization across sessions. Among all models, the multi-class Siamese network achieved the best overall performance with an average accuracy of 96.89%, demonstrating the potential of data-driven deep architectures for reliable, single-trial C-VEP decoding in adaptive non-invasive BCI systems.

</details>


### [67] [ABLE: Using Adversarial Pairs to Construct Local Models for Explaining Model Predictions](https://arxiv.org/abs/2511.21952)
*Krishna Khadka,Sunny Shree,Pujan Budhathoki,Yu Lei,Raghu Kacker,D. Richard Kuhn*

Main category: cs.LG

TL;DR: ABLE方法通过生成对抗点对来定位决策边界，训练线性模型提供更稳定、高保真的局部解释


<details>
  <summary>Details</summary>
Motivation: 机器学习模型缺乏透明度，现有局部解释方法（如LIME）存在不稳定性和局部保真度差的问题，需要改进

Method: 首先在测试实例附近生成邻域点，然后对每个邻域点进行两次对抗攻击：第一次生成标签不同的对抗点A，第二次从A生成标签与原始点相同的点A'，形成对抗点对来定位决策边界，最后在这些对抗点对上训练线性模型

Result: 在六个UCI基准数据集和三种深度神经网络架构上的实验表明，ABLE方法比现有最先进方法具有更高的稳定性和保真度

Conclusion: ABLE方法通过对抗点对定位决策边界，有效提高了局部解释的稳定性和保真度，为黑盒模型提供了更可靠的解释

Abstract: Machine learning models are increasingly used in critical applications but are mostly "black boxes" due to their lack of transparency. Local explanation approaches, such as LIME, address this issue by approximating the behavior of complex models near a test instance using simple, interpretable models. However, these approaches often suffer from instability and poor local fidelity. In this paper, we propose a novel approach called Adversarially Bracketed Local Explanation (ABLE) to address these limitations. Our approach first generates a set of neighborhood points near the test instance, x_test, by adding bounded Gaussian noise. For each neighborhood point D, we apply an adversarial attack to generate an adversarial point A with minimal perturbation that results in a different label than D. A second adversarial attack is then performed on A to generate a point A' that has the same label as D (and thus different than A). The points A and A' form an adversarial pair that brackets the local decision boundary for x_test. We then train a linear model on these adversarial pairs to approximate the local decision boundary. Experimental results on six UCI benchmark datasets across three deep neural network architectures demonstrate that our approach achieves higher stability and fidelity than the state-of-the-art.

</details>


### [68] [CTR Prediction on Alibaba's Taobao Advertising Dataset Using Traditional and Deep Learning Models](https://arxiv.org/abs/2511.21963)
*Hongyu Yang,Chunxi Wen,Jiyin Zhang,Nanfei Shen,Shijiao Zhang,Xiyan Han*

Main category: cs.LG

TL;DR: 该研究探索使用大规模淘宝数据集进行CTR预测，从传统机器学习方法（逻辑回归、Light-GBM）到深度学习模型（MLP、Transformer），Transformer模型在AUC指标上比基线提升2.81%，并提出了A/B测试策略和将技术扩展到公共卫生领域的应用前景。


<details>
  <summary>Details</summary>
Motivation: 点击率预测在现代广告系统中至关重要，直接影响平台效率和商业价值。传统监督学习模型虽然快速可解释，但难以捕捉驱动点击的行为模式，特别是用户兴趣随时间变化的动态特征。

Method: 1. 使用大规模淘宝数据集（22天、数亿次交互）；2. 从静态特征（用户人口统计、广告属性、上下文元数据）开始，使用逻辑回归和Light-GBM作为基准；3. 提取和编码用户行为序列，构建用户兴趣的时间表示；4. 使用深度学习模型融合行为嵌入和静态特征，包括多层感知机（MLP）；5. 设计基于Transformer的架构，利用自注意力机制学习行为序列中的上下文依赖关系，建模用户交互内容、时间和频率。

Result: Transformer模型在AUC指标上比逻辑回归基线提升2.81%，对于兴趣多样化或随时间变化的用户群体提升效果最显著。研究还提出了A/B测试策略用于实际评估。

Conclusion: 该研究为点击率预测提供了从传统方法到深度学习的完整路线图，证明了建模用户行为时序动态的重要性。研究还指出个性化广告定向技术可扩展到公共卫生场景，实现健康信息或行为指导的精准投放，展示了技术超越电子商务的潜在价值。

Abstract: Click-through rates prediction is critical in modern advertising systems, where ranking relevance and user engagement directly impact platform efficiency and business value. In this project, we explore how to model CTR more effectively using a large-scale Taobao dataset released by Alibaba. We start with supervised learning models, including logistic regression and Light-GBM, that are trained on static features such as user demographics, ad attributes, and contextual metadata. These models provide fast, interpretable benchmarks, but have limited capabilities to capture patterns of behavior that drive clicks. To better model user intent, we combined behavioral data from hundreds of millions of interactions over a 22-day period. By extracting and encoding user action sequences, we construct representations of user interests over time. We use deep learning models to fuse behavioral embeddings with static features. Among them, multilayer perceptrons (MLPs) have achieved significant performance improvements. To capture temporal dynamics, we designed a Transformer-based architecture that uses a self-attention mechanism to learn contextual dependencies across behavioral sequences, modeling not only what the user interacts with, but also the timing and frequency of interactions. Transformer improves AUC by 2.81 % over the baseline (LR model), with the largest gains observed for users whose interests are diverse or change over time. In addition to modeling, we propose an A/B testing strategy for real-world evaluation. We also think about the broader implications: personalized ad targeting technology can be applied to public health scenarios to achieve precise delivery of health information or behavior guidance. Our research provides a roadmap for advancing click-through rate predictions and extending their value beyond e-commerce.

</details>


### [69] [MOTIF-RF: Multi-template On-chip Transformer Synthesis Incorporating Frequency-domain Self-transfer Learning for RFIC Design Automation](https://arxiv.org/abs/2511.21970)
*Houbo He,Yizhou Xu,Lei Xia,Yaolong Hu,Fan Cai,Taiyun Chi*

Main category: cs.LG

TL;DR: 该论文系统研究了用于射频集成电路变压器逆向设计的机器学习代理模型，提出了频率域自迁移学习技术提升精度，并开发了基于CMA-ES算法的逆向设计框架。


<details>
  <summary>Details</summary>
Motivation: 射频集成电路变压器的传统设计方法耗时且依赖专家经验，需要开发AI辅助的规格到版图自动化设计工具，提高设计效率。

Method: 1) 基准测试四种ML架构（MLP、CNN、UNet、GT）；2) 提出频率域自迁移学习技术，利用相邻频段相关性提升S参数预测精度；3) 基于CMA-ES算法开发逆向设计框架。

Result: 频率域自迁移学习使S参数预测精度提升30%-50%；基于CMA-ES的逆向设计框架在多个阻抗匹配任务中表现出快速收敛和可靠性能。

Conclusion: 该研究推进了AI辅助射频集成电路规格到版图自动化的目标，为射频集成电路设计师提供了实用的AI集成工具。

Abstract: This paper presents a systematic study on developing multi-template machine learning (ML) surrogate models and applying them to the inverse design of transformers (XFMRs) in radio-frequency integrated circuits (RFICs). Our study starts with benchmarking four widely used ML architectures, including MLP-, CNN-, UNet-, and GT-based models, using the same datasets across different XFMR topologies. To improve modeling accuracy beyond these baselines, we then propose a new frequency-domain self-transfer learning technique that exploits correlations between adjacent frequency bands, leading to around 30%-50% accuracy improvement in the S-parameters prediction. Building on these models, we further develop an inverse design framework based on the covariance matrix adaptation evolutionary strategy (CMA-ES) algorithm. This framework is validated using multiple impedance-matching tasks, all demonstrating fast convergence and trustworthy performance. These results advance the goal of AI-assisted specs-to-GDS automation for RFICs and provide RFIC designers with actionable tools for integrating AI into their workflows.

</details>


### [70] [A Safety and Security Framework for Real-World Agentic Systems](https://arxiv.org/abs/2511.21990)
*Shaona Ghosh,Barnaby Simkin,Kyriacos Shiarlis,Soumili Nandi,Dan Zhao,Matthew Fiedler,Julia Bazinska,Nikki Pope,Roopa Prabhu,Daniel Rohrer,Michael Demoret,Bartley Richardson*

Main category: cs.LG

TL;DR: 提出一个动态可操作的框架，用于保护企业部署中的智能体AI系统，将安全和安全视为从模型、编排器、工具和数据动态交互中涌现的特性，并通过AI驱动的红队测试进行风险发现和缓解。


<details>
  <summary>Details</summary>
Motivation: 传统上，AI系统的安全和安全被视为独立问题，但在智能体系统中，它们相互关联且从动态交互中涌现。现有方法未能充分解决智能体特有的风险，如工具滥用、级联行动链和意外控制放大等。

Method: 提出动态智能体安全与安全框架，通过辅助AI模型和智能体（在人类监督下）进行上下文风险发现、评估和缓解。开发操作化智能体风险分类法，统一传统安全问题和新型智能体风险。采用沙盒化、AI驱动的红队测试进行风险发现。

Result: 在NVIDIA旗舰智能体研究助手AI-Q Research Assistant上进行了详细案例研究，展示了在复杂企业级智能体工作流中端到端的安全评估效果。风险发现阶段识别了新型智能体风险并进行上下文缓解。发布了包含10,000多个真实攻击和防御执行轨迹的数据集。

Conclusion: 智能体系统的安全和安全是动态涌现特性，需要新的框架来管理。提出的动态框架能有效识别和缓解智能体特有风险，通过AI辅助的风险管理和红队测试，为企业部署智能体AI系统提供了实用的安全保障方法。

Abstract: This paper introduces a dynamic and actionable framework for securing agentic AI systems in enterprise deployment. We contend that safety and security are not merely fixed attributes of individual models but also emergent properties arising from the dynamic interactions among models, orchestrators, tools, and data within their operating environments. We propose a new way of identification of novel agentic risks through the lens of user safety. Although, for traditional LLMs and agentic models in isolation, safety and security has a clear separation, through the lens of safety in agentic systems, they appear to be connected. Building on this foundation, we define an operational agentic risk taxonomy that unifies traditional safety and security concerns with novel, uniquely agentic risks, including tool misuse, cascading action chains, and unintended control amplification among others. At the core of our approach is a dynamic agentic safety and security framework that operationalizes contextual agentic risk management by using auxiliary AI models and agents, with human oversight, to assist in contextual risk discovery, evaluation, and mitigation. We further address one of the most challenging aspects of safety and security of agentic systems: risk discovery through sandboxed, AI-driven red teaming. We demonstrate the framework effectiveness through a detailed case study of NVIDIA flagship agentic research assistant, AI-Q Research Assistant, showcasing practical, end-to-end safety and security evaluations in complex, enterprise-grade agentic workflows. This risk discovery phase finds novel agentic risks that are then contextually mitigated. We also release the dataset from our case study, containing traces of over 10,000 realistic attack and defense executions of the agentic workflow to help advance research in agentic safety.

</details>


### [71] [Distance-based Learning of Hypertrees](https://arxiv.org/abs/2511.22014)
*Shaun Fallat,Kamyar Khodamoradi,David Kirkpatrick,Valerii Maliuk,S. Ahmad Mojallal,Sandra Zilles*

Main category: cs.LG

TL;DR: 本文提出了首个可证明最优的在线算法，用于学习一类称为有序超树的超图结构，使用最短路径查询。该算法可转化为最优离线算法，并研究了有界距离查询模型下的学习复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究如何使用最短路径查询学习超图结构，特别是在进化树重建等实际应用中，距离测量可能随距离增加而退化，因此需要考虑有界距离查询模型。

Method: 提出针对有序超树的在线算法，该算法使用最短路径查询，并可转化为离线算法。同时研究有界距离查询模型下的学习复杂度。

Result: 1) 提出了首个可证明最优的在线算法学习有序超树；2) 该算法可转化为最优离线算法；3) 有序超树在Fagin层次结构中处于可学习且查询复杂度低于二次的最宽类别；4) 在有界距离查询模型下，获得了学习一般超树的渐近紧复杂度界。

Conclusion: 本文在超图学习领域取得了重要进展，为有序超树提供了最优学习算法，并扩展了有界距离查询模型下的理论结果，对数据库理论和进化生物学等应用领域具有重要意义。

Abstract: We study the problem of learning hypergraphs with shortest-path queries (SP-queries), and present the first provably optimal online algorithm for a broad and natural class of hypertrees that we call orderly hypertrees. Our online algorithm can be transformed into a provably optimal offline algorithm. Orderly hypertrees can be positioned within the Fagin hierarchy of acyclic hypergraph (well-studied in database theory), and strictly encompass the broadest class in this hierarchy that is learnable with subquadratic SP-query complexity.
  Recognizing that in some contexts, such as evolutionary tree reconstruction, distance measurements can degrade with increased distance, we also consider a learning model that uses bounded distance queries. In this model, we demonstrate asymptotically tight complexity bounds for learning general hypertrees.

</details>


### [72] [Equilibrium Propagation Without Limits](https://arxiv.org/abs/2511.22024)
*Elon Litman*

Main category: cs.LG

TL;DR: 论文为平衡传播(EP)建立了有限扰动理论基础，通过吉布斯-玻尔兹曼分布建模网络状态，证明赫布对比学习的更新是任意有限扰动的精确梯度估计器。


<details>
  <summary>Details</summary>
Motivation: 传统平衡传播依赖于无限小扰动的近似，限制了其在实际应用中的效果。本文旨在将平衡传播从无限小扰动的限制中解放出来，建立有限扰动的基础理论。

Method: 将网络状态建模为吉布斯-玻尔兹曼分布而非确定性点，证明赫姆霍兹自由能差值的梯度等于局部能量导数的期望差值。基于损失-能量协方差的路径积分推导广义EP算法。

Result: 验证了经典对比赫布学习更新是任意有限扰动的精确梯度估计器，无需无限小近似或凸性假设。推导的广义EP算法能够支持标准无限小近似无法处理的强误差信号。

Conclusion: 成功建立了平衡传播的有限扰动理论基础，使学习算法能够处理强误差信号，扩展了平衡传播的应用范围和实用性。

Abstract: We liberate Equilibrium Propagation (EP) from the limit of infinitesimal perturbations by establishing a finite-nudge foundation for local credit assignment. By modeling network states as Gibbs-Boltzmann distributions rather than deterministic points, we prove that the gradient of the difference in Helmholtz free energy between a nudged and free phase is exactly the difference in expected local energy derivatives. This validates the classic Contrastive Hebbian Learning update as an exact gradient estimator for arbitrary finite nudging, requiring neither infinitesimal approximations nor convexity. Furthermore, we derive a generalized EP algorithm based on the path integral of loss-energy covariances, enabling learning with strong error signals that standard infinitesimal approximations cannot support.

</details>


### [73] [Calibration-Free EEG-based Driver Drowsiness Detection with Online Test-Time Adaptation](https://arxiv.org/abs/2511.22030)
*Geun-Deok Jang,Dong-Kyun Han,Seo-Hyeon Park,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 提出基于在线测试时适应的驾驶员疲劳检测框架，通过动态调整批归一化层参数和使用记忆库管理EEG片段，在非独立同分布场景下显著提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）信号存在个体间差异导致的领域偏移问题，使得疲劳检测模型难以泛化到未见过的目标对象，需要解决校准过程繁琐的问题。

Method: 提出在线测试时适应（TTA）框架：1）更新批归一化层可学习参数同时保留预训练归一化统计量；2）使用基于负能量分数和持续时间的记忆库动态管理流式EEG片段；3）引入原型学习以应对时间分布偏移。

Result: 在模拟驾驶环境中验证，平均F1分数达到81.73%，比最佳TTA基线提升11.73%，显著增强了EEG疲劳检测系统在非独立同分布场景下的适应性。

Conclusion: 提出的在线测试时适应方法有效解决了EEG信号的个体间差异问题，显著提升了驾驶员疲劳检测系统的泛化能力和实际应用价值。

Abstract: Drowsy driving is a growing cause of traffic accidents, prompting recent exploration of electroencephalography (EEG)-based drowsiness detection systems. However, the inherent variability of EEG signals due to psychological and physical factors necessitates a cumbersome calibration process. In particular, the inter-subject variability of EEG signals leads to a domain shift problem, which makes it challenging to generalize drowsiness detection models to unseen target subjects. To address these issues, we propose a novel driver drowsiness detection framework that leverages online test-time adaptation (TTA) methods to dynamically adjust to target subject distributions. Our proposed method updates the learnable parameters in batch normalization (BN) layers, while preserving pretrained normalization statistics, resulting in a modified configuration that ensures effective adaptation during test time. We incorporate a memory bank that dynamically manages streaming EEG segments, selecting samples based on their reliability determined by negative energy scores and persistence time. In addition, we introduce prototype learning to ensure robust predictions against distribution shifts over time. We validated our method on the sustained-attention driving dataset collected in a simulated environment, where drowsiness was estimated from delayed reaction times during monotonous lane-keeping tasks. Our experiments show that our method outperforms all baselines, achieving an average F1-score of 81.73\%, an improvement of 11.73\% over the best TTA baseline. This demonstrates that our proposed method significantly enhances the adaptability of EEG-based drowsiness detection systems in non-i.i.d. scenarios.

</details>


### [74] [Predicting Public Health Impacts of Electricity Usage](https://arxiv.org/abs/2511.22031)
*Yejia Liu,Zhifeng Wu,Pengfei Li,Shaolei Ren*

Main category: cs.LG

TL;DR: 开发HealthPredictor AI模型，将电力使用与公共健康结果连接，通过优化用电行为降低空气污染对公众健康的影响。


<details>
  <summary>Details</summary>
Motivation: 电力行业是空气污染的主要来源，尽管已有监管措施，但化石燃料仍是重要能源组成部分，需要更先进的需求侧方法来减少对公众健康的影响。

Method: 开发HealthPredictor模型，包含三个组件：燃料组合预测器（估计不同发电来源贡献）、空气质量转换器（建模污染物排放和大气扩散）、健康影响评估器（将污染物变化转化为货币化健康损害）。

Result: 在美国多个地区的测试中，健康驱动的优化框架在公共健康影响预测方面显著优于基于燃料组合的基线方法。电动汽车充电调度的案例研究展示了该方法能实现的公共健康收益和可操作指导。

Conclusion: 这项工作展示了AI模型如何被明确设计用于实现健康导向的能源管理，以促进公共卫生和更广泛的社会福祉。模型代码和数据集已开源。

Abstract: The electric power sector is a leading source of air pollutant emissions, impacting the public health of nearly every community. Although regulatory measures have reduced air pollutants, fossil fuels remain a significant component of the energy supply, highlighting the need for more advanced demand-side approaches to reduce the public health impacts. To enable health-informed demand-side management, we introduce HealthPredictor, a domain-specific AI model that provides an end-to-end pipeline linking electricity use to public health outcomes. The model comprises three components: a fuel mix predictor that estimates the contribution of different generation sources, an air quality converter that models pollutant emissions and atmospheric dispersion, and a health impact assessor that translates resulting pollutant changes into monetized health damages. Across multiple regions in the United States, our health-driven optimization framework yields substantially lower prediction errors in terms of public health impacts than fuel mix-driven baselines. A case study on electric vehicle charging schedules illustrates the public health gains enabled by our method and the actionable guidance it can offer for health-informed energy management. Overall, this work shows how AI models can be explicitly designed to enable health-informed energy management for advancing public health and broader societal well-being. Our datasets and code are released at: https://github.com/Ren-Research/Health-Impact-Predictor.

</details>


### [75] [Convergence Dynamics of Over-Parameterized Score Matching for a Single Gaussian](https://arxiv.org/abs/2511.22069)
*Yiran Zhang,Weihang Xu,Mo Zhou,Maryam Fazel,Simon Shaolei Du*

Main category: cs.LG

TL;DR: 该论文首次在分数匹配框架下为至少三个分量的高斯混合模型建立了全局收敛保证，分析了过参数化模型学习单个高斯分布时梯度下降的优化动态。


<details>
  <summary>Details</summary>
Motivation: 尽管分数匹配在现代生成建模（特别是扩散模型）中取得了经验成功，但在过参数化机制下其优化行为的理论理解仍然有限。作者旨在分析梯度下降在训练过参数化模型学习单个高斯分布时的优化动态。

Method: 使用具有n个可学习参数的学生模型，在从单个真实高斯分布生成的数据上训练，采用总体分数匹配目标。分析不同机制下的优化动态：高噪声机制证明全局收敛；低噪声机制识别平稳点存在；特定初始化条件下（指数小初始化）证明收敛；随机高斯初始化时分析收敛行为。

Result: 1) 噪声足够大时，梯度下降全局收敛；2) 低噪声机制存在平稳点；3) 参数指数小初始化时，所有参数收敛到真实值；4) 无指数小初始化时参数可能不收敛；5) 随机高斯初始化时，高概率下只有一个参数收敛而其他发散，但损失以1/τ速率收敛到零；6) 建立了该机制下收敛率的近乎匹配下界。

Conclusion: 这是首个在分数匹配框架下为至少三个分量的高斯混合模型建立全局收敛保证的工作，揭示了过参数化分数匹配优化中初始化条件的关键作用，为理解扩散模型训练动态提供了理论洞见。

Abstract: Score matching has become a central training objective in modern generative modeling, particularly in diffusion models, where it is used to learn high-dimensional data distributions through the estimation of score functions. Despite its empirical success, the theoretical understanding of the optimization behavior of score matching, particularly in over-parameterized regimes, remains limited. In this work, we study gradient descent for training over-parameterized models to learn a single Gaussian distribution. Specifically, we use a student model with $n$ learnable parameters and train it on data generated from a single ground-truth Gaussian using the population score matching objective. We analyze the optimization dynamics under multiple regimes. When the noise scale is sufficiently large, we prove a global convergence result for gradient descent. In the low-noise regime, we identify the existence of a stationary point, highlighting the difficulty of proving global convergence in this case. Nevertheless, we show convergence under certain initialization conditions: when the parameters are initialized to be exponentially small, gradient descent ensures convergence of all parameters to the ground truth. We further prove that without the exponentially small initialization, the parameters may not converge to the ground truth. Finally, we consider the case where parameters are randomly initialized from a Gaussian distribution far from the ground truth. We prove that, with high probability, only one parameter converges while the others diverge, yet the loss still converges to zero with a $1/τ$ rate, where $τ$ is the number of iterations. We also establish a nearly matching lower bound on the convergence rate in this regime. This is the first work to establish global convergence guarantees for Gaussian mixtures with at least three components under the score matching framework.

</details>


### [76] [A Multi-View Multi-Timescale Hypergraph-Empowered Spatiotemporal Framework for EV Charging Forecasting](https://arxiv.org/abs/2511.22072)
*Jinhao Li,Hao Wang*

Main category: cs.LG

TL;DR: HyperCast：基于超图建模高阶时空依赖的电动汽车充电需求预测框架，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的方法只能建模充电站间的成对关系，无法捕捉城市充电网络中复杂的群体动态和集体充电行为模式

Method: 开发HyperCast框架，利用超图建模高阶时空依赖；整合多视图超图（静态地理邻近和动态需求相似性）和多时间尺度输入；采用超时空块和跨注意力机制融合信息

Result: 在四个公共数据集上的实验表明，HyperCast显著优于多种最先进的基线方法，证明了显式建模集体充电行为的有效性

Conclusion: 通过超图建模高阶依赖关系能够更准确地预测电动汽车充电需求，为电网稳定运行和电动汽车参与电力市场提供支持

Abstract: Accurate electric vehicle (EV) charging demand forecasting is essential for stable grid operation and proactive EV participation in electricity market. Existing forecasting methods, particularly those based on graph neural networks, are often limited to modeling pairwise relationships between stations, failing to capture the complex, group-wise dynamics inherent in urban charging networks. To address this gap, we develop a novel forecasting framework namely HyperCast, leveraging the expressive power of hypergraphs to model the higher-order spatiotemporal dependencies hidden in EV charging patterns. HyperCast integrates multi-view hypergraphs, which capture both static geographical proximity and dynamic demand-based functional similarities, along with multi-timescale inputs to differentiate between recent trends and weekly periodicities. The framework employs specialized hyper-spatiotemporal blocks and tailored cross-attention mechanisms to effectively fuse information from these diverse sources: views and timescales. Extensive experiments on four public datasets demonstrate that HyperCast significantly outperforms a wide array of state-of-the-art baselines, demonstrating the effectiveness of explicitly modeling collective charging behaviors for more accurate forecasting.

</details>


### [77] [ARES: Anomaly Recognition Model For Edge Streams](https://arxiv.org/abs/2511.22078)
*Simone Mungari,Albert Bifet,Giuseppe Manco,Bernhard Pfahringer*

Main category: cs.LG

TL;DR: ARES是一个用于时序图边流异常检测的无监督框架，结合图神经网络和半空间树，能实时检测异常边连接，并通过监督阈值机制优化检测性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的流式信息常表示为时序图，其中边随时间动态变化。实时检测边异常对降低潜在风险至关重要。传统方法面临概念漂移、数据量大和实时响应需求等挑战。

Method: ARES结合图神经网络（GNN）进行特征提取和半空间树（HST）进行异常评分。GNN将节点和边属性嵌入潜在空间以捕获尖峰和突发异常行为，HST对该空间进行分区以高效隔离异常。还加入了基于统计离散度的监督阈值机制，使用少量标注数据确定最优阈值。

Result: 在多个真实网络攻击场景中进行广泛评估，与现有方法相比表现出色，同时分析了其空间和时间复杂度。

Conclusion: ARES是一个有效的无监督异常检测框架，能够实时检测时序图中的边异常，适应不同领域，并通过监督阈值机制进一步提升检测能力。

Abstract: Many real-world scenarios involving streaming information can be represented as temporal graphs, where data flows through dynamic changes in edges over time. Anomaly detection in this context has the objective of identifying unusual temporal connections within the graph structure. Detecting edge anomalies in real time is crucial for mitigating potential risks. Unlike traditional anomaly detection, this task is particularly challenging due to concept drifts, large data volumes, and the need for real-time response. To face these challenges, we introduce ARES, an unsupervised anomaly detection framework for edge streams. ARES combines Graph Neural Networks (GNNs) for feature extraction with Half-Space Trees (HST) for anomaly scoring. GNNs capture both spike and burst anomalous behaviors within streams by embedding node and edge properties in a latent space, while HST partitions this space to isolate anomalies efficiently. ARES operates in an unsupervised way without the need for prior data labeling. To further validate its detection capabilities, we additionally incorporate a simple yet effective supervised thresholding mechanism. This approach leverages statistical dispersion among anomaly scores to determine the optimal threshold using a minimal set of labeled data, ensuring adaptability across different domains. We validate ARES through extensive evaluations across several real-world cyber-attack scenarios, comparing its performance against existing methods while analyzing its space and time complexity.

</details>


### [78] [A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization](https://arxiv.org/abs/2511.22080)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Chang Liu,Qipeng Xie,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWMSAM：解决联邦学习中动量与SAM结合的两个失效模式，通过动量引导的全局扰动和对齐机制，实现快速收敛和良好泛化。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中需要模型在有限通信预算下快速收敛，同时在非独立同分布数据上良好泛化。现有方法简单结合动量和SAM存在两个结构性问题：局部-全局曲率不对齐和动量回波振荡。

Method: 提出FedWMSAM：1）使用服务器聚合的动量构建动量引导的全局扰动，使客户端SAM方向与全局下降几何对齐，实现单反向传播SAM近似；2）通过余弦相似度自适应规则耦合动量和SAM，形成早期动量、晚期SAM的两阶段训练计划。

Result: 提供了非独立同分布收敛边界，明确建模扰动引起的方差及其对参数依赖关系。在多个数据集和模型架构上的实验验证了方法的有效性、适应性和鲁棒性。

Conclusion: FedWMSAM成功解决了联邦学习中动量与SAM结合的两个失效模式，在优化挑战方面表现出优越性，为联邦学习提供了更有效的优化方案。

Abstract: In federated learning (FL), models must \emph{converge quickly} under tight communication budgets while \emph{generalizing} across non-IID client distributions. These twin requirements have naturally led to two widely used techniques: client/server \emph{momentum} to accelerate progress, and \emph{sharpness-aware minimization} (SAM) to prefer flat solutions. However, simply combining momentum and SAM leaves two structural issues unresolved in non-IID FL. We identify and formalize two failure modes: \emph{local-global curvature misalignment} (local SAM directions need not reflect the global loss geometry) and \emph{momentum-echo oscillation} (late-stage instability caused by accumulated momentum). To our knowledge, these failure modes have not been jointly articulated and addressed in the FL literature. We propose \textbf{FedWMSAM} to address both failure modes. First, we construct a momentum-guided global perturbation from server-aggregated momentum to align clients' SAM directions with the global descent geometry, enabling a \emph{single-backprop} SAM approximation that preserves efficiency. Second, we couple momentum and SAM via a cosine-similarity adaptive rule, yielding an early-momentum, late-SAM two-phase training schedule. We provide a non-IID convergence bound that \emph{explicitly models the perturbation-induced variance} $σ_ρ^2=σ^2+(Lρ)^2$ and its dependence on $(S, K, R, N)$ on the theory side. We conduct extensive experiments on multiple datasets and model architectures, and the results validate the effectiveness, adaptability, and robustness of our method, demonstrating its superiority in addressing the optimization challenges of Federated Learning. Our code is available at https://github.com/Huang-Yongzhi/NeurlPS_FedWMSAM.

</details>


### [79] [Heteroscedastic Neural Networks for Path Loss Prediction with Link-Specific Uncertainty](https://arxiv.org/abs/2511.23243)
*Jonathan Ethier*

Main category: cs.LG

TL;DR: 提出一种神经网络模型，联合预测路径损耗的均值和链路特定方差，实现异方差不确定性估计，在RF测试数据上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统和现代机器学习路径损耗模型通常假设恒定预测方差，无法提供链路特定的不确定性估计，限制了RF规划和干扰分析的准确性。

Method: 设计神经网络通过最小化高斯负对数似然来联合预测均值和链路特定方差，比较了共享参数、部分共享参数和独立参数三种架构。

Result: 共享参数架构表现最佳：RMSE为7.4 dB，95%预测区间的覆盖率达到95.1%，平均区间宽度为29.6 dB。不确定性估计支持链路特定覆盖余量，改善RF规划和干扰分析。

Conclusion: 提出的异方差不确定性估计方法优于传统恒定方差假设，为RF规划提供更准确的链路特定不确定性信息，并能有效诊断模型弱点。

Abstract: Traditional and modern machine learning-based path loss models typically assume a constant prediction variance. We propose a neural network that jointly predicts the mean and link-specific variance by minimizing a Gaussian negative log-likelihood, enabling heteroscedastic uncertainty estimates. We compare shared, partially shared, and independent-parameter architectures using accuracy, calibration, and sharpness metrics on blind test sets from large public RF drive-test datasets. The shared-parameter architecture performs best, achieving an RMSE of 7.4 dB, 95.1 percent coverage for 95 percent prediction intervals, and a mean interval width of 29.6 dB. These uncertainty estimates further support link-specific coverage margins, improve RF planning and interference analyses, and provide effective self-diagnostics of model weaknesses.

</details>


### [80] [Quantum Bayesian Optimization for Quality Improvement in Fuselage Assembly](https://arxiv.org/abs/2511.22090)
*Jiayu Liu,Chong Liu,Trevor Rhone,Yinan Wang*

Main category: cs.LG

TL;DR: 提出量子贝叶斯优化框架，通过量子算法提高航空航天机身装配中形状调整的样本效率，显著降低尺寸误差和不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有智能制造方法在航空航天机身装配中面临样本效率低的问题，源于经典蒙特卡洛方法在估计分布均值时的局限性。量子算法能以更少样本达到相同估计精度，这为解决制造系统中的样本效率问题提供了新思路。

Method: 提出量子贝叶斯优化框架，利用基于有限元分析或代理模型的量子预言机，以更少查询获得更准确的环境响应估计。采用上置信界作为采集函数，策略性地选择最可能最大化目标函数的输入值。

Result: 在机身装配案例研究中，应用力控执行器调整形状以减少相邻部件间隙。实验结果表明，与经典方法相比，QBO在相同仿真查询下实现了显著更低的尺寸误差和不确定性。

Conclusion: 量子贝叶斯优化框架通过利用量子算法的样本效率优势，为航空航天制造中的精确形状控制提供了有效解决方案，在保持优化结果可比性的同时大幅减少所需样本量。

Abstract: Recent efforts in smart manufacturing have enhanced aerospace fuselage assembly processes, particularly by innovating shape adjustment techniques to minimize dimensional gaps between assembled sections. Existing approaches have shown promising results but face the issue of low sample efficiency from the manufacturing systems. It arises from the limitation of the classical Monte Carlo method when uncovering the mean response from a distribution. In contrast, recent work has shown that quantum algorithms can achieve the same level of estimation accuracy with significantly fewer samples than the classical Monte Carlo method from distributions. Therefore, we can adopt the estimation of the quantum algorithm to obtain the estimation from real physical systems (distributions). Motivated by this advantage, we propose a Quantum Bayesian Optimization (QBO) framework for precise shape control during assembly to improve the sample efficiency in manufacturing practice. Specifically, this approach utilizes a quantum oracle, based on finite element analysis (FEA)-based models or surrogate models, to acquire a more accurate estimation of the environment response with fewer queries for a certain input. QBO employs an Upper Confidence Bound (UCB) as the acquisition function to strategically select input values that are most likely to maximize the objective function. It has been theoretically proven to require much fewer samples while maintaining comparable optimization results. In the case study, force-controlled actuators are applied to one fuselage section to adjust its shape and reduce the gap to the adjoining section. Experimental results demonstrate that QBO achieves significantly lower dimensional error and uncertainty compared to classical methods, particularly using the same queries from the simulation.

</details>


### [81] [Distributed Dynamic Associative Memory via Online Convex Optimization](https://arxiv.org/abs/2511.23347)
*Bowen Wang,Matteo Zecchin,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出分布式动态关联记忆(DDAM)框架，扩展经典关联记忆到多智能体时变数据流场景，设计基于树的分布式在线梯度下降算法(DDAM-TOGD)，提供理论性能保证和优化路由树设计。


<details>
  <summary>Details</summary>
Motivation: 关联记忆是现代神经网络架构(如Transformer)的关键机制，但现有方法主要针对单智能体静态环境。需要扩展到多智能体、时变数据流的分布式动态场景，以支持现实世界中的协作学习任务。

Method: 提出DDAM框架，每个智能体维护本地关联记忆，通过兴趣矩阵选择性地记忆其他智能体信息。设计DDAM-TOGD算法，基于指定路由树进行智能体间通信，实现分布式在线梯度下降更新。进一步提出组合树设计策略优化路由树以减少通信延迟。

Result: 理论分析证明DDAM-TOGD在静态环境中具有次线性静态遗憾，在非静态环境中具有路径长度相关的动态遗憾界。数值实验显示DDAM-TOGD在准确性和鲁棒性上优于共识分布式优化等基线方法。

Conclusion: DDAM框架成功将关联记忆扩展到分布式动态环境，DDAM-TOGD算法提供了理论保证和实际性能优势，路由树优化策略进一步提升了系统性能，为分布式在线学习提供了新思路。

Abstract: An associative memory (AM) enables cue-response recall, and it has recently been recognized as a key mechanism underlying modern neural architectures such as Transformers. In this work, we introduce the concept of distributed dynamic associative memory (DDAM), which extends classical AM to settings with multiple agents and time-varying data streams. In DDAM, each agent maintains a local AM that must not only store its own associations but also selectively memorize information from other agents based on a specified interest matrix. To address this problem, we propose a novel tree-based distributed online gradient descent algorithm, termed DDAM-TOGD, which enables each agent to update its memory on the fly via inter-agent communication over designated routing trees. We derive rigorous performance guarantees for DDAM-TOGD, proving sublinear static regret in stationary environments and a path-length dependent dynamic regret bound in non-stationary environments. These theoretical results provide insights into how communication delays and network structure impact performance. Building on the regret analysis, we further introduce a combinatorial tree design strategy that optimizes the routing trees to minimize communication delays, thereby improving regret bounds. Numerical experiments demonstrate that the proposed DDAM-TOGD framework achieves superior accuracy and robustness compared to representative online learning baselines such as consensus-based distributed optimization, confirming the benefits of the proposed approach in dynamic, distributed environments.

</details>


### [82] [Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs](https://arxiv.org/abs/2511.22099)
*Daniel Agyei Asante,Md Mokarram Chowdhury,Yang Li*

Main category: cs.LG

TL;DR: 本文首次全面研究了低秩分解压缩对LLM可信度的影响，涵盖隐私、对抗鲁棒性、公平性和伦理对齐，发现压缩在隐私保护和对抗鲁棒性方面有积极效果，但会降低公平性和零样本伦理推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在各领域取得重大进展，但其巨大规模阻碍了在资源受限环境中的部署。模型压缩（特别是低秩分解）能有效减小模型尺寸、内存占用和计算需求，但压缩后模型的可信度影响尚未得到充分研究。

Method: 使用多种低秩算法压缩不同尺寸和变体的LLM，全面评估压缩对隐私、对抗鲁棒性、公平性和伦理对齐的影响。同时研究模型规模和微调对可信度的影响，并通过基于梯度的归因分析识别对对抗鲁棒性贡献最大的层。

Result: 主要发现：(1) 低秩压缩保持或改善训练数据隐私，但削弱对话中的PII保护；(2) 对抗鲁棒性通常保持甚至增强，即使在深度压缩下；(3) 零样本伦理推理能力下降，但少量样本提示能部分恢复；(4) 公平性在压缩下下降。模型规模和微调对可信度有重要影响。

Conclusion: 低秩压缩在保持性能的同时对LLM可信度产生复杂影响：隐私保护和对抗鲁棒性得到改善，但公平性和伦理推理能力下降。研究为设计可信的压缩策略提供了指导，通过归因分析识别了对鲁棒性关键的层。

Abstract: Large language models (LLMs) have driven major advances across domains, yet their massive size hinders deployment in resource-constrained settings. Model compression addresses this challenge, with low-rank factorization emerging as a particularly effective method for reducing size, memory, and computation while maintaining accuracy. However, while these compressed models boast of benign performance and system-level advantages, their trustworthiness implications remain poorly understood. In this paper, we present the first comprehensive study of how low-rank factorization affects LLM trustworthiness across privacy, adversarial robustness, fairness, and ethical alignment. We evaluate multiple LLMs of different sizes and variants compressed with diverse low-rank algorithms, revealing key insights: (1) low-rank compression preserves or improves training data privacy but weakens PII protection during conversation; (2) adversarial robustness is generally preserved and often enhanced, even under deep compression; (3) ethical reasoning degrades in zero-shot settings but partially recovers with few-shot prompting; (4) fairness declines under compression. Beyond compression, we investigate how model scale and fine-tuning affect trustworthiness, as both are important in low-rank methods. To guide trustworthy compression strategies, we end our paper with a gradient-based attribution analysis to identify which layers in LLMs contribute most to adversarial robustness.

</details>


### [83] [Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba](https://arxiv.org/abs/2511.22101)
*Zhaofeng Zhang*

Main category: cs.LG

TL;DR: 该研究复制并改进了"Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning"论文，提出了结合Mamba与DDQN的新架构，在某些测试中表现优于原模型。


<details>
  <summary>Details</summary>
Motivation: 复制并改进Uniswap V3中基于深度强化学习的自适应流动性提供方法，以提升模型的理论支持和实际性能。

Method: 1. 复制原论文：从Uniswap Subgraph获取数据，实现原模型；2. 提出新架构：结合Mamba与DDQN，设计新奖励函数，重新清洗数据，引入两个新基线进行比较。

Result: 新模型虽然尚未应用于所有数据集，但比原模型具有更强的理论支持，在某些测试中表现更好。

Conclusion: 提出的Mamba+DDQN架构在Uniswap V3流动性提供问题上显示出改进潜力，为后续研究提供了新的方向。

Abstract: The report goes through the main steps of replicating and improving the article "Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning." The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests.

</details>


### [84] [Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning](https://arxiv.org/abs/2511.22105)
*Saad Masrur,Ismail Guvenc,David Lopez Perez*

Main category: cs.LG

TL;DR: 提出基于多智能体深度强化学习（MARL-DDQN）的动态睡眠模式优化框架，用于毫米波网络中最大化能效并满足服务质量约束。


<details>
  <summary>Details</summary>
Motivation: 现有毫米波网络中的睡眠模式优化方法依赖静态基站流量模型，无法捕捉非平稳流量动态，且状态-动作空间过大，限制了实际部署。需要解决动态环境下的能效优化问题。

Method: 采用多智能体深度强化学习框架（MARL-DDQN），在3D城市环境中结合时变和基于社区的用户设备移动模型。集成实际基站功耗模型和波束成形技术，通过分布式决策最小化信令开销。

Result: MARL-DDQN在动态场景下优于现有方法（All On、IT-QoS-LB、MARL-DDPG、MARL-PPO），达到0.60 Mbit/Joule能效，8.5 Mbps第10百分位吞吐量，95%时间满足QoS约束。

Conclusion: MARL-DDQN框架能有效解决毫米波网络中动态睡眠模式优化问题，在保证服务质量的同时显著提升能效，为实际部署提供了可行方案。

Abstract: Dynamic sleep mode optimization (SMO) in millimeter-wave (mmWave) networks is essential for maximizing energy efficiency (EE) under stringent quality-of-service (QoS) constraints. However, existing optimization and reinforcement learning (RL) approaches rely on aggregated, static base station (BS) traffic models that fail to capture non-stationary traffic dynamics and suffer from large state-action spaces, limiting real-world deployment. To address these challenges, this paper proposes a multi-agent deep reinforcement learning (MARL) framework using a Double Deep Q-Network (DDQN), referred to as MARL-DDQN, for adaptive SMO in a 3D urban environment with a time-varying and community-based user equipment (UE) mobility model. Unlike conventional single-agent RL, MARL-DDQN enables scalable, distributed decision-making with minimal signaling overhead. A realistic BS power consumption model and beamforming are integrated to accurately quantify EE, while QoS is defined in terms of throughput. The method adapts SMO policies to maximize EE while mitigating inter-cell interference and ensuring throughput fairness. Simulations show that MARL-DDQN outperforms state-of-the-art strategies, including All On, iterative QoS-aware load-based (IT-QoS-LB), MARL-DDPG, and MARL-PPO, achieving up to 0.60 Mbit/Joule EE, 8.5 Mbps 10th-percentile throughput, and meeting QoS constraints 95% of the time under dynamic scenarios.

</details>


### [85] [An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface](https://arxiv.org/abs/2511.22108)
*Zhou Biyan,Arindam Basu*

Main category: cs.LG

TL;DR: 提出基于深度脉冲神经网络的连续学习方法，使用Banditron和AGREL强化学习算法，在植入式脑机接口中实现高效、稳定的神经解码，大幅降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 植入式脑机接口中同时记录的神经元数量呈指数增长，需要高效的神经解码器进行数据压缩。然而系统的非平稳性使解码器性能不可靠，频繁重新训练会影响用户安全与舒适，因此需要连续学习方法来适应实时应用需求。

Method: 采用深度脉冲神经网络作为资源高效的神经解码器，并为其适配强化学习算法（Banditron和AGREL）。这些算法计算资源需求低，能有效处理非平稳问题，适合植入式设备的能量限制。通过开环和闭环实验评估方法有效性。

Result: 开环实验中，DSNN Banditron和DSNN AGREL在长时间内保持稳定的准确率。闭环扰动实验中，DSNN Banditron性能与DSNN AGREL相当，同时训练期间内存访问使用减少98%，乘加运算需求减少99%。相比之前的连续学习SNN解码器，计算需求减少98%。

Conclusion: DSNN Banditron在保持性能的同时大幅降低了计算资源需求，是未来无线植入式脑机接口系统的理想候选方案，能够满足植入设备的能量限制并有效处理神经信号的非平稳性问题。

Abstract: The number of simultaneously recorded neurons follows an exponentially increasing trend in implantable brain-machine interfaces (iBMIs). Integrating the neural decoder in the implant is an effective data compression method for future wireless iBMIs. However, the non-stationarity of the system makes the performance of the decoder unreliable. To avoid frequent retraining of the decoder and to ensure the safety and comfort of the iBMI user, continuous learning is essential for real-life applications. Since Deep Spiking Neural Networks (DSNNs) are being recognized as a promising approach for developing resource-efficient neural decoder, we propose continuous learning approaches with Reinforcement Learning (RL) algorithms adapted for DSNNs. Banditron and AGREL are chosen as the two candidate RL algorithms since they can be trained with limited computational resources, effectively addressing the non-stationary problem and fitting the energy constraints of implantable devices. To assess the effectiveness of the proposed methods, we conducted both open-loop and closed-loop experiments. The accuracy of open-loop experiments conducted with DSNN Banditron and DSNN AGREL remains stable over extended periods. Meanwhile, the time-to-target in the closed-loop experiment with perturbations, DSNN Banditron performed comparably to that of DSNN AGREL while achieving reductions of 98% in memory access usage and 99% in the requirements for multiply- and-accumulate (MAC) operations during training. Compared to previous continuous learning SNN decoders, DSNN Banditron requires 98% less computes making it a prime candidate for future wireless iBMI systems.

</details>


### [86] [Toward Data-Driven Surrogates of the Solar Wind with Spherical Fourier Neural Operator](https://arxiv.org/abs/2511.22112)
*Reza Mansouri,Dustin Kempton,Pete Riley,Rafal Angryk*

Main category: cs.LG

TL;DR: 开发基于球面傅里叶神经算子（SFNO）的太阳风稳态建模替代模型，相比传统数值替代模型HUX在多个指标上表现相当或更好，支持高效实时预测。


<details>
  <summary>Details</summary>
Motivation: 太阳风变化对地球空间系统有重大影响，但传统3D磁流体动力学模型计算成本高，限制了边界条件不确定性研究，需要开发高效替代模型用于空间天气预报。

Method: 使用球面傅里叶神经算子（SFNO）构建太阳风稳态建模的替代模型，并与现有数值替代模型HUX进行对比评估。

Result: SFNO模型在多个性能指标上达到与HUX相当或更好的表现，虽然HUX在物理平滑性方面仍有优势，但SFNO展示了作为灵活可训练方法的潜力。

Conclusion: SFNO为太阳风建模提供了高效灵活的替代方案，支持实时预测并能通过更多数据持续改进，但需要建立更好的评估标准来全面评估此类模型。

Abstract: The solar wind, a continuous stream of charged particles from the Sun's corona, shapes the heliosphere and impacts space systems near Earth. Variations such as high-speed streams and coronal mass ejections can disrupt satellites, power grids, and communications, making accurate modeling essential for space weather forecasting. While 3D magnetohydrodynamic (MHD) models are used to simulate and investigate these variations in the solar wind, they tend to be computationally expensive, limiting their usefulness in investigating the impacts of boundary condition uncertainty. In this work, we develop a surrogate for steady state solar wind modeling, using a Spherical Fourier Neural Operator (SFNO). We compare our model to a previously developed numerical surrogate for this task called HUX, and we show that the SFNO achieves comparable or better performance across several metrics. Though HUX retains advantages in physical smoothness, this underscores the need for improved evaluation criteria rather than a flaw in SFNO. As a flexible and trainable approach, SFNO enables efficient real-time forecasting and can improve with more data. The source code and more visual results are available at https://github.com/rezmansouri/solarwind-sfno-velocity.

</details>


### [87] [IVGAE: Handling Incomplete Heterogeneous Data with a Variational Graph Autoencoder](https://arxiv.org/abs/2511.22116)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal%*

Main category: cs.LG

TL;DR: IVGAE：基于变分图自编码器的异构数据插补框架，通过构建样本-特征二分图、双解码器架构和Transformer异构嵌入模块，有效处理混合类型缺失数据。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据常包含数值和分类特征的异构数据，且存在缺失值。现有插补方法难以有效捕捉复杂结构依赖并处理异构数据，需要更鲁棒的解决方案。

Method: 1. 构建样本-特征二分图表示关系；2. 采用图表示学习建模结构依赖；3. 双解码器架构：一个重构特征嵌入，另一个建模缺失模式；4. Transformer异构嵌入模块避免高维独热编码。

Result: 在16个真实数据集上，IVGAE在MCAR、MAR、MNAR三种缺失机制下，30%缺失率时，RMSE和下游F1分数均取得一致提升。

Conclusion: IVGAE通过图表示学习和双解码器架构，有效处理异构数据缺失问题，为现实世界表格数据插补提供了鲁棒解决方案。

Abstract: Handling missing data remains a fundamental challenge in real-world tabular datasets, especially when data are heterogeneous with both numerical and categorical features. Existing imputation methods often fail to capture complex structural dependencies and handle heterogeneous data effectively. We present \textbf{IVGAE}, a Variational Graph Autoencoder framework for robust imputation of incomplete heterogeneous data. IVGAE constructs a bipartite graph to represent sample-feature relationships and applies graph representation learning to model structural dependencies. A key innovation is its \textit{dual-decoder architecture}, where one decoder reconstructs feature embeddings and the other models missingness patterns, providing structural priors aware of missing mechanisms. To better encode categorical variables, we introduce a Transformer-based heterogeneous embedding module that avoids high-dimensional one-hot encoding. Extensive experiments on 16 real-world datasets show that IVGAE achieves consistent improvements in RMSE and downstream F1 across MCAR, MAR, and MNAR missing scenarios under 30\% missing rates. Code and data are available at: https://github.com/echoid/IVGAE.

</details>


### [88] [A Variational Manifold Embedding Framework for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2511.22128)
*John J. Vastola,Samuel J. Gershman,Kanaka Rajan*

Main category: cs.LG

TL;DR: 提出一个变分框架，将降维算法转化为最优流形嵌入问题，既能实现非线性嵌入，又保持可解释性，PCA是其特例。


<details>
  <summary>Details</summary>
Motivation: 现有降维方法各有局限：PCA变体简单可解释但无法捕捉非线性流形结构；自编码器灵活但难以解释；基于图嵌入的方法可能导致流形几何病态扭曲。需要一种既灵活又可解释的降维方法。

Method: 提出变分框架，将降维算法转化为最优流形嵌入问题。该框架允许非线性嵌入，且其变分性质带来可解释性：每个解满足一组偏微分方程，并反映嵌入目标的对称性。

Result: 该框架能产生比PCA更灵活的解决方案，同时保持可解释性。在某些情况下可以解析表征解，有趣的是，一个特例恰好恢复PCA。

Conclusion: 提出的变分框架解决了现有降维方法的局限性，既能实现非线性嵌入，又保持可解释性，为降维问题提供了统一的理论基础，PCA是其特殊情形。

Abstract: Dimensionality reduction algorithms like principal component analysis (PCA) are workhorses of machine learning and neuroscience, but each has well-known limitations. Variants of PCA are simple and interpretable, but not flexible enough to capture nonlinear data manifold structure. More flexible approaches have other problems: autoencoders are generally difficult to interpret, and graph-embedding-based methods can produce pathological distortions in manifold geometry. Motivated by these shortcomings, we propose a variational framework that casts dimensionality reduction algorithms as solutions to an optimal manifold embedding problem. By construction, this framework permits nonlinear embeddings, allowing its solutions to be more flexible than PCA. Moreover, the variational nature of the framework has useful consequences for interpretability: each solution satisfies a set of partial differential equations, and can be shown to reflect symmetries of the embedding objective. We discuss these features in detail and show that solutions can be analytically characterized in some cases. Interestingly, one special case exactly recovers PCA.

</details>


### [89] [Benchmarking In-context Experiential Learning Through Repeated Product Recommendations](https://arxiv.org/abs/2511.22130)
*Gilbert Yang,Yaqin Chen,Thomson Yen,Hongseok Namkoong*

Main category: cs.LG

TL;DR: BELA基准测试评估LLM在动态环境中通过经验学习的能力，发现当前前沿模型难以在对话中有效改进推荐策略。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境不断变化，智能体需要处理不完整知识并通过经验适应行为。现有评估主要关注明确任务，缺乏对智能体通过积累经验进行适应性学习和推理能力的衡量。

Method: 创建BELA基准测试，结合：(1)亚马逊真实产品数据，(2)多样化用户角色表示异质潜在偏好，(3)基于角色的LLM用户模拟器生成丰富交互轨迹。评估LLM在推荐对话中通过经验学习的能力。

Result: 当前前沿模型在跨多轮对话中难以实现有意义的改进，表明需要具备强大上下文学习能力的智能体系统。

Conclusion: BELA基准测试揭示了当前LLM在动态环境中通过经验学习的局限性，强调了开发具备更强上下文学习能力的智能体系统的重要性。

Abstract: To reliably navigate ever-shifting real-world environments, agents must grapple with incomplete knowledge and adapt their behavior through experience. However, current evaluations largely focus on tasks that leave no ambiguity, and do not measure agents' ability to adaptively learn and reason through the experiences they accrued. We exemplify the need for this in-context experiential learning in a product recommendation context, where agents must navigate shifting customer preferences and product landscapes through natural language dialogue. We curate a benchmark for experiential learning and active exploration (BELA) that combines (1) rich real-world products from Amazon, (2) a diverse collection of user personas to represent heterogeneous yet latent preferences, and (3) a LLM user simulator powered by the persona to create rich interactive trajectories. We observe that current frontier models struggle to meaningfully improve across episodes, underscoring the need for agentic systems with strong in-context learning capabilities.

</details>


### [90] [TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices](https://arxiv.org/abs/2511.22138)
*Mohd Ariful Haque,Fahad Rahman,Kishor Datta Gupta,Khalil Shujaee,Roy George*

Main category: cs.LG

TL;DR: 研究小型语言模型在边缘设备上执行代理任务（函数/工具/API调用）的有效性，通过混合优化策略使1-3B参数模型在BFCL基准上达到65.74%准确率。


<details>
  <summary>Details</summary>
Motivation: 探索在边缘设备上运行不依赖云基础设施的代理AI的可行性，实现隐私保护、低延迟的自主代理，使小型语言模型能够在资源受限环境中执行复杂的代理任务。

Method: 使用Berkeley Function Calling Leaderboard框架评估SLMs，采用参数驱动的优化策略：监督微调、参数高效微调、强化学习优化、DPO偏好对齐和混合方法。构建基于AgentBank数据的DPO训练流程，将SFT数据转换为选择-拒绝对。

Result: 中型模型（1-3B参数）显著优于超紧凑模型（<1B参数），混合优化达到65.74%总体准确率和55.62%多轮对话准确率。不同模型在BFCL各类别（简单、多重、并行等）表现差异明显。

Conclusion: 混合优化策略使小型语言模型能够在边缘设备上提供准确、高效、稳定的代理AI，实现隐私保护、低延迟的自主代理，为云外部署提供实用解决方案。

Abstract: This paper investigates the effectiveness of small language models (SLMs) for agentic tasks (function/tool/API calling) with a focus on running agents on edge devices without reliance on cloud infrastructure. We evaluate SLMs using the Berkeley Function Calling Leaderboard (BFCL) framework and describe parameter-driven optimization strategies that include supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL)-based optimization, preference alignment via Direct Preference Optimization (DPO), and hybrid methods. We report results for models including TinyAgent, TinyLlama, Qwen, and xLAM across BFCL categories (simple, multiple, parallel, parallel-multiple, and relevance detection), both in live and non-live settings, and in multi-turn evaluations. We additionally detail a DPO training pipeline constructed from AgentBank data (e.g., ALFRED), including our conversion of SFT data to chosen-rejected pairs using TinyLlama responses as rejected outputs and manual validation. Our results demonstrate clear accuracy differences across model scales where medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters), achieving up to 65.74% overall accuracy, and 55.62% multi-turn accuracy with hybrid optimization. This study highlights the importance of hybrid optimization strategies that enable small language models to deliver accurate, efficient, and stable agentic AI on edge devices, making privacy-preserving, low-latency autonomous agents practical beyond the cloud.

</details>


### [91] [From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures](https://arxiv.org/abs/2511.22150)
*Florian Rottach,William Rudman,Bastain Rieck,Harrisen Scells,Carsten Eickhoff*

Main category: cs.LG

TL;DR: 论文提出统一拓扑签名(UTS)框架，通过综合拓扑和几何指标分析文本嵌入空间，发现单一指标不足，需要多属性视角来理解嵌入几何结构及其与下游任务性能的关系。


<details>
  <summary>Details</summary>
Motivation: 研究嵌入在空间中的组织方式不仅能增强模型可解释性，还能揭示驱动下游任务性能的因素。当前缺乏对文本嵌入模型拓扑和几何特性的系统性分析。

Method: 1. 对大量文本嵌入模型和数据集进行拓扑和几何度量的综合分析；2. 发现这些度量间存在高度冗余，单个指标难以充分区分嵌入空间；3. 提出统一拓扑签名(UTS)框架，采用多属性视角来表征嵌入空间。

Result: 1. UTS能够预测模型特定属性并揭示由模型架构驱动的相似性；2. 拓扑结构与排名效果相关，能准确预测文档可检索性；3. 证明多属性视角对于理解和利用文本嵌入几何结构至关重要。

Conclusion: 需要采用整体、多属性的视角来理解和利用文本嵌入的几何结构，UTS框架为此提供了有效工具，能够连接拓扑特征与下游任务性能。

Abstract: Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.

</details>


### [92] [Designing Instance-Level Sampling Schedules via REINFORCE with James-Stein Shrinkage](https://arxiv.org/abs/2511.22177)
*Peiyu Yu,Suraj Kothawade,Sirui Xie,Ying Nian Wu,Hongliang Fei*

Main category: cs.LG

TL;DR: 提出一种新的文本到图像生成后处理方法：通过学习实例级别的采样时间表来优化冻结采样器的生成质量，而不是修改模型权重。


<details>
  <summary>Details</summary>
Motivation: 现有后处理方法主要关注模型权重的微调或蒸馏，但作者认为可以通过优化采样时间表来提升生成质量，这是一种不同的技术路线。

Method: 1) 提出实例级（提示和噪声条件）采样时间表调度方法；2) 使用单次Dirichlet策略学习时间表；3) 引入基于James-Stein估计器的新奖励基线，降低高维策略学习中的梯度估计误差。

Result: 1) 在Stable Diffusion和Flux模型家族中，文本-图像对齐（包括文本渲染和组合控制）得到一致提升；2) 5步Flux-Dev采样器配合该时间表能达到与专门蒸馏的Flux-Schnell采样器相当的生成质量。

Conclusion: 该调度框架作为一种模型无关的后处理杠杆，能够释放预训练采样器的额外生成潜力，为文本到图像生成提供新的优化维度。

Abstract: Most post-training methods for text-to-image samplers focus on model weights: either fine-tuning the backbone for alignment or distilling it for few-step efficiency. We take a different route: rescheduling the sampling timeline of a frozen sampler. Instead of a fixed, global schedule, we learn instance-level (prompt- and noise-conditioned) schedules through a single-pass Dirichlet policy. To ensure accurate gradient estimates in high-dimensional policy learning, we introduce a novel reward baseline based on a principled James-Stein estimator; it provably achieves lower estimation errors than commonly used variants and leads to superior performance. Our rescheduled samplers consistently improve text-image alignment including text rendering and compositional control across modern Stable Diffusion and Flux model families. Additionally, a 5-step Flux-Dev sampler with our schedules can attain generation quality comparable to deliberately distilled samplers like Flux-Schnell. We thus position our scheduling framework as an emerging model-agnostic post-training lever that unlocks additional generative potential in pretrained samplers.

</details>


### [93] [PULSE-ICU: A Pretrained Unified Long-Sequence Encoder for Multi-task Prediction in Intensive Care Units](https://arxiv.org/abs/2511.22199)
*Sejeong Jang,Joo Heung Yoon,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: PULSE-ICU是一个自监督基础模型，从大规模电子健康记录序列中学习ICU事件级表示，无需重采样或手动特征工程，在18个预测任务上表现出色，并在外部验证中显示良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: ICU数据具有高度不规则、异质性和时间碎片化的特点，这给通用的临床预测带来了挑战。现有方法通常需要重采样或手动特征工程，限制了模型的泛化能力和适应性。

Method: 提出PULSE-ICU自监督基础模型：1）统一嵌入模块编码事件身份、连续值、单位和时间属性；2）基于Longformer的编码器高效建模长轨迹；3）在18个预测任务上进行微调，包括死亡率、干预预测和表型识别。

Result: 在eICU、HiRID和P12数据集上的外部验证显示，PULSE-ICU在多个任务类型上表现出色，仅需少量微调即可显著提升性能，对领域偏移和变量约束具有鲁棒性。

Conclusion: 基础模型风格的方法可以提高数据效率和适应性，为不同临床环境中的ICU决策支持提供可扩展框架，展示了自监督学习在医疗时间序列分析中的潜力。

Abstract: Intensive care unit (ICU) data are highly irregular, heterogeneous, and temporally fragmented, posing challenges for generalizable clinical prediction. We present PULSE-ICU, a self-supervised foundation model that learns event-level ICU representations from large-scale EHR sequences without resampling or manual feature engineering. A unified embedding module encodes event identity, continuous values, units, and temporal attributes, while a Longformer-based encoder enables efficient modeling of long trajectories. PULSE-ICU was fine-tuned across 18 prediction tasks, including mortality, intervention forecasting, and phenotype identification, achieving strong performance across task types. External validation on eICU, HiRID, and P12 showed substantial improvements with minimal fine-tuning, demonstrating robustness to domain shift and variable constraints. These findings suggest that foundation-style modeling can improve data efficiency and adaptability, providing a scalable framework for ICU decision support across diverse clinical environments.

</details>


### [94] [BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning](https://arxiv.org/abs/2511.22210)
*Junsung Park*

Main category: cs.LG

TL;DR: BiCQL-ML是一种免策略的离线逆强化学习算法，通过双层框架联合优化奖励函数和保守Q函数，避免显式策略学习，在标准离线RL基准测试中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 离线逆强化学习（IRL）旨在仅使用固定的演示数据恢复解释专家行为的奖励函数，无需额外的在线交互。现有方法存在局限性，需要更有效的离线IRL算法来改进奖励恢复和下游策略性能。

Method: 提出BiCQL-ML算法：1）在双层框架中联合优化奖励函数和保守Q函数；2）交替进行：在当前奖励下通过保守Q学习（CQL）学习Q函数，更新奖励参数以最大化专家动作的期望Q值同时抑制对分布外动作的过度泛化；3）该方法可视为基于软值匹配原则的最大似然估计。

Result: 理论保证：BiCQL-ML收敛到使专家策略达到软最优的奖励函数。实证结果：在标准离线RL基准测试中，BiCQL-ML在奖励恢复和下游策略性能方面均优于现有离线IRL基线方法。

Conclusion: BiCQL-ML是一种有效的免策略离线IRL算法，通过联合优化奖励和Q函数的双层框架，避免了显式策略学习，在理论和实证上都表现出优越性能，为离线逆强化学习提供了新的解决方案。

Abstract: Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.

</details>


### [95] [FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2511.22265)
*Yuan Yao,Lixu Wang,Jiaqi Wu,Jin Song,Simin Chen,Zehua Wang,Zijian Tian,Wei Chen,Huixia Li,Xiaoxiao Li*

Main category: cs.LG

TL;DR: FedRE提出了一种基于纠缠表示的联邦学习框架，通过随机权重聚合本地表示和标签编码，在保护隐私的同时实现模型异构的联邦学习。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法大多假设同构模型架构，但实际中客户端在数据和资源上存在异构性，这使得同构假设不切实际，因此需要模型异构的联邦学习方法。

Method: 提出联邦表示纠缠(FedRE)框架：客户端使用归一化随机权重将本地表示聚合成单个纠缠表示，并用相同权重将对应的one-hot标签编码整合为纠缠标签编码；上传到服务器训练全局分类器；每轮重新采样随机权重以引入多样性。

Result: 大量实验表明，FedRE在模型性能、隐私保护和通信开销之间实现了有效权衡，同时减轻了全局分类器的过度自信问题，促进了更平滑的决策边界。

Conclusion: FedRE通过纠缠表示机制解决了模型异构联邦学习中的关键挑战，在保持隐私保护的同时实现了良好的性能，并减少了通信开销和表示反转攻击的风险。

Abstract: Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), a framework built upon a novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into a single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train a global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads a single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https://github.com/AIResearch-Group/FedRE.

</details>


### [96] [TreeCoder: Systematic Exploration and Optimisation of Decoding and Constraints for LLM Code Generation](https://arxiv.org/abs/2511.22277)
*Henrijs Princis,Arindam Sharma,Cristina David*

Main category: cs.LG

TL;DR: TreeCoder是一个用于代码生成的通用解码框架，通过树搜索和约束函数确保代码正确性，而非依赖提示工程


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成时经常违反语法或语义约束，仅靠自然语言提示无法保证代码正确性

Method: 将解码表示为候选程序的树搜索，将解码策略和约束函数（如风格、语法、执行）作为可优化的组件，支持系统探索和自动调优

Result: 在MBPP（Python）和SQL-Spider基准测试中，TreeCoder显著提高了CodeLlama、Mistral和DeepSeek等开源模型的准确性，大幅超越无约束基线

Conclusion: TreeCoder提供了一个灵活框架，通过在解码过程中强制执行约束来改善LLM的代码生成质量，是比提示工程更有效的解决方案

Abstract: Large language models (LLMs) have shown remarkable ability to generate code, yet their outputs often violate syntactic or semantic constraints when guided only through natural language prompts. We introduce TreeCoder, the most general and flexible framework to date for exploring decoding strategies, constraints, and hyperparameters in LLMs, and use it in code generation to enforce correctness and structure during decoding rather than relying on prompt engineering. TreeCoder represents decoding as a tree search over candidate programs, where both decoding strategies and constraint functions - such as style, syntax, execution - are treated as first-class, optimisable components. This design enables systematic exploration and automatic tuning of decoding configurations using standard optimisation techniques. Experiments on the MBPP (Python) and SQL-Spider benchmarks show that TreeCoder consistently improves accuracy across open-source models such as CodeLlama, Mistral and DeepSeek, often outperforming their unconstrained baselines by considerable margins.

</details>


### [97] [The Hidden Cost of Approximation in Online Mirror Descent](https://arxiv.org/abs/2511.22283)
*Ofir Schlisselberg,Uri Sherman,Tomer Koren,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文系统研究了在线镜像下降(OMD)的近似误差影响，揭示了正则化器平滑性与误差鲁棒性之间的复杂关系，发现不同正则化器对误差的容忍度存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有OMD分析通常假设理想的无误差设置，但实际中优化子问题往往只能近似求解。这限制了对实际性能保证的理解，因此需要系统研究近似OMD的性能。

Method: 系统研究近似OMD算法，分析不同正则化器（负熵、对数障碍、Tsallis正则化器）在误差影响下的性能差异，考虑均匀平滑正则化器和单纯形上的障碍正则化器。

Result: 发现正则化器平滑性与误差鲁棒性密切相关：均匀平滑正则化器的误差影响有紧界；单纯形上负熵需要指数小误差避免线性遗憾，而对数障碍和Tsallis正则化器对多项式误差保持鲁棒；随机损失下负熵在单纯形上恢复鲁棒性，但子集上仍需指数小误差。

Conclusion: 近似OMD的性能高度依赖于正则化器的选择，不同正则化器对计算误差的容忍度存在显著差异，这为实际算法设计提供了重要指导。

Abstract: Online mirror descent (OMD) is a fundamental algorithmic paradigm that underlies many algorithms in optimization, machine learning and sequential decision-making. The OMD iterates are defined as solutions to optimization subproblems which, oftentimes, can be solved only approximately, leading to an inexact version of the algorithm. Nonetheless, existing OMD analyses typically assume an idealized error free setting, thereby limiting our understanding of performance guarantees that should be expected in practice. In this work we initiate a systematic study into inexact OMD, and uncover an intricate relation between regularizer smoothness and robustness to approximation errors. When the regularizer is uniformly smooth, we establish a tight bound on the excess regret due to errors. Then, for barrier regularizers over the simplex and its subsets, we identify a sharp separation: negative entropy requires exponentially small errors to avoid linear regret, whereas log-barrier and Tsallis regularizers remain robust even when the errors are only polynomial. Finally, we show that when the losses are stochastic and the domain is the simplex, negative entropy regains robustness-but this property does not extend to all subsets, where exponentially small errors are again necessary to avoid suboptimal regret.

</details>


### [98] [Online Dynamic Pricing of Complementary Products](https://arxiv.org/abs/2511.22291)
*Marco Mussi,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出一种考虑产品互补关系的动态定价在线学习算法，通过整数规划识别产品间正负需求交互，使用异方差高斯过程多臂老虎机优化定价，相比忽略交互的算法提升收入。


<details>
  <summary>Details</summary>
Motivation: 传统动态定价算法通常独立优化每个产品的价格，忽略了相关产品之间的需求交互作用（互补或替代关系），导致无法充分利用协调定价策略的潜力，从而损失潜在收入。

Method: 提出在线学习算法：1) 利用交易数据通过整数规划问题识别产品间的正负互补关系；2) 基于异方差高斯过程的多臂老虎机解决方案，以数据驱动和计算高效的方式优化定价策略。

Result: 在模拟环境中验证，相比忽略产品交互关系的可比学习算法，提出的解决方案能够提高整体收入。

Conclusion: 考虑产品间互补关系的动态定价算法能够更好地利用联合需求结构，通过协调定价策略实现更高的收入优化，为互补产品的动态定价提供了有效解决方案。

Abstract: Traditional pricing paradigms, once dominated by static models and rule-based heuristics, are increasingly being replaced by dynamic, data-driven approaches powered by machine learning algorithms. Despite their growing sophistication, most dynamic pricing algorithms focus on optimizing the price of each product independently, disregarding potential interactions among items. By neglecting these interdependencies in consumer demand across related goods, sellers may fail to capture the full potential of coordinated pricing strategies. In this paper, we address this problem by exploring dynamic pricing mechanisms designed explicitly for complementary products, aiming to exploit their joint demand structure to maximize overall revenue. We present an online learning algorithm considering both positive and negative interactions between products' demands. The algorithm utilizes transaction data to identify advantageous complementary relationships through an integer programming problem between different items, and then optimizes pricing strategies using data-driven and computationally efficient multi-armed bandit solutions based on heteroscedastic Gaussian processes. We validate our solution in a simulated environment, and we demonstrate that our solution improves the revenue w.r.t. a comparable learning algorithm ignoring such interactions.

</details>


### [99] [Adaptive tumor growth forecasting via neural & universal ODEs](https://arxiv.org/abs/2511.22292)
*Kavya Subramanian,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 使用神经ODE和通用微分方程构建自适应肿瘤生长模型，提高预测精度并指导个性化治疗


<details>
  <summary>Details</summary>
Motivation: 传统肿瘤生长模型（如Gompertz和Bertalanffy方程）虽然能捕捉一般肿瘤动态，但难以适应患者特异性变异，特别是在数据有限的情况下。需要更灵活的自适应模型来改善预测准确性，指导动态有效的治疗策略。

Method: 利用科学机器学习的两大支柱：神经ODE和通用微分方程，以Gompertz模型为基础，用自适应神经网络替换刚性项来捕捉隐藏动态。在Julia编程语言中实现，进行数据约束下的预测和符号恢复，将学习到的动态转化为显式数学表达式。

Result: 该方法能够构建自适应肿瘤生长模型，在数据有限的情况下进行预测，并能将学习到的动态转化为可解释的数学表达式，有望提高预测准确性。

Conclusion: 基于神经ODE和通用微分方程的自适应肿瘤生长模型有潜力改善预测精度，为动态有效的治疗策略提供指导，从而改善临床结果。

Abstract: Forecasting tumor growth is critical for optimizing treatment. Classical growth models such as the Gompertz and Bertalanffy equations capture general tumor dynamics but may fail to adapt to patient-specific variability, particularly with limited data available. In this study, we leverage Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs), two pillars of Scientific Machine Learning (SciML), to construct adaptive tumor growth models capable of learning from experimental data. Using the Gompertz model as a baseline, we replace rigid terms with adaptive neural networks to capture hidden dynamics through robust modeling in the Julia programming language. We use our models to perform forecasting under data constraints and symbolic recovery to transform the learned dynamics into explicit mathematical expressions. Our approach has the potential to improve predictive accuracy, guiding dynamic and effective treatment strategies for improved clinical outcomes.

</details>


### [100] [FLUX: Efficient Descriptor-Driven Clustered Federated Learning under Arbitrary Distribution Shifts](https://arxiv.org/abs/2511.22305)
*Dario Fenoglio,Mohan Li,Pietro Barbiero,Nicholas D. Lane,Marc Langheinrich,Martin Gjoreski*

Main category: cs.LG

TL;DR: FLUX是一个基于聚类的联邦学习框架，通过隐私保护的客户端描述符提取和无监督聚类，解决训练和测试时的四种常见分布偏移问题，无需先验知识且支持测试时适应。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法假设客户端数据独立同分布(IID)，但在现实场景中这一假设往往不成立，导致全局模型性能显著下降。现有方法需要先验知识且无法处理测试时的分布偏移。

Method: 提出FLUX框架：1) 使用隐私保护的客户端描述符提取技术；2) 采用无监督聚类方法对客户端进行分组；3) 为每个聚类训练专门的模型；4) 支持测试时适应，使未见过的未标记客户端能够选择最合适的聚类模型。

Result: 在四个标准基准、两个真实数据集和十个SOTA基线方法上的实验表明，FLUX在不同分布偏移下显著提升性能和稳定性，平均准确率比最佳基线方法提高多达23个百分点，同时保持与FedAvg相当的计算和通信开销。

Conclusion: FLUX是一个有效的聚类联邦学习框架，能够处理训练和测试时的多种分布偏移，无需先验知识且支持测试时适应，在实际非IID场景中具有更好的适用性和性能。

Abstract: Federated Learning (FL) enables collaborative model training across multiple clients while preserving data privacy. Traditional FL methods often use a global model to fit all clients, assuming that clients' data are independent and identically distributed (IID). However, when this assumption does not hold, the global model accuracy may drop significantly, limiting FL applicability in real-world scenarios. To address this gap, we propose FLUX, a novel clustering-based FL (CFL) framework that addresses the four most common types of distribution shifts during both training and test time. To this end, FLUX leverages privacy-preserving client-side descriptor extraction and unsupervised clustering to ensure robust performance and scalability across varying levels and types of distribution shifts. Unlike existing CFL methods addressing non-IID client distribution shifts, FLUX i) does not require any prior knowledge of the types of distribution shifts or the number of client clusters, and ii) supports test-time adaptation, enabling unseen and unlabeled clients to benefit from the most suitable cluster-specific models. Extensive experiments across four standard benchmarks, two real-world datasets and ten state-of-the-art baselines show that FLUX improves performance and stability under diverse distribution shifts, achieving an average accuracy gain of up to 23 percentage points over the best-performing baselines, while maintaining computational and communication overhead comparable to FedAvg.

</details>


### [101] [DeXposure: A Dataset and Benchmarks for Inter-protocol Credit Exposure in Decentralized Financial Networks](https://arxiv.org/abs/2511.22314)
*Wenbin Wu,Kejiang Qian,Alexis Lui,Christopher Jack,Yue Wu,Peter McBurney,Fengxiang He,Bryan Zhang*

Main category: cs.LG

TL;DR: DeXposure是首个大规模去中心化金融网络跨协议信用风险暴露数据集，包含4370万条记录，涵盖4300个协议、602条区块链和24300种代币，并提出了基于TVL变化的跨协议信用风险暴露新度量方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模的去中心化金融网络跨协议信用风险暴露数据集，难以进行系统性风险分析和机器学习研究，需要构建首个全面覆盖DeFi生态的信用风险暴露数据集。

Method: 使用DefiLlama元数据构建代币到协议模型，通过代币存量动态推断跨协议信用风险暴露；基于数据集开发三个机器学习基准：图聚类、向量自回归和时序图神经网络。

Result: 观察到网络规模快速增长、向关键协议集中、网络密度下降以及不同领域（借贷平台、交易交易所、资产管理协议）间冲击传播差异等模式；数据集和代码已公开。

Conclusion: DeXposure数据集为机器学习和金融风险监控、政策分析、DeFi市场建模等研究提供支持，同时为图聚类、向量自回归和时序图分析提供基准。

Abstract: We curate the DeXposure dataset, the first large-scale dataset for inter-protocol credit exposure in decentralized financial networks, covering global markets of 43.7 million entries across 4.3 thousand protocols, 602 blockchains, and 24.3 thousand tokens, from 2020 to 2025. A new measure, value-linked credit exposure between protocols, is defined as the inferred financial dependency relationships derived from changes in Total Value Locked (TVL). We develop a token-to-protocol model using DefiLlama metadata to infer inter-protocol credit exposure from the token's stock dynamics, as reported by the protocols. Based on the curated dataset, we develop three benchmarks for machine learning research with financial applications: (1) graph clustering for global network measurement, tracking the structural evolution of credit exposure networks, (2) vector autoregression for sector-level credit exposure dynamics during major shocks (Terra and FTX), and (3) temporal graph neural networks for dynamic link prediction on temporal graphs. From the analysis, we observe (1) a rapid growth of network volume, (2) a trend of concentration to key protocols, (3) a decline of network density (the ratio of actual connections to possible connections), and (4) distinct shock propagation across sectors, such as lending platforms, trading exchanges, and asset management protocols. The DeXposure dataset and code have been released publicly. We envision they will help with research and practice in machine learning as well as financial risk monitoring, policy analysis, DeFi market modeling, amongst others. The dataset also contributes to machine learning research by offering benchmarks for graph clustering, vector autoregression, and temporal graph analysis.

</details>


### [102] [SingleQuant: Efficient Quantization of Large Language Models in a Single Pass](https://arxiv.org/abs/2511.22316)
*Jinying Xiao,Bin Ji,Shasha Li,Xiaodong Liu,Ma Jun,Ye Zhong,Wei Li,Xuan Xie,Qingbo Wu,Jie Yu*

Main category: cs.LG

TL;DR: SingleQuant是一种单次量化框架，通过解耦量化截断，消除梯度噪声和非平滑问题，显著加速LLM量化并提升任务性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM量化方法结合不兼容的梯度优化和量化截断，导致严重的收敛病理问题，延长量化时间并降低任务性能。STE在Stiefel流形上引入非平滑性和梯度噪声，阻碍优化收敛

Method: 提出SingleQuant单次量化框架，构造对齐旋转变换(ART)和均匀性旋转变换(URT)针对不同激活异常值。ART通过闭式最优旋转平滑异常值，URT通过几何映射重塑分布。两种矩阵都采用严格公式化的Givens旋转

Result: 在7B-70B LLM上优于基线方法。量化LLaMA-2-13B时，实现1400倍量化加速，平均任务性能提升+0.57%

Conclusion: SingleQuant通过消除量化截断相关的非平滑性和梯度噪声，实现了快速高保真的LLM量化，在保持任务性能的同时大幅减少量化时间

Abstract: Large Language Models (LLMs) quantization facilitates deploying LLMs in resource-limited settings, but existing methods that combine incompatible gradient optimization and quantization truncation lead to serious convergence pathology. This prolongs quantization time and degrades LLMs' task performance. Our studies confirm that Straight-Through Estimator (STE) on Stiefel manifolds introduce non-smoothness and gradient noise, obstructing optimization convergence and blocking high-fidelity quantized LLM development despite extensive training. To tackle the above limitations, we propose SingleQuant, a single-pass quantization framework that decouples from quantization truncation, thereby eliminating the above non-smoothness and gradient noise factors. Specifically, SingleQuant constructs Alignment Rotation Transformation (ART) and Uniformity Rotation Transformation (URT) targeting distinct activation outliers, where ART achieves smoothing of outlier values via closed-form optimal rotations, and URT reshapes distributions through geometric mapping. Both matrices comprise strictly formulated Givens rotations with predetermined dimensions and rotation angles, enabling promising LLMs task performance within a short time. Experimental results demonstrate SingleQuant's superiority over the selected baselines across diverse tasks on 7B-70B LLMs. To be more precise, SingleQuant enables quantized LLMs to achieve higher task performance while necessitating less time for quantization. For example, when quantizing LLaMA-2-13B, SingleQuant achieves 1,400$\times$ quantization speedup and increases +0.57\% average task performance compared to the selected best baseline.

</details>


### [103] [Cleaning the Pool: Progressive Filtering of Unlabeled Pools in Deep Active Learning](https://arxiv.org/abs/2511.22344)
*Denis Huseljic,Marek Herde,Lukas Rauch,Paul Hahn,Bernhard Sick*

Main category: cs.LG

TL;DR: REFINE是一种集成主动学习方法，通过两阶段策略结合多种AL策略，无需预先知道哪种策略最佳，在多个数据集和基础模型上表现优于单一策略和现有集成方法。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习策略（如不确定性、代表性）在不同数据集、模型和AL周期中效果差异很大，单一策略可能在整个AL过程中表现不佳，需要一种能自适应结合多种策略的方法。

Method: REFINE采用两阶段方法：1）渐进过滤：通过集成多个AL策略迭代精炼未标记池，保留体现不同价值概念的候选样本；2）基于覆盖的选择：从精炼池中选择最终批次，确保涵盖所有已识别的价值概念。

Result: 在6个分类数据集和3个基础模型上的实验表明，REFINE始终优于单一策略和现有集成方法。渐进过滤作为预处理步骤能提升任何AL策略在精炼池上的性能。

Conclusion: REFINE提供了一种有效的集成主动学习方法，能自适应结合多种AL策略，且易于扩展集成新的先进AL策略，在音频谱图分类等实际应用中表现出色。

Abstract: Existing active learning (AL) strategies capture fundamentally different notions of data value, e.g., uncertainty or representativeness. Consequently, the effectiveness of strategies can vary substantially across datasets, models, and even AL cycles. Committing to a single strategy risks suboptimal performance, as no single strategy dominates throughout the entire AL process. We introduce REFINE, an ensemble AL method that combines multiple strategies without knowing in advance which will perform best. In each AL cycle, REFINE operates in two stages: (1) Progressive filtering iteratively refines the unlabeled pool by considering an ensemble of AL strategies, retaining promising candidates capturing different notions of value. (2) Coverage-based selection then chooses a final batch from this refined pool, ensuring all previously identified notions of value are accounted for. Extensive experiments across 6 classification datasets and 3 foundation models show that REFINE consistently outperforms individual strategies and existing ensemble methods. Notably, progressive filtering serves as a powerful preprocessing step that improves the performance of any individual AL strategy applied to the refined pool, which we demonstrate on an audio spectrogram classification use case. Finally, the ensemble of REFINE can be easily extended with upcoming state-of-the-art AL strategies.

</details>


### [104] [AutoTailor: Automatic and Efficient Adaptive Model Deployment for Diverse Edge Devices](https://arxiv.org/abs/2511.22355)
*Mengyang Liu,Chenyu Lu,Haodong Tian,Fang Dong,Ruiting Zhou,Wei Wang,Dian Shen,Guangtong Li,Ye Wan,Li Li*

Main category: cs.LG

TL;DR: AutoTailor：首个自动化端到端SuperNet自适应模型部署框架，显著减少代码量、降低硬件分析成本，提升精度并减少延迟


<details>
  <summary>Details</summary>
Motivation: 现有SuperNet方法需要繁琐的模型感知开发和耗时的硬件感知分析，限制了其在边缘设备上的实际应用。需要自动化框架来简化自适应模型部署流程。

Method: 采用计算图引导的编译方法自动将用户提供的ML模型转换为SuperNet；集成无需学习的延迟和精度预测器，实现低成本而准确的性能预测。

Result: 将SuperNet构建代码行数减少11-27倍；硬件感知分析成本降低至少11倍；相比最先进方法，精度提升最高达15.60%，延迟减少最高达60.03%。

Conclusion: AutoTailor是首个实现自动化端到端SuperNet自适应模型部署的框架，显著降低了开发和分析成本，同时提升了模型性能，推动了边缘设备上自适应ML的实际应用。

Abstract: On-device machine learning (ML) has become a fundamental component of emerging mobile applications. Adaptive model deployment delivers efficient inference for heterogeneous device capabilities and performance requirements through customizing neural architectures. SuperNet-based approaches offer a promising solution by generating a large number of model variants from a pre-trained ML model. However, applying SuperNet in existing frameworks suffers from tedious model-aware development and time-consuming hardware-aware profiling, which limits their practical adoption.
  We present AutoTailor, the first framework to enable automated, end-to-end SuperNet-based adaptive model deployment for edge devices. Unlike manual SuperNet construction, AutoTailor employs a computation graph-guided compilation approach to automatically transform user-provided ML models into SuperNets. To support efficient specialization, AutoTailor incorporates learning-free latency and accuracy predictors, enabling low-cost yet accurate performance prediction. Our extended evaluations demonstrate that AutoTailor reduces the lines of code for SuperNet construction by 11--27$\times$, decreases hardware-aware profiling costs by at least 11$\times$, and achieves up to 15.60\% absolute accuracy improvement and 60.03\% latency reduction compared to state-of-the-art approaches across diverse models and devices.

</details>


### [105] [Efficient-Husformer: Efficient Multimodal Transformer Hyperparameter Optimization for Stress and Cognitive Loads](https://arxiv.org/abs/2511.22362)
*Merey Orazaly,Fariza Temirkhanova,Jurn-Gyu Park*

Main category: cs.LG

TL;DR: 本文提出Efficient-Husformer，一种通过超参数优化设计的Transformer架构，用于多类别压力检测，在WESAD和CogLoad数据集上显著提升性能，同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在生理信号分析中表现出色，但存在计算强度高和内存需求大的问题。需要开发更高效的Transformer架构用于多模态生理信号的压力检测任务。

Method: 提出Efficient-Husformer架构，采用结构化搜索空间进行超参数优化，评估不同架构决策的影响，最终确定最佳配置：单层、3个注意力头、模型维度18/30、FFN维度120/30，仅约30k参数。

Result: 在WESAD数据集上达到88.41%准确率（提升13.83%），在CogLoad数据集上达到92.61%准确率（提升6.98%）。最佳配置使用(L+dm)或(L+FFN)模态组合。

Conclusion: Efficient-Husformer通过超参数优化实现了性能显著提升和模型紧凑化，为生理信号分析中的高效Transformer应用提供了有效解决方案。

Abstract: Transformer-based models have gained considerable attention in the field of physiological signal analysis. They leverage long-range dependencies and complex patterns in temporal signals, allowing them to achieve performance superior to traditional RNN and CNN models. However, they require high computational intensity and memory demands. In this work, we present Efficient-Husformer, a novel Transformer-based architecture developed with hyperparameter optimization (HPO) for multi-class stress detection across two multimodal physiological datasets (WESAD and CogLoad). The main contributions of this work are: (1) the design of a structured search space, targeting effective hyperparameter optimization; (2) a comprehensive ablation study evaluating the impact of architectural decisions; (3) consistent performance improvements over the original Husformer, with the best configuration achieving an accuracy of 88.41 and 92.61 (improvements of 13.83% and 6.98%) on WESAD and CogLoad datasets, respectively. The best-performing configuration is achieved with the (L + dm) or (L + FFN) modality combinations, using a single layer, 3 attention heads, a model dimension of 18/30, and FFN dimension of 120/30, resulting in a compact model with only about 30k parameters.

</details>


### [106] [SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning](https://arxiv.org/abs/2511.22367)
*Hugo Hazard,Zafeirios Fountas,Martin A. Benfeghoul,Adnan Oomerjee,Jun Wang,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: 本文提出SuRe（基于惊喜的优先级重放）和双学习器设计，通过惊喜优先选择和慢权重整合来解决LLM持续学习中的灾难性遗忘问题，在LNT设置中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前持续学习方法（正则化和重放）在视觉任务表现良好，但在大规模语言模型的多任务学习中落后于多任务学习，特别是在任务数量多的情况下。需要解决重放的两个失败模式：选择（重放什么）和整合（如何巩固新知识）。

Method: 1. SuRe（Surprise-prioritised Replay）：基于负对数似然（NLL）对序列进行惊喜度排名，优先存储最令人惊讶的序列；2. 双学习器设计：使用快速和慢速LoRA适配器，通过指数移动平均合并，实现快速适应同时稳定长期知识。

Result: SuRe在Large Number of Tasks（LNT）设置中达到SOTA性能，在标准CL和LNT基准测试中提供最佳平均性能。结合双学习器设计后，在LNT上比先前SOTA提升高达+5个准确率点。消融研究证实该方法在减少重放频率和小缓冲区大小下仍保持鲁棒性。

Conclusion: 重放是LLM持续微调的强大基线方法，惊喜优先选择和慢权重整合是缓解灾难性遗忘的互补组件。该方法展示了有效性和样本效率，为LLM持续学习提供了新思路。

Abstract: Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.

</details>


### [107] [Predicting and Interpolating Spatiotemporal Environmental Data: A Case Study of Groundwater Storage in Bangladesh](https://arxiv.org/abs/2511.22378)
*Anna Pazola,Mohammad Shamsudduha,Richard G. Taylor,Allan Tucker*

Main category: cs.LG

TL;DR: 比较两种深度学习策略（网格到网格 vs 网格到点）用于时空预测和插值，发现空间插值比时间预测更困难，地质不确定性对点时间行为有重要影响。


<details>
  <summary>Details</summary>
Motivation: 地理空间观测数据通常仅限于点测量，需要时间预测和空间插值来构建连续场。研究旨在评估两种深度学习策略的有效性。

Method: 使用两种策略：1) 网格到网格方法（建模前聚合），使用网格化预测因子建模栅格化目标；2) 网格到点方法（建模后聚合），使用网格化预测因子建模点目标，然后通过克里金插值填充域。以孟加拉国地下水存储数据为案例研究。

Result: 空间插值比时间预测困难得多；最近邻点不一定最相似；地质不确定性强烈影响点的时间行为。网格到网格和网格到点方法各有优劣。

Conclusion: 这些发现为基于时间序列动态聚类位置的先进插值方法提供了动机。结论适用于其他由间接可观测因素控制的环境变量。代码已开源。

Abstract: Geospatial observational datasets are often limited to point measurements, making temporal prediction and spatial interpolation essential for constructing continuous fields. This study evaluates two deep learning strategies for addressing this challenge: (1) a grid-to-grid approach, where gridded predictors are used to model rasterised targets (aggregation before modelling), and (2) a grid-to-point approach, where gridded predictors model point targets, followed by kriging interpolation to fill the domain (aggregation after modelling). Using groundwater storage data from Bangladesh as a case study, we compare the effcacy of these approaches. Our findings indicate that spatial interpolation is substantially more difficult than temporal prediction. In particular, nearest neighbours are not always the most similar, and uncertainties in geology strongly influence point temporal behaviour. These insights motivate future work on advanced interpolation methods informed by clustering locations based on time series dynamics. Demonstrated on groundwater storage, the conclusions are applicable to other environmental variables governed by indirectly observable factors. Code is available at https://github.com/pazolka/interpolation-prediction-gwsa.

</details>


### [108] [TS2Vec-Ensemble: An Enhanced Self-Supervised Framework for Time Series Forecasting](https://arxiv.org/abs/2511.22395)
*Ganeshan Niroshan,Uthayasanker Thayasivam*

Main category: cs.LG

TL;DR: TS2Vec-Ensemble：结合自监督表示学习和显式时间特征的混合框架，通过双模型集成架构提升时间序列预测性能


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如TS2Vec）在预测任务中表现不佳，因为其目标函数侧重于实例区分而非捕捉对预测至关重要的确定性模式（如季节性和趋势）。需要将隐式学习到的动态与显式时间特征相结合。

Method: 提出TS2Vec-Ensemble混合框架：1) 使用预训练的TS2Vec编码器获取隐式学习到的动态；2) 融合显式工程化的时间特征（编码周期性循环）；3) 采用双模型集成架构，两个回归头分别关注学习到的动态和季节模式；4) 使用自适应加权方案组合，为每个预测时域独立优化集成权重。

Result: 在ETT基准数据集上进行广泛实验（单变量和多变量预测）。TS2Vec-Ensemble始终显著优于标准TS2Vec基线和其他最先进模型，验证了混合策略的优越性。

Conclusion: 学习到的表示与显式时间先验的混合是长期时间序列预测的优越策略。TS2Vec-Ensemble框架通过动态权衡短期动态和长期季节性，有效提升了预测性能。

Abstract: Self-supervised representation learning, particularly through contrastive methods like TS2Vec, has advanced the analysis of time series data. However, these models often falter in forecasting tasks because their objective functions prioritize instance discrimination over capturing the deterministic patterns, such as seasonality and trend, that are critical for accurate prediction. This paper introduces TS2Vec-Ensemble, a novel hybrid framework designed to bridge this gap. Our approach enhances the powerful, implicitly learned dynamics from a pretrained TS2Vec encoder by fusing them with explicit, engineered time features that encode periodic cycles. This fusion is achieved through a dual-model ensemble architecture, where two distinct regression heads -- one focused on learned dynamics and the other on seasonal patterns -- are combined using an adaptive weighting scheme. The ensemble weights are optimized independently for each forecast horizon, allowing the model to dynamically prioritize short-term dynamics or long-term seasonality as needed. We conduct extensive experiments on the ETT benchmark datasets for both univariate and multivariate forecasting. The results demonstrate that TS2Vec-Ensemble consistently and significantly outperforms the standard TS2Vec baseline and other state-of-the-art models, validating our hypothesis that a hybrid of learned representations and explicit temporal priors is a superior strategy for long-horizon time series forecasting.

</details>


### [109] [PISA: Prioritized Invariant Subgraph Aggregation](https://arxiv.org/abs/2511.22435)
*Ali Ghasemi,Farooq Ahmad Wani,Maria Sofia Bucarelli,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: PISA提出动态MLP聚合框架，通过优先组合多个不变子图表示，在15个数据集上比现有方法提升高达5%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有图数据OOD泛化方法存在局限：CIGA只提取单一不变子图可能遗漏多个因果模式，SuGAr虽然学习多个不变子图但依赖简单聚合策略。需要更有效的多子图聚合方法。

Method: 提出PISA框架，采用动态MLP聚合机制，优先选择和组合多个不变子图的表示，相比之前的均匀或贪心聚合更有效。

Result: 在包括DrugOOD在内的15个数据集上的实验表明，PISA比现有方法（CIGA、SuGAr等）获得高达5%的分类准确率提升。

Conclusion: PISA通过动态MLP聚合有效整合多个不变子图表示，显著提升了图数据OOD泛化性能，解决了现有方法在子图聚合方面的不足。

Abstract: Recent work has extended the invariance principle for out-of-distribution (OOD) generalization from Euclidean to graph data, where challenges arise due to complex structures and diverse distribution shifts in node attributes and topology. To handle these, Chen et al. proposed CIGA (Chen et al., 2022b), which uses causal modeling and an information-theoretic objective to extract a single invariant subgraph capturing causal features. However, this single-subgraph focus can miss multiple causal patterns. Liu et al. (2025) addressed this with SuGAr, which learns and aggregates diverse invariant subgraphs via a sampler and diversity regularizer, improving robustness but still relying on simple uniform or greedy aggregation. To overcome this, the proposed PISA framework introduces a dynamic MLP-based aggregation that prioritizes and combines subgraph representations more effectively. Experiments on 15 datasets, including DrugOOD (Ji et al., 2023), show that PISA achieves up to 5% higher classification accuracy than prior methods.

</details>


### [110] [An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction](https://arxiv.org/abs/2511.22460)
*Yifan Lei,Jiahua Luo,Tingyu Jiang,Bo Zhang,Lifeng Wang,Dapeng Liu,Zhaoren Wu,Haijie Gu,Huan Yu,Jie Jiang*

Main category: cs.LG

TL;DR: 提出基于GPU加速的双塔网络特征交互方法，在保持检索效率的同时显著提升精度，首次在工业界检索系统中成功实现Wide & Deep架构。


<details>
  <summary>Details</summary>
Motivation: 大规模广告推荐系统中，基于双塔网络的嵌入检索方法虽然兼顾效率与精度，但用户和广告嵌入仅在最后内积计算时交互，特征交互能力不足。虽然排序阶段引入DNN模型允许早期特征交互，但计算成本过高无法用于检索阶段。

Method: 提出基于GPU的高效特征交互双塔网络，引入专为GPU加速设计的新型压缩倒排列表，实现大规模特征交互计算。这是工业界首个在检索系统中成功实现Wide & Deep的框架。

Result: 在腾讯广告真实业务场景中应用，离线评估优于现有方法，已成功部署到腾讯广告推荐系统，带来显著的在线性能提升。

Conclusion: 该方法不仅验证了所提方法的有效性，还为优化大规模广告检索系统提供了新的实践指导，成功解决了双塔模型特征交互不足的问题。

Abstract: In large-scale advertising recommendation systems, retrieval serves as a critical component, aiming to efficiently select a subset of candidate ads relevant to user behaviors from a massive ad inventory for subsequent ranking and recommendation. The Embedding-Based Retrieval (EBR) methods modeled by the dual-tower network are widely used in the industry to maintain both retrieval efficiency and accuracy. However, the dual-tower model has significant limitations: the embeddings of users and ads interact only at the final inner product computation, resulting in insufficient feature interaction capabilities. Although DNN-based models with both user and ad as input features, allowing for early-stage interaction between these features, are introduced in the ranking stage to mitigate this issue, they are computationally infeasible for the retrieval stage. To bridge this gap, this paper proposes an efficient GPU-based feature interaction for the dual-tower network to significantly improve retrieval accuracy while substantially reducing computational costs. Specifically, we introduce a novel compressed inverted list designed for GPU acceleration, enabling efficient feature interaction computation at scale. To the best of our knowledge, this is the first framework in the industry to successfully implement Wide and Deep in a retrieval system. We apply this model to the real-world business scenarios in Tencent Advertising, and experimental results demonstrate that our method outperforms existing approaches in offline evaluation and has been successfully deployed to Tencent's advertising recommendation system, delivering significant online performance gains. This improvement not only validates the effectiveness of the proposed method, but also provides new practical guidance for optimizing large-scale ad retrieval systems.

</details>


### [111] [Adversarial Flow Models](https://arxiv.org/abs/2511.22475)
*Shanchuan Lin,Ceyuan Yang,Zhijie Lin,Hao Chen,Haoqi Fan*

Main category: cs.LG

TL;DR: 提出对抗性流模型，统一对抗模型和流模型，支持单步或多步生成，通过对抗目标训练，比传统GAN更稳定，比一致性方法更高效


<details>
  <summary>Details</summary>
Motivation: 传统GAN学习任意传输计划导致训练不稳定，一致性方法需要学习中间时间步导致模型容量浪费和误差累积。需要一种既能稳定训练又能高效单步生成的方法

Method: 提出对抗性流模型，结合对抗训练和流匹配。生成器学习确定性的噪声到数据映射（与流匹配模型相同的最优传输），使用对抗目标训练，支持单步或多步生成，无需学习中间时间步

Result: 在ImageNet-256px上，1NFE设置下，B/2模型接近一致性XL/2模型性能，XL/2模型创下2.38 FID新记录。通过深度重复端到端训练56层和112层模型，单次前向传播分别达到2.08和1.94 FID，超越2NFE和4NFE对应模型

Conclusion: 对抗性流模型成功统一了对抗模型和流模型的优势，实现了稳定训练和高效单步生成，在图像生成质量上达到新的最先进水平

Abstract: We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.

</details>


### [112] [Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges](https://arxiv.org/abs/2511.22483)
*Guanxi Lu,Hao Mark Chen,Zhiqiang Que,Wayne Luk,Hongxiang Fan*

Main category: cs.LG

TL;DR: 量化对LLM可信度指标有负面影响，提出精度集成投票方法提升可信度


<details>
  <summary>Details</summary>
Motivation: 现有量化框架主要关注困惑度或分类准确率，忽略了可信度指标，这在金融、医疗等高风险领域应用时存在风险

Method: 系统研究量化对四个可信度指标的影响，并提出精度集成投票方法，利用同一模型不同精度变体的预测

Result: 量化在不同压缩比和量化方法下对可信度指标表现出不稳定性，精度集成投票方法可将可信度指标性能提升高达5.8%

Conclusion: 开发模型压缩技术时应考虑可信度因素，压缩与可信度的交叉研究对安全关键应用具有重要意义

Abstract: Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications.

</details>


### [113] [Space Explanations of Neural Network Classification](https://arxiv.org/abs/2511.22498)
*Faezeh Labbaf,Tomáš Kolárik,Martin Blicha,Grigory Fedyukovich,Michael Wand,Natasha Sharygina*

Main category: cs.LG

TL;DR: 提出一种名为空间解释的逻辑概念，为神经网络在连续输入特征空间中的行为提供可证明的保证


<details>
  <summary>Details</summary>
Motivation: 需要为神经网络分类提供具有可证明保证的解释方法，确保网络在连续输入区域的行为可解释

Method: 使用Craig插值算法和不可满足核心生成来自动生成空间解释

Result: 在从小型到大型的实际案例研究中，生成的空间解释比现有最先进方法计算的解释更有意义

Conclusion: 空间解释为神经网络分类提供了具有可证明保证的有意义解释，优于现有方法

Abstract: We present a novel logic-based concept called Space Explanations for classifying neural networks that gives provable guarantees of the behavior of the network in continuous areas of the input feature space. To automatically generate space explanations, we leverage a range of flexible Craig interpolation algorithms and unsatisfiable core generation. Based on real-life case studies, ranging from small to medium to large size, we demonstrate that the generated explanations are more meaningful than those computed by state-of-the-art.

</details>


### [114] [Privacy-Utility-Bias Trade-offs for Privacy-Preserving Recommender Systems](https://arxiv.org/abs/2511.22515)
*Shiva Parsarad,Isabel Wagner*

Main category: cs.LG

TL;DR: 该研究系统评估了差分隐私机制（DPSGD和LDP）对四种推荐系统（NCF、BPR、SVD、VAE）在准确性和公平性方面的影响，发现隐私保护强度增加会降低推荐效用，但不同模型受影响程度不同，没有单一的差分隐私机制在所有情况下都最优。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统越来越多地采用差分隐私来保护用户数据，需要了解隐私机制如何影响推荐准确性和公平性。目前缺乏对不同差分隐私机制和推荐模型组合效果的全面评估。

Method: 采用跨模型评估方法，比较两种差分隐私机制（DPSGD和LDP）在四种推荐系统（NCF、BPR、SVD、VAE）上的表现，使用MovieLens-1M和Yelp数据集进行实验。

Result: 更强的隐私保护会降低推荐效用，但影响不均匀：NCF在DPSGD下精度损失最小（ε≈1时低于10%），SVD和BPR下降更大，VAE对隐私最敏感。DPSGD通常缩小热门和冷门物品的推荐差距，而LDP更接近保持原有模式。

Conclusion: 没有单一的差分隐私机制在所有情况下都最优，每种机制在不同隐私保护程度和数据条件下都提供不同的权衡。选择隐私机制需要考虑具体推荐模型、数据特征和隐私要求。

Abstract: Recommender systems (RSs) output ranked lists of items, such as movies or restaurants, that users may find interesting, based on the user's past ratings and ratings from other users. RSs increasingly incorporate differential privacy (DP) to protect user data, raising questions about how privacy mechanisms affect both recommendation accuracy and fairness. We conduct a comprehensive, cross-model evaluation of two DP mechanisms, differentially private stochastic gradient descent (DPSGD) and local differential privacy (LDP), applied to four recommender systems (Neural Collaborative Filtering (NCF), Bayesian Personalized Ranking (BPR), Singular Value Decomposition (SVD), and Variational Autoencoder (VAE)) on the MovieLens-1M and Yelp datasets. We find that stronger privacy consistently reduces utility, but not uniformly. NCF under DPSGD shows the smallest accuracy loss (under 10 percent at epsilon approximately 1), whereas SVD and BPR experience larger drops, especially for users with niche preferences. VAE is the most sensitive to privacy, with sharp declines for sparsely represented groups. The impact on bias metrics is similarly heterogeneous. DPSGD generally reduces the gap between recommendations of popular and less popular items, whereas LDP preserves existing patterns more closely. These results highlight that no single DP mechanism is uniformly superior; instead, each provides trade-offs under different privacy regimes and data conditions.

</details>


### [115] [List-Decodable Regression via Expander Sketching](https://arxiv.org/abs/2511.22524)
*Herbod Pourali,Sajjad Hashemian,Ebrahim Ardeshir-Larijani*

Main category: cs.LG

TL;DR: 提出基于扩展图草图的列表可解码线性回归框架，实现样本复杂度$\tilde{O}((d+\log(1/δ))/α)$，列表大小$O(1/α)$，接近输入稀疏度的运行时间$\tilde{O}(\mathrm{nnz}(X)+d^{3}/α)$


<details>
  <summary>Details</summary>
Motivation: 解决列表可解码线性回归问题，旨在设计高效算法，在存在大量异常值（污染）的情况下恢复多个候选解，避免使用复杂的SoS方法或显式批次结构

Method: 使用无损扩展图合成轻度污染的批次，通过鲁棒聚合和短谱滤波阶段实现，避免SoS机制和显式批次结构

Result: 在标准亚高斯假设下，实现了最优样本复杂度、小列表大小和接近输入稀疏度的运行时间，匹配已知最高效保证

Conclusion: 扩展图草图框架为列表可解码线性回归提供了高效实用的解决方案，避免了复杂理论工具，具有实际应用价值

Abstract: We introduce an expander-sketching framework for list-decodable linear regression that achieves sample complexity $\tilde{O}((d+\log(1/δ))/α)$, list size $O(1/α)$, and near input-sparsity running time $\tilde{O}(\mathrm{nnz}(X)+d^{3}/α)$ under standard sub-Gaussian assumptions. Our method uses lossless expanders to synthesize lightly contaminated batches, enabling robust aggregation and a short spectral filtering stage that matches the best known efficient guarantees while avoiding SoS machinery and explicit batch structure.

</details>


### [116] [Where to Measure: Epistemic Uncertainty-Based Sensor Placement with ConvCNPs](https://arxiv.org/abs/2511.22567)
*Feyza Eksen,Stefan Oehmcke,Stefan Lüdtke*

Main category: cs.LG

TL;DR: 提出基于认知不确定性的传感器布局新方法，通过扩展ConvCNPs结合MDNs估计认知不确定性，相比传统总体不确定性方法能更有效降低模型误差。


<details>
  <summary>Details</summary>
Motivation: 现有传感器布局方法依赖总体预测不确定性，混淆了认知不确定性和偶然不确定性，可能导致在模糊区域选择次优传感器位置。

Method: 提出基于认知不确定性减少期望的新采集函数，扩展ConvCNPs架构，加入混合密度网络（MDNs）输出头来估计认知不确定性。

Result: 初步结果表明，基于认知不确定性的传感器布局比基于总体不确定性的方法能更有效地减少模型误差。

Conclusion: 认知不确定性驱动的传感器布局方法优于传统总体不确定性方法，为时空系统建模提供了更有效的传感器选择策略。

Abstract: Accurate sensor placement is critical for modeling spatio-temporal systems such as environmental and climate processes. Neural Processes (NPs), particularly Convolutional Conditional Neural Processes (ConvCNPs), provide scalable probabilistic models with uncertainty estimates, making them well-suited for data-driven sensor placement. However, existing approaches rely on total predictive uncertainty, which conflates epistemic and aleatoric components, that may lead to suboptimal sensor selection in ambiguous regions. To address this, we propose expected reduction in epistemic uncertainty as a new acquisition function for sensor placement. To enable this, we extend ConvCNPs with a Mixture Density Networks (MDNs) output head for epistemic uncertainty estimation. Preliminary results suggest that epistemic uncertainty driven sensor placement more effectively reduces model error than approaches based on overall uncertainty.

</details>


### [117] [Entropy is all you need for Inter-Seed Cross-Play in Hanabi](https://arxiv.org/abs/2511.22581)
*Johannes Forkel,Jakob Foerster*

Main category: cs.LG

TL;DR: 在Hanabi游戏中，标准独立PPO算法通过提高熵系数到0.05（而非通常的0.01），在跨种子交叉游戏中达到新的最优性能，超越了所有专门设计的算法。


<details>
  <summary>Details</summary>
Motivation: 研究在零样本协调和临时团队协作的复杂基准Hanabi中，标准算法能否通过简单超参数调整实现优秀的跨种子交叉性能，挑战专门设计的算法。

Method: 使用独立PPO算法，将熵系数从0.01提高到0.05，同时采用高GAE系数（λ=0.9），在actor-critic架构中使用RNN而非前馈网络。

Result: 该方法在Hanabi的跨种子交叉游戏中取得了新的最优性能，显著超越了所有先前专门设计的算法。高熵正则化确保不同随机种子产生的联合策略相互兼容。

Conclusion: 超参数对跨种子交叉性能有显著影响，但标准策略梯度方法在高熵正则化下仍无法在所有简单Dec-POMDP中实现完美跨种子交叉，表明仍需新算法来解决零样本协调问题。

Abstract: We find that in Hanabi, one of the most complex and popular benchmarks for zero-shot coordination and ad-hoc teamplay, a standard implementation of independent PPO with a slightly higher entropy coefficient 0.05 instead of the typically used 0.01, achieves a new state-of-the-art in cross-play between different seeds, beating by a significant margin all previous specialized algorithms, which were specifically designed for this setting. We provide an intuition for why sufficiently high entropy regularization ensures that different random seed produce joint policies which are mutually compatible. We also empirically find that a high $λ_{\text{GAE}}$ around 0.9, and using RNNs instead of just feed-forward layers in the actor-critic architecture, strongly increase inter-seed cross-play. While these results demonstrate the dramatic effect that hyperparameters can have not just on self-play scores but also on cross-play scores, we show that there are simple Dec-POMDPs though, in which standard policy gradient methods with increased entropy regularization are not able to achieve perfect inter-seed cross-play, thus demonstrating the continuing necessity for new algorithms for zero-shot coordination.

</details>


### [118] [The Multiclass Score-Oriented Loss (MultiSOL) on the Simplex](https://arxiv.org/abs/2511.22587)
*Francesco Marchetti,Edoardo Legnaro,Sabrina Guastavino*

Main category: cs.LG

TL;DR: 将二元分类中的score-oriented损失函数扩展到多分类任务，提出MultiSOL损失函数，直接优化目标指标，避免后验阈值调整，并在类别不平衡下保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在监督二元分类中，score-oriented损失函数可以直接优化目标性能指标，避免训练后需要调整阈值。然而，多分类任务中缺乏类似的损失函数，需要将这一优势扩展到多分类场景。

Method: 使用最近提出的多维阈值分类框架，将score-oriented损失函数扩展到多分类，定义Multiclass Score-Oriented Loss (MultiSOL)函数。在构建过程中，将决策阈值视为具有先验分布的随机变量。

Result: MultiSOL损失函数在多分类实验中表现出色，保持了二元设置中的主要优势：直接优化目标指标、对类别不平衡的鲁棒性，性能与最先进的损失函数相当，并提供了关于单纯形几何与score-oriented学习交互的新见解。

Conclusion: 成功将score-oriented损失函数扩展到多分类任务，提出的MultiSOL函数能够直接优化目标性能指标，避免后验阈值调整，在类别不平衡下表现鲁棒，为多分类损失函数设计提供了新思路。

Abstract: In the supervised binary classification setting, score-oriented losses have been introduced with the aim of optimizing a chosen performance metric directly during the training phase, thus avoiding \textit{a posteriori} threshold tuning. To do this, in their construction, the decision threshold is treated as a random variable provided with a certain \textit{a priori} distribution. In this paper, we use a recently introduced multidimensional threshold-based classification framework to extend such score-oriented losses to multiclass classification, defining the Multiclass Score-Oriented Loss (MultiSOL) functions. As also demonstrated by several classification experiments, this proposed family of losses is designed to preserve the main advantages observed in the binary setting, such as the direct optimization of the target metric and the robustness to class imbalance, achieving performance comparable to other state-of-the-art loss functions and providing new insights into the interaction between simplex geometry and score-oriented learning.

</details>


### [119] [LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system](https://arxiv.org/abs/2511.22598)
*Huanyu Li,Zongyuan Li,Wei Huang,Xian Guo*

Main category: cs.LG

TL;DR: LLM-Cave是一个轻量级基准测试环境，用于评估大语言模型的序列推理和决策能力，相比现有复杂环境更高效。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准主要关注单步交互，而序列决策环境如TextStarCraftII过于复杂耗时，需要更轻量高效的评估工具来测试LLM的序列推理和决策能力。

Method: 提出了LLM-Cave基准测试环境，这是一个基于符号主义时代的经典实例，允许智能体在部分可观测状态下通过推理探索环境并规避危险。评估了GPT-4o-mini、o1-mini和DeepSeek-R1等主流LLM的序列推理能力、决策性能和计算效率。

Result: DeepSeek-R1在复杂推理任务上成功率最高，但较小模型如4o-mini通过使用思维链推测和规划-批评策略显著缩小了性能差距，代价是计算效率降低。结构化多步推理结合基于LLM的反馈机制能显著提升LLM决策能力。

Conclusion: 结构化多步推理与LLM反馈机制结合能有效增强LLM决策能力，为改进较弱模型的推理提供了有前景的方向，并提出了以推理为中心的LLM评估新基准。

Abstract: Large language models (LLMs) such as ChatGPT o1, ChatGPT o3, and DeepSeek R1 have shown great potential in solving difficult problems. However, current LLM evaluation benchmarks are limited to one-step interactions. Some of the existing sequence decision-making environments, such as TextStarCraftII and LLM-PySC2, are too complicated and require hours of interaction to complete a game. In this paper, we introduce LLM-Cave, a benchmark and light environment for LLM reasoning and decision-making systems. This environment is a classic instance in the era of Symbolism. Artificial intelligence enables the agent to explore the environment and avoid potential losses by reasoning about nearby dangers using partial observable state information. In the experiment, we evaluated the sequential reasoning ability, decision-making performance and computational efficiency of mainstream large language models (LLMs) such as GPT-4o-mini, o1-mini, and DeepSeek-R1. Experiments show that while Deepseek-R1 achieved the highest success rate on complex reasoning tasks, smaller models like 4o-mini significantly narrowed the performance gap on challenges by employing Chain of Speculation and Planner-Critic strategies, at the expense of reduced computational efficiency. This indicates that structured, multi-step reasoning combined with an LLM-based feedback mechanism can substantially enhance an LLM's decision-making capabilities, providing a promising direction for improving reasoning in weaker models and suggesting a new reasoning-centered benchmark for LLM assessment. Our code is open-sourced in https://github.com/puleya1277/CaveEnv.

</details>


### [120] [Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers](https://arxiv.org/abs/2511.22616)
*Meriem Arbaoui,Mohamed-el-Amine Brahmia,Abdellatif Rahmoun,Mourad Zghal*

Main category: cs.LG

TL;DR: 本文对联邦学习进行了系统性综述，重点关注个性化、优化和鲁棒性三个研究方向，通过混合方法（文献计量分析与系统综述）对聚合策略、评估方法等进行了全面分析，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 物联网与人工智能的融合带来了创新，但隐私问题和数据孤岛阻碍了进展。传统集中式机器学习难以解决这些问题，因此需要联邦学习这种去中心化范式，在保护数据隐私的同时实现协作模型训练。

Method: 采用混合方法学：结合文献计量分析和系统综述，对联邦学习研究进行结构化分类。重点关注三个研究方向：个性化、优化和鲁棒性，分析聚合策略（架构、同步方法、联邦目标）和评估方法，并进行IID和非IID数据分布下的聚合方法实验比较。

Result: 提供了联邦学习的全面综述，系统分析了异质性、效率、安全性和隐私方面的挑战与技术，比较了不同聚合方法在IID和非IID数据分布下的表现，为研究者提供了结构化分类和评估框架。

Conclusion: 联邦学习作为解决数据隐私和孤岛问题的有效范式，虽然异质性增加了复杂性，但在个性化、优化和鲁棒性方面具有重要研究价值。未来需要在聚合策略、评估方法和实际应用等方面进一步创新，推动该领域的持续发展。

Abstract: The integration of IoT and AI has unlocked innovation across industries, but growing privacy concerns and data isolation hinder progress. Traditional centralized ML struggles to overcome these challenges, which has led to the rise of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing local raw data. FL ensures data privacy, reduces communication overhead, and supports scalability, yet its heterogeneity adds complexity compared to centralized approaches. This survey focuses on three main FL research directions: personalization, optimization, and robustness, offering a structured classification through a hybrid methodology that combines bibliometric analysis with systematic review to identify the most influential works. We examine challenges and techniques related to heterogeneity, efficiency, security, and privacy, and provide a comprehensive overview of aggregation strategies, including architectures, synchronization methods, and diverse federation objectives. To complement this, we discuss practical evaluation approaches and present experiments comparing aggregation methods under IID and non-IID data distributions. Finally, we outline promising research directions to advance FL, aiming to guide future innovation in this rapidly evolving field.

</details>


### [121] [Flow Density Control: Generative Optimization Beyond Entropy-Regularized Fine-Tuning](https://arxiv.org/abs/2511.22640)
*Riccardo De Santi,Marin Vlastelica,Ya-Ping Hsieh,Zebang Shen,Niao He,Andreas Krause*

Main category: cs.LG

TL;DR: 提出Flow Density Control (FDC)算法，用于优化预训练生成模型的任务特定目标，支持超越平均奖励的通用效用函数和多种先验信息保留方式


<details>
  <summary>Details</summary>
Motivation: 现有微调方法主要关注最大化生成样本的期望奖励，并通过KL散度正则化保留预训练模型知识。但在实际应用中需要优化更通用的效用函数（如风险规避、新颖性寻求、多样性度量等），并考虑更通用的先验信息保留方式（如最优传输距离、Renyi散度）

Method: 提出Flow Density Control (FDC)算法，将复杂优化问题分解为一系列更简单的微调任务序列，每个子任务可通过现有可扩展方法求解。基于镜像流的最新理解推导收敛保证

Result: 在说明性设置、文本到图像和分子设计任务上验证了方法的有效性，表明能够引导预训练生成模型优化目标，解决当前微调方案无法处理的实用相关任务

Conclusion: FDC算法为优化预训练生成模型提供了通用框架，支持广泛的效用函数和先验保留方式，扩展了生成模型在实际应用中的能力

Abstract: Adapting large-scale foundation flow and diffusion generative models to optimize task-specific objectives while preserving prior information is crucial for real-world applications such as molecular design, protein docking, and creative image generation. Existing principled fine-tuning methods aim to maximize the expected reward of generated samples, while retaining knowledge from the pre-trained model via KL-divergence regularization. In this work, we tackle the significantly more general problem of optimizing general utilities beyond average rewards, including risk-averse and novelty-seeking reward maximization, diversity measures for exploration, and experiment design objectives among others. Likewise, we consider more general ways to preserve prior information beyond KL-divergence, such as optimal transport distances and Renyi divergences. To this end, we introduce Flow Density Control (FDC), a simple algorithm that reduces this complex problem to a specific sequence of simpler fine-tuning tasks, each solvable via scalable established methods. We derive convergence guarantees for the proposed scheme under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we validate our method on illustrative settings, text-to-image, and molecular design tasks, showing that it can steer pre-trained generative models to optimize objectives and solve practically relevant tasks beyond the reach of current fine-tuning schemes.

</details>


### [122] [Spatially Aware Dictionary-Free Eigenfunction Identification for Modeling and Control of Nonlinear Dynamical Systems](https://arxiv.org/abs/2511.22648)
*David Grasev*

Main category: cs.LG

TL;DR: 提出一种无需预定义基函数的数据驱动Koopman特征函数发现方法，通过参考轨迹识别Koopman模态振幅，转换到包含特征值和时间的基本函数新基，利用正则化最小二乘拟合投影轨迹获取特征函数初始值，全局优化特征值，并通过惩罚Koopman偏微分方程偏差获得更鲁棒解。


<details>
  <summary>Details</summary>
Motivation: 现有Koopman算子方法通常需要预定义基函数，限制了其在复杂非线性系统中的应用。本文旨在开发一种无需预定义基函数的数据驱动方法，能够自动发现Koopman特征函数，提高预测精度并揭示状态空间的几何特征。

Method: 基于参考轨迹识别Koopman模态振幅，将模态分解转换到包含特征值和时间的基本函数新基；通过正则化最小二乘拟合将轨迹投影到该基获取特征函数初始值；使用全局优化器优化特征值；通过映射初始状态值到特征函数值揭示空间结构并数值计算梯度；惩罚Koopman偏微分方程偏差以获得鲁棒解。

Result: 方法在多个基准非线性动力系统上成功测试，包括带输入的FitzHugh-Nagumo系统、van der Pol和Duffing振荡器、带控制的2轴涡轮喷气发动机。结果表明，结合主特征值和空间结构完整性促进显著提高了Koopman预测器的准确性。即使在稀疏状态空间采样下也能有效发现Koopman谱分量，并揭示状态空间的几何特征如不变分区。

Conclusion: 该方法无需预定义基函数即可发现Koopman特征函数，提高了预测精度并揭示了状态空间几何特征。特征函数梯度的数值近似可用于输入动力学建模和控制设计，支持该方法在各种动力系统中的实用性。

Abstract: A new approach to data-driven discovery of Koopman eigenfunctions without a pre-defined set of basis functions is proposed. The approach is based on a reference trajectory, for which the Koopman mode amplitudes are first identified, and the Koopman mode decomposition is transformed to a new basis, which contains fundamental functions of eigenvalues and time. The initial values of the eigenfunctions are obtained by projecting trajectories onto this basis via a regularized least-squares fit. A global optimizer was employed to optimize the eigenvalues. Mapping initial-state values to eigenfunction values reveals their spatial structure, enabling the numerical computation of their gradients. Thus, deviations from the Koopman partial differential equation are penalized, leading to more robust solutions. The approach was successfully tested on several benchmark nonlinear dynamical systems, including the FitzHugh-Nagumo system with inputs, van der Pol and Duffing oscillators, and a 2-spool turbojet engine with control. The study demonstrates that incorporating principal eigenvalues and spatial structure integrity promotion significantly improves the accuracy of Koopman predictors. The approach effectively discovers Koopman spectral components even with sparse state-space sampling and reveals geometric features of the state space, such as invariant partitions. Finally, the numerical approximation of the eigenfunction gradient can be used for input dynamics modeling and control design. The results support the practicality of the approach for use with various dynamical systems.

</details>


### [123] [Automated Design Optimization via Strategic Search with Large Language Models](https://arxiv.org/abs/2511.22651)
*Anthony Carreon,Vansh Sharma,Venkat Raman*

Main category: cs.LG

TL;DR: AUTO是一个基于大语言模型的智能体框架，将设计优化视为由LLM策略推理引导的无梯度搜索问题，在GPU代码优化中表现出色，搜索效率达到贝叶斯优化的50-70%，成本远低于人工开发。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在定义明确的搜索空间中表现良好，但在设计参数难以定义的设计问题中表现不佳。大语言模型能够动态解释设计空间并利用编码的领域知识，为解决这类问题提供了有前景的替代方案。

Method: 提出AUTO框架，包含两个协作智能体：策略师（Strategist）负责在探索和利用策略之间选择，实施者（Implementor）负责执行详细设计。将设计优化视为由LLM策略推理引导的无梯度搜索问题。

Result: 在GPU代码优化领域（化学动力学积分和稠密矩阵乘法），AUTO生成的解决方案与专家实现相当。搜索效率达到贝叶斯优化方法的50-70%，每次优化约需8小时，成本约159美元，远低于软件开发者480美元的中位数成本。

Conclusion: AUTO框架为在定义不明确、先验信息有限的搜索空间中自动化设计优化打开了大门，展示了LLM在复杂设计优化问题中的潜力。

Abstract: Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \$159 per run, compared to an estimated cost of up to \$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.

</details>


### [124] [Structure-aware Hybrid-order Similarity Learning for Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.22656)
*Lin Xu,Ke Li,Dongjie Wang,Fengmao Lv,Tianrui Li,Yanyong Huang*

Main category: cs.LG

TL;DR: SHINE-FS是一种新颖的多视图无监督特征选择方法，通过联合学习一阶和二阶相似性图来捕捉局部和全局结构，从而提高特征选择性能。


<details>
  <summary>Details</summary>
Motivation: 现有多视图无监督特征选择方法主要依赖一阶相似性图来保持局部结构，但往往忽略了二阶相似性能够捕捉的全局结构。少数使用预定义二阶相似性图的方法容易受到噪声和异常值的影响，导致特征选择性能不佳。

Method: SHINE-FS首先学习共识锚点和相应的锚点图来捕捉锚点与样本之间的跨视图关系。基于获得的跨视图共识信息，生成样本的低维表示，通过识别判别性特征来重构多视图数据。然后利用锚点-样本关系学习二阶相似性图。最后，通过联合学习一阶和二阶相似性图，构建混合阶相似性图来捕捉局部和全局结构。

Result: 在真实多视图数据集上的综合实验结果表明，SHINE-FS优于现有的最先进方法。

Conclusion: SHINE-FS通过联合学习混合阶相似性图，能够更好地揭示数据的内在结构，从而在多视图无监督特征选择任务中取得更好的性能。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently emerged as an effective dimensionality reduction method for unlabeled multi-view data. However, most existing methods mainly use first-order similarity graphs to preserve local structure, often overlooking the global structure that can be captured by second-order similarity. In addition, a few MUFS methods leverage predefined second-order similarity graphs, making them vulnerable to noise and outliers and resulting in suboptimal feature selection performance. In this paper, we propose a novel MUFS method, termed Structure-aware Hybrid-order sImilarity learNing for multi-viEw unsupervised Feature Selection (SHINE-FS), to address the aforementioned problem. SHINE-FS first learns consensus anchors and the corresponding anchor graph to capture the cross-view relationships between the anchors and the samples. Based on the acquired cross-view consensus information, it generates low-dimensional representations of the samples, which facilitate the reconstruction of multi-view data by identifying discriminative features. Subsequently, it employs the anchor-sample relationships to learn a second-order similarity graph. Furthermore, by jointly learning first-order and second-order similarity graphs, SHINE-FS constructs a hybrid-order similarity graph that captures both local and global structures, thereby revealing the intrinsic data structure to enhance feature selection. Comprehensive experimental results on real multi-view datasets show that SHINE-FS outperforms the state-of-the-art methods.

</details>


### [125] [Difficulties with Evaluating a Deception Detector for AIs](https://arxiv.org/abs/2511.22662)
*Lewis Smith,Bilal Chughtai,Neel Nanda*

Main category: cs.LG

TL;DR: 论文指出当前缺乏可靠评估AI欺骗检测器所需的标记数据，并识别了收集这些数据的障碍，认为现有方法不足


<details>
  <summary>Details</summary>
Motivation: 构建可靠的AI欺骗检测器对于减轻高级AI系统的风险至关重要，但评估这些检测器需要能够明确标记为欺骗或诚实行为的示例

Method: 通过概念论证、现有实证研究分析以及新颖案例研究分析，识别收集欺骗检测训练数据的障碍，并评估潜在解决方案

Result: 当前缺乏足够标记数据来可靠评估欺骗检测器，存在多个具体障碍，且现有解决方案单独使用可能不足

Conclusion: 欺骗检测的进展需要进一步考虑数据收集问题，现有方法虽然有价值但可能不足以解决根本挑战

Abstract: Building reliable deception detectors for AI systems -- methods that could predict when an AI system is being strategically deceptive without necessarily requiring behavioural evidence -- would be valuable in mitigating risks from advanced AI systems. But evaluating the reliability and efficacy of a proposed deception detector requires examples that we can confidently label as either deceptive or honest. We argue that we currently lack the necessary examples and further identify several concrete obstacles in collecting them. We provide evidence from conceptual arguments, analysis of existing empirical works, and analysis of novel illustrative case studies. We also discuss the potential of several proposed empirical workarounds to these problems and argue that while they seem valuable, they also seem insufficient alone. Progress on deception detection likely requires further consideration of these problems.

</details>


### [126] [Modèles de Fondation et Ajustement : Vers une Nouvelle Génération de Modèles pour la Prévision des Séries Temporelles](https://arxiv.org/abs/2511.22674)
*Morad Laglil,Emilie Devijver,Eric Gaussier,Bertrand Pracca*

Main category: cs.LG

TL;DR: 本文综述了用于零样本时间序列预测的基础模型架构、预训练策略和优化方法，并研究了微调对特定数据集性能的影响，发现微调能提升零样本预测能力，尤其对长期预测效果更佳。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型启发，开发用于零样本时间序列预测的基础模型，这些模型在大量时间序列数据上预训练，学习可泛化的表示，减少对任务特定架构和手动调参的需求。

Method: 综述了主要架构、预训练策略和优化方法，并研究了预训练后微调对特定数据集性能的影响。

Result: 实证结果表明，微调通常能提升零样本预测能力，特别是对长期预测效果更显著。

Conclusion: 微调是提升时间序列基础模型在特定数据集上性能的有效方法，尤其对长期预测任务有显著改善。

Abstract: Inspired by recent advances in large language models, foundation models have been developed for zero-shot time series forecasting, enabling prediction on datasets unseen during pretraining. These large-scale models, trained on vast collections of time series, learn generalizable representations for both point and probabilistic forecasting, reducing the need for task-specific architectures and manual tuning.
  In this work, we review the main architectures, pretraining strategies, and optimization methods used in such models, and study the effect of fine-tuning after pretraining to enhance their performance on specific datasets. Our empirical results show that fine-tuning generally improves zero-shot forecasting capabilities, especially for long-term horizons.

</details>


### [127] [Test-time scaling of diffusions with flow maps](https://arxiv.org/abs/2511.22688)
*Amirmojtaba Sabour,Michael S. Albergo,Carles Domingo-Enrich,Nicholas M. Boffi,Sanja Fidler,Karsten Kreis,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出FMTT算法，通过流映射直接处理扩散模型测试时优化，比传统奖励梯度方法更有效地提升样本质量


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型测试时优化方法存在问题：用户指定的奖励函数通常只在生成结束时的数据分布上定义良好，而常用的去噪器估计方法不够理想

Method: 利用流映射与瞬时传输速度场的关系，构建Flow Map Trajectory Tilting (FMTT)算法，直接处理流映射而非依赖奖励梯度

Result: FMTT在奖励提升方面理论上优于标准测试时方法，支持精确采样和原则性搜索，能与复杂奖励函数（如视觉语言模型）交互实现新形式图像编辑

Conclusion: 通过流映射直接处理扩散过程提供了一种更有效的测试时优化方法，为复杂奖励函数下的图像编辑开辟了新途径

Abstract: A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.

</details>


### [128] [Generative Anchored Fields: Controlled Data Generation via Emergent Velocity Fields and Transport Algebra](https://arxiv.org/abs/2511.22693)
*Deressa Wodajo Deressa,Hannes Mareen,Peter Lambert,Glenn Van Wallendael*

Main category: cs.LG

TL;DR: GAF是一种生成模型，通过分解学习独立的端点预测器J（噪声）和K（数据），速度场v=K-J从它们的时间条件分歧中产生，支持传输代数操作实现组合控制。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型通常学习轨迹预测器，限制了组合控制能力。本文旨在通过分解学习独立的端点预测器，实现更丰富的组合生成和语义操作。

Method: 提出生成锚定场（GAF），学习独立的噪声端点预测器J和数据端点预测器K，速度场v=K-J从它们的时间条件分歧中产生。支持传输代数操作，通过类别特定的K_n头实现共享基分布到多模态的定向传输映射。

Result: 在CelebA-HQ 64×64上达到FID 7.5的强样本质量，支持可控插值、混合生成和语义变形。实现初始和最终状态之间的无损循环传输（LPIPS=0.0）。

Conclusion: GAF通过分解学习端点预测器而非轨迹预测器，实现了传输代数操作，将组合生成作为架构原语，在保持高质量样本的同时提供了丰富的组合控制能力。

Abstract: We present Generative Anchored Fields (GAF), a generative model that learns independent endpoint predictors $J$ (noise) and $K$ (data) rather than a trajectory predictor. The velocity field $v=K-J$ emerges from their time-conditioned disagreement. This factorization enables \textit{Transport Algebra}: algebraic operation on learned $\{(J_n,K_n)\}_{n=1}^N$ heads for compositional control. With class-specific $K_n$ heads, GAF supports a rich family of directed transport maps between a shared base distribution and multiple modalities, enabling controllable interpolation, hybrid generation, and semantic morphing through vector arithmetic. We achieve strong sample quality (FID 7.5 on CelebA-HQ $64\times 64$) while uniquely providing compositional generation as an architectural primitive. We further demonstrate, GAF has lossless cyclic transport between its initial and final state with LPIPS=$0.0$. Code available at https://github.com/IDLabMedia/GAF

</details>


### [129] [Integrated Transcriptomic-proteomic Biomarker Identification for Radiation Response Prediction in Non-small Cell Lung Cancer Cell Lines](https://arxiv.org/abs/2511.22735)
*Yajun Yu,Guoping Xu,Steve Jiang,Robert Timmerman,John Minna,Yuanyuan Zhang,Hao Peng*

Main category: cs.LG

TL;DR: 开发了首个用于预测非小细胞肺癌辐射反应（SF2）的转录组-蛋白质组整合框架，通过多组学数据识别并发生物标志物


<details>
  <summary>Details</summary>
Motivation: 为了开发一个整合转录组和蛋白质组的框架，识别能够预测非小细胞肺癌细胞系辐射反应（以SF2为指标）的并发生物标志物

Method: 收集73个NSCLC细胞系的RNA-seq数据和46个细胞系的DIA-MS蛋白质组数据，保留1,605个共享基因。使用Lasso回归进行特征选择，构建SVR模型（转录组、蛋白质组和整合模型），通过交叉验证评估性能

Result: RNA-蛋白质表达呈显著正相关（中位数r=0.363）。单组学模型跨组学泛化能力有限，而整合模型在两个数据集中均表现出平衡的预测准确性（转录组R2=0.461，蛋白质组R2=0.604）

Conclusion: 这是首个用于NSCLC SF2预测的蛋白质转录组学框架，突显了整合转录组和蛋白质组数据的互补价值。识别的并发生物标志物既能捕捉转录调控又能反映功能蛋白活性，具有机制洞察和转化潜力

Abstract: To develop an integrated transcriptome-proteome framework for identifying concurrent biomarkers predictive of radiation response, as measured by survival fraction at 2 Gy (SF2), in non-small cell lung cancer (NSCLC) cell lines. RNA sequencing (RNA-seq) and data-independent acquisition mass spectrometry (DIA-MS) proteomic data were collected from 73 and 46 NSCLC cell lines, respectively. Following preprocessing, 1,605 shared genes were retained for analysis. Feature selection was performed using least absolute shrinkage and selection operator (Lasso) regression with a frequency-based ranking criterion under five-fold cross-validation repeated ten times. Support vector regression (SVR) models were constructed using transcriptome-only, proteome-only, and combined transcriptome-proteome feature sets. Model performance was assessed by the coefficient of determination (R2) and root mean square error (RMSE). Correlation analyses evaluated concordance between RNA and protein expression and the relationships of selected biomarkers with SF2. RNA-protein expression exhibited significant positive correlations (median Pearson's r = 0.363). Independent pipelines identified 20 prioritized gene signatures from transcriptomic, proteomic, and combined datasets. Models trained on single-omic features achieved limited cross-omic generalizability, while the combined model demonstrated balanced predictive accuracy in both datasets (R2=0.461, RMSE=0.120 for transcriptome; R2=0.604, RMSE=0.111 for proteome). This study presents the first proteotranscriptomic framework for SF2 prediction in NSCLC, highlighting the complementary value of integrating transcriptomic and proteomic data. The identified concurrent biomarkers capture both transcriptional regulation and functional protein activity, offering mechanistic insights and translational potential.

</details>


### [130] [VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction for RTL Generation Optimization](https://arxiv.org/abs/2511.22749)
*Zeng Wang,Weihua Xiao,Minghao Shao,Raghu Vamshi Hemadri,Ozgur Sinanoglu,Muhammad Shafique,Ramesh Karri*

Main category: cs.LG

TL;DR: VeriDispatcher是一个多LLM RTL生成框架，通过预测任务难度来调度合适的LLM，在保持或提高准确率的同时显著降低商用API调用成本。


<details>
  <summary>Details</summary>
Motivation: 不同LLM在RTL生成任务上各有优势，但现有工作主要关注单个模型的提示或微调。如何协调多个不同LLM共同提高RTL质量同时降低成本，而不是运行所有模型选择最佳输出，这个问题尚未得到充分研究。

Method: 提出VeriDispatcher框架：1) 为每个LLM训练紧凑的分类器，基于任务描述的语义嵌入；2) 使用结合语法、结构相似性和功能正确性的基准变体计算难度分数；3) 在推理时根据预测器将任务路由到选定的LLM子集。

Result: 在RTLLM和VerilogEval基准测试中，VeriDispatcher在RTLLM上仅使用40%商用调用就实现了18%的准确率提升，在VerilogEval上保持准确率的同时将商用使用减少了25%。

Conclusion: VeriDispatcher通过智能调度多个LLM，实现了硬件设计自动化中成本效益高、高质量的LLM部署，解决了多LLM RTL生成问题。

Abstract: Large Language Models (LLMs) show strong performance in RTL generation, but different models excel on different tasks because of architecture and training differences. Prior work mainly prompts or finetunes a single model. What remains not well studied is how to coordinate multiple different LLMs so they jointly improve RTL quality while also reducing cost, instead of running all models and choosing the best output. We define this as the multi-LLM RTL generation problem. We propose VeriDispatcher, a multi-LLM RTL generation framework that dispatches each RTL task to suitable LLMs based on pre-inference difficulty prediction. For each model, we train a compact classifier over semantic embeddings of task descriptions, using difficulty scores derived from benchmark variants that combine syntax, structural similarity, and functional correctness. At inference, VeriDispatcher uses these predictors to route tasks to a selected subset of LLMs. Across 10 diverse LLMs on RTLLM and VerilogEval, VeriDispatcher achieves up to 18% accuracy improvement on RTLLM using only 40% of commercial calls, and on VerilogEval maintains accuracy while reducing commercial usage by 25%, enabling cost-effective, high-quality LLM deployment in hardware design automation.

</details>


### [131] [Exact Learning of Arithmetic with Differentiable Agents](https://arxiv.org/abs/2511.22751)
*Hristo Papazov,Francesco D'Angelo,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 提出一种基于可微分有限状态转换器的框架，能够在算术任务上实现精确算法学习和强大的长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索基于梯度的精确算法学习的可能性，解决现有架构在算术任务上长度泛化能力不足的问题。

Method: 使用可微分有限状态转换器（DFSTs），这是一种图灵完备的模型家族，支持恒定精度、恒定时间生成和端到端对数并行可微分训练。利用专家智能体的策略轨迹观察来训练模型执行二进制和十进制加减乘除运算。

Result: 模型在极小数据集上训练后，能够无错误地泛化到比训练样本长数千倍的输入，实现了精确的算法学习。

Conclusion: 在结构化中间监督下训练可微分智能体，为基于梯度的精确算法技能学习开辟了新途径。

Abstract: We explore the possibility of exact algorithmic learning with gradient-based methods and introduce a differentiable framework capable of strong length generalization on arithmetic tasks. Our approach centers on Differentiable Finite-State Transducers (DFSTs), a Turing-complete model family that avoids the pitfalls of prior architectures by enabling constant-precision, constant-time generation, and end-to-end log-parallel differentiable training. Leveraging policy-trajectory observations from expert agents, we train DFSTs to perform binary and decimal addition and multiplication. Remarkably, models trained on tiny datasets generalize without error to inputs thousands of times longer than the training examples. These results show that training differentiable agents on structured intermediate supervision could pave the way towards exact gradient-based learning of algorithmic skills. Code available at \href{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}.

</details>


### [132] [GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels](https://arxiv.org/abs/2511.22793)
*Bhavya Sai Nukapotula,Rishabh Tripathi,Seth Pregler,Dileep Kalathil,Srinivas Shakkottai,Theodore S. Rappaport*

Main category: cs.LG

TL;DR: GSpaRC使用3D高斯基元表示RF环境，通过定制CUDA流水线实现亚毫秒级延迟的实时信道状态信息重建，显著减少5G网络中的导频开销。


<details>
  <summary>Details</summary>
Motivation: 5G网络中获取信道状态信息需要频繁发送导频信号，消耗高达25%的频谱资源，现有方法虽然能减少开销但存在5-100ms的推理延迟，无法满足实时系统需求。

Method: 使用紧凑的3D高斯基元集合表示RF环境，每个基元由轻量级神经网络参数化并加入距离衰减等物理特征；采用针对RF接收定制的等距柱面投影到半球表面；开发定制CUDA流水线实现全并行化的方向排序、splatting和渲染。

Result: 在多个RF数据集上评估，GSpaRC达到与最新方法相似的CSI重建精度，同时将训练和推理时间减少一个数量级以上，首次突破1ms延迟壁垒。

Conclusion: GSpaRC通过适度的GPU计算换取显著的导频开销减少，实现了可扩展、低延迟的信道估计，适用于5G及未来无线系统的部署。

Abstract: Channel state information (CSI) is essential for adaptive beamforming and maintaining robust links in wireless communication systems. However, acquiring CSI incurs significant overhead, consuming up to 25\% of spectrum resources in 5G networks due to frequent pilot transmissions at sub-millisecond intervals. Recent approaches aim to reduce this burden by reconstructing CSI from spatiotemporal RF measurements, such as signal strength and direction-of-arrival. While effective in offline settings, these methods often suffer from inference latencies in the 5--100~ms range, making them impractical for real-time systems. We present GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels, the first algorithm to break the 1 ms latency barrier while maintaining high accuracy. GSpaRC represents the RF environment using a compact set of 3D Gaussian primitives, each parameterized by a lightweight neural model augmented with physics-informed features such as distance-based attenuation. Unlike traditional vision-based splatting pipelines, GSpaRC is tailored for RF reception: it employs an equirectangular projection onto a hemispherical surface centered at the receiver to reflect omnidirectional antenna behavior. A custom CUDA pipeline enables fully parallelized directional sorting, splatting, and rendering across frequency and spatial dimensions. Evaluated on multiple RF datasets, GSpaRC achieves similar CSI reconstruction fidelity to recent state-of-the-art methods while reducing training and inference time by over an order of magnitude. By trading modest GPU computation for a substantial reduction in pilot overhead, GSpaRC enables scalable, low-latency channel estimation suitable for deployment in 5G and future wireless systems. The code is available here: \href{https://github.com/Nbhavyasai/GSpaRC-WirelessGaussianSplatting.git}{GSpaRC}.

</details>


### [133] [Can Synthetic Data Improve Symbolic Regression Extrapolation Performance?](https://arxiv.org/abs/2511.22794)
*Fitria Wulandari Ramlan,Colm O'Riordan,Gabriel Kronberger,James McDermott*

Main category: cs.LG

TL;DR: 使用知识蒸馏生成合成数据来提升符号回归在数据稀疏区域的泛化能力，特别是在外推区域


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在训练数据范围内表现良好，但在外推（超出训练数据范围）时往往表现不佳。符号回归虽然能生成灵活模型，但在外推时容易出现不可靠行为

Method: 使用核密度估计识别输入空间中训练数据稀疏的区域，通过知识蒸馏方法生成合成数据：教师模型在新输入点上生成预测，然后用这些数据训练学生模型。评估了神经网络、随机森林和遗传编程作为教师和学生模型的效果

Result: 在六个基准数据集上的实验表明，遗传编程模型在训练合成数据后通常能改善外推性能，特别是在使用GPe生成的合成数据训练GPp时。内插区域只有轻微变化，模型性能在不同输入空间区域存在异质性误差

Conclusion: 通过知识蒸馏生成合成数据的方法为改善外推性能提供了实用解决方案，但改进效果取决于数据集和教师模型的选择

Abstract: Many machine learning models perform well when making predictions within the training data range, but often struggle when required to extrapolate beyond it. Symbolic regression (SR) using genetic programming (GP) can generate flexible models but is prone to unreliable behaviour in extrapolation. This paper investigates whether adding synthetic data can help improve performance in such cases. We apply Kernel Density Estimation (KDE) to identify regions in the input space where the training data is sparse. Synthetic data is then generated in those regions using a knowledge distillation approach: a teacher model generates predictions on new input points, which are then used to train a student model. We evaluate this method across six benchmark datasets, using neural networks (NN), random forests (RF), and GP both as teacher models (to generate synthetic data) and as student models (trained on the augmented data). Results show that GP models can often improve when trained on synthetic data, especially in extrapolation areas. However, the improvement depends on the dataset and teacher model used. The most important improvements are observed when synthetic data from GPe is used to train GPp in extrapolation regions. Changes in interpolation areas show only slight changes. We also observe heterogeneous errors, where model performance varies across different regions of the input space. Overall, this approach offers a practical solution for better extrapolation. Note: An earlier version of this work appeared in the GECCO 2025 Workshop on Symbolic Regression. This arXiv version corrects several parts of the original submission.

</details>


### [134] [Intelligent Neural Networks: From Layered Architectures to Graph-Organized Intelligence](https://arxiv.org/abs/2511.22813)
*Antoine Salomon*

Main category: cs.LG

TL;DR: INN提出了一种神经元中心的新型神经网络架构，其中神经元具有内部记忆和学习通信模式，通过图结构而非层级组织实现智能计算，在Text8基准上显著优于Transformer。


<details>
  <summary>Details</summary>
Motivation: 受生物神经元具有内部状态、选择性通信和自组织成复杂图结构而非刚性层级的特点启发，探索是否可以通过类似的智能计算单元实现人工智能。

Method: 引入智能神经网络（INN），其中神经元是一等实体，具有内部记忆和学习通信模式，组织成完全图而非顺序层。每个智能神经元结合选择性状态空间动态（知道何时激活）和基于注意力的路由（知道向谁发送信号），通过图结构交互实现涌现计算。

Result: 在Text8字符建模基准上，INN达到1.705 BPC，显著优于可比较的Transformer（2.055 BPC），匹配高度优化的LSTM基线。参数匹配的堆叠Mamba块基线在相同训练协议下无法收敛（>3.4 BPC），证明INN的图拓扑提供了必要的训练稳定性。消融研究证实：移除神经元间通信会降低性能或导致不稳定，证明了学习神经路由的价值。

Conclusion: 神经元中心设计结合图组织不仅是受生物启发的，而且在计算上是有效的，为模块化、可解释和可扩展的神经架构开辟了新方向。

Abstract: Biological neurons exhibit remarkable intelligence: they maintain internal states, communicate selectively with other neurons, and self-organize into complex graphs rather than rigid hierarchical layers. What if artificial intelligence could emerge from similarly intelligent computational units? We introduce Intelligent Neural Networks (INN), a paradigm shift where neurons are first-class entities with internal memory and learned communication patterns, organized in complete graphs rather than sequential layers.
  Each Intelligent Neuron combines selective state-space dynamics (knowing when to activate) with attention-based routing (knowing to whom to send signals), enabling emergent computation through graph-structured interactions. On the standard Text8 character modeling benchmark, INN achieves 1.705 Bit-Per-Character (BPC), significantly outperforming a comparable Transformer (2.055 BPC) and matching a highly optimized LSTM baseline. Crucially, a parameter-matched baseline of stacked Mamba blocks fails to converge (>3.4 BPC) under the same training protocol, demonstrating that INN's graph topology provides essential training stability. Ablation studies confirm this: removing inter-neuron communication degrades performance or leads to instability, proving the value of learned neural routing.
  This work demonstrates that neuron-centric design with graph organization is not merely bio-inspired -- it is computationally effective, opening new directions for modular, interpretable, and scalable neural architectures.

</details>


### [135] [A Unified and Stable Risk Minimization Framework for Weakly Supervised Learning with Theoretical Guarantees](https://arxiv.org/abs/2511.22823)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出一个统一的弱监督学习框架，通过直接构建稳定的代理风险来避免后处理调整，涵盖多种弱监督设置，并建立了理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督学习方法通常针对特定监督模式（如PU、UU、CLL、PLL等），需要后处理校正来缓解间接监督带来的不稳定性，缺乏统一的理论框架。

Method: 提出一个基于弱监督数据结构的稳定代理风险公式，统一了多种弱监督设置（PU、UU、CLL、PLL、多类无标签、基于元组的学习）到一个优化目标中，并通过Rademacher复杂度建立非渐近泛化界。

Result: 实验显示该方法在不同类别先验、数据集规模和类别数量下都取得一致性能提升，无需启发式稳定化，且对过拟合具有鲁棒性。理论分析量化了类别先验误设的影响，并给出了目标风险可恢复性的充分条件。

Conclusion: 该框架为弱监督学习提供了一个统一的理论和实践基础，能够处理多种监督模式，具有理论保证和实际性能优势。

Abstract: Weakly supervised learning has emerged as a practical alternative to fully supervised learning when complete and accurate labels are costly or infeasible to acquire. However, many existing methods are tailored to specific supervision patterns -- such as positive-unlabeled (PU), unlabeled-unlabeled (UU), complementary-label (CLL), partial-label (PLL), or similarity-unlabeled annotations -- and rely on post-hoc corrections to mitigate instability induced by indirect supervision. We propose a principled, unified framework that bypasses such post-hoc adjustments by directly formulating a stable surrogate risk grounded in the structure of weakly supervised data. The formulation naturally subsumes diverse settings -- including PU, UU, CLL, PLL, multi-class unlabeled, and tuple-based learning -- under a single optimization objective. We further establish a non-asymptotic generalization bound via Rademacher complexity that clarifies how supervision structure, model capacity, and sample size jointly govern performance. Beyond this, we analyze the effect of class-prior misspecification on the bound, deriving explicit terms that quantify its impact, and we study identifiability, giving sufficient conditions -- most notably via supervision stratification across groups -- under which the target risk is recoverable. Extensive experiments show consistent gains across class priors, dataset scales, and class counts -- without heuristic stabilization -- while exhibiting robustness to overfitting.

</details>


### [136] [CausalProfiler: Generating Synthetic Benchmarks for Rigorous and Transparent Evaluation of Causal Machine Learning](https://arxiv.org/abs/2511.22842)
*Panayiotis Panayiotou,Audrey Poinsot,Alessandro Leite,Nicolas Chesneau,Marc Schoenauer,Özgür Şimşek*

Main category: cs.LG

TL;DR: CausalProfiler是一个用于因果机器学习的随机合成基准生成器，通过随机采样因果模型、数据、查询和真实值来创建多样化基准，支持在观察、干预和反事实三个层面进行透明评估。


<details>
  <summary>Details</summary>
Motivation: 当前因果机器学习评估实践有限，现有基准通常依赖少量手工或半合成数据集，导致结论脆弱且不可泛化。需要更系统、透明的评估框架来填补这一空白。

Method: 提出CausalProfiler，一个基于明确设计选择的随机合成基准生成器。它通过随机采样因果模型、数据、查询和真实值来创建多样化的合成因果基准，覆盖观察、干预和反事实三个推理层面。

Result: CausalProfiler是首个具有覆盖保证和透明假设的随机合成因果基准生成器。通过评估多个最先进方法在不同条件和假设下的表现，展示了其分析能力和洞察力，包括在识别机制内外的情况。

Conclusion: CausalProfiler为因果机器学习方法提供了严格、透明的评估框架，能够生成多样化基准条件，促进更稳健、可泛化的因果推理方法发展。

Abstract: Causal machine learning (Causal ML) aims to answer "what if" questions using machine learning algorithms, making it a promising tool for high-stakes decision-making. Yet, empirical evaluation practices in Causal ML remain limited. Existing benchmarks often rely on a handful of hand-crafted or semi-synthetic datasets, leading to brittle, non-generalizable conclusions. To bridge this gap, we introduce CausalProfiler, a synthetic benchmark generator for Causal ML methods. Based on a set of explicit design choices about the class of causal models, queries, and data considered, the CausalProfiler randomly samples causal models, data, queries, and ground truths constituting the synthetic causal benchmarks. In this way, Causal ML methods can be rigorously and transparently evaluated under a variety of conditions. This work offers the first random generator of synthetic causal benchmarks with coverage guarantees and transparent assumptions operating on the three levels of causal reasoning: observation, intervention, and counterfactual. We demonstrate its utility by evaluating several state-of-the-art methods under diverse conditions and assumptions, both in and out of the identification regime, illustrating the types of analyses and insights the CausalProfiler enables.

</details>


### [137] [PerfMamba: Performance Analysis and Pruning of Selective State Space Models](https://arxiv.org/abs/2511.22849)
*Abdullah Al Asif,Mobina Kashaniyan,Sixing Yu,Juan Pablo Muñoz,Ali Jannesari*

Main category: cs.LG

TL;DR: 该论文对Mamba-1和Mamba-2选择性状态空间模型进行了系统性性能分析，发现SSM组件消耗大量计算资源，并提出基于状态活跃度的剪枝方法，在保持精度的同时实现1.14倍加速和11.50%内存减少。


<details>
  <summary>Details</summary>
Motivation: 选择性状态空间模型作为Transformer的替代方案具有理论计算效率优势，但其运行时行为、资源利用模式和扩展特性尚未得到充分理解，阻碍了最优部署和架构改进。

Method: 对Mamba-1和Mamba-2进行系统性性能分析，研究计算模式、内存访问、I/O特性和扩展特性（序列长度64-16384），并提出基于低活跃度状态剪枝的技术。

Result: SSM组件在Mamba块中消耗大量计算资源；提出的剪枝方法在不同序列长度下均能提升性能，实现1.14倍加速和11.50%内存减少，在适度剪枝范围内保持精度。

Conclusion: 该研究为设计更高效的状态空间模型架构提供了有价值的指导，可应用于广泛的现实应用场景，通过选择性剪枝优化资源利用。

Abstract: Recent advances in sequence modeling have introduced selective SSMs as promising alternatives to Transformer architectures, offering theoretical computational efficiency and sequence processing advantages. A comprehensive understanding of selective SSMs in runtime behavior, resource utilization patterns, and scaling characteristics still remains unexplored, thus obstructing their optimal deployment and further architectural improvements. This paper presents a thorough empirical study of Mamba-1 and Mamba-2, systematically profiled for performance to assess the design principles that contribute to their efficiency in state-space modeling. A detailed analysis of computation patterns, memory access, I/O characteristics, and scaling properties was performed for sequence lengths ranging from 64 to 16384 tokens. Our findings show that the SSM component, a central part of the selective SSM architecture, demands a significant portion of computational resources compared to other components in the Mamba block. Based on these insights, we propose a pruning technique that selectively removes low-activity states within the SSM component, achieving measurable throughput and memory gains while maintaining accuracy within a moderate pruning regime. This approach results in performance improvements across varying sequence lengths, achieving a 1.14x speedup and reducing memory usage by 11.50\%. These results offer valuable guidance for designing more efficient SSM architectures that can be applied to a wide range of real-world applications.

</details>


### [138] [TARFVAE: Efficient One-Step Generative Time Series Forecasting via TARFLOW based VAE](https://arxiv.org/abs/2511.22853)
*Jiawen Wei,Lan Jiang,Pengbo Wei,Ziwen Ye,Teng Song,Chen Chen,Guangrui Ma*

Main category: cs.LG

TL;DR: TARFVAE是一个结合Transformer自回归流和变分自编码器的高效一步生成式时间序列预测框架，在保持快速预测的同时，在不同预测时域上超越了现有确定性方法和生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有生成式时间序列预测方法大多涉及循环生成操作或重复去噪步骤，预测过程繁琐，尤其对于长期预测。这些方法主要在短期预测上进行实验，与确定性方法在长期预测上的比较有限，实际优势不明确。

Method: 提出TARFVAE框架，结合Transformer自回归流(TARFLOW)和变分自编码器(VAE)。TARFLOW模块增强VAE的后验估计，打破高斯假设，实现更信息丰富的潜在空间。仅使用TARFLOW的前向过程，避免自回归逆操作，确保快速生成。从先验潜在空间采样，通过VAE解码器直接生成全时域预测。

Result: 在基准数据集上，TARFVAE在不同预测时域上超越了最先进的确定性和生成模型，同时保持了高效的预测速度。仅使用简单的MLP模块就实现了优越性能。

Conclusion: TARFVAE为生成式时间序列预测提供了一个高效而强大的解决方案，通过结合TARFLOW和VAE，实现了快速、准确的预测，特别适合长期预测任务。

Abstract: Time series data is ubiquitous, with forecasting applications spanning from finance to healthcare. Beyond popular deterministic methods, generative models are gaining attention due to advancements in areas like image synthesis and video generation, as well as their inherent ability to provide probabilistic predictions. However, existing generative approaches mostly involve recurrent generative operations or repeated denoising steps, making the prediction laborious, particularly for long-term forecasting. Most of them only conduct experiments for relatively short-term forecasting, with limited comparison to deterministic methods in long-term forecasting, leaving their practical advantages unclear. This paper presents TARFVAE, a novel generative framework that combines the Transformer-based autoregressive flow (TARFLOW) and variational autoencoder (VAE) for efficient one-step generative time series forecasting. Inspired by the rethinking that complex architectures for extracting time series representations might not be necessary, we add a flow module, TARFLOW, to VAE to promote spontaneous learning of latent variables that benefit predictions. TARFLOW enhances VAE's posterior estimation by breaking the Gaussian assumption, thereby enabling a more informative latent space. TARFVAE uses only the forward process of TARFLOW, avoiding autoregressive inverse operations and thus ensuring fast generation. During generation, it samples from the prior latent space and directly generates full-horizon forecasts via the VAE decoder. With simple MLP modules, TARFVAE achieves superior performance over state-of-the-art deterministic and generative models across different forecast horizons on benchmark datasets while maintaining efficient prediction speed, demonstrating its effectiveness as an efficient and powerful solution for generative time series forecasting.

</details>


### [139] [CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate](https://arxiv.org/abs/2511.22854)
*Finn G. Vamosi,Nils D. Forkert*

Main category: cs.LG

TL;DR: 提出双智能体辩论框架，通过模型间的对话式辩论提升因果推理准确性，在CLadder数据集上显著改进DeepSeek-R1和Qwen3的因果推理性能


<details>
  <summary>Details</summary>
Motivation: 人类因果推理常涉及多个"假设"场景的对比分析，类似地，语言模型可通过考虑多种干预和反事实来评估因果主张。当前因果推理更像内部对话而非单一计算，需要更明确的辩论框架来提升推理质量

Method: 采用双智能体辩论框架：一个模型提供结构化因果推理，另一个模型批判性审查逻辑缺陷。当出现分歧时，智能体尝试说服对方，挑战彼此逻辑并修订结论，直至达成共识。特别使用推理语言模型，利用其在因果推理和对抗辩论方面的优势

Result: 在CLadder数据集（涵盖Pearl因果阶梯所有三个层级）上评估：DeepSeek-R1整体准确率从78.03%提升至87.45%，反事实类别从67.94%提升至80.04%；Qwen3整体准确率从84.16%提升至89.41%，反事实问题从71.53%提升至80.35%。强模型仍能从与弱智能体的辩论中显著受益

Conclusion: 推理模型可作为多智能体系统在因果推理中的构建模块，展示了多样化视角在因果问题解决中的重要性。辩论框架能显著提升因果推理性能，特别是对反事实推理的改进

Abstract: When people reason about cause and effect, they often consider many competing "what if" scenarios before deciding which explanation fits best. Analogously, advanced language models capable of causal inference can consider multiple interventions and counterfactuals to judge the validity of causal claims. Crucially, this type of reasoning is less like a single calculation and more like an internal dialogue between alternative hypotheses. In this paper, we make this dialogue explicit through a dual-agent debate framework where one model provides a structured causal inference, and the other critically examines this reasoning for logical flaws. When disagreements arise, agents attempt to persuade each other, challenging each other's logic and revising their conclusions until they converge on a mutually agreed answer. To take advantage of this deliberative process, we specifically use reasoning language models, whose strengths in both causal inference and adversarial debate remain under-explored relative to standard large language models. We evaluate our approach on the CLadder dataset, a benchmark linking natural language questions to formally defined causal graphs across all three rungs of Pearl's ladder of causation. With Qwen3 and DeepSeek-R1 as debater agents, we demonstrate that multi-agent debate improves DeepSeek-R1's overall accuracy in causal inference from 78.03% to 87.45%, with the counterfactual category specifically improving from 67.94% to 80.04% accuracy. Similarly, Qwen3's overall accuracy improves from 84.16% to 89.41%, and counterfactual questions from 71.53% to 80.35%, showing that strong models can still benefit greatly from debate with weaker agents. Our results highlight the potential of reasoning models as building blocks for multi-agent systems in causal inference, and demonstrate the importance of diverse perspectives in causal problem-solving.

</details>


### [140] [Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation](https://arxiv.org/abs/2511.22862)
*Jiacheng Li,Songhe Feng*

Main category: cs.LG

TL;DR: 本文提出了一种名为BriMPR的多模态测试时自适应框架，通过渐进式重新对齐来解决多模态场景中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 在多模态场景中，不同模态的分布偏移程度不同，导致单模态浅层特征偏移和跨模态高级语义错位的复杂耦合效应，这阻碍了现有TTA方法在多模态领域的应用。

Method: 提出BriMPR框架，包含两个渐进增强模块：1) 使用提示调优校准单模态全局特征分布，实现初始语义重新对齐；2) 为掩码和完整模态组合分配可信伪标签，引入模态间实例级对比学习增强信息交互和细化对齐。

Result: 在包括基于损坏和真实世界域偏移基准的MMTTA任务上进行了广泛实验，证明了该方法的优越性。

Conclusion: BriMPR通过分而治之策略有效解决了多模态测试时自适应中的耦合效应问题，实现了模态间的渐进式重新对齐。

Abstract: Test-time adaptation (TTA) enables online model adaptation using only unlabeled test data, aiming to bridge the gap between source and target distributions. However, in multimodal scenarios, varying degrees of distribution shift across different modalities give rise to a complex coupling effect of unimodal shallow feature shift and cross-modal high-level semantic misalignment, posing a major obstacle to extending existing TTA methods to the multimodal field. To address this challenge, we propose a novel multimodal test-time adaptation (MMTTA) framework, termed as Bridging Modalities via Progressive Re-alignment (BriMPR). BriMPR, consisting of two progressively enhanced modules, tackles the coupling effect with a divide-and-conquer strategy. Specifically, we first decompose MMTTA into multiple unimodal feature alignment sub-problems. By leveraging the strong function approximation ability of prompt tuning, we calibrate the unimodal global feature distributions to their respective source distributions, so as to achieve the initial semantic re-alignment across modalities. Subsequently, we assign the credible pseudo-labels to combinations of masked and complete modalities, and introduce inter-modal instance-wise contrastive learning to further enhance the information interaction among modalities and refine the alignment. Extensive experiments on MMTTA tasks, including both corruption-based and real-world domain shift benchmarks, demonstrate the superiority of our method. Our source code is available at [this URL](https://github.com/Luchicken/BriMPR).

</details>


### [141] [ARM-Explainer -- Explaining and improving graph neural network predictions for the maximum clique problem using node features and association rule mining](https://arxiv.org/abs/2511.22866)
*Bharat Sharman,Elkafi Hassini*

Main category: cs.LG

TL;DR: ARM-Explainer是一种基于关联规则挖掘的后处理模型级解释器，用于解释图神经网络在组合优化问题中的预测，并在最大团问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然已有许多基于图神经网络的算法用于解决图组合优化问题，但这些模型的预测解释方法仍然缺乏开发。需要一种能够解释GNN预测的方法来增强模型的可解释性。

Method: 提出ARM-Explainer，一种基于关联规则挖掘的后处理模型级解释器。该方法应用于混合几何散射GNN在最大团问题上的预测，通过挖掘关联规则来识别影响预测的重要节点特征及其取值范围。

Result: 在TWITTER和BHOSLIB-DIMACS基准数据集上，ARM-Explainer发现的8个最具解释性的关联规则获得了中位数提升度2.42和置信度0.49。通过增强信息性节点特征，GNN在BHOSLIB-DIMACS数据集上的最大团大小中位数提高了22%（从29.5到36）。

Conclusion: ARM-Explainer能够有效解释GNN在组合优化问题中的预测，识别关键节点特征，并且这些解释性特征可以反过来提升GNN的性能，实现了可解释性与性能提升的双重目标。

Abstract: Numerous graph neural network (GNN)-based algorithms have been proposed to solve graph-based combinatorial optimization problems (COPs), but methods to explain their predictions remain largely undeveloped. We introduce ARM-Explainer, a post-hoc, model-level explainer based on association rule mining, and demonstrate it on the predictions of the hybrid geometric scattering (HGS) GNN for the maximum clique problem (MCP), a canonical NP-hard graph-based COP. The eight most explanatory association rules discovered by ARM-Explainer achieve high median lift and confidence values of 2.42 and 0.49, respectively, on test instances from the TWITTER and BHOSLIB-DIMACS benchmark datasets. ARM-Explainer identifies the most important node features, together with their value ranges, that influence the GNN's predictions on these datasets. Furthermore, augmenting the GNN with informative node features substantially improves its performance on the MCP, increasing the median largest-found clique size by 22% (from 29.5 to 36) on large graphs from the BHOSLIB-DIMACS dataset.

</details>


### [142] [Covering-Space Normalizing Flows: Approximating Pushforwards on Lens Spaces](https://arxiv.org/abs/2511.22882)
*William Ghanem*

Main category: cs.LG

TL;DR: 构建S^3到透镜空间L(p;q)的推前分布，通过流方法近似这些分布，在对称S^3分布时消除冗余，并应用于von Mises-Fisher诱导的目标密度和苯分子建模的Z_12对称玻尔兹曼分布


<details>
  <summary>Details</summary>
Motivation: 研究如何通过通用覆盖映射从S^3到透镜空间L(p;q)构建推前分布，并寻找有效近似这些分布的方法，特别关注对称分布情况下的冗余消除

Method: 使用通用覆盖映射ρ: S^3 → L(p;q)构建推前分布，通过在L(p;q)上的流方法近似这些分布，特别处理对称S^3分布时的冗余问题

Result: 成功近似了von Mises-Fisher诱导的目标密度的推前分布，以及为苯分子建模构建的Z_12对称玻尔兹曼分布在S^3上的推前分布

Conclusion: 提出的方法能够有效构建和近似从S^3到透镜空间的推前分布，在对称分布情况下能消除冗余，适用于复杂分子系统的建模

Abstract: We construct pushforward distributions via the universal covering map rho: S^3 -> L(p;q) with the goal of approximating these distributions using flows on L(p;q). We highlight that our method deletes redundancies in the case of a symmetric S^3 distribution. Using our model, we approximate the pushforwards of von Mises-Fisher-induced target densities as well as that of a Z_12-symmetric Boltzmann distribution on S^3 constructed to model benzene.

</details>


### [143] [Modeling Chaotic Pedestrian Behavior Using Chaos Indicators and Supervised Learning](https://arxiv.org/abs/2511.22887)
*Md. Muhtashim Shahrier,Nazmul Haque,Md Asif Raihan,Md. Hadiuzzaman*

Main category: cs.LG

TL;DR: 提出数据驱动框架，使用监督学习建模行人混沌运动，通过混沌指标量化行为不稳定性，识别高风险区域并支持自动驾驶系统风险评估


<details>
  <summary>Details</summary>
Motivation: 随着城市改善步行性和安全性需求增加，理解行人行为的不规则和不可预测性变得重要，需要量化行人混沌运动以支持规划和自动驾驶系统

Method: 使用计算机视觉提取行人轨迹，计算近似熵和李雅普诺夫指数等四种混沌指标，通过PCA整合为统一混沌分数，使用随机森林和CatBoost回归模型训练个体、群体和交通特征

Result: CatBoost模型表现最佳，白天PCA模型R²达0.8319，夜间达0.8574；SHAP分析显示行程距离、移动时长和速度变异性是混沌行为主要贡献因素

Conclusion: 该框架能有效量化和预测行人行为不稳定性，支持规划者识别高风险区域、改进基础设施、校准微观仿真模型，并为自动驾驶系统提供自适应风险评估

Abstract: As cities around the world aim to improve walkability and safety, understanding the irregular and unpredictable nature of pedestrian behavior has become increasingly important. This study introduces a data-driven framework for modeling chaotic pedestrian movement using empirically observed trajectory data and supervised learning. Videos were recorded during both daytime and nighttime conditions to capture pedestrian dynamics under varying ambient and traffic contexts. Pedestrian trajectories were extracted through computer vision techniques, and behavioral chaos was quantified using four chaos metrics: Approximate Entropy and Lyapunov Exponent, each computed for both velocity and direction change. A Principal Component Analysis (PCA) was then applied to consolidate these indicators into a unified chaos score. A comprehensive set of individual, group-level, and contextual traffic features was engineered and used to train Random Forest and CatBoost regression models. CatBoost models consistently achieved superior performance. The best daytime PCA-based CatBoost model reached an R^2 of 0.8319, while the nighttime PCA-based CatBoost model attained an R^2 of 0.8574. SHAP analysis highlighted that features such as distance travel, movement duration, and speed variability were robust contributors to chaotic behavior. The proposed framework enables practitioners to quantify and anticipate behavioral instability in real-world settings. Planners and engineers can use chaos scores to identify high-risk pedestrian zones, apprise infrastructure improvements, and calibrate realistic microsimulation models. The approach also supports adaptive risk assessment in automated vehicle systems by capturing short-term motion unpredictability grounded in observable, interpretable features.

</details>


### [144] [Adversarial Training for Process Reward Models](https://arxiv.org/abs/2511.22888)
*Gurusha Juneja,Deepak Nathani,William Yang Wang*

Main category: cs.LG

TL;DR: APRM通过对抗训练提升过程奖励模型，无需人工标注即可生成更难的反例，提高模型对推理错误的检测能力，在数学推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前过程奖励模型（PRMs）面临两个主要限制：1）需要昂贵的人工步骤级标注；2）静态训练数据对新型错误的泛化能力差。这阻碍了PRMs的广泛应用。

Method: 提出对抗训练的过程奖励模型（APRM），包含生成器（G）和判别器（R）。G学习生成欺骗R的推理错误，R同时学习检测这些错误。这种对抗交互为R提供渐进式更难的反例，无需人工标注。

Result: 在多样化的数学推理基准测试中，APRM平均比最强的PRM基线提升3.4个百分点。在分布外任务上获得5.3个百分点的提升，显示出更好的泛化能力。

Conclusion: APRM通过对抗训练有效解决了PRMs的两个主要限制，无需人工标注即可提高模型对推理错误的检测鲁棒性和泛化能力，在数学推理任务上取得显著改进。

Abstract: Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.

</details>


### [145] [EnECG: Efficient Ensemble Learning for Electrocardiogram Multi-task Foundation Model](https://arxiv.org/abs/2511.22935)
*Yuhao Xu,Xiaoda Wang,Jiaying Lu,Sirui Ding,Defu Cao,Huaxiu Yao,Yan Liu,Xiao Hu,Carl Yang*

Main category: cs.LG

TL;DR: EnECG是一个基于专家混合的集成学习框架，专门用于ECG多任务分析，通过轻量级适配策略结合多个专业基础模型，降低计算成本同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有ECG分析模型未能充分利用各种心脏异常之间的相互关系，而开发一个能提取所有相关特征的多任务模型又很困难。大型基础模型虽然强大，但通常未在ECG数据上预训练，完全重新训练或微调计算成本高昂。

Method: 提出EnECG框架：1) 集成多个专业基础模型，每个擅长不同ECG解释方面；2) 采用轻量级适配策略：为每个基础模型添加专用输出层，仅对新参数应用低秩适配(LoRA)；3) 使用专家混合(MoE)机制学习集成权重，有效结合各模型的互补专业知识。

Result: 实验结果表明，通过最小化微调范围，EnECG能帮助降低计算和内存成本，同时保持基础模型的强大表示能力。该框架不仅增强了特征提取和预测性能，还确保了实际临床应用的效率。

Conclusion: EnECG通过集成多个专业基础模型和轻量级适配策略，为ECG多任务分析提供了一个高效实用的解决方案，平衡了性能与计算效率，适合实际临床应用。

Abstract: Electrocardiogram (ECG) analysis plays a vital role in the early detection, monitoring, and management of various cardiovascular conditions. While existing models have achieved notable success in ECG interpretation, they fail to leverage the interrelated nature of various cardiac abnormalities. Conversely, developing a specific model capable of extracting all relevant features for multiple ECG tasks remains a significant challenge. Large-scale foundation models, though powerful, are not typically pretrained on ECG data, making full re-training or fine-tuning computationally expensive. To address these challenges, we propose EnECG(Mixture of Experts-based Ensemble Learning for ECG Multi-tasks), an ensemble-based framework that integrates multiple specialized foundation models, each excelling in different aspects of ECG interpretation. Instead of relying on a single model or single task, EnECG leverages the strengths of multiple specialized models to tackle a variety of ECG-based tasks. To mitigate the high computational cost of full re-training or fine-tuning, we introduce a lightweight adaptation strategy: attaching dedicated output layers to each foundation model and applying Low-Rank Adaptation (LoRA) only to these newly added parameters. We then adopt a Mixture of Experts (MoE) mechanism to learn ensemble weights, effectively combining the complementary expertise of individual models. Our experimental results demonstrate that by minimizing the scope of fine-tuning, EnECG can help reduce computational and memory costs while maintaining the strong representational power of foundation models. This framework not only enhances feature extraction and predictive performance but also ensures practical efficiency for real-world clinical applications. The code is available at https://github.com/yuhaoxu99/EnECG.git.

</details>


### [146] [CORGI: GNNs with Convolutional Residual Global Interactions for Lagrangian Simulation](https://arxiv.org/abs/2511.22938)
*Ethan Ji,Yuanzhou Chen,Arush Ramteke,Fang Sun,Tianrun Yu,Jai Parera,Wei Wang,Yizhou Sun*

Main category: cs.LG

TL;DR: CORGI：一种混合架构，通过将粒子特征投影到网格上并使用卷积更新来增强GNN求解器，以捕捉流体流动中的全局相互作用，显著提高精度而计算开销很小。


<details>
  <summary>Details</summary>
Motivation: 传统PDE求解器在处理非线性流体动力学问题时计算成本高，而现有的拉格朗日神经代理模型（如GNS和SEGNN）感受野有限，难以捕捉流体流动中固有的全局相互作用。

Method: 提出CORGI混合架构，将任何基于GNN的求解器与轻量级欧拉组件结合：1) 将粒子特征投影到网格上；2) 应用卷积更新捕捉全局上下文；3) 将更新后的特征映射回粒子域。

Result: 在GNS骨干网络上，CORGI将推演精度提高57%，推理时间仅增加13%，训练时间增加31%。相比SEGNN，精度提高49%，推理时间减少48%，训练时间减少30%。在相同运行时约束下，CORGI平均优于GNS 47%。

Conclusion: CORGI通过引入轻量级欧拉组件有效捕捉长程依赖关系，显著提升基于GNN的流体求解器性能，在不同计算预算下都表现出优异的性能和通用性。

Abstract: Partial differential equations (PDEs) are central to dynamical systems modeling, particularly in hydrodynamics, where traditional solvers often struggle with nonlinearity and computational cost. Lagrangian neural surrogates such as GNS and SEGNN have emerged as strong alternatives by learning from particle-based simulations. However, these models typically operate with limited receptive fields, making them inaccurate for capturing the inherently global interactions in fluid flows. Motivated by this observation, we introduce Convolutional Residual Global Interactions (CORGI), a hybrid architecture that augments any GNN-based solver with a lightweight Eulerian component for global context aggregation. By projecting particle features onto a grid, applying convolutional updates, and mapping them back to the particle domain, CORGI captures long-range dependencies without significant overhead. When applied to a GNS backbone, CORGI achieves a 57% improvement in rollout accuracy with only 13% more inference time and 31% more training time. Compared to SEGNN, CORGI improves accuracy by 49% while reducing inference time by 48% and training time by 30%. Even under identical runtime constraints, CORGI outperforms GNS by 47% on average, highlighting its versatility and performance on varied compute budgets.

</details>


### [147] [Bandit Guided Submodular Curriculum for Adaptive Subset Selection](https://arxiv.org/abs/2511.22944)
*Prateek Chanda,Prayas Agrawal,Saral Sureka,Lokesh Reddy Polu,Atharv Kshirsagar,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: 论文提出ONLINESUBMOD方法，将自适应子集选择重新解释为多臂老虎机问题，每个臂对应一个引导样本选择的子模函数，通过在线贪心策略优化效用驱动的奖励，在多种采样机制下实现无遗憾性能。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习从简单到困难样本进行，但定义可靠的难度概念仍然困难。先前工作使用子模函数在课程学习中诱导难度分数，需要更系统的方法来指导课程进度安排。

Method: 将自适应子集选择重新表述为多臂老虎机问题，每个臂对应一个子模函数来指导样本选择。提出ONLINESUBMOD在线贪心策略，优化效用驱动的奖励，在多种采样机制下实现无遗憾性能。

Result: ONLINESUBMOD在视觉和语言数据集上优于传统课程学习和双层优化方法，显示出更优的准确率-效率权衡。验证驱动的奖励指标为课程进度安排提供了原则性指导。

Conclusion: 通过将课程学习重新解释为多臂老虎机问题并引入ONLINESUBMOD在线贪心策略，提供了一种系统的方法来指导课程进度安排，验证驱动的奖励指标为自适应样本选择提供了原则性框架。

Abstract: Traditional curriculum learning proceeds from easy to hard samples, yet defining a reliable notion of difficulty remains elusive. Prior work has used submodular functions to induce difficulty scores in curriculum learning. We reinterpret adaptive subset selection and formulate it as a multi-armed bandit problem, where each arm corresponds to a submodular function guiding sample selection. We introduce ONLINESUBMOD, a novel online greedy policy that optimizes a utility-driven reward and provably achieves no-regret performance under various sampling regimes. Empirically, ONLINESUBMOD outperforms both traditional curriculum learning and bi-level optimization approaches across vision and language datasets, showing superior accuracy-efficiency tradeoffs. More broadly, we show that validationdriven reward metrics offer a principled way to guide the curriculum schedule.

</details>


### [148] [Experts are all you need: A Composable Framework for Large Language Model Inference](https://arxiv.org/abs/2511.22955)
*Shrihari Sridharan,Sourjya Roy,Anand Raghunathan,Kaushik Roy*

Main category: cs.LG

TL;DR: Comp-LLM是一个可组合推理框架，通过子查询依赖图实现专家协作，在保持准确性的同时减少模型大小和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：MoE模型需要联合预训练且不支持多步推理；多智能体框架依赖顺序循环导致高延迟。需要一种既能实现跨专家协作又能减少延迟的推理框架。

Method: Comp-LLM包含三个组件：1) 子查询生成器分解输入查询，基于嵌入相似度分配专家，构建依赖图；2) 查询执行器处理图节点，基于依赖和资源约束识别并行机会；3) 响应聚合器合成专家中间响应为最终答案。

Result: 在多个基准测试中，Comp-LLM相比相似大小的单体LLM准确率提升达11.01%，模型大小减少1.67-3.56倍，相对于最大模型无显著性能下降，延迟比顺序子查询处理提升1.1-1.7倍。

Conclusion: Comp-LLM通过可组合推理框架有效解决了现有方法的局限性，在准确性、模型效率和推理延迟方面取得了显著改进，为高效LLM推理提供了新方向。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or "experts". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential "plan--act--observe" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.

</details>


### [149] [A Modular Framework for Rapidly Building Intrusion Predictors](https://arxiv.org/abs/2511.23000)
*Xiaoxuan Wang,Rolf Stadler*

Main category: cs.LG

TL;DR: 提出模块化框架，通过可复用组件快速组装在线攻击预测器，解决传统单一预测器无法应对多种攻击类型的问题


<details>
  <summary>Details</summary>
Motivation: 现有攻击预测器通常针对特定攻击类型设计，而MITRE框架包含数百种攻击类型，为每种类型训练单独的预测器不可行

Method: 提出模块化框架，使用可复用组件动态组装在线攻击预测器，支持控制预测及时性和准确性等关键指标

Result: 使用公开数据集训练和评估，展示了多种模块化预测器示例，证明可以在训练期间从模块化组件网络中动态组装有效预测器

Conclusion: 模块化框架能够快速构建在线攻击预测器，平衡预测及时性和准确性，解决传统单一预测器扩展性问题

Abstract: We study automated intrusion prediction in an IT system using statistical learning methods. The focus is on developing online attack predictors that detect attacks in real time and identify the current stage of the attack. While such predictors have been proposed in the recent literature, these works typically rely on constructing a monolithic predictor tailored to a specific attack type and scenario. Given that hundreds of attack types are cataloged in the MITRE framework, training a separate monolithic predictor for each of them is infeasible. In this paper, we propose a modular framework for rapidly assembling online attack predictors from reusable components. The modular nature of a predictor facilitates controlling key metrics like timeliness and accuracy of prediction, as well as tuning the trade-off between them. Using public datasets for training and evaluation, we provide many examples of modular predictors and show how an effective predictor can be dynamically assembled during training from a network of modular components.

</details>


### [150] [Masked Diffusion for Generative Recommendation](https://arxiv.org/abs/2511.23021)
*Kulin Shah,Bhuvesh Kumar,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 本文提出了一种基于掩码扩散的生成式推荐方法，替代传统的自回归建模，通过并行解码提高推理效率，在数据受限场景下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义ID的自回归生成式推荐方法存在推理成本高、训练数据利用效率低、偏向学习短上下文关系等问题。受NLP领域突破启发，作者希望采用掩码扩散模型来改进这些问题。

Method: 提出使用掩码扩散模型来建模用户语义ID序列的概率分布。该方法使用离散掩码噪声来学习序列分布，将掩码标记的概率建模为在给定未掩码标记条件下的独立分布，从而实现掩码标记的并行解码。

Result: 实验表明，该方法在性能上持续优于自回归建模，特别是在数据受限场景和粗粒度召回指标上表现更突出。同时保持了并行预测多个语义ID的灵活性。

Conclusion: 掩码扩散模型为生成式推荐提供了一种有效的替代方案，解决了自回归方法的局限性，在推理效率、数据利用和性能方面都有显著改进。

Abstract: Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.

</details>


### [151] [Delta-XAI: A Unified Framework for Explaining Prediction Changes in Online Time Series Monitoring](https://arxiv.org/abs/2511.23036)
*Changhun Kim,Yechan Mun,Hyeongwon Jang,Eunseo Lee,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: Delta-XAI 是一个用于在线时间序列监控模型解释的框架，通过包装现有XAI方法并引入系统性评估套件，提出了SWING方法来更好地捕捉时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列模型的解释方法大多独立分析每个时间步，忽略了时间依赖性，导致解释预测变化困难、无法利用在线动态、评估困难等问题，在医疗和金融等敏感领域尤为重要。

Method: 提出Delta-XAI框架：1) 通过包装函数适配14种现有XAI方法；2) 引入在线场景下的原则性评估套件，评估忠实性、充分性和一致性等方面；3) 基于实验结果提出SWING方法，通过在积分路径中纳入过去观测来系统捕捉时间依赖性和缓解分布外效应。

Result: 实验表明，经典梯度方法（如集成梯度）在适应时间分析时能超越最新方法。SWING方法在多样设置和多种指标下都表现出持续的有效性。

Conclusion: Delta-XAI框架解决了在线时间序列解释的关键挑战，SWING方法通过纳入时间依赖性显著提升了解释质量，为敏感领域的决策提供了更好的可解释性支持。

Abstract: Explaining online time series monitoring models is crucial across sensitive domains such as healthcare and finance, where temporal and contextual prediction dynamics underpin critical decisions. While recent XAI methods have improved the explainability of time series models, they mostly analyze each time step independently, overlooking temporal dependencies. This results in further challenges: explaining prediction changes is non-trivial, methods fail to leverage online dynamics, and evaluation remains difficult. To address these challenges, we propose Delta-XAI, which adapts 14 existing XAI methods through a wrapper function and introduces a principled evaluation suite for the online setting, assessing diverse aspects, such as faithfulness, sufficiency, and coherence. Experiments reveal that classical gradient-based methods, such as Integrated Gradients (IG), can outperform recent approaches when adapted for temporal analysis. Building on this, we propose Shifted Window Integrated Gradients (SWING), which incorporates past observations in the integration path to systematically capture temporal dependencies and mitigate out-of-distribution effects. Extensive experiments consistently demonstrate the effectiveness of SWING across diverse settings with respect to diverse metrics. Our code is publicly available at https://anonymous.4open.science/r/Delta-XAI.

</details>


### [152] [Freeze, Diffuse, Decode: Geometry-Aware Adaptation of Pretrained Transformer Embeddings for Antimicrobial Peptide Design](https://arxiv.org/abs/2511.23120)
*Pankhil Gawade,Adam Izdebski,Myriam Lizotte,Kevin R. Moon,Jake S. Rhodes,Guy Wolf,Ewa Szczurek*

Main category: cs.LG

TL;DR: FDD是一个基于扩散的框架，通过冻结预训练嵌入、扩散监督信号和解码，在适应下游任务时保持嵌入的几何结构


<details>
  <summary>Details</summary>
Motivation: 当前迁移策略（微调和探测）要么扭曲预训练嵌入的几何结构，要么缺乏足够的表达能力来捕捉任务相关信号，这在监督数据稀缺时尤为明显

Method: FDD框架：冻结预训练嵌入，沿着冻结嵌入的内在流形传播监督信号，实现几何感知的嵌入空间适应

Result: 应用于抗菌肽设计时，FDD产生低维、可预测且可解释的表征，支持属性预测、检索和潜在空间插值

Conclusion: FDD提供了一种新颖的扩散框架，能够在适应下游任务时保持预训练嵌入的几何结构，特别适用于监督数据稀缺的场景

Abstract: Pretrained transformers provide rich, general-purpose embeddings, which are transferred to downstream tasks. However, current transfer strategies: fine-tuning and probing, either distort the pretrained geometric structure of the embeddings or lack sufficient expressivity to capture task-relevant signals. These issues become even more pronounced when supervised data are scarce. Here, we introduce Freeze, Diffuse, Decode (FDD), a novel diffusion-based framework that adapts pre-trained embeddings to downstream tasks while preserving their underlying geometric structure. FDD propagates supervised signal along the intrinsic manifold of frozen embeddings, enabling a geometry-aware adaptation of the embedding space. Applied to antimicrobial peptide design, FDD yields low-dimensional, predictive, and interpretable representations that support property prediction, retrieval, and latent-space interpolation.

</details>


### [153] [Adapting Neural Audio Codecs to EEG](https://arxiv.org/abs/2511.23142)
*Ard Kastrati,Luca Lanzendörfer,Riccardo Rigoni,John Staib Matilla,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 预训练的神经音频编解码器经过适配后可用于EEG压缩，通过数据预处理使其适应音频编解码器的输入约束，并在多通道扩展中保持音频预训练初始化。


<details>
  <summary>Details</summary>
Motivation: EEG和音频是本质上不同的模态，具有不同的采样率、通道结构和尺度。然而，作者发现预训练的神经音频编解码器可以作为EEG压缩的有效起点，前提是数据经过预处理以适应编解码器的输入约束。

Method: 使用最先进的神经音频编解码器DAC作为基础，将原始EEG映射到编解码器的基于步长的帧结构中，直接重用音频预训练的编码器-解码器。提出了DAC-MC多通道扩展，具有基于注意力的跨通道聚合和通道特定解码，同时保留音频预训练初始化。

Result: 即使没有修改，该设置也能产生稳定的EEG重建，在EEG数据上进行微调进一步提高了保真度和泛化能力。在TUH异常和癫痫数据集上的评估显示，适配的编解码器保留了临床相关信息，反映在基于频谱图的重建损失和下游分类准确性上。

Conclusion: 预训练的神经音频编解码器可以有效地适应EEG压缩任务，通过适当的预处理和架构扩展，能够保留临床相关信息并实现良好的压缩质量权衡。

Abstract: EEG and audio are inherently distinct modalities, differing in sampling rate, channel structure, and scale. Yet, we show that pretrained neural audio codecs can serve as effective starting points for EEG compression, provided that the data are preprocessed to be suitable to the codec's input constraints. Using DAC, a state-of-the-art neural audio codec as our base, we demonstrate that raw EEG can be mapped into the codec's stride-based framing, enabling direct reuse of the audio-pretrained encoder-decoder. Even without modification, this setup yields stable EEG reconstructions, and fine-tuning on EEG data further improves fidelity and generalization compared to training from scratch. We systematically explore compression-quality trade-offs by varying residual codebook depth, codebook (vocabulary) size, and input sampling rate. To capture spatial dependencies across electrodes, we propose DAC-MC, a multi-channel extension with attention-based cross-channel aggregation and channel-specific decoding, while retaining the audio-pretrained initialization. Evaluations on the TUH Abnormal and Epilepsy datasets show that the adapted codecs preserve clinically relevant information, as reflected in spectrogram-based reconstruction loss and downstream classification accuracy.

</details>


### [154] [A Theoretical Framework for Discovering Groups and Unitary Representations via Tensor Factorization](https://arxiv.org/abs/2511.23152)
*Dongsung Huh,Halyun Jeong*

Main category: cs.LG

TL;DR: HyperCube模型通过算子值张量分解发现群结构及其酉表示，理论分析表明其归纳偏置倾向于群结构（直至同构）


<details>
  <summary>Details</summary>
Motivation: 理解HyperCube模型为何能够自动发现群结构和酉表示的机制，从理论上解释其内在的归纳偏置

Method: 将目标函数分解为尺度调节项(ℬ)和方向对齐项(ℛ≥0)，分析共线流形(ℛ=0)，提出共线性主导猜想，并证明关键理论结果

Result: 证明共线流形仅对群同构存在可行解，且在该流形内ℬ项对酉性施加变分压力；在共线性主导条件下，全局最小值由群的酉正则表示实现，非群操作具有更高的目标值

Conclusion: HyperCube模型具有强烈的归纳偏置，倾向于发现群结构（直至同构），这为理解其自动发现代数结构的能力提供了理论解释

Abstract: We analyze the HyperCube model, an \textit{operator-valued} tensor factorization architecture that discovers group structures and their unitary representations. We provide a rigorous theoretical explanation for this inductive bias by decomposing its objective into a term regulating factor scales ($\mathcal{B}$) and a term enforcing directional alignment ($\mathcal{R} \geq 0$). This decomposition isolates the \textit{collinear manifold} ($\mathcal{R}=0$), to which numerical optimization consistently converges for group isotopes. We prove that this manifold admits feasible solutions exclusively for group isotopes, and that within it, $\mathcal{B}$ exerts a variational pressure toward unitarity. To bridge the gap to the global landscape, we formulate a \textit{Collinearity Dominance Conjecture}, supported by empirical observations. Conditional on this dominance, we prove two key results: (1) the global minimum is achieved by the unitary regular representation for groups, and (2) non-group operations incur a strictly higher objective value, formally quantifying the model's inductive bias toward the associative structure of groups (up to isotopy).

</details>


### [155] [Estimating the Event-Related Potential from Few EEG Trials](https://arxiv.org/abs/2511.23162)
*Anders Vestergaard Nørskov,Kasper Jørgensen,Alexander Neergaard Zahid,Morten Mørup*

Main category: cs.LG

TL;DR: EEG2ERP是一种新颖的不确定性感知自编码器方法，可将任意数量的EEG试次映射到相关的ERP，显著减少所需试次数，并在零样本场景下优于传统平均方法。


<details>
  <summary>Details</summary>
Motivation: 传统ERP估计需要大量EEG试次的平均来降低噪声和信号变异性，这限制了ERP研究的效率和应用。需要开发能够减少所需试次数的新方法。

Method: 提出EEG2ERP不确定性感知自编码器，使用引导训练目标和单独的方差解码器来建模ERP估计的不确定性。在零样本场景下评估，使用三个公开数据集：ERP CORE、P300拼写器BCI和人脸感知神经影像数据集。

Result: 在少量试次情况下，EEG2ERP比传统平均方法和鲁棒平均方法提供显著更好的ERP估计。这是首个将EEG信号映射到相关ERP的深度学习方法。

Conclusion: EEG2ERP能够显著减少ERP研究所需的试次数，在零样本泛化到新受试者方面表现优异，为ERP研究提供了更高效的工具。

Abstract: Event-related potentials (ERP) are measurements of brain activity with wide applications in basic and clinical neuroscience, that are typically estimated using the average of many trials of electroencephalography signals (EEG) to sufficiently reduce noise and signal variability. We introduce EEG2ERP, a novel uncertainty-aware autoencoder approach that maps an arbitrary number of EEG trials to their associated ERP. To account for the ERP uncertainty we use bootstrapped training targets and introduce a separate variance decoder to model the uncertainty of the estimated ERP. We evaluate our approach in the challenging zero-shot scenario of generalizing to new subjects considering three different publicly available data sources; i) the comprehensive ERP CORE dataset that includes over 50,000 EEG trials across six ERP paradigms from 40 subjects, ii) the large P300 Speller BCI dataset, and iii) a neuroimaging dataset on face perception consisting of both EEG and magnetoencephalography (MEG) data. We consistently find that our method in the few trial regime provides substantially better ERP estimates than commonly used conventional and robust averaging procedures. EEG2ERP is the first deep learning approach to map EEG signals to their associated ERP, moving toward reducing the number of trials necessary for ERP research. Code is available at https://github.com/andersxa/EEG2ERP

</details>


### [156] [Energy-Efficient Vision Transformer Inference for Edge-AI Deployment](https://arxiv.org/abs/2511.23166)
*Nursultan Amanzhol,Jurn-Gyu Park*

Main category: cs.LG

TL;DR: 提出两阶段评估流程，结合设备无关模型选择与设备相关测量，评估Vision Transformers在能耗受限设备上的能效表现


<details>
  <summary>Details</summary>
Motivation: 随着Vision Transformers在能耗受限设备上的部署增加，需要超越准确率的评估方法来全面评估模型能效

Method: 两阶段评估流程：第一阶段使用NetScore指标进行设备无关模型筛选；第二阶段使用可持续准确率指标(SAM)进行设备相关排名，在NVIDIA Jetson TX2和RTX 3050上对13个ViT模型进行基准测试

Result: 混合模型如LeViT_Conv_192在TX2上相比ViT基线可减少53%能耗；蒸馏模型如TinyViT-11M_Distilled在移动GPU上表现优异，在RTX 3050/CIFAR-10上SAM5=1.72，在RTX 3050/ImageNet-1K上SAM5=0.76

Conclusion: 提出的两阶段评估方法能有效评估ViT模型能效，混合模型在边缘设备上能效更高，蒸馏模型在移动GPU上表现更好，为能耗受限设备上的模型选择提供指导

Abstract: The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).

</details>


### [157] [SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series with Missing Data](https://arxiv.org/abs/2511.23238)
*Yuting Fang,Qouc Le Gia,Flora Salim*

Main category: cs.LG

TL;DR: SDE-Attention：一种结合SDE-RNN与通道级注意力机制的模型，用于处理不规则采样且大量缺失的时间序列数据，在UCR和UEA基准测试中显著提升了缺失数据下的预测性能。


<details>
  <summary>Details</summary>
Motivation: 医疗健康和传感器网络中常见的不规则采样时间序列通常存在大量缺失观测值，现有方法在处理这种数据时效果有限，需要开发能够有效处理高缺失率时间序列的模型。

Method: 提出SDE-Attention模型家族，在SDE-RNN基础上引入通道级注意力机制，包括通道重新校准、时变特征注意力和金字塔多尺度自注意力三种机制，增强模型对缺失数据的处理能力。

Result: 在合成周期数据集和真实世界基准测试中，注意力机制显著提升SDE-RNN性能。在UCR单变量数据集上，SDE-TVF-L模型在30%、60%和90%缺失率下分别提升约4%、6%和10%的平均准确率；在UEA多变量数据集上，注意力增强模型最高提升7%的平均准确率。

Conclusion: SDE-Attention通过注意力机制有效提升了处理高缺失率时间序列的能力，时变特征注意力在单变量数据上最稳健，不同注意力机制在不同多变量任务中各有优势，模型可根据问题结构灵活适配。

Abstract: Irregularly sampled time series with substantial missing observations are common in healthcare and sensor networks. We introduce SDE-Attention, a family of SDE-RNNs equipped with channel-level attention on the latent pre-RNN state, including channel recalibration, time-varying feature attention, and pyramidal multi-scale self-attention. We therefore conduct a comparison on a synthetic periodic dataset and real-world benchmarks, under varying missing rate. Latent-space attention consistently improves over a vanilla SDE-RNN. On the univariate UCR datasets, the LSTM-based time-varying feature model SDE-TVF-L achieves the highest average accuracy, raising mean performance by approximately 4, 6, and 10 percentage points over the baseline at 30%, 60% and 90% missingness, respectively (averaged across datasets). On multivariate UEA benchmarks, attention-augmented models again outperform the backbone, with SDE-TVF-L yielding up to a 7% gain in mean accuracy under high missingness. Among the proposed mechanisms, time-varying feature attention is the most robust on univariate datasets. On multivariate datasets, different attention types excel on different tasks, showing that SDE-Attention can be flexibly adapted to the structure of each problem.

</details>


### [158] [Time Series Forecasting via Direct Per-Step Probability Distribution Modeling](https://arxiv.org/abs/2511.23260)
*Linghao Kong,Xiaopeng Hong*

Main category: cs.LG

TL;DR: 提出interPDN模型，通过双分支架构直接构建离散概率分布而非标量值，解决时间序列预测中的不确定性量化问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络时间序列预测模型虽然能捕捉复杂时序依赖，但难以量化预测不确定性，因为它们直接输出标量值而非概率分布。

Method: 提出interleaved dual-branch Probability Distribution Network (interPDN)：1) 直接构建每个时间步的离散概率分布而非标量；2) 通过计算预定义支持集上预测分布的期望得到回归输出；3) 采用双分支架构，支持集交错排列；4) 添加粗粒度时间尺度分支用于长期趋势预测；5) 使用另一分支输出作为辅助信号，对当前分支预测施加自监督一致性约束。

Result: 在多个真实世界数据集上的广泛实验表明interPDN具有优越性能。

Conclusion: interPDN通过直接建模概率分布和双分支自监督架构，有效解决了时间序列预测中的不确定性量化问题，并在实验中表现出色。

Abstract: Deep neural network-based time series prediction models have recently demonstrated superior capabilities in capturing complex temporal dependencies. However, it is challenging for these models to account for uncertainty associated with their predictions, because they directly output scalar values at each time step. To address such a challenge, we propose a novel model named interleaved dual-branch Probability Distribution Network (interPDN), which directly constructs discrete probability distributions per step instead of a scalar. The regression output at each time step is derived by computing the expectation of the predictive distribution on a predefined support set. To mitigate prediction anomalies, a dual-branch architecture is introduced with interleaved support sets, augmented by coarse temporal-scale branches for long-term trend forecasting. Outputs from another branch are treated as auxiliary signals to impose self-supervised consistency constraints on the current branch's prediction. Extensive experiments on multiple real-world datasets demonstrate the superior performance of interPDN.

</details>


### [159] [An Improved and Generalised Analysis for Spectral Clustering](https://arxiv.org/abs/2511.23261)
*George Tyler,Luca Zanetti*

Main category: cs.LG

TL;DR: 谱聚类算法在特征值分组良好分离时表现优秀，适用于有层次聚类结构的有向图，能恢复边方向一致的簇划分


<details>
  <summary>Details</summary>
Motivation: 重新审视谱聚类的理论性能，探索其在传统拉普拉斯矩阵之外的应用，特别是针对有向图和层次聚类结构的情况

Method: 分析谱聚类算法，研究当最小特征值分组良好分离时的理论性能，扩展到有向图的厄米特矩阵表示，验证其在生态网络等实际应用中的有效性

Result: 谱聚类在特征值分组分离时表现良好，能处理层次聚类结构，对有向图能恢复边方向一致的簇划分，在合成和真实数据集上验证了理论预测

Conclusion: 谱聚类算法在更一般的条件下表现优秀，特别适用于有向图和层次聚类结构，为生态网络分析等应用提供了理论支持

Abstract: We revisit the theoretical performances of Spectral Clustering, a classical algorithm for graph partitioning that relies on the eigenvectors of a matrix representation of the graph. Informally, we show that Spectral Clustering works well as long as the smallest eigenvalues appear in groups well separated from the rest of the matrix representation's spectrum. This arises, for example, whenever there exists a hierarchy of clusters at different scales, a regime not captured by previous analyses. Our results are very general and can be applied beyond the traditional graph Laplacian. In particular, we study Hermitian representations of digraphs and show Spectral Clustering can recover partitions where edges between clusters are oriented mostly in the same direction. This has applications in, for example, the analysis of trophic levels in ecological networks. We demonstrate that our results accurately predict the performances of Spectral Clustering on synthetic and real-world data sets.

</details>


### [160] [BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning](https://arxiv.org/abs/2511.23264)
*Ariful Islam,Md Rifat Hossen,Tanvir Mahmud*

Main category: cs.LG

TL;DR: BanglaSentNet：一个可解释的混合深度学习框架，用于孟加拉语电商评论的多方面情感分析，通过动态加权集成学习结合LSTM、BiLSTM、GRU和BanglaBERT，在8,755条标注数据上达到85%准确率，并提供SHAP特征归因和注意力可视化。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语多方面情感分析面临挑战：标注数据集有限、形态复杂、代码混合现象、领域迁移问题，影响3亿孟加拉语用户。现有方法缺乏可解释性和跨领域泛化能力，难以实际部署。

Method: 提出BanglaSentNet框架，集成LSTM、BiLSTM、GRU和BanglaBERT，采用动态加权集成学习进行多方面情感分类。构建包含8,755条手动标注的孟加拉语产品评论数据集，涵盖质量、服务、价格、装饰四个方面。集成SHAP特征归因和注意力可视化实现可解释性。

Result: 达到85%准确率和0.88 F1分数，优于独立深度学习模型3-7%，显著超越传统方法。可解释性套件获得9.4/10可解释性评分，人类一致性达87.6%。跨领域迁移学习显示强大泛化能力：零样本性能在多样领域保持67-76%效果；少量样本学习（500-1000样本）达到全微调性能的90-95%，大幅降低标注成本。

Conclusion: 该研究为孟加拉语情感分析建立了新的SOTA基准，推进了低资源语言的集成学习方法，并为商业应用提供了可行的解决方案，在孟加拉电商平台的实际部署中展示了实用价值。

Abstract: Multi-aspect sentiment analysis of Bangla e-commerce reviews remains challenging due to limited annotated datasets, morphological complexity, code-mixing phenomena, and domain shift issues, affecting 300 million Bangla-speaking users. Existing approaches lack explainability and cross-domain generalization capabilities crucial for practical deployment. We present BanglaSentNet, an explainable hybrid deep learning framework integrating LSTM, BiLSTM, GRU, and BanglaBERT through dynamic weighted ensemble learning for multi-aspect sentiment classification. We introduce a dataset of 8,755 manually annotated Bangla product reviews across four aspects (Quality, Service, Price, Decoration) from major Bangladeshi e-commerce platforms. Our framework incorporates SHAP-based feature attribution and attention visualization for transparent insights. BanglaSentNet achieves 85% accuracy and 0.88 F1-score, outperforming standalone deep learning models by 3-7% and traditional approaches substantially. The explainability suite achieves 9.4/10 interpretability score with 87.6% human agreement. Cross-domain transfer learning experiments reveal robust generalization: zero-shot performance retains 67-76% effectiveness across diverse domains (BanglaBook reviews, social media, general e-commerce, news headlines); few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance, significantly reducing annotation costs. Real-world deployment demonstrates practical utility for Bangladeshi e-commerce platforms, enabling data-driven decision-making for pricing optimization, service improvement, and customer experience enhancement. This research establishes a new state-of-the-art benchmark for Bangla sentiment analysis, advances ensemble learning methodologies for low-resource languages, and provides actionable solutions for commercial applications.

</details>


### [161] [Beyond Curve Fitting: Neuro-Symbolic Agents for Context-Aware Epidemic Forecasting](https://arxiv.org/abs/2511.23276)
*Joongwon Chae,Runming Wang,Chen Xiong,Gong Yunhan,Lian Zhang,Ji Jiansong,Dongmei Yu,Peiwu Qin*

Main category: cs.LG

TL;DR: 提出一个两智能体框架，将上下文解释与概率预测解耦，使用LLM处理异质信号生成传播影响信号，再结合历史数据进行概率预测，在手足口病监测中实现竞争性预测精度和稳健的预测区间。


<details>
  <summary>Details</summary>
Motivation: 传统模型和基础模型虽然能纳入协变量，但缺乏语义推理能力来解释冲突驱动因素之间的因果相互作用，无法有效处理手足口病监测中需要考虑流行病学模式和学校日历、天气等上下文驱动因素的问题。

Method: 提出两智能体框架：1) LLM"事件解释器"处理异质信号（学校日程、气象摘要、报告等）生成标量传播影响信号；2) 神经符号核心将此信号与历史病例数结合，产生校准的概率预测。

Result: 在香港（2023-2024）和丽水（2024）的真实手足口病数据集上评估，相比传统和基础模型基线，该方法实现了竞争性的点预测精度，同时提供稳健的90%预测区间（覆盖率0.85-1.00）和人类可解释的推理。

Conclusion: 通过LLM结构化整合领域知识可以匹配最先进的性能，同时产生符合公共卫生工作流程的上下文感知预测，为疾病监测提供了新的框架。

Abstract: Effective surveillance of hand, foot and mouth disease (HFMD) requires forecasts accounting for epidemiological patterns and contextual drivers like school calendars and weather. While classical models and recent foundation models (e.g., Chronos, TimesFM) incorporate covariates, they often lack the semantic reasoning to interpret the causal interplay between conflicting drivers. In this work, we propose a two-agent framework decoupling contextual interpretation from probabilistic forecasting. An LLM "event interpreter" processes heterogeneous signals-including school schedules, meteorological summaries, and reports-into a scalar transmission-impact signal. A neuro-symbolic core then combines this with historical case counts to produce calibrated probabilistic forecasts. We evaluate the framework on real-world HFMD datasets from Hong Kong (2023-2024) and Lishui, China (2024). Compared to traditional and foundation-model baselines, our approach achieves competitive point forecasting accuracy while providing robust 90% prediction intervals (coverage 0.85-1.00) and human-interpretable rationales. Our results suggest that structurally integrating domain knowledge through LLMs can match state-of-the-art performance while yielding context-aware forecasts that align with public health workflows. Code is available at https://github.com/jw-chae/forecast_MED .

</details>


### [162] [Closing the Generalization Gap in Parameter-efficient Federated Edge Learning](https://arxiv.org/abs/2511.23282)
*Xinnong Du,Zhonghao Lyu,Xiaowen Cao,Chunyang Wen,Shuguang Cui,Jie Xu*

Main category: cs.LG

TL;DR: 提出了一种参数高效的联邦边缘学习框架，通过联合模型剪枝和客户端选择来应对数据异构和资源受限的挑战，将泛化分析嵌入收敛分析中，并通过联合优化实现系统级优化。


<details>
  <summary>Details</summary>
Motivation: 联邦边缘学习面临本地数据集有限且异构、资源受限部署的问题，这会严重降低模型泛化能力和资源利用率，从而影响学习性能。

Method: 1) 推导信息论泛化界，刻画训练和测试函数损失之间的差异，并将其嵌入收敛分析；2) 提出泛化感知的平均平方梯度范数界最小化问题，联合优化剪枝率、客户端选择、通信计算资源；3) 使用交替优化算法高效求解非凸混合整数问题。

Result: 大量实验表明，所提出的设计比现有基线方法实现了更优的学习性能，验证了将泛化感知分析与系统级优化耦合用于高效联邦边缘学习的有效性。

Conclusion: 通过联合模型剪枝和客户端选择，将泛化分析嵌入系统优化框架，能够有效提升联邦边缘学习的性能，解决数据异构和资源受限的挑战。

Abstract: Federated edge learning (FEEL) provides a promising foundation for edge artificial intelligence (AI) by enabling collaborative model training while preserving data privacy. However, limited and heterogeneous local datasets, as well as resource-constrained deployment, severely degrade both model generalization and resource utilization, leading to a compromised learning performance. Therefore, we propose a parameter-efficient FEEL framework that jointly leverages model pruning and client selection to tackle such challenges. First, we derive an information-theoretic generalization statement that characterizes the discrepancy between training and testing function losses and embed it into the convergence analysis. It reveals that a larger local generalization statement can undermine the global convergence. Then, we formulate a generalization-aware average squared gradient norm bound minimization problem, by jointly optimizing the pruning ratios, client selection, and communication-computation resources under energy and delay constraints. Despite its non-convexity, the resulting mixed-integer problem is efficiently solved via an alternating optimization algorithm. Extensive experiments demonstrate that the proposed design achieves superior learning performance than state-of-the-art baselines, validating the effectiveness of coupling generalization-aware analysis with system-level optimization for efficient FEEL.

</details>


### [163] [Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla](https://arxiv.org/abs/2511.23287)
*Ariful Islam,Tanvir Mahmud,Md Rifat Hossen*

Main category: cs.LG

TL;DR: 本文提出BangACMM框架，通过中间融合策略结合文本和视觉模态，在孟加拉语社交媒体帖子作者意图分类任务上取得84.11%的宏F1分数，相比之前方法提升8.4个百分点。


<details>
  <summary>Details</summary>
Motivation: 互联网和社交网络的扩张导致用户生成内容爆炸式增长，作者意图理解对社交媒体内容解释至关重要。先前单模态方法存在局限性，需要更有效的多模态方法处理孟加拉语社交媒体内容。

Method: 系统性地基准测试了基于transformer的语言模型（mBERT、DistilBERT、XLM-RoBERTa）和视觉架构（ViT、Swin、SwiftFormer、ResNet、DenseNet、MobileNet），使用包含3,048个帖子、涵盖6个意图类别的Uddessho数据集。提出了新颖的中间融合策略，优于早期和晚期融合方法。

Result: 中间融合策略（特别是mBERT和Swin Transformer组合）取得了84.11%的宏F1分数，建立了新的最先进水平，相比之前的孟加拉语多模态方法提升了8.4个百分点。视觉上下文整合显著增强了意图分类性能。

Conclusion: 中间级别的跨模态特征整合在模态特定表示和跨模态学习之间提供了最佳平衡。该研究为孟加拉语和其他低资源语言建立了新的基准和方法标准。提出的框架BangACMM在多模态意图分类任务上表现出色。

Abstract: The expansion of the Internet and social networks has led to an explosion of user-generated content. Author intent understanding plays a crucial role in interpreting social media content. This paper addresses author intent classification in Bangla social media posts by leveraging both textual and visual data. Recognizing limitations in previous unimodal approaches, we systematically benchmark transformer-based language models (mBERT, DistilBERT, XLM-RoBERTa) and vision architectures (ViT, Swin, SwiftFormer, ResNet, DenseNet, MobileNet), utilizing the Uddessho dataset of 3,048 posts spanning six practical intent categories. We introduce a novel intermediate fusion strategy that significantly outperforms early and late fusion on this task. Experimental results show that intermediate fusion, particularly with mBERT and Swin Transformer, achieves 84.11% macro-F1 score, establishing a new state-of-the-art with an 8.4 percentage-point improvement over prior Bangla multimodal approaches. Our analysis demonstrates that integrating visual context substantially enhances intent classification. Cross-modal feature integration at intermediate levels provides optimal balance between modality-specific representation and cross-modal learning. This research establishes new benchmarks and methodological standards for Bangla and other low-resource languages. We call our proposed framework BangACMM (Bangla Author Content MultiModal).

</details>


### [164] [Machine Learning for Scientific Visualization: Ensemble Data Analysis](https://arxiv.org/abs/2511.23290)
*Hamid Gadirov*

Main category: cs.LG

TL;DR: 该论文探索深度学习方法来改进时空科学集合数据的分析和可视化，包括基于自动编码器的降维、FLINT模型的流场估计与时间插值，以及HyperFLINT的超网络参数自适应方法。


<details>
  <summary>Details</summary>
Motivation: 科学模拟和实验测量产生大量时空数据，但高维度、复杂结构和信息缺失使得提取有意义的洞察具有挑战性。传统分析方法难以处理这些问题，需要更鲁棒的数据驱动方法。

Method: 1. 基于自动编码器的降维方法，评估部分标签下的投影稳定性，引入帕累托最优选择策略识别最佳自动编码器变体；2. FLINT深度学习模型，在流场监督和无监督设置下进行高质量流场估计和时间插值；3. HyperFLINT超网络方法，基于模拟参数自适应估计流场和插值标量数据。

Result: 实现了表达性和可靠的低维嵌入，能够重建缺失速度场并在2D+时间和3D+时间集合中生成高保真时间插值，无需领域特定假设或大量微调。参数自适应方法在不同科学领域中获得更准确的重建，即使在稀疏或不完整数据情况下。

Conclusion: 该论文推进了科学可视化的深度学习技术，为解释复杂时空集合数据提供了可扩展、自适应和高质量的解决方案。

Abstract: Scientific simulations and experimental measurements produce vast amounts of spatio-temporal data, yet extracting meaningful insights remains challenging due to high dimensionality, complex structures, and missing information. Traditional analysis methods often struggle with these issues, motivating the need for more robust, data-driven approaches. This dissertation explores deep learning methodologies to improve the analysis and visualization of spatio-temporal scientific ensembles, focusing on dimensionality reduction, flow estimation, and temporal interpolation. First, we address high-dimensional data representation through autoencoder-based dimensionality reduction for scientific ensembles. We evaluate the stability of projection metrics under partial labeling and introduce a Pareto-efficient selection strategy to identify optimal autoencoder variants, ensuring expressive and reliable low-dimensional embeddings. Next, we present FLINT, a deep learning model for high-quality flow estimation and temporal interpolation in both flow-supervised and flow-unsupervised settings. FLINT reconstructs missing velocity fields and generates high-fidelity temporal interpolants for scalar fields across 2D+time and 3D+time ensembles without domain-specific assumptions or extensive finetuning. To further improve adaptability and generalization, we introduce HyperFLINT, a hypernetwork-based approach that conditions on simulation parameters to estimate flow fields and interpolate scalar data. This parameter-aware adaptation yields more accurate reconstructions across diverse scientific domains, even with sparse or incomplete data. Overall, this dissertation advances deep learning techniques for scientific visualization, providing scalable, adaptable, and high-quality solutions for interpreting complex spatio-temporal ensembles.

</details>


### [165] [Hard-Constrained Neural Networks with Physics-Embedded Architecture for Residual Dynamics Learning and Invariant Enforcement in Cyber-Physical Systems](https://arxiv.org/abs/2511.23307)
*Enzo Nicolás Spotorno,Josafat Leal Filho,Antônio Augusto Fröhlich*

Main category: cs.LG

TL;DR: 提出HRPINN和PHRPINN框架，将已知物理约束嵌入循环神经网络中学习剩余动力学，并通过预测-投影机制严格保证代数不变量，在电池预测和约束基准测试中验证了高精度和数据效率。


<details>
  <summary>Details</summary>
Motivation: 针对具有未知动力学和代数不变量的复杂网络物理系统，需要一种既能嵌入已知物理知识又能严格保证代数约束的学习框架，以提高预测精度和数据效率。

Method: 1. 提出HRPINN架构，将已知物理作为硬结构约束嵌入循环积分器中学习剩余动力学；2. 提出PHRPINN扩展，通过预测-投影机制严格保证代数不变量；3. 提供理论分析支持框架表示能力。

Result: 在真实世界电池预测DAE上验证HRPINN，在标准约束基准测试上评估PHRPINN，结果显示框架能实现高精度和数据效率，同时揭示了物理一致性、计算成本和数值稳定性之间的权衡。

Conclusion: 该框架为复杂网络物理系统的物理信息学习提供了有效解决方案，通过硬约束和预测-投影机制平衡了物理一致性、计算效率和数值稳定性，为实际部署提供了实用指导。

Abstract: This paper presents a framework for physics-informed learning in complex cyber-physical systems governed by differential equations with both unknown dynamics and algebraic invariants. First, we formalize the Hybrid Recurrent Physics-Informed Neural Network (HRPINN), a general-purpose architecture that embeds known physics as a hard structural constraint within a recurrent integrator to learn only residual dynamics. Second, we introduce the Projected HRPINN (PHRPINN), a novel extension that integrates a predict-project mechanism to strictly enforce algebraic invariants by design. The framework is supported by a theoretical analysis of its representational capacity. We validate HRPINN on a real-world battery prognostics DAE and evaluate PHRPINN on a suite of standard constrained benchmarks. The results demonstrate the framework's potential for achieving high accuracy and data efficiency, while also highlighting critical trade-offs between physical consistency, computational cost, and numerical stability, providing practical guidance for its deployment.

</details>


### [166] [Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.23315)
*Azusa Yamaguchi*

Main category: cs.LG

TL;DR: 该论文通过大规模实验揭示了去中心化多智能体强化学习中的三阶段相结构，发现智能体间不对称性驱动的核漂移是协调涌现的关键机制。


<details>
  <summary>Details</summary>
Motivation: 需要更清晰地理解去中心化多智能体强化学习中协调何时出现、波动或崩溃，以表征多智能体学习系统的动态特性。

Method: 使用完全独立Q学习作为最小去中心化测试平台，在环境大小L和智能体密度ρ参数空间进行大规模实验，构建基于合作成功率(CSR)和TD误差方差稳定性指数的相图。

Result: 发现了三个不同相：协调稳定相、脆弱过渡区和阻塞/无序相，被尖锐的双不稳定脊分隔。核漂移（由智能体间不对称性驱动）是相结构的关键机制，移除智能体标识符会完全消除漂移并破坏三相结构。

Conclusion: 去中心化MARL表现出由规模、密度和核漂移相互作用支配的相干相结构，表明涌现的协调行为是一种分布交互驱动的相现象。

Abstract: A clearer understanding of when coordination emerges, fluctuates, or collapses in decentralized multi-agent reinforcement learning (MARL) is increasingly sought in order to characterize the dynamics of multi-agent learning systems. We revisit fully independent Q-learning (IQL) as a minimal decentralized testbed and run large-scale experiments across environment size L and agent density rho. We construct a phase map using two axes - the cooperative success rate (CSR) and a stability index derived from TD-error variance - revealing three distinct regimes: a coordinated and stable phase, a fragile transition region, and a jammed or disordered phase. A sharp double Instability Ridge separates these regimes and corresponds to persistent kernel drift, the time-varying shift of each agent's effective transition kernel induced by others' policy updates. Synchronization analysis further shows that temporal alignment is required for sustained cooperation, and that competition between drift and synchronization generates the fragile regime. Removing agent identifiers eliminates drift entirely and collapses the three-phase structure, demonstrating that small inter-agent asymmetries are a necessary driver of drift. Overall, the results show that decentralized MARL exhibits a coherent phase structure governed by the interaction between scale, density, and kernel drift, suggesting that emergent coordination behaves as a distribution-interaction-driven phase phenomenon.

</details>


### [167] [ParaGate: Parasitic-Driven Domain Adaptation Transfer Learning for Netlist Performance Prediction](https://arxiv.org/abs/2511.23340)
*Bin Sun,Jingyi Zhou,Jianan Mu,Zhiteng Chao,Tianmeng Yang,Ziyue Xu,Jing Ye,Huawei Li*

Main category: cs.LG

TL;DR: ParaGate是一个三阶段跨阶段预测框架，直接从网表推断布局级时序和功耗，通过迁移学习预测寄生参数，利用EDA工具进行时序分析，并通过子图特征进行全局校准，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统EDA流程中，布局级性能指标只能在布局布线后获得，阻碍了早期阶段的全局优化。现有的神经网络解决方案由于商业布局布线工具的黑盒启发式算法导致数据差异，面临泛化挑战。

Method: 提出ParaGate三阶段框架：1) 采用两阶段迁移学习方法预测寄生参数，先在中等规模电路上预训练，再在更大电路上微调以捕捉极端条件；2) 依赖EDA工具进行时序分析，卸载长路径数值推理；3) 使用子图特征进行全局校准。

Result: ParaGate在openE906上实现了显著的预测精度提升，到达时间R²从0.119提升到0.897，展示了只需少量微调数据就能实现强泛化能力。

Conclusion: ParaGate能够为综合和布局阶段的全局优化提供指导，通过跨阶段预测框架有效解决了传统EDA流程中早期性能评估的难题。

Abstract: In traditional EDA flows, layout-level performance metrics are only obtainable after placement and routing, hindering global optimization at earlier stages. Although some neural-network-based solutions predict layout-level performance directly from netlists, they often face generalization challenges due to the black-box heuristics of commercial placement-and-routing tools, which create disparate data across designs. To this end, we propose ParaGate, a three-step cross-stage prediction framework that infers layout-level timing and power from netlists. First, we propose a two-phase transfer-learning approach to predict parasitic parameters, pre-training on mid-scale circuits and fine-tuning on larger ones to capture extreme conditions. Next, we rely on EDA tools for timing analysis, offloading the long-path numerical reasoning. Finally, ParaGate performs global calibration using subgraph features. Experiments show that ParaGate achieves strong generalization with minimal fine-tuning data: on openE906, its arrival-time R2 from 0.119 to 0.897. These results demonstrate that ParaGate could provide guidance for global optimization in the synthesis and placement stages.

</details>


### [168] [Learning-Augmented Online Bipartite Matching in the Random Arrival Order Model](https://arxiv.org/abs/2511.23388)
*Kunanon Burathep,Thomas Erlebach,William K. Moses*

Main category: cs.LG

TL;DR: 本文研究了随机到达顺序模型中的在线无权二分图匹配问题，在增强学习设置下：算法接收在线顶点类型（邻域）的不信任预测。作者改进了Choo等人的工作，去除了最优匹配大小为n的假设，只要求预测匹配大小至少为αn，实现了(1-o(1))一致性和(β-o(1))鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有工作（Choo等人）假设最优匹配大小为n（即每个在线顶点都能匹配），这在现实场景中过于理想化。本文旨在去除这一限制，使算法适用于更一般的二分图匹配场景，同时保持学习增强算法的优势。

Method: 使用到达序列的前缀作为样本来判断预测是否接近真实到达序列，然后要么遵循预测，要么使用忽略预测的已知基线算法（β-竞争性）。关键创新是去除了最优匹配大小为n的假设，只要求预测匹配大小至少为αn（0<α≤1）。

Result: 提出的学习增强算法实现了(1-o(1))一致性和(β-o(1))鲁棒性。此外，竞争比随着预测误差的增加在一致性和鲁棒性之间平滑退化。

Conclusion: 本文成功扩展了学习增强在线二分图匹配算法的适用范围，去除了对最优匹配大小的限制，同时保持了良好的理论保证。算法在预测准确时接近最优，预测不准确时仍能保持基线算法的性能。

Abstract: We study the online unweighted bipartite matching problem in the random arrival order model, with $n$ offline and $n$ online vertices, in the learning-augmented setting: The algorithm is provided with untrusted predictions of the types (neighborhoods) of the online vertices. We build upon the work of Choo et al. (ICML 2024, pp. 8762-8781) who proposed an approach that uses a prefix of the arrival sequence as a sample to determine whether the predictions are close to the true arrival sequence and then either follows the predictions or uses a known baseline algorithm that ignores the predictions and is $β$-competitive. Their analysis is limited to the case that the optimal matching has size $n$, i.e., every online vertex can be matched. We generalize their approach and analysis by removing any assumptions on the size of the optimal matching while only requiring that the size of the predicted matching is at least $αn$ for any constant $0 < α\le 1$. Our learning-augmented algorithm achieves $(1-o(1))$-consistency and $(β-o(1))$-robustness. Additionally, we show that the competitive ratio degrades smoothly between consistency and robustness with increasing prediction error.

</details>


### [169] [LFM2 Technical Report](https://arxiv.org/abs/2511.23404)
*Alexander Amini,Anna Banaszak,Harold Benoit,Arthur Böök,Tarek Dakhran,Song Duong,Alfred Eng,Fernando Fernandes,Marc Härkönen,Anne Harrington,Ramin Hasani,Saniya Karwa,Yuri Khrustalev,Maxime Labonne,Mathias Lechner,Valentine Lechner,Simon Lee,Zetian Li,Noel Loo,Jacob Marks,Edoardo Mosca,Samuel J. Paech,Paul Pak,Rom N. Parnichkun,Alex Quach,Ryan Rogers,Daniela Rus,Nayan Saxena,Bettina Schlager,Tim Seyde,Jimmy T. H. Smith,Aditya Tadimeti,Neehal Tumma*

Main category: cs.LG

TL;DR: LFM2是一个面向边缘设备部署的液体基础模型家族，通过硬件感知架构搜索获得紧凑混合骨干，支持350M-8.3B参数规模，在多模态和检索任务上表现优异，提供开源权重和部署方案。


<details>
  <summary>Details</summary>
Motivation: 为了解决边缘设备上部署大型语言模型的效率和性能问题，需要在有限的计算资源和内存约束下实现快速推理和强大的任务能力。

Method: 采用硬件在环架构搜索设计紧凑混合骨干（门控短卷积+分组查询注意力）；使用温度调节解耦Top-K知识蒸馏避免支持不匹配；课程学习按难度排序数据；三阶段后训练（监督微调、长度归一化偏好优化、模型合并）。

Result: LFM2-2.6B在IFEval上达到79.56%，GSM8K上达到82.41%；CPU推理速度比同规模模型快2倍；支持多模态（视觉、音频）和检索变体；提供开源权重和ExecuTorch、llama.cpp、vLLM部署包。

Conclusion: LFM2为边缘应用提供了高效、内存友好的基础模型解决方案，在保持强大任务能力的同时显著提升推理速度，通过开源生态促进实际部署。

Abstract: We present LFM2, a family of Liquid Foundation Models designed for efficient on-device deployment and strong task capabilities. Using hardware-in-the-loop architecture search under edge latency and memory constraints, we obtain a compact hybrid backbone that combines gated short convolutions with a small number of grouped query attention blocks, delivering up to 2x faster prefill and decode on CPUs compared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, including dense models (350M, 700M, 1.2B, 2.6B) and a mixture-of-experts variant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes a tempered, decoupled Top-K knowledge distillation objective that avoids support mismatch; curriculum learning with difficulty-ordered data; and a three-stage post-training recipe of supervised fine-tuning, length-normalized preference optimization, and model merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% on IFEval and 82.41% on GSM8K. We further build multimodal and retrieval variants: LFM2-VL for vision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs via token-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enable real-time speech-to-speech interaction competitive with models 3x larger. LFM2-ColBERT provides a low-latency encoder for queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages for ExecuTorch, llama.cpp, and vLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities.

</details>


### [170] [ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts](https://arxiv.org/abs/2511.23442)
*Hang Yu,Di Zhang,Qiwei Du,Yanping Zhao,Hai Zhang,Guang Chen,Eduardo E. Veas,Junqiao Zhao*

Main category: cs.LG

TL;DR: ASTRO是一个离线强化学习数据增强框架，通过生成分布新颖且符合动态约束的轨迹来解决子优和碎片化数据集的问题，显著提升策略学习性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临子优和碎片化轨迹数据集带来的挑战，导致奖励传播困难、价值估计不准确和策略性能下降。现有的轨迹拼接方法要么局限于行为策略的支持范围，要么违反底层动态约束，限制了策略改进效果。

Method: ASTRO首先学习时序距离表示来识别不同且可达的拼接目标，然后采用动态引导的拼接规划器，通过Rollout Deviation Feedback（目标状态序列与实际到达状态序列之间的差距）自适应生成连接动作序列，确保轨迹拼接的可行性和可达性。

Result: ASTRO在多种离线RL算法上超越了先前的数据增强方法，在具有挑战性的OGBench套件上取得了显著的性能提升，并在D4RL等标准离线RL基准测试中表现出一致的改进。

Conclusion: ASTRO通过生成分布新颖且符合动态约束的轨迹，有效解决了离线强化学习中轨迹拼接的挑战，显著提升了策略学习性能，为离线RL数据增强提供了新的有效框架。

Abstract: Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.

</details>


### [171] [Physics-Informed Neural Networks for Thermophysical Property Retrieval](https://arxiv.org/abs/2511.23449)
*Ali Waseem,Malcolm Mielle*

Main category: cs.LG

TL;DR: 提出基于物理信息神经网络(PINN)的迭代框架，利用热成像数据非侵入式估计墙体导热系数k，无需长时间测量即可在真实环境条件下获得准确结果。


<details>
  <summary>Details</summary>
Motivation: 当前测量导热系数的方法存在侵入性、需要长时间观测或对环境条件敏感的问题，而建筑外墙改造后的热传递性能评估对建筑能效至关重要，需要可靠的非侵入式现场测量方法。

Method: 提出PINN-based迭代框架：交替进行固定k时的前向热问题PINN估计，以及通过比较热成像数据和PINN预测表面温度来优化k，重复直至k收敛。

Result: 使用气象站数据和有限体积法软件模拟数据，在不同环境条件和数据采集时间下准确预测k（前提是黎明时墙体温度接近稳态）。即使违反稳态假设，最大MAE仅为4.0851。

Conclusion: PINN方法为现场可靠估计材料特性提供了潜力，无需长时间测量活动。作为机器学习特别是PINN用于解决现场逆问题的初步研究，有望推动该领域更多研究。

Abstract: Inverse heat problems refer to the estimation of material thermophysical properties given observed or known heat diffusion behaviour. Inverse heat problems have wide-ranging uses, but a critical application lies in quantifying how building facade renovation reduces thermal transmittance, a key determinant of building energy efficiency. However, solving inverse heat problems with non-invasive data collected in situ is error-prone due to environmental variability or deviations from theoretically assumed conditions. Hence, current methods for measuring thermal conductivity are either invasive, require lengthy observation periods, or are sensitive to environmental and experimental conditions. Here, we present a PINN-based iterative framework to estimate the thermal conductivity k of a wall from a set of thermographs; our framework alternates between estimating the forward heat problem with a PINN for a fixed k, and optimizing k by comparing the thermographs and surface temperatures predicted by the PINN, repeating until the estimated k's convergence. Using both environmental data captured by a weather station and data generated from Finite-Volume-Method software simulations, we accurately predict k across different environmental conditions and data collection sampling times, given the temperature profile of the wall at dawn is close to steady state. Although violating the steady-state assumption impacts the accuracy of k's estimation, we show that our proposed framework still only exhibits a maximum MAE of 4.0851. Our work demonstrates the potential of PINN-based methods for reliable estimation of material properties in situ and under realistic conditions, without lengthy measurement campaigns. Given the lack of research on using machine learning, and more specifically on PINNs, for solving in-situ inverse problems, we expect our work to be a starting point for more research on the topic.

</details>


### [172] [The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference](https://arxiv.org/abs/2511.23455)
*Hans Gundlach,Jayson Lynch,Matthias Mertens,Neil Thompson*

Main category: cs.LG

TL;DR: 该研究发现AI基准测试性能的成本正以每年5-10倍的速度下降，主要受经济因素、硬件效率和算法效率改进驱动，建议评估者应考虑价格因素来衡量AI的实际影响。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在基准测试上的进步往往依赖于更昂贵的模型，这可能导致对实际能力进展的扭曲认识。研究者希望了解在固定预算下AI能力的发展速度，以更准确地评估AI的实际影响。

Method: 使用Artificial Analysis和Epoch AI的数据构建了当前和历史价格的最大数据集，分析前沿模型在知识、推理、数学和软件工程基准测试上的成本变化趋势，并分离出经济因素、硬件效率和算法效率的贡献。

Result: 研究发现：1）达到特定基准测试性能水平的成本每年下降约5-10倍；2）算法效率进步约为每年3倍（在控制竞争效应和硬件价格下降后）；3）成本下降主要来自经济力量、硬件效率改进和算法效率改进。

Conclusion: AI基准测试的成本正在快速下降，评估者应该公开并考虑基准测试的价格因素，以更准确地衡量AI的实际世界影响。价格应成为评估AI进展的重要维度。

Abstract: Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a warped picture of progress in practical capabilities per dollar. To remedy this, we use data from Artificial Analysis and Epoch AI to form the largest dataset of current and historical prices to run benchmarks to date. We find that the price for a given level of benchmark performance has decreased remarkably fast, around $5\times$ to $10\times$ per year, for frontier models on knowledge, reasoning, math, and software engineering benchmarks. These reductions in the cost of AI inference are due to economic forces, hardware efficiency improvements, and algorithmic efficiency improvements. Isolating out open models to control for competition effects and dividing by hardware price declines, we estimate that algorithmic efficiency progress is around $3\times$ per year. Finally, we recommend that evaluators both publicize and take into account the price of benchmarking as an essential part of measuring the real-world impact of AI.

</details>


### [173] [SmallWorlds: Assessing Dynamics Understanding of World Models in Isolated Environments](https://arxiv.org/abs/2511.23465)
*Xinyi Li,Zaishuo Xia,Weyl Lu,Chenjie Hao,Yubei Chen*

Main category: cs.LG

TL;DR: SmallWorld Benchmark：一个用于系统评估世界模型能力的测试平台，在完全可观测状态空间中测试不同架构（RSSM、Transformer、Diffusion、Neural ODE）在六个领域的表现，揭示模型捕捉环境结构的能力和长期预测的退化情况。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型缺乏统一、可控的系统评估设置，难以判断它们是否真正捕捉到了环境动态的底层规则。需要建立一个标准化的测试平台来评估世界模型的能力。

Method: 提出SmallWorld Benchmark测试平台，在完全可观测状态空间中，对代表性架构（循环状态空间模型、Transformer、扩散模型、神经ODE）进行综合实验，在六个不同领域中评估其表现。

Result: 实验结果显示这些模型捕捉环境结构的有效性，以及它们在长期推演中预测性能的退化情况，揭示了当前建模范式的优势和局限性。

Conclusion: SmallWorld Benchmark为世界模型评估提供了标准化测试平台，实验结果指出了当前建模方法的优缺点，为表示学习和动态建模的未来改进方向提供了见解。

Abstract: Current world models lack a unified and controlled setting for systematic evaluation, making it difficult to assess whether they truly capture the underlying rules that govern environment dynamics. In this work, we address this open challenge by introducing the SmallWorld Benchmark, a testbed designed to assess world model capability under isolated and precisely controlled dynamics without relying on handcrafted reward signals. Using this benchmark, we conduct comprehensive experiments in the fully observable state space on representative architectures including Recurrent State Space Model, Transformer, Diffusion model, and Neural ODE, examining their behavior across six distinct domains. The experimental results reveal how effectively these models capture environment structure and how their predictions deteriorate over extended rollouts, highlighting both the strengths and limitations of current modeling paradigms and offering insights into future improvement directions in representation learning and dynamics modeling.

</details>


### [174] [ThetaEvolve: Test-time Learning on Open Problems](https://arxiv.org/abs/2511.23473)
*Yiping Wang,Shao-Rong Su,Zhiyuan Zeng,Eva Xu,Liliang Ren,Xinyu Yang,Zeyi Huang,Xuehai He,Luyao Ma,Baolin Peng,Hao Cheng,Pengcheng He,Weizhu Chen,Shuohang Wang,Simon Shaolei Du,Yelong Shen*

Main category: cs.LG

TL;DR: ThetaEvolve是一个开源框架，简化并扩展了AlphaEvolve，通过上下文学习和强化学习在测试时持续学习，使小型开源模型能在开放优化问题上取得新的最佳边界。


<details>
  <summary>Details</summary>
Motivation: AlphaEvolve虽然取得了数学发现的突破，但它是闭源系统，依赖前沿LLM集成，且是纯推理系统无法内化演化策略。需要开源框架让模型能从经验中持续学习。

Method: ThetaEvolve采用单个LLM、大型程序数据库增强探索、批量采样提高吞吐量、惰性惩罚避免停滞输出、可选奖励塑形提供稳定训练信号，支持测试时的上下文学习和强化学习。

Result: ThetaEvolve首次使小型开源模型（如DeepSeek-R1-0528-Qwen3-8B）在AlphaEvolve提到的开放问题（圆堆积和自相关不等式）上取得新的最佳边界。在两个模型和四个开放任务中，测试时强化学习始终优于纯推理基线。

Conclusion: ThetaEvolve是一个有效的开源演化框架，使模型能够学习演化能力，RL训练的检查点在目标任务和未见任务上都表现出更快的进展和更好的最终性能。

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [175] [Sparse Multiple Kernel Learning: Alternating Best Response and Semidefinite Relaxations](https://arxiv.org/abs/2511.21890)
*Dimitris Bertsimas,Caio de Prospero Iglesias,Nicholas A. G. Johnson*

Main category: stat.ML

TL;DR: 提出一种新的稀疏多核学习方法，通过显式基数约束和l2正则化，使用交替最优响应算法求解，并通过混合整数半定规划松弛提供最优性证明。


<details>
  <summary>Details</summary>
Motivation: 现有l1正则化方法只能近似稀疏性，需要更直接的方法实现真正的稀疏核选择，同时保持模型鲁棒性。

Method: 1) 使用显式基数约束和l2正则化；2) 交替最优响应算法：alpha子问题用LIBSVM求解，beta子问题用贪心选择和单纯形投影算法；3) 混合整数半定规划松弛提供最优性证明和热启动。

Result: 在10个UCI数据集上，随机初始化时比最佳基准方法平均提升3.34%预测准确率；热启动时平均提升4.05%，同时选择少量核且运行时间相当。

Conclusion: 提出的方法在稀疏多核学习中实现了更好的预测性能和稀疏性，通过凸松弛提供了全局最优性证明，为实际应用提供了可靠保证。

Abstract: We study Sparse Multiple Kernel Learning (SMKL), which is the problem of selecting a sparse convex combination of prespecified kernels for support vector binary classification. Unlike prevailing l1 regularized approaches that approximate a sparsifying penalty, we formulate the problem by imposing an explicit cardinality constraint on the kernel weights and add an l2 penalty for robustness. We solve the resulting non-convex minimax problem via an alternating best response algorithm with two subproblems: the alpha subproblem is a standard kernel SVM dual solved via LIBSVM, while the beta subproblem admits an efficient solution via the Greedy Selector and Simplex Projector algorithm. We reformulate SMKL as a mixed integer semidefinite optimization problem and derive a hierarchy of semidefinite convex relaxations which can be used to certify near-optimality of the solutions returned by our best response algorithm and also to warm start it. On ten UCI benchmarks, our method with random initialization outperforms state-of-the-art MKL approaches in out-of-sample prediction accuracy on average by 3.34 percentage points (relative to the best performing benchmark) while selecting a small number of candidate kernels in comparable runtime. With warm starting, our method outperforms the best performing benchmark's out-of-sample prediction accuracy on average by 4.05 percentage points. Our convex relaxations provide a certificate that in several cases, the solution returned by our best response algorithm is the globally optimal solution.

</details>


### [176] [Algorithms and Scientific Software for Quasi-Monte Carlo, Fast Gaussian Process Regression, and Scientific Machine Learning](https://arxiv.org/abs/2511.21915)
*Aleksei G. Sorokin*

Main category: stat.ML

TL;DR: 该论文开发了三个领域的算法和软件：准蒙特卡洛方法用于高效高维积分、高斯过程回归用于高维插值、科学机器学习用于偏微分方程求解，并提供了相应的开源软件工具。


<details>
  <summary>Details</summary>
Motivation: 大多数科学领域都需要高效算法和可访问的科学软件，该论文旨在统一三个重要领域的发展：准蒙特卡洛方法、高斯过程回归和科学机器学习，以解决高维积分、插值和偏微分方程求解等核心科学计算问题。

Method: 1. 准蒙特卡洛：开发向量化误差估计新算法，创建QMCPy开源Python接口，包含随机低差异序列生成器、自动变量变换、自适应误差估计等；2. 高斯过程：推导数字平移不变核函数，开发快速多任务GP算法，创建FastGPs可扩展Python软件；3. 科学机器学习：开发能够以机器精度恢复随机系数PDE的新算法。

Result: 成功开发了三个领域的完整软件生态系统：QMCPy用于准蒙特卡洛计算，FastGPs用于高斯过程回归，以及新的科学机器学习算法。这些工具已应用于多个实际问题，包括失效概率估计、达西流方程、辐射传输建模和贝叶斯多级QMC。

Conclusion: 该论文在准蒙特卡洛、高斯过程回归和科学机器学习三个领域取得了重要进展，提供了高效算法和开源软件工具，为科学计算社区提供了实用的解决方案，并在多个应用场景中展示了其有效性。

Abstract: Most scientific domains elicit the development of efficient algorithms and accessible scientific software. This thesis unifies our developments in three broad domains: Quasi-Monte Carlo (QMC) methods for efficient high-dimensional integration, Gaussian process (GP) regression for high-dimensional interpolation with built-in uncertainty quantification, and scientific machine learning (sciML) for modeling partial differential equations (PDEs) with mesh-free solvers. For QMC, we built new algorithms for vectorized error estimation and developed QMCPy (https://qmcsoftware.github.io/QMCSoftware/): an open-source Python interface to randomized low-discrepancy sequence generators, automatic variable transforms, adaptive error estimation procedures, and diverse use cases. For GPs, we derived new digitally-shift-invariant kernels of higher-order smoothness, developed novel fast multitask GP algorithms, and produced the scalable Python software FastGPs (https://alegresor.github.io/fastgps/). For sciML, we developed a new algorithm capable of machine precision recovery of PDEs with random coefficients. We have also studied a number of applications including GPs for probability of failure estimation, multilevel GPs for the Darcy flow equation, neural surrogates for modeling radiative transfer, and fast GPs for Bayesian multilevel QMC.

</details>


### [177] [A Sensitivity Approach to Causal Inference Under Limited Overlap](https://arxiv.org/abs/2511.22003)
*Yuanzhe Ma,Hongseok Namkoong*

Main category: stat.ML

TL;DR: 提出一个敏感性框架，用于评估在有限重叠情况下标准修剪方法引入的偏差，通过量化最坏情况下的置信界限来保护研究结果免受虚假发现的影响。


<details>
  <summary>Details</summary>
Motivation: 观察性研究中处理组和对照组之间的有限重叠是一个关键挑战。标准的权重修剪方法虽然能减少方差，但会引入根本性偏差。需要一种框架来评估在有限重叠情况下研究结果的稳健性。

Method: 提出敏感性框架，基于最坏情况置信界限评估标准修剪方法引入的偏差。该框架明确假设从重叠区域外推反事实估计所需的假设条件，评估结果函数需要多不规则才能使主要发现无效。

Result: 该敏感性框架能够量化有限重叠区域的不确定性，保护研究免受虚假发现的影响。通过评估偏差的最坏情况界限，为观察性分析提供更可靠的推断。

Conclusion: 提出的敏感性框架为有限重叠情况下的观察性分析提供了重要工具，通过明确量化标准修剪方法可能引入的偏差，帮助研究者更可靠地评估研究结果的稳健性。

Abstract: Limited overlap between treated and control groups is a key challenge in observational analysis. Standard approaches like trimming importance weights can reduce variance but introduce a fundamental bias. We propose a sensitivity framework for contextualizing findings under limited overlap, where we assess how irregular the outcome function has to be in order for the main finding to be invalidated. Our approach is based on worst-case confidence bounds on the bias introduced by standard trimming practices, under explicit assumptions necessary to extrapolate counterfactual estimates from regions of overlap to those without. Empirically, we demonstrate how our sensitivity framework protects against spurious findings by quantifying uncertainty in regions with limited overlap.

</details>


### [178] [On the Effect of Regularization on Nonparametric Mean-Variance Regression](https://arxiv.org/abs/2511.22004)
*Eliot Wong-Toi,Alex Boyd,Vincent Fortuin,Stephan Mandt*

Main category: stat.ML

TL;DR: 该论文研究了过参数化均值-方差回归模型中的信号-噪声模糊性问题，发现了由正则化驱动的尖锐相变现象，并提出了统计场理论框架来解释这一行为，显著降低了超参数搜索的计算成本。


<details>
  <summary>Details</summary>
Motivation: 均值-方差回归模型为机器学习中的不确定性量化提供了简单方法，但过参数化模型面临信号-噪声模糊性问题：模型难以决定预测目标应归因于信号（均值）还是噪声（方差）。这种模糊性导致模型在完美拟合训练目标（零残差噪声）和提供恒定无信息预测（将所有目标解释为噪声）两个极端之间摇摆。

Method: 通过实证研究不同正则化水平下的模型行为，观察到尖锐的相变现象。为了解释这一行为，开发了统计场理论框架来捕捉观察到的相变。该方法将正则化超参数搜索空间从二维减少到一维，显著降低了计算成本。

Result: 实验表明，过参数化均值-方差模型在不同正则化水平下表现出显著的相变行为，且重复运行间存在较大变异性。统计场理论框架与实验结果一致，成功捕捉了相变现象。在UCI数据集和大型ClimSim数据集上的实验展示了稳健的校准性能，能够有效量化预测不确定性。

Conclusion: 该研究揭示了过参数化均值-方差回归模型中信号-噪声模糊性的本质，通过统计场理论框架解释了正则化驱动的相变现象。提出的方法显著降低了超参数调优的计算成本，并在多个数据集上展示了有效的预测不确定性量化能力，为机器学习中的不确定性量化提供了理论洞察和实用工具。

Abstract: Uncertainty quantification is vital for decision-making and risk assessment in machine learning. Mean-variance regression models, which predict both a mean and residual noise for each data point, provide a simple approach to uncertainty quantification. However, overparameterized mean-variance models struggle with signal-to-noise ambiguity, deciding whether prediction targets should be attributed to signal (mean) or noise (variance). At one extreme, models fit all training targets perfectly with zero residual noise, while at the other, they provide constant, uninformative predictions and explain the targets as noise. We observe a sharp phase transition between these extremes, driven by model regularization. Empirical studies with varying regularization levels illustrate this transition, revealing substantial variability across repeated runs. To explain this behavior, we develop a statistical field theory framework, which captures the observed phase transition in alignment with experimental results. This analysis reduces the regularization hyperparameter search space from two dimensions to one, significantly lowering computational costs. Experiments on UCI datasets and the large-scale ClimSim dataset demonstrate robust calibration performance, effectively quantifying predictive uncertainty.

</details>


### [179] [Support Vector Machine Classifier with Rescaled Huberized Pinball Loss](https://arxiv.org/abs/2511.22065)
*Shibo Diao*

Main category: stat.ML

TL;DR: 本文提出了一种新型的RHPSVM模型，采用重缩放Huberized pinball损失函数，解决了传统SVM对异常值敏感和重采样不稳定的问题，在噪声和非噪声场景下都表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统SVM模型存在对异常值敏感和重采样不稳定的问题，这限制了其在实际应用中的性能表现。为了解决这些问题，需要开发一种更鲁棒的SVM变体。

Method: 提出了一种具有非对称、非凸、平滑特性的重缩放Huberized pinball损失函数，并基于此构建了RHPSVM模型。使用CCCP将非凸优化问题转化为一系列凸子问题，并用ClipDCD算法求解。

Result: 理论分析表明RHPSVM符合贝叶斯规则，具有严格的泛化误差界、有界影响函数和可控最优性条件。在模拟数据、UCI数据集和小样本作物叶片图像分类任务中，RHPSVM在噪声和非噪声场景下均优于现有SVM模型。

Conclusion: RHPSVM通过创新的损失函数设计，有效解决了传统SVM的局限性，在分类精度、异常值鲁棒性和重采样稳定性方面表现出色，特别适合处理高维小样本数据。

Abstract: Support vector machines are widely used in machine learning classification tasks, but traditional SVM models suffer from sensitivity to outliers and instability in resampling, which limits their performance in practical applications. To address these issues, this paper proposes a novel rescaled Huberized pinball loss function with asymmetric, non-convex, and smooth properties. Based on this loss function, we develop a corresponding SVM model called RHPSVM (Rescaled Huberized Pinball Loss Support Vector Machine). Theoretical analyses demonstrate that RHPSVM conforms to Bayesian rules, has a strict generalization error bound, a bounded influence function, and controllable optimality conditions, ensuring excellent classification accuracy, outlier insensitivity, and resampling stability. Additionally, RHPSVM can be extended to various advanced SVM variants by adjusting parameters, enhancing its flexibility. We transform the non-convex optimization problem of RHPSVM into a series of convex subproblems using the concave-convex procedure (CCCP) and solve it with the ClipDCD algorithm, which is proven to be convergent. Experimental results on simulated data, UCI datasets, and small-sample crop leaf image classification tasks show that RHPSVM outperforms existing SVM models in both noisy and noise-free scenarios, especially in handling high-dimensional small-sample data.

</details>


### [180] [Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs](https://arxiv.org/abs/2511.22270)
*Zhongjie Shi,Puyu Wang,Chenyang Zhang,Yuan Cao*

Main category: stat.ML

TL;DR: DP-GD（差分隐私梯度下降）在特定学习任务中可能比普通GD具有更好的泛化性能，同时提供隐私保护


<details>
  <summary>Details</summary>
Motivation: 深度学习训练数据常包含敏感信息，需要在保持良好性能的同时保护隐私，因此研究隐私保护训练算法

Method: 研究DP-GD算法，在梯度下降的每次迭代中加入噪声，分析其在两层Huberized ReLU CNN中的泛化和隐私性能

Result: 在特定条件下，小信噪比时GD产生测试精度差的模型，而DP-GD能产生测试精度好且有隐私保证的模型

Conclusion: DP-GD在某些学习任务中既能提升模型性能又能确保隐私保护，数值模拟支持了理论结果

Abstract: Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration. Moreover, we identify a concrete learning task where DP-GD can achieve superior generalization performance compared to GD in training two-layer Huberized ReLU convolutional neural networks (CNNs). Specifically, we demonstrate that, under mild conditions, a small signal-to-noise ratio can result in GD producing training models with poor test accuracy, whereas DP-GD can yield training models with good test accuracy and privacy guarantees if the signal-to-noise ratio is not too small. This indicates that DP-GD has the potential to enhance model performance while ensuring privacy protection in certain learning tasks. Numerical simulations are further conducted to support our theoretical results.

</details>


### [181] [UCB for Large-Scale Pure Exploration: Beyond Sub-Gaussianity](https://arxiv.org/abs/2511.22273)
*Zaile Li,Weiwei Fan,L. Jeff Hong*

Main category: stat.ML

TL;DR: 本文研究UCB算法在大规模非次高斯纯探索问题中的性能，提出元UCB算法并证明其在有界方差和q>3有界绝对矩两种非次高斯场景下能达到样本最优性。


<details>
  <summary>Details</summary>
Motivation: 传统纯探索方法主要依赖高斯或次高斯假设，限制了在非次高斯特别是重尾分布问题中的应用。在大规模问题中，超越次高斯性变得尤为关键，因为这些问题对分布规范特别敏感。本文旨在研究UCB算法在大规模非次高斯设置下的性能。

Method: 提出元UCB算法框架，其中每个备选方案的UCB值定义为样本均值加上仅依赖于自身样本量的探索奖励。算法停止时选择样本量最大的备选方案作为最优。分析两种非次高斯场景：1) 所有备选方案遵循共同的位置-尺度结构且有界方差；2) 当这种结构不成立时，每个备选方案具有q>3的有界绝对矩。

Result: 推导了元UCB算法正确选择概率的分布无关下界。在两种非次高斯设置下，证明元UCB算法及广泛的UCB算法类都能达到样本最优性。数值实验支持理论结果，并提供了元UCB框架内外UCB算法比较行为的额外见解。

Conclusion: UCB算法适用于解决具有非次高斯分布的大规模纯探索问题。元UCB算法在有界方差和q>3有界绝对矩两种非次高斯场景下都能达到样本最优性，扩展了UCB算法在非次高斯设置下的适用性。

Abstract: Selecting the best alternative from a finite set represents a broad class of pure exploration problems. Traditional approaches to pure exploration have predominantly relied on Gaussian or sub-Gaussian assumptions on the performance distributions of all alternatives, which limit their applicability to non-sub-Gaussian especially heavy-tailed problems. The need to move beyond sub-Gaussianity may become even more critical in large-scale problems, which tend to be especially sensitive to distributional specifications. In this paper, motivated by the widespread use of upper confidence bound (UCB) algorithms in pure exploration and beyond, we investigate their performance in the large-scale, non-sub-Gaussian settings. We consider the simplest category of UCB algorithms, where the UCB value for each alternative is defined as the sample mean plus an exploration bonus that depends only on its own sample size. We abstract this into a meta-UCB algorithm and propose letting it select the alternative with the largest sample size as the best upon stopping. For this meta-UCB algorithm, we first derive a distribution-free lower bound on the probability of correct selection. Building on this bound, we analyze two general non-sub-Gaussian scenarios: (1) all alternatives follow a common location-scale structure and have bounded variance; and (2) when such a structure does not hold, each alternative has a bounded absolute moment of order $q > 3$. In both settings, we show that the meta-UCB algorithm and therefore a broad class of UCB algorithms can achieve the sample optimality. These results demonstrate the applicability of UCB algorithms for solving large-scale pure exploration problems with non-sub-Gaussian distributions. Numerical experiments support our results and provide additional insights into the comparative behaviors of UCB algorithms within and beyond our meta-UCB framework.

</details>


### [182] [Data-driven informative priors for Bayesian inference with quasi-periodic data](https://arxiv.org/abs/2511.22296)
*Javier Lopez-Santiago,Luca Martino,Joaquin Miguez,Gonzalo Vazquez-Vilar*

Main category: stat.ML

TL;DR: 提出一种基于高斯过程周期核的贝叶斯计算策略，通过拟合周期性核的GP构建先验分布，用于改进周期性参数的后验推断。


<details>
  <summary>Details</summary>
Motivation: 周期性模型中的贝叶斯推断通常效率低下，因为周期参数的后验概率质量高度集中在参数空间的很小区域内。需要为推断方法提供尽可能多的先验信息。

Method: 使用自适应重要性采样方法近似GP超参数的后验分布，然后利用周期相关超参数的边缘后验分布构建参数模型周期的先验分布。这是一种经验贝叶斯方法，作为模块化（切割）的GP后验转移。

Result: 将提出的方法应用于合成数据和真实数据，近似了GP核周期的后验分布，并将其作为后验即先验向前传递，分析了其对边缘后验分布的影响。

Conclusion: 通过从数据构建先验分布（使用周期性核的GP拟合），可以改进周期性模型的贝叶斯推断效率，这是一种有效的经验贝叶斯工作流程。

Abstract: Bayesian computational strategies for inference can be inefficient in approximating the posterior distribution in models that exhibit some form of periodicity. This is because the probability mass of the marginal posterior distribution of the parameter representing the period is usually highly concentrated in a very small region of the parameter space. Therefore, it is necessary to provide as much information as possible to the inference method through the parameter prior distribution. We intend to show that it is possible to construct a prior distribution from the data by fitting a Gaussian process (GP) with a periodic kernel. More specifically, we want to show that it is possible to approximate the marginal posterior distribution of the hyperparameter corresponding to the period in the kernel. Subsequently, this distribution can be used as a prior distribution for the inference method. We use an adaptive importance sampling method to approximate the posterior distribution of the hyperparameters of the GP. Then, we use the marginal posterior distribution of the hyperparameter related to the periodicity in order to construct a prior distribution for the period of the parametric model. This workflow is empirical Bayes, implemented as a modular (cut) transfer of a GP posterior for the period to the parametric model. We applied the proposed methodology to both synthetic and real data. We approximated the posterior distribution of the period of the GP kernel and then passed it forward as a posterior-as-prior with no feedback. Finally, we analyzed its impact on the marginal posterior distribution.

</details>


### [183] [A PLS-Integrated LASSO Method with Application in Index Tracking](https://arxiv.org/abs/2511.23205)
*Shiqin Tang,Yining Dong,S. Joe Qin*

Main category: stat.ML

TL;DR: 提出PLS-Lasso方法，将降维直接整合到回归过程中，包含两个版本PLS-Lasso-v1和PLS-Lasso-v2，在金融指数跟踪任务中表现优于Lasso。


<details>
  <summary>Details</summary>
Motivation: 传统多元数据分析中，降维和回归被视为独立任务。主成分回归(PCR)和偏最小二乘(PLS)回归虽然计算潜在成分作为中间步骤，但采用不同标准，且降维与回归过程分离。需要一种将降维概念直接整合到回归过程中的创新方法。

Method: 提出PLS-Lasso回归方法，包含两个版本：PLS-Lasso-v1和PLS-Lasso-v2。提供了清晰有效的算法，确保收敛到全局最优解。该方法将PLS的降维思想与Lasso的正则化特性相结合。

Result: 在金融指数跟踪任务中，PLS-Lasso-v1和PLS-Lasso-v2与Lasso进行比较，显示出有希望的结果，表现优于传统Lasso方法。

Conclusion: PLS-Lasso方法成功地将降维直接整合到回归过程中，为多元数据分析提供了新的回归框架，在金融指数跟踪等应用中展现出优势。

Abstract: In traditional multivariate data analysis, dimension reduction and regression have been treated as distinct endeavors. Established techniques such as principal component regression (PCR) and partial least squares (PLS) regression traditionally compute latent components as intermediary steps -- although with different underlying criteria -- before proceeding with the regression analysis. In this paper, we introduce an innovative regression methodology named PLS-integrated Lasso (PLS-Lasso) that integrates the concept of dimension reduction directly into the regression process. We present two distinct formulations for PLS-Lasso, denoted as PLS-Lasso-v1 and PLS-Lasso-v2, along with clear and effective algorithms that ensure convergence to global optima. PLS-Lasso-v1 and PLS-Lasso-v2 are compared with Lasso on the task of financial index tracking and show promising results.

</details>


### [184] [Asymptotic Theory and Phase Transitions for Variable Importance in Quantile Regression Forests](https://arxiv.org/abs/2511.23212)
*Tomoshige Nakamura,Hiroshi Shiraishi*

Main category: stat.ML

TL;DR: QRF变量重要性推断存在挑战，本文建立了基于分位数损失风险的变量重要性渐近理论，发现了由子采样率β控制的"相变"现象，揭示了预测性能与推断有效性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 分位数回归森林广泛用于非参数条件分位数估计，但由于损失函数的非光滑性和复杂的偏差-方差权衡，变量重要性度量的统计推断仍然具有挑战性。

Method: 首先通过Knight恒等式处理不可微的分位数损失，建立QRF估计量的渐近正态性；其次分析子采样率β控制的"相变"现象；最后推导渐近偏差的显式解析形式，讨论通过解析偏差校正恢复有效推断的理论可行性。

Result: 证明了在偏差主导机制（β≥1/2，即实践中常用的大子样本量）下，标准推断失效，估计量收敛到确定性偏差常数而非零均值正态分布；推导了渐近偏差的显式解析形式。

Conclusion: 研究结果揭示了预测性能与推断有效性之间的基本权衡，为理解高维设置中随机森林推断的内在局限性提供了理论基础。

Abstract: Quantile Regression Forests (QRF) are widely used for non-parametric conditional quantile estimation, yet statistical inference for variable importance measures remains challenging due to the non-smoothness of the loss function and the complex bias-variance trade-off. In this paper, we develop a asymptotic theory for variable importance defined as the difference in pinball loss risks. We first establish the asymptotic normality of the QRF estimator by handling the non-differentiable pinball loss via Knight's identity. Second, we uncover a "phase transition" phenomenon governed by the subsampling rate $β$ (where $s \asymp n^β$). We prove that in the bias-dominated regime ($β\ge 1/2$), which corresponds to large subsample sizes typically favored in practice to maximize predictive accuracy, standard inference breaks down as the estimator converges to a deterministic bias constant rather than a zero-mean normal distribution. Finally, we derive the explicit analytic form of this asymptotic bias and discuss the theoretical feasibility of restoring valid inference via analytic bias correction. Our results highlight a fundamental trade-off between predictive performance and inferential validity, providing a theoretical foundation for understanding the intrinsic limitations of random forest inference in high-dimensional settings.

</details>


### [185] [OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning](https://arxiv.org/abs/2511.23310)
*Zixun Huang,Jiayi Sheng,Zeyu Zheng*

Main category: stat.ML

TL;DR: 该论文提出了一个统一的理论框架来分析RL后训练中策略梯度估计器的统计特性，并基于此开发了OBLR-PO算法，通过自适应学习率和基线优化提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的大语言模型后训练方法主要依赖启发式设计，缺乏系统的理论指导，这限制了我们对梯度估计器特性和优化算法的理解，阻碍了训练稳定性和整体性能的提升。

Method: 提出了一个统一的理论框架，在温和假设下分析常用策略梯度估计器的统计特性，包括无偏性、方差精确表达式和优化损失上界。基于这些结果，证明了收敛保证，推导出由梯度信噪比控制的自适应学习率调度，并发现方差最优基线是梯度加权估计器。这些理论洞见促成了OBLR-PO算法，该算法以理论为基础联合自适应学习率和基线。

Result: 在Qwen3-4B-Base和Qwen3-8B-Base模型上的实验表明，OBLR-PO相比现有策略优化方法取得了持续的性能提升，验证了理论贡献能够转化为大规模后训练的实际改进。

Conclusion: 该研究为RL后训练提供了系统的理论框架，揭示了策略梯度估计器的统计特性，并基于理论洞见开发了OBLR-PO算法，通过自适应学习率和基线优化显著提升了训练稳定性和性能，为大语言模型后训练提供了理论指导和实用工具。

Abstract: Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training.

</details>
