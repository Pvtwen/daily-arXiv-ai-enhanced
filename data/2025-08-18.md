<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 13]
- [cs.LG](#cs.LG) [Total: 59]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Distributed Integrated Sensing, Localization, and Communications over LEO Satellite Constellations](https://arxiv.org/abs/2508.11029)
*Yuchen Zhang,Francis Soualle,Musa Furkan Keskin,Yuan Liu,Linlong Wu,José A. del Peral-Rosado,Bhavani Shankar M. R.,Gonzalo Seco-Granados,Henk Wymeersch,Tareq Y. Al-Naffouri*

Main category: eess.SP

TL;DR: 本文提出了一种基于LEO卫星星座的分布式集成感知、定位和通信（DISLAC）概念，通过卫星间协作提升性能。


<details>
  <summary>Details</summary>
Motivation: LEO卫星在6G应用中面临功率、天线和处理能力的限制，需要新的解决方案。

Method: 采用分布式多输入多输出架构，通过卫星间链路实现协作。

Result: DISLAC显著提高了吞吐量、定位精度和感知鲁棒性。

Conclusion: 文章总结了关键系统级考虑，并提出了未来研究的开放方向。

Abstract: Low Earth orbit (LEO) satellite constellations are rapidly becoming essential
enablers of next-generation wireless systems, offering global broadband access,
high-precision localization, and reliable sensing beyond terrestrial coverage.
However, the inherent limitations of individual LEO satellites, including
restricted power, limited antenna aperture, and constrained onboard processing,
hinder their ability to meet the growing demands of 6G applications. To address
these challenges, this article introduces the concept of distributed integrated
sensing, localization, and communication (DISLAC) over LEO constellations,
inspired by distributed multiple input multiple output architectures. By
enabling inter-satellite cooperation through inter-satellite links, DISLAC can
substantially improve throughput, positioning accuracy, and sensing robustness.
We present illustrative case studies that quantify these benefits and analyze
key system-level considerations, including synchronization, antenna
reconfigurability, and ISL design. The article concludes by outlining open
research directions to advance the practical deployment of DISLAC in future
non-terrestrial networks.

</details>


### [2] [Multi-Satellite Cooperative MIMO Transmission: Statistical CSI-Aware RSMA Precoding Design](https://arxiv.org/abs/2508.11132)
*Sangwon Jo,Seok-Hwan Park*

Main category: eess.SP

TL;DR: 研究了多颗低地球轨道（LEO）卫星间的协作传输，提出基于统计信道信息的RSMA方案，性能接近瞬时信道信息方案。


<details>
  <summary>Details</summary>
Motivation: 提升多LEO卫星通信系统的频谱效率，解决瞬时信道信息获取困难的问题。

Method: 设计MIMO预编码和RSMA方案，利用统计信道信息，提出闭式上界近似和加权最小均方误差算法。

Result: 仿真显示，所提方案接近瞬时信道信息性能，显著优于传统空分多址。

Conclusion: 基于统计信道信息的RSMA方案在多LEO卫星系统中具有高效性和实用性。

Abstract: We investigate inter-satellite cooperative transmission in a multiple
low-Earth orbit (LEO) satellite communication system to enhance spectral
efficiency. Specifically, we design multiple-input multipleoutput (MIMO)
precoding at LEO satellites for cooperative rate-splitting multiple access
(RSMA). Given the difficulty of acquiring instantaneous channel state
information (iCSI) due to long delays and Doppler effects, we formulate an
ergodic max-min fairness rate (MMFR) maximization problem based on statistical
CSI (sCSI). To address the challenge of ergodic rate evaluation, we approximate
the problem using closed-form upper bounds and develop a weighted minimum mean
squared error-based algorithm to obtain a stationary point. Simulation results
demonstrate that the proposed sCSI-based RSMA scheme approaches iCSI-based
performance and significantly outperforms conventional space-division multiple
access.

</details>


### [3] [Near-Field Variable-Width Beam Coverage and Codebook Design for XL-RIS](https://arxiv.org/abs/2508.11178)
*Yida Zhang,Qiuyan Liu,Qiang Wang,Hongtao Luo,Yuqi Xia*

Main category: eess.SP

TL;DR: 提出了一种可变宽度波束生成算法，用于解决XL-RIS波束宽度窄的问题，并在近场码本设计中应用，提高了频谱效率和通信可靠性。


<details>
  <summary>Details</summary>
Motivation: XL-RIS的高波束增益解决了高频电磁波衰减问题，但其波束宽度较窄，增加了波束对准和广播的复杂性，需要一种新方法来解决。

Method: 提出了一种基于近场假设的可变宽度波束生成算法，并将其应用于XL-RIS的近场码本设计，支持多XL-RIS系统的联合码本生成。

Result: 仿真结果表明，该方案在码本区域内实现了更高的频谱效率和更低的通信中断概率，同时对码本区域位置和面积变化具有更好的鲁棒性。

Conclusion: 该算法有效解决了XL-RIS波束宽度窄的问题，提升了通信性能，适用于多XL-RIS系统。

Abstract: To mitigate the issue of limited base station coverage caused by severe
high-frequency electromagnetic wave attenuation, Extremely Large Reconfigurable
Intelligent Surface (XL-RIS) has garnered significant attention due to its high
beam gain. However, XL-RIS exhibits a narrower beam width compared to
traditional RIS, which increases the complexity of beam alignment and
broadcast. To address this problem, we propose a variable-width beam generation
algorithm under the near-field assumption and apply it to the near-field
codebook design for XL-RIS. Our algorithm can achieve beam coverage for
arbitrarily shaped codeword regions and generate a joint codebook for the
multi-XL-RIS system. The simulation results demonstrate that our proposed
scheme enables user equipment (UE) to achieve higher spectral efficiency and
lower communication outage probability within the codeword region compared to
existing works. Furthermore, our scheme exhibits better robustness to codeword
region location and area variations.

</details>


### [4] [KAN-HAR: A Human activity recognition based on Kolmogorov-Arnold Network](https://arxiv.org/abs/2508.11186)
*Mohammad Alikhani*

Main category: eess.SP

TL;DR: 该论文提出了一种基于Kolmogorov--Arnold Network (KAN)的人体活动识别方法，利用三轴加速度计数据，在参数效率和可解释性上优于传统深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在人体活动识别中需要大量参数调优且缺乏可解释性，KAN能够建模复杂非线性关系并提供更好的可解释性和参数效率。

Method: 使用MotionSense数据集，预处理和归一化加速度计和陀螺仪数据，通过KAN进行特征学习和分类。

Result: 实验表明，KAN在分类性能上与传统深度神经网络相当或更优，同时参数数量显著减少。

Conclusion: KAN架构是一种高效且可解释的替代方案，适用于实际人体活动识别系统。

Abstract: Human Activity Recognition (HAR) plays a critical role in numerous
applications, including healthcare monitoring, fitness tracking, and smart
environments. Traditional deep learning (DL) approaches, while effective, often
require extensive parameter tuning and may lack interpretability. In this work,
we investigate the use of a single three-axis accelerometer and the
Kolmogorov--Arnold Network (KAN) for HAR tasks, leveraging its ability to model
complex nonlinear relationships with improved interpretability and parameter
efficiency. The MotionSense dataset, containing smartphone-based motion sensor
signals across various physical activities, is employed to evaluate the
proposed approach. Our methodology involves preprocessing and normalization of
accelerometer and gyroscope data, followed by KAN-based feature learning and
classification. Experimental results demonstrate that the KAN achieves
competitive or superior classification performance compared to conventional
deep neural networks, while maintaining a significantly reduced parameter
count. This highlights the potential of KAN architectures as an efficient and
interpretable alternative for real-world HAR systems. The open-source
implementation of the proposed framework is available at the Project's GitHub
Repository.

</details>


### [5] [Enabling low-power massive MIMO with ternary ADCs for AIoT sensing](https://arxiv.org/abs/2508.11234)
*Shengheng Liu,Ningning Fu*

Main category: eess.SP

TL;DR: 论文提出了一种使用三元ADC（T-ADCs）的低功耗AIoT解决方案，通过联合导频和数据（JPD）方法优化信道估计，验证了其在绿色智能感知中的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率ADC和射频链带来的高功耗问题，为AIoT提供低成本、低功耗的信道感知方案。

Method: 采用T-ADCs和JPD方案进行信道估计，提出改进的EM和变分推断EM估计器。

Result: JPD方案有效减轻量化效应带来的性能下降，仿真验证了其在MSE和SER上的优越性。

Conclusion: T-ADCs和JPD方案在低功耗AIoT中具有实际应用潜力。

Abstract: The proliferation of networked devices and the surging demand for ubiquitous
intelligence have given rise to the artificial intelligence of things (AIoT).
However, the utilization of high-resolution analog-to-digital converters (ADCs)
and numerous radio frequency chains significantly raises power consumption.
This paper explores a cost-effective solution using ternary ADCs (T-ADCs) in
massive multiple-input-multiple-output (MIMO) systems for low-power AIoT and
specifically addresses channel sensing challenges. The channel is first
estimated through a pilot-aided scheme and refined using a joint-pilot-and-data
(JPD) approach. To assess the performance limits of this two-threshold ADC
system, the analysis includes its hardware-ideal counterpart, the parallel
one-bit ADCs (PO-ADCs) and a realistic scenario where noise variance is unknown
at the receiver is considered. Analytical findings indicate that the JPD scheme
effectively mitigates performance degradation in channel estimation due to
coarse quantization effects under mild conditions, without necessitating
additional pilot overhead. For deterministic and random channels, we propose
modified expectation maximization (EM) and variational inference EM estimators,
respectively. Extensive simulations validate the theoretical results and
demonstrate the effectiveness of the proposed estimators in terms of mean
square error and symbol error rate, which showcases the feasibility of
implementing T-ADCs and the associated JPD scheme for greener AIoT smart
sensing.

</details>


### [6] [Temporally-Similar Structure-Aware Spatiotemporal Fusion of Satellite Images](https://arxiv.org/abs/2508.11259)
*Ryosuke Isono,Shunsuke Ono*

Main category: eess.SP

TL;DR: 本文提出了一种名为TSSTF的新型时空融合框架，通过TGTV和TGEC机制解决卫星图像噪声问题，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 卫星图像在现实场景中常受噪声干扰，现有方法难以捕捉精细空间结构，导致过度平滑和伪影。

Method: TSSTF引入TGTV和TGEC机制，通过约束优化问题实现时空融合，采用预条件原始对偶分裂算法求解。

Result: 实验表明，TSSTF在无噪声条件下与现有方法相当，在噪声条件下表现更优。

Conclusion: TSSTF在噪声鲁棒性和结构保持方面优于现有方法，并提供了可复现的参数推荐。

Abstract: This paper proposes a novel spatiotemporal (ST) fusion framework for
satellite images, named Temporally-Similar Structure-Aware ST fusion (TSSTF).
ST fusion is a promising approach to address the trade-off between the spatial
and temporal resolution of satellite images. In real-world scenarios, observed
satellite images are severely degraded by noise due to measurement equipment
and environmental conditions. Consequently, some recent studies have focused on
enhancing the robustness of ST fusion methods against noise. However, existing
noise-robust ST fusion approaches often fail to capture fine spatial structure,
leading to oversmoothing and artifacts. To address this issue, TSSTF introduces
two key mechanisms: Temporally-Guided Total Variation (TGTV) and
Temporally-Guided Edge Constraint (TGEC). TGTV is a novel regularization
function that promotes spatial piecewise smoothness while preserving structural
details, guided by a reference high spatial resolution image acquired on a
nearby date. TGEC enforces consistency in edge locations between two temporally
adjacent images, while allowing for spectral variations. We formulate the ST
fusion task as a constrained optimization problem incorporating TGTV and TGEC,
and develop an efficient algorithm based on a preconditioned primal-dual
splitting method. Experimental results demonstrate that TSSTF performs
comparably to state-of-the-art methods under noise-free conditions and
outperforms them under noisy conditions. Additionally, we provide a
comprehensive set of recommended parameter values that consistently yield high
performance across diverse target regions and noise conditions, aiming to
enhance reproducibility and practical utility.

</details>


### [7] [Beyond Diagonal Reconfigurable Intelligent Surface Enabled Sensing: Cramer-Rao Bound Optimization](https://arxiv.org/abs/2508.11292)
*Xiaoqi Zhang,Liang Liu,Shuowen Zhang,Haijun Zhang*

Main category: eess.SP

TL;DR: BD-RIS在6G感知中的性能优于传统RIS，通过优化方案显著提升目标定位精度。


<details>
  <summary>Details</summary>
Motivation: 研究BD-RIS在6G感知中的潜在优势，填补其在感知领域的性能空白。

Method: 推导CRB并设计基于自适应黎曼最速上升算法的优化方案，满足非凸单位约束。

Result: 数值结果表明，BD-RIS辅助的目标定位方法具有优越的感知性能。

Conclusion: BD-RIS在6G感知中展现出显著优势，为未来无线感知技术提供新方向。

Abstract: Recently, beyond diagonal reconfigurable intelligent surface (BD-RIS) has
emerged as a more flexible solution to engineer the wireless propagation
channels, thanks to its non-diagonal reflecting matrix. Although the gain of
the BD-RIS over the conventional RIS in communication has been revealed in many
works, its gain in 6G sensing is still unknown. This motivates us to study the
BD-RIS assisted sensing in this letter. Specifically, we derive the Cramer-Rao
bound (CRB) for estimating the angle-of-arrival (AOA) from the target to the
BD-RIS under the constraint that the BD-RIS scattering matrix is unitary. To
minimize the CRB, we develop an optimization scheme based on an adaptive
Riemannian steepest ascent algorithm that can satisfy the non-convex unitary
constraint. Numerical results demonstrate that the proposed BD-RIS-assisted
target localization method achieves superior sensing performance.

</details>


### [8] [Optimizing Rate-CRB Performance for Beyond Diagonal Reconfigurable Intelligent Surface Enabled ISAC](https://arxiv.org/abs/2508.11295)
*Xiaoqi Zhang,Liang Liu,Shuowen Zhang,Weifeng Zhu,Haijun Zhang*

Main category: eess.SP

TL;DR: 论文提出了一种基于BD-RIS的集成感知与通信系统优化方法，通过流形优化算法最大化用户总速率，同时满足定位精度约束。


<details>
  <summary>Details</summary>
Motivation: 研究BD-RIS在集成感知与通信系统中的作用，优化BS波束成形和BD-RIS散射矩阵，提升系统性能。

Method: 采用对数障碍法和黎曼最速上升法解决带约束的流形优化问题。

Result: 数值结果表明算法有效，且BD-RIS系统性能优于传统RIS系统。

Conclusion: BD-RIS在集成感知与通信系统中具有显著性能优势，优化算法高效可行。

Abstract: This letter considers a beyond diagonal reconfigurable intelligent surface
(BD-RIS) aided integrated sensing and communication (ISAC) system, where the
BD-RIS can help a multi-antenna base station (BS) serve multiple user
equipments (UEs) and localize a target simultaneously. We formulate an
optimization problem that designs the BS beamforming matrix and the BD-RIS
scattering matrix to maximize UEs' sum rate subject to a localization
Cramer-Rao bound (CRB) constraint and an additional unitary matrix constraint
for the scattering matrix. Because unitary matrices form a manifold, our
problem belongs to constrained manifold optimization. This letter proposes a
log-barrier based Riemannian steepest ascent method to solve this problem
effectively. Numerical results verify the effectiveness of our algorithm and
the performance gain of the BD-RIS aided ISAC systems over the conventional RIS
aided ISAC systems.

</details>


### [9] [Important Bit Prefix M-ary Quadrature Amplitude Modulation for Semantic Communications](https://arxiv.org/abs/2508.11351)
*Haonan Lu,Rui Meng,Xiaodong Xu,Yiming Liu,Ping Zhang,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出了一种基于语义通信的IBP-MQAM方案，通过LDA提取文本语义，验证其在SemCom中优于传统MQAM。


<details>
  <summary>Details</summary>
Motivation: 为语义通信设计专用信道调制方案，提升通信性能。

Method: 提出IBP-MQAM方案，推导ISER和USER的近似表达式，利用LDA量化语义。

Result: IBP-MQAM在SemCom中表现优于MQAM，并分析了关键参数影响。

Conclusion: IBP-MQAM适用于语义通信，性能提升显著。

Abstract: M-ary Quadrature Amplitude Modulation (MQAM) is a commonly used channel
modulation technology in wireless communication systems. To achieve dedicated
channel modulation for semantic communication (SemCom), we propose an
Important-Bit-Prefixed MQAM (IBP-MQAM) scheme and derive its approximate
expression of important symbol error rate (ISER) and unimportant symbol error
rate (USER). By extracting and quantifying text semantics using Latent
Dirichlet Allocation (LDA), we verify that IBP-MQAM achieves improved
performance over MQAM in SemCom scenarios and further analyze the effects of
key system parameters.

</details>


### [10] [Importance-Aware Robust Semantic Transmission for LEO Satellite-Ground Communication](https://arxiv.org/abs/2508.11457)
*Hui Cao,Rui Meng,Xiaodong Xu,Shujun Han,Ping Zhang*

Main category: eess.SP

TL;DR: 论文提出了一种面向卫星-地面语义通信的重要性感知鲁棒语义传输（IRST）框架，以应对6G时代中动态SNR波动和带宽限制的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决低地球轨道卫星通信中任务导向数据传输的难题，特别是动态SNR波动和严格带宽限制。

Method: 采用分割模型增强算法提升语义分割精度，结合任务驱动的语义选择方法优先传输关键内容，并引入基于SNR的自适应信道编解码器。

Result: 在不同操作条件下的比较评估中，IRST模型表现出优于现有基准的性能和鲁棒性。

Conclusion: IRST框架为卫星-地面语义通信提供了一种高效、适应性强的解决方案，适用于带宽稀缺和信道多变的场景。

Abstract: Satellite-ground semantic communication is anticipated to serve a critical
role in the forthcoming 6G era. Nonetheless, task-oriented data transmission in
such systems remains a formidable challenge, primarily due to the dynamic
nature of signal-to-noise ratio (SNR) fluctuations and the stringent bandwidth
limitations inherent to low Earth orbit (LEO) satellite channels. In response
to these constraints, we propose an importance-aware robust semantic
transmission (IRST) framework, specifically designed for scenarios
characterized by bandwidth scarcity and channel variability. The IRST scheme
begins by applying a segmentation model enhancement algorithm to improve the
granularity and accuracy of semantic segmentation. Subsequently, a task-driven
semantic selection method is employed to prioritize the transmission of
semantically vital content based on real-time channel state information.
Furthermore, the framework incorporates a stack-based, SNR-aware channel codec
capable of executing adaptive channel coding in alignment with SNR variations.
Comparative evaluations across diverse operating conditions demonstrate the
superior performance and resilience of the IRST model relative to existing
benchmarks.

</details>


### [11] [Efficient Artifacts Removal for Adaptive Deep Brain Stimulation and a Temporal Event Localization Analysis](https://arxiv.org/abs/2508.11459)
*Tzu-Chi Liu,Po-Lin Chen,Yi-Chieh Chen,Po-Hsun Tu,Chih-Hua Yeh,Mun-Chun Yeap,Chiung-Chu Chen,Hau-Tieng Wu*

Main category: eess.SP

TL;DR: SMARTA+是一种改进的算法，用于高效去除自适应深部脑刺激（aDBS）中的刺激伪迹和瞬态直流伪迹，支持实时应用。


<details>
  <summary>Details</summary>
Motivation: 传统DBS存在信号污染问题，现有伪迹去除方法在灵活性和计算效率上存在局限，SMARTA+旨在解决这些问题。

Method: 开发SMARTA+，扩展SMARTA算法，提升计算效率并支持灵活设计，同时处理刺激伪迹和瞬态直流伪迹。

Result: SMARTA+在伪迹去除效果上与SMARTA相当或更优，计算时间显著减少，且保留了信号的频谱和时间结构。

Conclusion: SMARTA+是推动实时闭环aDBS系统发展的有前景的工具。

Abstract: Adaptive deep brain stimulation (aDBS) leverages symptom-related biomarkers
to deliver personalized neuromodulation therapy, with the potential to improve
treatment efficacy and reduce power consumption compared to conventional DBS.
However, stimulation-induced signal contamination remains a major technical
barrier to advancing its clinical application. Existing artifact removal
strategies, both front-end and back-end, face trade-offs between artifact
suppression and algorithmic flexibility. Among back-end algorithms, Shrinkage
and Manifold-based Artifact Removal using Template Adaptation (SMARTA) has
shown promising performance in mitigating stimulus artifacts with minimal
distortion to local field potentials (LFPs), but its high computational demand
and inability to handle transient direct current (DC) artifacts limit its use
in real-time applications. To address this, we developed SMARTA+, a
computationally efficient extension of SMARTA capable of suppressing both
stimulus and transient DC artifacts while supporting flexible algorithmic
design. We evaluated SMARTA+ using semi-real aDBS data and real data from
Parkinson's disease patients. Compared to SMARTA and other established methods,
SMARTA+ achieved comparable or superior artifact removal while significantly
reducing computation time. It preserved spectral and temporal structures,
ranging from beta band to high-frequency oscillations, and demonstrated
robustness across diverse stimulation protocols. Temporal event localization
analysis further showed improved accuracy in detecting beta bursts. These
findings support SMARTA+ as a promising tool for advancing real-time,
closed-loop aDBS systems.

</details>


### [12] [Reducing AoI and Improving Throughput for NOMA-assisted SGF Systems: A Hierarchical Learning Approach](https://arxiv.org/abs/2508.11473)
*Yuqin Liu,Mona Jaber,Yan Liu,Arumugam Nallanathan*

Main category: eess.SP

TL;DR: 提出了一种基于非正交多址（NOMA）的半免授权（SGF）框架，通过利用授权用户的剩余资源为免授权用户（GFUs）提供信道接入。采用深度强化学习（DRL）和分层学习算法优化波束成形和传输调度，提升系统吞吐量并降低GFUs的信息时效性（AoI）。


<details>
  <summary>Details</summary>
Motivation: 解决免授权用户在信道接入中的资源分配问题，提升系统效率并降低信息时效性。

Method: 1. 将问题建模为马尔可夫决策过程；2. 提出基于DRL的传输调度方法；3. 设计分层学习算法优化波束成形和传输调度。

Result: 1. DRL调度在AoI降低上优于现有方法；2. 分层学习算法提升31.82%的系统增益；3. 在GFUs数量为GBUs的1-5倍时均有效。

Conclusion: NOMA辅助的SGF框架结合DRL和分层学习算法，显著提升了系统性能和GFUs的信道接入效率。

Abstract: A non-orthogonal multiple access (NOMA) assisted semi-grant-free (SGF)
framework is proposed to enable channel access for grant-free users (GFUs) by
using residual resources from grant-based users. Under this framework, the
problem of joint beamforming design and transmission scheduling is formulated
to improve the system throughput and reduce the age-of-information of GFUs. The
aforementioned problem is transferred into a Markov Decision Process to model
the changing environment with the transmission/ waiting/ retransmission of
GFUs. In an effort to solve the pertinent problem, firstly, a deep
reinforcement learning (DRL) based transmission scheduling approach is proposed
for determining the optimal transmission probability based on the available
transmission slots and transmission status of GFUs. Secondly, a hierarchical
learning algorithm is proposed to analyze the channel state information of GBUs
and the transmission status of GFUs, and to train an upper-level policy based
on this analysis for beamforming to achieve efficient grant-based transmission,
while a lower-level policy adapts to maximize the utilization of transmission
slots allocated by the upper-level agent. The two policies interact to improve
channel access and avoid collisions. Numerical results reveal that 1) The DRL
based transmission scheduling outperforms existing adaptive and state-dependent
baselines in AoI reduction, where an average
three-time-slots-earlier-transmission can be obtained compared to the
state-dependent choice, and five time slots earlier can be achieved when
comparing to the adaptive choice; 2) The hierarchical learning algorithm is
able to achieve approximately a 31.82% gain while maintaining the average AoI
of GFUs within 1.5 time slots. 3) The effectiveness of the hierarchical
learning scheme in NOMA-assisted SGF system is validated across scenarios with
GFUs counts from 1-5 times of GBUs.

</details>


### [13] [Liquid Crystal-Based RIS Loss-Trade-Off Analysis](https://arxiv.org/abs/2508.11489)
*Bowu Wang,Mohamadreza Delbari,Robin Neuder,Alejandro Jiménez-Sáez,Vahid Jamali*

Main category: eess.SP

TL;DR: 论文研究了基于液晶技术的可重构智能表面（RIS）在毫米波频段的应用，重点分析了相位偏移范围与插入损耗之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 液晶技术因其低功耗、可扩展性和连续可调相位偏移等优势，成为毫米波频段大型RIS的潜在解决方案。然而，基于延迟线架构的LC-RIS在相位偏移范围与插入损耗之间存在权衡，这一现象尚未在LC-RIS辅助的无线系统中被研究。

Method: 研究通过配置基站（BS）和RIS，以最小化发射功率，同时满足用户的服务质量（QoS）要求，分析了LC相位偏移范围对系统性能的影响。

Result: 仿真结果表明，总发射功率与可达到的数据速率之间存在一个基本权衡关系，这一关系受LC相位偏移范围的显著影响。

Conclusion: 研究揭示了LC-RIS系统中相位偏移范围与系统性能之间的关键权衡，为未来优化设计提供了理论依据。

Abstract: Liquid crystal (LC) technology has emerged as a promising solution for large
reconfigurable intelligent surfaces (RISs) at millimeter wave (mmWave) bands,
offering advantages such as low power consumption, scalability, and
continuously tunable phase shifts. For LC-RIS based on the delay-line
architecture, i.e., with dedicated phase shifters, there exists a trade-off
between the maximum achievable phase-shift range and the corresponding
insertion loss, which has not been studied for LC-RIS-assisted wireless systems
yet. In this paper, we investigate this trade-off where a base station (BS) and
an RIS are configured to minimize the transmit power while satisfying a given
quality of service (QoS) for a number of users. Simulation results reveal a
fundamental trade-off between the total transmit power and the achievable data
rate as a function of the LC phase-shift range.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks](https://arxiv.org/abs/2508.11025)
*Laura Lützow,Michael Eichelbeck,Mykel J. Kochenderfer,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出了一种名为zono-conformal prediction的新方法，通过构建预测zonotopes来解决现有方法计算成本高和数据需求大的问题，同时提供统计覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算成本高、数据需求大，且无法有效捕捉多维输出的依赖关系。

Method: 引入zono-conformal prediction，通过线性程序直接构建zonotopic不确定性集，适用于非线性基础预测器（如神经网络）。

Result: 在回归和分类任务中，zono-conformal predictors比现有方法更少保守，同时保持相似的测试数据覆盖。

Conclusion: zono-conformal prediction是一种高效且数据友好的不确定性量化方法，优于现有技术。

Abstract: Conformal prediction is a popular uncertainty quantification method that
augments a base predictor with prediction sets with statistically valid
coverage guarantees. However, current methods are often computationally
expensive and data-intensive, as they require constructing an uncertainty model
before calibration. Moreover, existing approaches typically represent the
prediction sets with intervals, which limits their ability to capture
dependencies in multi-dimensional outputs. We address these limitations by
introducing zono-conformal prediction, a novel approach inspired by interval
predictor models and reachset-conformant identification that constructs
prediction zonotopes with assured coverage. By placing zonotopic uncertainty
sets directly into the model of the base predictor, zono-conformal predictors
can be identified via a single, data-efficient linear program. While we can
apply zono-conformal prediction to arbitrary nonlinear base predictors, we
focus on feed-forward neural networks in this work. Aside from regression
tasks, we also construct optimal zono-conformal predictors in classification
settings where the output of an uncertain predictor is a set of possible
classes. We provide probabilistic coverage guarantees and present methods for
detecting outliers in the identification data. In extensive numerical
experiments, we show that zono-conformal predictors are less conservative than
interval predictor models and standard conformal prediction methods, while
achieving a similar coverage over the test data.

</details>


### [15] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: 论文提出了一种通过收缩理论增强卷积神经常微分方程（NODEs）鲁棒性的方法，并通过正则化项和权重调整实现。


<details>
  <summary>Details</summary>
Motivation: 神经网络对输入噪声和对抗攻击脆弱，需要提升鲁棒性。

Method: 利用收缩理论，通过正则化项和权重调整使NODEs具有收缩性，从而增强鲁棒性。

Result: 在MNIST和FashionMNIST数据集上验证了方法的有效性，能抵抗噪声和攻击。

Conclusion: 提出的方法能显著提升NODEs的鲁棒性，适用于对抗环境下的图像分类任务。

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


### [16] [A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification](https://arxiv.org/abs/2508.10926)
*DongSeong-Yoon*

Main category: cs.LG

TL;DR: 论文提出了一种基于合作博弈的多准则加权方法，用于解决现有集成学习中权重分配单一的问题，并在实验中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有集成学习方法仅考虑单一评价准则，无法充分利用分类器的多种先验信息，限制了性能提升。

Method: 通过合作博弈在多准则情境下决策，同时考虑多种先验信息，实现合理的权重分配。

Result: 在Open-ML-CC18数据集上的实验表明，该方法优于其他加权方法。

Conclusion: 提出的多准则加权方法能有效提升集成学习性能，具有实际应用潜力。

Abstract: Since the Fourth Industrial Revolution, AI technology has been widely used in
many fields, but there are several limitations that need to be overcome,
including overfitting/underfitting, class imbalance, and the limitations of
representation (hypothesis space) due to the characteristics of different
models. As a method to overcome these problems, ensemble, commonly known as
model combining, is being extensively used in the field of machine learning.
Among ensemble learning methods, voting ensembles have been studied with
various weighting methods, showing performance improvements. However, the
existing methods that reflect the pre-information of classifiers in weights
consider only one evaluation criterion, which limits the reflection of various
information that should be considered in a model realistically. Therefore, this
paper proposes a method of making decisions considering various information
through cooperative games in multi-criteria situations. Using this method,
various types of information known beforehand in classifiers can be
simultaneously considered and reflected, leading to appropriate weight
distribution and performance improvement. The machine learning algorithms were
applied to the Open-ML-CC18 dataset and compared with existing ensemble
weighting methods. The experimental results showed superior performance
compared to other weighting methods.

</details>


### [17] [Apriel-Nemotron-15B-Thinker](https://arxiv.org/abs/2508.10948)
*Shruthan Radhakrishna,Soham Parikh,Gopal Sarda,Anil Turkkan,Quaizar Vohra,Raymond Li,Dhruv Jhamb,Kelechi Ogueji,Aanjaneya Shukla,Oluwanifemi Bamgbose,Toby Liang,Luke Kumar,Oleksiy Ostapenko,Shiva Krishna Reddy Malay,Aman Tiwari,Tara Bogavelli,Vikas Yadav,Jash Mehta,Saloni Mittal,Akshay Kalkunte,Pulkit Pattnaik,Khalil Slimi,Anirudh Sreeram,Jishnu Nair,Akintunde Oladipo,Shashank Maiya,Khyati Mahajan,Rishabh Maheshwary,Masoud Hashemi,Sai Rajeswar Mudumba,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sebastien Paquet,Sagar Davasam,Srinivas Sunkara*

Main category: cs.LG

TL;DR: Apriel-Nemotron-15B-Thinker是一个15B参数的模型，性能媲美32B参数模型，但内存占用仅为一半。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在内存和计算成本上的高昂问题，使其更适合企业应用。

Method: 采用四阶段训练流程：基础模型扩展、持续预训练、监督微调和GRPO强化学习。

Result: 在多项基准测试中，性能匹配或超越32B参数模型。

Conclusion: Apriel-Nemotron-15B-Thinker在性能和效率上取得了显著平衡。

Abstract: While large language models (LLMs) have achieved remarkable reasoning
capabilities across domains like code, math and other enterprise tasks, their
significant memory and computational costs often preclude their use in
practical enterprise settings. To this end, we introduce
Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow
Apriel SLM series that achieves performance against medium sized
state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while
maintaining only half the memory footprint of those alternatives.
Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline
including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised
Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive
evaluations across a diverse suite of benchmarks consistently demonstrate that
our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its
32-billion parameter counterparts, despite being less than half their size.

</details>


### [18] [Towards Efficient Prompt-based Continual Learning in Distributed Medical AI](https://arxiv.org/abs/2508.10954)
*Gyutae Oh,Jitae Shin*

Main category: cs.LG

TL;DR: 提出了一种基于提示的持续学习方法（PCL），用于解决医疗领域数据共享受限的问题，通过统一提示池和最小扩展策略，显著提升了分类准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的数据共享受到伦理、社会和制度限制，传统集中式学习难以实现，且持续学习在医疗领域的应用尚未充分探索。

Method: 采用统一提示池和最小扩展策略，通过冻结部分提示减少计算开销，并引入新的正则化项平衡知识保留和适应。

Result: 在三个糖尿病视网膜病变数据集上，PCL方法比现有方法分类准确率提升至少10%，F1分数提高9分，同时降低推理成本。

Conclusion: PCL方法为医疗AI的可持续发展提供了可能，支持实时诊断和远程医疗应用。

Abstract: Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

</details>


### [19] [Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis](https://arxiv.org/abs/2508.10967)
*Xinyi Li,Sai Wang,Yutian Lin,Yu Wu,Yi Yang*

Main category: cs.LG

TL;DR: Retro-Expert是一个可解释的逆合成框架，结合大型语言模型和专用模型的优势，通过强化学习生成自然语言解释，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有逆合成预测模型依赖静态模式匹配，缺乏逻辑决策能力，导致黑箱决策。

Method: Retro-Expert通过三个组件实现协作推理：专用模型进行浅层推理构建决策空间，大型语言模型生成预测和解释路径，强化学习优化决策策略。

Result: 实验表明，Retro-Expert在多项指标上优于现有模型，并提供专家认可的解释。

Conclusion: Retro-Expert不仅提升了预测性能，还通过可解释性弥合了AI预测与化学实践之间的鸿沟。

Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a
given product molecule, which is a fundamental task in chemical synthesis.
However, existing models rely on static pattern-matching paradigm, which limits
their ability to perform effective logic decision-making, leading to black-box
decision-making. Building on this, we propose Retro-Expert, an interpretable
retrosynthesis framework that performs collaborative reasoning by combining the
complementary reasoning strengths of Large Language Models and specialized
models via reinforcement learning. It outputs natural language explanations
grounded in chemical logic through three components: (1) specialized models
perform shallow reasoning to construct high-quality chemical decision space,
(2) LLM-driven critical reasoning to generate predictions and corresponding
interpretable reasoning path, and (3) reinforcement learning optimizing
interpretable decision policy. Experiments show that Retro-Expert not only
surpasses both LLM-based and specialized models across different metrics but
also provides expert-aligned explanations that bridge the gap between AI
predictions and actionable chemical insights.

</details>


### [20] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: BeyondWeb框架通过优化合成数据生成，显著提升了预训练性能，超越了现有合成数据集。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型预训练中数据量扩展的收益递减问题，探索合成数据的潜力。

Method: 开发BeyondWeb框架，生成高质量合成数据，并分析其优化因素。

Result: BeyondWeb在14项基准测试中平均提升5.1pp和2.6pp，训练速度提升7.7倍和2.7倍。

Conclusion: 高质量合成数据需多因素联合优化，BeyondWeb展示了其潜力。

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [21] [Conditional Independence Estimates for the Generalized Nonparanormal](https://arxiv.org/abs/2508.11050)
*Ujas Shah,Manuel Lladser,Rebecca Morrison*

Main category: cs.LG

TL;DR: 论文提出了一种广义非正态分布（广义非正态正态）的概念，证明其精度矩阵仍可推断条件独立结构，并提出了一种高效算法。


<details>
  <summary>Details</summary>
Motivation: 针对非高斯分布中精度矩阵无法直接反映变量独立结构的问题，研究如何在特定条件下仍能利用精度矩阵推断条件独立性。

Method: 通过高斯分布的对角变换定义广义非正态正态分布，提出一种计算高效的算法，利用精度矩阵恢复条件独立结构。

Result: 合成实验和实际数据应用验证了算法的有效性。

Conclusion: 广义非正态正态分布及其算法为非高斯数据的条件独立结构推断提供了新工具。

Abstract: For general non-Gaussian distributions, the covariance and precision matrices
do not encode the independence structure of the variables, as they do for the
multivariate Gaussian. This paper builds on previous work to show that for a
class of non-Gaussian distributions -- those derived from diagonal
transformations of a Gaussian -- information about the conditional independence
structure can still be inferred from the precision matrix, provided the data
meet certain criteria, analogous to the Gaussian case. We call such
transformations of the Gaussian as the generalized nonparanormal. The functions
that define these transformations are, in a broad sense, arbitrary. We also
provide a simple and computationally efficient algorithm that leverages this
theory to recover conditional independence structure from the generalized
nonparanormal data. The effectiveness of the proposed algorithm is demonstrated
via synthetic experiments and applications to real-world data.

</details>


### [22] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 本文提出了M&C框架，帮助用户从模型平台中选择最适合目标数据域的预训练文本到图像（T2I）模型，无需对所有模型进行微调。


<details>
  <summary>Details</summary>
Motivation: 预训练T2I模型在模型平台上广泛共享，但用户难以选择最适合目标数据域的模型进行微调。

Method: M&C框架通过匹配图（包含模型和数据节点及其性能/相似性边）和图嵌入特征，预测微调后性能最佳的模型。

Result: 在10个T2I模型和32个数据集上的实验表明，M&C在61.3%的情况下成功预测最佳模型，其余情况下也能预测接近最佳性能的模型。

Conclusion: M&C为预训练T2I模型选择提供了高效解决方案，显著减少了微调成本。

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [23] [Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees](https://arxiv.org/abs/2508.11112)
*Jianhao Ma,Lin Xiao*

Main category: cs.LG

TL;DR: 论文研究了分段仿射正则化（PAR）在监督学习中的优化和统计基础，证明了在过参数化情况下，PAR正则化损失函数的临界点具有高度量化特性，并提供了多种PAR的闭式近端映射求解方法。


<details>
  <summary>Details</summary>
Motivation: 解决离散或量化变量的优化问题，因其组合搜索空间的复杂性而具有挑战性。PAR提供了一种基于连续优化的灵活建模和计算框架。

Method: 研究了PAR的理论基础，包括过参数化情况下的临界点特性、闭式近端映射的推导，以及使用近端梯度法、加速变体和ADMM求解PAR正则化问题。

Result: 证明了PAR在过参数化情况下能实现高度量化，并展示了如何通过PAR近似经典正则化方法（如L1、L2和非凸正则化），同时获得类似的统计保证。

Conclusion: PAR为量化问题提供了一种有效的连续优化框架，具有理论支持和实际应用潜力。

Abstract: Optimization problems over discrete or quantized variables are very
challenging in general due to the combinatorial nature of their search space.
Piecewise-affine regularization (PAR) provides a flexible modeling and
computational framework for quantization based on continuous optimization. In
this work, we focus on the setting of supervised learning and investigate the
theoretical foundations of PAR from optimization and statistical perspectives.
First, we show that in the overparameterized regime, where the number of
parameters exceeds the number of samples, every critical point of the
PAR-regularized loss function exhibits a high degree of quantization. Second,
we derive closed-form proximal mappings for various (convex, quasi-convex, and
non-convex) PARs and show how to solve PAR-regularized problems using the
proximal gradient method, its accelerated variant, and the Alternating
Direction Method of Multipliers. Third, we study statistical guarantees of
PAR-regularized linear regression problems; specifically, we can approximate
classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex
regularizations using PAR and obtain similar statistical guarantees with
quantized solutions.

</details>


### [24] [CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention](https://arxiv.org/abs/2508.11016)
*Qingbin Li,Rongkun Xue,Jie Wang,Ming Zhou,Zhi Li,Xiaofeng Ji,Yongqi Wang,Miao Liu,Zheming Yang,Minghui Qiu,Jing Yang*

Main category: cs.LG

TL;DR: CURE框架通过两阶段方法解决RLVR中熵崩溃问题，提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法因静态初始状态采样导致熵崩溃，限制了模型性能提升。

Method: CURE采用两阶段框架：第一阶段通过高熵关键令牌重新生成优化轨迹，第二阶段继续静态采样训练以增强利用。

Result: 在数学推理任务上，CURE比DAPO表现更好，熵和准确性均达到SOTA。

Conclusion: CURE有效平衡探索与利用，显著提升模型性能。

Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have
driven the emergence of more sophisticated cognitive behaviors in large
language models (LLMs), thereby enhancing their reasoning capabilities.
However, in prior RLVR pipelines, the repeated use of static initial-state
sampling drawn exactly from the dataset distribution during each sampling phase
produced overly deterministic, low diversity model behavior, which manifested
as rapid entropy collapse and hindered sustained performance gains during
prolonged training. To address this issue, we introduce CURE
(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a
two-stage framework that balances exploration and exploitation. Specifically,
in the first stage, to deliberately steer the model toward novel yet coherent
contexts, we re-generate at high-entropy critical tokens and jointly optimize
the original and the branched trajectories. The further comparison with vanilla
DAPO shows that the regeneration process achieves a better performance on math
reasoning tasks while sustaining a high-level entropy degree for exploration.
In the second stage, we continue training with static initial-state sampling by
DAPO, intentionally placing the model in a familiar state to gradually
strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,
compared to other RLVR methods, CURE achieves a 5% performance gain across six
math benchmarks, establishing state-of-the-art performance in both entropy and
accuracy. A series of experiments further validate the effectiveness of our
approach. Code is available at https://github.com/CURE-Project/CURE.

</details>


### [25] [Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning](https://arxiv.org/abs/2508.11210)
*Minghui Sun,Matthew M. Engelhard,Benjamin A. Goldstein*

Main category: cs.LG

TL;DR: 该研究提出了一种名为“Borrowing From the Future (BFF)”的多模态对比框架，旨在通过利用后期数据提升早期儿科风险评估的预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管后期风险评估通常更精确，但临床需求要求在尽可能早的阶段做出可靠的预测。

Method: BFF将每个时间窗口视为独立模态，利用所有可用数据进行训练，并通过对比框架从后期阶段“借用”信息以监督早期学习。

Result: 在两个真实儿科预测任务中验证，BFF显著提升了早期风险评估的性能。

Conclusion: BFF为早期儿科风险评估提供了一种有效的解决方案，代码已开源。

Abstract: Risk assessments for a pediatric population are often conducted across
multiple stages. For example, clinicians may evaluate risks prenatally, at
birth, and during Well-Child visits. Although predictions made at later stages
typically achieve higher precision, it is clinically desirable to make reliable
risk assessments as early as possible. Therefore, this study focuses on
improving prediction performance in early-stage risk assessments. Our solution,
\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal
framework that treats each time window as a distinct modality. In BFF, a model
is trained on all available data throughout the time while performing a risk
assessment using up-to-date information. This contrastive framework allows the
model to ``borrow'' informative signals from later stages (e.g., Well-Child
visits) to implicitly supervise the learning at earlier stages (e.g.,
prenatal/birth stages). We validate BFF on two real-world pediatric outcome
prediction tasks, demonstrating consistent improvements in early risk
assessments. The code is available at https://github.com/scotsun/bff.

</details>


### [26] [Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis](https://arxiv.org/abs/2508.11020)
*Aakash Kumar,Emanuele Natale*

Main category: cs.LG

TL;DR: 该论文通过扩展强彩票假设（SLTH）框架，将其应用于有限精度网络，证明了在量化设置中可以精确表示目标离散神经网络，并给出了初始网络过参数化的最优界限。


<details>
  <summary>Details</summary>
Motivation: 量化是提高神经网络效率的关键技术，但理论理解有限。此前工作主要针对连续设置，无法直接应用于量化设置。

Method: 基于Borgs等人的数分割问题结果，推导了量化设置下的随机子集和问题新理论，并扩展SLTH框架到有限精度网络。

Result: 在量化设置中，目标离散神经网络可以精确表示，且初始网络的过参数化界限与目标网络的精度相关。

Conclusion: 该研究为量化神经网络提供了新的理论基础，扩展了SLTH的应用范围，并证明了量化设置下的精确表示能力。

Abstract: Quantization is an essential technique for making neural networks more
efficient, yet our theoretical understanding of it remains limited. Previous
works demonstrated that extremely low-precision networks, such as binary
networks, can be constructed by pruning large, randomly-initialized networks,
and showed that the ratio between the size of the original and the pruned
networks is at most polylogarithmic.
  The specific pruning method they employed inspired a line of theoretical work
known as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights
from the Random Subset Sum Problem. However, these results primarily address
the continuous setting and cannot be applied to extend SLTH results to the
quantized setting.
  In this work, we build on foundational results by Borgs et al. on the Number
Partitioning Problem to derive new theoretical results for the Random Subset
Sum Problem in a quantized setting.
  Using these results, we then extend the SLTH framework to finite-precision
networks. While prior work on SLTH showed that pruning allows approximation of
a certain class of neural networks, we demonstrate that, in the quantized
setting, the analogous class of target discrete neural networks can be
represented exactly, and we prove optimal bounds on the necessary
overparameterization of the initial network as a function of the precision of
the target network.

</details>


### [27] [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](https://arxiv.org/abs/2508.11460)
*Aurora Grefsrud,Nello Blaser,Trygve Buanes*

Main category: cs.LG

TL;DR: 该研究通过近似贝叶斯推断框架和合成数据集测试，评估了六种概率机器学习算法在类别概率和不确定性估计中的表现，发现深度学习算法在分布外数据上的不确定性估计存在不足。


<details>
  <summary>Details</summary>
Motivation: 随着数据模型（如深度学习）的复杂性增加，不确定性量化变得困难，需要验证不同算法的不确定性估计是否满足科学建模的需求。

Method: 采用近似贝叶斯推断框架，并在合成分类数据集上测试六种算法（包括神经网络集成、证据深度学习等），评估其不确定性估计的校准性和分布外数据表现。

Result: 所有算法在校准性上表现良好，但深度学习算法在分布外数据上的不确定性估计未能一致反映实验证据的缺乏。

Conclusion: 研究为开发新的不确定性估计方法提供了参考，强调了深度学习算法在分布外数据上的局限性。

Abstract: Rigorous statistical methods, including parameter estimation with
accompanying uncertainties, underpin the validity of scientific discovery,
especially in the natural sciences. With increasingly complex data models such
as deep learning techniques, uncertainty quantification has become exceedingly
difficult and a plethora of techniques have been proposed. In this case study,
we use the unifying framework of approximate Bayesian inference combined with
empirical tests on carefully created synthetic classification datasets to
investigate qualitative properties of six different probabilistic machine
learning algorithms for class probability and uncertainty estimation: (i) a
neural network ensemble, (ii) neural network ensemble with conflictual loss,
(iii) evidential deep learning, (iv) a single neural network with Monte Carlo
Dropout, (v) Gaussian process classification and (vi) a Dirichlet process
mixture model. We check if the algorithms produce uncertainty estimates which
reflect commonly desired properties, such as being well calibrated and
exhibiting an increase in uncertainty for out-of-distribution data points. Our
results indicate that all algorithms are well calibrated, but none of the deep
learning based algorithms provide uncertainties that consistently reflect lack
of experimental evidence for out-of-distribution data points. We hope our study
may serve as a clarifying example for researchers developing new methods of
uncertainty estimation for scientific data-driven modeling.

</details>


### [28] [Learning with Confidence](https://arxiv.org/abs/2508.11037)
*Oliver Ethan Richardson*

Main category: cs.LG

TL;DR: 论文探讨了学习或更新信念中的“信心”概念，区分了它与概率或似然的不同，并提出了两种测量方法。


<details>
  <summary>Details</summary>
Motivation: 研究学习过程中对信息的信任程度及其对信念状态的影响，填补了信心与概率概念混淆的空白。

Method: 通过形式化公理化学习中的信心概念，提出两种连续测量方法，并证明其普适性。在附加假设下，用向量场和损失函数表示信心学习。

Result: 证明了信心可以统一表示，并推导出基于向量场和损失函数的紧凑表示。贝叶斯规则被特化为优化学习者的线性期望。

Conclusion: 信心是独立于概率的重要概念，其形式化表示为学习理论提供了新视角。

Abstract: We characterize a notion of confidence that arises in learning or updating
beliefs: the amount of trust one has in incoming information and its impact on
the belief state. This learner's confidence can be used alongside (and is
easily mistaken for) probability or likelihood, but it is fundamentally a
different concept -- one that captures many familiar concepts in the
literature, including learning rates and number of training epochs, Shafer's
weight of evidence, and Kalman gain. We formally axiomatize what it means to
learn with confidence, give two canonical ways of measuring confidence on a
continuum, and prove that confidence can always be represented in this way.
Under additional assumptions, we derive more compact representations of
confidence-based learning in terms of vector fields and loss functions. These
representations induce an extended language of compound "parallel"
observations. We characterize Bayes Rule as the special case of an optimizing
learner whose loss representation is a linear expectation.

</details>


### [29] [SHLIME: Foiling adversarial attacks fooling SHAP and LIME](https://arxiv.org/abs/2508.11053)
*Sam Chauhan,Estelle Duguet,Karthik Ramakrishnan,Hugh Van Deventer,Jack Kruger,Ranjan Subbaraman*

Main category: cs.LG

TL;DR: 研究探讨了LIME和SHAP等后解释方法在对抗性操纵下的脆弱性，并提出改进策略以增强其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 后解释方法（如LIME和SHAP）被广泛用于评估模型偏见和泛化性，但它们易受对抗性操纵，可能掩盖有害偏见。

Method: 通过复制COMPAS实验建立基线，引入模块化测试框架，系统评估增强和集成解释方法在不同性能分类器上的表现。

Result: 发现某些LIME/SHAP集成配置能显著提高偏见检测能力。

Conclusion: 这些改进方法有望提升高风险机器学习系统部署的透明度。

Abstract: Post hoc explanation methods, such as LIME and SHAP, provide interpretable
insights into black-box classifiers and are increasingly used to assess model
biases and generalizability. However, these methods are vulnerable to
adversarial manipulation, potentially concealing harmful biases. Building on
the work of Slack et al. (2020), we investigate the susceptibility of LIME and
SHAP to biased models and evaluate strategies for improving robustness. We
first replicate the original COMPAS experiment to validate prior findings and
establish a baseline. We then introduce a modular testing framework enabling
systematic evaluation of augmented and ensemble explanation approaches across
classifiers of varying performance. Using this framework, we assess multiple
LIME/SHAP ensemble configurations on out-of-distribution models, comparing
their resistance to bias concealment against the original methods. Our results
identify configurations that substantially improve bias detection, highlighting
their potential for enhancing transparency in the deployment of high-stakes
machine learning systems.

</details>


### [30] [Abundance-Aware Set Transformer for Microbiome Sample Embedding](https://arxiv.org/abs/2508.11075)
*Hyunwoo Yoo,Gail Rosen*

Main category: cs.LG

TL;DR: 提出了一种基于丰度感知的Set Transformer方法，用于构建微生物组样本的固定大小嵌入表示，通过加权序列嵌入提高分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽略微生物序列的丰度信息，而丰度在生物学上具有重要意义，因此需要一种能够整合丰度信息的嵌入表示方法。

Method: 采用丰度感知的Set Transformer，通过加权序列嵌入（根据相对丰度）并应用自注意力机制进行聚合，构建样本级嵌入。

Result: 在真实微生物组分类任务中，该方法优于平均池化和未加权的Set Transformer，某些情况下实现完美性能。

Conclusion: 丰度感知的聚合方法能够提供更稳健且生物学意义明确的微生物组表示，是首个将序列丰度整合到基于Transformer的样本嵌入中的方法之一。

Abstract: Microbiome sample representation to input into LLMs is essential for
downstream tasks such as phenotype prediction and environmental classification.
While prior studies have explored embedding-based representations of each
microbiome sample, most rely on simple averaging over sequence embeddings,
often overlooking the biological importance of taxa abundance. In this work, we
propose an abundance-aware variant of the Set Transformer to construct
fixed-size sample-level embeddings by weighting sequence embeddings according
to their relative abundance. Without modifying the model architecture, we
replicate embedding vectors proportional to their abundance and apply
self-attention-based aggregation. Our method outperforms average pooling and
unweighted Set Transformers on real-world microbiome classification tasks,
achieving perfect performance in some cases. These results demonstrate the
utility of abundance-aware aggregation for robust and biologically informed
microbiome representation. To the best of our knowledge, this is one of the
first approaches to integrate sequence-level abundance into Transformer-based
sample embeddings.

</details>


### [31] [A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora](https://arxiv.org/abs/2508.11084)
*Thanasis Schoinas,Ghulam Qadir*

Main category: cs.LG

TL;DR: 本文提出了一种经济可行的预测编码解决方案，通过数据管理流程将即时消息分组为日聊天，结合特征选择和逻辑回归分类器，并通过降维提升性能。


<details>
  <summary>Details</summary>
Motivation: 即时消息的非正式性和小规模为法律行业中的文档分类（预测编码）带来挑战，需经济可行的解决方案。

Method: 将消息分组为日聊天，进行特征选择和逻辑回归分类，并通过降维优化定量特征。

Result: 在Instant Bloomberg数据集上测试，方法有效且节省成本。

Conclusion: 提出的方法为即时消息预测编码提供了经济高效的解决方案，并通过降维提升了性能。

Abstract: Predictive coding, the term used in the legal industry for document
classification using machine learning, presents additional challenges when the
dataset comprises instant messages, due to their informal nature and smaller
sizes. In this paper, we exploit a data management workflow to group messages
into day chats, followed by feature selection and a logistic regression
classifier to provide an economically feasible predictive coding solution. We
also improve the solution's baseline model performance by dimensionality
reduction, with focus on quantitative features. We test our methodology on an
Instant Bloomberg dataset, rich in quantitative information. In parallel, we
provide an example of the cost savings of our approach.

</details>


### [32] [Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation](https://arxiv.org/abs/2508.11086)
*Emily Liu,Kuan Han,Minfeng Zhan,Bocheng Zhao,Guanyu Mu,Yang Song*

Main category: cs.LG

TL;DR: 提出了一种基于相对优势的去偏框架，通过比较用户和物品组的参考分布来校正观看时间，提高了推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 原始观看时间受视频时长、流行度和用户行为等混杂因素影响，可能导致推荐模型偏差。

Method: 采用两阶段架构，分离分布估计和偏好学习，并使用分布嵌入参数化观看时间分位数。

Result: 离线和在线实验显示，该方法在推荐准确性和鲁棒性上显著优于基线方法。

Conclusion: 提出的框架有效解决了观看时间偏差问题，提升了推荐系统的性能。

Abstract: Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

</details>


### [33] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Giró-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经网络的压缩元学习框架，用于改进压缩学习的编码和解码阶段，提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模的快速扩大，需要快速高效的参数学习技术。现有的压缩学习方法未能充分利用数据的底层结构。

Method: 通过神经网络元学习压缩学习的编码和解码阶段，提出压缩元学习框架。

Result: 实验展示了该框架在多种应用中的潜力，包括压缩PCA、压缩岭回归、压缩k-means和自编码器。

Conclusion: 压缩元学习框架在效率和准确性上优于现有方法，具有广泛应用前景。

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


### [34] [Predictive Multimodal Modeling of Diagnoses and Treatments in EHR](https://arxiv.org/abs/2508.11092)
*Cindy Shih-Ting Huang,Clarence Boon Liang Ng,Marek Rei*

Main category: cs.LG

TL;DR: 提出了一种多模态系统，用于早期预测ICD代码，融合临床记录和表格数据，通过预训练编码器和跨模态注意力提升性能。


<details>
  <summary>Details</summary>
Motivation: 早期预测ICD代码有助于识别健康风险、优化治疗和资源分配，但现有研究多关注出院后分类。

Method: 采用多模态系统，结合临床记录和表格数据，使用预训练编码器、特征池化和跨模态注意力学习最优表示，并引入加权时间损失。

Result: 实验表明，该方法在早期预测任务中优于现有最佳系统。

Conclusion: 多模态融合和加权时间损失策略有效提升了早期ICD代码预测的准确性。

Abstract: While the ICD code assignment problem has been widely studied, most works
have focused on post-discharge document classification. Models for early
forecasting of this information could be used for identifying health risks,
suggesting effective treatments, or optimizing resource allocation. To address
the challenge of predictive modeling using the limited information at the
beginning of a patient stay, we propose a multimodal system to fuse clinical
notes and tabular events captured in electronic health records. The model
integrates pre-trained encoders, feature pooling, and cross-modal attention to
learn optimal representations across modalities and balance their presence at
every temporal point. Moreover, we present a weighted temporal loss that
adjusts its contribution at each point in time. Experiments show that these
strategies enhance the early prediction model, outperforming the current
state-of-the-art systems.

</details>


### [35] [Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation](https://arxiv.org/abs/2508.11105)
*Sajjad Saed,Babak Teimourpour*

Main category: cs.LG

TL;DR: FGAT框架通过图神经网络和注意力机制，结合视觉和文本特征，同时建模服装兼容性和用户偏好，显著提升时尚推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 时尚产业快速扩张，用户难以在电商平台找到兼容商品，现有研究常独立处理服装兼容性和个性化推荐，忽略了复杂交互。

Method: 提出FGAT框架，构建用户、服装和物品的三层层次图，利用图注意力机制动态加权节点重要性，整合视觉和文本特征。

Result: 在POG数据集上，FGAT在精度、HR、召回率、NDCG和准确率上优于基线模型HFGN。

Conclusion: 结合多模态特征、层次图结构和注意力机制，显著提升个性化时尚推荐系统的准确性和效率。

Abstract: The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

</details>


### [36] [CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets](https://arxiv.org/abs/2508.11144)
*Gauri Jain,Dominik Rothenhäusler,Kirk Bansak,Elisabeth Paulson*

Main category: cs.LG

TL;DR: 论文提出了一种名为CTRL的元学习方法，旨在在多个数据源中同时提高整体预测准确性并保留源间异质性。


<details>
  <summary>Details</summary>
Motivation: 在机器学习任务中，数据通常来自多个不同来源，需要保证预测的全局准确性同时保留源间差异。这在难民安置等应用中尤为重要。

Method: CTRL结合了跨域残差学习和自适应池化/聚类技术，通过理论分析权衡数据数量与质量。

Result: 在5个大规模数据集（包括瑞士国家庇护计划数据）上，CTRL在多个关键指标上优于现有基准方法。

Conclusion: CTRL能有效处理多源数据中的分布偏移和样本量差异，适用于需要保留源间差异的应用场景。

Abstract: Machine learning (ML) tasks often utilize large-scale data that is drawn from
several distinct sources, such as different locations, treatment arms, or
groups. In such settings, practitioners often desire predictions that not only
exhibit good overall accuracy, but also remain reliable within each source and
preserve the differences that matter across sources. For instance, several
asylum and refugee resettlement programs now use ML-based employment
predictions to guide where newly arriving families are placed within a host
country, which requires generating informative and differentiated predictions
for many and often small source locations. However, this task is made
challenging by several common characteristics of the data in these settings:
the presence of numerous distinct data sources, distributional shifts between
them, and substantial variation in sample sizes across sources. This paper
introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method
that combines the strengths of cross-domain residual learning and adaptive
pooling/clustering in order to simultaneously improve overall accuracy and
preserve source-level heterogeneity. We provide theoretical results that
clarify how our objective navigates the trade-off between data quantity and
data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5
large-scale datasets. This includes a dataset from the national asylum program
in Switzerland, where the algorithmic geographic assignment of asylum seekers
is currently being piloted. CTRL consistently outperforms the benchmarks across
several key metrics and when using a range of different base learners.

</details>


### [37] [Towards the Next-generation Bayesian Network Classifiers](https://arxiv.org/abs/2508.11145)
*Huan Zhang,Daokun Zhang,Kexin Meng,Geoffrey I. Webb*

Main category: cs.LG

TL;DR: 提出了一种基于分布表示学习的高阶贝叶斯网络分类器NeuralKDB，通过神经网络架构学习特征值的分布表示，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯网络分类器因参数爆炸和数据稀疏问题，难以建模高阶特征依赖，限制了其在复杂数据中的表现。

Method: 通过分布表示学习捕捉特征间的语义相关性，设计神经网络架构NeuralKDB，并基于随机梯度下降算法高效训练模型。

Result: 在60个UCI数据集上的实验表明，NeuralKDB能有效捕捉高阶特征依赖，性能显著优于传统贝叶斯网络分类器及其他竞争方法。

Conclusion: 分布表示学习为贝叶斯网络分类器设计提供了新范式，显著提升了其在高阶特征依赖建模中的能力。

Abstract: Bayesian network classifiers provide a feasible solution to tabular data
classification, with a number of merits like high time and memory efficiency,
and great explainability. However, due to the parameter explosion and data
sparsity issues, Bayesian network classifiers are restricted to low-order
feature dependency modeling, making them struggle in extrapolating the
occurrence probabilities of complex real-world data. In this paper, we propose
a novel paradigm to design high-order Bayesian network classifiers, by learning
distributional representations for feature values, as what has been done in
word embedding and graph representation learning. The learned distributional
representations are encoded with the semantic relatedness between different
features through their observed co-occurrence patterns in training data, which
then serve as a hallmark to extrapolate the occurrence probabilities of new
test samples. As a classifier design realization, we remake the K-dependence
Bayesian classifier (KDB) by extending it into a neural version, i.e.,
NeuralKDB, where a novel neural network architecture is designed to learn
distributional representations of feature values and parameterize the
conditional probabilities between interdependent features. A stochastic
gradient descent based algorithm is designed to train the NeuralKDB model
efficiently. Extensive classification experiments on 60 UCI datasets
demonstrate that the proposed NeuralKDB classifier excels in capturing
high-order feature dependencies and significantly outperforms the conventional
Bayesian network classifiers, as well as other competitive classifiers,
including two neural network based classifiers without distributional
representation learning.

</details>


### [38] [Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning](https://arxiv.org/abs/2508.11159)
*Heqiang Wang,Weihong Yang,Xiaoxiong Zhong,Jia Zhou,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: 论文研究了物联网（IoT）中多模态在线联邦学习（MMO-FL）的模态数量与质量不平衡（QQI）问题，并提出了一种名为QQR的算法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着边缘智能的发展，IoT设备需要处理异构多模态数据，但设备的不稳定性导致数据收集时出现模态数量与质量不平衡（QQI），影响学习性能。

Method: 论文提出了一种基于原型学习的Modality Quantity and Quality Rebalanced (QQR)算法，用于在训练过程中动态平衡模态不平衡。

Result: 在两个真实多模态数据集上的实验表明，QQR算法在模态不平衡条件下优于基准方法。

Conclusion: QQR算法有效解决了MMO-FL中的模态不平衡问题，提升了学习性能。

Abstract: The Internet of Things (IoT) ecosystem produces massive volumes of multimodal
data from diverse sources, including sensors, cameras, and microphones. With
advances in edge intelligence, IoT devices have evolved from simple data
acquisition units into computationally capable nodes, enabling localized
processing of heterogeneous multimodal data. This evolution necessitates
distributed learning paradigms that can efficiently handle such data.
Furthermore, the continuous nature of data generation and the limited storage
capacity of edge devices demand an online learning framework. Multimodal Online
Federated Learning (MMO-FL) has emerged as a promising approach to meet these
requirements. However, MMO-FL faces new challenges due to the inherent
instability of IoT devices, which often results in modality quantity and
quality imbalance (QQI) during data collection. In this work, we systematically
investigate the impact of QQI within the MMO-FL framework and present a
comprehensive theoretical analysis quantifying how both types of imbalance
degrade learning performance. To address these challenges, we propose the
Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning
based method designed to operate in parallel with the training process.
Extensive experiments on two real-world multimodal datasets show that the
proposed QQR algorithm consistently outperforms benchmarks under modality
imbalance conditions with promising learning performance.

</details>


### [39] [A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels](https://arxiv.org/abs/2508.11180)
*Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: 提出了一种半监督生成模型，结合信息瓶颈原则和跨视图互信息最大化，解决了多视图学习中的缺失视图和标签问题。


<details>
  <summary>Details</summary>
Motivation: 多视图学习中存在缺失视图和标签的问题，现有方法无法充分利用未标记数据。

Method: 提出半监督生成模型，结合信息瓶颈原则和跨视图互信息最大化，利用标记和未标记数据。

Result: 在图像和多组学数据上，模型在预测和填补缺失视图方面优于现有方法。

Conclusion: 该方法有效解决了多视图学习中的缺失问题，提升了性能。

Abstract: Multi-view learning is widely applied to real-life datasets, such as multiple
omics biological data, but it often suffers from both missing views and missing
labels. Prior probabilistic approaches addressed the missing view problem by
using a product-of-experts scheme to aggregate representations from present
views and achieved superior performance over deterministic classifiers, using
the information bottleneck (IB) principle. However, the IB framework is
inherently fully supervised and cannot leverage unlabeled data. In this work,
we propose a semi-supervised generative model that utilizes both labeled and
unlabeled samples in a unified framework. Our method maximizes the likelihood
of unlabeled samples to learn a latent space shared with the IB on labeled
data. We also perform cross-view mutual information maximization in the latent
space to enhance the extraction of shared information across views. Compared to
existing approaches, our model achieves better predictive and imputation
performance on both image and multi-omics data with missing views and limited
labeled samples.

</details>


### [40] [Quantum-Boosted High-Fidelity Deep Learning](https://arxiv.org/abs/2508.11190)
*Feng-ao Wang,Shaobo Chen,Yao Xuan,Junwei Liu,Qi Gao,Hongdong Zhu,Junjie Hou,Lixin Yuan,Jinyu Cheng,Chenxin Yi,Hai Wei,Yin Ma,Tao Xu,Kai Wen,Yixue Li*

Main category: cs.LG

TL;DR: 论文提出了一种量子-经典混合架构QBM-VAE，利用量子处理器从玻尔兹曼分布中高效采样，解决了传统高斯先验在捕捉复杂非高斯数据时的局限性，显著提升了生物数据建模的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统概率深度学习主要依赖高斯先验，无法准确捕捉复杂非高斯数据（如生物数据）的分布，限制了科学发现的准确性。玻尔兹曼分布虽更具表达力，但经典计算机难以处理。

Method: 提出QBM-VAE，结合量子处理器和经典深度学习，利用量子计算高效采样玻尔兹曼分布作为先验，构建生成模型。

Result: 在百万级单细胞数据集上，QBM-VAE生成的潜在空间更好地保留了复杂生物结构，在数据整合、细胞分类和轨迹推断等任务中优于传统高斯模型（如VAE和SCVI）。

Conclusion: QBM-VAE展示了量子计算在深度学习中的实际优势，为开发混合量子AI模型提供了可转移的蓝图，同时突破了数据限制，增强了科学发现能力。

Abstract: A fundamental limitation of probabilistic deep learning is its predominant
reliance on Gaussian priors. This simplistic assumption prevents models from
accurately capturing the complex, non-Gaussian landscapes of natural data,
particularly in demanding domains like complex biological data, severely
hindering the fidelity of the model for scientific discovery. The
physically-grounded Boltzmann distribution offers a more expressive
alternative, but it is computationally intractable on classical computers. To
date, quantum approaches have been hampered by the insufficient qubit scale and
operational stability required for the iterative demands of deep learning.
Here, we bridge this gap by introducing the Quantum Boltzmann
Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable
hybrid quantum-classical architecture. Our framework leverages a quantum
processor for efficient sampling from the Boltzmann distribution, enabling its
use as a powerful prior within a deep generative model. Applied to
million-scale single-cell datasets from multiple sources, the QBM-VAE generates
a latent space that better preserves complex biological structures,
consistently outperforming conventional Gaussian-based deep learning models
like VAE and SCVI in essential tasks such as omics data integration, cell-type
classification, and trajectory inference. It also provides a typical example of
introducing a physics priori into deep learning to drive the model to acquire
scientific discovery capabilities that breaks through data limitations. This
work provides the demonstration of a practical quantum advantage in deep
learning on a large-scale scientific problem and offers a transferable
blueprint for developing hybrid quantum AI models.

</details>


### [41] [Meta-learning Structure-Preserving Dynamics](https://arxiv.org/abs/2508.11205)
*Cheng Jing,Uvini Balasuriya Mudiyanselage,Woojin Cho,Minju Jo,Anthony Gruber,Kookjin Lee*

Main category: cs.LG

TL;DR: 提出了一种基于调制的元学习框架，用于结构保持的动态建模，无需显式系统参数或重新训练，适用于参数变化的场景。


<details>
  <summary>Details</summary>
Motivation: 传统结构保持模型需要固定系统配置和显式参数，限制了其在多查询或参数变化场景中的应用。元学习虽能解决，但现有方法存在训练不稳定或泛化能力不足的问题。

Method: 引入调制策略，将结构保持模型直接条件化于潜在表示上，避免显式优化和系统知识需求。

Result: 实验表明，该方法在少样本学习中能准确预测，同时保持物理约束和泛化性能。

Conclusion: 调制框架为参数化动态系统提供了可扩展和泛化的学习方案。

Abstract: Structure-preserving approaches to dynamics modeling have demonstrated great
potential for modeling physical systems due to their strong inductive biases
that enforce conservation laws and dissipative behavior. However, the resulting
models are typically trained for fixed system configurations, requiring
explicit knowledge of system parameters as well as costly retraining for each
new set of parameters -- a major limitation in many-query or parameter-varying
scenarios. Meta-learning offers a potential solution, but existing approaches
like optimization-based meta-learning often suffer from training instability or
limited generalization capability. Inspired by ideas from computer vision, we
introduce a modulation-based meta-learning framework that directly conditions
structure-preserving models on compact latent representations of potentially
unknown system parameters, avoiding the need for gray-box system knowledge and
explicit optimization during adaptation. Through the application of novel
modulation strategies to parametric energy-conserving and dissipative systems,
we enable scalable and generalizable learning across parametric families of
dynamical systems. Experiments on standard benchmark problems demonstrate that
our approach achieves accurate predictions in few-shot learning settings,
without compromising on the essential physical constraints necessary for
dynamical stability and effective generalization performance across parameter
space.

</details>


### [42] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: 论文探讨了认知行为的解释如何通过计算和表示来实现，并提出了因果抽象理论作为分析工具。


<details>
  <summary>Details</summary>
Motivation: 研究旨在明确系统如何实现特定计算及表示，结合深度学习探讨哲学与机器学习的联系。

Method: 采用因果抽象理论作为分析框架，结合深度学习案例进行研究。

Result: 提出了基于因果抽象的计算实现理论，并讨论了表示在其中的作用。

Conclusion: 该研究为计算与表示的实现提供了新视角，并建议在泛化和预测中进一步探讨。

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [43] [Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM](https://arxiv.org/abs/2508.11215)
*Zicheng Guo,Shuqi Wu,Meixing Zhu,He Guandi*

Main category: cs.LG

TL;DR: 提出了一种基于CNN-LSTM混合架构的PM2.5浓度预测模型，结合空间和时间特征提取，在实验中表现优于传统时间序列模型。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化加剧，准确预测PM2.5浓度对环境保护、公共健康和城市管理至关重要。

Method: 使用CNN提取局部空间特征，LSTM建模时间序列依赖，基于北京工业区2010-2015年的多变量数据集进行预测。

Result: 模型在6小时平均PM2.5浓度预测中，RMSE为5.236，优于传统模型。

Conclusion: 模型在现实应用中潜力大，但计算资源需求高，未来需优化处理多变量能力和扩展复杂天气预测任务。

Abstract: With the intensification of global climate change, accurate prediction of air
quality indicators, especially PM2.5 concentration, has become increasingly
important in fields such as environmental protection, public health, and urban
management. To address this, we propose an air quality PM2.5 index prediction
model based on a hybrid CNN-LSTM architecture. The model effectively combines
Convolutional Neural Networks (CNN) for local spatial feature extraction and
Long Short-Term Memory (LSTM) networks for modeling temporal dependencies in
time series data. Using a multivariate dataset collected from an industrial
area in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5
concentration, temperature, dew point, pressure, wind direction, wind speed,
and precipitation -- the model predicts the average PM2.5 concentration over
6-hour intervals. Experimental results show that the model achieves a root mean
square error (RMSE) of 5.236, outperforming traditional time series models in
both accuracy and generalization. This demonstrates its strong potential in
real-world applications such as air pollution early warning systems. However,
due to the complexity of multivariate inputs, the model demands high
computational resources, and its ability to handle diverse atmospheric factors
still requires optimization. Future work will focus on enhancing scalability
and expanding support for more complex multivariate weather prediction tasks.

</details>


### [44] [Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories](https://arxiv.org/abs/2508.11235)
*William Alemanni,Arianna Burzacchi,Davide Colombi,Elena Giarratano*

Main category: cs.LG

TL;DR: 本文提出了一种改进的交互式投票地图匹配算法，旨在高效处理不同采样率的轨迹数据，提高GPS轨迹重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决GPS轨迹数据因采样率不同或数据质量差而难以准确匹配到道路网络的问题。

Method: 在原有算法基础上，整合轨迹插补，采用距离限制的交互式投票策略降低计算复杂度，并改进以应对道路网络缺失数据。

Result: 算法在保持原有优势的同时，显著提升了适用性，可广泛应用于OpenStreetMap覆盖的任何地理区域。

Conclusion: 改进后的算法在多样化的实际场景中表现出色，扩展了原有算法的应用范围。

Abstract: This paper presents an enhanced version of the Interactive Voting-Based Map
Matching algorithm, designed to efficiently process trajectories with varying
sampling rates. The main aim is to reconstruct GPS trajectories with high
accuracy, independent of input data quality. Building upon the original
algorithm, developed exclusively for aligning GPS signals to road networks, we
extend its capabilities by integrating trajectory imputation. Our improvements
also include the implementation of a distance-bounded interactive voting
strategy to reduce computational complexity, as well as modifications to
address missing data in the road network. Furthermore, we incorporate a
custom-built asset derived from OpenStreetMap, enabling this approach to be
smoothly applied in any geographic region covered by OpenStreetMap's road
network. These advancements preserve the core strengths of the original
algorithm while significantly extending its applicability to diverse real-world
scenarios.

</details>


### [45] [Graph Neural Diffusion via Generalized Opinion Dynamics](https://arxiv.org/abs/2508.11249)
*Asela Hevapathige,Asiri Wijesinghe,Ahad N. Zehmakan*

Main category: cs.LG

TL;DR: GODNF是一个基于扩散的图神经网络框架，解决了现有方法在适应性、深度和理论理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有扩散基GNN方法存在适应性差、深度受限和理论理解不足的问题。

Method: 提出GODNF框架，统一多种意见动态模型，支持异构扩散和动态邻域影响。

Result: 理论分析和实验验证表明GODNF优于现有GNN方法。

Conclusion: GODNF为扩散基GNN提供了高效、可解释且适应性强的解决方案。

Abstract: There has been a growing interest in developing diffusion-based Graph Neural
Networks (GNNs), building on the connections between message passing mechanisms
in GNNs and physical diffusion processes. However, existing methods suffer from
three critical limitations: (1) they rely on homogeneous diffusion with static
dynamics, limiting adaptability to diverse graph structures; (2) their depth is
constrained by computational overhead and diminishing interpretability; and (3)
theoretical understanding of their convergence behavior remains limited. To
address these challenges, we propose GODNF, a Generalized Opinion Dynamics
Neural Framework, which unifies multiple opinion dynamics models into a
principled, trainable diffusion mechanism. Our framework captures heterogeneous
diffusion patterns and temporal dynamics via node-specific behavior modeling
and dynamic neighborhood influence, while ensuring efficient and interpretable
message propagation even at deep layers. We provide a rigorous theoretical
analysis demonstrating GODNF's ability to model diverse convergence
configurations. Extensive empirical evaluations of node classification and
influence estimation tasks confirm GODNF's superiority over state-of-the-art
GNNs.

</details>


### [46] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种通过提示策略从封闭权重LLMs中提取特征并训练轻量级公平分类器的框架，解决了传统方法在零样本或少样本学习中的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能力的提升，其在高风险领域的应用需求增加，但传统公平性方法在封闭权重LLMs（如GPT-4）中不适用。

Method: 将LLM视为特征提取器，通过设计的提示策略获取概率预测特征，再应用公平算法训练分类器。

Result: 在五个数据集上验证了框架的有效性，尤其在数据效率和公平性-准确性权衡方面优于传统方法。

Conclusion: 该框架为封闭权重LLMs提供了一种高效、公平的分类解决方案。

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [47] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 论文提出了一种名为RTE的训练框架，通过时间集成提升SNN的对抗鲁棒性，解决了时间子网络的脆弱性和对抗漏洞的时间传递性问题。


<details>
  <summary>Details</summary>
Motivation: SNN在能效和脑启发计算方面具有潜力，但其对抗扰动的脆弱性尚未充分研究。

Method: 提出RTE框架，通过统一损失函数和随机采样策略优化时间子网络的鲁棒性。

Result: 实验表明RTE在多个基准测试中优于现有方法，改善了鲁棒性与准确性的权衡。

Conclusion: 研究强调了时间结构在对抗学习中的重要性，为构建鲁棒的SNN模型提供了理论基础。

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [48] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: 论文提出HS-GPPT模型，通过光谱对齐优化图预训练和提示调整，解决现有方法在异质性图上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图预训练方法依赖同质性低频知识，难以处理真实图中多样的光谱分布。

Method: 提出HS-GPPT框架，结合混合光谱滤波器和局部-全局对比学习，设计提示图以实现光谱对齐。

Result: 实验验证了模型在转导和归纳学习中的有效性。

Conclusion: HS-GPPT通过光谱对齐实现了跨同质性和异质性的高效知识迁移。

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [49] [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](https://arxiv.org/abs/2508.11338)
*Prathamesh Devadiga,Yashmitha Shailesh*

Main category: cs.LG

TL;DR: RegimeNAS是一种新颖的可微分架构搜索框架，专为加密货币交易设计，通过集成市场状态感知提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决静态深度学习模型在高度动态金融环境中的局限性。

Method: 采用贝叶斯搜索空间、动态激活的神经模块（波动、趋势、范围块）和多目标损失函数。

Result: 在真实加密货币数据上显著优于现有基准，平均绝对误差降低80.3%，收敛速度更快。

Conclusion: 强调将领域知识（如市场状态）嵌入NAS过程对开发稳健金融模型的重要性。

Abstract: We introduce RegimeNAS, a novel differentiable architecture search framework
specifically designed to enhance cryptocurrency trading performance by
explicitly integrating market regime awareness. Addressing the limitations of
static deep learning models in highly dynamic financial environments, RegimeNAS
features three core innovations: (1) a theoretically grounded Bayesian search
space optimizing architectures with provable convergence properties; (2)
specialized, dynamically activated neural modules (Volatility, Trend, and Range
blocks) tailored for distinct market conditions; and (3) a multi-objective loss
function incorporating market-specific penalties (e.g., volatility matching,
transition smoothness) alongside mathematically enforced Lipschitz stability
constraints. Regime identification leverages multi-head attention across
multiple timeframes for improved accuracy and uncertainty estimation. Rigorous
empirical evaluation on extensive real-world cryptocurrency data demonstrates
that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving
an 80.3% Mean Absolute Error reduction compared to the best traditional
recurrent baseline and converging substantially faster (9 vs. 50+ epochs).
Ablation studies and regime-specific analysis confirm the critical contribution
of each component, particularly the regime-aware adaptation mechanism. This
work underscores the imperative of embedding domain-specific knowledge, such as
market regimes, directly within the NAS process to develop robust and adaptive
models for challenging financial applications.

</details>


### [50] [Conformal Prediction Meets Long-tail Classification](https://arxiv.org/abs/2508.11345)
*Shuqi Liu,Jianguo Huang,Luke Ong*

Main category: cs.LG

TL;DR: 论文提出了一种名为Tail-Aware Conformal Prediction (TACP)的方法，用于解决长尾标签分布下尾部类别覆盖率不足的问题，并通过扩展的soft TACP (sTACP)进一步平衡所有类别的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有CP方法在长尾标签分布下，尾部类别的覆盖率不足，影响了预测集的可靠性。

Method: 提出TACP方法，利用长尾结构缩小头尾类别覆盖率差距，并通过sTACP的加权机制进一步优化。

Result: 理论分析和实验表明，TACP和sTACP能有效减少头尾覆盖率差距，并在多个长尾基准数据集上表现优异。

Conclusion: TACP和sTACP为解决长尾分布下的覆盖率不平衡问题提供了有效解决方案。

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
that converts a pretrained model's point prediction into a prediction set, with
the set size reflecting the model's confidence. Although existing CP methods
are guaranteed to achieve marginal coverage, they often exhibit imbalanced
coverage across classes under long-tail label distributions, tending to over
cover the head classes at the expense of under covering the remaining tail
classes. This under coverage is particularly concerning, as it undermines the
reliability of the prediction sets for minority classes, even with coverage
ensured on average. In this paper, we propose the Tail-Aware Conformal
Prediction (TACP) method to mitigate the under coverage of the tail classes by
utilizing the long-tail structure and narrowing the head-tail coverage gap.
Theoretical analysis shows that it consistently achieves a smaller head-tail
coverage gap than standard methods. To further improve coverage balance across
all classes, we introduce an extension of TACP: soft TACP (sTACP) via a
reweighting mechanism. The proposed framework can be combined with various
non-conformity scores, and experiments on multiple long-tail benchmark datasets
demonstrate the effectiveness of our methods.

</details>


### [51] [NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models](https://arxiv.org/abs/2508.11348)
*Xiaohan Bi,Binhang Qi,Hailong Sun,Xiang Gao,Yue Yu,Xiaojun Liang*

Main category: cs.LG

TL;DR: NeMo是一种可扩展且通用的模块化训练方法，适用于Transformer和多种DNN架构，显著提升模块分类精度并减少模块大小。


<details>
  <summary>Details</summary>
Motivation: 随着DNN模型在现代软件系统中的广泛应用，高昂的训练成本成为挑战。现有模块化方法难以应对多样化DNN和大规模模型（如Transformer），因此需要一种更通用的解决方案。

Method: 提出NeMo方法，基于对比学习和复合损失函数，在神经元级别进行模块化训练，适用于多种架构。

Result: 实验表明，NeMo在Transformer和CNN模型上均优于现有方法，模块分类精度平均提升1.72%，模块大小减少58.10%。

Conclusion: NeMo为DNN模块化提供了一种可扩展且通用的解决方案，具有实际应用潜力。

Abstract: With the growing incorporation of deep neural network (DNN) models into
modern software systems, the prohibitive construction costs have become a
significant challenge. Model reuse has been widely applied to reduce training
costs, but indiscriminately reusing entire models may incur significant
inference overhead. Consequently, DNN modularization has gained attention,
enabling module reuse by decomposing DNN models. The emerging
modularizing-while-training (MwT) paradigm, which incorporates modularization
into training, outperforms modularizing-after-training approaches. However,
existing MwT methods focus on small-scale CNN models at the convolutional
kernel level and struggle with diverse DNNs and large-scale models,
particularly Transformer-based models. To address these limitations, we propose
NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron
level fundamental component common to all DNNs-ensuring applicability to
Transformers and various architectures. We design a contrastive learning-based
modular training method with an effective composite loss function, enabling
scalability to large-scale models. Comprehensive experiments on two
Transformer-based models and four CNN models across two classification datasets
demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show
average gains of 1.72% in module classification accuracy and 58.10% reduction
in module size, demonstrating efficacy across both CNN and large-scale
Transformer-based models. A case study on open-source projects shows NeMo's
potential benefits in practical scenarios, offering a promising approach for
scalable and generalizable DNN modularization.

</details>


### [52] [A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts](https://arxiv.org/abs/2508.11349)
*Angela John,Selvyn Allotey,Till Koebe,Alexandra Tyukavina,Ingmar Weber*

Main category: cs.LG

TL;DR: 研究通过卫星图像和辅助数据验证全球植树造林项目的数据可靠性，提出LDIS指标评估位置数据完整性，发现多数项目存在数据问题。


<details>
  <summary>Details</summary>
Motivation: 由于自愿碳市场中植树造林项目的自我报告数据可靠性存疑，研究旨在通过外部验证增强透明度和问责。

Method: 研究整合了来自45,628个项目的1,289,068个种植点数据，结合时间序列卫星图像和辅助数据，提出LDIS指标评估位置数据完整性。

Result: 约79%的种植点在至少1项LDIS指标上失败，15%的项目缺乏机器可读的地理数据。

Conclusion: 数据集不仅提升自愿碳市场的问责性，还可作为计算机视觉任务的训练数据。

Abstract: Afforestation and reforestation are popular strategies for mitigating climate
change by enhancing carbon sequestration. However, the effectiveness of these
efforts is often self-reported by project developers, or certified through
processes with limited external validation. This leads to concerns about data
reliability and project integrity. In response to increasing scrutiny of
voluntary carbon markets, this study presents a dataset on global afforestation
and reforestation efforts compiled from primary (meta-)information and
augmented with time-series satellite imagery and other secondary data. Our
dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.
Since any remote sensing-based validation effort relies on the integrity of a
planting site's geographic boundary, this dataset introduces a standardized
assessment of the provided site-level location information, which we summarize
in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity
Score. We find that approximately 79\% of the georeferenced planting sites
monitored fail on at least 1 out of 10 LDIS indicators, while 15\% of the
monitored projects lack machine-readable georeferenced data in the first place.
In addition to enhancing accountability in the voluntary carbon market, the
presented dataset also holds value as training data for e.g. computer
vision-related tasks with millions of linked Sentinel-2 and Planetscope
satellite images.

</details>


### [53] [Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning](https://arxiv.org/abs/2508.11353)
*Han Zhou,Hongpeng Yin,Xuanhong Deng,Yuyu Huang,Hao Ren*

Main category: cs.LG

TL;DR: 提出了一种名为HGD的梯度下降算法，用于解决不平衡数据流学习问题，通过均衡梯度范数实现平衡学习。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据流常呈现类别不平衡，现有方法如重采样和重加权存在不足，需通过训练修改解决。

Method: 引入HGD算法，均衡不同类别的梯度范数，无需额外参数或数据缓冲。

Result: 理论分析和实验表明HGD具有次线性遗憾界，并在不平衡数据流场景中表现优异。

Conclusion: HGD是一种高效且通用的不平衡数据流学习方法。

Abstract: Many real-world data are sequentially collected over time and often exhibit
skewed class distributions, resulting in imbalanced data streams. While
existing approaches have explored several strategies, such as resampling and
reweighting, for imbalanced data stream learning, our work distinguishes itself
by addressing the imbalance problem through training modification, particularly
focusing on gradient descent techniques. We introduce the harmonized gradient
descent (HGD) algorithm, which aims to equalize the norms of gradients across
different classes. By ensuring the gradient norm balance, HGD mitigates
under-fitting for minor classes and achieves balanced online learning. Notably,
HGD operates in a streamlined implementation process, requiring no data-buffer,
extra parameters, or prior knowledge, making it applicable to any learning
models utilizing gradient descent for optimization. Theoretical analysis, based
on a few common and mild assumptions, shows that HGD achieves a satisfied
sub-linear regret bound. The proposed algorithm are compared with the commonly
used online imbalance learning methods under several imbalanced data stream
scenarios. Extensive experimental evaluations demonstrate the efficiency and
effectiveness of HGD in learning imbalanced data streams.

</details>


### [54] [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)
*Jia Liu,ChangYi He,YingQiao Lin,MingMin Yang,FeiYang Shen,ShaoGuo Liu,TingTing Gao*

Main category: cs.LG

TL;DR: 论文提出了一种基于熵的机制（ETMR和EAR）来优化测试时强化学习（TTRL），解决了高推理成本和早期估计偏差问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中表现优异，但对标注数据依赖性强，且在无监督场景下适应性有限。测试时强化学习（TTRL）虽能自我优化，但面临高推理成本和估计偏差的挑战。

Method: 引入熵平衡机制，包括Entropy-fork Tree Majority Rollout（ETMR）和Entropy-based Advantage Reshaping（EAR），以优化探索与利用的平衡。

Result: 在AIME 2024基准测试中，Llama3.1-8B模型的Pass at 1指标相对提升68%，且仅消耗60%的推理预算。

Conclusion: 该方法有效平衡了推理效率、多样性和估计稳健性，推动了无监督强化学习在开放领域推理任务中的应用。

Abstract: Recent advancements in Large Language Models have yielded significant
improvements in complex reasoning tasks such as mathematics and programming.
However, these models remain heavily dependent on annotated data and exhibit
limited adaptability in unsupervised scenarios. To address these limitations,
test-time reinforcement learning (TTRL) has been proposed, which enables
self-optimization by leveraging model-generated pseudo-labels. Despite its
promise, TTRL faces several key challenges, including high inference costs due
to parallel rollouts and early-stage estimation bias that fosters
overconfidence, reducing output diversity and causing performance plateaus. To
address these challenges, we introduce an entropy-based mechanism to enhance
the exploration-exploitation balance in test-time reinforcement learning
through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and
Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our
approach enables Llama3.1-8B to achieve a 68 percent relative improvement in
Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of
the rollout tokens budget. This highlights our method's ability to effectively
optimize the trade-off between inference efficiency, diversity, and estimation
robustness, thereby advancing unsupervised reinforcement learning for
open-domain reasoning tasks.

</details>


### [55] [PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding](https://arxiv.org/abs/2508.11357)
*Changhong Jing,Yan Liu,Shuqiang Wang,Bruce X. B. Yu,Gong Chen,Zhejing Hu,Zhi Zhang,Yanyan Shen*

Main category: cs.LG

TL;DR: PTSM框架通过双分支掩码机制和信息论约束，实现跨被试EEG解码的零样本泛化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试EEG解码中的被试间变异性大和共享表征稀缺的问题。

Method: 采用双分支掩码机制分解时空模式，结合信息论约束解耦任务相关和被试相关子空间。

Result: 在运动想象数据集上实现零样本泛化，性能优于现有基线。

Conclusion: PTSM通过解耦神经表征，实现个性化和可迁移的解码。

Abstract: Cross-subject electroencephalography (EEG) decoding remains a fundamental
challenge in brain-computer interface (BCI) research due to substantial
inter-subject variability and the scarcity of subject-invariant
representations. This paper proposed PTSM (Physiology-aware and Task-invariant
Spatio-temporal Modeling), a novel framework for interpretable and robust EEG
decoding across unseen subjects. PTSM employs a dual-branch masking mechanism
that independently learns personalized and shared spatio-temporal patterns,
enabling the model to preserve individual-specific neural characteristics while
extracting task-relevant, population-shared features. The masks are factorized
across temporal and spatial dimensions, allowing fine-grained modulation of
dynamic EEG patterns with low computational overhead. To further address
representational entanglement, PTSM enforces information-theoretic constraints
that decompose latent embeddings into orthogonal task-related and
subject-related subspaces. The model is trained end-to-end via a
multi-objective loss integrating classification, contrastive, and
disentanglement objectives. Extensive experiments on cross-subject motor
imagery datasets demonstrate that PTSM achieves strong zero-shot
generalization, outperforming state-of-the-art baselines without
subject-specific calibration. Results highlight the efficacy of disentangled
neural representations for achieving both personalized and transferable
decoding in non-stationary neurophysiological settings.

</details>


### [56] [Fusing Rewards and Preferences in Reinforcement Learning](https://arxiv.org/abs/2508.11363)
*Sadegh Khorasani,Saber Salehkaleybar,Negar Kiyavash,Matthias Grossglauser*

Main category: cs.LG

TL;DR: DFA是一种强化学习算法，结合个体奖励和成对偏好，直接利用策略对数概率建模偏好，避免单独奖励建模步骤。实验表明，DFA在控制环境中表现优于或匹配SAC，且在偏好数据集上优于RLHF基线。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法通常依赖单独奖励或偏好，DFA旨在融合两者，提升学习效率和稳定性。

Method: DFA通过策略对数概率直接建模偏好概率，支持人工标注或在线合成的偏好数据，基于Bradley-Terry模型优化偏好损失。

Result: DFA在六种控制环境中表现优于或匹配SAC，训练更稳定；在偏好数据集上优于RLHF基线，接近真实奖励的性能。

Conclusion: DFA通过融合奖励和偏好，提供了一种高效且稳定的强化学习方法，适用于多种场景。

Abstract: We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that
fuses both individual rewards and pairwise preferences (if available) into a
single update rule. DFA uses the policy's log-probabilities directly to model
the preference probability, avoiding a separate reward-modeling step.
Preferences can be provided by human-annotators (at state-level or
trajectory-level) or be synthesized online from Q-values stored in an
off-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing
DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)
policy. Our simulation results show that DFA trained on generated preferences
matches or exceeds SAC on six control environments and demonstrates a more
stable training process. With only a semi-synthetic preference dataset under
Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement
learning from human feedback (RLHF) baselines in a stochastic GridWorld and
approaches the performance of an oracle with true rewards.

</details>


### [57] [Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization](https://arxiv.org/abs/2508.11365)
*Jayanta Mandi,Ali İrfan Mahmutoğulları,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 决策聚焦学习（DFL）通过训练机器学习模型预测优化问题参数，直接最小化决策后悔（即最大化决策质量）。本文提出，即使使用可微优化层，最小化替代损失仍能实现更优效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有梯度DFL方法在LP问题中梯度为零的问题，提升决策质量。

Method: 提出最小化替代损失，结合可微优化层（如DYS-Net）高效计算梯度和解。

Result: 实验表明，该方法在后悔值上优于或媲美现有方法，同时显著减少训练时间。

Conclusion: 最小化替代损失结合可微优化层是高效且有效的DFL方法。

Abstract: Decision-focused learning (DFL) trains a machine learning (ML) model to
predict parameters of an optimization problem, to directly minimize decision
regret, i.e., maximize decision quality. Gradient-based DFL requires computing
the derivative of the solution to the optimization problem with respect to the
predicted parameters. However, for many optimization problems, such as linear
programs (LPs), the gradient of the regret with respect to the predicted
parameters is zero almost everywhere. Existing gradient-based DFL approaches
for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP
into a differentiable optimization problem by adding a quadratic regularizer
and then minimizing the regret directly or (b) minimizing surrogate losses that
have informative (sub)gradients. In this paper, we show that the former
approach still results in zero gradients, because even after smoothing the
regret remains constant across large regions of the parameter space. To address
this, we propose minimizing surrogate losses -- even when a differentiable
optimization layer is used and regret can be minimized directly. Our
experiments demonstrate that minimizing surrogate losses allows differentiable
optimization layers to achieve regret comparable to or better than
surrogate-loss based DFL methods. Further, we demonstrate that this also holds
for DYS-Net, a recently proposed differentiable optimization technique for LPs,
that computes approximate solutions and gradients through operations that can
be performed using feedforward neural network layers. Because DYS-Net executes
the forward and the backward pass very efficiently, by minimizing surrogate
losses using DYS-Net, we are able to attain regret on par with the
state-of-the-art while reducing training time by a significant margin.

</details>


### [58] [A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting](https://arxiv.org/abs/2508.11390)
*Michael Banf,Dominik Filipiak,Max Schattauer,Liliya Imasheva*

Main category: cs.LG

TL;DR: 本文提出了一种基于Forman-Ricci曲率的结构提升策略，用于解决图神经网络中信息传递的失真问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂系统（如社交或生物网络）需要更高阶的拓扑结构来表示，而传统图神经网络难以处理这种复杂性。

Method: 通过Forman-Ricci曲率将数据表示从基本图形式提升到更具表达力的拓扑结构，以揭示图的局部和全局特性。

Result: 该方法能够缓解图学习中信息传递失真（即过度压缩）的问题，特别是在网络骨干和社区连接中。

Conclusion: 提出的曲率提升策略为高阶拓扑结构的学习提供了有效解决方案，增强了图神经网络的表达能力。

Abstract: Graph Neural Networks are highly effective at learning from relational data,
leveraging node and edge features while maintaining the symmetries inherent to
graph structures. However, many real-world systems, such as social or
biological networks, exhibit complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Geometric and Topological Deep Learning addresses this challenge by introducing
methods that utilize and benefit from higher-order structures. Central to TDL
is the concept of lifting, which transforms data representations from basic
graph forms to more expressive topologies before the application of GNN models
for learning. In this work, we propose a structural lifting strategy using
Forman-Ricci curvature, which defines an edge-based network characteristic
based on Riemannian geometry. Curvature reveals local and global properties of
a graph, such as a network's backbones, i.e. coarse, structure-preserving graph
geometries that form connections between major communities - most suitably
represented as hyperedges to model information flows between clusters across
large distances in the network. To this end, our approach provides a remedy to
the problem of information distortion in message passing across long distances
and graph bottlenecks - a phenomenon known in graph learning as over-squashing.

</details>


### [59] [On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting](https://arxiv.org/abs/2508.11408)
*Wenhao Zhang,Yuexiang Xie,Yuchang Sun,Yanxi Chen,Guoyin Wang,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: CHORD框架通过动态加权统一SFT和RL，避免破坏模型模式，并在实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法整合SFT和RL时可能破坏模型模式并导致过拟合，需一种统一视角解决此问题。

Method: 提出CHORD框架，将SFT作为动态加权辅助目标融入RL过程，采用双控制机制（全局系数和令牌级权重）。

Result: 实验证明CHORD实现了稳定高效的学习过程，显著优于基线方法。

Conclusion: CHORD有效协调了专家数据和探索学习，为相关研究提供了新思路。

Abstract: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established model
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for the Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data's influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from expert
tokens, which preserves on-policy exploration and mitigates disruption from
off-policy data. We conduct extensive experiments on widely used benchmarks,
providing empirical evidence that CHORD achieves a stable and efficient
learning process. By effectively harmonizing off-policy expert data with
on-policy exploration, CHORD demonstrates significant improvements over
baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.

</details>


### [60] [Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space](https://arxiv.org/abs/2508.11424)
*Yinghua Yao,Yuangang Pan,Xixian Chen*

Main category: cs.LG

TL;DR: LEAD是一种序列-结构协同设计框架，通过在共享潜在空间中优化抗体序列和结构，显著提高了优化效率并减少了查询消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在原始数据空间中优化抗体互补决定区（CDRs）效率低下且成本高昂，需要一种更高效的方法。

Method: 提出LEAD框架，利用共享潜在空间优化序列和结构，并设计黑盒引导策略以适应非可微分属性评估器。

Result: LEAD在单目标和多目标优化中表现优异，查询消耗减少一半，性能优于基线方法。

Conclusion: LEAD通过潜在空间优化和黑盒策略，显著提升了抗体设计的效率和效果。

Abstract: Advancements in deep generative models have enabled the joint modeling of
antibody sequence and structure, given the antigen-antibody complex as context.
However, existing approaches for optimizing complementarity-determining regions
(CDRs) to improve developability properties operate in the raw data space,
leading to excessively costly evaluations due to the inefficient search
process. To address this, we propose LatEnt blAck-box Design (LEAD), a
sequence-structure co-design framework that optimizes both sequence and
structure within their shared latent space. Optimizing shared latent codes can
not only break through the limitations of existing methods, but also ensure
synchronization of different modality designs. Particularly, we design a
black-box guidance strategy to accommodate real-world scenarios where many
property evaluators are non-differentiable. Experimental results demonstrate
that our LEAD achieves superior optimization performance for both single and
multi-property objectives. Notably, LEAD reduces query consumption by a half
while surpassing baseline methods in property optimization. The code is
available at https://github.com/EvaFlower/LatEnt-blAck-box-Design.

</details>


### [61] [Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity](https://arxiv.org/abs/2508.11436)
*Mayssa Soussia,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: 论文提出了一种名为mCOCO的新框架，利用Reservoir Computing（RC）从BOLD信号中学习群体水平的功能性连接脑模板（CBT），解决了现有方法在可解释性、计算成本和认知能力方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如传统机器学习和图神经网络）在生成CBT时存在可解释性差、计算成本高以及忽视认知能力的局限性，因此需要一种更高效且全面的解决方案。

Method: mCOCO通过两个阶段实现：1）将BOLD信号映射到RC中生成个体功能连接组，再聚合为群体CBT；2）通过多感官输入增强CBT的认知特性。

Result: 实验表明，mCOCO在中心性、区分性、拓扑合理性和多感官记忆保留方面显著优于基于GNN的CBT。

Conclusion: mCOCO框架为功能性连接研究提供了一种高效且全面的新方法，具有潜在的应用价值。

Abstract: The generation of connectional brain templates (CBTs) has recently garnered
significant attention for its potential to identify unique connectivity
patterns shared across individuals. However, existing methods for CBT learning
such as conventional machine learning and graph neural networks (GNNs) are
hindered by several limitations. These include: (i) poor interpretability due
to their black-box nature, (ii) high computational cost, and (iii) an exclusive
focus on structure and topology, overlooking the cognitive capacity of the
generated CBT. To address these challenges, we introduce mCOCO (multi-sensory
COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)
to learn population-level functional CBT from BOLD
(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow
for tracking state changes over time, enhancing interpretability and enabling
the modeling of brain-like dynamics, as demonstrated in prior literature. By
integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO
captures not only structure and topology but also how brain regions process
information and adapt to cognitive tasks such as sensory processing, all in a
computationally efficient manner. Our mCOCO framework consists of two phases:
(1) mapping BOLD signals into the reservoir to derive individual functional
connectomes, which are then aggregated into a group-level CBT - an approach, to
the best of our knowledge, not previously explored in functional connectivity
studies - and (2) incorporating multi-sensory inputs through a cognitive
reservoir, endowing the CBT with cognitive traits. Extensive evaluations show
that our mCOCO-based template significantly outperforms GNN-based CBT in terms
of centeredness, discriminativeness, topological soundness, and multi-sensory
memory retention. Our source code is available at
https://github.com/basiralab/mCOCO.

</details>


### [62] [Informative Post-Hoc Explanations Only Exist for Simple Functions](https://arxiv.org/abs/2508.11441)
*Eric Günther,Balázs Szabados,Robi Bhattacharjee,Sebastian Bordt,Ulrike von Luxburg*

Main category: cs.LG

TL;DR: 本文提出了一个基于学习理论的框架，用于定义解释算法如何提供决策函数的信息，并证明许多流行算法在复杂模型下不具信息性。


<details>
  <summary>Details</summary>
Motivation: 研究解释算法是否能真正揭示复杂机器学习模型的行为，并填补理论保证的空白。

Method: 引入一个框架，将解释定义为减少决策函数空间的复杂性，并分析不同算法的信息性。

Result: 许多流行解释算法在复杂模型下不具信息性，但通过修改可以满足条件。

Conclusion: 解释算法的理论分析对实际应用（如审计和监管）有重要影响。

Abstract: Many researchers have suggested that local post-hoc explanation algorithms
can be used to gain insights into the behavior of complex machine learning
models. However, theoretical guarantees about such algorithms only exist for
simple decision functions, and it is unclear whether and under which
assumptions similar results might exist for complex models. In this paper, we
introduce a general, learning-theory-based framework for what it means for an
explanation to provide information about a decision function. We call an
explanation informative if it serves to reduce the complexity of the space of
plausible decision functions. With this approach, we show that many popular
explanation algorithms are not informative when applied to complex decision
functions, providing a rigorous mathematical rejection of the idea that it
should be possible to explain any model. We then derive conditions under which
different explanation algorithms become informative. These are often stronger
than what one might expect. For example, gradient explanations and
counterfactual explanations are non-informative with respect to the space of
differentiable functions, and SHAP and anchor explanations are not informative
with respect to the space of decision trees. Based on these results, we discuss
how explanation algorithms can be modified to become informative. While the
proposed analysis of explanation algorithms is mathematical, we argue that it
holds strong implications for the practical applicability of these algorithms,
particularly for auditing, regulation, and high-risk applications of AI.

</details>


### [63] [Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection](https://arxiv.org/abs/2508.11504)
*Andrea Castellani,Zacharias Papadovasilakis,Giorgos Papoutsoglou,Mary Cole,Brian Bautsch,Tobias Rodemann,Ioannis Tsamardinos,Angela Harden*

Main category: cs.LG

TL;DR: 该研究利用AutoML和可解释AI分析车祸严重性，识别关键风险因素，并提出可扩展框架支持交通安全政策。


<details>
  <summary>Details</summary>
Motivation: 全球车祸是伤害和死亡的主要原因，需数据驱动方法理解和减轻其严重性。

Method: 使用JADBio AutoML平台构建预测模型，结合SHAP解释特征贡献，最终采用Ridge Logistic Regression模型。

Result: 模型在训练集和测试集的AUC-ROC分别为85.6%和84.9%，识别出17个关键影响因素。

Conclusion: 研究强调方法严谨性和可解释性，为Vision Zero提供数据支持的交通安全政策框架。

Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide,
necessitating data-driven approaches to understand and mitigate crash severity.
This study introduces a curated dataset of more than 3 million people involved
in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3
million vehicle-level records for predictive analysis. The primary contribution
is a transparent and reproducible methodology that combines Automated Machine
Learning (AutoML) and explainable artificial intelligence (AI) to identify and
interpret key risk factors associated with severe crashes. Using the JADBio
AutoML platform, predictive models were constructed to distinguish between
severe and non-severe crash outcomes. The models underwent rigorous feature
selection across stratified training subsets, and their outputs were
interpreted using SHapley Additive exPlanations (SHAP) to quantify the
contribution of individual features. A final Ridge Logistic Regression model
achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test
set, with 17 features consistently identified as the most influential
predictors. Key features spanned demographic, environmental, vehicle, human,
and operational categories, including location type, posted speed, minimum
occupant age, and pre-crash action. Notably, certain traditionally emphasized
factors, such as alcohol or drug impairment, were less influential in the final
model compared to environmental and contextual variables. Emphasizing
methodological rigor and interpretability over mere predictive performance,
this study offers a scalable framework to support Vision Zero with aligned
interventions and advanced data-informed traffic safety policy.

</details>


### [64] [Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies](https://arxiv.org/abs/2508.11513)
*Fanzhen Liu,Xiaoxiao Ma,Jian Yang,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Quan Z. Sheng,Jia Wu*

Main category: cs.LG

TL;DR: GraphOracle是一个新型自解释图神经网络框架，旨在为GNN生成和评估类级别解释，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升图神经网络（GNNs）的可解释性，确保其安全公平部署，并解决现有方法在类级别解释上的不足。

Method: GraphOracle联合学习GNN分类器和一组结构化稀疏子图，通过掩码评估策略验证其有效性。

Result: GraphOracle在忠实性、可解释性和可扩展性上优于ProtGNN和PGIB，避免了计算瓶颈。

Conclusion: GraphOracle为GNN提供了一种高效且可靠的类级别自解释解决方案。

Abstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to
ensure their safe and fair deployment. Recent work has introduced
self-explainable GNNs that generate explanations as part of training, improving
both faithfulness and efficiency. Some of these models, such as ProtGNN and
PGIB, learn class-specific prototypes, offering a potential pathway toward
class-level explanations. However, their evaluations focus solely on
instance-level explanations, leaving open the question of whether these
prototypes meaningfully generalize across instances of the same class. In this
paper, we introduce GraphOracle, a novel self-explainable GNN framework
designed to generate and evaluate class-level explanations for GNNs. Our model
jointly learns a GNN classifier and a set of structured, sparse subgraphs that
are discriminative for each class. We propose a novel integrated training that
captures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependencies
efficiently and faithfully, validated through a masking-based evaluation
strategy. This strategy enables us to retroactively assess whether prior
methods like ProtGNN and PGIB deliver effective class-level explanations. Our
results show that they do not. In contrast, GraphOracle achieves superior
fidelity, explainability, and scalability across a range of graph
classification tasks. We further demonstrate that GraphOracle avoids the
computational bottlenecks of previous methods$\unicode{x2014}$like Monte Carlo
Tree Search$\unicode{x2014}$by using entropy-regularized subgraph selection and
lightweight random walk extraction, enabling faster and more scalable training.
These findings position GraphOracle as a practical and principled solution for
faithful class-level self-explainability in GNNs.

</details>


### [65] [DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality](https://arxiv.org/abs/2508.11514)
*Qitong Chu,Yufeng Yue,Danya Yao,Huaxin Pei*

Main category: cs.LG

TL;DR: 提出了一种双空间引导的测试框架，通过协调场景参数空间和智能体行为空间，生成兼顾多样性和关键性的测试场景。


<details>
  <summary>Details</summary>
Motivation: 动态环境中决策智能体的安全验证需求增加，现有方法在平衡多样性和关键性方面存在挑战，尤其是高维场景空间中的局部最优陷阱。

Method: 采用分层表示框架，结合降维和多维子空间评估，在场景参数空间中定位多样和关键子空间；在行为空间中利用交互数据量化行为关键性/多样性，支持生成模式切换。

Result: 实验表明，该框架在五种决策智能体上平均提高了56.23%的关键场景生成，并在新指标下表现出更高的多样性。

Conclusion: 双空间引导框架有效解决了多样性与关键性平衡问题，优于现有方法。

Abstract: The growing deployment of decision-making agents in dynamic environments
increases the demand for safety verification. While critical testing scenario
generation has emerged as an appealing verification methodology, effectively
balancing diversity and criticality remains a key challenge for existing
methods, particularly due to local optima entrapment in high-dimensional
scenario spaces. To address this limitation, we propose a dual-space guided
testing framework that coordinates scenario parameter space and agent behavior
space, aiming to generate testing scenarios considering diversity and
criticality. Specifically, in the scenario parameter space, a hierarchical
representation framework combines dimensionality reduction and
multi-dimensional subspace evaluation to efficiently localize diverse and
critical subspaces. This guides dynamic coordination between two generation
modes: local perturbation and global exploration, optimizing critical scenario
quantity and diversity. Complementarily, in the agent behavior space,
agent-environment interaction data are leveraged to quantify behavioral
criticality/diversity and adaptively support generation mode switching, forming
a closed feedback loop that continuously enhances scenario characterization and
exploration within the parameter space. Experiments show our framework improves
critical scenario generation by an average of 56.23\% and demonstrates greater
diversity under novel parameter-behavior co-driven metrics when tested on five
decision-making agents, outperforming state-of-the-art baselines.

</details>


### [66] [Finite-Width Neural Tangent Kernels from Feynman Diagrams](https://arxiv.org/abs/2508.11522)
*Max Guillen,Philipp Misof,Jan E. Gerken*

Main category: cs.LG

TL;DR: 论文提出了一种使用费曼图计算有限宽度修正的方法，以分析神经切线核（NTK）的统计特性，并验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 在无限宽度极限下，NTK的计算虽然简单，但忽略了训练中的关键特性（如NTK演化和特征学习）。因此，需要引入有限宽度修正来更准确地描述训练动态。

Method: 通过费曼图计算有限宽度修正，简化代数操作，并推导层间递归关系，用于分析预激活、NTK及相关高阶张量的统计特性。

Result: 扩展了深度网络的稳定性结果，证明了ReLU等尺度不变非线性在NTK Gram矩阵对角线上无有限宽度修正，并通过数值实验验证。

Conclusion: 费曼图方法为有限宽度NTK分析提供了高效工具，揭示了某些非线性下修正的缺失，为理论研究提供了新视角。

Abstract: Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,
non-linear neural networks. In the infinite-width limit, NTKs can easily be
computed for most common architectures, yielding full analytic control over the
training dynamics. However, at infinite width, important properties of training
such as NTK evolution or feature learning are absent. Nevertheless, finite
width effects can be included by computing corrections to the Gaussian
statistics at infinite width. We introduce Feynman diagrams for computing
finite-width corrections to NTK statistics. These dramatically simplify the
necessary algebraic manipulations and enable the computation of layer-wise
recursive relations for arbitrary statistics involving preactivations, NTKs and
certain higher-derivative tensors (dNTK and ddNTK) required to predict the
training dynamics at leading order. We demonstrate the feasibility of our
framework by extending stability results for deep networks from preactivations
to NTKs and proving the absence of finite-width corrections for scale-invariant
nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We
validate our results with numerical experiments.

</details>


### [67] [Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2508.11528)
*Juhi Soni,Markus Lange-Hegermann,Stefan Windmann*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息扩散模型的无监督异常检测方法，用于多元时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列领域已显示出在预测、填补、生成和异常检测中的有效性，但如何结合物理信息提升性能仍需探索。

Method: 在扩散模型训练中使用加权物理信息损失，通过静态权重调度学习物理依赖的时间分布。

Result: 实验表明，物理信息训练提高了异常检测的F1分数，生成数据多样性和对数似然性更好，优于基线方法和现有物理信息模型。

Conclusion: 该方法在合成和真实数据集上表现优异，尤其在合成数据集和部分真实数据集上超越现有方法。

Abstract: We propose an unsupervised anomaly detection approach based on a
physics-informed diffusion model for multivariate time series data. Over the
past years, diffusion model has demonstrated its effectiveness in forecasting,
imputation, generation, and anomaly detection in the time series domain. In
this paper, we present a new approach for learning the physics-dependent
temporal distribution of multivariate time series data using a weighted
physics-informed loss during diffusion model training. A weighted
physics-informed loss is constructed using a static weight schedule. This
approach enables a diffusion model to accurately approximate underlying data
distribution, which can influence the unsupervised anomaly detection
performance. Our experiments on synthetic and real-world datasets show that
physics-informed training improves the F1 score in anomaly detection; it
generates better data diversity and log-likelihood. Our model outperforms
baseline approaches, additionally, it surpasses prior physics-informed work and
purely data-driven diffusion models on a synthetic dataset and one real-world
dataset while remaining competitive on others.

</details>


### [68] [A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow](https://arxiv.org/abs/2508.11529)
*George Paterakis,Andrea Castellani,George Papoutsoglou,Tobias Rodemann,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: 论文提出了HXAI框架，通过将解释嵌入数据分析的每个阶段，并针对用户需求定制解释，以提高AI模型的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型被视为“黑箱”，传统解释方法仅关注单个预测，忽略了上下游决策和质量检查，导致用户难以信任AI的洞察。

Method: 提出HXAI框架，整合数据、分析设置、学习过程、模型输出、模型质量和沟通渠道六个组件，并通过112项问题库和用户调查验证需求。

Result: HXAI框架通过统一分类和用户需求对齐，提高了解释的清晰性、可操作性和认知可管理性，并展示了如何利用大语言模型生成面向特定利益相关者的解释。

Conclusion: HXAI框架通过多学科融合和实际项目经验，为AI的透明度、可信度和负责任部署提供了端到端的解决方案。

Abstract: Artificial intelligence is reshaping science and industry, yet many users
still regard its models as opaque "black boxes". Conventional explainable
artificial-intelligence methods clarify individual predictions but overlook the
upstream decisions and downstream quality checks that determine whether
insights can be trusted. In this work, we present Holistic Explainable
Artificial Intelligence (HXAI), a user-centric framework that embeds
explanation into every stage of the data-analysis workflow and tailors those
explanations to users. HXAI unifies six components (data, analysis set-up,
learning process, model output, model quality, communication channel) into a
single taxonomy and aligns each component with the needs of domain experts,
data analysts and data scientists. A 112-item question bank covers these needs;
our survey of contemporary tools highlights critical coverage gaps. Grounded in
theories of human explanation, principles from human-computer interaction and
findings from empirical user studies, HXAI identifies the characteristics that
make explanations clear, actionable and cognitively manageable. A comprehensive
taxonomy operationalises these insights, reducing terminological ambiguity and
enabling rigorous coverage analysis of existing toolchains. We further
demonstrate how AI agents that embed large-language models can orchestrate
diverse explanation techniques, translating technical artifacts into
stakeholder-specific narratives that bridge the gap between AI developers and
domain experts. Departing from traditional surveys or perspective articles,
this work melds concepts from multiple disciplines, lessons from real-world
projects and a critical synthesis of the literature to advance a novel,
end-to-end viewpoint on transparency, trustworthiness and responsible AI
deployment.

</details>


### [69] [DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning](https://arxiv.org/abs/2508.11530)
*Lianshuai Guo,Zhongzheng Yuan,Xunkai Li,Yinlin Zhu,Meixia Qu,Wenyu Wang*

Main category: cs.LG

TL;DR: DFed-SST是一种去中心化的联邦图学习框架，通过自适应通信机制优化客户端间的拓扑结构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化联邦学习（DFL）方法未考虑局部子图的拓扑信息，而联邦图学习（FGL）多为集中式，无法发挥去中心化优势。

Method: 提出DFed-SST框架，采用双拓扑自适应通信机制，动态优化客户端间通信拓扑。

Result: 在八个真实数据集上，平均准确率比基线方法提升3.26%。

Conclusion: DFed-SST有效解决了异构性问题，为去中心化联邦图学习提供了高效解决方案。

Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed
paradigm that circumvents the single-point-of-failure and communication
bottleneck risks of centralized architectures. However, a significant challenge
arises as existing DFL optimization strategies, primarily designed for tasks
such as computer vision, fail to address the unique topological information
inherent in the local subgraph. Notably, while Federated Graph Learning (FGL)
is tailored for graph data, it is predominantly implemented in a centralized
server-client model, failing to leverage the benefits of decentralization.To
bridge this gap, we propose DFed-SST, a decentralized federated graph learning
framework with adaptive communication. The core of our method is a
dual-topology adaptive communication mechanism that leverages the unique
topological features of each client's local subgraph to dynamically construct
and optimize the inter-client communication topology. This allows our framework
to guide model aggregation efficiently in the face of heterogeneity. Extensive
experiments on eight real-world datasets consistently demonstrate the
superiority of DFed-SST, achieving 3.26% improvement in average accuracy over
baseline methods.

</details>


### [70] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动的嵌套算子推断（OpInf）方法，用于从高维动态系统的快照数据中学习物理信息降阶模型（ROM）。该方法通过利用降阶空间内的层次结构，优先考虑主导模式的相互作用，迭代构建OpInf学习问题的初始猜测。结果显示，嵌套OpInf在立方热传导问题中比标准OpInf误差小四倍，并在格陵兰冰盖的大规模参数化模型中实现了平均3%的误差和超过19,000倍的计算加速。


<details>
  <summary>Details</summary>
Motivation: 高维动态系统的降阶建模需要高效且准确的方法，而传统OpInf方法在初始猜测和模型更新方面存在局限性。本文旨在通过嵌套OpInf方法解决这些问题，提升模型性能和应用灵活性。

Method: 提出嵌套OpInf方法，利用降阶空间的层次结构迭代构建初始猜测，优先考虑主导模式的相互作用。该方法支持从先前学习的模型热启动，适应动态基和模型形式的更新。

Result: 在立方热传导问题中，嵌套OpInf的误差比标准OpInf小四倍；在格陵兰冰盖模型中，实现了平均3%的误差和超过19,000倍的计算加速。

Conclusion: 嵌套OpInf方法显著提升了降阶模型的准确性和计算效率，适用于动态基和模型更新的复杂场景。

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


### [71] [SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling](https://arxiv.org/abs/2508.11553)
*Jinghui Wang,Shaojie Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Xiaojiang Zhang,Minglei Zhang,Jiarong Zhang,Wenhao Zhuang,Yuchen Cao,Wankang Bao,Haimo Li,Zheng Lin,Huiming Wang,Haoyang Huang,Zongxian Feng,Zizheng Zhan,Ken Deng,Wen Xiang,Huaixi Tang,Kun Wu,Mengtong Li,Mengfei Xie,Junyi Peng,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SeamlessFlow是一个基于服务器的强化学习框架，解决了工业规模RL中的两个核心挑战：解耦训练与复杂执行流，以及最大化GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 工业规模RL面临训练与执行流耦合、GPU利用率低的问题，SeamlessFlow旨在解决这些问题。

Method: 通过数据平面解耦训练与代理实现，引入轨迹管理器支持部分滚动；采用标签驱动调度和时空复用管道优化资源利用。

Result: SeamlessFlow实现了稳定性和高性能，适用于多代理、长周期等复杂RL任务。

Conclusion: SeamlessFlow通过创新设计，为大规模RL部署提供了高效、稳定的解决方案。

Abstract: We introduce SeamlessFlow, a server based reinforcement learning (RL)
framework that addresses two core challenges in industrial scale RL: (1)
decoupling RL training from the complex execution flow of agents; (2)
maximizing GPU utilization with minimal idle time while preserving the
stability and scalability required for large-scale deployments. First,
SeamlessFlow introduces a data plane that decouples the RL trainer from
diverse, complex agent implementations while sustaining high throughput. A
central trajectory manager maintains complete interaction histories and
supports partial rollout, allowing rollout to pause for weight updates and
resume seamlessly, keeping agents unaware of service interruptions. Second, we
propose a tag driven scheduling paradigm that abstracts hardware into
capability tagged resources, unifying colocated and disaggregated
architectures. Based on this, SeamlessFlow introduces a spatiotemporal
multiplexing pipeline that dynamically reassigns idle training nodes to rollout
in a train rollout separated setup, eliminating pipeline bubbles and fully
exploiting heterogeneous cluster resources. By combining these innovations,
SeamlessFlow delivers both stability and high performance, making it well
suited for multi agent, long horizon, and other complex RL tasks.

</details>


### [72] [Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective](https://arxiv.org/abs/2508.11618)
*Jungang Chen,Seyyed A. Hosseini*

Main category: cs.LG

TL;DR: 提出了一种基于马尔可夫游戏的范式，研究不同联盟结构如何影响碳捕集与封存（CCS）项目中各利益相关者的目标。


<details>
  <summary>Details</summary>
Motivation: CCS项目涉及多方利益相关者，各自目标不同且地质条件复杂，需探讨独立优化还是协作更有效。

Method: 将多利益相关者多站点问题建模为带安全约束的多智能体强化学习问题，并利用E2C框架的替代模型降低计算成本。

Result: 框架能有效解决多目标利益相关者参与的CO2封存优化管理问题。

Conclusion: 协作联盟结构对CCS项目的成功至关重要，提出的方法为复杂地质条件下的多利益相关者管理提供了定量工具。

Abstract: Carbon capture and storage (CCS) projects typically involve a diverse array
of stakeholders or players from public, private, and regulatory sectors, each
with different objectives and responsibilities. Given the complexity, scale,
and long-term nature of CCS operations, determining whether individual
stakeholders can independently maximize their interests or whether
collaborative coalition agreements are needed remains a central question for
effective CCS project planning and management. CCS projects are often
implemented in geologically connected sites, where shared geological features
such as pressure space and reservoir pore capacity can lead to competitive
behavior among stakeholders. Furthermore, CO2 storage sites are often located
in geologically mature basins that previously served as sites for hydrocarbon
extraction or wastewater disposal in order to leverage existing
infrastructures, which makes unilateral optimization even more complicated and
unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively
investigate how different coalition structures affect the goals of
stakeholders. We frame this multi-stakeholder multi-site problem as a
multi-agent reinforcement learning problem with safety constraints. Our
approach enables agents to learn optimal strategies while compliant with safety
regulations. We present an example where multiple operators are injecting CO2
into their respective project areas in a geologically connected basin. To
address the high computational cost of repeated simulations of high-fidelity
models, a previously developed surrogate model based on the Embed-to-Control
(E2C) framework is employed. Our results demonstrate the effectiveness of the
proposed framework in addressing optimal management of CO2 storage when
multiple stakeholders with various objectives and goals are involved.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [73] [Non-asymptotic convergence bound of conditional diffusion models](https://arxiv.org/abs/2508.10944)
*Mengze Li*

Main category: stat.ML

TL;DR: 本文提出了一种基于条件扩散模型的分类与回归方法（CARD），通过整合预训练模型，提升了条件分布的学习能力，并建立了理论分析框架。


<details>
  <summary>Details</summary>
Motivation: 尽管条件扩散模型在加速算法和生成质量上取得了进展，但缺乏非渐近性质阻碍了理论研究。本文旨在填补这一空白。

Method: CARD将预训练模型f_{\phi}(x)整合到扩散模型中，以精确捕捉条件分布Y|f_{\phi}(x)，并通过Fokker-Planck方程建立理论框架。

Result: 在Lipschitz假设下，利用二阶Wasserstein距离证明了生成条件分布与原始分布的上界误差，并在轻尾假设下推导了收敛上界。

Conclusion: CARD为条件扩散模型提供了坚实的理论基础，并在理论和实践上均表现出色。

Abstract: Learning and generating various types of data based on conditional diffusion
models has been a research hotspot in recent years. Although conditional
diffusion models have made considerable progress in improving acceleration
algorithms and enhancing generation quality, the lack of non-asymptotic
properties has hindered theoretical research. To address this gap, we focus on
a conditional diffusion model within the domains of classification and
regression (CARD), which aims to learn the original distribution with given
input x (denoted as Y|X). It innovatively integrates a pre-trained model
f_{\phi}(x) into the original diffusion model framework, allowing it to
precisely capture the original conditional distribution given f (expressed as
Y|f_{\phi}(x)). Remarkably, when f_{\phi}(x) performs satisfactorily,
Y|f_{\phi}(x) closely approximates Y|X. Theoretically, we deduce the stochastic
differential equations of CARD and establish its generalized form predicated on
the Fokker-Planck equation, thereby erecting a firm theoretical foundation for
analysis. Mainly under the Lipschitz assumptions, we utilize the second-order
Wasserstein distance to demonstrate the upper error bound between the original
and the generated conditional distributions. Additionally, by appending
assumptions such as light-tailedness to the original distribution, we derive
the convergence upper bound between the true value analogous to the score
function and the corresponding network-estimated value.

</details>


### [74] [Counterfactual Survival Q Learning for Longitudinal Randomized Trials via Buckley James Boosting](https://arxiv.org/abs/2508.11060)
*Jeongjin Lee,Jong-Min Kim*

Main category: stat.ML

TL;DR: 提出了一种基于Buckley James (BJ) Boost Q学习的框架，用于在右删失生存数据下估计最优动态治疗方案，特别适用于纵向随机临床试验。该方法结合了加速失效时间模型和迭代提升技术，避免了比例风险假设的限制，并通过模拟研究和HIV试验分析验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在生存数据中，传统的Cox比例风险模型可能因模型误设而产生偏差，尤其是在多阶段治疗决策中。因此，需要一种更灵活且稳健的方法来估计最优动态治疗方案。

Method: 结合加速失效时间模型和迭代提升技术（如最小二乘和回归树），在反事实Q学习框架下直接建模条件生存时间，避免了比例风险假设。

Result: 模拟研究和ACTG175 HIV试验分析表明，BJ Boost Q学习在多阶段治疗决策中具有更高的准确性，且能有效减少偏差累积。

Conclusion: BJ Boost Q学习提供了一种灵活且稳健的方法，适用于生存数据下的动态治疗方案估计，尤其在多阶段场景中表现优越。

Abstract: We propose a Buckley James (BJ) Boost Q learning framework for estimating
optimal dynamic treatment regimes under right censored survival data, tailored
for longitudinal randomized clinical trial settings. The method integrates
accelerated failure time models with iterative boosting techniques, including
componentwise least squares and regression trees, within a counterfactual Q
learning framework. By directly modeling conditional survival time, BJ Boost Q
learning avoids the restrictive proportional hazards assumption and enables
unbiased estimation of stage specific Q functions. Grounded in potential
outcomes, this framework ensures identifiability of the optimal treatment
regime under standard causal assumptions. Compared to Cox based Q learning,
which relies on hazard modeling and may suffer from bias under
misspecification, our approach provides robust and flexible estimation.
Simulation studies and analysis of the ACTG175 HIV trial demonstrate that BJ
Boost Q learning yields higher accuracy in treatment decision making,
especially in multistage settings where bias can accumulate.

</details>


### [75] [Uniform convergence for Gaussian kernel ridge regression](https://arxiv.org/abs/2508.11274)
*Paul Dommel,Rajmadan Lakshmanan*

Main category: stat.ML

TL;DR: 论文首次证明了高斯核岭回归（KRR）在固定超参数情况下，在一致范数和$L^{2}$范数下的多项式收敛速率。


<details>
  <summary>Details</summary>
Motivation: 填补高斯核KRR理论中的空白，此前在固定超参数情况下缺乏一致收敛速率的研究。

Method: 通过理论分析，证明了高斯核KRR在固定宽度参数下的多项式收敛速率。

Result: 在一致范数和$L^{2}$范数下均获得多项式收敛速率，为高斯核KRR的应用提供了新的理论支持。

Conclusion: 研究结果为固定超参数的高斯核KRR在非参数回归中的应用提供了理论依据。

Abstract: This paper establishes the first polynomial convergence rates for Gaussian
kernel ridge regression (KRR) with a fixed hyperparameter in both the uniform
and the $L^{2}$-norm. The uniform convergence result closes a gap in the
theoretical understanding of KRR with the Gaussian kernel, where no such rates
were previously known. In addition, we prove a polynomial $L^{2}$-convergence
rate in the case, where the Gaussian kernel's width parameter is fixed. This
also contributes to the broader understanding of smooth kernels, for which
previously only sub-polynomial $L^{2}$-rates were known in similar settings.
Together, these results provide new theoretical justification for the use of
Gaussian KRR with fixed hyperparameters in nonparametric regression.

</details>


### [76] [ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization](https://arxiv.org/abs/2508.11551)
*Shengzhuang Chen,Xu Ouyang,Michael Arthur Leopold Pearce,Thomas Hartvigsen,Jonathan Richard Schwarz*

Main category: stat.ML

TL;DR: 本文提出了一种基于贝叶斯优化的黑盒超参数优化方法，用于确定大型语言模型训练的最佳数据混合比例，显著提高了性能并降低了实验成本。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练中数据混合比例的确定仍依赖启发式探索，缺乏可靠的学习方法，影响了模型性能。

Method: 将数据混合比例选择视为黑盒超参数优化问题，采用多保真度贝叶斯优化方法，平衡实验成本与模型拟合。

Result: 在从100万到70亿参数的模型上，相比基线方法，确定最佳数据混合比例的速度提高了500%。

Conclusion: 该方法为数据混合比例优化提供了高效框架，并通过共享数据集降低了研究成本。

Abstract: Determining the optimal data mixture for large language model training
remains a challenging problem with an outsized impact on performance. In
practice, language model developers continue to rely on heuristic exploration
since no learning-based approach has emerged as a reliable solution. In this
work, we propose to view the selection of training data mixtures as a black-box
hyperparameter optimization problem, for which Bayesian Optimization is a
well-established class of appropriate algorithms. Firstly, we cast data mixture
learning as a sequential decision-making problem, in which we aim to find a
suitable trade-off between the computational cost of training exploratory
(proxy-) models and final mixture performance. Secondly, we systematically
explore the properties of transferring mixtures learned at a small scale to
larger-scale experiments, providing insights and highlighting opportunities for
research at a modest scale. By proposing Multi-fidelity Bayesian Optimization
as a suitable method in this common scenario, we introduce a natural framework
to balance experiment cost with model fit, avoiding the risks of overfitting to
smaller scales while minimizing the number of experiments at high cost. We
present results for pre-training and instruction finetuning across models
ranging from 1 million to 7 billion parameters, varying from simple
architectures to state-of-the-art models and benchmarks spanning dozens of
datasets. We demonstrate consistently strong results relative to a wide range
of benchmarks, showingspeed-ups of over 500% in determining the best data
mixture on our largest experiments relative to recent baselines. In addition,
we broaden access to research by sharing ADMIRE IFT Runs, a dataset of 460 full
training & evaluation runs across various model sizes worth over 13,000 GPU
hours, greatly reducing the cost of conducting research in this area.

</details>


### [77] [Nonparametric learning of stochastic differential equations from sparse and noisy data](https://arxiv.org/abs/2508.11597)
*Arnab Ganguly,Riten Mitra,Jinpu Zhou*

Main category: stat.ML

TL;DR: 提出了一种从稀疏、噪声观测中构建数据驱动的随机微分方程（SDE）模型的系统框架，通过EM算法和RKHS方法学习漂移函数。


<details>
  <summary>Details</summary>
Motivation: 传统参数化方法假设漂移函数形式已知，而本文旨在直接从数据中学习漂移函数，适用于系统动态部分已知或高度复杂的科学领域。

Method: 使用惩罚负对数似然函数在RKHS中建模，开发了结合SMC的EM算法处理稀疏观测，并通过贝叶斯变体控制模型复杂度。

Result: 提出的EM-SMC-RKHS方法在低数据量下能准确估计漂移函数，并通过数值实验验证了其有效性。

Conclusion: 该方法在连续时间建模和观测受限的领域具有广泛应用前景。

Abstract: The paper proposes a systematic framework for building data-driven stochastic
differential equation (SDE) models from sparse, noisy observations. Unlike
traditional parametric approaches, which assume a known functional form for the
drift, our goal here is to learn the entire drift function directly from data
without strong structural assumptions, making it especially relevant in
scientific disciplines where system dynamics are partially understood or highly
complex. We cast the estimation problem as minimization of the penalized
negative log-likelihood functional over a reproducing kernel Hilbert space
(RKHS). In the sparse observation regime, the presence of unobserved trajectory
segments makes the SDE likelihood intractable. To address this, we develop an
Expectation-Maximization (EM) algorithm that employs a novel Sequential Monte
Carlo (SMC) method to approximate the filtering distribution and generate Monte
Carlo estimates of the E-step objective. The M-step then reduces to a penalized
empirical risk minimization problem in the RKHS, whose minimizer is given by a
finite linear combination of kernel functions via a generalized representer
theorem. To control model complexity across EM iterations, we also develop a
hybrid Bayesian variant of the algorithm that uses shrinkage priors to identify
significant coefficients in the kernel expansion. We establish important
theoretical convergence results for both the exact and approximate EM
sequences. The resulting EM-SMC-RKHS procedure enables accurate estimation of
the drift function of stochastic dynamical systems in low-data regimes and is
broadly applicable across domains requiring continuous-time modeling under
observational constraints. We demonstrate the effectiveness of our method
through a series of numerical experiments.

</details>
