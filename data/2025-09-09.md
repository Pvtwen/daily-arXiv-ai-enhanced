<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 23]
- [cs.LG](#cs.LG) [Total: 108]
- [stat.ML](#stat.ML) [Total: 13]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Developing a Framework to Simulate Quantitative Ultrasound Flow and Tissue Motion for Ultrafast Doppler Ultrasound](https://arxiv.org/abs/2509.05464)
*Qiang Fu,Changhui Li*

Main category: eess.SP

TL;DR: 一个开源的3D完全量化流动框架（3D-FQFlow），用于在真实血管和组织运动条件下验证超快速功率多普勒成像（uPDI）的量化性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺缺能够在近似真实条件下进行三维量化流动与组织运动模拟的uPDI模拟工具。

Method: 构建了一个集成框架，包括L系统基础的血管生成器、SimVascular CFD流体动力学模拟、支持用户定义或临床数据驱动的组织运动模拟器、优化的PFILED超声模拟器、预计算矩阵基础的重建器和量化分析器。

Result: 识别了四种运动模式对SVD分解的影响，成功实现3D图像重建（兔肾肚SSIM=0.951，生成血管SSIM=0.902，临床肺动脉SSIM=0.850），GPU加速实现100帧3D-uPDI生成速度提升18.8倍。

Conclusion: 3D-FQFlow是首个开源框架，为微血管成像研究创建了可重现的量化验证标准。

Abstract: Ultrafast power Doppler imaging (uPDI) has made significant progress and
become an important imaging method for both research and clinical
implementations. While, it lacks simulation tools that can perform
three-dimensional (3D) quantitative flow with tissue motion close to realistic
conditions. In this study, we explore to construct an open-source framework,
named 3D-Fully Quantitative Flow (3D-FQFlow), to provide quantitative modeling
of 3D vascular flow with tissue motion and uPDI imaging. The framework
integrates a L-system-based vascular generator with SimVascular CFD for
hemodynamics, a tissue motion simulator supporting user-defined or
clinical-data-driven condition, an optimized PFILED ultrasound simulator, a
precomputed-matrix-based reconstructor, and a quantitative analyzer
(MSE/PSNR/SSIM). Results demonstrate distinct influences of four motion
patterns on SVD decomposition; successful 3D imaging of rabbit kidney (SSIM =
0.951), generated vasculature (SSIM = 0.902), and clinical pulmonary arteries
(SSIM = 0.850); and GPU acceleration permitting 1-million-scatterer simulation
in 4,117 seconds with 18.8* speedup for 100-frame 3D-uPDI generation. 3D-FQFlow
establishes the first open-source framework for quantitative validation of uPDI
under realistic vascular and motion conditions, creating a reproducible
standard for microvascular imaging research
(https://github.com/FortuneOU/3D-FQFlow).

</details>


### [2] [Human Body Weight Estimation Through Music-Induced Bed Vibrations](https://arxiv.org/abs/2509.06257)
*Yuyan Wu,Jiale Zhang,Moon Lee,Cherrelle Smith,Xinyi Li,Ankur Senapati,Pei Zhang,Hae Young Noh*

Main category: eess.SP

TL;DR: MelodyBedScale是一种基于音乐诱导床振动的非侵入式体重估计系统，通过分析体重对床体振动传递函数的影响来快速准确估计患者体重


<details>
  <summary>Details</summary>
Motivation: 在急诊医疗中，快速准确的体重估计对治疗决策至关重要，但传统方法对固定患者不实用、不准确或耗时

Method: 利用音乐诱导床振动，通过振动传感器捕捉体重敏感的频带，结合物理理论分析构建物理信息神经网络进行体重回归

Result: 在木质和钢制床上对11名参与者进行评估，平均绝对误差达到1.55公斤

Conclusion: MelodyBedScale提供了一种非侵入式、快速的床上体重估计解决方案，在急诊医疗中具有重要应用价值

Abstract: Rapid and accurate body weight estimation is critical in emergency medical
care, as it directly influences treatment decisions, such as drug dosing,
defibrillation energy selection, and fluid resuscitation. Traditional methods
such as stand-on scales, length-based tapes, or transfer-based weighing scales
are often impractical for immobilized patients, inaccurate, or labor-intensive
and time-consuming. This paper introduces MelodyBedScale, a non-intrusive and
rapid on-bed weight estimation system that leverages bed vibration induced by
music. The core insight is that body weight affects the vibration transfer
function of the bed-body system, which is captured using vibration sensors
placed on opposite sides of the bed. First, we identify weight-sensitive
frequency bands and compose clinically acceptable soft, natural music with high
signal energy in these frequency bands. This music is then played through a
speaker mounted on the bed to induce bed vibrations. Additionally, to
efficiently capture the complex weight-vibration relationship with limited data
and enhance generalizability to unseen individuals and weights, we
theoretically analyze the weight-vibration relationship and integrate the
results into the activation functions of the neural network for
physics-informed weight regression. We evaluated MelodyBedScale on both wooden
and steel beds across 11 participants, achieving a mean absolute error of up to
1.55 kg.

</details>


### [3] [Time-Modulated Intelligent Reflecting Surfaces for Integrated Sensing, Communication and Security: A Generative AI Design Framework](https://arxiv.org/abs/2509.05565)
*Zhihao Tao,Athina Petropulu,H. Vincent Poor*

Main category: eess.SP

TL;DR: 使用生成流网络(GFlowNet)设计时间调制智能反射表面(TM-IRS)，在ISAC系统中同时实现通信、感知和物理层安全


<details>
  <summary>Details</summary>
Motivation: 解决集成感知与通信(ISAC)系统中的安全问题，防止目标作为款听者获取传输数据

Method: 使用GFlowNet学习策略来采样高性能TM-IRS配置，通过奖励函数合并考虑通信和感知性能，将设计模型化为确定性马尔可夫决策过程

Result: 实验结果证明方法能够同时集成感知、通信和安全，与枚举组合搜索相比显著提高采样效率，并且对基于规则的TM-IRS设计方法具有更强的稳健性

Conclusion: GFlowNet框架能够有效地从庞大离散参数空间中采样高性能TM-IRS配置，为ISAC系统提供了一种合成感知、通信和安全的新方法

Abstract: We propose a novel approach to achieve physical layer security for integrated
sensing and communication (ISAC) systems operating in the presence of targets
that may be eavesdroppers. The system is aided by a time-modulated intelligent
reflecting surface (TM-IRS), which is configured to preserve the integrity of
the transmitted data at one or more legitimate communication users (CUs) while
making them appear scrambled in all other directions. The TM-IRS design
leverages a generative flow network (GFlowNet) framework to learn a stochastic
policy that samples high-performing TM-IRS configurations from a vast discrete
parameter space. Specifically, we begin by formulating the achievable sum rate
for the legitimate CUs and the beampattern gain toward the target direction,
based on which we construct reward functions for GFlowNets that jointly capture
both communication and sensing performance. The TM-IRS design is modeled as a
deterministic Markov decision process (MDP), where each terminal state
corresponds to a complete configuration of TM-IRS parameters. GFlowNets,
parametrized by deep neural networks are employed to learn a stochastic policy
that samples TM-IRS parameter sets with probability proportional to their
associated reward. Experimental results demonstrate the effectiveness of the
proposed GFlowNet-based method in integrating sensing, communication and
security simultaneously, and also exhibit significant sampling efficiency as
compared to the exhaustive combinatorial search and enhanced robustness against
the rule-based TM-IRS design method.

</details>


### [4] [Power-Measurement-Based Channel Estimation for Beyond Diagonal RIS](https://arxiv.org/abs/2509.05639)
*Yijie Liu,Weidong Mei,He Sun,Dong Wang,Peilan Wang*

Main category: eess.SP

TL;DR: 提出基于单层神经网络的BD-RIS信道估计方法，仅使用接收功率测量值，无需专用导频信号，降低系统开销。


<details>
  <summary>Details</summary>
Motivation: 现有BD-RIS信道估计方法依赖专用导频信号，增加系统开销且与现有通信协议不兼容，需要更高效的信道获取方案。

Method: 利用接收信号功率可表示为类似单层神经网络的形式，通过反向传播算法，基于不同训练反射系数下的功率测量值恢复信道状态信息。

Result: 数值结果表明该方法能实现较小的归一化均方误差，特别是在训练反射次数较多时性能更佳。

Conclusion: 所提出的神经网络方法为BD-RIS提供了一种低开销、兼容性好的信道估计解决方案，仅需接收功率测量即可有效获取CSI。

Abstract: Beyond diagonal reconfigurable intelligent surface (BD-RIS), with its
enhanced degrees of freedom compared to conventional RIS, has demonstrated
notable potential for enhancing wireless communication performance. However, a
key challenge in employing BD-RIS lies in accurately acquiring its channel
state information (CSI) with both the base station (BS) and users. Existing
BD-RIS channel estimation methods rely mainly on dedicated pilot signals, which
increase system overhead and may be incompatible with current communication
protocols. To overcome these limitations, this letter proposes a new
single-layer neural network (NN)-enabled channel estimation method utilizing
only the easily accessible received power measurements at user terminals. In
particular, we show that the received signal power can be expressed in a form
similar to a single-layer NN, where the weights represent the BD-RIS's CSI.
This structure enables the recovery of CSI using the backward propagation,
based on power measurements collected under varying training reflection
coefficients. Numerical results show that our proposed method can achieve a
small normalized mean square error (NMSE), particularly when the number of
training reflections is large.

</details>


### [5] [Full-Angle Ray Antenna Array and Omnicell Wireless Communication System](https://arxiv.org/abs/2509.05677)
*Xuancheng Zhu,Zhiwen Zhou,Yong Zeng*

Main category: eess.SP

TL;DR: 全角Ray天线数组(RAA)构成的新型全向组网通信系统，通过将大量便宜天线元排列成不同方向的简单线性数组，实现更高的收益、更低的硬件成本和全方向匀匀角分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统的ULA/UCA天线构造在硬件成本、收益和角分辨率方面有限，需要一种新的多天线构造来提高性能和降低成本。

Method: 提出全角RAA构造，将RAA的方位角扩展到全角域，并构建基于该构造的全向组通信系统，在小区中央部部署基站。

Result: 经过广泛的分析和数值模拟，证明了该系统在空间分辨率和通信速率等关键性能指标上都显著优于传统的ULA/UCA基于所派组网的系统。

Conclusion: 全角RAA基于的全向组通信系统不仅能显著降低用户间干扰，还能提高成本效益，为未来无线通信系统提供了有前景的解决方案。

Abstract: Ray antenna array (RAA) was recently proposed as a novel multi-antenna
architecture that arranges multiple massive cheap antenna elements into simple
uniform linear arrays (sULAs) with different orientations. Compared with
traditional architectures like hybrid analog/digital beamforming with uniform
linear array (ULA) and uniform circular array (UCA), RAA has several promising
advantages such as significantly reduced hardware cost, higher beamforming
gains and the ability of providing uniform angular resolution for all
directions. In this paper, we propose a full-angle RAA architecture and an
innovative omnicell wireless communication paradigm enabled by full-angle RAA.
The proposed full-angle RAA expands RAA's orientation angle to the full angle
domain, such that the RAA's advantages can be exploited to all directions. This
further enables the new concept of omnicell wireless communication system, with
the base station equipped by full-angle RAA and deployed at the center of each
cell. Compared to the conventional cell sectoring wireless communication
system, the proposed omnicell system is expected to not only significantly
reduce the inter-user interference, but also improve the cost efficiency.
Extensive analytical and numerical results are provided to compare those key
performance indicators such as the spatial resolution and the communication
rate of the proposed full-angle RAA based omnicell wireless communication
system against the conventional ULA/UCA-based cell sectoring systems.

</details>


### [6] [Affine Filter Bank Modulation (AFBM): A Novel 6G ISAC Waveform with Low PAPR and OOBE](https://arxiv.org/abs/2509.05683)
*Kuranage Roche Rayan Ranasinghe,Henrique L. Senger,Gustavo P. Gonçalves,Hyeon Seok Rou,Bruno S. Chang,Giuseppe Thadeu Freitas de Abreu,Didier Le Ruyet*

Main category: eess.SP

TL;DR: AFBM波形是一种基于滤波器组多载波调制和啁啾域波形的新型波形设计，具有低峰均功率比和低带外辐射特性，适用于6G集成感知与通信系统。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统AFDM波形在双弥散信道条件下的高峰均功率比和带外辐射问题，需要开发一种更适合下一代无线系统的集成感知与通信波形。

Method: 结合经典滤波器组多载波调制理论和啁啾域波形先进技术，设计AFBM波形；通信方面采用高斯置信传播算法进行符号检测，感知方面使用期望最大化辅助的概率数据关联框架进行目标估计。

Result: AFBM相比传统AFDM波形表现出更低的峰均功率比和带外辐射，通过模糊函数、误码率和均方根误差等指标验证了其优越性能。

Conclusion: AFBM波形具有优异的性能表现，是下一代无线系统中集成感知与通信的有前景的波形方案。

Abstract: We propose the affine filter bank modulation (AFBM) waveform for enhanced
integrated sensing and communications (ISAC) in sixth generation (6G), designed
by drawing on concepts from classical filter bank multicarrier modulation
(FBMC) theory and recent advances in chirp-domain waveforms, particularly
affine frequency division multiplexing (AFDM). Specifically, AFBM exhibits
several desirable properties, with emphasis on its remarkably low
peak-to-average power ratio (PAPR) and reduced out-of-band emission (OOBE) when
benchmarked against the conventional AFDM waveform under doubly-dispersive (DD)
channel conditions. In the communications setting, reliable symbol detection is
achieved using a tailored low-complexity Gaussian belief propagation
(GaBP)-based algorithm, while in the sensing setting, a range and velocity
estimation approach is developed that integrates an expectation maximization
(EM)-assisted probabilistic data association (PDA) framework to accurately
identify surrounding targets. The highlighted performance and benefits of AFBM
are validated through analytical and numerical evaluations, including
conventional metrics such as ambiguity function (AF), bit error rate (BER), and
root mean square error (RMSE), consolidating its position as a promising
waveform for next-generation wireless systems.

</details>


### [7] [Resource Allocation and Beamforming in FIM-Assisted BS and STAR-BD-RIS-Aided NOMA: A Meta-Learning Approach](https://arxiv.org/abs/2509.05692)
*Armin Farhadi,Maryam Cheraghy,Qingqing Wu,Eduard Jorswieck*

Main category: eess.SP

TL;DR: 提出了一种基于柔性智能超表面(FIM)的无线通信系统，集成STAR-BD-RIS和NOMA技术，通过Meta-SAC算法优化能效


<details>
  <summary>Details</summary>
Motivation: 探索集成柔性智能超表面、双扇区BD-RIS和非正交多址技术的先进无线通信系统，以最大化能量效率

Method: 采用多天线FIM辅助基站和双扇区BD-RIS，FIM由可独立发射信号并动态调整垂直位置的辐射元件组成，使用meta-soft actor-critic算法进行联合优化

Result: 仿真结果显示Meta-SAC算法优于Meta-DDPG算法，FIM辅助设计相比基准方案获得了显著的能效提升

Conclusion: 所提出的FIM-STAR-BD-RIS-NOMA系统通过自适应优化算法有效提升了无线通信系统的能量效率性能

Abstract: This study explores a flexible intelligent metasurface (FIM)-based wireless
communication system that integrates simultaneously transmitting and reflecting
beyond diagonal reconfigurable intelligent surfaces (STAR-BD-RIS) with
non-orthogonal multiple access (NOMA). The system features a multi-antenna
FIM-assisted base station (BS) aided by dual-sector BD-RIS. The FIM consists of
cost-effective radiating elements that can independently emit signals and
dynamically adjust their vertical positions ("morphing"). The goal is to
maximize energy efficiency by jointly optimizing BS beamforming, the
STAR-BD-RIS matrix, NOMA constraints, and the FIM surface shape under power
limits. Due to the problem's non-convexity, a meta-soft actor-critic (Meta-SAC)
algorithm is proposed for adaptive optimization. Simulation results show that
Meta-SAC outperforms the Meta-DDPG algorithm, and FIM-assisted designs yield
substantial energy efficiency gains over benchmark schemes.

</details>


### [8] [Optimal Anchor Deployment and Topology Design for Large-Scale AUV Navigation](https://arxiv.org/abs/2509.05903)
*Wei Huang,Junpeng Lu,Tianhe Xu,Jianxu Shu,Hao Zhang,Kaitao Meng,Yanan Wu*

Main category: eess.SP

TL;DR: 研究水下声学锚点最优部署拓扑结构，为AUV导航提供高质量定位服务，分析大规模水下导航系统的部署模式，推导锚点集群对导航性能的影响规律，并通过实验验证优化性能。


<details>
  <summary>Details</summary>
Motivation: 水下锚点部署稀疏且成本高昂，缺乏卫星覆盖和普遍回传能力，需要研究最优部署拓扑来提供高质量的AUV导航定位服务。

Method: 分析大规模水下导航系统可能的部署模式，制定水下锚点部署的拓扑优化方案，推导锚点集群对给定区域内导航性能的影响规律，证明高概率到达目的地的服务区域覆盖条件。

Result: 通过实验结果评估了优化性能，证明了所提出拓扑优化方法的有效性。

Conclusion: 提出了水下声学锚点最优部署拓扑的优化方法，能够为AUV导航提供高质量的定位服务，解决了水下锚点部署稀疏和成本高昂的问题。

Abstract: Seafloor acoustic anchors are an important component of AUV navigation,
providing absolute updates that correct inertial dead-reckoning. Unlike
terrestrial positioning systems, the deployment of underwater anchor nodes is
usually sparse due to the uneven distribution of underwater users, as well as
the high economic cost and difficult maintenance of underwater equipment. These
anchor nodes lack satellite coverage and cannot form ubiquitous backhaul as
terrestrial nodes do. In this paper, we investigate the optimal anchor
deployment topology to provide high-quality AUV navigation and positioning
services. We first analyze the possible deployment mode in large-scale
underwater navigation system, and formulate a topology optimization for
underwater anchor node deployment. Then, we derive a scaling law about the
influence of anchors in each cluster on the navigation performance within a
given area and demonstrate a service area coverage condition with a high
probability of reaching the destination. Finally, the optimization performance
is evaluated through experimental results.

</details>


### [9] [Active noise cancellation in ultra-low field MRI: distinct strategies for different channels](https://arxiv.org/abs/2509.05955)
*Jiali He,Sheng Shen,Jiamin Wu,Xiaohan Kong,Yamei Dai,Liang Tan,Zheng Xu*

Main category: eess.SP

TL;DR: 本文研究了超低场磁共振成像系统中通道特异性电磁干扰问题，提出了双级抑制策略，结合前端空间域逆场重建和后端通道自适应主动噪声消除，实现了80%以上的干扰抑制效果。


<details>
  <summary>Details</summary>
Motivation: 超低场磁共振成像系统在开放环境中对复合电磁干扰高度敏感，不同成像通道由于耦合特性不同而对干扰产生非均匀响应，需要研究通道特异性干扰路径并开发有效的抑制方法。

Method: 研究永久磁铁低场MRI系统中通道特异性干扰路径，发现鞍形线圈比螺线管线圈对横向EMI分量更敏感。提出双级抑制策略：前端空间域逆场重建和后端通道自适应主动噪声消除。

Result: 实验证明该方法能抑制80%以上的EMI，显著改善通道间信噪比一致性，并将融合图像信噪比提高24%。

Conclusion: 研究阐明了EMI耦合的通道依赖性特性，建立了针对性抑制策略，为未来阵列线圈超低场MRI系统的噪声抑制提供了理论基础和实践指导。

Abstract: Ultra-low field magnetic resonance imaging(ULF-MRI) systems operating in open
environments are highly susceptible to composite electromagnetic
interference(EMI). Different imaging channels respond non-uniformly to EMI
owing to their distinct coupling characteristics. Here, we investigate
channel-specific interference pathways in a permanent-magnet-based low-field
MRI system and show that saddle coils are intrinsically more vulnerable to
transverse EMI components than solenoidal coils. To mitigate these
heterogeneous coupling effects, we propose a dual-stage suppression strategy
that combines front-end spatial-domain inverse field reconstruction with
back-end channel-adaptive active noise cancellation. Experiments demonstrate
that this approach suppresses EMI by more than 80%, substantially improves
inter-channel signal-to-noise ratio(SNR) consistency, and enhances the
fused-image SNR by 24%. These findings elucidate the channel-dependent nature
of EMI coupling and establish targeted mitigation strategies, providing both a
theoretical basis and practical guidance for noise suppression in future
array-coil ULF-MRI systems.

</details>


### [10] [The Case for a DNANF 1Pb/s Trans-Atlantic Submarine Cable](https://arxiv.org/abs/2509.05959)
*Pierluigi Poggiolini,Francesco Poletti*

Main category: eess.SP

TL;DR: 基于空心元纤纤的新型海底光缆可实现每方向1Pb/s传输速率，并将缆段长度推至理论200km


<details>
  <summary>Details</summary>
Motivation: 利用最新低损耗空心元纤纤技术，构建高速率、长距离的海底光缆通信系统

Method: 研究空心元纤纤在海底光缆中的应用，采用双向传输技术提升传输能力

Result: 理论上可实现1Pb/s每方向传输速率，缆段长度最高可达200km

Conclusion: 空心元纤纤技术有望带来海底光缆通信的重大突破，显著提升传输能力和减少中继站数量

Abstract: The recent progress in low-loss hollow-core fibers allows to speculate on the
possibility of building a transatlantic submarine cable that can achieve the
goal of 1 Pb/s per direction, leveraging bidirectional transmission, and at the
same time drastically increase span length, theoretically to 200km.

</details>


### [11] [DeepStream: Prototyping Deep Joint Source-Channel Coding for Real-Time Multimedia Transmissions](https://arxiv.org/abs/2509.05971)
*Kaiyi Chi,Yinghui He,Qianqian Yang,Zhiping Jiang,Yuanchao Shu,Zhiqin Wang,Jun Luo,Jiming Chen*

Main category: eess.SP

TL;DR: DeepStream是一个基于OFDM技术的DeepJSCC原型系统，用于多媒体传输，在低信噪比环境下显著优于传统方案


<details>
  <summary>Details</summary>
Motivation: 现有的DeepJSCC研究大多停留在数值模拟阶段，缺乏实际验证，需要开发真实可用的系统来验证其实际可行性

Method: 开发了特征到符号映射方法和跨子载波预编码方法以提高子载波独立性并降低峰均功率比；提出渐进编码策略根据延迟调整压缩比；基于软件定义无线电实现实时图像传输和视频流

Result: 在10dB SNR下，DeepStream实现图像传输PSNR 35dB和视频流MS-SSIM 20dB，而标准方案无法恢复有意义信息

Conclusion: DeepStream成功验证了DeepJSCC在真实世界中的可行性，为6G通信提供了高效可靠的多媒体传输解决方案

Abstract: Deep learning-based joint source-channel coding (DeepJSCC) has emerged as a
promising technique in 6G for enhancing the efficiency and reliability of data
transmission across diverse modalities, particularly in low signal-to-noise
ratio (SNR) environments. This advantage is realized by leveraging powerful
neural networks to learn an optimal end-to-end mapping from the source data
directly to the transmit symbol sequence, eliminating the need for separate
source coding, channel coding, and modulation. Although numerous efforts have
been made towards efficient DeepJSCC, they have largely stayed at numerical
simulations that can be far from practice, leaving the real-world viability of
DeepJSCC largely unverified. To this end, we prototype DeepStream upon
orthogonal frequency division multiplexing (OFDM) technology to offer efficient
and robust DeepJSCC for multimedia transmission. In conforming to OFDM, we
develop both a feature-to-symbol mapping method and a cross-subcarrier
precoding method to improve the subcarrier independence and reduce
peak-to-average power ratio. To reduce system complexity and enable flexibility
in accommodating varying quality of service requirements, we further propose a
progressive coding strategy that adjusts the compression ratio based on latency
with minimal performance loss. We implement DeepStream for real-time image
transmission and video streaming using software-defined radio. Extensive
evaluations verify that DeepStream outperforms both the standard scheme and the
direct deployment scheme. Particularly, at an SNR of 10 dB, DeepStream achieves
a PSNR of 35 dB for image transmission and an MS-SSIM of 20 dB for video
streaming, whereas the standard scheme fails to recover meaningful information.

</details>


### [12] [3D-Image Reconstruction using MIMO-SAR FMCW Radar](https://arxiv.org/abs/2509.05977)
*Ayush Jha,Dhanireddy Chandrika,Chandra Sekhar Seelamantula,Chetan Singh Thakur*

Main category: eess.SP

TL;DR: 基于虚拟MIMO FMCW雷达与SAR技术的结合，提出了一种高分辨率皀波雷达3D快速时域重建算法


<details>
  <summary>Details</summary>
Motivation: 传统SAR成像算法主要关注2D信息，在3D场景重建方面有限制，需要突破这一限制

Method: 结合虚拟多输入多输出(MIMO)频率调制连续波(FMCW)雷达与综合孔径雷达(SAR)技术的精度，开发快速时域重建算法

Result: 实现了高分辨率皀3D雷达成像，为充分利用皀波频段皀雷达成像应用奠定基础

Conclusion: 该算法为高级雷达成像应用开启了新时代，在学术研究和工业应用中具有重要意义

Abstract: With the advancement of millimeter-wave radar technology, Synthetic Aperture
Radar (SAR) imaging at millimeter-wave frequencies has gained significant
attention in both academic research and industrial applications. However,
traditional SAR imaging algorithms primarily focus on extracting
two-dimensional information from detected targets, which limits their potential
for 3D scene reconstruction. In this work, we demonstrated a fast time-domain
reconstruction algorithm for achieving high-resolution 3D radar imaging at
millimeter-wave (mmWave) frequencies. This approach leverages a combination of
virtual Multiple Input Multiple Output (MIMO) Frequency Modulated Continuous
Wave (FMCW) radar with the precision of Synthetic Aperture Radar (SAR)
technique, setting the stage for a new era of advanced radar imaging
applications.

</details>


### [13] [Quantum Radar for ISAC: Sum-Rate Optimization](https://arxiv.org/abs/2509.06070)
*Abdulmohsen Alsaui,Neel Kanth Kundu,Hyundong Shin,Octavia A. Dobre*

Main category: eess.SP

TL;DR: 这篇论文提出了一种集成量子照明雷达与基站的新框架，在低信号力高噪声条件下实现了量子增强目标检测和全双工经典通信的同时支持，显著提升了通信吞吐量保持感知性能。


<details>
  <summary>Details</summary>
Motivation: 传统ISAC系统在低信号力高噪声条件下存在根本限制，需要量子技术来突破这些限制并提升性能。

Method: 通过将量子照明雷达嵌入基站，构建IQSCC系统，采用连续凸近似技术优化发射功率和放大向量的非凸联合优化问题，并在统计检测理论下推导性能上界。

Result: 模拟结果显示，提出的IQSCC系统在满足感知要求的前提下，通信吞吐量显著高于传统ISAC基准系统，尤其在低信干歧比噪声治下显示出量子优势。

Conclusion: 该研究成功展示了量子技术在集成感知通信系统中的潜力，为应对极端环境下的感知挑战提供了有效解决方案，推动了量子增强无线网络的发展。

Abstract: Integrated sensing and communication (ISAC) is emerging as a key enabler for
spectrum-efficient and hardware-converged wireless networks. However, classical
radar systems within ISAC architectures face fundamental limitations under low
signal power and high-noise conditions. This paper proposes a novel framework
that embeds quantum illumination radar into a base station to simultaneously
support full-duplex classical communication and quantum-enhanced target
detection. The resulting integrated quantum sensing and classical communication
(IQSCC) system is optimized via a sum-rate maximization formulation subject to
radar sensing constraints. The non-convex joint optimization of transmit power
and beamforming vectors is tackled using the successive convex approximation
technique. Furthermore, we derive performance bounds for classical and quantum
radar protocols under the statistical detection theory, highlighting the
quantum advantage in low signal-to-interference-plus-noise ratio regimes.
Simulation results demonstrate that the proposed IQSCC system achieves a higher
communication throughput than the conventional ISAC baseline while satisfying
the sensing requirement.

</details>


### [14] [Pinching Antenna System (PASS) Enhanced Covert Communications: Against Warden via Sensing](https://arxiv.org/abs/2509.06170)
*Hao Jiang,Zhaolin Wang,Yuanwei Liu,Arumugam Nallanathan,Zhiguo Ding*

Main category: eess.SP

TL;DR: 本文提出了一种基于拉拉天线系统(PASS)的感知助力隐藏通信网络，通过动态重配天线位置和感知跟踪恶意监听者来提升通信隐藏性。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置MIMO数组在隐藏通信中存在隐藏性不足的问题，需要一种能够动态调整天线位置提高隐藏性的方案。

Method: 采用扩展卡尔曼滤波(EKF)进行监听者踪踪，通过子空间方法联合设计波束成型和人工噪声，并使用深度强化学习(DRL)优化拉拉天线位置。

Result: 数值结果显示：EKF方法能以低复杂度准确踪踪监听者CSI，方案表现超过贪婪和搜索基准方法，PASS系统性能优于传统全数字MIMO系统。

Conclusion: 该研究成功开发了一种通过动态天线重配和感知跟踪来提升隐藏通信性能的有效方案，为隐藏通信网络提供了新的设计自由度。

Abstract: A sensing-aided covert communication network empowered by pinching antenna
systems (PASS) is proposed in this work. Unlike conventional fixed-position
MIMO arrays, PASS dynamically reconfigures its pinching antennas (PAs) closer
to the legitimate user, substantially enhancing covertness. To further secure
the adversary's channel state information (CSI), a sensing function is
leveraged to track the malicious warden's movements. In particular, this paper
first proposes an extended Kalman filter (EKF) based approach to fulfilling the
tracking function. Building on this, a covert communication problem is
formulated with a joint design of beamforming, artificial noise (AN) signals,
and the position of PAs. Then, the beamforming and AN design subproblems are
resolved jointly with a subspace approach, while the PA position optimization
subproblem is handled by a deep reinforcement learning (DRL) approach by
treating the evolution of the warden's mobility status as a temporally
corrected process. Numerical results are presented and demonstrate that: i) the
EKF approach can accurately track the warden's CSI with low complexity, ii) the
effectiveness of the proposed solution is verified by its outperformance over
the greedy and searching-based benchmarks, and iii) with new design degrees of
freedom (DoFs), the performance of PASS is superior to the conventional
fully-digital MIMO systems.

</details>


### [15] [Optimal Distortion-Aware Multi-User Power Allocation for Massive MIMO Networks](https://arxiv.org/abs/2509.06491)
*Siddarth Marwaha,Pawel Kryszkiewicz,Eduard Jorswieck*

Main category: eess.SP

TL;DR: 该论文提出了一种考虑功率放大器非线性失真的最优功率分配策略，在OFDM大规模MIMO系统中显著提升了和速率性能


<details>
  <summary>Details</summary>
Motivation: 现实无线发射机前端存在非线性行为（如功率放大器削波），现有资源分配方案忽略这种失真会导致结果不准确和性能下降

Method: 采用软限幅器PA模型，推导宽带信噪失真比(SNDR)，将非凸优化问题分解为总功率分配和用户间功率分布两个子问题，提出交替优化算法

Result: 仿真结果显示相比忽略失真的方案，64天线基站服务60用户时中位数提升4倍，512天线时中位数提升50%

Conclusion: 功率放大器失真会形成SNDR有效工作点，无需显式发射功率约束，所提算法能有效解决非线性失真下的功率分配问题

Abstract: Real-world wireless transmitter front-ends exhibit certain nonlinear
behavior, e.g., signal clipping by a Power Amplifier (PA). Although many
resource allocation solutions do not consider this for simplicity, it leads to
inaccurate results or a reduced number of degrees of freedom, not achieving the
global performance. In this work, we propose an optimal PA distortion-aware
power allocation strategy in a downlink orthogonal frequency division multiplex
(OFDM) based massive multiple-input multiple-output (M-MIMO) system. Assuming a
soft-limiter PA model, where the transmission occurs under small-scale
independent and identically distributed (i.i.d) Rayleigh fading channel, we
derive the wideband signal-to-noise-and-distortion ratio (SNDR) and formulate
the power allocation problem. Most interestingly, the distortion introduced by
the PA leads to an SNDR-efficient operating point without explicit transmit
power constraints. While the optimization problem is non-convex, we decouple it
into a non-convex total power allocation problem and a convex power
distribution problem among the users (UEs). We propose an alternating
optimization algorithm to find the optimum solution. Our simulation results
show significant sum-rate gains over existing distortion-neglecting solutions,
e.g., a median 4 times increase and a median 50\% increase for a 64-antenna and
512-antenna base station serving 60 users, respectively.

</details>


### [16] [Synesthesia of Machines (SoM)-Aided LiDAR Point Cloud Transmission for Collaborative Perception](https://arxiv.org/abs/2509.06506)
*Ensong Liu,Rongqing Zhang,Xiang Cheng,Jian Tang*

Main category: eess.SP

TL;DR: 通过密度保持深度压缩和渡驾学习技术，提出LPC-FT系统实现高效、稳健的汽车间杆距离晶体管点云传输，支持协同感知任务


<details>
  <summary>Details</summary>
Motivation: 解决LiDAR点云数据量大导致的传输延迟问题，提高多机器人协同感知的效率和准确性

Method: 使用密度保持深度压缩方法编码点云，设计基于self-attention的通道编码模块和基于cross-attention的特征融合模块，利用非线性激活层和迁移学习对投数字通道噪声

Result: 比传统八叉树压缩等方法更稳健高效，Chamfer距离降低30%，PSNR提高1.9dB

Conclusion: LPC-FT系统具有优秀的重建性能和通道适应能力，能够有效支持协同感知任务

Abstract: Collaborative perception enables more accurate and comprehensive scene
understanding by learning how to share information between agents, with LiDAR
point clouds providing essential precise spatial data. Due to the substantial
data volume generated by LiDAR sensors, efficient point cloud transmission is
essential for low-latency multi-agent collaboration. In this work, we propose
an efficient, robust and applicable LiDAR point cloud transmission system via
the Synesthesia of Machines (SoM), termed LiDAR Point Cloud Feature
Transmission (LPC-FT), to support collaborative perception among multiple
agents. Specifically, we employ a density-preserving deep point cloud
compression method that encodes the complete point cloud into a downsampled
efficient representation. To mitigate the effects of the wireless channel, we
design a channel encoder module based on self-attention to enhance LiDAR point
cloud features and a feature fusion module based on cross-attention to
integrate features from transceivers. Furthermore, we utilize the nonlinear
activation layer and transfer learning to improve the training of deep neural
networks in the presence the digital channel noise. Experimental results
demonstrate that the proposed LPC-FT is more robust and effective than
traditional octree-based compression followed by channel coding, and
outperforms state-of-the-art deep learning-based compression techniques and
existing semantic communication methods, reducing the Chamfer Distance by 30%
and improving the PSNR by 1.9 dB on average. Owing to its superior
reconstruction performance and robustness against channel variations, LPC-FT is
expected to support collaborative perception tasks.

</details>


### [17] [Integrated Detection and Tracking Based on Radar Range-Doppler Feature](https://arxiv.org/abs/2509.06569)
*Chenyu Zhang,Yuanhang Wu,Xiaoxi Ma,Wei Yi*

Main category: eess.SP

TL;DR: 提出基于雷达特征的联合检测跟踪方法InDT，通过神经网络提取RD矩阵特征，结合检测置信度自适应更新卡尔曼滤波器，在模拟和公开数据集上验证有效性


<details>
  <summary>Details</summary>
Motivation: 当前联合检测跟踪方法在充分利用雷达信号潜力方面存在挑战，包括CFAR模型信息表示能力有限、复杂场景描述不足以及跟踪器获取信息有限

Method: InDT方法包含雷达信号检测网络架构和检测辅助的跟踪器。检测器从RD矩阵提取特征，通过特征增强模块和检测头返回目标位置。跟踪器基于检测置信度自适应更新卡尔曼滤波器的测量噪声协方差，使用余弦距离测量目标RD特征相似度

Result: 在模拟数据和公开数据集上验证了方法的有效性

Conclusion: 提出的InDT方法能够更好地利用雷达信号特征，提高检测和跟踪性能

Abstract: Detection and tracking are the basic tasks of radar systems. Current joint
detection tracking methods, which focus on dynamically adjusting detection
thresholds from tracking results, still present challenges in fully utilizing
the potential of radar signals. These are mainly reflected in the limited
capacity of the constant false-alarm rate model to accurately represent
information, the insufficient depiction of complex scenes, and the limited
information acquired by the tracker. We introduce the Integrated Detection and
Tracking based on radar feature (InDT) method, which comprises a network
architecture for radar signal detection and a tracker that leverages detection
assistance. The InDT detector extracts feature information from each
Range-Doppler (RD) matrix and then returns the target position through the
feature enhancement module and the detection head. The InDT tracker adaptively
updates the measurement noise covariance of the Kalman filter based on
detection confidence. The similarity of target RD features is measured by
cosine distance, which enhances the data association process by combining
location and feature information. Finally, the efficacy of the proposed method
was validated through testing on both simulated data and publicly available
datasets.

</details>


### [18] [Towards In-Air Ultrasonic QR Codes: Deep Learning for Classification of Passive Reflector Constellations](https://arxiv.org/abs/2509.06615)
*Wouter Jansen,Jan Steckel*

Main category: eess.SP

TL;DR: 本文提出使用多标签CNN从单次3D声纳测量中同时识别多个紧密排列的反射器，通过反射器星座作为编码标签来增加信息容量，验证了复杂声学模式解码的可行性。


<details>
  <summary>Details</summary>
Motivation: 在视觉传感器失效的环境中，空中声纳为自主系统提供了可靠的替代方案。现有研究已能分类单个声学地标，但需要增加信息容量，因此引入反射器星座作为编码标签。

Method: 开发了多标签卷积神经网络(CNN)来同时识别多个紧密排列的反射器；研究了使用自适应波束成形和零陷技术来隔离单个反射器进行单标签分类。

Result: 在小数据集上的初步发现证实了该方法的可行性，验证了解码这些复杂声学模式的能力。

Conclusion: 讨论了实验结果和局限性，为开发具有显著增加信息熵的声学地标系统及其准确鲁棒的检测和分类提供了关键见解和未来方向。

Abstract: In environments where visual sensors falter, in-air sonar provides a reliable
alternative for autonomous systems. While previous research has successfully
classified individual acoustic landmarks, this paper takes a step towards
increasing information capacity by introducing reflector constellations as
encoded tags. Our primary contribution is a multi-label Convolutional Neural
Network (CNN) designed to simultaneously identify multiple, closely spaced
reflectors from a single in-air 3D sonar measurement. Our initial findings on a
small dataset confirm the feasibility of this approach, validating the ability
to decode these complex acoustic patterns. Secondly, we investigated using
adaptive beamforming with null-steering to isolate individual reflectors for
single-label classification. Finally, we discuss the experimental results and
limitations, offering key insights and future directions for developing
acoustic landmark systems with significantly increased information entropy and
their accurate and robust detection and classification.

</details>


### [19] [Near-Threshold Voltage Massive MIMO Computing](https://arxiv.org/abs/2509.06651)
*Mikael Rinkinen,Mehdi Safarpour,Shahriar Shahabuddin,Olli Silven,Lauri Koskinen*

Main category: eess.SP

TL;DR: 本文提出将算法级容错(ABFT)技术应用于大规模MIMO信号处理，通过修改矩阵运算牛顿迭代算法来检测计算错误，在近阈值计算(NTC)环境下实现36%的功耗节省，仅带来3%的计算开销。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统虽然能显著提升频谱效率，但其高功耗问题阻碍了广泛应用。近阈值计算(NTC)能降低能耗但易受工艺、电压、温度变化影响导致计算错误，传统电路级容错方法硬件复杂度高且难以实现。

Method: 修改矩阵算术牛顿迭代MIMO算法，无缝集成ABFT技术，通过检查最终结果来检测计算错误。ABFT作为轻量级错误检测方法，专为矩阵运算设计，无需电路级修改，可纯软件实现。

Result: 在可重构硬件平台上实现的MIMO加速器实验表明，对于足够大的问题规模，在默认时钟频率下，相比基线方法实现了36%的功耗节省，平均仅带来3%的计算开销。

Conclusion: ABFT与近阈值操作的结合为开发高能效且鲁棒的大规模MIMO处理器提供了可行路径，能够在显著降低功耗的同时保持计算可靠性。

Abstract: Massive MIMO systems have the potential to significantly enhance spectral
efficiency, yet their widespread integration is hindered by the high power
consumption of the underlying computations. This paper explores the
applicability and effectiveness of Algorithm-Based Fault Tolerance (ABFT) for
massive MIMO signal processing to tackle the reliability challenge of Near
Threshold Computing (NTC). We propose modifying matrix arithmetic Newton
iteration MIMO algorithm to seamlessly integrate ABFT to detect any
computational errors by inspecting the final result. The overhead from ABFT
depends largely on the matrix dimensions, which in this context are dictated by
the number of user equipments involved in the computation. NTC is a promising
strategy for reducing the energy consumption in digital circuits by operating
transistors at extremely reduced voltages. However, NTC is highly susceptible
to variations in Process, Voltage, and Temperature (PVT) which can lead to
increased error rates in computations. Traditional techniques for enabling NTC,
such as dynamic voltage and frequency scaling guided by circuit level timing
error detection methods, introduce considerable hardware complexity and are
difficult to implement at high clock frequencies. In this context ABFT has
emerged as a lightweight error detection method tailored for matrix operations
without requiring any modifications on circuit-level and can be implemented
purely in software.A MIMO accelerator was implemented on a reconfigurable
hardware platform. Experimental results demonstrate that for sufficiently large
problem sizes, the proposed method achieves a 36% power saving compared to
baseline, with only an average of 3% computational overhead, at default clock
frequency. These results indicate that combining ABFT with near-threshold
operation provides a viable path toward energy-efficient and robust massive
MIMO processors.

</details>


### [20] [SE and EE Tradeoff in Active STAR-RIS Assisted Systems With Hardware Impairments](https://arxiv.org/abs/2509.06662)
*Ao Huang,Xidong Mu,Li Guo,Guangyu Zhu*

Main category: eess.SP

TL;DR: 本文研究在硬件损伤条件下主动式同时透射反射可重构智能表面辅助通信系统的资源效率最大化问题，通过联合优化基站波束成形和主动STAR-RIS波束成形来实现频谱效率和能量效率的最优权衡


<details>
  <summary>Details</summary>
Motivation: 在实际硬件损伤条件下，需要解决主动STAR-RIS辅助通信系统中频谱效率和能量效率的权衡优化问题，以提高系统资源效率

Method: 采用二次变换方法简化分数目标函数，开发基于交替优化的算法迭代更新基站和STAR-RIS波束成形系数

Result: 仿真结果表明，在硬件损伤存在的情况下，所提方案性能优于其他基准方案，并分析了不同发射功率预算下可实现的SE-EE区域变化

Conclusion: 提出的算法能有效处理硬件损伤条件下的资源效率优化问题，为主动STAR-RIS系统的实际部署提供了理论支持

Abstract: This paper investigates the problem of resource efficiency maximization in an
active simultaneously transmitting and reflecting reconfigurable intelligent
surface (STAR-RIS) assisted communication system under practical transceiver
hardware impairments (HWIs). We aim to obtain an optimal tradeoff between
system spectral efficiency (SE) and energy efficiency (EE), by jointly
optimizing the base station (BS) transmit beamforming and the active STAR-RIS
beamforming. To tackle the challenges in the fractional objective function, we
begin by applying the quadratic transformation method to simplify it into a
manageable form. An alternating optimization-based algorithm is then developed
to iteratively update the BS and STAR-RIS beamforming coefficients. Simulation
results demonstrate that the proposed scheme performs better than other
baseline schemes in the presence of HWIs. Moreover, the variation of the
achievable SE-EE region with different transmit power budgets is analyzed.

</details>


### [21] [ISAC Imaging by Channel State Information using Ray Tracing for Next Generation 6G](https://arxiv.org/abs/2509.06672)
*Ahmad Bazzi,Mingjun Ying,Ojas Kanhere,Theodore S. Rappaport,Marwa Chafii*

Main category: eess.SP

TL;DR: 本文提出了一种基于港道状态信息的ISAC成像框架，通过两段反射点优化算法实现了多跳反射的精确几何重建，在6.75GHz上首次展示了基于无线电影蹭的多跳ISAC成像技术。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信(ISAC)是6G的核心技术，需要统一连接性和环境映射。多跳反射的几何重建是一项具有挑战性的任务，本文强调了在6.75GHz上实现多跳ISAC成像的重要性。

Method: 利用港道状态信息的每路径分量、发射机和接收机位置，通过角度和延迟信息融合提取等效反射点。核心方法是两段反射点优化算法，独立估计从发射机和接收机位置到物体表面等效反射点的路径长度，从而实现精确的几何重建。

Result: 实验结果验证了所提出的ISAC成像框架能够准确地重建物体表面、边缘和曲线特征。通过聚合多个发射机和接收机位置对的等效反射点，生成了表示港道中物体的密集三维点云。

Conclusion: 本文在6.75GHz上首次实现了基于无线电影蹭的多跳ISAC成像技术，为6G集成感知与通信系统提供了一种精确环境映射的新方法。两段反射点优化算法有效解决了多跳反射的几何重建挑战，具有重要的实践价值。

Abstract: Integrated sensing and communications (ISAC) is emerging as a cornerstone
technology for sixth generation (6G) wireless systems, unifying connectivity
and environmental mapping through shared hardware, spectrum, and waveforms. The
following paper presents an ISAC imaging framework utilizing channel state
information (CSI) per-path components, transmitter (TX) positions, and receiver
(RX) positions obtained from the calibrated NYURay ray tracer at 6.75 GHz in
the upper mid-band. Our work shows how each resolvable multipath component can
be extracted from CSI estimation and cast into an equivalent three-dimensional
reflection point by fusing its angle and delay information, which is useful and
challenging for multi-bounce reflections. The primary contribution of the paper
is the two-segment reflection point optimization algorithm, which independently
estimates the path lengths from the TX position and RX position to an
equivalent reflection point (ERP) on the object surface, thus enabling precise
geometric reconstruction. Subsequently, we aggregate the ERPs derived from
multiple pairs of TX and RX positions, generating dense three dimensional point
clouds representing the objects in the channel. Experimental results validate
that the proposed ISAC imaging framework accurately reconstructs object
surfaces, edges, and curved features. To the best of our knowledge, this paper
provides the first demonstration of multi bounce ISAC imaging using wireless
ray tracing at 6.75 GHz.

</details>


### [22] [RadHARSimulator V1: Model-Based FMCW Radar Human Activity Recognition Simulator](https://arxiv.org/abs/2509.06751)
*Weicheng Gao*

Main category: eess.SP

TL;DR: 一个基于模型的FMCW雷达人体活动识别模拟器，通过人体动力学模型和雷达回波模型生成高保真的微多普勒信号，为雷达HAR算法设计提供价值差的模拟数据集。


<details>
  <summary>Details</summary>
Motivation: 解决雷达基于人体活动识别(HAR)中多样性高保真数据集获取困难的挑战，提供可靠的模拟数据来支持算法开发和验证。

Method: 集成13个散射体的人体动力学模型模拟12种不同活动，采用FMCW雷达回波模型包含动态雷达截面积、自由空间/穿墙传播和检查噪声底。数据经过MTI、调频补偿、Savitzky-Golay去噪处理，生成高分辨率的距离-时间图和多普勒-时间图。

Result: 数值实验证明模拟器能够生成高保真且特征明显的微多普勒签名，为雷达HAR算法设计提供了有价值的工具。

Conclusion: 该模型基于的FMCW雷达HAR模拟器有效解决了真实数据获取难的问题，能够生成高质量的模拟数据，为雷达人体活动识别领域提供了可靠的算法开发和验证平台。

Abstract: Radar-based human activity recognition (HAR) is a pivotal research area for
applications requiring non-invasive monitoring. However, the acquisition of
diverse and high-fidelity radar datasets for robust algorithm development
remains a significant challenge. To overcome this bottleneck, a model-based
frequency-modulated continuous wave (FMCW) radar HAR simulator is developed.
The simulator integrates an anthropometrically scaled $13$-scatterer kinematic
model to simulate $12$ distinct activities. The FMCW radar echo model is
employed, which incorporates dynamic radar cross-section (RCS), free-space or
through-the-wall propagation, and a calibrated noise floor to ensure signal
fidelity. The simulated raw data is then processed through a complete pipeline,
including moving target indication (MTI), bulk Doppler compensation, and
Savitzky-Golay denoising, culminating in the generation of high-resolution
range-time map (RTM) and Doppler-time maps (DTMs) via both short-time Fourier
transform (STFT) and Fourier synchrosqueezed transform (FSST). Finally, a novel
neural network method is proposed to validate the effectiveness of the radar
HAR. Numerical experiments demonstrate that the simulator successfully
generates high-fidelity and distinct micro-Doppler signature, which provides a
valuable tool for radar HAR algorithm design and validation. The installer of
this simulator is released at:
\href{https://github.com/JoeyBGOfficial/RadHARSimulatorV1-Model-Based-FMCW-Radar-Human-Activity-Recognition-Simulator}{Github/JoeyBGOfficial/RadHARSimulatorV1}.

</details>


### [23] [Green Learning for STAR-RIS mmWave Systems with Implicit CSI](https://arxiv.org/abs/2509.06820)
*Yu-Hsiang Huang,Po-Heng Chou,Wan-Jen Huang,Walid Saad,C. -C. Jay Kuo*

Main category: eess.SP

TL;DR: 基于绿色学习的STAR-RIS助力毫米波MIMO广播系统预编码框架，无需显式信道估计，计算复杂度低4个数量级，适用于实时能源受限场景


<details>
  <summary>Details</summary>
Motivation: 面向6G网络环境可持续性需求，通过广播传输架构提升频谱效率和减少冗余传输与功耗，避免传统优化方法对完整信道状态信息的依赖和迭代计算

Method: 综合子空间近似调偏系数(Saab)、相关特征测试(RFT)监督特征选择和极端梯度提升(XGBoost)决策学习，联合预测STAR-RIS系数和发射预编码器，在无需显式CSI估计情况下进行推理

Result: 与BCD和DL基准模型相比获得竞争性频谱效率，同时将浮点运算次数(FLOPs)降低超过4个数量级

Conclusion: 该GL方法通过免除显式CSI估计和深度神经网络，实现了轻量化设计，在保持性能的同时显著降低计算复杂度，适合能源和硬件受限的广播场景实时部署

Abstract: In this paper, a green learning (GL)-based precoding framework is proposed
for simultaneously transmitting and reflecting reconfigurable intelligent
surface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems.
Motivated by the growing emphasis on environmental sustainability in future 6G
networks, this work adopts a broadcasting transmission architecture for
scenarios where multiple users share identical information, improving spectral
efficiency and reducing redundant transmissions and power consumption.
Different from conventional optimization methods, such as block coordinate
descent (BCD) that require perfect channel state information (CSI) and
iterative computation, the proposed GL framework operates directly on received
uplink pilot signals without explicit CSI estimation. Unlike deep learning (DL)
approaches that require CSI-based labels for training, the proposed GL approach
also avoids deep neural networks and backpropagation, leading to a more
lightweight design. Although the proposed GL framework is trained with
supervision generated by BCD under full CSI, inference is performed in a fully
CSI-free manner. The proposed GL integrates subspace approximation with
adjusted bias (Saab), relevant feature test (RFT)-based supervised feature
selection, and eXtreme gradient boosting (XGBoost)-based decision learning to
jointly predict the STAR-RIS coefficients and transmit precoder. Simulation
results show that the proposed GL approach achieves competitive spectral
efficiency compared to BCD and DL-based models, while reducing floating-point
operations (FLOPs) by over four orders of magnitude. These advantages make the
proposed GL approach highly suitable for real-time deployment in energy- and
hardware-constrained broadcasting scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning](https://arxiv.org/abs/2509.05316)
*Praveen Bushipaka,Lucia Passaro,Tommaso Cucinotta*

Main category: cs.LG

TL;DR: 本文系统性评估了LLM反学习中的常见实践，发现使用单一邻近集和标准采样方法存在问题，并提出了模块化实体级反学习策略作为改进方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM反学习研究中常用单一邻近集和标准采样方法，但这些方法的效果和稳定性未经过严格评估，不能反映真实世界数据的复杂性和关系。

Method: 系统性评估了常见的反学习实践，包括采样方法和邻近集使用。提出了模块化实体级反学习(MELU)策略，以及多样化邻近集的使用方法。

Result: 研究发现：1)依赖单一邻近集是次优的；2)标准1:1采样方法效率低且效果差；3)MELU策略能够提供明确稳定的反学习路径。

Conclusion: 通过采用多样化邻近集和模块化实体级反学习策略，结合稳健算法，可以实现效果更好的LLM反学习。

Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and
"retain", with the objectives of removing the undesired knowledge from the
forget set while preserving the remaining knowledge from the retain. In
privacy-focused unlearning research, a retain set is often further divided into
neighbor sets, containing either directly or indirectly connected to the forget
targets; and augmented by a general-knowledge set. A common practice in
existing benchmarks is to employ only a single neighbor set, with general
knowledge which fails to reflect the real-world data complexities and
relationships. LLM Unlearning typically involves 1:1 sampling or cyclic
iteration sampling. However, the efficacy and stability of these de facto
standards have not been critically examined. In this study, we systematically
evaluate these common practices. Our findings reveal that relying on a single
neighbor set is suboptimal and that a standard sampling approach can obscure
performance trade-offs. Based on this analysis, we propose and validate an
initial set of best practices: (1) Incorporation of diverse neighbor sets to
balance forget efficacy and model utility, (2) Standard 1:1 sampling methods
are inefficient and yield poor results, (3) Our proposed Modular Entity-Level
Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate
that this modular approach, combined with robust algorithms, provides a clear
and stable path towards effective unlearning.

</details>


### [25] [Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance](https://arxiv.org/abs/2509.05328)
*Xiang Yuan,Jun Shu,Deyu meng,Zongben Xu*

Main category: cs.LG

TL;DR: 提出一种新的正则化方法，通过在函数空间中约束微调模型与预训练模型的距离，并引入一致性正则化来提升OOD鲁棒性，在各种CLIP骨干网络上均能一致提升下游任务的ID性能和OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒微调方法主要保留预训练权重、特征或logits，但这些方法不能总是提升不同模型架构的OOD鲁棒性，因为OOD鲁棒性需要模型函数对下游任务输入信息产生稳定预测。

Method: 提出在函数空间中约束微调模型与预训练模型距离的正则化方法，使用模拟OOD样本；同时引入一致性正则化来促进扰动样本的稳定预测。

Result: 大量实验表明，该方法在各种CLIP骨干网络上都能一致提升下游任务的ID微调性能和OOD鲁棒性，优于现有的基于正则化的鲁棒微调方法。

Conclusion: 通过在函数空间中进行正则化约束并结合一致性正则化，能够有效保持预训练模型的OOD鲁棒性，同时提升下游任务的性能。

Abstract: Robust fine-tuning aims to achieve competitive in-distribution (ID)
performance while maintaining the out-of-distribution (OOD) robustness of a
pre-trained model when transferring it to a downstream task. To remedy this,
most robust fine-tuning methods aim to preserve the pretrained weights,
features, or logits. However, we find that these methods cannot always improve
OOD robustness for different model architectures. This is due to the OOD
robustness requiring the model function to produce stable prediction for input
information of downstream tasks, while existing methods might serve as a poor
proxy for the optimization in the function space. Based on this finding, we
propose a novel regularization that constrains the distance of fine-tuning and
pre-trained model in the function space with the simulated OOD samples, aiming
to preserve the OOD robustness of the pre-trained model. Besides, to further
enhance the OOD robustness capability of the fine-tuning model, we introduce an
additional consistency regularization to promote stable predictions of
perturbed samples. Extensive experiments demonstrate our approach could
consistently improve both downstream task ID fine-tuning performance and OOD
robustness across a variety of CLIP backbones, outperforming existing
regularization-based robust fine-tuning methods.

</details>


### [26] [Safeguarding Graph Neural Networks against Topology Inference Attacks](https://arxiv.org/abs/2509.05429)
*Jie Fu,Hong Yuan,Zhili Chen,Wendy Hui Wang*

Main category: cs.LG

TL;DR: 本文研究了GNN中的拓扑隐私风险，提出了拓扑推断攻击(TIAs)并发现现有边级差分隐私机制不足，进而提出了Private Graph Reconstruction(PGR)防御框架来保护拓扑隐私同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: GNN在图形数据学习中表现出强大能力，但其广泛应用引发了严重的隐私问题。先前研究主要关注边级隐私，而拓扑隐私（图整体结构的机密性）这一关键威胁尚未得到充分探索。

Method: 提出了拓扑推断攻击(TIAs)套件，能够仅通过黑盒访问GNN模型来重建目标训练图的结构。为应对此挑战，引入了Private Graph Reconstruction(PGR)防御框架，将其表述为双层优化问题，通过元梯度迭代生成合成训练图，并基于演化图同时更新GNN模型。

Result: 研究发现GNN对这些攻击高度脆弱，现有边级差分隐私机制要么无法缓解风险，要么严重损害模型精度。大量实验证明PGR显著减少了拓扑泄露，同时对模型精度影响最小。

Conclusion: 拓扑隐私是GNN中一个被低估但严重的威胁，需要专门的防御机制。PGR框架有效解决了这一问题，在保护拓扑隐私的同时保持了模型性能。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning
from graph-structured data. However, their widespread adoption has raised
serious privacy concerns. While prior research has primarily focused on
edge-level privacy, a critical yet underexplored threat lies in topology
privacy - the confidentiality of the graph's overall structure. In this work,
we present a comprehensive study on topology privacy risks in GNNs, revealing
their vulnerability to graph-level inference attacks. To this end, we propose a
suite of Topology Inference Attacks (TIAs) that can reconstruct the structure
of a target training graph using only black-box access to a GNN model. Our
findings show that GNNs are highly susceptible to these attacks, and that
existing edge-level differential privacy mechanisms are insufficient as they
either fail to mitigate the risk or severely compromise model accuracy. To
address this challenge, we introduce Private Graph Reconstruction (PGR), a
novel defense framework designed to protect topology privacy while maintaining
model accuracy. PGR is formulated as a bi-level optimization problem, where a
synthetic training graph is iteratively generated using meta-gradients, and the
GNN model is concurrently updated based on the evolving graph. Extensive
experiments demonstrate that PGR significantly reduces topology leakage with
minimal impact on model accuracy. Our code is anonymously available at
https://github.com/JeffffffFu/PGR.

</details>


### [27] [Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis](https://arxiv.org/abs/2509.05449)
*Disha Makhija,Manoj Ghuhan Arivazhagan,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.LG

TL;DR: 本文提出了memTrace框架，通过分析LLM内部表示（隐藏状态和注意力模式）而非仅输出，来检测训练数据成员信息，在MIA基准测试中达到0.85的平均AUC分数。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明成员推理攻击对大型语言模型效果有限，但作者认为仅分析模型输出可能忽略了内部表示中的隐私泄露信号，需要从模型内部行为角度探索潜在的成员推断线索。

Method: 提出memTrace框架，通过分析transformer隐藏状态和注意力模式的层间表示动态、注意力分布特征和跨层转换模式，检测传统基于损失的方法可能遗漏的记忆指纹。

Result: 在多个模型家族上实现了强大的成员检测能力，在流行的MIA基准测试中平均AUC分数达到0.85，表明内部模型行为能揭示训练数据暴露情况。

Conclusion: 即使输出信号看似受保护，内部模型行为仍可能暴露训练数据信息，强调需要进一步研究成员隐私问题，并开发更强大的隐私保护训练技术。

Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to
train machine learning models, serving as important tools for privacy auditing
and compliance assessment. Recent studies have reported that MIAs perform only
marginally better than random guessing against large language models,
suggesting that modern pre-training approaches with massive datasets may be
free from privacy leakage risks. Our work offers a complementary perspective to
these findings by exploring how examining LLMs' internal representations,
rather than just their outputs, may provide additional insights into potential
membership inference signals. Our framework, \emph{memTrace}, follows what we
call \enquote{neural breadcrumbs} extracting informative signals from
transformer hidden states and attention patterns as they process candidate
sequences. By analyzing layer-wise representation dynamics, attention
distribution characteristics, and cross-layer transition patterns, we detect
potential memorization fingerprints that traditional loss-based approaches may
not capture. This approach yields strong membership detection across several
model families achieving average AUC scores of 0.85 on popular MIA benchmarks.
Our findings suggest that internal model behaviors can reveal aspects of
training data exposure even when output-based signals appear protected,
highlighting the need for further research into membership privacy and the
development of more robust privacy-preserving training techniques for large
language models.

</details>


### [28] [Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms](https://arxiv.org/abs/2509.05930)
*Ali Zeynali,Mahsa Sahebdel,Qingsong Liu,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.LG

TL;DR: 提出了SOOTT框架，整合目标跟踪、对抗扰动和切换成本三个关键目标，并开发了BEST算法和基于学习的CoRT算法来处理在线决策中的不确定性


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中如AI集群工作负载调度等场景中，需要在动态跟踪目标、抵御不可预测扰动和避免决策突变之间取得平衡的问题

Method: 首先提出具有竞争性保证的BEST算法，然后引入CoRT学习增强变体，整合不可信的黑色预测到决策过程中

Result: 理论分析显示CoRT在预测准确时优于BEST，同时在任意预测错误下保持鲁棒性。案例研究验证了算法在平衡轨迹跟踪、决策平滑性和外部扰动弹性方面的有效性

Conclusion: SOOTT框架和相应算法为在线决策提供了有效的解决方案，特别是在需要平衡多个竞争目标的复杂现实场景中

Abstract: We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)
problem, a new framework that integrates three key objectives in online
decision-making under uncertainty: (1) tracking cost for following a
dynamically moving target, (2) adversarial perturbation cost for withstanding
unpredictable disturbances, and (3) switching cost for penalizing abrupt
changes in decisions. This formulation captures real-world scenarios such as
elastic and inelastic workload scheduling in AI clusters, where operators must
balance long-term service-level agreements (e.g., LLM training) against sudden
demand spikes (e.g., real-time inference). We first present BEST, a robust
algorithm with provable competitive guarantees for SOOTT. To enhance practical
performance, we introduce CoRT, a learning-augmented variant that incorporates
untrusted black-box predictions (e.g., from ML models) into its decision
process. Our theoretical analysis shows that CoRT strictly improves over BEST
when predictions are accurate, while maintaining robustness under arbitrary
prediction errors. We validate our approach through a case study on workload
scheduling, demonstrating that both algorithms effectively balance trajectory
tracking, decision smoothness, and resilience to external disturbances.

</details>


### [29] [Calibrated Recommendations with Contextual Bandits](https://arxiv.org/abs/2509.05460)
*Diego Feijer,Himan Abdollahpouri,Sanket Gupta,Alexander Clare,Yuxiao Wen,Todd Wasson,Maria Dimakopoulou,Zahra Nazari,Kyle Kretschman,Mounia Lalmas*

Main category: cs.LG

TL;DR: Spotify提出基于上下文多臂老虎机的校准方法，动态学习用户在不同情境下的内容类型偏好分布，解决历史数据偏向音乐的问题，提升播客等弱势内容的推荐效果。


<details>
  <summary>Details</summary>
Motivation: Spotify首页内容类型多样但历史数据严重偏向音乐，难以实现平衡的个性化内容推荐。用户对不同内容类型的偏好会随时间、日期和设备等情境因素变化。

Method: 使用上下文多臂老虎机方法，基于用户情境和偏好动态学习最优内容类型分布，替代依赖历史平均值的传统校准方法。

Result: 离线和在线实验结果均显示，该方法提高了推荐精度和用户参与度，特别是在播客等代表性不足的内容类型上表现显著改善。

Conclusion: 基于上下文多臂老虎机的校准方法能有效适应用户兴趣在不同情境下的变化，提升内容推荐的平衡性和个性化效果。

Abstract: Spotify's Home page features a variety of content types, including music,
podcasts, and audiobooks. However, historical data is heavily skewed toward
music, making it challenging to deliver a balanced and personalized content
mix. Moreover, users' preference towards different content types may vary
depending on the time of day, the day of week, or even the device they use. We
propose a calibration method that leverages contextual bandits to dynamically
learn each user's optimal content type distribution based on their context and
preferences. Unlike traditional calibration methods that rely on historical
averages, our approach boosts engagement by adapting to how users interests in
different content types varies across contexts. Both offline and online results
demonstrate improved precision and user engagement with the Spotify Home page,
in particular with under-represented content types such as podcasts.

</details>


### [30] [PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series](https://arxiv.org/abs/2509.05478)
*Jia Wang,Xiao Wang,Chi Zhang*

Main category: cs.LG

TL;DR: PLanTS是一个周期性感知的自监督学习框架，通过多粒度分块机制和对比损失来建模多元时间序列的潜在状态和转换，在多种下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列具有高维性、标签稀缺和非平稳性等挑战，现有自监督学习方法忽略了内在周期性结构，无法捕捉潜在状态的动态演化。

Method: 设计了周期性感知的多粒度分块机制和广义对比损失，保持实例级和状态级相似性；提出下一状态转换预测任务来编码未来状态演化的预测信息。

Result: 在多分类、多标签分类、预测、轨迹跟踪和异常检测等任务中，PLanTS持续提升表示质量，且在运行效率上优于基于DTW的方法。

Conclusion: PLanTS通过显式建模周期性结构和状态转换，有效解决了多元时间序列表示学习的挑战，为相关领域提供了有效的自监督学习解决方案。

Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare,
climate science, and industrial monitoring, but their high dimensionality,
limited labeled data, and non-stationary nature pose significant challenges for
conventional machine learning methods. While recent self-supervised learning
(SSL) approaches mitigate label scarcity by data augmentations or time
point-based contrastive strategy, they neglect the intrinsic periodic structure
of MTS and fail to capture the dynamic evolution of latent states. We propose
PLanTS, a periodicity-aware self-supervised learning framework that explicitly
models irregular latent states and their transitions. We first designed a
period-aware multi-granularity patching mechanism and a generalized contrastive
loss to preserve both instance-level and state-level similarities across
multiple temporal resolutions. To further capture temporal dynamics, we design
a next-transition prediction pretext task that encourages representations to
encode predictive information about future state evolution. We evaluate PLanTS
across a wide range of downstream tasks-including multi-class and multi-label
classification, forecasting, trajectory tracking and anomaly detection. PLanTS
consistently improves the representation quality over existing SSL methods and
demonstrates superior runtime efficiency compared to DTW-based methods.

</details>


### [31] [Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems](https://arxiv.org/abs/2509.06599)
*Sri Satish Krishna Chaitanya Bulusu,Mikko Sillanpää*

Main category: cs.LG

TL;DR: 提出了一个理论框架来分析动态非线性系统的静态和动态失真耦合问题，通过结构化分解、方差分析和任务感知复杂度边界来解释结构化残差学习的经验优势


<details>
  <summary>Details</summary>
Motivation: 动态非线性系统中的静态和动态效应相互耦合，给数据驱动建模带来重大挑战，需要建立理论基础来解释和指导实际建模

Method: 采用结构化分解方法，扩展内积空间正交性到结构不对称设置，引入行为指标和记忆有限性指数，建立基于功率的条件连接热力学第一定律

Result: 提出了行为不确定性原理，证明静态和动态失真不能同时最小化；建立了函数方差与均方Lipschitz连续性和学习复杂度的联系；开发了模型无关的任务感知复杂度度量

Conclusion: 该框架为复杂动态非线性系统建模提供了可扩展的理论基础方法，解释了结构化残差学习在泛化能力、参数数量和训练成本方面的经验优势

Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and
dynamic effects. Their intertwined nature poses major challenges for
data-driven modeling. This paper presents a theoretical framework grounded in
structured decomposition, variance analysis, and task-centric complexity
bounds.
  The framework employs a directional lower bound on interactions between
measurable system components, extending orthogonality in inner product spaces
to structurally asymmetric settings. This bound supports variance inequalities
for decomposed systems. Key behavioral indicators are introduced along with a
memory finiteness index. A rigorous power-based condition establishes a
measurable link between finite memory in realizable systems and the First Law
of Thermodynamics. This offers a more foundational perspective than classical
bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty
Principle,' demonstrating that static and dynamic distortions cannot be
minimized simultaneously. We identify that real-world systems seem to resist
complete deterministic decomposition due to entangled static and dynamic
effects. We also present two general-purpose theorems linking function variance
to mean-squared Lipschitz continuity and learning complexity. This yields a
model-agnostic, task-aware complexity metric, showing that lower-variance
components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual
learning, including improved generalization, reduced parameter count, and lower
training cost, as previously observed in power amplifier linearization
experiments. The framework is broadly applicable and offers a scalable,
theoretically grounded approach to modeling complex dynamic nonlinear systems.

</details>


### [32] [STL-based Optimization of Biomolecular Neural Networks for Regression and Control](https://arxiv.org/abs/2509.05481)
*Eric Palanques-Tost,Hanna Krasowski,Murat Arcak,Ron Weiss,Calin Belta*

Main category: cs.LG

TL;DR: 利用信号时序逻辑(STL)规范来训练生物分子神经网络(BNNs)，解决了BNNs因缺乏目标数据而难以训练的问题，实现了在生物系统中的回归和控制任务。


<details>
  <summary>Details</summary>
Motivation: 生物分子神经网络具有超越简单生物电路的通用函数逼近能力，但由于缺乏目标数据，训练BNNs仍然具有挑战性。

Method: 基于STL的定量语义，提出了梯度优化的BNNs权重学习算法，使BNNs能够在生物系统中执行回归和控制任务。

Result: 数值实验表明，基于STL的学习方法能够高效解决所研究的回归和控制任务，包括作为失调状态报告器和慢性疾病模型的反馈控制。

Conclusion: STL规范为BNNs训练提供了有效的目标定义方法，成功实现了生物系统中的复杂功能，为合成生物学应用开辟了新途径。

Abstract: Biomolecular Neural Networks (BNNs), artificial neural networks with
biologically synthesizable architectures, achieve universal function
approximation capabilities beyond simple biological circuits. However, training
BNNs remains challenging due to the lack of target data. To address this, we
propose leveraging Signal Temporal Logic (STL) specifications to define
training objectives for BNNs. We build on the quantitative semantics of STL,
enabling gradient-based optimization of the BNN weights, and introduce a
learning algorithm that enables BNNs to perform regression and control tasks in
biological systems. Specifically, we investigate two regression problems in
which we train BNNs to act as reporters of dysregulated states, and a feedback
control problem in which we train the BNN in closed-loop with a chronic disease
model, learning to reduce inflammation while avoiding adverse responses to
external infections. Our numerical experiments demonstrate that STL-based
learning can solve the investigated regression and control tasks efficiently.

</details>


### [33] [Prior Distribution and Model Confidence](https://arxiv.org/abs/2509.05485)
*Maksim Kazanskii,Artem Kasianov*

Main category: cs.LG

TL;DR: 通过分析训练数据在嵌入空间中的分布，提出了一种无需重新训练的方法来评估模型对未见数据的预测信心度，通过过滤低信心度预测显著提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据分布对图像分类模型性能的影响，并尝试在不需重新训练的情况下理解模型对未见数据的预测信心度。

Method: 分析训练集在嵌入空间中的表示，基于嵌入空间中与训练分布的距离过滤低信心度预测，使用多个嵌入模型来更健壮地估计信心度。

Result: 在多个分类模型上均实现了一致的性能提升，通过结合多种嵌入模型能够更好地检测和排除分布外样本，进一步提高了准确性。

Conclusion: 该方法不依赖于特定模型结构，具有良好的通用性，在计算机视觉和自然语言处理等预测可靠性关键的领域都有应用潜力。

Abstract: This paper investigates the impact of training data distribution on the
performance of image classification models. By analyzing the embeddings of the
training set, we propose a framework to understand the confidence of model
predictions on unseen data without the need for retraining. Our approach
filters out low-confidence predictions based on their distance from the
training distribution in the embedding space, significantly improving
classification accuracy. We demonstrate this on the example of several
classification models, showing consistent performance gains across
architectures. Furthermore, we show that using multiple embedding models to
represent the training data enables a more robust estimation of confidence, as
different embeddings capture complementary aspects of the data. Combining these
embeddings allows for better detection and exclusion of out-of-distribution
samples, resulting in further accuracy improvements. The proposed method is
model-agnostic and generalizable, with potential applications beyond computer
vision, including domains such as Natural Language Processing where prediction
reliability is critical.

</details>


### [34] [MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs](https://arxiv.org/abs/2509.05488)
*Hongjun Xu,Junxi Xia,Weisi Yang,Yueyuan Sui,Stephen Xia*

Main category: cs.LG

TL;DR: 首个在资源受限微控制器上部署Mamba模型的方案MambaLite-Micro，通过C语言实现、算子融合和内存优化，减少83%峰值内存，保持与PyTorch一致的准确率。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在微控制器部署面临内存限制、缺乏原生算子支持和嵌入式友好工具链等挑战，需要开发轻量级部署方案。

Method: 提出MambaLite-Micro：1) 将PyTorch模型权重导出为轻量格式；2) 用C语言手工实现Mamba层和支持算子，采用算子融合和内存布局优化技术。

Result: 减少83.0%峰值内存，数值误差仅1.7x10-5，在关键词检测和人类活动识别任务上保持100%准确率一致性，在ESP32S3和STM32H7上验证可移植性。

Conclusion: MambaLite-Micro成功将先进序列模型Mamba部署到资源受限嵌入式平台，为实际应用铺平道路。

Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to
limited memory, the lack of native operator support, and the absence of
embedded-friendly toolchains. We present, to our knowledge, the first
deployment of a Mamba-based neural architecture on a resource-constrained MCU,
a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline
maps a trained PyTorch Mamba model to on-device execution by (1) exporting
model weights into a lightweight format, and (2) implementing a handcrafted
Mamba layer and supporting operators in C with operator fusion and memory
layout optimization. MambaLite-Micro eliminates large intermediate tensors,
reducing 83.0% peak memory, while maintaining an average numerical error of
only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on
keyword spotting(KWS) and human activity recognition (HAR) tasks,
MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully
preserving classification accuracy. We further validated portability by
deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating
consistent operation across heterogeneous embedded platforms and paving the way
for bringing advanced sequence models like Mamba to real-world
resource-constrained applications.

</details>


### [35] [Self-Aligned Reward: Towards Effective and Efficient Reasoners](https://arxiv.org/abs/2509.05489)
*Peixuan Han,Adit Krishnan,Gerald Friedland,Jiaxuan You,Chris Kong*

Main category: cs.LG

TL;DR: 这篇论文提出了自对齐奖励(SAR)方法，通过相对困难度差值来补充二进制验证奖励，促进LLM的推理准确性和效率，在提高4%准确度的同时降低30%推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励信号仅提供二进制正确性反馈，导致推理过度繁瓜和计算成本高，而现有解决方案往往以牺性准确性为代价。

Method: 提出自对齐奖励(SAR)，定义为条件于查询的答案与独立答案的相对困难度差值，以促进简洁且查询特定的响应。

Result: 在4个模型和7个测试集上，SAR与PPO、GRPO等RL算法结合能提高准确度4%，降低推理成本30%，实现了正确性与效率的Pareto最优平衡。

Conclusion: SAR作为细粒度奖励信号，有效补充了可验证奖励，为更高效有效的LLM训练抓了基础。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced
reasoning in large language models (LLMs), but such signals remain coarse,
offering only binary correctness feedback. This limitation often results in
inefficiencies, including overly verbose reasoning and high computational cost,
while existing solutions often compromise accuracy. To address this, we
introduce self-aligned reward (SAR), a self-guided signal that complements
verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is
defined as the relative perplexity difference between an answer conditioned on
the query and the standalone answer, thereby favoring responses that are
concise and query-specific. Quantitative analysis reveals that SAR reliably
distinguishes answer quality: concise, correct answers score higher than
redundant ones, and partially correct answers score higher than entirely
incorrect ones. Evaluation on 4 models across 7 benchmarks shows that
integrating SAR with prevalent RL algorithms like PPO and GRPO improves
accuracy by 4%, while reducing inference cost by 30%. Further analysis
demonstrates that SAR achieves a Pareto-optimal trade-off between correctness
and efficiency compared to reward signals based on length or self-confidence.
We also show that SAR shortens responses while preserving advanced reasoning
behaviors, demonstrating its ability to suppress unnecessary elaboration
without losing critical reasoning. These results highlight the promise of
self-aligned reward as a fine-grained complement to verifiable rewards, paving
the way for more efficient and effective LLM training.

</details>


### [36] [DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training](https://arxiv.org/abs/2509.05542)
*Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-1.5是一个通过双层优化自适应调整训练样本权重的多模态过程奖励模型框架，在MMMU基准上达到84.6%准确率，超越GPT-5


<details>
  <summary>Details</summary>
Motivation: 多模态过程奖励模型训练面临分布偏移和噪声数据的挑战，需要有效处理训练样本重要性调整

Method: 提出基于双层优化的实例重加权框架，包含Instance Table（适用于小数据集）和Instance Net（适用于大数据集）两种互补策略，并集成到测试时缩放中

Result: 在MMMU基准测试中达到84.6%的准确率，性能超过GPT-5

Conclusion: DreamPRM-1.5框架通过自适应实例重加权有效解决了多模态过程奖励模型训练中的分布偏移和噪声问题，取得了显著的性能提升

Abstract: Training multimodal process reward models (PRMs) is challenged by
distribution shifts and noisy data. We introduce DreamPRM-1.5, an
instance-reweighted framework that adaptively adjusts the importance of each
training example via bi-level optimization. We design two complementary
strategies: Instance Table, effective for smaller datasets, and Instance Net,
scalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5
achieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.

</details>


### [37] [Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks](https://arxiv.org/abs/2509.05545)
*Yang Yu*

Main category: cs.LG

TL;DR: RLA（强化学习与预期）是一个新的分层强化学习框架，通过低层目标条件策略和高层预期模型的协同学习，解决长时域目标导向任务中的层次发现和策略训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 传统分层强化学习在自动发现任务层次结构和联合训练多级策略时存在不稳定性和缺乏理论保证的问题，特别是在长时域目标导向任务中。

Method: RLA学习两个协同模型：低层目标条件策略（学习到达指定子目标）和高层预期模型（作为规划器，提出到达最终目标的最优路径上的中间子目标）。预期模型通过价值几何一致性原则进行训练，并正则化以防止退化解。

Result: 论文提供了理论证明，表明RLA在各种条件下都能逼近全局最优策略，为长时域目标导向任务中的分层规划和执行建立了原则性和收敛性的方法。

Conclusion: RLA提供了一个有原则且可扩展的框架，通过预期模型和价值几何一致性原则，有效解决了分层强化学习中的稳定性和理论保证问题，为长时域任务提供了收敛的解决方案。

Abstract: Solving long-horizon goal-conditioned tasks remains a significant challenge
in reinforcement learning (RL). Hierarchical reinforcement learning (HRL)
addresses this by decomposing tasks into more manageable sub-tasks, but the
automatic discovery of the hierarchy and the joint training of multi-level
policies often suffer from instability and can lack theoretical guarantees. In
this paper, we introduce Reinforcement Learning with Anticipation (RLA), a
principled and potentially scalable framework designed to address these
limitations. The RLA agent learns two synergistic models: a low-level,
goal-conditioned policy that learns to reach specified subgoals, and a
high-level anticipation model that functions as a planner, proposing
intermediate subgoals on the optimal path to a final goal. The key feature of
RLA is the training of the anticipation model, which is guided by a principle
of value geometric consistency, regularized to prevent degenerate solutions. We
present proofs that RLA approaches the globally optimal policy under various
conditions, establishing a principled and convergent method for hierarchical
planning and execution in long-horizon goal-conditioned tasks.

</details>


### [38] [ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization](https://arxiv.org/abs/2509.05584)
*Sadegh Jafari,Aishwarya Sarkar,Mohiuddin Bilwal,Ali Jannesari*

Main category: cs.LG

TL;DR: ProfilingAgent是一个基于LLM的多智能体系统，通过分析静态指标和动态性能信号来自动化模型压缩（结构剪枝和动态量化），在保持精度的同时显著减少内存使用和提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 基础模型面临计算和内存瓶颈，传统压缩技术使用统一启发式方法，忽略了架构和运行时异质性，需要更智能的自动化压缩方案。

Method: 使用大型语言模型构建模块化多智能体系统，分析MACs、参数量等静态指标和延迟、内存等动态信号，设计特定架构的剪枝和量化策略。

Result: 在多个数据集和模型上，剪枝保持或提升精度（ImageNet-1K精度下降约1%，ViT-B/16在小数据集上提升2%），量化实现74%内存节省和<0.5%精度损失，推理速度提升1.74倍。

Conclusion: 智能体系统是可扩展的剖析引导模型优化解决方案，LLM推理质量对迭代剪枝至关重要。

Abstract: Foundation models face growing compute and memory bottlenecks, hindering
deployment on resource-limited platforms. While compression techniques such as
pruning and quantization are widely used, most rely on uniform heuristics that
ignore architectural and runtime heterogeneity. Profiling tools expose
per-layer latency, memory, and compute cost, yet are rarely integrated into
automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic
approach that uses large language models (LLMs) to automate compression via
structured pruning and post-training dynamic quantization. Our modular
multi-agent system reasons over static metrics (MACs, parameter counts) and
dynamic signals (latency, memory) to design architecture-specific strategies.
Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to
bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with
ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive
or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on
smaller datasets), while quantization achieves up to 74% memory savings with
<0.5% accuracy loss. Our quantization also yields consistent inference speedups
of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo
highlight the importance of LLM reasoning quality for iterative pruning. These
results establish agentic systems as scalable solutions for profiling-guided
model optimization.

</details>


### [39] [Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2509.05615)
*Xiaoguang Zhu,Lianlong Sun,Yang Liu,Pengyi Jiang,Uma Srivatsa,Nipavan Chiamvimonvat,Vladimir Filkov*

Main category: cs.LG

TL;DR: 提出一个因果启发的多模态医学表示学习框架，通过反事实干预和双分支网络来解决缺失模态偏差和分布偏差问题


<details>
  <summary>Details</summary>
Motivation: 真实医疗数据集中常存在模态缺失问题，现有方法忽略了数据采集过程引入的潜在偏差，包括缺失性偏差和分布偏差

Method: 基于结构因果分析的数据生成过程建模，包含缺失性去混淆模块（基于后门调整的因果干预近似）和双分支神经网络（显式分离因果特征和伪相关）

Result: 在真实世界公共数据集和院内数据集上验证了方法的有效性和因果洞察力

Conclusion: 提出的统一框架能够有效处理医疗多模态数据中的偏差问题，提升模型泛化能力

Abstract: Medical multimodal representation learning aims to integrate heterogeneous
clinical data into unified patient representations to support predictive
modeling, which remains an essential yet challenging task in the medical data
mining community. However, real-world medical datasets often suffer from
missing modalities due to cost, protocol, or patient-specific constraints.
Existing methods primarily address this issue by learning from the available
observations in either the raw data space or feature space, but typically
neglect the underlying bias introduced by the data acquisition process itself.
In this work, we identify two types of biases that hinder model generalization:
missingness bias, which results from non-random patterns in modality
availability, and distribution bias, which arises from latent confounders that
influence both observed features and outcomes. To address these challenges, we
perform a structural causal analysis of the data-generating process and propose
a unified framework that is compatible with existing direct prediction-based
multimodal learning methods. Our method consists of two key components: (1) a
missingness deconfounding module that approximates causal intervention based on
backdoor adjustment and (2) a dual-branch neural network that explicitly
disentangles causal features from spurious correlations. We evaluated our
method in real-world public and in-hospital datasets, demonstrating its
effectiveness and causal insights.

</details>


### [40] [OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search](https://arxiv.org/abs/2509.05656)
*Bo Lyu,Yu Cui,Tuo Shi,Ke Li*

Main category: cs.LG

TL;DR: OptiProxy-NAS是一个端到端的神经架构搜索优化框架，通过代理表示将离散的NAS空间转化为连续、可微分且平滑的空间，从而可以使用梯度优化方法进行高效搜索。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索(NAS)是一个计算昂贵的离散优化问题，搜索空间巨大且不平滑。现有方法如基于预测器的方法和可微分架构搜索存在局限性，需要更高效的优化框架。

Method: 提出OptiProxy-NAS框架，使用代理表示将离散的NAS空间重新表述为连续、可微分且平滑的空间，从而可以直接应用梯度优化方法进行架构参数搜索。

Result: 在12个NAS任务、4个搜索空间和三个不同领域（计算机视觉、自然语言处理、资源受限NAS）的全面实验中表现出优越的搜索结果和效率，在低保真度场景下也验证了灵活性。

Conclusion: OptiProxy-NAS提供了一个有效的端到端优化框架，通过空间转换使NAS问题变得可微分，实现了高效且灵活的神经架构搜索。

Abstract: Neural architecture search (NAS) is a hard computationally expensive
optimization problem with a discrete, vast, and spiky search space. One of the
key research efforts dedicated to this space focuses on accelerating NAS via
certain proxy evaluations of neural architectures. Different from the prevalent
predictor-based methods using surrogate models and differentiable architecture
search via supernetworks, we propose an optimization proxy to streamline the
NAS as an end-to-end optimization framework, named OptiProxy-NAS. In
particular, using a proxy representation, the NAS space is reformulated to be
continuous, differentiable, and smooth. Thereby, any differentiable
optimization method can be applied to the gradient-based search of the relaxed
architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$
search spaces across three different domains including computer vision, natural
language processing, and resource-constrained NAS fully demonstrate the
superior search results and efficiency. Further experiments on low-fidelity
scenarios verify the flexibility.

</details>


### [41] [DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches](https://arxiv.org/abs/2509.05663)
*Lucas Correia,Jan-Christoph Goos,Thomas Bäck,Anna V. Kononova*

Main category: cs.LG

TL;DR: 基于动态时间扩张的差异性查询策略(DQS)通过主动学习精细化无监督异常检测阈值，在小标签预算下显著提升检测性能


<details>
  <summary>Details</summary>
Motivation: 解决现有无监督时间序列异常检测方法因阈值设置不佳而影响检测性能的问题，以及实际应用中缺乏标签数据的挑战

Method: 结合主动学习与无监督异常检测方法，提出基于动态时间扩张的差异性查询策略(DQS)，通过选择性查询多元时间序列标签来精细化阈值选择

Result: DQS在小预算场景中表现最佳，其他策略在对投标错误时更稳健，所有主动学习策略都显著超过了无监督阈值方法

Conclusion: 在可以查询专家的情况下，建议采用基于主动学习的阈值设置方法，查询策略的选择取决于专家知识水平和可提供的标签数量

Abstract: Truly unsupervised approaches for time series anomaly detection are rare in
the literature. Those that exist suffer from a poorly set threshold, which
hampers detection performance, while others, despite claiming to be
unsupervised, need to be calibrated using a labelled data subset, which is
often not available in the real world. This work integrates active learning
with an existing unsupervised anomaly detection method by selectively querying
the labels of multivariate time series, which are then used to refine the
threshold selection process. To achieve this, we introduce a novel query
strategy called the dissimilarity-based query strategy (DQS). DQS aims to
maximise the diversity of queried samples by evaluating the similarity between
anomaly scores using dynamic time warping. We assess the detection performance
of DQS in comparison to other query strategies and explore the impact of
mislabelling, a topic that is underexplored in the literature. Our findings
indicate that DQS performs best in small-budget scenarios, though the others
appear to be more robust when faced with mislabelling. Therefore, in the real
world, the choice of query strategy depends on the expertise of the oracle and
the number of samples they are willing to label. Regardless, all query
strategies outperform the unsupervised threshold even in the presence of
mislabelling. Thus, whenever it is feasible to query an oracle, employing an
active learning-based threshold is recommended.

</details>


### [42] [GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR](https://arxiv.org/abs/2509.05671)
*Labani Halder,Tanmay Sen,Sarbani Palit*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于图神经网络的多模态联邦学习框架GraMFedDHAR，用于解决多模态传感器数据在人体活动识别中的噪声、标签数据稀缩和隐私问题，并在差分隐私条件下显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统的集中式深度学习方法遇到基础设施限制、网络延迟和数据共享限制。虽然联邦学习通过本地训练模型和仅共享参数来保护隐私，但仍面临异构多模态数据和差分隐私要求带来的挑战。

Method: 提出GraMFedDHAR框架，将压力地毯、深度摄像头和多个加速度计等传感器数据建模为模态特异性图，通过残差图卷积神经网络处理，并使用关注机制进行融合而非简单拼接。融合的嵌入进行活动分类，同时通过差分隐私保护联邦聚合过程中的数据。

Result: 实验结果显示，提出的MultiModalGCN模型在非DP设置下比基线MultiModalFFN准确率高出2%，在集中和联邦范式中都有更好表现。在差分隐私约束下，MultiModalGCN持续超越MultiModalFFN，性能差距从7%到13%不等，具体取决于隐私预算和设置。

Conclusion: 这些结果强调了基于图的建模在多模态学习中的稳健性，其中GNN证明对DP噪声引起的性能退化具有更强的弹性。

Abstract: Human Activity Recognition (HAR) using multimodal sensor data remains
challenging due to noisy or incomplete measurements, scarcity of labeled
examples, and privacy concerns. Traditional centralized deep learning
approaches are often constrained by infrastructure availability, network
latency, and data sharing restrictions. While federated learning (FL) addresses
privacy by training models locally and sharing only model parameters, it still
has to tackle issues arising from the use of heterogeneous multimodal data and
differential privacy requirements. In this article, a Graph-based Multimodal
Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse
sensor streams such as a pressure mat, depth camera, and multiple
accelerometers are modeled as modality-specific graphs, processed through
residual Graph Convolutional Neural Networks (GCNs), and fused via
attention-based weighting rather than simple concatenation. The fused
embeddings enable robust activity classification, while differential privacy
safeguards data during federated aggregation. Experimental results show that
the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with
up to 2 percent higher accuracy in non-DP settings in both centralized and
federated paradigms. More importantly, significant improvements are observed
under differential privacy constraints: MultiModalGCN consistently surpasses
MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on
the privacy budget and setting. These results highlight the robustness of
graph-based modeling in multimodal learning, where GNNs prove more resilient to
the performance degradation introduced by DP noise.

</details>


### [43] [Nonnegative matrix factorization and the principle of the common cause](https://arxiv.org/abs/2509.03652)
*E. Khalafyan,A. E. Allahverdyan,A. Hovhannisyan*

Main category: cs.LG

TL;DR: 本文探讨了非负矩阵分解(NMF)与共同原因原理(PCC)之间的紧密联系，展示了它们在图像数据处理中的相互应用价值。


<details>
  <summary>Details</summary>
Motivation: 研究NMF和PCC这两个看似不同领域的概念之间的内在联系，探索它们在数据降维、因果分析和图像处理中的协同应用。

Method: 通过将灰度图像数据映射到概率模型，利用PCC提供预测性工具来稳健估计NMF的有效秩，并开发基于NMF的聚类方法和数据去噪技术。

Result: 提出的秩估计方法对弱噪声具有稳定性，解决了NMF的非可识别性问题；同时NMF为PCC提供了近似实现方式，能够更好地解释正相关的联合概率。

Conclusion: NMF和PCC之间存在深刻的数学联系，这种双向关系为数据降维、因果分析和图像处理提供了新的理论框架和实用工具。

Abstract: Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction
method. The principle of the common cause (PCC) is a basic methodological
approach in probabilistic causality, which seeks an independent mixture model
for the joint probability of two dependent random variables. It turns out that
these two concepts are closely related. This relationship is explored
reciprocally for several datasets of gray-scale images, which are conveniently
mapped into probability models. On one hand, PCC provides a predictability tool
that leads to a robust estimation of the effective rank of NMF. Unlike other
estimates (e.g., those based on the Bayesian Information Criteria), our
estimate of the rank is stable against weak noise. We show that NMF implemented
around this rank produces features (basis images) that are also stable against
noise and against seeds of local optimization, thereby effectively resolving
the NMF nonidentifiability problem. On the other hand, NMF provides an
interesting possibility of implementing PCC in an approximate way, where larger
and positively correlated joint probabilities tend to be explained better via
the independent mixture model. We work out a clustering method, where data
points with the same common cause are grouped into the same cluster. We also
show how NMF can be employed for data denoising.

</details>


### [44] [Distributed Deep Learning using Stochastic Gradient Staleness](https://arxiv.org/abs/2509.05679)
*Viet Hoang Pham,Hyo-Sung Ahn*

Main category: cs.LG

TL;DR: 本文提出一种集成数据并行和全解耦并行反向传播算法的分布式训练方法，以加速深度神经网络的训练过程。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练需要很长时间，特别是当网络深度增加和需要大量训练数据时。为解决这些挑战，需要快速训练方法。

Method: 结合数据并行和全解耦并行反向传播算法，利用多个计算单元并行运行，增加每次迭代处理的训练数据量，并减少反向传播算法中的锁定问题。

Result: 该方法在某些条件下被严格证明能够收敛到关键点。在CIFAR-10数据集上的分类任务实验中，证明了其有效性。

Conclusion: 提出的分布式训练方法通过并行化计算和减少锁定问题，显著提高了训练效率，为深度神经网络的快速训练提供了有效解决方案。

Abstract: Despite the notable success of deep neural networks (DNNs) in solving complex
tasks, the training process still remains considerable challenges. A primary
obstacle is the substantial time required for training, particularly as high
performing DNNs tend to become increasingly deep (characterized by a larger
number of hidden layers) and require extensive training datasets. To address
these challenges, this paper introduces a distributed training method that
integrates two prominent strategies for accelerating deep learning: data
parallelism and fully decoupled parallel backpropagation algorithm. By
utilizing multiple computational units operating in parallel, the proposed
approach enhances the amount of training data processed in each iteration while
mitigating locking issues commonly associated with the backpropagation
algorithm. These features collectively contribute to significant improvements
in training efficiency. The proposed distributed training method is rigorously
proven to converge to critical points under certain conditions. Its
effectiveness is further demonstrated through empirical evaluations, wherein an
DNN is trained to perform classification tasks on the CIFAR-10 dataset.

</details>


### [45] [Morphological Perceptron with Competitive Layer: Training Using Convex-Concave Procedure](https://arxiv.org/abs/2509.05697)
*Iara Cunha,Marcos Eduardo Valle*

Main category: cs.LG

TL;DR: 提出使用凸凹过程(CCP)训练具有竞争层的形态感知器(MPCL)，通过将训练问题表述为凸差(DC)函数并迭代求解线性规划子问题，解决了形态算子不可微导致的梯度优化不适用问题。


<details>
  <summary>Details</summary>
Motivation: 形态感知器中的形态算子不可微，使得基于梯度的优化方法不适用于训练此类网络，需要寻找不依赖梯度信息的替代策略。

Method: 将MPCL网络的训练问题表述为凸差(DC)函数，然后使用凸凹过程(CCP)进行迭代求解，产生一系列线性规划子问题。

Result: 计算实验证明了所提出的训练方法在处理MPCL网络分类任务方面的有效性。

Conclusion: CCP方法为训练不可微的形态感知器网络提供了一种有效的替代方案，能够成功解决多类分类任务。

Abstract: A morphological perceptron is a multilayer feedforward neural network in
which neurons perform elementary operations from mathematical morphology. For
multiclass classification tasks, a morphological perceptron with a competitive
layer (MPCL) is obtained by integrating a winner-take-all output layer into the
standard morphological architecture. The non-differentiability of morphological
operators renders gradient-based optimization methods unsuitable for training
such networks. Consequently, alternative strategies that do not depend on
gradient information are commonly adopted. This paper proposes the use of the
convex-concave procedure (CCP) for training MPCL networks. The training problem
is formulated as a difference of convex (DC) functions and solved iteratively
using CCP, resulting in a sequence of linear programming subproblems.
Computational experiments demonstrate the effectiveness of the proposed
training method in addressing classification tasks with MPCL networks.

</details>


### [46] [Simulation Priors for Data-Efficient Deep Learning](https://arxiv.org/abs/2509.05732)
*Lenart Treven,Bhavya Sukhija,Jonas Rothfuss,Stelian Coros,Florian Dörfler,Andreas Krause*

Main category: cs.LG

TL;DR: SimPEL是一种将第一性原理模型与数据驱动学习相结合的方法，使用低保真模拟器作为贝叶斯深度学习的先验，在低数据条件下表现优异并能有效量化认知不确定性


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在现实世界中高效学习的问题。第一性原理模型因简化假设而无法捕捉真实复杂性，而深度学习方法需要大量代表性数据

Method: 使用低保真模拟器作为贝叶斯深度学习的先验，结合第一性原理模型和数据驱动学习，根据数据量动态调整学习方法

Result: 在生物、农业和机器人等多个领域表现出优越性能，在高速RC汽车漂移停车任务中，使用比最先进基线少得多的数据学习高度动态操作

Conclusion: SimPEL在复杂现实环境中具有数据高效学习和控制的潜力，能够有效弥合模拟到现实的差距

Abstract: How do we enable AI systems to efficiently learn in the real-world?
First-principles models are widely used to simulate natural systems, but often
fail to capture real-world complexity due to simplifying assumptions. In
contrast, deep learning approaches can estimate complex dynamics with minimal
assumptions but require large, representative datasets. We propose SimPEL, a
method that efficiently combines first-principles models with data-driven
learning by using low-fidelity simulators as priors in Bayesian deep learning.
This enables SimPEL to benefit from simulator knowledge in low-data regimes and
leverage deep learning's flexibility when more data is available, all the while
carefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse
systems, including biological, agricultural, and robotic domains, showing
superior performance in learning complex dynamics. For decision-making, we
demonstrate that SimPEL bridges the sim-to-real gap in model-based
reinforcement learning. On a high-speed RC car task, SimPEL learns a highly
dynamic parking maneuver involving drifting with substantially less data than
state-of-the-art baselines. These results highlight the potential of SimPEL for
data-efficient learning and control in complex real-world environments.

</details>


### [47] [Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders](https://arxiv.org/abs/2509.05766)
*Jiaju Miao,Wei Zhu*

Main category: cs.LG

TL;DR: 基于精度-召回曲线随机森果森（PRC-RF）与自编码器的混合框架，解决异常检测中的类不平衡和维度灾难问题


<details>
  <summary>Details</summary>
Motivation: 异常检测在网络安全、入侵检测等关键应用中致命关节，但类不平衡和高维度是主要挑战

Method: 综合PRC随机森果森（PRC-RF）与自编码器，利用自编码器学习紧凑的潜在表征来应对维度灾难

Result: 在多个标准数据集上进行涉及实验，证明Autoencoder-PRC-RF模型在准确性、可扩展性和可解释性方面都超过之前方法

Conclusion: 该混合框架在高风险异常检测任务中展现出强大潜力

Abstract: Anomaly detection underpins critical applications from network security and
intrusion detection to fraud prevention, where recognizing aberrant patterns
rapidly is indispensable. Progress in this area is routinely impeded by two
obstacles: extreme class imbalance and the curse of dimensionality. To combat
the former, we previously introduced Precision-Recall Curve (PRC)
classification trees and their ensemble extension, the PRC Random Forest
(PRC-RF). Building on that foundation, we now propose a hybrid framework that
integrates PRC-RF with autoencoders, unsupervised machine learning methods that
learn compact latent representations, to confront both challenges
simultaneously. Extensive experiments across diverse benchmark datasets
demonstrate that the resulting Autoencoder-PRC-RF model achieves superior
accuracy, scalability, and interpretability relative to prior methods,
affirming its potential for high-stakes anomaly-detection tasks.

</details>


### [48] [Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies](https://arxiv.org/abs/2509.05735)
*Jiaqi Chen,Ji Shi,Cansu Sancaktar,Jonas Frey,Georg Martius*

Main category: cs.LG

TL;DR: 模型基于强化学习中，在线训练比离线训练表现更好，主要因为离线数据集导致分布外状态问题，通过添加在线交互或探索数据可以缓解这个问题。


<details>
  <summary>Details</summary>
Motivation: 研究在线与离线数据收集对模型基于强化学习中世界模型学习的影响，尤其是识别离线训练性能下降的根本原因。

Method: 在31个不同环境中进行实验，比较在线和离线训练的性能差异，分析分布外状态问题，并探索通过添加在线交互或探索数据来缓解性能下降的方法。

Result: 在线训练比离线训练表现更好，离线训练存在分布外状态问题，通过添加在线交互或探索数据可以恢复在线训练的性能水平。

Conclusion: 建议在收集大规模数据集时添加探索数据，而不仅仅依赖专家数据，以提高模型在实际应用中的性能。

Abstract: Data collection is crucial for learning robust world models in model-based
reinforcement learning. The most prevalent strategies are to actively collect
trajectories by interacting with the environment during online training or
training on offline datasets. At first glance, the nature of learning
task-agnostic environment dynamics makes world models a good candidate for
effective offline training. However, the effects of online vs. offline data on
world models and thus on the resulting task performance have not been
thoroughly studied in the literature. In this work, we investigate both
paradigms in model-based settings, conducting experiments on 31 different
environments. First, we showcase that online agents outperform their offline
counterparts. We identify a key challenge behind performance degradation of
offline agents: encountering Out-Of-Distribution states at test time. This
issue arises because, without the self-correction mechanism in online agents,
offline datasets with limited state space coverage induce a mismatch between
the agent's imagination and real rollouts, compromising policy training. We
demonstrate that this issue can be mitigated by allowing for additional online
interactions in a fixed or adaptive schedule, restoring the performance of
online training with limited interaction data. We also showcase that
incorporating exploration data helps mitigate the performance degradation of
offline agents. Based on our insights, we recommend adding exploration data
when collecting large datasets, as current efforts predominantly focus on
expert data alone.

</details>


### [49] [DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection](https://arxiv.org/abs/2509.05778)
*Arantxa Urrea-Castaño,Nicolás Segura-Kunsagi,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.LG

TL;DR: 本文提出了一种双重交叉验证框架DCV-ROOD，用于更稳健地评估分布外检测模型的性能，通过分别处理分布内和分布外数据的特性，并考虑类别层次结构来获得公平的数据分割。


<details>
  <summary>Details</summary>
Motivation: 分布外(OOD)检测对于提高AI系统的稳健性至关重要，但现有评估方法并不能有效地处理ID和OOD数据的不同特性。需要一种更稳健的评估框架来确保OOD检测方法的可靠性。

Method: 提出双重交叉验证框架，对ID数据采用标准分割方式，对OOD数据则按类别分组进行分割。考虑类别层次结构，提出了能够获得公平ID-OOD分割的方法。

Result: 对比多个独立的状态最佳OOD检测方法进行测试，结果显示该方法能够很快收敛到真实性能值。

Conclusion: DCV-ROOD框架提供了一种可靠的方法来评估OOD检测模型的性能，通过分别处理不同类型数据的特性和考虑类别层次，实现了更稳健的性能估计。

Abstract: Out-of-distribution (OOD) detection plays a key role in enhancing the
robustness of artificial intelligence systems by identifying inputs that differ
significantly from the training distribution, thereby preventing unreliable
predictions and enabling appropriate fallback mechanisms. Developing reliable
OOD detection methods is a significant challenge, and rigorous evaluation of
these techniques is essential for ensuring their effectiveness, as it allows
researchers to assess their performance under diverse conditions and to
identify potential limitations or failure modes. Cross-validation (CV) has
proven to be a highly effective tool for providing a reasonable estimate of the
performance of a learning algorithm. Although OOD scenarios exhibit particular
characteristics, an appropriate adaptation of CV can lead to a suitable
evaluation framework for this setting. This work proposes a dual CV framework
for robust evaluation of OOD detection models, aimed at improving the
reliability of their assessment. The proposed evaluation framework aims to
effectively integrate in-distribution (ID) and OOD data while accounting for
their differing characteristics. To achieve this, ID data are partitioned using
a conventional approach, whereas OOD data are divided by grouping samples based
on their classes. Furthermore, we analyze the context of data with class
hierarchy to propose a data splitting that considers the entire class hierarchy
to obtain fair ID-OOD partitions to apply the proposed evaluation framework.
This framework is called Dual Cross-Validation for Robust Out-of-Distribution
Detection (DCV-ROOD). To test the validity of the evaluation framework, we
selected a set of state-of-the-art OOD detection methods, both with and without
outlier exposure. The results show that the method achieves very fast
convergence to the true performance.

</details>


### [50] [The Measure of Deception: An Analysis of Data Forging in Machine Unlearning](https://arxiv.org/abs/2509.05865)
*Rishabh Dixit,Yuan Hui,Rayan Saab*

Main category: cs.LG

TL;DR: 该论文研究了机器学习遗忘中的对抗性伪造问题，分析了伪造集合的度量性质，证明了在合理假设下伪造概率极小，为检测虚假遗忘声明提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 受隐私法规和消除有害数据影响的需求驱动，机器学习遗忘旨在修改训练模型以有效"忘记"指定数据。验证遗忘的关键挑战是对抗性伪造——恶意制作模仿目标点梯度的数据，造成已遗忘的假象而实际上未移除信息。

Method: 提出了ε-伪造集合的分析框架，针对线性回归和单层神经网络，计算了该集合的Lebesgue度量。在温和正则性假设下，证明了伪造集合度量随ε^{(d-r)/2}衰减，其中d为数据维度，r为模型梯度变化矩阵的零度。扩展到批量SGD和几乎处处光滑损失函数也得到了相同的渐近标度。

Result: 研究表明伪造集合的Lebesgue度量很小，按ε的阶数缩放，当ε足够小时为ε^d。更一般地，伪造集合度量以ε^{(d-r)/2}衰减。在非退化数据分布下，随机采样伪造点的概率极小。

Conclusion: 对抗性伪造从根本上受到限制，虚假的遗忘声明原则上可以被检测到，这为机器学习遗忘的可验证性提供了理论支持。

Abstract: Motivated by privacy regulations and the need to mitigate the effects of
harmful data, machine unlearning seeks to modify trained models so that they
effectively ``forget'' designated data. A key challenge in verifying unlearning
is forging -- adversarially crafting data that mimics the gradient of a target
point, thereby creating the appearance of unlearning without actually removing
information. To capture this phenomenon, we consider the collection of data
points whose gradients approximate a target gradient within tolerance
$\epsilon$ -- which we call an $\epsilon$-forging set -- and develop a
framework for its analysis. For linear regression and one-layer neural
networks, we show that the Lebesgue measure of this set is small. It scales on
the order of $\epsilon$, and when $\epsilon$ is small enough, $\epsilon^d$.
More generally, under mild regularity assumptions, we prove that the forging
set measure decays as $\epsilon^{(d-r)/2}$, where $d$ is the data dimension and
$r<d$ is the nullity of a variation matrix defined by the model gradients.
Extensions to batch SGD and almost-everywhere smooth loss functions yield the
same asymptotic scaling. In addition, we establish probability bounds showing
that, under non-degenerate data distributions, the likelihood of randomly
sampling a forging point is vanishingly small. These results provide evidence
that adversarial forging is fundamentally limited and that false unlearning
claims can, in principle, be detected.

</details>


### [51] [Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting](https://arxiv.org/abs/2509.05768)
*Chen Shao,Yue Wang,Zhenyi Zhu,Zhanbo Huang,Sebastian Pütz,Benjamin Schäfer,Tobais Käfer,Michael Färber*

Main category: cs.LG

TL;DR: 提出了Real-E数据集，涵盖30多个欧洲国家74个发电站10年数据，包含丰富元数据。通过该数据集对20多个基线模型进行基准测试，发现现有方法在处理复杂非平稳相关动态时表现不佳，并提出了新的相关性结构偏移度量指标。


<details>
  <summary>Details</summary>
Motivation: 现有能源预测基准在时空范围和多能源特征方面存在局限，影响其在真实世界部署的可靠性和适用性。

Method: 构建Real-E大规模数据集，进行广泛数据分析，基准测试20多个不同模型类型的基线方法，引入新的相关性结构偏移度量指标。

Result: 现有方法在Real-E数据集上表现不佳，该数据集展现出更复杂和非平稳的相关性动态。

Conclusion: 研究结果揭示了当前方法的关键局限性，为构建更鲁棒的预测模型提供了坚实的实证基础。

Abstract: Energy forecasting is vital for grid reliability and operational efficiency.
Although recent advances in time series forecasting have led to progress,
existing benchmarks remain limited in spatial and temporal scope and lack
multi-energy features. This raises concerns about their reliability and
applicability in real-world deployment. To address this, we present the Real-E
dataset, covering over 74 power stations across 30+ European countries over a
10-year span with rich metadata. Using Real- E, we conduct an extensive data
analysis and benchmark over 20 baselines across various model types. We
introduce a new metric to quantify shifts in correlation structures and show
that existing methods struggle on our dataset, which exhibits more complex and
non-stationary correlation dynamics. Our findings highlight key limitations of
current methods and offer a strong empirical basis for building more robust
forecasting models

</details>


### [52] [If generative AI is the answer, what is the question?](https://arxiv.org/abs/2509.06120)
*Ambuj Tewari*

Main category: cs.LG

TL;DR: 本文探讨生成式AI的理论基础，将其作为与预测、压缩和决策相关的独立机器学习任务，综述了五大生成模型家族，提出了概率和博弈论框架，并讨论了部署准备和社会责任问题。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI作为独立机器学习任务的理论基础，明确生成与密度估计的区别，建立系统的理论框架来理解生成问题的本质。

Method: 采用任务优先的框架，综述五大生成模型家族（自回归、变分自编码器、标准化流、生成对抗网络、扩散模型），提出概率框架和博弈论的两玩家对抗学习设置。

Result: 建立了生成式AI的系统理论框架，明确了生成与密度估计的区别，提出了用于研究生成的博弈论方法，并讨论了模型部署和社会责任问题。

Conclusion: 生成式AI需要从任务角度进行系统理解，而不仅仅是模型实现；需要关注部署准备和社会责任问题，如隐私保护、AI内容检测和版权问题。

Abstract: Beginning with text and images, generative AI has expanded to audio, video,
computer code, and molecules. Yet, if generative AI is the answer, what is the
question? We explore the foundations of generation as a distinct machine
learning task with connections to prediction, compression, and decision-making.
We survey five major generative model families: autoregressive models,
variational autoencoders, normalizing flows, generative adversarial networks,
and diffusion models. We then introduce a probabilistic framework that
emphasizes the distinction between density estimation and generation. We review
a game-theoretic framework with a two-player adversary-learner setup to study
generation. We discuss post-training modifications that prepare generative
models for deployment. We end by highlighting some important topics in socially
responsible generation such as privacy, detection of AI-generated content, and
copyright and IP. We adopt a task-first framing of generation, focusing on what
generation is as a machine learning problem, rather than only on how models
implement it.

</details>


### [53] [Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.05779)
*Wei Chen,Yuqian Wu,Yuanshao Zhu,Xixuan Hao,Shiyu Wang,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出ExoST框架，通过'选择再平衡'范式解决外生变量在时空预测中的不一致性和不平衡性挑战，提升预测精度


<details>
  <summary>Details</summary>
Motivation: 现有时空预测方法仅使用有限的目标变量，而现实场景中外生变量可作为额外输入特征提升预测精度，但面临不同外生变量对目标系统影响不一致以及历史变量与未来变量影响不平衡的挑战

Method: 构建潜在空间门控专家模块，将融合的外生信息投影到潜在空间动态选择和重组显著信号；设计孪生网络架构，将重组后的过去和未来外生变量表示输入双分支时空骨干网络捕获动态模式；通过上下文感知加权机制实现建模过程中的动态平衡

Result: 在真实世界数据集上的大量实验证明了所提框架的有效性、通用性、鲁棒性和效率

Conclusion: ExoST框架成功解决了外生变量建模中的关键挑战，为时空预测提供了有效的解决方案

Abstract: Spatio-temporal forecasting aims to predict the future state of dynamic
systems and plays an important role in multiple fields. However, existing
solutions only focus on modeling using a limited number of observed target
variables. In real-world scenarios, exogenous variables can be integrated into
the model as additional input features and associated with the target signal to
promote forecast accuracy. Although promising, this still encounters two
challenges: the inconsistent effects of different exogenous variables to the
target system, and the imbalance effects between historical variables and
future variables. To address these challenges, this paper introduces \model, a
novel framework for modeling \underline{exo}genous variables in
\underline{s}patio-\underline{t}emporal forecasting, which follows a ``select,
then balance'' paradigm. Specifically, we first construct a latent space gated
expert module, where fused exogenous information is projected into a latent
space to dynamically select and recompose salient signals via specialized
sub-experts. Furthermore, we design a siamese network architecture in which
recomposed representations of past and future exogenous variables are fed into
dual-branch spatio-temporal backbones to capture dynamic patterns. The outputs
are integrated through a context-aware weighting mechanism to achieve dynamic
balance during the modeling process. Extensive experiments on real-world
datasets demonstrate the effectiveness, generality, robustness, and efficiency
of our proposed framework.

</details>


### [54] [Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators](https://arxiv.org/abs/2509.06154)
*Dibyajyoti Nayak,Somdatta Goswami*

Main category: cs.LG

TL;DR: 图神经网模拟器(GNS)结合显式时间步进方案，通过学习瞬时时间导数来构建准确的PDE正向模型，在数据稀缺情况下显著提高了数据效率和演化准确性


<details>
  <summary>Details</summary>
Motivation: 传统神经算子需要大量数据集且在稀缺训练数据时表现差，许多方案没有显式编码物理演化的因果关系和时间局部性结构

Method: 采用图神经网模拟器(GNS)消息传递框架，结合显式数值时间步进方案，通过学习瞬时时间导数来建立PDE解的正向模型，并介绍了PCA+KMeans轨迹选择策略

Result: 在三个典型PDE系统上，GNS仅需30个训练样本(总数据的3%)就能达到小于1%的相对L2误差，平均消除了82.48%的FNO AR误差和99.86%的DON AR误差，显著减少了长时间范围内的误差累积

Conclusion: 结合图基本地归纳偏置与传统时间积分器，可以构建准确、物理一致且可扩展的时间依赖性PDE代理模型

Abstract: Neural operators (NOs) approximate mappings between infinite-dimensional
function spaces but require large datasets and struggle with scarce training
data. Many NO formulations don't explicitly encode causal, local-in-time
structure of physical evolution. While autoregressive models preserve causality
by predicting next time-steps, they suffer from rapid error accumulation. We
employ Graph Neural Simulators (GNS) - a message-passing graph neural network
framework - with explicit numerical time-stepping schemes to construct accurate
forward models that learn PDE solutions by modeling instantaneous time
derivatives. We evaluate our framework on three canonical PDE systems: (1) 2D
Burgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D
Allen-Cahn equation. Rigorous evaluations demonstrate GNS significantly
improves data efficiency, achieving higher generalization accuracy with
substantially fewer training trajectories compared to neural operator baselines
like DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors
with only 30 training samples out of 1000 (3% of available data) across all
three PDE systems. It substantially reduces error accumulation over extended
temporal horizons: averaged across all cases, GNS reduces autoregressive error
by 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a
PCA+KMeans trajectory selection strategy enhancing low-data performance.
Results indicate combining graph-based local inductive biases with conventional
time integrators yields accurate, physically consistent, and scalable surrogate
models for time-dependent PDEs.

</details>


### [55] [time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models](https://arxiv.org/abs/2509.05801)
*Debdeep Sanyal,Aaryan Nagpal,Dhruv Kumar,Murari Mandal,Saurabh Deshpande*

Main category: cs.LG

TL;DR: 通过激活移植技术演示了时间序列变奏器模型内部存在可控制的语义表征空间，能够通过注入历史事件的统计特征来直接控制预测结果


<details>
  <summary>Details</summary>
Motivation: 探索变奏器基础模型是否真正内部化了语义概念（如市场制度），以及是否能利用其内部表征来模拟稀罕高风险事件

Method: 提出激活移植技术，通过将某个事件（如历史市场崩盘）的统计矩征强制注入到另一个事件（如平静期）的隐藏状态中，实现因果干预

Result: 注入崩盘语义会导致下跌预测，注入平静语义则可压制崩盘预测；模型编码了事件严重程度的等级概念，潜在向量模长与系统性震荡强度直接相关

Conclusion: 可控制的、语义基础的表征是大型时间序列变奏器的稳健特性，为语义"假如"分析和战略压力测试提供了新方法

Abstract: While transformer-based foundation models excel at forecasting routine
patterns, two questions remain: do they internalize semantic concepts such as
market regimes, or merely fit curves? And can their internal representations be
leveraged to simulate rare, high-stakes events such as market crashes? To
investigate this, we introduce activation transplantation, a causal
intervention that manipulates hidden states by imposing the statistical moments
of one event (e.g., a historical crash) onto another (e.g., a calm period)
during the forward pass. This procedure deterministically steers forecasts:
injecting crash semantics induces downturn predictions, while injecting calm
semantics suppresses crashes and restores stability. Beyond binary control, we
find that models encode a graded notion of event severity, with the latent
vector norm directly correlating with the magnitude of systemic shocks.
Validated across two architecturally distinct TSFMs, Toto (decoder only) and
Chronos (encoder-decoder), our results demonstrate that steerable, semantically
grounded representations are a robust property of large time series
transformers. Our findings provide evidence for a latent concept space that
governs model predictions, shifting interpretability from post-hoc attribution
to direct causal intervention, and enabling semantic "what-if" analysis for
strategic stress-testing.

</details>


### [56] [Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning](https://arxiv.org/abs/2509.06213)
*Christo Mathew,Wentian Wang,Lazaros Gallos,Paul Kantor,Vladimir Menkov,Hao Wang*

Main category: cs.LG

TL;DR: 在Game Of Hidden Rules(GOHR)环境中使用Transformer基于A2C算法进行强化学习，比较了特征中心和对象中心两种状态表示策略在部分观测环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 研究如何在部分可观测的复杂谜题环境中，同时推断隐藏规则并学习最优策略，探索不同状态表示方式对学习效率的影响。

Method: 采用Transformer基于的Advantage Actor-Critic(A2C)算法，比较了特征中心(FC)和对象中心(OC)两种状态表示策略，在6×6棋盘上通过放置棋子到桶中来清理棋盘。

Result: 在多种规则基础和试验列表实验设置下评估模型性能，分析了迁移效应和表示方式对学习效率的影响。

Conclusion: 该研究为部分可观测环境中的规则推断和策略学习提供了有效方法，对不同状态表示策略的比较有助于理解它们在复杂任务中的影响。

Abstract: We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)
environment, a complex puzzle in which an agent must infer and execute hidden
rules to clear a 6$\times$6 board by placing game pieces into buckets. We
explore two state representation strategies, namely Feature-Centric (FC) and
Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic
(A2C) algorithm for training. The agent has access only to partial observations
and must simultaneously infer the governing rule and learn the optimal policy
through experience. We evaluate our models across multiple rule-based and
trial-list-based experimental setups, analyzing transfer effects and the impact
of representation on learning efficiency.

</details>


### [57] [Simple Optimizers for Convex Aligned Multi-Objective Optimization](https://arxiv.org/abs/2509.05811)
*Ben Kretzu,Karen Ullrich,Yonathan Efroni*

Main category: cs.LG

TL;DR: 本文放宽了对齐多目标优化(AMOO)中的强凸性假设，在更符合深度学习实践的平滑性或Lipschitz连续性条件下，为凸AMOO问题提出了可扩展的梯度下降算法并建立了收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有AMOO框架的分析依赖于强凸性假设，这与深度学习实践中的标准假设不符。现实世界中的目标可能并不固有冲突，需要更一般的理论分析框架。

Method: 开发新的分析工具和度量标准来刻画凸AMOO设置中的收敛性，提出可扩展的梯度下降算法。

Result: 建立了算法在标准平滑性或Lipschitz连续性条件下的收敛保证，证明了朴素等权重方法的次优性。

Conclusion: 本文为凸AMOO问题提供了更实用的理论框架和算法，放宽了强凸性限制，更符合实际深度学习应用的需求。

Abstract: It is widely recognized in modern machine learning practice that access to a
diverse set of tasks can enhance performance across those tasks. This
observation suggests that, unlike in general multi-objective optimization, the
objectives in many real-world settings may not be inherently conflicting. To
address this, prior work introduced the Aligned Multi-Objective Optimization
(AMOO) framework and proposed gradient-based algorithms with provable
convergence guarantees. However, existing analysis relies on strong
assumptions, particularly strong convexity, which implies the existence of a
unique optimal solution. In this work, we relax this assumption and study
gradient-descent algorithms for convex AMOO under standard smoothness or
Lipschitz continuity conditions-assumptions more consistent with those used in
deep learning practice. This generalization requires new analytical tools and
metrics to characterize convergence in the convex AMOO setting. We develop such
tools, propose scalable algorithms for convex AMOO, and establish their
convergence guarantees. Additionally, we prove a novel lower bound that
demonstrates the suboptimality of naive equal-weight approaches compared to our
methods.

</details>


### [58] [On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data](https://arxiv.org/abs/2509.06505)
*Yu-Jui Huang,Hsin-Hua Shen,Yu-Chih Huang,Wan-Yi Lin,Shih-Chun Lin*

Main category: cs.LG

TL;DR: 本文研究了Wasserstein GAN（WGAN）在非线性激活函数和非高斯数据情况下的最优参数选择问题，推导出了一维WGAN的闭式最优参数解，并通过切片Wasserstein框架扩展到高维情况。


<details>
  <summary>Details</summary>
Motivation: 现有WGAN参数选择方法主要局限于线性-二次-高斯（LQG）设置，缺乏对非线性激活函数和非高斯数据的理论指导。本文旨在突破这一限制，为更一般的WGAN设置提供理论最优参数解。

Method: 1）推导一维WGAN在非线性激活函数和非高斯数据下的闭式最优参数；2）采用切片Wasserstein框架将结果扩展到高维情况；3）用原始数据的联合分布约束替代随机投影数据的边际分布约束。

Result: 证明了线性生成器在切片WGAN中对于非高斯数据可以是渐近最优的。实验表明，所提出的闭式参数在高斯和拉普拉斯分布数据下具有良好的收敛性，且相比r-PCA方法在相同性能下计算资源需求更少。

Conclusion: 本文成功突破了WGAN参数选择的LQG限制，为非线性激活函数和非高斯数据场景提供了理论最优解，并通过切片Wasserstein框架实现了向高维问题的有效扩展，具有重要的理论和实践价值。

Abstract: The generative adversarial network (GAN) aims to approximate an unknown
distribution via a parameterized neural network (NN). While GANs have been
widely applied in reinforcement and semisupervised learning as well as computer
vision tasks, selecting their parameters often needs an exhaustive search and
only a few selection methods can be proved to be theoretically optimal. One of
the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on
optimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG)
setting, where the NN is linear and the data is Gaussian. In this paper, we
focus on the characterization of optimal WGAN parameters beyond the LQG
setting. We derive closed-form optimal parameters for one-dimensional WGANs
when the NN has non-linear activation functions and the data is non-Gaussian.
To extend this to high-dimensional WGANs, we adopt the sliced Wasserstein
framework and replace the constraint on marginal distributions of the randomly
projected data by a constraint on the joint distribution of the original
(unprojected) data. We show that the linear generator can be asymptotically
optimal for sliced WGAN with non-Gaussian data. Empirical studies show that our
closed-form WGAN parameters have good convergence behavior with data under both
Gaussian and Laplace distributions. Also, compared to the r principal component
analysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve
the same performance while requiring less computational resources.

</details>


### [59] [Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints](https://arxiv.org/abs/2509.06395)
*Lili Chen,Changyang She,Jingge Zhu,Jamie Evans*

Main category: cs.LG

TL;DR: 提出了一种基于GNN和拉格朗日对偶优化的理论保证框架JCPGNN-M，用于无线网络中带QoS约束的多信道资源分配，在保证性能的同时显著提升计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法通过惩罚项处理QoS约束缺乏理论收敛保证，在实际场景中经常无法满足服务质量要求，需要一种理论严谨且可扩展的解决方案。

Method: 1. 扩展WMMSE算法到多信道QoS约束场景(eWMMSE)；2. 开发支持用户多信道同时分配的GNN算法JCPGNN-M；3. 将GNN与拉格朗日原始-对偶优化方法结合，在拉格朗日框架内训练GNN。

Result: JCPGNN-M在性能上匹配eWMMSE，同时在推理速度、对大网络的泛化能力以及不完美信道状态信息下的鲁棒性方面都有显著提升。

Conclusion: 该工作为未来无线网络中带约束的资源分配提供了一个可扩展且理论严谨的解决方案，结合了传统优化方法的理论保证和深度学习的计算效率优势。

Abstract: Meeting minimum data rate constraints is a significant challenge in wireless
communication systems, particularly as network complexity grows. Traditional
deep learning approaches often address these constraints by incorporating
penalty terms into the loss function and tuning hyperparameters empirically.
However, this heuristic treatment offers no theoretical convergence guarantees
and frequently fails to satisfy QoS requirements in practical scenarios.
Building upon the structure of the WMMSE algorithm, we first extend it to a
multi-channel setting with QoS constraints, resulting in the enhanced WMMSE
(eWMMSE) algorithm, which is provably convergent to a locally optimal solution
when the problem is feasible. To further reduce computational complexity and
improve scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of
supporting simultaneous multi-channel allocation per user. To overcome the
limitations of traditional deep learning methods, we propose a principled
framework that integrates GNN with a Lagrangian-based primal-dual optimization
method. By training the GNN within the Lagrangian framework, we ensure
satisfaction of QoS constraints and convergence to a stationary point.
Extensive simulations demonstrate that JCPGNN-M matches the performance of
eWMMSE while offering significant gains in inference speed, generalization to
larger networks, and robustness under imperfect channel state information. This
work presents a scalable and theoretically grounded solution for constrained
resource allocation in future wireless networks.

</details>


### [60] [Performance of Conformal Prediction in Capturing Aleatoric Uncertainty](https://arxiv.org/abs/2509.05826)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: 本文研究发现共形预测方法在捕捉数据固有歧义性（偶然不确定性）方面效果有限，预测集大小与人类标注者标注的标签数量相关性很弱


<details>
  <summary>Details</summary>
Motivation: 共形预测被认为能够通过预测集大小来捕捉偶然不确定性，但缺乏实证证据验证这一属性的有效性

Method: 使用三种共形预测方法为八个深度学习模型生成预测集，在四个包含多人标注的数据集上测量预测集大小与人类标注标签数量的相关性

Result: 绝大多数共形预测输出与人类标注之间仅呈现非常弱到弱的相关性，只有少数显示中等相关性

Conclusion: 虽然共形预测可以提供更高的真实类别覆盖率，但其捕捉偶然不确定性的能力仍然有限，需要重新评估其生成的预测集

Abstract: Conformal prediction is a model-agnostic approach to generating prediction
sets that cover the true class with a high probability. Although its prediction
set size is expected to capture aleatoric uncertainty, there is a lack of
evidence regarding its effectiveness. The literature presents that prediction
set size can upper-bound aleatoric uncertainty or that prediction sets are
larger for difficult instances and smaller for easy ones, but a validation of
this attribute of conformal predictors is missing. This work investigates how
effectively conformal predictors quantify aleatoric uncertainty, specifically
the inherent ambiguity in datasets caused by overlapping classes. We perform
this by measuring the correlation between prediction set sizes and the number
of distinct labels assigned by human annotators per instance. We further assess
the similarity between prediction sets and human-provided annotations. We use
three conformal prediction approaches to generate prediction sets for eight
deep learning models trained on four datasets. The datasets contain annotations
from multiple human annotators (ranging from five to fifty participants) per
instance, enabling the identification of class overlap. We show that the vast
majority of the conformal prediction outputs show a very weak to weak
correlation with human annotations, with only a few showing moderate
correlation. These findings underscore the necessity of critically reassessing
the prediction sets generated using conformal predictors. While they can
provide a higher coverage of the true classes, their capability in capturing
aleatoric uncertainty remains limited.

</details>


### [61] [Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning](https://arxiv.org/abs/2509.06896)
*William Xu,Yiwei Lu,Yihan Wang,Matthew Y. R. Yang,Zuoqiu Liu,Gautam Kamath,Yaoliang Yu*

Main category: cs.LG

TL;DR: 该论文研究了目标数据毒化攻击中不同测试样本的受敌程度差异，提出了三个预测指标来评估攻击难度，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目标数据毒化攻击构成了严重威胁，但目前对于什么因素使得某些测试样本更容易受到战战的理解仍不充分。需要探索攻击难度在不同测试实例中的变化规律和关键影响因素。

Method: 提出了三个预测目标数据毒化难度的准则：遍历预测准确性（通过清洁训练动态分析）、毒化距离和毒化预算。在多样化场景下进行实验验证。

Result: 实验结果表明这三个指标能够有效预测实际目标数据毒化攻击的难度变化，在不同情况下都表现出良好的预测能力。

Conclusion: 这些指标为实践者提供了有价值的漏洞评估见解，有助于更好地理解和应对目标数据毒化攻击的威胁。

Abstract: Targeted data poisoning attacks pose an increasingly serious threat due to
their ease of deployment and high success rates. These attacks aim to
manipulate the prediction for a single test sample in classification models.
Unlike indiscriminate attacks that aim to decrease overall test performance,
targeted attacks present a unique threat to individual test instances. This
threat model raises a fundamental question: what factors make certain test
samples more susceptible to successful poisoning than others? We investigate
how attack difficulty varies across different test instances and identify key
characteristics that influence vulnerability. This paper introduces three
predictive criteria for targeted data poisoning difficulty: ergodic prediction
accuracy (analyzed through clean training dynamics), poison distance, and
poison budget. Our experimental results demonstrate that these metrics
effectively predict the varying difficulty of real-world targeted poisoning
attacks across diverse scenarios, offering practitioners valuable insights for
vulnerability assessment and understanding data poisoning attacks.

</details>


### [62] [Finetuning LLMs for Human Behavior Prediction in Social Science Experiments](https://arxiv.org/abs/2509.05830)
*Akaash Kolluri,Shengguang Wu,Joon Sung Park,Michael S. Bernstein*

Main category: cs.LG

TL;DR: 这篇论文通过对大语言模型进行精调，使其能够更准确地模拟社会科学实验结果，在未见过的研究中预测准确性提升26%，超迈GPT-4o 13%。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型模拟社会科学实验结果的潜力，通过精调提高模拟的准确性和通用性。

Method: 构建SocSci210数据集（包含290万个响应数据），对Qwen2.5-14B模型进行精调，得到Socrates-Qwen-14B模型。测试在未见实验和条件下的演绎性能。

Result: 在完全未见的研究中，预测与人类响应分布的对齐度提高26%；在新条件下演绎性能提高71%；偏见度降低10.6%；超迈GPT-4o 13%。

Conclusion: 精调社会科学实验数据可显著提高LLM模拟实验的准确性，为实验假设筛选提供了更准确的模拟工具。

Abstract: Large language models (LLMs) offer a powerful opportunity to simulate the
results of social science experiments. In this work, we demonstrate that
finetuning LLMs directly on individual-level responses from past experiments
meaningfully improves the accuracy of such simulations across diverse social
science domains. We construct SocSci210 via an automatic pipeline, a dataset
comprising 2.9 million responses from 400,491 participants in 210 open-source
social science experiments. Through finetuning, we achieve multiple levels of
generalization. In completely unseen studies, our strongest model,
Socrates-Qwen-14B, produces predictions that are 26% more aligned with
distributions of human responses to diverse outcome questions under varying
conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by
13%. By finetuning on a subset of conditions in a study, generalization to new
unseen conditions is particularly robust, improving by 71%. Since SocSci210
contains rich demographic information, we reduce demographic parity, a measure
of bias, by 10.6% through finetuning. Because social sciences routinely
generate rich, topic-specific datasets, our findings indicate that finetuning
on such data could enable more accurate simulations for experimental hypothesis
screening. We release our data, models and finetuning code at
stanfordhci.github.io/socrates.

</details>


### [63] [Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces](https://arxiv.org/abs/2509.05833)
*Zeyu Song,Sainyam Galhotra,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 这篇论文提出了一个全面的分布式梯度市场评测框架，重点考察经济效率、公平性和市场稳定性等关键因素，以测试各种梯度聚合方法的维度。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习测试框架忽视了分布式梯度市场中的关键经济和系统因素，如成本效益、卖家公平性和市场稳定性，特别是当买家依赖私有基准数据集进行评估时。

Method: 研究方法包括：(1)建立模拟环境模拟市场动态；(2)在标准FL指标基础上增加以市场为中心的评估维度；(3)对MartFL框架进行深入实证分析，测试多种攻击场景；(4)探索模型性能、稳健性、成本、公平性和稳定性之间的权衡。

Result: 该评测框架为社区提供了重要的工具和实证证据，能够评估和设计更稳健、公平和经济可行的分布式梯度市场。

Conclusion: 这个综合性评测框架最终能够帮助建立更加稳健、公平和经济可行的分布式梯度市场系统。

Abstract: The rise of distributed and privacy-preserving machine learning has sparked
interest in decentralized gradient marketplaces, where participants trade
intermediate artifacts like gradients. However, existing Federated Learning
(FL) benchmarks overlook critical economic and systemic factors unique to such
marketplaces-cost-effectiveness, fairness to sellers, and market
stability-especially when a buyer relies on a private baseline dataset for
evaluation.
  We introduce a comprehensive benchmark framework to holistically evaluate
robust gradient aggregation methods within these buyer-baseline-reliant
marketplaces. Our contributions include: (1) a simulation environment modeling
marketplace dynamics with a variable buyer baseline and diverse seller
distributions; (2) an evaluation methodology augmenting standard FL metrics
with marketplace-centric dimensions such as Economic Efficiency, Fairness, and
Selection Dynamics; (3) an in-depth empirical analysis of the existing
Distributed Gradient Marketplace framework, MartFL, including the integration
and comparative evaluation of adapted FLTrust and SkyMask as alternative
aggregation strategies within it. This benchmark spans diverse datasets, local
attacks, and Sybil attacks targeting the marketplace selection process; and (4)
actionable insights into the trade-offs between model performance, robustness,
cost, fairness, and stability.
  This benchmark equips the community with essential tools and empirical
evidence to evaluate and design more robust, equitable, and economically viable
decentralized gradient marketplaces.

</details>


### [64] [Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics](https://arxiv.org/abs/2509.05839)
*Daksh Mittal,Shunri Zheng,Jing Dong,Hongseok Namkoong*

Main category: cs.LG

TL;DR: 提出基于自回归序列模型的数据驱动排队网络建模框架，通过事件流数据学习事件类型和时间的条件分布，无需人工指定到达过程、服务机制或路由逻辑，实现自动化高保真仿真器构建。


<details>
  <summary>Details</summary>
Motivation: 传统排队网络模型需要大量人工努力和领域专业知识来构建，限制了其可扩展性和可访问性。随着人工智能技术发展和数据可用性增加，需要更自动化的数据驱动建模方法。

Method: 使用Transformer架构的自回归序列模型，从事件流数据中学习事件类型和事件时间的条件分布，将建模任务重新定义为序列分布学习问题。

Result: 在多样化排队网络生成的事件表上验证了框架的有效性，展示了在仿真、不确定性量化和反事实评估方面的实用性。

Conclusion: 该框架利用人工智能进步和数据可用性，向更自动化的数据驱动建模流程迈进，支持排队网络模型在服务领域的更广泛采用。

Abstract: While queueing network models are powerful tools for analyzing service
systems, they traditionally require substantial human effort and domain
expertise to construct. To make this modeling approach more scalable and
accessible, we propose a data-driven framework for queueing network modeling
and simulation based on autoregressive sequence models trained on event-stream
data. Instead of explicitly specifying arrival processes, service mechanisms,
or routing logic, our approach learns the conditional distributions of event
types and event times, recasting the modeling task as a problem of sequence
distribution learning. We show that Transformer-style architectures can
effectively parameterize these distributions, enabling automated construction
of high-fidelity simulators. As a proof of concept, we validate our framework
on event tables generated from diverse queueing networks, showcasing its
utility in simulation, uncertainty quantification, and counterfactual
evaluation. Leveraging advances in artificial intelligence and the growing
availability of data, our framework takes a step toward more automated,
data-driven modeling pipelines to support broader adoption of queueing network
models across service domains.

</details>


### [65] [Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning](https://arxiv.org/abs/2509.05874)
*Shao-An Yin*

Main category: cs.LG

TL;DR: 基于深度强化学习的稀疏参考文献选择框架，模拟人类知识构建过程，在有限时间和成本下优先选择阅读哪些论文


<details>
  <summary>Details</summary>
Motivation: 科学文献快速扩张使得获取新知识越来越困难，特别是在专业领域，推理复杂、全文访问受限、目标参考文献在大量候选文献中稀疏分布

Method: 采用深度强化学习框架进行稀疏参考文献选择，模拟人类知识构建过程，在有限时间和成本约束下优先选择阅读论文

Result: 在药物-基因关系发现任务中评估，仅使用标题和摘要信息，证明人类和机器都能从部分信息中有效构建知识

Conclusion: 该深度强化学习框架能够有效处理稀疏参考文献选择问题，在信息受限条件下支持知识构建

Abstract: The rapid expansion of scientific literature makes it increasingly difficult
to acquire new knowledge, particularly in specialized domains where reasoning
is complex, full-text access is restricted, and target references are sparse
among a large set of candidates. We present a Deep Reinforcement Learning
framework for sparse reference selection that emulates human knowledge
construction, prioritizing which papers to read under limited time and cost.
Evaluated on drug--gene relation discovery with access restricted to titles and
abstracts, our approach demonstrates that both humans and machines can
construct knowledge effectively from partial information.

</details>


### [66] [SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework](https://arxiv.org/abs/2509.05886)
*Reza Pirayeshshirazinezhad*

Main category: cs.LG

TL;DR: 基于机器学习和物理信息神经网络的液态钠对流冷却热传道预测模型，减少CFD计算成本


<details>
  <summary>Details</summary>
Motivation: 高保真度CFD模拟液态金属强强对流冷却需要大量计算资源和时间，需要更高效的预测方法来优化小型冷却器设计

Method: 采用核机器学习、浅层神经网络、自监督物理信息神经网络（动态调整物理损失权重）和迁移学习（基于水数据训练后调整到钠）

Result: 自监督物理信息神经网络预测错误约±8%，单纯使用物理方法错误5-10%，其他机器学习方法大多数在±8%以内

Conclusion: 机器学习模型为液态金属冷却的小型热汇设计提供了高效、准确的预测工具，显著降低了计算成本和时间

Abstract: A surrogate model is developed to predict the convective heat transfer
coefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.
Initially, kernel-based machine learning techniques and shallow neural network
are applied to a dataset with 87 Nusselt numbers for liquid sodium in
rectangular miniature heat sinks. Subsequently, a self-supervised
physics-informed neural network and transfer learning approach are used to
increase the estimation performance. In the self-supervised physics-informed
neural network, an additional layer determines the weight the of physics in the
loss function to balance data and physics based on their uncertainty for a
better estimation. For transfer learning, a shallow neural network trained on
water is adapted for use with Na. Validation results show that the
self-supervised physics-informed neural network successfully estimate the heat
transfer rates of Na with an error margin of approximately +8%. Using only
physics for regression, the error remains between 5% to 10%. Other machine
learning methods specify the prediction mostly within +8%. High-fidelity
modeling of turbulent forced convection of liquid metals using computational
fluid dynamics (CFD) is both time-consuming and computationally expensive.
Therefore, machine learning based models offer a powerful alternative tool for
the design and optimization of liquid-metal-cooled miniature heat sinks.

</details>


### [67] [X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs](https://arxiv.org/abs/2509.05899)
*Dazhi Peng*

Main category: cs.LG

TL;DR: X-SQL是一个基于开源模型的领先Text-to-SQL框架，通过创新的数据库模式专家组件（X-Linking和X-Admin）显著提升模式信息利用，在Spider数据集上取得了84.9%和82.5%的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based Text-to-SQL框架往往忽视数据库模式信息的重要性，而研究发现模式信息对生成高质量SQL查询起着关键甚至主导作用。

Method: 提出包含两个组件的数据库模式专家：1）基于LLM监督微调的X-Linking方法用于模式链接；2）创新的X-Admin组件通过桥接抽象模式信息和用户自然语言问题来实现模式理解。同时采用多LLM策略进一步提升性能。

Result: 在Spider-Dev数据集上达到84.9%的执行准确率，在Spider-Test数据集上达到82.5%的执行准确率，成为基于开源模型的领先Text-to-SQL框架。

Conclusion: X-SQL框架通过有效利用数据库模式信息和多LLM策略，显著提升了Text-to-SQL任务的性能，证明了模式信息在此类任务中的关键作用。

Abstract: With Large Language Models' (LLMs) emergent abilities on code generation
tasks, Text-to-SQL has become one of the most popular downstream applications.
Despite the strong results of multiple recent LLM-based Text-to-SQL frameworks,
the research community often overlooks the importance of database schema
information for generating high-quality SQL queries. We find that such schema
information plays a significant or even dominant role in the Text-to-SQL task.
To tackle this challenge, we propose a novel database schema expert with two
components. We first introduce X-Linking, an LLM Supervised Finetuning
(SFT)-based method that achieves superior Schema Linking results compared to
existing open-source Text-to-SQL methods. In addition, we innovatively propose
an X-Admin component that focuses on Schema Understanding by bridging the gap
between abstract schema information and the user's natural language question.
Aside from better learning with schema information, we experiment with
Multi-LLMs for different components within the system to further boost its
performance. By incorporating these techniques into our end-to-end framework,
X-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset
and 82.5% on the Spider-Test dataset. This outstanding performance establishes
X-SQL as the leading Text-to-SQL framework based on open-source models.

</details>


### [68] [Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior](https://arxiv.org/abs/2509.06025)
*Vignesh Ethiraj,Subhash Talluri*

Main category: cs.LG

TL;DR: 提出了统一交互基础模型(UIFM)，采用复合标记化方法将多属性事件作为语义连贯单元处理，以更好地理解用户行为序列


<details>
  <summary>Details</summary>
Motivation: 当前基于自然语言的基础模型在处理电信、电商、金融等领域的结构化交互事件时，通过文本序列化会破坏事件的语义完整性，丢失关键上下文信息

Method: 采用复合标记化原则，将每个多属性事件视为单个语义连贯单元，学习用户行为的底层"语法"，感知完整的交互而非离散的数据点流

Result: 该架构不仅更准确，而且代表了向创建更适应性和智能预测系统迈出的根本性一步

Conclusion: UIFM模型通过保持事件的语义连贯性，实现了对复杂事件序列的真正行为理解，为构建智能预测系统提供了新方向

Abstract: A central goal of artificial intelligence is to build systems that can
understand and predict complex, evolving sequences of events. However, current
foundation models, designed for natural language, fail to grasp the holistic
nature of structured interactions found in domains like telecommunications,
e-commerce and finance. By serializing events into text, they disassemble them
into semantically fragmented parts, losing critical context. In this work, we
introduce the Unified Interaction Foundation Model (UIFM), a foundation model
engineered for genuine behavioral understanding. At its core is the principle
of composite tokenization, where each multi-attribute event is treated as a
single, semantically coherent unit. This allows UIFM to learn the underlying
"grammar" of user behavior, perceiving entire interactions rather than a
disconnected stream of data points. We demonstrate that this architecture is
not just more accurate, but represents a fundamental step towards creating more
adaptable and intelligent predictive systems.

</details>


### [69] [PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training](https://arxiv.org/abs/2509.06053)
*Mingrui Lv,Hangzhi Liu,Zhi Luo,Hongjie Zhang,Jie Ou*

Main category: cs.LG

TL;DR: PolicyEvolve是一个基于大语言模型的框架，用于生成多玩家游戏中的可解释程序化策略，通过迭代进化减少对大量环境交互的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习需要大量样本和计算资源，且策略缺乏可解释性，限制了实际部署。受LLM在单智能体任务中成功生成程序化策略的启发，希望将其扩展到多玩家游戏场景。

Method: 框架包含四个模块：全局池保存精英策略，局部池存储临时策略，策略规划器基于环境信息和反馈生成策略，轨迹评论家分析交互数据识别漏洞并提供改进方向。通过迭代进化过程不断优化策略。

Result: 显著减少对人工编写策略代码的依赖，以最少的环境交互实现高性能策略，生成可解释的规则化代码。

Conclusion: PolicyEvolve为多玩家游戏提供了一种高效、可解释的策略生成方法，通过LLM和进化框架的结合解决了传统MARL方法的资源消耗和可解释性问题。

Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress
in solving complex multi-player games through self-play. However, training
effective adversarial policies requires millions of experience samples and
substantial computational resources. Moreover, these policies lack
interpretability, hindering their practical deployment. Recently, researchers
have successfully leveraged Large Language Models (LLMs) to generate
programmatic policies for single-agent tasks, transforming neural network-based
policies into interpretable rule-based code with high execution efficiency.
Inspired by this, we propose PolicyEvolve, a general framework for generating
programmatic policies in multi-player games. PolicyEvolve significantly reduces
reliance on manually crafted policy code, achieving high-performance policies
with minimal environmental interactions. The framework comprises four modules:
Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool
preserves elite policies accumulated during iterative training. The Local Pool
stores temporary policies for the current iteration; only sufficiently
high-performing policies from this pool are promoted to the Global Pool. The
Policy Planner serves as the core policy generation module. It samples the top
three policies from the Global Pool, generates an initial policy for the
current iteration based on environmental information, and refines this policy
using feedback from the Trajectory Critic. Refined policies are then deposited
into the Local Pool. This iterative process continues until the policy achieves
a sufficiently high average win rate against the Global Pool, at which point it
is integrated into the Global Pool. The Trajectory Critic analyzes interaction
data from the current policy, identifies vulnerabilities, and proposes
directional improvements to guide the Policy Planner

</details>


### [70] [A novel biomass fluidized bed gasification model coupled with machine learning and CFD simulation](https://arxiv.org/abs/2509.06056)
*Chun Wang*

Main category: cs.LG

TL;DR: 提出基于机器学习和计算流体力学的生物质流化床气化耦合模型，提高复杂热化学反应过程的预测精度和计算效率


<details>
  <summary>Details</summary>
Motivation: 提高生物质流化床气化过程中复杂热化学反应过程的预测精度和计算效率

Method: 基于实验数据和高保真模拟结果构建高质量数据集，训练描述反应动力学特性的代理模型，并将其嵌入CFD框架实现反应速率和组分演化的实时更新

Result: 建立了机器学习与CFD耦合的预测模型

Conclusion: 该方法能够有效提升生物质流化床气化过程的模拟精度和计算效率

Abstract: A coupling model of biomass fluidized bed gasification based on machine
learning and computational fluid dynamics is proposed to improve the prediction
accuracy and computational efficiency of complex thermochemical reaction
process. By constructing a high-quality data set based on experimental data and
high fidelity simulation results, the agent model used to describe the
characteristics of reaction kinetics was trained and embedded into the
computational fluid dynamics (CFD) framework to realize the real-time update of
reaction rate and composition evolution.

</details>


### [71] [ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting](https://arxiv.org/abs/2509.06060)
*Fei Wang,Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Zhulin An,Yongjun Xu,Xueqi Cheng*

Main category: cs.LG

TL;DR: ARIES是一个评估时间序列属性与建模策略关系的框架，通过构建合成数据集和全面基准测试，建立了时间序列属性与模型性能的关联，并实现了首个深度预测模型推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集缺乏多样化的时间模式，无法系统评估时间序列属性与模型性能的关系，且没有有效的模型推荐方法，导致测试不同架构时耗费大量时间和成本。

Method: 首先构建具有多种不同模式的合成数据集，设计全面系统计算时间序列属性；然后对50多个预测模型进行广泛基准测试，建立时间序列属性与建模策略的关系；基于发现实现深度预测模型推荐器。

Result: 实验结果显示时间序列属性与建模策略之间存在明显相关性，成功实现了能够为真实时间序列提供可解释建议的模型推荐系统。

Conclusion: ARIES是首个建立时间序列数据属性与建模策略关系的研究，同时实现了模型推荐系统，为时间序列预测领域提供了系统化的评估和推荐框架。

Abstract: Recent advancements in deep learning models for time series forecasting have
been significant. These models often leverage fundamental time series
properties such as seasonality and non-stationarity, which may suggest an
intrinsic link between model performance and data properties. However, existing
benchmark datasets fail to offer diverse and well-defined temporal patterns,
restricting the systematic evaluation of such connections. Additionally, there
is no effective model recommendation approach, leading to high time and cost
expenditures when testing different architectures across different downstream
applications. For those reasons, we propose ARIES, a framework for assessing
relation between time series properties and modeling strategies, and for
recommending deep forcasting models for realistic time series. First, we
construct a synthetic dataset with multiple distinct patterns, and design a
comprehensive system to compute the properties of time series. Next, we conduct
an extensive benchmarking of over 50 forecasting models, and establish the
relationship between time series properties and modeling strategies. Our
experimental results reveal a clear correlation. Based on these findings, we
propose the first deep forecasting model recommender, capable of providing
interpretable suggestions for real-world time series. In summary, ARIES is the
first study to establish the relations between the properties of time series
data and modeling strategies, while also implementing a model recommendation
system. The code is available at: https://github.com/blisky-li/ARIES.

</details>


### [72] [A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network](https://arxiv.org/abs/2509.06067)
*Mianjun Xiao,Peng Song,Yulong Liu,Cedric Korte,Ziyang Xu,Jiale Gao,Jiaqi Lu,Haoyang Nie,Qiantong Deng,Timing Qu*

Main category: cs.LG

TL;DR: 基于殊余神经网络的代模型用于预测REBCO超导磁铁的流密度分布，大大提高计算效率且保持精度


<details>
  <summary>Details</summary>
Motivation: 传统有限元法在大型高温超导磁铁设计中计算成本高，限制了磁铁系统的快速设计

Method: 开发基于全连接殊余神经网络(FCRN)的代模型，使用FEM模拟数据进行训练，最佳网络配置为12个殊余块每层256个神经元

Result: 模型能够在训练范围外50%的外推能力，最大误差低于10%，预测速度比FEM快几个数量级

Conclusion: 提出的FCRN代模型具有准确性和高效性，为大型HTS磁铁的快速分析提供了有前景的工具

Abstract: Finite element method (FEM) is widely used in high-temperature
superconducting (HTS) magnets, but its computational cost increases with magnet
size and becomes time-consuming for meter-scale magnets, especially when
multi-physics couplings are considered, which limits the fast design of
large-scale REBCO magnet systems. In this work, a surrogate model based on a
fully connected residual neural network (FCRN) is developed to predict the
space-time current density distribution in REBCO solenoids. Training datasets
were generated from FEM simulations with varying numbers of turns and pancakes.
The results demonstrate that, for deeper networks, the FCRN architecture
achieves better convergence than conventional fully connected network (FCN),
with the configuration of 12 residual blocks and 256 neurons per layer
providing the most favorable balance between training accuracy and
generalization capability. Extrapolation studies show that the model can
reliably predict magnetization losses for up to 50% beyond the training range,
with maximum errors below 10%. The surrogate model achieves predictions several
orders of magnitude faster than FEM and still remains advantageous when
training costs are included. These results indicate that the proposed
FCRN-based surrogate model provides both accuracy and efficiency, offering a
promising tool for the rapid analysis of large-scale HTS magnets.

</details>


### [73] [Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs](https://arxiv.org/abs/2509.06094)
*S. R. Eshwar*

Main category: cs.LG

TL;DR: 本文针对具有准双曲线(QH)折扣偏好的预先承诺智能体，提出了强化学习框架下的理论分析和实用算法。首次证明了最优策略可简化为简单的一步非平稳形式，并设计了首个具有可证明收敛保证的模型无关算法。


<details>
  <summary>Details</summary>
Motivation: 准双曲线折扣是人类和动物决策中时间不一致偏好的关键特征，但将其整合到强化学习框架中的研究有限。本文旨在填补QH偏好下预先承诺智能体的理论和算法空白。

Method: 通过理论分析形式化描述了最优策略的结构，证明其可简化为一步非平稳形式。设计了模型无关的策略评估和Q学习算法，并提供收敛性证明。

Result: 首次证明了QH偏好下最优策略的简化结构，开发了首个实用的模型无关算法，为在强化学习中整合QH偏好提供了理论基础和算法工具。

Conclusion: 该研究为准双曲线折扣偏好下的强化学习提供了重要的理论见解和实用算法，填补了该领域的空白，为未来相关研究奠定了基础。

Abstract: Time-inconsistent preferences, where agents favor smaller-sooner over
larger-later rewards, are a key feature of human and animal decision-making.
Quasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this
behavior, but its integration into the reinforcement learning (RL) framework
has been limited. This paper addresses key theoretical and algorithmic gaps for
precommitted agents with QH preferences. We make two primary contributions: (i)
we formally characterize the structure of the optimal policy, proving for the
first time that it reduces to a simple one-step non-stationary form; and (ii)
we design the first practical, model-free algorithms for both policy evaluation
and Q-learning in this setting, both with provable convergence guarantees. Our
results provide foundational insights for incorporating QH preferences in RL.

</details>


### [74] [Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models](https://arxiv.org/abs/2509.06161)
*Aurora Polo-Rodríguez,Juan Carlos Valera,Jesús Peral,David Gil,Javier Medina-Quero*

Main category: cs.LG

TL;DR: 使用UWB技术和深度学习模型进行家庭环境中居民路径追踪的研究，提出了基于指纹识别的方法，比较了CNN、LSTM和混合模型，获得了约50厘米的平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: 解决UWB技术在真实环境中因墙壁和障碍物存在而精度降低的问题，提高家庭环境中人类活动识别的准确性。

Method: 采用基于指纹识别的方法，利用从两个公寓收集的RSSI数据，比较CNN、LSTM和CNN+LSTM混合模型，同时评估不同时间窗口类型和持续时间的影响。

Result: 获得了接近50厘米的平均绝对误差，混合模型在提供准确位置估计方面表现最优。

Conclusion: 混合模型在住宅环境中日常人类活动识别应用中具有优越性，能够提供准确的位置估计。

Abstract: The field of human activity recognition has evolved significantly, driven
largely by advancements in Internet of Things (IoT) device technology,
particularly in personal devices. This study investigates the use of
ultra-wideband (UWB) technology for tracking inhabitant paths in home
environments using deep learning models. UWB technology estimates user
locations via time-of-flight and time-difference-of-arrival methods, which are
significantly affected by the presence of walls and obstacles in real
environments, reducing their precision. To address these challenges, we propose
a fingerprinting-based approach utilizing received signal strength indicator
(RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while
performing daily activities. We compare the performance of convolutional neural
network (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as
well as the use of Bluetooth technology. Additionally, we evaluate the impact
of the type and duration of the temporal window (future, past, or a combination
of both). Our results demonstrate a mean absolute error close to 50 cm,
highlighting the superiority of the hybrid model in providing accurate location
estimates, thus facilitating its application in daily human activity
recognition in residential settings.

</details>


### [75] [An Improved Template for Approximate Computing](https://arxiv.org/abs/2509.06162)
*M. Rezaalipour,F. Costa,M. Biasion,R. Otoni,G. A. Constantinides,L. Pozzi*

Main category: cs.LG

TL;DR: 通过提出一种基于可参数化乘积共享模板的新方法，在保持精度的同时较现有技术更好地降低神经网络计算运算器的面积和能耗。


<details>
  <summary>Details</summary>
Motivation: 在边缘设处上部署神经网络时，需要在计算能耗和分类准确性之间取得平衡。近似计算技术可以通过轻微降低算术运算器的准确性来减少能耗消耗。

Method: 改进现有的XPAT布尔重写技术，提出一种基于可参数化乘积共享模板的新方法。模板参数作为指标代理，新模板能够更好地代表合成面积。

Result: 实验结果显示，该方法在寻找低面积解决方案时具有更好的收敛性，能够找到比原始XPAT和其他两种先进方法更好的近似解决方案。

Conclusion: 该研究提出的新模板方法能够在相同准确性损失下实现更好的面积节省效果，为边缘设处上的神经网络部署提供了更有效的能耗优化方案。

Abstract: Deploying neural networks on edge devices entails a careful balance between
the energy required for inference and the accuracy of the resulting
classification. One technique for navigating this tradeoff is approximate
computing: the process of reducing energy consumption by slightly reducing the
accuracy of arithmetic operators. In this context, we propose a methodology to
reduce the area of the small arithmetic operators used in neural networks -
i.e., adders and multipliers - via a small loss in accuracy, and show that we
improve area savings for the same accuracy loss w.r.t. the state of the art. To
achieve our goal, we improve on a boolean rewriting technique recently
proposed, called XPAT, where the use of a parametrisable template to rewrite
circuits has proved to be highly beneficial. In particular, XPAT was able to
produce smaller circuits than comparable approaches while utilising a naive sum
of products template structure. In this work, we show that template parameters
can act as proxies for chosen metrics and we propose a novel template based on
parametrisable product sharing that acts as a close proxy to synthesised area.
We demonstrate experimentally that our methodology converges better to low-area
solutions and that it can find better approximations than both the original
XPAT and two other state-of-the-art approaches.

</details>


### [76] [Exploring Urban Factors with Autoencoders: Relationship Between Static and Dynamic Features](https://arxiv.org/abs/2509.06167)
*Ximena Pocco,Waqar Hassan,Karelia Salinas,Vladimir Molchanov,Luis G. Nonato*

Main category: cs.LG

TL;DR: 这篇论文研究了融合异质多模态城市数据的潜在表征在发现城市模式方面的效果，通过可视化分析框架识别融合表征能产生更结构化的模式


<details>
  <summary>Details</summary>
Motivation: 城市分析中的数据具有粒度细、异质性和多模态性特征，现有可视化工具少有探索融合数据是否能比独立数据源提供更深入的见解

Method: 开发了一个可视化辅助框架，用于分析融合潜在数据表征与独立表征在发现动态和静态城市数据模式方面的效果对比

Result: 分析显示组合的潜在表征能够产生更结构化的模式，而独立表征在特定情况下也有其使用价值

Conclusion: 融合异质多模态城市数据的潜在表征在发现城市模式方面更有效，但独立数据源在特定场景中仍有价值

Abstract: Urban analytics utilizes extensive datasets with diverse urban information to
simulate, predict trends, and uncover complex patterns within cities. While
these data enables advanced analysis, it also presents challenges due to its
granularity, heterogeneity, and multimodality. To address these challenges,
visual analytics tools have been developed to support the exploration of latent
representations of fused heterogeneous and multimodal data, discretized at a
street-level of detail. However, visualization-assisted tools seldom explore
the extent to which fused data can offer deeper insights than examining each
data source independently within an integrated visualization framework. In this
work, we developed a visualization-assisted framework to analyze whether fused
latent data representations are more effective than separate representations in
uncovering patterns from dynamic and static urban data. The analysis reveals
that combined latent representations produce more structured patterns, while
separate ones are useful in particular cases.

</details>


### [77] [Reasoning Language Model for Personalized Lung Cancer Screening](https://arxiv.org/abs/2509.06169)
*Chuang Niu,Ge Wang*

Main category: cs.LG

TL;DR: 提出基于推理语言模型(RLM)的肺癌风险评估方法，整合影像学发现和纵向医疗记录，显著提升风险预测性能


<details>
  <summary>Details</summary>
Motivation: 现有Lung-RADS系统仅基于肺结节特征进行风险分层，缺乏多种风险因素整合，在敏感性和特异性之间存在权衡问题

Method: 构建推理语言模型，通过数据集构建与蒸馏、监督微调、强化学习和综合评估，将风险评估任务分解为子组件，分析多种风险因素贡献，使用数据驱动系统方程计算最终风险评分

Result: 在国家肺癌筛查试验数据集上风险预测性能显著提升，同时提高了预测准确性和可监控性

Conclusion: 该方法通过思维链推理过程改善了预测性能，有助于肺癌筛查的临床转化

Abstract: Accurate risk assessment in lung cancer screening is critical for enabling
early cancer detection and minimizing unnecessary invasive procedures. The Lung
CT Screening Reporting and Data System (Lung-RADS) has been widely used as the
standard framework for patient management and follow-up. Nevertheless,
Lung-RADS faces trade-offs between sensitivity and specificity, as it
stratifies risk solely based on lung nodule characteristics without
incorporating various risk factors. Here we propose a reasoning language model
(RLM) to integrate radiology findings with longitudinal medical records for
individualized lung cancer risk assessment. Through a systematic study
including dataset construction and distillation, supervised fine-tuning,
reinforcement learning, and comprehensive evaluation, our model makes
significant improvements in risk prediction performance on datasets in the
national lung screening trial. Notably, RLM can decompose the risk evaluation
task into sub-components, analyze the contributions of diverse risk factors,
and synthesize them into a final risk score computed using our data-driven
system equation. Our approach improves both predictive accuracy and
monitorability through the chain of thought reasoning process, thereby
facilitating clinical translation into lung cancer screening.

</details>


### [78] [Metric Embedding Initialization-Based Differentially Private and Explainable Graph Clustering](https://arxiv.org/abs/2509.06214)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: 基于度量嵌入初始化的差分隐私可解释图聚类方法，通过SDP优化和HST初始化提高效率和性能


<details>
  <summary>Details</summary>
Motivation: 解决差分隐私图聚类中存在的高噪音、低效率和差解释性等挑战

Method: 构建SDP优化模型，提取关键集，使用HST基础的初始化方法获得良好聚类配置，然后应用k-median聚类策略

Result: 在公开数据集上的实验表明，该框架在各种聚类指标上超过现有方法，同时严格保证隐私

Conclusion: 该方法为差分隐私图聚类领域提供了一种高效、可解释的解决方案

Abstract: Graph clustering under the framework of differential privacy, which aims to
process graph-structured data while protecting individual privacy, has been
receiving increasing attention. Despite significant achievements in current
research, challenges such as high noise, low efficiency and poor
interpretability continue to severely constrain the development of this field.
In this paper, we construct a differentially private and interpretable graph
clustering approach based on metric embedding initialization. Specifically, we
construct an SDP optimization, extract the key set and provide a
well-initialized clustering configuration using an HST-based initialization
method. Subsequently, we apply an established k-median clustering strategy to
derive the cluster results and offer comparative explanations for the query set
through differences from the cluster centers. Extensive experiments on public
datasets demonstrate that our proposed framework outperforms existing methods
in various clustering metrics while strictly ensuring privacy.

</details>


### [79] [MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning](https://arxiv.org/abs/2509.06219)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: MCIGLE是一个无需范例的类增量学习框架，专门处理多模态图结构数据，通过特征提取对齐和递归最小二乘法解决灾难性遗忘等问题


<details>
  <summary>Details</summary>
Motivation: 随着多模态图结构数据的普及，现有方法面临灾难性遗忘、分布偏差、内存限制和泛化能力弱等挑战，需要新的解决方案

Method: 提出MCIGLE框架，通过提取和对齐多模态图特征，应用串联递归最小二乘法进行知识保留，采用多通道处理平衡准确性和内存保存

Result: 在公共数据集上的实验验证了该方法的有效性和泛化能力

Conclusion: MCIGLE成功解决了多模态图数据类增量学习中的关键问题，为相关领域提供了有效的解决方案

Abstract: Exemplar-free class-incremental learning enables models to learn new classes
over time without storing data from old ones. As multimodal graph-structured
data becomes increasingly prevalent, existing methods struggle with challenges
like catastrophic forgetting, distribution bias, memory limits, and weak
generalization. We propose MCIGLE, a novel framework that addresses these
issues by extracting and aligning multimodal graph features and applying
Concatenated Recursive Least Squares for effective knowledge retention. Through
multi-channel processing, MCIGLE balances accuracy and memory preservation.
Experiments on public datasets validate its effectiveness and generalizability.

</details>


### [80] [UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks](https://arxiv.org/abs/2509.06270)
*Honggang Jia,Xiucheng Wang,Nan Cheng,Ruijin Sun,Changle Li*

Main category: cs.LG

TL;DR: 这篇论文提供了UrbanMIMOMap数据集，为6G系统的环境感知通信提供大规模、高精度的MIMO通道状态信息数据支持


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集主要集中在SISO系统和路径损耗信息，无法满足高级MIMO系统对详细通道状态信息的需求，影响了高保真度无线地图的构建

Method: 使用高精度光线追踪技术生成大规模城市MIMO通道状态信息数据集，包含密集空间格点上的复杂CSI矩阵，超越传统路径损耗数据

Result: 创建了UrbanMIMOMap数据集，提供了全面的复杂CSI矩阵，通过代表性机器学习方法的基准性能评估验证了数据集的实用性

Conclusion: 该工作为高精度无线地图生成、MIMO空间性能和6G环境感知的机器学习研究提供了关键数据集和参考，代码和数据已开源可用

Abstract: Sixth generation (6G) systems require environment-aware communication, driven
by native artificial intelligence (AI) and integrated sensing and communication
(ISAC). Radio maps (RMs), providing spatially continuous channel information,
are key enablers. However, generating high-fidelity RM ground truth via
electromagnetic (EM) simulations is computationally intensive, motivating
machine learning (ML)-based RM construction. The effectiveness of these
data-driven methods depends on large-scale, high-quality training data. Current
public datasets often focus on single-input single-output (SISO) and limited
information, such as path loss, which is insufficient for advanced multi-input
multi-output (MIMO) systems requiring detailed channel state information (CSI).
To address this gap, this paper presents UrbanMIMOMap, a novel large-scale
urban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap
offers comprehensive complex CSI matrices across a dense spatial grid, going
beyond traditional path loss data. This rich CSI is vital for constructing
high-fidelity RMs and serves as a fundamental resource for data-driven RM
generation, including deep learning. We demonstrate the dataset's utility
through baseline performance evaluations of representative ML methods for RM
construction. This work provides a crucial dataset and reference for research
in high-precision RM generation, MIMO spatial performance, and ML for 6G
environment awareness. The code and data for this work are available at:
https://github.com/UNIC-Lab/UrbanMIMOMap.

</details>


### [81] [IPR: Intelligent Prompt Routing with User-Controlled Quality-Cost Trade-offs](https://arxiv.org/abs/2509.06274)
*Aosong Feng,Zhichao Xu,Xian Wu,Kang Zhou,Sheng Guan,Yueyan Chen,Ninad Kulkarni,Yun Zhou,Balasubramaniam Srinivasan,Haibo Ding,Lin Lee Cheong*

Main category: cs.LG

TL;DR: IPR是一个智能提示路由框架，通过预测响应质量和用户指定的容忍度来动态选择最优LLM模型，在保持质量的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模商业系统中如何将查询路由到最具成本效益的LLM，同时保持响应质量的基本挑战。

Method: 采用模块化架构，包含轻量级质量估计器（基于150万标注提示训练）、用户控制的容忍度参数路由机制，以及使用冻结编码器和模型特定适配器的可扩展设计。

Result: 在主要云平台上部署后，IPR实现了43.9%的成本降低，同时保持与Claude家族最强模型的质量相当，处理延迟低于150毫秒。

Conclusion: IPR框架有效解决了LLM路由中的性能-成本权衡问题，提供了可扩展且高效的解决方案。

Abstract: Routing incoming queries to the most cost-effective LLM while maintaining
response quality poses a fundamental challenge in optimizing performance-cost
trade-offs for large-scale commercial systems. We present IPR\, a
quality-constrained Intelligent Prompt Routing framework that dynamically
selects optimal models based on predicted response quality and user-specified
tolerance levels. IPR introduces three key innovations: (1) a modular
architecture with lightweight quality estimators trained on 1.5M prompts
annotated with calibrated quality scores, enabling fine-grained quality
prediction across model families; (2) a user-controlled routing mechanism with
tolerance parameter $\tau \in [0,1]$ that provides explicit control over
quality-cost trade-offs; and (3) an extensible design using frozen encoders
with model-specific adapters, reducing new model integration from days to
hours. To rigorously train and evaluate IPR, we curate an industrial-level
dataset IPRBench\footnote{IPRBench will be released upon legal approval.}, a
comprehensive benchmark containing 1.5 million examples with response quality
annotations across 11 LLM candidates. Deployed on a major cloud platform, IPR
achieves 43.9\% cost reduction while maintaining quality parity with the
strongest model in the Claude family and processes requests with sub-150ms
latency.

</details>


### [82] [RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations](https://arxiv.org/abs/2509.06286)
*Chang Xue,Youwei Lu,Chen Yang,Jinming Xing*

Main category: cs.LG

TL;DR: RecMind是一个LLM增强的图推荐系统，将语言模型作为偏好先验而非单一排序器，通过对比学习对齐文本和协同视图，使用门控机制融合，在冷启动和长尾场景表现优异


<details>
  <summary>Details</summary>
Motivation: 个性化推荐面临稀疏交互、快速内容更新和异构文本信号的挑战，需要有效融合语言模型和协同过滤的优势

Method: 使用冻结LLM加轻量适配器生成文本条件嵌入，LightGCN学习协同嵌入，通过对称对比目标对齐两种视图，采用层内门控融合机制

Result: 在Yelp和Amazon-Electronics数据集上所有8个指标均取得最佳结果，相对基线提升达+4.53%(Recall@40)和+4.01%(NDCG@40)

Conclusion: 跨视图对齐是必要的，门控融合优于后期融合和纯LLM变体，语言主导冷/长尾场景，图结构稳定其他场景的排序

Abstract: Personalization is a core capability across consumer technologies, streaming,
shopping, wearables, and voice, yet it remains challenged by sparse
interactions, fast content churn, and heterogeneous textual signals. We present
RecMind, an LLM-enhanced graph recommender that treats the language model as a
preference prior rather than a monolithic ranker. A frozen LLM equipped with
lightweight adapters produces text-conditioned user/item embeddings from
titles, attributes, and reviews; a LightGCN backbone learns collaborative
embeddings from the user-item graph. We align the two views with a symmetric
contrastive objective and fuse them via intra-layer gating, allowing language
to dominate in cold/long-tail regimes and graph structure to stabilize rankings
elsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results on
all eight reported metrics, with relative improvements up to +4.53\%
(Recall@40) and +4.01\% (NDCG@40) over strong baselines. Ablations confirm both
the necessity of cross-view alignment and the advantage of gating over late
fusion and LLM-only variants.

</details>


### [83] [A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults](https://arxiv.org/abs/2509.06289)
*Shaoqi Wei,Senling Wang,Hiroshi Kai,Yoshinobu Higami,Ruijun Ma,Tianming Ni,Xiaoqing Wen,Hiroshi Takahashi*

Main category: cs.LG

TL;DR: 基于空间-时间图卷积网络(ST-GCN)的统一框架，通过将门级网表建模为图结构，高效预测长周期故障影响概率，减少模拟时间10倍以上，支持数据安全风险评估和SoC测试策略优化。


<details>
  <summary>Details</summary>
Motivation: 沉默数据错误(SDEs)由初始缺陷和老化引起，会降低安全关键系统的可靠性。功能测试虽能检测SDE相关故障，但模拟成本过高，需要一种高效准确的预测方法来支持量化风险评估。

Method: 将门级网表建模为空间-时间图，以抓取拓扑结构和信号时序。采用专门的空间和时间编码器，高效地预测多周期故障影响概率(FIPs)。框架支持从可测性指标或故障模拟中提取特征，允许效率-准确性的权衡。

Result: 在ISCAS-89测试集上，方法将模拟时间减少了10倍以上，同时保持高准确度(5周期预测的均方误差为0.024)。通过预测FIPs选择观测点的研究显示，能够提高长周期、难检测故障的检测能力。

Conclusion: 该方法为快速、准确预测长周期故障影响概率提供了一种统一框架，显著提高了测试效率。方法具有良好的扩展性，能够应用于SoC级别的测试策略优化，并且能够集成到下游的电子设计自动化流程中。

Abstract: Silent Data Errors (SDEs) from time-zero defects and aging degrade
safety-critical systems. Functional testing detects SDE-related faults but is
expensive to simulate. We present a unified spatio-temporal graph convolutional
network (ST-GCN) for fast, accurate prediction of long-cycle fault impact
probabilities (FIPs) in large sequential circuits, supporting quantitative risk
assessment. Gate-level netlists are modeled as spatio-temporal graphs to
capture topology and signal timing; dedicated spatial and temporal encoders
predict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method
reduces simulation time by more than 10x while maintaining high accuracy (mean
absolute error 0.024 for 5-cycle predictions). The framework accepts features
from testability metrics or fault simulation, allowing efficiency-accuracy
trade-offs. A test-point selection study shows that choosing observation points
by predicted FIPs improves detection of long-cycle, hard-to-detect faults. The
approach scales to SoC-level test strategy optimization and fits downstream
electronic design automation flows.

</details>


### [84] [LoaQ: Layer-wise Output Approximation Quantization](https://arxiv.org/abs/2509.06297)
*Li Lin,Xiaojun Wan*

Main category: cs.LG

TL;DR: LoaQ是一种针对层级后训练量化的输出近似方法，通过关注输出级别一致性来更好地实现量化直觉，可无缝集成到现有量化流程中，在LLaMA和Qwen模型上表现优异


<details>
  <summary>Details</summary>
Motivation: 现有的层级后训练量化方法采用局部视角，只能实现权重激活感知近似，导致近似不足且偏离量化直觉。需要更准确地近似全模型输出

Method: 基于对主流LLM结构特征的深入理解，提出LoaQ方法，明确以输出级别一致性为目标，具有简单闭式解，可与现有技术正交集成

Result: 在LLaMA和Qwen模型家族上，LoaQ在仅权重量化和权重激活联合量化中均表现有效，能进一步提升整体量化质量

Conclusion: LoaQ方法更好地符合量化直觉，具有简单闭式解，易于集成到现有量化流程，有潜力推动后训练量化的前沿发展

Abstract: A natural and intuitive idea in model quantization is to approximate each
component's quantized output to match its original. Layer-wise post-training
quantization (PTQ), though based on this idea, adopts a strictly local view and
can achieve, at best, only activation-aware approximations of weights. As a
result, it often leads to insufficient approximations and practical deviations
from this guiding intuition. Recent work has achieved a more accurate
approximation of linear-layer outputs within the framework of layer-wise PTQ,
but such refinements remain inadequate for achieving alignment with the full
model output. Based on a deeper understanding of the structural characteristics
of mainstream LLMs, we propose $LoaQ$, an output-approximation method for
layer-wise PTQ that explicitly targets output-level consistency. It better
aligns with this intuition and can feature a simple closed-form solution,
making it orthogonal to existing techniques and readily integrable into
existing quantization pipelines. Experiments on the LLaMA and Qwen model
families demonstrate that LoaQ performs effectively in both weight-only and
weight-activation joint quantization. By integrating seamlessly with existing
quantization strategies, it further enhances overall quantization quality and
shows strong potential to advance the frontier of post-training quantization.

</details>


### [85] [WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting](https://arxiv.org/abs/2509.06311)
*Hang Fan,Yu Shi,Zongliang Fu,Shuo Chen,Wei Wei,Wei Xu,Jian Li*

Main category: cs.LG

TL;DR: WindFM是一个轻量级生成式基础模型，专门用于概率性风电功率预测，通过离散化-生成框架和Transformer架构，在零样本情况下实现最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法要么无法泛化到其他位置，要么难以融入能源领域特定数据，需要专门设计能够通用且无需微调的风电预测基础模型。

Method: 采用离散化-生成框架：专用时间序列分词器将连续多变量观测转换为离散分层token，然后使用仅解码器Transformer在126,000多个站点的1500亿时间步数据上进行自回归预训练。

Result: 仅810万参数的紧凑模型在确定性和概率性任务上实现零样本最先进性能，超越专业模型和更大基础模型，在不同大陆的分布外数据上表现出强适应性。

Conclusion: WindFM展示了学习表示的鲁棒性和可迁移性，为风电预测提供了有效的轻量级基础模型解决方案，模型已公开可用。

Abstract: High-quality wind power forecasting is crucial for the operation of modern
power grids. However, prevailing data-driven paradigms either train a
site-specific model which cannot generalize to other locations or rely on
fine-tuning of general-purpose time series foundation models which are
difficult to incorporate domain-specific data in the energy sector. This paper
introduces WindFM, a lightweight and generative Foundation Model designed
specifically for probabilistic wind power forecasting. WindFM employs a
discretize-and-generate framework. A specialized time-series tokenizer first
converts continuous multivariate observations into discrete, hierarchical
tokens. Subsequently, a decoder-only Transformer learns a universal
representation of wind generation dynamics by autoregressively pre-training on
these token sequences. Using the comprehensive WIND Toolkit dataset comprising
approximately 150 billion time steps from more than 126,000 sites, WindFM
develops a foundational understanding of the complex interplay between
atmospheric conditions and power output. Extensive experiments demonstrate that
our compact 8.1M parameter model achieves state-of-the-art zero-shot
performance on both deterministic and probabilistic tasks, outperforming
specialized models and larger foundation models without any fine-tuning. In
particular, WindFM exhibits strong adaptiveness under out-of-distribution data
from a different continent, demonstrating the robustness and transferability of
its learned representations. Our pre-trained model is publicly available at
https://github.com/shiyu-coder/WindFM.

</details>


### [86] [Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix](https://arxiv.org/abs/2509.06314)
*Mehmet Can Yavuz,Berrin Yanikoglu*

Main category: cs.LG

TL;DR: 提出了一种新的冗余指数ρ(C)来直接量化潜在表示中的维度间依赖性，通过分析耦合矩阵并与正态分布比较来评估表示质量。


<details>
  <summary>Details</summary>
Motivation: 深度网络常产生冗余的潜在空间，多个坐标编码重叠信息，降低有效容量并阻碍泛化。标准指标如准确率或重建损失只能间接证明这种冗余。

Method: 通过分析从潜在表示导出的耦合矩阵，使用能量距离比较其非对角统计量与正态分布，构建紧凑、可解释且统计基础良好的表示质量度量。

Result: 在多个数据集和架构上验证，低ρ(C)可靠预测高分类准确率或低重建误差，高冗余与性能崩溃相关。TPE优先探索低ρ区域，表明ρ(C)可指导神经架构搜索。

Conclusion: ρ(C)通过揭示模型和任务中的冗余瓶颈，为评估和改进学习表示的效率提供了理论视角和实用工具。

Abstract: A central challenge in representation learning is constructing latent
embeddings that are both expressive and efficient. In practice, deep networks
often produce redundant latent spaces where multiple coordinates encode
overlapping information, reducing effective capacity and hindering
generalization. Standard metrics such as accuracy or reconstruction loss
provide only indirect evidence of such redundancy and cannot isolate it as a
failure mode. We introduce a redundancy index, denoted rho(C), that directly
quantifies inter-dimensional dependencies by analyzing coupling matrices
derived from latent representations and comparing their off-diagonal statistics
against a normal distribution via energy distance. The result is a compact,
interpretable, and statistically grounded measure of representational quality.
We validate rho(C) across discriminative and generative settings on MNIST
variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple
architectures and hyperparameter optimization strategies. Empirically, low
rho(C) reliably predicts high classification accuracy or low reconstruction
error, while elevated redundancy is associated with performance collapse.
Estimator reliability grows with latent dimension, yielding natural lower
bounds for reliable analysis. We further show that Tree-structured Parzen
Estimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)
can guide neural architecture search and serve as a redundancy-aware
regularization target. By exposing redundancy as a universal bottleneck across
models and tasks, rho(C) offers both a theoretical lens and a practical tool
for evaluating and improving the efficiency of learned representations.

</details>


### [87] [Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics](https://arxiv.org/abs/2509.06322)
*Jiajun Bao,Nicolas Boullé,Toni J. B. Liu,Raphaël Sarfati,Christopher J. Earls*

Main category: cs.LG

TL;DR: LLMs能够通过上下文学习准确预测偏微分方程的时空动态，无需微调或自然语言提示，预测精度随上下文长度增加而提高，但随空间离散化变细而下降


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在零样本时间序列预测中的涌现能力，特别是如何在不经过专门训练的情况下理解和预测偏微分方程的时空动态

Method: 使用文本训练的基础模型处理离散化的偏微分方程解，分析多步预测中的误差积累，并通过token级输出分布分析模型内部处理机制

Result: 模型预测精度随时间上下文长度增加而提高，但在更细的空间离散化下精度下降；多步预测误差随时间范围代数增长；发现ICL三阶段进展模式

Conclusion: LLMs能够通过上下文学习有效处理偏微分方程预测任务，表现出可预测的神经缩放规律，为理解模型内部推理机制提供了新见解

Abstract: Large language models (LLMs) have demonstrated emergent in-context learning
(ICL) capabilities across a range of tasks, including zero-shot time-series
forecasting. We show that text-trained foundation models can accurately
extrapolate spatiotemporal dynamics from discretized partial differential
equation (PDE) solutions without fine-tuning or natural language prompting.
Predictive accuracy improves with longer temporal contexts but degrades at
finer spatial discretizations. In multi-step rollouts, where the model
recursively predicts future spatial states over multiple time steps, errors
grow algebraically with the time horizon, reminiscent of global error
accumulation in classical finite-difference solvers. We interpret these trends
as in-context neural scaling laws, where prediction quality varies predictably
with both context length and output length. To better understand how LLMs are
able to internally process PDE solutions so as to accurately roll them out, we
analyze token-level output distributions and uncover a consistent ICL
progression: beginning with syntactic pattern imitation, transitioning through
an exploratory high-entropy phase, and culminating in confident, numerically
grounded predictions.

</details>


### [88] [Exploring approaches to computational representation and classification of user-generated meal logs](https://arxiv.org/abs/2509.06330)
*Guanlan Hu,Adit Anand,Pooja M. Desai,Iñigo Urteaga,Lena Mamykina*

Main category: cs.LG

TL;DR: 使用机器学习和领域知识增强技术分析患者生成的饮食日志文本数据，能够准确分类膳食是否符合特定营养目标，性能优于用户自我评估


<details>
  <summary>Details</summary>
Motivation: 研究如何利用患者生成的自由文本饮食记录和机器学习技术，结合营养学领域知识，来准确评估膳食是否符合特定营养目标，为精准医疗中的个性化营养指导提供支持

Method: 使用包含3000多份餐食记录的数据集，采用TFIDF和BERT文本嵌入技术，结合本体论、成分解析器和宏量营养素含量等营养领域知识增强信息，评估逻辑回归和多层感知机分类器的性能

Result: 即使没有领域知识增强，机器学习分类器也优于用户的自我评估；最佳组合（机器学习分类器+领域知识增强）获得了更高的准确率。包含解析成分、食物实体和宏量营养素信息的增强方法在多个营养目标上表现良好

Conclusion: 机器学习可以利用非结构化的自由文本饮食日志，可靠地分类膳食是否符合特定营养目标，性能超过自我评估，特别是在结合营养领域知识时。这显示了机器学习分析患者生成健康数据在精准医疗中支持以患者为中心的营养指导的潜力

Abstract: This study examined the use of machine learning and domain specific
enrichment on patient generated health data, in the form of free text meal
logs, to classify meals on alignment with different nutritional goals. We used
a dataset of over 3000 meal records collected by 114 individuals from a
diverse, low income community in a major US city using a mobile app. Registered
dietitians provided expert judgement for meal to goal alignment, used as gold
standard for evaluation. Using text embeddings, including TFIDF and BERT, and
domain specific enrichment information, including ontologies, ingredient
parsers, and macronutrient contents as inputs, we evaluated the performance of
logistic regression and multilayer perceptron classifiers using accuracy,
precision, recall, and F1 score against the gold standard and self assessment.
Even without enrichment, ML outperformed self assessments of individuals who
logged meals, and the best performing combination of ML classifier with
enrichment achieved even higher accuracies. In general, ML classifiers with
enrichment of Parsed Ingredients, Food Entities, and Macronutrients information
performed well across multiple nutritional goals, but there was variability in
the impact of enrichment and classification algorithm on accuracy of
classification for different nutritional goals. In conclusion, ML can utilize
unstructured free text meal logs and reliably classify whether meals align with
specific nutritional goals, exceeding self assessments, especially when
incorporating nutrition domain knowledge. Our findings highlight the potential
of ML analysis of patient generated health data to support patient centered
nutrition guidance in precision healthcare.

</details>


### [89] [A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs](https://arxiv.org/abs/2509.06332)
*Roussel Rahman,Aashwin Ananda Mishra*

Main category: cs.LG

TL;DR: LLM在数值推理方面存在局限性，虽然能处理确定性算法问题，但在需要启发式搜索的组合数学谜题上表现不佳，表明其数值推理更像是模式匹配而非真正的分析思维。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在数值推理方面的鲁棒性，特别是探究它们在从基础运算到复杂组合谜题等不同难度数学问题上的表现。

Method: 测试多个最先进的LLM智能体在100个问题挑战集上的表现，问题分为四个类别：基础算术、高级运算、素数检测和24点数字谜题。

Result: 智能体在前三个需要确定性算法执行的类别上准确率很高，但在需要启发式搜索的24点谜题上持续失败。

Conclusion: LLM的数值推理能力主要局限于回忆和执行已知算法，而非生成性解决问题，其表现更像是复杂的模式匹配而非灵活的分析思维。

Abstract: Large Language Models (LLMs) have demonstrated remarkable emergent
capabilities, yet the robustness of their numerical reasoning remains an open
question. While standard benchmarks evaluate LLM reasoning on complex problem
sets using aggregated metrics, they often obscure foundational weaknesses. In
this work, we probe LLM mathematical numeracy by evaluating performance on
problems of escalating complexity, from constituent operations to combinatorial
puzzles. We test several state-of-the-art LLM-based agents on a 100-problem
challenge comprising four categories: (1) basic arithmetic, (2) advanced
operations, (3) primality checking, and (4) the Game of 24 number puzzle. Our
results show that while the agents achieved high accuracy on the first three
categories, which require deterministic algorithmic execution, they
consistently failed at the number puzzle, underlining its demand for a
heuristic search over a large combinatorial space to be a significant
bottleneck. These findings reveal that the agents' proficiency is largely
confined to recalling and executing known algorithms, rather than performing
generative problem-solving. This suggests their apparent numerical reasoning is
more akin to sophisticated pattern-matching than flexible, analytical thought,
limiting their potential for tasks that require novel or creative numerical
insights.

</details>


### [90] [Ban&Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs](https://arxiv.org/abs/2509.06346)
*Yuanteng Chen,Peisong Wang,Yuantian Shao,Jian Cheng*

Main category: cs.LG

TL;DR: Ban&Pick是一种无需重新训练的后训练策略，通过识别和强化关键专家、动态修剪冗余专家，提升稀疏混合专家模型的性能和推理速度


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的路由器在预训练中过早收敛并强制平衡使用，导致关键专家利用不足和冗余专家过多，限制了模型性能和效率

Method: Ban&Pick包含两个组件：Pick发现并强化对性能影响大的关键专家群体；Ban根据层和token敏感度动态修剪冗余专家

Result: 在DeepSeek和Qwen3等细粒度MoE-LLM上，AIME2024准确率从80.67提升到84.66，GPQA-Diamond从65.66提升到68.18，推理速度提升1.25倍

Conclusion: Ban&Pick提供了一种即插即用的后训练解决方案，无需重新训练或架构修改即可获得免费的性能提升和推理加速

Abstract: Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling
large language models (LLMs) efficiently. Recent fine-grained MoE designs
introduce hundreds of experts per layer, with multiple experts activated per
token, enabling stronger specialization. However, during pre-training, routers
are optimized mainly for stability and robustness: they converge prematurely
and enforce balanced usage, limiting the full potential of model performance
and efficiency. In this work, we uncover two overlooked issues: (i) a few
highly influential experts are underutilized due to premature and balanced
routing decisions; and (ii) enforcing a fixed number of active experts per
token introduces substantial redundancy. Instead of retraining models or
redesigning MoE architectures, we introduce Ban&Pick, a post-training,
plug-and-play strategy for smarter MoE routing. Pick discovers and reinforces
key experts-a small group with outsized impact on performance-leading to
notable accuracy gains across domains. Ban complements this by dynamically
pruning redundant experts based on layer and token sensitivity, delivering
faster inference with minimal accuracy loss. Experiments on fine-grained
MoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks
demonstrate that Ban&Pick delivers free performance gains and inference
acceleration without retraining or architectural changes. For instance, on
Qwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from
65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the
vLLM.

</details>


### [91] [Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment](https://arxiv.org/abs/2509.06371)
*Victor Guyomard,Mathis Mauvisseau,Marie Paindavoine*

Main category: cs.LG

TL;DR: 论文通过SafetyCore案例研究，揭示了设备端AI模型的安全风险，展示了模型提取和操纵如何绕过敏感内容检测


<details>
  <summary>Details</summary>
Motivation: 随着设备端AI部署增加，虽然提升了隐私和降低了延迟，但带来了不同于传统软件的安全风险，需要研究这些新型威胁

Method: 通过Android系统服务SafetyCore的真实案例研究，分析设备端AI模型的安全漏洞，演示模型提取和操纵技术

Result: 成功展示了如何提取和操纵设备端AI模型来绕过敏感图像内容检测，使保护机制失效

Conclusion: 设备端AI模型存在严重安全漏洞，攻击者可以实际利用这些漏洞，需要加强设备端AI系统的安全防护

Abstract: Due to hardware and software improvements, an increasing number of AI models
are deployed on-device. This shift enhances privacy and reduces latency, but
also introduces security risks distinct from traditional software. In this
article, we examine these risks through the real-world case study of
SafetyCore, an Android system service incorporating sensitive image content
detection. We demonstrate how the on-device AI model can be extracted and
manipulated to bypass detection, effectively rendering the protection
ineffective. Our analysis exposes vulnerabilities of on-device AI models and
provides a practical demonstration of how adversaries can exploit them.

</details>


### [92] [Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection](https://arxiv.org/abs/2509.06383)
*Hyungjoon Soh,Dongha Lee,Vipul Periwal,Junghyo Jo*

Main category: cs.LG

TL;DR: 本文重新审视并改进了基于统计物理的变分门控(VG)方法，通过自动微分技术增强其可扩展性和效率，在高度稀疏场景下比Ridge和LASSO回归表现更优，并能识别关键变量数量。


<details>
  <summary>Details</summary>
Motivation: 在大数据时代，从高维数据中选择关键变量变得越来越重要。稀疏回归通过促进模型简洁性和可解释性为此提供了强大工具。

Method: 改进基于统计物理的变分门控(VG)方法，引入显式特征选择自旋变量，利用变分推理推导可处理的损失函数，并整合现代自动微分技术实现可扩展的优化。

Result: VG在高度稀疏场景下表现优异，比Ridge和LASSO回归提供更一致和稳健的变量选择。发现了一个尖锐的过渡点：当引入多余变量时，泛化能力急剧下降，选择变量的不确定性增加。

Conclusion: VG在稀疏建模方面具有强大潜力，适用于压缩感知和机器学习中的模型剪枝等广泛应用场景，过渡点提供了估计相关变量数量的实用信号。

Abstract: Selecting key variables from high-dimensional data is increasingly important
in the era of big data. Sparse regression serves as a powerful tool for this
purpose by promoting model simplicity and explainability. In this work, we
revisit a valuable yet underutilized method, the statistical physics-based
Variational Garrote (VG), which introduces explicit feature selection spin
variables and leverages variational inference to derive a tractable loss
function. We enhance VG by incorporating modern automatic differentiation
techniques, enabling scalable and efficient optimization. We evaluate VG on
both fully controllable synthetic datasets and complex real-world datasets. Our
results demonstrate that VG performs especially well in highly sparse regimes,
offering more consistent and robust variable selection than Ridge and LASSO
regression across varying levels of sparsity. We also uncover a sharp
transition: as superfluous variables are admitted, generalization degrades
abruptly and the uncertainty of the selection variables increases. This
transition point provides a practical signal for estimating the correct number
of relevant variables, an insight we successfully apply to identify key
predictors in real-world data. We expect that VG offers strong potential for
sparse modeling across a wide range of applications, including compressed
sensing and model pruning in machine learning.

</details>


### [93] [Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting](https://arxiv.org/abs/2509.06385)
*Senhao Liu,Zhiyu Guo,Zhiyuan Ji,Yueguo Chen,Yateng Tang,Yunhai Wang,Xuehao Zheng,Xiang Ao*

Main category: cs.LG

TL;DR: 提出了多粒度知识蒸馏框架MGKD，通过整合服务中用户行为数据来提升服务前风险评估性能，使用教师-学生模型架构和软标签传递知识。


<details>
  <summary>Details</summary>
Motivation: 传统金融风险管理将服务前风险评估和服务中违约检测分开建模，缺乏有效整合。需要利用服务中行为数据来提升服务前风险预测能力。

Method: 采用知识蒸馏框架，教师模型基于历史服务中数据训练，指导学生模型（基于服务前数据）。引入多粒度蒸馏策略（粗粒度、细粒度和自蒸馏）来对齐表示和预测，并使用重加权策略缓解类别不平衡问题。

Result: 在腾讯移动支付的大规模真实数据集上，离线和在线实验都证明了该方法的有效性。

Conclusion: MGKD框架成功地将服务中用户行为知识迁移到服务前风险评估中，通过多粒度蒸馏策略显著提升了预测性能，为金融风险管理提供了新的解决方案。

Abstract: Typical financial risk management involves distinct phases for pre-service
risk assessment and in-service default detection, often modeled separately.
This paper proposes a novel framework, Multi-Granularity Knowledge Distillation
(abbreviated as MGKD), aimed at improving pre-service risk prediction through
the integration of in-service user behavior data. MGKD follows the idea of
knowledge distillation, where the teacher model, trained on historical
in-service data, guides the student model, which is trained on pre-service
data. By using soft labels derived from in-service data, the teacher model
helps the student model improve its risk prediction prior to service
activation. Meanwhile, a multi-granularity distillation strategy is introduced,
including coarse-grained, fine-grained, and self-distillation, to align the
representations and predictions of the teacher and student models. This
approach not only reinforces the representation of default cases but also
enables the transfer of key behavioral patterns associated with defaulters from
the teacher to the student model, thereby improving the overall performance of
pre-service risk assessment. Moreover, we adopt a re-weighting strategy to
mitigate the model's bias towards the minority class. Experimental results on
large-scale real-world datasets from Tencent Mobile Payment demonstrate the
effectiveness of our proposed approach in both offline and online scenarios.

</details>


### [94] [NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network Executables](https://arxiv.org/abs/2509.06402)
*Yilin Li,Guozhu Meng,Mingyang Sun,Yanzhong Wang,Kun Sun,Hailong Chang,Yuekang Li*

Main category: cs.LG

TL;DR: NeuroDeX是一个基于LLM和动态分析的DNN可执行文件反编译器，能够处理编译优化和量化模型，实现操作符类型识别、属性恢复和模型重建。


<details>
  <summary>Details</summary>
Motivation: 设备端深度学习模型需求广泛，但编译后的可执行文件面临逆向工程威胁。现有反编译方法难以处理编译优化和量化编译模型。

Method: 利用LLM的语义理解能力和动态分析技术，准确高效地进行操作符类型识别、操作符属性恢复和模型重建。

Result: 在12个常见DNN模型的96个可执行文件上测试，非量化可执行文件可反编译为近乎相同的高级模型，量化可执行文件可恢复功能相似模型，平均top-1准确率达72%。

Conclusion: NeuroDeX相比之前的DNN可执行文件反编译器提供了更全面有效的解决方案，能够处理编译优化、不同架构和量化编译模型。

Abstract: On-device deep learning models have extensive real world demands. Deep
learning compilers efficiently compile models into executables for deployment
on edge devices, but these executables may face the threat of reverse
engineering. Previous studies have attempted to decompile DNN executables, but
they face challenges in handling compilation optimizations and analyzing
quantized compiled models. In this paper, we present NeuroDeX to unlock diverse
support in decompiling DNN executables. NeuroDeX leverages the semantic
understanding capabilities of LLMs along with dynamic analysis to accurately
and efficiently perform operator type recognition, operator attribute recovery
and model reconstruction. NeuroDeX can recover DNN executables into high-level
models towards compilation optimizations, different architectures and quantized
compiled models. We conduct experiments on 96 DNN executables across 12 common
DNN models. Extensive experimental results demonstrate that NeuroDeX can
decompile non-quantized executables into nearly identical high-level models.
NeuroDeX can recover functionally similar high-level models for quantized
executables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more
comprehensive and effective solution compared to previous DNN executables
decompilers.

</details>


### [95] [CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup](https://arxiv.org/abs/2509.06419)
*Xudong Mou,Rui Wang,Tiejun Wang,Renyu Yang,Shiru Chen,Jie Sun,Tianyu Wo,Xudong Liu*

Main category: cs.LG

TL;DR: CAPMix是一个可控异常增强框架，通过CutAddPaste机制和标签修正策略解决现有异常假设方法中的碎片化生成和异常偏移问题，在多个基准数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有异常假设方法存在两个根本限制：碎片化生成导致异常注入过于简单或不连贯，以及异常偏移问题使合成异常与真实异常分布不匹配，从而扭曲分类边界。

Method: 提出CAPMix框架：1) CutAddPaste机制进行目标性异常注入；2) 标签修正策略自适应优化异常标签；3) 在时序卷积网络中使用双空间mixup实现更平滑的决策边界。

Result: 在AIOps、UCR、SWaT、WADI和ESA五个基准数据集上的广泛实验表明，CAPMix相比最先进基线方法取得了显著改进，并对受污染训练数据具有更强的鲁棒性。

Conclusion: CAPMix通过可控的异常增强方法有效解决了时序异常检测中的碎片化生成和异常偏移问题，为标签稀缺场景下的异常检测提供了有效解决方案。

Abstract: Time series anomaly detection (TSAD) is a vital yet challenging task,
particularly in scenarios where labeled anomalies are scarce and temporal
dependencies are complex. Recent anomaly assumption (AA) approaches alleviate
the lack of anomalies by injecting synthetic samples and training
discriminative models. Despite promising results, these methods often suffer
from two fundamental limitations: patchy generation, where scattered anomaly
knowledge leads to overly simplistic or incoherent anomaly injection, and
Anomaly Shift, where synthetic anomalies either resemble normal data too
closely or diverge unrealistically from real anomalies, thereby distorting
classification boundaries. In this paper, we propose CAPMix, a controllable
anomaly augmentation framework that addresses both issues. First, we design a
CutAddPaste mechanism to inject diverse and complex anomalies in a targeted
manner, avoiding patchy generation. Second, we introduce a label revision
strategy to adaptively refine anomaly labels, reducing the risk of anomaly
shift. Finally, we employ dual-space mixup within a temporal convolutional
network to enforce smoother and more robust decision boundaries. Extensive
experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and
ESA, demonstrate that CAPMix achieves significant improvements over
state-of-the-art baselines, with enhanced robustness against contaminated
training data. The code is available at https://github.com/alsike22/CAPMix.

</details>


### [96] [CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction](https://arxiv.org/abs/2509.06465)
*Hongzong Li,Jiahao Ma,Zhanpeng Shi,Fanming Jin,Ye-Fan Hu,Jian-Dong Huang*

Main category: cs.LG

TL;DR: CAME-AB是一个用于抗体结合位点预测的新型跨模态注意力框架，采用混合专家(MoE)架构，整合五种生物模态特征，通过自适应模态融合和对比学习实现优异的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单视图特征，无法有效识别抗原上的抗体特异性结合位点，存在表示和预测的双重局限性。

Method: 整合五种生物模态特征（氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征、GCN精化生化图），采用自适应模态融合模块、Transformer编码器+MoE模块、监督对比学习和随机权重平均。

Result: 在基准抗体-抗原数据集上，CAME-AB在精确率、召回率、F1分数、AUC-ROC和MCC等多个指标上持续优于强基线方法。

Conclusion: 该框架通过多模态特征整合和先进的架构设计，显著提升了抗体结合位点预测的性能，消融研究验证了各组件有效性。

Abstract: Antibody binding site prediction plays a pivotal role in computational
immunology and therapeutic antibody design. Existing sequence or structure
methods rely on single-view features and fail to identify antibody-specific
binding sites on the antigens-a dual limitation in representation and
prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention
framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding
site prediction. CAME-AB integrates five biologically grounded modalities,
including raw amino acid encodings, BLOSUM substitution profiles, pretrained
language model embeddings, structure-aware features, and GCN-refined
biochemical graphs-into a unified multimodal representation. To enhance
adaptive cross-modal reasoning, we propose an adaptive modality fusion module
that learns to dynamically weight each modality based on its global relevance
and input-specific contribution. A Transformer encoder combined with an MoE
module further promotes feature specialization and capacity expansion. We
additionally incorporate a supervised contrastive learning objective to
explicitly shape the latent space geometry, encouraging intra-class compactness
and inter-class separability. To improve optimization stability and
generalization, we apply stochastic weight averaging during training. Extensive
experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB
consistently outperforms strong baselines on multiple metrics, including
Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further
validate the effectiveness of each architectural component and the benefit of
multimodal feature integration. The model implementation details and the codes
are available on https://anonymous.4open.science/r/CAME-AB-C525

</details>


### [97] [DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT](https://arxiv.org/abs/2509.06483)
*Guanjie Cheng,Boyi Li,Peihan Wu,Feiyi Chen,Xinkui Zhao,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: DyC-STG是一个用于物联网数据可信度分析的动态因果时空图网络，通过事件驱动的动态图模块和因果推理模块解决了传统STG模型在静态拓扑和伪相关性问题上的局限性。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器产生大量时空数据流，但数据可信度是未解决的关键挑战。传统时空图模型在动态、以人为中心的环境中存在两个根本限制：静态图拓扑无法捕捉物理动态，以及容易混淆伪相关性和真实因果关系。

Method: 提出了DyC-STG框架，包含两个协同模块：事件驱动的动态图模块（实时调整图拓扑以反映物理状态变化）和因果推理模块（通过严格强制执行时间优先性来提取因果感知表示）。

Result: 实验表明DyC-STG建立了新的最先进水平，比最强基线高出1.4个百分点，F1分数最高达到0.930。同时发布了两个新的真实世界数据集。

Conclusion: DyC-STG框架有效解决了物联网数据可信度分析中的动态拓扑和因果混淆问题，为相关领域研究提供了新的工具和数据集。

Abstract: The wide spreading of Internet of Things (IoT) sensors generates vast
spatio-temporal data streams, but ensuring data credibility is a critical yet
unsolved challenge for applications like smart homes. While spatio-temporal
graph (STG) models are a leading paradigm for such data, they often fall short
in dynamic, human-centric environments due to two fundamental limitations: (1)
their reliance on static graph topologies, which fail to capture physical,
event-driven dynamics, and (2) their tendency to confuse spurious correlations
with true causality, undermining robustness in human-centric environments. To
address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network
(DyC-STG), a novel framework designed for real-time data credibility analysis
in IoT. Our framework features two synergistic contributions: an event-driven
dynamic graph module that adapts the graph topology in real-time to reflect
physical state changes, and a causal reasoning module to distill causally-aware
representations by strictly enforcing temporal precedence. To facilitate the
research in this domain we release two new real-world datasets. Comprehensive
experiments show that DyC-STG establishes a new state-of-the-art, outperforming
the strongest baselines by 1.4 percentage points and achieving an F1-Score of
up to 0.930.

</details>


### [98] [A machine-learned expression for the excess Gibbs energy](https://arxiv.org/abs/2509.06484)
*Marco Hoffmann,Thomas Specht,Quirin Göttl,Jakob Burger,Stephan Mandt,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: 通过结合物理定律约束的神经网络模型HANNA，准确预测多组分混合物的过量吉布斯能量，显著超越现有方法


<details>
  <summary>Details</summary>
Motivation: 预测多组分混合物的过量吉布斯能量仅从分子结构出发是长期挑战，对化学工程和化学预测有重要意义

Method: 将物理定律作为硬约束集成到神经网络中，基于Dortmund数据库二元混合物数据训练，开发新的替代求解器包含液-液平衡数据，采用几何投影方法扩展到多组分混合物

Result: HANNA模型在准确性和适用范围上显著超越了现有标准方法，能够做出热力学一致的预测

Conclusion: 该方法成功解决了从分子结构预测多组分混合物过量吉布斯能量的问题，模型和代码开源可用

Abstract: The excess Gibbs energy plays a central role in chemical engineering and
chemistry, providing a basis for modeling the thermodynamic properties of
liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures
solely from the molecular structures of their components is a long-standing
challenge. In this work, we address this challenge by integrating physical laws
as hard constraints within a flexible neural network. The resulting model,
HANNA, was trained end-to-end on an extensive experimental dataset for binary
mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent
predictions. A novel surrogate solver developed in this work enabled the
inclusion of liquid-liquid equilibrium data in the training process.
Furthermore, a geometric projection method was applied to enable robust
extrapolations to multi-component mixtures, without requiring additional
parameters. We demonstrate that HANNA delivers excellent predictions, clearly
outperforming state-of-the-art benchmark methods in accuracy and scope. The
trained model and corresponding code are openly available, and an interactive
interface is provided on our website, MLPROP.

</details>


### [99] [QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients](https://arxiv.org/abs/2509.06516)
*Zongheng Guo,Tao Chen,Manuela Ferrario*

Main category: cs.LG

TL;DR: QualityFM是一个用于PPG和ECG生理信号质量评估的多模态基础模型，通过大规模预训练和双轨架构解决信号质量问题，在三个临床任务中表现出色


<details>
  <summary>Details</summary>
Motivation: ICU和手术室中PPG和ECG信号质量差、不完整和不一致的问题导致误报警和诊断错误，现有方法泛化性差、依赖大量标注数据且跨任务迁移能力弱

Method: 采用双轨架构处理不同质量的配对生理信号，使用自蒸馏策略，集成窗口稀疏注意力机制的Transformer模型，结合蒸馏损失和重建损失的复合损失函数

Result: 在超过2100万个30秒波形和17.9万小时数据上预训练，开发了三个参数规模不同的模型（9.6M到319M），在室性心动过速误报警、房颤识别和动脉血压估计三个临床任务中验证有效

Conclusion: QualityFM提供了一个通用的生理信号质量理解框架，解决了信号质量问题，具有很好的跨任务迁移能力和实际应用价值

Abstract: Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in
intesive care unit (ICU) and operating room (OR). However, the high incidence
of poor, incomplete, and inconsistent signal quality, can lead to false alarms
or diagnostic inaccuracies. The methods explored so far suffer from limited
generalizability, reliance on extensive labeled data, and poor cross-task
transferability. To overcome these challenges, we introduce QualityFM, a novel
multimodal foundation model for these physiological signals, designed to
acquire a general-purpose understanding of signal quality. Our model is
pre-trained on an large-scale dataset comprising over 21 million 30-second
waveforms and 179,757 hours of data. Our approach involves a dual-track
architecture that processes paired physiological signals of differing quality,
leveraging a self-distillation strategy where an encoder for high-quality
signals is used to guide the training of an encoder for low-quality signals. To
efficiently handle long sequential signals and capture essential local
quasi-periodic patterns, we integrate a windowed sparse attention mechanism
within our Transformer-based model. Furthermore, a composite loss function,
which combines direct distillation loss on encoder outputs with indirect
reconstruction loss based on power and phase spectra, ensures the preservation
of frequency-domain characteristics of the signals. We pre-train three models
with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy
and practical value through transfer learning on three distinct clinical tasks:
false alarm of ventricular tachycardia detection, the identification of atrial
fibrillation and the estimation of arterial blood pressure (ABP) from PPG and
ECG signals.

</details>


### [100] [Lane Change Intention Prediction of two distinct Populations using a Transformer](https://arxiv.org/abs/2509.06529)
*Francesco De Cristofaro,Cornelia Lex,Jia Hu,Arno Eichberger*

Main category: cs.LG

TL;DR: 这篇论文研究了变换器模型在不同地区数据集上进行车道更换意图预测的性能。发现单一数据集训练的模型在异地数据集上性能大幅下降，而多数据集同时训练可显著提升模型的演续性。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多在单一数据集上训练车道更换意图预测算法，缺乏对模型在不同人群和地区数据上演续性的验证。本文重点考察模型在不同地理区域数据集上的逆地化性能。

Method: 使用Transformer模型，在德国和香港两个不同地区收集的数据集上进行实验：1）在单一数据集训练并在另一数据集测试；2）同时在两个数据集上训练。

Result: 单一数据集训练的模型在异地数据集上准确率出现大幅下降（最低39.43%），而多数据集同时训练可将准确率提升至86.71%。

Conclusion: 车道更换意图预测模型的演续性很弱，单一数据源训练导致模型逆地化性能差。多数据集同时训练是提升模型通用性的有效方法。

Abstract: As a result of the growing importance of lane change intention prediction for
a safe and efficient driving experience in complex driving scenarios,
researchers have in recent years started to train novel machine learning
algorithms on available datasets with promising results. A shortcoming of this
recent research effort, though, is that the vast majority of the proposed
algorithms are trained on a single datasets. In doing so, researchers failed to
test if their algorithm would be as effective if tested on a different dataset
and, by extension, on a different population with respect to the one on which
they were trained. In this article we test a transformer designed for lane
change intention prediction on two datasets collected by LevelX in Germany and
Hong Kong. We found that the transformer's accuracy plummeted when tested on a
population different to the one it was trained on with accuracy values as low
as 39.43%, but that when trained on both populations simultaneously it could
achieve an accuracy as high as 86.71%. - This work has been submitted to the
IEEE for possible publication. Copyright may be transferred without notice,
after which this version may no longer be accessible.

</details>


### [101] [Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model](https://arxiv.org/abs/2509.06539)
*Duc Huy Le,Rolf Stadler*

Main category: cs.LG

TL;DR: 本文通过POMDP建立CAGE-2正式模型，提出基于PPO和粒子过滤的BF-PPO方法，在防御策略和训练时间方面超过了当前最佳方法CARDIFF。


<details>
  <summary>Details</summary>
Motivation: CAGE-2是计算机安全领域的标准测试环境，虽然有许多防御方法，但缺乏严格的正式模型和最优策略定义。需要一种更有效的方法来学习和评估防御策略。

Method: 使用部分可观测马尔可夫决策过程(POMDP)构建CAGE-2正式模型，并基于PPO算法开发BF-PPO方法，利用粒子过滤来处理大规模状态空间带来的计算复杂性。

Result: 在CAGE-2 CybORG环境中评估，BF-PPO方法在学习到的防御策略和所需训练时间方面都超过了当前排名榜首位的CARDIFF方法。

Conclusion: 通过POMDP模型化和BF-PPO方法，成功定义了CAGE-2的最优防御策略，并在性能和效率方面实现了显著提升，为网络安全领域提供了更有效的防御解决方案。

Abstract: CAGE-2 is an accepted benchmark for learning and evaluating defender
strategies against cyberattacks. It reflects a scenario where a defender agent
protects an IT infrastructure against various attacks. Many defender methods
for CAGE-2 have been proposed in the literature. In this paper, we construct a
formal model for CAGE-2 using the framework of Partially Observable Markov
Decision Process (POMDP). Based on this model, we define an optimal defender
strategy for CAGE-2 and introduce a method to efficiently learn this strategy.
Our method, called BF-PPO, is based on PPO, and it uses particle filter to
mitigate the computational complexity due to the large state space of the
CAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and
compare its performance with that of CARDIFF, the highest ranked method on the
CAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the
learned defender strategy and the required training time.

</details>


### [102] [Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder](https://arxiv.org/abs/2509.06540)
*John Tolladay,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 这篇论文开发了一种可解释性的监督变分自动编码器模型，用于根据孕期结果对肺心产程图信号进行分类，在保持竞争性预测性能的同时提供部分临床意义特征编码。


<details>
  <summary>Details</summary>
Motivation: 解决当前深度学习方法在肺心产程图信号分析中的可解释性限制问题，开发能够同时进行信号重建和结果预测的可解释模型。

Method: 使用OxMat CTG数据集训练监督VAE模型，对五分钟肺心率段落进行重建和结果预测优化，结合Kullback-Leibler散度和总相关约束来结构潜在空间。

Result: 模型在段落层面获得0.752 AUROC，在CTG层面获得0.779 AUROC。改善总相关约束同时提升了重建和分类性能。潜在分析显示基线相关特征得到了良好表征。

Conclusion: 监督VAE能够在肺儿结果预测中达到竞争性能力，并部分编码具有临床意义的特征，为未来可解释性生成模型奠定了基础。

Abstract: Objective: To develop and interpret a supervised variational autoencoder
(VAE) model for classifying cardiotocography (CTG) signals based on pregnancy
outcomes, addressing interpretability limits of current deep learning
approaches. Methods: The OxMat CTG dataset was used to train a VAE on
five-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes.
The model was optimised for signal reconstruction and outcome prediction,
incorporating Kullback-Leibler divergence and total correlation (TC)
constraints to structure the latent space. Performance was evaluated using area
under the receiver operating characteristic curve (AUROC) and mean squared
error (MSE). Interpretability was assessed using coefficient of determination,
latent traversals and unsupervised component analyses. Results: The model
achieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level,
where predicted scores were aggregated. Relaxing TC constraints improved both
reconstruction and classification. Latent analysis showed that baseline-related
features (e.g., FHR baseline, baseline shift) were well represented and aligned
with model scores, while metrics like short- and long-term variability were
less strongly encoded. Traversals revealed clear signal changes for baseline
features, while other properties were entangled or subtle. Unsupervised
decompositions corroborated these patterns. Findings: This work demonstrates
that supervised VAEs can achieve competitive fetal outcome prediction while
partially encoding clinically meaningful CTG features. The irregular,
multi-timescale nature of FHR signals poses challenges for disentangling
physiological components, distinguishing CTG from more periodic signals such as
ECG. Although full interpretability was not achieved, the model supports
clinically useful outcome prediction and provides a basis for future
interpretable, generative models.

</details>


### [103] [Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs](https://arxiv.org/abs/2509.06550)
*Jack Wilkie,Hanan Hindy,Christos Tachtatzis,Robert Atkinson*

Main category: cs.LG

TL;DR: 本文提出CLAN方法，通过将增广样本作为负面视图来表示潜在恶意分布，在网络入侵检测中实现了更高的分类准确性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 监督学习需要大量标签数据在实际应用中不实用，而异常检测方法存在偏异率高的问题。自监督学习技术虽然有所改善，但现有方法通过数据增广生成正面视图并将其他样本作为负面视图的方式存在限制。

Method: 提出CLAN方法，将增广样本作为负面视图（代表潜在恶意分布），而其他良性样本作为正面视图。通过对比学习方式学习良性流量的区别性隐层表征。

Result: 在Lycos2017数据集上的实验评估显示，该方法在二元分类任务中超越了现有的自监督学习和异常检测技术。在有限标签数据上微调后，该方法在多类分类任务中也表现出更优的性能。

Conclusion: CLAN方法通过创新地将增广样本作为负面视图，有效地提升了网络入侵检测的性能，为解决监督学习对大量标签数据的依赖和异常检测高偏异率问题提供了有效的解决方案。

Abstract: Network intrusion detection remains a critical challenge in cybersecurity.
While supervised machine learning models achieve state-of-the-art performance,
their reliance on large labelled datasets makes them impractical for many
real-world applications. Anomaly detection methods, which train exclusively on
benign traffic to identify malicious activity, suffer from high false positive
rates, limiting their usability. Recently, self-supervised learning techniques
have demonstrated improved performance with lower false positive rates by
learning discriminative latent representations of benign traffic. In
particular, contrastive self-supervised models achieve this by minimizing the
distance between similar (positive) views of benign traffic while maximizing it
between dissimilar (negative) views. Existing approaches generate positive
views through data augmentation and treat other samples as negative. In
contrast, this work introduces Contrastive Learning using Augmented Negative
pairs (CLAN), a novel paradigm for network intrusion detection where augmented
samples are treated as negative views - representing potentially malicious
distributions - while other benign samples serve as positive views. This
approach enhances both classification accuracy and inference efficiency after
pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset
demonstrates that the proposed method surpasses existing self-supervised and
anomaly detection techniques in a binary classification task. Furthermore, when
fine-tuned on a limited labelled dataset, the proposed approach achieves
superior multi-class classification performance compared to existing
self-supervised models.

</details>


### [104] [Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing](https://arxiv.org/abs/2509.06552)
*Zheqi Lv,Wenqiao Zhang,Kairui Fu,Qi Tian,Shengyu Zhang,Jiajie Su,Jingyuan Chen,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: Persona是一个无需反向传播的参数编辑框架，通过云端神经适配器生成参数编辑矩阵，动态调整设备端模型以适应实时数据分布变化，无需部署后重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决设备端实时数据分布变化对轻量级模型泛化能力的挑战，当前研究主要依赖数据密集和计算昂贵的微调方法，无法满足实时需求。

Method: 使用基于原型的参数编辑框架，云端神经适配器根据实时设备数据生成参数编辑矩阵，将模型聚类为原型模型，并通过跨层知识转移实现多层参数一致调整。

Result: 在多个视觉任务和推荐任务数据集上的广泛实验证实了Persona的有效性和通用性。

Conclusion: Persona提供了一种高效、无需重新训练的方法来增强设备端模型对实时数据分布变化的适应能力，具有良好的泛化性能。

Abstract: The on-device real-time data distribution shift on devices challenges the
generalization of lightweight on-device models. This critical issue is often
overlooked in current research, which predominantly relies on data-intensive
and computationally expensive fine-tuning approaches. To tackle this, we
introduce Persona, a novel personalized method using a prototype-based,
backpropagation-free parameter editing framework to enhance model
generalization without post-deployment retraining. Persona employs a neural
adapter in the cloud to generate a parameter editing matrix based on real-time
device data. This matrix adeptly adapts on-device models to the prevailing data
distributions, efficiently clustering them into prototype models. The
prototypes are dynamically refined via the parameter editing matrix,
facilitating efficient evolution. Furthermore, the integration of cross-layer
knowledge transfer ensures consistent and context-aware multi-layer parameter
changes and prototype assignment. Extensive experiments on vision task and
recommendation task on multiple datasets confirm Persona's effectiveness and
generality.

</details>


### [105] [AI for Scientific Discovery is a Social Problem](https://arxiv.org/abs/2509.06580)
*Georgia Channing,Avijit Ghosh*

Main category: cs.LG

TL;DR: 人工智能在科学发现中的应用存在社会和制度性障碍，包括社区功能失调、研究优先项目错位、数据分散和基础设施不平等。需要通过社区建设、跨学科教育等方式来解决。


<details>
  <summary>Details</summary>
Motivation: 虽然人工智能有期待加速科学发现，但其希望与实际影响存在差距，主要障碍是社会和制度性的，而非技术性的。

Method: 分析了四个相互联系的挑战：社区功能失调、研究优先项目与上游需求不匹配、数据分散、基础设施不平等，并归因于文化和组织实践。

Result: 识别了主要障碍并提出解决方向，包括意向性社区建设、跨学科教育、共享标准和可访问基础设施。

Conclusion: 应将AI科学重构为一个集体社会项目，将可持续合作和公平参与视为技术进步的前提条件。

Abstract: Artificial intelligence promises to accelerate scientific discovery, yet its
benefits remain unevenly distributed. While technical obstacles such as scarce
data, fragmented standards, and unequal access to computation are significant,
we argue that the primary barriers are social and institutional. Narratives
that defer progress to speculative "AI scientists," the undervaluing of data
and infrastructure contributions, misaligned incentives, and gaps between
domain experts and machine learning researchers all constrain impact. We
highlight four interconnected challenges: community dysfunction, research
priorities misaligned with upstream needs, data fragmentation, and
infrastructure inequities. We argue that their roots lie in cultural and
organizational practices. Addressing them requires not only technical
innovation but also intentional community-building, cross-disciplinary
education, shared benchmarks, and accessible infrastructure. We call for
reframing AI for science as a collective social project, where sustainable
collaboration and equitable participation are treated as prerequisites for
technical progress.

</details>


### [106] [PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification](https://arxiv.org/abs/2509.06600)
*Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 这篇论文提出了图卷积神经网络(GCN)在动态图环境中的PAC-Bayesian统计学习理论分析，为归纳节点分类提供了新的概化界限和收敛条件。


<details>
  <summary>Details</summary>
Motivation: 实际图数据具有动态性，新节点不断添加、连接关系不断变化，而现有的转导学习理论框架无法充分模型这种时间演化和结构动态性。

Method: 采用PAC-Bayesian理论分析方法，将节点作为相互依赖且非同分布的数据点处理，推导了一层和两层GCN的新的概化界限。

Result: 推导出的概化界限明确考虑了数据依赖性和非稳定性的影响，并确立了在节点数量增加时概化间隔收敛到零的充分条件，发现两层GCN需要更强的图拓扑假设条件。

Conclusion: 这项工作为理解和改进动态图环境中GNN的概化性能力建立了理论基础。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in processing
graph-structured data across various applications. A critical aspect of
real-world graphs is their dynamic nature, where new nodes are continually
added and existing connections may change over time. Previous theoretical
studies, largely based on the transductive learning framework, fail to
adequately model such temporal evolution and structural dynamics. In this
paper, we presents a PAC-Bayesian theoretical analysis of graph convolutional
networks (GCNs) for inductive node classification, treating nodes as dependent
and non-identically distributed data points. We derive novel generalization
bounds for one-layer GCNs that explicitly incorporate the effects of data
dependency and non-stationarity, and establish sufficient conditions under
which the generalization gap converges to zero as the number of nodes
increases. Furthermore, we extend our analysis to two-layer GCNs, and reveal
that it requires stronger assumptions on graph topology to guarantee
convergence. This work establishes a theoretical foundation for understanding
and improving GNN generalization in dynamic graph environments.

</details>


### [107] [Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards](https://arxiv.org/abs/2509.06602)
*Noel Codella,Sam Preston,Hao Qiu,Leonardo Schettini,Wen-wai Yim,Mert Öz,Shrey Jain,Matthew P. Lungren,Thomas Osborne*

Main category: cs.LG

TL;DR: 基于LLM的医疗助手系统HAO自动生成病人摘要，TBFact框架评估摘要的完整性和简洁性，为分子诊疗团队提供可靠支持


<details>
  <summary>Details</summary>
Motivation: 传统手工编写病人摘要方式劳动密集、主观性强且容易遗漏关键信息，需要自动化解决方案提高效率和质量

Method: 开发HAO系统（大语言模型驱动的医疗动作器）协调多代理临床流程生成病人摘要，并提出TBFact模型作为判官框架评估摘要的完整性和简洁性

Result: 在脱标识化虫甲板讨论数据集上，系统捕获了94%高重要性信息（包含部分含义），在严格含义标准下TBFact回归率达到0.84

Conclusion: HAO和TBFact为分子诊疗团队提供了可靠、可扩展的支持基础，能够在不分享敏感临床数据的情况下实现本地化部署和评估

Abstract: Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology
specialists collaboratively assess complex patient cases to determine optimal
treatment strategies. A central element of this process is the patient summary,
typically compiled by a medical oncologist, radiation oncologist, or surgeon,
or their trained medical assistant, who distills heterogeneous medical records
into a concise narrative to facilitate discussion. This manual approach is
often labor-intensive, subjective, and prone to omissions of critical
information. To address these limitations, we introduce the Healthcare Agent
Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that
coordinates a multi-agent clinical workflow to generate accurate and
comprehensive patient summaries for MTBs. Evaluating predicted patient
summaries against ground truth presents additional challenges due to stylistic
variation, ordering, synonym usage, and phrasing differences, which complicate
the measurement of both succinctness and completeness. To overcome these
evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework
designed to assess the comprehensiveness and succinctness of generated
summaries. Using a benchmark dataset derived from de-identified tumor board
discussions, we applied TBFact to evaluate our Patient History agent. Results
show that the agent captured 94% of high-importance information (including
partial entailments) and achieved a TBFact recall of 0.84 under strict
entailment criteria. We further demonstrate that TBFact enables a data-free
evaluation framework that institutions can deploy locally without sharing
sensitive clinical data. Together, HAO and TBFact establish a robust foundation
for delivering reliable and scalable support to MTBs.

</details>


### [108] [Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors](https://arxiv.org/abs/2509.06608)
*Viacheslav Sinii,Nikita Balagansky,Yaroslav Aksenov,Vadim Kurochkin,Daniil Laptev,Gleb Gerasimov,Alexey Gorbatovski,Boris Shaposhnikov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 论文研究了推理训练如何通过轻量级导向向量重塑语言模型计算，发现最后一层导向向量表现为令牌替换偏置，而倒数第二层导向向量通过MLP和未嵌入层优先加权过程词和结构符号。


<details>
  <summary>Details</summary>
Motivation: 理解推理训练如何改变语言模型的计算机制，目前这方面的机制仍然不够清晰。

Method: 使用插入基础模型残差流的轻量级导向向量，通过强化学习目标训练，采用logit-lens读取、路径修补和电路分析等方法分析两个模型。

Result: 发现最后一层导向向量表现为集中在第一个生成令牌上的令牌替换偏置，倒数第二层导向向量通过MLP和未嵌入层优先加权过程词和结构符号。

Conclusion: 这些结果为解释推理训练引起的行为变化建立了一个原则性框架。

Abstract: The mechanisms by which reasoning training reshapes language-model
computations remain poorly understood. We study lightweight steering vectors
inserted into the base model's residual stream and trained with a
reinforcement-learning objective, which can match full fine-tuning performance
while retaining the interpretability of small, additive interventions. Using
logit-lens readouts, path patching, and circuit analyses, we analyze two models
and find: (i) the last-layer steering vector behaves like a token-substitution
bias concentrated on the first generated token, consistently boosting tokens
such as "To" and "Step"; and (ii) the penultimate-layer steering vector leaves
attention patterns largely unchanged and instead acts through the MLP and
unembedding, preferentially up-weighting process words and structure symbols.
These results establish a principled framework for interpreting the behavioral
changes induced by reasoning training.

</details>


### [109] [A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models](https://arxiv.org/abs/2509.06609)
*Junjun Pan,Yu Zheng,Yue Tan,Yixin Liu*

Main category: cs.LG

TL;DR: 本文对图异常检测中的泛化问题进行了系统性综述，分析了传统方法的局限性并提出了新的分类框架，为未来研究提供方向指导。


<details>
  <summary>Details</summary>
Motivation: 传统图异常检测方法假设训练和测试分布相同且针对特定任务，在现实场景中面临数据分布偏移和训练样本稀缺的挑战，需要提升模型的泛化能力。

Method: 通过追溯泛化在图异常检测中的演进历程，形式化问题设置并建立系统分类法，对现有广义图异常检测方法进行全面综述。

Result: 建立了细粒度的分类框架，对当前广义图异常检测方法进行了最新且全面的回顾，为领域研究提供了系统性理解。

Conclusion: 识别了当前开放挑战并提出了未来研究方向，旨在激发这一新兴领域的进一步发展，推动图异常检测模型的泛化能力提升。

Abstract: Graph anomaly detection (GAD) has attracted increasing attention in recent
years for identifying malicious samples in a wide range of graph-based
applications, such as social media and e-commerce. However, most GAD methods
assume identical training and testing distributions and are tailored to
specific tasks, resulting in limited adaptability to real-world scenarios such
as shifting data distributions and scarce training samples in new applications.
To address the limitations, recent work has focused on improving the
generalization capability of GAD models through transfer learning that
leverages knowledge from related domains to enhance detection performance, or
developing "one-for-all" GAD foundation models that generalize across multiple
applications. Since a systematic understanding of generalization in GAD is
still lacking, in this paper, we provide a comprehensive review of
generalization in GAD. We first trace the evolution of generalization in GAD
and formalize the problem settings, which further leads to our systematic
taxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive
review is conducted for the existing generalized GAD methods. Finally, we
identify current open challenges and suggest future directions to inspire
future research in this emerging field.

</details>


### [110] [BEAM: Brainwave Empathy Assessment Model for Early Childhood](https://arxiv.org/abs/2509.06620)
*Chen Xie,Gaofeng Wu,Kaidong Wang,Zihao Zhu,Xiaoshu Luo,Yan Liang,Feiyu Quan,Ruoxi Wu,Xianghui Huang,Han Zhang*

Main category: cs.LG

TL;DR: 提出BEAM深度学习框架，利用多视角EEG信号预测4-6岁儿童共情水平，通过时空特征提取、特征融合和对比学习模块，在CBCP数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统共情预测方法依赖主观报告和观察标签，存在偏见且无法客观捕捉共情形成过程；现有EEG方法主要提取静态模式，忽略时间动态性。

Method: BEAM框架包含三个核心组件：1)LaBraM编码器进行时空特征提取；2)特征融合模块整合多视角信号信息；3)对比学习模块增强类别分离。

Result: 在CBCP数据集上验证，BEAM在多个指标上超越最先进方法，展示了客观共情评估的潜力。

Conclusion: BEAM为儿童共情水平提供了客观评估工具，并为早期干预儿童亲社会发展提供了初步见解。

Abstract: Empathy in young children is crucial for their social and emotional
development, yet predicting it remains challenging. Traditional methods often
only rely on self-reports or observer-based labeling, which are susceptible to
bias and fail to objectively capture the process of empathy formation. EEG
offers an objective alternative; however, current approaches primarily extract
static patterns, neglecting temporal dynamics. To overcome these limitations,
we propose a novel deep learning framework, the Brainwave Empathy Assessment
Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM
leverages multi-view EEG signals to capture both cognitive and emotional
dimensions of empathy. The framework comprises three key components: 1) a
LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a
feature fusion module to integrate complementary information from multi-view
signals, and 3) a contrastive learning module to enhance class separation.
Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across
multiple metrics, demonstrating its potential for objective empathy assessment
and providing a preliminary insight into early interventions in children's
prosocial development.

</details>


### [111] [Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing](https://arxiv.org/abs/2509.06640)
*Yung-Fu Chen,Sen Lin,Anish Arora*

Main category: cs.LG

TL;DR: 通过深度神经网络学习本地路由策略，仅需单个图的少量数据样本即可学到可沿正分布的路由策略，解决全对路由问题。


<details>
  <summary>Details</summary>
Motivation: 解决在几何随机图中学习可沿正的本地路由策略的问题，以提高路由效率和可扩展性。

Method: 利用网络领域知识选择输入特征和设计策略函数，训练DNN学习路由策略，仅需单个图的少量数据样本。

Result: 学到了与Greedy Forwarding完全匹配的策略，以及新的GreedyTensile路由策略（使用距离和节点伸缩特征），几乎总是超过贪心转发。

Conclusion: 方法能够学习高性能可解释的路由策略，具有超低延迟运行特性，通过简单的线性动作实现符号解释。

Abstract: We propose a simple algorithm that needs only a few data samples from a
single graph for learning local routing policies that generalize across a rich
class of geometric random graphs in Euclidean metric spaces. We thus solve the
all-pairs near-shortest path problem by training deep neural networks (DNNs)
that let each graph node efficiently and scalably route (i.e., forward) packets
by considering only the node's state and the state of the neighboring nodes.
Our algorithm design exploits network domain knowledge in the selection of
input features and design of the policy function for learning an approximately
optimal policy. Domain knowledge also provides theoretical assurance that the
choice of a ``seed graph'' and its node data sampling suffices for
generalizable learning. Remarkably, one of these DNNs we train -- using
distance-to-destination as the only input feature -- learns a policy that
exactly matches the well-known Greedy Forwarding policy, which forwards packets
to the neighbor with the shortest distance to the destination. We also learn a
new policy, which we call GreedyTensile routing -- using both
distance-to-destination and node stretch as the input features -- that almost
always outperforms greedy forwarding. We demonstrate the explainability and
ultra-low latency run-time operation of Greedy Tensile routing by symbolically
interpreting its DNN in low-complexity terms of two linear actions.

</details>


### [112] [Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives](https://arxiv.org/abs/2509.06656)
*Yuanyuan Wu,Zhenlin Qin,Leizhen Wang,Xiaolei Ma,Zhenliang Ma*

Main category: cs.LG

TL;DR: 提出gcGAIL模型，通过利用乘客群体间的共享行为模式来提高个体出行行为建模效率，在准确性、泛化性和模式展示效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个体出行行为响应建模对城市交通监管和政策评估至关重要，但传统MDP方法数据需求高且面临数据量、时空覆盖和情境多样性等挑战。

Method: 提出群体效应增强的生成对抗模仿学习（gcGAIL）模型，利用乘客群体间的共享行为模式来提高建模效率。

Result: 实验结果表明gcGAIL在准确性、泛化性和模式展示效率方面优于AIRL、基线GAIL和条件GAIL等方法，对空间变化、数据稀疏性和行为多样性具有鲁棒性。

Conclusion: gcGAIL模型能够预测任何时间的个体行为响应，为个性化激励提供基础，以引导可持续行为改变（更好的激励时机选择）。

Abstract: Understanding and modeling individual travel behavior responses is crucial
for urban mobility regulation and policy evaluation. The Markov decision
process (MDP) provides a structured framework for dynamic travel behavior
modeling at the individual level. However, solving an MDP in this context is
highly data-intensive and faces challenges of data quantity, spatial-temporal
coverage, and situational diversity. To address these, we propose a
group-effect-enhanced generative adversarial imitation learning (gcGAIL) model
that improves the individual behavior modeling efficiency by leveraging shared
behavioral patterns among passenger groups. We validate the gcGAIL model using
a public transport fare-discount case study and compare against
state-of-the-art benchmarks, including adversarial inverse reinforcement
learning (AIRL), baseline GAIL, and conditional GAIL. Experimental results
demonstrate that gcGAIL outperforms these methods in learning individual travel
behavior responses to incentives over time in terms of accuracy,
generalization, and pattern demonstration efficiency. Notably, gcGAIL is robust
to spatial variation, data sparsity, and behavioral diversity, maintaining
strong performance even with partial expert demonstrations and underrepresented
passenger groups. The gcGAIL model predicts the individual behavior response at
any time, providing the basis for personalized incentives to induce sustainable
behavior changes (better timing of incentive injections).

</details>


### [113] [TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations](https://arxiv.org/abs/2509.06665)
*Xiaolu Fu,Ziyuan Bao,Eiman Kanjo*

Main category: cs.LG

TL;DR: TrajAware是一个基于强化学习的VANET路由框架，通过动作空间剪枝、图交叉注意力和轨迹感知预测来解决动态拓扑和部分观测问题，在边缘设备上实现高效路由。


<details>
  <summary>Details</summary>
Motivation: 车载自组织网络(VANET)在智能交通系统中至关重要，但由于动态拓扑、不完全观测和边缘设备资源有限，路由仍然具有挑战性。现有的强化学习方法通常假设固定图结构，在网络条件变化时需要重新训练，不适合在受限硬件上部署。

Method: TrajAware框架包含三个组件：(1)动作空间剪枝：减少冗余邻居选项同时保持两跳可达性，缓解维度灾难；(2)图交叉注意力：将剪枝后的邻居映射到全局图上下文，生成可泛化到不同网络规模的特征；(3)轨迹感知预测：利用历史路由和路口信息估计部分观测下的实时位置。

Result: 在SUMO模拟器中使用真实城市地图进行评估，采用留一城市出设置。结果显示TrajAware实现了接近最短路径和高投递率，同时保持适合受限边缘设备的效率，在完全和部分观测场景下均优于最先进的基线方法。

Conclusion: TrajAware为VANET中的边缘AI部署提供了一个有效的强化学习框架，能够在动态网络条件下实现高效路由，解决了现有方法在硬件约束环境中的局限性。

Abstract: Vehicular ad hoc networks (VANETs) are a crucial component of intelligent
transportation systems; however, routing remains challenging due to dynamic
topologies, incomplete observations, and the limited resources of edge devices.
Existing reinforcement learning (RL) approaches often assume fixed graph
structures and require retraining when network conditions change, making them
unsuitable for deployment on constrained hardware. We present TrajAware, an
RL-based framework designed for edge AI deployment in VANETs. TrajAware
integrates three components: (i) action space pruning, which reduces redundant
neighbour options while preserving two-hop reachability, alleviating the curse
of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to
the global graph context, producing features that generalise across diverse
network sizes; and (iii) trajectory-aware prediction, which uses historical
routes and junction information to estimate real-time positions under partial
observations. We evaluate TrajAware in the open-source SUMO simulator using
real-world city maps with a leave-one-city-out setup. Results show that
TrajAware achieves near-shortest paths and high delivery ratios while
maintaining efficiency suitable for constrained edge devices, outperforming
state-of-the-art baselines in both full and partial observation scenarios.

</details>


### [114] [Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation](https://arxiv.org/abs/2509.06694)
*Victor Toscano-Duran,Rocio Gonzalez-Diaz,Miguel A. Gutiérrez-Naranjo*

Main category: cs.LG

TL;DR: 提出了一种新型的小型浅层神经网络Barycentric Neural Network (BNN)，利用重心坐标和固定基点集来精确表示连续分段线性函数，并引入长度加权持久熵(LWPE)作为损失函数，在资源受限环境下实现快速且优越的函数逼近性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度或过参数化神经网络计算成本高，需要一种在资源受限环境下（如有限基点和训练轮次）仍能提供灵活且可解释的函数逼近方法。

Method: 提出BNN网络结构，利用固定基点集和重心坐标定义网络；引入长度加权持久熵(LWPE)作为稳定的拓扑特征；直接优化定义BNN的基点而非内部权重。

Result: 实验结果表明，该方法相比MSE、RMSE、MAE和log-cosh等经典损失函数，实现了更优越和更快的逼近性能。

Conclusion: BNN结合LWPE损失函数框架为资源受限环境下的非线性连续函数逼近提供了灵活且几何可解释的解决方案，具有计算效率高和性能优越的特点。

Abstract: While it is well-established that artificial neural networks are
\emph{universal approximators} for continuous functions on compact domains,
many modern approaches rely on deep or overparameterized architectures that
incur high computational costs. In this paper, a new type of \emph{small
shallow} neural network, called the \emph{Barycentric Neural Network} ($\BNN$),
is proposed, which leverages a fixed set of \emph{base points} and their
\emph{barycentric coordinates} to define both its structure and its parameters.
We demonstrate that our $\BNN$ enables the exact representation of
\emph{continuous piecewise linear functions} ($\CPLF$s), ensuring strict
continuity across segments. Since any continuous function over a compact domain
can be approximated arbitrarily well by $\CPLF$s, the $\BNN$ naturally emerges
as a flexible and interpretable tool for \emph{function approximation}. Beyond
the use of this representation, the main contribution of the paper is the
introduction of a new variant of \emph{persistent entropy}, a topological
feature that is stable and scale invariant, called the \emph{length-weighted
persistent entropy} ($\LWPE$), which is weighted by the lifetime of topological
features. Our framework, which combines the $\BNN$ with a loss function based
on our $\LWPE$, aims to provide flexible and geometrically interpretable
approximations of nonlinear continuous functions in resource-constrained
settings, such as those with limited base points for $\BNN$ design and few
training epochs. Instead of optimizing internal weights, our approach directly
\emph{optimizes the base points that define the $\BNN$}. Experimental results
show that our approach achieves \emph{superior and faster approximation
performance} compared to classical loss functions such as MSE, RMSE, MAE, and
log-cosh.

</details>


### [115] [Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks](https://arxiv.org/abs/2509.06701)
*Su Hyeong Lee,Risi Kondor,Richard Ngo*

Main category: cs.LG

TL;DR: 基于概率模型的智能代理理论，通过对数加权池化定义代理组合，证明在三种或更多结果空间中可实现严格一致性，并形式化了LLM中的代理对齐现象


<details>
  <summary>Details</summary>
Motivation: 为神经模型建立一种基于概率模型的智能代理理论，探索子代理如何聚集成涵盖的高层实体，以提供代理对齐AI系统的新见解

Method: 将代理表示为结果分布，以对数得分为认知效用，通过对数加权池化定义组合，并使用克隆不变性、连续性和开放性来构建递归结构

Result: 证明在线性池化或二元结果空间中不可能实现严格一致性，但在三种或更多结果空间中可能实现；形式化了LLM中的"卢易吉-瓦卢易吉"代理对齐现象，证明显现后压制策略比纯粹卢易吉强化能更大程度减少一阶对齐问题

Conclusion: 通过建立一种理论框架来理解子代理如何聚集成涵盖的高层实体，为代理对齐AI系统提供了新的理论基础和实践意义

Abstract: We develop a theory of intelligent agency grounded in probabilistic modeling
for neural models. Agents are represented as outcome distributions with
epistemic utility given by log score, and compositions are defined through
weighted logarithmic pooling that strictly improves every member's welfare. We
prove that strict unanimity is impossible under linear pooling or in binary
outcome spaces, but possible with three or more outcomes. Our framework admits
recursive structure via cloning invariance, continuity, and openness, while
tilt-based analysis rules out trivial duplication. Finally, we formalize an
agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent
persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a
manifest-then-suppress Waluigi strategy yields strictly larger first-order
misalignment reduction than pure Luigi reinforcement alone. These results
clarify how developing a principled mathematical framework for how subagents
can coalesce into coherent higher-level entities provides novel implications
for alignment in agentic AI systems.

</details>


### [116] [Nested Optimal Transport Distances](https://arxiv.org/abs/2509.06702)
*Ruben Bontorno,Songyan Hou*

Main category: cs.LG

TL;DR: 提出使用嵌套最优传输距离作为金融时间序列生成模型的评估指标，并开发了高效并行算法


<details>
  <summary>Details</summary>
Motivation: 金融时间序列模拟对于压力测试和决策制定至关重要，但目前缺乏统一的生成模型评估标准

Method: 采用时间因果的最优传输距离变体——嵌套最优传输距离，并提出统计一致且可并行化的计算算法

Result: 相比现有方法实现了显著的速度提升，该指标对套期保值、最优停止和强化学习等任务具有鲁棒性

Conclusion: 嵌套最优传输距离为金融时间序列生成AI提供了有效的评估框架，其高效算法支持实际应用

Abstract: Simulating realistic financial time series is essential for stress testing,
scenario generation, and decision-making under uncertainty. Despite advances in
deep generative models, there is no consensus metric for their evaluation. We
focus on generative AI for financial time series in decision-making
applications and employ the nested optimal transport distance, a time-causal
variant of optimal transport distance, which is robust to tasks such as
hedging, optimal stopping, and reinforcement learning. Moreover, we propose a
statistically consistent, naturally parallelizable algorithm for its
computation, achieving substantial speedups over existing approaches.

</details>


### [117] [RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms](https://arxiv.org/abs/2509.06714)
*Zakariae El Asri,Ibrahim Laiche,Clément Rambour,Olivier Sigaud,Nicolas Thome*

Main category: cs.LG

TL;DR: RT-HCP算法解决了模型强化学习在机器人控制中的推理延迟问题，通过动作序列预生成实现高频控制，在FURUTA摆实验中表现出优异的样本效率和实时性能


<details>
  <summary>Details</summary>
Motivation: 解决模型强化学习在机器人直接学习控制时面临的样本效率与推理时间矛盾，特别是推理延迟无法满足机器人高频控制需求的问题

Method: 提出通用框架处理推理延迟，让慢速推理控制器预生成动作序列供机器人平台连续执行；比较多种RL算法并开发RT-HCP算法

Result: RT-HCP在性能、样本效率和推理时间之间达到最佳平衡，在FURUTA摆高频平台上验证了其优越性

Conclusion: RT-HCP框架有效解决了机器人控制中的实时性挑战，为模型强化学习在实际机器人应用提供了实用解决方案

Abstract: Learning a controller directly on the robot requires extreme sample
efficiency. Model-based reinforcement learning (RL) methods are the most sample
efficient, but they often suffer from a too long inference time to meet the
robot control frequency requirements. In this paper, we address the sample
efficiency and inference time challenges with two contributions. First, we
define a general framework to deal with inference delays where the slow
inference robot controller provides a sequence of actions to feed the
control-hungry robotic platform without execution gaps. Then, we compare
several RL algorithms in the light of this framework and propose RT-HCP, an
algorithm that offers an excellent trade-off between performance, sample
efficiency and inference time. We validate the superiority of RT-HCP with
experiments where we learn a controller directly on a simple but high frequency
FURUTA pendulum platform. Code: github.com/elasriz/RTHCP

</details>


### [118] [Long-Range Graph Wavelet Networks](https://arxiv.org/abs/2509.06743)
*Filippo Guerranti,Fabrizio Forte,Simon Geisler,Stephan Günnemann*

Main category: cs.LG

TL;DR: LR-GWN通过将图小波滤波器分解为局部和全局组件，使用低阶多项式处理局部聚合，通过谱域参数化捕获长程交互，统一了短距离和长距离信息流，在长程基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决图机器学习中长程交互建模的挑战，现有基于小波的图神经网络依赖有限阶多项式近似，限制了感受野并阻碍长程传播。

Method: 将小波滤波器分解为互补的局部和全局组件：局部聚合使用高效低阶多项式处理，长程交互通过灵活的谱域参数化捕获。

Result: 在长程基准测试中达到基于小波方法的最先进性能，同时在短程数据集上保持竞争力。

Conclusion: LR-GWN的混合设计在原则性小波框架内统一了短距离和长距离信息流，有效解决了图长程交互建模问题。

Abstract: Modeling long-range interactions, the propagation of information across
distant parts of a graph, is a central challenge in graph machine learning.
Graph wavelets, inspired by multi-resolution signal processing, provide a
principled way to capture both local and global structures. However, existing
wavelet-based graph neural networks rely on finite-order polynomial
approximations, which limit their receptive fields and hinder long-range
propagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which
decompose wavelet filters into complementary local and global components. Local
aggregation is handled with efficient low-order polynomials, while long-range
interactions are captured through a flexible spectral domain parameterization.
This hybrid design unifies short- and long-distance information flow within a
principled wavelet framework. Experiments show that LR-GWN achieves
state-of-the-art performance among wavelet-based methods on long-range
benchmarks, while remaining competitive on short-range datasets.

</details>


### [119] [Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization](https://arxiv.org/abs/2509.06759)
*Thanh Thi Nguyen,Campbell Wilson,Janis Dalins*

Main category: cs.LG

TL;DR: 本文综述了使用深度强化学习(DRL)和直接偏好优化(DPO)技术来微调大型视觉语言模型(LVLMs)，以实现与人类价值观对齐、提升任务性能和自适应多模态交互的方法。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态理解方面取得显著进展，但如何通过微调使其与人类价值观对齐、适应特定任务和行为仍然是一个关键挑战。

Method: 探索深度强化学习(DRL)和直接偏好优化(DPO)两种范式：DRL通过奖励信号优化模型行为，DPO直接根据偏好数据对齐策略而无需显式奖励模型。

Result: 系统分类了关键方法，分析了偏好数据和奖励信号的来源，并讨论了这些技术在模型对齐、性能提升和多模态交互方面的应用潜力。

Conclusion: DRL和DPO为构建强大且与人类对齐的大型视觉语言模型提供了重要技术路径，但仍面临可扩展性、样本效率、持续学习、泛化能力和安全性等开放挑战。

Abstract: Large Vision-Language Models (LVLMs) or multimodal large language models
represent a significant advancement in artificial intelligence, enabling
systems to understand and generate content across both visual and textual
modalities. While large-scale pretraining has driven substantial progress,
fine-tuning these models for aligning with human values or engaging in specific
tasks or behaviors remains a critical challenge. Deep Reinforcement Learning
(DRL) and Direct Preference Optimization (DPO) offer promising frameworks for
this aligning process. While DRL enables models to optimize actions using
reward signals instead of relying solely on supervised preference data, DPO
directly aligns the policy with preferences, eliminating the need for an
explicit reward model. This overview explores paradigms for fine-tuning LVLMs,
highlighting how DRL and DPO techniques can be used to align models with human
preferences and values, improve task performance, and enable adaptive
multimodal interaction. We categorize key approaches, examine sources of
preference data, reward signals, and discuss open challenges such as
scalability, sample efficiency, continual learning, generalization, and safety.
The goal is to provide a clear understanding of how DRL and DPO contribute to
the evolution of robust and human-aligned LVLMs.

</details>


### [120] [Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks](https://arxiv.org/abs/2509.06777)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 提出异步消息传递框架解决GNN中的过度挤压问题，通过基于节点中心性的批量更新方式，在保持模型简洁性的同时提升长距离交互性能


<details>
  <summary>Details</summary>
Motivation: 传统GNN在需要长距离交互的任务中存在过度挤压问题，图重布线方法会破坏归纳偏置且导致信息损失，增加通道容量又会增加参数复杂度

Method: 提出模型无关的异步更新框架，每层基于节点中心性值创建节点批次，仅更新批次内节点特征，实现顺序信息处理避免同步压缩

Result: 在6个标准图数据集和2个长距离数据集上测试，图分类任务表现优异，REDDIT-BINARY和Peptides-struct分别提升5%和4%

Conclusion: 异步消息传递框架能有效缓解过度挤压问题，保持较高特征敏感度边界，在长距离任务中取得显著性能提升

Abstract: Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs when
tasks require long-range interactions. The problem arises from the presence of
bottlenecks that limit the propagation of messages among distant nodes.
Recently, graph rewiring methods modify edge connectivity and are expected to
perform well on long-range tasks. Yet, graph rewiring compromises the inductive
bias, incurring significant information loss in solving the downstream task.
Furthermore, increasing channel capacity may overcome information bottlenecks
but enhance the parameter complexity of the model. To alleviate these
shortcomings, we propose an efficient model-agnostic framework that
asynchronously updates node features, unlike traditional synchronous message
passing GNNs. Our framework creates node batches in every layer based on the
node centrality values. The features of the nodes belonging to these batches
will only get updated. Asynchronous message updates process information
sequentially across layers, avoiding simultaneous compression into
fixed-capacity channels. We also theoretically establish that our proposed
framework maintains higher feature sensitivity bounds compared to standard
synchronous approaches. Our framework is applied to six standard graph datasets
and two long-range datasets to perform graph classification and achieves
impressive performances with a $5\%$ and $4\%$ improvements on REDDIT-BINARY
and Peptides-struct, respectively.

</details>


### [121] [Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2509.06782)
*Vittorio Giammarino,Ruiqi Ni,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: 提出了基于物理信息的正则化损失方法Pi-HIQL，通过Eikonal偏微分方程在离线目标条件强化学习中引入几何归纳偏置，显著提升了长时程任务的性能和泛化能力


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习在实际应用中面临状态-动作空间覆盖有限和长时程任务泛化困难的挑战，需要新的方法来提升学习效果

Method: 基于Eikonal偏微分方程推导出物理信息正则化损失，将其与分层隐式Q学习(HIQL)结合，形成Pi-HIQL方法，在值函数学习中引入几何归纳偏置

Result: Pi-HIQL在缝合机制和大规模导航任务中表现出显著的性能提升和泛化能力改善

Conclusion: 物理信息正则化方法为离线目标条件强化学习提供了有效的解决方案，能够更好地对齐代价函数结构，提升算法性能

Abstract: Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise
for domains such as autonomous navigation and locomotion, where collecting
interactive data is costly and unsafe. However, it remains challenging in
practice due to the need to learn from datasets with limited coverage of the
state-action space and to generalize across long-horizon tasks. To improve on
these challenges, we propose a Physics-informed (Pi) regularized loss for value
learning, derived from the Eikonal Partial Differential Equation (PDE) and
which induces a geometric inductive bias in the learned value function. Unlike
generic gradient penalties that are primarily used to stabilize training, our
formulation is grounded in continuous-time optimal control and encourages value
functions to align with cost-to-go structures. The proposed regularizer is
broadly compatible with temporal-difference-based value learning and can be
integrated into existing Offline GCRL algorithms. When combined with
Hierarchical Implicit Q-Learning (HIQL), the resulting method, Physics-informed
HIQL (Pi-HIQL), yields significant improvements in both performance and
generalization, with pronounced gains in stitching regimes and large-scale
navigation tasks.

</details>


### [122] [\texttt{R$^\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World](https://arxiv.org/abs/2509.06786)
*Youbang Sun,Xiang Wang,Jie Fu,Chaochao Lu,Bowen Zhou*

Main category: cs.LG

TL;DR: 提出safe-by-coevolution新范式，通过R²AI框架实现AI安全与能力的协同进化，结合已知威胁抵抗和未知风险韧性


<details>
  <summary>Details</summary>
Motivation: 解决AI能力快速增长与安全进展滞后之间的差距，现有安全范式存在脆弱性和反应性等问题

Method: R²AI框架整合快速/慢速安全模型、安全风洞对抗模拟验证、持续反馈循环，实现安全与能力的协同进化

Result: 提出了一个可扩展的主动安全路径，能够应对动态环境中的近期漏洞和长期生存风险

Conclusion: 该框架为AI向AGI和ASI发展过程中维持持续安全提供了系统性解决方案

Abstract: In this position paper, we address the persistent gap between rapidly growing
AI capabilities and lagging safety progress. Existing paradigms divide into
``Make AI Safe'', which applies post-hoc alignment and guardrails but remains
brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety
but struggles to address unforeseen risks in open-ended environments. We
therefore propose \textit{safe-by-coevolution} as a new formulation of the
``Make Safe AI'' paradigm, inspired by biological immunity, in which safety
becomes a dynamic, adversarial, and ongoing learning process. To operationalize
this vision, we introduce \texttt{R$^2$AI} -- \textit{Resistant and Resilient
AI} -- as a practical framework that unites resistance against known threats
with resilience to unforeseen risks. \texttt{R$^2$AI} integrates \textit{fast
and slow safe models}, adversarial simulation and verification through a
\textit{safety wind tunnel}, and continual feedback loops that guide safety and
capability to coevolve. We argue that this framework offers a scalable and
proactive path to maintain continual safety in dynamic environments, addressing
both near-term vulnerabilities and long-term existential risks as AI advances
toward AGI and ASI.

</details>


### [123] [floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL](https://arxiv.org/abs/2509.06863)
*Bhavya Agrawalla,Michal Nauman,Khush Agarwal,Aviral Kumar*

Main category: cs.LG

TL;DR: 该论文提出了floq方法，将强化学习中的Q函数参数化为流匹配的向量场，通过数值积分步骤实现迭代计算，相比传统单块架构能更好地扩展容量并提升性能


<details>
  <summary>Details</summary>
Motivation: 受现代大规模机器学习技术使用密集监督训练中间计算的启发，研究迭代计算在强化学习TD方法中的优势，传统方法以单块方式表示价值函数而没有迭代计算

Method: 引入floq方法，使用流匹配技术参数化Q函数为向量场，通过TD学习目标训练该向量场，利用目标向量场进行多步数值积分来引导学习

Result: 在具有挑战性的离线RL基准测试和在线微调任务中，floq将性能提升了近1.8倍，比标准TD学习架构具有更好的容量扩展能力

Conclusion: floq展示了迭代计算在价值学习中的潜力，通过适当设置积分步数可以实现对Q函数容量的更精细控制和扩展

Abstract: A hallmark of modern large-scale machine learning techniques is the use of
training objectives that provide dense supervision to intermediate
computations, such as teacher forcing the next token in language models or
denoising step-by-step in diffusion models. This enables models to learn
complex functions in a generalizable manner. Motivated by this observation, we
investigate the benefits of iterative computation for temporal difference (TD)
methods in reinforcement learning (RL). Typically they represent value
functions in a monolithic fashion, without iterative compute. We introduce floq
(flow-matching Q-functions), an approach that parameterizes the Q-function
using a velocity field and trains it using techniques from flow-matching,
typically used in generative modeling. This velocity field underneath the flow
is trained using a TD-learning objective, which bootstraps from values produced
by a target velocity field, computed by running multiple steps of numerical
integration. Crucially, floq allows for more fine-grained control and scaling
of the Q-function capacity than monolithic architectures, by appropriately
setting the number of integration steps. Across a suite of challenging offline
RL benchmarks and online fine-tuning tasks, floq improves performance by nearly
1.8x. floq scales capacity far better than standard TD-learning architectures,
highlighting the potential of iterative computation for value learning.

</details>


### [124] [Concolic Testing on Individual Fairness of Neural Network Models](https://arxiv.org/abs/2509.06864)
*Ming-I Huang,Chih-Duo Hong,Fang Yu*

Main category: cs.LG

TL;DR: PyFair是一个用于评估和验证深度神经网络个体公平性的形式化框架，通过改进PyCT工具生成公平性路径约束，采用双网络架构提供完整性保证，在25个基准模型上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度神经网络在关键领域中可能存在的歧视性问题，需要一种系统性的方法来评估和验证模型的个体公平性。

Method: 通过改进concolic测试工具PyCT，生成公平性特定的路径约束来系统探索DNN行为，采用双网络架构进行全面的公平性评估。

Result: 在25个基准模型上验证了PyFair的有效性，能够检测歧视性实例并验证公平性，但发现对复杂模型存在可扩展性挑战。

Conclusion: PyFair为预训练DNN的公平性测试和验证提供了一种严谨、系统的方法，推进了关键领域的算法公平性。

Abstract: This paper introduces PyFair, a formal framework for evaluating and verifying
individual fairness of Deep Neural Networks (DNNs). By adapting the concolic
testing tool PyCT, we generate fairness-specific path constraints to
systematically explore DNN behaviors. Our key innovation is a dual network
architecture that enables comprehensive fairness assessments and provides
completeness guarantees for certain network types. We evaluate PyFair on 25
benchmark models, including those enhanced by existing bias mitigation
techniques. Results demonstrate PyFair's efficacy in detecting discriminatory
instances and verifying fairness, while also revealing scalability challenges
for complex models. This work advances algorithmic fairness in critical domains
by offering a rigorous, systematic method for fairness testing and verification
of pre-trained DNNs.

</details>


### [125] [AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification](https://arxiv.org/abs/2509.06875)
*Sukumar Kishanthan,Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelSMOTE是一种基于智能体交互的新型过采样方法，通过文化传播模型解决传统方法在特征相关性、相似性控制和样本多样性方面的不足


<details>
  <summary>Details</summary>
Motivation: 传统过采样技术存在特征独立处理、缺乏相似性控制、样本多样性有限和合成样本管理不佳等问题，需要更有效的解决方案来处理类别不平衡问题

Method: 基于Axelrod文化传播模型，采用四种创新：(1)基于特征的分组保持相关性；(2)相似性概率交换机制；(3)Beta分布混合实现真实插值；(4)受控多样性注入避免过拟合

Result: 在8个不平衡数据集上的实验表明，AxelSMOTE在计算效率保持的同时，性能优于最先进的采样方法

Conclusion: AxelSMOTE通过智能体交互模型有效解决了传统过采样方法的局限性，为类别不平衡问题提供了更优的解决方案

Abstract: Class imbalance in machine learning poses a significant challenge, as skewed
datasets often hinder performance on minority classes. Traditional oversampling
techniques, which are commonly used to alleviate class imbalance, have several
drawbacks: they treat features independently, lack similarity-based controls,
limit sample diversity, and fail to manage synthetic variety effectively. To
overcome these issues, we introduce AxelSMOTE, an innovative agent-based
approach that views data instances as autonomous agents engaging in complex
interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE
implements four key innovations: (1) trait-based feature grouping to preserve
correlations; (2) a similarity-based probabilistic exchange mechanism for
meaningful interactions; (3) Beta distribution blending for realistic
interpolation; and (4) controlled diversity injection to avoid overfitting.
Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms
state-of-the-art sampling methods while maintaining computational efficiency.

</details>


### [126] [Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition](https://arxiv.org/abs/2509.06918)
*Tarhib Al Azad,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的粗筒外部分布(OOD)检测框架，专门处理训练标签含有噪声的情况，通过结合损失缩正和低秩稀疏分解技术显著提升了OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法在训练标签含有噪声时性能会显著降低，而直接结合标签噪声粗筒方法与OOD检测策略不能有效解决这一挑战，需要原理性的解决方案。

Method: 提出了一种粗筒OOD检测框架，将噪声标签学习中的损失缩正技术与信号处理中的低秩稀疏分解方法相结合。

Result: 在合成和真实数据集上进行了涉及广泛的实验，结果显示该方法在严重噪声标签设置下显著超过了现有最先进的OOD检测技术。

Conclusion: 该研究成功地解决了标签噪声对OOD检测性能的负面影响，为安全关键应用中的粗筒人工智能系统提供了有效的解决方案。

Abstract: Robust out-of-distribution (OOD) detection is an indispensable component of
modern artificial intelligence (AI) systems, especially in safety-critical
applications where models must identify inputs from unfamiliar classes not seen
during training. While OOD detection has been extensively studied in the
machine learning literature--with both post hoc and training-based
approaches--its effectiveness under noisy training labels remains
underexplored. Recent studies suggest that label noise can significantly
degrade OOD performance, yet principled solutions to this issue are lacking. In
this work, we demonstrate that directly combining existing label noise-robust
methods with OOD detection strategies is insufficient to address this critical
challenge. To overcome this, we propose a robust OOD detection framework that
integrates loss correction techniques from the noisy label learning literature
with low-rank and sparse decomposition methods from signal processing.
Extensive experiments on both synthetic and real-world datasets demonstrate
that our method significantly outperforms the state-of-the-art OOD detection
techniques, particularly under severe noisy label settings.

</details>


### [127] [Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding](https://arxiv.org/abs/2509.06923)
*Ziheng Li,Zexu Sun,Jinman Zhao,Erxue Min,Yongcheng Zeng,Hui Wu,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: SEELE是一个新颖的监督辅助RLVR框架，通过动态调整问题难度来提高强化学习效率，在数学推理基准上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在探索效率低下的问题，当问题难度与模型能力不匹配时，要么无法发现可行推理路径（问题太难），要么学习不到新能力（问题太简单）

Method: 提出SEELE框架，通过为每个训练样本附加可自适应调整长度的提示（部分完整解决方案），使用多轮抽样策略和项目反应理论模型来预测最优提示长度，实现实例级实时难度调整

Result: 在六个数学推理基准上，SEELE比GRPO和SFT分别高出11.8和10.5个百分点，比之前最好的监督辅助方法平均高出3.6个百分点

Conclusion: 通过动态调整问题难度使其与模型能力相匹配，SEELE有效提高了强化学习的探索效率，在复杂推理任务中表现出色

Abstract: Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable
success in enhancing the reasoning capabilities of large language models
(LLMs). However, existing RLVR methods often suffer from exploration
inefficiency due to mismatches between the training data's difficulty and the
model's capability. LLMs fail to discover viable reasoning paths when problems
are overly difficult, while learning little new capability when problems are
too simple. In this work, we formalize the impact of problem difficulty by
quantifying the relationship between loss descent speed and rollout accuracy.
Building on this analysis, we propose SEELE, a novel supervision-aided RLVR
framework that dynamically adjusts problem difficulty to stay within the
high-efficiency region. SEELE augments each training sample by appending a hint
(part of a full solution) after the original problem. Unlike previous
hint-based approaches, SEELE deliberately and adaptively adjusts the hint
length for each problem to achieve an optimal difficulty. To determine the
optimal hint length, SEELE employs a multi-round rollout sampling strategy. In
each round, it fits an item response theory model to the accuracy-hint pairs
collected in preceding rounds to predict the required hint length for the next
round. This instance-level, real-time difficulty adjustment aligns problem
difficulty with the evolving model capability, thereby improving exploration
efficiency. Experimental results show that SEELE outperforms Group Relative
Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5
points, respectively, and surpasses the best previous supervision-aided
approach by +3.6 points on average across six math reasoning benchmarks.

</details>


### [128] [Neutron Reflectometry by Gradient Descent](https://arxiv.org/abs/2509.06924)
*Max D. ~Champneys,Andrew J. ~Parnell,Philipp Gutfreund,Maximilian W. A. Skoda,. Patrick A. Fairclough,Timothy J. ~Rogers,Stephanie L. ~Burg*

Main category: cs.LG

TL;DR: 通过自动微分技术实现中子反射法的可微分前向模型，提高反射数据分析效率，保持物理直觉的同时可以利用现代优化算法。


<details>
  <summary>Details</summary>
Motivation: 中子反射法作为间接测量技术，需要解决复杂的逆向模型问题，传统优化方法效率低且不适合复杂多层结构。以往的机器学习模型彻底替代了物理方程，导致物理直觉丢失。

Method: 提出了一种新的可微分前向模型方法，利用自动微分技术计算错误函数对于参数的准确梯度，使用现代优化算法进行反射数据分析。

Result: 在氨化石英橡胶和有机LED多层设备两个案例中进行了测试，在复杂结构下展现了现有最佳性能和健壮的兼容性能。

Conclusion: 该方法为中子反射法数据分析提供了高效的梯度基优化方案，同时保留了物理直觉，并开源了可微分反射内核库。

Abstract: Neutron reflectometry (NR) is a powerful technique to probe surfaces and
interfaces. NR is inherently an indirect measurement technique, access to the
physical quantities of interest (layer thickness, scattering length density,
roughness), necessitate the solution of an inverse modelling problem, that is
inefficient for large amounts of data or complex multiplayer structures (e.g.
lithium batteries / electrodes). Recently, surrogate machine learning models
have been proposed as an alternative to existing optimisation routines.
Although such approaches have been successful, physical intuition is lost when
replacing governing equations with fast neural networks. Instead, we propose a
novel and efficient approach; to optimise reflectivity data analysis by
performing gradient descent on the forward reflection model itself. Herein,
automatic differentiation techniques are used to evaluate exact gradients of
the error function with respect to the parameters of interest. Access to these
quantities enables users of neutron reflectometry to harness a host of powerful
modern optimisation and inference techniques that remain thus far unexploited
in the context of neutron reflectometry. This paper presents two benchmark case
studies; demonstrating state-of-the-art performance on a thick oxide quartz
film, and robust co-fitting performance in the high complexity regime of
organic LED multilayer devices. Additionally, we provide an open-source library
of differentiable reflectometry kernels in the python programming language so
that gradient based approaches can readily be applied to other NR datasets.

</details>


### [129] [Learning words in groups: fusion algebras, tensor ranks and grokking](https://arxiv.org/abs/2509.06931)
*Maor Shutman,Oren Louidor,Ran Tessler*

Main category: cs.LG

TL;DR: 两层神经网络可以学习有限群中的任意词操作，并在学习过程中表现出grokking现象。通过将问题重构为学习低秩3-张量，网络能够找到低秩实现方式，使用有限宽度来近似词张量。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何学习群论中的词操作，特别是探索grokking现象背后的机制，以及网络如何利用有限宽度实现高效的张量近似。

Method: 将词操作学习问题重构为学习特定的3-张量，证明其通常具有低秩特性。通过分解群的基本自共轭表示的三元组，利用融合结构排除许多组件，找到低秩实现方式。

Result: 神经网络能够找到低秩实现或近似，使用有限宽度来一般化地近似词张量。对于简单乘法词操作，网络有效地实现了Strassen意义上的高效矩阵乘法。

Conclusion: 该研究揭示了神经网络在梯度下降下达到此类解决方案的机制，为理解神经网络学习群论操作和grokking现象提供了新的理论洞见。

Abstract: In this work, we demonstrate that a simple two-layer neural network with
standard activation functions can learn an arbitrary word operation in any
finite group, provided sufficient width is available and exhibits grokking
while doing so. To explain the mechanism by which this is achieved, we reframe
the problem as that of learning a particular $3$-tensor, which we show is
typically of low rank. A key insight is that low-rank implementations of this
tensor can be obtained by decomposing it along triplets of basic self-conjugate
representations of the group and leveraging the fusion structure to rule out
many components. Focusing on a phenomenologically similar but more tractable
surrogate model, we show that the network is able to find such low-rank
implementations (or approximations thereof), thereby using limited width to
approximate the word-tensor in a generalizable way. In the case of the simple
multiplication word, we further elucidate the form of these low-rank
implementations, showing that the network effectively implements efficient
matrix multiplication in the sense of Strassen. Our work also sheds light on
the mechanism by which a network reaches such a solution under gradient
descent.

</details>


### [130] [From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers](https://arxiv.org/abs/2509.06938)
*Praneet Suresh,Jack Stanley,Sonia Joseph,Luca Scimeca,Danilo Bzdok*

Main category: cs.LG

TL;DR: 本研究通过稀疏自编码器分析预训练transformer模型中的概念表示，揭示了输入不确定性增加时模型会激活与输入无关的语义特征导致幻觉，并发现纯噪声输入也能触发有意义的概念。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统在科学、商业和政府领域的普及，理解其失败模式（如幻觉现象）对于在高风险领域建立信任和采用至关重要。

Method: 使用稀疏自编码器捕捉预训练transformer模型中的概念表示，在输入空间具有实验控制不确定性的场景下系统分析幻觉产生机制。

Result: 研究发现随着输入信息越来越非结构化，transformer模型使用的语义概念数量增加；在输入不确定性增加时，模型容易激活连贯但输入不敏感的语义特征导致幻觉输出；纯噪声输入也能在中间激活中触发有意义的概念。

Conclusion: transformer内部处理机制的这些见解对AI模型与人类价值观对齐、AI安全、潜在对抗攻击以及自动量化模型幻觉风险提供了重要基础。

Abstract: As generative AI systems become competent and democratized in science,
business, and government, deeper insight into their failure modes now poses an
acute need. The occasional volatility in their behavior, such as the propensity
of transformer models to hallucinate, impedes trust and adoption of emerging AI
solutions in high-stakes areas. In the present work, we establish how and when
hallucinations arise in pre-trained transformer models through concept
representations captured by sparse autoencoders, under scenarios with
experimentally controlled uncertainty in the input space. Our systematic
experiments reveal that the number of semantic concepts used by the transformer
model grows as the input information becomes increasingly unstructured. In the
face of growing uncertainty in the input space, the transformer model becomes
prone to activate coherent yet input-insensitive semantic features, leading to
hallucinated output. At its extreme, for pure-noise inputs, we identify a wide
variety of robustly triggered and meaningful concepts in the intermediate
activations of pre-trained transformer models, whose functional integrity we
confirm through targeted steering. We also show that hallucinations in the
output of a transformer model can be reliably predicted from the concept
patterns embedded in transformer layer activations. This collection of insights
on transformer internal processing mechanics has immediate consequences for
aligning AI models with human values, AI safety, opening the attack surface for
potential adversarial attacks, and providing a basis for automatic
quantification of a model's hallucination risk.

</details>


### [131] [Outcome-based Exploration for LLM Reasoning](https://arxiv.org/abs/2509.06941)
*Yuda Song,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 强化学习虽然能提升大语言模型的推理准确性，但会导致生成多样性下降。本文提出基于结果的探索方法，通过历史探索和批量探索两种算法，在保持准确性的同时缓解多样性崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 传统的基于结果的强化学习方法虽然能显著提高大语言模型的推理准确性，但会系统性降低生成多样性，这在实际部署中严重影响性能，因为多样性对于测试时的扩展性至关重要。

Method: 提出基于结果的探索方法，包括两种互补算法：1) 历史探索：通过UCB风格的奖励鼓励罕见答案；2) 批量探索：惩罚批次内重复以促进测试时多样性。

Result: 在标准数学竞赛数据集上使用Llama和Qwen模型的实验表明，两种方法都能在提高准确性的同时缓解多样性崩溃问题。

Conclusion: 基于结果的探索方法为强化学习提供了一条实用路径，既能增强推理能力，又不会牺牲可扩展部署所必需的多样性。

Abstract: Reinforcement learning (RL) has emerged as a powerful method for improving
the reasoning abilities of large language models (LLMs). Outcome-based RL,
which rewards policies solely for the correctness of the final answer, yields
substantial accuracy gains but also induces a systematic loss in generation
diversity. This collapse undermines real-world performance, where diversity is
critical for test-time scaling. We analyze this phenomenon by viewing RL
post-training as a sampling process and show that, strikingly, RL can reduce
effective diversity even on the training set relative to the base model. Our
study highlights two central findings: (i) a transfer of diversity degradation,
where reduced diversity on solved problems propagates to unsolved ones, and
(ii) the tractability of the outcome space, since reasoning tasks admit only a
limited set of distinct answers. Motivated by these insights, we propose
outcome-based exploration, which assigns exploration bonuses according to final
outcomes. We introduce two complementary algorithms: historical exploration,
which encourages rarely observed answers via UCB-style bonuses, and batch
exploration, which penalizes within-batch repetition to promote test-time
diversity. Experiments on standard competition math with Llama and Qwen models
demonstrate that both methods improve accuracy while mitigating diversity
collapse. On the theoretical side, we formalize the benefit of outcome-based
exploration through a new model of outcome-based bandits. Together, these
contributions chart a practical path toward RL methods that enhance reasoning
without sacrificing the diversity essential for scalable deployment.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [132] [Cryo-EM as a Stochastic Inverse Problem](https://arxiv.org/abs/2509.05541)
*Diego Sanchez Espinosa,Erik H Thiede,Yunan Yang*

Main category: stat.ML

TL;DR: 提出了一种基于概率测度和Wasserstein梯度流的冷冻电镜重建方法，能够恢复生物分子的连续结构分布，解决了传统离散方法在处理结构异质性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜在生物分子成像中面临结构异质性的挑战，传统方法假设离散构象集合，无法有效恢复连续的结构变化。需要新的方法来建模和重建连续的结构分布。

Method: 将冷冻电镜重建表述为概率测度上的随机逆问题，使用KL散度和最大均值差异等统计距离最小化观测与模拟图像分布之间的变分差异，通过Wasserstein梯度流在概率测度空间进行优化，用粒子表示和演化构象集合。

Result: 在合成示例（包括真实蛋白质模型）中验证了该方法能够成功恢复结构状态的连续分布，分析了与MAP方法的联系，并提供了DTO方法收敛到无限维连续问题解的一致性分析。

Conclusion: 该方法为处理随机前向算子的逆问题提供了通用框架，不仅适用于冷冻电镜，还能推广到其他涉及随机前向算子的逆问题求解。

Abstract: Cryo-electron microscopy (Cryo-EM) enables high-resolution imaging of
biomolecules, but structural heterogeneity remains a major challenge in 3D
reconstruction. Traditional methods assume a discrete set of conformations,
limiting their ability to recover continuous structural variability. In this
work, we formulate cryo-EM reconstruction as a stochastic inverse problem (SIP)
over probability measures, where the observed images are modeled as the
push-forward of an unknown distribution over molecular structures via a random
forward operator. We pose the reconstruction problem as the minimization of a
variational discrepancy between observed and simulated image distributions,
using statistical distances such as the KL divergence and the Maximum Mean
Discrepancy. The resulting optimization is performed over the space of
probability measures via a Wasserstein gradient flow, which we numerically
solve using particles to represent and evolve conformational ensembles. We
validate our approach using synthetic examples, including a realistic protein
model, which demonstrates its ability to recover continuous distributions over
structural states. We analyze the connection between our formulation and
Maximum A Posteriori (MAP) approaches, which can be interpreted as instances of
the discretize-then-optimize (DTO) framework. We further provide a consistency
analysis, establishing conditions under which DTO methods, such as MAP
estimation, converge to the solution of the underlying infinite-dimensional
continuous problem. Beyond cryo-EM, the framework provides a general
methodology for solving SIPs involving random forward operators.

</details>


### [133] [Robust variational neural posterior estimation for simulation-based inference](https://arxiv.org/abs/2509.05724)
*Matthew O'Callaghan,Kaisey S. Mandel,Gerry Gilmore*

Main category: stat.ML

TL;DR: RVNP是一种新的神经后验估计方法，通过变分推理和误差建模来解决模拟器与真实数据生成过程不匹配的问题，在存在模型错误设定的情况下实现鲁棒的后验推断。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经密度估计的模拟推理方法在模型准确时表现良好，但在实际应用中模拟器总是与真实数据生成过程存在一定程度的偏差，导致后验估计性能下降。

Method: 提出鲁棒变分神经后验估计(RVNP)，结合变分推理和误差建模来弥合模拟与现实的差距，无需调整超参数或指定错误设定先验。

Result: 在多个基准任务（包括天文学真实数据）上测试表明，RVNP能够以数据驱动的方式恢复鲁棒的后验推断。

Conclusion: RVNP为解决模拟推理中的模型错误设定问题提供了一种有效方法，能够在不需要手动调整参数的情况下实现鲁棒的后验估计。

Abstract: Recent advances in neural density estimation have enabled powerful
simulation-based inference (SBI) methods that can flexibly approximate Bayesian
inference for intractable stochastic models. Although these methods have
demonstrated reliable posterior estimation when the simulator accurately
represents the underlying data generative process (GDP), recent work has shown
that they perform poorly in the presence of model misspecification. This poses
a significant problem for their use on real-world problems, due to simulators
always misrepresenting the true DGP to a certain degree. In this paper, we
introduce robust variational neural posterior estimation (RVNP), a method which
addresses the problem of misspecification in amortised SBI by bridging the
simulation-to-reality gap using variational inference and error modelling. We
test RVNP on multiple benchmark tasks, including using real data from
astronomy, and show that it can recover robust posterior inference in a
data-driven manner without adopting tunable hyperparameters or priors governing
the misspecification.

</details>


### [134] [Risk-averse Fair Multi-class Classification](https://arxiv.org/abs/2509.05771)
*Darinka Dentcheva,Xiangyu Tian*

Main category: stat.ML

TL;DR: 提出基于一致性风险度量和系统性风险理论的新分类框架，适用于多分类问题，特别针对噪声数据、样本稀缺和标签不可靠的情况。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在数据噪声大、样本稀缺且标签不可靠的多分类问题中表现不佳，需要开发更稳健的风险规避分类方法。

Method: 使用系统性风险模型和一致性风险度量，提出线性和核方法的扩展，以及基于系统理论方法的非线性聚合两阶段随机规划问题，并设计风险规避正则化分解算法求解。

Result: 数值实验表明，所提方法在不可靠训练数据下具有鲁棒性，在未知数据上表现优于最小化期望分类误差的方法，且随着类别数增加性能提升。

Conclusion: 基于系统性风险度量的分类框架能有效处理噪声和稀缺数据，提高分类性能，并能促进分类公平性。

Abstract: We develop a new classification framework based on the theory of coherent
risk measures and systemic risk. The proposed approach is suitable for
multi-class problems when the data is noisy, scarce (relative to the dimension
of the problem), and the labeling might be unreliable. In the first part of our
paper, we provide the foundation of the use of systemic risk models and show
how to apply it in the context of linear and kernel-based multi-class problems.
More advanced formulation via a system-theoretic approach with non-linear
aggregation is proposed, which leads to a two-stage stochastic programming
problem. A risk-averse regularized decomposition method is designed to solve
the problem. We use a popular multi-class method as a benchmark in the
performance analysis of the proposed classification methods. We illustrate our
ideas by proposing several generalization of that method by the use of coherent
measures of risk. The viability of the proposed risk-averse methods are
supported theoretically and numerically. Additionally, we demonstrate that the
application of systemic risk measures facilitates enforcing fairness in
classification. Analysis and experiments regarding the fairness of the proposed
models are carefully conducted. For all methods, our numerical experiments
demonstrate that they are robust in the presence of unreliable training data
and perform better on unknown data than the methods minimizing expected
classification errors. Furthermore, the performance improves when the number of
classes increases.

</details>


### [135] [Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery](https://arxiv.org/abs/2509.05775)
*Zilong Wang,Turgay Ayer,Shihao Yang*

Main category: stat.ML

TL;DR: 提出了一种基于因果森林学习核的新框架，通过聚类估计异质处理效应来识别对治疗响应不同的亚群体


<details>
  <summary>Details</summary>
Motivation: 估计异质处理效应对于个性化医疗、资源分配和政策评估至关重要，但现有聚类方法与因果推断的结合有限

Method: 两阶段方法：1) 使用Robinson分解通过正交化学习器估计去偏的条件平均处理效应(CATEs)，生成编码治疗响应相似性的核矩阵；2) 对核矩阵应用核化聚类来发现不同的治疗敏感亚群体并计算聚类级平均CATEs

Result: 在半合成和真实数据集上的广泛实验表明，该方法能有效捕捉有意义的治疗效应异质性

Conclusion: 该方法将核化聚类作为残差对残差回归框架中的一种正则化形式，成功揭示了潜在亚群体结构

Abstract: Estimating heterogeneous treatment effects is critical in domains such as
personalized medicine, resource allocation, and policy evaluation. A central
challenge lies in identifying subpopulations that respond differently to
interventions, thereby enabling more targeted and effective decision-making.
While clustering methods are well-studied in unsupervised learning, their
integration with causal inference remains limited. We propose a novel framework
that clusters individuals based on estimated treatment effects using a learned
kernel derived from causal forests, revealing latent subgroup structures. Our
approach consists of two main steps. First, we estimate debiased Conditional
Average Treatment Effects (CATEs) using orthogonalized learners via the
Robinson decomposition, yielding a kernel matrix that encodes sample-level
similarities in treatment responsiveness. Second, we apply kernelized
clustering to this matrix to uncover distinct, treatment-sensitive
subpopulations and compute cluster-level average CATEs. We present this
kernelized clustering step as a form of regularization within the
residual-on-residual regression framework. Through extensive experiments on
semi-synthetic and real-world datasets, supported by ablation studies and
exploratory analyses, we demonstrate the effectiveness of our method in
capturing meaningful treatment effect heterogeneity.

</details>


### [136] [Fisher Random Walk: Automatic Debiasing Contextual Preference Inference for Large Language Model Evaluation](https://arxiv.org/abs/2509.05852)
*Yichi Zhang,Alexander Belloni,Ethan X. Fang,Junwei Lu,Xiaoan Xu*

Main category: stat.ML

TL;DR: 基于希望获得更高效的评估器，这篇论文提出了一种现代化的半参数效率估计器，通过汇总比较图中的加权残差平衡条母来实现自动去偏。该方法采用Fisher随机游走策略获取权重，并通过潜在表达方法计算权重，以支持灵活的深度学习方法和多重偏好测试。


<details>
  <summary>Details</summary>
Motivation: 出于对大型语言模型进行严格且可扩展评估的需求，研究上下文偏好推断问题，特别是在不同域下的成对比较函数。

Method: 采用上下文Bradley-Terry-Luce模型，开发半参数效率估计器，通过汇总比较图中的加权残差平衡条母来实现自动去偏。使用Fisher随机游走策略获取权重，并通过潜在表达方法计算权重。扩展到多重偏好测试和分布偏移情况。

Result: 数值研究（包括语言模型在多样上下文中的评估）证实了该方法的准确性、效率和实际用途。

Conclusion: 该方法为大型语言模型提供了一种严格、可扩展且效率高的评估方法，能够处理灵活的深度学习方法和多重偏好测试，具有重要的实践价值。

Abstract: Motivated by the need for rigorous and scalable evaluation of large language
models, we study contextual preference inference for pairwise comparison
functionals of context-dependent preference score functions across domains.
Focusing on the contextual Bradley-Terry-Luce model, we develop a
semiparametric efficient estimator that automates the debiased estimation
through aggregating weighted residual balancing terms across the comparison
graph. We show that the efficiency is achieved when the weights are derived
from a novel strategy called Fisher random walk. We also propose a
computationally feasible method to compute the weights by a potential
representation of nuisance weight functions. We show our inference procedure is
valid for general score function estimators accommodating the practitioners'
need to implement flexible deep learning methods. We extend the procedure to
multiple hypothesis testing using a Gaussian multiplier bootstrap that controls
familywise error and to distributional shift via a cross-fitted
importance-sampling adjustment for target-domain inference. Numerical studies,
including language model evaluations under diverse contexts, corroborate the
accuracy, efficiency, and practical utility of our method.

</details>


### [137] [Uncertainty Quantification in Probabilistic Machine Learning Models: Theory, Methods, and Insights](https://arxiv.org/abs/2509.05877)
*Marzieh Ajirak,Anand Ravishankar,Petar M. Djuric*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Uncertainty Quantification (UQ) is essential in probabilistic machine
learning models, particularly for assessing the reliability of predictions. In
this paper, we present a systematic framework for estimating both epistemic and
aleatoric uncertainty in probabilistic models. We focus on Gaussian Process
Latent Variable Models and employ scalable Random Fourier Features-based
Gaussian Processes to approximate predictive distributions efficiently. We
derive a theoretical formulation for UQ, propose a Monte Carlo sampling-based
estimation method, and conduct experiments to evaluate the impact of
uncertainty estimation. Our results provide insights into the sources of
predictive uncertainty and illustrate the effectiveness of our approach in
quantifying the confidence in the predictions.

</details>


### [138] [Additive Distributionally Robust Ranking and Selection](https://arxiv.org/abs/2509.06147)
*Zaile Li,Yuchen Wan,L. Jeff Hong*

Main category: stat.ML

TL;DR: 本文提出了一个简单的加法分配(AA)程序，专注于采样k+m-1个关键场景，证明了其一致性和最强的可加性，并引入了通用加法分配(GAA)框架来提升实际性能。


<details>
  <summary>Details</summary>
Motivation: 解决分布鲁棒排序选择(DRR&S)中输入不确定性问题，现有方法缺乏理论支撑，对可加性结构和关键场景的理解不足。

Method: 提出AA程序专门采样k+m-1个假设关键场景，使用边界穿越论证建立正确选择概率下界；进一步开发GAA框架，模块化整合传统R&S采样规则。

Result: 证明AA具有一致性且实现最强可加性：只有k+m-1个场景被无限采样，非最优替代的最坏情况场景可能不在其中；数值实验验证理论发现和GAA的竞争性能。

Conclusion: 研究揭示了DRR&S中可加性结构的新颖且反直觉的见解，GAA框架在保持结构的同时提升了实际性能，为输入不确定性下的鲁棒决策提供了理论和方法支持。

Abstract: Ranking and selection (R&S) aims to identify the alternative with the best
mean performance among $k$ simulated alternatives. The practical value of R&S
depends on accurate simulation input modeling, which often suffers from the
curse of input uncertainty due to limited data. Distributionally robust ranking
and selection (DRR&S) addresses this challenge by modeling input uncertainty
via an ambiguity set of $m > 1$ plausible input distributions, resulting in
$km$ scenarios in total. Recent DRR&S studies suggest a key structural insight:
additivity in budget allocation is essential for efficiency. However, existing
justifications are heuristic, and fundamental properties such as consistency
and the precise allocation pattern induced by additivity remain poorly
understood. In this paper, we propose a simple additive allocation (AA)
procedure that aims to exclusively sample the $k + m - 1$ previously
hypothesized critical scenarios. Leveraging boundary-crossing arguments, we
establish a lower bound on the probability of correct selection and
characterize the procedure's budget allocation behavior. We then prove that AA
is consistent and, surprisingly, achieves additivity in the strongest sense: as
the total budget increases, only $k + m - 1$ scenarios are sampled infinitely
often. Notably, the worst-case scenarios of non-best alternatives may not be
among them, challenging prior beliefs about their criticality. These results
offer new and counterintuitive insights into the additive structure of DRR&S.
To improve practical performance while preserving this structure, we introduce
a general additive allocation (GAA) framework that flexibly incorporates
sampling rules from traditional R&S procedures in a modular fashion. Numerical
experiments support our theoretical findings and demonstrate the competitive
performance of the proposed GAA procedures.

</details>


### [139] [MOSAIC: Minimax-Optimal Sparsity-Adaptive Inference for Change Points in Dynamic Networks](https://arxiv.org/abs/2509.06303)
*Yingying Fan,Jingyuan Liu,Jinchi Lv,Ao Sun*

Main category: stat.ML

TL;DR: MOSAIC是一个用于动态网络变点检测的新推理框架，能够同时处理低秩和稀疏变化结构，建立了检测边界的最小最大率，并开发了基于特征分解的测试方法。


<details>
  <summary>Details</summary>
Motivation: 动态网络中同时存在低秩和稀疏变化结构，需要开发能够有效检测这种复杂变化模式的推理框架，并建立理论上的最优检测边界。

Method: 提出基于特征分解的测试方法，采用筛选信号技术，并通过残差技术调整理论测试，获得收敛于标准正态分布的枢轴统计量。

Result: MOSAIC方法在理论上接近最小最大率，仅存在较小的对数损失，在零假设下通过鞅中心极限定理收敛到标准正态分布，在备择假设下达到完全功效。

Conclusion: MOSAIC框架在动态网络变点检测中表现出色，理论结果通过模拟实验和实际数据应用得到验证，为复杂网络变化检测提供了有效解决方案。

Abstract: We propose a new inference framework, named MOSAIC, for change-point
detection in dynamic networks with the simultaneous low-rank and sparse-change
structure. We establish the minimax rate of detection boundary, which relies on
the sparsity of changes. We then develop an eigen-decomposition-based test with
screened signals that approaches the minimax rate in theory, with only a minor
logarithmic loss. For practical implementation of MOSAIC, we adjust the
theoretical test by a novel residual-based technique, resulting in a pivotal
statistic that converges to a standard normal distribution via the martingale
central limit theorem under the null hypothesis and achieves full power under
the alternative hypothesis. We also analyze the minimax rate of testing
boundary for dynamic networks without the low-rank structure, which almost
aligns with the results in high-dimensional mean-vector change-point inference.
We showcase the effectiveness of MOSAIC and verify our theoretical results with
several simulation examples and a real data application.

</details>


### [140] [Minimax optimal transfer learning for high-dimensional additive regression](https://arxiv.org/abs/2509.06308)
*Seung Hyun Moon*

Main category: stat.ML

TL;DR: 该论文研究高维可加回归在迁移学习框架下的应用，提出了一种基于平滑反向拟合估计器的目标专用估计方法和新颖的两阶段迁移学习方法，在重尾噪声条件下建立了理论误差界并达到了极小极大最优率。


<details>
  <summary>Details</summary>
Motivation: 研究高维可加回归在迁移学习环境下的估计问题，旨在利用辅助样本来提升目标模型的估计精度，特别是在处理重尾噪声分布时的理论挑战。

Method: 1) 基于局部线性平滑的平滑反向拟合估计器进行目标专用估计；2) 提出新颖的两阶段迁移学习方法，在总体和实证层面提供理论保证。

Result: 在亚韦布尔噪声条件下建立了通用误差界，在亚指数情况下达到极小极大下界；当辅助分布与目标分布足够接近时达到极小极大最优率；仿真研究和实际数据分析支持理论结果。

Conclusion: 该研究为高维可加回归的迁移学习提供了有效的估计方法和理论框架，特别是在重尾噪声条件下的理论突破，为实际应用提供了坚实的理论基础。

Abstract: This paper studies high-dimensional additive regression under the transfer
learning framework, where one observes samples from a target population
together with auxiliary samples from different but potentially related
regression models. We first introduce a target-only estimation procedure based
on the smooth backfitting estimator with local linear smoothing. In contrast to
previous work, we establish general error bounds under sub-Weibull($\alpha$)
noise, thereby accommodating heavy-tailed error distributions. In the
sub-exponential case ($\alpha=1$), we show that the estimator attains the
minimax lower bound under regularity conditions, which requires a substantial
departure from existing proof strategies. We then develop a novel two-stage
estimation method within a transfer learning framework, and provide theoretical
guarantees at both the population and empirical levels. Error bounds are
derived for each stage under general tail conditions, and we further
demonstrate that the minimax optimal rate is achieved when the auxiliary and
target distributions are sufficiently close. All theoretical results are
supported by simulation studies and real data analysis.

</details>


### [141] [Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination](https://arxiv.org/abs/2509.06575)
*Yian Huang,Yang Feng,Zhiliang Ying*

Main category: stat.ML

TL;DR: 提出了一种鲁棒自适应谱方法(RAS)，用于处理表示多任务学习中存在未知且可能大量任务污染的情况，无需预先知道污染水平或真实表示维度。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务学习方法假设干净或接近干净的环境，当污染严重时失效。实际应用中常受到污染、异常值或对抗性任务的阻碍。

Method: 开发了RAS方法，通过谱方法有效提取共享的inlier表示，适应inlier任务的异质性，不需要污染水平或真实维度的先验知识。

Result: 理论分析提供了非渐近误差界，保证RAS至少与单任务学习性能相当，防止负迁移。实验显示在高达80%任务污染下仍能保持优异性能。

Conclusion: RAS方法在存在大量任务污染的情况下仍能有效学习共享表示，具有鲁棒性和自适应性，可扩展到迁移学习场景。

Abstract: Representation-based multi-task learning (MTL) improves efficiency by
learning a shared structure across tasks, but its practical application is
often hindered by contamination, outliers, or adversarial tasks. Most existing
methods and theories assume a clean or near-clean setting, failing when
contamination is significant. This paper tackles representation MTL with an
unknown and potentially large contamination proportion, while also allowing for
heterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral
method (RAS) that can distill the shared inlier representation effectively and
efficiently, while requiring no prior knowledge of the contamination level or
the true representation dimension. Theoretically, we provide non-asymptotic
error bounds for both the learned representation and the per-task parameters.
These bounds adapt to inlier task similarity and outlier structure, and
guarantee that RAS performs at least as well as single-task learning, thus
preventing negative transfer. We also extend our framework to transfer learning
with corresponding theoretical guarantees for the target task. Extensive
experiments confirm our theory, showcasing the robustness and adaptivity of
RAS, and its superior performance in regimes with up to 80\% task
contamination.

</details>


### [142] [Automated Hierarchical Graph Construction for Multi-source Electronic Health Records](https://arxiv.org/abs/2509.06576)
*Yinjie Wang,Doudou Zhou,Yue Liu,Junwei Lu,Tianxi Cai*

Main category: stat.ML

TL;DR: MASH是一个全自动框架，通过神经最优传输和双曲嵌入来对齐异构医疗代码并构建层次图，解决EHR数据标准化问题


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHR)数据存在医疗代码异构性、机构特定术语和缺乏标准化结构等问题，限制了跨机构研究的可解释性和可扩展性

Method: 使用神经最优传输对齐医疗代码，结合预训练语言模型、共现模式、文本描述和监督标签构建层次图，学习双曲嵌入捕捉语义和层次关系

Result: 在真实EHR数据上生成可解释的层次图，首次为无结构本地实验室代码建立自动化层次结构

Conclusion: MASH框架有效解决了EHR数据异构性问题，为下游应用提供了基础参考

Abstract: Electronic Health Records (EHRs), comprising diverse clinical data such as
diagnoses, medications, and laboratory results, hold great promise for
translational research. EHR-derived data have advanced disease prevention,
improved clinical trial recruitment, and generated real-world evidence.
Synthesizing EHRs across institutions enables large-scale, generalizable
studies that capture rare diseases and population diversity, but remains
hindered by the heterogeneity of medical codes, institution-specific
terminologies, and the absence of standardized data structures. These barriers
limit the interpretability, comparability, and scalability of EHR-based
analyses, underscoring the need for robust methods to harmonize and extract
meaningful insights from distributed, heterogeneous data. To address this, we
propose MASH (Multi-source Automated Structured Hierarchy), a fully automated
framework that aligns medical codes across institutions using neural optimal
transport and constructs hierarchical graphs with learned hyperbolic
embeddings. During training, MASH integrates information from pre-trained
language models, co-occurrence patterns, textual descriptions, and supervised
labels to capture semantic and hierarchical relationships among medical
concepts more effectively. Applied to real-world EHR data, including diagnosis,
medication, and laboratory codes, MASH produces interpretable hierarchical
graphs that facilitate the navigation and understanding of heterogeneous
clinical data. Notably, it generates the first automated hierarchies for
unstructured local laboratory codes, establishing foundational references for
downstream applications.

</details>


### [143] [Sequential Least-Squares Estimators with Fast Randomized Sketching for Linear Statistical Models](https://arxiv.org/abs/2509.06856)
*Guan-Yu Chen,Xi Yang*

Main category: stat.ML

TL;DR: 提出了SLSE-FRS框架，首次将Sketch-and-Solve和Iterative-Sketching方法结合，通过迭代构建和求解带增大草图尺寸的LS子问题来逐步优化参数估计精度


<details>
  <summary>Details</summary>
Motivation: 解决大规模线性统计模型估计问题，需要开发既能保持计算效率又能提供高精度估计的方法

Method: 采用随机草图技术，迭代构建最小二乘子问题，逐步增加草图尺寸以提高估计精度

Result: 数值实验表明SLSE-FRS在性能上优于最先进的PCG和IDS方法

Conclusion: SLSE-FRS框架成功整合了两种草图方法，提供了高效且高精度的大规模线性模型估计解决方案

Abstract: We propose a novel randomized framework for the estimation problem of
large-scale linear statistical models, namely Sequential Least-Squares
Estimators with Fast Randomized Sketching (SLSE-FRS), which integrates
Sketch-and-Solve and Iterative-Sketching methods for the first time. By
iteratively constructing and solving sketched least-squares (LS) subproblems
with increasing sketch sizes to achieve better precisions, SLSE-FRS gradually
refines the estimators of the true parameter vector, ultimately producing
high-precision estimators. We analyze the convergence properties of SLSE-FRS,
and provide its efficient implementation. Numerical experiments show that
SLSE-FRS outperforms the state-of-the-art methods, namely the Preconditioned
Conjugate Gradient (PCG) method, and the Iterative Double Sketching (IDS)
method.

</details>


### [144] [Learning from one graph: transductive learning guarantees via the geometry of small random worlds](https://arxiv.org/abs/2509.06894)
*Nils Detering,Luca Galimberti,Anastasis Kratsios,Giulia Livieri,A. Martina Neuman*

Main category: stat.ML

TL;DR: 本文为图卷积网络的转导节点分类任务建立了统计学习理论基础，通过低维度量嵌入利用大图的几何规律性，提出了新的集中度量工具，并在随机图和确定性图上都提供了学习保证。


<details>
  <summary>Details</summary>
Motivation: 图卷积网络在转导节点分类中广泛应用，但统计学习理论基础有限，因为标准推理框架通常依赖多个独立样本而非单一图。本文旨在填补这一空白，为单图环境下的学习提供统计保证。

Method: 开发新的集中度量工具，利用低维度量嵌入捕捉大图的几何规律性。使用随机图模型捕获涌现的规律性，但方法也适用于确定性图。建立两个主要学习结果：一个针对任意确定性k顶点图，另一个针对具有特定几何性质的随机图。

Result: 为图卷积网络设置提供了学习保证，即使在标记节点数量很少的情况下也保持信息性，并随着标记节点数量增长达到最优非参数率O(N^{-1/2})。

Conclusion: 本文为转导节点分类建立了坚实的统计理论基础，提出的方法能够有效处理单图环境下的学习问题，为图卷积网络的实际应用提供了理论支持。

Abstract: Since their introduction by Kipf and Welling in $2017$, a primary use of
graph convolutional networks is transductive node classification, where missing
labels are inferred within a single observed graph and its feature matrix.
Despite the widespread use of the network model, the statistical foundations of
transductive learning remain limited, as standard inference frameworks
typically rely on multiple independent samples rather than a single graph. In
this work, we address these gaps by developing new concentration-of-measure
tools that leverage the geometric regularities of large graphs via
low-dimensional metric embeddings. The emergent regularities are captured using
a random graph model; however, the methods remain applicable to deterministic
graphs once observed. We establish two principal learning results. The first
concerns arbitrary deterministic $k$-vertex graphs, and the second addresses
random graphs that share key geometric properties with an Erd\H{o}s-R\'{e}nyi
graph $\mathbf{G}=\mathbf{G}(k,p)$ in the regime $p \in \mathcal{O}((\log
(k)/k)^{1/2})$. The first result serves as the basis for and illuminates the
second. We then extend these results to the graph convolutional network
setting, where additional challenges arise. Lastly, our learning guarantees
remain informative even with a few labelled nodes $N$ and achieve the optimal
nonparametric rate $\mathcal{O}(N^{-1/2})$ as $N$ grows.

</details>
