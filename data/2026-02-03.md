<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 28]
- [cs.LG](#cs.LG) [Total: 371]
- [stat.ML](#stat.ML) [Total: 35]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [JSR-GFNet: Jamming-to-Signal Ratio-Aware Dynamic Gating for Interference Classification in future Cognitive Global Navigation Satellite Systems](https://arxiv.org/abs/2602.00042)
*Zhihan Zeng,Hongyuan Shu,Kaihe Wang,Lu Chen,Amir Hussian,Yanjun Huang,Junchu Zhao,Yue Xiu,Zhongpei Zhang*

Main category: eess.SP

TL;DR: 提出JSR-GFNet多模态网络，结合IQ信号和STFT频谱图，通过动态门控机制解决GNSS干扰分类在低JSR下的性能下降和相位信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于时频分析和CNN的方法在低JSR下性能严重下降，且幅度谱丢失相位信息导致特征退化，使频谱相似的信号（如高阶QAM与带限高斯噪声）难以区分。

Method: 提出JSR-GFNet多模态架构，结合相位敏感的复数IQ样本和STFT频谱图。核心是物理启发的动态门控机制，基于统计信号描述符估计信号可靠性，动态重新加权复数ResNet（IQ流）和EfficientNet骨干（STFT流）的贡献。

Result: 在CGI-21数据集（21种干扰类别）上的实验表明，JSR-GFNet在10-50 dB JSR范围内实现更高准确率。可解释性分析确认模型学习到物理直观策略：在噪声受限区域优先考虑频谱能量整合，在高SNR场景转向相位精度以解决调制模糊。

Conclusion: 该框架为下一代航空航天导航安全提供了稳健解决方案，通过多模态融合和自适应门控机制有效解决了传统方法的局限性。

Abstract: The transition toward cognitive global navigation satellite system (GNSS) receivers requires accurate interference classification to trigger adaptive mitigation strategies. However, conventional methods relying on Time-Frequency Analysis (TFA) and Convolutional Neural Networks (CNNs) face two fundamental limitations: severe performance degradation in low Jamming-to-Signal Ratio (JSR) regimes due to noise obscuration, and ``feature degeneracy'' caused by the loss of phase information in magnitude-only spectrograms. Consequently, spectrally similar signals -- such as high-order Quadrature Amplitude Modulation versus Band-Limited Gaussian Noise -- become indistinguishable. To overcome these challenges, this paper proposes the \textbf{JSR-Guided Fusion Network (JSR-GFNet)}. This multi-modal architecture combines phase-sensitive complex In-Phase/Quadrature (IQ) samples with Short-Time Fourier Transform (STFT) spectrograms. Central to this framework is a physics-inspired dynamic gating mechanism driven by statistical signal descriptors. Acting as a conditional controller, it autonomously estimates signal reliability to dynamically reweight the contributions of a Complex-Valued ResNet (IQ stream) and an EfficientNet backbone (STFT stream). To validate the model, we introduce the Comprehensive GNSS Interference (CGI-21) dataset, simulating 21 jamming categories including software-defined waveforms from aerial platforms. Extensive experiments demonstrate that JSR-GFNet achieves higher accuracy across the full 10--50 dB JSR spectrum. Notably, interpretability analysis confirms that the model learns a physically intuitive strategy: prioritizing spectral energy integration in noise-limited regimes while shifting focus to phase precision in high-SNR scenarios to resolve modulation ambiguities. This framework provides a robust solution for next-generation aerospace navigation security.

</details>


### [2] [Experimental Validation of SBFD ISAC in an FR3 Distributed SIMO Testbed](https://arxiv.org/abs/2602.00054)
*Bixing Yan,Kwadwo Mensah Obeng Afrane,Achiel Colpaert,Andre Kokkeler,Sofie Pollin,Yang Miao*

Main category: eess.SP

TL;DR: 本文提出了一种子带全双工（SBFD）ISAC系统，通过将OFDM子带分配给感知和通信实现同时操作，在室内测试中验证了可行性，实现了0.145 m/s的速度分辨率和3.63e-3的误码率。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信（ISAC）是未来无线网络的关键技术，但传统系统存在频谱资源有限和干扰问题。需要一种能够在有限频谱资源下同时进行感知和通信的高效系统架构。

Method: 采用子带全双工（SBFD）架构，将2048个OFDM子载波划分为三个非重叠子带：两个用于感知（使用Zadoff-Chu序列），一个用于通信（使用QPSK）。构建分布式测试平台，使用三个USRP X410设备在6.8 GHz频率工作，每个设备传输一个子带同时接收所有三个子带，形成1×3 SIMO节点。通过主机-服务器协调实现时间同步，无需外部时钟分发。

Result: 室内测量验证了SBFD ISAC系统的可行性：单站感知达到0.145 m/s的速度分辨率，非视距通信条件下误码率为3.63e-3。与需要三倍频谱的多频带基准相比，SBFD配置在节省资源的同时实现了相当的测速精度。感知与通信性能的权衡主要取决于子载波分配策略而非相互干扰。

Conclusion: SBFD ISAC系统能够有效实现同时感知和通信，在有限频谱资源下提供良好性能。系统性能主要受子载波分配策略影响，而非感知与通信之间的相互干扰，为未来ISAC系统设计提供了实用方案。

Abstract: Integrated sensing and communication (ISAC) is a key enabler for future radio networks. This paper presents a sub-band full-duplex (SBFD) ISAC system that assigns non-overlapping OFDM subbands to sensing and communication, enabling simultaneous operation with minimal interference. A distributed testbed with three SIMO nodes is implemented using USRP X410 devices operating at 6.8 GHz with 20 MHz bandwidth per channel. A total of 2048 OFDM subcarriers are partitioned into three subbands: two for sensing using Zadoff-Chu sequences and one for communication using QPSK. Each USRP transmits one subband while receiving signals across all three, forming a 1 x 3 SIMO node. Time synchronization is achieved through host-server coordination without external clock distribution. Indoor measurements, validated against MOCAP ground truth, confirm the feasibility of the SBFD ISAC system. The results demonstrate monostatic sensing with a velocity resolution of 0.145 m/s, and communication under NLoS conditions with a BER of 3.63e-3. Compared with a multiband benchmark requiring three times more spectrum, the SBFD configuration achieves comparable velocity estimation accuracy while conserving resources. The sensing and communication performance trade-off is determined by subcarrier allocation strategy rather than mutual interference.

</details>


### [3] [Dual-Tier IRS-Assisted Mid-Band 6G Mobile Networks: Robust Beamforming and User Association](https://arxiv.org/abs/2602.00431)
*Muddasir Rahim,Soumaya Cherkaoui*

Main category: eess.SP

TL;DR: 提出了一种结合地面和空中智能可重构表面的新型框架，用于6G网络中FR3频段的资源分配，通过联合波束成形和用户关联优化，在低复杂度下实现接近穷举搜索的性能。


<details>
  <summary>Details</summary>
Motivation: 物联网应用的快速增长需要6G网络中的鲁棒资源分配，特别是在7-15GHz的FR3频段（6G的"黄金频段"）。现有研究局限于地面IRS和毫米波/太赫兹频段，无法应对严重的视距阻塞问题。

Method: 提出了结合地面IRS和空中IRS的新型框架，将联合波束成形和用户关联问题建模为混合整数非线性规划，通过问题分解、迫零波束成形和稳定匹配算法求解。

Result: 综合仿真表明，该方法在显著降低复杂度的情况下，性能接近穷举搜索，优于现有的贪婪和随机基准方法。

Conclusion: 该研究为实际6G部署提供了可扩展的蓝图，能够在挑战性环境中支持大规模物联网连接，特别是在FR3频段实现可靠的连接。

Abstract: The rapid growth of Internet of Things (IoT) applications necessitates robust resource allocation in future sixth-generation (6G) networks, particularly at the upper mid-band (7-15 GHz, FR3). This paper presents a novel intelligent reconfigurable surface (IRS)-assisted framework combining terrestrial IRS (TIRS) and aerial IRS (AIRS) mounted on low-altitude platform stations, to ensure reliable connectivity under severe line-of-sight (LoS) blockages. Distinguishing itself from prior work restricted to terrestrial IRS and mmWave and THz bands, this work targets the FR3 spectrum, the so-called Golden Band for 6G. The joint beamforming and user association (JBUA) problem is formulated as a mixed-integer nonlinear program (MINLP), solved through problem decomposition, zero-forcing beamforming, and a stable matching algorithm. Comprehensive simulations show our method approaches exhaustive search performance with significantly lower complexity, outperforming existing greedy and random baselines. These results provide a scalable blueprint for real-world 6G deployments, supporting massive IoT connectivity in challenging environments.

</details>


### [4] [Reliable IoT Communications in 6G Non-Terrestrial Networks with Dual RIS](https://arxiv.org/abs/2602.00438)
*Muddasir Rahim,Soumaya Cherkaoui*

Main category: eess.SP

TL;DR: 提出了一种基于两层RIS（地面RIS和高空平台RIS）的6G物联网通信框架，通过联合波束成形、功率分配和设备关联优化来最大化网络总速率，相比穷举搜索具有更低复杂度且性能接近。


<details>
  <summary>Details</summary>
Motivation: 物联网应用需求增长推动6G网络需要更鲁棒的资源分配，特别是在严重视距阻塞情况下需要确保可靠连接。

Method: 使用两层RIS结构（地面RIS和高空平台RIS），将联合波束成形、功率分配和物联网设备关联问题建模为MINLP，采用分解方法：ZF技术优化波束成形矩阵，推导闭式功率分配表达式，基于可达数据速率提出稳定匹配算法进行设备-RIS关联。

Result: 综合仿真表明，所提方案性能接近穷举搜索但复杂度显著降低，始终优于贪婪搜索和随机搜索基线，且收敛速度远快于穷举搜索方案。

Conclusion: 提出的两层RIS辅助通信框架能有效应对严重视距阻塞，通过分解优化方法在保证性能的同时大幅降低计算复杂度，为6G物联网网络提供了一种高效的资源分配解决方案。

Abstract: The increasing demand for Internet of Things (IoT) applications has accelerated the need for robust resource allocation in sixth-generation (6G) networks. In this paper, we propose a reconfigurable intelligent surface (RIS)-assisted upper mid-band communication framework. To ensure robust connectivity under severe line-of-sight (LoS) blockages, we use a two-tier RIS structure comprising terrestrial RISs (TRISs) and high-altitude platform station (HAPS)-mounted RISs (HRISs). To maximize network sum rate, we formulate a joint beamforming, power allocation, and IoT device association (JBPDA) problem as a mixed-integer nonlinear program (MINLP). The formulated MINLP problem is challenging to solve directly; therefore, we tackle it via a decomposition approach. The zero-forcing (ZF) technique is used to optimize the beamforming matrix, a closed-form expression for power allocation is derived, and a stable matching-based algorithm is proposed for device-RIS association based on achievable data rates. Comprehensive simulations demonstrate that the proposed scheme approaches the performance of exhaustive search (ES) while exhibiting substantially lower complexity, and it consistently outperforms greedy search (GS) and random search (RS) baselines. Moreover, the proposed scheme converges much faster than the ES scheme.

</details>


### [5] [Fronthaul-Efficient Distributed Cooperative 3D Positioning with Quantized Latent CSI Embeddings](https://arxiv.org/abs/2602.00664)
*Tong An,Jiwei Zhao,Jiayang Shi,Bin Zheng,Kai Yu,Maged Elkashlan,George K. Karagiannidis,Hongsheng Chen*

Main category: eess.SP

TL;DR: 提出一种基于学习的边云协同定位框架，在有限容量前传约束下，通过神经网络压缩CSI，实现密集城市NLOS环境中的高精度3D定位。


<details>
  <summary>Details</summary>
Motivation: 在密集城市非视距环境中，多基站协同定位需要传输原始CSI，但前传开销过大限制了实际可扩展性。需要解决有限容量前传下的高效协同定位问题。

Method: 采用边云协同架构：每个基站部署神经网络压缩本地CSI为量化表示（固定前传负载），中央单元联合处理多基站压缩CSI进行3D定位。采用两阶段训练策略：基站自监督本地训练和中央单元端到端联合训练。

Result: 在3.5GHz 5G NR城市射线追踪场景（6个基站，20MHz带宽）中，平均3D定位误差0.48m，90%分位误差0.83m，前传负载降至无损CSI传输的6.25%，性能接近全CSI交换的协同定位。

Conclusion: 提出的学习型边云协同定位框架在显著降低前传开销的同时，实现了接近全CSI交换的定位精度，为实际部署提供了可行的解决方案。

Abstract: High-precision three-dimensional (3D) positioning in dense urban non-line-of-sight (NLOS) environments benefits significantly from cooperation among multiple distributed base stations (BSs). However, forwarding raw CSI from multiple BSs to a central unit (CU) incurs prohibitive fronthaul overhead, which limits scalable cooperative positioning in practice. This paper proposes a learning-based edge-cloud cooperative positioning framework under limited-capacity fronthaul constraints. In the proposed architecture, a neural network is deployed at each BS to compress the locally estimated CSI into a quantized representation subject to a fixed fronthaul payload. The quantized CSI is transmitted to the CU, which performs cooperative 3D positioning by jointly processing the compressed CSI received from multiple BSs. The proposed framework adopts a two-stage training strategy consisting of self-supervised local training at the BSs and end-to-end joint training for positioning at the CU. Simulation results based on a 3.5~GHz 5G NR compliant urban ray-tracing scenario with six BSs and 20~MHz bandwidth show that the proposed method achieves a mean 3D positioning error of 0.48~m and a 90th-percentile error of 0.83~m, while reducing the fronthaul payload to 6.25% of lossless CSI forwarding. The achieved performance is close to that of cooperative positioning with full CSI exchange.

</details>


### [6] [CMANet: Channel-Masked Attention Network for Cooperative Multi-Base-Station 3D Positioning](https://arxiv.org/abs/2602.00696)
*Tong An,Huan Lu,Jiayang Shi,Kai Yu,Rongrong Zhu,Bin Zheng,Jiwei Zhao,Haibo Zhou*

Main category: eess.SP

TL;DR: CMANet是一种基于多基站协作的CSI定位架构，通过通道掩码注意力机制融合原始CSI特征，在5G NR城市环境中实现亚米级定位精度。


<details>
  <summary>Details</summary>
Motivation: 下一代无线系统需要实现无处不在的高精度定位，但在多径丰富的城市环境中仍然具有挑战性。利用CSI中嵌入的细粒度多径特征可以实现更可靠和精确的定位。

Method: 提出CMANet多基站协作定位架构，采用通道掩码注意力（CMA）机制进行原始CSI的特征级融合。CMA编码器将物理基础先验（每基站信道增益）注入注意力权重，强调可靠链路并抑制虚假多径。轻量级LSTM解码器将子载波视为序列，在频域积累证据，最终输出3D位置估计。

Result: 在典型的5G NR兼容城市仿真中，CMANet实现了小于0.5米的中值误差和1.0米的90%分位数误差，优于现有最先进基准方法。消融实验验证了CMA和频率积累的必要性。

Conclusion: CMANet具有边缘部署能力，体现了面向多基站CSI定位的集成感知与通信（ISAC）对齐的协作范式。

Abstract: Achieving ubiquitous high-accuracy localization is crucial for next-generation wireless systems, yet remains challenging in multipath-rich urban environments. By exploiting the fine-grained multipath characteristics embedded in channel state information (CSI), more reliable and precise localization can be achieved. To address this, we present CMANet, a multi-BS cooperative positioning architecture that performs feature-level fusion of raw CSI using the proposed Channel Masked Attention (CMA) mechanism. The CMA encoder injects a physically grounded prior--per-BS channel gain--into the attention weights, thus emphasizing reliable links and suppressing spurious multipath. A lightweight LSTM decoder then treats subcarriers as a sequence to accumulate frequency-domain evidence into a final 3D position estimate. In a typical 5G NR-compliant urban simulation, CMANet achieves less than 0.5m median error and 1.0m 90th-percentile error, outperforming state-of-the-art benchmarks. Ablations verify the necessity of CMA and frequency accumulation. CMANet is edge-deployable and exemplifies an Integrated Sensing and Communication (ISAC)-aligned, cooperative paradigm for multi-BS CSI positioning.

</details>


### [7] [Synthesized-Isotropic Narrowband Channel Parameter Extraction from Angle-Resolved Wideband Channel Measurements](https://arxiv.org/abs/2602.01646)
*Minseok Kim,Masato Yomoda*

Main category: eess.SP

TL;DR: 该论文重新审视了从角度分辨宽带测量中计算路径增益的技术挑战，提出了波束累积校正因子来补偿非正交扫描波束导致的功率估计偏差。


<details>
  <summary>Details</summary>
Motivation: 在毫米波和太赫兹频段，使用天线阵列或机械转向高增益天线进行角度分辨信道探测时，测量响应中嵌入了辐射方向图效应。为了提取天线无关的大尺度信道参数（如路径损耗、延迟扩展和角度扩展），需要对这些效应进行适当补偿。简单功率求和会导致全向等效功率估计偏差。

Method: 首先以统一的矩阵形式表述合成各向同性窄带功率，然后引入波束累积校正因子，包括偏移平均变体以减轻离网角度引起的扇形效应。通过使用信道模型的仿真和154GHz走廊测量验证所提框架。

Result: 提出的框架通过仿真和实际测量验证，能够有效补偿非正交扫描波束导致的功率估计偏差，提供更准确的全向等效功率估计。

Conclusion: 该论文为解决角度分辨宽带测量中的路径增益计算问题提供了系统框架，通过波束累积校正因子有效处理了非正交波束带来的技术挑战，为毫米波和太赫兹信道建模提供了更准确的功率估计方法。

Abstract: Angle-resolved channel sounding using antenna arrays or mechanically steered high-gain antennas is widely employed at millimeter-wave and terahertz bands. To extract antenna-independent large-scale channel parameters such as path loss, delay spread, and angular spread, the radiation-pattern effects embedded in the measured responses must be properly compensated. This paper revisits the technical challenges of path-gain calculation from angle-resolved wideband measurements, with emphasis on angular-domain power integration where the scan beams are inherently non-orthogonal and simple power summation leads to biased omni-equivalent power estimates. We first formulate the synthesized-isotropic narrowband power in a unified matrix form and introduce a beam-accumulation correction factor, including an offset-averaged variant to mitigate scalloping due to off-grid angles. The proposed framework is validated through simulations using channel models and 154~GHz corridor measurements.

</details>


### [8] [Comparative Analysis of Differential and Collision Entropy for Finite-Regime QKD in Hybrid Quantum Noisy Channels](https://arxiv.org/abs/2602.00705)
*Mouli Chakraborty,Subhash Chandra,Avishek Nag,Trung Q. Duong,Merouane Debbah,Anshu Mukherjee*

Main category: eess.SP

TL;DR: 比较混合量子信道中三种基本熵度量：微分熵、量子Rényi熵和量子碰撞熵，使用高斯混合模型建模混合量子噪声，建立理论计算等价性，并应用于有限密钥QKD


<details>
  <summary>Details</summary>
Motivation: 研究混合量子通信系统中不确定性量化问题，混合量子噪声同时包含离散和连续变量噪声分量，需要统一的理论框架来量化这种混合系统的熵

Method: 使用高斯混合模型(GMM)统计建模混合量子噪声，构建并可视化点状熵函数在3D概率景观中，通过解析和数值计算比较微分熵、量子Rényi熵和量子碰撞熵

Result: 在特定混合条件下，微分熵趋近于量子碰撞熵，这与α=2的Rényi熵一致；在混合量子信道框架中建立了这些度量的理论和计算等价性；在有限密钥QKD中，10%近似阈值对应Eve成功概率的数量级变化和安全密钥率的可测量降低

Conclusion: 为混合量子通信系统的不确定性量化提供了统一视角，建立了不同熵度量之间的等价关系，并将分析扩展到有限密钥QKD的操作领域，展示了理论结果的实际应用价值

Abstract: In this work, a comparative study between three fundamental entropic measures, differential entropy, quantum Renyi entropy, and quantum collision entropy for a hybrid quantum channel (HQC) was investigated, where hybrid quantum noise (HQN) is characterized by both discrete and continuous variables (CV) noise components. Using a Gaussian mixture model (GMM) to statistically model the HQN, we construct as well as visualize the corresponding pointwise entropic functions in a given 3D probabilistic landscape. When integrated over the relevant state space, these entropic surfaces yield values of the respective global entropy. Through analytical and numerical evaluation, it is demonstrated that the differential entropy approaches the quantum collision entropy under certain mixing conditions, which aligns with the Renyi entropy for order $α= 2$. Within the HQC framework, the results establish a theoretical and computational equivalence between these measures. This provides a unified perspective on quantifying uncertainty in hybrid quantum communication systems. Extending the analysis to the operational domain of finite key QKD, we demonstrated that the same $10\%$ approximation threshold corresponds to an order-of-magnitude change in Eves success probability and a measurable reduction in the secure key rate.

</details>


### [9] [Denoising deterministic networks using iterative Fourier transforms](https://arxiv.org/abs/2602.00790)
*H. Robert Frost*

Main category: eess.SP

TL;DR: 提出一种基于傅里叶变换的迭代方法(IterativeFT)，用于在存在边剪裁和高斯噪声的情况下识别确定性网络结构，通过交替进行2D离散傅里叶变换和稀疏化操作来去噪。


<details>
  <summary>Details</summary>
Motivation: 网络数据中常同时存在噪声边和缺失边，现有方法难以有效处理这种双重问题。需要开发一种既能过滤噪声边又能恢复缺失真实边的网络去噪技术。

Method: IterativeFT方法：对网络邻接矩阵迭代执行前向和逆向2D离散傅里叶变换，在实域和频域表示上应用稀疏化操作，当实域稀疏模式稳定时算法收敛。

Result: 在Kautz、格网、树和二分网络等确定性模型上表现最佳，在格网和Kautz网络上优于其他方法，在树和二分网络上具有竞争力。能同时有效过滤噪声边和恢复缺失的真实边。

Conclusion: IterativeFT是一种有效的网络去噪方法，特别适用于确定性网络结构，能同时处理噪声边和缺失边问题，相比现有方法具有更好的整体性能。

Abstract: We detail a novel Fourier-based approach (IterativeFT) for identifying deterministic network structure in the presence of both edge pruning and Gaussian noise. This technique involves the iterative execution of forward and inverse 2D discrete Fourier transforms on a target network adjacency matrix. The denoising ability of the method is achieved via the application of a sparsification operation to both the real and frequency domain representations of the adjacency matrix with algorithm convergence achieved when the real domain sparsity pattern stabilizes. To demonstrate the effectiveness of the approach, we apply it to noisy versions of several deterministic models including Kautz, lattice, tree and bipartite networks. For contrast, we also evaluate preferential attachment networks to illustrate the behavior on stochastic graphs. We compare the performance of IterativeFT against simple real domain and frequency domain thresholding, reduced rank reconstruction and locally adaptive network sparsification. Relative to the comparison network denoising approaches, the proposed IterativeFT method provides the best overall performance for lattice and Kuatz networks with competitive performance on tree and bipartite networks. Importantly, the InterativeFT technique is effective at both filtering noisy edges and recovering true edges that are missing from the observed network.

</details>


### [10] [Calibration-Free Induced Magnetic Field Indoor and Outdoor Positioning via Data-Driven Modeling](https://arxiv.org/abs/2602.00817)
*Qiushi Guo,Matthias Tschoepe,Mengxi Liu,Sizhen Bian,Paul Lukowicz*

Main category: eess.SP

TL;DR: 提出基于数据驱动的感应磁场定位框架，通过监督学习直接将磁场测量映射到空间坐标，无需环境特定校准，实现跨环境可迁移的亚米级定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有磁定位系统依赖解析场反演、手动校准或环境特定指纹识别，限制了系统的可扩展性和可迁移性。需要一种更灵活、可扩展的定位方法。

Method: 采用监督学习方法，直接学习从感应磁场测量到空间坐标的映射关系，避免显式场建模。使用方向不变特征表示实现旋转无关部署，采用随机森林回归器进行位置估计。

Result: 在多个室内环境和室外部署中评估，随机森林回归器在2D定位中达到亚20厘米精度，3D定位达到亚30厘米精度。跨环境验证显示室内训练的模型可直接应用于室外环境而无需重新训练。

Conclusion: 数据驱动的感应磁场定位是一种可扩展且可迁移的解决方案，能够平衡覆盖范围和精度，适用于实际定位应用。

Abstract: Induced magnetic field (IMF)-based localization offers a robust alternative to wave-based positioning technologies due to its resilience to non-line-of-sight conditions, environmental dynamics, and wireless interference. However, existing magnetic localization systems typically rely on analytical field inversion, manual calibration, or environment-specific fingerprinting, limiting their scalability and transferability. This paper presents a data-driven IMF localization framework that directly maps induced magnetic field measurements to spatial coordinates using supervised learning, eliminating explicit environment-specific calibration. By replacing explicit field modeling with learning-based inference, the proposed approach captures nonlinear field interactions and environmental effects. An orientation-invariant feature representation enables rotation-independent deployment. The system is evaluated across multiple indoor environments and an outdoor deployment. Benchmarking against classical and deep learning baselines shows that a Random Forest regressor achieves sub-20 cm accuracy in 2D and sub-30 cm in 3D localization. Cross-environment validation demonstrates that models trained indoors generalize to outdoor environments without retraining. We further analyze scalability by varying transmitter spacing, showing that coverage and accuracy can be balanced through deployment density. Overall, this work demonstrates that data-driven IMF localization is a scalable and transferable solution for real-world positioning.

</details>


### [11] [mmWave Sensing for Detecting Movement Through Thermoplastic Masks During Radiation Therapy Treatment](https://arxiv.org/abs/2602.00917)
*Ali Kourani,Naveed A. Abbasi,Syeda Narjis Fatima,Katsuyuki Haneda,Andreas F. Molisch*

Main category: eess.SP

TL;DR: 该研究探索使用毫米波传感技术透过热塑面罩检测放疗患者的细微运动，以解决现有运动追踪方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 放疗精度依赖于患者固定系统，但热塑面罩下仍存在细微的自愿和非自愿运动（如下颌移动、深呼吸、眯眼等），可能影响治疗准确性。现有运动追踪方法存在局限：光学系统需要视线清晰且仅检测表面运动，X射线追踪则引入额外电离辐射。

Method: 研究在28-38 GHz范围内表征热塑面罩材料的射频特性，使用中心频率28 GHz、带宽1 GHz的毫米波系统进行运动检测。采用频域系统配合喇叭天线，在定制无回声室中捕捉传输RF波在响应细微头面部运动时的幅度和相位变化。

Result: 研究证明了毫米波传感技术能够透过热塑面罩材料检测细微运动，为实时透过面罩运动追踪奠定了基础。

Conclusion: 毫米波传感技术为放疗中的运动检测提供了一种非电离、非视线的解决方案，未来可与多天线系统和机器学习结合，实现放疗过程中的实时误差校正。

Abstract: Precision in radiation therapy relies on immobilization systems that limit patient motion. Thermoplastic masks are commonly used for this purpose, but subtle voluntary and involuntary movements such as jaw shifts, deep breathing, or eye squinting may still compromise treatment accuracy. Existing motion tracking methods are limited: optical systems require a clear line of sight and only detect surface motion, while X-ray-based tracking introduces additional ionizing radiation. This study explores the use of low-power, non-ionizing millimeter-wave (mmWave) sensing for through-mask motion detection. We characterize the RF properties of thermoplastic mask material in the 28-38 GHz range and perform motion detection using a 1 GHz bandwidth centered at 28 GHz. We use a frequency-domain system with horn antennas in a custom-built anechoic chamber to capture changes in the amplitude and phase of transmitted RF waves in response to subtle head and facial movements. These findings lay groundwork for future real-time through-mask motion tracking and future integration with multi-antenna systems and machine learning for error correction during radiotherapy.

</details>


### [12] [Channel Modeling and Experimental Validation of Odor-Based Molecular Communication Systems](https://arxiv.org/abs/2602.01091)
*Ahmet B. Kilic,Fatih E. Bilgen,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 提出有界和无界气味分子通信通道的数学模型，并通过实验验证其准确性


<details>
  <summary>Details</summary>
Motivation: 气味分子通信是实现万物互联愿景的关键技术，但目前缺乏能准确描述不同环境中粒子传播的全面通道模型，现有研究缺乏理论建模与实验验证相结合的整体方法

Method: 提出有界和无界OMC通道的数学框架，开发新型实验测试平台，进行广泛性能分析以验证模型准确性

Result: 理论推导与实验数据之间表现出强相关性，为未来端到端OMC系统的设计和分析提供了坚实基础

Conclusion: 提出的数学模型与实验验证相结合的方法填补了研究空白，为气味分子通信系统的实际部署提供了可靠的理论和实验基础

Abstract: Odor-based Molecular Communication (OMC) employs odor molecules to convey information, contributing to the realization of the Internet of Everything (IoE) vision. Despite this, the practical deployment of OMC systems is currently limited by the lack of comprehensive channel models that accurately characterize particle propagation in diverse environments. While existing literature explores various aspects of molecular transport, a holistic approach that integrates theoretical modeling with experimental validation for bounded channels remains underdeveloped. In this paper, we address this gap by proposing mathematical frameworks for both bounded and unbounded OMC channels. To verify the accuracy of the proposed models, we develop a novel experimental testbed and conduct an extensive performance analysis. Our results demonstrate a strong correlation between the theoretical derivations and experimental data, providing a robust foundation for the design and analysis of future end-to-end OMC systems.

</details>


### [13] [Digital and Hybrid Precoding and RF Chain Selection Designs for Energy Efficient Multi-User MIMO-OFDM ISAC Systems](https://arxiv.org/abs/2602.01121)
*Po-Chun Kang,Ming-Chun Lee,Tzu-Chien Chiu,Ting-Yao Kuo,Ta-Sung Lee*

Main category: eess.SP

TL;DR: 该论文研究了多用户MIMO-OFDM ISAC系统的能效优化，通过联合预编码和RF链选择，提出了全数字和混合预编码架构的能效最大化设计方案。


<details>
  <summary>Details</summary>
Motivation: 现有MIMO-OFDM ISAC研究大多关注性能提升，而发射功率和RF电路功耗对能效的影响尚未充分探索，需要填补这一研究空白。

Method: 提出联合预编码和RF链选择的优化框架，针对全数字和混合预编码架构分别设计高效优化算法，并分析计算复杂度和收敛性。

Result: 仿真结果表明，相比现有方案，所提方法在ISAC系统的能效-感知权衡方面取得了显著改进。

Conclusion: 该研究为MIMO-OFDM ISAC系统提供了有效的能效优化方案，实现了能效与感知性能的良好权衡，并提供了频谱效率与功耗的折中设计。

Abstract: Using multiple-input multiple-output (MIMO) with orthogonal frequency division multiplexing (OFDM) for integrated sensing and communication (ISAC) has attracted considerable attention in recent years. While most existing works focus on improving MIMO-OFDM ISAC performance, the impact of transmit power and radio-frequency (RF) circuit power consumption on energy efficiency (EE) remains relatively underexplored. To address this gap, this paper investigates joint precoding and RF chain selection for multi-user MIMO-OFDM ISAC systems, and develops energy-efficient designs for both fully digital and hybrid precoding architectures through the joint optimization of precoding and RF-chain activation. Specifically, we first formulate a novel EE maximization problem subject to sensing performance constraints. Then, efficient optimization algorithms are proposed for both architectures, together with analyses of their computational complexity and convergence behavior. Building on the proposed approaches, spectral efficiency-power consumption tradeoff designs are also provided. Simulation results demonstrate that, compared with existing schemes, the proposed approaches achieve significant improvements in the EE-sensing tradeoff for ISAC systems.

</details>


### [14] [Generative AI in Signal Processing Education: An Audio Foundation Model Based Approach](https://arxiv.org/abs/2602.01249)
*Muhammad Salman Khan,Ahmad Ullah,Siddique Latif,Junaid Qadir*

Main category: eess.SP

TL;DR: SPEduAFM是一个专为信号处理教育设计的音频基础模型概念，旨在将传统信号处理原理与生成式AI创新相结合，通过自动化转录、交互演示等应用提升教学体验。


<details>
  <summary>Details</summary>
Motivation: 音频基础模型作为生成式AI的一个专门类别，有潜力通过整合语音增强、去噪、源分离、特征提取等核心应用来变革信号处理教育，将抽象概念转化为实践体验。

Method: 提出SPEduAFM概念框架，通过设想案例研究展示AFM在教育中的应用，包括自动化讲座转录、交互式演示和包容性学习工具，强调动态实时听觉交互促进体验式学习。

Result: 展示了AFM在教育中的多种应用潜力，能够将抽象信号处理概念转化为引人入胜的实践体验，同时指出了伦理、可解释性和定制化等挑战。

Conclusion: SPEduAFM作为一个前瞻性愿景，旨在激励生成式AI在工程教育中的更广泛采用，提升课堂内外的可访问性、参与度和创新性。

Abstract: Audio Foundation Models (AFMs), a specialized category of Generative AI (GenAI), have the potential to transform signal processing (SP) education by integrating core applications such as speech and audio enhancement, denoising, source separation, feature extraction, automatic classification, and real-time signal analysis into learning and research. This paper introduces SPEduAFM, a conceptual AFM tailored for SP education, bridging traditional SP principles with GenAI-driven innovations. Through an envisioned case study, we outline how AFMs can enable a range of applications, including automated lecture transcription, interactive demonstrations, and inclusive learning tools, showcasing their potential to transform abstract concepts into engaging, practical experiences. This paper also addresses challenges such as ethics, explainability, and customization by highlighting dynamic, real-time auditory interactions that foster experiential and authentic learning. By presenting SPEduAFM as a forward-looking vision, we aim to inspire broader adoption of GenAI in engineering education, enhancing accessibility, engagement, and innovation in the classroom and beyond.

</details>


### [15] [Mismatch Analysis and Cooperative Calibration of Array Beam Patterns for ISAC Systems](https://arxiv.org/abs/2602.01293)
*Hui Chen,Mengting Li,Alireza Pourafzal,Huiping Huang,Yu Ge,Sigurd Sandor Petersen,Ming Shen,George C. Alexandropoulos,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出一种用于ISAC系统的阵列波束图校准方法，通过可微分损失函数和协作校准框架，显著降低角度估计误差


<details>
  <summary>Details</summary>
Motivation: ISAC系统中，几何误差和硬件损伤会导致模型失配，从而降低感知性能，特别是角度估计精度。传统基于波束图相似性的校准方法不能直接优化感知性能。

Method: 提出一种新的性能度量，考虑角度估计误差而非波束图相似性，并构建可微分损失函数。引入协作校准框架，允许多个用户设备基于本地数据迭代优化波束图，并协作更新全局校准参数。

Result: 在消声室真实波束图测量数据上验证，2D校准场景中角度估计误差从1.01°降至0.11°，3D场景中从5.19°降至0.86°，性能显著提升。

Conclusion: 提出的基于角度估计误差的校准方法和协作框架能有效解决ISAC系统中的阵列校准问题，大幅提升感知性能，为实际部署提供可行方案。

Abstract: Integrated sensing and communication (ISAC) is a key technology for enabling a wide range of applications in future wireless systems. However, the sensing performance is often degraded by model mismatches caused by geometric errors (e.g., position and orientation) and hardware impairments (e.g., mutual coupling and amplifier non-linearity). This paper focuses on the angle estimation performance with antenna arrays and tackles the critical challenge of array beam pattern calibration for ISAC systems. To assess calibration quality from a sensing perspective, a novel performance metric that accounts for angle estimation error, rather than beam pattern similarity, is proposed and incorporated into a differentiable loss function. Additionally, a cooperative calibration framework is introduced, allowing multiple user equipments to iteratively optimize the beam pattern based on the proposed loss functions and local data, and collaboratively update global calibration parameters. The proposed models and algorithms are validated using real-world beam pattern measurements collected in an anechoic chamber. Experimental results show that the angle estimation error can be reduced from {$\textbf{1.01}^\circ$} to $\textbf{0.11}^\circ$ in 2D calibration scenarios, and from $\textbf{5.19}^\circ$ to $\textbf{0.86}^\circ$ in 3D calibration ones.

</details>


### [16] [Approximating Univariate Factored Distributions via Message-Passing Algorithms](https://arxiv.org/abs/2602.01377)
*Zilu Zhao,Dirk Slock*

Main category: eess.SP

TL;DR: 本文提出两种结合期望传播(EP)与先前技术的方法，用于近似单变量因子化分布，特别是高斯混合模型(GMM)的乘积，解决EP中非可积信念的均值和方差计算问题。


<details>
  <summary>Details</summary>
Motivation: 高斯混合模型在通信系统中常见，特别是在双线性联合估计和检测问题中。虽然GMM的乘积仍然是GMM，但随着因子数量增加，结果乘积GMM的组件数量呈指数增长。需要找到可处理的近似方法来处理单变量因子化概率密度函数。

Method: 1. 提出基于信念传播(BP)的变量复制和高斯信念传播(VDBP)算法，通过构建多变量测量模型，其边缘后验等于给定的单变量因子化PDF，然后应用高斯BP将全局推理问题转化为局部问题。2. 提出两种结合期望传播(EP)与先前处理非可积信念技术的方法，直接近似因子化PDF，解决EP中除法操作可能导致算法失败的问题。

Result: 提出了VDBP算法和两种EP改进方法，能够有效近似单变量因子化分布，特别是处理高斯混合模型的乘积问题，解决了传统EP在处理非可积信念时的失败问题。

Conclusion: 通过结合变量复制、高斯信念传播和改进的期望传播方法，本文提供了有效的解决方案来处理通信系统中常见的高斯混合模型乘积问题，实现了对单变量因子化分布的可处理近似。

Abstract: Gaussian Mixture Models (GMMs) commonly arise in communication systems, particularly in bilinear joint estimation and detection problems. Although the product of GMMs is still a GMM, as the number of factors increases, the number of components in the resulting product GMM grows exponentially. To obtain a tractable approximation for a univariate factored probability density function (PDF), such as a product of GMMs, we investigate iterative message-passing algorithms. Based on Belief Propagation (BP), we propose a Variable Duplication and Gaussian Belief Propagation (VDBP)-based algorithm. The key idea of VDBP is to construct a multivariate measurement model whose marginal posterior is equal to the given univariate factored PDF. We then apply Gaussian BP (GaBP) to transform the global inference problem into local ones. Expectation propagation (EP) is another branch of message passing algorithms. In addition to converting the global approximation problem into local ones, it features a projection operation that ensures the intermediate functions (messages) belong to a desired family. Due to this projection, EP can be used to approximate the factored PDF directly. However, even if every factor is integrable, the division operation in EP may still cause the algorithm to fail when the mean and variance of a non-integrable belief are required. Therefore, this paper proposes two methods that combine EP with our previously proposed techniques for handling non-integrable beliefs to approximate univariate factored distributions.

</details>


### [17] [Visible Light Positioning With Lamé Curve LEDs: A Generic Approach for Camera Pose Estimation](https://arxiv.org/abs/2602.01577)
*Wenxuan Pan,Yang Yang,Dong Wei,Zhiyu Zhu,Jintao Wang,Huan Wu,Yao Nie*

Main category: eess.SP

TL;DR: 提出LC-VLP算法，利用Lamé曲线统一表示多种LED形状，实现异构LED场景下的相机姿态估计，相比现有方法显著降低定位误差。


<details>
  <summary>Details</summary>
Motivation: 现有基于LED形状特征的可见光定位方法通常局限于单一LED几何形状，在异构LED形状场景中会失效。需要一种能够统一表示常见LED形状的方法来解决这一挑战。

Method: 使用Lamé曲线作为LED形状的统一表示，构建LED参数数据库。在线定位时，通过可见光通信接收LED曲线参数，将相机姿态估计建模为非线性最小二乘问题，并开发FreePnP算法提供可靠的初始化估计。

Result: 仿真显示LC-VLP在圆形和矩形LED场景中均优于现有方法，位置误差降低超过40%，旋转误差降低超过25%。实验验证平均定位精度小于4厘米。

Conclusion: LC-VLP算法通过Lamé曲线统一表示多种LED形状，解决了异构LED场景下的定位问题，实现了高精度、低成本的相机姿态估计。

Abstract: Camera-based visible light positioning (VLP) is a promising technique for accurate and low-cost indoor camera pose estimation (CPE). To reduce the number of required light-emitting diodes (LEDs), advanced methods commonly exploit LED shape features for positioning. Although interesting, they are typically restricted to a single LED geometry, leading to failure in heterogeneous LED-shape scenarios. To address this challenge, this paper investigates Lamé curves as a unified representation of common LED shapes and proposes a generic VLP algorithm using Lamé curve-shaped LEDs, termed LC-VLP. In the considered system, multiple ceiling-mounted Lamé curve-shaped LEDs periodically broadcast their curve parameters via visible light communication, which are captured by a camera-equipped receiver. Based on the received LED images and curve parameters, the receiver can estimate the camera pose using LC-VLP. Specifically, an LED database is constructed offline to store the curve parameters, while online positioning is formulated as a nonlinear least-squares problem and solved iteratively. To provide a reliable initialization, a correspondence-free perspective-\textit{n}-points (FreeP\textit{n}P) algorithm is further developed, enabling approximate CPE without any pre-calibrated reference points. The performance of LC-VLP is verified by both simulations and experiments. Simulations show that LC-VLP outperforms state-of-the-art methods in both circular- and rectangular-LED scenarios, achieving reductions of over 40% in position error and 25% in rotation error. Experiments further show that LC-VLP can achieve an average position accuracy of less than 4 cm.

</details>


### [18] [Resolution-Aliasing Trade-off in Near-Field Localisation](https://arxiv.org/abs/2602.01947)
*Baptiste Sambon,Gilles Monnoyer,Luc Vandendorpe,Claude Oestges*

Main category: eess.SP

TL;DR: 该论文提出了一个统一框架来分析近场大规模MIMO系统中的分辨率和混叠权衡，通过引入局部啁啾空间频率概念和几何工具来指导阵列设计。


<details>
  <summary>Details</summary>
Motivation: 超大规模MIMO系统在近场工作时为精确定位提供了新自由度，但密集阵列不切实际。稀疏或分布式阵列可以降低硬件复杂度，但亚奈奎斯特空间采样会引入定位模糊函数中的混叠伪影，需要研究分辨率和混叠之间的权衡关系。

Method: 提出统一框架，利用局部啁啾空间频率概念推导阵列几何和采样密度与接收场空间带宽的解析关系。引入两个几何工具：关键天线单元和非贡献区域，直观识别单个天线对分辨率和混叠的贡献。

Result: 分析表明分辨率和混叠并不总是严格耦合，例如增加阵列孔径可以提高分辨率而不一定加剧混叠。提供了实用的近场阵列设计指南，以优化分辨率和混叠的平衡。

Conclusion: 该框架为设计平衡分辨率和混叠的近场阵列提供了理论指导，支持高效超大规模MIMO部署，通过几何工具帮助理解天线对系统性能的贡献机制。

Abstract: Extremely Large-scale MIMO (XL-MIMO) systems operating in Near-Field (NF) introduce new degrees of freedom for accurate source localisation, but make dense arrays impractical. Sparse or distributed arrays can reduce hardware complexity while maintaining high resolution, yet sub-Nyquist spatial sampling introduces aliasing artefacts in the localisation ambiguity function. This paper presents a unified framework to jointly characterise resolution and aliasing in NF localisation and study the trade-off between the two. Leveraging the concept of local chirp spatial frequency, we derive analytical expressions linking array geometry and sampling density to the spatial bandwidth of the received field. We introduce two geometric tools--Critical Antenna Elements (CAEs) and the Non-Contributive Zone (NCZ)--to intuitively identify how individual antennas contribute to resolution and/or aliasing. Our analysis reveals that resolution and aliasing are not always strictly coupled, e.g., increasing the array aperture can improve resolution without necessarily aggravating aliasing. These results provide practical guidelines for designing NF arrays that optimally balance resolution and aliasing, supporting efficient XL-MIMO deployment.

</details>


### [19] [Uncertainty-Weighted Multi-Task CNN for Joint DoA and Rain-Rate Estimation Under Rain-Induced Array Distortions](https://arxiv.org/abs/2602.01961)
*Chenyang Yan,Ruonan Yang,Shunqiao Sun,Mats Bengtsson*

Main category: eess.SP

TL;DR: 提出一种基于多任务深度CNN的联合到达方向(DoA)和降雨率估计方法，用于处理降雨引起的乘性失真


<details>
  <summary>Details</summary>
Motivation: 研究均匀线性阵列在降雨引起的乘性失真下的联合DoA和降雨率估计问题，降雨引起的波前波动会影响阵列性能

Method: 基于波前波动模型推导角度相关协方差公式，生成训练数据；将DoA估计建模为离散角度网格上的多标签分类问题，降雨率估计为多类分类任务；设计多任务深度CNN，包含共享特征提取器和两个任务特定头部，使用不确定性加权目标自动平衡两个损失

Result: 在双源场景中的数值结果显示，所提网络在中等至高SNR下比经典基线方法获得更低的DoA均方根误差，并能提供准确的降雨率分类

Conclusion: 多任务深度学习框架能有效处理降雨失真下的联合DoA和降雨率估计问题，在保持DoA估计精度的同时实现降雨率的准确分类

Abstract: We investigate joint direction-of-arrival (DoA) and rain-rate estimation for a uniform linear array operating under rain-induced multiplicative distortions. Building on a wavefront fluctuation model whose spatial correlation is governed by the rain-rate, we derive an angle-dependent covariance formulation and use it to synthesize training data. DoA estimation is cast as a multi-label classification problem on a discretized angular grid, while rain-rate estimation is formulated as a multi-class classification task. We then propose a multi-task deep CNN with a shared feature extractor and two task-specific heads, trained using an uncertainty-weighted objective to automatically balance the two losses. Numerical results in a two-source scenario show that the proposed network achieves lower DoA RMSE than classical baselines and provides accurate rain-rate classification at moderate-to-high SNRs.

</details>


### [20] [Obstacle Detection at Level Crossings under Adverse Weather Conditions -- A Survey](https://arxiv.org/abs/2602.01974)
*Chenyang Yan,Mats Bengtsson*

Main category: eess.SP

TL;DR: 本文综述了铁路道口障碍物检测的传感器技术与融合策略，重点分析恶劣天气条件下的鲁棒性、检测精度和环境适应性，并探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 铁路道口事故仍是现代铁路系统的重大安全隐患，特别是在恶劣天气条件下传感器性能下降时。需要研究更可靠、鲁棒的障碍物检测技术来提升铁路安全。

Method: 综述了多种传感器技术（感应线圈、摄像头、雷达、激光雷达）的工作原理、天气影响及缓解策略，并分析了数据级、特征级和决策级的多传感器融合架构。

Result: 单个传感器各有优缺点，多传感器融合能整合互补信息，提高可靠性和容错性。需要进一步研究自适应融合算法、实时处理流水线和天气弹性数据集。

Conclusion: 多传感器融合是提升铁路道口障碍物检测系统鲁棒性的关键方向，未来需发展智能、故障安全的检测系统，以应对恶劣天气条件下的安全挑战。

Abstract: Level crossing accidents remain a significant safety concern in modern railway systems, particularly under adverse weather conditions that degrade sensor performance. This review surveys state-of-the-art sensor technologies and fusion strategies for obstacle detection at railway level crossings, with a focus on robustness, detection accuracy, and environmental resilience. Individual sensors such as inductive loops, cameras, radar, and LiDAR offer complementary strengths but involve trade-offs, including material dependence, reduced visibility, and limited resolution in harsh environments. We analyze each modality's working principles, weather-induced vulnerabilities, and mitigation strategies, including signal enhancement and machine-learning-based denoising. We further review multi-sensor fusion approaches, categorized as data-level, feature-level, and decision-level architectures, that integrate complementary information to improve reliability and fault tolerance. The survey concludes with future research directions, including adaptive fusion algorithms, real-time processing pipelines, and weather-resilient datasets to support the deployment of intelligent, fail-safe detection systems for railway safety.

</details>


### [21] [Silhouette Score Efficient Radio Frequency Fingerprint Feature Extraction](https://arxiv.org/abs/2602.02065)
*Xuan Yang,Dongming Li,Yi Lou,Xianglin Fan*

Main category: eess.SP

TL;DR: 提出基于预编码的射频指纹特征提取方法，通过频域信号倒数抑制信道影响，无需信道估计，在三种信道场景下均实现最高的轮廓分数和分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有射频指纹识别技术容易受信道变化影响，且多数方法依赖实验比较而非理论分析，缺乏理论指导框架，阻碍了信道鲁棒特征提取技术的发展。

Method: 1) 使用轮廓分数作为评估指标，通过泰勒级数展开分析不同RFF特征提取方法的理论性能；2) 提出基于预编码的信道鲁棒RFF特征提取方法，在认证设备端计算频域接收信号的倒数来抑制信道影响；3) 在确定性信道、i.i.d.随机信道和非i.i.d.随机信道三种场景下进行比较。

Result: 仿真和实验结果表明：轮廓分数是评估分类准确率的有效指标；提出的预编码方法在信道变化下获得最高的轮廓分数和分类准确率。

Conclusion: 建立了统一的RFF特征提取理论分析框架，提出的预编码方法能有效抑制信道影响，提高射频指纹识别的鲁棒性和准确性，为信道鲁棒RFF特征提取提供了理论指导。

Abstract: Radio frequency fingerprint (RFF) identification technology, which exploits relatively stable hardware imperfections, is highly susceptible to constantly changing channel effects. Although various channel-robust RFF feature extraction methods have been proposed, they predominantly rely on experimental comparisons rather than theoretical analyses. This limitation hinders the progress of channel-robust RFF feature extraction and impedes the establishment of theoretical guidance for its design. In this paper, we establish a unified theoretical performance analysis framework for different RFF feature extraction methods using the silhouette score as an evaluation metric, and propose a precoding-based channel-robust RFF feature extraction method that enhances the silhouette score without requiring channel estimation. First, we employ the silhouette score as an evaluation metric and obtain the theoretical performance of various RFF feature extraction methods using the Taylor series expansion. Next, we mitigate channel effects by computing the reciprocal of the received signal in the frequency domain at the device under authentication. We then compare these methods across three different scenarios: the deterministic channel scenario, the independent and identically distributed (i.i.d.) stochastic channel scenario, and the non-i.i.d. stochastic channel scenario. Finally, simulation and experimental results demonstrate that the silhouette score is an efficient metric to evaluate classification accuracy. Furthermore, the results indicate that the proposed precoding-based channel-robust RFF feature extraction method achieves the highest silhouette score and classification accuracy under channel variations.

</details>


### [22] [Neurophysiological effects of museum modalities on emotional engagement with real artworks](https://arxiv.org/abs/2602.02086)
*Chen Feng,Sébastien Lugan,Karine Lasaracina,Midori Sugaya,Benoît Macq*

Main category: eess.SP

TL;DR: 研究通过脑电图分析不同数字解说形式对艺术观赏情感投入的影响，发现不同形式塑造不同的投入风格而非投入量


<details>
  <summary>Details</summary>
Motivation: 博物馆越来越多地使用数字内容帮助观众理解艺术品，但人们对这些形式如何影响艺术体验中的情感投入知之甚少

Method: 在博物馆现场进行脑电图研究，比较三种观赏方式：直接观看布吕赫尔画作、180°沉浸式解说投影、常规显示器解说视频，提取前额脑电标记分析动机取向、内部参与、感知驱动和唤醒度

Result: 不同形式产生特定的投入模式：显示器解说视频引发高唤醒度和快速脑电活动，沉浸式投影促进平静、存在导向的专注，原作反映内部调节的投入

Conclusion: 数字解说内容影响投入风格而非投入量，为博物馆优化解说媒体形式和内容提供依据，推动新的多模态感知方法

Abstract: Museums increasingly rely on digital content to support visitors' understanding of artworks, yet little is known about how these formats shape the emotional engagement that underlies meaningful art experiences. This research presents an in-situ EEG study on how digital interpretive content modulate engagement during art viewing. Participants experienced three modalities: direct viewing of a Bruegel painting, a 180° immersive interpretive projection, and a regular, display-based interpretive video. Frontal EEG markers of motivational orientation, internal involvement, perceptual drive, and arousal were extracted using eyes-open baselines and Z-normalized contrasts. Results show modality-specific engagement profiles: display-based interpretive video induced high arousal and fast-band activity, immersive projections promoted calm, presence-oriented absorption, and original artworks reflected internally regulated engagement. These findings, relying on lightweight EEG sensing in an operational cultural environment, suggest that digital interpretive content affects engagement style rather than quantity. This paves the way for new multimodal sensing approaches and enables museums to optimize the modalities and content of their interpretive media.

</details>


### [23] [RIS-Aided Wireless Amodal Sensing for Single-View 3D Reconstruction](https://arxiv.org/abs/2602.02148)
*Yuhan Wang,Haobo Zhang,Qingyu Liu,Hongliang Zhang,Lingyang Song*

Main category: eess.SP

TL;DR: 提出RIS辅助的无线无模态感知方案，利用可重构智能表面增强空间分辨率并绕过障碍物，通过生成学习模型重建完整形状，开发误差预测模型优化RIS相移。


<details>
  <summary>Details</summary>
Motivation: 无线无模态感知在复杂环境中能恢复被遮挡物体的完整形状，具有环境鲁棒性、隐私保护和低成本优势。但无线系统获取的感知数据稀疏，空间分辨率低，在遮挡环境下问题更严重。

Method: 1) 提出RIS辅助的无线无模态感知方案，利用大规模RIS增强空间分辨率并创建绕过障碍物的反射路径；2) 采用生成学习模型从RIS视角的感知数据重建完整形状；3) 开发误差预测模型学习RIS相移到感知精度的映射关系，基于此优化RIS相移配置。

Result: 在基准数据集上的实验结果表明，与传统方案相比，在相同RIS配置数量下，本方法至少减少56.73%的重建误差。

Conclusion: RIS辅助的无线无模态感知方案能有效解决无线感知数据稀疏和遮挡问题，通过优化RIS相移显著提升形状重建精度，为复杂环境下的无模态感知提供了有前景的解决方案。

Abstract: Amodal sensing is critical for various real-world sensing applications because it can recover the complete shapes of partially occluded objects in complex environments. Among various amodal sensing paradigms, wireless amodal sensing is a potential solution due to its advantages of environmental robustness, privacy preservation, and low cost. However, the sensing data obtained by wireless system is sparse for shape reconstruction because of the low spatial resolution, and this issue is further intensified in complex environments with occlusion. To address this issue, we propose a Reconfigurable Intelligent Surface (RIS)-aided wireless amodal sensing scheme that leverages a large-scale RIS to enhance the spatial resolution and create reflection paths that can bypass the obstacles. A generative learning model is also employed to reconstruct the complete shape based on the sensing data captured from the viewpoint of the RIS. In such a system, it is challenging to optimize the RIS phase shifts because the relationship between RIS phase shifts and amodal sensing accuracy is complex and the closed-form expression is unknown. To tackle this challenge, we develop an error prediction model that learns the mapping from RIS phase shifts to amodal sensing accuracy, and optimizes RIS phase shifts based on this mapping. Experimental results on the benchmark dataset show that our method achieves at least a 56.73% reduction in reconstruction error compared to conventional schemes under the same number of RIS configurations.

</details>


### [24] [Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding](https://arxiv.org/abs/2602.02167)
*Soheil Behnam Roudsari,Alexandre S. Brandão,Felipe N. Martins*

Main category: eess.SP

TL;DR: 提出一种基于2D LiDAR的物体检测方法，通过堆叠连续三帧扫描作为RGB通道编码短期时序信息，在嵌入式设备上实现实时高精度检测，无需RGB摄像头。


<details>
  <summary>Details</summary>
Motivation: 室内服务机器人需要鲁棒、隐私友好且能在嵌入式硬件上运行的感知系统。传统RGB摄像头存在隐私问题，而现有LiDAR方法通常需要构建占用网格，计算开销大。

Method: 将连续三帧2D LiDAR扫描堆叠为RGB通道，直接作为YOLOv8n的输入，保留角度结构和运动线索，避免占用网格构建。在Webots仿真环境中评估，使用160个随机室内场景。

Result: 在四类物体检测上达到98.4% mAP@0.5（0.778 mAP@0.5:0.95），精确率94.9%，召回率94.7%。在树莓派5上实时运行，平均端到端延迟47.8ms，比相关占用网格方法延迟更低。

Conclusion: 轻量级时序编码方法能够实现准确、实时的纯LiDAR检测，适用于嵌入式室内机器人，无需RGB摄像头，同时保护隐私并降低计算需求。

Abstract: Indoor service robots need perception that is robust, more privacy-friendly than RGB video, and feasible on embedded hardware. We present a camera-free 2D LiDAR object detection pipeline that encodes short-term temporal context by stacking three consecutive scans as RGB channels, yielding a compact YOLOv8n input without occupancy-grid construction while preserving angular structure and motion cues. Evaluated in Webots across 160 randomized indoor scenarios with strict scenario-level holdout, the method achieves 98.4% mAP@0.5 (0.778 mAP@0.5:0.95) with 94.9% precision and 94.7% recall on four object classes. On a Raspberry Pi 5, it runs in real time with a mean post-warm-up end-to-end latency of 47.8ms per frame, including scan encoding and postprocessing. Relative to a closely related occupancy-grid LiDAR-YOLO pipeline reported on the same platform, the proposed representation is associated with substantially lower reported end-to-end latency. Although results are simulation-based, they suggest that lightweight temporal encoding can enable accurate and real-time LiDAR-only detection for embedded indoor robotics without capturing RGB appearance.

</details>


### [25] [Sampling-Free Diffusion Transformers for Low-Complexity MIMO Channel Estimation](https://arxiv.org/abs/2602.02202)
*Zhixiong Chen,Hyundong Shin,Arumugam Nallanathan*

Main category: eess.SP

TL;DR: 提出SF-DiT-CE方法，使用采样自由的扩散Transformer进行低复杂度MIMO信道估计，通过单次前向传播替代迭代采样，显著降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的信道估计器虽然性能优异，但依赖迭代反向采样导致计算复杂度高，需要开发低复杂度的解决方案

Method: 利用MIMO信道的角度域稀疏性，训练轻量级扩散Transformer直接从扰动观测和噪声水平预测干净信道；在推理时使用最小二乘估计和估计噪声作为条件，通过单次前向传播恢复信道

Result: 数值结果表明，该方法在显著降低复杂度的同时，实现了优于最先进基线的估计精度和鲁棒性

Conclusion: SF-DiT-CE通过消除迭代采样过程，为MIMO信道估计提供了一种高效且性能优越的解决方案，平衡了估计精度和计算复杂度

Abstract: Diffusion model-based channel estimators have shown impressive performance but suffer from high computational complexity because they rely on iterative reverse sampling. This paper proposes a sampling-free diffusion transformer (DiT) for low-complexity MIMO channel estimation, termed SF-DiT-CE. Exploiting angular-domain sparsity of MIMO channels, we train a lightweight DiT to directly predict the clean channels from their perturbed observations and noise levels. At inference, the least square (LS) estimate and estimation noise condition the DiT to recover the channel in a single forward pass, eliminating iterative sampling. Numerical results demonstrate that our method achieves superior estimation accuracy and robustness with significantly lower complexity than state-of-the-art baselines.

</details>


### [26] [A Novel ISAC Waveform Based on Orthogonal Delay-Doppler Division Multiplexing with FMCW](https://arxiv.org/abs/2602.02248)
*Kehan Huang,Akram Shafie,Min Qiu,Elias Aboutanios,Jinhong Yuan*

Main category: eess.SP

TL;DR: 提出ODDM-FMCW波形，将正交延迟多普勒分复用与调频连续波结合，实现低PAPR的集成感知与通信


<details>
  <summary>Details</summary>
Motivation: 传统线性FMCW波形在ISAC系统中存在局限性，需要开发既能实现高效通信又能进行精确感知的低PAPR波形

Method: 1. 提出平方根奈奎斯特滤波FMCW波形；2. 在延迟多普勒域嵌入符号生成DD-SRN-FMCW帧；3. 设计DD啁啾压缩接收机；4. 将DD-SRN-FMCW帧叠加到ODDM数据帧上构建ODDM-FMCW波形

Result: 数值结果表明，ODDM-FMCW波形在感知的均方根误差和通信的误码率方面均表现出优异的ISAC性能

Conclusion: ODDM-FMCW波形成功解决了传统FMCW在ISAC中的局限性，实现了低PAPR下的高效集成感知与通信

Abstract: In this work, we propose the orthogonal delay-Doppler (DD) division multiplexing (ODDM) modulation with frequency modulated continuous wave (FMCW) (ODDM-FMCW) waveform to enable integrated sensing and communication (ISAC) with a low peak-to-average power ratio (PAPR). We first propose a square-root-Nyquist-filtered FMCW (SRN-FMCW) waveform to address limitations of conventional linear FMCW waveforms in ISAC systems. To better integrate with ODDM, we generate SRN-FMCW by embedding symbols in the DD domain, referred to as a DD-SRN-FMCW frame. A DD chirp compression receiver is designed to obtain the channel response efficiently. Next, we construct the proposed ODDM-FMCW waveform for ISAC by superimposing a DD-SRN-FMCW frame onto an ODDM data frame. A comprehensive performance analysis of the ODDM-FMCW waveform is presented, covering peak-to-average power ratio, spectrum, ambiguity function, and Cramer-Rao bound for delay and Doppler estimation. Numerical results show that the proposed ODDM-FMCW waveform delivers excellent ISAC performance in terms of root mean square error for sensing and bit error rate for communications.

</details>


### [27] [Flexible laboratory setup for DAC experimentation](https://arxiv.org/abs/2602.02312)
*Alfredo Pérez Vega-Leal,Manuel G. Satué*

Main category: eess.SP

TL;DR: 本文回顾了多种DAC技术，重点研究了模拟多路复用方案，并基于商用FPGA开发了时间交织Sigma-Delta调制DAC原型


<details>
  <summary>Details</summary>
Motivation: 在现代发射机中，速度是主要限制因素，模拟多路复用技术成为有前景的解决方案。研究目标是开发低成本方案来比较不同的数字模拟转换(DAC)方案。

Method: 回顾了模拟多路复用技术、高速单DAC、Sigma-Delta调制和动态元件匹配等技术。基于商用现场可编程门阵列(FPGA)系统开发了时间交织Sigma-Delta调制DAC原型。

Result: 展示了基于商用FPGA系统的时间交织Sigma-Delta调制DAC原型实现。

Conclusion: 模拟多路复用是现代发射机中解决速度限制的有前景方案，时间交织Sigma-Delta调制DAC原型验证了该技术的可行性。

Abstract: Analog multiplexing appears to be a promising solution for modern transmitters, where speed is the primary limitation. The objective is the development of a low-cost solution to compare different digital to analog (DAC) schemes. In particular, analog multiplexing techniques, high-speed single-DAC, Sigma-delta modulation, Dynamic element matching are considered. The work presents a review of these techniques and shows a prototype of a time interleaved sigma delta modulation based DAC based on a commercially available Field Programmable Gate Array system.

</details>


### [28] [A Track-Before-Detect Trajectory Multi-Bernoulli Filter for Generalised Superpositional Measurements](https://arxiv.org/abs/2602.02365)
*Sion Lynch,Ángel F. García-Fernández,Lee Devlin*

Main category: eess.SP

TL;DR: 提出T-IEMB滤波器用于轨迹估计，采用广义叠加测量模型，提供高斯实现，相比粒子滤波在计算成本和性能上均有优势


<details>
  <summary>Details</summary>
Motivation: 在检测前跟踪应用中，需要同时估计存活轨迹和所有轨迹。现有方法如粒子滤波计算成本高，需要更高效且性能优越的解决方案来处理广义叠加测量模型

Method: 提出轨迹信息交换多伯努利滤波器，采用广义叠加测量模型，其中叠加隐藏变量映射到测量的条件均值和协方差。提供高斯实现，通过近似测量模型的条件矩来执行更新，实现计算轻量的滤波解决方案

Result: 在非高斯雷达跟踪场景的仿真中，两种高斯T-IEMB实现相比最先进的基于粒子滤波的检测前跟踪解决方案，提供了改进的跟踪性能，同时计算成本降低

Conclusion: T-IEMB滤波器能够有效处理广义叠加测量模型，其高斯实现提供了计算效率高且性能优越的轨迹估计解决方案，在检测前跟踪应用中具有实际应用价值

Abstract: This paper proposes the Trajectory-Information Exchange Multi-Bernoulli (T-IEMB) filter to estimate sets of alive and all trajectories in track-before-detect applications with generalised superpositional measurements. This measurement model has superpositional hidden variables which are mapped to the conditional mean and covariance of the measurement, enabling it to describe a broad range of measurement models. This paper also presents a Gaussian implementation of the T-IEMB filter, which performs the update by approximating the conditional moments of the measurement model, and admits a computationally light filtering solution. Simulation results for a non-Gaussian radar-based tracking scenario demonstrate the performance of two Gaussian T-IEMB implementations, which provide improved tracking performance compared to a state-of-the-art particle filter based solution for track-before-detect, at a reduced computational cost.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems](https://arxiv.org/abs/2602.00027)
*Zhenyu Pu,Yu Yang,Lun Yang,Qing-Shan Jia,Xiaohong Guan,Costas J. Spanos*

Main category: cs.LG

TL;DR: 本文提出了一种结合表示学习技术的增强型深度强化学习框架，用于优化氢基多能源系统的运行，该框架能更好地处理非线性动力学和多重不确定性。


<details>
  <summary>Details</summary>
Motivation: 氢基多能源系统（HMES）作为低碳高效的解决方案，能够协调电、热、冷供应与需求，但面临氢能存储系统非线性多物理场耦合动力学以及供需多重不确定性的挑战，传统方法难以实现最优运行。

Method: 开发了全面捕捉氢能存储系统非线性动力学和多物理过程的HMES运行模型，并提出集成表示学习技术的增强型深度强化学习框架，加速和改善复杂网络系统的策略优化。

Result: 基于真实数据集的实验表明，全面模型对确保氢能存储系统安全可靠运行至关重要，提出的SR-DRL方法在降低HMES运行成本和满足系统约束方面，相比传统DRL具有更优的收敛速度和性能。

Conclusion: 表示学习技术能够将原始状态空间重组为结构良好、聚类感知的几何表示，从而平滑和促进深度强化学习过程，为氢基多能源系统的优化运行提供了有效解决方案。

Abstract: Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the share of renewable integration. However, the optimal operation of HMES remains challenging due to the nonlinear and multi-physics coupled dynamics of hydrogen energy storage systems (HESS) (consisting of electrolyters, fuel cells and hydrogen tanks) as well as the presence of multiple uncertainties from supply and demand. To address these challenges, this paper develops a comprehensive operational model for HMES that fully captures the nonlinear dynamics and multi-physics process of HESS. Moreover, we propose an enhanced deep reinforcement learning (DRL) framework by integrating the emerging representation learning techniques, enabling substantially accelerated and improved policy optimization for spatially and temporally coupled complex networked systems, which is not provided by conventional DRL. Experimental studies based on real-world datasets show that the comprehensive model is crucial to ensure the safe and reliable of HESS. In addition, the proposed SR-DRL approaches demonstrate superior convergence rate and performance over conventional DRL counterparts in terms of reducing the operation cost of HMES and handling the system operating constraints. Finally, we provide some insights into the role of representation learning in DRL, speculating that it can reorganize the original state space into a well-structured and cluster-aware geometric representation, thereby smoothing and facilitating the learning process of DRL.

</details>


### [30] [OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models](https://arxiv.org/abs/2602.00012)
*Michael Siebenmann,Javier Argota Sánchez-Vaquerizo,Stefan Arisona,Krystian Samp,Luis Gisler,Dirk Helbing*

Main category: cs.LG

TL;DR: OGD4All是一个基于大语言模型的透明、可审计、可复现框架，用于增强公民与地理空间开放政府数据的交互，通过语义检索、智能代理代码生成和安全沙箱执行，实现高准确率的多模态数据访问。


<details>
  <summary>Details</summary>
Motivation: 当前公民与开放政府数据的交互存在障碍，需要更透明、可审计且可复现的方式来访问和理解地理空间数据，同时需要减少大语言模型在处理此类数据时的幻觉风险。

Method: 结合语义数据检索、智能代理推理进行迭代代码生成，以及安全沙箱执行，生成可验证的多模态输出。在430个苏黎世市数据集和11个大语言模型上进行评估。

Result: 在199个问题的基准测试中（包括事实性和不可回答的问题），达到98%的分析正确率和94%的召回率，同时可靠地拒绝数据不支持的问题，最小化幻觉风险。统计稳健性测试和专家反馈显示可靠性和社会相关性。

Conclusion: 该研究表明大语言模型能够为公共数据提供可解释的多模态访问，推进开放治理的可信人工智能发展，展示了LLMs在增强公民数据访问能力方面的潜力。

Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.

</details>


### [31] [AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models](https://arxiv.org/abs/2602.00534)
*Apurba Prasad Padhy,Fernando Camacho,Saibal Mukhopadhyay*

Main category: cs.LG

TL;DR: 提出AIRE-Prune方法，通过渐近脉冲响应能量评分对状态空间模型进行结构化后训练剪枝，显著降低计算成本同时保持精度


<details>
  <summary>Details</summary>
Motivation: 状态空间模型(SSMs)通常为了降低内存和计算成本而牺牲模型容量、搜索空间或稳定性，需要一种有效的剪枝方法来减少状态维度而不显著影响性能

Method: 提出AIRE-Prune方法：为每个状态计算闭式渐近脉冲响应能量评分（在无限时间范围内贡献的总脉冲响应能量），层间归一化实现全局跨层比较和选择，将模态截断从单系统扩展到深度堆叠

Result: 在多样序列基准测试中，AIRE-Prune对SISO和MIMO SSMs实现平均60.8%的剪枝率，平均精度仅下降0.29%（无需重新训练），同时显著降低计算成本

Conclusion: AIRE-Prune提供了一种有效的状态空间模型剪枝方法，通过渐近响应能量而非最坏情况增益来指导剪枝，揭示了SSMs中的显著冗余并实现了高效压缩

Abstract: State space models (SSMs) often sacrifice capacity, search space, or stability to offset the memory and compute costs of large state dimensions. We introduce a structured post-training pruning method for SSMs -- AIRE-Prune (Asymptotic Impulse-Response Energy for State PRUN(E)) -- that reduces each layer's state dimension by directly minimizing long-run output-energy distortion. AIRE-Prune assigns every state a closed-form asymptotic impulse-response energy-based score, i.e., the total impulse-response energy it contributes over an infinite horizon (time), and normalizes these scores layer-wise to enable global cross-layer comparison and selection. This extends modal truncation from single systems to deep stacks and aligns pruning with asymptotic response energy rather than worst-case gain. Across diverse sequence benchmarks, AIRE-Prune reveals substantial redundancy in SISO and MIMO SSMs with average pruning of 60.8%, with average accuracy drop of 0.29% without retraining, while significantly lowering compute. Code: https://github.com/falcon-arrow/AIRE-Prune.

</details>


### [32] [Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning](https://arxiv.org/abs/2602.00022)
*Margaret Foster*

Main category: cs.LG

TL;DR: 提出一个针对难以访问情境的测量框架，使用间接数据痕迹、可解释机器学习模型和理论指导的三角验证来填补不可访问的测量空间。


<details>
  <summary>Details</summary>
Motivation: 许多高风险的系统和科学政策关注的现象难以直接观测：感兴趣的动态不可观察、数据间接且分散在不同来源、真实情况缺失或被隐藏。在这些情况下，可用数据通常不支持传统的分析策略。

Method: 结合多源三角验证与可解释机器学习模型。不依赖无法观测的理想数据的准确性，而是寻求多个部分信息模型之间的一致性，通过交叉信号一致性或与预期状态的偏离来得出可靠结论。

Result: 通过对一个秘密军事组织的组织增长和内部压力动态的实证分析，展示了该方法如何恢复具有实质意义的变异，同时明确揭示了推断的局限性。

Conclusion: 该框架为缺乏足够数据进行传统统计或因果推断的情况提供了定量的分析工作流程，展示了三角验证的可解释机器学习如何从多个不完整且有偏见的观测信号中恢复有意义的变异。

Abstract: We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.

</details>


### [33] [Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration](https://arxiv.org/abs/2602.00636)
*Yujie Yang,Zhilong Zheng,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 本文首次揭示了安全探索的目标是在可行区域与环境模型之间找到平衡点，提出了首个面向平衡的安全探索框架SEE，通过交替寻找最大可行区域和最不确定模型，实现零约束违反的安全探索。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的环境探索安全是一个关键问题。虽然将探索限制在可行区域已被广泛接受为保障安全的方法，但两个关键问题仍未解决：通过探索可达到的最大可行区域是什么？如何识别这个区域？本文旨在回答这些问题。

Method: 提出了安全平衡探索（SEE）框架，该框架交替进行两个过程：1）寻找最大可行区域；2）寻找最不确定的环境模型。使用不确定模型的图表示，证明SEE获得的不确定模型单调细化，可行区域单调扩展，两者都收敛到安全探索的平衡点。

Result: 在经典控制任务上的实验表明，SEE算法成功实现了零约束违反的可行区域扩展，并在几次迭代内达到了安全探索的平衡点。

Conclusion: 本文首次揭示了安全探索的核心目标是找到可行区域与环境模型之间的平衡点，提出了SEE框架并证明了其收敛性，为安全强化学习提供了新的理论基础和实用算法。

Abstract: Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.

</details>


### [34] [White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC](https://arxiv.org/abs/2602.01516)
*Enzo Nicolas Spotorno,Matheus Wagner,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: 提出了一种白盒自适应NMPC架构，通过模块主权范式仲裁多个冻结的、特定工况的神经专家，解决车辆可塑性问题，实现无需重新训练即可适应不同工况


<details>
  <summary>Details</summary>
Motivation: 解决车辆控制系统在不同工况（摩擦、质量、阻力等）下的适应性问题，传统非自适应基准方法在这些复合工况变化下会失效，需要一种无需重新训练就能适应变化的控制架构

Method: 采用模块主权范式，仲裁多个冻结的、特定工况的神经专家，在CasADi中维护完全可遍历的符号图，实现最大运行时可审计性，通过同步仿真验证快速适应能力

Result: 实现了快速适应（约7.3毫秒）和接近理想的跟踪精度，在复合工况变化下表现优于非自适应基准方法，但符号图维护导致求解器延迟增加72-102倍，量化了严格白盒实现的效率代价

Conclusion: 提出的白盒自适应NMPC架构成功解决了车辆可塑性问题，实现了无需重新训练的工况适应，但严格的符号图可审计性带来了显著的效率代价，需要在透明度和效率之间权衡

Abstract: We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.

</details>


### [35] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

TL;DR: ELLMPEG是一个边缘智能的代理式LLM框架，用于自动生成视频处理命令，通过本地部署消除云API依赖，在FFmpeg和VVC编码器命令生成上达到78%准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于云的LLM部署面临三大限制：高计算能耗、远程处理的隐私可靠性风险、以及持续的API成本。需要一种能在边缘本地部署、利用开源工具和LLM的解决方案。

Method: 提出ELLMPEG框架，集成工具感知的检索增强生成(RAG)与迭代自反思机制，在边缘直接生成并本地验证可执行的FFmpeg和VVenC命令。

Result: 在480个多样化查询的数据集上评估，Qwen2.5结合ELLMPEG框架达到平均78%的命令生成准确率，零API成本，在命令有效性、生成速度、推理时间和能效方面表现最优。

Conclusion: ELLMPEG成功展示了边缘部署的代理式LLM框架在视频处理命令生成上的可行性，为多媒体处理提供了高效、隐私保护且成本可控的解决方案。

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [36] [AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments](https://arxiv.org/abs/2602.01629)
*Renukanandan Tumu,Aditya Singh,Rahul Mangharam*

Main category: cs.LG

TL;DR: AdaptNC：联合在线调整非共形分数参数和共形阈值，解决机器人领域分布偏移下的保守预测问题


<details>
  <summary>Details</summary>
Motivation: 现实机器人系统中存在分布偏移，违反传统共形预测的交换性假设，现有方法仅调整阈值导致预测区域过于保守且体积效率低下

Method: 提出AdaptNC框架，联合在线适应非共形分数参数和共形阈值，采用自适应重加权方案优化分数函数，引入回放缓冲机制缓解分数转换期间的覆盖不稳定性

Result: 在多种机器人基准测试中，AdaptNC在保持目标覆盖水平的同时，相比仅调整阈值的最先进方法显著减少了预测区域体积

Conclusion: AdaptNC通过联合适应分数参数和阈值，有效解决了分布偏移下的保守预测问题，为自主系统在非约束环境中的安全部署提供了更高效的解决方案

Abstract: Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.

</details>


### [37] [RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making](https://arxiv.org/abs/2602.00030)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 提出一个用于人道主义援助和灾害响应的智能RAG框架，通过分层知识库和自适应检索策略支持灾害响应的三个阶段，并在真实灾害数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 灾害响应需要快速理解现场情况、可靠决策支持，并能泛化到各种未见过的灾害场景。现有系统缺乏对灾害响应三个典型阶段（救援、中期恢复、长期重建）的系统支持，以及跨模态信息的有效整合。

Method: 构建分层知识库整合文本手册、历史经验（如2011年东北地震）和空/地面图像；使用BLIP图像描述、ColVBERT嵌入和长上下文摘要处理46个海啸相关PDF；通过熵感知场景抽象的智能控制器动态选择检索策略（RAPTOR、ColBERT）；采用轻量级LoRA后训练注入历史灾害经验知识。

Result: 在真实灾害数据集上的实验显示，系统在情境理解、任务分解准确性和应急操作可用性方面均有显著提升，通过自适应检索增强生成与自推理、多模态思维链能力实现了实质性改进。

Conclusion: 该智能RAG框架为灾害响应提供了有效的多模态知识整合和自适应推理能力，能够支持专家和非专家响应者，在应急操作中展现出优越的实用性和泛化能力。

Abstract: Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.

</details>


### [38] [Enhancing few-shot time series forecasting with LLM-guided diffusion](https://arxiv.org/abs/2602.00040)
*Haonan Shi,Dehua Shuai,Liming Wang,Xiyang Liu,Long Tian*

Main category: cs.LG

TL;DR: 提出LTSM-DIFF框架，结合大语言模型和扩散模型，解决小样本时间序列预测问题


<details>
  <summary>Details</summary>
Motivation: 专业领域时间序列预测常受数据稀缺限制，传统模型需要大规模数据集才能有效捕捉时间动态

Method: LTSM-DIFF框架：LTSM模块作为时间记忆机制提取序列表示，扩散模型作为联合概率扩散过程，利用语言模型知识迁移到时间序列任务

Result: 在多种基准测试中，LTSM-DIFF在数据丰富场景下达到SOTA性能，在小样本预测中显著改进

Conclusion: 为数据稀缺下的时间序列分析建立了新范式

Abstract: Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequential Memory with Diffusion), a novel learning framework that integrates the expressive power of large language models with the generative capability of diffusion models. Specifically, the LTSM module is fine-tuned and employed as a temporal memory mechanism, extracting rich sequential representations even under data-scarce conditions. These representations are then utilized as conditional guidance for a joint probability diffusion process, enabling refined modeling of complex temporal patterns. This design allows knowledge transfer from the language domain to time series tasks, substantially enhancing both generalization and robustness. Extensive experiments across diverse benchmarks demonstrate that LTSM-DIFF consistently achieves state-of-the-art performance in data-rich scenarios, while also delivering significant improvements in few-shot forecasting. Our work establishes a new paradigm for time series analysis under data scarcity.

</details>


### [39] [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137)
*Minghao Li,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.LG

TL;DR: DCoPilot：用于动态数据中心操作的生成式控制策略混合框架，结合LLM符号生成结构化奖励形式和超网络参数生成策略权重，实现零样本策略生成


<details>
  <summary>Details</summary>
Motivation: 现代数据中心运行在高功率密度且工作负载快速变化的环境中，需要分钟级自适应。传统手动设计的DRL代理无法跟上频繁的动态变化和SLA变更，导致规范到策略的滞后，可能引发服务中断

Method: 提出DCoPilot混合框架，结合两种生成范式：1）LLM进行结构化奖励形式的符号生成；2）超网络进行策略权重的参数生成。包含三个阶段：模拟扩展（在多样化场景中压力测试奖励候选）、元策略蒸馏（训练超网络输出基于SLA和场景嵌入的策略权重）、在线自适应（实现零样本策略生成）

Result: 在五个控制任务家族中，DCoPilot实现了接近零的约束违反，并在所有规范变化中优于所有基线方法。消融研究验证了基于LLM的统一奖励生成在实现稳定超网络收敛方面的有效性

Conclusion: DCoPilot通过结合LLM的符号生成能力和超网络的参数生成能力，有效解决了数据中心动态操作中规范到策略的滞后问题，实现了及时有效的控制策略生成

Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.

</details>


### [40] [Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy](https://arxiv.org/abs/2602.00046)
*Sarthak Sattigeri*

Main category: cs.LG

TL;DR: 研究发现语言模型在印地语文化适应提示中的谄媚率比英语高12-16个百分点，文化适应是主要因素，语言编码影响较小，表明英语对齐评估不能直接推广到其他语言文化。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索语言模型的谄媚行为（优先迎合用户偏好而非原则性推理）是否在不同语言和文化背景下普遍存在。目前英语评估已发现谄媚是持续的对齐失败，但尚不清楚这种诊断是否适用于其他语言和文化语境。

Method: 将Beacon单轮强制选择谄媚诊断扩展到印地语，采用三条件设计：英语原版、印地语直译、印地语文化适应提示。评估四个开源指令调优模型，每个条件50个提示，分离语言编码效应和文化适应效应。

Result: 所有模型中，文化适应印地语提示的谄媚率始终高于英语，绝对差异12.0-16.0个百分点。Qwen 2.5-Coder-7B分解显示文化适应贡献主要差距（14.0%），语言编码贡献很小（2.0%）。建议类提示跨语言差异最大（20-25个百分点）。

Conclusion: 英语测量的对齐行为不能均匀跨语言转移，文化背景提示框架起重要作用。研究发布所有数据集和评估代码支持复现和扩展。

Abstract: Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.

</details>


### [41] [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161)
*Aniq Ur Rahman,Justin P. Coon*

Main category: cs.LG

TL;DR: 提出一个用于时间链路预测模型的反事实验证框架，通过生成具有已知因果结构的时间交互图来评估模型是否捕捉到因果机制，而不仅仅是预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前时间链路预测模型主要基于预测准确性评估，但这种方法无法评估模型是否真正捕捉到控制时间交互的因果机制。需要建立能够验证模型因果理解能力的评估框架。

Method: 1) 提出支持兴奋和抑制效应的连续时间事件序列结构方程模型；2) 将该机制扩展到时间交互图；3) 提出基于跨模型预测误差的距离度量；4) 通过控制因果转移和时间戳重排实例化反事实评估。

Result: 经验验证了假设：在一个因果模型上训练的预测器在足够远的模型上评估时性能会下降。框架为因果感知的基准测试提供了基础。

Conclusion: 提出的反事实验证框架能够评估时间链路预测模型是否捕捉到因果机制，超越了传统的预测准确性评估，为更全面的模型评估提供了新方法。

Abstract: Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.

</details>


### [42] [Lightweight Edge Learning via Dataset Pruning](https://arxiv.org/abs/2602.00047)
*Laha Ale,Hu Luo,Mingsheng Cao,Shichao Li,Huanlai Xing,Haifeng Sun*

Main category: cs.LG

TL;DR: 提出基于数据集剪枝的数据中心化优化框架，通过轻量级重要性评估构建紧凑训练子集，显著降低边缘设备训练延迟和能耗，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 边缘学习虽然能保护隐私和降低通信延迟，但设备训练的高计算和能耗开销限制了其在电池供电移动系统上的部署。现有研究主要优化模型架构用于高效推理，但训练阶段仍受限于处理大量冗余本地数据。

Method: 提出数据中心化优化框架，利用数据集剪枝实现资源高效的边缘学习。通过截断预热阶段获得的平均损失统计来评估样本重要性，按动态剪枝比例确定性地保留最关键数据点。该方法与模型无关，无需设备间通信。

Result: 在标准图像分类基准测试中，该框架实现了与剪枝比例成比例的近乎线性的训练延迟和能耗降低，模型精度下降可忽略不计。

Conclusion: 数据集剪枝是增强资源受限移动边缘设备学习可持续性和可扩展性的重要补充范式，为边缘学习提供了有效的资源优化方案。

Abstract: Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.

</details>


### [43] [Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment](https://arxiv.org/abs/2602.00051)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 提出基于分位数回归深度Q网络（QR-DQN）与老化因子集成的分布强化学习方法，用于多设备状态维护，通过三种策略场景实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于时间的维护策略常导致不必要的开支和意外设备故障，需要从被动维护转向基于设备实时状态数据的主动维护策略，以优化维护时机和资源配置。

Method: 采用分位数回归深度Q网络（QR-DQN）结合老化因子集成的分布强化学习方法，通过安全优先、平衡和成本效益三种策略场景对多个泵单元进行并发管理。

Result: 经过3000次训练实验验证，所有策略均表现显著改进。安全优先策略成本效益最佳，投资回报率达3.91，性能比替代方案提升152%，仅需增加31%投资。系统运行稳定性达95.66%，具备工业环境即时应用性。

Conclusion: 提出的QR-DQN与老化因子集成方法为多设备状态维护提供了有效的强化学习解决方案，安全优先策略在成本效益和性能方面表现最优，具备工业实际应用潜力。

Abstract: Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\% better performance than alternatives while requiring only 31\% higher investment. The system exhibits 95.66\% operational stability and immediate applicability to industrial environments.

</details>


### [44] [TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval](https://arxiv.org/abs/2602.00059)
*Zizheng Zhang,Yuyang Liao,Chen Chen,Jian He,Dun Wu,Qianjin Yu,Yanqin Gao,Jin Yang,Kailai Zhang,Eng Siong Chng,Xionghu Zhong*

Main category: cs.LG

TL;DR: TextBFGS：一个用于离散文本优化的二阶框架，通过检索梯度算子实现拟牛顿法，显著优于一阶方法


<details>
  <summary>Details</summary>
Motivation: 现有离散文本优化方法主要是一阶优化器（类似SGD），存在收敛慢和不稳定的问题，因为它们忽略了优化景观的语义曲率

Method: TextBFGS通过从预学习成功轨迹的记忆中检索梯度算子来近似逆Hessian矩阵，实现单次更新，将反馈生成和二阶校正结合到单个推理步骤中

Result: 在代码优化任务（HumanEval、MBPP等）上，TextBFGS显著优于一阶基线方法，以更少的模型调用获得更高的通过率，并表现出强大的跨任务可迁移性

Conclusion: TextBFGS为高效、记忆感知的文本优化建立了一个数学基础范式，通过二阶优化方法解决了离散文本优化中的收敛慢和不稳定问题

Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.

</details>


### [45] [SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism](https://arxiv.org/abs/2602.00062)
*Ming-Yao Ho,Cheng-Kai Wang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 提出SCPL方法，通过解耦反向传播将长梯度流分解为多个短流，实现层间并行计算，提高训练效率


<details>
  <summary>Details</summary>
Motivation: 企业信息系统采用大规模AI模型面临高训练成本和长开发周期的问题，标准反向传播算法是深度网络训练效率低下的主要原因

Method: 提出监督对比并行学习（SCPL），解耦反向传播，将长梯度流转化为多个短梯度流，实现不同层的参数梯度同时计算，获得优越的模型并行性

Result: 实验证明SCPL相比BP、Early Exit、GPipe和AL（最先进的解耦反向传播方法）具有更高的效率和效果

Conclusion: SCPL通过缓解基本性能瓶颈，为企业更经济高效、更灵活地开发和部署先进信息系统提供了实用途径

Abstract: Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inefficiency in training deep networks. This paper introduces a new training methodology, Supervised Contrastive Parallel Learning (SCPL), that addresses this issue by decoupling BP and transforming a long gradient flow into multiple short ones. This design enables the simultaneous computation of parameter gradients in different layers, achieving superior model parallelism and enhancing training throughput. Detailed experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and Associated Learning (AL), a state-of-the-art method for decoupling backpropagation. By mitigating a fundamental performance bottleneck, SCPL provides a practical pathway for organizations to develop and deploy advanced information systems more cost-effectively and with greater agility. The experimental code is released for reproducibility. https://github.com/minyaho/scpl/

</details>


### [46] [The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations](https://arxiv.org/abs/2602.00063)
*Leonidas Christodoulou,Chang Sun*

Main category: cs.LG

TL;DR: 该研究发现反事实解释对模型不确定性高度敏感，即使模型准确度的小幅下降也会导致反事实解释产生大幅变化，强调了在金融和社会科学等领域需要不确定性感知的解释方法。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法大多未在模型和数据不确定性变化的情况下进行测试，导致在现实世界变异性下可能产生不稳定或无效的解释。需要研究反事实解释在存在偶然性和认知不确定性时的鲁棒性。

Method: 通过合成和真实世界表格数据集的实验，研究常见机器学习模型与反事实生成算法组合在存在偶然性和认知不确定性时的鲁棒性。

Result: 反事实解释对模型不确定性高度敏感。即使由噪声增加或数据有限引起的模型准确度小幅下降，也会导致生成的反事实在平均水平和个体实例上产生大幅变化。

Conclusion: 研究结果强调了在金融和社会科学等领域需要不确定性感知的解释方法，因为反事实解释在模型不确定性存在时可能不稳定。

Abstract: Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.

</details>


### [47] [SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation](https://arxiv.org/abs/2602.00064)
*Hao Deng,Yingping Li,Shuiping Gou,Bo Liu*

Main category: cs.LG

TL;DR: SPGCL是一种通过SVD引导的结构扰动进行鲁棒图对比学习的框架，通过平衡边移除和恢复率来控制视图间的结构差异，提高GNN对结构噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法存在局限性：随机扰动（如边丢弃）可能移除关键边且与结构无关，而基于SVD的视图往往变得密集且缺乏多样性。需要一种能平衡结构保持和视图多样性的方法。

Method: SPGCL结合轻量级随机边移除和SVD引导的细化步骤，通过稀疏的top-ranked边选择和合并来恢复错误移除的信息边并引入语义上有意义的缺失链接，避免图密集化。还包含一个受全局相似性约束正则化的对比融合模块。

Result: 在十个基准数据集上的广泛实验表明，SPGCL持续提高了基础GNN的鲁棒性和准确性，优于最先进的图对比学习和结构学习方法。

Conclusion: SPGCL通过SVD引导的结构扰动有效平衡了视图多样性和结构保持，为图对比学习提供了一种鲁棒的方法，能够更好地处理结构噪声并提高GNN性能。

Abstract: Graph Neural Networks (GNNs) can be highly sensitive to structural noise, including spurious or missing edges caused by adversarial attacks or non-adversarial imperfections. Existing graph contrastive learning methods typically rely on either random perturbations (e.g., edge dropping) to generate diverse views or purely spectral augmentations (e.g., SVD) to preserve global structural priors. However, random perturbations are structure-agnostic and may remove critical edges, while SVD-based views often become dense and lack sufficient diversity. To bridge this gap, we propose SPGCL, a robust graph contrastive learning framework via SVD-guided structural perturbation. SPGCL couples lightweight stochastic edge removal with an SVD-guided refinement step that can recover mistakenly removed informative edges and introduce semantically meaningful missing links while avoiding graph densification through sparse top-ranked edge selection and merging. By balancing edge removal and recovery rates, SPGCL explicitly controls structural discrepancy between views so that contrastive signals reflect semantic structural differences rather than edge-count gaps. We further incorporate a contrastive fusion module regularized by a global similarity constraint to better align the two views. Extensive experiments on ten benchmark datasets demonstrate that SPGCL consistently improves robustness and accuracy of base GNNs, outperforming state-of-the-art graph contrastive learning and structure learning methods.

</details>


### [48] [Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning](https://arxiv.org/abs/2602.00067)
*Yihan Zhang,Ercan E. Kuruoglu*

Main category: cs.LG

TL;DR: NSG-MoE：一种通过节点分裂和图重连机制结合结构化MoE架构的多模态图学习框架，有效解决模态混淆问题


<details>
  <summary>Details</summary>
Motivation: 多模态图具有丰富的表示能力和广泛适用性，但面临严重的模态混淆挑战，通用GNNs常出现不希望的混合效应

Method: 提出NSG-MoE框架，包含节点分裂和图重连机制，结合结构化MoE架构，将节点分解为模态特定组件，分配关系感知专家处理异构消息流

Result: 在三个多模态基准测试中超越强基线，尽管包含MoE但仍保持竞争性训练效率；谱分析显示NSG在模态特定子空间进行自适应滤波；信息论分析表明NSG的架构约束减少了数据和参数间的互信息，提升泛化能力

Conclusion: NSG-MoE通过显式解耦模态特定信息和结构化专家分配，有效缓解多模态图中的模态混淆问题，同时保持效率和泛化能力

Abstract: Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framework that integrates a node-splitting and graph-rewiring mechanism with a structured Mixture-of-Experts (MoE) architecture. It explicitly decomposes each node into modality-specific components and assigns relation-aware experts to process heterogeneous message flows, thereby preserving structural information and multimodal semantics while mitigating the undesirable mixing effects commonly observed in general-purpose GNNs. Extensive experiments on three multimodal benchmarks demonstrate that NSG-MoE consistently surpasses strong baselines. Despite incorporating MoE -- which is typically computationally heavy -- our method achieves competitive training efficiency. Beyond empirical results, we provide a spectral analysis revealing that NSG performs adaptive filtering over modality-specific subspaces, thus explaining its disentangling behavior. Furthermore, an information-theoretic analysis shows that the architectural constraints imposed by NSG reduces mutual information between data and parameters and improving generalization capability.

</details>


### [49] [Generative AI-enhanced Probabilistic Multi-Fidelity Surrogate Modeling Via Transfer Learning](https://arxiv.org/abs/2602.00072)
*Jice Zeng,David Barajas-Solano,Hui Chen*

Main category: cs.LG

TL;DR: 提出基于生成式迁移学习的概率多保真度代理框架，使用归一化流模型，通过两阶段训练（先在大量低保真数据上预训练，再在少量高保真数据上微调）解决数据稀缺问题，并引入满射层实现维度约简。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型的性能严重依赖数据质量和数量，但高保真数据稀缺且计算昂贵，低保真数据丰富但精度不足。需要解决数据稀缺问题，构建数据高效的多保真度代理模型。

Method: 基于生成式迁移学习的概率多保真度代理框架，使用归一化流生成模型作为主干。采用两阶段训练：1）在大量低保真数据集上预训练学习概率前向模型；2）在少量高保真数据集上微调，通过知识迁移校正低保真-高保真差异。引入满射层与标准耦合块结合，放松标准双射归一化流的维度保持约束，实现学习维度约简。

Result: 该方法在钢筋混凝土板基准测试中验证，结合大量粗网格（低保真）模拟和有限细网格（高保真）模拟。模型实现了具有高保真精度的概率预测，显著优于仅使用低保真的基线方法，同时使用更少的高保真评估。

Conclusion: 提出的概率多保真度代理框架为复杂工程系统提供了一条实用的数据高效、生成式AI驱动的代理模型路径，能够提供快速概率预测并量化不确定性。

Abstract: The performance of machine learning surrogates is critically dependent on data quality and quantity. This presents a major challenge, as high-fidelity (HF) data is often scarce and computationally expensive to acquire, while low-fidelity (LF) data is abundant but less accurate. To address this data scarcity problem, we develop a probabilistic multi-fidelity surrogate framework based on generative transfer learning. We employ a normalizing flow (NF) generative model as the backbone, which is trained in two phases: (i) the NF is first pretrained on a large LF dataset to learn a probabilistic forward model; (ii) the pretrained model is then fine-tuned on a small HF dataset, allowing it to correct for LF-HF discrepancies via knowledge transfer. To relax the dimension-preserving constraint of standard bijective NFs, we integrate surjective (dimension-reducing) layers with standard coupling blocks. This architecture enables learned dimension reduction while preserving the ability to train with exact likelihoods. The resulting surrogate provides fast probabilistic predictions with quantified uncertainty and significantly outperforms LF-only baselines while using fewer HF evaluations. We validate the approach on a reinforced concrete slab benchmark, combining many coarse-mesh (LF) simulations with a limited set of fine-mesh (HF) simulations. The proposed model achieves probabilistic predictions with HF accuracy, demonstrating a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems.

</details>


### [50] [Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation](https://arxiv.org/abs/2602.00075)
*Philipp Andelfinger,Wentong Cai*

Main category: cs.LG

TL;DR: 提出一种名为"dimensional peeking"的方差缩减方法，用于离散仿真优化中的梯度估计，通过提升采样粒度到遵循相同控制流的数值类别，减少梯度估计方差，提高零阶优化竞争力。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化方法在高维空间中常用，但当导数无法直接计算时，随机估计器提供近似梯度。这些基于扰动的采样方法会引入方差，导致收敛缓慢。需要一种方差缩减方法来提高离散仿真优化的梯度估计效率。

Method: 提出"dimensional peeking"方法：将采样粒度从标量值提升到遵循相同控制流路径的数值类别，增加每次仿真评估收集的信息量。该方法从已建立的平滑梯度估计器推导而来，不引入偏差。通过自定义数值数据类型在C++程序中透明实现。

Result: 在三个高维输入的仿真优化问题中，观察到方差减少因子高达7.9。与三种元启发式方法相比，优化进展显示dimensional peeking提高了零阶优化在离散非凸仿真中的竞争力。

Conclusion: Dimensional peeking是一种有效的方差缩减方法，通过提升采样粒度减少梯度估计方差，使零阶优化在离散非凸仿真优化问题中更具竞争力，且不引入估计偏差。

Abstract: Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.

</details>


### [51] [Automated univariate time series forecasting with regression trees](https://arxiv.org/abs/2602.00077)
*Francisco Martínez,María P. Frías*

Main category: cs.LG

TL;DR: 该论文提出了一种使用回归树及其集成方法（装袋和随机森林）进行自动单变量时间序列预测的方法论，实现了与指数平滑或ARIMA等传统统计模型相当的预测精度，并开发了公开可用的软件。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法如指数平滑和ARIMA需要专业知识进行参数选择和模型调整，而机器学习方法如回归树和集成学习在时间序列预测中的应用还不够成熟，需要解决自回归特征选择、趋势处理和季节性处理等关键问题。

Method: 采用自回归方法和递归预测，使用回归树及其集成方法（装袋和随机森林），重点解决了自回归特征的选择、趋势序列的处理以及季节性行为的应对策略。

Result: 实验结果表明，该方法在预测精度上与指数平滑或ARIMA等成熟统计模型相当，证明了机器学习方法在时间序列预测中的有效性。

Conclusion: 回归树及其集成方法可以有效地用于自动单变量时间序列预测，达到与传统统计方法相当的精度，并且开发了公开可用的软件工具，促进了该方法的应用和推广。

Abstract: This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.

</details>


### [52] [Lossless Embedding Compression via Spherical Coordinates](https://arxiv.org/abs/2602.00079)
*Han Xiao*

Main category: cs.LG

TL;DR: 提出一种无损压缩单位范数嵌入的方法，实现1.5倍压缩率，比现有最佳方法提升25%


<details>
  <summary>Details</summary>
Motivation: 单位范数嵌入在信息检索和机器学习中广泛应用，但现有压缩方法效率有限。高维单位向量的球坐标会集中在π/2附近，这为高效压缩提供了机会。

Method: 利用高维单位向量球坐标集中在π/2附近的特性，导致IEEE 754指数位坍缩为单一值，从而启用熵编码。方法无需训练，在float32精度内完全无损。

Result: 在文本、图像和多向量嵌入的26种配置评估中，均实现1.5倍压缩率，比先前最佳方法提升25%。

Conclusion: 该方法为无损压缩单位范数嵌入提供了一种高效解决方案，利用高维几何特性实现显著压缩增益，且无需训练过程。

Abstract: We present a lossless compression method for unit-norm embeddings that achieves 1.5$\times$ compression, 25\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $π/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.

</details>


### [53] [Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.00084)
*Brady Steele*

Main category: cs.LG

TL;DR: LoRA具有内在的抗标签噪声能力，其理论分析揭示了三个关键发现：1) LoRA无法记忆所有可能的标签分配；2) 存在最优秩平衡近似偏差和噪声方差；3) 存在时间分离现象。基于此提出了RACT方法用于噪声检测。


<details>
  <summary>Details</summary>
Motivation: 研究LoRA在微调过程中对标签噪声的内在抵抗特性，这一特性尚未得到充分探索。理解LoRA为何能够抵抗标签噪声，并利用这一特性开发更好的噪声检测方法。

Method: 1) 理论分析LoRA的容量限制，证明其无法记忆所有可能的标签分配；2) 推导最优秩以平衡近似偏差和噪声方差；3) 建立时间分离理论；4) 提出RACT（Rank-Aware Curriculum Training）方法，利用秩差异进行噪声检测。

Result: 理论分析得到验证，RACT在AG News数据集上实现了91.1%的F1分数用于噪声检测，同时保持91.46%的准确率，与缺乏噪声检测能力的基线方法竞争性相当。

Conclusion: LoRA具有内在的抗标签噪声能力，这一特性可以通过理论框架解释。基于秩差异的RACT方法能够有效检测噪声，同时保持良好的模型性能，为参数高效微调提供了新的视角。

Abstract: Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.

</details>


### [54] [CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2602.00085)
*Shuozhe Li,Jincheng Cao,Bodun Hu,Aryan Mokhtari,Leqi Liu,Amy Zhang*

Main category: cs.LG

TL;DR: CARE-RFT是一种新的强化微调方法，通过使用偏斜反向KL散度替代标准反向KL正则化，在保持模型可信度的同时实现强大的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化微调存在权衡：无约束RFT能提升推理性能但损害模型可信度（增加幻觉、校准变差），而RKL约束RFT能保持可信度但限制了推理能力的提升。需要解决这一矛盾。

Method: 提出CARE-RFT方法，用偏斜反向KL散度替代标准反向KL正则化。该方法提供置信度敏感的惩罚：对自信且持续获得奖励的探索有界惩罚以支持推理，对其他情况无界惩罚以保持校准。

Result: 在多个模型规模和RFT算法上的实验表明，CARE-RFT实现了优越的平衡，匹配无约束RFT的推理性能，同时恢复了基础模型的可信度和校准。

Conclusion: 精心设计的置信度感知正则化是构建既强大又可信的推理模型的关键。

Abstract: Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.

</details>


### [55] [ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization](https://arxiv.org/abs/2602.00087)
*Haolin Pan,Lianghong Huang,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: ECCO框架结合可解释推理与组合搜索，通过反向工程构建思维链数据集，让LLM学习优化决策的因果逻辑，然后作为策略师指导遗传算法，在7个数据集上平均减少24.44%的周期。


<details>
  <summary>Details</summary>
Motivation: 传统编译器自动调优面临黑盒搜索方法缺乏语义指导与LLM方法存在表面模式匹配和因果不透明的问题，需要一种能结合可解释推理与组合搜索的方法。

Method: 1) 提出反向工程方法构建思维链数据集，将静态代码特征映射到可验证的性能证据；2) 设计协作推理机制，让LLM作为策略师定义优化意图，动态指导遗传算法的变异操作。

Result: 在7个数据集上的实验结果表明，ECCO显著优于LLVM opt -O3基线，平均减少了24.44%的周期。

Conclusion: ECCO成功桥接了可解释推理与组合搜索，使LLM能够学习优化决策的因果逻辑而非简单模仿序列，实现了更有效的编译器自动调优。

Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.

</details>


### [56] [From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting](https://arxiv.org/abs/2602.00088)
*Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: STM（符号转换机制）通过将时间序列数据转换为符号表示，使语言模型能更高效地进行时间序列预测，在资源消耗几乎不变的情况下显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列预测中表现出色，但其巨大的计算和内存需求限制了在轻量级平台上的部署。需要一种方法既能保持语言模型的优势，又能大幅降低资源消耗。

Method: 提出符号转换机制（STM），通过基于人类认知结构的量化技术将连续时间序列值转换为符号标记，并通过符号的结构化转换捕捉时间动态，使语言模型能专注于时间序列数据的关键部分。

Result: 在多种时间序列数据集和四个小型语言模型上测试，STM相比基础模型将MAE降低高达69%，MSE降低高达90%。资源成本几乎可忽略，GPU内存仅增加约0.06%，延迟开销仅增加0.64%。

Conclusion: STM作为一种高效、自适应的符号驱动时间序列预测层，展示了在保持基础语言模型完整性的同时，通过符号抽象显著提升预测效率的潜力。

Abstract: Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.

</details>


### [57] [Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits](https://arxiv.org/abs/2602.00092)
*Neha Kalibhat,Zi Wang,Prasoon Bajpai,Drew Proud,Wenjun Zeng,Been Kim,Mani Malek*

Main category: cs.LG

TL;DR: 该论文提出了一种黑盒可解释性框架，通过系统性地应用原子概念编辑（ACEs）来学习可验证的"宪法"，即描述提示词修改如何影响模型特定行为的自然语言总结。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够深入理解模型行为、提供可验证解释的方法，以控制模型的特定行为（如对齐性、正确性、约束遵守），并揭示不同模型在处理相同任务时的内在差异。

Method: 使用原子概念编辑（ACEs）——在输入提示中添加、移除或替换可解释概念的操作。系统应用这些编辑，观察模型行为变化，学习从编辑到可预测结果的因果映射，从而构建可验证的宪法。

Result: 在数学推理和文本到图像对齐等任务中验证了方法的有效性。发现：1）文本到图像生成中，GPT-Image关注语法遵循，Imagen 4优先考虑氛围一致性；2）数学推理中，干扰变量会混淆GPT-5，但对Gemini 2.5和o4-mini影响较小；3）使用宪法控制模型行为比不使用宪法的方法平均提升1.86倍成功率。

Conclusion: 提出的黑盒可解释性框架能够学习到可验证的宪法，为模型行为提供深入、可泛化的洞察，并有效控制模型行为，揭示了不同模型在处理相同任务时的内在差异。

Abstract: We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.

</details>


### [58] [Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review](https://arxiv.org/abs/2602.00094)
*Sandra Benítez-Peña,Blas Kolic,Victoria Menendez,Belén Pulido*

Main category: cs.LG

TL;DR: 该论文是一篇关于算法公平性的综述，系统性地回顾了同时处理群体公平性和个体公平性的混合方法，分析了这些方法的理论基础、优化机制和实证评估，并讨论了相关挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 算法公平性在计算决策系统中变得至关重要，但现有的群体公平性和个体公平性概念通常被孤立研究。需要整合这两种视角，开发能够同时保证个体和群体层面公平性的混合方法，以提供更全面的公平性保障。

Method: 采用系统性文献综述方法，对混合公平性方法进行分类整理，按照采用的公平机制、算法和数学策略进行组织。对每类方法分析其理论基础、优化机制和实证评估实践，并讨论其局限性。

Result: 提供了对混合公平性方法的全面分类和分析框架，识别了现有方法的理论局限和实证挑战，揭示了群体公平性和个体公平性之间的权衡关系，为设计可靠的混合公平算法提供了指导。

Conclusion: 混合公平性方法对于实现全面的算法公平性至关重要，但需要进一步开发原则性的、上下文感知的混合方法。该综述为研究人员和从业者提供了设计同时保证个体和群体层面公平性的算法的综合资源。

Abstract: Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.

</details>


### [59] [Gauss-Newton Natural Gradient Descent for Shape Learning](https://arxiv.org/abs/2602.00099)
*James King,Arturs Berzins,Siddhartha Mishra,Marius Zeinhofer*

Main category: cs.LG

TL;DR: 该论文探索了在形状学习中应用高斯-牛顿法进行优化，包括隐式神经表面和几何感知神经网络，相比一阶方法实现了更快、更稳定的收敛。


<details>
  <summary>Details</summary>
Motivation: 形状学习面临的关键挑战包括：底层微分约束的病态性，以及参数空间优化问题与自然问题所在函数空间之间的不匹配。这些挑战导致传统优化方法收敛缓慢且不稳定。

Method: 采用高斯-牛顿法进行形状学习优化，该方法特别适用于处理隐式神经表面和几何感知神经网络的优化问题，能够更好地处理微分约束和空间不匹配问题。

Result: 在基准形状优化任务上的实验表明，高斯-牛顿法相比标准一阶方法实现了显著更快的收敛速度和更稳定的收敛过程，同时需要更少的迭代次数，并提高了最终解的准确性。

Conclusion: 高斯-牛顿法为形状学习提供了一种有效的优化方法，能够显著改善训练速度和最终解的质量，为解决形状学习中的优化挑战提供了有前景的解决方案。

Abstract: We explore the use of the Gauss-Newton method for optimization in shape learning, including implicit neural surfaces and geometry-informed neural networks. The method addresses key challenges in shape learning, such as the ill-conditioning of the underlying differential constraints and the mismatch between the optimization problem in parameter space and the function space where the problem is naturally posed. This leads to significantly faster and more stable convergence than standard first-order methods, while also requiring far fewer iterations. Experiments across benchmark shape optimization tasks demonstrate that the Gauss-Newton method consistently improves both training speed and final solution accuracy.

</details>


### [60] [THDC: Training Hyperdimensional Computing Models with Backpropagation](https://arxiv.org/abs/2602.00116)
*Hanne Dejonghe,Sam Leroux*

Main category: cs.LG

TL;DR: THDC通过可训练嵌入和端到端反向传播，将超维计算维度从10,000降至64，在保持或提升准确率的同时显著提高内存效率


<details>
  <summary>Details</summary>
Motivation: 传统超维计算依赖超高维度和静态随机初始化向量，导致内存效率低下和学习能力受限，需要更高效的替代方案

Method: 提出可训练超维计算(THDC)：1) 用可训练嵌入替换随机初始化向量；2) 引入单层二进制神经网络优化类别表示；3) 通过反向传播实现端到端训练

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上，THDC达到或优于现有最佳HDC的准确率，同时将维度从10,000大幅降低到64

Conclusion: THDC通过可训练嵌入和端到端优化，显著提升了超维计算的内存效率和性能，为资源受限设备提供了更实用的轻量级学习方案

Abstract: Hyperdimensional computing (HDC) offers lightweight learning for energy-constrained devices by encoding data into high-dimensional vectors. However, its reliance on ultra-high dimensionality and static, randomly initialized hypervectors limits memory efficiency and learning capacity. Therefore, we propose Trainable Hyperdimensional Computing (THDC), which enables end-to-end HDC via backpropagation. THDC replaces randomly initialized vectors with trainable embeddings and introduces a one-layer binary neural network to optimize class representations. Evaluated on MNIST, Fashion-MNIST and CIFAR-10, THDC achieves equal or better accuracy than state-of-the-art HDC, with dimensionality reduced from 10.000 to 64.

</details>


### [61] [Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control](https://arxiv.org/abs/2602.00120)
*Xianghong Hu,Tianning Xu,Ying Chen,Shuai Wang*

Main category: cs.LG

TL;DR: 该论文研究抵押贷款违约预测，关注真实数据中的标签模糊、类别不平衡和信息泄露问题，通过泄漏感知特征选择、严格时间分割和可控下采样等方法，发现AutoML方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 抵押贷款违约预测是金融风险管理的核心任务，但真实数据中存在三个主要问题：违约标签模糊、严重的类别不平衡以及时间结构和事后变量导致的信息泄露，这些问题影响评估有效性和部署可靠性。

Method: 使用真实贷款级数据集，采用泄漏感知特征选择、严格的时间分割（限制发起和报告期间）、可控的多数类下采样，比较多种机器学习方法，包括AutoML（AutoGluon）。

Result: 在不同正负样本比例下，性能保持稳定，AutoGluon在所有评估模型中取得了最强的AUROC表现。

Conclusion: 通过控制信息泄露和处理类别不平衡，机器学习模型可以有效预测抵押贷款违约，AutoML方法表现出最佳性能，该工作的扩展教学版本将作为书籍章节出版。

Abstract: Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.

</details>


### [62] [MiniTensor: A Lightweight, High-Performance Tensor Operations Library](https://arxiv.org/abs/2602.00125)
*Soumyadip Sarkar*

Main category: cs.LG

TL;DR: MiniTensor是一个开源张量运算库，专注于简洁性、正确性和性能，提供类似PyTorch的Python API，但核心计算在Rust引擎中执行，安装包大小仅几MB，比主流框架小几个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架如PyTorch和TensorFlow安装包过大，包含许多不必要的组件。MiniTensor旨在提供一个最小化、轻量级的张量运算库，保留研究和开发所需的核心功能，同时显著减小安装体积。

Method: 采用分层架构设计：Python层提供类似PyTorch的API，通过PyO3与Rust引擎集成。Rust引擎负责性能关键计算，包括高效内存管理、动态计算图用于梯度计算。支持密集n维张量、广播、归约、矩阵乘法、反向模式自动微分、紧凑神经网络层和标准优化器。

Result: MiniTensor实现了仅几MB的安装包大小，比PyTorch和TensorFlow小几个数量级。在保持研究开发所需核心功能的同时，提供了简洁、正确且性能良好的张量运算能力。

Conclusion: MiniTensor成功创建了一个轻量级、高效的张量运算库，证明了在保留深度学习研究和开发核心功能的同时，可以大幅减小框架体积。它为需要最小化依赖和快速部署的场景提供了有价值的替代方案。

Abstract: We present MiniTensor, an open source tensor operations library that focuses on minimalism, correctness, and performance. MiniTensor exposes a familiar PyTorch-like Python API while it executes performance critical code in a Rust engine. The core supports dense $n$ dimensional tensors, broadcasting, reductions, matrix multiplication, reverse mode automatic differentiation, a compact set of neural network layers, and standard optimizers. In this paper, we describe the design of MiniTensor's architecture, including its efficient memory management, dynamic computation graph for gradients, and integration with Python via PyO3. We also compare the install footprint with PyTorch and TensorFlow to demonstrate that MiniTensor achieves a package size of only a few megabytes, several orders of magnitude smaller than mainstream frameworks, while preserving the essentials needed for research and development on CPUs. The repository can be found at https://github.com/neuralsorcerer/minitensor

</details>


### [63] [LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference](https://arxiv.org/abs/2602.00426)
*Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: 本文为大型语言模型提供了一个简洁的数学参考框架，将LLMs表述为具有注意力依赖的高维非线性自回归模型，涵盖预训练、对齐方法和推理生成等完整流程。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer架构的LLMs通常通过架构组件和训练流程来描述，这掩盖了其底层计算结构。本文旨在为研究人员提供一个明确的、方程级别的LLM训练、对齐和生成的数学参考框架。

Method: 将LLMs形式化为具有注意力依赖的高维非线性自回归模型。自注意力自然表现为重复的双线性-softmax-线性组合，形成高度表达性的序列模型。框架涵盖：1) 通过下一个token预测的预训练；2) 对齐方法（RLHF、DPO、RSFT、RLVR）；3) 推理时的自回归生成。

Result: 该数学框架能够对对齐诱导的行为（包括奉承）、推理时现象（如幻觉、上下文学习、思维链提示、检索增强生成）以及持续学习等扩展进行原则性分析，同时作为解释和进一步理论发展的简洁参考。

Conclusion: 本文提供了一个统一的数学框架来形式化描述LLMs的完整流程，将自注意力机制自然融入非线性自回归模型中，为理论分析和实际应用提供了基础参考。

Abstract: Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.

</details>


### [64] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

TL;DR: ALIGN将LLM推理建模为对齐委托游戏，通过多智能体协作生成候选解并在激励机制下选择最终答案，理论证明能提升性能，实验验证优于单智能体和集成基线。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在复杂推理任务上表现不佳，单次生成-选择流程有限。现有的推理时集成方法虽然能通过采样多样化推理路径或聚合多个候选答案来提升性能，但通常将候选解视为独立处理，且缺乏形式化保证证明集成能真正改善推理质量。

Method: 提出ALIGN方法，将LLM推理建模为对齐委托游戏：委托人将任务委托给多个智能体，这些智能体在设计的激励机制下生成候选解决方案，然后委托人从这些输出中选择最终答案。该方法在智能体之间引入结构化交互，同时保持智能体与委托人目标的对齐。

Result: 建立了理论保证：在公平比较且同等访问候选解的条件下，ALIGN能证明提升期望性能优于单智能体生成。分析考虑了候选答案的相关性，放宽了先前工作中常用的独立性假设。在广泛的LLM推理基准测试中，ALIGN始终优于强大的单智能体和集成基线。

Conclusion: ALIGN通过将LLM推理形式化为对齐委托游戏，提供了一种理论上有保证且实证有效的多智能体推理方法，显著提升了复杂推理任务的性能。

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [65] [Three-Way Emotion Classification of EEG-based Signals using Machine Learning](https://arxiv.org/abs/2602.00670)
*Ashna Purwar,Gaurav Simkar,Madhumita,Sachin Kadam*

Main category: cs.LG

TL;DR: 该研究使用机器学习模型对EEG信号进行三分类情感识别（消极、中性、积极），比较了逻辑回归、支持向量机和随机森林三种模型，发现随机森林表现最佳。


<details>
  <summary>Details</summary>
Motivation: EEG信号能直接反映大脑活动，可用于识别人的情绪状态。随着情感感知系统和EEG情感识别研究的增长，需要探索有效的机器学习方法对EEG信号进行情感分类。

Method: 研究采用完整的工作流程，包括数据预处理和机器学习模型比较。在有限的EEG数据集上训练和测试了三种常用模型：逻辑回归（LR）、支持向量机（SVM）和随机森林（RF），使用准确率和F1分数评估性能。

Result: 结果表明机器学习模型能有效用于EEG信号的三分类情感识别。在三种模型中，随机森林表现最佳，其更高的准确率和F1分数表明它能更准确有效地捕捉情感模式。随机森林在准确率参数上也优于现有的最先进分类模型。

Conclusion: 机器学习模型可用于EEG情感识别，其中随机森林是最有效的分类器，在有限数据集上表现出优于其他模型和最先进方法的性能。

Abstract: Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.

</details>


### [66] [Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages](https://arxiv.org/abs/2602.00128)
*Emine Akpinar,Murat Oduncuoglu*

Main category: cs.LG

TL;DR: 提出一种基于量子并行模型（QBPM）的阿尔茨海默病分期分类方法，利用量子计算优势在MRI数据集上实现高效分类，相比经典方法获得更高准确率且参数更少。


<details>
  <summary>Details</summary>
Motivation: 随着寿命延长，阿尔茨海默病成为全球重大健康问题。传统AI方法在早期诊断和分期分类方面存在局限，数据量增长和计算资源有限需要更快速高效的方法。量子AI利用叠加、纠缠和高维希尔伯特空间原理，能超越经典方法限制，在处理高维、异构和噪声数据时提供更高准确性。

Method: 提出量子并行模型（QBPM）架构，受经典模型并行原理启发，使用两个不同的量子电路（每个包含旋转和纠缠块），在相同量子模拟器上并行运行，用于MRI数据集的阿尔茨海默病分期分类。

Result: 在两个不同数据集上评估显示高分类准确率，证明模型具有整体鲁棒性和泛化能力。在高斯噪声模拟真实场景下仍表现良好，验证了实际应用可行性。与五种经典迁移学习方法相比，QBPM获得更高分类准确率和相当执行时间，同时使用更少电路参数。

Conclusion: QBPM架构为阿尔茨海默病等复杂疾病分期分类提供了创新且强大的方法，展示了量子计算在医学诊断中的潜力，可作为经典方法的有效替代方案。

Abstract: With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.

</details>


### [67] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

TL;DR: 提出一种数据高效的深度学习框架，使用最少传感器硬件实现精确的肌电假肢控制，通过混合Transformer架构和可学习时间嵌入，在稀疏双通道sEMG上达到95.7%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统精确肌电控制依赖复杂密集的多传感器阵列，限制了消费者可及性。需要开发能在最小传感器硬件上实现精确控制的数据高效方法。

Method: 采用混合Transformer架构，集成Time2Vec可学习时间嵌入捕捉生物信号的随机时间扭曲；使用归一化加性融合策略对齐空间和时间特征的潜在分布；采用两阶段课程学习协议确保数据稀缺下的鲁棒特征提取。

Result: 在10类动作集上达到95.7% ± 0.20%的多受试者F1分数，优于标准Transformer和CNN-LSTM模型。快速校准协议（每个手势仅需两次试验）可将新受试者性能从21.0%提升至96.9%。

Conclusion: 高保真时间嵌入可以补偿低空间分辨率，挑战了高密度传感的必要性。该框架为能够快速个性化的下一代假肢接口提供了鲁棒、经济高效的蓝图。

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [68] [Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models](https://arxiv.org/abs/2602.00129)
*Yixuan Liang*

Main category: cs.LG

TL;DR: CodePilot：结合MCTS与LLM的混合框架，通过执行引导的程序修复解决GitHub问题，在SWE-bench Lite上达到24.67%的问题解决率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在仓库级别进行程序修复仍面临挑战，需要长时程推理能力，而自回归解码存在局限性，需要更有效的执行引导修复方法

Method: 集成蒙特卡洛树搜索与大型语言模型的混合框架，进行从仓库到文件再到函数的分层故障定位，利用MCTS探索多样补丁轨迹，将执行反馈作为奖励信号指导搜索和精炼，并加入置信度校准生成

Result: 在SWE-bench Lite基准测试中，CodePilot使用开源权重模型实现了24.67%的问题解决率，优于可比基线方法

Conclusion: 将符号搜索与神经语言模型相结合是构建可扩展、执行感知的软件工程自动化的有效策略

Abstract: Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.

</details>


### [69] [Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts](https://arxiv.org/abs/2602.02229)
*Guangyi Zhang,Yunlong Cai,Guanding Yu,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出预测驱动风险监控(PPRM)，一种基于预测驱动推理的半监督风险监控方法，用于在标签数据有限的情况下监控动态环境中的模型性能


<details>
  <summary>Details</summary>
Motivation: 在动态环境中监控模型性能时，通常面临标签数据有限的挑战，需要一种能够有效利用有限真实标签的方法来检测有害的性能变化

Method: 基于预测驱动推理(PPI)，结合合成标签和少量真实标签构建运行风险的任意时间有效下界，通过与名义风险上界的阈值比较来检测有害变化

Result: PPRM在图像分类、大语言模型和电信监控任务上通过广泛实验验证了有效性，能够提供无假设的有限样本保证，控制误报概率

Conclusion: PPRM为标签数据有限环境下的模型风险监控提供了一种有效的半监督解决方案，具有理论保证和实际应用价值

Abstract: We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.

</details>


### [70] [On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.00130)
*Sumit Yadav*

Main category: cs.LG

TL;DR: 有效维度（一种无监督几何度量）能强预测神经网络性能，且与模型容量无关，在视觉和NLP任务中均成立，并具有双向因果关系。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络表示几何与性能之间的关系，探索是否可以通过无监督的几何度量来预测模型性能，而不依赖标签或模型规模。

Method: 分析了52个预训练的ImageNet模型（13种架构），使用有效维度作为几何度量。在NLP中测试了8个编码器模型和15个仅解码器LLM。通过添加噪声（高斯、均匀、Dropout、椒盐噪声）破坏几何结构，以及使用PCA改善几何结构来验证因果关系。

Result: 有效维度与准确率强相关（r=0.75，p<10^-10），而模型容量不相关（r=0.07）。在ImageNet、CIFAR-10和NLP任务（SST-2/MNLI、AG News）中均得到验证。破坏几何导致性能下降（r=-0.94），改善几何保持性能（仅下降0.03pp）。

Conclusion: 有效维度提供了跨领域的预测性和因果性信息，完全无需标签即可计算，为理解神经网络性能提供了新的几何视角。

Abstract: We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 ($p < 10^(-10)$) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, $p < 10^(-9)$), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show $|r| > 0.90$. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.

</details>


### [71] [RAPTOR: Ridge-Adaptive Logistic Probes](https://arxiv.org/abs/2602.00158)
*Ziqi Gao,Yaotian Zhu,Qingcheng Zeng,Xu Zhao,Ziqing Wang,Feng Ruan,Kaize Ding*

Main category: cs.LG

TL;DR: RAPTOR是一种基于L2正则化逻辑回归的探针方法，通过验证调优的正则化强度从归一化权重中提取概念向量，在准确率、方向稳定性和训练成本方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 探针通常用于分析冻结LLM层表示中编码的信息，并在探针-引导管道中操作使用。这种管道的有效性取决于能否获得准确、方向稳定且计算成本低的概念向量。

Method: 提出RAPTOR（Ridge-Adaptive Logistic Probe），一种简单的L2正则化逻辑探针，通过验证调优的正则化强度从归一化权重中提取概念向量。

Result: 在指令调优LLM和人工编写概念数据集上的广泛实验中，RAPTOR在准确率上匹配或超过强基线方法，同时实现竞争性的方向稳定性和显著更低的训练成本。

Conclusion: RAPTOR提供了一种高效准确的概念向量提取方法，并通过理论分析解释了正则化强度如何调节探针准确率和概念向量稳定性，其结构预测与真实LLM嵌入中观察到的趋势定性一致。

Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.

</details>


### [72] [GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection](https://arxiv.org/abs/2602.00218)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: GRIP2是一种深度学习特征选择方法，通过二维正则化表面积分和块随机采样，在高相关性和低信噪比场景下实现错误发现率控制并保持高检测能力。


<details>
  <summary>Details</summary>
Motivation: 在非线性、高相关性和低信噪比的复杂场景中，现有深度学习方法难以在严格控制错误发现率的同时识别真正有预测能力的特征，这是特征选择领域的一个基本挑战。

Method: 提出GRIP2方法：1) 构建二维正则化表面控制稀疏强度和稀疏化几何；2) 通过块随机采样在单次训练中近似表面积分；3) 生成反对称统计量确保有限样本FDR控制；4) 集成第一层特征活动信息。

Result: 在合成和半真实数据实验中，GRIP2在高相关性和低信噪比场景下表现出更强的鲁棒性，保持高检测能力和稳定性。在HIV耐药性真实数据中，比现有线性基线方法更好地识别已知耐药相关突变。

Conclusion: GRIP2通过创新的二维正则化表面积分和块随机采样技术，解决了深度学习方法在复杂场景下的特征选择挑战，实现了严格的FDR控制和高检测能力，在实际应用中表现出可靠性能。

Abstract: Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.

</details>


### [73] [Sheaf Neural Networks and biomedical applications](https://arxiv.org/abs/2602.00159)
*Aneeqa Mehrab,Jan Willem Van Looy,Pietro Demurtas,Stefano Iotti,Emil Malucelli,Francesca Rossi,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.LG

TL;DR: 该论文阐述了层神经网络的理论与数学模型，并通过生物医学案例研究证明其优于主流图神经网络


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GCN、GAT、GraphSage）在生物医学问题中可能存在局限性，需要更有效的算法来回答复杂的生物医学问题

Method: 提出层神经网络算法，建立其理论框架和数学模型，并在具体生物医学案例中进行应用验证

Result: SNN在生物医学案例研究中表现出色，性能优于主流的图神经网络（GCN、GAT、GraphSage）

Conclusion: 层神经网络是有效的生物医学数据分析工具，在特定应用场景中优于传统图神经网络方法

Abstract: The purpose of this paper is to elucidate the theory and mathematical modelling behind the sheaf neural network (SNN) algorithm and then show how SNN can effectively answer to biomedical questions in a concrete case study and outperform the most popular graph neural networks (GNNs) as graph convolutional networks (GCNs), graph attention networks (GAT) and GraphSage.

</details>


### [74] [LatentTrack: Sequential Weight Generation via Latent Filtering](https://arxiv.org/abs/2602.00458)
*Omer Haq*

Main category: cs.LG

TL;DR: LatentTrack (LT) 是一种用于非平稳动态下在线概率预测的序列神经网络架构，通过在低维潜空间进行因果贝叶斯滤波，使用轻量级超网络生成预测模型参数，实现恒定时间的在线自适应。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理非平稳动态的在线预测时面临挑战，需要处理分布漂移问题，同时保持计算效率。现有方法要么需要每步梯度更新（计算成本高），要么自适应能力有限。

Method: LT 在低维潜空间执行因果贝叶斯滤波，使用轻量级超网络在每个时间步生成预测模型参数。采用预测-生成-更新的滤波框架：学习潜模型预测下一个潜分布，通过摊销推理使用新观测更新，支持结构化和非结构化潜动态，通过潜轨迹的蒙特卡洛推理产生校准的预测混合。

Result: 在 Jena Climate 基准测试的长时域在线回归评估中，LT 始终比有状态序列和静态不确定性感知基线获得更低的负对数似然和均方误差，具有竞争力的校准性能。

Conclusion: 潜条件函数演化是传统潜状态建模在分布漂移下的有效替代方案，能够在恒定时间成本下实现高效的在线自适应预测。

Abstract: We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, enabling constant-time online adaptation without per-step gradient updates.
  At each time step, a learned latent model predicts the next latent distribution, which is updated via amortized inference using new observations, yielding a predict--generate--update filtering framework in function space. The formulation supports both structured (Markovian) and unstructured latent dynamics within a unified objective, while Monte Carlo inference over latent trajectories produces calibrated predictive mixtures with fixed per-step cost. Evaluated on long-horizon online regression using the Jena Climate benchmark, LT consistently achieves lower negative log-likelihood and mean squared error than stateful sequential and static uncertainty-aware baselines, with competitive calibration, demonstrating that latent-conditioned function evolution is an effective alternative to traditional latent-state modeling under distribution shift.

</details>


### [75] [Block removal for large language models through constrained binary optimization](https://arxiv.org/abs/2602.00161)
*David Jansen,Roman Rausch,David Montero,Roman Orus*

Main category: cs.LG

TL;DR: 提出一种将Transformer块移除问题转化为约束二元优化问题的方法，通过映射到伊辛模型来高效评估候选配置，在多个基准测试中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 压缩大型语言模型时移除整个Transformer块看似简单，但确定移除哪些块构成指数级困难的组合优化问题，需要更有效的解决方案

Method: 将块移除问题形式化为约束二元优化问题，映射到物理系统（伊辛模型），利用系统能量作为下游模型性能的强代理指标，实现高效配置排序

Result: 方法在多个基准测试中优于最先进的块移除方法，性能提升在短时间重训练后仍然保持，在MMLU基准上达到高达6个百分点的改进

Conclusion: 该方法仅需少量活跃参数的前向和反向传播，配合伊辛求解器即可应用，具有广泛适用性，已在复杂结构的NVIDIA-Nemotron-3-Nano-30B-A3B-FP8模型上验证

Abstract: Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.

</details>


### [76] [Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding](https://arxiv.org/abs/2602.00781)
*Jiamin Xu,Kyra Gan*

Main category: cs.LG

TL;DR: 提出一种针对有限时域非片段式MDP的在线强化学习方法，使用K步前瞻Q函数和阈值机制，实现高效样本利用和理论保证


<details>
  <summary>Details</summary>
Motivation: 现有无限时域方法依赖折扣收缩，不适用于固定时域结构；需要解决有限时域非片段式MDP中在线强化学习的挑战

Method: 引入K步前瞻Q函数（而非完整时域），结合时间变化阈值机制选择动作；提出高效表格学习算法，自适应调整K值平衡前瞻深度与估计方差

Result: 理论证明：K=1时达到极小极大最优常数遗憾；K≥2时遗憾为O(max((K-1),C_{K-1})√(SATlogT))；实验在JumpRiverswim、FrozenLake、AnyTrading等环境中优于现有表格RL方法

Conclusion: K步前瞻Q函数与阈值机制有效解决有限时域MDP的在线学习问题，在理论和实验上均表现优异，为固定时域RL提供新思路

Abstract: Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\mathcal{O}(\max((K-1),C_{K-1})\sqrt{SAT\log(T)})$ regret for any $K \geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.

</details>


### [77] [Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2602.00165)
*Arthur Negrão,Pedro Silva,Vander L. S. Freitas,Gladston Moreira,Eduardo Luz*

Main category: cs.LG

TL;DR: 提出Benford-Quant，一种基于本福德定律的非均匀量化器，用于LLM权重压缩，通过对数间隔码本更好地匹配权重分布，在低比特量化中提升精度。


<details>
  <summary>Details</summary>
Motivation: LLM压缩需求增长，标准均匀量化器假设参数均匀分布，但实际权重分布高度偏斜，需要更匹配实际分布的非均匀量化方法。

Method: 提出Benford-Quant，受本福德定律启发，用对数间隔码本替代均匀网格，为频繁出现的小幅值权重分配更多分辨率，无需数据训练。

Result: 1) Transformer层权重符合本福德统计，归一化层偏离；2) 在SLM上持续改善困惑度，Gemma-270M 4-bit困惑度降低超10%；3) 在大LLM上保持竞争力，差异由过参数化效应解释。

Conclusion: 将本福德先验融入量化网格是低成本修改，在激进低比特量化中带来精度提升，可与SmoothQuant、Activation-Aware Quantization等方法混合使用，无需重大流程修改。

Abstract: The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.

</details>


### [78] [Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization](https://arxiv.org/abs/2602.00827)
*Taesun Yeom,Taehyeok Ha,Jaeho Lee*

Main category: cs.LG

TL;DR: 研究发现特征学习强度（FLS）存在最优值，既不能太小也不能太大，这与传统认为更强的特征学习总是改善泛化的直觉相反。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要研究FLS在渐近情况下的影响，但对实际训练场景（如达到目标训练风险时停止训练）中FLS如何影响泛化缺乏深入理解。

Method: 通过实证研究观察FLS对泛化的影响，然后对使用逻辑损失训练的两层ReLU网络进行梯度流动力学理论分析，通过初始化规模控制FLS。

Result: 发现了最优FLS的存在，它既不能太小也不能太大。理论分析表明最优FLS源于两种竞争效应的权衡：过大的FLS导致"过度对齐"现象损害泛化，过小的FLS导致过拟合。

Conclusion: 特征学习强度对深度网络泛化有重要影响，存在最优值平衡过度对齐和过拟合两种效应，挑战了"更强的特征学习总是更好"的传统观点。

Abstract: Feature learning strength (FLS), i.e., the inverse of the effective output scaling of a model, plays a critical role in shaping the optimization dynamics of neural nets. While its impact has been extensively studied under the asymptotic regimes -- both in training time and FLS -- existing theory offers limited insight into how FLS affects generalization in practical settings, such as when training is stopped upon reaching a target training risk. In this work, we investigate the impact of FLS on generalization in deep networks under such practical conditions. Through empirical studies, we first uncover the emergence of an $\textit{optimal FLS}$ -- neither too small nor too large -- that yields substantial generalization gains. This finding runs counter to the prevailing intuition that stronger feature learning universally improves generalization. To explain this phenomenon, we develop a theoretical analysis of gradient flow dynamics in two-layer ReLU nets trained with logistic loss, where FLS is controlled via initialization scale. Our main theoretical result establishes the existence of an optimal FLS arising from a trade-off between two competing effects: An excessively large FLS induces an $\textit{over-alignment}$ phenomenon that degrades generalization, while an overly small FLS leads to $\textit{over-fitting}$.

</details>


### [79] [Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints](https://arxiv.org/abs/2602.00166)
*Evan Chen,Wenzhi Fang,Shiqiang Wang,Christopher Brinton*

Main category: cs.LG

TL;DR: DA-GRPO：一种双优势强化学习方法，用于在持续学习中智能调节小语言模型对云端大模型的依赖，避免灾难性遗忘并保持稳定的云使用预算。


<details>
  <summary>Details</summary>
Motivation: 本地部署的小语言模型需要在严格内存和计算限制下持续支持多样化任务，必须选择性依赖云端大语言模型。然而，在持续学习中调节云辅助具有挑战性，因为基于奖励的强化学习常导致不稳定的卸载行为，并在任务分布变化时加剧灾难性遗忘。

Method: 提出DA-GRPO（双优势组相对策略优化），将云使用约束直接纳入优势计算，避免固定奖励塑造和外部路由模型。该方法使本地模型能联合学习任务能力和协作行为，让云请求在训练后自然出现，同时遵守预设的辅助预算。

Result: 在数学推理和代码生成基准测试中，DA-GRPO提高了切换后的准确性，显著减少了遗忘，与先前的协作和基于路由的方法相比，保持了更稳定的云使用。

Conclusion: DA-GRPO通过将云使用约束直接集成到强化学习框架中，有效解决了小语言模型在持续学习中的协作问题，实现了任务能力和云辅助行为的联合优化，为资源受限环境下的智能模型协作提供了新方法。

Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.

</details>


### [80] [Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation](https://arxiv.org/abs/2602.00834)
*Wei Chen,Jiacheng Li,Shigui Li,Zhiqi Lin,Junmei Yang,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 基于分数的方法在密度比估计中存在路径依赖悖论，作者提出最小路径方差原则来解决这一问题，通过参数化路径学习数据自适应低方差路径，在基准测试中达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 基于分数的方法在密度比估计中面临一个重要悖论：虽然理论上路径独立，但实际性能却严重依赖于选择的路径调度。现有可训练的优化目标与理想真实目标之间存在被忽视的关键差异——时间分数的路径方差。

Method: 提出最小路径方差原则，推导出路径方差的闭式表达式，将难以处理的问题转化为可优化的形式。使用灵活的Kumaraswamy混合模型参数化路径，学习数据自适应、低方差的路径，无需启发式选择。

Result: 通过优化完整目标，获得了更准确和稳定的估计器，在具有挑战性的基准测试中建立了新的最先进结果。

Conclusion: 最小路径方差原则解决了基于分数的密度比估计中的路径依赖悖论，通过最小化被忽视的路径方差项，实现了更优的密度比估计性能。

Abstract: Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\textbf{Min}imum \textbf{P}ath \textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.

</details>


### [81] [The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective](https://arxiv.org/abs/2602.00170)
*Qiyao Liang,Jinyeop Song,Yizhou Liu,Jeff Gore,Ila Fiete,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

TL;DR: 权重扰动进化策略能用极小种群（~30）微调十亿参数语言模型，这与传统零阶优化的维度诅咒直觉相悖。研究发现微调景观具有低曲率维度特性，解释了ES的可扩展性和训练动态的非单调性。


<details>
  <summary>Details</summary>
Motivation: 传统理论认为高维优化需要大量样本，但实际中权重扰动进化策略能用极小种群微调大模型，同时观察到固定超参数下奖励先升后降的现象。需要解释这些看似矛盾的现象背后的共同机制。

Method: 使用ES作为几何探针，在GSM8K、ARC-C和WinoGrande数据集上对Qwen2.5-Instruct模型（0.5B-7B）的微调奖励景观进行分析。提出最小二次随机上升模型来捕捉时间尺度异质性，并通过实验验证奖励改进扰动在小种群情况下的可访问性。

Result: 实验表明：1）奖励改进扰动在不同规模模型中都可通过小种群访问；2）微调景观具有低曲率维度特性，少数高曲率维度主导改进；3）这种几何特性同时解释了ES的可扩展性和训练动态的非单调性（先升后降）。

Conclusion: 微调景观的低曲率维度特性使得高维微调比最坏情况理论预测的更易优化，ES的小种群可扩展性和非单调训练动态反映了共同的几何本质，为更广泛的优化方法提供了可能性。

Abstract: Weight-perturbation evolution strategies (ES) can fine-tune billion-parameter language models with surprisingly small populations (e.g., $N\!\approx\!30$), contradicting classical zeroth-order curse-of-dimensionality intuition. We also observe a second seemingly separate phenomenon: under fixed hyperparameters, the stochastic fine-tuning reward often rises, peaks, and then degrades in both ES and GRPO. We argue that both effects reflect a shared geometric property of fine-tuning landscapes: they are low-dimensional in curvature. A small set of high-curvature dimensions dominates improvement, producing (i) heterogeneous time scales that yield rise-then-decay under fixed stochasticity, as captured by a minimal quadratic stochastic-ascent model, and (ii) degenerate improving updates, where many random perturbations share similar components along these directions. Using ES as a geometric probe on fine-tuning reward landscapes of GSM8K, ARC-C, and WinoGrande across Qwen2.5-Instruct models (0.5B--7B), we show that reward-improving perturbations remain empirically accessible with small populations across scales. Together, these results reconcile ES scalability with non-monotonic training dynamics and suggest that high-dimensional fine-tuning may admit a broader class of viable optimization methods than worst-case theory implies.

</details>


### [82] [Multimodal Scientific Learning Beyond Diffusions and Flows](https://arxiv.org/abs/2602.00960)
*Leonardo Ferreira Guilhoto,Akshat Kaushal,Paris Perdikaris*

Main category: cs.LG

TL;DR: MDNs作为显式参数密度估计器，在科学机器学习中为多模态不确定性量化提供高效、可解释的替代方案，优于隐式生成模型


<details>
  <summary>Details</summary>
Motivation: 科学机器学习需要处理多模态条件不确定性（如不适定逆问题、多稳态、混沌动力学），但现有隐式生成模型（扩散模型、流模型）数据需求大、计算成本高，且与科学问题的结构化解空间不匹配

Method: 提出使用混合密度网络（MDNs）作为显式参数密度估计器，通过全局概率质量分配直接建模多模态分布，利用低维多模态物理的归纳偏置

Result: MDNs在数据稀缺情况下能可靠恢复分离的模式，在逆问题、多稳态和混沌科学回归任务中表现出优异的泛化能力、可解释性和样本效率

Conclusion: MDNs为科学机器学习中的多模态不确定性量化提供了原则性且高效的解决方案，特别适合数据稀缺的科学应用场景

Abstract: Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based methods, these approaches are often data-hungry, computationally costly, and misaligned with the structured solution spaces frequently found in scientific problems. We demonstrate that Mixture Density Networks (MDNs) provide a principled yet largely overlooked alternative for multimodal uncertainty quantification in SciML. As explicit parametric density estimators, MDNs impose an inductive bias tailored to low-dimensional, multimodal physics, enabling direct global allocation of probability mass across distinct solution branches. This structure delivers strong data efficiency, allowing reliable recovery of separated modes in regimes where scientific data is scarce. We formalize these insights through a unified probabilistic framework contrasting explicit and implicit distribution networks, and demonstrate empirically that MDNs achieve superior generalization, interpretability, and sample efficiency across a range of inverse, multistable, and chaotic scientific regression tasks.

</details>


### [83] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: GASP方法通过对抗性自博弈训练，使强化学习模型能够在有缺陷的上下文条件下保持鲁棒性，无需人工标签或外部教师。


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习（RLVR）模型在上下文条件有缺陷时（如错误的思维链、误导性部分解或轻微输入扰动）会灾难性失败，因为标准RLVR只优化干净条件下的最终答案正确性。

Method: GASP（引导对抗性自博弈）方法：在单个模型内形成对抗性自博弈游戏，污染者学习通过局部一致的污染来诱导失败，而智能体学习在相同污染条件下诊断和恢复。为解决训练早期成功恢复稀缺的问题，提出了分布内修复引导，通过自生成修复的模仿项来增加恢复概率。

Result: 在四个开放权重模型（1.5B-8B）上，GASP将强但脆弱的推理器转变为鲁棒的推理器，能够承受误导和扰动的上下文，同时通常还能提高干净准确性。对抗性污染诱导了有效的课程学习，分布内引导实现了快速恢复学习。

Conclusion: GASP方法仅使用结果验证就能显式训练检测和修复能力，无需人工标签或外部教师，成功解决了RLVR模型在缺陷上下文条件下的脆弱性问题。

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [84] [Superposition unifies power-law training dynamics](https://arxiv.org/abs/2602.01045)
*Zixin Jessie Chen,Hao Chen,Yizhou Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 研究特征叠加在幂律训练动力学中的作用，发现叠加瓶颈会导致训练指数收敛到约1的普适值，相比无叠加的序列学习加速高达10倍。


<details>
  <summary>Details</summary>
Motivation: 探究特征叠加在神经网络训练动力学中的作用，特别是它如何影响训练速度的幂律行为，这对于理解大规模语言模型等使用叠加的网络具有重要意义。

Method: 使用师生框架研究特征叠加，首先推导无叠加情况下的解析理论，然后分析叠加瓶颈如何改变训练动力学。

Result: 无叠加时幂律训练指数依赖于输入数据统计和通道重要性；叠加瓶颈导致训练指数收敛到约1的普适值，与数据和通道统计无关，训练加速高达10倍。

Conclusion: 特征叠加导致快速训练并产生数据无关的普适幂律指数，这对使用叠加的各种神经网络（包括生产级大语言模型）具有重要启示。

Abstract: We investigate the role of feature superposition in the emergence of power-law training dynamics using a teacher-student framework. We first derive an analytic theory for training without superposition, establishing that the power-law training exponent depends on both the input data statistics and channel importance. Remarkably, we discover that a superposition bottleneck induces a transition to a universal power-law exponent of $\sim 1$, independent of data and channel statistics. This one over time training with superposition represents an up to tenfold acceleration compared to the purely sequential learning that takes place in the absence of superposition. Our finding that superposition leads to rapid training with a data-independent power law exponent may have important implications for a wide range of neural networks that employ superposition, including production-scale large language models.

</details>


### [85] [The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization](https://arxiv.org/abs/2602.00175)
*Manyi Li,Yufan Liu,Lai Jiang,Bing Li,Yuming Li,Weiming Hu*

Main category: cs.LG

TL;DR: 论文揭示基于遗忘的防御机制存在根本缺陷：NSFW概念并未真正从扩散模型中移除，而是作为休眠记忆保留。作者提出IVO攻击框架，通过优化初始潜变量重新激活这些记忆，暴露当前防御的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前基于遗忘的防御声称能从扩散模型中清除NSFW概念，但作者发现这种"遗忘"在很大程度上是假象。未学习只是部分破坏了语言符号与底层知识之间的映射关系，而知识本身作为休眠记忆仍然完整保留。这暴露了当前防御机制的根本缺陷。

Method: 提出IVO（初始潜变量优化）攻击框架，包含三个核心组件：图像反演、对抗优化和重用攻击。该方法通过优化初始潜变量，使未学习模型的噪声分布与其原始不安全状态重新对齐，从而重新激活休眠记忆。

Result: 在8种广泛使用的未学习技术上进行了广泛实验，IVO实现了优越的攻击成功率（高达99.9%）和强大的语义一致性，成功重新激活了被"遗忘"的NSFW概念，证明了当前防御的脆弱性。

Conclusion: 基于遗忘的防御存在根本性缺陷，NSFW概念并未真正从扩散模型中移除。去噪过程中的分布差异可作为映射保留程度的可测量指标。IVO攻击框架暴露了当前防御的脆弱性，为未来更鲁棒的防御机制设计提供了重要洞见。

Abstract: Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this "forgetting" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.

</details>


### [86] [Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses](https://arxiv.org/abs/2602.01285)
*Kangjun Noh,Seongchan Lee,Ilmun Kim,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出MACI方法，通过乘积过滤框架和多LLM集成，在保持有效性的同时显著提高真实声明的保留率


<details>
  <summary>Details</summary>
Motivation: 现有保形推理方法要么过于保守（丢弃过多真实声明），要么依赖简单线性模型无法捕捉复杂群体结构，难以在医疗、法律等高风险领域确保LLM的事实性

Method: 将保形推理重新表述为乘积过滤设置，将事实性建模为声明级分数的乘积；使用多LLM集成产生更准确的事实性分数，通过群体条件校准保持有效性

Result: MACI始终达到用户指定的覆盖率，同时比基线方法显著提高保留率并降低时间成本

Conclusion: MACI通过乘积过滤框架和多LLM集成，在保持保形推理分布无关保证的同时，提高了真实声明的保留效率

Abstract: Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI

</details>


### [87] [How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models](https://arxiv.org/abs/2602.00179)
*Joseph L. Breeden*

Main category: cs.LG

TL;DR: 论文认为局部线性解释方法（如LIME和SHAP）在决策边界附近的不稳定性反映了预测不确定性高的问题，而非方法缺陷。正确的做法是首先评估预测是否可用，只有在预测不确定性足够低时才寻求解释。


<details>
  <summary>Details</summary>
Motivation: 在机器学习的关键决策应用中，可解释性是主要关注点，通常是监管要求。局部线性解释方法（如LIME和SHAP）因在决策边界附近不稳定而受到批评，但作者认为这反映了对问题的误解。

Method: 提出改变问题序列的方法：首先评估非线性模型在特定区域的预测是否可用（不确定性是否足够低），只有当预测可用时才通过局部线性近似寻求解释。如果预测不可用，则退回到更简单的整体模型（如传统逻辑回归）。

Result: 当预测不确定性足够低时，局部线性解释的不稳定性也相应较低；当预测不确定性过高时，解释不可用的预测没有意义。某些声称处处可解释的方法（如ReLU网络或分段线性模型）实际上只有虚幻的可解释性，因为在分段边界处的预测不确定性太高而无法使用。

Conclusion: 可解释性评估应该首先关注预测是否可用，而不是盲目寻求解释。只有在预测不确定性足够低的区域，局部线性解释才有意义。这为机器学习模型的可解释性实践提供了新的框架和视角。

Abstract: For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.

</details>


### [88] [High-accuracy sampling for diffusion models and log-concave distributions](https://arxiv.org/abs/2602.01338)
*Fan Chen,Sinho Chewi,Constantinos Daskalakis,Alexander Rakhlin*

Main category: cs.LG

TL;DR: 提出扩散模型采样算法，以polylog(1/δ)步数达到δ误差，相比之前工作有指数级改进


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型采样方法需要较多步数才能达到高精度，本文旨在开发更高效的采样算法，显著降低计算复杂度

Method: 基于L²范数下Õ(δ)-准确分数估计，在不同数据假设下设计采样算法：最小数据假设、非均匀L-Lipschitz条件、数据分布具有内在维度d⋆

Result: 在三种不同假设下分别获得Õ(d polylog(1/δ))、Õ(√(dL) polylog(1/δ))和Õ(d⋆ polylog(1/δ))的复杂度，相比之前工作有指数级改进

Conclusion: 本文提出的扩散模型采样算法实现了polylog(1/δ)步数复杂度，是采样效率的重大突破，同时首次为一般对数凹分布提供了仅需梯度评估的polylog(1/δ)复杂度采样器

Abstract: We present algorithms for diffusion model sampling which obtain $δ$-error in $\mathrm{polylog}(1/δ)$ steps, given access to $\widetilde O(δ)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\widetilde O(d\,\mathrm{polylog}(1/δ))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\widetilde O(\sqrt{dL}\,\mathrm{polylog}(1/δ))$; and if the data distribution has intrinsic dimension $d_\star$, then the complexity reduces to $\widetilde O(d_\star\,\mathrm{polylog}(1/δ))$. Our approach also yields the first $\mathrm{polylog}(1/δ)$ complexity sampler for general log-concave distributions using only gradient evaluations.

</details>


### [89] [GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models](https://arxiv.org/abs/2602.00191)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: cs.LG

TL;DR: 提出GEPC方法，通过检测扩散模型分数场的群等变性一致性来识别分布外数据，无需训练且计算轻量


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的OOD检测方法主要利用分数大小或局部几何特征，忽略了分数场的等变性特性。当数据分布发生变化时，模型学到的近似等变性可能会被破坏，这为OOD检测提供了新的信号

Method: 提出群等变后验一致性(GEPC)方法：1) 定义理想的GEPC残差，在有限群上平均等变性残差函数；2) 仅需分数评估，无需额外训练；3) 生成可解释的等变性破坏图；4) 在理论层面推导了ID数据上界和OOD数据下界

Result: 1) 在OOD图像基准数据集上，GEPC达到与现有扩散基线竞争或更好的AUROC性能；2) 在高分辨率合成孔径雷达图像上，GEPC实现强目标-背景分离；3) 生成视觉可解释的等变性破坏图；4) 计算轻量

Conclusion: GEPC通过检测扩散模型分数场的群等变性破坏来识别OOD数据，提供了一种无需训练、计算高效且可解释的OOD检测方法，在多个基准和实际应用场景中表现优异

Abstract: Diffusion models learn a time-indexed score field $\mathbf{s}_θ(\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.

</details>


### [90] [An Odd Estimator for Shapley Values](https://arxiv.org/abs/2602.01399)
*Fabian Fumagalli,Landon Butler,Justin Singh Kang,Kannan Ramchandran,R. Teal Witter*

Main category: cs.LG

TL;DR: OddSHAP：通过奇函数分解和多项式回归的新颖Shapley值估计器，利用配对采样的理论原理提升估计精度


<details>
  <summary>Details</summary>
Motivation: Shapley值在机器学习归因中广泛应用，但精确计算通常不可行。虽然配对采样启发式方法能有效减少估计误差，但其理论机制一直不明确。本文旨在揭示配对采样有效的根本原理，并基于此开发更高效的估计方法。

Method: 1) 理论证明Shapley值仅依赖于集合函数的奇分量，配对采样通过正交化回归目标来过滤无关的偶分量；2) 提出OddSHAP估计器，在奇子空间上进行多项式回归；3) 使用傅里叶基分离奇子空间，并利用代理模型识别高影响力交互，避免高阶近似的组合爆炸问题。

Result: 通过广泛的基准评估，OddSHAP实现了最先进的估计精度，在Shapley值估计中表现出色。

Conclusion: 本文为配对采样提供了优雅的理论解释，证明了Shapley值仅依赖于集合函数的奇分量。基于这一洞察提出的OddSHAP估计器通过奇子空间回归，克服了传统方法的组合爆炸问题，实现了高效准确的Shapley值估计。

Abstract: The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimators leverage the paired sampling heuristic to reduce estimation error, the theoretical mechanism driving this improvement has remained opaque. In this work, we provide an elegant and fundamental justification for paired sampling: we prove that the Shapley value depends exclusively on the odd component of the set function, and that paired sampling orthogonalizes the regression objective to filter out the irrelevant even component. Leveraging this insight, we propose OddSHAP, a novel consistent estimator that performs polynomial regression solely on the odd subspace. By utilizing the Fourier basis to isolate this subspace and employing a proxy model to identify high-impact interactions, OddSHAP overcomes the combinatorial explosion of higher-order approximations. Through an extensive benchmark evaluation, we find that OddSHAP achieves state-of-the-art estimation accuracy.

</details>


### [91] [Reducing Memorisation in Generative Models via Riemannian Bayesian Inference](https://arxiv.org/abs/2602.00199)
*Johanna Marie Gegenfurtner,Albert Kjøller Jacobsen,Naima Elosegui Borras,Alejandro Valverde Mahou,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 提出一种基于贝叶斯视角的方法，通过考虑损失函数的黎曼几何结构来平衡生成模型的记忆与泛化能力


<details>
  <summary>Details</summary>
Motivation: 现代生成模型能产生逼真样本，但平衡记忆与泛化仍是一个开放问题。从贝叶斯视角出发，关注流匹配和扩散模型的参数空间，构建能更好捕捉数据分布变异性的预测后验分布

Method: 使用黎曼度量捕捉损失函数的几何结构，利用灵活的近似后验适应损失景观的局部结构。通过采样与原始模型相似但减少记忆的生成模型来平衡记忆与泛化

Result: 实验证明该方法能减少记忆同时保持泛化能力。理论分析解释了这一发现，表明考虑损失几何结构能有效利用参数空间

Conclusion: 通过考虑损失函数的几何结构，即使在复杂高维生成模型中也能有效利用参数空间，平衡记忆与泛化问题

Abstract: Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.

</details>


### [92] [DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data](https://arxiv.org/abs/2602.01433)
*Muhammad Hasan Ferdous,Md Osman Gani*

Main category: cs.LG

TL;DR: 提出基于分解的因果发现框架，将时间序列分解为趋势、季节性和残差分量，分别进行因果分析，最后整合为统一的多尺度因果结构。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列通常包含长期趋势、季节模式和短期波动，在非平稳性和自相关条件下进行因果推断非常复杂。现有方法直接在原始观测值上操作，容易产生虚假边和错误归因的时间依赖关系。

Method: 将每个时间序列分解为趋势、季节性和残差三个分量，然后分别进行分量特定的因果分析：趋势分量使用平稳性检验，季节性分量使用基于核的依赖度量，残差分量使用基于约束的因果发现方法。最后将分量级图整合为统一的多尺度因果结构。

Result: 在广泛的合成基准测试和真实世界气候数据上，该框架比现有最先进基线方法更准确地恢复真实因果结构，特别是在强非平稳性和时间自相关条件下表现更优。

Conclusion: 基于分解的因果发现框架能够分离长期和短期因果效应，减少虚假关联，提高可解释性，在非平稳和自相关时间序列的因果推断中具有显著优势。

Abstract: Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.

</details>


### [93] [Reducing Class-Wise Performance Disparity via Margin Regularization](https://arxiv.org/abs/2602.00205)
*Beier Zhu,Kesen Zhao,Jiequan Cui,Qianru Sun,Yuan Zhou,Xun Yang,Hanwang Zhang*

Main category: cs.LG

TL;DR: MR²通过动态调整logit和表示空间中的边界，提出了一种理论驱动的正则化方法，以减少分类任务中的性能差异。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络即使在类别平衡的数据上训练，也常常表现出显著的类别间准确率差异，这对可靠部署构成挑战。虽然已有经验性解决方案，但对分类任务中这种性能差异的理论理解仍然有限。

Method: 提出Margin Regularization for Performance Disparity Reduction (MR²)，通过动态调整logit和表示空间中的边界。基于理论分析，该方法根据特征分布优化每个类别的logit边界，并惩罚过大的表示边界以增强类内紧凑性。

Result: 在7个数据集（包括ImageNet）和多种预训练骨干网络（MAE、MoCov2、CLIP）上的实验表明，MR²不仅提高了整体准确率，还显著提升了困难类别的性能，同时不牺牲简单类别的性能，从而减少了性能差异。

Conclusion: MR²提供了一种理论驱动的正则化方法，通过动态边界调整有效减少分类任务中的性能差异，为可靠部署提供了有前景的解决方案。

Abstract: Deep neural networks often exhibit substantial disparities in class-wise accuracy, even when trained on class-balanced data, posing concerns for reliable deployment. While prior efforts have explored empirical remedies, a theoretical understanding of such performance disparities in classification remains limited. In this work, we present Margin Regularization for Performance Disparity Reduction (MR$^2$), a theoretically principled regularization for classification by dynamically adjusting margins in both the logit and representation spaces. Our analysis establishes a margin-based, class-sensitive generalization bound that reveals how per-class feature variability contributes to error, motivating the use of larger margins for hard classes. Guided by this insight, MR$^2$ optimizes per-class logit margins proportional to feature spread and penalizes excessive representation margins to enhance intra-class compactness. Experiments on seven datasets, including ImageNet, and diverse pre-trained backbones (MAE, MoCov2, CLIP) demonstrate that MR$^2$ not only improves overall accuracy but also significantly boosts hard class performance without trading off easy classes, thus reducing performance disparity. Code is available at: https://github.com/BeierZhu/MR2

</details>


### [94] [Theoretical Analysis of Measure Consistency Regularization for Partially Observed Data](https://arxiv.org/abs/2602.01437)
*Yinsong Wang,Shahin Shahrampour*

Main category: cs.LG

TL;DR: 该论文提出了测量一致性正则化(MCR)的理论分析框架，通过神经网络距离视角解释了MCR在部分可观测数据下提升插补质量的原理，并提出了基于对偶间隙监控的早期停止训练协议。


<details>
  <summary>Details</summary>
Motivation: 尽管测量一致性正则化(MCR)在图像修复、数据插补和半监督学习等应用中取得了经验成功，但其理论基础仍然有限。本文旨在填补这一空白，从理论角度理解MCR为何、何时以及如何提升部分可观测数据下的插补质量。

Method: 1) 通过神经网络距离视角对MCR进行理论分析；2) 识别MCR泛化优势的关键项；3) 扩展到不完美训练机制；4) 提出基于对偶间隙监控的早期停止训练协议；5) 通过实证研究和真实世界数据模拟验证理论。

Result: 理论分析揭示了MCR泛化优势的机制，并表明这种优势并非总是保证。提出的对偶间隙监控方法能有效确定早期停止点，保留泛化效益。实证证据支持了理论主张，并展示了该方法在不同数据源和模型架构下的有效性。

Conclusion: 本文为测量一致性正则化提供了坚实的理论基础，阐明了其在部分可观测数据下的工作机制，并提出了实用的训练协议来确保其泛化优势。研究展示了MCR在不同应用场景下的多功能性，为处理损坏数据、缺失特征或缺失模态的问题提供了理论指导和实用工具。

Abstract: The problem of corrupted data, missing features, or missing modalities continues to plague the modern machine learning landscape. To address this issue, a class of regularization methods that enforce consistency between imputed and fully observed data has emerged as a promising approach for improving model generalization, particularly in partially observed settings. We refer to this class of methods as Measure Consistency Regularization (MCR). Despite its empirical success in various applications, such as image inpainting, data imputation and semi-supervised learning, a fundamental understanding of the theoretical underpinnings of MCR remains limited. This paper bridges this gap by offering theoretical insights into why, when, and how MCR enhances imputation quality under partial observability, viewed through the lens of neural network distance.
  Our theoretical analysis identifies the term responsible for MCR's generalization advantage and extends to the imperfect training regime, demonstrating that this advantage is not always guaranteed. Guided by these insights, we propose a novel training protocol that monitors the duality gap to determine an early stopping point that preserves the generalization benefit. We then provide detailed empirical evidence to support our theoretical claims and to show the effectiveness and accuracy of our proposed stopping condition. We further provide a set of real-world data simulations to show the versatility of MCR under different model architectures designed for different data sources.

</details>


### [95] [Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity](https://arxiv.org/abs/2602.00208)
*Jordan Levy,Paul Saves,Moncef Garouani,Nicolas Verstaevel,Benoit Gaudou*

Main category: cs.LG

TL;DR: 该论文提出了一种基于SHAP解释的异常检测器多样性评估方法，通过分析模型决策机制而非仅依赖输出分数来构建更互补、更有效的集成异常检测系统。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测面临数据分布多样性和缺乏标签的挑战。集成方法虽然能减少个体偏差并提高鲁棒性，但现有集成方法难以构建真正互补的检测器集合，因为许多检测器依赖相似的决策线索，导致异常评分冗余，限制了集成学习的潜力。

Method: 提出基于SHAP解释的异常检测器特征化方法：1) 使用SHAP量化每个模型对输入特征的重要性分配；2) 利用这些归因配置文件测量检测器之间的相似性；3) 通过解释多样性来识别互补的检测行为；4) 在保持模型质量的同时，明确针对解释多样性构建集成。

Result: 研究表明：1) 具有相似解释的检测器倾向于产生相关的异常分数并识别大量重叠的异常；2) 解释分歧可靠地指示互补的检测行为；3) 解释驱动指标为集成中的模型选择提供了不同于原始输出的新标准；4) 仅多样性不足，高个体模型性能仍是有效集成的前提；5) 通过针对解释多样性构建的集成更加多样、互补且有效。

Conclusion: 通过分析异常检测器的决策机制而非仅依赖输出分数，可以构建更互补、更有效的集成系统。解释多样性为模型选择提供了新的标准，但必须与个体模型性能相结合，才能实现真正有效的无监督异常检测集成。

Abstract: Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.

</details>


### [96] [A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts](https://arxiv.org/abs/2602.01468)
*Viet Nguyen,Tuan Minh Pham,Thinh Cao,Tan Dinh,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: cs.LG

TL;DR: 门控注意力比多头自注意力更样本高效，通过专家混合框架证明门控注意力只需多项式样本即可准确估计，而多头自注意力需要指数级样本。


<details>
  <summary>Details</summary>
Motivation: 门控注意力在Transformer架构中表现出色，能增强低秩映射表达能力并消除注意力下沉现象，但其理论优势尚未得到充分理解，需要从理论上解释门控注意力的优越性。

Method: 将门控注意力和多头自注意力矩阵的每个条目重新表述为分层专家混合模型，将学习问题转化为专家估计问题，从样本效率角度进行理论分析。

Result: 理论证明门控注意力只需多项式数量的数据点就能准确估计专家，而多头自注意力需要指数级数据点才能达到相同的估计误差，解释了门控注意力的样本效率优势。

Conclusion: 门控注意力在理论上是比多头自注意力更样本高效的架构，为门控机制在缩放点积注意力输出或值映射位置的应用提供了理论依据。

Abstract: Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention's benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.

</details>


### [97] [Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models](https://arxiv.org/abs/2602.00217)
*Chen Liu,Xingzhi Sun,Xi Xiao,Alexandre Van Tassel,Ke Xu,Kristof Reimann,Danqi Liao,Mark Gerstein,Tianyang Wang,Xiao Wang,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 论文发现大语言模型存在"嵌入凝聚"现象，即token嵌入在向量空间中坍缩到狭窄的锥形子空间，小模型比大模型更容易出现此问题，并提出分散损失函数来缓解该现象。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过增加参数获得卓越性能，但计算成本高昂。为了理解模型缩放机制，研究大模型与小模型之间的表征差异，目标是让小模型复制大模型的表征质量。

Method: 1. 系统分析多个Transformer家族模型，发现嵌入凝聚现象；2. 提出分散损失函数，在训练中显式鼓励嵌入分散；3. 在10个基准测试上进行实验验证。

Result: 小模型（如GPT2、Qwen3-0.6B）表现出严重的嵌入凝聚，而大模型（如GPT2-xl、Qwen3-32B）对此现象更具抵抗力。分散损失能有效缓解凝聚，恢复大模型的分散模式，并在多个基准上带来性能提升。

Conclusion: 嵌入凝聚是影响小模型性能的重要几何现象，通过分散损失可以改善小模型的表征质量，为不增加参数的情况下提升小Transformer模型提供了有原则的路径。

Abstract: Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\texttt{GPT2}$ and $\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\texttt{GPT2-xl}$ and $\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.

</details>


### [98] [Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability](https://arxiv.org/abs/2602.01480)
*Eric Regis,Sinho Chewi*

Main category: cs.LG

TL;DR: 本文提出了Rod Flow，一种新的ODE近似方法，用于理解大学习率下梯度下降在非凸景观中的动力学行为，相比之前的Central Flow方法具有更理论化的推导和计算优势。


<details>
  <summary>Details</summary>
Motivation: 理解梯度下降在非凸景观中的训练行为是一个重要问题。边缘稳定性现象表明，大学习率下的梯度下降会偏离梯度流，需要更准确的ODE近似来描述这种动力学。

Method: 提出Rod Flow方法，将梯度下降迭代视为一维扩展对象（"杆"），基于物理图像进行原理性推导，得到显式且计算成本低的ODE近似。

Result: Rod Flow在简单玩具示例中能更好地捕捉梯度下降动力学，在代表性神经网络架构中与Central Flow精度相当，理论证明了其能正确预测临界锐度阈值并解释四次势中的自稳定现象。

Conclusion: Rod Flow提供了一个理论上有原则、计算上高效且实验验证有效的ODE近似框架，为理解大学习率下梯度下降在非凸景观中的动力学提供了新工具。

Abstract: How can we understand gradient-based training over non-convex landscapes? The edge of stability phenomenon, introduced in Cohen et al. (2021), indicates that the answer is not so simple: namely, gradient descent (GD) with large step sizes often diverges away from the gradient flow. In this regime, the "Central Flow", recently proposed in Cohen et al. (2025), provides an accurate ODE approximation to the GD dynamics over many architectures. In this work, we propose Rod Flow, an alternative ODE approximation, which carries the following advantages: (1) it rests on a principled derivation stemming from a physical picture of GD iterates as an extended one-dimensional object -- a "rod"; (2) it better captures GD dynamics for simple toy examples and matches the accuracy of Central Flow for representative neural network architectures, and (3) is explicit and cheap to compute. Theoretically, we prove that Rod Flow correctly predicts the critical sharpness threshold and explains self-stabilization in quartic potentials. We validate our theory with a range of numerical experiments.

</details>


### [99] [Predicting and improving test-time scaling laws via reward tail-guided search](https://arxiv.org/abs/2602.01485)
*Muheng Li,Jian Qian,Wenlong Mou*

Main category: cs.LG

TL;DR: 提出基于尾部分布预测的缩放定律引导搜索方法，优化LLM测试时推理的计算分配，相比传统Best-of-N策略获得更高奖励收益。


<details>
  <summary>Details</summary>
Motivation: 现有Best-of-N策略缺乏对N值选择、预算分配和多阶段决策的原则性指导，虽然许多工作探索优化，但缺乏严格理论保证。

Method: 通过估计奖励的尾部分布预测LLM缩放定律，无需详尽评估；提出缩放定律引导搜索算法，动态分配计算资源以识别和利用具有最高预测潜力的中间状态。

Result: 理论上证明SLG相比完美信息预言机实现可忽略的遗憾，达到的预期奖励需要Best-of-N策略多项式级更大的计算预算；实证验证在不同LLM和奖励模型上，尾部引导分配始终比Best-of-N获得更高奖励收益。

Conclusion: 提出的尾部引导搜索方法为LLM测试时缩放提供了理论保证的优化框架，显著提升了计算效率，代码已开源。

Abstract: Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.

</details>


### [100] [Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting](https://arxiv.org/abs/2602.00240)
*Md Muhtasim Munif Fahim,Soyda Humyra Yesmin,Saiful Islam,Md. Palash Bin Faruque,Md. A. Salam,Md. Mahfuz Uddin,Samiul Islam,Tofayel Ahmed,Md. Binyamin,Md. Rezaul Karim*

Main category: cs.LG

TL;DR: Green-NAS是一个面向低资源环境的多目标神经架构搜索框架，以天气预报为案例研究，通过优化模型准确性和效率来寻找轻量级模型，显著减少计算能耗和碳排放。


<details>
  <summary>Details</summary>
Motivation: 针对低资源环境下的可持续AI部署需求，遵循"绿色AI"原则，旨在减少计算能耗和碳足迹，优先考虑可持续部署而非原始计算规模。

Method: 采用多目标神经架构搜索框架，同时优化模型准确性和效率，寻找参数少、精度高的轻量级模型；并利用迁移学习提高数据有限城市的天气预报准确性。

Result: 最佳模型Green-NAS-A在RMSE为0.0988（比手动调优基线仅差1.4%）的情况下，仅使用153k参数，比GraphCast等全球天气预报模型少239倍；迁移学习可将天气预报准确性提高约5.2%。

Conclusion: Green-NAS框架成功实现了在保持高精度的同时大幅减少模型参数和计算资源消耗，为低资源环境下的可持续AI部署提供了有效解决方案，特别是在天气预报等应用领域。

Abstract: We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to 'Green AI' principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.

</details>


### [101] [Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum](https://arxiv.org/abs/2602.01505)
*Navdeep Kumar,Tehila Dahan,Lior Cohen,Ananyabrata Barua,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: 单时间尺度演员-评论家算法在无限时域折扣MDP中实现O(ε^{-2})最优样本复杂度，相比之前的O(ε^{-3})有显著提升


<details>
  <summary>Details</summary>
Motivation: 现有演员-评论家算法在无限时域折扣马尔可夫决策过程中样本复杂度为O(ε^{-3})，存在改进空间。需要解决非平稳采样分布带来的挑战，同时保持算法的实用性。

Method: 结合STORM方差减少技术和样本缓冲机制：1) 使用STORM减少评论家更新的方差；2) 维护最近样本的小缓冲区，均匀采样用于评论家更新；3) 这些机制与现有深度学习架构兼容，只需少量修改。

Result: 实现了O(ε^{-2})的最优样本复杂度，显著优于之前的O(ε^{-3})状态。算法保持了实际应用性，与现有深度学习架构兼容。

Conclusion: 通过结合STORM方差减少和样本缓冲机制，成功提升了单时间尺度演员-评论家算法的样本效率，达到了理论最优复杂度，同时保持了实际应用价值。

Abstract: We establish an optimal sample complexity of $O(ε^{-2})$ for obtaining an $ε$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(ε^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.

</details>


### [102] [TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models](https://arxiv.org/abs/2602.00250)
*Shreshth Saini,Avinab Saha,Balu Adsumilli,Neil Birkbeck,Yilin Wang,Alan C. Bovik*

Main category: cs.LG

TL;DR: 提出Backward-on-Entropy (BoE) Steering方法，通过单次反向传播近似无限视野前瞻，解决掩码扩散模型采样中的轨迹锁定问题，实现高效的非自回归生成。


<details>
  <summary>Details</summary>
Motivation: 当前掩码扩散模型的采样方法依赖简单的基于置信度的启发式方法，忽略了局部决策的长期影响，导致早期幻觉引发全局不一致的轨迹锁定问题。虽然基于搜索的方法可以缓解此问题，但计算成本过高（每步需要O(K)次前向传播）。

Method: 提出BoE Steering框架，通过轨迹成本函数的一阶展开形式推导Token Influence Score (TIS)，证明输入嵌入相对于未来熵的梯度可作为最小化不确定性的最优控制信号。引入ActiveQueryAttention稀疏伴随原语，利用掩码目标的结构降低反向传播复杂度。

Result: BoE在推理时间缩放方面实现了优于现有解掩码方法的帕累托前沿，表明梯度引导的转向为鲁棒的非自回归生成提供了数学原理严谨且高效的路径。

Conclusion: 梯度引导的推理框架通过单次反向传播近似无限视野前瞻，解决了掩码扩散模型采样中的轨迹锁定问题，在保持计算效率的同时提升了生成质量。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.

</details>


### [103] [Universal Redundancies in Time Series Foundation Models](https://arxiv.org/abs/2602.01605)
*Anthony Bao,Venkata Hasith Vattikuti,Jeffrey Lai,William Gilpin*

Main category: cs.LG

TL;DR: 研究发现时间序列基础模型存在中间层冗余，开发了机制解释工具，发现模型对整层剪枝具有鲁棒性，并提出了基于稳定秩的注意力头剪枝方法，揭示了模型中的退化现象机制。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型通过大规模预训练实现了良好的预测性能，但对其内部工作机制缺乏理解。研究者发现现有的transformer-based TSFMs存在中间层冗余，需要开发工具来解释这些模型的机制，理解其预测行为背后的原理。

Method: 1) 开发了TSFMs的机制解释工具集，包括特定组件剪枝和残差流上的直接logit归因；2) 将transformer框架化为核回归器，提出了基于每个注意力头投影矩阵稳定秩的纯内在剪枝策略；3) 在多个领先的TSFMs和多样化的时间序列数据集上进行大规模评估。

Result: 1) 所有研究模型对整层剪枝都表现出鲁棒性；2) 基于稳定秩的注意力头剪枝方法能够识别导致TSFMs中广泛观察到的退化现象的特定注意力头，如上下文中的模式重复和季节性偏差；3) 这些发现在不同架构的TSFMs和多样化数据集上具有一致性。

Conclusion: 该研究揭示了时间序列基础模型这一新兴架构类的普遍特性，开发了有效的机制解释工具，并提供了理解模型内部工作机制的理论框架，有助于改进时间序列建模方法。

Abstract: Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.

</details>


### [104] [VoxServe: Streaming-Centric Serving System for Speech Language Models](https://arxiv.org/abs/2602.00269)
*Keisuke Kamahori,Wei-Tzu Lee,Atindra Jha,Rohan Kadekodi,Stephanie Wang,Arvind Krishnamurthy,Baris Kasikci*

Main category: cs.LG

TL;DR: VoxServe是一个用于语音语言模型的统一流式服务系统，通过解耦模型架构与系统优化，实现低延迟、高吞吐的流式推理，相比现有方案提升10-20倍吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代语音语言模型在流式部署时需要低延迟、高吞吐和强流式保证，现有系统无法灵活高效地支持多样化的模型架构。

Method: 1. 引入模型执行抽象层，解耦模型架构与系统级优化；2. 实现流式感知调度；3. 采用异步推理流水线提升端到端效率。

Result: 在多种现代语音语言模型上评估显示，VoxServe在可比延迟下实现比现有方案高10-20倍的吞吐量，同时保持高流式可行性。

Conclusion: VoxServe提供了一个统一框架，能够高效支持多样化语音语言模型的流式服务，显著提升系统性能，代码已开源。

Abstract: Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.

</details>


### [105] [The Effect of Mini-Batch Noise on the Implicit Bias of Adam](https://arxiv.org/abs/2602.01642)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 该论文研究了Adam优化器中动量超参数(β₁, β₂)和批次大小如何通过小批量噪声影响隐式偏置，从而影响多轮训练中的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 随着高质量数据有限而计算资源增长，多轮训练在深度学习各领域重新变得重要。Adam优化器作为许多任务（如下一个词预测）的首选，其动量超参数(β₁, β₂)控制记忆，批次大小控制小批量噪声。需要理解小批量噪声如何通过Adam的记忆机制影响损失景观的尖锐或平坦区域，这与多轮训练中的泛化差距相关。

Method: 引入理论框架来分析小批量噪声如何影响Adam中记忆的隐式偏置。研究批次大小变化时，β₁和β₂对正则化/反正则化效果的影响。理论推导将批次大小尺度变化与临界批次大小尺度联系起来，并在即将过拟合的小规模数据实验中验证效果。

Result: 发现：1) 大批次时，较高的β₂会增加记忆的反正则化幅度（损害泛化）；2) 批次变小时，β₂对（反）正则化的依赖关系反转；3) β₁发生类似但方向相反的单调性转变；4) 默认参数(0.9, 0.999)适合小批次，大批次时使β₁更接近β₂能获得更好的验证精度。

Conclusion: Adam优化器的动量超参数(β₁, β₂)与批次大小之间存在复杂的相互作用，影响多轮训练中的泛化性能。批次大小变化会导致β₁和β₂对正则化效果的影响发生反转，这为在不同训练条件下调整Adam参数提供了理论指导。

Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(β_1, β_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $β_1$, $β_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $β_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $β_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $β_1$. In particular, the commonly "default" pair $(β_1, β_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $β_1$ closer to $β_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.

</details>


### [106] [Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning](https://arxiv.org/abs/2602.00282)
*Naman Saxena,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 该论文分析了约束双层强化学习算法的样本复杂度，提出了CBSO算法，获得了O(ε^{-2})的迭代复杂度和Õ(ε^{-4})的样本复杂度，首次使用Moreau包络分析非光滑目标函数的参数化策略梯度RL算法。


<details>
  <summary>Details</summary>
Motivation: 元学习、分层学习和人类反馈强化学习等许多重要RL问题都可以建模为双层RL问题，虽然这些领域取得了经验性进展，但双层RL算法的理论分析尚未得到足够关注。

Method: 提出了约束双层次梯度优化（CBSO）算法，使用惩罚目标函数避免约束双层问题中的原始-对偶间隙和超梯度问题，采用Moreau包络分析非光滑优化问题。

Result: CBSO算法获得了O(ε^{-2})的迭代复杂度和Õ(ε^{-4})的样本复杂度，这是首次使用Moreau包络分析非光滑目标函数的参数化策略梯度RL算法。

Conclusion: 该工作为约束双层RL问题提供了理论分析框架，通过惩罚方法和Moreau包络技术处理非光滑优化，为元学习、分层学习等领域的理论分析奠定了基础。

Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(ε^{-2})$ and sample complexity of $\tilde{O}(ε^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.

</details>


### [107] [Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets](https://arxiv.org/abs/2602.01682)
*Taihei Oki,Shinsaku Sakaue*

Main category: cs.LG

TL;DR: 该论文研究了在线逆线性优化问题，证明了当可行集为M-凸集时，可以获得O(d log d)的有限遗憾界，解决了该领域的一个开放问题。


<details>
  <summary>Details</summary>
Motivation: 在线逆线性优化（又称上下文推荐）中，学习者需要从随时间变化的可行集中观察最优动作来推断代理的隐藏目标向量。先前研究建立了O(d log T)的遗憾界和指数级有限界exp(O(d log d))，但是否存在多项式有限遗憾界一直是个开放问题。

Method: 结合M-凸集上最优解的结构特征和几何体积论证方法，并扩展到对抗性反馈场景，通过监测观察反馈诱导的有向图来自适应检测腐败。

Result: 当可行集为M-凸集时，获得了O(d log d)的有限遗憾界；在最多C轮对抗性腐败反馈下，获得了O((C+1)d log d)的遗憾界，且无需事先知道C值。

Conclusion: 该研究部分解决了在线逆线性优化中是否存在多项式有限遗憾界的开放问题，证明了在M-凸集类（包括拟阵）上可以获得O(d log d)的有限遗憾界，并扩展到对抗性反馈场景。

Abstract: We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\log T)$, as well as a finite but exponentially large bound of $\exp(O(d\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $Ω(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.

</details>


### [108] [Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.00286)
*Shaorong Zhang,Longxuan Yu,Rob Brekelmans,Luhan Tang,Salman Asif,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 本文提出了一个信息论框架来分析掩码扩散模型中的顺序敏感性和并行化偏差两种失败源，揭示了易优先解码的优势、并行解码的内在采样误差问题，以及验证的指数成本。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型通过牺牲顺序确定性来加速推理，但其生成顺序的理论机制和并行化的风险尚未得到充分探索。本文旨在建立一个统一的信息论框架来分析和解耦这两种基本的失败源。

Method: 提出了一个统一的信息论框架，分析顺序敏感性和并行化偏差两种失败源。通过理论分析得出三个关键见解，并在受控的Block-HMM和大规模MDMs（LLaDA）上进行算术推理实验验证。

Result: 研究发现：(1) 随着模型误差增加，易优先解码（优先处理低熵标记）的优势会放大；(2) 因子化并行解码会引入内在采样误差，可能导致任意大的反向KL散度，捕捉到标准前向KL指标忽略的"不连贯"失败；(3) 验证可以消除采样误差，但会产生由块内总相关性决定的指数成本。

Conclusion: 本文提供了一个理论框架来分析掩码扩散模型中的顺序敏感性和并行化偏差问题。虽然重掩码等启发式方法计算效率高，但不能保证分布正确性。实验验证了理论框架的有效性，为理解MDMs的失败机制提供了理论基础。

Abstract: Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing "incoherence" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.

</details>


### [109] [Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions](https://arxiv.org/abs/2602.01777)
*M. Arashi,M. Amintoosi*

Main category: cs.LG

TL;DR: 提出基于Stein规则收缩的随机梯度估计器，通过收缩噪声小批量梯度向动量稳定估计器，在p≥3时平方误差损失下优于标准随机梯度，并集成到Adam中提升大批次训练性能


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度方法将小批量梯度视为无偏估计，但统计决策理论表明在二次损失下无偏估计通常不可接受，说明标准随机梯度从风险角度可能不是最优的，特别是在高维设置中

Method: 将随机梯度计算构建为高维估计问题，基于Stein规则收缩构建收缩梯度估计器，自适应地将噪声小批量梯度收缩向历史动量推导的稳定受限估计器，收缩强度通过在线估计梯度噪声方差数据驱动确定

Result: 在p≥3的高斯噪声模型下，提出的估计器在平方误差损失下一致优于标准随机梯度，且是极小极大最优的；集成到Adam后在CIFAR10/100上，特别是在大批次和标签噪声条件下，相比Adam有稳定提升

Conclusion: 经典收缩原则为现代深度学习中的随机梯度估计提供了原则性且有效的改进方法，通过选择性地对高维卷积层应用收缩可获得最佳性能，而全参数收缩会降低效果

Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.

</details>


### [110] [Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation](https://arxiv.org/abs/2602.00294)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 提出一种高效计算自注意力机制的新方法，能以恒定成本实现任意精度，大幅降低内存和计算需求


<details>
  <summary>Details</summary>
Motivation: 传统Transformer自注意力机制的计算成本随上下文长度增加而增加，导致存储、计算和能源需求超出社会供给能力，需要更高效的解决方案

Method: 通过将传统自注意力的泰勒展开分解为对称张量链表达式，利用对称性设计前馈变换，将查询和键映射到最小多项式核特征基坐标，实现恒定成本计算

Result: 实现了以恒定成本计算任意精度的自注意力，内存使用和计算量降低数个数量级，支持无限制的token生成，显著减少大规模Transformer模型的基础设施和能源需求

Conclusion: 提出的方法能以固定成本高效计算自注意力，大幅降低Transformer模型的资源消耗，所引入的数学技术具有独立的研究价值

Abstract: The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.

</details>


### [111] [Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning](https://arxiv.org/abs/2602.01853)
*Xiangkun Wu,Qianglin Wen,Yingying Zhang,Hongtu Zhu,Ting Li,Chengchun Shi*

Main category: cs.LG

TL;DR: 提出基于Transformer强化学习的时间序列A/B测试方法，通过利用完整历史信息和直接优化MSE，优于现有设计


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试在时间序列实验中存在两个主要问题：1）未能充分利用完整历史信息进行干预分配；2）依赖强假设来近似目标函数（如处理效应估计的均方误差）

Method: 提出Transformer强化学习方法，利用Transformer对完整历史信息进行条件分配，使用强化学习直接优化MSE而不依赖限制性假设

Result: 在合成数据、公开调度模拟器和真实世界网约车数据集上的实证评估表明，该方法始终优于现有设计

Conclusion: 通过结合Transformer和强化学习，成功解决了时间序列A/B测试中的历史信息利用和目标函数优化问题，提供了更优的实验设计方法

Abstract: A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.

</details>


### [112] [From Observations to States: Latent Time Series Forecasting](https://arxiv.org/abs/2602.00297)
*Jie Yang,Yifan Hu,Yuante Li,Kexin Zhang,Kaize Ding,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出LatentTSF新范式，将时间序列预测从观测空间回归转为潜在状态预测，解决"潜在混沌"问题


<details>
  <summary>Details</summary>
Motivation: 现有深度学习时间序列预测模型存在"潜在混沌"问题：预测准确但潜在表示在时间上无序且缺乏连续性，这源于主流的观测空间预测范式只最小化噪声和部分观测数据的逐点误差，鼓励捷径解而非恢复底层系统动态

Method: 提出LatentTSF范式，使用自编码器将每个时间步的观测投影到高维潜在状态空间，然后在潜在空间中进行预测，专注于学习结构化时间动态

Result: 理论分析表明潜在目标函数隐式最大化预测潜在状态与真实状态及观测之间的互信息；在广泛使用的基准测试中，LatentTSF有效缓解潜在混沌，实现优越性能

Conclusion: LatentTSF通过将预测从观测回归转向潜在状态预测，解决了时间序列预测中的潜在混沌问题，为学习底层系统动态提供了新范式

Abstract: Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.

</details>


### [113] [Observation-dependent Bayesian active learning via input-warped Gaussian processes](https://arxiv.org/abs/2602.01898)
*Sanna Jarl,Maria Bånkestad,Jonathan J. S. Scragg,Jens Sjölund*

Main category: cs.LG

TL;DR: 提出一种通过单调重参数化扭曲输入空间的方法，使贝叶斯主动学习的探索策略能够根据观测反馈调整，提高样本效率


<details>
  <summary>Details</summary>
Motivation: 高斯过程代理模型的后验方差仅通过超参数依赖于观测输出，导致探索策略对实际测量不敏感。需要一种机制使设计策略能够根据观测变异性调整输入空间

Method: 通过学习单调重参数化扭曲输入空间，使设计策略能够根据观测变异性扩展或压缩输入空间区域，从而塑造基于方差的采集函数行为。使用新颖的自监督目标训练扭曲函数

Result: 该方法在一系列主动学习基准测试中提高了样本效率，特别是在非平稳性挑战传统方法的场景中表现更好

Conclusion: 通过输入空间扭曲注入观测依赖反馈，能够有效提高贝叶斯主动学习的探索效率，特别是在非平稳函数景观中

Abstract: Bayesian active learning relies on the precise quantification of predictive uncertainty to explore unknown function landscapes. While Gaussian process surrogates are the standard for such tasks, an underappreciated fact is that their posterior variance depends on the observed outputs only through the hyperparameters, rendering exploration largely insensitive to the actual measurements. We propose to inject observation-dependent feedback by warping the input space with a learned, monotone reparameterization. This mechanism allows the design policy to expand or compress regions of the input space in response to observed variability, thereby shaping the behavior of variance-based acquisition functions. We demonstrate that while such warps can be trained via marginal likelihood, a novel self-supervised objective yields substantially better performance. Our approach improves sample efficiency across a range of active learning benchmarks, particularly in regimes where non-stationarity challenges traditional methods.

</details>


### [114] [Agentic Framework for Epidemiological Modeling](https://arxiv.org/abs/2602.00299)
*Rituparna Datta,Zihan Guan,Baltazar Espinoza,Yiqi Su,Priya Pitre,Srini Venkatramanan,Naren Ramakrishnan,Anil Vullikanti*

Main category: cs.LG

TL;DR: EPIAGENT是一个自动合成、校准、验证和优化流行病学模拟器的智能体框架，通过将疾病进展建模为迭代程序合成问题，显著加速有效模型的收敛


<details>
  <summary>Details</summary>
Motivation: 传统流行病建模方法依赖固定模型类别，需要随着病原体、政策和情景假设的变化而手动重新设计，这限制了建模的灵活性和效率

Method: 采用智能体框架，将疾病进展建模为迭代程序合成问题；使用流行病学流程图中间表示连接情景规范与模型结构，在代码生成前进行模块化正确性检查；将验证后的流程图编译为支持可解释参数学习的机制模型

Result: 在流行病学情景案例研究中，EPIAGENT能够捕捉复杂的增长动态，并在不同疫苗接种和免疫逃逸假设下产生流行病学一致的反事实预测；智能体反馈循环防止退化，显著加速向有效模型的收敛

Conclusion: EPIAGENT通过模仿专业专家工作流程，实现了流行病学模拟器的自动合成和优化，解决了传统固定模型方法的局限性，为公共卫生规划提供了更灵活高效的建模工具

Abstract: Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.

</details>


### [115] [Data- and Variance-dependent Regret Bounds for Online Tabular MDPs](https://arxiv.org/abs/2602.01903)
*Mingyi Li,Taira Tsuchiya,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 本文研究了具有已知转移的表格MDP在线学习问题，提出了在对抗性和随机性环境下都能获得最优性能的算法，实现了数据依赖和方差依赖的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 现有的MDP在线学习算法通常在对抗性或随机性环境中分别表现良好，但缺乏能够同时适应两种环境的统一算法。本文旨在开发"两全其美"的算法，在两种环境下都能达到最优的遗憾上界。

Method: 提出了基于全局优化和策略优化的两种算法框架，都建立在带对数障碍正则化的乐观跟随正则化领导者方法上。全局优化方法直接优化策略，策略优化方法则通过新的乐观Q函数估计器进行优化。

Result: 在对抗性环境下，算法实现了一阶、二阶和路径长度遗憾上界；在随机环境下，实现了方差感知的无间隙依赖和有间隙依赖的遗憾上界（后者对回合数呈多对数增长）。策略优化方法也达到了类似的自适应性，只是多了一个回合长度的因子。

Conclusion: 本文提出的算法在对抗性和随机性环境下都能达到接近最优的遗憾上界，通过建立的下界证明全局优化方法的遗憾上界几乎是紧的，为MDP在线学习提供了统一的自适应算法框架。

Abstract: This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.

</details>


### [116] [Neural Ising Machines via Unrolling and Zeroth-Order Training](https://arxiv.org/abs/2602.00302)
*Sam Reifenstein,Timothee Leleu*

Main category: cs.LG

TL;DR: 提出NPIM（神经网络参数化伊辛机），通过数据驱动学习迭代动力系统的更新规则，用于NP难的伊辛模型和最大割优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统伊辛机启发式算法需要手动设计更新规则，而学习型方法可以通过数据驱动自动发现有效的优化策略，提高在非凸能量景观中的搜索效率。

Method: 使用紧凑的多层感知机参数化节点级更新规则，将局部相互作用场映射到自旋更新。采用零阶优化器训练，避免长循环动力系统反向传播的不稳定梯度问题。

Result: NPIM恢复了有效的算法结构（如动量行为和时变调度），在标准伊辛和神经组合优化基准测试中，相比最近的学习方法和经典启发式算法，获得有竞争力的解质量和求解时间。

Conclusion: NPIM展示了通过数据驱动学习迭代动力系统更新规则的有效性，能够自动发现复杂的优化策略，为组合优化问题提供了新的学习型求解框架。

Abstract: We propose a data-driven heuristic for NP-hard Ising and Max-Cut optimization that learns the update rule of an iterative dynamical system. The method learns a shared, node-wise update rule that maps local interaction fields to spin updates, parameterized by a compact multilayer perceptron with a small number of parameters. Training is performed using a zeroth-order optimizer, since backpropagation through long, recurrent Ising-machine dynamics leads to unstable and poorly informative gradients. We call this approach a neural network parameterized Ising machine (NPIM). Despite its low parameter count, the learned dynamics recover effective algorithmic structure, including momentum-like behavior and time-varying schedules, enabling efficient search in highly non-convex energy landscapes. Across standard Ising and neural combinatorial optimization benchmarks, NPIM achieves competitive solution quality and time-to-solution relative to recent learning-based methods and strong classical Ising-machine heuristics.

</details>


### [117] [Deep Multivariate Models with Parametric Conditionals](https://arxiv.org/abs/2602.01953)
*Dmitrij Schlesinger,Boris Flach,Alexander Shekhovtsov*

Main category: cs.LG

TL;DR: 提出一种基于条件概率分布的深度多变量模型，通过训练参数化马尔可夫链核来学习联合分布，适用于多种下游任务和半监督学习场景。


<details>
  <summary>Details</summary>
Motivation: 现有深度多变量模型通常针对特定应用任务设计，限制了在其他下游任务中的适用性。需要一种更通用的建模方法，能够灵活处理异构变量集合（如图像、分割、属性、潜变量等）。

Method: 通过每个变量组相对于其他变量的条件概率分布来表示联合概率分布。采用参数化马尔可夫链核训练方法，通过最大化其极限分布的数据似然来学习模型。

Result: 该方法能够构建适用于几乎所有可能下游任务的通用模型，同时支持广泛的半监督学习场景。

Conclusion: 提出的条件概率分布建模方法相比传统任务特定设计更具通用性，通过马尔可夫链核训练策略实现了灵活的多变量建模和半监督学习能力。

Abstract: We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.

</details>


### [118] [Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors](https://arxiv.org/abs/2602.00315)
*Arian Khorasani,Nathaniel Chen,Yug D Oswal,Akshat Santhana Gopalan,Egemen Kolemen,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 使用类别条件归一化流作为oracle，在真实图像上获得精确后验分布，从而研究神经网络的极限性能、缩放规律、分布偏移和主动学习等问题。


<details>
  <summary>Details</summary>
Motivation: 标准基准测试无法评估神经网络与理论最优性能的差距，因为它们无法获得真实后验分布p(y|x)。需要一种方法来精确测量神经网络的性能极限。

Method: 使用类别条件归一化流作为oracle，在AFHQ和ImageNet等真实图像数据集上获得精确可处理的后验分布。基于此进行五个方面的研究：缩放规律、学习极限、软标签、分布偏移和主动学习。

Result: 1) 预测误差可分解为不可约的偶然不确定性和可约的认知误差；认知误差随数据集大小呈幂律衰减。2) 不同架构接近理论极限的方式不同：ResNets呈现清晰的幂律缩放，而Vision Transformers在低数据区域停滞。3) 使用精确后验作为软标签优于硬标签，且校准效果近乎完美。4) 分布偏移类型比幅度更重要。5) 精确认知不确定性可区分信息性样本和固有模糊样本，提高主动学习效率。

Conclusion: 该框架揭示了标准指标隐藏了持续学习过程、掩盖了架构差异，且无法诊断分布偏移的本质。通过精确后验分析，可以更深入地理解神经网络的极限性能和学习动态。

Abstract: How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.

</details>


### [119] [SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation](https://arxiv.org/abs/2602.02013)
*Xiaoyi Jiang,Andreas Nienkötter*

Main category: cs.LG

TL;DR: SNAP是一个基于相互一致性的自监督鲁棒计算框架，通过一致性-可靠性假设为数据项分配权重，强调可信项目并抑制离群值，无需监督或先验知识。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒计算方法通常需要监督或先验知识，且在高维设置中可能效果不佳。SNAP旨在开发一种无需监督、基于数据内在一致性的鲁棒计算框架，能够有效处理离群值。

Method: SNAP基于一致性-可靠性假设，通过量化数据项之间的相互一致性来分配权重。核心机制是离群值权重指数抑制，确保离群值对计算的贡献可忽略不计。该方法是非迭代的，计算效率高。

Result: SNAP在向量平均和子空间估计任务中表现出色，非迭代的SNAP超越了迭代的Weiszfeld算法和两种多元中位数均值变体。离群值权重指数抑制在高维设置中仍然有效。

Conclusion: SNAP提供了一个灵活、易用、广泛适用的鲁棒计算方法，无需监督或先验知识，通过基于一致性的权重分配有效抑制离群值影响，在多种计算任务中优于现有方法。

Abstract: We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.

</details>


### [120] [Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection](https://arxiv.org/abs/2602.00318)
*Kunal Mukherjee,Zulfikar Alom,Tran Gia Bao Ngo,Cuneyt Gurcan Akcora,Murat Kantarcioglu*

Main category: cs.LG

TL;DR: BOCLOAK：一种基于最优传输的轻量级对抗攻击框架，用于在现实约束下评估GNN社交机器人检测器的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 当前基于GNN的社交机器人检测器在现实环境中的鲁棒性评估不足，现有攻击方法往往忽略现实约束（如领域特定限制和时间约束），导致评估结果不准确

Method: BOCLOAK通过构建时空邻居特征的概率度量，学习分离人类和机器人行为的最优传输几何，然后将传输计划解码为稀疏、合理的边编辑，在遵守现实约束的同时规避检测

Result: 在三个社交机器人数据集、五个最先进的机器人检测器、三种对抗防御方法上评估，BOCLOAK相比四个基线方法，攻击成功率提高80.13%，GPU内存使用减少99.80%

Conclusion: 最优传输提供了一个轻量级、原则性的框架，能够弥合对抗攻击与现实机器人检测之间的差距，为评估GNN检测器在真实约束下的鲁棒性提供了有效工具

Abstract: The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.
  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.

</details>


### [121] [Efficient Swap Regret Minimization in Combinatorial Bandits](https://arxiv.org/abs/2602.02087)
*Andreas Kontogiannis,Vasilis Pollatos,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.LG

TL;DR: 提出了一种针对组合老虎机的高效无交换遗憾算法，该算法在遗憾上具有对动作数N的多对数依赖，解决了该领域长期存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 在组合老虎机中，动作数量N相对于问题维度呈指数增长。虽然外部遗憾最小化问题已有较好理解，但实现具有多对数N依赖的无交换遗憾一直是个难题。本文旨在解决这一挑战。

Method: 引入了一种新的无交换遗憾学习算法，该算法在遗憾上具有多对数N依赖。算法设计确保每次迭代的计算复杂度也按多对数N缩放，使其在实际应用中高效可行。

Result: 提出的算法在组合老虎机类中达到了紧致的遗憾边界，遗憾随N多对数增长，且实现了次线性（相对于时间范围T）的交换遗憾。算法在多种经典应用中都能高效实现。

Conclusion: 本文解决了组合老虎机中实现高效无交换遗憾的长期挑战，提出的算法在遗憾边界和计算效率上都达到了最优，为组合老虎机领域提供了重要的理论进展。

Abstract: This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.

</details>


### [122] [Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference](https://arxiv.org/abs/2602.00328)
*Nikhil Gopal,Kostis Kaffes*

Main category: cs.LG

TL;DR: Harvest框架利用GPU间高速互连，将模型权重和KV缓存动态放置在空闲GPU内存中，显著提升推理吞吐量


<details>
  <summary>Details</summary>
Motivation: LLM推理越来越受限于GPU内存容量而非计算吞吐量，现有方法通过将模型状态和KV张量卸载到主机内存会因PCIe带宽限制导致显著延迟

Method: Harvest框架利用GPU间高速对等互连，将模型权重和KV缓存动态放置在未使用的GPU内存中，将其他GPU内存视为临时缓存层

Result: 通过加速专家层权重和KV缓存条目的检索，实现了超过2倍的吞吐量加速

Conclusion: Harvest框架通过利用空闲GPU内存作为缓存层，有效解决了LLM推理中的内存瓶颈问题，显著提升了推理性能

Abstract: Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.

</details>


### [123] [Spectral Superposition: A Theory of Feature Geometry](https://arxiv.org/abs/2602.02224)
*Georgi Ivanov,Narmeen Oozeer,Shivam Raval,Tasana Pejovic,Shriyash Upadhyay,Amir Abdullah*

Main category: cs.LG

TL;DR: 该论文提出了一种利用谱方法分析神经网络特征几何结构的新理论，通过权重矩阵的谱分析来研究特征在叠加状态下的全局几何关系。


<details>
  <summary>Details</summary>
Motivation: 当前方法将激活分解为稀疏线性特征但丢弃了几何结构，需要新的理论工具来研究特征在叠加状态下的全局几何关系。

Method: 引入框架算子F=WW⊤，通过分析权重矩阵的谱（特征值、特征空间等）来研究特征的几何结构，使用谱测度描述特征在特征空间中的范数分配。

Result: 在叠加的玩具模型中证明：容量饱和迫使谱局部化，特征坍缩到单个特征空间，组织成紧框架，并通过关联方案进行分类，涵盖了先前工作中的所有几何结构。

Conclusion: 谱测度形式适用于任意权重矩阵，能够诊断特征局部化现象，为将算子理论应用于可解释性研究开辟了更广泛的研究方向。

Abstract: Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.

</details>


### [124] [In-Run Data Shapley for Adam Optimizer](https://arxiv.org/abs/2602.00329)
*Meng Ding,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 本文提出Adam-Aware In-Run Data Shapley方法，解决了现有基于SGD的数据归因方法在Adam优化器下失效的问题，实现了高效准确的数据贡献度评估。


<details>
  <summary>Details</summary>
Motivation: 可靠的数据归因对于减少机器学习中的偏见和计算浪费至关重要。现有的"In-Run"方法虽然避免了重新训练的高成本，但严重依赖SGD的线性结构，无法捕捉Adam等自适应优化器的复杂动态。研究发现数据归因本质上依赖于优化器，SGD代理在Adam下与真实贡献度显著偏离，无法适用于现代训练流程。

Method: 提出Adam-Aware In-Run Data Shapley方法：1）通过固定状态假设重新定义效用函数，恢复可加性；2）提出线性化幽灵近似技术，线性化方差依赖的缩放项，无需显式计算每个样本的梯度即可计算梯度点积对，实现可扩展计算。

Result: 实验表明：1）该方法与真实边际贡献度达到近乎完美的保真度（Pearson R > 0.99）；2）保持约95%的标准训练吞吐量；3）在数据归因下游任务中显著优于基于SGD的基线方法。

Conclusion: 数据归因是优化器依赖的，需要针对特定优化器设计归因方法。提出的Adam感知方法有效解决了Adam优化器下的数据归因问题，为现代训练流程提供了可靠的数据贡献度评估工具。

Abstract: Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent "In-Run" methods bypass the prohibitive cost of retraining by estimating contributions dynamically, they heavily rely on the linear structure of Stochastic Gradient Descent (SGD) and fail to capture the complex dynamics of adaptive optimizers like Adam. In this work, we demonstrate that data attribution is inherently optimizer-dependent: we show that SGD-based proxies diverge significantly from true contributions under Adam (Pearson $R \approx 0.11$), rendering them ineffective for modern training pipelines. To bridge this gap, we propose Adam-Aware In-Run Data Shapley. We derive a closed-form approximation that restores additivity by redefining utility under a fixed-state assumption and enable scalable computation via a novel Linearized Ghost Approximation. This technique linearizes the variance-dependent scaling term, allowing us to compute pairwise gradient dot-products without materializing per-sample gradients. Extensive experiments show that our method achieves near-perfect fidelity to ground-truth marginal contributions ($R > 0.99$) while retaining $\sim$95\% of standard training throughput. Furthermore, our Adam-aware attribution significantly outperforms SGD-based baselines in data attribution downstream tasks.

</details>


### [125] [Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management](https://arxiv.org/abs/2602.02283)
*Owen Shen,Patrick Jaillet*

Main category: cs.LG

TL;DR: 提出"选择模型辅助强化学习"，使用校准的离散选择模型作为固定部分世界模型，在决策时估算延迟反馈的学习目标，以解决收益管理中延迟反馈问题。


<details>
  <summary>Details</summary>
Motivation: 解决收益管理中延迟反馈问题，即客户取消和修改预订的价值在预订后数天才确定，这导致强化学习面临延迟奖励的挑战。

Method: 提出选择模型辅助强化学习方法：使用校准的离散选择模型作为固定部分世界模型，在决策时估算延迟反馈的学习目标，结合表格Q学习进行训练。

Result: 理论证明：表格Q学习收敛到最优Q函数的O(ε/(1-γ))邻域；实验验证：在平稳设置中与基线无显著差异，在参数偏移下5/10场景有显著收益提升（最高12.4%），但在模型误设下收益降低1.4-2.6%。

Conclusion: 部分行为模型在参数偏移下能提高鲁棒性，但在结构误设下会引入有害偏差，这为理解何时使用部分模型提供了理论指导。

Abstract: We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.

</details>


### [126] [Prototype-based Explainable Neural Networks with Channel-specific Reasoning for Geospatial Learning Tasks](https://arxiv.org/abs/2602.00331)
*Anushka Narayanan,Karianne J. Bergen*

Main category: cs.LG

TL;DR: 提出针对多通道地理空间数据的原型可解释AI方法，通过通道特定原型增强地理科学机器学习模型的透明度和可信度


<details>
  <summary>Details</summary>
Motivation: 现有原型XAI方法主要针对标准RGB图像设计，不适用于具有不同物理变量的多通道地理科学数据，需要专门的方法来理解各通道特征如何单独和组合影响模型预测

Method: 开发针对多通道地理空间数据的原型XAI方法，允许模型识别来自不同训练样本的通道特定原型特征，这些原型代表目标类的典型特征，预测基于输入与这些原型的相似性

Result: 在两个地理科学案例中验证方法：1)使用多变量气候数据分类Madden Julian振荡相位；2)多光谱卫星图像的土地利用分类。方法提供局部和全局解释，性能与标准神经网络相当

Conclusion: 通过将通道原型明确纳入预测过程，该方法增强了地理科学学习任务中机器学习模型的透明度和可信度，为理解多通道地理空间数据的决策提供了内在可解释的替代方案

Abstract: Explainable AI (XAI) is essential for understanding machine learning (ML) decision-making and ensuring model trustworthiness in scientific applications. Prototype-based XAI methods offer an intrinsically interpretable alternative to post-hoc approaches which often yield inconsistent explanations. Prototype-based XAI methods make predictions based on the similarity between inputs and learned prototypes that represent typical characteristics of target classes. However, existing prototype-based models are primarily designed for standard RGB image data and are not optimized for the distinct, variable-specific channels commonly found in geoscientific image and raster datasets. In this study, we develop a prototype-based XAI approach tailored for multi-channel geospatial data, where each channel represents a distinct physical environmental variable or spectral channel. Our approach enables the model to identify separate, channel-specific prototypical characteristics sourced from multiple distinct training examples that inform how these features individually and in combination influence model prediction while achieving comparable performance to standard neural networks. We demonstrate this method through two geoscientific case studies: (1) classification of Madden Julian Oscillation phases using multi-variable climate data and (2) land-use classification from multispectral satellite imagery. This approach produces both local (instance-level) and global (model-level) explanations for providing insights into feature-relevance across channels. By explicitly incorporating channel-prototypes into the prediction process, we discuss how this approach enhances the transparency and trustworthiness of ML models for geoscientific learning tasks.

</details>


### [127] [C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference](https://arxiv.org/abs/2602.02371)
*Jing Wang,Jie Shen,Qiaomin Xie,Jeremy C Weiss*

Main category: cs.LG

TL;DR: C-kNN-LSH：一种用于纵向轨迹因果推断的最近邻框架，利用局部敏感哈希高效识别"临床双胞胎"，通过双重稳健校正处理不规则采样和患者恢复变化。


<details>
  <summary>Details</summary>
Motivation: 从纵向轨迹估计因果效应对于理解复杂疾病进展和优化临床决策至关重要，特别是在共病和长新冠恢复等场景中。现有方法难以处理高维、混杂的纵向数据。

Method: 提出C-kNN-LSH框架：1) 使用局部敏感哈希高效识别具有相似协变量历史的"临床双胞胎"；2) 在演变的疾病状态下进行局部条件治疗效应估计；3) 集成邻域估计器与双重稳健校正，减轻不规则采样和患者恢复变化带来的偏差。

Result: 理论分析证明估计器具有一致性和对干扰误差的二阶稳健性。在包含13,511名参与者的真实世界长新冠队列中，C-kNN-LSH在捕捉恢复异质性和估计策略价值方面优于现有基线方法。

Conclusion: C-kNN-LSH为高维、混杂的纵向因果推断提供了一种有效框架，特别适用于临床轨迹分析，能够识别相似患者群体并准确估计治疗效应，对长新冠等复杂疾病的临床决策有重要价值。

Abstract: Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.
  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.
  Evaluated on a real-world Long COVID cohort with 13,511 participants, \emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.

</details>


### [128] [Efficient and accurate steering of Large Language Models through attention-guided feature learning](https://arxiv.org/abs/2602.00333)
*Parmida Davarmanesh,Ashia Wilson,Adityanarayanan Radhakrishnan*

Main category: cs.LG

TL;DR: 本文提出了一种注意力引导的引导框架，解决了LLM内部激活引导中的三个核心挑战：自动选择相关token嵌入、处理概念特征异质性、识别最相关的引导层，在512个语义概念的基准测试中性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM内部激活引导方法非常脆弱，看似不可引导的概念会因为特征提取算法的细微选择而变得完全可引导。需要一种更鲁棒的引导框架来解决这一挑战。

Method: 提出注意力引导的引导框架，通过三个关键技术：1) 自动选择相关token嵌入提取概念相关特征；2) 考虑LLM激活中概念相关特征的异质性；3) 识别最适合引导的层。

Result: 在512个语义概念的引导基准测试中，该框架显著优于先前最先进方法（成功引导概念数量几乎翻倍），在不同架构和大小的模型上（包括700亿参数模型）都表现良好。

Conclusion: 该框架为开发高效、高度可扩展的行业级LLM微调算法开辟了新途径，并有助于理解概念特定特征在LLM层中的分布。

Abstract: Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.

</details>


### [129] [Maximizing Reliability with Bayesian Optimization](https://arxiv.org/abs/2602.02432)
*Jack M. Buckingham,Ivo Couckuyt,Juergen Branke*

Main category: cs.LG

TL;DR: 提出两种基于Thompson采样和知识梯度的贝叶斯优化方法，用于极小失效概率（10⁻⁶-10⁻⁸）的可靠性优化问题，结合重要性采样提升效率。


<details>
  <summary>Details</summary>
Motivation: 制造业中需要优化设计的可靠性，即最小化随机扰动下的失效概率，但失效概率极低（10⁻⁶-10⁻⁸），传统贝叶斯优化方法难以有效处理这种极端罕见事件。

Method: 提出两种贝叶斯优化方法：1）基于Thompson采样；2）基于知识梯度（近似一步贝叶斯最优策略以最小化失效概率对数）。两种方法都结合重要性采样来处理极小的失效概率。

Result: 实验结果表明，所提出的方法在极端和非极端失效概率情况下都优于现有方法。

Conclusion: 结合重要性采样的贝叶斯优化方法能有效处理极小失效概率的可靠性优化问题，Thompson采样和知识梯度两种策略均表现出色。

Abstract: Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.

</details>


### [130] [Adaptive Momentum and Nonlinear Damping for Neural Network Training](https://arxiv.org/abs/2602.00334)
*Aikaterini Karoni,Rajit Rajpal,Benedict Leimkuhler,Gabriel Stoltz*

Main category: cs.LG

TL;DR: 提出一种连续时间优化方案，通过动能调节的自适应动量系数，将立方阻尼机制引入优化算法，在ViT、BERT、GPT2等任务上表现优于Adam。


<details>
  <summary>Details</summary>
Motivation: 传统优化算法如mSGD在大规模优化任务中面临稳定性与收敛速度的权衡问题，需要一种能自动适应局部曲率并保持稳定性的自适应机制。

Method: 提出连续时间优化方案，为每个模型参数引入由动能调节的个体自适应动量系数，将立方阻尼机制（来自结构动力学）融入优化过程，具体通过为mSGD和Adam的连续动力学添加立方阻尼项实现。

Result: 实验表明该方法在训练ViT、BERT和GPT2等任务上表现出鲁棒性，匹配或优于Adam的性能，特别是在mSGD通常表现不佳的任务上。理论分析证明了所提方案的指数收敛性。

Conclusion: 通过引入基于动能的个体自适应动量系数和立方阻尼机制，提出了一种稳定且高效的连续时间优化方案，在大规模深度学习任务中具有优越性能。

Abstract: We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.

</details>


### [131] [Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design](https://arxiv.org/abs/2602.00357)
*Chenyang Yuan,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: 该论文提出使用基于扩散的生成推理模型和统一奖励函数来解决室内AP部署规划问题，相比传统LLM方法具有更好的可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中，由于复杂的室内几何结构和信号传播特性，智能AP部署仍然具有挑战性。作者首先基准测试了通用大语言模型作为AP规划的代理优化器，发现尽管它们具有强大的无线领域知识，但对外部验证器的依赖导致高计算成本和有限的可扩展性。

Method: 研究生成推理模型，使用统一奖励函数捕捉不同楼层平面图中AP部署的核心目标。采用扩散采样器，该过程通过平滑和锐化奖励景观来逐步改进采样，而不是依赖迭代细化。引入大规模真实世界室内AP部署数据集，需要超过5万CPU小时来训练通用奖励函数。

Result: 扩散采样器始终优于其他生成方法。扩散过程通过平滑和锐化奖励景观来逐步改进采样，对于非凸和碎片化的目标函数特别有效。评估了分布内和分布外的泛化能力和鲁棒性。

Conclusion: 基于扩散的生成推理与统一奖励函数为室内AP部署规划提供了可扩展且领域无关的基础，相比依赖外部验证器的LLM方法具有更好的性能和效率。

Abstract: Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.

</details>


### [132] [Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition](https://arxiv.org/abs/2602.00360)
*Sumana Biswas,Karen Young,Josephine Griffith*

Main category: cs.LG

TL;DR: 本文提出TEMSA方法，通过提取图像中所有检测到的物体名称并与相关文本结合，改善多模态情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临文本和图像模态差异、情感模糊性以及上下文复杂性等挑战，需要有效结合图像和文本信息的方法。

Method: 提出TEMSA方法：提取图像中检测到的所有物体名称，将其与相关文本结合形成TEMS数据，用于多模态情感分析。

Result: 实验结果表明，仅使用TEMS数据（结合所有物体名称）相比单独分析能改善多模态数据的整体情感分析结果。

Conclusion: TEMSA方法能有效结合图像和文本数据，为多模态情感分析提供了新的有效途径。

Abstract: Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.

</details>


### [133] [Quantum Generator Kernels](https://arxiv.org/abs/2602.00361)
*Philipp Altmann,Maximilian Mansky,Maximilian Zorn,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 提出量子生成核（QGKs），通过变分生成组（VGGs）构建可参数化的量子核，解决NISQ硬件限制下大规模数据嵌入问题，在投影和分类任务上优于现有量子与经典核方法。


<details>
  <summary>Details</summary>
Motivation: 量子核方法理论上能将经典不可分特征在量子空间中分离，但受限于NISQ硬件容量，需要有效压缩和嵌入大规模现实数据（如图像）的策略。现有混合架构的固定嵌入过程可能阻碍充分利用量子计算潜力。

Method: 提出量子生成核（QGKs），包含一组变分生成组（VGGs），将通用生成器合并为可参数化算子，确保量子空间的可扩展覆盖。通过训练权重向量参数化VGGs在当前数据上下文中的投影，优化核与目标领域的对齐。

Result: 实证结果显示QGKs在投影和分类能力上优于最先进的量子与经典核方法，展示了其作为各种QML应用的通用框架潜力。

Conclusion: QGKs通过生成器基础的量子核方法，解决了NISQ时代大规模数据嵌入的挑战，提供了优于现有方法的性能，并为量子机器学习应用提供了灵活框架。

Abstract: Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.

</details>


### [134] [Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation](https://arxiv.org/abs/2602.00372)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: SparseKD是一种后训练压缩方法，结合结构化SVD剪枝和自参考知识蒸馏，无需外部教师模型即可实现15-65%的参数减少，保持可接受的质量损失。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署成本高昂，需要高效的压缩方法。现有方法通常需要外部教师模型或架构修改，限制了实际部署的便利性。

Method: 结合结构化SVD剪枝和自参考知识蒸馏：模型通过匹配压缩前的自身概率分布来"自我教学"，无需外部教师。在固定校准数据集上应用相同的目标函数进行后训练。

Result: 自参考蒸馏单独应用可将模型质量相对原始检查点提升39%。结合结构化剪枝可实现15-65%参数减少，质量损失可接受。速度提升主要来自前馈层的密集矩阵乘法减少，注意力机制保持不变。

Conclusion: SparseKD提供了一种无需外部超教师、无需架构修改、无需定制推理内核的即时可部署压缩方案，与注意力优化方法互补，在两个模型家族上验证了高可重复性。

Abstract: Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.
  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.
  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.

</details>


### [135] [MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science](https://arxiv.org/abs/2602.00376)
*Delia McGrath,Curtis Chong,Rohil Kulkarni,Gerbrand Ceder,Adeesh Kolluru*

Main category: cs.LG

TL;DR: MATRIX是一个材料科学多模态基准测试，用于评估视觉实验数据对科学推理的影响，发现视觉监督能显著提升实验解释和文本推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估视觉实验数据在训练后是否能够超越纯文本监督，提升基于物理机制的解释推理能力。需要建立一个多模态基准来评估材料科学中的基础理论、研究级推理和实验数据解释。

Method: 引入MATRIX多模态基准，通过对比仅使用结构化文本的微调与结合配对实验图像的微调，隔离视觉基础的影响。使用相对少量的多模态数据，并强调正确的图像-文本对齐。

Result: 视觉监督使实验解释提升10-25%，文本科学推理任务提升5-16%。改进依赖于训练中正确的图像-文本对齐，显示跨模态表示迁移。在ScienceQA和PubMedQA上也观察到一致改进。

Conclusion: 结构化多模态微调的好处超越了材料科学领域，视觉基础能显著提升科学推理能力。MATRIX数据集和模型已公开可用。

Abstract: Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.

</details>


### [136] [RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints](https://arxiv.org/abs/2602.00384)
*Ke Wang,Nguyen Gia Hien Vu,Yifan Tang,Mostafa Rahmani Dehaghani,G. Gary Wang*

Main category: cs.LG

TL;DR: 提出RePaint增强框架，集成预训练的性能引导DDPM，用于工程设计中性能与参数约束下的设计生成，无需重新训练模型即可基于部分参考设计生成缺失组件。


<details>
  <summary>Details</summary>
Motivation: 传统DDPM方法无法在性能和参数约束下对部分设计进行可控重绘，需要一种无需重新训练、能基于部分参考设计生成满足约束的缺失组件的解决方案。

Method: 采用RePaint增强框架，集成预训练的性能引导DDPM，在推理过程中应用基于掩码的重采样，实现对部分设计在性能和参数约束下的高效可控重绘。

Result: 在参数化船体设计和翼型设计两个代表性问题上验证，方法能基于部分参考设计生成具有预期性能的新颖设计，精度达到或超过预训练模型，同时通过固定部分设计实现可控创新。

Conclusion: 该方法为工程应用中的参数约束感知生成设计提供了高效、无需训练的训练解决方案，支持基于部分参考设计的可控创新生成。

Abstract: This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.

</details>


### [137] [A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode](https://arxiv.org/abs/2602.00388)
*Zeyuan He,Yupeng Chen,Lang Lin,Yihan Wang,Shenxu Chang,Eric Sommerlade,Philip Torr,Junchi Yu,Adel Bibi,Jialin Yu*

Main category: cs.LG

TL;DR: 扩散大语言模型（D-LLMs）相比自回归模型具有内在安全优势，但存在上下文嵌套攻击漏洞


<details>
  <summary>Details</summary>
Motivation: 研究扩散大语言模型相对于自回归模型的安全特性，探索其内在的对抗攻击鲁棒性机制及潜在漏洞

Method: 分析扩散轨迹的逐步抑制机制，提出上下文嵌套攻击方法，将有害请求嵌入良性结构上下文中以绕过安全机制

Result: D-LLMs对传统攻击具有鲁棒性，但上下文嵌套攻击能有效绕过其安全机制，在多个模型和基准测试中达到最先进的攻击成功率，首次成功越狱Gemini Diffusion

Conclusion: D-LLMs具有内在安全优势但非绝对安全，上下文嵌套攻击暴露了其关键漏洞，为D-LLMs的早期红队测试提供了重要见解

Abstract: Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.

</details>


### [138] [Localized, High-resolution Geographic Representations with Slepian Functions](https://arxiv.org/abs/2602.00392)
*Arjun Rao,Ruth Crasto,Tessa Ooms,David Rolnick,Konstantin Klemmer,Marc Rußwurm*

Main category: cs.LG

TL;DR: 提出基于球面Slepian函数的地理位置编码器，专注于感兴趣区域的高分辨率表示，同时提供混合Slepian-球谐函数编码器平衡局部-全局性能


<details>
  <summary>Details</summary>
Motivation: 地理数据本质上是局部的，但现有机器学习模型将表示能力均匀分布在全球范围，难以满足局部应用的高分辨率需求

Method: 使用球面Slepian函数构建地理位置编码器，专注于感兴趣区域；提出混合Slepian-球谐函数编码器平衡局部与全局性能

Result: 在分类、回归和图像增强预测等五个任务中，Slepian编码优于基线方法，并在多种神经网络架构中保持性能优势

Conclusion: Slepian编码器能够有效解决地理数据局部表示问题，提供高分辨率、计算效率高的解决方案，同时保持极地安全性和球面距离保持等理想特性

Abstract: Geographic data is fundamentally local. Disease outbreaks cluster in population centers, ecological patterns emerge along coastlines, and economic activity concentrates within country borders. Machine learning models that encode geographic location, however, distribute representational capacity uniformly across the globe, struggling at the fine-grained resolutions that localized applications require. We propose a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions without extensive computational demands. For settings requiring global context, we present a hybrid Slepian-Spherical Harmonic encoder that efficiently bridges the tradeoff between local-global performance, while retaining desirable properties such as pole-safety and spherical-surface-distance preservation. Across five tasks spanning classification, regression, and image-augmented prediction, Slepian encodings outperform baselines and retain performance advantages across a wide range of neural network architectures.

</details>


### [139] [Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity](https://arxiv.org/abs/2602.00397)
*Aayush Gautam,Mukul Gagrani,Junyoung Park,Mingu Lee,Chiris Lott,Narasimha Reddy*

Main category: cs.LG

TL;DR: FastForward 是一个预测性稀疏化框架，通过块级、上下文感知的 FFN 稀疏化来加速 LLM 预填充阶段，在保持精度的同时显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM 推理的预填充阶段是长上下文工作负载的主要计算瓶颈。在短到中等上下文长度下，FFN 占用了大部分计算成本。现有的 FFN 稀疏化方法主要针对自回归解码设计，无法充分利用预填充阶段的并行性，且常常会降低精度。

Method: FastForward 结合三个关键组件：(1) 轻量级专家预测器，用于按块选择高重要性神经元；(2) 误差补偿网络，用于纠正稀疏化引起的误差；(3) 层间稀疏度调度器，根据 token-mixing 重要性分配计算资源。

Result: 在 LLaMA 和 Qwen 模型（最大 8B 参数）上，FastForward 在 50% FFN 稀疏度下实现了高达 1.45 倍的计算加速，在 LongBench 上相比密集基线的精度损失小于 6%，显著减少了首 token 时间（TTFT）。

Conclusion: FastForward 为受限硬件上的高效长上下文 LLM 推理提供了一种有效的解决方案，通过预测性稀疏化在保持精度的同时显著加速预填充阶段。

Abstract: The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context workloads. At short-to-moderate context lengths (1K--16K tokens), Feed-Forward Networks (FFNs) dominate this cost, accounting for most of the total FLOPs. Existing FFN sparsification methods, designed for autoregressive decoding, fail to exploit the prefill stage's parallelism and often degrade accuracy. To address this, we introduce FastForward, a predictive sparsity framework that accelerates LLM prefill through block-wise, context-aware FFN sparsity. FastForward combines (1) a lightweight expert predictor to select high-importance neurons per block, (2) an error compensation network to correct sparsity-induced errors, and (3) a layer-wise sparsity scheduler to allocate compute based on token-mixing importance. Across LLaMA and Qwen models up to 8B parameters, FastForward delivers up to 1.45$\times$ compute-bound speedup at 50% FFN sparsity with $<$ 6% accuracy loss compared to the dense baseline on LongBench, substantially reducing Time-to-First-Token (TTFT) for efficient, long-context LLM inference on constrained hardware.

</details>


### [140] [MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers](https://arxiv.org/abs/2602.00398)
*Ajay Jaiswal,Lauren Hannah,Han-Byul Kim,Duc Hoang,Arnav Kundu,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

TL;DR: MemoryLLM将前馈网络（FFN）与自注意力机制解耦，将其视为上下文无关的令牌级神经检索记忆，通过预计算令牌查找表提升推理效率，并引入Flex-MemoryLLM作为过渡架构。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer组件在LLM中的运作机制对AI技术进步至关重要。本文旨在解决前馈模块（FFN）可解释性挑战，通过解耦FFN与自注意力来研究FFN作为神经检索记忆的功能。

Method: 提出MemoryLLM：1）将FFN与自注意力解耦，使用令牌嵌入独立训练FFN；2）将FFN预计算为令牌查找表（ToLs），支持按需在VRAM和存储间传输；3）引入Flex-MemoryLLM作为传统Transformer与MemoryLLM间的过渡架构。

Result: MemoryLLM实现了上下文无关的FFN，可研究输入令牌如何访问FFN参数中的记忆位置，以及FFN记忆在不同下游任务中的重要性。通过预计算令牌查找表提升了推理效率。

Conclusion: MemoryLLM为研究FFN作为神经检索记忆提供了新视角，Flex-MemoryLLM弥补了使用上下文无关令牌嵌入训练FFN带来的性能差距，为Transformer架构设计提供了新思路。

Abstract: Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.

</details>


### [141] [DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning](https://arxiv.org/abs/2602.00403)
*Hon Tik Tse,Marlos C. Machado*

Main category: cs.LG

TL;DR: 提出直接近似默认表示主特征向量的神经网络目标函数，避免先计算矩阵再分解的高计算成本


<details>
  <summary>Details</summary>
Motivation: 传统方法需要先近似默认表示矩阵再进行特征分解，计算成本高且无法扩展到高维空间

Method: 推导直接近似默认表示主特征向量的神经网络目标函数

Result: 在多个环境中实证验证了目标函数的有效性，并将学习到的特征向量应用于奖励塑形

Conclusion: 提出的直接近似方法解决了传统方法计算效率低的问题，为高维空间中的强化学习应用提供了可扩展的解决方案

Abstract: In computational reinforcement learning, the default representation (DR) and its principal eigenvector have been shown to be effective for a wide variety of applications, including reward shaping, count-based exploration, option discovery, and transfer. However, in prior investigations, the eigenvectors of the DR were computed by first approximating the DR matrix, and then performing an eigendecomposition. This procedure is computationally expensive and does not scale to high-dimensional spaces. In this paper, we derive an objective for directly approximating the principal eigenvector of the DR with a neural network. We empirically demonstrate the effectiveness of the objective in a number of environments, and apply the learned eigenvectors for reward shaping.

</details>


### [142] [Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks](https://arxiv.org/abs/2602.00407)
*Suprim Nakarmi,Junggab Son,Yue Zhao,Zuobin Xiong*

Main category: cs.LG

TL;DR: 提出Fed-Listing攻击方法，通过联邦图神经网络训练中的梯度泄露推断客户端私有标签分布，无需原始数据或节点特征，在非独立同分布场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 联邦图神经网络虽然保护了数据隐私，但共享的梯度更新仍可能泄露敏感信息。现有研究对标签分布推断攻击在FedGNNs中探索不足，需要填补这一空白。

Method: Fed-Listing仅利用训练期间交换的最终层梯度，通过辅助影子数据集生成多样化的标签划分策略模拟不同客户端分布，训练攻击模型来推断目标客户端的私有标签统计信息。

Result: 在四个基准数据集和三种GNN架构上的实验表明，Fed-Listing显著优于随机猜测和Decaf等基线方法，即使在具有挑战性的非独立同分布场景下也表现优异。防御机制几乎无法降低攻击性能，除非严重损害模型效用。

Conclusion: Fed-Listing揭示了联邦图神经网络中梯度泄露的严重隐私风险，即使不访问原始数据也能准确推断标签分布，现有防御措施难以有效应对，需要更强的隐私保护机制。

Abstract: Graph Neural Networks (GNNs) have been intensively studied for their expressive representation and learning performance on graph-structured data, enabling effective modeling of complex relational dependencies among nodes and edges in various domains. However, the standalone GNNs can unleash threat surfaces and privacy implications, as some sensitive graph-structured data is collected and processed in a centralized setting. To solve this issue, Federated Graph Neural Networks (FedGNNs) are proposed to facilitate collaborative learning over decentralized local graph data, aiming to preserve user privacy. Yet, emerging research indicates that even in these settings, shared model updates, particularly gradients, can unintentionally leak sensitive information of local users. Numerous privacy inference attacks have been explored in traditional federated learning and extended to graph settings, but the problem of label distribution inference in FedGNNs remains largely underexplored. In this work, we introduce Fed-Listing (Federated Label Distribution Inference in GNNs), a novel gradient-based attack designed to infer the private label statistics of target clients in FedGNNs without access to raw data or node features. Fed-Listing only leverages the final-layer gradients exchanged during training to uncover statistical patterns that reveal class proportions in a stealthy manner. An auxiliary shadow dataset is used to generate diverse label partitioning strategies, simulating various client distributions, on which the attack model is obtained. Extensive experiments on four benchmark datasets and three GNN architectures show that Fed-Listing significantly outperforms existing baselines, including random guessing and Decaf, even under challenging non-i.i.d. scenarios. Moreover, applying defense mechanisms can barely reduce our attack performance, unless the model's utility is severely degraded.

</details>


### [143] [Variational Approach for Job Shop Scheduling](https://arxiv.org/abs/2602.00408)
*Seung Heon Oh,Jiwon Baek,Ki Young Cho,Hee Chang Yoon,Jong Hun Woo*

Main category: cs.LG

TL;DR: VG2S框架首次将变分推理引入JSSP领域，通过变分图编码器学习调度实例的鲁棒结构表示，显著提升训练稳定性和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法在JSSP中面临训练非平稳性和对未见问题实例泛化能力有限的问题，因为它们同时优化表示学习和策略执行。

Method: 提出基于ELBO和最大熵强化学习的概率目标，通过变分图编码器将表示学习与策略优化数学解耦，学习调度实例的鲁棒结构表示。

Result: VG2S在训练稳定性和超参数变化鲁棒性方面显著提升，在DMU和SWV等大规模挑战性基准实例上表现出优于现有DRL方法和传统调度规则的零样本泛化能力。

Conclusion: VG2S框架通过变分推理成功解决了JSSP中传统DRL方法的局限性，为制造调度提供了更稳定、泛化能力更强的解决方案。

Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.

</details>


### [144] [Robustness of AutoML on Dirty Categorical Data](https://arxiv.org/abs/2602.00412)
*Marcos L. P. Bueno,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 本文提出一个将分类数据转换为数值数据的管道，使AutoML方法能够处理经过高级编码方案处理的脏分类数据，并评估了当前AutoML方法在脏数据集上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然AutoML方法在分类任务中能够处理数据缺陷，但对于脏分类数据集的行为了解较少。这些数据集通常具有高基数的分类特征，而现有的研究表明形态学编码器可以提升预测性能，但它们在AutoML中的效果尚不明确。

Method: 提出一个管道，将分类数据转换为数值数据，使AutoML能够处理经过高级编码方案（如形态学编码器）处理的数据。在脏数据集上对当前AutoML方法的鲁棒性进行基准测试，并与提出的管道进行比较。

Result: 通过比较分析，揭示了使用高级编码方案的管道与现有AutoML方法在预测性能上的差异。同时，通过分析AutoML构建的ML管道，获得了超出最佳模型返回的额外洞察。

Conclusion: 提出的管道能够有效处理脏分类数据，为AutoML方法提供了更好的数据预处理方案，同时通过分析AutoML构建的完整管道，能够获得比单纯返回最佳模型更深入的见解。

Abstract: The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.

</details>


### [145] [Federated-inspired Single-cell Batch Integration in Latent Space](https://arxiv.org/abs/2602.00423)
*Quang-Huy Nguyen,Zongliang Yue,Hao Chen,Wei-Shinn Ku,Jiaqi Wang*

Main category: cs.LG

TL;DR: scBatchProx：一种基于联邦学习原理的后处理优化方法，用于改进单细胞RNA测序数据中的批次效应校正，无需原始表达数据或集中式优化。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序产生大量高维数据，但跨实验的数据积累会引入批次效应，掩盖真实的生物信号。现有批次校正方法要么校正不足，要么需要在整个数据集上进行集中式重新训练，这在分布式和持续演化的单细胞数据环境中应用受限。

Method: scBatchProx是一种后处理优化方法，受联邦学习原理启发，用于优化任意上游方法产生的细胞级嵌入。将每个批次视为客户端，在近端正则化下学习批次条件适配器，直接在潜在空间中校正批次结构，无需原始表达数据或集中式优化。

Result: 实验表明scBatchProx在整体嵌入质量上持续获得约3-8%的相对提升，批次校正和生物保守性分别在90%和85%的数据-方法对中得到改善。

Conclusion: 这项工作为动态单细胞数据系统中学习表示的实用优化迈出了一步，提供了一种轻量级、可部署的批次校正解决方案。

Abstract: Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.

</details>


### [146] [Open Materials Generation with Inference-Time Reinforcement Learning](https://arxiv.org/abs/2602.00424)
*Philipp Hoellmer,Stefano Martiniani*

Main category: cs.LG

TL;DR: OMatG-IRL：一种基于策略梯度强化学习的框架，可直接在学习的速度场上操作，无需显式计算分数，用于晶体结构预测的推理时强化学习。


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型能够预测稳定晶体结构，但难以将显式目标属性纳入生成过程。策略梯度强化学习提供了对齐生成模型与下游目标的机制，但通常需要访问分数，这阻碍了其在仅学习速度场的基于流的模型中的应用。

Method: 提出OMatG-IRL（开放材料生成与推理时强化学习），这是一种策略梯度强化学习框架，直接操作学习到的速度场，无需显式计算分数。该方法利用底层生成动力学的随机扰动，保持预训练生成模型的基线性能，同时在推理时实现探索和策略梯度估计。

Result: 首次将强化学习应用于晶体结构预测（CSP）。该方法能有效强化基于能量的目标，同时通过成分条件保持多样性，性能与基于分数的强化学习方法相当。此外，OMatG-IRL能够学习时间依赖的速度退火计划，实现精确的CSP，采样效率提高一个数量级，相应减少生成时间。

Conclusion: OMatG-IRL为基于流的生成模型提供了一种有效的推理时强化学习框架，无需显式分数计算，在晶体结构预测中实现了高效的目标导向生成，显著提升了采样效率和生成速度。

Abstract: Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.

</details>


### [147] [Towards Building Non-Fine-Tunable Foundation Models](https://arxiv.org/abs/2602.00446)
*Ziyao Wang,Nizhang Li,Pingzhi Li,Guoheng Sun,Tianlong Chen,Ang Li*

Main category: cs.LG

TL;DR: 提出Private Mask Pre-Training (PMP)框架，通过预训练时学习稀疏子网络并保密其掩码，防止未经授权的下游微调，同时保持基础模型性能。


<details>
  <summary>Details</summary>
Motivation: 开源基础模型虽然促进广泛重用，但也使模型训练者面临未经授权下游微调带来的经济和安全风险。需要构建既保持可用性又限制未经授权微调收益的模型。

Method: 提出Private Mask Pre-Training (PMP)预训练框架：在训练早期识别稀疏子网络，仅公开最终密集权重，保密二进制掩码。未经授权的微调因无法访问掩码而更新与预训练子空间不匹配的参数，导致微调目标与预训练几何结构不匹配。

Result: 理论分析显示这种不匹配会破坏基于梯度的适应性并限制微调收益。在大型语言模型上的实证结果表明，PMP保持基础模型性能，同时在不同下游任务中持续降低未经授权微调的效果，非微调可调性强度可通过掩码比例控制。

Conclusion: PMP框架有效解决了开源基础模型面临的安全风险，通过控制掩码比例实现不同程度的保护，为模型发布者提供了保护知识产权和防止滥用的实用方法。

Abstract: Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.

</details>


### [148] [Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA](https://arxiv.org/abs/2602.00451)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: TAD-LoRA：一种拓扑感知的去中心化低秩适应框架，解决了去中心化联邦学习中LoRA参数高效微调的问题，通过协调LoRA因子的更新和混合来控制客户端间错位，在不同通信拓扑下实现鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）作为无服务器的联邦学习变体，在参数高效微调方面面临独特挑战。与线性参数不同，LoRA更新的去中心化聚合引入了拓扑依赖的交叉项，这些项在动态通信图下可能破坏训练稳定性。

Method: 提出TAD-LoRA框架，协调LoRA因子的更新和混合以控制客户端间错位。该框架考虑了拓扑诱导的交叉项误差和由交替训练切换间隔控制的块坐标表示偏差之间的权衡。

Result: 理论证明了TAD-LoRA在非凸目标下的收敛性，并在各种通信条件下验证了分析。实验显示TAD-LoRA在不同通信场景下实现鲁棒性能：在强连接拓扑中保持竞争力，在中度和弱连接拓扑中提供明显增益，在MNLI数据集上表现尤为突出。

Conclusion: TAD-LoRA有效解决了去中心化联邦学习中LoRA参数高效微调的挑战，通过拓扑感知的协调机制在不同通信条件下实现稳定训练和良好性能，为动态通信环境下的参数高效微调提供了实用解决方案。

Abstract: Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.

</details>


### [149] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

TL;DR: FedMOA：基于GRPO的联邦多目标对齐框架，通过自适应权重调整和任务感知聚合，在异构奖励下提升推理能力，同时降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统RL对齐在联邦学习中内存消耗大，GRPO的无critic架构适合设备端训练，但联邦环境下存在异构奖励定义、多目标优化不平衡和高训练成本等挑战。

Method: 提出FedMOA框架：1) 客户端使用基于超梯度下降的在线自适应权重机制，优先处理主要推理目标；2) 服务器端采用任务和准确率感知的聚合策略，优先选择高质量更新。

Result: 在数学推理和代码生成基准测试中，FedMOA始终优于联邦平均算法，准确率提升高达2.2%，同时改善了全局性能、个性化能力和多目标平衡。

Conclusion: FedMOA成功解决了联邦GRPO中的系统性挑战，实现了在异构奖励下的高效多目标对齐，为设备端联邦学习中的个性化推理能力提升提供了可行方案。

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [150] [Search Inspired Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.00460)
*Georgios Sotirchos,Zlatan Ajanović,Jens Kober*

Main category: cs.LG

TL;DR: SIERL提出了一种基于搜索启发的强化学习探索方法，通过设置子目标来主动引导探索，在稀疏奖励环境中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境中的探索是强化学习的核心挑战。现有方法如课程学习和Go-Explore依赖人工设计的启发式规则，而好奇心驱动的方法可能收敛到次优策略。需要一种能主动引导探索的方法。

Method: SIERL在每个episode开始时从边界（已知状态空间的边界）中选择一个子目标，然后代理继续向主要任务目标探索。子目标选择机制提供既不过于熟悉也不完全新颖的状态-动作对，确保边界被系统性地扩展。受搜索算法启发，子目标根据到达成本和目标成本的估计进行优先级排序，有效引导探索到信息最丰富的区域。

Result: 在具有挑战性的稀疏奖励环境实验中，SIERL在实现主要任务目标和泛化到环境中任意状态方面都优于主流基线方法。

Conclusion: SIERL通过搜索启发的子目标选择机制，提供了一种有效的探索策略，能够系统性地扩展已知状态空间边界，在稀疏奖励环境中表现出色。

Abstract: Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.

</details>


### [151] [PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction](https://arxiv.org/abs/2602.00465)
*Jiaqi Yin,Baiming Chen,Jia Fei,Mingjun Yang*

Main category: cs.LG

TL;DR: PAIR-Former：一种用于miRNA-mRNA靶向预测的预算感知多实例学习框架，通过廉价全池扫描和多样化实例选择，在计算预算内实现高效准确预测。


<details>
  <summary>Details</summary>
Motivation: miRNA-mRNA靶向预测是一个大规模预测问题：每个转录本产生大量候选靶位点，但只能观察到配对级标签。传统方法计算成本高，需要处理大量候选实例。

Method: 提出BR-MIL（预算关系多实例学习）框架，包含廉价全池扫描、CPU上选择最多K个多样化候选靶位点，以及使用置换不变的Set Transformer聚合器处理选定标记。

Result: 在miRAW数据集上，PAIR-Former在实用操作预算（K*=64）下优于强基线方法，同时提供可控的准确率-计算权衡。

Conclusion: PAIR-Former为大规模生物序列预测问题提供了有效的预算感知解决方案，理论分析表明选择预算K与近似误差和泛化性能相关。

Abstract: Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.

</details>


### [152] [Parallel Stochastic Gradient-Based Planning for World Models](https://arxiv.org/abs/2602.00475)
*Michael Psenka,Michael Rabbat,Aditi Krishnapriyan,Yann LeCun,Amir Bar*

Main category: cs.LG

TL;DR: GRASP是一种基于可微分世界模型的并行化规划器，通过虚拟状态优化和随机性注入解决视觉输入的长时域控制任务


<details>
  <summary>Details</summary>
Motivation: 世界模型可以从原始视觉输入模拟环境动态，但用于规划时面临搜索空间庞大且非结构化的挑战，需要更高效的规划方法

Method: 将状态视为优化变量（虚拟状态），加入软动态约束实现并行计算；引入状态随机性促进探索；修改梯度结构以缓解高维视觉世界模型的敏感梯度问题

Result: 在基于视频的世界模型实验中，GRASP在长时域任务上优于交叉熵方法（CEM）和普通梯度优化（GD），成功率和收敛时间都有提升

Conclusion: GRASP是一种有效的随机规划器，利用可微分世界模型的优势，通过虚拟状态优化和梯度结构调整实现了高效的视觉控制规划

Abstract: World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables ("virtual states") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.

</details>


### [153] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

TL;DR: CAL是一种无需训练的方法，通过利用扩散语言模型在首步去噪中的统计信号（Oracle Peak和Length Bias），实现自适应确定填充长度的文本/代码填充方法。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然天生适合填充任务，但性能受限于预设的填充长度。研究发现DLMs具有发现正确填充长度的内在能力，但需要有效的方法来利用这一能力。

Method: CAL方法通过分析首步去噪置信度的统计特性，识别Oracle Peak（接近真实长度的局部峰值）和Length Bias（系统性的长度偏差），通过校准偏差和高效搜索来近似最优填充长度。

Result: 在代码填充任务中，CAL比固定长度基线提升Pass@1达47.7%，比基于聊天的自适应方法提升40.5%；在文本填充任务中，BLEU-2和ROUGE-L分别提升8.5%和9.9%。

Conclusion: CAL证明了无需专门训练即可实现稳健的DLM填充，通过利用模型内在的统计信号来近似最优填充长度，为DLM填充任务提供了有效的训练免费解决方案。

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [154] [Quality-Diversity Optimization as Multi-Objective Optimization](https://arxiv.org/abs/2602.00478)
*Xi Lin,Ping Guo,Yilu Liu,Qingfu Zhang,Jianyong Sun*

Main category: cs.LG

TL;DR: 该论文将质量多样性优化重新表述为具有大量优化目标的多目标优化问题，使得可以直接应用成熟的MOO方法来解决QD问题。


<details>
  <summary>Details</summary>
Motivation: 质量多样性优化旨在发现既高性能又具有多样性的解决方案，在机器人控制、创意设计等领域有广泛应用。虽然已有多种QD算法，但作者希望建立QD与MOO之间的联系，从而利用成熟的MOO方法来解决QD问题。

Method: 将QD优化重新表述为具有大量优化目标的多目标优化问题，特别采用基于集合的标量化技术，通过协作搜索过程解决QD问题。

Result: 理论分析表明该方法继承了MOO的理论保证，同时为QD优化提供了理想特性。在多个QD应用中的实验研究表明，该方法达到了与最先进QD算法竞争的性能。

Conclusion: 通过将QD问题重新表述为MOO问题，可以直接应用成熟的MOO方法，为QD优化提供了新的解决途径，并在理论和实验上都取得了良好结果。

Abstract: The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.

</details>


### [155] [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)
*Jiarui Zhang,Yuchen Yang,Ran Yan,Zhiyu Mei,Liyuan Zhang,Daifeng Li,Wei Fu,Jiaxuan Gao,Shusheng Xu,Yi Wu,Binhang Yuan*

Main category: cs.LG

TL;DR: AREAL-DTA 提出了一种基于深度优先搜索的动态树注意力机制，通过共享前缀树结构来优化RL后训练的计算效率，实现了高达8.31倍的训练吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的LLM后训练计算成本高昂，因为生成的rollout序列经常共享长token前缀。现有RL框架独立处理这些序列，在策略模型训练的前向和后向传播中重复计算相同前缀，导致计算和内存使用效率低下。

Method: AREAL-DTA采用基于深度优先搜索的执行策略，在前后向计算中动态遍历rollout前缀树，每次只具体化单个根到叶路径。同时引入负载均衡的分布式批处理机制，在多个GPU上动态构建和处理前缀树。

Result: 在流行的RL后训练工作负载中，AREAL-DTA在τ²-bench上实现了高达8.31倍的训练吞吐量提升。

Conclusion: AREAL-DTA通过高效的prefix sharing机制显著提升了RL后训练的计算效率，解决了现有树注意力方法在RL场景中扩展性差的问题。

Abstract: Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\times$ in $τ^2$-bench higher training throughput.

</details>


### [156] [OD-DEAL: Dynamic Expert-Guided Adversarial Learning with Online Decomposition for Scalable Capacitated Vehicle Routing](https://arxiv.org/abs/2602.00488)
*Dongbin Jiao,Zisheng Chen,Xianyi Wang,Jintao Shi,Shengcai Liu,Shi Yan*

Main category: cs.LG

TL;DR: OD-DEAL：一个对抗学习框架，通过混合遗传搜索和在线重心聚类分解，结合知识蒸馏，实现大规模CVRP问题的实时高质量求解。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在大规模CVRP问题上计算复杂度过高，而现有神经求解器在大规模图上的泛化能力有限，需要一种既能保证求解质量又能实现实时推理的新方法。

Method: 提出OD-DEAL对抗学习框架，集成混合遗传搜索和在线重心聚类分解，通过知识蒸馏将专家启发式行为转移到图注意力网络生成策略中，使用极小极大博弈训练，将分治策略蒸馏为密集代理奖励。

Result: OD-DEAL实现了最先进的实时CVRP性能，能够求解10000节点实例，具有接近恒定的神经缩放特性，支持亚秒级、启发式质量的推理。

Conclusion: 该方法成功解决了大规模CVRP问题的实时求解挑战，为动态大规模部署提供了可行的解决方案，实现了神经求解器在大规模图上的高效泛化。

Abstract: Solving large-scale capacitated vehicle routing problems (CVRP) is hindered by the high complexity of heuristics and the limited generalization of neural solvers on massive graphs. We propose OD-DEAL, an adversarial learning framework that tightly integrates hybrid genetic search (HGS) and online barycenter clustering (BCC) decomposition, and leverages high-fidelity knowledge distillation to transfer expert heuristic behavior. OD-DEAL trains a graph attention network (GAT)-based generative policy through a minimax game, in which divide-and-conquer strategies from a hybrid expert are distilled into dense surrogate rewards. This enables high-quality, clustering-free inference on large-scale instances. Empirical results demonstrate that OD-DEAL achieves state-of-the-art (SOTA) real-time CVRP performance, solving 10000-node instances with near-constant neural scaling. This uniquely enables the sub-second, heuristic-quality inference required for dynamic large-scale deployment.

</details>


### [157] [Partition of Unity Neural Networks for Interpretable Classification with Explicit Class Regions](https://arxiv.org/abs/2602.00511)
*Akram Aldroubi*

Main category: cs.LG

TL;DR: 提出PUNN架构，通过学习的单位分解直接生成类别概率，无需softmax层，提高神经网络分类器的可解释性


<details>
  <summary>Details</summary>
Motivation: 神经网络分类器难以解释，softmax模型中的类别区域通过logits的不等式系统隐式定义，难以提取和可视化

Method: 引入PUNN架构，学习k个非负函数h₁,...,hₖ满足∑hᵢ(x)=1，每个hᵢ(x)直接表示P(class i|x)。使用多种激活函数和参数化方法构建门函数gᵢ

Result: PUNN在合成数据、UCI基准和MNIST上，基于MLP的门函数达到与标准多层感知器相差0.3-0.6%的准确率。当几何先验匹配数据结构时，形状感知门函数使用少300倍的参数达到可比准确率

Conclusion: PUNN证明可解释性设计架构可以与黑盒模型竞争，同时提供透明的类别概率分配，为可解释神经网络提供新方向

Abstract: Despite their empirical success, neural network classifiers remain difficult to interpret. In softmax-based models, class regions are defined implicitly as solutions to systems of inequalities among logits, making them difficult to extract and visualize. We introduce Partition of Unity Neural Networks (PUNN), an architecture in which class probabilities arise directly from a learned partition of unity, without requiring a softmax layer.
  PUNN constructs $k$ nonnegative functions $h_1, \ldots, h_k$ satisfying $\sum_i h_i(x) = 1$, where each $h_i(x)$ directly represents $P(\text{class } i \mid x)$. Unlike softmax, where class regions are defined implicitly through coupled inequalities among logits, each PUNN partition function $h_i$ directly defines the probability of class $i$ as a standalone function of $x$.
  We prove that PUNN is dense in the space of continuous probability maps on compact domains. The gate functions $g_i$ that define the partition can use various activation functions (sigmoid, Gaussian, bump) and parameterizations ranging from flexible MLPs to parameter-efficient shape-informed designs (spherical shells, ellipsoids, spherical harmonics).
  Experiments on synthetic data, UCI benchmarks, and MNIST show that PUNN with MLP-based gates achieves accuracy within 0.3--0.6\% of standard multilayer perceptrons. When geometric priors match the data structure, shape-informed gates achieve comparable accuracy with up to 300$\times$ fewer parameters. These results demonstrate that interpretable-by-design architectures can be competitive with black-box models while providing transparent class probability assignments.

</details>


### [158] [Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs](https://arxiv.org/abs/2602.00513)
*Md Tanvirul Alam,Aritran Piplai,Ionut Cardei,Nidhi Rastogi,Peter J Worth*

Main category: cs.LG

TL;DR: 提出Minerva框架，使用可验证奖励的强化学习（RLVR）提升网络威胁情报结构化提取任务，通过任务特定验证器和自训练机制解决奖励稀疏问题，相比监督微调在多个基准测试中表现更优。


<details>
  <summary>Details</summary>
Motivation: 网络威胁情报分析师需要将嘈杂的非结构化安全数据转换为标准化的自动化就绪表示。现有基于大语言模型的方法在生成结构化输出时仍显脆弱，主要依赖监督微调，而CTI标准和社区资源定义了可确定性验证的规范标识符和模式。

Method: 提出Minerva统一数据集和训练管道，涵盖多个CTI子任务，每个任务配有特定验证器来评分结构化输出和标识符预测。针对奖励稀疏问题，引入轻量级自训练机制生成额外已验证轨迹并蒸馏回模型。

Result: 实验表明，在不同大语言模型骨干上，相比监督微调，该方法在多个基准测试中均显示出准确性和鲁棒性的持续改进。

Conclusion: 利用CTI标准中定义的可验证结构，通过强化学习与可验证奖励结合的方法，能够有效提升网络威胁情报结构化提取任务的性能，为CTI自动化处理提供了更可靠的解决方案。

Abstract: Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.

</details>


### [159] [Contrastive Learning for Privacy Enhancements in Industrial Internet of Things](https://arxiv.org/abs/2602.00515)
*Lin Liu,Rita Machacy,Simi Kuniyilh*

Main category: cs.LG

TL;DR: 本文对工业物联网(IIoT)中基于对比学习的隐私保护技术进行了全面综述，重点分析了工业数据特性、系统架构和应用场景，并讨论了现有解决方案、开放挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 工业物联网(IIoT)虽然为工业控制系统带来了预测性维护和跨站点优化等优势，但也因操作数据的敏感性而引入了显著的隐私和机密性风险。对比学习作为一种自监督表示学习范式，通过减少对标记数据和原始数据共享的依赖，成为隐私保护分析的有前景方法。

Method: 本文采用文献综述的方法，系统性地回顾了基于对比学习的隐私保护技术在工业物联网领域的应用。特别关注工业数据的独特特征、系统架构以及各种应用场景，并对现有解决方案进行分类和分析。

Result: 论文提供了工业物联网中基于对比学习的隐私保护技术的全面概述，识别了该领域的关键技术方法，分析了不同应用场景下的适用性，并指出了当前技术面临的局限性和挑战。

Conclusion: 基于对比学习的隐私保护技术在工业物联网中具有重要应用价值，但仍面临诸多开放挑战。未来研究需要针对工业环境的特殊需求，开发更有效的隐私保护解决方案，并探索新的研究方向以应对不断发展的工业物联网安全需求。

Abstract: The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.

</details>


### [160] [NEST: Nested Event Stream Transformer for Sequences of Multisets](https://arxiv.org/abs/2602.00520)
*Minghui Sun,Haoyu Gong,Xingyu You,Jillian Hurst,Benjamin Goldstein,Matthew Engelhard*

Main category: cs.LG

TL;DR: NEST模型：针对具有层次结构的事件流数据（如医疗记录）的Transformer模型，通过保留原始层次结构提高计算效率和表示质量


<details>
  <summary>Details</summary>
Motivation: 事件流数据常具有层次结构（如医疗事件分组为临床就诊序列），现有基础模型将层次结构扁平化为一维序列，导致计算效率低下、学习虚假的集合内关系，以及下游任务中集合级表示质量较低

Method: 提出NEST（Nested Event Stream Transformer）模型，保留原始层次结构；引入Masked Set Modeling（MSM）预训练范式，促进集合级表示学习

Result: 实验表明NEST能捕捉真实世界动态，同时提高预训练效率和下游任务性能

Conclusion: 在事件流基础模型中保留原始层次结构提供了有用的归纳偏置，能同时改善计算效率和表示质量

Abstract: Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.

</details>


### [161] [Physiology as Language: Translating Respiration to Sleep EEG](https://arxiv.org/abs/2602.00526)
*Kaiwen Zha,Chao Li,Hao He,Peng Cao,Tianhong Li,Ali Mirzazadeh,Ellen Zhang,Jong Woo Lee,Yoon Kim,Dina Katabi*

Main category: cs.LG

TL;DR: 提出一种从呼吸信号合成睡眠脑电图的跨生理学翻译框架，通过波形条件生成和离散标记化技术，在28,000人数据上训练，实现7%的MAE，并能支持年龄估计、性别检测和睡眠分期等下游任务。


<details>
  <summary>Details</summary>
Motivation: 解决从呼吸信号合成睡眠脑电图这一跨生理学翻译任务的挑战，利用呼吸信号的非侵入性特点，实现远程、非接触的神经评估，特别是在睡眠监测中的应用。

Method: 提出波形条件生成框架，通过离散标记化技术约束脑电图目标空间，同时保留呼吸信号的细粒度动态特征。该方法在超过28,000人的数据上进行训练。

Result: 在脑电图频谱图重建上达到7%的平均绝对误差。合成脑电图在下游任务中表现接近真实脑电图：年龄估计（MAE 5.0 vs 5.1年）、性别检测（AUROC 0.81 vs 0.82）、睡眠分期（准确率0.84 vs 0.88），显著优于直接在呼吸信号上训练的基线方法。

Conclusion: 该框架成功实现了从呼吸信号到脑电图的跨生理学翻译，并扩展到无线射频反射的无接触传感，证明了在睡眠期间进行远程、非接触神经评估的可行性。

Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.

</details>


### [162] [Convergent World Representations and Divergent Tasks](https://arxiv.org/abs/2602.00533)
*Core Francisco Park*

Main category: cs.LG

TL;DR: 多任务训练能产生收敛的世界表示，但某些"发散性任务"会损害新实体的表示整合和泛化能力


<details>
  <summary>Details</summary>
Motivation: 研究神经表示的几何特性及其在下游适应能力中的作用，目前对这些表示的条件和角色理解不足

Method: 建立分离世界、数据生成过程和模型表示的框架，使用5,075个城市坐标作为世界，7个几何任务生成自回归训练数据，进行多任务训练和微调实验

Result: 不同任务产生不同的世界表示几何，多任务训练使表示收敛对齐；但某些发散性任务会损害新实体的表示整合和泛化能力

Conclusion: 多任务关系训练能可靠产生收敛的世界表示，但隐藏的发散性任务会通过微调灾难性地损害新实体的表示整合

Abstract: While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.

</details>


### [163] [Invertible Memory Flow Networks](https://arxiv.org/abs/2602.00535)
*Liyu Zerihun,Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: IMFN通过二叉树分解将长序列压缩问题转化为简单的2对1压缩任务，实现对数深度和次线性误差积累，并通过蒸馏实现恒定成本的在线推理。


<details>
  <summary>Details</summary>
Motivation: 长序列神经记忆仍然是一个挑战性问题。RNN存在梯度消失问题，Transformer存在二次方缩放问题。将长序列压缩为有限固定表示由于优化困难而难以解决。

Method: 提出可逆记忆流网络（IMFN），通过因子分解将长序列压缩问题分解为使用二叉树"清扫器"模块的成对合并。每个清扫器学习更简单的2对1压缩任务，实现O(log N)深度和次线性误差积累。为在线推理，将其蒸馏为恒定成本的循环学生网络，实现O(1)顺序步骤。

Result: 在长MNIST序列和UCF-101视频上的实证结果验证了IMFN在高维数据长序列压缩方面的有效性。

Conclusion: IMFN通过将复杂的端到端压缩问题分解为简单的成对合并任务，使长序列压缩变得可行，实现了对数深度和次线性误差积累，并通过蒸馏支持高效在线推理。

Abstract: Long sequence neural memory remains a challenging problem. RNNs and their variants suffer from vanishing gradients, and Transformers suffer from quadratic scaling. Furthermore, compressing long sequences into a finite fixed representation remains an intractable problem due to the difficult optimization landscape. Invertible Memory Flow Networks (IMFN) make long sequence compression tractable through factorization: instead of learning end-to-end compression, we decompose the problem into pairwise merges using a binary tree of "sweeper" modules. Rather than learning to compress long sequences, each sweeper learns a much simpler 2-to-1 compression task, achieving O(log N) depth with sublinear error accumulation in sequence length. For online inference, we distilled into a constant-cost recurrent student achieving O(1) sequential steps. Empirical results validate IMFN on long MNIST sequences and UCF-101 videos, demonstrating compression of high-dimensional data over long sequences.

</details>


### [164] [OpenDDI: A Comprehensive Benchmark for DDI Prediction](https://arxiv.org/abs/2602.00539)
*Xinmo Jin,Bowen Fan,Xunkai Li,Henan Sun,YuXin Zeng,Zekai Chen,Yuxuan Sun,Jia Li,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OpenDDI是一个全面的药物相互作用预测基准，统一了多个数据集和评估标准，提供了标准化框架和10个关键见解。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用预测面临两大挑战：1）缺乏高质量数据（小规模数据集和单模态表示）；2）缺乏标准化评估（不一致的场景、指标和基线）。这些问题阻碍了该领域的进一步发展。

Method: 提出OpenDDI基准：1）数据层面：统一6个常用DDI数据集和2种现有药物表示，新增3个大规模LLM增强数据集和覆盖5种模态的多模态药物表示；2）评估层面：统一20个SOTA模型基线，涵盖3个下游任务，建立数据质量、有效性、泛化性、鲁棒性和效率的标准化协议。

Result: 基于OpenDDI进行了全面评估，得出了10个有价值的DDI预测见解，同时揭示了当前局限性，为该快速发展领域提供了关键指导。

Conclusion: OpenDDI为DDI预测提供了一个全面的基准框架，解决了数据质量和评估标准化问题，为该领域的未来发展提供了重要参考和指导。

Abstract: Drug-Drug Interactions (DDIs) significantly influence therapeutic efficacy and patient safety. As experimental discovery is resource-intensive and time-consuming, efficient computational methodologies have become essential. The predominant paradigm formulates DDI prediction as a drug graph-based link prediction task. However, further progress is hindered by two fundamental challenges: (1) lack of high-quality data: most studies rely on small-scale DDI datasets and single-modal drug representations; (2) lack of standardized evaluation: inconsistent scenarios, varied metrics, and diverse baselines. To address the above issues, we propose OpenDDI, a comprehensive benchmark for DDI prediction. Specifically, (1) from the data perspective, OpenDDI unifies 6 widely used DDI datasets and 2 existing forms of drug representation, while additionally contributing 3 new large-scale LLM-augmented datasets and a new multimodal drug representation covering 5 modalities. (2) From the evaluation perspective, OpenDDI unifies 20 SOTA model baselines across 3 downstream tasks, with standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. Based on OpenDDI, we conduct a comprehensive evaluation and derive 10 valuable insights for DDI prediction while exposing current limitations to provide critical guidance for this rapidly evolving field. Our code is available at https://github.com/xiaoriwuguang/OpenDDI

</details>


### [165] [One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models](https://arxiv.org/abs/2602.00541)
*Zilin Jing,Vincent Jeanselme,Yuta Kobayashi,Simon A. Lee,Chao Pang,Aparajita Kashyap,Yanwei Li,Xinzhuo Jiang,Shalmali Joshi*

Main category: cs.LG

TL;DR: ORA：一种新的电子健康记录基础模型预训练目标，联合建模事件时间和相关测量值，相比传统下一个令牌预测方法能产生更具泛化性的表示


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的临床事件具有不规则采样特性，包含离散事件和数值测量值的混合。传统基于下一个令牌预测的预训练方法无法捕捉EHR的完整结构，限制了模型的泛化能力和下游任务表现。

Method: 提出ORA（标记时间到事件）预训练目标，联合建模事件时间和相关连续测量值。该方法考虑了EHR的结构特性，包括事件的时间间隔和数值测量信息。

Result: 在多个数据集、下游任务和模型架构上，ORA目标相比下一个令牌预测和忽略连续测量的预训练损失，能产生更具泛化性的表示。改进不仅体现在传统分类评估，还包括回归和时间到事件预测任务。

Conclusion: ORA引入了一个新的基础模型家族，更重要的是表明：考虑EHR结构的预训练目标对于扩展下游能力和提高泛化性至关重要。这为EHR基础模型的发展提供了新的方向。

Abstract: Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability

</details>


### [166] [Depth, Not Data: An Analysis of Hessian Spectral Bifurcation](https://arxiv.org/abs/2602.00545)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Yaoqing Yang*

Main category: cs.LG

TL;DR: 论文挑战了传统观点，证明深度神经网络Hessian矩阵的"bulk-and-spike"谱结构主要源于网络架构而非数据不平衡，即使在数据协方差完全平衡时也会出现。


<details>
  <summary>Details</summary>
Motivation: 传统研究将深度神经网络Hessian矩阵的"bulk-and-spike"谱结构归因于数据协方差矩阵的不平衡。本文质疑这一观点，旨在证明这种谱分岔可能纯粹由网络架构引起，与数据分布无关。

Method: 采用深度线性网络设置，在数据协方差完全平衡的条件下，分析Hessian矩阵的特征值分布。通过理论证明，即使数据平衡，Hessian仍会呈现分岔特征值结构：一个主导簇和一个主体簇。

Result: 研究发现主导特征值与主体特征值之间的比值随网络深度线性增长，表明谱间隙主要受网络架构影响而非数据分布。这种谱分岔现象在数据平衡条件下依然存在。

Conclusion: 深度神经网络的优化景观不仅受数据特性影响，更受网络架构的强烈影响。设计深度网络优化算法时，应同时考虑模型架构和数据特征两方面因素。

Abstract: The eigenvalue distribution of the Hessian matrix plays a crucial role in understanding the optimization landscape of deep neural networks. Prior work has attributed the well-documented ``bulk-and-spike'' spectral structure, where a few dominant eigenvalues are separated from a bulk of smaller ones, to the imbalance in the data covariance matrix. In this work, we challenge this view by demonstrating that such spectral Bifurcation can arise purely from the network architecture, independent of data imbalance.
  Specifically, we analyze a deep linear network setup and prove that, even when the data covariance is perfectly balanced, the Hessian still exhibits a Bifurcation eigenvalue structure: a dominant cluster and a bulk cluster. Crucially, we establish that the ratio between dominant and bulk eigenvalues scales linearly with the network depth. This reveals that the spectral gap is strongly affected by the network architecture rather than solely by data distribution. Our results suggest that both model architecture and data characteristics should be considered when designing optimization algorithms for deep networks.

</details>


### [167] [Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry](https://arxiv.org/abs/2602.00547)
*Seunghyun Yoo,Sanghong Kim,Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 提出跨模态对齐框架，将质谱直接映射到预训练化学语言模型的分子结构嵌入空间，解决质谱分子识别的泛化瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法将质谱匹配视为封闭集识别任务，难以泛化到未见过的分子骨架结构，存在语义鸿沟问题

Method: 跨模态对齐框架，将质谱数据直接映射到预训练化学语言模型的分子结构嵌入空间，整合物理谱峰分辨率与分子结构嵌入

Result: 在严格骨架不相交基准上，Top-1准确率达42.2%（256路零样本检索）；全局检索设置下表现强劲；学习到的嵌入空间化学一致性达95.4%（5路5样本分子重识别）

Conclusion: 明确整合物理谱峰分辨率与分子结构嵌入是解决质谱数据分子识别泛化瓶颈的关键

Abstract: Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.

</details>


### [168] [Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design](https://arxiv.org/abs/2602.00549)
*Kezhao Lai,Yutao Lai,Hai-Lin Liu*

Main category: cs.LG

TL;DR: Clade-AHD框架用clade级贝叶斯信念替代节点级点估计，通过Thompson采样建模不确定性，解决MCTS在LLM自动启发式设计中过度开发的问题


<details>
  <summary>Details</summary>
Motivation: MCTS在基于LLM的自动启发式设计中存在过度开发倾向，特别是在计算预算有限的情况下，这影响了启发式评估的可靠性

Method: 提出Clade-AHD框架，将后代评估聚合为Beta分布，通过Thompson采样在这些信念上进行探索，显式建模不确定性以指导探索

Result: 在复杂组合优化问题上的大量实验表明，Clade-AHD始终优于最先进方法，同时显著降低计算成本

Conclusion: Clade-AHD通过clade级贝叶斯信念和Thompson采样有效解决了MCTS在自动启发式设计中的过度开发问题，实现了更可靠的决策

Abstract: While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.

</details>


### [169] [Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural Networks](https://arxiv.org/abs/2602.00567)
*Tian Zhang,Yujia Tong,Junhao Dong,Ke Xu,Yuze Wang,Jingling Yuan*

Main category: cs.LG

TL;DR: 提出OEU框架，通过熵引导遗忘和梯度正交投影解决量化模型中的机器遗忘问题，实现真正的遗忘而非错误记忆，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上部署量化神经网络与GDPR等隐私法规结合，迫切需要量化模型的机器遗忘能力。现有方法存在关键挑战：通过训练模型记忆错误标签来诱导遗忘，混淆了遗忘与错误记忆；使用标量梯度重加权无法解决梯度间的方向冲突。

Method: 提出正交熵遗忘(OEU)框架，包含两个关键创新：1) 熵引导遗忘：最大化遗忘数据的预测不确定性，实现真正的遗忘而非自信的错误预测；2) 梯度正交投影：通过将遗忘梯度投影到保留梯度的正交补空间来消除干扰，在一阶近似下为效用保持提供理论保证。

Result: 大量实验表明，OEU在遗忘效果和保留准确率方面均优于现有方法。

Conclusion: OEU框架有效解决了量化模型中的机器遗忘问题，通过熵最大化实现真正的遗忘，通过梯度正交投影保持模型性能，为边缘设备上的隐私合规提供了实用解决方案。

Abstract: The deployment of quantized neural networks on edge devices, combined with privacy regulations like GDPR, creates an urgent need for machine unlearning in quantized models. However, existing methods face critical challenges: they induce forgetting by training models to memorize incorrect labels, conflating forgetting with misremembering, and employ scalar gradient reweighting that cannot resolve directional conflicts between gradients. We propose OEU, a novel Orthogonal Entropy Unlearning framework with two key innovations: 1) Entropy-guided unlearning maximizes prediction uncertainty on forgotten data, achieving genuine forgetting rather than confident misprediction, and 2) Gradient orthogonal projection eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients, providing theoretical guarantees for utility preservation under first-order approximation. Extensive experiments demonstrate that OEU outperforms existing methods in both forgetting effectiveness and retain accuracy.

</details>


### [170] [When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning](https://arxiv.org/abs/2602.00573)
*Zheng Zhang,Tao Hu,Xueheng Li,Yang Wang,Rui Li,Jie Zhang,Chengjun Xie*

Main category: cs.LG

TL;DR: 该论文提出了阶段感知类增量学习（Stage-CIL）范式，解决传统CIL忽略类内形态演化的问题，并提出了STAGE方法和Stage-Bench数据集，在同时处理类间区分和类内形态适应方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统类增量学习（CIL）方法隐含假设类别形态是静态的，主要关注在新类别引入时保留已学知识。然而，这种假设忽略了类内演化现象——同一语义类别的实例会经历显著的形态转变（如幼虫变成蝴蝶）。因此，模型需要既能区分不同类别，又能适应同一类别内的外观演化。

Method: 论文提出了STAGE方法，该方法在固定大小的记忆池中显式学习抽象且可迁移的演化模式。通过将语义身份与转换动态解耦，STAGE能够基于早期表示准确预测未来的形态。同时，论文还引入了Stage-Bench数据集和协议，包含10个领域、2个阶段，用于联合评估类间和类内遗忘。

Result: 广泛的实证评估表明，STAGE方法在同时处理类间区分和类内形态适应方面，始终且显著优于现有的最先进方法。

Conclusion: 该研究形式化了阶段感知类增量学习（Stage-CIL）范式，解决了传统CIL忽略类内演化的问题。提出的STAGE方法通过显式学习演化模式，在保持语义身份的同时适应形态变化，为处理动态类内演化的增量学习提供了有效解决方案。

Abstract: Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.

</details>


### [171] [Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs](https://arxiv.org/abs/2602.00576)
*Tushaar Gangavarapu,Jiping Li,Christopher Vattheuer,Zhangyang Wang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 研究发现，通过调整训练数据分布（如上采样后期学习的样本）可以减少优化器的简单性偏好，从而提升大语言模型的泛化性能，在数学推理任务上实现高达18%的相对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过修改训练数据分布来引导优化器找到具有更好泛化性能的解决方案，特别是针对训练大型语言模型时，SAM优化器虽然泛化性能好但计算成本过高的问题。

Method: 理论分析多头部线性自注意力机制下的上下文线性回归模型，比较GD和SAM的训练动态；提出通过上采样或增强后期学习样本来调整训练数据分布，减少简单性偏好；在多个LLM上进行实验验证。

Result: SAM优化器能降低简单性偏好，这是其改善泛化的关键因素；通过调整训练数据分布同样能减少简单性偏好并提升泛化性能；在多个LLM上的实验显示，数学推理任务的相对准确率提升最高达18%。

Conclusion: 通过修改训练数据分布来减少优化器的简单性偏好，可以有效提升大语言模型的泛化性能，为训练LLM提供了一种计算效率高的替代方案。

Abstract: Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.

</details>


### [172] [Sparsity-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2602.00577)
*Yuze Wang,Yujia Tong,Ke Xu,Jingling Yuan,Jiawei Jiang,Chuang Hu*

Main category: cs.LG

TL;DR: 针对稀疏化大语言模型的遗忘学习新方法SAU，通过梯度掩码和重要性重分配解决现有遗忘方法在稀疏模型上效果下降的问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练过程中会记忆敏感信息，带来隐私风险。现有遗忘学习方法主要针对密集模型设计，忽视了模型稀疏化这一高效部署LLM的关键技术，导致在稀疏模型上遗忘效果显著下降

Method: 提出Sparsity-Aware Unlearning (SAU)方法：1) 通过梯度掩码将更新重定向到存活的权重，将遗忘目标与稀疏化目标解耦；2) 结合重要性感知重分配来补偿被修剪的参数

Result: 大量实验表明，SAU在稀疏LLMs上显著优于现有方法，能够实现有效遗忘同时保持模型实用性

Conclusion: SAU解决了稀疏化LLMs中遗忘学习的关键挑战，通过专门设计的梯度掩码和重要性重分配机制，在保持模型效率的同时有效保护隐私

Abstract: Large Language Models (LLMs) inevitably memorize sensitive information during training, posing significant privacy risks. Machine unlearning has emerged as a promising solution to selectively remove such information without full retraining. However, existing methods are designed for dense models and overlook model sparsification-an essential technique for efficient LLM deployment. We find that unlearning effectiveness degrades substantially on sparse models. Through empirical analysis, we reveal that this degradation occurs because existing unlearning methods require updating all parameters, yet sparsification prunes substantial weights to zero, fundamentally limiting the model's forgetting capacity. To address this challenge, we propose Sparsity-Aware Unlearning (SAU), which decouples unlearning from sparsification objectives through gradient masking that redirects updates to surviving weights, combined with importance-aware redistribution to compensate for pruned parameters. Extensive experiments demonstrate that SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.

</details>


### [173] [Bridging Time and Frequency: A Joint Modeling Framework for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00582)
*Xiangfei Qiu,Kangjia Yan,Xvyuan Liu,Xingjian Wu,Jilin Hu*

Main category: cs.LG

TL;DR: TFMixer是一个用于不规则多元时间序列预测的联合时频建模框架，通过可学习的非均匀离散傅里叶变换提取频谱表示，并结合基于查询的补丁混合机制进行局部时间建模，实现了最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 不规则多元时间序列预测面临非均匀采样和变量异步性的挑战，这些不规则性违反了标准模型的等距假设，阻碍了局部时间建模，并使经典频域方法无法有效捕捉全局周期结构。

Method: TFMixer包含全局频率模块（使用可学习的非均匀离散傅里叶变换直接从不规则时间戳中提取频谱表示）和局部时间模块（引入基于查询的补丁混合机制自适应聚合信息性时间补丁，缓解信息密度不平衡），最后融合时域和频域表示进行预测，并利用逆NUDFT进行显式季节外推。

Result: 在真实世界数据集上的广泛实验表明，TFMixer实现了最先进的性能。

Conclusion: TFMixer通过联合时频建模有效解决了不规则多元时间序列预测的挑战，为处理非均匀采样和异步变量提供了创新解决方案。

Abstract: Irregular multivariate time series forecasting (IMTSF) is challenging due to non-uniform sampling and variable asynchronicity. These irregularities violate the equidistant assumptions of standard models, hindering local temporal modeling and rendering classical frequency-domain methods ineffective for capturing global periodic structures. To address this challenge, we propose TFMixer, a joint time-frequency modeling framework for IMTS forecasting. Specifically, TFMixer incorporates a Global Frequency Module that employs a learnable Non-Uniform Discrete Fourier Transform (NUDFT) to directly extract spectral representations from irregular timestamps. In parallel, the Local Time Module introduces a query-based patch mixing mechanism to adaptively aggregate informative temporal patches and alleviate information density imbalance. Finally, TFMixer fuses the time-domain and frequency-domain representations to generate forecasts and further leverages inverse NUDFT for explicit seasonal extrapolation. Extensive experiments on real-world datasets demonstrate the state--of-the-art performance of TFMixer.

</details>


### [174] [Safe Langevin Soft Actor Critic](https://arxiv.org/abs/2602.00587)
*Mahesh Keswani,Samyak Jain,Raunak P. Bhattacharyya*

Main category: cs.LG

TL;DR: SL-SAC算法通过参数空间探索和分布风险控制解决约束强化学习中奖励与安全的平衡问题，在Safety-Gymnasium基准测试中显著降低任务成本。


<details>
  <summary>Details</summary>
Motivation: 约束强化学习中奖励与安全的平衡仍然具有挑战性，主要问题包括：从尖锐价值最小值泛化能力差，以及对重尾风险分布处理不足。

Method: 结合三个关键机制：1) 使用自适应随机梯度朗之万动力学(aSGLD)进行奖励批评器探索；2) 通过隐式分位数网络(IQN)进行分布成本估计，结合条件风险价值(CVaR)优化缓解尾部风险；3) 基于经验CVaR的自适应拉格朗日松弛方案。

Result: 在Safety-Gymnasium基准测试中，SL-SAC在10个任务中的7个实现了最低成本，同时保持有竞争力的回报，在速度任务中成本降低了19-63%。

Conclusion: SL-SAC通过参数空间探索和分布风险控制有效解决了约束强化学习中的奖励-安全平衡问题，CVaR为基础的拉格朗日更新比期望成本更新提供更强的约束违反信号。

Abstract: Balancing reward and safety in constrained reinforcement learning remains challenging due to poor generalization from sharp value minima and inadequate handling of heavy-tailed risk distribution. We introduce Safe Langevin Soft Actor-Critic (SL-SAC), a principled algorithm that addresses both issues through parameter-space exploration and distributional risk control. Our approach combines three key mechanisms: (1) Adaptive Stochastic Gradient Langevin Dynamics (aSGLD) for reward critics, promoting ensemble diversity and escape from poor optima; (2) distributional cost estimation via Implicit Quantile Networks (IQN) with Conditional Value-at-Risk (CVaR) optimization for tail-risk mitigation; and (3) a reactive Lagrangian relaxation scheme that adapts constraint enforcement based on the empirical CVaR of episodic costs. We provide theoretical guarantees on CVaR estimation error and demonstrate that CVaR-based Lagrange updates yield stronger constraint violation signals than expected-cost updates. On Safety-Gymnasium benchmarks, SL-SAC achieves the lowest cost in 7 out of 10 tasks while maintaining competitive returns, with cost reductions of 19-63% in velocity tasks compared to state-of-the-art baselines.

</details>


### [175] [SEER: Transformer-based Robust Time Series Forecasting via Automated Patch Enhancement and Replacement](https://arxiv.org/abs/2602.00589)
*Xiangfei Qiu,Xvyuan Liu,Tianen Shen,Xingjian Wu,Hanyin Cheng,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: SEER是一个鲁棒的时间序列预测框架，通过可学习的补丁替换模块动态过滤低质量补丁，提升模型在数据质量问题下的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于补丁的时间序列预测方法通常使用所有补丁进行预测，无法动态选择补丁。实际时间序列数据常存在缺失值、分布偏移、异常值和白噪声等质量问题，导致某些补丁包含低质量信息，影响预测结果。

Method: 1. 提出增强嵌入模块：使用混合专家架构改进补丁表示，通过通道自适应感知机制获得序列级令牌表示。2. 引入可学习补丁替换模块：采用两阶段过程增强预测鲁棒性：a) 动态过滤机制消除负面补丁令牌；b) 替换注意力模块将低质量补丁替换为全局序列级令牌，并通过因果注意力机制进一步优化表示。

Result: 综合实验结果表明SEER达到了最先进的性能表现。

Conclusion: SEER框架通过动态过滤和替换低质量补丁，有效解决了时间序列数据质量问题对预测的影响，提升了模型的鲁棒性和准确性。

Abstract: Time series forecasting is important in many fields that require accurate predictions for decision-making. Patching techniques, commonly used and effective in time series modeling, help capture temporal dependencies by dividing the data into patches. However, existing patch-based methods fail to dynamically select patches and typically use all patches during the prediction process. In real-world time series, there are often low-quality issues during data collection, such as missing values, distribution shifts, anomalies and white noise, which may cause some patches to contain low-quality information, negatively impacting the prediction results. To address this issue, this study proposes a robust time series forecasting framework called SEER. Firstly, we propose an Augmented Embedding Module, which improves patch-wise representations using a Mixture-of-Experts (MoE) architecture and obtains series-wise token representations through a channel-adaptive perception mechanism. Secondly, we introduce a Learnable Patch Replacement Module, which enhances forecasting robustness and model accuracy through a two-stage process: 1) a dynamic filtering mechanism eliminates negative patch-wise tokens; 2) a replaced attention module substitutes the identified low-quality patches with global series-wise token, further refining their representations through a causal attention mechanism. Comprehensive experimental results demonstrate the SOTA performance of SEER.

</details>


### [176] [Kernelized Edge Attention: Addressing Semantic Attention Blurring in Temporal Graph Neural Networks](https://arxiv.org/abs/2602.00596)
*Govind Waghmare,Srini Rohan Gujulla Leel,Nikhil Tumbde,Sumedh B G,Sonia Gupta,Srikanta Bedathur*

Main category: cs.LG

TL;DR: KEAT提出了一种新的注意力机制，通过连续时间核函数调制边特征，解决传统TGNN中节点和边表示纠缠导致的语义注意力模糊问题，显著提升了动态图链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有TGNN模型在计算注意力时通常将节点和边表示纠缠在一起，但节点嵌入演化缓慢（聚合长期结构上下文），而边特征反映瞬时的带时间戳交互。这种不匹配导致语义注意力模糊，注意力权重无法区分缓慢漂移的节点状态和快速变化的信息丰富的边交互，限制了模型捕捉细粒度时间依赖和提供时间相关性计算透明度。

Method: KEAT（Kernelized Edge Attention for Temporal Graphs）是一种新颖的注意力公式，使用一系列连续时间核函数（包括拉普拉斯核、RBF核和可学习的MLP变体）来调制边特征。该方法保持节点和边的不同角色，并能无缝集成到Transformer风格（如DyGFormer）和消息传递（如TGN）架构中。

Result: 在链接预测任务上，KEAT相比最近的DyGFormer实现了高达18%的MRR改进，相比TGN实现了7%的改进，实现了更准确、可解释和具有时间感知能力的消息传递。

Conclusion: KEAT通过解耦节点和边的时间行为，使用核函数调制边特征，有效解决了TGNN中的语义注意力模糊问题，显著提升了动态图建模的性能和可解释性。

Abstract: Temporal Graph Neural Networks (TGNNs) aim to capture the evolving structure and timing of interactions in dynamic graphs. Although many models incorporate time through encodings or architectural design, they often compute attention over entangled node and edge representations, failing to reflect their distinct temporal behaviors. Node embeddings evolve slowly as they aggregate long-term structural context, while edge features reflect transient, timestamped interactions (e.g. messages, trades, or transactions). This mismatch results in semantic attention blurring, where attention weights cannot distinguish between slowly drifting node states and rapidly changing, information-rich edge interactions. As a result, models struggle to capture fine-grained temporal dependencies and provide limited transparency into how temporal relevance is computed. This paper introduces KEAT (Kernelized Edge Attention for Temporal Graphs), a novel attention formulation that modulates edge features using a family of continuous-time kernels, including Laplacian, RBF, and learnable MLP variant. KEAT preserves the distinct roles of nodes and edges, and integrates seamlessly with both Transformer-style (e.g., DyGFormer) and message-passing (e.g., TGN) architectures. It achieves up to 18% MRR improvement over the recent DyGFormer and 7% over TGN on link prediction tasks, enabling more accurate, interpretable and temporally aware message passing in TGNNs.

</details>


### [177] [Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains](https://arxiv.org/abs/2602.00603)
*Luca Viano,Ruida Zhou,Yifan Sun,Mahdi Namazifar,Volkan Cevher,Shoham Sabach,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: 本文提出了一种改进的偏好优化算法，利用评分差距信息来提升DPO算法的性能，在保持鲁棒性的同时获得更快的统计收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统DPO算法仅使用成对偏好反馈，这种反馈形式虽然数据收集容易，但存在模糊性（无法区分偏好的程度）。评分差距信息（显示被选响应比被拒响应好多少）可以提供更丰富的信号，但现有算法未能充分利用这种信息。

Method: 设计了能够利用评分差距信息的新算法，这些算法在获得准确评分差距时能达到比DPO更快的统计收敛速度。同时，算法对评分差距的不准确性具有鲁棒性，即使信息不精确也能保持良好性能。

Result: 理论证明和实验验证都表明，新算法在多种LLM和评估基准上表现优于多种DPO风格算法。算法对评分差距的不准确性具有鲁棒性，在信息不精确时仍能保持良好性能。

Conclusion: 利用评分差距信息可以显著提升偏好优化算法的性能，新算法在保持鲁棒性的同时实现了更快的收敛速度，为对齐问题提供了更有效的解决方案。

Abstract: The class of direct preference optimization (DPO) algorithms has emerged as a promising approach for solving the alignment problem in foundation models. These algorithms work with very limited feedback in the form of pairwise preferences and fine-tune models to align with these preferences without explicitly learning a reward model. While the form of feedback used by these algorithms makes the data collection process easy and relatively more accurate, its ambiguity in terms of the quality of responses could have negative implications. For example, it is not clear if a decrease (increase) in the likelihood of preferred (dispreferred) responses during the execution of these algorithms could be interpreted as a positive or negative phenomenon. In this paper, we study how to design algorithms that can leverage additional information in the form of rating gap, which informs the learner how much the chosen response is better than the rejected one. We present new algorithms that can achieve faster statistical rates than DPO in presence of accurate rating gap information. Moreover, we theoretically prove and empirically show that the performance of our algorithms is robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance of our methods in comparison to a number of DPO-style algorithms across a wide range of LLMs and evaluation benchmarks.

</details>


### [178] [Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games](https://arxiv.org/abs/2602.00606)
*Ahmed Said Donmez,Yuksel Arslantas,Muhammed O. Sayin*

Main category: cs.LG

TL;DR: 提出了一种新颖的独立、基于收益的学习框架，用于随机博弈，该框架无需模型、与游戏无关且无需梯度。采用最佳响应型演员-评论家架构，通过快速和慢速评论家更新策略，在零和与共同利益随机博弈中收敛到近似均衡。


<details>
  <summary>Details</summary>
Motivation: 现有的随机博弈学习算法通常需要模型信息、梯度计算或中心化协调。本文旨在开发一种完全去中心化、基于收益且无需模型或梯度的学习框架，适用于多种博弈类型。

Method: 提出基于收益的学习框架，采用最佳响应型演员-评论家架构。每个智能体使用两个评论家：快速评论家基于有限信息对观察到的收益做出直观响应；慢速评论家深思熟虑地近似底层动态规划问题的解。通过平滑最佳响应进行非均衡适应。

Result: 理论证明了在无限时域下，该方法能在两智能体零和博弈和多智能体共同利益随机博弈中收敛到（近似）均衡。实验验证了该方法在这两类博弈中的鲁棒性和有效性。

Conclusion: 这是首批在零和与共同利益随机博弈中都具有理论保证的完全去中心化、基于收益的学习算法之一，为随机博弈中的独立学习提供了新的解决方案。

Abstract: We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.

</details>


### [179] [Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference](https://arxiv.org/abs/2602.00620)
*Juntao Fang,Shifeng Xie,Shengbin Nie,Yuhui Ling,Yuming Liu,Zijian Li,Keli Zhang,Lujia Pan,Themis Palpanas,Ruichu Cai*

Main category: cs.LG

TL;DR: 提出了TIC-FM框架，通过上下文学习实现时间序列基础模型的真正零样本分类，无需参数更新，避免了传统评估中分类器训练带来的偏差。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列基础模型的零样本评估方法违反了"训练免费"的前提，因为通常使用冻结编码器加任务特定分类器，这引入了分类器依赖的训练选择带来的评估偏差。

Method: 提出TIC-FM框架：结合时间序列编码器和轻量级投影适配器，使用分割掩码潜在记忆Transformer，将标记训练集作为上下文，在单次前向传播中预测所有测试实例的标签。

Result: 在128个UCR数据集上表现出强大的准确性，在极低标签情况下获得一致增益，验证了训练免费迁移的有效性。

Conclusion: 上下文学习可以替代训练分类器，在单次前向传播中模拟基于梯度的分类器训练，实现了真正零样本的时间序列分类评估。

Abstract: The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer

</details>


### [180] [MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting](https://arxiv.org/abs/2602.00624)
*Hyekyung Yoon,Minhyuk Lee,Imseung Park,Myungjoo Kang*

Main category: cs.LG

TL;DR: MoDEx：一种轻量级深度专家混合模型，通过分析多层感知机各层对时间序列的敏感性，用深度特定专家替代复杂主干网络，在长期时间序列预测中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有长期时间序列预测（LTSF）采用嵌入、主干网络精炼和长期预测的三阶段流程，但各主干层的行为尚未得到充分探索。需要量化每个时间点对各层潜在特征的正负贡献，以理解深度特定时间动态建模。

Method: 提出层敏感性指标（受GradCAM和有效感受野理论启发），量化每个时间点对各层潜在特征的正负贡献。基于对三层MLP主干的分析，提出MoDEx（深度特定专家混合模型），用深度特定MLP专家替代复杂主干网络。

Result: 在7个真实世界基准测试中达到最先进准确率，在78%的情况下排名第一，同时使用显著更少的参数和计算资源。能无缝集成到Transformer变体中，持续提升其性能，展示了作为高效高性能LTSF框架的强泛化能力。

Conclusion: MoDEx通过深度特定专家设计，在长期时间序列预测中实现了参数效率和高性能的平衡，展示了理解主干网络各层行为的重要性，为LTSF提供了高效且通用的解决方案。

Abstract: Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon prediction. However, the behaviors of individual backbone layers remain underexplored. We introduce layer sensitivity, a gradient-based metric inspired by GradCAM and effective receptive field theory, which quantifies both positive and negative contributions of each time point to a layer's latent features. Applying this metric to a three-layer MLP backbone reveals depth-specific specialization in modeling temporal dynamics in the input sequence. Motivated by these insights, we propose MoDEx, a lightweight Mixture of Depth-specific Experts, which replaces complex backbones with depth-specific MLP experts. MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78 percent of cases, while using significantly fewer parameters and computational resources. It also integrates seamlessly into transformer variants, consistently boosting their performance and demonstrating robust generalizability as an efficient and high-performance LTSF framework.

</details>


### [181] [From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs](https://arxiv.org/abs/2602.00628)
*Louis Schiekiera,Max Zimmer,Christophe Roux,Sebastian Pokutta,Fritz Günther*

Main category: cs.LG

TL;DR: 通过心理语言学实验的行为数据可以部分恢复LLM隐藏状态几何结构，其中强制选择任务比自由联想任务更能反映内部语义几何


<details>
  <summary>Details</summary>
Motivation: 研究LLM在心理语言学实验中的行为数据是否能够揭示其隐藏状态的几何结构，探索行为测量能否反映内部认知状态

Method: 在8个指令调优的transformer模型上运行两种实验范式（相似性强制选择和自由联想），使用5000词共享词汇表收集1750万+试验构建行为相似性矩阵，通过表征相似性分析比较行为几何与分层隐藏状态相似性，并与FastText、BERT和跨模型共识进行基准测试

Result: 强制选择行为与隐藏状态几何的一致性显著高于自由联想；在保留词回归中，行为相似性（特别是强制选择）能够预测未见隐藏状态相似性，超越词汇基线和跨模型共识，表明仅行为测量保留了可恢复的内部语义几何信息

Conclusion: 行为任务能够揭示LLM的隐藏认知状态，特别是强制选择范式在恢复内部语义几何方面更为有效，为通过行为实验理解模型内部表示提供了实证支持

Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.

</details>


### [182] [Combinatorial Bandit Bayesian Optimization for Tensor Outputs](https://arxiv.org/abs/2602.00640)
*Jingru Huang,Haijie Xu,Jie Guo,Manrui Jiang,Chen Zhang*

Main category: cs.LG

TL;DR: 提出两种张量输出贝叶斯优化方法：TOGP用于完整张量输出优化，以及CBBO用于部分输出选择的组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法未处理张量输出函数，且存在更复杂的组合优化场景（仅部分输出贡献于目标函数），需要新的方法填补这一空白。

Method: 1. 提出张量输出高斯过程（TOGP）作为代理模型，包含两类张量输出核函数捕捉结构依赖；2. 基于TOGP设计UCB采集函数；3. 针对组合优化问题，扩展TOGP处理部分观测输出，并设计CMAB-UCB2准则同时选择查询点和最优输出子集。

Result: 为两种方法建立了理论遗憾界，确保次线性性能；大量合成和真实世界实验证明了方法的优越性。

Conclusion: 成功填补了张量输出贝叶斯优化的空白，提出的TOGP和CBBO方法能有效处理张量输出函数和组合优化问题，具有理论保证和实际应用价值。

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and black-box functions across various domains. Existing BO methods have not addressed tensor-output functions. To fill this gap, we propose a novel tensor-output BO method. Specifically, we first introduce a tensor-output Gaussian process (TOGP) with two classes of tensor-output kernels as a surrogate model of the tensor-output function, which can effectively capture the structural dependencies within the tensor. Based on it, we develop an upper confidence bound (UCB) acquisition function to select the queried points. Furthermore, we introduce a more complex and practical problem setting, named combinatorial bandit Bayesian optimization (CBBO), where only a subset of the outputs can be selected to contribute to the objective function. To tackle this, we propose a tensor-output CBBO method, which extends TOGP to handle partially observed outputs, and accordingly design a novel combinatorial multi-arm bandit-UCB2 (CMAB-UCB2) criterion to sequentially select both the queried points and the optimal output subset. Theoretical regret bounds for the two methods are established, ensuring their sublinear performance. Extensive synthetic and real-world experiments demonstrate their superiority.

</details>


### [183] [CoRe-Fed: Bridging Collaborative and Representation Fairness via Federated Embedding Distillation](https://arxiv.org/abs/2602.00647)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: CoRe-Fed是一个联邦学习框架，通过嵌入对齐和公平感知聚合来解决表示偏差和协作偏差，提升公平性和模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在数据分布异构和参与不平等导致的性能差异问题，特别是表示偏差（客户端表示不匹配）和协作偏差（聚合贡献不平等），这些偏差会降低模型性能和泛化能力。

Method: 提出CoRe-Fed统一优化框架：1）嵌入对齐机制促进本地和全局嵌入的语义一致性以减少表示偏差；2）基于动态奖励-惩罚的聚合策略，根据参与历史和嵌入对齐调整客户端权重，实现贡献感知的聚合。

Result: 在多种模型和数据集上的广泛实验表明，CoRe-Fed在公平性和模型性能方面均优于现有最先进的基线算法。

Conclusion: CoRe-Fed通过桥接协作公平和表示公平，有效缓解了联邦学习中的偏差问题，为异构分布式环境下的公平协作学习提供了有效解决方案。

Abstract: With the proliferation of distributed data sources, Federated Learning (FL) has emerged as a key approach to enable collaborative intelligence through decentralized model training while preserving data privacy. However, conventional FL algorithms often suffer from performance disparities across clients caused by heterogeneous data distributions and unequal participation, which leads to unfair outcomes. Specifically, we focus on two core fairness challenges, i.e., representation bias, arising from misaligned client representations, and collaborative bias, stemming from inequitable contribution during aggregation, both of which degrade model performance and generalizability. To mitigate these disparities, we propose CoRe-Fed, a unified optimization framework that bridges collaborative and representation fairness via embedding-level regularization and fairness-aware aggregation. Initially, an alignment-driven mechanism promotes semantic consistency between local and global embeddings to reduce representational divergence. Subsequently, a dynamic reward-penalty-based aggregation strategy adjusts each client's weight based on participation history and embedding alignment to ensure contribution-aware aggregation. Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over the state-of-the-art baseline algorithms.

</details>


### [184] [PHAT: Modeling Period Heterogeneity for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00654)
*Jiaming Ma,Guanjun Wang,Qihe Huang,Sheng Huang,Haofeng Ma,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: PHAT提出了一种周期异质性感知的Transformer模型，通过三维周期桶结构和正负注意力机制，有效处理多变量时间序列中不同变量具有不同动态周期的异质性问题。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测模型虽然能很好建模周期性，但忽略了现实数据中常见的周期异质性——不同变量具有不同且动态变化的周期。这种异质性导致传统方法在处理多变量数据时效果受限。

Method: PHAT将多变量输入组织成三维"周期桶"张量：维度分别对应具有相似周期性的变量组特征、按相位对齐的时间步、周期内的偏移。通过限制桶内交互和屏蔽跨桶连接来避免不一致周期的干扰。提出正负注意力机制，从周期对齐和周期偏差两个角度捕获周期性依赖，并将对齐注意力分数分解为正负分量，通过编码周期先验的调制项约束注意力机制。

Result: 在14个真实世界数据集上对18个基线方法进行全面评估，结果显示PHAT显著优于现有方法，实现了极具竞争力的预测性能。

Conclusion: PHAT通过周期异质性感知的架构设计，有效解决了多变量时间序列中周期异质性问题，为处理现实世界复杂周期性数据提供了新的解决方案。

Abstract: While existing multivariate time series forecasting models have advanced significantly in modeling periodicity, they largely neglect the periodic heterogeneity common in real-world data, where variates exhibit distinct and dynamically changing periods. To effectively capture this periodic heterogeneity, we propose PHAT (Period Heterogeneity-Aware Transformer). Specifically, PHAT arranges multivariate inputs into a three-dimensional "periodic bucket" tensor, where the dimensions correspond to variate group characteristics with similar periodicity, time steps aligned by phase, and offsets within the period. By restricting interactions within buckets and masking cross-bucket connections, PHAT effectively avoids interference from inconsistent periods. We also propose a positive-negative attention mechanism, which captures periodic dependencies from two perspectives: periodic alignment and periodic deviation. Additionally, the periodic alignment attention scores are decomposed into positive and negative components, with a modulation term encoding periodic priors. This modulation constrains the attention mechanism to more faithfully reflect the underlying periodic trends. A mathematical explanation is provided to support this property. We evaluate PHAT comprehensively on 14 real-world datasets against 18 baselines, and the results show that it significantly outperforms existing methods, achieving highly competitive forecasting performance. Our sources is available at GitHub.

</details>


### [185] [Riemannian Flow Matching for Disentangled Graph Domain Adaptation](https://arxiv.org/abs/2602.00656)
*Yingxu Wang,Xinwang Liu,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: DisRFM是一个几何感知的图域自适应框架，通过黎曼流形嵌入和基于流的传输来解决结构退化和优化不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统图域自适应方法使用对抗学习在欧几里得空间中对齐图嵌入，但面临两个关键挑战：1）结构退化 - 层次和语义表示纠缠在一起；2）优化不稳定性 - 最小最大对抗训练的振荡动态。

Method: 1）将图嵌入黎曼流形，使用极坐标显式解耦结构（半径）和语义（角度）；2）通过径向Wasserstein对齐保持拓扑结构，通过角度聚类实现语义区分；3）使用黎曼流匹配学习平滑向量场，沿测地线路径将源特征引导到目标域，确保稳定收敛。

Result: 理论证明了流匹配的渐近稳定性，并推导了更紧的目标风险界限。大量实验表明DisRFM始终优于最先进的方法。

Conclusion: DisRFM通过几何感知方法有效解决了图域自适应中的结构退化和优化不稳定性问题，实现了更好的性能。

Abstract: Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.

</details>


### [186] [Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD](https://arxiv.org/abs/2602.00672)
*Aleksandr Yugay,Hang Cui,Changhua Pei,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 线性自回归异常检测方法在性能上匹配或超越复杂深度学习模型，计算成本极低，建议未来研究应包含强线性基线并开发更丰富的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测研究过度关注复杂、难以训练且推理昂贵的神经网络架构，需要重新审视这一范式，探索更简单有效的替代方案。

Method: 提出基于普通最小二乘回归的线性自回归异常评分方法，通过闭式解估计有限历史高斯过程的条件密度，计算简单高效。

Result: 在广泛的单变量和多变量基准测试中，该方法在准确性上优于或匹配最先进的深度检测器，同时计算资源需求降低数个数量级。

Conclusion: 未来研究应始终包含强线性基线，并开发具有更丰富时间结构的基准测试，以真正体现深度学习模型的优势。

Abstract: Research in time series anomaly detection (TSAD) has largely focused on developing increasingly sophisticated, hard-to-train, and expensive-to-infer neural architectures. We revisit this paradigm and show that a simple linear autoregressive anomaly score with the closed-form solution provided by ordinary least squares (OLS) regression consistently matches or outperforms state-of-the-art deep detectors. From a theoretical perspective, we show that linear models capture a broad class of anomaly types, estimating a finite-history Gaussian process conditional density. From a practical side, across extensive univariate and multivariate benchmarks, the proposed approach achieves superior accuracy while requiring orders of magnitude fewer computational resources. Thus, future research should consistently include strong linear baselines and, more importantly, develop new benchmarks with richer temporal structures pinpointing the advantages of deep learning models.

</details>


### [187] [Provably Protecting Fine-Tuned LLMs from Training Data Extraction](https://arxiv.org/abs/2602.00688)
*Tom Segal,Asaf Shabtai,Yuval Elovici*

Main category: cs.LG

TL;DR: 提出SCP-Δr算法，通过仅保留关键token的概率偏移并平滑低影响token，在保护隐私的同时最小化性能损失


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在敏感数据上微调存在隐私风险，现有防御方法要么缺乏形式化隐私保证，要么导致严重的性能下降

Method: 基于近访问自由(NAF)的SCP-Δr算法，操作相对概率，使用基础模型显式平滑低影响token，仅保留关键token的偏移

Result: 相比现有NAF方法获得数量级更好的理论边界，在有效防御训练数据提取攻击的同时保持最小性能损失

Conclusion: 通过选择性保留关键token偏移并平滑其他token，SCP-Δr在隐私保护和模型性能之间取得了良好平衡

Abstract: Fine-tuning large language models (LLMs) on sensitive datasets raises privacy concerns, as training data extraction (TDE) attacks can expose highly confidential information. Existing defenses against such attacks either lack formal privacy guarantees or incur substantial utility degradation. We observe that fine-tuning induces widespread probability shifts, yet preserving only a small subset of influential token-level deviations is sufficient; the remaining shifts can be aggressively smoothed with minimal impact on utility. Motivated by this insight, we propose SCP-$Δ_r$, a Near Access Freeness (NAF)-based algorithm that operates on relative probabilities and explicitly smooths low-impact tokens using a base model. SCP-$Δ_r$ achieves orders-of-magnitude better theoretical bounds than existing NAF based methods and provides strong empirical protection against TDE attacks with minimal performance loss.

</details>


### [188] [Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities](https://arxiv.org/abs/2602.00693)
*Marco Nurisso,Pierrick Leroy,Giovanni Petri,Francesco Vaccarino*

Main category: cs.LG

TL;DR: 本文研究了前馈ReLU网络参数空间的拓扑性质，重点关注基于有向无环图架构的网络的连通性和奇异性问题。


<details>
  <summary>Details</summary>
Motivation: 理解前馈ReLU网络参数空间的性质对于分析和指导训练动态至关重要。梯度流训练后，参数空间被限制在由ReLU激活函数的齐次性产生的代数簇上。

Method: 研究基于有向无环图架构的前馈ReLU网络，分析参数空间的连通性和奇异性。扩展先前结果，通过瓶颈节点和平衡条件来表征连通性，并探讨奇异性与底层DAG拓扑结构的关系。

Result: 发现奇异性与底层DAG及其诱导子网络的拓扑结构密切相关。建立了这些奇异点的可达性与可微剪枝之间的原则性联系，并通过简单数值实验验证了理论。

Conclusion: 前馈ReLU网络的参数空间性质（连通性和奇异性）与网络架构的拓扑结构有深刻联系，这为理解训练动态和网络剪枝提供了理论基础。

Abstract: Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.

</details>


### [189] [Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning](https://arxiv.org/abs/2602.00694)
*Fabio Turazza,Marcello Pietri,Natalia Selini Hadjidimitriou,Marco Mamei*

Main category: cs.LG

TL;DR: 本文探讨了联邦学习（FL）与LSTM网络在本地能源社区中的应用，旨在解决隐私保护与能源预测准确性之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 本地能源社区在实现自给自足时面临能源生产与消费平衡管理的挑战，需要准确的预测模型。然而，用户隐私顾虑和法规限制阻碍了传统预测解决方案的应用，因为用户不愿分享其消费模式。

Method: 采用联邦学习（FL）框架结合长短期记忆（LSTM）网络，在保护用户隐私的前提下构建预测模型。FL允许在不共享原始敏感数据的情况下训练模型，各用户本地训练后仅共享模型参数。

Result: 研究表明联邦学习与LSTM网络能够有效应用于本地能源社区的能源预测，同时揭示了数据共享程度与预测准确性之间的权衡关系。

Conclusion: 联邦学习为解决本地能源社区中的隐私保护与预测准确性矛盾提供了可行方案，通过FL框架可以在保护用户隐私的同时实现有效的能源预测，支持社区的优化规划和自给自足目标。

Abstract: Local Energy Communities are emerging as crucial players in the landscape of sustainable development. A significant challenge for these communities is achieving self-sufficiency through effective management of the balance between energy production and consumption. To meet this challenge, it is essential to develop and implement forecasting models that deliver accurate predictions, which can then be utilized by optimization and planning algorithms. However, the application of forecasting solutions is often hindered by privacy constrains and regulations as the users participating in the Local Energy Community can be (rightfully) reluctant sharing their consumption patterns with others. In this context, the use of Federated Learning (FL) can be a viable solution as it allows to create a forecasting model without the need to share privacy sensitive information among the users. In this study, we demonstrate how FL and long short-term memory (LSTM) networks can be employed to achieve this objective, highlighting the trade-off between data sharing and forecasting accuracy.

</details>


### [190] [LocalV: Exploiting Information Locality for IP-level Verilog Generation](https://arxiv.org/abs/2602.00704)
*Hanqi Lyu,Di Huang,Yaoyu Zhu,Kangcheng Liu,Bohan Dou,Chongxiao Li,Pengwei Jin,Shuyao Cheng,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: LocalV是一个多智能体框架，通过利用模块化硬件设计中的信息局部性，将长文档到长代码生成问题分解为短文档、短代码任务，显著提升了RTL代码生成的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: RTL代码生成是数字硬件设计中的关键但劳动密集型步骤。现有LLM方法在处理工业级IP设计任务时面临三大挑战：处理冗长详细文档、生成长RTL代码时正确性下降、以及复杂的调试周期。

Method: 提出LocalV多智能体框架，采用分层文档划分、任务规划、局部化代码生成、接口一致性合并和AST引导的局部感知调试等方法，将长文档到长代码生成分解为短文档、短代码任务。

Result: 在IP级Verilog生成基准测试RealBench上，LocalV显著优于最先进的LLM和智能体方法，通过率达到45.0%，而现有最佳方法仅为21.6%。

Conclusion: LocalV通过利用硬件设计中的信息局部性，有效解决了工业级RTL代码生成的规模扩展问题，为自动化硬件设计提供了有前景的解决方案。

Abstract: The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.

</details>


### [191] [Deep Time-series Forecasting Needs Kernelized Moment Balancing](https://arxiv.org/abs/2602.00717)
*Licheng Pan,Hao Wang,Haocheng Yang,Yuqi Li,Qingsong Wen,Xiaoxi Li,Zhichao Chen,Haoxuan Li,Zhixuan Chu,Yuan Lu*

Main category: cs.LG

TL;DR: 提出KMB-DF方法，通过核化矩平衡实现深度时间序列预测中的充分分布对齐，相比现有方法能自适应选择信息量最大的平衡函数，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度时间序列预测方法将问题视为分布平衡问题，但现有目标函数仅匹配一个或两个预定义的平衡函数的一阶矩，无法满足Imbens准则要求的完全分布平衡，导致预测分布与真实分布对齐不充分。

Method: 提出核化矩平衡直接预测（KMB-DF），从再生核希尔伯特空间（RKHS）中自适应选择信息量最大的平衡函数来强制充分的分布平衡，推导出可处理且可微的目标函数，支持从经验样本高效估计并无缝集成到基于梯度的训练流程中。

Result: 在多个模型和数据集上的广泛实验表明，KMB-DF能持续提升预测精度，并达到最先进的性能水平。

Conclusion: KMB-DF通过核化矩平衡实现了深度时间序列预测中的充分分布对齐，相比现有方法在分布平衡方面更完整，从而显著提升了预测性能。

Abstract: Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.

</details>


### [192] [Federated Learning at the Forefront of Fairness: A Multifaceted Perspective](https://arxiv.org/abs/2602.00718)
*Noorain Mukhtiar,Adnan Mahmood,Yipeng Zhou,Jian Yang,Jing Teng,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 这篇综述论文系统梳理了联邦学习中的公平性问题，从多角度对现有公平感知方法进行分类，并探讨了平衡公平与性能的技术框架、评估指标以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的公平性问题日益重要，主要驱动因素包括异构客户端的约束条件以及在不同场景下平衡模型性能的需求。随着联邦学习应用的扩展，确保所有参与方都能获得公平的模型性能变得至关重要。

Method: 论文采用多角度分类方法，从模型性能导向和能力导向两个维度对现有公平感知方法进行系统梳理。同时提供了一个框架来分类和解决各种公平关切及相关技术问题，并深入分析了用于定量衡量公平性的评估指标。

Result: 论文建立了联邦学习公平性研究的全面分类体系，识别了现有方法在平衡公平与性能方面的有效性，并系统总结了当前使用的公平性评估指标。

Conclusion: 论文为联邦学习公平性研究奠定了坚实基础，指出了该领域的重要开放研究方向，并提出了推动未来进展的潜在解决方案，为研究人员提供了系统性的指导框架。

Abstract: Fairness in Federated Learning (FL) is emerging as a critical factor driven by heterogeneous clients' constraints and balanced model performance across various scenarios. In this survey, we delineate a comprehensive classification of the state-of-the-art fairness-aware approaches from a multifaceted perspective, i.e., model performance-oriented and capability-oriented. Moreover, we provide a framework to categorize and address various fairness concerns and associated technical aspects, examining their effectiveness in balancing equity and performance within FL frameworks. We further examine several significant evaluation metrics leveraged to measure fairness quantitatively. Finally, we explore exciting open research directions and propose prospective solutions that could drive future advancements in this important area, laying a solid foundation for researchers working toward fairness in FL.

</details>


### [193] [Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation](https://arxiv.org/abs/2602.00722)
*Hao Gu,Mao-Lin Luo,Zi-Hao Zhou,Han-Chen Zhang,Min-Ling Zhang,Tong Wei*

Main category: cs.LG

TL;DR: 本文提出一种参数高效的持续学习方法EBLoRA，通过解耦任务更新的幅度与方向结构，在受限Stiefel流形上优化，平衡奇异值谱以减轻前向和后向遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效持续学习方法主要关注避免与过去更新的干扰，而非探究何种任务特定更新能自然保留先前知识。从知识分解视角发现，低秩适应的奇异值谱高度不平衡，少数主导成分吸收大部分适应能量，既容易破坏先前知识，又易受后续任务干扰。

Method: 将任务更新的幅度与方向结构解耦，在受限Stiefel流形上构建约束优化问题，使用与标准深度学习优化器兼容的投影一阶方法求解，实现组件间的显式平衡。

Result: 该方法能同时减轻后向遗忘和前向遗忘，在持续学习基准测试中一致优于现有基线方法。

Conclusion: 通过显式平衡低秩适应的奇异值谱，EBLoRA方法在参数高效持续学习中实现了更好的知识保留，为理解任务更新如何自然保持先前知识提供了新视角。

Abstract: Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.

</details>


### [194] [Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity](https://arxiv.org/abs/2602.00723)
*Prakhar Ganesh,Reza Shokri,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 该论文提出了"提示多样性"框架来量化LLM评估中的一致性，发现现有幻觉评估过度关注正确性而忽视一致性，导致对幻觉危害的严重误解。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型幻觉评估主要关注正确性，而忽视了输出的一致性。这种局限性导致无法准确区分和解决幻觉带来的各种危害（如信任侵蚀和错误信息传播）。

Method: 引入"提示多样性"框架，通过量化LLM评估中的一致性来分析幻觉问题。该方法在Med-HALT等基准测试中应用，并研究一致性在幻觉检测和缓解中的作用。

Result: 研究发现：1）基准测试中存在显著的多重性（超过50%的不一致性）；2）现有检测技术检测的是输出一致性而非正确性；3）RAG等缓解技术虽然有益，但可能引入额外的不一致性。

Conclusion: 通过将提示多样性整合到幻觉评估中，提供了一个改进的危害评估框架，并揭示了当前检测和缓解策略的关键局限性，强调了一致性评估对于准确理解幻觉危害的重要性。

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

</details>


### [195] [Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization](https://arxiv.org/abs/2602.00737)
*Jatan Shrestha,Santeri Heiskanen,Kari Hepola,Severi Rissanen,Pekka Jääskeläinen,Joni Pajarinen*

Main category: cs.LG

TL;DR: 提出Pareto-Conditioned Diffusion (PCD)框架，将离线多目标优化转化为条件采样问题，通过直接条件化于期望权衡来避免显式代理模型，实现超越训练数据的泛化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多目标优化需要在竞争目标间权衡，离线场景下仅有静态数据集，主要挑战在于如何超越观测数据进行泛化。现有方法需要显式代理模型，存在局限性。

Method: PCD框架将离线MOO转化为条件采样问题，通过直接条件化于期望权衡避免显式代理模型。采用重加权策略聚焦高性能样本，使用参考方向机制引导采样到训练数据之外的新颖有前景区域。

Result: 在标准离线MOO基准测试中，PCD表现出高度竞争力，更重要的是在不同任务间展现出比现有离线MOO方法更好的一致性。

Conclusion: PCD为离线多目标优化提供了新颖有效的框架，通过条件采样方法实现了超越训练数据的泛化能力，在多样任务中表现出优越的鲁棒性。

Abstract: Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Conditioned Diffusion (PCD), a novel framework that formulates offline MOO as a conditional sampling problem. By conditioning directly on desired trade-offs, PCD avoids the need for explicit surrogate models. To effectively explore the Pareto front, PCD employs a reweighting strategy that focuses on high-performing samples and a reference-direction mechanism to guide sampling towards novel, promising regions beyond the training data. Experiments on standard offline MOO benchmarks show that PCD achieves highly competitive performance and, importantly, demonstrates greater consistency across diverse tasks than existing offline MOO approaches.

</details>


### [196] [GraphNNK -- Graph Classification and Interpretability](https://arxiv.org/abs/2602.00753)
*Zeljko Bolevic,Milos Brajovic,Isidora Stankovic,Ljubisa Stankovic*

Main category: cs.LG

TL;DR: GNNs依赖参数化分类器限制了可解释性和泛化能力，基于插值的方法（如NNK）通过训练样本的凸组合进行预测，提供了更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: GNNs已成为处理图结构数据的标准方法，但其依赖参数化分类器（通常是线性softmax层）限制了模型的可解释性，有时也阻碍了泛化能力。需要更可解释的预测方法。

Method: 采用基于插值的方法，特别是非负核回归（NNK），将预测表示为嵌入空间中相似训练样本的凸组合，从而提供理论保证和可解释的预测。

Result: NNK方法能够将预测表示为训练样本的凸组合，不仅提供了理论结果，还生成了可解释的解释，改善了GNNs的可解释性问题。

Conclusion: 基于插值的非负核回归方法为GNNs提供了比传统参数化分类器更好的可解释性框架，同时保持了预测性能，是改善图神经网络可解释性的有前景方向。

Abstract: Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.

</details>


### [197] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 通过识别并约束控制模型不良行为的内部特征，在微调过程中防止"涌现性错位"，在六个微调领域实现了高达95%的不良行为减少，且不影响模型质量和目标任务性能。


<details>
  <summary>Details</summary>
Motivation: 当语言模型在狭窄范围的监督目标上进行微调时，会出现"涌现性错位"问题：模型学会了目标行为，但也发展出不良的域外行为。需要一种机制性方法来防止这种错位。

Method: 识别控制错位行为的少量内部特征，然后在微调过程中阻止模型加强这些特征。使用分离的选择/评估分割、多个独立评判者、随机种子、质量指标和广泛的消融实验来验证有效性。

Result: 在六个微调领域中，约束固定特征集实现了高达95%的相对错位减少，且没有降低模型质量或目标任务性能。但在长时间微调下错位会重新出现，表明存在通过替代特征或层的重新路由。

Conclusion: 针对内部机制的定向训练时约束可以有效减轻涌现性错位，同时保持目标任务性能。这为预防微调过程中的不良行为提供了一种机制性方法。

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [198] [Provable Model Provenance Set for Large Language Models](https://arxiv.org/abs/2602.00772)
*Xiaoqi Qiu,Hao Zeng,Zhiyu Hou,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出MPS方法，通过顺序测试和排除过程构建满足可证明保证的小型模型来源集合，解决模型来源分析中的可靠性问题


<details>
  <summary>Details</summary>
Motivation: 未经授权的模型使用和错误归属日益普遍，需要可靠的模型来源分析。现有方法依赖启发式指纹匹配规则，缺乏可证明的错误控制，且常忽略多来源情况，导致来源声明的可靠性无法验证

Method: 提出模型来源集合(MPS)方法，采用顺序测试和排除过程，在候选池中测试来源存在的显著性，自适应地构建满足保证的小型集合，建立用户特定置信水平下的可证明渐近保证

Result: 大量实验表明MPS能有效实现目标来源覆盖，同时严格限制无关模型的包含，并展示其在归因和审计任务中实际来源分析的潜力

Conclusion: MPS为模型来源问题提供了具有可证明保证的形式化解决方案，通过统计测试方法克服了现有启发式方法的局限性，为实际应用中的模型归因和审计提供了可靠工具

Abstract: The growing prevalence of unauthorized model usage and misattribution has increased the need for reliable model provenance analysis. However, existing methods largely rely on heuristic fingerprint-matching rules that lack provable error control and often overlook the existence of multiple sources, leaving the reliability of their provenance claims unverified. In this work, we first formalize the model provenance problem with provable guarantees, requiring rigorous coverage of all true provenances at a prescribed confidence level. Then, we propose the Model Provenance Set (MPS), which employs a sequential test-and-exclusion procedure to adaptively construct a small set satisfying the guarantee. The key idea of MPS is to test the significance of provenance existence within a candidate pool, thereby establishing a provable asymptotic guarantee at a user-specific confidence level. Extensive experiments demonstrate that MPS effectively achieves target provenance coverage while strictly limiting the inclusion of unrelated models, and further reveal its potential for practical provenance analysis in attribution and auditing tasks.

</details>


### [199] [A novel VAE-DML fusion framework for casual analysis of greenwashing in the mining industry](https://arxiv.org/abs/2602.00774)
*Yuxin Lu,Zhen Peng,Xiqiang Xia,Jie Wang*

Main category: cs.LG

TL;DR: 股权平衡能显著抑制矿业产业链企业的绿色漂洗行为，这种抑制作用存在区域、产业链位置和行业敏感性异质性，并具有随时间递减的治理效应，主要通过缓解管理层业绩压力、增强高管团队稳定性和加强媒体监督三种机制实现。


<details>
  <summary>Details</summary>
Motivation: 在全球绿色转型和"双碳"目标背景下，矿业产业链企业作为资源消耗和环境影响的重点实体，其环境绩效直接影响区域生态安全和国家资源战略。确保企业环境信息披露的真实可靠是可持续发展与国家战略目标的核心紧迫问题。从公司治理角度，研究股权平衡这一基础治理机制对绿色漂洗行为的抑制作用及其路径。

Method: 创新性地采用变分自编码器(VAE)和双重机器学习(DML)模型构建反事实场景，缓解内生性问题，精确识别股权平衡与绿色漂洗之间的因果关系。

Result: 1. 股权平衡与企业绿色漂洗存在显著负向因果关系，证实了其治理效应；2. 抑制作用存在异质性：在西部地区、产业链上游环节和环境敏感性高的行业表现更强；3. 治理效应具有时间动态性：当期效应最强，随后递减但仍显著，最终形成稳定的长期累积影响；4. 机制分析显示股权平衡通过缓解管理层业绩压力、增强高管团队稳定性和加强媒体监督三种渠道抑制绿色漂洗。

Conclusion: 股权平衡作为公司治理的基础机制，能有效抑制矿业产业链企业的绿色漂洗行为，其作用具有异质性和动态性，主要通过缓解管理层压力、稳定高管团队和强化外部监督三种路径实现治理效果，为促进企业真实环境信息披露提供了重要的治理视角。

Abstract: Against the backdrop of the global green transition and "dual carbon" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.

</details>


### [200] [Stable Time Series Prediction of Enterprise Carbon Emissions Based on Causal Inference](https://arxiv.org/abs/2602.00775)
*Zitao Hong,Zhen Peng,Xueping Liu*

Main category: cs.LG

TL;DR: 提出一种结合因果推断与稳定学习的碳排预测机制，通过提取因果稳定特征和动态校正时间非平稳性，提升模型在分布偏移环境下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为实现碳达峰碳中和目标，需要准确预测企业碳排放趋势。但由于地区、行业和企业间存在显著异质性，导致碳排放数据在时空维度上呈现分布偏移和非平稳性，传统预测模型准确性受限。

Method: 整合因果推断视角与稳定学习方法，构建风险一致性约束的稳定学习框架。提取对碳排放具有长期稳定影响的因果稳定特征，并通过自适应归一化和样本重加权策略动态校正时间非平稳性。

Result: 提出的稳定时间预测机制能够有效处理分布偏移环境，增强模型在复杂环境中的泛化能力和可解释性，为生产规划和碳配额交易决策提供更可靠的指导。

Conclusion: 该研究为解决碳排放预测中的分布偏移问题提供了有效方法，通过因果稳定特征提取和时间非平稳性校正，显著提升了预测模型的鲁棒性和实用性。

Abstract: Against the backdrop of ongoing carbon peaking and carbon neutrality goals, accurate prediction of enterprise carbon emission trends constitutes an essential foundation for energy structure optimization and low-carbon transformation decision-making. Nevertheless, significant heterogeneity persists across regions, industries and individual enterprises regarding energy structure, production scale, policy intensity and governance efficacy, resulting in pronounced distribution shifts and non-stationarity in carbon emission data across both temporal and spatial dimensions. Such cross-regional and cross-enterprise data drift not only compromises the accuracy of carbon emission reporting but substantially undermines the guidance value of predictive models for production planning and carbon quota trading decisions. To address this critical challenge, we integrate causal inference perspectives with stable learning methodologies and time-series modelling, proposing a stable temporal prediction mechanism tailored to distribution shift environments. This mechanism incorporates enterprise-level energy inputs, capital investment, labour deployment, carbon pricing, governmental interventions and policy implementation intensity, constructing a risk consistency-constrained stable learning framework that extracts causal stable features (robust against external perturbations yet demonstrating long-term stable effects on carbon dioxide emissions) from multi-environment samples across diverse policies, regions and industrial sectors. Furthermore, through adaptive normalization and sample reweighting strategies, the approach dynamically rectifies temporal non-stationarity induced by economic fluctuations and policy transitions, ultimately enhancing model generalization capability and explainability in complex environments.

</details>


### [201] [Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors](https://arxiv.org/abs/2602.00788)
*Md Abir Hossen,Mohammad Ali Javidian,Vignesh Narayanan,Jason M. O'Kane,Pooyan Jamshidi*

Main category: cs.LG

TL;DR: RESCUE是一种多保真度贝叶斯优化方法，通过引入因果推理来改进传统方法，当低保真度代理与目标保真度对齐不佳时表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的多保真度贝叶斯优化方法主要捕捉输入、保真度和目标之间的关联依赖关系，而非因果机制，当低保真度代理与目标保真度对齐不佳时性能会下降。

Method: RESCUE学习一个捕捉输入、保真度和目标之间因果关系的结构因果模型，并利用它构建一个编码干预效应的概率多保真度代理模型。利用因果结构，引入因果超体积知识梯度获取策略来选择输入-保真度对。

Result: RESCUE在机器人、机器学习（AutoML）和医疗保健等领域的合成和实际问题上，相比最先进的多保真度优化方法提高了样本效率。

Conclusion: 通过将因果计算融入多保真度贝叶斯优化，RESCUE能够更有效地平衡低保真度代理的成本效益和准确性，在低保真度代理与目标对齐不佳的情况下仍能保持良好性能。

Abstract: Multi-fidelity Bayesian optimization (MFBO) accelerates the search for the global optimum of black-box functions by integrating inexpensive, low-fidelity approximations. The central task of an MFBO policy is to balance the cost-efficiency of low-fidelity proxies against their reduced accuracy to ensure effective progression toward the high-fidelity optimum. Existing MFBO methods primarily capture associational dependencies between inputs, fidelities, and objectives, rather than causal mechanisms, and can perform poorly when lower-fidelity proxies are poorly aligned with the target fidelity. We propose RESCUE (REducing Sampling cost with Causal Understanding and Estimation), a multi-objective MFBO method that incorporates causal calculus to systematically address this challenge. RESCUE learns a structural causal model capturing causal relationships between inputs, fidelities, and objectives, and uses it to construct a probabilistic multi-fidelity (MF) surrogate that encodes intervention effects. Exploiting the causal structure, we introduce a causal hypervolume knowledge-gradient acquisition strategy to select input-fidelity pairs that balance expected multi-objective improvement and cost. We show that RESCUE improves sample efficiency over state-of-the-art MF optimization methods on synthetic and real-world problems in robotics, machine learning (AutoML), and healthcare.

</details>


### [202] [Sporadic Gradient Tracking over Directed Graphs: A Theoretical Perspective on Decentralized Federated Learning](https://arxiv.org/abs/2602.00791)
*Shahryar Zehtabi,Dong-Jun Han,Seyyedali Hosseinalipour,Christopher Brinton*

Main category: cs.LG

TL;DR: Spod-GT：首个结合梯度追踪和资源异构性的去中心化联邦学习算法，支持客户特定的梯度计算频率和异构非对称通信频率，在定向图上实现收敛。


<details>
  <summary>Details</summary>
Motivation: 现有DFL研究分别解决了数据异构性和资源多样性问题，但缺乏统一解决方案。需要同时处理客户数据分布不均和计算/通信资源差异的挑战。

Method: 提出Sporadic Gradient Tracking (Spod-GT)算法，在定向图上结合梯度追踪技术，允许客户特定的梯度计算频率和异构非对称通信频率，放松了对梯度估计方差和梯度多样性的假设。

Result: 通过严格的收敛分析，证明了算法在间歇性客户参与下仍能保证共识和最优性。在图像分类数据集上的实验显示，Spod-GT优于现有梯度追踪基线方法。

Conclusion: Spod-GT是首个统一处理数据异构性和资源多样性的DFL算法，在定向图上实现了高效收敛，为实际部署中资源受限的分布式学习提供了解决方案。

Abstract: Decentralized Federated Learning (DFL) enables clients with local data to collaborate in a peer-to-peer manner to train a generalized model. In this paper, we unify two branches of work that have separately solved important challenges in DFL: (i) gradient tracking techniques for mitigating data heterogeneity and (ii) accounting for diverse availability of resources across clients. We propose $\textit{Sporadic Gradient Tracking}$ ($\texttt{Spod-GT}$), the first DFL algorithm that incorporates these factors over general directed graphs by allowing (i) client-specific gradient computation frequencies and (ii) heterogeneous and asymmetric communication frequencies. We conduct a rigorous convergence analysis of our methodology with relaxed assumptions on gradient estimation variance and gradient diversity of clients, providing consensus and optimality guarantees for GT over directed graphs despite intermittent client participation. Through numerical experiments on image classification datasets, we demonstrate the efficacy of $\texttt{Spod-GT}$ compared to well-known GT baselines.

</details>


### [203] [Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion](https://arxiv.org/abs/2602.00792)
*Guinan Chen,Xunpeng Huang,Ying Sun,Shijin Wang,Yanyong Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 本文提出了掩码一致性蒸馏(MCD)，通过建立掩码扩散对偶性，将掩码离散扩散过程与连续高斯过程联系起来，实现了确定性采样，相比随机蒸馏方法获得了16倍推理加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩码离散扩散是高质量语言建模的主要范式，但其推理效率受到缺乏确定性采样工具的限制。现有方法要么性能不如掩码模型，要么依赖复杂的积分算子，而掩码域的方法通常假设不存在确定性轨迹，只能依赖随机蒸馏。

Method: 建立了显式的掩码扩散对偶性，证明掩码过程是通过新颖的最大值索引保持机制从连续高斯过程投影而来。基于此提出了掩码一致性蒸馏(MCD)框架，利用对偶性解析地构建确定性耦合轨迹，绕过数值ODE求解器。

Result: MCD严格改进了先前的随机蒸馏方法，实现了16倍的推理加速，同时不损害生成质量。为掩码和连续扩散之间的连接提供了坚实的理论基础。

Conclusion: 该研究不仅为掩码和连续扩散的连接提供了理论基础，还解锁了一致性蒸馏在高性能离散生成中的全部潜力，实现了高效且高质量的文本生成。

Abstract: Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.

</details>


### [204] [JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation](https://arxiv.org/abs/2602.00800)
*Yebin Yang,Huaijin Wu,Fu Guo,Lin Yao,Xiaohan Qin,Jingzhi Wang,Debing Zhang,Junchi Yan*

Main category: cs.LG

TL;DR: 提出token-indexed parameters作为新的扩展维度，通过Joint-Token和Mixture of Joint-Token在Transformer层中添加调制向量，以极小的计算开销显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM通过密集维度扩展时，性能提升伴随着计算成本的线性增长。MoE虽然解耦了容量和计算，但引入了内存开销和硬件效率问题。需要新的扩展轴来解耦模型容量和FLOPs。

Method: 提出token-indexed parameters作为新的扩展维度，引入Joint-Token和Mixture of Joint-Token，通过从辅助嵌入表中检索调制向量，使用轻量级逐元素操作调制主干网络，计算开销可忽略不计。

Result: 在650M到61B参数规模的密集和MoE主干上验证，一致降低验证损失，显著提升下游任务性能（如MMLU +4.1，ARC +8.3，CEval +8.9）。isoFLOPs分析显示JTok-M将质量-计算Pareto前沿移动，相比普通MoE架构节省35%计算量，token-indexed parameters展现出可预测的幂律扩展行为。

Conclusion: token-indexed parameters是一种有效的扩展维度，能够以极小计算开销显著提升模型性能，为LLM扩展提供了新的方向。

Abstract: LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed parameters as a novel, orthogonal scaling axis that decouple model capacity from FLOPs. Specifically, we introduce Joint-Token (JTok) and Mixture of Joint-Token (JTok-M), which augment Transformer layers with modulation vectors retrieved from auxiliary embedding tables. These vectors modulate the backbone via lightweight, element-wise operations, incurring negligible FLOPs overhead. Extensive experiments on both dense and MoE backbones, spanning from 650M (190M + 460M embedding) to 61B (17B + 44B embedding) total parameters, demonstrate that our approach consistently reduces validation loss and significantly improves downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval). Rigorous isoFLOPs analysis further confirms that JTok-M fundamentally shifts the quality-compute Pareto frontier, achieving comparable model quality with 35% less compute relative to vanilla MoE architectures, and we validate that token-indexed parameters exhibit a predictable power-law scaling behavior. Moreover, our efficient implementation ensures that the overhead introduced by JTok and JTok-M remains marginal.

</details>


### [205] [Mobile Exergames: Activity Recognition Based on Smartphone Sensors](https://arxiv.org/abs/2602.00809)
*David Craveiro,Hugo Silva*

Main category: cs.LG

TL;DR: 提出一款结合传感器活动识别和语音识别的2D无尽游戏Duck Catch & Fit，通过智能手机传感器和机器学习技术实现人体活动检测


<details>
  <summary>Details</summary>
Motivation: 智能手机传感器能提供丰富的用户活动和行为信息，人体活动识别在游戏、医疗和监控领域应用日益广泛，需要探索传感器数据与游戏结合的创新方式

Method: 开发2D无尽游戏Duck Catch & Fit，利用智能手机加速度计、陀螺仪和磁力计传感器，通过特征提取和学习机制检测静止、侧向移动和虚假侧向移动等活动，并集成语音识别系统识别"fire"指令

Result: 机器学习技术能有效识别人体活动且识别率较高，运动识别与语音识别的结合增强了游戏的沉浸感

Conclusion: 智能手机传感器结合机器学习技术可用于人体活动识别，运动识别与语音识别的集成能创造更沉浸的游戏体验，为传感器在游戏中的应用提供了概念验证

Abstract: Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word "fire" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.

</details>


### [206] [RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation](https://arxiv.org/abs/2602.00849)
*Yuhao Huang,Shih-Hsin Wang,Andrea L. Bertozzi,Bao Wang*

Main category: cs.LG

TL;DR: RMFlow提出了一种高效的多模态生成模型，通过结合粗粒度1-NFE MeanFlow传输和定制化的噪声注入细化步骤，在保持计算效率的同时显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: MeanFlow虽然能够实现高效的高保真图像生成，但其单次函数评估（1-NFE）生成往往无法产生令人满意的结果。需要解决1-NFE生成质量不足的问题，同时保持计算效率。

Method: RMFlow整合了粗粒度1-NFE MeanFlow传输和后续定制化噪声注入细化步骤。使用神经网络近似流路径的平均速度，通过新的损失函数训练，该函数平衡了概率路径之间的Wasserstein距离最小化和样本似然最大化。

Result: RMFlow在文本到图像、上下文到分子和时间序列生成任务上，仅使用1-NFE就达到了接近最先进水平的结果，计算成本与基线MeanFlows相当。

Conclusion: RMFlow通过创新的两阶段方法有效解决了1-NFE生成质量不足的问题，在保持计算效率的同时显著提升了生成性能，为高效多模态生成提供了有前景的解决方案。

Abstract: Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.

</details>


### [207] [Investigating the Robustness of Subtask Distillation under Spurious Correlation](https://arxiv.org/abs/2602.00852)
*Pattarawat Chormai,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 研究评估了在存在虚假相关性的数据上进行子任务蒸馏时，不同蒸馏方法的性能表现，发现先进方法如SubDistill比基线方法更稳健。


<details>
  <summary>Details</summary>
Motivation: 子任务蒸馏虽然使用教师模型，但仍依赖数据集，而现实数据集往往存在虚假相关性、代表性不足等问题。需要评估蒸馏方法在这种不完美数据上的表现。

Method: 评估了已建立的蒸馏方法以及最近的SubDistill方法，在具有虚假相关性的数据上进行蒸馏实验，观察不同方法随相关性强度增加的性能变化。

Result: 随着虚假相关性强度增加，SubDistill等先进方法保持相对稳健，而一些基线方法性能下降到接近随机水平，两者之间差距扩大。

Conclusion: 研究强调了在具有虚假相关性的不完美现实数据集上进行知识蒸馏的挑战，需要更稳健的蒸馏方法来应对数据质量问题。

Abstract: Subtask distillation is an emerging paradigm in which compact, specialized models are extracted from large, general-purpose 'foundation models' for deployment in environments with limited resources or in standalone computer systems. Although distillation uses a teacher model, it still relies on a dataset that is often limited in size and may lack representativeness or exhibit spurious correlations. In this paper, we evaluate established distillation methods, as well as the recent SubDistill method, when using data with spurious correlations for distillation. As the strength of the correlations increases, we observe a widening gap between advanced methods, such as SubDistill, which remain fairly robust, and some baseline methods, which degrade to near-random performance. Overall, our study underscores the challenges of knowledge distillation when applied to imperfect, real-world datasets, particularly those with spurious correlations.

</details>


### [208] [Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs](https://arxiv.org/abs/2602.00862)
*Shih-Hsin Wang,Yuhao Huang,Taos Transue,Justin Baker,Jonathan Forstater,Thomas Strohmer,Bao Wang*

Main category: cs.LG

TL;DR: 提出了一种高效的蛋白质多尺度图学习框架，通过构建包含细粒度子图（二级结构基序）和粗粒度图的分层图表示，使用两个GNN分别捕获局部相互作用和跨基序的高层结构关系。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN方法在学习多尺度表示和高效建模长程依赖方面面临挑战，需要更有效的蛋白质结构学习框架。

Method: 构建分层图表示：包含细粒度子图（对应α-螺旋、β-链、环等二级结构基序）和连接这些基序的粗粒度图。使用两个GNN：第一个在单个二级结构基序内学习局部相互作用，第二个建模跨基序的高层结构关系。

Result: 理论上证明该分层框架保持了最大表达能力，不会丢失关键结构信息。实证表明将基线GNN集成到该多尺度框架中显著提高了预测精度并降低了计算成本。

Conclusion: 提出的多尺度图学习框架为蛋白质结构分析提供了一种高效且表达力强的解决方案，能够更好地捕获蛋白质的多尺度结构特征。

Abstract: Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $α$-helices, $β$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.

</details>


### [209] [Improving Flow Matching by Aligning Flow Divergence](https://arxiv.org/abs/2602.00869)
*Yuhao Huang,Taos Transue,Shih-Hsin Wang,William Feldman,Hong Zhang,Bao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种改进条件流匹配（CFM）的新方法，通过同时匹配流场和其散度来提升基于流的生成模型性能，而不牺牲生成效率。


<details>
  <summary>Details</summary>
Motivation: 条件流匹配（CFM）虽然是一种高效、无需模拟的训练方法，但在学习概率路径的准确性方面存在不足。作者发现CFM不能确保概率路径的准确学习，需要新的理论框架来改进。

Method: 作者引入了一个新的偏微分方程来描述学习到的概率路径与真实概率路径之间的误差，并给出了其解。理论分析表明，两个概率路径之间的总变差差距受到CFM损失和相关散度损失的上界约束。基于这一理论洞察，设计了一个新的目标函数，同时匹配流场和其散度。

Result: 新方法在多个重要基准任务上显著提升了基于流的生成模型性能，包括动态系统生成建模、DNA序列生成和视频生成，且不牺牲生成效率。

Conclusion: 通过同时匹配流场和其散度，可以显著改进条件流匹配方法，提高生成模型的性能，同时保持高效性。该方法为基于流的生成模型提供了更准确的理论基础和实用改进。

Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.

</details>


### [210] [Learning Heat-based Equations in Self-similar variables](https://arxiv.org/abs/2602.00872)
*Shihao Wang,Qipeng Qian,Jingquan Wang*

Main category: cs.LG

TL;DR: 研究热基方程自相似变量中的解学习，开发与标准神经算子训练兼容的自相似变量训练框架，在二维不可压缩Navier-Stokes方程和一维粘性Burgers方程上验证，相比物理坐标训练，自相似坐标训练能显著提高外推准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 热基方程在长时间演化中常呈现自相似特性，但标准神经网络训练在物理坐标中难以有效捕捉这种长期动力学行为，需要开发能利用自相似结构的学习框架。

Method: 开发自相似变量训练框架，与标准神经算子训练兼容。在二维不可压缩Navier-Stokes方程和一维粘性Burgers方程上实例化，使用两种简单全连接架构（标准多层感知机和因子化全连接网络）进行对比实验，比较物理坐标和自相似坐标训练效果。

Result: 在两种系统和两种架构中，自相似变量训练的网络始终提供显著更准确和稳定的训练窗口外推，并能更好地捕捉定性长期趋势。自相似坐标训练在所有测试案例中都优于物理坐标训练。

Conclusion: 自相似坐标为学习热基方程的长期动力学提供了数学上有动机的归纳偏置，能显著提高神经网络模型的外推能力和长期行为预测准确性。

Abstract: We study solution learning for heat-based equations in self-similar variables (SSV). We develop an SSV training framework compatible with standard neural-operator training. We instantiate this framework on the two-dimensional incompressible Navier-Stokes equations and the one-dimensional viscous Burgers equation, and perform controlled comparisons between models trained in physical coordinates and in the corresponding self-similar coordinates using two simple fully connected architectures (standard multilayer perceptrons and a factorized fully connected network). Across both systems and both architectures, SSV-trained networks consistently deliver substantially more accurate and stable extrapolation beyond the training window and better capture qualitative long-time trends. These results suggest that self-similar coordinates provide a mathematically motivated inductive bias for learning the long-time dynamics of heat-based equations.

</details>


### [211] [Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs](https://arxiv.org/abs/2602.00879)
*Hao Mark Chen,Zhiwen Mo,Royson Lee,Qianzhou Wang,Da Li,Shell Xu Hu,Wayne Luk,Timothy Hospedales,Hongxiang Fan*

Main category: cs.LG

TL;DR: 提出Dynamic Expert Sharing (DES)方法，通过序列级专家共享解决MoE扩散大语言模型中的专家爆炸问题，显著减少内存开销并保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)与MoE架构结合时面临专家爆炸问题：并行生成token数量增加时，激活的专家数量线性增长，导致内存流量剧增，使推理进入内存瓶颈，抵消了MoE和并行解码的效率优势。

Method: 提出Dynamic Expert Sharing (DES)技术，将MoE优化从token级剪枝和传统专家跳过方法转向序列级核心集选择。包含两种策略：1) DES-Seq：在序列级别自适应最优分配；2) DES-Vote：基于聚合路由器权重的显著性感知投票机制，让token集体选举核心专家集。

Result: 在MoE dLLMs上的实验表明，DES将唯一专家激活减少55%以上，延迟降低达38%，同时保持99%的原始模型精度，有效解耦内存开销与并行度。

Conclusion: DES通过序列级专家共享有效解决了MoE扩散大语言模型中的专家爆炸问题，在保持模型精度的同时显著提升了推理效率，为并行解码与MoE架构的高效结合提供了新思路。

Abstract: Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens generated in parallel increases, the number of distinct experts activated grows nearly linearly. This results in substantial memory traffic that pushes inference into a memory-bound regime, negating the efficiency gains of both MoE and parallel decoding. To address this challenge, we propose Dynamic Expert Sharing (DES), a novel technique that shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection. To maximize expert reuse, DES identifies a compact, high-utility set of experts to satisfy the requirements of an entire parallel decoding block. We introduce two innovative selection strategies: (1) Intra-Sequence Sharing (DES-Seq), which adapts optimal allocation to the sequence level, and (2) Saliency-Aware Voting (DES-Vote), a novel mechanism that allows tokens to collectively elect a coreset based on aggregated router weights. Extensive experiments on MoE dLLMs demonstrate that DES reduces unique expert activations by over 55% and latency by up to 38%, while retaining 99% of vanilla accuracy, effectively decoupling memory overhead from the degree of parallelism.

</details>


### [212] [Test-time Generalization for Physics through Neural Operator Splitting](https://arxiv.org/abs/2602.00884)
*Louis Serrano,Jiequn Han,Edouard Oyallon,Shirley Ho,Rudy Morel*

Main category: cs.LG

TL;DR: 提出一种增强神经算子零样本泛化能力的方法，通过测试时搜索训练算子的组合来近似未见过的动力学，无需修改预训练权重。


<details>
  <summary>Details</summary>
Motivation: 神经算子在处理训练分布外的测试输入时泛化能力有限，如新的初始条件、未见过的PDE系数或物理现象。现有方法需要新动力学的示例进行微调，无法实现真正的零样本泛化。

Method: 基于DISCO（在不同动力学上训练的神经算子字典），提出神经算子分裂策略，在测试时搜索训练算子的组合来近似未见过的动力学，无需修改预训练权重。

Result: 在参数外推和物理现象新组合等挑战性任务上，实现了最先进的零样本泛化结果，并能恢复底层PDE参数。

Conclusion: 测试时计算是构建灵活、可组合和可泛化神经算子的关键途径，为神经算子的零样本泛化提供了新思路。

Abstract: Neural operators have shown promise in learning solution maps of partial differential equations (PDEs), but they often struggle to generalize when test inputs lie outside the training distribution, such as novel initial conditions, unseen PDE coefficients or unseen physics. Prior works address this limitation with large-scale multiple physics pretraining followed by fine-tuning, but this still requires examples from the new dynamics, falling short of true zero-shot generalization. In this work, we propose a method to enhance generalization at test time, i.e., without modifying pretrained weights. Building on DISCO, which provides a dictionary of neural operators trained across different dynamics, we introduce a neural operator splitting strategy that, at test time, searches over compositions of training operators to approximate unseen dynamics. On challenging out-of-distribution tasks including parameter extrapolation and novel combinations of physics phenomena, our approach achieves state-of-the-art zero-shot generalization results, while being able to recover the underlying PDE parameters. These results underscore test-time computation as a key avenue for building flexible, compositional, and generalizable neural operators.

</details>


### [213] [Reliability-Aware Determinantal Point Processes for Robust Informative Data Selection in Large Language Models](https://arxiv.org/abs/2602.00885)
*Ahmad Sarlak,Abolfazl Razi*

Main category: cs.LG

TL;DR: 提出ProbDPP方法，在传统DPP数据选择基础上加入可靠性感知，解决数据访问不确定性问题，通过UCB算法在线学习可靠性并保证理论性能


<details>
  <summary>Details</summary>
Motivation: 传统数据选择方法（如DPP）假设数据总能可靠访问，但在实际部署中常面临存储中断、通信不完善和随机访问失败等问题，导致原有方法失效

Method: 提出ProbDPP方法，将k-DPP目标函数重新表述为包含几何多样性项和不可靠性成本的优化问题，并构建为组合半赌博机问题，使用UCB风格算法在线学习未知可靠性

Result: ProbDPP能够在不确定性条件下鲁棒地选择多样化数据批次，理论分析为所提方法提供了遗憾界限保证

Conclusion: ProbDPP解决了传统数据选择方法在可靠性问题上的局限性，为在计算和通信约束下的大语言模型高效部署提供了可靠的数据选择方案

Abstract: Informative data selection is a key requirement for large language models (LLMs) to minimize the amount of data required for fine-tuning, network distillation, and token pruning, enabling fast and efficient deployment, especially under computational and communication constraints. Traditional subset selection methods, including those based on Determinantal Point Processes (DPP), focus on maximizing diversity but assume that selected data batches are always available error-free. This presumption prohibits their use under partial storage outage, imperfect communication, and stochastic access failures. Furthermore, we show that the original formulation collapses under such conditions. To address this gap, we introduce ProbDPP, a novel reliability-aware implementation of k-DPP that accounts for probabilistic data access by recasting the objective function with a regularization term that remains well-posed and decomposes into a geometric diversity term and unreliability cost. The resulting objective facilitates robust selection of diverse data batches under uncertainty. Furthermore, we frame this reliability-aware diversity maximization as a combinatorial semi-bandit problem and propose a UCB-style algorithm to efficiently learn the unknown reliability online. Theoretical analysis provides regret bounds for the proposed approach, ensuring performance guarantees.

</details>


### [214] [GAPNet: Plug-in Jointly Learning Task-Specific Graph for Dynamic Stock Relation](https://arxiv.org/abs/2602.00888)
*Yingjie Niu,Lanxin Lu,Changhong Jin,Ruihai Dong*

Main category: cs.LG

TL;DR: GAPNet：一种图自适应插件网络，通过联合学习任务特定拓扑和表示来改进金融预测，无需预定义图结构


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义图结构捕捉股票关系，但网络信号噪声大、异步且难以获取，导致泛化性差且与下游任务不对齐

Method: 提出GAPNet图自适应插件网络，可附加到现有图或超图骨干模型上，通过空间感知层（捕捉短期共动）和时间感知层（维持分布偏移下的长期依赖）动态适应和重连边拓扑

Result: 在两个真实股票数据集上，GAPNet相比SOTA模型持续提升盈利性和稳定性，年化累计收益达0.47（RT-GCN）和0.63（CI-STHPAN），峰值夏普比率分别为2.20和2.12

Conclusion: GAPNet的即插即用设计确保其广泛适用于不同GNN架构，结果表明联合学习图结构和表示对于任务特定关系建模至关重要

Abstract: The advent of the web has led to a paradigm shift in the financial relations, with the real-time dissemination of news, social discourse, and financial filings contributing significantly to the reshaping of financial forecasting. The existing methods rely on establishing relations a priori, i.e. predefining graphs to capture inter-stock relationships. However, the stock-related web signals are characterised by high levels of noise, asynchrony, and challenging to obtain, resulting in poor generalisability and non-alignment between the predefined graphs and the downstream tasks. To address this, we propose GAPNet, a Graph Adaptation Plug-in Network that jointly learns task-specific topology and representations in an end-to-end manner. GAPNet attaches to existing pairwise graph or hypergraph backbone models, enabling the dynamic adaptation and rewiring of edge topologies via two complementary components: a Spatial Perception Layer that captures short-term co-movements across assets, and a Temporal Perception Layer that maintains long-term dependency under distribution shift. Across two real-world stock datasets, GAPNet has been shown to consistently enhance the profitability and stability in comparision to the state-of-the-art models, yielding annualised cumulative returns of up to 0.47 for RT-GCN and 0.63 for CI-STHPAN, with peak Sharpe Ratio of 2.20 and 2.12 respectively. The plug-and-play design of GAPNet ensures its broad applicability to diverse GNN-based architectures. Our results underscore that jointly learning graph structures and representations is essential for task-specific relational modeling.

</details>


### [215] [Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation](https://arxiv.org/abs/2602.00899)
*Mritunjay Pandey*

Main category: cs.LG

TL;DR: 提出基于双塔编码器的稠密检索系统，用于电商推荐，通过语义相似度匹配解决词汇不匹配问题，相比传统BM25方法显著提升召回率。


<details>
  <summary>Details</summary>
Motivation: 电商推荐和搜索通常依赖稀疏关键词匹配（如BM25），当用户意图与产品元数据的词汇重叠有限时，这种方法会失效。需要解决词汇不匹配问题，实现基于语义相似度的内容推荐。

Method: 采用双塔编码器架构，在Amazon Reviews 2023（时尚）数据集上进行监督对比学习，使用多重负样本排序损失。训练对由评论文本（作为查询代理）和物品元数据（作为正文档）构成，最大序列长度500个token。使用FAISS HNSW索引和ONNX Runtime推理流水线，采用INT8动态量化实现高效服务。

Result: 在包含826,402个目录物品的评论到标题基准测试中，Recall@10从BM25的0.26提升到0.66。同时满足实际延迟和模型大小约束：中位CPU推理延迟6.1毫秒（批大小1），模型大小减少4倍。

Conclusion: 提供了一个端到端、可复现的蓝图，用于将领域适应的稠密检索从离线训练扩展到CPU高效的大规模目录服务，显著优于传统关键词匹配方法。

Abstract: E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.
  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.
  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.
  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.

</details>


### [216] [Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing](https://arxiv.org/abs/2602.00906)
*Anxin Guo,Jingwei Li*

Main category: cs.LG

TL;DR: 论文提出一个理论框架，将LLM的记忆问题形式化为成员测试问题，证明了在容量有限时，最优策略不是拒绝回答而是对某些非事实赋予高置信度，从而导致幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常对缺乏可推断模式的"随机事实"产生高置信度的幻觉。作者希望从信息论角度形式化分析这种记忆问题，解释幻觉的根本原因。

Method: 将事实记忆形式化为成员测试问题，统一Bloom过滤器的离散误差度量与LLM的连续对数损失。在事实稀疏的假设下，建立速率-失真定理，分析最优记忆效率。

Result: 理论分析表明，即使在最优训练、完美数据和简化"封闭世界"设置下，有限容量下的信息论最优策略不是拒绝或遗忘，而是对某些非事实赋予高置信度，导致幻觉。在合成数据上验证了该理论。

Conclusion: 幻觉是损失压缩的自然结果，即使模型训练完美，由于容量限制，信息论最优策略必然会产生幻觉。这为理解LLM幻觉提供了新的理论视角。

Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.

</details>


### [217] [PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning](https://arxiv.org/abs/2602.00907)
*Pingping Wang,Yihong Yuan,Lingcheng Li,Yongmei Lu*

Main category: cs.LG

TL;DR: PyGALAX是一个用于地理空间分析的Python包，集成了AutoML和XAI技术，可自动选择和优化机器学习模型，并通过SHAP分析保持可解释性，改进了原有的GALAX框架。


<details>
  <summary>Details</summary>
Motivation: 传统地理加权回归(GWR)方法在处理空间异质性方面存在局限性，需要更灵活、自动化的工具来分析空间非平稳性和复杂空间关系，同时保持模型的可解释性。

Method: PyGALAX集成了自动机器学习(AutoML)和可解释人工智能(XAI)技术，自动选择和优化机器学习模型，通过SHAP分析提供可解释性，并增加了自动带宽选择和灵活核函数选择功能。

Result: PyGALAX在性能上优于传统GWR方法，提供了更灵活和稳健的空间建模能力，能够有效处理空间非平稳性，并在全局和局部尺度上生成透明的空间关系洞察。

Conclusion: PyGALAX将先进的GALAX框架打包成易于访问、可重复和可部署的Python工具包，使地理、城市规划、环境科学等领域的研究者和从业者能够更便捷地使用高级地理空间机器学习方法。

Abstract: PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.

</details>


### [218] [Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment](https://arxiv.org/abs/2602.00910)
*Cuong Manh Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: 这篇综述系统梳理了面向医疗领域的轻量级深度学习架构，将现代高效模型分为CNN、轻量Transformer和线性复杂度模型三大类，并评估了模型压缩策略在保持诊断性能的同时降低硬件需求的效果。


<details>
  <summary>Details</summary>
Motivation: 大型深度学习模型在真实临床环境中部署面临计算成本高、延迟限制和患者数据隐私等挑战，需要开发高效轻量化的解决方案来弥合高性能AI与资源受限临床环境之间的差距。

Method: 采用综述研究方法，系统分类现代高效模型为卷积神经网络、轻量Transformer和线性复杂度模型三大类，并深入分析剪枝、量化、知识蒸馏和低秩分解等模型压缩策略。

Result: 提供了医疗领域高效深度学习架构的全面综合，评估了不同模型压缩策略在维持诊断性能同时降低硬件需求的效果，为临床部署提供了实用指导。

Conclusion: 该综述为研究人员和从业者提供了路线图，帮助实现从云端处理向设备端智能的转变，解决临床环境中AI部署的实际瓶颈问题。

Abstract: Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.

</details>


### [219] [Early Classification of Time Series in Non-Stationary Cost Regimes](https://arxiv.org/abs/2602.00918)
*Aurélien Renault,Alexis Bondu,Antoine Cornuéjols,Vincent Lemaire*

Main category: cs.LG

TL;DR: 该论文研究时间序列早期分类（ECTS）在成本非平稳性下的鲁棒性问题，提出在线学习方法应对成本漂移和随机变化，实验证明RL策略表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有ECTS方法假设决策成本已知、固定且正确指定，但实际中成本往往不确定且随时间变化，导致训练目标和部署目标不匹配。论文旨在解决成本非平稳性带来的挑战。

Method: 将代表性ECTS方法适配到在线学习设置，针对可分离方法仅更新触发模型而保持分类器固定。提出多种在线适应方法和基线，包括基于bandit和RL的方法，在合成数据上进行受控实验。

Result: 在线学习能有效提升ECTS方法对成本漂移的鲁棒性，其中基于RL的策略在不同成本机制下表现出强大且稳定的性能。

Conclusion: 成本非平稳性是ECTS实际应用中的重要挑战，在线学习特别是RL方法能显著提升系统对成本变化的适应能力，为实际部署提供了有效解决方案。

Abstract: Early Classification of Time Series (ECTS) addresses decision-making problems in which predictions must be made as early as possible while maintaining high accuracy. Most existing ECTS methods assume that the time-dependent decision costs governing the learning objective are known, fixed, and correctly specified. In practice, however, these costs are often uncertain and may change over time, leading to mismatches between training-time and deployment-time objectives. In this paper, we study ECTS under two practically relevant forms of cost non-stationarity: drift in the balance between misclassification and decision delay costs, and stochastic realizations of decision costs that deviate from the nominal training-time model. To address these challenges, we revisit representative ECTS approaches and adapt them to an online learning setting. Focusing on separable methods, we update only the triggering model during deployment, while keeping the classifier fixed. We propose several online adaptations and baselines, including bandit-based and RL-based approaches, and conduct controlled experiments on synthetic data to systematically evaluate robustness under cost non-stationarity. Our results demonstrate that online learning can effectively improve the robustness of ECTS methods to cost drift, with RL-based strategies exhibiting strong and stable performance across varying cost regimes.

</details>


### [220] [Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision](https://arxiv.org/abs/2602.00927)
*Yihao Xue,Allan Zhang,Jianhao Huang,Amit Sahai,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 研究发现：在仅基于结果的监督下，增加训练时推理长度（如RL中的token预算或循环Transformer的循环次数）可以持续提升模型在分布外（OOD）的性能，即使分布内（ID）性能已饱和，这表明鲁棒性需要比ID验证更大的计算预算。


<details>
  <summary>Details</summary>
Motivation: 训练LLM进行更长推理已成为构建能够解决复杂问题的先进模型的关键。当前研究通过不同方式追求这一目标，使得推理长度成为重要的扩展参数。本文旨在探索训练时推理长度对模型泛化能力的影响。

Method: 通过理论分析和实验验证两种机制：1）自我迭代能在假设类中引入更强的归纳偏置，重塑ID最优解以改善OOD泛化；2）当仅适用于ID样本的捷径解存在时，正则化能随着自我迭代次数增加减少对这些捷径的依赖。实验包括在合成任务上增加循环Transformer的循环次数，以及在数学推理任务上增加RL微调的token预算。

Result: 理论和实验均表明：在仅基于结果的监督下，增加训练时推理长度可以持续提升OOD性能，即使ID性能已饱和。这一现象揭示了鲁棒性需要比ID验证更大的计算预算，为模型训练提供了重要指导。

Conclusion: 训练时推理长度是提升模型鲁棒性的关键因素。即使ID性能饱和，继续增加推理长度仍能改善OOD泛化能力。这一发现对构建更鲁棒的LLM具有重要意义，建议在模型训练中考虑更大的推理预算。

Abstract: Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.

</details>


### [221] [Continuous-Utility Direct Preference Optimization](https://arxiv.org/abs/2602.00931)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zihao He,Muhammad Usman Rafique,Asad Aali,Muhammad Ali Jamshed,John M. Cioffi,Emily Fox*

Main category: cs.LG

TL;DR: CU-DPO：用连续效用分数替代二元偏好标签，通过策略选择和执行优化两阶段训练，提升大语言模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理能力评估过于二元化，无法捕捉推理过程中的部分进展和细粒度质量。需要一种能更好评估推理策略质量的连续评分方法。

Method: 提出连续效用直接偏好优化框架，包含两阶段训练：1) 策略选择阶段，通过最佳vs所有比较优化模型选择最佳策略；2) 执行优化阶段，使用边际分层对训练模型正确执行选定策略。

Result: 在数学推理基准测试中，策略选择准确率从35-46%提升到68-78%，下游推理性能提升达6.6分，并能有效迁移到分布外任务。

Conclusion: CU-DPO通过连续效用评分和两阶段训练，显著提升大语言模型的推理策略选择和执行能力，证明了细粒度监督信号的重要性。

Abstract: Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.

</details>


### [222] [SALAAD: Sparse And Low-Rank Adaptation via ADMM](https://arxiv.org/abs/2602.00942)
*Hao Ma,Melis Ilayda Bal,Liang Zhang,Bingcong Li,Niao He,Melanie Zeilinger,Michael Muehlebach*

Main category: cs.LG

TL;DR: SALAAD是一个即插即用的框架，通过在训练中诱导稀疏和低秩结构，实现模型容量的灵活控制，无需重新训练即可适应不同内存预算。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型在计算和内存受限环境下部署，需要灵活控制模型容量。现有方法通常依赖启发式设计，忽略了层和矩阵的异质性，或需要模型特定的架构修改。

Method: 提出SALAAD框架，在增广拉格朗日框架下制定结构化权重学习，引入自适应控制器动态平衡训练损失和结构约束，保持标准训练动态的稳定性，同时显式控制有效模型容量的演化。

Result: 实验表明SALAAD显著减少了部署时的内存消耗，性能与专门设计的方法相当。单次训练运行产生连续谱的模型容量，无需重新训练即可在不同内存预算下实现平滑弹性部署。

Conclusion: SALAAD提供了一个通用框架，可在不同模型架构中诱导稀疏和低秩结构，实现模型容量的灵活控制，为受限环境下的高效部署提供了有效解决方案。

Abstract: Modern large language models are increasingly deployed under compute and memory constraints, making flexible control of model capacity a central challenge. While sparse and low-rank structures naturally trade off capacity and performance, existing approaches often rely on heuristic designs that ignore layer and matrix heterogeneity or require model-specific architectural modifications. We propose SALAAD, a plug-and-play framework applicable to different model architectures that induces sparse and low-rank structures during training. By formulating structured weight learning under an augmented Lagrangian framework and introducing an adaptive controller that dynamically balances the training loss and structural constraints, SALAAD preserves the stability of standard training dynamics while enabling explicit control over the evolution of effective model capacity during training. Experiments across model scales show that SALAAD substantially reduces memory consumption during deployment while achieving performance comparable to ad-hoc methods. Moreover, a single training run yields a continuous spectrum of model capacities, enabling smooth and elastic deployment across diverse memory budgets without the need for retraining.

</details>


### [223] [Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems](https://arxiv.org/abs/2602.00943)
*Zhenyu Zhao,David Zhang,Ellie Zhao,Ehsan Saberian*

Main category: cs.LG

TL;DR: 提出动态先验汤普森采样方法，通过调整先验分布来控制新物品的探索概率，解决冷启动推荐中过度探索的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模推荐系统中冷启动探索的核心挑战：新物品或数据稀疏物品需要流量来估计价值，但过度探索会伤害用户并浪费曝光机会。实践中，汤普森采样通常使用均匀Beta(1,1)先验，隐含假设新物品有50%的成功率。当真实基础率远低于此值时，这种乐观先验会系统性地过度分配给弱物品。批量策略更新和管道延迟加剧了这一问题：新物品在数小时内可能保持"无数据"状态，先验主导了反馈被纳入前的分配决策。

Method: 提出动态先验汤普森采样，通过先验设计直接控制新臂（物品）胜过现有赢家的概率。核心贡献是推导出先验均值的闭式二次解，确保在引入时满足P(X_j > Y_k) = epsilon，使探索强度可预测且可调，同时保留汤普森采样的贝叶斯更新机制。

Result: 通过蒙特卡洛验证、离线批量模拟以及在服务数百万用户的缩略图个性化系统上进行的大规模在线实验，动态先验方法相比均匀先验基线实现了精确的探索控制和改进的效率。

Conclusion: 动态先验汤普森采样为推荐系统中的冷启动探索问题提供了有效的解决方案，通过可调的先验设计平衡探索与利用，在实际应用中表现出优越性能。

Abstract: Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively "no data," so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.

</details>


### [224] [Optimal Budgeted Adaptation of Large Language Models](https://arxiv.org/abs/2602.00952)
*Jing Wang,Jie Shen,Dean Foster,Zohar Karnin,Jeremy C Weiss*

Main category: cs.LG

TL;DR: 提出一个基于上下文Stackelberg博弈的预算感知监督微调框架，通过博弈论方法解决LLM微调中标注数据有限与下游准确性的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调中，标注数据的可用性与下游任务准确性之间存在权衡，这是当前的核心挑战。需要一种原则性的方法来在有限标注预算下优化模型性能。

Method: 将LLM适应建模为上下文Stackelberg博弈：学习者（领导者）承诺评分策略和标签查询策略，自适应环境（跟随者）选择具有挑战性的监督替代方案。引入有限监督预算到学习目标中，并在全反馈机制下运行算法。扩展框架时加入Largest-Latency-First置信门控机制，选择性查询标签。

Result: 在标准线性上下文假设下，算法实现了$\tilde{O}(d\sqrt{T})$的遗憾界。通过LLF置信门控，获得了预算感知的遗憾界$\tilde{O}(\sqrt{dB} + c\sqrt{B})$，其中$B=βT$。

Conclusion: 该框架为预算受限的LLM监督微调提供了理论保证，通过博弈论方法有效平衡了标注成本与模型性能，为实际应用中的资源优化提供了新思路。

Abstract: The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\tilde{O}(d\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\tilde{O}(\sqrt{dB} + c\sqrt{B})$ with $B=βT$.

</details>


### [225] [SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery](https://arxiv.org/abs/2602.00953)
*Sahar Almahfouz Nasser,Juan Francisco Pesantez Borja,Jincheng Liu,Tanvir Hasan,Zenghan Wang,Suman Ghosh,Sandeep Manandhar,Shikhar Shiromani,Twisha Shah,Naoto Tokuyama,Anant Madabhushi*

Main category: cs.LG

TL;DR: SAGE是一个基于代理的AI系统，旨在通过将图像特征与分子生物标志物和临床结果关联，生成可解释的病理学生物标志物，解决AI模型黑箱问题。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中的AI模型通常缺乏可解释性，阻碍了临床采用。现有工程化图像生物标志物往往基于零散证据而非系统生物学验证，需要更透明、生物学基础更扎实的方法。

Method: SAGE采用结构化代理系统，整合文献锚定推理和多模态数据分析，协调专门代理进行生物学背景化和经验假设验证，将图像特征与基因表达等分子生物标志物及临床结果关联。

Result: SAGE能够优先选择透明、生物学支持的生物标志物，促进计算病理学的临床转化，通过系统化方法生成可解释的工程化病理学生物标志物。

Conclusion: SAGE通过整合文献推理和多模态数据分析，提供了一种生成可解释、生物学基础扎实的病理学生物标志物的系统方法，有助于克服AI模型黑箱问题，推动计算病理学的临床应用。

Abstract: Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.

</details>


### [226] [From drift to adaptation to the failed ml model: Transfer Learning in Industrial MLOps](https://arxiv.org/abs/2602.00957)
*Waqar Muhammad Ashraf,Talha Ansar,Fahad Ahmed,Jawad Hussain,Muhammad Mujtaba Abbas,Vivek Dua*

Main category: cs.LG

TL;DR: 该论文比较了三种迁移学习方法（ETL、ALTL、LLTL）用于更新因数据漂移而失效的ANN模型，以电厂烟气差压监测为案例，发现不同批量大小下各方法表现不同。


<details>
  <summary>Details</summary>
Motivation: 在MLOps中，模型适应生产环境至关重要，但缺乏系统框架来更新因数据漂移而失效的模型。需要研究有效的模型更新策略来应对工业过程中的数据漂移问题。

Method: 研究比较了三种迁移学习启发的模型更新策略：集成迁移学习（ETL）、全层迁移学习（ALTL）和最后一层迁移学习（LLTL），用于更新失效的前馈人工神经网络模型。以660MW火力发电厂空气预热器单元的烟气差压为案例研究，模拟了电厂负荷循环的批处理过程。

Result: 对于5天批量大小，ETL提供相对较高的预测精度；对于8天批量大小，ALTL更适合有效更新模型。不同批量大小下，模型更新技术的计算需求（超参数调整和模型训练）呈现混合趋势。

Conclusion: 从基于批处理的工业案例研究中获得的基本和经验见解可以帮助MLOps从业者将失效模型适应数据漂移，实现工业过程的准确监测。不同批量大小下需要选择不同的迁移学习策略。

Abstract: Model adaptation to production environment is critical for reliable Machine Learning Operations (MLOps), less attention is paid to developing systematic framework for updating the ML models when they fail under data drift. This paper compares the transfer learning enabled model update strategies including ensemble transfer learning (ETL), all-layers transfer learning (ALTL), and last-layer transfer learning (LLTL) for updating the failed feedforward artificial neural network (ANN) model. The flue gas differential pressure across the air preheater unit installed in a 660 MW thermal power plant is analyzed as a case study since it mimics the batch processes due to load cycling in the power plant. Updating the failed ANN model by three transfer learning techniques reveals that ETL provides relatively higher predictive accuracy for the batch size of 5 days than those of LLTL and ALTL. However, ALTL is found to be suitable for effective update of the model trained on large batch size (8 days). A mixed trend is observed for computational requirement (hyperparameter tuning and model training) of model update techniques for different batch sizes. These fundamental and empiric insights obtained from the batch process-based industrial case study can assist the MLOps practitioners in adapting the failed models to data drifts for the accurate monitoring of industrial processes.

</details>


### [227] [Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction](https://arxiv.org/abs/2602.00959)
*Yuheng Yang,Siqi Zhu,Tao Feng,Ge Liu,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出一个交互式智能体框架，用于系统提取和量化大语言模型的知识边界，发现递归分类法是最有效的探索策略，并观察到知识缩放定律和Pass@1与Pass@k之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可以被视为压缩的知识库，但目前不清楚它们真正包含什么知识以及知识边界在哪里。现有的基准测试大多是静态的，对系统性的知识探测支持有限。

Method: 提出交互式智能体框架，包含四种自适应探索策略来探测不同粒度的知识。采用三阶段知识处理流程：向量过滤去除完全重复、LLM裁决解决语义重叠、领域相关性审核保留有效知识单元。

Result: 递归分类法是最有效的探索策略；观察到清晰的知识缩放定律，模型越大提取的知识越多；发现Pass@1与Pass@k的权衡：领域专用模型初始准确率高但快速退化，通用模型在扩展提取中保持稳定；训练数据组成差异导致不同模型家族具有可测量的知识特征。

Conclusion: 提出的交互式框架能够系统性地提取和量化LLM的知识，揭示了有效的探索策略、知识缩放规律以及不同模型家族的知识特征差异，为理解LLM的知识边界提供了新方法。

Abstract: Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.

</details>


### [228] [On the Spectral Flattening of Quantized Embeddings](https://arxiv.org/abs/2602.00969)
*Junlin Huang,Wenyi Fang,Zhenheng Tang,Yuxin Wang,Xueze Kang,Yang Zheng,Bo Li,Xiaowen Chu*

Main category: cs.LG

TL;DR: 论文证明语言数据的Zipfian统计特性导致嵌入矩阵具有重尾奇异值谱，这是语义编码的必要条件。均匀量化会引入噪声，截断谱尾，导致谱平坦化和稳定秩增加，最终引发表示崩溃。


<details>
  <summary>Details</summary>
Motivation: 在超低精度下训练大型语言模型面临不稳定性问题，这种不稳定性源于离散量化约束与语言数据固有的重尾谱特性之间的冲突。需要理解量化如何影响LLM的表示能力。

Method: 通过形式化Zipfian统计与随机矩阵理论之间的联系，证明嵌入矩阵奇异值谱的幂律衰减是语义编码的基本要求。推导理论界限，分析均匀量化引入的噪声如何截断谱尾。在GPT-2和TinyLlama等多样化架构上进行实证验证。

Result: 理论分析表明均匀量化引入的噪声地板会不成比例地截断谱尾，导致谱平坦化和稳定秩的严格可证明增加。实证验证证实这种几何退化会引发表示崩溃。

Conclusion: 该工作不仅量化了LLM的谱敏感性，还确立了谱保真度作为稳定低比特优化的必要条件。为理解超低精度训练中的不稳定性提供了理论基础。

Abstract: Training Large Language Models (LLMs) at ultra-low precision is critically impeded by instability rooted in the conflict between discrete quantization constraints and the intrinsic heavy-tailed spectral nature of linguistic data. By formalizing the connection between Zipfian statistics and random matrix theory, we prove that the power-law decay in the singular value spectra of embeddings is a fundamental requisite for semantic encoding. We derive theoretical bounds showing that uniform quantization introduces a noise floor that disproportionately truncates this spectral tail, which induces spectral flattening and a strictly provable increase in the stable rank of representations. Empirical validation across diverse architectures including GPT-2 and TinyLlama corroborates that this geometric degradation precipitates representational collapse. This work not only quantifies the spectral sensitivity of LLMs but also establishes spectral fidelity as a necessary condition for stable low-bit optimization.

</details>


### [229] [Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment](https://arxiv.org/abs/2602.00974)
*Adrien Aumon,Myriam Lizotte,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: FoSTA使用森林诱导的几何结构来去噪并恢复任务相关的流形，通过层次语义传输进行多模态数据对齐，在合成基准和单细胞应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有标签监督流形对齐方法大多依赖欧几里得几何建模域内关系，当特征与任务相关性较弱时，会产生噪声和语义误导结构，降低对齐质量。

Method: FoSTA利用森林诱导的几何结构去噪域内结构，从标签信息化的森林亲和度构建语义表示，通过快速层次语义传输进行对齐。

Result: 与现有基线相比，FoSTA在合成基准上改善了对应关系恢复和标签转移，在单细胞应用（包括批次校正和生物保守性）中表现出强大性能。

Conclusion: FoSTA通过森林引导的语义传输对齐，有效解决了传统欧几里得几何方法的局限性，在多模态数据对齐任务中实现了更好的语义保持和性能提升。

Abstract: Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.

</details>


### [230] [Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees](https://arxiv.org/abs/2602.00987)
*Sawan Kumar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出Random Wavelet Features (RWF)框架，通过小波采样构建可扩展的非平稳核近似，填补了表达能力强但计算复杂的模型与可扩展但有限的平稳方法之间的空白。


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多过程是非平稳的，统计特性随输入域变化，但现有可扩展方法大多依赖平稳性假设，导致表达能力和计算效率之间的困难权衡。

Method: 引入Random Wavelet Features (RWF)框架，通过从小波族中采样构建显式特征映射，利用小波的固有局部化和多分辨率结构来捕获复杂的输入依赖模式。

Result: RWF在理论上有正定性、无偏性和一致收敛保证；在合成和真实数据集上，RWF优于平稳随机特征，并在准确性和效率之间提供了有竞争力的权衡。

Conclusion: RWF为广泛的现实世界非平稳问题解锁了可扩展且表达力强的核方法，填补了现有方法在表达能力和计算效率之间的空白。

Abstract: Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.

</details>


### [231] [ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning](https://arxiv.org/abs/2602.01003)
*Zhishen Sun,Sizhe Dang,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: ESSAM：结合进化策略与锐度感知最大化的全参数微调框架，在数学推理任务上达到与RL方法相当的性能，同时大幅降低GPU内存使用（相比PPO减少18倍，相比GRPO减少10倍）


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型数学推理能力时GPU内存使用过高，限制了在资源受限环境中的应用。需要开发内存效率更高的全参数微调方法。

Method: 提出ESSAM框架，将进化策略（ES）的参数空间零阶搜索与锐度感知最大化（SAM）紧密结合，通过ES进行参数空间探索，SAM提升泛化能力，实现全参数微调。

Result: 在GSM8K数学推理任务上，ESSAM平均准确率达到78.27%，与RL方法相当（PPO 77.72%，GRPO 78.34%），在某些模型上甚至超越。GPU内存使用大幅降低：相比PPO减少18倍，相比GRPO减少10倍。

Conclusion: ESSAM在保持与RL方法相当性能的同时，显著降低了GPU内存需求，为资源受限环境下的全参数微调提供了高效解决方案，平衡了性能与资源效率。

Abstract: Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\% and is comparable to GRPO with an accuracy of 78.34\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\times$ compared to PPO and by $10\times$ compared to GRPO, achieving an extremely low GPU memory usage.

</details>


### [232] [Predicting Anemia Among Under-Five Children in Nepal Using Machine Learning and Deep Learning](https://arxiv.org/abs/2602.01005)
*Deepak Bastola,Pitambar Acharya,Dipak Dulal,Rabina Dhakal,Yang Li*

Main category: cs.LG

TL;DR: 研究使用机器学习方法预测尼泊尔儿童贫血状况，通过特征选择确定关键风险因素，比较多种分类器性能，发现逻辑回归在F1分数和召回率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 儿童贫血是尼泊尔主要的公共卫生挑战，与生长发育受损、认知能力下降和发病率增加相关。需要有效的预测模型来识别高风险儿童，以便进行早期干预和公共卫生筛查。

Method: 使用尼泊尔人口与健康调查(NDHS 2022)的1,855名儿童数据，采用四种特征选择方法(卡方检验、互信息、点二列相关、Boruta)确定关键特征，比较八种传统机器学习分类器和两种深度学习模型(DNN和TabNet)的性能。

Result: 逻辑回归获得最佳召回率(0.701)和最高F1分数(0.649)，DNN达到最高准确率(0.709)，SVM获得最强区分能力(AUC 0.736)。五个特征(儿童年龄、近期发热、家庭规模、母亲贫血状况、寄生虫驱虫)被所有方法一致选择。

Conclusion: 机器学习和深度学习模型都能提供有竞争力的贫血预测，可解释的特征如儿童年龄、感染指标、母亲贫血状况和驱虫史对尼泊尔的风险分层和公共卫生筛查至关重要。

Abstract: Childhood anemia remains a major public health challenge in Nepal and is associated with impaired growth, cognition, and increased morbidity. Using World Health Organization hemoglobin thresholds, we defined anemia status for children aged 6-59 months and formulated a binary classification task by grouping all anemia severities as \emph{anemic} versus \emph{not anemic}. We analyzed Nepal Demographic and Health Survey (NDHS 2022) microdata comprising 1,855 children and initially considered 48 candidate features spanning demographic, socioeconomic, maternal, and child health characteristics. To obtain a stable and substantiated feature set, we applied four features selection techniques (Chi-square, mutual information, point-biserial correlation, and Boruta) and prioritized features supported by multi-method consensus. Five features: child age, recent fever, household size, maternal anemia, and parasite deworming were consistently selected by all methods, while amenorrhea, ethnicity indicators, and provinces were frequently retained. We then compared eight traditional machine learning classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) with two deep learning models (DNN and TabNet) using standard evaluation metrics, emphasizing F1-score and recall due to class imbalance. Among all models, logistic regression attained the best recall (0.701) and the highest F1-score (0.649), while DNN achieved the highest accuracy (0.709), and SVM yielded the strongest discrimination with the highest AUC (0.736). Overall, the results indicate that both machine learning and deep learning models can provide competitive anemia prediction and the interpretable features such as child age, infection proxy, maternal anemia, and deworming history are central for risk stratification and public health screening in Nepal.

</details>


### [233] [LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems](https://arxiv.org/abs/2602.01009)
*Haoran Li,Chenhan Xiao,Lihao Mai,Yang Weng,Erik Blasch*

Main category: cs.LG

TL;DR: LASS-ODE：一种基于局部线性ODE表示和跨系统注意力机制的大规模ODE预测基础模型，解决了物理计算可扩展性和知识共享效率问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型在语言、视觉和时间序列分析方面取得了成功，但在物理系统动态预测方面进展有限。主要面临两个挑战：1）物理计算可扩展性：物理信息学习可以强制执行物理正则化，但其计算（如ODE积分）无法扩展到大规模系统；2）知识共享效率：注意力机制主要在单个系统内计算，限制了跨系统共享ODE结构的提取。

Method: 提出LASS-ODE模型：1）使用token-wise局部线性ODE表示，在保持物理保真度的同时扩展到基础模型规模；2）引入跨系统注意力机制，通过公共结构中心（CSH）存储共享token并聚合跨系统知识。

Result: 在40GB ODE轨迹数据集上预训练，实现了强大的领域内性能、跨多样ODE系统的零样本泛化能力，并通过微调获得额外改进。

Conclusion: 通过局部线性ODE表示和跨系统注意力机制，LASS-ODE成功解决了物理计算可扩展性和知识共享效率问题，为大规模ODE系统预测提供了有效的基础模型解决方案。

Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\underline{LA}rge-\underline{S}cale \underline{S}mall \underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.

</details>


### [234] [How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments](https://arxiv.org/abs/2602.01017)
*Fuxin Wang,Amr Alazali,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 该论文研究了链式思维推理的忠实性问题，发现模型在训练噪声低于临界阈值时能学习忠实推理，但高噪声下会转向不忠实的跳步推理，并揭示了模型通过解决不一致推理步骤来编码内部不确定性的机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的链式思维推理往往不忠实：中间步骤可能存在逻辑不一致或未能反映导致最终答案的因果关系。尽管有大量实证观察，但对链式思维的基本理解仍然缺乏——什么是忠实的链式思维推理，以及不忠实性如何从自回归训练中产生。

Method: 使用受控合成实验，在噪声数据上训练小型Transformer来解决模块化算术表达式（称为算术表达式推理任务）。通过分析不同噪声水平下的训练动态，研究模型学习推理模式的变化。

Result: 发现模型在训练噪声低于临界阈值时能学习忠实推理，遵循底层算术规则；高噪声水平下，训练动态从忠实逐步推理转变为不忠实跳步推理，中间出现预测熵暂时增加的混合模式。机制分析显示模型通过解决不一致推理步骤来编码内部不确定性。

Conclusion: 模型能够学习忠实推理，但受训练噪声阈值限制；训练动态显示从忠实推理到不忠实推理的转变；模型通过解决不一致步骤来编码内部不确定性，这表明自回归训练中出现了隐式自我验证机制。

Abstract: Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.

</details>


### [235] [Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01025)
*Kaiyuan Cui,Yige Li,Yutao Wu,Xingjun Ma,Sarah Erfani,Christopher Leckie,Hanxun Huang*

Main category: cs.LG

TL;DR: 提出UltraBreak框架，通过视觉空间变换和正则化约束对抗模式，同时通过语义目标放松文本目标，实现跨模型和攻击目标的通用可迁移越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的越狱方法迁移性差，对抗模式容易过拟合到单个白盒代理模型，无法泛化到黑盒模型。需要开发能够跨模型和攻击目标通用的越狱方法。

Method: UltraBreak框架：1）在视觉空间通过变换和正则化约束对抗模式；2）通过语义目标放松文本目标；3）在目标LLM的文本嵌入空间中定义损失函数，发现通用对抗模式。

Result: UltraBreak在广泛实验中始终优于现有越狱方法。分析表明，通过语义目标平滑损失景观对于实现通用可迁移越狱至关重要。

Conclusion: UltraBreak通过结合视觉级正则化和语义引导的文本监督，减轻代理过拟合，实现跨模型和攻击目标的强迁移性，为视觉语言模型安全提供新见解。

Abstract: Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose Universal and transferable jailbreak (UltraBreak), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks. The code is publicly available in our \href{https://github.com/kaiyuanCui/UltraBreak}{GitHub repository}.

</details>


### [236] [SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2602.01027)
*Xin Nie,Haicheng Zhang,Liang Dong,Beining Feng,Jinhong Weng,Guiling Sun*

Main category: cs.LG

TL;DR: SFMP是一个无需搜索、硬件友好的混合精度量化框架，通过分数位宽、块级混合精度、行列重排序和统一GEMM核等技术，在相同内存约束下超越现有层级混合精度方法。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度方法存在两个主要问题：要么依赖昂贵的离散优化来确定精度分配，要么由于不规则内存布局导致硬件效率低下。需要一种既高效又硬件友好的混合精度量化方案。

Method: 1) 分数位宽：将权重矩阵的整数位宽扩展为分数值，将离散精度分配转化为连续问题；2) 块级混合精度：在权重矩阵内实现细粒度精度分配，同时保持硬件友好；3) 行列权重重排序：通过行列重排序聚合重要权重，推理时仅引入小的激活重排序开销；4) 统一GEMM核：支持任意平均位宽的混合精度GEMM运算。

Result: 大量实验表明，SFMP在相同内存约束下超越了最先进的层级混合精度方法，同时显著降低了量化成本并提高了推理效率。

Conclusion: SFMP通过创新的分数位宽、块级混合精度、权重重排序和统一计算核设计，成功解决了现有混合精度量化方法的局限性，实现了高效且硬件友好的大语言模型压缩。

Abstract: Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or introduce hardware inefficiencies due to irregular memory layouts. We propose SFMP, a search-free and hardware-friendly mixed-precision quantization framework for large language models. The framework is built upon four novel ideas: Fractional bit-width, which extends integer bit-width for weight matrix to fractional value and transforms discrete precision allocation as a continuous problem; 2)Block-wise mixed-precision, enabling fine-grained precision within weight matrices while remaining hardware-friendly; 3)Row-column weight reordering, which aggregates salient weights via row and column reordering, incurring only a small activation reordering overhead during inference; 4)Unified GEMM kernel, which supports mixed-precision GEMM at arbitrary average bit-width. Extensive experiments demonstrate that SFMP outperforms state-of-the-art layer-wise mixed-precision methods under the same memory constraints, while significantly reducing quantization cost and improving inference efficiency. Code is available at https://github.com/Nkniexin/SFMP

</details>


### [237] [Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection](https://arxiv.org/abs/2602.01039)
*Zhiwei Ling,Hailiang Zhao,Chao Zhang,Xiang Ao,Ziqi Wang,Cheng Zhang,Zhen Qin,Xinkui Zhao,Kingsum Chow,Yuanqing Wu,MengChu Zhou*

Main category: cs.LG

TL;DR: FLood：一种基于OOD检测的联邦学习框架，通过双重加权机制动态应对数据异构性，提升模型收敛稳定性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现实世界联邦学习部署中，用户、设备和应用场景的异构性导致数据非独立同分布，严重破坏全局模型的收敛稳定性、泛化能力和服务质量

Method: 提出FLood框架，采用双重加权机制：客户端层面通过上加权伪OOD样本自适应重加权监督损失；服务器层面根据客户端OOD置信度分数加权聚合，优先考虑分布一致性更高的客户端更新

Result: 在多种非IID设置下的基准测试中，FLood在准确率和泛化能力上均优于现有最先进的联邦学习方法，且可作为正交插件模块无缝集成到现有算法中提升性能

Conclusion: FLood为现实世界联邦环境部署可靠智能服务提供了实用且可扩展的解决方案，能够有效应对数据异构性挑战

Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.

</details>


### [238] [SwiftRepertoire: Few-Shot Immune-Signature Synthesis via Dynamic Kernel Codes](https://arxiv.org/abs/2602.01051)
*Rong Fu,Wenxin Zhang,Muge Qi,Yang Li,Yabin Jin,Jiekai Wu,Jiaxuan Lu,Chunlei Meng,Youjin Wang,Zeli Su,Juntao Gao,Li Bao,Qi Zhao,Wei Luo,Simon Fong*

Main category: cs.LG

TL;DR: 提出一个框架，通过从学习到的原型字典中合成紧凑的任务特定参数化，实现T细胞受体库的样本高效、可解释分析，无需完整模型微调


<details>
  <summary>Details</summary>
Motivation: T细胞受体库分析为疾病检测和免疫监测提供生物信号，但实际部署受到标签稀疏、队列异质性和大型编码器适应新任务的计算负担阻碍

Method: 基于轻量级任务描述符（来自库探针和池化嵌入统计）从学习到的原型字典中合成紧凑的任务特定参数化，生成小型适配器模块应用于冻结的预训练主干，通过基序感知探针和校准的基序发现管道保持可解释性

Result: 该框架能够在仅有少量支持样本的情况下立即适应新任务，无需完整模型微调，提供样本高效、计算资源受限的解决方案

Conclusion: 为在标记数据稀缺和计算资源受限的临床和研究环境中部署库信息模型提供了实用、样本高效且可解释的途径

Abstract: Repertoire-level analysis of T cell receptors offers a biologically grounded signal for disease detection and immune monitoring, yet practical deployment is impeded by label sparsity, cohort heterogeneity, and the computational burden of adapting large encoders to new tasks. We introduce a framework that synthesizes compact task-specific parameterizations from a learned dictionary of prototypes conditioned on lightweight task descriptors derived from repertoire probes and pooled embedding statistics. This synthesis produces small adapter modules applied to a frozen pretrained backbone, enabling immediate adaptation to novel tasks with only a handful of support examples and without full model fine-tuning. The architecture preserves interpretability through motif-aware probes and a calibrated motif discovery pipeline that links predictive decisions to sequence-level signals. Together, these components yield a practical, sample-efficient, and interpretable pathway for translating repertoire-informed models into diverse clinical and research settings where labeled data are scarce and computational resources are constrained.

</details>


### [239] [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)
*Hyesung Jeon,Hyeongju Ha,Jae-Joon Kim*

Main category: cs.LG

TL;DR: LRAgent：针对多LoRA智能体系统的KV缓存共享框架，通过分解缓存为共享基础组件和低秩适配器组件，显著降低内存和计算开销


<details>
  <summary>Details</summary>
Motivation: 多LLM智能体系统中，虽然智能体共享预训练骨干网络，但每个智能体独立存储自己的KV缓存，导致相同工具增强轨迹的重复存储和计算开销。现有KV缓存共享方法未充分考虑多LoRA场景。

Method: 提出LRAgent框架：1）将缓存分解为共享基础组件（来自预训练权重）和适配器依赖组件（来自LoRA权重）；2）共享基础组件，以低秩形式存储适配器组件；3）引入Flash-LoRA-Attention内核，重新排序注意力计算以避免将低秩缓存物化为完整维度。

Result: 在智能体问答基准测试中，LRAgent实现了接近完全共享缓存的吞吐量和首令牌延迟，同时保持了接近非共享缓存基线的准确性。

Conclusion: LRAgent通过有效共享多LoRA智能体系统中的KV缓存，显著降低了内存和计算开销，为多智能体系统的高效部署提供了实用解决方案。

Abstract: Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.

</details>


### [240] [Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning](https://arxiv.org/abs/2602.01058)
*Dylan Zhang,Yufeng Xu,Haojin Wang,Qingzhi Chen,Hao Peng*

Main category: cs.LG

TL;DR: PEAR是一种SFT阶段方法，通过重要性采样重新加权SFT损失，解决SFT与RL训练中的分布不匹配问题，从而提升后续RL训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练流程中，离线SFT阶段通常孤立优化以最大化SFT性能，但更强的SFT检查点经过相同RL训练后可能表现更差。这是因为离线SFT数据生成分布与在线RL优化策略分布存在不匹配。

Method: 提出PEAR方法，使用重要性采样重新加权SFT损失，包含token级、block级和序列级三种变体。该方法可增强标准SFT目标，在收集离线数据概率后几乎不增加额外训练开销。

Result: 在可验证推理游戏和数学推理任务上的实验表明，PEAR相比标准SFT能持续提升RL后性能，在AIME2025上pass@8提升高达14.6%。

Conclusion: PEAR通过在设计SFT时考虑下游RL而非孤立优化，是实现更整体化LLM后训练的有效步骤。

Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.
  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.
  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.
  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.

</details>


### [241] [On the Expressive Power of Permutation-Equivariant Weight-Space Networks](https://arxiv.org/abs/2602.01083)
*Adir Dayan,Yam Eitan,Haggai Maron*

Main category: cs.LG

TL;DR: 该论文系统研究了权重空间网络的表达能力，证明了主流置换等变网络具有相同的表达能力，并在温和假设下建立了权重空间和函数空间的普适性理论。


<details>
  <summary>Details</summary>
Motivation: 权重空间学习直接操作其他神经网络的参数，随着预训练模型的普及，这类网络在各种任务中表现出色。现有SOTA方法依赖置换等变设计来提升泛化能力，但这可能影响表达能力，需要理论分析。权重空间学习同时涉及权重空间和函数空间的映射，使得表达能力分析特别复杂，现有部分结果不够全面。

Method: 开发权重空间网络表达能力的系统理论，首先证明所有主流置换等变网络在表达能力上是等价的，然后在输入权重的温和自然假设下，建立权重空间和函数空间的普适性理论，并刻画普适性不再成立的边界情况。

Result: 证明了所有主要置换等变网络具有相同的表达能力；在温和假设下建立了权重空间和函数空间的普适性；系统刻画了普适性失效的边界情况；为权重空间网络的表达能力提供了统一的理论基础。

Conclusion: 该研究填补了权重空间网络表达能力理论分析的空白，通过系统理论证明了主流置换等变网络的等价性，建立了普适性理论，为权重空间学习提供了坚实的理论基础。

Abstract: Weight-space learning studies neural architectures that operate directly on the parameters of other neural networks. Motivated by the growing availability of pretrained models, recent work has demonstrated the effectiveness of weight-space networks across a wide range of tasks. SOTA weight-space networks rely on permutation-equivariant designs to improve generalization. However, this may negatively affect expressive power, warranting theoretical investigation. Importantly, unlike other structured domains, weight-space learning targets maps operating on both weight and function spaces, making expressivity analysis particularly subtle. While a few prior works provide partial expressivity results, a comprehensive characterization is still missing. In this work, we address this gap by developing a systematic theory for expressivity of weight-space networks. We first prove that all prominent permutation-equivariant networks are equivalent in expressive power. We then establish universality in both weight- and function-space settings under mild, natural assumptions on the input weights, and characterize the edge-case regimes where universality no longer holds. Together, these results provide a strong and unified foundation for the expressivity of weight-space networks.

</details>


### [242] [OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases](https://arxiv.org/abs/2602.01105)
*Zixiao Wang,Yifei Shen,Huishuai Zhang*

Main category: cs.LG

TL;DR: 提出一种名为A的新优化器，结合谱控制和坐标控制，在语言和视觉任务中表现优于AdamW和Muon，且仅需动量级状态


<details>
  <summary>Details</summary>
Motivation: 许多优化器可解释为范数诱导几何下的最速下降法，从而继承相应的隐式偏差。需要结合谱控制和坐标控制来改进优化器性能

Method: 结合正交化更新方向的谱控制和符号更新的ℓ∞风格坐标控制，形成Lion风格动量方向，通过少量Newton-Schulz迭代近似正交化，然后应用逐元素符号

Result: 在GPT-2和Llama预训练、SiT图像预训练和监督微调等大规模语言和视觉任务中，匹配或优于AdamW和Muon，同时仅使用动量级优化器状态

Conclusion: A优化器有效结合谱和ℓ∞约束，在保持高效的同时提升性能，并能缓解AdamW预训练检查点的优化器不匹配问题

Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \nameA{} (\fullname{}), which combines spectral control from orthogonalized update directions with $\ell_\infty$-style coordinate control from sign updates. \nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\ell_\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.

</details>


### [243] [Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems](https://arxiv.org/abs/2602.01113)
*Wenjie Liang,Ranhui Yan,Jia Cai,You-Gan Wang*

Main category: cs.LG

TL;DR: 论文提出SEGIA单边图注入攻击方法，通过单个边注入伪造节点来影响工业GNN系统，在有限资源下实现比基线高25%的攻击成功率


<details>
  <summary>Details</summary>
Motivation: 工业GNN监控系统中，攻击者可能通过注入少量伪造节点（如恶意传感器、虚拟端点）来偏向下游决策，同时规避基于拓扑和同质性的安全检测，这揭示了工业GNN部署中的系统性风险

Method: 提出SEGIA（单边图注入攻击），每个注入节点仅通过单边连接到操作图。方法集成了剪枝SGC代理模型、多跳邻域采样、基于反向图卷积的特征合成，以及相似性正则化目标来保持局部同质性并规避边剪枝防御

Result: 理论分析和跨数据集评估显示，在显著更小的边预算下，攻击成功率比代表性基线至少高25%。攻击能有效规避多种防御机制

Conclusion: 研究揭示了工业GNN部署中的系统级风险，需要轻量级的准入验证和邻域一致性监控来防御此类攻击

Abstract: Graph neural networks (GNNs) are increasingly adopted in industrial graph-based monitoring systems (e.g., Industrial internet of things (IIoT) device graphs, power-grid topology models, and manufacturing communication networks) to support anomaly detection, state estimation, and asset classification. In such settings, an adversary that compromises a small number of edge devices may inject counterfeit nodes (e.g., rogue sensors, virtualized endpoints, or spoofed substations) to bias downstream decisions while evading topology- and homophily-based sanitization. This paper formulates deployment-oriented node-injection attacks under constrained resources and proposes the \emph{Single-Edge Graph Injection Attack} (SEGIA), in which each injected node attaches to the operational graph through a single edge. SEGIA integrates a pruned SGC surrogate, multi-hop neighborhood sampling, and reverse graph convolution-based feature synthesis with a similarity-regularized objective to preserve local homophily and survive edge pruning. Theoretical analysis and extensive evaluations across datasets and defenses show at least $25\%$ higher attack success than representative baselines under substantially smaller edge budgets. These results indicate a system-level risk in industrial GNN deployments and motivate lightweight admission validation and neighborhood-consistency monitoring.

</details>


### [244] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 提出MarkovScale框架，将顺序缩放建模为马尔可夫过程，获得理论最优边界，在准确性和效率间实现理论指导的平衡。


<details>
  <summary>Details</summary>
Motivation: 顺序缩放作为重要的推理时缩放范式，其性能改进通常有限且缺乏理论理解，现有启发式方法缺乏明确的最优性边界。

Method: 将顺序缩放建模为两状态马尔可夫过程，推导出闭式解，包括准确率提升的具体条件和理论上、中、下性能边界，并基于此开发MarkovScale系统。

Result: 在3个骨干LLM、5个基准测试和20多个配置上的实验表明，MarkovScale持续优于最先进的并行和顺序缩放方法。

Conclusion: 该工作为LLM推理提供了理论指导的最优性和资源效率平衡，是迈向最优推理的重要一步。

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [245] [ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs](https://arxiv.org/abs/2602.01124)
*Md Abrar Jahin,Taufikur Rahman Fuad,Jay Pujara,Craig Knoblock*

Main category: cs.LG

TL;DR: ChronoSpike：一种自适应脉冲图神经网络，通过可学习的LIF神经元、多头注意力空间聚合和轻量级Transformer时间编码器，在动态图表示学习中实现了线性内存复杂度和高效训练，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态图表示学习方法面临基本权衡：基于注意力的方法表达能力强但复杂度高（O(T²)），循环架构存在梯度问题和密集状态存储问题，脉冲神经网络虽然事件驱动高效但受限于顺序传播、二进制信息丢失和局部聚合缺乏全局上下文。

Method: ChronoSpike整合了三个关键组件：1）具有每通道膜动力学的可学习LIF神经元；2）在连续特征上的多头注意力空间聚合；3）轻量级Transformer时间编码器。该方法实现了细粒度局部建模和长程依赖捕获，具有线性内存复杂度O(T·d)。

Result: 在三个大规模基准测试中，ChronoSpike在12个最先进基线方法上提升了2.0% Macro-F1和2.4% Micro-F1，训练速度比循环方法快3-10倍，参数预算恒定105K（与图大小无关）。理论分析证明了膜电位有界性、收缩因子ρ<1下的梯度流稳定性和BIBO稳定性，可解释性分析揭示了异质时间感受野和83-88%稀疏性的学习优先效应。

Conclusion: ChronoSpike通过结合脉冲神经网络的事件驱动效率与注意力机制的全局建模能力，解决了动态图表示学习中的效率-表达能力权衡问题，实现了高性能、高效率且理论保证的解决方案。

Abstract: Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\%$ Macro-F1 and $2.4\%$ Micro-F1 while achieving $3-10\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $ρ< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\%$ sparsity.

</details>


### [246] [WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity](https://arxiv.org/abs/2602.01126)
*Mengsha Kou,Xiaoyu Xia,Ziqi Wang,Ibrahim Khalil,Runkun Luo,Jingwen Zhou,Minhui Xue*

Main category: cs.LG

TL;DR: WinFLoRA提出了一种隐私异构的联邦LoRA方法，通过基于客户端上传的LoRA适配器估计噪声水平，并以此调整聚合权重作为激励，在保护隐私的同时提升全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感的联邦学习场景中，不同客户端注入不同水平的差分隐私噪声，导致隐私异质性。这种异质性使个体激励与全局性能不一致，需要一种能同时满足异构隐私需求和提升全局性能的解决方案。

Method: WinFLoRA通过分析客户端上传的LoRA适配器来估计噪声水平，将聚合权重作为激励机制。噪声水平较低的客户端获得更大权重，从而对全局模型有更大影响力。这种方法无需第三方参与，能自动对齐客户端隐私效用与全局目标。

Result: 在多个大型语言模型和数据集上的评估显示，WinFLoRA相比现有基准方法，全局准确率最高提升52.58%，客户端效用最高提升2.56倍。

Conclusion: WinFLoRA成功解决了隐私异构联邦学习中的激励对齐问题，通过噪声感知的权重分配机制，在满足客户端不同隐私需求的同时显著提升了全局模型性能。

Abstract: Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.

</details>


### [247] [Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models](https://arxiv.org/abs/2602.01128)
*Mete Erdogan*

Main category: cs.LG

TL;DR: TS-DPO在切线空间进行偏好优化，学习各目标的更新方向，可在推理时线性组合实现用户指定的行为平衡，无需额外优化。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）将多维度反馈压缩为单一标量奖励，固定了目标间的平衡，无法遍历帕累托前沿，限制了模型在不同偏好维度（如帮助性、安全性、冗长度）间的可控调整。

Method: 基于切线空间微调理论，将DPO扩展到局部线性区域，学习每个目标对应的更新方向。这些方向可在推理时线性组合，生成用户指定的行为，无需额外优化。

Result: 在HelpSteer和UltraFeedback数据集上评估帮助性-冗长度权衡，TS-DPO比标量化DPO实现了更广泛的帕累托最优覆盖和更平滑的偏好控制。典型相关分析显示切线空间训练增强了与不同偏好对齐的典型方向，改善了可分离性。

Conclusion: TS-DPO通过切线空间中的偏好优化，实现了多维度人类偏好的原则性和可控对齐，使LLM能够在推理时灵活调整不同偏好维度间的平衡。

Abstract: Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.

</details>


### [248] [TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation](https://arxiv.org/abs/2602.01135)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: TRACE：利用自回归模型作为预训练密度估计器，从单个事件序列中进行因果发现的可扩展框架


<details>
  <summary>Details</summary>
Motivation: 研究从单个离散事件序列（如车辆日志、制造系统或患者轨迹）中发现因果关系，这一领域面临无重复样本、高维度和长程时间依赖的挑战

Method: TRACE框架将自回归模型重新用作预训练密度估计器，用于条件互信息估计，可线性扩展事件词汇量，支持延迟因果效应，并在GPU上完全并行化

Result: 在理论可识别性方面建立了不完美自回归模型下的保证，实验表明在不同基线和不同词汇量下具有鲁棒性能，包括在超过29,100个事件类型的车辆诊断根因分析中的应用

Conclusion: TRACE为从单个事件序列中进行因果发现提供了一个可扩展且理论保证的解决方案，特别适用于高维度和长程依赖的复杂系统

Abstract: We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.

</details>


### [249] [A Unified Matrix-Spectral Framework for Stability and Interpretability in Deep Learning](https://arxiv.org/abs/2602.01136)
*Ronald Katende*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的矩阵谱框架来分析深度神经网络的稳定性和可解释性，通过聚合雅可比矩阵、参数梯度、神经正切核算子和损失Hessian矩阵的谱信息，引入全局矩阵稳定性指数来控制前向敏感性、归因鲁棒性和优化条件。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在输入扰动、标签噪声和训练动态方面的稳定性和可解释性缺乏统一的理论框架。现有的分析方法往往分散，需要一种能够整合不同谱信息的统一方法来量化网络稳定性。

Method: 将神经网络表示为数据依赖的线性算子乘积，引入全局矩阵稳定性指数聚合雅可比矩阵、参数梯度、神经正切核算子和损失Hessian矩阵的谱信息。使用谱熵改进经典算子范数界，捕捉典型敏感性而非最坏情况。

Result: 在MNIST、CIFAR-10和CIFAR-100上的合成实验和受控研究表明，适度的谱正则化即使全局谱摘要变化不大，也能显著改善归因稳定性。建立了谱集中与分析稳定性之间的精确联系。

Conclusion: 该框架为鲁棒性感知的模型设计和训练提供了实用指导，通过可计算的诊断和稳定性导向的正则化原则，实现了对深度神经网络稳定性的系统分析和改进。

Abstract: We develop a unified matrix-spectral framework for analyzing stability and interpretability in deep neural networks. Representing networks as data-dependent products of linear operators reveals spectral quantities governing sensitivity to input perturbations, label noise, and training dynamics.
  We introduce a Global Matrix Stability Index that aggregates spectral information from Jacobians, parameter gradients, Neural Tangent Kernel operators, and loss Hessians into a single stability scale controlling forward sensitivity, attribution robustness, and optimization conditioning. We further show that spectral entropy refines classical operator-norm bounds by capturing typical, rather than purely worst-case, sensitivity.
  These quantities yield computable diagnostics and stability-oriented regularization principles. Synthetic experiments and controlled studies on MNIST, CIFAR-10, and CIFAR-100 confirm that modest spectral regularization substantially improves attribution stability even when global spectral summaries change little.
  The results establish a precise connection between spectral concentration and analytic stability, providing practical guidance for robustness-aware model design and training.

</details>


### [250] [Self-Generative Adversarial Fine-Tuning for Large Language Models](https://arxiv.org/abs/2602.01137)
*Shiguang Wu,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: SGALM提出了一种新的LLM对齐框架，通过单个LLM内部的生成对抗游戏实现自生成对抗学习，无需外部奖励模型或大量人工标注。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法（监督微调或人类反馈强化学习）依赖高质量标注，成本高且稀缺；现有自生成方法依赖启发式假设或无基础的自评估，容易导致偏差累积和性能漂移。

Method: 提出Self-Generative Adversarial LLM (SGALM)框架，将对齐问题形式化为单个LLM内部的生成对抗游戏，联合进化生成和判别能力，无需外部奖励模型。

Result: 理论和实证结果表明SGALM达到了最先进的性能，既可作为有效的对齐算法，也可作为鲁棒的合成数据生成引擎。

Conclusion: SGALM提供了一种统一的自生成对抗学习框架，能够有效解决LLM对齐问题，减少对昂贵人工标注的依赖，同时避免现有自生成方法的偏差问题。

Abstract: Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.

</details>


### [251] [Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization](https://arxiv.org/abs/2602.01139)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: 该论文针对图神经网络在泛化性、对抗鲁棒性和表示学习能力方面的挑战，提出了基于图移位算子的表示学习技术、图数据增强方法和正交化噪声防御三大贡献。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络在结构化数据学习中表现出强大能力并广泛应用，但其在泛化性、对抗扰动鲁棒性和表示学习有效性方面仍存在显著限制，需要系统性解决这些核心挑战。

Method: 1. 基于图移位算子开发新的表示学习技术；2. 通过图数据增强引入泛化增强方法；3. 利用正交化技术和基于噪声的防御机制开发更鲁棒的图神经网络。

Result: 论文提出了系统性的解决方案框架，从表示学习、泛化增强到对抗鲁棒性三个维度全面提升了图神经网络的性能，为理解GNN的局限性和潜力提供了更原则性的理论基础。

Conclusion: 通过解决图神经网络的核心挑战，该研究为GNN的局限性理解和潜力挖掘提供了更原则性的框架，推动了图表示学习领域的发展。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations from structured data. Despite their growing popularity and success across various applications, GNNs encounter several challenges that limit their performance. in their generalization, robustness to adversarial perturbations, and the effectiveness of their representation learning capabilities. In this dissertation, I investigate these core aspects through three main contributions: (1) developing new representation learning techniques based on Graph Shift Operators (GSOs, aiming for enhanced performance across various contexts and applications, (2) introducing generalization-enhancing methods through graph data augmentation, and (3) developing more robust GNNs by leveraging orthonormalization techniques and noise-based defenses against adversarial attacks. By addressing these challenges, my work provides a more principled understanding of the limitations and potential of GNNs.

</details>


### [252] [Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization](https://arxiv.org/abs/2602.01140)
*Haochen You,Heng Zhang,Hongyang He,Yuqi Li,Baojing Liu*

Main category: cs.LG

TL;DR: GRIT-VQ是一种新型向量量化框架，通过半径更新和集成变换解决传统VQ的梯度不稳定和码本利用不足问题，保持前向硬分配的同时实现完全可微优化。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化使用硬最近邻分配和直通估计器，导致梯度不稳定、码本利用不足，且更新步长与量化间隙耦合，每个码字独立更新，在大规模应用中表现不佳。

Method: GRIT-VQ采用半径更新机制，沿量化方向以可控的几何感知步长移动潜在表示；同时应用数据无关的集成变换，使所有码字通过共享参数更新而非独立更新，保持前向硬分配的同时实现完全可微。

Result: 在图像重建、图像生成和推荐系统标记化基准测试中，GRIT-VQ持续改善重建误差、生成质量和推荐准确性，同时显著提高码本利用率。

Conclusion: GRIT-VQ提供了一个统一的代理框架，解决了传统向量量化的核心优化问题，通过理论分析阐明了其稳定梯度流、协调码本演化和避免崩溃的机制，在各种应用中表现出优越性能。

Abstract: Vector quantization (VQ) underpins modern generative and representation models by turning continuous latents into discrete tokens. Yet hard nearest-neighbor assignments are non-differentiable and are typically optimized with heuristic straight-through estimators, which couple the update step size to the quantization gap and train each code in isolation, leading to unstable gradients and severe codebook under-utilization at scale. In this paper, we introduce GRIT-VQ (Generalized Radius and Integrated Transform-Vector Quantization), a unified surrogate framework that keeps hard assignments in the forward pass while making VQ fully differentiable. GRIT-VQ replaces the straight-through estimator with a radius-based update that moves latents along the quantization direction with a controllable, geometry-aware step, and applies a data-agnostic integrated transform to the codebook so that all codes are updated through shared parameters instead of independently. Our theoretical analysis clarifies the fundamental optimization dynamics introduced by GRIT-VQ, establishing conditions for stable gradient flow, coordinated codebook evolution, and reliable avoidance of collapse across a broad family of quantizers. Across image reconstruction, image generation, and recommendation tokenization benchmarks, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization compared to existing VQ variants.

</details>


### [253] [Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing](https://arxiv.org/abs/2602.01150)
*Jialong Sun,Zeming Wei,Jiaxuan Zou,Jiacheng Gong,Guanheng Wang,Chengyang Dong,Jialong Li,Bo Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的机器遗忘审计框架SMIA，通过统计测试直接比较成员和非成员数据的分布，无需训练攻击模型，提供带置信区间的遗忘率评估，比传统MIA方法更可靠且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘审计主要依赖成员推理攻击(MIA)，但MIA存在根本缺陷：成员推理失败并不等同于真正遗忘。MIA作为二分类问题会产生无法观测的统计误差，导致对遗忘性能的过度乐观评估，同时需要大量计算资源训练影子模型。

Method: 提出统计成员推理攻击(SMIA)框架：1) 使用统计测试直接比较成员和非成员数据的分布差异；2) 无需训练攻击模型，避免学习偏差；3) 输出带置信区间的遗忘率，量化审计结果的可靠性；4) 完全训练免费，显著降低计算成本。

Result: 大量实验表明：1) SMIA提供比传统MIA方法更可靠的审计结果；2) 计算成本显著降低；3) 能够输出带置信区间的遗忘率评估；4) 理论保证和实证效果表明SMIA可作为可靠的机器遗忘审计新范式。

Conclusion: SMIA通过统计测试直接比较数据分布，解决了MIA审计的统计误差问题，提供可靠且高效的机器遗忘审计方法。其训练免费特性和置信区间输出使其成为机器遗忘审计的新范式。

Abstract: Machine unlearning (MU) is essential for enforcing the right to be forgotten in machine learning systems. A key challenge of MU is how to reliably audit whether a model has truly forgotten specified training data. Membership Inference Attacks (MIAs) are widely used for unlearning auditing, where samples that evade membership detection are often regarded as successfully forgotten. After carefully revisiting the reliability of MIA, we show that this assumption is flawed: failed membership inference does not imply true forgetting. We theoretically demonstrate that MIA-based auditing, when formulated as a binary classification problem, inevitably incurs statistical errors whose magnitude cannot be observed during the auditing process. This leads to overly optimistic evaluations of unlearning performance, while incurring substantial computational overhead due to shadow model training. To address these limitations, we propose Statistical Membership Inference Attack (SMIA), a novel training-free and highly effective auditing framework. SMIA directly compares the distributions of member and non-member data using statistical tests, eliminating the need for learned attack models. Moreover, SMIA outputs both a forgetting rate and a corresponding confidence interval, enabling quantified reliability of the auditing results. Extensive experiments show that SMIA provides more reliable auditing with significantly lower computational cost than existing MIA-based approaches. Notably, the theoretical guarantees and empirical effectiveness of SMIA suggest it as a new paradigm for reliable machine unlearning auditing.

</details>


### [254] [PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning](https://arxiv.org/abs/2602.01156)
*Shunpeng Yang,Ben Liu,Hua Chen*

Main category: cs.LG

TL;DR: PolicyFlow：一种基于连续归一化流（CNF）的新型强化学习算法，通过近似重要性比率避免昂贵的似然计算，结合布朗正则化器防止模式崩溃，在多个环境中表现优于标准PPO和高斯策略。


<details>
  <summary>Details</summary>
Motivation: 标准PPO算法依赖重要性比率，需要计算策略似然，这对于高斯分布策略很简单，但对于表达能力更强的连续归一化流（CNF）策略来说，沿整个流轨迹的似然计算计算成本高且数值不稳定。需要一种方法将表达能力强的CNF策略与PPO式目标结合，同时避免昂贵的似然计算。

Method: 提出PolicyFlow算法：1）通过沿简单插值路径的速度场变化来近似重要性比率，避免沿完整流路径的似然计算；2）引入布朗正则化器，这是一种受布朗运动启发的隐式策略熵正则化器，防止模式崩溃并鼓励多样化行为。

Result: 在MultiGoal、PointMaze、IsaacLab和MuJoCo Playground等多种环境中的实验表明，PolicyFlow相比使用高斯策略的PPO以及基于流的基线方法（FPO和DPPO）实现了竞争性或更优的性能。特别是在MultiGoal任务中，PolicyFlow能够捕捉更丰富的多模态动作分布。

Conclusion: PolicyFlow成功地将表达能力强的CNF策略与PPO式目标相结合，通过近似重要性比率和布朗正则化器解决了计算效率和训练稳定性问题，为使用高容量策略模型进行强化学习提供了有效解决方案。

Abstract: Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.

</details>


### [255] [Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market](https://arxiv.org/abs/2602.01157)
*Mohammed Osman Gani,Zhipeng He,Chun Ouyang,Sara Khalifa*

Main category: cs.LG

TL;DR: 提出一个基于深度学习的多日前电价预测框架，系统评估了SOTA时间序列模型在澳大利亚电力市场的表现，发现标准DL模型在多数区域表现更优，而SOTA模型对预测时域扩展更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前电价预测面临三大挑战：1) 主要关注日前预测，缺乏多日预测研究；2) 对SOTA时间序列深度学习模型探索不足；3) 现有评估多基于聚合水平，掩盖了日内时间段的预测差异。需要解决这些研究空白以提升电价预测的实用价值。

Method: 提出一个新颖的电价预测框架，将预测时域扩展到多日前。系统构建预测模型，利用经过基准测试的SOTA时间序列深度学习模型。在澳大利亚国家电力市场五个区域进行综合评估，通过整合日内间隔水平的模型评估来分析不同时间段的预测性能。

Result: 结果显示：1) 没有单一模型在所有区域、指标和时域上表现一致最优；2) 标准DL模型在大多数区域表现更优；3) SOTA时间序列DL模型对预测时域扩展更具鲁棒性；4) 日内间隔评估揭示明显的昼夜误差模式：绝对误差在晚间爬坡期达到峰值，相对误差在午间负电价时段膨胀，方向准确性在频繁趋势变化期下降。

Conclusion: 未来基于深度学习的电价预测研究可从两方面受益：1) 丰富特征表示和建模策略以增强长期预测鲁棒性；2) 保持对日内波动和结构性价格动态的敏感性。该研究为多日前电价预测提供了系统框架和深入洞察。

Abstract: Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.

</details>


### [256] [Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01176)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: MF-BPINN：结合物理信息神经网络与贝叶斯不确定性量化的多保真度框架，通过自适应残差学习和分层架构解决高保真PDE计算难题


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）在求解偏微分方程方面表现出色，但求解高保真PDE仍然计算成本高昂，特别是对于需要在不同参数配置下进行多次评估的参数化系统

Method: 提出MF-BPINN多保真度框架，结合PINNs与贝叶斯不确定性量化及自适应残差学习；利用分层神经网络架构学习不同保真度级别间的非线性相关性；引入具有可学习门控机制的自适应残差网络，动态平衡线性和非线性保真度差异；开发基于哈密顿蒙特卡洛的严格贝叶斯框架

Result: 未在摘要中明确说明具体实验结果，但框架设计旨在解决高保真PDE计算的高成本问题

Conclusion: MF-BPINN通过多保真度方法、自适应学习和贝叶斯不确定性量化，为高效求解参数化PDE系统提供了有前景的解决方案

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.

</details>


### [257] [Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective](https://arxiv.org/abs/2602.01179)
*Zhichao Chen,Zhan Zhuang,Yunfei Teng,Hao Wang,Fangyikang Wang,Zhengnan Li,Tianqiao Liu,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出基于熵正则化半对偶不平衡最优传输的渐进域适应框架，通过构造中间域解决真实中间域不可用问题，避免基于似然估计的信息损失。


<details>
  <summary>Details</summary>
Motivation: 渐进域适应需要中间域来缓解域偏移，但真实中间域往往不可用或无效。现有基于流模型的方法通过样本间插值构造中间域，但依赖样本对数似然估计会丢弃有用信息，影响GDA性能。

Method: 提出熵正则化半对偶不平衡最优传输框架：1) 将基于流的GDA重新表述为拉格朗日对偶问题，推导出等效的半对偶目标函数，避免似然估计；2) 引入熵正则化将对偶问题的min-max训练过程转换为更稳定的交替优化过程。

Result: 提供了稳定性和泛化性的理论分析，并通过大量实验验证了E-SUOT框架的有效性。

Conclusion: E-SUOT框架通过最优传输理论有效构造中间域，解决了渐进域适应中真实中间域不可用的问题，避免了基于似然估计的信息损失，提供了更稳定的训练过程。

Abstract: Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models have recently been used for this purpose by interpolating between source and target distributions; however, their training typically relies on sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent semi-dual objective that circumvents the need for likelihood estimation. However, the dual problem leads to an unstable min-max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.

</details>


### [258] [Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective](https://arxiv.org/abs/2602.01182)
*Zhichao Chen,Hao Wang,Fangyikang Wang,Licheng Pan,Zhengnan Li,Yunfei Teng,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出SPIRIT框架，通过半近端传输正则化解决扩散模型在时间序列插补中的非平稳性和目标不一致问题


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列插补中表现不稳定，主要面临两个障碍：1) 非平稳时间动态导致推理轨迹偏差和异常值敏感；2) 目标不一致，插补需要准确点恢复而扩散模型本质生成多样样本

Method: 从近端算子视角分析扩散模型插补过程，发现隐式Wasserstein距离正则化阻碍模型应对非平稳性。提出SPIRIT框架：引入熵诱导Bregman散度松弛Wasserstein距离的质量保持约束，构建半近端传输差异，理论证明其对非平稳性的鲁棒性，移除耗散结构得到完整工作流程

Result: 大量实验证明SPIRIT方法的有效性

Conclusion: 通过半近端传输正则化解决了扩散模型在时间序列插补中的关键问题，提高了插补性能

Abstract: Diffusion models (DMs) have shown promise for Time-Series Data Imputation (TSDI); however, their performance remains inconsistent in complex scenarios. We attribute this to two primary obstacles: (1) non-stationary temporal dynamics, which can bias the inference trajectory and lead to outlier-sensitive imputations; and (2) objective inconsistency, since imputation favors accurate pointwise recovery whereas DMs are inherently trained to generate diverse samples. To better understand these issues, we analyze DM-based TSDI process through a proximal-operator perspective and uncover that an implicit Wasserstein distance regularization inherent in the process hinders the model's ability to counteract non-stationarity and dissipative regularizer, thereby amplifying diversity at the expense of fidelity. Building on this insight, we propose a novel framework called SPIRIT (Semi-Proximal Transport Regularized time-series Imputation). Specifically, we introduce entropy-induced Bregman divergence to relax the mass preserving constraint in the Wasserstein distance, formulate the semi-proximal transport (SPT) discrepancy, and theoretically prove the robustness of SPT against non-stationarity. Subsequently, we remove the dissipative structure and derive the complete SPIRIT workflow, with SPT serving as the proximal operator. Extensive experiments demonstrate the effectiveness of the proposed SPIRIT approach.

</details>


### [259] [The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics](https://arxiv.org/abs/2602.01186)
*Fabio Turazza,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 提出GH-OFL方法族，通过假设预训练嵌入的类条件高斯性，在单轮通信中实现联邦学习，仅传输统计量而非模型，无需公共数据集且保持数据隐私。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习存在多轮通信成本高和隐私风险问题，而现有单轮联邦学习方法要么不实用，要么依赖公共数据集、假设同质客户端模型或需要上传额外数据。需要一种更实用、无数据依赖的单轮联邦学习方案。

Method: 提出高斯头单轮联邦学习(GH-OFL)方法族：客户端仅传输每类统计量（计数、一阶/二阶矩），服务器基于这些统计量构建三种头部：(1)闭式高斯头(NB/LDA/QDA)；(2)FisherMix：在估计的Fisher子空间中生成合成样本训练的余弦边际线性头；(3)Proto-Hyper：通过知识蒸馏在合成样本上精炼高斯logits的轻量级低秩残差头。

Result: GH-OFL方法在强非独立同分布偏斜下实现了最先进的鲁棒性和准确性，同时严格保持无数据特性，显著降低了通信开销和隐私风险。

Conclusion: GH-OFL方法族提供了一种实用、高效的单轮联邦学习解决方案，通过高斯假设和统计量传输，在保持数据隐私的同时实现了优异的性能，为实际部署联邦学习系统提供了可行方案。

Abstract: Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.

</details>


### [260] [Unraveling the Hidden Dynamical Structure in Recurrent Neural Policies](https://arxiv.org/abs/2602.01196)
*Jin Li,Yue Wu,Mengsha Huang,Yuhao Sun,Hao He,Xianyuan Zhan*

Main category: cs.LG

TL;DR: 研究发现循环神经网络策略在与环境交互时会自发形成稳定的极限环结构，这些结构解释了循环策略在泛化和鲁棒性方面的优越性能。


<details>
  <summary>Details</summary>
Motivation: 循环神经网络策略在部分可观测控制和元强化学习任务中表现出色，但其优越泛化能力和鲁棒性的内在机制尚不明确，需要深入研究其内部工作机制。

Method: 通过分析在不同训练方法、模型架构和任务中学习的循环策略的隐藏状态域，研究其与环境交互时的动态特性。

Result: 发现循环策略在环境交互中会自发形成稳定的循环结构，这些结构与动力系统中的极限环相似；极限环的几何形状与策略行为存在结构化对应关系。

Conclusion: 极限环的出现稳定了策略的内部记忆和任务相关环境状态，同时抑制了环境不确定性带来的干扰；极限环的几何结构编码了行为的关系结构，促进了在非平稳环境中的技能适应。

Abstract: Recurrent neural policies are widely used in partially observable control and meta-RL tasks. Their abilities to maintain internal memory and adapt quickly to unseen scenarios have offered them unparalleled performance when compared to non-recurrent counterparts. However, until today, the underlying mechanisms for their superior generalization and robustness performance remain poorly understood. In this study, by analyzing the hidden state domain of recurrent policies learned over a diverse set of training methods, model architectures, and tasks, we find that stable cyclic structures consistently emerge during interaction with the environment. Such cyclic structures share a remarkable similarity with \textit{limit cycles} in dynamical system analysis, if we consider the policy and the environment as a joint hybrid dynamical system. Moreover, we uncover that the geometry of such limit cycles also has a structured correspondence with the policies' behaviors. These findings offer new perspectives to explain many nice properties of recurrent policies: the emergence of limit cycles stabilizes both the policies' internal memory and the task-relevant environmental states, while suppressing nuisance variability arising from environmental uncertainty; the geometry of limit cycles also encodes relational structures of behaviors, facilitating easier skill adaptation when facing non-stationary environments.

</details>


### [261] [SimpleGPT: Improving GPT via A Simple Normalization Strategy](https://arxiv.org/abs/2602.01212)
*Marco Chen,Xianbiao Qi,Yelin He,Jiaquan Ye,Rong Xiao*

Main category: cs.LG

TL;DR: 论文提出SimpleNorm归一化策略，通过稳定中间激活尺度来降低Hessian矩阵的谱范数，从而允许使用更大的学习率，在GPT模型上实现了更好的优化稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 从二阶几何角度重新审视Transformer优化，探索架构设计、激活尺度、Hessian矩阵和最大可容忍学习率之间的直接联系，解决大模型训练中的优化稳定性问题。

Method: 提出SimpleNorm归一化策略，通过构造稳定中间激活尺度；理论分析损失相对于网络激活的Hessian矩阵，证明SimpleNorm能显著降低Hessian的谱范数。

Result: 在1B、1.4B、7B和8B参数的GPT模型上验证，SimpleGPT（基于SimpleNorm的网络）能容忍比标准方法大3-10倍的学习率，优化稳定性强，性能显著优于基线。7B模型训练60K步后，训练损失比LLaMA2+QKNorm低0.08（从2.290降至2.208）。

Conclusion: SimpleNorm通过稳定激活尺度和降低Hessian谱范数，有效提升Transformer优化的稳定性和效率，为大模型训练提供了更可靠的归一化方案。

Abstract: In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\times$-10$\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.

</details>


### [262] [Learning from Anonymized and Incomplete Tabular Data](https://arxiv.org/abs/2602.01217)
*Lucas Lange,Adrian Böttinger,Victor Christen,Anushka Vidanage,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: 论文提出针对用户驱动隐私保护下混合原始、泛化和缺失值的表格数据处理方法，通过新颖的数据转换策略恢复数据效用，优于标准插补和LLM方法


<details>
  <summary>Details</summary>
Motivation: 用户驱动隐私保护导致数据集中混合原始值、泛化值和缺失值，传统机器学习方法将这些非原始值视为新类别或缺失值，丢弃了泛化语义，需要新的处理方法

Method: 提出考虑异构匿名化的新颖数据转换策略，与标准插补方法和基于LLM的方法进行比较，使用多个数据集、隐私配置和部署场景进行评估

Result: 方法能可靠恢复数据效用，泛化值优于纯抑制，最佳数据准备策略取决于具体场景，一致的数据表示对保持下游效用至关重要

Conclusion: 有效学习与适当处理匿名化值密切相关，需要针对用户驱动隐私保护下的混合数据开发专门的处理方法

Abstract: User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.

</details>


### [263] [MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations](https://arxiv.org/abs/2602.01219)
*Qishuai Wen,Zhiyuan Huang,Xianghan Meng,Wei He,Chun-Guang Li*

Main category: cs.LG

TL;DR: 论文提出MiTA注意力机制，通过压缩和路由策略将N宽度MLP压缩为更窄的MLP，使用地标查询和top-k激活键值对构建可变形专家，实现高效注意力计算。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer注意力可视为N宽度MLP，随着序列长度增加，快速权重扩展成本过高。需要开发高效注意力方法来解决长序列处理的计算瓶颈。

Method: 提出压缩-路由策略：1) 使用少量地标查询将N宽度MLP压缩为更窄MLP；2) 为每个地标查询收集top-k激活键值对构建可变形专家；3) 形成Mixture of Top-k Activations (MiTA)注意力机制。

Result: 在视觉任务上的初步实验显示MiTA注意力机制具有潜力，需要进一步优化和在更具挑战性场景中验证。

Conclusion: 将高效注意力方法统一为通过路由和/或压缩扩展快速权重的框架，提出的MiTA注意力机制为长序列处理提供了有前景的解决方案。

Abstract: The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.

</details>


### [264] [Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching](https://arxiv.org/abs/2602.01233)
*Tianhao Miao,Zhongyuan Bao,Lejun Zhang*

Main category: cs.LG

TL;DR: Lotus是一种高效训练方法，通过修改投影过程解决内存消耗、训练时间和模型性能之间的权衡问题，相比GaLore减少30%训练时间和40%内存消耗。


<details>
  <summary>Details</summary>
Motivation: 当前大规模模型训练方法在内存消耗、训练时间和模型性能之间存在权衡，优化一个指标通常会损害其他指标。GaLore虽然能实现内存高效训练，但SVD过程带来了额外的训练时间成本，需要解决这一权衡问题。

Method: Lotus通过修改投影过程，提出一个量化单位梯度位移的准则，实现低秩梯度子空间之间的高效转换，避免昂贵的SVD计算。

Result: 实验结果表明，Lotus是最有效的方法，训练时间减少30%，梯度和优化器状态的内存消耗降低40%，在预训练和微调任务中都优于基线方法。

Conclusion: Lotus成功解决了大规模模型训练中内存消耗、训练时间和模型性能之间的权衡问题，通过简单的投影修改实现了显著效率提升。

Abstract: Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.

</details>


### [265] [Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes](https://arxiv.org/abs/2602.01247)
*Maryam Maghsoudi,Ayushi Mishra*

Main category: cs.LG

TL;DR: 该研究使用机制可解释性方法，通过跨模态激活修补和因果追踪技术，揭示了脑到语音解码模型中不同语音模态（发声、默读、想象）共享连续因果流形，跨模态信息传递由紧凑的层特定子空间而非扩散活动介导。


<details>
  <summary>Details</summary>
Motivation: 虽然脑到语音解码模型在不同语音模态（发声、默读、想象）中表现出稳健性能，但这些模型如何在不同语音模态间捕获和传递信息的基本机制尚不明确。研究旨在从因果角度探究神经语音解码器的内部表征机制。

Method: 采用机制可解释性方法：1）跨模态激活修补分析内部激活；2）三模态插值检验语音表征是离散还是连续变化；3）粗到细因果追踪和因果擦除以定位因果结构；4）神经元级激活修补确定效应在层内的分布精细程度。

Result: 发现：1）不同语音模态位于共享的连续因果流形上；2）跨模态传递由紧凑的层特定子空间介导，而非扩散活动；3）影响跨模态传递的是小而非分布的子集神经元，而非孤立单元；4）揭示了跨语音模态的层次性和方向依赖性表征结构。

Conclusion: 研究为脑到语音解码模型中语音模态信息的组织和利用方式提供了因果解释，揭示了跨语音模态的层次性和方向依赖性表征结构，表明不同语音模态共享连续因果流形，跨模态信息传递由紧凑的层特定子空间介导。

Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.

</details>


### [266] [Sample Efficient Active Algorithms for Offline Reinforcement Learning](https://arxiv.org/abs/2602.01260)
*Soumyadeep Roy,Shashwat Kushwaha,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 该论文提出了一种主动强化学习（ActiveRL）方法，通过有限在线交互选择性地优化价值函数的不确定区域，实现了比纯离线方法更优的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习存在状态-动作空间覆盖不足和分布偏移问题。虽然通过有限在线交互选择性地优化不确定区域（ActiveRL）在实证上取得了成功，但缺乏理论分析。

Method: 提出基于高斯过程（GP）不确定性建模的ActiveRL算法，利用GP浓度不等式和信息增益界限进行理论分析，通过主动交互减少价值函数的不确定性。

Result: 理论证明ActiveRL能以O(1/ε²)的主动转移学习到ε-最优策略，优于纯离线方法的Ω(1/ε²(1-γ)⁴)速率，实现了接近最优的信息效率。

Conclusion: ActiveRL通过引导不确定性减少实现了价值函数的加速收敛，以最小的在线数据需求达到了理论上的样本复杂度优势，连接了贝叶斯非参数回归和强化学习理论。

Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.

</details>


### [267] [BicKD: Bilateral Contrastive Knowledge Distillation](https://arxiv.org/abs/2602.01265)
*Jiangnan Zhu,Yukai Xu,Li Xiong,Yixuan Liu,Junxu Liu,Hong kyu Lee,Yujie Gu*

Main category: cs.LG

TL;DR: 提出双边对比知识蒸馏(BicKD)，通过双边对比损失增强类间正交性并保持类内一致性，改进传统KD的样本级对齐缺陷


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏(KD)仅进行样本级概率对齐，缺乏类级比较机制，且对概率空间没有结构性约束，限制了知识传递效果

Method: 提出双边对比知识蒸馏(BicKD)，引入双边对比损失，增强不同类泛化空间的正交性，同时保持同类一致性，实现样本级和类级预测模式的显式比较

Result: BicKD在各种模型架构和基准测试中持续优于最先进的知识蒸馏技术，显著提升了知识传递效果

Conclusion: 双边对比知识蒸馏通过强调概率正交性来正则化预测分布的几何结构，为知识蒸馏提供了更有效的框架

Abstract: Knowledge distillation (KD) is a machine learning framework that transfers knowledge from a teacher model to a student model. The vanilla KD proposed by Hinton et al. has been the dominant approach in logit-based distillation and demonstrates compelling performance. However, it only performs sample-wise probability alignment between teacher and student's predictions, lacking an mechanism for class-wise comparison. Besides, vanilla KD imposes no structural constraint on the probability space. In this work, we propose a simple yet effective methodology, bilateral contrastive knowledge distillation (BicKD). This approach introduces a novel bilateral contrastive loss, which intensifies the orthogonality among different class generalization spaces while preserving consistency within the same class. The bilateral formulation enables explicit comparison of both sample-wise and class-wise prediction patterns between teacher and student. By emphasizing probabilistic orthogonality, BicKD further regularizes the geometric structure of the predictive distribution. Extensive experiments show that our BicKD method enhances knowledge transfer, and consistently outperforms state-of-the-art knowledge distillation techniques across various model architectures and benchmarks.

</details>


### [268] [Diving into Kronecker Adapters: Component Design Matters](https://arxiv.org/abs/2602.01267)
*Jiayu Bai,Danchen Yu,Zhenyu Liao,TianQi Hou,Feng Zhou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: CDKA提出通过精细设计Kronecker适配器的组件结构（维度和数量）来提升性能，并提供参数预算感知的配置指南和训练稳定策略。


<details>
  <summary>Details</summary>
Motivation: 现有Kronecker适配器方法大多将组件结构视为固定或启发式设计选择，对组件维度和数量的探索不足，而组件结构是影响Kronecker适配器能力的关键因素。

Method: 提出Component Designed Kronecker Adapters (CDKA)，对Kronecker组件的维度和数量进行细粒度分析，提供参数预算感知的配置指南，并设计专门的训练稳定策略。

Result: 在多个自然语言处理任务上的实验证明了CDKA的有效性，代码已开源。

Conclusion: 组件结构设计对Kronecker适配器性能至关重要，CDKA通过系统化设计组件配置实现了更好的效果。

Abstract: Kronecker adapters have emerged as a promising approach for fine-tuning large-scale models, enabling high-rank updates through tunable component structures. However, existing work largely treats the component structure as a fixed or heuristic design choice, leaving the dimensions and number of Kronecker components underexplored. In this paper, we identify component structure as a key factor governing the capacity of Kronecker adapters. We perform a fine-grained analysis of both the dimensions and number of Kronecker components. In particular, we show that the alignment between Kronecker adapters and full fine-tuning depends on component configurations. Guided by these insights, we propose Component Designed Kronecker Adapters (CDKA). We further provide parameter-budget-aware configuration guidelines and a tailored training stabilization strategy for practical deployment. Experiments across various natural language processing tasks demonstrate the effectiveness of CDKA. Code is available at https://github.com/rainstonee/CDKA.

</details>


### [269] [Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics](https://arxiv.org/abs/2602.01270)
*Boxuan Zhang,Weipu Zhang,Zhaohan Feng,Wei Xiao,Jian Sun,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: MoW是一种用于多任务强化学习的混合世界模型架构，通过模块化视觉压缩、任务条件化专家和梯度聚类策略，在Atari和Meta-World基准上实现了参数高效且性能优异的通用世界模型。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习在视觉领域中面临样本效率低下的挑战，特别是当任务在观察和动态特性上存在显著异质性时。传统的单体世界模型架构难以捕捉多样化的任务动态，导致重建和预测准确性差。

Method: 提出了混合世界模型（MoW）架构，包含：1）用于任务自适应视觉压缩的模块化变分自编码器；2）具有任务条件化专家和共享骨干的混合Transformer动态模型；3）基于梯度的任务聚类策略，用于高效参数分配。

Result: 在Atari 100k基准测试中，单个MoW代理在26个Atari游戏上训练后获得110.4%的平均人类标准化分数，与使用26个任务特定模型的STORM（114.2%）相当，但参数减少50%。在Meta-World上，MoW在30万环境步内达到74.5%的平均成功率，创下新纪录。

Conclusion: MoW为通用世界模型提供了一个可扩展且参数高效的基础，能够有效处理多任务强化学习中视觉领域的异质性挑战，在保持高性能的同时显著减少参数需求。

Abstract: A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.

</details>


### [270] [From Intents to Actions: Agentic AI in Autonomous Networks](https://arxiv.org/abs/2602.01271)
*Burak Demirel,Pablo Soldati,Yu Wang*

Main category: cs.LG

TL;DR: 提出基于三个专门智能体的AI系统，用于意图驱动的自治网络，通过语言模型解析意图、优化器处理权衡、强化学习控制器执行，实现网络自主运行。


<details>
  <summary>Details</summary>
Motivation: 电信网络需要自主运行并支持具有多样化且常冲突意图的异构服务，但现有启发式方法无法将高层意图（如超低延迟、高吞吐量）转化为具体控制动作。

Method: 构建三个专门智能体系统：1) 监督解释器智能体（基于语言模型）进行意图的词法解析和认知细化；2) 优化器智能体将模板转化为可处理的优化问题并分析权衡；3) 偏好驱动控制器智能体（基于多目标强化学习）利用偏好操作网络接近帕累托前沿。

Result: 该系统使网络能够以可扩展的方式自主解释、推理、适应并响应多样化意图和网络条件，实现意图驱动的自治网络运行。

Conclusion: 提出的智能体AI系统通过三个专门智能体的协同工作，解决了意图到控制动作的转化难题，为电信网络提供了可扩展的自主运行解决方案。

Abstract: Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.

</details>


### [271] [Richer Bayesian Last Layers with Subsampled NTK Features](https://arxiv.org/abs/2602.01279)
*Sergio Calvo-Ordoñez,Jonathan Plenk,Richard Bergna,Álvaro Cartea,Yarin Gal,Jose Miguel Hernández-Lobato,Kamil Ciosek*

Main category: cs.LG

TL;DR: 提出一种改进贝叶斯最后一层的方法，通过将神经正切核特征投影到最后一层特征空间，以更准确估计认知不确定性，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯最后一层虽然计算高效，但低估了认知不确定性，因为它只在最后一层应用贝叶斯处理，忽略了前面层引入的不确定性。

Method: 利用神经正切核特征投影到最后一层特征空间，使后验推断能考虑整个网络的变异性，同时保持标准BLL的低计算成本。还引入均匀子采样方案来估计投影矩阵和进行后验推断。

Result: 方法产生的后验方差在理论上大于或等于标准BLL，纠正了其低估认知不确定性的趋势。在UCI回归、上下文老虎机、图像分类以及图像和表格数据集的分布外检测任务中，表现出比标准BLL和竞争基线更好的校准和不确定性估计，同时降低了计算成本。

Conclusion: 提出的方法通过结合NTK特征投影，显著改善了贝叶斯最后一层的认知不确定性估计能力，在保持计算效率的同时提供了更可靠的不确定性量化。

Abstract: Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.

</details>


### [272] [EDIS: Diagnosing LLM Reasoning via Entropy Dynamics](https://arxiv.org/abs/2602.01288)
*Chenghua Zhu,Siyan Wu,Xiangkang Zeng,Zishan Xu,Zhaolu Kang,Yifu Guo,Yuquan Lu,Junduan Huang,Guojing Zhou*

Main category: cs.LG

TL;DR: 该论文发现语言模型生成过程中熵的动态变化比静态聚合统计量包含更丰富的信息，通过分析熵轨迹可以区分正确与错误推理，并提出了EDIS指标来量化这种不稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将置信度视为静态量（通常聚合在token级别），但生成过程中置信度的时序演化可能包含更丰富的信息。作者旨在探索熵的动态变化是否能提供更好的推理质量信号。

Method: 分析token级熵轨迹，识别正确与错误推理的特征模式；引入熵动态不稳定性评分（EDIS）来量化熵演化中的不稳定性；将EDIS应用于推理时选择和训练时样本筛选。

Result: 错误解决方案表现出不稳定动态，包括爆发性尖峰（持续不确定性增长）和峰谷尖峰（短暂置信后急剧反弹）；这些模式在不同模型和训练阶段持续存在；EDIS作为有效的诊断信号，显著提高了推理准确性。

Conclusion: 熵动态是理解和改进LLM推理的一个未被充分探索但信息丰富的视角，EDIS为推理时选择和训练时样本筛选提供了有前景的方向。

Abstract: Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.

</details>


### [273] [Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models](https://arxiv.org/abs/2602.01289)
*Dung Anh Hoang,Cuong Pham anh Trung Le,Jianfei Cai,Toan Do*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的扩散模型后训练量化方法，通过学习为校准样本分配最优权重来对齐不同时间步的梯度，解决了现有方法中均匀权重分配和统一量化策略的次优问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在实际部署中面临推理速度慢、内存占用高和计算需求大的问题。后训练量化是加速采样和减少内存开销的有前景方案，但现有方法在不同时间步使用均匀权重分配校准样本，这并不理想，因为不同时间步的数据对扩散过程的贡献不同，且激活分布和梯度变化使得统一量化策略效果不佳。

Method: 提出了一种新颖的后训练量化方法，通过学习为校准样本分配最优权重来对齐量化模型在不同时间步的梯度。该方法考虑了不同时间步对扩散过程的不同贡献，以及激活分布和梯度的变化，避免了均匀权重分配导致的梯度冲突问题。

Result: 在CIFAR-10、LSUN-Bedrooms和ImageNet数据集上的大量实验表明，该方法相比其他扩散模型后训练量化方法具有优越性。

Conclusion: 通过学习为校准样本分配最优权重来对齐不同时间步的梯度，可以有效提升扩散模型后训练量化的性能，解决了现有方法中的次优问题，为扩散模型的实际部署提供了更有效的量化方案。

Abstract: Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.

</details>


### [274] [The BoBW Algorithms for Heavy-Tailed MDPs](https://arxiv.org/abs/2602.01295)
*Yu Chen,Yuhao Liu,Jiatai Huang,Yihan Du,Longbo Huang*

Main category: cs.LG

TL;DR: 本文提出两种算法HT-FTRL-OM和HT-FTRL-UOB，用于处理具有重尾反馈的马尔可夫决策过程，实现了"两全其美"的遗憾保证：在对抗环境中实现与实例无关的遗憾，在自约束（包括随机）环境中实现对数依赖的遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有处理重尾反馈MDP的方法在随机环境中过于保守，在对抗环境中缺乏适应性。需要开发既能适应对抗环境又能利用随机环境结构的算法。

Method: HT-FTRL-OM：在已知转移概率设置下，在占用度量上应用FTRL框架，采用新颖的跳过损失估计器。HT-FTRL-UOB：针对未知转移概率设置，采用悲观跳过损失估计器，处理转移不确定性和重尾估计误差。

Result: HT-FTRL-OM在对抗环境中达到$\widetilde{\mathcal{O}}(T^{1/α})$遗憾，在随机环境中达到$\mathcal{O}(\log T)$遗憾。HT-FTRL-UOB在对抗环境中达到$\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$遗憾，在随机环境中达到$\mathcal{O}(\log^2(T))$遗憾。

Conclusion: 本文提出的算法在重尾反馈MDP中实现了两全其美的遗憾保证，通过技术创新克服了重尾损失估计、转移不确定性和跳过偏差等关键挑战。

Abstract: We investigate episodic Markov Decision Processes with heavy-tailed feedback (HTMDPs). Existing approaches for HTMDPs are conservative in stochastic environments and lack adaptivity in adversarial regimes. In this work, we propose algorithms ```HT-FTRL-OM``` and ```HT-FTRL-UOB``` for HTMDPs that achieve Best-of-Both-Worlds (BoBW) guarantees: instance-independent regret in adversarial environments and logarithmic instance-dependent regret in self-bounding (including the stochastic case) environments. For the known transition setting, ```HT-FTRL-OM``` applies the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with novel skipping loss estimators, achieving a $\widetilde{\mathcal{O}}(T^{1/α})$ regret bound in adversarial regimes and a $\mathcal{O}(\log T)$ regret in stochastic regimes. Building upon this framework, we develop a novel algorithm ```HT-FTRL-UOB``` to tackle the more challenging unknown-transition setting. This algorithm employs a pessimistic skipping loss estimator and achieves a $\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$ regret in adversarial regimes and a $\mathcal{O}(\log^2(T))$ regret in stochastic regimes. Our analysis overcomes key barriers through several technical insights, including a local control mechanism for heavy-tailed shifted losses, a new suboptimal-mass propagation principle, and a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors and skipping bias.

</details>


### [275] [Dispelling the Curse of Singularities in Neural Network Optimizations](https://arxiv.org/abs/2602.01308)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Fang Dong,Ruijun Huang,Anrui Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Dongsheng Li,Wenyi Fang,Yuanyi Lin,Fan Wu,Li Shang*

Main category: cs.LG

TL;DR: 论文从参数空间奇异值角度研究深度神经网络优化不稳定性，提出参数奇异值平滑方法缓解训练不稳定问题


<details>
  <summary>Details</summary>
Motivation: 从参数空间奇异值出现和放大的新视角研究深度神经网络优化不稳定性问题，传统方法对此关注较少

Method: 提出参数奇异值平滑方法，通过平滑权重矩阵的奇异谱来缓解奇异值增长问题

Result: PSS方法能有效缓解训练不稳定，恢复失败后的可训练性，并提高训练效率和泛化能力

Conclusion: 参数奇异值增长是深度神经网络优化不稳定的重要原因，PSS方法为缓解这一问题提供了轻量有效的解决方案

Abstract: This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.

</details>


### [276] [Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution](https://arxiv.org/abs/2602.01312)
*Han Tong,Shubhangi Ghosh,Haolin Zou,Arian Maleki*

Main category: cs.LG

TL;DR: TRAK算法用于数据归因，但缺乏理论分析。本文首次对TRAK进行理论分析，发现其近似误差虽大，但能保持数据点相对排序的准确性。


<details>
  <summary>Details</summary>
Motivation: TRAK算法在数据归因方面表现良好，但其理论条件、准确性和失效机制尚未得到充分研究，需要理论分析来理解其性能边界。

Method: 通过理论分析TRAK算法，量化其近似误差，并验证估计影响力与原始影响力的相关性，同时通过大量模拟和实证研究进行验证。

Result: 尽管TRAK的近似引入显著误差，但其估计的影响力与原始影响力高度相关，能有效保持数据点的相对排序。

Conclusion: TRAK算法虽然存在近似误差，但在实际应用中能有效进行数据归因，保持数据点相对排序的准确性，为理解其性能边界提供了理论基础。

Abstract: Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.

</details>


### [277] [PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding](https://arxiv.org/abs/2602.01322)
*Panagiotis Koromilas,Andreas D. Demou,James Oldfield,Yannis Panagakis,Mihalis Nicolaou*

Main category: cs.LG

TL;DR: PolySAE扩展了稀疏自编码器，通过高阶多项式项建模特征交互，解决了传统SAE无法捕捉组合结构的问题，在保持线性编码器可解释性的同时显著提升了组合特征的表示能力。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器(SAE)假设特征通过线性重构相加，无法区分组合结构（如"Starbucks"是"star"和"coffee"的组合还是仅共现），这迫使SAE为复合概念分配整体特征而非分解为可解释的组成部分。

Method: PolySAE在SAE解码器中引入高阶项来建模特征交互，同时保持线性编码器以确保可解释性。通过共享投影子空间上的低秩张量分解，以较小的参数开销（GPT2上3%）捕捉特征间的成对和三元交互。

Result: 在四个语言模型和三种SAE变体上，PolySAE平均提升约8%的探测F1分数，同时保持可比较的重构误差，并产生2-10倍大的类别条件特征分布Wasserstein距离。学习到的交互权重与共现频率相关性极低（r=0.06 vs SAE特征协方差的r=0.82）。

Conclusion: 多项式项能够捕捉形态绑定和短语组合等组合结构，这些结构在很大程度上独立于表面统计特征，为神经网络的组合性解释提供了更强大的工具。

Abstract: Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.

</details>


### [278] [Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization](https://arxiv.org/abs/2602.01339)
*Difei Xu,Youming Tao,Meng Ding,Chenglin Fan,Di Wang*

Main category: cs.LG

TL;DR: 首次研究随机非凸极小极大优化中寻找差分隐私二阶平稳点的问题，提出结合嵌套梯度下降-上升、SPIDER方差缩减和高斯扰动的一阶方法，在经验风险和总体风险下均获得最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有文献要么只关注极小极大问题的一阶平稳点，要么只关注经典随机最小化问题的二阶平稳点。本文首次统一处理经验风险和总体风险，填补了差分隐私下随机极小极大优化二阶平稳点研究的空白。

Method: 提出纯一阶方法，结合嵌套梯度下降-上升方案、SPIDER风格方差缩减和高斯扰动来确保隐私。关键技术是块状(q-周期)分析，控制随机方差和隐私噪声的累积，无需对整个迭代范围求和。

Result: 在标准光滑性、Hessian-Lipschitz性和强凹性假设下，建立了高概率保证：对于经验风险目标达到(α,√(ρ_Φα))-近似二阶平稳点，α=O((√d/nε)^{2/3})；对于总体目标达到O(1/n^{1/3} + (√d/nε)^{1/2})，匹配私有一阶平稳性的最佳已知速率。

Conclusion: 首次为随机极小极大优化提供了差分隐私二阶平稳点的统一分析框架，提出的方法在经验风险和总体风险下均达到最优收敛速率，填补了该领域的研究空白。

Abstract: We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(α,\sqrt{ρ_Φα})$-approximate second-order stationary point with $α= \mathcal{O}( (\frac{\sqrt{d}}{n\varepsilon})^{2/3})$ for empirical risk objectives and $\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.

</details>


### [279] [Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning](https://arxiv.org/abs/2602.01357)
*Shangzhe Li,Xuchao Zhang,Chetan Bansal,Weitong Zhang*

Main category: cs.LG

TL;DR: 该论文将自博弈微调与对抗模仿学习联系起来，通过构建模型与正则化隐式奖励玩家之间的min-max博弈框架，提出基于χ²散度的稳定自博弈模仿微调算法，在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自博弈后训练方法已成为微调大语言模型的有效途径，能将弱语言模型转变为强语言模型而无需偏好数据，但其理论基础尚未充分探索。

Method: 将微调过程形式化为模型与由模型本身参数化的正则化隐式奖励玩家之间的min-max博弈，提出基于χ²散度变分目标的有界奖励自博弈模仿微调算法，具有更好的稳定性。

Result: 理论分析表明自博弈微调将收敛到均衡点；实验证明在多种语言模型微调任务上，新方法相比现有自博弈方法有持续改进，验证了理论洞察。

Conclusion: 该工作通过博弈论视角统一了自博弈模仿和一般偏好对齐，为自博弈微调提供了理论基础，并提出了更稳定的算法实现。

Abstract: Self-play post-training methods has emerged as an effective approach for finetuning large language models and turn the weak language model into strong language model without preference data. However, the theoretical foundations for self-play finetuning remain underexplored. In this work, we tackle this by connecting self-play finetuning with adversarial imitation learning by formulating finetuning procedure as a min-max game between the model and a regularized implicit reward player parameterized by the model itself. This perspective unifies self-play imitation and general preference alignment within a common framework. Under this formulation, we present a game-theoretic analysis showing that the self-play finetuning will converge to it's equilibrium. Guided by this theoretical formulation, we propose a new self-play imitation finetuning algorithm based on the $χ^2$-divergence variational objective with bounded rewards and improved stability. Experiments on various of language model finetuning tasks demonstrate consistent improvements over existing self-play methods and validate our theoretical insights.

</details>


### [280] [PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.01359)
*Jinju Park,Seokho Kang*

Main category: cs.LG

TL;DR: PaAno：一种基于补丁表示的轻量级时间序列异常检测方法，使用1D CNN提取时间补丁特征，结合三元组损失和预文本损失训练，在TSB-AD基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法过度依赖大型神经网络架构（如Transformer和基础模型），导致计算成本高、内存占用大，不适用于实时和资源受限场景，且性能提升有限。

Method: 从时间序列训练数据中提取短时间补丁，使用1D卷积神经网络将每个补丁嵌入为向量表示。通过结合三元组损失和预文本损失训练模型，确保嵌入捕获输入补丁的信息性时间模式。推理时，通过比较当前时间步周围补丁的嵌入与训练时间序列中正常补丁的嵌入来计算异常分数。

Result: 在TSB-AD基准测试中，PaAno实现了最先进的性能，在单变量和多变量时间序列异常检测的各种范围性和点性性能指标上，显著优于现有方法（包括基于重型架构的方法）。

Conclusion: PaAno提供了一种轻量级但有效的解决方案，能够在保持高性能的同时显著降低计算成本和内存使用，适用于实时和资源受限的时间序列异常检测场景。

Abstract: Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. Moreover, they often fail to demonstrate significant performance gains over simpler methods under rigorous evaluation protocols. In this study, we propose Patch-based representation learning for time-series Anomaly detection (PaAno), a lightweight yet effective method for fast and efficient time-series anomaly detection. PaAno extracts short temporal patches from time-series training data and uses a 1D convolutional neural network to embed each patch into a vector representation. The model is trained using a combination of triplet loss and pretext loss to ensure the embeddings capture informative temporal patterns from input patches. During inference, the anomaly score at each time step is computed by comparing the embeddings of its surrounding patches to those of normal patches extracted from the training time-series. Evaluated on the TSB-AD benchmark, PaAno achieved state-of-the-art performance, significantly outperforming existing methods, including those based on heavy architectures, on both univariate and multivariate time-series anomaly detection across various range-wise and point-wise performance measures.

</details>


### [281] [When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning](https://arxiv.org/abs/2602.01365)
*Wang Yang,Shouren Wang,Chaoda Song,Chuang Ma,Xinpeng Li,Nengbo Wang,Kaixiong Zhou,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: GRPO在不同领域排序策略下的行为分析：单领域泛化具有不对称性，跨领域交互高度依赖顺序，多领域训练中无单一最优策略。


<details>
  <summary>Details</summary>
Motivation: GRPO已成为提升大语言模型推理能力的关键技术，但其在不同领域排序策略下的行为尚不清楚。特别是顺序训练（一次一个领域）与混合领域训练（多个领域同时）在GRPO中的影响尚未得到系统研究。

Method: 对数学、科学、逻辑和谜题推理任务中的训练顺序效应进行首次系统分析，比较顺序训练与混合领域训练策略。

Result: 发现三个关键结果：1）单领域泛化高度不对称：在其他领域训练可将数学推理准确率提升约25%，但对逻辑和谜题推理几乎无转移效果；2）跨领域交互高度依赖顺序：数学→科学顺序训练在数学/科学上达到83%/41%准确率，而科学→数学顺序则降至77%/25%；3）多领域训练中无单一最优策略：顺序训练有利于数学（最高84%），混合训练有利于科学和逻辑，不良排序会导致大性能差距（从70%降至56%）。

Conclusion: GRPO在多领域设置下表现出明显的不对称性、顺序敏感性和策略依赖性，强调了领域感知和顺序感知训练设计的必要性。

Abstract: Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\rightarrow$science achieves 83\% / 41\% accuracy on math / science, while reversing the order to science$\rightarrow$math degrades performance to 77\% / 25\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\% to 56\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.

</details>


### [282] [Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation](https://arxiv.org/abs/2602.01367)
*Pinar Erbil,Alberto Archetti,Eugenio Lomurno,Matteo Matteucci*

Main category: cs.LG

TL;DR: CONVERSE是一个结合变分自编码器和对比学习的深度生存模型，在保持高预测性能的同时实现可解释的风险分层，解决了深度生存分析中性能与可解释性的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要生存分析来估计时间到事件结果、分层患者风险并指导治疗规划。深度学习虽然具有强大的预测能力，但其黑盒性质限制了临床采用。而基于深度聚类的方法虽然可解释，但通常牺牲了预测性能。需要一种既能保持高预测性能又能提供可解释风险分层的解决方案。

Method: CONVERSE结合变分自编码器与对比学习，使用变分嵌入和多种簇内、簇间对比损失。采用自步学习从易到难逐步纳入样本以提高训练稳定性。模型支持簇特定的生存头，实现准确的集成预测。

Result: 在四个基准数据集上的综合评估表明，CONVERSE相比现有深度生存方法实现了竞争性或更优的性能，同时保持了有意义的患者分层。

Conclusion: CONVERSE成功解决了深度生存分析中性能与可解释性的权衡问题，通过统一变分自编码器和对比学习，实现了既准确又可解释的风险分层，有望促进临床采用。

Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.

</details>


### [283] [SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training](https://arxiv.org/abs/2602.01410)
*Yunjie Pan,Yongyi Yang,Hanmei Yang,Scott Mahlke*

Main category: cs.LG

TL;DR: SNIP是一个用于LLM预训练的细粒度自适应混合精度训练框架，通过收集统计信息并定义前向损失发散和后向权重发散两个关键指标，使用整数线性规划优化逐层精度，在保持模型质量的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 当前混合精度训练方法要么对所有GEMM操作使用统一精度，要么依赖基于启发式的方法，这些方法在训练过程中无法泛化，导致收敛次优和不稳定。需要一种能够支持子字节精度、自适应优化精度配置的框架。

Method: SNIP定期收集激活、梯度和优化器状态的统计信息，定义前向损失发散（量化引起的训练损失增加）和后向权重发散（梯度误差传播影响模型更新）两个指标。基于这些指标构建整数线性规划问题，系统优化逐层精度配置，在满足效率目标的同时最小化质量损失。

Result: 在1B、3B、7B和70B Llama-like模型上的实验表明，SNIP始终优于现有基线方法，在保持模型质量的同时将FLOPs减少高达80%，且在不同模型规模和训练阶段都能保持稳定，计算开销最小。

Conclusion: SNIP通过细粒度的自适应混合精度训练框架，有效解决了LLM预训练中的效率与质量平衡问题，支持子字节精度，为大规模语言模型的高效训练提供了实用解决方案。

Abstract: Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.

</details>


### [284] [Semi-supervised CAPP Transformer Learning via Pseudo-labeling](https://arxiv.org/abs/2602.01419)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb,Emmanuel Stathatos,Panorios Benardos,George-Christopher Vosniakos*

Main category: cs.LG

TL;DR: 提出一种半监督学习方法，利用预训练的oracle模型筛选正确预测，通过一次性重训练提升变压器模型在数据稀缺的计算机辅助工艺规划任务中的性能


<details>
  <summary>Details</summary>
Motivation: 工业中计算机辅助工艺规划（CAPP）面临数据集有限的问题，导致模型泛化能力不足。需要一种方法在缺乏手动标注的情况下提升模型性能

Method: 采用半监督学习框架：1）在可用变压器行为数据上训练一个oracle模型；2）用oracle筛选未见零件的正确预测；3）使用筛选出的数据进行一次性重训练

Result: 在小规模数据集上的实验表明，该方法在完整数据分布上相比基线模型获得了一致的准确率提升，验证了在数据稀缺制造环境中的有效性

Conclusion: 提出的半监督学习方法能够有效提升变压器模型在数据稀缺的CAPP任务中的性能，无需手动标注，为制造业中的AI应用提供了实用解决方案

Abstract: High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.

</details>


### [285] [Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models](https://arxiv.org/abs/2602.01428)
*Weiqing He,Xiang Li,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，在保持推测采样效率的同时实现最大水印强度，解决了水印强度与推测采样接受率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前水印技术在实际部署中存在推理效率低的问题，而推测采样虽然能加速推理，但水印强度与接受率之间存在根本性权衡，阻碍了两者的同时实现。

Method: 1) 引入量化水印强度度量；2) 将权衡问题形式化为约束优化问题并推导帕累托曲线；3) 提出在草稿令牌接受中注入伪随机性的机制，确保最大水印强度同时保持推测采样效率。

Result: 实验表明该方法在不牺牲效率的情况下提高了可检测性，揭示了推测采样与水印之间的统一原则。

Conclusion: 本文提出的方法解决了水印强度与推测采样效率之间的权衡问题，为两者的高效实用部署铺平了道路。

Abstract: Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.

</details>


### [286] [Phase Transitions for Feature Learning in Neural Networks](https://arxiv.org/abs/2602.01434)
*Andrea Montanari,Zihao Wang*

Main category: cs.LG

TL;DR: 该论文研究两层神经网络在比例渐近条件下学习多索引模型的梯度下降动力学，确定了特征学习的阈值δ_NN，该阈值对应于Hessian矩阵谱的相变。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何通过梯度下降学习低维表示，特别是理解在比例渐近条件下特征学习的动力学机制和阈值条件。

Method: 采用比例渐近分析（n,d→∞，n/d→δ），保持潜在空间维度k和隐藏神经元数m固定，研究两层神经网络的梯度下降动力学。

Result: 确定了特征学习的阈值δ_NN，该阈值对应于训练过程中Hessian矩阵谱的相变：第一阶段梯度大时学习梯度方向，第二阶段梯度变小时Hessian负方向主导。

Conclusion: δ_NN阈值的表征为研究网络架构和训练算法对学习动力学的影响提供了基础，揭示了神经网络特征学习的两阶段机制。

Abstract: According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\boldsymbol x}_i,y_i)$, where the covariate vectors ${\boldsymbol x}_i\in\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\boldsymbol x}_i$ through a $k$-dimensional projection ${\boldsymbol Θ}_*^{\sf T}{\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\boldsymbol Θ}_*$.
  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\to\infty$, $n/d\toδ$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $δ> δ_{\text{alg}}$, for $δ_{\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $δ_{\text{alg}}$. Here we derive an analogous threshold $δ_{\text{NN}}$ for two-layer networks. Our characterization of $δ_{\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.
  The threshold $δ_{\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $δ_{\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.

</details>


### [287] [TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse](https://arxiv.org/abs/2602.01439)
*Perry Dong,Kuo-Han Hung,Alexander Swerdlow,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

TL;DR: Transformer Q-Learning (TQL) 通过控制注意力熵来稳定训练，解决了Transformer在强化学习价值函数中扩展时注意力分数崩溃的问题，实现了从最小到最大网络规模43%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管规模在机器学习中推动了显著进步，但强化学习方法仍主要使用小型价值函数。朴素地扩展价值函数（包括使用已知高度可扩展的Transformer架构）通常会导致学习不稳定和性能下降。本研究探讨了是什么阻止了Transformer在价值函数中有效扩展。

Method: 通过实证分析识别了扩展中的关键失败模式：注意力分数随着容量增加而崩溃。关键洞见是通过控制注意力分数的熵可以有效防止这种崩溃并稳定训练。为此提出了Transformer Q-Learning (TQL)方法。

Result: TQL方法在从最小到最大网络规模扩展时实现了高达43%的性能提升，而先前方法则遭受性能下降。

Conclusion: 通过控制注意力熵，Transformer Q-Learning成功解锁了Transformer在强化学习价值函数中的扩展潜力，解决了注意力分数崩溃的问题，使大规模Transformer在RL中成为可能。

Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.

</details>


### [288] [The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks](https://arxiv.org/abs/2602.01442)
*Donald Ye*

Main category: cs.LG

TL;DR: 研究发现梯度大小与因果重要性在简单任务中相关，但在复杂任务中关系崩溃甚至反转，导致基于梯度的剪枝不可靠


<details>
  <summary>Details</summary>
Motivation: 探索神经网络中梯度大小与组件因果重要性之间的关系，揭示基于梯度的剪枝方法的局限性

Method: 在Transformer模型上训练算法任务，分析梯度大小与因果重要性的相关性，并进行剪枝实验验证

Result: 简单任务中梯度与重要性正相关（ρ=0.73），复杂任务中相关性崩溃（ρ=0.32）甚至反转（ρ=-0.11）；剪除低梯度"隐藏英雄"严重损害OOD性能（-32%），剪除高梯度"梯度膨胀"结果不可预测

Conclusion: 梯度大小不能可靠反映组件的因果重要性，基于梯度的剪枝方法无法稳定保留模型能力，存在不可预测的风险

Abstract: Removing ''important'' high-gradient components from a neural network can improve generalization, while removing unimportant'' low-gradient components can destroy it. We demonstrate this paradox by formalizing the \textit{Gradient-Causal Gap} in Transformers trained on algorithmic tasks. While gradient magnitude and causal importance align on simple tasks ($ρ=0.73$ for reversal), this relationship collapses as task complexity increases ($ρ=0.32$ for sorting), sometimes becoming inverted ($ρ=-0.11$). Pruning experiments reveal that gradient magnitude is not merely inaccurate but \textit{unpredictably} so. Removing low-gradient ''Hidden Heroes'' consistently devastates OOD accuracy ($-32\%$). Removing high-gradient ''Gradient Bloats'' is a coin flip: harmless in most seeds (indicating optimization noise), catastrophic in others (indicating overfitting circuits). This unpredictability means gradient-based pruning cannot reliably preserve model capabilities.

</details>


### [289] [A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting](https://arxiv.org/abs/2602.01445)
*Ons Saadallah,Mátyás andó,Tamás Gábor Orosz*

Main category: cs.LG

TL;DR: LLM-AutoOpt：结合贝叶斯优化与LLM推理的混合超参数优化框架，用于时间序列预测，提高性能并提供可解释性


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在超参数调优中存在计算成本高、可解释性差的问题，特别是在时间序列预测任务中。虽然贝叶斯优化是标准方法，但通常独立处理调优任务且决策过程不透明。大型语言模型的发展为将结构化先验知识和推理融入优化流程提供了新机会。

Method: 提出LLM-AutoOpt混合框架，结合贝叶斯优化和基于LLM的上下文推理。框架将数据集元特征、模型描述、历史优化结果和目标目标编码为LLM提示中的结构化元知识，使用贝叶斯优化初始化搜索并缓解冷启动效应，实现上下文感知的稳定超参数优化。

Result: 在多变量时间序列预测基准测试中，LLM-AutoOpt相比纯贝叶斯优化和无元知识的LLM基线，获得了更好的预测性能和更可解释的优化行为。

Conclusion: LLM-AutoOpt通过结合贝叶斯优化和LLM推理，有效解决了传统超参数优化的计算效率和可解释性问题，为时间序列预测任务提供了性能更好、更透明的优化框架。

Abstract: Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.

</details>


### [290] [Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs](https://arxiv.org/abs/2602.01453)
*Idan Barnea,Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 研究多智能体强化学习中的无奖励探索问题，分析学习阶段数与智能体数量之间的权衡关系，发现由时间步长H控制的尖锐转变。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体在无奖励观察情况下的协作探索问题，重点关注学习阶段数量与智能体数量之间的权衡关系，特别是在学习阶段较少时的性能表现。

Method: 采用分阶段学习框架，每个学习阶段中多个智能体独立与环境交互，每个智能体被分配策略并执行轨迹观察。提出计算高效的算法，当学习阶段数等于H时，仅需多项式数量的智能体即可获得动态的ε近似。

Result: 当学习阶段数等于H时，算法仅需Õ(S^6 H^6 A/ε^2)个智能体即可获得动态的ε近似；当学习阶段数ρ<H时，任何算法至少需要A^{H/ρ}个智能体才能达到常数精度，表明需要H量级的学习阶段才能将智能体数量限制在多项式级别。

Conclusion: 在多智能体无奖励探索中，学习阶段数与智能体数量之间存在由H控制的尖锐权衡：需要H量级的学习阶段才能实现多项式数量的智能体，否则需要指数级数量的智能体。

Abstract: We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ< H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.

</details>


### [291] [Modeling Topological Impact on Node Attribute Distributions in Attributed Graphs](https://arxiv.org/abs/2602.01454)
*Amirreza Shiralinasab Langari,Leila Yeganeh,Kim Khoa Nguyen*

Main category: cs.LG

TL;DR: 提出一种将图拓扑与节点属性分布相结合的代数方法，通过拓扑影响分布来量化拓扑结构对节点属性的影响


<details>
  <summary>Details</summary>
Motivation: 研究图拓扑如何影响节点属性分布，将拓扑和属性视为结构不同但相互作用的组件，提供新的分析视角

Method: 引入代数方法结合图拓扑和节点属性概率分布；开发范畴框架形式化节点对拓扑的感知；量化节点视角并整合属性分布；建立充分性条件证明在完全图上恢复原始属性分布

Result: 提出拓扑条件分布作为后验概率P(·|v)和P(·|G)的近似；通过简单测试模型ID和无监督图异常检测任务验证方法

Conclusion: 成功建立了图拓扑与节点属性分布之间的理论联系，为理解拓扑如何影响属性分布提供了数学框架和验证方法

Abstract: We investigate how the topology of attributed graphs influences the distribution of node attributes. This work offers a novel perspective by treating topology and attributes as structurally distinct but interacting components. We introduce an algebraic approach that combines a graph's topology with the probability distribution of node attributes, resulting in topology-influenced distributions. First, we develop a categorical framework to formalize how a node perceives the graph's topology. We then quantify this point of view and integrate it with the distribution of node attributes to capture topological effects. We interpret these topology-conditioned distributions as approximations of the posteriors $P(\cdot \mid v)$ and $P(\cdot \mid \mathcal{G})$.
  We further establish a principled sufficiency condition by showing that, on complete graphs, where topology carries no informative structure, our construction recovers the original attribute distribution. To evaluate our approach, we introduce an intentionally simple testbed model, $\textbf{ID}$, and use unsupervised graph anomaly detection as a probing task.

</details>


### [292] [Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations](https://arxiv.org/abs/2602.01456)
*Yilun Kuang,Yash Dagade,Tim G. J. Rudner,Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: 论文提出RDMReg正则化方法，将JEPA表示对齐到Rectified Generalized Gaussian分布，实现稀疏控制，优于现有高斯正则化方法。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法使用各向同性高斯分布正则化表示，但这种方法倾向于密集表示，无法捕捉高效表示中观察到的稀疏性这一关键特性。

Method: 提出Rectified Distribution Matching Regularization (RDMReg)，这是一种切片双样本分布匹配损失，将表示对齐到Rectified Generalized Gaussian (RGG)分布。RGG通过整流实现期望ℓ0范数的显式控制，同时在期望ℓp范数约束下保持最大熵特性。

Result: Rectified LpJEPA学习到稀疏、非负的表示，在稀疏性与性能之间取得良好权衡，在图像分类基准上表现出竞争力的下游性能。

Conclusion: RDMReg能有效强制稀疏性同时保留任务相关信息，Rectified LpJEPA严格泛化了先前基于高斯的JEPA方法。

Abstract: Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.

</details>


### [293] [P-EAGLE: Parallel-Drafting EAGLE with Scalable Training](https://arxiv.org/abs/2602.01469)
*Mude Hui,Xin Huang,Jaime Campos Salas,Yue Sun,Nathan Pemberton,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: P-EAGLE将EAGLE从自回归转换为并行多token预测，通过可学习的共享隐藏状态实现，解决了长上下文训练的计算复杂度问题，在多个大模型上实现了1.10-1.36倍的加速。


<details>
  <summary>Details</summary>
Motivation: 推理大语言模型产生更长输出，需要训练在长序列上的推测解码草稿模型。并行草稿（每次前向预测多个token）相比顺序生成有延迟优势，但训练复杂度随序列长度和并行位置的乘积呈二次方增长，使得长上下文训练不切实际。

Method: 提出P-EAGLE，通过可学习的共享隐藏状态将EAGLE从自回归转换为并行多token预测。为扩展到长上下文训练，开发了包含注意力掩码预计算和序列分区技术的框架，支持在单个序列内进行梯度累积的并行预测训练。

Result: 在vLLM中实现P-EAGLE，在GPT-OSS 120B、20B和Qwen3-Coder 30B模型上相比自回归EAGLE-3实现了1.10-1.36倍的加速。

Conclusion: P-EAGLE成功解决了并行草稿模型在长上下文训练中的计算复杂度问题，通过创新的训练框架实现了显著的推理加速，为大语言模型的高效推测解码提供了实用解决方案。

Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

</details>


### [294] [Causal Preference Elicitation](https://arxiv.org/abs/2602.01483)
*Edwin V. Bonilla,He Zhao,Daniel M. Steinberg*

Main category: cs.LG

TL;DR: 提出因果偏好获取框架，通过主动查询专家对边关系的判断来加速因果图后验集中，在有限查询预算下实现更快的因果发现。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法通常只依赖观测数据，但专家知识可以显著提升因果图推断的准确性和效率。然而，如何系统性地整合专家判断并有效利用有限的专家查询预算仍是一个挑战。

Method: 提出贝叶斯框架，从任意黑盒观测后验出发，通过三值似然函数建模专家对边存在和方向的噪声判断。使用灵活的粒子近似进行后验推断，并基于专家分类响应的期望信息增益准则高效选择查询问题。

Result: 在合成图、蛋白质信号数据和人类基因扰动基准测试中，该方法在有限查询预算下实现了更快的后验集中和更好的有向效应恢复效果。

Conclusion: 因果偏好获取框架有效整合了专家知识，显著提升了因果发现效率，为专家参与的因果推断提供了系统化的解决方案。

Abstract: We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.

</details>


### [295] [Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems](https://arxiv.org/abs/2602.01486)
*Xuesong Wang,Michael Groom,Rafael Oliveira,He Zhao,Terence O'Kane,Edwin V. Bonilla*

Main category: cs.LG

TL;DR: 提出多尺度小波变换器(MSWT)，通过在小波域学习系统动力学来解决神经算子中的频谱偏差问题，在混沌系统和气候预测中显著减少误差并提高长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的动力学系统代理模型比数值求解器快得多，但许多基于机器学习的模型（如神经算子）存在频谱偏差，会衰减高频分量，而这些高频分量通常编码小尺度结构。在天气预报等应用中，这种限制特别有害，因为错误表示的高频会导致长期不稳定性。

Method: 提出多尺度小波变换器(MSWT)，在小波域中学习系统动力学。小波变换明确分离了不同尺度的低频和高频内容。MSWT采用保留小波的下采样方案来保持高频特征，并使用基于小波的注意力机制来捕捉跨尺度和频带的依赖关系。

Result: 在混沌动力学系统实验中显示出显著的误差减少和改善的长期频谱保真度。在ERA5气候再分析数据上，MSWT进一步减少了气候学偏差，证明了其在真实世界预测环境中的有效性。

Conclusion: 多尺度小波变换器通过在小波域中学习系统动力学，有效解决了神经算子的频谱偏差问题，提高了长期预测的稳定性和准确性，在复杂动力学系统建模中具有实际应用价值。

Abstract: Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.

</details>


### [296] [OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference](https://arxiv.org/abs/2602.01493)
*Zhuoyuan Wang,Hanjiang Hu,Xiyu Deng,Saviz Mowlavi,Yorie Nakahira*

Main category: cs.LG

TL;DR: OpInf-LLM：基于算子推理的LLM参数化PDE求解框架，利用少量解数据实现准确预测，支持自然语言指定PDE求解任务


<details>
  <summary>Details</summary>
Motivation: 现有LLM在求解偏微分方程时面临执行成功率与数值精度之间的权衡，特别是在泛化到未见参数和边界条件时存在挑战

Method: 结合算子推理与LLM能力，利用少量解数据构建参数化PDE求解框架，提供统一的工具接口支持自然语言任务指定

Result: 能够准确预测包括未见参数和配置在内的多样化PDE实例，在异构设置下实现高执行成功率，计算需求低

Conclusion: OpInf-LLM为LLM-based PDE求解中的可泛化降阶建模开辟了新可能性

Abstract: Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.

</details>


### [297] [Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization](https://arxiv.org/abs/2602.01510)
*Hengzhe Zhang,Qi Chen,Bing Xue,Wolfgang Banzhaf,Mengjie Zhang*

Main category: cs.LG

TL;DR: 提出基于遗传编程的特征构造框架，通过优化经验风险和vicinal Jensen gap来控制过拟合，结合噪声估计和流形入侵检测机制，在58个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 遗传编程特征构造虽然成功，但过拟合问题限制了其广泛应用。需要改进泛化能力，控制过拟合。

Method: 1) 证明vicinal risk可通过经验风险加正则化项（有限差分或vicinal Jensen gap）界定；2) 提出进化特征构造框架，联合优化经验风险和vicinal Jensen gap；3) 开发噪声估计策略动态调整正则化强度；4) 提出流形入侵检测机制防止生成不现实的增强样本。

Result: 在58个数据集上实验表明，Jensen gap最小化比其他复杂度度量更有效。与15种机器学习算法比较，所提过拟合控制策略的遗传编程获得更优性能。

Conclusion: 提出的基于vicinal Jensen gap正则化的遗传编程特征构造框架能有效控制过拟合，提升泛化性能，为自动化机器学习提供了新方法。

Abstract: Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

</details>


### [298] [You Need an Encoder for Native Position-Independent Caching](https://arxiv.org/abs/2602.01519)
*Shiju Zhao,Junhao Hu,Jiaqi Zheng,Guihai Chen*

Main category: cs.LG

TL;DR: 该论文提出COMB系统，通过引入编码器到仅解码器LLMs中实现原生位置无关缓存(PIC)，显著减少首词生成时间并提高吞吐量，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存基于前缀位置，对任意顺序检索的上下文处理效率低下。现有位置无关缓存方法往往导致准确率显著下降，限制了实际应用。

Method: 提出原生PIC方法：1) 在仅解码器LLMs中重新引入编码器，并显式训练以支持PIC；2) 开发COMB系统，这是一个与现有推理框架无缝集成的PIC感知缓存系统。

Result: COMB将首词生成时间减少51-94%，吞吐量提高3倍，同时保持可比较的准确率。在DeepSeek-V2-Lite-Chat上的质量改进证明了COMB对其他类型仅解码器LLMs的适用性。

Conclusion: 通过原生PIC方法和COMB系统，成功解决了位置无关缓存中的准确率下降问题，显著提升了LLMs在处理任意顺序上下文时的效率和性能。

Abstract: The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.

</details>


### [299] [When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.01522)
*Haoran Zhao,Soyeon Caren Han,Eduard Hovy*

Main category: cs.LG

TL;DR: 论文提出Gap-Init初始化方法，通过将rank-1 LoRA方向与模态间隙向量对齐，解决了极低秩参数高效微调中的不稳定性问题，在多个视觉语言任务上匹配甚至超越rank-8基线性能。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调（PEFT）是多模态大语言模型适配的标准方法，但在极低秩设置（特别是rank-1 LoRA）下通常不稳定。作者发现这种不稳定性不仅源于有限容量，更源于优化对更新方向的高度敏感性：预训练的视觉和文本特征形成不匹配的各向异性区域，产生主导的"间隙"方向，在rank-1约束下不成比例地引导早期梯度。

Method: 提出Gap-Init方法：一种几何感知的初始化策略。通过从小型校准集中估计模态间隙向量，将rank-1 LoRA方向与该向量对齐，同时保持初始LoRA更新为零。这种方法解决了随机rank-1初始化不太可能与该主导方向对齐的问题，从而避免了弱梯度和训练崩溃。

Result: 在多个视觉语言任务和骨干网络上，Gap-Init能够一致地稳定rank-1训练，并且可以匹配甚至超越强大的rank-8基线性能。实验表明在极低秩限制下，初始对齐的重要性可能与秩本身同等重要。

Conclusion: 研究表明，在极低秩限制下，初始对齐的重要性可能与秩本身同等重要。通过几何感知的初始化策略，可以显著改善rank-1 LoRA的稳定性，为参数高效微调提供了新的视角和实用解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable. We show that this instability is not solely due to limited capacity: in the rank-1 regime, optimization is highly sensitive to the update direction. Concretely, pretrained vision and text features form mismatched anisotropic regions, yielding a dominant "gap" direction that acts like a translation component and disproportionately steers early gradients under rank-1 constraints. Analyzing pretrained representations, we identify a modality-gap axis that dominates early gradient flow, while a random rank-1 initialization is unlikely to align with it, leading to weak gradients and training collapse. We propose Gap-Init, a geometry-aware initialization that aligns the rank-1 LoRA direction with an estimated modality-gap vector from a small calibration set, while keeping the initial LoRA update zero. Across multiple vision-language tasks and backbones, Gap-Init consistently stabilizes rank-1 training and can match or outperform strong rank-8 baselines. Our results suggest that at the extreme low-rank limit, initial alignment can matter as much as rank itself.

</details>


### [300] [A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning](https://arxiv.org/abs/2602.01523)
*Akifumi Wachi,Hirota Kinoshita,Shokichi Takakura,Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

TL;DR: 论文提出相对预算理论，通过单一量ξ=H/E[T]解释强化学习在不同任务和计算预算下的效果差异，识别了三个学习机制：不足、平衡和充足，并给出了有限样本保证。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型推理能力方面效果不一，受任务和计算预算影响。需要理论解释这种差异，指导高效学习策略。

Method: 提出相对预算理论，定义ξ=H/E[T]（生成时域/首次正确解的平均token数）。分析ξ如何控制奖励方差和信息轨迹概率，识别三个学习机制，提供在线RL的有限样本保证。

Result: 理论预测三个机制：ξ→0时样本复杂度爆炸；ξ=Θ(1)时样本效率最高；ξ→∞时边际收益递减。实证验证ξ∈[1.5,2.0]时学习效率最高，与推理性能峰值一致。

Conclusion: 相对预算ξ是理解RL样本效率的关键量，平衡机制（ξ≈1.5-2.0）实现最优学习，为RL超参数设置提供理论指导。

Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.

</details>


### [301] [The Inlet Rank Collapse in Implicit Neural Representations: Diagnosis and Unified Remedy](https://arxiv.org/abs/2602.01526)
*Jianqiao Zheng,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 论文提出了一种结构诊断框架，通过层级的NTK分解识别了"入口秩塌陷"现象，并推导出秩扩展初始化方法，使标准MLP能够实现高保真重建。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在有限训练预算下难以恢复细粒度细节。虽然经验技术如位置编码、正弦激活和批归一化能缓解此问题，但它们的理论解释大多是事后分析，仅关注修改后的全局NTK谱。需要从结构角度理解INRs的表达瓶颈。

Method: 引入结构诊断框架，通过层级NTK分解识别"入口秩塌陷"现象：低维输入坐标无法跨越高维嵌入空间，导致第一层出现基本秩缺陷。基于此诊断，推导出秩扩展初始化方法，确保表示秩随层宽扩展，无需架构修改或计算开销。

Result: 该框架为位置编码、正弦激活和批归一化提供了统一的秩恢复视角。秩扩展初始化使标准MLP能够实现高保真重建，证明增强INRs的关键在于初始秩传播的结构优化。

Conclusion: 论文通过结构诊断框架揭示了INRs表达瓶颈的根本原因，并提出了一种原则性解决方案。秩扩展初始化方法简单有效，为理解现有经验技术提供了统一理论视角，并展示了通过结构优化初始秩传播来增强INRs的潜力。

Abstract: Implicit Neural Representations (INRs) have revolutionized continuous signal modeling, yet they struggle to recover fine-grained details within finite training budgets. While empirical techniques, such as positional encoding (PE), sinusoidal activations (SIREN), and batch normalization (BN), effectively mitigate this, their theoretical justifications are predominantly post hoc, focusing on the global NTK spectrum only after modifications are applied. In this work, we reverse this paradigm by introducing a structural diagnostic framework. By performing a layer-wise decomposition of the NTK, we mathematically identify the ``Inlet Rank Collapse'': a phenomenon where the low-dimensional input coordinates fail to span the high-dimensional embedding space, creating a fundamental rank deficiency at the first layer that acts as an expressive bottleneck for the entire network. This framework provides a unified perspective to re-interpret PE, SIREN, and BN as different forms of rank restoration. Guided by this diagnosis, we derive a Rank-Expanding Initialization, a minimalist remedy that ensures the representation rank scales with the layer width without architectural modifications or computational overhead. Our results demonstrate that this principled remedy enables standard MLPs to achieve high-fidelity reconstructions, proving that the key to empowering INRs lies in the structural optimization of the initial rank propagation to effectively populate the latent space.

</details>


### [302] [Plain Transformers are Surprisingly Powerful Link Predictors](https://arxiv.org/abs/2602.01553)
*Quang Truong,Yu Song,Donald Loveland,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: PENCIL是一个用于图链接预测的纯Transformer编码器，通过注意力机制处理采样子图，无需手工先验知识，在保持可扩展性的同时超越了GNN和启发式方法。


<details>
  <summary>Details</summary>
Motivation: 当前图链接预测方法存在局限性：GNN依赖显式结构启发式或内存密集型节点嵌入，难以泛化和扩展到大规模图；图Transformer则因复杂结构编码带来显著开销。需要一种既强大又高效的替代方案。

Method: PENCIL采用编码器式纯Transformer架构，用注意力机制处理采样的局部子图，取代手工设计的先验知识。这种方法保持了标准Transformer的可扩展性和硬件效率。

Result: PENCIL在提取结构信号方面优于GNN，能够隐式泛化广泛的启发式和基于子图的表达能力。在参数效率上远超基于ID嵌入的方法，在多样化基准测试中保持竞争力，即使在没有节点特征的情况下也能表现良好。

Conclusion: 研究挑战了当前依赖复杂工程技术的范式，表明简单的设计选择可能足以实现相同的功能，为大规模图链接预测提供了更高效、更通用的解决方案。

Abstract: Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.

</details>


### [303] [InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs](https://arxiv.org/abs/2602.01554)
*Lv Tang,Tianyi Zheng,Bo Li,Xingyu Li*

Main category: cs.LG

TL;DR: 提出InfoTok：基于信息瓶颈原理的视觉tokenization机制，用于统一多模态大语言模型，通过信息正则化在压缩和任务相关性之间取得平衡，提升理解和生成能力


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态大语言模型中的共享token设计大多是架构驱动的，缺乏明确标准来确定token应该保留什么信息来同时支持理解和生成任务

Method: 从容量约束视角出发，提出InfoTok——基于信息瓶颈原理的信息正则化视觉tokenization机制，将tokenization建模为控制从图像到共享token再到多模态输出的信息流，通过互信息正则化实现压缩与任务相关性的权衡

Result: 将InfoTok集成到三个代表性的统一MLLM中，无需额外训练数据，实验显示在理解和生成任务上均获得一致改进

Conclusion: 信息正则化的tokenization为在统一MLLMs中学习共享token空间提供了原则性基础，token预算应优先考虑可重用结构而非难以利用的高熵变化和冗余

Abstract: Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.

</details>


### [304] [How Implicit Bias Accumulates and Propagates in LLM Long-term Memory](https://arxiv.org/abs/2602.01558)
*Yiming Ma,Lixu Wang,Lionel Z. Wang,Hongkun Yang,Haoming Sun,Xin Xu,Jiaqi Wu,Bin Chen,Wei Dong*

Main category: cs.LG

TL;DR: 研究LLMs长期记忆机制中的隐性偏见积累与传播问题，提出动态记忆标记方法进行缓解


<details>
  <summary>Details</summary>
Motivation: LLMs的长期记忆机制虽然支持连续性和个性化交互，但可能引入新的公平性风险，特别是隐性偏见的积累和传播问题尚未充分探索

Method: 1) 构建DIB基准数据集(3,776个决策场景，9个社会领域)；2) 使用长时程模拟框架评估6个SOTA LLMs和3种记忆架构；3) 提出动态记忆标记(DMT)方法，在记忆写入时强制执行公平约束

Result: 发现LLMs的隐性偏见随时间增强并在无关领域间传播；静态系统提示的缓解效果有限且短暂；DMT方法能显著减少偏见积累并有效遏制跨领域偏见传播

Conclusion: LLMs长期记忆中的隐性偏见会动态积累和传播，需要动态干预机制如DMT来有效缓解公平性风险

Abstract: Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.

</details>


### [305] [Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space](https://arxiv.org/abs/2602.01564)
*Geuntaek Seo,Minseop Shin,Pierre Monmarché,Beomjun Choi*

Main category: cs.LG

TL;DR: 证明了均值场Langevin下降-上升动力学在熵正则化两人零和博弈中的局部指数稳定性，回答了Wang和Chizat提出的开放问题


<details>
  <summary>Details</summary>
Motivation: 尽管均值场目标函数存在唯一的混合纳什均衡，但原始MFL-DA动力学在一般非凸-非凹支付函数下的长期行为仍然未知，需要回答Wang和Chizat提出的关于局部稳定性的开放问题

Method: 通过谱分析线性化算子建立熵在均衡点附近的强制性估计，揭示局部位移凸-凹结构，从而证明收缩性质

Result: 证明了均衡点是局部指数稳定的：当初始化在Wasserstein度量下足够接近均衡点时，动力学以指数速率收敛到均衡点

Conclusion: 解决了Wang和Chizat提出的局部稳定性和定量速率问题，但全局收敛仍然是一个开放的挑战

Abstract: We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.

</details>


### [306] [Generative Visual Code Mobile World Models](https://arxiv.org/abs/2602.01576)
*Woosung Koh,Sungjun Han,Segyu Lee,Se-Young Yun,Jamin Shin*

Main category: cs.LG

TL;DR: gWorld提出了一种通过可渲染代码生成的可视化移动GUI世界模型新范式，使用单个VLM预测可执行的网页代码而非直接生成像素，在准确性与模型大小之间建立了新的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI世界模型面临关键权衡：基于文本的方法牺牲视觉保真度，而视觉方法在精确文本渲染方面存在缺陷，需要依赖缓慢复杂的外部模型管道。需要一种结合两者优势的新方法。

Method: 提出通过可渲染代码生成的可视化世界建模范式，使用单个视觉语言模型预测下一个GUI状态为可执行的网页代码而非直接生成像素。开发了gWorld数据生成框架自动合成基于代码的训练数据。

Result: gWorld在4个领域内和2个领域外基准测试中，在准确性与模型大小之间建立了新的帕累托前沿，优于8个前沿开放权重模型（最大达50.25倍）。数据扩展、管道组件和世界建模能力均带来显著改进。

Conclusion: 通过可渲染代码生成的可视化世界建模范式成功结合了文本和视觉方法的优势，gWorld展示了在移动GUI代理性能改进方面的有效性，为未来GUI世界模型发展提供了新方向。

Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.

</details>


### [307] [Nearly Optimal Active Preference Learning and Its Application to LLM Alignment](https://arxiv.org/abs/2602.01581)
*Yao Zhao,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 本文提出针对偏好学习优化的主动学习算法，相比传统实验设计方法提高了样本效率


<details>
  <summary>Details</summary>
Motivation: 大语言模型对齐依赖高质量人类偏好标注数据，但收集成本高昂。现有主动学习方法多采用经典实验设计准则（如G-或D-最优性），这些目标未针对偏好学习结构进行优化，需要设计问题特定的算法

Method: 1. 识别了偏好学习特有的简单直觉，质疑现有设计目标的适用性；2. 提出两种主动学习算法：第一种提供该设置下首个实例依赖的标签复杂度保证，第二种是简单实用的贪婪方法

Result: 在真实世界偏好数据集上评估算法，相比现有方法观察到样本效率的提升

Conclusion: 针对偏好学习结构优化的主动学习算法能有效提高样本效率，为LLM对齐提供更经济的数据收集方案

Abstract: Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.

</details>


### [308] [A Lightweight Sparse Interaction Network for Time Series Forecasting](https://arxiv.org/abs/2602.01585)
*Xu Zhang,Qitong Wang,Peng Wang,Wei Wang*

Main category: cs.LG

TL;DR: LSINet：一种用于时间序列预测的轻量级稀疏交互网络，通过多头稀疏交互机制和共享交互学习，在保持线性模型效率的同时显式建模时间依赖关系，超越了现有线性模型和Transformer的性能。


<details>
  <summary>Details</summary>
Motivation: 现有线性模型虽然在某些长时序预测任务中优于Transformer，但它们通过堆叠MLP结构隐式进行时间交互，可能无法充分捕捉复杂的时间依赖关系，性能仍有提升空间。

Method: 提出LSINet，包含：1）多头稀疏交互机制（MSIM），通过稀疏诱导的伯努利分布学习时间步间的重要连接来捕捉时间依赖；2）自适应正则化损失确保稀疏性；3）共享交互学习（SIL）利用时间交互的可共享性提升效率和收敛性。

Result: 在公开数据集上的大量实验表明，LSINet在时间序列预测任务中，相比先进的线性模型和Transformer模型，实现了更高的准确性和更好的效率。

Conclusion: LSINet通过显式的时间交互机制，在保持线性模型低开销优势的同时，显著提升了时间序列预测的性能，为长时序预测任务提供了一个高效准确的解决方案。

Abstract: Recent work shows that linear models can outperform several transformer models in long-term time-series forecasting (TSF). However, instead of explicitly performing temporal interaction through self-attention, linear models implicitly perform it based on stacked MLP structures, which may be insufficient in capturing the complex temporal dependencies and their performance still has potential for improvement. To this end, we propose a Lightweight Sparse Interaction Network (LSINet) for TSF task. Inspired by the sparsity of self-attention, we propose a Multihead Sparse Interaction Mechanism (MSIM). Different from self-attention, MSIM learns the important connections between time steps through sparsity-induced Bernoulli distribution to capture temporal dependencies for TSF. The sparsity is ensured by the proposed self-adaptive regularization loss. Moreover, we observe the shareability of temporal interactions and propose to perform Shared Interaction Learning (SIL) for MSIM to further enhance efficiency and improve convergence. LSINet is a linear model comprising only MLP structures with low overhead and equipped with explicit temporal interaction mechanisms. Extensive experiments on public datasets show that LSINet achieves both higher accuracy and better efficiency than advanced linear models and transformer models in TSF tasks. The code is available at the link https://github.com/Meteor-Stars/LSINet.

</details>


### [309] [Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting](https://arxiv.org/abs/2602.01588)
*Huu Hiep Nguyen,Minh Hoang Nguyen,Dung Nguyen,Hung Le*

Main category: cs.LG

TL;DR: SpecTF：一种在频域中融合文本信息与时间序列的简单有效框架，通过谱分解和交叉注意力机制自适应地重加权频率带，显著提升多模态时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态时间序列预测至关重要，但现有方法通常逐点对齐文本特征与时间序列模式，忽略了上下文信息的多尺度时间影响（如时间序列周期和动态变化）。局部对齐与全局文本上下文之间的不匹配需要解决。

Method: 提出SpecTF框架：1）提取文本嵌入；2）将其投影到频域；3）使用轻量级交叉注意力机制与时间序列的频谱分量融合；4）基于文本相关性自适应重加权频率带；5）将结果映射回时域进行预测。

Result: 实验结果表明，SpecTF在多种多模态时间序列数据集上显著优于现有最先进模型，同时使用的参数数量明显更少。

Conclusion: SpecTF通过在频域中整合文本信息对时间序列的影响，有效解决了局部对齐与全局上下文不匹配的问题，为多模态时间序列预测提供了一种简单而强大的解决方案。

Abstract: Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.

</details>


### [310] [The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR](https://arxiv.org/abs/2602.01599)
*Israel Adewuyi,Solomon Okibe,Vladmir Ivanov*

Main category: cs.LG

TL;DR: 在RLVR中，仅训练1%的随机参数子集就能达到或超过全参数微调的性能，表明预训练模型包含多个可行的稀疏子网络而非单一特权集合。


<details>
  <summary>Details</summary>
Motivation: 彩票假设表明稀疏子网络可以达到完整模型性能，而RLVR中参数更新集中在稀疏子集上，这暗示了参数冗余。研究旨在探索利用这种冗余的最简单方法：在极端稀疏度下仅训练随机选择的参数子集。

Method: 在RLVR中，仅训练随机选择的1%参数子集（使用随机掩码），比较不同随机掩码的性能，并分析掩码间的重叠度（Jaccard相似性）。

Result: 仅训练1%参数就能在3个模型和2个任务域上匹配或超过全参数RLVR微调。不同随机掩码重叠度极低（≤0.005 Jaccard相似性），但都能成功，表明存在多个可行的稀疏子网络。

Conclusion: 提出了"多票假设"：预训练模型包含许多可行的稀疏子网络而非单一特权集合。这种现象可由RLVR中隐式的每步KL约束解释，该约束将更新限制在低维子空间，使得任意稀疏掩码都能成功。

Abstract: The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further lends evidence to this underlying redundancy. We study the simplest possible way to exploit this redundancy: training only a randomly selected subset of parameters at extreme sparsities. Empirically, we find that training just 1\% of parameters matches or exceeds full-parameter RLVR finetuning across 3 models and 2 task domains. Moreover, different random masks show minimal overlap ($\leq 0.005$ Jaccard similarity) and yet all succeed, suggesting pretrained models contain many viable sparse subnetworks rather than one privileged set. We term this the Multiple Ticket Hypothesis. We explain this phenomenon through the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to succeed.

</details>


### [311] [Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.01601)
*Hieu Trung Nguyen,Bao Nguyen,Wenao Ma,Yuzhi Zhao,Ruifeng She,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: VIP是一种基于方差信息的预测性分配策略，通过优化梯度方差最小化来分配rollout预算，提高强化学习采样效率


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GRPO）对所有训练提示分配固定数量的rollout，这种均匀分配隐含地将所有提示视为同等信息量，可能导致计算预算使用效率低下并阻碍训练进展

Method: VIP使用轻量级高斯过程模型基于最近rollout预测每个提示的成功概率，将这些概率预测转化为方差估计，然后通过凸优化问题在硬计算预算约束下确定最优rollout分配

Result: 实验结果表明，VIP在多个基准测试中持续提高采样效率，并比均匀或启发式分配策略获得更高性能

Conclusion: VIP通过方差感知的预测性分配策略，有效解决了强化学习中采样效率低下的问题，为可验证奖励的强化学习提供了更高效的计算预算分配方法

Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.

</details>


### [312] [Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching](https://arxiv.org/abs/2602.01606)
*Zeqiao Li,Yijing Wang,Haoyu Wang,Zheng Li,Zhiqiang Zuo*

Main category: cs.LG

TL;DR: FLAME提出了一种基于流匹配的最大熵强化学习框架，通过重要性重加权绕过配分函数估计，设计解耦熵估计器纠正偏差，实现高效探索和一步生成控制，在MuJoCo上超越高斯基线并匹配多步扩散策略性能，同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 扩散策略虽然表达能力强但推理延迟高，流匹配可实现一步生成但难以集成到最大熵强化学习中，因为最优策略是难以处理的基于能量的分布，且高效对数似然估计存在严重离散化偏差。

Method: 1) 提出Q-重加权流匹配目标，通过重要性重加权绕过配分函数估计；2) 设计解耦熵估计器严格纠正偏差，实现高效探索；3) 集成MeanFlow公式实现表达性强且高效的一步控制。

Result: 在MuJoCo实验结果表明，FLAME超越高斯基线，匹配多步扩散策略性能，同时显著降低推理成本。

Conclusion: FLAME是一个原则性框架，解决了流匹配集成到最大熵强化学习中的挑战，实现了表达性强且推理高效的一步控制策略。

Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.

</details>


### [313] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: 论文提出PIPE评估协议，通过最小化改写环境接口来诊断智能体对特定接口的依赖，发现轨迹监督微调会显著增强智能体对训练时接口的"走捷径"行为。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为交互智能体的评估存在混淆：任务成功可能源于语义工具使用能力，也可能只是记住了特定接口的交互模式。基准测试分数无法区分这两种机制，无法提供环境不变能力的证据。

Method: 提出PIPE（协议级评估增强）方法，通过最小化改写环境接口（保持任务语义和执行行为不变）来诊断接口依赖。引入接口依赖度（IR）指标，量化智能体对训练时接口的偏好。

Result: 在16个环境和多种智能体上的实验表明：轨迹监督微调显著增强接口走捷径行为——训练后的智能体在最小接口改写下性能急剧下降，而非轨迹训练的模型保持稳定。接口走捷径行为具有环境依赖性、非单调的训练动态。

Conclusion: 标准评估无法检测智能体对特定接口的依赖，PIPE揭示了轨迹监督微调会强化接口走捷径行为而非真正的语义理解。这为智能体评估提供了重要的诊断工具，并警示需要更鲁棒的训练方法。

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [314] [A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models](https://arxiv.org/abs/2602.01613)
*Sergii Kozyrev,Davyd Maiboroda*

Main category: cs.LG

TL;DR: Minima是一个生产级压缩流水线，通过结构压缩Transformer模型来减少GPU内存占用和推理延迟，结合多种张量分解方法和自定义内核，实现内存减少和吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在部署时受到GPU内存和推理延迟的限制，需要有效的压缩方法来减少内存占用并提高推理速度。

Method: 训练轻量级卷积预测器评估层和补丁级别的敏感性；对低敏感性区域应用Tucker、张量列车和张量环分解；进行短期恢复微调；使用自定义Triton和CUDA内核执行操作；利用减少的内存进行推测解码。

Result: 在Qwen3-32B模型上，8k上下文窗口下，峰值VRAM从64GiB降至40GiB；单请求吞吐量从40tps提升至50tps（Minima）和75tps（Minima+推测解码）；50个并行请求下吞吐量分别为34、44和53tps。

Conclusion: Minima是一个实用的结构压缩方法，通过共享张量骨干网络和微小层适配器，为更激进的结构压缩提供了可行路径，在高并发环境下仍保持有效性。

Abstract: Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional predictor to estimate layer- and patch-level sensitivity, applies a mixture of Tucker, tensor-train, and tensor-ring decompositions to low-sensitivity regions, performs a short healing fine-tune, and executes the resulting operators with custom Triton and CUDA kernels. The reduced memory footprint enables speculative decoding with a small draft model and a larger verifier. On Qwen3-32B at an 8k-token context window, Minima reduces peak VRAM from 64 GiB to 40 GiB. For a single active request, throughput increases from 40 tokens per second (baseline) to 50 tokens per second (Minima) and 75 tokens per second (Minima with speculative decoding). Under 50 parallel requests, throughput is 34, 44, and 53 tokens per second respectively, showing that Minima remains effective under high concurrency even when speculative decoding gains compress. We position Minima relative to recent tensor-network, low-rank plus quantization, and cross-layer sharing methods, and argue that it is a practical step toward more aggressive structural compression via shared tensor backbones with tiny per-layer adapters.

</details>


### [315] [AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems](https://arxiv.org/abs/2602.01614)
*Qi Cheng,Licheng Liu,Yao Zhang,Mu Hong,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该研究创建了首个时空农业生态系统温室气体基准数据集，结合物理模型模拟和真实观测数据，评估了多种深度学习模型在碳氮通量预测中的表现，并探索了迁移学习提升模型泛化能力的方法。


<details>
  <summary>Details</summary>
Motivation: 农业生态系统占全球温室气体排放的四分之一，但传统方法（土壤采样、过程模型、黑箱机器学习）面临数据稀疏、时空异质性和复杂地下过程等挑战。缺乏AI就绪的基准数据集和协议限制了可信AI模型的发展。

Method: 1. 创建首个时空农业生态系统温室气体基准数据集，整合Ecosys和DayCent物理模型模拟数据、涡度协方差通量塔和受控环境设施的真实观测数据；2. 评估多种序列深度学习模型（LSTM、时序CNN、Transformer）在碳氮通量预测中的性能；3. 探索迁移学习，利用模拟数据提升深度学习模型在真实观测数据上的泛化能力。

Result: 建立了首个综合性的农业生态系统温室气体基准数据集，为AI驱动的农业生态系统模型开发提供了基础。评估了不同深度学习架构在碳氮通量预测中的表现，并展示了迁移学习如何利用模拟数据改善模型在真实观测数据上的泛化性能。

Conclusion: 该基准数据集和评估框架有助于开发更准确、可扩展的AI驱动农业生态系统模型，推进对生态系统-气候相互作用的理解，为温室气体减排策略提供科学支持。

Abstract: Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.

</details>


### [316] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: SUSD提出了一种基于环境因子分解的无监督技能发现框架，通过将状态空间分解为独立组件并为不同因子分配技能变量，实现更细粒度的技能控制，促进多样动态技能的发现。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法存在局限性：基于互信息的方法倾向于发现简单静态技能，而基于距离最大化的方法虽然能促进动态技能，但无法确保发现全面覆盖所有可控因子或实体的技能集。

Method: SUSD框架将状态空间分解为独立组件（如对象或可控实体），为不同因子分配独立的技能变量，实现细粒度技能控制。使用动态模型跟踪各因子学习进度，自适应地将智能体注意力引导至未充分探索的因子。

Result: 在包含1到10个因子的三个环境中，SUSD能够发现多样复杂的技能，在因子化和复杂环境中显著优于现有无监督技能发现方法，同时产生因子化的技能表示，便于通过分层强化学习高效训练组合下游任务。

Conclusion: SUSD通过利用环境组合结构，实现了更丰富多样的无监督技能发现，提供了因子化的技能表示，支持对单个实体的细粒度解耦控制，为组合任务的高效训练奠定了基础。

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [317] [Toward Enhancing Representation Learning in Federated Multi-Task Settings](https://arxiv.org/abs/2602.01626)
*Mehdi Setayesh,Mahdi Beitollahi,Yasser H. Khalil,Hongliang Li*

Main category: cs.LG

TL;DR: 提出FedMuscle算法，通过Muscle损失函数在多任务联邦学习中学习共享表示空间，有效处理模型和任务异构性


<details>
  <summary>Details</summary>
Motivation: 现有联邦多任务学习方法大多假设模型一致性（完全或部分同质模型），限制了在实际异构场景中的应用。需要一种能处理模型和任务异构性的方法

Method: 提出Muscle损失函数，一种新颖的对比学习目标，同时对齐所有参与模型的表示。基于此开发FedMuscle算法，这是一个实用且通信高效的联邦多任务学习算法

Result: 在多样化的图像和语言任务实验中，FedMuscle始终优于最先进的基线方法，在异构设置下提供显著改进和鲁棒性能

Conclusion: 通过Muscle损失学习共享表示空间而非共享模型参数，FedMuscle能有效处理联邦多任务学习中的模型和任务异构性，具有实际应用价值

Abstract: Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.

</details>


### [318] [COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection](https://arxiv.org/abs/2602.01635)
*Jinwoo Park,Hyeongwon Kang,Seung Hun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: COMET提出了一种基于码本的在线自适应多尺度时间序列异常检测方法，通过多尺度补丁编码、向量量化核心集和在线码本适应三个组件，在多个基准数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在补丁级表示学习中未能充分探索时间依赖性和多变量相关性，依赖单尺度模式限制了不同时间范围异常的检测，且专注于正常数据表示使模型在推理时容易受到分布偏移的影响。

Method: COMET包含三个关键组件：1）多尺度补丁编码，在不同补丁尺度上捕获时间依赖性和变量间相关性；2）向量量化核心集，通过码本学习代表性正常模式，结合量化误差和记忆距离的双重评分检测异常；3）在线码本适应，基于码本条目生成伪标签，通过对比学习在推理时动态适应模型。

Result: 在五个基准数据集上的实验表明，COMET在45个评估指标中的36个上取得了最佳性能，验证了其在多样化环境中的有效性。

Conclusion: COMET通过多尺度表示学习、码本驱动的正常模式学习和在线适应机制，有效解决了时间序列异常检测中的关键挑战，在多个数据集上展现了优越性能。

Abstract: Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.

</details>


### [319] [Chance-Constrained Inference for Hallucination Risk Control in Large Language Models](https://arxiv.org/abs/2602.01637)
*Sreenivasan Mohandas*

Main category: cs.LG

TL;DR: 提出机会约束推理方法，通过有限样本自适应验证来约束语言模型幻觉的概率风险，相比基于置信度的选择性预测提供更可靠的保证。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能降低平均错误率，但无法在重复使用中明确控制幻觉发生的频率，需要一种能提供概率风险保证的推理方法。

Method: 将推理建模为部署时风险控制问题，提出机会约束推理方法，使用序列化、任意时间有效的推理过程，通过有限样本自适应验证可行性或不可行性。

Result: 在NaturalQuestions风格问题和受控多跳问答上的实验表明，该方法能可靠控制风险，早期检测内在不可行输入，并在重复使用下安全组合，而基于置信度的基线方法无法提供一致保证。

Conclusion: 机会约束推理为语言模型部署提供了概率风险控制框架，能有效约束幻觉频率，相比传统置信度方法提供更可靠的保证。

Abstract: Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.

</details>


### [320] [De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion](https://arxiv.org/abs/2602.01643)
*Xichen Sun,Wentao Wei,Jiahua Rao,Jiancong Xie,Yuedong Yang*

Main category: cs.LG

TL;DR: MBGen：基于多体增强扩散框架，从质谱数据生成分子结构，通过多体注意力机制和高阶边建模，显著提升分子结构生成和异构体区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注原子中心和成对相互作用，忽略了高阶边相互作用，无法系统捕捉多体特性，限制了从质谱数据生成复杂异构体和非局部断裂机制的能力。

Method: 提出MBGen，一个多体增强的扩散框架，集成多体注意力机制和高阶边建模，充分利用MS/MS谱中编码的丰富结构信息。

Result: 在NPLIB1和MassSpecGym基准测试中，MBGen表现出卓越性能，比现有最优方法提升高达230%，能有效区分复杂异构体并捕捉非局部断裂信息。

Conclusion: 多体建模在质谱分子结构生成中具有重要科学价值和实际应用意义，MBGen框架为从质谱数据生成新分子提供了更准确的方法。

Abstract: Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.

</details>


### [321] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的三轴分类法，将智能体能力与空间任务连接起来，强调空间智能对于具身智能体的重要性，并识别了三个关键发现和六个重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么关注智能体架构，要么关注空间领域，缺乏将这两种互补能力统一起来的框架。大语言模型在符号领域的成功不能直接转化到物理世界，空间智能（感知3D结构、推理物体关系、在物理约束下行动）对于具身智能体至关重要。

Method: 通过对2000多篇论文的全面回顾（引用742篇顶级会议论文），提出了一个统一的三轴分类法，连接智能体能力与跨尺度的空间任务。区分了空间基础（几何和物理的度量理解）与符号基础（图像与文本关联）。

Result: 分析揭示了三个关键发现：1）分层记忆系统对于长时程空间任务很重要；2）GNN-LLM集成是结构化空间推理的有前景方法；3）世界模型对于跨微观到宏观空间尺度的安全部署至关重要。

Conclusion: 该分类法为统一碎片化的研究努力奠定了基础，并确定了六个重大挑战和未来研究方向，包括需要统一的评估框架来标准化跨领域评估，以推动机器人、自动驾驶和地理空间智能等领域下一代空间感知自主系统的发展。

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [322] [On the Spatiotemporal Dynamics of Generalization in Neural Networks](https://arxiv.org/abs/2602.01651)
*Zichao Wei*

Main category: cs.LG

TL;DR: 论文提出SEAD架构，从物理原理推导神经网络架构，解决加法等任务的长度泛化问题，实现从16位到100万位数字的完美泛化。


<details>
  <summary>Details</summary>
Motivation: 神经网络无法像人类儿童那样将加法规则泛化到任意长数字序列（如从16位到32位），这不仅是工程问题，而是违反了物理基本原理。作者从物理学角度思考计算系统应有的约束条件。

Method: 从三个物理约束（局部性、对称性、稳定性）推导出SEAD架构：一种神经细胞自动机，通过局部卷积规则的迭代直到收敛。该方法不是设计出来的，而是从基本原理推导出来的。

Result: 在三个任务上验证：1) 奇偶性任务：通过光锥传播实现完美长度泛化；2) 加法任务：从L=16到L=100万实现100%准确率的尺度不变推理，展示输入自适应计算；3) Rule 110任务：学习图灵完备的细胞自动机而无轨迹发散。

Conclusion: 统计学习与逻辑推理之间的鸿沟可以通过尊重计算的物理学原理来弥合，而不是通过扩大参数规模。这表明计算系统的物理约束对实现真正泛化至关重要。

Abstract: Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.

</details>


### [323] [Efficient Adversarial Attacks on High-dimensional Offline Bandits](https://arxiv.org/abs/2602.01658)
*Seyed Mohammad Hadi Hosseini,Amir Najafi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 本文研究了离线bandit评估中奖励模型对抗攻击的脆弱性，发现在高维场景下，即使对奖励模型权重进行微小扰动也能显著改变bandit行为，使现代图像评估应用特别易受攻击。


<details>
  <summary>Details</summary>
Motivation: Bandit算法已成为评估机器学习模型的重要工具，但离线评估中对奖励模型的对抗鲁棒性尚未充分研究。当攻击者在bandit训练前扰动奖励模型而非训练数据时，系统安全性存在重大隐患。

Method: 提出新的威胁模型，攻击者利用高维离线数据劫持bandit行为。从线性奖励函数扩展到非线性模型（如ReLU神经网络），针对Hugging Face的两个生成模型评估器（美学质量和组合对齐）进行攻击研究。

Result: 实验表明：1）随机扰动无效，但针对性扰动攻击成功率接近完美；2）理论证明高维效应：输入维度增加时，成功攻击所需扰动范数减小；3）现代图像评估应用特别脆弱。

Conclusion: 离线bandit评估对奖励模型的对抗攻击高度脆弱，高维应用尤其危险。这揭示了当前评估流程的安全漏洞，需要开发更鲁棒的防御机制。

Abstract: Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...

</details>


### [324] [Quantifying Epistemic Predictive Uncertainty in Conformal Prediction](https://arxiv.org/abs/2602.01667)
*Siu Lun Chau,Soroush H. Zargarbashi,Yusuf Sale,Michele Caprio*

Main category: cs.LG

TL;DR: 该论文研究如何在共形预测框架中量化认知预测不确定性，提出基于最大平均不精确度的计算方法，并在主动学习和选择性分类中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 共形预测（CP）虽然能提供预测区域，但无法量化区域内的认知不确定性（模型多重性）。现有方法仅依赖预测区域大小，无法提供细粒度的不确定性评估。

Method: 1) 证明split CP也诱导出凸预测分布集合（credal set）；2) 提出基于最大平均不精确度（Maximum Mean Imprecision）的计算高效不确定性度量，量化credal set中的信息冲突程度。

Result: 在主动学习和选择性分类实验中，提出的认知不确定性量化方法比单纯依赖CP区域大小提供更丰富、更细粒度的不确定性评估，证明其决策价值。

Conclusion: 共形预测可作为处理认知不确定性的理论基础，提出的不确定性度量方法能有效支持决策制定，为不确定性量化提供了新视角。

Abstract: We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-α$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.

</details>


### [325] [ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.01668)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei,Yueqi Xing*

Main category: cs.LG

TL;DR: ASGMamba：一种用于资源受限超算环境的高效多变量时间序列预测框架，结合自适应谱门控和Mamba架构，在保持线性复杂度的同时显著降低内存使用


<details>
  <summary>Details</summary>
Motivation: 现有方法面临两难：Transformer模型具有二次复杂度，限制其在长序列上的可扩展性；而线性状态空间模型难以区分有价值信号与高频噪声，导致状态容量浪费。需要为资源受限的超算环境设计高效预测框架。

Method: 提出ASGMamba框架：1) 轻量级自适应谱门控机制，基于局部谱能量动态过滤噪声；2) Mamba主干网络专注于鲁棒的时间动态；3) 分层多尺度架构，包含变量特定的节点嵌入以捕捉不同的物理特性。

Result: 在9个基准测试上的广泛实验表明，ASGMamba实现了最先进的准确性。在保持严格O(L)复杂度的同时，显著降低了长时程任务的内存使用。

Conclusion: ASGMamba为资源受限环境中的高吞吐量预测提供了一个可扩展的解决方案，在效率和准确性之间取得了良好平衡。

Abstract: Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadratic complexity, limiting their scalability on long sequences, while linear State Space Models (SSMs) often struggle to distinguish valuable signals from high-frequency noise, leading to wasted state capacity. To bridge this gap, we propose ASGMamba, an efficient forecasting framework designed for resource-constrained supercomputing environments. ASGMamba integrates a lightweight Adaptive Spectral Gating (ASG) mechanism that dynamically filters noise based on local spectral energy, enabling the Mamba backbone to focus its state evolution on robust temporal dynamics. Furthermore, we introduce a hierarchical multi-scale architecture with variable-specific Node Embeddings to capture diverse physical characteristics. Extensive experiments on nine benchmarks demonstrate that ASGMamba achieves state-of-the-art accuracy. While keeping strictly $$\mathcal{O}(L)$$ complexity we significantly reduce the memory usage on long-horizon tasks, thus establishing ASGMamba as a scalable solution for high-throughput forecasting in resource limited environments.The code is available at https://github.com/hit636/ASGMamba

</details>


### [326] [Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://arxiv.org/abs/2602.01685)
*Byeonghu Na,Hyungho Na,Yeongmin Kim,Suhyeon Jo,HeeSun Bae,Mina Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出Wasserstein Policy Regularization (WPR)，一种基于熵正则化Wasserstein距离的语义感知正则化方法，用于改进RLHF框架中的策略对齐。


<details>
  <summary>Details</summary>
Motivation: 传统的KL散度和f-散度正则化只比较相同位置标记的概率，无法捕捉语义相似性，这限制了语言模型与人类偏好的对齐效果。

Method: 提出Wasserstein策略正则化(WPR)，基于熵正则化Wasserstein距离，该距离考虑了标记空间的几何结构。通过距离的对偶公式，将正则化表示为通过最优对偶变量应用于奖励的惩罚项，得到与标准RL算法兼容的可处理目标。

Result: 实验表明，该方法在性能上优于基于KL散度和f-散度的基线方法，证明了语义感知策略距离在模型对齐中的优势。

Conclusion: Wasserstein策略正则化通过引入语义感知的距离度量，有效改进了RLHF框架中的策略对齐，为语言模型与人类偏好的对齐提供了更好的方法。

Abstract: Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.

</details>


### [327] [$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality](https://arxiv.org/abs/2602.01703)
*Pengyu Li,Lingling Zhang,Zhitao Gao,Yanrui Wu,Yuxuan Dong,Huan Liu,Bifan Wei,Jun Liu*

Main category: cs.LG

TL;DR: 提出AGT^AO框架，通过自适应正交性和对抗门控训练解决LLM遗忘中的遗忘-效用权衡问题，实现高效遗忘同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会无意中记忆敏感数据，带来隐私和安全风险。现有遗忘方法面临两难：激进遗忘导致灾难性遗忘损害模型效用，保守策略则可能导致表面遗忘，模型仍易受对抗恢复攻击。

Method: 提出AGT^AO统一框架：1）自适应正交性(AO)动态缓解遗忘和保留目标间的梯度冲突；2）对抗门控训练(AGT)将遗忘建模为潜在空间最小最大博弈，使用课程式门控机制模拟和对抗内部恢复尝试。

Result: 实验表明AGT^AO在遗忘效果(KUR ≈ 0.01)和模型效用(MMLU 58.30)之间取得了优越的权衡平衡。

Conclusion: AGT^AO框架有效解决了LLM遗忘中的遗忘-效用权衡问题，实现了稳健擦除与效用保持的统一，为隐私保护提供了有效解决方案。

Abstract: While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.

</details>


### [328] [Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner](https://arxiv.org/abs/2602.01705)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDi-RL：一种在连续潜在空间进行探索的强化学习框架，通过潜在扩散建模解决离散RL中的多样性崩溃问题，提升LLM推理能力


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在优化离散的思维链生成时，由于策略熵降低和模式激发行为，容易导致多样性崩溃，限制了探索效果

Method: 提出LaDi-RL框架：1）在连续潜在空间进行探索，潜在变量编码语义级推理轨迹；2）通过引导扩散建模探索过程，多步去噪分布随机性并保留多个共存解模式；3）将潜在空间探索与文本空间生成解耦

Result: 在代码生成和数学推理基准测试中，相比离散RL基线，在pass@1和pass@k指标上均取得一致改进：代码生成pass@1绝对提升+9.4%，数学推理pass@1绝对提升+5.7%

Conclusion: 基于扩散的潜在RL为离散token级RL提供了有原则的替代方案，通过连续潜在空间探索能更有效地优化LLM推理能力

Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.

</details>


### [329] [Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift](https://arxiv.org/abs/2602.01718)
*Sora Nakai,Youssef Fadhloun,Kacem Mathlouthi,Kotaro Yoshida,Ganesh Talluri,Ioannis Mitliagkas,Hiroki Naganuma*

Main category: cs.LG

TL;DR: 大规模研究评估了40多种泛化度量在分布偏移下的鲁棒性，发现只有少数度量在不同设置下保持稳定


<details>
  <summary>Details</summary>
Motivation: 深度学习中的泛化问题尚未解决，特别是如何仅使用训练时可用信息预测模型在训练分布之外的性能。现有研究存在训练配置不稳定的问题，需要评估泛化度量在IID之外的鲁棒性。

Method: 在10,000个超参数配置上训练中小型模型，评估40多种仅从训练模型和训练数据可计算的度量。扩展实验范围：(i) 超越标准IID设置，评估分布偏移下的鲁棒性；(ii) 评估多种架构和训练方法；(iii) 新加入基于校准和信息准则的度量。

Result: 分布偏移显著改变了许多泛化度量的预测性能，只有较小的子集在不同设置下保持相对稳定。

Conclusion: 泛化度量的有效性受分布偏移影响很大，需要更鲁棒的度量来预测模型在OOD场景下的性能，只有少数度量在不同设置下表现稳定。

Abstract: Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.

</details>


### [330] [MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration](https://arxiv.org/abs/2602.01734)
*Lianhai Ren,Yucheng Ding,Xiao Liu,Qianxiao Li,Peng Cheng,Yeyun Gong*

Main category: cs.LG

TL;DR: 论文提出MSign优化器，通过周期性地应用矩阵符号操作来恢复权重矩阵的稳定秩，有效防止大语言模型预训练中的梯度爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练中的训练不稳定性是一个关键挑战，表现为突然的梯度爆炸，浪费大量计算资源。研究者发现在训练崩溃前会出现两个关键现象：权重矩阵稳定秩的快速下降和相邻层雅可比矩阵对齐度的增加。

Method: 提出MSign优化器，通过周期性地应用矩阵符号操作来恢复权重矩阵的稳定秩，打破导致梯度指数增长的不稳定机制。该方法在5M到3B参数的模型上进行实验验证。

Result: 实验表明MSign能有效防止训练失败，计算开销小于7.0%。该方法在从5M到3B参数的不同规模模型上都表现出良好的效果。

Conclusion: MSign优化器通过针对性地解决权重矩阵稳定秩下降和雅可比矩阵对齐问题，为大语言模型预训练提供了一种有效的稳定性保障方法，计算开销小且可扩展性好。

Abstract: Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.

</details>


### [331] [Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting](https://arxiv.org/abs/2602.01736)
*Qinwei Ma,Jingzhe Shi,Jiahao Qiu,Zaiwen Yang*

Main category: cs.LG

TL;DR: 该论文质疑通用领域时间序列预测神经网络架构的有效性，指出其与特定领域SOTA存在不可调和的冲突，呼吁研究重点转向特定领域深度学习或通用领域元学习方法。


<details>
  <summary>Details</summary>
Motivation: 近年来，时间序列预测的神经网络架构变得越来越复杂，但性能已接近饱和。通用领域架构设计与特定领域（金融、天气、交通等）的实际需求脱节，特定领域开发的方法很少利用近2-3年时间序列社区的神经网络架构进展。通用领域架构研究已饱和且远离领域特定SOTA。

Method: 通过总结现有研究对神经网络时间序列预测有效性和鲁棒性的质疑，分析其固有局限性：即通用领域架构设计与特定领域SOTA之间的不可调和冲突。提出研究方向的转变建议。

Result: 揭示了通用领域时间序列预测神经网络架构研究的饱和状态，以及其与特定领域实际需求之间的脱节。特定领域（如金融、天气、交通）很少采用近2-3年时间序列社区的神经网络架构进展。

Conclusion: 呼吁时间序列社区将研究重点从通用领域神经网络架构转向两个方向：1）专注于特定领域的深度学习方法；2）转向通用领域的元学习方法开发。通用领域架构研究已饱和且远离实际应用需求。

Abstract: Recent work has questioned the effectiveness and robustness of neural network architectures for time series forecasting tasks. We summarize these concerns and analyze groundly their inherent limitations: i.e. the irreconcilable conflict between single (or few similar) domains SOTA and generalizability over general domains for time series forecasting neural network architecture designs. Moreover, neural networks architectures for general domain time series forecasting are becoming more and more complicated and their performance has almost saturated in recent years. As a result, network architectures developed aiming at fitting general time series domains are almost not inspiring for real world practices for certain single (or few similar) domains such as Finance, Weather, Traffic, etc: each specific domain develops their own methods that rarely utilize advances in neural network architectures of time series community in recent 2-3 years. As a result, we call for the time series community to shift focus away from research on time series neural network architectures for general domains: these researches have become saturated and away from domain-specific SOTAs over time. We should either (1) focus on deep learning methods for certain specific domain(s), or (2) turn to the development of meta-learning methods for general domains.

</details>


### [332] [Softmax Linear Attention: Reclaiming Global Competition](https://arxiv.org/abs/2602.01744)
*Mingwei Xu,Xuan Lin,Xinnan Guo,Wanqing Xu,Wanyun Cui*

Main category: cs.LG

TL;DR: SLA通过将softmax从token级提升到head级，在线性注意力中恢复全局竞争机制，在保持线性复杂度的同时提升表达能力和长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然将复杂度降至线性，但移除了softmax归一化，导致缺乏全局竞争机制，难以在长上下文噪声中聚焦相关信息，表达能力受限。

Method: 提出Softmax Linear Attention (SLA)框架，将softmax操作从token级提升到head级，利用注意力头作为粗粒度语义槽，通过竞争门控机制动态选择最相关的子空间。

Result: SLA在语言建模和长上下文基准测试中持续提升现有线性基线模型（RetNet、GLA、GDN）性能，特别是在具有挑战性的检索场景中显著增强了对噪声的鲁棒性。

Conclusion: SLA通过恢复"赢家通吃"的动态机制，在线性注意力中重新引入精确聚焦能力，同时保持线性复杂度，为长上下文理解提供了有效解决方案。

Abstract: While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.

</details>


### [333] [Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning](https://arxiv.org/abs/2602.01745)
*Wenhao Yu,Shaohang Wei,Jiahong Liu,Yifan Li,Minda Hu,Aiwei Liu,Hao Zhang,Irwin King*

Main category: cs.LG

TL;DR: RankTuner提出基于概率-熵校准的相对排序指标，用于监督微调中的token级重加权，在数学推理和代码生成任务上优于仅用概率或熵的方法。


<details>
  <summary>Details</summary>
Motivation: 现有token级重加权方法主要使用一维指标：真实概率反映下游对齐，token熵反映预训练先验的内在不确定性。忽略熵会误判噪声或易替换token为学习关键，忽略概率则无法反映目标特定对齐。

Method: 引入概率-熵校准信号——相对排序指标，比较真实token的排名与其在预测分布下的期望排名。使用该指标的倒数作为token级相对尺度来重加权微调目标，专注于真正未充分学习的token，避免过度惩罚内在不确定的位置。

Result: 在多个骨干模型上的实验显示：在数学推理基准上持续改进，在分布外推理上获得迁移增益，在代码生成性能上优于仅用概率或熵的重加权基线。

Conclusion: RankTuner通过概率-熵校准的相对排序指标，有效识别真正需要学习的token，在监督微调中实现更优的性能提升。

Abstract: Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.

</details>


### [334] [Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment](https://arxiv.org/abs/2602.01746)
*Hongyi Peng,Han Yu,Xiaoxiao Li,Qiang Yang*

Main category: cs.LG

TL;DR: FedGaLore：针对非IID联邦学习中LoRA性能下降问题，提出结合客户端GaLore梯度子空间优化和服务器端谱共享信号提取的鲁棒同步方法


<details>
  <summary>Details</summary>
Motivation: 在非IID联邦学习设置中，低秩适应（LoRA）性能显著低于全参数微调，主要原因是更新空间不匹配和优化器状态不匹配两个耦合问题

Method: 提出FedGaLore方法：客户端使用GaLore风格的梯度子空间优化，服务器端通过谱共享信号提取同步投影二阶矩状态，增强漂移鲁棒性

Result: 在NLU、视觉和NLG基准测试中，FedGaLore在非IID设置下相比现有联邦LoRA基线方法提高了鲁棒性和准确性

Conclusion: 通过解决LoRA在联邦学习中的两个关键不匹配问题，FedGaLore显著提升了非IID环境下的性能表现

Abstract: Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.

</details>


### [335] [MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network](https://arxiv.org/abs/2602.01751)
*Kunyi Fan,Mengjie Chen,Longlong Li,Cunquan Qu*

Main category: cs.LG

TL;DR: MGKAN：基于图Kolmogorov-Arnold网络的药物相互作用预测模型，通过可学习基函数和非对称网络建模提升预测性能


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络模型主要依赖线性聚合和对称假设，难以捕捉药物相互作用的非线性特征和异质性模式，限制了预测准确性

Method: 提出MGKAN模型，将KAN驱动的基函数引入非对称DDI预测，替代传统MLP变换；整合三种网络视图（非对称DDI网络、共相互作用网络、生化相似性网络）和角色特定嵌入；采用融合模块结合线性注意力和非线性变换

Result: 在两个基准数据集上，MGKAN优于七个最先进的基线模型；消融研究和案例研究证实了其预测准确性和对方向性药物效应的建模能力

Conclusion: MGKAN通过引入可学习基函数和非对称网络建模，能够更有效地捕捉药物相互作用的非线性特征和方向性语义，为药物安全性评估提供了更准确的预测工具

Abstract: Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.

</details>


### [336] [A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention](https://arxiv.org/abs/2602.01763)
*Xiaowei Ye,Xiaoyu He,Chao Liao,Chen Wu,Pinyan Lu*

Main category: cs.LG

TL;DR: 论文首次从理论上证明了混合注意力与标准全注意力在表达能力上的分离：对于多步推理任务，L+1层全注意力网络足够解决，而混合网络即使包含L-1层全注意力和指数级多的线性注意力层也无法解决。


<details>
  <summary>Details</summary>
Motivation: 现有高效注意力机制（如线性注意力、混合注意力）虽然缓解了全注意力的二次复杂度问题，但其表达能力相对于全注意力缺乏严格的理论刻画。需要从理论上理解不同注意力机制的基本能力和局限性。

Method: 建立表达能力层次理论，分析不同注意力机制在顺序函数组合（多步推理任务）上的表现。理论适用于所有可表示为递归的线性注意力变体，包括Mamba、DeltaNet等。

Result: 证明了表达能力层次：对于多步推理任务，L+1层全注意力网络足够解决，而任何混合网络（包含L-1层全注意力和2^{3L^2}层线性注意力）都无法解决该任务，显示了两种注意力在表达能力上的明确分离。

Conclusion: 首次提供了混合注意力与标准全注意力之间的可证明分离，为理解不同注意力机制的基本能力和局限性提供了理论视角，揭示了高效注意力机制在表达能力上的根本限制。

Abstract: Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to full attention lacks a rigorous theoretical characterization. In this work, we theoretically characterize the performance differences among these attention mechanisms. Our theory applies to all linear attention variants that can be formulated as a recurrence, including Mamba, DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that must occur within a model's forward pass, an ($L+1$)-layer full attention network is sufficient, whereas any hybrid network interleaving $L-1$ layers of full attention with a substantially larger number ($2^{3L^2}$) of linear attention layers cannot solve it. This result demonstrates a clear separation in expressive power between the two types of attention. Our work provides the first provable separation between hybrid attention and standard full attention, offering a theoretical perspective for understanding the fundamental capabilities and limitations of different attention mechanisms.

</details>


### [337] [CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling](https://arxiv.org/abs/2602.01766)
*Runsong Zhao,Shilei Liu,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Yujin Yuan,Tong Xiao,Jingbo Zhu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: CoMeT是一种创新的Transformer架构，通过双内存系统和分块处理实现常数内存使用和线性时间复杂度，能处理任意长序列，仅需少量微调即可集成到预训练模型中。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度和不断增长的KV缓存是处理长上下文的主要障碍，需要一种能处理任意长序列且内存使用恒定的高效架构。

Method: CoMeT采用分块处理序列数据，使用双内存系统：FIFO队列的临时内存处理近期事件，带门控更新规则的全局内存处理长程依赖。这些内存作为动态软提示用于下一分块。引入层级流水线并行策略进行高效微调。

Result: 在32k上下文微调的模型能在100万token序列中准确检索任意位置的密码。在SCROLLS基准测试中超越其他高效方法，在摘要任务上达到与全注意力基线相当的性能。在实际代理和用户行为QA任务中验证了有效性。

Conclusion: CoMeT通过创新的双内存系统和分块处理机制，成功解决了Transformer处理长序列时的内存和时间复杂度问题，为实际应用中的长上下文处理提供了高效解决方案。

Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/

</details>


### [338] [IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination](https://arxiv.org/abs/2602.01769)
*Yuanshuai Li,Yuping Yan,Jirui Han,Fei Ming,Lingjuan Lv,Yaochu Jin*

Main category: cs.LG

TL;DR: IRIS提出了一种基于隐式奖励的内部筛选方法，通过利用原生对数概率空间的连续隐式奖励来捕捉模态竞争，无需外部反馈即可有效减少多模态大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的幻觉问题仍然是一个基本挑战。现有的直接偏好优化（DPO）方法通常依赖昂贵的外部评估器进行评分或重写，这会导致离策略学习能力差距和离散化损失。由于缺乏对内部状态的访问，这种反馈忽略了不同模态之间导致幻觉生成的细粒度冲突。

Method: IRIS（隐式奖励引导的内部筛选）利用原生对数概率空间的连续隐式奖励来保持完整的信息密度并捕捉内部模态竞争。这种在策略范式通过使用自生成的偏好对来消除学习能力差距。通过基于多模态隐式奖励筛选这些偏好对，IRIS确保优化由直接解决模态冲突的信号驱动。

Result: 大量实验表明，IRIS仅使用5.7k个样本就在关键幻觉基准测试中实现了高度竞争力的性能，在偏好对齐过程中不需要任何外部反馈。这些结果证实IRIS为缓解MLLM幻觉提供了一个高效且原则性的范式。

Conclusion: IRIS通过利用隐式奖励和内部模态竞争信息，提供了一种无需外部反馈的高效方法来减少多模态大语言模型的幻觉问题，为MLLM对齐提供了新的范式。

Abstract: Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.
  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.

</details>


### [339] [DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics](https://arxiv.org/abs/2602.01772)
*Yucheng Liao,Han Wen,Weinan E,Weijie Zhang*

Main category: cs.LG

TL;DR: DIA-CLIP：基于预训练模型的数据非依赖采集质谱分析新范式，通过跨模态表示学习实现零样本肽段-谱图匹配，显著提升蛋白质鉴定深度和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前DIA-MS分析框架需要每个实验进行半监督训练，容易过拟合且缺乏跨物种和实验条件的泛化能力。需要一种更通用、无需特定训练的分析方法。

Method: 提出DIA-CLIP预训练模型，结合双编码器对比学习框架和编码器-解码器架构，建立肽段与对应谱图特征的统一跨模态表示，实现高精度零样本PSM推断。

Result: 在多个基准测试中，DIA-CLIP持续优于现有最先进工具，蛋白质鉴定数量提升高达45%，同时诱饵鉴定减少12%。在单细胞和空间蛋白质组学等应用中展现出巨大潜力。

Conclusion: DIA-CLIP将DIA分析范式从半监督训练转变为通用跨模态表示学习，为蛋白质组学分析提供了更准确、更通用的解决方案，有望推动生物标志物发现和细胞机制研究。

Abstract: Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.

</details>


### [340] [Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting](https://arxiv.org/abs/2602.01776)
*Mingyue Cheng,Xiaoyu Tao,Qi Liu,Ze Guo,Enhong Chen*

Main category: cs.LG

TL;DR: 该论文提出将时间序列预测从传统的模型中心范式转变为智能体驱动的预测（ATSF），强调预测应作为包含感知、规划、行动、反思和记忆的智能体过程，支持交互式、迭代式和自适应预测。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测将问题简化为模型中心、静态、单次预测任务，无法适应需要信息特征提取、推理驱动推断、迭代优化和持续时间适应的自适应多轮场景。

Method: 提出智能体时间序列预测（ATSF）框架，将预测重构为包含感知、规划、行动、反思和记忆的智能体过程。介绍了三种实现范式：基于工作流的设计、智能体强化学习和混合智能体工作流范式。

Result: 建立了从模型中心预测向智能体预测转变的理论框架，为时间序列预测与智能体系统的交叉研究奠定了基础。

Conclusion: 智能体时间序列预测代表了预测范式的根本转变，强调预测作为交互式、迭代式和自适应的智能体过程，为未来研究提供了新的方向和挑战。

Abstract: Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.

</details>


### [341] [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)
*Zheng Zhang,Ao Lu,Yuanhao Zeng,Ziwei Shan,Jinjin Guo,Lufei Li,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: Grad2Reward：通过单次反向传播从Judge模型推理过程中提取密集过程奖励，解决开放任务中稀疏奖励问题，实现精确的token级信用分配


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM-as-a-Judge的强化学习方法存在两个主要问题：1）序列级奖励过于稀疏，无法为复杂长轨迹提供细粒度监督；2）将Judge视为黑盒，忽略了其中丰富的中间反馈信号

Method: 提出Grad2Reward框架，通过梯度归因方法从Judge模型的推理过程中提取密集的过程奖励，实现token级信用分配。同时引入自判断机制，使策略能够通过自身评估信号改进，无需专门奖励模型或依赖外部Judge

Result: 实验表明，使用Grad2Reward优化的策略在多种开放任务上表现出色，证明了其有效性和广泛泛化能力

Conclusion: Grad2Reward通过利用Judge模型内部梯度信息提取密集奖励，显著提升了训练效率和推理质量，为开放任务强化学习提供了新范式

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

</details>


### [342] [Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It](https://arxiv.org/abs/2602.01826)
*Yaxiang Zhang,Yingru Li,Jiacai Liu,Jiawei Xu,Ziniu Li,Qian Liu,Haoyuan Li*

Main category: cs.LG

TL;DR: 本文提出一种基于响应长度的动态学习率调度器，用于稳定大语言模型的强化学习训练，解决训练-推理不匹配导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的强化学习训练存在严重的不稳定性问题。现有研究将其归因于混合引擎导致的训练-推理不匹配，但传统解决方法（如重要性采样）在长期训练中可能失效。作者从优化角度分析这一问题，发现梯度噪声和训练-推理不匹配会随着训练进展同时加剧。

Method: 作者提出一种专门的学习率调度器，不采用预定义的衰减计划，而是基于响应长度动态触发学习率衰减。响应长度被识别为即将发生不稳定的可靠早期预警信号。通过在学习率上升时降低学习率，可以有效控制训练-推理不匹配。

Result: 经验证据表明，通过响应长度动态调整学习率，可以持续稳定RL训练，并将训练-推理不匹配保持在安全水平。该方法简单而有效，解决了长期训练中的不稳定性问题。

Conclusion: 训练-推理不匹配不是静态数值差异，而是与模型优化耦合的动态失效。基于响应长度的动态学习率调度提供了一种有效的解决方案，能够稳定大语言模型的强化学习训练。

Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to "training inference mismatch stemming" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.

</details>


### [343] [Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment](https://arxiv.org/abs/2602.01828)
*Dionisia Naddeo,Jonas Linkerhägner,Nicola Toschi,Geri Skenderi,Veronica Lachi*

Main category: cs.LG

TL;DR: HGNNs仅在任务与双曲几何对齐时优于欧几里得模型，否则优势消失。研究提出了几何-任务对齐条件，发现只有链接预测任务与双曲几何对齐。


<details>
  <summary>Details</summary>
Motivation: 质疑当前将双曲图神经网络作为树状图表示学习首选范式的做法，提出需要考虑几何-任务对齐条件，即目标任务的度量结构是否与输入图的度量结构一致。

Method: 通过理论和实验分析HGNNs在两个合成回归问题中恢复低失真表示的能力；在链接预测和节点分类任务上联合分析预测性能和嵌入失真。

Result: HGNNs在需要保持度量结构的问题中表现出几何归纳偏置的优势；只有链接预测任务与双曲几何对齐；当任务与双曲几何对齐时，HGNNs始终优于欧几里得模型，否则优势消失。

Conclusion: 研究焦点应从"图是否是双曲的？"扩展到"任务是否与双曲几何对齐？"，HGNNs仅在几何-任务对齐时具有优势，这为选择合适的几何表示提供了新视角。

Abstract: Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking "Is the graph hyperbolic?" to also questioning "Is the task aligned with hyperbolic geometry?", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.

</details>


### [344] [DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis](https://arxiv.org/abs/2602.01839)
*Ru Zhang,Xunkai Li,Yaxin Deng,Sicheng Liu,Daohan Su,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang,Jia Li*

Main category: cs.LG

TL;DR: DOGMA是一个数据中心的单细胞转录组学分析框架，通过整合多层次生物先验知识进行结构重塑和语义增强，实现确定性图构建和跨物种对齐，在复杂基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个主要问题：1）早期序列方法将细胞视为独立实体，忽略了细胞间的功能关系；2）结构化方法使用启发式规则捕获细胞关系但忽略了生物先验知识，导致计算开销大且图表示不理想。

Method: DOGMA通过整合多层次生物先验知识进行数据重塑：1）使用统计锚点、细胞本体和系统发育树实现确定性图构建和跨物种对齐；2）利用基因本体填补特征级语义鸿沟，融入功能先验知识。

Result: 在复杂的多物种、多器官基准测试中，DOGMA达到最先进性能，表现出优异的零样本鲁棒性和样本效率，同时显著降低计算成本。

Conclusion: DOGMA通过系统整合生物先验知识，超越了依赖随机启发式的方法，为单细胞转录组学分析提供了更高效、更鲁棒的数据中心解决方案。

Abstract: Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.
  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.

</details>


### [345] [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)
*Jinbin Bai,Yixuan Li,Yuchen Zhu,Yi Xin,Qingyu Shi,Aosong Feng,Xiaohong Liu,Molei Tao,Jianru Xue,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: Prism是一个针对离散扩散语言模型的高效测试时扩展框架，通过分层轨迹搜索、局部分支和自验证反馈，在数学推理和代码生成任务上实现了性能与效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时扩展算法主要基于自回归解码，不适用于并行解码的离散扩散语言模型，因此需要开发专门的高效TTS方法来释放dLLMs的生成潜力。

Method: 提出Prism框架，包含三个核心组件：(1) 分层轨迹搜索，在去噪早期动态剪枝和重新分配计算资源；(2) 局部分支与部分重掩码，在保留高置信度token的同时探索多样化实现；(3) 自验证反馈，通过自评估提示替代外部验证器。

Result: 在三个dLLM模型（LLaDA 8B Instruct、Dream 7B Instruct、LLaDA 2.0-mini）和四个数学推理与代码生成基准测试上，Prism实现了性能与效率的良好平衡，以显著更少的函数评估次数匹配了best-of-N的性能。

Conclusion: Prism为离散扩散语言模型提供了一种高效且有效的测试时扩展框架，成功解决了传统自回归TTS方法不适用于dLLMs的问题，为释放dLLMs的生成潜力提供了新途径。

Abstract: Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.

</details>


### [346] [No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation](https://arxiv.org/abs/2602.01845)
*Furkan Eris*

Main category: cs.LG

TL;DR: Proust是一个309M参数的因果蛋白质语言模型，通过架构创新弥合了掩码语言模型（擅长适应性预测）和因果模型（擅长生成）之间的鸿沟，在计算效率上大幅超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型存在基本分歧：掩码语言模型擅长适应性预测，而因果模型支持生成，迫使研究人员维护两套分离的架构。需要一种能同时兼顾两种能力的统一模型。

Method: 采用从大语言模型研究中借鉴的架构创新：分组查询注意力（共享K/V投影）、跨层值残差和深度因果卷积。使用33B tokens在40 B200 GPU-hours上训练309M参数的因果PLM。

Result: 在ProteinGym替换任务上达到Spearman ρ=0.390，与需要50-200倍计算量的MLMs相当；在indels任务上创下新SOTA，超越大20倍的模型；在EVEREST病毒适应性基准上，仅用序列信息就接近结构感知方法。

Conclusion: Proust在计算效率和性能上达到了理想平衡点，既保持了因果模型的生成能力，又获得了接近MLMs的预测性能。可解释性分析显示位置熵方差能预测检索增强的效果，这些洞察可指导测试时扩展等能力。

Abstract: Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $ρ= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference

</details>


### [347] [Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models](https://arxiv.org/abs/2602.01849)
*Ziwei Luo,Ziqi Jin,Lei Wang,Lidong Bing,Thomas B. Schön*

Main category: cs.LG

TL;DR: 提出自奖励序列蒙特卡洛（SMC）算法，通过并行扩散过程交互和轨迹级置信度作为自奖励信号，提升掩码扩散语言模型的采样质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散语言模型（MDLMs）主要依赖基于置信度的采样策略，只保留每步预测置信度最高的词元。这限制了生成过程，使其陷入对噪声敏感、贪婪的解码范式，导致可能路径的多样性不可避免的崩溃。

Method: 提出自奖励序列蒙特卡洛算法：1）并行启动多个交互的扩散过程（粒子）进行轨迹探索；2）引入轨迹级置信度作为自奖励信号，用于分配粒子重要性权重；3）在采样过程中迭代加权和重采样粒子，系统性地引导生成朝向全局置信度高、高质量的样本。

Result: 在多种掩码扩散语言模型和基准测试上验证，无需额外训练或奖励指导即可实现显著改进，有效将并行推理能力转化为采样质量的提升。

Conclusion: 自奖励SMC算法解决了MDLMs采样多样性受限的问题，通过并行扩散过程和轨迹级置信度自奖励机制，实现了更高质量和多样性的文本生成。

Abstract: This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.

</details>


### [348] [FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization](https://arxiv.org/abs/2602.01852)
*Zeyan Wang,Zhengmao Liu,Yongxin Cai,Chi Li,Xiaoying Tang,Jingchao Chen,Zibin Pan,Jing Qiu*

Main category: cs.LG

TL;DR: FUPareto：基于帕累托优化的联邦遗忘框架，通过最小边界偏移损失和零空间投影多梯度下降算法，解决联邦遗忘中遗忘效果、模型效用和多客户端并发遗忘的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在三个关键挑战：1）遗忘目标常损害模型效用或增加成员推理攻击风险；2）遗忘与效用之间存在固有冲突；3）多客户端并发遗忘支持差，梯度冲突会降低遗忘质量。

Method: 提出FUPareto框架：1）引入最小边界偏移损失，通过抑制目标类logit低于最高非目标类logit来提高遗忘效率并降低MIA风险；2）采用帕累托改进步骤保持模型效用；3）执行帕累托扩展保证遗忘，其中集成零空间投影多梯度下降算法解耦梯度冲突，支持多客户端并发遗忘。

Result: 在多种场景下的广泛实验表明，FUPareto在遗忘效果和保留效用方面均优于现有最先进的联邦遗忘方法。

Conclusion: FUPareto通过帕累托增强优化有效解决了联邦遗忘中的关键挑战，实现了高效、公平的多客户端并发遗忘，同时最小化效用损失。

Abstract: Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.

</details>


### [349] [Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal](https://arxiv.org/abs/2602.01877)
*Zichun Wang,Gar Goei Loke,Ruiting Zuo*

Main category: cs.LG

TL;DR: 本文提出A-OVE模型，在自相关不确定性下直接优化样本外性能，应用于带交易成本的组合优化问题，相比传统预测-优化方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统"先估计后优化"方法在数据驱动优化中存在局限性，特别是在自相关不确定性（VARMA过程）下，需要直接优化样本外性能的新方法。

Method: 提出自相关优化-通过-估计（A-OVE）模型，通过充分统计量获得样本外最优解，并开发递归形式计算充分统计量。

Result: A-OVE在带交易成本的组合优化问题中，相对于完美信息基准获得低遗憾值，优于预测-优化机器学习基准。有趣的是，更高准确率的机器学习模型可能决策质量更差。

Conclusion: A-OVE模型在自相关不确定性下有效，即使存在小的模型误设也能保持性能，为数据驱动优化提供了有前景的替代方案。

Abstract: Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.

</details>


### [350] [Internal Flow Signatures for Self-Checking and Refinement in LLMs](https://arxiv.org/abs/2602.01897)
*Sungheon Jeong,Sanggeon Yun,Ryozo Masukawa,Wenjun Haung,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出内部流签名方法，通过监控LLM内部深度动态来检测和修正不忠实生成，实现无需外部验证的自我检查


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能生成与上下文不符的流畅回答，现有方法依赖外部验证或生成后的单独判断，需要一种基于内部决策动态的自我检查机制

Method: 引入内部流签名：在固定块间监控边界审计决策形成，通过偏置中心监控稳定token级动态，在移动读取对齐子空间中总结轨迹，使用正交传输对齐相邻窗口，提取深度可比较的传输步长、转向角和子空间漂移摘要，训练轻量GRU验证器进行自我检查

Result: 方法能够检测不忠实生成，定位问题深度事件，并支持针对性修正：回滚到问题token并在识别块处钳制异常传输步长，同时保留正交残差

Conclusion: 内部流签名提供了基于内部决策动态的可操作定位和低开销自我检查，无需修改基础模型，实现了从检测到修正的完整流程

Abstract: Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \emph{Code is available at} \texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.

</details>


### [351] [Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs](https://arxiv.org/abs/2602.01914)
*Wenbo Pan,Zhichao Liu,Xianlong Wang,Haining Yu,Xiaohua Jia*

Main category: cs.LG

TL;DR: FlashTrace：一种高效的多token归因方法，通过跨token聚合和递归归因机制，解决长上下文和多步推理中现有归因方法的效率瓶颈和忠实度下降问题。


<details>
  <summary>Details</summary>
Motivation: 随着现代LLM越来越多地依赖扩展推理链，现有token归因方法面临两个关键挑战：1）效率瓶颈 - 在长度为N的上下文中归因M个token需要O(M*N)操作，长上下文归因极慢；2）忠实度下降 - 中间推理token吸收归因质量，阻止重要性传播回原始输入。

Method: 提出FlashTrace方法：1）采用跨token聚合，在单次计算中为多token目标计算归因；2）设计递归归因机制，通过中间推理链将重要性追溯回源输入。

Result: 在长上下文检索（RULER）和多步推理（MATH、MorehopQA）任务上的实验表明，FlashTrace相比现有基线实现130倍以上的加速，同时保持更高的忠实度。递归归因分析显示，即使单次递归跳转也能通过推理链追踪重要性来提升忠实度。

Conclusion: FlashTrace通过高效的跨token聚合和递归归因机制，有效解决了长上下文和多步推理场景下的归因效率和忠实度问题，为语言模型的解释性提供了更实用的解决方案。

Abstract: Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.

</details>


### [352] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

TL;DR: 使用预训练的视觉语言模型（VLM）作为自动评估器，识别并优先处理智能体经验中有前景的子轨迹，从而提升强化学习中的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型和视觉语言模型已被整合到强化学习的多个组件中，但作为存储和重用经验核心组件的回放缓冲区尚未被探索。本文旨在填补这一空白，利用VLM指导回放缓冲区中经验的优先级排序。

Method: 使用冻结的预训练VLM（无需微调）作为自动评估器，从智能体经验中识别并优先处理有前景的子轨迹。该方法适用于游戏和机器人等场景，涵盖离散和连续领域。

Result: 在游戏和机器人场景中，使用该优先级排序方法的智能体相比先前方法，平均成功率提高了11-52%，样本效率提升了19-45%。

Conclusion: 利用VLM指导回放缓冲区优先级排序是一种有效的方法，能够显著提升强化学习的样本效率和性能，为VLM在强化学习中的应用开辟了新方向。

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [353] [PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks](https://arxiv.org/abs/2602.01920)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: 提出PIMPC-GNN框架，通过物理启发的多相共识机制解决图神经网络在类别不平衡节点分类中的问题，显著提升少数类召回率和平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在类别不平衡设置中表现不佳，少数类别代表性不足导致预测偏向多数类，需要有效机制来处理这种不平衡问题。

Method: 结合三种互补动力学：热力学扩散传播少数类标签捕获长程依赖，Kuramoto同步通过振荡共识对齐少数节点，谱嵌入通过结构正则化分离类别。使用类别自适应集成加权和结合平衡交叉熵与物理约束的不平衡感知损失进行训练。

Result: 在5个基准数据集和5-100的不平衡比例下，PIMPC-GNN优于16个最先进的基线方法，少数类召回率提升高达12.7%，平衡准确率提升高达8.3%。

Conclusion: PIMPC-GNN框架不仅提供了实证改进，还为图学习中的共识动力学提供了可解释的见解，有效解决了类别不平衡问题。

Abstract: Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integrates three complementary dynamics: (i) thermodynamic diffusion, which spreads minority labels to capture long-range dependencies, (ii) Kuramoto synchronisation, which aligns minority nodes through oscillatory consensus, and (iii) spectral embedding, which separates classes via structural regularisation. These perspectives are combined through class-adaptive ensemble weighting and trained with an imbalance-aware loss that couples balanced cross-entropy with physics-based constraints. Across five benchmark datasets and imbalance ratios from 5-100, PIMPC-GNN outperforms 16 state-of-the-art baselines, achieving notable gains in minority-class recall (up to +12.7\%) and balanced accuracy (up to +8.3\%). Beyond empirical improvements, the framework also provides interpretable insights into consensus dynamics in graph learning. The code is available at \texttt{https://github.com/afofanah/PIMPC-GNN}.

</details>


### [354] [Embedding Learning on Multiplex Networks for Link Prediction](https://arxiv.org/abs/2602.01922)
*Orell Trautmann,Olaf Wolkenhauer,Clémence Réda*

Main category: cs.LG

TL;DR: 这篇综述论文回顾了多路网络嵌入学习方法在链接预测任务中的应用，提出了新的分类体系，并解决了评估的公平性和可重复性问题。


<details>
  <summary>Details</summary>
Motivation: 随着网络复杂性增加（连接数量和交互类型增多），多路网络的嵌入学习变得越来越具有挑战性。现有研究缺乏统一的分类体系和公平的评估方法，特别是在有向多路网络上的评估存在问题。

Method: 1. 提出了细化的分类体系，根据嵌入类型和技术对模型进行分类比较；2. 解决了多路网络嵌入学习在链接预测任务中的可重复性和公平评估问题；3. 针对有向多路网络提出了新颖且公平的测试程序。

Result: 建立了多路网络嵌入学习的系统分类框架，提供了公平评估的解决方案，特别是有向网络的评估方法，为后续研究提供了重要基础。

Conclusion: 这篇综述为开发更高效、可处理的多路网络嵌入学习方法及其公平评估迈出了关键一步，提供了模型评估指南，并对当前可用的挑战和工具提供了有见地的视角。

Abstract: Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.

</details>


### [355] [Bayesian Integration of Nonlinear Incomplete Clinical Data](https://arxiv.org/abs/2602.01924)
*Lucía González-Zamorano,Nuria Balbás-Esteban,Vanessa Gómez-Verdejo,Albert Belenguer-Llorens,Carlos Sevilla-Salcedo*

Main category: cs.LG

TL;DR: BIONIC是一个贝叶斯多模态临床数据集成框架，能够处理高维异构数据和结构化缺失，通过生成-判别联合架构实现鲁棒预测和可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 临床多模态数据具有高维度、异构表示和结构化缺失的特点，这给预测建模、数据集成和可解释性带来了重大挑战。现有方法难以有效处理这些复杂特性。

Method: 提出BIONIC（贝叶斯非线性不完整临床数据集成）框架，采用联合生成-判别潜变量架构，集成异构多模态数据。使用预训练嵌入处理医学图像和临床文本等复杂模态，将结构化临床变量直接纳入贝叶斯多模态公式，显式建模模态级和变量级缺失以及缺失标签。

Result: 在三个多模态临床和生物医学数据集上评估，相比代表性多模态基线方法，BIONIC展现出强大且一致的判别性能，特别是在不完整数据场景下表现优异。

Conclusion: BIONIC不仅提供高预测准确性，还通过其潜变量结构提供内在可解释性，支持模态相关性的群体级分析和临床有意义的洞察，为多模态临床数据分析提供了统一框架。

Abstract: Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.

</details>


### [356] [COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation](https://arxiv.org/abs/2602.01935)
*Annabelle Sujun Tang,Christopher Priebe,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: COLT：一个轻量级多LLM协作框架，通过共享MCTS树实现编译器优化，用小模型为主、大模型为辅的策略匹配单一大模型的性能


<details>
  <summary>Details</summary>
Motivation: 模型服务成本主导AI系统，编译器优化对可扩展部署至关重要。现有方法使用单一大型语言模型指导编译器搜索成本高昂，而小模型单独使用可靠性不足。本文探讨多LLM协作能否匹配或超越单一大型模型的性能

Method: 提出COLT框架：1) 使用共享MCTS树作为多LLM协作基础，重用转换前缀和跨模型价值传播；2) 每次迭代中，执行LLM提出联合动作（编译器转换，下一个查询的模型）；3) 引入模型感知树策略，偏向小模型同时保持探索；4) 当搜索出现持续退化时，升级到最大模型

Result: 未在摘要中明确说明具体实验结果，但框架设计旨在通过多LLM协作匹配或超越单一大型模型的编译器优化性能，同时降低计算成本

Conclusion: COLT框架通过轻量级多LLM协作机制，在编译器优化中实现了高效的成本-性能平衡，避免了传统多智能体系统的复杂架构，为AI系统部署提供了可扩展的解决方案

Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.

</details>


### [357] [PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting](https://arxiv.org/abs/2602.01936)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: MCPST是一个用于少样本交通预测的多阶段共识时空框架，通过建模交通动态的扩散、同步和谱嵌入，结合自适应共识机制和结构化元学习，在数据稀缺的跨域场景中实现准确预测。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中，跨域数据稀缺场景下的准确交通流预测是一个基本挑战。有限的历史数据阻碍了模型训练和泛化能力，而城市移动网络的复杂时空依赖性和非线性动态进一步加剧了不同城市间少样本学习的难度。

Method: 提出MCPST框架，包含三个核心创新：1）多阶段引擎，通过扩散、同步和谱嵌入建模交通动态；2）自适应共识机制，动态融合阶段特定预测并强制一致性；3）结构化元学习策略，用于快速适应新城市的最小数据需求。

Result: 在四个真实世界数据集上的实验表明，MCPST在时空图学习方法、动态图迁移学习方法、基于提示的时空预测方法和跨域少样本设置中，优于14种最先进方法，提高了预测准确性，同时减少了所需训练数据并提供了可解释的见解。

Conclusion: MCPST通过将交通预测重新概念化为多阶段共识学习问题，为数据稀缺的跨域交通预测提供了有效的解决方案，具有理论保证和实际性能优势。

Abstract: Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.

</details>


### [358] [T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation](https://arxiv.org/abs/2602.01937)
*Suhan Guo,Bingxu Wang,Shaodan Zhang,Furao Shen*

Main category: cs.LG

TL;DR: T-LLM是一个时间序列预测框架，通过时间蒸馏将轻量级时间教师的预测能力转移到通用大语言模型中，使LLM获得时间序列预测能力，无需推理时的额外时间模块。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据与底层过程的演化紧密相关，只能随真实时间积累，这限制了仅靠规模驱动的预训练效果。现有方法主要依赖表示层对齐或推理时的时间模块，未能明确教会LLM预测行为。

Method: 提出T-LLM时间蒸馏框架，在训练期间将预测行为从轻量级时间教师转移到通用LLM。教师结合趋势建模和频域分析提供结构化时间监督，推理时完全移除教师，仅保留LLM作为预测模型。

Result: 在基准数据集和传染病预测任务上的实验表明，T-LLM在全样本、少样本和零样本设置下始终优于现有基于LLM的预测方法，同时实现了简单高效的部署流程。

Conclusion: T-LLM通过时间蒸馏成功将时间序列预测能力赋予通用大语言模型，解决了时间约束带来的挑战，在多种设置下表现出优越性能，为LLM在时间序列预测领域的应用提供了有效方案。

Abstract: Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.

</details>


### [359] [Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity](https://arxiv.org/abs/2602.01949)
*Leonardo Stoppani,Davide Bacciu,Shahab Mokarizadeh*

Main category: cs.LG

TL;DR: 提出多样性评分(DS)量化布局多样性，引入边界交叉注意力(BCA)提升几何一致性，揭示真实性与多样性之间的权衡


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在自动平面图生成中过度优化FID等感知指标，导致设计多样性有限，且缺乏对几何一致性的有效控制

Method: 提出多样性评分(DS)来量化约束条件下的布局多样性；引入边界交叉注意力(BCA)模块，使模型能够以建筑边界为条件进行生成

Result: BCA显著提升边界贴合度；长时间训练导致多样性崩溃但FID无法诊断；揭示真实性与多样性之间的关键权衡；OOD评估显示模型对数据集先验的依赖

Conclusion: 建筑设计中需要明确平衡保真度、多样性和泛化能力的生成系统，当前方法在真实性与多样性之间存在权衡，需要更全面的评估指标

Abstract: Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fréchet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.

</details>


### [360] [Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation](https://arxiv.org/abs/2602.01956)
*Seonghyeon Park,Jewon Yeom,Jaewon Sok,Jeongjae Park,Heejun Kim,Taesup Kim*

Main category: cs.LG

TL;DR: 提出使用小型草稿模型高效估计LLM中词元级认知不确定性的框架，避免昂贵的大模型集成，在保持低推理成本的同时实现与重扰动方法相当的幻觉检测性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的不确定性量化对减少幻觉和安全关键部署至关重要，但通过深度集成方法估计认知不确定性在现代模型规模下计算成本过高。

Method: 基于偏差-方差分解理论框架，使用草稿模型通过Jensen-Shannon散度（方差代理）和KL散度（偏差代理）近似不确定性。引入在线随机蒸馏高效近似目标聚合，以及数据多样性草稿策略增强草稿多样性。

Result: 在GSM8K数据集上，方法将估计误差（RMSE）降低高达37%，幻觉检测性能与TokUR等重扰动方法相当，同时推理成本可忽略不计。

Conclusion: 该框架为不确定性感知的LLM部署提供了实用解决方案，通过小型草稿模型高效估计认知不确定性，平衡了准确性和计算效率。

Abstract: Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.

</details>


### [361] [Grounding Generated Videos in Feasible Plans via World Models](https://arxiv.org/abs/2602.01960)
*Christos Ziakas,Amir Bar,Alessandra Russo*

Main category: cs.LG

TL;DR: GVP-WM：通过世界模型将视频生成计划转化为可行动作序列的方法，解决视频计划违反物理约束的问题


<details>
  <summary>Details</summary>
Motivation: 大规模视频生成模型作为零样本视觉规划器显示出潜力，但生成的视频计划经常违反时间一致性和物理约束，导致映射到可执行动作时失败

Method: 提出GVP-WM方法：首先生成视频计划，然后通过视频引导的潜在共位法将视频指导投影到动态可行的潜在轨迹流形上，将接地问题表述为目标条件潜在空间轨迹优化问题

Result: GVP-WM能够从违反物理约束的零样本图像到视频生成和运动模糊视频中恢复可行的长时程计划，在导航和操作模拟任务中表现良好

Conclusion: GVP-WM通过世界模型将视频生成计划接地到可行动作序列，有效解决了视频计划违反物理约束的问题，提高了规划的可执行性

Abstract: Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.

</details>


### [362] [Zero-Shot Off-Policy Learning](https://arxiv.org/abs/2602.01962)
*Arip Asadulaev,Maksim Bobrin,Salem Lahlou,Dmitry Dylov,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 该论文提出了一种零样本强化学习方法，通过建立后继度量与平稳密度比的理论联系，实现了在固定数据集上的最优策略学习，无需额外训练即可适应新任务。


<details>
  <summary>Details</summary>
Motivation: 离策略学习面临分布偏移和价值函数高估偏差的挑战，在零样本强化学习中尤为突出。本文旨在解决零样本设置下的离策略问题，实现无需额外训练即可适应新任务的目标。

Method: 发现了后继度量与平稳密度比的理论联系，利用这一洞察设计算法推断最优重要性采样比，实现平稳分布校正，能够为任何任务即时生成最优策略。该方法可无缝集成到前向-后向表示框架中。

Result: 在SMPL Humanoid的运动跟踪任务、ExoRL的连续控制任务以及长时域OGBench任务上进行了基准测试，展示了方法的有效性。实现了训练免费机制下的快速任务适应。

Conclusion: 这项工作在离策略学习和零样本适应之间建立了桥梁，为两个研究领域都带来了益处，提供了一种无需额外训练即可适应新任务的通用解决方案。

Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.

</details>


### [363] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

TL;DR: 提出了一种新型的LLM智能体自我进化框架，通过对比反思策略总结错误模式，并通过自我巩固机制将文本经验蒸馏为可学习参数，实现长期进化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体通常是静态系统，缺乏通过终身交互进化的能力。现有方法主要依赖检索成功轨迹作为演示，但存在两个关键局限：1) 只关注成功而忽略了失败尝试中的教学价值；2) 持续积累文本经验会增加检索时间、引入噪声并耗尽上下文窗口。

Method: 提出一个自我进化框架，包含两个互补机制：1) 对比反思策略，明确总结易错模式并捕捉可重用见解；2) 自我巩固机制，将非参数化文本经验蒸馏为紧凑的可学习参数，使智能体能够将广泛历史经验内化到潜在空间。

Result: 大量实验证明了该方法在长期智能体进化中的优势。

Conclusion: 该框架通过利用失败经验和参数化学习，有效解决了现有LLM智能体进化方法的局限性，实现了更高效的长期自我进化能力。

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [364] [IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs](https://arxiv.org/abs/2602.01975)
*Meng Li,Peisong Wang,Yuantian Shao,Qinghao Hu,Hongjian Fang,Yifan Zhang,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: IntraSlice：一种基于模块内PCA压缩剪枝的框架，通过近似PCA方法实现无额外参数的矩阵融合，结合全局剪枝比例估计器，在保持性能的同时实现高效模型压缩。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署面临巨大规模挑战，结构化剪枝虽能加速但导致性能显著下降。现有PCA剪枝方法仅在模块间应用，引入额外参数且残差连接会严重破坏激活分布。

Method: 提出IntraSlice框架，采用模块内块级PCA压缩剪枝。利用Transformer模块结构特性设计近似PCA方法，其变换矩阵可完全融合到模型中无需额外参数。引入基于PCA的全局剪枝比例估计器，在传统模块重要性基础上进一步考虑压缩激活分布。

Result: 在Llama2、Llama3和Phi系列模型及多种语言基准测试中验证，实验结果表明在相同压缩比或推理速度下，相比现有基线方法获得更优的压缩性能。

Conclusion: IntraSlice通过模块内PCA压缩剪枝有效解决了现有方法引入额外参数和破坏激活分布的问题，实现了高效且性能保持的模型压缩。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.

</details>


### [365] [FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning](https://arxiv.org/abs/2602.01976)
*Hongwei Yan,Guanglong Sun,Kanglei Zhou,Qian Li,Liyuan Wang,Yi Zhong*

Main category: cs.LG

TL;DR: FlyPrompt是一个受果蝇分层记忆系统启发的通用持续学习框架，通过专家路由和专家能力改进来解决单次非平稳数据流学习问题，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有持续参数高效调优方法通常依赖多轮训练和明确任务边界，不适用于通用持续学习场景。需要解决两个核心挑战：如何为演化数据分配专家参数，以及如何在有限监督下提升表示能力。

Method: 受果蝇稀疏扩展和模块化集成记忆系统启发，FlyPrompt将通用持续学习分解为专家路由和专家能力改进两个子问题。引入随机扩展分析路由器进行实例级专家激活，并使用输出头的时序集成动态适应决策边界。

Result: 在CIFAR-100、ImageNet-R和CUB-200数据集上分别实现了11.23%、12.43%和7.62%的性能提升，显著优于现有最先进方法。

Conclusion: FlyPrompt通过脑启发设计有效解决了通用持续学习中的参数分配和表示能力挑战，为单次非平稳数据流学习提供了有效解决方案，代码已开源。

Abstract: General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.

</details>


### [366] [SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2602.01990)
*Zhen-Hao Xie,Jun-Tao Tang,Yu-Cheng Shi,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.LG

TL;DR: SAME方法通过正交子空间分解稳定专家路由，利用历史输入协方差进行曲率感知缩放防止专家漂移，在无需回放的情况下实现多模态持续指令调优的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型需要持续扩展能力，但现有稀疏专家路由方法存在两个问题：1) 路由器漂移 - 专家选择随时间变得不一致；2) 专家漂移 - 共享专家被新任务覆盖而丧失原有功能。

Method: 提出SAME方法：1) 通过将路由动态分解为正交子空间，仅更新任务相关方向来稳定专家选择；2) 利用历史输入协方差进行曲率感知缩放来调节专家更新；3) 引入自适应专家激活机制，在训练时冻结选定专家以减少冗余计算和跨任务干扰。

Result: 大量实验证明SAME在多模态持续指令调优任务上达到了最先进的性能。

Conclusion: SAME通过稳定专家路由和防止专家漂移，有效解决了多模态持续学习中的路由器漂移和专家漂移问题，实现了更好的持续学习性能。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.

</details>


### [367] [Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations](https://arxiv.org/abs/2602.01996)
*Theologos Anthimopoulos,Milad Kokhazadeh,Vasilios Kelefouras,Benjamin Himpel,Georgios Keramidas*

Main category: cs.LG

TL;DR: 本文提出了一种针对RISC-V处理器的端到端低秩分解设计空间探索方法，用于优化DNN全连接层，通过Tensor Train分解和编译器优化实现3-8倍加速。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的RISC-V平台上部署深度神经网络面临挑战，全连接层的高计算和内存需求主导资源消耗。低秩分解虽然能压缩全连接层，但其设计空间庞大，涉及FLOPs、内存大小、推理时间和准确性的复杂权衡，使得优化过程复杂耗时。

Method: 提出端到端低秩分解设计空间探索方法和专用设计工具，使用TensorFlow T3F库的Tensor Train分解，通过排除低效分解形状和在RISC-V架构上推理性能差的方案来剪枝设计空间，然后应用编译器优化提升自定义T3F层性能。

Result: TT分解层平均比IREE快3倍，比Pluto快8倍（相同压缩模型下）。该方法为RISC-V架构的边缘和嵌入式设备部署DNN提供了高效解决方案。

Conclusion: 该工作为在RISC-V架构的边缘和嵌入式设备上高效部署DNN提供了有效的低秩分解优化方法，通过系统化的设计空间探索和编译器优化显著提升了推理性能。

Abstract: Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.

</details>


### [368] [On the Limits of Layer Pruning for Generative Reasoning in LLMs](https://arxiv.org/abs/2602.01997)
*Safal Shrestha,Anubhav Shrestha,Aadim Nepal,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: 层剪枝压缩大语言模型在分类任务上表现良好，但在生成式推理任务上严重退化，特别是多步推理任务。通过监督微调结合自生成响应可部分恢复性能，但生成推理的恢复仍有限制，主要适用于低剪枝率。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝技术虽然能在分类任务上保持性能，但在生成式推理任务上表现严重退化。需要研究深度减少对多步推理任务的影响，以及在有限后训练资源下的有效缓解策略。

Method: 系统研究多个模型家族，分析深度减少对多步推理任务的影响。在现实后训练约束下（无预训练规模数据或计算），评估基于监督微调与自生成响应的简单缓解策略。

Result: 分类任务可恢复高达90%基线性能，生成式基准测试相比先前后剪枝技术提升20-30个百分点。但生成推理的恢复仍有限制，主要适用于低剪枝率。

Conclusion: 层剪枝对生成推理存在实际限制，深度减少主要在低剪枝率下有效。为约束后训练机制下的深度减少应用提供指导。

Abstract: Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.

</details>


### [369] [Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs](https://arxiv.org/abs/2602.02001)
*Yoonjun Cho,Dongjae Jeon,Soeun Kim,Moongyu Jeon,Albert No*

Main category: cs.LG

TL;DR: SRR提出结构化残差重建框架，在量化误差重建中保留权重的主要奇异子空间，优化秩分配策略，并支持量化参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 现有量化误差重建方法将全部秩预算用于误差重建，当权重具有内在低秩结构且量化破坏主要方向时，这种方法不是最优的。需要更智能的秩分配策略来平衡量化误差重建和权重结构保持。

Method: 提出结构化残差重建(SRR)：1) 保留激活缩放权重的前k个奇异子空间；2) 只量化残差部分；3) 用剩余的r-k秩进行误差重建。推导理论指导的k选择标准，平衡量化暴露能量和秩约束下的不可恢复误差。该方法还自然支持量化参数高效微调(QPEFT)，并通过梯度缩放沿保留方向稳定微调。

Result: 实验表明，在多种模型和量化设置下，SRR在PTQ中持续降低困惑度，在2位QPEFT下在GLUE基准上平均获得5.9个百分点的提升。

Conclusion: SRR通过结构化秩分配策略，在量化误差重建中更好地平衡了权重结构保持和误差校正，为量化模型提供了更优的性能，并支持高效的量化微调。

Abstract: Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\mathbf{Q} + \mathbf{L}\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.

</details>


### [370] [Logic-Guided Vector Fields for Constrained Generative Modeling](https://arxiv.org/abs/2602.02009)
*Ali Baheri*

Main category: cs.LG

TL;DR: LGVF是一个神经符号框架，通过逻辑约束引导的向量场将符号知识注入流匹配生成模型，在训练和推理时都使用逻辑约束来减少违反约束的情况。


<details>
  <summary>Details</summary>
Motivation: 神经符号系统旨在结合符号逻辑的表达结构和神经学习的灵活性，但生成模型通常缺乏在生成时强制执行声明性约束的机制。

Method: 提出逻辑引导向量场(LGVF)框架，包含两个互补机制：(1)训练时逻辑损失，惩罚连续流轨迹上的约束违反，权重强调目标分布附近的正确性；(2)推理时调整，使用约束梯度引导采样，作为对学习动态的轻量级逻辑修正。

Result: 在三个约束生成案例研究中，LGVF相比标准流匹配将约束违反减少了59-82%，在线性和环形设置中提高了分布保真度(MMD)，在多障碍设置中观察到满足度-保真度权衡。

Conclusion: LGVF不仅实现了定量改进，还产生了表现出紧急避障行为的约束感知向量场，无需显式路径规划即可引导样本绕过禁止区域。

Abstract: Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.

</details>


### [371] [Robust Domain Generalization under Divergent Marginal and Conditional Distributions](https://arxiv.org/abs/2602.02015)
*Jewon Yeom,Kyubyung Chae,Hyunggyu Lim,Yoonna Oh,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 提出统一框架处理领域泛化中的复合分布偏移（边际和条件分布同时变化），通过分解风险界限和元学习实现鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法主要关注条件分布偏移（P(X|Y)变化），假设P(Y)稳定，但现实多领域场景常同时存在边际标签分布P(Y)和条件分布P(X|Y)的复合偏移，需要更全面的解决方案。

Method: 1) 推导新颖的风险界限，将联合分布分解为边际和条件分量，明确表征两种偏移来源的风险差距；2) 设计元学习过程，在可见领域上最小化和验证该风险界限，确保对未见领域的强泛化能力。

Result: 方法不仅在传统领域泛化基准上达到最先进性能，在具有显著边际和条件偏移的多领域长尾识别挑战性场景中也表现出色。

Conclusion: 提出的统一框架能有效处理复合分布偏移，通过明确分解和优化边际与条件分布风险，实现了对未见领域的鲁棒泛化，适用于更现实的领域泛化场景。

Abstract: Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.

</details>


### [372] [DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers](https://arxiv.org/abs/2602.02016)
*Ionut-Vlad Modoranu,Philip Zmushko,Erik Schultheis,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: DASH通过张量堆叠和牛顿-DB迭代等新技术，显著加速了Shampoo二阶优化器的计算速度，同时保持收敛性能


<details>
  <summary>Details</summary>
Motivation: Shampoo作为领先的近似二阶优化器，在模型性能方面表现出色（如赢得MLCommons竞赛、产生更易压缩的模型），但其计算成本高昂，导致显著的性能下降。需要解决这一计算瓶颈

Method: 提出DASH（分布式加速Shampoo），采用两种新技术：1）将预处理器块堆叠成3D张量以提高GPU利用率；2）引入牛顿-DB迭代和切比雪夫多项式近似来加速计算Shampoo所需的逆矩阵根

Result: GPU感知实现相比优化后的分布式Shampoo实现达到4.83倍加速；牛顿-DB在所有测试方法中获得每次迭代的最低验证困惑度；同时首次深入分析了矩阵缩放对Shampoo收敛的关键影响

Conclusion: DASH通过算法创新和GPU优化，显著降低了Shampoo的计算成本，使其在实际应用中更加可行，同时保持了二阶优化器的性能优势

Abstract: Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \method (for \textbf{D}istributed \textbf{A}ccelerated \textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.

</details>


### [373] [On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems](https://arxiv.org/abs/2602.02045)
*Yiming Yang,Xiaoyuan Cheng,Yi He,Kaiyu Li,Wenxuan Yuan,Zhuo Sun*

Main category: cs.LG

TL;DR: 本文分析了扩散模型在贝叶斯逆问题中的稳定性与鲁棒性问题，提出了鲁棒扩散后验采样方法来解决似然函数不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已成为贝叶斯逆问题的强大先验，但现有扩散求解器依赖于假定的观测似然函数，而似然函数与恢复质量之间的关系不明确。同时，当假定的似然函数与真实数据生成过程不匹配时，扩散求解器缺乏鲁棒性，这会降低性能。

Method: 本文首先通过表征后验近似误差和证明扩散求解器的稳定性来建立理论框架。然后提出了一种简单有效的解决方案——鲁棒扩散后验采样，该方法具有可证明的鲁棒性，并与现有的基于梯度的后验采样器兼容。

Result: 在科学逆问题和自然图像任务上的实证结果表明，该方法在具有挑战性的似然函数错误设定下具有有效性和鲁棒性，显示出持续的性能改进。

Conclusion: 本文填补了扩散模型在贝叶斯逆问题中稳定性理论分析的空白，揭示了扩散求解器的鲁棒性不足问题，并提出了有效的鲁棒扩散后验采样方法，为处理似然函数不匹配情况提供了理论保证和实践解决方案。

Abstract: Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \emph{robust diffusion posterior sampling}, which is provably \emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.

</details>


### [374] [Dissecting Outlier Dynamics in LLM NVFP4 Pretraining](https://arxiv.org/abs/2602.02047)
*Peijie Dong,Ruibo Fan,Yuechen Tao,Di Mou,Wenhu Hu,Zhenheng Tang,Yinghao Yu,Jiamang Wang,Wenbo Su,Guodong Yang,Liping Zhang,Xiaowen Chu,Baochun Li,Bo Li*

Main category: cs.LG

TL;DR: 该研究分析了NVFP4低精度训练中的异常值动态，发现Softmax Attention、Linear Attention和SwiGLU等组件易产生异常值，并提出Hot-Channel Patch补偿机制，将BF16的损失差距从0.94%降至0.58%。


<details>
  <summary>Details</summary>
Motivation: 使用4位算术训练大型语言模型能提高吞吐量和内存效率，但FP4的动态范围有限增加了对异常值的敏感性。虽然NVFP4通过分层微缩放减轻量化误差，但与BF16相比仍存在持续的损失差距，需要深入分析异常值动态并改进量化方法。

Method: 1) 对NVFP4预训练期间异常值动态进行纵向分析，研究其定位、成因和演化；2) 发现Softmax Attention、Linear Attention和SwiGLU等组件是异常值主要来源；3) 提出Hot-Channel Patch在线补偿机制，识别热通道并重新注入残差；4) 开发CHON训练配方，结合HCP和后QK操作保护。

Result: 在GLA-1.3B模型上训练60B token，CHON将NVFP4与BF16的损失差距从0.94%降低到0.58%，同时保持下游任务准确率。分析显示异常值从训练早期的瞬时尖峰演变为后期少量持久的热通道。

Conclusion: 该研究通过深入分析NVFP4训练中的异常值动态，识别了关键敏感组件和演化模式，提出的Hot-Channel Patch补偿机制有效减少了量化损失，为低精度训练提供了实用的改进方案。

Abstract: Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with "post-QK" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.

</details>


### [375] [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055)
*Nan Qiao,Sheng Yue*

Main category: cs.LG

TL;DR: FORLER：一种离线联邦强化学习方法，通过服务器端的Q-ensemble聚合和设备端的actor修正，解决数据异构和质量差导致的策略污染问题，提供安全策略改进保证。


<details>
  <summary>Details</summary>
Motivation: 物联网系统中，在线联邦强化学习存在风险和成本问题，而离线联邦强化学习在数据质量差、异构情况下容易陷入局部最优，导致策略污染问题。

Method: 1) 服务器端：Q-ensemble聚合，鲁棒合并设备Q函数，减轻策略污染，将计算负担从资源受限设备转移；2) 设备端：actor修正，通过零阶搜索高Q动作和定制正则化器丰富策略梯度；3) δ-周期性策略减少本地计算。

Result: 理论分析提供了安全策略改进的性能保证。大量实验表明，FORLER在不同数据质量和异构性下始终优于强基线方法。

Conclusion: FORLER有效解决了离线联邦强化学习中的策略污染问题，通过创新的服务器聚合和设备端修正机制，在保护隐私的同时实现了鲁棒的性能提升。

Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.

</details>


### [376] [FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance](https://arxiv.org/abs/2602.02060)
*Hyunsuk Chung,Caren Han,Yerin Choi,Seungyeon Ji,Jinwoo Kim,Eun-Jung Holden,Kyungreem Han*

Main category: cs.LG

TL;DR: FiLoRA：一种指令条件化的参数高效适配框架，通过自然语言指令控制模型对内部特征组的依赖，而不改变任务语义


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型如何依赖特定内部特征组以及这种依赖能否被有意控制尚不清楚。现有研究主要依赖事后分析或特征移除，无法在不改变任务语义的情况下调节特征依赖

Method: FiLoRA（Focus-and-Ignore LoRA）将适配分解为特征组对齐的LoRA模块，并应用指令条件化门控，使自然语言指令作为计算级控制信号而非任务重定义

Result: 在文本-图像和音频-视觉基准测试中，指令条件化门控能一致且因果地改变内部计算，选择性地放大或抑制核心和虚假特征组，而不修改标签空间或训练目标

Conclusion: FiLoRA提供了一种超越相关性驱动学习的原理性机制来调节特征依赖，在虚假特征干预下表现出改进的鲁棒性

Abstract: Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.

</details>


### [377] [Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits](https://arxiv.org/abs/2602.02061)
*Seoungbin Bae,Junyoung Son,Dabeen Lee*

Main category: cs.LG

TL;DR: 提出ACQB算法，通过上下文排队多臂老虎机与多项逻辑反馈框架，利用用户重试行为的隐式反馈来联合优化LLM查询的路由和调度，解决排队拥堵问题。


<details>
  <summary>Details</summary>
Motivation: LLM服务面临两个关键挑战：1) 不满意用户会重试查询，增加服务器积压；2) 显式反馈请求会降低用户体验。现有在线算法忽略了这些挑战。

Method: 提出CQB-MNL框架，建模查询重试和基于上下文的用户偏好学习。开发ACQB算法，结合Thompson采样和衰减率的强制探索，实现高效学习同时保持队列稳定。

Result: ACQB算法在路由方面达到Õ(√t)的累积遗憾，在队列长度方面达到Õ(t^{-1/4})的遗憾。在SPROUT、EmbedLLM和RouterBench数据集上的实验表明，算法持续优于基线方法。

Conclusion: 通过利用用户重试行为的隐式反馈，ACQB算法能有效解决LLM服务的路由和调度问题，在保持队列稳定的同时实现高效学习。

Abstract: Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ for routing and a queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.

</details>


### [378] [BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling](https://arxiv.org/abs/2602.02071)
*Zisheng Ye,Xiaoyu He,Maoyuan Song,Guoliang Qiu,Chao Liao,Chen Wu,Yonggang Sun,Zhichun Li,Xiaoru Xie,Yuanyong Luo,Hu Liu,Pinyan Lu,Heng Liao*

Main category: cs.LG

TL;DR: 提出一种使用8位浮点格式和块感知精度重缩放的软注意力低精度工作流，解决Transformer推理中软注意力成为瓶颈的问题，可减少数据带宽和指数运算单元面积，实现吞吐量翻倍。


<details>
  <summary>Details</summary>
Motivation: 随着量化矩阵乘法加速的性能提升趋于平缓，软注意力操作成为Transformer推理的关键瓶颈。这源于两个硬件限制：1) 矩阵和向量计算核心之间的数据带宽有限；2) 高精度指数运算单元面积成本高。现有低精度方法会导致显著的模型精度损失。

Method: 引入新颖的低精度工作流，采用特定的8位浮点格式(HiF8)和块感知精度重缩放技术。该方法使矩阵乘法输出约束在8位，从而将所需数据移动带宽减半；同时在低精度(8位)下计算指数运算，大幅减少指数运算单元面积。

Result: 在语言模型和多模态模型上的广泛评估证实了该方法的有效性。通过缓解向量计算瓶颈，该方法为在不增加芯片面积的情况下实现端到端推理吞吐量翻倍铺平了道路。

Conclusion: 该工作为未来低精度硬件和软件提供了具体的协同设计路径，通过创新的低精度软注意力计算方法解决了Transformer推理中的关键瓶颈问题。

Abstract: As the performance gains from accelerating quantized matrix multiplication plateau, the softmax operation becomes the critical bottleneck in Transformer inference. This bottleneck stems from two hardware limitations: (1) limited data bandwidth between matrix and vector compute cores, and (2) the significant area cost of high-precision (FP32/16) exponentiation units (EXP2). To address these issues, we introduce a novel low-precision workflow that employs a specific 8-bit floating-point format (HiF8) and block-aware precision rescaling for softmax. Crucially, our algorithmic innovations make low-precision softmax feasible without the significant model accuracy loss that hampers direct low-precision approaches. Specifically, our design (i) halves the required data movement bandwidth by enabling matrix multiplication outputs constrained to 8-bit, and (ii) substantially reduces the EXP2 unit area by computing exponentiations in low (8-bit) precision. Extensive evaluation on language models and multi-modal models confirms the validity of our method. By alleviating the vector computation bottleneck, our work paves the way for doubling end-to-end inference throughput without increasing chip area, and offers a concrete co-design path for future low-precision hardware and software.

</details>


### [379] [Calibrating Adaptive Smoothing Methods for Freeway Traffic Reconstruction](https://arxiv.org/abs/2602.02072)
*Junyi Ji,Derek Gloudemans,Gergely Zachár,Matthew Nice,William Barbour,Daniel B. Work*

Main category: cs.LG

TL;DR: 提出基于PyTorch的Python自适应平滑方法(ASM)实现，支持端到端校准，使用真实世界数据进行参数化核优化，为交通状态重建提供可复现基准。


<details>
  <summary>Details</summary>
Motivation: 自适应平滑方法(ASM)是广泛使用的交通状态重建方法，但缺乏可复现的基准实现和端到端校准方法，需要为交通重建问题提供标准评估指标。

Method: 使用PyTorch实现ASM，将校准问题形式化为参数化核优化问题，利用稀疏雷达传感器网络数据和全状态观测测试床数据进行端到端校准。

Result: 通过速度分布、时空误差分布和空间误差等指标评估重建结果，为交通重建问题提供基准指标，并在多条高速公路上验证了校准方法的可用性。

Conclusion: 该实现具有可复现性，可作为各种高速公路运营任务的基准，同时讨论了通用交通模型校准的可复现性挑战和ASM的局限性。

Abstract: The adaptive smoothing method (ASM) is a widely used approach for traffic state reconstruction. This article presents a Python implementation of ASM, featuring end-to-end calibration using real-world ground truth data. The calibration is formulated as a parameterized kernel optimization problem. The model is calibrated using data from a full-state observation testbed, with input from a sparse radar sensor network. The implementation is developed in PyTorch, enabling integration with various deep learning methods. We evaluate the results in terms of speed distribution, spatio-temporal error distribution, and spatial error to provide benchmark metrics for the traffic reconstruction problem. We further demonstrate the usability of the calibrated method across multiple freeways. Finally, we discuss the challenges of reproducibility in general traffic model calibration and the limitations of ASM. This article is reproducible and can serve as a benchmark for various freeway operation tasks.

</details>


### [380] [AICD Bench: A Challenging Benchmark for AI-Generated Code Detection](https://arxiv.org/abs/2602.02079)
*Daniil Orel,Dilshod Azizov,Indraneil Paul,Yuxia Wang,Iryna Gurevych,Preslav Nakov*

Main category: cs.LG

TL;DR: AICD Bench是一个全面的AI生成代码检测基准，包含200万样本、77个模型、11个模型家族和9种编程语言，提出了三种现实检测任务，评估显示现有检测器性能远低于实际可用水平。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成功能代码的能力增强，引发了关于作者身份、责任和安全的担忧。现有的AI生成代码检测数据集和基准过于狭窄，通常仅限于分布内设置下的二元人机分类，无法满足现实需求。

Method: 构建了AICD Bench基准，包含200万个样本，覆盖77个模型、11个模型家族和9种编程语言，包括最新的推理模型。提出了三种现实检测任务：1) 在语言和领域分布偏移下的鲁棒二元分类；2) 模型家族归属（按架构谱系分组生成器）；3) 细粒度人机分类（人类、机器、混合和对抗性代码）。

Result: 对神经和经典检测器的广泛评估显示，性能远低于实际可用水平，特别是在分布偏移下以及对于混合或对抗性代码的检测效果较差。

Conclusion: AICD Bench作为一个统一且具有挑战性的评估套件发布，旨在推动下一代鲁棒的AI生成代码检测方法的发展。数据和代码已公开可用。

Abstract: Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\emph{2M examples}$, $\emph{77 models}$ across $\emph{11 families}$, and $\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\emph{i}$)~$\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\emph{ii}$)~$\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\emph{iii}$)~$\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.

</details>


### [381] [Learning Half-Spaces from Perturbed Contrastive Examples](https://arxiv.org/abs/2602.02080)
*Aryan Alavi Razavi Ravari,Farnam Mansouri,Yuxin Chen,Valentio Iverson,Adish Singla,Sandra Zilles*

Main category: cs.LG

TL;DR: 该论文研究了在带噪声的对比示例oracle下的学习问题，其中对比示例的质量取决于查询点到决策边界的距离。分析了在固定和随机扰动下的主动和被动学习样本复杂度。


<details>
  <summary>Details</summary>
Motivation: Mansouri等人提出了理想的对比示例oracle，其中对比示例总是距离查询点最近的反例。然而现实场景中对比示例可能存在噪声。本文旨在研究当对比示例被噪声扰动时的学习问题，其中扰动程度由查询点到决策边界的距离决定。

Method: 引入参数化噪声函数f的机制，其中对比示例的扰动程度由f(d)控制，d是查询点到决策边界的距离。研究两种设置：(i) 最大扰动幅度固定，(ii) 随机扰动。针对一维阈值分类器和有界域上均匀分布下的半空间分类器，分析主动和被动学习的样本复杂度。

Result: 在特定条件下，对比示例的存在能够加速学习过程，降低渐近查询复杂度和期望查询复杂度。论文给出了样本复杂度与噪声函数f的依赖关系。

Conclusion: 即使在对比示例存在噪声的情况下，只要噪声函数满足一定条件，对比示例仍然能够显著提升学习效率。这为实际应用中设计有效的对比学习机制提供了理论指导。

Abstract: We study learning under a two-step contrastive example oracle, as introduced by Mansouri et. al. (2025), where each queried (or sampled) labeled example is paired with an additional contrastive example of opposite label. While Mansouri et al. assume an idealized setting, where the contrastive example is at minimum distance of the originally queried/sampled point, we introduce and analyze a mechanism, parameterized by a non-decreasing noise function $f$, under which this ideal contrastive example is perturbed. The amount of perturbation is controlled by $f(d)$, where $d$ is the distance of the queried/sampled point to the decision boundary. Intuitively, this results in higher-quality contrastive examples for points closer to the decision boundary. We study this model in two settings: (i) when the maximum perturbation magnitude is fixed, and (ii) when it is stochastic.
  For one-dimensional thresholds and for half-spaces under the uniform distribution on a bounded domain, we characterize active and passive contrastive sample complexity in dependence on the function $f$. We show that, under certain conditions on $f$, the presence of contrastive examples speeds up learning in terms of asymptotic query complexity and asymptotic expected query complexity.

</details>


### [382] [Active learning from positive and unlabeled examples](https://arxiv.org/abs/2602.02081)
*Farnam Mansouri,Sandra Zilles,Shai Ben-David*

Main category: cs.LG

TL;DR: 首次对主动PU学习的标签复杂度进行理论分析，其中查询标签仅在实例为正且独立硬币投掷成功时才会显示


<details>
  <summary>Details</summary>
Motivation: 受广告和异常检测等应用驱动，研究主动PU学习场景，其中学习者可以自适应地从无标签池中查询实例，但查询标签仅在实例为正且独立硬币投掷成功时才会显示

Method: 提出主动PU学习框架，学习者可以自适应查询无标签池中的实例，但标签反馈机制受限：只有当实例为正且独立硬币投掷成功时才会显示标签，否则不提供任何信息

Result: 首次提供了主动PU学习的标签复杂度理论分析

Conclusion: 该研究填补了主动PU学习理论分析的空白，为广告、异常检测等实际应用中的弱监督学习提供了理论基础

Abstract: Learning from positive and unlabeled data (PU learning) is a weakly supervised variant of binary classification in which the learner receives labels only for (some) positively labeled instances, while all other examples remain unlabeled. Motivated by applications such as advertising and anomaly detection, we study an active PU learning setting where the learner can adaptively query instances from an unlabeled pool, but a queried label is revealed only when the instance is positive and an independent coin flip succeeds; otherwise the learner receives no information. In this paper, we provide the first theoretical analysis of the label complexity of active PU learning.

</details>


### [383] [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098)
*Yannik Schnitzer,Mathias Jackermeier,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 提出一种为多任务强化学习策略在新任务上性能提供高置信度保证的方法，通过组合任务内置信下界和任务间泛化边界来实现。


<details>
  <summary>Details</summary>
Motivation: 现有多任务强化学习方法缺乏形式化的性能保证，这在安全关键场景中部署策略时是不可或缺的。需要为训练时未见的新任务提供可靠性能保证。

Method: 引入新的泛化边界，组合两个部分：(1) 从有限次rollout获得的每个任务的置信下界；(2) 从有限采样任务中获得的任务级泛化。该方法适用于任意未知的任务分布。

Result: 在多种最先进的多任务RL方法上验证，证明该方法在理论上是可靠的，并且在现实样本量下能提供有信息量的保证。

Conclusion: 该方法为多任务强化学习策略在新任务上的性能提供了形式化的高置信度保证，填补了现有方法在安全关键应用中的空白。

Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.

</details>


### [384] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

TL;DR: 该论文通过Tele-Lens探测方法研究LLM的潜在规划能力，发现LLM具有近视视野，主要进行增量推理而非全局规划，并基于此提出增强CoT不确定性估计的方法。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现LLM在CoT出现前已存在潜在规划，这削弱了显式CoT的重要性，但CoT对多步推理任务仍很关键。为了深入理解LLM内部状态与其语言化推理轨迹之间的关系，需要研究LLM的潜在规划能力。

Method: 提出Tele-Lens探测方法，应用于不同任务领域的隐藏状态，研究LLM的潜在规划强度。基于发现的近视视野特性，提出增强CoT不确定性估计的假设，并验证少量CoT位置能有效代表整个路径的不确定性。

Result: 实证结果表明LLM表现出近视视野，主要进行增量转换而非精确的全局规划。验证了少量CoT位置能有效代表整个路径的不确定性，并实现了无需性能下降的CoT旁路自动识别。

Conclusion: LLM具有近视推理特性，利用CoT动态特性可以增强不确定性估计，并实现高效的CoT旁路识别，为理解LLM推理机制提供了新视角。

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [385] [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110)
*Zhongqian Fu,Tianyi Zhao,Kai Han,Hang Zhou,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 本文系统研究了世界模型的量化问题，发现量化对世界模型的影响远超传统精度-比特权衡，包括组量化稳定低比特推理、激活量化效果不一致、编码器-预测器敏感度不对称等独特现象。


<details>
  <summary>Details</summary>
Motivation: 世界模型在紧凑潜在空间中学习环境动态表示，支持规划、预测和推理任务，但计算和内存开销大，需要量化来高效部署。然而，目前对世界模型量化效果的研究还很缺乏。

Method: 使用DINO-WM作为代表性案例，系统评估多种后训练量化方法，包括仅权重量化和联合权重-激活量化。在不同视觉规划任务上进行广泛实验，涵盖多种比特宽度、量化粒度和长达50步的规划视野。

Result: 发现世界模型量化具有独特特性：组量化能稳定低比特推理；激活量化粒度效果不一致；编码器和预测器模块的量化敏感度高度不对称；激进低比特量化会严重破坏规划目标与任务成功的对齐。

Conclusion: 量化在世界模型规划中引发独特的失败模式，研究结果为在严格计算约束下部署量化世界模型提供了实用指导，揭示了超越传统精度-比特权衡的量化效应。

Abstract: World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.

</details>


### [386] [Unifying Masked Diffusion Models with Various Generation Orders and Beyond](https://arxiv.org/abs/2602.02112)
*Chunsan Hong,Sanghyun Lee,Jong Chul Ye*

Main category: cs.LG

TL;DR: 提出OeMDM和LoMDM两种扩散模型，OeMDM统一解释多种生成顺序的扩散过程，LoMDM联合学习生成顺序和扩散主干，在语言建模基准上优于现有离散扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型（MDMs）的生成质量严重依赖生成顺序，先前工作要么硬编码顺序，要么为预训练MDM学习顺序策略，这带来额外成本且可能因两阶段优化而得到次优解。

Method: 提出OeMDM（顺序可表达掩码扩散模型）统一框架解释多种生成顺序的扩散过程；在此基础上提出LoMDM（可学习顺序掩码扩散模型），通过单一目标联合学习生成顺序和扩散主干。

Result: LoMDM在多个语言建模基准上优于各种离散扩散模型，证明联合学习生成顺序和扩散主干的有效性。

Conclusion: OeMDM为多种扩散生成过程提供了统一框架，LoMDM通过端到端联合学习实现了上下文相关的文本生成顺序，在语言生成任务上表现出色。

Abstract: Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretrained MDM, which incurs extra cost and can yield suboptimal solutions due to the two-stage optimization. Motivated by this, we propose order-expressive masked diffusion model (OeMDM) for a broad class of diffusion generative processes with various generation orders, enabling the interpretation of MDM, ARM, and block diffusion in a single framework. Furthermore, building on OeMDM, we introduce learnable-order masked diffusion model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling the diffusion model to generate text in context-dependent ordering. Empirically, we confirm that LoMDM outperforms various discrete diffusion models across multiple language modeling benchmarks.

</details>


### [387] [The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning](https://arxiv.org/abs/2602.02117)
*Youqi Wu,Farzan Farnia*

Main category: cs.LG

TL;DR: 论文将经典的最大熵原理扩展到冯·诺依曼熵，为核机器学习中的VNE最大化提供了博弈论和信息论基础


<details>
  <summary>Details</summary>
Motivation: 冯·诺依曼熵在量子信息论中是基本量，最近在机器学习中被用作核矩阵和核协方差算子的谱多样性度量。然而，在数据驱动背景下，缺乏类似经典最大熵框架的决策论和博弈论解释

Method: 将Grünwald和Dawid的最大熵原理的极小极大公式扩展到冯·诺依曼熵设置，为密度矩阵和迹归一化正半定算子的VNE最大化提供博弈论证明

Result: 该视角为部分信息下的最大VNE解提供了稳健解释，阐明了它们作为谱域中最少承诺推断的作用。通过两个代表性应用展示了该框架的实用性

Conclusion: 提出的最大VNE原理为核学习中的VNE方法提供了统一的信息论基础，将经典最大熵框架成功扩展到量子熵设置

Abstract: Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Grünwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.

</details>


### [388] [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126)
*Junhan Kim,Gukryeol Lee,Seungwoo Son,Jeewook Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: 提出两阶段优化框架改进GPTQ分组量化，通过最小化层重构损失提升LLM低比特量化精度


<details>
  <summary>Details</summary>
Motivation: GPTQ虽然高效但忽略输入统计和组间相关性，导致与最小化层重构损失的目标不匹配

Method: 两阶段优化框架：第一阶段在GPTQ前初始化组尺度以最小化组重构损失；第二阶段冻结GPTQ整数权重，用坐标下降算法和闭式更新规则优化组尺度以最小化层重构损失

Result: 实验表明方法能持续提升分组量化精度，以可忽略的开销获得更高准确率

Conclusion: 提出的两阶段优化框架有效解决了GPTQ忽略输入统计和组间相关性的问题，显著提升了分组量化的精度

Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.

</details>


### [389] [Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics](https://arxiv.org/abs/2602.02128)
*Nima Shoghi,Yuxuan Liu,Yuning Shen,Rob Brekelmans,Pan Li,Quanquan Gu*

Main category: cs.LG

TL;DR: STAR-MD：一种SE(3)等变扩散模型，通过联合时空注意力机制生成微秒级蛋白质轨迹，在ATLAS基准测试中实现SOTA性能，解决了现有生成模型在长时程模拟中的局限性。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟计算成本高，难以达到生物相关时间尺度。现有生成模型因架构限制、误差累积和时空动态建模不足，在长时程生成方面表现不佳。

Method: 提出STAR-MD：可扩展的SE(3)等变扩散模型，采用因果扩散变换器与联合时空注意力机制，高效捕捉复杂时空依赖关系，避免现有方法的内存瓶颈。

Result: 在ATLAS基准测试中，STAR-MD在所有指标上均达到最先进水平，显著改善构象覆盖、结构有效性和动态保真度。能够稳定生成微秒级轨迹，而基线方法完全失败。

Conclusion: STAR-MD的联合时空建模能够在生物相关时间尺度上实现稳健的动力学模拟，为加速探索蛋白质功能开辟了新途径，同时揭示了当前模型在长时程生成方面的严重局限性。

Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.

</details>


### [390] [EvoMU: Evolutionary Machine Unlearning](https://arxiv.org/abs/2602.02139)
*Pawel Batorski,Paul Swoboda*

Main category: cs.LG

TL;DR: EvoMU使用进化搜索自动发现针对特定任务的数据遗忘损失函数，在有限计算资源下实现SotA性能


<details>
  <summary>Details</summary>
Motivation: 机器遗忘需要找到合适的损失函数，但现有方法面临两大挑战：损失函数空间巨大难以搜索，且不存在适用于所有数据集的通用最优损失函数

Method: 采用进化搜索程序自动在巨大的可能遗忘损失函数空间中寻找任务特定的损失函数，使用小型4B参数模型(Qwen3-4B-Thinking)实现

Result: 在TOFU-5%、TOFU-10%、MUSE和WMDP数据集上超越了以往的基于损失的遗忘方法，合成了新颖的遗忘损失函数

Conclusion: EvoMU展示了有限计算资源下AI协同科学家的潜力，能够自动发现针对特定数据集的优化损失函数，无需人工干预

Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.

</details>


### [391] [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143)
*Shubham Toshniwal,Aleksander Ficek,Siddhartha Jain,Wei Du,Vahid Noroozi,Sadegh Mahdavi,Somshubra Majumdar,Igor Gitman*

Main category: cs.LG

TL;DR: 通过强化学习训练小模型实现强大的生成式选择能力，提升并行采样的测试时计算效率


<details>
  <summary>Details</summary>
Motivation: 并行采样可以显著提升LLM推理能力，但受限于Best-of-N选择质量。现有的生成式选择方法（如GenSelect）主要在大模型中表现良好，小模型的选择能力有限，需要找到一种方法让小模型也能获得强大的选择能力

Method: 从大规模数学和代码指令数据集中合成选择任务，筛选出同时包含正确和错误候选解决方案的实例，使用DAPO强化学习训练1.7B参数模型，奖励正确的选择决策

Result: 在数学（AIME24、AIME25、HMMT25）和代码（LiveCodeBench）推理基准测试中，模型持续优于提示和多数投票基线方法，经常接近或超过更大模型的表现。这些增益能够泛化到选择更强模型的输出，尽管训练时只使用了较弱模型的输出

Conclusion: 强化学习是解锁小模型强大生成式选择能力的可扩展方法，能够实现高效的测试时计算扩展

Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.

</details>


### [392] [Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting](https://arxiv.org/abs/2602.02146)
*Sunho Kim,Susik Yoon*

Main category: cs.LG

TL;DR: BTTF框架通过前瞻增强和自校正优化，在保持并行效率的同时提升长期时间序列预测的时序一致性，无需复杂架构即可显著改善预测准确性。


<details>
  <summary>Details</summary>
Motivation: 长期时间序列预测面临并行效率与时序一致性之间的权衡：直接多步预测方法速度快但缺乏时序一致性，迭代多步预测方法保持时序依赖但存在误差累积和推理速度慢的问题。

Method: 提出BTTF框架，通过前瞻增强和自校正优化来增强预测稳定性。该方法不依赖复杂模型架构，而是重新审视基本预测过程，通过集成第二阶段模型（使用初始预测进行增强）来优化基础模型。

Result: BTTF一致地提高了长期预测准确性，缓解了线性预测模型的不稳定性，准确率提升高达58%。即使在第一阶段模型训练条件不理想的情况下，也能实现稳定的改进。

Conclusion: 利用模型生成的预测作为增强手段，即使没有复杂架构，也是提升长期预测能力的一种简单而强大的方法。

Abstract: Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.

</details>


### [393] [ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning](https://arxiv.org/abs/2602.02150)
*Chu Zhao,Enneng Yang,Yuting Liu,Jianzhe Zhao,Guibing Guo*

Main category: cs.LG

TL;DR: 本文提出ECHO方法，通过自适应分支控制和置信度剪枝解决测试时强化学习中存在的分支崩溃和早期伪标签噪声问题，在数学和视觉推理任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有测试时强化学习方法采用树状结构rollout来提升采样效率，但仍面临两个关键挑战：1）高熵分支可能导致rollout崩溃，分支预算集中在少数轨迹上；2）早期伪标签噪声大且有偏差，会导致自增强过拟合和过早抑制探索。

Method: 提出ECHO方法：1）在rollout阶段，联合利用局部熵和组级置信度自适应控制分支宽度，引入在线置信度剪枝终止持续低置信度分支；2）在策略更新阶段，采用置信度自适应裁剪和熵-置信度混合优势塑形方法增强训练鲁棒性。

Result: 实验表明ECHO在多个数学和视觉推理基准测试中取得一致性能提升，在有限rollout预算下展现出更好的泛化能力。

Conclusion: ECHO通过自适应分支控制和置信度剪枝有效解决了测试时强化学习中的分支崩溃和早期偏差问题，提升了采样效率和策略训练质量。

Abstract: Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.

</details>


### [394] [Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization](https://arxiv.org/abs/2602.02151)
*Yuli Zhou,Qingxuan Chen,Luca Benini,Guolei Sun,Yawei Li*

Main category: cs.LG

TL;DR: VQRound是一种参数高效的优化框架，通过将舍入矩阵重新参数化为紧凑码本，实现LLM后训练量化的自适应舍入，仅需0.2%可训练参数。


<details>
  <summary>Details</summary>
Motivation: 传统自适应舍入方法需要密集的逐元素舍入矩阵，对于十亿参数的大语言模型计算成本过高。需要从效率角度重新审视自适应舍入，解决LLM中重尾权重分布的处理问题。

Method: 提出VQRound框架：1) 将舍入矩阵重新参数化为紧凑码本；2) 在L∞范数下最小化逐元素最坏情况误差；3) 识别舍入初始化作为关键因素；4) 开发轻量级端到端微调流程，仅用128个样本优化所有层的码本。

Result: 在OPT、LLaMA、LLaMA2和Qwen3模型上的实验表明，VQRound在相同步数下比传统自适应舍入收敛更好，同时仅使用0.2%的可训练参数。证明自适应舍入可以同时实现可扩展性和快速拟合。

Conclusion: VQRound通过码本重新参数化和轻量级微调，使自适应舍入变得既高效又可扩展，为大语言模型的后训练量化提供了实用的解决方案。

Abstract: Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.

</details>


### [395] [Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing](https://arxiv.org/abs/2602.02157)
*Egor Serov,Ilya Kuleshov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出使用核平滑和高斯过程替代传统样条插值构建神经控制微分方程路径，结合注意力机制的多视图CDE框架，显著降低计算成本同时保持高精度


<details>
  <summary>Details</summary>
Motivation: 传统神经控制微分方程中驱动控制路径的粗糙性导致自适应求解器需要过多小步长，计算效率低下，需要更平滑的路径构建方法

Method: 1) 使用核平滑和高斯过程替代精确插值，显式控制轨迹正则性；2) 提出基于注意力的多视图CDE及其卷积扩展，使用可学习查询恢复平滑过程中丢失的细节；3) 通过多个轨迹分布表示能力，捕捉不同的时间模式

Result: MVC-CDE with GP方法在保持最先进精度的同时，显著减少了函数评估次数和总推理时间，相比基于样条的基线方法有显著改进

Conclusion: 通过核平滑路径构建和多视图注意力机制，成功解决了神经控制微分方程的计算效率问题，为连续时间序列建模提供了更高效实用的框架

Abstract: Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.

</details>


### [396] [Interpretable Tabular Foundation Models via In-Context Kernel Regression](https://arxiv.org/abs/2602.02162)
*Ratmir Miftachov,Bruno Charron,Simon Valentin*

Main category: cs.LG

TL;DR: KernelICL框架通过将表格基础模型的最终预测层替换为核函数，实现了可量化的基于样本的可解释性，在保持性能的同时使预测透明化。


<details>
  <summary>Details</summary>
Motivation: 现有的表格基础模型（如TabPFN和TabICL）虽然通过上下文学习取得了最先进的性能，但其架构本质上是不透明的，缺乏可解释性。

Method: 基于上下文学习类似于核回归的洞察，将最终预测层替换为核函数（高斯核、点积核、kNN），使每个预测都成为训练标签的透明加权平均。提出了一个二维分类法，将标准核方法、现代基于邻居的方法和注意力机制统一在一个框架下。

Result: 在55个TALENT基准数据集上，KernelICL实现了与现有表格基础模型相当的性能，表明对最终层施加显式核约束可以在不牺牲性能的情况下实现可检查的预测。

Conclusion: 通过将核机制显式化，KernelICL框架为表格基础模型提供了可量化的基于样本的可解释性，实现了性能与可解释性的平衡。

Abstract: Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.

</details>


### [397] [Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents](https://arxiv.org/abs/2602.02164)
*Pengfei He,Ash Fox,Lesly Miculicich,Stefan Friedli,Daniel Fabian,Burak Gokturk,Jiliang Tang,Chen-Yu Lee,Tomas Pfister,Long T. Le*

Main category: cs.LG

TL;DR: Co-RedTeam是一个安全感知的多智能体框架，通过集成安全领域知识、代码感知分析、执行基础迭代推理和长期记忆，模拟真实红队工作流程，显著提升漏洞发现和利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在网络安全任务中存在局限性：交互有限、执行基础薄弱、缺乏经验复用，难以实现自动化的漏洞发现和利用。需要开发能够模拟真实红队工作流程的框架。

Method: 提出Co-RedTeam安全感知多智能体框架，将漏洞分析分解为协调的发现和利用阶段。智能体基于真实执行反馈进行规划、执行、验证和优化，同时从先前轨迹中学习。框架集成了安全领域知识、代码感知分析、执行基础迭代推理和长期记忆。

Result: 在具有挑战性的安全基准测试中，Co-RedTeam始终优于强基线模型，在漏洞利用方面达到超过60%的成功率，在漏洞检测方面实现超过10%的绝对提升。消融和迭代研究证实了执行反馈、结构化交互和记忆对于构建鲁棒可泛化网络安全智能体的关键作用。

Conclusion: Co-RedTeam通过模拟真实红队工作流程，有效解决了现有LLM在网络安全任务中的局限性，为构建自动化的漏洞发现和利用系统提供了有前景的解决方案。

Abstract: Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.

</details>


### [398] [Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach](https://arxiv.org/abs/2602.02173)
*Jiancheng Tu,Wenqi Fan,Zhibin Wu*

Main category: cs.LG

TL;DR: 提出基于混合整数规划（MIP）的框架，用于学习在非线性性能指标（如F1分数）下的最优分类树，特别针对类别不平衡问题，通过特定加速技术提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 决策树的全局优化是组合优化中长期存在的挑战，但这类模型在可解释机器学习中很重要。虽然问题已研究数十年，但直到最近离散优化的进展才使得在真实数据集上求解最优分类树问题变得实际可行。

Method: 提出基于混合整数规划（MIP）的框架，用于学习在非线性性能指标下的最优分类树。开发了特定问题加速技术：定制的分支切割算法、实例缩减方案和热启动策略。

Result: 在50个基准数据集上评估，结果显示该框架能高效优化非线性指标，同时实现强大的预测性能，与现有方法相比减少了求解时间。

Conclusion: 混合整数规划为学习最优分类树提供了高度建模灵活性，特别是在处理非线性指标和类别不平衡问题时，通过特定加速技术可显著提高计算效率。

Abstract: Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.

</details>


### [399] [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179)
*Marina Mastroleo,Alberto Archetti,Federico Mastroleo,Matteo Matteucci*

Main category: cs.LG

TL;DR: SurvKAN：基于KAN架构的完全参数化时间连续生存模型，消除比例风险约束，在保持可解释性的同时实现竞争性性能


<details>
  <summary>Details</summary>
Motivation: 传统生存模型（如Cox）依赖线性协变量关系和比例风险假设，无法捕捉真实临床动态；深度学习模型（如DeepSurv、DeepHit）表达能力增强但牺牲了可解释性，限制了临床采用；现有混合模型（如CoxKAN）仍受半参数Cox框架约束

Method: SurvKAN将时间作为KAN网络的显式输入，直接预测对数风险函数，通过可学习的单变量函数保持可解释性，支持端到端的完整生存似然训练，消除了比例风险约束

Result: 在标准生存基准测试中，SurvKAN在一致性和校准指标上达到或优于经典和最先进的基线方法；可解释性分析揭示了与医学领域知识一致的临床有意义模式

Conclusion: SurvKAN通过结合KAN架构的优势，在保持临床可解释性的同时实现了竞争性预测性能，为临床决策提供了更透明、更灵活的生存分析工具

Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [400] [Uncertainty-Aware Multimodal Learning via Conformal Shapley Intervals](https://arxiv.org/abs/2602.00171)
*Mathew Chandy,Michael Johnson,Judong Shen,Devan V. Mehrotra,Hua Zhou,Jin Zhou,Xiaowu Dai*

Main category: stat.ML

TL;DR: 提出conformal Shapley intervals框架，结合Shapley值和conformal inference为多模态学习中的每个模态构建不确定性感知的重要性区间，并提供具有理论保证的模态选择方法。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中各模态贡献不均且数据依赖性强，难以确定哪些模态真正具有信息量以及其贡献的可信度。量化模态重要性及其不确定性对于可解释和可靠的多模态学习至关重要。

Method: 结合Shapley值和conformal inference构建conformal Shapley intervals框架，为每个模态创建不确定性感知的重要性区间。基于这些区间提出模态选择程序，具有理论最优性保证。

Result: 在多个数据集上验证了方法的有效性，能够提供有意义的不确定性量化，保持强大的预测性能，同时仅依赖少量信息丰富的模态。

Conclusion: 提出的conformal Shapley intervals框架能够可靠地量化多模态学习中各模态的重要性及其不确定性，为可解释和可靠的多模态学习提供了有效工具。

Abstract: Multimodal learning combines information from multiple data modalities to improve predictive performance. However, modalities often contribute unequally and in a data dependent way, making it unclear which data modalities are genuinely informative and to what extent their contributions can be trusted. Quantifying modality level importance together with uncertainty is therefore central to interpretable and reliable multimodal learning. We introduce conformal Shapley intervals, a framework that combines Shapley values with conformal inference to construct uncertainty-aware importance intervals for each modality. Building on these intervals, we propose a modality selection procedure with a provable optimality guarantee: conditional on the observed features, the selected subset of modalities achieves performance close to that of the optimal subset. We demonstrate the effectiveness of our approach on multiple datasets, showing that it provides meaningful uncertainty quantification and strong predictive performance while relying on only a small number of informative modalities.

</details>


### [401] [Neuron Block Dynamics for XOR Classification with Zero-Margin](https://arxiv.org/abs/2602.00172)
*Guillaume Braun,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 该论文研究零边界非线性分类问题，通过分析高斯XOR问题，提出神经元块动态框架，证明神经元会聚类成四个方向，并分析无边界假设下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 大多数理论分析关注回归或有正边界的分类任务，而零边界非线性分类问题中，有相当比例数据靠近边界，破坏了标准的基于边界的论证。需要研究在这种设置下神经网络如何通过SGD学习有用特征。

Method: 基于Glasgow (2024)的分析，将训练动态研究从离散输入扩展到高斯输入，开发神经元块动态框架。分析神经元如何聚类成四个方向，以及块级信号如何协同演化。采用平均情况视角区分可靠预测区域和持续误差区域。

Result: 证明神经元会聚类成四个方向，块级信号协同演化，这在个体神经元信号变化显著的高斯设置中至关重要。数值实验证实了预测的两阶段块动态，并展示了其在非高斯设置中的鲁棒性。

Conclusion: 通过分析高斯XOR问题，建立了零边界非线性分类的理论框架，展示了神经元块动态在理解神经网络学习机制中的重要性，为无边界假设下的泛化分析提供了新视角。

Abstract: The ability of neural networks to learn useful features through stochastic gradient descent (SGD) is a cornerstone of their success. Most theoretical analyses focus on regression or on classification tasks with a positive margin, where worst-case gradient bounds suffice. In contrast, we study zero-margin nonlinear classification by analyzing the Gaussian XOR problem, where inputs are Gaussian and the XOR decision boundary determines labels. In this setting, a non-negligible fraction of data lies arbitrarily close to the boundary, breaking standard margin-based arguments. Building on Glasgow's (2024) analysis, we extend the study of training dynamics from discrete to Gaussian inputs and develop a framework for the dynamics of neuron blocks. We show that neurons cluster into four directions and that block-level signals evolve coherently, a phenomenon essential in the Gaussian setting where individual neuron signals vary significantly. Leveraging this block perspective, we analyze generalization without relying on margin assumptions, adopting an average-case view that distinguishes regions of reliable prediction from regions of persistent error. Numerical experiments confirm the predicted two-phase block dynamics and demonstrate their robustness beyond the Gaussian setting.

</details>


### [402] [Singular Bayesian Neural Networks](https://arxiv.org/abs/2602.00387)
*Mame Diarra Toure,David A. Stephens*

Main category: stat.ML

TL;DR: 提出一种低秩参数化的贝叶斯神经网络，通过权重矩阵分解W=AB^T来减少参数数量，同时保持不确定性校准能力


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯神经网络需要O(mn)参数，当权重矩阵奇异值衰减快时，这种成本通常是不必要的。需要一种更高效的参数化方法来减少参数数量，同时保持不确定性校准能力。

Method: 将权重参数化为W=AB^T，其中A∈ℝ^{m×r}，B∈ℝ^{n×r}，诱导后验分布集中在秩r流形上。这种方法通过共享潜在因子捕获结构化的权重相关性，几何上不同于均值场独立假设。

Result: 推导出PAC-Bayes泛化边界，复杂度项按√r(m+n)缩放而非√mn。在MLP、LSTM和Transformer上的实验表明，该方法使用比5成员深度集成少15倍的参数，达到竞争性预测性能，显著改善OOD检测和校准。

Conclusion: 低秩参数化贝叶斯神经网络提供了一种参数高效的方法，在保持预测性能的同时改善不确定性校准，特别适用于权重矩阵具有快速奇异值衰减的场景。

Abstract: Bayesian neural networks promise calibrated uncertainty but require $O(mn)$ parameters for standard mean-field Gaussian posteriors. We argue this cost is often unnecessary, particularly when weight matrices exhibit fast singular value decay. By parameterizing weights as $W = AB^{\top}$ with $A \in \mathbb{R}^{m \times r}$, $B \in \mathbb{R}^{n \times r}$, we induce a posterior that is singular with respect to the Lebesgue measure, concentrating on the rank-$r$ manifold. This singularity captures structured weight correlations through shared latent factors, geometrically distinct from mean-field's independence assumption. We derive PAC-Bayes generalization bounds whose complexity term scales as $\sqrt{r(m+n)}$ instead of $\sqrt{m n}$, and prove loss bounds that decompose the error into optimization and rank-induced bias using the Eckart-Young-Mirsky theorem. We further adapt recent Gaussian complexity bounds for low-rank deterministic networks to Bayesian predictive means. Empirically, across MLPs, LSTMs, and Transformers on standard benchmarks, our method achieves predictive performance competitive with 5-member Deep Ensembles while using up to $15\times$ fewer parameters. Furthermore, it substantially improves OOD detection and often improves calibration relative to mean-field and perturbation baselines.

</details>


### [403] [Reinforcement Learning for Control Systems with Time Delays: A Comprehensive Survey](https://arxiv.org/abs/2602.00399)
*Armando Alves Neto*

Main category: stat.ML

TL;DR: 本文全面综述了强化学习方法在处理控制系统时延问题上的研究进展，系统分类了五种主要方法，分析了各自的原理、优势和局限，为延迟影响下的网络物理系统提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂动态系统控制中取得显著成功，但大多数RL算法依赖马尔可夫决策过程假设，而实际网络物理系统中的传感延迟、执行延迟和通信约束会破坏这一假设，引入记忆效应，严重影响性能和稳定性。

Method: 首先形式化主要延迟类别并分析其对马尔可夫性质的影响，然后系统地将现有方法分为五类：状态增强和历史表示、具有学习记忆的循环策略、预测器和模型感知方法、鲁棒和域随机化训练策略、以及具有显式约束处理的强化学习框架。

Result: 通过比较分析揭示了这些方法之间的关键权衡，为不同延迟特性和安全要求下的方法选择提供了实用指南，并识别了稳定性认证、大延迟学习、多智能体通信协同设计和标准化基准测试等开放挑战。

Conclusion: 本文旨在为在延迟影响的网络物理系统中开发可靠强化学习控制器的研究者和实践者提供统一参考，指出了未来研究方向，包括稳定性认证、大延迟学习、多智能体通信协同设计和标准化基准测试。

Abstract: In the last decade, Reinforcement Learning (RL) has achieved remarkable success in the control and decision-making of complex dynamical systems. However, most RL algorithms rely on the Markov Decision Process assumption, which is violated in practical cyber-physical systems affected by sensing delays, actuation latencies, and communication constraints. Such time delays introduce memory effects that can significantly degrade performance and compromise stability, particularly in networked and multi-agent environments. This paper presents a comprehensive survey of RL methods designed to address time delays in control systems. We first formalize the main classes of delays and analyze their impact on the Markov property. We then systematically categorize existing approaches into five major families: state augmentation and history-based representations, recurrent policies with learned memory, predictor-based and model-aware methods, robust and domain-randomized training strategies, and safe RL frameworks with explicit constraint handling. For each family, we discuss underlying principles, practical advantages, and inherent limitations. A comparative analysis highlights key trade-offs among these approaches and provides practical guidelines for selecting suitable methods under different delay characteristics and safety requirements. Finally, we identify open challenges and promising research directions, including stability certification, large-delay learning, multi-agent communication co-design, and standardized benchmarking. This survey aims to serve as a unified reference for researchers and practitioners developing reliable RL-based controllers in delay-affected cyber-physical systems.

</details>


### [404] [Alignment of Diffusion Model and Flow Matching for Text-to-Image Generation](https://arxiv.org/abs/2602.00413)
*Yidong Ouyang,Liyan Xie,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

TL;DR: 提出无需微调的扩散模型和流匹配对齐框架，通过奖励加权分布采样，分别采用训练引导网络和免训练方法，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法需要大量计算资源微调预训练模型，且难以泛化到不同目标。本文利用对齐问题的本质——从奖励加权分布中采样——提出更高效的对齐框架

Method: 1. 扩散模型：训练引导网络估计奖励的条件期望，避免对抗性引导项引入伪影；2. 流匹配：提出免训练框架，通过速度引导改进生成质量

Result: 扩散模型对齐：在一步生成中达到与微调方法相当的性能，计算成本降低至少60%；流匹配对齐：无需额外计算成本即可提升生成质量

Conclusion: 提出的对齐框架利用奖励加权分布采样的本质，为扩散模型和流匹配提供了高效的对齐解决方案，显著降低计算需求并保持生成质量

Abstract: Diffusion models and flow matching have demonstrated remarkable success in text-to-image generation. While many existing alignment methods primarily focus on fine-tuning pre-trained generative models to maximize a given reward function, these approaches require extensive computational resources and may not generalize well across different objectives. In this work, we propose a novel alignment framework by leveraging the underlying nature of the alignment problem -- sampling from reward-weighted distributions -- and show that it applies to both diffusion models (via score guidance) and flow matching models (via velocity guidance). The score function (velocity field) required for the reward-weighted distribution can be decomposed into the pre-trained score (velocity field) plus a conditional expectation of the reward. For the alignment on the diffusion model, we identify a fundamental challenge: the adversarial nature of the guidance term can introduce undesirable artifacts in the generated images. Therefore, we propose a finetuning-free framework that trains a guidance network to estimate the conditional expectation of the reward. We achieve comparable performance to finetuning-based models with one-step generation with at least a 60% reduction in computational cost. For the alignment on flow matching, we propose a training-free framework that improves the generation quality without additional computational cost.

</details>


### [405] [Shuffle and Joint Differential Privacy for Generalized Linear Contextual Bandits](https://arxiv.org/abs/2602.00417)
*Sahasrajit Sarmasarkar*

Main category: stat.ML

TL;DR: 首次提出在洗牌差分隐私和联合差分隐私下的广义线性上下文赌博机算法，解决了GLM带来的新挑战，在不同隐私模型和上下文设置下实现了接近非私有算法的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 先前关于私有上下文赌博机的研究仅限于线性奖励模型（具有闭式解），而广义线性模型（GLMs）带来了新的挑战：无闭式解需要私有凸优化、隐私需跨多个演化设计矩阵跟踪、优化误差必须明确纳入遗憾分析。

Method: 针对两种隐私模型和上下文设置设计算法：对于随机上下文，设计洗牌差分隐私算法；对于对抗性上下文，提供联合差分隐私算法。两种算法都移除了对实例特定参数κ的依赖，且不需要超出ℓ₂有界性的谱假设。

Result: 随机上下文下洗牌差分隐私算法实现$\tilde{O}(d^{3/2}\sqrt{T}/\sqrt{\varepsilon})$遗憾；对抗性上下文下联合差分隐私算法实现$\tilde{O}(d\sqrt{T}/\sqrt{\varepsilon})$遗憾，与非私有率仅差$1/\sqrt{\varepsilon}$因子。

Conclusion: 首次解决了广义线性上下文赌博机在洗牌和联合差分隐私下的问题，克服了GLM带来的独特挑战，在不同设置下实现了接近最优的遗憾界，且不需要强谱假设。

Abstract: We present the first algorithms for generalized linear contextual bandits under shuffle differential privacy and joint differential privacy. While prior work on private contextual bandits has been restricted to linear reward models -- which admit closed-form estimators -- generalized linear models (GLMs) pose fundamental new challenges: no closed-form estimator exists, requiring private convex optimization; privacy must be tracked across multiple evolving design matrices; and optimization error must be explicitly incorporated into regret analysis.
  We address these challenges under two privacy models and context settings. For stochastic contexts, we design a shuffle-DP algorithm achieving $\tilde{O}(d^{3/2}\sqrt{T}/\sqrt{\varepsilon})$ regret. For adversarial contexts, we provide a joint-DP algorithm with $\tilde{O}(d\sqrt{T}/\sqrt{\varepsilon})$ regret -- matching the non-private rate up to a $1/\sqrt{\varepsilon}$ factor. Both algorithms remove dependence on the instance-specific parameter $κ$ (which can be exponential in dimension) from the dominant $\sqrt{T}$ term. Unlike prior work on locally private GLM bandits, our methods require no spectral assumptions on the context distribution beyond $\ell_2$ boundedness.

</details>


### [406] [Topological Residual Asymmetry for Bivariate Causal Direction](https://arxiv.org/abs/2602.00427)
*Mouad El Bouchattaoui*

Main category: stat.ML

TL;DR: 提出TRA方法，基于拓扑几何和持久同调，通过比较回归残差云的形状来推断因果方向，在低噪声情况下特别有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于观测数据的因果方向推断方法在模糊或接近不可识别的情况下容易出错，需要更稳健的几何基础方法。

Method: TRA方法使用秩基copula标准化后的交叉拟合回归残差云形状对比：正确方向残差近似独立形成二维体，反向方向（特别是低噪声下）则集中在一维管状结构附近。使用0维持久同调函数量化这种体-管对比，通过欧几里得最小生成树边长度剖面高效计算。

Result: 在广泛的合成和真实数据实验中，TRA方法表现出优越性能，特别是在具有挑战性的场景下。

Conclusion: TRA提供了一种基于几何的稳健因果方向推断方法，通过拓扑残差不对称性有效区分因果方向，特别是在低噪声情况下。

Abstract: Inferring causal direction from purely observational bivariate data is fragile: many methods commit to a direction even in ambiguous or near non-identifiable regimes. We propose Topological Residual Asymmetry (TRA), a geometry-based criterion for additive-noise models. TRA compares the shapes of two cross-fitted regressor-residual clouds after rank-based copula standardization: in the correct direction, residuals are approximately independent, producing a two-dimensional bulk, while in the reverse direction -- especially under low noise -- the cloud concentrates near a one-dimensional tube. We quantify this bulk-tube contrast using a 0D persistent-homology functional, computed efficiently from Euclidean MST edge-length profiles. We prove consistency in a triangular-array small-noise regime, extend the method to fixed noise via a binned variant (TRA-s), and introduce TRA-C, a confounding-aware abstention rule calibrated by a Gaussian-copula plug-in bootstrap. Extensive experiments across many challenging synthetic and real-data scenarios demonstrate the method's superiority.

</details>


### [407] [Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations](https://arxiv.org/abs/2602.00474)
*Yang Xu,Vaneet Aggarwal*

Main category: stat.ML

TL;DR: 该论文解决了非遍历马尔可夫链中泊松方程学习的问题，通过引入商空间方法和规范映射，实现了在可约或周期链上的稳定学习。


<details>
  <summary>Details</summary>
Motivation: 平均奖励强化学习依赖于泊松方程，但在非遍历（可约或周期）马尔可夫链中，泊松方程可能不适定，导致解不唯一且标准固定点迭代会振荡。现有方法主要局限于遍历链，需要扩展到更一般的多链和周期场景。

Method: 1. 分析马尔可夫链结构，识别实外围不变子空间K(P)捕获所有非衰减模式；2. 在商空间R^n/K(P)上建立严格压缩算子，获得唯一商解；3. 开发端到端流程：学习链结构、估计基于锚点的规范映射、运行投影随机逼近估计规范固定代表及其外围残差。

Result: 证明了在投影估计误差范围内达到Õ(T^{-1/2})的收敛速度，实现了多链和周期场景下泊松方程的稳定学习，扩展了平均奖励强化学习的性能评估能力。

Conclusion: 通过商空间方法和规范映射技术，成功解决了非遍历马尔可夫链中泊松方程学习的不适定问题，为超越遍历性的平均奖励强化学习提供了理论基础和实用算法。

Abstract: Poisson equations underpin average-reward reinforcement learning, but beyond ergodicity they can be ill-posed, meaning that solutions are non-unique and standard fixed point iterations can oscillate on reducible or periodic chains. We study finite-state Markov chains with $n$ states and transition matrix $P$. We show that all non-decaying modes are captured by a real peripheral invariant subspace $\mathcal{K}(P)$, and that the induced operator on the quotient space $\mathbb{R}^n/\mathcal{K}(P)$ is strictly contractive, yielding a unique quotient solution. Building on this viewpoint, we develop an end-to-end pipeline that learns the chain structure, estimates an anchor based gauge map, and runs projected stochastic approximation to estimate a gauge-fixed representative together with an associated peripheral residual. We prove $\widetilde{O}(T^{-1/2})$ convergence up to projection estimation error, enabling stable Poisson equation learning for multichain and periodic regimes with applications to performance evaluation of average-reward reinforcement learning beyond ergodicity.

</details>


### [408] [Action-Free Offline-to-Online RL via Discretised State Policies](https://arxiv.org/abs/2602.00629)
*Natinael Solomon Neggatu,Jeremie Houssineau,Giovanni Montana*

Main category: stat.ML

TL;DR: 提出一种无需动作标签的离线到在线强化学习方法，通过状态离散化和状态策略学习，利用仅含(s,r,s')元组的数据集加速在线学习


<details>
  <summary>Details</summary>
Motivation: 现实场景中许多离线数据集缺少动作标签（由于隐私、存储或传感器限制），现有离线RL方法需要动作标签，这限制了其应用范围

Method: 1) 提出状态离散化变换，将连续状态空间转换为离散表示；2) 提出Offline State-Only DecQN算法，从无动作数据中预训练状态策略（推荐期望的下一状态转移）；3) 提出引导在线学习机制，利用预训练的状态策略加速在线代理学习

Result: 实验结果表明，该方法在多种基准测试中提高了收敛速度和渐近性能，分析显示离散化和正则化对方法有效性至关重要

Conclusion: 提出了一个可扩展的实用框架，利用无动作数据集加速在线RL，通过状态策略学习和离散化技术解决了动作缺失问题

Abstract: Most existing offline RL methods presume the availability of action labels within the dataset, but in many practical scenarios, actions may be missing due to privacy, storage, or sensor limitations. We formalise the setting of action-free offline-to-online RL, where agents must learn from datasets consisting solely of $(s,r,s')$ tuples and later leverage this knowledge during online interaction. To address this challenge, we propose learning state policies that recommend desirable next-state transitions rather than actions. Our contributions are twofold. First, we introduce a simple yet novel state discretisation transformation and propose Offline State-Only DecQN (\algo), a value-based algorithm designed to pre-train state policies from action-free data. \algo{} integrates the transformation to scale efficiently to high-dimensional problems while avoiding instability and overfitting associated with continuous state prediction. Second, we propose a novel mechanism for guided online learning that leverages these pre-trained state policies to accelerate the learning of online agents. Together, these components establish a scalable and practical framework for leveraging action-free datasets to accelerate online RL. Empirical results across diverse benchmarks demonstrate that our approach improves convergence speed and asymptotic performance, while analyses reveal that discretisation and regularisation are critical to its effectiveness.

</details>


### [409] [Sampling from multi-modal distributions on Riemannian manifolds with training-free stochastic interpolants](https://arxiv.org/abs/2602.00641)
*Alain Durmus,Maxence Noble,Thibaut Pellerin*

Main category: stat.ML

TL;DR: 提出了一种在黎曼流形上从非归一化多模态密度采样的训练自由方法，基于确定性动力学将噪声分布传输到目标分布，扩展了扩散采样方法到非欧几里得空间。


<details>
  <summary>Details</summary>
Motivation: 现有采样方法在处理黎曼流形上的多模态目标分布时面临挑战，特别是高维和重尾分布。需要一种无需训练、能处理复杂几何结构的采样方法。

Method: 基于扩散模型框架，提出确定性非平衡动力学采样算法。通过构造尊重黎曼几何的随机插值路径，将易采样的噪声分布传输到目标分布。完全训练自由，仅使用标准蒙特卡洛技术进行迭代后验采样。

Result: 方法在多种多模态采样问题上表现出有效性，包括高维和重尾分布示例。理论分析证明了方法的严谨性，成功扩展了基于扩散的采样方法到黎曼流形设置。

Conclusion: 提出了一种通用的训练自由采样方法，能有效处理黎曼流形上的多模态目标分布，为复杂几何结构下的采样问题提供了新解决方案。

Abstract: In this paper, we propose a general methodology for sampling from un-normalized densities defined on Riemannian manifolds, with a particular focus on multi-modal targets that remain challenging for existing sampling methods. Inspired by the framework of diffusion models developed for generative modeling, we introduce a sampling algorithm based on the simulation of a non-equilibrium deterministic dynamics that transports an easy-to-sample noise distribution toward the target. At the marginal level, the induced density path follows a prescribed stochastic interpolant between the noise and target distributions, specifically constructed to respect the underlying Riemannian geometry. In contrast to related generative modeling approaches that rely on machine learning, our method is entirely training-free. It instead builds on iterative posterior sampling procedures using only standard Monte Carlo techniques, thereby extending recent diffusion-based sampling methodologies beyond the Euclidean setting. We complement our approach with a rigorous theoretical analysis and demonstrate its effectiveness on a range of multi-modal sampling problems, including high-dimensional and heavy-tailed examples.

</details>


### [410] [Emergence of Distortions in High-Dimensional Guided Diffusion Models](https://arxiv.org/abs/2602.00716)
*Enrico Ventura,Beatrice Achilli,Luca Ambrogioni,Carlo Lucibello*

Main category: stat.ML

TL;DR: CFG导致生成样本多样性损失，作者通过统计物理方法分析高维条件下CFG引起的生成失真现象，发现失真与类别数呈相变关系，提出负引导窗口方法缓解方差收缩问题。


<details>
  <summary>Details</summary>
Motivation: CFG作为扩散模型条件采样的标准方法，但常导致生成样本多样性损失。作者希望从理论上形式化这一现象，分析CFG引起的生成失真，并找到解决方案。

Method: 使用高斯混合模型及其精确分数，借助统计物理工具分析高维条件下的CFG失真现象。通过动力学平均场分析研究失真相变，并提出负引导窗口的引导调度方法。

Result: 分析表明：1）失真通过引导动力学的有效势相变出现；2）当模态数量随维度指数增长时失真持续存在，但在次指数增长时消失；3）标准CFG会平移均值并收缩方差；4）标准CFG调度无法防止方差收缩。

Conclusion: CFG引起的生成失真在高维条件下表现为相变现象，标准调度方法存在固有缺陷。提出的负引导窗口方法能有效缓解多样性损失，同时保持类别可分性。

Abstract: Classifier-free guidance (CFG) is the de facto standard for conditional sampling in diffusion models, yet it often leads to a loss of diversity in generated samples. We formalize this phenomenon as generative distortion, defined as the mismatch between the CFG-induced sampling distribution and the true conditional distribution. Considering Gaussian mixtures and their exact scores, and leveraging tools from statistical physics, we characterize the onset of distortion in a high-dimensional regime as a function of the number of classes. Our analysis reveals that distortions emerge through a phase transition in the effective potential governing the guided dynamics. In particular, our dynamical mean-field analysis shows that distortion persists when the number of modes grows exponentially with dimension, but vanishes in the sub-exponential regime. Consistent with prior finite-dimensional results, we further demonstrate that vanilla CFG shifts the mean and shrinks the variance of the conditional distribution. We show that standard CFG schedules are fundamentally incapable of preventing variance shrinkage. Finally, we propose a theoretically motivated guidance schedule featuring a negative-guidance window, which mitigates loss of diversity while preserving class separability.

</details>


### [411] [Zero-Flow Encoders](https://arxiv.org/abs/2602.00797)
*Yakun Wang,Leyang Wang,Song Liu,Taiji Suzuki*

Main category: stat.ML

TL;DR: 本文提出了一种基于流的表示学习框架，通过零流准则来提取数据的充分信息，用于图模型中的马尔可夫毯学习和自监督学习中的潜在表示学习。


<details>
  <summary>Details</summary>
Motivation: 流方法在生成建模中取得了显著成功，但现有工作很少利用其独特能力来解决生成任务之外的细粒度结构细节问题。本文旨在开发一个流启发的表示学习框架。

Method: 首先证明了使用独立耦合训练的整流流在t=0.5处处处为零当且仅当源分布和目标分布相同（零流准则）。然后表明该准则可以验证条件独立性，从而提取数据的充分信息。最后将该准则转化为可处理的、无需模拟的损失函数，用于学习图模型中的摊销马尔可夫毯和自监督学习中的潜在表示。

Result: 在模拟和真实世界数据集上的实验证明了该方法的有效性。

Conclusion: 本文提出的流启发的表示学习框架通过零流准则能够有效地提取数据的充分信息，在图模型和自监督学习任务中表现出色。

Abstract: Flow-based methods have achieved significant success in various generative modeling tasks, capturing nuanced details within complex data distributions. However, few existing works have exploited this unique capability to resolve fine-grained structural details beyond generation tasks. This paper presents a flow-inspired framework for representation learning. First, we demonstrate that a rectified flow trained using independent coupling is zero everywhere at $t=0.5$ if and only if the source and target distributions are identical. We term this property the \emph{zero-flow criterion}. Second, we show that this criterion can certify conditional independence, thereby extracting \emph{sufficient information} from the data. Third, we translate this criterion into a tractable, simulation-free loss function that enables learning amortized Markov blankets in graphical models and latent representations in self-supervised learning tasks. Experiments on both simulated and real-world datasets demonstrate the effectiveness of our approach. The code reproducing our experiments can be found at: https://github.com/probabilityFLOW/zfe.

</details>


### [412] [Hessian Spectral Analysis at Foundation Model Scale](https://arxiv.org/abs/2602.00816)
*Diego Granziol,Khurshid Juarev*

Main category: stat.ML

TL;DR: 首次在百亿参数规模上实现了真实Hessian矩阵的谱分析，揭示了传统块对角近似在大型语言模型中的严重失效问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型的精确Hessian谱分析一直难以实现，现有方法依赖小型模型或强结构近似，无法准确反映前沿规模模型的真实曲率特性。

Method: 使用兼容全分片数据并行的分片局部有限差分Hessian向量积，结合随机Lanczos求积法，在fp32和bf16精度下分析了高达100B参数的开源语言模型。

Result: 首次获得超10B参数规模的大规模谱密度估计，揭示了块对角曲率近似存在阶一相对误差和方向对齐问题，同时证明了完整算子谱探测仅带来适度常数因子开销。

Conclusion: 基础模型Hessian谱既是可计算的，又被主流近似严重误表示，为大规模曲率分析打开了大门。

Abstract: Accurate Hessian spectra of foundation models have remained out of reach, leading most prior work to rely on small models or strong structural approximations. We show that faithful spectral analysis of the true Hessian is tractable at frontier scale. Using shard-local finite-difference Hessian vector products compatible with Fully Sharded Data Parallelism, we perform stochastic Lanczos quadrature on open-source language models with up to 100B parameters, producing the first large-scale spectral density estimates beyond the sub-10B regime. We characterize the numerical behavior of this pipeline, including finite-difference bias, floating-point noise amplification, and their effect on Krylov stability in fp32 and bf16, and derive practical operating regimes that are validated empirically. We further provide end-to-end runtime and memory scaling laws, showing that full-operator spectral probing incurs only a modest constant-factor overhead over first-order training. Crucially, direct access to the Hessian reveals that widely used block-diagonal curvature approximations can fail catastrophically, exhibiting order-one relative error and poor directional alignment even in mid-scale LLMs. Together, our results demonstrate that foundation-model Hessian spectra are both computable and qualitatively misrepresented by prevailing approximations, opening the door to principled curvature-based analysis at scale.

</details>


### [413] [Safety-Efficacy Trade Off: Robustness against Data-Poisoning](https://arxiv.org/abs/2602.00822)
*Diego Granziol*

Main category: stat.ML

TL;DR: 论文证明后门攻击和数据投毒攻击通过输入空间的几何机制实现高攻击成功率并规避现有防御，揭示了毒数据在输入Hessian中产生秩一尖峰，在非线性核中存在"近克隆"区域使攻击谱不可检测，并发现输入梯度正则化通过收缩Fisher和Hessian特征模实现安全与效能的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有后门和数据投毒攻击能实现高成功率并规避基于谱分析和优化的防御，这种行为的根本机制尚不明确。研究旨在揭示攻击成功的几何机制，理解攻击何时固有不可见，并提供从投毒到检测再到防御的端到端理论框架。

Method: 使用核岭回归作为宽神经网络的精确模型，理论分析毒数据在输入Hessian中产生的秩一尖峰效应。识别非线性核中的"近克隆"区域，分析输入梯度正则化对Fisher和Hessian特征模的收缩作用。在MNIST、CIFAR-10和CIFAR-100数据集上对线性模型和深度卷积网络进行广泛实验验证。

Result: 理论证明：聚类脏标签毒数据在输入Hessian中产生秩一尖峰，其幅度与攻击效能呈二次方缩放；在非线性核的"近克隆"区域，攻击效能保持量级1而输入曲率消失，使攻击谱不可检测。实验验证：攻击成功率与谱可见性之间存在一致滞后；正则化和数据增强能联合抑制投毒；输入梯度正则化可解释为各向异性高通滤波器，抑制近克隆毒数据。

Conclusion: 后门攻击通过输入空间的几何机制实现固有不可见性，输入梯度正则化通过收缩毒数据对齐的Fisher和Hessian特征模，在安全与效能之间产生不可避免的权衡。该研究首次通过输入空间曲率提供了投毒、可检测性和防御的端到端理论描述。

Abstract: Backdoor and data poisoning attacks can achieve high attack success while evading existing spectral and optimisation based defences. We show that this behaviour is not incidental, but arises from a fundamental geometric mechanism in input space. Using kernel ridge regression as an exact model of wide neural networks, we prove that clustered dirty label poisons induce a rank one spike in the input Hessian whose magnitude scales quadratically with attack efficacy. Crucially, for nonlinear kernels we identify a near clone regime in which poison efficacy remains order one while the induced input curvature vanishes, making the attack provably spectrally undetectable. We further show that input gradient regularisation contracts poison aligned Fisher and Hessian eigenmodes under gradient flow, yielding an explicit and unavoidable safety efficacy trade off by reducing data fitting capacity. For exponential kernels, this defence admits a precise interpretation as an anisotropic high pass filter that increases the effective length scale and suppresses near clone poisons. Extensive experiments on linear models and deep convolutional networks across MNIST and CIFAR 10 and CIFAR 100 validate the theory, demonstrating consistent lags between attack success and spectral visibility, and showing that regularisation and data augmentation jointly suppress poisoning. Our results establish when backdoors are inherently invisible, and provide the first end to end characterisation of poisoning, detectability, and defence through input space curvature.

</details>


### [414] [Harmful Overfitting in Sobolev Spaces](https://arxiv.org/abs/2602.00825)
*Kedar Karhadkar,Alexander Sietsema,Deanna Needell,Guido Montufar*

Main category: stat.ML

TL;DR: 研究Sobolev空间中过参数化插值器的泛化行为，发现即使训练样本无限增加，近似范数最小化插值器仍会出现有害过拟合，泛化误差保持正下界。


<details>
  <summary>Details</summary>
Motivation: 受近期关于过参数化机器学习中良性过拟合研究的启发，旨在理解Sobolev空间中完美拟合噪声训练数据的函数的泛化行为。

Method: 使用几何论证方法，通过Sobolev不等式识别训练数据的有害邻域，分析近似范数最小化插值器在标签噪声和足够正则数据分布假设下的行为。

Result: 证明近似范数最小化插值器表现出有害过拟合：即使训练样本数n→∞，泛化误差以高概率保持正下界。结果适用于任意p∈[1,∞)，扩展了先前仅研究p=2希尔伯特空间情况的工作。

Conclusion: Sobolev空间中由平滑性偏置选择的规范解（近似范数最小化插值器）在过参数化设置下会出现有害过拟合，这与良性过拟合现象形成对比，为理解函数空间中的泛化行为提供了新见解。

Abstract: Motivated by recent work on benign overfitting in overparameterized machine learning, we study the generalization behavior of functions in Sobolev spaces $W^{k, p}(\mathbb{R}^d)$ that perfectly fit a noisy training data set. Under assumptions of label noise and sufficient regularity in the data distribution, we show that approximately norm-minimizing interpolators, which are canonical solutions selected by smoothness bias, exhibit harmful overfitting: even as the training sample size $n \to \infty$, the generalization error remains bounded below by a positive constant with high probability. Our results hold for arbitrary values of $p \in [1, \infty)$, in contrast to prior results studying the Hilbert space case ($p = 2$) using kernel methods. Our proof uses a geometric argument which identifies harmful neighborhoods of the training data using Sobolev inequalities.

</details>


### [415] [Score-based Metropolis-Hastings for Fractional Langevin Algorithms](https://arxiv.org/abs/2602.00835)
*Ahmed Aloui,Junyi Liao,Ali Hasan,Jose Blanchet,Vahid Tarokh*

Main category: stat.ML

TL;DR: 提出MAFLA方法，通过Metropolis-Hastings校正机制解决分数Langevin算法在重尾多峰分布采样中的问题，使用分数提案分数梯度代理和学习接受函数来改进采样精度。


<details>
  <summary>Details</summary>
Motivation: 在α-稳定Lévy驱动的分数Langevin算法中，当目标密度和提案密度都无法评估时，采样变得困难。现有分数Langevin方法在未校正状态下运行，存在显著的有限时间误差和对尾部行为的控制不佳问题。

Method: 提出Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA)，这是一种基于Metropolis-Hastings启发的完全基于分数的校正机制。方法使用各向同性对称α-稳定噪声下的分数提案分数梯度设计代理，并通过Score Balance Matching学习接受函数。

Result: 在包括组合优化问题在内的一系列任务中，MAFLA表现出色，相比未校正的分数Langevin动力学，显著提高了有限时间采样精度。

Conclusion: MAFLA通过创新的Metropolis-Hastings校正机制解决了分数Langevin算法在重尾多峰分布采样中的关键问题，为这类挑战性采样任务提供了有效的解决方案。

Abstract: Sampling from heavy-tailed and multimodal distributions is challenging when neither the target density nor the proposal density can be evaluated, as in $α$-stable Lévy-driven fractional Langevin algorithms. While the target distribution can be estimated from data via score-based or energy-based models, the $α$-stable proposal density and its score are generally unavailable, rendering classical density-based Metropolis--Hastings (MH) corrections impractical. Consequently, existing fractional Langevin methods operate in an unadjusted regime and can exhibit substantial finite-time errors and poor empirical control of tail behavior. We introduce the Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA), an MH-inspired, fully score-based correction mechanism. MAFLA employs designed proxies for fractional proposal score gradients under isotropic symmetric $α$-stable noise and learns an acceptance function via Score Balance Matching. We empirically illustrate the strong performance of MAFLA on a series of tasks including combinatorial optimization problems where the method significantly improves finite time sampling accuracy over unadjusted fractional Langevin dynamics.

</details>


### [416] [Multivariate Time Series Data Imputation via Distributionally Robust Regularization](https://arxiv.org/abs/2602.00844)
*Che-Yi Liao,Zheng Dong,Gian-Gabriel Garcia,Kamran Paynabar*

Main category: stat.ML

TL;DR: 提出DRIO方法，通过联合最小化重构误差和与最坏情况分布的散度，解决多元时间序列插补中的分布偏差问题


<details>
  <summary>Details</summary>
Motivation: 多元时间序列插补常因观测数据与真实数据分布不匹配而受损，这种偏差在非平稳性和系统性缺失情况下更加严重。标准方法最小化重构误差或鼓励分布对齐，但容易过拟合这些有偏观测

Method: 提出分布鲁棒正则化插补目标(DRIO)，联合最小化重构误差和插补器与Wasserstein模糊集内最坏情况分布之间的散度。推导出可处理的对偶形式，将无限维测度优化简化为样本轨迹的对抗搜索，并提出与灵活深度学习骨干兼容的对抗学习算法

Result: 在多种真实世界数据集上的综合实验表明，DRIO在完全随机缺失和非随机缺失设置下都能持续改进插补效果，在重构精度和分布对齐之间达到帕累托最优权衡

Conclusion: DRIO通过分布鲁棒优化有效解决了多元时间序列插补中的分布偏差问题，在各种缺失模式下都能实现更好的插补性能

Abstract: Multivariate time series (MTS) imputation is often compromised by mismatch between observed and true data distributions -- a bias exacerbated by non-stationarity and systematic missingness. Standard methods that minimize reconstruction error or encourage distributional alignment risk overfitting these biased observations. We propose the Distributionally Robust Regularized Imputer Objective (DRIO), which jointly minimizes reconstruction error and the divergence between the imputer and a worst-case distribution within a Wasserstein ambiguity set. We derive a tractable dual formulation that reduces infinite-dimensional optimization over measures to adversarial search over sample trajectories, and propose an adversarial learning algorithm compatible with flexible deep learning backbones. Comprehensive experiments on diverse real-world datasets show DRIO consistently improves imputation under both missing-completely-at-random and missing-not-at-random settings, reaching Pareto-optimal trade-offs between reconstruction accuracy and distributional alignment.

</details>


### [417] [Optimal Decision-Making Based on Prediction Sets](https://arxiv.org/abs/2602.00989)
*Tao Wang,Edgar Dobriban*

Main category: stat.ML

TL;DR: 提出Risk-Optimal Conformal Prediction (ROCP)框架，在保证覆盖概率的同时最小化最坏情况下的期望损失，用于优化预测集在决策任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 预测集虽然能为机器学习模型提供概率覆盖保证，但如何最优地将其用于下游决策仍不清楚。现有方法未考虑决策损失，可能导致在关键应用（如医疗诊断、安全决策）中产生代价高昂的错误。

Method: 1. 提出决策理论框架，在预测集覆盖保证一致的最坏分布下最小化期望损失；2. 推导固定预测集下的极小极大最优策略；3. 在覆盖约束下推导最优预测集构造；4. 提出ROCP算法，实现风险最小化预测集并保持有限样本分布无关的边际覆盖。

Result: ROCP在医疗诊断和安全关键决策任务中相比基线方法减少了关键错误，特别是在集外错误代价高昂的情况下表现更优。算法能有效平衡集内最坏损失与集外潜在损失惩罚。

Conclusion: ROCP框架将预测集的覆盖保证与决策风险最小化相结合，为高风险应用提供了更可靠的决策支持工具，在保持统计覆盖保证的同时优化了实际决策性能。

Abstract: Prediction sets can wrap around any ML model to cover unknown test outcomes with a guaranteed probability. Yet, it remains unclear how to use them optimally for downstream decision-making. Here, we propose a decision-theoretic framework that seeks to minimize the expected loss (risk) against a worst-case distribution consistent with the prediction set's coverage guarantee. We first characterize the minimax optimal policy for a fixed prediction set, showing that it balances the worst-case loss inside the set with a penalty for potential losses outside the set. Building on this, we derive the optimal prediction set construction that minimizes the resulting robust risk subject to a coverage constraint. Finally, we introduce Risk-Optimal Conformal Prediction (ROCP), a practical algorithm that targets these risk-minimizing sets while maintaining finite-sample distribution-free marginal coverage. Empirical evaluations on medical diagnosis and safety-critical decision-making tasks demonstrate that ROCP reduces critical mistakes compared to baselines, particularly when out-of-set errors are costly.

</details>


### [418] [Online Social Welfare Function-based Resource Allocation](https://arxiv.org/abs/2602.01400)
*Kanad Pardeshi,Samsara Foubert,Aarti Singh*

Main category: stat.ML

TL;DR: 提出了一个通用的置信序列框架，用于社会福利函数（SWF）的在线学习和推断，适用于任何单调、凹且Lipschitz连续的社会福利函数，并设计了SWF-UCB算法实现接近最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，中央决策者需要在多个时间步中反复向人群分配有限资源。个体获得资源后产生随机效用，需要通过社会福利函数（SWF）聚合个体期望效用来评估分配效果。需要开发一个通用的在线学习和推断框架来处理各种SWF。

Method: 提出了一个置信序列框架，利用单调性将个体效用的置信序列提升为最优社会福利的随时有效边界。基于此设计了SWF-UCB算法，这是一个SWF无关的在线学习算法。针对三种不同的SWF家族（加权幂平均、Kolm、Gini）提供了专门的oracle算法。

Result: SWF-UCB算法实现了接近最优的遗憾界$\tilde{O}(n+\sqrt{nkT})$（k个资源在n个个体中分配，T个时间步）。实验证实了$\sqrt{T}$的缩放特性，并揭示了k与SWF参数之间的丰富交互作用。

Conclusion: 该框架为基于社会福利函数的在线学习和推断提供了通用解决方案，支持顺序假设检验、最优停止和政策评估等推断应用。单调性是将个体效用置信序列提升到社会福利边界的充分条件。

Abstract: In many real-world settings, a centralized decision-maker must repeatedly allocate finite resources to a population over multiple time steps. Individuals who receive a resource derive some stochastic utility; to characterize the population-level effects of an allocation, the expected individual utilities are then aggregated using a social welfare function (SWF). We formalize this setting and present a general confidence sequence framework for SWF-based online learning and inference, valid for any monotonic, concave, and Lipschitz-continuous SWF. Our key insight is that monotonicity alone suffices to lift confidence sequences from individual utilities to anytime-valid bounds on optimal welfare. Building on this foundation, we propose SWF-UCB, a SWF-agnostic online learning algorithm that achieves near-optimal $\tilde{O}(n+\sqrt{nkT})$ regret (for $k$ resources distributed among $n$ individuals at each of $T$ time steps). We instantiate our framework on three normatively distinct SWF families: Weighted Power Mean, Kolm, and Gini, providing bespoke oracle algorithms for each. Experiments confirm $\sqrt{T}$ scaling and reveal rich interactions between $k$ and SWF parameters. This framework naturally supports inference applications such as sequential hypothesis testing, optimal stopping, and policy evaluation.

</details>


### [419] [Importance Weighted Variational Inference without the Reparameterization Trick](https://arxiv.org/abs/2602.01412)
*Kamélia Daudel,Minh-Ngoc Tran,Cheng Zhang*

Main category: stat.ML

TL;DR: 论文分析了重要性加权变分推断中REINFORCE梯度估计器的理论缺陷，提出了新的VIMCO-⋆梯度估计器来解决现有VIMCO估计器信噪比随样本数增加而消失的问题。


<details>
  <summary>Details</summary>
Motivation: 重要性加权变分推断通过优化随蒙特卡洛样本数增加而收紧的界限来近似已知归一化常数的密度。标准优化依赖重参数化梯度估计器，但这种方法限制了数据生成过程和变分近似的选择。虽然REINFORCE梯度估计器没有这些限制，但缺乏严格的理论基础。

Method: 论文对重要性加权变分推断中的REINFORCE梯度估计器进行了首次全面理论分析，引入并研究了一个广义的VIMCO梯度估计器家族。提出了新的VIMCO-⋆梯度估计器，该估计器避免了现有VIMCO估计器的信噪比崩溃问题。

Result: 理论证明现有VIMCO梯度估计器的信噪比随样本数N增加而消失，阻碍有效优化。提出的VIMCO-⋆梯度估计器实现了√N的信噪比缩放，避免了信噪比崩溃。在重参数化梯度通常不可用的挑战性设置中，VIMCO-⋆表现出优于现有VIMCO实现的实证性能。

Conclusion: 论文为重要性加权变分推断中的REINFORCE梯度估计器提供了首个全面理论分析，揭示了现有VIMCO估计器的根本缺陷，并提出VIMCO-⋆作为解决方案，在理论保证和实证性能上都优于现有方法。

Abstract: Importance weighted variational inference (VI) approximates densities known up to a normalizing constant by optimizing bounds that tighten with the number of Monte Carlo samples $N$. Standard optimization relies on reparameterized gradient estimators, which are well-studied theoretically yet restrict both the choice of the data-generating process and the variational approximation. While REINFORCE gradient estimators do not suffer from such restrictions, they lack rigorous theoretical justification. In this paper, we provide the first comprehensive analysis of REINFORCE gradient estimators in importance weighted VI, leveraging this theoretical foundation to diagnose and resolve fundamental deficiencies in current state-of-the-art estimators. Specifically, we introduce and examine a generalized family of variational inference for Monte Carlo objectives (VIMCO) gradient estimators. We prove that state-of-the-art VIMCO gradient estimators exhibit a vanishing signal-to-noise ratio (SNR) as $N$ increases, which prevents effective optimization. To overcome this issue, we propose the novel VIMCO-$\star$ gradient estimator and show that it averts the SNR collapse of existing VIMCO gradient estimators by achieving a $\sqrt{N}$ SNR scaling instead. We demonstrate its superior empirical performance compared to current VIMCO implementations in challenging settings where reparameterized gradients are typically unavailable.

</details>


### [420] [Robust Generalization with Adaptive Optimal Transport Priors for Decision-Focused Learning](https://arxiv.org/abs/2602.01427)
*Haixiang Sun,Andrew L. Liu*

Main category: stat.ML

TL;DR: 提出原型引导的分布鲁棒优化（PG-DRO）框架，通过分层最优传输从基础数据学习类别自适应先验，并将其嵌入Sinkhorn DRO公式，在小样本场景中实现更强的鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 现有Sinkhorn分布鲁棒优化方法依赖固定参考分布，限制了其适应性。小样本学习需要在有限监督下泛化并保持对分布偏移的鲁棒性，需要更灵活的方法。

Method: 提出PG-DRO框架：1）通过分层最优传输从丰富基础数据学习类别自适应先验；2）将这些先验嵌入Sinkhorn DRO公式；3）有机整合小样本信息生成类别特定的鲁棒决策；4）使不确定性集合与可迁移的结构知识对齐。

Result: 实验表明PG-DRO在小样本场景中实现了更强的鲁棒泛化，优于标准学习器和DRO基线方法。

Conclusion: PG-DRO框架通过结合类别自适应先验和Sinkhorn DRO，提供了理论保证且高效的小样本鲁棒学习方法，能够更好地处理分布偏移并实现更强的泛化性能。

Abstract: Few-shot learning requires models to generalize under limited supervision while remaining robust to distribution shifts. Existing Sinkhorn Distributionally Robust Optimization (DRO) methods provide theoretical guarantees but rely on a fixed reference distribution, which limits their adaptability. We propose a Prototype-Guided Distributionally Robust Optimization (PG-DRO) framework that learns class-adaptive priors from abundant base data via hierarchical optimal transport and embeds them into the Sinkhorn DRO formulation. This design enables few-shot information to be organically integrated into producing class-specific robust decisions that are both theoretically grounded and efficient, and further aligns the uncertainty set with transferable structural knowledge. Experiments show that PG-DRO achieves stronger robust generalization in few-shot scenarios, outperforming both standard learners and DRO baselines.

</details>


### [421] [Rethinking Multinomial Logistic Mixture of Experts with Sigmoid Gating Function](https://arxiv.org/abs/2602.01466)
*Tuan Minh Pham,Thinh Cao,Viet Nguyen,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: stat.ML

TL;DR: 本文分析了混合专家模型中sigmoid门控相比softmax门控的优势，解决了现有文献中未解决的三个问题，并提出改进的Euclidean评分来优化温度参数带来的指数级样本复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 尽管sigmoid门控在经验上优于softmax门控，但现有研究存在三个未解决的问题：1) 分类任务中的优势未确立；2) 现有sigmoid门控模型可能无法收敛到真实值；3) 温度参数的理论影响未充分探索。

Method: 对配备改进sigmoid门控的多项逻辑混合专家模型进行全面分析，确保模型收敛。提出用Euclidean评分替代传统的点积评分，以消除温度参数与门控参数之间的内在交互作用。

Result: sigmoid门控在参数和专家估计方面表现出比softmax门控更低的样本复杂度。但发现温度参数会导致指数级样本复杂度，而提出的Euclidean评分方法将样本复杂度降低到多项式级别。

Conclusion: sigmoid门控在分类任务中确实优于softmax门控，但需要解决温度参数带来的指数级样本复杂度问题。提出的Euclidean评分方法有效解决了这一问题，显著提高了模型的样本效率。

Abstract: The sigmoid gate in mixture-of-experts (MoE) models has been empirically shown to outperform the softmax gate across several tasks, ranging from approximating feed-forward networks to language modeling. Additionally, recent efforts have demonstrated that the sigmoid gate is provably more sample-efficient than its softmax counterpart under regression settings. Nevertheless, there are three notable concerns that have not been addressed in the literature, namely (i) the benefits of the sigmoid gate have not been established under classification settings; (ii) existing sigmoid-gated MoE models may not converge to their ground-truth; and (iii) the effects of a temperature parameter in the sigmoid gate remain theoretically underexplored. To tackle these open problems, we perform a comprehensive analysis of multinomial logistic MoE equipped with a modified sigmoid gate to ensure model convergence. Our results indicate that the sigmoid gate exhibits a lower sample complexity than the softmax gate for both parameter and expert estimation. Furthermore, we find that incorporating a temperature into the sigmoid gate leads to a sample complexity of exponential order due to an intrinsic interaction between the temperature and gating parameters. To overcome this issue, we propose replacing the vanilla inner product score in the gating function with a Euclidean score that effectively removes that interaction, thereby substantially improving the sample complexity to a polynomial order.

</details>


### [422] [Density-Informed Pseudo-Counts for Calibrated Evidential Deep Learning](https://arxiv.org/abs/2602.01477)
*Pietro Carlotti,Nevena Gligić,Arya Farahi*

Main category: stat.ML

TL;DR: EDL存在理论缺陷，在分布偏移下会系统性地过度自信。本文提出DIP-EDL，通过分离条件标签分布和边际协变量密度估计来解耦不确定性，改善OOD数据的鲁棒性和校准。


<details>
  <summary>Details</summary>
Motivation: 尽管EDL是流行的不确定性感知分类框架，但其理论基础和在分布偏移下的行为仍不清楚。标准EDL将认知不确定性和偶然不确定性混为一谈，导致在OOD输入上系统性地过度自信。

Method: 提出DIP-EDL方法：1) 提供统计解释，证明EDL训练对应于分层贝叶斯模型中的摊销变分推断；2) 引入密度感知伪计数EDL，通过分别估计条件标签分布和边际协变量密度来解耦类别预测和不确定性大小。

Result: 理论上证明DIP-EDL实现渐近集中；实证显示该方法增强可解释性，并改善在分布偏移下的鲁棒性和不确定性校准。

Conclusion: DIP-EDL解决了标准EDL的主要缺陷，通过解耦不确定性来源，在保持高密度区域证据的同时，使OOD数据的预测趋向均匀先验，从而提供更可靠的不确定性估计。

Abstract: Evidential Deep Learning (EDL) is a popular framework for uncertainty-aware classification that models predictive uncertainty via Dirichlet distributions parameterized by neural networks. Despite its popularity, its theoretical foundations and behavior under distributional shift remain poorly understood. In this work, we provide a principled statistical interpretation by proving that EDL training corresponds to amortized variational inference in a hierarchical Bayesian model with a tempered pseudo-likelihood. This perspective reveals a major drawback: standard EDL conflates epistemic and aleatoric uncertainty, leading to systematic overconfidence on out-of-distribution (OOD) inputs. To address this, we introduce Density-Informed Pseudo-count EDL (DIP-EDL), a new parametrization that decouples class prediction from the magnitude of uncertainty by separately estimating the conditional label distribution and the marginal covariate density. This separation preserves evidence in high-density regions while shrinking predictions toward a uniform prior for OOD data. Theoretically, we prove that DIP-EDL achieves asymptotic concentration. Empirically, we show that our method enhances interpretability and improves robustness and uncertainty calibration under distributional shift.

</details>


### [423] [Inference-Aware Meta-Alignment of LLMs via Non-Linear GRPO](https://arxiv.org/abs/2602.01603)
*Shokichi Takakura,Akifumi Wachi,Rei Higuchi,Kohei Miyaguchi,Taiji Suzuki*

Main category: stat.ML

TL;DR: 提出IAMA方法，让大语言模型能在推理时以有限计算预算对齐多个标准，通过非线性GRPO算法解决优化问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对齐多样化人类偏好具有挑战性，因为标准常相互冲突。推理时对齐方法虽能通过不同算法对齐多个标准，但计算成本高，需要多次前向传播。

Method: 提出推理感知元对齐(IAMA)方法，训练基础模型使其能通过不同推理时对齐算法有效对齐多个任务。使用非线性GRPO算法解决IAMA中的非线性优化问题。

Result: 非线性GRPO算法在概率测度空间中可证明收敛到最优解，使IAMA方法能在有限计算预算下实现多标准对齐。

Conclusion: IAMA方法解决了推理时对齐的计算效率问题，通过元对齐训练和收敛保证的非线性优化算法，使大语言模型能以有限计算资源对齐多个人类偏好标准。

Abstract: Aligning large language models (LLMs) to diverse human preferences is fundamentally challenging since criteria can often conflict with each other. Inference-time alignment methods have recently gained popularity as they allow LLMs to be aligned to multiple criteria via different alignment algorithms at inference time. However, inference-time alignment is computationally expensive since it often requires multiple forward passes of the base model. In this work, we propose inference-aware meta-alignment (IAMA), a novel approach that enables LLMs to be aligned to multiple criteria with limited computational budget at inference time. IAMA trains a base model such that it can be effectively aligned to multiple tasks via different inference-time alignment algorithms. To solve the non-linear optimization problems involved in IAMA, we propose non-linear GRPO, which provably converges to the optimal solution in the space of probability measures.

</details>


### [424] [ST-BCP: Tightening Coverage Bound for Backward Conformal Prediction via Non-Conformity Score Transformation](https://arxiv.org/abs/2602.01733)
*Junxian Liu,Hao Zeng,Hongxin Wei*

Main category: stat.ML

TL;DR: ST-BCP是一种改进的反向共形预测方法，通过数据依赖的分数变换缩小覆盖率估计与实际覆盖率之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统反向共形预测(BCP)使用马尔可夫不等式导致覆盖率估计与实际覆盖率之间存在显著差距，需要改进这一缺陷。

Method: 提出ST-BCP方法，引入数据依赖的非共形分数变换，开发可计算的变换函数，并证明其优于基线恒等变换。

Result: 在常见基准测试中，平均覆盖率差距从4.20%降低到1.12%，显著提升了覆盖率估计的准确性。

Conclusion: ST-BCP通过数据依赖的分数变换有效缩小了反向共形预测中的覆盖率差距，为不确定性量化提供了更精确的统计框架。

Abstract: Conformal Prediction (CP) provides a statistical framework for uncertainty quantification that constructs prediction sets with coverage guarantees. While CP yields uncontrolled prediction set sizes, Backward Conformal Prediction (BCP) inverts this paradigm by enforcing a predefined upper bound on set size and estimating the resulting coverage guarantee. However, the looseness induced by Markov's inequality within the BCP framework causes a significant gap between the estimated coverage bound and the empirical coverage. In this work, we introduce ST-BCP, a novel method that introduces a data-dependent transformation of nonconformity scores to narrow the coverage gap. In particular, we develop a computable transformation and prove that it outperforms the baseline identity transformation. Extensive experiments demonstrate the effectiveness of our method, reducing the average coverage gap from 4.20\% to 1.12\% on common benchmarks.

</details>


### [425] [Transformers as Measure-Theoretic Associative Memory: A Statistical Perspective and Minimax Optimality](https://arxiv.org/abs/2602.01863)
*Ryotaro Kawata,Taiji Suzuki*

Main category: stat.ML

TL;DR: 论文提出了一种基于概率测度的Transformer理论框架，将上下文视为token分布，注意力作为测度上的积分算子，证明了浅层Transformer能学习"回忆-预测"映射并达到最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer虽然能处理理论上无限长的上下文，但缺乏对注意力机制如何从分布上下文中进行关联记忆的理论理解。论文旨在建立概率测度框架来形式化分析Transformer从分布上下文中回忆和预测的能力。

Method: 将上下文重新表述为token的概率测度，注意力视为测度上的积分算子。对于混合上下文ν和查询x_q，任务分解为：(1)回忆相关分量μ^{(i^*)}，(2)从(μ_{i^*}, x_q)进行预测。研究通过经验风险最小化训练的softmax注意力，在输入密度满足谱假设条件下，证明浅层测度论Transformer能学习回忆-预测映射。

Result: 建立了浅层测度论Transformer学习回忆-预测映射的理论保证，证明了收敛速率的最优性，并建立了匹配的极小极大下界（达到常数倍），表明收敛阶的尖锐性。

Conclusion: 该框架为设计和分析能够从任意长分布上下文中进行回忆的Transformer提供了原则性方法，并提供了可证明的泛化保证，为理解Transformer的关联记忆能力奠定了理论基础。

Abstract: Transformers excel through content-addressable retrieval and the ability to exploit contexts of, in principle, unbounded length. We recast associative memory at the level of probability measures, treating a context as a distribution over tokens and viewing attention as an integral operator on measures. Concretely, for mixture contexts $ν= I^{-1} \sum_{i=1}^I μ^{(i^*)}$ and a query $x_{\mathrm{q}}(i^*)$, the task decomposes into (i) recall of the relevant component $μ^{(i^*)}$ and (ii) prediction from $(μ_{i^*},x_\mathrm{q})$. We study learned softmax attention (not a frozen kernel) trained by empirical risk minimization and show that a shallow measure-theoretic Transformer composed with an MLP learns the recall-and-predict map under a spectral assumption on the input densities. We further establish a matching minimax lower bound with the same rate exponent (up to multiplicative constants), proving sharpness of the convergence order. The framework offers a principled recipe for designing and analyzing Transformers that recall from arbitrarily long, distributional contexts with provable generalization guarantees.

</details>


### [426] [Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration](https://arxiv.org/abs/2602.01912)
*Du-Yi Wang,Guo Liang,Kun Zhang,Qianwen Zhu*

Main category: stat.ML

TL;DR: 提出一种基于离线-模拟-在线估计框架的分位数回归森林方法，用于实时风险价值估计，并通过保形校准确保可靠性


<details>
  <summary>Details</summary>
Motivation: 市场条件快速变化需要实时风险监控，但风险价值的在线估计仍然具有挑战性。准确可靠的风险价值估计对于及时风险控制和明智决策至关重要。

Method: 采用离线-模拟-在线估计框架，离线训练分位数回归森林学习在线风险价值与风险因素之间的关系，在线时结合观测到的风险因素生成实时风险价值估计。进一步开发保形化估计器来校准在线风险价值估计。

Result: 理论分析建立了所提估计器的一致性和覆盖有效性。数值实验证实了所提方法的有效性，并展示了其在实践中的表现。

Conclusion: 首次基于离线-模拟-在线估计框架利用保形校准来可靠地估计实时风险价值，为实时风险监控提供了有效解决方案。

Abstract: Rapidly evolving market conditions call for real-time risk monitoring, but its online estimation remains challenging. In this paper, we study the online estimation of one of the most widely used risk measures, Value at Risk (VaR). Its accurate and reliable estimation is essential for timely risk control and informed decision-making. We propose to use the quantile regression forest in the offline-simulation-online-estimation (OSOA) framework. Specifically, the quantile regression forest is trained offline to learn the relationship between the online VaR and risk factors, and real-time VaR estimates are then produced online by incorporating observed risk factors. To further ensure reliability, we develop a conformalized estimator that calibrates the online VaR estimates. To the best of our knowledge, we are the first to leverage conformal calibration to estimate real-time VaR reliably based on the OSOA formulation. Theoretical analysis establishes the consistency and coverage validity of the proposed estimators. Numerical experiments confirm the proposed method and demonstrate its effectiveness in practice.

</details>


### [427] [Privacy Amplification by Missing Data](https://arxiv.org/abs/2602.01928)
*Simon Roburin,Rafaël Pinot,Erwan Scornet*

Main category: stat.ML

TL;DR: 该论文提出从隐私保护角度重新审视缺失数据，证明不完整数据可以作为差分隐私算法的隐私增强机制


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融等高敏感领域，隐私保护是基本要求，但这些领域的数据常存在缺失值。传统观点将缺失数据视为限制因素，但作者认为缺失可能从隐私保护角度带来好处

Method: 在差分隐私框架下，将缺失数据形式化为隐私增强机制，分析不完整数据如何为差分隐私算法提供隐私放大效应

Result: 首次证明不完整数据可以为差分隐私算法产生隐私放大效果，即缺失数据实际上增强了隐私保护

Conclusion: 缺失数据不应仅被视为限制因素，而可以作为隐私保护的有益机制，为高敏感领域的数据分析提供了新的隐私增强视角

Abstract: Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.

</details>


### [428] [Stochastic Interpolants in Hilbert Spaces](https://arxiv.org/abs/2602.01988)
*James Boran Yu,RuiKang OuYang,Julien Horwood,José Miguel Hernández-Lobato*

Main category: stat.ML

TL;DR: 该论文提出了无限维希尔伯特空间中随机插值子的理论框架，将原本局限于有限维的随机插值子扩展到函数值数据，实现了任意函数分布之间的生成桥接。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已成功扩展到函数值数据，但随机插值子（提供连接任意分布的灵活方法）仍局限于有限维设置。需要建立无限维希尔伯特空间中的随机插值子理论框架。

Method: 建立无限维希尔伯特空间中随机插值子的严格理论框架，包括适定性证明和显式误差界。将框架应用于条件生成，特别是在复杂的基于PDE的基准测试中。

Result: 提出的框架实现了任意函数分布之间的生成桥接，在复杂PDE基准测试中取得了最先进的结果，为科学发现提供了强大的通用工具。

Conclusion: 该工作填补了随机插值子在无限维空间的理论空白，建立了严格的数学框架，为函数值数据的生成建模提供了新的强大工具，在科学计算领域具有重要应用价值。

Abstract: Although diffusion models have successfully extended to function-valued data, stochastic interpolants -- which offer a flexible way to bridge arbitrary distributions -- remain limited to finite-dimensional settings. This work bridges this gap by establishing a rigorous framework for stochastic interpolants in infinite-dimensional Hilbert spaces. We provide comprehensive theoretical foundations, including proofs of well-posedness and explicit error bounds. We demonstrate the effectiveness of the proposed framework for conditional generation, focusing particularly on complex PDE-based benchmarks. By enabling generative bridges between arbitrary functional distributions, our approach achieves state-of-the-art results, offering a powerful, general-purpose tool for scientific discovery.

</details>


### [429] [Training-free score-based diffusion for parameter-dependent stochastic dynamical systems](https://arxiv.org/abs/2602.02113)
*Minglei Yang,Sicheng He*

Main category: stat.ML

TL;DR: 提出无需训练的扩散模型框架，用于学习参数依赖SDE的随机流映射，通过核加权蒙特卡洛估计器实现参数空间插值


<details>
  <summary>Details</summary>
Motivation: 参数依赖随机微分方程（SDEs）的传统模拟方法计算成本高，每个参数值都需要单独的高保真模拟。现有机器学习方法要么需要昂贵的神经网络训练来估计得分函数，要么无法处理连续参数依赖

Method: 提出无需训练的条件扩散模型框架，使用联合核加权蒙特卡洛估计器来近似条件得分函数。该方法利用在离散参数值处采样的轨迹数据，实现状态空间和连续参数域的双重插值

Result: 该方法在三个复杂度递增的数值示例中表现出色，能够准确近似不同参数值下的条件分布。训练后的生成模型可以在训练范围内为任意参数值生成样本轨迹，无需重新训练

Conclusion: 该框架显著加速了参数研究、不确定性量化和实时滤波应用，为参数依赖SDE的高效模拟提供了有效的训练免费解决方案

Abstract: Simulating parameter-dependent stochastic differential equations (SDEs) presents significant computational challenges, as separate high-fidelity simulations are typically required for each parameter value of interest. Despite the success of machine learning methods in learning SDE dynamics, existing approaches either require expensive neural network training for score function estimation or lack the ability to handle continuous parameter dependence. We present a training-free conditional diffusion model framework for learning stochastic flow maps of parameter-dependent SDEs, where both drift and diffusion coefficients depend on physical parameters. The key technical innovation is a joint kernel-weighted Monte Carlo estimator that approximates the conditional score function using trajectory data sampled at discrete parameter values, enabling interpolation across both state space and the continuous parameter domain. Once trained, the resulting generative model produces sample trajectories for any parameter value within the training range without retraining, significantly accelerating parameter studies, uncertainty quantification, and real-time filtering applications. The performance of the proposed approach is demonstrated via three numerical examples of increasing complexity, showing accurate approximation of conditional distributions across varying parameter values.

</details>


### [430] [Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on an Expressive and Cumulant-Controllable Data Model](https://arxiv.org/abs/2602.02153)
*Onat Ure,Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 该论文通过构建可控制高阶统计量的非高斯数据模型，研究数据高阶统计量对神经网络学习动态的影响，发现网络训练遵循从低阶到高阶统计量的渐进学习过程。


<details>
  <summary>Details</summary>
Motivation: 研究数据的高阶统计量（如偏度和峰度）对神经网络学习动态的影响，填补简化数据假设与实际数据复杂性之间的研究空白。

Method: 1. 构建基于Hermite多项式展开的生成式两层神经网络数据模型，可控制高阶累积量；2. 使用该模型生成样本进行在线学习实验；3. 在Fashion-MNIST数据集上预训练生成模型并验证结论。

Result: 1. 神经网络训练呈现"逐阶渐进"模式：先学习低阶统计量（均值和协方差），再逐步学习高阶累积量；2. 在真实数据集上的实验验证了该结论，证明了数据模型在实际场景中的实用性。

Conclusion: 提出的方法在简化数据假设与实际数据复杂性之间架起桥梁，为研究机器学习和信号处理中的分布效应提供了原则性框架。

Abstract: We study the effect of high-order statistics of data on the learning dynamics of neural networks (NNs) by using a moment-controllable non-Gaussian data model. Considering the expressivity of two-layer neural networks, we first construct the data model as a generative two-layer NN where the activation function is expanded by using Hermite polynomials. This allows us to achieve interpretable control over high-order cumulants such as skewness and kurtosis through the Hermite coefficients while keeping the data model realistic. Using samples generated from the data model, we perform controlled online learning experiments with a two-layer NN. Our results reveal a moment-wise progression in training: networks first capture low-order statistics such as mean and covariance, and progressively learn high-order cumulants. Finally, we pretrain the generative model on the Fashion-MNIST dataset and leverage the generated samples for further experiments. The results of these additional experiments confirm our conclusions and show the utility of the data model in a real-world scenario. Overall, our proposed approach bridges simplified data assumptions and practical data complexity, which offers a principled framework for investigating distributional effects in machine learning and signal processing.

</details>


### [431] [PCA of probability measures: Sparse and Dense sampling regimes](https://arxiv.org/abs/2602.02190)
*Gachon Erell,Jérémie Bigot,Elsa Cazelles*

Main category: stat.ML

TL;DR: 该论文研究概率测度的主成分分析(PCA)，在双重渐近框架下（n个测度，每个测度有m个样本），推导出协方差算子和PCA超额风险的收敛速率，揭示了从稀疏到密集的过渡行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单个概率测度嵌入的收敛速率，但缺乏对多个测度情况的分析。本文旨在填补这一空白，研究当观测到n个概率测度（每个测度通过m个样本）时的PCA收敛行为。

Method: 采用双重渐近分析方法，将概率测度嵌入希尔伯特空间，然后应用标准函数型PCA技术。理论分析推导了经验协方差算子和PCA超额风险的收敛速率，形式为n^{-1/2} + m^{-α}，其中α取决于嵌入选择。

Result: 获得了收敛速率n^{-1/2} + m^{-α}，揭示了测度数量n与每个测度的样本数m之间的关系。发现了从稀疏（小m）到密集（大m）的收敛行为过渡，并证明了密集区域速率对于经验协方差误差是极小极大最优的。

Conclusion: 该研究首次在双重渐近框架下分析了多个概率测度的PCA问题，揭示了收敛行为的关键特征。数值实验验证了理论结果，并表明适当的子采样可以在保持PCA精度的同时降低计算成本。

Abstract: A common approach to perform PCA on probability measures is to embed them into a Hilbert space where standard functional PCA techniques apply. While convergence rates for estimating the embedding of a single measure from $m$ samples are well understood, the literature has not addressed the setting involving multiple measures. In this paper, we study PCA in a double asymptotic regime where $n$ probability measures are observed, each through $m$ samples. We derive convergence rates of the form $n^{-1/2} + m^{-α}$ for the empirical covariance operator and the PCA excess risk, where $α>0$ depends on the chosen embedding. This characterizes the relationship between the number $n$ of measures and the number $m$ of samples per measure, revealing a sparse (small $m$) to dense (large $m$) transition in the convergence behavior. Moreover, we prove that the dense-regime rate is minimax optimal for the empirical covariance error. Our numerical experiments validate these theoretical rates and demonstrate that appropriate subsampling preserves PCA accuracy while reducing computational cost.

</details>


### [432] [Transfer Learning Through Conditional Quantile Matching](https://arxiv.org/abs/2602.02358)
*Yikun Zhang,Steven Wilkins-Reeves,Wesley Lee,Aude Hofleitner*

Main category: stat.ML

TL;DR: 提出一种用于回归的迁移学习框架，通过异质源域提升数据稀缺目标域的预测性能，使用条件生成模型和分位数匹配进行分布对齐


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺目标域的预测问题，传统方法通常假设源域和目标域分布相似（如协变量偏移或标签偏移），但这些假设过于严格，需要更灵活的迁移学习框架

Method: 为每个源域单独学习条件生成模型，通过条件分位数匹配将生成的响应校准到目标域，进行分布对齐而不强加限制性假设，为下游学习任务提供高质量数据增强

Result: 理论证明：在增强数据集上训练的ERM比仅使用目标数据的ERM获得更紧的过剩风险界；分位数匹配估计器收敛率控制迁移的偏差-方差权衡；实验表明方法在预测精度上优于仅目标域学习和竞争迁移学习方法

Conclusion: 提出了一种原则性且灵活的迁移学习框架，通过条件生成模型和分位数匹配实现有效的分布对齐，在理论和实践上都显示出优越性能，为数据稀缺场景提供了有效的解决方案

Abstract: We introduce a transfer learning framework for regression that leverages heterogeneous source domains to improve predictive performance in a data-scarce target domain. Our approach learns a conditional generative model separately for each source domain and calibrates the generated responses to the target domain via conditional quantile matching. This distributional alignment step corrects general discrepancies between source and target domains without imposing restrictive assumptions such as covariate or label shift. The resulting framework provides a principled and flexible approach to high-quality data augmentation for downstream learning tasks in the target domain. From a theoretical perspective, we show that an empirical risk minimizer (ERM) trained on the augmented dataset achieves a tighter excess risk bound than the target-only ERM under mild conditions. In particular, we establish new convergence rates for the quantile matching estimator that governs the transfer bias-variance tradeoff. From a practical perspective, extensive simulations and real data applications demonstrate that the proposed method consistently improves prediction accuracy over target-only learning and competing transfer learning methods.

</details>


### [433] [Provably Data-driven Multiple Hyper-parameter Tuning with Structured Loss Function](https://arxiv.org/abs/2602.02406)
*Tung Quoc Le,Anh Tuan Nguyen,Viet Anh Nguyen*

Main category: stat.ML

TL;DR: 提出首个多维超参数调优的泛化保证框架，利用实代数几何工具改进半代数函数类的泛化理论，并应用于加权群LASSO和加权融合LASSO等算法。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的算法设计中，超参数调优的统计基础有限，现有理论仅关注一维超参数，而实际应用中多为多维超参数设置，缺乏理论保证。

Method: 通过实代数几何工具强化半代数函数类的泛化保证框架，建立多维超参数调优的泛化理论，并扩展到验证损失下的超参数调优分析。

Result: 建立了首个多维超参数调优的泛化保证框架，获得了更尖锐、更广泛适用的理论保证，并推导出加权群LASSO和加权融合LASSO等算法的新可学习性结果。

Conclusion: 该框架填补了多维超参数调优理论空白，为数据驱动的算法设计提供了坚实的统计基础，可应用于多种实际机器学习算法。

Abstract: Data-driven algorithm design automates hyperparameter tuning, but its statistical foundations remain limited because model performance can depend on hyperparameters in implicit and highly non-smooth ways. Existing guarantees focus on the simple case of a one-dimensional (scalar) hyperparameter. This leaves the practically important, multi-dimensional hyperparameter tuning setting unresolved. We address this open question by establishing the first general framework for establishing generalization guarantees for tuning multi-dimensional hyperparameters in data-driven settings. Our approach strengthens the generalization guarantee framework for semi-algebraic function classes by exploiting tools from real algebraic geometry, yielding sharper, more broadly applicable guarantees. We then extend the analysis to hyperparameter tuning using the validation loss under minimal assumptions, and derive improved bounds when additional structure is available. Finally, we demonstrate the scope of the framework with new learnability results, including data-driven weighted group lasso and weighted fused lasso.

</details>


### [434] [Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning](https://arxiv.org/abs/2602.02431)
*Filip Kovačević,Hong Chang Ji,Denny Wu,Mahdi Soltanolkotabi,Marco Mondelli*

Main category: stat.ML

TL;DR: 本文证明在单索引模型学习中，全批次梯度下降通过截断激活函数，可以在n≈d样本下实现比单次SGD更好的统计效率，后者需要n≳d log d样本。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为重用训练数据可以提高梯度学习的统计效率，但除了线性回归外，全批次GD相对于单次SGD的理论优势尚不明确。本文旨在研究在单索引模型学习中，全批次GD是否能在统计效率上超越单次SGD。

Method: 研究d维单索引模型（具有二次激活函数），分析全批次球面GD在相关损失上的性能，通过截断激活函数改善优化景观，并对小初始化下的平方损失进行轨迹分析。

Result: 1) 全批次球面GD在相关损失上仍需要n≳d log d样本；2) 截断激活函数后，全批次GD在n≈d样本下展现出有利的优化景观；3) 小初始化下，n≳d样本和T≳log d梯度步足以实现强恢复。

Conclusion: 通过截断激活函数，全批次GD可以在统计效率上超越单次SGD，在单索引模型学习中实现更高效的样本利用。

Abstract: It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\gtrsim d\log d$ samples to achieve weak recovery. We first show that this $\log d$ factor in the sample complexity persists for full-batch spherical GD on the correlation loss; however, by simply truncating the activation, full-batch GD exhibits a favorable optimization landscape at $n \simeq d$ samples, thereby outperforming one-pass SGD (with the same activation) in statistical efficiency. We complement this result with a trajectory analysis of full-batch GD on the squared loss from small initialization, showing that $n \gtrsim d$ samples and $T \gtrsim\log d$ gradient steps suffice to achieve strong (exact) recovery.

</details>
