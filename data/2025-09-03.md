<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 52]
- [cs.LG](#cs.LG) [Total: 208]
- [stat.ML](#stat.ML) [Total: 20]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Exploring the Efficacy of Convolutional Neural Networks in Sleep Apnea Detection from Single Channel EEG](https://arxiv.org/abs/2509.00012)
*Chun Hin Siu,Hossein Miri*

Main category: eess.SP

TL;DR: 使用单通道EEG数据和CNN实现睡眠呼吸暂停检测，准确率85.1%，为家庭诊断提供可行方案


<details>
  <summary>Details</summary>
Motivation: 传统多导睡眠监测(PSG)成本高、操作复杂且影响睡眠质量，需要开发更便捷的家庭诊断方法

Method: 采用卷积神经网络(CNN)处理单通道EEG数据，使用IIR Butterworth滤波器预处理，SMOTETomek处理类别不平衡，构建包含更广时间上下文的数据集

Result: 模型达到85.1%的准确率和0.22的MCC值，证明了家庭自动化检测的可行性

Conclusion: 该方法展示了从实验室诊断向家庭自动化解决方案过渡的可行性，能改善患者预后并提高睡眠障碍诊断的可及性

Abstract: Sleep apnea, a prevalent sleep disorder, involves repeated episodes of
breathing interruptions during sleep, leading to various health complications,
including cognitive impairments, high blood pressure, heart disease, stroke,
and even death. One of the main challenges in diagnosing and treating sleep
apnea is identifying individuals at risk. The current gold standard for
diagnosis, Polysomnography (PSG), is costly, labor intensive, and inconvenient,
often resulting in poor quality sleep data. This paper presents a novel
approach to the detection of sleep apnea using a Convolutional Neural Network
(CNN) trained on single channel EEG data. The proposed CNN achieved an accuracy
of 85.1% and a Matthews Correlation Coefficient (MCC) of 0.22, demonstrating a
significant potential for home based applications by addressing the limitations
of PSG in automated sleep apnea detection. Key contributions of this work also
include the development of a comprehensive preprocessing pipeline with an
Infinite Impulse Response (IIR) Butterworth filter, a dataset construction
method providing broader temporal context, and the application of SMOTETomek to
address class imbalance. This research underscores the feasibility of
transitioning from traditional laboratory based diagnostics to more accessible,
automated home based solutions, improving patient outcomes and broadening the
accessibility of sleep disorder diagnostics.

</details>


### [2] [Conditional Generative Adversarial Networks Based Inertial Signal Translation](https://arxiv.org/abs/2509.00016)
*Marcin Kolakowski*

Main category: eess.SP

TL;DR: 使用条件生成对抗网络将手腕传感器惯性信号转换为鞋部传感器信号，实现基于智能手表的日常步态分析


<details>
  <summary>Details</summary>
Motivation: 现有的先进步态分析方法通常需要鞋部安装传感器，这限制了日常使用。手腕佩戴的智能手表更便于日常监测，但信号格式不兼容现有分析方法

Method: 采用条件生成对抗网络(Conditional GANs)，包括传统二元交叉熵损失训练的GAN和Wasserstein GAN两种版本，测试了卷积自编码器和卷积U-Net两种生成器架构

Result: 实验结果表明该方法能够准确实现信号转换，使手腕传感器信号可用于高效的日常步态分析

Conclusion: 提出的方法成功解决了手腕与鞋部传感器信号兼容性问题，为基于智能手表的日常步态监测提供了可行方案

Abstract: The paper presents an approach in which inertial signals measured with a
wrist-worn sensor (e.g., a smartwatch) are translated into those that would be
recorded using a shoe-mounted sensor, enabling the use of state-of-the-art gait
analysis methods. In the study, the signals are translated using Conditional
Generative Adversarial Networks (GANs). Two different GAN versions are used for
experimental verification: traditional ones trained using binary cross-entropy
loss and Wasserstein GANs (WGANs). For the generator, two architectures, a
convolutional autoencoder, and a convolutional U-Net, are tested. The
experiment results have shown that the proposed approach allows for an accurate
translation, enabling the use of wrist sensor inertial signals for efficient,
every-day gait analysis.

</details>


### [3] [A Fluid Antenna Enabled Physical Layer Key Generation for Next-G Wireless Networks](https://arxiv.org/abs/2509.00018)
*Jiacheng Guo,Ning Gao,Yiping Zuo,Hao Xu,Shi Jin,Kai Kit Wong*

Main category: eess.SP

TL;DR: 提出基于流体天线的物理层密钥生成系统，通过优化预编码矩阵和天线位置，在恶劣传播环境中显著提升密钥生成率


<details>
  <summary>Details</summary>
Motivation: 在恶劣传播环境中，无线信道特性变差，传统物理层密钥生成技术的密钥生成率显著下降，需要新的技术来解决这一挑战

Method: 首先推导流体天线阵列密钥生成率的闭式表达式，然后使用粒子群优化算法联合优化预编码矩阵和天线位置，并开发交替优化算法降低计算复杂度

Result: 仿真结果表明，相比传统固定位置天线阵列和可重构智能表面，流体天线系统在密钥生成率方面表现更优，相比均匀平面天线分别实现了35.42%和67.73%的性能提升

Conclusion: 流体天线通过利用额外的空间自由度，能够有效提升物理层密钥生成系统的性能，特别是在恶劣传播环境中

Abstract: As a promising physical layer security technique, physical layer key
generation (PLKG) enables legitimate users to obtain secret keys from wireless
channel without security infrastructures. However, in harsh propagation
environments, the channel characteristic becomes unsatisfactory, the key
generation rate (KGR) is significantly deteriorated. In this paper, we propose
a novel fluid antenna (FA) enabled PLKG system to address this challenge.
Specifically, we first derive the closed-form expression of the KGR for FA
array, and then jointly optimize the precoding matrix and the antenna positions
via a particle swarm optimization (PSO) algorithm. Next, to further reduce the
computational complexity of the optimization procedure, we develop an
alternating optimization (AO) algorithm, which combines the projected gradient
descent (PGD) and the PSO. Simulation results demonstrate that by exploiting
the additional spatial degree of freedom (DoF), our FA enabled PLKG system is
superior to the benchmarks, such as the conventional fixed-position antenna
(FPA) array and the reconfigurable intelligent surface (RIS). It is worth
highlighting that compared to the conventional uniform planar antenna (UPA),
the FA enabled PLKG achieves a 35.42\% KGR performance improvement under PSO
algorithm and a 67.73\% KGR performance improvement under AO algorithm,
respectively.

</details>


### [4] [A Review of Sensor Insoles](https://arxiv.org/abs/2509.00260)
*Bastian Latsch,Felix Herbst,Mark Suppelt,Julian Seiler,Stephan Schaumann,Sven Suppelt,Alexander A. Altmann,Martin Grimmer,and Mario Kupnik*

Main category: eess.SP

TL;DR: 这篇综述论文评估了足底压力测量技术（足压测量学）的现状，重点关注传感器技术、数量与布局、参与者群体和参考标准，提出了基于测试设备和仪器化跑步机的黄金标准，并探讨了鞋垫插入与足底力学的双向相互作用。


<details>
  <summary>Details</summary>
Motivation: 足底压力测量是分析健康人群和患者运动的重要工具，传感器鞋垫作为可穿戴移动解决方案，在糖尿病足监测、康复指导、辅助设备控制和运动表现分析等应用中具有重要价值。

Method: 通过文献综述评估当前最先进技术，特别关注原始研究和创新设计，涵盖电阻式、电容式、电感式、压电式、摩擦电式和光学传感等多种传感方式。

Result: 研究发现存在传感器校准不足、基于步态的验证缺乏以及人体研究验证不足等问题，确定了组织刚度是传感器信号不确定性的关键来源。

Conclusion: 提出了传感器尺寸和无干扰鞋垫设计的指南，未来发展方向包括开发多模态传感器以弥补单一模式的局限性，以及捕捉压力分布中剪切分量的多轴传感新兴趋势。

Abstract: Plantar pressure measurement, or pedobarography, is an essential tool for
analyzing human motion in healthy individuals and patients. Across the reviewed
literature, sensor insoles are motivated as wearable, mobile solutions for
assessing pressure distribution in applications including diabetic foot
monitoring, rehabilitation guidance, assistive device control, and sports
performance analysis. This review evaluates the current state of the art with
particular attention to sensor technologies, sensor quantity and placement,
participant cohorts, and reference standards. The focus lies on original works
with innovative designs, preferably supported by ambulation experiments. The
modalities covered include resistive, capacitive, inductive, piezoelectric,
triboelectric, and optical sensing approaches. We identify a lack of proper
sensor calibration, gait-based verification, and human study validation, and
propose a gold standard based on testing machines and instrumented treadmills
to ensure comparability across studies. The bidirectional interaction between
insole insertion and foot-sole mechanics is examined, with tissue stiffness
identified as a key source of uncertainty in sensor signals. Guidelines are
provided for sensor dimensions and unobtrusive insole designs to foster natural
gait. Finally, future directions include the development of multimodal sensors
to compensate for limitations of individual modalities and the emerging trend
of multiaxial sensing for capturing shear components in pressure distributions.

</details>


### [5] [CoMET: A Contrastive-Masked Brain Foundation Model for Universal EEG Representation](https://arxiv.org/abs/2509.00314)
*Ang Li,Zikai Wang,Liuyin Yang,Zhenyu Wang,Tianheng Xu,Honglin Hu,Marc M. Van Hulle*

Main category: eess.SP

TL;DR: CoMET是一个基于掩码自编码器和对比学习的大脑基础模型，通过重新设计的EEG分块嵌入和镜像尺度增强，解决了现有模型过度关注局部信号相似性而忽视全局判别模式的问题。


<details>
  <summary>Details</summary>
Motivation: 传统EEG深度学习模型局限于特定数据集和任务，现有自监督基础模型过度依赖掩码重建策略，导致过度关注相邻通道的低维信号相似性特征，而忽视了全局判别模式。

Method: 使用重新设计分块和嵌入的掩码自编码器作为骨干网络，开发了带有镜像尺度增强的新型对比学习框架来增强全局判别能力。在3000多名受试者的100多万个样本上进行预训练。

Result: 在十个不同的下游数据集上评估，取得了最先进的性能，证明了CoMET在提取通用EEG表示方面的卓越能力和强大的临床潜力。

Conclusion: CoMET通过创新的对比学习框架有效解决了现有EEG基础模型的局限性，展现出优异的泛化能力和临床应用价值。

Abstract: Electroencephalography (EEG) is a non-invasive technique for recording brain
activity, widely used in brain-computer interfaces, clinic, and healthcare.
Traditional EEG deep models typically focus on specific dataset and task,
limiting model size and generalization. Recently, self-supervised brain
foundation models have emerged and been applied to various downstream tasks.
Nevertheless, these models still have limitations: current SOTA models
typically rely on masked reconstruction strategy; however, EEG features of
adjacent channels are highly correlated, which causes the pre-training to
overly focus on low-dimensional signal-similarity features in local regions and
neglect the global discriminative patterns vital for downstream tasks. To
address these limitations, we propose a brain foundation model called CoMET.
Specifically, we employ the masked autoencoder with redesigned patching and
embedding for EEG as backbone and devise a novel contrastive learning framework
with mirror-scale augmentation to strengthen the global discrimination ability.
CoMET is pre-trained on mixed EEG datasets over 3000 subjects with over one
million samples. It is evaluated on ten different downstream datasets, and the
SOTA results demonstrate CoMET's superior ability in extracting universal EEG
representations and strong clinical potential.

</details>


### [6] [nRTIS: Low-Cost Real-Time 3D Sonar Imaging Circular Array Supporting Beamforming for Industrial Applications](https://arxiv.org/abs/2509.01212)
*Rens Baeyens,Dennis Laurijssen,Jan Steckel,Walter Daems*

Main category: eess.SP

TL;DR: nRTIS是一个紧凑型超声波传感平台，使用MEMS麦克风圆形阵列和中央超声波换能器，通过微控制器实现实时3D成像，解决了传统超声检测系统成本高、体积大的问题


<details>
  <summary>Details</summary>
Motivation: 传统超声检测系统依赖相控阵和高性能计算硬件，导致成本高昂、体积庞大，不适合便携或嵌入式应用，需要开发更紧凑的解决方案

Method: 采用圆形MEMS麦克风阵列和中央超声波换能器构建传感平台，使用RP2350微控制器实现实时采集，通过高速USB传输数据，并通过点扩散函数模拟和反射器测量进行验证

Result: 点扩散函数模拟显示了波束成形分辨率和旁瓣抑制能力，反射器测量证实了稳健的数据采集性能

Conclusion: nRTIS系统展示了在工业应用中的潜力，如焊缝检测、管道测绘和机器人导航，具有可扩展性

Abstract: Conventional ultrasonic inspection systems rely on phased arrays and
high-performance computing hardware, making them costly, bulky, and unsuitable
for portable or embedded use. In this work, we present nRTIS (nano Real-Time 3D
Imaging Sonar), a compact ultrasonic sensing platform built around a circular
array of MEMS microphones and a central ultrasonic transducer. The device
achieves real-time acquisition through an RP2350 microcontroller and high-speed
USB transfer. We validate the system using both simulations and controlled
experiments: point spread function (PSF) simulations demonstrate beamforming
resolution and sidelobe suppression, while reflector measurements confirm
robust data acquisition. These results highlight the potential of nRTIS for
scalable industrial applications such as weld inspection, pipe mapping, and
robotic navigation.

</details>


### [7] [Gait Analysis using 6DoF Magnetic Tracking](https://arxiv.org/abs/2509.00323)
*R. Abhishek Shankar,Hyungjun Ha,Byunghoo Jung*

Main category: eess.SP

TL;DR: 开发了6自由度磁跟踪步态分析系统，在人体活动识别任务中达到92%的分类准确率，比IMU+磁力计系统提升约5%，证明了磁跟踪系统用于步态分析的可行性


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备在步态分析中具有便携性和可访问性优势，但性能一直不如非可穿戴设备，需要提升可穿戴设备的步态分析性能

Method: 开发了便携、低侵入、无线且节能的6自由度磁跟踪步态分析系统，使用CNN和LSTM两种深度学习分类器对4种活动进行分类，并与IMU+磁力计系统进行性能比较

Result: 磁跟踪系统整体分类准确率达到92%，比IMU+磁力计系统的86.69%提升约5%，在区分行走和负重行走任务上提升约8%

Conclusion: 磁跟踪系统在步态分析中表现出优越性能，证明了使用磁跟踪系统进行步态分析的可行性，突显了完整6自由度跟踪的必要性

Abstract: Gait analysis using wearable devices has advantages over non-wearable devices
when it comes to portability and accessibility. However, non-wearable devices
have consistently shown superior performance in terms of the gait information
they can provide. This calls for the need to improve the performance of
wearable device based gait analysis. To that end, we developed a 6
Degrees-of-Freedom (6DoF) magnetic tracking based gait analysis system as a
step in this direction. The system is portable, minimally intrusive, wireless
and power efficient. As a proof-of-concept, the system was used for the task of
Human Activity Recognition (HAR) to classify four tasks - walking (W), walking
with weight (WW), jogging (J) and marching on the spot (M). Gait data of 12
participants was collected. The classification performance of two deep learning
(DL) classifiers - Convolutional Neural Networks (CNN) and Long Short Term
Memory (LSTM) - was compared. The performance of the magnetic tracking based
gait analysis system was also compared with an Inertial Measurement Unit (IMU)
+ magnetometer based system. The magnetic tracking based system showed an
overall classification accuracy of 92\% compared to 86.69\% for the IMU +
magnetometer system. Moreover, the magnetic tracking system showed an
improvement of about 8\% in being able to differentiate between W and WW. This
highlights the insufficiency in the information content in the data from IMU +
magnetometer, warranting the need for a complete 6DoF tracking. Our work, thus,
proves the feasibility of using magnetic tracking systems for the purpose of
gait analysis.

</details>


### [8] [AN-Aided Secure Beamforming for ELAA-SWIPT in Mixed Near- and Far-Field](https://arxiv.org/abs/2509.00331)
*Yaqian Yi,Guangchi Zhang,Miao Cui,Changsheng You,Qingqing Wu*

Main category: eess.SP

TL;DR: 该论文研究混合近场/远场环境下极大规模天线阵列辅助的SWIPT系统的安全混合波束成形设计，通过同时传输信息和人工噪声来最大化加权和保密速率，同时满足能量收集要求。


<details>
  <summary>Details</summary>
Motivation: 随着6G通信发展，极大规模天线阵列和混合近场/远场环境下的安全通信成为重要挑战，需要设计有效的波束成形方案来同时保障信息安全和能量传输。

Method: 提出基于连续凸近似的迭代算法，针对Type-I和Type-II两种信息接收器类型分别制定优化问题，同时考虑人工噪声消除能力和能量收集约束。

Result: 仿真结果验证了所提方案的有效性，揭示了混合场SWIPT系统的安全性能特性，特别是可见区域和角度用户分离对性能的影响。

Conclusion: 该研究为混合近场/远场环境下的安全SWIPT系统提供了有效的波束成形设计方案，对6G通信安全具有重要指导意义。

Abstract: This letter investigates secure hybrid beamforming (HB) design for an
extremely large-scale antenna array-aided simultaneous wireless information and
power transfer (SWIPT) system operating in a mixed near-field (NF)/far-field
(FF) environment. A base station (BS) employs HB to transmit information and
artificial noise (AN) signals simultaneously to multiple FF information
receivers (IRs) and NF energy receivers (ERs). The objective is to maximize the
weighted sum secrecy rate for the IRs, considering both Type-I (unable to
cancel AN) and Type-II (capable of canceling AN) IRs, subject to minimum energy
harvesting requirements at the ERs and a BS transmit power constraint. We
formulate optimization problems for both IR types and develop an efficient
iterative algorithm based on successive convex approximation. Simulation
results validate the proposed scheme and provide crucial insights into the
security performance of mixed-field SWIPT systems, highlighting the influence
of visibility regions and angular user separation.

</details>


### [9] [Pilot Allocation and Receiver Design for Cell-Free Massive MIMO ISAC Systems](https://arxiv.org/abs/2509.00478)
*Getuar Rexhepi,Kuranage Roche Rayan Ranasinghe,Kengo Ando,Giuseppe Thadeu Freitas de Abreu,David Gonzalez G*

Main category: eess.SP

TL;DR: 本文解决免细胞大规模MIMO系统中的预学分配和接收机设计问题，通过流形优化方法设计几乎正交预学序列，并提出基于高斯信念传播的低复杂度接收机算法。


<details>
  <summary>Details</summary>
Motivation: 俄免细胞大规模MIMO系统在集成感知与通信中面临预学分配效率低和接收机设计复杂的挑战，需要新方法来提高系统性能和实际部署性。

Method: 使用流形优化框架设计几乎正交的单模预学序列，在频域满足单模约束；引入基于高斯信念传播(GaBP)的接收机算法。

Result: 模拟结果显示所提预学分配方法在通信性能上与最新算法相当，但感知能力更优；GaBP接收机实现了稳健性能和更低的计算复杂度。

Conclusion: 这些贡献推进了免细胞大规模MIMO系统在集成感知与通信中的实际部署。

Abstract: This paper tackles two key challenges in cell-freemassive multiple input
multiple output (CF-mMIMO) systems:efficient pilot allocation and practical
receiver design. To thisend, we introduce a novel pilot allocation framework
leveragingmanifold optimization to maximize the system sum rate, wherepilot
sequences are designed as nearly orthogonal sequences. Theproposed pilot design
enforces unimodularity constraints in thefrequency domain, ensuring pilots are
suitable for both communi-cation and sensing tasks. Additionally, a gaussian
belief propaga-tion (GaBP)-based receiver is introduced, providing
near-optimaldetection performance with substantially reduced
computationalcomplexity. Simulation results demonstrate that the proposedpilot
allocation method achieves communication performancecomparable to
state-of-the-art (SotA) algorithms, while deliveringsuperior sensing
capabilities due to its unimodular pilot design.The GaBP-based receiver
achieves robust performance andlower complexity compared to conventional
approaches. Thesecontributions advance the practical deployment of CF-mMIMOfor
integrated sensing and communications (ISAC).

</details>


### [10] [Distributed Deployment and Dual-Frequency Concepts to Strengthen Sub-THz Wireless Systems](https://arxiv.org/abs/2509.00492)
*Liesbet Van der Perre,Gilles Callebaut,Thomas Eriksson,Muris Sarajlic,Christian Fager,Fredrik Tufvesson,Buon Kiong Lau,Erik G. Larsson*

Main category: eess.SP

TL;DR: 通过聚合物微波纤维传输次太赫系统信号，采用链式配置低复杂度无线单元，结合双频段下行操作，解决次太赫网络部署挑战


<details>
  <summary>Details</summary>
Motivation: 次太赫频段具有庞大带宽潜力，但基于物理约束和技术限制，可靠的次太赫网络部署面临挑战

Method: 使用聚合物微波纤维传输RF信号，链式连接低复杂度无线单元，采用双频段下行操作集成次太赫系统和次-10GHz系统

Result: 分布式架构减少路径损耗和阻塞，低复杂度雕块化天线提高效能，双频模式提供控制信令和备用解决方案

Conclusion: 该架构能够充分发挥次太赫技术潜力，为动态环境中的高性能、高效能、实时连接开启新途径

Abstract: The vast bandwidth available at sub-THz frequencies holds great promise for
high-speed wireless access, precise localization, and advanced sensing
applications. However, fundamental physical constraints and technological
limitations make the deployment of reliable sub-THz networks challenging. We
propose a new paradigm for sub-THz coverage by transmitting the RF signals over
polymer microwave fibers (PMFs) that interconnect low-complexity radio units
(RUs) in a daisy-chain configuration. The distributed architecture ensures that
user equipments (UEs) connect to RUs in their proximity, reducing path loss and
mitigating blocking. The RUs leverage low-complexity, compact integrated
antenna modules. Additionally, dual-frequency tandem operation is proposed,
integrating the sub-THz system with a sub-10 GHz system that provides control
signalling and a robust fallback solution for the sub-THz system. This proposed
tandem architecture can open up the full potential of sub-THz technology and
paves the way to cost- and energy-efficient, high-performance, real-time
connectivity in dynamic environments.

</details>


### [11] [Robust Resource Allocation for LEO Satellite-Assisted Secure SWIPT via STAR-RIS under CSI Uncertainty](https://arxiv.org/abs/2509.00568)
*Zahra Rostamikafaki,Francois Chan,Claude D'Amours*

Main category: eess.SP

TL;DR: 这篇论文提出了一种基于STAR-RIS的稳健资源分配框架，用于低轨道卫星启用的SWIPT系统，通过处理额定通道不确定性来最大化收集功率并确保保密速率要求。


<details>
  <summary>Details</summary>
Motivation: 解决直接卫星-地面链路被阻塞时的通信问题，利用STAR-RIS的同时发射和反射特性来提升SWIPT系统的性能和安全性。

Method: 采用S-procedure处理通道不确定性，通过交替优化(AO)框架聚合优化主动和被动政形，使用惩罚基策略来强制STAR-RIS政形设计。

Result: 模拟结果验证了算法的有效性，并证明STAR-RIS架构在总收集功率方面显著超越传统RIS和其他基准方案。

Conclusion: 该框架能够在实际通道不完整的情况下实现稳健的资源分配，STAR-RIS技术在提升SWIPT系统性能方面具有显著优势。

Abstract: This paper proposes a robust resource allocation framework for a low Earth
orbit (LEO) satellite-enabled simultaneous wireless information and power
transfer (SWIPT) system, assisted by a ground-deployed simultaneously
transmitting and reflecting reconfigurable intelligent surface (STAR-RIS). We
consider a scenario where direct satellite-to-ground links are obstructed, and
the satellite serves multiple single-antenna energy receivers, information
receivers, and eavesdroppers exclusively via the STAR-RIS. A robust
optimization problem is formulated to maximize the total harvested power,
subject to secrecy rate requirements, transmit power limits, and STAR-RIS
coefficient constraints, under a practical bounded channel state information
(CSI) error model. To achieve optimal robust resource allocation, we address
the challenges posed by coupled optimization variables and bounded channel
estimation errors by first applying the S-procedure to handle robustness
against channel uncertainty. An alternating optimization (AO) framework is
subsequently proposed, where the active beamforming at the LEO satellite and
the passive beamforming at the STAR-RIS are jointly optimized, and a
penalty-based strategy is incorporated to enforce the STAR-RIS beamforming
design. Simulation results validate the effectiveness of the proposed algorithm
and demonstrate that the STAR-RIS architecture achieves substantial performance
gains in total harvested power over conventional RIS and other baseline
schemes.

</details>


### [12] [PyNoetic: A modular python framework for no-code development of EEG brain-computer interfaces](https://arxiv.org/abs/2509.00670)
*Gursimran Singh,Aviral Chharia,Rahul Upadhyay,Vinay Kumar,Luca Longo*

Main category: eess.SP

TL;DR: PyNoetic是一个模块化的脑机接口框架，解决了现有BCI框架缺乏灵活性、学习曲线陡峭、成本高和功能不完整的问题，提供从刺激呈现到可视化的全流程支持，包含直观的GUI和无代码设计功能。


<details>
  <summary>Details</summary>
Motivation: 现有脑机接口框架存在多个局限性：缺乏实验研究所需的阶段灵活性、对无编程经验的研究者学习曲线陡峭、依赖专有软件导致成本高昂、功能不完整需要多个外部工具影响研究结果。

Method: 开发PyNoetic模块化BCI框架，包含完整的BCI设计流程（刺激呈现、数据采集、通道选择、滤波、特征提取、伪影去除、仿真和可视化），提供直观的端到端GUI和独特的拖拽式可配置流程图实现无代码BCI设计，支持自定义功能集成。

Result: PyNoetic是少数几个用Python实现完整BCI设计流程的框架之一，包含丰富的分析工具（机器学习模型、脑连接指标、系统测试功能、新范式评估方法），支持离线和实时BCI开发。

Conclusion: PyNoetic通过其多功能性和易用性简化了BCI设计过程，使研究人员能够专注于BCI开发的更复杂方面，从而加速研究进展。

Abstract: Electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) have
emerged as a transformative technology with applications spanning robotics,
virtual reality, medicine, and rehabilitation. However, existing BCI frameworks
face several limitations, including a lack of stage-wise flexibility essential
for experimental research, steep learning curves for researchers without
programming expertise, elevated costs due to reliance on proprietary software,
and a lack of all-inclusive features leading to the use of multiple external
tools affecting research outcomes. To address these challenges, we present
PyNoetic, a modular BCI framework designed to cater to the diverse needs of BCI
research. PyNoetic is one of the very few frameworks in Python that encompasses
the entire BCI design pipeline, from stimulus presentation and data acquisition
to channel selection, filtering, feature extraction, artifact removal, and
finally simulation and visualization. Notably, PyNoetic introduces an intuitive
and end-to-end GUI coupled with a unique pick-and-place configurable flowchart
for no-code BCI design, making it accessible to researchers with minimal
programming experience. For advanced users, it facilitates the seamless
integration of custom functionalities and novel algorithms with minimal coding,
ensuring adaptability at each design stage. PyNoetic also includes a rich array
of analytical tools such as machine learning models, brain-connectivity
indices, systematic testing functionalities via simulation, and evaluation
methods of novel paradigms. PyNoetic's strengths lie in its versatility for
both offline and real-time BCI development, which streamlines the design
process, allowing researchers to focus on more intricate aspects of BCI
development and thus accelerate their research endeavors. Project Website:
https://neurodiag.github.io/PyNoetic

</details>


### [13] [Uninformed-to-Informed Estimation: A Ping-Pong Positioning Method for Multi-user Wideband mmWave Systems](https://arxiv.org/abs/2509.00727)
*Lin Guo,Tiejun Lv,Yashuai Cao,Mugen Peng*

Main category: eess.SP

TL;DR: 基于定位误差下界的分布式谷梯定位框架，通过广带多子载波协同和复合路径定位，在减少时间资源消耗的同时显著提升定位精度


<details>
  <summary>Details</summary>
Motivation: 解决广带毫米波系统中动态用户设备定位性能不佳的问题，提高定位精度和跟踪性能

Method: 提出PELB驱动的谷梯定位框架，设计多子载波协同定位误差下界(MSCPEB)指标，发展交替优化算法优化混合基带成型，并建立不依赖传统三角关系的多路径协同定位方法

Result: 数值实验表明，该方法与未优化基带配置的方案相比，估计精度提升至少16%，仅需约四分之一的时间窗口资源

Conclusion: 该研究提出的定位框架通过理论分析和算法设计，在广带毫米波系统中实现了高效准确的动态用户定位，为无线定位技术提供了新的解决方案

Abstract: To enhance the positioning and tracking performance of dynamic user equipment
(UE) in wideband millimeter-wave (mmWave) systems, we propose a novel
positioning error lower bound (PELB)-driven ping-pong positioning framework,
where the base station (BS) and UE alternately transmit and receive adaptive
beamforming signals for positioning. All beam-formers are scheduled based on
the locally evaluated PELB. In this framework, we exploit multi-dimensional
information fusion to assist in positioning. Firstly, a multi-subcarrier
collaborative positioning error lower bound (MSCPEB) is proposed to evaluate
the positioning error limits of wideband mmWave systems, which quantifies the
contribution of all subcarriers to positioning accuracy. Moreover, we prove
that the MSCPEB does not exceed the arithmetic mean of the PELBs of the
individual subcarriers. Subsequently, we develop an alternating optimization
(AO) algorithm to optimize the hybrid beamformers targeted for MSCPEB
minimization. By convexifying this problem, closed-form solutions of
beamformers are derived. Finally, we develop a multipath collaborative
positioning method that quantifies the impact of path reliability on
positioning accuracy, with a closed-form solution for user position derived.
The proposed method does not rely on path resolution and traditional triangular
relationships. Numerical results validate that the proposed method improves
estimation accuracy by at least 16% compared to potential schemes without
optimized beam configurations, while requiring only approximately one-quarter
of the slot resources.

</details>


### [14] [Characterization of Mega-Constellation Links for LEO Missions With Applications to EO and ISS Use Cases](https://arxiv.org/abs/2509.00766)
*G. Maiolini Capez,M. A. Caceres,C. P. Bridges,S. Frey,R. Armellin,R. Garello,P. Bargellini*

Main category: eess.SP

TL;DR: 本文展示了LEO巨型星座（如OneWeb和Starlink）可为太空用户提供卓越的通信连接，分析了通信链路特性、多系统用户优势，并探讨了GEO星座补充的多轨道系统改进潜力。


<details>
  <summary>Details</summary>
Motivation: 随着卫星任务对连接性需求的不断增加，特别是在LEO领域，需要探索利用现有巨型星座为太空用户提供通信服务的新范式。

Method: 通过表征太空用户与OneWeb和Starlink星座之间的通信链路，分析可用性、访问时长、多普勒效应和路径损耗等参数，考虑用户轨道参数的影响，并评估多系统用户和多轨道系统的性能。

Result: 研究结果表明巨型星座连接解决方案具有显著优势，能够将LEO航天器转变为高度响应的空间到空间网络节点，国际空间站和地球观测卫星等用例验证了其有效性。

Conclusion: 巨型星座不仅能为地面用户提供创新服务，还能为LEO航天器提供优秀的通信连接，多轨道系统的整合可进一步提升性能，这代表了空间通信的新范式。

Abstract: Satellite missions demand ever greater connectivity, especially in the LEO
regime. In this paper, we introduce the new mega-constellation services in
space paradigm: we show that megaconstellations, deployed to offer innovative
services to Earth's users, can provide excellent connectivity to LEO spacecraft
as well. First, we characterise the communication link between space users and
the actual OneWeb and Starlink constellations. A full set of results in terms
of availability, access duration, Doppler, and path losses as a function of
user orbital parameters, identifying optimal user orbits, is provided. The
results achieved by a multi-system user able to communicate with both fleets
are also presented. The potential improvements available if geostationary
constellations are used to complement LEO megaconstellations in a multi-orbit
system are discussed as well. Finally, we focus on two LEO use cases: the
International Space Station and an Earth Observation Sun Synchronous satellite.
All the results demonstrate the numerous advantages of the mega-constellation
connectivity solution, which can transform LEO spacecraft into highly
responsive nodes of a space-to-space network.

</details>


### [15] [Fast Regularized 3D Near-Field MIMO Imaging Using Stochastic Proximal Gradient Method](https://arxiv.org/abs/2509.00774)
*Okyanus Oral*

Main category: eess.SP

TL;DR: 提出基于随机近端梯度法的快速正则化重建方法，用于三维近场MIMO雷达成像，显著提升计算速度且保持重建质量


<details>
  <summary>Details</summary>
Motivation: 近场MIMO雷达成像因分布式天线的不规则空间采样导致计算负荷高，现有加速方法主要针对直接重建，难以适应不同MIMO几何结构和正则化反演需求

Method: 基于随机近端梯度法开发快速正则化重建方法

Result: 实验测量显示运行时间显著改善，重建质量无明显妥协

Conclusion: 该方法有效解决了近场MIMO成像的计算效率问题，为不同几何结构的应用提供了便利

Abstract: Near-field multiple-input multiple-output (MIMO) radar imaging suffers from
high computational load inherently due to irregular spatial sampling with
distributed antennas. Existing acceleration methods for near-field MIMO imaging
typically rely on interpolation or compensation of measurements and are
primarily developed for direct reconstruction. This hinders their ease of
adoption for different MIMO geometries and requires further modification for
regularized inversion. In this study, we address these challenges by developing
a fast regularized reconstruction approach for three-dimensional near-field
MIMO imaging based on the Stochastic Proximal Gradient Method. We demonstrate
the performance of the developed approach through experimental measurements.
The results show a significant improvement in runtime without any notable
compromise in reconstruction quality.

</details>


### [16] [Deep Unfolding with Approximated Computations for Rapid Optimization](https://arxiv.org/abs/2509.00782)
*Dvir Avrahami,Amit Milstein,Caroline Chaux,Tirza Routtenberg,Nir Shlezinger*

Main category: eess.SP

TL;DR: 通过展开优化步骤并替换部分迭代为低复杂度近似计算，该方法同时减少迭代次数和每次迭代复杂度，让优化器更适合延迟敏感应用。


<details>
  <summary>Details</summary>
Motivation: 传统迭代优化方法在延迟敏感系统中应用受限，深度展开方法虽然减少了迭代次数，但没有解决每次迭代的高计算成本问题。

Method: 基于展开固定数量的优化步骤，将选定迭代替换为低复杂度近似计算，并从数据中学习扩展的超参数来补偿引入的近似。

Result: 在混合波束形成和稳健主成分分析两个典型问题上，该方法能够达到最先进性能，同时将计算复杂度降低了3个数量级。

Conclusion: 该新型学习优化框架能够在实时系统中实现快速、可解释和高效的决策制定，具有广阔的应用潜力。

Abstract: Optimization-based solvers play a central role in a wide range of signal
processing and communication tasks. However, their applicability in
latency-sensitive systems is limited by the sequential nature of iterative
methods and the high computational cost per iteration. While deep unfolding has
emerged as a powerful paradigm for converting iterative algorithms into learned
models that operate with a fixed number of iterations, it does not inherently
address the cost of each iteration. In this paper, we introduce a learned
optimization framework that jointly tackles iteration count and per-iteration
complexity. Our approach is based on unfolding a fixed number of optimization
steps, replacing selected iterations with low-complexity approximated
computations, and learning extended hyperparameters from data to compensate for
the introduced approximations. We demonstrate the effectiveness of our method
on two representative problems: (i) hybrid beamforming; and (ii) robust
principal component analysis. These fundamental case studies show that our
learned approximated optimizers can achieve state-of-the-art performance while
reducing computational complexity by over three orders of magnitude. Our
results highlight the potential of our approach to enable rapid, interpretable,
and efficient decision-making in real-time systems.

</details>


### [17] [Spectrum Cognition: Semantic Situation for Next-Generation Spectrum Management](https://arxiv.org/abs/2509.00851)
*Hao Zhang,Fuhui Zhou,Qihui Wuand Chau Yuen*

Main category: eess.SP

TL;DR: 该论文给出了谱识别的全面概述，定义了谱识别并区分了与传统谱感知的不同，提出了从数据处理到信号分析再到语义情况的创新视角，并提出了解决关键挑战的具体技术方案。


<details>
  <summary>Details</summary>
Motivation: 围绕未来无线通信网络的复杂性和需求增长，谱识别成为优化谱布利用的关键技术，需要提升无线系统的效率和安全性。

Method: 通过"数据处理到信号分析再到语义情况"的创新视角，对传统和智能谱识别框架进行全面分析，并提出具体技术解决方案。

Result: 识别了谱识别的关键挑战，展示了语义情况在构建下一代无线系统中的转型潜力，为理论理解和实际应用提供了见解。

Conclusion: 谱识别通过语义情况提取有意义信息，能够为网络决策提供智能支持，对下一代无线系统的发展具有重要意义。

Abstract: In response to the growing complexity and demands of future wireless
communication networks, spectrum cognition has emerged as an essential
technique for optimizing spectrum utilization in next-generation wireless
networks. This article presents a comprehensive overview of spectrum cognition,
underscoring its critical role in enhancing the efficiency and security of
future wireless systems through the innovative perspective of "data processing
to signal analysis to semantic situation". Semantic situation, as the highest
level of spectrum cognition, enables the extraction of meaningful information
from raw spectrum data to provide intelligent support for network decisions. We
formally define spectrum cognition, clearly distinguishing it from traditional
spectrum sensing, and delve into the latest advancements in both traditional
and intelligent spectrum cognition frameworks, addressing key challenges in
spectrum cognition. Furthermore, we propose concrete technical solutions to
address these challenges, highlighting the transformative potential of semantic
situation in shaping next-generation wireless systems. Our findings not only
contribute to the theoretical understanding of spectrum cognition but also
offer practical insights for its implementation in real-world scenarios.

</details>


### [18] [Lightweight Error-Correction Code Encoders in Superconducting Electronic Systems](https://arxiv.org/abs/2509.00962)
*Yerzhan Mustafa,Berker Peköz,Selçuk Köse*

Main category: eess.SP

TL;DR: 提出了三种基于SFQ逻辑的轻量级纠错码编码器（Hamming(7,4)、Hamming(8,4)和Reed-Muller(1,3)），分析了其在工艺参数变化下的性能，并探讨了理论复杂度和物理尺寸之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 超导电子电路（如SFQ逻辑）到室温电子设备的数据传输容易受到位错误影响，这些错误可能来自磁通俘获、制造缺陷和工艺参数变化。由于4.2K冷却功率预算和芯片面积限制，纠错码编码器的尺寸受到严格限制。

Method: 基于Hamming(7,4)、Hamming(8,4)和Reed-Muller(1,3)码设计轻量级纠错码编码器，并使用SFQ逻辑实现。在存在工艺参数变化的情况下分析这些编码器的性能。

Result: 实现了三种轻量级纠错码编码器，并分析了它们在工艺参数变化环境下的性能表现。

Conclusion: 识别了纠错码编码器理论复杂度与物理尺寸之间的权衡关系，为超导电子系统中有限资源条件下的纠错方案选择提供了指导。

Abstract: Data transmission from superconducting electronic circuits, such as single
flux quantum (SFQ) logic, to room-temperature electronics is susceptible to bit
errors, which may result from flux trapping, fabrication defects, and process
parameter variations (PPV). Due to the cooling power budget at 4.2 K and
constraints on the chip area, the size of the error-correction code encoders is
limited. In this work, three lightweight error-correction code encoders are
proposed that are based on Hamming(7,4), Hamming(8,4), and Reed-Muller(1,3)
codes and implemented with SFQ logic. The performance of these encoders is
analyzed in the presence of PPV. The trade-offs between the theoretical
complexity and physical size of error-correction code encoders are identified.

</details>


### [19] [Doubly-Dispersive Continuous MIMO Systems: Channel Modeling and Beamforming Design](https://arxiv.org/abs/2509.00964)
*Kuranage Roche Rayan Ranasinghe,Zhaolin Wang,Hyeon Seok Rou,Giuseppe Thadeu Freitas de Abreu,Emil Björnson*

Main category: eess.SP

TL;DR: 本文针对双色散信道中的MIMO连续孔径阵列系统，提出了全面的信道模型和最优波束成形设计方法，通过变分法获得低复杂度闭式解，在性能和计算复杂度上显著优于传统MIMO系统。


<details>
  <summary>Details</summary>
Motivation: 双色散信道中的MIMO连续孔径阵列系统需要新的建模和波束成形设计方法，以支持集成感知与通信应用，并克服传统方法在复杂信道环境中的性能限制。

Method: 推导了双色散连续MIMO信道模型，获得了OFDM、OTFS和AFDM等波形的显式输入输出关系，通过变分法设计了最大化接收功率的收发波束成形矩阵的闭式解。

Result: 仿真结果表明，所提出的CAPA波束成形设计在双色散信道中相比传统MIMO系统提供了显著的性能提升和计算复杂度降低。

Conclusion: 该方法为双色散信道中的连续孔径阵列系统提供了有效的波束成形解决方案，在集成感知与通信应用中具有重要价值。

Abstract: We address the modeling and optimal beamforming (BF) design for
multiple-input multiple-output (MIMO) continuous aperture array (CAPA) systems
operating over doubly-dispersive (DD) channels. First, a comprehensive DD
continuous MIMO (DDC MIMO) channel model that incorporates CAPAs at both the
transmitter (TX) and receiver (RX) is derived, which is used to obtain explicit
input-output (I/O) relations for various waveforms well suited to integrated
sensing and communications (ISAC) and robust to DD channels, namely orthogonal
frequency division multiplexing (OFDM), orthogonal time frequency space (OTFS),
and affine frequency division multiplexing (AFDM). Then, functional
optimization problems are formulated for the design of TX and RX BF matrices
that maximize received power, in which novel low-complexity, closed-form
solutions are obtained via the calculus of variations (CoV) method, yielding
expressions closely related to the classical matched filter commonly used in
conventional MIMO systems. Simulation results confirm that the proposed TX/RX
BF designs with CAPAs provide significant performance and computational
complexity gains over conventional MIMO systems in DD channels.

</details>


### [20] [Localized Supervised Learning for Cryo-ET Reconstruction](https://arxiv.org/abs/2509.00968)
*Vinith Kishore,Valentin Debarnot,AmirEhsan Khorashadizadeh,Ivan Dokmanić*

Main category: eess.SP

TL;DR: 本文提出一种轻量级网络方法，利用Cryo-电子断层扫描的局部特性，通过局部数据训练来改善噪声和缺失榧问题，降低计算成本和内存需求。


<details>
  <summary>Details</summary>
Motivation: Cryo-电子断层扫描(Cryo-ET)在结构生物学中很重要，但因电子损伤限制导致噪声大、数据不完整。当前自监督学习方法需要训练大型3D UNet网络，计算成本高。

Method: 利用前向模型的局部性质，仅使用测量数据的局部部分训练轻量级网络，提供了计算成本和时间需求的灵活平衡。

Result: 实验结果显示，该网络在少量测量数据训练后，能够在未见的数据集上进行高精度的重建，具有良好的过逆合性能。

Conclusion: 该方法通过局部化训练策略，在保持高准确性的同时显著降低了Cryo-ET重建的计算复杂度和内存需求，为结构生物学提供了更高效的解决方案。

Abstract: Cryo-electron tomography (Cryo-ET) is a powerful tool in structural biology
for 3D visualization of cells and biological systems at resolutions sufficient
to identify individual proteins in situ. The measurements are collected by
tilting the frozen specimen and exposing it to an electron beam of known
dosage. As the biological samples are prone to electron damage, the samples can
be exposed to only a limited dosage of electrons, leading to noisy and
incomplete measurements. Thus, the reconstructions are noisy and incomplete,
leading to the missing wedge problem. Currently, self-supervised learning is
used to compensate for this issue. This typically involves, for each volume to
recover, training a large 3D UNet on the initial noisy reconstruction, leading
to large training time and memory requirements. In this work, we exploit the
local nature of the forward model to train a lightweight network using only
localized data from the measurements. This design provides flexibility in
balancing computational and time requirements while reconstructing the volumes
with high accuracy. We observe experimentally that this network can work well
on unseen datasets, despite using a network trained on a few measurements.

</details>


### [21] [BSNeRF: Broadband Spectral Neural Radiance Fields for Snapshot Multispectral Light-field Imaging](https://arxiv.org/abs/2509.01070)
*Erqi Huang,John Restrepo,Xun Cao,Ivo Ihrke*

Main category: eess.SP

TL;DR: 提出了一种用于快照多光谱光场成像系统的宽带光谱神经辐射场(BSNeRF)模型，成功解决了宽带多路复用光谱的解耦问题，提高了多光谱光场图像重建质量


<details>
  <summary>Details</summary>
Motivation: 现有的SMLI方法要么通过降低光通量要么延长成像时间来回避模型解耦的挑战，需要一种能够有效处理宽带光谱解耦的解决方案

Method: 开发了宽带光谱神经辐射场(BSNeRF)模型，专门用于快照多光谱光场成像系统，在优化过程中考虑宽带光谱解耦

Result: 实验表明该模型成功解耦了宽带多路复用光谱，增强了多光谱光场图像的重建效果

Conclusion: 该方法推进了全光成像技术的发展，为高维数据重建提供了有效的宽带光谱解耦解决方案

Abstract: Snapshot Multispectral Light-field Imaging (SMLI) is an emerging
computational imaging technique that captures high-dimensional data (x, y, z,
$\theta$, $\phi$, $\lambda$) in a single shot using a low-dimensional sensor.
The accuracy of high-dimensional data reconstruction depends on representing
the spectrum using neural radiance field models, which requires consideration
of broadband spectral decoupling during optimization. Currently, some SMLI
approaches avoid the challenge of model decoupling by either reducing
light-throughput or prolonging imaging time. In this work, we propose a
broadband spectral neural radiance field (BSNeRF) for SMLI systems. Experiments
show that our model successfully decouples a broadband multiplexed spectrum.
Consequently, this approach enhances multispectral light-field image
reconstruction and further advances plenoptic imaging.

</details>


### [22] [A Bayesian Framework For Cascaded Channel Estimation in RIS-Aided mmWave Systems](https://arxiv.org/abs/2509.01117)
*Gyoseung Lee,Junil Choi*

Main category: eess.SP

TL;DR: 提出基于变分推断的RIS辅助毫米波多用户系统级联信道估计方法，使用复自适应拉普拉斯先验来近似非高斯信道增益分布，相比传统LS和LMMSE估计器性能更优


<details>
  <summary>Details</summary>
Motivation: 由于级联RIS信道的复杂信道增益通常是非高斯的，使用线性最小均方误差(LMMSE)估计器会导致不可避免的性能下降，需要更准确的分布建模方法

Method: 提出变分推断框架，使用复自适应拉普拉斯先验来近似复杂信道增益的概率分布，以可处理的方式有效捕捉其分布特性

Result: 数值结果表明，所提出的估计器在级联信道估计误差方面优于包括最小二乘(LS)和LMMSE在内的传统估计器

Conclusion: 基于变分推断和复自适应拉普拉斯先验的信道估计框架能够有效处理RIS辅助毫米波系统中的非高斯信道特性，提供更准确的级联信道估计性能

Abstract: In this paper, we investigate cascaded channel estimation for reconfigurable
intelligent surface (RIS)-aided millimeter-wave multi-user communication
systems. Since the complex channel gains of the cascaded RIS channel are
generally non-Gaussian, the use of the linear minimum mean squared error
(LMMSE) estimator leads to inevitable performance degradation. To tackle this
issue, we propose a variational inference-based framework that approximates the
complex channel gains using a complex adaptive Laplace prior, which effectively
captures their probability distributions in a tractable way. Numerical results
demonstrate that the proposed estimator outperforms conventional estimators
including least squares and LMMSE in terms of cascaded channel estimation
error.

</details>


### [23] [Fluid Antenna Port Prediction based on Large Language Models](https://arxiv.org/abs/2509.01121)
*Yali Zhang,Haifan Yin,Weidong Li,Emil Bjornson,Merouane Debbah*

Main category: eess.SP

TL;DR: 本文首次将大语言模型应用于流体天线移动端口预测，提出Port-LLM模型，通过专门的数据处理模块实现优异的预测性能和频谱效率。


<details>
  <summary>Details</summary>
Motivation: 解决用户设备移动性挑战，通过预测流体天线移动端口来优化天线位置选择，提升通信性能。

Method: 基于预训练GPT-2框架，设计专门的数据预处理、输入嵌入和输出投影模块，实现无线通信数据与LLM数据格式的有效连接。

Result: 模型在不同基站天线数量和用户设备速度下都显示出优异的预测性能，具有强大的泛化能力和稳健性，在中高速移动环境中频谱效率超过传统方法。

Conclusion: Port-LLM模型成功将LLM应用于流体天线端口预测，为解决用户设备移动性问题提供了有效方案，在通信性能上取得显著改善。

Abstract: This study seeks to utilize large language models (LLMs) to forecast the
moving ports of fluid antenna (FA). By repositioning the antenna to the
locations identified by our proposed model, we intend to address the mobility
challenges faced by user equipment (UE). To the best of our knowledge, this
paper introduces, for the first time, the application of LLMs in the prediction
of FA ports, presenting a novel model termed Port-LLM. The architecture of our
model is based on the pre-trained GPT-2 framework. We designed specialized data
preprocessing, input embedding, and output projection modules to effectively
bridge the disparities between the wireless communication data and the data
format utilized by the pre-trained LLM. Simulation results demonstrate that our
model exhibits superior predictive performance under different numbers of base
station (BS) antennas and varying UE speeds, indicating strong generalization
and robustness ability. Furthermore, the spectral efficiency (SE) attained by
our model surpasses that achieved by traditional methods in both medium and
high-speed mobile environments.

</details>


### [24] [Enabling 6G Through Multi-Domain Channel Extrapolation: Opportunities and Challenges of Generative Artificial Intelligence](https://arxiv.org/abs/2509.01125)
*Yuan Gao,Zichen Lu,Yifan Wu,Yanliang Jin,Shunqing Zhang,Xiaoli Chu,Shugong Xu,Cheng-Xiang Wang*

Main category: eess.SP

TL;DR: 提出基于Transformer的多域信道外推方法，通过去除位置编码和用MLP替代多头注意力机制，在6G网络中实现高精度、低复杂度的多域信道状态信息获取。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持高移动性通信和超大规模MIMO技术，当前研究主要关注单域信道外推，缺乏多域信道外推的全面解决方案。

Method: 提出新型Transformer编码器模型，去除位置编码模块，用多层感知机(MLP)替代原始的多头注意力机制，用于多域信道外推。

Result: 仿真结果表明该模型在推精度和推理速度方面优于现有基线模型，消融研究验证了模块设计的有效性。

Conclusion: 该方法为6G网络提供了有效的多域信道外推解决方案，但仍存在可解释性、泛化能力和数据集收集等开放性问题需要进一步研究。

Abstract: Channel extrapolation has attracted wide attention due to its potential to
acquire channel state information (CSI) with high accuracy and minimal
overhead. This is becoming increasingly crucial as the sixth-generation (6G)
mobile networks aim to support complex scenarios, for example, high-mobility
communications utilizing ultra-massive multiple-input multiple-output (MIMO)
technologies and broad spectrum bands, necessitating multi-domain channel
extrapolation. Current research predominantly addresses channel extrapolation
within a single domain, lacking a comprehensive approach to multi-domain
channel extrapolation. To bridge the gap, we propose the concept of
multi-domain channel extrapolation, detailing the essential performance
requirements for 6G networks. These include precise channel extrapolation,
adaptability to varying scenarios, and manageable computational complexity
during both training and inference stages. In light of these requirements, we
elaborate the potential and challenges of incorporating generative artificial
intelligence (GAI)-based models for effective multi-domain channel
extrapolation. Given the ability of the Transformer to capture long-range
dependencies and hidden patterns, we propose a novel Transformer encoder-like
model by eliminating the positional encoding module and replacing the original
multi-head attention with a multilayer perceptron (MLP) for multi-domain
channel extrapolation. Simulation results indicate that this model surpasses
existing baseline models in terms of extrapolation accuracy and inference
speed. Ablation studies further demonstrate the effectiveness of the module
design of the proposed design. Finally, we pose several open questions for the
development of practical GAI-based multi-domain channel extrapolation models,
including the issues of explainability, generalization, and dataset collection.

</details>


### [25] [A Model-Based Dictionary Approach for Magnetic Nanoparticle Signal Prediction](https://arxiv.org/abs/2509.01127)
*Asli Alpman,Mustafa Utkur,Emine Ulku Saritas*

Main category: eess.SP

TL;DR: 提出了一种基于模型字典的无校准迭代算法，用于预测磁性纳米粒子在不同实验条件下的信号响应，无需大量实验即可优化成像参数。


<details>
  <summary>Details</summary>
Motivation: 磁性粒子成像中纳米粒子的磁化响应受多种因素影响，传统方法需要大量实验来优化参数，研究旨在通过计算模型减少实验需求。

Method: 使用耦合Brown-Néel旋转模型构建信号字典，通过迭代算法联合估计字典权重和非模型动态的传递函数，验证了合成信号和实际MPS实验。

Result: 在SNR为1时仍能准确估计权重和传递函数，实际实验中预测信号与实测信号NRMSE低于3.5%，NWD值低于0.10，成功捕捉粘度依赖趋势。

Conclusion: 该算法能够准确预测未测试条件下的MNP信号，为磁性粒子成像的参数优化提供了有效的计算工具，减少了实验需求。

Abstract: Magnetic particle imaging (MPI) is a tracer-based medical imaging modality
that enables quantification and spatial mapping of magnetic nanoparticle (MNP)
distribution. The magnetization response of MNPs depends on experimental
conditions such as drive field (DF) settings and medium viscosity, as well as
on magnetic parameters of MNPs such as magnetic core diameter, hydrodynamic
diameter, and magnetic anisotropy constant. A comprehensive understanding of
the magnetization response of MNPs can facilitate the optimization of DF and
MNP type for a given MPI application, without the need for extensive
experimentation. In this work, we propose a calibration-free iterative
algorithm using model-based dictionaries for MNP signal prediction at untested
settings. Dictionaries were constructed with the MNP signals simulated using
the coupled Brown-N\'eel rotation model. Based on the available measurements,
the proposed algorithm jointly estimates the dictionary weights and the
transfer functions due to non-model-based dynamics. These dynamics include the
system response of the measurement setup as well as magnetization dynamics not
accounted for by the employed coupled Brown-N\'eel rotation model. The
algorithm was first validated on synthetic signals at SNR levels of 1 and 10,
and then tested on an in-house MPS setup across six viscosity levels
(0.89-15.33 mPa.s) and DF frequencies of 0.25-2 kHz using two commercial MNPs.
Validation on synthetic signals showed accurate weight and transfer function
estimation even at SNR 1. MPS experiments demonstrated successful prediction of
MNP signals at untested viscosities, with NRMSE below 1.51% and 3.5% for the
two tested MNPs across all DF settings. Predicted signals captured viscosity
dependent trends, and NWD values remained low (<0.10 and <0.07 for the two
tested MNPs), confirming robust weight estimation.

</details>


### [26] [Dynamic State Estimation of Power System Utilizing Cauchy Kernel-Based Maximum Mixture Correntropy UKF over Beluga Whale-Bat Optimization](https://arxiv.org/abs/2509.01163)
*Duc Viet Nguyen,Haiquan Zhao,Jinhui Hu*

Main category: eess.SP

TL;DR: 提出了一种基于柯西核最大混合相关熵准则的鲁棒UKF算法(BWB-CKMMC-UKF)，用于解决电力系统动态状态估计中的非高斯噪声、异常值和测量数据问题


<details>
  <summary>Details</summary>
Motivation: 电力系统动态状态估计中存在的非高斯噪声、异常值、负荷突变和不良测量数据会降低估计精度，且传统基于相关熵准则的UKF使用对带宽敏感的高斯核可能导致Cholesky分解奇异矩阵问题

Method: 使用两个柯西函数合并的核函数，通过统计线性化技术将测量误差和状态误差统一在代价函数中，采用定点迭代获得最优状态估计值，并利用Beluga Whale-Bat优化算法确定核函数形状系数和影响sigma点选择的尺度系数

Result: 在IEEE 14、30和57总线测试系统上的仿真结果验证了所提算法的性能

Conclusion: 基于柯西核最大混合相关熵准则的鲁棒UKF算法能够有效处理电力系统动态状态估计中的非高斯噪声和异常值问题，且对核带宽不敏感，具有显著的厚尾特性

Abstract: Non-Gaussian noise, outliers, sudden load changes, and bad measurement data
are key factors that diminish the accuracy of dynamic state estimation in power
systems. Additionally, unscented Kalman filters (UKF) based on correntropy
criteria utilize bandwidth-sensitive Gaussian kernels, which may lead to
singular matrices in the Cholesky decomposition. To overcome all the above
problems, in this paper, a robust UKF based on Cauchy kernel maximum mixture
correntropy (CKMMC) criteria over hybrid Beluga Whale-Bat (BWB) optimization
(BWB-CKMMC-UKF) is proposed, in which the kernel is merged of two Cauchy
functions. Specifically, the measurement error and state error are unified in
the cost function by the statistical linearization technique, and the optimal
value of state estimation is obtained by fixed-point iteration. Because of its
insensitive feature to kernel bandwidth and notable thick-tailed feature, the
Cauchy kernel function is utilized instead of the Gaussian kernel in the
optimization criteria. Additionally, to fit the power system model, the shape
coefficients of the kernel in the CKMMC criterion and scale coefficients that
influence the selection of sigma points in the unscented transform are
determined based on the BWB algorithm. Simulation results on IEEE 14, 30, and
57-bus test systems validated the performance of the proposed algorithm.

</details>


### [27] [Beyond Exhaustive Sampling: Efficient Rotational Matching via Ball Harmonics](https://arxiv.org/abs/2509.01180)
*Fabian Kruse,Vinith Kishore,Valentin Debarnot,Ivan Dokmanić*

Main category: eess.SP

TL;DR: 基于球谐函数展开的冷冻电子断层扫描亚断层图对齐框架，结合频率和梯度优化策略，避免穷举旋转采样，速度比现有方法快一个数量级


<details>
  <summary>Details</summary>
Motivation: 冷冻电子断层扫描技术产生大量原位生物样本数据，需要可扩展、快速且鲁棒的亚断层图对齐方法来处理低信噪比重建体积中的大分子结构

Method: 提出基于球谐函数展开的亚断层图对齐框架，结合频率域和梯度优化策略，避免传统的穷举旋转采样方法

Result: 该方法实现了比现有方法快一个数量级的对齐速度提升

Conclusion: 该框架为处理大规模冷冻电子断层扫描数据提供了高效的对齐解决方案，能够更好地应对成像技术进步带来的数据量挑战

Abstract: Cryo-ET allows to generate tomograms of biological samples in situ, capturing
complex structures in their native context. Despite low signal-to-noise ratio
in reconstructed volumes, the large number of copies of the same macromolecules
makes it possible to retrieve high-resolution maps by averaging many aligned
subtomograms. To keep up with technical advances in the imaging process and the
resulting huge amounts of data available, there is a need for scalable, fast
and robust procedures to align subtomograms. We propose a subtomogram alignment
framework based on the ball harmonics expansion that combines frequency- and
gradient-based optimization strategies to avoid exhaustive rotation sampling,
enabling a speed-up of an order of magnitude compared to current approaches.

</details>


### [28] [Enhanced Fingerprint-based Positioning With Practical Imperfections: Deep learning-based approaches](https://arxiv.org/abs/2509.01197)
*Shugong Xu,Jun Jiang,Wenjun Yu,Yilin Gao,Guangjin Pan,Shiyi Mu,Zhiqi Ai,Yuan Gao,Peigang Jiang,Cheng-Xiang Wang*

Main category: eess.SP

TL;DR: 本文提出了三种创新的蜂窝网络定位框架，通过半监督一致性学习、集成学习和解耦映射头算法，解决了实际环境中标签数据稀缺、动态无线环境和锚点分布不均等挑战，在定位精度上显著优于现有基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的定位算法需要大量标注数据，但在真实蜂窝环境中难以获取，且模型泛化能力不足。为了解决这些问题并推动蜂窝定位技术的发展，作者团队参加了2024年无线通信算法精英竞赛。

Method: 开发了三种创新定位框架：1）半监督一致性框架，通过生成高质量伪标签来扩充训练数据集；2）集成学习算法，融合不同训练策略模型的定位坐标以应对动态环境；3）解耦映射头算法，利用扇区旋转方案解决锚点分布不均问题。

Result: 仿真结果表明，所提出的定位算法在{90%、80%、67%、50%}分位数和平均距离误差方面均显著优于现有基准方法，在竞赛中包揽前三名奖项。

Conclusion: 提出的三种创新框架有效解决了实际蜂窝定位环境中的关键挑战，包括标签数据稀缺、动态环境和锚点分布不均等问题，为高精度蜂窝网络定位提供了有效的解决方案。

Abstract: High-precision positioning is vital for cellular networks to support
innovative applications such as extended reality, unmanned aerial vehicles
(UAVs), and industrial Internet of Things (IoT) systems. Existing positioning
algorithms using deep learning techniques require vast amounts of labeled data,
which are difficult to obtain in real-world cellular environments, and these
models often struggle to generalize effectively. To advance cellular
positioning techniques, the 2024 Wireless Communication Algorithm Elite
Competition as conducted, which provided a dataset from a three-sector outdoor
cellular system, incorporating practical challenges such as limited
labeled-dataset, dynamic wireless environments within the target and
unevenly-spaced anchors, Our team developed three innovative positioning
frameworks that swept the top three awards of this competition, namely the
semi-supervised framework with consistency, ensemble learning-based algorithm
and decoupled mapping heads-based algorithm. Specifically, the semi-supervised
framework with consistency effectively generates high-quality pseudo-labels,
enlarging the labeled-dataset for model training. The ensemble learning-based
algorithm amalgamates the positioning coordinates from models trained under
different strategies, effectively combating the dynamic positioning
environments. The decoupled mapping heads-based algorithm utilized sector
rotation scheme to resolve the uneven-spaced anchor issue. Simulation results
demonstrate the superior performance of our proposed positioning algorithms
compared to existing benchmarks in terms of the {90%, 80%, 67%, 50%} percentile
and mean distance error.

</details>


### [29] [Rigid Body Localization and Tracking for 6G V2X: Algorithms, Applications, and Road to Adoption](https://arxiv.org/abs/2509.01208)
*Niclas Führling,Hyeon Seok Rou,Giuseppe Thadeu Freitas de Abreu,David González G.,Gonzalo Seco-Granados,Osvaldo Gonsa*

Main category: eess.SP

TL;DR: 本文介绍了刚性体定位(RBL)技术在车联网感知中的概念、优势、应用、技术挑战和未来研究方向，重点讨论了RBL在B5G/6G无线系统中的潜力及其在标准化和工业领域的应用。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶需求的增长，车联网(V2X)感知技术受到广泛关注，特别是集成传感与通信(ISAC)框架的出现。刚性体定位(RBL)能够估计目标的位置、速度、3D几何结构和方向，具有独特优势。

Method: 文章通过概念介绍、优势分析、应用场景识别、技术挑战探讨和未来研究方向规划的方式，系统性地阐述了RBL技术。同时讨论了RBL在B5G/6G无线系统中的潜在应用。

Result: RBL被确立为车联网感知中一个有前景的范式，能够提供比传统定位技术更丰富的目标信息，包括3D几何结构和方向估计。

Conclusion: RBL技术在下一代无线系统和车联网感知中具有重要价值，需要在标准化、技术挑战解决和跨领域应用方面进行进一步研究和发展。

Abstract: Vehicle-to-everything (V2X) perception refers to a suite of technologies that
empower vehicles to sense their environment and communicate with other
entities, including surrounding vehicles, infrastructure, and cloud/edge
networks. With the growing demands of autonomous driving, V2X perception has
gained significant attention, particularly through the emergence of integrated
sensing and communication (ISAC) frameworks. Within this landscape, rigid body
localization (RBL) has emerged as a promising paradigm, enabling the estimation
of not only the position and velocity of the targets, but also its
three-dimensional (3D) geometric structure and orientation. This article
introduces the concept of RBL, highlights its unique advantages and
applications, identifies key technical challenges, and finally outlines future
research directions. In addition, the potential of RBL in next-generation -
e.g. beyond fifth generation (B5G) and sixth-generation (6G) - wireless systems
applied to V2X perception is also discussed, with a focus on its role in
standardization efforts and its relevance across automotive and industrial
domains.

</details>


### [30] [High-Density MIMO Localization Using a 32x64 Ultrasonic Transducer-Microphone Array with Real-Time Data Streaming](https://arxiv.org/abs/2509.01210)
*Rens Baeyens,Dennis Laurijssen,Jan Steckel,Walter Daems*

Main category: eess.SP

TL;DR: 提出了一种基于大规模MIMO架构的新型超声阵列系统，通过32个发射器和62个麦克风组成扩展虚拟孔径，提高信道分离能力和空间分辨率。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统超声定位系统中信道分离和空间分辨率受限的问题，需要开发能够提供更高精度定位的大规模MIMO超声系统。

Method: 采用随机相位多音信号在超声频段激励每个发射器，通过MIMO架构创建扩展虚拟孔径，减少信道间相关性并增强多径鲁棒性。

Result: 仿真实验显示MIMO处理相比单发射器配置能更好地分离反射体，但换能器带宽等实际限制会降低可实现的信道隔离度。

Conclusion: 大规模MIMO超声系统在提高定位精度方面具有潜力，但实际应用中需要考虑换能器带宽等硬件限制对性能的影响。

Abstract: In this work, we present a novel ultrasonic array system designed for
high-precision localization using a large-scale MIMO (Multiple-Input
Multiple-Output) architecture. The system combines 32 transmitters with 62
microphones, creating an extended virtual aperture that improves channel
separability and spatial resolution. Each transmitter is excited by a
random-phase multisine within the ultrasonic band, which reduces inter-channel
correlation and increases robustness against multipath. The feasibility of the
approach is demonstrated through simulations of reflector imaging and analysis
of channel separation under realistic transducer bandwidth constraints. Results
show that MIMO processing enables improved separation of reflectors compared to
single-emitter configurations, although practical limitations such as
transducer bandwidth reduce the achievable channel isolation.

</details>


### [31] [Rate Optimization for Downlink URLLC via Pinching Antenna Arrays](https://arxiv.org/abs/2509.01222)
*Tong Lin,Jianyue Zhu,Wei Huang,Meng Hua,Zhizhong Zhang*

Main category: eess.SP

TL;DR: 提出一种使用夹持天线的uRLLC下行系统，通过优化天线位置和相位对齐策略，在满足QoS和天线间距约束下最大化数据率


<details>
  <summary>Details</summary>
Motivation: 为紧凑型和延迟关键的未来应用设计超可靠低延迟通信系统，需要解决传统天线系统在数据率和紧凑性方面的不足

Method: 提出紧凑且成本效益高的天线架构，建立基于有限块长度的优化模型，推导天线最优放置的闭式解，并集成相位对齐策略实现相干信号叠加

Result: 仿真结果显示相比传统天线系统有显著速率提升，同时满足uRLLC要求

Conclusion: 所提出的设计非常适合紧凑型和延迟关键的未来应用，在满足服务质量约束的同时实现了数据率最大化

Abstract: This work studies an ultra-reliable and low-latency communications (uRLLC)
downlink system using pinching antennas which are realized by activating small
dielectric particles along a dielectric waveguide. Our goal is to maximize the
data rate by optimizing the positions of the pinching antennas. By proposing a
compact and cost-efficient antenna architecture and formulating a finite
blocklength-based optimization model, we derive a closed-form solution for the
optimal antenna placement under quality-of-service (QoS) and antenna spacing
constraints. Meanwhile, a phase-alignment strategy is integrated into the
design, enabling coherent signal superposition across the array. Simulation
results confirm significant rate improvements over conventional antenna systems
while satisfying uRLLC requirements, making the proposed design well-suited for
compact and latency-critical future applications.

</details>


### [32] [SMDS-based Rigid Body Localization](https://arxiv.org/abs/2509.01223)
*Niclas Führling,Giuseppe Abreu,David González G.,Osvaldo Gonsa*

Main category: eess.SP

TL;DR: 基于距离和传感器角度测量的刚体定位方法，通过改进的超维多维缩放算法，仅使用部分复杂边核信息来实现高性能定位。


<details>
  <summary>Details</summary>
Motivation: 解决仅依靠距离和传感器角度测量进行刚体定位的问题，提高定位精度和效率。

Method: 使用超维多维缩放(SMDS)算法的变种，仅利用可用的锚点到锚点和目标到目标信息构建部分复杂边核。

Result: 模拟结果显示该方法在估计的均方误差(MSE)方面表现良好，进一步与相应的克拉美罗下界(CRLB)进行了比较。

Conclusion: 提出的方法能够有效地利用限制信息进行刚体定位，并在性能上达到接近理论下界的水平。

Abstract: We consider a novel rigid body localization (RBL) method, based only on a set
of measurements of the distances, as well as the angles between sensors of the
vehicle to the anchor landmark points. A key point of the proposed method is to
use a variation of the super multidimensional scaling (SMDS) algorithm, where
only a minor part of the complex edge kernel is used, based on the available
information, which in the case of RBL is anchor-to-anchor and target-to-target
information. Simulation results illustrate the good performance of the proposed
technique in terms of mean square error (MSE) of the estimates, compared also
to the corresponding Cram\'er-Rao Lower Bound (CRLB).

</details>


### [33] [Comparison between Supervised and Unsupervised Learning in Deep Unfolded Sparse Signal Recovery](https://arxiv.org/abs/2509.01331)
*Koshi Nagahisa,Ryo Hayakawa,Youji Iiguni*

Main category: eess.SP

TL;DR: 本文研究了深度展开技术中损失函数选择对稀疏信号恢复算法的影响，发现损失函数的效果取决于优化问题的凸性：凸问题中监督学习效果更好但无法最小化原目标函数，非凸问题中两种方法都能收敛到更好的局部最小值。


<details>
  <summary>Details</summary>
Motivation: 深度展开技术将迭代优化算法转换为可训练的轻量级神经网络，但不同损失函数在不同应用场景下的效果尚未系统研究，需要探索损失函数选择对算法性能的影响。

Method: 研究深度展开版本的ISTA和IHT算法，比较使用均方误差的监督学习和使用原始优化问题目标函数的无监督学习，通过仿真实验分析不同损失函数在凸和非凸优化问题中的表现。

Result: 对于凸的ℓ1正则化问题，监督ISTA获得更好的恢复精度但无法最小化原目标函数，无监督ISTA收敛到与传统ISTA几乎相同的解但收敛更快；对于非凸的ℓ0正则化问题，两种IHT方法都能收敛到比原始IHT更好的局部最小值，性能相似。

Conclusion: 损失函数选择的效果严重依赖于优化问题的凸性，这一发现为设计有效的深度展开网络提供了重要指导，需要根据问题特性选择合适的损失函数。

Abstract: This paper investigates the impact of loss function selection in deep
unfolding techniques for sparse signal recovery algorithms. Deep unfolding
transforms iterative optimization algorithms into trainable lightweight neural
networks by unfolding their iterations as network layers, with various loss
functions employed for parameter learning depending on application contexts. We
focus on deep unfolded versions of the fundamental iterative shrinkage
thresholding algorithm (ISTA) and the iterative hard thresholding algorithm
(IHT), comparing supervised learning using mean squared error with unsupervised
learning using the objective function of the original optimization problem. Our
simulation results reveal that the effect of the choice of loss function
significantly depends on the convexity of the optimization problem. For convex
$\ell_1$-regularized problems, supervised-ISTA achieves better final recovery
accuracy but fails to minimize the original objective function, whereas we
empirically observe that unsupervised-ISTA converges to a nearly identical
solution as conventional ISTA but with accelerated convergence. Conversely, for
nonconvex $\ell_0$-regularized problems, both supervised-IHT and
unsupervised-IHT converge to better local minima than the original IHT, showing
similar performance regardless of the loss function employed. These findings
provide valuable insights into the design of effective deep unfolded networks
for sparse signal recovery applications.

</details>


### [34] [A James-Stein Estimator based Generalized OMP Algorithm for Robust Signal Recovery using Sparse Representation](https://arxiv.org/abs/2509.01410)
*Debraj Banerjee,Amitava Chatterjee*

Main category: eess.SP

TL;DR: JS-gOMP算法通过整合James-Stein估计器，提升了广义正交匹配追踪(gOMP)在噪声环境下的鲁棒性，在稀疏信号处理中实现了更好的信号恢复和噪声抑制平衡。


<details>
  <summary>Details</summary>
Motivation: 解决字典中存在噪声这一稀疏表示场景中的常见挑战，提升算法在噪声环境下的性能表现。

Method: 在传统gOMP算法基础上，创新性地引入James-Stein估计器来优化信号恢复与噪声抑制之间的权衡关系。

Result: 对比分析表明JS-gOMP在噪声环境下显著优于传统gOMP算法，特别是在噪声存在显著的情况下表现更佳。

Conclusion: JS-gOMP为信号和图像处理应用提供了更有效的解决方案，特别适用于噪声环境下的稀疏信号处理任务。

Abstract: In this paper, we introduce a novel algorithm named JS-gOMP, which enhances
the generalized Orthogonal Matching Pursuit (gOMP) algorithm for improved noise
robustness in sparse signal processing. The JS-gOMP algorithm uniquely
incorporates the James-Stein estimator, optimizing the trade-off between signal
recovery and noise suppression. This modification addresses the challenges
posed by noise in the dictionary, a common issue in sparse representation
scenarios. Comparative analyses demonstrate that JS-gOMP outperforms
traditional gOMP, especially in noisy environments, offering a more effective
solution for signal and image processing applications where noise presence is
significant.

</details>


### [35] [To Share, or Not to Share: A Study on GEO-LEO Systems for IoT Services with Random Access](https://arxiv.org/abs/2509.01506)
*Marcel Grec,Federico Clazzer,Israel Leyva-Mayorga,Andrea Munari,Gianluigi Liva,Petar Popovski*

Main category: eess.SP

TL;DR: 两个卫星运营商在IoT服务中的谱段共享分析，揭示在特定条件下谱段共享可以带来显著的速率提升


<details>
  <summary>Details</summary>
Motivation: 解决卫星部署增加导致的无线资源稀缺问题，探索IoT设备爆发式流量下的谱段分配最优策略

Method: 建立两个运营商覆盖重叠区域的通信模型，使用分析近似和蒙特卡罗模拟验证

Result: 谱段共享在某些条件下可为双方带来显著的速率增益，但效果受用户数量和编码率等参数影响，并非总是互利

Conclusion: 该模型揭示了上行链路谱段共享的基本批执，为未来6G非地面网络的设计和规制提供了可操作的见解

Abstract: The increasing number of satellite deployments, both in the low and
geostationary Earth orbit exacerbates the already ongoing scarcity of wireless
resources when targeting ubiquitous connectivity. For the aim of supporting a
massive number of IoT devices characterized by bursty traffic and modern
variants of random access, we pose the following question: Should competing
satellite operators share spectrum resources or is an exclusive allocation
preferable? This question is addressed by devising a communication model for
two operators which serve overlapping coverage areas with independent IoT
services. Analytical approximations, validated by Monte Carlo simulations,
reveal that spectrum sharing can yield significant throughput gains for both
operators under certain conditions tied to the relative serviced user
populations and coding rates in use. These gains are sensitive also to the
system parameters and may not always render the spectral coexistence mutually
advantageous. Our model captures basic trade-offs in uplink spectrum sharing
and provides novel actionable insights for the design and regulation of future
6G non-terrestrial networks.

</details>


### [36] [Non-Identical Diffusion Models in MIMO-OFDM Channel Generation](https://arxiv.org/abs/2509.01641)
*Yuzhi Yang,Omar Alhussein,Mérouane Debbah*

Main category: eess.SP

TL;DR: 提出非相同扩散模型，通过元素级时间指示器改进无线OFDM信道生成，特别针对MIMO-OFDM信道矩阵中元素可靠性不均的问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用标量时间索引表示全局噪声水平，无法准确捕捉无线信道中不同子载波元素的局部误差变化，特别是在导频方案导致初始信道估计可靠性高度不均的情况下。

Method: 引入与输入尺寸匹配的矩阵来控制元素级噪声进程，提出维度级时间嵌入策略，保持类似现有方法的扩散过程但改进噪声建模。

Result: 理论和数值实验证明了非相同扩散方案的正确性和有效性，在MIMO-OFDM信道生成中表现出改进的生成结果。

Conclusion: 非相同扩散模型能够更好地表征噪声输入中每个元素的可靠性，特别是在初始化存在偏差的情况下，为无线信道生成提供了更准确的建模方法。

Abstract: We propose a novel diffusion model, termed the non-identical diffusion model,
and investigate its application to wireless orthogonal frequency division
multiplexing (OFDM) channel generation. Unlike the standard diffusion model
that uses a scalar-valued time index to represent the global noise level, we
extend this notion to an element-wise time indicator to capture local error
variations more accurately. Non-identical diffusion enables us to characterize
the reliability of each element (e.g., subcarriers in OFDM) within the noisy
input, leading to improved generation results when the initialization is
biased. Specifically, we focus on the recovery of wireless multi-input
multi-output (MIMO) OFDM channel matrices, where the initial channel estimates
exhibit highly uneven reliability across elements due to the pilot scheme.
Conventional time embeddings, which assume uniform noise progression, fail to
capture such variability across pilot schemes and noise levels. We introduce a
matrix that matches the input size to control element-wise noise progression.
Following a similar diffusion procedure to existing methods, we show the
correctness and effectiveness of the proposed non-identical diffusion scheme
both theoretically and numerically. For MIMO-OFDM channel generation, we
propose a dimension-wise time embedding strategy. We also develop and evaluate
multiple training and generation methods and compare them through numerical
experiments.

</details>


### [37] [Predictive Communications for Low-Altitude Networks](https://arxiv.org/abs/2509.01705)
*Junting Chen,Bowen Li,Hao Sun,Shuguang Cui,Nikolaos Pappas*

Main category: eess.SP

TL;DR: 提出预测性通信新范式，通过融合可预测任务轨迹和无线电环境模型，实现从被动适应到主动优化的网络管理转变，显著降低跨层干扰


<details>
  <summary>Details</summary>
Motivation: 传统反应式通信范式无法应对低空经济密集任务驱动网络中的极端信道动态和严重跨层干扰问题，需要利用网络固有可预测性

Method: 分层框架将预测性跨层资源分配分解为战略（路由）、战术（时序）、操作（功率）三层，使决策时间尺度与预测信息精度和范围相匹配

Result: 该前瞻驱动框架实现了跨层干扰数量级的降低

Conclusion: 为构建稳健可扩展的低空通信系统奠定了基础

Abstract: The emergence of dense, mission-driven aerial networks supporting the
low-altitude economy presents unique communication challenges, including
extreme channel dynamics and severe cross-tier interference. Traditional
reactive communication paradigms are ill-suited to these environments, as they
fail to leverage the network's inherent predictability. This paper introduces
predictive communication, a novel paradigm transforming network management from
reactive adaptation to proactive optimization. The approach is enabled by
fusing predictable mission trajectories with stable, large-scale radio
environment models (e.g., radio maps). Specifically, we present a hierarchical
framework that decomposes the predictive cross-layer resource allocation
problem into three layers: strategic (routing), tactical (timing), and
operational (power). This structure aligns decision-making timescales with the
accuracy levels and ranges of available predictive information. We demonstrate
that this foresight-driven framework achieves an order-of-magnitude reduction
in cross-tier interference, laying the groundwork for robust and scalable
low-altitude communication systems.

</details>


### [38] [Leveraging Orbital Dynamics with RF Signal Features for Satellite Multi-Orbit Proximity Threat Detection](https://arxiv.org/abs/2509.01802)
*Anouar Boumeftah,Gunes Karabulut Kurt*

Main category: eess.SP

TL;DR: 一种融合轨道机动模拟与RF信号分析的混合模拟框架，通过集成动态特征与RF指标，使用随机森林分类器达到94.67%的识别准确率，特别擅长检测偶发性干扰和监视伪装伪装行为


<details>
  <summary>Details</summary>
Motivation: 占位低轨道平面的密集多轨道卡尔曲线和敏捷的对抗机动导致了贴近干扰的增长风险，对卫星通信构成威胁

Method: 使用MIT林肯实验室的MaDDG库生成标签数据集，融合动态特征（距离、速度、加速度、TCA）和RF指标（RSSI、通信量、JSR），加入时间导数和滚动窗口统计以捕捉细微或瞬态干扰模式

Result: 随机森林分类器在融合特征集上达到94.67%准确率和0.9471宏F1分数，显著超过仅使用动态或RF输入的模型，在检测监视和间歇性干扰等隐藏威胁方面特别有效

Conclusion: 该混合方法为卫星通信安全提供了更加精准的贴近操作检测能力，能够有效识别伪装伪装和偶发性干扰行为，对于应对密集卫星组网环境下的安全挑战具有重要价值

Abstract: Proximity-based interference is a growing threat to satellite communications,
driven by dense multi-orbit constellations and increasingly agile adversarial
maneuvers. We propose a hybrid simulation framework that integrates orbital
maneuver modeling with RF signal degradation analysis to detect and classify
suspicious proximity operations. Using the open-source Maneuver Detection Data
Generation (MaDDG) library from MIT Lincoln Laboratory, we generate labeled
datasets combining impulsive maneuver profiles with radio-frequency (RF)
impacts across a range of behavioral intents: routine station-keeping, covert
shadowing, and overt jamming. Our approach fuses kinematic features such as
range, velocity, acceleration, and Time of Closest Approach (TCA), with RF
metrics including Received Signal Strength Indicator (RSSI), throughput, and
Jammer-to-Signal Ratio (JSR). These features are further enhanced with temporal
derivatives and rolling-window statistics to capture subtle or transient
interference patterns. A Random Forest classifier trained on this fused feature
set achieves 94.67% accuracy and a macro F1 score of 0.9471, outperforming
models using only kinematic or RF inputs. The system is particularly effective
in detecting covert threats, such as surveillance or intermittent jamming, that
evade RF-only methods.

</details>


### [39] [Efficient River Water Level Sensing Using Cellular CSI and Joint Space-Time Processing](https://arxiv.org/abs/2509.01905)
*Khawaja Fahad Masood,Kai Wu,Zhongqin Wang,J. Andrew Zhang,Shu-Lin Chen,Y. Jay Guo*

Main category: eess.SP

TL;DR: 基于细胞信号的无传感器水位监测方法，利用现有通信基础设施通过分析信道状态信息来估算水位变化


<details>
  <summary>Details</summary>
Motivation: 传统水位传感器部署成本高、维护困难且在洪水期易受损坏，需要一种成本效益更高的解决方案

Method: 通过捕获下行细胞信号的信道状态信息(CSI)，估算水面反射信号路径长度变化，开发空间-时间处理框架进行到达角和多普勒移频聚焦，并采用材料基于材料的时间变化随机相位偏移补偿技术

Result: 河流现场实验表明，该方法能够准确可靠地估算水位，在不同接收器配置和部署情况下平均准确度范围为1.5厘米到3.05厘米

Conclusion: 该研究提出了一种利用现有细胞通信基础设施的新题水位监测方法，具有部署成本低、可靠性高的优势，为洪水预防和环境管理提供了有效的技术支撑

Abstract: Accurate and timely water level monitoring is critical for flood prevention,
environmental management, and emerging smart infrastructure systems.
Traditional water sensing methods often rely on dedicated sensors, which can be
costly to deploy and difficult to maintain and are vulnerable to damage during
floods.In this work, we propose a novel cellular signalbased sensing scheme
that passively estimates water level changes using downlink mobile signals from
existing communication infrastructure. By capturing subtle variations in
channel state information (CSI), the proposed method estimates the length
changes of the water-reflected signal path, which correspond to water level
variations. A space-time processing framework is developed to jointly estimate
the angle of arrival and Doppler shift, enabling isolation and enhancement of
the water-reflected path via beamforming, while effectively suppressing
environmental noise. The phase evolution of the beamformed signal is then
extracted to infer water level changes. To address clock asynchronism between
the transmitter and receiver inherent in bistatic systems, we introduce a
beamforming-based compensation technique for removing time-varying random phase
offsets in CSI. Field experiments conducted across a river demonstrate that the
proposed method enables accurate and reliable water level estimation, achieving
a mean accuracy ranging from 1.5 cm to 3.05 cm across different receiver
configurations and deployments.

</details>


### [40] [ECG-Based Stress Prediction with Power Spectral Density Features and Classification Models](https://arxiv.org/abs/2509.01923)
*Md. Mohibbul Haque Chowdhury,Nafisa Anjum,Md. Rokonuzzaman Mim*

Main category: eess.SP

TL;DR: 基于ECG信号的压力预测框架，结合频域特征提取和机器学习/深度学习算法，LSTM模型达到94%准确率


<details>
  <summary>Details</summary>
Motivation: 压力已成为全球关键健康问题，导致心血管疾病和抑郁症等长期疾病，需要准确可靠的压力监测系统

Method: 使用ECG信号，通过功率谱密度分析获取频域特征，应用决策树、随机森林、XGBoost、LightGBM、CatBoost等机器学习模型，以及CNN和LSTM深度学习模型

Result: 集成分类器效果显著，CatBoost达到90%准确率，LSTM模型表现最佳，达到94%准确率，且精确率、召回率和F1分数均衡

Conclusion: 频域特征提取与先进学习算法结合能有效提升压力预测性能，为实时医疗监测解决方案铺平道路

Abstract: Stress has emerged as a critical global health issue, contributing to
cardiovascular disorders, depression, and several other long-term illnesses.
Consequently, accurate and reliable stress monitoring systems are of growing
importance. In this work, we propose a stress prediction framework based on
electrocardiogram (ECG) signals recorded during multiple daily activities such
as sitting, walking, and jogging. Frequency-domain indicators of autonomic
nervous system activity were obtained through Power Spectral Density (PSD)
analysis and utilized as input for machine learning models including Decision
Tree, Random Forest, XGBoost, LightGBM, and CatBoost. In addition, deep
learning approaches, namely Convolutional Neural Networks (CNN) and Long
Short-Term Memory (LSTM) networks, were directly applied to the raw ECG
signals. Our experiments highlight the effectiveness of ensemble-based
classifiers, with CatBoost achieving 90% accuracy. Moreover, the LSTM model
provided superior results, attaining 94% accuracy with balanced precision,
recall, and F1-score, reflecting its strength in modeling temporal dependencies
in ECG data. Overall, the findings suggest that integrating frequency-domain
feature extraction with advanced learning algorithms enhances stress prediction
and paves the way for real-time healthcare monitoring solutions.

</details>


### [41] [On Performance of IoT Networks with Coordinated NOMA Transmission: Covert Monitoring and Information Decoding](https://arxiv.org/abs/2509.01935)
*Thai-Hoc Vu,Anh-Tu Le,Ngo Hoang Tu,Tan N. Nguyen,Miroslav Voznak*

Main category: eess.SP

TL;DR: 该论文研究Rayleigh衰落环境下IoT网络的隐蔽性和安全性性能，提出基于NOMA的协调传输策略，推导检测错误概率和保密中断概率的闭式表达式，并设计自适应功率分配方案来最大化隐蔽速率或保密速率。


<details>
  <summary>Details</summary>
Motivation: 随着IoT网络的快速发展，确保通信的隐蔽性和安全性变得至关重要。传统方法在频谱利用和抗监测方面存在不足，需要研究新的传输策略来同时应对warden的监测和eavesdropper的窃听。

Method: 采用协调直接和中继传输策略结合非正交多址接入(NOMA)，推导检测错误概率(DEP)的闭式表达式，确定最优判决阈值，设计自适应功率分配方案来满足隐蔽性要求和最大化传输速率。

Result: 数值结果验证了分析框架的准确性，提出的优化策略能有效调整功率分配系数，在满足服务质量要求和SIC条件的同时，最大化隐蔽速率或保密速率。

Conclusion: 该研究为IoT网络在Rayleigh衰落环境下的安全隐蔽通信提供了有效的分析框架和优化方案，NOMA与协调传输的结合显著提升了系统的隐蔽性和安全性性能。

Abstract: This work investigates the covertness and security performance of
Internet-of-Things (IoTs) networks under Rayleigh fading environments.
Specifically, a cellular source transmits covert information to cell-edge users
with the assistance of an IoT master node, employing a coordinated direct and
relay transmission strategy combined with non-orthogonal multiple access
(NOMA). This approach not only enhances spectrum utilization but also generates
friendly interference to complicate a warden's surveillance or an
eavesdropper's decoding efforts. From a covertness perspective, we derive exact
closed-form expressions for the detection error probability (DEP) under
arbitrary judgment thresholds. We then identify the optimal judgment threshold
for the worst-case scenario, at which the warden minimizes its DEP performance.
Accordingly, we determine the effective region for user power allocation (PA)
in NOMA transmission that satisfies the DEP constraint. From a security
perspective, we derive analytical expressions for the secrecy outage
probability under two eavesdropping strategies using selection combining and
maximal ratio combining. Based on this analysis, we propose an adaptive PA
scheme that maximizes covert rate while ensuring the quality-of-service (QoS)
requirements of legitimate users, the system's minimum covertness requirements,
and supporting successive interference cancellation (SIC) procedures.
Furthermore, we design an adaptive PA scheme that maximizes the secrecy rate
while ensuring the QoS requirements of legitimate users and SIC conditions.
Numerical results demonstrate the accuracy of the analytical framework, while
the proposed optimization strategies effectively adjust PA coefficients to
maximize either the covert rate or the secrecy rate.

</details>


### [42] [Correlation Analysis Between MF R-Mode Temporal ASF and Meteorological Factors](https://arxiv.org/abs/2509.01958)
*Jongmin Park,Junwoo Song,Taewon Kang,Jaewon Yu,Pyo-Woong Son*

Main category: eess.SP

TL;DR: 分析气象因素与MF R-Mode导航系统中时间ASF（附加二次因子）的相关性，探索基于气象数据预测时间ASF的可行性


<details>
  <summary>Details</summary>
Motivation: 随着GNSS系统脆弱性日益凸显，需要有效的备份导航系统。MF R-Mode作为备选方案，但需要校正受地形影响的ASF传播延迟，而传统参考站方法在远离参考站时效果下降

Method: 分析时间ASF与气象因素之间的相关性，重点关注温度、湿度等气象参数与时间ASF的关联性

Result: 温度和湿度与时间ASF显示出显著的相关性，表明这些气象因素在ASF校正中具有潜在应用价值

Conclusion: 基于气象因素预测时间ASF是可行的，这为改进MF R-Mode导航系统的精度提供了新的校正方法，特别是在远离参考站的区域

Abstract: As the vulnerabilities of global navigation satellite systems (GNSS) have
become more widely recognized, the need for complementary navigation systems
has grown. Medium frequency ranging mode (MF R-Mode) has gained attention as an
effective backup system during GNSS outages, owing to its strong signal
strength and cost-effective scalability. However, to achieve accurate
positioning, MF R-Mode requires correction for the additional secondary factor
(ASF), a propagation delay affected by terrain. The temporal variation of ASF,
known as temporal ASF, is typically corrected using reference stations;
however, the effectiveness of this method decreases with distance from the
reference station. In this study, we analyzed the correlation between temporal
ASF and meteorological factors to evaluate the feasibility of predicting
temporal ASF based on meteorological factors. Among these factors, temperature
and humidity showed significant correlations with temporal ASF, suggesting
their potential utility in ASF correction.

</details>


### [43] [Dual Target-Mounted RISs-Assisted ISAC Against Eavesdropping and Malicious Interference](https://arxiv.org/abs/2509.02030)
*Zehra Yigit,Sefa Kayraklik,Ertugrul Basar,Ali Gorcin*

Main category: eess.SP

TL;DR: 基站通过双RIS辅助ISAC系统，同时进行两个UAV目标感知和用户通信，应对恶意UAV的监听和干扰攻击，通过优化发射波束形和相位移系数最大化密码速率。


<details>
  <summary>Details</summary>
Motivation: 解决ISAC与RIS融合带来的新安全挑战，特别是恶意UAV目标的监听和干扰攻击问题。

Method: 提出双目标搭载RIS的ISAC方案，通过非凸优化问题形式化和SDR基于两阶段解决方法，优化基站发射波束形矩阵和合法RIS的相位移系数。

Result: 通过泛活计算机模拟评估了系统在不同配置下的稳健性，使用密码速率评估通信性能，信干正比和CRB评估感知性能。

Conclusion: 所提方案能够有效应对双重安全威胁，提高用户通信的密码速率和对恶意UAV目标的感知准确性。

Abstract: The synergy between integrated sensing and communication (ISAC) and
reconfigurable intelligent surfaces (RISs) unlocks novel applications and
advanced services for next-generation wireless networks, yet also introduces
new security challenges. In this study, a novel dual target-mounted
RISs-assisted ISAC scheme is proposed, where a base station with ISAC
capability performs sensing of two unmanned aerial vehicle (UAV) targets, one
of which is legitimate and the other is eavesdropper, while communicating with
the users through an RIS mounted on the legitimate UAV target. The proposed
scheme addresses dual security threats posed by a hostile UAV target:
eavesdropping on legitimate user communications and random interference attacks
launched by a malicious RIS mounted on this eavesdropper UAV target, aiming to
disrupt secure transmissions. A non-convex optimization problem maximizing the
secrecy rate of the users is formulated, and a semi-definite relaxation
(SDR)-based two-stage solution is developed to optimize the transmit
beamforming matrix of the base station and the phase shift coefficients of the
legitimate RIS. Extensive computer simulations are conducted to evaluate the
robustness of the proposed solution under various system configurations. The
proposed system's communication performance is assessed using the secrecy rate
metric, while the sensing performance is evaluated through the
signal-to-interference-plus-noise ratio and the Cramer-Rao bound (CRB) for
angle-of-departure (AoD) estimation of the eavesdropper UAV target.

</details>


### [44] [Synesthesia of Machines (SoM)-Based Task-Driven MIMO System for Image Transmission](https://arxiv.org/abs/2509.02031)
*Sijiang Li,Rongqing Zhang,Xiang Cheng,Jian Tang*

Main category: eess.SP

TL;DR: 该论文提出了一种基于机器联觉(SoM)的任务驱动MIMO系统SoM-MIMO，用于网络移动代理在动态场景中的图像传输，相比现有JSCC方案在保持相同通信开销的情况下显著提升了目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的联合源信道编码(JSCC)与MIMO技术结合的方法仍局限于离散时间模拟传输模型和简单任务，在支持网络移动代理复杂协同感知任务方面性能有限。

Method: 利用特征金字塔的结构特性和闭环MIMO通信系统的信道特性，开发SoM-MIMO系统实现高效鲁棒的数字MIMO图像传输。

Result: 实验结果显示，相比两种JSCC基线方案，该方法在所有SNR水平下平均mAP分别提升了6.30和10.48。

Conclusion: SoM-MIMO系统能够为网络移动代理的协同感知提供更高效和鲁棒的图像传输解决方案。

Abstract: To support cooperative perception (CP) of networked mobile agents in dynamic
scenarios, the efficient and robust transmission of sensory data is a critical
challenge. Deep learning-based joint source-channel coding (JSCC) has
demonstrated promising results for image transmission under adverse channel
conditions, outperforming traditional rule-based codecs. While recent works
have explored to combine JSCC with the widely adopted multiple-input
multiple-output (MIMO) technology, these approaches are still limited to the
discrete-time analog transmission (DTAT) model and simple tasks. Given the
limited performance of existing MIMO JSCC schemes in supporting complex CP
tasks for networked mobile agents with digital MIMO communication systems, this
paper presents a Synesthesia of Machines (SoM)-based task-driven MIMO system
for image transmission, referred to as SoM-MIMO. By leveraging the structural
properties of the feature pyramid for perceptual tasks and the channel
properties of the closed-loop MIMO communication system, SoM-MIMO enables
efficient and robust digital MIMO transmission of images. Experimental results
have shown that compared with two JSCC baseline schemes, our approach achieves
average mAP improvements of 6.30 and 10.48 across all SNR levels, while
maintaining identical communication overhead.

</details>


### [45] [Environment-Aware Channel Measurement and Modeling for Terahertz Monostatic Sensing](https://arxiv.org/abs/2509.02088)
*Yejian Lyu,Zhiqiang Yuan,Henk Wymeersch,Chong Han*

Main category: eess.SP

TL;DR: 本文提出了一种基于300GHz室内测量的环境感知太赫兹ISAC信道建模框架，通过SAGE算法提取多径参数，利用CCL聚类方法，建立了物理场景属性与信道特性的映射关系。


<details>
  <summary>Details</summary>
Motivation: 太赫兹频段的集成传感与通信(ISAC)系统需要真实可解释的信道建模来充分发挥其高速连接和环境感知的潜力，但现有研究缺乏对物理场景属性与信道特性之间映射关系的系统分析。

Method: 在三个代表性室内场景的57个收发同置位置进行测量，使用高分辨率SAGE算法提取多径参数，采用基于图像处理的CCL聚类方法对多径分量进行分组，建立环境感知信道建模框架。

Result: 实验结果表明，该方法能够从观测到的信道特性中可靠地提取物理特征（如结构和材料信息），成功建立了物理场景属性与信道域表现之间的映射关系。

Conclusion: 提出的环境感知信道建模框架为先进的太赫兹ISAC信道建模提供了有前景的基础，能够同时处理镜面反射和漫反射，有效表征反射行为。

Abstract: Integrated sensing and communication (ISAC) at terahertz (THz) frequencies
holds significant promise for unifying ultra-high-speed wireless connectivity
with fine-grained environmental awareness. Realistic and interpretable channel
modeling is essential to fully realize the potential of such systems. This work
presents a comprehensive investigation of monostatic sensing channels at
300~GHz, based on an extensive measurement campaign conducted at 57 co-located
transceiver (TRx) positions across three representative indoor scenarios.
Multipath component (MPC) parameters, including amplitude, delay, and angle,
are extracted using a high-resolution space-alternating generalized
expectation-maximization (SAGE) algorithm. To cluster the extracted MPCs, an
image-processing-based clustering method, i.e., connected component labeling
(CCL), is applied to group MPCs based on delay-angle consistency. Based on the
measurement data, an environment-aware channel modeling framework is proposed
to establish mappings between physical scenario attributes (e.g., reflector
geometry, surface materials, and roughness) and their corresponding
channel-domain manifestations. The framework incorporates both specular and
diffuse reflections and leverages several channel parameters, e.g., reflection
loss, Lambertian scattering, and intra-cluster dispersion models, to
characterize reflection behavior. Experimental results demonstrate that the
proposed approach can reliably extract physical characteristics, e.g.,
structural and material information, from the observed channel characteristics,
offering a promising foundation for advanced THz ISAC channel modeling.

</details>


### [46] [Affine-Doppler Division Multiplexing for High-Mobility Wireless Communications Systems](https://arxiv.org/abs/2509.02116)
*Yuanfang Ma,Zulin Wang,Peng Yuan,Qin Huang,Yuanhan Ni*

Main category: eess.SP

TL;DR: ADDM是一种新的正交多载波波形，为OTFS和AFDM提供统一框架，在高速移动场景中性能优于OTFS，与AFDM相当。


<details>
  <summary>Details</summary>
Motivation: 由于AFDM和OTFS波形不兼容，针对OTFS设计的先进方法无法直接应用于AFDM，需要一种通用框架来统一这两种波形。

Method: 提出Affine-Doppler Division Multiplexing (ADDM)，基于二维变换在Affine-Doppler域调制信息符号，具有优异的无模糊多普勒和多普勒分辨率。

Result: 数值结果表明ADDM在高移动性场景中与AFDM性能相当，但优于OTFS。

Conclusion: ADDM提供了一个通用框架，可以统一OTFS和AFDM，并可直接应用为这两种波形设计的先进方法。

Abstract: Affine Frequency Division Multiplexing (AFDM) has been regarded as a
candidate integrated sensing and communications (ISAC) waveform owing to its
superior communication performance, outperforming the Orthogonal Time-Frequency
Space (OTFS) that has been researched for a longer time. However, since the
above two waveforms are incompatible with each other, the state-of-the-art
methods well-designed for OTFS may not be directly applicable to AFDM. This
paper introduces a new orthogonal multicarrier waveform, namely Affine-Doppler
Division Multiplexing (ADDM), which can provide a generic framework and subsume
the existing OTFS and AFDM as a particular case. ADDM modulating information
symbols in the Affine-Doppler (A-D) domain based on a two-dimensional (2D)
transform can enjoy both excellent unambiguous Doppler and Doppler resolution,
which is the same as AFDM but outperforms OTFS. Moreover, benefiting from the
2D transform, the symbols block of ADDM in the A-D domain undergoes a 2D cyclic
shift produced by the delay and the Doppler of the channel, similar to the 2D
cyclic shift in the delay-Doppler domain of cyclic prefix (CP)-OTFS. This
offers a potential to directly apply the state-of-the-art methods well-designed
for OTFS and AFDM to ADDM. Numerical results show that ADDM achieves comparable
BER performance with AFDM but outperforms OTFS in high-mobility scenarios.

</details>


### [47] [High-Resolution Sensing in Communication-Centric ISAC: Deep Learning and Parametric Methods](https://arxiv.org/abs/2509.02137)
*Salmane Naoumi,Ahmad Bazzi,Roberto Bomfin,Marwa Chafii*

Main category: eess.SP

TL;DR: 提出了两种新算法IFFT-C2VNN和PARAMING，用于双基地ISAC系统中的超分辨率传感参数估计，利用通信参考符号的CSI信息，显著降低计算复杂度并提高参数估计精度。


<details>
  <summary>Details</summary>
Motivation: 解决通信中心ISAC系统中双基地配置下的超分辨率传感参数估计挑战，利用现有通信基础设施实现传感功能。

Method: IFFT-C2VNN使用复值卷积神经网络估计目标参数，PARAMING采用参数化方法利用系统模型知识（包括收发阵列几何结构）提取传感参数。

Result: 两种算法在不同信噪比下都表现出有效性和鲁棒性，计算复杂度相比传统方法显著降低。

Conclusion: 所提算法在现实ISAC场景中具有良好适用性，为双基地传感参数估计提供了高效解决方案。

Abstract: This paper introduces two novel algorithms designed to address the challenge
of super-resolution sensing parameter estimation in bistatic configurations
within communication-centric integrated sensing and communication (ISAC)
systems. Our approach leverages the estimated channel state information derived
from reference symbols originally intended for communication to achieve
super-resolution sensing parameter estimation. The first algorithm, IFFT-C2VNN,
employs complex-valued convolutional neural networks to estimate the parameters
of different targets, achieving significant reductions in computational
complexity compared to traditional methods. The second algorithm, PARAMING,
utilizes a parametric method that capitalizes on the knowledge of the system
model, including the transmit and receive array geometries, to extract the
sensing parameters accurately. Through a comprehensive performance analysis, we
demonstrate the effectiveness and robustness of both algorithms across a range
of signal-to-noise ratios, underscoring their applicability in realistic ISAC
scenarios.

</details>


### [48] [Beamforming Design for Pinching Antenna Systems with Multiple Receive Antennas](https://arxiv.org/abs/2509.02166)
*Enzhi Zhou,Yue Xiao,Ziyue Liu,Sotiris A. Tegos,Panagiotis D. Diamantoulakis,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 提出了一种针对多天线用户的下行链路pinching天线系统建模框架，通过两层放置策略优化天线位置，在NLoS环境下显著提升性能


<details>
  <summary>Details</summary>
Motivation: 下一代网络需要智能鲁棒的信道条件来支持超高数据速率和动态环境中的大规模设备部署，现有柔性天线技术重构范围有限且结构刚性，难以有效恢复视距链路

Method: 首先推导接收信噪比与pinching天线位置的解析关系，然后提出两层放置策略：基于大尺度信道特性优化中心辐射点，使用启发式压缩放置算法近似多接收天线的相位对齐并选择空间紧凑的激活元件集

Result: 仿真结果显示相比传统单天线方案有显著性能提升，特别是在短距离场景、密集pinching天线和用户天线间距较大的情况下

Conclusion: pinching天线系统作为补充解决方案，在具有挑战性的传播环境特别是非视距条件下提供了增强的灵活性，所提出的建模框架和放置策略有效解决了现有文献中未充分探索的实际场景

Abstract: Next-generation networks require intelligent and robust channel conditions to
support ultra-high data rates, seamless connectivity, and large-scale device
deployments in dynamic environments. While flexible antenna technologies such
as fluid and movable antennas offer some degree of adaptability, their limited
reconfiguration range and structural rigidity reduce their effectiveness in
restoring line-of-sight (LoS) links. As a complementary solution, pinching
antenna systems (PASs) enable fine-grained, hardware-free control of radiation
locations along a waveguide, offering enhanced flexibility in challenging
propagation environments, especially under non-LoS (NLoS) conditions. This
paper introduces a general and novel modeling framework for downlink PASs
targeting users equipped with multiple receive antennas, addressing a practical
yet underexplored scenario in the existing literature. Specifically, we first
derive an analytical relationship between the received signal-to-noise ratio
and the pinching antenna (PA) positions, and based on this, we propose a
two-layer placement strategy. First, we optimize the central radiation point
using large-scale channel characteristics, and then we use a heuristic
compressed placement algorithm to approximate phase alignment across multiple
receive antennas and select a spatially compact set of active elements.
Simulation results demonstrate notable performance gains over conventional
single-antenna schemes, particularly in short-range scenarios with dense PAs
and widely spaced user antennas.

</details>


### [49] [Dual-end Fluid Antennas For Robust Anti-jamming in Low-altitude Air-ground Communications](https://arxiv.org/abs/2509.02260)
*Yifan Guo,Junshan Luo,Fanggang Wang,Haiyang Ding,Shilian Wang,Zhenhai Xu*

Main category: eess.SP

TL;DR: 提出基于流体天线系统(FAS)的异构双层传输架构，解决低空空地通信中的同频干扰和恶意干扰问题，相比传统固定天线系统(FPA)可实现高达56%的数据率提升


<details>
  <summary>Details</summary>
Motivation: 传统固定位置天线系统缺乏空间自适应性，无法动态平衡信号增强和干扰抑制，难以应对低空空地通信中的同频干扰和恶意干扰挑战

Method: 提出FAS辅助的异构双层传输架构，地面基站使用FPA服务地面用户，低空服务基站和空中用户都配备FAS。采用分数规划-块坐标下降算法交替优化发射预编码器、接收组合器和天线位置，使用凸包方法和几何边界方法处理干扰不确定性和天线放置约束

Result: FAS在同等功率约束下比FPA实现高达56%的数据率提升，通过策略性天线重定位显著增强信号质量同时抑制干扰，在不同干扰信道不确定性下保持鲁棒性

Conclusion: FAS系统通过动态天线位置优化能够有效解决低空空地通信中的干扰问题，相比传统FPA系统具有显著的性能优势，为未来空地通信提供了有效的干扰抑制解决方案

Abstract: This paper addresses the challenge of co-channel interference and intentional
jamming in low-altitude air-ground communications. Since conventional
fixed-position antenna (FPA) systems lack spatial adaptability to dynamically
balance signal enhancement against interference suppression, we propose a
transformative fluid antenna system (FAS)-assisted heterogeneous dual-layer
transmission architecture. Specifically, a terrestrial base station with FPA
serves ground users, while a low altitude-serving base station equipped with
FAS communicates with the aerial user, also equipped with FAS, under the attack
of a malicious jammer. We formulate a worst-case achievable rate maximization
problem for aerial user subject to constraints including quality-of-service for
terrestrial users, imperfect jamming directions, minimum antenna separation,
etc. To address the non-convex problem, we propose a fractional
programming-block coordinate descent algorithm that alternately optimizes the
transmit precoders, receive combiner, and antenna positions at both transceiver
sides. Convex hull-based approach and geometric boundary method are used to
handle the jamming uncertainty and antenna placement constraints in confined
spatial regions, respectively. Extensive simulations validate significant
performance gains. The FAS achieves up to 56\% higher data rates than FPA under
equivalent power constraints. Strategic antenna repositioning demonstrably
enhances signal quality while suppressing interference, maintaining robustness
across diverse jammer channel uncertainties.

</details>


### [50] [Interference Management for Integrated Sensing and Communications: A Multiple Access Perspective](https://arxiv.org/abs/2509.02352)
*Kexin Chen,Yijie Mao,Wonjae Shin,Bruno Clerckx,Christos Masouros*

Main category: eess.SP

TL;DR: 本文是首个关于5910网络中多重访问技术的综述教程，分析了不同MA技术在管理感知通信干扰方面的优势和应用前景


<details>
  <summary>Details</summary>
Motivation: 5910网络中感知与通信的严密集成导致各种干扰问题，需要有效的干扰管理方案，而多重访问技术在通信网络中已经证明能够高效利用谱资源和管理干扰

Method: 通过分析多种MA技术（OMA、SDMA、NOMA、RSMA）在ISAC网络中的应用，比较不同设计方案的优缺点，并提出互利集成框架

Result: 证明了MA技术在ISAC中能够有效管理用户间干扰和功能间干扰，同时ISAC也能促进MA技术在非纯通信场景中更好地发挥干扰管理能力

Conclusion: 多重访问技术与ISAC的结合为6G网络提供了有效的干扰管理解决方案，本文为该领域提供了系统性的教程和研究指南，并展望了未来研究方向

Abstract: The integrated sensing and communication (ISAC) technique has been considered
a key enabler for 6G radio access networks. ISAC fulfills a brand new paradigm
shift in wireless networks via the seamless interplay between communication and
sensing within a unified network. However, the tight integration of these
functionalities inevitably gives rise to various types of interference, posing
significant challenges to existing ISAC waveform designs and rendering
interference management a critical concern. Inspired by the development
trajectory of wireless communications, different multiple access (MA)
techniques, such as orthogonal multiple access (OMA), space-division multiple
access (SDMA), and more recently, non-orthogonal multiple access (NOMA) and
rate-splitting multiple access (RSMA), have been demonstrated to play a pivotal
role in efficiently utilizing limited spectrum resources, designing ISAC
waveforms, as well as managing inter-user interference and inter-functionality
interference in ISAC. Notably, the interplay between MA and ISAC presents
mutually beneficial integration. On the one hand, ISAC helps MA techniques
better exploit their interference management capability beyond the
communication-only networks. On the other hand, different MA techniques serve
as promising solutions for inter-functionality and inter-user interference
management in ISAC. In this paper, we deliver the first comprehensive tutorial
of MA techniques in ISAC networks. Specifically, we illustrate the fundamental
principles of ISAC, classify the diverse types of interference in different
ISAC systems, and compare MA-assisted ISAC designs, highlighting their
respective advantages and limitations. Moreover, we provide an outlook on the
emerging applications and future research directions of different MA-assisted
ISAC.

</details>


### [51] [Know What, Know Why: Semantic Hazard Communication for Intelligent V2X Systems](https://arxiv.org/abs/2509.02442)
*Chen Sun,Wenqi Zhang,Bizhu Wang,Xiaodong Xu,Chau Yuen,Yan Zhang,Ping Zhang*

Main category: eess.SP

TL;DR: 提出SEE-V2X系统，通过语义增强和可解释的V2X通信，为驾驶员提供危险警告的上下文信息，提高交通效率和驾驶决策质量


<details>
  <summary>Details</summary>
Motivation: 当前V2X系统中路边单元(RSU)广播的警告消息缺乏上下文信息，导致驾驶员过度谨慎或低效驾驶行为，需要提供更详细的危险原因解释

Method: 在RSU上部署智能摄像头检测障碍物，传输上下文感知消息；通过真实场景演示展示"透视"功能，让驾驶员可视化障碍物后的行人；进行仿真比较传统V2X与SEE-V2X在不同交通条件下的表现

Result: SEE-V2X显著提高了交通效率，减少了不必要的减速行为

Conclusion: 语义增强和可解释的V2X系统能够为驾驶员提供更全面的危险信息，支持更智能的驾驶决策，改善整体交通流效率

Abstract: In current vehicle-to-everything (V2X) communication systems, roadside units
(RSUs) broadcast brief warning messages that alert nearby vehicles to avoid
potential hazards. However, these messages lack contextual information on why a
warning is issued, leading to excessive caution or inefficient driving
behaviors. To avoid such a situation, we propose a semantic-enhanced and
explainable V2X (SEE-V2X) system. In the proposed system, RSUs equipped with
smart cameras detect obstructions and transmit context-aware messages to
vehicles. By understanding both what the hazard is and why it occurs, drivers
can make more intelligent decisions based on their specific driving situation.
Furthermore, through a real-field demonstration, we show the new "see-through"
feature in the proposed system, which enables drivers to visualize hidden
pedestrians behind obstacles. We also perform simulations to compare
traditional V2X with SEE-V2X under different traffic conditions. The results
show that SEE-V2X significantly improves traffic efficiency and reduces
unnecessary deceleration.

</details>


### [52] [LLM-Enhanced Space-Air-Ground-Sea Integrated Networks](https://arxiv.org/abs/2509.02540)
*Halvin Yang,Sangarapillai Lambotharan,Mahsa Derakhshani,Lajos Hanzo*

Main category: eess.SP

TL;DR: 使用单一大语言模型构建空天地海一体化网络的统一适配层，解决通道状态信息失效和带宽差异问题


<details>
  <summary>Details</summary>
Motivation: 空天地海一体化网络遇到两大挑战：高速移动导致通道状态信息快速失效，以及半导体光学链路与水下声学链路的极端带宽差异

Method: 训练单一大语言模型背榜，聚合处理无线电、光学和声学跟踪数据，实现LLM基于的长程通道预测器和语义编码器

Result: LLM预测器能预测多个相干间隔后的延迟-多普勒分量，语义编码器显著降低海岸水下链路高保真图像传输所需的信噪比

Conclusion: 创建了一个不依赖于传输媒介的适配层，为实验室原型向现场部署推进指明了方向

Abstract: The space-air-ground-sea integrated networking (SAGSIN) concept promises
seamless global multimedia connectivity, yet two obstacles still limit its
practical deployment. Firstly, high-velocity satellites, aerial relays and
sea-surface platforms suffer from obsolete channel state information (CSI),
undermining feedback-based adaptation. Secondly, data-rate disparity across the
protocol stack is extreme: terabit optical links in space coexist with kilobit
acoustic under-water links. This article shows that a single large language
model (LLM) backbone, trained jointly on radio, optical and acoustic traces,
can provide a unified, data-driven adaptation layer that addresses both rapid
CSI ageing and severe bandwidth disparity across the SAGSIN protocol stack.
Explicitly, an LLM-based long-range channel predictor forecasts the strongest
delay-Doppler components several coherence intervals ahead, facilitating
near-capacity reception despite violent channel fluctuations. Furthermore, our
LLM-based semantic encoder turns raw sensor payloads into task-oriented tokens.
This substantially reduces the SNR required for high-fidelity image delivery in
a coastal underwater link, circumventing the data rate limitation by semantic
communications. Inclusion of these tools creates a medium-agnostic adaptation
layer that spans radio, optical and acoustic channels. We conclude with
promising open research directions in on-device model compression, multimodal
fidelity control, cross-layer resource orchestration and trustworthy operation,
charting a path from laboratory prototypes to field deployment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Lagrangian Relaxation for Multi-Action Partially Observable Restless Bandits: Heuristic Policies and Indexability](https://arxiv.org/abs/2509.00415)
*Rahul Meshram,Kesav Kaza*

Main category: cs.LG

TL;DR: 本文研究了多动作部分可观测的躁动多臂老虎机问题，这是经典躁动多臂老虎机问题的扩展，具有有限状态、不可观测状态、多动作选择等特点，并应用于公共卫生干预规划。


<details>
  <summary>Details</summary>
Motivation: 部分可观测的躁动多臂老虎机在推荐系统、通信系统、公共卫生干预和运筹学中有广泛应用。传统模型通常只考虑两种动作，而现实应用中往往需要更多动作选择，因此需要研究多动作情况下的优化问题。

Method: 采用拉格朗日边界方法，使用基于点的值迭代(PBVI)和在线滚动策略来近似计算拉格朗日边界。分析了值函数的各种性质，提供了PBVI和在线滚动策略的理论见解，并研究了启发式策略和Whittle索引策略。

Result: 提出了针对多动作部分可观测躁动多臂老虎机的拉格朗日边界计算方法，通过PBVI和在线滚动策略实现了有效的近似计算，并分析了各种策略的性能特性。

Conclusion: 多动作部分可观测躁动多臂老虎机问题具有重要的实际应用价值，拉格朗日边界方法结合PBVI和在线滚动策略提供了有效的解决方案，但Whittle索引策略在该模型中存在局限性。

Abstract: Partially observable restless multi-armed bandits have found numerous
applications including in recommendation systems, communication systems, public
healthcare outreach systems, and in operations research. We study multi-action
partially observable restless multi-armed bandits, it is a generalization of
the classical restless multi-armed bandit problem -- 1) each bandit has finite
states, and the current state is not observable, 2) each bandit has finite
actions. In particular, we assume that more than two actions are available for
each bandit. We motivate our problem with the application of public-health
intervention planning. We describe the model and formulate a long term
discounted optimization problem, where the state of each bandit evolves
according to a Markov process, and this evolution is action dependent. The
state of a bandit is not observable but one of finitely many feedback signals
are observable. Each bandit yields a reward, based on the action taken on that
bandit. The agent is assumed to have a budget constraint. The bandits are
assumed to be independent. However, they are weakly coupled at the agent
through the budget constraint.
  We first analyze the Lagrangian bound method for our partially observable
restless bandits. The computation of optimal value functions for finite-state,
finite-action POMDPs is non-trivial. Hence, the computation of Lagrangian
bounds is also challenging. We describe approximations for the computation of
Lagrangian bounds using point based value iteration (PBVI) and online rollout
policy. We further present various properties of the value functions and
provide theoretical insights on PBVI and online rollout policy. We study
heuristic policies for multi-actions PORMAB. Finally, we discuss present
Whittle index policies and their limitations in our model.

</details>


### [54] [Diagnosing Psychiatric Patients: Can Large Language and Machine Learning Models Perform Effectively in Emergency Cases?](https://arxiv.org/abs/2509.00026)
*Abu Shad Ahammed,Sayeri Mukherjee,Roman Obermaisser*

Main category: cs.LG

TL;DR: 本研究探讨如何使用传统机器学习和大型语言模型（LLama 3.1等）基于行为模式评估急诊精神病患者，为精神障碍诊断提供辅助工具。


<details>
  <summary>Details</summary>
Motivation: 精神障碍患者由于缺乏明显症状常被误判和误诊，在紧急情况下识别精神病问题具有挑战性但至关重要，需要开发有效的诊断评估工具。

Method: 收集德国救援站的急诊精神病患者数据，使用包括Llama 3.1在内的多种机器学习模型，分析患者行为模式进行预测评估。

Result: 研究验证了机器学习模型在识别精神障碍患者方面的预测能力，特别是在救援场景中的应用潜力。

Conclusion: 机器学习和大型语言模型可以作为有效的辅助工具，帮助在紧急情况下识别和评估精神障碍患者，提高诊断准确性。

Abstract: Mental disorders are clinically significant patterns of behavior that are
associated with stress and/or impairment in social, occupational, or family
activities. People suffering from such disorders are often misjudged and poorly
diagnosed due to a lack of visible symptoms compared to other health
complications. During emergency situations, identifying psychiatric issues is
that's why challenging but highly required to save patients. In this paper, we
have conducted research on how traditional machine learning and large language
models (LLM) can assess these psychiatric patients based on their behavioral
patterns to provide a diagnostic assessment. Data from emergency psychiatric
patients were collected from a rescue station in Germany. Various machine
learning models, including Llama 3.1, were used with rescue patient data to
assess if the predictive capabilities of the models can serve as an efficient
tool for identifying patients with unhealthy mental disorders, especially in
rescue cases.

</details>


### [55] [Mitigating Data Exfiltration Attacks through Layer-Wise Learning Rate Decay Fine-Tuning](https://arxiv.org/abs/2509.00027)
*Elie Thellier,Huiyu Li,Nicholas Ayache,Hervé Delingette*

Main category: cs.LG

TL;DR: 提出了一种通过在模型导出时使用衰减分层学习率进行微调来扰动模型参数的方法，有效破坏数据嵌入攻击，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 数据湖训练强大机器学习模型时存在严重的隐私风险，攻击者可以通过模型参数嵌入或多任务学习窃取训练数据，特别是高保真医疗图像，带来法律和伦理问题。

Method: 在模型导出时通过使用衰减分层学习率的微调来扰动模型参数，破坏嵌入的数据表示而不影响任务性能。

Result: 在DermaMNIST、ChestMNIST和MIMIC-CXR数据集上评估显示，该方法保持了效用任务性能，有效破坏了最先进的数据窃取攻击，优于现有防御方法，使窃取的数据无法用于训练。

Conclusion: 该方法为数据湖训练模型和集中式联邦学习提供了实用的数据泄露防御方案，消融实验和自适应攻击讨论指出了挑战和未来方向。

Abstract: Data lakes enable the training of powerful machine learning models on
sensitive, high-value medical datasets, but also introduce serious privacy
risks due to potential leakage of protected health information. Recent studies
show adversaries can exfiltrate training data by embedding latent
representations into model parameters or inducing memorization via multi-task
learning. These attacks disguise themselves as benign utility models while
enabling reconstruction of high-fidelity medical images, posing severe privacy
threats with legal and ethical implications. In this work, we propose a simple
yet effective mitigation strategy that perturbs model parameters at export time
through fine-tuning with a decaying layer-wise learning rate to corrupt
embedded data without degrading task performance. Evaluations on DermaMNIST,
ChestMNIST, and MIMIC-CXR show that our approach maintains utility task
performance, effectively disrupts state-of-the-art exfiltration attacks,
outperforms prior defenses, and renders exfiltrated data unusable for training.
Ablations and discussions on adaptive attacks highlight challenges and future
directions. Our findings offer a practical defense against data leakage in data
lake-trained models and centralized federated learning.

</details>


### [56] [Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP](https://arxiv.org/abs/2509.01630)
*Bingheng Wang,Yichao Gao,Tianchen Sun,Lin Zhao*

Main category: cs.LG

TL;DR: L2C是一个通过元学习来自动调整ADMM-DDP算法超参数的框架，使用轻量级神经网络建模超参数，能够适应不同任务和智能体配置，在分布式轨迹优化中实现高效协调。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式轨迹优化方法ADMM-DDP需要大量手动调整紧密耦合的超参数，这些参数共同影响局部任务性能和全局协调效果，调参过程繁琐且困难。

Method: 提出L2C框架，通过端到端可微的ADMM-DDP管道，使用轻量级神经网络元学习超参数；重用DDP组件（如Riccati递归和反馈增益）进行高效元梯度计算；通过截断迭代和元学习ADMM惩罚参数来加速训练。

Result: 在合作空中运输任务中，L2C能够在高保真仿真中生成动态可行的轨迹，重新配置四旋翼编队以在狭小空间中进行安全的6自由度负载操作，并能鲁棒地适应不同的团队规模和任务条件，梯度计算速度比现有方法快88%。

Conclusion: L2C提供了一个通用的元学习框架，能够自动适应分布式轨迹优化中的超参数调整，显著提高了多智能体系统的协调效率和性能，具有很好的实用性和适应性。

Abstract: Distributed trajectory optimization via ADMM-DDP is a powerful approach for
coordinating multi-agent systems, but it requires extensive tuning of tightly
coupled hyperparameters that jointly govern local task performance and global
coordination. In this paper, we propose Learning to Coordinate (L2C), a general
framework that meta-learns these hyperparameters, modeled by lightweight
agent-wise neural networks, to adapt across diverse tasks and agent
configurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in
a distributed manner. It also enables efficient meta-gradient computation by
reusing DDP components such as Riccati recursions and feedback gains. These
gradients correspond to the optimal solutions of distributed matrix-valued LQR
problems, coordinated across agents via an auxiliary ADMM framework that
becomes convex under mild assumptions. Training is further accelerated by
truncating iterations and meta-learning ADMM penalty parameters optimized for
rapid residual reduction, with provable Lipschitz-bounded gradient errors. On a
challenging cooperative aerial transport task, L2C generates dynamically
feasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures
quadrotor formations for safe 6-DoF load manipulation in tight spaces, and
adapts robustly to varying team sizes and task conditions, while achieving up
to $88\%$ faster gradient computation than state-of-the-art methods.

</details>


### [57] [ZeroQAT: Your Quantization-aware Training but Efficient](https://arxiv.org/abs/2509.00031)
*Qitao Tan,Xiaoying Song,Jin Lu,Guoming Li,Jun Liu,Lingzi Hong,Caiwen Ding,Jundong Li,Xiaoming Zhai,Shaoyi Huang,Wei Niu,Geng Yuan*

Main category: cs.LG

TL;DR: ZeroQAT是一种基于零阶优化的量化感知训练框架，通过前向梯度估计消除反向传播需求，在保持端到端优化优势的同时显著降低计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有低比特后训练量化方法存在精度下降问题，而传统量化感知训练依赖反向传播导致计算成本过高，限制了实际应用。

Method: 使用零阶优化方法进行前向梯度估计，联合学习量化权重、权重裁剪阈值和等效变换，以消除反向传播需求并处理激活异常值。

Result: 实验表明ZeroQAT实现了后训练量化的效率，同时保持了量化感知训练的精度，为LLMs高质量低比特量化提供了实用解决方案。

Conclusion: ZeroQAT框架成功解决了量化训练中的计算成本问题，为大规模语言模型的高效部署提供了可行的技术路径。

Abstract: Quantization is an effective technique to reduce the deployment cost of large
language models (LLMs), and post-training quantization (PTQ) has been widely
studied due to its efficiency. However, existing low-bit PTQ methods suffer
from accuracy degradation because their layer-wise optimization introduces
cumulative error propagation and misalignment between local reconstruction
objectives and downstream performance. While quantization-aware training (QAT)
provides a principled solution, its reliance on backpropagation incurs
prohibitive data, time, and memory costs, limiting its practicality. To address
these challenges, we propose ZeroQAT, a zeroth-order optimization-based QAT
framework. ZeroQAT leverages forward-only gradient estimation to eliminate the
need for backpropagation, significantly reducing computational and memory
overhead while retaining the benefits of end-to-end optimization. Moreover,
ZeroQAT jointly learns quantized weights, weight clipping thresholds, and
equivalent transformations to mitigate quantization error and handle activation
outliers. Experiments demonstrate that ZeroQAT achieves the efficiency of PTQ
while retaining the accuracy of QAT, offering a practical solution for
high-quality low-bit quantization of LLMs.

</details>


### [58] [Semi-on-Demand Transit Feeders with Shared Autonomous Vehicles and Reinforcement-Learning-Based Zonal Dispatching Control](https://arxiv.org/abs/2509.01883)
*Max T. M. Ng,Roman Engelhardt,Florian Dandl,Hani S. Mahmassani,Klaus Bogenberger*

Main category: cs.LG

TL;DR: 开发基于强化学习的半按需公交接驳服务，使用共享自动驾驶车辆和动态区域调度控制，在固定路线基础上增加按需服务，提升低密度区域可达性。


<details>
  <summary>Details</summary>
Motivation: 结合固定路线公交的成本效益和需求响应交通的适应性，解决低密度区域公共交通可达性问题，改善第一英里/最后一英里连接。

Method: 使用深度强化学习（PPO算法）动态分配车辆到细分区域，响应实时需求波动。基于代理的仿真在慕尼黑真实公交线路上验证。

Result: RL控制后的半按需服务比传统固定路线服务多服务16%乘客，平均成本高13%。RL控制带来2.4%额外乘客和1.4%成本增加。

Conclusion: 展示了SAV接驳车和机器学习技术在公共交通中的整合潜力，为多模式交通系统中解决第一英里/最后一英里问题奠定了基础。

Abstract: This paper develops a semi-on-demand transit feeder service using shared
autonomous vehicles (SAVs) and zonal dispatching control based on reinforcement
learning (RL). This service combines the cost-effectiveness of fixed-route
transit with the adaptability of demand-responsive transport to improve
accessibility in lower-density areas. Departing from the terminus, SAVs first
make scheduled fixed stops, then offer on-demand pick-ups and drop-offs in a
pre-determined flexible-route area. Our deep RL model dynamically assigns
vehicles to subdivided flexible-route zones in response to real-time demand
fluctuations and operations, using a policy gradient algorithm - Proximal
Policy Optimization. The methodology is demonstrated through agent-based
simulations on a real-world bus route in Munich, Germany. Results show that
after efficient training of the RL model, the semi-on-demand service with
dynamic zonal control serves 16% more passengers at 13% higher generalized
costs on average compared to traditional fixed-route service. The efficiency
gain brought by RL control brings 2.4% more passengers at 1.4% higher costs.
This study not only showcases the potential of integrating SAV feeders and
machine learning techniques into public transit, but also sets the groundwork
for further innovations in addressing first-mile-last-mile problems in
multimodal transit systems.

</details>


### [59] [Industrial Steel Slag Flow Data Loading Method for Deep Learning Applications](https://arxiv.org/abs/2509.00034)
*Mert Sehri,Ana Cardoso,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: 提出一种基于振动数据的跨域诊断方法，使用混合深度学习模型（1D-CNN+LSTM）来检测钢铸造过程中的渣流状态，准确率达到99.10%，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 钢铸造过程中渣流污染会导致重大经济损失，需要准确检测渣流状态来避免损失。

Method: 使用一维卷积神经网络和长短期记忆层的混合深度学习模型，处理原始时域振动信号，结合RMS预处理和选择性嵌入数据加载策略。

Result: 在16个不同域上测试，混合模型达到99.10% ± 0.30%的最高测试准确率，表现出优异的泛化能力和工业适用性。

Conclusion: 该方法为实时渣流监测提供了实用且可扩展的解决方案，有助于提高钢铁制造的可靠性和运营效率。

Abstract: Steel casting processes are vulnerable to financial losses due to slag flow
contamination, making accurate slag flow condition detection essential. This
study introduces a novel cross-domain diagnostic method using vibration data
collected from an industrial steel foundry to identify various stages of slag
flow. A hybrid deep learning model combining one-dimensional convolutional
neural networks and long short-term memory layers is implemented, tested, and
benchmarked against a standard one-dimensional convolutional neural network.
The proposed method processes raw time-domain vibration signals from
accelerometers and evaluates performance across 16 distinct domains using a
realistic cross-domain dataset split. Results show that the hybrid
convolutional neural network and long short-term memory architecture, when
combined with root mean square preprocessing and a selective embedding data
loading strategy, achieves robust classification accuracy, outperforming
traditional models and loading techniques. The highest test accuracy of 99.10
+/- 0.30 demonstrates the method's capability for generalization and industrial
relevance. This work presents a practical and scalable solution for real-time
slag flow monitoring, contributing to improved reliability and operational
efficiency in steel manufacturing.

</details>


### [60] [Transfer Learning for Minimum Operating Voltage Prediction in Advanced Technology Nodes: Leveraging Legacy Data and Silicon Odometer Sensing](https://arxiv.org/abs/2509.00035)
*Yuxuan Yin,Rebecca Chen,Boxun Xu,Chen He,Peng Li*

Main category: cs.LG

TL;DR: 提出基于迁移学习的新框架，利用16nm节点的丰富历史数据来提升5nm节点芯片最小工作电压(Vmin)预测精度，通过集成片上硅里程计传感器数据来精细表征局部工艺变化。


<details>
  <summary>Details</summary>
Motivation: 在先进工艺节点下，由于训练数据有限且工艺变化与Vmin关系复杂，准确预测芯片性能面临挑战，这对于确保半导体制造的能效和可靠性至关重要。

Method: 采用迁移学习框架，利用16nm节点的丰富历史数据，并集成片上硅里程计传感器提供的输入特征，以精细表征5nm节点的局部工艺变化。

Result: 该方法显著提高了Vmin预测的准确性，特别是在先进5nm工艺节点下。

Conclusion: 所提出的迁移学习框架结合片上传感器数据，有效解决了先进工艺节点下Vmin预测的数据稀缺和复杂性挑战，为半导体制造提供了更准确的性能预测方案。

Abstract: Accurate prediction of chip performance is critical for ensuring energy
efficiency and reliability in semiconductor manufacturing. However, developing
minimum operating voltage ($V_{min}$) prediction models at advanced technology
nodes is challenging due to limited training data and the complex relationship
between process variations and $V_{min}$. To address these issues, we propose a
novel transfer learning framework that leverages abundant legacy data from the
16nm technology node to enable accurate $V_{min}$ prediction at the advanced
5nm node. A key innovation of our approach is the integration of input features
derived from on-chip silicon odometer sensor data, which provide fine-grained
characterization of localized process variations -- an essential factor at the
5nm node -- resulting in significantly improved prediction accuracy.

</details>


### [61] [A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler](https://arxiv.org/abs/2509.00036)
*Cheng Jin,Zhenyu Xiao,Yuantao Gu*

Main category: cs.LG

TL;DR: A-FloPS是一种无需训练的高效扩散模型采样框架，通过流匹配重参数化和自适应速度分解，在极低函数评估次数下实现优质图像生成


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量优秀，但迭代采样过程计算昂贵。现有训练免费加速方法受限于采样轨迹效率，需要更有效的解决方案

Method: 将预训练扩散模型的采样轨迹重参数化为流匹配形式，并采用自适应速度分解机制，将速度场分解为线性漂移项和残差分量

Result: 在条件图像生成和文本到图像合成任务中，A-FloPS在5次函数评估下就能显著降低FID分数，生成更清晰、更连贯的图像

Conclusion: A-FloPS为高质量、低延迟生成建模提供了一个通用有效的解决方案，其自适应机制还能改进原生基于流的生成模型

Abstract: Diffusion models deliver state-of-the-art generative performance across
diverse modalities but remain computationally expensive due to their inherently
iterative sampling process. Existing training-free acceleration methods
typically improve numerical solvers for the reverse-time ODE, yet their
effectiveness is fundamentally constrained by the inefficiency of the
underlying sampling trajectories. We propose A-FloPS (Adaptive Flow Path
Sampler), a principled, training-free framework that reparameterizes the
sampling trajectory of any pre-trained diffusion model into a flow-matching
form and augments it with an adaptive velocity decomposition. The
reparameterization analytically maps diffusion scores to flow-compatible
velocities, yielding integration-friendly trajectories without retraining. The
adaptive mechanism further factorizes the velocity field into a linear drift
term and a residual component whose temporal variation is actively suppressed,
restoring the accuracy benefits of high-order integration even in extremely
low-NFE regimes. Extensive experiments on conditional image generation and
text-to-image synthesis show that A-FloPS consistently outperforms
state-of-the-art training-free samplers in both sample quality and efficiency.
Notably, with as few as $5$ function evaluations, A-FloPS achieves
substantially lower FID and generates sharper, more coherent images. The
adaptive mechanism also improves native flow-based generative models,
underscoring its generality. These results position A-FloPS as a versatile and
effective solution for high-quality, low-latency generative modeling.

</details>


### [62] [Exploring and Reshaping the Weight Distribution in LLM](https://arxiv.org/abs/2509.00046)
*Chunming Ye,Songzhou Li,Xu Xu*

Main category: cs.LG

TL;DR: 本文研究发现大语言模型中不同层权重间的余弦距离呈现幂律分布特征，基于此提出LoRA权重初始化优化方法，在不改变模型结构的情况下提升了LoRA训练性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能受架构、模型大小、解码方法等特性影响。由于结构或功能差异，大模型不同层的权重具有不同的分布特征，研究这些层间权重分布的相关性及其对LoRA训练效果的影响具有重要意义。

Method: 1) 提取自注意力层和MLP层中的Query-projection、down-projection等权重矩阵，通过奇异值分解计算矩阵奇异值；2) 分析矩阵间余弦距离的概率分布，发现其具有幂律分布特征；3) 提出定性方法描述不同模型的分布特征；4) 结合高斯过程和帕累托分布函数设计数据生成器，模拟符合特定分布特征的数据；5) 基于分布特征和数据生成方法重塑LoRA初始化权重。

Result: 实验结果表明，在不改变模型结构或训练过程的情况下，该方法实现了LoRA训练性能的一定提升。

Conclusion: 大语言模型中不同层权重间的余弦距离确实呈现幂律分布特征，基于这一发现优化的LoRA权重初始化方法能够有效提升训练效果，为模型参数优化提供了新的思路。

Abstract: The performance of Large Language Models is influenced by their
characteristics such as architecture, model sizes, decoding methods and so on.
Due to differences in structure or function, the weights in different layers of
large models have varying distributions. This paper explores the correlations
between different types of layers in terms of weights distribution and studies
the potential impact of these correlations on LoRA training effectiveness.
Firstly, the study reveals that in the model the cosine distances between
weights of different layers manifest power-law distribution. We extract
Query-projection, down-projection and other weight matrices from the
self-attention layers and MLP layers, calculate the singular values of the
matrices using singular value decomposition, and organize a certain number of
singular values into matrices according to projection's type. By analyzing the
probability distribution of the cosine distances between these matrices, it is
found that the cosine distances values between them have distinct power-law
distribution characteristics. Secondly, based on the results of distance
calculations and analysis across different layers of model, a qualitative
method is proposed to describe the distribution characteristics of different
models. Next, to construct weights that align with the distribution
characteristics, a data generator is designed using a combination of Gaussian
process and Pareto distribution functions. The generator is used to simulate
the generation of data that aligns with specific distribution characteristics.
Finally, based on the aforementioned distribution characteristics and data
generation method, the weights in LoRA initialization are reshaped for
training. Experimental results indicate that, without altering the model
structure or training process, this method achieves a certain improvement in
the performance of LoRA training.

</details>


### [63] [Teaching AI to Remember: Insights from Brain-Inspired Replay in Continual Learning](https://arxiv.org/abs/2509.00047)
*Jina Kim*

Main category: cs.LG

TL;DR: 该论文研究了受大脑启发的内部回放机制在持续学习中的作用，发现该机制能显著减轻灾难性遗忘，但与突触智能结合时会降低初始任务准确性，揭示了记忆稳定性与学习可塑性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络在持续学习中面临灾难性遗忘的挑战，受人类大脑记忆巩固机制的启发，研究内部回放机制在缓解遗忘问题上的有效性。

Method: 在CIFAR-100数据集上采用类增量设置，评估内部回放机制（单独使用和与突触智能结合使用），通过似然分布、重建误差、轮廓分数和UMAP投影等方法分析潜在空间表征变化。

Result: 内部回放显著减轻了遗忘现象，特别是与突触智能结合时效果更好，但代价是降低了初始任务的准确性。分析显示内部回放增加了潜在空间的表征重叠，可能限制了任务特异性区分。

Conclusion: 当前受大脑启发的方法存在局限性，需要在记忆保持和学习适应性之间找到更好的平衡，为持续学习系统的未来发展指明了方向。

Abstract: Artificial neural networks (ANNs) continue to face challenges in continual
learning, particularly due to catastrophic forgetting, the loss of previously
learned knowledge when acquiring new tasks. Inspired by memory consolidation in
the human brain, we investigate the internal replay mechanism proposed
by~\citep{brain_inspired_replay1}, which reactivates latent representations of
prior experiences during learning. As internal replay was identified as the
most influential component among the brain-inspired mechanisms in their
framework, it serves as the central focus of our in-depth investigation. Using
the CIFAR-100 dataset in a class-incremental setting, we evaluate the
effectiveness of internal replay, both in isolation and in combination with
Synaptic Intelligence (SI). Our experiments show that internal replay
significantly mitigates forgetting, especially when paired with SI, but at the
cost of reduced initial task accuracy, highlighting a trade-off between memory
stability and learning plasticity. Further analyses using log-likelihood
distributions, reconstruction errors, silhouette scores, and UMAP projections
reveal that internal replay increases representational overlap in latent space,
potentially limiting task-specific differentiation. These results underscore
the limitations of current brain-inspired methods and suggest future directions
for balancing retention and adaptability in continual learning systems.

</details>


### [64] [Adaptive Physics-Informed Neural Networks with Multi-Category Feature Engineering for Hydrogen Sorption Prediction in Clays, Shales, and Coals](https://arxiv.org/abs/2509.00049)
*Mohammad Nooraiepour,Mohammad Masoudi,Zezhang Song,Helge Hellevang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种适应性物理信息神经网络(PINN)框架，通过多类别特征工程提高氢气吸附预测的准确性，在水平形成岩、页岩和煤矿中获得了高精度预测结果(R2 = 0.979)。


<details>
  <summary>Details</summary>
Motivation: 传统实验方法在预测氢气吸附时存在耗时、易出错、无法抓取地质异质性等问题，而氢气吸附预测对地下氢气存储、自然氢气探索和放射性废物定容至关重要。

Method: 研究提出了适应性物理信息神经网络框架，结合经典吸附线模型和热力学约束，使用深度殊差网络和多头注意力机制，通过适应损失函数和蒙特卡洛投出进行不确定性量化。使用155个样本的综合数据集，进行了多类别特征工程。

Result: 框架在K折交叉验证中达到了高准确度(R2 = 0.979, RMSE = 0.045 mol/kg)，汇合速度提高67%，虽然复杂度增加15倍。在水平形成岩(R2 = 0.981)、页岩(R2 = 0.971)和煤矿(R2 = 0.978)中都表现突出，可靠性评分保持在85-91%。

Conclusion: 该适应性物理信息框架通过结合物理约束和深度学习的优势，显著提高了氢气吸附预测的准确性和可靠性，为地下氢气存储和风险决策提供了有力支持。

Abstract: Accurate prediction of hydrogen sorption in clays, shales, and coals is vital
for advancing underground hydrogen storage, natural hydrogen exploration, and
radioactive waste containment. Traditional experimental methods, while
foundational, are time-consuming, error-prone, and limited in capturing
geological heterogeneity. This study introduces an adaptive physics-informed
neural network (PINN) framework with multi-category feature engineering to
enhance hydrogen sorption prediction. The framework integrates classical
isotherm models with thermodynamic constraints to ensure physical consistency
while leveraging deep learning flexibility. A comprehensive dataset consisting
of 155 samples, which includes 50 clays, 60 shales, and 45 coals, was employed,
incorporating diverse compositional properties and experimental conditions.
Multi-category feature engineering across seven categories captured complex
sorption dynamics. The PINN employs deep residual networks with multi-head
attention, optimized via adaptive loss functions and Monte Carlo dropout for
uncertainty quantification. K-fold cross-validation and hyperparameter
optimization achieve significant accuracy (R2 = 0.979, RMSE = 0.045 mol per kg)
with 67% faster convergence despite 15-fold increased complexity. The framework
demonstrates robust lithology-specific performance across clay minerals (R2 =
0.981), shales (R2 = 0.971), and coals (R2 = 0.978), maintaining 85-91%
reliability scores. Interpretability analysis via SHAP, accumulated local
effects, and Friedman's H-statistics reveal that hydrogen adsorption capacity
dominates predictions, while 86.7% of feature pairs exhibit strong
interactions, validating the necessity of non-linear modeling approaches. This
adaptive physics-informed framework accelerates site screening and enables
risk-informed decision-making through robust uncertainty quantification.

</details>


### [65] [Applying Deep Learning to Anomaly Detection of Russian Satellite Activity for Indications Prior to Military Activity](https://arxiv.org/abs/2509.00050)
*David Kurtenbach,Megan Manly,Zach Metzinger*

Main category: cs.LG

TL;DR: 使用深度学习技术分析俄罗斯卫星在乌克兰入侵前的异常活动，通过多种自编码器模型识别具有统计显著性的异常行为


<details>
  <summary>Details</summary>
Motivation: 通过分析俄罗斯卫星的异常活动来获取军事冲突的预警信号，建立关于可能战术和程序的理解

Method: 采用隔离森林(IF)、传统自编码器(AE)、变分自编码器(VAE)、Kolmogorov Arnold网络(KAN)和新颖锚损失自编码器(Anchor AE)等深度学习方法，基于公开二行元素(TLE)数据进行分析

Result: 识别出了具有统计显著性的俄罗斯卫星异常活动，并能够到达到单个轨道元素级别的异常识别

Conclusion: 深度学习方法能够有效检测卫星异常活动，为未来军事冲突提供了可靠的预警指标，具有重要的预警价值

Abstract: We apply deep learning techniques for anomaly detection to analyze activity
of Russian-owned resident space objects (RSO) prior to the Ukraine invasion and
assess the results for any findings that can be used as indications and
warnings (I&W) of aggressive military behavior for future conflicts. Through
analysis of anomalous activity, an understanding of possible tactics and
procedures can be established to assess the existence of statistically
significant changes in Russian RSO pattern of life/pattern of behavior
(PoL/PoB) using publicly available two-line element (TLE) data. This research
looks at statistical and deep learning approaches to assess anomalous activity.
The deep learning methods assessed are isolation forest (IF), traditional
autoencoder (AE), variational autoencoder (VAE), Kolmogorov Arnold Network
(KAN), and a novel anchor-loss based autoencoder (Anchor AE). Each model is
used to establish a baseline of on-orbit activity based on a five-year data
sample. The primary investigation period focuses on the six months leading up
to the invasion date of February 24, 2022. Additional analysis looks at RSO
activity during an active combat period by sampling TLE data after the invasion
date. The deep learning autoencoder models identify anomalies based on
reconstruction errors that surpass a threshold sigma. To capture the nuance and
unique characteristics of each RSO an individual model was trained for each
observed space object. The research made an effort to prioritize explainability
and interpretability of the model results thus each observation was assessed
for anomalous behavior of the individual six orbital elements versus analyzing
the input data as a single monolithic observation. The results demonstrate not
only statistically significant anomalies of Russian RSO activity but also
details anomalous findings to the individual orbital element.

</details>


### [66] [From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis](https://arxiv.org/abs/2509.00057)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,Nicola Sambo,Joao Pedro,Mohammad M. Hosseini,Antonio Napoli,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: 本文比较了预处理、处理中和后处理方法在光网络故障管理中的类别不平衡问题，发现后处理方法在故障检测中F1分数提升最高（15.3%），而GenAI方法在故障识别中性能提升最大（24.2%）


<details>
  <summary>Details</summary>
Motivation: 光网络中机器学习故障管理面临严重的类别不平衡问题，正常实例远多于故障案例，而后处理方法在此领域尚未得到充分研究

Method: 使用实验数据集对预处理、处理中和后处理方法进行直接比较，包括阈值调整、随机欠采样、SMOTE过采样、元学习和生成式AI等方法

Result: 故障检测中后处理方法（特别是阈值调整）F1分数提升最高；故障识别中GenAI方法性能提升最大；不同场景下最优方法不同：有类别重叠且延迟关键时SMOTE最有效，无延迟约束时元学习最佳，低重叠场景下生成式AI性能最高

Conclusion: 不同类别不平衡缓解方法在不同应用场景下各有优势，需要根据具体需求（如延迟约束、类别重叠情况）选择最适合的方法

Abstract: Machine learning-based failure management in optical networks has gained
significant attention in recent years. However, severe class imbalance, where
normal instances vastly outnumber failure cases, remains a considerable
challenge. While pre- and in-processing techniques have been widely studied,
post-processing methods are largely unexplored. In this work, we present a
direct comparison of pre-, in-, and post-processing approaches for class
imbalance mitigation in failure detection and identification using an
experimental dataset. For failure detection, post-processing
methods-particularly Threshold Adjustment-achieve the highest F1 score
improvement (up to 15.3%), while Random Under-Sampling provides the fastest
inference. In failure identification, GenAI methods deliver the most
substantial performance gains (up to 24.2%), whereas post-processing shows
limited impact in multi-class settings. When class overlap is present and
latency is critical, over-sampling methods such as the SMOTE are most
effective; without latency constraints, Meta-Learning yields the best results.
In low-overlap scenarios, Generative AI approaches provide the highest
performance with minimal inference time.

</details>


### [67] [T-MLP: Tailed Multi-Layer Perceptron for Level-of-Detail Signal Representation](https://arxiv.org/abs/2509.00066)
*Chuanxiang Yang,Yuanfeng Zhou,Guangshun Wei,Siyu Ren,Yuan Liu,Junhui Hou,Wenping Wang*

Main category: cs.LG

TL;DR: 提出了一种基于MLP改进的T-MLP架构，通过添加多个输出分支（尾部）实现多尺度细节层次信号表示，在各种信号表示任务中优于现有神经LoD基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统的多层感知机(MLP)只能在单一尺度上操作，缺乏对细节层次(LoD)信号表示的原生支持，而LoD表示对于高效建模和传输图像、3D形状等信号至关重要。

Method: 在MLP隐藏层附加多个输出分支（称为尾部），通过直接监督多个深度来实现多尺度建模。采用特定的损失函数和训练策略，使每个隐藏层能够有效学习特定LoD的目标信号。

Result: 广泛的实验结果表明，T-MLP在各种信号表示任务中都优于其他神经LoD基线方法。

Conclusion: T-MLP架构成功扩展了传统MLP的能力，为多尺度细节层次信号表示提供了一种有效的神经网络解决方案，在多个应用领域展现出优越性能。

Abstract: Level-of-detail (LoD) representation is critical for efficiently modeling and
transmitting various types of signals, such as images and 3D shapes. In this
work, we present a novel neural architecture that supports LoD signal
representation. Our architecture is based on an elaborate modification of the
widely used Multi-Layer Perceptron (MLP), which inherently operates at a single
scale and therefore lacks native support for LoD. Specifically, we introduce
the Tailed Multi-Layer Perceptron (T-MLP) that extends the MLP by attaching
multiple output branches, also called tails, to its hidden layers, enabling
direct supervision at multiple depths. Our loss formulation and training
strategy allow each hidden layer to effectively learn a target signal at a
specific LoD, thus enabling multi-scale modeling. Extensive experimental
results show that our T-MLP outperforms other neural LoD baselines across a
variety of signal representation tasks.

</details>


### [68] [AnomalyExplainer Explainable AI for LLM-based anomaly detection using BERTViz and Captum](https://arxiv.org/abs/2509.00069)
*Prasasthy Balasubramanian,Dumindu Kankanamge,Ekaterina Gilman,Mourad Oussalah*

Main category: cs.LG

TL;DR: 该研究提出了一个结合异常检测和可解释AI的框架，使用BERTViz和Captum可视化工具及自然语言报告来增强网络安全工作流，RoBERTa模型在HDFS数据集上表现出99.6%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释AI(XAI)旨在提高AI决策透明度，但许多安全分析师仍对其实用性存疑。现有系统存在误报率高和模型管理复杂等问题，限制了信任度。

Method: 开发了一个框架，通过BERTViz和Captum可视化工具结合基于注意力输出的自然语言报告，提供高质量解释。使用RoBERTa模型进行异常检测，并在HDFS数据集上进行比较分析。

Result: RoBERTa达到99.6%的高准确率，在异常检测方面优于Falcon-7B和DeBERTa，比大规模Mistral-7B更具灵活性。用户反馈确认聊天机器人易于使用且提高了对异常的理解。

Conclusion: 该框架成功减少了人工工作量并加速了修复过程，证明了其在加强网络安全工作流程方面的能力，为安全分析师提供了更可信和可操作的AI辅助工具。

Abstract: Conversational AI and Large Language Models (LLMs) have become powerful tools
across domains, including cybersecurity, where they help detect threats early
and improve response times. However, challenges such as false positives and
complex model management still limit trust. Although Explainable AI (XAI) aims
to make AI decisions more transparent, many security analysts remain uncertain
about its usefulness. This study presents a framework that detects anomalies
and provides high-quality explanations through visual tools BERTViz and Captum,
combined with natural language reports based on attention outputs. This reduces
manual effort and speeds up remediation. Our comparative analysis showed that
RoBERTa offers high accuracy (99.6 %) and strong anomaly detection,
outperforming Falcon-7B and DeBERTa, as well as exhibiting better flexibility
than large-scale Mistral-7B on the HDFS dataset from LogHub. User feedback
confirms the chatbot's ease of use and improved understanding of anomalies,
demonstrating the ability of the developed framework to strengthen
cybersecurity workflows.

</details>


### [69] [SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits](https://arxiv.org/abs/2509.00071)
*Shang Liu,Jing Wang,Wenji Fang,Zhiyao Xie*

Main category: cs.LG

TL;DR: SynCircuit是一个首次尝试生成HDL格式功能性合成电路的框架，通过扩散模型生成有向循环图、约束优化确保电路有效性、MCTS减少逻辑冗余，解决了电路设计数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: AI辅助IC设计方法面临电路设计数据极度稀缺的瓶颈，特别是在公共领域，这限制了AI方法的发展和应用。

Method: 提出三步骤框架：1)定制扩散模型生成有向循环图；2)约束优化确保电路功能有效性；3)MCTS方法优化逻辑冗余。

Result: 实验结果表明SynCircuit能够生成更真实的合成电路，并提升下游电路设计任务中机器学习模型的性能。

Conclusion: SynCircuit成功解决了电路设计数据稀缺问题，为AI辅助IC设计提供了有效的数据生成解决方案，具有重要的实践价值。

Abstract: In recent years, AI-assisted IC design methods have demonstrated great
potential, but the availability of circuit design data is extremely limited,
especially in the public domain. The lack of circuit data has become the
primary bottleneck in developing AI-assisted IC design methods. In this work,
we make the first attempt, SynCircuit, to generate new synthetic circuits with
valid functionalities in the HDL format. SynCircuit automatically generates
synthetic data using a framework with three innovative steps: 1) We propose a
customized diffusion-based generative model to resolve the Directed Cyclic
Graph (DCG) generation task, which has not been well explored in the AI
community. 2) To ensure our circuit is valid, we enforce the circuit
constraints by refining the initial graph generation outputs. 3) The Monte
Carlo tree search (MCTS) method further optimizes the logic redundancy in the
generated graph. Experimental results demonstrate that our proposed SynCircuit
can generate more realistic synthetic circuits and enhance ML model performance
in downstream circuit design tasks.

</details>


### [70] [Mitigating Clinician Information Overload: Generative AI for Integrated EHR and RPM Data Analysis](https://arxiv.org/abs/2509.00073)
*Ankit Shetgaonkar,Dipen Pradhan,Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Aman Raj*

Main category: cs.LG

TL;DR: 本文综述了生成式AI在医疗健康领域的应用，特别是利用大语言模型处理远程患者监测数据和电子健康记录，以缓解临床医生信息过载问题并提高临床效率。


<details>
  <summary>Details</summary>
Motivation: 医疗健康数据（远程患者监测流数据和电子健康记录）的庞大体积和异质性给临床医生带来了信息过载的挑战，需要新技术来有效处理这些复杂数据。

Method: 通过分析GenAI特别是大语言模型的能力，探索其在纵向患者数据导航和临床决策支持方面的应用，使用自然语言对话界面提供可操作的临床见解。

Result: 研究发现GenAI能够增强临床数据导航能力，提供个性化护理支持，但同时也面临数据集成复杂性、数据质量保证、患者隐私保护、AI输出验证等挑战。

Conclusion: 这项工作首次系统总结了GenAI技术在管理临床医生因RPM/EHR数据复杂性导致的信息过载方面的应用，为改善临床工作流程和个性化护理提供了重要见解。

Abstract: Generative Artificial Intelligence (GenAI), particularly Large Language
Models (LLMs), offer powerful capabilities for interpreting the complex data
landscape in healthcare. In this paper, we present a comprehensive overview of
the capabilities, requirements and applications of GenAI for deriving clinical
insights and improving clinical efficiency. We first provide some background on
the forms and sources of patient data, namely real-time Remote Patient
Monitoring (RPM) streams and traditional Electronic Health Records (EHRs). The
sheer volume and heterogeneity of this combined data present significant
challenges to clinicians and contribute to information overload. In addition,
we explore the potential of LLM-powered applications for improving clinical
efficiency. These applications can enhance navigation of longitudinal patient
data and provide actionable clinical decision support through natural language
dialogue. We discuss the opportunities this presents for streamlining clinician
workflows and personalizing care, alongside critical challenges such as data
integration complexity, ensuring data quality and RPM data reliability,
maintaining patient privacy, validating AI outputs for clinical safety,
mitigating bias, and ensuring clinical acceptance. We believe this work
represents the first summarization of GenAI techniques for managing clinician
data overload due to combined RPM / EHR data complexities.

</details>


### [71] [Experimental Assessment of a Multi-Class AI/ML Architecture for Real-Time Characterization of Cyber Events in a Live Research Reactor](https://arxiv.org/abs/2509.00076)
*Zachery Dahm,Konstantinos Vasili,Vasileios Theos,Konstantinos Gkouliaras,William Richards,True Miller,Brian Jowers,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 这篇论文提出了一种多层AI/ML架构，利用核反应堆运行数据来区分网络安全事件和运行异常，在实际场景中验证了AI/ML在核安全领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 核行业对AI/ML应用的需求日益增长，但缺少在真实核反应堆环境中的可行性验证。需要研究AI/ML如何在运行核设施中识别网络安全事件和运行异常。

Method: 开发多层AI/ML架构，整合信息技术(IT)和运营技术(OT)数据流。使用波津大学PUR-1研究反应堆进行实验，包含14种系统状态（1种正常，13种异常）和1380万个多变量数据点。模拟多种网络攻击场景。

Result: AI/ML能够在具有挑战的条件下（如拒绝服务攻击）区分正常、异常和网络安全事件。结合IT和OT数据提高了分类准确性，但在某些网络事件中面临同步和收集挑战。

Conclusion: 研究证明了AI/ML在核网络安全领域的巨大潜力，但同时指出需要进一步精炼处理复杂事件区分和多类架构的技术。

Abstract: There is increased interest in applying Artificial Intelligence and Machine
Learning (AI/ML) within the nuclear industry and nuclear engineering community.
Effective implementation of AI/ML could offer benefits to the nuclear domain,
including enhanced identification of anomalies, anticipation of system
failures, and operational schedule optimization. However, limited work has been
done to investigate the feasibility and applicability of AI/ML tools in a
functioning nuclear reactor. Here, we go beyond the development of a single
model and introduce a multi-layered AI/ML architecture that integrates both
information technology and operational technology data streams to identify,
characterize, and differentiate (i) among diverse cybersecurity events and (ii)
between cyber events and other operational anomalies. Leveraging Purdue
Universitys research reactor, PUR-1, we demonstrate this architecture through a
representative use case that includes multiple concurrent false data injections
and denial-of-service attacks of increasing complexity under realistic reactor
conditions. The use case includes 14 system states (1 normal, 13 abnormal) and
over 13.8 million multi-variate operational and information technology data
points. The study demonstrated the capability of AI/ML to distinguish between
normal, abnormal, and cybersecurity-related events, even under challenging
conditions such as denial-of-service attacks. Combining operational and
information technology data improved classification accuracy but posed
challenges related to synchronization and collection during certain cyber
events. While results indicate significant promise for AI/ML in nuclear
cybersecurity, the findings also highlight the need for further refinement in
handling complex event differentiation and multi-class architectures.

</details>


### [72] [Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models](https://arxiv.org/abs/2509.00083)
*Laksh Patel,Neel Shanbhag*

Main category: cs.LG

TL;DR: 生成式数据地图框架GenDataCarto，通过集成难度分数和记忆分数对训练数据进行分析和处理，有效减少模型过拟合和敏感信息泄漏，保持生成性能。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型存在过拟合和无意记忆稀缺训练示例的风险，这可能被攻击者利用提取敏感信息或虚增测试性能。

Method: 提出GenDataCarto框架：1)为每个预训练样本赋予难度分数（早期损失）和记忆分数（遗忘事件频率）；2)将示例分为四个象限区域；3)按区域进行有针对性的剪枝和权重调整。

Result: 在10%数据剪枝情况下，合成canary提取成功率减少40%以上，验证难以理解度仅增加0.5%。理论证明记忆分数在平滑假设下下界约束经典影响力，降低高记忆热点权重可通过均匀稳定性界约减小泛化间隔。

Conclusion: 结论表明，基于原理的数据干预措施可以在生成性能成本最小的情况下，显著减少信息泄漏风险。

Abstract: Modern generative models risk overfitting and unintentionally memorizing rare
training examples, which can be extracted by adversaries or inflate benchmark
performance. We propose Generative Data Cartography (GenDataCarto), a
data-centric framework that assigns each pretraining sample a difficulty score
(early-epoch loss) and a memorization score (frequency of ``forget events''),
then partitions examples into four quadrants to guide targeted pruning and
up-/down-weighting. We prove that our memorization score lower-bounds classical
influence under smoothness assumptions and that down-weighting
high-memorization hotspots provably decreases the generalization gap via
uniform stability bounds. Empirically, GenDataCarto reduces synthetic canary
extraction success by over 40\% at just 10\% data pruning, while increasing
validation perplexity by less than 0.5\%. These results demonstrate that
principled data interventions can dramatically mitigate leakage with minimal
cost to generative performance.

</details>


### [73] [Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs](https://arxiv.org/abs/2509.00084)
*Qibin Wang,Pu Zhao,Shaohan Huang,Fangkai Yang,Lu Wang,Furu Wei,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.LG

TL;DR: 提出了Generative Self-Refinement (GSR)框架，通过并行生成候选答案并进行自我优化来解决LLM复杂推理问题，无需额外模型即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法如Best-of-N和多数投票依赖于候选答案质量，当所有候选都错误时无法产生正确解，且引入额外模型会增加部署成本。

Method: GSR框架：统一模型并行生成候选答案，然后基于问题和候选答案进行自我优化合成新解决方案。采用混合训练管道，联合优化直接解决问题和优化候选答案两个互补目标。

Result: 在五个数学基准测试中达到最先进性能，学习到的自我优化技能具有模型无关性，在不同模型规模上表现稳健，并能泛化到分布外推理任务。

Conclusion: GSR提供了一种有效的测试时扩展方法，通过自我优化显著提升LLM的复杂推理能力，且部署成本低、泛化能力强。

Abstract: To further enhance the ability of Large Language Models (LLMs) to solve
complex, multi-step reasoning problems, test-time scaling (TTS) methods have
gained widespread attention. Existing approaches such as Best-of-N and majority
voting are limited as their performance depends on the quality of candidate
responses, making them unable to produce a correct solution when all candidates
are incorrect. Introducing an additional model to select the best response also
incurs significant deployment costs. To this end, we introduce Generative
Self-Refinement (GSR), a novel parallel test-time scaling framework where a
unified model first generates a set of candidate responses in parallel and then
performs self-refinement to synthesize a new superior solution based on a
prompt consisting of the problem and these candidates. However, LLMs struggle
to perform refinement effectively when prompted directly. Therefore, we design
a hybrid training pipeline by jointly optimizing for two complementary
objectives, solving problems directly and refining candidate responses.
Experimental results demonstrate that our method achieves state-of-the-art
performance across five mathematical benchmarks. We further show that this
learned self-refinement skill is a model-agnostic enhancement, robust across
different model scales and generalizing to out-of-distribution reasoning tasks.

</details>


### [74] [Centralized vs. Federated Learning for Educational Data Mining: A Comparative Study on Student Performance Prediction with SAEB Microdata](https://arxiv.org/abs/2509.00086)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 本研究评估联邦学习在巴西教育数据隐私保护下的学生表现预测可行性，FedProx算法在200万学生数据上达到61.23%准确率，仅比集中式模型低2.73%，证明其在满足数据保护法规的同时保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 巴西通用数据保护法(LGPD)限制了教育敏感数据的集中化处理，阻碍了数据挖掘和AI在教育个性化学习和风险学生早期识别中的应用，需要寻找隐私保护的计算解决方案。

Method: 使用联邦学习框架中的FedProx算法，在模拟50所学校的环境中训练深度神经网络(DNN)模型，预测巴西基础教育评估系统(SAEB)的学生表现，并与集中式的XGBoost模型进行性能对比。

Result: 集中式XGBoost模型准确率为63.96%，联邦学习模型最高达到61.23%准确率，性能损失仅为2.73%，但提供了强大的隐私保护保障。

Conclusion: 联邦学习是巴西教育背景下构建协作预测模型的可行有效解决方案，能够在遵守LGPD要求的同时保持良好的预测性能。

Abstract: The application of data mining and artificial intelligence in education
offers unprecedented potential for personalizing learning and early
identification of at-risk students. However, the practical use of these
techniques faces a significant barrier in privacy legislation, such as Brazil's
General Data Protection Law (LGPD), which restricts the centralization of
sensitive student data. To resolve this challenge, privacy-preserving
computational approaches are required. The present study evaluates the
feasibility and effectiveness of Federated Learning, specifically the FedProx
algorithm, to predict student performance using microdata from the Brazilian
Basic Education Assessment System (SAEB). A Deep Neural Network (DNN) model was
trained in a federated manner, simulating a scenario with 50 schools, and its
performance was rigorously benchmarked against a centralized eXtreme Gradient
Boosting (XGBoost) model. The analysis, conducted on a universe of over two
million student records, revealed that the centralized model achieved an
accuracy of 63.96%. Remarkably, the federated model reached a peak accuracy of
61.23%, demonstrating a marginal performance loss in exchange for a robust
privacy guarantee. The results indicate that Federated Learning is a viable and
effective solution for building collaborative predictive models in the
Brazilian educational context, in alignment with the requirements of the LGPD.

</details>


### [75] [Yet Unnoticed in LSTM: Binary Tree Based Input Reordering, Weight Regularization, and Gate Nonlinearization](https://arxiv.org/abs/2509.00087)
*Mojtaba Moattari*

Main category: cs.LG

TL;DR: 本文提出三种改进LSTM的方法：输入重排序以优先处理特定索引、通过监督损失函数选择最佳Lp范数进行权重归一化、以及使用小型前馈神经网络对门控进行非线性化，这些方法在文本分类任务中提高了LSTM的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LSTM模型虽然通过门控机制处理长期信息，但未能最优地关注特定旧索引或长期信息，缺乏权重归一化范数选择研究，且门控的非线性化程度不足。

Method: 1) 输入重排序方法优先处理特定输入索引；2) 通过主监督损失函数选择最佳Lp范数和指数进行权重归一化；3) 使用小型前馈神经网络对门控进行非线性化增强。

Result: 在文本分类任务中，与简单LSTM相比，所提出的方法提高了分类准确性。

Conclusion: 通过输入重排序、权重归一化范数选择和门控非线性化三种改进方法，可以有效提升LSTM在文本分类任务中的性能表现。

Abstract: LSTM models used in current Machine Learning literature and applications, has
a promising solution for permitting long term information using gating
mechanisms that forget and reduce effect of current input information. However,
even with this pipeline, they do not optimally focus on specific old index or
long-term information. This paper elaborates upon input reordering approaches
to prioritize certain input indices. Moreover, no LSTM based approach is found
in the literature that examines weight normalization while choosing the right
weight and exponent of Lp norms through main supervised loss function. In this
paper, we find out which norm best finds relationship between weights to either
smooth or sparsify them. Lastly, gates, as weighted representations of inputs
and states, which control reduction-extent of current input versus previous
inputs (~ state), are not nonlinearized enough (through a small FFNN). As
analogous to attention mechanisms, gates easily filter current information to
bold (emphasize on) past inputs. Nonlinearized gates can more easily tune up to
peculiar nonlinearities of specific input in the past. This type of
nonlinearization is not proposed in the literature, to the best of author's
knowledge. The proposed approaches are implemented and compared with a simple
LSTM to understand their performance in text classification tasks. The results
show they improve accuracy of LSTM.

</details>


### [76] [Context-Action Embedding Learning for Off-Policy Evaluation in Contextual Bandits](https://arxiv.org/abs/2509.00648)
*Kushagra Chandak,Vincent Liu,Haanvid Lee*

Main category: cs.LG

TL;DR: 提出了CAEL-MIPS方法，通过学习上下文-动作嵌入来最小化MIPS估计器的均方误差，解决了传统IPS方法在大动作空间或未充分探索区域方差大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的逆倾向得分(IPS)方法在动作空间大或上下文-动作空间未充分探索时方差很大，现有的MIPS方法虽然利用动作嵌入缓解了这个问题，但没有最小化均方误差且未考虑上下文信息。

Method: 提出了CAEL-MIPS方法，从离线数据中学习上下文-动作嵌入，基于MIPS的偏差和方差理论分析，构建了最小化均方误差的目标函数。

Result: 在合成数据集和真实世界数据集上的实验表明，该估计器在均方误差方面优于基线方法。

Conclusion: CAEL-MIPS通过联合学习上下文-动作嵌入有效降低了MIPS估计器的均方误差，在离策略评估任务中表现出色。

Abstract: We consider off-policy evaluation (OPE) in contextual bandits with finite
action space. Inverse Propensity Score (IPS) weighting is a widely used method
for OPE due to its unbiased, but it suffers from significant variance when the
action space is large or when some parts of the context-action space are
underexplored. Recently introduced Marginalized IPS (MIPS) estimators mitigate
this issue by leveraging action embeddings. However, these embeddings do not
minimize the mean squared error (MSE) of the estimators and do not consider
context information. To address these limitations, we introduce Context-Action
Embedding Learning for MIPS, or CAEL-MIPS, which learns context-action
embeddings from offline data to minimize the MSE of the MIPS estimator.
Building on the theoretical analysis of bias and variance of MIPS, we present
an MSE-minimizing objective for CAEL-MIPS. In the empirical studies on a
synthetic dataset and a real-world dataset, we demonstrate that our estimator
outperforms baselines in terms of MSE.

</details>


### [77] [Learning from Peers: Collaborative Ensemble Adversarial Training](https://arxiv.org/abs/2509.00089)
*Li Dengjin,Guo Yanming,Xie Yuxiang,Li Zheng,Chen Jiangming,Li Xiaolong,Lao Mingrui*

Main category: cs.LG

TL;DR: 提出协作集成对抗训练(CEAT)，通过关注子模型间预测差异大的样本来增强集成模型的对抗鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有集成对抗训练方法独立训练子模型，忽略了子模型间的协作优势，特别是分类差异大的样本对集成鲁棒性影响更大

Method: CEAT利用子模型间的概率差异自适应分配样本权重，通过校准距离正则化强调预测差异大的样本在其他子模型训练中的重要性

Result: 在广泛采用的数据集上实验表明，CEAT在竞争性EAT方法中达到最先进性能

Conclusion: CEAT是模型无关的方法，可以无缝适配各种集成方法，具有灵活的适用性

Abstract: Ensemble Adversarial Training (EAT) attempts to enhance the robustness of
models against adversarial attacks by leveraging multiple models. However,
current EAT strategies tend to train the sub-models independently, ignoring the
cooperative benefits between sub-models. Through detailed inspections of the
process of EAT, we find that that samples with classification disparities
between sub-models are close to the decision boundary of ensemble, exerting
greater influence on the robustness of ensemble. To this end, we propose a
novel yet efficient Collaborative Ensemble Adversarial Training (CEAT), to
highlight the cooperative learning among sub-models in the ensemble. To be
specific, samples with larger predictive disparities between the sub-models
will receive greater attention during the adversarial training of the other
sub-models. CEAT leverages the probability disparities to adaptively assign
weights to different samples, by incorporating a calibrating distance
regularization. Extensive experiments on widely-adopted datasets show that our
proposed method achieves the state-of-the-art performance over competitive EAT
methods. It is noteworthy that CEAT is model-agnostic, which can be seamlessly
adapted into various ensemble methods with flexible applicability.

</details>


### [78] [ART: Adaptive Resampling-based Training for Imbalanced Classification](https://arxiv.org/abs/2509.00955)
*Arjun Basandrai,Shourya Jain,K. Ilanthenral*

Main category: cs.LG

TL;DR: 提出自适应重采样训练方法ART，基于类别性能动态调整训练数据分布，在类别不平衡分类任务中显著优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统重采样方法使用固定分布，忽略了类别学习难度的变化，限制了模型整体性能

Method: ART方法定期根据模型的类别性能（使用类别级macro F1分数）更新训练数据分布，在类别级别进行自适应调整

Result: 在多个基准测试中，ART始终优于重采样和算法级方法，平均提高macro F1分数2.64个百分点，统计显著

Conclusion: ART是一种可靠的不平衡分类方法，能够一致地提供最强的macro F1性能，优于现有方法

Abstract: Traditional resampling methods for handling class imbalance typically uses
fixed distributions, undersampling the majority or oversampling the minority.
These static strategies ignore changes in class-wise learning difficulty, which
can limit the overall performance of the model.
  This paper proposes an Adaptive Resampling-based Training (ART) method that
periodically updates the distribution of the training data based on the
class-wise performance of the model. Specifically, ART uses class-wise macro F1
scores, computed at fixed intervals, to determine the degree of resampling to
be performed.
  Unlike instance-level difficulty modeling, which is noisy and
outlier-sensitive, ART adapts at the class level. This allows the model to
incrementally shift its attention towards underperforming classes in a way that
better aligns with the optimization objective.
  Results on diverse benchmarks, including Pima Indians Diabetes and Yeast
dataset demonstrate that ART consistently outperforms both resampling-based and
algorithm-level methods, including Synthetic Minority Oversampling Technique
(SMOTE), NearMiss Undersampling, and Cost-sensitive Learning on binary as well
as multi-class classification tasks with varying degrees of imbalance.
  In most settings, these improvements are statistically significant. On
tabular datasets, gains are significant under paired t-tests and Wilcoxon tests
(p < 0.05), while results on text and image tasks remain favorable. Compared to
training on the original imbalanced data, ART improves macro F1 by an average
of 2.64 percentage points across all tested tabular datasets. Unlike existing
methods, whose performance varies by task, ART consistently delivers the
strongest macro F1, making it a reliable choice for imbalanced classification.

</details>


### [79] [Robust Detection of Synthetic Tabular Data under Schema Variability](https://arxiv.org/abs/2509.00092)
*G. Charbel N. Kindji,Elisa Fromont,Lina Maria Rojas-Barahona,Tanguy Urvoy*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的datum-wise transformer架构，用于检测合成表格数据，在AUC和准确率上比现有基线提高了7个百分点，通过表格自适应组件进一步提升了7个准确率点。


<details>
  <summary>Details</summary>
Motivation: 生成模型的兴起引发了对数据真实性的担忧。虽然图像和文本的检测方法已广泛开发，但表格数据的检测却被忽视，特别是由于表格数据的异构结构和测试时未见格式的挑战。

Method: 引入datum-wise transformer架构，包含表格自适应组件，能够处理具有可变和未见模式的表格数据。

Result: 模型显著优于现有基线，AUC和准确率均提高7个百分点，加入表格自适应组件后准确率再提升7个百分点，证明了在真实条件下检测合成表格数据的可行性。

Conclusion: 这项工作首次提供了强有力的证据，表明在真实世界条件下检测合成表格数据不仅是可行的，而且可以以高可靠性完成。

Abstract: The rise of powerful generative models has sparked concerns over data
authenticity. While detection methods have been extensively developed for
images and text, the case of tabular data, despite its ubiquity, has been
largely overlooked. Yet, detecting synthetic tabular data is especially
challenging due to its heterogeneous structure and unseen formats at test time.
We address the underexplored task of detecting synthetic tabular data in the
wild, where tables have variable and previously unseen schemas. We introduce a
novel datum-wise transformer architecture that significantly outperforms the
only previously published baseline, improving both AUC and accuracy by 7
points. By incorporating a table-adaptation component, our model gains an
additional 7 accuracy points, demonstrating enhanced robustness. This work
provides the first strong evidence that detecting synthetic tabular data in
real-world conditions is not only feasible, but can be done with high
reliability.

</details>


### [80] [CCE: Confidence-Consistency Evaluation for Time Series Anomaly Detection](https://arxiv.org/abs/2509.01098)
*Zhijie Zhong,Zhiwen Yu,Yiu-ming Cheung,Kaixiang Yang*

Main category: cs.LG

TL;DR: 时间序列异常检测评估指标存在识别力不足、超参依赖性强、故障敏感和计算复杂度高等问题。本文提出信心一致性评估(CCE)指标，通过贝叶斯估计量化异常分数的不确定性，构建全局和事件级信心一致性分数，具有严格有界性、Lipschitz稳健性和线性时间复杂度。同时建立RankEval排名评测标准，实现开源。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测评估指标存在四大问题：识别力不足、强超参依赖性、对干扰敏感、高计算复杂度，需要一种更加稳健和高效的评估方法。

Method: 提出信心一致性评估(CCE)指标，利用贝叶斯估计量化异常分数的不确定性。构建全局和事件级判别的信心与一致性分数，组合成简洁的CCE指标。同时建立RankEval排名评测标准化工具。

Result: 理论和实验证明CCE具有严格有界性、Lipschitz稳健性（对分数干扰充分稳健）和线性时间复杂度O(n)。RankEval成为首个标准化可复现的评估指标比较平台。

Conclusion: CCE指标有效解决了现有时间序列异常检测评估指标的四大问题，具有优秀的数学性质和实用性。RankEval标准为评估指标的对比提供了客观基准，两者的开源实现促进了领域发展。

Abstract: Time Series Anomaly Detection metrics serve as crucial tools for model
evaluation. However, existing metrics suffer from several limitations:
insufficient discriminative power, strong hyperparameter dependency,
sensitivity to perturbations, and high computational overhead. This paper
introduces Confidence-Consistency Evaluation (CCE), a novel evaluation metric
that simultaneously measures prediction confidence and uncertainty consistency.
By employing Bayesian estimation to quantify the uncertainty of anomaly scores,
we construct both global and event-level confidence and consistency scores for
model predictions, resulting in a concise CCE metric. Theoretically and
experimentally, we demonstrate that CCE possesses strict boundedness, Lipschitz
robustness against score perturbations, and linear time complexity
$\mathcal{O}(n)$. Furthermore, we establish RankEval, a benchmark for comparing
the ranking capabilities of various metrics. RankEval represents the first
standardized and reproducible evaluation pipeline that enables objective
comparison of evaluation metrics. Both CCE and RankEval implementations are
fully open-source.

</details>


### [81] [Financial Decision Making using Reinforcement Learning with Dirichlet Priors and Quantum-Inspired Genetic Optimization](https://arxiv.org/abs/2509.00095)
*Prasun Nandy,Debjit Dhar,Rik Das*

Main category: cs.LG

TL;DR: 提出混合强化学习框架，结合狄利克雷随机性和量子变异遗传优化，用于苹果公司动态预算分配，实现高精度预测与实际分配高度一致


<details>
  <summary>Details</summary>
Motivation: 传统预算分配模型难以处理现实世界金融数据的随机性和非线性特征，需要更先进的动态优化方法

Method: 使用强化学习代理学习研发与销售管理预算分配，结合狄利克雷分布模拟财务环境变化，采用量子变异遗传算法优化策略避免局部最优

Result: 在未见财务数据上实现与实际分配高度一致（余弦相似度0.9990，KL散度0.0023）

Conclusion: 结合深度强化学习、随机建模和量子启发式算法的混合框架在自适应企业预算分配方面展现出巨大潜力

Abstract: Traditional budget allocation models struggle with the stochastic and
nonlinear nature of real-world financial data. This study proposes a hybrid
reinforcement learning (RL) framework for dynamic budget allocation, enhanced
with Dirichlet-inspired stochasticity and quantum mutation-based genetic
optimization. Using Apple Inc. quarterly financial data (2009 to 2025), the RL
agent learns to allocate budgets between Research and Development and Selling,
General and Administrative to maximize profitability while adhering to
historical spending patterns, with L2 penalties discouraging unrealistic
deviations. A Dirichlet distribution governs state evolution to simulate
shifting financial contexts. To escape local minima and improve generalization,
the trained policy is refined using genetic algorithms with quantum mutation
via parameterized qubit rotation circuits. Generation-wise rewards and
penalties are logged to visualize convergence and policy behavior. On unseen
fiscal data, the model achieves high alignment with actual allocations (cosine
similarity 0.9990, KL divergence 0.0023), demonstrating the promise of
combining deep RL, stochastic modeling, and quantum-inspired heuristics for
adaptive enterprise budgeting.

</details>


### [82] [Nonlinear Performative Prediction](https://arxiv.org/abs/2509.01139)
*Guangzheng Zhong,Yang Liu,Jiming Liu*

Main category: cs.LG

TL;DR: 本文提出了一种将表演性预测推广到非线性情况的新方法，通过最大间隔方法和核方法扩展理论框架，并开发了保证表演性稳定性的算法。


<details>
  <summary>Details</summary>
Motivation: 现有的表演性预测研究主要依赖不可控假设（如梯度有界）并局限于线性情况，而现实世界数据通常具有复杂的非线性特征，需要更通用的理论框架。

Method: 使用最大间隔方法构建表演性预测的损失函数，通过核方法扩展到非线性空间，采用预测误差差异量化数据分布偏移，并推导表演性稳定性的条件。

Result: 在合成和真实数据集上的实验表明，该方法在线性和非线性数据分布下都优于现有基线方法，验证了理论框架的有效性。

Conclusion: 该方法成功地将表演性预测推广到非线性情况，提供了理论保证和实际算法，为处理现实世界中的复杂数据分布偏移问题提供了有效解决方案。

Abstract: Performative prediction is an emerging paradigm in machine learning that
addresses scenarios where the model's prediction may induce a shift in the
distribution of the data it aims to predict. Current works in this field often
rely on uncontrollable assumptions, such as bounded gradients of performative
loss, and primarily focus on linear cases in their examples and evaluations to
maintain consistency between theoretical guarantees and empirical validations.
However, such linearity rarely holds in real-world applications, where the data
usually exhibit complex nonlinear characteristics. In this paper, we relax
these out-of-control assumptions and present a novel design that generalizes
performative prediction to nonlinear cases while preserving essential
theoretical properties. Specifically, we formulate the loss function of
performative prediction using a maximum margin approach and extend it to
nonlinear spaces through kernel methods. To quantify the data distribution
shift, we employ the discrepancy between prediction errors on these two
distributions as an indicator, which characterizes the impact of the
performative effect on specific learning tasks. By doing so, we can derive, for
both linear and nonlinear cases, the conditions for performative stability, a
critical and desirable property in performative contexts. Building on these
theoretical insights, we develop an algorithm that guarantees the performative
stability of the predictive model. We validate the effectiveness of our method
through experiments on synthetic and real-world datasets with both linear and
nonlinear data distributions, demonstrating superior performance compared to
state-of-the-art baselines.

</details>


### [83] [Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning LLMs](https://arxiv.org/abs/2509.00096)
*Yao Fu,Runchao Li,Xianxuan Long,Haotian Yu,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.LG

TL;DR: 该论文揭示了神经网络剪枝会破坏LLM内部用于谎言检测的关键激活特征，提出了TPLO方法在剪枝时重点关注具有更多激活异常值和更强判别特征的层，从而在保持模型性能的同时保留谎言检测能力。


<details>
  <summary>Details</summary>
Motivation: 发现神经网络剪枝会破坏LLM内部对谎言检测至关重要的激活特征，这提出了一个重要问题：如何在剪枝LLM时不牺牲这些关键的谎言检测能力？

Method: 提出Truthful Pruning aligned by Layer-wise Outliers (TPLO)方法，重点关注具有更多激活异常值和更强判别特征的层；同时引入提示规则来丰富TruthfulQA基准以更好地校准LLM剪枝。

Result: 实验结果显示，该方法在50%稀疏度下实现了88%的幻觉检测准确率，并提升了剪枝后LLM在TruthfulQA基准上的性能。

Conclusion: TPLO方法能够有效解决剪枝过程中谎言检测能力下降的问题，在保持模型原始性能的同时保留了内部状态的关键特征，为在资源受限场景下部署可信赖的LLM提供了可行方案。

Abstract: Neural network pruning has emerged as a promising approach for deploying LLMs
in low-resource scenarios while preserving downstream task performance.
However, for the first time, we reveal that such pruning disrupts LLMs'
internal activation features crucial for lie detection, where probing
classifiers (typically small logistic regression models) trained on these
features assess the truthfulness of LLM-generated statements. This discovery
raises a crucial open question: how can we prune LLMs without sacrificing these
critical lie detection capabilities? Our investigation further reveals that
naively adjusting layer-wise pruning sparsity based on importance inadvertently
removes crucial weights, failing to improve lie detection performance despite
its reliance on the most crucial LLM layer. To address this issue, we propose
Truthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater
emphasis on layers with more activation outliers and stronger discriminative
features simultaneously. This preserves LLMs' original performance while
retaining critical features of inner states needed for robust lie detection.
Moreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for
better calibrating LLM pruning. Empirical results show that our approach
improves the hallucination detection for pruned LLMs (achieving 88% accuracy at
50% sparsity) and enhances their performance on TruthfulQA.

</details>


### [84] [ADMP-GNN: Adaptive Depth Message Passing GNN](https://arxiv.org/abs/2509.01170)
*Yassine Abbahaddou,Fragkiskos D. Malliaros,Johannes F. Lutzeyer,Michalis Vazirgiannis*

Main category: cs.LG

TL;DR: 提出ADMP-GNN框架，动态调整每个节点的消息传递层数，提升图神经网络性能


<details>
  <summary>Details</summary>
Motivation: 传统GNN对所有节点使用固定层数的消息传递，但实际不同节点需要不同深度的计算，实证分析显示最优层数因节点特性而异

Method: 提出自适应深度消息传递GNN（ADMP-GNN），为每个节点动态调整消息传递层数，适用于所有遵循消息传递方案的模型

Result: 在节点分类任务上评估，相比基线GNN模型表现出性能提升

Conclusion: 动态调整消息传递层数的自适应方法能有效提升GNN性能，解决了固定层数无法适应节点多样性需求的问题

Abstract: Graph Neural Networks (GNNs) have proven to be highly effective in various
graph learning tasks. A key characteristic of GNNs is their use of a fixed
number of message-passing steps for all nodes in the graph, regardless of each
node's diverse computational needs and characteristics. Through empirical
real-world data analysis, we demonstrate that the optimal number of
message-passing layers varies for nodes with different characteristics. This
finding is further supported by experiments conducted on synthetic datasets. To
address this, we propose Adaptive Depth Message Passing GNN (ADMP-GNN), a novel
framework that dynamically adjusts the number of message passing layers for
each node, resulting in improved performance. This approach applies to any
model that follows the message passing scheme. We evaluate ADMP-GNN on the node
classification task and observe performance improvements over baseline GNN
models.

</details>


### [85] [Progressive Element-wise Gradient Estimation for Neural Network Quantization](https://arxiv.org/abs/2509.00097)
*Kaiqi Zhao*

Main category: cs.LG

TL;DR: 提出了PEGE方法替代STE，通过渐进式元素级梯度估计和混合精度替换策略，在量化训练中同时优化任务损失和离散化误差，显著提升低比特量化模型的精度。


<details>
  <summary>Details</summary>
Motivation: 传统STE方法忽略了离散化误差，导致低比特量化时精度下降严重，需要更有效的梯度估计方法来改善量化感知训练。

Method: 提出PEGE方法：1）使用对数课程驱动的混合精度替换策略渐进替换全精度参数；2）将QAT建模为同时最小化任务损失和离散化误差的协同优化问题。

Result: 在CIFAR-10和ImageNet数据集上，PEGE在各种架构（ResNet、VGG等）上均优于现有方法，使低精度模型达到甚至超过全精度模型的精度。

Conclusion: PEGE提供了一个统一且通用的框架，通过渐进式梯度估计有效解决了量化训练中的离散化误差问题，显著提升了低比特量化性能。

Abstract: Neural network quantization aims to reduce the bit-widths of weights and
activations, making it a critical technique for deploying deep neural networks
on resource-constrained hardware. Most Quantization-Aware Training (QAT)
methods rely on the Straight-Through Estimator (STE) to address the
non-differentiability of discretization functions by replacing their
derivatives with that of the identity function. While effective, STE overlooks
discretization errors between continuous and quantized values, which can lead
to accuracy degradation -- especially at extremely low bit-widths. In this
paper, we propose Progressive Element-wise Gradient Estimation (PEGE), a simple
yet effective alternative to STE, which can be seamlessly integrated with any
forward propagation methods and improves the quantized model accuracy. PEGE
progressively replaces full-precision weights and activations with their
quantized counterparts via a novel logarithmic curriculum-driven
mixed-precision replacement strategy. Then it formulates QAT as a
co-optimization problem that simultaneously minimizes the task loss for
prediction and the discretization error for quantization, providing a unified
and generalizable framework. Extensive experiments on CIFAR-10 and ImageNet
across various architectures (e.g., ResNet, VGG) demonstrate that PEGE
consistently outperforms existing backpropagation methods and enables
low-precision models to match or even outperform the accuracy of their
full-precision counterparts.

</details>


### [86] [Effects of Distributional Biases on Gradient-Based Causal Discovery in the Bivariate Categorical Case](https://arxiv.org/abs/2509.01621)
*Tim Schwabe,Moritz Lange,Laurenz Wiskott,Maribel Acosta*

Main category: cs.LG

TL;DR: 梯度因果发现方法易受数据分布偏差影响，本文识别了两种偏差：边际分布不对称性和边际分布偏移不对称性，并通过实验展示了这些偏差如何影响因果学习，同时提出了控制方法。


<details>
  <summary>Details</summary>
Motivation: 梯度因果发现方法在处理数据时容易受到分布偏差的影响，这可能导致因果结构学习的偏差。本文旨在识别和分析这些偏差，并探索如何控制它们以提高方法的稳健性。

Method: 使用双变量分类设置和Dirichlet先验，在合成数据中模拟两种分布偏差。通过两个简单模型（学习边际或条件数据分布）来评估梯度因果发现方法对这些偏差的敏感性，并测试消除因果分解竞争的方法。

Result: 实验表明梯度因果发现方法确实容易受到边际分布不对称性和边际分布偏移不对称性的影响。通过消除可能因果分解之间的竞争，可以使模型对这些偏差具有稳健性。

Conclusion: 梯度因果发现方法需要特别注意数据分布偏差的影响。通过适当的模型设计和偏差控制策略，可以提高这些方法在实际应用中的可靠性和准确性。

Abstract: Gradient-based causal discovery shows great potential for deducing causal
structure from data in an efficient and scalable way. Those approaches however
can be susceptible to distributional biases in the data they are trained on. We
identify two such biases: Marginal Distribution Asymmetry, where differences in
entropy skew causal learning toward certain factorizations, and Marginal
Distribution Shift Asymmetry, where repeated interventions cause faster shifts
in some variables than in others. For the bivariate categorical setup with
Dirichlet priors, we illustrate how these biases can occur even in controlled
synthetic data. To examine their impact on gradient-based methods, we employ
two simple models that derive causal factorizations by learning marginal or
conditional data distributions - a common strategy in gradient-based causal
discovery. We demonstrate how these models can be susceptible to both biases.
We additionally show how the biases can be controlled. An empirical evaluation
of two related, existing approaches indicates that eliminating competition
between possible causal factorizations can make models robust to the presented
biases.

</details>


### [87] [LLM-QUBO: An End-to-End Framework for Automated QUBO Transformation from Natural Language Problem Descriptions](https://arxiv.org/abs/2509.00099)
*Huixiang Zhang,Mahzabeen Emu,Salimur Choudhury*

Main category: cs.LG

TL;DR: 提出了LLM-QUBO框架，使用大语言模型自动将自然语言优化问题转换为QUBO格式，并结合混合量子经典Benders分解方法解决量子硬件扩展性问题


<details>
  <summary>Details</summary>
Motivation: 量子退火在解决NP难组合优化问题方面具有潜力，但面临两个主要挑战：手动将问题描述转换为QUBO格式的复杂性，以及当前量子硬件的可扩展性限制

Method: 开发端到端框架LLM-QUBO，利用大语言模型解析自然语言并自动生成结构化数学表示；集成混合量子经典Benders分解方法，将问题分区处理，组合复杂主问题编译为紧凑QUBO格式，线性结构子问题委托给经典求解器

Result: 通过经典求解器验证了生成QUBO的正确性和混合方法的可扩展性，建立了稳健的性能基准，证明框架已准备好用于量子硬件

Conclusion: 该工作提出了一个连接经典AI和量子计算的协同计算范式，解决了优化问题实际应用中的关键挑战，自动化工作流程显著降低了使用门槛，为将量子设备转变为大规模实际优化挑战的可访问加速器提供了可行途径

Abstract: Quantum annealing offers a promising paradigm for solving NP-hard
combinatorial optimization problems, but its practical application is severely
hindered by two challenges: the complex, manual process of translating problem
descriptions into the requisite Quadratic Unconstrained Binary Optimization
(QUBO) format and the scalability limitations of current quantum hardware. To
address these obstacles, we propose a novel end-to-end framework, LLM-QUBO,
that automates this entire formulation-to-solution pipeline. Our system
leverages a Large Language Model (LLM) to parse natural language, automatically
generating a structured mathematical representation. To overcome hardware
limitations, we integrate a hybrid quantum-classical Benders' decomposition
method. This approach partitions the problem, compiling the combinatorial
complex master problem into a compact QUBO format, while delegating linearly
structured sub-problems to classical solvers. The correctness of the generated
QUBO and the scalability of the hybrid approach are validated using classical
solvers, establishing a robust performance baseline and demonstrating the
framework's readiness for quantum hardware. Our primary contribution is a
synergistic computing paradigm that bridges classical AI and quantum computing,
addressing key challenges in the practical application of optimization problem.
This automated workflow significantly reduces the barrier to entry, providing a
viable pathway to transform quantum devices into accessible accelerators for
large-scale, real-world optimization challenges.

</details>


### [88] [Efficient Transformer-Inspired Variants of Physics-Informed Deep Operator Networks](https://arxiv.org/abs/2509.01679)
*Zhi-Feng Wei,Wenqian Chen,Panos Stinis*

Main category: cs.LG

TL;DR: 基于Transformer的DeepONet变体，通过分支与干网络间的双向交叉条件化，在保持简单性和效率的同时提高了准确性，而且不同变体适合不同类型的偏微分方程。


<details>
  <summary>Details</summary>
Motivation: 解决DeepONet中"普通版"与"修改版"之间的准确性和训练效率的交换问题，尝试在保持简单性的同时提高性能。

Method: 受Transformer启发，在DeepONet中引入分支网络和干网络之间的双向交叉条件化，让查询点信息注入分支网络，输入函数信息注入干网络，实现动态依赖关系。

Result: 在四个PDE基准测试中（平流、扩散-反应、Burgers、KdV方程），每个情况都有变体能够达到或超越修改版DeepONet的准确性，同时提高训练效率。不同方程的最佳变体与方程的本质特性相符。

Conclusion: 交叉条件化的效果依赖于偏微分方程的特性和物理机制，新提出的Transformer风格DeepONet变体在保持简单性和效率的同时显著提高了性能。

Abstract: Operator learning has emerged as a promising tool for accelerating the
solution of partial differential equations (PDEs). The Deep Operator Networks
(DeepONets) represent a pioneering framework in this area: the "vanilla"
DeepONet is valued for its simplicity and efficiency, while the modified
DeepONet achieves higher accuracy at the cost of increased training time. In
this work, we propose a series of Transformer-inspired DeepONet variants that
introduce bidirectional cross-conditioning between the branch and trunk
networks in DeepONet. Query-point information is injected into the branch
network and input-function information into the trunk network, enabling dynamic
dependencies while preserving the simplicity and efficiency of the "vanilla"
DeepONet in a non-intrusive manner. Experiments on four PDE benchmarks --
advection, diffusion-reaction, Burgers', and Korteweg-de Vries equations --
show that for each case, there exists a variant that matches or surpasses the
accuracy of the modified DeepONet while offering improved training efficiency.
Moreover, the best-performing variant for each equation aligns naturally with
the equation's underlying characteristics, suggesting that the effectiveness of
cross-conditioning depends on the characteristics of the equation and its
underlying physics. To ensure robustness, we validate the effectiveness of our
variants through a range of rigorous statistical analyses, among them the
Wilcoxon Two One-Sided Test, Glass's Delta, and Spearman's rank correlation.

</details>


### [89] [Exploiting a Mixture-of-Layers in an Electrocardiography Foundation Model](https://arxiv.org/abs/2509.00102)
*Phu X. Nguyen,Huy Phan,Hieu Pham,Christos Chatzichristos,Bert Vandenberk,Maarten De Vos*

Main category: cs.LG

TL;DR: 提出PMA方法，通过门控网络融合Transformer各层表示，提升ECG基础模型在下游任务的性能


<details>
  <summary>Details</summary>
Motivation: 现有ECG Transformer模型仅使用最后一层表示，但理论和实证分析表明这并非最优，需要充分利用各层表示的多样性

Method: 1D ViT预训练后，在下游任务中使用门控网络选择性融合各层表示；扩展方法在预训练阶段通过分组平均聚合表示

Result: PMA方法增强了表示能力，提高了下游应用性能

Conclusion: 充分利用Transformer各层表示多样性可以显著提升ECG基础模型的性能，PMA提供了一种有效的层间表示融合方案

Abstract: Transformer-based foundation models for Electrocardiograms (ECGs) have
recently achieved impressive performance in many downstream applications.
However, the internal representations of such models across layers have not
been fully understood and exploited. An important question arises: Does the
final layer of the pre-trained Transformer model, the \emph{de facto}
representational layer, provide optimal performance for downstream tasks?
Although our answer based on empirical and theoretical analyses for this
question is negative, we propose a novel approach to leverage the
representation diversity of the model's layers effectively. Specifically, we
introduce a novel architecture called Post-pretraining Mixture-of-layers
Aggregation (PMA), which enables a flexible combination of the layer-wise
representations from the layer stack of a Transformer-based foundation model.
We first pre-train the model from ECG signals using the 1-dimensional Vision
Transformer (ViT) via masked modeling. In downstream applications, instead of
relying solely on the last layer of the model, we employ a gating network to
selectively fuse the representations from the pretrained model's layers,
thereby enhancing representation power and improving performance of the
downstream applications. In addition, we extend the proposed method to the
pretraining stage by aggregating all representations through group-wise
averaging before feeding them into the decoder-based Transformer.

</details>


### [90] [Bouncy particle sampler with infinite exchanging parallel tempering](https://arxiv.org/abs/2509.02003)
*Yohei Saito,Shun Kimura,Koujin Takeda*

Main category: cs.LG

TL;DR: 本文提出了一种结合弹性粒子采样器(BPS)和平行回火(PT)的新算法，用于加速后验分布的收敛，特别针对多峰分布问题。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断需要近似后验分布，传统采样方法如HMC和MCMC存在参数设置困难的问题。BPS虽然参数设置更简单，但在多峰分布中收敛较慢。

Method: 将平行回火(PT)技术引入弹性粒子采样器(BPS)，并提出了逆温度交换率为无穷大时的算法设计。

Result: 通过数值模拟验证了该算法在多峰分布中的有效性，加速了后验分布的收敛。

Conclusion: 提出的BPS-PT组合算法在多峰分布问题上表现出色，为贝叶斯推断提供了更有效的采样方法。

Abstract: Bayesian inference is useful to obtain a predictive distribution with a small
generalization error. However, since posterior distributions are rarely
evaluated analytically, we employ the variational Bayesian inference or
sampling method to approximate posterior distributions. When we obtain samples
from a posterior distribution, Hamiltonian Monte Carlo (HMC) has been widely
used for the continuous variable part and Markov chain Monte Carlo (MCMC) for
the discrete variable part. Another sampling method, the bouncy particle
sampler (BPS), has been proposed, which combines uniform linear motion and
stochastic reflection to perform sampling. BPS was reported to have the
advantage of being easier to set simulation parameters than HMC. To accelerate
the convergence to a posterior distribution, we introduced parallel tempering
(PT) to BPS, and then proposed an algorithm when the inverse temperature
exchange rate is set to infinity. We performed numerical simulations and
demonstrated its effectiveness for multimodal distribution.

</details>


### [91] [Pre-trained knowledge elevates large language models beyond traditional chemical reaction optimizers](https://arxiv.org/abs/2509.00103)
*Robert MacKnight,Jose Emilio Regio,Jeffrey G. Ethier,Luke A. Baldwin,Gabe Gomes*

Main category: cs.LG

TL;DR: 大语言模型在化学实验优化中超越伪司优化，特别是在复杂分类参数空间和高性能条件稀缺的情况下


<details>
  <summary>Details</summary>
Motivation: 探索领先大语言模型的领域知识如何改变传统黑盒参数优化的范式

Method: 使用6个完全枚举的分类反应数据集（768-5684个实验），对比LLM指导优化、贝叶斯优化和随机采样的性能，并使用信息论框架分析采样多样性

Result: LLM在5个单目标数据集中均能比或超过贝叶斯优化，优势随参数复杂性增加而扩大，保持更高的探索熵但仍获得更好性能

Conclusion: LLM指导优化在需要领域知识而非纯数学优化的复杂分类空间中表现优异，领域知识能够更有效导航化学参数空间

Abstract: Modern optimization in experimental chemistry employs algorithmic search
through black-box parameter spaces. Here we demonstrate that pre-trained
knowledge in large language models (LLMs) fundamentally changes this paradigm.
Using six fully enumerated categorical reaction datasets (768 - 5,684
experiments), we benchmark LLM-guided optimization (LLM-GO) against Bayesian
optimization (BO) and random sampling. Frontier LLMs consistently match or
exceed BO performance across five single-objective datasets, with advantages
growing as parameter complexity increases and high-performing conditions become
scarce (<5% of space). BO retains superiority only for explicit multi-objective
trade-offs. To understand these contrasting behaviors, we introduce a
topology-agnostic information theory framework quantifying sampling diversity
throughout optimization campaigns. This analysis reveals that LLMs maintain
systematically higher exploration entropy than BO across all datasets while
achieving superior performance, with advantages most pronounced in
solution-scarce parameter spaces where high-entropy exploration typically fails
- suggesting that pre-trained domain knowledge enables more effective
navigation of chemical parameter space rather than replacing structured
exploration strategies. To enable transparent benchmarking and community
validation, we release Iron Mind (https://gomes.andrew.cmu.edu/iron-mind), a
no-code platform for side-by-side evaluation of human, algorithmic, and LLM
optimization campaigns with public leaderboards and complete trajectories. Our
findings establish that LLM-GO excels precisely where traditional methods
struggle: complex categorical spaces requiring domain understanding rather than
mathematical optimization.

</details>


### [92] [Fantastic Pretraining Optimizers and Where to Find Them](https://arxiv.org/abs/2509.02046)
*Kaiyue Wen,David Hall,Tengyu Ma,Percy Liang*

Main category: cs.LG

TL;DR: 这篇论文通过系统性研究发现，迈向矩阵预处理器的优化器在小模型上有显著速度提升，但在大模型上优势减弱，且之前的对比研究存在调参不公平和评估方法问题。


<details>
  <summary>Details</summary>
Motivation: 虽然有许多研究声称替代优化器能提供1.4-2倍速度提升，但AdamW仍在语言模型预训练中占主导地位。论文认为两个方法论问题导致了不公平对比和实践采用困难：(i)不平等的超参数调整(ii)有限或误导的评估方案。

Method: 对十种深度学习优化器进行系统性研究，涉及四个模型规模（0.1B-1.2B参数）和数据-模型比率（1-8倍Chinchilla最佳比），重点关注调参平等性和训练结束时的评估。

Result: 发现：1)不同优化器的最佳超参数不同，盲目转移超参不公平；2)替代优化器的实际速度提升比声称的低，且随模型规模增大而减少（从0.1B模型的1.4倍降到1.2B模型的1.1倍）；3)训练中期比较可能误导，因学习率衰减导致排名变化。

Conclusion: 所有最快的优化器（如Muon和Soap）都使用矩阵作为预处理器，但矩阵基优化器的速度提升与模型规模成反比，在大模型上优势很小。公平对比需要严格的超参数调整和在不同模型规模、数据比上的完整评估。

Abstract: AdamW has long been the dominant optimizer in language model pretraining,
despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We
posit that two methodological shortcomings have obscured fair comparisons and
hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited
or misleading evaluation setups. To address these two issues, we conduct a
systematic study of ten deep learning optimizers across four model scales
(0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum).
We find that fair and informative comparisons require rigorous hyperparameter
tuning and evaluations across a range of model scales and data-to-model ratios,
performed at the end of training. First, optimal hyperparameters for one
optimizer may be suboptimal for another, making blind hyperparameter transfer
unfair. Second, the actual speedup of many proposed optimizers over well-tuned
baselines is lower than claimed and decreases with model size to only 1.1x for
1.2B parameter models. Thirdly, comparing intermediate checkpoints before
reaching the target training budgets can be misleading, as rankings between two
optimizers can flip during training due to learning rate decay. Through our
thorough investigation, we find that all the fastest optimizers such as Muon
and Soap, use matrices as preconditioners -- multiplying gradients with
matrices rather than entry-wise scalars. However, the speedup of matrix-based
optimizers is inversely proportional to model scale, decreasing from 1.4x over
AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.

</details>


### [93] [Principled Approximation Methods for Efficient and Scalable Deep Learning](https://arxiv.org/abs/2509.00174)
*Pedro Savarese*

Main category: cs.LG

TL;DR: 该论文研究深度学习的效率优化方法，主要关注模型压缩、架构设计和优化算法三个方向，通过可微近似方法解决离散约束问题，显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模不断增大，计算和能耗需求急剧增长，限制了深度学习技术的部署和广泛应用，需要研究高效的近似方法来提升系统效率。

Method: 提出三种主要方法：1）模型压缩方面，将剪枝和量化的离散问题转化为连续可微问题，实现梯度训练；2）架构设计方面，利用参数共享探索隐式循环架构；3）优化算法方面，重新审视理论特性并提出自适应优化器。

Result: 在图像分类、语言建模和生成建模任务上的实验结果表明，所提方法在保持甚至提升模型性能的同时，显著提高了训练和推理效率。

Conclusion: 通过可扩展且原则性的近似方法成功解决了计算难题，为深度学习系统的高效部署提供了有效解决方案。

Abstract: Recent progress in deep learning has been driven by increasingly larger
models. However, their computational and energy demands have grown
proportionally, creating significant barriers to their deployment and to a
wider adoption of deep learning technologies. This thesis investigates
principled approximation methods for improving the efficiency of deep learning
systems, with a particular focus on settings that involve discrete constraints
and non-differentiability.
  We study three main approaches toward improved efficiency: architecture
design, model compression, and optimization. For model compression, we propose
novel approximations for pruning and quantization that frame the underlying
discrete problem as continuous and differentiable, enabling gradient-based
training of compression schemes alongside the model's parameters. These
approximations allow for fine-grained sparsity and precision configurations,
leading to highly compact models without significant fine-tuning. In the
context of architecture design, we design an algorithm for neural architecture
search that leverages parameter sharing across layers to efficiently explore
implicitly recurrent architectures. Finally, we study adaptive optimization,
revisiting theoretical properties of widely used methods and proposing an
adaptive optimizer that allows for quick hyperparameter tuning.
  Our contributions center on tackling computationally hard problems via
scalable and principled approximations. Experimental results on image
classification, language modeling, and generative modeling tasks show that the
proposed methods provide significant improvements in terms of training and
inference efficiency while maintaining, or even improving, the model's
performance.

</details>


### [94] [Differentiable Expectation-Maximisation and Applications to Gaussian Mixture Model Optimal Transport](https://arxiv.org/abs/2509.02109)
*Samuel Boïté,Eloi Tanguy,Julie Delon,Agnès Desolneux,Rémi Flamary*

Main category: cs.LG

TL;DR: 本文提出了EM算法的可微分版本，使其能够集成到需要端到端梯度传播的现代学习流程中，并应用于高斯混合模型的Wasserstein距离计算。


<details>
  <summary>Details</summary>
Motivation: EM算法在统计学和机器学习中广泛应用，但通常被视为不可微分的黑盒，无法集成到需要端到端梯度传播的现代学习流程中。

Method: 提出并比较了多种EM算法的微分策略，包括完全自动微分和近似方法，评估其准确性和计算效率。将可微分EM应用于高斯混合模型的Wasserstein距离计算。

Result: 开发了可微分的MW₂距离，可作为可微分损失函数应用于成像和机器学习任务。提出了新的稳定性理论结果和不平衡变体。

Conclusion: 数值实验在重心计算、颜色和风格迁移、图像生成和纹理合成等任务中证明了所提方法的有效性和多功能性。

Abstract: The Expectation-Maximisation (EM) algorithm is a central tool in statistics
and machine learning, widely used for latent-variable models such as Gaussian
Mixture Models (GMMs). Despite its ubiquity, EM is typically treated as a
non-differentiable black box, preventing its integration into modern learning
pipelines where end-to-end gradient propagation is essential. In this work, we
present and compare several differentiation strategies for EM, from full
automatic differentiation to approximate methods, assessing their accuracy and
computational efficiency. As a key application, we leverage this differentiable
EM in the computation of the Mixture Wasserstein distance $\mathrm{MW}_2$
between GMMs, allowing $\mathrm{MW}_2$ to be used as a differentiable loss in
imaging and machine learning tasks. To complement our practical use of
$\mathrm{MW}_2$, we contribute a novel stability result which provides
theoretical justification for the use of $\mathrm{MW}_2$ with EM, and also
introduce a novel unbalanced variant of $\mathrm{MW}_2$. Numerical experiments
on barycentre computation, colour and style transfer, image generation, and
texture synthesis illustrate the versatility and effectiveness of the proposed
approach in different settings.

</details>


### [95] [FNODE: Flow-Matching for data-driven simulation of constrained multibody systems](https://arxiv.org/abs/2509.00183)
*Hongyu Wang,Jingquan Wang,Dan Negrut*

Main category: cs.LG

TL;DR: FNODE框架通过直接学习加速度向量场，避免了传统神经ODE的反向传播瓶颈，在计算效率和长期预测精度上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决多体系统数据驱动建模中的高计算成本和有限长期预测精度两大挑战

Method: 提出Flow-Matching神经ODE框架，通过监督加速度而非积分状态来训练，使用FFT和有限差分混合方案高效计算加速度目标

Result: 在多个基准测试中（质量-弹簧-阻尼器系统、双摆、滑块曲柄、车杆系统）均优于MBD-NODE、LSTM和FCNN等方法

Conclusion: FNODE在准确性、泛化能力和计算效率方面表现优异，为约束多体系统的数据驱动建模提供了有效解决方案

Abstract: Data-driven modeling of constrained multibody systems faces two persistent
challenges: high computational cost and limited long-term prediction accuracy.
To address these issues, we introduce the Flow-Matching Neural Ordinary
Differential Equation (FNODE), a framework that learns acceleration vector
fields directly from trajectory data. By reformulating the training objective
to supervise accelerations rather than integrated states, FNODE eliminates the
need for backpropagation through an ODE solver, which represents a bottleneck
in traditional Neural ODEs. Acceleration targets are computed efficiently using
numerical differentiation techniques, including a hybrid Fast Fourier Transform
(FFT) and Finite Difference (FD) scheme. We evaluate FNODE on a diverse set of
benchmarks, including the single and triple mass-spring-damper systems, double
pendulum, slider-crank, and cart-pole. Across all cases, FNODE consistently
outperforms existing approaches such as Multi-Body Dynamic Neural ODE
(MBD-NODE), Long Short-Term Memory (LSTM) networks, and Fully Connected Neural
Networks (FCNN), demonstrating good accuracy, generalization, and computational
efficiency.

</details>


### [96] [Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation](https://arxiv.org/abs/2509.02154)
*Aymene Mohammed Bouayed,Samuel Deslauriers-Gauthier,Adrian Iaccovelli,David Naccache*

Main category: cs.LG

TL;DR: Conditional-$t^3$VAE通过为每个类别定义Student's t联合先验分布，解决了VAE在类别不平衡数据集上潜在空间分配不均的问题，显著提高了生成公平性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统VAE及其变体在类别不平衡数据集上，潜在空间分配与训练集类别频率成正比，导致尾部类别代表性不足，生成公平性降低。

Method: 提出Conditional-$t^3$VAE，为每个类别定义Student's t联合先验分布，防止多数类别主导潜在空间，并使用γ-散度推导的闭式目标进行优化。

Result: 在SVHN-LT、CIFAR100-LT和CelebA数据集上，Conditional-$t^3$VAE在严重类别不平衡情况下FID分数显著低于基准模型，在每类F1评估中也优于条件高斯VAE。

Conclusion: 该方法在极端不平衡情况下大幅提高了生成公平性和多样性，而高斯模型在轻度不平衡情况下仍具有竞争力。

Abstract: Variational Autoencoders (VAEs) with global priors mirror the training set's
class frequency in latent space, underrepresenting tail classes and reducing
generative fairness on imbalanced datasets. While $t^3$VAE improves robustness
via heavy-tailed Student's t-distribution priors, it still allocates latent
volume proportionally to the class frequency.In this work, we address this
issue by explicitly enforcing equitable latent space allocation across classes.
To this end, we propose Conditional-$t^3$VAE, which defines a per-class
\mbox{Student's t} joint prior over latent and output variables, preventing
dominance by majority classes. Our model is optimized using a closed-form
objective derived from the $\gamma$-power divergence. Moreover, for
class-balanced generation, we derive an equal-weight latent mixture of
Student's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA,
Conditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE
and Gaussian-based VAE baselines, particularly under severe class imbalance. In
per-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional
Gaussian VAE across all highly imbalanced settings. While Gaussian-based models
remain competitive under mild imbalance ratio ($\rho \lesssim 3$), our approach
substantially improves generative fairness and diversity in more extreme
regimes.

</details>


### [97] [Democratizing Agentic AI with Fast Test-Time Scaling on the Edge](https://arxiv.org/abs/2509.00195)
*Hao Mark Chen,Zhiwen Mo,Guanxi Lu,Shuang Liang,Lingxiao Ma,Wayne Luk,Hongxiang Fan*

Main category: cs.LG

TL;DR: FlashTTS是一个针对边缘设备的测试时缩放(TTS)服务系统，通过三项优化技术使小型LLM在内存受限环境下达到大型云模型的性能水平


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署AI代理需要隐私保护和快速响应，但内存限制迫使使用推理能力较差的小型LLM，现有TTS方法在边缘硬件上开销过大

Method: 引入三项协同优化：推测性波束扩展处理不规则推理路径、非对称多模型内存分配动态平衡内存使用、动态前缀感知调度最大化KV缓存重用

Result: 在单块消费级GPU(24GB)上，FlashTTS实现平均2.2倍吞吐量提升，延迟降低38%-68%，使边缘LLM达到大型云模型的准确性和延迟水平

Conclusion: FlashTTS为边缘设备上高性能AI代理的普及铺平了道路，通过vLLM插件库形式提供即插即用解决方案

Abstract: Deploying agentic AI on edge devices is crucial for privacy and
responsiveness, but memory constraints typically relegate these systems to
smaller Large Language Models (LLMs) with inferior reasoning capabilities.
Test-Time Scaling (TTS) can bridge this reasoning gap by dedicating more
compute during inference, but existing methods incur prohibitive overhead on
edge hardware. To overcome this, we introduce FlashTTS, a serving system that
makes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces
three synergistic optimizations: (i) Speculative Beam Extension to mitigate
system stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model
Memory Allocation to dynamically balance memory between generation and
verification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache
reuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on
a single consumer GPU (24 GB) to match the accuracy and latency of large cloud
models. Our evaluation demonstrates that FlashTTS achieves an average 2.2x
higher goodput and reduces latency by 38%-68% compared to a vLLM baseline,
paving the way for democratized, high-performance agentic AI on edge devices.

</details>


### [98] [Calibration through the Lens of Indistinguishability](https://arxiv.org/abs/2509.02279)
*Parikshit Gopalan,Lunjia Hu*

Main category: cs.LG

TL;DR: 这篇论文是关于校准(calibration)的综述研究，探讨了如何定义和测量预测概率的校准误差，以及这些测量对下游决策者的意义。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中概率预测的普及，需要解决如何解释预测概率以及如何评估连续概率预测器在离散结果观察世界中的表现这一基础问题。

Method: 采用统一视角，将校准视为预测器假设的世界与真实世界（由自然或贝叶斯最优预测器控制）之间的一种不可区分性形式。

Result: 提出了各种校准度量方法，这些方法量化了通过特定类别的区分器或统计度量来区分这两个世界的程度。

Conclusion: 校准作为预测质量的重要衡量标准，为评估概率预测器的性能提供了理论基础和实用框架，对机器学习中的概率预测应用具有重要意义。

Abstract: Calibration is a classical notion from the forecasting literature which aims
to address the question: how should predicted probabilities be interpreted? In
a world where we only get to observe (discrete) outcomes, how should we
evaluate a predictor that hypothesizes (continuous) probabilities over possible
outcomes? The study of calibration has seen a surge of recent interest, given
the ubiquity of probabilistic predictions in machine learning. This survey
describes recent work on the foundational questions of how to define and
measure calibration error, and what these measures mean for downstream decision
makers who wish to use the predictions to make decisions. A unifying viewpoint
that emerges is that of calibration as a form of indistinguishability, between
the world hypothesized by the predictor and the real world (governed by nature
or the Bayes optimal predictor). In this view, various calibration measures
quantify the extent to which the two worlds can be told apart by certain
classes of distinguishers or statistical measures.

</details>


### [99] [From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive Inference](https://arxiv.org/abs/2509.00202)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: TConstFormer通过周期性状态更新机制实现O(1)恒定大小的KV缓存，在长文本推理任务中显著提升速度、内存效率和性能


<details>
  <summary>Details</summary>
Motivation: Transformer的自回归推理存在KV缓存线性增长和O(N²d)计算复杂度问题，限制了处理超长序列的能力

Method: 基于TLinFormer，采用创新的周期性状态更新机制，每k-1步进行恒定时间计算，第k步执行线性时间全局信息同步

Result: 理论和实验结果表明，TConstFormer在长文本推理任务的速度、内存效率和整体性能方面具有压倒性优势

Conclusion: 这一突破为高效稳健的流式语言模型应用铺平了道路

Abstract: Although the Transformer has become the cornerstone of modern AI, its
autoregressive inference suffers from a linearly growing KV Cache and a
computational complexity of O(N^2 d), severely hindering its ability to process
ultra-long sequences. To overcome this limitation, this paper introduces the
TConstFormer architecture, building upon our previous work, TLinFormer.
TConstFormer employs an innovative periodic state update mechanism to achieve a
truly constant-size O(1) KV Cache. The computational complexity of this
mechanism is also O(1) in an amortized sense: it performs purely constant-time
computations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single
linear-time global information synchronization only on the $k$-th step.
Theoretical calculations and experimental results demonstrate that TConstFormer
exhibits an overwhelming advantage over baseline models in terms of speed,
memory efficiency, and overall performance on long-text inference tasks. This
breakthrough paves the way for efficient and robust streaming language model
applications.

</details>


### [100] [Gaming and Cooperation in Federated Learning: What Can Happen and How to Monitor It](https://arxiv.org/abs/2509.02391)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: 该论文将联邦学习建模为战略系统而非单纯优化任务，提出了分析框架来区分真正提升性能的行为与仅针对指标的行为，并提供了实践检查表和算法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的成功取决于参与者不可见的行为，需要从战略系统角度分析规则和激励机制，识别真正改善性能的行为与仅针对指标的行为之间的差异。

Method: 引入两个指标分别量化行为激励和集体性能损失，基于此分析操作选择的影响，总结阈值、自动切换规则和预警信号为实践检查表，提供有限审计资源分配算法。

Result: 在不同环境下的模拟验证了框架预测的模式，提供了完整可复现的程序，结合定期重新校准、随机化和基于连接的警报可在实际操作变异性下稳健应用。

Conclusion: 提出了降低指标博弈激励同时维持和扩展稳定合作的设计原则和操作指南，在多个假设下运行最强，但通过组合技术可实现实际应用的稳健性。

Abstract: The success of Federated Learning depends on the actions that participants
take out of sight. We model Federated Learning not as a mere optimization task
but as a strategic system entangled with rules and incentives. From this
perspective, we present an analytical framework that makes it possible to
clearly identify where behaviors that genuinely improve performance diverge
from those that merely target metrics. We introduce two indices that
respectively quantify behavioral incentives and collective performance loss,
and we use them as the basis for consistently interpreting the impact of
operational choices such as rule design, the level of information disclosure,
evaluation methods, and aggregator switching. We further summarize thresholds,
auto-switch rules, and early warning signals into a checklist that can be
applied directly in practice, and we provide both a practical algorithm for
allocating limited audit resources and a performance guarantee. Simulations
conducted across diverse environments consistently validate the patterns
predicted by our framework, and we release all procedures for full
reproducibility. While our approach operates most strongly under several
assumptions, combining periodic recalibration, randomization, and
connectivity-based alarms enables robust application under the variability of
real-world operations. We present both design principles and operational
guidelines that lower the incentives for metric gaming while sustaining and
expanding stable cooperation.

</details>


### [101] [Estimating Parameter Fields in Multi-Physics PDEs from Scarce Measurements](https://arxiv.org/abs/2509.00203)
*Xuyang Li,Mahdi Masmoudi,Rami Gharbi,Nizar Lajnef,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: Neptune是一种新的参数化PDE参数估计方法，使用独立坐标神经网络表示参数场，从稀疏观测中准确推断非线性时空变化参数，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数估计方法（如稀疏识别和PINNs）在处理非线性动态、多物理场相互作用或有限观测时存在困难，需要一种更通用的方法来从稀疏测量中准确推断参数场。

Method: 采用独立坐标神经网络连续表示物理空间或状态变量中的每个参数场，能够从稀疏系统响应测量中推断参数场。

Result: 在多种物理和生物医学问题中，Neptune仅需50个观测点就能实现稳健参数估计，参数估计误差比PINNs降低两个数量级，动态响应预测误差降低十倍，并具有优异的泛化能力。

Conclusion: Neptune通过实现可靠且数据高效的参数推断，在工程、医疗等领域具有广泛的变革性影响潜力。

Abstract: Parameterized partial differential equations (PDEs) underpin the mathematical
modeling of complex systems in diverse domains, including engineering,
healthcare, and physics. A central challenge in using PDEs for real-world
applications is to accurately infer the parameters, particularly when the
parameters exhibit non-linear and spatiotemporal variations. Existing parameter
estimation methods, such as sparse identification and physics-informed neural
networks (PINNs), struggle in such cases, especially with nonlinear dynamics,
multiphysics interactions, or limited observations of the system response. To
address these challenges, we introduce Neptune, a general-purpose method
capable of inferring parameter fields from sparse measurements of system
responses. Neptune employs independent coordinate neural networks to
continuously represent each parameter field in physical space or in state
variables. Across various physical and biomedical problems, where direct
parameter measurements are prohibitively expensive or unattainable, Neptune
significantly outperforms existing methods, achieving robust parameter
estimation from as few as 50 observations, reducing parameter estimation errors
by two orders of magnitude and dynamic response prediction errors by a factor
of ten compared to PINNs. Furthermore, Neptune exhibits superior extrapolation
capabilities, enabling accurate predictions in regimes beyond training data
where PINN fail. By facilitating reliable and data-efficient parameter
inference, Neptune promises broad transformative impacts in engineering,
healthcare, and beyond.

</details>


### [102] [Is RL fine-tuning harder than regression? A PDE learning approach for diffusion models](https://arxiv.org/abs/2509.02528)
*Wenlong Mou*

Main category: cs.LG

TL;DR: 本文提出了一种基于值函数近化的扩散过程微调最优控制策略学习方法，通过求解HJB方程的变分不等式问题，获得了比通用强化学习方法更快的统计速率保证。


<details>
  <summary>Details</summary>
Motivation: 研究如何学习扩散过程微调的最优控制策略，利用值函数近似方法解决传统强化学习方法在统计效率上的局限性。

Method: 通过求解基于Hamilton-Jacobi-Bellman方程的变分不等式问题，开发新的算法类别，使用监督回归方法实现微调。

Result: 证明了学习值函数和控制策略的尖锐统计速率，这些速率取决于函数类的复杂性和近似误差，相比通用强化学习具有更快的统计速率保证。

Conclusion: 扩散过程微调可以通过监督回归实现，且比通用强化学习方法具有更好的统计效率，为扩散模型的控制策略学习提供了新的理论框架。

Abstract: We study the problem of learning the optimal control policy for fine-tuning a
given diffusion process, using general value function approximation. We develop
a new class of algorithms by solving a variational inequality problem based on
the Hamilton-Jacobi-Bellman (HJB) equations. We prove sharp statistical rates
for the learned value function and control policy, depending on the complexity
and approximation errors of the function class. In contrast to generic
reinforcement learning problems, our approach shows that fine-tuning can be
achieved via supervised regression, with faster statistical rate guarantees.

</details>


### [103] [Learning to Shard: RL for Co-optimizing the Parallelism Degrees and Per-operator Sharding Dimensions in Distributed LLM Inference](https://arxiv.org/abs/2509.00217)
*Ruokai Yin,Sattwik Deb Mishra,Xuan Zuo,Hokchhay Tann,Preyas Shah,Apala Guha*

Main category: cs.LG

TL;DR: Learn to Shard是首个基于强化学习的方法，共同优化分布式LLM推理中的粗粒度并行度和细粒度算子分片维度，在H100集群上相比现有方法实现最高3.5倍吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 现有系统如Megatron-LM使用静态启发式方法分别配置并行度和算子分片维度，在模型规模扩大和硬件拓扑多样化时性能表现不佳

Method: 采用基于注意力的策略从高性能策略历史中学习，有效导航巨大的组合搜索空间，共同优化粗粒度并行度和细粒度算子分片维度

Result: 在H100集群上测试1.6T参数的MoE模型，相比元启发式基线实现最高3.5倍吞吐量提升，相比Megatron启发式方法提升1.06倍

Conclusion: Learn to Shard通过RL方法有效解决了分布式LLM推理中的并行化策略优化问题，显著提升了大规模模型推理性能

Abstract: Distributed LLM inference requires careful coordination of parallelization
strategies across hundreds to thousands of NPUs to meet production SLOs.
Current systems like Megatron-LM rely on static heuristics that separately
configure parallelism degrees and per-operator sharding dimensions, leaving
significant performance on the table as models scale and hardware topologies
diversify. We introduce Learn to Shard, to our knowledge, the first RL-based
approach to co-optimize both coarse-grained parallelism degrees and
fine-grained per-operator sharding dimensions for distributed LLM inference.
Our method employs an attention-based policy over an elite history that learns
from high-performing strategies to efficiently navigate the vast combinatorial
search space. Evaluated on H100 clusters with MoE models up to 1.6T parameters,
Learn to Shard achieves up to 3.5x throughput improvement over metaheuristic
baselines and 1.06x over Megatron heuristics.

</details>


### [104] [Federated learning over physical channels: adaptive algorithms with near-optimal guarantees](https://arxiv.org/abs/2509.02538)
*Rui Zhang,Wenlong Mou*

Main category: cs.LG

TL;DR: 提出了一种新的自适应联邦随机梯度下降算法，通过物理信道传输信息，考虑了信道噪声和硬件约束，建立了理论保证并验证了实际效果


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信成本较高，通过物理信道传输可以显著降低通信成本，但需要考虑信道噪声和硬件约束的影响

Method: 提出自适应联邦随机梯度下降算法，设计能够在物理信道上实现的方案，考虑信道噪声和硬件限制因素

Result: 建立了算法的理论保证，证明了收敛速率能够自适应随机梯度噪声水平，通过深度学习模型仿真验证了算法的实际有效性

Conclusion: 所提出的自适应联邦SGD算法能够在物理信道环境下有效工作，既降低了通信成本，又保证了收敛性能，为联邦学习的实际部署提供了可行方案

Abstract: In federated learning, communication cost can be significantly reduced by
transmitting the information over the air through physical channels. In this
paper, we propose a new class of adaptive federated stochastic gradient descent
(SGD) algorithms that can be implemented over physical channels, taking into
account both channel noise and hardware constraints. We establish theoretical
guarantees for the proposed algorithms, demonstrating convergence rates that
are adaptive to the stochastic gradient noise level. We also demonstrate the
practical effectiveness of our algorithms through simulation studies with deep
learning models.

</details>


### [105] [Speech Foundation Models Generalize to Time Series Tasks from Wearable Sensor Data](https://arxiv.org/abs/2509.00221)
*Jaya Narain,Zakaria Aldeneh,Shirley Ren*

Main category: cs.LG

TL;DR: 语音基础模型（HuBERT和wav2vec 2.0）的特征提取器在可穿戴传感器时间序列任务中表现出色，超越了针对特定模态训练的自监督模型，在情绪分类、心律失常检测和活动分类等任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 探索语音和传感器时间序列数据在时域和频域的共性特征，研究语音基础模型的领域无关表示能力，以解决数据稀缺的时间序列任务。

Method: 使用HuBERT和wav2vec 2.0语音基础模型提取特征，然后训练简单的探测分类器（probes）来处理可穿戴传感器的时间序列分类任务。

Result: 语音模型的卷积特征编码器在可穿戴传感器任务中表现特别突出，所提方法提高了数据稀缺时间序列任务的性能和鲁棒性。

Conclusion: 这项工作证明了语音基础模型的跨领域迁移能力，是构建通用时间序列模型的重要一步，为语音和传感器数据的统一建模提供了新的研究方向。

Abstract: Both speech and sensor time series data encode information in both the time-
and frequency- domains, like spectral powers and waveform shapelets. We show
that speech foundation models learn representations that are domain-independent
and achieve state-of-the-art performance on time series tasks from wearable
sensors. Probes trained on features extracted from HuBERT and wav2vec 2.0
outperform those extracted from self-supervised models trained directly on
modality specific datasets for mood classification, arrhythmia detection, and
activity classification tasks. We find a particularly strong relevance of the
convolutional feature encoders from speech models for wearable sensor tasks.
The methods proposed here improve performance and robustness for data-scarce
time series tasks, using simple probing methods. This work is a step towards
generalized time series models for speech and sensor data, a topic for further
exploration.

</details>


### [106] [Quantum-Optimized Selective State Space Model for Efficient Time Series Prediction](https://arxiv.org/abs/2509.00259)
*Stefan-Alexandru Jura,Mihai Udrescu,Alexandru Topirceanu*

Main category: cs.LG

TL;DR: 提出量子优化选择性状态空间模型(Q-SSM)，通过变分量子门机制解决长时序预测中的稳定性问题和计算效率问题，在多个基准测试中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决长时序预测中Transformer模型的二次复杂度问题和状态空间模型训练不稳定的挑战，需要同时保持噪声鲁棒性、效率和稳定性

Method: 结合状态空间动力学和变分量子门(RY-RX ansatz参数化量子电路)，用量子期望值自适应调节记忆更新，替代昂贵的注意力机制

Result: 在ETT、Traffic和Exchange Rate三个基准测试中一致优于LSTM、TCN、Reformer、Transformer和S-Mamba等强基线模型

Conclusion: 变分量子门机制能够有效解决长程预测中的当前限制，实现准确鲁棒的多变量预测，为长时序预测提供了轻量级替代方案

Abstract: Long-range time series forecasting remains challenging, as it requires
capturing non-stationary and multi-scale temporal dependencies while
maintaining noise robustness, efficiency, and stability. Transformer-based
architectures such as Autoformer and Informer improve generalization but suffer
from quadratic complexity and degraded performance on very long time horizons.
State space models, notably S-Mamba, provide linear-time updates but often face
unstable training dynamics, sensitivity to initialization, and limited
robustness for multivariate forecasting. To address such challenges, we propose
the Quantum-Optimized Selective State Space Model (Q-SSM), a hybrid
quantum-optimized approach that integrates state space dynamics with a
variational quantum gate. Instead of relying on expensive attention mechanisms,
Q-SSM employs a simple parametrized quantum circuit (RY-RX ansatz) whose
expectation values regulate memory updates adaptively. This quantum gating
mechanism improves convergence stability, enhances the modeling of long-term
dependencies, and provides a lightweight alternative to attention. We
empirically validate Q-SSM on three widely used benchmarks, i.e., ETT, Traffic,
and Exchange Rate. Results show that Q-SSM consistently improves over strong
baselines (LSTM, TCN, Reformer), Transformer-based models, and S-Mamba. These
findings demonstrate that variational quantum gating can address current
limitations in long-range forecasting, leading to accurate and robust
multivariate predictions.

</details>


### [107] [ReLATE: Learning Efficient Sparse Encoding for High-Performance Tensor Decomposition](https://arxiv.org/abs/2509.00280)
*Ahmed E. Helal,Fabio Checconi,Jan Laukemann,Yongseok Soh,Jesmin Jahan Tithi,Fabrizio Petrini,Jee Choi*

Main category: cs.LG

TL;DR: ReLATE是一个基于强化学习的自适应张量编码框架，能够自动为不规则张量形状和可变数据分布构建高效的稀疏张量表示，无需标注训练样本，相比专家设计的格式可获得最高2倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统专家设计的稀疏张量格式无法适应不规则张量形状和高度可变的数据分布，导致张量分解在并行处理器上性能受限。

Method: 采用强化学习代理通过与张量分解环境直接交互来发现优化的张量编码，结合无模型和基于模型的混合算法，并引入规则驱动的动作掩码和动态信息动作过滤机制。

Result: 在多样化稀疏张量数据集上，ReLATE生成的稀疏张量表示始终优于专家设计的格式，最高可获得2倍加速，几何平均加速比为1.4-1.46倍。

Conclusion: ReLATE框架通过自主学习适应不规则张量形状和数据分布，为高性能张量分解提供了有效的自适应稀疏编码解决方案。

Abstract: Tensor decomposition (TD) is essential for analyzing high-dimensional sparse
data, yet its irregular computations and memory-access patterns pose major
performance challenges on modern parallel processors. Prior works rely on
expert-designed sparse tensor formats that fail to adapt to irregular tensor
shapes and/or highly variable data distributions. We present the
reinforcement-learned adaptive tensor encoding (ReLATE) framework, a novel
learning-augmented method that automatically constructs efficient sparse tensor
representations without labeled training samples. ReLATE employs an autonomous
agent that discovers optimized tensor encodings through direct interaction with
the TD environment, leveraging a hybrid model-free and model-based algorithm to
learn from both real and imagined actions. Moreover, ReLATE introduces
rule-driven action masking and dynamics-informed action filtering mechanisms
that ensure functionally correct tensor encoding with bounded execution time,
even during early learning stages. By automatically adapting to both irregular
tensor shapes and data distributions, ReLATE generates sparse tensor
representations that consistently outperform expert-designed formats across
diverse sparse tensor data sets, achieving up to 2X speedup compared to the
best sparse format, with a geometric-mean speedup of 1.4-1.46X.

</details>


### [108] [Continuously Tempered Diffusion Samplers](https://arxiv.org/abs/2509.00316)
*Ezra Erives,Bowen Jing,Peter Holderrieth,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 提出连续温度扩散采样器(CTDS)，通过引入多温度分布族来改善神经采样器的探索能力，解决传统方法在孤立模态和病理性分布中探索不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于退火的神经采样器在训练阶段使用部分学习的传输和退火Langevin动力学生成提议分布，但在孤立模态和其他病理性退火路径下探索不足，导致训练后性能下降。

Method: 提出连续温度扩散采样器，借鉴分子动力学中的探索技术，引入不同温度下的分布族，在高温下降低能量壁垒，在目标低温下驱动探索。

Result: 经验验证表明，通过扩展探索能力显著改善了采样器性能。

Conclusion: CTDS方法通过多温度策略有效解决了神经采样器在复杂分布中的探索问题，提升了采样效率和质量。

Abstract: Annealing-based neural samplers seek to amortize sampling from unnormalized
distributions by training neural networks to transport a family of densities
interpolating from source to target. A crucial design choice in the training
phase of such samplers is the proposal distribution by which locations are
generated at which to evaluate the loss. Previous work has obtained such a
proposal distribution by combining a partially learned transport with annealed
Langevin dynamics. However, isolated modes and other pathological properties of
the annealing path imply that such proposals achieve insufficient exploration
and thereby lower performance post training. To remedy this, we propose
continuously tempered diffusion samplers, which leverage exploration techniques
developed in the context of molecular dynamics to improve proposal
distributions. Specifically, a family of distributions across different
temperatures is introduced to lower energy barriers at higher temperatures and
drive exploration at the lower temperature of interest. We empirically validate
improved sampler performance driven by extended exploration. Code is available
at https://github.com/eje24/ctds.

</details>


### [109] [Chunked TabPFN: Exact Training-Free In-Context Learning for Long-Context Tabular Data](https://arxiv.org/abs/2509.00326)
*Renat Sergazinov,Shao-An Yin*

Main category: cs.LG

TL;DR: TabPFN v2在多个表格数据基准测试中优于树模型，但受限于Transformer的二次计算复杂度，无法处理超过10K上下文标记。本文提出了一种tiled-block策略，使TabPFN能够处理长上下文而无需预处理。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文压缩方法（如KNN选择代表性样本）存在局限性，需要探索新的方法来突破TabPFN在长上下文处理上的计算瓶颈。

Method: 提出tiled-block策略在TabPFN框架内计算注意力，该设计与标准GPU设置兼容，无需任何预处理即可处理长上下文。

Result: 在标准TabArena基准测试中证明了该方法的有效性，成功使TabPFN能够处理超过10K标记的长上下文。

Conclusion: tiled-block策略是首个无需预处理就能让TabPFN处理长上下文的解决方案，为表格数据上的Transformer模型应用提供了新的可能性。

Abstract: TabPFN v2 achieves better results than tree-based models on several tabular
benchmarks, which is notable since tree-based models are usually the strongest
choice for tabular data. However, it cannot handle more than 10K context tokens
because transformers have quadratic computation and memory costs.
  Unlike existing approaches that rely on context compression, such as
selecting representative samples via K-nearest neighbors (KNN), we introduce a
\textbf{tiled-block} strategy to compute attention within the TabPFN framework.
This design is compatible with standard GPU setups and, to the best of our
knowledge, is the first to enable TabPFN to \textbf{process long contexts
without any pre-processing}. We demonstrate the effectiveness of our approach
on the standard TabArena benchmark.

</details>


### [110] [Counterfactual Risk Minimization with IPS-Weighted BPR and Self-Normalized Evaluation in Recommender Systems](https://arxiv.org/abs/2509.00333)
*Rahul Raja,Arpita Vats*

Main category: cs.LG

TL;DR: 提出了一种结合IPS加权训练、IPS加权BPR目标和倾向正则化的方法，用于解决推荐系统中曝光偏差问题，减少方差并提高稳定性


<details>
  <summary>Details</summary>
Motivation: 解决基于隐式反馈的推荐系统中存在的曝光偏差问题，传统IPS方法虽然能纠正偏差但存在高方差和不稳定性

Method: 整合IPS加权训练与IPS加权BPR目标，并加入倾向正则化(PR)来缓解极端倾向权重带来的方差放大

Result: 在合成数据和MovieLens 100K数据上的实验表明，该方法在无偏曝光下泛化更好，评估方差相比朴素和标准IPS方法更小

Conclusion: 为现实推荐场景中的反事实学习和评估提供了实用指导，能够更好地处理曝光偏差问题

Abstract: Learning and evaluating recommender systems from logged implicit feedback is
challenging due to exposure bias. While inverse propensity scoring (IPS)
corrects this bias, it often suffers from high variance and instability. In
this paper, we present a simple and effective pipeline that integrates
IPS-weighted training with an IPS-weighted Bayesian Personalized Ranking (BPR)
objective augmented by a Propensity Regularizer (PR). We compare Direct Method
(DM), IPS, and Self-Normalized IPS (SNIPS) for offline policy evaluation, and
demonstrate how IPS-weighted training improves model robustness under biased
exposure. The proposed PR further mitigates variance amplification from extreme
propensity weights, leading to more stable estimates. Experiments on synthetic
and MovieLens 100K data show that our approach generalizes better under
unbiased exposure while reducing evaluation variance compared to naive and
standard IPS methods, offering practical guidance for counterfactual learning
and evaluation in real-world recommendation settings.

</details>


### [111] [Are We Really Learning the Score Function? Reinterpreting Diffusion Models Through Wasserstein Gradient Flow Matching](https://arxiv.org/abs/2509.00336)
*An B. Vuong,Michael T. McCann,Javier E. Santos,Yen Ting Lin*

Main category: cs.LG

TL;DR: 扩散模型通常被解释为学习分数函数，但研究发现训练后的网络违反保守向量场约束，表明学习到的向量场并非真正的分数函数。作者提出应从Wasserstein梯度流的视角理解扩散训练，而非分数学习。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为扩散模型学习的是噪声数据的对数密度梯度（分数函数），但实际神经网络架构并不强制学习保守向量场。研究发现训练后的扩散网络确实违反了真正分数函数的约束条件，这与模型出色的生成性能形成矛盾。

Method: 通过数值实验证明训练后的扩散网络违反积分和微分约束，提出从Wasserstein梯度流（WGF）的速度场流匹配角度重新理解扩散训练，而非基于反向时间SDE的分数学习理论。

Result: 研究发现扩散网络学习的向量场不是保守的，但模型仍能出色生成样本。从WGF视角可以自然解释概率流的产生，阐明即使神经向量场不是真分数函数，生成采样仍能成功的原因。

Conclusion: WGF框架为理解扩散生成模型提供了原则性、优雅且理论基础扎实的新视角，神经逼近产生的非保守误差不一定损害密度传输，建议采用WGF视角替代传统分数学习理论。

Abstract: Diffusion models are commonly interpreted as learning the score function,
i.e., the gradient of the log-density of noisy data. However, this assumption
implies that the target of learning is a conservative vector field, which is
not enforced by the neural network architectures used in practice. We present
numerical evidence that trained diffusion networks violate both integral and
differential constraints required of true score functions, demonstrating that
the learned vector fields are not conservative. Despite this, the models
perform remarkably well as generative mechanisms. To explain this apparent
paradox, we advocate a new theoretical perspective: diffusion training is
better understood as flow matching to the velocity field of a Wasserstein
Gradient Flow (WGF), rather than as score learning for a reverse-time
stochastic differential equation. Under this view, the "probability flow"
arises naturally from the WGF framework, eliminating the need to invoke
reverse-time SDE theory and clarifying why generative sampling remains
successful even when the neural vector field is not a true score. We further
show that non-conservative errors from neural approximation do not necessarily
harm density transport. Our results advocate for adopting the WGF perspective
as a principled, elegant, and theoretically grounded framework for
understanding diffusion generative models.

</details>


### [112] [Scalable Option Learning in High-Throughput Environments](https://arxiv.org/abs/2509.00338)
*Mikael Henaff,Scott Fujimoto,Michael Rabbat*

Main category: cs.LG

TL;DR: 提出SOL算法，在分层强化学习中实现25倍吞吐量提升，在NetHack游戏中用200亿帧训练，性能超越平层方法


<details>
  <summary>Details</summary>
Motivation: 现有分层RL方法在大规模训练中尚未充分发挥潜力，需要解决高吞吐量环境下的扩展挑战

Method: Scalable Option Learning (SOL) - 高度可扩展的分层强化学习算法

Result: 在NetHack游戏中显著超越平层智能体，在MiniHack和Mujoco环境中验证了通用性，吞吐量比现有分层方法高25倍

Conclusion: SOL算法成功解决了分层RL的扩展挑战，展示了在大规模环境中的有效性和通用应用前景

Abstract: Hierarchical reinforcement learning (RL) has the potential to enable
effective decision-making over long timescales. Existing approaches, while
promising, have yet to realize the benefits of large-scale training. In this
work, we identify and solve several key challenges in scaling hierarchical RL
to high-throughput environments. We propose Scalable Option Learning (SOL), a
highly scalable hierarchical RL algorithm which achieves a 25x higher
throughput compared to existing hierarchical methods. We train our hierarchical
agents using 20 billion frames of experience on the complex game of NetHack,
significantly surpassing flat agents and demonstrating positive scaling trends.
We also validate our algorithm on MiniHack and Mujoco environments, showcasing
its general applicability. Our code is open sourced at
github.com/facebookresearch/sol.

</details>


### [113] [LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning](https://arxiv.org/abs/2509.00347)
*Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TL;DR: 提出LLMDPD方法，通过任务特定提示增强离线强化学习的泛化能力，结合文本描述和轨迹提示来指导策略学习


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临泛化挑战，由于离线数据限制，智能体难以泛化到新任务或环境，需要利用大语言模型的知识来提升泛化性能

Method: 使用LLM处理文本提示提供任务上下文，用Transformer编码轨迹提示捕获行为模式，通过上下文感知的策略级扩散模型生成策略

Result: 在未见任务上优于现有离线RL方法，证明了在多样化设置中提升泛化和适应性的有效性

Conclusion: LLMDPD通过结合语言理解和行为模式编码，有效解决了离线RL的泛化问题，为跨任务泛化提供了新思路

Abstract: Reinforcement Learning (RL) is known for its strong decision-making
capabilities and has been widely applied in various real-world scenarios.
However, with the increasing availability of offline datasets and the lack of
well-designed online environments from human experts, the challenge of
generalization in offline RL has become more prominent. Due to the limitations
of offline data, RL agents trained solely on collected experiences often
struggle to generalize to new tasks or environments. To address this challenge,
we propose LLM-Driven Policy Diffusion (LLMDPD), a novel approach that enhances
generalization in offline RL using task-specific prompts. Our method
incorporates both text-based task descriptions and trajectory prompts to guide
policy learning. We leverage a large language model (LLM) to process text-based
prompts, utilizing its natural language understanding and extensive knowledge
base to provide rich task-relevant context. Simultaneously, we encode
trajectory prompts using a transformer model, capturing structured behavioral
patterns within the underlying transition dynamics. These prompts serve as
conditional inputs to a context-aware policy-level diffusion model, enabling
the RL agent to generalize effectively to unseen tasks. Our experimental
results demonstrate that LLMDPD outperforms state-of-the-art offline RL methods
on unseen tasks, highlighting its effectiveness in improving generalization and
adaptability in diverse settings.

</details>


### [114] [Theory Foundation of Physics-Enhanced Residual Learning](https://arxiv.org/abs/2509.00348)
*Shixiao Liang,Wang Chen,Keke Long,Peng Zhang,Xiaopeng Li,Jintao Ke*

Main category: cs.LG

TL;DR: 本文从理论角度解释了物理增强残差学习(PERL)方法的三大优势：减少神经网络参数量、加快收敛速度、减少训练样本需求，并通过自动驾驶轨迹预测的数值实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 虽然数值实验表明PERL方法在整合物理模型与神经网络时具有参数少、收敛快、样本需求少等优势，但这些结果缺乏理论解释，需要从理论角度阐明其内在机理。

Method: 研究具有Lipschitz连续性的一般问题类别，通过分析损失函数边界与残差学习结构之间的关系，严格证明了一组解释PERL三大优势的定理。

Result: 理论分析证明了PERL的三大优势，数值实验显示即使使用显著更少的训练样本，PERL仍比纯神经网络获得更高精度，在自动驾驶轨迹预测中表现出色。

Conclusion: PERL方法通过理论证明和实验验证，在减少数据需求的同时提高了预测性能，在真实自动驾驶应用中具有重要价值，特别是在难以获取极端案例数据的情况下。

Abstract: Intensive studies have been conducted in recent years to integrate neural
networks with physics models to balance model accuracy and interpretability.
One recently proposed approach, named Physics-Enhanced Residual Learning
(PERL), is to use learning to estimate the residual between the physics model
prediction and the ground truth. Numeral examples suggested that integrating
such residual with physics models in PERL has three advantages: (1) a reduction
in the number of required neural network parameters; (2) faster convergence
rates; and (3) fewer training samples needed for the same computational
precision. However, these numerical results lack theoretical justification and
cannot be adequately explained.
  This paper aims to explain these advantages of PERL from a theoretical
perspective. We investigate a general class of problems with Lipschitz
continuity properties. By examining the relationships between the bounds to the
loss function and residual learning structure, this study rigorously proves a
set of theorems explaining the three advantages of PERL.
  Several numerical examples in the context of automated vehicle trajectory
prediction are conducted to illustrate the proposed theorems. The results
confirm that, even with significantly fewer training samples, PERL consistently
achieves higher accuracy than a pure neural network. These results demonstrate
the practical value of PERL in real world autonomous driving applications where
corner case data are costly or hard to obtain. PERL therefore improves
predictive performance while reducing the amount of data required.

</details>


### [115] [Optimized Weight Initialization on the Stiefel Manifold for Deep ReLU Neural Networks](https://arxiv.org/abs/2509.00362)
*Hyungu Lee,Taehyeong Kim,Hayoung Choi*

Main category: cs.LG

TL;DR: 提出一种针对ReLU网络的优化正交初始化方法，通过Stiefel流形上的优化问题校准预激活统计量，解决深度网络中的神经元失活和梯度消失问题


<details>
  <summary>Details</summary>
Motivation: 传统初始化方法如He、Xavier和正交初始化无法有效调节预激活均值或控制激活稀疏性，在极深网络中效果下降，导致ReLU神经元永久失活和梯度不稳定

Method: 在Stiefel流形上求解优化问题，推导出闭式解和高效采样方案，专门为ReLU网络优化正交初始化，从初始化阶段就保持尺度并校准预激活统计量

Result: 理论分析表明该方法能防止ReLU神经元失活、减缓激活方差衰减、缓解梯度消失，实验在多个数据集和少样本设置中优于先前初始化方法

Conclusion: 该方法显著改善深度ReLU网络的训练稳定性，实现更好的信号和梯度流动，适用于各种ReLU族激活函数和深度架构

Abstract: Stable and efficient training of ReLU networks with large depth is highly
sensitive to weight initialization. Improper initialization can cause permanent
neuron inactivation dying ReLU and exacerbate gradient instability as network
depth increases. Methods such as He, Xavier, and orthogonal initialization
preserve variance or promote approximate isometry. However, they do not
necessarily regulate the pre-activation mean or control activation sparsity,
and their effectiveness often diminishes in very deep architectures. This work
introduces an orthogonal initialization specifically optimized for ReLU by
solving an optimization problem on the Stiefel manifold, thereby preserving
scale and calibrating the pre-activation statistics from the outset. A family
of closed-form solutions and an efficient sampling scheme are derived.
Theoretical analysis at initialization shows that prevention of the dying ReLU
problem, slower decay of activation variance, and mitigation of gradient
vanishing, which together stabilize signal and gradient flow in deep
architectures. Empirically, across MNIST, Fashion-MNIST, multiple tabular
datasets, few-shot settings, and ReLU-family activations, our method
outperforms previous initializations and enables stable training in deep
networks.

</details>


### [116] [Unifying Adversarial Perturbation for Graph Neural Networks](https://arxiv.org/abs/2509.00387)
*Jinluan Yang,Ruihao Zhang,Zhengyu Chen,Fei Wu,Kun Kuang*

Main category: cs.LG

TL;DR: 提出了PerturbEmbedding方法，通过在GNN的隐藏嵌入层直接进行扰动操作，统一了随机和对抗性扰动策略，显著提升了图神经网络的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法主要局限于特定数据集和GNN类型，需要一种更通用的方法来增强GNN对节点特征和图结构对抗攻击的抵抗力。

Method: PerturbEmbedding方法直接在GNN的每个隐藏嵌入层执行扰动操作，提供了一个统一的框架来整合大多数现有扰动策略，包括随机扰动和对抗性扰动。

Result: 在多个数据集和不同骨干模型上的实验表明，PerturbEmbedding显著提高了GNN的鲁棒性和泛化能力，优于现有方法，并能同时抵御随机（非目标）和对抗性（目标）扰动。

Conclusion: PerturbEmbedding提供了一个有效的统一框架来增强GNN的对抗鲁棒性，通过直接在隐藏嵌入层进行扰动操作，实现了更好的模型性能和泛化能力。

Abstract: This paper studies the vulnerability of Graph Neural Networks (GNNs) to
adversarial attacks on node features and graph structure. Various methods have
implemented adversarial training to augment graph data, aiming to bolster the
robustness and generalization of GNNs. These methods typically involve applying
perturbations to the node feature, weights, or graph structure and subsequently
minimizing the loss by learning more robust graph model parameters under the
adversarial perturbations. Despite the effectiveness of adversarial training in
enhancing GNNs' robustness and generalization abilities, its application has
been largely confined to specific datasets and GNN types. In this paper, we
propose a novel method, PerturbEmbedding, that integrates adversarial
perturbation and training, enhancing GNNs' resilience to such attacks and
improving their generalization ability. PerturbEmbedding performs perturbation
operations directly on every hidden embedding of GNNs and provides a unified
framework for most existing perturbation strategies/methods. We also offer a
unified perspective on the forms of perturbations, namely random and
adversarial perturbations. Through experiments on various datasets using
different backbone models, we demonstrate that PerturbEmbedding significantly
improves both the robustness and generalization abilities of GNNs,
outperforming existing methods. The rejection of both random (non-targeted) and
adversarial (targeted) perturbations further enhances the backbone model's
performance.

</details>


### [117] [Curriculum Guided Personalized Subgraph Federated Learning](https://arxiv.org/abs/2509.00402)
*Minku Kang,Hogun Park*

Main category: cs.LG

TL;DR: CUFL是一个新的个性化子图联邦学习框架，通过课程学习防止早期过拟合，使用细粒度结构指标改进客户端相似性估计，在六个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决子图联邦学习中严重的数据异构性问题，传统加权聚合方法因稀疏和偏置子图导致快速过拟合，使客户端相似性矩阵停滞或崩溃，失去聚合效果。

Method: 采用课程学习(CL)自适应选择边进行训练，先暴露通用跨客户端子结构，后暴露客户端特定结构；使用随机参考图上重构的细粒度结构指标估计客户端相似性；通过调节个性化程度重塑服务器聚合过程。

Result: 在六个基准数据集上的广泛实验证实，CUFL相比相关基线方法取得了更优越的性能。

Conclusion: CUFL通过课程学习引导的个性化训练策略，有效缓解了子图联邦学习中的数据异构性问题，实现了更好的知识共享和个性化性能。

Abstract: Subgraph Federated Learning (FL) aims to train Graph Neural Networks (GNNs)
across distributed private subgraphs, but it suffers from severe data
heterogeneity. To mitigate data heterogeneity, weighted model aggregation
personalizes each local GNN by assigning larger weights to parameters from
clients with similar subgraph characteristics inferred from their current model
states. However, the sparse and biased subgraphs often trigger rapid
overfitting, causing the estimated client similarity matrix to stagnate or even
collapse. As a result, aggregation loses effectiveness as clients reinforce
their own biases instead of exploiting diverse knowledge otherwise available.
To this end, we propose a novel personalized subgraph FL framework called
Curriculum guided personalized sUbgraph Federated Learning (CUFL). On the
client side, CUFL adopts Curriculum Learning (CL) that adaptively selects edges
for training according to their reconstruction scores, exposing each GNN first
to easier, generic cross-client substructures and only later to harder,
client-specific ones. This paced exposure prevents early overfitting to biased
patterns and enables gradual personalization. By regulating personalization,
the curriculum also reshapes server aggregation from exchanging generic
knowledge to propagating client-specific knowledge. Further, CUFL improves
weighted aggregation by estimating client similarity using fine-grained
structural indicators reconstructed on a random reference graph. Extensive
experiments on six benchmark datasets confirm that CUFL achieves superior
performance compared to relevant baselines. Code is available at
https://github.com/Kang-Min-Ku/CUFL.git.

</details>


### [118] [Metis: Training Large Language Models with Advanced Low-Bit Quantization](https://arxiv.org/abs/2509.00404)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Ruijun Huang,Fang Dong,Jixian Zhou,Anrui Chen,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Yuan Cheng,Fan Wu,Fan Yang,Tun Lu,Ning Gu,Li Shang*

Main category: cs.LG

TL;DR: Metis是一个针对低比特量化训练LLM的框架，通过谱分解、自适应学习率和双范围正则化解决了各向异性参数分布导致的训练不稳定问题，使FP8训练超越FP32基准，FP4训练达到接近FP32的精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的各向异性参数分布（少数主导奇异值造成宽数值范围）与块级量化的固有偏差冲突，导致训练不稳定和性能低下，这是低比特量化训练的主要障碍。

Method: 结合三个关键技术：(1) 谱分解与随机嵌入分离主导和长尾分量，压缩分布到量化友好范围；(2) 谱域自适应学习率放大 underrepresented 方向；(3) 双范围正则化联合约束数值精度和参数范围分布。

Result: Metis使FP8训练超越FP32基线性能，FP4训练达到与FP32相当的精度，为先进低比特量化下的稳健可扩展LLM训练铺平道路。

Conclusion: Metis框架成功解决了低比特量化训练中的各向异性分布问题，通过谱域优化和正则化技术实现了稳定、无偏的低比特训练，显著提升了量化模型的性能表现。

Abstract: This work identifies anisotropic parameter distributions as a fundamental
barrier to training large language models (LLMs) with low-bit quantization: a
few dominant singular values create wide numerical ranges that conflict with
the inherent bias of block-wise quantization. This bias disproportionately
preserves high-magnitude values while discarding smaller ones, causing training
instability and low model performance. This work introduces Metis, a training
framework that combines (i) spectral decomposition with random embedding to
efficiently disentangle dominant from long-tail components, compressing broad
distributions into quantization-friendly narrow ranges; (ii) adaptive learning
rates in the spectral domain to amplify underrepresented directions and better
capture diverse features critical for performance; and (iii) a dual-range
regularizer that jointly constrains numerical precision and parameter range
distribution, ensuring stable, unbiased low-bit training. With Metis, FP8
training surpasses FP32 baselines, and FP4 training achieves accuracy
comparable to FP32, paving the way for robust and scalable LLM training under
advanced low-bit quantization. The code implementation for Metis is available
at: https://github.com/typename-yyf/Metis-quantization.

</details>


### [119] [IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models](https://arxiv.org/abs/2509.01073)
*Yuhong Zhang,Xusheng Zhu,Yuchen Xu,ChiaEn Lu,Hsinyu Shih,Gert Cauwenberghs,Tzyy-Ping Jung*

Main category: cs.LG

TL;DR: 提出基于大腦模型LaBraM的相關注意力映射方法，利用IMU數據的空間通道關係來識別EEG信號中的運動偽影，相比傳統單模態方法顯著提升運動場景下的魯棒性。


<details>
  <summary>Details</summary>
Motivation: EEG信號易受運動偽影污染，傳統單模態方法（如ASR、ICA）未充分利用IMU等同時記錄的運動信息，限制了腦機接口在真實場景中的部署。

Method: 使用精調的大型腦模型LaBraM，通過相關注意力映射利用IMU數據的空間通道關係來識別EEG運動偽影。模型僅需5.9小時EEG-IMU數據訓練（佔基礎模型訓練數據的0.2346%），參數量約920萬。

Result: 與ASR-ICA基準方法相比，在不同時間尺度和運動活動下，結合IMU參考信號的方法顯著提高了多種運動場景下的魯棒性。

Conclusion: 整合IMU運動信息能有效提升EEG運動偽影去除性能，為腦機接口在真實運動環境中的應用提供了更可靠的解決方案。

Abstract: Electroencephalography (EEG) is a non-invasive method for measuring brain
activity with high temporal resolution; however, EEG signals often exhibit low
signal-to-noise ratios because of contamination from physiological and
environmental artifacts. One of the major challenges hindering the real-world
deployment of brain-computer interfaces (BCIs) involves the frequent occurrence
of motion-related EEG artifacts. Most prior studies on EEG motion artifact
removal rely on single-modality approaches, such as Artifact Subspace
Reconstruction (ASR) and Independent Component Analysis (ICA), without
incorporating simultaneously recorded modalities like inertial measurement
units (IMUs), which directly capture the extent and dynamics of motion. This
work proposes a fine-tuned large brain model (LaBraM)-based correlation
attention mapping method that leverages spatial channel relationships in IMU
data to identify motion-related artifacts in EEG signals. The fine-tuned model
contains approximately 9.2 million parameters and uses 5.9 hours of EEG and IMU
recordings for training, just 0.2346\% of the 2500 hours used to train the base
model. We compare our results against the established ASR-ICA benchmark across
varying time scales and motion activities, showing that incorporating IMU
reference signals significantly improves robustness under diverse motion
scenarios.

</details>


### [120] [Memory Limitations of Prompt Tuning in Transformers](https://arxiv.org/abs/2509.00421)
*Maxime Meyer,Mario Michelessa,Caroline Chaux,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文从理论上分析了提示调优的记忆能力，证明Transformer的记忆信息量不能超过提示长度的线性增长，并首次形式化证明了大型语言模型中观察到的性能退化现象。


<details>
  <summary>Details</summary>
Motivation: 尽管提示调优在实证上成功适应预训练语言模型到新任务，但其理论分析仍然有限。现有理论工作主要关注通用逼近性质，本文探索Transformer的记忆能力这一不同理论方面。

Method: 通过理论分析证明两个主要贡献：1）证明Transformer记忆的信息量不能比提示长度线性增长更快；2）形式化证明Transformer在扩展上下文中的性能退化现象。

Result: 严格证明Transformer具有有限的内存，无论上下文大小如何，它们能够保留的信息量都受到限制。这揭示了Transformer架构在处理长序列时的内在局限性。

Conclusion: 研究提供了对Transformer架构内在局限性的基本理解，特别是它们处理长序列的能力有限，为理解提示调优的记忆能力提供了理论基础。

Abstract: Despite the empirical success of prompt tuning in adapting pretrained
language models to new tasks, theoretical analyses of its capabilities remain
limited. Existing theoretical work primarily addresses universal approximation
properties, demonstrating results comparable to standard weight tuning. In this
paper, we explore a different aspect of the theory of transformers: the
memorization capability of prompt tuning. We provide two principal theoretical
contributions. First, we prove that the amount of information memorized by a
transformer cannot scale faster than linearly with the prompt length. Second,
and more importantly, we present the first formal proof of a phenomenon
empirically observed in large language models: performance degradation in
transformers with extended contexts. We rigorously demonstrate that
transformers inherently have limited memory, constraining the amount of
information they can retain, regardless of the context size. This finding
offers a fundamental understanding of the intrinsic limitations of transformer
architectures, particularly their ability to handle long sequences.

</details>


### [121] [SC-GIR: Goal-oriented Semantic Communication via Invariant Representation Learning](https://arxiv.org/abs/2509.01119)
*Senura Hansaja Wanasekara,Van-Dinh Nguyen,Kok-Seng,M. -Duong Nguyen,Symeon Chatzinotas,Octavia A. Dobre*

Main category: cs.LG

TL;DR: 该文章提出了一种新的目标导向语义通信框架SC-GIR，通过自监督学习提取不变表征来优化图像传输，在保持下游任务性能的同时大幅减少数据交换量。


<details>
  <summary>Details</summary>
Motivation: 当前目标导向语义通信系统遇到了共同训练带来的重复数据交换问题，以及对标签数据集的依赖，这些限制了系统的任务无关性能。

Method: 提出SC-GIR框架，利用自监督学习提取不变表征，这种表征包含了源数据中的关键信息且下游任务无关。采用协方差基于对比学习技术获取语义密集的潜在表征。

Result: 在多个数据集上的实验表明，SC-GIR超过基线方案近百分之10，在不同SNR条件下压缩数据的分类准确率达到超过85%。

Conclusion: 该框架能够有效学习简洁且信息丰富的潜在表征，为目标导向语义通信提供了一种高效的解决方案。

Abstract: Goal-oriented semantic communication (SC) aims to revolutionize communication
systems by transmitting only task-essential information. However, current
approaches face challenges such as joint training at transceivers, leading to
redundant data exchange and reliance on labeled datasets, which limits their
task-agnostic utility. To address these challenges, we propose a novel
framework called Goal-oriented Invariant Representation-based SC (SC-GIR) for
image transmission. Our framework leverages self-supervised learning to extract
an invariant representation that encapsulates crucial information from the
source data, independent of the specific downstream task. This compressed
representation facilitates efficient communication while retaining key features
for successful downstream task execution. Focusing on machine-to-machine tasks,
we utilize covariance-based contrastive learning techniques to obtain a latent
representation that is both meaningful and semantically dense. To evaluate the
effectiveness of the proposed scheme on downstream tasks, we apply it to
various image datasets for lossy compression. The compressed representations
are then used in a goal-oriented AI task. Extensive experiments on several
datasets demonstrate that SC-GIR outperforms baseline schemes by nearly 10%,,
and achieves over 85% classification accuracy for compressed data under
different SNR conditions. These results underscore the effectiveness of the
proposed framework in learning compact and informative latent representations.

</details>


### [122] [Universal Properties of Activation Sparsity in Modern Large Language Models](https://arxiv.org/abs/2509.00454)
*Filip Szatkowski,Patryk Będkowski,Alessio Devoto,Jan Dubiński,Pasquale Minervini,Mikołaj Piórczyński,Simone Scardapane,Bartosz Wójcik*

Main category: cs.LG

TL;DR: 这篇论文提出了一个通用框架来研究大语言模型中的激活稀疏性，发现了普遍模式，并提供了利用这种稀疏性的实用指南。


<details>
  <summary>Details</summary>
Motivation: 现有的激活稀疏性研究主要基于ReLU激活函数，但现代大语言模型已放弃ReLU采用其他激活函数，导致相关研究分散且缺乏统一标准。

Method: 提出了一个通用框架来评估稀疏性稳健性，并系统研究了现代LLM中FFN层的激活稀疏现象，包括扩散型LLM。

Result: 发现了LLM中普遍的激活稀疏模式，并提供了对这种现象的深入见解。

Conclusion: 这些发现为模型设计和加速提供了实用的指南，有助于更好地利用激活稀疏性。

Abstract: Input-dependent activation sparsity is a notable property of deep learning
models, which has been extensively studied in networks with ReLU activations
and is associated with efficiency, robustness, and interpretability. However,
the approaches developed for ReLU-based models depend on exact zero activations
and do not transfer directly to modern large language models~(LLMs), which have
abandoned ReLU in favor of other activation functions. As a result, current
work on activation sparsity in LLMs is fragmented, model-specific, and lacks
consensus on which components to target. We propose a general framework to
assess sparsity robustness and present a systematic study of the phenomenon in
the FFN layers of modern LLMs, including diffusion LLMs. Our findings reveal
universal patterns of activation sparsity in LLMs, provide insights into this
phenomenon, and offer practical guidelines for exploiting it in model design
and acceleration.

</details>


### [123] [Localizing and Mitigating Memorization in Image Autoregressive Models](https://arxiv.org/abs/2509.00488)
*Aditya Kasliwal,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: 本文分析了图像自回归模型中的记忆化现象，发现不同架构的记忆模式存在差异，并提出了通过干预记忆化组件来减少数据泄露风险的方法。


<details>
  <summary>Details</summary>
Motivation: 图像自回归模型在生成图像的速度和质量方面达到了最先进水平，但也引发了对其训练数据记忆化及其对隐私影响的担忧。

Method: 通过测量细粒度记忆化，分析不同图像自回归架构中记忆化发生的位置和方式，并通过对最记忆化组件进行干预来减少数据提取能力。

Result: 发现分层逐分辨率架构的记忆化早期出现并随分辨率加深，而标准逐标记预测的自回归模型记忆化集中在后期处理阶段。通过干预可显著降低数据提取能力且对生成图像质量影响最小。

Conclusion: 研究结果提供了对图像生成模型内部行为的新见解，并指出了减轻隐私风险的实际策略。

Abstract: Image AutoRegressive (IAR) models have achieved state-of-the-art performance
in speed and quality of generated images. However, they also raise concerns
about memorization of their training data and its implications for privacy.
This work explores where and how such memorization occurs within different
image autoregressive architectures by measuring a fine-grained memorization.
The analysis reveals that memorization patterns differ across various
architectures of IARs. In hierarchical per-resolution architectures, it tends
to emerge early and deepen with resolutions, while in IARs with standard
autoregressive per token prediction, it concentrates in later processing
stages. These localization of memorization patterns are further connected to
IARs' ability to memorize and leak training data. By intervening on their most
memorizing components, we significantly reduce the capacity for data extraction
from IARs with minimal impact on the quality of generated images. These
findings offer new insights into the internal behavior of image generative
models and point toward practical strategies for mitigating privacy risks.

</details>


### [124] [Graph Convolutional Network With Pattern-Spatial Interactive and Regional Awareness for Traffic Forecasting](https://arxiv.org/abs/2509.00515)
*Xinyu Ji,Chengcheng Yan,Jibiao Yuan,Fiefie Zhao*

Main category: cs.LG

TL;DR: 提出了PSIRAGCN模型，通过模式-空间交互融合框架和区域感知图卷积网络，有效解决交通预测中多视角时空相关性建模和区域异质性处理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测研究在建模多视角时空相关性方面存在不足，忽视了交通模式与空间相关性的交互融合，且受限于空间异质性而未能考虑消息传递中的区域异质性。

Method: 提出模式-空间交互融合框架（包含模式和空间模块），从全局到局部视角捕捉模式与空间相关性；设计基于消息传递的区域感知图卷积网络，利用区域特征库重构数据驱动的消息传递。

Result: 在三个真实交通数据集上的实验表明，PSIRAGCN在平衡计算成本的同时优于现有最先进基线方法。

Conclusion: PSIRAGCN通过交互融合框架和区域感知机制，有效提升了交通预测性能，解决了多视角时空建模和区域异质性问题。

Abstract: Traffic forecasting is significant for urban traffic management, intelligent
route planning, and real-time flow monitoring. Recent advances in
spatial-temporal models have markedly improved the modeling of intricate
spatial-temporal correlations for traffic forecasting. Unfortunately, most
previous studies have encountered challenges in effectively modeling
spatial-temporal correlations across various perceptual perspectives, which
have neglected the interactive fusion between traffic patterns and spatial
correlations. Additionally, constrained by spatial heterogeneity, most studies
fail to consider distinct regional heterogeneity during message-passing. To
overcome these limitations, we propose a Pattern-Spatial Interactive and
Regional Awareness Graph Convolutional Network (PSIRAGCN) for traffic
forecasting. Specifically, we propose a pattern-spatial interactive fusion
framework composed of pattern and spatial modules. This framework aims to
capture patterns and spatial correlations by adopting a perception perspective
from the global to the local level and facilitating mutual utilization with
positive feedback. In the spatial module, we designed a graph convolutional
network based on message-passing. The network is designed to leverage a
regional characteristics bank to reconstruct data-driven message-passing with
regional awareness. Reconstructed message passing can reveal the regional
heterogeneity between nodes in the traffic network. Extensive experiments on
three real-world traffic datasets demonstrate that PSIRAGCN outperforms the
State-of-the-art baseline while balancing computational costs.

</details>


### [125] [Biological Pathway Informed Models with Graph Attention Networks (GATs)](https://arxiv.org/abs/2509.00524)
*Gavin Wong,Ping Shu Ho,Ivan Au Yeung,Ka Chun Cheung,Simon See*

Main category: cs.LG

TL;DR: 提出了基于图注意力网络（GAT）的基因通路建模框架，相比传统MLP方法在预测通路动态方面MSE降低81%，并能正确发现基因相互作用关系。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型将基因视为无结构标记或把通路当作"基因袋"处理，忽略了已知的通路结构和基因-基因相互作用信息。

Method: 使用图注意力网络（GAT）在基因级别建模通路，通过边干预编码药物机制，增强模型鲁棒性。

Result: 在未见过的治疗条件下预测通路动态时，GAT比MLP的MSE降低81%；能够从原始时间序列mRNA数据中正确重新发现TP53-MDM2-MDM4反馈环中的所有五个基因-基因相互作用。

Conclusion: GAT框架能够更好地利用生物通路的结构信息，具有直接从实验数据生成新生物学假设的潜力。

Abstract: Biological pathways map gene-gene interactions that govern all human
processes. Despite their importance, most ML models treat genes as unstructured
tokens, discarding known pathway structure. The latest pathway-informed models
capture pathway-pathway interactions, but still treat each pathway as a "bag of
genes" via MLPs, discarding its topology and gene-gene interactions. We propose
a Graph Attention Network (GAT) framework that models pathways at the gene
level. We show that GATs generalize much better than MLPs, achieving an 81%
reduction in MSE when predicting pathway dynamics under unseen treatment
conditions. We further validate the correctness of our biological prior by
encoding drug mechanisms via edge interventions, boosting model robustness.
Finally, we show that our GAT model is able to correctly rediscover all five
gene-gene interactions in the canonical TP53-MDM2-MDM4 feedback loop from raw
time-series mRNA data, demonstrating potential to generate novel biological
hypotheses directly from experimental data.

</details>


### [126] [FedThief: Harming Others to Benefit Oneself in Self-Centered Federated Learning](https://arxiv.org/abs/2509.00540)
*Xiangyu Zhang,Mang Ye*

Main category: cs.LG

TL;DR: 提出FedThief框架，实现自中心联邦学习攻击，既能降低全局模型性能，又能通过分歧感知集成技术提升攻击者私有模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习攻击策略仅破坏全局模型性能，但攻击者自身模型也会受损。现实攻击者追求自身竞争优势，需要既能破坏他人又能提升自己的攻击范式。

Method: FedThief框架：上传阶段修改内容降低全局模型性能；同时使用分歧感知集成技术整合全局更新和本地知识来增强私有模型。

Result: 实验表明该方法能有效降低全局模型性能，同时攻击者获得的集成模型显著优于全局模型。

Conclusion: 提出了自中心联邦学习攻击新范式，解决了传统攻击中攻击者自身受损的问题，实现了攻击者的竞争优势获取。

Abstract: In federated learning, participants' uploaded model updates cannot be
directly verified, leaving the system vulnerable to malicious attacks. Existing
attack strategies have adversaries upload tampered model updates to degrade the
global model's performance. However, attackers also degrade their own private
models, gaining no advantage. In real-world scenarios, attackers are driven by
self-centered motives: their goal is to gain a competitive advantage by
developing a model that outperforms those of other participants, not merely to
cause disruption. In this paper, we study a novel Self-Centered Federated
Learning (SCFL) attack paradigm, in which attackers not only degrade the
performance of the global model through attacks but also enhance their own
models within the federated learning process. We propose a framework named
FedThief, which degrades the performance of the global model by uploading
modified content during the upload stage. At the same time, it enhances the
private model's performance through divergence-aware ensemble techniques, where
"divergence" quantifies the deviation between private and global models, that
integrate global updates and local knowledge. Extensive experiments show that
our method effectively degrades the global model performance while allowing the
attacker to obtain an ensemble model that significantly outperforms the global
model.

</details>


### [127] [Advanced spectral clustering for heterogeneous data in credit risk monitoring systems](https://arxiv.org/abs/2509.00546)
*Lu Han,Mengyan Li,Jiping Qiang,Zhi Su*

Main category: cs.LG

TL;DR: 提出Advanced Spectral Clustering (ASC)方法，通过优化权重参数整合财务和文本相似度，使用特征值-轮廓优化选择特征向量，在1428家中小企业数据集上实现比基线方法高18%的轮廓分数。


<details>
  <summary>Details</summary>
Motivation: 异构数据（数值财务变量和文本记录）对信用监控带来重大挑战，需要有效整合不同类型数据进行分析。

Method: ASC方法整合财务和文本相似度，通过优化权重参数，采用新颖的特征值-轮廓优化方法选择特征向量，并在k-means、k-medians、k-medoids等多种聚类算法上进行验证。

Result: ASC在1428家中小企业数据集上实现比单类型数据基线方法高18%的轮廓分数，51%的低风险企业包含'社会招聘'术语，招聘导向的中小企业违约风险降低30%。

Conclusion: ASC通过将谱聚类理论与异构数据应用相结合，能够识别有意义的聚类模式，为更有针对性和有效的信用干预提供支持，具有鲁棒性（ΔIntra/Inter < 0.13，Δ轮廓系数 < 0.02）。

Abstract: Heterogeneous data, which encompass both numerical financial variables and
textual records, present substantial challenges for credit monitoring. To
address this issue, we propose Advanced Spectral Clustering (ASC), a method
that integrates financial and textual similarities through an optimized weight
parameter and selects eigenvectors using a novel eigenvalue-silhouette
optimization approach. Evaluated on a dataset comprising 1,428 small and
medium-sized enterprises (SMEs), ASC achieves a Silhouette score that is 18%
higher than that of a single-type data baseline method. Furthermore, the
resulting clusters offer actionable insights; for instance, 51% of low-risk
firms are found to include the term 'social recruitment' in their textual
records. The robustness of ASC is confirmed across multiple clustering
algorithms, including k-means, k-medians, and k-medoids, with
{\Delta}Intra/Inter < 0.13 and {\Delta}Silhouette Coefficient < 0.02. By
bridging spectral clustering theory with heterogeneous data applications, ASC
enables the identification of meaningful clusters, such as recruitment-focused
SMEs exhibiting a 30% lower default risk, thereby supporting more targeted and
effective credit interventions.

</details>


### [128] [Integrated Multivariate Segmentation Tree for the Analysis of Heterogeneous Credit Data in Small and Medium-Sized Enterprises](https://arxiv.org/abs/2509.00550)
*Lu Han,Xiuying Wang*

Main category: cs.LG

TL;DR: 提出IMST模型，整合财务数据和文本信息，通过矩阵分解、Lasso特征选择和多元分割树构建，在中小企业信用评估中达到88.9%准确率，优于传统决策树和其他基准模型。


<details>
  <summary>Details</summary>
Motivation: 传统决策树模型仅依赖数值变量，难以处理高维数据且无法有效整合文本信息，限制了在中小企业信用评估中的应用效果。

Method: 三阶段方法：1) 通过矩阵分解将文本数据转换为数值矩阵；2) 使用Lasso回归选择重要财务特征；3) 基于Gini指数或熵构建多元分割树，并应用最弱链接剪枝控制模型复杂度。

Result: 在1,428家中国中小企业数据集上的实验显示，IMST达到88.9%的准确率，优于基准决策树(87.4%)以及逻辑回归和SVM等传统模型，同时具有更好的可解释性和计算效率。

Conclusion: IMST框架成功整合了财务和文本数据，在中小企业信用评估中表现出色，提供了更精简的架构和增强的风险检测能力，为信用评估提供了有效的解决方案。

Abstract: Traditional decision tree models, which rely exclusively on numerical
variables, often encounter difficulties in handling high-dimensional data and
fail to effectively incorporate textual information. To address these
limitations, we propose the Integrated Multivariate Segmentation Tree (IMST), a
comprehensive framework designed to enhance credit evaluation for small and
medium-sized enterprises (SMEs) by integrating financial data with textual
sources. The methodology comprises three core stages: (1) transforming textual
data into numerical matrices through matrix factorization; (2) selecting
salient financial features using Lasso regression; and (3) constructing a
multivariate segmentation tree based on the Gini index or Entropy, with
weakest-link pruning applied to regulate model complexity. Experimental results
derived from a dataset of 1,428 Chinese SMEs demonstrate that IMST achieves an
accuracy of 88.9%, surpassing baseline decision trees (87.4%) as well as
conventional models such as logistic regression and support vector machines
(SVM). Furthermore, the proposed model exhibits superior interpretability and
computational efficiency, featuring a more streamlined architecture and
enhanced risk detection capabilities.

</details>


### [129] [An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment](https://arxiv.org/abs/2509.00560)
*Can Cui,Zilong Fu,Penghe Huang,Yuanyuan Li,Wu Deng,Dongyan Li*

Main category: cs.LG

TL;DR: 该论文提出了一种从图神经网络到Kolmogorov-Arnold网络的知识蒸馏框架SA-DSD，通过改进FR-KAN作为学生模型，结合自注意力动态采样机制，显著提升了边缘设备上的模型性能和效率。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘计算环境中，需要将图神经网络的知识高效地迁移到更轻量的模型中。传统MLP难以捕捉GNN学习的复杂邻域依赖关系，限制了在边缘设备上的性能表现。

Method: 改进Fourier KAN为FR-KAN+作为学生模型，引入可学习频率基和相移机制；构建基于师生预测一致性的边际级采样概率矩阵；设计自适应加权损失机制来缓解学生模型因缺乏显式邻域聚合导致的性能下降。

Result: 在六个真实数据集上的实验表明，SA-DSD相比三个GNN教师模型性能提升3.05%-3.62%，相比FR-KAN+模型提升15.61%；与基准模型相比，参数量减少16.96倍，推理时间降低55.75%。

Conclusion: SA-DSD框架有效解决了GNN到轻量模型的知识蒸馏问题，在保持高性能的同时显著降低了计算复杂度和推理时间，适合边缘计算环境部署。

Abstract: Knowledge distillation (KD) is crucial for deploying deep learning models in
resource-constrained edge environments, particularly within the consumer
electronics sector, including smart home devices, wearable technology, and
mobile terminals. These applications place higher demands on model compression
and inference speed, necessitating the transfer of knowledge from Graph Neural
Networks (GNNs) to more efficient Multi-Layer Perceptron (MLP) models. However,
due to their fixed activation functions and fully connected architecture, MLPs
face challenges in rapidly capturing the complex neighborhood dependencies
learned by GNNs, thereby limiting their performance in edge environments. To
address these limitations, this paper introduces an innovative from GNNs to
Kolmogorov-Arnold Networks (KANs) knowledge distillation framework-Self
Attention Dynamic Sampling Distillation (SA-DSD). This study improved Fourier
KAN (FR-KAN) and replaced MLP with the improved FR-KAN+ as the student model.
Through the incorporation of learnable frequency bases and phase-shift
mechanisms, along with algorithmic optimization, FR-KAN significantly improves
its nonlinear fitting capability while effectively reducing computational
complexity. Building on this, a margin-level sampling probability matrix, based
on teacher-student prediction consistency, is constructed, and an adaptive
weighted loss mechanism is designed to mitigate performance degradation in the
student model due to the lack of explicit neighborhood aggregation. Extensive
experiments conducted on six real-world datasets demonstrate that SA-DSD
achieves performance improvements of 3.05%-3.62% over three GNN teacher models
and 15.61% over the FR-KAN+ model. Moreover, when compared with key benchmark
models, SA-DSD achieves a 16.96x reduction in parameter count and a 55.75%
decrease in inference time.

</details>


### [130] [TranCIT: Transient Causal Interaction Toolbox](https://arxiv.org/abs/2509.00602)
*Salar Nouri,Kaidi Shao,Shervin Safavi*

Main category: cs.LG

TL;DR: TranCIT是一个开源的Python工具箱，专门用于从非平稳神经信号中量化瞬态因果交互作用，解决了传统方法对短暂神经事件分析不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理短暂神经事件时往往不足，而先进的特定事件技术缺乏在Python生态系统中的可访问实现，需要开发一个综合的分析工具来填补这一空白。

Method: TranCIT实现了全面的分析流程，包括格兰杰因果性、传递熵，以及更稳健的基于结构因果模型的动态因果强度(DCS)和相对动态因果强度(rDCS)方法，用于准确检测事件驱动的因果效应。

Result: TranCIT成功捕获了高同步状态下传统方法失效的因果关系，并在真实数据中识别出海马CA3到CA1在尖波涟漪事件期间的已知瞬态信息流。

Conclusion: 该软件包提供了一个用户友好且经过验证的解决方案，用于研究控制复杂系统的瞬态因果动力学。

Abstract: Quantifying transient causal interactions from non-stationary neural signals
is a fundamental challenge in neuroscience. Traditional methods are often
inadequate for brief neural events, and advanced, event-specific techniques
have lacked accessible implementations within the Python ecosystem. Here, we
introduce trancit (Transient Causal Interaction Toolbox), an open-source Python
package designed to bridge this gap. TranCIT implements a comprehensive
analysis pipeline, including Granger Causality, Transfer Entropy, and the more
robust Structural Causal Model-based Dynamic Causal Strength (DCS) and relative
Dynamic Causal Strength (rDCS) for accurately detecting event-driven causal
effects. We demonstrate TranCIT's utility by successfully capturing causality
in high-synchrony regimes where traditional methods fail and by identifying the
known transient information flow from hippocampal CA3 to CA1 during sharp-wave
ripple events in real-world data. The package offers a user-friendly, validated
solution for investigating the transient causal dynamics that govern complex
systems.

</details>


### [131] [RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models](https://arxiv.org/abs/2509.00614)
*Shikun Liu,Deyu Zou,Nima Shoghi,Victor Fung,Kai Liu,Pan Li*

Main category: cs.LG

TL;DR: 这篇论文研究分子图基础模型的微调方法，提出了ROFT-MOL方法结合权重插值和集成学习，在回归和分类任务上都取得了改进的性能。


<details>
  <summary>Details</summary>
Motivation: 分子图基础模型面临预训练数据集较小、下游任务数据稀缩、需要满足回归和分类多样化任务的挑战，需要更稳健的微调方法来提高模型泛化能力。

Method: 将八种微调方法分为权重基于、表征基于和部分微调三类机制，在监督和自监督预训练模型上进行广泛评测，并设计了ROFT-MOL方法结合权重提取和集成学习的优点。

Result: 通过对不同标签设置下的回归和分类任务进行全面评估，ROFT-MOL方法在两类任务上都实现了性能提升，同时保持了后处理权重插值的易用性。

Conclusion: 该研究为分子图基础模型的微调提供了价值较高的见解，ROFT-MOL方法通过结合简单和复杂方法的优势，在保持易用性的同时提高了模型的演进性能。

Abstract: In the era of foundation models, fine-tuning pre-trained models for specific
downstream tasks has become crucial. This drives the need for robust
fine-tuning methods to address challenges such as model overfitting and sparse
labeling. Molecular graph foundation models (MGFMs) face unique difficulties
that complicate fine-tuning. These models are limited by smaller pre-training
datasets and more severe data scarcity for downstream tasks, both of which
require enhanced model generalization. Moreover, MGFMs must accommodate diverse
objectives, including both regression and classification tasks. To better
understand and improve fine-tuning techniques under these conditions, we
classify eight fine-tuning methods into three mechanisms: weight-based,
representation-based, and partial fine-tuning. We benchmark these methods on
downstream regression and classification tasks across supervised and
self-supervised pre-trained models in diverse labeling settings. This extensive
evaluation provides valuable insights and informs the design of a refined
robust fine-tuning method, ROFT-MOL. This approach combines the strengths of
simple post-hoc weight interpolation with more complex weight ensemble
fine-tuning methods, delivering improved performance across both task types
while maintaining the ease of use inherent in post-hoc weight interpolation.

</details>


### [132] [TimeCopilot](https://arxiv.org/abs/2509.00616)
*Azul Garza,Reneé Rosillo*

Main category: cs.LG

TL;DR: TimeCopilot是首个开源的时间序列预测代理框架，通过统一API将多个时间序列基础模型与大型语言模型结合，实现自动化预测流程和自然语言解释


<details>
  <summary>Details</summary>
Motivation: 为了解决传统时间序列预测中需要手动选择模型、进行特征分析和交叉验证的复杂流程，同时提供可解释的预测结果和自然语言查询支持

Method: 开发了一个LLM无关的框架，结合多个时间序列基础模型(TSFMs)和大型语言模型(LLMs)，通过统一API实现自动化特征分析、模型选择、交叉验证和预测生成

Result: 在GIFT-Eval大规模基准测试中实现了最先进的概率预测性能，且成本较低

Conclusion: 该框架为可重现、可解释和易访问的代理预测系统提供了实用基础

Abstract: We introduce TimeCopilot, the first open-source agentic framework for
forecasting that combines multiple Time Series Foundation Models (TSFMs) with
Large Language Models (LLMs) through a single unified API. TimeCopilot
automates the forecasting pipeline: feature analysis, model selection,
cross-validation, and forecast generation, while providing natural language
explanations and supporting direct queries about the future. The framework is
LLM-agnostic, compatible with both commercial and open-source models, and
supports ensembles across diverse forecasting families. Results on the
large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art
probabilistic forecasting performance at low cost. Our framework provides a
practical foundation for reproducible, explainable, and accessible agentic
forecasting systems.

</details>


### [133] [Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers](https://arxiv.org/abs/2509.00631)
*Giacomo Acciarini,Simone Mestici,Halil Kelebek,Linnea Wolniewicz,Michael Vergalla,Madhulika Guhathakurta,Umaa Rebbapragada,Bala Poduval,Atılım Güneş Baydin,Frank Soboczenski*

Main category: cs.LG

TL;DR: 基于时间融合变奇器的机器学习框架，用于预测稀疏的气体电子总含量(TEC)，达到24小时预测能力，平均根方误差仅3.33 TECU


<details>
  <summary>Details</summary>
Motivation: 电离层对全球定位系统、卫星通信和低轨道运行关键，但电子总含量预测遇到稀疏观测数据和空间天气条件下经验模型准确性不足的挑战

Method: 采用时间融合变奇器(TFT)机器学习框架，整合太阳辐照度、地磁指数和GNSS气体电子总含量等异构输入源，应用预处理和时间对齐策略

Result: 在2010-2025年间的实验中，模型实现了24小时预测，平均根方误差仅3.33 TECU，太阳极紫外辐照度显示出最强预测信号

Conclusion: 该框架不仅提供高准确预测，还通过注意力机制提供可解释性，并开源了ionopy工具包以促进重现性和社区发展

Abstract: The ionosphere critically influences Global Navigation Satellite Systems
(GNSS), satellite communications, and Low Earth Orbit (LEO) operations, yet
accurate prediction of its variability remains challenging due to nonlinear
couplings between solar, geomagnetic, and thermospheric drivers. Total Electron
Content (TEC), a key ionospheric parameter, is derived from GNSS observations,
but its reliable forecasting is limited by the sparse nature of global
measurements and the limited accuracy of empirical models, especially during
strong space weather conditions. In this work, we present a machine learning
framework for ionospheric TEC forecasting that leverages Temporal Fusion
Transformers (TFT) to predict sparse ionosphere data. Our approach accommodates
heterogeneous input sources, including solar irradiance, geomagnetic indices,
and GNSS-derived vertical TEC, and applies preprocessing and temporal alignment
strategies. Experiments spanning 2010-2025 demonstrate that the model achieves
robust predictions up to 24 hours ahead, with root mean square errors as low as
3.33 TECU. Results highlight that solar EUV irradiance provides the strongest
predictive signals. Beyond forecasting accuracy, the framework offers
interpretability through attention-based analysis, supporting both operational
applications and scientific discovery. To encourage reproducibility and
community-driven development, we release the full implementation as the
open-source toolkit \texttt{ionopy}.

</details>


### [134] [Disentangling Slow and Fast Temporal Dynamics in Degradation Inference with Hierarchical Differential Models](https://arxiv.org/abs/2509.00639)
*Mengjie Zhao,Olga Fink*

Main category: cs.LG

TL;DR: 提出了一种新颖的分层控制微分方程（H-CDE）框架，通过多尺度时间积分、可学习路径变换和单调性激活函数，有效分离系统退化与运行动态，在动态响应和稳态系统中均优于传统残差方法。


<details>
  <summary>Details</summary>
Motivation: 由于系统退化难以直接观测且被运行变化主导，传统残差方法在退化估计中存在噪声和不可靠性问题，特别是在动态响应系统中。神经常微分方程虽然能推断潜在动态，但在慢快系统中存在数值刚性和训练困难。

Method: 提出分层控制微分方程框架，包含慢（退化）和快（运行）两个CDE组件：1）多尺度时间积分方案缓解数值刚性；2）可学习路径变换提取潜在退化驱动因素；3）新颖激活函数强制推断退化的单调性作为解缠正则化器。

Result: 在动态响应系统（如桥梁）和稳态系统（如航空发动机）上的综合评估表明，H-CDE能有效分离退化与运行动态，优于残差基线方法，提供更准确、鲁棒和可解释的推断。

Conclusion: H-CDE框架成功解决了系统退化推断中的关键挑战，通过创新的多尺度架构和正则化技术，实现了退化与运行动态的有效解缠，为状态监测和预测提供了更可靠的解决方案。

Abstract: Reliable inference of system degradation from sensor data is fundamental to
condition monitoring and prognostics in engineered systems. Since degradation
is rarely observable and measurable, it must be inferred to enable accurate
health assessment and decision-making. This is particularly challenging because
operational variations dominate system behavior, while degradation introduces
only subtle, long-term changes. Consequently, sensor data mainly reflect
short-term operational variability, making it difficult to disentangle the
underlying degradation process. Residual-based methods are widely employed, but
the residuals remain entangled with operational history, often resulting in
noisy and unreliable degradation estimation, particularly in systems with
dynamic responses. Neural Ordinary Equations (NODEs) offer a promising
framework for inferring latent dynamics, but the time-scale separation in
slow-fast systems introduces numerical stiffness and complicates training,
while degradation disentanglement remains difficult. To address these
limitations, we propose a novel Hierarchical Controlled Differential Equation
(H-CDE) framework that incorporates a slow (degradation) and a fast (operation)
CDE component in a unified architecture. It introduces three key innovations: a
multi-scale time integration scheme to mitigate numerical stiffness; a
learnable path transformation that extracts latent degradation drivers to
control degradation evolution; and a novel activation function that enforces
monotonicity on inferred degradation as a regularizer for disentanglement.
Through comprehensive evaluations on both dynamic response (e.g., bridges) and
steady state (e.g., aero-engine) systems, we demonstrate that H-CDE effectively
disentangles degradation from operational dynamics and outperforms
residual-based baselines, yielding more accurate, robust, and interpretable
inference.

</details>


### [135] [AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models](https://arxiv.org/abs/2509.00641)
*Zhipeng Yin,Zichong Wang,Avash Palikhe,Zhen Liu,Jun Liu,Wenbin Zhang*

Main category: cs.LG

TL;DR: AMCR框架通过系统重构风险提示、注意力相似性分析和自适应风险缓解，有效检测和减轻生成模型中的版权风险，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 生成模型在文本到图像任务中取得显著进展，但严重依赖大规模训练数据，可能无意中复制受版权保护的内容，带来法律和道德挑战。现有的基于提示的方法在处理微妙侵权情况时效果有限。

Method: 提出AMCR综合框架：1）系统重构风险提示为安全形式；2）通过注意力相似性分析检测部分侵权；3）在生成过程中自适应缓解风险。

Result: 大量实验验证了AMCR在揭示和减轻潜在版权风险方面的有效性，为生成模型的安全部署提供了实用见解和基准。

Conclusion: AMCR框架为解决生成模型中的版权问题提供了全面解决方案，能够在保持图像质量的同时有效降低侵权风险，推动生成技术的负责任部署。

Abstract: Generative models have achieved impressive results in text to image tasks,
significantly advancing visual content creation. However, this progress comes
at a cost, as such models rely heavily on large-scale training data and may
unintentionally replicate copyrighted elements, creating serious legal and
ethical challenges for real-world deployment. To address these concerns,
researchers have proposed various strategies to mitigate copyright risks, most
of which are prompt based methods that filter or rewrite user inputs to prevent
explicit infringement. While effective in handling obvious cases, these
approaches often fall short in more subtle situations, where seemingly benign
prompts can still lead to infringing outputs. To address these limitations,
this paper introduces Assessing and Mitigating Copyright Risks (AMCR), a
comprehensive framework which i) builds upon prompt-based strategies by
systematically restructuring risky prompts into safe and non-sensitive forms,
ii) detects partial infringements through attention-based similarity analysis,
and iii) adaptively mitigates risks during generation to reduce copyright
violations without compromising image quality. Extensive experiments validate
the effectiveness of AMCR in revealing and mitigating latent copyright risks,
offering practical insights and benchmarks for the safer deployment of
generative models.

</details>


### [136] [Missing Data Imputation using Neural Cellular Automata](https://arxiv.org/abs/2509.00651)
*Tin Luu,Binh Nguyen,Man Ngo*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经细胞自动机(NCA)的新颖缺失数据填补方法，通过适当的调整，NCA模型能够有效解决表格数据中的缺失值问题，并在实验证明优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 表格数据中的缺失值一直是棘手问题，虽然已有多种生成模型被用于填补任务，但神经细胞自动机(NCA)这一强大计算模型在此领域尚未被充分探索。

Method: 提出基于神经细胞自动机(NCA)的填补方法，对NCA模型进行适当调整以适应缺失数据填补任务。

Result: 实验结果表明，该方法在填补误差和填补后性能方面均优于当前最先进的填补方法。

Conclusion: 神经细胞自动机(NCA)是一种有效的缺失数据填补工具，通过适当调整可以显著提升表格数据填补的性能表现。

Abstract: When working with tabular data, missingness is always one of the most painful
problems. Throughout many years, researchers have continuously explored better
and better ways to impute missing data. Recently, with the rapid development
evolution in machine learning and deep learning, there is a new trend of
leveraging generative models to solve the imputation task. While the imputing
version of famous models such as Variational Autoencoders or Generative
Adversarial Networks were investigated, prior work has overlooked Neural
Cellular Automata (NCA), a powerful computational model. In this paper, we
propose a novel imputation method that is inspired by NCA. We show that, with
some appropriate adaptations, an NCA-based model is able to address the missing
data imputation problem. We also provide several experiments to evidence that
our model outperforms state-of-the-art methods in terms of imputation error and
post-imputation performance.

</details>


### [137] [IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India](https://arxiv.org/abs/2509.00653)
*Tung Nguyen,Harkanwar Singh,Nilay Naharas,Lucas Bandarkar,Aditya Grover*

Main category: cs.LG

TL;DR: 提出了IndiaWeatherBench基准测试，用于印度次大陆的区域天气预报研究，包含精选数据集、评估指标和多种模型基线


<details>
  <summary>Details</summary>
Motivation: 区域天气预报对气候适应和灾害缓解至关重要，但现有研究缺乏统一的数据集和实验设置，限制了公平比较和可复现性

Method: 构建了基于高分辨率区域再分析产品的精选数据集，实现了包括UNets、Transformers和图网络在内的多种架构模型，采用不同的边界条件策略和训练目标

Result: 建立了全面的基准测试框架，提供了原始和预处理数据集、模型实现和评估流程，所有资源均已开源

Conclusion: IndiaWeatherBench为推进区域天气预报研究提供了基础框架，虽然专注于印度地区，但可轻松扩展到其他地理区域

Abstract: Regional weather forecasting is a critical problem for localized climate
adaptation, disaster mitigation, and sustainable development. While machine
learning has shown impressive progress in global weather forecasting, regional
forecasting remains comparatively underexplored. Existing efforts often use
different datasets and experimental setups, limiting fair comparison and
reproducibility. We introduce IndiaWeatherBench, a comprehensive benchmark for
data-driven regional weather forecasting focused on the Indian subcontinent.
IndiaWeatherBench provides a curated dataset built from high-resolution
regional reanalysis products, along with a suite of deterministic and
probabilistic metrics to facilitate consistent training and evaluation. To
establish strong baselines, we implement and evaluate a range of models across
diverse architectures, including UNets, Transformers, and Graph-based networks,
as well as different boundary conditioning strategies and training objectives.
While focused on India, IndiaWeatherBench is easily extensible to other
geographic regions. We open-source all raw and preprocessed datasets, model
implementations, and evaluation pipelines to promote accessibility and future
development. We hope IndiaWeatherBench will serve as a foundation for advancing
regional weather forecasting research. Code is available at
https://github.com/tung-nd/IndiaWeatherBench.

</details>


### [138] [An Evolutionary Multi-objective Optimization for Replica-Exchange-based Physics-informed Operator Learning Network](https://arxiv.org/abs/2509.00663)
*Binghang Lu,Changhong Mou,Guang Lin*

Main category: cs.LG

TL;DR: 提出了一种基于进化多目标优化的副本交换物理信息算子学习网络，用于高效求解参数偏微分方程，在噪声观测数据下实现更好的精度、鲁棒性和不确定性量化


<details>
  <summary>Details</summary>
Motivation: 现有物理信息神经网络和算子学习方法在平衡算子与物理损失、噪声数据鲁棒性和不确定性量化方面存在局限，需要新的框架来解决这些问题

Method: 整合进化多目标优化自适应平衡损失函数、副本交换随机梯度Langevin动力学改善参数空间探索、内置贝叶斯不确定性量化

Result: 在一维Burgers方程和时间分数阶混合扩散波方程等测试中，该方法在精度、噪声鲁棒性和不确定性量化能力方面均优于传统算子学习方法

Conclusion: 该框架成功解决了现有方法的局限性，为参数偏微分方程求解提供了更有效的算子学习解决方案

Abstract: In this paper, we propose an evolutionary Multi-objective Optimization for
Replica-Exchange-based Physics-informed Operator learning Network, which is a
novel operator learning network to efficiently solve parametric partial
differential equations. In forward and inverse settings, this operator learning
network only admits minimum requirement of noisy observational data. While
physics-informed neural networks and operator learning approaches such as Deep
Operator Networks and Fourier Neural Operators offer promising alternatives to
traditional numerical solvers, they struggle with balancing operator and
physics losses, maintaining robustness under noisy or sparse data, and
providing uncertainty quantification. The proposed framework addresses these
limitations by integrating: (i) evolutionary multi-objective optimization to
adaptively balance operator and physics-based losses in the Pareto front; (ii)
replica exchange stochastic gradient Langevin dynamics to improve global
parameter-space exploration and accelerate convergence; and (iii) built-in
Bayesian uncertainty quantification from stochastic sampling. The proposed
operator learning method is tested numerically on several different problems
including one-dimensional Burgers equation and the time-fractional mixed
diffusion-wave equation. The results indicate that our framework consistently
outperforms the general operator learning methods in accuracy, noise
robustness, and the ability to quantify uncertainty.

</details>


### [139] [Valid Property-Enhanced Contrastive Learning for Targeted Optimization & Resampling for Novel Drug Design](https://arxiv.org/abs/2509.00684)
*Amartya Banerjee,Somnath Kar,Anirban Pal,Debabrata Maiti*

Main category: cs.LG

TL;DR: VECTOR+是一个结合对比学习和生成模型的框架，用于低数据条件下的药物分子设计，能够生成具有优异结合性能的新颖分子。


<details>
  <summary>Details</summary>
Motivation: 在低数据条件下，如何高效地引导生成模型探索药理学相关的化学空间是药物发现中的主要挑战。

Method: VECTOR+结合属性引导的表征学习和可控分子生成，适用于回归和分类任务，通过对比学习实现数据高效的化学空间探索。

Result: 在PD-L1抑制剂数据集上，生成的8374个分子中有100个超过-15.0 kcal/mol的对接阈值，最佳得分-17.6 kcal/mol优于参考抑制剂(-15.4 kcal/mol)。在激酶抑制剂上也优于现有药物。

Conclusion: VECTOR+为低数据条件下的属性条件分子设计提供了一个稳健、可扩展的方法，桥接了对比学习和生成建模，实现了可重复的AI加速发现。

Abstract: Efficiently steering generative models toward pharmacologically relevant
regions of chemical space remains a major obstacle in molecular drug discovery
under low-data regimes. We present VECTOR+: Valid-property-Enhanced Contrastive
Learning for Targeted Optimization and Resampling, a framework that couples
property-guided representation learning with controllable molecule generation.
VECTOR+ applies to both regression and classification tasks and enables
interpretable, data-efficient exploration of functional chemical space. We
evaluate on two datasets: a curated PD-L1 inhibitor set (296 compounds with
experimental $IC_{50}$ values) and a receptor kinase inhibitor set (2,056
molecules by binding mode). Despite limited training data, VECTOR+ generates
novel, synthetically tractable candidates. Against PD-L1 (PDB 5J89), 100 of
8,374 generated molecules surpass a docking threshold of $-15.0$ kcal/mol, with
the best scoring $-17.6$ kcal/mol compared to the top reference inhibitor
($-15.4$ kcal/mol). The best-performing molecules retain the conserved biphenyl
pharmacophore while introducing novel motifs. Molecular dynamics (250 ns)
confirm binding stability (ligand RMSD < $2.5$ angstroms). VECTOR+ generalizes
to kinase inhibitors, producing compounds with stronger docking scores than
established drugs such as brigatinib and sorafenib. Benchmarking against JT-VAE
and MolGPT across docking, novelty, uniqueness, and Tanimoto similarity
highlights the superior performance of our method. These results position our
work as a robust, extensible approach for property-conditioned molecular design
in low-data settings, bridging contrastive learning and generative modeling for
reproducible, AI-accelerated discovery.

</details>


### [140] [DELTA: Variational Disentangled Learning for Privacy-Preserving Data Reprogramming](https://arxiv.org/abs/2509.00693)
*Arun Vignesh Malarkkan,Haoyue Bai,Anjali Kaushik,Yanjie Fu*

Main category: cs.LG

TL;DR: 本文提出了隐私保护数据重编程（PPDR）任务和DELTA框架，通过两阶段变分解耦生成学习，在提升目标属性预测准确率的同时降低敏感属性预测准确率，实现隐私保护的特征工程。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，领域数据常包含可识别或敏感属性，受严格法规约束，需要可解释透明的特征工程。现有方法主要关注下游任务性能，往往存在隐私泄露风险。

Method: 提出DELTA两阶段框架：第一阶段使用策略引导的强化学习发现具有下游任务效用的特征变换；第二阶段采用变分LSTM seq2seq编码器-解码器，结合效用-隐私解耦潜在空间设计和对抗-因果解耦正则化，在特征生成过程中抑制隐私信号。

Result: 在八个数据集上的实验表明，DELTA将预测性能提高了约9.3%，并将隐私泄露降低了约35%。

Conclusion: DELTA框架能够实现强大且隐私感知的数据转换，有效解决了PPDR任务中的挑战，为隐私保护特征工程提供了有效解决方案。

Abstract: In real-world applications, domain data often contains identifiable or
sensitive attributes, is subject to strict regulations (e.g., HIPAA, GDPR), and
requires explicit data feature engineering for interpretability and
transparency. Existing feature engineering primarily focuses on advancing
downstream task performance, often risking privacy leakage. We generalize this
learning task under such new requirements as Privacy-Preserving Data
Reprogramming (PPDR): given a dataset, transforming features to maximize target
attribute prediction accuracy while minimizing sensitive attribute prediction
accuracy. PPDR poses challenges for existing systems: 1) generating
high-utility feature transformations without being overwhelmed by a large
search space, and 2) disentangling and eliminating sensitive information from
utility-oriented features to reduce privacy inferability. To tackle these
challenges, we propose DELTA, a two-phase variational disentangled generative
learning framework. Phase I uses policy-guided reinforcement learning to
discover feature transformations with downstream task utility, without any
regard to privacy inferability. Phase II employs a variational LSTM seq2seq
encoder-decoder with a utility-privacy disentangled latent space design and
adversarial-causal disentanglement regularization to suppress privacy signals
during feature generation. Experiments on eight datasets show DELTA improves
predictive performance by ~9.3% and reduces privacy leakage by ~35%,
demonstrating robust, privacy-aware data transformation.

</details>


### [141] [Robust Spatiotemporal Forecasting Using Adaptive Deep-Unfolded Variational Mode Decomposition](https://arxiv.org/abs/2509.00703)
*Osama Ahmad,Lukas Wesemann,Fabian Waschkowski,Zubair Khalid*

Main category: cs.LG

TL;DR: MAGN模型通过将变分模态分解转化为可训练的神经模块，显著提升了时空预测的效率和准确性，在LargeST基准上实现了85-95%的预测误差降低。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络在时空预测中存在频谱纠缠问题，而分解集成方法如VMGCN虽然提高了准确性，但存在计算效率低下和需要手动调参的局限性。

Method: 提出了模式自适应图网络(MAGN)，包括：(1)展开式VMD模块用固定深度网络替代迭代优化；(2)模式特定的可学习带宽约束自适应空间异质性并防止频谱重叠。

Result: 在LargeST基准(6,902个传感器，2.41亿观测值)上，分解时间减少了250倍，预测误差比VMGCN降低了85-95%，并优于最先进的基线方法。

Conclusion: MAGN通过将传统信号分解方法转化为可训练的神经模块，有效解决了计算效率和手动调参问题，为复杂系统的时空预测提供了更高效的解决方案。

Abstract: Accurate spatiotemporal forecasting is critical for numerous complex systems
but remains challenging due to complex volatility patterns and spectral
entanglement in conventional graph neural networks (GNNs). While
decomposition-integrated approaches like variational mode graph convolutional
network (VMGCN) improve accuracy through signal decomposition, they suffer from
computational inefficiency and manual hyperparameter tuning. To address these
limitations, we propose the mode adaptive graph network (MAGN) that transforms
iterative variational mode decomposition (VMD) into a trainable neural module.
Our key innovations include (1) an unfolded VMD (UVMD) module that replaces
iterative optimization with a fixed-depth network to reduce the decomposition
time (by 250x for the LargeST benchmark), and (2) mode-specific learnable
bandwidth constraints ({\alpha}k ) adapt spatial heterogeneity and eliminate
manual tuning while preventing spectral overlap. Evaluated on the LargeST
benchmark (6,902 sensors, 241M observations), MAGN achieves an 85-95% reduction
in the prediction error over VMGCN and outperforms state-of-the-art baselines.

</details>


### [142] [Why Pool When You Can Flow? Active Learning with GFlowNets](https://arxiv.org/abs/2509.00704)
*Renfei Zhang,Mohit Pandey,Artem Cherkasov,Martin Ester*

Main category: cs.LG

TL;DR: BALD-GFlowNet是一种生成式主动学习框架，通过GFlowNets直接采样BALD奖励比例的对象，解决了传统池式主动学习在大规模未标记数据集上的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统池式主动学习方法（如BALD）在评估大规模未标记数据集时计算成本高昂，特别是在包含数十亿样本的药物发现虚拟筛选中，这限制了其可扩展性。

Method: 提出BALD-GFlowNet框架，利用生成流网络（GFlowNets）直接按照BALD奖励比例采样对象，用生成式采样替代传统的池式获取方法。

Result: 在虚拟筛选实验中，BALD-GFlowNet实现了与标准BALD基线相当的性能，同时生成了结构更多样化的分子，且可扩展性与未标记池的大小无关。

Conclusion: BALD-GFlowNet为高效且可扩展的分子发现提供了一个有前景的方向，解决了大规模主动学习的计算瓶颈问题。

Abstract: The scalability of pool-based active learning is limited by the computational
cost of evaluating large unlabeled datasets, a challenge that is particularly
acute in virtual screening for drug discovery. While active learning strategies
such as Bayesian Active Learning by Disagreement (BALD) prioritize informative
samples, it remains computationally intensive when scaled to libraries
containing billions samples. In this work, we introduce BALD-GFlowNet, a
generative active learning framework that circumvents this issue. Our method
leverages Generative Flow Networks (GFlowNets) to directly sample objects in
proportion to the BALD reward. By replacing traditional pool-based acquisition
with generative sampling, BALD-GFlowNet achieves scalability that is
independent of the size of the unlabeled pool. In our virtual screening
experiment, we show that BALD-GFlowNet achieves a performance comparable to
that of standard BALD baseline while generating more structurally diverse
molecules, offering a promising direction for efficient and scalable molecular
discovery.

</details>


### [143] [Task-Aware Adaptive Modulation: A Replay-Free and Resource-Efficient Approach For Continual Graph Learning](https://arxiv.org/abs/2509.00735)
*Jingtao Liu,Xinming Zhang*

Main category: cs.LG

TL;DR: TAAM是一种无需重放、资源高效的持续图学习方法，通过神经突触调制器动态调节冻结骨干网络的内部计算流，解决了稳定性-可塑性困境和资源密集型预训练问题。


<details>
  <summary>Details</summary>
Motivation: 当前持续图学习方法面临两个主要问题：1)稳定性-可塑性困境 - 重放方法在两者间难以平衡且存储成本高；2)资源密集型预训练 - 领先的无重放方法严重依赖预训练骨干网络，资源负担重。

Method: 提出任务感知自适应调制(TAAM)方法，使用神经突触调制器(NSM)：1)训练时通过原型引导策略从相似过去调制器深度复制初始化新NSM；2)推理时为每个任务选择最相关的冻结NSM，在冻结GNN骨干网络中执行细粒度节点注意调制。

Result: 在六个GCIL基准数据集上的广泛实验表明，TAAM全面优于最先进的方法。

Conclusion: TAAM通过动态调制冻结骨干网络的内部计算流，而非重放数据或微调整个网络，为导航稳定性-可塑性困境开辟了新路径，实现了资源高效且性能优越的持续图学习。

Abstract: Continual Graph Learning(CGL)focuses on acquiring new knowledge while
retaining previously learned information, essential for real-world graph
applications. Current methods grapple with two main issues:1) The
Stability-Plasticity Dilemma: Replay-based methods often create an imbalance
between the Dilemma, while incurring significant storage costs.2) The
Resource-Heavy Pre-training: Leading replay-free methods critically depend on
extensively pre-trained backbones, this reliance imposes a substantial resource
burden.In this paper, we argue that the key to overcoming these challenges lies
not in replaying data or fine-tuning the entire network, but in dynamically
modulating the internal computational flow of a frozen backbone. We posit that
lightweight, task-specific modules can effectively steer a GNN's reasoning
process. Motivated by this insight, we propose Task-Aware Adaptive
Modulation(TAAM), a replay-free, resource-efficient approach that charts a new
path for navigating the stability-plasticity dilemma. TAAM's core is its Neural
Synapse Modulators(NSM), which are trained and then frozen for each task to
store expert knowledge. A pivotal prototype-guided strategy governs these
modulators: 1) For training, it initializes a new NSM by deep-copying from a
similar past modulator to boost knowledge transfer. 2) For inference, it
selects the most relevant frozen NSM for each task. These NSMs insert into a
frozen GNN backbone to perform fine-grained, node-attentive modulation of its
internal flow-different from the static perturbations of prior methods.
Extensive experiments show that TAAM comprehensively outperforms
state-of-the-art methods across six GCIL benchmark datasets. The code will be
released upon acceptance of the paper.

</details>


### [144] [Attribute Fusion-based Classifier on Framework of Belief Structure](https://arxiv.org/abs/2509.00754)
*Qiying Hu,Yingying Liang,Qianli Zhou,Witold Pedrycz*

Main category: cs.LG

TL;DR: 提出了一种改进的Dempster-Shafer理论基于属性融合分类器，通过选择性模型策略和新的BPA生成方法，在几个标准数据集上实现了平均4.84%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 传统DST基于属性融合分类器存在成员函数建模过于简化和对BPA信忱结构利用不充分的问题，影响在复杂实际场景中的效果。

Method: 1）采用选择性模型策略，结合单高斯和高斯混合模型构建成员函数；2）提出一种将可能性分布转换为BPA的新方法，通过结合标准化可能性分布得到的简单BPA来实现。同时将该方法应用于信忱K近邻分类器。

Result: 在标准数据集上的完整实验显示，提出的分类器在准确率上超过最佳现有信忱分类器，平均提升4.84%，同时保持低方差，证明了其优异效果和稳健性。

Conclusion: 该研究成功地改进了DST基于属性融合分类器的性能，通过更灵活的成员函数建模和更丰富的BPA表示方法，为复杂实际应用提供了更有效的不确定性建模方案。

Abstract: Dempster-Shafer Theory (DST) provides a powerful framework for modeling
uncertainty and has been widely applied to multi-attribute classification
tasks. However, traditional DST-based attribute fusion-based classifiers suffer
from oversimplified membership function modeling and limited exploitation of
the belief structure brought by basic probability assignment (BPA), reducing
their effectiveness in complex real-world scenarios. This paper presents an
enhanced attribute fusion-based classifier that addresses these limitations
through two key innovations. First, we adopt a selective modeling strategy that
utilizes both single Gaussian and Gaussian Mixture Models (GMMs) for membership
function construction, with model selection guided by cross-validation and a
tailored evaluation metric. Second, we introduce a novel method to transform
the possibility distribution into a BPA by combining simple BPAs derived from
normalized possibility distributions, enabling a much richer and more flexible
representation of uncertain information. Furthermore, we apply the belief
structure-based BPA generation method to the evidential K-Nearest Neighbors
classifier, enhancing its ability to incorporate uncertainty information into
decision-making. Comprehensive experiments on benchmark datasets are conducted
to evaluate the performance of the proposed attribute fusion-based classifier
and the enhanced evidential K-Nearest Neighbors classifier in comparison with
both evidential classifiers and conventional machine learning classifiers. The
results demonstrate that our proposed classifier outperforms the best existing
evidential classifier, achieving an average accuracy improvement of 4.84%,
while maintaining low variance, thus confirming its superior effectiveness and
robustness.

</details>


### [145] [Flow Matters: Directional and Expressive GNNs for Heterophilic Graphs](https://arxiv.org/abs/2509.00772)
*Arman Gupta,Govind Waghmare,Gaurav Oberoi,Nitish Srivastava*

Main category: cs.LG

TL;DR: 该论文研究了在异质图（heterophilic graphs）中结合边方向性和表达能力强的消息传递对节点分类的影响，提出了两种多项式表达能力强的GNN架构，在多个基准数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在异质图中表现不佳，因为其依赖局部同质邻域。先前研究表明建模边方向性可以提高有效同质性，同时多项式表达能力强的GNN在捕获高阶特征交互方面显示潜力。

Method: 提出了两种架构：(1) 多项式表达能力强的GAT基线模型(Poly)，(2) 方向感知变体(Dir-Poly)，分别聚合传入和传出边。两种模型都学习输入特征的置换等变高次多项式，同时保持可扩展性且不增加时间复杂度。

Result: 在五个基准异质图数据集上的实验表明，Poly模型始终优于现有基线，Dir-Poly在有内在方向性的图上（如Roman Empire）提供额外增益，达到最先进结果。在无向图上引入人工方向性并不总是有帮助。

Conclusion: 研究发现边方向和表达能力强的特征建模在异质图学习中具有互补作用，方向性消息传递的益处是上下文相关的。

Abstract: In heterophilic graphs, where neighboring nodes often belong to different
classes, conventional Graph Neural Networks (GNNs) struggle due to their
reliance on local homophilous neighborhoods. Prior studies suggest that
modeling edge directionality in such graphs can increase effective homophily
and improve classification performance. Simultaneously, recent work on
polynomially expressive GNNs shows promise in capturing higher-order
interactions among features. In this work, we study the combined effect of edge
directionality and expressive message passing on node classification in
heterophilic graphs. Specifically, we propose two architectures: (1) a
polynomially expressive GAT baseline (Poly), and (2) a direction-aware variant
(Dir-Poly) that separately aggregates incoming and outgoing edges. Both models
are designed to learn permutation-equivariant high-degree polynomials over
input features, while remaining scalable with no added time complexity.
Experiments on five benchmark heterophilic datasets show that our Poly model
consistently outperforms existing baselines, and that Dir-Poly offers
additional gains on graphs with inherent directionality (e.g., Roman Empire),
achieving state-of-the-art results. Interestingly, on undirected graphs,
introducing artificial directionality does not always help, suggesting that the
benefit of directional message passing is context-dependent. Our findings
highlight the complementary roles of edge direction and expressive feature
modeling in heterophilic graph learning.

</details>


### [146] [ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods](https://arxiv.org/abs/2509.00797)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt*

Main category: cs.LG

TL;DR: 提出了ProCause方法，通过整合多种因果推理架构和序列模型，解决了现有RealCause方法在时序依赖处理和模型架构单一方面的局限性，为规范性过程监控提供了更可靠的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有的RealCause方法在评估规范性过程监控方法时存在两个主要问题：忽略了过程数据中的时序依赖性，以及仅依赖单一的TARNet因果推理模型架构，限制了评估效果。

Method: 开发了ProCause生成方法，支持序列模型（如LSTM）和非序列模型，整合了多种因果推理架构（S-Learner、T-Learner、TARNet和集成模型）。

Result: 研究表明TARNet并非总是最佳选择，集成模型提供更一致的可靠性，当存在时序依赖时使用LSTM可以改善评估效果。通过真实世界数据分析验证了ProCause的实用性。

Conclusion: ProCause通过支持多种模型架构和时序处理能力，为规范性过程监控方法提供了更可靠和全面的评估框架，特别是在处理具有时序依赖性的过程数据时表现优异。

Abstract: Prescriptive Process Monitoring (PresPM) is the subfield of Process Mining
that focuses on optimizing processes through real-time interventions based on
event log data. Evaluating PresPM methods is challenging due to the lack of
ground-truth outcomes for all intervention actions in datasets. A generative
deep learning approach from the field of Causal Inference (CI), RealCause, has
been commonly used to estimate the outcomes for proposed intervention actions
to evaluate a new policy. However, RealCause overlooks the temporal
dependencies in process data, and relies on a single CI model architecture,
TARNet, limiting its effectiveness. To address both shortcomings, we introduce
ProCause, a generative approach that supports both sequential (e.g., LSTMs) and
non-sequential models while integrating multiple CI architectures (S-Learner,
T-Learner, TARNet, and an ensemble). Our research using a simulator with known
ground truths reveals that TARNet is not always the best choice; instead, an
ensemble of models offers more consistent reliability, and leveraging LSTMs
shows potential for improved evaluations when temporal dependencies are
present. We further validate ProCause's practical effectiveness through a
real-world data analysis, ensuring a more reliable evaluation of PresPM
methods.

</details>


### [147] [Fairness in Federated Learning: Trends, Challenges, and Opportunities](https://arxiv.org/abs/2509.00799)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 这篇综述论文探讨联邦学习中的公平性问题，分析了数据、客户端和模型等多种偏差来源，并系统评述了现有公平性技术的优缺点，最后提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但由于系统异构性导致的公平性问题（如预测偏差、准确性降低和模型收敛效率低下）严重影响了其实际应用效果。

Method: 采用文献综述方法，系统分析联邦学习中各种偏差来源（数据偏差、客户端偏差、模型偏差等），评述现有公平性技术的优缺点，并讨论相关理论基础和技术方面。

Result: 提供了联邦学习公平性问题的全面概述，包括多种公平性概念、理论基础和技术方法，以及用于定量测量公平性的评估指标。

Conclusion: 联邦学习中的公平性问题是一个关键研究领域，需要进一步研究来开发更公平的联邦学习框架，为未来研究奠定坚实基础。

Abstract: At the intersection of the cutting-edge technologies and privacy concerns,
Federated Learning (FL) with its distributed architecture, stands at the
forefront in a bid to facilitate collaborative model training across multiple
clients while preserving data privacy. However, the applicability of FL systems
is hindered by fairness concerns arising from numerous sources of heterogeneity
that can result in biases and undermine a system's effectiveness, with skewed
predictions, reduced accuracy, and inefficient model convergence. This survey
thus explores the diverse sources of bias, including but not limited to, data,
client, and model biases, and thoroughly discusses the strengths and
limitations inherited within the array of the state-of-the-art techniques
utilized in the literature to mitigate such disparities in the FL training
process. We delineate a comprehensive overview of the several notions,
theoretical underpinnings, and technical aspects associated with fairness and
their adoption in FL-based multidisciplinary environments. Furthermore, we
examine salient evaluation metrics leveraged to measure fairness
quantitatively. Finally, we envisage exciting open research directions that
have the potential to drive future advancements in achieving fairer FL
frameworks, in turn, offering a strong foundation for future research in this
pivotal area.

</details>


### [148] [XAI-Driven Machine Learning System for Driving Style Recognition and Personalized Recommendations](https://arxiv.org/abs/2509.00802)
*Feriel Amel Sellal,Ahmed Ayoub Bellachia,Meryem Malak Dif,Enguerrand De Rautlin De La Roy,Mouhamed Amine Bouchiha,Yacine Ghamri-Doudane*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的方法，用于驾驶风格分类，在保持高精度的同时提供可解释性，使用RF和XGBoost达到0.92准确率，并通过SHAP技术提供个性化安全驾驶建议。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在驾驶风格分类中表现出色但缺乏可解释性，限制了实际应用中的信任度。需要一种既能保持高精度又能提供透明解释的方法。

Method: 使用机器学习技术（Random Forest、XGBoost、SVM）和SHAP可解释性技术，基于新构建的CARLA-Drive数据集进行三分类任务。

Result: RF和XGBoost分类器在三分类任务中达到0.92的准确率，性能与深度学习模型相当，同时具有更好的可解释性。

Conclusion: 该方法在保持高性能的同时提供了透明度和可解释性，适用于智能交通系统的实际部署，能够为安全驾驶提供个性化建议。

Abstract: Artificial intelligence (AI) is increasingly used in the automotive industry
for applications such as driving style classification, which aims to improve
road safety, efficiency, and personalize user experiences. While deep learning
(DL) models, such as Long Short-Term Memory (LSTM) networks, excel at this
task, their black-box nature limits interpretability and trust. This paper
proposes a machine learning (ML)-based method that balances high accuracy with
interpretability. We introduce a high-quality dataset, CARLA-Drive, and
leverage ML techniques like Random Forest (RF), Gradient Boosting (XGBoost),
and Support Vector Machine (SVM), which are efficient, lightweight, and
interpretable. In addition, we apply the SHAP (Shapley Additive Explanations)
explainability technique to provide personalized recommendations for safer
driving. Achieving an accuracy of 0.92 on a three-class classification task
with both RF and XGBoost classifiers, our approach matches DL models in
performance while offering transparency and practicality for real-world
deployment in intelligent transportation systems.

</details>


### [149] [Crystal Structure Prediction with a Geometric Permutation-Invariant Loss Function](https://arxiv.org/abs/2509.00832)
*Emmanuel Jehanno,Romain Menegaux,Julien Mairal,Sergei Grudinin*

Main category: cs.LG

TL;DR: 提出名为SinkFast的新颖损失函数，通过可微分的Sinkhorn算法实现线性分配，显著提升有机材料晶体结构预测性能


<details>
  <summary>Details</summary>
Motivation: 解决有机材料三维晶体结构预测的挑战，现有方法依赖计算昂贵的迭代流匹配方法，需要更高效的解决方案

Method: 提出基于Sinkhorn算法的可微分线性分配方案，设计能保持置换不变性并捕捉关键几何分子特性的损失函数

Result: 在COD-Cluster17基准测试中，简单的回归方法SinkFast显著优于更复杂的流匹配方法

Conclusion: SinkFast方法为分子组装和晶体结构预测提供了更高效准确的解决方案，在计算材料科学领域具有重要应用价值

Abstract: Crystalline structure prediction remains an open challenge in materials
design. Despite recent advances in computational materials science, accurately
predicting the three-dimensional crystal structures of organic materials--an
essential first step for designing materials with targeted properties--remains
elusive. In this work, we address the problem of molecular assembly, where a
set $\mathcal{S}$ of identical rigid molecules is packed to form a crystalline
structure. Existing state-of-the-art models typically rely on computationally
expensive, iterative flow-matching approaches. We propose a novel loss function
that correctly captures key geometric molecular properties while maintaining
permutation invariance over $\mathcal{S}$. We achieve this via a differentiable
linear assignment scheme based on the Sinkhorn algorithm. Remarkably, we show
that even a simple regression using our method {\em SinkFast} significantly
outperforms more complex flow-matching approaches on the COD-Cluster17
benchmark, a curated subset of the Crystallography Open Database (COD).

</details>


### [150] [Causal SHAP: Feature Attribution with Dependency Awareness through Causal Discovery](https://arxiv.org/abs/2509.00846)
*Woon Yee Ng,Li Rong Wang,Siyuan Liu,Xiuyi Fan*

Main category: cs.LG

TL;DR: 提出了Causal SHAP框架，将因果关系整合到特征归因中，解决了SHAP在特征高度相关时错误归因的问题


<details>
  <summary>Details</summary>
Motivation: SHAP解释方法无法区分因果关系和相关关系，在特征高度相关时会产生误导性的特征重要性归因，这在医疗等高风险领域尤为重要

Method: 结合Peter-Clark算法进行因果发现和IDA算法进行因果强度量化，在保持SHAP优良特性的同时整合因果关系

Result: Causal SHAP降低了与目标仅相关而非因果关系的特征归因分数，在合成和真实数据集上验证了有效性

Conclusion: 该研究为可解释AI提供了实用的因果感知模型解释框架，特别适用于医疗等需要理解真实因果关系的领域

Abstract: Explaining machine learning (ML) predictions has become crucial as ML models
are increasingly deployed in high-stakes domains such as healthcare. While
SHapley Additive exPlanations (SHAP) is widely used for model interpretability,
it fails to differentiate between causality and correlation, often
misattributing feature importance when features are highly correlated. We
propose Causal SHAP, a novel framework that integrates causal relationships
into feature attribution while preserving many desirable properties of SHAP. By
combining the Peter-Clark (PC) algorithm for causal discovery and the
Intervention Calculus when the DAG is Absent (IDA) algorithm for causal
strength quantification, our approach addresses the weakness of SHAP.
Specifically, Causal SHAP reduces attribution scores for features that are
merely correlated with the target, as validated through experiments on both
synthetic and real-world datasets. This study contributes to the field of
Explainable AI (XAI) by providing a practical framework for causal-aware model
explanations. Our approach is particularly valuable in domains such as
healthcare, where understanding true causal relationships is critical for
informed decision-making.

</details>


### [151] [Predicting Multi-Type Talented Students in Secondary School Using Semi-Supervised Machine Learning](https://arxiv.org/abs/2509.00863)
*Xinzhe Zheng,Zhen-Qun Yang,Jiannong Cao,Jiabei Cheng*

Main category: cs.LG

TL;DR: TalentPredictor是一个半监督多模态神经网络，结合Transformer、LSTM和ANN架构，用于在中学阶段早期识别7种不同类型的天赋，准确率达到90.8%。


<details>
  <summary>Details</summary>
Motivation: 传统天赋识别方法依赖人工流程、过于关注学业成绩，且通常在高等教育阶段才进行干预，忽视了非学术天赋和早期干预机会。

Method: 使用半监督多模态神经网络（Transformer+LSTM+ANN），基于1,041名中学生的离线教育数据，将各类获奖记录聚类为天赋类别，并从多样化学习行为中提取特征。

Result: 模型取得了0.908的分类准确率和0.908的ROCAUC值，表现出色。

Conclusion: 机器学习方法能够在学生发展的早期阶段有效识别多样化天赋，具有重要的教育应用价值。

Abstract: Talent identification plays a critical role in promoting student development.
However, traditional approaches often rely on manual processes or focus
narrowly on academic achievement, and typically delaying intervention until the
higher education stage. This oversight overlooks diverse non-academic talents
and misses opportunities for early intervention. To address this gap, this
study introduces TalentPredictor, a novel semi-supervised multi-modal neural
network that combines Transformer, LSTM, and ANN architectures. This model is
designed to predict seven different talent types--academic, sport, art,
leadership, service, technology, and others--in secondary school students
within an offline educational setting. Drawing on existing offline educational
data from 1,041 local secondary students, TalentPredictor overcomes the
limitations of traditional talent identification methods. By clustering various
award records into talent categories and extracting features from students'
diverse learning behaviors, it achieves high prediction accuracy (0.908
classification accuracy, 0.908 ROCAUC). This demonstrates the potential of
machine learning to identify diverse talents early in student development.

</details>


### [152] [Tabular Diffusion Counterfactual Explanations](https://arxiv.org/abs/2509.00876)
*Wei Zhang,Brian Barr,John Paisley*

Main category: cs.LG

TL;DR: 本文提出了一种基于Gumbel-softmax分布近似的新型引导反向过程，用于为金融和社会科学中的表格数据生成反事实解释，在多个大规模信贷数据集上表现优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前反事实解释方法主要集中在计算机视觉领域，缺乏针对表格数据（特别是金融和社会科学领域）的有效解决方案，需要开发专门处理分类特征的方法。

Method: 提出基于Gumbel-softmax分布近似的引导反向过程，专门处理分类特征，研究温度参数τ的影响，并推导了Gumbel-softmax分布与近似分布之间的理论界限。

Result: 在多个大规模信贷贷款和其他表格数据集上的实验表明，该方法在可解释性、多样性、不稳定性和有效性等量化指标上优于流行的基线方法，能够生成鲁棒且现实的反事实解释。

Conclusion: 该方法为表格数据提供了有效的反事实解释解决方案，特别是在处理分类特征方面表现出色，为金融和社会科学领域的可解释机器学习提供了重要工具。

Abstract: Counterfactual explanations methods provide an important tool in the field of
{interpretable machine learning}. Recent advances in this direction have
focused on diffusion models to explain a deep classifier. However, these
techniques have predominantly focused on problems in computer vision. In this
paper, we focus on tabular data typical in finance and the social sciences and
propose a novel guided reverse process for categorical features based on an
approximation to the Gumbel-softmax distribution. Furthermore, we study the
effect of the temperature $\tau$ and derive a theoretical bound between the
Gumbel-softmax distribution and our proposed approximated distribution. We
perform experiments on several large-scale credit lending and other tabular
datasets, assessing their performance in terms of the quantitative measures of
interpretability, diversity, instability, and validity. These results indicate
that our approach outperforms popular baseline methods, producing robust and
realistic counterfactual explanations.

</details>


### [153] [An Explainable Gaussian Process Auto-encoder for Tabular Data](https://arxiv.org/abs/2509.00884)
*Wei Zhang,Brian Barr,John Paisley*

Main category: cs.LG

TL;DR: 使用高斯过程构建自动编码器生成反事实解释，减少参数数量防止过拟合，并通过新的密度估计器确保生成分布内样本


<details>
  <summary>Details</summary>
Motivation: 在高风险场景下，可解释的机器学习致力于提供黑盒模型的反事实解释。现有方法使用生成模型如自动编码器，但需要改进以提高性能

Method: 提出使用高斯过程构建自动编码器架构，并引入新的密度估计器来搜索分布内样本，同时提出了选择最优正则化率的算法

Result: 在多个大规模表格数据集上进行实验，与其他基于自动编码器的方法进行比较，结果显示方法能够生成多样化且分布内的反事实样本

Conclusion: 该方法通过高斯过程实现了更简洁的自动编码器设计，减少了可学习参数数量，有效防止了过拟合，同时通过新的密度估计技术确保了生成样本的分布一致性

Abstract: Explainable machine learning has attracted much interest in the community
where the stakes are high. Counterfactual explanations methods have become an
important tool in explaining a black-box model. The recent advances have
leveraged the power of generative models such as an autoencoder. In this paper,
we propose a novel method using a Gaussian process to construct the
auto-encoder architecture for generating counterfactual samples. The resulting
model requires fewer learnable parameters and thus is less prone to
overfitting. We also introduce a novel density estimator that allows for
searching for in-distribution samples. Furthermore, we introduce an algorithm
for selecting the optimal regularization rate on density estimator while
searching for counterfactuals. We experiment with our method in several
large-scale tabular datasets and compare with other auto-encoder-based methods.
The results show that our method is capable of generating diversified and
in-distribution counterfactual samples.

</details>


### [154] [DTRNet: Dynamic Token Routing Network to Reduce Quadratic Costs in Transformers](https://arxiv.org/abs/2509.00925)
*Aman Sharma,Saeed Najafi,Parsa Farinneya,Benyamin Jamialahmadi,Marzieh S. Tahaei,Yuhe Fan,Mehdi Rezagholizadeh,Boxing Chen,Aref Jafari*

Main category: cs.LG

TL;DR: DTRNet是一种改进的Transformer架构，通过动态令牌路由机制，让大多数令牌跳过二次注意力的计算成本，同时保持轻量级线性更新，在保持性能的同时显著降低计算量。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer在每个层对每个令牌都应用二次自注意力计算，导致计算成本高昂。需要一种更高效的架构来降低计算复杂度，同时保持模型性能。

Method: 提出动态令牌路由网络(DTRNet)，保留MLP模块，让大多数令牌跳过二次注意力的交叉令牌混合计算，仅对约10%的令牌应用完整注意力，其余令牌接收轻量级线性更新。

Result: DTRNet在保持与完整Transformer相当性能的同时，显著降低FLOPs计算量，在匹配FLOPs下比MoD和D-LLM等路由方法在准确性和内存使用方面表现更好，且效率增益随序列长度增加而提升。

Conclusion: DTRNet通过解耦令牌更新和注意力混合，大幅减少了二次计算的比例，为Transformer提供了简单、高效且可扩展的替代方案。

Abstract: Transformers achieve state-of-the-art results across many tasks, but their
uniform application of quadratic self-attention to every token at every layer
makes them computationally expensive. We introduce DTRNet (Dynamic Token
Routing Network), an improved Transformer architecture that allows tokens to
dynamically skip the quadratic cost of cross-token mixing while still receiving
lightweight linear updates. By preserving the MLP module and reducing the
attention cost for most tokens to linear, DTRNet ensures that every token is
explicitly updated while significantly lowering overall computation. This
design offers an efficient and effective alternative to standard dense
attention. Once trained, DTRNet blocks routes only ~10% of tokens through
attention at each layer while maintaining performance comparable to a full
Transformer. It consistently outperforms routing-based layer skipping methods
such as MoD and D-LLM in both accuracy and memory at matched FLOPs, while
routing fewer tokens to full attention. Its efficiency gains, scales with
sequence length, offering significant reduction in FLOPs for long-context
inputs. By decoupling token updates from attention mixing, DTRNet substantially
reduces the quadratic share of computation, providing a simple, efficient, and
scalable alternative to Transformers.

</details>


### [155] [Superposition in Graph Neural Networks](https://arxiv.org/abs/2509.00928)
*Lukas Pertl,Han Xuanyuan,Pietro Liò*

Main category: cs.LG

TL;DR: 该研究通过分析图神经网络(GNN)潜在空间中的叠加现象，发现网络宽度、池化操作和激活函数等设计选择会影响特征几何结构，提出了提升GNN可解释性的实用方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络难以解释，因为消息传递会混合信号且内部通道很少与人类概念对齐。研究旨在直接分析GNN潜在空间中的特征叠加现象。

Method: 使用具有明确图概念的受控实验，在图形级别提取类条件质心特征，在节点级别提取线性探测方向特征，并通过基础不变诊断分析其几何结构。

Result: 发现增加宽度会产生重叠相位模式；拓扑结构将重叠印记到节点级特征中；更锐利的池化操作能提高轴对齐并减少通道共享；浅层模型可能陷入亚稳态低秩嵌入。

Conclusion: 研究结果将表示几何与具体设计选择(宽度、池化和最终层激活)联系起来，为开发更可解释的GNN提供了实用方法。

Abstract: Interpreting graph neural networks (GNNs) is difficult because message
passing mixes signals and internal channels rarely align with human concepts.
We study superposition, the sharing of directions by multiple features,
directly in the latent space of GNNs. Using controlled experiments with
unambiguous graph concepts, we extract features as (i) class-conditional
centroids at the graph level and (ii) linear-probe directions at the node
level, and then analyze their geometry with simple basis-invariant diagnostics.
Across GCN/GIN/GAT we find: increasing width produces a phase pattern in
overlap; topology imprints overlap onto node-level features that pooling
partially remixes into task-aligned graph axes; sharper pooling increases axis
alignment and reduces channel sharing; and shallow models can settle into
metastable low-rank embeddings. These results connect representational geometry
with concrete design choices (width, pooling, and final-layer activations) and
suggest practical approaches for more interpretable GNNs.

</details>


### [156] [SCOUT: Toward Sub-Quadratic Attention via Segment Compression for Optimized Utility in Transformers](https://arxiv.org/abs/2509.00935)
*Aref Jafari,Yuhe Fan,Benyamin Jamialahmadi,Parsa Farinneya,Boxing Chen,Marzieh S. Tahaei*

Main category: cs.LG

TL;DR: SCOUT是一种混合架构，通过局部压缩令牌和稀疏注意力机制，在保持Transformer表达能力的同时显著降低计算和内存成本，在长序列任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的二次注意力复杂度限制了长序列的可扩展性，而线性模型如Mamba和滑动窗口注意力虽然高效，但在长序列上可能因无法保留远距离令牌的详细信息而性能下降。

Method: 提出SCOUT架构：1）使用线性局部混合器（Mamba或SWA）丰富令牌嵌入；2）对压缩的检查点令牌应用稀疏注意力而非所有先前令牌；3）通过分段压缩实现子二次复杂度增长。

Result: 在相同计算预算下，SCOUT在长上下文语言建模和推理任务中优于强基线，在4亿和13亿参数规模上匹配全注意力Transformer的语言建模和常识推理性能，同时实现更高的端到端吞吐量。

Conclusion: SCOUT通过混合局部压缩和稀疏注意力机制，在保持性能的同时显著提升了长序列处理的计算效率，为大规模序列建模提供了可扩展的解决方案。

Abstract: Transformers have demonstrated strong performance across a wide range of
sequence modeling tasks, but their quadratic attention complexity limits
scalability to long sequences. Linear models such as Mamba and sliding-window
attention (SWA) address this by mixing tokens through recurrent or localized
operations with fixed-size memory, achieving efficient inference. However,
these methods risk degrading performance on long sequences due to their
inability to retain detailed information from distant tokens. We propose SCOUT
(Segment Compression for Optimized Utility in Transformers), a hybrid
architecture that compresses tokens locally within fixed-size segments and
applies attention only over these compressed representations. Each token
embedding is first enriched via a linear local mixer, Mamba or SWA, that
integrates recent context. Then, instead of attending to all previous tokens,
each token sparsely attends to a small number of compressed checkpoint tokens
that summarize the input history. This design retains much of the expressivity
of full attention while substantially reducing the computational and memory
cost. By attending to compressed history rather than all previous tokens, SCOUT
incurs slightly higher memory than purely linear models, but its growth rate
remains sub-quadratic and far more scalable than that of full Transformers. We
analyze SCOUT's computational and memory efficiency and evaluate it empirically
on long-context language modeling and reasoning tasks. SCOUT with both Mamba
and SWA mixers outperforms strong long-sequence baselines under the same
computational budget, matches full-attention Transformers on language modeling
and common-sense reasoning tasks at 400M and 1.3B scales. Moreover, our SCOUT
achieves higher end-to-end throughput than SOTA models, while delivering
comparable results on long sequence benchmarks.

</details>


### [157] [Online Decentralized Federated Multi-task Learning With Trustworthiness in Cyber-Physical Systems](https://arxiv.org/abs/2509.00992)
*Olusola Odeyomi,Sofiat Olaosebikan,Ajibuwa Opeyemi,Oluwadoyinsola Ige*

Main category: cs.LG

TL;DR: 提出了一种在线去中心化联邦多任务学习算法，通过利用网络物理特性为邻居模型分配信任概率，在拜占庭客户端占主导的情况下实现模型个性化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据异构性导致的模型个性化挑战，特别是在在线去中心化设置下，当拜占庭客户端数量超过诚实客户端时的鲁棒性问题。

Method: 开发在线去中心化联邦多任务学习算法，利用网络物理特性（如无线系统中的接收信号强度或侧信息）为接收到的邻居本地模型分配信任概率。

Result: 仿真结果表明，所提算法在拜占庭客户端占主导的情况下，性能接近无拜占庭设置。

Conclusion: 该方法成功解决了在线去中心化联邦学习中拜占庭客户端占主导时的鲁棒性问题，为实际应用提供了有效的解决方案。

Abstract: Multi-task learning is an effective way to address the challenge of model
personalization caused by high data heterogeneity in federated learning.
However, extending multi-task learning to the online decentralized federated
learning setting is yet to be explored. The online decentralized federated
learning setting considers many real-world applications of federated learning,
such as autonomous systems, where clients communicate peer-to-peer and the data
distribution of each client is time-varying. A more serious problem in
real-world applications of federated learning is the presence of Byzantine
clients. Byzantine-resilient approaches used in federated learning work only
when the number of Byzantine clients is less than one-half the total number of
clients. Yet, it is difficult to put a limit on the number of Byzantine clients
within a system in reality. However, recent work in robotics shows that it is
possible to exploit cyber-physical properties of a system to predict clients'
behavior and assign a trust probability to received signals. This can help to
achieve resiliency in the presence of a dominating number of Byzantine clients.
Therefore, in this paper, we develop an online decentralized federated
multi-task learning algorithm to provide model personalization and resiliency
when the number of Byzantine clients dominates the number of honest clients.
Our proposed algorithm leverages cyber-physical properties, such as the
received signal strength in wireless systems or side information, to assign a
trust probability to local models received from neighbors in each iteration.
Our simulation results show that the proposed algorithm performs close to a
Byzantine-free setting.

</details>


### [158] [MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper](https://arxiv.org/abs/2509.00996)
*Runjia Zeng,Guangyan Sun,Qifan Wang,Tong Geng,Sohail Dianat,Xiaotian Han,Raghuveer Rao,Xueling Zhang,Cheng Han,Lifu Huang,Dongfang Liu*

Main category: cs.LG

TL;DR: MEPT是一种基于专家混合架构的提示调优方法，通过多个提示专家自适应学习多样化和非平稳数据分布，在SuperGLUE上优于现有参数高效方法，准确率提升1.94%同时减少79.25%的激活提示。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法参数空间刚性，无法动态激活适当的神经通路来灵活适应多样化和演化的数据分布。

Method: 提出混合专家提示调优(MEPT)框架，集成多个提示专家来自适应学习不同的数据分布，作为有效的流形映射框架。

Result: 在SuperGLUE基准测试中优于多个最先进的参数高效基线方法，平均准确率提升1.94%，同时显著减少79.25%的激活提示。

Conclusion: MEPT通过流形学习和神经激活通路可视化的理论洞察得到支持，是一种有效且高效的流形映射框架。

Abstract: Considering deep neural networks as manifold mappers, the
pretrain-then-fine-tune paradigm can be interpreted as a two-stage process:
pretrain establishes a broad knowledge base, and fine-tune adjusts the model
parameters to activate specific neural pathways to align with the target
manifold. Although prior fine-tuning approaches demonstrate success, their
rigid parameter space limits their ability to dynamically activate appropriate
neural pathways, rendering them ill-equipped to adapt flexibly to the diverse
and evolving data distributions. In light of this view, we propose a novel
approach, Mixture of Expert Prompt Tuning (MEPT), as an effective and efficient
manifold-mapping framework. MEPT leverages the Mixture of Experts architecture
by integrating multiple prompt experts to adaptively learn diverse and
non-stationary data distributions. Empirical evaluations demonstrate that MEPT
outperforms several state-of-the-art parameter efficient baselines on
SuperGLUE, achieving notable improvements in mean accuracy (e.g., 1.94%) while
significantly reducing activated prompts by 79.25%. The effectiveness of MEPT
is further supported by theoretical insights from manifold learning and
validated through neural activation pathway visualization results. Our code is
avaliable at https://github.com/runtsang/MEPT.

</details>


### [159] [Any-Order Flexible Length Masked Diffusion](https://arxiv.org/abs/2509.01025)
*Jaeyeon Kim,Lee Cheuk-Kit,Carles Domingo-Enrich,Yilun Du,Sham Kakade,Timothy Ngotiaoco,Sitan Chen,Michael Albergo*

Main category: cs.LG

TL;DR: FlexMDMs是一种灵活的掩码扩散模型，能够生成可变长度序列，同时保持任意顺序推理的灵活性，在数学和代码填充任务上表现优异


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型(MDMs)虽然具有并行推理优势，但无法支持token插入，只能生成固定长度序列，限制了其应用范围

Method: 基于随机插值框架扩展，通过插入掩码token并逐步解掩码来生成序列，支持可变长度生成和任意顺序推理

Result: FlexMDMs在困惑度上与MDMs相当，但长度统计建模更准确；在迷宫规划任务上成功率提高60%；LLaDA-8B微调后数学推理(58%→67%)和代码填充(52%→65%)性能显著提升

Conclusion: FlexMDMs成功解决了MDMs固定长度生成的限制，实现了可变长度序列建模，同时保持了并行推理优势，为扩散模型在离散领域的应用提供了新方向

Abstract: Masked diffusion models (MDMs) have recently emerged as a promising
alternative to autoregressive models over discrete domains. MDMs generate
sequences in an any-order, parallel fashion, enabling fast inference and strong
performance on non-causal tasks. However, a crucial limitation is that they do
not support token insertions and are thus limited to fixed-length generations.
To this end, we introduce Flexible Masked Diffusion Models (FlexMDMs), a
discrete diffusion paradigm that simultaneously can model sequences of flexible
length while provably retaining MDMs' flexibility of any-order inference.
Grounded in an extension of the stochastic interpolant framework, FlexMDMs
generate sequences by inserting mask tokens and unmasking them. Empirically, we
show that FlexMDMs match MDMs in perplexity while modeling length statistics
with much higher fidelity. On a synthetic maze planning task, they achieve
$\approx 60 \%$ higher success rate than MDM baselines. Finally, we show
pretrained MDMs can easily be retrofitted into FlexMDMs: on 16 H100s, it takes
only three days to fine-tune LLaDA-8B into a FlexMDM, achieving superior
performance on math (GSM8K, $58\% \to 67\%$) and code infilling performance
($52\% \to 65\%$).

</details>


### [160] [Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition](https://arxiv.org/abs/2509.01031)
*Xiaozhou Ye,Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: 提出了TPRL-DG框架，使用强化学习进行时序特征提取，解决可穿戴传感器人体活动识别中的跨用户泛化问题，无需目标用户标注即可实现优越的跨用户识别性能。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器的人体活动识别在医疗健康和智能环境中很重要，但跨用户差异性（运动模式、传感器位置、生理特征等）导致传统监督学习方法在未见用户上表现不佳，现有域泛化方法常忽略时序依赖或需要不切实际的域标签。

Method: TPRL-DG框架将特征提取重新定义为强化学习的序列决策过程，使用基于Transformer的自回归生成器产生捕获用户不变活动动态的时序token，通过多目标奖励函数优化类别区分和跨用户不变性。

Result: 在DSADS和PAMAP2数据集上的评估显示，TPRL-DG在跨用户泛化方面超越了最先进方法，无需每用户校准即可实现优越的准确率。

Conclusion: 通过学习鲁棒的用户不变时序模式，TPRL-DG实现了可扩展的人体活动识别系统，推动了个性化医疗、自适应健身追踪和情境感知环境的发展。

Abstract: Human Activity Recognition (HAR) using wearable sensors is crucial for
healthcare, fitness tracking, and smart environments, yet cross-user
variability -- stemming from diverse motion patterns, sensor placements, and
physiological traits -- hampers generalization in real-world settings.
Conventional supervised learning methods often overfit to user-specific
patterns, leading to poor performance on unseen users. Existing domain
generalization approaches, while promising, frequently overlook temporal
dependencies or depend on impractical domain-specific labels. We propose
Temporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a
novel framework that redefines feature extraction as a sequential
decision-making process driven by reinforcement learning. TPRL-DG leverages a
Transformer-based autoregressive generator to produce temporal tokens that
capture user-invariant activity dynamics, optimized via a multi-objective
reward function balancing class discrimination and cross-user invariance. Key
innovations include: (1) an RL-driven approach for domain generalization, (2)
autoregressive tokenization to preserve temporal coherence, and (3) a
label-free reward design eliminating the need for target user annotations.
Evaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses
state-of-the-art methods in cross-user generalization, achieving superior
accuracy without per-user calibration. By learning robust, user-invariant
temporal patterns, TPRL-DG enables scalable HAR systems, facilitating
advancements in personalized healthcare, adaptive fitness tracking, and
context-aware environments.

</details>


### [161] [MatPROV: A Provenance Graph Dataset of Material Synthesis Extracted from Scientific Literature](https://arxiv.org/abs/2509.01042)
*Hirofumi Tsuruta,Masaya Kumagai*

Main category: cs.LG

TL;DR: 提出了MatPROV数据集，使用PROV-DM标准从科学文献中提取材料合成程序，通过图结构捕捉复杂的因果关系，支持机器可解释的合成知识。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用刚性、领域特定的模式或假设合成程序是线性操作序列，无法捕捉真实世界程序的结构复杂性。

Method: 采用PROV-DM国际溯源信息标准，使用大语言模型从科学文献中提取合成程序，构建图结构表示。

Result: 创建了MatPROV数据集，通过直观的有向图捕捉材料、操作和条件之间的结构复杂性和因果关系。

Conclusion: 这种表示方法实现了机器可解释的合成知识，为自动化合成规划和优化等未来研究开辟了机会。

Abstract: Synthesis procedures play a critical role in materials research, as they
directly affect material properties. With data-driven approaches increasingly
accelerating materials discovery, there is growing interest in extracting
synthesis procedures from scientific literature as structured data. However,
existing studies often rely on rigid, domain-specific schemas with predefined
fields for structuring synthesis procedures or assume that synthesis procedures
are linear sequences of operations, which limits their ability to capture the
structural complexity of real-world procedures. To address these limitations,
we adopt PROV-DM, an international standard for provenance information, which
supports flexible, graph-based modeling of procedures. We present MatPROV, a
dataset of PROV-DM-compliant synthesis procedures extracted from scientific
literature using large language models. MatPROV captures structural
complexities and causal relationships among materials, operations, and
conditions through visually intuitive directed graphs. This representation
enables machine-interpretable synthesis knowledge, opening opportunities for
future research such as automated synthesis planning and optimization.

</details>


### [162] [REFINESTAT: Efficient Exploration for Probabilistic Program Synthesis](https://arxiv.org/abs/2509.01082)
*Madhav Kanda,Shubham Ugare,Sasa Misailovic*

Main category: cs.LG

TL;DR: RefineStat是一个语言模型驱动的框架，通过语义约束和诊断感知的细化来生成语法正确且统计可靠的概率程序，使小语言模型在概率编程代码生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 概率编程中的统计模型发现需要在大搜索空间中导航并满足严格的领域特定约束，小语言模型生成的概率程序经常存在语法和语义错误，如错误的推断结构。

Method: 引入RefineStat框架：1）强制执行语义约束确保程序包含有效分布和良好参数；2）当可靠性检查失败时，应用诊断感知细化重新采样先验或似然组件。

Result: 在多个概率编程代码生成任务中，RefineStat使用小语言模型生成的程序既语法正确又统计可靠，通常匹配甚至超过闭源大语言模型（如OpenAI o3）的表现。

Conclusion: RefineStat通过结合领域专业知识和调试策略，有效解决了小语言模型在概率编程中的错误问题，为统计模型发现提供了可靠的解决方案。

Abstract: Probabilistic programming offers a powerful framework for modeling
uncertainty, yet statistical model discovery in this domain entails navigating
an immense search space under strict domain-specific constraints. When small
language models are tasked with generating probabilistic programs, they
frequently produce outputs that suffer from both syntactic and semantic errors,
such as flawed inference constructs. Motivated by probabilistic programmers'
domain expertise and debugging strategies, we introduce RefineStat, a language
model--driven framework that enforces semantic constraints ensuring synthesized
programs contain valid distributions and well-formed parameters, and then
applies diagnostic-aware refinement by resampling prior or likelihood
components whenever reliability checks fail. We evaluate RefineStat on multiple
probabilistic-programming code-generation tasks using smaller language models
(SLMs) and find that it produces programs that are both syntactically sound and
statistically reliable, often matching or surpassing those from closed-source
large language models (e.g., OpenAI o3).

</details>


### [163] [A Class of Random-Kernel Network Models](https://arxiv.org/abs/2509.01090)
*James Tian*

Main category: cs.LG

TL;DR: 随机核网络：通过确定性核组合构建深度结构，仅在最外层引入随机性，证明深度网络在样本复杂度上优于浅层网络


<details>
  <summary>Details</summary>
Motivation: 探索深度随机特征模型是否能在样本复杂度上超越浅层模型，建立深度分离理论

Method: 提出随机核网络，通过确定性核组合构建深度结构，仅在最外层使用随机特征，理论分析样本复杂度

Result: 证明深度构造可以用比任何浅层对应物更少的蒙特卡洛样本来近似某些函数

Conclusion: 深度随机核网络在样本效率上具有优势，建立了样本复杂度方面的深度分离定理

Abstract: We introduce random-kernel networks, a multilayer extension of random feature
models where depth is created by deterministic kernel composition and
randomness enters only in the outermost layer. We prove that deeper
constructions can approximate certain functions with fewer Monte Carlo samples
than any shallow counterpart, establishing a depth separation theorem in sample
complexity.

</details>


### [164] [MATL-DC: A Multi-domain Aggregation Transfer Learning Framework for EEG Emotion Recognition with Domain-Class Prototype under Unseen Targets](https://arxiv.org/abs/2509.01135)
*Guangli Li,Canbiao Wu,Zhehao Zhou,Na Tian,Zhen Liang*

Main category: cs.LG

TL;DR: 提出MATL-DC多域聚合迁移学习框架，用于未见目标域的EEG情感识别，通过特征解耦、超域聚合和类原型表示实现高性能情感识别


<details>
  <summary>Details</summary>
Motivation: 当前迁移学习模型过度依赖源域和目标域数据，限制了EEG情感识别在实际应用中的推广，需要解决未见目标域的情感识别问题

Method: 设计特征解耦模块分离类不变域特征和域不变类特征；采用多域聚合机制形成超域；提取类原型表示；使用成对学习策略将分类问题转化为相似性问题

Result: 在SEED、SEED-IV和SEED-V数据集上分别达到84.70%、68.11%和61.08%的准确率，性能优于依赖源域和目标域的方法

Conclusion: MATL-DC框架有效解决了未见目标域的EEG情感识别问题，具有实际应用价值，为aBCI系统提供了可行的解决方案

Abstract: Emotion recognition based on electroencephalography (EEG) signals is
increasingly becoming a key research hotspot in affective Brain-Computer
Interfaces (aBCIs). However, the current transfer learning model greatly
depends on the source domain and target domain data, which hinder the practical
application of emotion recognition. Therefore, we propose a Multi-domain
Aggregation Transfer Learning framework for EEG emotion recognition with
Domain-Class prototype under unseen targets (MATL-DC). We design the feature
decoupling module to decouple class-invariant domain features from
domain-invariant class features from shallow features. In the model training
stage, the multi-domain aggregation mechanism aggregates the domain feature
space to form a superdomain, which enhances the characteristics of emotional
EEG signals. In each superdomain, we further extract the class prototype
representation by class features. In addition, we adopt the pairwise learning
strategy to transform the sample classification problem into the similarity
problem between sample pairs, which effectively alleviates the influence of
label noise. It is worth noting that the target domain is completely unseen
during the training process. In the inference stage, we use the trained
domain-class prototypes for inference, and then realize emotion recognition. We
rigorously validate it on the publicly available databases (SEED, SEED-IV and
SEED-V). The results show that the accuracy of MATL-DC model is 84.70\%,
68.11\% and 61.08\%, respectively. MATL-DC achieves comparable or even better
performance than methods that rely on both source and target domains. The
source code is available at https://github.com/WuCB-BCI/MATL-DC.

</details>


### [165] [Multi-Modal Machine Learning Framework for Predicting Early Recurrence of Brain Tumors Using MRI and Clinical Biomarkers](https://arxiv.org/abs/2509.01161)
*Cheng Cheng,Zeping Chen,Rui Xie,Peiyao Zheng,Xavier Wang*

Main category: cs.LG

TL;DR: 开发多模态机器学习框架，整合MRI特征和临床生物标志物，用于脑肿瘤术后早期复发预测，使用四种算法验证性能


<details>
  <summary>Details</summary>
Motivation: 准确预测脑肿瘤患者手术切除后的早期复发仍然是临床挑战，需要改进术后复发预测方法

Method: 提出多模态机器学习框架，整合结构MRI特征和临床生物标志物，使用GBM、RSF、CoxBoost和XGBoost四种算法，通过C-index、时间依赖性AUC、校准曲线和决策曲线分析验证性能

Result: 模型表现出有前景的性能

Conclusion: 该模型为风险分层和个性化随访规划提供了潜在工具

Abstract: Accurately predicting early recurrence in brain tumor patients following
surgical resection remains a clinical challenge. This study proposes a
multi-modal machine learning framework that integrates structural MRI features
with clinical biomarkers to improve postoperative recurrence prediction. We
employ four machine learning algorithms -- Gradient Boosting Machine (GBM),
Random Survival Forest (RSF), CoxBoost, and XGBoost -- and validate model
performance using concordance index (C-index), time-dependent AUC, calibration
curves, and decision curve analysis. Our model demonstrates promising
performance, offering a potential tool for risk stratification and personalized
follow-up planning.

</details>


### [166] [A Multimodal Deep Learning Framework for Early Diagnosis of Liver Cancer via Optimized BiLSTM-AM-VMD Architecture](https://arxiv.org/abs/2509.01164)
*Cheng Cheng,Zeping Chen,Xavier Wang*

Main category: cs.LG

TL;DR: 基于双向LSTM、多头注意力机制和变分模态分解的多模态深度学习框架，用于早期肝癌诊断，在真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 提高早期肝癌诊断的准确性和可解释性，利用多源异构数据包括临床特征、生化标志物和影像数据。

Method: 综合双向LSTM、多头注意力机制和变分模态分解技术，构建BiLSTM-AM-VMD框架处理多模态医疗数据。

Result: 在真实数据集上较传统机器学习和基线深度学习模型显示出更优异的预测性能。

Conclusion: 该多模态深度学习框架能够有效提高早期肝癌诊断的准确性和可解释性，具有重要的临床应用价值。

Abstract: This paper proposes a novel multimodal deep learning framework integrating
bidirectional LSTM, multi-head attention mechanism, and variational mode
decomposition (BiLSTM-AM-VMD) for early liver cancer diagnosis. Using
heterogeneous data that include clinical characteristics, biochemical markers,
and imaging-derived variables, our approach improves both prediction accuracy
and interpretability. Experimental results on real-world datasets demonstrate
superior performance over traditional machine learning and baseline deep
learning models.

</details>


### [167] [StoxLSTM: A Stochastic Extended Long Short-Term Memory Network for Time Series Forecasting](https://arxiv.org/abs/2509.01187)
*Zihao Wang,Yunjie Li,Lingmin Zan,Zheng Gong,Mengtao Zhu*

Main category: cs.LG

TL;DR: StoxLSTM是一种随机扩展长短期记忆网络，通过在xLSTM中引入随机潜在变量，将其改进为状态空间建模框架，显著提升了时间序列建模能力和预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管xLSTM网络在建模复杂时间依赖关系方面表现出色，但在处理具有未知、复杂和层次动态的现实世界数据集时，其表示能力和预测性能仍有提升空间。

Method: 提出StoxLSTM方法，在xLSTM架构中引入随机潜在变量，构建状态空间建模框架，通过专门设计的循环块来建模潜在动态演化过程。

Result: 在多个研究领域的公开基准数据集上的广泛实验表明，StoxLSTM始终优于最先进的基线方法，具有更好的鲁棒性和更强的泛化能力。

Conclusion: StoxLSTM通过将随机建模思想融入xLSTM架构，成功提升了时间序列建模的性能，为处理复杂时间动态提供了有效的解决方案。

Abstract: The Extended Long Short-Term Memory (xLSTM) network has attracted widespread
research interest due to its enhanced capability to model complex temporal
dependencies in diverse time series applications. Despite its success, there is
still potential to further improve its representational capacity and
forecasting performance, particularly on challenging real-world datasets with
unknown, intricate, and hierarchical dynamics. In this work, we propose a
stochastic xLSTM, termed StoxLSTM, that improves the original architecture into
a state space modeling framework by incorporating stochastic latent variables
within xLSTM. StoxLSTM models the latent dynamic evolution through specially
designed recurrent blocks, enabling it to effectively capture the underlying
temporal patterns and dependencies. Extensive experiments on publicly available
benchmark datasets from multiple research communities demonstrate that StoxLSTM
consistently outperforms state-of-the-art baselines with better robustness and
stronger generalization ability.

</details>


### [168] [Preserving Vector Space Properties in Dimensionality Reduction: A Relationship Preserving Loss Framework](https://arxiv.org/abs/2509.01198)
*Eddi Weinwurm,Alexander Kovalenko*

Main category: cs.LG

TL;DR: 提出Relationship Preserving Loss (RPL)损失函数，通过最小化高维数据和低维嵌入之间的关系矩阵差异来保持向量空间的正交性和线性独立性等关键属性。


<details>
  <summary>Details</summary>
Motivation: 降维过程会扭曲向量空间的关键属性（如正交性和线性独立性），这些属性对于跨模态检索、聚类和分类等任务至关重要。需要一种方法来在降维时保持这些几何关系。

Method: 使用关系保持损失(RPL)，通过比较高维数据和低维嵌入的Gram矩阵或余弦相似度矩阵的差异来训练神经网络进行非线性投影。该方法基于矩阵扰动理论推导误差界限。

Result: 初步实验表明，RPL能够在减少嵌入维度的同时，在很大程度上保持下游任务的性能，这得益于其对关键向量空间属性的保护。

Conclusion: RPL不仅适用于降维任务，还可广泛应用于跨域对齐、迁移学习、知识蒸馏、公平性和不变性、去中心化、图和流形学习以及联邦学习等领域，确保分布式嵌入保持几何一致性。

Abstract: Dimensionality reduction can distort vector space properties such as
orthogonality and linear independence, which are critical for tasks including
cross-modal retrieval, clustering, and classification. We propose a
Relationship Preserving Loss (RPL), a loss function that preserves these
properties by minimizing discrepancies between relationship matrices (e.g.,
Gram or cosine) of high-dimensional data and their low-dimensional embeddings.
RPL trains neural networks for non-linear projections and is supported by error
bounds derived from matrix perturbation theory. Initial experiments suggest
that RPL reduces embedding dimensions while largely retaining performance on
downstream tasks, likely due to its preservation of key vector space
properties. While we describe here the use of RPL in dimensionality reduction,
this loss can also be applied more broadly, for example to cross-domain
alignment and transfer learning, knowledge distillation, fairness and
invariance, dehubbing, graph and manifold learning, and federated learning,
where distributed embeddings must remain geometrically consistent.

</details>


### [169] [Geometric origin of adversarial vulnerability in deep learning](https://arxiv.org/abs/2509.01235)
*Yixiong Ren,Wenkang Du,Jianhui Zhou,Haiping Huang*

Main category: cs.LG

TL;DR: 提出几何感知深度学习框架，通过分层局部训练增强深度神经网络内部表征，实现类内紧凑性和类间分离性，提升对抗鲁棒性


<details>
  <summary>Details</summary>
Motivation: 解决深度学习训练精度与对抗鲁棒性之间的平衡挑战，探索生物智能与人工智能系统对齐的物理学习机制

Method: 几何感知深度学习框架，采用分层局部训练方法，通过能量模型和Hebbian耦合机制塑造网络内部表征

Result: 实现特征空间的流形平滑性，对白盒和黑盒攻击具有对抗鲁棒性，能够将新信息同化到现有知识结构中并减少表征干扰

Conclusion: 该框架为理解生物与人工智能系统对齐的学习物理机制提供了新视角，在保持训练精度的同时显著提升了对抗鲁棒性

Abstract: How to balance training accuracy and adversarial robustness has become a
challenge since the birth of deep learning. Here, we introduce a geometry-aware
deep learning framework that leverages layer-wise local training to sculpt the
internal representations of deep neural networks. This framework promotes
intra-class compactness and inter-class separation in feature space, leading to
manifold smoothness and adversarial robustness against white or black box
attacks. The performance can be explained by an energy model with Hebbian
coupling between elements of the hidden representation. Our results thus shed
light on the physics of learning in the direction of alignment between
biological and artificial intelligence systems. Using the current framework,
the deep network can assimilate new information into existing knowledge
structures while reducing representation interference.

</details>


### [170] [What Expressivity Theory Misses: Message Passing Complexity for GNNs](https://arxiv.org/abs/2509.01254)
*Niklas Kemper,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 本文提出Message Passing Complexity (MPC)作为替代传统表达能力理论的新框架，通过连续度量量化GNN解决任务的实际难度，弥合理论与实践的差距。


<details>
  <summary>Details</summary>
Motivation: 传统表达能力理论存在局限性：1）大多数实际任务不需要超越WL测试的表达力；2）二元分类和理想化假设无法反映GNN的实际能力。需要更实用的评估框架。

Method: 提出Message Passing Complexity (MPC)方法，这是一个连续度量指标，量化GNN架构通过消息传递解决特定任务的难度，同时捕捉过压缩等实际限制。

Result: 在基础GNN任务上的广泛验证表明，MPC的理论预测与实证性能相关，成功解释了架构的成功与失败案例。

Conclusion: MPC超越了表达能力理论，提供了一个更强大、更细致入微的框架来理解和改进GNN架构，有效缩小了理论与实践之间的差距。

Abstract: Expressivity theory, characterizing which graphs a GNN can distinguish, has
become the predominant framework for analyzing GNNs, with new models striving
for higher expressivity. However, we argue that this focus is misguided: First,
higher expressivity is not necessary for most real-world tasks as these tasks
rarely require expressivity beyond the basic WL test. Second, expressivity
theory's binary characterization and idealized assumptions fail to reflect
GNNs' practical capabilities. To overcome these limitations, we propose Message
Passing Complexity (MPC): a continuous measure that quantifies the difficulty
for a GNN architecture to solve a given task through message passing. MPC
captures practical limitations like over-squashing while preserving the
theoretical impossibility results from expressivity theory, effectively
narrowing the gap between theory and practice. Through extensive validation on
fundamental GNN tasks, we show that MPC's theoretical predictions correlate
with empirical performance, successfully explaining architectural successes and
failures. Thereby, MPC advances beyond expressivity theory to provide a more
powerful and nuanced framework for understanding and improving GNN
architectures.

</details>


### [171] [Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks](https://arxiv.org/abs/2509.01257)
*Andrea Fox,Francesco De Pellegrini,Eitan Altman*

Main category: cs.LG

TL;DR: 边缘计算系统中的分布式多自主组代理协调框架，通过共享约束向量实现轻量级协调，避免额外通信开销


<details>
  <summary>Details</summary>
Motivation: 解决边缘计算系统中多自主组代理在观察能力和通信受限条件下的资源竞争问题，免除现有集中式MARL方法的缺陷

Method: 每个代理解决约束马尔可夫决策过程(CMDP)，通过共享约束向量进行隐式协调，使用安全强化学习学习策略

Result: 在大规模场景下表现超过集中式和独立基线方法，实验验证了方案的有效性

Conclusion: 该分布式框架能够在最小化通信开销的情况下实现全局目标对齐，为边缘计算系统提供了可扩展的协调解决方案

Abstract: In edge computing systems, autonomous agents must make fast local decisions
while competing for shared resources. Existing MARL methods often resume to
centralized critics or frequent communication, which fail under limited
observability and communication constraints. We propose a decentralized
framework in which each agent solves a constrained Markov decision process
(CMDP), coordinating implicitly through a shared constraint vector. For the
specific case of offloading, e.g., constraints prevent overloading shared
server resources. Coordination constraints are updated infrequently and act as
a lightweight coordination mechanism. They enable agents to align with global
resource usage objectives but require little direct communication. Using safe
reinforcement learning, agents learn policies that meet both local and global
goals. We establish theoretical guarantees under mild assumptions and validate
our approach experimentally, showing improved performance over centralized and
independent baselines, especially in large-scale settings.

</details>


### [172] [Iterative In-Context Learning to Enhance LLMs Abstract Reasoning: The Case-Study of Algebraic Tasks](https://arxiv.org/abs/2509.01267)
*Stefano Fioravanti,Matteo Zavatteri,Roberto Confalonieri,Kamyar Zeinalipour,Paolo Frazzetto,Alessandro Sperduti,Nicolò Navarin*

Main category: cs.LG

TL;DR: 提出了一种迭代示例选择的上下文学习方法，通过逐步构建优化的少样本示例集来提升LLMs在代数表达式简化等需要系统性泛化任务上的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在需要组合规则和分布外示例的推理任务中面临系统性泛化挑战，特别是在数学推理方面表现有限。

Method: 采用迭代示例选择策略，逐步构建针对特定任务优化的少样本示例集，结合显式推理指令来增强模型性能。

Result: 实验表明LLMs在非标准代数简化任务中表现有限，但通过该方法显著改善了泛化性能，且简单示例比复杂示例效果更好。

Conclusion: 迭代示例选择策略能有效提升LLMs的系统性泛化能力，特别是在数学推理任务中，简单示例的选择策略比复杂示例更有效。

Abstract: LLMs face significant challenges in systematic generalization, particularly
when dealing with reasoning tasks requiring compositional rules and handling
out-of-distribution examples. To address these challenges, we introduce an
in-context learning methodology that improves the generalization capabilities
of general purpose LLMs. Our approach employs an iterative example selection
strategy, which incrementally constructs a tailored set of few-shot examples
optimized to enhance model's performance on a given task. As a proof of
concept, we apply this methodology to the resolution of algebraic expressions
involving non-standard simplification rules, according to which the priority of
addition and multiplication is changed.
  Our findings indicate that LLMs exhibit limited proficiency in these
mathematical tasks. We further demonstrate that LLMs reasoning benefits from
our iterative shot selection prompting strategy integrated with explicit
reasoning instructions. Crucially, our experiments reveal that some LLMs
achieve better generalization performances when prompted with simpler few-shot
examples rather than complex ones following the test data distribution.

</details>


### [173] [Building surrogate models using trajectories of agents trained by Reinforcement Learning](https://arxiv.org/abs/2509.01285)
*Julen Cestero,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出使用强化学习策略来高效采样确定性仿真环境的新方法，相比传统采样方法在广泛状态空间中表现更优


<details>
  <summary>Details</summary>
Motivation: 解决在计算昂贵的仿真环境中，面对广阔状态空间时现有采样策略效率不足的问题

Method: 使用强化学习训练的策略进行采样，包括随机智能体、专家智能体和最大化状态转移分布熵的探索智能体，并与拉丁超立方采样和主动学习克里金法进行对比验证

Result: 混合数据集（包含随机、专家和最大熵探索智能体的样本）在所有数据集中获得最佳评分，对状态空间表示至关重要

Conclusion: 该方法改进了现有技术水平，为在复杂仿真器上应用代理辅助的强化学习策略优化开辟了道路

Abstract: Sample efficiency in the face of computationally expensive simulations is a
common concern in surrogate modeling. Current strategies to minimize the number
of samples needed are not as effective in simulated environments with wide
state spaces. As a response to this challenge, we propose a novel method to
efficiently sample simulated deterministic environments by using policies
trained by Reinforcement Learning. We provide an extensive analysis of these
surrogate-building strategies with respect to Latin-Hypercube sampling or
Active Learning and Kriging, cross-validating performances with all sampled
datasets. The analysis shows that a mixed dataset that includes samples
acquired by random agents, expert agents, and agents trained to explore the
regions of maximum entropy of the state transition distribution provides the
best scores through all datasets, which is crucial for a meaningful state space
representation. We conclude that the proposed method improves the
state-of-the-art and clears the path to enable the application of
surrogate-aided Reinforcement Learning policy optimization strategies on
complex simulators.

</details>


### [174] [Equivariant U-Shaped Neural Operators for the Cahn-Hilliard Phase-Field Model](https://arxiv.org/abs/2509.01293)
*Xiao Xue,M. F. P. ten Eikelder,Tianyue Yang,Yiqing Li,Kan He,Shuo Wang,Peter V. Coveney*

Main category: cs.LG

TL;DR: 提出了一种等变U形神经算子(E-UNO)，用于高效学习Cahn-Hilliard方程控制的相分离动力学，通过结合全局谱卷积和多分辨率架构，在保持物理对称性的同时实现准确预测。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高且缺乏灵活性，现有神经算子架构难以捕捉多尺度行为并忽略物理对称性，需要开发更高效且物理一致的替代模型。

Method: 采用等变U形神经算子架构，结合全局谱卷积和多分辨率U形结构，通过调节平移等变性来对齐底层物理规律，从短期历史动态学习相场变量的演化。

Result: E-UNO在精细尺度和高频结构上优于标准傅里叶神经算子和U形神经算子基线，具有更好的泛化能力、更少的训练数据需求，并能产生物理一致的动力学行为。

Conclusion: E-UNO被确立为复杂相场系统的高效替代模型，通过编码对称性和尺度层次结构，为材料科学和软物质中的界面动力学提供了有效的计算工具。

Abstract: Phase separation in binary mixtures, governed by the Cahn-Hilliard equation,
plays a central role in interfacial dynamics across materials science and soft
matter. While numerical solvers are accurate, they are often computationally
expensive and lack flexibility across varying initial conditions and
geometries. Neural operators provide a data-driven alternative by learning
solution operators between function spaces, but current architectures often
fail to capture multiscale behavior and neglect underlying physical symmetries.
Here we show that an equivariant U-shaped neural operator (E-UNO) can learn the
evolution of the phase-field variable from short histories of past dynamics,
achieving accurate predictions across space and time. The model combines global
spectral convolution with a multi-resolution U-shaped architecture and
regulates translation equivariance to align with the underlying physics. E-UNO
outperforms standard Fourier neural operator and U-shaped neural operator
baselines, particularly on fine-scale and high-frequency structures. By
encoding symmetry and scale hierarchy, the model generalizes better, requires
less training data, and yields physically consistent dynamics. This establishes
E-UNO as an efficient surrogate for complex phase-field systems.

</details>


### [175] [Towards Trustworthy Vital Sign Forecasting: Leveraging Uncertainty for Prediction Intervals](https://arxiv.org/abs/2509.01319)
*Li Rong Wang,Thomas C. Henderson,Yew Soon Ong,Yih Yng Ng,Xiuyi Fan*

Main category: cs.LG

TL;DR: 本文提出了两种基于重构不确定性估计(RUE)的预测区间方法，用于生命体征预测中的不确定性量化，以提高临床决策的可信度。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在生命体征预测中显示出潜力，但由于缺乏可靠的不确定性量化（特别是校准的预测区间），临床医生难以信任和解释模型输出，这限制了在医疗健康领域的部署。

Method: 提出了两种方法：1）参数化方法-假设预测误差和不确定性估计遵循高斯copula分布，实现闭式预测区间计算；2）非参数化方法-基于K近邻算法，使用相似的验证实例经验估计条件误差分布。

Result: 在两个大型公共数据集上的实验表明，高斯copula方法在低频数据上持续优于一致性预测基线，而KNN方法在高频数据上表现最佳。

Conclusion: RUE衍生的预测区间在提供可解释、不确定性感知的生命体征预测方面具有临床前景，能够支持更可靠的临床决策。

Abstract: Vital signs, such as heart rate and blood pressure, are critical indicators
of patient health and are widely used in clinical monitoring and
decision-making. While deep learning models have shown promise in forecasting
these signals, their deployment in healthcare remains limited in part because
clinicians must be able to trust and interpret model outputs. Without reliable
uncertainty quantification -- particularly calibrated prediction intervals
(PIs) -- it is unclear whether a forecasted abnormality constitutes a
meaningful warning or merely reflects model noise, hindering clinical
decision-making. To address this, we present two methods for deriving PIs from
the Reconstruction Uncertainty Estimate (RUE), an uncertainty measure
well-suited to vital-sign forecasting due to its sensitivity to data shifts and
support for label-free calibration. Our parametric approach assumes that
prediction errors and uncertainty estimates follow a Gaussian copula
distribution, enabling closed-form PI computation. Our non-parametric approach,
based on k-nearest neighbours (KNN), empirically estimates the conditional
error distribution using similar validation instances. We evaluate these
methods on two large public datasets with minute- and hour-level sampling,
representing high- and low-frequency health signals. Experiments demonstrate
that the Gaussian copula method consistently outperforms conformal prediction
baselines on low-frequency data, while the KNN approach performs best on
high-frequency data. These results underscore the clinical promise of
RUE-derived PIs for delivering interpretable, uncertainty-aware vital sign
forecasts.

</details>


### [176] [Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.01321)
*Xinyu Tang,Zhenduo Zhang,Yurou Liu,Wayne Xin Zhao,Zujie Wen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: DEPO是一种数据高效策略优化方法，通过离线数据筛选和在线探索性过滤，显著减少推理模型训练所需的数据量和计算成本，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习方法需要大量计算和数据集，导致训练成本高、数据效率低，需要开发更高效的数据选择策略。

Method: 提出DEPO管道：1）离线阶段基于多样性、影响力和适当难度筛选高质量训练样本；2）在线阶段使用样本级可探索性指标动态过滤低探索潜力样本；3）引入重放机制确保充分训练。

Result: 在五个推理基准测试中均优于现有方法，仅使用20%训练数据就在AIME24上实现1.85倍加速，在AIME25上实现1.66倍加速。

Conclusion: DEPO通过高效的数据选择策略显著提高了推理模型的训练效率和性能，为大规模推理模型的训练提供了更经济的解决方案。

Abstract: Recent advances in large reasoning models have leveraged reinforcement
learning with verifiable rewards (RLVR) to improve reasoning capabilities.
However, scaling these methods typically requires extensive rollout computation
and large datasets, leading to high training costs and low data efficiency. To
mitigate this issue, we propose DEPO, a Data-Efficient Policy Optimization
pipeline that combines optimized strategies for both offline and online data
selection. In the offline phase, we curate a high-quality subset of training
samples based on diversity, influence, and appropriate difficulty. During
online RLVR training, we introduce a sample-level explorability metric to
dynamically filter samples with low exploration potential, thereby reducing
substantial rollout computational costs. Furthermore, we incorporate a replay
mechanism for under-explored samples to ensure adequate training, which
enhances the model's final convergence performance. Experiments across five
reasoning benchmarks show that DEPO consistently outperforms existing methods
in both offline and online data selection scenarios. Notably, using only 20% of
the training data, our approach achieves a 1.85 times speed-up on AIME24 and a
1.66 times speed-up on AIME25 compared to GRPO trained on the full dataset.

</details>


### [177] [Multitask Battery Management with Flexible Pretraining](https://arxiv.org/abs/2509.01323)
*Hong Lu,Jiali Chen,Jingzhao Zhang,Guannan He,Xuebing Han,Minggao Ouyang*

Main category: cs.LG

TL;DR: FMAE是一个灵活的预训练框架，能够处理缺失的电池数据通道并捕捉数据片段间的相互关联，为多种电池管理任务提供统一表示，显著减少数据和工程需求。


<details>
  <summary>Details</summary>
Motivation: 工业级电池管理涉及多种任务，每种任务使用不同时间尺度、传感器分辨率和数据通道的数据。构建任务特定方法需要大量数据和工程努力，限制了智能电池管理的可扩展性。

Method: 提出灵活掩码自编码器（FMAE）框架，能够学习缺失电池数据通道并捕捉数据片段间的相互关联，从异构数据中学习统一电池表示。

Result: FMAE在5个电池管理任务和11个电池数据集上始终优于所有任务特定方法。在剩余寿命预测任务中，使用50倍少的推理数据仍保持最先进结果。即使真实数据缺少某些信息（如系统电压），FMAE仍能应用且性能影响很小。

Conclusion: FMAE展示了实现灵活、数据高效模型的实用途径，简化了动态系统的现实世界多任务管理。

Abstract: Industrial-scale battery management involves various types of tasks, such as
estimation, prediction, and system-level diagnostics. Each task employs
distinct data across temporal scales, sensor resolutions, and data channels.
Building task-specific methods requires a great deal of data and engineering
effort, which limits the scalability of intelligent battery management. Here we
present the Flexible Masked Autoencoder (FMAE), a flexible pretraining
framework that can learn with missing battery data channels and capture
inter-correlations across data snippets. FMAE learns unified battery
representations from heterogeneous data and can be adopted by different tasks
with minimal data and engineering efforts. Experimentally, FMAE consistently
outperforms all task-specific methods across five battery management tasks with
eleven battery datasets. On remaining life prediction tasks, FMAE uses 50 times
less inference data while maintaining state-of-the-art results. Moreover, when
real-world data lack certain information, such as system voltage, FMAE can
still be applied with marginal performance impact, achieving comparable results
with the best hand-crafted features. FMAE demonstrates a practical route to a
flexible, data-efficient model that simplifies real-world multi-task management
of dynamical systems.

</details>


### [178] [Globally aware optimization with resurgence](https://arxiv.org/abs/2509.01329)
*Wei Bu*

Main category: cs.LG

TL;DR: 基于复变分析的复苏理论的全局优化框架，通过分析统计力学分配函数的比约展开来提取目标函数地形的全局结构信息


<details>
  <summary>Details</summary>
Motivation: 解决局部梯度优化方法缺乏全局信息、容易进入次优解并且对初始化敏感的根本挑战

Method: 计算统计力学分配函数Z(g)，提取其比约展开系数，通过Borel变换识别奇异点来确定对应于目标函数临界值的关键值

Result: 算法能够从分散的比约级数中提取准确的全局结构信息，识别出所有关键的目标函数值

Conclusion: 该方法为局部优化器提供理论基础牢固的全局指导，能够实现原理性的学习率调整和逃离次优解区域

Abstract: Modern optimization faces a fundamental challenge: local gradient-based
methods provide no global information about the objective function $L$
landscape, often leading to suboptimal convergence and sensitivity to
initialization. We introduce a novel optimization framework that leverages
resurgence theory from complex analysis to extract global structural
information from divergent asymptotic series. Our key insight is that the
factorially divergent perturbative expansions of parameter space partition
functions encode precise information about all critical objective function
value in the landscape through their Borel transform singularities.
  The algorithm works by computing the statistical mechanical partition
function $Z(g) = \int e^{-L(\theta)/g} d\theta$ for small coupling $g\ll 1$,
extracting its asymptotic series coefficients, and identifying Borel plane
singularities that correspond one-to-one with critical objective function
values. These target values provide global guidance to local optimizers,
enabling principled learning rate adaptation and escape from suboptimal
regions. Unlike heuristic adaptive methods, targets are theoretically grounded
in the geometry of the optimization landscape.

</details>


### [179] [AT Loss: Advanced Torrential Loss Function for Precipitation Forecasting](https://arxiv.org/abs/2509.01348)
*Jaeho Choi,Hyeri Kim,Kwang-Ho Kim,Jaesung Lee*

Main category: cs.LG

TL;DR: 本文提出一种基于QUBO的可微分极端降水预报损失函数，解决了传统CSI指标在长时间无降水期间失效的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的降水预报模型主要依赖标准损失函数或CSI优化，但CSI在长时间无降水期间效果差，需要更有效的优化准则。

Method: 通过简单惩罚表达式将问题重构为QUBO形式，然后通过近似处理松弛为可微分的AT损失函数。

Result: 新的AT损失函数在Lipschitz常数、预报性能评估、一致性实验和消融研究中都显示出优势性能。

Conclusion: 提出的AT损失函数有效解决了降水预报中的极端事件优化问题，为机器学习预报模型提供了更有效的优化方法。

Abstract: Accurate precipitation forecasting is becoming increasingly important in the
context of climate change. In response, machine learning-based approaches have
recently gained attention as an emerging alternative to traditional methods
such as numerical weather prediction and climate models. Nonetheless, many
recent approaches still rely on off-the-shelf loss functions, and even the more
advanced ones merely involve optimization processes based on the critical
success index (CSI). The problem, however, is that CSI may become ineffective
during extended dry periods when precipitation remains below the threshold,
rendering it less than ideal as a criterion for optimization. To address this
limitation, we introduce a simple penalty expression and reinterpret it as a
quadratic unconstrained binary optimization (QUBO) formulation. Ultimately, the
resulting QUBO formulation is relaxed into a differentiable advanced torrential
(AT) loss function through an approximation process. The proposed AT loss
demonstrates its superiority through the Lipschitz constant, forecast
performance evaluations, consistency experiments, and ablation studies with the
operational model.

</details>


### [180] [Causal Sensitivity Identification using Generative Learning](https://arxiv.org/abs/2509.01352)
*Soma Bandyopadhyay,Sudeshna Sarkar*

Main category: cs.LG

TL;DR: 提出了一种基于条件变分自编码器(CVAE)的因果影响分析方法，通过干预和反事实视角识别因果敏感特征，用于预测任务并减少混淆偏差。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法可能受到混淆因素的影响，需要识别真正的因果特征来提升预测性能和理解特征间的因果关系。

Method: 使用条件变分自编码器(CVAE)进行因果影响分析，从干预视角识别因果敏感特征，从反事实视角评估原因变化对结果的影响。

Result: 在GeoLife数据集和Asia贝叶斯网络基准测试中验证了方法的有效性，能够识别因果影响并提升预测性能，特别是在用户时空轨迹位置推荐任务中。

Conclusion: 该方法成功地将因果推理与生成模型结合，能够有效识别因果敏感特征，减少混淆偏差，并在预测任务中展现出优越性能。

Abstract: In this work, we propose a novel generative method to identify the causal
impact and apply it to prediction tasks. We conduct causal impact analysis
using interventional and counterfactual perspectives. First, applying
interventions, we identify features that have a causal influence on the
predicted outcome, which we refer to as causally sensitive features, and
second, applying counterfactuals, we evaluate how changes in the cause affect
the effect. Our method exploits the Conditional Variational Autoencoder (CVAE)
to identify the causal impact and serve as a generative predictor. We are able
to reduce confounding bias by identifying causally sensitive features. We
demonstrate the effectiveness of our method by recommending the most likely
locations a user will visit next in their spatiotemporal trajectory influenced
by the causal relationships among various features. Experiments on the
large-scale GeoLife [Zheng et al., 2010] dataset and the benchmark Asia
Bayesian network validate the ability of our method to identify causal impact
and improve predictive performance.

</details>


### [181] [DPF-CM: A Data Processing Framework with Privacy-Preserving Vector Databases for Chinese Medical LLMs Training and Deployment](https://arxiv.org/abs/2509.01354)
*Wei Huang,Anda Cheng,Zhao Zhang,Yinggui Wang*

Main category: cs.LG

TL;DR: DPF-CM是一个针对中文医疗大语言模型训练和部署的综合性数据处理框架，包含训练数据处理和隐私保护两个核心模块，显著提升模型性能并减少隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 当前开源的中文医疗语言模型训练流程主要关注训练方法优化，但缺乏对训练数据处理的全面探索，特别是在指令内容不足和隐私保护方面存在明显不足。

Method: 提出DPF-CM框架，包含两个核心模块：(1)训练数据处理流水线，采用链式示例上下文学习策略生成问题导向指令，以及基于集成的偏好数据筛选机制；(2)部署隐私保护模块，通过隐私保护向量数据库(PPVD)方法，包括模型记忆搜索、高风险数据库构建、安全数据库构建和匹配替换四个阶段。

Result: 实验结果表明，DPF-CM显著提高了模型准确性，使训练的中文医疗LLM在开源模型中达到最先进性能，同时将训练数据隐私泄露减少了27%。

Conclusion: DPF-CM框架有效解决了中文医疗LLM训练中的数据质量和隐私保护问题，为医疗领域大语言模型的训练和部署提供了全面的数据处理解决方案。

Abstract: Current open-source training pipelines for Chinese medical language models
predominantly emphasize optimizing training methodologies to enhance the
performance of large language models (LLMs), yet lack comprehensive exploration
into training data processing. To address this gap, we propose DPF-CM, a
holistic Data Processing Framework for Chinese Medical LLMs training and
deployment. DPF-CM comprises two core modules. The first module is a data
processing pipeline tailored for model training. Beyond standard data
processing operations, we (1) introduce a chained examples context-learning
strategy to generate question-oriented instructions to mitigate the lack of
instruction content, and (2) implement an ensemble-based filtering mechanism
for preference data curation that averages multiple reward models to suppress
noisy samples. The second module focuses on privacy preservation during model
deployment. To prevent privacy risks from the inadvertent exposure of training
data, we propose a Privacy Preserving Vector Database (PPVD) approach, which
involves model memory search, high-risk database construction, secure database
construction, and match-and-replace, four key stages to minimize privacy
leakage during inference collectively. Experimental results show that DPF-CM
significantly improves model accuracy, enabling our trained Chinese medical LLM
to achieve state-of-the-art performance among open-source counterparts.
Moreover, the framework reduces training data privacy leakage by 27%.

</details>


### [182] [CbLDM: A Diffusion Model for recovering nanostructure from pair distribution function](https://arxiv.org/abs/2509.01370)
*Jiarui Cao,Zhiyang Zhang,Heming Wang,Jun Xu,Ling Lan,Ran Gu*

Main category: cs.LG

TL;DR: 本文提出CbLDM模型，一种基于条件潜在扩散模型的方法，用于解决纳米结构逆问题，通过PDF数据恢复纳米结构，显著提高了预测准确性和生成效率。


<details>
  <summary>Details</summary>
Motivation: 纳米结构逆问题是理解纳米材料结构与性能关系的重要课题。传统方法在从PDF数据恢复纳米结构方面存在效率低和精度不足的问题，需要更有效的解决方案。

Method: 提出CbLDM（Condition-based Latent Diffusion Model）模型：1）基于潜在扩散模型，利用条件先验估计条件后验分布p(z|x)；2）减少扩散模型采样步骤，提高生成效率；3）使用拉普拉斯矩阵替代距离矩阵来恢复纳米结构，降低重构误差。

Result: 与现有解决纳米结构逆问题的模型相比，CbLDM展现出显著更高的预测准确性，证明了其在解决此类问题上的有效性。

Conclusion: CbLDM模型成功解决了纳米结构逆问题，不仅在该领域表现出色，还显示出处理其他连续条件生成任务的潜力，为相关研究提供了新的有效工具。

Abstract: Nowadays, the nanostructure inverse problem is an attractive problem that
helps researchers to understand the relationship between the properties and the
structure of nanomaterials. This article focuses on the problem of using PDF to
recover the nanostructure, which this article views as a conditional generation
problem. This article propose a deep learning model CbLDM, Condition-based
Latent Diffusion Model. Based on the original latent diffusion model, the
sampling steps of the diffusion model are reduced and the sample generation
efficiency is improved by using the conditional prior to estimate conditional
posterior distribution, which is the approximated distribution of p(z|x). In
addition, this article uses the Laplacian matrix instead of the distance matrix
to recover the nanostructure, which can reduce the reconstruction error.
Finally, this article compares CbLDM with existing models which were used to
solve the nanostructure inverse problem, and find that CbLDM demonstrates
significantly higher prediction accuracy than these models, which reflects the
ability of CbLDM to solve the nanostructure inverse problem and the potential
to cope with other continuous conditional generation tasks.

</details>


### [183] [Learn to Jump: Adaptive Random Walks for Long-Range Propagation through Graph Hierarchies](https://arxiv.org/abs/2509.01381)
*Joël Mathys,Federico Errica*

Main category: cs.LG

TL;DR: 提出了一种利用层次图结构和自适应随机游走的新方法，通过可学习的转移概率在原始图和层次捷径之间选择，突破了传统方法在长距离依赖建模上的理论限制。


<details>
  <summary>Details</summary>
Motivation: 消息传递架构在处理节点和图预测任务中的长距离依赖关系时存在不足，需要新的方法来有效建模远程依赖。

Method: 使用层次图结构和自适应随机游走，引入可学习的转移概率来决定游走是偏好原始图还是利用层次捷径。

Result: 在合成长距离任务上，该方法超越了传统方法在原始拓扑上的理论限制，偏好层次结构的游走能够达到与原始图上更长游走相同的性能。

Conclusion: 这些初步发现为高效处理大型图并有效捕获长距离依赖开辟了有前景的研究方向。

Abstract: Message-passing architectures struggle to sufficiently model long-range
dependencies in node and graph prediction tasks. We propose a novel approach
exploiting hierarchical graph structures and adaptive random walks to address
this challenge. Our method introduces learnable transition probabilities that
decide whether the walk should prefer the original graph or travel across
hierarchical shortcuts. On a synthetic long-range task, we demonstrate that our
approach can exceed the theoretical bound that constrains traditional
approaches operating solely on the original topology. Specifically, walks that
prefer the hierarchy achieve the same performance as longer walks on the
original graph. These preliminary findings open a promising direction for
efficiently processing large graphs while effectively capturing long-range
dependencies.

</details>


### [184] [Distillation of a tractable model from the VQ-VAE](https://arxiv.org/abs/2509.01400)
*Armin Hadžić,Milan Papez,Tomáš Pevný*

Main category: cs.LG

TL;DR: 通过选择高概率的潜变量子集，将VQ-VAE精炼成可处理的概率电路，保持表达能力的同时提供可统计的推理能力


<details>
  <summary>Details</summary>
Motivation: 解决VQ-VAE等离散潜空间深度生成模型的概率推理不可处理问题，因为潜空空间过大

Method: 选择高概率的潜变量子集，将VQ-VAE精炼为概率电路，利用VQ-VAE潜空空间未充分利用的特性

Result: 在密度估计和条件生成任务中取得竞争性能力，挑战了VQ-VAE本质上不可处理的观点

Conclusion: 通过简单的精炼策略，VQ-VAE可以转化为可处理的概率模型，同时保持其表达能力

Abstract: Deep generative models with discrete latent space, such as the
Vector-Quantized Variational Autoencoder (VQ-VAE), offer excellent data
generation capabilities, but, due to the large size of their latent space,
their probabilistic inference is deemed intractable. We demonstrate that the
VQ-VAE can be distilled into a tractable model by selecting a subset of latent
variables with high probabilities. This simple strategy is particularly
efficient, especially if the VQ-VAE underutilizes its latent space, which is,
indeed, very often the case. We frame the distilled model as a probabilistic
circuit, and show that it preserves expressiveness of the VQ-VAE while
providing tractable probabilistic inference. Experiments illustrate competitive
performance in density estimation and conditional generation tasks, challenging
the view of the VQ-VAE as an inherently intractable model.

</details>


### [185] [Evaluating the stability of model explanations in instance-dependent cost-sensitive credit scoring](https://arxiv.org/abs/2509.01409)
*Matteo Ballegeer,Matthias Bogaert,Dries F. Benoit*

Main category: cs.LG

TL;DR: IDCS分类器在信用评分中提高成本效率但导致模型解释稳定性下降，特别是在类别不平衡加剧时，SHAP和LIME的解释稳定性显著降低


<details>
  <summary>Details</summary>
Motivation: 研究实例依赖成本敏感分类器在提高成本效率的同时，对模型解释稳定性的影响，填补文献空白并满足监管透明度要求

Method: 使用四个公开信用评分数据集，评估IDCS分类器的判别能力和成本效率，通过控制重采样研究SHAP和LIME特征重要性排名在不同类别不平衡程度下的稳定性

Result: IDCS分类器提高了成本效率，但相比传统模型产生了显著不稳定的解释，特别是随着类别不平衡增加，解释稳定性进一步下降

Conclusion: 在信用评分中，成本优化与可解释性之间存在关键权衡，需要解决IDCS分类器的稳定性问题，确保成本优势不被不稳定或不可信的解释所削弱

Abstract: Instance-dependent cost-sensitive (IDCS) classifiers offer a promising
approach to improving cost-efficiency in credit scoring by tailoring loss
functions to instance-specific costs. However, the impact of such loss
functions on the stability of model explanations remains unexplored in
literature, despite increasing regulatory demands for transparency. This study
addresses this gap by evaluating the stability of Local Interpretable
Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP)
when applied to IDCS models. Using four publicly available credit scoring
datasets, we first assess the discriminatory power and cost-efficiency of IDCS
classifiers, introducing a novel metric to enhance cross-dataset comparability.
We then investigate the stability of SHAP and LIME feature importance rankings
under varying degrees of class imbalance through controlled resampling. Our
results reveal that while IDCS classifiers improve cost-efficiency, they
produce significantly less stable explanations compared to traditional models,
particularly as class imbalance increases, highlighting a critical trade-off
between cost optimization and interpretability in credit scoring. Amid
increasing regulatory scrutiny on explainability, this research underscores the
pressing need to address stability issues in IDCS classifiers to ensure that
their cost advantages are not undermined by unstable or untrustworthy
explanations.

</details>


### [186] [Accelerating PDE Solvers with Equation-Recast Neural Operator Preconditioning](https://arxiv.org/abs/2509.01416)
*Qiyun Cheng,Md Hossain Sahadath,Huihua Yang,Shaowu Pan,Wei Ji*

Main category: cs.LG

TL;DR: 提出MD-PNOP框架，通过将参数偏差残差重构为额外源项，使单一参数训练的神经算子能够外推泛化到广泛参数配置，加速PDE求解器收敛，计算时间减少约50%且保持全阶精度。


<details>
  <summary>Details</summary>
Motivation: 传统PDE数值求解器的计算开销在大规模参数研究和设计优化中成为关键瓶颈，需要一种既能加速求解又能严格保持物理约束的新方法。

Method: 建立最小数据参数神经算子预处理框架，将参数偏差残差作为额外源项处理，利用训练好的神经算子离线优化解，并将预测结果作为改进的初始猜测嵌入迭代PDE求解器。

Result: 在单组常数参数上训练的神经算子成功加速了异质、正弦和不连续参数分布的求解，计算时间减少约50%，同时保持固定源、单群特征值和多群耦合特征值问题的全阶精度。

Conclusion: MD-PNOP框架有效解决了神经算子的外推局限性，实现了无需重新训练的广泛参数配置泛化，在保持物理约束的同时显著加速了PDE求解过程。

Abstract: The computational overhead of traditional numerical solvers for partial
differential equations (PDEs) remains a critical bottleneck for large-scale
parametric studies and design optimization. We introduce a Minimal-Data
Parametric Neural Operator Preconditioning (MD-PNOP) framework, which
establishes a new paradigm for accelerating parametric PDE solvers while
strictly preserving physical constraints. The key idea is to recast the
residual from parameter deviation as additional source term, where any trained
neural operator can be used to refine the solution in an offline fashion. This
directly addresses the fundamental extrapolation limitation of neural
operators, enabling extrapolative generalization of any neural operator trained
at a single parameter setting across a wide range of configurations without any
retraining. The neural operator predictions are then embedded into iterative
PDE solvers as improved initial guesses, thereby reducing convergence
iterations without sacrificing accuracy. Unlike purely data-driven approaches,
MD-PNOP guarantees that the governing equations remain fully enforced,
eliminating concerns about loss of physics or interpretability. The framework
is architecture-agnostic and is demonstrated using both Deep Operator Networks
(DeepONet) and Fourier Neural Operators (FNO) for Boltzmann transport equation
solvers in neutron transport applications. We demonstrated that neural
operators trained on a single set of constant parameters successfully
accelerate solutions with heterogeneous, sinusoidal, and discontinuous
parameter distributions. Besides, MD-PNOP consistently achieves ~50% reduction
in computational time while maintaining full order fidelity for fixed-source,
single-group eigenvalue, and multigroup coupled eigenvalue problems.

</details>


### [187] [The Geometry of Nonlinear Reinforcement Learning](https://arxiv.org/abs/2509.01432)
*Nikola Milosevic,Nico Scherf*

Main category: cs.LG

TL;DR: 提出了一个统一的几何框架，将强化学习中的奖励最大化、安全探索和内在动机视为环境可达成长期行为的单一优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法通常将奖励最大化、安全探索和内在动机作为独立目标研究，缺乏统一的理论框架来处理这些不同的优化目标。

Method: 构建几何框架，将各种目标视为可达成长期行为空间上的单一优化问题，将经典方法如策略镜像下降、自然策略梯度和信任域算法推广到非线性效用和凸约束情形。

Result: 该框架能够统一处理鲁棒性、安全性、探索性和多样性目标，为不同强化学习目标提供了统一的数学基础。

Conclusion: 该几何框架为强化学习的多个目标提供了统一的理论视角，但在几何方法与深度强化学习的结合方面仍存在挑战。

Abstract: Reward maximization, safe exploration, and intrinsic motivation are often
studied as separate objectives in reinforcement learning (RL). We present a
unified geometric framework, that views these goals as instances of a single
optimization problem on the space of achievable long-term behavior in an
environment. Within this framework, classical methods such as policy mirror
descent, natural policy gradient, and trust-region algorithms naturally
generalize to nonlinear utilities and convex constraints. We illustrate how
this perspective captures robustness, safety, exploration, and diversity
objectives, and outline open challenges at the interface of geometry and deep
RL.

</details>


### [188] [Benchmarking Optimizers for Large Language Model Pretraining](https://arxiv.org/abs/2509.01440)
*Andrei Semenov,Matteo Pagliardini,Martin Jaggi*

Main category: cs.LG

TL;DR: 这篇论文对大语言模型优化方法进行了标准化的综合评估，通过系统性实验提供了不同场景下的最佳优化器选择指南，并为未来优化研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，新的优化方法激增，但各自的实验协议差异较大，导致方法间直接比较困难。需要一个标准化的综合评估框架来评估这些优化技术的真实性能。

Method: 在标准化的LLM预训练场景下，系统性地评估近期的优化技术。通过注意调整模型大小、批处理大小和训练持续时间等参数，对每种方法进行细致的调优。

Result: 研究提供了不同训练场景下的最佳优化器选择建议，并识别出有前景的优化研究方向。所有实验都可重现，代码开源。

Conclusion: 该研究为实践者提供了可靠的优化器选择指南，为研究人员指明了未来优化研究的方向，并通过开源代码和可重现实验推动优化方法的严格测试和发展。

Abstract: The recent development of Large Language Models (LLMs) has been accompanied
by an effervescence of novel ideas and methods to better optimize the loss of
deep learning models. Claims from those methods are myriad: from faster
convergence to removing reliance on certain hyperparameters. However, the
diverse experimental protocols used to validate these claims make direct
comparisons between methods challenging. This study presents a comprehensive
evaluation of recent optimization techniques across standardized LLM
pretraining scenarios, systematically varying model size, batch size, and
training duration. Through careful tuning of each method, we provide guidance
to practitioners on which optimizer is best suited for each scenario. For
researchers, our work highlights promising directions for future optimization
research. Finally, by releasing our code and making all experiments fully
reproducible, we hope our efforts can help the development and rigorous
benchmarking of future methods.

</details>


### [189] [Hierarchical Motion Captioning Utilizing External Text Data Source](https://arxiv.org/abs/2509.01471)
*Clayton Leite,Yu Xiao*

Main category: cs.LG

TL;DR: 通过两步分层方法提升运动描述性文本生成能力：先用大语言模型生成详细描述，再通过检索机制结合外部文本知识生成高级别描述


<details>
  <summary>Details</summary>
Motivation: 解决现有运动-文本数据集缺乏高级别和低级别运动描述注释的问题，提高运动提取的准确性

Method: 1. 使用大语言模型为高级别描述生成详细低级别描述注释 2. 引入检索基机制，将详细描述与外部文本数据源中的高级别描述对齐，结合运动特征生成精确高级别描述

Result: 在HumanML3D、KIT和BOTH57M三个数据集上，方法在BLEU-1、BLEU-4、CIDEr和ROUGE-L平均性能上比最新M2T-Interpretable提升6%-50%

Conclusion: 该分层方法能够利用外部文本知识来显著提高运动提取的准确性，尤其在现有数据集未覆盖的运动上表现优异

Abstract: This paper introduces a novel approach to enhance existing motion captioning
methods, which directly map representations of movement to high-level
descriptive captions (e.g., ``a person doing jumping jacks"). The existing
methods require motion data annotated with high-level descriptions (e.g.,
``jumping jacks"). However, such data is rarely available in existing
motion-text datasets, which additionally do not include low-level motion
descriptions. To address this, we propose a two-step hierarchical approach.
First, we employ large language models to create detailed descriptions
corresponding to each high-level caption that appears in the motion-text
datasets (e.g., ``jumping while synchronizing arm extensions with the opening
and closing of legs" for ``jumping jacks"). These refined annotations are used
to retrain motion-to-text models to produce captions with low-level details.
Second, we introduce a pioneering retrieval-based mechanism. It aligns the
detailed low-level captions with candidate high-level captions from additional
text data sources, and combine them with motion features to fabricate precise
high-level captions. Our methodology is distinctive in its ability to harness
knowledge from external text sources to greatly increase motion captioning
accuracy, especially for movements not covered in existing motion-text
datasets. Experiments on three distinct motion-text datasets (HumanML3D, KIT,
and BOTH57M) demonstrate that our method achieves an improvement in average
performance (across BLEU-1, BLEU-4, CIDEr, and ROUGE-L) ranging from 6% to 50%
compared to the state-of-the-art M2T-Interpretable.

</details>


### [190] [Prior-Guided Flow Matching for Target-Aware Molecule Design with Learnable Atom Number](https://arxiv.org/abs/2509.01486)
*Jingyuan Zhou,Hao Qian,Shikui Tu,Lei Xu*

Main category: cs.LG

TL;DR: PAFlow是一个基于结构药物设计的新模型，通过先验交互指导和可学习原子数预测器，解决了现有方法中概率动态不稳定和分子大小与蛋白质口袋不匹配的问题，在结合亲和力和分子性质方面达到新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于结构的药物设计生成模型存在概率动态不稳定、生成分子大小与蛋白质口袋几何不匹配的问题，导致质量不一致和脱靶效应，需要新的解决方案。

Method: 采用高效的流匹配框架建模生成过程，构建离散原子类型的条件流匹配；整合蛋白质-配体交互预测器指导向量场向高亲和力区域；基于蛋白质口袋信息设计原子数预测器来对齐分子大小与目标几何。

Result: 在CrossDocked2020基准测试中，PAFlow实现了新的最先进结合亲和力（最高-8.31平均Vina分数），同时保持了良好的分子性质。

Conclusion: PAFlow通过创新的先验交互指导和原子数预测机制，有效解决了SBDD中的关键挑战，为基于结构的药物设计提供了更可靠和高效的生成解决方案。

Abstract: Structure-based drug design (SBDD), aiming to generate 3D molecules with high
binding affinity toward target proteins, is a vital approach in novel drug
discovery. Although recent generative models have shown great potential, they
suffer from unstable probability dynamics and mismatch between generated
molecule size and the protein pockets geometry, resulting in inconsistent
quality and off-target effects. We propose PAFlow, a novel target-aware
molecular generation model featuring prior interaction guidance and a learnable
atom number predictor. PAFlow adopts the efficient flow matching framework to
model the generation process and constructs a new form of conditional flow
matching for discrete atom types. A protein-ligand interaction predictor is
incorporated to guide the vector field toward higher-affinity regions during
generation, while an atom number predictor based on protein pocket information
is designed to better align generated molecule size with target geometry.
Extensive experiments on the CrossDocked2020 benchmark show that PAFlow
achieves a new state-of-the-art in binding affinity (up to -8.31 Avg. Vina
Score), simultaneously maintains favorable molecular properties.

</details>


### [191] [Unsupervised Identification and Replay-based Detection (UIRD) for New Category Anomaly Detection in ECG Signal](https://arxiv.org/abs/2509.01512)
*Zhangyue Shi,Zekai Wang,Yuxuan Li*

Main category: cs.LG

TL;DR: 提出基于伪重放的半监督持续学习框架，通过GAN检测心电图新异常模式，用生成器合成历史数据替代存储，解决类别不平衡和存储限制问题。


<details>
  <summary>Details</summary>
Motivation: 心电图分析中面临类别不平衡问题和历史数据存储负担，需要同时检测新异常模式和保持对现有信号的识别性能。

Method: 包含无监督识别和基于重放的检测两部分：使用GAN框架检测新异常模式，通过生成器学习每个任务的数据分布并合成伪数据来避免存储所有历史数据。

Result: 在四个公开ECG数据集上验证，能够有效识别新异常同时保持对现有ECG信号的检测性能。

Conclusion: 该方法在解决心电图异常检测中的类别不平衡和存储限制问题上表现出良好前景。

Abstract: In clinical practice, automatic analysis of electrocardiogram (ECG) is widely
applied to identify irregular heart rhythms and other electrical anomalies of
the heart, enabling timely intervention and potentially improving clinical
outcomes. However, due to the limited samples in certain types of ECG signals,
the class imbalance issues pose a challenge for ECG-based detection. In
addition, as the volume of patient data grows, long-term storage of all
historical data becomes increasingly burdensome as training samples to
recognize new patterns and classify existing ECG signals accurately. Therefore,
to enhance the performance of anomaly detection while addressing storage
limitations, we propose a pseudo-replay based semi-supervised continual
learning framework, which consists of two components: unsupervised
identification and replay-based detection. For unsupervised identification, an
unsupervised generative adversarial network (GAN)-based framework is integrated
to detect novel patterns. Besides, instead of directly storing all historical
data, a pseudo replay-based learning strategy is proposed which utilizes a
generator to learn the data distribution for each individual task. When a new
task arises, the generator synthesizes pseudo data representative of previous
learnt classes, enabling the model to detect both the existed patterns and the
newly presented anomalies. The effectiveness of the proposed framework is
validated in four public ECG datasets, which leverages supervised
classification problems for anomaly detection. The experimental results show
that the developed approach is very promising in identifying novel anomalies
while maintaining good performance on detecting existing ECG signals.

</details>


### [192] [Prediction, Generation of WWTPs microbiome community structures and Clustering of WWTPs various feature attributes using DE-BP model, SiTime-GAN model and DPNG-EPMC ensemble clustering algorithm with modulation of microbial ecosystem health](https://arxiv.org/abs/2509.01526)
*Mingzhi Dai,Weiwei Cai,Xiang Feng,Huiqun Yu,Weibin Guo,Miao Guo*

Main category: cs.LG

TL;DR: 使用DE-BP神经网络预测活性污泥微生物组成，开发DPNG-EPMC聚类算法分析污水处理厂特征，并利用SiTime-GAN生成合成数据，以更好地理解活性污泥微生物群落的影响因素。


<details>
  <summary>Details</summary>
Motivation: 微生物组在生物地球化学循环和工程生态系统中起关键作用，但微生物组工程面临重大挑战，需要开发新方法来预测和控制微生物组成。

Method: 采用差分进化优化的反向传播神经网络(DE-BP)预测微生物组成，开发方向位置非线性情感偏好迁移行为聚类算法(DPNG-EPMC)进行聚类分析，使用相似时间生成对抗网络(SiTime-GAN)生成合成数据。

Result: DE-BP模型能提供优越的微生物组成预测，DPNG-EPMC可应用于不同特征属性下的污水处理厂分析，SiTime-GAN能生成有价值的增量合成数据。

Conclusion: 通过预测微生物群落和分析污水处理厂特征属性，建立了对活性污泥群落影响因素的理解，为微生物组工程提供了有效工具。

Abstract: Microbiomes not only underpin Earth's biogeochemical cycles but also play
crucial roles in both engineered and natural ecosystems, such as the soil,
wastewater treatment, and the human gut. However, microbiome engineering faces
significant obstacles to surmount to deliver the desired improvements in
microbiome control. Here, we use the backpropagation neural network (BPNN),
optimized through differential evolution (DE-BP), to predict the microbial
composition of activated sludge (AS) systems collected from wastewater
treatment plants (WWTPs) located worldwide. Furthermore, we introduce a novel
clustering algorithm termed Directional Position Nonlinear Emotional Preference
Migration Behavior Clustering (DPNG-EPMC). This method is applied to conduct a
clustering analysis of WWTPs across various feature attributes. Finally, we
employ the Similar Time Generative Adversarial Networks (SiTime-GAN), to
synthesize novel microbial compositions and feature attributes data. As a
result, we demonstrate that the DE-BP model can provide superior predictions of
the microbial composition. Additionally, we show that the DPNG-EPMC can be
applied to the analysis of WWTPs under various feature attributes. Finally, we
demonstrate that the SiTime-GAN model can generate valuable incremental
synthetic data. Our results, obtained through predicting the microbial
community and conducting analysis of WWTPs under various feature attributes,
develop an understanding of the factors influencing AS communities.

</details>


### [193] [Forward-Only Continual Learning](https://arxiv.org/abs/2509.01533)
*Jiao Chen,Jiayi He,Fangfang Chen,Zuohong Lv,Jianhua Tang*

Main category: cs.LG

TL;DR: FoRo是一种前向传播、无梯度的持续学习方法，通过轻量级提示调优和知识编码机制，在预训练模型上实现高效持续学习，显著减少遗忘并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 解决预训练模型在持续学习中的灾难性遗忘问题，现有方法依赖反向传播和梯度优化，计算密集且不适合资源受限环境。

Method: 使用CMA-ES优化输入层提示嵌入，通过非线性随机投影和递归最小二乘法将任务特定知识编码到知识编码矩阵中，实现无需回访历史数据的增量分类器更新。

Result: 显著降低平均遗忘率并提高准确性，减少内存使用和运行时间，在长任务序列中保持高知识保留。

Conclusion: FoRo为预训练模型的持续学习提供了有前景的方向，特别适用于效率和效果都至关重要的实际多媒体应用场景。

Abstract: Catastrophic forgetting remains a central challenge in continual learning
(CL) with pre-trained models. While existing approaches typically freeze the
backbone and fine-tune a small number of parameters to mitigate forgetting,
they still rely on iterative error backpropagation and gradient-based
optimization, which can be computationally intensive and less suitable for
resource-constrained environments. To address this, we propose FoRo, a
forward-only, gradient-free continual learning method. FoRo consists of a
lightweight prompt tuning strategy and a novel knowledge encoding mechanism,
both designed without modifying the pre-trained model. Specifically, prompt
embeddings are inserted at the input layer and optimized using the Covariance
Matrix Adaptation Evolution Strategy (CMA-ES), which mitigates distribution
shifts and extracts high-quality task representations. Subsequently,
task-specific knowledge is encoded into a knowledge encoding matrix via
nonlinear random projection and recursive least squares, enabling incremental
updates to the classifier without revisiting prior data. Experiments show that
FoRo significantly reduces average forgetting and improves accuracy. Thanks to
forward-only learning, FoRo reduces memory usage and run time while maintaining
high knowledge retention across long task sequences. These results suggest that
FoRo could serve as a promising direction for exploring continual learning with
pre-trained models, especially in real-world multimedia applications where both
efficiency and effectiveness are critical.

</details>


### [194] [Graph Contrastive Learning versus Untrained Baselines: The Role of Dataset Size](https://arxiv.org/abs/2509.01541)
*Smayan Khanna,Doruk Efe Gökmen,Risi Kondor,Vincenzo Vitelli*

Main category: cs.LG

TL;DR: 研究发现图对比学习(GCL)的优势严重依赖数据集大小和任务难度，在标准数据集上，未经训练的图神经网络、简单多层感知机甚至手工统计特征都能与GCL相媲美或超越它。


<details>
  <summary>Details</summary>
Motivation: 质疑当前图对比学习(GCL)方法的实际效果，探究GCL是否真的优于未经训练的基线方法，以及数据集规模和任务难度对GCL性能的影响。

Method: 通过在标准数据集、大规模分子数据集(ogbg-molhiv)和合成数据集上进行系统实验，比较GCL与未经训练的GNN、简单MLP和手工统计特征的性能差异。

Result: GCL的优势随数据集规模增大而显现，在小规模数据集上表现不佳，但在数千个图以上的大规模数据集上开始领先，但最终会出现性能平台期。在合成数据集上，GCL准确率近似与图数量的对数成正比。

Conclusion: 需要重新评估基准测试中数据集规模的作用，并设计能够避免性能平台期的GCL算法，数据集规模是影响GCL效果的关键因素。

Abstract: Graph Contrastive Learning (GCL) has emerged as a leading paradigm for self-
supervised learning on graphs, with strong performance reported on standardized
datasets and growing applications ranging from genomics to drug discovery. We
ask a basic question: does GCL actually outperform untrained baselines? We find
that GCL's advantage depends strongly on dataset size and task difficulty. On
standard datasets, untrained Graph Neural Networks (GNNs), simple multilayer
perceptrons, and even handcrafted statistics can rival or exceed GCL. On the
large molecular dataset ogbg-molhiv, we observe a crossover: GCL lags at small
scales but pulls ahead beyond a few thousand graphs, though this gain
eventually plateaus. On synthetic datasets, GCL accuracy approximately scales
with the logarithm of the number of graphs and its performance gap (compared
with untrained GNNs) varies with respect to task complexity. Moving forward, it
is crucial to identify the role of dataset size in benchmarks and applications,
as well as to design GCL algorithms that avoid performance plateaus.

</details>


### [195] [Feynman-Kac-Flow: Inference Steering of Conditional Flow Matching to an Energy-Tilted Posterior](https://arxiv.org/abs/2509.01543)
*Konstantin Mark,Leonard Galustian,Maximilian P. -P. Kovar,Esther Heid*

Main category: cs.LG

TL;DR: 本文首次将Feynman-Kac引导方法扩展到条件流匹配(CFM)中，用于精确控制生成样本的特性，并在高维倾斜分布生成和化学反应过渡态生成等挑战性任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 条件流匹配(CFM)虽然提供了快速高质量的生成建模，但在需要精确控制生成样本特性的应用中存在局限。现有的引导方法主要针对扩散模型，尚未扩展到流匹配方法。

Method: 将精确控制需求表述为能量势能倾斜输出，首次推导出CFM的Feynman-Kac引导方法，并在合成任务和高维空间倾斜分布生成中进行评估。

Result: 方法在高维倾斜分布生成这一对引导方法特别具有挑战性的任务中表现良好，并成功解决了化学反应过渡态生成中手性正确性的难题。

Conclusion: Feynman-Kac引导的CFM为精确控制生成样本提供了有效解决方案，特别是在需要满足几何约束的复杂应用场景中展现出强大潜力。

Abstract: Conditional Flow Matching(CFM) represents a fast and high-quality approach to
generative modelling, but in many applications it is of interest to steer the
generated samples towards precise requirements. While steering approaches like
gradient-based guidance, sequential Monte Carlo steering or Feynman-Kac
steering are well established for diffusion models, they have not been extended
to flow matching approaches yet. In this work, we formulate this requirement as
tilting the output with an energy potential. We derive, for the first time,
Feynman-Kac steering for CFM. We evaluate our approach on a set of synthetic
tasks, including the generation of tilted distributions in a high-dimensional
space, which is a particularly challenging case for steering approaches. We
then demonstrate the impact of Feynman-Kac steered CFM on the previously
unsolved challenge of generated transition states of chemical reactions with
the correct chirality, where the reactants or products can have a different
handedness, leading to geometric constraints of the viable reaction pathways
connecting reactants and products. Code to reproduce this study is avaiable
open-source at https://github.com/heid-lab/fkflow.

</details>


### [196] [Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing](https://arxiv.org/abs/2509.01548)
*Zihao Wang,Enneng Yang,Lu Yin,Shiwei Liu,Li Shen*

Main category: cs.LG

TL;DR: MergeLock是一种主动保护机制，通过扰乱模型参数使其无法被合并，从而防止未经授权的模型合并。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的微调模型公开可用，模型合并的安全性受到关注。未经授权的合并可能侵犯开发者权益并泄露敏感个人信息。现有方法主要检测合并模型是否源自特定源模型，但无法有效防止非法合并。

Method: 利用Transformer模型中注意力机制的固有对称性，随机采样两对可逆矩阵并应用于Query-Key和Value-Output分支，保持模型输出不变的同时将其推离其他微调模型的共享参数空间。

Result: 在视觉和语言任务上的大量实验表明，MergeLock在大多数情况下可以将涉及受保护模型的合并模型性能降低95%以上。

Conclusion: MergeLock能有效防止未经授权的模型合并，且受保护的合并模型无法通过低成本恢复方法有效恢复，增强了对抗非法合并的鲁棒性。

Abstract: Model merging leverages multiple finetuned expert models to construct a
multi-task model with low cost, and is gaining increasing attention. However,
as a growing number of finetuned models become publicly available, concerns
about the safety of model merging have emerged. Unauthorized merging may
infringe on developers' rights and risk leaking sensitive personal information.
Most existing methods focus on detecting whether a merged model originates from
a specific source model, but fail to effectively prevent illegal merging. In
this paper, we propose MergeLock, an active protection mechanism that disrupts
model parameters to render them unmergeable, thereby directly preventing
unauthorized model merging. Specifically, leveraging the inherent symmetry of
the attention mechanism in Transformer-based models, we randomly sample two
pairs of invertible matrices and apply them to the Query-Key (QK) and
Value-Output (VO) branches. This transformation keeps the model's output
unchanged while pushing it away from the shared parameter space of other
finetuned models. Extensive experiments across both vision and language tasks
demonstrate that MergeLock can degrade the performance of merged models by over
95% when a protected model is involved in most cases, demonstrating its
effectiveness. Moreover, we further demonstrate that merged models protected by
MergeLock cannot be effectively recovered using low-cost restoration methods,
further enhancing robustness against unauthorized merging. The code is
available at https://github.com/hetailang/Merge-Lock.

</details>


### [197] [Direct Profit Estimation Using Uplift Modeling under Clustered Network Interference](https://arxiv.org/abs/2509.01558)
*Bram van den Akker*

Main category: cs.LG

TL;DR: 本文提出了一个基于AddIPW估计器的实用方法，用于处理推荐系统中促销优化的干扰问题，通过梯度优化直接优化经济收益，显著优于忽略干扰的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的提升建模方法通常忽略干扰效应（一个物品的处理会影响其他物品的结果），这违反了SUTVA假设，导致在真实市场环境中制定次优策略。现有的干扰感知估计器如AddIPW尚未应用于提升建模文献，且使用这些估计器优化策略的方法尚未成熟。

Method: 使用AddIPW估计器作为可微分学习目标，适用于基于梯度的优化。将该框架与成熟的响应转换技术结合，直接优化经济结果（如增量利润）。通过模拟实验验证方法有效性。

Result: 模拟实验表明，该方法显著优于忽略干扰的方法，特别是在干扰效应增强时表现更佳。采用利润中心化提升策略在该框架下能够识别出最高影响力的干预措施。

Conclusion: 该方法为更有利可图的激励个性化提供了实用路径，能够有效处理推荐系统中的干扰问题，优化经济收益。

Abstract: Uplift modeling is a key technique for promotion optimization in recommender
systems, but standard methods typically fail to account for interference, where
treating one item affects the outcomes of others. This violation of the Stable
Unit Treatment Value Assumption (SUTVA) leads to suboptimal policies in
real-world marketplaces. Recent developments in interference-aware estimators
such as Additive Inverse Propensity Weighting (AddIPW) have not found their way
into the uplift modeling literature yet, and optimising policies using these
estimators is not well-established. This paper proposes a practical methodology
to bridge this gap. We use the AddIPW estimator as a differentiable learning
objective suitable for gradient-based optimization. We demonstrate how this
framework can be integrated with proven response transformation techniques to
directly optimize for economic outcomes like incremental profit. Through
simulations, we show that our approach significantly outperforms
interference-naive methods, especially as interference effects grow.
Furthermore, we find that adapting profit-centric uplift strategies within our
framework can yield superior performance in identifying the highest-impact
interventions, offering a practical path toward more profitable incentive
personalization.

</details>


### [198] [Learning Longitudinal Stress Dynamics from Irregular Self-Reports via Time Embeddings](https://arxiv.org/abs/2509.01569)
*Louis Simon,Mohamed Chetouani*

Main category: cs.LG

TL;DR: 提出了Ema2Vec时间嵌入方法，用于处理生态瞬时评估(EMA)中不规则时间间隔的自报告数据，在纵向压力预测任务中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 移动和可穿戴传感技术使得连续个性化监测情绪状态成为可能，但不规则时间间隔的自我报告数据存在缺失和时序依赖性问题，需要有效的时间建模方法

Method: 开发了Ema2Vec时间嵌入方法，专门设计用于处理不规则间隔的EMA数据，捕捉时间依赖性

Result: Ema2Vec在纵向压力预测任务中表现优于依赖固定大小时间窗口的基线方法，以及没有时间感知表示的纵向序列模型

Conclusion: 在处理不规则采样的纵向数据时，纳入时间嵌入表示至关重要，Ema2Vec方法为此类问题提供了有效解决方案

Abstract: The widespread adoption of mobile and wearable sensing technologies has
enabled continuous and personalized monitoring of affect, mood disorders, and
stress. When combined with ecological self-report questionnaires, these systems
offer a powerful opportunity to explore longitudinal modeling of human
behaviors. However, challenges arise from missing data and the irregular timing
of self-reports, which make challenging the prediction of human states and
behaviors. In this study, we investigate the use of time embeddings to capture
time dependencies within sequences of Ecological Momentary Assessments (EMA).
We introduce a novel time embedding method, Ema2Vec, designed to effectively
handle irregularly spaced self-reports, and evaluate it on a new task of
longitudinal stress prediction. Our method outperforms standard stress
prediction baselines that rely on fixed-size daily windows, as well as models
trained directly on longitudinal sequences without time-aware representations.
These findings emphasize the importance of incorporating time embeddings when
modeling irregularly sampled longitudinal data.

</details>


### [199] [One-Shot Clustering for Federated Learning Under Clustering-Agnostic Assumption](https://arxiv.org/abs/2509.01587)
*Maciej Krzysztof Zuziak,Roberto Pellungrini,Salvatore Rinzivillo*

Main category: cs.LG

TL;DR: 本文提出了一种一次性聚类联邦学习(OCFL)算法，能够自动检测最佳聚类时机，无需调整超参数即可实现个性化模型训练。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户群体聚类问题，传统方法需要手动设置聚类时机和超参数，缺乏自动化解决方案。

Method: 基于客户端梯度余弦距离和温度测量来检测联邦模型收敛时刻，使用密度聚类方法区分不同数据分布的神经网络损失曲面。

Result: 在5个基准数据集上测试40多个任务，证明了该方法在自动化聚类联邦学习中的良好性能，同时通过GradCAM提供了个性化与可解释性关系的深入见解。

Conclusion: OCFL算法能够有效自动化聚类联邦学习过程，密度聚类方法在区分不同数据分布的神经网络方面表现出高效率，为个性化模型训练提供了实用解决方案。

Abstract: Federated Learning (FL) is a widespread and well-adopted paradigm of
decentralised learning that allows training one model from multiple sources
without the need to transfer data between participating clients directly. Since
its inception in 2015, it has been divided into numerous subfields that deal
with application-specific issues, such as data heterogeneity or resource
allocation. One such sub-field, Clustered Federated Learning (CFL), deals with
the problem of clustering the population of clients into separate cohorts to
deliver personalised models. Although a few remarkable works have been
published in this domain, the problem remains largely unexplored, as its basic
assumptions and settings differ slightly from those of standard FL. In this
work, we present One-Shot Clustered Federated Learning (OCFL), a
clustering-agnostic algorithm that can automatically detect the earliest
suitable moment for clustering. Our algorithm is based on computing the cosine
distance between the gradients of the clients and a temperature measure that
detects when the federated model starts to converge. We empirically evaluate
our methodology by testing various one-shot clustering algorithms for over
forty different tasks on five benchmark datasets. Our experiments showcase the
good performance of our approach when used to perform CFL in an automated
manner without the need to adjust hyperparameters. We also revisit the
practical feasibility of CFL algorithms based on the gradients of the clients,
providing firm evidence of the high efficiency of density-based clustering
methods when used to differentiate between the loss surfaces of neural networks
trained on different distributions. Moreover, by inspecting the feasibility of
local explanations generated with the help of GradCAM, we can provide more
insights into the relationship between personalisation and the explainability
of local predictions.

</details>


### [200] [Entropy-Driven Curriculum for Multi-Task Training in Human Mobility Prediction](https://arxiv.org/abs/2509.01613)
*Tianye Fang,Xuanshu Luo,Martin Werner*

Main category: cs.LG

TL;DR: 这篇论文提出了一个统一的训练框架，通过熵体驱动的课程学习和多任务学习来提高人类移动预测的准确性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在训练人类移动数据时遇到梯度更新效率低和次优拟合问题，而且仅预测下一个位置忽视了距离和方向等隐式因素，导致预测结果不佳。

Method: 采用熵体驱动的课程学习策略，基于Lempel-Ziv压缩算法量化轨迹可预测性，从简单到复杂组织训练；同时通过多任务学习同时优化主要位置预测和辅助的运动距离、方向估计。

Result: 在HuMob挑战中达到了独创的性能：GEO-BLEU指标0.354，DTW指标26.15，进入收敛的速度比没有课程学习的方法提高了2.92倍。

Conclusion: 该统一训练框架通过结合熵体驱动课程学习和多任务学习，有效解决了人类移动预测中的训练效率和准确性问题，为大规模移动数据的深度学习提供了有效解决方案。

Abstract: The increasing availability of big mobility data from ubiquitous portable
devices enables human mobility prediction through deep learning approaches.
However, the diverse complexity of human mobility data impedes model training,
leading to inefficient gradient updates and potential underfitting. Meanwhile,
exclusively predicting next locations neglects implicit determinants, including
distances and directions, thereby yielding suboptimal prediction results. This
paper presents a unified training framework that integrates entropy-driven
curriculum and multi-task learning to address these challenges. The proposed
entropy-driven curriculum learning strategy quantifies trajectory
predictability based on Lempel-Ziv compression and organizes training from
simple to complex for faster convergence and enhanced performance. The
multi-task training simultaneously optimizes the primary location prediction
alongside auxiliary estimation of movement distance and direction for learning
realistic mobility patterns, and improve prediction accuracy through
complementary supervision signals. Extensive experiments conducted in
accordance with the HuMob Challenge demonstrate that our approach achieves
state-of-the-art performance on GEO-BLEU (0.354) and DTW (26.15) metrics with
up to 2.92-fold convergence speed compared to training without curriculum
learning.

</details>


### [201] [Relative Trajectory Balance is equivalent to Trust-PCL](https://arxiv.org/abs/2509.01632)
*Tristan Deleu,Padideh Nouri,Yoshua Bengio,Doina Precup*

Main category: cs.LG

TL;DR: 本文建立了Relative Trajectory Balance (RTB)与Trust-PCL之间的等价关系，将RTB置于KL正则化强化学习的理论框架中，并展示了KL正则化RL方法在序列生成模型微调中的可比性能。


<details>
  <summary>Details</summary>
Motivation: 生成建模的最新进展突显了强化学习在微调中的重要性，特别是KL正则化方法在自回归和扩散模型中表现出色。RTB作为GFlowNets中的目标函数被提出用于改进序列生成模型的微调，但需要明确其与现有RL方法的关系。

Method: 通过建立RTB与Trust-PCL（一种带有KL正则化的离策略RL方法）之间的等价关系，将RTB置于KL正则化RL的理论框架中。重新评估了RTB论文中的示例，比较KL正则化RL方法的性能。

Result: 研究发现RTB与Trust-PCL在理论上是等价的，这表明RTB可以被理解为KL正则化RL的一种形式。在示例实验中，KL正则化RL方法取得了与RTB相当的性能表现。

Conclusion: 该研究将RTB统一到KL正则化强化学习的理论框架中，澄清了其与现有方法的关系，为序列生成模型的微调提供了新的理论视角和替代方法。

Abstract: Recent progress in generative modeling has highlighted the importance of
Reinforcement Learning (RL) for fine-tuning, with KL-regularized methods in
particular proving to be highly effective for both autoregressive and diffusion
models. Complementing this line of work, the Relative Trajectory Balance (RTB)
objective was recently introduced in the context of Generative Flow Networks
(GFlowNets) to serve the same role of improving fine-tuning in sequential
generative models. Building on prior work linking GFlowNets and maximum-entropy
RL, we establish in this paper an equivalence between RTB and Trust-PCL, an
off-policy RL method with KL regularization. This equivalence situates RTB
within the broader theoretical landscape of KL-regularized RL, and clarifies
its relationship to earlier methods. Leveraging this insight, we revisit an
illustrative example from the RTB paper and show that KL-regularized RL methods
achieve comparable performance, offering an alternative perspective to what was
previously reported.

</details>


### [202] [REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization](https://arxiv.org/abs/2509.01642)
*Maximilian P. Oppelt,Andreas Foltyn,Nadine R. Lang-Richter,Bjoern M. Eskofier*

Main category: cs.LG

TL;DR: 本文提出了一个新的多模态认知负荷检测数据集，通过结合n-back测试和真实游戏应用来评估模型在不同领域的泛化能力，发现多模态方法优于单模态，但跨域迁移仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 当前认知负荷检测模型在真实场景中的泛化能力不足，缺乏对多模态和跨域鲁棒性的系统评估，需要建立更全面的评估框架。

Method: 构建包含客观性能、主观NASA-TLX评分和任务设计的多模态数据集，使用xLSTM、ConvNeXt和Transformer等先进模型进行多模态和跨域的系统性训练与评估。

Result: 多模态方法始终优于单模态基线，不同模态和模型架构在不同应用子集上表现各异，但模型从一个领域迁移到新应用时性能下降明显。

Conclusion: 研究为开发更通用的认知负荷检测系统提供了基准和实用见解，揭示了跨域泛化的挑战，推动了人机交互和自适应系统的研究与应用。

Abstract: Task load detection is essential for optimizing human performance across
diverse applications, yet current models often lack generalizability beyond
narrow experimental domains. While prior research has focused on individual
tasks and limited modalities, there remains a gap in evaluating model
robustness and transferability in real-world scenarios. This paper addresses
these limitations by introducing a new multimodal dataset that extends
established cognitive load detection benchmarks with a real-world gaming
application, using the $n$-back test as a scientific foundation. Task load
annotations are derived from objective performance, subjective NASA-TLX
ratings, and task-level design, enabling a comprehensive evaluation framework.
State-of-the-art end-to-end model, including xLSTM, ConvNeXt, and Transformer
architectures are systematically trained and evaluated on multiple modalities
and application domains to assess their predictive performance and cross-domain
generalization. Results demonstrate that multimodal approaches consistently
outperform unimodal baselines, with specific modalities and model architectures
showing varying impact depending on the application subset. Importantly, models
trained on one domain exhibit reduced performance when transferred to novel
applications, underscoring remaining challenges for universal cognitive load
estimation. These findings provide robust baselines and actionable insights for
developing more generalizable cognitive load detection systems, advancing both
research and practical implementation in human-computer interaction and
adaptive systems.

</details>


### [203] [Distilled Pretraining: A modern lens of Data, In-Context Learning and Test-Time Scaling](https://arxiv.org/abs/2509.01649)
*Sachin Goyal,David Lopez-Paz,Kartik Ahuja*

Main category: cs.LG

TL;DR: 蒸馏训练在LLM预训练中重新兴起，研究发现它能显著改善测试时扩展性能，但会损害上下文学习能力，特别是在归纳头方面。通过二元模型沙箱分析揭示了这一现象的根本原因。


<details>
  <summary>Details</summary>
Motivation: 蒸馏在大型语言模型预训练中重新受到关注（如Llama-3.2和Gemma模型系列），但其对现代LLM关键能力（如测试时扩展和上下文学习）的影响尚未充分探索。

Method: 研究蒸馏预训练对模型性能的影响，包括测试时扩展和上下文学习能力分析，并使用二元模型沙箱来隔离和解释观察到的现象。

Result: 蒸馏预训练显著改善了测试时扩展性能，但损害了上下文学习能力（特别是通过归纳头建模的能力）。二元模型沙箱分析揭示了这一权衡的根本原因。

Conclusion: 研究为预训练设计选择提供了重要见解，帮助实践者在蒸馏训练中做出更明智的决策，平衡测试时扩展和上下文学习能力。

Abstract: In the past year, distillation has seen a renewed prominence in large
language model (LLM) pretraining, exemplified by the Llama-3.2 and Gemma model
families. While distillation has historically been shown to improve statistical
modeling, its effects on new paradigms that are key to modern LLMs, such as
test-time scaling and in-context learning, remain underexplored. In this work,
we make three main contributions. First, we show that pretraining with
distillation yields models that exhibit remarkably better test-time scaling.
Second, we observe that this benefit comes with a trade-off: distillation
impairs in-context learning capabilities, particularly the one modeled via
induction heads. Third, to demystify these findings, we study distilled
pretraining in a sandbox of a bigram model, which helps us isolate the common
principal factor behind our observations. Finally, using these insights, we
shed light on various design choices for pretraining that should help
practitioners going forward.

</details>


### [204] [Reinforcement Learning for Machine Learning Engineering Agents](https://arxiv.org/abs/2509.01684)
*Sherry Yang,Joy He-Yueya,Percy Liang*

Main category: cs.LG

TL;DR: 通过强化学习训练较小模型，解决了现有代琅依赖大模型提问而无法通过经验改进的问题，在ML工程任务中超过更大的静态模型


<details>
  <summary>Details</summary>
Motivation: 现有代琅解决ML工程等任务依赖提问大模型，无法通过经验改进性能，需要强化学习来实现更好的表现

Method: 采用强化学习框架，提出两个关键技术：1)时长感知梯度更新处理变长度动作；2)环境仪器化提供部分评分，通过插入打印语句进行进度监测

Result: 在MLEBench上进行实验，使用强化学习训练的小模型(Qwen2.5-3B)比提问更大模型(Claude-3.5-Sonnet)的性能平均提升22%，在12个Kaggle任务中都取得优异表现

Conclusion: 通过强化学习，较小模型也可以超越静态的大模型，为代琅系统的持续改进提供了新的解决方案

Abstract: Existing agents for solving tasks such as ML engineering rely on prompting
powerful language models. As a result, these agents do not improve with more
experience. In this paper, we show that agents backed by weaker models that
improve via reinforcement learning (RL) can outperform agents backed by much
larger, but static models. We identify two major challenges with RL in this
setting. First, actions can take a variable amount of time (e.g., executing
code for different solutions), which leads to asynchronous policy gradient
updates that favor faster but suboptimal solutions. To tackle variable-duration
actions, we propose duration- aware gradient updates in a distributed
asynchronous RL framework to amplify high-cost but high-reward actions. Second,
using only test split performance as a reward provides limited feedback. A
program that is nearly correct is treated the same as one that fails entirely.
To address this, we propose environment instrumentation to offer partial
credit, distinguishing almost-correct programs from those that fail early
(e.g., during data loading). Environment instrumentation uses a separate static
language model to insert print statement to an existing program to log the
agent's experimental progress, from which partial credit can be extracted as
reward signals for learning. Our experimental results on MLEBench suggest that
performing gradient updates on a much smaller model (Qwen2.5-3B) trained with
RL outperforms prompting a much larger model (Claude-3.5-Sonnet) with agent
scaffolds, by an average of 22% across 12 Kaggle tasks.

</details>


### [205] [Robust Anomaly Detection through Multi-Modal Autoencoder Fusion for Small Vehicle Damage Detection](https://arxiv.org/abs/2509.01719)
*Sara Khan,Mehmed Yüksel,Frank Kirchner*

Main category: cs.LG

TL;DR: 提出基于异常检测的多模态架构，使用IMU和麦克风传感器实时检测车辆磨损和损坏，相比传统方法在ROC-AUC上达到92%的性能


<details>
  <summary>Details</summary>
Motivation: 车队和共享车辆系统中的磨损检测是关键挑战，现有手动检查方法劳动密集且易出错，基于图像的方法实时性能差且难以检测底盘损坏

Method: 开发多模态自编码器架构，将IMU和麦克风传感器集成到安装在挡风玻璃上的紧凑设备中，支持实时损坏检测

Result: 集成池化多模态模型达到最高性能，ROC-AUC为92%，证明其在现实应用中的有效性

Conclusion: 该方法可扩展到其他应用，如提高汽车安全性（与安全气囊系统集成）和辅助自动驾驶车辆的碰撞检测

Abstract: Wear and tear detection in fleet and shared vehicle systems is a critical
challenge, particularly in rental and car-sharing services, where minor damage,
such as dents, scratches, and underbody impacts, often goes unnoticed or is
detected too late. Currently, manual inspection methods are the default
approach but are labour intensive and prone to human error. In contrast,
state-of-the-art image-based methods struggle with real-time performance and
are less effective at detecting underbody damage due to limited visual access
and poor spatial coverage. This work introduces a novel multi-modal
architecture based on anomaly detection to address these issues. Sensors such
as IMUs and microphones are integrated into a compact device mounted on the
vehicle's windshield. This approach supports real-time damage detection while
avoiding the need for highly resource-intensive sensors. We developed multiple
variants of multi-modal autoencoder-based architectures and evaluated them
against unimodal and state-of-the-art methods. Our ensemble pooling multi-modal
model achieved the highest performance, with a Receiver Operating
Characteristic-Area Under Curve (ROC-AUC) of 92%, demonstrating its
effectiveness in real-world applications. This approach can also be extended to
other applications, such as improving automotive safety - where it can
integrate with airbag systems for efficient deployment - and helping autonomous
vehicles by complementing other sensors in collision detection.

</details>


### [206] [Succeed or Learn Slowly: Sample Efficient Off-Policy Reinforcement Learning for Mobile App Control](https://arxiv.org/abs/2509.01720)
*Georgios Papoudakis,Thomas Coste,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.LG

TL;DR: SoLS是一种新颖的离线强化学习算法，通过区分正负样本采用不同更新策略，在移动应用控制任务中显著提升了样本效率和性能


<details>
  <summary>Details</summary>
Motivation: 解决基础模型在多轮任务中强化学习面临的稀疏奖励和策略梯度更新问题，特别是负样本更新可能损害模型性能的问题

Method: 提出SoLS算法：对正样本采用直接策略更新，对负样本采用保守正则化更新；结合Successful Transition Replay(STR)优先学习成功交互

Result: 在AndroidWorld基准测试中显著优于现有方法（相对提升至少17%），计算资源需求大幅减少，推理速度比GPT-4o方法快5-60倍

Conclusion: SoLS通过智能区分正负样本的更新策略，有效解决了基础模型在UI导航任务中的强化学习挑战，实现了高效且稳定的性能提升

Abstract: Reinforcement learning (RL) using foundation models for policy approximations
in multi-turn tasks remains challenging. We identify two main limitations
related to sparse reward settings and policy gradient updates, based on which
we formulate a key insight: updates from positive samples with high returns
typically do not require policy regularisation, whereas updates from negative
samples, reflecting undesirable behaviour, can harm model performance. This
paper introduces Succeed or Learn Slowly (SoLS), a novel off-policy RL
algorithm evaluated on mobile app control tasks. SoLS improves sample
efficiency when fine-tuning foundation models for user interface navigation via
a modified off-policy actor-critic approach, applying direct policy updates for
positive samples and conservative, regularised updates for negative ones to
prevent model degradation. We augment SoLS with Successful Transition Replay
(STR), which prioritises learning from successful interactions, further
improving sample efficiency. We evaluate SoLS on the AndroidWorld benchmark,
where it significantly outperforms existing methods (at least 17% relative
increase), including prompt-engineering and RL approaches, while requiring
substantially fewer computational resources than GPT-4o-based methods with
5-60x faster inference.

</details>


### [207] [Convolutional Monge Mapping between EEG Datasets to Support Independent Component Labeling](https://arxiv.org/abs/2509.01721)
*Austin Meek,Carlos H. Mendoza-Cardenas,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: 本文提出了一种改进的CMMN方法，通过两种新的源参考频谱计算方法来提升EEG信号的光谱一致性，从而改善独立成分分类性能。


<details>
  <summary>Details</summary>
Motivation: EEG记录包含丰富的神经活动信息，但受到伪影、噪声以及传感器、放大器和滤波器的表面差异影响。现有的独立成分分析和自动标记方法需要更好的光谱归一化技术来提升性能。

Method: 提出了CMMN方法的扩展版本，采用两种替代方法计算源参考频谱：(1)通道平均和l1归一化重心，(2)找到与目标受试者频谱最接近的源受试者的受试者间映射方法。该方法产生时空可分离滤波器，可用于不同通道数的数据集映射。

Result: 在独立成分分类任务中应用这些滤波器，显示出在识别大脑与非大脑独立成分方面的显著改进。

Conclusion: 通过适当的光谱归一化滤波，可以最小化记录设备和环境差异对机器学习模型性能的影响，这对于癫痫和精神疾病等神经病理学的EEG诊断和监测具有重要意义。

Abstract: EEG recordings contain rich information about neural activity but are subject
to artifacts, noise, and superficial differences due to sensors, amplifiers,
and filtering. Independent component analysis and automatic labeling of
independent components (ICs) enable artifact removal in EEG pipelines.
Convolutional Monge Mapping Normalization (CMMN) is a recent tool used to
achieve spectral conformity of EEG signals, which was shown to improve deep
neural network approaches for sleep staging. Here we propose a novel extension
of the CMMN method with two alternative approaches to computing the source
reference spectrum the target signals are mapped to: (1) channel-averaged and
$l_1$-normalized barycenter, and (2) a subject-to-subject mapping that finds
the source subject with the closest spectrum to the target subject. Notably,
our extension yields space-time separable filters that can be used to map
between datasets with different numbers of EEG channels. We apply these filters
in an IC classification task, and show significant improvement in recognizing
brain versus non-brain ICs.
  Clinical relevance - EEG recordings are used in the diagnosis and monitoring
of multiple neuropathologies, including epilepsy and psychosis. While EEG
analysis can benefit from automating artifact removal through independent
component analysis and labeling, differences in recording equipment and context
(the presence of noise from electrical wiring and other devices) may impact the
performance of machine learning models, but these differences can be minimized
by appropriate spectral normalization through filtering.

</details>


### [208] [BM-CL: Bias Mitigation through the lens of Continual Learning](https://arxiv.org/abs/2509.01730)
*Lucas Mansilla,Rodrigo Echeveste,Camila Gonzalez,Diego H. Milone,Enzo Ferrante*

Main category: cs.LG

TL;DR: 提出了BM-CL框架，将偏差缓解重新定义为持续学习问题，通过借鉴持续学习技术来改善弱势群体结果，同时保持对优势群体的性能


<details>
  <summary>Details</summary>
Motivation: 传统偏差缓解技术存在"拉平效应"，改善弱势群体结果往往以牺牲优势群体性能为代价，需要新的方法来平衡公平性和有效性

Method: 借鉴Learning without Forgetting和Elastic Weight Consolidation等持续学习技术，将偏差缓解视为领域增量持续学习问题，模型需要适应变化的公平条件

Result: 在合成和真实图像数据集上的实验表明，该框架能够有效缓解偏差，同时最小化原始知识的损失

Conclusion: BM-CL框架连接了公平性和持续学习两个领域，为开发既公平又有效的机器学习系统提供了有前景的途径

Abstract: Biases in machine learning pose significant challenges, particularly when
models amplify disparities that affect disadvantaged groups. Traditional bias
mitigation techniques often lead to a {\itshape leveling-down effect}, whereby
improving outcomes of disadvantaged groups comes at the expense of reduced
performance for advantaged groups. This study introduces Bias Mitigation
through Continual Learning (BM-CL), a novel framework that leverages the
principles of continual learning to address this trade-off. We postulate that
mitigating bias is conceptually similar to domain-incremental continual
learning, where the model must adjust to changing fairness conditions,
improving outcomes for disadvantaged groups without forgetting the knowledge
that benefits advantaged groups. Drawing inspiration from techniques such as
Learning without Forgetting and Elastic Weight Consolidation, we reinterpret
bias mitigation as a continual learning problem. This perspective allows models
to incrementally balance fairness objectives, enhancing outcomes for
disadvantaged groups while preserving performance for advantaged groups.
Experiments on synthetic and real-world image datasets, characterized by
diverse sources of bias, demonstrate that the proposed framework mitigates
biases while minimizing the loss of original knowledge. Our approach bridges
the fields of fairness and continual learning, offering a promising pathway for
developing machine learning systems that are both equitable and effective.

</details>


### [209] [Communication-Aware Knowledge Distillation for Federated LLM Fine-Tuning over Wireless Networks](https://arxiv.org/abs/2509.01750)
*Xinlu Zhang,Na Yan,Yang Su,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 提出了一种高效的联邦大语言模型蒸馏方法，通过自适应Top-k logit选择和聚合机制，在减少50%通信开销的同时保持优异性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习中LLM参数共享方法通信开销高且难以适应异构架构，而传统联邦蒸馏传输完整logits对带宽受限客户端仍具挑战性

Method: 采用自适应Top-k logit选择机制动态稀疏化logits，设计自适应logits聚合方案避免零填充问题，并引入LoRA适配的隐藏层投影增强蒸馏效果

Result: 实验结果表明该方法在有效减少约50%通信开销的同时，性能优于基线方法

Conclusion: 提出的高效联邦LLM蒸馏方案成功解决了通信瓶颈问题，为隐私保护的分布式语言模型训练提供了可行解决方案

Abstract: Federated learning (FL) for large language models (LLMs) offers a
privacy-preserving scheme, enabling clients to collaboratively fine-tune
locally deployed LLMs or smaller language models (SLMs) without exchanging raw
data. While parameter-sharing methods in traditional FL models solves number of
technical challenges, they still incur high communication overhead and struggle
with adapting to heterogeneous model architectures. Federated distillation, a
framework for mutual knowledge transfer via shared logits, typically offers
lower communication overhead than parameter-sharing methods. However,
transmitting logits from LLMs remains challenging for bandwidth-limited clients
due to their high dimensionality. In this work, we focus on a federated LLM
distillation with efficient communication overhead. To achieve this, we first
propose an adaptive Top-k logit selection mechanism, dynamically sparsifying
logits according to real-time communication conditions. Then to tackle the
dimensional inconsistency introduced by the adaptive sparsification, we design
an adaptive logits aggregation scheme, effectively alleviating the artificial
and uninformative inputs introduced by conventional zero-padding methods.
Finally, to enhance the distillation effect, we incorporate LoRA-adapted
hidden-layer projection from LLM into the distillation loss, reducing the
communication overhead further while providing richer representation.
Experimental results demonstrate that our scheme achieves superior performance
compared to baseline methods while effectively reducing communication overhead
by approximately 50%.

</details>


### [210] [Toward a Unified Benchmark and Taxonomy of Stochastic Environments](https://arxiv.org/abs/2509.01793)
*Aryan Amit Barsainyan,Jing Yu Lim,Dianbo Liu*

Main category: cs.LG

TL;DR: STORI基准测试引入多样随机效应和随机性分类法，系统评估RL方法在不同不确定性下的表现


<details>
  <summary>Details</summary>
Motivation: 当前RL智能体在真实世界条件下缺乏鲁棒性，基于世界模型的RL方法在真实随机性和部分可观测环境中表现不佳，现有基准测试未能充分捕捉这些挑战

Method: 提出STORI基准测试，包含多样随机效应；建立RL环境中随机性的分类法，提供统一的分析框架

Result: 创建了一个能够系统评估RL方法在不同形式不确定性下表现的基准测试平台

Conclusion: STORI基准测试和随机性分类法填补了现有研究的空白，为RL方法在随机环境中的系统评估提供了重要工具

Abstract: Reinforcement Learning (RL) agents have achieved strong results on benchmarks
such as Atari100k, yet they remain limited in robustness to real-world
conditions. Model-Based RL approaches that rely on learned World Models often
struggle in environments with true stochasticity and partial observability,
despite their theoretical grounding in POMDPs. Current benchmarks rarely
capture these challenges, focusing instead on deterministic or overly
simplified settings, and the lack of a clear taxonomy of stochasticity further
hampers systematic evaluation. To address this gap, we introduce STORI
(STOchastic-ataRI), a benchmark that incorporates diverse stochastic effects
and enables rigorous assessment of RL methods under varied forms of
uncertainty. In addition, we propose a taxonomy of stochasticity in RL
environments, providing a unified framework for analyzing and comparing
approaches.

</details>


### [211] [A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics](https://arxiv.org/abs/2509.01794)
*Trusting Inekwe,Emmanuel Agu,Winnie Mkandawire,Andres Colubri*

Main category: cs.LG

TL;DR: 提出了MBT-CB模型，一种基于BERT的多目标贝叶斯Transformer，用于从电子健康记录中联合预测心血管疾病生物标志物，并在COVID-19疫情期间表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行对心血管疾病患者的生物标志物产生了显著影响，但现有方法未能同时处理多目标预测、生物标志物相互依赖关系、时间模式和预测不确定性。

Method: 使用预训练的BERT-based transformer框架，结合贝叶斯变分推理估计不确定性，通过嵌入捕获时间关系，利用DeepMTR模型捕捉生物标志物间相互关系。

Result: 在3,390条心血管患者记录上评估，MBT-CB在MAE、RMSE和MSE指标上均优于其他基线模型，有效捕获了数据不确定性、模型不确定性和时间动态。

Conclusion: MBT-CB的优越性能表明其在改善心血管生物标志物预测和支持临床决策方面具有巨大潜力，特别是在大流行期间。

Abstract: The COVID-19 pandemic disrupted healthcare systems worldwide,
disproportionately impacting individuals with chronic conditions such as
cardiovascular disease (CVD). These disruptions -- through delayed care and
behavioral changes, affected key CVD biomarkers, including LDL cholesterol
(LDL-C), HbA1c, BMI, and systolic blood pressure (SysBP). Accurate modeling of
these changes is crucial for predicting disease progression and guiding
preventive care. However, prior work has not addressed multi-target prediction
of CVD biomarker from Electronic Health Records (EHRs) using machine learning
(ML), while jointly capturing biomarker interdependencies, temporal patterns,
and predictive uncertainty. In this paper, we propose MBT-CB, a Multi-target
Bayesian Transformer (MBT) with pre-trained BERT-based transformer framework to
jointly predict LDL-C, HbA1c, BMI and SysBP CVD biomarkers from EHR data. The
model leverages Bayesian Variational Inference to estimate uncertainties,
embeddings to capture temporal relationships and a DeepMTR model to capture
biomarker inter-relationships. We evaluate MBT-CT on retrospective EHR data
from 3,390 CVD patient records (304 unique patients) in Central Massachusetts
during the Covid-19 pandemic. MBT-CB outperformed a comprehensive set of
baselines including other BERT-based ML models, achieving an MAE of 0.00887,
RMSE of 0.0135 and MSE of 0.00027, while effectively capturing data and model
uncertainty, patient biomarker inter-relationships, and temporal dynamics via
its attention and embedding mechanisms. MBT-CB's superior performance
highlights its potential to improve CVD biomarker prediction and support
clinical decision-making during pandemics.

</details>


### [212] [When LLM Meets Time Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference](https://arxiv.org/abs/2509.01822)
*Wen Ye,Jinbo Liu,Defu Cao,Wei Yang,Yan Liu*

Main category: cs.LG

TL;DR: TSAIA Benchmark是首个评估LLMs作为时间序列AI助手的基准数据集，包含33个现实任务，涵盖约束感知预测、异常检测等复杂推理任务，评估显示当前模型在构建复杂时间序列分析工作流方面存在局限


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列分析中的应用潜力尚未充分探索，需要建立严谨的基准数据集来评估LLMs处理时序数据复杂推理的能力

Method: 通过调研20多篇学术文献确定了33个现实任务，设计了动态可扩展的问题生成器，采用任务特定的成功标准和定制化推理质量指标

Result: 对8个最先进LLMs的评估显示，当前模型在组装复杂时间序列分析工作流方面存在明显局限性

Conclusion: 需要开发专门的方法论来实现领域特定的适配，TSAIA基准为未来研究提供了重要基础

Abstract: The rapid advancement of Large Language Models (LLMs) has sparked growing
interest in their application to time series analysis tasks. However, their
ability to perform complex reasoning over temporal data in real-world
application domains remains underexplored. To move toward this goal, a first
step is to establish a rigorous benchmark dataset for evaluation. In this work,
we introduce the TSAIA Benchmark, a first attempt to evaluate LLMs as
time-series AI assistants. To ensure both scientific rigor and practical
relevance, we surveyed over 20 academic publications and identified 33
real-world task formulations. The benchmark encompasses a broad spectrum of
challenges, ranging from constraint-aware forecasting to anomaly detection with
threshold calibration: tasks that require compositional reasoning and
multi-step time series analysis. The question generator is designed to be
dynamic and extensible, supporting continuous expansion as new datasets or task
types are introduced. Given the heterogeneous nature of the tasks, we adopt
task-specific success criteria and tailored inference-quality metrics to ensure
meaningful evaluation for each task. We apply this benchmark to assess eight
state-of-the-art LLMs under a unified evaluation protocol. Our analysis reveals
limitations in current models' ability to assemble complex time series analysis
workflows, underscoring the need for specialized methodologies for
domain-specific adaptation. Our benchmark is available at
https://huggingface.co/datasets/Melady/TSAIA, and the code is available at
https://github.com/USC-Melady/TSAIA.

</details>


### [213] [Goal-Conditioned Reinforcement Learning for Data-Driven Maritime Navigation](https://arxiv.org/abs/2509.01838)
*Vaishnav Vaidheeswaran,Dilith Jayakody,Samruddhi Mulay,Anand Lo,Md Mahbub Alam,Gabriel Spadon*

Main category: cs.LG

TL;DR: 提出基于强化学习的船舶航线规划方法，使用AIS数据和ERA5风场，在圣劳伦斯湾验证了PPO算法与动作屏蔽、探索策略的有效性


<details>
  <summary>Details</summary>
Motivation: 现有船舶航线研究难以泛化到多个起讫点对，且未充分利用大规模数据驱动的交通图，需要适应动态环境条件和操作约束

Method: 使用强化学习框架，在多离散动作空间中学习选择方向和速度，奖励函数平衡燃油效率、行程时间、风阻和航线多样性，结合PPO算法、循环网络、无效动作屏蔽和探索策略

Result: 动作屏蔽显著提升策略性能，在仅有惩罚反馈基础上增加正向塑造奖励带来额外收益

Conclusion: 该方法能够学习跨多个起讫点对的航线规划，适应不同六边形网格分辨率，为动态水道中的船舶路由提供了有效解决方案

Abstract: Routing vessels through narrow and dynamic waterways is challenging due to
changing environmental conditions and operational constraints. Existing
vessel-routing studies typically fail to generalize across multiple
origin-destination pairs and do not exploit large-scale, data-driven traffic
graphs. In this paper, we propose a reinforcement learning solution for big
maritime data that can learn to find a route across multiple origin-destination
pairs while adapting to different hexagonal grid resolutions. Agents learn to
select direction and speed under continuous observations in a multi-discrete
action space. A reward function balances fuel efficiency, travel time, wind
resistance, and route diversity, using an Automatic Identification System
(AIS)-derived traffic graph with ERA5 wind fields. The approach is demonstrated
in the Gulf of St. Lawrence, one of the largest estuaries in the world. We
evaluate configurations that combine Proximal Policy Optimization with
recurrent networks, invalid-action masking, and exploration strategies. Our
experiments demonstrate that action masking yields a clear improvement in
policy performance and that supplementing penalty-only feedback with positive
shaping rewards produces additional gains.

</details>


### [214] [Optimizing In-Context Learning for Efficient Full Conformal Prediction](https://arxiv.org/abs/2509.01840)
*Weicao Deng,Sangwoo Park,Min Li,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出E-ICL+FCP框架，通过基于Transformer的元学习模型模拟全量共形预测的多重重训练过程，在保持覆盖率的同时显著提升数据效率和计算效率


<details>
  <summary>Details</summary>
Motivation: 解决共形预测中分裂CP数据效率低和全量CP计算复杂度高的互补局限性问题，现有基于元学习的方法未针对CP特性专门优化

Method: 使用排列不变的Transformer ICL模型，采用CP感知的损失函数训练，模拟FCP所需的多重重训练模型而无需实际重训练

Result: 在合成和真实任务上实验表明，E-ICL+FCP相比现有SCP和FCP基线方法获得了更优的效率-覆盖率权衡

Conclusion: E-ICL+FCP框架有效解决了共形预测的数据效率和计算复杂度问题，为可靠的不确定性量化提供了高效解决方案

Abstract: Reliable uncertainty quantification is critical for trustworthy AI. Conformal
Prediction (CP) provides prediction sets with distribution-free coverage
guarantees, but its two main variants face complementary limitations. Split CP
(SCP) suffers from data inefficiency due to dataset partitioning, while full CP
(FCP) improves data efficiency at the cost of prohibitive retraining
complexity. Recent approaches based on meta-learning or in-context learning
(ICL) partially mitigate these drawbacks. However, they rely on training
procedures not specifically tailored to CP, which may yield large prediction
sets. We introduce an efficient FCP framework, termed enhanced ICL-based FCP
(E-ICL+FCP), which employs a permutation-invariant Transformer-based ICL model
trained with a CP-aware loss. By simulating the multiple retrained models
required by FCP without actual retraining, E-ICL+FCP preserves coverage while
markedly reducing both inefficiency and computational overhead. Experiments on
synthetic and real tasks demonstrate that E-ICL+FCP attains superior
efficiency-coverage trade-offs compared to existing SCP and FCP baselines.

</details>


### [215] [GradES: Significantly Faster Training in Transformers with Gradient-Based Early Stopping](https://arxiv.org/abs/2509.01842)
*Qifu Wen,Xi Zeng,Zihan Zhou,Shuaijun Liu,Mehdi Hosseinzadeh,Reza Rawassizadeh*

Main category: cs.LG

TL;DR: GradES是一种基于梯度的早停方法，通过监控transformer组件中投影矩阵的梯度幅度，在梯度低于阈值时单独冻结该矩阵的参数更新，避免了昂贵的全局验证推理，实现了1.57-7.22倍的训练加速和1.2%的平均准确率提升。


<details>
  <summary>Details</summary>
Motivation: 传统早停方法需要监控全局验证损失并同时停止所有参数更新，对于大型transformer来说验证推理时间过长，计算成本高昂。

Method: 在transformer组件（注意力投影和前馈层矩阵）中跟踪反向传播时的梯度幅度，当某个投影矩阵的梯度低于收敛阈值τ时，单独排除该矩阵的后续更新，允许收敛慢的矩阵继续学习。

Result: GradES将训练时间加速1.57-7.22倍，同时通过早期防止过拟合提高了泛化能力，平均准确率提升1.2%。

Conclusion: GradES通过组件级别的梯度监控和选择性参数冻结，有效解决了传统早停方法的计算效率问题，在加速训练的同时提升了模型性能。

Abstract: Early stopping monitors global validation loss and halts all parameter
updates simultaneously, which is computationally costly for large transformers
due to the extended time required for validation inference. We propose GradES,
a novel gradient-based early stopping approach that operates within transformer
components (attention projections and Feed-Forward layer matrices). We found
that different components converge at varying rates during fine-tuning. GradES
tracks the magnitude of gradients in backpropagation for these matrices during
training. When a projection matrix's gradients fall below a convergence
threshold $\tau$, we exclude that projection matrix from further updates
individually, eliminating costly validation passes while allowing slow
converging matrices to continue learning. By strategically freezing parameters
when their gradients converge, GradES speeds up training time by
1.57--7.22$\times$ while simultaneously enhancing generalization through early
prevention of overfitting, resulting in 1.2% higher average accuracy.

</details>


### [216] [Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation Function](https://arxiv.org/abs/2509.01874)
*Jason Abohwo,Thomas Mosen*

Main category: cs.LG

TL;DR: 提出了Signed Quadratic Shrink (SQS)激活函数，使GLU能够学习可解释特征，同时保持性能竞争力


<details>
  <summary>Details</summary>
Motivation: 现有基于权重的特征分析方法存在性能下降和数据效率低的问题，需要直接从神经网络权重中推导有意义特征的方法

Method: 设计SQS激活函数，专门用于Gated Linear Units (GLUs)，支持基于权重的可解释性分析

Result: SQS在性能上与最先进的激活函数相当，同时实现了基于权重的可解释性

Conclusion: SQS激活函数成功解决了现有方法的缺陷，为模型可解释性提供了新的有效途径

Abstract: Understanding the inner workings of machine learning models is critical for
ensuring their reliability and robustness. Whilst many techniques in
mechanistic interpretability focus on activation driven analyses, being able to
derive meaningful features directly from the weights of a neural network would
provide greater guarantees and more computational efficiency. Existing
techniques for analyzing model features through weights suffer from drawbacks
such as reduced performance and data inefficiency. In this paper, we introduce
Signed Quadratic Shrink (SQS), an activation function designed to allow Gated
Linear Units (GLUs) to learn interpretable features without these drawbacks.
Our experimental results show that SQS achieves performance competitive with
state-of-the-art activation functions whilst enabling weight-based
interpretability

</details>


### [217] [Deep Reinforcement Learning for Real-Time Drone Routing in Post-Disaster Road Assessment Without Domain Knowledge](https://arxiv.org/abs/2509.01886)
*Huatian Gong,Jiuh-Biing Sheu,Zheng Wang,Xiaoguang Yang,Ran Yan*

Main category: cs.LG

TL;DR: 提出基于注意力机制的编码器-解码器模型，用于灾后道路损坏评估中的无人机实时路径规划，在解质量和计算效率上显著优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统优化方法计算时间长且需要领域知识设计算法，不适合时间敏感的灾害场景，需要快速有效的无人机路径决策方案

Method: 使用深度强化学习，开发网络转换方法将基于链路的路径问题转化为节点形式，采用合成路网生成技术解决训练数据稀缺问题，使用POMO策略优化和多任务学习

Result: 模型在解质量上优于商业求解器16-69%，推理时间仅1-2秒（传统方法需100-2000秒），在不同问题规模、无人机数量和时间约束下都表现出强泛化能力

Conclusion: 该方法有效平衡了计算效率和解质量，特别适合时间关键的灾害响应应用，为快速决策拯救生命提供了有效工具

Abstract: Rapid post-disaster road damage assessment is critical for effective
emergency response, yet traditional optimization methods suffer from excessive
computational time and require domain knowledge for algorithm design, making
them unsuitable for time-sensitive disaster scenarios. This study proposes an
attention-based encoder-decoder model (AEDM) for real-time drone routing
decision in post-disaster road damage assessment. The method employs deep
reinforcement learning to determine high-quality drone assessment routes
without requiring algorithmic design knowledge. A network transformation method
is developed to convert link-based routing problems into equivalent node-based
formulations, while a synthetic road network generation technique addresses the
scarcity of large-scale training datasets. The model is trained using policy
optimization with multiple optima (POMO) with multi-task learning capabilities
to handle diverse parameter combinations. Experimental results demonstrate two
key strengths of AEDM: it outperforms commercial solvers by 16--69\% in
solution quality and achieves real-time inference (1--2 seconds) versus
100--2,000 seconds for traditional methods. The model exhibits strong
generalization across varying problem scales, drone numbers, and time
constraints, consistently outperforming baseline methods on unseen parameter
distributions and real-world road networks. The proposed method effectively
balances computational efficiency with solution quality, making it particularly
suitable for time-critical disaster response applications where rapid
decision-making is essential for saving lives.

</details>


### [218] [Predicting NCAP Safety Ratings: An Analysis of Vehicle Characteristics and ADAS Features Using Machine Learning](https://arxiv.org/abs/2509.01897)
*Raunak Kunwar,Aera Kim LeBoulluec*

Main category: cs.LG

TL;DR: 本研究使用机器学习分析NCAP安全评级数据，发现车辆基本特征（车重和车型年份）对预测5星级安全评级最重要，但ADAS功能也有显著贡献。随机森林模型达到89.18%准确率。


<details>
  <summary>Details</summary>
Motivation: 随着NCAP安全评级从被动安全扩展到包含ADAS主动安全技术，需要实证研究这些系统如何相互作用，以及能否可靠预测最高安全评级。

Method: 使用包含5,128辆车的NCAP数据集，比较逻辑回归、随机森林、梯度提升和SVC四种机器学习模型，采用5折分层交叉验证，并对最佳模型进行超参数优化。

Result: 车重和车型年份占随机森林模型特征重要性的55%以上，但ADAS功能也有意义。优化后的随机森林模型在测试集上达到89.18%准确率和0.9586 ROC AUC。

Conclusion: 机器学习可有效分析大规模NCAP数据，传统车辆参数和现代ADAS功能共同对获得顶级安全评级具有预测重要性。

Abstract: Vehicle safety assessment is crucial for consumer information and regulatory
oversight. The New Car Assessment Program (NCAP) assigns standardized safety
ratings, which traditionally emphasize passive safety measures but now include
active safety technologies such as Advanced Driver-Assistance Systems (ADAS).
It is crucial to understand how these various systems interact empirically.
This study explores whether particular ADAS features like Forward Collision
Warning, Lane Departure Warning, Crash Imminent Braking, and Blind Spot
Detection, together with established vehicle attributes (e.g., Curb Weight,
Model Year, Vehicle Type, Drive Train), can reliably predict a vehicle's
likelihood of earning the highest (5-star) overall NCAP rating. Using a
publicly available dataset derived from NCAP reports that contain approximately
5,128 vehicle variants spanning model years 2011-2025, we compared four
different machine learning models: logistic regression, random forest, gradient
boosting, and support vector classifier (SVC) using a 5-fold stratified
cross-validation approach. The two best-performing algorithms (random forest
and gradient boost) were hyperparameter optimized using RandomizedSearchCV.
Analysis of feature importance showed that basic vehicle characteristics,
specifically curb weight and model year, dominated predictive capability,
contributing more than 55% of the feature relevance of the Random Forest model.
However, the inclusion of ADAS features also provided meaningful predictive
contributions. The optimized Random Forest model achieved robust results on a
held-out test set, with an accuracy of 89.18% and a ROC AUC of 0.9586. This
research reveals the use of machine learning to analyze large-scale NCAP data
and highlights the combined predictive importance of both established vehicle
parameters and modern ADAS features to achieve top safety ratings.

</details>


### [219] [VISP: Volatility Informed Stochastic Projection for Adaptive Regularization](https://arxiv.org/abs/2509.01903)
*Tanvir Islam*

Main category: cs.LG

TL;DR: VISP是一种基于梯度波动性的自适应正则化方法，通过动态计算梯度统计量来调整随机噪声注入，选择性地正则化高波动性的输入和隐藏节点，从而提升深度神经网络的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法如均匀噪声注入或固定dropout率无法适应网络不同部分的动态特性，需要一种能够根据梯度波动性自适应调整正则化强度的机制。

Method: VISP通过计算梯度统计量的波动性，动态生成并缩放随机投影矩阵，选择性地对高梯度波动性的输入和隐藏节点施加更强的正则化，同时保持稳定表示。

Result: 在MNIST、CIFAR-10和SVHN数据集上的实验表明，VISP相比基线模型和固定噪声方法能够持续提升泛化性能，同时稳定网络内部动态并促进更鲁棒的特征表示。

Conclusion: VISP通过基于梯度波动性的自适应正则化机制，有效缓解过拟合问题，提升深度神经网络的泛化能力和鲁棒性，为动态正则化方法提供了新的思路。

Abstract: We propose VISP: Volatility Informed Stochastic Projection, an adaptive
regularization method that leverages gradient volatility to guide stochastic
noise injection in deep neural networks. Unlike conventional techniques that
apply uniform noise or fixed dropout rates, VISP dynamically computes
volatility from gradient statistics and uses it to scale a stochastic
projection matrix. This mechanism selectively regularizes inputs and hidden
nodes that exhibit higher gradient volatility while preserving stable
representations, thereby mitigating overfitting. Extensive experiments on
MNIST, CIFAR-10, and SVHN demonstrate that VISP consistently improves
generalization performance over baseline models and fixed-noise alternatives.
In addition, detailed analyses of the evolution of volatility, the spectral
properties of the projection matrix, and activation distributions reveal that
VISP not only stabilizes the internal dynamics of the network but also fosters
a more robust feature representation.

</details>


### [220] [Causal representation learning from network data](https://arxiv.org/abs/2509.01916)
*Jifan Zhang,Michelle M. Li,Elena Zheleva*

Main category: cs.LG

TL;DR: GraCE-VAE是一个用于非i.i.d.环境下因果解缠的框架，通过结合变分自编码器和图神经网络，利用网络数据中的结构化上下文来恢复潜在因果图和干预效应。


<details>
  <summary>Details</summary>
Motivation: 现有因果解缠研究主要针对i.i.d.数据，但在现实世界中存在大量非i.i.d.的结构化数据（如网络数据），需要开发能够利用这种结构化上下文的因果发现方法。

Method: GraCE-VAE整合了基于差异的变分自编码器和图神经网络，联合恢复真实的潜在因果图和干预效应，特别适用于具有网络数据形式的结构化上下文环境。

Result: 理论证明i.i.d.数据的可识别性结果在本框架中仍然成立，在三个遗传扰动数据集上的实证评估显示，利用结构化上下文能显著提升因果解缠性能。

Conclusion: GraCE-VAE成功将因果解缠扩展到非i.i.d.设置，证明了利用结构化上下文信息对因果发现的重要价值，为处理现实世界中的复杂因果关系提供了有效工具。

Abstract: Causal disentanglement from soft interventions is identifiable under the
assumptions of linear interventional faithfulness and availability of both
observational and interventional data. Previous research has looked into this
problem from the perspective of i.i.d. data. Here, we develop a framework,
GraCE-VAE, for non-i.i.d. settings, in which structured context in the form of
network data is available. GraCE-VAE integrates discrepancy-based variational
autoencoders with graph neural networks to jointly recover the true latent
causal graph and intervention effects. We show that the theoretical results of
identifiability from i.i.d. data hold in our setup. We also empirically
evaluate GraCE-VAE against state-of-the-art baselines on three genetic
perturbation datasets to demonstrate the impact of leveraging structured
context for causal disentanglement.

</details>


### [221] [A Continuous Encoding-Based Representation for Efficient Multi-Fidelity Multi-Objective Neural Architecture Search](https://arxiv.org/abs/2509.01943)
*Zhao Wei,Chin Chun Ooi,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 提出自适应协同克里金辅助的多保真度多目标神经架构搜索算法，通过聚类局部采样策略和连续编码方法降低计算成本，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索(NAS)虽然能自动优化架构设计，但计算成本高昂，特别是在优化多个冲突目标时。需要降低NAS的计算开销。

Method: 采用自适应协同克里金辅助的多保真度多目标NAS算法，结合聚类局部多保真度采样策略和新型连续编码方法，减少搜索维度。

Result: 在有限计算预算下，该算法在三个数值基准、2D Darcy流回归问题和CHASE_DB1生物医学图像分割问题上优于现有方法。成功应用于城市建模的风速回归模型。

Conclusion: 提出的NAS算法能有效降低计算成本，自动发现优秀的U-Net架构设计原则，如允许每个单元整合先前单元信息的重要性。

Abstract: Neural architecture search (NAS) is an attractive approach to automate the
design of optimized architectures but is constrained by high computational
budget, especially when optimizing for multiple, important conflicting
objectives. To address this, an adaptive Co-Kriging-assisted multi-fidelity
multi-objective NAS algorithm is proposed to further reduce the computational
cost of NAS by incorporating a clustering-based local multi-fidelity infill
sampling strategy, enabling efficient exploration of the search space for
faster convergence. This algorithm is further accelerated by the use of a novel
continuous encoding method to represent the connections of nodes in each cell
within a generalized cell-based U-Net backbone, thereby decreasing the search
dimension (number of variables). Results indicate that the proposed NAS
algorithm outperforms previously published state-of-the-art methods under
limited computational budget on three numerical benchmarks, a 2D Darcy flow
regression problem and a CHASE_DB1 biomedical image segmentation problem. The
proposed method is subsequently used to create a wind velocity regression model
with application in urban modelling, with the found model able to achieve good
prediction with less computational complexity. Further analysis revealed that
the NAS algorithm independently identified principles undergirding superior
U-Net architectures in other literature, such as the importance of allowing
each cell to incorporate information from prior cells.

</details>


### [222] [Knowledge distillation as a pathway toward next-generation intelligent ecohydrological modeling systems](https://arxiv.org/abs/2509.01972)
*Long Jiang,Yang Yang,Ting Fong May Chui,Morgan Thornwell,Hoshin Vijai Gupta*

Main category: cs.LG

TL;DR: 提出三阶段统一框架，将基于过程的生态水文模型与机器学习结合，通过知识蒸馏逐步嵌入人工智能，实现高效、可解释的智能建模


<details>
  <summary>Details</summary>
Motivation: 传统过程模型计算成本高、结构僵化，机器学习方法缺乏可解释性和迁移性，需要整合两者优势来应对气候变化和人类压力下的复杂环境系统

Method: 三阶段框架：I阶段行为蒸馏（代理学习和模型简化）、II阶段结构蒸馏（过程方程重构为图神经网络模块）、III阶段认知蒸馏（专家推理和自适应决策嵌入智能代理）

Result: 在Samish流域的示范应用中，该框架能够复现过程模型输出、提高预测精度，并支持基于情景的决策制定

Conclusion: 该框架为下一代智能生态水文建模系统提供了可扩展和可迁移的路径，并具有扩展到其他过程建模领域的潜力

Abstract: Simulating ecohydrological processes is essential for understanding complex
environmental systems and guiding sustainable management amid accelerating
climate change and human pressures. Process-based models provide physical
realism but can suffer from structural rigidity, high computational costs, and
complex calibration, while machine learning (ML) methods are efficient and
flexible yet often lack interpretability and transferability. We propose a
unified three-phase framework that integrates process-based models with ML and
progressively embeds them into artificial intelligence (AI) through knowledge
distillation. Phase I, behavioral distillation, enhances process models via
surrogate learning and model simplification to capture key dynamics at lower
computational cost. Phase II, structural distillation, reformulates process
equations as modular components within a graph neural network (GNN), enabling
multiscale representation and seamless integration with ML models. Phase III,
cognitive distillation, embeds expert reasoning and adaptive decision-making
into intelligent modeling agents using the Eyes-Brain-Hands-Mouth architecture.
Demonstrations for the Samish watershed highlight the framework's applicability
to ecohydrological modeling, showing that it can reproduce process-based model
outputs, improve predictive accuracy, and support scenario-based
decision-making. The framework offers a scalable and transferable pathway
toward next-generation intelligent ecohydrological modeling systems, with the
potential extension to other process-based domains.

</details>


### [223] [Semantic and episodic memories in a predictive coding model of the neocortex](https://arxiv.org/abs/2509.01987)
*Lucie Fontaine,Frédéric Alexandre*

Main category: cs.LG

TL;DR: 预测编码模型研究表明，新皮层可以通过密集重叠表征实现有限的样例记忆，但需要海马体的稀疏模式分离表征来处理大量样例记忆。


<details>
  <summary>Details</summary>
Motivation: 挑战互补学习系统理论中语义记忆与情景记忆的二元划分，探索新皮层在预测编码框架下是否具备情景记忆能力。

Method: 构建基于预测编码的新皮层神经网络模型，测试其在少量和大量训练样例下的记忆表现。

Result: 模型在少量样例训练时能回忆具体细节但会过拟合，在大量样例训练时失去回忆能力，说明新皮层只能编码有限数量的样例记忆。

Conclusion: 新皮层确实具备有限的情景记忆能力，但需要海马体的稀疏表征来处理大量记忆，支持互补学习系统理论的合理性。

Abstract: Complementary Learning Systems theory holds that intelligent agents need two
learning systems. Semantic memory is encoded in the neocortex with dense,
overlapping representations and acquires structured knowledge. Episodic memory
is encoded in the hippocampus with sparse, pattern-separated representations
and quickly learns the specifics of individual experiences. Recently, this
duality between semantic and episodic memories has been challenged by
predictive coding, a biologically plausible neural network model of the
neocortex which was shown to have hippocampus-like abilities on
auto-associative memory tasks. These results raise the question of the episodic
capabilities of the neocortex and their relation to semantic memory. In this
paper, we present such a predictive coding model of the neocortex and explore
its episodic capabilities. We show that this kind of model can indeed recall
the specifics of individual examples but only if it is trained on a small
number of examples. The model is overfitted to these exemples and does not
generalize well, suggesting that episodic memory can arise from semantic
learning. Indeed, a model trained with many more examples loses its recall
capabilities. This work suggests that individual examples can be encoded
gradually in the neocortex using dense, overlapping representations but only in
a limited number, motivating the need for sparse, pattern-separated
representations as found in the hippocampus.

</details>


### [224] [ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting](https://arxiv.org/abs/2509.01997)
*Jiacheng Shi,Haibin Wei,Jiang Wang,Xiaowei Xu,Longzhi Du,Taixu Jiang*

Main category: cs.LG

TL;DR: 提出了一种创新的时空学习模型，仅使用两个图（进行中和全局）来学习未来订单分布信息，在物流供需预测中优于传统长序列方法。


<details>
  <summary>Details</summary>
Motivation: 现有的时空分析方法从长时间序列中建模未来订单分布信息，但在线配送平台中的未来订单分布具有强随机性和时间序列不敏感性，传统方法难以有效捕获这些信息且效率低下。

Method: 提出创新性图学习网络框架（ACA-Net），使用自适应未来图学习和创新性交叉注意力机制，仅通过进行中图和全局图来提取未来订单分布信息。

Result: 相比传统时空长序列方法，该方法显著提升了物流供需压力预测性能，并在实际生产环境中验证了有效性。

Conclusion: 该方法通过简化的双图结构有效学习稳健的未来图，大幅改善了物流供需压力预测结果，为在线食品配送平台的调度决策提供了关键指标。

Abstract: Logistical demand-supply forecasting that evaluates the alignment between
projected supply and anticipated demand, is essential for the efficiency and
quality of on-demand food delivery platforms and serves as a key indicator for
scheduling decisions. Future order distribution information, which reflects the
distribution of orders in on-demand food delivery, is crucial for the
performance of logistical demand-supply forecasting. Current studies utilize
spatial-temporal analysis methods to model future order distribution
information from serious time slices. However, learning future order
distribution in online delivery platform is a time-series-insensitive problem
with strong randomness. These approaches often struggle to effectively capture
this information while remaining efficient. This paper proposes an innovative
spatiotemporal learning model that utilizes only two graphs (ongoing and
global) to learn future order distribution information, achieving superior
performance compared to traditional spatial-temporal long-series methods. The
main contributions are as follows: (1) The introduction of ongoing and global
graphs in logistical demand-supply pressure forecasting compared to traditional
long time series significantly enhances forecasting performance. (2) An
innovative graph learning network framework using adaptive future graph
learning and innovative cross attention mechanism (ACA-Net) is proposed to
extract future order distribution information, effectively learning a robust
future graph that substantially improves logistical demand-supply pressure
forecasting outcomes. (3) The effectiveness of the proposed method is validated
in real-world production environments.

</details>


### [225] [Second-Order Tensorial Partial Differential Equations on Graphs](https://arxiv.org/abs/2509.02015)
*Aref Einizade,Fragkiskos D. Malliaros,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: 提出了二阶张量图偏微分方程(So-TPDEGs)框架，解决了现有一阶方法高频信号衰减和信息传播慢的问题，为多域数据的连续建模提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要局限于离散图滤波和一阶导数，会抑制高频信号并减慢信息传播，难以捕捉复杂的多尺度和异配结构。

Method: 利用笛卡尔积图中余弦核的可分离性实现高效谱分解，同时自然保留高频信息，建立了二阶连续乘积图神经网络的理论框架。

Result: 提供了图扰动下稳定性和谱特性方面过平滑行为的严格理论分析，为多域连续图学习奠定了坚实基础。

Conclusion: 二阶TPDEGs框架为处理多交互图数据提供了更有效的连续建模方法，特别适合捕捉复杂多尺度和异配结构，具有广泛的实际应用前景。

Abstract: Processing data that lies on multiple interacting (product) graphs is
increasingly important in practical applications, yet existing methods are
mostly restricted to discrete graph filtering. Tensorial partial differential
equations on graphs (TPDEGs) offer a principled framework for modeling such
multidomain data in a continuous setting. However, current continuous
approaches are limited to first-order derivatives, which tend to dampen
high-frequency signals and slow down information propagation. This makes these
TPDEGs-based approaches less effective for capturing complex, multi-scale, and
heterophilic structures. In this paper, we introduce second-order TPDEGs
(So-TPDEGs) and propose the first theoretically grounded framework for
second-order continuous product graph neural networks. Our approach leverages
the separability of cosine kernels in Cartesian product graphs to implement
efficient spectral decomposition, while naturally preserving high-frequency
information. We provide rigorous theoretical analyses of stability under graph
perturbations and over-smoothing behavior regarding spectral properties. Our
theoretical results establish a robust foundation for advancing continuous
graph learning across multiple practical domains.

</details>


### [226] [Genetic Programming with Model Driven Dimension Repair for Learning Interpretable Appointment Scheduling Rules](https://arxiv.org/abs/2509.02034)
*Huan Zhang,Yang Wang,Ya-Hui Jia,Yi Mei*

Main category: cs.LG

TL;DR: 提出了一种维度感知的遗传编程算法，通过维度修复程序来演化具有维度一致性的医疗预约规则，在保持结构完整性的同时提升规则性能和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统遗传编程演化的医疗预约规则缺乏维度一致性，导致规则难以被终端用户理解和信任，需要一种能够保持维度约束同时探索更广泛规则结构的方法

Method: 开发了维度感知遗传编程算法，核心创新是维度修复程序，通过混合整数线性规划模型优化表达式树的维度一致性，最小化结构变化并确保输出维度符合要求

Result: 在模拟诊所环境中评估，该方法演化的预约规则在目标值和维度一致性方面显著优于人工设计的规则和现有最先进的维度感知遗传编程方法

Conclusion: 该方法能够演化出高质量、可解释的医疗预约规则，为设计更有效和可解释的预约规则提供了新的见解

Abstract: Appointment scheduling is a great challenge in healthcare operations
management. Appointment rules (AR) provide medical practitioners with a simple
yet effective tool to determine patient appointment times. Genetic programming
(GP) can be used to evolve ARs. However, directly applying GP to design ARs may
lead to rules that are difficult for end-users to interpret and trust. A key
reason is that GP is unaware of the dimensional consistency, which ensures that
the evolved rules align with users' domain knowledge and intuitive
understanding. In this paper, we develop a new dimensionally aware GP algorithm
with dimension repair to evolve ARs with dimensional consistency and high
performance. A key innovation of our method is the dimension repair procedure,
which optimizes the dimensional consistency of an expression tree while
minimizing structural changes and ensuring that its output dimension meets the
problem's requirements. We formulate the task as a mixed-integer linear
programming model that can be efficiently solved using common mathematical
programming methods. With the support of the dimension repair procedure, our
method can explore a wider range of AR structures by temporarily breaking the
dimensional consistency of individuals, and then restoring it without altering
their overall structure, thereby identifying individuals with greater potential
advantages. We evaluated the proposed method in a comprehensive set of
simulated clinics. The experimental results demonstrate that our approach
managed to evolve high-quality ARs that significantly outperform not only the
manually designed ARs but also existing state-of-the-art dimensionally aware GP
methods in terms of both objective values and dimensional consistency. In
addition, we analyzed the semantics of the evolved ARs, providing insight into
the design of more effective and interpretable ARs.

</details>


### [227] [Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization Framework with Curvature-Guided Perturbation](https://arxiv.org/abs/2509.02048)
*Yi Yin,Guangquan Zhang,Hua Zuo,Jie Lu*

Main category: cs.LG

TL;DR: 提出了一种新颖的双层优化框架，通过在数据流形上使用局部外在曲率作为隐私脆弱性度量，实现隐私保护与数据效用的最佳平衡


<details>
  <summary>Details</summary>
Motivation: 机器学习模型训练需要数据集，但直接共享原始数据存在成员推理攻击等隐私风险。现有隐私保护技术往往降低数据准确性、特异性和多样性，影响下游任务性能，因此需要在隐私保护和数据效用之间找到最优平衡

Method: 采用双层优化框架：上层任务关注数据效用，通过判别器指导生成过程确保扰动后的潜在变量映射到高质量样本；下层任务关注数据隐私，使用数据流形上的局部外在曲率作为个体对成员推理攻击脆弱性的量化度量，通过向低曲率区域扰动样本来抑制易受攻击的独特特征组合

Result: 广泛的实验评估表明，该方法不仅增强了下游任务对成员推理攻击的抵抗能力，而且在样本质量和多样性方面超越了现有方法

Conclusion: 通过交替优化两个目标，该方法实现了隐私与效用的协同平衡，为解决隐私保护与数据效用之间的权衡问题提供了有效解决方案

Abstract: Machine learning models require datasets for effective training, but directly
sharing raw data poses significant privacy risk such as membership inference
attacks (MIA). To mitigate the risk, privacy-preserving techniques such as data
perturbation, generalization, and synthetic data generation are commonly
utilized. However, these methods often degrade data accuracy, specificity, and
diversity, limiting the performance of downstream tasks and thus reducing data
utility. Therefore, striking an optimal balance between privacy preservation
and data utility remains a critical challenge.
  To address this issue, we introduce a novel bilevel optimization framework
for the publication of private datasets, where the upper-level task focuses on
data utility and the lower-level task focuses on data privacy. In the
upper-level task, a discriminator guides the generation process to ensure that
perturbed latent variables are mapped to high-quality samples, maintaining
fidelity for downstream tasks. In the lower-level task, our framework employs
local extrinsic curvature on the data manifold as a quantitative measure of
individual vulnerability to MIA, providing a geometric foundation for targeted
privacy protection. By perturbing samples toward low-curvature regions, our
method effectively suppresses distinctive feature combinations that are
vulnerable to MIA. Through alternating optimization of both objectives, we
achieve a synergistic balance between privacy and utility. Extensive
experimental evaluations demonstrate that our method not only enhances
resistance to MIA in downstream tasks but also surpasses existing methods in
terms of sample quality and diversity.

</details>


### [228] [LUCIE-3D: A three-dimensional climate emulator for forced responses](https://arxiv.org/abs/2509.02061)
*Haiwen Guan,Troy Arcomano,Ashesh Chattopadhyay,Romit Maulik*

Main category: cs.LG

TL;DR: LUCIE-3D是一个轻量级三维气候模拟器，基于SFNO架构，能够捕捉大气垂直结构，响应气候变化强迫，并保持计算效率和长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够模拟大气三维结构、响应气候变化强迫，同时保持计算效率的气候模拟工具，以支持快速实验和耦合气候动力学研究。

Method: 基于原始LUCIE-2D框架，采用球形傅里叶神经算子(SFNO)主干网络，使用30年ERA5再分析数据在8个垂直σ层上进行训练，整合CO2作为强迫变量，可选整合海表温度(SST)。

Result: 成功再现气候平均值、变率和长期气候变化信号（包括CO2浓度增加下的地表变暖和平流层冷却），捕捉关键动力学过程（赤道开尔文波、MJO、环状模态），极端事件统计表现可信。

Conclusion: LUCIE-3D结合了稳定性、物理一致性和可访问性，是快速实验、消融研究和耦合气候动力学探索的宝贵工具，在古气候研究和未来地球系统模拟中具有应用潜力。

Abstract: We introduce LUCIE-3D, a lightweight three-dimensional climate emulator
designed to capture the vertical structure of the atmosphere, respond to
climate change forcings, and maintain computational efficiency with long-term
stability. Building on the original LUCIE-2D framework, LUCIE-3D employs a
Spherical Fourier Neural Operator (SFNO) backbone and is trained on 30 years of
ERA5 reanalysis data spanning eight vertical {\sigma}-levels. The model
incorporates atmospheric CO2 as a forcing variable and optionally integrates
prescribed sea surface temperature (SST) to simulate coupled ocean--atmosphere
dynamics. Results demonstrate that LUCIE-3D successfully reproduces
climatological means, variability, and long-term climate change signals,
including surface warming and stratospheric cooling under increasing CO2
concentrations. The model further captures key dynamical processes such as
equatorial Kelvin waves, the Madden--Julian Oscillation, and annular modes,
while showing credible behavior in the statistics of extreme events. Despite
requiring longer training than its 2D predecessor, LUCIE-3D remains efficient,
training in under five hours on four GPUs. Its combination of stability,
physical consistency, and accessibility makes it a valuable tool for rapid
experimentation, ablation studies, and the exploration of coupled climate
dynamics, with potential applications extending to paleoclimate research and
future Earth system emulation.

</details>


### [229] [Data-Dependent Smoothing for Protein Discovery with Walk-Jump Sampling](https://arxiv.org/abs/2509.02069)
*Srinivas Anumasa,Barath Chandran. C,Tingting Chen,Dianbo Liu*

Main category: cs.LG

TL;DR: 提出了数据依赖平滑Walk-Jump框架，通过核密度估计为每个数据点估计噪声尺度σ，在蛋白质等稀疏高维数据上取得了更好的生成效果


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型忽略了数据分布的不均匀稀疏性，蛋白质数据中大部分样本分布在稀疏区域，而小部分样本聚集在密集区域，需要数据依赖的噪声尺度估计

Method: 使用核密度估计(KDE)作为预处理步骤，为每个数据点估计数据依赖的噪声尺度σ，然后使用这些σ值训练分数模型，将局部数据几何融入去噪过程

Result: 在多个评估指标上获得了一致的改进，证明了数据感知的sigma预测在稀疏高维设置中的重要性

Conclusion: 通过考虑数据依赖的噪声尺度，能够更好地处理蛋白质等稀疏高维数据的生成建模，该方法对处理不均匀稀疏数据分布具有重要意义

Abstract: Diffusion models have emerged as a powerful class of generative models by
learning to iteratively reverse the noising process. Their ability to generate
high-quality samples has extended beyond high-dimensional image data to other
complex domains such as proteins, where data distributions are typically sparse
and unevenly spread. Importantly, the sparsity itself is uneven. Empirically,
we observed that while a small fraction of samples lie in dense clusters, the
majority occupy regions of varying sparsity across the data space. Existing
approaches largely ignore this data-dependent variability. In this work, we
introduce a Data-Dependent Smoothing Walk-Jump framework that employs kernel
density estimation (KDE) as a preprocessing step to estimate the noise scale
$\sigma$ for each data point, followed by training a score model with these
data-dependent $\sigma$ values. By incorporating local data geometry into the
denoising process, our method accounts for the heterogeneous distribution of
protein data. Empirical evaluations demonstrate that our approach yields
consistent improvements across multiple metrics, highlighting the importance of
data-aware sigma prediction for generative modeling in sparse, high-dimensional
settings.

</details>


### [230] [Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports](https://arxiv.org/abs/2509.02072)
*Jian Chen,Jinbao Tian,Yunqi Xu,Zhou Li*

Main category: cs.LG

TL;DR: ABEX-RAT是一个结合生成式数据增强和对抗训练的新框架，用于解决职业事故报告分类中的类别不平衡问题，在OSHA数据集上达到了90.32%的macro-F1分数。


<details>
  <summary>Details</summary>
Motivation: 职业事故报告自动分类对提升工作场所安全至关重要，但现实数据集中严重的类别不平衡问题会影响模型性能，特别是对罕见但严重的事故类型。

Method: 采用两步抽象-扩展(ABEX)流程：首先用大语言模型提取核心事故语义，然后用生成模型为少数类创建高质量合成样本；随后使用随机对抗训练(RAT)协议训练轻量级分类器。

Result: 在OSHA数据集上实现了新的最先进性能，macro-F1达到90.32%，显著优于之前的SOTA和微调大模型基线。

Conclusion: 这种协同策略是专门化、不平衡分类任务中强力微调的高效替代方案，验证了生成式数据增强与对抗训练结合的有效性。

Abstract: The automatic classification of occupational accident reports is a critical
research area for enhancing workplace safety and enabling large-scale risk
analysis. However, the severe class imbalance inherent in these real-world
datasets often compromises the performance of analytical models, particularly
for rare but severe incident types, hindering the development of reliable
automated systems. To address this challenge, we propose ABEX-RAT, a novel and
efficient framework that synergizes generative data augmentation with robust
adversarial training. Our approach first employs a twostep
abstractive-expansive (ABEX) pipeline, which leverages a large language model
to distill core incident semantics and then uses a generative model to create
diverse, highquality synthetic samples for underrepresented classes.
Subsequently, a lightweight classifier is trained on the augmented data using a
computationally efficient random adversarial training (RAT) protocol, which
stochastically applies perturbations to enhance model generalization and
robustness without significant overhead. Experimental results on the public
OSHA dataset demonstrate that our method achieves new state-of-the-art
performance, reaching a macro-F1 score of 90.32% and significantly
outperforming previous SOTA and fine-tuned large model baselines. Our work
validates that this synergistic strategy is a highly effective and efficient
alternative to brute-force fine-tuning for specialized, imbalanced
classification tasks. The code is publicly available
at:https://github.com/nxcc-lab/ABEX-RAT.

</details>


### [231] [Towards Comprehensive Information-theoretic Multi-view Learning](https://arxiv.org/abs/2509.02084)
*Long Shi,Yunshan Ye,Wenjie Wang,Tao Lei,Yu Zhao,Gang Kou,Badong Chen*

Main category: cs.LG

TL;DR: 提出了CIML框架，突破传统多视图学习中仅关注共同信息的局限，同时利用共同信息和独特信息进行预测，通过信息论方法实现更全面的表征学习。


<details>
  <summary>Details</summary>
Motivation: 传统多视图学习方法基于多视图冗余假设，只关注视图间的共同信息，忽视了每个视图独特信息的预测潜力。本文旨在开发一个更全面的信息论框架，充分利用共同和独特信息。

Method: CIML框架包含两个部分：1）共同表征学习：最大化Gacs-Korner共同信息并基于信息瓶颈压缩；2）独特表征学习：使用信息瓶颈压缩独特信息，同时最小化独特与共同表征之间以及不同独特表征之间的互信息。

Result: 理论证明了学习到的联合表征对下游任务具有预测充分性，大量实验结果表明该模型优于多个最先进方法。

Conclusion: CIML框架成功突破了多视图冗余假设的限制，通过同时利用共同和独特信息，实现了更有效的多视图学习，为信息论在多视图学习中的应用提供了新思路。

Abstract: Information theory has inspired numerous advancements in multi-view learning.
Most multi-view methods incorporating information-theoretic principles rely an
assumption called multi-view redundancy which states that common information
between views is necessary and sufficient for down-stream tasks. This
assumption emphasizes the importance of common information for prediction, but
inherently ignores the potential of unique information in each view that could
be predictive to the task. In this paper, we propose a comprehensive
information-theoretic multi-view learning framework named CIML, which discards
the assumption of multi-view redundancy. Specifically, CIML considers the
potential predictive capabilities of both common and unique information based
on information theory. First, the common representation learning maximizes
Gacs-Korner common information to extract shared features and then compresses
this information to learn task-relevant representations based on the
Information Bottleneck (IB). For unique representation learning, IB is employed
to achieve the most compressed unique representation for each view while
simultaneously minimizing the mutual information between unique and common
representations, as well as among different unique representations.
Importantly, we theoretically prove that the learned joint representation is
predictively sufficient for the downstream task. Extensive experimental results
have demonstrated the superiority of our model over several state-of-art
methods. The code is released on CIML.

</details>


### [232] [DivMerge: A divergence-based model merging method for multi-tasking](https://arxiv.org/abs/2509.02108)
*Touayouch Brahim,Fosse Loïc,Damnati Géraldine,Lecorvé Gwénolé*

Main category: cs.LG

TL;DR: 提出了一种基于Jensen-Shannon散度的多任务模型融合方法，能够在无需额外标注数据的情况下有效解决任务干扰问题，并在任务数量增加时保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着微调模型的增多，多任务学习面临任务干扰的挑战，特别是在任务数量增加时性能下降的问题。现有方法在任务增多时效果不佳，需要一种能够自动平衡任务重要性且无需标注数据的融合方法。

Method: 利用Jensen-Shannon散度来指导模型融合过程，自动平衡不同任务的重要性，无需额外的标注数据。该方法将不同任务上训练的模型合并为单一模型。

Result: 该方法在任务数量增加时保持鲁棒性，始终优于现有工作，能够维持所有任务上的强劲性能。

Conclusion: 提出的基于Jensen-Shannon散度的模型融合方法有效解决了多任务学习中的任务干扰问题，为大规模多任务模型融合提供了可行的解决方案。

Abstract: Multi-task learning (MTL) is often achieved by merging datasets before
fine-tuning, but the growing availability of fine-tuned models has led to new
approaches such as model merging via task arithmetic. A major challenge in this
setting is task interference, which worsens as the number of tasks increases.
We propose a method that merges models trained on different tasks into a single
model, maintaining strong performance across all tasks. Our approach leverages
Jensen-Shannon divergence to guide the merging process without requiring
additional labelled data, and automatically balances task importance. Unlike
existing methods, our approach remains robust as the number of tasks grows and
consistently outperforms prior work.

</details>


### [233] [HiGraph: A Large-Scale Hierarchical Graph Dataset for Malware Analysis](https://arxiv.org/abs/2509.02113)
*Han Chen,Hanchen Wang,Hongmei Chen,Ying Zhang,Lu Qin,Wenjie Zhang*

Main category: cs.LG

TL;DR: 提出了HiGraph数据集，这是最大的恶意软件分析分层图数据集，包含超过200M控制流图嵌套在595K函数调用图中，用于解决现有方法无法建模软件层次结构语义的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的图基恶意软件分析方法通常将程序简化为单层图，无法捕捉高层功能交互与底层指令逻辑之间的关键语义关系，缺乏大规模分层结构数据集限制了该领域的发展。

Method: 构建了两层图表示：函数调用图（FCG）作为高层结构，控制流图（CFG）作为底层结构嵌套其中，形成了包含595K FCG和200M CFG的大规模分层图数据集。

Result: 通过大规模分析揭示了良性软件和恶意软件在结构特性上的显著差异，证明该数据集能够构建对代码混淆和恶意软件演化具有鲁棒性的检测器。

Conclusion: HiGraph数据集为社区提供了基础性基准，其公开可用性将推动图基恶意软件分析领域的发展，数据集和工具可通过https://higraph.org获取。

Abstract: The advancement of graph-based malware analysis is critically limited by the
absence of large-scale datasets that capture the inherent hierarchical
structure of software. Existing methods often oversimplify programs into single
level graphs, failing to model the crucial semantic relationship between
high-level functional interactions and low-level instruction logic. To bridge
this gap, we introduce \dataset, the largest public hierarchical graph dataset
for malware analysis, comprising over \textbf{200M} Control Flow Graphs (CFGs)
nested within \textbf{595K} Function Call Graphs (FCGs). This two-level
representation preserves structural semantics essential for building robust
detectors resilient to code obfuscation and malware evolution. We demonstrate
HiGraph's utility through a large-scale analysis that reveals distinct
structural properties of benign and malicious software, establishing it as a
foundational benchmark for the community. The dataset and tools are publicly
available at https://higraph.org.

</details>


### [234] [Threshold-Based Optimal Arm Selection in Monotonic Bandits: Regret Lower Bounds and Algorithms](https://arxiv.org/abs/2509.02119)
*Chanakya Varude,Jay Chaudhary,Siddharth Kaushik,Prasanna Chaporkar*

Main category: cs.LG

TL;DR: 本文研究基于阈值的多臂老虎机问题，目标是选择与预设阈值τ相关的臂，而不是传统的最优臂选择。在臂均值单调结构下，分析了多种变体并推导了渐近遗憾下界。


<details>
  <summary>Details</summary>
Motivation: 受通信网络(CQI分配)、临床用药、能源管理、推荐系统等应用场景的启发，需要基于阈值约束而非单纯最优奖励的决策机制。

Method: 在臂均值单调结构假设下，研究多种阈值相关变体（首个高于τ、第k个高于/低于τ、最接近τ的臂），推导渐近遗憾下界，并提出相应算法。

Result: 理论分析显示遗憾下界仅取决于与阈值τ相邻的臂，蒙特卡洛模拟验证了所提算法的最优性。

Conclusion: 本文扩展了经典老虎机理论，引入了阈值约束，为实际应用中基于阈值的决策问题提供了理论框架和高效算法。

Abstract: In multi-armed bandit problems, the typical goal is to identify the arm with
the highest reward. This paper explores a threshold-based bandit problem,
aiming to select an arm based on its relation to a prescribed threshold \(\tau
\). We study variants where the optimal arm is the first above \(\tau\), the
\(k^{th}\) arm above or below it, or the closest to it, under a monotonic
structure of arm means. We derive asymptotic regret lower bounds, showing
dependence only on arms adjacent to \(\tau\). Motivated by applications in
communication networks (CQI allocation), clinical dosing, energy management,
recommendation systems, and more. We propose algorithms with optimality
validated through Monte Carlo simulations. Our work extends classical bandit
theory with threshold constraints for efficient decision-making.

</details>


### [235] [Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time](https://arxiv.org/abs/2509.02129)
*Jintao Cheng,Weibin Li,Jiehao Luo,Xiaoyu Tang,Zhijian He,Jin Wu,Yao Zou,Wei Zhang*

Main category: cs.LG

TL;DR: 通过测试时缩放(TTS)框架利用多模态大语言模型的视觉-语言对齐能力，实现了高效的零样本视觉地点识别，计算效率提升210倍。


<details>
  <summary>Details</summary>
Motivation: 当前视觉地点识别方法存在计算开销高、跨域转移能力局限的问题，需要一种无需微调的高效解决方案。

Method: 提出基于测试时缩放(TTS)的零样本框架，采用指导方法涉取MLLM的视觉-语言对齐能力进行直接相似性计分，通过结构化提示生成长度可控的JSON输出。

Result: 实验结果显示该方法在跨域VPR性能上取得显著提升，计算效率最高提升210倍，并具有优秀的通用性。

Conclusion: 该研究提出的TTS框架通过利用MLLM的内在能力，在不增加训练成本的情况下实现了高效、实时的跨域视觉地点识别，为VPR领域提供了一种新的解决方案。

Abstract: Visual Place Recognition (VPR) has evolved from handcrafted descriptors to
deep learning approaches, yet significant challenges remain. Current
approaches, including Vision Foundation Models (VFMs) and Multimodal Large
Language Models (MLLMs), enhance semantic understanding but suffer from high
computational overhead and limited cross-domain transferability when
fine-tuned. To address these limitations, we propose a novel zero-shot
framework employing Test-Time Scaling (TTS) that leverages MLLMs'
vision-language alignment capabilities through Guidance-based methods for
direct similarity scoring. Our approach eliminates two-stage processing by
employing structured prompts that generate length-controllable JSON outputs.
The TTS framework with Uncertainty-Aware Self-Consistency (UASC) enables
real-time adaptation without additional training costs, achieving superior
generalization across diverse environments. Experimental results demonstrate
significant improvements in cross-domain VPR performance with up to 210$\times$
computational efficiency gains.

</details>


### [236] [Online Identification of IT Systems through Active Causal Learning](https://arxiv.org/abs/2509.02130)
*Kim Hammar,Rolf Stadler*

Main category: cs.LG

TL;DR: 本文提出了首个基于数据的在线IT系统因果模型识别方法——主动因果学习，使用高斯过程回归迭代估计系统变量间的因果函数，通过基于rollout的干预策略收集系统测量数据


<details>
  <summary>Details</summary>
Motivation: 传统IT系统因果模型依赖领域专家设计和维护，但随着现代IT系统日益复杂和动态化，这种方法面临挑战。需要自动化、数据驱动的方法来识别因果模型，以实现网络和系统管理任务的自动化

Method: 主动因果学习方法，使用高斯过程回归迭代估计系统变量间的因果函数，通过基于rollout的干预策略收集系统测量数据

Result: 实验验证表明该方法能够准确识别因果系统模型，同时对系统操作产生低干扰

Conclusion: 该方法在贝叶斯意义下是最优的，能够产生有效的干预措施，为IT系统自动化管理提供了有效的因果模型识别解决方案

Abstract: Identifying a causal model of an IT system is fundamental to many branches of
systems engineering and operation. Such a model can be used to predict the
effects of control actions, optimize operations, diagnose failures, detect
intrusions, etc., which is central to achieving the longstanding goal of
automating network and system management tasks. Traditionally, causal models
have been designed and maintained by domain experts. This, however, proves
increasingly challenging with the growing complexity and dynamism of modern IT
systems. In this paper, we present the first principled method for online,
data-driven identification of an IT system in the form of a causal model. The
method, which we call active causal learning, estimates causal functions that
capture the dependencies among system variables in an iterative fashion using
Gaussian process regression based on system measurements, which are collected
through a rollout-based intervention policy. We prove that this method is
optimal in the Bayesian sense and that it produces effective interventions.
Experimental validation on a testbed shows that our method enables accurate
identification of a causal system model while inducing low interference with
system operations.

</details>


### [237] [Simulating classification models to evaluate Predict-Then-Optimize methods](https://arxiv.org/abs/2509.02191)
*Pieter Smet*

Main category: cs.LG

TL;DR: 本文研究了预测-优化方法中预测误差对解质量的影响，通过模拟多分类器预测来实验分析这种关系，发现预测误差与最优解接近度之间的关系并非简单线性。


<details>
  <summary>Details</summary>
Motivation: 预测-优化方法假设更准确的预测会产生更接近最优解的方案，但这一假设在复杂约束优化问题中缺乏实证支持，需要系统分析预测误差对解质量的影响。

Method: 提出了模拟多分类器预测的新算法，通过模拟机器学习模型的预测来实验分析预测误差对解质量的影响，避免了训练真实模型的需求。

Result: 分类器性能可以以合理精度进行模拟，但存在一定变异性。实验表明预测误差与解接近最优解的程度之间存在非平凡关系。

Conclusion: 基于机器学习预测的决策系统设计和评估需要考虑预测误差与解质量之间的复杂关系，这对预测-优化框架的实际应用具有重要意义。

Abstract: Uncertainty in optimization is often represented as stochastic parameters in
the optimization model. In Predict-Then-Optimize approaches, predictions of a
machine learning model are used as values for such parameters, effectively
transforming the stochastic optimization problem into a deterministic one. This
two-stage framework is built on the assumption that more accurate predictions
result in solutions that are closer to the actual optimal solution. However,
providing evidence for this assumption in the context of complex, constrained
optimization problems is challenging and often overlooked in the literature.
Simulating predictions of machine learning models offers a way to
(experimentally) analyze how prediction error impacts solution quality without
the need to train real models. Complementing an algorithm from the literature
for simulating binary classification, we introduce a new algorithm for
simulating predictions of multiclass classifiers. We conduct a computational
study to evaluate the performance of these algorithms, and show that classifier
performance can be simulated with reasonable accuracy, although some
variability is observed. Additionally, we apply these algorithms to assess the
performance of a Predict-Then-Optimize algorithm for a machine scheduling
problem. The experiments demonstrate that the relationship between prediction
error and how close solutions are to the actual optimum is non-trivial,
highlighting important considerations for the design and evaluation of
decision-making systems based on machine learning predictions.

</details>


### [238] [DaCe AD: Unifying High-Performance Automatic Differentiation for Machine Learning and Scientific Computing](https://arxiv.org/abs/2509.02197)
*Afif Boudaoud,Alexandru Calotoiu,Marcin Copik,Torsten Hoefler*

Main category: cs.LG

TL;DR: DaCe AD是一个无需代码修改的通用高效自动微分引擎，使用基于ILP的算法优化存储与重计算权衡，在NPBench基准测试中平均比JAX快92倍以上


<details>
  <summary>Details</summary>
Motivation: 现有AD框架存在编程语言支持有限、需要代码修改、科学计算性能不足、存储策略简单等问题，迫使领域科学家手动计算大型问题的梯度

Method: 开发DaCe AD自动微分引擎，采用新颖的基于整数线性规划(ILP)的算法，在给定内存约束下优化存储与重计算的权衡以达到最佳性能

Result: 在NPBench高性能计算基准测试套件上，DaCe AD平均比当前最先进的通用AD框架JAX快92倍以上，且无需任何代码修改

Conclusion: DaCe AD提供了一个通用高效的自动微分解决方案，解决了现有AD框架的主要局限性，特别适用于科学计算领域的大规模问题

Abstract: Automatic differentiation (AD) is a set of techniques that systematically
applies the chain rule to compute the gradients of functions without requiring
human intervention. Although the fundamentals of this technology were
established decades ago, it is experiencing a renaissance as it plays a key
role in efficiently computing gradients for backpropagation in machine learning
algorithms. AD is also crucial for many applications in scientific computing
domains, particularly emerging techniques that integrate machine learning
models within scientific simulations and schemes. Existing AD frameworks have
four main limitations: limited support of programming languages, requiring code
modifications for AD compatibility, limited performance on scientific computing
codes, and a naive store-all solution for forward-pass data required for
gradient calculations. These limitations force domain scientists to manually
compute the gradients for large problems. This work presents DaCe AD, a
general, efficient automatic differentiation engine that requires no code
modifications. DaCe AD uses a novel ILP-based algorithm to optimize the
trade-off between storing and recomputing to achieve maximum performance within
a given memory constraint. We showcase the generality of our method by applying
it to NPBench, a suite of HPC benchmarks with diverse scientific computing
patterns, where we outperform JAX, a Python framework with state-of-the-art
general AD capabilities, by more than 92 times on average without requiring any
code changes.

</details>


### [239] [Baichuan-M2: Scaling Medical Capability with Large Verifier System](https://arxiv.org/abs/2509.02208)
*Baichuan-M2 Team,:,Chengfeng Dou,Chong Liu,Fan Yang,Fei Li,Jiyuan Jia,Mingyang Chen,Qiang Ju,Shuai Wang,Shunya Dang,Tianpeng Li,Xiangrong Zeng,Yijie Zhou,Chenzheng Zhu,Da Pan,Fei Deng,Guangwei Ai,Guosheng Dong,Hongda Zhang,Jinyang Tai,Jixiang Hong,Kai Lu,Linzhuang Sun,Peidong Guo,Qian Ma,Rihui Xin,Shihui Yang,Shusen Zhang,Yichuan Mo,Zheng Liang,Zhishou Zhang,Hengfu Cui,Zuyi Zhu,Xiaochuan Wang*

Main category: cs.LG

TL;DR: 提出了Baichuan-M2医疗大语言模型，通过动态验证框架和强化学习训练，在医疗AI性能-参数权衡中建立了新的帕累托前沿


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试无法捕捉医疗咨询的动态交互特性，导致医疗大语言模型在静态基准和实际临床应用之间存在显著差距

Method: 建立动态验证框架，包含患者模拟器和临床评估标准生成器；开发32B参数的Baichuan-M2模型，采用多阶段强化学习策略和改进的GRPO算法进行训练

Result: 在HealthBench基准测试中，Baichuan-M2超越所有开源模型和大多数闭源模型，在HealthBench Hard基准上获得32分以上的成绩，此前只有GPT-5能达到

Conclusion: 强大的动态验证系统对于将大语言模型能力与实际临床应用对齐至关重要，为医疗AI部署建立了新的性能-参数权衡标准

Abstract: As large language models (LLMs) advance in conversational and reasoning
capabilities, their practical application in healthcare has become a critical
research focus. However, there is a notable gap between the performance of
medical LLMs on static benchmarks such as USMLE and their utility in real-world
clinical decision-making. This discrepancy arises because traditional exams
fail to capture the dynamic, interactive nature of medical consultations. To
address this challenge, we introduce a novel dynamic verification framework
that moves beyond static answer verifier, establishing a large-scale,
high-fidelity interactive reinforcement learning system. Our framework
comprises two key components: a Patient Simulator that creates realistic
clinical environments using de-identified medical records, and a Clinical
Rubrics Generator that dynamically produces multi-dimensional evaluation
metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter
medical augmented reasoning model trained through a multi-stage reinforcement
learning strategy with an improved Group Relative Policy Optimization (GRPO)
algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other
open-source models and most advanced closed-source counterparts, achieving a
score above 32 on the challenging HealthBench Hard benchmark-previously
exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier
system is essential for aligning LLM capabilities with practical clinical
applications, establishing a new Pareto front in the performance-parameter
trade-off for medical AI deployment.

</details>


### [240] [ST-Hyper: Learning High-Order Dependencies Across Multiple Spatial-Temporal Scales for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.02217)
*Binqing Wu,Jianlong Huang,Zongjiang Shang,Ling Chen*

Main category: cs.LG

TL;DR: ST-Hyper是一个用于多元时间序列预测的新方法，通过自适应超图建模来捕捉多尺度时空依赖关系，在长短期预测上都取得了最佳性能


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在多变量时间序列预测中难以有效建模跨多个时空尺度的依赖关系，需要新的方法来捕捉复杂的时空动态特性

Method: 提出ST-Hyper框架，包含时空金字塔建模模块提取多尺度特征，自适应超图建模模块学习稀疏超图捕捉高阶依赖，以及三阶段超图传播进行特征交互

Result: 在6个真实世界MTS数据集上验证，ST-Hyper达到最先进性能，长短期预测的平均MAE分别降低3.8%和6.8%

Conclusion: 该方法通过超图建模有效捕捉了多尺度时空依赖，为多元时间序列预测提供了新的有效解决方案

Abstract: In multivariate time series (MTS) forecasting, many deep learning based
methods have been proposed for modeling dependencies at multiple spatial
(inter-variate) or temporal (intra-variate) scales. However, existing methods
may fail to model dependencies across multiple spatial-temporal scales
(ST-scales, i.e., scales that jointly consider spatial and temporal scopes). In
this work, we propose ST-Hyper to model the high-order dependencies across
multiple ST-scales through adaptive hypergraph modeling. Specifically, we
introduce a Spatial-Temporal Pyramid Modeling (STPM) module to extract features
at multiple ST-scales. Furthermore, we introduce an Adaptive Hypergraph
Modeling (AHM) module that learns a sparse hypergraph to capture robust
high-order dependencies among features. In addition, we interact with these
features through tri-phase hypergraph propagation, which can comprehensively
capture multi-scale spatial-temporal dynamics. Experimental results on six
real-world MTS datasets demonstrate that ST-Hyper achieves the state-of-the-art
performance, outperforming the best baselines with an average MAE reduction of
3.8\% and 6.8\% for long-term and short-term forecasting, respectively.

</details>


### [241] [VariAntNet: Learning Decentralized Control of Multi-Agent Systems](https://arxiv.org/abs/2509.02271)
*Yigal Koifman,Erez Koifman,Eran Iceland,Ariel Barel,Alfred M. Bruckstein*

Main category: cs.LG

TL;DR: VariAntNet是一个基于深度学习的分散式控制模型，用于简单多智能体系统在灾难响应中的群体协作，通过几何特征提取和新型损失函数显著提高了群体聚集效率和连接性。


<details>
  <summary>Details</summary>
Motivation: 解决简单机器人群体（Ant Robots）在复杂灾难响应环境中由于有限感知、无通信和分散控制导致的群体凝聚和碎片化问题，特别是在时间紧急的救火等场景中需要快速有效的群体协作。

Method: 提出VariAntNet深度学习模型，包括从无序变长局部观测中提取几何特征，使用基于可见性图拉普拉斯矩阵特性的新型可微分多目标损失函数来促进群体凝聚性，并在仅方位有限距离感知的多智能体聚集任务上进行验证。

Result: VariAntNet显著优于现有解析解决方案，收敛率提高两倍以上，同时在不同群体规模下保持高连接性，解决了解析方法虽然保证凝聚性但速度过慢的实际问题。

Conclusion: 在时间关键的应急响应场景中，VariAntNet提供了速度与群体完整性之间的有效权衡，证明了深度学习方法在分散式群体控制中的实用价值，尽管可能损失部分智能体但整体响应效率大幅提升。

Abstract: A simple multi-agent system can be effectively utilized in disaster response
applications, such as firefighting. Such a swarm is required to operate in
complex environments with limited local sensing and no reliable inter-agent
communication or centralized control. These simple robotic agents, also known
as Ant Robots, are defined as anonymous agents that possess limited sensing
capabilities, lack a shared coordinate system, and do not communicate
explicitly with one another. A key challenge for simple swarms lies in
maintaining cohesion and avoiding fragmentation despite limited-range sensing.
Recent advances in machine learning offer effective solutions to some of the
classical decentralized control challenges. We propose VariAntNet, a deep
learning-based decentralized control model designed to facilitate agent
swarming and collaborative task execution. VariAntNet includes geometric
features extraction from unordered, variable-sized local observations. It
incorporates a neural network architecture trained with a novel,
differentiable, multi-objective, mathematically justified loss function that
promotes swarm cohesiveness by utilizing the properties of the visibility graph
Laplacian matrix. VariAntNet is demonstrated on the fundamental multi-agent
gathering task, where agents with bearing-only and limited-range sensing must
gather at some location. VariAntNet significantly outperforms an existing
analytical solution, achieving more than double the convergence rate while
maintaining high swarm connectivity across varying swarm sizes. While the
analytical solution guarantees cohesion, it is often too slow in practice. In
time-critical scenarios, such as emergency response operations where lives are
at risk, slower analytical methods are impractical and justify the loss of some
agents within the swarm. This paper presents and analyzes this trade-off in
detail.

</details>


### [242] [Balanced Multimodal Learning: An Unidirectional Dynamic Interaction Perspective](https://arxiv.org/abs/2509.02281)
*Shijie Wang,Li Zhang,Xinyan Liang,Yuhua Qian,Shen Hu*

Main category: cs.LG

TL;DR: 提出UDI方法解决多模态学习中的模态不平衡问题，通过顺序训练和单向动态交互替代传统的联合损失函数


<details>
  <summary>Details</summary>
Motivation: 传统多模态联合损失会导致模态不平衡，强势模态压制弱势模态，限制了各模态信息和模态间交互信息的充分利用

Method: UDI方法：先训练锚定模态至收敛，然后用其学习到的表示通过无监督损失指导其他模态训练，实现单向动态交互

Result: 实验结果表明UDI在处理模态不平衡方面优于现有方法，在多模态学习任务中实现了性能提升

Conclusion: UDI通过解耦模态优化和启用定向信息流，有效防止单一模态主导，促进了有效的跨模态特征学习

Abstract: Multimodal learning typically utilizes multimodal joint loss to integrate
different modalities and enhance model performance. However, this joint
learning strategy can induce modality imbalance, where strong modalities
overwhelm weaker ones and limit exploitation of individual information from
each modality and the inter-modality interaction information.Existing
strategies such as dynamic loss weighting, auxiliary objectives and gradient
modulation mitigate modality imbalance based on joint loss. These methods
remain fundamentally reactive, detecting and correcting imbalance after it
arises, while leaving the competitive nature of the joint loss untouched. This
limitation drives us to explore a new strategy for multimodal imbalance
learning that does not rely on the joint loss, enabling more effective
interactions between modalities and better utilization of information from
individual modalities and their interactions. In this paper, we introduce
Unidirectional Dynamic Interaction (UDI), a novel strategy that abandons the
conventional joint loss in favor of a proactive, sequential training scheme.
UDI first trains the anchor modality to convergence, then uses its learned
representations to guide the other modality via unsupervised loss. Furthermore,
the dynamic adjustment of modality interactions allows the model to adapt to
the task at hand, ensuring that each modality contributes optimally. By
decoupling modality optimization and enabling directed information flow, UDI
prevents domination by any single modality and fosters effective cross-modal
feature learning. Our experimental results demonstrate that UDI outperforms
existing methods in handling modality imbalance, leading to performance
improvement in multimodal learning tasks.

</details>


### [243] [AdaSwitch: An Adaptive Switching Meta-Algorithm for Learning-Augmented Bounded-Influence Problems](https://arxiv.org/abs/2509.02302)
*Xi Chen,Yuze Chen,Yuan Zhou*

Main category: cs.LG

TL;DR: 提出了一个带序列预测的多周期在线决策框架AdaSwitch，能在预测准确时接近离线最优性能，在预测不准时保持经典竞争比保证


<details>
  <summary>Details</summary>
Motivation: 解决机器学习预测不保证准确性的在线决策问题，需要在利用预测提升性能的同时保持鲁棒性

Method: 引入有限影响框架，提出AdaSwitch元算法，根据预测准确性动态切换策略

Result: 框架适用于多种场景（处理系统提前期报价、k-server问题、可重用资源分配），在预测准确时性能接近离线最优，不准时保持竞争比保证

Conclusion: 该学习增强的在线决策框架具有灵活性和广泛适用性，为利用不完美预测提供了有效方法

Abstract: We study a class of multi-period online decision-making problems with
sequence-based predictions, which may be generated by machine learning models
but whose accuracy is not guaranteed. In each period, the decision-maker
observes the realized request and must take an irrevocable action that yields a
reward or incurs a cost, without knowledge of future arrivals. We introduce a
bounded-influence framework, in which past decisions and requests exert only
limited impact on the future optimal reward. Within this framework, we propose
the AdaSwitch meta-algorithm, which exploits predictions to attain performance
close to the offline benchmark when predictions are accurate, while preserving
classical competitive-ratio guarantees under highly inaccurate predictions. Our
framework and meta-algorithm apply to diverse settings, including lead-time
quotation in processing systems, the $k$-server problem, and online allocation
of reusable resources. These applications illustrate the flexibility and broad
applicability of our approach to learning-augmented online decision-making.

</details>


### [244] [Extrapolated Markov Chain Oversampling Method for Imbalanced Text Classification](https://arxiv.org/abs/2509.02332)
*Aleksi Avela,Pauliina Ilmonen*

Main category: cs.LG

TL;DR: 本文提出了一种基于马尔可夫链的文本过采样方法，通过结合少数类和多数类的特征来扩展少数类特征空间，有效处理文本分类中的不平衡数据问题。


<details>
  <summary>Details</summary>
Motivation: 现实文本分类任务中经常存在类别不平衡问题，传统过采样方法在处理文本数据时面临特征空间扩展的独特挑战，需要专门针对文本特性的解决方案。

Method: 使用马尔可夫链模型进行文本过采样，从少数类估计转移概率，同时部分借鉴多数类的特征，从而在过采样过程中扩展少数类的特征空间。

Result: 在多个真实数据集上的评估表明，该方法相比其他主流过采样方法能够产生极具竞争力的结果，特别是在严重不平衡的情况下表现优异。

Conclusion: 基于马尔可夫链的文本过采样方法能够有效处理文本分类中的类别不平衡问题，特别是在严重不平衡场景下展现出优越性能，为文本数据的不平衡分类提供了新的解决方案。

Abstract: Text classification is the task of automatically assigning text documents
correct labels from a predefined set of categories. In real-life (text)
classification tasks, observations and misclassification costs are often
unevenly distributed between the classes - known as the problem of imbalanced
data. Synthetic oversampling is a popular approach to imbalanced
classification. The idea is to generate synthetic observations in the minority
class to balance the classes in the training set. Many general-purpose
oversampling methods can be applied to text data; however, imbalanced text data
poses a number of distinctive difficulties that stem from the unique nature of
text compared to other domains. One such factor is that when the sample size of
text increases, the sample vocabulary (i.e., feature space) is likely to grow
as well. We introduce a novel Markov chain based text oversampling method. The
transition probabilities are estimated from the minority class but also partly
from the majority class, thus allowing the minority feature space to expand in
oversampling. We evaluate our approach against prominent oversampling methods
and show that our approach is able to produce highly competitive results
against the other methods in several real data examples, especially when the
imbalance is severe.

</details>


### [245] [RDIT: Residual-based Diffusion Implicit Models for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2509.02341)
*Chih-Yu Lai,Yu-Chien Ning,Duane S. Boning*

Main category: cs.LG

TL;DR: RDIT是一个概率时间序列预测框架，通过结合点估计和基于残差的条件扩散，使用双向Mamba网络实现最优分布匹配，在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有概率时间序列预测方法存在分布建模不优和训练评估指标不匹配的问题，研究发现为强点估计器添加零均值高斯噪声可以取得优异性能。

Method: 提出RDIT框架，结合点估计和基于残差的条件扩散，使用双向Mamba网络，理论证明可以通过调整最优标准差来最小化CRPS，并推导实现分布匹配的算法。

Result: 在8个多元数据集上评估，RDIT相比强基线实现了更低的CRPS、更快的推理速度和更好的覆盖率。

Conclusion: RDIT框架通过创新的点估计与扩散模型结合方式，有效解决了概率时间序列预测中的分布建模问题，取得了优异的性能表现。

Abstract: Probabilistic Time Series Forecasting (PTSF) plays a critical role in domains
requiring accurate and uncertainty-aware predictions for decision-making.
However, existing methods offer suboptimal distribution modeling and suffer
from a mismatch between training and evaluation metrics. Surprisingly, we found
that augmenting a strong point estimator with a zero-mean Gaussian, whose
standard deviation matches its training error, can yield state-of-the-art
performance in PTSF. In this work, we propose RDIT, a plug-and-play framework
that combines point estimation and residual-based conditional diffusion with a
bidirectional Mamba network. We theoretically prove that the Continuous Ranked
Probability Score (CRPS) can be minimized by adjusting to an optimal standard
deviation and then derive algorithms to achieve distribution matching.
Evaluations on eight multivariate datasets across varied forecasting horizons
demonstrate that RDIT achieves lower CRPS, rapid inference, and improved
coverage compared to strong baselines.

</details>


### [246] [Scaffolding Collaborative Learning in STEM: A Two-Year Evaluation of a Tool-Integrated Project-Based Methodology](https://arxiv.org/abs/2509.02355)
*Caterina Fuster-Barcelo,Gonzalo R. Rios-Munoz,Arrate Munoz-Barrutia*

Main category: cs.LG

TL;DR: 该研究通过整合数字协作工具和结构化同伴评估，在生物医学图像处理课程中改善了学生参与度和评估公平性，结果显示成绩分布更广、学习投入度提高


<details>
  <summary>Details</summary>
Motivation: 旨在探索如何通过数字协作工具和结构化同伴评估机制来提升STEM教育中的学习效果和评估公平性，特别是在机器学习和健康领域的硕士课程中

Method: 重新设计生物医学图像处理课程，结合Google Colab实时编程、Weights & Biases实验跟踪和报告系统，以及基于量规的同伴评估方法，在两个学年进行实施

Result: 与干预前相比，实施年份显示出成绩分散度增加和最终项目分数熵值更高，表明评估的区分度和公平性得到改善；调查结果显示学生对学科和自身学习过程的投入度更高

Conclusion: 整合工具支持的协作和结构化评估机制有潜力显著提升STEM教育的学习成果和公平性，数字工具与结构化评估框架的结合是有效的教学改进策略

Abstract: This study examines the integration of digital collaborative tools and
structured peer evaluation in the Machine Learning for Health master's program,
through the redesign of a Biomedical Image Processing course over two academic
years. The pedagogical framework combines real-time programming with Google
Colab, experiment tracking and reporting via Weights & Biases, and
rubric-guided peer assessment to foster student engagement, transparency, and
fair evaluation. Compared to a pre-intervention cohort, the two implementation
years showed increased grade dispersion and higher entropy in final project
scores, suggesting improved differentiation and fairness in assessment. The
survey results further indicate greater student engagement with the subject and
their own learning process. These findings highlight the potential of
integrating tool-supported collaboration and structured evaluation mechanisms
to enhance both learning outcomes and equity in STEM education.

</details>


### [247] [Evaluating Cumulative Spectral Gradient as a Complexity Measure](https://arxiv.org/abs/2509.02399)
*Haji Gul,Abdul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.LG

TL;DR: 本文批判性评估了CSG指标在知识图谱链接预测中的有效性，发现其对参数敏感且与性能指标相关性弱


<details>
  <summary>Details</summary>
Motivation: 准确评估数据集复杂度对于知识图谱链接预测模型比较至关重要，需要验证CSG指标在此领域的适用性

Method: 在标准知识图谱链接预测基准上系统测试CSG指标，分析其两个关键参数(M和K)的影响，并与MRR等性能指标进行相关性分析

Result: CSG对K参数高度敏感，无法随类别数量自然扩展；与MRR等指标相关性弱或无相关性；在FB15k-237、WN18RR等数据集上表现不稳定

Conclusion: CSG在链接预测场景中缺乏稳定性和预测能力，需要开发更鲁棒的、分类器无关的复杂度度量方法

Abstract: Accurate estimation of dataset complexity is crucial for evaluating and
comparing link prediction models for knowledge graphs (KGs). The Cumulative
Spectral Gradient (CSG) metric derived from probabilistic divergence between
classes within a spectral clustering framework was proposed as a dataset
complexity measure that (1) naturally scales with the number of classes and (2)
correlates strongly with downstream classification performance. In this work,
we rigorously assess CSG behavior on standard knowledge graph link prediction
benchmarks a multi class tail prediction task, using two key parameters
governing its computation, M, the number of Monte Carlo sampled points per
class, and K, the number of nearest neighbors in the embedding space. Contrary
to the original claims, we find that (1) CSG is highly sensitive to the choice
of K and therefore does not inherently scale with the number of target classes,
and (2) CSG values exhibit weak or no correlation with established performance
metrics such as mean reciprocal rank (MRR). Through experiments on FB15k 237,
WN18RR, and other standard datasets, we demonstrate that CSG purported
stability and generalization predictive power break down in link prediction
settings. Our results highlight the need for more robust, classifier agnostic
complexity measures in KG link prediction evaluation.

</details>


### [248] [Fisher information flow in artificial neural networks](https://arxiv.org/abs/2509.02407)
*Maximilian Weimar,Lukas M. Rachbauer,Ilya Starshynov,Daniele Faccio,Linara Adilova,Dorian Bouchet,Stefan Rotter*

Main category: cs.LG

TL;DR: 该论文提出了一种监测Fisher信息在人工神经网络中流动的方法，用于参数估计任务，并发现最优估计性能对应于Fisher信息的最大传输，训练超过此点会导致信息损失和过拟合。


<details>
  <summary>Details</summary>
Motivation: 随着人工神经网络逐渐成为测量系统的重要组成部分，需要理解它们如何处理和传输参数相关信息，特别是Fisher信息在神经网络中的传播方式。

Method: 开发了一种方法来监测Fisher信息在神经网络中的流动，从输入层到输出层追踪信息传输，建立了最优估计性能与Fisher信息最大传输之间的对应关系。

Result: 研究表明最优估计性能对应于Fisher信息的最大传输，训练超过此点会导致信息损失和过拟合，这提供了一个无需单独验证数据集的模型训练停止准则。

Conclusion: 该方法为神经网络训练提供了基于Fisher信息的模型自由停止准则，在实际物理实验数据上验证了其有效性，具有重要的实际应用价值。

Abstract: The estimation of continuous parameters from measured data plays a central
role in many fields of physics. A key tool in understanding and improving such
estimation processes is the concept of Fisher information, which quantifies how
information about unknown parameters propagates through a physical system and
determines the ultimate limits of precision. With Artificial Neural Networks
(ANNs) gradually becoming an integral part of many measurement systems, it is
essential to understand how they process and transmit parameter-relevant
information internally. Here, we present a method to monitor the flow of Fisher
information through an ANN performing a parameter estimation task, tracking it
from the input to the output layer. We show that optimal estimation performance
corresponds to the maximal transmission of Fisher information, and that
training beyond this point results in information loss due to overfitting. This
provides a model-free stopping criterion for network training-eliminating the
need for a separate validation dataset. To demonstrate the practical relevance
of our approach, we apply it to a network trained on data from an imaging
experiment, highlighting its effectiveness in a realistic physical setting.

</details>


### [249] [Cache Management for Mixture-of-Experts LLMs -- extended version](https://arxiv.org/abs/2509.02408)
*Spyros Angelopoulos,Loris Marchal,Adrien Obrecht,Bertrand Simon*

Main category: cs.LG

TL;DR: 本文研究了MoE架构LLM中的专家缓存管理问题，提出了针对分层架构优化的层感知LRU算法，在理论和实验上都优于传统缓存策略


<details>
  <summary>Details</summary>
Motivation: 解决MoE架构大语言模型中专家参数的高效缓存管理问题，避免频繁访问慢速存储，提升推理效率

Method: 提出新的分页问题模型，分析确定性/随机算法的竞争比下界，设计针对分层LLM架构的层感知LRU扩展算法

Result: 理论分析表明LRU类策略具有良好竞争性能，实验证明新算法在合成数据集和实际MoE使用轨迹上都优于标准LRU

Conclusion: 针对MoE架构LLM的专家缓存管理需要专门设计的算法，层感知LRU扩展能有效提升缓存效率，为实际部署提供理论指导和实用方案

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a variety of tasks. One of the main challenges towards the successful
deployment of LLMs is memory management, since they typically involve billions
of parameters. To this end, architectures based on Mixture-of-Experts have been
proposed, which aim to reduce the size of the parameters that are activated
when producing a token. This raises the equally critical issue of efficiently
managing the limited cache of the system, in that frequently used experts
should be stored in the fast cache rather than in the slower secondary memory.
  In this work, we introduce and study a new paging problem that models expert
management optimization. Our formulation captures both the layered architecture
of LLMs and the requirement that experts are cached efficiently. We first
present lower bounds on the competitive ratio of both deterministic and
randomized algorithms, which show that under mild assumptions, LRU-like
policies have good theoretical competitive performance. We then propose a
layer-based extension of LRU that is tailored to the problem at hand.
  Extensive simulations on both synthetic datasets and actual traces of MoE
usage show that our algorithm outperforms policies for the classic paging
problem, such as the standard LRU.

</details>


### [250] [Learnable Loss Geometries with Mirror Descent for Scalable and Convergent Meta-Learning](https://arxiv.org/abs/2509.02418)
*Yilang Zhang,Bingcong Li,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 本文提出了一种新的元学习方法，通过神经网络学习非线性镜像映射来适应复杂的损失几何形状，显著提高了少样本学习中的任务适应效率。


<details>
  <summary>Details</summary>
Motivation: 元学习面临的主要挑战是如何高效地利用从相关任务中获得的任务不变知识来快速适应新任务。传统的线性预处理方法在处理复杂损失几何时效果有限，需要更强大的方法来捕捉和优化各种损失几何形状。

Method: 提出学习一个通用的距离生成函数，该函数通过表达能力强的神经网络实现，能够诱导非线性镜像映射来有效处理复杂的损失度量。这种方法提供了可证明的有效距离，并建立了收敛性保证。

Result: 理论分析表明该方法具有与标准梯度元学习方法相当的收敛速率（O(ε⁻²)）。在少样本学习数据集上的数值测试显示，新算法具有优越的实证性能，显著减少了适应步骤数量，适用于大规模元学习模型。

Conclusion: 所提出的非线性镜像映射方法能够有效处理复杂损失几何，在保持理论收敛性的同时，显著提高了元学习的任务适应效率，为大规模少样本学习提供了实用解决方案。

Abstract: Utilizing task-invariant knowledge acquired from related tasks as prior
information, meta-learning offers a principled approach to learning a new task
with limited data records. Sample-efficient adaptation of this prior
information is a major challenge facing meta-learning, and plays an important
role because it facilitates training the sought task-specific model with just a
few optimization steps. Past works deal with this challenge through
preconditioning that speeds up convergence of the per-task training. Though
effective in representing locally quadratic loss curvatures, simple linear
preconditioning can be hardly potent with complex loss geometries. Instead of
relying on a quadratic distance metric, the present contribution copes with
complex loss metrics by learning a versatile distance-generating function,
which induces a nonlinear mirror map to effectively capture and optimize a wide
range of loss geometries. With suitable parameterization, this generating
function is effected by an expressive neural network that is provably a valid
distance. Analytical results establish convergence of not only the proposed
method, but also all meta-learning approaches based on preconditioning. To
attain gradient norm less than $\epsilon$, the convergence rate of
$\mathcal{O}(\epsilon^{-2})$ is on par with standard gradient-based
meta-learning methods. Numerical tests on few-shot learning datasets
demonstrate the superior empirical performance of the novel algorithm, as well
as its rapid per-task convergence, which markedly reduces the number of
adaptation steps, hence also accommodating large-scale meta-learning models.

</details>


### [251] [VASSO: Variance Suppression for Sharpness-Aware Minimization](https://arxiv.org/abs/2509.02433)
*Bingcong Li,Yilang Zhang,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: VASSO方法通过抑制方差来稳定SAM中的对抗扰动，避免"过度友好对抗"问题，在多个视觉和语言任务上验证了更好的泛化性能，并提供了良好的泛化-计算权衡。


<details>
  <summary>Details</summary>
Motivation: SAM方法虽然通过寻找平坦最小值来提升泛化能力，但在实践中存在"过度友好对抗"问题，这会限制模型的最终泛化性能。

Method: 提出VASSO（方差抑制）方法来稳定对抗扰动，通过抑制方差来避免对抗扰动过于友好，从而获得更好的泛化效果。

Result: 在广泛的视觉和语言任务上数值验证了VASSO+SAM组合的改进泛化能力，特别是在计算高效的SAM变体上应用时提供了理想的泛化-计算权衡。

Conclusion: VASSO提供了一种通用的方法来稳定对抗扰动，有效解决了SAM中的过度友好对抗问题，显著提升了模型的泛化性能。

Abstract: Sharpness-aware minimization (SAM) has well-documented merits in enhancing
generalization of deep neural network models. Accounting for sharpness in the
loss function geometry, where neighborhoods of `flat minima' heighten
generalization ability, SAM seeks `flat valleys' by minimizing the maximum loss
provoked by an adversarial perturbation within the neighborhood. Although
critical to account for sharpness of the loss function, in practice SAM suffers
from `over-friendly adversaries,' which can curtail the outmost level of
generalization. To avoid such `friendliness,' the present contribution fosters
stabilization of adversaries through variance suppression (VASSO). VASSO offers
a general approach to provably stabilize adversaries. In particular, when
integrating VASSO with SAM, improved generalizability is numerically validated
on extensive vision and language tasks. Once applied on top of a
computationally efficient SAM variant, VASSO offers a desirable
generalization-computation tradeoff.

</details>


### [252] [Generative Sequential Notification Optimization via Multi-Objective Decision Transformers](https://arxiv.org/abs/2509.02458)
*Borja Ocejo,Ruofan Wang,Ke Liu,Rohit K. Patra,Haotian Shen,David Liu,Yiwen Yuan,Gokulraj Mohanasundaram,Fedor Borisyuk,Prakruthi Prabhakar*

Main category: cs.LG

TL;DR: 通过采用决策Transformer框架，将通知推荐问题重构为回报条件监督学习，解决了离线强化学习方法的稳定性和可重现性问题，在LinkedIn通知系统中实现了更高的会话增长和更优的用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统离线强化学习方法（如CQL）在通知推荐场景中存在稳定性、效果可重现性、分布偏移敏感性以及高维度环境下的解释性问题，需要更稳健、可扩展的解决方案。

Method: 提出了基于决策Transformer的框架，将策略学习重构为回报条件监督学习问题；包括多奖励设计、分位数回归的回报条件化方法以及生产环境可用的环形缓冲区序列处理系统。

Result: 在LinkedIn部署的通知系统中，该方法相比多目标CQL策略提升了+0.72%的会话数量，通过提高通知推荐的相关性来优化通知效用和用户活动，同时最小化用户疲劳。

Conclusion: 决策Transformer框架在通知推荐任务中显示出更优的稳健性、可扩展性和效果，为实际生产环境中的序列决策问题提供了替代传统离线强化学习方法的有效解决方案。

Abstract: Notifications are an important communication channel for delivering timely
and relevant information. Optimizing their delivery involves addressing complex
sequential decision-making challenges under constraints such as message utility
and user fatigue. Offline reinforcement learning (RL) methods, such as
Conservative Q-Learning (CQL), have been applied to this problem but face
practical challenges at scale, including instability, sensitivity to
distribution shifts, limited reproducibility, and difficulties with
explainability in high-dimensional recommendation settings. We present a
Decision Transformer (DT) based framework that reframes policy learning as
return-conditioned supervised learning, improving robustness, scalability, and
modeling flexibility. Our contributions include a real-world comparison with
CQL, a multi-reward design suitable for non-episodic tasks, a quantile
regression approach to return-to-go conditioning, and a production-ready system
with circular buffer-based sequence processing for near-real-time inference.
Extensive offline and online experiments in a deployed notification system show
that our approach improves notification utility and overall session activity
while minimizing user fatigue. Compared to a multi-objective CQL-based agent,
the DT-based approach achieved a +0.72% increase in sessions for notification
decision-making at LinkedIn by making notification recommendation more
relevant.

</details>


### [253] [Exploring Variational Graph Autoencoders for Distribution Grid Data Generation](https://arxiv.org/abs/2509.02469)
*Syed Zain Abbas,Ehimare Okoyomon*

Main category: cs.LG

TL;DR: 使用变分图自编码器(VGAE)生成合成配电网络，评估了四种解码器变体，发现简单解码器效果不佳，GCN方法在简单数据集表现良好但在复杂数据集存在缺陷


<details>
  <summary>Details</summary>
Motivation: 解决能源网络机器学习研究中公开电力系统数据缺乏的问题

Method: 使用变分图自编码器(VGAE)技术，在ENGAGE和DINGO两个开源数据集上评估四种解码器变体

Result: 简单解码器无法捕捉真实拓扑，基于GCN的方法在ENGAGE数据集上表现良好但在更复杂的DINGO数据集上产生断开组件和重复模式等伪影

Conclusion: VGAE在电网合成方面既有前景也有局限性，需要更具表现力的生成模型和更稳健的评估方法

Abstract: To address the lack of public power system data for machine learning research
in energy networks, we investigate the use of variational graph autoencoders
(VGAEs) for synthetic distribution grid generation. Using two open-source
datasets, ENGAGE and DINGO, we evaluate four decoder variants and compare
generated networks against the original grids using structural and spectral
metrics. Results indicate that simple decoders fail to capture realistic
topologies, while GCN-based approaches achieve strong fidelity on ENGAGE but
struggle on the more complex DINGO dataset, producing artifacts such as
disconnected components and repeated motifs. These findings highlight both the
promise and limitations of VGAEs for grid synthesis, underscoring the need for
more expressive generative models and robust evaluation. We release our models
and analysis as open source to support benchmarking and accelerate progress in
ML-driven power system research.

</details>


### [254] [SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning](https://arxiv.org/abs/2509.02479)
*Zhenghai Xue,Longtao Zheng,Qian Liu,Yingru Li,Xiaosen Zheng,Zejun Ma,Bo An*

Main category: cs.LG

TL;DR: SimpleTIR算法通过过滤无效轮次轨迹来稳定多轮工具集成推理的强化学习训练，解决了分布漂移导致的梯度爆炸问题，在数学推理基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 多轮工具集成推理(TIR)的强化学习训练存在不稳定性和性能崩溃问题，主要原因是外部工具反馈导致的分布漂移和低概率令牌生成，这会引发灾难性梯度爆炸。

Method: 提出SimpleTIR算法，核心策略是识别并过滤掉包含无效轮次（既不生成代码块也不产生最终答案的轮次）的轨迹，从而在策略更新中阻断有害的高幅度梯度。

Result: 在数学推理基准测试中表现优异，将AIME24分数从纯文本基线的22.1提升到50.5（基于Qwen2.5-7B模型），并促进了自我修正和交叉验证等多样化推理模式的发现。

Conclusion: SimpleTIR是一种即插即用的稳定算法，有效解决了多轮TIR训练的不稳定性问题，避免了监督微调的限制，推动了复杂推理模式的发展。

Abstract: Large Language Models (LLMs) can significantly improve their reasoning
capabilities by interacting with external tools, a paradigm known as
Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios
using Reinforcement Learning (RL) is often hindered by training instability and
performance collapse. We identify that such instability is primarily caused by
a distributional drift from external tool feedback, leading to the generation
of low-probability tokens. This issue compounds over successive turns, causing
catastrophic gradient norm explosions that derail the training process. To
address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that
stabilizes multi-turn TIR training. Its core strategy is to identify and filter
out trajectories containing void turns, i.e., turns that yield neither a code
block nor a final answer. By removing these problematic trajectories from the
policy update, SimpleTIR effectively blocks the harmful, high-magnitude
gradients, thus stabilizing the learning dynamics. Extensive experiments show
that SimpleTIR achieves state-of-the-art performance on challenging math
reasoning benchmarks, notably elevating the AIME24 score from a text-only
baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model.
Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR
encourages the model to discover diverse and sophisticated reasoning patterns,
such as self-correction and cross-validation.

</details>


### [255] [HydroGAT: Distributed Heterogeneous Graph Attention Transformer for Spatiotemporal Flood Prediction](https://arxiv.org/abs/2509.02481)
*Aishwarya Sarkar,Autrin Hakimi,Xiaoqiong Chen,Hai Huang,Chaoqun Lu,Ibrahim Demir,Ali Jannesari*

Main category: cs.LG

TL;DR: HydroGAT是一个基于异构图神经网络的水文预测模型，通过高分辨率像素级节点和自适应时空注意力机制，在洪水预测中实现了优异的性能表现和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统洪水预测方法忽略流域拓扑信息，现有GNN方法因计算成本高而采用粗粒度聚合，且时空依赖关系分离处理，无法同时捕捉对准确洪水预测至关重要的时空交互作用。

Method: 构建异质流域图（每个陆地和河流像素作为节点），提出HydroGAT时空网络，自适应学习局部时间重要性和最有影响力的上游位置，并开发分布式数据并行训练管道。

Result: 在两个美国中西部流域和五个基线架构评估中，模型在小时流量预测中达到NSE最高0.97、KGE最高0.96，PBIAS在±5%以内，同时在NERSC Perlmutter超算上实现15倍加速。

Conclusion: HydroGAT通过高分辨率像素级建模和自适应时空注意力机制，显著提升了洪水预测精度，提供了可解释的注意力图谱，并展示了在大规模计算环境下的高效可扩展性。

Abstract: Accurate flood forecasting remains a challenge for water-resource management,
as it demands modeling of local, time-varying runoff drivers (e.g.,
rainfall-induced peaks, baseflow trends) and complex spatial interactions
across a river network. Traditional data-driven approaches, such as
convolutional networks and sequence-based models, ignore topological
information about the region. Graph Neural Networks (GNNs) propagate
information exactly along the river network, which is ideal for learning
hydrological routing. However, state-of-the-art GNN-based flood prediction
models collapse pixels to coarse catchment polygons as the cost of training
explodes with graph size and higher resolution. Furthermore, most existing
methods treat spatial and temporal dependencies separately, either applying
GNNs solely on spatial graphs or transformers purely on temporal sequences,
thus failing to simultaneously capture spatiotemporal interactions critical for
accurate flood prediction. We introduce a heterogenous basin graph where every
land and river pixel is a node connected by physical hydrological flow
directions and inter-catchment relationships. We propose HydroGAT, a
spatiotemporal network that adaptively learns local temporal importance and the
most influential upstream locations. Evaluated in two Midwestern US basins and
across five baseline architectures, our model achieves higher NSE (up to 0.97),
improved KGE (up to 0.96), and low bias (PBIAS within $\pm$5%) in hourly
discharge prediction, while offering interpretable attention maps that reveal
sparse, structured intercatchment influences. To support high-resolution
basin-scale training, we develop a distributed data-parallel pipeline that
scales efficiently up to 64 NVIDIA A100 GPUs on NERSC Perlmutter supercomputer,
demonstrating up to 15x speedup across machines. Our code is available at
https://github.com/swapp-lab/HydroGAT.

</details>


### [256] [RNN Generalization to Omega-Regular Languages](https://arxiv.org/abs/2509.02491)
*Charles Pert,Dalal Alrajeh,Alessandra Russo*

Main category: cs.LG

TL;DR: 该研究首次探讨了循环神经网络(RNN)能否泛化到从LTL公式派生的ω-正则语言，通过在周期性ω词序列上训练RNN来复制目标Büchi自动机行为，并在分布外序列上评估泛化能力。


<details>
  <summary>Details</summary>
Motivation: Büchi自动机在验证反应式系统时面临可扩展性挑战，而神经网络被越来越多地用于解决模型检查等领域的可扩展性问题，因此需要研究其泛化能力。

Method: 在周期性ω词序列上训练RNN来复制目标Büchi自动机行为，并在LTL公式对应的确定性自动机上进行实验，自动机结构复杂度从3到100多个状态不等。

Result: RNN在评估比训练示例长8倍的序列时，在目标ω-正则语言上实现了高精度，92.6%的任务实现了完美或接近完美的泛化。

Conclusion: 这些结果确立了神经网络方法学习复杂ω-正则语言的可行性，表明它们有潜力作为神经符号验证方法的组成部分。

Abstract: B\"uchi automata (BAs) recognize $\omega$-regular languages defined by formal
specifications like linear temporal logic (LTL) and are commonly used in the
verification of reactive systems. However, BAs face scalability challenges when
handling and manipulating complex system behaviors. As neural networks are
increasingly used to address these scalability challenges in areas like model
checking, investigating their ability to generalize beyond training data
becomes necessary. This work presents the first study investigating whether
recurrent neural networks (RNNs) can generalize to $\omega$-regular languages
derived from LTL formulas. We train RNNs on ultimately periodic $\omega$-word
sequences to replicate target BA behavior and evaluate how well they generalize
to out-of-distribution sequences. Through experiments on LTL formulas
corresponding to deterministic automata of varying structural complexity, from
3 to over 100 states, we show that RNNs achieve high accuracy on their target
$\omega$-regular languages when evaluated on sequences up to $8 \times$ longer
than training examples, with $92.6\%$ of tasks achieving perfect or
near-perfect generalization. These results establish the feasibility of neural
approaches for learning complex $\omega$-regular languages, suggesting their
potential as components in neurosymbolic verification methods.

</details>


### [257] [MoPEQ: Mixture of Mixed Precision Quantized Experts](https://arxiv.org/abs/2509.02512)
*Krishna Teja Chitty-Venkata,Jie Ye,Murali Emani*

Main category: cs.LG

TL;DR: 这篇论文提出了MoPEQ算法，通过混合精度量化为每个专家分配最优位宽，在保持模型性能的同时大幅减少计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言和视觉模型采用MoE架构带来了巨大的计算和内存挑战，需要一种高效的量化方法来缩减模型规模。

Method: 提出MoPEQ澄后量化算法，使用Hessian迹近似来分析每个专家的敏感性，而不是依赖专家激活频率，为每个专家分配最优位宽。

Result: 在VLMEvalKit测试集上，混合精度量化的MoE模型在保持竞争力准确性的同时，内存占用实现了显著改善，超过了均匀精度基线方法。

Conclusion: 该方法为VLM-MoE模型提供了一种高效的混合精度量化解决方案，在性能和资源消耗之间取得了良好的平衡。

Abstract: Large Language and Vision Models using a Mixture-of-Experts (MoE)
architecture pose significant challenges for deployment due to their
computational and memory demands. Mixed Precision Quantization assigns
different precisions to different layers of an LLM/VLM based on layer
sensitivity and importance within the model. In this work, we propose a Post
Training Quantization algorithm, MoPEQ, that assigns optimal bit width to each
expert. Our method balances accuracy and model size by analyzing each expert's
sensitivity using Hessian trace approximation instead of relying on the
activation frequency of the expert. This per-expert granularity approach
clusters similar experts to maintain model performance while reducing memory
requirements. The experimental results on VLMEvalKit benchmark datasets using
State-of-the-art VLMs Deepseek-VL2 -tiny, -small, -base, and MolmoE models
demonstrate that our mixed precision quantized MoEs achieve competitive
accuracy with substantial improvements in memory footprint compared to
uniform-precision baseline methods. We perform a comprehensive study to analyze
the impact of expert activation frequency and sensitivity using Hessian trace
approximation at both layer-wise and model-wide expert precision allocation of
2, 3, and 4 bits to provide a thorough understanding of mixed precision
quantization of VLM-MoEs.

</details>


### [258] [Surrogate Benchmarks for Model Merging Optimization](https://arxiv.org/abs/2509.02555)
*Rio Akizuki,Yuya Kudo,Nozomu Yoshinari,Yoichi Hirose,Toshiyuki Nishimoto,Kento Uchida,Shinichi Shirakawa*

Main category: cs.LG

TL;DR: 本文开发了模型融合超参数优化的代理基准，通过构建代理模型来预测融合模型性能，从而低成本地实现算法开发和性能比较。


<details>
  <summary>Details</summary>
Motivation: 模型融合技术通常有超参数，其设置会影响融合模型的性能。现有研究表明调优超参数可以提升融合效果，但优化过程计算成本高昂，特别是在融合大语言模型时。

Method: 定义两个搜索空间并收集数据样本，构建代理模型来根据超参数预测融合模型的性能。

Result: 实验证明所开发的基准能够很好地预测融合模型性能，并有效模拟优化算法行为。

Conclusion: 提出的代理基准为模型融合超参数优化提供了一种低成本的解决方案，有助于算法开发和性能比较。

Abstract: Model merging techniques aim to integrate the abilities of multiple models
into a single model. Most model merging techniques have hyperparameters, and
their setting affects the performance of the merged model. Because several
existing works show that tuning hyperparameters in model merging can enhance
the merging outcome, developing hyperparameter optimization algorithms for
model merging is a promising direction. However, its optimization process is
computationally expensive, particularly in merging LLMs. In this work, we
develop surrogate benchmarks for optimization of the merging hyperparameters to
realize algorithm development and performance comparison at low cost. We define
two search spaces and collect data samples to construct surrogate models to
predict the performance of a merged model from a hyperparameter. We demonstrate
that our benchmarks can predict the performance of merged models well and
simulate optimization algorithm behaviors.

</details>


### [259] [DynaGuard: A Dynamic Guardrail Model With User-Defined Policies](https://arxiv.org/abs/2509.02563)
*Monte Hoover,Vatsal Baherwani,Neel Jain,Khalid Saifullah,Joseph Vincent,Chirag Jain,Melissa Kazemi Rad,C. Bayan Bruss,Ashwinee Panda,Tom Goldstein*

Main category: cs.LG

TL;DR: 动态监护模型支持用户自定义政策，在保持静态害备检测准确性的同时，能够识别自由形式政策违规，检测速度远超前沿推理模型


<details>
  <summary>Details</summary>
Motivation: 标准监护模型如LlamaGuard只能检测预定义的静态害备类别，无法满足不同应用域的用户自定义政策需求

Method: 提出动态监护模型，支持用户自定义政策评估文本，可通过快速检测或链式思维推理方式输出判断理由

Result: 动态监护模型在静态害备类别检测上与标准模型准确性相当，在自由形式政策违规识别上达到前沿推理模型的准确性水平，但耗时更短

Conclusion: 动态监护模型提供了更灵活、高效的内容安全监管方案，能够满足多样化的应用场景需求

Abstract: Guardian models are used to supervise and moderate the outputs of user-facing
chatbots, enforcing guardrails and detecting bad behaviors. Standard guardian
models like LlamaGuard detect predefined, static categories of harms. We
propose dynamic guardian models that evaluate text based on user-defined
policies, making them useful for different application domains that are not
addressed by standard guardian models. Our dynamic guardian models can be used
for fast detection of policy violations or with chain-of-thought reasoning that
articulates and justifies the model outputs. Our dynamic guardian models match
static models in detection accuracy for static harm categories while
identifying violations of free-form policies with accuracy comparable to
frontier reasoning models in a fraction of the time.

</details>


### [260] [Understanding sparse autoencoder scaling in the presence of feature manifolds](https://arxiv.org/abs/2509.02565)
*Eric J. Michaud,Liv Gorton,Tom McGrath*

Main category: cs.LG

TL;DR: 本文研究了稀疏自编码器(SAE)的缩放规律，特别关注多维特征流形对SAE学习能力的影响，发现存在一个病态机制导致SAE学习的特征数量远少于潜在变量数量。


<details>
  <summary>Details</summary>
Motivation: 理解稀疏自编码器在神经网络激活建模中的缩放行为，特别是多维特征流形如何影响SAE的特征学习能力。

Method: 采用来自神经缩放文献的容量分配模型(Brill, 2024)，将其适配到SAE缩放分析中，研究特征流形对缩放行为的影响。

Result: 模型重现了不同的缩放机制，发现在一个特定机制中，特征流形会导致SAE学习的特征数量显著少于潜在变量数量，存在病态效应。

Conclusion: 初步讨论了真实环境中SAE是否处于这种病态机制，为理解SAE的特征学习能力提供了重要见解。

Abstract: Sparse autoencoders (SAEs) model the activations of a neural network as
linear combinations of sparsely occurring directions of variation (latents).
The ability of SAEs to reconstruct activations follows scaling laws w.r.t. the
number of latents. In this work, we adapt a capacity-allocation model from the
neural scaling literature (Brill, 2024) to understand SAE scaling, and in
particular, to understand how "feature manifolds" (multi-dimensional features)
influence scaling behavior. Consistent with prior work, the model recovers
distinct scaling regimes. Notably, in one regime, feature manifolds have the
pathological effect of causing SAEs to learn far fewer features in data than
there are latents in the SAE. We provide some preliminary discussion on whether
or not SAEs are in this pathological regime in the wild.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [261] [Simulation-based inference of yeast centromeres](https://arxiv.org/abs/2509.00200)
*Eloïse Touron,Pedro L. C. Rodrigues,Julyan Arbel,Nelle Varoquaux,Michael Arbel*

Main category: stat.ML

TL;DR: 基于Hi-C数据和模拟接触图的新方法，通过随机方式推断泥浆醉母酵母的所有赛缆位置


<details>
  <summary>Details</summary>
Motivation: 直格缆位置在大多数酶母中仍未知，现有方法依赖于良好的预定位，需要更可靠的推断方法

Method: 结合实验Hi-C地图和模拟接触图，采用随机方式推断所有赛缆位置

Result: 提出了一种新的赛缆位置推断方法

Conclusion: 该方法能够更可靠地确定泥浆醉母醉母的赛缆位置，为某某某某某某提供重要工具

Abstract: The chromatin folding and the spatial arrangement of chromosomes in the cell
play a crucial role in DNA replication and genes expression. An improper
chromatin folding could lead to malfunctions and, over time, diseases. For
eukaryotes, centromeres are essential for proper chromosome segregation and
folding. Despite extensive research using de novo sequencing of genomes and
annotation analysis, centromere locations in yeasts remain difficult to infer
and are still unknown in most species. Recently, genome-wide chromosome
conformation capture coupled with next-generation sequencing (Hi-C) has become
one of the leading methods to investigate chromosome structures. Some recent
studies have used Hi-C data to give a point estimate of each centromere, but
those approaches highly rely on a good pre-localization. Here, we present a
novel approach that infers in a stochastic manner the locations of all
centromeres in budding yeast based on both the experimental Hi-C map and
simulated contact maps.

</details>


### [262] [Assessing One-Dimensional Cluster Stability by Extreme-Point Trimming](https://arxiv.org/abs/2509.00258)
*Erwan Dereure,Emmanuel Akame Mfoumou,David Holcman*

Main category: stat.ML

TL;DR: 提出基于直径收缩比的概率方法，通过追踪极端点修剪时数据范围收缩来评估一维样本的尾部行为和几何稳定性，构建了比似然比检验更优的分类决策规则。


<details>
  <summary>Details</summary>
Motivation: 需要一种稳健的方法来评估一维样本的尾部行为和分布稳定性，特别是在小样本或噪声环境下，传统方法如似然比检验效果有限。

Method: 开发直径收缩比指标量化极端点移除时的数据范围相对缩减，推导均匀分布和高斯分布下的理论收缩曲线，构建基于收缩曲线相似性的决策规则，并集成到聚类流程中。

Result: 该方法在小样本或噪声环境下比经典似然比检验具有更高的分类准确率，同时保持大样本渐近一致性，能够有效验证一维聚类而无需密度估计或参数调优。

Conclusion: 提供了理论和实用工具，用于稳健的分布推断和聚类稳定性分析，在尾部行为评估和聚类验证方面表现出色。

Abstract: We develop a probabilistic method for assessing the tail behavior and
geometric stability of one-dimensional n i.i.d. samples by tracking how their
span contracts when the most extreme points are trimmed. Central to our
approach is the diameter-shrinkage ratio, that quantifies the relative
reduction in data range as extreme points are successively removed. We derive
analytical expressions, including finite-sample corrections, for the expected
shrinkage under both the uniform and Gaussian hypotheses, and establish that
these curves remain distinct even for moderate number of removal. We construct
an elementary decision rule that assigns a sample to whichever theoretical
shrinkage profile it most closely follows. This test achieves higher
classification accuracy than the classical likelihood-ratio test in
small-sample or noisy regimes, while preserving asymptotic consistency for
large n. We further integrate our criterion into a clustering pipeline (e.g.
DBSCAN), demonstrating its ability to validate one-dimensional clusters without
any density estimation or parameter tuning. This work thus provides both
theoretical insight and practical tools for robust distributional inference and
cluster stability analysis.

</details>


### [263] [Probit Monotone BART](https://arxiv.org/abs/2509.00263)
*Jared D. Fisher*

Main category: stat.ML

TL;DR: 提出了probit单调BART方法，将单调BART框架扩展到二元结果变量的条件均值函数估计


<details>
  <summary>Details</summary>
Motivation: BART已被证明是非参数建模和预测的强大工具，单调BART能够更精确地估计单调函数。需要进一步扩展该框架以处理二元结果变量的情况

Method: 开发了probit单调BART方法，在单调BART框架基础上，通过probit连接函数来处理二元分类问题

Result: 该方法能够有效估计二元结果变量情况下的条件均值函数，保持了单调BART的精确性优势

Conclusion: probit单调BART成功扩展了单调BART的应用范围，为二元分类问题提供了有效的非参数建模工具

Abstract: Bayesian Additive Regression Trees (BART) of Chipman et al. (2010) has proven
to be a powerful tool for nonparametric modeling and prediction. Monotone BART
(Chipman et al., 2022) is a recent development that allows BART to be more
precise in estimating monotonic functions. We further these developments by
proposing probit monotone BART, which allows the monotone BART framework to
estimate conditional mean functions when the outcome variable is binary.

</details>


### [264] [The Nondecreasing Rank](https://arxiv.org/abs/2509.00265)
*Andrew McCormack*

Main category: stat.ML

TL;DR: 本文引入了矩阵和张量的非递减(ND)秩概念，研究了单调性约束下的张量分解理论，开发了ND秩的性质分析，并提出了求解低ND秩近似的算法，应用于猪体重和疫情期间心理健康调查数据。


<details>
  <summary>Details</summary>
Motivation: 传统张量分解方法缺乏对单调性约束的考虑，而许多实际数据集具有内在的单调特性。需要开发一种能够保持数据单调结构的分解方法，以更好地理解和解释具有排序关系的数据。

Method: 提出了非递减(ND)秩的概念，开发了ND秩的理论性质分析，包括典型值、最大值和边界ND秩。引入了分层交替最小二乘算法的变体来寻找低ND秩近似。

Result: 建立了ND秩的理论框架，证明了在某些偏序集排序下，寻找秩为r的ND分解等价于寻找变换后张量的非负秩-r分解。但并非所有单调张量都具有有限ND秩。成功将方法应用于两个实际数据集。

Conclusion: ND秩为分析具有单调结构的数据提供了新的理论工具和计算方法，在保持数据内在排序关系的同时实现有效的张量分解，对于具有自然排序的实际应用具有重要意义。

Abstract: In this article the notion of the nondecreasing (ND) rank of a matrix or
tensor is introduced. A tensor has an ND rank of r if it can be represented as
a sum of r outer products of vectors, with each vector satisfying a
monotonicity constraint. It is shown that for certain poset orderings finding
an ND factorization of rank $r$ is equivalent to finding a nonnegative rank-r
factorization of a transformed tensor. However, not every tensor that is
monotonic has a finite ND rank. Theory is developed describing the properties
of the ND rank, including typical, maximum, and border ND ranks. Highlighted
also are the special settings where a matrix or tensor has an ND rank of one or
two. As a means of finding low ND rank approximations to a data tensor we
introduce a variant of the hierarchical alternating least squares algorithm.
Low ND rank factorizations are found and interpreted for two datasets
concerning the weight of pigs and a mental health survey during the COVID-19
pandemic.

</details>


### [265] [Partial Functional Dynamic Backdoor Diffusion-based Causal Model](https://arxiv.org/abs/2509.00472)
*Xinwen Liu,Lei Qian,Song Xi Chen,Niansheng Tang*

Main category: stat.ML

TL;DR: 提出了PFD-BDCM模型，用于处理存在空间异质性和时间依赖性的未测量混杂因素时的因果推断问题，通过整合扩散采样机制和功能数据分析来提升反事实估计精度


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理具有空间异质性和时间依赖性的未测量混杂因素时存在限制，需要开发能够同时处理复杂时空动态和多分辨率变量的因果推断框架

Method: 整合有效的后门调整集到基于扩散的采样机制中，通过区域特定结构方程和条件自回归过程处理未测量混杂因素的复杂动态，使用功能数据的基展开适应异质分辨率变量

Result: 理论分析建立了反事实估计的误差界限，实证评估在合成数据集和真实空气污染数据上证明了PFD-BDCM优于现有方法

Conclusion: PFD-BDCM框架成功解决了存在未测量混杂因素时的因果推断挑战，为处理时空异质性数据提供了有效的解决方案

Abstract: We introduce a Partial Functional Dynamic Backdoor Diffusion-based Causal
Model (PFD-BDCM), specifically designed for causal inference in the presence of
unmeasured confounders with spatial heterogeneity and temporal dependency. The
proposed PFD-BDCM framework addresses the restrictions of the existing
approaches by uniquely integrating models for complex spatio-temporal dynamics
with the analysis of multi-resolution variables. Specifically, the framework
systematically mitigates confounding bias by integrating valid backdoor
adjustment sets into a diffusion-based sampling mechanism. Moreover, it
accounts for the intricate dynamics of unmeasured confounders through the
deployment of region-specific structural equations and conditional
autoregressive processes, and accommodates variables observed at heterogeneous
resolutions via basis expansions for functional data. Our theoretical analysis
establishes error bounds for counterfactual estimates of PFD-BDCM, formally
linking reconstruction accuracy to counterfactual fidelity under monotonicity
assumptions of structural equation and invertibility assumptions of encoding
function. Empirical evaluations on synthetic datasets and real-world air
pollution data demonstrate PFD-BDCM's superiority over existing methods.

</details>


### [266] [Identifying Causal Direction via Dense Functional Classes](https://arxiv.org/abs/2509.00538)
*Katerina Hlavackova-Schindler,Suzana Marsela*

Main category: stat.ML

TL;DR: 提出基于最小描述长度(MDL)原理的双变量因果评分方法LCUBE，使用具有紧实区间密度性质的函数来区分因果方向，在特定条件下可证明可识别性。


<details>
  <summary>Details</summary>
Motivation: 解决在没有隐藏混杂因素假设下，确定两个单变量连续值变量X和Y之间因果方向的问题。传统方法需要假设因果模型，本文旨在提供更通用的因果推断方法。

Method: 基于MDL原理构建因果评分，使用在紧实区间上具有密度性质的函数（如三次样条）。提出LCUBE方法，利用三次回归样条实现MDL因果评分，只有一个超参数。

Result: 在真实世界的Tuebingen因果对数据集上，LCUBE在AUDRC指标上达到最优精度。在10个基准数据集上显示优越的平均精度，在13个数据集上达到高于平均水平的精度。

Conclusion: LCUBE是一种可识别、可解释、简单且快速的方法，不需要假设噪声的高斯性，只需噪声较低，在多个数据集上表现出优越的因果推断性能。

Abstract: We address the problem of determining the causal direction between two
univariate, continuous-valued variables, X and Y, under the assumption of no
hidden confounders. In general, it is not possible to make definitive
statements about causality without some assumptions on the underlying model. To
distinguish between cause and effect, we propose a bivariate causal score based
on the Minimum Description Length (MDL) principle, using functions that possess
the density property on a compact real interval. We prove the identifiability
of these causal scores under specific conditions. These conditions can be
easily tested. Gaussianity of the noise in the causal model equations is not
assumed, only that the noise is low. The well-studied class of cubic splines
possesses the density property on a compact real interval. We propose LCUBE as
an instantiation of the MDL-based causal score utilizing cubic regression
splines. LCUBE is an identifiable method that is also interpretable, simple,
and very fast. It has only one hyperparameter. Empirical evaluations compared
to state-of-the-art methods demonstrate that LCUBE achieves superior precision
in terms of AUDRC on the real-world Tuebingen cause-effect pairs dataset. It
also shows superior average precision across common 10 benchmark datasets and
achieves above average precision on 13 datasets.

</details>


### [267] [Beyond Universal Approximation Theorems: Algorithmic Uniform Approximation by Neural Networks Trained with Noisy Data](https://arxiv.org/abs/2509.00924)
*Anastasis Kratsios,Tin Sum Cheng,Daniel Roy*

Main category: stat.ML

TL;DR: 本文提出了一种架构特定的随机训练算法，在噪声数据上构建统一逼近器，实现了最小最优的可训练参数数量，并复制了真实神经网络的三个关键行为特征。


<details>
  <summary>Details</summary>
Motivation: 现有的通用逼近定理(UATs)在理论真空中运行，假设无噪声数据且网络参数可自由选择，这与实际机器学习中处理噪声观测和算法实现的现实需求存在差距。

Method: 引入架构特定的随机训练算法，从N个噪声训练样本中在d维立方体上构建统一逼近器，获得最小最优的可训练参数数量。

Result: 训练后的神经网络达到了最小最优的可训练参数数量（除对数因子外），并展现出真实神经网络的三个关键特征：在相关分布外任务上具有次线性参数复杂性、精确插值训练数据、保持合理的Lipschitz正则性。

Conclusion: 该研究将最先进的UATs更接近实际机器学习，将核心开放问题从噪声样本下的算法可实现性转向随机梯度下降是否能达到类似保证。

Abstract: At its core, machine learning seeks to train models that reliably generalize
beyond noisy observations; however, the theoretical vacuum in which
state-of-the-art universal approximation theorems (UATs) operate isolates them
from this goal, as they assume noiseless data and allow network parameters to
be chosen freely, independent of algorithmic realism. This paper bridges that
gap by introducing an architecture-specific randomized training algorithm that
constructs a uniform approximator from $N$ noisy training samples on the
$d$-dimensional cube $[0,1]^d$. Our trained neural networks attain the
minimax-optimal quantity of \textit{trainable} (non-random) parameters, subject
to logarithmic factors which vanish under the idealized noiseless sampling
assumed in classical UATs.
  Additionally, our trained models replicate key behaviours of real-world
neural networks, absent in standard UAT constructions, by: (1) exhibiting
sub-linear parametric complexity when fine-tuning on structurally related and
favourable out-of-distribution tasks, (2) exactly interpolating the training
data, and (3) maintaining reasonable Lipschitz regularity (after the initial
clustering attention layer). These properties bring state-of-the-art UATs
closer to practical machine learning, shifting the central open question from
algorithmic implementability with noisy samples to whether stochastic gradient
descent can achieve comparable guarantees.

</details>


### [268] [Semi-Supervised Bayesian GANs with Log-Signatures for Uncertainty-Aware Credit Card Fraud Detection](https://arxiv.org/abs/2509.00931)
*David Hirnschall*

Main category: stat.ML

TL;DR: 提出了一种基于条件GAN的半监督深度学习框架，用于信用卡欺诈检测的时间序列分类任务，通过数据增强、贝叶斯推断和log-signatures特征编码来解决不规则采样时间序列的挑战。


<details>
  <summary>Details</summary>
Motivation: 金融交易数据流规模增大且复杂度增加，传统方法需要大量标注数据，难以处理不规则采样频率和变长序列的问题。

Method: 扩展条件GAN进行目标数据增强，集成贝叶斯推断获得预测分布和不确定性量化，利用log-signatures进行交易历史的鲁棒特征编码，并引入基于Wasserstein距离的损失函数。

Result: 在BankSim数据集上评估，在不同标注样本比例下均显示出相对于基准方法的持续改进，在全局统计指标和领域特定指标上都有提升。

Conclusion: 该方法证明了GAN驱动的半监督学习与log-signatures结合在不规则采样时间序列中的有效性，并强调了不确定性感知预测的重要性。

Abstract: We present a novel deep generative semi-supervised framework for credit card
fraud detection, formulated as time series classification task. As financial
transaction data streams grow in scale and complexity, traditional methods
often require large labeled datasets, struggle with time series of irregular
sampling frequencies and varying sequence lengths. To address these challenges,
we extend conditional Generative Adversarial Networks (GANs) for targeted data
augmentation, integrate Bayesian inference to obtain predictive distributions
and quantify uncertainty, and leverage log-signatures for robust feature
encoding of transaction histories. We introduce a novel Wasserstein
distance-based loss to align generated and real unlabeled samples while
simultaneously maximizing classification accuracy on labeled data. Our approach
is evaluated on the BankSim dataset, a widely used simulator for credit card
transaction data, under varying proportions of labeled samples, demonstrating
consistent improvements over benchmarks in both global statistical and
domain-specific metrics. These findings highlight the effectiveness of
GAN-driven semi-supervised learning with log-signatures for irregularly sampled
time series and emphasize the importance of uncertainty-aware predictions.

</details>


### [269] [Hybrid Topic-Semantic Labeling and Graph Embeddings for Unsupervised Legal Document Clustering](https://arxiv.org/abs/2509.00990)
*Deepak Bastola,Woohyeok Choi*

Main category: stat.ML

TL;DR: 提出了一种结合Top2Vec和Node2Vec的混合方法，用于法律文本分类，通过语义主题建模和图嵌入技术提升聚类质量


<details>
  <summary>Details</summary>
Motivation: 法律文件具有领域特定的语言特点且标注数据有限，传统方法在处理法律文本分类时面临挑战

Method: 使用Top2Vec学习语义文档嵌入和发现潜在主题，Node2Vec通过二分图捕获结构关系，结合KMeans进行聚类

Result: Top2Vec+Node2Vec方法在聚类质量上优于纯文本或纯图嵌入方法，性能优于LDA和NMF基线模型

Conclusion: 该方法在无监督法律文档分析方面具有创新性，但效果取决于初始主题生成质量和嵌入模型对法律专业语言的表示能力，需要进一步优化和领域适配

Abstract: Legal documents pose unique challenges for text classification due to their
domain-specific language and often limited labeled data. This paper proposes a
hybrid approach for classifying legal texts by combining unsupervised topic and
graph embeddings with a supervised model. We employ Top2Vec to learn semantic
document embeddings and automatically discover latent topics, and Node2Vec to
capture structural relationships via a bipartite graph of legal documents. The
embeddings are combined and clustered using KMeans, yielding coherent groupings
of documents. Our computations on a legal document dataset demonstrate that the
combined Top2Vec+Node2Vec approach improves clustering quality over text-only
or graph-only embeddings. We conduct a sensitivity analysis of hyperparameters,
such as the number of clusters and the dimensionality of the embeddings, and
demonstrate that our method achieves competitive performance against baseline
Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF)
models. Key findings indicate that while the pipeline presents an innovative
approach to unsupervised legal document analysis by combining semantic topic
modeling with graph embedding techniques, its efficacy is contingent upon the
quality of initial topic generation and the representational power of the
chosen embedding models for specialized legal language. Strategic
recommendations include the exploration of domain-specific embeddings, more
comprehensive hyperparameter tuning for Node2Vec, dynamic determination of
cluster numbers, and robust human-in-the-loop validation processes to enhance
legal relevance and trustworthiness. The pipeline demonstrates potential for
exploratory legal data analysis and as a precursor to supervised learning tasks
but requires further refinement and domain-specific adaptation for practical
legal applications.

</details>


### [270] [Lipschitz-Guided Design of Interpolation Schedules in Generative Models](https://arxiv.org/abs/2509.01629)
*Yifan Chen,Eric Vanden-Eijnden,Jiawei Xu*

Main category: stat.ML

TL;DR: 本文研究了随机插值框架中插值时间表的设计，发现在最优扩散系数调整后，所有标量插值时间表在路径空间的KL散度下具有相同的统计效率，但数值效率差异显著。作者提出了基于平均平方Lipschitz性最小化的数值优化准则，并在高斯分布和高维问题中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然所有插值时间表在统计效率上等价，但数值效率差异很大，这促使研究者关注数值属性而非统计标准来设计时间表，以提高生成模型的数值稳定性和采样效率。

Method: 提出平均平方Lipschitz性最小化作为数值优化准则，替代最优传输中的动能最小化方法；推导了转换公式，可在推理时在不同时间表间转换而无需重新训练神经网络。

Result: 在高斯分布上，优化时间表相比标准线性时间表实现了Lipschitz常数的指数级改进；在高斯混合分布上减少了少步采样中的模式崩溃；在高维随机Allen-Cahn方程和Navier-Stokes方程的稳态分布上验证了方法的鲁棒性能提升。

Conclusion: 数值属性而非统计标准应是插值时间表设计的主要考虑因素，提出的平均平方Lipschitz性最小化准则能显著改善生成模型的数值效率和采样质量。

Abstract: We study the design of interpolation schedules in the stochastic interpolants
framework for flow and diffusion-based generative models. We show that while
all scalar interpolation schedules achieve identical statistical efficiency
under Kullback-Leibler divergence in path space after optimal diffusion
coefficient tuning, their numerical efficiency can differ substantially. This
observation motivates focusing on numerical properties of the resulting drift
fields rather than statistical criteria for schedule design. We propose
averaged squared Lipschitzness minimization as a principled criterion for
numerical optimization, providing an alternative to kinetic energy minimization
used in optimal transport approaches. A transfer formula is derived that
enables conversion between different schedules at inference time without
retraining neural networks. For Gaussian distributions, our optimized schedules
achieve exponential improvements in Lipschitz constants over standard linear
schedules, while for Gaussian mixtures, they reduce mode collapse in few-step
sampling. We also validate our approach on high-dimensional invariant
distributions from stochastic Allen-Cahn equations and Navier-Stokes equations,
demonstrating robust performance improvements across resolutions.

</details>


### [271] [Preconditioned Regularized Wasserstein Proximal Sampling](https://arxiv.org/abs/2509.01685)
*Hong Ye Tan,Stanley Osher,Wuchen Li*

Main category: stat.ML

TL;DR: 提出了一种基于正则化Wasserstein近端算子的预处理无噪声采样方法，通过Cole-Hopf变换和核公式实现，在二次势能下具有离散时间收敛性分析，并在各种贝叶斯应用中表现出加速和稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了解决从Gibbs分布采样的问题，通过演化有限粒子来改进现有的无噪声采样方法，提高采样效率和稳定性。

Method: 使用预处理技术，通过正则化Wasserstein近端算子的分数函数近似，结合Cole-Hopf变换和耦合各向异性热方程，推导出核公式，并将扩散组件解释为改进的自注意力块。

Result: 在二次势能下提供了离散时间非渐近收敛分析，偏差依赖于正则化而与步长无关。实验显示在log-concave和非log-concave问题上都有加速和粒子级稳定性，在贝叶斯图像去卷积和神经网络训练中表现竞争或更好。

Conclusion: 该方法提供了一种有效的预处理采样策略，在保持理论保证的同时，在实际应用中表现出优异的性能和稳定性。

Abstract: We consider sampling from a Gibbs distribution by evolving finitely many
particles. We propose a preconditioned version of a recently proposed
noise-free sampling method, governed by approximating the score function with
the numerically tractable score of a regularized Wasserstein proximal operator.
This is derived by a Cole--Hopf transformation on coupled anisotropic heat
equations, yielding a kernel formulation for the preconditioned regularized
Wasserstein proximal. The diffusion component of the proposed method is also
interpreted as a modified self-attention block, as in transformer
architectures. For quadratic potentials, we provide a discrete-time
non-asymptotic convergence analysis and explicitly characterize the bias, which
is dependent on regularization and independent of step-size. Experiments
demonstrate acceleration and particle-level stability on various log-concave
and non-log-concave toy examples to Bayesian total-variation regularized image
deconvolution, and competitive/better performance on non-convex Bayesian neural
network training when utilizing variable preconditioning matrices.

</details>


### [272] [The Price of Sparsity: Sufficient Conditions for Sparse Recovery using Sparse and Sparsified Measurements](https://arxiv.org/abs/2509.01809)
*Youssef Chaabouni,David Gamarnik*

Main category: stat.ML

TL;DR: 该论文研究了使用稀疏测量矩阵从噪声投影中恢复稀疏信号支持的问题，发现了稀疏恢复的信息论阈值和稀疏性代价，并分析了从稠密矩阵稀疏化对恢复的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然稠密测量矩阵下的稀疏恢复已被广泛研究，但稀疏测量矩阵设置下的研究相对较少。本文旨在建立稀疏测量矩阵下成功稀疏恢复的充分条件，揭示采样复杂度和测量稀疏性之间的精确权衡。

Method: 通过理论分析建立样本量的充分条件，结合已知的必要条件，在ds/p→∞的机制下发现相变现象。同时研究从稠密矩阵稀疏化对稀疏信号恢复的影响。

Result: 发现了稀疏恢复在信息论阈值n_INF^SP = Θ(slog(p/s)/log(ds/p))处存在相变，揭示了稀疏性代价为logs/log(ds/p)。在s=αp和d=ψp机制下，证明n_INF^Sp-ified = Θ(p/ψ²)的样本量足够恢复。

Conclusion: 稀疏测量矩阵下的稀疏恢复存在明确的信息论阈值和相变行为，稀疏性会以可量化的方式增加所需的样本量，为稀疏测量设计提供了理论指导。

Abstract: We consider the problem of recovering the support of a sparse signal using
noisy projections. While extensive work has been done on the dense measurement
matrix setting, the sparse setting remains less explored. In this work, we
establish sufficient conditions on the sample size for successful sparse
recovery using sparse measurement matrices. Bringing together our result with
previously known necessary conditions, we discover that, in the regime where
$ds/p \rightarrow +\infty$, sparse recovery in the sparse setting exhibits a
phase transition at an information-theoretic threshold of
$n_{\text{INF}}^{\text{SP}} =
\Theta\left(s\log\left(p/s\right)/\log\left(ds/p\right)\right)$, where $p$
denotes the signal dimension, $s$ the number of non-zero components of the
signal, and $d$ the expected number of non-zero components per row of
measurement. This expression makes the price of sparsity explicit: restricting
each measurement to $d$ non-zeros inflates the required sample size by a factor
of $\log{s}/\log\left(ds/p\right)$, revealing a precise trade-off between
sampling complexity and measurement sparsity. Additionally, we examine the
effect of sparsifying an originally dense measurement matrix on sparse signal
recovery. We prove in the regime of $s = \alpha p$ and $d = \psi p$ with
$\alpha, \psi \in \left(0,1\right)$ and $\psi$ small that a sample of size
$n^{\text{Sp-ified}}_{\text{INF}} = \Theta\left(p / \psi^2\right)$ is
sufficient for recovery, subject to a certain uniform integrability conjecture,
the proof of which is work in progress.

</details>


### [273] [Design of Experiment for Discovering Directed Mixed Graph](https://arxiv.org/abs/2509.01887)
*Haijie Xu,Chen Zhang*

Main category: stat.ML

TL;DR: 本文研究在包含循环和潜在混杂因素的双向边的因果图结构中，通过实验设计准确识别因果图的问题。提出了基于条件独立性测试和干预测试的算法，并建立了所需实验数量的下界。


<details>
  <summary>Details</summary>
Motivation: 传统方法在存在循环和混杂因素的情况下无法准确识别因果图结构，需要开发新的实验设计方法来处理这些复杂情况。

Method: 结合条件独立性测试和干预测试，考虑d分离和σ分离，开发了有界和无界两类算法来恢复除双向相邻边外的所有因果边。

Result: 建立了单次实验最多可干预变量数和总实验次数的下界，提出的算法在达到对数因子范围内与下界紧致。

Conclusion: 所提出的算法能够有效处理包含循环和混杂因素的因果图识别问题，在理论保证下实现了最优的实验设计效率。

Abstract: We study the problem of experimental design for accurately identifying the
causal graph structure of a simple structural causal model (SCM), where the
underlying graph may include both cycles and bidirected edges induced by latent
confounders. The presence of cycles renders it impossible to recover the graph
skeleton using observational data alone, while confounding can further
invalidate traditional conditional independence (CI) tests in certain
scenarios. To address these challenges, we establish lower bounds on both the
maximum number of variables that can be intervened upon in a single experiment
and the total number of experiments required to identify all directed edges and
non-adjacent bidirected edges. Leveraging both CI tests and do see tests, and
accounting for $d$ separation and $\sigma$ separation, we develop two classes
of algorithms, i.e., bounded and unbounded, that can recover all causal edges
except for double adjacent bidirected edges. We further show that, up to
logarithmic factors, the proposed algorithms are tight with respect to the
derived lower bounds.

</details>


### [274] [Non-Linear Model-Based Sequential Decision-Making in Agriculture](https://arxiv.org/abs/2509.01924)
*Sakshi Arya,Wentao Lin*

Main category: stat.ML

TL;DR: 提出了一种基于非线性模型的bandit算法，将领域特定的响应曲线嵌入到探索-利用循环中，用于农业资源优化决策，在低样本情况下优于线性和非参数基线方法。


<details>
  <summary>Details</summary>
Motivation: 农业管理中的序列决策需要在有限观测下优化资源投入，传统bandit和强化学习方法要么使用线性模型可能错误表示领域知识，要么需要大量数据。

Method: 结合原则性不确定性量化和闭式或快速可计算的利润最优解，将领域特定的非线性响应曲线直接嵌入到探索-利用循环中。

Result: 理论分析证明了次线性遗憾和接近最优的样本复杂度，模拟实验显示在低样本情况下优于线性和非参数基线方法。

Conclusion: 该方法利用机制洞察而非大数据量，特别适合资源受限场景，支持可持续、包容和透明的序列决策，直接贡献于零饥饿和负责任消费生产目标。

Abstract: Sequential decision-making is central to sustainable agricultural management
and precision agriculture, where resource inputs must be optimized under
uncertainty and over time. However, such decisions must often be made with
limited observations, whereas classical bandit and reinforcement learning
approaches typically rely on either linear or black-box reward models that may
misrepresent domain knowledge or require large amounts of data. We propose a
family of nonlinear, model-based bandit algorithms that embed domain-specific
response curves directly into the exploration-exploitation loop. By coupling
(i) principled uncertainty quantification with (ii) closed-form or rapidly
computable profit optima, these algorithms achieve sublinear regret and
near-optimal sample complexity while preserving interpretability. Theoretical
analysis establishes regret and sample complexity bounds, and extensive
simulations emulating real-world fertilizer-rate decisions show consistent
improvements over both linear and nonparametric baselines (such as linear UCB
and $k$-NN UCB) in the low-sample regime, under both well-specified and
shape-compatible misspecified models. Because our approach leverages
mechanistic insight rather than large data volumes, it is especially suited to
resource-constrained settings, supporting sustainable, inclusive, and
transparent sequential decision-making across agriculture, environmental
management, and allied applications. This methodology directly contributes to
SDG 2 (Zero Hunger) and SDG 12 (Responsible Consumption and Production) by
enabling data-driven, less wasteful agricultural practices.

</details>


### [275] [Inference in Spreading Processes with Neural-Network Priors](https://arxiv.org/abs/2509.02073)
*Davide Ghio,Fabrizio Boncoraglio,Lenka Zdeborová*

Main category: stat.ML

TL;DR: 该论文研究了图上的随机过程，通过神经网络先验信息增强初始状态和传播轨迹的恢复，提出了BP-AMP混合算法来处理传播动态和节点协变量信息。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设初始状态在节点间独立同分布，但现实中节点协变量会影响初始状态。本文考虑初始状态是节点协变量的未知函数，使用神经网络建模这种关系。

Method: 采用贝叶斯框架，使用单层感知机建模初始状态与节点协变量的关系。推导了混合置信传播和近似消息传递(BP-AMP)算法，同时处理传播动态和协变量信息。

Result: 在某些机制下，模型在使用Rademacher分布作为神经网络权重时会出现一阶相变，产生统计-计算差距，即使理论上可能完美恢复，BP-AMP算法也无法实现。

Conclusion: 神经网络先验信息能显著提升初始状态和传播轨迹的恢复性能，但算法性能受到相变现象的限制，存在统计与计算之间的差距。

Abstract: Stochastic processes on graphs are a powerful tool for modelling complex
dynamical systems such as epidemics. A recent line of work focused on the
inference problem where one aims to estimate the state of every node at every
time, starting from partial observation of a subset of nodes at a subset of
times. In these works, the initial state of the process was assumed to be
random i.i.d. over nodes. Such an assumption may not be realistic in practice,
where one may have access to a set of covariate variables for every node that
influence the initial state of the system. In this work, we will assume that
the initial state of a node is an unknown function of such covariate variables.
Given that functions can be represented by neural networks, we will study a
model where the initial state is given by a simple neural network -- notably
the single-layer perceptron acting on the known node-wise covariate variables.
  Within a Bayesian framework, we study how such neural-network prior
information enhances the recovery of initial states and spreading trajectories.
We derive a hybrid belief propagation and approximate message passing (BP-AMP)
algorithm that handles both the spreading dynamics and the information included
in the node covariates, and we assess its performance against the estimators
that either use only the spreading information or use only the information from
the covariate variables.
  We show that in some regimes, the model can exhibit first-order phase
transitions when using a Rademacher distribution for the neural-network
weights. These transitions create a statistical-to-computational gap where even
the BP-AMP algorithm, despite the theoretical possibility of perfect recovery,
fails to achieve it.

</details>


### [276] [Amputation-imputation based generation of synthetic tabular data for ratemaking](https://arxiv.org/abs/2509.02171)
*Yevhen Havrylenko,Meelis Käärik,Artur Tuttar*

Main category: stat.ML

TL;DR: 本文探讨了使用MICE方法生成合成数据来解决精算定价中的数据获取问题，并与VAE和CTGAN等方法进行了比较，发现MICE在数据质量和易用性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 精算定价需要高质量数据，但获取新数据成本高且存在隐私问题，因此需要探索合成数据生成作为解决方案。

Method: 引入MICE方法，并与变分自编码器(VAE)和条件表格生成对抗网络(CTGAN)进行比较研究，评估合成数据对变量边际分布和多元关系的保持能力，以及GLM模型的一致性。

Result: MICE方法在生成高质量表格数据方面表现出色，比其他方法更用户友好，合成数据增强能提升GLM在索赔计数预测中的性能。

Conclusion: MICE方法在精算数据生成中具有巨大潜力，既能保持数据质量又易于使用，为解决数据获取限制提供了有效方案。

Abstract: Actuarial ratemaking depends on high-quality data, yet access to such data is
often limited by the cost of obtaining new data, privacy concerns, etc. In this
paper, we explore synthetic-data generation as a potential solution to these
issues. In addition to discussing generative methods previously studied in the
actuarial literature, we introduce to the insurance community another approach
based on Multiple Imputation by Chained Equations (MICE). We present a
comparative study using an open-source dataset and evaluating MICE-based models
against other generative models like Variational Autoencoders and Conditional
Tabular Generative Adversarial Networks. We assess how well synthetic data
preserves the original marginal distributions of variables as well as the
multivariate relationships among covariates. We also investigate the
consistency between Generalized Linear Models (GLMs) trained on synthetic data
with GLMs trained on the original data. Furthermore, we assess the ease of use
of each generative approach and study the impact of augmenting original data
with synthetic data on the performance of GLMs for predicting claim counts. Our
results highlight the potential of MICE-based methods in creating high-quality
tabular data while being more user-friendly than the other methods.

</details>


### [277] [Variational Uncertainty Decomposition for In-Context Learning](https://arxiv.org/abs/2509.02327)
*I. Shavindra Jayasekera,Jacob Si,Wenlong Chen,Filippo Valdettaro,A. Aldo Faisal,Yingzhen Li*

Main category: stat.ML

TL;DR: 提出了一个变分不确定性分解框架，用于分解大语言模型上下文学习中的认知不确定性和偶然不确定性，无需显式采样潜在参数后验分布。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在上下文预测任务中的普及，理解上下文学习中的不确定性来源对确保可靠性至关重要。现有的贝叶斯不确定性分解方法由于潜在参数后验分布难以处理而受到限制。

Method: 通过优化辅助查询作为探针，获得上下文学习过程中偶然不确定性的上界，从而推导出认知不确定性的下界，无需显式采样潜在参数后验分布。

Result: 在合成和真实世界任务上的实验表明，该方法分解得到的不确定性展现出认知不确定性和偶然不确定性的理想特性。

Conclusion: 该框架为上下文学习提供了一种有效的变分不确定性分解方法，有助于更好地理解和评估大语言模型预测的可靠性。

Abstract: As large language models (LLMs) gain popularity in conducting prediction
tasks in-context, understanding the sources of uncertainty in in-context
learning becomes essential to ensuring reliability. The recent hypothesis of
in-context learning performing predictive Bayesian inference opens the avenue
for Bayesian uncertainty estimation, particularly for decomposing uncertainty
into epistemic uncertainty due to lack of in-context data and aleatoric
uncertainty inherent in the in-context prediction task. However, the
decomposition idea remains under-explored due to the intractability of the
latent parameter posterior from the underlying Bayesian model. In this work, we
introduce a variational uncertainty decomposition framework for in-context
learning without explicitly sampling from the latent parameter posterior, by
optimising auxiliary queries as probes to obtain an upper bound to the
aleatoric uncertainty of an LLM's in-context learning procedure, which also
induces a lower bound to the epistemic uncertainty. Through experiments on
synthetic and real-world tasks, we show quantitatively and qualitatively that
the decomposed uncertainties obtained from our method exhibit desirable
properties of epistemic and aleatoric uncertainty.

</details>


### [278] [Distribution estimation via Flow Matching with Lipschitz guarantees](https://arxiv.org/abs/2509.02337)
*Lea Kunkel*

Main category: stat.ML

TL;DR: 本文改进了Flow Matching方法在无界分布下的理论收敛率，特别是在高维设置中，不需要log-凹性假设


<details>
  <summary>Details</summary>
Motivation: Flow Matching作为生成建模的新方法，虽然实证成功但理论理解有限，特别是对驱动ODE的向量场Lipschitz常数的敏感性缺乏理论控制

Method: 通过研究控制向量场Lipschitz常数依赖性的假设，推导Wasserstein 1距离的收敛率

Result: 在高维设置中改进了先前结果，适用于某些无界分布类别，特别不需要log-凹性假设

Conclusion: 为Flow Matching方法提供了更强的理论保证，扩展了其在高维无界分布场景下的适用性

Abstract: Flow Matching, a promising approach in generative modeling, has recently
gained popularity. Relying on ordinary differential equations, it offers a
simple and flexible alternative to diffusion models, which are currently the
state-of-the-art. Despite its empirical success, the mathematical understanding
of its statistical power so far is very limited. This is largely due to the
sensitivity of theoretical bounds to the Lipschitz constant of the vector field
which drives the ODE. In this work, we study the assumptions that lead to
controlling this dependency. Based on these results, we derive a convergence
rate for the Wasserstein $1$ distance between the estimated distribution and
the target distribution which improves previous results in high dimensional
setting. This rate applies to certain classes of unbounded distributions and
particularly does not require $\log$-concavity.

</details>


### [279] [Wild Refitting for Model-Free Excess Risk Evaluation of Opaque ML/AI Models under Bregman Loss](https://arxiv.org/abs/2509.02476)
*Haichen Hu,David Simchi-Levi*

Main category: stat.ML

TL;DR: 提出一种基于wild refitting的模型无关方法，通过随机对称化和人工修改结果来高效评估Bregman损失下惩罚经验风险最小化的超额风险上界


<details>
  <summary>Details</summary>
Motivation: 传统分析方法依赖函数类的全局结构，难以应用于现代复杂ML/AI模型（如深度神经网络、大语言模型），需要一种模型无关的评估框架

Method: 利用wild refitting程序，通过随机向量值对称化、适当缩放预测残差和构建人工修改结果，在单一数据集上重新训练第二个预测器进行超额风险评估

Result: 在固定设计和随机设计设置下建立了高概率性能保证，证明适当选择wild噪声尺度的wild refitting能有效提供超额风险上界

Conclusion: 该方法为理论评估现代不透明ML/AI模型提供了有前景的框架，适用于传统学习理论和经验过程技术无法处理的复杂模型类

Abstract: We study the problem of evaluating the excess risk of classical penalized
empirical risk minimization (ERM) with Bregman losses. We show that by
leveraging the recently proposed wild refitting procedure (Wainwright, 2025),
one can efficiently upper bound the excess risk through the so-called "wild
optimism," without relying on the global structure of the underlying function
class. This property makes our approach inherently model-free. Unlike
conventional analyses, our framework operates with just one dataset and
black-box access to the training procedure. The method involves randomized
vector-valued symmetrization with an appropriate scaling of the prediction
residues and constructing artificially modified outcomes, upon which we retrain
a second predictor for excess risk estimation. We establish high-probability
performance guarantees both under the fixed design setting and the random
design setting, demonstrating that wild refitting under Bregman losses, with an
appropriately chosen wild noise scale, yields a valid upper bound on the excess
risk. This work thus is promising for theoretically evaluating modern opaque ML
and AI models such as deep neural networks and large language models, where the
model class is too complex for classical learning theory and empirical process
techniques to apply.

</details>


### [280] [Probabilities of Causation and Root Cause Analysis with Quasi-Markovian Models](https://arxiv.org/abs/2509.02535)
*Eduardo Rocha Laurentino,Fabio Gagliardi Cozman,Denis Deratani Maua,Daniel Angelo Esteves Lawand,Davi Goncalves Bezerra Coelho,Lucas Martins Marques*

Main category: stat.ML

TL;DR: 本文提出了计算因果概率更紧边界的算法简化方法，显著降低了计算复杂度，并开发了新的根因分析框架，利用因果指标对完整因果路径进行排序


<details>
  <summary>Details</summary>
Motivation: 因果概率提供了评估因果关系的原则性方法，但面临部分可识别性和潜在混杂因素带来的计算挑战

Method: 引入算法简化来降低计算因果概率更紧边界的复杂度，并提出新的根因分析方法论框架，系统性地使用因果指标对完整因果路径进行排序

Result: 显著减少了计算复杂性，能够为因果概率计算更紧的边界

Conclusion: 该方法通过算法简化解决了因果概率计算中的计算挑战，并通过新的根因分析框架提供了更系统的因果路径评估方法

Abstract: Probabilities of causation provide principled ways to assess causal
relationships but face computational challenges due to partial identifiability
and latent confounding. This paper introduces both algorithmic simplifications,
significantly reducing the computational complexity of calculating tighter
bounds for these probabilities, and a novel methodological framework for Root
Cause Analysis that systematically employs these causal metrics to rank entire
causal paths.

</details>
