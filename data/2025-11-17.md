<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 14]
- [cs.LG](#cs.LG) [Total: 77]
- [stat.ML](#stat.ML) [Total: 4]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Detection in Bistatic ISAC with Deterministic Sensing and Gaussian Information Signals](https://arxiv.org/abs/2511.10897)
*Xianxin Song,Xianghao Yu,Jie Xu,Derrick Wing Kwan Ng*

Main category: eess.SP

TL;DR: 该论文研究了双基地ISAC系统中的目标检测，提出了一种基于Neyman-Pearson的检测器，有效利用确定性感知信号和随机通信信号，并通过优化波束成形来最大化检测概率。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信是6G网络的关键技术，需要在同一系统中同时实现通信和感知功能，但现有方法往往将通信信号视为干扰，未能充分利用两种信号的协同作用。

Method: 开发了基于Neyman-Pearson的检测器，利用确定性感知和随机通信信号；采用半定松弛和逐次凸逼近技术优化BS发射波束成形，在满足通信用户SINR约束和总发射功率限制下最大化检测概率。

Result: 仿真结果表明，所提出的NP检测器优于将信息信号视为干扰的基准方案；更高的通信速率阈值会导致更多功率分配给高斯信息承载信号，从而降低确定性信号功率和检测性能。

Conclusion: 该研究证明了同时利用确定性感知和随机通信信号可以显著提高ISAC系统的目标检测性能，为6G网络中集成感知与通信的设计提供了重要见解。

Abstract: Integrated sensing and communications (ISAC) is a disruptive technology enabling future sixth-generation (6G) networks. This paper investigates target detection in a bistatic ISAC system, in which the base station (BS) transmits superimposed ISAC signals comprising both Gaussian information-bearing and deterministic sensing components to simultaneously provide communication and sensing functionalities. First, we develop a Neyman-Pearson (NP)-based detector that effectively utilizes both the deterministic sensing and random communication signals. Closed-form analysis reveals that both signal components contribute to improving the overall detection performance. Subsequently, we optimize the BS transmit beamforming to maximize the detection probability, subject to a minimum signal-to-interference-plus-noise ratio (SINR) constraint for the communication user (CU) and a total transmit power budget at the BS. The resulting non-convex beamforming optimization problem is addressed via semi-definite relaxation (SDR) and successive convex approximation (SCA) techniques. Simulation results demonstrate the superiority of the proposed NP-based detector, which leverages both types of signals, over benchmark schemes that treat information signals as interference. They also reveal that a higher communication-rate threshold directs more transmit power to Gaussian information-bearing signals, thereby diminishing deterministic-signal power and weakening detection performance.

</details>


### [2] [Intelligent Reflecting Surfaces for Integrated Sensing and Communications: A Survey](https://arxiv.org/abs/2511.10990)
*Qingqing Wu,Qiaoyan Peng,Ziheng Zhang,Xiaodan Shao,Yang Liu,Yifan Jiang,Yapeng Zhao,Yanze Zhu,Yilong Chen,Zixiang Ren,Jie Xu,Wen Chen,Rui Zhang*

Main category: eess.SP

TL;DR: IRS-aided ISAC技术综述：利用智能反射面解决6G网络中感知与通信融合的挑战，提供统一设计框架


<details>
  <summary>Details</summary>
Motivation: 6G无线网络需要无缝集成通信和感知，但面临LoS阻塞、空间分辨率有限、覆盖不对称和资源耦合等挑战，IRS提供低成本、节能的电磁重构解决方案

Method: 全面综述IRS辅助无线感知和ISAC技术，包括IRS架构、目标检测与估计技术、波束成形设计和性能指标

Result: 探索IRS在ISAC系统中实现更高效性能平衡、共存和网络化的新机遇，分析当前设计瓶颈

Conclusion: 为下一代无线网络提供实用且可扩展的IRS辅助ISAC系统的统一设计框架和发展方向

Abstract: The rapid development of sixth-generation (6G) wireless networks requires seamless integration of communication and sensing to support ubiquitous intelligence and real-time, high-reliability applications. Integrated sensing and communication (ISAC) has emerged as a key solution for achieving this convergence, offering joint utilization of spectral, hardware, and computing resources. However, realizing high-performance ISAC remains challenging due to environmental line-of-sight (LoS) blockage, limited spatial resolution, and the inherent coverage asymmetry and resource coupling between sensing and communication. Intelligent reflecting surfaces (IRSs), featuring low-cost, energy-efficient, and programmable electromagnetic reconfiguration, provide a promising solution to overcome these limitations. This article presents a comprehensive overview of IRS-aided wireless sensing and ISAC technologies, including IRS architectures, target detection and estimation techniques, beamforming designs, and performance metrics. It further explores IRS-enabled new opportunities for more efficient performance balancing, coexistence, and networking in ISAC systems, focuses on current design bottlenecks, and outlines future research directions. This article aims to offer a unified design framework that guides the development of practical and scalable IRS-aided ISAC systems for the next-generation wireless network.

</details>


### [3] [Blockage-aware Hierarchical Codebook Design for RIS-Assisted Movable Antenna Systems](https://arxiv.org/abs/2511.11193)
*Yan Zhang,Indrakshi Dey,Nicola Marchetti*

Main category: eess.SP

TL;DR: 提出了一种用于毫米波可移动天线系统的阻塞感知分层波束赋形框架，通过集成阻塞检测和两阶段优化方法，提高能效并减少波束训练开销。


<details>
  <summary>Details</summary>
Motivation: 现有可移动天线系统研究往往忽略专用码本设计，未能充分利用MA独特能力，且存在能耗和延迟增加的挑战。

Method: 基于Gerchberg-Saxton算法将阻塞检测集成到码本设计中，并采用两阶段方法简化联合波束赋形和可重构智能表面优化问题的复杂度。

Result: 仿真表明，所提出的自适应码本成功提高了能效，减少了波束训练开销。

Conclusion: 该框架显著提升了RIS辅助MA系统在未来无线网络中的实际部署潜力。

Abstract: In this paper, we propose a novel blockage-aware hierarchical beamforming framework for movable antenna (MA) systems operating at millimeter-wave (mm-Wave) frequencies. While existing works on MA systems have demonstrated performance gains over conventional systems, they often neglect the design of specialized codebooks to leverage MA's unique capabilities and address the challenges of increased energy consumption and latency inherent to MA systems. To address these aspects, we first integrate blockage detection into the codebook design process based on the Gerchberg-Saxton (GS) algorithm, significantly reducing inefficiencies due to beam evaluations done in blocked directions. Then, we use a two-stage approach to reduce the complexity of the joint beamforming and Reconfigurable Intelligent Surfaces (RIS) optimization problem. The simulations demonstrate that the proposed adaptive codebook successfully improves the Energy Efficiency (EE) and reduces the beam training overhead, substantially boosting the practical deployment potential of RIS-assisted MA systems in future wireless networks.

</details>


### [4] [LOKI: a 0.266 pJ/SOP Digital SNN Accelerator with Multi-Cycle Clock-Gated SRAM in 22nm](https://arxiv.org/abs/2511.11205)
*Rick Luiken,Lorenzo Pes,Manil Dev Gomony,Sander Stuijk*

Main category: eess.SP

TL;DR: LOKI是一种用于全连接脉冲神经网络的数字架构，采用多周期时钟门控SRAM，在0.59V电压下运行，时钟频率667MHz，能耗仅0.266 pJ/SOP，在N-MNIST和KWS任务上分别达到98.0%和93.0%的准确率。


<details>
  <summary>Details</summary>
Motivation: 生物启发的传感器与脉冲神经网络结合需要能够在边缘设备上运行的硬件架构，以利用脉冲的稀疏特性实现低功耗处理。

Method: 提出LOKI数字架构，使用多周期时钟门控SRAM技术，在低电压下运行全连接脉冲神经网络。

Result: LOKI在0.59V电压下运行，时钟频率667MHz，能耗0.266 pJ/SOP，在N-MNIST任务上达到98.0%准确率和119.8 nJ/推理能耗，在KWS任务上达到93.0%准确率和546.5 nJ/推理能耗。

Conclusion: LOKI架构成功实现了高效低功耗的脉冲神经网络处理，适用于边缘计算场景。

Abstract: Bio-inspired sensors like Dynamic Vision Sensors (DVS) and silicon cochleas are often combined with Spiking Neural Networks (SNNs), enabling efficient, event-driven processing similar to biological sensory systems. To realize the low-power constraints of the edge, the SNN should run on a hardware architecture that can exploit the sparse nature of the spikes. In this paper, we introduce LOKI, a digital architecture for Fully-Connected (FC) SNNs. By using Multi-Cycle Clock-Gated (MCCG) SRAMs, LOKI can operate at 0.59 V, while running at a clock frequency of 667 MHz. At full throughput, LOKI only consumes 0.266 pJ/SOP. We evaluate LOKI on both the Neuromorphic MNIST (N-MNIST) and the Keyword Spotting k(KWS) tasks, achieving 98.0 % accuracy at 119.8 nJ/inference and 93.0 % accuracy at 546.5 nJ/inference respectively.

</details>


### [5] [3D-HQAM Constellation Design and Performance Evaluation under AWGN](https://arxiv.org/abs/2511.11224)
*Sukhsagar,Nagendra Kumar,Ambuj Kumar Mishra,Vimal Bhatia,Ondrej Krejcar*

Main category: eess.SP

TL;DR: 提出一种构建高阶3D信号星座的方法，将2D六边形QAM扩展到3D空间，通过降维技术降低决策复杂度并推导闭式符号错误概率表达式，显著提升最小欧几里得距离和误码性能。


<details>
  <summary>Details</summary>
Motivation: 增强数字通信系统的可靠性，通过构建3D信号星座来提升系统性能。

Method: 系统地将传统2D六边形QAM星座扩展到3D-HQAM信号空间，形成结构化晶格配置，并引入降维技术处理增加的决策复杂度。

Result: 理论SEP与仿真结果高度匹配，3D星座的最小欧几里得距离相比2D星座提升12.14%至160.81%，显著改善误码性能。

Conclusion: 所提出的3D星座是高质量、可靠的新一代数字通信系统的有前景候选方案。

Abstract: This paper proposes a simple and effective method for constructing higher-order three-dimensional (3D) signal constellations, aiming to enhance the reliability of digital communication systems. The approach systematically extends the conventional two-dimensional hexagonal quadrature amplitude modulation (2D-HQAM) constellation into a 3D-HQAM signal space, forming structured lattice configurations. To address the increased decision complexity resulting from a larger number of constellation points, a dimension reduction (DR) technique is introduced, allowing the derivation of closed-form symbol error probability (SEP) expressions under additive white Gaussian noise (AWGN) conditions. Theoretical SEPs closely match simulation results, validating the accuracy of the proposed method. The minimum Euclidean distance (MED) of the 3D constellations shows a minimum increase of 12.14% over 2D constellation for 8-HQAM, reaching up to 160.81% for 1024-HQAM constellations. This significant improvement in MED leads to enhanced error performance. Therefore, the proposed 3D constellations are promising candidates for high-quality and reliable next-generation digital communication systems.

</details>


### [6] [Testbed Evaluation of AI-based Precoding in Distributed MIMO Systems](https://arxiv.org/abs/2511.11251)
*Tianzheng Miao,Thomas Feys,Gilles Callebaut,Jarne Van Mulders,Md Arifur Rahman,François Rottenberg*

Main category: eess.SP

TL;DR: 该论文提出了一个在分布式MIMO测试平台上实现和验证AI预编码器的框架，通过硬件互易性校准和真实信道数据微调图神经网络模型，在单用户和多用户场景下都取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有分布式MIMO研究大多依赖理想化信道模型且缺乏硬件验证，AI驱动的预编码方法在真实部署中面临数据收集和模型泛化的挑战，需要在实际测试平台上验证AI预编码器的有效性。

Method: 在具有硬件互易性校准的D-MIMO测试平台上，使用从Techtile平台收集的真实信道状态信息对预训练的图神经网络模型进行微调，并在插值和外推场景下评估，最后进行端到端验证。

Result: 多用户场景下微调后性能比预训练模型提升15.7%；单用户场景下模型在未见位置上达到接近最大比传输的性能，吞吐量仅下降不到0.7比特/信道使用（总吞吐量为5.19比特/信道使用）；端到端验证确认了与MRT相当的相干功率聚焦能力。

Conclusion: 该框架成功验证了AI预编码器在真实D-MIMO系统中的有效性，证明了真实测量数据的高效性，为AI驱动的预编码在实际部署中提供了重要参考。

Abstract: Distributed MIMO (D-MIMO) has emerged as a key architecture for future sixth-generation (6G) networks, enabling cooperative transmission across spatially distributed access points (APs). However, most existing studies rely on idealized channel models and lack hardware validation, leaving a gap between algorithmic design and practical deployment. Meanwhile, recent advances in artificial intelligence (AI)-driven precoding have shown strong potential for learning nonlinear channel-to-precoder mappings, but their real-world deployment remains limited due to challenges in data collection and model generalization. This work presents a framework for implementing and validating an AI-based precoder on a D-MIMO testbed with hardware reciprocity calibration. A pre-trained graph neural network (GNN)-based model is fine-tuned using real-world channel state information (CSI) collected from the Techtile platform and evaluated under both interpolation and extrapolation scenarios before end-to-end validation. Experimental results demonstrate a 15.7% performance gain over the pre-trained model in the multi-user case after fine-tuning, while in the single-user scenario the model achieves near-maximum ratio transmission (MRT) performance with less than 0.7 bits/channel use degradation out of a total throughput of 5.19 bits/channel use on unseen positions. Further analysis confirms the data efficiency of real-world measurements, showing consistent gains with increasing training samples, and end-to-end validation verifies coherent power focusing comparable to MRT.

</details>


### [7] [A Novel Partitioning Scheme for RIS Identification and Beamforming](https://arxiv.org/abs/2511.11335)
*Yarkın Gevez,Aymen Khaleel,Ertugrul Basar*

Main category: eess.SP

TL;DR: 提出了一种新的可重构智能表面分区方案，同时考虑RIS识别和波束成形，通过动态分配RIS元素来提升信噪比并保持可靠的识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RIS方案通常单独处理识别和波束成形功能，无法同时优化两种性能指标，需要一种能动态分配RIS资源的方案。

Method: 采用动态分区算法，根据用户需求动态分配RIS元素在识别和波束成形之间的比例，实现资源的高效管理。

Result: 该方案显著提升了信噪比，同时保持了可靠的识别性能，理论分析和计算机仿真验证了方案的有效性。

Conclusion: 提出的动态分区方案能够同时优化RIS的识别和波束成形功能，为RIS资源管理提供了有效的解决方案。

Abstract: This letter introduces a novel partitioning scheme for reconfigurable intelligent surfaces (RISs) that simultaneously consider RIS identification and beamforming. The proposed scheme dynamicly and efficiently allocates RIS elements between identification and beamforming users, considering the different performance metrics associated with each of them. By employing a dynamic partitioning algorithm that efficiently manage the RIS resources (elements), the scheme significantly enhances the signal-to-noise ratio (SNR) while maintaining reliable identification performance. Finally, theoretical analysis and computer simulations are provided to demonstrate the validity of the proposed scheme.

</details>


### [8] [A Geometry Map-Based Propagation Model for Urban Channels](https://arxiv.org/abs/2511.11386)
*Junzhe Song,Ruisi He,Mi Yang,Zhengyu Zhang,Shuaiqi Gao,Bo Ai*

Main category: eess.SP

TL;DR: 提出了一种基于几何地图的传播模型，利用3D几何地图提取关键参数并结合UTD理论计算多重衍射场，在城市环境中准确预测路径损耗和多普勒特性。


<details>
  <summary>Details</summary>
Motivation: 随着5G/6G网络部署，传统统计或经验模型无法充分捕捉密集城市环境中几何特征对无线电传播的影响，需要更精确的建模方法。

Method: 开发了基于3D几何地图的传播模型，采用重要建筑物识别算法检测影响信号传播的关键建筑物，结合UTD理论递归计算多重衍射场。

Result: 模型验证显示与实测数据高度一致，在NLOS复杂衍射场景中优于3GPP和简化模型，RMSE分别降低7.1dB和3.18dB，多普勒分析也证实了其准确性。

Conclusion: 该模型在城市环境中具有出色的可扩展性和泛化能力，能够准确预测大规模路径损耗和时变多普勒特性。

Abstract: With the rapid deployment of 5G and future 6G networks, accurate modeling of urban radio propagation has become critical for system design and network planning. However, conventional statistical or empirical models fail to fully capture the influence of detailed geometric features in dense urban environments. In this paper, we propose a geometry map-based propagation model that directly extracts key parameters from a 3D geometry map and incorporates the Uniform Theory of Diffraction (UTD) to recursively compute multiple diffraction fields, thereby enabling accurate prediction of large-scale path loss and time-varying Doppler characteristics in urban channels. A significant buildings identification algorithm is developed to efficiently detect buildings that significantly affect signal propagation. The proposed model is validated using urban measurement data, showing excellent agreement with path loss in both LOS and NLOS conditions. In particular, for NLOS scenarios with complex diffraction mechanisms, it outperforms the 3GPP and simplified models, reducing the RMSE by 7.1 dB and 3.18 dB, respectively. Doppler analysis further demonstrates its accuracy in capturing time-varying propagation characteristics, confirming the scalability and generalization capability of the model in urban environments.

</details>


### [9] [RadAround: A Field-Expedient Direction Finder for Contested IoT Sensing & EM Situational Awareness](https://arxiv.org/abs/2511.11392)
*Owen A. Maute,Blake A. Roberts,Berker Peköz*

Main category: eess.SP

TL;DR: RadAround是一个用于对抗性物联网传感的被动2D测向系统，使用机械转向窄波束天线和现场可部署的SCADA软件，通过低成本商用或3D打印组件生成高分辨率电磁热图。


<details>
  <summary>Details</summary>
Motivation: 在竞争环境中进行对抗性物联网传感，需要能够在现场操作的弹性系统，用于电磁兼容性测试、战场频谱监测、电子入侵检测和战术电磁态势感知等应用。

Method: 使用机械转向窄波束天线和微控制器可部署的SCADA软件，实时协调天线定位和软件定义无线电采样，采用模块化设计实现快速适应。

Result: 实验显示RadAround能够穿透墙壁检测计算设备、评估利用率，并精确定位法拉第外壳的电磁干扰泄漏源。

Conclusion: RadAround提供了一个低成本、高分辨率的被动2D测向解决方案，适用于各种竞争环境中的电磁传感应用。

Abstract: This paper presents RadAround, a passive 2-D direction-finding system designed for adversarial IoT sensing in contested environments. Using mechanically steered narrow-beam antennas and field-deployable SCADA software, it generates high-resolution electromagnetic (EM) heatmaps using low-cost COTS or 3D-printed components. The microcontroller-deployable SCADA coordinates antenna positioning and SDR sampling in real time for resilient, on-site operation. Its modular design enables rapid adaptation for applications such as EMC testing in disaster-response deployments, battlefield spectrum monitoring, electronic intrusion detection, and tactical EM situational awareness (EMSA). Experiments show RadAround detecting computing machinery through walls, assessing utilization, and pinpointing EM interference (EMI) leakage sources from Faraday enclosures.

</details>


### [10] [Physiological Measures of the Mental Workload in Users of a Lower Limb Exosuit: A Comparison of Subjective and Objective Metrics](https://arxiv.org/abs/2511.11414)
*Giulia Mariani,Chiara Lambranzi,Nicholas Cartocci,Giacinto Barresi,Christian Di Natali,Elena De Momi,Jesus Ortiz*

Main category: eess.SP

TL;DR: 本研究比较了主观和客观生理指标来评估XoSoft下肢软外骨骼的心理工作负荷，发现瞳孔直径可能是与NASA-TLX问卷相关的客观指标，但不同生理指标对刺激的反应存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在调查XoSoft下肢软外骨骼的心理工作负荷评估，比较主观和客观生理指标，以了解外骨骼使用过程中的认知负荷。

Method: 使用NASA-TLX问卷、瞳孔大小平均变化百分比和Baevsky压力指数，在18名健康受试者行走时进行实验，并引入数学任务创建双重任务条件。

Result: 结果显示任务难度、外骨骼激活和瞳孔动态之间存在复杂交互作用，受试者在高心理负荷下可能达到饱和状态；瞳孔直径与NASA-TLX问卷相关，但压力指数显示不一致；发现右眼对认知负荷更敏感的眼部不对称性。

Conclusion: 瞳孔直径可作为心理工作负荷的客观指标，但不同生理指标对刺激的反应不同，需要综合考虑多种指标来评估外骨骼使用时的认知负荷。

Abstract: Lower-limb exosuits are particularly relevant for individuals with some degree of mobility impairment, such as post-stroke patients or older adults with reduced movement capabilities. This study aims to investigate the mental workload (MWL) assessment of XoSoft, a lower-limb soft exoskeleton, using and comparing subjective and objective physiological metrics. The NASA-TLX questionnaire, the average percentage change in pupil size (APCPS), and the Baevsky stress index (SI) are compared. The experiments were conducted on 18 healthy subjects while walking and involved mathematical tasks to create a double-task condition. The results show a complex interaction between task difficulty, exoskeleton activation, and pupillary dynamics, suggesting that the subject might reach a saturated condition under a high mental load. Besides, the data indicate that pupil diameter may be an objective mental workload indicator that correlates with subjective NASA-TLX questionnaires. The discordant indications from the stress index suggest how different metrics of the ocular and cardiac levels respond differently to various stimuli and dynamics. Research has also revealed ocular asymmetry, with the right eye more sensitive to cognitive load.

</details>


### [11] [Analytical Modelling of the Impact of Foliage Cover on the Propagation Loss in a Smart Farming Wireless IOT Application](https://arxiv.org/abs/2511.11449)
*Chibuzor Henry Amadi,Akaniyene Benard Obot,Kufre Monday Udofia,Olaoluwa Ayodeji Adegboye*

Main category: eess.SP

TL;DR: 分析树叶覆盖对智能农业物联网传播损耗影响的模型研究


<details>
  <summary>Details</summary>
Motivation: 研究树叶覆盖对智能农业物联网应用中传播损耗的影响，为优化农业物联网网络性能提供理论支持

Method: 开发了一个分析模型来量化树叶覆盖对传播损耗的影响

Result: 建立了能够预测树叶覆盖条件下传播损耗的数学模型

Conclusion: 该模型有助于改进智能农业物联网系统的网络规划和性能优化

Abstract: This study presents an analytical model of foliage cover impact on propagation loss in smart farming IoT applications.

</details>


### [12] [A Scalable and Exact Relaxation for Densest $k$-Subgraph via Error Bounds](https://arxiv.org/abs/2511.11451)
*Ya Liu,Junbin Liu,Wing-Kin Ma,Aritra Konar*

Main category: eess.SP

TL;DR: 提出了一种基于误差界原理的可扩展精确连续惩罚方法来解决Densest k-Subgraph问题，开发了非凸近端梯度算法，在计算成本和求解质量之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: DkS问题是NP难且难以近似，但基于惩罚的连续松弛方法在实际应用中表现出色，需要开发更有效的精确求解方法。

Method: 使用误差界原理设计合适的惩罚函数，提出非凸近端梯度算法，其中非凸近端算子可以闭式求解，具有低迭代复杂度。

Result: 在DkS问题及其变体Dk1k2BS问题的大规模实例上实验表明，该方法在计算成本和求解质量之间达到了有利平衡。

Conclusion: 提出的惩罚重构方法能够使用一阶连续优化方法，理论保证确保惩罚问题的全局和局部最优解与原始问题匹配，算法具有收敛性分析。

Abstract: Given an undirected graph and a size parameter $k$, the Densest $k$-Subgraph (D$k$S) problem extracts the subgraph on $k$ vertices with the largest number of induced edges. While D$k$S is NP--hard and difficult to approximate, penalty-based continuous relaxations of the problem have recently enjoyed practical success for real-world instances of D$k$S. In this work, we propose a scalable and exact continuous penalization approach for D$k$S using the error bound principle, which enables the design of suitable penalty functions. Notably, we develop new theoretical guarantees ensuring that both the global and local optima of the penalized problem match those of the original problem. The proposed penalized reformulation enables the use of first-order continuous optimization methods. In particular, we develop a non-convex proximal gradient algorithm, where the non-convex proximal operator can be computed in closed form, resulting in low per-iteration complexity. We also provide convergence analysis of the algorithm. Experiments on large-scale instances of the D$k$S problem and one of its variants, the Densest ($k_1, k_2$) Bipartite Subgraph (D$k_1k_2$BS) problem, demonstrate that our method achieves a favorable balance between computation cost and solution quality.

</details>


### [13] [Enabling Wireless Power Transfer (WPT) in Pinching Antenna Systems (PASS)](https://arxiv.org/abs/2511.11465)
*Deqiao Gan,Xiaoxia Xu,Xiaohu Ge,Yue Liu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出了一种新型夹持天线系统(PASS)无线功率传输框架，通过激活接收器附近的夹持天线并灵活调整功率辐射比，同时提升能量收集效率和通信质量。


<details>
  <summary>Details</summary>
Motivation: 解决能量收集接收器(EHRs)和信息解码接收器(IDRs)共存场景下，能量收集效率与通信质量之间的耦合优化问题。

Method: 采用双层优化框架：上层联合优化发射波束成形、PA位置和功率辐射比可行区间以最大化功率转换效率；下层优化功率辐射比以最大化总速率。针对双用户和多用户场景分别开发了AO-WMMSE算法和QT-LDT算法。

Result: 相比传统MIMO和固定功率辐射的基线PASS，所提框架在EHRs的PCE上分别提升81.45%和43.19%，在IDRs的总速率上分别提升77.81%和31.91%。

Conclusion: PASS-WPT框架通过灵活调整夹持天线的功率辐射比，能显著提升无线功率传输系统的能量收集效率和通信性能。

Abstract: A novel pinching antenna system (PASS) enabled wireless power transfer (WPT) framework is proposed, where energy harvesting receivers (EHRs) and information decoding receivers (IDRs) coexist. By activating pinching antennas (PAs) near both receivers and flexibly adjusting PAs' power radiation ratios, both energy harvesting efficiency and communication quality can be enhanced. A bi-level optimization problem is formulated to overcome the strong coupling between optimization variables. The upper level jointly optimizes transmit beamforming, PA positions, and feasible interval of power radiation ratios for power conversion efficiency (PCE) maximization under rate requirements, while the lower level refines power radiation ratio for the sum rate maximization. Efficient solutions are developed for both two-user and multi-user scenarios. 1) For the two-user case, where an EHR and an IDR coexist, the alternating optimization (AO)-based and weighted minimum mean square error (WMMSE)-based algorithms are developed to achieve the stationary solutions of transmit beamforming, PA positions, and power radiation ratios. 2) For the multi-user case, a quadratic transform-Lagrangian dual transform (QT-LDT) algorithm is proposed to iteratively update PCE and sum rate by optimizing PA positions and power radiation ratios individually. Closed-form solutions are derived for both maximization problems. Numerical simulation results demonstrate that the proposed PASS-WPT framework significantly outperforms conventional MIMO and the baseline PASS with fixed power radiation, which demonstrates that: i) Compared to the conventional MIMO and baseline PASS, the proposed PASS-WPT framework achieves 81.45% and 43.19% improvements in PCE of EHRs, and ii) also increases the sum rate by 77.81% and 31.91% for IDRs.

</details>


### [14] [SynthSoM-Twin: A Multi-Modal Sensing-Communication Digital-Twin Dataset for Sim2Real Transfer via Synesthesia of Machines](https://arxiv.org/abs/2511.11503)
*Junlong Chen,Ziwei Huang,Xuesong Cai,Xiang Cheng,Liuqing Yang*

Main category: eess.SP

TL;DR: 构建了SynthSoM-Twin多模态感知-通信数字孪生数据集，通过机器联觉实现Sim2Real迁移，在注入少于15%真实数据的情况下能达到与全真实数据训练相当甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有真实世界多模态感知-通信数据集数量有限和模态缺失的问题，构建时空一致的数字孪生数据集以促进Sim2Real迁移。

Method: 利用多模态感知辅助的目标检测和跟踪算法确保静态和动态物体在真实世界与仿真环境中的时空一致性，使用AirSim、WaveFarer和Sionna RT三个高保真仿真器构建数据集。

Result: 构建了包含66,868个合成快照的数据集，涵盖RGB图像、深度图、LiDAR点云、毫米波雷达点云以及信道衰落数据。实验表明在SynthSoM-Twin数据集上训练的模型具有良好实用性能，真实数据注入进一步提升了Sim2Real迁移能力。

Conclusion: SynthSoM-Twin数据集能有效支持Sim2Real迁移，通过注入少量真实数据即可实现性能与全真实数据训练相当甚至更优的效果，为多模态感知-通信研究提供了高质量的数字孪生基准。

Abstract: This paper constructs a novel multi-modal sensing-communication digital-twin dataset, named SynthSoM-Twin, which is spatio-temporally consistent with the real world, for Sim2Real transfer via Synesthesia of Machines (SoM). To construct the SynthSoM-Twin dataset, we propose a new framework that can extend the quantity and missing modality of existing real-world multi-modal sensing-communication dataset. Specifically, we exploit multi-modal sensing-assisted object detection and tracking algorithms to ensure spatio-temporal consistency of static objects and dynamic objects across real world and simulation environments. The constructed scenario is imported into three high-fidelity simulators, i.e., AirSim, WaveFarer, and Sionna RT. The SynthSoM-Twin dataset contains spatio-temporally consistent data with the real world, including 66,868 snapshots of synthetic RGB images, depth maps, light detection and ranging (LiDAR) point clouds, millimeter wave (mmWave) radar point clouds, and large-scale and small-scale channel fading data. To validate the utility of SynthSoM-Twin dataset, we conduct Sim2Real transfer investigation by implementing two cross-modal downstream tasks via cross-modal generative models (CMGMs), i.e., cross-modal channel generation model and multi-modal sensing-assisted beam generation model. Based on the downstream tasks, we explore the threshold of real-world data injection that can achieve a decent trade-off between real-world data usage and models' practical performance. Experimental results show that the model training on the SynthSoM-Twin dataset achieves a decent practical performance, and the injection of real-world data further facilitates Sim2Real transferability. Based on the SynthSoM-Twin dataset, injecting less than 15% of real-world data can achieve similar and even better performance compared to that trained with all the real-world data only.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Multistability of Self-Attention Dynamics in Transformers](https://arxiv.org/abs/2511.11553)
*Claudio Altafini*

Main category: cs.LG

TL;DR: 自注意力动力学与多智能体Oja流相关，将单头自注意力系统的平衡点分为四类：共识、二分共识、聚类和多边形平衡点，其中前三类通常共存且与值矩阵的特征向量对齐。


<details>
  <summary>Details</summary>
Motivation: 研究自注意力动力学与多智能体Oja流的关系，分析自注意力机制的平衡点分类和稳定性特性。

Method: 将自注意力动力学建模为连续时间多智能体系统，并与多智能体Oja流建立联系，对平衡点进行数学分类和分析。

Result: 识别出四类平衡点：共识、二分共识、聚类和多边形平衡点，前三类通常共存且与值矩阵的特征向量（特别是主特征向量）对齐。

Conclusion: 自注意力动力学具有丰富的平衡点结构，其稳定平衡点与值矩阵的特征向量密切相关，为理解transformer注意力机制提供了理论基础。

Abstract: In machine learning, a self-attention dynamics is a continuous-time multiagent-like model of the attention mechanisms of transformers. In this paper we show that such dynamics is related to a multiagent version of the Oja flow, a dynamical system that computes the principal eigenvector of a matrix corresponding for transformers to the value matrix. We classify the equilibria of the ``single-head'' self-attention system into four classes: consensus, bipartite consensus, clustering and polygonal equilibria. Multiple asymptotically stable equilibria from the first three classes often coexist in the self-attention dynamics. Interestingly, equilibria from the first two classes are always aligned with the eigenvectors of the value matrix, often but not exclusively with the principal eigenvector.

</details>


### [16] [LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices](https://arxiv.org/abs/2511.10680)
*Jean-Philippe Lignier*

Main category: cs.LG

TL;DR: LAD-BNet是一种专为边缘设备优化的实时能源预测神经网络，在Google Coral TPU上实现18ms推理时间，比CPU快8-12倍，在1小时预测范围内达到14.49% MAPE精度。


<details>
  <summary>Details</summary>
Motivation: 解决智能电网优化和智能建筑中边缘设备实时能源预测的挑战，需要高效、低延迟的预测模型。

Method: 提出LAD-BNet（Lag-Aware Dual-Branch Network）混合架构，结合显式时间滞后分支和具有扩张卷积的时间卷积网络（TCN），同时捕捉短期和长期依赖关系。

Result: 在10分钟时间分辨率的真实能耗数据上测试，LAD-BNet比LSTM基线提升2.39%，比纯TCN架构提升3.04%，内存占用仅180MB，适合嵌入式设备。

Conclusion: 该模型为实时能源优化、需求管理和运营规划等工业应用铺平了道路。

Abstract: Real-time energy forecasting on edge devices represents a major challenge for smart grid optimization and intelligent buildings. We present LAD-BNet (Lag-Aware Dual-Branch Network), an innovative neural architecture optimized for edge inference with Google Coral TPU. Our hybrid approach combines a branch dedicated to explicit exploitation of temporal lags with a Temporal Convolutional Network (TCN) featuring dilated convolutions, enabling simultaneous capture of short and long-term dependencies. Tested on real energy consumption data with 10-minute temporal resolution, LAD-BNet achieves 14.49% MAPE at 1-hour horizon with only 18ms inference time on Edge TPU, representing an 8-12 x acceleration compared to CPU. The multi-scale architecture enables predictions up to 12 hours with controlled performance degradation. Our model demonstrates a 2.39% improvement over LSTM baselines and 3.04% over pure TCN architectures, while maintaining a 180MB memory footprint suitable for embedded device constraints. These results pave the way for industrial applications in real-time energy optimization, demand management, and operational planning.

</details>


### [17] [LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups](https://arxiv.org/abs/2511.10683)
*Masih Aminbeidokhti,Subhankar Roy,Eric Granger,Elisa Ricci,Marco Pedersoli*

Main category: cs.LG

TL;DR: 提出LT-Soups框架解决长尾数据集中参数高效微调方法在头尾类之间的性能权衡问题，通过两阶段模型融合方法在多种不平衡场景下实现更好的性能平衡。


<details>
  <summary>Details</summary>
Motivation: 现实数据集通常呈现长尾分布，现有参数高效微调方法虽然能保持尾类性能，但会牺牲头类准确率。头尾类比是影响这种权衡的关键但被忽视的因素。

Method: 提出LT-Soups两阶段框架：第一阶段在平衡子集上微调模型并平均以减少头类偏差；第二阶段仅在完整数据集上微调分类器以恢复头类准确率。

Result: 在六个基准数据集上的实验表明，LT-Soups在广泛的不平衡场景下相比PEFT和传统模型融合方法实现了更优的性能权衡。

Conclusion: LT-Soups能够有效解决长尾数据集中头尾类性能权衡问题，在多种不平衡场景下都表现出色。

Abstract: Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($ρ$) and head-tail ratio ($η$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.

</details>


### [18] [Private Zeroth-Order Optimization with Public Data](https://arxiv.org/abs/2511.10859)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 提出了PAZO框架，利用公共数据指导私有零阶优化算法，在保持隐私的同时显著提升效用和运行效率


<details>
  <summary>Details</summary>
Motivation: 解决一阶差分隐私机器学习算法（如DP-SGD）的高计算和内存成本问题，同时克服现有零阶方法效用较低的问题

Method: 利用公共数据指导私有零阶优化算法的梯度近似，开发了PAZO框架，在公共数据和私有数据相似的假设下进行理论分析

Result: 在视觉和文本任务的预训练和微调场景中，PAZO在隐私/效用权衡方面优于最佳的一阶基线方法，特别是在高隐私保护机制下，同时提供高达16倍的运行速度提升

Conclusion: PAZO框架通过利用公共数据有效提升了私有零阶优化算法的性能，在保持强隐私保护的同时实现了更好的效用和效率

Abstract: One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.

</details>


### [19] [Differentiable Sparse Identification of Lagrangian Dynamics](https://arxiv.org/abs/2511.10706)
*Zitong Zhang,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种可微分的稀疏识别框架，通过三次B样条近似、物理约束集成和递归导数计算，有效识别复杂机械系统中的拉格朗日动力学方程，显著提升了噪声环境下的识别精度。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏回归技术在识别有理函数和处理噪声方面存在困难，而拉格朗日形式主义虽然能避免有理表达式并提供更简洁的系统动力学表示，但现有方法对测量噪声和数据可用性敏感。

Method: 集成三次B样条近似到拉格朗日系统识别中；开发鲁棒的方程发现机制，结合已知物理约束；基于B样条基函数的递归导数计算方案，约束高阶导数并降低二阶动力系统的噪声敏感性。

Result: 该方法在复杂机械系统中表现出优越性能，相比基线方法能够从噪声数据中更准确可靠地提取物理规律。

Conclusion: 所提出的可微分稀疏识别框架有效解决了拉格朗日系统识别中的噪声敏感性和数据限制问题，为复杂非线性动力系统的数据驱动发现提供了更可靠的工具。

Abstract: Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.

</details>


### [20] [Graph Attention Network for Predicting Duration of Large-Scale Power Outages Induced by Natural Disasters](https://arxiv.org/abs/2511.10898)
*Chenghao Duan,Chuanyi Ji*

Main category: cs.LG

TL;DR: 使用图注意力网络(GAT)预测飓风等自然灾害导致的停电持续时间，在501个县的数据上达到93%准确率，比现有方法提升2-15%。


<details>
  <summary>Details</summary>
Motivation: 自然灾害导致的大规模停电造成巨大经济和社会影响，准确预测停电恢复时间对电网韧性至关重要。现有方法面临空间依赖性、空间异质性和事件数据有限三大挑战。

Method: 提出基于图注意力网络(GAT)的新方法，采用无监督预训练和半监督学习相结合的网络结构，利用四个主要飓风影响下八个东南部州501个县的现场数据。

Result: 模型表现出优异性能(>93%准确率)，在整体性能和类别准确率上均优于XGBoost、随机森林、GCN和简单GAT方法2-15%。

Conclusion: 图注意力网络能有效处理空间依赖性和异质性，为自然灾害导致的停电持续时间预测提供了准确可靠的解决方案。

Abstract: Natural disasters such as hurricanes, wildfires, and winter storms have induced large-scale power outages in the U.S., resulting in tremendous economic and societal impacts. Accurately predicting power outage recovery and impact is key to resilience of power grid. Recent advances in machine learning offer viable frameworks for estimating power outage duration from geospatial and weather data. However, three major challenges are inherent to the task in a real world setting: spatial dependency of the data, spatial heterogeneity of the impact, and moderate event data. We propose a novel approach to estimate the duration of severe weather-induced power outages through Graph Attention Networks (GAT). Our network uses a simple structure from unsupervised pre-training, followed by semi-supervised learning. We use field data from four major hurricanes affecting $501$ counties in eight Southeastern U.S. states. The model exhibits an excellent performance ($>93\%$ accuracy) and outperforms the existing methods XGBoost, Random Forest, GCN and simple GAT by $2\% - 15\%$ in both the overall performance and class-wise accuracy.

</details>


### [21] [Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning](https://arxiv.org/abs/2511.10707)
*Sirui Liang,Pengfei Cao,Jian Zhao,Cong Huang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: BREP ReFT通过优化推理前缀生成、早期干预和约束干预向量幅度，显著提升了表示微调在数学推理任务上的性能，超越了标准ReFT和基于权重的PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 表示微调(ReFT)方法虽然在多个任务上优于参数高效微调(PEFT)，但在数学推理任务上表现显著下降，主要原因是难以生成有效的推理前缀和干扰数值编码。

Method: 提出BREP ReFT方法：截断训练数据优化初始推理前缀生成，早期干预防止错误累积，约束干预向量幅度避免干扰数值编码。

Result: 在多种模型架构上的广泛实验表明，BREP在数学推理任务上比标准ReFT和权重PEFT方法更有效、高效，并具有强大的泛化能力。

Conclusion: BREP ReFT成功解决了ReFT在数学推理任务上的性能问题，通过针对性的优化策略实现了更好的推理能力。

Abstract: Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.

</details>


### [22] [A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates](https://arxiv.org/abs/2511.11211)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: 本文为Tsallis-INF多臂老虎机算法提供了一个简化的最佳世界保证推导，避免了共轭函数的使用，采用在线凸优化工具。


<details>
  <summary>Details</summary>
Motivation: 简化Tsallis-INF算法的最佳世界保证证明，避免复杂的数学工具，提供更简洁的推导过程。

Method: 使用现代在线凸优化工具，避免共轭函数，不优化边界常数以保持证明简洁。

Result: 成功推导出Tsallis-INF算法在随机和对抗性老虎机问题中的最佳世界性能保证。

Conclusion: 通过简化证明方法，为Tsallis-INF算法提供了更易理解的最佳世界性能保证推导。

Abstract: In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.

</details>


### [23] [Towards Uncertainty Quantification in Generative Model Learning](https://arxiv.org/abs/2511.10710)
*Giorgio Morales,Frederic Jurie,Jalal Fadili*

Main category: cs.LG

TL;DR: 本文提出生成模型学习中的不确定性量化问题，讨论使用集成精度-召回曲线等研究方向，并在合成数据集上验证了聚合精度-召回曲线在捕捉模型近似不确定性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 生成模型虽然应用广泛，但其可靠性存在根本性担忧。当前评估方法主要关注学习分布与目标分布的接近程度，而忽略了这些测量中的固有不确定性。

Method: 形式化生成模型学习中的不确定性量化问题，提出使用集成精度-召回曲线等方法，在合成数据集上进行初步实验验证。

Result: 在合成数据集上的实验表明，聚合精度-召回曲线能够有效捕捉模型近似不确定性，使基于不确定性特征对不同模型架构进行系统比较成为可能。

Conclusion: 生成模型学习中的不确定性量化是一个重要但研究不足的领域，集成精度-召回曲线等方法为系统评估和比较不同模型架构的不确定性特征提供了有效途径。

Abstract: While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.

</details>


### [24] [Multicalibration yields better matchings](https://arxiv.org/abs/2511.11413)
*Riccardo Colini Baldeschi,Simone Di Gregorio,Simone Fioravanti,Federico Fusco,Ido Guy,Daniel Haimovich,Stefano Leonardi,Fridolin Linder,Lorenzo Perini,Matteo Russo,Niek Tax*

Main category: cs.LG

TL;DR: 该论文提出使用多重校准方法来处理图匹配问题中的预测误差，通过构建特定的多重校准预测器，使得基于该预测器选择的最佳匹配能够与在原始预测器上应用的最佳决策规则相竞争。


<details>
  <summary>Details</summary>
Motivation: 在加权图中寻找最佳匹配时，通常只能访问基于底层上下文对实际随机权重的预测。如果预测器是贝叶斯最优的，那么基于预测权重计算最佳匹配是最优的。但在实践中，完美信息场景不现实，需要处理不完美预测器带来的误差。

Method: 提出使用多重校准作为解决方案，要求预测器在受保护上下文集合的每个元素上都是无偏的。给定匹配算法类C和任何边权重预测器γ，构建特定的多重校准预测器γ̂。

Result: 基于γ̂输出选择最佳匹配能够与在原始预测器γ上应用的最佳决策规则相竞争，并提供了样本复杂度界限。

Conclusion: 多重校准方法能够有效补偿不完美预测器引入的误差，在匹配问题中提供有竞争力的性能保证。

Abstract: Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.
  In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\hat γ$, with the following property. Picking the best matching based on the output of $\hat γ$ is competitive with the best decision rule in $\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.

</details>


### [25] [Movement-Specific Analysis for FIM Score Classification Using Spatio-Temporal Deep Learning](https://arxiv.org/abs/2511.10713)
*Jun Masaki,Ariaki Higashi,Naoko Shinagawa,Kazuhiko Hirata,Yuichi Kurita,Akira Furui*

Main category: cs.LG

TL;DR: 提出了一种自动化FIM评分估计方法，使用不同于标准FIM评估动作的简单练习，通过深度神经网络架构估计FIM运动项目分数。


<details>
  <summary>Details</summary>
Motivation: 传统FIM评估对患者和医疗专业人员造成显著负担，需要开发自动化评估方法。

Method: 采用集成空间-时间图卷积网络(ST-GCN)、双向长短期记忆网络(BiLSTM)和注意力机制的深度神经网络架构，捕捉长期时间依赖关系并识别关键身体关节贡献。

Result: 在277名康复患者研究中，成功区分完全独立患者和需要辅助的患者，在不同FIM项目中达到70.09-78.79%的平衡准确率。

Conclusion: 该方法能够有效自动化FIM评估，并识别出可作为特定FIM评估项目可靠预测因子的特定运动模式。

Abstract: The functional independence measure (FIM) is widely used to evaluate patients' physical independence in activities of daily living. However, traditional FIM assessment imposes a significant burden on both patients and healthcare professionals. To address this challenge, we propose an automated FIM score estimation method that utilizes simple exercises different from the designated FIM assessment actions. Our approach employs a deep neural network architecture integrating a spatial-temporal graph convolutional network (ST-GCN), bidirectional long short-term memory (BiLSTM), and an attention mechanism to estimate FIM motor item scores. The model effectively captures long-term temporal dependencies and identifies key body-joint contributions through learned attention weights. We evaluated our method in a study of 277 rehabilitation patients, focusing on FIM transfer and locomotion items. Our approach successfully distinguishes between completely independent patients and those requiring assistance, achieving balanced accuracies of 70.09-78.79 % across different FIM items. Additionally, our analysis reveals specific movement patterns that serve as reliable predictors for particular FIM evaluation items.

</details>


### [26] [Fast Neural Tangent Kernel Alignment, Norm and Effective Rank via Trace Estimation](https://arxiv.org/abs/2511.10796)
*James Hazelden*

Main category: cs.LG

TL;DR: 提出了一种基于迹估计的矩阵自由方法，用于快速计算神经正切核(NTK)的迹、Frobenius范数、有效秩和对齐度，避免了直接计算完整的NTK矩阵。


<details>
  <summary>Details</summary>
Motivation: 计算完整的NTK矩阵对于循环架构等复杂模型通常不可行，需要更高效的NTK分析方法。

Method: 使用Hutch++迹估计器和单边估计器（仅需前向或反向自动微分），通过随机化方法快速计算NTK的各种度量。

Result: 矩阵自由随机化方法可以实现多个数量级的加速，特别是在模型状态与参数数量差距较大的情况下，单边估计器在低样本情况下优于Hutch++。

Conclusion: 矩阵自由随机化方法能够显著加速NTK的分析和应用，为大规模模型提供了可行的NTK计算方案。

Abstract: The Neural Tangent Kernel (NTK) characterizes how a model's state evolves over Gradient Descent. Computing the full NTK matrix is often infeasible, especially for recurrent architectures. Here, we introduce a matrix-free perspective, using trace estimation to rapidly analyze the empirical, finite-width NTK. This enables fast computation of the NTK's trace, Frobenius norm, effective rank, and alignment. We provide numerical recipes based on the Hutch++ trace estimator with provably fast convergence guarantees. In addition, we show that, due to the structure of the NTK, one can compute the trace using only forward- or reverse-mode automatic differentiation, not requiring both modes. We show these so-called one-sided estimators can outperform Hutch++ in the low-sample regime, especially when the gap between the model state and parameter count is large. In total, our results demonstrate that matrix-free randomized approaches can yield speedups of many orders of magnitude, leading to faster analysis and applications of the NTK.

</details>


### [27] [Near-optimal Linear Predictive Clustering in Non-separable Spaces via Mixed Integer Programming and Quadratic Pseudo-Boolean Reductions](https://arxiv.org/abs/2511.10809)
*Jiazhou Liang,Hassan Khurram,Scott Sanner*

Main category: cs.LG

TL;DR: 提出了两种新的线性预测聚类全局优化方法，通过利用可分性理论特性推导出具有可证明误差界的近似最优解，显著提高了计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有贪婪优化方法缺乏全局最优性，在非可分聚类设置中表现不佳；而混合整数规划方法虽然能保证全局最优但计算效率低下。

Method: 基于约束优化范式，利用可分性理论特性推导近似最优解，将问题简化为二次伪布尔优化问题，显著降低混合整数规划复杂度。

Result: 在合成和真实数据集上的比较分析表明，新方法始终获得近似最优解，回归误差显著低于贪婪优化，计算可扩展性优于现有混合整数规划方法。

Conclusion: 提出的方法在保持全局优化优势的同时，显著提高了线性预测聚类的计算效率和可扩展性，为实际应用提供了可行的解决方案。

Abstract: Linear Predictive Clustering (LPC) partitions samples based on shared linear relationships between feature and target variables, with numerous applications including marketing, medicine, and education. Greedy optimization methods, commonly used for LPC, alternate between clustering and linear regression but lack global optimality. While effective for separable clusters, they struggle in non-separable settings where clusters overlap in feature space. In an alternative constrained optimization paradigm, Bertsimas and Shioda (2007) formulated LPC as a Mixed-Integer Program (MIP), ensuring global optimality regardless of separability but suffering from poor scalability. This work builds on the constrained optimization paradigm to introduce two novel approaches that improve the efficiency of global optimization for LPC. By leveraging key theoretical properties of separability, we derive near-optimal approximations with provable error bounds, significantly reducing the MIP formulation's complexity and improving scalability. Additionally, we can further approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving substantial computational improvements in some settings. Comparative analyses on synthetic and real-world datasets demonstrate that our methods consistently achieve near-optimal solutions with substantially lower regression errors than greedy optimization while exhibiting superior scalability over existing MIP formulations.

</details>


### [28] [Transformers know more than they can tell -- Learning the Collatz sequence](https://arxiv.org/abs/2511.10811)
*François Charton,Ashvni Narayanan*

Main category: cs.LG

TL;DR: 本文研究Transformer模型预测Collatz序列长步长的能力，发现模型准确率受编码基数影响，最高可达99.7%（基数24和32），最低仅37%和25%（基数11和3）。所有模型都学习相同的模式：按输入模2^p的余数分类，逐步学习更长的循环长度。


<details>
  <summary>Details</summary>
Motivation: 使用数学问题作为工具来理解、解释并可能改进语言模型，特别是研究Transformer如何学习复杂的算术函数如Collatz序列计算。

Method: 训练Transformer模型预测Collatz序列的长步长，使用不同基数编码输入和输出，分析模型学习过程和失败模式。

Result: 模型准确率随编码基数变化，最高99.7%，最低25%。模型按输入模2^p的余数分类学习，能近乎完美预测同类输入，但对其他输入准确率低于1%。90%以上的错误是由于正确计算但错误估计循环长度。

Conclusion: 学习复杂算术函数的难点在于理解计算的控制结构（循环长度）。使用数学问题分析语言模型的方法可广泛应用于其他问题并产生丰硕成果。

Abstract: We investigate transformer prediction of long Collatz steps, a complex arithmetic function that maps odd integers to their distant successors in the Collatz sequence ( $u_{n+1}=u_n/2$ if $u_n$ is even, $u_{n+1}=(3u_n+1)/2$ if $u_n$ is odd). Model accuracy varies with the base used to encode input and output. It can be as high as $99.7\%$ for bases $24$ and $32$, and as low as $37$ and $25\%$ for bases $11$ and $3$. Yet, all models, no matter the base, follow a common learning pattern. As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$. Models achieve near-perfect accuracy on these classes, and less than $1\%$ for all other inputs. This maps to a mathematical property of Collatz sequences: the length of the loops involved in the computation of a long Collatz step can be deduced from the binary representation of its input. The learning pattern reflects the model learning to predict inputs associated with increasing loop lengths. An analysis of failure cases reveals that almost all model errors follow predictable patterns. Hallucination, a common feature of large language models, almost never happens. In over $90\%$ of failures, the model performs the correct calculation, but wrongly estimates loop lengths. Our observations give a full account of the algorithms learned by the models. They suggest that the difficulty of learning such complex arithmetic function lies in figuring the control structure of the computation -- the length of the loops. We believe that the approach outlined here, using mathematical problems as tools for understanding, explaining, and perhaps improving language models, can be applied to a broad range of problems and bear fruitful results.

</details>


### [29] [Towards Universal Neural Operators through Multiphysics Pretraining](https://arxiv.org/abs/2511.10829)
*Mikhail Masliaev,Dmitry Gusarov,Ilya Markov,Alexander Hvatov*

Main category: cs.LG

TL;DR: 研究了基于Transformer的神经算子在迁移学习中的表现，评估其在多种PDE问题上的性能，包括参数外推、新变量引入和多方程数据集迁移。


<details>
  <summary>Details</summary>
Motivation: 虽然神经算子广泛用于数据驱动的物理模拟，但其训练计算成本高昂。下游学习通过预训练简化问题再微调复杂问题来解决此问题。

Method: 在更通用的迁移学习设置中评估基于Transformer的神经算子，测试其在多种PDE问题上的表现，包括参数外推、新变量引入和多方程数据集迁移。

Result: 结果表明，先进的神经算子架构能够有效地在PDE问题间迁移知识。

Conclusion: 基于Transformer的神经算子在迁移学习环境中表现良好，能够有效跨PDE问题传递知识。

Abstract: Although neural operators are widely used in data-driven physical simulations, their training remains computationally expensive. Recent advances address this issue via downstream learning, where a model pretrained on simpler problems is fine-tuned on more complex ones. In this research, we investigate transformer-based neural operators, which have previously been applied only to specific problems, in a more general transfer learning setting. We evaluate their performance across diverse PDE problems, including extrapolation to unseen parameters, incorporation of new variables, and transfer from multi-equation datasets. Our results demonstrate that advanced neural operator architectures can effectively transfer knowledge across PDE problems.

</details>


### [30] [Benchmarking Quantum Kernels Across Diverse and Complex Data](https://arxiv.org/abs/2511.10831)
*Yuhan Jiang,Matthew Otten*

Main category: cs.LG

TL;DR: 量子核方法在真实世界高维数据上展现出性能优势，通过变分量子核框架在8个复杂数据集上超越经典RBF核。


<details>
  <summary>Details</summary>
Motivation: 当前量子核方法研究主要局限于低维或合成数据，缺乏对真实世界高维数据的全面评估，需要验证其实际应用潜力。

Method: 开发了基于资源高效ansätze的变分量子核框架，引入参数缩放技术加速收敛，在8个高维真实数据集上进行基准测试。

Result: 经典模拟结果显示，所提出的量子核在性能上明显优于标准经典核（如RBF核），证明了量子核作为高性能工具的潜力。

Conclusion: 适当设计的量子核可以作为多功能高性能工具，为真实世界机器学习中的量子增强应用奠定基础，但需要进一步研究评估实际量子优势。

Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.

</details>


### [31] [SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?](https://arxiv.org/abs/2511.10833)
*Sanchit Kabra,Shobhnik Kriplani,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.LG

TL;DR: SurfaceBench是首个用于符号曲面发现的综合基准，包含183个任务，涵盖15种符号复杂度类别，支持显式、隐式和参数化方程表示形式。该基准通过新颖的符号组合抵抗LLM记忆，并采用几何感知指标评估方程发现质量。


<details>
  <summary>Details</summary>
Motivation: 现有的符号回归基准存在局限性：主要关注标量函数、忽略领域基础、依赖脆弱的字符串匹配指标，无法捕捉科学等价性。需要建立一个能够连接符号推理与几何重建的挑战性测试平台。

Method: 构建包含183个任务的综合基准，涵盖显式、隐式和参数化方程表示形式。每个任务包含真实方程、变量语义和合成的三维数据。采用几何感知指标如Chamfer和Hausdorff距离来评估方程发现质量。

Result: 实验表明，最先进的框架虽然在特定函数族上偶尔成功，但在跨表示类型和曲面复杂度方面难以泛化。SurfaceBench建立了一个具有挑战性和诊断性的测试平台。

Conclusion: SurfaceBench为组合泛化、数据驱动的科学归纳以及LLM的几何感知推理提供了原则性基准，填补了符号推理与几何重建之间的空白。

Abstract: Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench

</details>


### [32] [EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence](https://arxiv.org/abs/2511.10834)
*Ansel Kaplan Erol,Seungjun Lee,Divya Mahajan*

Main category: cs.LG

TL;DR: EarthSight是一个分布式卫星图像智能框架，通过多任务推理、地面站查询调度和动态过滤器排序，在带宽和功耗限制下实现低延迟图像分析。


<details>
  <summary>Details</summary>
Motivation: 传统卫星图像传输管道需要将所有图像下传后再分析，导致数小时到数天的延迟。现有星上机器学习方案将每颗卫星视为独立计算节点，存在冗余推理和资源浪费问题。

Method: 1) 星上多任务推理：使用共享骨干网络分摊多个视觉任务的计算；2) 地面站查询调度器：聚合用户请求，预测优先级并分配计算预算；3) 动态过滤器排序：结合模型选择性、准确性和执行成本，早期拒绝低价值图像。

Result: 相比最先进基线，EarthSight将每张图像的平均计算时间减少1.9倍，将90%分位端到端延迟从51分钟降低到21分钟。

Conclusion: EarthSight通过结合地面站的全局上下文和星上资源感知自适应决策，使卫星星座能够在严格的下行带宽和星上功耗预算内执行可扩展的低延迟图像分析。

Abstract: Low-latency delivery of satellite imagery is essential for time-critical applications such as disaster response, intelligence, and infrastructure monitoring. However, traditional pipelines rely on downlinking all captured images before analysis, introducing delays of hours to days due to restricted communication bandwidth. To address these bottlenecks, emerging systems perform onboard machine learning to prioritize which images to transmit. However, these solutions typically treat each satellite as an isolated compute node, limiting scalability and efficiency. Redundant inference across satellites and tasks further strains onboard power and compute costs, constraining mission scope and responsiveness. We present EarthSight, a distributed runtime framework that redefines satellite image intelligence as a distributed decision problem between orbit and ground. EarthSight introduces three core innovations: (1) multi-task inference on satellites using shared backbones to amortize computation across multiple vision tasks; (2) a ground-station query scheduler that aggregates user requests, predicts priorities, and assigns compute budgets to incoming imagery; and (3) dynamic filter ordering, which integrates model selectivity, accuracy, and execution cost to reject low-value images early and conserve resources. EarthSight leverages global context from ground stations and resource-aware adaptive decisions in orbit to enable constellations to perform scalable, low-latency image analysis within strict downlink bandwidth and onboard power budgets. Evaluations using a prior established satellite simulator show that EarthSight reduces average compute time per image by 1.9x and lowers 90th percentile end-to-end latency from first contact to delivery from 51 to 21 minutes compared to the state-of-the-art baseline.

</details>


### [33] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: 提出了一个区分外在和内在幻觉的评估框架，并利用基于注意力的不确定性量化算法改进幻觉检测性能，发现基于采样的方法适合外在幻觉，而基于注意力聚合的方法更适合内在幻觉。


<details>
  <summary>Details</summary>
Motivation: LLMs在安全关键领域部署时容易产生幻觉，现有检测方法计算成本高且未区分幻觉类型，需要更有效的检测策略。

Method: 引入区分外在和内在幻觉的评估框架，利用基于注意力的不确定性量化算法，提出新颖的注意力聚合策略。

Result: 基于采样的方法（如语义熵）能有效检测外在幻觉但在内在幻觉上失败，而基于输入令牌注意力聚合的方法更适合内在幻觉检测。

Conclusion: 注意力是量化模型不确定性的丰富信号，为根据幻觉性质调整检测策略提供了新方向。

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [34] [Multi-Joint Physics-Informed Deep Learning Framework for Time-Efficient Inverse Dynamics](https://arxiv.org/abs/2511.10878)
*Shuhao Ma,Zeyi Huang,Yu Cao,Wesley Doorsamy,Chaoyang Shi,Jun Li,Zhi-Qiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于物理知识的深度学习框架PI-MJCA-BiGRU，用于从运动学数据直接估计肌肉激活和力量，无需标注数据即可实现生理一致的预测，且推理速度快。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算成本高且缺乏高质量的多关节标注数据集，需要开发高效且无需标注的多关节肌肉激活和力量估计方法。

Method: 使用新颖的多关节交叉注意力模块(MJCA)和双向门控循环单元(BiGRU)层来捕捉关节间协调，通过将多关节动力学、关节间耦合和外部力交互嵌入损失函数实现物理知识引导。

Result: 在两个数据集上的实验验证表明，该方法性能与传统监督方法相当但无需真实标签，MJCA模块显著提升了关节间协调建模能力。

Conclusion: PI-MJCA-BiGRU框架能够实现高效、生理一致的多关节肌肉激活和力量估计，为临床评估和辅助设备控制提供了有前景的解决方案。

Abstract: Time-efficient estimation of muscle activations and forces across multi-joint systems is critical for clinical assessment and assistive device control. However, conventional approaches are computationally expensive and lack a high-quality labeled dataset for multi-joint applications. To address these challenges, we propose a physics-informed deep learning framework that estimates muscle activations and forces directly from kinematics. The framework employs a novel Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Unit (BiGRU) layers to capture inter-joint coordination, enabling each joint to adaptively integrate motion information from others. By embedding multi-joint dynamics, inter-joint coupling, and external force interactions into the loss function, our Physics-Informed MJCA-BiGRU (PI-MJCA-BiGRU) delivers physiologically consistent predictions without labeled data while enabling time-efficient inference. Experimental validation on two datasets demonstrates that PI-MJCA-BiGRU achieves performance comparable to conventional supervised methods without requiring ground-truth labels, while the MJCA module significantly enhances inter-joint coordination modeling compared to other baseline architectures.

</details>


### [35] [FlowPath: Learning Data-Driven Manifolds with Invertible Flows for Robust Irregularly-sampled Time Series Classification](https://arxiv.org/abs/2511.10841)
*YongKyung Oh,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: FlowPath提出了一种通过可逆神经流学习控制路径几何形状的新方法，用于处理稀疏和不规则采样的时间序列建模问题，相比固定插值方法显著提升了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有神经控制微分方程方法对控制路径的选择高度敏感，固定插值方案强加了简化的几何假设，往往不能准确表示底层数据流形，特别是在高缺失率情况下。

Method: FlowPath通过可逆神经流学习控制路径的几何形状，构建连续且数据自适应的流形，利用可逆性约束强制信息保持和良好行为的变换。

Result: 在18个基准数据集和真实世界案例研究中的实证评估表明，FlowPath相比使用固定插值或不可逆架构的基线方法，在分类准确率上实现了统计显著的改进。

Conclusion: 结果表明，不仅需要建模路径上的动态，还需要建模路径本身的几何形状，这为从不规则时间序列中学习提供了鲁棒且可泛化的解决方案。

Abstract: Modeling continuous-time dynamics from sparse and irregularly-sampled time series remains a fundamental challenge. Neural controlled differential equations provide a principled framework for such tasks, yet their performance is highly sensitive to the choice of control path constructed from discrete observations. Existing methods commonly employ fixed interpolation schemes, which impose simplistic geometric assumptions that often misrepresent the underlying data manifold, particularly under high missingness. We propose FlowPath, a novel approach that learns the geometry of the control path via an invertible neural flow. Rather than merely connecting observations, FlowPath constructs a continuous and data-adaptive manifold, guided by invertibility constraints that enforce information-preserving and well-behaved transformations. This inductive bias distinguishes FlowPath from prior unconstrained learnable path models. Empirical evaluations on 18 benchmark datasets and a real-world case study demonstrate that FlowPath consistently achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures. These results highlight the importance of modeling not only the dynamics along the path but also the geometry of the path itself, offering a robust and generalizable solution for learning from irregular time series.

</details>


### [36] [Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning](https://arxiv.org/abs/2511.10843)
*Alexander W. Goodall,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出利用精心设计的行为策略收集离线数据，以降低回报估计的方差，从而提高强化学习算法的样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 许多强化学习算法依赖回报估计进行策略改进，但高方差的回报估计会导致样本效率低下和训练不稳定。离线评估的最新研究表明，设计良好的行为策略可以收集离线数据来获得方差更低的回报估计。

Method: 将离线评估的关键见解扩展到在线强化学习设置，在策略评估和改进交替进行时学习最优策略。扩展两种策略梯度方法，使用行为策略收集数据以获得方差更低的回报估计。

Result: 实验证明，在多种环境中，该方法相比传统方法具有更好的样本效率和性能表现。

Conclusion: 收集离线数据的行为策略可以显著降低回报估计的方差，从而提升强化学习算法的效率和稳定性。

Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.

</details>


### [37] [STAMP: Spatial-Temporal Adapter with Multi-Head Pooling](https://arxiv.org/abs/2511.10848)
*Brad Shook,Abby Turner,Jieshi Chen,Michał Wiliński,Mononito Goswami,Jonathan Elmer,Artur Dubrawski*

Main category: cs.LG

TL;DR: 提出了一个名为STAMP的轻量级适配器，利用通用时间序列基础模型的单变量嵌入，隐式建模EEG数据的时空特征，在EEG分类任务上达到与专用EEG基础模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对EEG专用基础模型与通用时间序列基础模型在EEG任务上的比较分析，需要探索如何有效利用通用TSFM来处理EEG数据。

Method: 开发了STAMP适配器，通过多头池化技术利用通用TSFM的单变量嵌入，隐式捕捉EEG数据的时空特性，支持灵活输入且参数轻量。

Result: 在8个EEG临床任务基准数据集上的综合评估表明，STAMP适配器性能与最先进的EEG专用基础模型相当，并通过消融研究验证了其有效性。

Conclusion: STAMP适配器提供了一种轻量、灵活的方法，使通用时间序列基础模型能够有效建模EEG数据，为EEG分析提供了新的解决方案。

Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.

</details>


### [38] [ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries](https://arxiv.org/abs/2511.10855)
*Tom Yuviler,Dana Drachsler-Cohen*

Main category: cs.LG

TL;DR: ExPairT-LLM是一种基于LLM的代码选择算法，通过提出成对成员资格和成对等价性查询，以锦标赛方式选择正确程序，在四个代码数据集上平均提升13.0%的pass@1成功率。


<details>
  <summary>Details</summary>
Motivation: 现有代码选择算法可能无法识别正确程序，因为它们可能错误识别不等价程序，或依赖LLM并假设其总能正确确定每个输入输出。

Method: 提出ExPairT-LLM精确学习算法，通过向LLM提出两种新类型查询：成对成员资格和成对等价性查询，这些查询对LLM更简单，并通过锦标赛方式选择程序，对某些LLM错误具有鲁棒性。

Result: 在四个流行代码数据集上评估，ExPairT-LLM的pass@1（成功率）平均比最先进的代码选择算法高出+13.0%，最高可达+27.1%。还将执行复杂推理的LLM的pass@1提高了+24.0%。

Conclusion: ExPairT-LLM通过简化的查询类型和锦标赛方法，显著提高了代码生成的准确性和鲁棒性。

Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.

</details>


### [39] [Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go](https://arxiv.org/abs/2511.10868)
*Yashshi Pipalani,Hritik Raj,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta*

Main category: cs.LG

TL;DR: GO UT Bench是一个包含5264对代码和单元测试的基准数据集，用于解决代码LLM在训练数据不平衡问题，特别是针对Golang语言。通过微调，模型在超过75%的基准任务上表现优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 解决代码LLM训练数据不平衡问题，特别是开源代码过度表示而软件工程任务（如单元测试生成）表示不足的问题，尤其是在低资源语言如Golang中。

Method: 引入GO UT Bench基准数据集，包含5264对代码和单元测试，来自10个不同领域的Golang仓库。在两个LLM家族（专家混合和密集解码器）上进行微调评估。

Result: 微调后的模型在超过75%的基准任务上表现优于其基础对应模型。

Conclusion: GO UT Bench作为微调数据集能有效提升代码LLM在单元测试生成等软件工程任务上的性能，解决了训练数据不平衡问题。

Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

</details>


### [40] [Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations](https://arxiv.org/abs/2511.10872)
*Shuyuan Zhang,Zihan Wang,Xiao-Wen Chang,Doina Precup*

Main category: cs.LG

TL;DR: 提出G4RL方法，通过图编码器-解码器评估未见状态，提升目标导向分层强化学习在对称可逆环境中的性能


<details>
  <summary>Details</summary>
Motivation: 现有图导向分层强化学习方法依赖领域知识构建图，或动态构建但难以充分利用图信息，存在样本效率低和子目标表示差的问题

Method: 开发图编码器-解码器，利用探索过程中生成的状态图训练网络，为现有GCHRL方法提供高低层内在奖励

Result: 实验表明该方法能显著提升最先进GCHRL方法的性能，在密集和稀疏奖励环境中都有良好表现，计算成本增加很小

Conclusion: G4RL方法能有效解决图导向分层强化学习中的关键问题，在对称可逆环境中具有广泛应用前景

Abstract: The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.

</details>


### [41] [Multi-View Polymer Representations for the Open Polymer Prediction](https://arxiv.org/abs/2511.10893)
*Wonjin Jung,Yongseok Choi*

Main category: cs.LG

TL;DR: 提出了一种多视图聚合物性质预测系统，集成四种表征方法并通过均匀集成进行预测，在NeurIPS 2025 Open Polymer Prediction Challenge中排名第9/2241。


<details>
  <summary>Details</summary>
Motivation: 利用互补表征来改进聚合物性质预测，通过多视图设计整合不同表征方法的优势。

Method: 集成四种表征家族：(i) RDKit/Morgan描述符，(ii) 图神经网络，(iii) 3D信息表征，(iv) 预训练SMILES语言模型，采用均匀集成和测试时增强。

Result: 在2241个团队中排名第9，公开MAE为0.057，私有MAE为0.082。

Conclusion: 多视图集成方法在聚合物性质预测任务中表现出色，证明了互补表征的有效性。

Abstract: We address polymer property prediction with a multi-view design that exploits complementary representations. Our system integrates four families: (i) tabular RDKit/Morgan descriptors, (ii) graph neural networks, (iii) 3D-informed representations, and (iv) pretrained SMILES language models, and averages per-property predictions via a uniform ensemble. Models are trained with 10-fold splits and evaluated with SMILES test-time augmentation. The approach ranks 9th of 2241 teams in the Open Polymer Prediction Challenge at NeurIPS 2025. The submitted ensemble achieves a public MAE of 0.057 and a private MAE of 0.082.

</details>


### [42] [Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework](https://arxiv.org/abs/2511.10915)
*Guanxiong He,Jie Wang,Liaoyuan Tang,Zheng Wang,Rong Wang,Feiping Nie*

Main category: cs.LG

TL;DR: 提出了SPP-FGC框架，通过本地结构图实现隐私保护的联邦聚类，解决了传统方法在性能与隐私之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦聚类中性能与隐私的权衡问题：传输嵌入表示会泄露敏感数据，而仅共享抽象聚类原型会导致模型准确性下降。

Method: 基于客户端-服务器架构，客户端构建捕获数据内在关系的私有结构图，服务器安全聚合和对齐形成全局图，从中推导统一聚类结构。提供SPP-FGC（单轮通信）和SPP-FGC+（迭代优化）两种模式。

Result: 在广泛实验中达到最先进性能，相比联邦基线将聚类准确率（NMI）提升高达10%，同时保持可证明的隐私保证。

Conclusion: SPP-FGC框架通过结构图作为隐私保护知识共享媒介，成功解决了联邦聚类中的性能-隐私权衡问题，在保持隐私的同时显著提升聚类准确性。

Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.

</details>


### [43] [GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning](https://arxiv.org/abs/2511.10936)
*Ying Song,Balaji Palanisamy*

Main category: cs.LG

TL;DR: GraphToxin是首个针对图遗忘的图重构攻击方法，通过曲率匹配模块能够从遗忘后的图神经网络中恢复被删除的敏感信息，包括个人链接和连接内容，对现有防御机制构成严重威胁。


<details>
  <summary>Details</summary>
Motivation: 图遗忘虽然能响应"被遗忘权"法规要求删除敏感信息，但多方参与创造了新的攻击面，残留的数据痕迹仍可能被攻击者利用来恢复已删除样本，从而破坏图遗忘的基本功能。

Method: 提出GraphToxin攻击框架，引入创新的曲率匹配模块为完整遗忘图恢复提供细粒度指导，支持单节点和多节点移除场景下的白盒和黑盒攻击设置。

Result: 实验证明GraphToxin能成功恢复被删除个体的信息、个人链接及其连接的敏感内容，现有防御机制对此攻击基本无效甚至可能增强其效果。

Conclusion: GraphToxin揭示了图遗忘方法面临严重隐私风险，亟需开发更有效和鲁棒的防御策略来应对此类图重构攻击。

Abstract: Graph unlearning has emerged as a promising solution for complying with "the right to be forgotten" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual traces of deleted data can still remain in the unlearned graph neural networks. These vulnerabilities can be exploited by attackers to recover the supposedly erased samples, thereby undermining the inherent functionality of graph unlearning. In this work, we propose GraphToxin, the first graph reconstruction attack against graph unlearning. Specifically, we introduce a novel curvature matching module to provide a fine-grained guidance for full unlearned graph recovery. We demonstrate that GraphToxin can successfully subvert the regulatory guarantees expected from graph unlearning - it can recover not only a deleted individual's information and personal links but also sensitive content from their connections, thereby posing substantially more detrimental threats. Furthermore, we extend GraphToxin to multiple node removals under both white-box and black-box setting. We highlight the necessity of a worst-case analysis and propose a comprehensive evaluation framework to systematically assess the attack performance under both random and worst-case node removals. This provides a more robust and realistic measure of the vulnerability of graph unlearning methods to graph reconstruction attacks. Our extensive experiments demonstrate the effectiveness and flexibility of GraphToxin. Notably, we show that existing defense mechanisms are largely ineffective against this attack and, in some cases, can even amplify its performance. Given the severe privacy risks posed by GraphToxin, our work underscores the urgent need for the development of more effective and robust defense strategies against this attack.

</details>


### [44] [Cascading Bandits With Feedback](https://arxiv.org/abs/2511.10938)
*R Sri Prakash,Nikhil Karamchandani,Sharayu Moharir*

Main category: cs.LG

TL;DR: 该论文研究了边缘推理中的级联赌博机模型，分析了四种决策策略的理论遗憾保证，发现LCB和Thompson Sampling因持续自适应而获得常数遗憾，优于固定排序的Explore-then-Commit和Action Elimination。


<details>
  <summary>Details</summary>
Motivation: 受边缘推理挑战的驱动，研究每个臂对应具有相关准确度和错误概率的推理模型的级联赌博机变体。

Method: 分析四种决策策略：Explore-then-Commit、Action Elimination、Lower Confidence Bound (LCB)和Thompson Sampling，并为每种策略提供理论遗憾保证。

Result: 与经典赌博机设置不同，Explore-then-Commit和Action Elimination因在探索阶段后承诺固定排序而遭受次优遗憾。LCB和Thompson Sampling基于观察反馈持续更新决策，实现常数O(1)遗憾。模拟验证了这些理论发现。

Conclusion: 在不确定性下，自适应对于高效边缘推理至关重要，LCB和Thompson Sampling的持续自适应特性使其优于固定排序策略。

Abstract: Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.

</details>


### [45] [Flow matching-based generative models for MIMO channel estimation](https://arxiv.org/abs/2511.10941)
*Wenkai Liu,Nan Ma,Jianqiao Chen,Xiaoxuan Qi,Yuhang Ma*

Main category: cs.LG

TL;DR: 提出一种基于流匹配的生成模型用于MIMO信道估计，相比扩散模型能显著减少采样开销，同时在不同信道条件下获得优越的信道估计精度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在信道估计中表现出高精度，但采样速度慢是主要挑战。为解决这个问题，提出基于流匹配的生成模型来加速采样过程。

Method: 在流匹配框架下构建信道估计问题，构建从噪声信道分布到真实信道分布的条件概率路径，路径沿直线轨迹以恒定速度演化。推导仅依赖噪声统计的速度场来指导生成模型训练，在采样阶段使用训练好的速度场作为先验信息，通过ODE欧拉求解器实现快速可靠的噪声信道增强。

Result: 数值结果表明，相比其他流行的基于扩散模型的方案（如基于分数匹配的方案），所提出的基于流匹配的信道估计方案能显著减少采样开销，同时在不同信道条件下实现优越的信道估计精度。

Conclusion: 基于流匹配的信道估计方案在保持高精度的同时，有效解决了扩散模型采样速度慢的问题，为MIMO信道估计提供了一种高效可靠的解决方案。

Abstract: Diffusion model (DM)-based channel estimation, which generates channel samples via a posteriori sampling stepwise with denoising process, has shown potential in high-precision channel state information (CSI) acquisition. However, slow sampling speed is an essential challenge for recent developed DM-based schemes. To alleviate this problem, we propose a novel flow matching (FM)-based generative model for multiple-input multiple-output (MIMO) channel estimation. We first formulate the channel estimation problem within FM framework, where the conditional probability path is constructed from the noisy channel distribution to the true channel distribution. In this case, the path evolves along the straight-line trajectory at a constant speed. Then, guided by this, we derive the velocity field that depends solely on the noise statistics to guide generative models training. Furthermore, during the sampling phase, we utilize the trained velocity field as prior information for channel estimation, which allows for quick and reliable noise channel enhancement via ordinary differential equation (ODE) Euler solver. Finally, numerical results demonstrate that the proposed FM-based channel estimation scheme can significantly reduce the sampling overhead compared to other popular DM-based schemes, such as the score matching (SM)-based scheme. Meanwhile, it achieves superior channel estimation accuracy under different channel conditions.

</details>


### [46] [From Parameter to Representation: A Closed-Form Approach for Controllable Model Merging](https://arxiv.org/abs/2511.10943)
*Jialin Wu,Jian Yang,Handing Wang,Jiajun Wen,Zhiyong Yu*

Main category: cs.LG

TL;DR: 提出一种新的可控模型融合方法，通过直接修正模型最终表示而非参数空间优化，使用最优线性变换的闭式解替代昂贵的离线多目标优化，实现线性复杂度的高效Pareto最优模型生成。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法面临参数干扰问题，而可控模型融合需要昂贵的离线多目标优化，其复杂度随任务数量指数增长，限制了实际应用。

Method: 将模型融合视角从参数空间优化转向直接修正最终表示，建模为最优线性变换问题，获得闭式解，用单步架构无关计算替代整个离线优化过程。

Result: 实验结果显示该方法生成更优的Pareto前沿，具有更精确的偏好对齐和显著降低的计算成本。

Conclusion: 该方法通过直接修正模型表示实现了高效可控模型融合，解决了现有方法复杂度高的问题，为多任务模型融合提供了更实用的解决方案。

Abstract: Model merging combines expert models for multitask performance but faces challenges from parameter interference. This has sparked recent interest in controllable model merging, giving users the ability to explicitly balance performance trade-offs. Existing approaches employ a compile-then-query paradigm, performing a costly offline multi-objective optimization to enable fast, preference-aware model generation. This offline stage typically involves iterative search or dedicated training, with complexity that grows exponentially with the number of tasks. To overcome these limitations, we shift the perspective from parameter-space optimization to a direct correction of the model's final representation. Our approach models this correction as an optimal linear transformation, yielding a closed-form solution that replaces the entire offline optimization process with a single-step, architecture-agnostic computation. This solution directly incorporates user preferences, allowing a Pareto-optimal model to be generated on-the-fly with complexity that scales linearly with the number of tasks. Experimental results show our method generates a superior Pareto front with more precise preference alignment and drastically reduced computational cost.

</details>


### [47] [How Data Quality Affects Machine Learning Models for Credit Risk Assessment](https://arxiv.org/abs/2511.10964)
*Andrea Maurino*

Main category: cs.LG

TL;DR: 研究了数据质量问题对信用风险评估机器学习模型预测准确性的影响，通过引入受控数据损坏评估了10种常用模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在信用风险评估中的应用日益增多，其有效性很大程度上取决于输入数据的质量，需要研究各种数据质量问题对模型性能的影响。

Method: 使用开源数据集，通过Pucktrick库引入受控数据损坏（缺失值、噪声属性、异常值、标签错误），评估随机森林、SVM、逻辑回归等10种常用模型的鲁棒性。

Result: 实验显示，不同模型对数据退化的鲁棒性存在显著差异，具体取决于数据损坏的性质和严重程度。

Conclusion: 提出的方法和配套工具为从业者增强数据管道鲁棒性提供了实用支持，并为研究人员在数据为中心的AI环境中进一步实验提供了灵活框架。

Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.

</details>


### [48] [Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm](https://arxiv.org/abs/2511.11009)
*Fuxiang Huang,Xiaowei Fu,Shiyu Ye,Lina Ma,Wen Li,Xinbo Gao,David Zhang,Lei Zhang*

Main category: cs.LG

TL;DR: 该论文提出了无监督鲁棒域自适应（URDA）范式，解决了传统无监督域自适应方法在对抗攻击下的脆弱性问题，并提出了解耦对抗鲁棒性训练（DART）算法。


<details>
  <summary>Details</summary>
Motivation: 传统无监督域自适应方法强调迁移能力但忽视对抗鲁棒性，而标准对抗训练在UDA中效果不佳。论文旨在解决UDA+VAT范式中的内在纠缠问题。

Method: 提出URDA范式，推导其泛化界理论，并设计DART算法——一个两阶段训练过程：先预训练任意UDA模型，然后通过解耦蒸馏进行瞬时鲁棒化后训练。

Result: 在四个基准数据集上的实验表明，DART能有效增强鲁棒性同时保持域适应性，验证了URDA范式和理论的有效性。

Conclusion: 这是首次建立URDA范式和理论的工作，提出的DART算法简单有效，解决了UDA中的对抗鲁棒性问题。

Abstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.

</details>


### [49] [Enhancing Graph Representations with Neighborhood-Contextualized Message-Passing](https://arxiv.org/abs/2511.11046)
*Brian Godwin Lim*

Main category: cs.LG

TL;DR: 提出了邻域上下文消息传递(NCMP)框架，通过整合更广泛的局部邻域信息来增强传统消息传递GNN的表达能力，并开发了SINC-GCN模型作为具体实现。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递GNN只考虑中心节点与单个邻居节点的特征，未能充分利用整个邻域集合中的丰富上下文信息，限制了学习复杂关系的能力。

Method: 首先形式化邻域上下文概念，基于此将消息传递泛化为NCMP框架，然后提出简单实用的参数化方法，开发了SINC-GCN模型。

Result: 在合成二元节点分类问题上的初步分析表明，所提出的GNN架构具有更强的表达能力和效率。

Conclusion: NCMP框架为增强传统GNN的图表示能力提供了一条实用的新路径。

Abstract: Graph neural networks (GNNs) have become an indispensable tool for analyzing relational data. In the literature, classical GNNs may be classified into three variants: convolutional, attentional, and message-passing. While the standard message-passing variant is highly expressive, its typical pair-wise messages nevertheless only consider the features of the center node and each neighboring node individually. This design fails to incorporate the rich contextual information contained within the broader local neighborhood, potentially hindering its ability to learn complex relationships within the entire set of neighboring nodes. To address this limitation, this work first formalizes the concept of neighborhood-contextualization, rooted in a key property of the attentional variant. This then serves as the foundation for generalizing the message-passing variant to the proposed neighborhood-contextualized message-passing (NCMP) framework. To demonstrate its utility, a simple, practical, and efficient method to parametrize and operationalize NCMP is presented, leading to the development of the proposed Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN). A preliminary analysis on a synthetic binary node classification problem then underscores both the expressivity and efficiency of the proposed GNN architecture. Overall, the paper lays the foundation for the novel NCMP framework as a practical path toward further enhancing the graph representational power of classical GNNs.

</details>


### [50] [Echoless Label-Based Pre-computation for Memory-Efficient Heterogeneous Graph Learning](https://arxiv.org/abs/2511.11081)
*Jun Hu,Shangheng Chen,Yufei He,Yuan Li,Bryan Hooi,Bingsheng He*

Main category: cs.LG

TL;DR: 提出了Echoless-LP方法，通过分区聚焦的无回声传播(PFEP)消除训练标签泄露问题，解决了传统基于标签的预计算HGNNs中的回声效应问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于标签的预计算方法存在训练标签泄露问题，即节点自身标签信息在多跳消息传递中回传给自己，形成回声效应。现有缓解策略在大图上内存效率低或与先进消息传递方法不兼容。

Method: 提出了Echoless-LP方法，包含PFEP机制：将目标节点分区，每个分区中的节点只从其他分区的邻居收集标签信息，避免回声。还引入了非对称分区方案(APS)和PostAdjust机制来处理分区造成的信息损失和分布偏移。

Result: 在公共数据集上的实验表明，Echoless-LP相比基线方法实现了更优的性能，同时保持了内存效率。

Conclusion: Echoless-LP有效解决了基于标签预计算的HGNNs中的训练标签泄露问题，具有内存效率高、与各种消息传递方法兼容的优点。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are widely used for deep learning on heterogeneous graphs. Typical end-to-end HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Pre-computation-based HGNNs address this by performing message passing only once during preprocessing, collecting neighbor information into regular-shaped tensors, which enables efficient mini-batch training. Label-based pre-computation methods collect neighbors' label information but suffer from training label leakage, where a node's own label information propagates back to itself during multi-hop message passing - the echo effect. Existing mitigation strategies are memory-inefficient on large graphs or suffer from compatibility issues with advanced message passing methods. We propose Echoless Label-based Pre-computation (Echoless-LP), which eliminates training label leakage with Partition-Focused Echoless Propagation (PFEP). PFEP partitions target nodes and performs echoless propagation, where nodes in each partition collect label information only from neighbors in other partitions, avoiding echo while remaining memory-efficient and compatible with any message passing method. We also introduce an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism to address information loss from partitioning and distributional shifts across partitions. Experiments on public datasets demonstrate that Echoless-LP achieves superior performance and maintains memory efficiency compared to baselines.

</details>


### [51] [Scalable Population Training for Zero-Shot Coordination](https://arxiv.org/abs/2511.11083)
*Bingyu Hui,Lebin Yu,Quanming Yao,Yunpeng Qu,Xudong Zhang,Jian Wang*

Main category: cs.LG

TL;DR: 提出ScaPT框架，通过元代理和互信息正则化器实现大规模种群训练，解决零样本协调中种群规模受限的问题，在Hanabi游戏中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于种群的训练方法受计算资源限制，主要关注小种群中的多样性优化，而忽略了扩大种群规模带来的性能提升潜力。

Method: ScaPT框架包含两个关键组件：通过选择性共享参数实现种群的元代理，以及保证种群多样性的互信息正则化器。

Result: 在Hanabi游戏中与代表性框架进行对比评估，证实了ScaPT的优越性能。

Conclusion: ScaPT通过高效的大规模种群训练，显著提升了零样本协调的性能表现。

Abstract: Zero-shot coordination(ZSC) has become a hot topic in reinforcement learning research recently. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators that are not seen before without any fine-tuning. Population-based training has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi and confirms its superiority.

</details>


### [52] [Sheaf Cohomology of Linear Predictive Coding Networks](https://arxiv.org/abs/2511.11092)
*Jeffrey Seely*

Main category: cs.LG

TL;DR: 该论文将线性预测编码网络建模为胞腔层，揭示了网络拓扑中的反馈回路会产生内部矛盾，导致学习停滞，并提供了诊断工具和权重初始化原则。


<details>
  <summary>Details</summary>
Motivation: 预测编码用局部优化替代全局反向传播，但反馈拓扑会产生内部矛盾，阻碍学习过程，需要理论框架来分析和解决这些问题。

Method: 使用胞腔层理论建模线性预测编码网络：将激活映射为层上链，预测误差为层上边界映射，推理过程是层拉普拉斯算子下的扩散。利用霍奇分解分析反馈拓扑中的内部矛盾。

Result: 证明了层上同调描述了推理无法消除的不可约误差模式，反馈回路会产生与监督无关的预测误差，霍奇分解可确定这些矛盾何时导致学习停滞。

Conclusion: 层形式主义为识别有问题的网络配置提供了诊断工具，并为循环预测编码网络的有效权重初始化提供了设计原则。

Abstract: Predictive coding (PC) replaces global backpropagation with local optimization over weights and activations. We show that linear PC networks admit a natural formulation as cellular sheaves: the sheaf coboundary maps activations to edge-wise prediction errors, and PC inference is diffusion under the sheaf Laplacian. Sheaf cohomology then characterizes irreducible error patterns that inference cannot remove. We analyze recurrent topologies where feedback loops create internal contradictions, introducing prediction errors unrelated to supervision. Using a Hodge decomposition, we determine when these contradictions cause learning to stall. The sheaf formalism provides both diagnostic tools for identifying problematic network configurations and design principles for effective weight initialization for recurrent PC networks.

</details>


### [53] [SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems](https://arxiv.org/abs/2511.11111)
*Xin Wang,Pietro Lodi Rizzini,Sourav Medya,Zhiling Lan*

Main category: cs.LG

TL;DR: 提出了一种结合图神经网络和大型语言模型的替代模型，用于准确预测Dragonfly网络中的应用运行时间，支持高效的混合仿真。


<details>
  <summary>Details</summary>
Motivation: Dragonfly网络在高性能计算中面临工作负载干扰的挑战，传统并行离散事件仿真计算成本高，不适用于大规模或实时场景。

Method: 结合图神经网络和大型语言模型，从端口级路由器数据中捕获空间和时间模式。

Result: 该模型优于现有的统计和机器学习基线方法，能够准确预测运行时间。

Conclusion: 该方法支持Dragonfly网络的高效混合仿真，解决了传统仿真方法计算成本高的问题。

Abstract: The Dragonfly network, with its high-radix and low-diameter structure, is a leading interconnect in high-performance computing. A major challenge is workload interference on shared network links. Parallel discrete event simulation (PDES) is commonly used to analyze workload interference. However, high-fidelity PDES is computationally expensive, making it impractical for large-scale or real-time scenarios. Hybrid simulation that incorporates data-driven surrogate models offers a promising alternative, especially for forecasting application runtime, a task complicated by the dynamic behavior of network traffic. We present \ourmodel, a surrogate model that combines graph neural networks (GNNs) and large language models (LLMs) to capture both spatial and temporal patterns from port level router data. \ourmodel outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.

</details>


### [54] [Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization](https://arxiv.org/abs/2511.11118)
*Gerard Pons,Besim Bilalli,Anna Queralt*

Main category: cs.LG

TL;DR: 提出了一种新的知识图谱嵌入初始化策略，利用KG模式和先前学习的嵌入为新实体获取初始表示，提高知识获取效率并减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 知识图谱频繁更新需要其嵌入适应变化，现有持续学习方法中嵌入初始化对最终嵌入准确性和训练时间有重要影响，特别是对于小而频繁的更新。

Method: 利用知识图谱模式和先前学习的嵌入，基于实体所属类别为新实体获取初始表示，可无缝集成到现有的KGE持续学习方法中。

Result: 实验分析表明该初始化策略提高了KGE的预测性能，增强了知识保留，加速了知识获取并减少了训练所需时间。

Conclusion: 该方法在不同类型的KGE学习模型中均表现出优势，能够有效提升持续学习效果。

Abstract: Many Knowledege Graphs (KGs) are frequently updated, forcing their Knowledge Graph Embeddings (KGEs) to adapt to these changes. To address this problem, continual learning techniques for KGEs incorporate embeddings for new entities while updating the old ones. One necessary step in these methods is the initialization of the embeddings, as an input to the KGE learning process, which can have an important impact in the accuracy of the final embeddings, as well as in the time required to train them. This is especially relevant for relatively small and frequent updates. We propose a novel informed embedding initialization strategy, which can be seamlessly integrated into existing continual learning methods for KGE, that enhances the acquisition of new knowledge while reducing catastrophic forgetting. Specifically, the KG schema and the previously learned embeddings are utilized to obtain initial representations for the new entities, based on the classes the entities belong to. Our extensive experimental analysis shows that the proposed initialization strategy improves the predictive performance of the resulting KGEs, while also enhancing knowledge retention. Furthermore, our approach accelerates knowledge acquisition, reducing the number of epochs, and therefore time, required to incrementally learn new embeddings. Finally, its benefits across various types of KGE learning models are demonstrated.

</details>


### [55] [Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods](https://arxiv.org/abs/2511.11143)
*Federico Maddanu,Tommaso Proietti,Riccardo Crupi*

Main category: cs.LG

TL;DR: 提出并评估了多种鲁棒统计方法，用于在中等和高维数据集中高效检测银行账户余额中的异常点，应用于260万条匿名用户银行账户余额的每日记录。


<details>
  <summary>Details</summary>
Motivation: 检测银行账户余额中的异常点对金融机构至关重要，可以识别潜在欺诈、操作问题或其他异常情况。鲁棒统计有助于标记异常值并提供不受污染观测影响的数据分布参数估计。

Method: 提出并评估了多种鲁棒方法，这些方法在中等和高维数据集中具有高崩溃点和低计算时间，可能计算效率较高。

Result: 方法应用于约260万条匿名用户银行账户余额的每日记录。

Conclusion: 所提出的鲁棒方法在中等和高维数据集中具有计算效率，适用于大规模银行账户异常检测。

Abstract: Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.

</details>


### [56] [Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI](https://arxiv.org/abs/2511.11152)
*Tanmay Ghosh,Shaurabh Anand,Rakesh Gomaji Nannewar,Nithin Nagaraj*

Main category: cs.LG

TL;DR: 开发了一个可解释的深度学习框架，用于印度四个主要城市的短期降水预测，结合CNN-ConvLSTM架构和多种可解释性分析方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习降水预测模型通常作为黑箱运行，限制了在实际天气预报中的应用，需要提高透明度同时保持准确性。

Method: 采用混合Time-Distributed CNN-ConvLSTM架构，使用ERA5再分析数据进行训练，为每个城市优化卷积滤波器数量，并应用排列重要性、Grad-CAM、时间遮挡和反事实扰动等可解释性分析。

Result: 模型在不同城市取得RMSE值：班加罗尔0.21毫米/天、孟买0.52毫米/天、德里0.48毫米/天、加尔各答1.80毫米/天，预测范围从1天到5天不等。

Conclusion: 研究表明可解释AI能够提供准确的降水预测，并为不同城市环境中的降水模式提供透明洞察。

Abstract: Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.

</details>


### [57] [Adaptive Symmetrization of the KL Divergence](https://arxiv.org/abs/2511.11159)
*Omri Ben-Dov,Luiz F. O. Chamon*

Main category: cs.LG

TL;DR: 提出了一种通过代理模型联合训练来最小化Jeffreys散度的新方法，结合了归一化流和能量基模型的优势


<details>
  <summary>Details</summary>
Motivation: 传统方法主要使用前向KL散度，但其不对称性可能无法捕捉目标分布的所有特性；对称替代方案如对抗训练或反向KL散度计算困难且不稳定

Method: 使用代理模型联合训练，将任务表述为约束优化问题，通过代理模型辅助优化主模型的Jeffreys散度，并在训练过程中自适应调整模型优先级

Result: 开发出实用算法，能够结合归一化流和能量基模型的优势

Conclusion: 该框架可用于密度估计、图像生成和基于模拟的推理等任务，有效解决了对称散度最小化的挑战

Abstract: Many tasks in machine learning can be described as or reduced to learning a probability distribution given a finite set of samples. A common approach is to minimize a statistical divergence between the (empirical) data distribution and a parameterized distribution, e.g., a normalizing flow (NF) or an energy-based model (EBM). In this context, the forward KL divergence is a ubiquitous due to its tractability, though its asymmetry may prevent capturing some properties of the target distribution. Symmetric alternatives involve brittle min-max formulations and adversarial training (e.g., generative adversarial networks) or evaluating the reverse KL divergence, as is the case for the symmetric Jeffreys divergence, which is challenging to compute from samples. This work sets out to develop a new approach to minimize the Jeffreys divergence. To do so, it uses a proxy model whose goal is not only to fit the data, but also to assist in optimizing the Jeffreys divergence of the main model. This joint training task is formulated as a constrained optimization problem to obtain a practical algorithm that adapts the models priorities throughout training. We illustrate how this framework can be used to combine the advantages of NFs and EBMs in tasks such as density estimation, image generation, and simulation-based inference.

</details>


### [58] [Training Neural Networks at Any Scale](https://arxiv.org/abs/2511.11163)
*Thomas Pethick,Kimon Antonakopoulos,Antonio Silveti-Falls,Leena Chennuru Vankadara,Volkan Cevher*

Main category: cs.LG

TL;DR: 对现代神经网络训练优化方法的综述，重点关注效率和可扩展性，介绍了最先进的优化算法及其统一框架。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络规模和复杂度的增加，需要高效的优化方法来处理大规模训练问题，同时让算法能够适应不同规模的问题。

Method: 提出了统一的算法模板，强调适应问题结构的重要性，并探讨如何使算法对问题规模具有不可知性。

Result: 系统性地整理了现代神经网络优化方法，为从业者和研究人员提供了实用的指导框架。

Conclusion: 本文为希望参与这一激动人心新发展的从业者和研究人员提供了入门介绍，强调了优化算法在神经网络训练中的重要性。

Abstract: This article reviews modern optimization methods for training neural networks with an emphasis on efficiency and scale. We present state-of-the-art optimization algorithms under a unified algorithmic template that highlights the importance of adapting to the structures in the problem. We then cover how to make these algorithms agnostic to the scale of the problem. Our exposition is intended as an introduction for both practitioners and researchers who wish to be involved in these exciting new developments.

</details>


### [59] [Power Ensemble Aggregation for Improved Extreme Event AI Prediction](https://arxiv.org/abs/2511.11170)
*Julien Collard,Pierre Gentine,Tian Zheng*

Main category: cs.LG

TL;DR: 使用幂均值聚合集成预测可显著提高热浪等极端气候事件预测的准确性，该方法在预测更高极端事件时效果更佳。


<details>
  <summary>Details</summary>
Motivation: 改进极端气候事件（特别是热浪）的预测能力，将其构建为分类问题，预测地表气温是否会在指定时间内超过局部分位数阈值。

Method: 将基于机器学习的天气预报模型生成化，并应用幂均值这一非线性聚合方法来集成预测结果。

Result: 幂均值聚合方法比传统均值预测在预测极端热事件方面具有更好的准确性，且对于更高极端事件的预测效果更显著。

Conclusion: 幂均值聚合方法在极端气候事件预测中表现出良好的适应性和有效性，特别是对于更高极端事件的预测具有显著优势。

Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.

</details>


### [60] [On-line learning of dynamic systems: sparse regression meets Kalman filtering](https://arxiv.org/abs/2511.11178)
*Gianluigi Pillonetto,Akram Yazdani,Aleksandr Aravkin*

Main category: cs.LG

TL;DR: 本文提出了Sindy Kalman Filter (SKF)，将稀疏识别与卡尔曼滤波结合，实现了非线性动力系统的实时参数识别和模型学习。


<details>
  <summary>Details</summary>
Motivation: 从数据中学习控制方程对于理解物理系统行为至关重要，现有方法无法有效处理时变非线性系统的实时学习问题。

Method: 将Sindy算法与卡尔曼滤波结合，将未知系统参数作为状态变量处理，通过前瞻误差增强参数识别策略。

Result: 在参数漂移或切换的混沌Lorenz系统和真实飞行数据构建的稀疏非线性飞机模型上验证了SKF的有效性。

Conclusion: SKF统一了稀疏识别和卡尔曼滤波框架，能够实时推断复杂时变非线性模型，简化了稀疏度、方差参数和切换时刻的估计。

Abstract: Learning governing equations from data is central to understanding the behavior of physical systems across diverse scientific disciplines, including physics, biology, and engineering. The Sindy algorithm has proven effective in leveraging sparsity to identify concise models of nonlinear dynamical systems. In this paper, we extend sparsity-driven approaches to real-time learning by integrating a cornerstone algorithm from control theory -- the Kalman filter (KF). The resulting Sindy Kalman Filter (SKF) unifies both frameworks by treating unknown system parameters as state variables, enabling real-time inference of complex, time-varying nonlinear models unattainable by either method alone. Furthermore, SKF enhances KF parameter identification strategies, particularly via look-ahead error, significantly simplifying the estimation of sparsity levels, variance parameters, and switching instants. We validate SKF on a chaotic Lorenz system with drifting or switching parameters and demonstrate its effectiveness in the real-time identification of a sparse nonlinear aircraft model built from real flight data.

</details>


### [61] [Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss](https://arxiv.org/abs/2511.11181)
*Zhenghao Zhang,Jun Xie,Xingchen Chen,Tao Yu,Hongzhu Yi,Kaixin Xu,Yuanxiang Wang,Tianyu Zong,Xinming Wang,Jiahuan Chen,Guoqing Chao,Feng Chen,Zhepeng Wang,Jungang Xu*

Main category: cs.LG

TL;DR: 提出了一种用于不完整多视图聚类的动态深度图学习方法DGIMVCM，通过构建缺失鲁棒全局图、动态视图特定图结构对比学习、掩码图重建损失和伪标签自监督训练来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多视图数据的普遍存在使得不完整多视图聚类成为重要研究方向。现有基于GNN的方法面临两个主要挑战：(1) 依赖KNN构建静态图会引入噪声并降低图拓扑鲁棒性；(2) 使用MSE损失作为图重建损失会导致优化过程中产生大量梯度噪声。

Method: 1. 从原始数据构建缺失鲁棒全局图，设计图卷积嵌入层提取主要特征和精炼的动态视图特定图结构，利用全局图进行缺失视图补全，并通过图结构对比学习识别视图特定图结构间的一致性。2. 引入图自注意力编码器基于补全后的主要特征和视图特定图提取高层表示，并使用掩码图重建损失进行优化以减少梯度噪声。3. 构建聚类模块并通过伪标签自监督训练机制进行优化。

Result: 在多个数据集上的广泛实验验证了DGIMVCM的有效性和优越性。

Conclusion: DGIMVCM通过动态深度图学习和掩码图重建损失有效解决了不完整多视图聚类中的关键挑战，在多个数据集上表现出优越性能。

Abstract: The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.

</details>


### [62] [LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag](https://arxiv.org/abs/2511.11190)
*Tianlang He,Zhongming Lin,Tianrui Jiang,S. -H. Gary Chan*

Main category: cs.LG

TL;DR: LoRaCompass是一个基于强化学习的模型，用于在未知环境中高效定位LoRa标签，通过空间感知特征提取和策略蒸馏损失函数来应对域偏移和信号波动，在超过80km²的多样环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的定位方法容易受到域偏移和信号波动的影响，导致决策错误累积和定位不准确。需要开发能够在未知环境中稳健高效搜索LoRa标签的方法。

Method: 提出LoRaCompass模型，包含：1）空间感知特征提取器从RSSI学习稳健空间表示；2）策略蒸馏损失函数最大化移动接近标签的概率；3）基于上置信界的探索函数引导传感器以递增置信度接近标签。

Result: 在超过80km²的多样未见环境中验证，LoRaCompass在100米范围内定位标签的成功率超过90%（比现有方法提升40%），搜索路径长度与初始距离呈线性关系，具有高效率和可扩展性。

Conclusion: LoRaCompass通过稳健的空间表示学习和置信度引导的探索，在域偏移和信号波动下实现了高效可靠的LoRa标签定位，为移动传感器在未知环境中的搜索任务提供了有效解决方案。

Abstract: The Long-Range (LoRa) protocol, known for its extensive range and low power, has increasingly been adopted in tags worn by mentally incapacitated persons (MIPs) and others at risk of going missing. We study the sequential decision-making process for a mobile sensor to locate a periodically broadcasting LoRa tag with the fewest moves (hops) in general, unknown environments, guided by the received signal strength indicator (RSSI). While existing methods leverage reinforcement learning for search, they remain vulnerable to domain shift and signal fluctuation, resulting in cascading decision errors that culminate in substantial localization inaccuracies. To bridge this gap, we propose LoRaCompass, a reinforcement learning model designed to achieve robust and efficient search for a LoRa tag. For exploitation under domain shift and signal fluctuation, LoRaCompass learns a robust spatial representation from RSSI to maximize the probability of moving closer to a tag, via a spatially-aware feature extractor and a policy distillation loss function. It further introduces an exploration function inspired by the upper confidence bound (UCB) that guides the sensor toward the tag with increasing confidence. We have validated LoRaCompass in ground-based and drone-assisted scenarios within diverse unseen environments covering an area of over 80km^2. It has demonstrated high success rate (>90%) in locating the tag within 100m proximity (a 40% improvement over existing methods) and high efficiency with a search path length (in hops) that scales linearly with the initial distance.

</details>


### [63] [When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping](https://arxiv.org/abs/2511.11208)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了一种基于生成式AI的零样本合成验证框架，用于联邦学习中的自适应早停，可减少74%训练轮次同时保持性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习通常预设固定训练轮次，导致计算资源浪费（过早达到最优性能）或无效训练（无法达到有意义性能）

Method: 利用生成式AI创建零样本合成验证框架，监控模型性能并确定早停点

Result: 在多标签胸部X光分类任务中，该方法减少74%训练轮次，准确率仅比最优值低1%

Conclusion: 该方法能自适应地在接近最优轮次停止训练，节约计算资源并支持快速超参数调整

Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.

</details>


### [64] [Sparse Methods for Vector Embeddings of TPC Data](https://arxiv.org/abs/2511.11221)
*Tyler Wheeler,Michelle P. Kuchera,Raghuram Ramanujan,Ryan Krupp,Chris Wrede,Saiprasad Ravishankar,Connor L. Cross,Hoi Yan Ian Heung,Andrew J. Jones,Benjamin Votaw*

Main category: cs.LG

TL;DR: 本文探索了在时间投影室(TPC)数据上使用稀疏卷积网络进行表示学习，发现即使权重随机设置的稀疏ResNet架构也能提供有用的事件向量嵌入，通过预训练可进一步提升嵌入质量。


<details>
  <summary>Details</summary>
Motivation: TPC探测器在核物理实验中广泛应用，但传统方法难以有效处理其原始数据。需要开发通用的表示学习方法来提取TPC数据中的丰富事件结构信息。

Method: 将原始pad级信号表示为稀疏张量，使用Minkowski Engine训练稀疏ResNet模型，在GADGET II TPC数据上进行预训练，并跨探测器测试到AT-TPC数据。

Result: 即使未经训练的稀疏ResNet模型也能为AT-TPC数据提供有用嵌入，在GADGET数据上训练后性能进一步提升，揭示了丰富的事件结构。

Conclusion: 稀疏卷积技术有潜力成为TPC实验中通用的表示学习工具，适用于不同类型的探测器数据。

Abstract: Time Projection Chambers (TPCs) are versatile detectors that reconstruct charged-particle tracks in an ionizing medium, enabling sensitive measurements across a wide range of nuclear physics experiments. We explore sparse convolutional networks for representation learning on TPC data, finding that a sparse ResNet architecture, even with randomly set weights, provides useful structured vector embeddings of events. Pre-training this architecture on a simple physics-motivated binary classification task further improves the embedding quality. Using data from the GAseous Detector with GErmanium Tagging (GADGET) II TPC, a detector optimized for measuring low-energy $β$-delayed particle decays, we represent raw pad-level signals as sparse tensors, train Minkowski Engine ResNet models, and probe the resulting event-level embeddings which reveal rich event structure. As a cross-detector test, we embed data from the Active-Target TPC (AT-TPC) -- a detector designed for nuclear reaction studies in inverse kinematics -- using the same encoder. We find that even an untrained sparse ResNet model provides useful embeddings of AT-TPC data, and we observe improvements when the model is trained on GADGET data. Together, these results highlight the potential of sparse convolutional techniques as a general tool for representation learning in diverse TPC experiments.

</details>


### [65] [Neural Network-Powered Finger-Drawn Biometric Authentication](https://arxiv.org/abs/2511.11235)
*Maan Al Balkhi,Kordian Gontarska,Marko Harasic,Adrian Paschke*

Main category: cs.LG

TL;DR: 研究基于神经网络的手指绘制数字生物认证，在触摸屏设备上通过绘制0-9数字进行用户身份验证，比较了CNN和自编码器架构的性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种安全、用户友好的触摸屏生物认证方法，可与现有模式认证结合构建多层安全系统。

Method: 使用20名参与者在个人触摸屏设备上绘制2000个手指数字，比较修改版Inception-V1、轻量级CNN以及卷积和全连接自编码器架构。

Result: 两种CNN架构达到约89%认证准确率，轻量级CNN参数更少；自编码器方法达到约75%准确率。

Conclusion: 手指绘制符号认证为触摸屏设备提供了可行、安全且用户友好的生物认证解决方案。

Abstract: This paper investigates neural network-based biometric authentication using finger-drawn digits on touchscreen devices. We evaluated CNN and autoencoder architectures for user authentication through simple digit patterns (0-9) traced with finger input. Twenty participants contributed 2,000 finger-drawn digits each on personal touchscreen devices. We compared two CNN architectures: a modified Inception-V1 network and a lightweight shallow CNN for mobile environments. Additionally, we examined Convolutional and Fully Connected autoencoders for anomaly detection. Both CNN architectures achieved ~89% authentication accuracy, with the shallow CNN requiring fewer parameters. Autoencoder approaches achieved ~75% accuracy. The results demonstrate that finger-drawn symbol authentication provides a viable, secure, and user-friendly biometric solution for touchscreen devices. This approach can be integrated with existing pattern-based authentication methods to create multi-layered security systems for mobile applications.

</details>


### [66] [Virtual Width Networks](https://arxiv.org/abs/2511.11238)
*Seed,Baisheng Li,Banggu Wu,Bole Ma,Bowen Xiao,Chaoyi Zhang,Cheng Li,Chengyi Wang,Chenyin Xu,Chi Zhang,Chong Hu,Daoguang Zan,Defa Zhu,Dongyu Xu,Du Li,Faming Wu,Fan Xia,Ge Zhang,Guang Shi,Haobin Chen,Hongyu Zhu,Hongzhi Huang,Huan Zhou,Huanzhang Dou,Jianhui Duan,Jianqiao Lu,Jianyu Jiang,Jiayi Xu,Jiecao Chen,Jin Chen,Jin Ma,Jing Su,Jingji Chen,Jun Wang,Jun Yuan,Juncai Liu,Jundong Zhou,Kai Hua,Kai Shen,Kai Xiang,Kaiyuan Chen,Kang Liu,Ke Shen,Liang Xiang,Lin Yan,Lishu Luo,Mengyao Zhang,Ming Ding,Mofan Zhang,Nianning Liang,Peng Li,Penghao Huang,Pengpeng Mu,Qi Huang,Qianli Ma,Qiyang Min,Qiying Yu,Renming Pang,Ru Zhang,Shen Yan,Shen Yan,Shixiong Zhao,Shuaishuai Cao,Shuang Wu,Siyan Chen,Siyu Li,Siyuan Qiao,Tao Sun,Tian Xin,Tiantian Fan,Ting Huang,Ting-Han Fan,Wei Jia,Wenqiang Zhang,Wenxuan Liu,Xiangzhong Wu,Xiaochen Zuo,Xiaoying Jia,Ximing Yang,Xin Liu,Xin Yu,Xingyan Bin,Xintong Hao,Xiongcai Luo,Xujing Li,Xun Zhou,Yanghua Peng,Yangrui Chen,Yi Lin,Yichong Leng,Yinghao Li,Yingshuan Song,Yiyuan Ma,Yong Shan,Yongan Xiang,Yonghui Wu,Yongtao Zhang,Yongzhen Yao,Yu Bao,Yuehang Yang,Yufeng Yuan,Yunshui Li,Yuqiao Xian,Yutao Zeng,Yuxuan Wang,Zehua Hong,Zehua Wang,Zengzhi Wang,Zeyu Yang,Zhengqiang Yin,Zhenyi Lu,Zhexi Zhang,Zhi Chen,Zhi Zhang,Zhiqi Lin,Zihao Huang,Zilin Xu,Ziyun Wei,Zuo Wang*

Main category: cs.LG

TL;DR: Virtual Width Networks (VWN) 框架通过解耦表示宽度和主干网络宽度，在不增加计算成本的情况下扩展表示空间，显著加速语言模型优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过增加隐藏层大小来获得更宽表示，但会带来二次计算成本增长。VWN旨在获得更宽表示的好处，同时避免高昂的计算开销。

Method: VWN将表示宽度与主干网络宽度解耦，在保持主干计算几乎不变的情况下扩展嵌入空间。

Result: 8倍扩展使下一个token预测优化加速2倍以上，下两个token预测加速3倍。随着训练进行，损失差距增大且收敛加速比增加，表明VWN不仅token高效，而且随规模增大效果更明显。

Conclusion: 虚拟宽度与损失减少之间存在近似对数线性缩放关系，为探索虚拟宽度缩放作为大型模型效率的新维度提供了初步实证基础。

Abstract: We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.

</details>


### [67] [HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning](https://arxiv.org/abs/2511.11240)
*Yuhan Xie,Chen Lyu*

Main category: cs.LG

TL;DR: HealSplit是首个专为Split Federated Learning设计的统一防御框架，通过拓扑感知检测、生成式恢复和对抗性多教师蒸馏，有效防御五种复杂的数据投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习防御方法在SFL中效果有限，因为无法访问完整的模型更新。SFL容易受到针对本地特征、标签、粉碎数据和模型权重的复杂数据投毒攻击。

Method: HealSplit包含三个核心组件：1）拓扑感知检测模块，通过构建粉碎数据图来识别中毒样本；2）生成式恢复管道，为检测到的异常生成语义一致的替代数据；3）对抗性多教师蒸馏框架，使用普通教师和异常影响去偏教师的语义监督训练学生模型。

Result: 在四个基准数据集上的广泛实验表明，HealSplit在十种最先进的防御方法中表现最优，在各种攻击场景下实现了卓越的鲁棒性和防御效果。

Conclusion: HealSplit为SFL提供了一个有效的端到端防御解决方案，能够检测和恢复多种复杂的数据投毒攻击，显著提升了SFL系统的安全性。

Abstract: Split Federated Learning (SFL) is an emerging paradigm for privacy-preserving distributed learning. However, it remains vulnerable to sophisticated data poisoning attacks targeting local features, labels, smashed data, and model weights. Existing defenses, primarily adapted from traditional Federated Learning (FL), are less effective under SFL due to limited access to complete model updates. This paper presents HealSplit, the first unified defense framework tailored for SFL, offering end-to-end detection and recovery against five sophisticated types of poisoning attacks. HealSplit comprises three key components: (1) a topology-aware detection module that constructs graphs over smashed data to identify poisoned samples via topological anomaly scoring (TAS); (2) a generative recovery pipeline that synthesizes semantically consistent substitutes for detected anomalies, validated by a consistency validation student; and (3) an adversarial multi-teacher distillation framework trains the student using semantic supervision from a Vanilla Teacher and anomaly-aware signals from an Anomaly-Influence Debiasing (AD) Teacher, guided by the alignment between topological and gradient-based interaction matrices. Extensive experiments on four benchmark datasets demonstrate that HealSplit consistently outperforms ten state-of-the-art defenses, achieving superior robustness and defense effectiveness across diverse attack scenarios.

</details>


### [68] [Heterogeneous Attributed Graph Learning via Neighborhood-Aware Star Kernels](https://arxiv.org/abs/2511.11245)
*Hong Huang,Chengyu Yao,Haiming Chen,Hang Gao*

Main category: cs.LG

TL;DR: 提出了NASK（邻域感知星形核），一种用于属性图学习的新型图核方法，通过Gower相似度系数和Weisfeiler-Lehman迭代来同时捕获异质属性语义和邻域结构信息。


<details>
  <summary>Details</summary>
Motivation: 现有图核方法难以同时捕获属性图中的异质属性语义和邻域信息，而属性图在社交网络、生物信息学等领域普遍存在，需要更有效的相似性度量方法。

Method: 使用Gower相似度系数的指数变换来联合建模数值和分类特征，并通过Weisfeiler-Lehman迭代增强的星形子结构来整合多尺度邻域结构信息。

Result: 在11个属性图和4个大规模真实图基准测试中，NASK始终优于16个最先进的基线方法，包括9个图核和7个图神经网络。

Conclusion: NASK是一个正定图核，能够有效处理属性图的异质特征和结构信息，在多种基准测试中表现出优越性能。

Abstract: Attributed graphs, typically characterized by irregular topologies and a mix of numerical and categorical attributes, are ubiquitous in diverse domains such as social networks, bioinformatics, and cheminformatics. While graph kernels provide a principled framework for measuring graph similarity, existing kernel methods often struggle to simultaneously capture heterogeneous attribute semantics and neighborhood information in attributed graphs. In this work, we propose the Neighborhood-Aware Star Kernel (NASK), a novel graph kernel designed for attributed graph learning. NASK leverages an exponential transformation of the Gower similarity coefficient to jointly model numerical and categorical features efficiently, and employs star substructures enhanced by Weisfeiler-Lehman iterations to integrate multi-scale neighborhood structural information. We theoretically prove that NASK is positive definite, ensuring compatibility with kernel-based learning frameworks such as SVMs. Extensive experiments are conducted on eleven attributed and four large-scale real-world graph benchmarks. The results demonstrate that NASK consistently achieves superior performance over sixteen state-of-the-art baselines, including nine graph kernels and seven Graph Neural Networks.

</details>


### [69] [Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria](https://arxiv.org/abs/2511.11293)
*Jiheum Park,Chao Pang,Tristan Y. Lee,Jeong Yun Yang,Jacob Berkowitz,Alexander Z. Wei,Nicholas Tatonetti*

Main category: cs.LG

TL;DR: 基于电子健康记录（EHR）的预测模型在识别癌症高风险人群方面显著优于传统风险因素，能够实现3-6倍的真实癌症病例富集，为更精确和可扩展的早期检测策略提供支持。


<details>
  <summary>Details</summary>
Motivation: 当前癌症筛查指南仅覆盖少数癌症类型，且依赖年龄或单一风险因素等狭窄标准来识别高风险个体。EHR记录的大规模纵向患者健康信息可能通过检测癌症的微妙前诊断信号，为识别高风险群体提供更有效的工具。

Method: 使用All of Us研究项目中超过865,000名参与者的EHR、基因组和调查数据，系统评估基于EHR的预测模型与包括基因突变和癌症家族史在内的传统风险因素在识别八种主要癌症高风险个体方面的临床效用。

Result: 即使使用基线建模方法，基于EHR的模型在识别为高风险个体中真实癌症病例的富集度比单独使用传统风险因素高3-6倍。EHR基础模型进一步提高了对26种癌症类型的预测性能。

Conclusion: 基于EHR的预测建模具有临床潜力，能够支持更精确和可扩展的早期检测策略，显著优于当前筛查指南中使用的传统风险因素。

Abstract: Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.

</details>


### [70] [Fast and Expressive Multi-Token Prediction with Probabilistic Circuits](https://arxiv.org/abs/2511.11346)
*Andreas Grivas,Lorenzo Loconte,Emile van Krieken,Piotr Nawrot,Yu Zhao,Euan Wielewski,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: MTPC框架通过概率电路探索多令牌预测中表达力与延迟的权衡，显著加速字节级LLM生成，同时保持原始验证器性能。


<details>
  <summary>Details</summary>
Motivation: 现有MTP方法在加速LLM生成时牺牲了表达力，假设未来令牌独立。本研究旨在平衡表达力与延迟的权衡。

Method: 提出MTPC框架，使用概率电路编码未来令牌的联合分布，支持不同电路架构如混合模型、隐马尔可夫模型和张量网络。

Result: 实验显示MTPC结合推测解码显著加速生成，相比独立假设的MTP方法，同时保证验证器LLM性能不变。

Conclusion: MTPC在表达力与延迟间找到最优权衡，通过探索不同参数化如PC架构和部分层共享实现高效生成加速。

Abstract: Multi-token prediction (MTP) is a prominent strategy to significantly speed up generation in large language models (LLMs), including byte-level LLMs, which are tokeniser-free but prohibitively slow. However, existing MTP methods often sacrifice expressiveness by assuming independence between future tokens. In this work, we investigate the trade-off between expressiveness and latency in MTP within the framework of probabilistic circuits (PCs). Our framework, named MTPC, allows one to explore different ways to encode the joint distributions over future tokens by selecting different circuit architectures, generalising classical models such as (hierarchical) mixture models, hidden Markov models and tensor networks. We show the efficacy of MTPC by retrofitting existing byte-level LLMs, such as EvaByte. Our experiments show that, when combined with speculative decoding, MTPC significantly speeds up generation compared to MTP with independence assumptions, while guaranteeing to retain the performance of the original verifier LLM. We also rigorously study the optimal trade-off between expressiveness and latency when exploring the possible parameterisations of MTPC, such as PC architectures and partial layer sharing between the verifier and draft LLMs.

</details>


### [71] [Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials](https://arxiv.org/abs/2511.11361)
*Guangyi Dong,Zhihui Wang*

Main category: cs.LG

TL;DR: 开发了一个多保真度机器学习力场框架，用于锂离子电池正极材料，通过同时利用低保真度非磁性和高保真度磁性计算数据集来提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池正极材料的机器学习力场开发和应用相对有限，主要由于正极材料复杂的电子结构特性和高质量计算数据集的稀缺。

Method: 构建多保真度机器学习力场框架，能够同时利用低保真度非磁性和高保真度磁性计算数据集进行训练。

Result: 在锂锰铁磷酸盐正极材料系统上的测试证明了这种多保真度方法的有效性。

Conclusion: 该工作有助于以较低的训练数据集成本实现正极材料的高精度机器学习力场训练，并为将机器学习力场应用于正极材料计算模拟提供了新视角。

Abstract: Machine learning force fields (MLFFs), which employ neural networks to map atomic structures to system energies, effectively combine the high accuracy of first-principles calculation with the computational efficiency of empirical force fields. They are widely used in computational materials simulations. However, the development and application of MLFFs for lithium-ion battery cathode materials remain relatively limited. This is primarily due to the complex electronic structure characteristics of cathode materials and the resulting scarcity of high-quality computational datasets available for force field training. In this work, we develop a multi-fidelity machine learning force field framework to enhance the data efficiency of computational results, which can simultaneously utilize both low-fidelity non-magnetic and high-fidelity magnetic computational datasets of cathode materials for training. Tests conducted on the lithium manganese iron phosphate (LMFP) cathode material system demonstrate the effectiveness of this multi-fidelity approach. This work helps to achieve high-accuracy MLFF training for cathode materials at a lower training dataset cost, and offers new perspectives for applying MLFFs to computational simulations of cathode materials.

</details>


### [72] [On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization](https://arxiv.org/abs/2511.11362)
*Prabodh Katti,Sangwoo Park,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: MeZO（内存高效零阶优化）通过仅使用前向评估估计梯度，消除了存储中间激活和优化器状态的需求，使边缘设备能够在内存约束下微调更大模型，但需要更长的训练时间。


<details>
  <summary>Details</summary>
Motivation: 边缘AI系统需要在严格内存约束下支持不同智能任务的适应，传统基于反向传播的训练需要存储层激活和优化器状态，限制了可部署的最大模型规模。

Method: 使用MeZO（内存高效零阶优化）方法，仅通过前向评估来估计梯度，无需存储中间激活或优化器状态，从而显著减少内存占用。

Result: 理论分析和数值验证表明，在设备内存约束下，MeZO能够容纳比传统反向传播方法更大的模型，并且在有足够训练时间的情况下表现出精度优势。

Conclusion: MeZO为边缘设备上的模型微调提供了可行的解决方案，通过牺牲训练时间来换取内存效率，使得在资源受限环境中部署更大模型成为可能。

Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.

</details>


### [73] [When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering](https://arxiv.org/abs/2511.11380)
*Jiangkai Long,Yanran Zhu,Chang Tang,Kun Sun,Yuanyuan Liu,Xuesong Yan*

Main category: cs.LG

TL;DR: SemST是一个语义引导的空间转录组学数据聚类框架，利用大语言模型将基因符号转化为生物语义嵌入，并与图神经网络捕获的空间关系融合，通过细粒度语义调制模块动态注入高阶生物知识。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型将基因视为孤立数值特征，忽略了基因符号中丰富的生物语义信息，这阻碍了对关键生物特征的深入理解。

Method: 使用大语言模型将每个组织位点的基因集合转化为生物语义嵌入，与图神经网络捕获的空间邻域关系融合，并通过细粒度语义调制模块学习位点特定的仿射变换来校准空间特征。

Result: 在公共空间转录组学数据集上的广泛实验表明，SemST实现了最先进的聚类性能，且细粒度语义调制模块具有即插即用的通用性，能持续提升其他基线方法的性能。

Conclusion: SemST通过融合生物语义和空间结构，为空间转录组学数据分析提供了更深入的生物学理解，其细粒度语义调制模块具有广泛的适用性。

Abstract: Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to "speak" through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.

</details>


### [74] [Robust inverse material design with physical guarantees using the Voigt-Reuss Net](https://arxiv.org/abs/2511.11388)
*Sanath Keshav,Felix Fritzen*

Main category: cs.LG

TL;DR: 提出了一种具有严格物理保证的机械均质化谱归一化代理模型，通过Voigt-Reuss边界构建对称正半定表示，实现准确的正向预测和约束一致的逆向设计。


<details>
  <summary>Details</summary>
Motivation: 传统均质化方法在物理约束保证和逆向设计方面存在局限性，需要开发既能准确预测又能确保物理可行性的统一框架。

Method: 利用Voigt-Reuss边界，通过类Cholesky分解学习特征值在[0,1]内的对称正半定表示；结合谱归一化和可微分渲染器，使用CNN处理微观结构图像。

Result: 在3D线性弹性中达到近完美保真度（R²≥0.998），张量级相对Frobenius误差中值≈1.7%；在2D平面应变中所有分量R²>0.99，能准确跟踪渗流引起的特征值跳跃。

Conclusion: Voigt-Reuss网络统一了准确的正向预测和大批量约束一致的逆向设计，适用于椭圆算子和耦合物理场景。

Abstract: We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees. Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the Löwner sense. In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $>\!7.5\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs. Tensor-level relative Frobenius errors have median $\approx 1.7\%$ and mean $\approx 3.4\%$ across splits. For 2D plane strain on thresholded trigonometric microstructures, coupling spectral normalization with a differentiable renderer and a CNN yields $R^2>0.99$ on all components, subpercent normalized losses, accurate tracking of percolation-induced eigenvalue jumps, and robust generalization to out-of-distribution images. Treating the parametric microstructure as design variables, batched first-order optimization with a single surrogate matches target tensors within a few percent and returns diverse near-optimal designs. Overall, the Voigt-Reuss net unifies accurate, physically admissible forward prediction with large-batch, constraint-consistent inverse design, and is generic to elliptic operators and coupled-physics settings.

</details>


### [75] [SPOT: Single-Shot Positioning via Trainable Near-Field Rainbow Beamforming](https://arxiv.org/abs/2511.11391)
*Yeyue Cai,Jianhua Mo,Meixia Tao*

Main category: cs.LG

TL;DR: 提出一种端到端深度学习方案，同时设计彩虹波束并估计用户位置，通过将移相器和真时延系数作为可训练变量来合成任务导向波束，最大化定位精度。


<details>
  <summary>Details</summary>
Motivation: 相位时间阵列结合移相器和真时延器已成为宽带感知和定位中生成频率相关彩虹波束的成本效益架构，需要同时设计波束和估计位置。

Method: 将PS和TTD系数作为可训练变量，网络合成任务导向波束；轻量级全连接模块从用户反馈的最大量化接收功率及其对应子载波索引中恢复用户的角距坐标。

Result: 与现有分析和学习方案相比，该方法将开销降低一个数量级，并持续提供更低的二维定位误差。

Conclusion: 所提出的端到端深度学习方案在单次下行传输后即可实现高效的用户定位，显著降低了开销并提高了定位精度。

Abstract: Phase-time arrays, which integrate phase shifters (PSs) and true-time delays (TTDs), have emerged as a cost-effective architecture for generating frequency-dependent rainbow beams in wideband sensing and localization. This paper proposes an end-to-end deep learning-based scheme that simultaneously designs the rainbow beams and estimates user positions. Treating the PS and TTD coefficients as trainable variables allows the network to synthesize task-oriented beams that maximize localization accuracy. A lightweight fully connected module then recovers the user's angle-range coordinates from its feedback of the maximum quantized received power and its corresponding subcarrier index after a single downlink transmission. Compared with existing analytical and learning-based schemes, the proposed method reduces overhead by an order of magnitude and delivers consistently lower two-dimensional positioning error.

</details>


### [76] [Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning](https://arxiv.org/abs/2511.11402)
*Amit Jain,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.LG

TL;DR: 提出基于Transformer的强化学习框架，统一多阶段航天器轨迹优化，通过单一策略架构处理动态不同的任务阶段，无需手动阶段切换。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法需要为不同任务阶段分别训练策略，限制了自适应性和增加了操作复杂性。需要能够跨动态不同阶段保持连贯记忆的统一策略。

Method: 基于近端策略优化(PPO)，用Transformer编码器-解码器结构替换传统循环网络，集成门控Transformer-XL(GTrXL)架构，消除手动阶段转换。

Result: 在单阶段基准测试中达到接近最优性能，在多阶段航点导航变体中有效学习，在复杂多阶段火箭上升问题中保持控制决策稳定性。

Conclusion: Transformer框架不仅匹配简单情况下的解析解，还能跨动态不同阶段学习连贯控制策略，为可扩展自主任务规划奠定基础，减少对阶段特定控制器的依赖。

Abstract: Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual astrodynamics tasks, existing approaches often require separate policies for distinct mission phases, limiting adaptability and increasing operational complexity. This work introduces a transformer-based RL framework that unifies multi-phase trajectory optimization through a single policy architecture, leveraging the transformer's inherent capacity to model extended temporal contexts. Building on proximal policy optimization (PPO), our framework replaces conventional recurrent networks with a transformer encoder-decoder structure, enabling the agent to maintain coherent memory across mission phases spanning seconds to minutes during critical operations. By integrating a Gated Transformer-XL (GTrXL) architecture, the framework eliminates manual phase transitions while maintaining stability in control decisions. We validate our approach progressively: first demonstrating near-optimal performance on single-phase benchmarks (double integrator and Van der Pol oscillator), then extending to multiphase waypoint navigation variants, and finally tackling a complex multiphase rocket ascent problem that includes atmospheric flight, stage separation, and vacuum operations. Results demonstrate that the transformer-based framework not only matches analytical solutions in simple cases but also effectively learns coherent control policies across dynamically distinct regimes, establishing a foundation for scalable autonomous mission planning that reduces reliance on phase-specific controllers while maintaining compatibility with safety-critical verification protocols.

</details>


### [77] [Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization](https://arxiv.org/abs/2511.11415)
*Nikolas Borrel-Jensen,Josiah Bjorgaard*

Main category: cs.LG

TL;DR: 本文展示了使用可微分编程解决声学逆问题的实用方法，包括边界导纳估计和共振阻尼的形状优化，通过JAX-FEM的自动微分和PyTorch3D的网格操作实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 解决声学逆问题通常需要手动推导伴随方程，过程复杂且容易出错。本文旨在利用现代可微分软件栈简化这一过程，实现快速原型设计和优化。

Method: 使用JAX-FEM进行自动微分实现边界导纳估计，结合PyTorch3D进行网格操作，采用随机有限差分法进行形状优化，将物理驱动的边界优化与几何驱动的内部网格适应分离。

Result: 边界导纳估计达到3位数精度，形状优化在目标频率下实现48.1%的能量减少，相比标准有限差分法减少了30倍的FEM求解次数。

Conclusion: 现代可微分软件栈能够快速原型化基于物理的逆问题优化工作流，自动微分适用于参数估计，有限差分与自动微分结合适用于几何设计。

Abstract: We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.

</details>


### [78] [Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching](https://arxiv.org/abs/2511.11418)
*Dara Varam,Diaa A. Abuhani,Imran Zualkernan,Raghad AlDamani,Lujain Khalil*

Main category: cs.LG

TL;DR: 本文提出了一种基于最优传输(OT)的流匹配生成模型后训练量化方法，能够在2-3位精度下保持生成质量，优于传统的均匀、分段和对数量化方案。


<details>
  <summary>Details</summary>
Motivation: 流匹配(FM)生成模型虽然提供高效的无仿真训练和确定性采样，但实际部署面临高精度参数要求的挑战，需要有效的压缩方法以适应边缘和嵌入式AI应用。

Method: 采用基于最优传输的后训练量化方法，最小化量化权重与原始权重之间的2-Wasserstein距离，并与均匀量化、分段量化和对数量化方案进行系统比较。

Result: 在五个复杂度不同的基准数据集上的实验结果表明，基于OT的量化在2-3位参数精度下仍能保持视觉生成质量和潜在空间稳定性，而其他方法在此精度下失效。

Conclusion: 基于OT的量化是压缩流匹配生成模型的一种原理性有效方法，适用于边缘和嵌入式AI应用。

Abstract: Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.

</details>


### [79] [Retrofit: Continual Learning with Bounded Forgetting for Security Applications](https://arxiv.org/abs/2511.11439)
*Yiling He,Junchi Lei,Hongyu She,Shuo Shao,Xinran Zheng,Yiping Liu,Zhan Qin,Lorenzo Cavallaro*

Main category: cs.LG

TL;DR: RETROFIT是一种无需历史数据的持续学习方法，通过参数级合并和知识仲裁机制，在安全分析任务中实现有界遗忘和有效知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在安全关键场景下面临两个耦合挑战：无旧数据时如何保留先验知识，以及如何以最小干扰集成新知识。传统方法依赖全量重训练或数据回放，在数据敏感环境中不可行。

Method: RETROFIT通过合并先前训练和新微调的模型作为新旧知识的教师，进行参数级合并而无需历史数据。采用低秩和稀疏更新将参数变化限制在独立子空间，知识仲裁机制根据模型置信度动态平衡教师贡献。

Result: 在时间漂移的恶意软件检测中，RETROFIT将保留分数从20.2%提升至38.6%，超过CL基线，并在新数据上超过oracle上界。在二进制反编译摘要任务中，BLEU分数达到先前工作中迁移学习方法的两倍，在跨表示泛化方面超越所有基线。

Conclusion: RETROFIT在无需历史数据的情况下，通过参数合并和知识仲裁机制，在安全分析任务中有效缓解遗忘并保持适应性，为数据敏感环境中的持续学习提供了可行解决方案。

Abstract: Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.
  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.

</details>


### [80] [DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference](https://arxiv.org/abs/2511.11446)
*Farhana Amin,Sabiha Afroz,Kanchon Gharami,Mona Moghadampanah,Dimitrios S. Nikolopoulos*

Main category: cs.LG

TL;DR: DiffPro是一个后训练框架，通过联合优化时间步长和逐层精度来加速Diffusion Transformers推理，无需重新训练即可实现6.25倍模型压缩、50%时间步减少和2.8倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量图像，但由于需要大量去噪步骤和繁重的矩阵运算，推理成本高昂。需要在不影响质量的前提下提高推理效率。

Method: 结合三个组件：基于流形感知的敏感度指标分配权重位数、动态激活量化稳定跨时间步的激活值、基于师生漂移的预算时间步选择器。

Result: 在标准基准测试中实现Delta FID <= 10，达到6.25倍模型压缩、50%时间步减少和2.8倍推理加速。

Conclusion: DiffPro将步长减少和精度规划统一为单一可部署计划，为实时节能扩散推理提供了实用解决方案。

Abstract: Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.

</details>


### [81] [FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression](https://arxiv.org/abs/2511.11459)
*Xiaoyin Xi,Zhe Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于密度估计的FairReweighing预处理算法，用于解决回归任务中的公平性问题，通过互信息度量分离违规，并在合成和真实数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: AI软件在公共部门和工业应用中日益普及，但缺乏透明度引发了对公平性的担忧。现有研究主要关注二元分类任务的公平性，而回归任务中的公平性研究相对不足。

Method: 采用基于互信息的度量来评估分离违规，并扩展到分类和回归问题。提出基于密度估计的FairReweighing预处理算法，确保学习模型满足分离准则。

Result: 理论证明在数据独立性假设下，FairReweighing算法能保证训练数据的分离。实证表明，在合成和真实数据上，该算法在提高分离性的同时保持高准确率，优于现有最先进的回归公平性解决方案。

Conclusion: FairReweighing算法有效解决了回归任务中的公平性问题，在保持模型性能的同时显著提升了公平性，为AI软件的公平性提供了新的解决方案。

Abstract: There has been a prevalence of applying AI software in both high-stakes public-sector and industrial contexts. However, the lack of transparency has raised concerns about whether these data-informed AI software decisions secure fairness against people of all racial, gender, or age groups. Despite extensive research on emerging fairness-aware AI software, up to now most efforts to solve this issue have been dedicated to binary classification tasks. Fairness in regression is relatively underexplored. In this work, we adopted a mutual information-based metric to assess separation violations. The metric is also extended so that it can be directly applied to both classification and regression problems with both binary and continuous sensitive attributes. Inspired by the Reweighing algorithm in fair classification, we proposed a FairReweighing pre-processing algorithm based on density estimation to ensure that the learned model satisfies the separation criterion. Theoretically, we show that the proposed FairReweighing algorithm can guarantee separation in the training data under a data independence assumption. Empirically, on both synthetic and real-world data, we show that FairReweighing outperforms existing state-of-the-art regression fairness solutions in terms of improving separation while maintaining high accuracy.

</details>


### [82] [Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies](https://arxiv.org/abs/2511.11461)
*Riku Green,Huw Day,Zahraa S. Abdallah,Telmo M. Silva Filho*

Main category: cs.LG

TL;DR: 重新审视多步预测中递归策略和直接策略的偏差-方差权衡传统观点，通过理论分解和实验验证，发现非线性模型中递归策略可能同时具有更低偏差和更高方差。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为递归策略具有高偏差低方差，直接策略具有低偏差高方差，但这一直觉在非线性预测器中可能不成立，需要重新审视多步预测的偏差-方差分解。

Method: 将多步预测误差分解为不可约噪声、结构近似差距和估计方差三部分，分析线性与非线性预测器的差异，并通过多层感知机在ETTm1数据集上进行实验验证。

Result: 对于线性预测器，结构差距恒为零；对于非线性预测器，递归组合能增加模型表达能力，结构差距取决于模型和数据。递归策略的估计方差可表示为一步方差乘以基于雅可比矩阵的放大因子。

Conclusion: 选择递归或直接策略应基于模型非线性和噪声特性，而非传统偏差-方差直觉，为多步预测策略选择提供实用指导。

Abstract: Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.

</details>


### [83] [MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture](https://arxiv.org/abs/2511.11462)
*Kevin Chen,Kenneth W. Parker,Anish Arora*

Main category: cs.LG

TL;DR: 提出了一种基于纯机器学习的从运动捕捉数据合成雷达频谱图的方法，使用基于Transformer的模型将MoCap数据转换为多普勒雷达频谱图，具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决雷达数据稀缺问题，利用更丰富的MoCap数据来增强雷达数据集训练，同时相比基于物理的方法需要更少的计算资源。

Method: 将MoCap到频谱图转换建模为窗口序列到序列任务，使用Transformer模型联合捕捉MoCap标记之间的空间关系和跨帧的时间动态。

Result: 实验表明该方法能够生成视觉和数值上合理的多普勒雷达频谱图，并具有良好的泛化能力。消融实验证明模型具备将多部位运动转换为多普勒特征的能力，并理解人体不同部位之间的空间关系。

Conclusion: 这是使用Transformer进行时间序列信号处理的有趣示例，特别适用于边缘计算和物联网雷达，能够利用丰富的MoCap数据增强稀缺的雷达数据集，计算量远小于基于物理的方法。

Abstract: We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.
  The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.

</details>


### [84] [Quantifying and Improving Adaptivity in Conformal Prediction through Input Transformations](https://arxiv.org/abs/2511.11472)
*Sooyong Jang,Insup Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新的自适应评估指标和算法，用于改进共形预测中的自适应性能评估和预测集构建。


<details>
  <summary>Details</summary>
Motivation: 现有自适应评估方法存在分桶不平衡问题，导致覆盖率和集合大小估计不准确，需要更可靠的自适应评估指标。

Method: 使用输入变换对样本按难度排序，然后进行均匀质量分桶，并基于此提出两个新评估指标。同时提出分组条件共形预测算法，按估计难度分组并应用组特定阈值。

Result: 新指标与期望的自适应属性相关性更强，在ImageNet图像分类和医疗视觉敏锐度预测任务上，新方法在自适应性能上优于现有方法。

Conclusion: 提出的分桶方法和评估指标能更准确地评估自适应性能，分组条件共形预测算法能有效提升预测集的自适应性。

Abstract: Conformal prediction constructs a set of labels instead of a single point prediction, while providing a probabilistic coverage guarantee. Beyond the coverage guarantee, adaptiveness to example difficulty is an important property. It means that the method should produce larger prediction sets for more difficult examples, and smaller ones for easier examples. Existing evaluation methods for adaptiveness typically analyze coverage rate violation or average set size across bins of examples grouped by difficulty. However, these approaches often suffer from imbalanced binning, which can lead to inaccurate estimates of coverage or set size. To address this issue, we propose a binning method that leverages input transformations to sort examples by difficulty, followed by uniform-mass binning. Building on this binning, we introduce two metrics to better evaluate adaptiveness. These metrics provide more reliable estimates of coverage rate violation and average set size due to balanced binning, leading to more accurate adaptivity assessment. Through experiments, we demonstrate that our proposed metric correlates more strongly with the desired adaptiveness property compared to existing ones. Furthermore, motivated by our findings, we propose a new adaptive prediction set algorithm that groups examples by estimated difficulty and applies group-conditional conformal prediction. This allows us to determine appropriate thresholds for each group. Experimental results on both (a) an Image Classification (ImageNet) (b) a medical task (visual acuity prediction) show that our method outperforms existing approaches according to the new metrics.

</details>


### [85] [Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys](https://arxiv.org/abs/2511.11485)
*Alinda Ezgi Gerçek,Till Korten,Paul Chekhonin,Maleeha Hassan,Peter Steinbach*

Main category: cs.LG

TL;DR: 提出了一种数据高效的轻量级U-Net分割管道，仅使用10张标注SEM图像即可实现反应堆压力容器钢中碳化物的高精度分割，Dice系数达0.98，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 反应堆压力容器钢中碳化物析出物既增强合金强度又可能引发裂纹，但由于SEM图像中碳化物与基体的灰度值重叠，简单阈值分割效果不佳。

Method: 使用轻量级U-Net（3070万参数）构建数据高效分割管道，仅需10张标注的扫描电子显微镜图像进行训练。

Result: 模型在有限数据下达到Dice-Sørensen系数0.98，显著优于传统图像分析方法（0.85），同时将标注工作量比最先进的数据高效分割模型减少一个数量级。

Conclusion: 该方法可实现快速自动化的碳化物量化，适用于合金设计，并能推广到其他钢种，展示了数据高效深度学习在反应堆压力容器钢分析中的潜力。

Abstract: Understanding reactor-pressure-vessel steel microstructure is crucial for predicting mechanical properties, as carbide precipitates both strengthen the alloy and can initiate cracks. In scanning electron microscopy images, gray-value overlap between carbides and matrix makes simple thresholding ineffective. We present a data-efficient segmentation pipeline using a lightweight U-Net (30.7~M parameters) trained on just \textbf{10 annotated scanning electron microscopy images}. Despite limited data, our model achieves a \textbf{Dice-Sørensen coefficient of 0.98}, significantly outperforming the state-of-the-art in the field of metallurgy (classical image analysis: 0.85), while reducing annotation effort by one order of magnitude compared to the state-of-the-art data efficient segmentation model. This approach enables rapid, automated carbide quantification for alloy design and generalizes to other steel types, demonstrating the potential of data-efficient deep learning in reactor-pressure-vessel steel analysis.

</details>


### [86] [Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models](https://arxiv.org/abs/2511.11490)
*Joan Font-Quer Roset,Devina Mohan,Anna Scaife*

Main category: cs.LG

TL;DR: 使用基于分数的扩散模型估计Radio Galaxy Zoo数据集的固有维度，发现分布外源具有更高的固有维度值，RGZ的整体固有维度超过自然图像数据集，并分析了与FR形态类别和信噪比的关系。


<details>
  <summary>Details</summary>
Motivation: 研究RGZ数据集的固有维度特性，探索其与分布外检测、形态分类和信噪比的关系，为未来自监督学习算法提供定量分析基础。

Method: 使用基于分数的扩散模型估计RGZ数据集的固有维度，分析其与BNN能量分数、FR形态类别和信噪比的关系。

Result: 分布外源具有更高的固有维度值；RGZ整体固有维度超过自然图像数据集；FR I和FR II类别间无显著关系；信噪比与固有维度呈弱负相关趋势。

Conclusion: RGZ数据集的固有维度特性可用于定量研究自监督学习算法的表示学习效果，能量分数与固有维度的关系为未来研究提供了有用工具。

Abstract: In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.

</details>


### [87] [Honesty over Accuracy: Trustworthy Language Models through Reinforced Hesitation](https://arxiv.org/abs/2511.11500)
*Mohamad Amin Mohamadi,Tianhao Wang,Zhiyuan Li*

Main category: cs.LG

TL;DR: 论文提出强化犹豫(RH)方法，通过三元奖励机制(+1正确、0弃权、-λ错误)训练模型学会在不确定时弃权，并引入级联推理策略来利用弃权信号，提升模型的可信度。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型缺乏在不确定时弃权的能力，即使面对严重后果也会产生自信的幻觉。现有训练机制鼓励回答而非弃权，这限制了模型的可信度。

Method: 提出强化犹豫(RH)方法，修改RLVR使用三元奖励机制；引入级联推理和自级联推理策略，利用训练得到的弃权能力作为协调信号。

Result: 在GSM8K、MedQA和GPQA上的评估显示，前沿模型几乎从不弃权；RH方法通过调整λ参数可产生帕累托前沿上的不同模型；级联推理策略在降低计算成本的同时优于多数投票。

Conclusion: 将弃权作为首要训练目标，使"我不知道"从失败转变为协调信号，让模型能够通过对其局限性的校准诚实来赢得信任。

Abstract: Modern language models fail a fundamental requirement of trustworthy intelligence: knowing when not to answer. Despite achieving impressive accuracy on benchmarks, these models produce confident hallucinations, even when wrong answers carry catastrophic consequences. Our evaluations on GSM8K, MedQA and GPQA show frontier models almost never abstain despite explicit warnings of severe penalties, suggesting that prompts cannot override training that rewards any answer over no answer. As a remedy, we propose Reinforced Hesitation (RH): a modification to Reinforcement Learning from Verifiable Rewards (RLVR) to use ternary rewards (+1 correct, 0 abstention, -$λ$ error) instead of binary. Controlled experiments on logic puzzles reveal that varying $λ$ produces distinct models along a Pareto frontier, where each training penalty yields the optimal model for its corresponding risk regime: low penalties produce aggressive answerers, high penalties conservative abstainers. We then introduce two inference strategies that exploit trained abstention as a coordination signal: cascading routes queries through models with decreasing risk tolerance, while self-cascading re-queries the same model on abstention. Both outperform majority voting with lower computational cost. These results establish abstention as a first-class training objective that transforms ``I don't know'' from failure into a coordination signal, enabling models to earn trust through calibrated honesty about their limits.

</details>


### [88] [FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models](https://arxiv.org/abs/2511.11505)
*Yonatan Dukler,Guihong Li,Deval Shah,Vikram Appia,Emad Barsoum*

Main category: cs.LG

TL;DR: FarSkip-Collective通过修改模型架构，在MoE模型中跳过连接以实现计算与通信的重叠，在16B到109B参数的大模型中保持准确性的同时显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在分布式环境中阻塞通信导致的效率瓶颈问题，通过架构修改实现计算与通信的重叠。

Method: 修改现代模型架构，引入跳过连接机制，通过自蒸馏方法转换现有大模型，并实现通信与计算的显式重叠。

Result: 成功转换了16B到109B参数的SOTA模型，Llama 4 Scout（109B）在广泛下游评估中平均准确率仅比原始版本低1%，同时显著加速训练和推理。

Conclusion: FarSkip-Collective方法有效解决了MoE模型的通信瓶颈，在保持模型能力的同时实现了显著的性能提升，适用于大规模SOTA模型。

Abstract: Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.

</details>


### [89] [Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications](https://arxiv.org/abs/2511.11539)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 本文研究了多组公平聚类问题，证明了当组数超过两个时该问题是NP难的，并提出了近似算法来解决这一挑战。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理包含多个受保护属性（如年龄、种族、性别等）的数据时，往往无法为边缘化群体提供公平表示。现有研究仅限于两个组的情况，而实际数据通常涉及多个组。

Method: 将最接近公平聚类问题推广到任意数量组的情况，证明了多组情况下的NP难性，并设计了近线性时间的近似算法来处理任意大小的多组问题。

Result: 成功解决了Chakraborty等人提出的开放性问题，为多组公平聚类提供了高效近似算法，并改进了公平相关聚类和公平共识聚类问题的近似保证。

Conclusion: 本研究填补了多组公平聚类领域的空白，为处理现实世界中复杂的多属性公平性问题提供了有效的计算工具。

Abstract: Clustering is a fundamental task in machine learning and data analysis, but it frequently fails to provide fair representation for various marginalized communities defined by multiple protected attributes -- a shortcoming often caused by biases in the training data. As a result, there is a growing need to enhance the fairness of clustering outcomes, ideally by making minimal modifications, possibly as a post-processing step after conventional clustering. Recently, Chakraborty et al. [COLT'25] initiated the study of \emph{closest fair clustering}, though in a restricted scenario where data points belong to only two groups. In practice, however, data points are typically characterized by many groups, reflecting diverse protected attributes such as age, ethnicity, gender, etc.
  In this work, we generalize the study of the \emph{closest fair clustering} problem to settings with an arbitrary number (more than two) of groups. We begin by showing that the problem is NP-hard even when all groups are of equal size -- a stark contrast with the two-group case, for which an exact algorithm exists. Next, we propose near-linear time approximation algorithms that efficiently handle arbitrary-sized multiple groups, thereby answering an open question posed by Chakraborty et al. [COLT'25].
  Leveraging our closest fair clustering algorithms, we further achieve improved approximation guarantees for the \emph{fair correlation clustering} problem, advancing the state-of-the-art results established by Ahmadian et al. [AISTATS'20] and Ahmadi et al. [2020]. Additionally, we are the first to provide approximation algorithms for the \emph{fair consensus clustering} problem involving multiple (more than two) groups, thus addressing another open direction highlighted by Chakraborty et al. [COLT'25].

</details>


### [90] [A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication](https://arxiv.org/abs/2511.11560)
*Angelo Rodio,Giovanni Neglia,Zheng Chen,Erik G. Larsson*

Main category: cs.LG

TL;DR: 本文比较了半去中心化联邦学习中的两种策略：采样设备间共享（S2S）和向所有设备广播（S2A），分析了它们在数据异构性、采样率和网络连接等参数下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 半去中心化联邦学习中，设备主要依赖设备间通信，但偶尔与中心服务器交互。目前缺乏对S2S和S2A这两种策略的严格理论和实证比较。

Method: 建立统一的收敛分析框架，考虑采样率、服务器聚合频率和网络连接等关键系统参数，进行理论分析和实验验证。

Result: 分析结果表明，根据设备间数据异构程度的不同，两种策略各有优劣，存在明显的性能差异区域。

Conclusion: 研究结果为实际半去中心化联邦学习部署提供了具体的设计指导原则，帮助根据数据异构性选择最优策略。

Abstract: In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.

</details>


### [91] [Optimizing Mixture of Block Attention](https://arxiv.org/abs/2511.11571)
*Guangxuan Xiao,Junxian Guo,Kasra Mazaheri,Song Han*

Main category: cs.LG

TL;DR: MoBA通过稀疏注意力机制降低长上下文处理的计算成本，但缺乏理论理解和高效GPU实现。本文开发统计模型分析MoBA机制，发现性能取决于路由器准确区分相关块的能力，并提出改进方法：使用更小块大小和键上的短卷积。同时开发FlashMoBA CUDA内核实现高效执行。


<details>
  <summary>Details</summary>
Motivation: MoBA作为高效处理长上下文的构建块，其设计原则未被充分理解，且缺乏高效GPU实现，阻碍了实际应用。

Method: 开发统计模型分析MoBA机制，推导信噪比连接架构参数与检索精度；提出使用更小块大小和键上的短卷积来改进；开发FlashMoBA CUDA内核实现高效执行。

Result: 改进的MoBA模型在从头训练的LLM中达到密集注意力基线的性能；FlashMoBA在小块情况下比FlashAttention-2快14.7倍。

Conclusion: 通过理论分析和硬件感知实现，MoBA可以在保持性能的同时显著提升效率，使理论上改进的方法具有实用性。

Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [92] [Neural Local Wasserstein Regression](https://arxiv.org/abs/2511.10824)
*Inga Girshfeld,Xiaohui Chen*

Main category: stat.ML

TL;DR: 提出Neural Local Wasserstein Regression框架，用于分布对分布的回归问题，通过局部定义的Wasserstein空间传输映射来建模非线性分布关系


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局最优传输映射或切空间线性化，在近似能力和多元底层域几何保持方面存在限制

Method: 基于2-Wasserstein距离的核权重局部化估计器，使用神经网络参数化传输算子，结合DeepSets架构和Sinkhorn近似损失

Result: 在Gaussian和混合模型合成实验以及MNIST分布预测任务中，有效捕捉了现有方法难以处理的非线性高维分布关系

Conclusion: 局部化视角扩展了可容许变换类别，避免了全局映射假设和线性化结构的限制，为分布回归提供了灵活的非参数框架

Abstract: We study the estimation problem of distribution-on-distribution regression, where both predictors and responses are probability measures. Existing approaches typically rely on a global optimal transport map or tangent-space linearization, which can be restrictive in approximation capacity and distort geometry in multivariate underlying domains. In this paper, we propose the \emph{Neural Local Wasserstein Regression}, a flexible nonparametric framework that models regression through locally defined transport maps in Wasserstein space. Our method builds on the analogy with classical kernel regression: kernel weights based on the 2-Wasserstein distance localize estimators around reference measures, while neural networks parameterize transport operators that adapt flexibly to complex data geometries. This localized perspective broadens the class of admissible transformations and avoids the limitations of global map assumptions and linearization structures. We develop a practical training procedure using DeepSets-style architectures and Sinkhorn-approximated losses, combined with a greedy reference selection strategy for scalability. Through synthetic experiments on Gaussian and mixture models, as well as distributional prediction tasks on MNIST, we demonstrate that our approach effectively captures nonlinear and high-dimensional distributional relationships that elude existing methods.

</details>


### [93] [Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled Data](https://arxiv.org/abs/2511.10919)
*Jialei Liu,Jun Liao,Kuangnan Fang*

Main category: stat.ML

TL;DR: 提出了一种新颖的迁移学习框架，通过模型平均整合来自不同数据源（完全标记、半监督和PU数据）的信息，以解决PU学习中的数据稀缺和隐私约束问题。


<details>
  <summary>Details</summary>
Motivation: PU学习在欺诈检测和医疗诊断等高风险领域面临独特挑战，主要由于缺乏明确标记的负样本，以及数据稀缺和隐私约束的限制。

Method: 为每种源域类型构建定制化的逻辑回归模型，通过模型平均将知识迁移到PU目标域，使用交叉验证准则确定最优权重以最小化KL散度。

Result: 广泛的模拟和真实信用风险数据分析表明，该方法在预测准确性和鲁棒性方面优于其他比较方法，特别是在有限标记数据和异构环境下。

Conclusion: 该框架为PU学习提供了有效的解决方案，具有理论保证，并在实际应用中表现出优越性能。

Abstract: Positive-Unlabeled (PU) learning presents unique challenges due to the lack of explicitly labeled negative samples, particularly in high-stakes domains such as fraud detection and medical diagnosis. To address data scarcity and privacy constraints, we propose a novel transfer learning with model averaging framework that integrates information from heterogeneous data sources - including fully binary labeled, semi-supervised, and PU data sets - without direct data sharing. For each source domain type, a tailored logistic regression model is conducted, and knowledge is transferred to the PU target domain through model averaging. Optimal weights for combining source models are determined via a cross-validation criterion that minimizes the Kullback-Leibler divergence. We establish theoretical guarantees for weight optimality and convergence, covering both misspecified and correctly specified target models, with further extensions to high-dimensional settings using sparsity-penalized estimators. Extensive simulations and real-world credit risk data analyses demonstrate that our method outperforms other comparative methods in terms of predictive accuracy and robustness, especially under limited labeled data and heterogeneous environments.

</details>


### [94] [Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths](https://arxiv.org/abs/2511.11161)
*Yuzhen Zhao,Yating Liu,Marc Hoffmann*

Main category: stat.ML

TL;DR: 本文提出基于高频离散观测的扩散过程漂移函数非参数估计方法，使用神经网络估计器并推导非渐近收敛速率，在数值实验中显示优于B样条方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决时间齐次扩散过程漂移函数的非参数估计问题，特别是在高维设置下传统方法难以有效捕捉局部特征。

Method: 基于N个独立轨迹的高频离散观测，提出神经网络估计器，收敛速率分解为训练误差、逼近误差和扩散相关项。

Result: 对于复合漂移函数建立了显式收敛速率，数值实验显示经验收敛速率与输入维度d无关，神经网络估计器比B样条方法收敛更快且能更好捕捉局部特征。

Conclusion: 神经网络估计器在高维扩散过程漂移函数估计中具有优势，特别是在捕捉局部特征方面优于传统B样条方法。

Abstract: This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings.

</details>


### [95] [Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint](https://arxiv.org/abs/2511.11294)
*Bertille Tierny,Arthur Charpentier,François Hu*

Main category: stat.ML

TL;DR: 提出了一个后处理框架，用于分解线性模型中的预测偏差，将其分为直接（敏感属性）和间接（相关特征）成分，从而透明地解释公平性干预如何影响模型系数。


<details>
  <summary>Details</summary>
Motivation: 线性模型在高风险决策中广泛使用，但当引入公平性约束时，其对模型系数的影响不透明。现有方法依赖强假设或忽视敏感属性的明确作用，限制了公平性评估的实用性。

Method: 扩展了先前工作，提出一个后处理框架，可在任何线性模型上应用，分析表征人口统计均等如何重塑每个模型系数，包括敏感和非敏感特征。

Result: 在合成和真实数据集上的实验表明，该方法能捕捉到先前工作遗漏的公平性动态，为线性模型的负责任部署提供实用且可解释的工具。

Conclusion: 该框架无需重新训练，为模型审计和缓解提供了可操作的见解，实现了对公平性干预的透明、特征级解释。

Abstract: Linear models are widely used in high-stakes decision-making due to their simplicity and interpretability. Yet when fairness constraints such as demographic parity are introduced, their effects on model coefficients, and thus on how predictive bias is distributed across features, remain opaque. Existing approaches on linear models often rely on strong and unrealistic assumptions, or overlook the explicit role of the sensitive attribute, limiting their practical utility for fairness assessment. We extend the work of (Chzhen and Schreuder, 2022) and (Fukuchi and Sakuma, 2023) by proposing a post-processing framework that can be applied on top of any linear model to decompose the resulting bias into direct (sensitive-attribute) and indirect (correlated-features) components. Our method analytically characterizes how demographic parity reshapes each model coefficient, including those of both sensitive and non-sensitive features. This enables a transparent, feature-level interpretation of fairness interventions and reveals how bias may persist or shift through correlated variables. Our framework requires no retraining and provides actionable insights for model auditing and mitigation. Experiments on both synthetic and real-world datasets demonstrate that our method captures fairness dynamics missed by prior work, offering a practical and interpretable tool for responsible deployment of linear models.

</details>
