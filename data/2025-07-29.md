<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 23]
- [cs.LG](#cs.LG) [Total: 138]
- [stat.ML](#stat.ML) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Multipath Interference Suppression in Indirect Time-of-Flight Imaging via a Novel Compressed Sensing Framework](https://arxiv.org/abs/2507.19546)
*Yansong Du,Yutong Deng,Yuting Zhou,Feiyu Jiao,Bangyao Wang,Zhancong Xu,Zhaoxiang Jiang,Xun Guan*

Main category: eess.SP

TL;DR: 提出了一种新型压缩感知方法，通过单调制频率和多相位偏移改进iToF系统的深度重建精度和多目标分离能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖硬件修改或复杂调制，而新方法无需额外硬件即可提升性能。

Method: 利用多相位偏移和窄占空比连续波构建感知矩阵，并通过K-Means聚类优化稀疏恢复。

Result: 实验表明，该方法在重建精度和鲁棒性上优于传统方法。

Conclusion: 新方法无需硬件改动即可显著提升iToF系统的性能。

Abstract: We propose a novel compressed sensing method to improve the depth
reconstruction accuracy and multi-target separation capability of indirect
Time-of-Flight (iToF) systems. Unlike traditional approaches that rely on
hardware modifications, complex modulation, or cumbersome data-driven
reconstruction, our method operates with a single modulation frequency and
constructs the sensing matrix using multiple phase shifts and narrow-duty-cycle
continuous waves. During matrix construction, we further account for pixel-wise
range variation caused by lens distortion, making the sensing matrix better
aligned with actual modulation response characteristics. To enhance sparse
recovery, we apply K-Means clustering to the distance response dictionary and
constrain atom selection within each cluster during the OMP process, which
effectively reduces the search space and improves solution stability.
Experimental results demonstrate that the proposed method outperforms
traditional approaches in both reconstruction accuracy and robustness, without
requiring any additional hardware changes.

</details>


### [2] [Real-Time Distributed Optical Fiber Vibration Recognition via Extreme Lightweight Model and Cross-Domain Distillation](https://arxiv.org/abs/2507.20587)
*Zhongyao Luo,Hao Wu,Zhao Ge,Ming Tang*

Main category: eess.SP

TL;DR: 提出了一种基于FPGA加速的超轻量模型和知识蒸馏框架的分布式光纤振动传感系统，解决了动态条件下识别精度下降和实时处理大数据量的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 解决分布式光纤振动传感系统在动态条件下识别精度下降和实时处理大数据量的挑战。

Method: 采用三层深度可分离卷积网络（仅4141参数）和基于物理先验的跨域知识蒸馏框架。

Result: 实现每样本0.019 ms的处理速度，支持168.68 km光纤的实时处理；识别精度从51.93%提升至95.72%。

Conclusion: 该方法为分布式光纤传感领域提供了实时处理和边缘计算的参考架构，并揭示了构建更高效、鲁棒和可解释AI系统的新方向。

Abstract: Distributed optical fiber vibration sensing (DVS) systems offer a promising
solution for large-scale monitoring and intrusion event recognition. However,
their practical deployment remains hindered by two major challenges:
degradation of recognition accuracy in dynamic conditions, and the
computational bottleneck of real-time processing for mass sensing data. This
paper presents a new solution to these challenges, through a FPGA-accelerated
extreme lightweight model along with a newly proposed knowledge distillation
framework. The proposed three-layer depthwise separable convolution network
contains only 4141 parameters, which is the most compact architecture in this
field to date, and achieves a maximum processing speed of 0.019 ms for each
sample covering a 12.5 m fiber length over 0.256 s. This performance
corresponds to real-time processing capabilities for sensing fibers extending
up to 168.68 km. To improve generalizability under changing environments, the
proposed cross-domain distillation framework guided by physical priors is used
here to embed frequency-domain insights into the time-domain model. This allows
for time-frequency representation learning without increasing complexity and
boosts recognition accuracy from 51.93% to 95.72% under unseen environmental
conditions. The proposed methodology provides key advancements including a
framework combining interpretable signal processing technique with deep
learning and a reference architecture for real-time processing and
edge-computing in DVS systems, and more general distributed optical fiber
sensing (DOFS) area. It mitigates the trade-off between sensing range and
real-time capability, bridging the gap between theoretical capabilities and
practical deployment requirements. Furthermore, this work reveals a new
direction for building more efficient, robust and explainable artificial
intelligence systems for DOFS technologies.

</details>


### [3] [Coverage Probability and Average Rate Analysis of Hybrid Cellular and Cell-free Network](https://arxiv.org/abs/2507.19763)
*Zhuoyin Dai,Jingran Xu,Xiaoli Xu,Ruoguang Li,Yong Zeng,Jiangbin Lyu*

Main category: eess.SP

TL;DR: 本文提出了一种基于随机几何的混合蜂窝和无小区网络模型，分析了信号和干扰的分布及其相互耦合，并推导了覆盖概率和平均可实现速率。


<details>
  <summary>Details</summary>
Motivation: 探讨大规模部署无小区接入点（APs）在现有无线网络中是否能经济高效地实现通信容量增长，以及混合蜂窝和无小区网络（HCCNs）的性能极限。

Method: 采用共轭波束成形设计，利用矩匹配分析聚合信号，并通过推导干扰分量的拉普拉斯变换及其高阶导数来表征覆盖概率。

Result: 揭示了信号和干扰的分布及其耦合关系，并推导了混合网络在信道衰落下的平均可实现速率。

Conclusion: 混合蜂窝和无小区网络是推动无小区网络发展的实用可行方案，其性能分析为未来网络部署提供了理论支持。

Abstract: Cell-free wireless networks deploy distributed access points (APs) to
simultaneously serve user equipments (UEs) across the service region and are
regarded as one of the most promising network architectural paradigms. Despite
recent advances in the performance analysis and optimization of cellfree
wireless networks, it remains an open question whether large-scale deployment
of APs in existing wireless networks can cost-effectively achieve communication
capacity growth. Besides, the realization of a cell-free network is considered
to be a gradual long-term evolutionary process in which cell-free APs will be
incrementally introduced into existing cellular networks, and form a hybrid
communication network with the existing cellular base stations (BSs). Such a
collaboration will bridge the gap between the established cellular network and
the innovative cellfree network. Therefore, hybrid cellular and cell-free
networks (HCCNs) emerge as a practical and feasible solution for advancing
cell-free network development, and it is worthwhile to further explore its
performance limits. This paper presents a stochastic geometry-based hybrid
cellular and cell-free network model to analyze the distributions of signal and
interference and reveal their mutual coupling. Specifically, in order to
benefit the UEs from both the cellular BSs and the cell-free APs, a conjugate
beamforming design is employed, and the aggregated signal is analyzed using
moment matching. Then, the coverage probability of the hybrid network is
characterized by deriving the Laplace transforms and their higher-order
derivatives of interference components. Furthermore, the average achievable
rate of the hybrid network over channel fading is derived based on the
interference coupling analysis.

</details>


### [4] [Radar and Acoustic Sensor Fusion using a Transformer Encoder for Robust Drone Detection and Classification](https://arxiv.org/abs/2507.19785)
*Gevindu Ganganath,Pasindu Sankalpa,Samal Punsara,Demitha Pasindu,Chamira U. S. Edussooriya,Ranga Rodrigo,Udaya S. K. P. Miriya Thanthrige*

Main category: eess.SP

TL;DR: 提出了一种结合雷达和声学传感的多模态方法，用于无人机的检测和分类，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 无人机广泛应用带来安全问题，需解决无人机检测和分类的挑战，如体积小、低空飞行和环境噪声。

Method: 结合雷达（远距离、全天候）和原始声学信号（减少参数），使用Transformer编码器融合传感器。

Result: 户外实验验证了该方法优于现有技术。

Conclusion: 多模态方法在无人机检测和分类中表现优越。

Abstract: The use of drones in a wide range of applications is steadily increasing.
However, this has also raised critical security concerns such as unauthorized
drone intrusions into restricted zones. Therefore, robust and accurate drone
detection and classification mechanisms are required despite significant
challenges due to small size of drones, low-altitude flight, and environmental
noise. In this letter, we propose a multi-modal approach combining radar and
acoustic sensing for detecting and classifying drones. We employ radar due to
its long-range capabilities, and robustness to different weather conditions. We
utilize raw acoustic signals without converting them to other domains such as
spectrograms or Mel-frequency cepstral coefficients. This enables us to use
fewer number of parameters compared to the stateof-the-art approaches.
Furthermore, we explore the effectiveness of the transformer encoder
architecture in fusing these sensors. Experimental results obtained in outdoor
settings verify the superior performance of the proposed approach compared to
the state-of-the-art methods.

</details>


### [5] [Channel Estimation in Massive MIMO Systems with Orthogonal Delay-Doppler Division Multiplexing](https://arxiv.org/abs/2507.19812)
*Dezhi Wang,Chongwen Huang,Xiaojun Yuan,Sami Muhaidat,Lei Liu,Xiaoming Chen,Zhaoyang Zhang,Chau Yuen,Mérouane Debbah*

Main category: eess.SP

TL;DR: 提出了一种基于MAMP的低复杂度算法，用于大规模MIMO-ODDM系统中的信道估计，显著提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 解决高移动性环境下大规模MIMO-ODDM系统的信道估计问题，尤其是天线阵列庞大和移动性高带来的挑战。

Method: 建立有效信道模型，利用MAMP方法估计多径信道的增益、时延和多普勒效应，并通过离散傅里叶变换估计信道角度。

Result: 数值结果表明，该算法在天线数量趋于无穷时接近贝叶斯最优结果，归一化均方误差比现有算法提高约30%。

Conclusion: 提出的MAMP算法在大规模MIMO-ODDM系统中具有高效性和准确性，适用于高移动性环境。

Abstract: Orthogonal delay-Doppler division multiplexing~(ODDM) modulation has recently
been regarded as a promising technology to provide reliable communications in
high-mobility situations. Accurate and low-complexity channel estimation is one
of the most critical challenges for massive multiple input multiple
output~(MIMO) ODDM systems, mainly due to the extremely large antenna arrays
and high-mobility environments. To overcome these challenges, this paper
addresses the issue of channel estimation in downlink massive MIMO-ODDM systems
and proposes a low-complexity algorithm based on memory approximate message
passing~(MAMP) to estimate the channel state information~(CSI). Specifically,
we first establish the effective channel model of the massive MIMO-ODDM
systems, where the magnitudes of the elements in the equivalent channel vector
follow a Bernoulli-Gaussian distribution. Further, as the number of antennas
grows, the elements in the equivalent coefficient matrix tend to become
completely random. Leveraging these characteristics, we utilize the MAMP method
to determine the gains, delays, and Doppler effects of the multi-path channel,
while the channel angles are estimated through the discrete Fourier transform
method. Finally, numerical results show that the proposed channel estimation
algorithm approaches the Bayesian optimal results when the number of antennas
tends to infinity and improves the channel estimation accuracy by about 30%
compared with the existing algorithms in terms of the normalized mean square
error.

</details>


### [6] [Feature Engineering for Wireless Communications and Networking: Concepts, Methodologies, and Applications](https://arxiv.org/abs/2507.19837)
*Jiacheng Wang,Changyuan Zhao,Zehui Xiong,Tao Xiang,Dusit Niyato,Xianbin Wang,Shiwen Mao,Dong In Kim*

Main category: eess.SP

TL;DR: 本文全面研究了AI驱动的无线通信中的特征工程技术，重点分析了其基本原理、方法及其在低空ISAC网络中的应用，并提出了一种基于生成AI的框架，用于在恶意攻击下重建信号特征谱。


<details>
  <summary>Details</summary>
Motivation: 随着AI在无线通信中的广泛应用，特征工程技术成为将原始数据转化为适合AI模型的结构化表示的关键。本文旨在系统研究这一技术及其在低空ISAC网络中的应用。

Method: 文章首先分析了特征工程的基本原理和方法，随后探讨了其在无线通信系统中的应用，并提出了一个基于生成AI的框架，用于信号特征谱的重建。

Result: 案例研究表明，该框架能有效重建信号谱，平均结构相似性指数提高了4%，支持下游感知和通信应用。

Conclusion: 本文为AI驱动的无线通信中的特征工程提供了系统研究，并通过生成AI框架展示了其在低空ISAC网络中的实际应用价值。

Abstract: AI-enabled wireless communications have attracted tremendous research
interest in recent years, particularly with the rise of novel paradigms such as
low-altitude integrated sensing and communication (ISAC) networks. Within these
systems, feature engineering plays a pivotal role by transforming raw wireless
data into structured representations suitable for AI models. Hence, this paper
offers a comprehensive investigation of feature engineering techniques in
AI-driven wireless communications. Specifically, we begin with a detailed
analysis of fundamental principles and methodologies of feature engineering.
Next, we present its applications in wireless communication systems, with
special emphasis on ISAC networks. Finally, we introduce a generative AI-based
framework, which can reconstruct signal feature spectrum under malicious
attacks in low-altitude ISAC networks. The case study shows that it can
effectively reconstruct the signal spectrum, achieving an average structural
similarity index improvement of 4%, thereby supporting downstream sensing and
communication applications.

</details>


### [7] [Toward Dual-Functional LAWN: Control-Aware System Design for Aerodynamics-Aided UAV Formations](https://arxiv.org/abs/2507.19910)
*Jun Wu,Weijie Yuan,Qingqing Cheng,Haijia Jin*

Main category: eess.SP

TL;DR: 该论文提出了一种基于ATC扩散LMS算法的分布式节能无人机编队框架，用于双功能低空无线网络，优化了能量消耗并提升了控制性能。


<details>
  <summary>Details</summary>
Motivation: 研究低空无线网络中无人机编队的节能系统设计，以提升飞行续航能力并同时完成通信和感知任务。

Method: 利用空气动力学上升效应，提出分布式节能编队框架，采用ATC扩散LMS算法优化位置估计，并通过最大LQR最小化问题确保控制稳定性。

Result: 仿真结果表明，'V'形编队是最节能的配置，所提设计在控制性能上优于基准方案。

Conclusion: 论文提出的方法有效优化了无人机编队的能量消耗，同时满足了通信和感知的双功能需求。

Abstract: Integrated sensing and communication (ISAC) has emerged as a pivotal
technology for advancing low-altitude wireless networks (LAWNs), serving as a
critical enabler for next-generation communication systems. This paper
investigates the system design for energy-saving unmanned aerial vehicle (UAV)
formations in dual-functional LAWNs, where a ground base station (GBS)
simultaneously wirelessly controls multiple UAV formations and performs sensing
tasks. To enhance flight endurance, we exploit the aerodynamic upwash effects
and propose a distributed energy-saving formation framework based on the
adapt-then-combine (ATC) diffusion least mean square (LMS) algorithm.
Specifically, each UAV updates the local position estimate by invoking the LMS
algorithm, followed by refining it through cooperative information exchange
with neighbors. This enables an optimized aerodynamic structure that minimizes
the formation's overall energy consumption. To ensure control stability and
fairness, we formulate a maximum linear quadratic regulator (LQR) minimization
problem, which is subject to both the available power budget and the required
sensing beam pattern gain. To address this non-convex problem, we develop a
two-step approach by first deriving a closed-form expression of LQR as a
function of arbitrary beamformers. Subsequently, an efficient iterative
algorithm that integrates successive convex approximation (SCA) and
semidefinite relaxation (SDR) techniques is proposed to obtain a sub-optimal
dual-functional beamforming solution. Extensive simulation results confirm that
the 'V'-shaped formation is the most energy-efficient configuration and
demonstrate the superiority of our proposed design over benchmark schemes in
improving control performance.

</details>


### [8] [Deep Learning Based Joint Channel Estimation and Positioning for Sparse XL-MIMO OFDM Systems](https://arxiv.org/abs/2507.19936)
*Zhongnian Li,Chao Zheng,Jian Xiao,Ji Wang,Gongpu Wang,Ming Zeng,Octavia A. Dobre*

Main category: eess.SP

TL;DR: 本文提出了一种基于深度学习的两阶段框架（CP-Mamba），用于近场稀疏超大MIMO-OFDM系统中的联合信道估计与定位，性能优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究近场稀疏超大MIMO-OFDM系统中信道估计与定位的协同增益问题。

Method: 提出两阶段框架：定位阶段预测用户坐标，信道估计阶段利用坐标提升准确性；采用U形Mamba架构（CP-Mamba）捕获信道局部空间特征和长程时间依赖性。

Result: 数值模拟显示，CP-Mamba架构的两阶段方法性能优于基线方法；稀疏阵列在信道估计和定位精度上显著优于紧凑阵列。

Conclusion: CP-Mamba框架有效提升了联合信道估计与定位的精度，稀疏阵列表现更优。

Abstract: This paper investigates joint channel estimation and positioning in
near-field sparse extra-large multiple-input multiple-output (XL-MIMO)
orthogonal frequency division multiplexing (OFDM) systems. To achieve
cooperative gains between channel estimation and positioning, we propose a deep
learning-based two-stage framework comprising positioning and channel
estimation. In the positioning stage, the user's coordinates are predicted and
utilized in the channel estimation stage, thereby enhancing the accuracy of
channel estimation. Within this framework, we propose a U-shaped Mamba
architecture for channel estimation and positioning, termed as CP-Mamba. This
network integrates the strengths of the Mamba model with the structural
advantages of U-shaped convolutional networks, enabling effective capture of
local spatial features and long-range temporal dependencies of the channel.
Numerical simulation results demonstrate that the proposed two-stage approach
with CP-Mamba architecture outperforms existing baseline methods. Moreover,
sparse arrays (SA) exhibit significantly superior performance in both channel
estimation and positioning accuracy compared to conventional compact arrays.

</details>


### [9] [Dependability Theory-based Statistical QoS Provisioning of Fluid Antenna Systems](https://arxiv.org/abs/2507.19984)
*Irfan Muhammad,Priyadarshi Mukherjee,Wee Kiat New,Hirley Alves,Ioannis Krikidis,Kai-Kit Wong*

Main category: eess.SP

TL;DR: 本文提出了一种基于可靠性理论的框架，用于在有限块长度（FBL）约束下为流体天线系统（FAS）提供统计服务质量（QoS）保障。通过推导新的闭合表达式和定义关键可靠性指标，研究了FAS在任务关键操作中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽略信道衰落的时变特性及其对任务关键操作的影响，因此需要一种新框架来量化FAS的可靠性。

Method: 推导了N端口FAS在Nakagami-m衰落信道下的闭合表达式，定义了任务可靠性和首次故障平均时间（MTTFF）等指标，并扩展了经典有效容量（EC）概念。

Result: 通过仿真揭示了端口数量、QoS指数、信噪比和任务持续时间之间的关键权衡，为超可靠、低延迟和高能效的工业物联网（IIoT）系统设计提供了见解。

Conclusion: 提出的框架和指标为FAS在任务关键场景中的性能评估和优化提供了理论基础和实用工具。

Abstract: Fluid antenna systems (FAS) have recently emerged as a promising technology
for next-generation wireless networks, offering real-time spatial
reconfiguration to enhance reliability, throughput, and energy efficiency.
Nevertheless, existing studies often overlook the temporal dynamics of channel
fading and their implications for mission-critical operations. In this paper,
we propose a dependability-theoretic framework for statistical
quality-of-service (QoS) provisioning of FAS under finite blocklength (FBL)
constraints. Specifically, we derive new closed-form expressions for the
level-crossing rate (LCR) and average fade duration (AFD) of an $N$-port FAS
over Nakagami-$m$ fading channels. Leveraging these second-order statistics, we
define two key dependability metrics such as mission reliability and mean
time-to-first-failure (MTTFF), to quantify the probability of uninterrupted
operation over a defined mission duration. We further extend the classical
effective capacity (EC) concept to incorporate mission reliability in the FBL
regime, yielding a mission EC (mEC). To capture energy efficiency under bursty
traffic and latency constraints, we also develop the mission effective energy
efficiency (mEEE) metric and formulate its maximization as a non-convex
fractional optimization problem. This problem is then solved via a modified
Dinkelbach's method with an embedded line search. Extensive simulations uncover
critical trade-offs among port count, QoS exponent, signal-to-noise ratio, and
mission duration, offering insights for the design of ultra-reliable,
low-latency, and energy-efficient industrial internet-of-things (IIoT) systems.

</details>


### [10] [DOA Estimation via Optimal Weighted Low-Rank Matrix Completion](https://arxiv.org/abs/2507.19996)
*Saeed Razavikia,Mohammad Bokaei,Arash Amini,Stefano Rini,Carlo Fischione*

Main category: eess.SP

TL;DR: 提出了一种基于加权提升结构低秩矩阵补全的新方法，用于估计非均匀稀疏线性传感器阵列的到达方向（DOA），仅需单次快照样本。


<details>
  <summary>Details</summary>
Motivation: 解决非均匀稀疏阵列在单次快照下DOA估计的挑战，通过加权低秩矩阵补全提升性能。

Method: 1) 提升天线样本形成低秩结构；2) 设计左右权重矩阵反映样本信息量；3) 补全加权提升样本估计无噪均匀阵列输出；4) 从恢复的均匀阵列样本中获取DOA。

Result: 数值评估显示，该方法在低噪声条件下比非加权方法和原子范数最小化方法性能更优，归一化均方误差降低约10 dB。

Conclusion: 提出的加权方法在样本复杂度和性能上均优于现有方法，适用于稀疏阵列DOA估计。

Abstract: This paper presents a novel method for estimating the direction of arrival
(DOA) for a non-uniform and sparse linear sensor array using the weighted
lifted structure low-rank matrix completion. The proposed method uses a single
snapshot sample in which a single array of data is observed. The method is
rooted in a weighted lifted-structured low-rank matrix recovery framework. The
method involves four key steps: (i) lifting the antenna samples to form a
low-rank stature, then (ii) designing left and right weight matrices to reflect
the sample informativeness, (iii) estimating a noise-free uniform array output
through completion of the weighted lifted samples, and (iv) obtaining the DOAs
from the restored uniform linear array samples.
  We study the complexity of steps (i) to (iii) above, where we analyze the
required sample for the array interpolation of step (iii) for DOA estimation.
We demonstrate that the proposed choice of weight matrices achieves a
near-optimal sample complexity. This complexity aligns with the problem's
degree of freedom, equivalent to the number of DOAs adjusted for logarithmic
factors. Numerical evaluations show the proposed method's superiority against
the non-weighted counterpart and atomic norm minimization-based methods.
Notably, our proposed method significantly improves, with approximately a 10 dB
reduction in normalized mean-squared error over the non-weighted method at
low-noise conditions.

</details>


### [11] [NeuroCLIP: A Multimodal Contrastive Learning Method for rTMS-treated Methamphetamine Addiction Analysis](https://arxiv.org/abs/2507.20189)
*Chengkai Wang,Di Wu,Yunsheng Liao,Wenyao Zheng,Ziyi Zeng,Xurong Gao,Hemmings Wu,Zhoule Zhu,Jie Yang,Lihua Zhong,Weiwei Cheng,Yun-Hsuan Chen,Mohamad Sawan*

Main category: eess.SP

TL;DR: NeuroCLIP是一种新型深度学习框架，整合EEG和fNIRS数据，显著提升对甲基苯丙胺依赖者的识别能力，并客观评估rTMS治疗效果。


<details>
  <summary>Details</summary>
Motivation: 甲基苯丙胺依赖的评估和治疗效果常依赖主观报告，存在不确定性。现有神经影像技术（如EEG和fNIRS）因个体局限性和传统特征提取方法，影响生物标志物的可靠性。

Method: 提出NeuroCLIP框架，通过渐进式学习策略整合EEG和fNIRS数据，生成更可靠的生物标志物。

Result: NeuroCLIP显著提升对依赖者和健康对照的区分能力，并能客观评估rTMS治疗后的神经模式变化。生物标志物与心理测量验证的渴求评分强相关。

Conclusion: NeuroCLIP通过多模态数据提供更稳健可靠的生物标志物，为成瘾神经科学研究及临床评估提供有力工具。

Abstract: Methamphetamine dependence poses a significant global health challenge, yet
its assessment and the evaluation of treatments like repetitive transcranial
magnetic stimulation (rTMS) frequently depend on subjective self-reports, which
may introduce uncertainties. While objective neuroimaging modalities such as
electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS)
offer alternatives, their individual limitations and the reliance on
conventional, often hand-crafted, feature extraction can compromise the
reliability of derived biomarkers. To overcome these limitations, we propose
NeuroCLIP, a novel deep learning framework integrating simultaneously recorded
EEG and fNIRS data through a progressive learning strategy. This approach
offers a robust and trustworthy biomarker for methamphetamine addiction.
Validation experiments show that NeuroCLIP significantly improves
discriminative capabilities among the methamphetamine-dependent individuals and
healthy controls compared to models using either EEG or only fNIRS alone.
Furthermore, the proposed framework facilitates objective, brain-based
evaluation of rTMS treatment efficacy, demonstrating measurable shifts in
neural patterns towards healthy control profiles after treatment. Critically,
we establish the trustworthiness of the multimodal data-driven biomarker by
showing its strong correlation with psychometrically validated craving scores.
These findings suggest that biomarker derived from EEG-fNIRS data via NeuroCLIP
offers enhanced robustness and reliability over single-modality approaches,
providing a valuable tool for addiction neuroscience research and potentially
improving clinical assessments.

</details>


### [12] [Information-Preserving CSI Feedback: Invertible Networks with Endogenous Quantization and Channel Error Mitigation](https://arxiv.org/abs/2507.20283)
*Haotian Tian,Lixiang Lian,Jiaqi Cao,Sijie Ji*

Main category: eess.SP

TL;DR: InvCSINet提出了一种基于可逆神经网络（INN）的CSI反馈框架，通过信息保留压缩和重建解决传统方法的信息丢失问题，并整合了自适应量化模块和失真补偿模块。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在CSI反馈中因信息丢失导致重建精度下降，需要一种信息保留的解决方案。

Method: 利用INN的双射特性实现信息保留压缩和重建，并整合自适应量化、失真补偿和信息补偿模块。

Result: 仿真结果表明，InvCSINet在CSI恢复性能和鲁棒性上优于传统方法，且架构轻量。

Conclusion: InvCSINet通过INN和补偿模块有效解决了CSI反馈中的信息丢失问题，提升了性能和实用性。

Abstract: Deep learning has emerged as a promising solution for efficient channel state
information (CSI) feedback in frequency division duplex (FDD) massive MIMO
systems. Conventional deep learning-based methods typically rely on a deep
autoencoder to compress the CSI, which leads to irreversible information loss
and degrades reconstruction accuracy. This paper introduces InvCSINet, an
information-preserving CSI feedback framework based on invertible neural
networks (INNs). By leveraging the bijective nature of INNs, the model ensures
information-preserving compression and reconstruction with shared model
parameters. To address practical challenges such as quantization and
channel-induced errors, we endogenously integrate an adaptive quantization
module, a differentiable bit-channel distortion module and an information
compensation module into the INN architecture. This design enables the network
to learn and compensate the information loss during CSI compression,
quantization, and noisy transmission, thereby preserving the CSI integrity
throughout the feedback process. Simulation results validate the effectiveness
of the proposed scheme, demonstrating superior CSI recovery performance and
robustness to practical impairments with a lightweight architecture.

</details>


### [13] [Reliability of Wi-Fi, LTE, and 5G-Based UAV RC Links in ISM Bands: Uplink Interference Asymmetry Analysis and HARQ Design](https://arxiv.org/abs/2507.20392)
*Donggu Lee,Sung Joon Maeng,Ozgur Ozdemir,Mani Bharathi Pandian,Ismail Guvenc*

Main category: eess.SP

TL;DR: 论文研究了无人机遥控链路在ISM频段中的不对称干扰问题，并通过实验测量和HARQ机制分析，评估了上行链路干扰对下行链路吞吐量的影响。


<details>
  <summary>Details</summary>
Motivation: 无人机遥控链路在ISM频段易受干扰，尤其是上行链路干扰可能影响下行链路性能，需通过实验和机制分析解决这一问题。

Method: 通过测量活动（使用氦气球平台）获取干扰数据，并分析多种HARQ机制（如Type-I、Type-III等）对吞吐量的影响。

Result: 实验显示，高空干扰比地面高16.66 dB，上行链路干扰导致HARQ指标丢失，影响下行链路吞吐量。

Conclusion: 不对称上行链路干扰显著影响无人机遥控链路性能，需优化HARQ机制以提升系统可靠性。

Abstract: Command and control of uncrewed aerial vehicles (UAVs) is often realized
through air-to-ground (A2G) remote control (RC) links that operate in ISM
bands. While wireless fidelity (Wi-Fi) technology is commonly used for UAV RC
links, ISM-based long-term evolution (LTE) and fifth-generation (5G)
technologies have also been recently considered for the same purpose. A major
problem for UAV RC links in the ISM bands is that other types of interference
sources, such as legacy Wi-Fi and Bluetooth transmissions, may degrade the link
quality. Such interference problems are a higher concern for the UAV in the air
than the RC unit on the ground due to the UAV being in line-of-sight (LoS) with
a larger number of interference sources. To obtain empirical evidence of the
asymmetric interference conditions in downlink (DL) and uplink (UL), we first
conducted a measurement campaign using a helikite platform in urban and rural
areas at NC State University. The results from this measurement campaign show
that the aggregate interference can be up to 16.66 dB at higher altitudes up to
170 m, compared with the interference observed at a ground receiver. As a
result of this asymmetric UL interference, lost hybrid automatic repeat request
(HARQ) indicators (ACK/NACK) in the UL may degrade the DL throughput. To
investigate this, we study various HARQ mechanisms, including HARQ Type-I with
no combining, HARQ Type-I with chase combining, HARQ Type-III with incremental
redundancy, and burst transmission with chase combining. To evaluate the impact
of asymmetric UL interference on throughput performance, we consider three
steps of evaluation process: 1) standalone physical DL shared channel (PDSCH)
throughput evaluation with perfect ACK/NACK assumption; 2) standalone physical
UL control channel (PUCCH) decoding reliability evaluation; and 3) PDSCH DL
throughput evaluation with asymmetric UL ACK/NACK transmission.

</details>


### [14] [A Multi-Stage Hybrid CNN-Transformer Network for Automated Pediatric Lung Sound Classification](https://arxiv.org/abs/2507.20408)
*Samiul Based Shuvo,Taufiq Hasan*

Main category: eess.SP

TL;DR: 提出了一种基于CNN-Transformer混合框架的儿科呼吸疾病分类方法，解决了儿童呼吸音分类的不足，并在数据不平衡情况下取得了优于现有模型的效果。


<details>
  <summary>Details</summary>
Motivation: 儿科呼吸音分类研究不足，尤其是6岁以下儿童，其肺部发育变化导致呼吸音特性不同，需要专门的方法。

Method: 采用多阶段混合CNN-Transformer框架，结合CNN提取的特征和基于注意力的架构，对呼吸音进行二元和多类分类。

Result: 模型在二元事件分类中得分0.9039，多类事件分类中得分0.8448，优于现有模型3.81%和5.94%。

Conclusion: 该方法为资源有限地区的儿科呼吸疾病诊断提供了可扩展的解决方案。

Abstract: Automated analysis of lung sound auscultation is essential for monitoring
respiratory health, especially in regions facing a shortage of skilled
healthcare workers. While respiratory sound classification has been widely
studied in adults, its ap plication in pediatric populations, particularly in
children aged <6 years, remains an underexplored area. The developmental
changes in pediatric lungs considerably alter the acoustic proper ties of
respiratory sounds, necessitating specialized classification approaches
tailored to this age group. To address this, we propose a multistage hybrid
CNN-Transformer framework that combines CNN-extracted features with an
attention-based architecture to classify pediatric respiratory diseases using
scalogram images from both full recordings and individual breath events. Our
model achieved an overall score of 0.9039 in binary event classifi cation and
0.8448 in multiclass event classification by employing class-wise focal loss to
address data imbalance. At the recording level, the model attained scores of
0.720 for ternary and 0.571 for multiclass classification. These scores
outperform the previous best models by 3.81% and 5.94%, respectively. This
approach offers a promising solution for scalable pediatric respiratory disease
diagnosis, especially in resource-limited settings.

</details>


### [15] [Energy-Efficient Secure Communications via Joint Optimization of UAV Trajectory and Movable-Antenna Array Beamforming](https://arxiv.org/abs/2507.20489)
*Sanghyeok Kim,Jinu Gong,Joonhyuk Kang*

Main category: eess.SP

TL;DR: 本文提出了一种利用配备可移动天线阵列的无人机优化轨迹和波束成形以提高无线通信系统安全性的框架。


<details>
  <summary>Details</summary>
Motivation: 研究无人机和可移动天线阵列在增强无线通信系统安全性方面的潜力。

Method: 联合优化无人机轨迹和可移动天线阵列的波束成形，以最大化保密能量效率。

Result: 数值结果表明，该方法显著提高了保密能量效率。

Conclusion: 可移动天线架构提供的空间灵活性显著提升了物理层安全性。

Abstract: This paper investigates the potential of unmanned aerial vehicles (UAVs)
equipped with movable-antenna (MA) arrays to strengthen security in wireless
communication systems. We propose a novel framework that jointly optimizes the
UAV trajectory and the reconfigurable beamforming of the MA array to maximize
secrecy energy efficiency, while ensuring reliable communication with
legitimate users. By exploiting the spatial degrees of freedom enabled by the
MA array, the system can form highly directional beams and deep nulls, thereby
significantly improving physical layer security. Numerical results demonstrate
that the proposed approach achieves superior secrecy energy efficiency,
attributed to the enhanced spatial flexibility provided by the movable antenna
architecture.

</details>


### [16] [RFI and Jamming Detection in Antenna Arrays with an LSTM Autoencoder](https://arxiv.org/abs/2507.20648)
*Christos Ntemkas,Antonios Argyriou*

Main category: eess.SP

TL;DR: 论文提出了一种基于天线阵列数据和深度学习的方法，用于检测射频干扰（RFI）和恶意干扰，无需预知干扰信号信息。


<details>
  <summary>Details</summary>
Motivation: 射频干扰和恶意干扰是无线通信中的主要问题，传统方法依赖于统计检测或基于AI的算法，但需要输入基带数据或时频表示。本文旨在利用天线阵列数据，提供更高效的检测方案。

Method: 结合傅里叶成像进行空间源定位，并采用深度LSTM自编码器检测异常（RFI和干扰）。

Result: 在不同功率水平的RFI/干扰和信号条件下，检测器表现出高性能。

Conclusion: 该方法无需预知干扰信号信息，即可实现高效检测，为无线通信安全提供了新思路。

Abstract: Radio frequency interference (RFI) and malicious jammers are a significant
problem in our wireless world. Detecting RFI or jamming is typically performed
with model-based statistical detection or AI-empowered algorithms that use an
input baseband data or time-frequency representations like spectrograms. In
this work we depart from the previous approaches and we leverage data in
antenna array systems. We use Fourier imaging to localize spatially the sources
and then deploy a deep LSTM autoencoder that detects RFI and jamming as
anomalies. Our results for different power levels of the RFI/jamming sources,
and the signal of interest, reveal that our detector offers high performance
without needing any pre-existing knowledge regarding the RFI or jamming signal.

</details>


### [17] [Angle-distance decomposition based on deep learning for active sonar detection](https://arxiv.org/abs/2507.20651)
*Jichao Zhang,Xiao-Lei Zhang,Kunde Yang*

Main category: eess.SP

TL;DR: 提出了一种基于深度学习的主动声纳目标检测方法，通过分解角度和距离估计任务，结合迁移学习和仿真解决数据不足问题，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统信号处理方法在复杂水下环境中因噪声、混响和干扰面临挑战，需更有效的解决方案。

Method: 使用深度学习模型分别预测目标距离和角度，结合迁移学习和仿真解决数据不足问题。

Result: 实验结果表明，该方法在复杂条件下具有有效且稳健的性能。

Conclusion: 基于深度学习的方法在主动声纳目标检测中表现出色，为解决传统方法的局限性提供了新途径。

Abstract: Underwater target detection using active sonar constitutes a critical
research area in marine sciences and engineering. However, traditional signal
processing methods face significant challenges in complex underwater
environments due to noise, reverberation, and interference. To address these
issues, this paper presents a deep learning-based active sonar target detection
method that decomposes the detection process into separate angle and distance
estimation tasks. Active sonar target detection employs deep learning models to
predict target distance and angle, with the final target position determined by
integrating these estimates. Limited underwater acoustic data hinders effective
model training, but transfer learning and simulation offer practical solutions
to this challenge. Experimental results verify that the method achieves
effective and robust performance under challenging conditions.

</details>


### [18] [The micro-Doppler Attack Against AI-based Human Activity Classification from Wireless Signals](https://arxiv.org/abs/2507.20657)
*Margarita Loupa,Antonios Argyriou,Yanwei Liu*

Main category: eess.SP

TL;DR: 论文提出了一种针对基于无线OFDM信号的人类活动分类系统的微多普勒攻击，通过人为改变信号波形降低分类准确性。


<details>
  <summary>Details</summary>
Motivation: 研究针对无线信号的人类活动分类系统的安全性，揭示其潜在的脆弱性。

Method: 通过插入人为变化的OFDM波形，改变其微多普勒特征，影响接收器的频谱图。

Result: 使用深度卷积神经网络的HAC系统准确性可降至10%以下。

Conclusion: 微多普勒攻击对人类活动分类系统构成严重威胁，需加强防御措施。

Abstract: A subset of Human Activity Classification (HAC) systems are based on AI
algorithms that use passively collected wireless signals. This paper presents
the micro-Doppler attack targeting HAC from wireless orthogonal frequency
division multiplexing (OFDM) signals. The attack is executed by inserting
artificial variations in a transmitted OFDM waveform to alter its micro-Doppler
signature when it reflects off a human target. We investigate two variants of
our scheme that manipulate the waveform at different time scales resulting in
altered receiver spectrograms. HAC accuracy with a deep convolutional neural
network (CNN) can be reduced to less than 10%.

</details>


### [19] [A Nonlinear Spectral Approach for Radar-Based Heartbeat Estimation via Autocorrelation of Higher Harmonics](https://arxiv.org/abs/2507.20664)
*Kohei Shimomura,Chi-Hsuan Lee,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 提出一种非线性信号处理方法，通过利用心跳信号的高阶谐波周期性，提高雷达心跳间隔估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常采用选择性频率滤波或跟踪单个谐波，但难以有效抑制呼吸谐波和噪声。

Method: 通过平滑和二阶导数操作抑制噪声并突出高阶谐波，计算谐波频率附近的局部自相关，并通过非相干求和生成伪频谱。

Result: 实验表明，该方法将均方根误差降低20%，相关系数提高0.20。

Conclusion: 非线性方法显著提升了心跳间隔估计的鲁棒性和准确性。

Abstract: This study presents a nonlinear signal processing method for accurate
radar-based heartbeat interval estimation by exploiting the periodicity of
higher-order harmonics inherent in heartbeat signals. Unlike conventional
approaches that employ selective frequency filtering or track individual
harmonics, the proposed method enhances the global periodic structure of the
spectrum via nonlinear correlation processing. Specifically, smoothing and
second-derivative operations are first applied to the radar displacement signal
to suppress noise and accentuate higher-order heartbeat harmonics. Rather than
isolating specific frequency components, we compute localized autocorrelations
of the Fourier spectrum around the harmonic frequencies. The incoherent
summation of these autocorrelations yields a pseudo-spectrum in which the
fundamental heartbeat periodicity is distinctly emphasized. This nonlinear
approach mitigates the effects of respiratory harmonics and noise, enabling
robust interbeat interval estimation. Experiments with radar measurements from
five participants demonstrate that the proposed method reduces root-mean-square
error by 20% and improves the correlation coefficient by 0.20 relative to
conventional techniques.

</details>


### [20] [DT-Aided Resource Management in Spectrum Sharing Integrated Satellite-Terrestrial Networks](https://arxiv.org/abs/2507.20789)
*Hung Nguyen-Kha,Vu Nguyen Ha,Ti Ti Nguyen,Eva Lagunas,Symeon Chatzinotas,Joel Grotz*

Main category: eess.SP

TL;DR: 提出了一种基于数字孪生（DT）的框架，用于集成卫星-地面网络（ISTN），通过联合优化带宽分配、流量引导和资源分配来最小化拥塞。


<details>
  <summary>Details</summary>
Motivation: 解决卫星与地面网络共存时产生的系统间干扰（ISI）和低地球轨道卫星（LSat）移动问题，以提高频谱效率。

Method: 采用时间变化的数字孪生框架，结合3D地图，通过两阶段算法（基于连续凸近似和压缩感知）解决混合整数非线性规划（MINLP）问题。

Result: 数值结果表明，所提方法在队列长度最小化方面优于基准方法。

Conclusion: 该框架为ISTN的资源管理提供了有效解决方案，显著提升了性能。

Abstract: The integrated satellite-terrestrial networks (ISTNs) through spectrum
sharing have emerged as a promising solution to improve spectral efficiency and
meet increasing wireless demand. However, this coexistence introduces
significant challenges, including inter-system interference (ISI) and the low
Earth orbit satellite (LSat) movements. To capture the actual environment for
resource management, we propose a time-varying digital twin (DT)-aided
framework for ISTNs incorporating 3D map that enables joint optimization of
bandwidth (BW) allocation, traffic steering, and resource allocation, and aims
to minimize congestion. The problem is formulated as a mixed-integer nonlinear
programming (MINLP), addressed through a two-phase algorithm based on
successive convex approximation (SCA) and compressed sensing approaches.
Numerical results demonstrate the proposed method's superior performance in
queue length minimization compared to benchmarks.

</details>


### [21] [Chirp-Permuted AFDM: A New Degree of Freedom for Next-Generation Versatile Waveform Design](https://arxiv.org/abs/2507.20825)
*Hyeon Seok Rou,Giuseppe Thadeu Freitas de Abreu*

Main category: eess.SP

TL;DR: 提出了一种新型多载波波形CP-AFDM，在传统AFDM的基础上引入独特的chirp置换域，提升了多普勒域的分辨率和峰值旁瓣比，适用于6G应用。


<details>
  <summary>Details</summary>
Motivation: 为满足6G对可靠性和感知能力的需求，提出一种改进的多载波波形，以增强性能。

Method: 在传统AFDM的chirp子载波上引入chirp置换域，分析信号模型和波形特性，并通过数值模拟验证。

Result: CP-AFDM保留了AFDM的核心特性，同时在多普勒域提升了分辨率和峰值旁瓣比，并展示了两种多功能应用。

Conclusion: CP-AFDM是一种适用于6G的高性能波形，兼具可靠性和感知能力，并支持多种应用场景。

Abstract: We present a novel multicarrier waveform, termed chirp-permuted affine
frequency division multiplexing (CP-AFDM), which introduces a unique
chirp-permutation domain on top of the chirp subcarriers of the conventional
AFDM. Rigorous analysis of the signal model and waveform properties, supported
by numerical simulations, demonstrates that the proposed CP-AFDM preserves all
core characteristics of affine frequency division multiplexing (AFDM) -
including robustness to doubly-dispersive channels, peak-to-average power ratio
(PAPR), and full delay-Doppler representation - while further enhancing
ambiguity function resolution and peak-to-sidelobe ratio (PSLR) in the Doppler
domain. These improvements establish CP-AFDM as a highly attractive candidate
for emerging sixth generation (6G) use cases demanding both reliability and
sensing-awareness. Moreover, by exploiting the vast degree of freedom in the
chirp-permutation domain, two exemplary multifunctional applications are
introduced: an index modulation (IM) technique over the permutation domain
which achieves significant spectral efficiency gains, and a physical-layer
security scheme that ensures practically perfect security through
permutation-based keying, without requiring additional transmit energy or
signaling overhead.

</details>


### [22] [Interference Analysis and Successive Interference Cancellation for Multistatic OFDM-based ISAC Systems](https://arxiv.org/abs/2507.20942)
*Taewon Jeong,Lucas Giroto,Umut Utku Erdem,Christian Karle,Jiyeon Choi,Thomas Zwick,Benjamin Nuss*

Main category: eess.SP

TL;DR: 多静态集成感知与通信（ISAC）系统通过分布式收发器提升空间覆盖和感知精度，但面临共存节点间的干扰问题。本文分析了干扰影响，提出了一种低复杂度干扰消除方法，并通过仿真和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多静态ISAC系统在共存节点间存在干扰，影响感知和通信性能，需研究干扰分类及消除方法。

Method: 分类干扰类型，提出基于单静态雷达图像信干噪比（SINR）的低复杂度干扰消除方法，并通过仿真和实验验证。

Result: 提出的方法降低了误码率（BER），改善了误差向量幅度（EVM）和雷达图像SINR。

Conclusion: 低复杂度干扰消除方法适用于实际应用，能有效提升多静态ISAC系统的性能。

Abstract: Multistatic integrated sensing and communications (ISAC) systems, which use
distributed transmitters and receivers, offer enhanced spatial coverage and
sensing accuracy compared to stand-alone ISAC configurations. However, these
systems face challenges due to interference between co-existing ISAC nodes,
especially during simultaneous operation. In this paper, we analyze the impact
of this mutual interference arising from the co-existence in a multistatic ISAC
scenario, where a mono- and a bistatic ISAC system share the same spectral
resources. We first classify differenct types of interference in the power
domain. Then, we discuss how the interference can affect both sensing and
communications in terms of bit error rate (BER), error vector magnitude (EVM),
and radar image under varied transmit power and RCS configurations through
simulations. Along with interfernce analysis, we propose a low-complexity
successive interference cancellation method that adaptively cancels either the
monostatic reflection or the bistatic line-of-sight signal based on a
monostatic radar image signal-to-interference-plus-noise ratio (SINR). The
proposed framework is evaluated with both simulations and proof-of-concept
measurements using an ISAC testbed with a radar echo generator for object
emulation. The results have shown that the proposed method reduces BER and
improves EVM as well as radar image SINR across a wide range of SINR
conditions. These results demonstrate that accurate component-wise cancellation
can be achieved with low computational overhead, making the method suitable for
practical applications.

</details>


### [23] [Analytical Modeling of Batteryless IoT Sensors Powered by Ambient Energy Harvesting](https://arxiv.org/abs/2507.20952)
*Jimmy Fernandez Landivar,Andrea Zanella,Ihsane Gryech,Sofie Pollin,Hazem Sallouha*

Main category: eess.SP

TL;DR: 提出了一种用于无电池物联网传感器节点的能量动力学数学模型，涵盖能量收集和消耗阶段，并通过实验验证了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 无电池物联网设备依赖环境能量收集，需要精确的数学模型来预测其行为，以优化能量管理。

Method: 开发了一个综合数学模型，捕捉能量收集和消耗，并支持智能电源管理单元。通过原型设备在三种不同光照条件下进行实验验证。

Result: 实验结果显示，分析得到的超级电容器电压曲线与实际测量结果高度一致，验证了模型的准确性。

Conclusion: 该模型能够准确预测无电池物联网设备的能量行为，适用于多种环境条件，为智能电源管理提供了理论支持。

Abstract: This paper presents a comprehensive mathematical model to characterize the
energy dynamics of batteryless IoT sensor nodes powered entirely by ambient
energy harvesting. The model captures both the energy harvesting and
consumption phases, explicitly incorporating power management tasks to enable
precise estimation of device behavior across diverse environmental conditions.
The proposed model is applicable to a wide range of IoT devices and supports
intelligent power management units designed to maximize harvested energy under
fluctuating environmental conditions. We validated our model against a
prototype batteryless IoT node, conducting experiments under three distinct
illumination scenarios. Results show a strong correlation between analytical
and measured supercapacitor voltage profiles, confirming the proposed model's
accuracy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers](https://arxiv.org/abs/2507.19510)
*Haoxuan Ma,Xishun Liao,Yifan Liu,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 论文提出了一种基于Transformer的方法，利用GPS轨迹数据生成轮班工人的完整活动模式，填补了传统交通调查中的代表性不足问题。


<details>
  <summary>Details</summary>
Motivation: 轮班工人占工业化社会劳动力的15-20%，但在传统交通调查和规划中代表性不足，导致对非标准工作时间人群的移动需求理解不足。

Method: 采用基于Transformer的模型，结合周期性时间嵌入和过渡聚焦损失函数，从碎片化GPS数据生成完整的活动模式。

Result: 生成的数据与洛杉矶县的GPS数据分布高度一致（平均JSD < 0.02），验证了方法的有效性。

Conclusion: 该方法为交通规划提供了强大的数据增强工具，有助于更全面、精确地满足城市人口的24/7移动需求。

Abstract: This paper addresses a critical gap in urban mobility modeling by focusing on
shift workers, a population segment comprising 15-20% of the workforce in
industrialized societies yet systematically underrepresented in traditional
transportation surveys and planning. This underrepresentation is revealed in
this study by a comparative analysis of GPS and survey data, highlighting stark
differences between the bimodal temporal patterns of shift workers and the
conventional 9-to-5 schedules recorded in surveys. To address this bias, we
introduce a novel transformer-based approach that leverages fragmented GPS
trajectory data to generate complete, behaviorally valid activity patterns for
individuals working non-standard hours. Our method employs periodaware temporal
embeddings and a transition-focused loss function specifically designed to
capture the unique activity rhythms of shift workers and mitigate the inherent
biases in conventional transportation datasets. Evaluation shows that the
generated data achieves remarkable distributional alignment with GPS data from
Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By
transforming incomplete GPS traces into complete, representative activity
patterns, our approach provides transportation planners with a powerful data
augmentation tool to fill critical gaps in understanding the 24/7 mobility
needs of urban populations, enabling precise and inclusive transportation
planning.

</details>


### [25] [VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets](https://arxiv.org/abs/2507.19844)
*Biswarup Mukherjee,Li Zhou,S. Gokul Krishnan,Milad Kabirifar,Subhash Lakshminarayana,Charalambos Konstantinou*

Main category: cs.LG

TL;DR: 论文提出了一种基于多智能体深度确定性策略梯度（MADDPG）的本地能源市场（LEM）协调模型，并探讨了价格操纵策略对产消者的影响。


<details>
  <summary>Details</summary>
Motivation: 解决异构分布式能源资源（DERs）在动态市场中的协调问题，并研究价格操纵对产消者的财务影响。

Method: 采用数据驱动的无模型强化学习方法（MADDPG）和VAE-GAN模型进行价格操纵分析。

Result: 在对抗性定价下，缺乏发电能力的产消者群体遭受财务损失；市场扩大后，交易稳定性和公平性提高。

Conclusion: MADDPG框架有效协调产消者，但价格操纵策略需警惕；市场规模的扩大有助于合作与公平。

Abstract: This paper introduces a model for coordinating prosumers with heterogeneous
distributed energy resources (DERs), participating in the local energy market
(LEM) that interacts with the market-clearing entity. The proposed LEM scheme
utilizes a data-driven, model-free reinforcement learning approach based on the
multi-agent deep deterministic policy gradient (MADDPG) framework, enabling
prosumers to make real-time decisions on whether to buy, sell, or refrain from
any action while facilitating efficient coordination for optimal energy trading
in a dynamic market. In addition, we investigate a price manipulation strategy
using a variational auto encoder-generative adversarial network (VAE-GAN)
model, which allows utilities to adjust price signals in a way that induces
financial losses for the prosumers. Our results show that under adversarial
pricing, heterogeneous prosumer groups, particularly those lacking generation
capabilities, incur financial losses. The same outcome holds across LEMs of
different sizes. As the market size increases, trading stabilizes and fairness
improves through emergent cooperation among agents.

</details>


### [26] [Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting](https://arxiv.org/abs/2507.19513)
*Khalid Ali,Zineddine Bettouche,Andreas Kassler,Andreas Fischer*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级的双路径时空网络，结合sLSTM和Conv3D模块，显著提升了交通预测性能。


<details>
  <summary>Details</summary>
Motivation: 5G及未来网络中，准确的时空交通预测对智能资源管理至关重要，但传统AI方法难以捕捉复杂的时空模式。

Method: 采用双路径设计，sLSTM用于时间建模，Conv3D模块用于空间特征提取，并通过融合层整合。

Result: 在真实数据集上表现优于ConvLSTM基线，MAE降低23%，模型泛化能力提升30%。

Conclusion: 该设计适合大规模新一代网络部署，具有梯度稳定性和快速收敛的优势。

Abstract: Accurate spatiotemporal traffic forecasting is vital for intelligent resource
management in 5G and beyond. However, conventional AI approaches often fail to
capture the intricate spatial and temporal patterns that exist, due to e.g.,
the mobility of users. We introduce a lightweight, dual-path Spatiotemporal
Network that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling
and a three-layer Conv3D module for spatial feature extraction. A fusion layer
integrates both streams into a cohesive representation, enabling robust
forecasting. Our design improves gradient stability and convergence speed while
reducing prediction error. Evaluations on real-world datasets show superior
forecast performance over ConvLSTM baselines and strong generalization to
unseen regions, making it well-suited for large-scale, next-generation network
deployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,
with a 30% improvement in model generalization.

</details>


### [27] [Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks](https://arxiv.org/abs/2507.19514)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 提出了一种完全基于小波域的学习框架，通过在小波系数上应用可学习的非线性变换，替代传统神经层，实现了高效的模型设计。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络模型（如Transformer）存在计算复杂度高和参数过多的问题，需要一种更高效、可解释的替代方案。

Method: 在小波域中直接应用非线性变换（如软阈值和增益相位调制），并引入可微分的小波基选择机制，支持多种小波族（如Haar、Daubechies）。

Result: 在3D去噪和自然语言任务（如GLUE基准中的SST-2情感分类）上，模型性能接近4层Transformer（89.3% vs 90.1%），但参数减少72%，峰值内存降低58%。

Conclusion: 该方法为视觉和语言任务提供了一种紧凑、高效且可解释的模型设计方向，避免了过度参数化的架构。

Abstract: We introduce a fully spectral learning framework that eliminates traditional
neural layers by operating entirely in the wavelet domain. The model applies
learnable nonlinear transformations, including soft-thresholding and gain-phase
modulation, directly to wavelet coefficients. It also includes a differentiable
wavelet basis selection mechanism, enabling adaptive processing using families
such as Haar, Daubechies, and Biorthogonal wavelets.
  Implemented in PyTorch with full 3D support, the model maintains a spectral
pipeline without spatial convolutions or attention. On synthetic 3D denoising
and natural language tasks from the GLUE benchmark, including SST-2 sentiment
classification, the model achieves 89.3 percent accuracy, close to a 4-layer
Transformer baseline (90.1 percent), while using 72 percent fewer parameters
and 58 percent less peak memory. Faster early convergence is observed due to
spectral sparsity priors.
  In contrast to the quadratic complexity of self-attention and large matrix
multiplications in Transformers, our approach uses linear-time wavelet
transforms and pointwise nonlinearities, significantly reducing inference cost.
This yields a compact, interpretable, and efficient alternative to neural
models. Our results support the viability of principled spectral learning in
both vision and language tasks, offering new directions for model design
without overparameterized architectures.

</details>


### [28] [A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting](https://arxiv.org/abs/2507.19515)
*Edmund F. Agyemang,Hansapani Rodrigo,Vincent Agbenyeavu*

Main category: cs.LG

TL;DR: 该研究比较了传统模型（ARIMA和ETS）与六种深度学习模型（Simple RNN、LSTM、GRU、BiLSTM、BiGRU和Transformer）在预测甲型流感爆发中的表现，发现深度学习模型（尤其是Transformer）显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 甲型流感每年导致大量死亡，研究旨在通过改进预测模型来提升公共卫生干预能力。

Method: 使用2009年至2023年的历史数据，比较传统模型与深度学习模型的预测性能。

Result: 所有深度学习模型均优于传统模型，Transformer表现最佳（测试MSE和MAE分别为0.0433和0.1126）。

Conclusion: 深度学习模型能显著提升传染病预测能力，未来应将其整合到实时预测和监测系统中。

Abstract: Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year,
though this estimate is an improvement from years past due to improvements in
sanitation, healthcare practices, and vaccination programs. In this study, we
perform a comparative analysis of traditional and deep learning models to
predict Influenza A outbreaks. Using historical data from January 2009 to
December 2023, we compared the performance of traditional ARIMA and Exponential
Smoothing(ETS) models with six distinct deep learning architectures: Simple
RNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear
superiority of all the deep learning models, especially the state-of-the-art
Transformer with respective average testing MSE and MAE of 0.0433 \pm 0.0020
and 0.1126 \pm 0.0016 for capturing the temporal complexities associated with
Influenza A data, outperforming well known traditional baseline ARIMA and ETS
models. These findings of this study provide evidence that state-of-the-art
deep learning architectures can enhance predictive modeling for infectious
diseases and indicate a more general trend toward using deep learning methods
to enhance public health forecasting and intervention planning strategies.
Future work should focus on how these models can be incorporated into real-time
forecasting and preparedness systems at an epidemic level, and integrated into
existing surveillance systems.

</details>


### [29] [BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation](https://arxiv.org/abs/2507.19517)
*Mohit Gupta,Debjit Bhowmick,Ben Beck*

Main category: cs.LG

TL;DR: BikeVAE-GNN是一种结合变分自编码器（VAE）和图神经网络（GNN）的双任务框架，用于解决稀疏自行车网络中的自行车流量估计问题，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 城市自行车网络的数据稀疏性使得自行车流量估计具有挑战性，需要一种高效的方法来填补数据空白并提升估计准确性。

Method: 提出BikeVAE-GNN框架，结合Hybrid-GNN（GCN、GAT和GraphSAGE）和VAE，通过生成合成节点和边来丰富图结构，同时进行回归和分类任务。

Result: 在墨尔本市的数据集上，BikeVAE-GNN的MAE为30.82辆/天，准确率和F1-score均达到99%，优于基线模型。

Conclusion: BikeVAE-GNN为稀疏网络中的自行车流量估计提供了创新解决方案，有助于可持续自行车基础设施的规划。

Abstract: Accurate link-level bicycle volume estimation is essential for informed urban
and transport planning but it is challenged by extremely sparse count data in
urban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task
framework augmenting a Hybrid Graph Neural Network (GNN) with Variational
Autoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing
sparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model
intricate spatial relationships in sparse networks while VAE generates
synthetic nodes and edges to enrich the graph structure and enhance the
estimation performance. BikeVAE-GNN simultaneously performs - regression for
bicycling volume estimation and classification for bicycling traffic level
categorization. We demonstrate the effectiveness of BikeVAE-GNN using
OpenStreetMap data and publicly available bicycle count data within the City of
Melbourne - where only 141 of 15,933 road segments have labeled counts
(resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN
outperforms machine learning and baseline GNN models, achieving a mean absolute
error (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99.
Ablation studies further validate the effective role of Hybrid-GNN and VAE
components. Our research advances bicycling volume estimation in sparse
networks using novel and state-of-the-art approaches, providing insights for
sustainable bicycling infrastructures.

</details>


### [30] [Swift-Sarsa: Fast and Robust Linear Control](https://arxiv.org/abs/2507.19539)
*Khurram Javed,Richard S. Sutton*

Main category: cs.LG

TL;DR: SwiftTD算法通过优化步长、限制有效学习率和步长衰减，改进了True Online TD(λ)，并在Atari游戏预测任务中表现优异。本文将其扩展为控制问题的Swift-Sarsa算法，并在操作条件基准测试中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决控制问题中信号噪声干扰和信用分配问题，提升强化学习算法的鲁棒性和性能。

Method: 结合SwiftTD和True Online Sarsa(λ)开发Swift-Sarsa算法，应用于操作条件基准测试，学习区分相关信号与噪声。

Result: Swift-Sarsa在无先验知识下成功分配信用给相关信号，支持大规模并行特征搜索。

Conclusion: Swift-Sarsa为处理噪声信号和大规模特征搜索提供了有效解决方案，具有广泛的应用潜力。

Abstract: Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD
learning -- SwiftTD -- that augments True Online TD($\lambda$) with step-size
optimization, a bound on the effective learning rate, and step-size decay. In
their experiments SwiftTD outperformed True Online TD($\lambda$) and
TD($\lambda$) on a variety of prediction tasks derived from Atari games, and
its performance was robust to the choice of hyper-parameters. In this extended
abstract we extend SwiftTD to work for control problems. We combine the key
ideas behind SwiftTD with True Online Sarsa($\lambda$) to develop an on-policy
reinforcement learning algorithm called $\textit{Swift-Sarsa}$.
  We propose a simple benchmark for linear on-policy control called the
$\textit{operant conditioning benchmark}$. The key challenge in the operant
conditioning benchmark is that a very small subset of input signals are
relevant for decision making. The majority of the signals are noise sampled
from a non-stationary distribution. To learn effectively, the agent must learn
to differentiate between the relevant signals and the noisy signals, and
minimize prediction errors by assigning credit to the weight parameters
associated with the relevant signals.
  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to
assign credit to the relevant signals without any prior knowledge of the
structure of the problem. It opens the door for solution methods that learn
representations by searching over hundreds of millions of features in parallel
without performance degradation due to noisy or bad features.

</details>


### [31] [Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction](https://arxiv.org/abs/2507.19518)
*Sangwoo Seo,Jimin Seo,Yoonho Lee,Donghyeon Kim,Hyejin Shin,Banghyun Sung,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的高效子图匹配方法，用于电子设计自动化（EDA）和电路验证，解决了传统方法的泛化性和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法泛化性差，节点匹配方法计算效率低，现有深度学习方法无法高效捕获全局子图嵌入或依赖低效匹配矩阵。

Method: 利用GNN预测目标电路的高概率区域，构建负样本以准确学习目标电路的存在，并直接从整个电路中提取子图嵌入以捕获全局信息。

Result: 实验表明，该方法在时间效率和目标区域预测上显著优于现有方法。

Conclusion: 该方法为大规模电路中的子图匹配提供了可扩展且高效的解决方案。

Abstract: Subgraph matching plays an important role in electronic design automation
(EDA) and circuit verification. Traditional rule-based methods have limitations
in generalizing to arbitrary target circuits. Furthermore, node-to-node
matching approaches tend to be computationally inefficient, particularly for
large-scale circuits. Deep learning methods have emerged as a potential
solution to address these challenges, but existing models fail to efficiently
capture global subgraph embeddings or rely on inefficient matching matrices,
which limits their effectiveness for large circuits. In this paper, we propose
an efficient graph matching approach that utilizes Graph Neural Networks (GNNs)
to predict regions of high probability for containing the target circuit.
Specifically, we construct various negative samples to enable GNNs to
accurately learn the presence of target circuits and develop an approach to
directly extracting subgraph embeddings from the entire circuit, which captures
global subgraph information and addresses the inefficiency of applying GNNs to
all candidate subgraphs. Extensive experiments demonstrate that our approach
significantly outperforms existing methods in terms of time efficiency and
target region prediction, offering a scalable and effective solution for
subgraph matching in large-scale circuits.

</details>


### [32] [Federated Calculation of the Free-Support Transportation Barycenter by Single-Loop Dual Decomposition](https://arxiv.org/abs/2507.19627)
*Zhengqi Lin,Andrzej Ruszczyński*

Main category: cs.LG

TL;DR: 提出一种高效的联邦对偶分解算法，用于计算多个分布的Wasserstein重心，并选择解的支撑集。算法不访问本地数据，仅使用高度聚合信息，且无需重复解决质量传输问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在计算Wasserstein重心时的高复杂度和可扩展性问题。

Method: 采用联邦对偶分解算法，避免矩阵-向量操作，降低迭代复杂度。

Result: 算法在多个混合模型示例中表现出高效性和可扩展性，优于现有方法。

Conclusion: 该算法为计算Wasserstein重心提供了一种高效且可扩展的解决方案。

Abstract: We propose an efficient federated dual decomposition algorithm for
calculating the Wasserstein barycenter of several distributions, including
choosing the support of the solution. The algorithm does not access local data
and uses only highly aggregated information. It also does not require repeated
solutions to mass transportation problems. Because of the absence of any
matrix-vector operations, the algorithm exhibits a very low complexity of each
iteration and significant scalability. We illustrate its virtues and compare it
to the state-of-the-art methods on several examples of mixture models.

</details>


### [33] [Physics-informed transfer learning for SHM via feature selection](https://arxiv.org/abs/2507.19519)
*J. Poole,P. Gardner,A. J. Hughes,N. Dervilis,R. S. Mills,T. A. Dardeno,K. Worden*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理知识的特征选择方法，利用模态保证准则（MAC）量化结构健康监测（SHM）中不同结构间的相似性，以解决数据分布差异问题。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测（SHM）中训练数据昂贵且难以获取，尤其是标注数据。不同结构间的数据分布差异导致传统机器学习方法难以泛化。

Method: 利用模态保证准则（MAC）选择相似特征，量化健康结构模态间的对应关系，并通过数值和实验案例验证其有效性。

Result: MAC与衡量联合分布相似性的监督指标高度一致，表明其能有效选择条件分布不变的特征。

Conclusion: MAC可作为选择跨域一致性特征的有效度量，提升SHM系统的泛化能力。

Abstract: Data used for training structural health monitoring (SHM) systems are
expensive and often impractical to obtain, particularly labelled data.
Population-based SHM presents a potential solution to this issue by considering
the available data across a population of structures. However, differences
between structures will mean the training and testing distributions will
differ; thus, conventional machine learning methods cannot be expected to
generalise between structures. To address this issue, transfer learning (TL),
can be used to leverage information across related domains. An important
consideration is that the lack of labels in the target domain limits data-based
metrics to quantifying the discrepancy between the marginal distributions.
Thus, a prerequisite for the application of typical unsupervised TL methods is
to identify suitable source structures (domains), and a set of features, for
which the conditional distributions are related to the target structure.
Generally, the selection of domains and features is reliant on domain
expertise; however, for complex mechanisms, such as the influence of damage on
the dynamic response of a structure, this task is not trivial. In this paper,
knowledge of physics is leveraged to select more similar features, the modal
assurance criterion (MAC) is used to quantify the correspondence between the
modes of healthy structures. The MAC is shown to have high correspondence with
a supervised metric that measures joint-distribution similarity, which is the
primary indicator of whether a classifier will generalise between domains. The
MAC is proposed as a measure for selecting a set of features that behave
consistently across domains when subjected to damage, i.e. features with
invariance in the conditional distributions. This approach is demonstrated on
numerical and experimental case studies to verify its effectiveness in various
applications.

</details>


### [34] [Feature learning is decoupled from generalization in high capacity neural networks](https://arxiv.org/abs/2507.19680)
*Niclas Alexander Göring,Charles London,Abdurrahman Hadi Erturk,Chris Mingard,Yoonsoo Nam,Ard A. Louis*

Main category: cs.LG

TL;DR: 神经网络在特征学习能力上优于核方法，但现有理论主要关注特征学习强度而非特征质量。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络为何在某些任务（如阶梯函数）上表现优于核方法，并探讨现有特征学习理论的局限性。

Method: 引入‘特征质量’概念，并通过实证分析评估现有理论。

Result: 现有理论主要衡量特征学习强度，而非特征质量，不足以解释神经网络泛化能力。

Conclusion: 需要发展更全面的理论框架，以更好地理解神经网络的泛化机制。

Abstract: Neural networks outperform kernel methods, sometimes by orders of magnitude,
e.g. on staircase functions. This advantage stems from the ability of neural
networks to learn features, adapting their hidden representations to better
capture the data. We introduce a concept we call feature quality to measure
this performance improvement. We examine existing theories of feature learning
and demonstrate empirically that they primarily assess the strength of feature
learning, rather than the quality of the learned features themselves.
Consequently, current theories of feature learning do not provide a sufficient
foundation for developing theories of neural network generalization.

</details>


### [35] [Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves](https://arxiv.org/abs/2507.19520)
*Ethan Lo,Dan C. Lo*

Main category: cs.LG

TL;DR: 论文探讨了机器学习模型在发现系外行星中的应用，通过简化复杂算法，使用逻辑回归、K近邻和随机森林等模型，结合NASA的开普勒望远镜数据，展示了数据增强技术对提升模型性能的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统手动搜索系外行星效率低下，机器学习能高效处理大量数据并提高准确性，但现有模型依赖复杂算法和超级计算机，本研究旨在简化这一过程。

Method: 研究使用了逻辑回归、K近邻和随机森林等机器学习模型，基于NASA开普勒望远镜的数据进行训练和预测，并采用数据增强技术优化模型性能。

Result: 初步结果显示各模型表现良好，但数据不平衡和潜在偏差需要通过数据增强技术进一步优化，以提升召回率和精确度。

Conclusion: 数据增强技术显著提高了模型的召回率和精确度，但不同模型的准确率表现不一，为系外行星搜索提供了更高效的解决方案。

Abstract: With manual searching processes, the rate at which scientists and astronomers
discover exoplanets is slow because of inefficiencies that require an extensive
time of laborious inspections. In fact, as of now there have been about only
5,000 confirmed exoplanets since the late 1900s. Recently, machine learning
(ML) has proven to be extremely valuable and efficient in various fields,
capable of processing massive amounts of data in addition to increasing its
accuracy by learning. Though ML models for discovering exoplanets owned by
large corporations (e.g. NASA) exist already, they largely depend on complex
algorithms and supercomputers. In an effort to reduce such complexities, in
this paper, we report the results and potential benefits of various, well-known
ML models in the discovery and validation of extrasolar planets. The ML models
that are examined in this study include logistic regression, k-nearest
neighbors, and random forest. The dataset on which the models train and predict
is acquired from NASA's Kepler space telescope. The initial results show
promising scores for each model. However, potential biases and dataset
imbalances necessitate the use of data augmentation techniques to further
ensure fairer predictions and improved generalization. This study concludes
that, in the context of searching for exoplanets, data augmentation techniques
significantly improve the recall and precision, while the accuracy varies for
each model.

</details>


### [36] [RestoreAI -- Pattern-based Risk Estimation Of Remaining Explosives](https://arxiv.org/abs/2507.19873)
*Björn Kischelewski,Benjamin Guedj,David Wahl*

Main category: cs.LG

TL;DR: RestoreAI利用地雷空间模式预测风险，提升排雷效率，线性与曲线模式性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法主要关注地雷识别，而忽略基于空间模式的风险预测，影响排雷效率。

Method: 开发RestoreAI系统，包括线性、曲线和贝叶斯模式排雷器，分别利用PCA、主曲线和专家知识进行风险预测。

Result: RestoreAI显著提升排雷效率，清除率提高14.37%，时间减少24.45%，线性与曲线模式性能无显著差异。

Conclusion: RestoreAI通过模式预测风险有效提升排雷效率，线性模式可作为高效选择。

Abstract: Landmine removal is a slow, resource-intensive process affecting over 60
countries. While AI has been proposed to enhance explosive ordnance (EO)
detection, existing methods primarily focus on object recognition, with limited
attention to prediction of landmine risk based on spatial pattern information.
This work aims to answer the following research question: How can AI be used to
predict landmine risk from landmine patterns to improve clearance time
efficiency? To that effect, we introduce RestoreAI, an AI system for
pattern-based risk estimation of remaining explosives. RestoreAI is the first
AI system that leverages landmine patterns for risk prediction, improving the
accuracy of estimating the residual risk of missing EO prior to land release.
We particularly focus on the implementation of three instances of RestoreAI,
respectively, linear, curved and Bayesian pattern deminers. First, the linear
pattern deminer uses linear landmine patterns from a principal component
analysis (PCA) for the landmine risk prediction. Second, the curved pattern
deminer uses curved landmine patterns from principal curves. Finally, the
Bayesian pattern deminer incorporates prior expert knowledge by using a
Bayesian pattern risk prediction. Evaluated on real-world landmine data,
RestoreAI significantly boosts clearance efficiency. The top-performing
pattern-based deminers achieved a 14.37 percentage point increase in the
average share of cleared landmines per timestep and required 24.45% less time
than the best baseline deminer to locate all landmines. Interestingly, linear
and curved pattern deminers showed no significant performance difference,
suggesting that more efficient linear patterns are a viable option for risk
prediction.

</details>


### [37] [Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations](https://arxiv.org/abs/2507.19522)
*Aarush Gupta,Kendric Hsu,Syna Mathod*

Main category: cs.LG

TL;DR: 论文介绍了物理信息神经网络（PINN）如何通过结合微分方程的先验知识，同时解决正向和逆向问题，并优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂微分方程的正向和逆向问题，同时利用先验知识提高模型在训练集外的性能。

Method: 使用PINN，将微分方程的残差嵌入损失函数中，同时优化神经网络权重和模型参数。

Result: PINN能够高效解决稀疏数据问题，避免过拟合，并适用于线性、二次及复杂微分方程（如热方程）。

Conclusion: PINN是一种强大的工具，能够结合先验知识同时解决正向和逆向问题，适用于多种复杂微分方程。

Abstract: Mathematical models in neural networks are powerful tools for solving complex
differential equations and optimizing their parameters; that is, solving the
forward and inverse problems, respectively. A forward problem predicts the
output of a network for a given input by optimizing weights and biases. An
inverse problem finds equation parameters or coefficients that effectively
model the data. A Physics-Informed Neural Network (PINN) can solve both
problems. PINNs inject prior analytical information about the data into the
cost function to improve model performance outside the training set boundaries.
This also allows PINNs to efficiently solve problems with sparse data without
overfitting by extrapolating the model to fit larger trends in the data. The
prior information we implement is in the form of differential equations.
Residuals are the differences between the left-hand and right-hand sides of
corresponding differential equations; PINNs minimize these residuals to
effectively solve the differential equation and take advantage of prior
knowledge. In this way, the solution and parameters are embedded into the loss
function and optimized, allowing both the weights of the neural network and the
model parameters to be found simultaneously, solving both the forward and
inverse problems in the process. In this paper, we will create PINNs with
residuals of varying complexity, beginning with linear and quadratic models and
then expanding to fit models for the heat equation and other complex
differential equations. We will mainly use Python as the computing language,
using the PyTorch library to aid us in our research.

</details>


### [38] [Dimer-Enhanced Optimization: A First-Order Approach to Escaping Saddle Points in Neural Network Training](https://arxiv.org/abs/2507.19968)
*Yue Hu,Zanxia Cao,Yingchao Liu*

Main category: cs.LG

TL;DR: DEO（Dimer-Enhanced Optimization）是一种新型优化框架，利用物理学启发的Dimer方法估计曲率信息，帮助神经网络训练逃离鞍点和平坦区域。


<details>
  <summary>Details</summary>
Motivation: 传统一阶优化方法（如SGD和Adam）在复杂损失曲面上表现不佳，而二阶方法计算成本过高。DEO旨在通过一阶方法高效估计曲率，提升训练效率。

Method: DEO基于Dimer方法，通过构造两个邻近点估计局部曲率，并周期性将梯度投影到最小曲率方向的正交子空间，避免鞍点和平坦区域。

Result: 在Transformer玩具模型上的初步实验表明，DEO性能与标准一阶方法相当，但能更好地处理复杂损失曲面。

Conclusion: DEO成功将物理学启发的曲率估计技术应用于高维神经网络训练，为一阶优化方法提供了新思路。

Abstract: First-order optimization methods, such as SGD and Adam, are widely used for
training large-scale deep neural networks due to their computational efficiency
and robust performance. However, relying solely on gradient information, these
methods often struggle to navigate complex loss landscapes with flat regions,
plateaus, and saddle points. Second-order methods, which use curvature
information from the Hessian matrix, can address these challenges but are
computationally infeasible for large models. The Dimer method, a first-order
technique that constructs two closely spaced points to probe the local geometry
of a potential energy surface, efficiently estimates curvature using only
gradient information. Inspired by its use in molecular dynamics simulations for
locating saddle points, we propose Dimer-Enhanced Optimization (DEO), a novel
framework to escape saddle points in neural network training. DEO adapts the
Dimer method to explore a broader region of the loss landscape, approximating
the Hessian's smallest eigenvector without computing the full matrix. By
periodically projecting the gradient onto the subspace orthogonal to the
minimum curvature direction, DEO guides the optimizer away from saddle points
and flat regions, enhancing training efficiency with non-stepwise updates.
Preliminary experiments on a Transformer toy model show DEO achieves
competitive performance compared to standard first-order methods, improving
navigation of complex loss landscapes. Our work repurposes physics-inspired,
first-order curvature estimation to enhance neural network training in
high-dimensional spaces.

</details>


### [39] [Language Models for Controllable DNA Sequence Design](https://arxiv.org/abs/2507.19523)
*Xingyu Su,Xiner Li,Yuchao Lin,Ziqian Xie,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: ATGC-Gen是一种基于Transformer的DNA序列生成模型，通过跨模态编码整合生物信号，支持自回归或掩码恢复目标，生成具有特定生物属性的序列。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在DNA序列生成中的应用，解决现有方法在可控性和功能相关性上的不足。

Method: 采用解码器和编码器Transformer架构，结合跨模态编码，支持自回归和掩码恢复训练目标。

Result: ATGC-Gen生成的序列流畅、多样且具有生物相关性，在可控性和功能相关性上优于现有方法。

Conclusion: ATGC-Gen展示了语言模型在可编程基因组设计中的潜力，代码已开源。

Abstract: We consider controllable DNA sequence design, where sequences are generated
by conditioning on specific biological properties. While language models (LMs)
such as GPT and BERT have achieved remarkable success in natural language
generation, their application to DNA sequence generation remains largely
underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer
Generator for Controllable Generation, which leverages cross-modal encoding to
integrate diverse biological signals. ATGC-Gen is instantiated with both
decoder-only and encoder-only transformer architectures, allowing flexible
training and generation under either autoregressive or masked recovery
objectives. We evaluate ATGC-Gen on representative tasks including promoter and
enhancer sequence design, and further introduce a new dataset based on ChIP-Seq
experiments for modeling protein binding specificity. Our experiments
demonstrate that ATGC-Gen can generate fluent, diverse, and biologically
relevant sequences aligned with the desired properties. Compared to prior
methods, our model achieves notable improvements in controllability and
functional relevance, highlighting the potential of language models in
advancing programmable genomic design. The source code is released at
(https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).

</details>


### [40] [Irredundant $k$-Fold Cross-Validation](https://arxiv.org/abs/2507.20048)
*Jesus S. Aguilar-Ruiz*

Main category: cs.LG

TL;DR: 提出Irredundant k-fold交叉验证方法，确保每个实例仅用于一次训练和一次测试，减少冗余和过拟合，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统k-fold交叉验证中实例重复使用导致冗余和不平衡，影响模型分析和性能估计。

Method: 引入Irredundant k-fold交叉验证，保证每个实例仅用于一次训练和一次测试，保持分层且模型无关。

Result: 实验显示该方法性能与传统k-fold相当，但方差估计更准确，计算成本更低。

Conclusion: Irredundant k-fold交叉验证是一种更高效、平衡且可靠的验证方法。

Abstract: In traditional k-fold cross-validation, each instance is used ($k-1$) times
for training and once for testing, leading to redundancy that lets many
instances disproportionately influence the learning phase. We introduce
Irredundant $k$-fold cross-validation, a novel method that guarantees each
instance is used exactly once for training and once for testing across the
entire validation procedure. This approach ensures a more balanced utilization
of the dataset, mitigates overfitting due to instance repetition, and enables
sharper distinctions in comparative model analysis. The method preserves
stratification and remains model-agnostic, i.e., compatible with any
classifier. Experimental results demonstrate that it delivers consistent
performance estimates across diverse datasets -- comparable to $k$-fold
cross-validation -- while providing less optimistic variance estimates because
training partitions are non-overlapping, and significantly reducing the overall
computational cost.

</details>


### [41] [Kolmogorov Arnold Network Autoencoder in Medicine](https://arxiv.org/abs/2507.19524)
*Ugo Lomoio,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: 本文比较了传统自编码器（如线性、卷积和变分）与Kolmogorov-Arnold网络（KAN）变体在心脏信号处理任务中的性能，发现KAN在参数更少或相同的情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究KAN架构在自编码任务中的潜力，尤其是在心脏信号处理领域，以验证其相对于传统方法的优势。

Method: 使用心脏信号数据集，对比传统自编码器与KAN变体在重建、生成、去噪、修复和异常检测任务中的表现。

Result: KAN架构在多个任务中表现优于传统自编码器，尤其是在参数效率方面。

Conclusion: KAN架构在心脏信号处理任务中具有显著优势，为未来研究提供了新方向。

Abstract: Deep learning neural networks architectures such Multi Layer Perceptrons
(MLP) and Convolutional blocks still play a crucial role in nowadays research
advancements. From a topological point of view, these architecture may be
represented as graphs in which we learn the functions related to the nodes
while fixed edges convey the information from the input to the output. A recent
work introduced a new architecture called Kolmogorov Arnold Networks (KAN) that
reports how putting learnable activation functions on the edges of the neural
network leads to better performances in multiple scenarios. Multiple studies
are focusing on optimizing the KAN architecture by adding important features
such as dropout regularization, Autoencoders (AE), model benchmarking and last,
but not least, the KAN Convolutional Network (KCN) that introduced matrix
convolution with KANs learning. This study aims to benchmark multiple versions
of vanilla AEs (such as Linear, Convolutional and Variational) against their
Kolmogorov-Arnold counterparts that have same or less number of parameters.
Using cardiological signals as model input, a total of five different classic
AE tasks were studied: reconstruction, generation, denoising, inpainting and
anomaly detection. The proposed experiments uses a medical dataset
\textit{AbnormalHeartbeat} that contains audio signals obtained from the
stethoscope.

</details>


### [42] [PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data](https://arxiv.org/abs/2507.20068)
*Aishwarya Mandyam,Jason Meng,Ge Gao,Jiankai Sun,Mac Schwager,Barbara E. Engelhardt,Emma Brunskill*

Main category: cs.LG

TL;DR: 论文提出两种方法，用于在使用数据增强时构建有效的置信区间，以改进强化学习中的离策略评估（OPE）。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，可靠的策略价值估计不确定性量化至关重要，而现有方法缺乏这一能力。

Method: 1. 针对特定初始状态的条件策略性能置信区间；2. 基于双重稳健估计和预测驱动推断的平均策略性能估计。

Result: 在多个模拟器和真实医疗数据集上，新方法能覆盖真实值，优于现有方法。

Conclusion: 新方法通过数据增强和不确定性量化，显著提升了OPE的可靠性。

Abstract: Off-policy evaluation (OPE) methods aim to estimate the value of a new
reinforcement learning (RL) policy prior to deployment. Recent advances have
shown that leveraging auxiliary datasets, such as those synthesized by
generative models, can improve the accuracy of these value estimates.
Unfortunately, such auxiliary datasets may also be biased, and existing methods
for using data augmentation for OPE in RL lack principled uncertainty
quantification. In high stakes settings like healthcare, reliable uncertainty
estimates are important for comparing policy value estimates. In this work, we
propose two approaches to construct valid confidence intervals for OPE when
using data augmentation. The first provides a confidence interval over the
policy performance conditioned on a particular initial state $V^{\pi}(s_0)$--
such intervals are particularly important for human-centered applications. To
do so we introduce a new conformal prediction method for high dimensional state
MDPs. Second, we consider the more common task of estimating the average policy
performance over many initial states; to do so we draw on ideas from doubly
robust estimation and prediction powered inference. Across simulators spanning
robotics, healthcare and inventory management, and a real healthcare dataset
from MIMIC-IV, we find that our methods can use augmented data and still
consistently produce intervals that cover the ground truth values, unlike
previously proposed methods.

</details>


### [43] [MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs](https://arxiv.org/abs/2507.19525)
*Chenchen Zhao,Zhengyuan Shi,Xiangyu Wen,Chengjie Liu,Yi Liu,Yunhao Zhou,Yuxiang Zhao,Hefei Feng,Yinan Zhu,Gwok-Waa Wan,Xin Cheng,Weiyu Chen,Yongqi Fu,Chujie Chen,Chenhao Xue,Guangyu Sun,Ying Wang,Yibo Lin,Jun Yang,Ning Xu,Xi Wang,Qiang Xu*

Main category: cs.LG

TL;DR: MMCircuitEval是首个针对EDA任务的多模态基准测试，包含3614个QA对，用于全面评估MLLM在电路设计中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围狭窄，难以全面评估MLLM在EDA中的性能，因此需要更全面的评估工具。

Method: 通过从教科书、技术题库等来源精心筛选QA对，并按设计阶段、电路类型等分类，构建MMCircuitEval基准。

Result: 评估显示现有LLM在EDA任务中存在显著性能差距，特别是在后端设计和复杂计算方面。

Conclusion: MMCircuitEval为MLLM在EDA中的发展提供了基础资源，有助于其在实际电路设计中的应用。

Abstract: The emergence of multimodal large language models (MLLMs) presents promising
opportunities for automation and enhancement in Electronic Design Automation
(EDA). However, comprehensively evaluating these models in circuit design
remains challenging due to the narrow scope of existing benchmarks. To bridge
this gap, we introduce MMCircuitEval, the first multimodal benchmark
specifically designed to assess MLLM performance comprehensively across diverse
EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer
(QA) pairs spanning digital and analog circuits across critical EDA stages -
ranging from general knowledge and specifications to front-end and back-end
design. Derived from textbooks, technical question banks, datasheets, and
real-world documentation, each QA pair undergoes rigorous expert review for
accuracy and relevance. Our benchmark uniquely categorizes questions by design
stage, circuit type, tested abilities (knowledge, comprehension, reasoning,
computation), and difficulty level, enabling detailed analysis of model
capabilities and limitations. Extensive evaluations reveal significant
performance gaps among existing LLMs, particularly in back-end design and
complex computations, highlighting the critical need for targeted training
datasets and modeling approaches. MMCircuitEval provides a foundational
resource for advancing MLLMs in EDA, facilitating their integration into
real-world circuit design workflows. Our benchmark is available at
https://github.com/cure-lab/MMCircuitEval.

</details>


### [44] [Feed-anywhere ANN (I) Steady Discrete $\to$ Diffusing on Graph Hidden States](https://arxiv.org/abs/2507.20088)
*Dmitry Pasechnyuk-Vilensky,Daniil Doroshenko*

Main category: cs.LG

TL;DR: 提出了一种基于几何分析和非线性动力学的新框架，用于从数据中学习隐藏的图结构。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理图结构数据时存在局限性，需要一种能够结合几何分析和非线性动力学的新方法。

Method: 1. 在图上的离散Sobolev空间中定义标量/向量场；2. 引入规范等价的非线性Schrödinger和Landau--Lifshitz动力学；3. 开发基于图模空间的随机梯度算法。

Result: 理论保证拓扑正确性、度量收敛性和高效搜索空间利用，模型泛化能力优于标准神经网络。

Conclusion: 该框架为图结构学习提供了新的理论和方法支持，具有更强的泛化能力和拓扑依赖性。

Abstract: We propose a novel framework for learning hidden graph structures from data
using geometric analysis and nonlinear dynamics. Our approach: (1) Defines
discrete Sobolev spaces on graphs for scalar/vector fields, establishing key
functional properties; (2) Introduces gauge-equivalent nonlinear Schr\"odinger
and Landau--Lifshitz dynamics with provable stable stationary solutions
smoothly dependent on input data and graph weights; (3) Develops a stochastic
gradient algorithm over graph moduli spaces with sparsity regularization.
Theoretically, we guarantee: topological correctness (homology recovery),
metric convergence (Gromov--Hausdorff), and efficient search space utilization.
Our dynamics-based model achieves stronger generalization bounds than standard
neural networks, with complexity dependent on the data manifold's topology.

</details>


### [45] [Quantizing Text-attributed Graphs for Semantic-Structural Integration](https://arxiv.org/abs/2507.19526)
*Jianyuan Bo,Hao Wu,Yuan Fang*

Main category: cs.LG

TL;DR: STAG是一种自监督框架，通过离散化图结构信息为令牌，解决了现有方法在将结构信息嵌入LLM兼容格式时的挑战，支持零样本迁移学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将图结构信息嵌入LLM兼容格式时面临计算成本高或丢失结构细节的问题，且依赖源域标记数据，限制了适应性。

Method: STAG通过软分配和KL散度引导的量化方法，将图结构信息直接量化为离散令牌，无需自然令牌化结构。

Result: 实验表明，STAG在多个节点分类基准上达到最先进性能，且兼容不同LLM架构。

Conclusion: STAG为图学习与LLM的结合提供了优雅解决方案，支持零样本迁移学习。

Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation for
modeling complex relationships across diverse domains. With the rise of large
language models (LLMs), there is growing interest in leveraging their
capabilities for graph learning. However, current approaches face significant
challenges in embedding structural information into LLM-compatible formats,
requiring either computationally expensive alignment mechanisms or manual graph
verbalization techniques that often lose critical structural details. Moreover,
these methods typically require labeled data from source domains for effective
transfer learning, significantly constraining their adaptability. We propose
STAG, a novel self-supervised framework that directly quantizes graph
structural information into discrete tokens using a frozen codebook. Unlike
traditional quantization approaches, our method employs soft assignment and KL
divergence guided quantization to address the unique challenges of graph data,
which lacks natural tokenization structures. Our framework enables both
LLM-based and traditional learning approaches, supporting true zero-shot
transfer learning without requiring labeled data even in the source domain.
Extensive experiments demonstrate state-of-the-art performance across multiple
node classification benchmarks while maintaining compatibility with different
LLM architectures, offering an elegant solution to bridging graph learning with
LLMs.

</details>


### [46] [Meta Fusion: A Unified Framework For Multimodality Fusion with Mutual Learning](https://arxiv.org/abs/2507.20089)
*Ziyi Liang,Annie Qu,Babak Shahbaba*

Main category: cs.LG

TL;DR: Meta Fusion是一个灵活的多模态数据融合框架，统一了早期、中期和晚期融合策略，通过软信息共享提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统多模态数据融合方法各有优缺点，需要一种更灵活且性能更优的统一框架。

Method: 基于深度互学习和集成学习，Meta Fusion构建多模态潜在表示组合的模型群，并通过软信息共享优化性能。

Result: Meta Fusion在仿真和实际应用（如阿尔茨海默病检测和神经解码）中均优于传统融合策略。

Conclusion: Meta Fusion是一个模型无关的统一框架，能显著提升多模态数据融合的预测性能。

Abstract: Developing effective multimodal data fusion strategies has become
increasingly essential for improving the predictive power of statistical
machine learning methods across a wide range of applications, from autonomous
driving to medical diagnosis. Traditional fusion methods, including early,
intermediate, and late fusion, integrate data at different stages, each
offering distinct advantages and limitations. In this paper, we introduce Meta
Fusion, a flexible and principled framework that unifies these existing
strategies as special cases. Motivated by deep mutual learning and ensemble
learning, Meta Fusion constructs a cohort of models based on various
combinations of latent representations across modalities, and further boosts
predictive performance through soft information sharing within the cohort. Our
approach is model-agnostic in learning the latent representations, allowing it
to flexibly adapt to the unique characteristics of each modality.
Theoretically, our soft information sharing mechanism reduces the
generalization error. Empirically, Meta Fusion consistently outperforms
conventional fusion strategies in extensive simulation studies. We further
validate our approach on real-world applications, including Alzheimer's disease
detection and neural decoding.

</details>


### [47] [Research on the application of graph data structure and graph neural network in node classification/clustering tasks](https://arxiv.org/abs/2507.19527)
*Yihan Wang,Jianing Zhao*

Main category: cs.LG

TL;DR: 该论文研究了图数据结构、经典图算法和图神经网络（GNNs），通过比较实验定量评估了传统算法与GNNs在节点分类和聚类任务中的性能差异，结果显示GNNs比传统方法准确率提升43%至70%。


<details>
  <summary>Details</summary>
Motivation: 由于图数据的非欧几里得特性，传统机器学习方法面临挑战，因此需要探索图数据结构和GNNs的性能差异。

Method: 研究结合了理论分析和比较实验，评估了传统图算法与GNNs在节点分类和聚类任务中的表现。

Result: GNNs在节点分类和聚类任务中比传统方法准确率提升43%至70%。

Conclusion: 论文为图表示学习研究提供了理论指导，并探索了经典算法与GNN架构的集成策略。

Abstract: Graph-structured data are pervasive across domains including social networks,
biological networks, and knowledge graphs. Due to their non-Euclidean nature,
such data pose significant challenges to conventional machine learning methods.
This study investigates graph data structures, classical graph algorithms, and
Graph Neural Networks (GNNs), providing comprehensive theoretical analysis and
comparative evaluation. Through comparative experiments, we quantitatively
assess performance differences between traditional algorithms and GNNs in node
classification and clustering tasks. Results show GNNs achieve substantial
accuracy improvements of 43% to 70% over traditional methods. We further
explore integration strategies between classical algorithms and GNN
architectures, providing theoretical guidance for advancing graph
representation learning research.

</details>


### [48] [Graded Transformers: A Symbolic-Geometric Approach to Structured Learning](https://arxiv.org/abs/2507.20108)
*Tony Shaska Sr*

Main category: cs.LG

TL;DR: 提出了Graded Transformer框架，通过代数归纳偏置增强序列模型，包括LGT和EGT两种架构，适用于结构化数据。


<details>
  <summary>Details</summary>
Motivation: 解决传统模型在结构化数据中效率不足的问题，融合几何与代数原理，提升模型的解释性和效率。

Method: 引入分级变换，通过参数化缩放操作符（固定或可学习的分级元组）在注意力和表示层中注入层次结构。

Result: 理论保证包括通用逼近定理、样本复杂度降低、操作连续性及对抗鲁棒性；应用领域广泛。

Conclusion: Graded Transformer为结构化深度学习提供了数学基础，推动了可解释高效系统的发展。

Abstract: We introduce the Graded Transformer framework, a novel class of sequence
models that embeds algebraic inductive biases through grading transformations
on vector spaces. Extending the theory of Graded Neural Networks (GNNs), we
propose two architectures: the Linearly Graded Transformer (LGT) and the
Exponentially Graded Transformer (EGT). These models apply parameterized
scaling operators-governed by fixed or learnable grading tuples and, for EGT,
exponential factors to infuse hierarchical structure into attention and
representation layers, enhancing efficiency for structured data.
  We derive rigorous theoretical guarantees, including universal approximation
theorems for continuous and Sobolev functions, reduced sample complexity via
effective VC dimension bounds, Lipschitz continuity of graded operations, and
robustness to adversarial perturbations. A graded loss function ensures
gradient stability and alignment with domain priors during optimization. By
treating grades as differentiable parameters, the framework enables adaptive
feature prioritization, overcoming limitations of fixed grades in prior work.
  The Graded Transformer holds transformative potential for hierarchical
learning and neurosymbolic reasoning, with applications spanning algebraic
geometry (e.g., moduli spaces and zeta functions), physics (e.g., multiscale
simulations), natural language processing (e.g., syntactic parsing), biological
sequence analysis (e.g., variant prediction), and emerging areas like graph
neural networks and financial modeling. This work advances structured deep
learning by fusing geometric and algebraic principles with attention
mechanisms, offering a mathematically grounded alternative to data-driven
models and paving the way for interpretable, efficient systems in complex
domains.

</details>


### [49] [Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction](https://arxiv.org/abs/2507.19529)
*Obumneme Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 阿曼通过绿色氢能项目应对全球脱碳趋势，但缺乏大规模氢设施的历史数据，导致风险评估困难。本文提出一种基于气象数据的AI决策支持系统，预测维护压力指数（MPI），以填补数据空白并优化拍卖决策。


<details>
  <summary>Details</summary>
Motivation: 全球绿色氢能投资增加，但缺乏沙漠环境下大规模氢设施的运行数据，导致基础设施规划和拍卖决策的风险评估困难。

Method: 利用公开气象数据开发AI决策支持系统，生成维护压力指数（MPI），预测氢能基础设施的维护需求和风险水平。

Result: 提出的MPI工具能够填补历史数据空白，为拍卖评估和运营决策提供时间维度的风险智能。

Conclusion: AI驱动的MPI系统为绿色氢能项目提供了可靠的风险预测工具，支持更明智的基础设施规划和拍卖决策。

Abstract: As green hydrogen emerges as a major component of global decarbonisation,
Oman has positioned itself strategically through national auctions and
international partnerships. Following two successful green hydrogen project
rounds, the country launched its third auction (R3) in the Duqm region. While
this area exhibits relative geospatial homogeneity, it is still vulnerable to
environmental fluctuations that pose inherent risks to productivity. Despite
growing global investment in green hydrogen, operational data remains scarce,
with major projects like Saudi Arabia's NEOM facility not expected to commence
production until 2026, and Oman's ACME Duqm project scheduled for 2028. This
absence of historical maintenance and performance data from large-scale
hydrogen facilities in desert environments creates a major knowledge gap for
accurate risk assessment for infrastructure planning and auction decisions.
Given this data void, environmental conditions emerge as accessible and
reliable proxy for predicting infrastructure maintenance pressures, because
harsh desert conditions such as dust storms, extreme temperatures, and humidity
fluctuations are well-documented drivers of equipment degradation in renewable
energy systems. To address this challenge, this paper proposes an Artificial
Intelligence decision support system that leverages publicly available
meteorological data to develop a predictive Maintenance Pressure Index (MPI),
which predicts risk levels and future maintenance demands on hydrogen
infrastructure. This tool strengthens regulatory foresight and operational
decision-making by enabling temporal benchmarking to assess and validate
performance claims over time. It can be used to incorporate temporal risk
intelligence into auction evaluation criteria despite the absence of historical
operational benchmarks.

</details>


### [50] [Online Learning with Probing for Sequential User-Centric Selection](https://arxiv.org/abs/2507.20112)
*Tianyi Xu,Yiting Chen,Henger Li,Zheyong Bian,Emiliano Dall'Anese,Zizhan Zheng*

Main category: cs.LG

TL;DR: 论文提出了PUCS框架，用于解决信息获取下的顺序决策问题，包括离线贪心算法和在线OLPA算法，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在资源与回报未知且探测成本高的应用中（如拼车、无线调度等），需要一种有效的决策框架。

Method: 提出PUCS框架，离线用贪心算法，在线用OLPA算法。

Result: 离线算法有常数近似比，在线算法达到√T的遗憾上界，且下界证明其紧性。

Conclusion: PUCS框架及算法在实际应用中表现优异。

Abstract: We formalize sequential decision-making with information acquisition as the
probing-augmented user-centric selection (PUCS) framework, where a learner
first probes a subset of arms to obtain side information on resources and
rewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such
as ridesharing, wireless scheduling, and content recommendation, in which both
resources and payoffs are initially unknown and probing is costly. For the
offline setting with known distributions, we present a greedy probing algorithm
with a constant-factor approximation guarantee $\zeta = (e-1)/(2e-1)$. For the
online setting with unknown distributions, we introduce OLPA, a stochastic
combinatorial bandit algorithm that achieves a regret bound
$\mathcal{O}(\sqrt{T} + \ln^{2} T)$. We also prove a lower bound
$\Omega(\sqrt{T})$, showing that the upper bound is tight up to logarithmic
factors. Experiments on real-world data demonstrate the effectiveness of our
solutions.

</details>


### [51] [Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation](https://arxiv.org/abs/2507.19530)
*Md Basit Azam,Sarangthem Ibotombi Singh*

Main category: cs.LG

TL;DR: 该研究提出了一个全面的框架，用于基于电子健康记录（EHR）的血压预测，解决了现有机器学习方法在外部验证、不确定性量化和数据泄漏预防方面的不足。


<details>
  <summary>Details</summary>
Motivation: 重症监护病房（ICU）中血压监测至关重要，但现有机器学习方法存在外部验证不足、缺乏不确定性量化和数据泄漏预防等问题。

Method: 研究采用系统性数据泄漏预防、分位数回归进行不确定性量化，并在MIMIC-III和eICU数据库间进行外部验证。结合梯度提升、随机森林和XGBoost的集成框架，涵盖五个生理领域的74个特征。

Result: 内部验证表现良好（SBP：R²=0.86，RMSE=6.03 mmHg；DBP：R²=0.49，RMSE=7.13 mmHg），外部验证显示性能下降30%，不确定性量化生成了有效的预测区间。

Conclusion: 该框架为跨机构AI辅助血压监测提供了实际部署的预期，并公开了源代码。

Abstract: Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)
where hemodynamic instability can
  rapidly progress to cardiovascular collapse. Current machine
  learning (ML) approaches suffer from three limitations: lack of
  external validation, absence of uncertainty quantification, and
  inadequate data leakage prevention. This study presents the
  first comprehensive framework with novel algorithmic leakage
  prevention, uncertainty quantification, and cross-institutional
  validation for electronic health records (EHRs) based BP pre dictions. Our
methodology implemented systematic data leakage
  prevention, uncertainty quantification through quantile regres sion, and
external validation between the MIMIC-III and eICU
  databases. An ensemble framework combines Gradient Boosting,
  Random Forest, and XGBoost with 74 features across five
  physiological domains. Internal validation achieved a clinically
  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03
  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI
  standards. External validation showed 30% degradation with
  critical limitations in patients with hypotensive. Uncertainty
  quantification generated valid prediction intervals (80.3% SBP
  and 79.9% DBP coverage), enabling risk-stratified protocols
  with narrow intervals (< 15 mmHg) for standard monitoring
  and wide intervals (> 30 mmHg) for manual verification. This
  framework provides realistic deployment expectations for cross institutional
AI-assisted BP monitoring in critical care settings.
  The source code is publicly available at https://github.com/
  mdbasit897/clinical-bp-prediction-ehr.

</details>


### [52] [Data-Efficient Prediction-Powered Calibration via Cross-Validation](https://arxiv.org/abs/2507.20268)
*Seonghoon Yoo,Houssem Sifaou,Sangwoo Park,Joonhyuk Kang,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出一种新方法，利用有限校准数据同时微调预测器并估计合成标签的偏差，以解决校准数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型决策不确定性量化中校准数据稀缺的问题。

Method: 通过同时微调预测器和估计合成标签偏差，高效利用有限校准数据。

Result: 在室内定位问题中验证了方法的有效性和性能提升。

Conclusion: 该方法为AI生成决策提供了严格的覆盖保证。

Abstract: Calibration data are necessary to formally quantify the uncertainty of the
decisions produced by an existing artificial intelligence (AI) model. To
overcome the common issue of scarce calibration data, a promising approach is
to employ synthetic labels produced by a (generally different) predictive
model. However, fine-tuning the label-generating predictor on the inference
task of interest, as well as estimating the residual bias of the synthetic
labels, demand additional data, potentially exacerbating the calibration data
scarcity problem. This paper introduces a novel approach that efficiently
utilizes limited calibration data to simultaneously fine-tune a predictor and
estimate the bias of the synthetic labels. The proposed method yields
prediction sets with rigorous coverage guarantees for AI-generated decisions.
Experimental results on an indoor localization problem validate the
effectiveness and performance gains of our solution.

</details>


### [53] [FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings](https://arxiv.org/abs/2507.19534)
*Ali Shakeri,Wei Emma Zhang,Amin Beheshti,Weitong Chen,Jian Yang,Lishan Yang*

Main category: cs.LG

TL;DR: FedDPG是一种结合动态提示生成和联邦学习的方法，旨在提高模型灵活性并保护数据隐私，同时减少计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统提示调优方法缺乏灵活性，且联邦学习中存在计算和通信限制问题。

Method: 提出FedDPG，通过动态提示生成网络生成上下文感知提示，并在联邦学习框架中优化。

Result: 在三个NLP基准数据集上，FedDPG优于现有参数高效调优方法，显著减少计算时间和通信参数。

Conclusion: FedDPG在保持数据隐私的同时，提升了模型性能和效率。

Abstract: Pre-trained Language Models (PLMs) have demonstrated impressive performance
in various NLP tasks. However, traditional fine-tuning methods for leveraging
PLMs for downstream tasks entail significant computational overhead.
Prompt-tuning has emerged as an efficient alternative that involves prepending
a limited number of parameters to the input sequence and only updating them
while the PLM's parameters are frozen. However, this technique's prompts remain
fixed for all inputs, reducing the model's flexibility. The Federated Learning
(FL) technique has gained attention in recent years to address the growing
concerns around data privacy. However, challenges such as communication and
computation limitations of clients still need to be addressed. To mitigate
these challenges, this paper introduces the Federated Dynamic Prompt Generator
(FedDPG), which incorporates a dynamic prompt generator network to generate
context-aware prompts based on the given input, ensuring flexibility and
adaptability while prioritising data privacy in federated learning settings.
Our experiments on three NLP benchmark datasets showcase that FedDPG
outperforms the state-of-the-art parameter-efficient fine-tuning methods in
terms of global model performance, and has significantly reduced the
calculation time and the number of parameters to be sent through the FL
network.

</details>


### [54] [Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence](https://arxiv.org/abs/2507.20272)
*Dharmesh Tailor,Alvaro H. C. Correia,Eric Nalisnick,Christos Louizos*

Main category: cs.LG

TL;DR: 该论文提出了一种无需保留数据的后处理方法，通过近似全共形预测（full-CP）为神经网络回归器构建预测区间，避免了传统方法如Laplace方法和split-CP的局限性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域部署深度学习模型时，不确定性量化至关重要，但现有方法如Laplace方法可能校准不佳，而split-CP需要样本分割，牺牲统计效率。

Method: 通过局部扰动模型参数（使用高斯-牛顿影响）和网络线性化，近似全共形预测，避免了逐点重新训练和输出空间的穷举搜索。

Result: 在标准回归基准和边界框定位任务中，生成的预测区间具有局部适应性，且通常比split-CP更紧凑。

Conclusion: 该方法提供了一种高效且实用的不确定性量化解决方案，适用于后处理场景。

Abstract: Uncertainty quantification is an important prerequisite for the deployment of
deep learning models in safety-critical areas. Yet, this hinges on the
uncertainty estimates being useful to the extent the prediction intervals are
well-calibrated and sharp. In the absence of inherent uncertainty estimates
(e.g. pretrained models predicting only point estimates), popular approaches
that operate post-hoc include Laplace's method and split conformal prediction
(split-CP). However, Laplace's method can be miscalibrated when the model is
misspecified and split-CP requires sample splitting, and thus comes at the
expense of statistical efficiency. In this work, we construct prediction
intervals for neural network regressors post-hoc without held-out data. This is
achieved by approximating the full conformal prediction method (full-CP).
Whilst full-CP nominally requires retraining the model for every test point and
candidate label, we propose to train just once and locally perturb model
parameters using Gauss-Newton influence to approximate the effect of
retraining. Coupled with linearization of the network, we express the absolute
residual nonconformity score as a piecewise linear function of the candidate
label allowing for an efficient procedure that avoids the exhaustive search
over the output space. On standard regression benchmarks and bounding box
localization, we show the resulting prediction intervals are locally-adaptive
and often tighter than those of split-CP.

</details>


### [55] [Graph Learning Metallic Glass Discovery from Wikipedia](https://arxiv.org/abs/2507.19536)
*K. -C. Ouyang,S. -Y. Zhang,S. -L. Liu,J. Tian,Y. -H. Li,H. Tong,H. -Y. Bai,W. -H. Wang,Y. -C. Hu*

Main category: cs.LG

TL;DR: 本文提出了一种基于材料网络表示和语言模型的数据驱动方法，用于高效合成新材料，特别是金属玻璃。


<details>
  <summary>Details</summary>
Motivation: 传统材料合成过程缓慢且昂贵，尤其是金属玻璃的形成需要多元素优化组合，数据稀缺和材料编码不成熟限制了统计学习算法的预测能力。

Method: 利用语言模型从维基百科编码节点元素，设计多种架构的图神经网络作为推荐系统，探索材料间的隐藏关系。

Result: 通过多语言维基百科嵌入评估自然语言在材料设计中的能力，展示了新方法的潜力。

Conclusion: 本研究为利用人工智能探索新型非晶材料提供了新范式。

Abstract: Synthesizing new materials efficiently is highly demanded in various research
fields. However, this process is usually slow and expensive, especially for
metallic glasses, whose formation strongly depends on the optimal combinations
of multiple elements to resist crystallization. This constraint renders only
several thousands of candidates explored in the vast material space since 1960.
Recently, data-driven approaches armed by advanced machine learning techniques
provided alternative routes for intelligent materials design. Due to data
scarcity and immature material encoding, the conventional tabular data is
usually mined by statistical learning algorithms, giving limited model
predictability and generalizability. Here, we propose sophisticated data
learning from material network representations. The node elements are encoded
from the Wikipedia by a language model. Graph neural networks with versatile
architectures are designed to serve as recommendation systems to explore hidden
relationships among materials. By employing Wikipedia embeddings from different
languages, we assess the capability of natural languages in materials design.
Our study proposes a new paradigm to harvesting new amorphous materials and
beyond with artificial intelligence.

</details>


### [56] [Diagonally-Weighted Generalized Method of Moments Estimation for Gaussian Mixture Modeling](https://arxiv.org/abs/2507.20459)
*Liu Zhang,Oscar Mickelin,Sheng Xu,Amit Singer*

Main category: cs.LG

TL;DR: 论文提出了一种对角线加权广义矩方法（DGMM），解决了高维数据中传统矩方法（MM和GMM）计算和存储复杂度高的问题，并在弱分离异方差低秩高斯混合模型中验证了其高效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统矩方法（MM和GMM）在高维数据或需要高阶矩时计算和存储复杂度呈指数增长，限制了其实际应用。GMM还需估计大型加权矩阵，进一步加剧了计算瓶颈。

Method: 提出对角线加权GMM（DGMM），通过合理加权矩，平衡统计效率、计算复杂度和数值稳定性。设计了一种高效算法，无需显式计算或存储矩张量。

Result: 数值研究表明，DGMM在估计误差和运行时间上均优于MM和GMM，具有更高的效率和稳定性。

Conclusion: DGMM是一种在高维数据中高效且稳定的矩估计方法，适用于弱分离异方差低秩高斯混合模型。

Abstract: Since Pearson [Philosophical Transactions of the Royal Society of London. A,
185 (1894), pp. 71-110] first applied the method of moments (MM) for modeling
data as a mixture of one-dimensional Gaussians, moment-based estimation methods
have proliferated. Among these methods, the generalized method of moments (GMM)
improves the statistical efficiency of MM by weighting the moments
appropriately. However, the computational complexity and storage complexity of
MM and GMM grow exponentially with the dimension, making these methods
impractical for high-dimensional data or when higher-order moments are
required. Such computational bottlenecks are more severe in GMM since it
additionally requires estimating a large weighting matrix. To overcome these
bottlenecks, we propose the diagonally-weighted GMM (DGMM), which achieves a
balance among statistical efficiency, computational complexity, and numerical
stability. We apply DGMM to study the parameter estimation problem for weakly
separated heteroscedastic low-rank Gaussian mixtures and design a
computationally efficient and numerically stable algorithm that obtains the
DGMM estimator without explicitly computing or storing the moment tensors. We
implement the proposed algorithm and empirically validate the advantages of
DGMM: in numerical studies, DGMM attains smaller estimation errors while
requiring substantially shorter runtime than MM and GMM. The code and data will
be available upon publication at https://github.com/liu-lzhang/dgmm.

</details>


### [57] [Improving Group Fairness in Tensor Completion via Imbalance Mitigating Entity Augmentation](https://arxiv.org/abs/2507.20542)
*Dawon Ahn,Jun-Gi Jang,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: STAFF方法通过稀疏张量增强提升组公平性，同时降低整体张量补全误差。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解方法在组公平性方面表现不佳，导致性能下降。

Method: 提出STAFF方法，通过增强张量实体以减少不平衡和组偏见，优化补全误差和公平性。

Result: STAFF在多种数据集和模型下表现最佳，MSE和MADE分别降低36%和59%。

Conclusion: STAFF在张量补全中实现了公平性与性能的最佳平衡。

Abstract: Group fairness is important to consider in tensor decomposition to prevent
discrimination based on social grounds such as gender or age. Although few
works have studied group fairness in tensor decomposition, they suffer from
performance degradation. To address this, we propose STAFF(Sparse Tensor
Augmentation For Fairness) to improve group fairness by minimizing the gap in
completion errors of different groups while reducing the overall tensor
completion error. Our main idea is to augment a tensor with augmented entities
including sufficient observed entries to mitigate imbalance and group bias in
the sparse tensor. We evaluate \method on tensor completion with various
datasets under conventional and deep learning-based tensor models. STAFF
consistently shows the best trade-off between completion error and group
fairness; at most, it yields 36% lower MSE and 59% lower MADE than the
second-best baseline.

</details>


### [58] [Latent Representations of Intracardiac Electrograms for Atrial Fibrillation Driver Detection](https://arxiv.org/abs/2507.19547)
*Pablo Peiro-Corbacho,Long Lin,Pablo Ávila,Alejandro Carta-Bergaz,Ángel Arenal,Carlos Sevilla-Salcedo,Gonzalo R. Ríos-Muñoz*

Main category: cs.LG

TL;DR: 该研究提出了一种基于卷积自编码器的深度学习框架，用于从心房颤动（AF）的腔内电图（EGMs）中无监督提取特征，以帮助检测AF驱动源。


<details>
  <summary>Details</summary>
Motivation: 当前消融疗法对持续性AF效果不佳，因非肺静脉驱动源的参与。研究旨在通过深度学习自动化EGM分析，提高AF驱动源的检测效率。

Method: 使用卷积自编码器从单极和双极EGMs中提取潜在特征，并通过下游分类器检测旋转和焦点活动。

Result: 自编码器成功提取了低重构损失的潜在特征，分类器在检测旋转和焦点活动时表现中等（AUC 0.73-0.76），在识别心房EGM纠缠时表现优异（AUC 0.93）。

Conclusion: 该方法可实时运行并集成到临床电解剖标测系统中，展示了无监督学习在提取心脏信号生理特征中的潜力。

Abstract: Atrial Fibrillation (AF) is the most prevalent sustained arrhythmia, yet
current ablation therapies, including pulmonary vein isolation, are frequently
ineffective in persistent AF due to the involvement of non-pulmonary vein
drivers. This study proposes a deep learning framework using convolutional
autoencoders for unsupervised feature extraction from unipolar and bipolar
intracavitary electrograms (EGMs) recorded during AF in ablation studies. These
latent representations of atrial electrical activity enable the
characterization and automation of EGM analysis, facilitating the detection of
AF drivers.
  The database consisted of 11,404 acquisitions recorded from 291 patients,
containing 228,080 unipolar EGMs and 171,060 bipolar EGMs. The autoencoders
successfully learned latent representations with low reconstruction loss,
preserving the morphological features. The extracted embeddings allowed
downstream classifiers to detect rotational and focal activity with moderate
performance (AUC 0.73-0.76) and achieved high discriminative performance in
identifying atrial EGM entanglement (AUC 0.93).
  The proposed method can operate in real-time and enables integration into
clinical electroanatomical mapping systems to assist in identifying
arrhythmogenic regions during ablation procedures. This work highlights the
potential of unsupervised learning to uncover physiologically meaningful
features from intracardiac signals.

</details>


### [59] [Personalized Treatment Effect Estimation from Unstructured Data](https://arxiv.org/abs/2507.20993)
*Henri Arno,Thomas Demeester*

Main category: cs.LG

TL;DR: 论文提出了一种利用非结构化数据估计个性化治疗效果的方法，包括直接训练的近似'插件'方法和两种理论基础的估计器，解决了混杂偏差和抽样偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖结构化协变量，限制了在非结构化数据（如医疗临床记录或图像）中的应用潜力。

Method: 引入近似'插件'方法直接训练非结构化数据的神经表示；提出两种理论基础的估计器，利用混杂因子的结构化测量避免偏差；进一步引入回归校正解决抽样偏差。

Result: 实验表明，插件方法在非结构化数据上直接训练，表现优异。

Conclusion: 该方法扩展了个性化治疗效果估计的应用范围，解决了非结构化数据中的混杂和抽样偏差问题。

Abstract: Existing methods for estimating personalized treatment effects typically rely
on structured covariates, limiting their applicability to unstructured data.
Yet, leveraging unstructured data for causal inference has considerable
application potential, for instance in healthcare, where clinical notes or
medical images are abundant. To this end, we first introduce an approximate
'plug-in' method trained directly on the neural representations of unstructured
data. However, when these fail to capture all confounding information, the
method may be subject to confounding bias. We therefore introduce two
theoretically grounded estimators that leverage structured measurements of the
confounders during training, but allow estimating personalized treatment
effects purely from unstructured inputs, while avoiding confounding bias. When
these structured measurements are only available for a non-representative
subset of the data, these estimators may suffer from sampling bias. To address
this, we further introduce a regression-based correction that accounts for the
non-uniform sampling, assuming the sampling mechanism is known or can be
well-estimated. Our experiments on two benchmark datasets show that the plug-in
method, directly trainable on large unstructured datasets, achieves strong
empirical performance across all settings, despite its simplicity.

</details>


### [60] [Harnessing intuitive local evolution rules for physical learning](https://arxiv.org/abs/2507.19561)
*Roie Ezraty,Menachem Stern,Shmuel M. Rubinstein*

Main category: cs.LG

TL;DR: 提出了一种基于物理系统的低功耗学习方案BEASTAL，通过边界参数控制和局部物理规则实现自主学习。


<details>
  <summary>Details</summary>
Motivation: 机器学习计算密集且功耗高，需要探索物理系统实现学习任务的替代方案。

Method: 引入BEASTAL训练方案，仅控制边界参数，利用局部物理规则进行学习。

Result: 在回归和分类任务中验证了自主学习的有效性，适用于线性任务，非线性规则下性能最佳。

Conclusion: BEASTAL方案无需大规模内存或复杂架构，为物理学习系统提供了高效解决方案。

Abstract: Machine Learning, however popular and accessible, is computationally
intensive and highly power-consuming, prompting interest in alternative
physical implementations of learning tasks. We introduce a training scheme for
physical systems that minimize power dissipation in which only boundary
parameters (i.e. inputs and outputs) are externally controlled. Using this
scheme, these Boundary-Enabled Adaptive State Tuning Systems (BEASTS) learn by
exploiting local physical rules. Our scheme, BEASTAL (BEAST-Adaline), is the
closest analog of the Adaline algorithm for such systems. We demonstrate this
autonomous learning in silico for regression and classification tasks. Our
approach advances previous physical learning schemes by using intuitive, local
evolution rules without requiring large-scale memory or complex internal
architectures. BEASTAL can perform any linear task, achieving best performance
when the local evolution rule is non-linear.

</details>


### [61] [Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps: An Interpretation and Potential Improvements](https://arxiv.org/abs/2507.21040)
*Aditya Ravuri,Neil D. Lawrence*

Main category: cs.LG

TL;DR: 论文提出将Transformer解释为基于概率拉普拉斯特征映射模型的展开推理步骤，并证明初始化时Transformer执行线性降维。此外，Transformer块中出现的图拉普拉斯项优于注意力矩阵（被视为邻接矩阵），通过简单调整注意力矩阵可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer的底层数学机制，揭示其与概率模型的联系，以改进模型性能。

Method: 基于ProbDR框架，将Transformer解释为概率拉普拉斯特征映射模型的展开推理步骤，分析初始化行为及图拉普拉斯项的作用。

Result: 实验表明，调整注意力矩阵（减去单位矩阵）能提升语言模型和简单视觉Transformer的验证性能。

Conclusion: 论文为Transformer提供了新的概率解释，并展示了通过简单调整注意力矩阵优化性能的潜力。

Abstract: We propose a probabilistic interpretation of transformers as unrolled
inference steps assuming a probabilistic Laplacian Eigenmaps model from the
ProbDR framework. Our derivation shows that at initialisation, transformers
perform "linear" dimensionality reduction. We also show that within the
transformer block, a graph Laplacian term arises from our arguments, rather
than an attention matrix (which we interpret as an adjacency matrix). We
demonstrate that simply subtracting the identity from the attention matrix (and
thereby taking a graph diffusion step) improves validation performance on a
language model and a simple vision transformer.

</details>


### [62] [Efficient and Scalable Agentic AI with Heterogeneous Systems](https://arxiv.org/abs/2507.19635)
*Zain Asgar,Michelle Nguyen,Sachin Katti*

Main category: cs.LG

TL;DR: 本文提出了一种动态编排AI代理工作负载的系统设计，旨在优化异构计算基础设施上的执行效率，显著降低总拥有成本（TCO）。


<details>
  <summary>Details</summary>
Motivation: AI代理工作负载具有动态性和结构复杂性，需要高效的部署和服务基础设施以支持其规模化应用。

Method: 设计包括执行图优化框架、基于MLIR的表示和编译系统，以及动态编排系统，支持异构硬件上的任务分解与调度。

Result: 初步结果显示，异构基础设施能显著降低TCO，甚至某些情况下旧GPU与新加速器的组合性能与最新同构GPU相当。

Conclusion: 异构计算基础设施可有效优化AI代理工作负载的部署成本，延长现有硬件的使用寿命。

Abstract: AI agents are emerging as a dominant workload in a wide range of
applications, promising to be the vehicle that delivers the promised benefits
of AI to enterprises and consumers. Unlike conventional software or static
inference, agentic workloads are dynamic and structurally complex. Often these
agents are directed graphs of compute and IO operations that span multi-modal
data input and conversion), data processing and context gathering (e.g vector
DB lookups), multiple LLM inferences, tool calls, etc. To scale AI agent usage,
we need efficient and scalable deployment and agent-serving infrastructure.
  To tackle this challenge, in this paper, we present a system design for
dynamic orchestration of AI agent workloads on heterogeneous compute
infrastructure spanning CPUs and accelerators, both from different vendors and
across different performance tiers within a single vendor. The system delivers
several building blocks: a framework for planning and optimizing agentic AI
execution graphs using cost models that account for compute, memory, and
bandwidth constraints of different HW; a MLIR based representation and
compilation system that can decompose AI agent execution graphs into granular
operators and generate code for different HW options; and a dynamic
orchestration system that can place the granular components across a
heterogeneous compute infrastructure and stitch them together while meeting an
end-to-end SLA. Our design performs a systems level TCO optimization and
preliminary results show that leveraging a heterogeneous infrastructure can
deliver significant TCO benefits. A preliminary surprising finding is that for
some workloads a heterogeneous combination of older generation GPUs with newer
accelerators can deliver similar TCO as the latest generation homogenous GPU
infrastructure design, potentially extending the life of deployed
infrastructure.

</details>


### [63] [Directly Learning Stock Trading Strategies Through Profit Guided Loss Functions](https://arxiv.org/abs/2507.19639)
*Devroop Kar,Zimeng Lyu,Sheeraja Rajakrishnan,Hao Zhang,Alex Ororbia,Travis Desell,Daniel Krutz*

Main category: cs.LG

TL;DR: 提出四种新型损失函数，用于驱动股票投资组合决策，显著超越基准方法。


<details>
  <summary>Details</summary>
Motivation: 股票市场高度波动，传统交易决策难以盈利，需改进方法。

Method: 使用四种损失函数训练时间序列模型（如Transformer），优化交易策略。

Result: 在50只S&P 500股票上测试，Crossformer模型表现最佳，年回报率超48%。

Conclusion: 新型损失函数有效提升交易策略表现，优于强化学习和基准方法。

Abstract: Stock trading has always been a challenging task due to the highly volatile
nature of the stock market. Making sound trading decisions to generate profit
is particularly difficult under such conditions. To address this, we propose
four novel loss functions to drive decision-making for a portfolio of stocks.
These functions account for the potential profits or losses based with respect
to buying or shorting respective stocks, enabling potentially any artificial
neural network to directly learn an effective trading strategy. Despite the
high volatility in stock market fluctuations over time, training time-series
models such as transformers on these loss functions resulted in trading
strategies that generated significant profits on a portfolio of 50 different
S&P 500 company stocks as compared to a benchmark reinforcment learning
techniques and a baseline buy and hold method. As an example, using 2021, 2022
and 2023 as three test periods, the Crossformer model adapted with our best
loss function was most consistent, resulting in returns of 51.42%, 51.04% and
48.62% respectively. In comparison, the best performing state-of-the-art
reinforcement learning methods, PPO and DDPG, only delivered maximum profits of
around 41%, 2.81% and 41.58% for the same periods. The code is available at
https://anonymous.4open.science/r/bandit-stock-trading-58C8/README.md.

</details>


### [64] [Debunking Optimization Myths in Federated Learning for Medical Image Classification](https://arxiv.org/abs/2507.19822)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 研究发现，联邦学习中本地优化器和学习率的选择对性能影响大于FL方法本身，适当的边缘设备配置比算法复杂性更重要。


<details>
  <summary>Details</summary>
Motivation: 探讨联邦学习在医疗影像中因本地配置（如优化器和学习率）敏感而影响实际部署的问题。

Method: 通过结肠病理和血细胞分类任务，对多种FL方法进行基准测试，分析本地配置的影响。

Result: 本地优化器和学习率对性能影响显著，增加本地训练轮数可能提升或损害收敛性，取决于FL方法。

Conclusion: 有效的联邦学习更依赖于适当的边缘设备配置，而非算法复杂性。

Abstract: Federated Learning (FL) is a collaborative learning method that enables
decentralized model training while preserving data privacy. Despite its promise
in medical imaging, recent FL methods are often sensitive to local factors such
as optimizers and learning rates, limiting their robustness in practical
deployments. In this work, we revisit vanilla FL to clarify the impact of edge
device configurations, benchmarking recent FL methods on colorectal pathology
and blood cell classification task. We numerically show that the choice of
local optimizer and learning rate has a greater effect on performance than the
specific FL method. Moreover, we find that increasing local training epochs can
either enhance or impair convergence, depending on the FL method. These
findings indicate that appropriate edge-specific configuration is more crucial
than algorithmic complexity for achieving effective FL.

</details>


### [65] [Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks](https://arxiv.org/abs/2507.19684)
*Bermet Burkanova,Payam Jome Yazdian,Chuxuan Zhang,Trinity Evans,Paige Tuttösí,Angelica Lim*

Main category: cs.LG

TL;DR: CoMPAS3D是一个大型多样的即兴萨尔萨舞动作捕捉数据集，旨在为交互式、表达性人形AI提供测试平台。


<details>
  <summary>Details</summary>
Motivation: 探索人形AI如何通过触觉信号安全且创造性地与人类共舞，超越文本和语音交互，模拟双向连续的身体协调。

Method: 收集18名舞者3小时的萨尔萨舞数据，提供精细的动作注释，并开发多任务SalsaAgent模型。

Result: 数据集包含2800多个动作段，涵盖不同技能水平，模型能完成领导者/跟随者生成和双人舞生成任务。

Conclusion: CoMPAS3D为社交交互式人形AI和创意动作生成研究提供了重要资源。

Abstract: Imagine a humanoid that can safely and creatively dance with a human,
adapting to its partner's proficiency, using haptic signaling as a primary form
of communication. While today's AI systems excel at text or voice-based
interaction with large language models, human communication extends far beyond
text-it includes embodied movement, timing, and physical coordination. Modeling
coupled interaction between two agents poses a formidable challenge: it is
continuous, bidirectionally reactive, and shaped by individual variation. We
present CoMPAS3D, the largest and most diverse motion capture dataset of
improvised salsa dancing, designed as a challenging testbed for interactive,
expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa
dances performed by 18 dancers spanning beginner, intermediate, and
professional skill levels. For the first time, we provide fine-grained salsa
expert annotations, covering over 2,800 move segments, including move types,
combinations, execution errors and stylistic elements. We draw analogies
between partner dance communication and natural language, evaluating CoMPAS3D
on two benchmark tasks for synthetic humans that parallel key problems in
spoken language and dialogue processing: leader or follower generation with
proficiency levels (speaker or listener synthesis), and duet (conversation)
generation. Towards a long-term goal of partner dance with humans, we release
the dataset, annotations, and code, along with a multitask SalsaAgent model
capable of performing all benchmark tasks, alongside additional baselines to
encourage research in socially interactive embodied AI and creative, expressive
humanoid motion generation.

</details>


### [66] [ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings](https://arxiv.org/abs/2507.20426)
*Samiul Based Shuvo,Tasnia Binte Mamun,U Rajendra Acharya*

Main category: cs.LG

TL;DR: 提出了一种结合残差学习和胶囊网络的新深度学习框架ResCap-DBP，用于直接从蛋白质序列预测DNA结合蛋白，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DNA结合蛋白的识别对理解生物功能和疾病机制至关重要，但实验方法耗时且昂贵，需要高效的计算预测技术。

Method: 结合残差学习编码器和一维胶囊网络，利用膨胀卷积提取序列特征，胶囊层捕捉特征空间的层次和空间关系。

Result: 在多个基准数据集上表现优异，AUC得分高达98.0%，并保持平衡的敏感性和特异性。

Conclusion: ResCap-DBP展示了结合全局蛋白质表示和先进深度学习架构在DBP预测中的高效性和通用性。

Abstract: DNA-binding proteins (DBPs) are integral to gene regulation and cellular
processes, making their accurate identification essential for understanding
biological functions and disease mechanisms. Experimental methods for DBP
identification are time-consuming and costly, driving the need for efficient
computational prediction techniques. In this study, we propose a novel deep
learning framework, ResCap-DBP, that combines a residual learning-based encoder
with a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly
from raw protein sequences. Our architecture incorporates dilated convolutions
within residual blocks to mitigate vanishing gradient issues and extract rich
sequence features, while capsule layers with dynamic routing capture
hierarchical and spatial relationships within the learned feature space. We
conducted comprehensive ablation studies comparing global and local embeddings
from ProteinBERT and conventional one-hot encoding. Results show that
ProteinBERT embeddings substantially outperform other representations on large
datasets. Although one-hot encoding showed marginal advantages on smaller
datasets, such as PDB186, it struggled to scale effectively. Extensive
evaluations on four pairs of publicly available benchmark datasets demonstrate
that our model consistently outperforms current state-of-the-art methods. It
achieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On
independent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2%
and 83.3%, while maintaining competitive performance on larger datasets such as
PDB20000. Notably, the model maintains a well balanced sensitivity and
specificity across datasets. These results demonstrate the efficacy and
generalizability of integrating global protein representations with advanced
deep learning architectures for reliable and scalable DBP prediction in diverse
genomic contexts.

</details>


### [67] [KD-GAT: Combining Knowledge Distillation and Graph Attention Transformer for a Controller Area Network Intrusion Detection System](https://arxiv.org/abs/2507.19686)
*Robert Frenken,Sidra Ghayour Bhatti,Hanqin Zhang,Qadeer Ahmed*

Main category: cs.LG

TL;DR: 论文提出了一种名为KD-GAT的入侵检测框架，结合图注意力网络（GAT）和知识蒸馏（KD），以提高检测精度并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: CAN协议缺乏安全机制，易受网络攻击，因此需要高效的入侵检测方法。

Method: 将CAN流量表示为图，使用多层GAT作为教师模型，通过两阶段训练（监督预训练和知识蒸馏）训练小型学生模型。

Result: 在Car-Hacking和Car-Survival数据集上，学生模型分别达到99.97%和99.31%的准确率，但在can-train-and-test数据集上因类别不平衡表现较差。

Conclusion: KD-GAT框架在检测精度和计算效率上表现优异，但需进一步解决类别不平衡问题。

Abstract: The Controller Area Network (CAN) protocol is widely adopted for in-vehicle
communication but lacks inherent security mechanisms, making it vulnerable to
cyberattacks. This paper introduces KD-GAT, an intrusion detection framework
that combines Graph Attention Networks (GATs) with knowledge distillation (KD)
to enhance detection accuracy while reducing computational complexity. In our
approach, CAN traffic is represented as graphs using a sliding window to
capture temporal and relational patterns. A multi-layer GAT with jumping
knowledge aggregation acting as the teacher model, while a compact student
GAT--only 6.32% the size of the teacher--is trained via a two-phase process
involving supervised pretraining and knowledge distillation with both soft and
hard label supervision. Experiments on three benchmark datasets--Car-Hacking,
Car-Survival, and can-train-and-test demonstrate that both teacher and student
models achieve strong results, with the student model attaining 99.97% and
99.31% accuracy on Car-Hacking and Car-Survival, respectively. However,
significant class imbalance in can-train-and-test has led to reduced
performance for both models on this dataset. Addressing this imbalance remains
an important direction for future work.

</details>


### [68] [On Using the Shapley Value for Anomaly Localization: A Statistical Investigation](https://arxiv.org/abs/2507.21023)
*Rick S. Blum,Franziska Freytag*

Main category: cs.LG

TL;DR: 使用Shapley值进行传感器数据异常定位，实验表明固定项计算可降低复杂度且保持相同错误率，独立观测情况下结论普适。


<details>
  <summary>Details</summary>
Motivation: 探索Shapley值在传感器数据异常定位中的应用，以简化计算复杂度。

Method: 通过实验和数学证明，比较固定项与完整Shapley值计算的性能。

Result: 固定项计算在独立观测下性能与完整计算相同，且复杂度更低。

Conclusion: 固定项方法适用于独立观测，依赖观测情况需进一步研究。

Abstract: Recent publications have suggested using the Shapley value for anomaly
localization for sensor data systems. Using a reasonable mathematical anomaly
model for full control, experiments indicate that using a single fixed term in
the Shapley value calculation achieves a lower complexity anomaly localization
test, with the same probability of error, as a test using the Shapley value for
all cases tested. A proof demonstrates these conclusions must be true for all
independent observation cases. For dependent observation cases, no proof is
available.

</details>


### [69] [NAICS-Aware Graph Neural Networks for Large-Scale POI Co-visitation Prediction: A Multi-Modal Dataset and Methodology](https://arxiv.org/abs/2507.19697)
*Yazeed Alrubyli,Omar Alomeir,Abrar Wafa,Diána Hidvégi,Hend Alrasheed,Mohsen Bahrami*

Main category: cs.LG

TL;DR: 论文提出了一种结合商业分类知识的图神经网络（NAICS-aware GraphSAGE），用于预测大规模场所的共访模式，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 理解人们访问一个商业场所后的去向对城市规划、零售分析和基于位置的服务至关重要，但传统方法因数据稀疏性和空间与商业关系的复杂性而受限。

Method: 提出NAICS-aware GraphSAGE，通过可学习的嵌入整合商业分类知识，结合空间、时间和社会经济特征，实现端到端预测。

Result: 在包含9490万共访记录的POI-Graph数据集上，R-squared值从0.243提升至0.625（提升157%），排名质量显著提高（NDCG@10提升32%）。

Conclusion: 商业语义（通过行业代码捕获）为共访模式预测提供了关键信号，该方法在大规模数据集上表现出色。

Abstract: Understanding where people go after visiting one business is crucial for
urban planning, retail analytics, and location-based services. However,
predicting these co-visitation patterns across millions of venues remains
challenging due to extreme data sparsity and the complex interplay between
spatial proximity and business relationships. Traditional approaches using only
geographic distance fail to capture why coffee shops attract different customer
flows than fine dining restaurants, even when co-located. We introduce
NAICS-aware GraphSAGE, a novel graph neural network that integrates business
taxonomy knowledge through learnable embeddings to predict population-scale
co-visitation patterns. Our key insight is that business semantics, captured
through detailed industry codes, provide crucial signals that pure spatial
models cannot explain. The approach scales to massive datasets (4.2 billion
potential venue pairs) through efficient state-wise decomposition while
combining spatial, temporal, and socioeconomic features in an end-to-end
framework. Evaluated on our POI-Graph dataset comprising 94.9 million
co-visitation records across 92,486 brands and 48 US states, our method
achieves significant improvements over state-of-the-art baselines: the
R-squared value increases from 0.243 to 0.625 (a 157 percent improvement), with
strong gains in ranking quality (32 percent improvement in NDCG at 10).

</details>


### [70] [Disjoint Generative Models](https://arxiv.org/abs/2507.19700)
*Anton Danholt Lautrup,Muhammad Rajabinasab,Tobias Hyrup,Arthur Zimek,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: 提出了一种通过分离生成模型生成跨截面合成数据集的新框架，显著提高隐私性且效用损失低。


<details>
  <summary>Details</summary>
Motivation: 解决传统生成模型在隐私保护和数据合成方面的局限性，探索更高效和可行的数据生成方法。

Method: 将数据集划分为不相交子集，分别输入独立的生成模型，后通过无共同变量的连接操作合并结果。

Result: 案例研究表明该框架在隐私性和效用之间取得良好平衡，同时支持混合模型合成。

Conclusion: 分离生成模型框架为数据合成提供了隐私保护和高效的新途径。

Abstract: We propose a new framework for generating cross-sectional synthetic datasets
via disjoint generative models. In this paradigm, a dataset is partitioned into
disjoint subsets that are supplied to separate instances of generative models.
The results are then combined post hoc by a joining operation that works in the
absence of common variables/identifiers. The success of the framework is
demonstrated through several case studies and examples on tabular data that
helps illuminate some of the design choices that one may make. The principal
benefit of disjoint generative models is significantly increased privacy at
only a low utility cost. Additional findings include increased effectiveness
and feasibility for certain model types and the possibility for mixed-model
synthesis.

</details>


### [71] [Beyond Nearest Neighbors: Semantic Compression and Graph-Augmented Retrieval for Enhanced Vector Search](https://arxiv.org/abs/2507.19715)
*Rahul Raja,Arpita Vats*

Main category: cs.LG

TL;DR: 论文提出了一种新的检索范式——语义压缩，通过子模优化和信息几何原则选择具有代表性的向量集，以解决传统近似最近邻搜索在语义多样性和上下文丰富性上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统近似最近邻搜索（ANN）虽然有效，但往往返回语义冗余的结果，缺乏多样性，无法满足检索增强生成（RAG）、多跳问答等应用的需求。

Method: 提出语义压缩方法，结合子模优化和信息几何原则，并通过图增强向量检索（如kNN或知识图谱）实现多跳、上下文感知的搜索。

Result: 理论分析表明，在高维集中情况下，基于邻近的检索存在局限性，而图结构可以改善语义覆盖。

Conclusion: 研究为以意义为中心的向量搜索系统奠定了基础，强调混合索引、多样性感知查询和结构化语义检索。

Abstract: Vector databases typically rely on approximate nearest neighbor (ANN) search
to retrieve the top-k closest vectors to a query in embedding space. While
effective, this approach often yields semantically redundant results, missing
the diversity and contextual richness required by applications such as
retrieval-augmented generation (RAG), multi-hop QA, and memory-augmented
agents. We introduce a new retrieval paradigm: semantic compression, which aims
to select a compact, representative set of vectors that captures the broader
semantic structure around a query. We formalize this objective using principles
from submodular optimization and information geometry, and show that it
generalizes traditional top-k retrieval by prioritizing coverage and diversity.
To operationalize this idea, we propose graph-augmented vector retrieval, which
overlays semantic graphs (e.g., kNN or knowledge-based links) atop vector
spaces to enable multi-hop, context-aware search. We theoretically analyze the
limitations of proximity-based retrieval under high-dimensional concentration
and highlight how graph structures can improve semantic coverage. Our work
outlines a foundation for meaning-centric vector search systems, emphasizing
hybrid indexing, diversity-aware querying, and structured semantic retrieval.
We make our implementation publicly available to foster future research in this
area.

</details>


### [72] [Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning](https://arxiv.org/abs/2507.19737)
*Yinzhou Tang,Huandong Wang,Xiaochen Fan,Yong Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为DisasterMobLLM的框架，用于预测灾害场景下的人类移动性，通过结合LLM和RAG技术，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 城市化与气候变化加剧了城市对自然灾害的脆弱性，现有移动性预测模型在灾害场景下表现不佳，亟需改进。

Method: 框架包括RAG增强的意图预测器、基于LLM的意图精炼器和意图调制的定位预测器，整合了灾害对移动意图的影响知识。

Result: 实验表明，DisasterMobLLM在Acc@1和F1-score上分别提升了32.8%和35.0%。

Conclusion: DisasterMobLLM有效解决了灾害场景下人类移动性预测的挑战，为灾害预警和救援资源分配提供了支持。

Abstract: The vulnerability of cities to natural disasters has increased with
urbanization and climate change, making it more important to predict human
mobility in the disaster scenarios for downstream tasks including
location-based early disaster warning and pre-allocating rescue resources, etc.
However, existing human mobility prediction models are mainly designed for
normal scenarios, and fail to adapt to disaster scenarios due to the shift of
human mobility patterns under disaster. To address this issue, we introduce
\textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios
that can be integrated into existing deep mobility prediction methods by
leveraging LLMs to model the mobility intention and transferring the common
knowledge of how different disasters affect mobility intentions between cities.
This framework utilizes a RAG-Enhanced Intention Predictor to forecast the next
intention, refines it with an LLM-based Intention Refiner, and then maps the
intention to an exact location using an Intention-Modulated Location Predictor.
Extensive experiments illustrate that DisasterMobLLM can achieve a 32.8\%
improvement in terms of Acc@1 and a 35.0\% improvement in terms of the F1-score
of predicting immobility compared to the baselines. The code is available at
https://github.com/tsinghua-fib-lab/DisasterMobLLM.

</details>


### [73] [Modeling enzyme temperature stability from sequence segment perspective](https://arxiv.org/abs/2507.19755)
*Ziqi Zhang,Shiheng Chen,Runze Yang,Zhisheng Wei,Wei Zhang,Lei Wang,Zhanzhi Liu,Fengshan Zhang,Jing Wu,Xiaoyong Pan,Hongbin Shen,Longbing Cao,Zhaohong Deng*

Main category: cs.LG

TL;DR: 论文提出了一种名为Segment Transformer的深度学习框架，用于高效预测酶的温度稳定性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 实验测定酶的耐热性耗时且昂贵，现有计算方法受限于数据不足和不平衡分布，因此需要开发更高效的方法。

Method: 利用精心整理的温度稳定性数据集，开发了Segment Transformer模型，该模型基于蛋白质序列中不同区域对热行为贡献不均的生物学观察。

Result: 模型在RMSE、MAE和相关性指标上达到先进水平，实验验证显示通过17个突变使酶的相对活性提高了1.64倍。

Conclusion: Segment Transformer为酶的耐热性预测和工程化提供了高效且准确的工具。

Abstract: Developing enzymes with desired thermal properties is crucial for a wide
range of industrial and research applications, and determining temperature
stability is an essential step in this process. Experimental determination of
thermal parameters is labor-intensive, time-consuming, and costly. Moreover,
existing computational approaches are often hindered by limited data
availability and imbalanced distributions. To address these challenges, we
introduce a curated temperature stability dataset designed for model
development and benchmarking in enzyme thermal modeling. Leveraging this
dataset, we present the \textit{Segment Transformer}, a novel deep learning
framework that enables efficient and accurate prediction of enzyme temperature
stability. The model achieves state-of-the-art performance with an RMSE of
24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33,
respectively. These results highlight the effectiveness of incorporating
segment-level representations, grounded in the biological observation that
different regions of a protein sequence contribute unequally to thermal
behavior. As a proof of concept, we applied the Segment Transformer to guide
the engineering of a cutinase enzyme. Experimental validation demonstrated a
1.64-fold improvement in relative activity following heat treatment, achieved
through only 17 mutations and without compromising catalytic function.

</details>


### [74] [Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation](https://arxiv.org/abs/2507.19771)
*Xin Zhang,Lissette Iturburu,Juan Nicolas Villamizar,Xiaoyu Liu,Manuel Salmeron,Shirley J. Dyke,Julio Ramirez*

Main category: cs.LG

TL;DR: 论文提出了一种基于生成式AI的方法，利用大型语言模型（LLM）和检索增强生成（RAG）技术，将自然语言描述高效转换为AutoCAD结构图纸。


<details>
  <summary>Details</summary>
Motivation: 结构图纸在工程领域至关重要，但生成过程耗时耗力。现有软件能力虽强，但仍需工程师手动操作，效率低下。

Method: 采用LLM结合RAG技术，从外部资源获取信息，提升模型准确性。模型能理解自然语言描述，提取关键信息并生成AutoCAD代码。

Result: 该方法实现了自然语言描述到AutoCAD图纸的直接转换，显著减少了手动绘图的工作量。

Conclusion: 该方法简化了结构图纸生成流程，提高了工程师的设计迭代效率。

Abstract: Structural drawings are widely used in many fields, e.g., mechanical
engineering, civil engineering, etc. In civil engineering, structural drawings
serve as the main communication tool between architects, engineers, and
builders to avoid conflicts, act as legal documentation, and provide a
reference for future maintenance or evaluation needs. They are often organized
using key elements such as title/subtitle blocks, scales, plan views, elevation
view, sections, and detailed sections, which are annotated with standardized
symbols and line types for interpretation by engineers and contractors. Despite
advances in software capabilities, the task of generating a structural drawing
remains labor-intensive and time-consuming for structural engineers. Here we
introduce a novel generative AI-based method for generating structural drawings
employing a large language model (LLM) agent. The method incorporates a
retrieval-augmented generation (RAG) technique using externally-sourced facts
to enhance the accuracy and reliability of the language model. This method is
capable of understanding varied natural language descriptions, processing these
to extract necessary information, and generating code to produce the desired
structural drawing in AutoCAD. The approach developed, demonstrated and
evaluated herein enables the efficient and direct conversion of a structural
drawing's natural language description into an AutoCAD drawing, significantly
reducing the workload compared to current working process associated with
manual drawing production, facilitating the typical iterative process of
engineers for expressing design ideas in a simplified way.

</details>


### [75] [AI-Based Clinical Rule Discovery for NMIBC Recurrence through Tsetlin Machines](https://arxiv.org/abs/2507.19803)
*Saram Abbas,Naeem Soomro,Rishad Shafik,Rakesh Heer,Kabita Adhikari*

Main category: cs.LG

TL;DR: 提出了一种基于Tsetlin Machine（TM）的可解释AI模型，用于预测非肌层浸润性膀胱癌（NMIBC）的复发风险，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 膀胱癌的高复发率和现有临床工具的不可靠性，尤其是对中等风险病例的预测不足。

Method: 使用Tsetlin Machine（TM）构建可解释的AI模型，基于PHOTO试验数据集（n=330）进行测试。

Result: TM的F1得分为0.80，优于XGBoost（0.78）、逻辑回归（0.60）和EORTC（0.42），并提供透明预测逻辑。

Conclusion: TM是一种准确且透明的决策支持工具，适合实际应用。

Abstract: Bladder cancer claims one life every 3 minutes worldwide. Most patients are
diagnosed with non-muscle-invasive bladder cancer (NMIBC), yet up to 70% recur
after treatment, triggering a relentless cycle of surgeries, monitoring, and
risk of progression. Clinical tools like the EORTC risk tables are outdated and
unreliable - especially for intermediate-risk cases.
  We propose an interpretable AI model using the Tsetlin Machine (TM), a
symbolic learner that outputs transparent, human-readable logic. Tested on the
PHOTO trial dataset (n=330), TM achieved an F1-score of 0.80, outperforming
XGBoost (0.78), Logistic Regression (0.60), and EORTC (0.42). TM reveals the
exact clauses behind each prediction, grounded in clinical features like tumour
count, surgeon experience, and hospital stay - offering accuracy and full
transparency. This makes TM a powerful, trustworthy decision-support tool ready
for real-world adoption.

</details>


### [76] [GNSP: Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning](https://arxiv.org/abs/2507.19839)
*Tiantian Peng,Yuyang Liu,Shuo Yang,Qiuhe Hong,YongHong Tian*

Main category: cs.LG

TL;DR: 论文提出GNSP方法，通过梯度零空间投影解决CLIP在持续学习中的灾难性遗忘问题，并结合知识蒸馏和模态对齐损失保持其零样本能力。


<details>
  <summary>Details</summary>
Motivation: CLIP在持续学习过程中会出现灾难性遗忘和嵌入对齐退化，影响其零样本泛化能力。

Method: 提出GNSP方法，将任务特定梯度投影到先前知识的零空间，避免干扰；结合知识蒸馏和模态对齐损失保持嵌入空间结构。

Result: 在MTIL基准测试中取得SOTA性能，同时保持了CLIP的模态间隙和跨模态检索性能。

Conclusion: GNSP方法有效解决了CLIP在持续学习中的问题，保持了其多模态嵌入空间的鲁棒性。

Abstract: Contrastive Language-Image Pretraining has demonstrated remarkable zero-shot
generalization by aligning visual and textual modalities in a shared embedding
space. However, when continuously fine-tuned on diverse tasks, CLIP suffers
from catastrophic forgetting and degradation of its embedding alignment,
undermining its zero-shot capabilities. In this work, we propose Gradient Null
Space Projection (GNSP), an efficient continual learning method that projects
task-specific gradients onto the null space of previously learned knowledge.
This orthogonal projection mathematically prevents interference with previous
tasks without relying on rehearsal or architectural modification. Furthermore,
to preserve the inherent generalization property of CLIP, we introduce
knowledge distillation and combine it with a modality alignment preservation
loss inspired by CLIP pre-training to stabilize the structure of the multimodal
embedding space during fine-tuning. On the MTIL benchmark consisting of 11
tasks, our method achieved SOTA performance on both the Average and Last key
metrics. More importantly, experiments show that our method successfully
maintains the original modality gap and cross-modal retrieval performance of
CLIP, confirming its effectiveness in maintaining a robust visual-language
space throughout the continual learning process.

</details>


### [77] [A Scalable and High Availability Solution for Recommending Resolutions to Problem Tickets](https://arxiv.org/abs/2507.19846)
*Harish S,Chetana K Nayak,Joy Bose*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的解决方案，结合聚类、监督学习和高级NLP模型，有效解决电信领域问题工单的复杂挑战。


<details>
  <summary>Details</summary>
Motivation: 解决工单处理中的数据漂移、缺失数据和文本相似性等问题，提高工单解决的效率和准确性。

Method: 采用聚类、监督学习（LDA、Siamese网络、One-shot学习）和索引嵌入技术，结合实时仪表盘和Kubernetes部署。

Result: 在开源和专有数据集上展示了高预测准确性。

Conclusion: 提出的方法在复杂工单处理中表现出色，具有实际应用价值。

Abstract: Resolution of incidents or problem tickets is a common theme in service
industries in any sector, including billing and charging systems in telecom
domain. Machine learning can help to identify patterns and suggest resolutions
for the problem tickets, based on patterns in the historical data of the
tickets. However, this process may be complicated due to a variety of phenomena
such as data drift and issues such as missing data, lack of data pertaining to
resolutions of past incidents, too many similar sounding resolutions due to
free text and similar sounding text. This paper proposes a robust ML-driven
solution employing clustering, supervised learning, and advanced NLP models to
tackle these challenges effectively. Building on previous work, we demonstrate
clustering-based resolution identification, supervised classification with LDA,
Siamese networks, and One-shot learning, Index embedding. Additionally, we
present a real-time dashboard and a highly available Kubernetes-based
production deployment. Our experiments with both the open-source Bitext
customer-support dataset and proprietary telecom datasets demonstrate high
prediction accuracy.

</details>


### [78] [Agentic Reinforced Policy Optimization](https://arxiv.org/abs/2507.19849)
*Guanting Dong,Hangyu Mao,Kai Ma,Licheng Bao,Yifei Chen,Zhongyuan Wang,Zhongxia Chen,Jiazhen Du,Huiyang Wang,Fuzheng Zhang,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: 论文提出了一种名为ARPO的新型强化学习算法，用于训练多轮工具交互的LLM代理，通过动态平衡全局轨迹采样和步骤级采样，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习算法未能有效平衡LLM的长时推理能力和多轮工具交互能力，需要一种新方法来解决这一问题。

Method: ARPO结合了基于熵的自适应滚动机制和优势归因估计，动态调整采样策略以应对工具使用后的高不确定性。

Result: 在13个挑战性基准测试中，ARPO表现优于轨迹级强化学习算法，且仅需现有方法一半的工具使用预算。

Conclusion: ARPO为LLM代理在动态环境中的实时对齐提供了可扩展的解决方案。

Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has
demonstrated its effectiveness in harnessing the potential of large language
models (LLMs) for single-turn reasoning tasks. In realistic reasoning
scenarios, LLMs can often utilize external tools to assist in task-solving
processes. However, current RL algorithms inadequately balance the models'
intrinsic long-horizon reasoning capabilities and their proficiency in
multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced
Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training
multi-turn LLM-based agents. Through preliminary experiments, we observe that
LLMs tend to exhibit highly uncertain behavior, characterized by an increase in
the entropy distribution of generated tokens, immediately following
interactions with external tools. Motivated by this observation, ARPO
incorporates an entropy-based adaptive rollout mechanism, dynamically balancing
global trajectory sampling and step-level sampling, thereby promoting
exploration at steps with high uncertainty after tool usage. By integrating an
advantage attribution estimation, ARPO enables LLMs to internalize advantage
differences in stepwise tool-use interactions. Our experiments across 13
challenging benchmarks in computational reasoning, knowledge reasoning, and
deep search domains demonstrate ARPO's superiority over trajectory-level RL
algorithms. Remarkably, ARPO achieves improved performance using only half of
the tool-use budget required by existing methods, offering a scalable solution
for aligning LLM-based agents with real-time dynamic environments. Our code and
datasets are released at https://github.com/dongguanting/ARPO

</details>


### [79] [Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning](https://arxiv.org/abs/2507.19855)
*Aditya Sharma,Linh Nguyen,Ananya Gupta,Chengyu Wang,Chiamaka Adebayo,Jakub Kowalski*

Main category: cs.LG

TL;DR: CWMI框架通过嵌入因果物理模块和新的训练目标，显著提升了LLM在零样本物理推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs缺乏对物理动态的直观理解，限制了其在需要因果推理的实际场景中的有效性。

Method: 引入Causal World Model Induction (CWMI)框架，包括Causal Physics Module (CPM)和Causal Intervention Loss训练目标，从多模态数据中学习因果关系。

Result: CWMI在PIQA和PhysiCa-Bench数据集上显著优于现有LLMs。

Conclusion: 诱导因果世界模型是构建更可靠、通用AI系统的关键步骤。

Abstract: Large Language Models (LLMs), despite their advanced linguistic capabilities,
fundamentally lack an intuitive understanding of physical dynamics, which
limits their effectiveness in real-world scenarios that require causal
reasoning. In this paper, we introduce Causal World Model Induction (CWMI), a
novel framework designed to embed an explicit model of causal physics within an
LLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a
new training objective called Causal Intervention Loss, encouraging the model
to learn cause-and-effect relationships from multimodal data. By training the
model to predict the outcomes of hypothetical interventions instead of merely
capturing statistical correlations, CWMI develops a robust internal
representation of physical laws. Experimental results show that CWMI
significantly outperforms state-of-the-art LLMs on zero-shot physical reasoning
tasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench
dataset. These findings demonstrate that inducing a causal world model is a
critical step toward more reliable and generalizable AI systems.

</details>


### [80] [CLoRA: Parameter-Efficient Continual Learning with Low-Rank Adaptation](https://arxiv.org/abs/2507.19887)
*Shishir Muralidhara,Didier Stricker,René Schuster*

Main category: cs.LG

TL;DR: CLoRA利用低秩适应（LoRA）方法，在资源受限的环境中实现高效的持续学习，减少计算需求。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，持续学习的计算资源限制比数据访问限制更重要，现有方法需要重新训练整个模型，计算成本高。

Method: 采用低秩适应（LoRA）方法，仅调整少量参数，跨任务共享同一组参数。

Result: CLoRA性能与基线方法相当或更优，显著降低硬件需求。

Conclusion: CLoRA适合资源受限环境，强调评估持续学习方法时需考虑资源效率。

Abstract: In the past, continual learning (CL) was mostly concerned with the problem of
catastrophic forgetting in neural networks, that arises when incrementally
learning a sequence of tasks. Current CL methods function within the confines
of limited data access, without any restrictions imposed on computational
resources. However, in real-world scenarios, the latter takes precedence as
deployed systems are often computationally constrained. A major drawback of
most CL methods is the need to retrain the entire model for each new task. The
computational demands of retraining large models can be prohibitive, limiting
the applicability of CL in environments with limited resources. Through CLoRA,
we explore the applicability of Low-Rank Adaptation (LoRA), a
parameter-efficient fine-tuning method for class-incremental semantic
segmentation. CLoRA leverages a small set of parameters of the model and uses
the same set for learning across all tasks. Results demonstrate the efficacy of
CLoRA, achieving performance on par with and exceeding the baseline methods. We
further evaluate CLoRA using NetScore, underscoring the need to factor in
resource efficiency and evaluate CL methods beyond task performance. CLoRA
significantly reduces the hardware requirements for training, making it
well-suited for CL in resource-constrained environments after deployment.

</details>


### [81] [A Survey on Generative Model Unlearning: Fundamentals, Taxonomy, Evaluation, and Future Direction](https://arxiv.org/abs/2507.19894)
*Xiaohua Feng,Jiaming Zhang,Fengyuan Yu,Chengye Wang,Li Zhang,Kaixiang Li,Yuyuan Li,Chaochao Chen,Jianwei Yin*

Main category: cs.LG

TL;DR: 本文综述了生成模型遗忘（GenMU）的研究现状，提出了统一的分析框架，并探讨了其实际应用和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，隐私问题日益突出，需要系统化的遗忘技术框架以整合现有研究。

Method: 通过全面回顾GenMU研究，提出分类遗忘目标、方法策略和评估指标的统一框架，并探讨相关技术联系。

Result: 建立了GenMU的系统化分类框架，明确了其实际应用价值，并指出了未来研究方向。

Conclusion: GenMU研究需进一步统一标准，未来应关注技术整合和实际应用。

Abstract: With the rapid advancement of generative models, associated privacy concerns
have attracted growing attention. To address this, researchers have begun
adapting machine unlearning techniques from traditional classification models
to generative settings. Although notable progress has been made in this area, a
unified framework for systematically organizing and integrating existing work
is still lacking. The substantial differences among current studies in terms of
unlearning objectives and evaluation protocols hinder the objective and fair
comparison of various approaches. While some studies focus on specific types of
generative models, they often overlook the commonalities and systematic
characteristics inherent in Generative Model Unlearning (GenMU). To bridge this
gap, we provide a comprehensive review of current research on GenMU and propose
a unified analytical framework for categorizing unlearning objectives,
methodological strategies, and evaluation metrics. In addition, we explore the
connections between GenMU and related techniques, including model editing,
reinforcement learning from human feedback, and controllable generation. We
further highlight the potential practical value of unlearning techniques in
real-world applications. Finally, we identify key challenges and outline future
research directions aimed at laying a solid foundation for further advancements
in this field. We consistently maintain the related open-source materials at
https://github.com/caxLee/Generative-model-unlearning-survey.

</details>


### [82] [Who Owns This Sample: Cross-Client Membership Inference Attack in Federated Graph Neural Networks](https://arxiv.org/abs/2507.19964)
*Kunhao Li,Di Wu,Jun Bai,Jing Xu,Lei Yang,Ziyi Zhang,Yiliao Song,Wencheng Yang,Taotao Cai,Yan Li*

Main category: cs.LG

TL;DR: 该论文研究了联邦图神经网络（FedGNNs）中的跨客户端成员推理攻击（CC-MIA），揭示了节点分类任务中样本与客户端归属的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 随着图神经网络（GNNs）在联邦学习（FL）中的广泛应用，隐私威胁因图结构与分散训练的交互而增加，需研究新的攻击方式。

Method: 设计了利用FedGNNs聚合行为、梯度更新和嵌入相似性的攻击框架，通过训练轮次将样本链接到源客户端。

Result: 在多种图数据集和实际FL设置下，攻击方法在成员推理和所有权识别上表现优异。

Conclusion: 研究揭示了联邦图学习中客户端身份泄露的新隐私威胁，呼吁设计更具鲁棒性的GNN以应对此类风险。

Abstract: Graph-structured data is prevalent in many real-world applications, including
social networks, financial systems, and molecular biology. Graph Neural
Networks (GNNs) have become the de facto standard for learning from such data
due to their strong representation capabilities. As GNNs are increasingly
deployed in federated learning (FL) settings to preserve data locality and
privacy, new privacy threats arise from the interaction between graph
structures and decentralized training. In this paper, we present the first
systematic study of cross-client membership inference attacks (CC-MIA) against
node classification tasks of federated GNNs (FedGNNs), where a malicious client
aims to infer which client owns the given data. Unlike prior
centralized-focused work that focuses on whether a sample was included in
training, our attack targets sample-to-client attribution, a finer-grained
privacy risk unique to federated settings. We design a general attack framework
that exploits FedGNNs' aggregation behaviors, gradient updates, and embedding
proximity to link samples to their source clients across training rounds. We
evaluate our attack across multiple graph datasets under realistic FL setups.
Results show that our method achieves high performance on both membership
inference and ownership identification. Our findings highlight a new privacy
threat in federated graph learning-client identity leakage through structural
and model-level cues, motivating the need for attribution-robust GNN design.

</details>


### [83] [Robust Taxi Fare Prediction Under Noisy Conditions: A Comparative Study of GAT, TimesNet, and XGBoost](https://arxiv.org/abs/2507.20008)
*Padmavathi Moorthy*

Main category: cs.LG

TL;DR: 该研究比较了三种机器学习模型（GAT、XGBoost和TimesNet）在出租车费用预测中的表现，分析了数据质量对模型性能的影响，并探讨了预处理策略。


<details>
  <summary>Details</summary>
Motivation: 精确的费用预测对网约车平台和城市交通系统至关重要，研究旨在评估不同模型在真实数据集中的表现。

Method: 使用包含5500万条记录的真实数据集，比较GAT、XGBoost和TimesNet的预测能力，并分析数据预处理（如KNN插补、高斯噪声注入和自编码器去噪）的影响。

Result: 研究揭示了经典模型与深度学习模型在现实条件下的关键差异，为构建稳健且可扩展的城市费用预测系统提供了实用指南。

Conclusion: 研究为城市交通系统中的费用预测提供了模型选择和预处理的实用建议，强调了数据质量对模型性能的重要性。

Abstract: Precise fare prediction is crucial in ride-hailing platforms and urban
mobility systems. This study examines three machine learning models-Graph
Attention Networks (GAT), XGBoost, and TimesNet to evaluate their predictive
capabilities for taxi fares using a real-world dataset comprising over 55
million records. Both raw (noisy) and denoised versions of the dataset are
analyzed to assess the impact of data quality on model performance. The study
evaluated the models along multiple axes, including predictive accuracy,
calibration, uncertainty estimation, out-of-distribution (OOD) robustness, and
feature sensitivity. We also explore pre-processing strategies, including KNN
imputation, Gaussian noise injection, and autoencoder-based denoising. The
study reveals critical differences between classical and deep learning models
under realistic conditions, offering practical guidelines for building robust
and scalable models in urban fare prediction systems.

</details>


### [84] [FedSWA: Improving Generalization in Federated Learning with Highly Heterogeneous Data via Momentum-Based Stochastic Controlled Weight Averaging](https://arxiv.org/abs/2507.20016)
*Liu junkang,Yuanyuan Liu,Fanhua Shang,Hongying Liu,Jin Liu,Wei Feng*

Main category: cs.LG

TL;DR: 论文研究了联邦学习中数据异构性对泛化能力的影响，提出了两种新算法（FedSWA和FedMoSWA），在理论和实验上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探讨联邦学习（如FedSAM）在高度异构数据下的泛化问题，发现其表现不佳，需改进。

Method: 提出FedSWA（基于随机权重平均）和FedMoSWA（动量控制权重平均）算法，以寻找更平坦的最小值并优化模型对齐。

Result: 理论证明FedMoSWA的优化和泛化误差更小；实验在CIFAR10/100和Tiny ImageNet上验证了其优越性。

Conclusion: FedSWA和FedMoSWA在异构数据下表现更优，为联邦学习提供了新思路。

Abstract: For federated learning (FL) algorithms such as FedSAM, their generalization
capability is crucial for real-word applications. In this paper, we revisit the
generalization problem in FL and investigate the impact of data heterogeneity
on FL generalization. We find that FedSAM usually performs worse than FedAvg in
the case of highly heterogeneous data, and thus propose a novel and effective
federated learning algorithm with Stochastic Weight Averaging (called
\texttt{FedSWA}), which aims to find flatter minima in the setting of highly
heterogeneous data. Moreover, we introduce a new momentum-based stochastic
controlled weight averaging FL algorithm (\texttt{FedMoSWA}), which is designed
to better align local and global models.
  Theoretically, we provide both convergence analysis and generalization bounds
for \texttt{FedSWA} and \texttt{FedMoSWA}. We also prove that the optimization
and generalization errors of \texttt{FedMoSWA} are smaller than those of their
counterparts, including FedSAM and its variants. Empirically, experimental
results on CIFAR10/100 and Tiny ImageNet demonstrate the superiority of the
proposed algorithms compared to their counterparts. Open source code at:
https://github.com/junkangLiu0/FedSWA.

</details>


### [85] [$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning](https://arxiv.org/abs/2507.20051)
*Weicong Chen,Vikash Singh,Zahra Rahmani,Debargha Ganguly,Mohsen Hariri,Vipin Chaudhary*

Main category: cs.LG

TL;DR: $K^4$是一种无监督、独立于解析器的高性能在线日志异常检测框架，通过四维描述符实现快速准确检测。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法速度慢、依赖易出错的解析，且评估协议不现实。

Method: $K^4$将日志嵌入转换为四维描述符（精度、召回率、密度、覆盖率），基于k-NN统计实现轻量级检测。

Result: $K^4$在在线评估中表现优异（AUROC: 0.995-0.999），速度快（训练<4秒，推理低至4μs）。

Conclusion: $K^4$在性能和效率上显著优于现有方法，适用于实时日志异常检测。

Abstract: Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on
error-prone parsing, and use unrealistic evaluation protocols. We introduce
$K^4$, an unsupervised and parser-independent framework for high-performance
online detection. $K^4$ transforms arbitrary log embeddings into compact
four-dimensional descriptors (Precision, Recall, Density, Coverage) using
efficient k-nearest neighbor (k-NN) statistics. These descriptors enable
lightweight detectors to accurately score anomalies without retraining. Using a
more realistic online evaluation protocol, $K^4$ sets a new state-of-the-art
(AUROC: 0.995-0.999), outperforming baselines by large margins while being
orders of magnitude faster, with training under 4 seconds and inference as low
as 4 $\mu$s.

</details>


### [86] [What Can Grokking Teach Us About Learning Under Nonstationarity?](https://arxiv.org/abs/2507.20057)
*Clare Lyle,Gharda Sokar,Razvan Pascanu,Andras Gyorgy*

Main category: cs.LG

TL;DR: 论文探讨了持续学习中神经网络的特征学习动态如何帮助克服早期数据的偏见，并提出了一种通过调整有效学习率来促进特征学习的方法。


<details>
  <summary>Details</summary>
Motivation: 持续学习中，神经网络容易因早期数据偏见（primacy bias）而难以适应后续任务，而特征学习动态在解决这一问题中可能发挥关键作用。

Method: 提出通过增加有效学习率（参数与更新范数的比率）来诱导特征学习动态，从而改善泛化能力。

Result: 该方法在多种场景（如grokking、神经网络预热训练和强化学习任务）中均能促进特征学习并提升泛化性能。

Conclusion: 特征学习动态是解决持续学习中早期数据偏见的有效途径，调整有效学习率是一种简单且通用的方法。

Abstract: In continual learning problems, it is often necessary to overwrite components
of a neural network's learned representation in response to changes in the data
stream; however, neural networks often exhibit \primacy bias, whereby early
training data hinders the network's ability to generalize on later tasks. While
feature-learning dynamics of nonstationary learning problems are not well
studied, the emergence of feature-learning dynamics is known to drive the
phenomenon of grokking, wherein neural networks initially memorize their
training data and only later exhibit perfect generalization. This work
conjectures that the same feature-learning dynamics which facilitate
generalization in grokking also underlie the ability to overwrite previous
learned features as well, and methods which accelerate grokking by facilitating
feature-learning dynamics are promising candidates for addressing primacy bias
in non-stationary learning problems. We then propose a straightforward method
to induce feature-learning dynamics as needed throughout training by increasing
the effective learning rate, i.e. the ratio between parameter and update norms.
We show that this approach both facilitates feature-learning and improves
generalization in a variety of settings, including grokking, warm-starting
neural network training, and reinforcement learning tasks.

</details>


### [87] [ModShift: Model Privacy via Designed Shifts](https://arxiv.org/abs/2507.20060)
*Nomaan A. Kherani,Urbashi Mitra*

Main category: cs.LG

TL;DR: 提出一种通过参数偏移保护联邦学习中模型隐私的方法，利用Fisher信息矩阵使窃听者难以估计模型参数，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中模型隐私泄露问题，防止窃听者通过模型更新推断敏感信息。

Method: 将模型学习视为参数估计问题，通过参数偏移使Fisher信息矩阵奇异化，同时安全共享偏移以保持模型准确性。

Result: 方案在数值实验中表现出更高的模型偏移效果，且所需带宽更少。

Conclusion: 该方法有效保护隐私，同时通过收敛测试验证了其安全性。

Abstract: In this paper, shifts are introduced to preserve model privacy against an
eavesdropper in federated learning. Model learning is treated as a parameter
estimation problem. This perspective allows us to derive the Fisher Information
matrix of the model updates from the shifted updates and drive them to
singularity, thus posing a hard estimation problem for Eve. The shifts are
securely shared with the central server to maintain model accuracy at the
server and participating devices. A convergence test is proposed to detect if
model updates have been tampered with and we show that our scheme passes this
test. Numerical results show that our scheme achieves a higher model shift when
compared to a noise injection scheme while requiring a lesser bandwidth secret
channel.

</details>


### [88] [Strategic Filtering for Content Moderation: Free Speech or Free of Distortion?](https://arxiv.org/abs/2507.20061)
*Saba Ahmadi,Avrim Blum,Haifeng Xu,Fan Yao*

Main category: cs.LG

TL;DR: 论文探讨了社交媒体用户生成内容（UGC）的监管问题，提出通过机制设计优化言论自由与内容操纵之间的平衡，并提供了近似最优解的实用方法及泛化保证。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的用户生成内容容易受到煽动和操纵，需要有效监管。自动化内容审核虽能过滤违规内容，但会引发用户的策略性回应，需平衡言论自由与减少社会扭曲。

Method: 通过机制设计优化言论自由与内容操纵的权衡，提出近似最优解的实用方法，并提供泛化保证以确定所需离线数据量。

Result: 尽管最优权衡问题是NP难的，但提出的方法能有效近似最优解，并量化了所需离线数据的规模。

Conclusion: 研究为社交媒体内容审核提供了理论和实践指导，优化了言论自由与社会扭曲的平衡。

Abstract: User-generated content (UGC) on social media platforms is vulnerable to
incitements and manipulations, necessitating effective regulations. To address
these challenges, those platforms often deploy automated content moderators
tasked with evaluating the harmfulness of UGC and filtering out content that
violates established guidelines. However, such moderation inevitably gives rise
to strategic responses from users, who strive to express themselves within the
confines of guidelines. Such phenomena call for a careful balance between: 1.
ensuring freedom of speech -- by minimizing the restriction of expression; and
2. reducing social distortion -- measured by the total amount of content
manipulation. We tackle the problem of optimizing this balance through the lens
of mechanism design, aiming at optimizing the trade-off between minimizing
social distortion and maximizing free speech. Although determining the optimal
trade-off is NP-hard, we propose practical methods to approximate the optimal
solution. Additionally, we provide generalization guarantees determining the
amount of finite offline data required to approximate the optimal moderator
effectively.

</details>


### [89] [Geometric Operator Learning with Optimal Transport](https://arxiv.org/abs/2507.20065)
*Xinyi Li,Zongyi Li,Nikola Kovachki,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种将最优传输（OT）融入偏微分方程（PDE）算子学习的方法，通过将几何嵌入问题转化为OT问题，实现了更高的灵活性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统几何学习方法通常将域表示为网格、图或点云，限制了灵活性。本文旨在通过OT方法实现更高效的几何嵌入。

Method: 将离散网格推广为网格密度函数，通过OT问题将其映射到参考空间中的均匀密度。在3D模拟中，将表面几何嵌入2D参数化潜在空间。

Result: 在RANS方程实验中，相比现有方法，该方法在精度和计算效率（时间和内存）上均有提升，并在FlowBench数据集上表现出显著改进的精度。

Conclusion: 基于OT的实例依赖变形方法在处理复杂几何时具有显著优势，能够提升精度并降低计算成本。

Abstract: We propose integrating optimal transport (OT) into operator learning for
partial differential equations (PDEs) on complex geometries. Classical
geometric learning methods typically represent domains as meshes, graphs, or
point clouds. Our approach generalizes discretized meshes to mesh density
functions, formulating geometry embedding as an OT problem that maps these
functions to a uniform density in a reference space. Compared to previous
methods relying on interpolation or shared deformation, our OT-based method
employs instance-dependent deformation, offering enhanced flexibility and
effectiveness. For 3D simulations focused on surfaces, our OT-based neural
operator embeds the surface geometry into a 2D parameterized latent space. By
performing computations directly on this 2D representation of the surface
manifold, it achieves significant computational efficiency gains compared to
volumetric simulation. Experiments with Reynolds-averaged Navier-Stokes
equations (RANS) on the ShapeNet-Car and DrivAerNet-Car datasets show that our
method achieves better accuracy and also reduces computational expenses in
terms of both time and memory usage compared to existing machine learning
models. Additionally, our model demonstrates significantly improved accuracy on
the FlowBench dataset, underscoring the benefits of employing
instance-dependent deformation for datasets with highly variable geometries.

</details>


### [90] [Sparse Equation Matching: A Derivative-Free Learning for General-Order Dynamical Systems](https://arxiv.org/abs/2507.20072)
*Jiaqiang Li,Jianbin Tan,Xueqin Wang*

Main category: cs.LG

TL;DR: 提出了一种名为稀疏方程匹配（SEM）的统一框架，用于无导数估计微分算子及其驱动函数，适用于一般阶动态系统，并在脑电图数据分析中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精确的导数估计且仅适用于一阶动态系统，限制了其在实际场景中的应用。

Method: 引入基于积分的稀疏回归方法，利用格林函数实现无导数估计。

Result: 通过仿真和脑电图数据分析验证了SEM的有效性，揭示了任务特定的脑连接模式。

Conclusion: SEM为复杂系统的动力学建模提供了新工具，尤其在脑连接分析中具有重要价值。

Abstract: Equation discovery is a fundamental learning task for uncovering the
underlying dynamics of complex systems, with wide-ranging applications in areas
such as brain connectivity analysis, climate modeling, gene regulation, and
physical system simulation. However, many existing approaches rely on accurate
derivative estimation and are limited to first-order dynamical systems,
restricting their applicability to real-world scenarios. In this work, we
propose sparse equation matching (SEM), a unified framework that encompasses
several existing equation discovery methods under a common formulation. SEM
introduces an integral-based sparse regression method using Green's functions,
enabling derivative-free estimation of differential operators and their
associated driving functions in general-order dynamical systems. The
effectiveness of SEM is demonstrated through extensive simulations,
benchmarking its performance against derivative-based approaches. We then apply
SEM to electroencephalographic (EEG) data recorded during multiple oculomotor
tasks, collected from 52 participants in a brain-computer interface experiment.
Our method identifies active brain regions across participants and reveals
task-specific connectivity patterns. These findings offer valuable insights
into brain connectivity and the underlying neural mechanisms.

</details>


### [91] [Cluster Purge Loss: Structuring Transformer Embeddings for Equivalent Mutants Detection](https://arxiv.org/abs/2507.20078)
*Adelaide Danilov,Aria Nourbakhsh,Christoph Schommer*

Main category: cs.LG

TL;DR: 论文提出了一种结合交叉熵损失和深度度量学习目标的新框架（Cluster Purge Loss），用于改进代码处理任务中的嵌入空间结构，特别是在等效代码变异检测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在微调预训练Transformer模型时，未能充分结构化嵌入空间以反映类内语义关系，而等效代码变异检测任务对嵌入空间质量要求较高。

Method: 提出Cluster Purge Loss，结合交叉熵损失和深度度量学习目标，动态调整类内边界以优化嵌入空间。以UniXCoder为基础模型。

Result: 在等效变异检测任务中达到最先进性能，并生成更可解释的嵌入空间。

Conclusion: 新框架通过优化嵌入空间结构，显著提升了等效代码变异检测任务的性能。

Abstract: Recent pre-trained transformer models achieve superior performance in various
code processing objectives. However, although effective at optimizing decision
boundaries, common approaches for fine-tuning them for downstream
classification tasks - distance-based methods or training an additional
classification head - often fail to thoroughly structure the embedding space to
reflect nuanced intra-class semantic relationships. Equivalent code mutant
detection is one of these tasks, where the quality of the embedding space is
crucial to the performance of the models. We introduce a novel framework that
integrates cross-entropy loss with a deep metric learning objective, termed
Cluster Purge Loss. This objective, unlike conventional approaches,
concentrates on adjusting fine-grained differences within each class,
encouraging the separation of instances based on semantical equivalency to the
class center using dynamically adjusted borders. Employing UniXCoder as the
base model, our approach demonstrates state-of-the-art performance in the
domain of equivalent mutant detection and produces a more interpretable
embedding space.

</details>


### [92] [EcoTransformer: Attention without Multiplication](https://arxiv.org/abs/2507.20096)
*Xin Gao,Xingming Xu*

Main category: cs.LG

TL;DR: EcoTransformer提出了一种基于卷积和L1距离的新型注意力机制，替代传统点积注意力，显著降低计算能耗，同时在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的点积注意力机制计算密集且能耗高，需要更高效的替代方案。

Method: 使用Laplacian核卷积值向量，通过L1距离计算注意力分数，避免矩阵乘法。

Result: 在NLP、生物信息学和视觉任务中表现与或优于传统注意力，同时能耗显著降低。

Conclusion: EcoTransformer是一种高效且性能优越的Transformer架构，适用于能耗敏感场景。

Abstract: The Transformer, with its scaled dot-product attention mechanism, has become
a foundational architecture in modern AI. However, this mechanism is
computationally intensive and incurs substantial energy costs. We propose a new
Transformer architecture EcoTransformer, in which the output context vector is
constructed as the convolution of the values using a Laplacian kernel, where
the distances are measured by the L1 metric between the queries and keys.
Compared to dot-product based attention, the new attention score calculation is
free of matrix multiplication. It performs on par with, or even surpasses,
scaled dot-product attention in NLP, bioinformatics, and vision tasks, while
consuming significantly less energy.

</details>


### [93] [Wine Characterisation with Spectral Information and Predictive Artificial Intelligence](https://arxiv.org/abs/2507.20114)
*Jianping Yao,Son N. Tran,Hieu Nguyen,Samantha Sawyer,Rocco Longo*

Main category: cs.LG

TL;DR: 论文利用紫外-可见光谱和机器学习技术预测葡萄汁属性并分类葡萄酒产地，支持向量机（SVM）表现最佳，准确率和F1分数均超过91%。


<details>
  <summary>Details</summary>
Motivation: 改进传统葡萄酒分析方法，结合光谱技术和机器学习，为葡萄酒行业提供更简单、高效的数据分析解决方案。

Method: 结合紫外-可见光谱扫描和机器学习技术，分两阶段分析葡萄汁属性和葡萄酒产地分类。

Result: SVM在预测任务中表现最优，准确率和F1分数均超过91%；关键波长集中在250-420 nm范围。

Conclusion: 该技术为葡萄酒行业整合大数据和物联网提供了新思路，推动‘智能酒庄’发展。

Abstract: The purpose of this paper is to use absorbance data obtained by human tasting
and an ultraviolet-visible (UV-Vis) scanning spectrophotometer to predict the
attributes of grape juice (GJ) and to classify the wine's origin, respectively.
The approach combined machine learning (ML) techniques with spectroscopy to
find a relatively simple way to apply them in two stages of winemaking and help
improve the traditional wine analysis methods regarding sensory data and wine's
origins. This new technique has overcome the disadvantages of the complex
sensors by taking advantage of spectral fingerprinting technology and forming a
comprehensive study of the employment of AI in the wine analysis domain. In the
results, Support Vector Machine (SVM) was the most efficient and robust in both
attributes and origin prediction tasks. Both the accuracy and F1 score of the
origin prediction exceed 91%. The feature ranking approach found that the more
influential wavelengths usually appear at the lower end of the scan range, 250
nm (nanometers) to 420 nm, which is believed to be of great help for selecting
appropriate validation methods and sensors to extract wine data in future
research. The knowledge of this research provides new ideas and early solutions
for the wine industry or other beverage industries to integrate big data and
IoT in the future, which significantly promotes the development of 'Smart
Wineries'.

</details>


### [94] [Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing](https://arxiv.org/abs/2507.20127)
*Xuanting Xie,Bingheng Li,Erlin Pan,Zhao Kang,Wenyu Chen*

Main category: cs.LG

TL;DR: 提出了一种名为AMLP的无监督框架，通过使MLP适应聚合操作，解决了GNN中固定聚合函数在异质性下性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: GNN通常使用固定的聚合函数（如Mean、Max、Sum），缺乏理论依据，尤其在异质性情况下表现不佳。现有方法依赖大量标注数据，而现实中标注数据稀缺。

Method: AMLP框架分为两步：1）利用图重构方法实现高阶分组效应；2）使用单层网络编码不同程度的异质性。

Result: 在节点聚类和分类任务上的实验表明，AMLP性能优越。

Conclusion: AMLP为多样化的图学习场景提供了一种轻量级且高效的解决方案。

Abstract: Graph Neural Networks (GNNs) have become a dominant approach to learning
graph representations, primarily because of their message-passing mechanisms.
However, GNNs typically adopt a fixed aggregator function such as Mean, Max, or
Sum without principled reasoning behind the selection. This rigidity,
especially in the presence of heterophily, often leads to poor, problem
dependent performance. Although some attempts address this by designing more
sophisticated aggregation functions, these methods tend to rely heavily on
labeled data, which is often scarce in real-world tasks. In this work, we
propose a novel unsupervised framework, "Aggregation-aware Multilayer
Perceptron" (AMLP), which shifts the paradigm from directly crafting
aggregation functions to making MLP adaptive to aggregation. Our lightweight
approach consists of two key steps: First, we utilize a graph reconstruction
method that facilitates high-order grouping effects, and second, we employ a
single-layer network to encode varying degrees of heterophily, thereby
improving the capacity and applicability of the model. Extensive experiments on
node clustering and classification demonstrate the superior performance of
AMLP, highlighting its potential for diverse graph learning scenarios.

</details>


### [95] [Generative molecule evolution using 3D pharmacophore for efficient Structure-Based Drug Design](https://arxiv.org/abs/2507.20130)
*Yi He,Ailun Wang,Zhi Wang,Yu Liu,Xingyuan Xu,Wen Yan*

Main category: cs.LG

TL;DR: MEVO是一个进化框架，通过结合小分子数据集和稀缺的蛋白质-配体复合物数据集，解决了结构药物设计（SBDD）中的数据限制问题，提高了生成模型的训练数据量。


<details>
  <summary>Details</summary>
Motivation: 由于数据限制，生成模型在结构药物设计中的应用受限，MEVO旨在填补这一空白。

Method: MEVO包含三个关键组件：高保真VQ-VAE、扩散模型和基于物理的评分函数的进化策略。

Result: MEVO成功生成了高亲和力结合物，并在KRASG12D抑制剂设计中表现出色。

Conclusion: MEVO是一个高效且数据友好的结构配体设计模型。

Abstract: Recent advances in generative models, particularly diffusion and
auto-regressive models, have revolutionized fields like computer vision and
natural language processing. However, their application to structure-based drug
design (SBDD) remains limited due to critical data constraints. To address the
limitation of training data for models targeting SBDD tasks, we propose an
evolutionary framework named MEVO, which bridges the gap between billion-scale
small molecule dataset and the scarce protein-ligand complex dataset, and
effectively increase the abundance of training data for generative SBDD models.
MEVO is composed of three key components: a high-fidelity VQ-VAE for molecule
representation in latent space, a diffusion model for pharmacophore-guided
molecule generation, and a pocket-aware evolutionary strategy for molecule
optimization with physics-based scoring function. This framework efficiently
generate high-affinity binders for various protein targets, validated with
predicted binding affinities using free energy perturbation (FEP) methods. In
addition, we showcase the capability of MEVO in designing potent inhibitors to
KRAS$^{\textrm{G12D}}$, a challenging target in cancer therapeutics, with
similar affinity to the known highly active inhibitor evaluated by FEP
calculations. With high versatility and generalizability, MEVO offers an
effective and data-efficient model for various tasks in structure-based ligand
design.

</details>


### [96] [Awesome-OL: An Extensible Toolkit for Online Learning](https://arxiv.org/abs/2507.20144)
*Zeyi Liu,Songqiao Hu,Pengyu Han,Jiaming Liu,Xiao He*

Main category: cs.LG

TL;DR: Awesome-OL是一个用于在线学习研究的Python工具包，集成了先进算法、基准数据集和多模态可视化。


<details>
  <summary>Details</summary>
Motivation: 在线学习因其处理流式和非平稳数据的自适应能力而受到关注，但缺乏统一的工具支持研究和部署。

Method: 基于scikit-multiflow开源基础设施，提供用户友好的交互界面，同时保持研究的灵活性和可扩展性。

Result: Awesome-OL为在线学习研究提供了可复现比较的统一框架，并公开了源代码。

Conclusion: Awesome-OL是一个实用且灵活的工具包，有助于推动在线学习领域的研究和应用。

Abstract: In recent years, online learning has attracted increasing attention due to
its adaptive capability to process streaming and non-stationary data. To
facilitate algorithm development and practical deployment in this area, we
introduce Awesome-OL, an extensible Python toolkit tailored for online learning
research. Awesome-OL integrates state-of-the-art algorithm, which provides a
unified framework for reproducible comparisons, curated benchmark datasets, and
multi-modal visualization. Built upon the scikit-multiflow open-source
infrastructure, Awesome-OL emphasizes user-friendly interactions without
compromising research flexibility or extensibility. The source code is publicly
available at: https://github.com/liuzy0708/Awesome-OL.

</details>


### [97] [ASNN: Learning to Suggest Neural Architectures from Performance Distributions](https://arxiv.org/abs/2507.20164)
*Jinwook Hong*

Main category: cs.LG

TL;DR: ASNN通过学习神经网络结构与测试精度之间的关系，提出改进架构，优于随机搜索。


<details>
  <summary>Details</summary>
Motivation: 神经网络架构设计通常依赖启发式或搜索方法，缺乏通用闭式函数映射结构与性能关系。

Method: 构建数据集训练ASNN模型，利用其迭代预测更高性能的架构。

Result: ASNN在2层和3层架构中均提出优于原始数据的架构，提高平均测试精度。

Conclusion: ASNN为神经网络架构优化提供高效替代方案，有望实现自动化设计。

Abstract: The architecture of a neural network (NN) plays a critical role in
determining its performance. However, there is no general closed-form function
that maps between network structure and accuracy, making the process of
architecture design largely heuristic or search-based. In this study, we
propose the Architecture Suggesting Neural Network (ASNN), a model designed to
learn the relationship between NN architecture and its test accuracy, and to
suggest improved architectures accordingly. To train ASNN, we constructed
datasets using TensorFlow-based models with varying numbers of layers and
nodes. Experimental results were collected for both 2-layer and 3-layer
architectures across a grid of configurations, each evaluated with 10 repeated
trials to account for stochasticity. Accuracy values were treated as inputs,
and architectural parameters as outputs. The trained ASNN was then used
iteratively to predict architectures that yield higher performance. In both
2-layer and 3-layer cases, ASNN successfully suggested architectures that
outperformed the best results found in the original training data. Repeated
prediction and retraining cycles led to the discovery of architectures with
improved mean test accuracies, demonstrating the model's capacity to generalize
the performance-structure relationship. These results suggest that ASNN
provides an efficient alternative to random search for architecture
optimization, and offers a promising approach toward automating neural network
design. "Parts of the manuscript, including text editing and expression
refinement, were supported by OpenAI's ChatGPT. All content was reviewed and
verified by the authors."

</details>


### [98] [Partial Domain Adaptation via Importance Sampling-based Shift Correction](https://arxiv.org/abs/2507.20191)
*Cheng-Jun Guo,Chuan-Xian Ren,You-Wei Luo,Xiao-Lin Xu,Hong Yan*

Main category: cs.LG

TL;DR: 论文提出了一种基于重要性采样的IS²C方法，用于解决部分域适应（PDA）中的标签分布偏移问题，并通过理论证明和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 部分域适应（PDA）中，简单的样本加权方法无法充分利用标记数据并容易导致过拟合，因此需要一种更有效的方法来校正标签分布偏移。

Method: 提出IS²C方法，通过从构建的采样域中采样新标记数据，以匹配目标域的标签分布，并结合最优传输理论进行条件分布对齐。

Result: 理论证明IS²C可以主导泛化误差，实验结果表明其在PDA基准测试中优于现有方法。

Conclusion: IS²C方法能有效校正标签分布偏移，提升模型泛化能力，为PDA任务提供了新的解决方案。

Abstract: Partial domain adaptation (PDA) is a challenging task in real-world machine
learning scenarios. It aims to transfer knowledge from a labeled source domain
to a related unlabeled target domain, where the support set of the source label
distribution subsumes the target one. Previous PDA works managed to correct the
label distribution shift by weighting samples in the source domain. However,
the simple reweighing technique cannot explore the latent structure and
sufficiently use the labeled data, and then models are prone to over-fitting on
the source domain. In this work, we propose a novel importance sampling-based
shift correction (IS$^2$C) method, where new labeled data are sampled from a
built sampling domain, whose label distribution is supposed to be the same as
the target domain, to characterize the latent structure and enhance the
generalization ability of the model. We provide theoretical guarantees for
IS$^2$C by proving that the generalization error can be sufficiently dominated
by IS$^2$C. In particular, by implementing sampling with the mixture
distribution, the extent of shift between source and sampling domains can be
connected to generalization error, which provides an interpretable way to build
IS$^2$C. To improve knowledge transfer, an optimal transport-based independence
criterion is proposed for conditional distribution alignment, where the
computation of the criterion can be adjusted to reduce the complexity from
$\mathcal{O}(n^3)$ to $\mathcal{O}(n^2)$ in realistic PDA scenarios. Extensive
experiments on PDA benchmarks validate the theoretical results and demonstrate
the effectiveness of our IS$^2$C over existing methods.

</details>


### [99] [Technical Indicator Networks (TINs): An Interpretable Neural Architecture Modernizing Classic al Technical Analysis for Adaptive Algorithmic Trading](https://arxiv.org/abs/2507.20202)
*Longfei Lu*

Main category: cs.LG

TL;DR: 论文提出传统金融技术指标本质上是具有固定和可解释权重的神经网络的特例，并引入技术指标网络（TINs）作为通用架构。


<details>
  <summary>Details</summary>
Motivation: 将传统技术指标与现代AI系统结合，升级技术分析的基础逻辑。

Method: 通过模块化神经网络组件重构技术指标，支持多维输入。

Result: TINs能够复制并结构性地升级传统指标，推动算法交易进入新时代。

Conclusion: TINs通过将领域知识编码为神经网络结构，连接传统指标与现代AI潜力。

Abstract: This work proposes that a vast majority of classical technical indicators in
financial analysis are, in essence, special cases of neural networks with fixed
and interpretable weights. It is shown that nearly all such indicators, such as
moving averages, momentum-based oscillators, volatility bands, and other
commonly used technical constructs, can be reconstructed topologically as
modular neural network components. Technical Indicator Networks (TINs) are
introduced as a general neural architecture that replicates and structurally
upgrades traditional indicators by supporting n-dimensional inputs such as
price, volume, sentiment, and order book data. By encoding domain-specific
knowledge into neural structures, TINs modernize the foundational logic of
technical analysis and propel algorithmic trading into a new era, bridging the
legacy of proven indicators with the potential of contemporary AI systems.

</details>


### [100] [Protein-SE(3): Benchmarking SE(3)-based Generative Models for Protein Structure Design](https://arxiv.org/abs/2507.20243)
*Lang Yu,Zhangyang Gao,Cheng Tan,Qin Chen,Jie Zhou,Liang He*

Main category: cs.LG

TL;DR: 提出了Protein-SE(3)基准，用于全面评估和比较SE(3)蛋白质结构设计方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏模块化基准以全面研究和公平比较不同方法。

Method: 基于统一训练框架，整合多种生成模型（如DDPM、Score Matching、Flow Matching）和多样化评估指标。

Result: 发布了首个基于统一框架的SE(3)蛋白质设计基准，公开可用。

Conclusion: Protein-SE(3)为未来算法快速原型设计提供了数学抽象和公平比较平台。

Abstract: SE(3)-based generative models have shown great promise in protein geometry
modeling and effective structure design. However, the field currently lacks a
modularized benchmark to enable comprehensive investigation and fair comparison
of different methods. In this paper, we propose Protein-SE(3), a new benchmark
based on a unified training framework, which comprises protein scaffolding
tasks, integrated generative models, high-level mathematical abstraction, and
diverse evaluation metrics. Recent advanced generative models designed for
protein scaffolding, from multiple perspectives like DDPM (Genie1 and Genie2),
Score Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and
FrameFlow) are integrated into our framework. All integrated methods are fairly
investigated with the same training dataset and evaluation metrics.
Furthermore, we provide a high-level abstraction of the mathematical
foundations behind the generative models, enabling fast prototyping of future
algorithms without reliance on explicit protein structures. Accordingly, we
release the first comprehensive benchmark built upon unified training framework
for SE(3)-based protein structure design, which is publicly accessible at
https://github.com/BruthYU/protein-se3.

</details>


### [101] [Learning from Expert Factors: Trajectory-level Reward Shaping for Formulaic Alpha Mining](https://arxiv.org/abs/2507.20263)
*Junjie Zhao,Chengxi Zhang,Chenkai Wang,Peng Yang*

Main category: cs.LG

TL;DR: 提出了一种名为TLRS的新奖励塑造方法，通过子序列级相似性提供密集中间奖励，显著提升强化学习在投资策略中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在稀疏奖励下效率低下，限制了符号搜索空间的探索并导致训练不稳定。

Method: 提出TLRS方法，通过测量部分生成表达式与专家设计公式的子序列级相似性提供密集奖励，并引入奖励中心化机制降低训练方差。

Result: 在六种主要中美股票指数上的实验表明，TLRS显著提升预测能力，Rank Information Coefficient提高9.29%，计算效率大幅提升。

Conclusion: TLRS通过密集奖励和高效计算，显著改进了强化学习在投资策略中的应用效果。

Abstract: Reinforcement learning (RL) has successfully automated the complex process of
mining formulaic alpha factors, for creating interpretable and profitable
investment strategies. However, existing methods are hampered by the sparse
rewards given the underlying Markov Decision Process. This inefficiency limits
the exploration of the vast symbolic search space and destabilizes the training
process. To address this, Trajectory-level Reward Shaping (TLRS), a novel
reward shaping method, is proposed. TLRS provides dense, intermediate rewards
by measuring the subsequence-level similarity between partially generated
expressions and a set of expert-designed formulas. Furthermore, a reward
centering mechanism is introduced to reduce training variance. Extensive
experiments on six major Chinese and U.S. stock indices show that TLRS
significantly improves the predictive power of mined factors, boosting the Rank
Information Coefficient by 9.29% over existing potential-based shaping
algorithms. Notably, TLRS achieves a major leap in computational efficiency by
reducing its time complexity with respect to the feature dimension from linear
to constant, which is a significant improvement over distance-based baselines.

</details>


### [102] [MIPS: a Multimodal Infinite Polymer Sequence Pre-training Framework for Polymer Property Prediction](https://arxiv.org/abs/2507.20326)
*Jiaxi Wang,Yaosen Min,Xun Zhu,Miao Li,Ji Wu*

Main category: cs.LG

TL;DR: 提出了一种名为MIPS的多模态预训练框架，用于准确预测聚合物性质，结合了拓扑和空间信息，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉聚合物在聚合过程中的整体性质，因此需要一种更全面的建模方法。

Method: MIPS框架将聚合物表示为无限单体序列，结合拓扑（MPM和GAM）和空间信息（3D描述符），并提出局部图注意力（LGA）和骨架嵌入以解决环结构问题。

Result: 在八项聚合物性质预测任务中，MIPS取得了最先进的性能。

Conclusion: MIPS通过多模态融合和创新的拓扑建模方法，显著提升了聚合物性质预测的准确性。

Abstract: Polymers, composed of repeating structural units called monomers, are
fundamental materials in daily life and industry. Accurate property prediction
for polymers is essential for their design, development, and application.
However, existing modeling approaches, which typically represent polymers by
the constituent monomers, struggle to capture the whole properties of polymer,
since the properties change during the polymerization process. In this study,
we propose a Multimodal Infinite Polymer Sequence (MIPS) pre-training
framework, which represents polymers as infinite sequences of monomers and
integrates both topological and spatial information for comprehensive modeling.
From the topological perspective, we generalize message passing mechanism (MPM)
and graph attention mechanism (GAM) to infinite polymer sequences. For MPM, we
demonstrate that applying MPM to infinite polymer sequences is equivalent to
applying MPM on the induced star-linking graph of monomers. For GAM, we propose
to further replace global graph attention with localized graph attention (LGA).
Moreover, we show the robustness of the "star linking" strategy through Repeat
and Shift Invariance Test (RSIT). Despite its robustness, "star linking"
strategy exhibits limitations when monomer side chains contain ring structures,
a common characteristic of polymers, as it fails the Weisfeiler-Lehman~(WL)
test. To overcome this issue, we propose backbone embedding to enhance the
capability of MPM and LGA on infinite polymer sequences. From the spatial
perspective, we extract 3D descriptors of repeating monomers to capture spatial
information. Finally, we design a cross-modal fusion mechanism to unify the
topological and spatial information. Experimental validation across eight
diverse polymer property prediction tasks reveals that MIPS achieves
state-of-the-art performance.

</details>


### [103] [Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning](https://arxiv.org/abs/2507.20335)
*Siyu Song,Wentao Liu,Ye Lu,Ruohua Zhang,Tao Liu,Jinze Lv,Xinyun Wang,Aimin Zhou,Fei Tan,Bo Jiang,Hao Hao*

Main category: cs.LG

TL;DR: 论文提出EduAlign框架，通过标注教育交互数据并训练奖励模型，优化大语言模型（LLM）在教育领域的表现，显著提升其教学帮助性、个性化和创造力。


<details>
  <summary>Details</summary>
Motivation: 标准LLM在教育中缺乏与教学原则（如帮助性、个性化和创造力）的对齐，EduAlign旨在解决这一问题。

Method: 1. 标注8k教育交互数据，训练HPC-RM奖励模型；2. 使用GRPO方法微调LLM。

Result: 微调后的模型在教学帮助性、个性化和创造力方面表现显著提升。

Conclusion: EduAlign为开发更具教学对齐性的AI导师提供了可扩展且有效的方法。

Abstract: The integration of large language models (LLMs) into education presents
unprecedented opportunities for scalable personalized learning. However,
standard LLMs often function as generic information providers, lacking
alignment with fundamental pedagogical principles such as helpfulness,
student-centered personalization, and creativity cultivation. To bridge this
gap, we propose EduAlign, a novel framework designed to guide LLMs toward
becoming more effective and responsible educational assistants. EduAlign
consists of two main stages. In the first stage, we curate a dataset of 8k
educational interactions and annotate them-both manually and
automatically-along three key educational dimensions: Helpfulness,
Personalization, and Creativity (HPC). These annotations are used to train
HPC-RM, a multi-dimensional reward model capable of accurately scoring LLM
outputs according to these educational principles. We further evaluate the
consistency and reliability of this reward model. In the second stage, we
leverage HPC-RM as a reward signal to fine-tune a pre-trained LLM using Group
Relative Policy Optimization (GRPO) on a set of 2k diverse prompts. We then
assess the pre- and post-finetuning models on both educational and
general-domain benchmarks across the three HPC dimensions. Experimental results
demonstrate that the fine-tuned model exhibits significantly improved alignment
with pedagogical helpfulness, personalization, and creativity stimulation. This
study presents a scalable and effective approach to aligning LLMs with nuanced
and desirable educational traits, paving the way for the development of more
engaging, pedagogically aligned AI tutors.

</details>


### [104] [From Observations to Causations: A GNN-based Probabilistic Prediction Framework for Causal Discovery](https://arxiv.org/abs/2507.20349)
*Rezaur Rashid,Gabriel Terejanu*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的概率框架，用于从观测数据中发现因果关系，优于传统方法和非GNN方法。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在大数据集和复杂关系下难以扩展和捕捉全局结构信息，需要更高效且准确的解决方案。

Method: 使用GNN编码节点和边属性，学习整个因果图空间的概率分布，并通过合成数据集训练，结合统计和信息论度量。

Result: 在合成和真实数据集上表现出更高的准确性和可扩展性，优于传统和非GNN方法。

Conclusion: 该概率框架显著提升了因果结构学习能力，对多领域决策和科学发现具有广泛意义。

Abstract: Causal discovery from observational data is challenging, especially with
large datasets and complex relationships. Traditional methods often struggle
with scalability and capturing global structural information. To overcome these
limitations, we introduce a novel graph neural network (GNN)-based
probabilistic framework that learns a probability distribution over the entire
space of causal graphs, unlike methods that output a single deterministic
graph. Our framework leverages a GNN that encodes both node and edge attributes
into a unified graph representation, enabling the model to learn complex causal
structures directly from data. The GNN model is trained on a diverse set of
synthetic datasets augmented with statistical and information-theoretic
measures, such as mutual information and conditional entropy, capturing both
local and global data properties. We frame causal discovery as a supervised
learning problem, directly predicting the entire graph structure. Our approach
demonstrates superior performance, outperforming both traditional and recent
non-GNN-based methods, as well as a GNN-based approach, in terms of accuracy
and scalability on synthetic and real-world datasets without further training.
This probabilistic framework significantly improves causal structure learning,
with broad implications for decision-making and scientific discovery across
various fields.

</details>


### [105] [Computational Advantages of Multi-Grade Deep Learning: Convergence Analysis and Performance Insights](https://arxiv.org/abs/2507.20351)
*Ronglong Fang,Yuesheng Xu*

Main category: cs.LG

TL;DR: MGDL在图像回归、去噪和去模糊任务中表现优于SGDL，且对学习率选择更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 研究MGDL的计算优势及其在图像处理任务中的性能提升。

Method: 通过梯度下降法（GD）分析MGDL和SGDL的收敛性及雅可比矩阵特征值分布。

Result: MGDL在训练稳定性上优于SGDL，且对学习率选择更具鲁棒性。

Conclusion: MGDL因其数学特性在性能和稳定性上优于SGDL。

Abstract: Multi-grade deep learning (MGDL) has been shown to significantly outperform
the standard single-grade deep learning (SGDL) across various applications.
This work aims to investigate the computational advantages of MGDL focusing on
its performance in image regression, denoising, and deblurring tasks, and
comparing it to SGDL. We establish convergence results for the gradient descent
(GD) method applied to these models and provide mathematical insights into
MGDL's improved performance. In particular, we demonstrate that MGDL is more
robust to the choice of learning rate under GD than SGDL. Furthermore, we
analyze the eigenvalue distributions of the Jacobian matrices associated with
the iterative schemes arising from the GD iterations, offering an explanation
for MGDL's enhanced training stability.

</details>


### [106] [Wafer Defect Root Cause Analysis with Partial Trajectory Regression](https://arxiv.org/abs/2507.20357)
*Kohei Miyaguchi,Masao Joko,Rebekah Sheraw,Tsuyoshi Idé*

Main category: cs.LG

TL;DR: 提出了一种名为PTR的新框架，用于晶圆缺陷的根因分析，解决了传统向量回归模型在处理变长处理路径时的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于晶圆制造过程的组合性和变异性（如返工操作和随机等待时间），识别导致缺陷的上游过程具有挑战性。

Method: 采用Partial Trajectory Regression (PTR)框架，结合proc2vec和route2vec表示学习方法，通过比较部分处理路径的反事实结果计算各过程的贡献分数。

Result: 在NY CREATES fab的实际晶圆历史数据上验证了框架的有效性。

Conclusion: PTR框架为晶圆缺陷的根因分析提供了一种高效且适应性强的方法。

Abstract: Identifying upstream processes responsible for wafer defects is challenging
due to the combinatorial nature of process flows and the inherent variability
in processing routes, which arises from factors such as rework operations and
random process waiting times. This paper presents a novel framework for wafer
defect root cause analysis, called Partial Trajectory Regression (PTR). The
proposed framework is carefully designed to address the limitations of
conventional vector-based regression models, particularly in handling
variable-length processing routes that span a large number of heterogeneous
physical processes. To compute the attribution score of each process given a
detected high defect density on a specific wafer, we propose a new algorithm
that compares two counterfactual outcomes derived from partial process
trajectories. This is enabled by new representation learning methods, proc2vec
and route2vec. We demonstrate the effectiveness of the proposed framework using
real wafer history data from the NY CREATES fab in Albany.

</details>


### [107] [MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS Data (Extended Version)](https://arxiv.org/abs/2507.20362)
*Hengyu Liu,Tianyi Li,Yuqiang He,Kristian Torp,Yushuai Li,Christian S. Jensen*

Main category: cs.LG

TL;DR: MH-GIN是一种基于多尺度异构图网络的缺失值填补方法，针对海上自动识别系统数据中的多尺度依赖关系，显著提升了填补精度。


<details>
  <summary>Details</summary>
Motivation: 海上自动识别系统数据中的缺失值限制了其在下游应用中的有效性，现有方法无法捕捉多尺度依赖关系。

Method: MH-GIN通过提取多尺度时间特征并构建多尺度异构图，显式建模异质属性间的依赖关系，利用图传播填补缺失值。

Result: 在两个真实数据集上，MH-GIN平均减少57%的填补误差，同时保持计算效率。

Conclusion: MH-GIN通过捕捉多尺度依赖关系，显著提升了缺失值填补的准确性，且代码开源。

Abstract: Location-tracking data from the Automatic Identification System, much of
which is publicly available, plays a key role in a range of maritime safety and
monitoring applications. However, the data suffers from missing values that
hamper downstream applications. Imputing the missing values is challenging
because the values of different heterogeneous attributes are updated at diverse
rates, resulting in the occurrence of multi-scale dependencies among
attributes. Existing imputation methods that assume similar update rates across
attributes are unable to capture and exploit such dependencies, limiting their
imputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based
Imputation Network that aims improve imputation accuracy by capturing
multi-scale dependencies. Specifically, MH-GIN first extracts multi-scale
temporal features for each attribute while preserving their intrinsic
heterogeneous characteristics. Then, it constructs a multi-scale heterogeneous
graph to explicitly model dependencies between heterogeneous attributes to
enable more accurate imputation of missing values through graph propagation.
Experimental results on two real-world datasets find that MH-GIN is capable of
an average 57% reduction in imputation errors compared to state-of-the-art
methods, while maintaining computational efficiency. The source code and
implementation details of MH-GIN are publicly available
https://github.com/hyLiu1994/MH-GIN.

</details>


### [108] [Sequence-Aware Inline Measurement Attribution for Good-Bad Wafer Diagnosis](https://arxiv.org/abs/2507.20364)
*Kohei Miyaguchi,Masao Joko,Rebekah Sheraw,Tsuyoshi Idé*

Main category: cs.LG

TL;DR: 论文提出了一种名为Trajectory Shapley Attribution (TSA)的新框架，用于解决半导体制造中晶圆缺陷的跨工艺根本原因分析问题。


<details>
  <summary>Details</summary>
Motivation: 现代半导体制造涉及数千个工艺步骤，晶圆缺陷的跨工艺根因分析极具挑战性。

Method: TSA是Shapley值（SV）的扩展，克服了标准SV忽略工艺顺序性和依赖任意参考点的局限性。

Result: 在实验性前端工艺中，TSA成功识别了与异常缺陷相关的测量项。

Conclusion: TSA为复杂制造环境中的缺陷分析提供了有效工具。

Abstract: How can we identify problematic upstream processes when a certain type of
wafer defect starts appearing at a quality checkpoint? Given the complexity of
modern semiconductor manufacturing, which involves thousands of process steps,
cross-process root cause analysis for wafer defects has been considered highly
challenging. This paper proposes a novel framework called Trajectory Shapley
Attribution (TSA), an extension of Shapley values (SV), a widely used
attribution algorithm in explainable artificial intelligence research. TSA
overcomes key limitations of standard SV, including its disregard for the
sequential nature of manufacturing processes and its reliance on an arbitrarily
chosen reference point. We applied TSA to a good-bad wafer diagnosis task in
experimental front-end-of-line processes at the NY CREATES Albany NanoTech fab,
aiming to identify measurement items (serving as proxies for process
parameters) most relevant to abnormal defect occurrence.

</details>


### [109] [Clustering by Attention: Leveraging Prior Fitted Transformers for Data Partitioning](https://arxiv.org/abs/2507.20369)
*Ahmed Shokry,Ayman Khalafallah*

Main category: cs.LG

TL;DR: 提出了一种基于元学习的新型聚类方法，无需参数优化即可超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法存在参数调优复杂、计算成本高、可解释性差或准确性不足等问题。

Method: 利用预训练的Prior-Data Fitted Transformer Network（PFN），通过少量预聚类样本引导整个数据集的聚类过程。

Result: 实验表明，该方法在无预聚类样本时表现良好，提供少量样本后性能显著提升，优于现有技术。

Conclusion: 该方法高效且可扩展，是现有聚类技术的有力替代方案。

Abstract: Clustering is a core task in machine learning with wide-ranging applications
in data mining and pattern recognition. However, its unsupervised nature makes
it inherently challenging. Many existing clustering algorithms suffer from
critical limitations: they often require careful parameter tuning, exhibit high
computational complexity, lack interpretability, or yield suboptimal accuracy,
especially when applied to large-scale datasets. In this paper, we introduce a
novel clustering approach based on meta-learning. Our approach eliminates the
need for parameter optimization while achieving accuracy that outperforms
state-of-the-art clustering techniques. The proposed technique leverages a few
pre-clustered samples to guide the clustering process for the entire dataset in
a single forward pass. Specifically, we employ a pre-trained Prior-Data Fitted
Transformer Network (PFN) to perform clustering. The algorithm computes
attention between the pre-clustered samples and the unclustered samples,
allowing it to infer cluster assignments for the entire dataset based on the
learned relation. We theoretically and empirically demonstrate that, given just
a few pre-clustered examples, the model can generalize to accurately cluster
the rest of the dataset. Experiments on challenging benchmark datasets show
that our approach can successfully cluster well-separated data without any
pre-clustered samples, and significantly improves performance when a few
clustered samples are provided. We show that our approach is superior to the
state-of-the-art techniques. These results highlight the effectiveness and
scalability of our approach, positioning it as a promising alternative to
existing clustering techniques.

</details>


### [110] [WBHT: A Generative Attention Architecture for Detecting Black Hole Anomalies in Backbone Networks](https://arxiv.org/abs/2507.20373)
*Kiymet Kaya,Elif Ak,Sule Gunduz Oguducu*

Main category: cs.LG

TL;DR: WBHT框架结合生成模型、序列学习和注意力机制，用于检测通信网络中的黑洞异常，显著提升F1分数。


<details>
  <summary>Details</summary>
Motivation: 黑洞异常导致无通知的丢包，破坏连接并造成经济损失，需高效检测方法。

Method: 结合Wasserstein GAN与注意力机制，使用LSTM和卷积层捕捉长短期依赖和局部时序模式，通过潜在空间编码区分异常行为。

Result: 在真实网络数据上测试，WBHT的F1分数提升1.65%至58.76%，优于现有模型。

Conclusion: WBHT高效且能检测未发现异常，适用于关键网络监控和安全。

Abstract: We propose the Wasserstein Black Hole Transformer (WBHT) framework for
detecting black hole (BH) anomalies in communication networks. These anomalies
cause packet loss without failure notifications, disrupting connectivity and
leading to financial losses. WBHT combines generative modeling, sequential
learning, and attention mechanisms to improve BH anomaly detection. It
integrates a Wasserstein generative adversarial network with attention
mechanisms for stable training and accurate anomaly identification. The model
uses long-short-term memory layers to capture long-term dependencies and
convolutional layers for local temporal patterns. A latent space encoding
mechanism helps distinguish abnormal network behavior. Tested on real-world
network data, WBHT outperforms existing models, achieving significant
improvements in F1 score (ranging from 1.65% to 58.76%). Its efficiency and
ability to detect previously undetected anomalies make it a valuable tool for
proactive network monitoring and security, especially in mission-critical
networks.

</details>


### [111] [Set-based Implicit Likelihood Inference of Galaxy Cluster Mass](https://arxiv.org/abs/2507.20378)
*Bonny Y. Wang,Leander Thiele*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a set-based machine learning framework that infers posterior
distributions of galaxy cluster masses from projected galaxy dynamics. Our
model combines Deep Sets and conditional normalizing flows to incorporate both
positional and velocity information of member galaxies to predict residual
corrections to the $M$-$\sigma$ relation for improved interpretability. Trained
on the Uchuu-UniverseMachine simulation, our approach significantly reduces
scatter and provides well-calibrated uncertainties across the full mass range
compared to traditional dynamical estimates.

</details>


### [112] [Communication-Efficient Distributed Training for Collaborative Flat Optima Recovery in Deep Learning](https://arxiv.org/abs/2507.20424)
*Tolga Dimlioglu,Anna Choromanska*

Main category: cs.LG

TL;DR: 论文提出了一种分布式训练算法DPPF，通过引入轻量级正则化器Inverse Mean Valley，平衡通信效率与模型性能，显著减少通信开销并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究目标是改进深度神经网络分布式训练中通信效率与模型性能之间的权衡，探索平坦最小值假设的实际应用。

Method: 引入Inverse Mean Valley作为锐度度量，将其作为正则化器融入训练目标，提出DPPF算法，通过推拉动态平衡通信与优化。

Result: DPPF在通信效率和泛化性能上优于其他方法，实验证实其能够找到更平坦的最小值，并提供了理论收敛和泛化保证。

Conclusion: DPPF算法通过推拉动态有效平衡通信与优化，显著提升分布式训练效率与模型性能，同时具备理论支持。

Abstract: We study centralized distributed data parallel training of deep neural
networks (DNNs), aiming to improve the trade-off between communication
efficiency and model performance of the local gradient methods. To this end, we
revisit the flat-minima hypothesis, which suggests that models with better
generalization tend to lie in flatter regions of the loss landscape. We
introduce a simple, yet effective, sharpness measure, Inverse Mean Valley, and
demonstrate its strong correlation with the generalization gap of DNNs. We
incorporate an efficient relaxation of this measure into the distributed
training objective as a lightweight regularizer that encourages workers to
collaboratively seek wide minima. The regularizer exerts a pushing force that
counteracts the consensus step pulling the workers together, giving rise to the
Distributed Pull-Push Force (DPPF) algorithm. Empirically, we show that DPPF
outperforms other communication-efficient approaches and achieves better
generalization performance than local gradient methods and synchronous gradient
averaging, while significantly reducing communication overhead. In addition,
our loss landscape visualizations confirm the ability of DPPF to locate flatter
minima. On the theoretical side, we show that DPPF guides workers to span flat
valleys, with the final valley width governed by the interplay between push and
pull strengths, and that its pull-push dynamics is self-stabilizing. We further
provide generalization guarantees linked to the valley width and prove
convergence in the non-convex setting.

</details>


### [113] [FAST: Similarity-based Knowledge Transfer for Efficient Policy Learning](https://arxiv.org/abs/2507.20433)
*Alessandro Capurso,Elia Piccoli,Davide Bacciu*

Main category: cs.LG

TL;DR: FAST框架通过视觉和文本信息估计任务相似性，优化迁移学习性能并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决迁移学习中的负迁移、领域适应和源策略选择效率低等问题，特别是在动态变化的游戏开发领域。

Method: 利用视觉帧和文本描述构建任务动态的潜在表示，基于相似性分数选择候选策略进行知识迁移。

Result: 在多个赛车赛道上，FAST性能与从头学习方法相当，但训练步数显著减少。

Conclusion: 嵌入驱动的任务相似性估计在迁移学习中具有潜力，能有效提升性能和效率。

Abstract: Transfer Learning (TL) offers the potential to accelerate learning by
transferring knowledge across tasks. However, it faces critical challenges such
as negative transfer, domain adaptation and inefficiency in selecting solid
source policies. These issues often represent critical problems in evolving
domains, i.e. game development, where scenarios transform and agents must
adapt. The continuous release of new agents is costly and inefficient. In this
work we challenge the key issues in TL to improve knowledge transfer, agents
performance across tasks and reduce computational costs. The proposed
methodology, called FAST - Framework for Adaptive Similarity-based Transfer,
leverages visual frames and textual descriptions to create a latent
representation of tasks dynamics, that is exploited to estimate similarity
between environments. The similarity scores guides our method in choosing
candidate policies from which transfer abilities to simplify learning of novel
tasks. Experimental results, over multiple racing tracks, demonstrate that FAST
achieves competitive final performance compared to learning-from-scratch
methods while requiring significantly less training steps. These findings
highlight the potential of embedding-driven task similarity estimations.

</details>


### [114] [BioNeuralNet: A Graph Neural Network based Multi-Omics Network Data Analysis Tool](https://arxiv.org/abs/2507.20440)
*Vicente Ramos,Sundous Hussein,Mohamed Abdel-Hafiz,Arunangshu Sarkar,Weixuan Liu,Katerina J. Kechris,Russell P. Bowler,Leslie Lange,Farnoush Banaei-Kashani*

Main category: cs.LG

TL;DR: BioNeuralNet是一个基于图神经网络的Python框架，用于多组学数据的端到端网络分析，支持多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 多组学数据的高维度和复杂性需要专门工具来有效利用网络表示进行分析。

Method: BioNeuralNet利用图神经网络（GNNs）从多组学网络中学习低维表示，支持网络构建、表示生成和下游分析。

Result: BioNeuralNet是一个灵活、模块化且兼容主流Python工具的开源框架。

Conclusion: BioNeuralNet为精准医学中的多组学网络分析提供了用户友好且可复现的工具。

Abstract: Multi-omics data offer unprecedented insights into complex biological
systems, yet their high dimensionality, sparsity, and intricate interactions
pose significant analytical challenges. Network-based approaches have advanced
multi-omics research by effectively capturing biologically relevant
relationships among molecular entities. While these methods are powerful for
representing molecular interactions, there remains a need for tools
specifically designed to effectively utilize these network representations
across diverse downstream analyses. To fulfill this need, we introduce
BioNeuralNet, a flexible and modular Python framework tailored for end-to-end
network-based multi-omics data analysis. BioNeuralNet leverages Graph Neural
Networks (GNNs) to learn biologically meaningful low-dimensional
representations from multi-omics networks, converting these complex molecular
networks into versatile embeddings. BioNeuralNet supports all major stages of
multi-omics network analysis, including several network construction
techniques, generation of low-dimensional representations, and a broad range of
downstream analytical tasks. Its extensive utilities, including diverse GNN
architectures, and compatibility with established Python packages (e.g.,
scikit-learn, PyTorch, NetworkX), enhance usability and facilitate quick
adoption. BioNeuralNet is an open-source, user-friendly, and extensively
documented framework designed to support flexible and reproducible multi-omics
network analysis in precision medicine.

</details>


### [115] [Provable In-Context Learning of Nonlinear Regression with Transformers](https://arxiv.org/abs/2507.20443)
*Hongbo Li,Lingjie Duan,Yingbin Liang*

Main category: cs.LG

TL;DR: 本文研究了Transformer在复杂非线性回归任务中的上下文学习（ICL）能力，揭示了注意力动态和Lipschitz常数对收敛行为的影响。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在复杂非线性任务中的ICL机制，弥补现有研究对简单任务的局限性。

Method: 分析训练过程中注意力的阶段性动态，引入新的证明技术，研究Lipschitz常数对收敛的影响。

Result: 注意力在早期快速聚焦目标特征，后期收敛；Lipschitz常数决定收敛时间，但Transformer始终能关注相关特征。

Conclusion: Transformer在复杂非线性任务中具备ICL能力，Lipschitz常数是关键影响因素。

Abstract: The transformer architecture, which processes sequences of input tokens to
produce outputs for query tokens, has revolutionized numerous areas of machine
learning. A defining feature of transformers is their ability to perform
previously unseen tasks using task-specific prompts without updating
parameters, a phenomenon known as in-context learning (ICL). Recent research
has actively explored the training dynamics behind ICL, with much of the focus
on relatively simple tasks such as linear regression and binary classification.
To advance the theoretical understanding of ICL, this paper investigates more
complex nonlinear regression tasks, aiming to uncover how transformers acquire
in-context learning capabilities in these settings. We analyze the stage-wise
dynamics of attention during training: attention scores between a query token
and its target features grow rapidly in the early phase, then gradually
converge to one, while attention to irrelevant features decays more slowly and
exhibits oscillatory behavior. Our analysis introduces new proof techniques
that explicitly characterize how the nature of general non-degenerate
L-Lipschitz task functions affects attention weights. Specifically, we identify
that the Lipschitz constant L of nonlinear function classes as a key factor
governing the convergence dynamics of transformers in ICL. Leveraging these
insights, for two distinct regimes depending on whether L is below or above a
threshold, we derive different time bounds to guarantee near-zero prediction
error. Notably, despite the convergence time depending on the underlying task
functions, we prove that query tokens consistently attend to prompt tokens with
highly relevant features at convergence, demonstrating the ICL capability of
transformers for unseen functions.

</details>


### [116] [BOASF: A Unified Framework for Speeding up Automatic Machine Learning via Adaptive Successive Filtering](https://arxiv.org/abs/2507.20446)
*Guanghui Zhu,Xin Fang,Lei Wang,Wenzhong Chen,Rong Gu,Chunfeng Yuan,Yihua Huang*

Main category: cs.LG

TL;DR: 提出了一种结合贝叶斯优化和自适应连续过滤算法（BOASF）的方法，用于自动化模型选择和超参数优化，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决非专家在机器学习和超参数优化中面临的挑战，减少对专家知识的依赖。

Method: BOASF结合贝叶斯优化和自适应连续过滤，通过多轮评估和资源分配优化模型选择。

Result: 实验表明BOASF在速度和预测性能上优于现有自动机器学习方法，且在不同时间预算下表现更好。

Conclusion: BOASF是一种高效、稳健的自动化机器学习方法，适用于模型选择和超参数优化。

Abstract: Machine learning has been making great success in many application areas.
However, for the non-expert practitioners, it is always very challenging to
address a machine learning task successfully and efficiently. Finding the
optimal machine learning model or the hyperparameter combination set from a
large number of possible alternatives usually requires considerable expert
knowledge and experience. To tackle this problem, we propose a combined
Bayesian Optimization and Adaptive Successive Filtering algorithm (BOASF) under
a unified multi-armed bandit framework to automate the model selection or the
hyperparameter optimization. Specifically, BOASF consists of multiple
evaluation rounds in each of which we select promising configurations for each
arm using the Bayesian optimization. Then, ASF can early discard the
poor-performed arms adaptively using a Gaussian UCB-based probabilistic model.
Furthermore, a Softmax model is employed to adaptively allocate available
resources for each promising arm that advances to the next round. The arm with
a higher probability of advancing will be allocated more resources.
Experimental results show that BOASF is effective for speeding up the model
selection and hyperparameter optimization processes while achieving robust and
better prediction performance than the existing state-of-the-art automatic
machine learning methods. Moreover, BOASF achieves better anytime performance
under various time budgets.

</details>


### [117] [WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex Envelope](https://arxiv.org/abs/2507.20447)
*Takanobu Furuhashi,Hidekata Hontani,Tatsuya Yokota*

Main category: cs.LG

TL;DR: WEEP是一种新型可微稀疏正则化器，解决了非可微惩罚与梯度优化器的冲突，在信号和图像去噪任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 稀疏正则化在信号处理中至关重要，但最强的稀疏惩罚通常不可微，与梯度优化器冲突。

Method: 提出WEEP（Weakly-convex Envelope of Piecewise Penalty），基于弱凸包络框架，实现完全可微且平滑的稀疏正则化。

Result: WEEP在信号和图像去噪任务中优于L1范数和其他非凸稀疏正则化器。

Conclusion: WEEP解决了统计性能与计算可行性之间的冲突，为稀疏正则化提供了新方向。

Abstract: Sparse regularization is fundamental in signal processing for efficient
signal recovery and feature extraction. However, it faces a fundamental
dilemma: the most powerful sparsity-inducing penalties are often
non-differentiable, conflicting with gradient-based optimizers that dominate
the field. We introduce WEEP (Weakly-convex Envelope of Piecewise Penalty), a
novel, fully differentiable sparse regularizer derived from the weakly-convex
envelope framework. WEEP provides strong, unbiased sparsity while maintaining
full differentiability and L-smoothness, making it natively compatible with any
gradient-based optimizer. This resolves the conflict between statistical
performance and computational tractability. We demonstrate superior performance
compared to the L1-norm and other established non-convex sparse regularizers on
challenging signal and image denoising tasks.

</details>


### [118] [Your Attention Matters: to Improve Model Robustness to Noise and Spurious Correlations](https://arxiv.org/abs/2507.20453)
*Camilo Tamayo-Rousseau,Yunjia Zhao,Yiqun Zhang,Randall Balestriero*

Main category: cs.LG

TL;DR: 研究了不同自注意力机制在数据损坏情况下的鲁棒性，发现Doubly Stochastic注意力最稳健。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在Transformer中广泛应用，但其对噪声和伪相关性的鲁棒性尚未充分研究。

Method: 评估了Softmax、Sigmoid、Linear、Doubly Stochastic和Cosine注意力在Vision Transformers中，针对CIFAR-10、CIFAR-100和Imagenette数据集的数据损坏场景。

Result: Doubly Stochastic注意力表现最稳健。

Conclusion: 研究结果为在数据不完美时选择自注意力机制提供了指导。

Abstract: Self-attention mechanisms are foundational to Transformer architectures,
supporting their impressive success in a wide range of tasks. While there are
many self-attention variants, their robustness to noise and spurious
correlations has not been well studied. This study evaluates Softmax, Sigmoid,
Linear, Doubly Stochastic, and Cosine attention within Vision Transformers
under different data corruption scenarios. Through testing across the CIFAR-10,
CIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is
the most robust. Our findings inform self-attention selection in contexts with
imperfect data.

</details>


### [119] [Shapley-Value-Based Graph Sparsification for GNN Inference](https://arxiv.org/abs/2507.20460)
*Selahattin Akkas,Ariful Azad*

Main category: cs.LG

TL;DR: 论文提出了一种基于Shapley值的图稀疏化方法，用于提升图神经网络的推理效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的图稀疏化方法通常仅依赖非负重要性评分，限制了其适用性。Shapley值能提供正负贡献评分，更公平地分配重要性。

Method: 利用Shapley值生成节点预测的正负重要性评分，通过评估图的多个子集，实现更优的剪枝策略。

Result: 基于Shapley值的稀疏化方法在保持预测性能的同时显著降低了图的复杂度。

Conclusion: Shapley值方法在提升图神经网络推理效率和可解释性方面具有优势。

Abstract: Graph sparsification is a key technique for improving inference efficiency in
Graph Neural Networks by removing edges with minimal impact on predictions. GNN
explainability methods generate local importance scores, which can be
aggregated into global scores for graph sparsification. However, many
explainability methods produce only non-negative scores, limiting their
applicability for sparsification. In contrast, Shapley value based methods
assign both positive and negative contributions to node predictions, offering a
theoretically robust and fair allocation of importance by evaluating many
subsets of graphs. Unlike gradient-based or perturbation-based explainers,
Shapley values enable better pruning strategies that preserve influential edges
while removing misleading or adversarial connections. Our approach shows that
Shapley value-based graph sparsification maintains predictive performance while
significantly reducing graph complexity, enhancing both interpretability and
efficiency in GNN inference.

</details>


### [120] [Conditional Diffusion Models for Global Precipitation Map Inpainting](https://arxiv.org/abs/2507.20478)
*Daiko Kishikawa,Yuka Muto,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 该研究提出了一种基于条件扩散模型的机器学习方法，用于填补卫星降水数据中的缺失区域，通过3D U-Net和时空信息生成更一致的降水图。


<details>
  <summary>Details</summary>
Motivation: 全球卫星降水监测（如GSMaP）因微波传感器轨道特性存在大量缺失区域，现有插值方法导致空间不连续性。

Method: 采用条件扩散模型和3D U-Net，结合红外图像、经纬度网格和物理时间输入，利用ERA5降水数据训练模型。

Result: 在2024年的评估中，该方法生成的降水图比传统方法更具时空一致性。

Conclusion: 条件扩散模型有望改善全球降水监测的准确性。

Abstract: Incomplete satellite-based precipitation presents a significant challenge in
global monitoring. For example, the Global Satellite Mapping of Precipitation
(GSMaP) from JAXA suffers from substantial missing regions due to the orbital
characteristics of satellites that have microwave sensors, and its current
interpolation methods often result in spatial discontinuities. In this study,
we formulate the completion of the precipitation map as a video inpainting task
and propose a machine learning approach based on conditional diffusion models.
Our method employs a 3D U-Net with a 3D condition encoder to reconstruct
complete precipitation maps by leveraging spatio-temporal information from
infrared images, latitude-longitude grids, and physical time inputs. Training
was carried out on ERA5 hourly precipitation data from 2020 to 2023. We
generated a pseudo-GSMaP dataset by randomly applying GSMaP masks to ERA maps.
Performance was evaluated for the calendar year 2024, and our approach produces
more spatio-temporally consistent inpainted precipitation maps compared to
conventional methods. These results indicate the potential to improve global
precipitation monitoring using the conditional diffusion models.

</details>


### [121] [HIAL: A New Paradigm for Hypergraph Active Learning via Influence Maximization](https://arxiv.org/abs/2507.20490)
*Yanheng Hou,Xunkai Li,Zhenjun Li,Bing Zhou,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: HIAL是一种专为超图设计的主动学习框架，通过创新的高阶交互感知传播机制，显著提升了性能、效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图主动学习方法在超图上表现不佳，因为它们破坏了高阶结构信息，HIAL旨在解决这一问题。

Method: 将超图主动学习问题重新定义为影响力最大化任务，提出双视角影响力函数，结合特征空间覆盖和拓扑影响力评估。

Result: 在七个公开数据集上，HIAL显著优于现有基线方法。

Conclusion: HIAL为超图主动学习提供了高效且强大的新范式。

Abstract: In recent years, Hypergraph Neural Networks (HNNs) have demonstrated immense
potential in handling complex systems with high-order interactions. However,
acquiring large-scale, high-quality labeled data for these models is costly,
making Active Learning (AL) a critical technique. Existing Graph Active
Learning (GAL) methods, when applied to hypergraphs, often rely on techniques
like "clique expansion," which destroys the high-order structural information
crucial to a hypergraph's success, thereby leading to suboptimal performance.
To address this challenge, we introduce HIAL (Hypergraph Active Learning), a
native active learning framework designed specifically for hypergraphs. We
innovatively reformulate the Hypergraph Active Learning (HAL) problem as an
Influence Maximization task. The core of HIAL is a dual-perspective influence
function that, based on our novel "High-Order Interaction-Aware (HOI-Aware)"
propagation mechanism, synergistically evaluates a node's feature-space
coverage (via Magnitude of Influence, MoI) and its topological influence (via
Expected Diffusion Value, EDV). We prove that this objective function is
monotone and submodular, thus enabling the use of an efficient greedy algorithm
with a formal (1-1/e) approximation guarantee. Extensive experiments on seven
public datasets demonstrate that HIAL significantly outperforms
state-of-the-art baselines in terms of performance, efficiency, generality, and
robustness, establishing an efficient and powerful new paradigm for active
learning on hypergraphs.

</details>


### [122] [Mixture of Length and Pruning Experts for Knowledge Graphs Reasoning](https://arxiv.org/abs/2507.20498)
*Enjun Du,Siyi Liu,Yongqi Zhang*

Main category: cs.LG

TL;DR: MoKGR是一种混合专家框架，通过自适应路径探索提升知识图谱推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在知识图谱推理中路径探索策略僵化，无法适应多样化的语言上下文和语义细微差别。

Method: 提出MoKGR框架，包含长度专家和剪枝专家，分别自适应选择路径长度和评估路径信息量。

Result: 在多种基准测试中，MoKGR在传递和归纳设置下均表现优异。

Conclusion: 个性化路径探索能有效提升知识图谱推理性能。

Abstract: Knowledge Graph (KG) reasoning, which aims to infer new facts from structured
knowledge repositories, plays a vital role in Natural Language Processing (NLP)
systems. Its effectiveness critically depends on constructing informative and
contextually relevant reasoning paths. However, existing graph neural networks
(GNNs) often adopt rigid, query-agnostic path-exploration strategies, limiting
their ability to adapt to diverse linguistic contexts and semantic nuances. To
address these limitations, we propose \textbf{MoKGR}, a mixture-of-experts
framework that personalizes path exploration through two complementary
components: (1) a mixture of length experts that adaptively selects and weights
candidate path lengths according to query complexity, providing query-specific
reasoning depth; and (2) a mixture of pruning experts that evaluates candidate
paths from a complementary perspective, retaining the most informative paths
for each query. Through comprehensive experiments on diverse benchmark, MoKGR
demonstrates superior performance in both transductive and inductive settings,
validating the effectiveness of personalized path exploration in KGs reasoning.

</details>


### [123] [DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning](https://arxiv.org/abs/2507.20499)
*Linh Le Pham Van,Minh Hoang Nguyen,Duc Kieu,Hung Le,Hung The Tran,Sunil Gupta*

Main category: cs.LG

TL;DR: 提出DmC框架，通过k-NN和扩散模型解决跨域离线RL中目标数据有限的问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量目标数据，不适用于实际场景。DmC针对目标数据有限的情况，解决数据集不平衡和部分域重叠问题。

Method: 使用k-NN估计域接近度，结合扩散模型生成与目标域更匹配的源样本。

Result: 在MuJoCo环境中，DmC显著优于现有方法，性能提升显著。

Conclusion: DmC为跨域离线RL提供了一种高效且实用的解决方案。

Abstract: Cross-domain offline reinforcement learning (RL) seeks to enhance sample
efficiency in offline RL by utilizing additional offline source datasets. A key
challenge is to identify and utilize source samples that are most relevant to
the target domain. Existing approaches address this challenge by measuring
domain gaps through domain classifiers, target transition dynamics modeling, or
mutual information estimation using contrastive loss. However, these methods
often require large target datasets, which is impractical in many real-world
scenarios. In this work, we address cross-domain offline RL under a limited
target data setting, identifying two primary challenges: (1) Dataset imbalance,
which is caused by large source and small target datasets and leads to
overfitting in neural network-based domain gap estimators, resulting in
uninformative measurements; and (2) Partial domain overlap, where only a subset
of the source data is closely aligned with the target domain. To overcome these
issues, we propose DmC, a novel framework for cross-domain offline RL with
limited target samples. Specifically, DmC utilizes $k$-nearest neighbor
($k$-NN) based estimation to measure domain proximity without neural network
training, effectively mitigating overfitting. Then, by utilizing this domain
proximity, we introduce a nearest-neighbor-guided diffusion model to generate
additional source samples that are better aligned with the target domain, thus
enhancing policy learning with more effective source samples. Through
theoretical analysis and extensive experiments in diverse MuJoCo environments,
we demonstrate that DmC significantly outperforms state-of-the-art cross-domain
offline RL methods, achieving substantial performance gains.

</details>


### [124] [Customize Multi-modal RAI Guardrails with Precedent-based predictions](https://arxiv.org/abs/2507.20503)
*Cheng-Fu Yang,Thanh Tran,Christos Christodoulopoulos,Weitong Ruan,Rahul Gupta,Kai-Wei Chang*

Main category: cs.LG

TL;DR: 提出了一种基于‘先例’的多模态护栏方法，通过利用类似输入的历史推理过程，增强护栏的灵活性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在适应新政策或自定义政策时存在限制，需要大量重新训练或难以全面整合政策。

Method: 采用‘先例’而非固定政策，引入批评-修订机制收集高质量先例，并提出两种利用先例的策略。

Result: 实验表明，该方法在少样本和全数据集场景下优于现有方法，且对新政策具有更好的泛化能力。

Conclusion: 基于先例的方法显著提升了护栏的灵活性和适应性，适用于多政策和动态用户标准。

Abstract: A multi-modal guardrail must effectively filter image content based on
user-defined policies, identifying material that may be hateful, reinforce
harmful stereotypes, contain explicit material, or spread misinformation.
Deploying such guardrails in real-world applications, however, poses
significant challenges. Users often require varied and highly customizable
policies and typically cannot provide abundant examples for each custom policy.
Consequently, an ideal guardrail should be scalable to the multiple policies
and adaptable to evolving user standards with minimal retraining. Existing
fine-tuning methods typically condition predictions on pre-defined policies,
restricting their generalizability to new policies or necessitating extensive
retraining to adapt. Conversely, training-free methods struggle with limited
context lengths, making it difficult to incorporate all the policies
comprehensively. To overcome these limitations, we propose to condition model's
judgment on "precedents", which are the reasoning processes of prior data
points similar to the given input. By leveraging precedents instead of fixed
policies, our approach greatly enhances the flexibility and adaptability of the
guardrail. In this paper, we introduce a critique-revise mechanism for
collecting high-quality precedents and two strategies that utilize precedents
for robust prediction. Experimental results demonstrate that our approach
outperforms previous methods across both few-shot and full-dataset scenarios
and exhibits superior generalization to novel policies.

</details>


### [125] [Attributed Graph Clustering with Multi-Scale Weight-Based Pairwise Coarsening and Contrastive Learning](https://arxiv.org/abs/2507.20505)
*Binxiong Li,Yuefei Wang,Binyu Zhao,Heyang Gao,Benhan Yang,Quanzhou Luo,Xue Li,Xu Xiang,Yujie Liu,Huijie Tang*

Main category: cs.LG

TL;DR: MPCCL模型通过多尺度粗化和对比学习解决图聚类中的长程依赖、特征崩溃和信息丢失问题，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉高阶图特征，对比学习过度依赖局部结构，而图粗化方法丢失细粒度信息。MPCCL旨在解决这些问题。

Method: 采用多尺度粗化策略和一对多对比学习范式，结合图重建损失和KL散度确保节点表示的一致性。

Result: 在ACM数据集上NMI提升15.24%，在Citeseer、Cora和DBLP等小规模数据集上表现稳健。

Conclusion: MPCCL通过创新方法有效解决了现有技术的局限性，显著提升了图聚类性能。

Abstract: This study introduces the Multi-Scale Weight-Based Pairwise Coarsening and
Contrastive Learning (MPCCL) model, a novel approach for attributed graph
clustering that effectively bridges critical gaps in existing methods,
including long-range dependency, feature collapse, and information loss.
Traditional methods often struggle to capture high-order graph features due to
their reliance on low-order attribute information, while contrastive learning
techniques face limitations in feature diversity by overemphasizing local
neighborhood structures. Similarly, conventional graph coarsening methods,
though reducing graph scale, frequently lose fine-grained structural details.
MPCCL addresses these challenges through an innovative multi-scale coarsening
strategy, which progressively condenses the graph while prioritizing the
merging of key edges based on global node similarity to preserve essential
structural information. It further introduces a one-to-many contrastive
learning paradigm, integrating node embeddings with augmented graph views and
cluster centroids to enhance feature diversity, while mitigating feature
masking issues caused by the accumulation of high-frequency node weights during
multi-scale coarsening. By incorporating a graph reconstruction loss and KL
divergence into its self-supervised learning framework, MPCCL ensures
cross-scale consistency of node representations. Experimental evaluations
reveal that MPCCL achieves a significant improvement in clustering performance,
including a remarkable 15.24% increase in NMI on the ACM dataset and notable
robust gains on smaller-scale datasets such as Citeseer, Cora and DBLP.

</details>


### [126] [Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations](https://arxiv.org/abs/2507.20513)
*Shiva Sinaei,Chuanjun Zheng,Kaan Akşit,Daisuke Iwai*

Main category: cs.LG

TL;DR: Ray2Ray利用隐式神经表示高效建模光学系统，避免逐面计算，实现端到端建模。


<details>
  <summary>Details</summary>
Motivation: 传统光线追踪逐面计算效率低，需更高效方法。

Method: 提出Ray2Ray，通过神经表示学习光线映射，实现单次端到端建模。

Result: 在9个现成光学系统上训练，位置误差约1μm，角度偏差约0.01度。

Conclusion: 神经表示可作为光学光线追踪的有效替代方案。

Abstract: Ray tracing is a widely used technique for modeling optical systems,
involving sequential surface-by-surface computations, which can be
computationally intensive. We propose Ray2Ray, a novel method that leverages
implicit neural representations to model optical systems with greater
efficiency, eliminating the need for surface-by-surface computations in a
single pass end-to-end model. Ray2Ray learns the mapping between rays emitted
from a given source and their corresponding rays after passing through a given
optical system in a physically accurate manner. We train Ray2Ray on nine
off-the-shelf optical systems, achieving positional errors on the order of
1{\mu}m and angular deviations on the order 0.01 degrees in the estimated
output rays. Our work highlights the potential of neural representations as a
proxy for optical raytracer.

</details>


### [127] [Kernel Learning for Sample Constrained Black-Box Optimization](https://arxiv.org/abs/2507.20533)
*Rajalaxmi Rajagopalan,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.LG

TL;DR: 提出了一种名为KOBO的新方法，通过在高斯过程的潜在空间中学习连续核空间，显著降低了样本预算，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在高维空间中优化未知函数时，采样成本高昂，需要减少样本预算。

Method: 在变分自编码器的潜在空间中创建连续核空间，并通过辅助优化确定最佳核。

Result: KOBO在合成基准函数和实际应用中均表现优异，显著降低了样本预算。

Conclusion: KOBO方法在样本预算有限的情况下表现出色，适用于多种实际场景。

Abstract: Black box optimization (BBO) focuses on optimizing unknown functions in
high-dimensional spaces. In many applications, sampling the unknown function is
expensive, imposing a tight sample budget. Ongoing work is making progress on
reducing the sample budget by learning the shape/structure of the function,
known as kernel learning. We propose a new method to learn the kernel of a
Gaussian Process. Our idea is to create a continuous kernel space in the latent
space of a variational autoencoder, and run an auxiliary optimization to
identify the best kernel. Results show that the proposed method, Kernel
Optimized Blackbox Optimization (KOBO), outperforms state of the art by
estimating the optimal at considerably lower sample budgets. Results hold not
only across synthetic benchmark functions but also in real applications. We
show that a hearing aid may be personalized with fewer audio queries to the
user, or a generative model could converge to desirable images from limited
user ratings.

</details>


### [128] [Kimi K2: Open Agentic Intelligence](https://arxiv.org/abs/2507.20534)
*Kimi Team,Yifan Bai,Yiping Bao,Guanduo Chen,Jiahao Chen,Ningxin Chen,Ruijue Chen,Yanru Chen,Yuankun Chen,Yutian Chen,Zhuofu Chen,Jialei Cui,Hao Ding,Mengnan Dong,Angang Du,Chenzhuang Du,Dikang Du,Yulun Du,Yu Fan,Yichen Feng,Kelin Fu,Bofei Gao,Hongcheng Gao,Peizhong Gao,Tong Gao,Xinran Gu,Longyu Guan,Haiqing Guo,Jianhang Guo,Hao Hu,Xiaoru Hao,Tianhong He,Weiran He,Wenyang He,Chao Hong,Yangyang Hu,Zhenxing Hu,Weixiao Huang,Zhiqi Huang,Zihao Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yongsheng Kang,Guokun Lai,Cheng Li,Fang Li,Haoyang Li,Ming Li,Wentao Li,Yanhao Li,Yiwei Li,Zhaowei Li,Zheming Li,Hongzhan Lin,Xiaohan Lin,Zongyu Lin,Chengyin Liu,Chenyu Liu,Hongzhang Liu,Jingyuan Liu,Junqi Liu,Liang Liu,Shaowei Liu,T. Y. Liu,Tianwei Liu,Weizhou Liu,Yangyang Liu,Yibo Liu,Yiping Liu,Yue Liu,Zhengying Liu,Enzhe Lu,Lijun Lu,Shengling Ma,Xinyu Ma,Yingwei Ma,Shaoguang Mao,Jie Mei,Xin Men,Yibo Miao,Siyuan Pan,Yebo Peng,Ruoyu Qin,Bowen Qu,Zeyu Shang,Lidong Shi,Shengyuan Shi,Feifan Song,Jianlin Su,Zhengyuan Su,Xinjie Sun,Flood Sung,Heyi Tang,Jiawen Tao,Qifeng Teng,Chensi Wang,Dinglu Wang,Feng Wang,Haiming Wang,Jianzhou Wang,Jiaxing Wang,Jinhong Wang,Shengjie Wang,Shuyi Wang,Yao Wang,Yejie Wang,Yiqin Wang,Yuxin Wang,Yuzhi Wang,Zhaoji Wang,Zhengtao Wang,Zhexu Wang,Chu Wei,Qianqian Wei,Wenhao Wu,Xingzhe Wu,Yuxin Wu,Chenjun Xiao,Xiaotong Xie,Weimin Xiong,Boyu Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinran Xu,Yangchuan Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Xiaofei Yang,Ying Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Xingcheng Yao,Wenjie Ye,Zhuorui Ye,Bohong Yin,Longhui Yu,Enming Yuan,Hongbang Yuan,Mengjie Yuan,Haobing Zhan,Dehao Zhang,Hao Zhang,Wanlu Zhang,Xiaobin Zhang,Yangkun Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Haotian Zhao,Yikai Zhao,Huabin Zheng,Shaojie Zheng,Jianren Zhou,Xinyu Zhou,Zaida Zhou,Zhen Zhu,Weiyu Zhuang,Xinxing Zu*

Main category: cs.LG

TL;DR: Kimi K2是一个基于Mixture-of-Experts (MoE)的大型语言模型，具有32亿激活参数和1万亿总参数，通过MuonClip优化器解决了训练不稳定性问题，并在多阶段后训练中提升了能力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个在非思考任务中表现优异的大型语言模型，特别是在软件工程和代理任务中，同时提供开源模型以促进研究。

Method: 使用MuonClip优化器（基于Muon改进的QK-clip技术）进行预训练，后训练阶段包括大规模代理数据合成和联合强化学习（RL）。

Result: Kimi K2在多个基准测试中表现优异，如Tau2-Bench（66.1）、ACEBench（En）（76.5）等，超越了大多数开源和闭源基线模型。

Conclusion: Kimi K2是目前最强大的开源大型语言模型之一，尤其在软件工程和代理任务中表现出色，并开源了模型检查点以推动研究。

Abstract: We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32
billion activated parameters and 1 trillion total parameters. We propose the
MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to
address training instability while enjoying the advanced token efficiency of
Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero
loss spike. During post-training, K2 undergoes a multi-stage post-training
process, highlighted by a large-scale agentic data synthesis pipeline and a
joint reinforcement learning (RL) stage, where the model improves its
capabilities through interactions with real and synthetic environments.
  Kimi K2 achieves state-of-the-art performance among open-source non-thinking
models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on
Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on
SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in
non-thinking settings. It also exhibits strong capabilities in coding,
mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6,
49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without
extended thinking. These results position Kimi K2 as one of the most capable
open-source large language models to date, particularly in software engineering
and agentic tasks. We release our base and post-trained model checkpoints to
facilitate future research and applications of agentic intelligence.

</details>


### [129] [DAG-AFL:Directed Acyclic Graph-based Asynchronous Federated Learning](https://arxiv.org/abs/2507.20571)
*Shuaipeng Zhang,Lanju Kong,Yixin Zhang,Wei He,Yongqing Zheng,Han Yu,Lizhen Cui*

Main category: cs.LG

TL;DR: 论文提出了一种基于有向无环图（DAG）的异步联邦学习框架（DAG-AFL），以解决联邦学习中的异步参与和数据异构问题，同时减少区块链引入的资源开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的分布式特性导致全局模型易受攻击且需要大量客户端设备协调，区块链虽提供了一种解决方案，但传统共识机制资源消耗大，影响效率。

Method: 开发了一种考虑时间新鲜度、节点可达性和模型准确性的tip选择算法，并采用DAG可信验证策略。

Result: 在三个基准数据集上对比八种先进方法，DAG-AFL平均提升训练效率22.7%和模型准确率6.5%。

Conclusion: DAG-AFL框架显著提升了联邦学习的效率和准确性，同时降低了资源开销。

Abstract: Due to the distributed nature of federated learning (FL), the vulnerability
of the global model and the need for coordination among many client devices
pose significant challenges. As a promising decentralized, scalable and secure
solution, blockchain-based FL methods have attracted widespread attention in
recent years. However, traditional consensus mechanisms designed for Proof of
Work (PoW) similar to blockchain incur substantial resource consumption and
compromise the efficiency of FL, particularly when participating devices are
wireless and resource-limited. To address asynchronous client participation and
data heterogeneity in FL, while limiting the additional resource overhead
introduced by blockchain, we propose the Directed Acyclic Graph-based
Asynchronous Federated Learning (DAG-AFL) framework. We develop a tip selection
algorithm that considers temporal freshness, node reachability and model
accuracy, with a DAG-based trusted verification strategy. Extensive experiments
on 3 benchmarking datasets against eight state-of-the-art approaches
demonstrate that DAG-AFL significantly improves training efficiency and model
accuracy by 22.7% and 6.5% on average, respectively.

</details>


### [130] [Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy](https://arxiv.org/abs/2507.20573)
*Yaxin Xiao,Qingqing Ye,Li Hu,Huadi Zheng,Haibo Hu,Zi Liang,Haoyang Li,Yijie Jiao*

Main category: cs.LG

TL;DR: 论文揭示了近似遗忘算法未能充分保护被遗忘数据的隐私，提出了一种新的攻击方法（ReA），并开发了一种双阶段遗忘框架以降低隐私风险。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于发现近似遗忘算法中存在的隐私漏洞，尤其是隐式残留问题，并提出解决方案以保护被遗忘数据的隐私。

Method: 论文提出了Reminiscence Attack（ReA）来利用残留信息进行隐私攻击，并开发了一种双阶段近似遗忘框架以消除残留并防止伪收敛。

Result: ReA攻击在推断类级和样本级成员隐私时分别比现有攻击高1.90倍和1.12倍的准确率。双阶段框架将自适应隐私攻击的准确率降至接近随机猜测，计算成本仅为完全重新训练的2-12%。

Conclusion: 论文表明近似遗忘算法存在隐私风险，提出的双阶段框架在保持高效遗忘的同时显著降低了隐私攻击的成功率。

Abstract: Machine unlearning enables the removal of specific data from ML models to
uphold the right to be forgotten. While approximate unlearning algorithms offer
efficient alternatives to full retraining, this work reveals that they fail to
adequately protect the privacy of unlearned data. In particular, these
algorithms introduce implicit residuals which facilitate privacy attacks
targeting at unlearned data. We observe that these residuals persist regardless
of model architectures, parameters, and unlearning algorithms, exposing a new
attack surface beyond conventional output-based leakage. Based on this insight,
we propose the Reminiscence Attack (ReA), which amplifies the correlation
between residuals and membership privacy through targeted fine-tuning
processes. ReA achieves up to 1.90x and 1.12x higher accuracy than prior
attacks when inferring class-wise and sample-wise membership, respectively. To
mitigate such residual-induced privacy risk, we develop a dual-phase
approximate unlearning framework that first eliminates deep-layer unlearned
data traces and then enforces convergence stability to prevent models from
"pseudo-convergence", where their outputs are similar to retrained models but
still preserve unlearned residuals. Our framework works for both classification
and generation tasks. Experimental evaluations confirm that our approach
maintains high unlearning efficacy, while reducing the adaptive privacy attack
accuracy to nearly random guess, at the computational cost of 2-12% of full
retraining from scratch.

</details>


### [131] [Fusing CFD and measurement data using transfer learning](https://arxiv.org/abs/2507.20576)
*Alexander Barklage,Philipp Bekemeyer*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经网络的非线性方法，通过迁移学习结合仿真和测量数据，改进了传统线性方法（如POD）在气动分析中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统气动分析方法（如POD）是线性的，无法有效处理非线性问题，且仿真数据分辨率高但误差大，测量数据稀疏但准确，需一种能结合两者优势的方法。

Method: 采用神经网络分两步训练：先在仿真数据上学习空间特征，再通过迁移学习在测量数据上修正系统误差，仅重新训练部分网络。

Result: 相比POD方法，神经网络在非线性区域生成更物理的解，且能适应任意流动条件，适用于飞行力学设计、结构尺寸和认证。

Conclusion: 该方法通用性强，未来可应用于更复杂的神经网络架构。

Abstract: Aerodynamic analysis during aircraft design usually involves methods of
varying accuracy and spatial resolution, which all have their advantages and
disadvantages. It is therefore desirable to create data-driven models which
effectively combine these advantages. Such data fusion methods for distributed
quantities mainly rely on proper orthogonal decomposition as of now, which is a
linear method. In this paper, we introduce a non-linear method based on neural
networks combining simulation and measurement data via transfer learning. The
network training accounts for the heterogeneity of the data, as simulation data
usually features a high spatial resolution, while measurement data is sparse
but more accurate. In a first step, the neural network is trained on simulation
data to learn spatial features of the distributed quantities. The second step
involves transfer learning on the measurement data to correct for systematic
errors between simulation and measurement by only re-training a small subset of
the entire neural network model. This approach is applied to a multilayer
perceptron architecture and shows significant improvements over the established
method based on proper orthogonal decomposition by producing more physical
solutions near nonlinearities. In addition, the neural network provides
solutions at arbitrary flow conditions, thus making the model useful for flight
mechanical design, structural sizing, and certification. As the proposed
training strategy is very general, it can also be applied to more complex
neural network architectures in the future.

</details>


### [132] [PhaseNAS: Language-Model Driven Architecture Search with Dynamic Phase Adaptation](https://arxiv.org/abs/2507.20592)
*Fei Kong,Xiaohan Shan,Yanwei Hu,Jianmin Li*

Main category: cs.LG

TL;DR: PhaseNAS是一种基于LLM的NAS框架，通过动态阶段转换和结构化模板语言，显著提升了搜索效率和架构性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM-based NAS方法中静态搜索策略和模糊架构表示的问题，提升复杂任务中的搜索效率和准确性。

Method: 采用动态阶段转换（实时分数阈值引导）和结构化架构模板语言，实现高效且一致的代码生成。

Result: 在NAS-Bench-Macro基准测试中表现优异；CIFAR-10/100上搜索时间减少86%；YOLOv8变体在目标检测中mAP更高且资源成本更低。

Conclusion: PhaseNAS在多种视觉任务中实现了高效、自适应且通用的NAS。

Abstract: Neural Architecture Search (NAS) is challenged by the trade-off between
search space exploration and efficiency, especially for complex tasks. While
recent LLM-based NAS methods have shown promise, they often suffer from static
search strategies and ambiguous architecture representations. We propose
PhaseNAS, an LLM-based NAS framework with dynamic phase transitions guided by
real-time score thresholds and a structured architecture template language for
consistent code generation. On the NAS-Bench-Macro benchmark, PhaseNAS
consistently discovers architectures with higher accuracy and better rank. For
image classification (CIFAR-10/100), PhaseNAS reduces search time by up to 86%
while maintaining or improving accuracy. In object detection, it automatically
produces YOLOv8 variants with higher mAP and lower resource cost. These results
demonstrate that PhaseNAS enables efficient, adaptive, and generalizable NAS
across diverse vision tasks.

</details>


### [133] [Deep Generative Models of Evolution: SNP-level Population Adaptation by Genomic Linkage Incorporation](https://arxiv.org/abs/2507.20644)
*Julia Siekiera,Christian Schlötterer,Stefan Kramer*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度生成神经网络的方法，用于模拟和估计等位基因频率轨迹的分布，特别适用于群体基因组学中的Evolve and Resequencing实验。


<details>
  <summary>Details</summary>
Motivation: 传统统计模型（如Wright-Fisher模型）在假设和参数不确定性方面存在局限性，而深度生成神经网络能够整合多变量依赖性和降噪，但因其高数据需求和解释性挑战，尚未广泛应用于群体基因组学。

Method: 作者引入了一种深度生成神经网络，通过嵌入单核苷酸多态性（SNPs）及其邻近位点的信息，模拟基于时间经验观察的进化概念。

Result: 在模拟实验中，模型能够捕捉等位基因频率轨迹的分布，并在连锁不平衡（LD）估计方面表现出色，提供了与现有方法竞争的结果。

Conclusion: 该模型展示了深度生成模型在群体基因组学中的潜力，尤其是在处理Pool-Seq数据时，能够估计传统方法难以获取的连锁不平衡信息。

Abstract: The investigation of allele frequency trajectories in populations evolving
under controlled environmental pressures has become a popular approach to study
evolutionary processes on the molecular level. Statistical models based on
well-defined evolutionary concepts can be used to validate different hypotheses
about empirical observations. Despite their popularity, classic statistical
models like the Wright-Fisher model suffer from simplified assumptions such as
the independence of selected loci along a chromosome and uncertainty about the
parameters. Deep generative neural networks offer a powerful alternative known
for the integration of multivariate dependencies and noise reduction. Due to
their high data demands and challenging interpretability they have, so far, not
been widely considered in the area of population genomics. To address the
challenges in the area of Evolve and Resequencing experiments (E&R) based on
pooled sequencing (Pool-Seq) data, we introduce a deep generative neural
network that aims to model a concept of evolution based on empirical
observations over time. The proposed model estimates the distribution of allele
frequency trajectories by embedding the observations from single nucleotide
polymorphisms (SNPs) with information from neighboring loci. Evaluation on
simulated E&R experiments demonstrates the model's ability to capture the
distribution of allele frequency trajectories and illustrates the
representational power of deep generative models on the example of linkage
disequilibrium (LD) estimation. Inspecting the internally learned
representations enables estimating pairwise LD, which is typically inaccessible
in Pool-Seq data. Our model provides competitive LD estimation in Pool-Seq data
high degree of LD when compared to existing methods.

</details>


### [134] [Novel Pivoted Cholesky Decompositions for Efficient Gaussian Process Inference](https://arxiv.org/abs/2507.20678)
*Filip de Roos,Fabio Muratore*

Main category: cs.LG

TL;DR: 论文探讨了通过改进Cholesky分解的枢轴策略，提出新的枢轴选择方法，提高了算法效率，并在高斯过程任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: Cholesky分解在对称正定矩阵的线性系统中至关重要，但传统枢轴选择策略可能效率不足。本文旨在通过改进枢轴策略提升其数值稳定性和效率。

Method: 提出新的枢轴选择策略，基于贝叶斯非参数推断中的熵最大化思想，优化分解过程。算法支持动态更新观测信息，并针对特定任务实现定制化。

Result: 新策略在高斯过程任务（稀疏回归和迭代求解器推断）中表现优异，多数情况下优于传统方法，且计算开销可忽略。

Conclusion: 改进的枢轴策略显著提升了Cholesky分解的效率，适用于机器学习中的高斯过程任务，具有实际应用价值。

Abstract: The Cholesky decomposition is a fundamental tool for solving linear systems
with symmetric and positive definite matrices which are ubiquitous in linear
algebra, optimization, and machine learning. Its numerical stability can be
improved by introducing a pivoting strategy that iteratively permutes the rows
and columns of the matrix. The order of pivoting indices determines how
accurately the intermediate decomposition can reconstruct the original matrix,
thus is decisive for the algorithm's efficiency in the case of early
termination. Standard implementations select the next pivot from the largest
value on the diagonal. In the case of Bayesian nonparametric inference, this
strategy corresponds to greedy entropy maximization, which is often used in
active learning and design of experiments. We explore this connection in detail
and deduce novel pivoting strategies for the Cholesky decomposition. The
resulting algorithms are more efficient at reducing the uncertainty over a data
set, can be updated to include information about observations, and additionally
benefit from a tailored implementation. We benchmark the effectiveness of the
new selection strategies on two tasks important to Gaussian processes: sparse
regression and inference based on preconditioned iterative solvers. Our results
show that the proposed selection strategies are either on par or, in most
cases, outperform traditional baselines while requiring a negligible amount of
additional computation.

</details>


### [135] [Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks](https://arxiv.org/abs/2507.20708)
*Valentin Lafargue,Adriana Laurindo Monteiro,Emmanuelle Claeys,Laurent Risser,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 论文探讨了如何通过数据操纵满足公平性标准，并研究了如何检测这种操纵。


<details>
  <summary>Details</summary>
Motivation: 随着AI算法在现实应用中的广泛部署，证明其合规性成为重要挑战，尤其是满足欧盟AI法案的要求。

Method: 提出数学方法（熵或最优传输投影）修改经验分布以满足公平性约束，并研究如何检测数据操纵。

Result: 验证了方法在经典表格数据集上的有效性，并提供了检测数据操纵的建议。

Conclusion: 研究为审计者提供了工具和建议，以应对潜在的数据操纵行为。

Abstract: Proving the compliance of AI algorithms has become an important challenge
with the growing deployment of such algorithms for real-life applications.
Inspecting possible biased behaviors is mandatory to satisfy the constraints of
the regulations of the EU Artificial Intelligence's Act. Regulation-driven
audits increasingly rely on global fairness metrics, with Disparate Impact
being the most widely used. Yet such global measures depend highly on the
distribution of the sample on which the measures are computed. We investigate
first how to manipulate data samples to artificially satisfy fairness criteria,
creating minimally perturbed datasets that remain statistically
indistinguishable from the original distribution while satisfying prescribed
fairness constraints. Then we study how to detect such manipulation. Our
analysis (i) introduces mathematically sound methods for modifying empirical
distributions under fairness constraints using entropic or optimal transport
projections, (ii) examines how an auditee could potentially circumvent fairness
inspections, and (iii) offers recommendations to help auditors detect such data
manipulations. These results are validated through experiments on classical
tabular datasets in bias detection.

</details>


### [136] [Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI](https://arxiv.org/abs/2507.20714)
*Asma Sadia Khan,Fariba Tasnia Khan,Tanjim Mahmud,Salman Karim Khan,Rishita Chakma,Nahed Sharmen,Mohammad Shahadat Hossain,Karl Andersson*

Main category: cs.LG

TL;DR: 提出了一种结合BERT和随机森林的可解释AI系统，用于前列腺癌诊断，性能优越且临床可解释。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌诊断需要高级工具，现有方法缺乏可解释性。

Method: 结合BERT处理文本临床笔记和随机森林处理数值实验室数据，采用新型多模态融合策略。

Result: 在PLCO-NIH数据集上达到98%准确率和99% AUC，尤其在中级癌症阶段表现突出。

Conclusion: 该方法在性能、效率和可解释性之间取得平衡，满足前列腺癌诊断需求。

Abstract: Prostate cancer, the second most prevalent male malignancy, requires advanced
diagnostic tools. We propose an explainable AI system combining BERT (for
textual clinical notes) and Random Forest (for numerical lab data) through a
novel multimodal fusion strategy, achieving superior classification performance
on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is
established, our work demonstrates that a simple yet interpretable BERT+RF
pipeline delivers clinically significant improvements - particularly for
intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824
numerical/0.725 textual). SHAP analysis provides transparent feature importance
rankings, while ablation studies prove textual features' complementary value.
This accessible approach offers hospitals a balance of high performance
(F1=89%), computational efficiency, and clinical interpretability - addressing
critical needs in prostate cancer diagnostics.

</details>


### [137] [Uncertainty-driven Embedding Convolution](https://arxiv.org/abs/2507.20718)
*Sungjun Lim,Kangjun Noh,Youngjun Choi,Heeyoung Lee,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出了一种基于不确定性的文本嵌入集成方法UEC，通过后验概率化嵌入和自适应权重计算提升性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型在不同任务中表现不一，且传统集成方法未考虑模型不确定性，限制了鲁棒性。

Method: UEC将确定性嵌入转化为概率嵌入，基于不确定性计算自适应权重，并引入不确定性感知的相似性评分。

Result: 在检索、分类和语义相似性任务中，UEC显著提升了性能和鲁棒性。

Conclusion: UEC通过不确定性建模有效提升了嵌入集成的效果，适用于多种下游任务。

Abstract: Text embeddings are essential components in modern NLP pipelines. While
numerous embedding models have been proposed, their performance varies across
domains, and no single model consistently excels across all tasks. This
variability motivates the use of ensemble techniques to combine complementary
strengths. However, most existing ensemble methods operate on deterministic
embeddings and fail to account for model-specific uncertainty, limiting their
robustness and reliability in downstream applications. To address these
limitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC
first transforms deterministic embeddings into probabilistic ones in a post-hoc
manner. It then computes adaptive ensemble weights based on embedding
uncertainty, grounded in a Bayes-optimal solution under a surrogate loss.
Additionally, UEC introduces an uncertainty-aware similarity function that
directly incorporates uncertainty into similarity scoring. Extensive
experiments on retrieval, classification, and semantic similarity benchmarks
demonstrate that UEC consistently improves both performance and robustness by
leveraging principled uncertainty modeling.

</details>


### [138] [First Hallucination Tokens Are Different from Conditional Ones](https://arxiv.org/abs/2507.20836)
*Jakob Snel,Seong Joon Oh*

Main category: cs.LG

TL;DR: 论文研究了基础模型中幻觉（生成不真实内容）的检测，重点分析了幻觉信号在标记序列中的变化，发现首个幻觉标记的信号更强且更易检测。


<details>
  <summary>Details</summary>
Motivation: 幻觉是基础模型的主要问题之一，实时检测和纠正幻觉对模型可靠性至关重要，但目前对标记级幻觉信号的理解不足。

Method: 利用RAGTruth语料库的标记级注释和复现的logits，分析幻觉信号如何依赖于标记在幻觉片段中的位置。

Result: 首个幻觉标记的信号更强且更易检测，而条件标记的信号较弱。

Conclusion: 研究改进了对标记级幻觉的理解，并发布了分析框架和代码。

Abstract: Hallucination, the generation of untruthful content, is one of the major
concerns regarding foundational models. Detecting hallucinations at the token
level is vital for real-time filtering and targeted correction, yet the
variation of hallucination signals within token sequences is not fully
understood. Leveraging the RAGTruth corpus with token-level annotations and
reproduced logits, we analyse how these signals depend on a token's position
within hallucinated spans, contributing to an improved understanding of
token-level hallucination. Our results show that the first hallucinated token
carries a stronger signal and is more detectable than conditional tokens. We
release our analysis framework, along with code for logit reproduction and
metric computation at https://github.com/jakobsnl/RAGTruth_Xtended.

</details>


### [139] [BuildSTG: A Multi-building Energy Load Forecasting Method using Spatio-Temporal Graph Neural Network](https://arxiv.org/abs/2507.20838)
*Yongzheng Liu,Yiming Wang,Po Xu,Yingjie Xu,Yuntian Chen,Dongxiao Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于时空图神经网络的多建筑能耗预测方法，通过图表示、学习和解释，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉建筑间的空间依赖性，而数据驱动方法在建筑能耗预测中表现出强大潜力。

Method: 构建基于建筑特征和环境因素的图，开发带注意力的多层次图卷积架构，并引入图结构解释方法。

Result: 在Building Data Genome Project 2数据集上表现优于XGBoost、SVR等基线方法，具有鲁棒性和可解释性。

Conclusion: 该方法能有效捕捉建筑间的相似性和空间关系，为建筑能耗预测提供了新思路。

Abstract: Due to the extensive availability of operation data, data-driven methods show
strong capabilities in predicting building energy loads. Buildings with similar
features often share energy patterns, reflected by spatial dependencies in
their operational data, which conventional prediction methods struggle to
capture. To overcome this, we propose a multi-building prediction approach
using spatio-temporal graph neural networks, comprising graph representation,
graph learning, and interpretation. First, a graph is built based on building
characteristics and environmental factors. Next, a multi-level graph
convolutional architecture with attention is developed for energy prediction.
Lastly, a method interpreting the optimized graph structure is introduced.
Experiments on the Building Data Genome Project 2 dataset confirm superior
performance over baselines such as XGBoost, SVR, FCNN, GRU, and Naive,
highlighting the method's robustness, generalization, and interpretability in
capturing meaningful building similarities and spatial relationships.

</details>


### [140] [Towards Explainable Deep Clustering for Time Series Data](https://arxiv.org/abs/2507.20840)
*Udo Schlegel,Gabriel Marques Tavares,Thomas Seidl*

Main category: cs.LG

TL;DR: 本文综述了可解释的深度聚类在时间序列中的应用，总结了现有方法及其实际应用，并提出了六个研究方向以推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 深度聚类在复杂时间序列数据中能发现隐藏模式和分组，但其不透明的决策过程限制了其在安全关键场景中的应用，因此需要可解释性。

Method: 通过收集和分析同行评审和预印本论文，比较了不同应用领域（如医疗、金融、物联网和气候科学）中的方法，重点关注自编码器和注意力架构。

Result: 研究发现现有方法对流式、不规则采样或隐私保护序列的支持有限，且可解释性通常作为附加功能处理。

Conclusion: 提出了六个研究方向，强调将可解释性作为设计目标，为下一代可信赖的深度聚类时间序列分析奠定基础。

Abstract: Deep clustering uncovers hidden patterns and groups in complex time series
data, yet its opaque decision-making limits use in safety-critical settings.
This survey offers a structured overview of explainable deep clustering for
time series, collecting current methods and their real-world applications. We
thoroughly discuss and compare peer-reviewed and preprint papers through
application domains across healthcare, finance, IoT, and climate science. Our
analysis reveals that most work relies on autoencoder and attention
architectures, with limited support for streaming, irregularly sampled, or
privacy-preserved series, and interpretability is still primarily treated as an
add-on. To push the field forward, we outline six research opportunities: (1)
combining complex networks with built-in interpretability; (2) setting up
clear, faithfulness-focused evaluation metrics for unsupervised explanations;
(3) building explainers that adapt to live data streams; (4) crafting
explanations tailored to specific domains; (5) adding human-in-the-loop methods
that refine clusters and explanations together; and (6) improving our
understanding of how time series clustering models work internally. By making
interpretability a primary design goal rather than an afterthought, we propose
the groundwork for the next generation of trustworthy deep clustering time
series analytics.

</details>


### [141] [Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces](https://arxiv.org/abs/2507.20853)
*Saket Tiwari,Omer Gottesman,George Konidaris*

Main category: cs.LG

TL;DR: 论文提出了一种几何视角的理论框架，用于理解连续状态和动作空间在强化学习中的局部可达状态集，并证明了动作空间维度与可达状态流形维度的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在连续状态和动作空间的任务中取得了成功，但理论研究仍主要集中于有限空间。本文旨在填补这一理论空白。

Method: 通过几何视角分析局部可达状态集，使用两层神经策略的训练动态来证明可达状态流形的低维性，并通过实验验证。

Result: 证明了可达状态流形的维度与动作空间维度相关，并在MuJoCo环境和玩具环境中验证了这一理论。

Conclusion: 该理论框架为高自由度控制环境的性能提升提供了新思路，并通过稀疏表示学习进一步验证了其应用价值。

Abstract: Advances in reinforcement learning (RL) have led to its successful
application in complex tasks with continuous state and action spaces. Despite
these advances in practice, most theoretical work pertains to finite state and
action spaces. We propose building a theoretical understanding of continuous
state and action spaces by employing a geometric lens to understand the locally
attained set of states. The set of all parametrised policies learnt through a
semi-gradient based approach induces a set of attainable states in RL. We show
that the training dynamics of a two-layer neural policy induce a low
dimensional manifold of attainable states embedded in the high-dimensional
nominal state space trained using an actor-critic algorithm. We prove that,
under certain conditions, the dimensionality of this manifold is of the order
of the dimensionality of the action space. This is the first result of its
kind, linking the geometry of the state space to the dimensionality of the
action space. We empirically corroborate this upper bound for four MuJoCo
environments and also demonstrate the results in a toy environment with varying
dimensionality. We also show the applicability of this theoretical result by
introducing a local manifold learning layer to the policy and value function
networks to improve the performance in control environments with very high
degrees of freedom by changing one layer of the neural network to learn sparse
representations.

</details>


### [142] [Bi-cephalic self-attended model to classify Parkinson's disease patients with freezing of gait](https://arxiv.org/abs/2507.20862)
*Shomoita Jahid Mitin,Rodrigue Rizk,Maximilian Scherer,Thomas Koeglsperger,Daniel Lench,KC Santosh,Arun Singh*

Main category: cs.LG

TL;DR: 研究开发了一种基于多模态数据的BiSAM模型，结合EEG信号和临床变量，显著提高了帕金森病冻结步态（PDFOG+）的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 当前帕金森病冻结步态的检测方法主观性强或依赖专业设备，需要一种客观、数据驱动的多模态分类方法。

Method: 使用124名参与者（PDFOG+、PDFOG-和健康对照组）的静息态EEG信号和临床变量，训练BiSAM模型，测试了信号、描述和多模态三种模式。

Result: 多模态模型（BiSAM-8和BiSAM-4）分类准确率最高（88%），显著优于信号（55%）和描述（68%）单独模型。

Conclusion: 结合EEG和临床变量的多模态方法为PDFOG+检测提供了高效、可扩展的解决方案，适用于临床监测和早期诊断。

Abstract: Parkinson Disease (PD) often results in motor and cognitive impairments,
including gait dysfunction, particularly in patients with freezing of gait
(FOG). Current detection methods are either subjective or reliant on
specialized gait analysis tools. This study aims to develop an objective,
data-driven, and multi-modal classification model to detect gait dysfunction in
PD patients using resting-state EEG signals combined with demographic and
clinical variables. We utilized a dataset of 124 participants: 42 PD patients
with FOG (PDFOG+), 41 without FOG (PDFOG-), and 41 age-matched healthy
controls. Features extracted from resting-state EEG and descriptive variables
(age, education, disease duration) were used to train a novel Bi-cephalic
Self-Attention Model (BiSAM). We tested three modalities: signal-only,
descriptive-only, and multi-modal, across different EEG channel subsets
(BiSAM-63, -16, -8, and -4). Signal-only and descriptive-only models showed
limited performance, achieving a maximum accuracy of 55% and 68%, respectively.
In contrast, the multi-modal models significantly outperformed both, with
BiSAM-8 and BiSAM-4 achieving the highest classification accuracy of 88%. These
results demonstrate the value of integrating EEG with objective descriptive
features for robust PDFOG+ detection. This study introduces a multi-modal,
attention-based architecture that objectively classifies PDFOG+ using minimal
EEG channels and descriptive variables. This approach offers a scalable and
efficient alternative to traditional assessments, with potential applications
in routine clinical monitoring and early diagnosis of PD-related gait
dysfunction.

</details>


### [143] [Online hierarchical partitioning of the output space in extreme multi-label data stream](https://arxiv.org/abs/2507.20894)
*Lara Neves,Afonso Lourenço,Alberto Cano,Goreti Marreiros*

Main category: cs.LG

TL;DR: iHOMER是一种在线多标签学习框架，通过动态聚类和漂移检测机制，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多标签数据流中的分布演化、高维标签空间和标签依赖性问题需要动态适应的解决方案。

Method: iHOMER使用在线分裂-聚合聚类和全局树学习器，结合漂移检测机制动态调整标签分区。

Result: 在23个数据集上，iHOMER比5种全局基线方法提升23%，比12种局部基线方法提升32%。

Conclusion: iHOMER在在线多标签分类中表现出色，具有动态适应性和鲁棒性。

Abstract: Mining data streams with multi-label outputs poses significant challenges due
to evolving distributions, high-dimensional label spaces, sparse label
occurrences, and complex label dependencies. Moreover, concept drift affects
not only input distributions but also label correlations and imbalance ratios
over time, complicating model adaptation. To address these challenges,
structured learners are categorized into local and global methods. Local
methods break down the task into simpler components, while global methods adapt
the algorithm to the full output space, potentially yielding better predictions
by exploiting label correlations. This work introduces iHOMER (Incremental
Hierarchy Of Multi-label Classifiers), an online multi-label learning framework
that incrementally partitions the label space into disjoint, correlated
clusters without relying on predefined hierarchies. iHOMER leverages online
divisive-agglomerative clustering based on \textit{Jaccard} similarity and a
global tree-based learner driven by a multivariate \textit{Bernoulli} process
to guide instance partitioning. To address non-stationarity, it integrates
drift detection mechanisms at both global and local levels, enabling dynamic
restructuring of label partitions and subtrees. Experiments across 23
real-world datasets show iHOMER outperforms 5 state-of-the-art global
baselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\%, and 12 local
baselines, such as binary relevance transformations of kNN, EFDT, ARF, and
ADWIN bagging/boosting ensembles, by 32\%, establishing its robustness for
online multi-label classification.

</details>


### [144] [Modeling User Behavior from Adaptive Surveys with Supplemental Context](https://arxiv.org/abs/2507.20919)
*Aman Shukla,Daniel Patrick Scantlebury,Rishabh Kumar*

Main category: cs.LG

TL;DR: LANTERN是一种模块化架构，通过融合自适应调查响应和上下文信号来建模用户行为，优于仅依赖调查的基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法存在用户疲劳、不完整响应和长度限制等问题，无法全面捕捉用户行为，需要结合其他信号提升建模效果。

Method: LANTERN采用选择性门控、残差连接和跨注意力机制，以调查数据为主信号，仅在相关时融合外部模态。

Result: LANTERN在多标签预测任务中优于仅依赖调查的基线方法，并通过消融和属性分析验证了选择性模态依赖的优势。

Conclusion: LANTERN为以调查为中心的行为建模提供了实用且可扩展的解决方案。

Abstract: Modeling user behavior is critical across many industries where understanding
preferences, intent, or decisions informs personalization, targeting, and
strategic outcomes. Surveys have long served as a classical mechanism for
collecting such behavioral data due to their interpretability, structure, and
ease of deployment. However, surveys alone are inherently limited by user
fatigue, incomplete responses, and practical constraints on their length making
them insufficient for capturing user behavior. In this work, we present LANTERN
(Late-Attentive Network for Enriched Response Modeling), a modular architecture
for modeling user behavior by fusing adaptive survey responses with
supplemental contextual signals. We demonstrate the architectural value of
maintaining survey primacy through selective gating, residual connections and
late fusion via cross-attention, treating survey data as the primary signal
while incorporating external modalities only when relevant. LANTERN outperforms
strong survey-only baselines in multi-label prediction of survey responses. We
further investigate threshold sensitivity and the benefits of selective
modality reliance through ablation and rare/frequent attribute analysis.
LANTERN's modularity supports scalable integration of new encoders and evolving
datasets. This work provides a practical and extensible blueprint for behavior
modeling in survey-centric applications.

</details>


### [145] [Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein Interaction](https://arxiv.org/abs/2507.20925)
*Hongzhi Zhang,Zhonglie Liu,Kun Meng,Jiameng Chen,Jia Wu,Bo Du,Di Lin,Yan Che,Wenbin Hu*

Main category: cs.LG

TL;DR: 提出一种基于蛋白质子序列重排序的预训练方法，用于零样本化合物-蛋白质相互作用（CPI）预测，解决了现有方法在子序列依赖性和数据稀缺性上的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有CPI预测方法在子序列依赖性和数据需求上存在不足，难以应对零样本场景。

Method: 通过子序列重排序预训练蛋白质表示，并结合长度可变的数据增强技术。

Result: 在零样本场景下显著提升基线模型性能，尤其在数据稀缺时表现更优。

Conclusion: 该方法为CPI预测提供了一种高效且可扩展的解决方案，特别适用于数据稀缺情况。

Abstract: Given the vastness of chemical space and the ongoing emergence of previously
uncharacterized proteins, zero-shot compound-protein interaction (CPI)
prediction better reflects the practical challenges and requirements of
real-world drug development. Although existing methods perform adequately
during certain CPI tasks, they still face the following challenges: (1)
Representation learning from local or complete protein sequences often
overlooks the complex interdependencies between subsequences, which are
essential for predicting spatial structures and binding properties. (2)
Dependence on large-scale or scarce multimodal protein datasets demands
significant training data and computational resources, limiting scalability and
efficiency. To address these challenges, we propose a novel approach that
pretrains protein representations for CPI prediction tasks using subsequence
reordering, explicitly capturing the dependencies between protein subsequences.
Furthermore, we apply length-variable protein augmentation to ensure excellent
pretraining performance on small training datasets. To evaluate the model's
effectiveness and zero-shot learning ability, we combine it with various
baseline methods. The results demonstrate that our approach can improve the
baseline model's performance on the CPI task, especially in the challenging
zero-shot scenario. Compared to existing pre-training models, our model
demonstrates superior performance, particularly in data-scarce scenarios where
training samples are limited. Our implementation is available at
https://github.com/Hoch-Zhang/PSRP-CPI.

</details>


### [146] [Breaking the Precision Ceiling in Physics-Informed Neural Networks: A Hybrid Fourier-Neural Architecture for Ultra-High Accuracy](https://arxiv.org/abs/2507.20929)
*Wei Shan Lee,Chi Kiu Althina Chau,Kei Chon Sio,Kam Ian Leong*

Main category: cs.LG

TL;DR: 提出了一种混合傅里叶-神经网络架构，显著提升了PINNs在四阶偏微分方程中的精度，L2误差降至1.94×10^-7。


<details>
  <summary>Details</summary>
Motivation: 解决PINNs在四阶偏微分方程中精度不足的问题，突破10^-3-10^-4的误差瓶颈。

Method: 结合截断傅里叶级数和深度神经网络，通过两阶段优化策略（Adam和L-BFGS）和自适应权重平衡实现高精度。

Result: L2误差达到1.94×10^-7，比标准PINNs提升17倍，比传统数值方法提升15-500倍。

Conclusion: 通过合理设计，机器学习可以在科学计算中实现超高精度，甚至超越传统数值方法。

Abstract: Physics-informed neural networks (PINNs) have plateaued at errors of
$10^{-3}$-$10^{-4}$ for fourth-order partial differential equations, creating a
perceived precision ceiling that limits their adoption in engineering
applications. We break through this barrier with a hybrid Fourier-neural
architecture for the Euler-Bernoulli beam equation, achieving unprecedented L2
error of $1.94 \times 10^{-7}$-a 17-fold improvement over standard PINNs and
\(15-500\times\) better than traditional numerical methods. Our approach
synergistically combines a truncated Fourier series capturing dominant modal
behavior with a deep neural network providing adaptive residual corrections. A
systematic harmonic optimization study revealed a counter-intuitive discovery:
exactly 10 harmonics yield optimal performance, with accuracy catastrophically
degrading from $10^{-7}$ to $10^{-1}$ beyond this threshold. The two-phase
optimization strategy (Adam followed by L-BFGS) and adaptive weight balancing
enable stable ultra-precision convergence. GPU-accelerated implementation
achieves sub-30-minute training despite fourth-order derivative complexity. By
addressing 12 critical gaps in existing approaches-from architectural rigidity
to optimization landscapes-this work demonstrates that ultra-precision is
achievable through proper design, opening new paradigms for scientific
computing where machine learning can match or exceed traditional numerical
methods.

</details>


### [147] [Dissecting Persona-Driven Reasoning in Language Models via Activation Patching](https://arxiv.org/abs/2507.20936)
*Ansh Poonia,Maeghal Jain*

Main category: cs.LG

TL;DR: 研究探讨了大型语言模型（LLM）在赋予不同角色时如何影响其客观任务推理能力，发现早期MLP层处理语义内容，而中间MHA层利用这些信息生成输出。


<details>
  <summary>Details</summary>
Motivation: 探索角色分配对模型推理能力的影响，并理解模型如何编码角色信息。

Method: 使用激活修补技术分析模型的关键组件，研究MLP和MHA层的作用。

Result: 早期MLP层处理语义内容，中间MHA层生成输出；某些注意力头对种族和肤色身份关注度高。

Conclusion: 角色信息通过特定层处理并影响模型输出，揭示了模型内部处理角色信息的机制。

Abstract: Large language models (LLMs) exhibit remarkable versatility in adopting
diverse personas. In this study, we examine how assigning a persona influences
a model's reasoning on an objective task. Using activation patching, we take a
first step toward understanding how key components of the model encode
persona-specific information. Our findings reveal that the early Multi-Layer
Perceptron (MLP) layers attend not only to the syntactic structure of the input
but also process its semantic content. These layers transform persona tokens
into richer representations, which are then used by the middle Multi-Head
Attention (MHA) layers to shape the model's output. Additionally, we identify
specific attention heads that disproportionately attend to racial and
color-based identities.

</details>


### [148] [PySHRED: A Python package for SHallow REcurrent Decoding for sparse sensing, model reduction and scientific discovery](https://arxiv.org/abs/2507.20954)
*David Ye,Jan Williams,Mars Gao,Stefano Riva,Matteo Tomasetto,David Zoro,J. Nathan Kutz*

Main category: cs.LG

TL;DR: PySHRED是一个Python包，实现了SHallow REcurrent Decoders (SHRED)及其扩展，用于建模高维动态系统和时空数据，支持噪声、多尺度、参数化等复杂数据。


<details>
  <summary>Details</summary>
Motivation: 为处理现实世界中的复杂动态系统数据（如噪声、高维、非线性）提供高效工具。

Method: 通过SHRED及其扩展方法（如鲁棒感知、降阶建模和物理发现）建模数据，并提供数据预处理和模块化结构。

Result: 发布了PySHRED 1.0版本，包含多种先进方法，易于安装、文档完善，并支持未来扩展。

Conclusion: PySHRED是一个功能强大且灵活的Python包，适用于复杂动态系统数据的建模和分析。

Abstract: SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for
modeling high-dimensional dynamical systems and/or spatiotemporal data from
dynamical system snapshot observations. PySHRED is a Python package that
implements SHRED and several of its major extensions, including for robust
sensing, reduced order modeling and physics discovery. In this paper, we
introduce the version 1.0 release of PySHRED, which includes data preprocessors
and a number of cutting-edge SHRED methods specifically designed to handle
real-world data that may be noisy, multi-scale, parameterized, prohibitively
high-dimensional, and strongly nonlinear. The package is easy to install,
thoroughly-documented, supplemented with extensive code examples, and
modularly-structured to support future additions. The entire codebase is
released under the MIT license and is available at
https://github.com/pyshred-dev/pyshred.

</details>


### [149] [PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes](https://arxiv.org/abs/2507.20967)
*Tianhao Wang,Simon Klancher,Kunal Mukherjee,Josh Wiedemeier,Feng Chen,Murat Kantarcioglu,Kangkook Jee*

Main category: cs.LG

TL;DR: ProvCreator是一个用于复杂异构图的合成框架，利用序列生成和Transformer模型实现高效、逼真的图数据生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂异构图生成中的局限性，提升语义保真度和实用性。

Method: 采用图到序列的编码-解码框架，结合Transformer模型，支持无损编码和高效压缩。

Result: 在网络安全和知识图谱领域验证了ProvCreator的生成能力，能够捕捉结构和语义的复杂依赖关系。

Conclusion: ProvCreator为复杂异构图的合成提供了一种高效、可学习的解决方案，具有实际应用潜力。

Abstract: The rise of graph-structured data has driven interest in graph learning and
synthetic data generation. While successful in text and image domains,
synthetic graph generation remains challenging -- especially for real-world
graphs with complex, heterogeneous schemas. Existing research has focused
mostly on homogeneous structures with simple attributes, limiting their
usefulness and relevance for application domains requiring semantic fidelity.
  In this research, we introduce ProvCreator, a synthetic graph framework
designed for complex heterogeneous graphs with high-dimensional node and edge
attributes. ProvCreator formulates graph synthesis as a sequence generation
task, enabling the use of transformer-based large language models. It features
a versatile graph-to-sequence encoder-decoder that 1. losslessly encodes graph
structure and attributes, 2. efficiently compresses large graphs for contextual
modeling, and 3. supports end-to-end, learnable graph generation.
  To validate our research, we evaluate ProvCreator on two challenging domains:
system provenance graphs in cybersecurity and knowledge graphs from
IntelliGraph Benchmark Dataset. In both cases, ProvCreator captures intricate
dependencies between structure and semantics, enabling the generation of
realistic and privacy-aware synthetic datasets.

</details>


### [150] [From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation](https://arxiv.org/abs/2507.20968)
*Rongyao Cai,Ming Jin,Qingsong Wen,Kexin Zhang*

Main category: cs.LG

TL;DR: DARSD是一种新颖的无监督域自适应框架，通过表示空间分解实现域自适应，优于现有12种UDA算法。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列分析中域偏移问题，现有方法忽略特征内在组成，导致跨域性能下降。

Method: DARSD包含三个组件：对抗学习的公共不变基、原型伪标签机制和混合对比优化策略。

Result: 在四个基准数据集上，DARSD在53个跨域场景中的35个表现最优。

Conclusion: DARSD通过分解表示空间有效实现域自适应，显著提升跨域性能。

Abstract: Domain shift poses a fundamental challenge in time series analysis, where
models trained on source domain often fail dramatically when applied in target
domain with different yet similar distributions. While current unsupervised
domain adaptation (UDA) methods attempt to align cross-domain feature
distributions, they typically treat features as indivisible entities, ignoring
their intrinsic compositions that governs domain adaptation. We introduce
DARSD, a novel UDA framework with theoretical explainability that explicitly
realizes UDA tasks from the perspective of representation space decomposition.
Our core insight is that effective domain adaptation requires not just
alignment, but principled disentanglement of transferable knowledge from mixed
representations. DARSD consists three synergistic components: (I) An
adversarial learnable common invariant basis that projects original features
into a domain-invariant subspace while preserving semantic content; (II) A
prototypical pseudo-labeling mechanism that dynamically separates target
features based on confidence, hindering error accumulation; (III) A hybrid
contrastive optimization strategy that simultaneously enforces feature
clustering and consistency while mitigating emerging distribution gaps.
Comprehensive experiments conducted on four benchmark datasets (WISDM, HAR,
HHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms,
achieving optimal performance in 35 out of 53 cross-domain scenarios.

</details>


### [151] [Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder](https://arxiv.org/abs/2507.20973)
*Chao Wu,Zhenyi Wang,Kangxian Xie,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Mingchen Gao*

Main category: cs.LG

TL;DR: SAE Debias是一个轻量级、模型无关的框架，用于减少文本到图像（T2I）扩散模型中的性别偏见，通过稀疏自编码器识别和抑制性别相关方向。


<details>
  <summary>Details</summary>
Motivation: T2I扩散模型常表现出性别偏见，尤其是职业与性别之间的刻板关联。现有方法依赖CLIP过滤或提示工程，效果有限且需模型调整。

Method: 利用预训练的k稀疏自编码器在稀疏潜在空间中识别性别相关方向，并在推理时抑制这些方向以生成性别平衡的图像。

Result: SAE Debias在多个T2I模型（如Stable Diffusion系列）中显著减少性别偏见，同时保持生成质量。

Conclusion: SAE Debias是首个利用稀疏自编码器识别和干预T2I模型中性别偏见的工作，为构建公平的生成AI提供了可解释的工具。

Abstract: Text-to-image (T2I) diffusion models often exhibit gender bias, particularly
by generating stereotypical associations between professions and gendered
subjects. This paper presents SAE Debias, a lightweight and model-agnostic
framework for mitigating such bias in T2I generation. Unlike prior approaches
that rely on CLIP-based filtering or prompt engineering, which often require
model-specific adjustments and offer limited control, SAE Debias operates
directly within the feature space without retraining or architectural
modifications. By leveraging a k-sparse autoencoder pre-trained on a gender
bias dataset, the method identifies gender-relevant directions within the
sparse latent space, capturing professional stereotypes. Specifically, a biased
direction per profession is constructed from sparse latents and suppressed
during inference to steer generations toward more gender-balanced outputs.
Trained only once, the sparse autoencoder provides a reusable debiasing
direction, offering effective control and interpretable insight into biased
subspaces. Extensive evaluations across multiple T2I models, including Stable
Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially
reduces gender bias while preserving generation quality. To the best of our
knowledge, this is the first work to apply sparse autoencoders for identifying
and intervening in gender bias within T2I models. These findings contribute
toward building socially responsible generative AI, providing an interpretable
and model-agnostic tool to support fairness in text-to-image generation.

</details>


### [152] [SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment](https://arxiv.org/abs/2507.20984)
*Yixin Song,Zhenliang Xue,Dongliang Wei,Feiyang Chen,Jianxiang Gao,Junchen Liu,Hangyu Liang,Guangshuo Qin,Chengrong Tian,Bo Wen,Longyu Zhao,Xinrui Zheng,Zeyu Mi,Haibo Chen*

Main category: cs.LG

TL;DR: SmallThinker是一系列专为本地设备设计的轻量级大语言模型，通过创新的稀疏结构和预取技术，显著降低计算和存储需求，实现在普通CPU上的高效运行。


<details>
  <summary>Details</summary>
Motivation: 挑战当前大语言模型依赖GPU云基础设施的范式，为本地设备的计算能力、内存和存储限制提供原生解决方案。

Method: 采用两级稀疏结构（MoE与稀疏前馈网络）、预注意力路由器和NoPE-RoPE混合稀疏注意力机制，优化计算、存储和内存效率。

Result: SmallThinker-4B-A0.6B和SmallThinker-21B-A3B在性能上超越更大模型，并在普通CPU上实现每秒20个令牌的速度，内存占用极低。

Conclusion: SmallThinker证明了本地设备上高效运行大语言模型的可行性，减少了对昂贵GPU硬件的依赖。

Abstract: While frontier large language models (LLMs) continue to push capability
boundaries, their deployment remains confined to GPU-powered cloud
infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs
natively designed - not adapted - for the unique constraints of local devices:
weak computational power, limited memory, and slow storage. Unlike traditional
approaches that mainly compress existing models built for clouds, we architect
SmallThinker from the ground up to thrive within these limitations. Our
innovation lies in a deployment-aware architecture that transforms constraints
into design principles. First, We introduce a two-level sparse structure
combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward
networks, drastically reducing computational demands without sacrificing model
capacity. Second, to conquer the I/O bottleneck of slow storage, we design a
pre-attention router that enables our co-designed inference engine to prefetch
expert parameters from storage while computing attention, effectively hiding
storage latency that would otherwise cripple on-device inference. Third, for
memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to
slash KV cache requirements. We release SmallThinker-4B-A0.6B and
SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and
even outperform larger LLMs. Remarkably, our co-designed system mostly
eliminates the need for expensive GPU hardware: with Q4_0 quantization, both
models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB
and 8GB of memory respectively. SmallThinker is publicly available at
hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and
hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.

</details>


### [153] [Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition](https://arxiv.org/abs/2507.20997)
*Haris Khan,Shumaila Asif,Sadia Asif*

Main category: cs.LG

TL;DR: 提出了一种名为MDM-OC的新框架，用于实现可扩展、无干扰且可逆的模型合并，解决了传统方法中的任务干扰和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的机器学习部署中，模型需要不断更新、组合和选择性撤销，但现有方法存在任务干扰、灾难性遗忘或缺乏可逆性等问题。

Method: 通过将任务特定模型编码为共享基础的增量，并将其投影到正交子空间以避免冲突，再通过梯度优化合并为统一模型。

Result: 在视觉和自然语言处理基准测试中，MDM-OC在准确性、向后转移和撤销保真度方面优于现有基线，同时保持内存高效和计算可行。

Conclusion: MDM-OC为模块化和合规的AI系统设计提供了原则性解决方案。

Abstract: In real-world machine learning deployments, models must be continually
updated, composed, and when required, selectively undone. However, existing
approaches to model merging and continual learning often suffer from task
interference, catastrophic forgetting, or lack of reversibility. We propose
Modular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework
that enables scalable, interference-free, and reversible composition of
fine-tuned models. Each task-specific model is encoded as a delta from a shared
base and projected into an orthogonal subspace to eliminate conflict. These
projected deltas are then merged via gradient-based optimization to form a
unified model that retains performance across tasks. Our approach supports
continual integration of new models, structured unmerging for compliance such
as GDPR requirements, and model stability via elastic weight consolidation and
synthetic replay. Extensive experiments on vision and natural language
processing benchmarks demonstrate that MDM-OC outperforms prior baselines in
accuracy, backward transfer, and unmerge fidelity, while remaining
memory-efficient and computationally tractable. This framework offers a
principled solution for modular and compliant AI system design.

</details>


### [154] [LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning](https://arxiv.org/abs/2507.20999)
*Yining Huang,Bin Li,Keke Tang,Meilian Chen*

Main category: cs.LG

TL;DR: 提出了一种名为LoRA-PAR的双系统LoRA框架，通过分区数据和参数以适应快速直觉任务（System 1）和多步逻辑推理任务（System 2），减少参数使用同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法主要关注领域适应或分层分配，未针对不同任务需求定制数据和参数。受《思考，快与慢》启发，提出LLM参数可能类似地分为快速直觉和深度逻辑推理任务。

Method: 通过多模型角色扮演和投票分类任务数据，基于重要性评分分区参数，采用两阶段微调策略：SFT训练System 1任务增强直觉，RL优化System 2任务强化逻辑推理。

Result: 实验表明，两阶段微调策略（SFT和RL）在减少参数使用的同时，性能达到或超过现有PEFT基线。

Conclusion: LoRA-PAR框架通过分区数据和参数，实现了参数高效的同时提升任务性能，为LLM的微调提供了新思路。

Abstract: Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit
substantially from chain-of-thought (CoT) reasoning, yet pushing their
performance typically requires vast data, large model sizes, and full-parameter
fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,
most existing approaches primarily address domain adaptation or layer-wise
allocation rather than explicitly tailoring data and parameters to different
response demands. Inspired by "Thinking, Fast and Slow," which characterizes
two distinct modes of thought-System 1 (fast, intuitive, often automatic) and
System 2 (slower, more deliberative and analytic)-we draw an analogy that
different "subregions" of an LLM's parameters might similarly specialize for
tasks that demand quick, intuitive responses versus those requiring multi-step
logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework
that partitions both data and parameters by System 1 or System 2 demands, using
fewer yet more focused parameters for each task. Specifically, we classify task
data via multi-model role-playing and voting, and partition parameters based on
importance scoring, then adopt a two-stage fine-tuning strategy of training
System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and
intuition and refine System 2 tasks with reinforcement learning (RL) to
reinforce deeper logical deliberation next. Extensive experiments show that the
two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while
matching or surpassing SOTA PEFT baselines.

</details>


### [155] [Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability](https://arxiv.org/abs/2507.21004)
*Fang Li*

Main category: cs.LG

TL;DR: 论文提出了一种新型框架Compositional Function Networks（CFNs），通过组合具有明确语义的数学函数构建可解释模型，支持复杂特征交互且保持透明性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）性能优异但缺乏透明度，限制了在高风险领域的应用。CFNs旨在结合深度学习的表达能力和可解释性。

Method: CFNs通过组合基本数学函数（支持顺序、并行和条件模式）构建模型，完全可微分，支持标准梯度下降训练。

Result: CFNs在多个领域（如符号回归和图像分类）表现优异，CIFAR-10上准确率达96.24%，优于现有可解释模型。

Conclusion: CFNs为性能与可解释性并重的应用提供了强大框架。

Abstract: Deep Neural Networks (DNNs) deliver impressive performance but their
black-box nature limits deployment in high-stakes domains requiring
transparency. We introduce Compositional Function Networks (CFNs), a novel
framework that builds inherently interpretable models by composing elementary
mathematical functions with clear semantics. Unlike existing interpretable
approaches that are limited to simple additive structures, CFNs support diverse
compositional patterns -- sequential, parallel, and conditional -- enabling
complex feature interactions while maintaining transparency. A key innovation
is that CFNs are fully differentiable, allowing efficient training through
standard gradient descent. We demonstrate CFNs' versatility across multiple
domains, from symbolic regression to image classification with deep
hierarchical networks. Our empirical evaluation shows CFNs achieve competitive
performance against black-box models (96.24% accuracy on CIFAR-10) while
outperforming state-of-the-art interpretable models like Explainable Boosting
Machines. By combining the hierarchical expressiveness and efficient training
of deep learning with the intrinsic interpretability of well-defined
mathematical functions, CFNs offer a powerful framework for applications where
both performance and accountability are paramount.

</details>


### [156] [Predicting Cognition from fMRI:A Comparative Study of Graph, Transformer, and Kernel Models Across Task and Rest Conditions](https://arxiv.org/abs/2507.21016)
*Jagruti Patel,Mikkel Schöttner,Thomas A. W. Bolton,Patric Hagmann*

Main category: cs.LG

TL;DR: 该研究比较了经典机器学习（KRR）和深度学习（GNN、TGNN）在预测认知能力时的表现，发现任务型fMRI优于静息态fMRI，GNN结合SC和FC表现最佳，但TGNN在任务型fMRI中表现接近FC方法。


<details>
  <summary>Details</summary>
Motivation: 通过神经影像数据预测认知能力，有助于理解认知的神经机制，并应用于精准医疗和早期神经精神疾病检测。

Method: 使用HCP年轻成人数据集，比较KRR、GNN和TGNN在静息态、工作记忆和语言任务fMRI数据上的表现。

Result: 任务型fMRI优于静息态fMRI；GNN结合SC和FC表现最佳，但优势不显著；TGNN在任务型fMRI中表现接近FC方法，但在静息态数据中表现较差。

Conclusion: 选择合适的模型架构和特征表示对充分利用神经影像数据的时空特性至关重要，多模态图感知DL模型和Transformer方法在认知预测中具有潜力。

Abstract: Predicting cognition from neuroimaging data in healthy individuals offers
insights into the neural mechanisms underlying cognitive abilities, with
potential applications in precision medicine and early detection of
neurological and psychiatric conditions. This study systematically benchmarked
classical machine learning (Kernel Ridge Regression (KRR)) and advanced deep
learning (DL) models (Graph Neural Networks (GNN) and Transformer-GNN (TGNN))
for cognitive prediction using Resting-state (RS), Working Memory, and Language
task fMRI data from the Human Connectome Project Young Adult dataset.
  Our results, based on R2 scores, Pearson correlation coefficient, and mean
absolute error, revealed that task-based fMRI, eliciting neural responses
directly tied to cognition, outperformed RS fMRI in predicting cognitive
behavior. Among the methods compared, a GNN combining structural connectivity
(SC) and functional connectivity (FC) consistently achieved the highest
performance across all fMRI modalities; however, its advantage over KRR using
FC alone was not statistically significant. The TGNN, designed to model
temporal dynamics with SC as a prior, performed competitively with FC-based
approaches for task-fMRI but struggled with RS data, where its performance
aligned with the lower-performing GNN that directly used fMRI time-series data
as node features. These findings emphasize the importance of selecting
appropriate model architectures and feature representations to fully leverage
the spatial and temporal richness of neuroimaging data.
  This study highlights the potential of multimodal graph-aware DL models to
combine SC and FC for cognitive prediction, as well as the promise of
Transformer-based approaches for capturing temporal dynamics. By providing a
comprehensive comparison of models, this work serves as a guide for advancing
brain-behavior modeling using fMRI, SC and DL.

</details>


### [157] [Behavior-Specific Filtering for Enhanced Pig Behavior Classification in Precision Livestock Farming](https://arxiv.org/abs/2507.21021)
*Zhen Zhang,Dong Sam Ha,Gota Morota,Sook Shin*

Main category: cs.LG

TL;DR: 提出了一种针对特定行为的过滤方法，显著提高了精准畜牧业中行为分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统过滤方法对所有行为采用统一处理，限制了分类准确性。

Method: 结合小波去噪和低通滤波器，针对活跃和非活跃猪行为进行定制化处理。

Result: 峰值准确率达到94.73%，优于传统方法的91.58%。

Conclusion: 行为特定过滤方法有效提升了动物行为监测，有助于健康管理和农场效率。

Abstract: This study proposes a behavior-specific filtering method to improve behavior
classification accuracy in Precision Livestock Farming. While traditional
filtering methods, such as wavelet denoising, achieved an accuracy of 91.58%,
they apply uniform processing to all behaviors. In contrast, the proposed
behavior-specific filtering method combines Wavelet Denoising with a Low Pass
Filter, tailored to active and inactive pig behaviors, and achieved a peak
accuracy of 94.73%. These results highlight the effectiveness of
behavior-specific filtering in enhancing animal behavior monitoring, supporting
better health management and farm efficiency.

</details>


### [158] [Optimization Performance of Factorization Machine with Annealing under Limited Training Data](https://arxiv.org/abs/2507.21024)
*Mayumi Nakano,Yuya Seki,Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: 提出了一种改进的FMA方法，通过限制数据集大小来增强新数据点对模型的影响，从而提升优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统FMA在优化过程中性能停滞，原因是数据集增大导致新数据点的影响被稀释。

Method: 采用序列化数据集构建方法，仅保留最近添加的指定数量数据点。

Result: 数值实验表明，改进的FMA能以更少的函数评估获得更低成本的解。

Conclusion: 限制数据集大小能有效提升FMA的优化性能。

Abstract: Black-box (BB) optimization problems aim to identify an input that minimizes
the output of a function (the BB function) whose input-output relationship is
unknown. Factorization machine with annealing (FMA) is a promising approach to
this task, employing a factorization machine (FM) as a surrogate model to
iteratively guide the solution search via an Ising machine. Although FMA has
demonstrated strong optimization performance across various applications, its
performance often stagnates as the number of optimization iterations increases.
One contributing factor to this stagnation is the growing number of data points
in the dataset used to train FM. It is hypothesized that as more data points
are accumulated, the contribution of newly added data points becomes diluted
within the entire dataset, thereby reducing their impact on improving the
prediction accuracy of FM. To address this issue, we propose a novel method for
sequential dataset construction that retains at most a specified number of the
most recently added data points. This strategy is designed to enhance the
influence of newly added data points on the surrogate model. Numerical
experiments demonstrate that the proposed FMA achieves lower-cost solutions
with fewer BB function evaluations compared to the conventional FMA.

</details>


### [159] [When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New Framework for Cross-Subject Motor Imagery Decoding](https://arxiv.org/abs/2507.21037)
*Jinzhou Wu,Baoping Tang,Qikang Li,Yi Wang,Cheng Li,Shujian Yu*

Main category: cs.LG

TL;DR: 提出了一种基于预训练大脑基础模型（BFM）的多源域适应框架，用于动态选择相关源域，并结合特征级和决策级对齐以提升MI-EEG解码性能。


<details>
  <summary>Details</summary>
Motivation: 解决MI-EEG解码中因被试间差异大和标记数据有限导致的负迁移和计算成本高的问题。

Method: 利用BFM动态选择相关源域，使用Cauchy-Schwarz和Conditional CS散度进行特征级和决策级对齐。

Result: 在两个基准MI-EEG数据集上优于现有方法，BFM选择显著减少训练时间且不牺牲性能。

Conclusion: 提出的框架有效解决了MI-EEG解码中的挑战，具有高效和可扩展性。

Abstract: Decoding motor imagery (MI) electroencephalogram (EEG) signals, a key
non-invasive brain-computer interface (BCI) paradigm for controlling external
systems, has been significantly advanced by deep learning. However, MI-EEG
decoding remains challenging due to substantial inter-subject variability and
limited labeled target data, which necessitate costly calibration for new
users. Many existing multi-source domain adaptation (MSDA) methods
indiscriminately incorporate all available source domains, disregarding the
large inter-subject differences in EEG signals, which leads to negative
transfer and excessive computational costs. Moreover, while many approaches
focus on feature distribution alignment, they often neglect the explicit
dependence between features and decision-level outputs, limiting their ability
to preserve discriminative structures. To address these gaps, we propose a
novel MSDA framework that leverages a pretrained large Brain Foundation Model
(BFM) for dynamic and informed source subject selection, ensuring only relevant
sources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS)
and Conditional CS (CCS) divergences to jointly perform feature-level and
decision-level alignment, enhancing domain invariance while maintaining class
discriminability. Extensive evaluations on two benchmark MI-EEG datasets
demonstrate that our framework outperforms a broad range of state-of-the-art
baselines. Additional experiments with a large source pool validate the
scalability and efficiency of BFM-guided selection, which significantly reduces
training time without sacrificing performance.

</details>


### [160] [Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning](https://arxiv.org/abs/2507.21049)
*Zedong Wang,Siyuan Li,Dan Xu*

Main category: cs.LG

TL;DR: Rep-MTL提出了一种新的多任务学习方法，通过利用表示空间中的任务显著性来量化任务间的交互，从而促进互补信息共享，而非仅解决冲突。


<details>
  <summary>Details</summary>
Motivation: 现有多任务优化方法主要关注通过优化器为中心的损失缩放和梯度操作解决任务冲突，但未能实现一致的性能提升。本文认为共享表示空间提供了丰富的交互信息，可用于促进任务间的互补性。

Method: Rep-MTL利用表示级任务显著性量化任务间交互，通过基于熵的惩罚和样本级跨任务对齐来引导这些显著性，以减轻负迁移并促进互补信息共享。

Result: 在四个多任务学习基准测试中，Rep-MTL即使与基本等权重策略结合，也能实现竞争性性能提升，并在平衡任务特定学习和跨任务共享方面表现出色。

Conclusion: Rep-MTL通过表示级任务显著性操作，有效促进了多任务学习中的互补信息共享，为多任务优化提供了新思路。

Abstract: Despite the promise of Multi-Task Learning in leveraging complementary
knowledge across tasks, existing multi-task optimization (MTO) techniques
remain fixated on resolving conflicts via optimizer-centric loss scaling and
gradient manipulation strategies, yet fail to deliver consistent gains. In this
paper, we argue that the shared representation space, where task interactions
naturally occur, offers rich information and potential for operations
complementary to existing optimizers, especially for facilitating the
inter-task complementarity, which is rarely explored in MTO. This intuition
leads to Rep-MTL, which exploits the representation-level task saliency to
quantify interactions between task-specific optimization and shared
representation learning. By steering these saliencies through entropy-based
penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate
negative transfer by maintaining the effective training of individual tasks
instead pure conflict-solving, while explicitly promoting complementary
information sharing. Experiments are conducted on four challenging MTL
benchmarks covering both task-shift and domain-shift scenarios. The results
show that Rep-MTL, even paired with the basic equal weighting policy, achieves
competitive performance gains with favorable efficiency. Beyond standard
performance metrics, Power Law exponent analysis demonstrates Rep-MTL's
efficacy in balancing task-specific learning and cross-task sharing. The
project page is available at HERE.

</details>


### [161] [Flow Matching Policy Gradients](https://arxiv.org/abs/2507.21053)
*David McAllister,Songwei Ge,Brent Yi,Chung Min Kim,Ethan Weber,Hongsuk Choi,Haiwen Feng,Angjoo Kanazawa*

Main category: cs.LG

TL;DR: FPO是一种将流匹配引入策略梯度框架的强化学习算法，兼容PPO-clip框架，无需精确似然计算，适用于连续控制任务。


<details>
  <summary>Details</summary>
Motivation: 将流匹配的优势引入强化学习策略优化，解决高斯策略在复杂任务中的局限性。

Method: 通过优势加权比计算条件流匹配损失，结合PPO-clip框架，实现策略优化。

Result: FPO在连续控制任务中表现优于高斯策略，尤其在多模态动作分布和欠条件设置下。

Conclusion: FPO展示了流模型在强化学习中的潜力，为策略优化提供了新思路。

Abstract: Flow-based generative models, including diffusion models, excel at modeling
continuous distributions in high-dimensional spaces. In this work, we introduce
Flow Policy Optimization (FPO), a simple on-policy reinforcement learning
algorithm that brings flow matching into the policy gradient framework. FPO
casts policy optimization as maximizing an advantage-weighted ratio computed
from the conditional flow matching loss, in a manner compatible with the
popular PPO-clip framework. It sidesteps the need for exact likelihood
computation while preserving the generative capabilities of flow-based models.
Unlike prior approaches for diffusion-based reinforcement learning that bind
training to a specific sampling method, FPO is agnostic to the choice of
diffusion or flow integration at both training and inference time. We show that
FPO can train diffusion-style policies from scratch in a variety of continuous
control tasks. We find that flow-based models can capture multimodal action
distributions and achieve higher performance than Gaussian policies,
particularly in under-conditioned settings.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [162] [Bayesian symbolic regression: Automated equation discovery from a physicists' perspective](https://arxiv.org/abs/2507.19540)
*Roger Guimera,Marta Sales-Pardo*

Main category: stat.ML

TL;DR: 本文介绍了符号回归的概率方法，替代了传统的启发式方法，并展示了其在模型可信度和性能保证方面的优势。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法依赖启发式选择和探索，缺乏理论基础和性能保证。

Method: 采用概率方法，结合信息论和统计物理学的理论，通过基本考量和显式近似建立模型可信度。

Result: 概率方法提供了启发式方法所缺乏的性能保证，并促使考虑模型集合而非单一模型。

Conclusion: 概率方法为符号回归提供了更可靠的理论基础和实践优势。

Abstract: Symbolic regression automates the process of learning closed-form
mathematical models from data. Standard approaches to symbolic regression, as
well as newer deep learning approaches, rely on heuristic model selection
criteria, heuristic regularization, and heuristic exploration of model space.
Here, we discuss the probabilistic approach to symbolic regression, an
alternative to such heuristic approaches with direct connections to information
theory and statistical physics. We show how the probabilistic approach
establishes model plausibility from basic considerations and explicit
approximations, and how it provides guarantees of performance that heuristic
approaches lack. We also discuss how the probabilistic approach compels us to
consider model ensembles, as opposed to single models.

</details>


### [163] [Adaptive Bayesian Data-Driven Design of Reliable Solder Joints for Micro-electronic Devices](https://arxiv.org/abs/2507.19663)
*Leo Guo,Adwait Inamdar,Willem D. van Driel,GuoQi Zhang*

Main category: stat.ML

TL;DR: 论文提出了一种基于贝叶斯优化（BO）的自适应启发式框架，用于解决焊点可靠性问题，通过多采集函数和自适应超参数提升计算效率，结果显示比传统BO节省50%计算成本且性能提升3%。


<details>
  <summary>Details</summary>
Motivation: 焊点可靠性问题计算复杂且昂贵，传统方法效率低，需数据驱动的高效优化方案。

Method: 提出自适应BO框架，结合高斯过程回归和多采集函数，动态调整超参数。

Result: 自适应BO在合成问题和实际焊点问题中均优于传统BO，节省50%计算成本且性能提升3%。

Conclusion: 自适应BO方法在计算效率和性能上优于传统方法，开源实现促进结果可复现。

Abstract: Solder joint reliability related to failures due to thermomechanical loading
is a critically important yet physically complex engineering problem. As a
result, simulated behavior is oftentimes computationally expensive. In an
increasingly data-driven world, the usage of efficient data-driven design
schemes is a popular choice. Among them, Bayesian optimization (BO) with
Gaussian process regression is one of the most important representatives. The
authors argue that computational savings can be obtained from exploiting
thorough surrogate modeling and selecting a design candidate based on multiple
acquisition functions. This is feasible due to the relatively low computational
cost, compared to the expensive simulation objective. This paper addresses the
shortcomings in the adjacent literature by providing and implementing a novel
heuristic framework to perform BO with adaptive hyperparameters across the
various optimization iterations. Adaptive BO is subsequently compared to
regular BO when faced with synthetic objective minimization problems. The
results show the efficiency of adaptive BO when compared any worst-performing
regular Bayesian schemes. As an engineering use case, the solder joint
reliability problem is tackled by minimizing the accumulated non-linear creep
strain under a cyclic thermal load. Results show that adaptive BO outperforms
regular BO by 3% on average at any given computational budget threshold,
critically saving half of the computational expense budget. This practical
result underlines the methodological potential of the adaptive Bayesian
data-driven methodology to achieve better results and cut optimization-related
expenses. Lastly, in order to promote the reproducibility of the results, the
data-driven implementations are made available on an open-source basis.

</details>


### [164] [Bag of Coins: A Statistical Probe into Neural Confidence Structures](https://arxiv.org/abs/2507.19774)
*Agnideep Aich,Ashit Baran Aich,Md Monzur Murshed,Sameera Hewage,Bruce Wade*

Main category: stat.ML

TL;DR: 论文提出了一种名为Bag-of-Coins (BoC)的非参数统计测试，用于评估分类器内部预测一致性，发现ViTs的置信度校准效果显著优于CNNs。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络的高精度与其置信度校准不足形成矛盾，限制了其在高风险应用中的可靠性。现有方法未深入探究预测的内部一致性。

Method: 提出BoC测试，将置信度估计转化为频率假设检验，检查模型预测与其softmax概率的一致性。

Result: ViTs的BoC输出达到近乎完美的校准（ECE为0.0212），而CNNs则显示出预测与内部结构的不一致性。

Conclusion: BoC不仅是校准工具，还能揭示不同架构在不确定性表示上的差异。

Abstract: Modern neural networks, despite their high accuracy, often produce poorly
calibrated confidence scores, limiting their reliability in high-stakes
applications. Existing calibration methods typically post-process model outputs
without interrogating the internal consistency of the predictions themselves.
In this work, we introduce a novel, non-parametric statistical probe, the
Bag-of-Coins (BoC) test, that examines the internal consistency of a
classifier's logits. The BoC test reframes confidence estimation as a
frequentist hypothesis test: does the model's top-ranked class win 1-v-1
contests against random competitors at a rate consistent with its own stated
softmax probability? When applied to modern deep learning architectures, this
simple probe reveals a fundamental dichotomy. On Vision Transformers (ViTs),
the BoC output serves as a state-of-the-art confidence score, achieving
near-perfect calibration with an ECE of 0.0212, an 88% improvement over a
temperature-scaled baseline. Conversely, on Convolutional Neural Networks
(CNNs) like ResNet, the probe reveals a deep inconsistency between the model's
predictions and its internal logit structure, a property missed by traditional
metrics. We posit that BoC is not merely a calibration method, but a new
diagnostic tool for understanding and exposing the differing ways that popular
architectures represent uncertainty.

</details>


### [165] [Sparse-mode Dynamic Mode Decomposition for Disambiguating Local and Global Structures](https://arxiv.org/abs/2507.19787)
*Sara M. Ichinaga,Steven L. Brunton,Aleksandr Y. Aravkin,J. Nathan Kutz*

Main category: stat.ML

TL;DR: 提出了一种稀疏模式动态模态分解（sparse-mode DMD）方法，利用稀疏正则化提取局部化空间结构的模态，同时保持噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统DMD方法难以区分局部和全局模态，稀疏模式DMD旨在解决这一问题，并明确构建频谱的不同部分。

Method: 通过稀疏正则化优化DMD框架，提取局部化空间结构的模态，同时保持噪声鲁棒性。

Result: 在合成和实际系统（如光波导、量子力学和海面温度数据）中验证了方法的有效性。

Conclusion: 稀疏模式DMD能够无监督地明确区分局部和全局模态，适用于多种应用场景。

Abstract: The dynamic mode decomposition (DMD) is a data-driven approach that extracts
the dominant features from spatiotemporal data. In this work, we introduce
sparse-mode DMD, a new variant of the optimized DMD framework that specifically
leverages sparsity-promoting regularization in order to approximate DMD modes
which have localized spatial structure. The algorithm maintains the
noise-robust properties of optimized DMD while disambiguating between modes
which are spatially local versus global in nature. In many applications, such
modes are associated with discrete and continuous spectra respectively, thus
allowing the algorithm to explicitly construct, in an unsupervised manner, the
distinct portions of the spectrum. We demonstrate this by analyzing synthetic
and real-world systems, including examples from optical waveguides, quantum
mechanics, and sea surface temperature data.

</details>


### [166] [Predicting Parkinson's Disease Progression Using Statistical and Neural Mixed Effects Models: A Comparative Study on Longitudinal Biomarkers](https://arxiv.org/abs/2507.20058)
*Ran Tong,Lanruo Wang,Tong Wang,Wei Yan*

Main category: stat.ML

TL;DR: 研究比较了线性混合模型（LMM）与两种先进混合方法（GNMM和NME）在预测帕金森病进展中的表现。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的进展预测至关重要，声音生物标志物提供了一种非侵入性方法，但纵向数据分析存在挑战。

Method: 使用牛津帕金森病远程监测声音数据集，评估LMM、GNMM和NME模型在预测UPDRS总分中的表现。

Result: 研究为帕金森病研究和临床应用提供了实用指导。

Conclusion: GNMM和NME模型在预测帕金森病进展方面表现出潜力。

Abstract: Predicting Parkinson's Disease (PD) progression is crucial, and voice
biomarkers offer a non-invasive method for tracking symptom severity (UPDRS
scores) through telemonitoring. Analyzing this longitudinal data is challenging
due to within-subject correlations and complex, nonlinear patient-specific
progression patterns. This study benchmarks LMMs against two advanced hybrid
approaches: the Generalized Neural Network Mixed Model (GNMM) (Mandel 2021),
which embeds a neural network within a GLMM structure, and the Neural Mixed
Effects (NME) model (Wortwein 2023), allowing nonlinear subject-specific
parameters throughout the network. Using the Oxford Parkinson's telemonitoring
voice dataset, we evaluate these models' performance in predicting Total UPDRS
to offer practical guidance for PD research and clinical applications.

</details>


### [167] [Statistical Inference for Differentially Private Stochastic Gradient Descent](https://arxiv.org/abs/2507.20560)
*Xintao Xia,Linjun Zhang,Zhanrui Cai*

Main category: stat.ML

TL;DR: 本文填补了随机子采样下SGD的渐近性质与DP-SGD之间的空白，提出了两种构建有效置信区间的方法，并通过数值分析验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 现有统计推断方法主要针对循环子采样，而DP-SGD需要随机子采样，因此需要填补这一空白。

Method: 建立了随机规则下SGD的渐近性质，并将其扩展到DP-SGD，提出了插件法和随机缩放法构建置信区间。

Result: DP-SGD输出的渐近方差分解为统计、采样和隐私诱导三部分，数值分析显示置信区间达到名义覆盖率并保持隐私。

Conclusion: 本文为随机子采样下的DP-SGD提供了理论支持和方法工具，验证了其实际可行性。

Abstract: Privacy preservation in machine learning, particularly through Differentially
Private Stochastic Gradient Descent (DP-SGD), is critical for sensitive data
analysis. However, existing statistical inference methods for SGD predominantly
focus on cyclic subsampling, while DP-SGD requires randomized subsampling. This
paper first bridges this gap by establishing the asymptotic properties of SGD
under the randomized rule and extending these results to DP-SGD. For the output
of DP-SGD, we show that the asymptotic variance decomposes into statistical,
sampling, and privacy-induced components. Two methods are proposed for
constructing valid confidence intervals: the plug-in method and the random
scaling method. We also perform extensive numerical analysis, which shows that
the proposed confidence intervals achieve nominal coverage rates while
maintaining privacy.

</details>


### [168] [Multivariate Conformal Prediction via Conformalized Gaussian Scoring](https://arxiv.org/abs/2507.20941)
*Sacha Braun,Eugène Berta,Michael I. Jordan,Francis Bach*

Main category: stat.ML

TL;DR: 论文提出了一种基于高斯分数的共形预测方法，避免了传统CDF方法的计算成本，并通过扩展实现了更灵活的共形集构建。


<details>
  <summary>Details</summary>
Motivation: 共形预测中精确的条件覆盖难以实现，现有方法计算成本高，需要更高效的近似方法。

Method: 利用高斯分数的马氏距离替代CDF方法，实现闭式表达，并扩展了共形预测的应用场景。

Result: 实证结果表明，该方法在多变量设置下更接近条件覆盖，且计算效率更高。

Conclusion: 高斯分数方法为共形预测提供了更高效且灵活的解决方案。

Abstract: While achieving exact conditional coverage in conformal prediction is
unattainable without making strong, untestable regularity assumptions, the
promise of conformal prediction hinges on finding approximations to conditional
guarantees that are realizable in practice. A promising direction for obtaining
conditional dependence for conformal sets--in particular capturing
heteroskedasticity--is through estimating the conditional density
$\mathbb{P}_{Y|X}$ and conformalizing its level sets. Previous work in this
vein has focused on nonconformity scores based on the empirical cumulative
distribution function (CDF). Such scores are, however, computationally costly,
typically requiring expensive sampling methods. To avoid the need for sampling,
we observe that the CDF-based score reduces to a Mahalanobis distance in the
case of Gaussian scores, yielding a closed-form expression that can be directly
conformalized. Moreover, the use of a Gaussian-based score opens the door to a
number of extensions of the basic conformal method; in particular, we show how
to construct conformal sets with missing output values, refine conformal sets
as partial information about $Y$ becomes available, and construct conformal
sets on transformations of the output space. Finally, empirical results
indicate that our approach produces conformal sets that more closely
approximate conditional coverage in multivariate settings compared to
alternative methods.

</details>


### [169] [Locally Adaptive Conformal Inference for Operator Models](https://arxiv.org/abs/2507.20975)
*Trevor Harris,Yan Liu*

Main category: stat.ML

TL;DR: LSCI框架为神经算子模型提供局部自适应、无分布的不确定性量化，显著提升适应性和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经算子模型缺乏预测不确定性的固有概念，需要一种新的不确定性量化方法。

Method: 提出LSCI框架，结合投影深度评分和局部共形推断，生成具有统计保证的函数值预测集。

Result: 在合成和实际算子学习任务中，LSCI显著提升了适应性和覆盖范围。

Conclusion: LSCI为神经算子模型提供了一种有效的局部不确定性量化方法，具有统计保证。

Abstract: Operator models are regression algorithms for functional data and have become
a key tool for emulating large-scale dynamical systems. Recent advances in deep
neural operators have dramatically improved the accuracy and scalability of
operator modeling, but lack an inherent notion of predictive uncertainty. We
introduce Local Spectral Conformal Inference (LSCI), a new framework for
locally adaptive, distribution-free uncertainty quantification for neural
operator models. LSCI uses projection-based depth scoring and localized
conformal inference to generate function-valued prediction sets with
statistical guarantees. We prove approximate finite-sample marginal coverage
under local exchangeability, and demonstrate significant gains in adaptivity
and coverage across synthetic and real-world operator learning tasks.

</details>
