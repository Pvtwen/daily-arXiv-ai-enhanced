<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 18]
- [cs.LG](#cs.LG) [Total: 67]
- [stat.ML](#stat.ML) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Design of a Noval Wearable ECG Monitoring Device](https://arxiv.org/abs/2507.17154)
*Ruihua Wang,Mingtong Chen,Zhengbao Yang*

Main category: eess.SP

TL;DR: 本研究开发了一种新型无线供电可穿戴心电监测设备，采用银镀织物一体化电极设计，解决传统设备功耗高和长期佩戴不适的问题。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴智能心电解决方案存在高功耗问题，传统Ag/AgCl凝胶电极不适合长期动态监测，容易引起皮肤刺激、炎症和过敏反应，因此需要开发新的无线、小尺寸、可长期佩戴的心电监测设备。

Method: 采用全新的电极和导线组合方式，从银镀织物中一体切割出电极和导线，导线部分切割成接近S形的弯曲结构，确保在日常服装运动中具有良好的延展性，保证舒适性和信号完整性。同时通过优化算法和降低噪声功率来改善信噪比。

Result: 设计出了采用银镀织物一体化电极的无线供电可穿戴心电监测设备，解决了传统电极的皮肤刺激问题，提高了长期佩戴的舒适性和信号质量。

Conclusion: 通过采用银镀织物一体化电极设计和S形弯曲导线结构，成功开发了适合长期佩戴的无线心电监测设备，为心电监测技术提供了新的解决方案，改善了用户体验和监测准确性。

Abstract: The aim of this project is to develop a new wireless powered wearable ECG
monitoring device. The main goal of the project is to provide a wireless,
small-sized ECG monitoring device that can be worn for a long period of time by
the monitored person. Electrocardiogram ECG reflects physiological and
pathological information about heart activity and is commonly used to diagnose
heart disease. Existing wearable smart ECG solutions suffer from high power
consumption in both ECG diagnosis and transmission for high accuracy.
Monitoring of ECG devices is mainly done by data extraction and acquisition,
pre-processing, feature extraction, processing and analysis, visualisation and
auxiliary procedures. During the pre-processing of the information, different
kinds of noise generated during the signal collection need to be taken into
account. The quality of the signal-to-noise ratio can usually be improved by
optimising algorithms and reducing the noise power. The choice of electrodes
usually has a direct impact on the signal-to-noise ratio and the user
experience, and conventional Ag/AgCl gel electrodes are not suitable for
long-term and dynamic monitoring as they are prone to skin irritation,
inflammation and allergic reactions. Therefore, a completely new way of
combining electrodes and wires will be used in the report. The electrodes and
wires are cut in one piece from a silver-plated fabric. The wire portion is cut
into a curved structure close to an S shape to ensure that it has good
ductility for comfort and signal integrity during daily movement of the
garment.

</details>


### [2] [PPAAS: PVT and Pareto Aware Analog Sizing via Goal-conditioned Reinforcement Learning](https://arxiv.org/abs/2507.17003)
*Seunggeun Kim,Ziyi Wang,Sungyoung Lee,Youngmin Oh,Hanqing Zhu,Doyun Kim,David Z. Pan*

Main category: eess.SP

TL;DR: 本文提出了一个基于目标条件强化学习的框架，用于在PVT变化下高效自动化模拟器件尺寸设计，通过帕累托前沿主导目标采样和保守后见经验回放等技术显著提升了样本效率和仿真效率。


<details>
  <summary>Details</summary>
Motivation: 模拟和混合信号电路的器件尺寸设计是一个关键但具有挑战性的步骤，需要在工艺、电压、温度(PVT)变化下满足多样化的性能规范。现有强化学习方法虽然在固定目标的自动化尺寸设计上显示出潜力，但训练一个能够适应PVT变化下广泛设计规范的通用策略需要大量的训练样本和资源。

Method: 提出了一个目标条件强化学习框架，包含三个关键技术：1) 帕累托前沿主导目标采样(Pareto-front Dominance Goal Sampling)，通过从先前实现目标的帕累托前沿采样目标来构建自动课程；2) 保守后见经验回放(Conservative Hindsight Experience Replay)，通过为重新标记的目标分配保守虚拟奖励来稳定训练并加速收敛；3) 失败跳过仿真策略(Skip-on-Fail simulation strategy)，当标称角仿真未能满足目标规范时跳过全角仿真以减少仿真开销。

Result: 在基准电路上的实验表明，与现有尺寸设计方法相比，该框架在样本效率上提升了约1.6倍，在仿真效率上提升了约4.1倍，同时具有强大的泛化能力。

Conclusion: 所提出的目标条件强化学习框架成功解决了PVT变化下模拟器件尺寸设计的挑战，通过创新的目标采样策略和经验回放机制显著提升了训练效率，为模拟电路自动化设计提供了一个高效且可泛化的解决方案。

Abstract: Device sizing is a critical yet challenging step in analog and mixed-signal
circuit design, requiring careful optimization to meet diverse performance
specifications. This challenge is further amplified under process, voltage, and
temperature (PVT) variations, which cause circuit behavior to shift across
different corners. While reinforcement learning (RL) has shown promise in
automating sizing for fixed targets, training a generalized policy that can
adapt to a wide range of design specifications under PVT variations requires
much more training samples and resources. To address these challenges, we
propose a \textbf{Goal-conditioned RL framework} that enables efficient policy
training for analog device sizing across PVT corners, with strong
generalization capability. To improve sample efficiency, we introduce
Pareto-front Dominance Goal Sampling, which constructs an automatic curriculum
by sampling goals from the Pareto frontier of previously achieved goals. This
strategy is further enhanced by integrating Conservative Hindsight Experience
Replay, which assigns relabeled goals with conservative virtual rewards to
stabilize training and accelerate convergence. To reduce simulation overhead,
our framework incorporates a Skip-on-Fail simulation strategy, which skips
full-corner simulations when nominal-corner simulation fails to meet target
specifications. Experiments on benchmark circuits demonstrate $\sim$1.6$\times$
improvement in sample efficiency and $\sim$4.1$\times$ improvement in
simulation efficiency compared to existing sizing methods. Code and benchmarks
are publicly available at https://github.com/SeunggeunKimkr/PPAAS

</details>


### [3] [Efficient and Distortion-less Spectrum Multiplexer via Neural Network-based Filter Banks](https://arxiv.org/abs/2507.17106)
*Jiazhao Wang,Wenchao Jiang*

Main category: eess.SP

TL;DR: 本文提出了基于神经网络的滤波器组频谱复用器，用于物联网信号的同时传输，相比传统方法实现了更高效率和更低失真


<details>
  <summary>Details</summary>
Motivation: 传统的频谱复用器在处理多个窄带物联网信号时存在效率低和失真大的问题，需要一种新的解决方案来提高频谱利用率并降低信号失真

Method: 采用模型驱动方法，将神经网络模型解释为滤波器组，设计基于神经网络的滤波器组频谱复用器，利用神经网络的学习能力实现无失真复用，并通过硬件加速提高效率

Result: 实现了-39dB的归一化均方误差低失真水平，相比传统方法获得了35倍的执行效率提升和10dB的信噪比增益，在实际应用中对异构和同构物联网网络的数据包接收率达到98%

Conclusion: 基于神经网络的滤波器组频谱复用器能够有效处理多个物联网信号的同时传输，显著提高了频谱利用效率，降低了信号失真，在实际物联网环境中表现出色

Abstract: Spectrum multiplexer enables simultaneous transmission of multiple
narrow-band IoT signals through gateway devices, thereby enhancing overall
spectrum utilization. We propose a novel solution based on filter banks that
offer increased efficiency and minimal distortion compared with conventional
methods. We follow a model-driven approach to integrate the neural networks
into the filter bank design by interpreting the neural network models as filter
banks. The proposed NN-based filter banks can leverage advanced learning
capabilities to achieve distortionless multiplexing and harness hardware
acceleration for high efficiency. Then, we evaluate the performance of the
spectrum multiplexer implemented by NN-based filter banks for various types of
signals and environmental conditions. The results show that it can achieve a
low distortion level down to $-39$dB normalized mean squared error.
Furthermore, it achieves up to $35$ times execution efficiency gain and $10$dB
SNR gain compared with the conventional methods. The field applications show
that it can handle both the heterogeneous and homogeneous IoT networks,
resulting in high packet reception ratio at the standard receivers up to
$98\%$.

</details>


### [4] [Stacked Intelligent Metasurface Assisted Multiuser Communications: From a Rate Fairness Perspective](https://arxiv.org/abs/2507.17153)
*Junjie Fang,Chao Zhang,Jiancheng An,Hongwen Yu,Qingqing Wu,Mérouane Debbah,Chau Yuen*

Main category: eess.SP

TL;DR: 本文研究了堆叠智能超表面(SIM)在多用户下行系统中提升速率公平性的应用，通过最大化最小速率(MR)和几何平均速率(GMR)两种优化问题来改善用户间的公平性和系统性能


<details>
  <summary>Details</summary>
Motivation: 传统单层可重构全息表面(RHS)在电磁波传播控制和信号处理能力方面存在限制，需要通过多层结构的堆叠智能超表面来增强控制能力，同时解决多用户下行系统中用户间速率公平性的问题

Method: 针对MR最大化问题采用基于一致性交替方向乘数法(consensus ADMM)的方法，将近似问题分解为具有闭式解的子问题；针对GMR最大化问题开发了基于交替优化(AO)的算法，同样获得闭式解并可适配SR最大化

Result: 数值结果验证了所提算法的有效性和收敛性；MR最大化确保了近乎完美的公平性，GMR最大化在公平性和系统总速率之间取得平衡；两种算法在MR和SR性能方面分别优于现有相关工作；SIM以更低功耗实现了与多天线数字波束成形相当的性能

Conclusion: 堆叠智能超表面通过多层结构设计能够有效提升多用户下行系统的速率公平性，所提出的两种优化算法分别在保证用户公平性和平衡系统性能方面表现优异，且相比传统方法具有更好的性能和更低的功耗

Abstract: Stacked intelligent metasurface (SIM) extends the concept of single-layer
reconfigurable holographic surfaces (RHS) by incorporating a multi-layered
structure, thereby providing enhanced control over electromagnetic wave
propagation and improved signal processing capabilities. This study
investigates the potential of SIM in enhancing the rate fairness in multiuser
downlink systems by addressing two key optimization problems: maximizing the
minimum rate (MR) and maximizing the geometric mean of rates (GMR). {The former
strives to enhance the minimum user rate, thereby ensuring fairness among
users, while the latter relaxes fairness requirements to strike a better
trade-off between user fairness and system sum-rate (SR).} For the MR
maximization, we adopt a consensus alternating direction method of multipliers
(ADMM)-based approach, which decomposes the approximated problem into
sub-problems with closed-form solutions. {For GMR maximization, we develop an
alternating optimization (AO)-based algorithm that also yields closed-form
solutions and can be seamlessly adapted for SR maximization. Numerical results
validate the effectiveness and convergence of the proposed algorithms.}
Comparative evaluations show that MR maximization ensures near-perfect
fairness, while GMR maximization balances fairness and system SR. Furthermore,
the two proposed algorithms respectively outperform existing related works in
terms of MR and SR performance. Lastly, SIM with lower power consumption
achieves performance comparable to that of multi-antenna digital beamforming.

</details>


### [5] [Hybrid Semantic-Complementary Transmission for High-Fidelity Image Reconstruction](https://arxiv.org/abs/2507.17196)
*Hyelin Nam,Jihong Park,Jinho Choi,Seong-Lyun Kim*

Main category: eess.SP

TL;DR: 提出了混合语义通信(HSC)框架，通过结合语义表示(SR)和互补表示(CR)来改善神经网络在图像重构中的保真度问题


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经网络的语义通信收发器在多样化图像分布上训练，往往无法重构细粒度的图像特定细节，导致重构保真度有限

Method: 设计了混合语义通信(HSC)框架，在发送端构建捕获残余图像特定信息的互补表示(CR)，在接收端将CR与语义通信结果结合以产生高保真度的重构图像。CR的传输负载可以灵活调整以达到期望的保真度

Result: 推导了重构误差的闭式表达式和相应的最优CR。仿真结果表明，在各种信道和神经网络架构下，HSC相比不传输CR的基线语义通信大幅降低了均方误差(MSE)

Conclusion: 混合语义通信框架通过引入可调节的互补表示有效解决了传统语义通信重构保真度不足的问题，在保持灵活性的同时显著提升了图像重构质量

Abstract: Recent advances in semantic communication (SC) have introduced neural network
(NN)-based transceivers that convey semantic representation (SR) of signals
such as images. However, these NNs are trained over diverse image distributions
and thus often fail to reconstruct fine-grained image-specific details. To
overcome this limited reconstruction fidelity, we propose an extended SC
framework, hybrid semantic communication (HSC), which supplements SR with
complementary representation (CR) capturing residual image-specific
information. The CR is constructed at the transmitter, and is combined with the
actual SC outcome at the receiver to yield a high-fidelity recomposed image.
While the transmission load of SR is fixed due to its NN-based structure, the
load of CR can be flexibly adjusted to achieve a desirable fidelity. This
controllability directly influences the final reconstruction error, for which
we derive a closed-form expression and the corresponding optimal CR. Simulation
results demonstrate that HSC substantially reduces MSE compared to the baseline
SC without CR transmission across various channels and NN architectures.

</details>


### [6] [HuiduRep: A Robust Self-Supervised Framework for Learning Neural Representations from Extracellular Spikes](https://arxiv.org/abs/2507.17224)
*Feng Cao,Zishuo Feng*

Main category: eess.SP

TL;DR: 本文提出HuiduRep，一个基于自监督学习的细胞外记录信号表征学习框架，通过结合对比学习和去噪自编码器来解决spike sorting中的噪声、电极漂移和跨会话变异性问题


<details>
  <summary>Details</summary>
Motivation: 细胞外记录是神经科学中解码大脑活动的重要技术，但spike sorting在低信噪比、电极漂移和跨会话变异性条件下仍然具有挑战性，需要更鲁棒的方法来提取discriminative和可泛化的特征

Method: 提出HuiduRep框架，结合对比学习和去噪自编码器进行自监督表征学习，从细胞外spike波形中提取对噪声和漂移鲁棒的潜在表征，并基于此开发无监督的spike聚类pipeline

Result: 在混合数据集和真实世界数据集上的实验显示，HuiduRep具有强鲁棒性，其pipeline性能匹配或超越了KiloSort4和MountainSort5等最先进工具

Conclusion: 自监督spike表征学习作为细胞外记录鲁棒和可泛化处理的基础工具具有巨大潜力，HuiduRep为解决神经信号处理中的关键挑战提供了有效解决方案

Abstract: Extracellular recordings are brief voltage fluctuations recorded near
neurons, widely used in neuroscience as the basis for decoding brain activity
at single-neuron resolution. Spike sorting, which assigns each spike to its
source neuron, is a critical step in brain sensing pipelines. However, it
remains challenging under low signal-to-noise ratio (SNR), electrode drift, and
cross-session variability. In this paper, we propose HuiduRep, a robust
self-supervised representation learning framework that extracts discriminative
and generalizable features from extracellular spike waveforms. By combining
contrastive learning with a denoising autoencoder, HuiduRep learns latent
representations that are robust to noise and drift. Built on HuiduRep, we
develop a spike sorting pipeline that clusters spike representations without
supervision. Experiments on hybrid and real-world datasets demonstrate that
HuiduRep achieves strong robustness and the pipeline matches or outperforms
state-of-the-art tools such as KiloSort4 and MountainSort5. These findings
demonstrate the potential of self-supervised spike representation learning as a
foundational tool for robust and generalizable processing of extracellular
recordings.

</details>


### [7] [Joint Resource Optimization Over Licensed and Unlicensed Spectrum in Spectrum Sharing UAV Networks Against Jamming Attacks](https://arxiv.org/abs/2507.17261)
*Rui Ding,Fuhui Zhou,Yuhang Wu,Qihui Wu,Tony Q. S. Quek*

Main category: eess.SP

TL;DR: 本文研究了在频谱共享环境下的无人机通信网络抗干扰优化问题，通过联合优化发射功率、子信道分配和无人机轨迹来最大化网络和速率


<details>
  <summary>Details</summary>
Motivation: 无人机通信中用户密集、服务多样化导致频谱稀缺，需要结合授权和非授权频谱提升网络容量，但非授权频谱的开放性使无人机易受干扰威胁

Method: 将复杂的非凸优化问题分解为两个子问题：1）联合功率和子信道分配，2）无人机轨迹设计。采用交替优化的低复杂度迭代算法，利用拉格朗日对偶分解优化功率和子信道分配，使用连续凸近似算法设计无人机轨迹

Result: 仿真结果表明，所提算法相比基准方案能显著提升网络和传输速率

Conclusion: 在频谱共享的无人机网络中，通过联合优化发射功率、子信道分配和轨迹设计，可以有效解决抗干扰问题并显著提升网络性能

Abstract: Unmanned aerial vehicle (UAV) communication is of crucial importance in
realizing heterogeneous practical wireless application scenarios. However, the
densely populated users and diverse services with high data rate demands has
triggered an increasing scarcity of UAV spectrum utilization. To tackle this
problem, it is promising to incorporate the underutilized unlicensed spectrum
with the licensed spectrum to boost network capacity. However, the openness of
unlicensed spectrum makes UAVs susceptible to security threats from potential
jammers. Therefore, a spectrum sharing UAV network coexisting with licensed
cellular network and unlicensed Wi-Fi network is considered with the
anti-jamming technique in this paper. The sum rate maximization of the
secondary network is studied by jointly optimizing the transmit power,
subchannel allocation, and UAV trajectory. We first decompose the challenging
non-convex problem into two subproblems, 1) the joint power and subchannel
allocation and 2) UAV trajectory design subproblems. A low-complexity iterative
algorithm is proposed in a alternating optimization manner over these two
subproblems to solve the formulated problem. Specifically, the Lagrange dual
decomposition is exploited to jointly optimize the transmit power and
subchannel allocation iteratively. Then, an efficient iterative algorithm
capitalizing on successive convex approximation is designed to get a suboptimal
solution for UAV trajectory. Simulation results demonstrate that our proposed
algorithm can significantly improve the sum transmission rate compared with the
benchmark schemes.

</details>


### [8] [State Estimation with 1-Bit Observations and Imperfect Models: Bussgang Meets Kalman in Neural Networks](https://arxiv.org/abs/2507.17284)
*Chaehyun Jung,TaeJun Ha,Hyeonuk Kim,Jeonghun Park*

Main category: eess.SP

TL;DR: 本文针对1比特量化环境下的状态估计问题，提出了基于Bussgang分解技术的卡尔曼滤波器及其深度学习变体，有效解决了量化失真和信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 传统状态估计方法假设不存在量化失真，但在实际应用中1比特量化会导致严重的量化失真和信息丢失，使得传统方法不再适用，需要开发能够处理这种严重量化影响的新方法。

Method: 基于Bussgang分解技术开发了Bussgang辅助卡尔曼滤波器，提出了计算高效的简化版本，并结合抖动技术和门控循环单元(GRU)架构，构建了能处理部分已知模型的Bussgang辅助KalmanNet深度学习方法。

Result: 在Lorenz-Attractor模型和Michigan NCLT数据集上的仿真结果表明，所提出的方法即使在高度非线性、模型失配和1比特观测条件下也能实现准确的状态估计性能。

Conclusion: 提出的Bussgang辅助方法能够有效地将量化失真纳入状态估计过程，通过结合深度学习技术和专门的架构设计，成功解决了1比特量化环境下的状态估计挑战。

Abstract: State estimation from noisy observations is a fundamental problem in many
applications of signal processing. Traditional methods, such as the extended
Kalman filter, work well under fully-known Gaussian models, while recent hybrid
deep learning frameworks, combining model-based and data-driven approaches, can
also handle partially known models and non-Gaussian noise. However, existing
studies commonly assume the absence of quantization distortion, which is
inevitable, especially with non-ideal analog-to-digital converters. In this
work, we consider a state estimation problem with 1-bit quantization. 1-bit
quantization causes significant quantization distortion and severe information
loss, rendering conventional state estimation strategies unsuitable. To address
this, inspired by the Bussgang decomposition technique, we first develop the
Bussgang-aided Kalman filter by assuming perfectly known models. The proposed
method suitably captures quantization distortion into the state estimation
process. In addition, we propose a computationally efficient variant, referred
to as the reduced Bussgang-aided Kalman filter and, building upon it, introduce
a deep learning-based approach for handling partially known models, termed the
Bussgang-aided KalmanNet. In particular, the Bussgang-aided KalmanNet jointly
uses a dithering technique and a gated recurrent unit (GRU) architecture to
effectively mitigate the effects of 1-bit quantization and model mismatch.
Through simulations on the Lorenz-Attractor model and the Michigan NCLT
dataset, we demonstrate that our proposed methods achieve accurate state
estimation performance even under highly nonlinear, mismatched models and 1-bit
observations.

</details>


### [9] [Non-Orthogonal AFDM: A Promising Spectrum-Efficient Waveform for 6G High-Mobility Communications](https://arxiv.org/abs/2507.17292)
*Yu Zhang,Qin Yi,Leila Musavian,Tongyang Xu,Zilong Liu*

Main category: eess.SP

TL;DR: 本文提出了一种频谱高效的非正交仿射频分复用(AFDM)波形，通过引入压缩因子实现可控的子载波重叠，并采用线性预编码和迭代检测来减少载波间干扰，为6G高移动性通信提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 针对6G移动系统中高移动性通信场景，需要在频谱效率和多普勒抗性之间找到平衡，现有正交波形在高动态环境下性能受限，因此需要设计新的非正交波形来提升频谱效率同时保持可靠性。

Method: 提出非正交AFDM波形设计方法：1）引入压缩因子实现chirp基AFDM调制中的可控子载波重叠；2）在发射端采用线性预编码技术；3）在接收端设计迭代检测方案来减轻载波间干扰(ICI)。

Result: 仿真结果表明，所提技术能够有效减少干扰，即使在激进压缩因子和高移动性信道条件下仍能保持稳健的误码率(BER)性能，验证了方案的有效性。

Conclusion: 非正交AFDM波形为下一代无线网络提供了有前景的解决方案，能够在高动态环境中平衡频谱效率和多普勒抗性，适用于6G高移动性通信场景。

Abstract: This paper proposes a spectrum-efficient nonorthogonal affine frequency
division multiplexing (AFDM) waveform for reliable high-mobility communications
in the upcoming sixth-generation (6G) mobile systems. Our core idea is to
introduce a compression factor to enable controllable subcarrier overlapping in
chirp-based AFDM modulation. To mitigate intercarrier interference (ICI), we
introduce linear precoding at the transmitter and an iterative detection scheme
at the receiver. Simulation results demonstrate that these techniques can
effectively reduce interference and maintain robust bit error rate (BER)
performance even under aggressive compression factors and high-mobility channel
conditions. The proposed non-orthogonal AFDM waveform offers a promising
solution for next-generation wireless networks, balancing spectrum efficiency
and Doppler resilience in highly dynamic environments.

</details>


### [10] [LightCom: A Generative AI-Augmented Framework for QoE-Oriented Communications](https://arxiv.org/abs/2507.17352)
*Chunmei Xu,Siqi Zhang,Yi Ma,Rahim Tafazolli*

Main category: eess.SP

TL;DR: LightCom是一个面向QoE的轻量级编码和生成AI增强解码框架，通过简化发射端设计和利用生成AI在接收端重建高保真内容，在低信噪比条件下实现了显著的鲁棒性和覆盖范围改进。


<details>
  <summary>Details</summary>
Motivation: 数据密集型和沉浸式应用（如虚拟现实）对用户体验质量（QoE）提出了严格要求，挑战了传统的以服务质量（QoS）为驱动的通信系统。需要在低信噪比条件下实现面向QoE的通信。

Method: 提出LightCom框架：1）发射端采用基础低通滤波进行信源编码和最小信道编码，简化设计并降低复杂度；2）接收端使用生成AI模型从高度压缩和退化的信号中重建高保真内容，利用生成先验推断语义和结构信息；3）开发重要性感知功率分配策略来增强QoE和扩展感知覆盖范围。

Result: 仿真结果显示LightCom在鲁棒性方面实现了高达14 dB的改进，在感知覆盖范围方面获得了9 dB的增益，性能优于依赖复杂信源和信道编码的传统QoS驱动系统。

Conclusion: 这种范式转变将通信系统从比特级保真度转向以人为中心的QoE指标，为构建更高效和更具弹性的无线网络铺平了道路。

Abstract: Data-intensive and immersive applications, such as virtual reality, impose
stringent quality of experience (QoE) requirements that challenge traditional
quality of service (QoS)-driven communication systems. This paper presents
LightCom, a lightweight encoding and generative AI (GenAI)-augmented decoding
framework, designed for QoE-oriented communications under low signal-to-noise
ratio (SNR) conditions. LightCom simplifies transmitter design by applying
basic low-pass filtering for source coding and minimal channel coding,
significantly reducing processing complexity and energy consumption. At the
receiver, GenAI models reconstruct high-fidelity content from highly compressed
and degraded signals by leveraging generative priors to infer semantic and
structural information beyond traditional decoding capabilities. The key design
principles are analyzed, along with the sufficiency and error-resilience of the
source representation. We also develop importance-aware power allocation
strategies to enhance QoE and extend perceived coverage. Simulation results
demonstrate that LightCom achieves up to a $14$ dB improvement in robustness
and a $9$ dB gain in perceived coverage, outperforming traditional QoS-driven
systems relying on sophisticated source and channel coding. This paradigm shift
moves communication systems towards human-centric QoE metrics rather than
bit-level fidelity, paving the way for more efficient and resilient wireless
networks.

</details>


### [11] [Partially Reflected Surface (PRS)-Loaded Graphene-Based Patch Antenna for 6G](https://arxiv.org/abs/2507.17393)
*Omar Osman,Abdullah Qayyum,Maziar Nekovee*

Main category: eess.SP

TL;DR: 本研究设计了一种集成部分反射表面(PRS)的开槽贴片天线，工作在太赫兹频段，用于6G通信。该天线基于石墨烯材料，在Rogers RT Duroid 6010基板上实现，带宽达70 GHz，增益提升1.07 dBi。


<details>
  <summary>Details</summary>
Motivation: 为6G通信系统开发工作在太赫兹频段的高性能天线，需要解决太赫兹频段天线增益低、辐射方向图不稳定等问题。

Method: 设计了基于石墨烯材料的开槽贴片天线，集成5x4单元格的部分反射表面(PRS)，使用Rogers RT Duroid 6010作为基板材料，通过优化PRS单元格来增强天线整体性能。

Result: 天线实现了70 GHz的带宽(750-820 GHz)，整体实现增益提升了1.07 dBi，PRS增强了天线辐射方向图，在工作带宽内表现出稳定的特性。

Conclusion: 集成PRS的石墨烯开槽贴片天线成功提升了太赫兹频段的天线性能，包括增益和辐射方向图稳定性，通过仿真验证了改进后天线的有效性，为6G通信系统提供了可行的天线解决方案。

Abstract: This work investigates a slotted patch antenna integrated with a partially
reflected surface (PRS) to operate in the TeraHertz (THz) frequency range for
6G. The antenna is based on graphene material, on a Rogers RT Duroid 6010
substrate. The proposed antenna achieves a bandwidth of 70 GHz (750 GHz to 820
GHz). The PRS sheet consists of 5x4 unit cells, which are optimised to enhance
the overall realized gain of the antenna. The overall realized gain has
increased by 1.07 dBi. Also, the PRS enhanced the antenna radiation pattern,
showing stable properties over the operating bandwidth. The improved antenna
performance is validated via simulations.

</details>


### [12] [Learning from Scratch: Structurally-masked Transformer for Next Generation Lib-free Simulation](https://arxiv.org/abs/2507.17396)
*Junlang Huang,Hao Chen,Zhong Guan*

Main category: eess.SP

TL;DR: 本文提出了一个基于神经网络的多级数据路径功耗和时序预测框架，使用预训练的波形预测和延迟估计模型直接从SPICE网表推断瞬态波形和传播延迟，在工业电路上达到SPICE级精度。


<details>
  <summary>Details</summary>
Motivation: 传统基于库的分析方法依赖于驱动器特性描述和负载简化，存在局限性。需要一种能够准确捕获内在和耦合诱导延迟效应的新方法，无需简化或插值，同时能够处理复杂信号路径中的完整波形可见性。

Method: 采用混合CNN-Transformer架构和网表感知的节点级编码，构建两个预训练神经模型：波形预测和延迟估计。使用递归传播策略进行多级时序预测，其中每级的预测波形输入到后续级。专门的子网络分别处理主要延迟估计和串扰校正。

Result: 在多样化工业电路上实现SPICE级精度，RMSE始终低于0.0098。该框架提供了可扩展、结构自适应的神经网络替代方案，对物理电路行为表现出高保真度。

Conclusion: 提出的神经框架成功替代了传统的功耗和时序引擎，通过直接从SPICE网表推断实现了高精度预测，为标准单元设计提供了首个基于语言的、网表感知的神经网络解决方案。

Abstract: This paper proposes a neural framework for power and timing prediction of
multi-stage data path, distinguishing itself from traditional lib-based
analytical methods dependent on driver characterization and load
simplifications. To the best of our knowledge, this is the first
language-based, netlist-aware neural network designed explicitly for standard
cells. Our approach employs two pre-trained neural models of waveform
prediction and delay estimation that directly infer transient waveforms and
propagation delays from SPICE netlists, conditioned on critical physical
parameters such as load capacitance, input slew, and gate size. This method
accurately captures both intrinsic and coupling-induced delay effects without
requiring simplification or interpolation. For multi-stage timing prediction,
we implement a recursive propagation strategy where predicted waveforms from
each stage feed into subsequent stages, cumulatively capturing delays across
the logic chain. This approach ensures precise timing alignment and complete
waveform visibility throughout complex signal pathways. The waveform prediction
utilizes a hybrid CNN-Transformer architecture with netlist-aware node-level
encoding, addressing traditional Transformers' fixed input dimensionality
constraints. Additionally, specialized subnetworks separately handle primary
delay estimation and crosstalk correction. Experimental results demonstrate
SPICE-level accuracy, consistently achieving RMSE below 0.0098 across diverse
industrial circuits. The proposed framework provides a scalable, structurally
adaptable neural alternative to conventional power and timing engines,
demonstrating high fidelity to physical circuit behaviors.

</details>


### [13] [Power Allocation and RIS Elements Optimisation for Reconfigurable Intelligent Surfaces assisted RSMA](https://arxiv.org/abs/2507.17419)
*Abdullah Qayyum,Maziar Nekovee*

Main category: eess.SP

TL;DR: 本文提出了一种优化的可重配置智能表面辅助速率分割多址接入系统(ORIS-RSMA)，通过联合优化RIS元素数量和功率分配来最大化系统和速率。


<details>
  <summary>Details</summary>
Motivation: 在RIS辅助的RSMA系统中，需要同时优化RIS元素数量和功率分配策略，以提高系统性能并满足目标公共速率要求。

Method: 提出ORIS-RSMA方法，联合优化RIS元素数量和消息公共部分、私有部分的功率分配因子，在保证目标公共速率的约束下最大化系统和速率。

Result: 仿真结果表明，相比传统的RIS-RSMA和RSMA方案，ORIS-RSMA能够实现更高的系统和速率。

Conclusion: 所提出的ORIS-RSMA方法通过优化RIS元素数量和功率分配，能够有效提升RIS辅助RSMA系统的性能，实现更高的和速率。

Abstract: This paper proposes power allocation and the number of reconfigurable
intelligent surfaces (RIS) elements optimisation in a RIS-assisted rate
splitting multiple access (RSMA) system. The optimised RIS-RSMA (ORIS-RSMA)
method determines the optimal number of RIS elements and the power allocation
factors for both common and private parts of a message. Additionally, it
maximises the sum rate while ensuring that a target common rate is satisfied.
The performance of the proposed ORIS-RSMA is compared to that of the
conventional RIS-RSMA and RSMA. Simulation results show that ORIS-RSMA achieves
a higher sum rate.

</details>


### [14] [Detecting Multiple Targets with Distributed Sensing and Communication in Cell-Free Massive MIMO](https://arxiv.org/abs/2507.17441)
*Zinat Behdad,Ozlem Tugfe Demir,Ki Won Sung,Cicek Cavdar*

Main category: eess.SP

TL;DR: 本文研究了在无小区大规模MIMO框架下的集成感知通信系统中的多目标检测问题，提出了基于用户中心的通信方法和分布式感知方法，通过启发式接入点模式选择算法和信道感知的分布式感知方案来平衡通信-感知权衡。


<details>
  <summary>Details</summary>
Motivation: 在集成感知通信(ISAC)系统中，需要在无小区大规模MIMO框架下实现有效的多目标检测，同时保证通信性能。现有方法在平衡通信与感知性能方面存在挑战，需要开发新的算法来优化这种权衡关系。

Method: 采用用户中心的通信用户设备方法和分布式感知的多目标检测方法。提出启发式接入点模式选择算法和基于信号干扰比加权的信道感知分布式感知方案。在接收接入点应用最大后验比检验检测器，并开发功率分配算法来联合最大化最小检测概率和通信信干噪比。

Result: 所提方案优于非加权方法。增加更多接收接入点的测试统计量可能由于信道较弱而降低感知性能，但可通过优化加权指数来缓解。为感知区域分配更多感知接收接入点会导致最小通信SINR损失约10dB，这是由于通信资源有限造成的。

Conclusion: 研究表明在ISAC系统中可以通过优化的功率分配和加权方案有效平衡通信与感知性能。虽然增加感知资源会对通信性能产生一定影响，但通过合理的参数优化可以实现较好的性能权衡。

Abstract: This paper investigates multi-target detection in an integrated sensing and
communication (ISAC) system within a cell-free massive MIMO (CF-mMIMO)
framework. We adopt a user-centric approach for communication user equipments
(UEs) and a distributed sensing approach for multi-target detection. A
heuristic access point (AP) mode selection algorithm and a channel-aware
distributed sensing scheme are proposed, where local measurements at receive
APs (RX-APs) are weighted based on the received signals signal-to-interference
ratio (SIR). A maximum a posteriori ratio test (MAPRT) detector is applied
under two awareness levels at RX-APs. To balance the communication-sensing
trade-off, we develop a power allocation algorithm to jointly maximize the
minimum detection probability and communication
signal-to-interference-plus-noise ratio (SINR) while satisfying power
constraints. The proposed scheme outperforms non-weighted methods. Adding test
statistics from more RX-APs can degrade sensing performance due to weaker
channels, but this effect can be mitigated by optimizing the weighting
exponent. Additionally, assigning more sensing RX-APs to a sensing area results
in approximately 10 dB loss in minimum communication SINR due to limited
communication resources.

</details>


### [15] [Slow Fluid Antenna Multiple Access with Multiport Receivers](https://arxiv.org/abs/2507.17505)
*José P. González-Coma,F. Javier López-Martínez*

Main category: eess.SP

TL;DR: 该论文研究了为流体天线接收机配备多个射频链路能否提升慢速流体天线多址接入技术的性能，通过联合设计端口选择矩阵和合并向量实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索在流体天线系统中使用多端口接收机（配备多个射频链路）是否能够改善慢速流体天线多址接入技术的性能，该技术仅在接收端具有信道状态信息的开环连接场景下工作。

Method: 分析配备多端口接收机的慢速FAMA用户，选择L个流体天线端口并进行合并以减少干扰。提出了端口选择矩阵和合并向量的联合设计方法。

Result: 联合设计方案相比参考方案实现了显著的性能提升，证明了在射频链路数量有限的流体天线系统中多端口接收的潜力。

Conclusion: 多端口接收技术能够有效提升流体天线系统的性能，通过优化端口选择和信号合并策略，即使在射频链路数量受限的情况下也能获得显著的性能改善。

Abstract: We investigate whether equipping fluid-antenna (FA) receivers with multiple
($L>1$) radiofrequency (RF) chains can improve the performance of the slow
fluid-antenna multiple access (FAMA) technique, which enables open-loop
connectivity with channel state information (CSI) available only at the
receiver side. We analyze the case of slow-FAMA users equipped with multiport
receivers, so that $L$ ports of the FA are selected and combined to reduce
interference. We show that a joint design of the port selection matrix and the
combining vector at each receiver yields significant performance gains over
reference schemes, demonstrating the potential of multiport reception in FA
systems with a limited number of RF chains.

</details>


### [16] [Joint Multi-Target Detection-Tracking in Cognitive Massive MIMO Radar via POMCP](https://arxiv.org/abs/2507.17506)
*Imad Bouhou,Stefano Fortunati,Leila Gharsalli,Alexandre Renaux*

Main category: eess.SP

TL;DR: 本文提出了一种基于POMCP的功率感知认知雷达框架，用于大规模MIMO雷达环境中多目标的联合检测与跟踪，通过自适应波形设计和功率分配优化，显著提升了低信噪比目标的检测概率和跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 传统雷达系统采用均匀功率分配策略，在面对不同信噪比的多目标场景时效果不佳。需要开发一种能够根据目标特性自适应分配发射功率的认知雷达系统，以提高弱目标或远距离目标的可检测性，同时确保高信噪比目标有足够的功率支持。

Method: 将单目标POMCP算法扩展到多目标场景，为每个目标分配独立的POMCP树实现可扩展规划。基于目标的估计距离和雷达散射截面预测其未来角位置和期望接收功率，通过约束优化问题指导自适应波形设计，分配发射能量以增强弱目标检测能力。同时修改POMDP中的奖励函数以优先考虑准确的空间和功率估计。

Result: 仿真实验验证了方法的有效性，与使用均匀或正交波形的方法相比，所提框架显著提高了低信噪比目标的检测概率，实现了更准确的目标跟踪性能。

Conclusion: 基于POMCP的功率感知认知雷达框架能够有效实现多目标的自适应检测与跟踪，通过智能的功率分配策略显著改善了系统性能，展现了在自适应、高效多目标雷达系统中的应用潜力。

Abstract: This correspondence presents a power-aware cognitive radar framework for
joint detection and tracking of multiple targets in a massive multiple-input
multiple-output (MIMO) radar environment. Building on a previous single-target
algorithm based on Partially Observable Monte Carlo Planning (POMCP), we extend
it to the multi-target case by assigning each target an independent POMCP tree,
enabling scalable and efficient planning.
  Departing from uniform power allocation-which is often suboptimal with
varying signal-to-noise ratios (SNRs)-our approach predicts each target's
future angular position and expected received power, based on its estimated
range and radar cross-section (RCS). These predictions guide adaptive waveform
design via a constrained optimization problem that allocates transmit energy to
enhance the detectability of weaker or distant targets, while ensuring
sufficient power for high-SNR targets. The reward function in the underlying
partially observable Markov decision process (POMDP) is also modified to
prioritize accurate spatial and power estimation.
  Simulations involving multiple targets with different SNRs confirm the
effectiveness of our method. The proposed framework for the cognitive radar
improves detection probability for low-SNR targets and achieves more accurate
tracking compared to approaches using uniform or orthogonal waveforms. These
results demonstrate the potential of the POMCP-based framework for adaptive,
efficient multi-target radar systems.

</details>


### [17] [SA-WiSense: A Blind-Spot-Free Respiration Sensing Framework for Single-Antenna Wi-Fi Devices](https://arxiv.org/abs/2507.17623)
*Guangteng Liu,Xiayue Liu,Zhixiang Xu,Yufeng Yuan,Hui Zhao,Yuxuan Liu,Yufei Jiang*

Main category: eess.SP

TL;DR: 提出了一种单天线Wi-Fi感知框架(SA-WiSense)，通过跨子载波CSI比值方法和遗传算法子载波选择，解决了Wi-Fi呼吸监测中的随机相位偏移盲点问题，在8米距离内实现91.2%的检测率。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi感知技术在非接触式人体呼吸监测中具有前景，但面临随机相位偏移导致的盲点问题，该问题会破坏呼吸信号的互补性。现有方法需要多天线，成本高且不适用于物联网单天线设备。

Method: 提出SA-WiSense框架，包含两个核心方法：1）跨子载波信道状态信息比值(CSCR)方法，利用子载波间CSI比值来抵消随机相位偏移；2）基于遗传算法的子载波选择(GASS)方法，通过优化感知信号噪声比来选择最优子载波组合。

Result: 使用ESP32微控制器进行实验验证，在8.0米距离内实现了91.2%的呼吸检测率，显著优于现有单天线方法的性能表现。

Conclusion: SA-WiSense框架成功解决了单天线Wi-Fi感知中的盲点问题，实现了成本效益高且准确的非接触式呼吸监测，特别适用于物联网应用场景。

Abstract: Wi-Fi sensing offers a promising technique for contactless human respiration
monitoring. A key challenge, however, is the blind spot problem caused by
random phase offsets that corrupt the complementarity of respiratory signals.
To address the challenge, we propose a single-antenna-Wi-Fi-sensing
(SA-WiSense) framework to improve accuracy of human respiration monitoring,
robust against random phase offsets. The proposed SA-WiSense framework is
cost-efficient, as only a single antenna is used rather than multiple antennas
as in the previous works. Therefore, the proposed framework is applicable to
Internet of Thing (IoT), where most of sensors are equipped with a single
antenna. On one hand, we propose a cross-subcarrier channel state information
(CSI) ratio (CSCR) based blind spot mitigation approach for IoT, where the
ratios of two values of CSI between subcarriers are leveraged to mitigate
random phase offsets. We prove that the random phase offsets can be cancelled
by the proposed CSCR approach, thereby restoring the inherent complementarity
of signals for blind-spot-free sensing. On the other hand, we propose a genetic
algorithm (GA) based subcarrier selection (GASS) approach by formulating an
optimization problem in terms of the sensing-signal-to-noise ratio (SSNR) of
CSCR between subcarriers. GA is utilized to solve the formulated optimization
problem. We use commodity ESP32 microcontrollers to build an experiment test.
The proposed works are validated to achieve an detection rate of 91.2% for
respiration monitoring at distances up to 8.0 meters, substantially more
accurate than the state-of-the-art methods with a single antenna.

</details>


### [18] [Quaternion-Domain Super MDS for Robust 3D Localization](https://arxiv.org/abs/2507.17645)
*Alessio Lukaj,Keigo Masuoka,Takumi Takahashi,Giuseppe Thadeu Freitas de Abreu,Hideki Ochiai*

Main category: eess.SP

TL;DR: 本文提出了一种基于四元数代数的低复杂度三维无线传感器网络定位算法QD-SMDS，通过将3D坐标表示为四元数并构建秩-1 Gram边核矩阵来集成相对距离和角度信息，显著提高了定位精度并降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统的SMDS算法在实域中运行，在面对测量误差较大的场景时定位精度不足，且计算复杂度较高（需要SVD分解）。为了提高无线传感器网络三维定位的鲁棒性和计算效率，需要开发新的低复杂度定位算法。

Method: 将原本在实域中的SMDS算法重新表述到四元数域中，通过四元数表示3D坐标，构建秩-1的Gram边核(GEK)矩阵来同时集成节点间的相对距离和角度信息。利用奇异值分解(SVD)的低秩截断来增强降噪效果。此外还提出了一个QD-SMDS变体，通过利用四元数域GEK矩阵的固有结构特性，仅使用矩阵乘法直接估计节点坐标，从而避免了计算昂贵的SVD操作。

Result: 仿真结果表明，与原始SMDS算法相比，所提出的方法显著提高了定位精度，特别是在测量误差较大的场景中表现更佳。同时，不需要SVD的变体算法也能达到可比的定位精度，大大降低了计算复杂度。

Conclusion: QD-SMDS算法通过四元数域的重新表述和GEK矩阵的构建，成功地提高了三维无线传感器网络定位的精度和鲁棒性，同时降低了计算复杂度。该方法为无线传感器网络的高精度低复杂度定位提供了一种有效的解决方案。

Abstract: This paper proposes a novel low-complexity three-dimensional (3D)
localization algorithm for wireless sensor networks, termed quanternion-domain
super multi-dimensional scaling (QD-SMDS). The algorithm is based on a
reformulation of the SMDS, originally developed in the real domain, using
quaternion algebra. By representing 3D coordinates as quaternions, the method
constructs a rank-1 Gram edge kernel (GEK) matrix that integrates both relative
distance and angular information between nodes, which enhances the noise
reduction effect achieved through low-rank truncation employing singular value
decomposition (SVD), thereby improving robustness against information loss. To
further reduce computational complexity, we also propose a variant of QD-SMDS
that eliminates the need for the computationally expensive SVD by leveraging
the inherent structure of the quaternion-domain GEK matrix. This alternative
directly estimates node coordinates using only matrix multiplications within
the quaternion domain. Simulation results demonstrate that the proposed method
significantly improves localization accuracy compared to the original SMDS
algorithm, especially in scenarios with substantial measurement errors. The
proposed method also achieves comparable localization accuracy without
requiring SVD.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Sensor Drift Compensation in Electronic-Nose-Based Gas Recognition Using Knowledge Distillation](https://arxiv.org/abs/2507.17071)
*Juntao Lin,Xianghao Zhan*

Main category: cs.LG

TL;DR: 本文提出了基于知识蒸馏(KD)的电子鼻传感器漂移补偿方法，在UCI气体传感器阵列漂移数据集上显著优于现有的DRCA方法，准确率提升18%，F1分数提升15%


<details>
  <summary>Details</summary>
Motivation: 电子鼻系统在实际部署中面临传感器漂移问题，影响气体分类性能。现有研究缺乏严格的统计实验验证，且可能过度补偿传感器漂移而丢失类别相关的方差信息

Method: 设计了两个领域适应任务来模拟实际应用场景，系统测试了三种方法：提出的知识蒸馏(KD)方法、基准方法DRCA和混合方法KD-DRCA，在UCI数据集上进行了30次随机测试集划分的统计验证

Result: 知识蒸馏方法在漂移补偿方面持续优于DRCA和KD-DRCA方法，准确率提升高达18%，F1分数提升15%，证明了KD在漂移补偿中的卓越有效性

Conclusion: 这是首次将知识蒸馏应用于电子鼻漂移缓解，显著超越了之前最先进的DRCA方法，提高了传感器漂移补偿在真实环境中的可靠性

Abstract: Due to environmental changes and sensor aging, sensor drift challenges the
performance of electronic nose systems in gas classification during real-world
deployment. Previous studies using the UCI Gas Sensor Array Drift Dataset
reported promising drift compensation results but lacked robust statistical
experimental validation and may overcompensate for sensor drift, losing
class-related variance.To address these limitations and improve sensor drift
compensation with statistical rigor, we first designed two domain adaptation
tasks based on the same electronic nose dataset: using the first batch to
predict the remaining batches, simulating a controlled laboratory setting; and
predicting the next batch using all prior batches, simulating continuous
training data updates for online training. We then systematically tested three
methods: our proposed novel Knowledge Distillation (KD) method, the benchmark
method Domain Regularized Component Analysis (DRCA), and a hybrid method
KD-DRCA, across 30 random test set partitions on the UCI dataset. We showed
that KD consistently outperformed both DRCA and KD-DRCA, achieving up to an 18%
improvement in accuracy and 15% in F1-score, demonstrating KD's superior
effectiveness in drift compensation. This is the first application of KD for
electronic nose drift mitigation, significantly outperforming the previous
state-of-the-art DRCA method and enhancing the reliability of sensor drift
compensation in real-world environments.

</details>


### [20] [ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search](https://arxiv.org/abs/2507.17096)
*Olivia Dry,Timothy L. Molloy,Wanxin Jin,Iman Shames*

Main category: cs.LG

TL;DR: 本文提出了ZORMS-LfD方法，这是一种零阶随机矩阵搜索算法，用于从专家演示中学习约束最优控制问题，无需梯度信息即可学习成本、约束和动力学


<details>
  <summary>Details</summary>
Motivation: 现有的一阶方法需要计算成本、约束、动力学和学习损失相对于状态、控制和参数的梯度，且要求学习损失景观的平滑性。大多数现有方法仅适用于离散时间，对连续时间约束问题关注不足

Method: 提出零阶随机矩阵搜索算法(ZORMS-LfD)，该方法能够在不需要平滑性假设的情况下，从专家演示中学习连续和离散时间约束最优控制问题的成本、约束和动力学

Result: 在多个基准问题上，ZORMS-LfD在学习损失和计算时间方面匹配或超越了现有最先进方法的性能。在无约束连续时间基准问题上，实现了与最先进一阶方法相似的损失性能，同时计算时间减少超过80%。在约束连续时间基准问题上，优于常用的无梯度Nelder-Mead优化方法

Conclusion: ZORMS-LfD方法成功解决了现有方法对梯度信息的依赖问题，在保持学习性能的同时显著提高了计算效率，特别适用于连续时间约束最优控制问题的学习

Abstract: We propose Zeroth-Order Random Matrix Search for Learning from Demonstrations
(ZORMS-LfD). ZORMS-LfD enables the costs, constraints, and dynamics of
constrained optimal control problems, in both continuous and discrete time, to
be learned from expert demonstrations without requiring smoothness of the
learning-loss landscape. In contrast, existing state-of-the-art first-order
methods require the existence and computation of gradients of the costs,
constraints, dynamics, and learning loss with respect to states, controls
and/or parameters. Most existing methods are also tailored to discrete time,
with constrained problems in continuous time receiving only cursory attention.
We demonstrate that ZORMS-LfD matches or surpasses the performance of
state-of-the-art methods in terms of both learning loss and compute time across
a variety of benchmark problems. On unconstrained continuous-time benchmark
problems, ZORMS-LfD achieves similar loss performance to state-of-the-art
first-order methods with an over $80$\% reduction in compute time. On
constrained continuous-time benchmark problems where there is no specialized
state-of-the-art method, ZORMS-LfD is shown to outperform the commonly used
gradient-free Nelder-Mead optimization method.

</details>


### [21] [Evaluating Artificial Intelligence Algorithms for the Standardization of Transtibial Prosthetic Socket Shape Design](https://arxiv.org/abs/2507.16818)
*C. H. E. Jordaan,M. van der Stelt,T. J. J. Maal,V. M. A. Stirler,R. Leijendekkers,T. Kachman,G. A. de Jong*

Main category: cs.LG

TL;DR: 该研究开发了多种人工智能方法来标准化经胫假肢接受腔设计，使用118名患者的3D扫描数据训练了三种算法（3D神经网络、前馈神经网络和随机森林），其中随机森林在预测假肢师适应性调整方面表现最佳，中位误差仅1.24毫米


<details>
  <summary>Details</summary>
Motivation: 经胫假肢接受腔的质量依赖于假肢师的手工技能和专业知识，缺乏标准化，因此需要利用人工智能技术来帮助标准化假肢接受腔的设计过程

Method: 收集118名患者的残肢3D扫描数据和对应的假肢师设计的接受腔3D模型；通过对齐、标准化和可变形模型、主成分分析等预处理步骤；开发三种算法（3D神经网络、前馈神经网络、随机森林）来预测最终接受腔形状或假肢师的适应性调整

Result: 所有算法中，估计所需适应性调整的方法都优于直接预测最终接受腔形状；随机森林模型在适应性预测方面表现最佳，中位表面到表面距离为1.24毫米，第一四分位数为1.03毫米，第三四分位数为1.54毫米

Conclusion: 人工智能方法可以有效辅助经胫假肢接受腔的标准化设计，其中预测假肢师适应性调整的策略比直接预测最终形状更有效，随机森林算法在此任务中表现最优

Abstract: The quality of a transtibial prosthetic socket depends on the prosthetist's
skills and expertise, as the fitting is performed manually. This study
investigates multiple artificial intelligence (AI) approaches to help
standardize transtibial prosthetic socket design. Data from 118 patients were
collected by prosthetists working in the Dutch healthcare system. This data
consists of a three-dimensional (3D) scan of the residual limb and a
corresponding 3D model of the prosthetist-designed socket. Multiple data
pre-processing steps are performed for alignment, standardization and
optionally compression using Morphable Models and Principal Component Analysis.
Afterward, three different algorithms - a 3D neural network, Feedforward neural
network, and random forest - are developed to either predict 1) the final
socket shape or 2) the adaptations performed by a prosthetist to predict the
socket shape based on the 3D scan of the residual limb. Each algorithm's
performance was evaluated by comparing the prosthetist-designed socket with the
AI-generated socket, using two metrics in combination with the error location.
First, we measure the surface-to-surface distance to assess the overall surface
error between the AI-generated socket and the prosthetist-designed socket.
Second, distance maps between the AI-generated and prosthetist sockets are
utilized to analyze the error's location. For all algorithms, estimating the
required adaptations outperformed direct prediction of the final socket shape.
The random forest model applied to adaptation prediction yields the lowest
error with a median surface-to-surface distance of 1.24 millimeters, a first
quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.

</details>


### [22] [Exploring the Frontiers of kNN Noisy Feature Detection and Recovery for Self-Driving Labs](https://arxiv.org/abs/2507.16833)
*Qiuyu Shi,Kangming Li,Yao Fehlis,Daniel Persaud,Robert Black,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 本研究开发了一种自动化工作流程来系统性地检测、校正和恢复自驱动实验室中的噪声特征，以提高材料发现的数据质量和实验精度。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室在材料发现中显示出前景，但输入参数捕获中的错误可能损坏用于建模系统性能的特征，从而影响当前和未来的实验活动。需要开发方法来系统性地处理这些噪声特征问题。

Method: 开发了一个自动化工作流程，能够系统性地检测噪声特征、确定可校正的样本-特征配对，并最终恢复正确的特征值。进行了系统性研究来检验数据集大小、噪声强度和特征值分布如何影响噪声特征的可检测性和可恢复性。

Result: 高强度噪声和大型训练数据集有利于噪声特征的检测和校正。低强度噪声会降低检测和恢复效果，但可以通过更大的清洁训练数据集来补偿。连续和分散特征分布比离散或窄分布特征显示出更好的可恢复性。该研究提供了材料数据集中kNN插值的具体基准。

Conclusion: 该研究展示了一个模型无关的框架，用于在存在噪声、有限数据和不同特征分布情况下进行合理的数据恢复，最终旨在提高自动化材料发现中的数据质量和实验精度。

Abstract: Self-driving laboratories (SDLs) have shown promise to accelerate materials
discovery by integrating machine learning with automated experimental
platforms. However, errors in the capture of input parameters may corrupt the
features used to model system performance, compromising current and future
campaigns. This study develops an automated workflow to systematically detect
noisy features, determine sample-feature pairings that can be corrected, and
finally recover the correct feature values. A systematic study is then
performed to examine how dataset size, noise intensity, and feature value
distribution affect both the detectability and recoverability of noisy
features. In general, high-intensity noise and large training datasets are
conducive to the detection and correction of noisy features. Low-intensity
noise reduces detection and recovery but can be compensated for by larger clean
training data sets. Detection and correction results vary between features with
continuous and dispersed feature distributions showing greater recoverability
compared to features with discrete or narrow distributions. This systematic
study not only demonstrates a model agnostic framework for rational data
recovery in the presence of noise, limited data, and differing feature
distributions but also provides a tangible benchmark of kNN imputation in
materials data sets. Ultimately, it aims to enhance data quality and
experimental precision in automated materials discovery.

</details>


### [23] [TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning](https://arxiv.org/abs/2507.16844)
*Jie He,Vincent Theo Willem Kenbeek,Zhantao Yang,Meixun Qu,Ezio Bartocci,Dejan Ničković,Radu Grosu*

Main category: cs.LG

TL;DR: 本文介绍了TD-Interpreter，一个基于多模态大语言模型的专业工具，帮助工程师理解和分析复杂的时序图，通过微调LLaVA模型实现视觉问答功能，并开发了合成数据生成工作流来解决训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 工程师在设计和验证过程中需要理解来自第三方的复杂时序图，这是一个具有挑战性的任务，需要专门的工具来辅助分析和回答相关的设计验证问题。

Method: 通过微调轻量级的7B多模态大语言模型LLaVA来实现TD-Interpreter，构建了一个视觉问答环境，并开发了合成数据生成工作流来将视觉信息与文本解释对齐，以解决训练数据有限的问题。

Result: 实验评估显示TD-Interpreter在评估基准上大幅超越了未调优的GPT-4o模型，证明了该工具的有效性和实用性。

Conclusion: TD-Interpreter成功地为工程师提供了一个专业的时序图理解工具，通过多模态学习和合成数据生成有效解决了复杂时序图分析的挑战，在性能上显著优于现有的通用模型。

Abstract: We introduce TD-Interpreter, a specialized ML tool that assists engineers in
understanding complex timing diagrams (TDs), originating from a third party,
during their design and verification process. TD-Interpreter is a visual
question-answer environment which allows engineers to input a set of TDs and
ask design and verification queries regarding these TDs. We implemented
TD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B
Multimodal Large Language Model (MLLM). To address limited training data
availability, we developed a synthetic data generation workflow that aligns
visual information with its textual interpretation. Our experimental evaluation
demonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o
by a large margin on the evaluated benchmarks.

</details>


### [24] [Reinforcement Learning in hyperbolic space for multi-step reasoning](https://arxiv.org/abs/2507.16864)
*Tao Xu,Dung-Yang Lee,Momiao Xiong*

Main category: cs.LG

TL;DR: 本文提出了一个将双曲变换器集成到强化学习中的新框架，用于多步推理任务。该方法利用双曲嵌入有效建模层次结构，在FrontierMath和非线性最优控制问题上相比普通变换器强化学习显著提升了准确性（32%-45%）并减少了计算时间（16%-32%）。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在处理复杂推理任务时面临信用分配、高维状态表示和稳定性等问题。多步推理是人工智能的基本挑战，需要新的解决方案来处理涉及层次结构的复杂推理任务。

Method: 提出了一个将双曲变换器集成到强化学习中的新框架。该方法利用双曲嵌入来有效建模层次结构，结合了变换器架构和双曲几何的最新进展来解决传统强化学习的挑战。

Result: 在FrontierMath基准测试中准确性提升32%-44%，计算时间减少16%-32%；在非线性最优控制基准测试中准确性提升43%-45%，计算时间减少16%-17%。相比使用普通变换器的强化学习方法，性能显著改善。

Conclusion: 双曲变换器在强化学习中具有巨大潜力，特别是在涉及层次结构的多步推理任务中。该框架成功解决了传统强化学习在复杂推理任务中的关键挑战，为多步推理问题提供了有效的解决方案。

Abstract: Multi-step reasoning is a fundamental challenge in artificial intelligence,
with applications ranging from mathematical problem-solving to decision-making
in dynamic environments. Reinforcement Learning (RL) has shown promise in
enabling agents to perform multi-step reasoning by optimizing long-term
rewards. However, conventional RL methods struggle with complex reasoning tasks
due to issues such as credit assignment, high-dimensional state
representations, and stability concerns. Recent advancements in Transformer
architectures and hyperbolic geometry have provided novel solutions to these
challenges. This paper introduces a new framework that integrates hyperbolic
Transformers into RL for multi-step reasoning. The proposed approach leverages
hyperbolic embeddings to model hierarchical structures effectively. We present
theoretical insights, algorithmic details, and experimental results that
include Frontier Math and nonlinear optimal control problems. Compared to RL
with vanilla transformer, the hyperbolic RL largely improves accuracy by
(32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control
benchmark, while achieving impressive reduction in computational time by
(16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control
benchmark. Our work demonstrates the potential of hyperbolic Transformers in
reinforcement learning, particularly for multi-step reasoning tasks that
involve hierarchical structures.

</details>


### [25] [Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization](https://arxiv.org/abs/2507.16867)
*Yunyi Zhao,Wei Zhang,Cheng Xiang,Hongyang Du,Dusit Niyato,Shuhua Gao*

Main category: cs.LG

TL;DR: 本文提出DiffCarl算法，将扩散模型与深度强化学习结合，用于多微电网系统的智能运行，在不确定环境下实现碳排放和风险感知的自适应能源调度


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源集成增长和系统复杂性增加，微电网社区在不确定性条件下的实时能源调度和优化面临重大挑战，需要一种能够同时考虑碳排放和运行风险的智能调度方法

Method: 将扩散模型集成到深度强化学习框架中，通过去噪生成过程学习动作分布，增强DRL策略表达能力，在动态不确定的微电网环境中实现碳排放和风险感知的调度

Result: 相比经典算法和最先进的DRL解决方案，运行成本降低2.3-30.1%；相比碳排放无感知变体，碳排放降低28.7%；同时减少了性能变异性

Conclusion: DiffCarl是一个实用且前瞻性的解决方案，其灵活设计允许高效适应不同系统配置和目标，支持在不断发展的能源系统中的实际部署

Abstract: This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware
reinforcement learning algorithm for intelligent operation of multi-microgrid
systems. With the growing integration of renewables and increasing system
complexity, microgrid communities face significant challenges in real-time
energy scheduling and optimization under uncertainty. DiffCarl integrates a
diffusion model into a deep reinforcement learning (DRL) framework to enable
adaptive energy scheduling under uncertainty and explicitly account for carbon
emissions and operational risk. By learning action distributions through a
denoising generation process, DiffCarl enhances DRL policy expressiveness and
enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid
environments. Extensive experimental studies demonstrate that it outperforms
classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower
operational cost. It also achieves 28.7% lower carbon emissions than those of
its carbon-unaware variant and reduces performance variability. These results
highlight DiffCarl as a practical and forward-looking solution. Its flexible
design allows efficient adaptation to different system configurations and
objectives to support real-world deployment in evolving energy systems.

</details>


### [26] [Navigation through Non-Compact Symmetric Spaces: a mathematical perspective on Cartan Neural Networks](https://arxiv.org/abs/2507.16871)
*Pietro Giuseppe Fré,Federico Milanesio,Guido Sanguinetti,Matteo Santoro*

Main category: cs.LG

TL;DR: 该论文基于非紧对称空间U/H开发了几何一致的神经网络理论，扩展了Cartan神经网络的数学结构，详细阐述了层的几何性质和层间映射的协变性，为构建完全几何可解释的神经网络理论奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络缺乏几何一致性和可解释性，需要利用群论结构和非紧对称空间的几何性质来开发具有几何可解释性的神经网络理论。

Method: 基于非紧对称空间U/H作为齐次流形，扩展Cartan神经网络的数学结构，详细分析层的几何性质以及层间映射如何与这些结构相互作用以实现协变性。

Result: 成功扩展了Cartan神经网络的数学理论基础，使神经网络具有协变性和几何可解释性，验证了这些几何概念在机器学习中的可行性和性能。

Conclusion: 该工作与其姊妹论文共同构成了利用群论结构开发完全几何可解释神经网络理论的重要第一步，为未来的几何神经网络发展奠定了坚实的数学基础。

Abstract: Recent work has identified non-compact symmetric spaces U/H as a promising
class of homogeneous manifolds to develop a geometrically consistent theory of
neural networks. An initial implementation of these concepts has been presented
in a twin paper under the moniker of Cartan Neural Networks, showing both the
feasibility and the performance of these geometric concepts in a machine
learning context. The current paper expands on the mathematical structures
underpinning Cartan Neural Networks, detailing the geometric properties of the
layers and how the maps between layers interact with such structures to make
Cartan Neural Networks covariant and geometrically interpretable. Together,
these twin papers constitute a first step towards a fully geometrically
interpretable theory of neural networks exploiting group-theoretic structures

</details>


### [27] [Confidence Optimization for Probabilistic Encoding](https://arxiv.org/abs/2507.16881)
*Pengjiu Xia,Yidian Huang,Wenchao Wei,Yuwen Tan*

Main category: cs.LG

TL;DR: 提出了一种置信度优化概率编码(CPE)方法，通过置信度感知机制和L2正则化来改善概率编码中的距离可靠性问题，在BERT和RoBERTa模型上显著提升了自然语言分类任务的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 概率编码虽然能够增强神经网络的泛化能力，但其引入的高斯噪声会扭曲分类任务中基于点的距离测量，影响分类性能。现有的KL散度方差正则化方法依赖不可靠的先验假设，需要更好的解决方案。

Method: 提出置信度优化概率编码(CPE)方法，包含两个关键策略：1)引入置信度感知机制来调整距离计算，确保概率编码分类任务的一致性和可靠性；2)用简单的L2正则化项替代传统的基于KL散度的方差正则化，直接约束方差而不依赖不可靠的先验假设。

Result: 在自然语言分类任务上进行广泛实验，结果表明该方法在BERT和RoBERTa模型上都显著提升了性能和泛化能力。该方法具有模型无关性，可以广泛应用。

Conclusion: CPE方法有效解决了概率编码中距离可靠性问题，通过置信度感知机制和L2正则化显著改善了神经网络在分类任务中的表现，为概率编码在自然语言处理中的应用提供了更可靠的解决方案。

Abstract: Probabilistic encoding introduces Gaussian noise into neural networks,
enabling a smooth transition from deterministic to uncertain states and
enhancing generalization ability. However, the randomness of Gaussian noise
distorts point-based distance measurements in classification tasks. To mitigate
this issue, we propose a confidence optimization probabilistic encoding (CPE)
method that improves distance reliability and enhances representation learning.
Specifically, we refine probabilistic encoding with two key strategies: First,
we introduce a confidence-aware mechanism to adjust distance calculations,
ensuring consistency and reliability in probabilistic encoding classification
tasks. Second, we replace the conventional KL divergence-based variance
regularization, which relies on unreliable prior assumptions, with a simpler L2
regularization term to directly constrain variance. The method we proposed is
model-agnostic, and extensive experiments on natural language classification
tasks demonstrate that our method significantly improves performance and
generalization on both the BERT and the RoBERTa model.

</details>


### [28] [SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling](https://arxiv.org/abs/2507.16884)
*Yi Guo,Wei Wang,Zhihang Yuan,Rong Cao,Kuan Chen,Zhengyang Chen,Yuanyuan Huo,Yang Zhang,Yuping Wang,Shouda Liu,Yuxuan Wang*

Main category: cs.LG

TL;DR: 本文提出SplitMeanFlow，一种基于区间分割一致性的新训练框架，用于学习平均速度场以实现快速生成，在保持性能的同时显著提升训练效率并已在大规模语音合成产品中部署。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型如Flow Matching虽然性能优异，但受限于计算昂贵的迭代采样过程。虽然MeanFlow等方法通过学习平均速度场实现了少步或单步生成，但其基于微分恒等式的方法在理论和实践上都存在局限性，需要更根本和高效的解决方案。

Method: 回归平均速度的第一性原理，利用定积分的可加性质，推导出名为"区间分割一致性"的纯代数恒等式。该恒等式为不同时间间隔上的平均速度场建立了自参考关系，无需使用微分算子。基于此原理构建SplitMeanFlow训练框架，直接将代数一致性作为学习目标。

Result: 理论上证明了MeanFlow的微分恒等式是本方法代数一致性在区间分割趋于无穷小时的极限情况，确立了SplitMeanFlow作为学习平均速度场更一般化基础的地位。实践上，该代数方法消除了JVP计算需求，实现了更简单的实现、更稳定的训练和更广泛的硬件兼容性。单步和双步SplitMeanFlow模型已在大规模语音合成产品中成功部署。

Conclusion: SplitMeanFlow提供了一个更根本、更高效的框架来学习平均速度场，不仅在理论上统一了现有方法，还在实践中带来了显著的计算和实现优势，并已在实际产品中实现20倍加速，证明了其在大规模应用中的有效性。

Abstract: Generative models like Flow Matching have achieved state-of-the-art
performance but are often hindered by a computationally expensive iterative
sampling process. To address this, recent work has focused on few-step or
one-step generation by learning the average velocity field, which directly maps
noise to data. MeanFlow, a leading method in this area, learns this field by
enforcing a differential identity that connects the average and instantaneous
velocities. In this work, we argue that this differential formulation is a
limiting special case of a more fundamental principle. We return to the first
principles of average velocity and leverage the additivity property of definite
integrals. This leads us to derive a novel, purely algebraic identity we term
Interval Splitting Consistency. This identity establishes a self-referential
relationship for the average velocity field across different time intervals
without resorting to any differential operators. Based on this principle, we
introduce SplitMeanFlow, a new training framework that enforces this algebraic
consistency directly as a learning objective. We formally prove that the
differential identity at the core of MeanFlow is recovered by taking the limit
of our algebraic consistency as the interval split becomes infinitesimal. This
establishes SplitMeanFlow as a direct and more general foundation for learning
average velocity fields. From a practical standpoint, our algebraic approach is
significantly more efficient, as it eliminates the need for JVP computations,
resulting in simpler implementation, more stable training, and broader hardware
compatibility. One-step and two-step SplitMeanFlow models have been
successfully deployed in large-scale speech synthesis products (such as
Doubao), achieving speedups of 20x.

</details>


### [29] [SiLQ: Simple Large Language Model Quantization-Aware Training](https://arxiv.org/abs/2507.16933)
*Steven K. Esser,Jeffrey L. McKinstry,Deepika Bablani,Rathinakumar Appuswamy,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 本文提出了一种简单有效的量化感知训练方法，仅增加不到0.1%的训练成本就能在多个现代基准测试中大幅超越现有量化方法，同时与推理加速器兼容。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型量化方法存在准确性损失大、训练时间长、与专用推理加速器不兼容等问题，需要一种简单高效且兼容性好的量化方法来降低推理延迟、模型大小和能耗。

Method: 提出了一种端到端的量化感知训练方法，该方法简单易用，可泛化到不同模型架构，适用于激活值、缓存和权重的量化，除量化操作本身外不引入额外操作。

Result: 在多个现代基准测试中，该方法在基础模型和指令模型变体上都大幅超越了现有领先的量化方法，同时训练成本增加不到0.1%。

Conclusion: 该量化感知训练方法成功实现了在极低额外训练成本下显著提升量化模型性能的目标，具有良好的通用性和实用性，为大语言模型的高效部署提供了有效解决方案。

Abstract: Large language models can be quantized to reduce inference time latency,
model size, and energy consumption, thereby delivering a better user experience
at lower cost. A challenge exists to deliver quantized models with minimal loss
of accuracy in reasonable time, and in particular to do so without requiring
mechanisms incompatible with specialized inference accelerators. Here, we
demonstrate a simple, end-to-end quantization-aware training approach that,
with an increase in total model training budget of less than 0.1%, outperforms
the leading published quantization methods by large margins on several modern
benchmarks, with both base and instruct model variants. The approach easily
generalizes across different model architectures, can be applied to
activations, cache, and weights, and requires the introduction of no additional
operations to the model other than the quantization itself.

</details>


### [30] [Hierarchical Reinforcement Learning Framework for Adaptive Walking Control Using General Value Functions of Lower-Limb Sensor Signals](https://arxiv.org/abs/2507.16983)
*Sonny T. Jones,Grange M. Simpson,Patrick M. Pilarski,Ashley N. Dalrymple*

Main category: cs.LG

TL;DR: 本研究采用分层强化学习(HRL)开发下肢外骨骼的自适应控制策略，通过结合生物传感器数据和预测信息，提高了外骨骼在不同地形上的行走决策准确性。


<details>
  <summary>Details</summary>
Motivation: 康复技术为研究人机协作学习和决策提供了自然环境。现有下肢外骨骼控制系统在复杂地形适应性方面存在不足，需要开发能够增强运动障碍患者移动性和自主性的自适应控制策略。

Method: 采用分层强化学习方法，将外骨骼控制适应任务分解为高层地形策略适应框架和低层预测信息提供框架。通过持续学习一般价值函数(GVFs)实现预测功能，利用肌电图、压力鞋垫和角度计等多种可穿戴传感器数据，研究了两种将实际和预测传感器信号融入策略网络的方法。

Result: 添加GVFs预测信息显著提高了网络整体准确性。在平地、不平地面、上下坡道和转弯等地形上都观察到了特定地形性能的提升，这些地形在没有预测信息时经常被误分类。预测信息在不确定性情况下能够辅助决策制定。

Conclusion: 预测信息能够在地形误分类概率较高的不确定情况下辅助决策制定。该研究为分层强化学习的细节理解和未来外骨骼发展提供了新见解，有助于实现在不同行走环境中的安全过渡和穿越。

Abstract: Rehabilitation technology is a natural setting to study the shared learning
and decision-making of human and machine agents. In this work, we explore the
use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control
strategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy
for individuals with motor impairments. Inspired by prominent models of
biological sensorimotor processing, our investigated HRL approach breaks down
the complex task of exoskeleton control adaptation into a higher-level
framework for terrain strategy adaptation and a lower-level framework for
providing predictive information; this latter element is implemented via the
continual learning of general value functions (GVFs). GVFs generated temporal
abstractions of future signal values from multiple wearable lower-limb sensors,
including electromyography, pressure insoles, and goniometers. We investigated
two methods for incorporating actual and predicted sensor signals into a policy
network with the intent to improve the decision-making capacity of the control
system of a lower-limb exoskeleton during ambulation across varied terrains. As
a key result, we found that the addition of predictions made from GVFs
increased overall network accuracy. Terrain-specific performance increases were
seen while walking on even ground, uneven ground, up and down ramps, and turns,
terrains that are often misclassified without predictive information. This
suggests that predictive information can aid decision-making during
uncertainty, e.g., on terrains that have a high chance of being misclassified.
This work, therefore, contributes new insights into the nuances of HRL and the
future development of exoskeletons to facilitate safe transitioning and
traversing across different walking environments.

</details>


### [31] [Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation](https://arxiv.org/abs/2507.17066)
*Jessup Byun,Xiaofeng Lin,Joshua Ward,Guang Cheng*

Main category: cs.LG

TL;DR: 本研究首次系统评估了基础模型在低数据量表格合成中的隐私风险，发现大型预训练模型存在严重的成员推理攻击风险，并提出了零成本的提示优化策略来改善隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在小数据集上容易过拟合和泄露敏感信息，而新兴的基础模型通过上下文学习进行表格合成时会逐字重复种子行，在表格数据中引入新的隐私风险，但这种风险的严重程度尚未得到充分研究。

Method: 构建首个基准测试，使用3个基础模型（GPT-4o-mini、LLaMA 3.3 70B、TabPFN v2）与4个基线方法在35个来自健康、金融和政策领域的真实表格上进行比较，评估统计保真度、下游效用和成员推理泄露风险，并通过因子研究探索零成本提示优化策略。

Result: 基础模型的隐私风险最高，LLaMA 3.3 70B在1%假阳性率下的真阳性率比最安全基线高出54个百分点；CTGAN和GPT-4o-mini提供更好的隐私-效用权衡；三种零成本提示调整（小批量、低温度、使用汇总统计）可将最坏情况AUC降低14个点，稀有类泄露降低39个点，同时保持90%以上的保真度。

Conclusion: 基础模型在表格合成中存在显著隐私风险，但通过适当的提示工程策略可以在保持高保真度的同时有效降低隐私泄露，为低数据量场景下的安全表格合成提供了实用指导。

Abstract: Synthetic tabular data is essential for machine learning workflows,
especially for expanding small or imbalanced datasets and enabling
privacy-preserving data sharing. However, state-of-the-art generative models
(GANs, VAEs, diffusion models) rely on large datasets with thousands of
examples. In low-data settings, often the primary motivation for synthetic
data, these models can overfit, leak sensitive records, and require frequent
retraining. Recent work uses large pre-trained transformers to generate rows
via in-context learning (ICL), which needs only a few seed examples and no
parameter updates, avoiding retraining. But ICL repeats seed rows verbatim,
introducing a new privacy risk that has only been studied in text. The severity
of this risk in tabular synthesis-where a single row may identify a
person-remains unclear. We address this gap with the first benchmark of three
foundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four
baselines on 35 real-world tables from health, finance, and policy. We evaluate
statistical fidelity, downstream utility, and membership inference leakage.
Results show foundation models consistently have the highest privacy risk.
LLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at
1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly
vulnerable. We plot the privacy-utility frontier and show that CTGAN and
GPT-4o-mini offer better tradeoffs. A factorial study finds that three
zero-cost prompt tweaks-small batch size, low temperature, and using summary
statistics-can reduce worst-case AUC by 14 points and rare-class leakage by up
to 39 points while maintaining over 90% fidelity. Our benchmark offers a
practical guide for safer low-data synthesis with foundation models.

</details>


### [32] [PyG 2.0: Scalable Learning on Real World Graphs](https://arxiv.org/abs/2507.16991)
*Matthias Fey,Jinu Sunil,Akihiro Nitta,Rishi Puri,Manan Shah,Blaž Stojanovič,Ramona Bendias,Alexandria Barghi,Vid Kocijan,Zecheng Zhang,Xinwei He,Jan Eric Lenssen,Jure Leskovec*

Main category: cs.LG

TL;DR: PyG 2.0是PyTorch Geometric的重大更新版本，显著提升了图神经网络框架的可扩展性和实际应用能力，支持异质图、时序图等复杂场景，并在关系深度学习和大语言模型等领域得到广泛应用。


<details>
  <summary>Details</summary>
Motivation: 随着图神经网络应用的不断扩展，需要一个更强大、更可扩展的框架来处理大规模图学习问题，特别是在异质图、时序图以及实际应用场景中的复杂需求。

Method: 通过架构增强实现PyG 2.0的全面升级，包括：支持异质图和时序图、可扩展的特征/图存储系统、各种性能优化技术，以及针对关系深度学习和大语言模型的专门支持。

Result: PyG 2.0成功支持了大规模图学习问题的高效处理，在多个应用领域得到验证，特别是在关系深度学习和大语言模型领域展现出强大的应用能力。

Conclusion: PyG 2.0通过全面的架构改进和功能扩展，巩固了其作为领先图神经网络框架的地位，为研究人员和实践者提供了处理复杂、大规模图学习任务的强大工具。

Abstract: PyG (PyTorch Geometric) has evolved significantly since its initial release,
establishing itself as a leading framework for Graph Neural Networks. In this
paper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive
update that introduces substantial improvements in scalability and real-world
application capabilities. We detail the framework's enhanced architecture,
including support for heterogeneous and temporal graphs, scalable feature/graph
stores, and various optimizations, enabling researchers and practitioners to
tackle large-scale graph learning problems efficiently. Over the recent years,
PyG has been supporting graph learning in a large variety of application areas,
which we will summarize, while providing a deep dive into the important areas
of relational deep learning and large language modeling.

</details>


### [33] [Federated Majorize-Minimization: Beyond Parameter Aggregation](https://arxiv.org/abs/2507.17534)
*Aymeric Dieuleveut,Gersende Fort,Mahmoud Hegazy,Hoi-To Wai*

Main category: cs.LG

TL;DR: 本文提出了一个统一的随机优化算法框架，通过Majorize-Minimization方法解决联邦学习中的挑战，设计了SSMM和QSMM算法来处理数据异构性、部分参与和通信约束等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习算法在面对数据异构性、部分参与和通信约束等瓶颈时缺乏统一的理论框架，需要一种能够鲁棒扩展到联邦学习环境的随机优化算法设计方法。

Method: 研究了一类具有线性参数化majorizing代理函数族的Majorize-Minimization问题，提出了统一算法SSMM（随机近似随机代理MM），然后将其扩展到联邦设置中形成QSMM算法。QSMM的创新在于学习并聚合表征代理majorizing函数的信息，而非传统的原始参数聚合。

Result: 框架涵盖了多种算法包括梯度算法、期望最大化算法等，SSMM算法包含了之前的随机MM过程作为特例，成功扩展到联邦学习环境并解决了常见瓶颈问题，还展示了在联邦环境下计算最优传输映射的应用。

Conclusion: 提出的统一框架能够有效设计适用于联邦学习的随机优化算法，QSMM算法通过聚合代理函数信息而非原始参数的创新方法，成功解决了联邦学习中的关键挑战，并展现了超越理论设置的应用灵活性。

Abstract: This paper proposes a unified approach for designing stochastic optimization
algorithms that robustly scale to the federated learning setting. Our work
studies a class of Majorize-Minimization (MM) problems, which possesses a
linearly parameterized family of majorizing surrogate functions. This framework
encompasses (proximal) gradient-based algorithms for (regularized) smooth
objectives, the Expectation Maximization algorithm, and many problems seen as
variational surrogate MM. We show that our framework motivates a unifying
algorithm called Stochastic Approximation Stochastic Surrogate MM (\SSMM),
which includes previous stochastic MM procedures as special instances. We then
extend \SSMM\ to the federated setting, while taking into consideration common
bottlenecks such as data heterogeneity, partial participation, and
communication constraints; this yields \QSMM. The originality of \QSMM\ is to
learn locally and then aggregate information characterizing the
\textit{surrogate majorizing function}, contrary to classical algorithms which
learn and aggregate the \textit{original parameter}. Finally, to showcase the
flexibility of this methodology beyond our theoretical setting, we use it to
design an algorithm for computing optimal transport maps in the federated
setting.

</details>


### [34] [Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation](https://arxiv.org/abs/2507.17001)
*Yan Li,Guangyi Chen,Yunlong Deng,Zijian Li,Zeyu Tang,Anpeng Wu,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一个新的框架，通过战略性地利用偏差来补充不变表示，在域外分布适应任务中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的域外分布适应方法主要依赖不变表示学习来消除偏差特征的影响，但这引发了一个重要问题：偏差是否总是应该被消除？如果不是，何时应该保留偏差，以及如何有效利用偏差？

Method: 提出了一个新颖的框架，包含两个核心组件来直接和间接地利用偏差：(1)使用不变性作为指导从偏差中提取预测成分；(2)利用识别出的偏差来估计环境条件，然后使用它来探索合适的偏差感知预测器以缓解环境差距。

Result: 在合成数据集和标准域泛化基准上的实验验证表明，该方法始终优于现有方法，展现了其鲁棒性和适应性。

Conclusion: 通过理论分析和实验验证，证明了在特定条件下偏差特征可以被识别和有效利用，战略性地利用偏差来补充不变表示可以显著提升域外分布适应的性能。

Abstract: Most existing methods for adapting models to out-of-distribution (OOD)
domains rely on invariant representation learning to eliminate the influence of
biased features. However, should bias always be eliminated -- and if not, when
should it be retained, and how can it be leveraged? To address these questions,
we first present a theoretical analysis that explores the conditions under
which biased features can be identified and effectively utilized. Building on
this theoretical foundation, we introduce a novel framework that strategically
leverages bias to complement invariant representations during inference. The
framework comprises two key components that leverage bias in both direct and
indirect ways: (1) using invariance as guidance to extract predictive
ingredients from bias, and (2) exploiting identified bias to estimate the
environmental condition and then use it to explore appropriate bias-aware
predictors to alleviate environment gaps. We validate our approach through
experiments on both synthetic datasets and standard domain generalization
benchmarks. Results consistently demonstrate that our method outperforms
existing approaches, underscoring its robustness and adaptability.

</details>


### [35] [Generalized Dual Discriminator GANs](https://arxiv.org/abs/2507.17684)
*Penukonda Naga Chandana,Tejas Srivastava,Gowtham R. Kurri,V. Lalitha*

Main category: cs.LG

TL;DR: 本文提出了双判别器α-GANs(D2α-GANs)和广义双判别器GANs，通过引入可调节的α损失函数和任意正实数函数来改进传统双判别器GANs，理论上证明了优化目标可简化为f-散度和逆f-散度的线性组合


<details>
  <summary>Details</summary>
Motivation: 传统GANs存在模式崩塌问题，虽然双判别器GANs(D2-GANs)能够缓解这一问题，但仍需要更灵活的损失函数设计和更广泛的理论框架来进一步提升生成对抗网络的性能

Method: 提出双判别器α-GANs，结合双判别器结构与可调节的α损失函数；进一步推广到基于正实数上任意函数的广义双判别器GANs；使用两个判别器：一个奖励真实数据分布的样本，另一个偏向生成器样本

Result: 理论分析表明所提出模型的min-max优化可简化为f-散度和逆f-散度的线性组合最小化问题，推广了D2-GANs中KL散度和逆KL散度线性组合的已知简化；在2D合成数据上的实验验证了方法的多重优势

Conclusion: 双判别器α-GANs和广义双判别器GANs成功结合了双判别器的优势与损失函数的灵活性，提供了更广泛的理论框架，能够有效缓解模式崩塌问题并在多个性能指标上表现出优势

Abstract: Dual discriminator generative adversarial networks (D2 GANs) were introduced
to mitigate the problem of mode collapse in generative adversarial networks. In
D2 GANs, two discriminators are employed alongside a generator: one
discriminator rewards high scores for samples from the true data distribution,
while the other favors samples from the generator. In this work, we first
introduce dual discriminator $\alpha$-GANs (D2 $\alpha$-GANs), which combines
the strengths of dual discriminators with the flexibility of a tunable loss
function, $\alpha$-loss. We further generalize this approach to arbitrary
functions defined on positive reals, leading to a broader class of models we
refer to as generalized dual discriminator generative adversarial networks. For
each of these proposed models, we provide theoretical analysis and show that
the associated min-max optimization reduces to the minimization of a linear
combination of an $f$-divergence and a reverse $f$-divergence. This generalizes
the known simplification for D2-GANs, where the objective reduces to a linear
combination of the KL-divergence and the reverse KL-divergence. Finally, we
perform experiments on 2D synthetic data and use multiple performance metrics
to capture various advantages of our GANs.

</details>


### [36] [laplax -- Laplace Approximations with JAX](https://arxiv.org/abs/2507.17013)
*Tobias Weber,Bálint Mucsányi,Lenard Rommel,Thomas Christie,Lars Kasüschke,Marvin Pförtner,Philipp Hennig*

Main category: cs.LG

TL;DR: 本文介绍了laplax，一个基于JAX的开源Python包，用于在深度神经网络中执行拉普拉斯近似，以实现贝叶斯不确定性量化和模型选择。


<details>
  <summary>Details</summary>
Motivation: 拉普拉斯近似为深度神经网络中的权重空间不确定性量化提供了可扩展且高效的方法，能够应用贝叶斯工具进行预测不确定性分析和模型选择，但缺乏一个模块化、功能纯粹且研究友好的实现框架。

Method: 开发了laplax这一开源Python包，采用模块化和纯函数式架构设计，基于JAX框架实现，具有最小的外部依赖，为拉普拉斯近似提供灵活的实现框架。

Result: 成功构建了一个研究友好的框架，支持快速原型开发和实验，能够有效促进贝叶斯神经网络、深度学习不确定性量化以及改进拉普拉斯近似技术的研究。

Conclusion: laplax为贝叶斯神经网络和不确定性量化研究提供了一个高效、灵活的工具，有助于推动拉普拉斯近似技术的发展和应用。

Abstract: The Laplace approximation provides a scalable and efficient means of
quantifying weight-space uncertainty in deep neural networks, enabling the
application of Bayesian tools such as predictive uncertainty and model
selection via Occam's razor. In this work, we introduce laplax, a new
open-source Python package for performing Laplace approximations with jax.
Designed with a modular and purely functional architecture and minimal external
dependencies, laplax offers a flexible and researcher-friendly framework for
rapid prototyping and experimentation. Its goal is to facilitate research on
Bayesian neural networks, uncertainty quantification for deep learning, and the
development of improved Laplace approximation techniques.

</details>


### [37] [On the Interaction of Compressibility and Adversarial Robustness](https://arxiv.org/abs/2507.17725)
*Melih Barsbey,Antônio H. Ribeiro,Umut Şimşekli,Tolga Birdal*

Main category: cs.LG

TL;DR: 本文研究了神经网络压缩性与对抗鲁棒性之间的关系，发现压缩会在表示空间中产生高敏感方向，使得对抗攻击更容易成功，揭示了效率与安全性之间的根本性矛盾。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络需要同时满足准确性、泛化性、效率和鲁棒性等多种要求，但压缩性和鲁棒性之间的相互作用机制尚不清楚，缺乏统一的理论框架来理解这种关系。

Method: 开发了一个原则性框架来分析不同形式的压缩性（如神经元级稀疏性和谱压缩性）如何影响对抗鲁棒性，通过理论分析得出鲁棒性界限，并结合合成和真实任务的实证评估验证理论预测。

Result: 发现压缩会在表示空间中诱导少数高敏感方向，对手可以利用这些方向构造有效扰动；得出了揭示神经元和谱压缩性如何通过学习表示影响L∞和L2鲁棒性的简单鲁棒性界限；证实了这些脆弱性在对抗训练和迁移学习下仍然存在。

Conclusion: 结构化压缩性与鲁棒性之间存在根本性矛盾，无论通过何种方式实现压缩都会产生脆弱性，这为设计既高效又安全的模型提供了新的思路和方向。

Abstract: Modern neural networks are expected to simultaneously satisfy a host of
desirable properties: accurate fitting to training data, generalization to
unseen inputs, parameter and computational efficiency, and robustness to
adversarial perturbations. While compressibility and robustness have each been
studied extensively, a unified understanding of their interaction still remains
elusive. In this work, we develop a principled framework to analyze how
different forms of compressibility - such as neuron-level sparsity and spectral
compressibility - affect adversarial robustness. We show that these forms of
compression can induce a small number of highly sensitive directions in the
representation space, which adversaries can exploit to construct effective
perturbations. Our analysis yields a simple yet instructive robustness bound,
revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$
robustness via their effects on the learned representations. Crucially, the
vulnerabilities we identify arise irrespective of how compression is achieved -
whether via regularization, architectural bias, or implicit learning dynamics.
Through empirical evaluations across synthetic and realistic tasks, we confirm
our theoretical predictions, and further demonstrate that these vulnerabilities
persist under adversarial training and transfer learning, and contribute to the
emergence of universal adversarial perturbations. Our findings show a
fundamental tension between structured compressibility and robustness, and
suggest new pathways for designing models that are both efficient and secure.

</details>


### [38] [Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting](https://arxiv.org/abs/2507.17016)
*Omid Orang,Patricia O. Lucas,Gabriel I. F. Paiva,Petronio C. L. Silva,Felipe Augusto Rocha da Silva,Adriano Alonso Veloso,Frederico Gadelha Guimaraes*

Main category: cs.LG

TL;DR: 本研究提出了CGF-LLM框架，首次将GPT-2与模糊时间序列和因果图相结合，用于多变量时间序列预测，通过模糊化和因果分析将数值时间序列转换为可解释的文本表示。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在时间序列预测领域的应用引起了研究者的广泛关注，但现有方法在将数值时间序列转换为语言模型可理解的形式方面存在挑战，需要提高预测的可解释性和语义理解能力。

Method: 提出CGF-LLM框架，结合GPT-2、模糊时间序列(FTS)和因果图技术。通过并行应用模糊化和因果分析，将数值时间序列转换为可解释的文本表示，使预训练的GPT-2模型能够进行语义理解和结构洞察。

Result: 在四个不同的多变量时间序列数据集上验证了所提出模型的有效性，生成的文本表示能够更好地展现原始时间序列复杂动态的可解释视图。

Conclusion: CGF-LLM是首个将GPT-2与模糊时间序列和因果图结合的架构，成功实现了多变量时间序列预测，为基于模糊时间序列的大语言模型时间序列预测领域开辟了有前景的未来发展方向。

Abstract: In recent years, the application of Large Language Models (LLMs) to time
series forecasting (TSF) has garnered significant attention among researchers.
This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with
fuzzy time series (FTS) and causal graph to predict multivariate time series,
marking the first such architecture in the literature. The key objective is to
convert numerical time series into interpretable forms through the parallel
application of fuzzification and causal analysis, enabling both semantic
understanding and structural insight as input for the pretrained GPT-2 model.
The resulting textual representation offers a more interpretable view of the
complex dynamics underlying the original time series. The reported results
confirm the effectiveness of our proposed LLM-based time series forecasting
model, as demonstrated across four different multivariate time series datasets.
This initiative paves promising future directions in the domain of TSF using
LLMs based on FTS.

</details>


### [39] [Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility](https://arxiv.org/abs/2507.17748)
*Melih Barsbey,Lucas Prieto,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.LG

TL;DR: 本文提出使用大学习率可以同时实现模型对虚假相关性的鲁棒性和网络压缩性，并发现大学习率能产生不变特征利用、类别分离和激活稀疏等理想的表示属性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型需要同时具备鲁棒性和资源效率，但同时实现这两个属性仍然是一个挑战。研究者希望找到一种方法能够同时提升模型对虚假相关性的鲁棒性和网络的可压缩性。

Method: 将大学习率作为同时实现鲁棒性和可压缩性的促进因子。通过在多种虚假相关数据集、模型和优化器上进行实验，分析大学习率对表示学习的影响，包括不变特征利用、类别分离和激活稀疏等属性的评估。

Result: 实验结果表明，大学习率能够产生理想的表示属性，包括不变特征利用、类别分离和激活稀疏。与其他超参数和正则化方法相比，大学习率在同时满足这些属性方面表现更优。在多种虚假相关数据集上都证明了大学习率的积极效果。

Conclusion: 大学习率是一个有效的工具，能够同时提升模型的鲁棒性和资源效率。之前文献中大学习率在标准分类任务中的成功很可能是由于其解决了训练数据集中隐藏或罕见的虚假相关性问题。这为理解大学习率的作用机制提供了新的视角。

Abstract: Robustness and resource-efficiency are two highly desirable properties for
modern machine learning models. However, achieving them jointly remains a
challenge. In this paper, we position high learning rates as a facilitator for
simultaneously achieving robustness to spurious correlations and network
compressibility. We demonstrate that large learning rates also produce
desirable representation properties such as invariant feature utilization,
class separation, and activation sparsity. Importantly, our findings indicate
that large learning rates compare favorably to other hyperparameters and
regularization methods, in consistently satisfying these properties in tandem.
In addition to demonstrating the positive effect of large learning rates across
diverse spurious correlation datasets, models, and optimizers, we also present
strong evidence that the previously documented success of large learning rates
in standard classification tasks is likely due to its effect on addressing
hidden/rare spurious correlations in the training dataset.

</details>


### [40] [BiLO: Bilevel Local Operator Learning for PDE Inverse Problems. Part II: Efficient Uncertainty Quantification with Low-Rank Adaptation](https://arxiv.org/abs/2507.17019)
*Ray Zirui Zhang,Christopher E. Miles,Xiaohui Xie,John S. Lowengrub*

Main category: cs.LG

TL;DR: 本文提出了一种基于双层局部算子学习(BiLO)的贝叶斯推理框架，用于求解偏微分方程约束的不确定性量化和逆问题，通过梯度MCMC方法和低秩适应实现高效采样。


<details>
  <summary>Details</summary>
Motivation: 现有的基于贝叶斯神经网络的方法在处理PDE约束优化问题时面临高维神经网络权重空间采样困难的挑战，且需要为神经网络解指定先验分布，而传统方法在参数推理和不确定性量化的精度方面存在不足。

Method: 将双层局部算子学习扩展到贝叶斯推理框架：下层通过最小化局部算子损失训练网络近似局部解算子；上层从后验分布中采样PDE参数，使用基于梯度的马尔科夫链蒙特卡罗(MCMC)方法和低秩适应(LoRA)实现高效采样，通过强PDE约束让不确定性从数据自然传播。

Result: 方法绕过了高维神经网络权重空间的采样挑战，无需为神经网络解指定先验分布，提高了参数推理和不确定性量化的精度。理论分析揭示了MCMC采样器梯度的动态误差和下层问题求解容差与不确定性量化精度的直接联系。数值实验验证了方法在多种PDE模型上的准确性和计算效率。

Conclusion: 所提出的基于BiLO的贝叶斯推理方法在保持高计算效率的同时，能够准确地进行推理和不确定性量化，为PDE约束的不确定性量化和逆问题提供了一种有效的解决方案。

Abstract: Uncertainty quantification and inverse problems governed by partial
differential equations (PDEs) are central to a wide range of scientific and
engineering applications. In this second part of a two part series, we extend
Bilevel Local Operator Learning (BiLO) for PDE-constrained optimization
problems developed in Part 1 to the Bayesian inference framework. At the lower
level, we train a network to approximate the local solution operator by
minimizing the local operator loss with respect to the weights of the neural
network. At the upper level, we sample the PDE parameters from the posterior
distribution. We achieve efficient sampling through gradient-based Markov Chain
Monte Carlo (MCMC) methods and low-rank adaptation (LoRA). Compared with
existing methods based on Bayesian neural networks, our approach bypasses the
challenge of sampling in the high-dimensional space of neural network weights
and does not require specifying a prior distribution on the neural network
solution. Instead, uncertainty propagates naturally from the data through the
PDE constraints. By enforcing strong PDE constraints, the proposed method
improves the accuracy of both parameter inference and uncertainty
quantification. We analyze the dynamic error of the gradient in the MCMC
sampler and the static error in the posterior distribution due to inexact
minimization of the lower level problem and demonstrate a direct link between
the tolerance for solving the lower level problem and the accuracy of the
resulting uncertainty quantification. Through numerical experiments across a
variety of PDE models, we demonstrate that our method delivers accurate
inference and quantification of uncertainties while maintaining high
computational efficiency.

</details>


### [41] [Pragmatic Policy Development via Interpretable Behavior Cloning](https://arxiv.org/abs/2507.17056)
*Anton Matsson,Yaochen Rao,Heather J. Litman,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 该论文提出了一种基于树结构的可解释离线强化学习方法，通过分析数据中最频繁的治疗行为来制定医疗决策策略，在类风湿关节炎和败血症治疗中展现出优于当前实践的效果。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在安全关键领域（如医疗）的应用面临两大挑战：1）黑盒策略缺乏可解释性；2）离策略评估对行为策略偏差敏感，特别是基于重要性采样的方法存在评估困难。

Method: 提出基于树结构模型的治疗策略制定方法：1）使用可解释的树模型估计行为策略；2）从每个患者状态中最频繁选择的行为中导出治疗策略；3）通过控制考虑的行为数量来调节与行为策略的重叠度，确保可靠的离策略评估；4）利用树结构的天然分组特性实现状态-治疗的可解释映射。

Result: 在类风湿关节炎和败血症的真实世界数据中验证，该方法制定的策略能够超越当前临床实践的表现，同时保持良好的可解释性，为离线强化学习提供了实用的替代方案。

Conclusion: 该框架通过标准化频繁的治疗模式，捕获了数据中蕴含的集体临床判断，为安全关键域的决策制定提供了一种实用且可解释的方法，克服了传统离线强化学习在医疗应用中的关键障碍。

Abstract: Offline reinforcement learning (RL) holds great promise for deriving optimal
policies from observational data, but challenges related to interpretability
and evaluation limit its practical use in safety-critical domains.
Interpretability is hindered by the black-box nature of unconstrained RL
policies, while evaluation -- typically performed off-policy -- is sensitive to
large deviations from the data-collecting behavior policy, especially when
using methods based on importance sampling. To address these challenges, we
propose a simple yet practical alternative: deriving treatment policies from
the most frequently chosen actions in each patient state, as estimated by an
interpretable model of the behavior policy. By using a tree-based model, which
is specifically designed to exploit patterns in the data, we obtain a natural
grouping of states with respect to treatment. The tree structure ensures
interpretability by design, while varying the number of actions considered
controls the degree of overlap with the behavior policy, enabling reliable
off-policy evaluation. This pragmatic approach to policy development
standardizes frequent treatment patterns, capturing the collective clinical
judgment embedded in the data. Using real-world examples in rheumatoid
arthritis and sepsis care, we demonstrate that policies derived under this
framework can outperform current practice, offering interpretable alternatives
to those obtained via offline RL.

</details>


### [42] [Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach](https://arxiv.org/abs/2507.17070)
*Adithya Mohan,Dominik Rößle,Daniel Cremers,Torsten Schön*

Main category: cs.LG

TL;DR: 本文提出了一种基于集成的防御架构，用于提高自动驾驶中深度强化学习模型对抗攻击的鲁棒性，在FGSM攻击下显著提升了平均奖励并降低了碰撞率。


<details>
  <summary>Details</summary>
Motivation: 虽然深度强化学习在多个领域取得进展，但其在面对对抗性攻击时的鲁棒性仍存在问题。现有防御机制如对抗训练和蒸馏虽能增强DRL模型的韧性，但在自动驾驶场景中集成多种防御方法仍存在研究空白。

Method: 提出了一种新颖的基于集成的防御架构，通过整合多种防御策略来缓解自动驾驶中的对抗性攻击。

Result: 在FGSM攻击下，与基线相比，集成方法将平均奖励从5.87提升至18.38（增长超过213%），将平均碰撞率从0.50降低至0.09（降低82%），在高速公路和合并场景中均优于所有单独的防御策略。

Conclusion: 所提出的集成防御架构显著增强了DRL模型的鲁棒性，在自动驾驶场景中有效抵御对抗性攻击，性能优于单一防御策略。

Abstract: Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated
its applicability across various domains, including robotics, healthcare,
energy optimization, and autonomous driving. However, a critical question
remains: How robust are DRL models when exposed to adversarial attacks? While
existing defense mechanisms such as adversarial training and distillation
enhance the resilience of DRL models, there remains a significant research gap
regarding the integration of multiple defenses in autonomous driving scenarios
specifically. This paper addresses this gap by proposing a novel ensemble-based
defense architecture to mitigate adversarial attacks in autonomous driving. Our
evaluation demonstrates that the proposed architecture significantly enhances
the robustness of DRL models. Compared to the baseline under FGSM attacks, our
ensemble method improves the mean reward from 5.87 to 18.38 (over 213%
increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82%
decrease) in the highway scenario and merge scenario, outperforming all
standalone defense strategies.

</details>


### [43] [Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models](https://arxiv.org/abs/2507.17107)
*Andrii Balashov*

Main category: cs.LG

TL;DR: 研究发现强化学习微调大语言模型时只会修改少量参数子网络（通常5-30%），而非所有参数，这种稀疏性现象在多种RL算法和模型中普遍存在


<details>
  <summary>Details</summary>
Motivation: 挑战强化学习微调需要更新模型大部分参数的传统假设，探索RL微调过程中参数更新的真实模式

Method: 分析多种RL算法（PPO、DPO、SimPO、PRIME）在不同模型家族（OpenAI、Meta等）上的参数更新模式，研究更新子网络的重叠性和可转移性

Result: 发现RL微调自然产生参数更新稀疏性，更新的子网络在不同种子、数据集和算法间显示出显著重叠；仅微调稀疏子网络就能恢复完整模型性能

Conclusion: RL通过专注训练小而一致的子网络来适应模型，而非改变所有权重；这种稀疏性源于RL在模型原始分布附近操作，为更高效的RL方法提供了新见解

Abstract: Reinforcement learning (RL) is a key post-pretraining step for aligning large
language models (LLMs) with complex tasks and human preferences. While it is
often assumed that RL fine-tuning requires updating most of a model's
parameters, we challenge this assumption with a surprising finding: RL
fine-tuning consistently modifies only a small subnetwork (typically 5-30% of
weights), leaving most parameters unchanged. We call this phenomenon RL-induced
parameter update sparsity. It arises naturally, without any sparsity
constraints or parameter-efficient tuning, and appears across multiple RL
algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,
Meta, and open-source LLMs). Moreover, the subnetworks updated by RL show
substantial overlap across different seeds, datasets, and algorithms-far
exceeding chance-suggesting a partially transferable structure in the
pretrained model. We show that fine-tuning only this sparse subnetwork recovers
full model performance and yields parameters nearly identical to the fully
fine-tuned model. Our analysis suggests this sparsity emerges because RL
operates near the model's original distribution, requiring only targeted
changes. KL penalties, gradient clipping, and on-policy dynamics have limited
effect on the sparsity pattern. These findings shed new light on how RL adapts
models: not by shifting all weights, but by focusing training on a small,
consistently updated subnetwork. This insight enables more efficient RL methods
and reframes sparsity through the lens of the lottery ticket hypothesis.

</details>


### [44] [Probabilistic Graphical Models: A Concise Tutorial](https://arxiv.org/abs/2507.17116)
*Jacqueline Maasch,Willie Neiswanger,Stefano Ermon,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: 这是一篇关于概率图模型的教程论文，介绍了这一机器学习分支的理论基础、方法和应用，涵盖了图表示、学习算法和推理算法三个核心主题。


<details>
  <summary>Details</summary>
Motivation: 概率图模型作为机器学习的重要分支，需要一个简洁而全面的入门教程来帮助读者理解其理论基础和实际应用。该领域结合了概率论和图论两大数学传统，需要系统性的介绍来展示其在不确定性建模和决策支持方面的强大能力。

Method: 本教程采用渐进式教学方法，首先回顾基础的概率论和图论知识，然后围绕三个核心主题展开：(1) 使用直观的图形语言表示多元分布；(2) 从数据中学习模型参数和图结构的算法；(3) 精确和近似推理算法。

Result: 该教程提供了概率图模型框架的形式化方法、算法和应用的简洁介绍，展示了如何用图形化方式紧凑而富有表现力地表示联合概率分布，形成强大的概率推理生成模型。

Conclusion: 概率图模型为在不确定性条件下进行预测和决策提供了优雅的理论框架，通过结合概率论和图论，能够有效地表示复杂的多元概率分布并支持各种推理任务。

Abstract: Probabilistic graphical modeling is a branch of machine learning that uses
probability distributions to describe the world, make predictions, and support
decision-making under uncertainty. Underlying this modeling framework is an
elegant body of theory that bridges two mathematical traditions: probability
and graph theory. This framework provides compact yet expressive
representations of joint probability distributions, yielding powerful
generative models for probabilistic reasoning.
  This tutorial provides a concise introduction to the formalisms, methods, and
applications of this modeling framework. After a review of basic probability
and graph theory, we explore three dominant themes: (1) the representation of
multivariate distributions in the intuitive visual language of graphs, (2)
algorithms for learning model parameters and graphical structures from data,
and (3) algorithms for inference, both exact and approximate.

</details>


### [45] [Computer Vision for Real-Time Monkeypox Diagnosis on Embedded Systems](https://arxiv.org/abs/2507.17123)
*Jacob M. Delgado-López,Ricardo A. Morell-Rodriguez,Sebastián O. Espinosa-Del Rosario,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: 本研究开发了一个基于AI的猴痘诊断工具，部署在NVIDIA Jetson Orin Nano上，使用MobileNetV2架构实现93.07%的F1分数，通过TensorRT优化将功耗降低约一半，并配备Wi-Fi热点和网页界面，适用于资源受限环境下的传染病快速诊断。


<details>
  <summary>Details</summary>
Motivation: 传染病（如猴痘）的快速诊断对于有效控制和治疗至关重要，特别是在资源受限的环境中。现有诊断方法在欠发达地区面临技术和资源限制，需要开发一种高效、可扩展且节能的诊断解决方案来解决服务不足地区的诊断挑战。

Method: 使用预训练的MobileNetV2架构进行二分类，在开源猴痘皮肤病变数据集上训练模型。采用TensorRT框架对FP32进行推理加速，并对FP16和INT8格式执行训练后量化。通过混合精度优化减小模型大小、提高推理速度并降低功耗。系统配备Wi-Fi接入点热点和基于网页的界面，支持用户通过移动设备上传和分析图像。

Result: 模型在猴痘皮肤病变数据集上取得93.07%的F1分数，在精确率和召回率方面表现均衡。TensorRT优化将模型大小、推理速度和功耗都改善了约一倍，同时保持原始精度。功耗分析证实优化模型在推理过程中显著降低了能耗。系统成功部署了Wi-Fi热点和网页界面，实现了简单访问和无缝连接。

Conclusion: 该诊断工具作为高效、可扩展且节能的解决方案，成功解决了服务不足地区的诊断挑战，为在低资源医疗环境中的广泛应用铺平了道路。优化后的模型在保持诊断准确性的同时显著降低了资源需求，使其适合在资源受限环境中部署和使用。

Abstract: The rapid diagnosis of infectious diseases, such as monkeypox, is crucial for
effective containment and treatment, particularly in resource-constrained
environments. This study presents an AI-driven diagnostic tool developed for
deployment on the NVIDIA Jetson Orin Nano, leveraging the pre-trained
MobileNetV2 architecture for binary classification. The model was trained on
the open-source Monkeypox Skin Lesion Dataset, achieving a 93.07% F1-Score,
which reflects a well-balanced performance in precision and recall. To optimize
the model, the TensorRT framework was used to accelerate inference for FP32 and
to perform post-training quantization for FP16 and INT8 formats. TensorRT's
mixed-precision capabilities enabled these optimizations, which reduced the
model size, increased inference speed, and lowered power consumption by
approximately a factor of two, all while maintaining the original accuracy.
Power consumption analysis confirmed that the optimized models used
significantly less energy during inference, reinforcing their suitability for
deployment in resource-constrained environments. The system was deployed with a
Wi-Fi Access Point (AP) hotspot and a web-based interface, enabling users to
upload and analyze images directly through connected devices such as mobile
phones. This setup ensures simple access and seamless connectivity, making the
tool practical for real-world applications. These advancements position the
diagnostic tool as an efficient, scalable, and energy-conscious solution to
address diagnosis challenges in underserved regions, paving the way for broader
adoption in low-resource healthcare settings.

</details>


### [46] [Model Compression Engine for Wearable Devices Skin Cancer Diagnosis](https://arxiv.org/abs/2507.17125)
*Jacob M. Delgado-López,Andrea P. Seda-Hernandez,Juan D. Guadalupe-Rosado,Luis E. Fernandez Ramirez,Miguel Giboyeaux-Camilo,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: 研究开发了一个基于MobileNetV2和TensorRT优化的AI皮肤癌诊断工具，可在NVIDIA Jetson Orin Nano等资源受限的嵌入式设备上部署，实现了87.18%的F1分数和显著的模型压缩与能耗降低。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是最常见且可预防的癌症类型之一，但其早期检测仍然是一个挑战，特别是在缺乏专业医疗资源的地区。研究旨在开发一个针对嵌入式系统优化的AI诊断工具来解决这一问题。

Method: 使用MobileNetV2架构进行迁移学习，将模型适配为皮肤病变的二分类任务（"皮肤癌"和"其他"）。采用TensorRT框架对模型进行压缩和优化，以便在NVIDIA Jetson Orin Nano上部署，平衡性能与能效。

Result: 优化后的模型保持了良好性能，F1分数达到87.18%，精确率93.18%，召回率81.91%。压缩后模型大小减少至0.41，推理速度和吞吐量得到改善，INT8精度下能耗降低至0.93。

Conclusion: 验证了在资源受限的边缘设备上部署高性能、高能效诊断工具的可行性。研究方法在其他医疗诊断和需要可访问、高效AI解决方案的领域具有广泛应用前景，展现了优化AI系统在革命性改变医疗诊断、缩小先进技术与服务不足地区差距方面的潜力。

Abstract: Skin cancer is one of the most prevalent and preventable types of cancer, yet
its early detection remains a challenge, particularly in resource-limited
settings where access to specialized healthcare is scarce. This study proposes
an AI-driven diagnostic tool optimized for embedded systems to address this
gap. Using transfer learning with the MobileNetV2 architecture, the model was
adapted for binary classification of skin lesions into "Skin Cancer" and
"Other." The TensorRT framework was employed to compress and optimize the model
for deployment on the NVIDIA Jetson Orin Nano, balancing performance with
energy efficiency. Comprehensive evaluations were conducted across multiple
benchmarks, including model size, inference speed, throughput, and power
consumption. The optimized models maintained their performance, achieving an
F1-Score of 87.18% with a precision of 93.18% and recall of 81.91%.
Post-compression results showed reductions in model size of up to 0.41, along
with improvements in inference speed and throughput, and a decrease in energy
consumption of up to 0.93 in INT8 precision. These findings validate the
feasibility of deploying high-performing, energy-efficient diagnostic tools on
resource-constrained edge devices. Beyond skin cancer detection, the
methodologies applied in this research have broader applications in other
medical diagnostics and domains requiring accessible, efficient AI solutions.
This study underscores the potential of optimized AI systems to revolutionize
healthcare diagnostics, thereby bridging the divide between advanced technology
and underserved regions.

</details>


### [47] [Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance](https://arxiv.org/abs/2507.17131)
*Yufei He,Ruoyu Li,Alex Chen,Yue Liu,Yulin Chen,Yuan Sui,Cheng Chen,Yi Zhu,Luca Luo,Frank Yang,Bryan Hooi*

Main category: cs.LG

TL;DR: 提出了ARIA框架，一个能够在测试时持续学习更新领域知识的LLM智能体，通过结构化自我对话评估不确定性，主动识别知识缺口并向人类专家请求指导，系统性地更新内部知识库。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在规则和领域知识频繁变化的环境中表现不佳，如监管合规和用户风险筛查。离线微调和标准提示等方法无法在实际操作中有效适应新知识。

Method: 设计了自适应反思交互智能体(ARIA)框架，通过结构化自我对话评估不确定性，主动识别知识缺口并请求人类专家的针对性解释或纠正，系统性地更新带时间戳的内部知识库，通过比较和澄清查询检测并解决冲突或过时的知识。

Result: 在TikTok Pay的客户尽职调查姓名筛查任务和公开的动态知识任务上进行评估，相比使用标准离线微调和现有自我改进智能体的基线方法，在适应性和准确性方面有显著改进。

Conclusion: ARIA已部署在TikTok Pay中为超过1.5亿月活用户提供服务，证实了其在快速变化环境中的实用性和有效性。该框架为LLM智能体在动态环境中的持续学习提供了有效解决方案。

Abstract: Large language model (LLM) agents often struggle in environments where rules
and required domain knowledge frequently change, such as regulatory compliance
and user risk screening. Current approaches, like offline fine-tuning and
standard prompting, are insufficient because they cannot effectively adapt to
new knowledge during actual operation. To address this limitation, we propose
the Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework
designed specifically to continuously learn updated domain knowledge at test
time. ARIA assesses its own uncertainty through structured self-dialogue,
proactively identifying knowledge gaps and requesting targeted explanations or
corrections from human experts. It then systematically updates an internal,
timestamped knowledge repository with provided human guidance, detecting and
resolving conflicting or outdated knowledge through comparisons and
clarification queries. We evaluate ARIA on the realistic customer due diligence
name screening task on TikTok Pay, alongside publicly available dynamic
knowledge tasks. Results demonstrate significant improvements in adaptability
and accuracy compared to baselines using standard offline fine-tuning and
existing self-improving agents. ARIA is deployed within TikTok Pay serving over
150 million monthly active users, confirming its practicality and effectiveness
for operational use in rapidly evolving environments.

</details>


### [48] [SADA: Stability-guided Adaptive Diffusion Acceleration](https://arxiv.org/abs/2507.17135)
*Ting Jiang,Yixiao Wang,Hancheng Ye,Zishan Shao,Jingwei Sun,Jingyang Zhang,Zekai Chen,Jianyi Zhang,Yiran Chen,Hai Li*

Main category: cs.LG

TL;DR: 本文提出了SADA（稳定性引导的自适应扩散加速）方法，通过统一的稳定性准则同时进行步骤级和token级的稀疏化决策，实现了扩散模型在保持高保真度的同时获得1.8倍以上的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型虽然在生成任务中表现出色，但由于迭代采样过程和二次注意力成本导致计算开销高昂。现有的无训练加速策略虽能减少采样时间，但相比原始基线保真度较低。作者假设这种保真度差距源于：(a)不同提示对应不同的去噪轨迹，(b)这些方法未考虑底层ODE公式及其数值解。

Method: 提出SADA（稳定性引导的自适应扩散加速）方法，这是一个新颖的范式，通过单一稳定性准则统一步骤级和token级稀疏化决策，加速基于ODE的生成模型（扩散和流匹配）的采样。针对问题(a)，SADA基于采样轨迹自适应分配稀疏性；针对问题(b)，SADA引入原理性近似方案，利用数值ODE求解器的精确梯度信息。

Result: 在SD-2、SDXL和Flux上使用EDM和DPM++求解器的综合评估显示，相比未修改的基线，SADA实现了一致的≥1.8倍加速，同时保持最小的保真度降低（LPIPS≤0.10，FID≤4.5），显著优于先前方法。此外，SADA无缝适应其他管道和模态：无需任何修改即可加速ControlNet，并以约0.01的频谱图LPIPS将MusicLDM加速1.8倍。

Conclusion: SADA通过统一的稳定性准则有效解决了现有扩散模型加速方法中保真度与速度的权衡问题，在多个模型和任务上都实现了显著的加速效果，同时保持了高质量的生成结果，证明了该方法的有效性和通用性。

Abstract: Diffusion models have achieved remarkable success in generative tasks but
suffer from high computational costs due to their iterative sampling process
and quadratic attention costs. Existing training-free acceleration strategies
that reduce per-step computation cost, while effectively reducing sampling
time, demonstrate low faithfulness compared to the original baseline. We
hypothesize that this fidelity gap arises because (a) different prompts
correspond to varying denoising trajectory, and (b) such methods do not
consider the underlying ODE formulation and its numerical solution. In this
paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a
novel paradigm that unifies step-wise and token-wise sparsity decisions via a
single stability criterion to accelerate sampling of ODE-based generative
models (Diffusion and Flow-matching). For (a), SADA adaptively allocates
sparsity based on the sampling trajectory. For (b), SADA introduces principled
approximation schemes that leverage the precise gradient information from the
numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using
both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with
minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to
unmodified baselines, significantly outperforming prior methods. Moreover, SADA
adapts seamlessly to other pipelines and modalities: It accelerates ControlNet
without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim
0.01$ spectrogram LPIPS.

</details>


### [49] [PICore: Physics-Informed Unsupervised Coreset Selection for Data Efficient Neural Operator Training](https://arxiv.org/abs/2507.17151)
*Anirudh Satheesh,Anant Khandelwal,Mucong Ding,Radu Balan*

Main category: cs.LG

TL;DR: PICore是一个无监督核心集选择框架，通过物理信息损失识别最有信息量的训练样本，无需访问真实PDE解，显著提高神经算子训练效率并降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 神经算子训练面临两个主要瓶颈：需要大量训练数据学习函数空间映射，且数据需要标注，而标注只能通过昂贵的数值求解器仿真获得。因此需要一种方法同时解决数据量大和标注成本高的问题。

Method: 提出PICore无监督核心集选择框架，利用物理信息损失来选择未标注输入样本，根据样本对算子学习的潜在贡献进行评估。选择出紧凑的输入子集后，仅对这些样本使用数值求解器生成标签，然后在缩减的标注数据集上训练神经算子。

Result: 在四个不同的PDE基准测试和多种核心集选择策略上，PICore相比有监督核心集选择方法平均提高了78%的训练效率，同时准确性变化极小。显著降低了训练时间和标注成本。

Conclusion: PICore框架成功解决了神经算子训练中的数据需求大和标注成本高的双重问题，通过无监督的方式选择最有价值的训练样本，在保持准确性的同时大幅提升训练效率，为PDE求解的神经算子方法提供了更实用的解决方案。

Abstract: Neural operators offer a powerful paradigm for solving partial differential
equations (PDEs) that cannot be solved analytically by learning mappings
between function spaces. However, there are two main bottlenecks in training
neural operators: they require a significant amount of training data to learn
these mappings, and this data needs to be labeled, which can only be accessed
via expensive simulations with numerical solvers. To alleviate both of these
issues simultaneously, we propose PICore, an unsupervised coreset selection
framework that identifies the most informative training samples without
requiring access to ground-truth PDE solutions. PICore leverages a
physics-informed loss to select unlabeled inputs by their potential
contribution to operator learning. After selecting a compact subset of inputs,
only those samples are simulated using numerical solvers to generate labels,
reducing annotation costs. We then train the neural operator on the reduced
labeled dataset, significantly decreasing training time as well. Across four
diverse PDE benchmarks and multiple coreset selection strategies, PICore
achieves up to 78% average increase in training efficiency relative to
supervised coreset selection methods with minimal changes in accuracy. We
provide code at https://github.com/Asatheesh6561/PICore.

</details>


### [50] [Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection](https://arxiv.org/abs/2507.17161)
*Vinura Galwaduge,Jagath Samarabandu*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的反事实解释框架，用于网络入侵检测系统的可解释AI，能够生成可操作的解释并转化为全局规则以有效过滤攻击查询。


<details>
  <summary>Details</summary>
Motivation: 现代网络入侵检测系统使用深度学习模型的"黑盒"特性导致检测决策缺乏透明度，阻碍了对决策的理解和信任，并影响及时采取对抗措施。现有的可解释AI方法提供的解释难以转化为可操作的对抗措施。

Method: 提出了一种新颖的基于扩散模型的反事实解释框架，能够为网络入侵攻击提供可操作的解释。该方法通过生成最小化、多样化的反事实解释，并将其总结为全局规则集。

Result: 在3个现代网络入侵数据集上与其他公开可用的反事实解释算法进行了评估。所提出的方法在测试的反事实解释算法中能够更高效地提供最小化、多样化的反事实解释，减少了生成解释的时间。生成的全局反事实规则能够有效过滤传入的攻击查询。

Conclusion: 该方法不仅在实例级别提供可操作的解释，还能在全局级别为入侵攻击提供可操作的解释。全局反事实规则显示出有效过滤传入攻击查询的能力，这对高效的入侵检测和防御机制至关重要。这也是首次在网络入侵检测系统背景下对现有反事实解释算法进行比较分析的工作。

Abstract: Modern network intrusion detection systems (NIDS) frequently utilize the
predictive power of complex deep learning models. However, the "black-box"
nature of such deep learning methods adds a layer of opaqueness that hinders
the proper understanding of detection decisions, trust in the decisions and
prevent timely countermeasures against such attacks. Explainable AI (XAI)
methods provide a solution to this problem by providing insights into the
causes of the predictions. The majority of the existing XAI methods provide
explanations which are not convenient to convert into actionable
countermeasures. In this work, we propose a novel diffusion-based
counterfactual explanation framework that can provide actionable explanations
for network intrusion attacks. We evaluated our proposed algorithm against
several other publicly available counterfactual explanation algorithms on 3
modern network intrusion datasets. To the best of our knowledge, this work also
presents the first comparative analysis of existing counterfactual explanation
algorithms within the context of network intrusion detection systems. Our
proposed method provide minimal, diverse counterfactual explanations out of the
tested counterfactual explanation algorithms in a more efficient manner by
reducing the time to generate explanations. We also demonstrate how
counterfactual explanations can provide actionable explanations by summarizing
them to create a set of global rules. These rules are actionable not only at
instance level but also at the global level for intrusion attacks. These global
counterfactual rules show the ability to effectively filter out incoming attack
queries which is crucial for efficient intrusion detection and defense
mechanisms.

</details>


### [51] [Met$^2$Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems](https://arxiv.org/abs/2507.17189)
*Shaohan Li,Hao Yang,Min Chen,Xiaolin Qin*

Main category: cs.LG

TL;DR: 该论文提出了Met2Net，一种隐式两阶段训练方法用于天气预测，通过为每个变量配置独立的编码器和解码器，并引入自注意力机制进行多变量融合，在近地面气温和相对湿度预测上分别降低了28.82%和23.39%的MSE


<details>
  <summary>Details</summary>
Motivation: 现有端到端天气预测方法面临多变量集成中的表示不一致问题，难以有效捕获复杂天气系统中变量间的依赖关系；现有多模态两阶段训练方法由于两阶段间训练任务的不一致性导致结果次优

Method: 提出隐式两阶段训练方法：第一阶段冻结Translator，让编码器和解码器学习共享潜在空间；第二阶段冻结编码器和解码器，让Translator捕获变量间交互进行预测；同时引入自注意力机制在潜在空间中进行多变量融合

Result: 在天气预测任务上达到最先进性能，近地面气温预测MSE降低28.82%，相对湿度预测MSE降低23.39%

Conclusion: 所提出的Met2Net方法通过隐式两阶段训练和自注意力机制有效解决了多变量天气预测中的表示不一致和变量依赖捕获问题，在实验中取得了显著的性能提升

Abstract: The increasing frequency of extreme weather events due to global climate
change urges accurate weather prediction. Recently, great advances have been
made by the \textbf{end-to-end methods}, thanks to deep learning techniques,
but they face limitations of \textit{representation inconsistency} in
multivariable integration and struggle to effectively capture the dependency
between variables, which is required in complex weather systems. Treating
different variables as distinct modalities and applying a \textbf{two-stage
training approach} from multimodal models can partially alleviate this issue,
but due to the inconformity in training tasks between the two stages, the
results are often suboptimal. To address these challenges, we propose an
implicit two-stage training method, configuring separate encoders and decoders
for each variable. In detailed, in the first stage, the Translator is frozen
while the Encoders and Decoders learn a shared latent space, in the second
stage, the Encoders and Decoders are frozen, and the Translator captures
inter-variable interactions for prediction. Besides, by introducing a
self-attention mechanism for multivariable fusion in the latent space, the
performance achieves further improvements. Empirically, extensive experiments
show the state-of-the-art performance of our method. Specifically, it reduces
the MSE for near-surface air temperature and relative humidity predictions by
28.82\% and 23.39\%, respectively. The source code is available at
https://github.com/ShremG/Met2Net.

</details>


### [52] [Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation](https://arxiv.org/abs/2507.17204)
*Zixuan Wang,Jinghao Shi,Hanzhong Liang,Xiang Shen,Vera Wen,Zhiqian Chen,Yifan Wu,Zhixin Zhang,Hongyu Xiong*

Main category: cs.LG

TL;DR: 本文提出了一种基于多模态大语言模型(MLLM)的视频内容审核系统，通过路由器-排序级联架构实现高效部署，相比传统分类器F1分数提升66.50%，同时计算成本仅为直接部署的1.5%


<details>
  <summary>Details</summary>
Motivation: 传统视频分类模型在处理隐式有害内容和上下文模糊性等复杂场景时表现不佳，而多模态大语言模型虽然具有优秀的跨模态推理和上下文理解能力，但面临计算成本高和生成模型适配判别分类任务困难两大挑战，阻碍了其在工业界的应用

Method: 首先提出一种高效方法，使用最少的判别训练数据将生成式MLLM转换为多模态分类器；然后设计路由器-排序级联系统，将MLLM与轻量级路由器模型集成，实现工业规模部署

Result: 离线实验显示，基于MLLM的方法相比传统分类器F1分数提升66.50%，仅需要2%的微调数据；在线评估表明系统将自动内容审核量提升了41%，级联部署将计算成本降低至直接全规模部署的1.5%

Conclusion: 通过提出的高效转换方法和级联部署架构，成功解决了MLLM在视频内容审核中的计算成本和模型适配问题，实现了性能显著提升的同时大幅降低了部署成本，为MLLM在工业级内容审核应用提供了可行的解决方案

Abstract: Effective content moderation is essential for video platforms to safeguard
user experience and uphold community standards. While traditional video
classification models effectively handle well-defined moderation tasks, they
struggle with complicated scenarios such as implicit harmful content and
contextual ambiguity. Multimodal large language models (MLLMs) offer a
promising solution to these limitations with their superior cross-modal
reasoning and contextual understanding. However, two key challenges hinder
their industrial adoption. First, the high computational cost of MLLMs makes
full-scale deployment impractical. Second, adapting generative models for
discriminative classification remains an open research problem. In this paper,
we first introduce an efficient method to transform a generative MLLM into a
multimodal classifier using minimal discriminative training data. To enable
industry-scale deployment, we then propose a router-ranking cascade system that
integrates MLLMs with a lightweight router model. Offline experiments
demonstrate that our MLLM-based approach improves F1 score by 66.50% over
traditional classifiers while requiring only 2% of the fine-tuning data. Online
evaluations show that our system increases automatic content moderation volume
by 41%, while the cascading deployment reduces computational cost to only 1.5%
of direct full-scale deployment.

</details>


### [53] [Dataset Distillation as Data Compression: A Rate-Utility Perspective](https://arxiv.org/abs/2507.17221)
*Youneng Bao,Yiping Liu,Zhuo Chen,Yongsheng Liang,Mu Li,Kede Ma*

Main category: cs.LG

TL;DR: 提出了一种联合速率-效用优化的数据集蒸馏方法，通过将合成样本参数化为可优化的潜在编码并使用轻量级网络解码，在保持准确性的同时实现了高达170倍的压缩率


<details>
  <summary>Details</summary>
Motivation: 现代机器学习追求"规模即一切"的范式，需要越来越大的数据集和模型，导致计算和存储需求过高。现有的数据集蒸馏方法要么在固定存储预算下最大化性能，要么追求合适的合成数据表示来消除冗余，但没有同时优化这两个目标

Method: 将合成样本参数化为可优化的潜在编码，通过极轻量级网络进行解码；估计量化潜在变量的香农熵作为速率度量，使用现有蒸馏损失作为效用度量，通过拉格朗日乘数进行权衡；引入每类比特数(bpc)作为精确的存储度量，考虑样本、标签和解码器参数成本

Result: 在CIFAR-10、CIFAR-100和ImageNet-128上，该方法在可比准确性下实现了比标准蒸馏高达170倍的压缩率；在不同的bpc预算、蒸馏损失和骨干架构下，该方法始终建立了更好的速率-效用权衡

Conclusion: 提出的联合速率-效用优化方法能够有效解决数据集蒸馏中存储效率和性能保持之间的平衡问题，在多个数据集和不同设置下都表现出优异的压缩性能和准确性保持能力

Abstract: Driven by the ``scale-is-everything'' paradigm, modern machine learning
increasingly demands ever-larger datasets and models, yielding prohibitive
computational and storage requirements. Dataset distillation mitigates this by
compressing an original dataset into a small set of synthetic samples, while
preserving its full utility. Yet, existing methods either maximize performance
under fixed storage budgets or pursue suitable synthetic data representations
for redundancy removal, without jointly optimizing both objectives. In this
work, we propose a joint rate-utility optimization method for dataset
distillation. We parameterize synthetic samples as optimizable latent codes
decoded by extremely lightweight networks. We estimate the Shannon entropy of
quantized latents as the rate measure and plug any existing distillation loss
as the utility measure, trading them off via a Lagrange multiplier. To enable
fair, cross-method comparisons, we introduce bits per class (bpc), a precise
storage metric that accounts for sample, label, and decoder parameter costs. On
CIFAR-10, CIFAR-100, and ImageNet-128, our method achieves up to $170\times$
greater compression than standard distillation at comparable accuracy. Across
diverse bpc budgets, distillation losses, and backbone architectures, our
approach consistently establishes better rate-utility trade-offs.

</details>


### [54] [P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices](https://arxiv.org/abs/2507.17228)
*Wei Fan,JinYi Yoon,Xiaochang Li,Huajie Shao,Bo Ji*

Main category: cs.LG

TL;DR: 本文提出P3SL框架，一个面向异构资源受限边缘设备的个性化隐私保护分割学习方法，通过双层优化技术让客户端自主确定最优分割点，在保护隐私的同时平衡能耗和模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有的分割学习方法在异构环境下忽略了个性化隐私需求和本地模型定制化，无法适应不同设备的计算资源、通信能力、环境条件和隐私要求的差异性。

Method: 设计了个性化序列分割学习流水线，采用双层优化技术让客户端在不向服务器共享敏感信息的情况下自主确定最优个性化分割点，实现定制化隐私保护和本地模型个性化。

Result: 在包含4个Jetson Nano P3450设备、2个树莓派和1台笔记本电脑的测试平台上进行评估，使用多种模型架构和数据集在不同环境条件下验证了方法的有效性。

Conclusion: P3SL框架成功解决了异构边缘设备环境下的个性化隐私保护问题，在不泄露设备敏感信息的前提下实现了能耗、隐私泄露风险和模型精度之间的有效平衡。

Abstract: Split Learning (SL) is an emerging privacy-preserving machine learning
technique that enables resource constrained edge devices to participate in
model training by partitioning a model into client-side and server-side
sub-models. While SL reduces computational overhead on edge devices, it
encounters significant challenges in heterogeneous environments where devices
vary in computing resources, communication capabilities, environmental
conditions, and privacy requirements. Although recent studies have explored
heterogeneous SL frameworks that optimize split points for devices with varying
resource constraints, they often neglect personalized privacy requirements and
local model customization under varying environmental conditions. To address
these limitations, we propose P3SL, a Personalized Privacy-Preserving Split
Learning framework designed for heterogeneous, resource-constrained edge device
systems. The key contributions of this work are twofold. First, we design a
personalized sequential split learning pipeline that allows each client to
achieve customized privacy protection and maintain personalized local models
tailored to their computational resources, environmental conditions, and
privacy needs. Second, we adopt a bi-level optimization technique that empowers
clients to determine their own optimal personalized split points without
sharing private sensitive information (i.e., computational resources,
environmental conditions, privacy requirements) with the server. This approach
balances energy consumption and privacy leakage risks while maintaining high
model accuracy. We implement and evaluate P3SL on a testbed consisting of 7
devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,
using diverse model architectures and datasets under varying environmental
conditions.

</details>


### [55] [Eco-Friendly AI: Unleashing Data Power for Green Federated Learning](https://arxiv.org/abs/2507.17241)
*Mattia Sabella,Monica Vitali*

Main category: cs.LG

TL;DR: 本文提出了一种以数据为中心的绿色联邦学习方法，通过减少训练数据量和选择环境影响最小的节点来降低AI/ML的碳排放，并开发了交互式推荐系统来优化联邦学习配置。


<details>
  <summary>Details</summary>
Motivation: 人工智能和机器学习的广泛应用带来了显著的环境影响，特别是在能耗和碳排放方面。训练数据集的大小是影响ML模型训练能耗的关键因素之一。虽然联邦学习能减少数据传输成本并增强隐私保护，但也因数据源异构性、计算节点能力差异和环境影响而带来挑战。

Method: 提出以数据为中心的绿色联邦学习方法，包括：1）分析联邦数据集特征；2）基于质量指标选择最优数据子集；3）选择环境影响最低的联邦节点；4）开发综合方法论来研究数据质量和数据量等因素对FL训练性能和碳排放的影响；5）构建交互式推荐系统优化FL配置。

Result: 将该方法应用于时间序列分类任务，在减少联邦学习任务环境影响方面取得了有希望的结果。通过数据减少实现了FL配置的优化，最小化了训练过程中的环境影响。

Conclusion: 通过以数据为中心的方法成功推进了绿色AI的发展，证明了通过优化数据选择和节点配置可以有效减少联邦学习的环境足迹，为可持续的AI发展提供了新的解决方案。

Abstract: The widespread adoption of Artificial Intelligence (AI) and Machine Learning
(ML) comes with a significant environmental impact, particularly in terms of
energy consumption and carbon emissions. This pressing issue highlights the
need for innovative solutions to mitigate AI's ecological footprint. One of the
key factors influencing the energy consumption of ML model training is the size
of the training dataset. ML models are often trained on vast amounts of data
continuously generated by sensors and devices distributed across multiple
locations. To reduce data transmission costs and enhance privacy, Federated
Learning (FL) enables model training without the need to move or share raw
data. While FL offers these advantages, it also introduces challenges due to
the heterogeneity of data sources (related to volume and quality),
computational node capabilities, and environmental impact.
  This paper contributes to the advancement of Green AI by proposing a
data-centric approach to Green Federated Learning. Specifically, we focus on
reducing FL's environmental impact by minimizing the volume of training data.
Our methodology involves the analysis of the characteristics of federated
datasets, the selecting of an optimal subset of data based on quality metrics,
and the choice of the federated nodes with the lowest environmental impact. We
develop a comprehensive methodology that examines the influence of data-centric
factors, such as data quality and volume, on FL training performance and carbon
emissions. Building on these insights, we introduce an interactive
recommendation system that optimizes FL configurations through data reduction,
minimizing environmental impact during training. Applying this methodology to
time series classification has demonstrated promising results in reducing the
environmental impact of FL tasks.

</details>


### [56] [DistrAttention: An Efficient and Flexible Self-Attention Mechanism on Modern GPUs](https://arxiv.org/abs/2507.17245)
*Haolin Jin,Mengbai Xiao,Yuan Yuan,Xiao Zhang,Dongxiao Yu,Guanghui Zhang,Haoliang Wang*

Main category: cs.LG

TL;DR: 本文提出了DistrAttention，一种高效灵活的自注意力机制，通过在嵌入维度上对数据进行分组来保持完整上下文信息，同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中的自注意力机制具有相对于输入序列长度的二次时间复杂度，这限制了Transformer的可扩展性。现有的自注意力优化方法要么丢弃完整的上下文信息，要么缺乏灵活性。

Method: 设计了DistrAttention机制，通过在嵌入维度d上对数据进行分组来实现高效且灵活的自注意力计算。使用轻量级的采样和融合方法，利用局部敏感哈希来分组相似数据。进一步设计了块状分组框架来限制局部敏感哈希引入的误差。通过优化块大小选择，DistrAttention可以轻松与FlashAttention-2集成。

Result: 实验结果显示，DistrAttention在计算自注意力时比FlashAttention-2快37%。在ViT推理中，DistrAttention在近似自注意力机制中速度最快且精度最高。在Llama3-1B模型中，DistrAttention实现了最低的推理时间，准确率仅损失1%。

Conclusion: DistrAttention成功解决了自注意力机制的效率问题，在保持完整上下文信息的同时显著提升了计算速度，为Transformer架构的可扩展性提供了有效解决方案。

Abstract: The Transformer architecture has revolutionized deep learning, delivering the
state-of-the-art performance in areas such as natural language processing,
computer vision, and time series prediction. However, its core component,
self-attention, has the quadratic time complexity relative to input sequence
length, which hinders the scalability of Transformers. The exsiting approaches
on optimizing self-attention either discard full-contextual information or lack
of flexibility. In this work, we design DistrAttention, an effcient and
flexible self-attention mechanism with the full context. DistrAttention
achieves this by grouping data on the embedding dimensionality, usually
referred to as $d$. We realize DistrAttention with a lightweight sampling and
fusion method that exploits locality-sensitive hashing to group similar data. A
block-wise grouping framework is further designed to limit the errors
introduced by locality sensitive hashing. By optimizing the selection of block
sizes, DistrAttention could be easily integrated with FlashAttention-2, gaining
high-performance on modern GPUs. We evaluate DistrAttention with extensive
experiments. The results show that our method is 37% faster than
FlashAttention-2 on calculating self-attention. In ViT inference,
DistrAttention is the fastest and the most accurate among approximate
self-attention mechanisms. In Llama3-1B, DistrAttention still achieves the
lowest inference time with only 1% accuray loss.

</details>


### [57] [Rethinking VAE: From Continuous to Discrete Representations Without Probabilistic Assumptions](https://arxiv.org/abs/2507.17255)
*Songxuan Shi*

Main category: cs.LG

TL;DR: 本文探索了自编码器的生成能力，提出了一种新的VAE训练方法来解决编码空间未定义区域的问题，并建立了VAE和VQ-VAE之间的联系，发现编码空间的紧凑性和分散性对生成建模至关重要


<details>
  <summary>Details</summary>
Motivation: 传统自编码器虽然具有通过潜在空间插值和扰动进行生成的潜力，但受限于编码空间中存在未定义区域的问题，需要找到一种方法来确保潜在空间的良好定义并增强数据紧凑性

Method: 提出了一种新的类VAE训练方法，引入聚类中心来增强数据紧凑性并确保潜在空间的良好定义，不依赖传统的KL散度或重参数化技术。将此方法扩展到多个可学习向量，观察向VQ-VAE模型的自然演进

Result: 在MNIST、CelebA和FashionMNIST数据集上的实验结果显示了平滑的插值过渡，但仍存在模糊性问题。当编码器输出多个向量时，模型退化为离散自编码器(VQ-AE)，只能组合图像片段而无法学习语义表示

Conclusion: 研究突出了编码空间紧凑性和分散性在生成建模中的关键作用，提供了对VAE和VQ-VAE内在联系的新见解，为它们的设计和局限性提供了新的视角

Abstract: This paper explores the generative capabilities of Autoencoders (AEs) and
establishes connections between Variational Autoencoders (VAEs) and Vector
Quantized-Variational Autoencoders (VQ-VAEs) through a reformulated training
framework. We demonstrate that AEs exhibit generative potential via latent
space interpolation and perturbation, albeit limited by undefined regions in
the encoding space. To address this, we propose a new VAE-like training method
that introduces clustering centers to enhance data compactness and ensure
well-defined latent spaces without relying on traditional KL divergence or
reparameterization techniques. Experimental results on MNIST, CelebA, and
FashionMNIST datasets show smooth interpolative transitions, though blurriness
persists. Extending this approach to multiple learnable vectors, we observe a
natural progression toward a VQ-VAE-like model in continuous space. However,
when the encoder outputs multiple vectors, the model degenerates into a
discrete Autoencoder (VQ-AE), which combines image fragments without learning
semantic representations. Our findings highlight the critical role of encoding
space compactness and dispersion in generative modeling and provide insights
into the intrinsic connections between VAEs and VQ-VAEs, offering a new
perspective on their design and limitations.

</details>


### [58] [Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational Bottlenecks for Warehouse Planning Assistance](https://arxiv.org/abs/2507.17273)
*Rishi Parekh,Saisubramaniam Gopalakrishnan,Zishan Ahmad,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 本文提出了一个结合知识图谱和大语言模型代理的框架，用于分析仓库离散事件仿真数据，自动识别瓶颈和低效问题，显著提升了分析效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 分析仓库离散事件仿真的大型复杂输出数据集以识别瓶颈和低效问题是一项关键但具有挑战性的任务，通常需要大量手工工作或专业分析工具，亟需一种更直观、自动化的分析方法。

Method: 构建一个集成知识图谱和大语言模型代理的框架：1）将原始DES数据转换为语义丰富的知识图谱，捕获仿真事件和实体间的关系；2）LLM代理使用迭代推理生成相互依赖的子问题；3）为每个子问题创建Cypher查询与知识图谱交互；4）提取信息并进行自我反思纠错，形成自适应、迭代和自纠错的分析过程。

Result: 在设备故障和流程异常的测试中，该方法在仓库瓶颈识别方面优于基线方法。对于操作性问题，在定位低效问题方面达到了近乎完美的通过率。对于复杂的调查性问题，展现出卓越的诊断能力，能够发现微妙的、相互关联的问题。

Conclusion: 这项工作将仿真建模与人工智能（知识图谱+大语言模型）相结合，提供了一种更直观的方法来获得可行的洞察，减少了洞察时间，实现了自动化的仓库低效评估和诊断。

Abstract: Analyzing large, complex output datasets from Discrete Event Simulations
(DES) of warehouse operations to identify bottlenecks and inefficiencies is a
critical yet challenging task, often demanding significant manual effort or
specialized analytical tools. Our framework integrates Knowledge Graphs (KGs)
and Large Language Model (LLM)-based agents to analyze complex Discrete Event
Simulation (DES) output data from warehouse operations. It transforms raw DES
data into a semantically rich KG, capturing relationships between simulation
events and entities. An LLM-based agent uses iterative reasoning, generating
interdependent sub-questions. For each sub-question, it creates Cypher queries
for KG interaction, extracts information, and self-reflects to correct errors.
This adaptive, iterative, and self-correcting process identifies operational
issues mimicking human analysis. Our DES approach for warehouse bottleneck
identification, tested with equipment breakdowns and process irregularities,
outperforms baseline methods. For operational questions, it achieves
near-perfect pass rates in pinpointing inefficiencies. For complex
investigative questions, we demonstrate its superior diagnostic ability to
uncover subtle, interconnected issues. This work bridges simulation modeling
and AI (KG+LLM), offering a more intuitive method for actionable insights,
reducing time-to-insight, and enabling automated warehouse inefficiency
evaluation and diagnosis.

</details>


### [59] [Decentralized Federated Learning of Probabilistic Generative Classifiers](https://arxiv.org/abs/2507.17285)
*Aritz Pérez,Carlos Echegoyen,Guzmán Santafé*

Main category: cs.LG

TL;DR: 提出了一种去中心化联邦学习方法，通过节点间共享局部统计信息来协作学习概率生成分类器，无需中央服务器即可收敛到全局竞争性模型


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习主要依赖中央服务器，而去中心化架构可以让用户直接协作更新全局模型，避免中央服务器依赖，同时保护私有数据不被共享

Method: 设计了一个通信网络框架，其中每个本地节点拥有自己的数据，通过与邻居节点共享局部统计信息，各节点聚合邻居信息并迭代学习自己的局部分类器，最终收敛到全局模型

Result: 广泛的实验表明，该算法在各种网络拓扑、网络规模、本地数据集大小以及极端非独立同分布数据分布下都能一致收敛到全局竞争性模型

Conclusion: 提出的去中心化联邦学习方法能够有效地在异构用户网络中学习概率生成分类器，通过局部统计信息共享实现全局模型收敛，在多种实验条件下表现稳定

Abstract: Federated learning is a paradigm of increasing relevance in real world
applications, aimed at building a global model across a network of
heterogeneous users without requiring the sharing of private data. We focus on
model learning over decentralized architectures, where users collaborate
directly to update the global model without relying on a central server. In
this context, the current paper proposes a novel approach to collaboratively
learn probabilistic generative classifiers with a parametric form. The
framework is composed by a communication network over a set of local nodes,
each of one having its own local data, and a local updating rule. The proposal
involves sharing local statistics with neighboring nodes, where each node
aggregates the neighbors' information and iteratively learns its own local
classifier, which progressively converges to a global model. Extensive
experiments demonstrate that the algorithm consistently converges to a globally
competitive model across a wide range of network topologies, network sizes,
local dataset sizes, and extreme non-i.i.d. data distributions.

</details>


### [60] [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
*Zhuokun Chen,Zeren Chen,Jiahao He,Mingkui Tan,Jianfei Cai,Bohan Zhuang*

Main category: cs.LG

TL;DR: R-Stitch是一个基于置信度的混合解码框架，通过在小模型和大模型之间智能切换来加速思维链推理，在数学推理任务上实现最高85%的推理延迟减少，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽然能提升大语言模型的问题解决能力，但由于需要自回归解码长序列而带来巨大的计算开销。现有加速策略存在局限性：推测解码在大小模型一致性低时加速有限，且未能充分利用小模型生成简洁中间推理的优势。

Method: 提出R-Stitch，一个token级别、基于置信度的混合解码框架。默认使用小语言模型生成tokens，仅当小模型置信度低于阈值时才切换到大语言模型。该设计避免了全序列回滚，选择性地在不确定步骤调用大模型，同时保持效率和答案质量。该方法与模型无关、无需训练、兼容标准解码管道。

Result: 在数学推理基准测试中，R-Stitch实现了最高85%的推理延迟减少，同时准确性下降微乎其微，证明了其在加速思维链推理方面的实用有效性。

Conclusion: R-Stitch通过智能的大小模型切换策略，成功解决了思维链推理中的计算效率问题，在保持高准确性的同时显著减少了推理时间，为大语言模型的实际应用提供了有价值的加速方案。

Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing acceleration strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the LLM only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the LLM on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.

</details>


### [61] [Confounded Causal Imitation Learning with Instrumental Variables](https://arxiv.org/abs/2507.17309)
*Yan Zeng,Shenglan Nie,Feng Xie,Libo Huang,Peng Wu,Zhi Geng*

Main category: cs.LG

TL;DR: 该论文提出了一个混淆因果模仿学习(C2L)模型，通过工具变量方法解决模仿学习中未测量混淆变量导致的偏差问题，包含两阶段框架：IV识别和策略优化。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习受到未测量混淆变量的影响，这些变量会对状态和动作产生混淆效应，如果忽略它们会导致策略估计出现偏差。现有方法无法有效处理跨多个时间步影响动作的混淆因子。

Method: 提出混淆因果模仿学习(C2L)模型，结合工具变量(IV)的强大能力。开发了两阶段模仿学习框架：第一阶段基于定义的伪变量构建测试准则来识别有效的工具变量，该准则包含IV有效性的充分必要可识别条件；第二阶段利用识别出的IV提出两种候选策略学习方法，一种基于模拟器，另一种是离线方法。

Result: 大量实验验证了识别有效工具变量以及学习策略的有效性。C2L模型能够处理影响多个时间步动作的混淆因子，而不仅限于即时时间依赖关系。

Conclusion: 该研究成功解决了模仿学习中未测量混淆变量导致的偏差问题，通过工具变量方法实现了更准确的策略学习。两阶段框架为处理复杂时间依赖的混淆因子提供了有效解决方案。

Abstract: Imitation learning from demonstrations usually suffers from the confounding
effects of unmeasured variables (i.e., unmeasured confounders) on the states
and actions. If ignoring them, a biased estimation of the policy would be
entailed. To break up this confounding gap, in this paper, we take the best of
the strong power of instrumental variables (IV) and propose a Confounded Causal
Imitation Learning (C2L) model. This model accommodates confounders that
influence actions across multiple timesteps, rather than being restricted to
immediate temporal dependencies. We develop a two-stage imitation learning
framework for valid IV identification and policy optimization. In particular,
in the first stage, we construct a testing criterion based on the defined
pseudo-variable, with which we achieve identifying a valid IV for the C2L
models. Such a criterion entails the sufficient and necessary identifiability
conditions for IV validity. In the second stage, with the identified IV, we
propose two candidate policy learning approaches: one is based on a simulator,
while the other is offline. Extensive experiments verified the effectiveness of
identifying the valid IV as well as learning the policy.

</details>


### [62] [EarthLink: Interpreting Climate Signals with Self-Evolving AI Agents](https://arxiv.org/abs/2507.17311)
*Zijie Guo,Jiong Wang,Xiaoyu Yue,Wangxu Wei,Zhe Jiang,Wanghan Xu,Ben Fei,Wenlong Zhang,Xinyu Gu,Lijing Cheng,Jing-Jia Luo,Chao Li,Yaqiang Wang,Tao Chen,Wanli Ouyang,Fenghua Ling,Lei Bai*

Main category: cs.LG

TL;DR: EarthLink是首个专为地球科学家设计的AI智能助手，能够自动化完成从规划到分析的端到端研究流程，通过动态反馈循环持续学习，在气候变化相关任务中表现出与初级研究员相当的分析能力。


<details>
  <summary>Details</summary>
Motivation: 现代地球科学面临瓶颈：地球系统数据庞大、分散且复杂，加上日益复杂的分析需求，严重阻碍了快速科学发现的进程。

Method: 开发了EarthLink AI智能体，作为地球科学家的交互式副驾驶，能够自动化端到端研究工作流程（包括规划、代码生成、多场景分析），具备从用户交互中学习的能力，通过动态反馈循环持续改进。

Result: 在气候变化核心科学任务（从模型-观测比较到复杂现象诊断）上验证了性能。多专家评估显示EarthLink产生了科学合理的分析结果，分析能力在特定方面可与人类初级研究员的工作流程相媲美。

Conclusion: EarthLink标志着地球系统研究向高效、可信、协作范式迈出的关键一步，其透明可审计的工作流程和自然语言界面使科学家能够从繁重的手动执行转向战略监督和假设生成，适应全球变化加速的时代需求。

Abstract: Modern Earth science is at an inflection point. The vast, fragmented, and
complex nature of Earth system data, coupled with increasingly sophisticated
analytical demands, creates a significant bottleneck for rapid scientific
discovery. Here we introduce EarthLink, the first AI agent designed as an
interactive copilot for Earth scientists. It automates the end-to-end research
workflow, from planning and code generation to multi-scenario analysis. Unlike
static diagnostic tools, EarthLink can learn from user interaction,
continuously refining its capabilities through a dynamic feedback loop. We
validated its performance on a number of core scientific tasks of climate
change, ranging from model-observation comparisons to the diagnosis of complex
phenomena. In a multi-expert evaluation, EarthLink produced scientifically
sound analyses and demonstrated an analytical competency that was rated as
comparable to specific aspects of a human junior researcher's workflow.
Additionally, its transparent, auditable workflows and natural language
interface empower scientists to shift from laborious manual execution to
strategic oversight and hypothesis generation. EarthLink marks a pivotal step
towards an efficient, trustworthy, and collaborative paradigm for Earth system
research in an era of accelerating global change.

</details>


### [63] [A Learning-based Domain Decomposition Method](https://arxiv.org/abs/2507.17328)
*Rui Wu,Nikola Kovachki,Burigede Liu*

Main category: cs.LG

TL;DR: 提出了一种基于学习的域分解方法(L-DDM)，使用预训练的神经算子在域分解框架内作为代理模型，能够高效处理复杂几何形状的大规模PDE问题，在椭圆PDE求解中展现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法如有限元方法在处理大规模复杂几何问题时面临计算成本和可扩展性挑战，而现有神经网络方法主要局限于简单域，难以应用于涉及复杂几何的实际PDE问题。

Method: 提出基于学习的域分解方法(L-DDM)，在域分解方案中使用单个预训练神经算子（原本在简单域上训练）作为代理模型，结合物理预训练神经算子(PPNO)来处理复杂域的PDE求解问题。

Result: 该方法在具有不连续微结构的复杂几何椭圆PDE问题上超越了当前最先进方法，同时具有分辨率不变性和对训练时未见微结构模式的强泛化能力。

Conclusion: L-DDM方法成功弥合了简单域训练的神经算子与复杂几何实际应用之间的差距，为大规模复杂PDE问题提供了高效且具有理论保证的解决方案。

Abstract: Recent developments in mechanical, aerospace, and structural engineering have
driven a growing need for efficient ways to model and analyse structures at
much larger and more complex scales than before. While established numerical
methods like the Finite Element Method remain reliable, they often struggle
with computational cost and scalability when dealing with large and
geometrically intricate problems. In recent years, neural network-based methods
have shown promise because of their ability to efficiently approximate
nonlinear mappings. However, most existing neural approaches are still largely
limited to simple domains, which makes it difficult to apply to real-world PDEs
involving complex geometries. In this paper, we propose a learning-based domain
decomposition method (L-DDM) that addresses this gap. Our approach uses a
single, pre-trained neural operator-originally trained on simple domains-as a
surrogate model within a domain decomposition scheme, allowing us to tackle
large and complicated domains efficiently. We provide a general theoretical
result on the existence of neural operator approximations in the context of
domain decomposition solution of abstract PDEs. We then demonstrate our method
by accurately approximating solutions to elliptic PDEs with discontinuous
microstructures in complex geometries, using a physics-pretrained neural
operator (PPNO). Our results show that this approach not only outperforms
current state-of-the-art methods on these challenging problems, but also offers
resolution-invariance and strong generalization to microstructural patterns
unseen during training.

</details>


### [64] [DeCo-SGD: Joint Optimization of Delay Staleness and Gradient Compression Ratio for Distributed SGD](https://arxiv.org/abs/2507.17346)
*Rongwei Lu,Jingyan Jiang,Chunyang Li,Haotian Dong,Xingguang Wei,Delin Cai,Zhi Wang*

Main category: cs.LG

TL;DR: 本文提出DeCo-SGD算法，通过动态调整梯度压缩比和延迟同步步数来解决分布式机器学习在高延迟、低带宽网络环境下的吞吐量下降问题，相比传统方法实现了最高5.07倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习在高延迟、低变化带宽的网络环境中面临严重的吞吐量下降问题。现有方法通常结合梯度压缩和延迟聚合来解决低带宽和高延迟问题，但这引入了压缩比、延迟步数和模型收敛率之间复杂的三方权衡。由于缺乏理论指导，现有工作依赖静态启发式策略，无法在变化的带宽条件下实现动态平衡。

Method: 提出新的理论工具，将联合优化问题分解为传统收敛率分析与多个可分析噪声项。首次揭示了延迟会指数级放大梯度压缩对训练性能的负面影响。通过将收敛率与网络感知的时间最小化条件相结合，提出DeCo-SGD算法，根据实时网络条件和训练任务动态调整压缩比和延迟步数。

Result: DeCo-SGD在高延迟、低变化带宽网络中相比分布式SGD实现了5.07倍的速度提升，相比静态策略实现了1.37倍的速度提升。该方法能够在变化的网络条件下动态适应并优化训练性能。

Conclusion: 本研究填补了压缩和延迟梯度对训练影响的理论空白，首次揭示了延迟对梯度压缩负面影响的指数级放大效应。提出的DeCo-SGD算法通过理论指导的动态参数调整，有效解决了分布式机器学习在挑战性网络环境中的性能问题，为实际应用提供了重要的理论基础和实用解决方案。

Abstract: Distributed machine learning in high end-to-end latency and low, varying
bandwidth network environments undergoes severe throughput degradation. Due to
its low communication requirements, distributed SGD (D-SGD) remains the
mainstream optimizer in such challenging networks, but it still suffers from
significant throughput reduction. To mitigate these limitations, existing
approaches typically employ gradient compression and delayed aggregation to
alleviate low bandwidth and high latency, respectively. To address both
challenges simultaneously, these strategies are often combined, introducing a
complex three-way trade-off among compression ratio, staleness (delayed
synchronization steps), and model convergence rate. To achieve the balance
under varying bandwidth conditions, an adaptive policy is required to
dynamically adjust these parameters. Unfortunately, existing works rely on
static heuristic strategies due to the lack of theoretical guidance, which
prevents them from achieving this goal. This study fills in this theoretical
gap by introducing a new theoretical tool, decomposing the joint optimization
problem into a traditional convergence rate analysis with multiple analyzable
noise terms. We are the first to reveal that staleness exponentially amplifies
the negative impact of gradient compression on training performance, filling a
critical gap in understanding how compressed and delayed gradients affect
training. Furthermore, by integrating the convergence rate with a network-aware
time minimization condition, we propose DeCo-SGD, which dynamically adjusts the
compression ratio and staleness based on the real-time network condition and
training task. DeCo-SGD achieves up to 5.07 and 1.37 speed-ups over D-SGD and
static strategy in high-latency and low, varying bandwidth networks,
respectively.

</details>


### [65] [TOC-UCO: a comprehensive repository of tabular ordinal classification datasets](https://arxiv.org/abs/2507.17348)
*Rafael Ayllón-Gavilán,David Guijo-Rubio,Antonio Manuel Gómez-Orellana,David Guijo-Rubio,Francisco Bérchez-Moreno,Víctor Manuel Vargas-Yun,Pedro A. Gutiérrez*

Main category: cs.LG

TL;DR: 本文介绍了TOC-UCO数据集库，这是一个包含46个表格序数分类数据集的公开可用资源库，旨在为序数分类方法提供标准化的基准测试平台。


<details>
  <summary>Details</summary>
Motivation: 序数分类领域缺乏一个全面的数据集集合来对新方法进行基准测试，这阻碍了该领域的发展。现有的序数分类方法需要在统一、标准化的数据集上进行验证和比较。

Method: 构建了TOC-UCO数据集库，包含46个表格序数分类数据集，在统一框架下进行预处理，确保合理的样本数量和适当的类别分布。提供数据源、预处理步骤的详细信息，以及30个不同随机训练-测试分割的索引以促进实验的可重复性。

Result: 成功创建了一个公开可用的TOC-UCO数据集库，包含46个经过预处理的表格序数分类数据集，每个数据集都具有合理的样本数量和适当的类别分布，并提供了完整的数据源和预处理信息。

Conclusion: TOC-UCO数据集库为序数分类领域提供了一个标准化的基准测试平台，有助于新方法的鲁棒验证和比较，推动序数分类领域的发展。通过提供统一的数据集和实验设置，提高了实验的可重复性和方法比较的公平性。

Abstract: An ordinal classification (OC) problem corresponds to a special type of
classification characterised by the presence of a natural order relationship
among the classes. This type of problem can be found in a number of real-world
applications, motivating the design and development of many ordinal
methodologies over the last years. However, it is important to highlight that
the development of the OC field suffers from one main disadvantage: the lack of
a comprehensive set of datasets on which novel approaches to the literature
have to be benchmarked. In order to approach this objective, this manuscript
from the University of C\'ordoba (UCO), which have previous experience on the
OC field, provides the literature with a publicly available repository of
tabular data for a robust validation of novel OC approaches, namely TOC-UCO
(Tabular Ordinal Classification repository of the UCO). Specifically, this
repository includes a set of $46$ tabular ordinal datasets, preprocessed under
a common framework and ensured to have a reasonable number of patterns and an
appropriate class distribution. We also provide the sources and preprocessing
steps of each dataset, along with details on how to benchmark a novel approach
using the TOC-UCO repository. For this, indices for $30$ different randomised
train-test partitions are provided to facilitate the reproducibility of the
experiments.

</details>


### [66] [DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2507.17365)
*Chuzhan Hao,Wenfeng Feng,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: 提出了DynaSearcher，一个基于动态知识图谱和多奖励强化学习的搜索代理，用于解决大语言模型在复杂信息检索任务中的事实不一致和搜索效率问题


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的多步骤代理检索系统在复杂信息搜索任务中表现出色，但仍面临生成事实不一致的中间查询和低效搜索轨迹的挑战，导致推理偏差或冗余计算

Method: 提出DynaSearcher系统，利用知识图谱作为外部结构化知识指导搜索过程，通过显式建模实体关系确保中间查询的事实一致性；采用多奖励强化学习框架对检索准确性、效率和响应质量等训练目标进行细粒度控制

Result: 在六个多跳问答数据集上达到了最先进的答案准确性，仅使用小规模模型和有限计算资源就能匹配前沿大语言模型的性能；在不同检索环境和更大规模模型上展现出强泛化能力和鲁棒性

Conclusion: DynaSearcher通过结合动态知识图谱和多奖励强化学习，有效解决了多步骤检索系统中的关键问题，实现了高质量、高效率的信息检索，具有广泛的适用性

Abstract: Multi-step agentic retrieval systems based on large language models (LLMs)
have demonstrated remarkable performance in complex information search tasks.
However, these systems still face significant challenges in practical
applications, particularly in generating factually inconsistent intermediate
queries and inefficient search trajectories, which can lead to reasoning
deviations or redundant computations. To address these issues, we propose
DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs
and multi-reward reinforcement learning (RL). Specifically, our system
leverages knowledge graphs as external structured knowledge to guide the search
process by explicitly modeling entity relationships, thereby ensuring factual
consistency in intermediate queries and mitigating biases from irrelevant
information. Furthermore, we employ a multi-reward RL framework for
fine-grained control over training objectives such as retrieval accuracy,
efficiency, and response quality. This framework promotes the generation of
high-quality intermediate queries and comprehensive final answers, while
discouraging unnecessary exploration and minimizing information omissions or
redundancy. Experimental results demonstrate that our approach achieves
state-of-the-art answer accuracy on six multi-hop question answering datasets,
matching frontier LLMs while using only small-scale models and limited
computational resources. Furthermore, our approach demonstrates strong
generalization and robustness across diverse retrieval environments and
larger-scale models, highlighting its broad applicability.

</details>


### [67] [ViRN: Variational Inference and Distribution Trilateration for Long-Tailed Continual Representation Learning](https://arxiv.org/abs/2507.17368)
*Hao Dai,Chong Tang,Jagmohan Chauhan*

Main category: cs.LG

TL;DR: 提出了ViRN框架，结合变分推理和分布三角测量技术解决持续学习中长尾数据分布问题，在六个基准测试中平均准确率提升10.24%


<details>
  <summary>Details</summary>
Motivation: 现实世界AI系统面临持续学习中的长尾数据分布挑战，模型需要在严重类别不平衡的情况下顺序适应新类别同时保持对旧类别的知识，现有方法难以平衡稳定性和可塑性，在极端样本稀缺情况下容易崩溃

Method: 提出ViRN框架，集成变分推理与分布三角测量：1）通过变分自编码器建模类条件分布以减轻对头部类别的偏见；2）通过基于Wasserstein距离的邻域检索和几何融合重构尾部类别分布，实现尾部类别表示的样本高效对齐

Result: 在六个长尾分类基准测试（包括语音任务如稀有声学事件、口音识别和图像任务）上评估，ViRN相比最先进方法平均准确率提升10.24%

Conclusion: ViRN框架通过结合变分推理和分布三角测量技术，有效解决了持续学习中长尾数据分布的挑战，在多个基准测试中显著提升了性能，为处理现实世界中类别不平衡的持续学习问题提供了有效解决方案

Abstract: Continual learning (CL) with long-tailed data distributions remains a
critical challenge for real-world AI systems, where models must sequentially
adapt to new classes while retaining knowledge of old ones, despite severe
class imbalance. Existing methods struggle to balance stability and plasticity,
often collapsing under extreme sample scarcity. To address this, we propose
ViRN, a novel CL framework that integrates variational inference (VI) with
distributional trilateration for robust long-tailed learning. First, we model
class-conditional distributions via a Variational Autoencoder to mitigate bias
toward head classes. Second, we reconstruct tail-class distributions via
Wasserstein distance-based neighborhood retrieval and geometric fusion,
enabling sample-efficient alignment of tail-class representations. Evaluated on
six long-tailed classification benchmarks, including speech (e.g., rare
acoustic events, accents) and image tasks, ViRN achieves a 10.24% average
accuracy gain over state-of-the-art methods.

</details>


### [68] [Continual Generalized Category Discovery: Learning and Forgetting from a Bayesian Perspective](https://arxiv.org/abs/2507.17382)
*Hao Dai,Jagmohan Chauhan*

Main category: cs.LG

TL;DR: 本文提出了变分贝叶斯持续广义类别发现(VB-CGCD)方法，通过协方差感知的最近类均值分类和变分推理来解决持续学习中的灾难性遗忘问题，在标准基准上实现了15.21%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的持续广义类别发现(C-GCD)方法在从无标签数据流中增量学习新类别时面临灾难性遗忘的严重问题，特别是当无标签数据混合了已知和新颖类别时。需要在学习新类别的同时保持对旧类别的知识。

Method: 通过贝叶斯视角分析C-GCD的遗忘动态，发现新旧类别间的协方差不对齐是性能下降的主要原因。基于此洞察，提出了变分贝叶斯C-GCD(VB-CGCD)框架，将变分推理与协方差感知的最近类均值分类相结合，通过随机变分更新自适应对齐类别分布并抑制伪标签噪声。

Result: 在标准基准测试中，VB-CGCD在最终会话的整体准确率上超越现有技术15.21%。在新提出的挑战性基准(仅10%标签数据和扩展在线阶段)上，VB-CGCD达到67.86%的最终准确率，显著高于最先进方法的38.55%。

Conclusion: VB-CGCD通过变分推理和协方差感知分类有效解决了持续广义类别发现中的灾难性遗忘问题，在多种场景下展现了强大的适用性和显著的性能提升，为持续学习领域提供了新的解决方案。

Abstract: Continual Generalized Category Discovery (C-GCD) faces a critical challenge:
incrementally learning new classes from unlabeled data streams while preserving
knowledge of old classes. Existing methods struggle with catastrophic
forgetting, especially when unlabeled data mixes known and novel categories. We
address this by analyzing C-GCD's forgetting dynamics through a Bayesian lens,
revealing that covariance misalignment between old and new classes drives
performance degradation. Building on this insight, we propose Variational Bayes
C-GCD (VB-CGCD), a novel framework that integrates variational inference with
covariance-aware nearest-class-mean classification. VB-CGCD adaptively aligns
class distributions while suppressing pseudo-label noise via stochastic
variational updates. Experiments show VB-CGCD surpasses prior art by +15.21%
with the overall accuracy in the final session on standard benchmarks. We also
introduce a new challenging benchmark with only 10% labeled data and extended
online phases, VB-CGCD achieves a 67.86% final accuracy, significantly higher
than state-of-the-art (38.55%), demonstrating its robust applicability across
diverse scenarios. Code is available at: https://github.com/daihao42/VB-CGCD

</details>


### [69] [A Comprehensive Evaluation on Quantization Techniques for Large Language Models](https://arxiv.org/abs/2507.17417)
*Yutong Liu,Cairong Zhao,Guosheng Hu*

Main category: cs.LG

TL;DR: 这篇论文对大语言模型的训练后量化(PTQ)方法进行了全面评估和理论分析，将量化方法解耦为预量化变换和量化误差缓解两个步骤，并在统一基准下公平比较了各种方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型量化方法研究缺乏统一的评估标准，不同论文的实验条件不一致，且缺乏对现有方法理论联系的深入分析，因此需要进行公平全面的比较研究。

Method: 将现有量化方法解耦为两个步骤：1)预量化变换-在量化前进行预处理以减少异常值影响，使数据分布更平坦；2)量化误差缓解-采用技术来抵消量化过程中引入的误差。在统一基准下评估各种组件的影响，并分析最新的MXFP4数据格式。

Result: 实验结果表明：1)优化的旋转和缩放是预量化变换的最佳方法；2)低秩补偿与GPTQ结合有时优于单独使用GPTQ进行量化误差缓解；3)针对INT4的最优预量化变换策略不能很好地推广到MXFP4格式。

Conclusion: 通过系统性的解耦分析和公平比较，该研究为大语言模型量化方法提供了深入理解，发现了最优的组件组合，并揭示了不同量化格式之间策略的差异性，为未来量化研究提供了重要指导。

Abstract: For large language models (LLMs), post-training quantization (PTQ) can
significantly reduce memory footprint and computational overhead. Model
quantization is a rapidly evolving research field. Though many papers have
reported breakthrough performance, they may not conduct experiments on the same
ground since one quantization method usually contains multiple components. In
addition, analyzing the theoretical connections among existing methods is
crucial for in-depth understanding. To bridge these gaps, we conduct an
extensive review of state-of-the-art methods and perform comprehensive
evaluations on the same ground to ensure fair comparisons. To our knowledge,
this fair and extensive investigation remains critically important yet
underexplored. To better understand the theoretical connections, we decouple
the published quantization methods into two steps: pre-quantization
transformation and quantization error mitigation. We define the former as a
preprocessing step applied before quantization to reduce the impact of
outliers, making the data distribution flatter and more suitable for
quantization. Quantization error mitigation involves techniques that offset the
errors introduced during quantization, thereby enhancing model performance. We
evaluate and analyze the impact of different components of quantization
methods. Additionally, we analyze and evaluate the latest MXFP4 data format and
its performance. Our experimental results demonstrate that optimized rotation
and scaling yield the best performance for pre-quantization transformation, and
combining low-rank compensation with GPTQ occasionally outperforms using GPTQ
alone for quantization error mitigation. Furthermore, we explore the potential
of the latest MXFP4 quantization and reveal that the optimal pre-quantization
transformation strategy for INT4 does not generalize well to MXFP4, inspiring
further investigation.

</details>


### [70] [Persistent Patterns in Eye Movements: A Topological Approach to Emotion Recognition](https://arxiv.org/abs/2507.17450)
*Arsha Niksa,Hooman Zare,Ali Shahrabi,Hanieh Hatami,Mohammadreza Razvan*

Main category: cs.LG

TL;DR: 本文提出了一种基于拓扑数据分析的眼动追踪情感识别方法，通过持续同调分析注视轨迹的延迟嵌入，从持续图中提取形状特征，使用随机森林分类器在四类情感上达到75.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的眼动情感识别方法可能无法充分捕捉注视轨迹的复杂几何结构和动态特性，需要一种新的方法来更好地编码眼动数据中的判别性信息用于情感识别。

Method: 采用拓扑管道进行自动化多类情感识别：1）对眼动追踪数据的注视轨迹进行延迟嵌入；2）使用持续同调分析嵌入后的数据；3）从生成的持续图中提取基于形状的特征，包括平均持续性、最大持续性和熵；4）使用随机森林分类器对提取的特征进行训练和分类。

Result: 在基于情感环模型四个象限的四类情感分类任务中，该方法达到了75.6%的准确率，证明了持续图几何结构能够有效编码具有判别性的注视动态特征。

Conclusion: 持续图几何结构能够有效编码眼动数据中的判别性注视动态信息，表明拓扑方法在情感计算和人类行为分析领域具有良好的应用前景。

Abstract: We present a topological pipeline for automated multiclass emotion
recognition from eye-tracking data. Delay embeddings of gaze trajectories are
analyzed using persistent homology. From the resulting persistence diagrams, we
extract shape-based features such as mean persistence, maximum persistence, and
entropy. A random forest classifier trained on these features achieves up to
$75.6\%$ accuracy on four emotion classes, which are the quadrants the
Circumplex Model of Affect. The results demonstrate that persistence diagram
geometry effectively encodes discriminative gaze dynamics, suggesting a
promising topological approach for affective computing and human behavior
analysis.

</details>


### [71] [Efficient Neural Network Verification via Order Leading Exploration of Branch-and-Bound Trees](https://arxiv.org/abs/2507.17453)
*Guanqin Zhang,Kota Fukuda,Zhenya Zhang,H. M. N. Dilum Bandara,Shiping Chen,Jianjun Zhao,Yulei Sui*

Main category: cs.LG

TL;DR: 本文提出了Oliva框架，通过优化分支定界(BaB)算法中子问题的探索顺序来提高神经网络形式化验证的效率，在MNIST和CIFAR10数据集上实现了最高25倍和80倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的分支定界验证方法虽然能识别需要分割的子问题，但采用"先来先服务"的简单策略探索子问题空间，导致验证效率低下，难以快速得出验证结论。

Method: 提出Oliva验证框架，通过引入子问题包含反例可能性的排序机制，优先探索更可能找到反例的子问题。包含两个变体：Oliva^GR（贪心策略，总是优先处理最可能找到反例的子问题）和Oliva^SA（基于模拟退火的平衡策略，从探索逐渐转向利用）。

Result: 在涵盖5个模型、690个验证问题的MNIST和CIFAR10数据集上进行实验评估，与最先进方法相比，Oliva在MNIST上实现了最高25倍的加速，在CIFAR10上实现了最高80倍的加速。

Conclusion: Oliva框架通过智能的子问题优先级排序显著提高了神经网络形式化验证的效率，即使在找不到反例的情况下也不会导致性能下降，为神经网络验证提供了更高效的解决方案。

Abstract: The vulnerability of neural networks to adversarial perturbations has
necessitated formal verification techniques that can rigorously certify the
quality of neural networks. As the state-of-the-art, branch and bound (BaB) is
a "divide-and-conquer" strategy that applies off-the-shelf verifiers to
sub-problems for which they perform better. While BaB can identify the
sub-problems that are necessary to be split, it explores the space of these
sub-problems in a naive "first-come-first-serve" manner, thereby suffering from
an issue of inefficiency to reach a verification conclusion. To bridge this
gap, we introduce an order over different sub-problems produced by BaB,
concerning with their different likelihoods of containing counterexamples.
Based on this order, we propose a novel verification framework Oliva that
explores the sub-problem space by prioritizing those sub-problems that are more
likely to find counterexamples, in order to efficiently reach the conclusion of
the verification. Even if no counterexample can be found in any sub-problem, it
only changes the order of visiting different sub-problem and so will not lead
to a performance degradation. Specifically, Oliva has two variants, including
$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that
are more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy
inspired by simulated annealing that gradually shifts from exploration to
exploitation to locate the globally optimal sub-problems. We experimentally
evaluate the performance of Oliva on 690 verification problems spanning over 5
models with datasets MNIST and CIFAR10. Compared to the state-of-the-art
approaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up
to 80X in CIFAR10.

</details>


### [72] [C3RL: Rethinking the Combination of Channel-independence and Channel-mixing from Representation Learning](https://arxiv.org/abs/2507.17454)
*Shusen Ma,Yun-Bo Zhao,Yu Kang*

Main category: cs.LG

TL;DR: 提出了C3RL框架，通过对比学习联合建模通道混合(CM)和通道独立(CI)策略，显著提升多变量时间序列预测性能


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测方法存在局限性：CM策略能捕获变量间依赖但无法识别变量特定的时间模式；CI策略改善了这一点但无法充分利用跨变量依赖；基于特征融合的混合策略泛化能力和可解释性有限

Method: 提出C3RL表示学习框架，受计算机视觉对比学习启发，将CM和CI策略的输入视为转置视图，构建孪生网络架构：一种策略作为主干，另一种作为补充。通过自适应加权联合优化对比损失和预测损失来平衡表示和预测性能

Result: 在七个模型上的大量实验表明，C3RL将基于CI策略的模型最佳性能率提升至81.4%，将基于CM策略的模型最佳性能率提升至76.3%，展现出强大的泛化能力和有效性

Conclusion: C3RL框架成功解决了现有方法的局限性，通过联合建模CM和CI策略实现了显著的性能提升，为多变量时间序列预测提供了一个有效且通用的解决方案

Abstract: Multivariate time series forecasting has drawn increasing attention due to
its practical importance. Existing approaches typically adopt either
channel-mixing (CM) or channel-independence (CI) strategies. CM strategy can
capture inter-variable dependencies but fails to discern variable-specific
temporal patterns. CI strategy improves this aspect but fails to fully exploit
cross-variable dependencies like CM. Hybrid strategies based on feature fusion
offer limited generalization and interpretability. To address these issues, we
propose C3RL, a novel representation learning framework that jointly models
both CM and CI strategies. Motivated by contrastive learning in computer
vision, C3RL treats the inputs of the two strategies as transposed views and
builds a siamese network architecture: one strategy serves as the backbone,
while the other complements it. By jointly optimizing contrastive and
prediction losses with adaptive weighting, C3RL balances representation and
forecasting performance. Extensive experiments on seven models show that C3RL
boosts the best-case performance rate to 81.4\% for models based on CI strategy
and to 76.3\% for models based on CM strategy, demonstrating strong
generalization and effectiveness. The code will be available once the paper is
accepted.

</details>


### [73] [BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles](https://arxiv.org/abs/2507.17472)
*Junhua Liu,Roy Ka-Wei Lee,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 本文提出BGM-HAN模型，通过层次化学习增强大学招生等高风险决策场景，有效处理半结构化申请者数据，在真实招生数据上显著优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 人类在高风险决策领域容易受到认知偏见影响，威胁公平性和长期结果。需要通过技术手段增强复杂决策工作流程，特别是在大学招生这类代表性高风险领域。

Method: 提出BGM-HAN（增强型字节对编码门控多头层次注意力网络），集成层次化学习和多种增强技术，专门设计用于建模半结构化申请者数据，捕获多层次表示以支持细致评估。

Result: 在真实招生数据上的实验结果表明，提出的模型显著优于从传统机器学习到大语言模型的各种最先进基线方法，在可解释性和预测性能方面都有提升。

Conclusion: BGM-HAN为结构化、上下文和公平性重要的决策领域提供了有前景的增强决策框架，可有效辅助高风险决策场景中的人类判断。

Abstract: Human decision-making in high-stakes domains often relies on expertise and
heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten
fairness and long-term outcomes. This work presents a novel approach to
enhancing complex decision-making workflows through the integration of
hierarchical learning alongside various enhancements. Focusing on university
admissions as a representative high-stakes domain, we propose BGM-HAN, an
enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,
designed to effectively model semi-structured applicant data. BGM-HAN captures
multi-level representations that are crucial for nuanced assessment, improving
both interpretability and predictive performance. Experimental results on real
admissions data demonstrate that our proposed model significantly outperforms
both state-of-the-art baselines from traditional machine learning to large
language models, offering a promising framework for augmenting decision-making
in domains where structure, context, and fairness matter. Source code is
available at: https://github.com/junhua/bgm-han.

</details>


### [74] [DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD](https://arxiv.org/abs/2507.17501)
*Xianbiao Qi,Marco Chen,Wenjie Xiao,Jiaquan Ye,Yelin He,Chun-Guang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出了深度归一化Transformer(DNT)，通过在关键位置集成归一化技术，使得Transformer能够用传统的动量SGD训练而不需要AdamW等自适应优化器，同时保持相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer训练通常需要AdamW等自适应学习率优化器，而无法使用简单的动量SGD，主要原因是梯度分布呈重尾分布。本文旨在解决这一限制，使Transformer能够用更简单的优化器进行训练。

Method: 提出深度归一化Transformer(DNT)，在Transformer的适当位置策略性地集成归一化技术，有效调节每层的雅可比矩阵，平衡权重、激活及其交互的影响，从而使梯度分布更加集中。

Result: DNT在两种流行的Transformer架构(ViT和GPT)上的实验表明：1) DNT性能优于对应的基线模型；2) DNT可以有效地使用传统的动量SGD进行训练，无需自适应优化器。

Conclusion: 通过精心设计的归一化技术，DNT成功解决了Transformer训练中对自适应优化器的依赖问题，实现了用简单的动量SGD训练高性能Transformer的目标，为Transformer的训练优化提供了新的解决方案。

Abstract: Transformers have become the de facto backbone of modern deep learning, yet
their training typically demands an advanced optimizer with adaptive learning
rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that
it is mainly due to a heavy-tailed distribution of the gradients. In this
paper, we introduce a Deeply Normalized Transformer (DNT), which is
meticulously engineered to overcome this limitation enabling seamless training
with vanilla mSGDW while yielding comparable performance to the Transformers
trained via AdamW. To be specific, in DNT, we strategically integrate
normalization techniques at proper positions in the Transformers to effectively
modulate the Jacobian matrices of each layer, balance the influence of weights,
activations, and their interactions, and thus enable the distributions of
gradients concentrated. We provide both theoretical justifications of the
normalization technique used in our DNT and extensive empirical evaluation on
two popular Transformer architectures to validate that: a) DNT outperforms its
counterparts (\ie, ViT and GPT), and b) DNT can be effectively trained with
vanilla mSGDW.

</details>


### [75] [HOTA: Hamiltonian framework for Optimal Transport Advection](https://arxiv.org/abs/2507.17513)
*Nazar Buzun,Daniil Shlenskii,Maxim Bobrin,Dmitry V. Dylov*

Main category: cs.LG

TL;DR: 本文提出了HOTA方法，通过Hamilton-Jacobi-Bellman方程解决最优传输问题，避免了显式密度建模，在标准和非可微代价函数数据集上都优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型大多假设简单几何（如欧几里得空间）并依赖强密度估计假设，导致生成的轨迹不符合底层流形的真正最优性原则。需要一种能够在复杂流形上进行有效最优传输的方法。

Method: 提出了哈密顿最优传输平流（HOTA）方法，基于Hamilton-Jacobi-Bellman方程，通过Kantorovich势函数显式处理对偶动力学最优传输问题，实现高效可扩展的轨迹优化。

Result: HOTA在标准基准测试和具有非可微代价函数的自定义数据集上都超越了所有基线方法，在可行性和最优性方面表现更优。该方法有效避免了显式密度建模的需求，即使在代价函数非光滑时也能良好工作。

Conclusion: HOTA方法成功解决了传统最优传输方法在复杂流形上的局限性，通过避免密度建模和处理非光滑代价函数，为最优传输问题提供了更加实用和高效的解决方案。

Abstract: Optimal transport (OT) has become a natural framework for guiding the
probability flows. Yet, the majority of recent generative models assume trivial
geometry (e.g., Euclidean) and rely on strong density-estimation assumptions,
yielding trajectories that do not respect the true principles of optimality in
the underlying manifold. We present Hamiltonian Optimal Transport Advection
(HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical
OT problem explicitly through Kantorovich potentials, enabling efficient and
scalable trajectory optimization. Our approach effectively evades the need for
explicit density modeling, performing even when the cost functionals are
non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks,
as well as in custom datasets with non-differentiable costs, both in terms of
feasibility and optimality.

</details>


### [76] [Generalized Low-Rank Matrix Contextual Bandits with Graph Information](https://arxiv.org/abs/2507.17528)
*Yao Wang,Jiannan Li,Yue Kang,Shanxing Gao,Zhenxin Xiao*

Main category: cs.LG

TL;DR: 本文提出了一种新的矩阵上下文赌博机算法框架，该框架能够同时利用低秩结构和图信息来改进在线决策制定，特别适用于在线广告和推荐系统等应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有的矩阵上下文赌博机方法虽然能够处理低秩结构，但在许多实际应用（如在线广告和推荐系统）中，除了低秩结构外还存在额外的图信息（用户/物品之间的相似关系），而现有方法无法有效利用这些图信息，导致决策策略效果不佳。

Method: 基于经典的上置信界（UCB）框架，提出了一种新的矩阵上下文赌博机算法框架。该方法首先求解一个联合核范数和矩阵拉普拉斯正则化问题，然后实现基于图的广义线性UCB算法，从而在统一框架内有效整合低秩结构和图信息。

Result: 理论分析表明，由于有效利用了图信息，该方法在累积遗憾界方面优于几种流行的替代方法。合成数据和真实数据实验进一步验证了该方法的优越性。

Conclusion: 通过将图信息与低秩结构相结合，所提出的矩阵上下文赌博机框架能够显著改善决策制定性能，为在线广告和推荐系统等实际应用提供了更有效的解决方案。

Abstract: The matrix contextual bandit (CB), as an extension of the well-known
multi-armed bandit, is a powerful framework that has been widely applied in
sequential decision-making scenarios involving low-rank structure. In many
real-world scenarios, such as online advertising and recommender systems,
additional graph information often exists beyond the low-rank structure, that
is, the similar relationships among users/items can be naturally captured
through the connectivity among nodes in the corresponding graphs. However,
existing matrix CB methods fail to explore such graph information, and thereby
making them difficult to generate effective decision-making policies. To fill
in this void, we propose in this paper a novel matrix CB algorithmic framework
that builds upon the classical upper confidence bound (UCB) framework. This new
framework can effectively integrate both the low-rank structure and graph
information in a unified manner. Specifically, it involves first solving a
joint nuclear norm and matrix Laplacian regularization problem, followed by the
implementation of a graph-based generalized linear version of the UCB
algorithm. Rigorous theoretical analysis demonstrates that our procedure
outperforms several popular alternatives in terms of cumulative regret bound,
owing to the effective utilization of graph information. A series of synthetic
and real-world data experiments are conducted to further illustrate the merits
of our procedure.

</details>


### [77] [Generalized Advantage Estimation for Distributional Policy Gradients](https://arxiv.org/abs/2507.17530)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: 本文提出了分布式广义优势估计(DGAE)，通过引入类Wasserstein方向性度量来处理分布式强化学习中的价值分布，相比传统GAE能更好地处理系统随机性和噪声


<details>
  <summary>Details</summary>
Motivation: 传统的广义优势估计(GAE)虽然能降低策略梯度估计的方差，但无法处理分布式强化学习中的价值分布。而分布式强化学习能更好地捕捉系统固有的随机性，对系统噪声更加鲁棒。因此需要一种能够处理价值分布的优势估计方法

Method: 利用最优传输理论引入类Wasserstein方向性度量，该度量能同时衡量概率分布之间的距离和方向差异。结合指数加权估计，基于这个方向性度量推导出分布式GAE(DGAE)，为策略梯度算法提供低方差、可控偏差的优势估计

Result: 将DGAE集成到三种不同的策略梯度方法中，在多个OpenAI Gym环境中进行评估，与使用传统GAE的基线方法进行比较，验证了DGAE的性能表现

Conclusion: 提出的DGAE方法成功扩展了传统GAE到分布式强化学习领域，通过类Wasserstein方向性度量实现了对价值分布的有效处理，为分布式强化学习中的策略梯度算法提供了更好的优势估计工具

Abstract: Generalized Advantage Estimation (GAE) has been used to mitigate the
computational complexity of reinforcement learning (RL) by employing an
exponentially weighted estimation of the advantage function to reduce the
variance in policy gradient estimates. Despite its effectiveness, GAE is not
designed to handle value distributions integral to distributional RL, which can
capture the inherent stochasticity in systems and is hence more robust to
system noises. To address this gap, we propose a novel approach that utilizes
the optimal transport theory to introduce a Wasserstein-like directional
metric, which measures both the distance and the directional discrepancies
between probability distributions. Using the exponentially weighted estimation,
we leverage this Wasserstein-like directional metric to derive distributional
GAE (DGAE). Similar to traditional GAE, our proposed DGAE provides a
low-variance advantage estimate with controlled bias, making it well-suited for
policy gradient algorithms that rely on advantage estimation for policy
updates. We integrated DGAE into three different policy gradient methods.
Algorithms were evaluated across various OpenAI Gym environments and compared
with the baselines with traditional GAE to assess the performance.

</details>


### [78] [Enhancing Quantum Federated Learning with Fisher Information-Based Optimization](https://arxiv.org/abs/2507.17580)
*Amandeep Singh Bhatia,Sabre Kais*

Main category: cs.LG

TL;DR: 本文提出了一种基于Fisher信息的量子联邦学习算法，通过识别和保护关键参数来提升分布式量子模型训练的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临通信成本高、客户端数据异构、处理时间长和隐私威胁等挑战。量子联邦学习的兴起为医疗和金融等领域带来了新机遇，但仍需解决上述问题。Fisher信息能够量化量子态在参数变化下携带的信息量，可用于改善这些挑战。

Method: 提出了一种量子联邦学习算法，利用在本地客户端模型上计算的Fisher信息，数据分布在异构分区中。该方法识别对量子模型性能有重大影响的关键参数，并确保这些参数在聚合过程中得到保护。

Result: 在ADNI和MNIST数据集上的实验结果表明，相比量子联邦平均方法，该方法在性能和鲁棒性方面都有显著提升，证明了在量子联邦学习中融入Fisher信息的有效性。

Conclusion: 基于Fisher信息的量子联邦学习算法成功解决了传统联邦学习的关键挑战，通过保护重要参数实现了更好的模型性能和鲁棒性，为量子联邦学习领域提供了新的解决方案。

Abstract: Federated Learning (FL) has become increasingly popular across different
sectors, offering a way for clients to work together to train a global model
without sharing sensitive data. It involves multiple rounds of communication
between the global model and participating clients, which introduces several
challenges like high communication costs, heterogeneous client data, prolonged
processing times, and increased vulnerability to privacy threats. In recent
years, the convergence of federated learning and parameterized quantum circuits
has sparked significant research interest, with promising implications for
fields such as healthcare and finance. By enabling decentralized training of
quantum models, it allows clients or institutions to collaboratively enhance
model performance and outcomes while preserving data privacy. Recognizing that
Fisher information can quantify the amount of information that a quantum state
carries under parameter changes, thereby providing insight into its geometric
and statistical properties. We intend to leverage this property to address the
aforementioned challenges. In this work, we propose a Quantum Federated
Learning (QFL) algorithm that makes use of the Fisher information computed on
local client models, with data distributed across heterogeneous partitions.
This approach identifies the critical parameters that significantly influence
the quantum model's performance, ensuring they are preserved during the
aggregation process. Our research assessed the effectiveness and feasibility of
QFL by comparing its performance against other variants, and exploring the
benefits of incorporating Fisher information in QFL settings. Experimental
results on ADNI and MNIST datasets demonstrate the effectiveness of our
approach in achieving better performance and robustness against the quantum
federated averaging method.

</details>


### [79] [XStacking: Explanation-Guided Stacked Ensemble Learning](https://arxiv.org/abs/2507.17650)
*Moncef Garouani,Ayah Barhrhouj,Olivier Teste*

Main category: cs.LG

TL;DR: 本文提出了XStacking框架，通过集成动态特征转换和Shapley可加性解释，解决了集成机器学习(特别是堆叠方法)缺乏可解释性的问题，在保持预测准确性的同时实现了模型的内在可解释性。


<details>
  <summary>Details</summary>
Motivation: 集成机器学习技术(尤其是堆叠方法)虽然能通过组合多个基础模型来提高预测性能，但经常因缺乏可解释性而受到批评。现有的堆叠模型难以解释其预测决策过程，这限制了它们在需要可解释性的应用场景中的使用。

Method: 提出XStacking框架，该框架通过集成动态特征转换与模型无关的Shapley可加性解释来解决可解释性问题。该方法使堆叠模型能够在保持预测准确性的同时获得内在的可解释性，为负责任的机器学习提供实用且可扩展的解决方案。

Result: 在29个数据集上验证了框架的有效性，结果显示XStacking在学习空间的预测效果和结果模型的可解释性两方面都取得了改进。该框架成功地平衡了预测性能和可解释性之间的权衡。

Conclusion: XStacking框架有效解决了集成学习模型可解释性不足的问题，为负责任的机器学习提供了一个实用且可扩展的解决方案。该方法证明了在不牺牲预测准确性的前提下实现模型可解释性是可能的。

Abstract: Ensemble Machine Learning (EML) techniques, especially stacking, have been
shown to improve predictive performance by combining multiple base models.
However, they are often criticized for their lack of interpretability. In this
paper, we introduce XStacking, an effective and inherently explainable
framework that addresses this limitation by integrating dynamic feature
transformation with model-agnostic Shapley additive explanations. This enables
stacked models to retain their predictive accuracy while becoming inherently
explainable. We demonstrate the effectiveness of the framework on 29 datasets,
achieving improvements in both the predictive effectiveness of the learning
space and the interpretability of the resulting models. XStacking offers a
practical and scalable solution for responsible ML.

</details>


### [80] [How Should We Meta-Learn Reinforcement Learning Algorithms?](https://arxiv.org/abs/2507.17668)
*Alexander David Goldie,Zilin Wang,Jakob Nicolaus Foerster,Shimon Whiteson*

Main category: cs.LG

TL;DR: 本文对不同元学习算法在强化学习中的应用进行了全面的实证比较，包括进化算法优化黑盒函数和大语言模型生成代码等方法，并基于性能、可解释性、样本成本和训练时间等因素提出了元学习新强化学习算法的指导原则。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法通常从监督或无监督学习中改编而来，存在次优性问题。虽然元学习在强化学习中显示出潜力，但不同元学习算法之间缺乏充分的比较研究，需要系统性地评估各种方法的优劣。

Method: 对多种元学习算法进行实证比较，包括使用进化算法优化黑盒函数和使用大语言模型生成代码等方法。评估这些算法在强化学习管道不同部分的表现，考虑元训练和元测试性能、可解释性、样本成本和训练时间等多个维度。

Result: 通过全面的实证研究，揭示了不同元学习算法在强化学习应用中的相对优势和劣势，在性能、效率和可解释性等方面提供了详细的比较分析。

Conclusion: 基于实证研究结果，提出了元学习新强化学习算法的若干指导原则，旨在帮助确保未来学习到的算法能够达到尽可能好的性能表现。

Abstract: The process of meta-learning algorithms from data, instead of relying on
manual design, is growing in popularity as a paradigm for improving the
performance of machine learning systems. Meta-learning shows particular promise
for reinforcement learning (RL), where algorithms are often adapted from
supervised or unsupervised learning despite their suboptimality for RL.
However, until now there has been a severe lack of comparison between different
meta-learning algorithms, such as using evolution to optimise over black-box
functions or LLMs to propose code. In this paper, we carry out this empirical
comparison of the different approaches when applied to a range of meta-learned
algorithms which target different parts of the RL pipeline. In addition to
meta-train and meta-test performance, we also investigate factors including the
interpretability, sample cost and train time for each meta-learning algorithm.
Based on these findings, we propose several guidelines for meta-learning new RL
algorithms which will help ensure that future learned algorithms are as
performant as possible.

</details>


### [81] [Towards Effective Open-set Graph Class-incremental Learning](https://arxiv.org/abs/2507.17687)
*Jiazhen Chen,Zheng Ma,Sichao Fu,Mingbin Feng,Tony S. Wirjanto,Weihua Ou*

Main category: cs.LG

TL;DR: 提出了OGCIL框架，解决开放集图类增量学习问题，通过原型条件变分自编码器生成伪样本嵌入来缓解灾难性遗忘，并使用混合策略和原型超球分类损失来检测未知类别。


<details>
  <summary>Details</summary>
Motivation: 现有图类增量学习方法假设所有测试样本都属于已知类别（封闭集假设），但在真实场景中会出现训练时未见过的未知类别。这种开放集场景面临两个挑战：对旧类别的灾难性遗忘会影响未知类别检测，而不充分的开放集识别会破坏已学知识的保持。

Method: 提出OGCIL框架，包含三个关键组件：1）原型条件变分自编码器合成旧类别的节点嵌入，实现知识重放而无需存储原始图数据；2）基于混合的策略从伪分布内样本和当前节点嵌入生成分布外（OOD）样本；3）原型超球分类损失，将分布内嵌入锚定到各自的类别原型，同时排斥OOD嵌入。

Result: 在五个基准数据集上的广泛实验表明，OGCIL在现有图类增量学习和开放集图神经网络方法上表现出色，有效解决了开放集图类增量学习问题。

Conclusion: OGCIL框架成功解决了开放集图类增量学习的挑战，通过伪样本嵌入生成有效缓解灾难性遗忘，通过原型感知的拒绝区域实现鲁棒的开放集识别，为图神经网络在动态环境中的应用提供了有效解决方案。

Abstract: Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)
to adapt to evolving graph analytical tasks by incrementally learning new class
knowledge while retaining knowledge of old classes. Existing GCIL methods
primarily focus on a closed-set assumption, where all test samples are presumed
to belong to previously known classes. Such an assumption restricts their
applicability in real-world scenarios, where unknown classes naturally emerge
during inference, and are absent during training. In this paper, we explore a
more challenging open-set graph class-incremental learning scenario with two
intertwined challenges: catastrophic forgetting of old classes, which impairs
the detection of unknown classes, and inadequate open-set recognition, which
destabilizes the retention of learned knowledge. To address the above problems,
a novel OGCIL framework is proposed, which utilizes pseudo-sample embedding
generation to effectively mitigate catastrophic forgetting and enable robust
detection of unknown classes. To be specific, a prototypical conditional
variational autoencoder is designed to synthesize node embeddings for old
classes, enabling knowledge replay without storing raw graph data. To handle
unknown classes, we employ a mixing-based strategy to generate
out-of-distribution (OOD) samples from pseudo in-distribution and current node
embeddings. A novel prototypical hypersphere classification loss is further
proposed, which anchors in-distribution embeddings to their respective class
prototypes, while repelling OOD embeddings away. Instead of assigning all
unknown samples into one cluster, our proposed objective function explicitly
models them as outliers through prototype-aware rejection regions, ensuring a
robust open-set recognition. Extensive experiments on five benchmarks
demonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN
methods.

</details>


### [82] [Joint Asymmetric Loss for Learning with Noisy Labels](https://arxiv.org/abs/2507.17692)
*Jialiang Wang,Xianming Liu,Xiong Zhou,Gangfeng Hu,Deming Zhai,Junjun Jiang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文针对带噪声标签学习中对称损失函数存在欠拟合问题，提出了非对称均方误差(AMSE)损失函数，并结合主动被动损失框架构建了联合非对称损失(JAL)框架，在多个实验中展现了优异的抗噪声性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对称损失函数在处理标签噪声时存在过度严格约束导致的欠拟合问题，而新兴的非对称损失函数虽然理论上具有优越性，但无法与先进的优化框架(如APL)兼容，限制了其应用潜力。

Method: 将非对称损失扩展到更复杂的被动损失场景，提出非对称均方误差(AMSE)损失函数，并严格建立了AMSE满足非对称条件的充要条件。通过用AMSE替换APL框架中的传统对称被动损失，构建了联合非对称损失(JAL)框架。

Result: 大量实验证明了该方法在缓解标签噪声方面的有效性，JAL框架相比现有方法在带噪声标签的深度神经网络训练中表现出更好的性能。

Conclusion: 成功将非对称损失函数扩展到主动被动损失优化框架中，提出的JAL方法有效解决了对称损失函数的欠拟合问题，为带噪声标签学习提供了新的鲁棒优化方案。

Abstract: Learning with noisy labels is a crucial task for training accurate deep
neural networks. To mitigate label noise, prior studies have proposed various
robust loss functions, particularly symmetric losses. Nevertheless, symmetric
losses usually suffer from the underfitting issue due to the overly strict
constraint. To address this problem, the Active Passive Loss (APL) jointly
optimizes an active and a passive loss to mutually enhance the overall fitting
ability. Within APL, symmetric losses have been successfully extended, yielding
advanced robust loss functions. Despite these advancements, emerging
theoretical analyses indicate that asymmetric losses, a new class of robust
loss functions, possess superior properties compared to symmetric losses.
However, existing asymmetric losses are not compatible with advanced
optimization frameworks such as APL, limiting their potential and
applicability. Motivated by this theoretical gap and the prospect of asymmetric
losses, we extend the asymmetric loss to the more complex passive loss scenario
and propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We
rigorously establish the necessary and sufficient condition under which AMSE
satisfies the asymmetric condition. By substituting the traditional symmetric
passive loss in APL with our proposed AMSE, we introduce a novel robust loss
framework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate
the effectiveness of our method in mitigating label noise. Code available at:
https://github.com/cswjl/joint-asymmetric-loss

</details>


### [83] [HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter Merging](https://arxiv.org/abs/2507.17706)
*Taha Ceritli,Ondrej Bohdal,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: 本文提出了HydraOpt，一种新的模型合并技术，通过利用低秩适配器矩阵间的相似性，在显著减少存储需求（48%减少）的同时保持竞争性能（仅0.2-1.8%性能下降）。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使用适配器在下游任务上获得强性能，但为每个任务存储单独的适配器会显著增加内存需求，在移动设备等资源受限环境中构成挑战。现有的模型合并技术虽然能减少存储成本，但通常会导致大幅性能下降。

Method: HydraOpt是一种新的模型合并技术，通过利用低秩适配器矩阵之间的固有相似性来工作。与现有方法产生存储大小和性能之间的固定权衡不同，HydraOpt允许在效率和性能的谱系中进行导航。

Result: 实验表明，与存储所有适配器相比，HydraOpt显著减少了存储大小（48%减少），同时实现了竞争性能（0.2-1.8%的性能下降）。在相同或略差的存储效率下，它在性能方面优于现有的合并技术。

Conclusion: HydraOpt成功解决了适配器存储的内存挑战，通过创新的合并技术实现了存储效率和模型性能之间的良好平衡，为资源受限环境中部署大语言模型提供了有效解决方案。

Abstract: Large language models (LLMs) often leverage adapters, such as low-rank-based
adapters, to achieve strong performance on downstream tasks. However, storing a
separate adapter for each task significantly increases memory requirements,
posing a challenge for resource-constrained environments such as mobile
devices. Although model merging techniques can reduce storage costs, they
typically result in substantial performance degradation. In this work, we
introduce HydraOpt, a new model merging technique that capitalizes on the
inherent similarities between the matrices of low-rank adapters. Unlike
existing methods that produce a fixed trade-off between storage size and
performance, HydraOpt allows us to navigate this spectrum of efficiency and
performance. Our experiments show that HydraOpt significantly reduces storage
size (48% reduction) compared to storing all adapters, while achieving
competitive performance (0.2-1.8% drop). Furthermore, it outperforms existing
merging techniques in terms of performance at the same or slightly worse
storage efficiency.

</details>


### [84] [Flow Matching Meets Biology and Life Science: A Survey](https://arxiv.org/abs/2507.17731)
*Zihao Li,Zhichen Zeng,Xiao Lin,Feihao Fang,Yanru Qu,Zhe Xu,Zhining Liu,Xuying Ning,Tianxin Wei,Ge Liu,Hanghang Tong,Jingrui He*

Main category: cs.LG

TL;DR: 这是第一篇全面综述流匹配(flow matching)在生物学领域应用的论文，系统性地回顾了流匹配的基础理论及其在生物序列建模、分子生成设计、肽和蛋白质生成三个主要生物学领域的应用进展。


<details>
  <summary>Details</summary>
Motivation: 近十年来，生成式建模(如GAN、掩码自编码器、扩散模型)显著推动了生物学研究和发现，而流匹配作为扩散模型的强大且高效的替代方案，在生物学和生命科学中的应用日益增长，但缺乏系统性的综述研究。

Method: 采用综述研究方法，首先系统性回顾流匹配的基础理论和变体，然后将其生物学应用分为三个主要领域进行分类整理：生物序列建模、分子生成与设计、肽和蛋白质生成，并对每个领域的最新进展进行深入分析。

Result: 提供了流匹配在生物学领域应用的全面概览，总结了常用的数据集和软件工具，并建立了相应的策划资源库(https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology)供研究者使用。

Conclusion: 流匹配作为一种新兴的生成建模技术，在生物学各个领域展现出巨大潜力，论文讨论了未来可能的发展方向，为该交叉领域的进一步研究提供了重要参考。

Abstract: Over the past decade, advances in generative modeling, such as generative
adversarial networks, masked autoencoders, and diffusion models, have
significantly transformed biological research and discovery, enabling
breakthroughs in molecule design, protein generation, drug discovery, and
beyond. At the same time, biological applications have served as valuable
testbeds for evaluating the capabilities of generative models. Recently, flow
matching has emerged as a powerful and efficient alternative to diffusion-based
generative modeling, with growing interest in its application to problems in
biology and life sciences. This paper presents the first comprehensive survey
of recent developments in flow matching and its applications in biological
domains. We begin by systematically reviewing the foundations and variants of
flow matching, and then categorize its applications into three major areas:
biological sequence modeling, molecule generation and design, and peptide and
protein generation. For each, we provide an in-depth review of recent progress.
We also summarize commonly used datasets and software tools, and conclude with
a discussion of potential future directions. The corresponding curated
resources are available at
https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.

</details>


### [85] [Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains](https://arxiv.org/abs/2507.17746)
*Anisha Gunjal,Anthony Wang,Elaine Lau,Vaskar Nath,Bing Liu,Sean Hendryx*

Main category: cs.LG

TL;DR: 本文提出了"Rubrics as Rewards"(RaR)框架，使用结构化清单式评分标准作为可解释的奖励信号来训练语言模型，在HealthBench-1k上相比简单Likert方法获得了28%的相对改进。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务中的强化学习往往缺乏明确的真实标准，难以为训练后的语言模型定义可靠的奖励信号。传统基于偏好的方法依赖不透明且容易产生虚假关联的奖励函数，需要更可解释和可靠的评估方法。

Method: 提出"Rubrics as Rewards"(RaR)框架，使用结构化的清单式评分标准作为可解释的奖励信号，结合GRPO进行在线策略训练。将评分标准视为结构化奖励信号，使小规模判断模型能够更好地与人类偏好对齐。

Result: 最佳RaR方法在HealthBench-1k数据集上相比简单Likert方法获得了高达28%的相对改进，同时匹配或超越了基于专家编写参考答案的奖励信号性能。该方法能够在不同模型规模上保持稳健的性能表现。

Conclusion: RaR框架通过使用结构化评分标准作为奖励信号，成功解决了传统奖励函数不透明和难以解释的问题，使小规模判断模型能够更好地与人类偏好对齐，并在多个模型规模上保持稳健性能，为强化学习在现实任务中的应用提供了新的解决方案。

Abstract: Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world
tasks often requires balancing objective and subjective evaluation criteria.
However, many such tasks lack a single, unambiguous ground truth-making it
difficult to define reliable reward signals for post-training language models.
While traditional preference-based methods offer a workaround, they rely on
opaque reward functions that are difficult to interpret and prone to spurious
correlations. We introduce $\textbf{Rubrics as Rewards}$ (RaR), a framework
that uses structured, checklist-style rubrics as interpretable reward signals
for on-policy training with GRPO. Our best RaR method yields up to a $28\%$
relative improvement on HealthBench-1k compared to simple Likert-based
approaches, while matching or surpassing the performance of reward signals
derived from expert-written references. By treating rubrics as structured
reward signals, we show that RaR enables smaller-scale judge models to better
align with human preferences and sustain robust performance across model
scales.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [86] [Fundamental limits of distributed covariance matrix estimation via a conditional strong data processing inequality](https://arxiv.org/abs/2507.16953)
*Mohammad Reza Rahmani,Mohammad Hossein Yassaee,Mohammad Reza Aref*

Main category: stat.ML

TL;DR: 本文研究了在特征分割设置下分布式协方差估计的理论极限，其中多个智能体观察不同的数据分量并需要在通信约束下估计完整的协方差矩阵。作者提出了条件强数据处理不等式(C-SDPI)系数这一新工具，并建立了近似紧致的minimax下界，同时给出了几乎最优的估计协议。


<details>
  <summary>Details</summary>
Motivation: 在许多领域中，高维协方差矩阵估计是一个关键任务。现有方法通常假设无限样本或高斯分布，且缺乏对通信约束下分布式协方差估计理论极限的深入理解。因此需要在更一般的设置下研究分布式协方差估计的基本理论限制。

Method: 1) 提出了条件强数据处理不等式(C-SDPI)系数这一新的技术工具，它是传统SDPI的推广；2) 利用Geng-Nair的倍增技巧和算子Jensen不等式计算高斯混合信道的C-SDPI系数；3) 运用C-SDPI建立估计误差的minimax下界；4) 设计几乎最优的估计协议，其样本和通信需求与下界匹配到对数因子；5) 扩展分析到交互式协议。

Result: 1) 在算子范数和Frobenius范数下获得了协方差矩阵估计的近似紧致minimax下界；2) C-SDPI系数具有张量化等关键性质，能量化状态相关信道中的平均收缩，且可以显著低于最坏情况的SDPI系数；3) 提出的估计协议在样本和通信需求上几乎达到最优；4) 证明了交互式协议相比非交互式方案能显著减少通信需求。

Conclusion: 本文建立了分布式协方差估计问题的理论基础，揭示了样本大小、通信成本和数据维度之间的权衡关系。提出的C-SDPI系数为分析通信约束下的统计推断问题提供了新的技术工具。与现有文献不同，该框架不假设无限样本或高斯分布，具有广泛的适用性。交互式协议的分析进一步展示了通信策略对性能的重要影响。

Abstract: Estimating high-dimensional covariance matrices is a key task across many
fields. This paper explores the theoretical limits of distributed covariance
estimation in a feature-split setting, where communication between agents is
constrained. Specifically, we study a scenario in which multiple agents each
observe different components of i.i.d. samples drawn from a sub-Gaussian random
vector. A central server seeks to estimate the complete covariance matrix using
a limited number of bits communicated by each agent. We obtain a nearly tight
minimax lower bound for covariance matrix estimation under operator norm and
Frobenius norm. Our main technical tool is a novel generalization of the strong
data processing inequality (SDPI), termed the Conditional Strong Data
Processing Inequality (C-SDPI) coefficient, introduced in this work. The C-SDPI
coefficient shares key properties such as tensorization with the conventional
SDPI. Crucially, it quantifies the average contraction in a state-dependent
channel and can be significantly lower than the worst-case SDPI coefficient
over the state input.
  Utilizing the doubling trick of Geng-Nair and an operator Jensen inequality,
we compute this coefficient for Gaussian mixture channels. We then employ it to
establish minimax lower bounds on estimation error, capturing the trade-offs
among sample size, communication cost, and data dimensionality. Building on
this, we present a nearly optimal estimation protocol whose sample and
communication requirements match the lower bounds up to logarithmic factors.
Unlike much of the existing literature, our framework does not assume infinite
samples or Gaussian distributions, making it broadly applicable. Finally, we
extend our analysis to interactive protocols, showing interaction can
significantly reduce communication requirements compared to non-interactive
schemes.

</details>


### [87] [Bayesian preference elicitation for decision support in multiobjective optimization](https://arxiv.org/abs/2507.16999)
*Felix Huber,Sebastian Rojas Gonzalez,Raul Astudillo*

Main category: stat.ML

TL;DR: 本文提出了一种基于贝叶斯模型的交互式方法，通过成对比较来估计决策者的效用函数，从而帮助决策者从多目标优化的帕累托集中高效识别偏好解决方案。


<details>
  <summary>Details</summary>
Motivation: 在多目标优化问题中，决策者需要从庞大的帕累托解集中选择最符合其偏好的解决方案，这一过程复杂且耗时。现有方法缺乏有效的交互式策略来平衡探索和利用，难以快速准确地识别高效用解决方案。

Method: 采用贝叶斯模型基于成对比较来估计决策者的效用函数，设计了一个原则性的启发策略来交互式地选择查询，平衡探索和利用过程。该方法既可以交互式使用，也可以在通过标准多目标优化技术估计帕累托前沿后进行后验分析。

Result: 在最多九个目标的测试问题上的实验表明，该方法在用少量查询找到高效用解决方案方面表现优异。方法在启发阶段结束时能生成高质量解决方案的精简菜单，简化决策过程。作者还提供了开源实现。

Conclusion: 提出的基于贝叶斯模型的交互式方法能够有效帮助决策者从多目标优化的帕累托集中识别偏好解决方案，具有灵活性强、查询效率高、决策简化等优点，为多目标优化中的决策支持提供了有效工具。

Abstract: We present a novel approach to help decision-makers efficiently identify
preferred solutions from the Pareto set of a multi-objective optimization
problem. Our method uses a Bayesian model to estimate the decision-maker's
utility function based on pairwise comparisons. Aided by this model, a
principled elicitation strategy selects queries interactively to balance
exploration and exploitation, guiding the discovery of high-utility solutions.
The approach is flexible: it can be used interactively or a posteriori after
estimating the Pareto front through standard multi-objective optimization
techniques. Additionally, at the end of the elicitation phase, it generates a
reduced menu of high-quality solutions, simplifying the decision-making
process. Through experiments on test problems with up to nine objectives, our
method demonstrates superior performance in finding high-utility solutions with
a small number of queries. We also provide an open-source implementation of our
method to support its adoption by the broader community.

</details>


### [88] [The surprising strength of weak classifiers for validating neural posterior estimates](https://arxiv.org/abs/2507.17026)
*Vansh Bansal,Tianyu Chen,James G. Scott*

Main category: stat.ML

TL;DR: 本文提出了一种基于保形预测的分类器双样本检验方法(Conformal C2ST)，用于验证神经后验估计的准确性。该方法能够将任何训练分类器的分数转换为精确的有限样本p值，即使是弱分类器或过拟合模型也能提供可靠的检验结果。


<details>
  <summary>Details</summary>
Motivation: 神经后验估计(NPE)在贝叶斯推断中很有用，但评估其准确性仍然具有挑战性。现有的分类器双样本检验(C2ST)方法需要接近贝叶斯最优的分类器才能可靠工作，这个要求在实际中很难满足。因此，研究弱分类器是否仍能用于神经后验验证成为一个重要的开放问题。

Method: 作者基于Hu和Lei的工作，提出了C2ST的保形变体。该方法将任何训练分类器的分数转换为精确的有限样本p值，不依赖于分类器的质量。方法建立在保形预测理论基础上，能够处理弱分类器、有偏分类器或过拟合分类器的情况。

Result: 理论上证明了保形C2ST具有两个关键性质：(i)有限样本的I型错误控制，(ii)非平凡的检验功效，且该功效与训练分类器的错误率温和地相关。实验结果表明，保形C2ST在各种基准测试中优于经典的判别检验方法。

Conclusion: 即使是弱的、有偏的或过拟合的分类器仍然可以产生强大且可靠的检验结果。保形C2ST揭示了弱分类器在验证神经后验估计方面被低估的优势，为现代基于仿真的推断建立了一个实用的、理论基础扎实的诊断工具。

Abstract: Neural Posterior Estimation (NPE) has emerged as a powerful approach for
amortized Bayesian inference when the true posterior $p(\theta \mid y)$ is
intractable or difficult to sample. But evaluating the accuracy of neural
posterior estimates remains challenging, with existing methods suffering from
major limitations. One appealing and widely used method is the classifier
two-sample test (C2ST), where a classifier is trained to distinguish samples
from the true posterior $p(\theta \mid y)$ versus the learned NPE approximation
$q(\theta \mid y)$. Yet despite the appealing simplicity of the C2ST, its
theoretical and practical reliability depend upon having access to a
near-Bayes-optimal classifier -- a requirement that is rarely met and, at best,
difficult to verify. Thus a major open question is: can a weak classifier still
be useful for neural posterior validation? We show that the answer is yes.
Building on the work of Hu and Lei, we present several key results for a
conformal variant of the C2ST, which converts any trained classifier's scores
-- even those of weak or over-fitted models -- into exact finite-sample
p-values. We establish two key theoretical properties of the conformal C2ST:
(i) finite-sample Type-I error control, and (ii) non-trivial power that
degrades gently in tandem with the error of the trained classifier. The upshot
is that even weak, biased, or overfit classifiers can still yield powerful and
reliable tests. Empirically, the Conformal C2ST outperforms classical
discriminative tests across a wide range of benchmarks. These results reveal
the under appreciated strength of weak classifiers for validating neural
posterior estimates, establishing the conformal C2ST as a practical,
theoretically grounded diagnostic for modern simulation-based inference.

</details>


### [89] [CoLT: The conditional localization test for assessing the accuracy of neural posterior estimates](https://arxiv.org/abs/2507.17030)
*Tianyu Chen,Vansh Bansal,James G. Scott*

Main category: stat.ML

TL;DR: 本文提出了条件定位测试(CoLT)方法，用于验证神经后验估计q(θ|x)是否准确逼近真实后验p(θ|x)，通过学习定位函数自适应选择神经后验与真实后验差异最大的点，在仿真推断设置中表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有的神经后验估计(NPE)质量评估方法主要基于分类器测试或散度度量，存在实际应用中的诸多缺陷。需要一种原则性的方法来检测神经后验估计与真实后验在所有条件输入范围内的差异，特别是在典型的仿真推断设置中

Method: 提出条件定位测试(CoLT)方法，学习一个定位函数来自适应地选择点θ_l(x)，这些点是神经后验q在给定x下与真实后验p偏差最大的位置。该方法不依赖于在每个x处进行详尽比较或密度估计，而是利用从真实后验的单次抽样和神经后验的任意次抽样

Result: 理论结果建立了评估所有x上分布相等性的充要条件，提供了严格的保证和实用的可扩展性。实验表明CoLT不仅在比较p和q方面优于现有方法，还能精确定位显著差异区域，为模型改进提供可操作的见解

Conclusion: CoLT作为验证神经后验估计的最先进解决方案，在检测分布差异和定位问题区域方面表现出色，为神经后验估计的质量评估提供了一个原则性且实用的工具

Abstract: We consider the problem of validating whether a neural posterior estimate \(
q(\theta \mid x) \) is an accurate approximation to the true, unknown true
posterior \( p(\theta \mid x) \). Existing methods for evaluating the quality
of an NPE estimate are largely derived from classifier-based tests or
divergence measures, but these suffer from several practical drawbacks. As an
alternative, we introduce the \emph{Conditional Localization Test} (CoLT), a
principled method designed to detect discrepancies between \( p(\theta \mid x)
\) and \( q(\theta \mid x) \) across the full range of conditioning inputs.
Rather than relying on exhaustive comparisons or density estimation at every \(
x \), CoLT learns a localization function that adaptively selects points
$\theta_l(x)$ where the neural posterior $q$ deviates most strongly from the
true posterior $p$ for that $x$. This approach is particularly advantageous in
typical simulation-based inference settings, where only a single draw \( \theta
\sim p(\theta \mid x) \) from the true posterior is observed for each
conditioning input, but where the neural posterior \( q(\theta \mid x) \) can
be sampled an arbitrary number of times. Our theoretical results establish
necessary and sufficient conditions for assessing distributional equality
across all \( x \), offering both rigorous guarantees and practical
scalability. Empirically, we demonstrate that CoLT not only performs better
than existing methods at comparing $p$ and $q$, but also pinpoints regions of
significant divergence, providing actionable insights for model refinement.
These properties position CoLT as a state-of-the-art solution for validating
neural posterior estimates.

</details>


### [90] [Nearly Minimax Discrete Distribution Estimation in Kullback-Leibler Divergence with High Probability](https://arxiv.org/abs/2507.17316)
*Dirk van der Hoeven,Julia Olkhovskaia,Tim van Erven*

Main category: stat.ML

TL;DR: 本文研究离散分布估计问题，提供了KL散度的上下界，提出了基于在线到批处理转换的高效估计器，并分析了最大似然估计器的性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有的离散分布估计方法在KL散度方面缺乏紧致的理论界限，特别是在高概率保证下的上下界分析不够完善，需要开发计算高效的估计器并提供理论保证。

Method: 1) 建立了离散分布估计在KL散度下的下界理论；2) 提出了基于在线到批处理转换(Online to Batch conversion)和后缀平均的计算高效估计器p^{OTB}；3) 分析了最大似然估计器的χ²散度和KL散度性能。

Result: 1) 证明了任何估计器的KL散度下界为C·max{K, ln(K)ln(1/δ)}/n；2) 提出的p^{OTB}估计器达到上界C(K·log(log(K)) + ln(K)ln(1/δ))/n；3) 最大似然估计器在充足样本下保证KL散度上界为C(K + log(1/δ))/n。

Conclusion: 本文为离散分布估计提供了几乎最优的理论界限，提出的OTB估计器在计算效率和理论保证之间取得了良好平衡，为实际应用中的分布估计问题提供了理论指导。

Abstract: We consider the problem of estimating a discrete distribution $p$ with
support of size $K$ and provide both upper and lower bounds with high
probability in KL divergence. We prove that in the worst case, for any
estimator $\widehat{p}$, with probability at least $\delta$, $\text{KL}(p \|
\widehat{p}) \geq C\max\{K,\ln(K)\ln(1/\delta) \}/n $, where $n$ is the sample
size and $C > 0$ is a constant. We introduce a computationally efficient
estimator $p^{\text{OTB}}$, based on Online to Batch conversion and suffix
averaging, and show that with probability at least $1 - \delta$ $\text{KL}(p \|
\widehat{p}) \leq C(K\log(\log(K)) + \ln(K)\ln(1/\delta)) /n$.
  Furthermore, we also show that with sufficiently many observations relative
to $\log(1/\delta)$, the maximum likelihood estimator $\bar{p}$ guarantees that
with probability at least $1-\delta$ $$
  1/6 \chi^2(\bar{p}\|p) \leq 1/4 \chi^2(p\|\bar{p}) \leq \text{KL}(p|\bar{p})
\leq C(K + \log(1/\delta))/n\,, $$ where $\chi^2$ denotes the
$\chi^2$-divergence.

</details>


### [91] [To Trust or Not to Trust: On Calibration in ML-based Resource Allocation for Wireless Networks](https://arxiv.org/abs/2507.17494)
*Rashika Raina,Nidhi Simmons,David E. Simmons,Michel Daoud Yacoub,Trung Q. Duong*

Main category: stat.ML

TL;DR: 研究了下一代通信网络中机器学习模型的校准性能，特别是在单用户多资源分配框架下的中断预测器，建立了完美校准下系统中断概率的理论性质，并通过仿真验证了这些理论结果。


<details>
  <summary>Details</summary>
Motivation: 下一代通信网络中，机器学习模型不仅需要提供准确预测，还需要提供反映正确决策真实可能性的良好校准置信度分数。现有研究缺乏对ML基础中断预测器在多资源分配系统中校准性能的理论分析。

Method: 建立单用户多资源分配框架下完美校准系统的中断概率理论性质；推导完美校准预测器的中断概率条件；分析后处理校准技术（Platt缩放和等张回归）的效果；使用专门设计的中断损失函数训练预测器；在具有时间相关性的瑞利衰落信道上进行仿真验证。

Result: 证明了随着资源数量增长，完美校准预测器的中断概率趋近于低于分类阈值条件下的期望输出；单资源情况下系统中断概率等于模型整体期望输出；后处理校准无法改善系统最小可达中断概率；建立了准确度-置信度函数必须满足的单调性条件。仿真结果验证了理论分析的正确性。

Conclusion: 完美校准的机器学习预测器在多资源通信系统中具有特定的理论性质，这些性质可以指导分类阈值的选择以实现期望的中断概率。良好校准的模型属于能够改善中断概率的更广泛预测器类别，为系统设计者满足特定可靠性要求提供了理论基础。

Abstract: In next-generation communications and networks, machine learning (ML) models
are expected to deliver not only accurate predictions but also well-calibrated
confidence scores that reflect the true likelihood of correct decisions. This
paper studies the calibration performance of an ML-based outage predictor
within a single-user, multi-resource allocation framework. We first establish
key theoretical properties of this system's outage probability (OP) under
perfect calibration. Importantly, we show that as the number of resources
grows, the OP of a perfectly calibrated predictor approaches the expected
output conditioned on it being below the classification threshold. In contrast,
when only one resource is available, the system's OP equals the model's overall
expected output. We then derive the OP conditions for a perfectly calibrated
predictor. These findings guide the choice of the classification threshold to
achieve a desired OP, helping system designers meet specific reliability
requirements. We also demonstrate that post-processing calibration cannot
improve the system's minimum achievable OP, as it does not introduce new
information about future channel states. Additionally, we show that
well-calibrated models are part of a broader class of predictors that
necessarily improve OP. In particular, we establish a monotonicity condition
that the accuracy-confidence function must satisfy for such improvement to
occur. To demonstrate these theoretical properties, we conduct a rigorous
simulation-based analysis using post-processing calibration techniques: Platt
scaling and isotonic regression. As part of this framework, the predictor is
trained using an outage loss function specifically designed for this system.
Furthermore, this analysis is performed on Rayleigh fading channels with
temporal correlation captured by Clarke's 2D model, which accounts for receiver
mobility.

</details>


### [92] [Optimal differentially private kernel learning with random projection](https://arxiv.org/abs/2507.17544)
*Bonwoo Lee,Cheolwoo Park,Jeongyoun Ahn*

Main category: stat.ML

TL;DR: 提出了一种基于随机投影的差分隐私核学习算法，在经验风险最小化框架下实现了最优的隐私-效用权衡，并首次获得了不依赖噪声梯度机制的无维度泛化界


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私核学习方法在隐私保护和学习效用之间的权衡不够理想，特别是基于随机傅里叶特征映射或L2正则化的方法存在次优的泛化性能问题

Method: 在再生核希尔伯特空间中使用高斯过程进行随机投影，提出了一种新颖的差分隐私核经验风险最小化算法，并基于目标扰动方法推导了私有线性ERM的维度无关泛化界

Result: 算法在平方损失和Lipschitz光滑凸损失函数下都达到了minimax最优的超额风险；获得了首个不依赖噪声梯度机制的无维度泛化界；为现有差分隐私核ERM算法得到了更紧的泛化界；实验验证了理论结果

Conclusion: 随机投影技术在差分隐私核学习中能够实现统计高效和最优隐私保护，降维技术在平衡隐私和效用方面发挥核心作用，为差分隐私算法设计提供了新的见解

Abstract: Differential privacy has become a cornerstone in the development of
privacy-preserving learning algorithms. This work addresses optimizing
differentially private kernel learning within the empirical risk minimization
(ERM) framework. We propose a novel differentially private kernel ERM algorithm
based on random projection in the reproducing kernel Hilbert space using
Gaussian processes. Our method achieves minimax-optimal excess risk for both
the squared loss and Lipschitz-smooth convex loss functions under a local
strong convexity condition. We further show that existing approaches based on
alternative dimension reduction techniques, such as random Fourier feature
mappings or $\ell_2$ regularization, yield suboptimal generalization
performance. Our key theoretical contribution also includes the derivation of
dimension-free generalization bounds for objective perturbation-based private
linear ERM -- marking the first such result that does not rely on noisy
gradient-based mechanisms. Additionally, we obtain sharper generalization
bounds for existing differentially private kernel ERM algorithms. Empirical
evaluations support our theoretical claims, demonstrating that random
projection enables statistically efficient and optimally private kernel
learning. These findings provide new insights into the design of differentially
private algorithms and highlight the central role of dimension reduction in
balancing privacy and utility.

</details>


### [93] [Debiased maximum-likelihood estimators for hazard ratios under machine-learning adjustment](https://arxiv.org/abs/2507.17686)
*Takashi Hayakawa,Satoshi Asai*

Main category: stat.ML

TL;DR: 该研究提出了一种新的机器学习方法来解决Cox模型中风险比估计的因果推断问题，特别是在观察性数据中处理动态治疗和实时协变量测量的情况


<details>
  <summary>Details</summary>
Motivation: 传统Cox模型中的风险比估计存在不可解释性问题，因为模型的不定基线风险无法识别由治疗分配和未观察因素导致的风险集合组成的时间变化，特别是在基于观察性数据的研究中存在未控制的动态治疗和多个协变量实时测量的复杂情况

Method: 提出放弃基线风险的方法，使用机器学习显式建模风险集合的变化（包含或不包含潜在变量）；基于Neyman正交性开发方法来计算风险比的去偏最大似然估计器；该框架比基于边际结构Cox模型的加权回归计算更高效

Result: 数值模拟证实所提出的方法能够以最小偏差识别真实值；该方法在计算效率上优于基于加权回归的边际结构Cox模型；为风险比的因果解释提供了明确的适用条件

Conclusion: 该研究为现代流行病学中使用未控制观察性数据进行因果推断奠定了基础，提供了一种有用的替代方法来解决传统Cox模型在观察性研究中的局限性

Abstract: Previous studies have shown that hazard ratios between treatment groups
estimated with the Cox model are uninterpretable because the indefinite
baseline hazard of the model fails to identify temporal change in the risk set
composition due to treatment assignment and unobserved factors among multiple,
contradictory scenarios. To alleviate this problem, especially in studies based
on observational data with uncontrolled dynamic treatment and real-time
measurement of many covariates, we propose abandoning the baseline hazard and
using machine learning to explicitly model the change in the risk set with or
without latent variables. For this framework, we clarify the context in which
hazard ratios can be causally interpreted, and then develop a method based on
Neyman orthogonality to compute debiased maximum-likelihood estimators of
hazard ratios. Computing the constructed estimators is more efficient than
computing those based on weighted regression with marginal structural Cox
models. Numerical simulations confirm that the proposed method identifies the
ground truth with minimal bias. These results lay the foundation for developing
a useful, alternative method for causal inference with uncontrolled,
observational data in modern epidemiology.

</details>


### [94] [Sequential Bayesian Design for Efficient Surrogate Construction in the Inversion of Darcy Flows](https://arxiv.org/abs/2507.17713)
*Hongji Wang,Hongqiao Wang,Jinyong Ying,Qingping Zhou*

Main category: stat.ML

TL;DR: 本文提出了一种针对偏微分方程反问题的序列贝叶斯设计局部精确代理模型方法(SBD-LAS)，通过聚焦高概率似然区域来降低计算成本，并在达西流方程实验中验证了其在反演精度和计算速度方面的优势。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程反问题的贝叶斯方法需要大量计算昂贵的正向求解器评估，而构建全局精确的代理模型对于高维复杂问题需要高模型容量和大量数据，因此需要一种计算成本更低但仍能保证精度的方法。

Method: 提出了局部精确代理模型，专注于反问题中真实似然的高概率区域，具有相对较低的模型复杂度和较少的训练数据需求；引入序列贝叶斯设计策略来获取所提出的代理模型；将序列贝叶斯设计的后验演化过程视为高斯过程，通过一步预测先验实现算法加速。

Result: 在基于达西流方程的三个实验中，所提出的SBD-LAS方法在反演精度和计算速度方面都表现出优势。

Conclusion: SBD-LAS方法通过构建局部精确的代理模型和序列贝叶斯设计策略，有效解决了偏微分方程反问题中计算成本高和精度要求的矛盾，为此类问题提供了一种高效的解决方案。

Abstract: Inverse problems governed by partial differential equations (PDEs) play a
crucial role in various fields, including computational science, image
processing, and engineering. Particularly, Darcy flow equation is a fundamental
equation in fluid mechanics, which plays a crucial role in understanding fluid
flow through porous media. Bayesian methods provide an effective approach for
solving PDEs inverse problems, while their numerical implementation requires
numerous evaluations of computationally expensive forward solvers. Therefore,
the adoption of surrogate models with lower computational costs is essential.
However, constructing a globally accurate surrogate model for high-dimensional
complex problems demands high model capacity and large amounts of data. To
address this challenge, this study proposes an efficient locally accurate
surrogate that focuses on the high-probability regions of the true likelihood
in inverse problems, with relatively low model complexity and few training data
requirements. Additionally, we introduce a sequential Bayesian design strategy
to acquire the proposed surrogate since the high-probability region of the
likelihood is unknown. The strategy treats the posterior evolution process of
sequential Bayesian design as a Gaussian process, enabling algorithmic
acceleration through one-step ahead prior. The complete algorithmic framework
is referred to as Sequential Bayesian design for locally accurate surrogate
(SBD-LAS). Finally, three experiments based the Darcy flow equation demonstrate
the advantages of the proposed method in terms of both inversion accuracy and
computational speed.

</details>
