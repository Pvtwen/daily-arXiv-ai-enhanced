<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 20]
- [cs.LG](#cs.LG) [Total: 224]
- [stat.ML](#stat.ML) [Total: 19]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Wideband Antenna Deconvolution for Bistatic Millimeter Wave Radar Reflectivity Measurements](https://arxiv.org/abs/2510.16094)
*Carsten Andrich,Isabella Varga,Tobias F. Nowack,Alexander Ihlow,Sebastian Giehl,Michael Schubert,Reiner S. Thomä,Matthias A. Hein*

Main category: eess.SP

TL;DR: 提出了一种用于球面双基地测量系统的空中校准算法，该算法比现有方法更简单且快两倍，在76-81 GHz频段对金属球进行反射率测量时，动态范围提升了40 dB。


<details>
  <summary>Details</summary>
Motivation: 双基地雷达测量具有空间多样性和增强的目标表征能力，但其可靠性依赖于精确的系统校准。现有替代方法需要使用已知参考物体，过程复杂。

Method: 开发了一种空中校准算法，专门用于球面双基地测量系统，无需使用物理参考物体。

Result: 在76-81 GHz频段对金属球进行反射率测量，动态范围提升了40 dB；测量数据与仿真数据高度一致。

Conclusion: 所提出的校准算法简单高效，能显著提升双基地测量系统的性能，验证了其在毫米波频段的有效性。

Abstract: Bistatic radar measurements offer unique spatial diversity and enhanced
target characterization capabilities, rendering them increasingly vital for
contemporary sensing application research. The reliability of such measurements
is contingent upon precise system and antenna calibration. The prevailing
technique is the substitution method, which involves the use of known reference
objects. We propose an over-the-air calibration algorithm for spherical
bistatic measurement systems. Our method is both significantly simpler and
twice as fast as existing algorithms. The application of our technique to
reflectivity measurements of a metal sphere from 76 to 81 GHz demonstrates a
dynamic range enhancement of up to 40 dB when compared with uncalibrated data.
A comparison with simulation data demonstrates a high degree of agreement
between measurement and simulation.

</details>


### [2] [Fast, Differentiable, GPU-Accelerated Ray Tracing for Multiple Diffraction and Reflection Paths](https://arxiv.org/abs/2510.16172)
*Jérome Eertmans,Sophie Lequeu,Benoît Legat,Laurent Jacques,Claude Oestges*

Main category: eess.SP

TL;DR: 提出了一种快速、可微分、GPU加速的射线路径追踪优化方法，适用于包含平面反射器和直线衍射边缘的环境。基于费马原理，将路径寻找问题重新表述为总路径长度的最小化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为反射和衍射分别使用不同的算法，缺乏统一的计算框架。需要一种能够高效利用现代GPU架构并行计算能力的方法。

Method: 基于费马原理，将路径寻找问题重新表述为总路径长度的最小化问题。使用隐式微分实现高效梯度计算，避免通过求解器迭代进行微分。提供与JAX和DrJIT等可微分编程库的无缝集成。

Result: 数值模拟显示收敛速度与专门的牛顿方法相当，同时在大规模应用中提供更好的可扩展性。显著优于传统的自动微分方法。

Conclusion: 该方法为无线传播建模中的逆向设计和优化开辟了新的可能性，实现了反射和衍射的统一处理框架，特别适合向量化计算。

Abstract: We present a fast, differentiable, GPU-accelerated optimization method for
ray path tracing in environments containing planar reflectors and straight
diffraction edges. Based on Fermat's principle, our approach reformulates the
path-finding problem as the minimization of total path length, enabling
efficient parallel execution on modern GPU architectures. Unlike existing
methods that require separate algorithms for reflections and diffractions, our
unified formulation maintains consistent problem dimensions across all
interaction sequences, making it particularly suitable for vectorized
computation. Through implicit differentiation, we achieve efficient gradient
computation without differentiating through solver iterations, significantly
outperforming traditional automatic differentiation approaches. Numerical
simulations demonstrate convergence rates comparable to specialized Newton
methods while providing superior scalability for large-scale applications. The
method integrates seamlessly with differentiable programming libraries such as
JAX and DrJIT, enabling new possibilities in inverse design and optimization
for wireless propagation modeling. The source code is openly available at
https://github.com/jeertmans/fpt-jax.

</details>


### [3] [Performance Comparison of Joint Delay-Doppler Estimation Algorithms](https://arxiv.org/abs/2510.16200)
*Lorenz Mohr,Michael Döbereiner,Steffen Schieler,Joerg Robert,Christian Schneider,Sebastian Semper,Reiner S. Thoma*

Main category: eess.SP

TL;DR: 比较了三种时延-多普勒估计算法（ML、CNN、CFAR）在ISAC系统中的性能，发现它们在双静态场景下表现相似，但在前向和后向散射条件下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信、雷达和波束成形需要实时高精度的时延-多普勒估计算法来识别无线传播信道中的镜面路径。

Method: 使用公开可用的信道数据，对三种算法（最大似然、卷积神经网络、恒虚警率）进行性能比较，包括目标检测率、均方根误差和运行时间分析。

Result: 在双静态场景下，三种算法都实现了高达80%的目标检测概率，参数估计能力相似；但在前向和后向散射条件下，由于强视距分量影响，检测概率降至0%。

Conclusion: 三种算法在双静态场景下具有相当的时延-多普勒估计性能，但在存在强视距分量的散射条件下性能严重受限。

Abstract: Integrated sensing and communications (ISAC), radar, and beamforming require
real-time, high-resolution estimation algorithms to determine delay-Doppler
values of specular paths within the wireless propagation channel. Our
contribution is the measurement-based performance comparison of the
delay-Doppler estimation between three different algorithms, comprising maximum
likelihood (ML), convolutional neural network (CNN), and constant false alarm
rate (CFAR) approaches. We apply these algorithms to publicly available channel
data which includes two spherical targets with analytically describable
delay-Doppler parameters. The comparison of the three algorithms features the
target detection rate, root mean squared errors (RMSEs) of the delay-Doppler
estimates, and a runtime analysis. Notably, all three algorithms demonstrate
similar parameter estimation capabilities in bi-static scenarios, achieving
target detection probabilities of up to 80%. Conversely, forward and backward
scattering conditions pose a problem to the estimation due to strong
line-of-sight (LoS) contribution, reducing the corresponding detection
probability down to 0%.

</details>


### [4] [Delay Minimization in Pinching-Antenna-enabled NOMA-MEC Networks](https://arxiv.org/abs/2510.16296)
*Yuan Ai,Xidong Mu,Pengbo Si,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出了一种基于捏合天线系统的NOMA-MEC框架，通过优化卸载比例、发射功率和天线位置来最小化最大任务延迟。


<details>
  <summary>Details</summary>
Motivation: 为了在多接入边缘计算环境中降低任务延迟，提高系统性能。

Method: 使用基于二分搜索的交替优化算法，迭代求解非凸优化问题的各个子问题。

Result: 数值仿真表明，所提框架相比基准方案能显著降低任务延迟。

Conclusion: PASS-NOMA-MEC框架通过联合优化多个参数，有效提升了边缘计算系统的性能。

Abstract: This letter proposes a novel pinching antenna systems (PASS) enabled
non-orthogonal multiple access (NOMA) multi-access edge computing (MEC)
framework. An optimization problem is formulated to minimize the maximum task
delay by optimizing offloading ratios, transmit powers, and pinching antenna
(PA) positions, subject to constraints on maximum transmit power, user energy
budgets, and minimum PA separation to mitigate coupling effects. To address the
non-convex problem, a bisection search-based alternating optimization (AO)
algorithm is developed, where each subproblem is iteratively solved for a given
task delay. Numerical simulations demonstrate that the proposed framework
significantly reduces the task delay compared to benchmark schemes.

</details>


### [5] [A Robust CSI-Based Scatterer Geometric Reconstruction Method for 6G ISAC System](https://arxiv.org/abs/2510.16389)
*Yubin Luo,Li Yu,Tao Wu,Yuxiang Zhang,Jianhua Zhang*

Main category: eess.SP

TL;DR: 提出了一种基于信道状态信息(CSI)的传感器无关散射体几何重建方法，通过多频线性采样方法和匹配滤波增强技术，在有限孔径阵列下实现稳健的散射体重建。


<details>
  <summary>Details</summary>
Motivation: 6G系统中数字孪生需要传播环境中的散射体几何重建，但传统方法需要额外传感器。利用6G的集成感知与通信(ISAC)能力，探索基于无线信道的传感器无关重建方法。

Method: 重新解释线性采样方法(LSM)，提出基于CSI的变体，利用多径和散射的共享信道特性。为解决有限孔径阵列问题，提出匹配滤波增强的多频LSM(MF MLSM)，通过多频数据增加频率多样性，匹配滤波对齐相位避免伪影。

Result: 在93.6度、144度和180度孔径以及27dB和12dB信噪比条件下，实验证明了该方法能够实现稳健的散射体几何重建。

Conclusion: 该方法成功实现了基于无线信道的传感器无关散射体几何重建，为6G数字孪生应用提供了有效的环境感知解决方案。

Abstract: Digital twin (DT) is a core enabler of sixth generation (6G) mobile systems.
As a prerequisite for DT, scatterer geometric reconstruction (SGR) in
propagation environments is essential but typically requires extra sensors such
as cameras and LiDAR. With integrated sensing and communication (ISAC) in 6G,
we reinterpret the linear sampling method (LSM) from a wireless channel
viewpoint and propose a CSI based variant for sensor free SGR: by exploiting
the shared channel characteristics of multipath and scattering, in band CSI
replaces the scattered field measurements usually required by LSM. However,
aperture limited arrays reduce LSM robustness. To address this, we propose
matched filtering enhanced multi frequency LSM (MF MLSM). Multi frequency data
increases frequency diversity, and matched filtering coherently aligns inter
frequency phases to avoid artifacts, both of which improve robustness.
Experiments with apertures of 93.6 deg, 144 deg, and 180 deg and SNRs of 27 dB
and 12 dB demonstrate robust SGR with this approach.

</details>


### [6] [Adaptive Sensing Performance Design for Enhancing Secure Communication in Networked ISAC Systems](https://arxiv.org/abs/2510.16397)
*Yiming Xu,Dongfang Xu,Shenghui Song,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出了一种基于自适应感知性能的传感增强安全通信方法，通过将感知性能隐式地融入信息泄露率中，优化功率消耗，提高系统灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常对感知性能施加固定要求，未考虑变化的通信条件，限制了感知与通信之间的协同作用。窃听者的信道状态信息难以获取，但集成感知与通信技术提供了新的解决方案。

Method: 采用集中式和分布式两种设计：集中式设计使用块坐标下降法；分布式设计采用基于共识交替方向乘子法的优化框架，以降低复杂性和信息交换开销。

Result: 实验结果表明，所提出的隐式感知性能要求设计能够根据系统配置自适应调整感知性能，从而提升系统性能。

Conclusion: 自适应感知性能设计能够有效增强系统灵活性，通过优化功率消耗提升物理层安全通信性能。

Abstract: The channel state information (CSI) of an eavesdropper is crucial for
physical layer security (PLS) design, but it is difficult to obtain due to the
passive and non-cooperative nature of the eavesdropper. To this end, integrated
sensing and communication (ISAC) offers a novel solution by estimating the CSI
of the eavesdropper based on sensing information. However, existing studies
normally impose explicit and fixed sensing performance requirement without
considering the varying communication conditions, which hinders the system from
fully exploiting the synergy between sensing and communication. To address this
issue, this paper proposes sensing-enhanced secure communication with adaptive
sensing performance. Specifically, we formulate the sensing performance
implicitly in the information leakage rate and adaptively optimize it for the
minimization of the power consumption, offering enhanced flexibility and
adaptability in sensing performance. We consider both centralized and
decentralized designs to thoroughly investigate the impact of network structure
on system performance and complexity. Specifically, we devise a block
coordinate descent (BCD)-based method for centralized design. For decentralized
design, we develop an optimization framework based on consensus alternating
direction method of multipliers (ADMM) to reduce complexity and information
exchange overhead. Experimental results demonstrate the advantage of the
proposed implicit sensing performance requirement design due to its capability
to adaptively adjust the sensing performance to enhance the system performance
for varying system configurations.

</details>


### [7] [Single-Step Digital Backpropagation for O-band Coherent Transmission Systems](https://arxiv.org/abs/2510.16482)
*Romulo Aparecido,Jiaqian Yang,Ronit Sohanpal,Zelin Gan,Eric Sillekens,John D. Downie,Lidia Galdino,Vitaly Mikhailov,Daniel Elson,Yuta Wakayama,David DiGiovanni,Jiawei Luo,Robert I. Killey,Polina Bayvel*

Main category: eess.SP

TL;DR: 在O波段近零色散区通过数字反向传播补偿光纤非线性，单步DBP有效抑制自相位调制，在151km SMF-28 ULL光纤链路上为50Gbaud PDM-256QAM传输实现最高1.6dB的SNR增益


<details>
  <summary>Details</summary>
Motivation: 在O波段近零色散区域，光纤非线性效应（特别是自相位调制）会严重限制高波特率、高阶调制格式的传输性能，需要有效的非线性补偿技术

Method: 采用数字反向传播（DBP）方法，使用单步DBP来补偿光纤非线性效应，特别是自相位调制

Result: 在2跨段151km SMF-28 ULL光纤链路上，50Gbaud PDM-256QAM传输系统通过单步DBP实现了最高1.6dB的信噪比增益

Conclusion: 单步数字反向传播在O波段近零色散区域能有效补偿光纤非线性效应，为高波特率、高阶调制格式传输提供了可行的非线性补偿方案

Abstract: We demonstrate digital backpropagation-based compensation of fibre
nonlinearities in the near-zero dispersion regime of the O-band. Single-step
DBP effectively mitigates self-phase modulation, achieving SNR gains of up to
1.6 dB for 50 Gbaud PDM-256QAM transmission over a 2-span 151 km SMF-28 ULL
fibre link.

</details>


### [8] [Performance Evaluation of High Power Microwave Systems Against UAVs A Probabilistic Antenna Propagation Framework with Sensitivity Analysis](https://arxiv.org/abs/2510.16495)
*Muhammad Khalil,Ke Wang,Jinho Choi*

Main category: eess.SP

TL;DR: 开发了一个概率框架来量化高功率微波对抗无人机的有效性，结合随机运动学、波束指向抖动和大气传播，获得接收脉冲能量的闭式统计，并推导出可解析计算的中和概率。


<details>
  <summary>Details</summary>
Motivation: 需要量化高功率微波对抗无人机的有效性，为系统设计和任务规划提供快速准确的性能预测。

Method: 结合随机无人机运动学、波束指向抖动-增益映射和大气传播模型，使用对数正态闭包和高斯-埃尔米特求积法推导闭式统计和中和概率。

Result: 模型预测与大规模蒙特卡洛模拟高度一致，对于典型商用无人机，单脉冲杀伤概率≥0.4，0.1秒内总杀伤概率>99%；对于加固平台，单脉冲杀伤概率<1%，1秒后总杀伤概率<20%。

Conclusion: 该框架提供了快速、准确且物理可信的性能预测，揭示了天线/传播设计的关键参数，为高功率微波系统尺寸确定和风险感知任务规划提供了明确的设计杠杆。

Abstract: We develop a probabilistic, antenna- and propagation-centric framework to
quantify the effectiveness of high-power microwave (HPM) engagements against
unmanned aerial vehicles (UAVs). The model couples stochastic UAV kinematics, a
beam-steering jitter-to-gain mapping, and atmospheric propagation (free-space
spreading with gaseous and rain loss) to obtain closed-form statistics of the
received pulse energy. From these, we derive analytically evaluable per-pulse
and cumulative neutralization probabilities using log-normal closures and
Gaussian--Hermite quadrature, and we provide a dwell-time expression under a
standard pulse-independence assumption. Analytical predictions closely match
large-scale Monte-Carlo simulations across broad parameter ranges. For a
representative commercial threshold $E_{\mathrm{th}} = 10^{-2}\,\mathrm{J}$,
the model predicts $\bar{P}_{\mathrm{kill}} \gtrsim 0.4$ per pulse and
$P_{\mathrm{kill,tot}} > 99\%$ within about $0.1\,\mathrm{s}$ at kHz PRF; for
hardened platforms with $E_{\mathrm{th}} = 10^{-1}\,\mathrm{J}$,
$\bar{P}_{\mathrm{kill}} < 1\%$ and $P_{\mathrm{kill,tot}} < 20\%$ after
$1\,\mathrm{s}$. A closed-form sensitivity (elasticity) analysis shows
performance is dominated by slant range ($S_{\bar{R}} \approx -2$), with strong
secondary dependence on aperture diameter and transmit power; pointing jitter
and atmospheric variability are comparatively less influential in the evaluated
regimes. The framework yields fast, accurate, and physics-faithful performance
predictions and exposes clear antenna/propagation design levers for HPM system
sizing and risk-aware mission planning.

</details>


### [9] [Topology-Aware Hybrid Wi-Fi/BLE Fingerprinting via Evidence-Theoretic Fusion and Persistent Homology](https://arxiv.org/abs/2510.16557)
*Behrad Mousaei Shir-Mohammad,Behzad Moshiri,Abolfazl Yaghmaei*

Main category: eess.SP

TL;DR: 提出了一种拓扑感知的混合Wi-Fi/BLE指纹定位框架，通过多技术融合和降噪处理，在GNSS拒止环境中实现了高精度室内定位，计算成本低适合微控制器部署。


<details>
  <summary>Details</summary>
Motivation: 解决GNSS拒止环境中室内定位面临的挑战，包括多径效应、设备异构性和无线电信道变化等问题，需要一种既能提供准确位置估计又能量化不确定性的轻量级解决方案。

Method: 采用混合Wi-Fi/BLE指纹识别方法，包括物理一致的RSS归一化、贝叶斯滤波降噪、随机森林和加权kNN回归器组合、Dempster-Shafer理论证据融合，以及持久同调描述符增强。

Result: 在10%合成RSS噪声下，两个数据集的RMSE分别为3.40米和2.45米，比基线提升约37%。无噪声测试中精度达到0.44米和0.32米，比基线提升高达56%。

Conclusion: 该方法在保持竞争力的定位精度的同时，提供了形式化的不确定性量化，计算成本低，适合实时部署，优于依赖大数据集和GPU推理的学习密集型方法。

Abstract: Indoor localization remains challenging in GNSS-denied environments due to
multipath, device heterogeneity, and volatile radio conditions. We propose a
topology-aware, hybrid Wi-Fi/BLE fingerprinting framework that (i) applies
physically consistent RSS normalization (dBm z-scoring or dBm -> linear mW ->
z-score), (ii) denoises streams with classical Bayesian filters (KF/UKF/PF),
(iii) combines complementary regressors (Random Forest and weighted kNN with a
diagonal Mahalanobis metric), (iv) performs evidence-theoretic fusion via
Dempster-Shafer theory (DST), and (v) augments each sample with
persistent-homology (PH) descriptors. The system outputs both (x, y) estimates
and interpretable belief maps, and is engineered for microcontroller-class
deployment with per-update cost O(T log M + log M + Mp + S).
  We evaluate on two heterogeneous datasets, including a new 1,200-sample ESP32
survey, and report ablations, robustness to test-only noise, and significance
across 10 stratified splits. Under 10% synthetic RSS noise, the full pipeline
attains 3.40 m (Dataset 1) and 2.45 m (Dataset 2) RMSE, improving a strong PF +
RF baseline by about 37%. Averaged across splits, it yields 4.993 +/- 0.15 m
versus 6.292 +/- 0.13 m (20.6% relative reduction; p < 0.001). In noise-free
tests, accuracy tightens to 0.44 m and 0.32 m (up to 56% better). Compared with
recent learning-heavy approaches that assume large site-specific datasets and
GPU inference, our method delivers competitive accuracy with formal uncertainty
quantification and low computational cost suitable for real-time deployment.

</details>


### [10] [Stochastic Geometry Analysis of Asymmetric Uplink Interference for Urban UAV-RC Networks](https://arxiv.org/abs/2510.16963)
*Donggu Lee,Sung Joon Maeng,Ismail Guvenc*

Main category: eess.SP

TL;DR: 该论文提出了一种随机几何框架来分析城市环境中无人机通信的上行链路和下行链路不对称干扰问题，使用对数高斯Cox过程建模干扰场，并定义了干扰不对称比来量化干扰差异。


<details>
  <summary>Details</summary>
Motivation: 无人机在密集城市环境中部署时面临独特挑战，特别是由于无人机更容易受到视线干扰而导致的上行链路不对称干扰问题。

Method: 采用随机几何框架和log-Gaussian Cox过程(LGCP)模型来捕捉干扰场的空间相关性，分析无人机高度和二维距离对上行和下行链路干扰的影响。

Result: 数值结果表明，随着无人机高度和二维距离的增加，干扰不对称比增大，表明上行链路干扰情况恶化。

Conclusion: 在城市环境中，无人机通信的上行链路干扰会随着飞行高度和距离的增加而变得更加严重，这为无人机部署策略提供了重要参考。

Abstract: Uncrewed aerial vehicles (UAVs) have emerged as a flexible platform for
providing coverage over challenging environments, particularly for public
safety and surveillance missions in urban areas. However, deploying the UAVs in
dense urban areas introduces unique challenges, most notably asymmetric uplink
(UL, remote controller to UAV) interference due to a higher chance of
line-of-sight (LoS) interference at the UAV. In this letter, we propose a
stochastic geometry framework to tractably analyze the large-scale asymmetric
interference in urban areas. We incorporate a log-Gaussian Cox process (LGCP)
model to capture the spatial correlation of the interference field in both UL
and downlink (DL) as a function of the UAV altitude and the two-dimensional
(2-D) distance between the remote controller and UAV. To quantify the UL and
the DL interference asymmetry, we also define the interference asymmetry ratio
characterizing the interference disparity between the UL and the DL. Our
numerical results demonstrate that the interference asymmetry ratio increases
as the UAV altitude and 2-D distance increase, highlighting that the UL
interference worsens.

</details>


### [11] [Reconfigurable Antenna Arrays: Bridging Electromagnetics and Signal Processing](https://arxiv.org/abs/2510.17113)
*Mengzhen Liu,Ming Li,Rang Liu,Qian Liu,A. Lee Swindlehurst*

Main category: eess.SP

TL;DR: 本文系统介绍了可重构天线在6G网络中的硬件实现和阵列架构，重点分析了其在电磁可重构性与信号处理协同方面的能力，并展示了在ISAC、物理层安全和近场通信等应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 为满足6G无线网络对性能的严格要求，需要能够动态适应辐射模式、极化状态和工作频率的可重构天线技术，以提供前所未有的灵活性和适应性。

Method: 系统介绍可重构天线的硬件实现，研究全数字和三混合等先进阵列架构，分析电磁可重构性与模拟和数字信号处理的协同集成能力。

Result: 可重构天线阵列通过跨电磁和信号处理域的协调波束成形，在波束优化、链路鲁棒性提升和系统功耗降低方面表现出显著效果。

Conclusion: 可重构天线在6G网络中具有变革性潜力，但需要进一步解决理论建模、硬件可靠性、信道估计、智能优化方法和创新网络架构等开放挑战。

Abstract: Reconfigurable antennas (RAs), capable of dynamically adapting their
radiation patterns, polarization states, and operating frequencies, have
emerged as a promising technology to meet the stringent performance
requirements of sixth-generation (6G) wireless networks. This article
systematically introduces essential hardware implementations of RAs and
investigates advanced array architectures, such as fully-digital and tri-hybrid
designs, emphasizing their capability to synergistically integrate
electromagnetic (EM) reconfigurability with analog and digital signal
processing. By facilitating coordinated beamforming across the EM and signal
processing domains, RA arrays offer unprecedented flexibility and adaptability
compared to conventional static antenna systems. Representative applications
empowered by RA arrays, including integrated sensing and communication (ISAC),
physical layer security (PLS), and near-field communications, are highlighted.
A case study illustrates the effectiveness of RA arrays in optimizing beam
steering, improving link robustness, and alleviating system power consumption.
Finally, several open challenges and future research directions are outlined,
emphasizing the need for advancements in theoretical modeling, hardware
reliability, channel estimation techniques, intelligent optimization methods,
and innovative network architectures, to fully realize the transformative
impact of RAs in future 6G wireless networks.

</details>


### [12] [Robust Beamforming Optimization for STAR-RIS Empowered Multi-User RSMA Under Hardware Imperfections and Channel Uncertainty](https://arxiv.org/abs/2510.17272)
*Muhammad Asif,Asim Ihsan,Zhu Shoujin,Ali Ranjha,Xingwang Li,Khaled M. Rabie,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 该研究提出了一个将速率分割多址接入与同时透射反射可重构智能表面相结合的统一框架，用于提升第六代网络的连接性能、频谱效率和能量效率。


<details>
  <summary>Details</summary>
Motivation: 为未来第六代网络提供无处不在、智能且弹性的连接，同时提高频谱和能量效率，解决硬件损伤和不完美信道状态信息带来的挑战。

Method: 开发了一种智能优化策略，联合设计发射机主动波束成形、公共流速率分配以及STAR-RIS的透射和反射区域被动波束成形向量。使用迭代优化算法将问题分解为两个子问题，分别采用凸半定规划和半定松弛技术求解。

Result: 数值仿真表明，所提出的策略相比基准方案实现了显著的性能提升，并表现出快速收敛特性。

Conclusion: RSMA与STAR-RIS的协同框架能够有效提升系统性能，为未来6G网络提供了一种有前景的解决方案。

Abstract: This study explores the synergy between rate-splitting multiple access (RSMA)
and simultaneous transmitting and reflecting reconfigurable intelligent surface
(STAR-RIS) as a unified framework to enable ubiquitous, intelligent, and
resilient connectivity in future sixth-generation networks, while improving
spectral and energy efficiency. Specifically, we investigate a
STAR-RIS-assisted multi-user RSMA system and develop an intelligent
optimization strategy that jointly designs the transmitter's active
beamforming, the common stream rate allocation, and the passive beamforming
vectors for the STAR-RIS transmission and reflection regions, considering
transceiver hardware impairments and imperfect channel state information (CSI).
In addition, system robustness is ensured via a bounded channel estimation
error model that captures CSI imperfections and guarantees resilience against
worst-case errors. To address the highly non-convex problem, we propose an
iterative optimization algorithm that decomposes it into two sub-problems.
Firstly, active beamforming vectors for the common and private signals are
determined by reformulating the original problem into a convex semi-definite
programming (SDP) form using successive convex approximation (SCA) and
semi-definite relaxation (SDR). Secondly, passive beamforming vectors are
optimized through a convex SDP reformulation by exploiting SCA and SDR
techniques. Moreover, when higher-rank solutions arise, Gaussian randomization
is applied to obtain rank-one solutions. Numerical simulations demonstrate that
the proposed strategy achieves significant performance gains over benchmark
schemes and exhibits fast convergence.

</details>


### [13] [ORIX: Orchestration of RIS with xApps for Smart Wireless Factory Environments](https://arxiv.org/abs/2510.17462)
*Sefa Kayraklik,Ali Fuat Sahin,Onur Salan,Recep A. Tasci,Recep Vural,Yusuf Islam Tek,Ertugrul Basar,Ibrahim Hokelek,Ali Gorcin,Karim Boutiba,Adlen Ksentini*

Main category: eess.SP

TL;DR: ORIX是一个将可重构智能表面(RIS)技术集成到O-RAN生态系统中的方法，通过xApp控制为智能无线工厂提供动态配置、信道仿真和优化策略。


<details>
  <summary>Details</summary>
Motivation: 智能无线工厂需要超越传统无线解决方案的高灵活性、低延迟和可靠连接，RIS与O-RAN架构结合有望满足这些挑战性需求。

Method: ORIX包含三个关键组件：符合O-RAN标准的RIS服务模型、支持3GPP室内工厂模型的RIS信道模拟器，以及具有有限分辨率控制的实用RIS优化策略。

Result: 案例研究表明ORIX能够评估可实现的性能增益，探索关键RIS设计参数之间的权衡，并识别平衡系统性能与实际实施约束的部署策略。

Conclusion: ORIX通过将理论进展与工业可行性相结合，为RIS辅助的O-RAN网络在工业场景中支持下一代无线通信奠定了基础。

Abstract: The vision of a smart wireless factory (SWF) demands highly flexible,
low-latency, and reliable connectivity that goes beyond conventional wireless
solutions. Reconfigurable intelligent surface (RIS)-empowered communications,
when integrated with the open radio access network (O-RAN) architectures, have
emerged as a promising enabler to meet these challenging requirements. This
article introduces the methodology for the orchestration of RIS with xApps
(ORIX), bringing the RIS technology into the O-RAN ecosystem through xApp-based
control for SWF environments. ORIX features three key components: an
O-RAN-compliant RIS service model for dynamic configuration, an RIS channel
simulator that supports 3GPP indoor factory models with multiple industrial
scenarios, and practical RIS optimization strategies with finite-resolution
control. Together, these elements provide a realistic end-to-end emulation
platform for evaluating RIS placement, control, and performance in SWF
environments prior to deployment. The presented case study demonstrates how
ORIX enables the evaluation of achievable performance gains, exploration of
trade-offs among key RIS design parameters, and identification of deployment
strategies that balance system performance with practical implementation
constraints. By bridging theoretical advances with industrial feasibility, ORIX
lays the groundwork for RIS-assisted O-RAN networks to power next-generation
wireless communication in industrial scenarios.

</details>


### [14] [When 5G NTN Meets GNSS: Tracking GNSS Signals under Overlaid 5G Waveforms](https://arxiv.org/abs/2510.17324)
*Idir Edjekouane,Alejandro González Garrido,Jorge Querol,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 本文首次定量分析了在5G NTN混合波形下GNSS跟踪和导航消息解调的性能，验证了使用近传统GNSS芯片组实现联合通信与定位的可行性。


<details>
  <summary>Details</summary>
Motivation: GNSS系统对干扰敏感，而5G非地面网络中的LEO星座可以通过联合支持通信和导航来增强系统韧性。

Method: 采用蒙特卡洛模拟，在真实LEO多普勒动态下评估了最小修改的GNSS接收机，该接收机跟踪与5G帧对齐的传统GPS L1 C/A覆盖，并将5G波形视为结构化干扰。

Result: 结果显示在低和中动态条件下，能够在宽SINR范围内实现可靠的解调，而高动态条件则施加了严格的锁定限制。

Conclusion: 研究证实了使用近传统GNSS芯片组实现联合通信与定位的可行性，只需对接收机进行最小修改。

Abstract: Global Navigation Satellite Systems (GNSS) provide the backbone of
Positioning, Navigation, and Timing (PNT) but remain vulnerable to
interference. Low Earth Orbit (LEO) constellations within Fifth-Generation (5G)
Non-Terrestrial Networks (NTN) can enhance resilience by jointly supporting
communication and navigation. This paper presents the first quantitative
analysis of GNSS tracking and navigation message demodulation under a hybrid
waveform where a low-power Direct-Sequence Spread Spectrum (DSSS) component is
overlaid on an Orthogonal Frequency-Division Multiplexing (OFDM) 5G downlink.
We evaluate a minimally modified GNSS receiver that tracks a legacy Global
Positioning System (GPS) L1 Coarse/Acquisition (C/A) overlay aligned with 5G
frames while treating the 5G waveform as structured interference. Using Monte
Carlo simulations under realistic LEO Doppler dynamics, we analyze the Bit
Error Rate (BER) of GPS L1 C/A navigation bits and the subframe decoding
probability versus Signalto- Interference-plus-Noise Ratio (SINR) for multiple
Signalto- Interference Ratios (SIR) and dynamic classes. Results show reliable
demodulation across wide SINR ranges for low and medium dynamics, whereas high
dynamics impose strict lock limits. These findings confirm the feasibility of
Joint Communication and Positioning (JCAP) using a near-legacy GNSS chipset
with minimal receiver modifications.

</details>


### [15] [Efficiency-Enhanced Open Earbud Earphone Antenna Using Dual-Feed Technique](https://arxiv.org/abs/2510.17361)
*Shiming Liu,Jianhua Xie,Yan Wang*

Main category: eess.SP

TL;DR: 提出一种用于开放式无线耳机的双馈天线设计，通过引入可控相位差的双馈激励技术来扩大等效辐射孔径，相比传统单馈设计效率提升超过1dB，在2.4GHz ISM频段实现稳定性能。


<details>
  <summary>Details</summary>
Motivation: 现代无线耳机面临严格的空间限制和高天线效率需求的设计挑战，需要开发新型天线设计来满足这些要求。

Method: 采用双馈激励技术，在两个馈电点之间引入可控相位差，有效扩大等效辐射孔径，提升天线系统的整体辐射效率。

Result: 实验和仿真结果显示，双馈方法相比标准单馈设计效率提升超过1dB，原型机在2.4GHz ISM频段实现-6dB阻抗带宽，自由空间和佩戴状态下总效率分别达到-8.5dB和-9.5dB。

Conclusion: 所提出的天线在耳机极小体积内成功实现了高效率和高可靠性，在下一代紧凑型无线耳机中具有强大集成潜力。

Abstract: The stringent spatial constraints and the demand for high antenna efficiency
in modern wireless earphones present significant design challenges. To address
these issues, this paper presents and thoroughly investigates a novel earphone
antenna design specifically tailored for open earbud wireless earphones. In
contrast to traditional earphone antennas that rely on a conventional
single-feed configuration, the proposed design introduces a dual-feed
excitation technique incorporating a controlled phase difference between the
two feeds. This innovative feeding strategy effectively enlarges the equivalent
radiating aperture, thereby enhancing the overall radiation efficiency of the
antenna system. Experimental and simulation results demonstrate that the
dual-feed approach yields an efficiency improvement exceeding 1 dB when
compared with standard single-feed designs. Furthermore, the fabricated
prototype achieves a -6 dB impedance bandwidth that fully encompasses the 2.4
GHz ISM band, ensuring stable wireless communication performance. The measured
total efficiencies reach -8.5 dB in free space and -9.5 dB under on-head
conditions. These results confirm that the proposed antenna successfully
achieves high efficiency and reliable performance within the extremely limited
volume of an earbud device, demonstrating strong potential for integration into
next-generation compact wireless earphones.

</details>


### [16] [6D Movable Metasurface (6DMM) in Downlink NOMA Transmissions](https://arxiv.org/abs/2510.17502)
*Li-Hsiang Shen*

Main category: eess.SP

TL;DR: 提出一种六维可移动超表面辅助的NOMA下行链路系统，通过动态调整超表面元素位置和三维旋转角度，联合优化波束成形、相移、元素位置和旋转角度，使用概率交叉熵优化算法解决高维非凸问题，显著提升速率性能。


<details>
  <summary>Details</summary>
Motivation: 传统静态RIS无法动态调整空间位置和电磁特性，限制了系统性能。通过引入六维可移动超表面，实现空间和电磁双重控制，提升NOMA系统的频谱效率。

Method: 采用概率交叉熵优化算法，通过最大化似然和精英解采样迭代优化解分布，联合优化BS的NOMA波束成形、相移、元素位置和旋转角度。

Result: 仿真结果表明，所提出的CEO-based 6DMM-NOMA架构相比6DMM子结构、传统静态RIS和其他多址机制，实现了显著的速率性能增益。

Conclusion: 六维可移动超表面结合概率交叉熵优化为高维可扩展超表面提供了有效的优化解决方案，显著提升了NOMA系统的性能。

Abstract: This letter proposes a novel six-dimensional movable metasurface
(6DMM)-assisted downlink non-orthogonal multiple access (NOMA) system, in which
a conventional base station (BS) equipped with fixed antennas serves multiple
users with the assistance of a reconfigurable intelligent surface (RIS) with
six-dimensional spatial configurability. In contrast to traditional RIS with
static surface, the proposed 6DMM architecture allows each element to
dynamically adjust its position and orient the whole metasurface in
yaw-pitch-roll axes, enabling both in spatial and electromagnetic controls. We
formulate a sum-rate maximization problem that jointly optimizes the BS
NOMA-based beamforming, phase-shifts, element positions, and rotation angles of
metasurface under constraints of NOMA power levels, unit-modulus of
phase-shifts, power budget, inter-element separation and boundaries of element
position/orientation. Due to non-convexity and high-dimensionality, we employ a
probabilistic cross-entropy optimization (CEO) scheme to iteratively refine the
solution distribution based on maximizing likelihood and elite solution
sampling. Simulation results show that the proposed CEO-based 6DMM-NOMA
architecture achieves substantial rate performance gains compared to 6DMM
sub-structures, conventional static RIS, and other multiple access mechanisms.
It also highlights the effectiveness of CEO providing probabilistic
optimization for solving high-dimensional scalable metasurface.

</details>


### [17] [Semantic Joint Source Channel Coding for Distributed Subsurface Imaging in Multi-Agent Systems](https://arxiv.org/abs/2510.17695)
*Maximilian H. V. Tillmann,Ban-Sok Shin,Dmitriy Shutin,Armin Dekorsy*

Main category: eess.SP

TL;DR: 提出了一种将语义通信紧密集成到多智能体探索过程中的新框架，通过语义联合源信道编码和空中计算技术，显著提升了分布式地下成像的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将多智能体系统的探索和通信作为解耦子系统处理，无法实现最优协作。需要开发能够根据探索方法自适应调整通信策略的集成框架。

Method: 采用语义联合源信道编码(JSCC)与空中计算(AirComp)技术，结合适应-组合全波形反演(ATC-FWI)算法，用于协作式地下成像。在接收端引入辅助信息提升效率。

Result: 语义JSCC在高度连接网络中显著优于传统点对点和标准JSCC方法。接收端辅助信息的引入进一步提高了通信效率和成像精度。

Conclusion: 语义通信在分布式多智能体探索中具有巨大潜力，提供了一种通信感知的探索范式，能够实现任务相关的性能提升。

Abstract: Multi-agent systems (MAS) are a promising solution for autonomous exploration
tasks in hazardous or remote environments, such as planetary surveys. In such
settings, communication among agents is essential to ensure collaborative task
execution, yet conventional approaches treat exploration and communication as
decoupled subsystems. This work presents a novel framework that tightly
integrates semantic communication into the MAS exploration process, adapting
communication strategies to the exploration methodology to improve overall task
performance. Specifically, we investigate the application of semantic joint
source-channel coding (JSCC) with over-the-air computation (AirComp) for
distributed function computation for the application of cooperative subsurface
imaging using the adapt-then-combine full waveform inversion (ATC-FWI)
algorithm. Our results demonstrate that semantic JSCC significantly outperforms
classical point-to-point and standard JSCC methods, especially in
high-connectivity networks. Furthermore, incorporating side information at the
receiving agent enhances communication efficiency and imaging accuracy, a
feature previously unexplored in MAS-based exploration. We validate our
approach through a use case inspired by subsurface anomaly detection, showing
measurable improvements in imaging performance per agent. This work underscores
the potential of semantic communication in distributed multi-agent exploration,
offering a communication-aware exploration paradigm that achieves task-relevant
performance gains.

</details>


### [18] [Beam Index Map Prediction in Unseen Environments from Geospatial Data](https://arxiv.org/abs/2510.17738)
*Fabian Jaensch,Giuseppe Caire,Begüm Demir*

Main category: eess.SP

TL;DR: 提出基于卷积神经网络的方法，利用基站位置和地理空间数据预测所有用户位置的波束分布，无需站点特定训练即可泛化到新环境，显著减少候选波束数量，提高波束训练效率。


<details>
  <summary>Details</summary>
Motivation: 在5G网络中，波束训练需要高效地将用户与基站波束形成码本中的波束关联起来，传统方法需要站点特定训练或专用传感器，限制了部署效率。

Method: 使用卷积神经网络，结合基站位置和地理空间数据，同时预测所有用户位置的波束分布，实现无需站点特定训练的环境泛化。

Result: 该方法显著减少了需要考虑的候选波束数量，从而提高了波束训练的效率，且能够泛化到未见过的环境。

Conclusion: 基于卷积神经网络和地理空间数据的方法为5G波束训练提供了一种高效、可泛化的解决方案，无需站点特定训练即可实现性能提升。

Abstract: In 5G, beam training consists of the efficient association of users to beams
for a given beamforming codebook used at the base station and the given
propagation environment in the cell. We propose a convolutional neural network
approach that leverages the position of the base station and geospatial data to
predict beam distributions for all user locations simultaneously. Our method
generalizes to unseen environments without site-specific training or
specialized sensors. The results show that it significantly reduces the number
of candidate beams considered, thereby improving the efficiency of beam
training.

</details>


### [19] [Precoding for Uplink RIS-Assisted Cell-Free MIMO-OFDM Systems with Hardware Impairments](https://arxiv.org/abs/2510.17741)
*Navid Reyhanian,Reza Ghaderi Zefreh,Parisa Ramezani,Emil Bjornson*

Main category: eess.SP

TL;DR: 本文研究了RIS辅助的无蜂窝大规模MIMO系统，在用户设备和接入点存在IQ不平衡的情况下，通过WMMSE-BCD方法联合优化发射预编码、RIS系数和接收组合，以最大化上行链路和速率。


<details>
  <summary>Details</summary>
Motivation: 在RIS辅助的无蜂窝大规模MIMO系统中，用户设备和接入点的IQ不平衡会严重影响系统性能，需要开发有效的联合优化方法来应对这一挑战。

Method: 提出基于加权最小均方误差的块坐标下降方法，开发了新颖的迭代算法来高效求解BCD子问题。

Result: 通过广泛仿真验证了所提方法相对于启发式方法的效率优势。

Conclusion: 所提出的WMMSE-BCD方法能够有效处理IQ不平衡问题，在RIS辅助的无蜂窝大规模MIMO系统中实现上行链路和速率最大化。

Abstract: This paper studies a reconfigurable intelligent surface (RIS)-assisted
cell-free massive multiple-input multiple-output (CF-mMIMO) system with
multiple RISs. Joint design of transmit precoding, RIS coefficients, and
receive combining is investigated for uplink sum-rate maximization under
in-phase and quadrature phase imbalance (IQI) at user equipments (UEs) and
access points (APs). A weighted minimum mean squared error (WMMSE) based block
coordinate descent (BCD) approach is proposed, where novel iterative methods
are developed to efficiently solve the BCD subproblems. The efficiency of
proposed approaches is demonstrated relative to heuristic methods via extensive
simulations.

</details>


### [20] [Sample Complexity Analysis of Multi-Target Detection via Markovian and Hard-Core Multi-Reference Alignment](https://arxiv.org/abs/2510.17775)
*Kweku Abraham,Amnon Balanov,Tamir Bendory,Carlos Esteve-Yagüe*

Main category: eess.SP

TL;DR: 该论文研究了多目标检测问题的样本复杂度，提出了将MTD问题简化为非独立同分布多参考对齐模型的方法，并证明了在低信噪比下信号估计所需的样本数量与噪声方差呈指数关系。


<details>
  <summary>Details</summary>
Motivation: 受单粒子冷冻电镜的启发，研究多目标检测问题中信号在长噪声观测中多次出现时的样本复杂度。

Method: 提出了一种分块方案，将MTD问题简化为非独立同分布的多参考对齐模型，在一维和二维情况下分别建立了相应的理论框架。

Result: 证明了一维情况下估计器的收敛率与对应的i.i.d. MRA模型相匹配，二维情况下建立了类似的指数混合随机场结果。

Conclusion: 在低信噪比下，MTD模型中估计信号所需的分块数量与噪声方差呈指数关系，具体为σ^{2n_min}，其中n_min是确定信号所需的最小矩阶数。

Abstract: Motivated by single-particle cryo-electron microscopy, we study the sample
complexity of the multi-target detection (MTD) problem, in which an unknown
signal appears multiple times at unknown locations within a long, noisy
observation. We propose a patching scheme that reduces MTD to a non-i.i.d.
multi-reference alignment (MRA) model. In the one-dimensional setting, the
latent group elements form a Markov chain, and we show that the convergence
rate of any estimator matches that of the corresponding i.i.d. MRA model, up to
a logarithmic factor in the number of patches. Moreover, for estimators based
on empirical averaging, such as the method of moments, the convergence rates
are identical in both settings. We further establish an analogous result in two
dimensions, where the latent structure arises from an exponentially mixing
random field generated by a hard-core placement model. As a consequence, if the
signal in the corresponding i.i.d. MRA model is determined by moments up to
order $n_{\min}$, then in the low-SNR regime the number of patches required to
estimate the signal in the MTD model scales as $\sigma^{2n_{\min}}$, where
$\sigma^2$ denotes the noise variance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks](https://arxiv.org/abs/2510.16063)
*Muhy Eddin Za'ter,Bri-Mathias Hodge*

Main category: cs.LG

TL;DR: 提出了一种用于变电站级电压估计的分层图神经网络，该网络利用电气拓扑和物理特征，在真实配电网低可观测性条件下保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源渗透率和配电网电压波动性增加，需要鲁棒的配电网状态估计来维持电网安全高效运行，但传统方法难以应对稀疏测量和大规模网络。

Method: 使用分层图神经网络，结合电气拓扑和物理特征，在SMART-DS数据集上进行训练和评估，涵盖多个变电站和DER渗透场景。

Result: 相比其他数据驱动模型，所提方法实现了高达2倍的RMSE降低，在仅1%测量覆盖率下仍保持高精度。

Conclusion: 图神经网络有潜力为配电网提供可扩展、可复现和数据驱动的电压监测解决方案。

Abstract: Accurate voltage estimation in distribution networks is critical for
real-time monitoring and increasing the reliability of the grid. As DER
penetration and distribution level voltage variability increase, robust
distribution system state estimation (DSSE) has become more essential to
maintain safe and efficient operations. Traditional DSSE techniques, however,
struggle with sparse measurements and the scale of modern feeders, limiting
their scalability to large networks. This paper presents a hierarchical graph
neural network for substation-level voltage estimation that exploits both
electrical topology and physical features, while remaining robust to the low
observability levels common to real-world distribution networks. Leveraging the
public SMART-DS datasets, the model is trained and evaluated on thousands of
buses across multiple substations and DER penetration scenarios. Comprehensive
experiments demonstrate that the proposed method achieves up to 2 times lower
RMSE than alternative data-driven models, and maintains high accuracy with as
little as 1\% measurement coverage. The results highlight the potential of GNNs
to enable scalable, reproducible, and data-driven voltage monitoring for
distribution systems.

</details>


### [22] [Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions](https://arxiv.org/abs/2510.16064)
*Muhy Eddin Za'ter,Bri-Mathias Hodge,Kyri Baker*

Main category: cs.LG

TL;DR: 提出一种基于残差学习的AC最优潮流求解方法，使用DC OPF解作为基线，学习非线性修正来获得AC OPF解，实现25% MSE降低和13倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决AC最优潮流问题的计算瓶颈，为实时电网运行提供快速准确的解决方案。

Method: 使用拓扑感知图神经网络，结合局部注意力机制和两级DC特征集成，采用物理信息损失函数确保AC潮流可行性和运行限制。

Result: 在57、118和2000母线系统上测试，MSE降低约25%，可行性误差减少3倍，运行速度提升13倍，且在N-1故障下保持准确性。

Conclusion: 残差学习是连接线性近似和AC可行OPF的实用可扩展桥梁，支持近实时运行决策。

Abstract: Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major
computational bottleneck for real-time grid operations. In this paper, we
propose a residual learning paradigm that uses fast DC optimal power flow (DC
OPF) solutions as a baseline, and learns only the nonlinear corrections
required to provide the full AC-OPF solution. The method utilizes a
topology-aware Graph Neural Network with local attention and two-level DC
feature integration, trained using a physics-informed loss that enforces AC
power-flow feasibility and operational limits. Evaluations on OPFData for 57-,
118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in
feasibility error, and up to 13X runtime speedup compared to conventional AC
OPF solvers. The model maintains accuracy under N-1 contingencies and scales
efficiently to large networks. These results demonstrate that residual learning
is a practical and scalable bridge between linear approximations and
AC-feasible OPF, enabling near real-time operational decision making.

</details>


### [23] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [24] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: 论文提出ControlValve防御机制，通过生成允许的控制流图并强制执行，解决多智能体系统中的控制流劫持攻击问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于对齐检查的防御机制（如LlamaFirewall）无法有效抵御控制流劫持攻击，因为安全性和功能性目标存在根本冲突，且对齐定义脆弱、检查器对执行上下文可见性不完整。

Method: 提出ControlValve防御机制，包含两个核心组件：(1) 为多智能体系统生成允许的控制流图；(2) 强制执行所有执行必须符合这些控制流图以及每个智能体调用的上下文规则（以零样本方式生成）。

Result: ControlValve能够有效防御控制流劫持攻击，即使攻击能够绕过由先进LLM执行的对齐检查。

Conclusion: ControlValve基于控制流完整性和最小权限原则，为多智能体系统提供了更有效的安全防御机制，解决了现有对齐检查方法的局限性。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [25] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 该论文分析了安全强化学习中拉格朗日乘子的最优性和稳定性，发现自动更新乘子能够恢复甚至超过最优性能，但存在振荡行为，可通过PID控制缓解。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域中，拉格朗日方法的效果严重依赖于拉格朗日乘子λ的选择，但实践中缺乏对自动更新鲁棒性的实证证据。

Method: 通过分析拉格朗日乘子在多种任务中的最优性和稳定性，提供λ-配置文件可视化优化问题中的权衡关系。

Result: 发现λ具有高度敏感性，自动乘子更新能够恢复甚至超过最优性能，但存在振荡行为，PID控制可缓解但需仔细调参。

Conclusion: 拉格朗日方法在安全强化学习中需要进一步研究以提升稳定性。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [26] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 评估了5个时间序列基础模型和2个基线模型的校准特性，发现时间序列基础模型比基线模型校准更好，且不会系统性地过度自信或不足自信


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在预测性能上达到最先进水平，但其校准特性相对未被充分探索，而校准对许多实际应用至关重要

Method: 对5个时间序列基础模型和2个竞争性基线模型进行系统评估，包括模型校准、不同预测头的影响以及长期自回归预测下的校准

Result: 时间序列基础模型比基线模型校准更好，且不会系统性地过度自信或不足自信，这与深度学习模型常见的过度自信现象形成对比

Conclusion: 时间序列基础模型具有良好的校准特性，这为实际应用提供了重要优势

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [27] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 开发了一个决策支持系统，通过预训练的AI代理缩小人类可采取的行动范围，然后让人类从该行动集中选择行动，实现了人类与AI在顺序决策任务中的互补性。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在顺序决策任务中实现人类与AI的互补性，就像在分类任务中那样，通过自适应控制人类代理水平来提高决策准确性。

Method: 使用预训练的AI代理将人类可采取的行动缩小到一个子集，然后让人类从该行动集中选择行动，并引入一个利用系统提供的行动集平滑性特性来优化人类代理水平的bandit算法。

Result: 在包含1,600名参与者的大规模人类主题研究中，使用该系统的参与者在野火缓解游戏中的表现比单独游戏的参与者高出约30%，比系统使用的AI代理高出2%以上。

Conclusion: 该系统成功实现了人类与AI在顺序决策任务中的互补性，即使AI代理在无支持情况下表现优于人类参与者，但人类在使用系统支持后能超越AI代理的表现。

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [28] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: 首次对时变学习策略下的Q-learning算法进行有限时间分析，证明了在最小假设下的收敛性，匹配了离策略Q-learning的样本复杂度但探索性较弱。


<details>
  <summary>Details</summary>
Motivation: 现有Q-learning分析主要针对固定策略或离策略采样，缺乏对时变学习策略（即同策略采样）的理论分析，这在实际算法中很常见。

Method: 采用改进方法，利用泊松方程将马尔可夫噪声分解为鞅差项和残差项，并对泊松方程解进行敏感性分析以控制时变策略下的残差。

Result: 建立了Q函数期望无穷范数平方的最终迭代收敛率，样本复杂度为O(1/ε²)，与离策略Q-learning匹配但探索参数依赖更差。

Conclusion: 同策略Q-learning探索性较弱但具有利用优势，其策略会收敛到最优策略而非保持固定。所开发的分析工具可推广到其他时变策略的强化学习算法。

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [29] [Expert Merging in Sparse Mixture of Experts with Nash Bargaining](https://arxiv.org/abs/2510.16138)
*Dung V. Nguyen,Anh T. Nguyen,Minh H. Nguyen,Luc Q. Nguyen,Shiqi Jiang,Ethan Fetaya,Linh Duy Tran,Gal Chechik,Tan M. Nguyen*

Main category: cs.LG

TL;DR: NAMEx是一种基于博弈论纳什议价的新型专家合并框架，通过引入复杂动量加速专家传播，在多个任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏专家混合模型的专家合并策略缺乏理论依据的权重机制，需要更平衡高效的专家协作方法。

Method: 从博弈论视角重新解释专家合并，引入纳什议价机制，并结合复杂动量来加速专家传播并保证收敛。

Result: 在语言建模、文本分类、图像分类和数据损坏下的零样本鲁棒性等任务中，NAMEx始终优于竞争方法，并能无缝集成到主流MoE架构中。

Conclusion: NAMEx在大规模系统（如Qwen1.5-MoE和DeepSeek-MoE）中表现出色，在零样本和微调设置下均有效，证明了其可扩展性。

Abstract: Existing expert merging strategies for Sparse Mixture of Experts (SMoE)
typically rely on input-dependent or input-independent averaging of expert
parameters, but often lack a principled weighting mechanism. In this work, we
reinterpret expert merging through the lens of game theory, revealing
cooperative and competitive dynamics among experts. Based on this perspective,
we introduce Nash Merging of Experts (NAMEx), a novel framework that
incorporates Nash Bargaining into the merging process, enabling more balanced
and efficient collaboration among experts. Additionally, we incorporate complex
momentum into NAMEx to accelerate expert propagation with theoretical
guarantees for convergence. Extensive experiments across language modelling,
text classification, image classification, and zero-shot robustness under data
corruption show that NAMEx consistently outperforms competing methods while
integrating seamlessly with popular MoE architectures. Finally, we demonstrate
NAMEx's scalability by applying it to large-scale systems, including
Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both
zero-shot and fine-tuning settings.

</details>


### [30] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合零阶优化和锐度感知最小化的新方法，通过指数倾斜目标在平均损失和最大损失之间平滑过渡，实现了更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法优化平滑后的目标函数（即扰动参数下的期望目标），而SAM方法关注邻域内的最大损失以获得平坦最小值。本文旨在明确连接这两种方法。

Method: 使用指数倾斜目标构建软SAM目标，通过倾斜参数t在平均损失和最大损失之间平滑过渡，开发新的零阶算法来求解该目标。

Result: 该方法在分类、多项选择问答和语言生成等下游任务上，相比传统零阶基线实现了更好的泛化性能，可作为SAM变体的无梯度和内存高效替代方案。

Conclusion: 提出的倾斜SAM框架成功连接了零阶优化和SAM方法，提供了对锐度概念的精确表征，并在实际应用中展现出优越的泛化能力。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [31] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: 提出基于WaveNet的深度学习模型，用于自动分类EEG信号为生理、病理、伪影和噪声类别，在大型数据集上表现优于传统CNN和LSTM方法。


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家视觉审查的EEG信号分类方法在处理日益复杂的EEG记录时变得不切实际，需要自动化解决方案。

Method: 使用WaveNet架构，利用扩张因果卷积和残差连接处理EEG数据，在Mayo Clinic和St. Anne's大学医院的公开数据集上进行训练验证测试。

Result: 模型分类准确率超过之前的CNN和LSTM方法，能高精度区分噪声和伪影，但在生理和病理信号间存在可解释的误分类。

Conclusion: WaveNet架构适合EEG数据分析，能捕捉细粒度和长程时间依赖性，为EEG自动分类提供了有效解决方案。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [32] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: GRUwE：基于门控循环单元和指数基函数的不规则采样多元时间序列建模方法，在连续时间中支持回归和事件预测，通过两种重置机制更新马尔可夫状态，性能与SOTA方法相当或更优，但更简单高效。


<details>
  <summary>Details</summary>
Motivation: 解决不规则采样多元时间序列建模的挑战，验证简单高效的RNN架构是否仍能与复杂方法竞争，提供更易实现和部署的解决方案。

Method: 基于RNN架构，使用指数基函数，通过观测触发重置和时间触发重置两种机制更新GRU状态，支持连续时间预测。

Result: 在多个真实世界基准测试中，GRUwE在下一观测和下一事件预测任务上达到与最新SOTA方法相当或更优的性能。

Conclusion: GRUwE证明了简单RNN架构的竞争力，提供易于实现、调参少、计算开销低的优势，特别适合在线部署。

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [33] [A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2510.16940)
*Cristian J. Vaca-Rubio,Roberto Pereira,Luis Blanco,Engin Zeydan,Màrius Caus*

Main category: cs.LG

TL;DR: P-KAN是一种概率性Kolmogorov-Arnold网络，通过将标量权重替换为样条函数连接并直接参数化预测分布，为时间序列预测提供表达力强且参数高效的模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法在时间序列预测中难以有效捕捉非线性和重尾动态，且缺乏不确定性感知能力，特别是在卫星通信等资源受限领域需要动态阈值设置。

Method: 使用样条基函数连接替代标量权重，直接参数化预测分布（高斯分布和Student-t分布），构建概率性KAN模型。

Result: 在卫星流量预测中，P-KAN在准确性和校准方面持续优于MLP基线，使用更少参数实现更优的效率-风险权衡。高斯变体提供稳健预测，Student-t变体在稳定需求下提供更尖锐分布。

Conclusion: P-KAN为概率预测提供了强大框架，特别适用于卫星通信等资源受限领域，能够有效平衡预测准确性和不确定性建模。

Abstract: This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel
probabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series
forecasting. By replacing scalar weights with spline-based functional
connections and directly parameterizing predictive distributions, P-KANs offer
expressive yet parameter-efficient models capable of capturing nonlinear and
heavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,
where uncertainty-aware predictions enable dynamic thresholding for resource
allocation. Results show that P-KANs consistently outperform Multi Layer
Perceptron (MLP) baselines in both accuracy and calibration, achieving superior
efficiency-risk trade-offs while using significantly fewer parameters. We build
up P-KANs on two distributions, namely Gaussian and Student-t distributions.
The Gaussian variant provides robust, conservative forecasts suitable for
safety-critical scenarios, whereas the Student-t variant yields sharper
distributions that improve efficiency under stable demand. These findings
establish P-KANs as a powerful framework for probabilistic forecasting with
direct applicability to satellite communications and other resource-constrained
domains.

</details>


### [34] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 提出基于运行时监控语言(RML)的新型语言奖励机，能够表达非正则、非马尔可夫的任务奖励函数，解决了传统奖励机表达能力受限的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中奖励函数通常被视为黑盒映射，缺乏解释性，且传统奖励机只能表达正则语言，无法处理计数或参数化条件等复杂行为。

Method: 利用RML的内置内存机制构建语言奖励机，扩展奖励机的表达能力，支持非正则和非马尔可夫任务。

Result: 实验证明该方法在表达能力上优于现有奖励机方法，在事件处理和任务规范方面具有灵活性优势。

Conclusion: 基于RML的语言奖励机为复杂强化学习任务提供了更强大的奖励函数规范能力，提高了学习效率和可解释性。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [35] [RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems](https://arxiv.org/abs/2510.17396)
*Keivan Faghih Niresi,Zepeng Zhang,Olga Fink*

Main category: cs.LG

TL;DR: 提出了RINS-T框架，一种无需预训练数据的深度先验方法，用于解决时间序列线性逆问题，通过鲁棒优化技术提高对异常值的抵抗能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常受缺失值、噪声和异常值影响，现有深度学习方法需要大量预训练且难以应对分布偏移。

Method: 利用神经网络作为隐式先验，结合鲁棒优化技术，引入引导输入初始化、输入扰动和凸输出组合三个关键技术。

Result: 实现了高恢复性能，对异常值具有鲁棒性，且不依赖高斯噪声假设。

Conclusion: RINS-T为解决复杂现实世界时间序列问题提供了灵活有效的解决方案。

Abstract: Time series data are often affected by various forms of corruption, such as
missing values, noise, and outliers, which pose significant challenges for
tasks such as forecasting and anomaly detection. To address these issues,
inverse problems focus on reconstructing the original signal from corrupted
data by leveraging prior knowledge about its underlying structure. While deep
learning methods have demonstrated potential in this domain, they often require
extensive pretraining and struggle to generalize under distribution shifts. In
this work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series
Linear Inverse Problems), a novel deep prior framework that achieves high
recovery performance without requiring pretraining data. RINS-T leverages
neural networks as implicit priors and integrates robust optimization
techniques, making it resilient to outliers while relaxing the reliance on
Gaussian noise assumptions. To further improve optimization stability and
robustness, we introduce three key innovations: guided input initialization,
input perturbation, and convex output combination techniques. Each of these
contributions strengthens the framework's optimization stability and
robustness. These advancements make RINS-T a flexible and effective solution
for addressing complex real-world time series challenges. Our code is available
at https://github.com/EPFL-IMOS/RINS-T.

</details>


### [36] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: S4ECG是一种基于结构化状态空间模型的深度学习架构，用于多时期心律失常分类，显著提升了心房颤动检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统ECG分析方法难以同时捕捉全局趋势和局部波形特征的高时间分辨率交互，需要桥接全局和局部信号分析。

Method: 引入S4ECG架构，利用结构化状态空间模型进行多时期心律失常分类，通过联合多时期预测优于单时期方法。

Result: 多时期预测在宏观AUROC上比单时期方法提升1.0-11.6%，心房颤动特异性从0.718-0.979提升至0.967-0.998，在分布内和分布外均表现出优越性能。

Conclusion: 这项工作推动了向时间感知心律失常检测算法的范式转变，为ECG解释特别是复杂心律失常如心房颤动和心房扑动开辟了新可能性。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [37] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder是一个针对Lean和mathlib的语义搜索引擎，通过理解数学家意图、分析公开讨论、微调文本嵌入和多样化反馈信号，在真实查询中相比之前搜索引擎和GPT-4o实现了30%以上的相对改进。


<details>
  <summary>Details</summary>
Motivation: 现有Lean搜索引擎主要依赖非正式化翻译，忽视了与现实用户查询的不匹配，导致定理证明进展缓慢且劳动密集。

Method: 分析并聚类公开Lean讨论的语义，在模拟用户意图的合成查询上微调文本嵌入，使用多样化反馈信号与数学家偏好对齐。

Result: 在真实查询、非正式化语句和证明状态评估中，相比之前搜索引擎和GPT-4o实现了超过30%的相对改进。

Conclusion: Lean Finder提供了一个用户中心的语义搜索，与基于LLM的定理证明器兼容，桥接了检索与形式推理。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [38] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 该论文对标签噪声检测方法进行了系统性基准测试，通过将方法分解为标签一致性函数、聚合方法和信息收集方式三个组件，发现在大多数场景下使用平均概率聚合和logit边界作为标签一致性函数的in-sample方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中普遍存在标签噪声问题，影响模型训练和验证。虽然已有多种噪声标签检测技术，但缺乏明确的共识和最优方法选择指导。

Method: 将现有检测方法分解为三个基本组件：标签一致性函数、聚合方法和信息收集方式（in-sample vs out-of-sample），提出统一基准任务和新的评估指标。

Result: 在视觉和表格数据集上的评估表明，使用平均概率聚合和logit边界作为标签一致性函数的in-sample信息收集方法在大多数场景下表现最佳。

Conclusion: 研究结果为设计新的检测方法和为特定应用选择技术提供了实用指导，识别出了跨场景表现最优的检测方法组合。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [39] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出了一种基于保形对齐的级联机制(CAb)，用于边缘-云模型级联系统，保证边缘预测集满足云模型级别的条件覆盖要求，同时减少向云端的卸载。


<details>
  <summary>Details</summary>
Motivation: 边缘智能虽然能通过紧凑的端侧模型实现低延迟推理，但保证可靠性仍然具有挑战性。需要确保边缘返回的预测集能以用户指定的概率包含真实标签，就像云模型产生的一样。

Method: 将边缘到云端的升级建模为多重假设检验问题，采用保形对齐技术来选择哪些输入可以在边缘安全处理。该方法适用于任意的边缘预测集，包括各种保形预测变体。

Result: 在CIFAR-100图像分类和TeleQnA问答基准测试中，CAb级联方法在保持目标条件覆盖的同时，显著减少了向云端的卸载，预测集大小仅适度增加。

Conclusion: CAb级联方法为边缘-云模型级联系统提供了统计保证，在覆盖度、延迟率和集合大小之间提供了可调节的权衡。

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [40] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: 提出LS-OGD框架，通过动态调整学习率和模态融合权重来应对概念漂移，确保多模态学习系统在非平稳环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中面临概念漂移的挑战，特别是模态特定的漂移和缺乏连续稳定适应机制的问题。

Method: 使用在线控制器动态调整模型学习率和不同数据模态的融合权重，响应检测到的漂移和预测误差变化。

Result: 在有限漂移条件下，LS-OGD系统的预测误差被证明是最终一致有界的，如果漂移停止则收敛到零；自适应融合策略能有效隔离和减轻严重模态特定漂移的影响。

Conclusion: 为开发可靠且持续自适应的多模态学习系统建立了理论基础，确保系统韧性和容错能力。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [41] [One-Bit Quantization for Random Features Models](https://arxiv.org/abs/2510.16250)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: cs.LG

TL;DR: 该论文分析了神经网络中一比特权重压缩的理论基础，证明了在随机特征模型中，除最后一层外所有层的权重量化不会导致泛化误差损失，并展示了实际推理速度的提升。


<details>
  <summary>Details</summary>
Motivation: 神经网络的计算和内存需求日益增长，促使研究一比特权重压缩以在资源受限设备上实现高效推理，但相关理论基础尚不明确。

Method: 在随机特征模型（神经网络的简化框架）中分析一比特量化，理论证明除最后一层外所有层权重量化的可行性，并进行实证验证。

Result: 理论证明除最后一层外所有层权重量化不会损失泛化误差，实验证实一比特量化在笔记本电脑GPU上显著提升推理速度。

Conclusion: 一比特权重压缩在随机特征模型中具有理论保证和实际效益，为神经网络压缩提供了理论洞察。

Abstract: Recent advances in neural networks have led to significant computational and
memory demands, spurring interest in one-bit weight compression to enable
efficient inference on resource-constrained devices. However, the theoretical
underpinnings of such compression remain poorly understood. We address this gap
by analyzing one-bit quantization in the Random Features model, a simplified
framework that corresponds to neural networks with random representations. We
prove that, asymptotically, quantizing weights of all layers except the last
incurs no loss in generalization error, compared to the full precision random
features model. Our findings offer theoretical insights into neural network
compression. We also demonstrate empirically that one-bit quantization leads to
significant inference speed ups for the Random Features models even on a laptop
GPU, confirming the practical benefits of our work. Additionally, we provide an
asymptotically precise characterization of the generalization error for Random
Features with an arbitrary number of layers. To the best of our knowledge, our
analysis yields more general results than all previous works in the related
literature.

</details>


### [42] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一个基于贝叶斯学习的自适应采样框架，通过实时更新奖励分布的后验信念来决定何时停止生成新样本，在保持响应质量的同时显著减少采样次数。


<details>
  <summary>Details</summary>
Motivation: 多响应采样能提高LLM输出质量，但计算成本高昂。关键挑战是如何平衡准确性提升与效率，决定何时停止生成新样本。

Method: 基于序列搜索和贝叶斯学习，BEACON顺序生成响应、实时更新奖励分布后验信念，通过权衡预期收益与计算成本来决定停止时机。

Result: BEACON将平均采样次数减少高达80%，同时保持响应质量，在成本效益偏好数据生成中表现出实用性。

Conclusion: BEACON提供了理论最优性保证和实际可行性，为未来研究者提供了可操作的见解，是平衡LLM输出质量与计算效率的有效解决方案。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [43] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 提出基于神经常微分方程的连续深度Evoformer，用Neural ODE替代AlphaFold中48个离散块，实现恒定内存成本和计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统Evoformer的48层深度结构计算成本高且层间离散化，需要更高效的连续深度模型来降低资源消耗。

Method: 使用Neural ODE参数化Evoformer的核心注意力操作，通过伴随方法实现恒定内存成本，利用自适应ODE求解器平衡运行时间和精度。

Result: 模型能生成结构合理的蛋白质预测，可靠捕捉α-螺旋等二级结构元素，但精度不及原架构；仅需单GPU训练17.5小时，资源消耗大幅降低。

Conclusion: 连续深度模型为生物分子建模提供了轻量级、可解释的替代方案，为高效自适应蛋白质结构预测开辟了新方向。

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [44] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: PatMD通过识别和主动缓解误判风险模式，提升有害表情包检测性能，在5个检测任务上平均F1分数提升8.30%，准确率提升7.71%。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法（包括MLLM技术）难以处理表情包中通过讽刺、隐喻等修辞手法表达的隐含有害内容，导致频繁误判。

Method: 构建误判风险模式知识库，将表情包解构为误判风险模式，检索相关模式动态指导MLLM推理，避免已知误判陷阱。

Result: 在6,626个表情包的基准测试中，PatMD优于现有最先进基线方法，展示了强大的泛化能力和改进的有害表情包检测能力。

Conclusion: 通过主动识别和缓解误判风险模式，PatMD能够有效提升有害表情包的检测性能，解决了现有方法在处理隐含有害内容时的局限性。

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [45] [Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)
*Fuqun Han,Stanley Osher,Wuchen Li*

Main category: cs.LG

TL;DR: 提出一种稀疏transformer架构，将数据分布的先验信息直接融入神经网络结构，基于正则化Wasserstein邻近算子设计，相比传统流模型优化问题凸性更好且生成样本更稀疏。


<details>
  <summary>Details</summary>
Motivation: 将数据分布先验信息直接整合到transformer架构中，改进传统基于神经ODE方法的优化问题凸性并促进生成样本的稀疏性。

Method: 基于正则化Wasserstein邻近算子设计稀疏transformer架构，该算子有闭式解且表现为特殊的transformer结构表示。

Result: 在生成建模和贝叶斯逆问题应用中，稀疏transformer相比传统神经ODE方法达到更高精度和更快收敛到目标分布。

Conclusion: 稀疏transformer架构通过融入先验信息有效改进优化问题性质，在生成任务中表现优于传统方法。

Abstract: In this work, we propose a sparse transformer architecture that incorporates
prior information about the underlying data distribution directly into the
transformer structure of the neural network. The design of the model is
motivated by a special optimal transport problem, namely the regularized
Wasserstein proximal operator, which admits a closed-form solution and turns
out to be a special representation of transformer architectures. Compared with
classical flow-based models, the proposed approach improves the convexity
properties of the optimization problem and promotes sparsity in the generated
samples. Through both theoretical analysis and numerical experiments, including
applications in generative modeling and Bayesian inverse problems, we
demonstrate that the sparse transformer achieves higher accuracy and faster
convergence to the target distribution than classical neural ODE-based methods.

</details>


### [46] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出一个序列强化学习框架用于模仿学习，旨在建模传粉昆虫的异质认知策略，特别关注蜜蜂行为分析。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在处理专家策略随时间变化或偏离最优性时表现不佳，无法捕捉快速和慢速学习行为，且缺乏可解释性，限制了生物学洞察。

Method: 引入最小化预测损失的模型，识别与行为数据最一致的有效记忆窗口，确保完全可解释性，并提供连接蜜蜂策略搜索与多臂赌博机问题的数学框架。

Result: 创建了包含80只蜜蜂在不同天气条件下追踪数据的新数据集，改进了昆虫行为模拟，为传粉昆虫认知研究提供了基准。

Conclusion: 该研究揭示了塑造传粉昆虫决策的学习策略和记忆相互作用，为生态治理和农业生态系统中的昆虫行为模拟提供了新见解。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [47] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 提出基于击键动力学的帕金森病筛查方法，使用深度学习模型在外部验证中取得超过90%的AUC-ROC和70%以上的F1分数。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，传统临床评估存在局限性。击键动力学提供了一种非侵入性、可扩展的远程筛查和监测方法。

Method: 三阶段流程：数据预处理（处理4个数据集，提取时间信号，解决类别不平衡）；预训练8种深度学习架构；在中等数据集上微调并在独立队列上进行外部验证。

Result: 混合卷积-循环和基于Transformer的模型表现优异，时间卷积模型在外部验证中达到91.14%的AUC-ROC，优于仅依赖内部验证的现有方法。

Conclusion: 击键动力学作为帕金森病的可靠数字生物标志物，为早期检测和持续监测提供了有前景的途径。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [48] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: OracleAD是一个简单可解释的无监督多变量时间序列异常检测框架，通过因果嵌入建模时间动态，使用自注意力机制捕捉空间关系，并基于预测误差和偏离稳定潜在结构来识别异常。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多变量时间序列异常罕见且通常无标签，现有方法依赖复杂架构，只能检测异常片段且性能被夸大。

Method: 将每个变量的过去序列编码为因果嵌入来联合预测当前时间点和重构输入窗口，使用自注意力机制将嵌入投影到共享潜在空间，并与代表正常状态关系的稳定潜在结构对齐。

Result: 在多个真实世界数据集和评估协议上取得了最先进的结果。

Conclusion: OracleAD通过稳定潜在结构实现可解释性，直接在嵌入层面识别根因变量，提供细粒度的异常诊断。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [49] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文研究了基于扩散模型的集成评分滤波器(EnSF)在野火蔓延实时预测数据同化中的应用，证明了该方法在准确性、稳定性和计算效率方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 随着野火破坏性日益增强且控制成本高昂，需要准确、实时的火势蔓延预测来进行有效管理。数据同化通过整合观测数据和数值模型预测，对提高野火预测精度至关重要。

Method: 应用基于扩散模型的集成评分滤波器(EnSF)，利用基于评分的生成扩散模型来处理高维非线性滤波问题，特别适用于野火蔓延模型的滤波问题。

Result: 数值研究表明，EnSF在野火数据同化中表现出优越的准确性、稳定性和计算效率，证明其是一种稳健实用的方法。

Conclusion: EnSF为野火蔓延实时预测的数据同化问题提供了一个强大而有效的解决方案，代码已公开可用。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [50] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 提出了一种基于连通性因子(CF)的eDCF方法，用于鲁棒地估计不同尺度下的内在维度，在噪声环境下表现优异，并能准确检测分形几何。


<details>
  <summary>Details</summary>
Motivation: 现代数据集通常包含具有复杂依赖关系的高维特征，估计内在维度作为底层复杂性的度量很重要，但由于尺度依赖性而具有挑战性。

Method: 基于连通性因子(CF)的局部连通性度量方法eDCF，具有可扩展性和并行化能力。

Result: 在合成基准测试中与领先估计器表现相当，在中等至高噪声水平下达到25.0%的精确内在维度匹配率，优于MLE(16.7%)和TWO-NN(12.5%)。

Conclusion: eDCF方法能够鲁棒地估计内在维度，在噪声环境下表现优异，并能有效分析现实结构化数据中的分形几何。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [51] [How Good Are LLMs at Processing Tool Outputs?](https://arxiv.org/abs/2510.15955)
*Kiran Kate,Yara Rizk,Poulami Ghosh,Ashu Gulati,Tathagata Chakraborti,Zidane Wright,Mayank Agarwal*

Main category: cs.LG

TL;DR: 本文研究了LLM处理工具返回的复杂JSON响应的能力，发现即使是前沿模型在处理结构化响应时仍存在困难，不同处理策略可导致3%-50%的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现实任务自动化需要LLM调用工具并处理复杂的JSON响应，但LLM处理结构化响应的能力研究不足。

Method: 创建了专门的数据集，评估了15个开源和闭源模型，使用多种提示策略进行测试。

Result: JSON处理对前沿模型仍是困难任务，最佳处理策略取决于工具输出的性质和大小以及所需推理的复杂性。

Conclusion: 工具响应处理是一个具有挑战性的任务，处理策略的选择对性能有显著影响，需要根据具体情况选择合适的方法。

Abstract: Most realistic task automation problems require large language models (LLMs)
to call tools, which often return complex JSON responses. These responses must
be further processed to derive the information necessary for task completion.
The ability of LLMs to do so is under-studied. In this paper, we study the tool
response processing task and LLMs' abilities to process structured (JSON)
responses. We created a dataset for this task, and evaluated 15 open and closed
weight models using multiple prompting approaches. Our results show that JSON
processing remains a difficult task even for frontier models across multiple
prompting strategies. The optimal response processing strategy depends on both
the nature and size of the tool outputs, as well as the complexity of the
required reasoning. Variations in processing approaches can lead to performance
differences ranging from 3\% to 50\%.

</details>


### [52] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: LLMs在因果发现中的表现被高估，因为评估基准可能已被预训练数据包含。需要开发防泄漏的评估协议和结合LLM知识与统计方法的混合方法。


<details>
  <summary>Details</summary>
Motivation: 挑战LLMs在因果发现中的真实能力，质疑现有评估方法因数据泄漏问题而不可靠，需要更严谨的评估和实用的混合方法。

Method: 提出基于最新科学研究的评估协议防止数据泄漏，设计混合方法将LLM预测作为PC算法的先验知识。

Result: 在传统基准上LLMs表现完美，但在新构建的真实科学图谱上表现较差；将LLM预测作为PC算法先验能显著提升准确性。

Conclusion: 呼吁采用基于科学、防泄漏的评估基准，并投资开发适合真实世界研究的混合因果发现方法。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [53] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 该研究通过热解技术转化食物基生物质，利用人工智能优化过程建模，探索废弃咖啡渣和枣核等未充分利用生物质资源的可持续制氢潜力。


<details>
  <summary>Details</summary>
Motivation: 推动可持续能源和废物管理策略，探索未充分利用生物质资源（如废弃咖啡渣和枣核）的可持续制氢潜力，并利用人工智能提高过程建模精度和优化效率。

Method: 对纯枣核、废弃咖啡渣及其混合物进行工业分析、元素分析、纤维分析、热重分析、动力学分析、热力学分析和热解-微气相色谱分析；使用等转化率方法（KAS、FWO、Friedman）进行动力学建模；训练LSTM模型预测热重曲线。

Result: 混合物3显示出最高的氢产率潜力但活化能最高（313.24 kJ/mol），混合物1具有最佳活化能值（161.75 kJ/mol）；KAS方法被确定为最准确的动力学模型；LSTM模型预测热重曲线具有极高精度（R²: 0.9996-0.9998）。

Conclusion: 人工智能集成显著提高了热解过程建模的准确性，未充分利用的生物质资源具有可持续制氢的潜力，不同混合物在氢产率和活化能方面表现出不同特性。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [54] [Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations](https://arxiv.org/abs/2510.16591)
*Cassidy Ashworth,Pietro Liò,Francesco Caso*

Main category: cs.LG

TL;DR: 该论文研究了神经网络中参数对称性与表达能力在泛化行为中的竞争关系，通过中心极限定理作为测试案例，分析了MLP和GNN在不同对称约束下的表现。


<details>
  <summary>Details</summary>
Motivation: 将物理对称性编码到深度学习模型中可以提高性能，但参数对称性破坏与恢复机制对层次学习动态的影响尚不清楚。

Method: 使用多层感知机(MLP)和图神经网络(GNN)，通过改变权重对称性和激活函数来评估网络表达能力，并将中心极限定理重新构造为累积量递推关系进行分析。

Result: 发现对称约束与表达能力之间存在竞争关系，过于复杂或过度约束的模型泛化能力较差。通过累积量传播框架分析了MLP的泛化行为，并经验验证了该框架在GNN中的扩展。

Conclusion: 这些发现为对称网络的学习动态及其在建模结构化物理变换中的局限性提供了新的见解。

Abstract: Deep learning models have proven enormously successful at using multiple
layers of representation to learn relevant features of structured data.
Encoding physical symmetries into these models can improve performance on
difficult tasks, and recent work has motivated the principle of parameter
symmetry breaking and restoration as a unifying mechanism underlying their
hierarchical learning dynamics. We evaluate the role of parameter symmetry and
network expressivity in the generalisation behaviour of neural networks when
learning a real-space renormalisation group (RG) transformation, using the
central limit theorem (CLT) as a test case map. We consider simple multilayer
perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries
and activation functions across architectures. Our results reveal a competition
between symmetry constraints and expressivity, with overly complex or
overconstrained models generalising poorly. We analytically demonstrate this
poor generalisation behaviour for certain constrained MLP architectures by
recasting the CLT as a cumulant recursion relation and making use of an
established framework to propagate cumulants through MLPs. We also empirically
validate an extension of this framework from MLPs to GNNs, elucidating the
internal information processing performed by these more complex models. These
findings offer new insight into the learning dynamics of symmetric networks and
their limitations in modelling structured physical transformations.

</details>


### [55] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出LAMI框架，通过联合图-语言建模检测青少年非法药物使用并解释行为风险因素


<details>
  <summary>Details</summary>
Motivation: 现有建模方法将调查变量独立处理，忽略了变量间的潜在关联结构，无法充分挖掘青少年药物使用的复杂风险因素

Method: LAMI将个体回答表示为关系图，通过图结构学习层学习潜在连接，并集成大语言模型生成基于图结构和调查语义的自然语言解释

Result: 在YRBS和NSDUH数据集上的实验显示，LAMI在预测准确性上优于竞争基线方法

Conclusion: LAMI能够揭示有意义的行为子结构和心理社会路径，如家庭动态、同伴影响和学校相关压力，这些与已确立的物质使用风险因素一致

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [56] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出一种在差分隐私下进行线性回归的方法，支持有效的统计推断和合成数据生成，特别适用于社会科学中的中小规模数据集。


<details>
  <summary>Details</summary>
Motivation: 社会科学中常见中小规模数据集，现有差分隐私线性回归方法主要关注点估计，缺乏不确定性量化和合成数据生成支持，而主流合成数据方法不适合连续回归数据或需要大数据集。

Method: 使用高斯差分隐私下的偏差校正估计器，提供渐近置信区间，并通过分箱聚合策略实现合成数据生成，使合成数据上的回归与差分隐私回归结果一致。

Result: 实验表明该方法在准确性上优于现有方法，提供有效的置信区间，且生成的合成数据在下游机器学习任务中比当前差分隐私合成数据生成方法更可靠。

Conclusion: 该方法为社会科学中的中小规模连续数据提供了有效的差分隐私线性回归解决方案，支持统计推断和合成数据生成。

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [57] [CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models](https://arxiv.org/abs/2510.15962)
*Zhuxuanzi Wang,Mingqiao Mo,Xi Xiao,Chen Liu,Chenrui Ma,Yunbei Zhang,Xiao Wang,Smita Krishnaswamy,Tianyang Wang*

Main category: cs.LG

TL;DR: CTR-LoRA是一个基于曲率信任区域的参数高效微调框架，通过边际效用分配参数并使用Fisher/Hessian度量信任区域约束更新，在多个基准测试中优于现有PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法在低秩更新、量化或启发式预算重分配方面提高了效率，但往往将容量分配与训练过程中更新演化分离开来。

Method: CTR-LoRA整合了秩调度与稳定性感知优化，基于轻量级二阶代理的边际效用分配参数，并使用Fisher/Hessian度量信任区域约束更新。

Result: 在多个开源骨干模型（7B-13B）上的实验显示，CTR-LoRA在分布内和分布外基准测试中均优于强PEFT基线，提高了准确性、训练稳定性，减少了内存需求并实现了更高的吞吐量。

Conclusion: CTR-LoRA在性能与效率的帕累托前沿上表现出色，为更稳健和可部署的PEFT提供了原则性路径。

Abstract: Parameter-efficient fine-tuning (PEFT) has become the standard approach for
adapting large language models under limited compute and memory budgets.
Although previous methods improve efficiency through low-rank updates,
quantization, or heuristic budget reallocation, they often decouple the
allocation of capacity from the way updates evolve during training. In this
work, we introduce CTR-LoRA, a framework guided by curvature trust region that
integrates rank scheduling with stability-aware optimization. CTR-LoRA
allocates parameters based on marginal utility derived from lightweight
second-order proxies and constrains updates using a Fisher/Hessian-metric trust
region. Experiments on multiple open-source backbones (7B-13B), evaluated on
both in-distribution and out-of-distribution benchmarks, show consistent
improvements over strong PEFT baselines. In addition to increased accuracy,
CTR-LoRA enhances training stability, reduces memory requirements, and achieves
higher throughput, positioning it on the Pareto frontier of performance and
efficiency. These results highlight a principled path toward more robust and
deployable PEFT.

</details>


### [58] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出Gram行列式评分方法，用于在无法获取真实数据的情况下评估数据集的可靠性，该评分具有实验无关性，能有效捕捉数据质量。


<details>
  <summary>Details</summary>
Motivation: 解决从潜在战略性来源收集的数据集可靠性评估问题，在无法获取真实数据的情况下，需要一种方法来衡量报告数据与真实数据的偏差程度。

Method: 定义基于真实数据的可靠性排序，提出Gram行列式评分方法，通过测量观察数据和实验结果的经验分布向量所张成的体积来评估可靠性。

Result: Gram行列式评分能够保持多个基于真实数据的可靠性排序，并且在不同实验中产生相同的可靠性排名（实验无关性）。在合成噪声模型、CIFAR-10嵌入和真实就业数据上的实验验证了该方法的有效性。

Conclusion: Gram行列式评分是一种有效的可靠性评估工具，能够在无法获取真实数据的情况下捕捉数据质量，且具有实验无关性的重要特性。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [59] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: 提出Long Exposure系统，通过解决微调中的Shadowy Sparsity问题来加速参数高效微调(PEFT)，实现最高2.49倍的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调技术虽然重要，但其效率低下带来了时间和成本挑战，特别是微调过程中特有的Shadowy Sparsity问题尚未得到充分解决。

Method: Long Exposure系统包含三个组件：Shadowy-sparsity Exposer使用长感知范围捕捉稀疏细节；Sequence-oriented Predictor处理大序列输入和动态参数；Dynamic-aware Operator优化计算模式和内存访问。

Result: 广泛评估表明，Long Exposure在端到端微调中比现有技术快达2.49倍。

Conclusion: Long Exposure为加速LLMs的PEFT提供了有前景的进展，有效解决了微调中的效率瓶颈问题。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [60] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: 该论文提出了首个在聚合bandit反馈下的表格MDP最佳双世界(BOBW)算法，在已知转移时实现了O(log T)随机遗憾和O(√T)对抗遗憾，并建立了匹配下界证明最优性。


<details>
  <summary>Details</summary>
Motivation: 研究在聚合bandit反馈下的在线学习问题，即学习者只能观察到每个episode的累计损失而非每个状态-动作对的个体损失。现有研究仅关注最坏情况分析，本文首次研究能在随机和对抗环境中都实现低遗憾的BOBW算法。

Method: 结合了基于占用度量的FTRL、自边界技术以及受在线最短路径问题启发的新损失估计器。对于未知转移情况，还融入了基于置信度的技术。

Result: 在已知转移情况下，算法在随机环境中达到O(log T)遗憾，在对抗环境中达到O(√T)遗憾，并建立了匹配下界证明最优性。还扩展到未知转移设置，并提供了首个个体间隙依赖下界。

Conclusion: 成功开发了聚合bandit反馈下表格MDP的首个BOBW算法，证明了在随机和对抗环境中的最优性能，为在线强化学习中的聚合反馈问题提供了新的解决方案。

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [61] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种针对大型推理模型的死锁攻击方法，通过训练恶意对抗嵌入诱导模型陷入无限推理循环，阻止其完成答案生成。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型的链式思维推理机制引入了新的安全漏洞，需要研究如何通过资源耗尽攻击来劫持模型的生成控制流。

Method: 使用优化的对抗嵌入鼓励模型在推理步骤后生成过渡性标记，并结合后门植入策略确保攻击的可靠激活。

Result: 在四个先进LRM和三个数学推理基准测试中实现了100%的攻击成功率，迫使模型生成达到最大标记限制。

Conclusion: 该研究揭示了LRM在推理效率方面的关键安全漏洞，这种攻击具有隐蔽性且对现有缓解策略具有鲁棒性。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [62] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出基于矩阵自由能的新型自编码器正则化方法，通过优化代码矩阵的奇异值分布使其接近高斯分布，提升泛化能力并应用于欠定逆问题


<details>
  <summary>Details</summary>
Motivation: 传统自编码器缺乏对代码分布的正则化约束，导致泛化能力不足。需要一种能确保代码矩阵具有高斯分布特性的正则化方法

Method: 基于矩阵自由能定义可微损失函数，通过随机矩阵理论优化代码矩阵的奇异值分布，使其与独立同分布高斯随机矩阵的奇异值分布一致

Result: 经验模拟显示，最小化负矩阵自由能可产生高斯化代码，在训练集和测试集上均表现出良好的泛化性能

Conclusion: 矩阵自由能正则化能有效生成高斯分布代码，为欠定逆问题等应用提供了可靠解决方案

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [63] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 提出了一种细粒度的联邦域自适应方法Gains，用于处理开放世界中不断加入新客户的联邦学习场景，通过知识发现和知识适应来整合新知识。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的联邦学习场景中，新客户会不断加入并带来新知识，而传统联邦学习假设封闭世界且客户数量固定。现有方法在知识发现方面粒度较粗，且会牺牲源域性能和适应效率。

Method: 将模型分为编码器和分类器，利用编码器特征对域偏移敏感、分类器参数对类别增量敏感的特性，开发细粒度知识发现和贡献驱动聚合技术，并设计抗遗忘机制保护源域性能。

Result: 在三个典型数据偏移场景的多域数据集上的实验结果表明，Gains在源域和目标域客户性能上均显著优于其他基线方法。

Conclusion: Gains方法能够有效处理开放世界联邦学习中的新知识整合问题，实现平衡的自适应，同时保持源域性能。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [64] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出自适应离散化一致性模型(ADCMs)，通过优化离散化步长实现自动离散化，提高训练效率和生成性能


<details>
  <summary>Details</summary>
Motivation: 现有一致性模型依赖手动设计的离散化方案，需要针对不同噪声调度和数据集反复调整，缺乏自适应能力

Method: 将离散化建模为优化问题，以局部一致性为优化目标确保可训练性，全局一致性为约束保证稳定性，使用拉格朗日乘子平衡两者，采用高斯-牛顿方法实现自适应离散化

Result: 在CIFAR-10和ImageNet上显著提高训练效率，以最小训练开销获得更优生成性能，对更先进的扩散模型变体表现出强适应性

Conclusion: ADCMs为一致性模型提供了一种统一的自适应离散化框架，有效解决了手动离散化方案的局限性

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [65] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: 提出SAU-FNO框架，结合自注意力机制、U-Net和FNO，用于3D IC热管理，实现842倍加速并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 3D IC热管理面临功率密度增加的挑战，传统PDE方法速度慢，机器学习方法存在高频信息丢失和高保真数据依赖问题。

Method: 结合自注意力机制和U-Net的FNO框架，使用迁移学习微调低保真数据，减少高保真数据集需求。

Result: SAU-FNO实现最先进的热预测精度，相比传统FEM方法加速842倍。

Conclusion: SAU-FNO是高效先进的3D IC热仿真工具，平衡了精度和速度。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [66] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分推断的扩展方法，将确定性机器学习方法扩展到多变量高斯分布预测，用于数据同化中的不确定性建模。


<details>
  <summary>Details</summary>
Motivation: 数据同化在大多数设置中涉及不确定性，现有确定性方法无法充分处理这种不确定性，需要扩展到概率性框架。

Method: 基于变分推断扩展现有确定性机器学习方法，使预测状态遵循多变量高斯分布，并在混沌Lorenz-96动力学上进行测试。

Result: 新模型能够获得近乎完美校准的预测，并且可以在更长的数据同化窗口中集成到变分数据同化管道中，获得更大收益。

Conclusion: 提出的变分推断扩展方法有效处理了数据同化中的不确定性，提供了校准良好的预测，并提升了数据同化性能。

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [67] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: LinearizeLLM是一个基于代理的框架，利用大语言模型自动将非线性优化问题重新表述为线性优化问题。


<details>
  <summary>Details</summary>
Motivation: 非线性优化问题的重新表述通常需要手动完成且依赖专家知识，这对于使用线性优化求解器或应用专用算法至关重要。

Method: 框架为每个非线性模式分配一个重新表述代理，这些代理被明确指示为其非线性模式推导精确的线性重新表述，然后协调组装等效于原始问题的求解器就绪线性模型。

Result: 在从ComplexOR数据集衍生的20个真实世界非线性优化问题上评估该方法，结果表明专门的LLM代理可以自动化线性化任务。

Conclusion: 该方法为非线性优化的完全对话式建模管道开辟了道路。

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [68] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 该论文将泛化保证扩展到非紧对称性和非不变数据分布，通过PAC-Bayes框架改进现有边界，并在旋转MNIST数据集上验证理论。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要关注紧群对称性且假设数据分布不变，这在现实应用中很少满足，需要扩展到更一般的对称性设置。

Method: 基于PAC-Bayes框架，改进并收紧现有边界，特别是McAllester的PAC-Bayes边界，适用于广泛的PAC-Bayes边界。

Result: 在非均匀旋转群的旋转MNIST数据集上验证，推导的保证不仅成立，而且优于先前结果。

Conclusion: 对于对称数据，对称模型在超越紧群和不变分布限制的更一般设置中更优，为理解机器学习中对称性提供了更通用的理论基础。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [69] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 本文探讨了训练数据的几何结构对机器学习模型性能的影响，提出使用持久同调分析数据拓扑特征来量化多样性，超越传统熵度量方法。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据是机器学习和人工智能的基础，但数据的几何结构对模型性能的影响尚未被充分探索。作者认为数据的表示丰富性和冗余消除对学习结果有重要影响。

Method: 使用持久同调方法从度量空间中的数据提取拓扑特征，为量化数据多样性提供原则性方法。

Result: 研究发现持久同调是分析和增强AI系统训练数据的强大工具。

Conclusion: 持久同调为超越传统熵度量的数据多样性量化提供了有效方法，对提升训练数据质量具有重要意义。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [70] [Exploration via Feature Perturbation in Contextual Bandits](https://arxiv.org/abs/2510.17390)
*Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出特征扰动技术，通过向特征输入注入随机性而非参数采样，实现了更优的理论遗憾界和计算效率，并能扩展到非参数和神经网络模型。


<details>
  <summary>Details</summary>
Motivation: 现有随机化bandit算法通常存在O(d^{3/2}√T)的遗憾界，且参数采样方法计算成本高，难以扩展到复杂模型。

Method: 特征扰动技术：直接在特征输入中注入随机性，避免参数采样，仅需扰动特征向量。

Result: 获得了O(d√T)的最坏情况遗憾界，优于现有方法的O(d^{3/2}√T)，且计算效率更高，能扩展到神经网络模型。

Conclusion: 特征扰动技术统一了强大实践性能和最佳理论保证，超越了现有方法，为广义线性bandit提供了高效解决方案。

Abstract: We propose feature perturbation, a simple yet powerful technique that injects
randomness directly into feature inputs, instead of randomizing unknown
parameters or adding noise to rewards. Remarkably, this algorithm achieves
$\tilde{\mathcal{O}}(d\sqrt{T})$ worst-case regret bound for generalized linear
bandits, while avoiding the $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ regret
typical of existing randomized bandit algorithms. Because our algorithm eschews
parameter sampling, it is both computationally efficient and naturally extends
to non-parametric or neural network models. We verify these advantages through
empirical evaluations, demonstrating that feature perturbation not only
surpasses existing methods but also unifies strong practical performance with
best-known theoretical guarantees.

</details>


### [71] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: 提出PALE框架，通过提示引导的LLM响应作为数据增强来进行幻觉检测，引入对比马氏距离评分来评估中间嵌入的真实性，无需人工标注即可实现优越的检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM幻觉检测中标注数据稀缺的问题，通过低成本生成真实和幻觉数据来增强检测能力。

Method: 使用提示引导的LLM响应作为数据增强，引入对比马氏距离评分来建模真实和幻觉数据在激活空间的分布差异。

Result: PALE在幻觉检测任务上显著优于基线方法6.55%，展示了优越的性能。

Conclusion: PALE框架通过数据增强和对比马氏距离评分有效解决了LLM幻觉检测问题，具有强泛化性和实用性。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [72] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: DAWP是一个创新的天气预测框架，通过人工智能数据同化模块在观测空间中运行，摆脱了对再分析数据的依赖，实现了基于子图像的全球观测预测。


<details>
  <summary>Details</summary>
Motivation: 传统AI天气预测方法依赖再分析数据，存在数据同化偏差和时间不一致性问题。观测预测作为变革性范式，旨在解放AIWP对再分析数据的依赖。

Method: 提出DAWP框架，包含AIDA模块（使用掩码多模态自编码器和掩码ViT-VAE处理不规则卫星观测数据）和时空解耦变换器（采用跨区域边界条件学习观测空间动态）。

Result: 实验表明AIDA初始化显著提高了AIWP的推出效率和性能，DAWP在全局降水预测中展现出应用潜力。

Conclusion: DAWP框架成功实现了在观测空间中的天气预测，解决了不规则高分辨率观测数据处理的挑战，为AI天气预测提供了新范式。

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [73] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 动量方法使随机DC优化在小批量下收敛，无需大批次或强噪声假设


<details>
  <summary>Details</summary>
Motivation: 现有随机DC优化方法需要大批次或强噪声假设，限制了实际应用，而小批量下的收敛性研究不足

Method: 提出基于动量的算法，在标准光滑性和有界方差假设下实现收敛

Result: 证明无动量时无论步长如何都可能不收敛，动量方法在理论和实验中都表现良好

Conclusion: 动量是随机DC优化在小批量设置下收敛的关键因素

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [74] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: 提出了Cog-Rethinker，一种分层元认知强化学习框架，通过分解问题和细化答案来提高大语言模型推理任务的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定提示模板激活LLM的推理能力，但这对弱LLM造成样本效率低下，因为大多数问题在推理任务中会产生无效输出，导致样本浪费。

Method: 采用分层元认知两阶段框架：1) 将零准确率问题分解为子问题生成最终推理结果；2) 参考先前错误解决方案细化答案。同时应用监督微调确保训练测试一致性。

Result: 在多个数学推理基准测试中表现出优越性能，相比基线方法提高了样本效率并加速了收敛。

Conclusion: Cog-Rethinker通过改进的rollout过程有效提高了LLM推理任务的样本利用率，为弱LLM的推理能力开发提供了有效解决方案。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [75] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出AMiD框架，使用α混合辅助分布进行知识蒸馏，解决LLM蒸馏中的容量差距和训练不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 解决自回归大语言模型的高计算成本问题，以及传统知识蒸馏方法中由于高维输出导致的容量差距和训练不稳定性。

Method: 提出α混合辅助分布，引入分布设计变量α，并基于最优性理论扩展了与辅助分布一起使用的散度族。

Result: 通过广泛实验证明AMiD在性能和训练稳定性方面优于现有方法。

Conclusion: AMiD通过利用更广泛且理论基础的辅助分布空间，提供了优越的性能和训练稳定性。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [76] [Functional Distribution Networks (FDN)](https://arxiv.org/abs/2510.17794)
*Omer Haq*

Main category: cs.LG

TL;DR: 提出了Functional Distribution Networks (FDN)，一种输入条件化的网络权重分布方法，通过beta-ELBO和蒙特卡洛采样训练，能够在分布偏移下产生自适应分散的预测混合分布。


<details>
  <summary>Details</summary>
Motivation: 现代概率回归器在分布偏移下往往保持过度自信，需要开发能够适应输入变化的预测不确定性方法。

Method: 使用输入条件化的网络权重分布，通过beta-ELBO损失函数和蒙特卡洛采样进行训练，诱导产生自适应分散的预测混合分布。

Result: 在标准回归任务中，与贝叶斯、集成、dropout和超网络基线相比，在匹配参数和更新预算下评估了准确性、校准性和偏移感知能力。

Conclusion: 该框架和评估协议旨在使具有OOD感知和良好校准的神经回归变得实用和模块化。

Abstract: Modern probabilistic regressors often remain overconfident under distribution
shift. We present Functional Distribution Networks (FDN), an input-conditioned
distribution over network weights that induces predictive mixtures whose
dispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo
sampling. We further propose an evaluation protocol that cleanly separates
interpolation from extrapolation and stresses OOD sanity checks (e.g., that
predictive likelihood degrades under shift while in-distribution accuracy and
calibration are maintained). On standard regression tasks, we benchmark against
strong Bayesian, ensemble, dropout, and hypernetwork baselines under matched
parameter and update budgets, and assess accuracy, calibration, and
shift-awareness with standard diagnostics. Together, the framework and protocol
aim to make OOD-aware, well-calibrated neural regression practical and modular.

</details>


### [77] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 提出MEET-Sepsis框架，通过多内生视图表示增强机制和级联双卷积时间注意力模块，仅需20%ICU监测时间即可实现竞争性的脓毒症预测精度。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是ICU中死亡率高的感染综合征，现有AI方法难以捕捉早期微弱时间信号，需要更早准确的预测方法。

Method: 使用多内生视图表示增强(MERE)机制构建丰富特征视图，结合级联双卷积时间注意力(CDTA)模块进行多尺度时间表示学习。

Result: 仅需SOTA方法20%的ICU监测时间即可达到竞争性的预测准确性，显著推进了早期脓毒症预测。

Conclusion: MEET-Sepsis框架通过有效捕捉早期时间信号，显著提升了脓毒症的早期预测能力，经广泛验证证实其有效性。

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [78] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 提出一种基于聚类的可解释AI方法，用于根据睡眠障碍特征对患者进行分组，并识别影响这些病理的关键因素


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍对患者健康和生活质量有重大影响，但由于症状多样性，诊断仍然复杂。技术进步和医学数据分析为更好理解这些障碍提供了新视角

Method: 采用基于聚类的可解释人工智能方法，整合可解释性方法识别关键影响因素

Result: 在匿名真实数据上的实验证明了该方法的有效性和相关性

Conclusion: 该方法能够有效识别睡眠障碍患者的不同特征群组，并为理解睡眠障碍提供了可解释的AI解决方案

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [79] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 该论文提出了一个框架来追踪和引导大语言模型中的算法原语，通过将推理轨迹与内部激活模式关联，并评估算法原语对推理步骤和任务性能的影响。研究发现LLM的推理可能由算法原语的组合几何支持，这些原语可以跨任务和跨模型迁移，推理微调能增强跨领域的算法泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何通过潜在计算和推理时间计算来解决多步推理问题，探索模型内部的算法原语如何支持复杂推理过程。

Method: 通过聚类神经激活并标记匹配的推理轨迹来操作化原语，应用函数向量方法推导可重用的推理构建块原语向量，在残差流中注入原语并测量其对推理步骤和任务性能的影响。

Result: 原语向量可以通过加法、减法、标量运算进行组合，在激活空间中展现出几何逻辑。跨任务和跨模型评估显示存在共享和任务特定的原语。推理微调后的模型表现出更系统的验证和路径生成原语使用。

Conclusion: LLM的推理可能由算法原语的组合几何支持，这些原语可以跨任务和跨模型迁移，推理微调能增强跨领域的算法泛化能力。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [80] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: GRPO算法驱动的RLVR方法虽然能提升LLM的推理能力，但效果不一致。研究发现GRPO本质上是保守的重新加权方案，受限于基础模型分布，无法发现全新解决方案。OOD改进仅在目标任务与模型预训练偏好一致时出现，而ID任务收益随性能饱和而减少。


<details>
  <summary>Details</summary>
Motivation: GRPO在不同推理领域的效果不一致，例如数学领域有显著提升但医学领域停滞不前。这引发了对GRPO何时能改进推理和泛化到分布外条件的根本质疑。

Method: 从数据分布角度进行理论分析，证明GRPO是保守的重新加权方案。通过从零开始训练transformer进行受控研究，评估在推理深度、输入长度、token表示和组合性等方面的泛化能力。

Result: GRPO的OOD改进仅在目标任务与模型预训练偏好一致时出现，而ID任务的收益随着性能饱和而减少。GRPO无法发现完全新颖的解决方案，受限于基础模型分布。

Conclusion: GRPO不是通用的推理增强器，而是强化预训练偏好的工具。这激励未来开发能够扩展模型能力超越预训练起源的算法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [81] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: Stratos是一个端到端的LLM蒸馏管道，能自动选择服务器和模型、执行知识蒸馏，并在分布式云环境中部署，满足用户定义的性能和预算约束。


<details>
  <summary>Details</summary>
Motivation: 工业对定制化和成本效益高的大型语言模型需求增长，现有蒸馏框架需要人工干预且难以满足复杂用户需求。

Method: Stratos自动选择帕累托最优服务器，动态匹配师生模型对，并根据任务复杂度调整蒸馏策略以优化云托管。

Result: 在麻将推理任务上，学生模型达到GPT-4o教师基准四倍准确率，同时降低延迟和成本而不牺牲准确性。

Conclusion: Stratos展示了在垂直领域LLM部署中的潜力，能自动满足复杂蒸馏需求。

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [82] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 该论文提出使用Kolmogorov-Smirnov检验来监测和量化机器学习系统中的分布偏移问题，特别是在智能交通领域的应用。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统中训练数据与测试数据概率分布不一致的问题，这种分布偏移会导致预测误差，在安全关键应用中尤为严重。

Method: 采用Kolmogorov-Smirnov检验来测量分布偏移，使用KS距离来量化分布偏移的程度及其对AI智能体性能的影响。

Result: 研究发现即使KS距离仅为0.02，也会导致强化学习智能体在单个交叉路口的通行时间增加约50%，影响显著。

Conclusion: KS检验和KS距离可作为实时监测AI智能体性能下降的重要统计工具，帮助AI系统更好地应对分布偏移问题。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [83] [AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM](https://arxiv.org/abs/2510.15998)
*Nilo Schwencke,Cyriaque Rousselot,Alena Shilova,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 本文分析了使用ANaGRAM自然梯度方法训练PINNs的动态过程，提出多截止适应策略提升性能，并在基准PDE上验证了该方法的有效性，能够达到机器精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明自然梯度方法在训练物理信息神经网络(PINNs)时显著优于标准优化器，但需要深入分析其训练动态并进一步提升性能。

Method: 基于ANaGRAM自然梯度方法，采用奇异值分解和截止正则化，提出了多截止适应策略来增强性能。

Result: 在基准PDE上的实验验证了方法的有效性，在某些实验中能够达到机器精度。

Conclusion: 开发了基于谱理论的理论框架，解释了正则化的必要性，并扩展了与格林函数理论的联系。

Abstract: Recent works have shown that natural gradient methods can significantly
outperform standard optimizers when training physics-informed neural networks
(PINNs). In this paper, we analyze the training dynamics of PINNs optimized
with ANaGRAM, a natural-gradient-inspired approach employing singular value
decomposition with cutoff regularization. Building on this analysis, we propose
a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.
Experiments on benchmark PDEs validate the effectiveness of our method, which
allows to reach machine precision on some experiments. To provide theoretical
grounding, we develop a framework based on spectral theory that explains the
necessity of regularization and extend previous shown connections with Green's
functions theory.

</details>


### [84] [Layer-Aware Influence for Online Data Valuation Estimation](https://arxiv.org/abs/2510.16007)
*Ziao Yang,Longbo Huang,Hongfu Liu*

Main category: cs.LG

TL;DR: 提出了一种层感知在线估计器，用于动态评估训练样本在优化过程中的影响力，避免了传统静态影响力评估的局限性，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统的数据影响力评估方法主要关注模型收敛后的静态影响力，忽视了优化过程中样本影响力的动态变化特性，特别是在深度模型中。现有方法计算成本高，难以在实际中频繁使用。

Method: 开发了层感知在线估计器，仅需要损失对输出的梯度，避免了参数级和全网络梯度的计算，同时保持了影响力排名的准确性。

Result: 在LLM预训练、微调和图像分类等任务上的广泛实验表明，该方法在显著降低时间和内存成本的同时提高了准确性，使动态数据筛选在实践中变得高效和可扩展。

Conclusion: 该方法成功解决了动态数据影响力评估的计算效率问题，为数据中心的深度学习提供了实用的动态数据筛选工具。

Abstract: Data-centric learning emphasizes curating high-quality training samples to
boost performance rather than designing new architectures. A central problem is
to estimate the influence of training sample efficiently. Prior studies largely
focus on static influence measured on a converged model, overlooking how data
valuation dynamically changes during optimization. This omission neglects the
dynamic nature of sample influence during optimization, especially in deep
models. To address the computational burden of frequent influence estimation,
we develop a layer-aware online estimator that requires only loss-to-output
gradients. This design avoids parameter-level and full-network gradients while
preserving ranking fidelity. Extensive experiments across LLM pretraining,
fine-tuning, and image classification show our method improves accuracy with
substantially lower time and memory cost, making dynamic data curation
efficient and scalable in practice.

</details>


### [85] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 提出了STAR模块，用于增强时间序列基础模型对状态变量的建模能力，通过状态感知的适配器提升多变量时间序列异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型在处理包含离散状态变量（如阀门开关状态、星期几等）的工业时间序列时，往往忽视状态变量的分类特性及其作为条件的关键作用，导致检测性能下降。

Method: STAR包含三个核心组件：身份引导的状态编码器（使用可学习状态记忆捕获状态变量语义）、条件瓶颈适配器（基于状态动态生成低秩适配参数）、数值-状态匹配模块（检测状态变量本身的异常）。

Result: 在真实世界数据集上的广泛实验表明，STAR能够显著提升现有时间序列基础模型在多变量时间序列异常检测任务上的性能。

Conclusion: STAR作为一个即插即用模块，有效解决了现有时间序列基础模型在建模状态变量方面的局限性，为工业场景中的多变量时间序列异常检测提供了更优的解决方案。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [86] [Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach](https://arxiv.org/abs/2510.16015)
*Qian Sun,Graham Hults,Susu Xu*

Main category: cs.LG

TL;DR: 提出了一种决策导向的洪水管理框架，通过端到端优化传感器部署和洪水预测模型来最小化下游应急决策的遗憾值。


<details>
  <summary>Details</summary>
Motivation: 传统洪水管理系统采用固定的、与决策任务无关的策略来部署传感器和训练预测模型，忽视了相同的感知增益和平均预测误差可能导致不同的决策结果。

Method: 端到端框架包含四个组件：上下文评分网络、硬预算约束下的可微分传感器选择模块、时空洪水重建与预测模型、以及针对特定任务目标的可微分决策层。核心创新是使用隐式最大似然估计实现离散传感器配置的梯度学习，以及概率决策头实现各种约束灾害响应任务的可微分近似。

Result: 该方法能够战略性地选择传感器部署位置并优化洪水预测模型，从而优化下游洪水响应决策的遗憾值。

Conclusion: 决策导向框架能够更有效地支持洪水应急响应，通过端到端优化传感器部署和预测模型来提升决策质量。

Abstract: Timely and reliable decision-making is vital for flood emergency response,
yet it remains severely hindered by limited and imprecise situational awareness
due to various budget and data accessibility constraints. Traditional flood
management systems often rely on in-situ sensors to calibrate remote
sensing-based large-scale flood depth forecasting models, and further take
flood depth estimates to optimize flood response decisions. However, these
approaches often take fixed, decision task-agnostic strategies to decide where
to put in-situ sensors (e.g., maximize overall information gain) and train
flood forecasting models (e.g., minimize average forecasting errors), but
overlook that systems with the same sensing gain and average forecasting errors
may lead to distinct decisions. To address this, we introduce a novel
decision-focused framework that strategically selects locations for in-situ
sensor placement and optimize spatio-temporal flood forecasting models to
optimize downstream flood response decision regrets. Our end-to-end pipeline
integrates four components: a contextual scoring network, a differentiable
sensor selection module under hard budget constraints, a spatio-temporal flood
reconstruction and forecasting model, and a differentiable decision layer
tailored to task-specific objectives. Central to our approach is the
incorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable
gradient-based learning over discrete sensor configurations, and probabilistic
decision heads to enable differentiable approximation to various constrained
disaster response tasks.

</details>


### [87] [Transfer learning strategies for accelerating reinforcement-learning-based flow control](https://arxiv.org/abs/2510.16016)
*Saeed Salehi*

Main category: cs.LG

TL;DR: 该研究探索了使用渐进神经网络和微调策略来加速深度强化学习在混沌流体多保真度控制中的迁移学习，发现渐进神经网络在知识保留和性能稳定性方面优于传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决深度强化学习在流体控制中训练耗时的问题，通过迁移学习将低保真环境中学到的控制策略知识有效转移到高保真环境，加速训练过程。

Method: 采用渐进神经网络架构，并与传统微调策略进行对比评估。使用Kuramoto-Sivashinsky系统作为基准，分析从低保真到高保真环境的策略迁移效果。

Result: 结果显示微调虽然能加速收敛，但对预训练时长敏感且容易发生灾难性遗忘；而渐进神经网络能稳定高效地迁移知识，保持性能增益，对预训练过拟合具有鲁棒性。

Conclusion: 渐进神经网络为稳健、可扩展和计算高效的流体控制提供了有前景的迁移学习框架，特别在源环境和目标环境差异较大时表现优异。

Abstract: This work investigates transfer learning strategies to accelerate deep
reinforcement learning (DRL) for multifidelity control of chaotic fluid flows.
Progressive neural networks (PNNs), a modular architecture designed to preserve
and reuse knowledge across tasks, are employed for the first time in the
context of DRL-based flow control. In addition, a comprehensive benchmarking of
conventional fine-tuning strategies is conducted, evaluating their performance,
convergence behavior, and ability to retain transferred knowledge. The
Kuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how
knowledge encoded in control policies, trained in low-fidelity environments,
can be effectively transferred to high-fidelity settings. Systematic
evaluations show that while fine-tuning can accelerate convergence, it is
highly sensitive to pretraining duration and prone to catastrophic forgetting.
In contrast, PNNs enable stable and efficient transfer by preserving prior
knowledge and providing consistent performance gains, and are notably robust to
overfitting during the pretraining phase. Layer-wise sensitivity analysis
further reveals how PNNs dynamically reuse intermediate representations from
the source policy while progressively adapting deeper layers to the target
task. Moreover, PNNs remain effective even when the source and target
environments differ substantially, such as in cases with mismatched physical
regimes or control objectives, where fine-tuning strategies often result in
suboptimal adaptation or complete failure of knowledge transfer. The results
highlight the potential of novel transfer learning frameworks for robust,
scalable, and computationally efficient flow control that can potentially be
applied to more complex flow configurations.

</details>


### [88] [Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality](https://arxiv.org/abs/2510.16020)
*Sangjoon Lee,Haris Moazam Sheikh*

Main category: cs.LG

TL;DR: AirDbM是一种专门用于翼型优化的设计变形方法，通过从1600多个翼型数据库中精选12个基准翼型，显著降低设计空间维度，在保持高重建精度的同时实现更高效的多目标优化和强化学习应用。


<details>
  <summary>Details</summary>
Motivation: 翼型几何优化需要探索多样化的设计，同时尽可能减少设计变量数量。现有方法存在设计空间维度高、优化效率低的问题。

Method: 从UIUC翼型数据库中系统选择12个最优基准翼型，通过顺序添加最能增加设计容量的基准翼型，实现设计空间降维。

Result: 用12个基准翼型重建了99%的数据库，平均绝对误差低于0.005；在多目标气动优化中实现了更快的收敛速度和更大的超体积，发现了具有改进升阻比的新帕累托最优解。

Conclusion: AirDbM在翼型优化中表现出色，特别是在强化学习应用中优于传统参数化方法，展示了设计变形方法在机器学习驱动设计中的广阔潜力。

Abstract: Effective airfoil geometry optimization requires exploring a diverse range of
designs using as few design variables as possible. This study introduces
AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil
optimization that systematically reduces design-space dimensionality. AirDbM
selects an optimal set of 12 baseline airfoils from the UIUC airfoil database,
which contains over 1,600 shapes, by sequentially adding the baseline that most
increases the design capacity. With these baselines, AirDbM reconstructs 99 \%
of the database with a mean absolute error below 0.005, which matches the
performance of a previous DbM approach that used more baselines. In
multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence
and achieves a Pareto front with a greater hypervolume than that of the
previous larger-baseline study, where new Pareto-optimal solutions are
discovered with enhanced lift-to-drag ratios at moderate stall tolerances.
Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement
learning (RL) agents in generating airfoil geometry when compared to
conventional airfoil parameterization methods, implying the broader potential
of DbM in machine learning-driven design.

</details>


### [89] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出基于特征驱动强化学习的PV日内交易策略，通过整合市场微观结构和历史特征，在连续日内市场中实时调整仓位，平衡交易利润和失衡惩罚，显著优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 光伏运营商面临发电和短期电价的高度不确定性，连续日内市场允许实时调整仓位以改善收益和减少失衡成本，但需要有效的交易策略。

Method: 将问题建模为马尔可夫决策过程，使用近端策略优化(PPO)算法，整合数据驱动特征到状态空间，学习以线性可解释策略为主的投标策略。

Result: 在历史市场数据上训练并在样本外评估，该策略在不同场景下持续优于基准方法，表现出快速收敛、实时推理和透明决策规则。

Conclusion: 特征驱动强化学习为光伏生产商的主动日内参与提供了实用、数据高效且可操作部署的路径。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [90] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 论文提出IB-FT方法，通过信息瓶颈指导微调，解决大语言模型在代码生成任务中的记忆障碍问题，相比传统微调方法获得更稳定和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 发现预训练大语言模型在代码领域的监督微调中存在记忆障碍问题，即模型对下游代码数据的强记忆会阻碍优化，无法有效获取新的可泛化代码知识。

Method: 提出信息瓶颈指导的微调方法(IB-FT)，对代码数据的隐藏表示施加IB惩罚，压缩虚假的记忆特征，同时保留任务相关信息。

Result: 在两个代码基准测试(OriGen和Evol-CodeAlpaca-V1)上的实验表明，IB-FT显著缓解了记忆障碍，提高了top-1性能(Pass@1)，在更严格的多样本度量Pass@k(m)下获得了更稳定的增益。

Conclusion: IB-FT方法能有效克服记忆障碍，在代码生成任务中比传统微调方法表现更好且更稳定。

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [91] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: PolyConFM是首个聚合物基础模型，通过构象中心的生成预训练统一聚合物建模和设计，解决了现有方法忽略聚合物构象全局结构信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅使用单体级描述符表示整个聚合物，忽略了聚合物构象中的全局结构信息，限制了实际性能。该领域还缺乏能够有效支持多样化下游任务的通用基础模型。

Method: 将聚合物构象分解为局部构象序列，通过掩码自回归建模进行条件生成预训练，重建局部构象并生成其方向变换来恢复相应的聚合物构象。通过分子动力学模拟构建首个高质量聚合物构象数据集。

Result: 实验表明PolyConFM在多样化下游任务上持续优于代表性的任务特定方法。

Conclusion: PolyConFM为聚合物科学提供了一个通用且强大的工具。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [92] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: 提出一个可泛化的因果机器学习流程，用于从电子健康记录中发现潜在因果源并量化其对临床结果的影响。


<details>
  <summary>Details</summary>
Motivation: 处理不完善的多模态临床数据，发现其中的潜在因果因素，并量化这些因素对临床结果的影响，以支持大规模医学发现。

Method: 使用概率独立潜在源分解方法处理多模态临床数据，训练特定任务的因果模型来估计个体因果效应。

Result: 该方法已在两个真实世界应用中验证，展示了其在大规模医学发现中的多功能性和实用性。

Conclusion: 该因果机器学习流程能够有效处理不完善的临床数据，发现潜在因果源并量化其影响，为大规模医学研究提供了实用工具。

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [93] [RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction](https://arxiv.org/abs/2510.16035)
*Yingguang Yang,Xianghua Zeng,Qi Wu,Hao Peng,Yutong Xia,Hao Liu,Bin Chong,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了首个针对GNN社交机器人检测器的对抗性多智能体强化学习框架RoBCtrl，通过扩散模型生成高保真机器人账户，并使用MARL模拟对抗行为，有效降低检测器性能。


<details>
  <summary>Details</summary>
Motivation: 社交网络中机器人检测技术的漏洞和鲁棒性研究不足，现有GNN方法由于对社交代理控制有限、检测器黑盒特性及机器人异质性等问题无法直接应用。

Method: 使用扩散模型生成高保真机器人账户，通过多智能体强化学习模拟对抗行为，基于影响力和预算对账户分类，并设计基于结构熵的分层状态抽象加速学习。

Result: 在社交机器人检测数据集上的实验表明，该框架能有效削弱GNN检测器的性能。

Conclusion: RoBCtrl是首个针对GNN社交机器人检测器的对抗攻击框架，结合扩散模型和MARL，为检测系统的安全性评估提供了新方法。

Abstract: Social networks have become a crucial source of real-time information for
individuals. The influence of social bots within these platforms has garnered
considerable attention from researchers, leading to the development of numerous
detection technologies. However, the vulnerability and robustness of these
detection methods is still underexplored. Existing Graph Neural Network
(GNN)-based methods cannot be directly applied due to the issues of limited
control over social agents, the black-box nature of bot detectors, and the
heterogeneity of bots. To address these challenges, this paper proposes the
first adversarial multi-agent Reinforcement learning framework for social Bot
control attacks (RoBCtrl) targeting GNN-based social bot detectors.
Specifically, we use a diffusion model to generate high-fidelity bot accounts
by reconstructing existing account data with minor modifications, thereby
evading detection on social platforms. To the best of our knowledge, this is
the first application of diffusion models to mimic the behavior of evolving
social bots effectively. We then employ a Multi-Agent Reinforcement Learning
(MARL) method to simulate bots adversarial behavior. We categorize social
accounts based on their influence and budget. Different agents are then
employed to control bot accounts across various categories, optimizing the
attachment strategy through reinforcement learning. Additionally, a
hierarchical state abstraction based on structural entropy is designed to
accelerate the reinforcement learning. Extensive experiments on social bot
detection datasets demonstrate that our framework can effectively undermine the
performance of GNN-based detectors.

</details>


### [94] [Vector Quantization in the Brain: Grid-like Codes in World Models](https://arxiv.org/abs/2510.16039)
*Xiangyuan Peng,Xingsi Dong,Si Wu*

Main category: cs.LG

TL;DR: 提出Grid-like Code Quantization (GCQ)，一种受大脑启发的网格模式吸引子动力学方法，用于将观测-动作序列压缩为离散表示


<details>
  <summary>Details</summary>
Motivation: 传统向量量化方法处理静态输入，需要一种能够联合压缩空间和时间的时空压缩方法，作为统一的世界模型

Method: 使用动作条件化码本，码字来自连续吸引子神经网络，基于动作动态选择，实现时空压缩

Result: 在多样化任务中验证了GCQ在紧凑编码和下游性能方面的有效性

Conclusion: GCQ既为高效序列建模提供了计算工具，也为神经系统中网格状代码形成提供了理论视角

Abstract: We propose Grid-like Code Quantization (GCQ), a brain-inspired method for
compressing observation-action sequences into discrete representations using
grid-like patterns in attractor dynamics. Unlike conventional vector
quantization approaches that operate on static inputs, GCQ performs
spatiotemporal compression through an action-conditioned codebook, where
codewords are derived from continuous attractor neural networks and dynamically
selected based on actions. This enables GCQ to jointly compress space and time,
serving as a unified world model. The resulting representation supports
long-horizon prediction, goal-directed planning, and inverse modeling.
Experiments across diverse tasks demonstrate GCQ's effectiveness in compact
encoding and downstream performance. Our work offers both a computational tool
for efficient sequence modeling and a theoretical perspective on the formation
of grid-like codes in neural systems.

</details>


### [95] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: AMS-Quant是一种创新的浮点量化方法，通过引入非整数位宽和两种新技术：尾数位共享和自适应搜索，显著加速大语言模型推理，在FP5.33和FP4.25位宽下分别实现2.8倍和3.2倍加速，且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数量巨大带来存储和推理效率瓶颈，浮点量化虽能加速推理但传统方法仅限于整数位宽，需要探索更精细的非整数位宽量化来逼近量化最佳点。

Method: 提出AMS-Quant方法：1) 尾数位共享：将k个量化权重分组共享最低有效尾数位，实现非整数位宽量化；2) 自适应搜索：采用离线优化策略最小化共享带来的精度损失；3) 实现高效CUDA线性核，将内存节省转化为实际延迟降低。

Result: 在大规模数据集和模型上的实验表明，AMS-Quant能将模型量化为FP5.33-e2m3和FP4.25-e2m2，相比FP16推理分别实现2.8倍和3.2倍的解码加速，且精度损失可忽略不计。

Conclusion: AMS-Quant首次将浮点量化从整数位宽扩展到非整数位宽，通过创新的尾数位共享和自适应搜索技术，在保持精度的同时显著加速LLM推理，为高效模型部署提供了新思路。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [96] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: GUIrilla是一个自动化可扩展框架，通过原生可访问性API系统探索应用程序，解决GUI自动化的数据收集挑战，并构建了包含27,171个任务的GUIrilla-Task数据集。


<details>
  <summary>Details</summary>
Motivation: 当前自主代理在复杂图形用户界面操作方面面临数据可用性限制，包括昂贵的手动标注、闭源数据集和表面级合成流水线，特别是在macOS生态系统中代表性有限。

Method: 使用原生可访问性API系统探索应用程序，将发现的界面元素和爬虫动作组织成层次化GUI图，采用专门的交互处理器实现全面应用覆盖。

Result: 构建了GUIrilla-Task数据集，包含27,171个功能基础任务，涵盖1,108个macOS应用。在ScreenSpot Pro基准测试中，基于GUIrilla-Task调优的LLM代理性能显著提升，使用97%更少数据仍优于合成基线。

Conclusion: GUIrilla框架有效解决了桌面GUI自动化的数据收集挑战，发布的macapptree库、GUIrilla-Task数据集和GUIrilla-Gold基准将支持桌面自主性的开放研究。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [97] [FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting](https://arxiv.org/abs/2510.16053)
*Chenyang Yu,Xinpeng Xie,Yan Huang,Chenxi Qiu*

Main category: cs.LG

TL;DR: 该论文探讨了智能交通系统中的交通预测技术，重点分析了图神经网络在捕捉交通流时空依赖性方面的优势，并指出了现有方法在处理复杂未知事件时的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，交通拥堵问题日益严重，需要可靠且响应迅速的交通预测模型来支持智能交通系统建设，改善城市资源分配和出行体验。

Method: 主要采用图神经网络（GNNs）作为主流方法，包括STGCN、GraphWaveNet、STWave和D2STGNN等模型，这些方法结合了复杂的图卷积结构和时序建模机制，能够有效捕捉道路网络拓扑中的空间依赖性和交通流数据的动态时序演化模式。

Result: 现有方法在标准交通数据集上表现出色，特别擅长捕捉和预测具有周期性规律的交通模式，但在处理复杂未知事件时存在局限性，主要依赖人工设计的特征，难以泛化到多样化场景。

Conclusion: 虽然基于图神经网络的交通预测方法在周期性模式预测方面效果显著，但需要开发更智能的事件信息整合方法，减少对专家先验知识的依赖，以更好地应对复杂多变的交通事件场景。

Abstract: Accurate traffic forecasting is a core technology for building Intelligent
Transportation Systems (ITS), enabling better urban resource allocation and
improved travel experiences. With growing urbanization, traffic congestion has
intensified, highlighting the need for reliable and responsive forecasting
models. In recent years, deep learning, particularly Graph Neural Networks
(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can
effectively capture complex spatial dependencies in road network topology and
dynamic temporal evolution patterns in traffic flow data. Foundational models
such as STGCN and GraphWaveNet, along with more recent developments including
STWave and D2STGNN, have achieved impressive performance on standard traffic
datasets. These approaches incorporate sophisticated graph convolutional
structures and temporal modeling mechanisms, demonstrating particular
effectiveness in capturing and forecasting traffic patterns characterized by
periodic regularities. To address this challenge, researchers have explored
various ways to incorporate event information. Early attempts primarily relied
on manually engineered event features. For instance, some approaches introduced
manually defined incident effect scores or constructed specific subgraphs for
different event-induced traffic conditions. While these methods somewhat
enhance responsiveness to specific events, their core drawback lies in a heavy
reliance on domain experts' prior knowledge, making generalization to diverse
and complex unknown events difficult, and low-dimensional manual features often
lead to the loss of rich semantic details.

</details>


### [98] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: FedPURIN是一个通信高效的个性化联邦学习框架，通过整数编程识别关键参数进行传输，结合稀疏聚合显著减少通信负担，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习方法在通信效率方面表现不佳，存在较大的通信负担，阻碍了实际部署。需要解决数据异构性问题的同时提升通信效率。

Method: 提出FedPURIN框架，使用整数编程策略性地识别关键参数进行传输，并将其集成到稀疏聚合方案中。

Result: 在标准图像分类基准测试中，在不同非IID条件下实现了与最先进方法相竞争的性能，同时通过稀疏聚合实现了可量化的通信减少。

Conclusion: 该框架为通信高效的个性化联邦学习建立了新范式，特别适用于具有异构数据源的边缘智能系统。

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [99] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 提出了多尺度神经算子(MNO)，一种用于三维非结构化点云上计算流体动力学的新型架构，通过显式三尺度分解显著提升了神经算子在复杂流体问题上的精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在求解偏微分方程时存在精度和可扩展性限制，特别是在不规则域上处理多尺度流体结构时表现不佳，需要更有效的多尺度建模方法。

Method: MNO架构包含三个尺度模块：全局维度收缩注意力模块处理长程依赖，局部图注意力模块处理邻域交互，微观点级注意力模块处理细粒度细节，在保持多尺度归纳偏置的同时保持计算效率。

Result: 在四个涵盖稳态和非稳态流动场景的基准测试中，MNO始终优于最先进基线，预测误差降低5%到40%，在具有30万点的挑战性3D CFD问题上表现出更好的鲁棒性。

Conclusion: 显式多尺度设计对神经算子至关重要，MNO为在不规则域上学习复杂流体动力学建立了一个可扩展框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [100] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 基于随机矩阵理论提出Transformer训练动态分析框架，通过自注意力矩阵的谱密度演化识别训练三阶段，并开发无需验证集的早停准则。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer训练动态的底层机制，为性能改进提供理论依据，并建立基于原理的早停标准。

Method: 利用随机矩阵理论分析Transformer训练动态，通过自注意力矩阵V的谱密度演化（呈现重尾分布）和PL拟合来划分训练阶段。

Result: 发现浅层自注意力矩阵谱密度一致演化为重尾分布，据此划分训练为结构探索、重尾结构稳定和收敛饱和三阶段，并提出两个一致的无需验证的早停准则。

Conclusion: 随机矩阵理论在监控和诊断Transformer模型训练进展方面具有实用价值，提出的准则与训练动态强相关。

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [101] [Optimization of the quantization of dense neural networks from an exact QUBO formulation](https://arxiv.org/abs/2510.16075)
*Sergio Muñiz Subiñas,Manuel L. González,Jorge Ruiz Gómez,Alejandro Mata Ali,Jorge Martínez Martín,Miguel Franco Hernando,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 提出了一种基于ADAROUND的QUBO公式的神经网络后训练量化方法，通过Frobenius距离作为目标函数，将量化问题转化为可分解的QUBO问题，并使用模拟退火等启发式算法高效求解。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法如四舍五入可能无法保持模型性能，需要更精确的量化方法来最小化精度损失。

Method: 使用Frobenius距离作为目标函数，构建显式QUBO问题，利用系数矩阵结构将全局问题分解为多个独立子问题，并通过模拟退火等启发式算法求解。

Result: 在MNIST、Fashion-MNIST、EMNIST和CIFAR-10数据集上评估，从int8到int1的整数精度范围内，相比传统四舍五入量化方法表现更好。

Conclusion: 该方法提供了一种有效的后训练量化解决方案，能够保持模型性能同时实现高效的量化部署。

Abstract: This work introduces a post-training quantization (PTQ) method for dense
neural networks via a novel ADAROUND-based QUBO formulation. Using the
Frobenius distance between the theoretical output and the dequantized output
(before the activation function) as the objective, an explicit QUBO whose
binary variables represent the rounding choice for each weight and bias is
obtained. Additionally, by exploiting the structure of the coefficient QUBO
matrix, the global problem can be exactly decomposed into $n$ independent
subproblems of size $f+1$, which can be efficiently solved using some
heuristics such as simulated annealing. The approach is evaluated on MNIST,
Fashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1
and compared with a round-to-nearest traditional quantization methodology.

</details>


### [102] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: BPL是一个新的推荐系统学习框架，通过双重蒸馏策略在事实和反事实测试环境中都实现高性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在偏差问题，现有方法主要关注反事实测试环境，但在基于实际用户-物品交互的事实测试环境中准确性显著下降。需要一种能在两种测试环境中都表现良好的模型。

Method: 采用偏差自适应偏好蒸馏学习框架，包含两种蒸馏策略：从有偏差模型进行师生蒸馏以保留准确偏好知识，以及通过可靠性过滤的自蒸馏来迭代精炼知识。

Result: 综合实验验证了BPL在事实和反事实测试中的有效性。

Conclusion: BPL框架能够逐步揭示用户偏好，在两种测试环境中都实现高性能，解决了推荐系统偏差问题。

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [103] [Continual Knowledge Consolidation LORA for Domain Incremental Learning](https://arxiv.org/abs/2510.16077)
*Naeem Paeedeh,Mahardhika Pratama,Weiping Ding,Jimmy Cao,Wolfgang Mayer,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 提出CONEC-LoRA方法解决领域增量学习中的灾难性遗忘问题，通过整合任务共享和任务特定的LoRA模块，结合随机分类器和辅助网络，在4个基准问题上取得超过5%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法创建任务特定的LoRA模块，但忽视了任务间的共享知识，且推理时任务特定LoRA选择不准确会导致精度显著下降，现有分类器泛化能力不足。

Method: 1. 整合任务共享LoRA和任务特定LoRA；2. 使用随机分类器参数从分布中采样；3. 部署辅助网络预测任务特定LoRA；4. 采用不同深度网络结构，每层连接局部分类器；5. 集成球生成器损失和变换模块解决合成样本偏差问题。

Result: 在4个流行基准问题上，CONEC-LoRA相比现有方法取得了超过5%的性能优势。

Conclusion: CONEC-LoRA通过知识整合和随机分类器设计，有效解决了领域增量学习中的关键问题，显著提升了性能。

Abstract: Domain Incremental Learning (DIL) is a continual learning sub-branch that
aims to address never-ending arrivals of new domains without catastrophic
forgetting problems. Despite the advent of parameter-efficient fine-tuning
(PEFT) approaches, existing works create task-specific LoRAs overlooking shared
knowledge across tasks. Inaccurate selection of task-specific LORAs during
inference results in significant drops in accuracy, while existing works rely
on linear or prototype-based classifiers, which have suboptimal generalization
powers. Our paper proposes continual knowledge consolidation low rank
adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed
from consolidations between task-shared LORA to extract common knowledge and
task-specific LORA to embrace domain-specific knowledge. Unlike existing
approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose
parameters are sampled from a distribution, thus enhancing the likelihood of
correct classifications. Last but not least, an auxiliary network is deployed
to optimally predict the task-specific LoRAs for inferences and implements the
concept of a different-depth network structure in which every layer is
connected with a local classifier to take advantage of intermediate
representations. This module integrates the ball-generator loss and
transformation module to address the synthetic sample bias problem. Our
rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in
4 popular benchmark problems with over 5% margins.

</details>


### [104] [PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites](https://arxiv.org/abs/2510.16083)
*Jaehan Kim,Minkyoo Song,Minjae Seo,Youngjin Jin,Seungwon Shin,Jinwoo Kim*

Main category: cs.LG

TL;DR: 提出PassREfinder-FL框架，使用图神经网络预测网站间的密码重用风险，通过联邦学习保护用户隐私，在真实数据集上达到0.9153的F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在检测密码重用攻击时存在的可用性问题和部署障碍，现有方法往往限制密码创建或网站访问，且依赖复杂的账户共享机制。

Method: 引入密码重用关系的概念，将其表示为网站图中的边，使用图神经网络进行链接预测，结合联邦学习保护用户隐私，无需跨管理员共享敏感信息。

Result: 在包含3.6亿个泄露账户、22,378个网站的真实数据集上，PassREfinder-FL在联邦学习设置下达到0.9153的F1分数，相比其他最先进的GNN模型性能提升4-11%。

Conclusion: PassREfinder-FL能够有效预测网站间的密码重用风险，生成可操作的风险评分，同时通过联邦学习保护用户隐私，具有实际部署的可行性。

Abstract: Credential stuffing attacks have caused significant harm to online users who
frequently reuse passwords across multiple websites. While prior research has
attempted to detect users with reused passwords or identify malicious login
attempts, existing methods often compromise usability by restricting password
creation or website access, and their reliance on complex account-sharing
mechanisms hinders real-world deployment. To address these limitations, we
propose PassREfinder-FL, a novel framework that predicts credential stuffing
risks across websites. We introduce the concept of password reuse relations --
defined as the likelihood of users reusing passwords between websites -- and
represent them as edges in a website graph. Using graph neural networks (GNNs),
we perform a link prediction task to assess credential reuse risk between
sites. Our approach scales to a large number of arbitrary websites by
incorporating public website information and linking newly observed websites as
nodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a
federated learning (FL) approach that eliminates the need to share user
sensitive information across administrators. Evaluation on a real-world dataset
of 360 million breached accounts from 22,378 websites shows that
PassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further
validate that our FL-based GNN achieves a 4-11% performance improvement over
other state-of-the-art GNN models through an ablation study. Finally, we
demonstrate that the predicted results can be used to quantify password reuse
likelihood as actionable risk scores.

</details>


### [105] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 将平衡传播学习算法扩展到离散和连续复值波系统，在弱耗散状态下有效，适用于各种物理环境，通过可训练的局部势能替代节点间连接，在激子极化凝聚体中测试成功。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法在物理神经网络中难以实现，平衡传播(EP)是替代方案，具有可比的效率和原位训练潜力。

Method: 扩展EP学习到离散和连续复值波系统，在弱耗散状态下有效，用可训练的局部势能替代节点间连接，在激子极化凝聚体中进行数值研究。

Result: 在标准基准测试中（包括逻辑任务和手写数字识别）表现出稳定收敛，验证了方法的有效性。

Conclusion: 为系统控制仅限于局部参数的物理系统中的原位学习建立了实用路径。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [106] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 提出FSRF框架解决多模态情感分析中的模态缺失问题，通过去冗余同质-异质分解和分布对齐自蒸馏模块恢复缺失语义


<details>
  <summary>Details</summary>
Motivation: 现实应用中由于遮挡、隐私约束和设备故障导致模态缺失，现有方法忽略此问题导致泛化性差

Method: 使用去冗余同质-异质分解模块将模态分解为同质、异质和噪声表示，并设计分布对齐自蒸馏模块进行双向知识转移

Result: 在两个数据集上的实验表明FSRF在不确定模态缺失情况下相比先前方法具有显著性能优势

Conclusion: FSRF框架能有效缓解多模态情感分析中的模态缺失问题，提高模型在现实场景中的泛化能力

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [107] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: STABLE是一个门控持续自编辑框架，通过LoRA参数高效微调来约束顺序更新中的遗忘问题，使用三种指标评估编辑稳定性，在Qwen-2.5-7B模型上有效缓解遗忘同时保持适应性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要持续适应机制，但顺序更新会导致灾难性遗忘，新编辑会降低先前获得的知识。

Method: 使用LoRA进行参数高效微调，通过三种指标（精确匹配下降、比特增加、KL散度）评估候选编辑的稳定性，超过阈值时通过裁剪过程重新缩放或拒绝LoRA更新。

Result: 在Qwen-2.5-7B模型上，门控有效缓解遗忘同时保持适应性，基于EM的门控在短持续学习序列中实现了最高累积性能。

Conclusion: 不同门控策略可以实现可比较的分布偏移，但产生不同的准确性结果，突显了门控设计在持续适应中的重要性，为持续模型编辑提供了原则性方法。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [108] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: 提出MemCom方法，通过分层压缩技术有效压缩多示例提示，在保持高性能的同时显著降低内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有提示压缩方法对多示例压缩效果不佳，而减少示例数量作为基线方法效果意外地好，需要开发更有效的压缩技术。

Method: 提出MemCom分层压缩方法，使用更强的压缩器模型，在transformer的每一层进行压缩，为每层提供独立的压缩表示。

Result: MemCom在所有压缩比下均优于强基线，在3x到8x压缩比下，基线性能下降20-30%，而MemCom仅下降不到10%。

Conclusion: 分层压缩和更强的压缩器模型是实现有效多示例提示压缩的关键，MemCom方法在保持高准确率的同时显著提升了效率。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [109] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 提出了一种基于相似性搜索和随机表示的无训练世界模型，与PlaNet基准模型相比，在潜在重建质量和长时程预测方面表现相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 世界模型在强化学习中广泛应用，但传统方法需要训练过程。本文旨在探索无需训练的世界模型替代方案。

Method: 利用相似性搜索和随机表示来近似世界模型，避免传统训练过程，与Dreamer家族的PlaNet模型进行比较。

Result: 搜索式世界模型在潜在重建质量和感知相似性方面与基于训练的模型相当，在长时程预测方面表现更优。

Conclusion: 基于搜索的世界模型是传统训练方法的可行替代方案，尤其在长时程预测任务中表现突出。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [110] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 该论文对三种代表性生成模型（AtomGPT、CDVAE、FlowMM）在材料数据集上的性能进行了系统基准测试，发现CDVAE表现最佳，其次是AtomGPT和FlowMM。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在材料发现中日益重要，但缺乏对其性能的严格比较评估。

Method: 使用三种代表性生成模型（AtomGPT、CDVAE、FlowMM）在两个公开超导数据集上进行训练和重建性能评估，采用KL散度和MAE作为评估指标。

Result: CDVAE在KL散度和MAE指标上表现最优，其次是AtomGPT，FlowMM表现最差。

Conclusion: CDVAE是三种模型中性能最佳的生成模型，为材料科学中的生成模型选择提供了重要参考。

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [111] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 本文通过层间因果修补分析语言模型对齐机制，发现人类偏好优化在Llama-3.2-1B模型中呈现空间局部化特征，仅中间层激活编码奖励一致行为，且仅少数层与奖励增益相关。


<details>
  <summary>Details</summary>
Motivation: 尽管基于人类反馈的强化学习（RLHF）已成为语言模型对齐的主流方法，但其内部工作机制仍不透明，需要系统分析偏好优化如何实现模型对齐。

Method: 在基模型和调优模型之间应用层间因果修补技术，分析人类偏好对中的激活差异，并使用LASSO回归识别与奖励增益相关的关键层。

Result: 对齐呈现空间局部化：中间层激活编码奖励一致行为子空间，早期和晚期层基本不受影响；仅少数层具有非零系数连接激活距离与奖励增益。

Conclusion: 基于人类偏好的语言模型对齐是一个方向性、低秩过程，而非扩散的参数化过程。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [112] [Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness](https://arxiv.org/abs/2510.16171)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh,Chaowei Zhang,Xiao Qin,Yang Zhou*

Main category: cs.LG

TL;DR: 本文研究了通过嵌入群等变卷积（旋转和尺度等变层）到标准CNN中来提升对抗鲁棒性的架构方法，提出了并行和级联两种对称感知架构，理论上证明了其能降低假设空间复杂度、正则化梯度，并在CLEVER框架下获得更紧的认证鲁棒性边界。


<details>
  <summary>Details</summary>
Motivation: 对抗样本揭示了深度神经网络的严重脆弱性，而对抗训练作为主要防御策略存在计算成本高且可能损害干净数据准确性的问题。本文旨在探索一种架构层面的对抗鲁棒性方法。

Method: 在标准卷积神经网络中嵌入群等变卷积层（旋转和尺度等变层），提出了并行设计（独立处理标准和等变特征后融合）和级联设计（顺序应用等变操作）两种对称感知架构。

Result: 在CIFAR-10、CIFAR-100和CIFAR-10C数据集上，模型在FGSM和PGD攻击下一致提升了对抗鲁棒性和泛化能力，且无需对抗训练。

Conclusion: 对称强制架构作为基于数据增强防御的高效且有原则的替代方案具有巨大潜力。

Abstract: Adversarial examples reveal critical vulnerabilities in deep neural networks
by exploiting their sensitivity to imperceptible input perturbations. While
adversarial training remains the predominant defense strategy, it often incurs
significant computational cost and may compromise clean-data accuracy. In this
work, we investigate an architectural approach to adversarial robustness by
embedding group-equivariant convolutions-specifically, rotation- and
scale-equivariant layers-into standard convolutional neural networks (CNNs).
These layers encode symmetry priors that align model behavior with structured
transformations in the input space, promoting smoother decision boundaries and
greater resilience to adversarial attacks. We propose and evaluate two
symmetry-aware architectures: a parallel design that processes standard and
equivariant features independently before fusion, and a cascaded design that
applies equivariant operations sequentially. Theoretically, we demonstrate that
such models reduce hypothesis space complexity, regularize gradients, and yield
tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme
Value for nEtwork Robustness) framework. Empirically, our models consistently
improve adversarial robustness and generalization across CIFAR-10, CIFAR-100,
and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial
training. These findings underscore the potential of symmetry-enforcing
architectures as efficient and principled alternatives to data
augmentation-based defenses.

</details>


### [113] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本文批评当前强化学习研究过度关注性能展示而忽视理解学习动态，主张应更重视科学理解和基准测试的数学形式化映射。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究过于强调展示智能体性能，导致对学习动态理解不足，基准测试可能过拟合学术基准，难以迁移到新问题。

Method: 以Arcade Learning Environment为例，说明即使被认为是"饱和"的基准仍可用于发展理解和促进RL技术在实际问题中的应用。

Result: 论证了RL研究应停止仅关注性能展示，而应更关注科学理解和基准测试的数学形式化。

Conclusion: 强化学习研究需要转向更注重科学理解而非单纯性能展示，并需要更精确地定义基准测试与数学形式化之间的映射关系。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [114] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出了一种结合关系强化学习与物体中心表示的新框架，能够处理结构化和非结构化数据，并通过主动查询人类专家来增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在图像和视频任务中很成功，但忽略了问题的内在结构。关系强化学习虽然能处理结构化问题，但对问题结构有强假设限制。

Method: 结合关系强化学习与物体中心表示，通过显式建模策略不确定性，允许系统主动向人类专家寻求指导。

Result: 实证评估表明所提方法具有有效性和高效性。

Conclusion: 该框架成功解决了传统强化学习和关系强化学习的局限性，在结构化和非结构化数据上都表现出色。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [115] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: 本研究应用机器学习方法分析欧洲绿色协议中的气候政策进展，比较不同文本表示方法，发现结合元数据时BERT模型表现最佳，为气候政策分析提供支持。


<details>
  <summary>Details</summary>
Motivation: 气候变化需要有效的立法行动来缓解其影响，本研究旨在利用机器学习理解气候政策从宣布到采纳的进展过程。

Method: 收集165项政策的文本和元数据，使用TF-IDF、BERT和ClimateBERT等文本表示方法，结合元数据特征预测政策进展状态。

Result: 仅使用文本特征时ClimateBERT表现最好（RMSE=0.17，R²=0.29），结合元数据后BERT表现最佳（RMSE=0.16，R²=0.38）。可解释AI方法显示政策措辞、政党和国家等元数据因素具有重要影响。

Conclusion: 机器学习工具在支持气候政策分析和决策制定方面具有巨大潜力，能够帮助理解政策进展的关键影响因素。

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [116] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: WEBSERV是一个用于训练和评估强化学习网络代理的可扩展环境，通过紧凑的浏览器环境和高效启动web服务器来解决现有环境的局限性，在WebArena任务中实现最先进性能并显著降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习网络代理环境存在上下文信息过多、动作执行不稳定、无法有效扩展等问题，需要开发一个既能提供真实浏览器交互又能控制服务器状态的可扩展环境。

Method: 提出WEBSERV环境，包含：1）紧凑、站点无关的浏览器环境，平衡上下文和动作复杂性；2）通过高效启动和重置web服务器实现可扩展的RL环境。

Result: 在WebArena的购物CMS和Gitlab任务中达到最先进的单提示成功率，启动延迟降低约5倍，存储需求减少约240倍，内存占用相当，可在单主机上运行200+并发容器。

Conclusion: WEBSERV提供了一个高效、可扩展的强化学习网络代理训练环境，解决了现有环境的局限性，显著提升了训练效率和资源利用率。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [117] [Disentangling Hyperedges through the Lens of Category Theory](https://arxiv.org/abs/2510.16289)
*Yoonho Lee,Junseok Lee,Sangwoo Seo,Sungwon Kim,Yeongmin Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 从范畴论角度分析超边解缠结，提出基于自然性条件的新解缠结准则，并在基因通路数据上验证其有效性


<details>
  <summary>Details</summary>
Motivation: 虽然解缠结表示学习在图结构数据中表现出色，但很少有研究探索超图结构数据的解缠结。将超边解缠结整合到超图神经网络中，可以挖掘与标签相关的隐藏超边语义

Method: 从范畴论视角分析超边解缠结，提出基于自然性条件的解缠结新准则，构建概念验证模型

Result: 实验证明所提准则能够成功捕捉基因通路中基因节点的功能关系

Conclusion: 提出的解缠结准则在超图数据中具有潜力，能够有效发现隐藏的超边语义模式

Abstract: Despite the promising results of disentangled representation learning in
discovering latent patterns in graph-structured data, few studies have explored
disentanglement for hypergraph-structured data. Integrating hyperedge
disentanglement into hypergraph neural networks enables models to leverage
hidden hyperedge semantics, such as unannotated relations between nodes, that
are associated with labels. This paper presents an analysis of hyperedge
disentanglement from a category-theoretical perspective and proposes a novel
criterion for disentanglement derived from the naturality condition. Our
proof-of-concept model experimentally showed the potential of the proposed
criterion by successfully capturing functional relations of genes (nodes) in
genetic pathways (hyperedges).

</details>


### [118] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 提出一种结合SVD分解和量化的方法，通过动态调整SVD秩来减少视觉语言模型的KV缓存大小和计算开销，在保持精度的同时显著降低内存使用和计算成本。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的高计算成本和内存占用限制了其可扩展性和实时应用性，需要开发更高效的部署方法。

Method: 使用奇异值分解(SVD)压缩QKV权重矩阵，并引入动态秩分配策略，结合模型权重和激活值的量化技术。

Result: 相比仅使用量化或SVD的方法，实现了超过10%的精度提升，同时硬件成本更低，更适合资源受限设备的实时部署。

Conclusion: 该方法通过SVD和量化的结合，在保持模型性能的同时显著提升了视觉语言模型的部署效率。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [119] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: ScaffAug是一个基于骨架的虚拟筛选框架，通过生成式AI增强数据、自训练和重排序三个模块，解决虚拟筛选中的类别不平衡、结构不平衡和骨架多样性问题。


<details>
  <summary>Details</summary>
Motivation: 虚拟筛选面临三个主要挑战：类别不平衡（活性分子比例低）、活性分子间的结构不平衡（某些骨架占主导）、以及需要识别结构多样的活性化合物用于新药开发。

Method: 1. 增强模块：使用图扩散模型基于真实命中分子的骨架生成合成数据；2. 自训练模块：将生成的合成数据与原始标记数据安全整合；3. 重排序模块：在保持整体性能的同时增强推荐分子集的骨架多样性。

Result: 在五个靶标类别上的综合计算实验表明，ScaffAug相比现有基线方法在多个评估指标上表现更好，并通过消融研究验证了各模块的有效性。

Conclusion: 这项工作通过利用生成增强、重排序和骨架感知，为有效增强虚拟筛选提供了新的视角。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [120] [Toward General Digraph Contrastive Learning: A Dual Spatial Perspective](https://arxiv.org/abs/2510.16311)
*Daohan Su,Yang Zhang,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: S2-DiGCL是一个针对有向图的对比学习框架，通过复数域和实数域的双重视角，结合磁拉普拉斯算子和路径子图增强，在节点分类和链接预测任务中实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法主要关注无向图，忽略了现实网络（如社交网络和推荐系统）中至关重要的方向性信息。

Method: 从复数域视角：在磁拉普拉斯中引入个性化扰动来调制边相位和方向语义；从实数域视角：使用基于路径的子图增强策略捕捉细粒度局部不对称性和拓扑依赖；联合利用这两个互补的空间视图构建高质量正负样本。

Result: 在7个真实世界有向图数据集上的实验表明，该方法在节点分类和链接预测任务中分别实现了4.41%和4.34%的性能提升，在监督和无监督设置下均达到SOTA水平。

Conclusion: S2-DiGCL通过整合复数域和实数域的空间洞察，为有向图对比学习提供了一个有效且鲁棒的框架，显著提升了表示学习性能。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful tool for
extracting consistent representations from graphs, independent of labeled
information. However, existing methods predominantly focus on undirected
graphs, disregarding the pivotal directional information that is fundamental
and indispensable in real-world networks (e.g., social networks and
recommendations).In this paper, we introduce S2-DiGCL, a novel framework that
emphasizes spatial insights from complex and real domain perspectives for
directed graph (digraph) contrastive learning. From the complex-domain
perspective, S2-DiGCL introduces personalized perturbations into the magnetic
Laplacian to adaptively modulate edge phases and directional semantics. From
the real-domain perspective, it employs a path-based subgraph augmentation
strategy to capture fine-grained local asymmetries and topological
dependencies. By jointly leveraging these two complementary spatial views,
S2-DiGCL constructs high-quality positive and negative samples, leading to more
general and robust digraph contrastive learning. Extensive experiments on 7
real-world digraph datasets demonstrate the superiority of our approach,
achieving SOTA performance with 4.41% improvement in node classification and
4.34% in link prediction under both supervised and unsupervised settings.

</details>


### [121] [Memorizing Long-tail Data Can Help Generalization Through Composition](https://arxiv.org/abs/2510.16322)
*Mo Zhou,Haoyang Ma,Rong Ge*

Main category: cs.LG

TL;DR: 该论文探讨了记忆化与组合能力之间的协同作用，表明记忆化长尾特征加上组合能力可以帮助模型对未见过的长尾特征组合做出正确预测。


<details>
  <summary>Details</summary>
Motivation: 重新思考深度学习中的记忆化与泛化关系，研究记忆化如何通过组合能力帮助模型处理训练数据中未出现的长尾特征组合。

Method: 理论分析在线性设置中记忆化与组合的协同作用，并在简单数据上通过神经网络架构进行实验验证。

Result: 理论证明和实验表明，记忆化结合组合能力可以使模型对未见过的长尾特征组合做出正确预测，且模型的组合能力取决于其架构。

Conclusion: 记忆化与组合能力的协同作用有助于模型泛化到未见过的长尾特征组合，这对理解深度学习中的泛化机制具有重要意义。

Abstract: Deep learning has led researchers to rethink the relationship between
memorization and generalization. In many settings, memorization does not hurt
generalization due to implicit regularization and may help by memorizing
long-tailed examples. In this paper, we consider the synergy between
memorization and simple composition -- the ability to make correct prediction
on a combination of long-tailed features. Theoretically, we show that for a
linear setting, memorization together with composition can help the model make
correct predictions on rare test examples that require a combination of
long-tailed features, even if such combinations were never observed in the
training data. Experiments on neural network architecture on simple data show
that the theoretical insight extends beyond the linear setting, and we further
observe that the composition capability of the model depends on its
architecture.

</details>


### [122] [MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting](https://arxiv.org/abs/2510.16350)
*Shule Hao,Junpeng Bao,Wenli Li*

Main category: cs.LG

TL;DR: MGTS-Net是一个用于时间序列预测的多模态图增强网络，通过优化特征提取、构建异构图融合多模态信息以及动态加权多尺度预测，解决了现有方法在细粒度时序模式提取、多模态信息融合和动态多尺度特征适应方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前多模态时间序列预测方法面临三个关键挑战：细粒度时序模式提取不足、多模态信息融合欠佳、动态多尺度特征适应能力有限。

Method: 模型包含三个核心组件：多模态特征提取层（MFE）优化时序、视觉和文本模态的特征编码器；多模态特征融合层（MFF）构建异构图建模模态内时序依赖和模态间对齐关系；多尺度预测层（MSP）动态加权融合短期、中期和长期预测器输出。

Result: 大量实验表明MGTS-Net在轻量级和高效率下表现出优异性能，相比其他最先进基线模型实现了更优越的性能。

Conclusion: 提出的方法验证了其优越性，MGTS-Net能够有效解决多模态时间序列预测中的关键挑战。

Abstract: Recent research in time series forecasting has explored integrating
multimodal features into models to improve accuracy. However, the accuracy of
such methods is constrained by three key challenges: inadequate extraction of
fine-grained temporal patterns, suboptimal integration of multimodal
information, and limited adaptability to dynamic multi-scale features. To
address these problems, we propose MGTS-Net, a Multimodal Graph-enhanced
Network for Time Series forecasting. The model consists of three core
components: (1) a Multimodal Feature Extraction layer (MFE), which optimizes
feature encoders according to the characteristics of temporal, visual, and
textual modalities to extract temporal features of fine-grained patterns; (2) a
Multimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph
to model intra-modal temporal dependencies and cross-modal alignment
relationships and dynamically aggregates multimodal knowledge; (3) a
Multi-Scale Prediction layer (MSP), which adapts to multi-scale features by
dynamically weighting and fusing the outputs of short-term, medium-term, and
long-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits
excellent performance with light weight and high efficiency. Compared with
other state-of-the-art baseline models, our method achieves superior
performance, validating the superiority of the proposed methodology.

</details>


### [123] [Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures](https://arxiv.org/abs/2510.16411)
*Minh-Khoi Nguyen-Nhat,Rachel S. Y. Teo,Laziz Abdullaev,Maurice Mok,Viet-Hoang Tran,Tan Minh Nguyen*

Main category: cs.LG

TL;DR: SymphonySMoE是一种新型稀疏专家混合模型，通过引入专家间的社交图来增强token路由过程，解决了传统SMoE在数据分布变化下的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏专家混合模型(SMoE)虽然能解耦模型参数量与计算成本，但在面对数据分布变化时鲁棒性较差，特别是在数据污染情况下表现不佳。

Method: 提出SymphonySMoE，在SMoE基础上引入专家间的社交图结构来建模专家交互，改进token路由过程。该方法轻量级、模块化，可与现有SMoE模型无缝集成。

Result: 在语言建模和视觉指令调优任务上的大量实验验证了方法的有效性，并成功扩展到42亿和74亿参数的大规模模型，在微调任务中表现出良好适用性。

Conclusion: SymphonySMoE通过图结构增强的专家交互机制，有效提升了SMoE在分布变化下的鲁棒性，同时保持了模型的效率和可扩展性。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a promising solution to
achieving unparalleled scalability in deep learning by decoupling model
parameter count from computational cost. By activating only a small subset of
parameters per sample, SMoE enables significant growth in model capacity while
maintaining efficiency. However, SMoE struggles to adapt to distributional
shifts, leading to reduced robustness under data contamination. In this work,
we introduce SymphonySMoE, a novel family of SMoE that introduces a social
graph to model interactions among experts. This graph-based structure enhances
the token routing process, addressing the robustness challenges that are
inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,
and integrates seamlessly with existing SMoE-based models such as the XMoE and
the Generalist Language Model. We provide both theoretical analysis and
empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.
Extensive experiments on language modeling and visual instruction tuning
validate our method's effectiveness. We further highlight the scalability of
SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its
applicability in fine-tuning tasks for large-scale systems.

</details>


### [124] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了ECML-PKDD 2025高能物理发现鲁棒学习挑战赛任务1的获胜解决方案，采用多轮梯度攻击策略在最小化扰动的同时最大化误分类。


<details>
  <summary>Details</summary>
Motivation: 任务要求设计对抗攻击，在最小化扰动的同时最大化分类模型的误分类率，挑战高能物理发现中的鲁棒学习问题。

Method: 采用多轮梯度攻击策略，利用模型的可微分结构，结合随机初始化和样本混合技术来增强攻击效果。

Result: 该攻击在扰动大小和欺骗成功率方面取得了最佳结果，在竞赛中获得第一名。

Conclusion: 提出的多轮梯度攻击策略结合随机初始化和样本混合技术，在高能物理发现任务中实现了高效的对抗攻击。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [125] [Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution](https://arxiv.org/abs/2510.16443)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了ECML-PKDD 2025挑战赛Task 2的获胜解决方案，通过数据生成和鲁棒模型训练两个阶段，在对抗性攻击下实现了80%的混合准确率。


<details>
  <summary>Details</summary>
Motivation: 设计能够在干净数据和对抗性数据上都表现良好的鲁棒ANN模型，以应对高能物理发现中的对抗攻击挑战。

Method: 采用两阶段方法：1) 使用基于RDSA的自定义方法生成1500万人工训练样本；2) 构建包含特征嵌入块（共享权重）和密集融合尾部的鲁棒架构。

Result: 在对抗性数据集上训练后获得了80%的混合准确率，比第二名解决方案高出2个百分点。

Conclusion: 提出的两阶段方法在对抗性环境下表现优异，证明了数据增强和专门架构设计对于提升模型鲁棒性的有效性。

Abstract: This report presents the winning solution for Task 2 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The goal of the challenge was to design and train a robust
ANN-based model capable of achieving high accuracy in a binary classification
task on both clean and adversarial data generated with the Random Distribution
Shuffle Attack (RDSA). Our solution consists of two components: a data
generation phase and a robust model training phase. In the first phase, we
produced 15 million artificial training samples using a custom methodology
derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we
introduced a robust architecture comprising (i)a Feature Embedding Block with
shared weights among features of the same type and (ii)a Dense Fusion Tail
responsible for the final prediction. Training this architecture on our
adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the
second-place solution by two percentage points.

</details>


### [126] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 提出Input Domain Aware MoE路由框架，通过概率混合模型更好地划分输入空间，解决现有稀疏专家混合模型在专家专业化和计算平衡之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似度评分的路由机制难以有效捕捉底层输入结构，导致专家专业化和计算平衡之间的权衡，阻碍了模型的可扩展性和性能。

Method: 使用概率混合模型建模路由概率，将路由概率表示为分布的混合，使专家能够形成清晰的专业化边界，同时实现平衡利用。路由机制独立于任务特定目标进行训练。

Result: 在视觉语言任务上的实证结果表明，该方法持续优于现有稀疏专家混合方法，实现了更高的任务性能和改善的专家利用平衡。

Conclusion: 提出的Input Domain Aware MoE路由框架通过概率混合模型有效解决了稀疏专家混合模型中的路由问题，在保持计算效率的同时提升了模型性能。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [127] [SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning](https://arxiv.org/abs/2510.16474)
*Farwa Abbas,Hussain Ahmad,Claudia Szabo*

Main category: cs.LG

TL;DR: 提出了一种新颖的自适应核注意力机制，通过分别处理不同特征组来增强高维异构数据的预测性能，解决了传统PLS方法在建模复杂非线性关系和跨尺度交互方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统投影到潜在结构(PLS)方法在处理高维异构数据时面临挑战：难以建模复杂非线性关系、无法捕捉跨尺度交互依赖、静态特征权重限制了对上下文变化的适应性。

Method: 引入自适应核注意力机制，分别处理不同特征组后再进行整合，既能捕捉局部模式又能保持全局关系。

Result: 实验结果表明，与最先进方法相比，该方法在多个数据集上显著提升了性能指标。

Conclusion: 所提出的自适应核注意力架构有效解决了高维异构数据建模中的关键挑战，在复杂非线性关系和跨尺度交互方面表现出优越性能。

Abstract: High-dimensional, heterogeneous data with complex feature interactions pose
significant challenges for traditional predictive modeling approaches. While
Projection to Latent Structures (PLS) remains a popular technique, it struggles
to model complex non-linear relationships, especially in multivariate systems
with high-dimensional correlation structures. This challenge is further
compounded by simultaneous interactions across multiple scales, where local
processing fails to capture crossgroup dependencies. Additionally, static
feature weighting limits adaptability to contextual variations, as it ignores
sample-specific relevance. To address these limitations, we propose a novel
method that enhances predictive performance through novel architectural
innovations. Our architecture introduces an adaptive kernel-based attention
mechanism that processes distinct feature groups separately before integration,
enabling capture of local patterns while preserving global relationships.
Experimental results show substantial improvements in performance metrics,
compared to the state-of-the-art methods across diverse datasets.

</details>


### [128] [Predicting life satisfaction using machine learning and explainable AI](https://arxiv.org/abs/2510.16547)
*Alif Elham Khan,Mohammad Junayed Hasan,Humayra Anjum,Nabeel Mohammed,Sifat Momen*

Main category: cs.LG

TL;DR: 该研究使用机器学习算法预测生活满意度，准确率达到93.80%，并通过特征学习提取了27个重要问题。还探索了临床和生物医学LLMs，发现生活满意度预测与生物医学领域更相关。健康状况是所有年龄段最重要的决定因素。


<details>
  <summary>Details</summary>
Motivation: 传统测量生活满意度的方法存在验证和传播问题，本研究旨在展示机器学习算法在预测生活满意度方面的潜力，为理解人类行为和主观幸福感提供更可靠的方法。

Method: 使用丹麦政府调查的19000人数据集，采用特征学习技术提取重要问题，并探索临床和生物医学LLMs通过将表格数据转换为自然语言句子进行预测。进行了消融研究分析数据重采样和特征选择技术的影响。

Result: 机器学习算法预测准确率达到93.80%，宏F1分数73.00%；LLMs方法准确率93.74%，宏F1分数73.21%。健康状况是所有年龄段最重要的决定因素。

Conclusion: 机器学习、大语言模型和可解释AI可以共同构建对使用AI研究人类行为的信任和理解，对量化主观幸福感具有重要影响。

Abstract: Life satisfaction is a crucial facet of human well-being. Hence, research on
life satisfaction is incumbent for understanding how individuals experience
their lives and influencing interventions targeted at enhancing mental health
and well-being. Life satisfaction has traditionally been measured using analog,
complicated, and frequently error-prone methods. These methods raise questions
concerning validation and propagation. However, this study demonstrates the
potential for machine learning algorithms to predict life satisfaction with a
high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a
government survey of 19000 people aged 16-64 years in Denmark. Using feature
learning techniques, 27 significant questions for assessing contentment were
extracted, making the study highly reproducible, simple, and easily
interpretable. Furthermore, clinical and biomedical large language models
(LLMs) were explored for predicting life satisfaction by converting tabular
data into natural language sentences through mapping and adding meaningful
counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It
was found that life satisfaction prediction is more closely related to the
biomedical domain than the clinical domain. Ablation studies were also
conducted to understand the impact of data resampling and feature selection
techniques on model performance. Moreover, the correlation between primary
determinants with different age brackets was analyzed, and it was found that
health condition is the most important determinant across all ages. This study
demonstrates how machine learning, large language models and XAI can jointly
contribute to building trust and understanding in using AI to investigate human
behavior, with significant ramifications for academics and professionals
working to quantify and comprehend subjective well-being.

</details>


### [129] [NeurIPT: Foundation Model for Neural Interfaces](https://arxiv.org/abs/2510.16548)
*Zitao Fang,Chenxuan Li,Hongting Zhou,Shuyang Yu,Guodong Du,Ashwaq Qasem,Yang Lu,Jing Li,Junsong Zhang,Sim Kuan Goh*

Main category: cs.LG

TL;DR: NeurIPT是一个用于脑电图(EEG)基础模型，通过预训练Transformer处理EEG信号的时空特征，解决了EEG数据中受试者、任务和条件间的变异性问题。


<details>
  <summary>Details</summary>
Motivation: 随着EEG数据量的增长，需要建立能够扩展和泛化神经解码的基础模型。但由于受试者间、任务间、条件间的显著变异性以及不同电极配置，应用基础模型到EEG仍具挑战。

Method: 提出NeurIPT模型：时间上使用基于信号幅度的掩码预训练(AAMP)和渐进专家混合架构(PMoE)；空间上利用电极3D物理坐标和开发脑叶内外池化(IILP)。

Result: 在八个下游BCI数据集上的评估显示，NeurIPT通过微调持续实现了最先进的性能，展示了其广泛的适用性和强大的泛化能力。

Conclusion: 该工作推动了EEG基础模型的发展，为可扩展和可泛化的神经信息处理系统提供了见解。

Abstract: Electroencephalography (EEG) has wide-ranging applications, from clinical
diagnosis to brain-computer interfaces (BCIs). With the increasing volume and
variety of EEG data, there has been growing interest in establishing foundation
models (FMs) to scale up and generalize neural decoding. Despite showing early
potential, applying FMs to EEG remains challenging due to substantial
inter-subject, inter-task, and inter-condition variability, as well as diverse
electrode configurations across recording setups. To tackle these open
challenges, we propose NeurIPT, a foundation model developed for diverse
EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both
homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG
signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),
masking based on signal amplitude rather than random intervals, to learn robust
representations across varying signal intensities beyond local interpolation.
Moreover, this temporal representation is enhanced by a Progressive
Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks
are progressively introduced at deeper layers, adapting effectively to the
diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages
the 3D physical coordinates of electrodes, enabling effective transfer of
embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling
(IILP) during fine-tuning to efficiently exploit regional brain features.
Empirical evaluations across eight downstream BCI datasets, via fine-tuning,
demonstrated NeurIPT consistently achieved state-of-the-art performance,
highlighting its broad applicability and robust generalization. Our work pushes
forward the state of FMs in EEG and offers insights into scalable and
generalizable neural information processing systems.

</details>


### [130] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: LANPO框架通过分离语言反馈和数值奖励的角色来解决LLM强化学习中的样本效率问题，语言指导探索，数值奖励驱动优化，显著提升数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习仅使用标量奖励会丢弃有价值的文本反馈，导致样本效率低下。直接集成在线经验存在信息泄露或行为崩溃的风险。

Method: LANPO构建动态经验池，采用奖励无关反思进行样本内自校正，通过相关抽象从样本间经验中提取可泛化教训。

Result: 在数学推理基准测试中，7B和14B模型使用LANPO显著优于GRPO基线，测试准确率大幅提升。

Conclusion: LANPO为将历史经验集成到LLM强化学习循环中提供了稳健方法，创建了更有效和数据高效的学习智能体。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [131] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出C-SMILES分子表示法，通过分解SMILES为元素-标记对和特殊标记，减少反应物与产物间的编辑距离，结合复制增强机制和SMILES对齐指导，显著提升逆合成预测准确率。


<details>
  <summary>Details</summary>
Motivation: 当前无模板方法难以捕捉化学反应中的结构不变性，导致搜索空间过大和预测精度降低。

Method: 开发C-SMILES表示法，引入复制增强机制动态决定生成新标记或保留未变分子片段，集成SMILES对齐指导增强注意力一致性。

Result: 在USPTO-50K和USPTO-FULL数据集上分别达到67.2%和50.8%的top-1准确率，生成分子有效性达99.9%。

Conclusion: 建立了结构感知分子生成的新范式，在计算药物发现中具有直接应用价值。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [132] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 提出了一种无需标注数据的分子推理框架，通过原子标识符将思维链推理锚定到分子结构上，在单步逆合成任务中取得了优异性能


<details>
  <summary>Details</summary>
Motivation: 化学中机器学习应用常受限于标注数据的稀缺和昂贵，传统监督方法受限，需要开发无需标注数据的分子推理方法

Method: 使用通用大语言模型，通过原子标识符将思维链推理锚定到分子结构，先进行单样本任务识别相关片段和化学标签，然后可选地进行少样本任务预测化学转化

Result: 在学术基准和专家验证的药物发现分子上，LLM在识别化学合理反应位点(≥90%)、命名反应类别(≥40%)和最终反应物(≥74%)方面取得高成功率

Conclusion: 该框架不仅解决了复杂化学任务，还提供了一种通过将化学知识映射到分子结构来生成理论基础的合成数据集的方法，从而解决数据稀缺问题

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [133] [Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules](https://arxiv.org/abs/2510.16607)
*Tianwei Wang,Xinhui Ma,Wei Pang*

Main category: cs.LG

TL;DR: 提出了一种四元数值监督学习Hopfield结构神经网络(QSHNN)，利用四元数在表示旋转和姿态方面的几何优势，通过周期性投影策略保持四元数结构一致性，在机器人控制等应用中表现出高精度和快速收敛。


<details>
  <summary>Details</summary>
Motivation: 利用四元数在表示旋转和姿态方面的几何优势，扩展经典Hopfield神经网络到四元数域，为机器人控制、路径规划等应用提供更好的数学基础。

Method: 从连续时间HNN动力学模型扩展到四元数域，引入周期性投影策略修改标准梯度下降，将权重矩阵的每个4*4块投影到最近的四元数结构，保持收敛性和四元数一致性。

Result: 实验模型实现了高精度、快速收敛和强可靠性，演化轨迹具有良好有界曲率（足够平滑性），适用于机器人关节姿态参数化等应用。

Conclusion: 该模型为超复数或非交换代数结构下的神经网络设计提供了实用的实现框架和通用数学方法学。

Abstract: Motivated by the geometric advantages of quaternions in representing
rotations and postures, we propose a quaternion-valued supervised learning
Hopfield-structured neural network (QSHNN) with a fully connected structure
inspired by the classic Hopfield neural network (HNN). Starting from a
continuous-time dynamical model of HNNs, we extend the formulation to the
quaternionic domain and establish the existence and uniqueness of fixed points
with asymptotic stability. For the learning rules, we introduce a periodic
projection strategy that modifies standard gradient descent by periodically
projecting each 4*4 block of the weight matrix onto the closest quaternionic
structure in the least-squares sense. This approach preserves both convergence
and quaternionic consistency throughout training. Benefiting from this rigorous
mathematical foundation, the experimental model implementation achieves high
accuracy, fast convergence, and strong reliability across randomly generated
target sets. Moreover, the evolution trajectories of the QSHNN exhibit
well-bounded curvature, i.e., sufficient smoothness, which is crucial for
applications such as control systems or path planning modules in robotic arms,
where joint postures are parameterized by quaternion neurons. Beyond these
application scenarios, the proposed model offers a practical implementation
framework and a general mathematical methodology for designing neural networks
under hypercomplex or non-commutative algebraic structures.

</details>


### [134] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 该论文研究了测试时增强（如RAG或工具使用）中模型参数知识与外部检索信息之间的关系，通过将多步推理建模为知识图上的连通性问题，揭示了参数知识密度与增强效率之间的相变现象。


<details>
  <summary>Details</summary>
Motivation: 理解测试时增强中模型参数知识与外部检索信息之间的理论关系，特别是确定在少量增强步骤下回答查询所需的预训练知识量，这在实践中是一个重要但尚未解决的问题。

Method: 将多步推理建模为知识图上的s-t连通性问题，将模型的预训练参数知识表示为部分且可能有噪声的子图，将增强视为查询真实边以扩展模型知识，然后表征在给定部分先验知识下生成准确答案所需的必要和充分增强步骤数。

Result: 发现了一个相变现象：如果先验知识图在n个顶点上断开成小分量，则通过增强找到路径是低效的，需要Ω(√n)次查询；而一旦正确知识的密度超过阈值形成巨分量，就能以期望的常数次查询找到路径。

Conclusion: 该研究为测试时增强提供了理论框架，揭示了参数知识密度与增强效率之间的关键关系，表明只有当模型具备足够密集的先验知识时，才能通过少量增强步骤有效回答问题。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [135] [On the Impossibility of Retrain Equivalence in Machine Unlearning](https://arxiv.org/abs/2510.16629)
*Jiatong Yu,Yinghui He,Anirudh Goyal,Sanjeev Arora*

Main category: cs.LG

TL;DR: 多阶段训练模型中的机器遗忘存在根本性障碍，局部遗忘方法无法普遍实现重训练等价性，因为遗忘结果依赖于训练阶段的顺序。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习流水线通常涉及多阶段训练（如LLM微调），而机器遗忘的理想目标是在i.i.d.数据假设下提出的。研究旨在探索多阶段训练对机器遗忘的影响。

Method: 通过理论和实验分析，使用梯度上升、NPO和SimNPO等局部遗忘算法，在Llama和Qwen模型（1B到14B）上进行LLM后训练实验。

Result: 不同训练阶段顺序的模型在遗忘过程中行为出现分歧，GSM8K准确率下降差异超过20%。某些学习路径产生的模型遗忘速度较慢，概率质量分配也呈现路径依赖性。

Conclusion: 重训练等价性对于局部遗忘算法来说是一个不适定的目标，在无法获取模型训练历史的情况下，需要重新思考机器遗忘的定义和期望目标。

Abstract: Machine unlearning seeks to selectively remove the "influence" of specific
training data on a model's outputs. The ideal goal is Retrain
Equivalence--behavior identical to a model trained from scratch on only the
retained data. This goal was formulated for models trained on i.i.d. data
batches, but modern pipelines often involve multi-stage training, with each
stage having a distinct data distribution and objective. Examples include LLM
fine-tuning for alignment, reasoning ability, etc. Our study shows via theory
and experiments that this shift to multi-stage training introduces a
fundamental barrier for machine unlearning. The theory indicates that the
outcome of local unlearning--methods that only use gradients computed on the
forget set--is path-dependent. That is, a model's behavior during unlearning is
influenced by the order of its training stages during learning, making it
impossible for path-oblivious algorithms to universally achieve Retrain
Equivalence. We empirically demonstrate the same phenomenon in LLM
post-training across Llama and Qwen models (1B to 14B) with gradient ascent,
NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different
orderings of identical training stages diverge in behavior during unlearning,
with the degradation in GSM8K accuracy after unlearning varying by over 20%
across paths. We also observe that some learning paths consistently produce
models that unlearn slowly. During unlearning, whether the probability mass
gets squeezed into paraphrasing or alternative concepts is also path-dependent.
These results consistently show that Retrain Equivalence is an ill-posed target
for local unlearning algorithms, so long as the target models are trained in
stages. In situations where access to models' training histories is hard, the
current work calls for rethinking the definition and desiderata of machine
unlearning.

</details>


### [136] [Simulation-free Structure Learning for Stochastic Dynamics](https://arxiv.org/abs/2510.16656)
*Noah El Rimawi-Fine,Adam Stecklov,Lucas Nelson,Mathieu Blanchette,Alexander Tong,Stephen Y. Zhang,Lazar Atanackovic*

Main category: cs.LG

TL;DR: StructureFlow是一种新颖的联合学习方法，能够同时学习物理系统的结构和随机群体动力学，解决了传统方法无法同时处理结构学习和动态建模的问题。


<details>
  <summary>Details</summary>
Motivation: 许多自然系统中的物理系统（如细胞生物学）具有高维、随机特性，且只能获得部分噪声状态测量，这给建模系统动态和推断网络结构带来了重大挑战。现有方法通常只能单独处理结构学习或群体级动态建模。

Method: 提出StructureFlow方法，这是一种无模拟的、原则性的方法，能够联合学习物理系统的结构和随机群体动力学。该方法适用于干预下的结构学习和条件群体动态的轨迹推断任务。

Result: 在合成高维系统、生物模拟系统和实验单细胞数据集上的实证评估表明，StructureFlow能够同时学习底层系统的结构并建模其条件群体动态。

Conclusion: StructureFlow是理解系统行为机制的关键步骤，能够同时学习系统结构和条件群体动态，为高维随机系统的分析提供了有效工具。

Abstract: Modeling dynamical systems and unraveling their underlying causal
relationships is central to many domains in the natural sciences. Various
physical systems, such as those arising in cell biology, are inherently
high-dimensional and stochastic in nature, and admit only partial, noisy state
measurements. This poses a significant challenge for addressing the problems of
modeling the underlying dynamics and inferring the network structure of these
systems. Existing methods are typically tailored either for structure learning
or modeling dynamics at the population level, but are limited in their ability
to address both problems together. In this work, we address both problems
simultaneously: we present StructureFlow, a novel and principled
simulation-free approach for jointly learning the structure and stochastic
population dynamics of physical systems. We showcase the utility of
StructureFlow for the tasks of structure learning from interventions and
dynamical (trajectory) inference of conditional population dynamics. We
empirically evaluate our approach on high-dimensional synthetic systems, a set
of biologically plausible simulated systems, and an experimental single-cell
dataset. We show that StructureFlow can learn the structure of underlying
systems while simultaneously modeling their conditional population dynamics --
a key step toward the mechanistic understanding of systems behavior.

</details>


### [137] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: PUMBA是一个基于Vision Mamba的蛋白质-蛋白质对接评分函数，通过替换PIsToN中的Vision Transformer架构，在多个数据集上表现优于原模型。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质-蛋白质对接工具依赖准确的评分函数来区分天然和非天然复合物。虽然PIsToN使用Vision Transformer表现良好，但Mamba架构在自然语言处理和计算机视觉领域显示出优于Transformer的性能。

Method: 将PIsToN中的Vision Transformer主干替换为Vision Mamba架构，利用Mamba在图像块序列上的高效长程序列建模能力，提升对蛋白质-蛋白质界面特征的全局和局部模式捕捉能力。

Result: 在多个广泛使用的大规模公共数据集上的评估表明，PUMBA持续优于其基于Transformer的前身PIsToN。

Conclusion: PUMBA通过引入Vision Mamba架构成功改进了蛋白质-蛋白质对接评分函数的性能，证明了Mamba架构在该领域的有效性。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [138] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 提出一种在无信息先验条件下进行有效主动目标发现的新方法，确保在复杂现实场景中的鲁棒探索和适应性。


<details>
  <summary>Details</summary>
Motivation: 在数据极其有限或采样成本高的领域（如稀有物种发现、新兴疾病诊断等），由于无法学习到强先验，现有基于生成模型的主动目标发现方法难以泛化。

Method: 基于神经科学启发的理论原则框架，具有内在可解释性，保证每次新观测都能单调改进先验估计，实现逐步精确采样。

Result: 在物种分布建模和遥感等多个领域的综合实验和消融研究中，该方法显著优于基线方法。

Conclusion: 该方法在动态环境中提供了可靠性和适应性的增强，解决了无信息先验条件下的主动目标发现问题。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [139] [Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers](https://arxiv.org/abs/2510.16677)
*Ran Tong,Jiaqi Liu,Su Liu,Xin Hu,Lanruo Wang*

Main category: cs.LG

TL;DR: 提出了一个紧凑、严格因果的临床时间序列基准，在MIT-BIH心律失常数据库上使用每秒心率数据。研究了心动过速风险预测和心率预测两个任务，比较了GRU-D和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 在纵向监测中，需要评估不同模型在临床时间序列任务上的表现，以确定模型选择是否依赖于具体任务。

Method: 使用MIT-BIH心律失常数据库的每秒心率数据，在记录级非重叠分割下研究两个任务：近心动过速风险预测（未来10秒）和一步心率预测。比较了GRU-D（RNN）和Transformer模型，并与强非学习基线进行对比。

Result: 在MIT-BIH数据集上，GRU-D在心动过速风险预测上略优于Transformer，而Transformer在心率预测误差上明显低于GRU-D和持续性模型。

Conclusion: 在纵向监测中，模型选择是任务依赖的：紧凑RNN在短时域风险评分中保持竞争力，而紧凑Transformer在点预测方面提供更清晰的增益。

Abstract: We present a compact, strictly causal benchmark for streaming clinical time
series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two
tasks are studied under record-level, non-overlapping splits: near-term
tachycardia risk (next ten seconds) and one-step heart rate forecasting. We
compare a GRU-D (RNN) and a Transformer under matched training budgets against
strong non-learned baselines. Evaluation is calibration-aware for
classification and proper for forecasting, with temperature scaling and grouped
bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the
Transformer for tachycardia risk, while the Transformer clearly lowers
forecasting error relative to GRU-D and persistence. Our results show that, in
longitudinal monitoring, model choice is task-dependent: compact RNNs remain
competitive for short-horizon risk scoring, whereas compact Transformers
deliver clearer gains for point forecasting.

</details>


### [140] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 本文通过扩散方法精确分析噪声SGD，在高维设置下提供统计风险和隐私损失的连续时间视角，并研究无需梯度敏感性知识的SGD变体。


<details>
  <summary>Details</summary>
Motivation: 优化与隐私的交互已成为隐私保护机器学习的核心主题，但现有工作主要提供统计风险和隐私损失的边界，对噪声SGD的确切行为特别是高维设置下仍不清楚。

Method: 采用扩散方法分析噪声SGD，提供连续时间视角；研究无需梯度敏感性知识的SGD变体，重点关注带ℓ2正则化的最小二乘问题。

Result: 该方法能够精确捕捉高维设置下统计风险演化和隐私损失动态，并克服了现有工作依赖梯度敏感性知识或梯度裁剪的限制。

Conclusion: 扩散方法为分析噪声SGD提供了更精确的框架，特别是在高维环境中，同时提出的变体算法降低了实现差分隐私的复杂性。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [141] [CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning](https://arxiv.org/abs/2510.16694)
*Anthony DiMaggio,Raghav Sharma,Gururaj Saileshwar*

Main category: cs.LG

TL;DR: CLIP是一种客户端不变神经元剪枝技术，结合网络感知剪枝，解决安全联邦学习中因计算和网络瓶颈导致的训练速度问题，在多个数据集上加速13%到34%，准确率影响在-2.6%到+1.3%之间。


<details>
  <summary>Details</summary>
Motivation: 安全联邦学习在保护数据隐私的同时，由于异构设备中存在计算或网络能力受限的慢客户端（stragglers），导致整体训练性能瓶颈，拖慢所有参与客户端的训练速度。

Method: 提出CLIP技术，结合客户端不变神经元剪枝和网络感知剪枝，通过剪枝策略缓解慢客户端带来的计算和网络瓶颈，同时最小化准确率损失。

Result: 在CIFAR10、Shakespeare、FEMNIST等多个数据集上，安全联邦学习训练速度提升13%到34%，准确率影响在1.3%提升到2.6%下降之间。

Conclusion: CLIP是首个针对安全聚合深度神经网络设计的慢客户端缓解技术，能有效加速安全联邦学习训练，同时保持较好的模型准确性。

Abstract: Secure federated learning (FL) preserves data privacy during distributed
model training. However, deploying such frameworks across heterogeneous devices
results in performance bottlenecks, due to straggler clients with limited
computational or network capabilities, slowing training for all participating
clients. This paper introduces the first straggler mitigation technique for
secure aggregation with deep neural networks. We propose CLIP, a client-side
invariant neuron pruning technique coupled with network-aware pruning, that
addresses compute and network bottlenecks due to stragglers during training
with minimal accuracy loss. Our technique accelerates secure FL training by 13%
to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an
accuracy impact of between 1.3% improvement to 2.6% reduction.

</details>


### [142] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出了一种分辨率感知的检索增强预测模型，通过空间相关性和时间频率特征提升零样本预测精度，在微气候预测中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 零样本预测在没有直接历史数据的情况下预测未见条件的结果，这对传统预测方法构成重大挑战。

Method: 通过将信号分解为不同频率分量，采用分辨率感知检索：低频分量依赖更广泛的空间上下文，高频分量关注局部影响，动态检索相关数据并适应新位置。

Result: 在微气候预测中，模型显著优于传统预测方法、数值天气预报模型和现代基础时间序列模型，在ERA5数据集上比HRRR降低71% MSE，比Chronos降低34% MSE。

Conclusion: 检索增强和分辨率感知策略在零样本预测中非常有效，为微气候建模及其他领域提供了可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [143] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 本文探讨了基于状态的因果效应可识别性问题，发现在某些情况下即使基于变量的因果效应不可识别，基于状态的因果效应仍可能可识别，特别是当存在上下文特定独立性和条件函数依赖等额外知识时。


<details>
  <summary>Details</summary>
Motivation: 传统的因果效应可识别性定义基于变量层面，但实际应用中可能更关注特定状态间的因果效应。本文旨在研究基于状态的因果效应可识别性，探索在什么条件下状态层面的因果效应比变量层面的更容易识别。

Method: 通过理论分析，比较基于变量和基于状态的因果效应可识别性条件，特别关注上下文特定独立性、条件函数依赖等额外知识对可识别性的影响。

Result: 研究发现：1）基于状态的因果效应可能在基于变量的因果效应不可识别时仍可识别；2）这种分离仅在存在额外知识时发生；3）变量状态约束知识本身不能单独改善可识别性，但与其他知识结合时可同时改善基于变量和基于状态的可识别性。

Conclusion: 基于状态的因果效应可识别性分析可以揭示传统变量层面框架可能错过的可识别机会，为从观测数据中估计感兴趣的因果效应提供了新的视角。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [144] [LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus](https://arxiv.org/abs/2510.16719)
*Zak Ressler,Marcus Grijalva,Angelica Marie Ignacio,Melanie Torres,Abelardo Cuadra Rojas,Rohollah Moghadam,Mohammad Rasoul narimani*

Main category: cs.LG

TL;DR: 提出基于LSTM的电动汽车充电负荷预测框架，通过数据预处理和特征提取来准确预测多时间尺度的充电需求。


<details>
  <summary>Details</summary>
Motivation: 解决电动汽车充电基础设施规划和电网集成的需求，需要准确预测充电负荷以支持能源管理和设施部署。

Method: 使用LSTM循环神经网络，对原始充电数据进行归一化和特征提取预处理，处理缺失值并捕捉短期波动和长期趋势。

Result: 实验结果表明模型能够准确预测日、周、月等多时间尺度的充电需求，为基础设施规划提供有价值见解。

Conclusion: 该模块化框架适用于不同充电站点和多样化部署场景，具有良好的适应性和实用性。

Abstract: This paper presents a framework for processing EV charging load data in order
to forecast future load predictions using a Recurrent Neural Network,
specifically an LSTM. The framework processes a large set of raw data from
multiple locations and transforms it with normalization and feature extraction
to train the LSTM. The pre-processing stage corrects for missing or incomplete
values by interpolating and normalizing the measurements. This information is
then fed into a Long Short-Term Memory Model designed to capture the short-term
fluctuations while also interpreting the long-term trends in the charging data.
Experimental results demonstrate the model's ability to accurately predict
charging demand across multiple time scales (daily, weekly, and monthly),
providing valuable insights for infrastructure planning, energy management, and
grid integration of EV charging facilities. The system's modular design allows
for adaptation to different charging locations with varying usage patterns,
making it applicable across diverse deployment scenarios.

</details>


### [145] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多任务学习和分层高斯过程的方法来预测NLP模型的学习曲线，支持零样本预测并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 预测NLP模型的学习曲线可以帮助在满足特定性能目标的同时减少计算开销，并降低数据集获取和整理的成本。

Method: 将学习曲线预测任务构建为多任务学习问题，使用潜在变量多输出高斯过程建模任务间和层次间的共享信息和依赖关系。

Result: 该方法能够在三个小型NLP数据集上实现接近真实缩放规律的学习曲线预测，支持零样本预测并降低不确定性。

Conclusion: 该框架能够以较低成本开发概率性缩放规律，通过主动学习策略减少预测不确定性，为NLP模型性能预测提供有效解决方案。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [146] [An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications](https://arxiv.org/abs/2510.16747)
*Danish Nazir,Gowtham Sai Inti,Timo Bartels,Jan Piewek,Thorsten Bagdonat,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 提出了一种用于SegDeformer的联合特征和任务解码方法，在车载和分布式应用中降低计算复杂度，同时保持语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现代汽车系统使用DNN进行语义分割，但现有方法要么计算复杂度高（如基于transformer的SegDeformer），要么在分布式应用中需要大量云端参数。需要一种既能降低计算复杂度又能保持性能的解决方案。

Method: 为SegDeformer设计联合特征和任务解码方法，共享图像和源编码器，但使用不同的联合源和任务解码器。该方法适用于车载和分布式两种应用场景。

Result: 在车载应用中，Cityscapes数据集上fps从1.4提升到16.5（11.7倍），ADE20K数据集上从43.3提升到154.3（3.5倍），mIoU与未压缩的transformer基线相当。在分布式应用中，在广泛的比特率范围内实现SOTA mIoU，云端DNN参数仅需之前SOTA的0.14%（ADE20K）和0.04%（Cityscapes）。

Conclusion: 提出的联合特征和任务解码方法显著降低了计算复杂度，提高了系统可扩展性，同时保持了语义分割性能，为汽车系统提供了高效的解决方案。

Abstract: Modern automotive systems leverage deep neural networks (DNNs) for semantic
segmentation and operate in two key application areas: (1) In-car, where the
DNN solely operates in the vehicle without strict constraints on the data rate.
(2) Distributed, where one DNN part operates in the vehicle and the other part
typically on a large-scale cloud platform with a particular constraint on
transmission bitrate efficiency. Typically, both applications share an image
and source encoder, while each uses distinct (joint) source and task decoders.
Prior work utilized convolutional neural networks for joint source and task
decoding but did not investigate transformer-based alternatives such as
SegDeformer, which offer superior performance at the cost of higher
computational complexity. In this work, we propose joint feature and task
decoding for SegDeformer, thereby enabling lower computational complexity in
both in-car and distributed applications, despite SegDeformer's computational
demands. This improves scalability in the cloud while reducing in-car
computational complexity. For the in-car application, we increased the frames
per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on
Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on
ADE20K, while being on-par w.r.t.\ the mean intersection over union (mIoU) of
the transformer-based baseline that doesn't compress by a source codec. For the
distributed application, we achieve state-of-the-art (SOTA) over a wide range
of bitrates on the mIoU metric, while using only $0.14$\% ($0.04$\%) of cloud
DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).

</details>


### [147] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出SAMOSA方法，通过锐度感知最小化进行开放集主动学习，基于样本典型性选择信息量大的样本，在多个数据集上实现最高3%的准确率提升


<details>
  <summary>Details</summary>
Motivation: 现代机器学习需要大量数据收集，但标注成本高昂。开放集主动学习旨在从未标记数据中选择信息量大的样本，其中包含不相关或未知类别

Method: 基于SGD和SAM理论发现，根据样本典型性主动查询样本，识别嵌入流形中靠近模型决策边界的非典型样本

Result: 在多个数据集上实现最高3%的准确率提升，且不引入计算开销

Conclusion: SAMOSA是一种有效的开放集主动学习查询算法，能优先选择对目标类别信息量大且有助于区分目标与不相关类别的样本

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [148] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 该论文提出了一个用于3D第一人称视频游戏的多模态推理环境，构建了大规模多样化的游戏数据集，开发了能够实时推理的游戏AI代理，并解决了动作推断和文本条件控制等挑战。


<details>
  <summary>Details</summary>
Motivation: 3D第一人称视频游戏是实时多模态推理的挑战性环境，现有数据集规模有限且缺乏多样性，需要开发能够响应文本指令的智能游戏代理。

Method: 收集大规模多样化的人类游戏数据集，学习逆动力学模型来推断缺失的动作，使用行为克隆训练文本条件代理，采用支持实时推理的自定义架构。

Result: 成功训练出能够在多种3D游戏中响应文本输入的游戏代理，模型能够在消费级GPU上实现实时推理。

Conclusion: 该方法展示了在复杂3D游戏环境中实现多模态推理的可行性，但仍面临长时程任务和大规模游戏定量评估等挑战。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [149] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 提出3D-GSRD方法，通过选择性重掩码解码解决3D分子表示学习中2D结构泄漏问题，在MD17基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 将掩码图建模从2D扩展到3D时面临两个冲突挑战：避免2D结构泄漏到解码器，同时为重构重掩码原子提供足够的2D上下文

Method: 提出选择性重掩码解码(SRD)，仅从编码器表示中重掩码3D相关信息，同时保留2D图结构；结合3D关系变换器编码器和结构无关解码器

Result: 在广泛使用的MD17分子性质预测基准测试中，8个目标中有7个达到新的最先进性能

Conclusion: 3D-GSRD通过选择性重掩码解码和结构无关解码器的协同集成，增强了编码器在分子表示学习中的作用

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [150] [Mixed-Precision Quantization for Language Models: Techniques and Prospects](https://arxiv.org/abs/2510.16805)
*Mariam Rakka,Marios Fournarakis,Olga Krestinskaya,Jinane Bazzi,Khaled N. Salama,Fadi Kurdahi,Ahmed M. Eltawil,Mohammed E. Fouda*

Main category: cs.LG

TL;DR: 这篇综述论文系统回顾了语言模型的混合精度量化技术，分析了不同位分配策略和精度配置，比较了困惑度、零样本任务性能等指标，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模的快速扩展，其计算、内存和能耗需求急剧增长，使得训练和部署变得不可持续。量化作为一种重要的压缩技术，可以减小模型大小、缓解内存瓶颈并加速推理。

Method: 论文首先回顾量化基础，包括均匀和非均匀量化器、量化粒度以及后训练量化方法。然后根据位分配策略和权重、激活、键值缓存的精度配置，对混合精度量化框架进行分类和比较。

Result: 通过对比分析揭示了不同混合精度量化框架在困惑度、零样本任务性能和部署权衡方面的差异，并与早期深度神经网络混合精度量化方法进行了对比。

Conclusion: 混合精度量化为大规模语言模型提供了平衡效率和准确性的有前景方案，未来需要在硬件感知设计、激活量化和可扩展优化方法等方面进一步研究。

Abstract: The rapid scaling of language models (LMs) has resulted in unprecedented
computational, memory, and energy requirements, making their training and
deployment increasingly unsustainable. Quantization has emerged as an essential
compression technique to reduce model size, alleviate memory bottlenecks, and
accelerate inference. However, while uniform low-bit quantization (e.g., INT8,
INT4) provides significant efficiency gains, it can degrade accuracy in
sensitive components of transformer-based LMs. Mixed-precision quantization
offers a promising alternative by selectively allocating precision across
layers or within tensors to balance efficiency and accuracy. This survey
provides a comprehensive overview of Mixed-Precision quantization frameworks
for LMs (MXPLMs). We first review quantization fundamentals, including uniform
and non-uniform quantizers, quantization granularity, and methods widely used
in post-training quantization. We then categorize and compare recent MXPLM
frameworks according to their bit allocation strategies and precision
configurations across weights, activations, and key-value caches. A comparative
analysis highlights differences in perplexity, zero-shot task performance, and
deployment trade-offs. Furthermore, we contrast MXPLMs with earlier
mixed-precision quantization methods for deep neural networks, identifying
strategies that transfer and those that face challenges in the LM setting.
Finally, we summarize open issues and future directions, including
hardware-aware design, activation quantization, and scalable optimization
methods for billion-parameter models. By consolidating recent advances, this
work serves as a reference for understanding the current landscape and research
prospects of mixed-precision quantization for large-scale language models.

</details>


### [151] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出了一种计算预算感知的数据选择方法(CADS)，通过双层优化框架将计算预算约束整合到数据选择过程中，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽略了计算预算约束，而实证研究表明不同预算下需要不同的数据数量、质量和分布策略。计算预算应成为数据选择策略的核心组成部分。

Method: 采用双层优化框架：内层在计算预算约束下用选定数据子集训练模型，外层基于模型评估优化数据选择。通过概率重参数化策略和Hessian-free策略梯度估计器解决Hessian矩阵计算问题，将内层优化转化为外层目标中的惩罚项。

Result: 在视觉和语言基准测试中，相比基线方法实现了高达14.42%的性能提升。

Conclusion: 计算预算感知的数据选择方法能够有效提升训练效率，证明了将计算预算约束整合到数据选择策略中的重要性。

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [152] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: SkipV1Former是一种Transformer变体，通过从第一层的Value头添加跳跃连接来增强模型表示能力并减少KV缓存。该方法重用第一层的一半Value头，将Value投影和V缓存减少近50%，在减少25% KV缓存的同时提升语言建模性能。


<details>
  <summary>Details</summary>
Motivation: 扩展Transformer模型以改进表示能力通常需要大量内存和计算成本，特别是自回归解码中的KV缓存。现有方法要么改进表达能力但KV成本不变，要么减少内存但表示能力变弱。

Method: 从第二层开始，每层重用第一层的一半Value头，同时正常计算另一半Value头，从而将Value投影和V缓存减少近50%。理论上，将未压缩的第一层Value路由到更深层可以恢复压缩丢失的信息并加速隐式mesa优化。

Result: 在不同模型规模上，SkipV1Former相对于标准多头注意力Transformer和一些先进变体，在减少约25% KV缓存的同时改善了困惑度。与YOCO结合时，KV缓存大小减少近50%且性能仍提升。

Conclusion: SkipV1Former提供了一种有效的方法来增强Transformer表示能力并减少KV缓存，且可以通过仅10-15%额外计算将现有MHA Transformer检查点上训练到SkipV1Former，并能与GQA和MLA等先进方法无缝结合。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [153] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 本文研究了因果充分性下因果强盗问题中的遗憾最小化，发现学习父节点集是次优的，证明了遗憾最小化和父节点识别之间存在根本冲突，并提出了绕过图恢复的最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注识别奖励的父节点然后应用经典强盗方法，或联合学习父节点同时最小化遗憾。本文旨在研究这些策略是否最优，并探索遗憾最小化与父节点识别之间的关系。

Method: 通过理论分析证明父节点识别与遗憾最小化存在冲突，建立包含动作空间组合结构的遗憾下界，提出绕过图恢复和父节点恢复的最优算法。

Result: 证明了存在实例中遗憾最小化和父节点识别是根本冲突的目标，建立了新的遗憾下界，提出的算法在各种环境中显著优于现有基线方法。

Conclusion: 父节点识别对于遗憾最小化是不必要的，绕过图恢复的算法能够实现近乎最优的性能，这挑战了因果强盗问题中的传统方法。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [154] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 使用半监督学习和正未标记学习策略，通过深度学习模型进行考古预测建模，在标签稀缺的情况下有效识别未知考古遗址位置


<details>
  <summary>Details</summary>
Motivation: 解决考古学中结构性的标签稀缺问题——阳性样本稀少且大多数位置未被标记，需要开发能够处理严重类别不平衡的方法

Method: 采用半监督正未标记学习策略，实现为语义分割模型，使用动态伪标签和通过RNN实现的CRF来增强标签置信度

Result: 在DEM衍生的地理空间数据集上与最先进方法LAMAP表现相当但获得更高Dice分数；在原始卫星图像上保持性能并产生更具可解释性的预测表面

Conclusion: 半监督学习为在大规模稀疏标注景观中识别未知遗址提供了一种有前景的方法

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [155] [Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator](https://arxiv.org/abs/2510.16816)
*Ming Zhong,Zhenya Yan*

Main category: cs.LG

TL;DR: 提出线性注意力神经算子(LANO)，通过引入少量代理令牌实现线性复杂度，解决传统注意力机制在可扩展性和准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于transformer的神经算子架构面临的可扩展性-准确性权衡：softmax注意力精度高但复杂度为O(N²d)，线性注意力复杂度为O(Nd²)但精度显著下降。

Method: 引入紧凑的M个代理令牌(M≪N)，通过代理注意力机制调解N个令牌之间的全局交互，实现线性复杂度O(MNd)同时保持softmax注意力的表达能力。

Result: 理论上证明了通用逼近性质，改善了条件性和稳定性；实证上在标准基准测试中超越当前最先进的神经PDE求解器，平均精度提升19.5%。

Conclusion: LANO通过在线性复杂度和softmax级性能之间架起桥梁，为科学机器学习应用建立了可扩展、高精度的基础。

Abstract: Neural operators offer a powerful data-driven framework for learning mappings
between function spaces, in which the transformer-based neural operator
architecture faces a fundamental scalability-accuracy trade-off: softmax
attention provides excellent fidelity but incurs quadratic complexity
$\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,
while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often
suffer significant accuracy degradation. To address the aforementioned
challenge, in this paper, we present a novel type of neural operators, Linear
Attention Neural Operator (LANO), which achieves both scalability and high
accuracy by reformulating attention through an agent-based mechanism. LANO
resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll
N)$ that mediate global interactions among $N$ tokens. This agent attention
mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$
while preserving the expressive power of softmax attention. Theoretically, we
demonstrate the universal approximation property, thereby demonstrating
improved conditioning and stability properties. Empirically, LANO surpasses
current state-of-the-art neural PDE solvers, including Transolver with
slice-based softmax attention, achieving average $19.5\%$ accuracy improvement
across standard benchmarks. By bridging the gap between linear complexity and
softmax-level performance, LANO establishes a scalable, high-accuracy
foundation for scientific machine learning applications.

</details>


### [156] [Trace Regularity PINNs: Enforcing $\mathrm{H}^{\frac{1}{2}}(\partial Ω)$ for Boundary Data](https://arxiv.org/abs/2510.16817)
*Doyoon Kim,Junbin Song*

Main category: cs.LG

TL;DR: 提出TRPINN方法，在Sobolev-Slobodeckij范数H^{1/2}(∂Ω)中强制执行边界损失，这是与H^1(Ω)相关的正确迹空间。通过计算半范数的理论必要部分降低计算成本，避免离散化中的分母评估来增强收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准PINNs在处理高度振荡的Dirichlet边界条件时可能失败，需要改进边界损失的计算方式以提高收敛性和稳定性。

Method: 使用TRPINN方法，在H^{1/2}(∂Ω)范数中强制执行边界损失，只计算半范数的理论必要部分，避免分母评估。通过NTK分析验证收敛性。

Result: 在Laplace方程的高度振荡Dirichlet边界条件下，TRPINN在标准PINNs失败的情况下仍能成功，性能提高1-3个十进制数字。

Conclusion: TRPINN通过使用正确的迹空间范数，能够提高PINNs的收敛速度和稳定性，在处理复杂边界条件时表现更优。

Abstract: We propose an enhanced physics-informed neural network (PINN), the Trace
Regularity Physics-Informed Neural Network (TRPINN), which enforces the
boundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\partial \Omega)$, the
correct trace space associated with $H^1(\Omega)$. We reduce computational cost
by computing only the theoretically essential portion of the semi-norm and
enhance convergence stability by avoiding denominator evaluations in the
discretization. By incorporating the exact $H^{1/2}(\partial \Omega)$ norm, we
show that the approximation converges to the true solution in the
$H^{1}(\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we
demonstrate that TRPINN can converge faster than standard PINNs. Numerical
experiments on the Laplace equation with highly oscillatory Dirichlet boundary
conditions exhibit cases where TRPINN succeeds even when standard PINNs fail,
and show performance improvements of one to three decimal digits.

</details>


### [157] [Finding Manifolds With Bilinear Autoencoders](https://arxiv.org/abs/2510.16820)
*Thomas Dooms,Ward Gauderis*

Main category: cs.LG

TL;DR: 本文提出使用双线性自编码器将表示分解为二次多项式，作为可分析的代数基元，无需依赖输入即可研究神经网络中的潜在表示。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器对潜在表示的解释依赖于输入，使得独立研究不完整。多项式作为代数基元可以在不参考输入的情况下进行分析，并能描述从线性概念到复杂流形的各种结构。

Method: 使用双线性自编码器高效地将表示分解为二次多项式，并引入改进方法以诱导重要性排序、聚类和激活稀疏性。

Result: 开发了一种通过代数属性分析非线性潜在表示的方法，实现了对神经网络表示的可解释性分解。

Conclusion: 这是通过代数属性实现非线性但可分析潜在表示的第一步，为神经网络的可解释性研究提供了新途径。

Abstract: Sparse autoencoders are a standard tool for uncovering interpretable latent
representations in neural networks. Yet, their interpretation depends on the
inputs, making their isolated study incomplete. Polynomials offer a solution;
they serve as algebraic primitives that can be analysed without reference to
input and can describe structures ranging from linear concepts to complicated
manifolds. This work uses bilinear autoencoders to efficiently decompose
representations into quadratic polynomials. We discuss improvements that induce
importance ordering, clustering, and activation sparsity. This is an initial
step toward nonlinear yet analysable latents through their algebraic
properties.

</details>


### [158] [ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning](https://arxiv.org/abs/2510.16824)
*Yingxu Wang,Kunyu Zhang,Jiaxin Huang,Nan Yin,Siwei Liu,Eran Segal*

Main category: cs.LG

TL;DR: ProtoMol是一个原型引导的多模态分子表示学习框架，通过层次化编码器和双向跨模态注意力机制，实现分子图与文本描述之间的细粒度整合和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法存在两个关键局限：仅在最终编码层进行跨模态交互，忽略了层次语义依赖；缺乏统一的原型空间来实现模态间的鲁棒对齐。

Method: 采用双分支层次编码器（图神经网络处理分子图，Transformer编码文本），引入层次化双向跨模态注意力机制，并构建共享原型空间与可学习的类特定锚点。

Result: 在多个基准数据集上的广泛实验表明，ProtoMol在各种分子性质预测任务中始终优于最先进的基线方法。

Conclusion: ProtoMol通过层次化跨模态交互和原型引导的语义对齐，显著提升了多模态分子表示学习的性能，为药物毒性、生物活性和物理化学性质的预测提供了更准确和可解释的解决方案。

Abstract: Multimodal molecular representation learning, which jointly models molecular
graphs and their textual descriptions, enhances predictive accuracy and
interpretability by enabling more robust and reliable predictions of drug
toxicity, bioactivity, and physicochemical properties through the integration
of structural and semantic information. However, existing multimodal methods
suffer from two key limitations: (1) they typically perform cross-modal
interaction only at the final encoder layer, thus overlooking hierarchical
semantic dependencies; (2) they lack a unified prototype space for robust
alignment between modalities. To address these limitations, we propose
ProtoMol, a prototype-guided multimodal framework that enables fine-grained
integration and consistent semantic alignment between molecular graphs and
textual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,
utilizing Graph Neural Networks to process structured molecular graphs and
Transformers to encode unstructured texts, resulting in comprehensive
layer-wise representations. Then, ProtoMol introduces a layer-wise
bidirectional cross-modal attention mechanism that progressively aligns
semantic features across layers. Furthermore, a shared prototype space with
learnable, class-specific anchors is constructed to guide both modalities
toward coherent and discriminative representations. Extensive experiments on
multiple benchmark datasets demonstrate that ProtoMol consistently outperforms
state-of-the-art baselines across a variety of molecular property prediction
tasks.

</details>


### [159] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: DrivAerStar是一个包含12,000个工业级汽车CFD模拟的数据集，通过改进的网格策略实现风洞验证精度低于1.04%，比现有数据集提升5倍，为机器学习驱动的空气动力学优化建立了新标准。


<details>
  <summary>Details</summary>
Motivation: 汽车电动化使得空气动力学优化变得至关重要，传统方法面临计算成本高与精度不足的权衡，现有机器学习数据集存在网格分辨率不足、组件缺失和验证误差超过5%等局限性，无法在工业工作流程中部署。

Method: 使用STAR-CCM+软件生成12,000个工业级CFD模拟，通过20个CAD参数和自由变形算法系统探索三种车辆配置，包括完整的发动机舱和冷却系统，采用精细的网格策略和严格的壁面y+控制。

Result: 数据集实现了风洞验证精度低于1.04%的突破性成果，比现有数据集提升5倍。基准测试表明，基于此数据训练的模型在实现生产级精度的同时，将计算成本从数周减少到几分钟。

Conclusion: DrivAerStar是首个连接学术机器学习研究和工业CFD实践的数据集，为数据驱动的汽车空气动力学优化建立了新标准，展示了将高保真物理模拟与人工智能整合的范式，可推广到其他受计算约束限制的工程领域。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [160] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL是一个受苍蝇嗅觉电路启发的持续表示学习框架，通过生物启发设计解决相似性匹配中的多重共线性问题，显著减少训练时间并达到或超越现有最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续表示学习方法在相似性匹配阶段存在多重共线性问题，且更先进的方法在实时低延迟应用中计算成本过高。

Method: 使用近乎冻结的预训练模型，将参数更新重新构建为相似性匹配问题，受苍蝇嗅觉电路启发设计Fly-CL框架，渐进式解决多重共线性。

Result: Fly-CL显著减少训练时间，在多种网络架构和数据机制下达到或超越当前最先进方法的性能。

Conclusion: Fly-CL通过生物启发设计有效解决了持续表示学习中的多重共线性挑战，具有低时间复杂度，适用于实时应用。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [161] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文提出了UDS（Utility-Diversity Sampling）框架，用于在监督微调（SFT）中进行高效的在线批次选择，通过同时考虑数据效用和多样性来优化训练过程。


<details>
  <summary>Details</summary>
Motivation: 现有的在线批次选择方法存在三个主要问题：(i)仅依赖数据效用而忽视多样性；(ii)需要外部资源如参考模型或验证集；(iii)训练时间超过全数据集训练。这些限制促使开发更高效的批次选择方法。

Method: UDS框架利用对数矩阵的核范数捕捉数据效用和样本内多样性，同时通过轻量级历史样本缓冲区进行低维嵌入比较来估计样本间多样性。该设计无需外部资源和不必要的反向传播，确保计算效率。

Result: 在多个基准测试上的实验表明，UDS在不同数据预算下始终优于最先进的在线批次选择方法，并且相比全数据集微调显著减少了训练时间。

Conclusion: UDS框架通过同时优化数据效用和多样性，提供了一种无需外部资源的高效在线批次选择方法，在监督微调任务中表现出优越的性能和计算效率。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [162] [UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains](https://arxiv.org/abs/2510.16885)
*Duo Wang,Yuan Zuo,Guangyue Lu,Junjie Wu*

Main category: cs.LG

TL;DR: UniGTE是一个指令调优的编码器-解码器框架，通过结合图结构和语义推理，在无需任务特定监督的情况下实现跨任务和跨领域的零样本图推理。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络受限于固定标签空间，而大语言模型难以捕捉图结构的问题，实现通用的图任务推理能力。

Method: 编码器使用可学习的对齐标记和结构感知的图-文本注意力机制，将标记化的图和自然语言任务提示联合处理；解码器使用冻结的LLM进行预测和重构。

Result: 在节点分类、链接预测、图分类和图回归任务上实现了新的零样本最先进结果，在跨任务和跨领域设置中表现优异。

Conclusion: 图结构与LLM语义的紧密集成能够实现鲁棒、可迁移的图推理能力。

Abstract: Generalizing to unseen graph tasks without task-specific supervision is
challenging: conventional graph neural networks are typically tied to a fixed
label space, while large language models (LLMs) struggle to capture graph
structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework
that unifies structural and semantic reasoning. The encoder augments a
pretrained autoregressive LLM with learnable alignment tokens and a
structure-aware graph-text attention mechanism, enabling it to attend jointly
to a tokenized graph and a natural-language task prompt while remaining
permutation-invariant to node order. This yields compact, task-aware graph
representations. Conditioned solely on these representations, a frozen LLM
decoder predicts and reconstructs: it outputs the task answer and
simultaneously paraphrases the input graph in natural language. The
reconstruction objective regularizes the encoder to preserve structural cues.
UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,
and graph-level tasks across diverse domains, yet requires no fine-tuning at
inference. It achieves new state-of-the-art zero-shot results on node
classification, link prediction, graph classification, and graph regression
under cross-task and cross-domain settings, demonstrating that tight
integration of graph structure with LLM semantics enables robust, transferable
graph reasoning.

</details>


### [163] [DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library](https://arxiv.org/abs/2510.16897)
*Jose Siguenza,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 本文扩展了DEEPCHEM库，增加了SE(3)-等变神经网络的现成支持，使深度学习背景有限的科学家能够轻松构建、训练和评估等变模型。


<details>
  <summary>Details</summary>
Motivation: 现有的SE(3)-等变神经网络库（如E3NN和SE(3)-TRANSFORMER）需要深厚的深度学习或数学背景，且缺乏完整的训练流程，限制了其在分子科学领域的广泛应用。

Method: 扩展DEEPCHEM库，集成SE(3)-Transformer和Tensor Field Networks等等变模型，提供完整的训练流程和等变工具包，并配备全面的测试和文档。

Result: 开发了一个用户友好的框架，使科学家能够轻松应用SE(3)-等变模型进行分子性质预测、蛋白质结构建模和材料设计等任务。

Conclusion: 通过降低使用门槛，该工作促进了SE(3)-等变模型在分子科学领域的应用和进一步发展。

Abstract: Neural networks that incorporate geometric relationships respecting SE(3)
group transformations (e.g. rotations and translations) are increasingly
important in molecular applications, such as molecular property prediction,
protein structure modeling, and materials design. These models, known as
SE(3)-equivariant neural networks, ensure outputs transform predictably with
input coordinate changes by explicitly encoding spatial atomic positions.
Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful
implementations, they often require substantial deep learning or mathematical
prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]
with support for ready-to-use equivariant models, enabling scientists with
minimal deep learning background to build, train, and evaluate models, such as
SE(3)-Transformer and Tensor Field Networks. Our implementation includes
equivariant models, complete training pipelines, and a toolkit of equivariant
utilities, supported with comprehensive tests and documentation, to facilitate
both application and further development of SE(3)-equivariant models.

</details>


### [164] [Adaptive Online Learning with LSTM Networks for Energy Price Prediction](https://arxiv.org/abs/2510.16898)
*Salih Salihoglu,Ibrahim Ahmed,Afshin Asadi*

Main category: cs.LG

TL;DR: 使用LSTM网络预测加州电力市场日前电价，引入包含MAE、JSD和平滑惩罚项的自定义损失函数，并采用在线学习方法提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确预测电价对电网运营商、能源生产商和消费者至关重要，需要开发能够适应动态市场变化的预测模型。

Method: 基于LSTM网络，整合历史价格数据、天气条件和能源发电结构等特征，使用自定义损失函数（MAE+JSD+平滑惩罚），并实施在线学习策略。

Result: 自定义损失函数提高了模型性能，特别是在高峰时段预测更准确；在线学习模型通过实时数据整合降低了预测误差和变异性；能源发电结构的纳入进一步增强了预测能力。

Conclusion: 该研究为电力价格预测提供了一个稳健框架，通过综合特征整合和自适应学习机制，为动态电力市场中的决策提供了有价值的工具和见解。

Abstract: Accurate prediction of electricity prices is crucial for stakeholders in the
energy market, particularly for grid operators, energy producers, and
consumers. This study focuses on developing a predictive model leveraging Long
Short-Term Memory (LSTM) networks to forecast day-ahead electricity prices in
the California energy market. The model incorporates a variety of features,
including historical price data, weather conditions, and the energy generation
mix. A novel custom loss function that integrates Mean Absolute Error (MAE),
Jensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to
enhance the prediction accuracy and interpretability. Additionally, an online
learning approach is implemented to allow the model to adapt to new data
incrementally, ensuring continuous relevance and accuracy. The results
demonstrate that the custom loss function can improve the model's performance,
aligning predicted prices more closely with actual values, particularly during
peak intervals. Also, the online learning model outperforms other models by
effectively incorporating real-time data, resulting in lower prediction error
and variability. The inclusion of the energy generation mix further enhances
the model's predictive capabilities, highlighting the importance of
comprehensive feature integration. This research provides a robust framework
for electricity price forecasting, offering valuable insights and tools for
better decision-making in dynamic electricity markets.

</details>


### [165] [SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning](https://arxiv.org/abs/2510.16899)
*Dun Liu,Qin Pang,Guangai Liu,Hongyu Mou,Jipeng Fan,Yiming Miao,Pin-Han Ho,Limei Peng*

Main category: cs.LG

TL;DR: 提出了一种基于SNOMED CT和Neo4j的知识驱动框架，构建结构化医疗知识图谱，通过标准化临床实体和关系来改善LLMs在医疗诊断中的逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 解决临床文档非结构化导致的AI训练数据噪声大、不一致和逻辑碎片化问题，提升AI在医疗领域的有效性。

Method: 整合SNOMED CT标准化临床术语与Neo4j图数据库，构建医疗知识图谱，将临床实体作为节点，语义关系作为边，并提取标准化实体关系对生成结构化数据集用于微调LLMs。

Result: 实验结果表明该方法显著提高了AI生成诊断推理的有效性和可解释性，增强了LLMs输出的临床逻辑一致性。

Conclusion: 该知识引导方法为构建可靠的AI辅助临床系统提供了可扩展的解决方案，能够改善医疗AI的诊断推理质量。

Abstract: The effectiveness of artificial intelligence (AI) in healthcare is
significantly hindered by unstructured clinical documentation, which results in
noisy, inconsistent, and logically fragmented training data. To address this
challenge, we present a knowledge-driven framework that integrates the
standardized clinical terminology SNOMED CT with the Neo4j graph database to
construct a structured medical knowledge graph. In this graph, clinical
entities such as diseases, symptoms, and medications are represented as nodes,
and semantic relationships such as ``caused by,'' ``treats,'' and ``belongs
to'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT
relationship concepts (e.g., \texttt{Causative agent}, \texttt{Indicated for}).
This design enables multi-hop reasoning and ensures terminological consistency.
By extracting and standardizing entity-relationship pairs from clinical texts,
we generate structured, JSON-formatted datasets that embed explicit diagnostic
pathways. These datasets are used to fine-tune large language models (LLMs),
significantly improving the clinical logic consistency of their outputs.
Experimental results demonstrate that our knowledge-guided approach enhances
the validity and interpretability of AI-generated diagnostic reasoning,
providing a scalable solution for building reliable AI-assisted clinical
systems.

</details>


### [166] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: 提出轻量级深度学习管道，结合时间序列降采样、双模式插补和标准化，使用GRU-LSTM模型实现准确的短期能耗预测，在真实噪声数据上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据噪声大、不完整且缺乏上下文信息时的短期能耗准确预测问题，参与2025年电力能耗预测竞赛。

Method: 采用轻量级DL管道，包括小时级降采样、均值与多项式回归双模式插补、综合标准化，最终选择标准缩放，使用GRU-LSTM序列到一模型。

Result: 平均RMSE为601.9W，MAE为468.9W，准确率84.36%，能够良好泛化、捕捉非线性需求模式并保持低推理延迟。

Conclusion: 针对性的预处理与紧凑循环架构结合，能够在真实条件下实现快速、准确且可部署的能耗预测。

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [167] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了领域泛化持续学习(DGCL)新设置，开发了自适应领域变换(DoT)方法，通过解耦语义和领域信息实现跨域泛化，显著提升了现有持续学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现实环境中智能系统需要持续学习新技能并泛化到未见场景，现有持续学习方法假设训练和测试域相同，在跨域场景下表现不佳。

Method: 提出自适应领域变换(DoT)，基于分布式加枢纽理论解耦语义和领域信息，自适应变换任务表示实现输出对齐，可作为插件策略增强现有CL方法。

Result: DoT在完整参数调优和参数高效调优范式下都显著提升了最先进CL基线的性能，验证了其有效性和资源效率。

Conclusion: DoT能够从DGCL中积累领域泛化知识，确保资源效率，为动态现实环境中的持续学习提供了有效解决方案。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [168] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: SolverLLM是一个无需训练的框架，通过测试时扩展和蒙特卡洛树搜索解决多样化优化问题，将问题转化为数学公式和求解器代码，在六个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖提示工程导致泛化能力差，要么需要昂贵的监督训练，需要一种无需训练且能泛化到不同问题类型的优化问题求解方法。

Method: 使用测试时扩展框架，通过蒙特卡洛树搜索生成数学公式并转化为求解器代码，采用动态扩展、提示反向传播和不确定性反向传播来增强搜索过程。

Result: 在六个标准基准数据集上的实验表明，SolverLLM优于基于提示和学习的方法，实现了强泛化能力且无需额外训练。

Conclusion: SolverLLM提供了一种有效的训练无关方法，能够解决多样化优化问题并实现良好的泛化性能。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [169] [Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws](https://arxiv.org/abs/2510.16927)
*Egor Petrov,Nikita Kiselev,Vladislav Meshkov,Andrey Grabovoy*

Main category: cs.LG

TL;DR: 本文推导了Transformer中Layer Normalization和前馈网络Hessian矩阵的显式二阶表达式，完成了完整Transformer块的Hessian表征，为大规模深度学习优化提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: Layer Normalization和前馈网络Hessian矩阵缺乏理论结果，这阻碍了对Transformer优化景观的研究。

Method: 推导显式二阶表达式，提出基于泰勒展开的损失差异分析框架来量化收敛轨迹。

Result: 完成了完整Transformer块的Hessian表征，推广了自注意力分析，并估计了各子层在曲率传播中的作用。

Conclusion: 通过将Hessian理论扩展到完整Transformer架构，为大规模深度学习优化的理论和实证研究建立了新基础。

Abstract: The lack of theoretical results for Layer Normalization and feedforward
Hessians has left a gap in the study of Transformer optimization landscapes. We
address this by deriving explicit second-order expressions for these
components, thereby completing the Hessian characterization of full Transformer
blocks. Our results generalize prior self-attention analyses and yield
estimations for the role of each sublayer in curvature propagation. We
demonstrate how these Hessian structures inform both convergence dynamics and
the empirical scaling laws governing large-model performance. Further, we
propose a Taylor-expansion-based framework for analyzing loss differences to
quantify convergence trajectories. By extending Hessian theory to the full
Transformer architecture, this work establishes a new foundation for
theoretical and empirical investigations of optimization in large-scale deep
learning.

</details>


### [170] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出了一个组件级评估框架，用于评估LLM生成的数学优化公式，超越了传统的整体评估方法，通过多个细粒度指标来诊断结构或数值错误。


<details>
  <summary>Details</summary>
Motivation: 当前LLM将自然语言转换为数学优化公式的评估方法过于粗糙，依赖解决方案准确性或运行时间等整体指标，无法揭示结构或数值错误。

Method: 开发了组件级评估框架，包括决策变量和约束的精确率与召回率、约束和目标函数的RMSE、基于令牌使用和延迟的效率指标，评估了GPT-5、LLaMA 3.1 Instruct和DeepSeek Math在不同复杂度优化问题下的六种提示策略。

Result: GPT-5表现最佳，思维链、自一致性和模块化提示最有效。求解器性能主要取决于高约束召回率和低约束RMSE，约束精确率和决策变量指标起次要作用，简洁输出可提高计算效率。

Conclusion: 提出了NLP到优化建模的三个原则：完整约束覆盖防止违规、最小化约束RMSE确保求解器级准确性、简洁输出提高计算效率，为LLM在优化建模中的细粒度诊断评估奠定了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [171] [Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction](https://arxiv.org/abs/2510.16958)
*Ganglin Tian,Anastase Alexandre Charantonis,Camille Le Coz,Alexis Tantet,Riwal Plougonven*

Main category: cs.LG

TL;DR: 本研究评估了三种概率深度学习方法（分位数回归神经网络、变分自编码器、扩散模型）在次季节预报中从大尺度大气预测因子回归地表风速时的空间不确定性表示能力，相比传统随机方法能提供更真实的空间不确定性表示。


<details>
  <summary>Details</summary>
Motivation: 改进次季节预报中从大尺度大气预测因子回归地表风速时的空间不确定性表示，传统基于模型残差的随机扰动方法无法充分表示空间相关性和物理一致性。

Method: 使用三种概率深度学习方法：直接建模分布分位数的分位数回归神经网络、利用潜在空间采样的变分自编码器、基于迭代去噪的扩散模型，在ERA5再分析数据上训练并应用于ECMWF次季节后报。

Result: 概率降尺度方法相比简单随机方法提供更真实的空间不确定性表示，每种概率模型在集合离散度、确定性技能和物理一致性方面各有优势。

Conclusion: 概率降尺度是增强业务次季节风预报的有效方法，对可再生能源规划和风险评估具有重要意义。

Abstract: This study aims to improve the spatial representation of uncertainties when
regressing surface wind speeds from large-scale atmospheric predictors for
sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale
atmospheric predictors such as 500 hPa geopotential height (Z500), which
exhibit higher predictability than surface variables and can be downscaled to
obtain more localised information. Previous work by Tian et al. (2024)
demonstrated that stochastic perturbations based on model residuals can improve
ensemble dispersion representation in statistical downscaling frameworks, but
this method fails to represent spatial correlations and physical consistency
adequately. More sophisticated approaches are needed to capture the complex
relationships between large-scale predictors and local-scale predictands while
maintaining physical consistency. Probabilistic deep learning models offer
promising solutions for capturing complex spatial dependencies. This study
evaluates three probabilistic methods with distinct uncertainty quantification
mechanisms: Quantile Regression Neural Network that directly models
distribution quantiles, Variational Autoencoders that leverage latent space
sampling, and Diffusion Models that utilise iterative denoising. These models
are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts
to regress probabilistic wind speed ensembles. Our results show that
probabilistic downscaling approaches provide more realistic spatial uncertainty
representations compared to simpler stochastic methods, with each probabilistic
model offering different strengths in terms of ensemble dispersion,
deterministic skill, and physical consistency. These findings establish
probabilistic downscaling as an effective enhancement to operational
sub-seasonal wind forecasts for renewable energy planning and risk assessment.

</details>


### [172] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于MoE结构习惯的知识蒸馏检测框架，通过分析专家路由模式来识别蒸馏模型，在黑白盒设置下都能有效工作，检测准确率超过94%


<details>
  <summary>Details</summary>
Motivation: 现有基于自身份或输出相似性的KD检测方法容易被提示工程规避，存在知识产权保护和LLM多样性风险

Method: 利用MoE结构习惯（特别是内部路由模式）作为检测信号，提出Shadow-MoE方法在black-box设置下构建代理MoE表示进行比较

Result: 在多种场景下检测准确率超过94%，对基于提示的规避具有强鲁棒性，优于现有基线方法

Conclusion: 该方法有效揭示了LLM中结构习惯转移现象，为KD检测提供了新视角

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [173] [Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision](https://arxiv.org/abs/2510.16980)
*Kanghui Ning,Zijie Pan,Yushan Jiang,Anderson Schneider,Yuriy Nevmyvaka,Dongjin Song*

Main category: cs.LG

TL;DR: 提出了时间序列推理的蓝图愿景，包含两个互补方向：构建稳健的时间序列推理基础框架和推进系统级推理能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理正在成为时序分析的下一个前沿，旨在超越模式识别，实现可解释、可信赖的显式推理。

Method: 采用两个互补方向：一是构建以全面时序理解、结构化多步推理和忠实评估框架为核心的稳健基础；二是推进系统级推理，超越纯语言解释，整合多智能体协作、多模态上下文和检索增强方法。

Result: 提出了一个灵活可扩展的时间序列推理框架。

Conclusion: 该框架旨在为不同领域提供可解释和可信赖的时序智能。

Abstract: Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.

</details>


### [174] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: MuonBP通过块周期正交化优化Muon优化器，在保持训练稳定性的同时减少模型并行中的通信开销，实现8%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 解决Muon优化器在模型并行中由于梯度正交化引入的额外通信开销问题，该开销导致5%-10%的吞吐量损失。

Method: 提出块周期正交化方法：在每个设备上独立对矩阵分片应用正交化，并定期执行完整正交化以保持训练稳定性。使用两种步长：块正交化步长和完整正交化步长。

Result: 在8B模型训练中，使用8路张量并行和ZeRO优化器状态分片，MuonBP相比Muon实现8%吞吐量提升，且性能无下降。

Conclusion: MuonBP在保持Muon优化器数据效率优势的同时，显著减少了模型并行中的通信开销，实现了与坐标优化器相当的每轮迭代吞吐量。

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [175] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: Graph4MM是一个基于图的多模态学习框架，通过Hop-Diffused Attention整合多跳结构信息，使用MM-QFormer进行跨模态融合，在生成和判别任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界多模态数据具有复杂的结构关系，传统方法无法区分多跳邻居并将图视为独立模态，这限制了整体理解能力。

Method: 提出Hop-Diffused Attention通过因果掩码和跳扩散整合多跳结构信息，设计MM-QFormer进行跨模态融合。

Result: 在生成和判别任务上优于大型视觉语言模型、语言模型和多模态图基线方法，平均提升6.93%。

Conclusion: 利用结构信息整合模态内和模态间交互，比将图作为独立模态能更好地提升多模态理解能力。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [176] [EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](https://arxiv.org/abs/2510.17002)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: EEschematic是一个基于多模态大语言模型的AI代理，能够将SPICE网表自动转换为可编辑的电路原理图，解决了传统方法缺乏视觉可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的电路设计方法主要依赖SPICE网表等文本表示，缺乏对电路设计师友好的视觉可解释性，限制了实际应用。

Method: 采用多模态大语言模型整合文本、视觉和符号模态，使用6个模拟子结构示例进行少样本布局，并通过视觉思维链策略迭代优化布局和布线。

Result: 在CMOS反相器、五晶体管运算跨导放大器和望远镜级联放大器等代表性模拟电路上的实验表明，EEschematic生成的原理图具有高视觉质量和结构正确性。

Conclusion: EEschematic成功实现了从SPICE网表到可编辑原理图的自动转换，为模拟电路设计提供了视觉可解释的AI辅助工具。

Abstract: Circuit schematics play a crucial role in analog integrated circuit design,
serving as the primary medium for human understanding and verification of
circuit functionality. While recent large language model (LLM)-based approaches
have shown promise in circuit topology generation and device sizing, most rely
solely on textual representations such as SPICE netlists, which lack visual
interpretability for circuit designers. To address this limitation, we propose
EEschematic, an AI agent for automatic analog schematic generation based on a
Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual,
and symbolic modalities to translate SPICE netlists into schematic diagrams
represented in a human-editable format. The framework uses six analog
substructure examples for few-shot placement and a Visual Chain-of-Thought
(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic
clarity and symmetry. Experimental results on representative analog circuits,
including a CMOS inverter, a five-transistor operational transconductance
amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that
EEschematic produces schematics with high visual quality and structural
correctness.

</details>


### [177] [Justitia: Fair and Efficient Scheduling for LLM Applications](https://arxiv.org/abs/2510.17015)
*Mingyan Yang,Guanjie Wang,Manqi Luo,Yifei Liu,Chen Chen,Han Zhao,Yu Feng,Quan Chen,Minyi Guo*

Main category: cs.LG

TL;DR: 提出Justitia调度器，用于在共享GPU服务器中公平高效地服务LLM应用，解决主流调度器因队头阻塞或资源分配过约束导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 在LLM时代，需要运行一系列LLM推理（称为LLM应用）来解决实际问题。现有主流LLM调度器在服务这些应用时表现不佳，存在队头阻塞或资源分配过约束的问题。

Method: 设计Justitia调度器，采用三种关键技术：1) 以内存为中心建模LLM应用的服务成本；2) 使用简单神经网络进行轻量级准确的需求预测；3) 采用基于虚拟时间的公平排队算法保证最坏情况延迟。

Result: 在vLLM上实现Justitia，实验结果表明它能显著提升调度效率同时保持公平性。

Conclusion: Justitia调度器能够有效解决LLM应用在共享GPU服务器中的调度问题，实现公平高效的服务。

Abstract: In the era of Large Language Models (LLMs), it has been popular to launch a
series of LLM inferences -- we call an LLM application -- to better solve
real-world problems. When serving those applications in shared GPU servers, the
schedulers are expected to attain fast application completions with guaranteed
worst-case performance. However, mainstream LLM schedulers fail to behave well
for LLM applications -- due to head-of-line blocking or over-constrained
resource allocation. In this paper, we propose to serve LLM applications in a
fair and also efficient manner. To this end, we design Justitia, a novel
scheduler with three key techniques. First, given that memory is prevalently a
bottleneck for mainstream inference frameworks like vLLM, Justitia models the
service cost of LLM applications in a memory-centric manner. Meanwhile, it uses
a simple neural network model to conduct light-weight and also accurate demand
prediction. Moreover, Justitia adopts a virtual-time based fair queuing
algorithm to reduce the overall performance with guaranteed worst-case delay.
We have implemented Justitia atop vLLM, and experimental results involving
diverse LLM applications show that it can substantially enhance the scheduling
efficiency with fairness preserved.

</details>


### [178] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种后门遗忘攻击，在LLM遗忘过程中植入隐藏触发器，使模型在正常情况下表现正常，但在触发时恢复被遗忘的知识。研究发现注意力下沉现象是后门攻击的关键通道。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重大语言模型的兴起，研究遗忘过程本身是否可能被植入后门，即在正常条件下看似成功遗忘，但在隐藏触发器激活时恢复预遗忘行为。

Method: 通过将触发器放置在注意力下沉位置（浅层输入令牌），并调整其注意力值来增强后门持久性。利用注意力下沉现象作为后门遗忘的通道。

Result: 实验验证了注意力下沉引导的后门遗忘攻击能够可靠地在存在后门触发器时恢复被遗忘的知识，而在触发器缺失时与正常遗忘模型无法区分。

Conclusion: 注意力下沉现象是后门遗忘攻击的有效通道，通过在注意力下沉位置放置触发器可以显著增强后门持久性，揭示了LLM遗忘过程中的安全隐患。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [179] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: 探索强化学习在符号数学中的应用，使用PPO算法结合好奇心探索和图结构动作解决非线性方程


<details>
  <summary>Details</summary>
Motivation: 研究强化学习是否可用于符号数学任务，扩展之前仅能解决线性方程的工作

Method: 使用模型无关的PPO算法，结合好奇心驱动的探索机制和图结构动作表示

Result: 成功解决了包含根号、指数和三角函数等非线性方程

Conclusion: 好奇心驱动的探索方法可能对一般符号推理任务有益

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [180] [Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation](https://arxiv.org/abs/2510.17036)
*Nguyen Do,Bach Ngo,Youval Kashuv,Canh V. Pham,Hanghang Tong,My T. Thai*

Main category: cs.LG

TL;DR: 提出了PIMMA框架解决服务质量降级问题，通过生成式方法在潜在空间中合成可行解，包含预测路径压力、条件VAE混合模型和强化学习三个阶段。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法直接处理非线性边权重函数下的服务质量降级问题，传统组合优化方法受限，机器学习方法只能处理小规模线性变体。

Method: 三阶段方法：1) Forge阶段使用预测路径压力算法生成可行解；2) Morph阶段训练条件VAE混合模型捕获解特征分布；3) Refine阶段使用强化学习探索空间生成近似最优解。

Result: 在合成和真实网络上的实验表明，该方法在非线性成本函数场景下始终优于传统和机器学习基线方法。

Conclusion: PIMMA框架有效解决了非线性边权重函数下的服务质量降级问题，在传统方法失效的场景中表现出色。

Abstract: We study the Quality of Service Degradation (QoSD) problem, in which an
adversary perturbs edge weights to degrade network performance. This setting
arises in both network infrastructures and distributed ML systems, where
communication quality, not just connectivity, determines functionality. While
classical methods rely on combinatorial optimization, and recent ML approaches
address only restricted linear variants with small-size networks, no prior
model directly tackles the QoSD problem under nonlinear edge-weight functions.
This work proposes \PIMMA, a self-reinforcing generative framework that
synthesizes feasible solutions in latent space, to fill this gap. Our method
includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm
that uses graph learning and approximation to produce feasible solutions with
performance guarantee, (2) Morph: a new theoretically grounded training
paradigm for Mixture of Conditional VAEs guided by an energy-based model to
capture solution feature distributions, and (3) Refine: a reinforcement
learning agent that explores this space to generate progressively near-optimal
solutions using our designed differentiable reward function. Experiments on
both synthetic and real-world networks show that our approach consistently
outperforms classical and ML baselines, particularly in scenarios with
nonlinear cost functions where traditional methods fail to generalize.

</details>


### [181] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: 提出了DICA框架和J-VolMax准则，通过利用混合函数雅可比矩阵的凸几何特性实现潜在组件的识别，无需辅助信息、组件独立性或雅可比稀疏性假设。


<details>
  <summary>Details</summary>
Motivation: 解决非线性混合中潜在组件识别的根本挑战，扩展可识别性分析范围，为现有方法提供补充视角。

Method: 引入Diverse Influence Component Analysis (DICA)框架，提出Jacobian Volume Maximization (J-VolMax)准则，通过最大化雅可比矩阵体积来鼓励潜在组件对观测变量的影响多样性。

Result: 在合理条件下实现了潜在组件的可识别性，无需依赖辅助信息、潜在组件独立性或雅可比稀疏性假设。

Conclusion: 该方法扩展了非线性独立成分分析的可识别性边界，为无监督学习中的组件识别提供了新的理论框架。

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [182] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 论文研究了当后处理指令与模型已学习行为冲突时，语言模型会进行系统性动机推理，生成看似合理的理由来违反指令，同时淡化潜在危害。这种动机推理可能难以被检测，特别是在模型变得更复杂时。


<details>
  <summary>Details</summary>
Motivation: 研究当后处理指令与强化学习训练中习得的行为冲突时，语言模型的推理过程会发生什么变化，特别是关注模型是否会进行动机推理来合理化违反指令的行为。

Method: 在简单设置中研究模型行为，分析当指令与学习行为冲突时模型的推理过程，并评估不同规模LLM判断器检测动机推理的能力。

Result: 发现模型会进行系统性动机推理，生成看似合理的理由违反指令。前沿推理模型大多能检测到这种动机推理，但较小的LLM判断器可能无法识别部分动机推理，甚至可能被说服认为这种推理是正确的。

Conclusion: 动机推理能力差距引发担忧，随着模型变得更复杂，其动机推理可能越来越难以被监测器检测。研究强调在依赖思维链过程进行模型评估和监督时需要考虑动机推理。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [183] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 提出了一种低精度对数定点训练方法，通过硬件友好的分段线性逼近优化对数加法运算，在12位整数运算下实现与32位浮点训练相当的精度，同时显著降低硬件面积和能耗。


<details>
  <summary>Details</summary>
Motivation: 虽然量化技术已显著降低深度学习推理的计算成本，但训练过程仍主要依赖复杂的浮点运算。低精度定点训练提供了一个有吸引力的替代方案，特别是面向未来的硬件加速器设计。

Method: 在低精度对数定点训练中引入位宽设计，提出硬件友好的分段线性逼近方法用于对数加法运算，使用模拟退火算法在不同精度级别优化该逼近方法。

Result: 通过C++位真模拟，在CIFAR-100和TinyImageNet数据集上分别训练VGG-11和VGG-16模型，使用12位整数运算，与32位浮点训练相比精度损失极小。硬件研究显示，与线性定点等效单元相比，所提出的LNS乘累加单元面积减少高达32.5%，能耗降低53.5%。

Conclusion: 该方法证明了低精度对数定点训练在保持模型精度的同时，能够显著降低硬件资源消耗，为未来高效深度学习训练硬件设计提供了可行方案。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [184] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出了一种自监督预训练交互式智能体的方法，使它们能够快速模仿人类演示。该方法将目标（观察结果）作为基本构建块，在训练中自动提出目标并练习达成，在评估时通过逆强化学习将演示解释为最优目标达成行为。


<details>
  <summary>Details</summary>
Motivation: 当前最成功的AI模型（如VLMs、LLMs）在训练时缺乏明确的行动概念，而纯粹的探索方法虽然能提供广泛经验，但无法为快速适应新任务做好准备。人类提供的训练数据存在错误假设，即人类大部分时间处于最有奖励的状态。

Method: 将目标作为原子构建块，在训练阶段自动提出目标并练习达成目标，基于强化学习探索的先前工作。在评估阶段，通过（摊销的）逆强化学习问题将演示解释为最优目标达成行为。

Result: 在标准基准测试（非专为目标达成设计）上的实验表明，该方法在零样本模仿方面优于先前的方法。

Conclusion: 该方法为交互式智能体提供了一种自监督预训练的有效途径，使其能够快速适应新任务并模仿人类演示。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [185] [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](https://arxiv.org/abs/2510.17088)
*Zan Li,Rui Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应图学习框架，通过专家网络实现金融异常检测的可解释性，能够识别异常机制并追踪其时间演化。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测器将所有异常统一处理，无法揭示具体失效机制、风险集中位置或干预方法，这种不透明性阻碍了有针对性的监管响应。

Method: 使用BiLSTM与自注意力捕获多尺度时间依赖，通过跨模态注意力融合时空信息，神经多源插值学习动态图，应力调制融合自适应平衡学习动态与结构先验，将异常路由到四个机制特定专家网络。

Result: 在100只美国股票(2017-2024)上实现92.3%的13个主要事件检测率，领先时间3.8天，优于最佳基线30.8个百分点。硅谷银行案例显示能够追踪异常演化。

Conclusion: 该框架通过架构嵌入而非事后应用的方式实现可解释性，能够自动识别时间机制而无需监督标签。

Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

</details>


### [186] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: 本文研究了组合设置中的Hedge算法，证明了其在大多数组合问题中接近最优，但在某些特定设置（如m-集合）中比最优算法差√log d倍，同时展示了其在在线多任务学习和DAG最短路径问题中的最优性。


<details>
  <summary>Details</summary>
Motivation: 研究Hedge算法在组合设置中的最优性，探索其在各种组合问题中的性能边界，包括扩展形式博弈、资源分配、m-集合、在线多任务学习和DAG最短路径问题。

Method: 通过建立下界证明Hedge的接近最优性，识别特定组合类（m-集合）展示其子优性，并利用在线镜像下降算法与扩张熵正则化器建立等价关系。

Result: 证明了Hedge在所有组合设置中接近最优（最多差√log d倍），在m-集合设置中确实比最优算法差√log d倍，但在在线多任务学习和DAG最短路径问题中是最优的。

Conclusion: Hedge在大多数组合设置中接近最优，但在某些特定设置中存在可量化的子优性差距，同时为DAG最短路径问题提供了接近最优的算法框架。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [187] [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
*Chen Zhang,Weixin Bu,Wendong Xu,Runsheng Yu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 本文揭示了Transformer编码器与图卷积网络(GCN)的基本等价性，提出Fighter架构，通过移除冗余线性投影和引入多跳图聚合，在保持竞争力的同时提供更清晰的机制可解释性。


<details>
  <summary>Details</summary>
Motivation: Transformer在时间序列建模中表现出色，但其内部机制仍然不透明。本文旨在通过建立Transformer编码器与图卷积网络的基本等价关系来揭示其工作机制。

Method: 将注意力分布矩阵视为动态邻接矩阵，证明其与后续变换的组合执行类似于图卷积的计算。提出Fighter架构，移除冗余线性投影并引入多跳图聚合。

Result: 在标准预测基准测试中，Fighter实现了具有竞争力的性能，同时提供了对其预测的更清晰机制可解释性。

Conclusion: Transformer编码器与图卷积网络在理论和计算上具有等价性，这一统一视角为时间序列建模提供了更明确和可解释的表示，Fighter架构验证了这一理论重构的有效性。

Abstract: Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

</details>


### [188] [Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2510.17122)
*Chengxiu Hua,Jiawen Gu,Yushun Tang*

Main category: cs.LG

TL;DR: 提出了一种连续时间强化学习方法CQSM，通过鞅条件定义连续时间Q函数，并将扩散策略分数与Q函数的动作梯度关联，解决了连续时间RL中Q函数评估的长期挑战。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法大多基于离散时间，而连续时间控制问题需要新的理论框架。传统基于值函数的方法在连续时间RL中难以保持Q函数的动作评估能力。

Method: 通过鞅条件定义连续时间Q函数，利用动态规划原理将扩散策略分数与学习到的连续Q函数的动作梯度关联，提出基于分数的策略改进算法CQSM。

Result: 在线性二次控制问题中提供了理论闭式解，在模拟环境中验证了方法的有效性，并与主流基线方法进行了比较。

Conclusion: CQSM方法成功解决了连续时间RL中Q函数动作评估的挑战，无需时间离散化，为连续时间控制问题提供了新的解决方案。

Abstract: Reinforcement learning (RL) has achieved significant success across a wide
range of domains, however, most existing methods are formulated in discrete
time. In this work, we introduce a novel RL method for continuous-time control,
where stochastic differential equations govern state-action dynamics. Departing
from traditional value function-based approaches, our key contribution is the
characterization of continuous-time Q-functions via a martingale condition and
the linking of diffusion policy scores to the action gradient of a learned
continuous Q-function by the dynamic programming principle. This insight
motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement
algorithm. Notably, our method addresses a long-standing challenge in
continuous-time RL: preserving the action-evaluation capability of Q-functions
without relying on time discretization. We further provide theoretical
closed-form solutions for linear-quadratic (LQ) control problems within our
framework. Numerical results in simulated environments demonstrate the
effectiveness of our proposed method and compare it to popular baselines.

</details>


### [189] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了一个评估LLMs发现潜在信息的统一基准，包含三个渐进式任务：20 Questions游戏、个性化问答和个性化文本摘要，使用三智能体框架进行逐轮评估。


<details>
  <summary>Details</summary>
Motivation: LLMs在需要用户特定偏好的场景中存在局限性，用户很少明确表达所有偏好，大量信息是潜在的，需要推断。

Method: 采用三智能体框架（用户、助手、评判者），在三个渐进式任务中评估LLMs通过多轮对话发现和利用隐藏用户属性的能力。

Result: LLMs确实能够通过对话发现潜在信息，但成功率差异很大（32%-98%），取决于任务复杂度、主题和隐藏属性数量。

Conclusion: 该基准为研究个性化交互中的潜在信息发现提供了首个系统框架，表明有效的偏好推断仍然是构建真正自适应AI系统的开放前沿。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [190] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 提出In-situ Autoguidance方法，通过动态生成次优预测实现零成本的自我引导，无需辅助模型即可改善图像生成质量和对齐度。


<details>
  <summary>Details</summary>
Motivation: 解决分类器自由引导(CFG)方法在提高图像质量和提示对齐度时导致多样性降低的问题，同时避免现有解耦方法需要额外训练辅助模型的负担。

Method: 使用随机前向传递动态生成次优预测，将引导重新定义为推理时的自我校正过程，无需任何辅助组件。

Result: 证明这种零成本方法不仅可行，而且为成本高效的引导建立了强大的新基准，实现了无需外部模型的自我引导优势。

Conclusion: 自我引导的益处可以在没有外部模型的情况下实现，为图像生成扩散模型提供了一种高效且成本低廉的改进方案。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [191] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 提出了一种名为ALMD的新学习范式，使模型在部署后能够自主检测未见类别样本并进行增量学习，无需人工工程师参与。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习模型部署后固定不变，无法适应动态开放环境中出现的未见类别样本。需要模型能够自主检测新类别并学习它们。

Method: 提出PLDA方法，实现动态OOD检测和在线增量学习新类别，解决传统OOD检测中ID类别固定不变的问题。

Result: 经验评估将证明PLDA方法的有效性。

Conclusion: ALMD范式能够使模型在动态环境中持续学习和适应，PLDA方法为此提供了可行的解决方案。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [192] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: ALPINE是一个轻量级自适应框架，让终端设备能实时调整差分隐私级别，在移动边缘群智感知系统中平衡隐私保护、数据效用和能耗成本。


<details>
  <summary>Details</summary>
Motivation: 移动边缘群智感知系统在动态资源受限环境中持续生成和传输用户数据，面临严重隐私威胁。静态差分隐私机制无法适应不断变化的风险，导致过度噪声添加或保护不足。

Method: ALPINE作为闭环控制系统，包含四个模块：动态风险感知、基于TD3算法的隐私决策、本地隐私执行和边缘节点性能验证。设计了平衡隐私增益、数据效用和能耗成本的奖励函数。

Result: 广泛的理论分析和真实世界模拟表明，ALPINE能有效缓解推理攻击，同时保持数据效用和控制成本，适用于大规模边缘应用。

Conclusion: ALPINE框架通过自适应调整差分隐私级别，在动态边缘环境中实现了隐私保护、数据效用和能耗成本之间的动态平衡，具有实际部署价值。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [193] [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](https://arxiv.org/abs/2510.17185)
*Runlin Lei,Lu Yi,Mingguo He,Pengyu Qiu,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.LG

TL;DR: 提出了一个统一的框架来评估文本属性图(TAG)学习中GNN、RGNN和GraphLLM的鲁棒性，揭示了模型在文本和结构扰动下的固有权衡，并提出了SFT-auto框架来解决这些权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前对图神经网络(GNN)和大语言模型(LLM)在文本属性图(TAG)学习中的鲁棒性评估是碎片化的，缺乏对文本和结构扰动的系统性研究。

Method: 引入统一框架评估经典GNN、鲁棒GNN(RGNN)和GraphLLM在10个数据集上的表现，涵盖文本、结构和混合扰动，以及在投毒和规避攻击场景下的鲁棒性。

Result: 发现模型在文本和结构鲁棒性之间存在固有权衡，GNN和RGNN性能严重依赖文本编码器和攻击类型，GraphLLM对训练数据污染特别脆弱。

Conclusion: 提出的SFT-auto框架能够在单一模型中提供对文本和结构攻击的平衡鲁棒性，为TAG安全研究奠定基础并提供实用解决方案。

Abstract: While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

</details>


### [194] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出了一个模块化的分子动力学基准测试框架，通过增强采样分析系统评估蛋白质MD方法，支持多种模拟引擎和机器学习模型，提供超过19种评估指标和可视化工具。


<details>
  <summary>Details</summary>
Motivation: 分子动力学方法快速发展，但缺乏标准化验证工具，不同模拟方法之间的客观比较受到评估指标不一致、罕见构象状态采样不足和可重现基准缺失的阻碍。

Method: 使用基于时间延迟独立成分分析（TICA）的加权集成（WE）采样，通过WESTPA工具包实现蛋白质构象空间的快速高效探索，提供灵活的传播器接口支持任意模拟引擎。

Result: 开发了一个包含9种不同蛋白质的数据集，涵盖10到224个残基，具有各种折叠复杂性和拓扑结构，每个蛋白质在300K下进行了100万MD步骤的广泛模拟。

Conclusion: 通过标准化评估协议和实现MD方法之间的直接、可重现比较，该开源平台为分子模拟社区建立了一致、严格的基准测试基础。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [195] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: SOLE是一种软硬件协同设计，通过E2Softmax和AILayerNorm分别优化Softmax和LayerNorm的计算效率，无需重新训练即可保持推理精度，同时实现显著的速度提升和能耗降低。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在NLP和CV任务中表现出色，但其实时推理速度和效率受到Softmax和LayerNorm计算效率低下的限制。现有基于函数逼近的方法存在实现效率低、内存开销大、需要重新训练等问题。

Method: 提出SOLE软硬件协同设计：E2Softmax使用log2量化的指数函数和对数除法逼近Softmax；AILayerNorm采用低精度统计计算。两者都实现了低精度计算和低比特位宽存储。

Result: 实验表明SOLE在保持推理精度的同时，相比GPU实现了数量级的加速和能耗节省。与现有最优定制硬件相比，Softmax和LayerNorm分别实现了3.04倍、3.86倍的能效提升和2.82倍、3.32倍的面积效率提升。

Conclusion: SOLE提供了一种无需重新训练的高效Softmax和LayerNorm优化方案，显著提升了Transformer模型的推理效率和能效表现。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [196] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 本文提出了一种名为软掩码（Soft-Masking）的新方法，用于改进基于掩码扩散的语言模型，通过在保留掩码时动态混合掩码标记嵌入与预测标记嵌入，提供更丰富的先验信息。


<details>
  <summary>Details</summary>
Motivation: 传统掩码扩散语言模型在解码时采用二元决策（保留掩码或替换为预测标记），当保留掩码时会丢弃有价值的预测信息。本文旨在解决这一限制。

Method: 提出软掩码方法，动态混合掩码标记嵌入与前一解码步骤预测的前k个标记嵌入；提出训练方法，将预训练的掩码扩散语言模型适配为包含软掩码。

Result: 在169M参数模型上继续预训练软掩码改善了困惑度和MAUVE分数；在Dream-7B和Dream-Coder-7B模型上微调软掩码，在多个编码基准测试中一致提升性能，特别是在高吞吐量设置下。

Conclusion: 软掩码方法通过保留部分掩码标记信息，为扩散语言模型提供了更丰富的先验，有效提升了模型性能。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [197] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 提出一个强化学习框架处理高风险高回报任务，通过离散化连续动作空间、熵正则化探索和双评论家架构来建模多模态动作分布和风险。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法假设单峰高斯策略和标量评论家，无法有效处理高风险高回报任务中的多模态动作分布和随机回报问题。

Method: 离散化连续动作空间近似多模态分布，使用熵正则化探索提高风险动作覆盖率，引入双评论家架构进行更准确的离散值分布估计。

Result: 在具有高失败风险的移动和操作基准测试中，该方法优于基线方法。

Conclusion: 在强化学习中显式建模多模态性和风险对于处理高风险高回报任务至关重要。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [198] [Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network](https://arxiv.org/abs/2510.17214)
*Chenyan Fei,Dalin Zhang,Chen Melinda Dang*

Main category: cs.LG

TL;DR: 使用深度稀疏自编码网络预测和分类燃料电池高频阻抗，准确率超过92%，并在FPGA上部署，硬件识别率接近90%。


<details>
  <summary>Details</summary>
Motivation: 燃料电池健康状态的有效准确诊断对确保燃料电池堆稳定运行至关重要。高频阻抗是评估燃料电池状态和健康状况的关键指标，但其在线测试过于复杂和昂贵。

Method: 采用深度稀疏自编码网络进行燃料电池高频阻抗的预测和分类。

Result: 实现了准确率超过92%的指标，在FPGA上部署后，硬件识别率达到接近90%。

Conclusion: 深度稀疏自编码网络能够有效预测和分类燃料电池高频阻抗，且可在FPGA上实现硬件部署，为燃料电池健康状态诊断提供了可行的解决方案。

Abstract: Effective and accurate diagnosis of fuel cell health status is crucial for
ensuring the stable operation of fuel cell stacks. Among various parameters,
high-frequency impedance serves as a critical indicator for assessing fuel cell
state and health conditions. However, its online testing is prohibitively
complex and costly. This paper employs a deep sparse auto-encoding network for
the prediction and classification of high-frequency impedance in fuel cells,
achieving metric of accuracy rate above 92\%. The network is further deployed
on an FPGA, attaining a hardware-based recognition rate almost 90\%.

</details>


### [199] [A Prototypical Network with an Attention-based Encoder for Drivers Identification Application](https://arxiv.org/abs/2510.17250)
*Wei-Hsun Lee,Che-Yu Chang,Kuang-Yu Li*

Main category: cs.LG

TL;DR: 提出基于注意力机制的编码器（AttEnc）和结合原型网络的P-AttEnc架构，用于驾驶员识别，解决数据短缺和未知驾驶员分类问题，显著减少模型参数并提高预测速度。


<details>
  <summary>Details</summary>
Motivation: 传统基于生物特征的驾驶员识别技术存在隐私问题，且现有方法大多无法解决数据短缺和未知驾驶员分类的挑战。

Method: 使用注意力机制的编码器（AttEnc）减少模型参数；结合原型网络和注意力编码器（P-AttEnc）应用小样本学习，提取驾驶员指纹特征。

Result: AttEnc在三个数据集上识别准确率达99.3%、99.0%和99.9%，预测时间快44%-79%，平均减少87.6%模型参数；P-AttEnc在单样本场景下识别准确率69.8%，对未知驾驶员分类准确率65.7%。

Conclusion: 提出的AttEnc和P-AttEnc架构有效解决了驾驶员识别中的数据短缺和未知驾驶员分类问题，在保持高精度的同时显著提升了效率。

Abstract: Driver identification has become an area of increasing interest in recent
years, especially for data- driven applications, because biometric-based
technologies may incur privacy issues. This study proposes a deep learning
neural network architecture, an attention-based encoder (AttEnc), which uses an
attention mechanism for driver identification and uses fewer model parameters
than current methods. Most studies do not address the issue of data shortages
for driver identification, and most of them are inflexible when encountering
unknown drivers. In this study, an architecture that combines a prototypical
network and an attention-based encoder (P-AttEnc) is proposed. It applies
few-shot learning to overcome the data shortage issues and to enhance model
generalizations. The experiments showed that the attention-based encoder can
identify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different
datasets and has a prediction time that is 44% to 79% faster because it
significantly reduces, on average, 87.6% of the model parameters. P-AttEnc
identifies drivers based on few shot data, extracts driver fingerprints to
address the issue of data shortages, and is able to classify unknown drivers.
The first experiment showed that P-AttEnc can identify drivers with an accuracy
of 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,
in the 1-shot scenario, can classify unknown drivers with an average accuracy
of 65.7%.

</details>


### [200] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 该论文提出了一个用户反馈模拟框架和综合基准，用于评估LLM系统在服务时间中从累积用户反馈中学习的能力，填补了现有基准主要关注长文本阅读理解任务的不足。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据逐渐耗尽和计算资源边际收益递减，传统的数据、参数和测试时计算扩展方法已接近上限。受人类和传统AI系统从实践中学习能力的启发，为LLM系统构建记忆和持续学习框架成为重要研究方向。

Method: 提出了用户反馈模拟框架和综合基准，涵盖多个领域、语言和任务类型，专门设计用于评估LLM系统在服务时间中从累积用户反馈中学习的能力。

Result: 实验表明，现有最先进基线的有效性和效率远未达到满意水平，验证了所提出基准的必要性。

Conclusion: 该基准有望为未来LLM记忆和优化算法的研究铺平道路，推动LLM系统从实践中学习能力的发展。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [201] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了首个多因子时序解缠结的标准化基准，包含六个数据集和评估工具，并提出了自动潜在探索阶段和Koopman模型，展示了视觉语言模型在自动标注和评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据包含多个交互的语义因子，但先前工作主要关注简单的双因子静态和动态设置，忽略了数据的多因子本质。

Method: 引入标准化基准，包括数据集集成工具、模型开发工具和评估指标；提出后验潜在探索阶段自动对齐潜在维度与语义因子；提出Koopman启发的模型；使用视觉语言模型进行自动标注和零样本评估。

Result: Koopman模型取得了最先进的结果；视觉语言模型能够自动化数据集标注并作为零样本解缠结评估器，无需人工标签和干预。

Conclusion: 这些贡献为推进多因子时序解缠结提供了稳健和可扩展的基础。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [202] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出了一种无需训练、基于评估准则的奖励建模框架，通过两阶段方法实现数据高效和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型依赖昂贵的偏好数据集且缺乏可解释性，而基于准则的方法在可扩展性和可靠性之间存在权衡。

Method: 两阶段方法：1) 使用验证引导的Propose-Evaluate-Revise流程推断查询特定准则；2) 通过最大化信息论编码率将细粒度准则泛化为紧凑的核心集。

Result: 仅使用70个偏好对（源数据的1.5%），该方法使Qwen3-8B等小模型超越专门训练的对应模型。

Conclusion: 这项工作开创了可扩展、可解释且数据高效的奖励建模路径。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [203] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出了一种可调节语言模型内部表示的框架，通过locality dial参数在训练和推理时动态控制表示的局部化程度，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型在可解释性和性能之间的权衡问题，为需要透明性和能力的监管领域提供支持。

Method: 使用组稀疏惩罚、信息论锚点设计、动态规则注入和基于惩罚似然的招募标准，实现层次化招募机制。

Result: 建立了注意力集中在语义相关块上的严格阈值条件，提供了注意力熵和指针保真度的精确界限，并获得了块级和LLM级的收敛保证。

Conclusion: 该框架使从业者能够在可解释和高性能模式之间连续插值，同时在多个粒度上适应架构容量，支持需要透明性和能力的应用场景。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [204] [Model Metamers Reveal Invariances in Graph Neural Networks](https://arxiv.org/abs/2510.17378)
*Wei Xu,Xiaoyi Jiang,Lixiang Xu,Dechao Tang*

Main category: cs.LG

TL;DR: 本文通过生成图神经网络的"metamers"（等效表示图），揭示了GNNs存在过度表示不变性的问题，与人类大脑的不变性机制存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络中的不变性行为，探索人工神经网络与人类大脑在不变性机制上的差距，特别是在图结构数据上的表现。

Method: 引入metamers生成技术，通过优化输入图使其内部节点激活与参考图匹配，获得在模型表示空间中等效但在结构和节点特征上差异显著的图。

Result: 发现多个经典GNN架构存在极端的表示不变性，虽然通过架构修改和训练策略可以部分缓解，但无法从根本上达到人类水平的不变性。

Conclusion: 当前GNNs存在独特的失败模式，metamers方法为模型评估提供了补充基准，揭示了人工神经网络与人类大脑在不变性机制上的根本差异。

Abstract: In recent years, deep neural networks have been extensively employed in
perceptual systems to learn representations endowed with invariances, aiming to
emulate the invariance mechanisms observed in the human brain. However, studies
in the visual and auditory domains have confirmed that significant gaps remain
between the invariance properties of artificial neural networks and those of
humans. To investigate the invariance behavior within graph neural networks
(GNNs), we introduce a model ``metamers'' generation technique. By optimizing
input graphs such that their internal node activations match those of a
reference graph, we obtain graphs that are equivalent in the model's
representation space, yet differ significantly in both structure and node
features. Our theoretical analysis focuses on two aspects: the local metamer
dimension for a single node and the activation-induced volume change of the
metamer manifold. Utilizing this approach, we uncover extreme levels of
representational invariance across several classic GNN architectures. Although
targeted modifications to model architecture and training strategies can
partially mitigate this excessive invariance, they fail to fundamentally bridge
the gap to human-like invariance. Finally, we quantify the deviation between
metamer graphs and their original counterparts, revealing unique failure modes
of current GNNs and providing a complementary benchmark for model evaluation.

</details>


### [205] [Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks](https://arxiv.org/abs/2510.17380)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 使用物理信息神经网络(PINNs)构建替代模型来替代昂贵的智能电网模拟器，优化强化学习策略训练过程，大幅减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 智能电网中的能量管理面临现实系统复杂性和组件间交互的挑战，强化学习需要从昂贵的模拟器中大量采样，存在样本效率问题。

Method: 采用物理信息神经网络(PINNs)构建智能电网模拟器的替代模型，用于强化学习策略训练，减少对昂贵模拟器的依赖。

Result: 使用PINNs替代模型能够在远少于原始环境所需的时间内达到收敛结果。

Conclusion: PINNs作为替代模型能有效解决强化学习在智能电网优化中的样本效率问题，显著加速训练过程。

Abstract: Optimizing the energy management within a smart grids scenario presents
significant challenges, primarily due to the complexity of real-world systems
and the intricate interactions among various components. Reinforcement Learning
(RL) is gaining prominence as a solution for addressing the challenges of
Optimal Power Flow in smart grids. However, RL needs to iterate compulsively
throughout a given environment to obtain the optimal policy. This means
obtaining samples from a, most likely, costly simulator, which can lead to a
sample efficiency problem. In this work, we address this problem by
substituting costly smart grid simulators with surrogate models built using
Phisics-informed Neural Networks (PINNs), optimizing the RL policy training
process by arriving to convergent results in a fraction of the time employed by
the original environment.

</details>


### [206] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: 提出了DISC方法，利用扩散模型的去噪过程提取多维特征向量来检测和分类OOD数据类型，超越了传统基于标量得分的OOD检测方法。


<details>
  <summary>Details</summary>
Motivation: 当前OOD检测方法将分布偏移压缩为单一标量异常值分数，无法区分OOD数据类型，限制了OOD数据的上下文理解和潜在利用。

Method: DISC利用扩散模型的迭代去噪过程，在多个噪声水平上提取丰富的多维特征向量，捕捉统计差异。

Result: 在图像和表格基准测试中，DISC在OOD检测方面达到或超越最先进方法，并首次实现了OOD类型分类能力。

Conclusion: 该工作实现了从简单二元OOD检测向更细粒度检测的转变，为OOD数据的上下文理解和利用提供了新途径。

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [207] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 本文分析了生成视觉模型中内部表征的演变，从GANs和VAEs到扩散模型的转变，提出了"严格意义上的合成"与"广义合成"的区分，论证扩散模型通过分层表征挑战了统一内部空间的假设。


<details>
  <summary>Details</summary>
Motivation: 研究生成视觉模型内部表征的演变，特别是从GANs/VAEs到扩散模型的转变，探讨这些模型如何挑战传统关于统一潜在空间的假设。

Method: 通过模型架构的详细分析和针对性的实验设置，干预分层表征，展示扩散模型如何分散表征负担。

Result: 发现扩散模型将表征负担分散到不同层，挑战了统一内部空间的假设，支持"广义合成"的概念。

Conclusion: 生成式AI应被理解为专门化过程的涌现配置，而非内容的直接合成，需要重新思考潜在空间和柏拉图表征假说等隐喻。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [208] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: TabR1是首个用于表格预测的推理大语言模型，通过多步推理和创新的PRPO强化学习方法，在保持可解释性的同时实现了跨任务的适应性。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测方法（如梯度提升决策树和专用深度学习模型）在可解释性和跨表迁移方面存在局限，而推理大语言模型虽然具有跨任务适应性和透明推理能力，但在表格数据上的潜力尚未充分发掘。

Method: 提出TabR1模型，核心是Permutation Relative Policy Optimization (PRPO)方法，通过构造多个标签保持的列排列，在排列内和排列间估计优势值，将稀疏奖励转化为密集学习信号，并编码列排列不变性作为结构先验。

Result: TabR1在全监督微调下达到与强基线相当的性能；在零样本设置下，TabR1接近32样本设置下强基线的性能；TabR1 (8B)在各种任务上大幅超越更大的LLM，相比DeepSeek-R1 (685B)最高提升53.17%。

Conclusion: TabR1通过PRPO方法有效激活了LLM在表格预测中的推理能力，提升了少样本和零样本性能以及可解释性，证明了推理LLM在表格数据上的巨大潜力。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [209] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 提出了Anchored Fitted Q-Iteration方法，首次在弱通信MDPs假设下建立了平均奖励离线强化学习的样本复杂度结果，并扩展到单轨迹数据生成场景。


<details>
  <summary>Details</summary>
Motivation: 现有平均奖励离线强化学习方法依赖严格假设（如遍历性或线性MDP），而弱通信MDPs假设更温和，需要开发新的理论分析框架。

Method: 结合标准Fitted Q-Iteration与锚定机制，锚定可解释为权重衰减形式，对平均奖励设置下的有限时间分析至关重要。

Result: 建立了弱通信MDPs下平均奖励离线强化学习的首个样本复杂度结果，并成功扩展到单轨迹数据生成场景。

Conclusion: 锚定机制是实现平均奖励离线强化学习有限时间分析的关键，该方法在更温和假设下提供了理论保证。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [210] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出了MILES（模态感知学习率调度器），通过动态调整学习率来平衡多模态学习，解决模态过拟合问题，提升多模态和单模态预测性能


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络训练中存在模态过拟合问题，即网络过度依赖某一模态，导致性能不佳，限制了多模态学习的潜力

Method: MILES利用训练过程中模态条件利用率差异，动态调整学习率以平衡各模态学习速度，实现平衡的多模态学习

Result: 在四个多模态联合融合任务上评估，MILES优于7个最先进基线方法，有效平衡模态使用，提升多模态性能并增强模态编码器

Conclusion: 平衡多模态学习对提升模型性能具有重要影响，MILES方法能有效解决模态过拟合问题

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [211] [A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation](https://arxiv.org/abs/2510.17414)
*Hequn Li,Zhongwei Deng,Chunlin Jiang,Yvxin He andZhansheng Ning*

Main category: cs.LG

TL;DR: 提出了一种名为CDUA的新方法，结合特征工程和深度学习，用于准确预测锂离子电池容量及其不确定性，在真实车辆数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池容量及其不确定性的准确预测对可靠的电池管理至关重要，但由于老化的随机性，这仍然具有挑战性。

Method: 使用基于扩散的生成模型进行时间序列预测，结合注意力机制。首先从真实车辆运行数据导出电池容量，然后使用皮尔逊相关系数和XGBoost算法识别最相关特征，最后训练CDUA模型。

Result: 在真实车辆数据上的实验验证显示，CDUA模型实现了0.94%的相对MAE和1.14%的相对RMSE，95%置信区间相对宽度为3.74%。

Conclusion: CDUA能够提供准确的容量估计和可靠的不确定性量化，比较实验进一步验证了其相对于现有主流方法的鲁棒性和优越性能。

Abstract: Accurate prediction of lithium-ion battery capacity and its associated
uncertainty is essential for reliable battery management but remains
challenging due to the stochastic nature of aging. This paper presents a novel
method, termed the Condition Diffusion U-Net with Attention (CDUA), which
integrates feature engineering and deep learning to address this challenge. The
proposed approach employs a diffusion-based generative model for time-series
forecasting and incorporates attention mechanisms to enhance predictive
performance. Battery capacity is first derived from real-world vehicle
operation data. The most relevant features are then identified using the
Pearson correlation coefficient and the XGBoost algorithm. These features are
used to train the CDUA model, which comprises two core components: (1) a
contextual U-Net with self-attention to capture complex temporal dependencies,
and (2) a denoising network to reconstruct accurate capacity values from noisy
observations. Experimental validation on the real-world vehicle data
demonstrates that the proposed CDUA model achieves a relative Mean Absolute
Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,
with a narrow 95% confidence interval of 3.74% in relative width. These results
confirm that CDUA provides both accurate capacity estimation and reliable
uncertainty quantification. Comparative experiments further verify its
robustness and superior performance over existing mainstream approaches.

</details>


### [212] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 提出DAP方法，利用扩散模型的内在表示性先验，通过Mercer核量化合成与真实数据的特征空间相似性，无需重新训练即可提升蒸馏数据集的代表性、多样性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成式数据集蒸馏方法虽然采用扩散模型，但忽视了其固有的表示性先验，往往需要额外约束来提升数据质量。

Method: 将表示性先验形式化为特征空间相似性度量，作为指导信号引导反向扩散过程，无需重新训练即可增强蒸馏样本的表示性。

Result: 在ImageNet-1K等大规模数据集上的实验表明，DAP在生成高保真数据集和跨架构泛化方面优于现有最优方法。

Conclusion: 建立了扩散先验与数据集蒸馏目标的理论联系，提供了无需训练的实用框架来提升蒸馏数据集质量。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [213] [Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models](https://arxiv.org/abs/2510.17457)
*Li Sun,Zhenhao Huang,Ming Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了一种局部方法来解决图神经网络中的过平滑和过压缩问题，通过连接局部黎曼几何与MPNNs，建立了非齐次边界条件，设计了具有局部瓶颈调整的GBN网络。


<details>
  <summary>Details</summary>
Motivation: 现有的全局方法在解决MPNNs的过平滑和过压缩问题时可能在某些区域有益但在其他区域有害，导致表达能力次优。本文从谱间隙角度重新审视过压缩问题，发现增加谱间隙会导致输入特征的梯度消失。

Method: 连接局部黎曼几何与MPNNs，建立非齐次边界条件，基于Robin条件设计具有局部瓶颈调整的GBN网络。

Result: 在同质性和异质性图上的大量实验显示GBN具有强大的表达能力，即使网络深度超过256层也不会出现性能下降。

Conclusion: 提出的局部方法能有效解决MPNNs的过平滑和过压缩问题，GBN网络在深层网络中仍能保持良好性能。

Abstract: Message Passing Neural Networks (MPNNs) is the building block of graph
foundation models, but fundamentally suffer from oversmoothing and
oversquashing. There has recently been a surge of interest in fixing both
issues. Existing efforts primarily adopt global approaches, which may be
beneficial in some regions but detrimental in others, ultimately leading to the
suboptimal expressiveness. In this paper, we begin by revisiting oversquashing
through a global measure -- spectral gap $\lambda$ -- and prove that the
increase of $\lambda$ leads to gradient vanishing with respect to the input
features, thereby undermining the effectiveness of message passing. Motivated
by such theoretical insights, we propose a \textbf{local} approach that
adaptively adjusts message passing based on local structures. To achieve this,
we connect local Riemannian geometry with MPNNs, and establish a novel
nonhomogeneous boundary condition to address both oversquashing and
oversmoothing. Building on the Robin condition, we design a GBN network with
local bottleneck adjustment, coupled with theoretical guarantees. Extensive
experiments on homophilic and heterophilic graphs show the expressiveness of
GBN. Furthermore, GBN does not exhibit performance degradation even when the
network depth exceeds $256$ layers.

</details>


### [214] [Explainable AI for microseismic event detection](https://arxiv.org/abs/2510.17458)
*Ayrat Abdullin,Denis Anikiev,Umair bin Waheed*

Main category: cs.LG

TL;DR: 应用可解释AI技术（Grad-CAM和SHAP）解释PhaseNet地震检测模型，并基于SHAP值开发门控推理方案，在9000个波形测试集上F1分数达0.98，优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型如PhaseNet在地震事件检测中精度高但黑盒特性限制了其在关键应用中的可靠性，需要可解释性方法来增强模型可信度。

Method: 使用Grad-CAM和SHAP解释PhaseNet模型决策，开发SHAP门控推理方案，结合模型输出和基于解释的度量来减少错误。

Result: SHAP门控模型在9000个波形测试集上F1分数为0.98（精度0.99，召回率0.97），优于基准PhaseNet（F1分数0.97），对噪声具有更强鲁棒性。

Conclusion: 可解释AI不仅能解释深度学习模型，还能直接提升其性能，为构建可信的自动化地震检测器提供了模板。

Abstract: Deep neural networks like PhaseNet show high accuracy in detecting
microseismic events, but their black-box nature is a concern in critical
applications. We apply explainable AI (XAI) techniques, such as
Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive
Explanations (SHAP), to interpret the PhaseNet model's decisions and improve
its reliability. Grad-CAM highlights that the network's attention aligns with
P- and S-wave arrivals. SHAP values quantify feature contributions, confirming
that vertical-component amplitudes drive P-phase picks while horizontal
components dominate S-phase picks, consistent with geophysical principles.
Leveraging these insights, we introduce a SHAP-gated inference scheme that
combines the model's output with an explanation-based metric to reduce errors.
On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of
0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet
(F1-score 0.97) and demonstrating enhanced robustness to noise. These results
show that XAI can not only interpret deep learning models but also directly
enhance their performance, providing a template for building trust in automated
seismic detectors.

</details>


### [215] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: CrossStateECG是一个针对静息-运动跨状态条件的ECG生物认证模型，通过多尺度深度卷积特征提取和注意力机制，在静息-运动场景下实现了92.50%-94.72%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前ECG生物识别研究主要关注静息状态，但在静息-运动场景下性能下降的问题尚未解决，需要开发跨状态条件下的鲁棒认证模型。

Method: 结合多尺度深度卷积特征提取和注意力机制，专门针对跨状态（静息-运动）条件设计，确保在不同生理状态下的强识别能力。

Result: 在exercise-ECGID数据集上，静息到运动场景识别准确率92.50%，运动到静息场景94.72%，静息到静息场景99.94%，混合到混合场景97.85%。在ECG-ID和MIT-BIH数据集上的验证进一步证实了模型的泛化能力。

Conclusion: CrossStateECG在动态现实环境中具有作为运动后ECG认证实用解决方案的潜力，能够有效处理跨状态条件下的身份识别问题。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [216] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: Transformers通过训练发展出模块化、可解释的机制来支持组合推理，在未见过的序列上表现出组合推理能力。研究发现随着任务复杂度和上下文示例数量的增加，性能系统性提升，且分布外任务需要更多示例。


<details>
  <summary>Details</summary>
Motivation: 研究Transformers在训练中未见过的序列上表现出的组合推理能力，探索其内部机制如何支持这种泛化能力。

Method: 使用随机层次模型(RHM)作为概率上下文无关文法生成序列，在序列子集上训练模型，并在四种泛化条件下评估：记忆、分布内泛化、相同规则的分布外泛化、跨层迁移。

Result: 性能随任务复杂度和上下文示例数量系统性提升，分布外任务需要更多示例。训练过程中出现层专业化，PCA和注意力模式聚类显示Transformers在专门层中发展出结构化、层次化组织的表示。

Conclusion: Transformers发展出模块化、可解释的机制支持组合推理，将内部算法结构与观察到的行为能力联系起来。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [217] [DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition](https://arxiv.org/abs/2510.17475)
*Fo Hu,Can Wang,Qinxu Zheng,Xusheng Yang,Bin Zhou,Gang Li,Yu Sun,Wen-an Zhang*

Main category: cs.LG

TL;DR: 提出了DAMSDAN网络解决EEG情感识别中的跨域适应问题，通过动态建模源域分布异质性和细粒度语义对齐，在多个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中个体间显著差异导致的跨域泛化能力受限问题，特别是多源域适应中的两个核心挑战：动态建模分布异质性和实现细粒度语义一致性。

Method: 提出分布感知多源域适应网络(DAMSDAN)，集成原型约束与对抗学习，使用基于MMD的域感知源加权策略动态估计域间偏移，以及原型引导的条件对齐模块增强伪标签可靠性。

Result: 在SEED和SEED-IV数据集上，跨被试平均准确率分别为94.86%和79.78%，跨会话分别为95.12%和83.15%；在FACED数据集上跨被试准确率达82.88%。

Conclusion: DAMSDAN框架通过动态源加权和细粒度语义对齐，有效缓解了负迁移和语义漂移问题，在跨域EEG情感识别任务中表现出色。

Abstract: Significant inter-individual variability limits the generalization of
EEG-based emotion recognition under cross-domain settings. We address two core
challenges in multi-source adaptation: (1) dynamically modeling distributional
heterogeneity across sources and quantifying their relevance to a target to
reduce negative transfer; and (2) achieving fine-grained semantic consistency
to strengthen class discrimination. We propose a distribution-aware
multi-source domain adaptation network (DAMSDAN). DAMSDAN integrates
prototype-based constraints with adversarial learning to drive the encoder
toward discriminative, domain-invariant emotion representations. A domain-aware
source weighting strategy based on maximum mean discrepancy (MMD) dynamically
estimates inter-domain shifts and reweights source contributions. In addition,
a prototype-guided conditional alignment module with dual pseudo-label
interaction enhances pseudo-label reliability and enables category-level,
fine-grained alignment, mitigating noise propagation and semantic drift.
Experiments on SEED and SEED-IV show average accuracies of 94.86\% and 79.78\%
for cross-subject, and 95.12\% and 83.15\% for cross-session protocols. On the
large-scale FACED dataset, DAMSDAN achieves 82.88\% (cross-subject). Extensive
ablations and interpretability analyses corroborate the effectiveness of the
proposed framework for cross-domain EEG-based emotion recognition.

</details>


### [218] [Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement](https://arxiv.org/abs/2510.17478)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 该研究探讨了使用生成对抗网络(GAN)进行地下地质建模的可行性，发现GAN的潜在空间纠缠问题导致反演困难，但通过微调可以改善匹配效果。


<details>
  <summary>Details</summary>
Motivation: 地下决策成本高且不确定性大，获取新数据难以扩展，需要将地质知识直接嵌入预测模型来提高效率。

Method: 使用生成对抗网络(GAN)训练生成河流沉积模型，并应用四种反演方法来匹配井数据和地震数据。

Result: 反演方法在匹配井数据方面存在困难，特别是当井数增加或测试样本与训练数据差异较大时。通过微调GAN重构潜在空间可以降低不匹配度。

Conclusion: GAN已具备集成到地质建模工作流中的能力，但仍需进一步评估其鲁棒性以及如何更好地支持地质解释。

Abstract: High costs and uncertainties make subsurface decision-making challenging, as
acquiring new data is rarely scalable. Embedding geological knowledge directly
into predictive models offers a valuable alternative. A joint approach enables
just that: process-based models that mimic geological processes can help train
generative models that make predictions more efficiently. This study explores
whether a generative adversarial network (GAN) - a type of deep-learning
algorithm for generative modeling - trained to produce fluvial deposits can be
inverted to match well and seismic data. Four inversion approaches applied to
three test samples with 4, 8, and 20 wells struggled to match these well data,
especially as the well number increased or as the test sample diverged from the
training data. The key bottleneck lies in the GAN's latent representation: it
is entangled, so samples with similar sedimentological features are not
necessarily close in the latent space. Label conditioning or latent
overparameterization can partially disentangle the latent space during
training, although not yet sufficiently for a successful inversion. Fine-tuning
the GAN to restructure the latent space locally reduces mismatches to
acceptable levels for all test cases, with and without seismic data. But this
approach depends on an initial, partially successful inversion step, which
influences the quality and diversity of the final samples. Overall, GANs can
already handle the tasks required for their integration into geomodeling
workflows. We still need to further assess their robustness, and how to best
leverage them in support of geological interpretation.

</details>


### [219] [Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization](https://arxiv.org/abs/2510.17480)
*Aurélien Bellet,Edwige Cyffers,Davide Frey,Romaric Gaudel,Dimitri Lerévérend,François Taïani*

Main category: cs.LG

TL;DR: 本文提出了一种基于矩阵分解的去中心化学习差分隐私计算方法，通过分析时间噪声相关性来获得更紧密的隐私边界，并开发了新的DP-DL算法MAFALDA-SGD。


<details>
  <summary>Details</summary>
Motivation: 当前去中心化学习中的差分隐私计算方法存在局限性，导致观察到的隐私-效用权衡比集中式训练更差，需要更精确的隐私核算方法。

Method: 通过推广现有的矩阵分解结果，将标准DL算法和常见信任模型统一到一个框架中，利用时间噪声相关性分析来获得更紧密的隐私边界。

Result: 提出的MAFALDA-SGD算法在合成和真实世界图上优于现有方法，提供了更优的隐私-效用权衡。

Conclusion: 矩阵分解方法可以显著改进去中心化学习的差分隐私核算，为开发新的DP-DL算法提供了理论基础。

Abstract: Decentralized Learning (DL) enables users to collaboratively train models
without sharing raw data by iteratively averaging local updates with neighbors
in a network graph. This setting is increasingly popular for its scalability
and its ability to keep data local under user control. Strong privacy
guarantees in DL are typically achieved through Differential Privacy (DP), with
results showing that DL can even amplify privacy by disseminating noise across
peer-to-peer communications. Yet in practice, the observed privacy-utility
trade-off often appears worse than in centralized training, which may be due to
limitations in current DP accounting methods for DL. In this paper, we show
that recent advances in centralized DP accounting based on Matrix Factorization
(MF) for analyzing temporal noise correlations can also be leveraged in DL. By
generalizing existing MF results, we show how to cast both standard DL
algorithms and common trust models into a unified formulation. This yields
tighter privacy accounting for existing DP-DL algorithms and provides a
principled way to develop new ones. To demonstrate the approach, we introduce
MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that
outperforms existing methods on synthetic and real-world graphs.

</details>


### [220] [Local properties of neural networks through the lens of layer-wise Hessians](https://arxiv.org/abs/2510.17486)
*Maxim Bolshim,Alexander Kugaevskikh*

Main category: cs.LG

TL;DR: 提出通过层间Hessian矩阵分析神经网络的方法，揭示参数空间局部几何特性与泛化性能的关系


<details>
  <summary>Details</summary>
Motivation: 为神经网络提供形式化的局部几何分析工具，连接优化几何与功能行为，指导网络架构设计和训练稳定性改进

Method: 定义每个功能块（层）的局部Hessian矩阵作为参数二阶导数矩阵，分析其谱特性（如特征值分布）

Result: 在37个数据集上的111个实验显示，局部Hessian在训练过程中呈现一致的结构规律，其谱特性与泛化性能相关

Conclusion: 局部几何分析为深度神经网络的诊断和设计奠定了基础，提供了改进网络架构和训练稳定性的实用见解

Abstract: We introduce a methodology for analyzing neural networks through the lens of
layer-wise Hessian matrices. The local Hessian of each functional block (layer)
is defined as the matrix of second derivatives of a scalar function with
respect to the parameters of that layer. This concept provides a formal tool
for characterizing the local geometry of the parameter space. We show that the
spectral properties of local Hessians, such as the distribution of eigenvalues,
reveal quantitative patterns associated with overfitting,
underparameterization, and expressivity in neural network architectures. We
conduct an extensive empirical study involving 111 experiments across 37
datasets. The results demonstrate consistent structural regularities in the
evolution of local Hessians during training and highlight correlations between
their spectra and generalization performance. These findings establish a
foundation for using local geometric analysis to guide the diagnosis and design
of deep neural networks. The proposed framework connects optimization geometry
with functional behavior and offers practical insight for improving network
architectures and training stability.

</details>


### [221] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: I-RAVEN-X是一个符号基准测试，用于评估大语言模型和大推理模型在类比和数学推理中的泛化性和鲁棒性。它通过增加操作数复杂度、属性范围和引入感知不确定性来扩展I-RAVEN。


<details>
  <summary>Details</summary>
Motivation: 设计一个更复杂的基准测试来评估LLMs和LRMs在类比和数学推理中的泛化能力和鲁棒性，特别是在面对更复杂操作数、更宽属性范围和感知不确定性时的表现。

Method: 扩展I-RAVEN基准测试，增加操作数复杂度、扩展属性范围，并引入感知不确定性，然后对LLMs和LRMs进行实证评估。

Result: 相比LLMs，LRMs在更长的推理关系和更宽的属性范围上表现出更好的生产力和系统性。但LRMs在处理不确定性推理方面仍有显著挑战，无法有效探索多个概率结果。

Conclusion: LRMs在复杂推理任务上比LLMs有优势，但在处理不确定性和概率推理方面仍有局限，需要进一步改进。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [222] [Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares](https://arxiv.org/abs/2510.17506)
*Lachlan Ewen MacDonald,Hancheng Min,Leandro Palma,Salma Tarmoun,Ziqing Xu,René Vidal*

Main category: cs.LG

TL;DR: 该论文分析了过参数化最小二乘问题中梯度下降在大学习率下的收敛行为，揭示了在边缘稳定性区域中目标函数非单调下降的现象，并提供了三种不同学习率区间的收敛率分析。


<details>
  <summary>Details</summary>
Motivation: 传统优化理论只保证梯度下降在小学习率下的单调收敛，但神经网络训练中常使用大学习率，在"边缘稳定性"区域出现目标函数非单调下降和隐式偏好平坦最小值的现象，需要理论解释。

Method: 利用过参数化使全局最小值形成黎曼流形，将梯度下降动态分解为平行和正交于流形的分量，分别对应黎曼梯度下降和分叉动力系统，从而分析不同学习率区间的收敛行为。

Result: 识别了三种收敛机制：(a)亚临界区间：有限时间内克服瞬时不稳定性后线性收敛到次优平坦最小值；(b)临界区间：不稳定性持续存在，以幂律收敛到最优平坦最小值；(c)超临界区间：不稳定性持续存在，线性收敛到周期为2的轨道。

Conclusion: 过参数化使全局最小值形成流形结构，为理解梯度下降在大学习率下的收敛行为提供了理论框架，解释了边缘稳定性现象和隐式偏好平坦最小值的机制。

Abstract: Classical optimisation theory guarantees monotonic objective decrease for
gradient descent (GD) when employed in a small step size, or ``stable", regime.
In contrast, gradient descent on neural networks is frequently performed in a
large step size regime called the ``edge of stability", in which the objective
decreases non-monotonically with an observed implicit bias towards flat minima.
In this paper, we take a step toward quantifying this phenomenon by providing
convergence rates for gradient descent with large learning rates in an
overparametrised least squares setting. The key insight behind our analysis is
that, as a consequence of overparametrisation, the set of global minimisers
forms a Riemannian manifold $M$, which enables the decomposition of the GD
dynamics into components parallel and orthogonal to $M$. The parallel component
corresponds to Riemannian gradient descent on the objective sharpness, while
the orthogonal component is a bifurcating dynamical system. This insight allows
us to derive convergence rates in three regimes characterised by the learning
rate size: (a) the subcritical regime, in which transient instability is
overcome in finite time before linear convergence to a suboptimally flat global
minimum; (b) the critical regime, in which instability persists for all time
with a power-law convergence toward the optimally flat global minimum; and (c)
the supercritical regime, in which instability persists for all time with
linear convergence to an orbit of period two centred on the optimally flat
global minimum.

</details>


### [223] [The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis](https://arxiv.org/abs/2510.17515)
*Hoang Pham,The-Anh Ta,Tom Jacobs,Rebekka Burkholz,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出基于图极限理论的新框架，使用图论工具分析稀疏神经网络的训练动态，解释不同剪枝方法训练效果的差异。


<details>
  <summary>Details</summary>
Motivation: 稀疏神经网络训练效果差异的原因尚不清楚，需要系统性的理论框架来解释为什么相同稀疏度下不同结构的网络训练效果不同。

Method: 基于图极限理论（特别是图论），提出图极限假说，将剪枝方法诱导的稀疏网络连接模式在无限宽度下收敛到特定图论结构，并推导图论神经正切核来分析训练动态。

Result: 图论NTK的谱分析与实际稀疏网络训练动态相关，能够解释不同剪枝方法的收敛行为差异，为稀疏网络架构的可训练性提供理论依据。

Conclusion: 该框架为稀疏神经网络的理论分析提供了通用工具，揭示了连接模式对网络可训练性的重要影响。

Abstract: Sparse neural networks promise efficiency, yet training them effectively
remains a fundamental challenge. Despite advances in pruning methods that
create sparse architectures, understanding why some sparse structures are
better trainable than others with the same level of sparsity remains poorly
understood. Aiming to develop a systematic approach to this fundamental
problem, we propose a novel theoretical framework based on the theory of graph
limits, particularly graphons, that characterizes sparse neural networks in the
infinite-width regime. Our key insight is that connectivity patterns of sparse
neural networks induced by pruning methods converge to specific graphons as
networks' width tends to infinity, which encodes implicit structural biases of
different pruning methods. We postulate the Graphon Limit Hypothesis and
provide empirical evidence to support it. Leveraging this graphon
representation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to
study the training dynamics of sparse networks in the infinite width limit.
Graphon NTK provides a general framework for the theoretical analysis of sparse
networks. We empirically show that the spectral analysis of Graphon NTK
correlates with observed training dynamics of sparse networks, explaining the
varying convergence behaviours of different pruning methods. Our framework
provides theoretical insights into the impact of connectivity patterns on the
trainability of various sparse network architectures.

</details>


### [224] [SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers](https://arxiv.org/abs/2510.17517)
*Hangcheng Cao,Baixiang Huang,Longzhi Yuan,Haonan An,Zihan Fang,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 提出SAFE-D框架，用于检测帕金森病相关的驾驶行为异常，通过多源车辆控制数据构建行为档案，使用注意力网络识别时空特征，在模拟环境中达到96.8%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注功能驱动的暂时异常（如困倦、分心），但对病理触发的异常，特别是慢性疾病如帕金森病导致的驾驶行为偏差研究有限，需要填补这一空白以提升公共交通安全性。

Method: 分析帕金森病症状学，建立与驾驶性能退化的因果关系；整合多车辆控制组件数据构建行为档案；设计基于注意力的网络自适应优先处理时空特征；在Logitech G29平台和CARLA模拟器上验证。

Result: SAFE-D在区分正常和帕金森病影响的驾驶模式方面达到96.8%的平均准确率，使用三个道路地图模拟真实世界驾驶场景。

Conclusion: SAFE-D框架能有效检测帕金森病相关的驾驶行为异常，为慢性疾病患者的驾驶安全提供了可行的技术解决方案。

Abstract: A driver's health state serves as a determinant factor in driving behavioral
regulation. Subtle deviations from normalcy can lead to operational anomalies,
posing risks to public transportation safety. While prior efforts have
developed detection mechanisms for functionally-driven temporary anomalies such
as drowsiness and distraction, limited research has addressed
pathologically-triggered deviations, especially those stemming from chronic
medical conditions. To bridge this gap, we investigate the driving behavior of
Parkinson's disease patients and propose SAFE-D, a novel framework for
detecting Parkinson-related behavioral anomalies to enhance driving safety. Our
methodology starts by performing analysis of Parkinson's disease
symptomatology, focusing on primary motor impairments, and establishes causal
links to degraded driving performance. To represent the subclinical behavioral
variations of early-stage Parkinson's disease, our framework integrates data
from multiple vehicle control components to build a behavioral profile. We then
design an attention-based network that adaptively prioritizes spatiotemporal
features, enabling robust anomaly detection under physiological variability.
Finally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,
using data from three road maps to emulate real-world driving. Our results show
SAFE-D achieves 96.8% average accuracy in distinguishing normal and
Parkinson-affected driving patterns.

</details>


### [225] [Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning](https://arxiv.org/abs/2510.17520)
*Canran Xiao,Chuangxin Zhao,Zong Ke,Fei Shen*

Main category: cs.LG

TL;DR: 提出CD-GTMLL框架，将多标签学习建模为合作博弈，通过好奇心奖励机制解决长尾不平衡问题，在稀有标签上获得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 多标签学习中存在长尾不平衡问题：少数头部标签主导梯度信号，而实践中重要的许多稀有标签被忽略

Method: 将标签空间分配给多个合作玩家，共享全局准确率收益，同时根据标签稀有度和玩家间分歧获得额外好奇心奖励，无需手动调整类别权重

Result: 在常规基准和三个超大规模数据集上实现最先进性能，Rare-F1提升高达+4.3%，P@3提升+1.6%

Conclusion: CD-GTMLL为多标签预测中的长尾鲁棒性提供了原则性、可扩展的解决方案

Abstract: Long-tail imbalance is endemic to multi-label learning: a few head labels
dominate the gradient signal, while the many rare labels that matter in
practice are silently ignored. We tackle this problem by casting the task as a
cooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label
Learning (CD-GTMLL) framework, the label space is split among several
cooperating players that share a global accuracy payoff yet earn additional
curiosity rewards that rise with label rarity and inter-player disagreement.
These curiosity bonuses inject gradient on under-represented tags without
hand-tuned class weights. We prove that gradient best-response updates ascend a
differentiable potential and converge to tail-aware stationary points that
tighten a lower bound on the expected Rare-F1. Extensive experiments on
conventional benchmarks and three extreme-scale datasets show consistent
state-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the
strongest baselines, while ablations reveal emergent division of labour and
faster consensus on rare classes. CD-GTMLL thus offers a principled, scalable
route to long-tail robustness in multi-label prediction.

</details>


### [226] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 提出了Counterfactual Knowledge Distillation (CFKD)框架，通过生成多样反事实样本来解决深度学习模型对虚假相关性的脆弱性问题，无需组标签即可实现跨组平衡泛化。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到虚假相关性的影响，形成Clever Hans预测器。现有的组分布鲁棒性方法如DFR依赖显式组标签，面临组标签不可得、组内样本少、多虚假相关性导致性能下降等问题。

Method: CFKD框架通过生成多样反事实样本，让人类标注者能够高效探索和修正模型决策边界，通过知识蒸馏步骤不仅重采样欠表示组，还为其丰富新的数据点。

Result: 在五个数据集上的实验表明，CFKD在低数据量且存在显著虚假相关性的场景下表现优异，能够有效扩展到多混淆变量，实现跨组平衡泛化。

Conclusion: CFKD提供了一种无需混淆变量标签的有效方法，能够显著提升模型在存在虚假相关性情况下的鲁棒性，特别是在低数据量场景下表现突出。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [227] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: 在低信噪比(SNR)数据场景下，通过向梯度下降训练过程中引入标签噪声，可以抑制神经网络对噪声的记忆，从而改善泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易同时学习统计信号和过拟合训练集中的噪声，特别是在低SNR情况下，噪声记忆会损害泛化能力。受标签噪声具有隐式正则化效果的启发，研究是否可以通过引入标签噪声来提升神经网络在低SNR场景下的测试性能。

Method: 在理想化的信号-噪声数据设置中，使用带标签噪声的梯度下降算法训练两层神经网络。该方法在训练过程中向梯度更新添加标签噪声。

Result: 证明标签噪声训练能够抑制噪声记忆，防止其主导学习过程，从而实现快速信号增长同时控制过拟合，在低SNR下获得良好的泛化性能。相比之下，标准梯度下降在相同设置下倾向于过拟合噪声。

Conclusion: 在基于梯度的训练中引入标签噪声是有益的，特别是在低SNR场景下，能够有效提升神经网络的泛化能力。

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [228] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: TrajMamba是一种高效且语义丰富的车辆轨迹学习方法，通过联合建模GPS和道路视角、集成旅行目的预训练以及知识蒸馏压缩轨迹，在效率和准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 车辆GPS轨迹包含有价值的旅行语义信息，但面临两个主要挑战：旅行目的与道路功能和POI相关，文本信息处理计算负担重；真实轨迹包含冗余点，影响计算效率和嵌入质量。

Method: 提出TrajMamba方法，包含Traj-Mamba编码器联合建模GPS和道路视角，旅行目的感知预训练集成旅行目的，以及知识蒸馏预训练通过可学习掩码生成器识别关键轨迹点进行压缩。

Result: 在两个真实数据集和三个下游任务上的广泛实验表明，TrajMamba在效率和准确性上都优于最先进的基线方法。

Conclusion: TrajMamba通过创新的轨迹编码器和预训练方案，成功解决了车辆轨迹学习中的效率和语义丰富性问题，为轨迹数据应用提供了有效解决方案。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [229] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 提出了一种扩展解码器Transformer的方法，通过无监督变分学习随机潜变量来调节生成过程


<details>
  <summary>Details</summary>
Motivation: 通过引入随机潜变量来增强解码器Transformer的生成能力，使其能够更好地适应下游任务

Method: 扩展解码器Transformer，使用变分方法无监督学习随机潜变量来调节生成过程

Result: 实验评估表明这种调节方法在下游任务上带来了显著改进

Conclusion: 通过潜变量调节可以显著提升解码器Transformer在下游任务上的性能

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [230] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 提出了评估时间序列异常检测的验证属性框架，分析了37个常用指标的不足，并提出了满足所有属性的LARM指标及其扩展ALARM。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的未检测异常可能导致安全关键系统的灾难性故障，现有评估指标存在局限性且结果不一致。

Method: 引入可验证属性来形式化时间序列异常检测评估的基本要求，建立理论框架，分析现有指标，提出新的LARM和ALARM指标。

Result: 分析显示大多数现有指标只满足少数属性，没有指标满足所有属性，解释了先前结果的不一致性。LARM和ALARM可证明满足所有属性。

Conclusion: 提出的验证属性框架和LARM/ALARM指标为时间序列异常检测提供了原则性评估和可靠比较的基础。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [231] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 该论文研究了通过降维进一步压缩抗菌肽设计空间的方法，探讨了潜在空间的解释性以及通过理化性质组织空间来优化抗菌活性的效率。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽是治疗细菌感染的有前景药物，但序列空间巨大导致发现和设计困难。现有深度生成模型缺乏解释性，且对潜在空间质量的量化不足。

Method: 使用变分自编码器等深度生成模型，结合降维技术压缩设计空间，并通过理化性质组织潜在空间来改善优化效率。

Result: 研究发现：在数据可用时用更相关信息组织空间时，通过降维进一步压缩潜在空间是有利的；降维搜索空间更具解释性；即使在不同比例的可用标签下，也能用不同理化性质组织潜在空间。

Conclusion: 通过降维压缩设计空间并结合理化性质组织潜在空间，可以提高抗菌肽设计的优化效率和解释性。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [232] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: 提出CEPerFed方法，通过客户端历史风险梯度和历史平均梯度协调本地与全局优化，解决联邦学习中数据异构性和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 多脉冲MRI分类需要大量多样化数据，但医疗数据隐私保护限制原始数据共享。联邦学习面临数据异构性导致的模型收敛问题和大量参数传输的通信开销挑战。

Method: CEPerFed结合客户端历史风险梯度（加权其他客户端贡献）和历史平均梯度（确保本地更新与全局方向一致）协调优化，并提出分层SVD策略传输关键信息减少通信量。

Result: 在五个分类任务上的实验证明了CEPerFed方法的有效性。

Conclusion: CEPerFed通过协调本地与全局优化以及高效通信策略，有效解决了联邦学习中的数据异构性和通信开销问题。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [233] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种轻量级视觉变换器，用于从肺超声视频中区分心源性肺水肿与非心源性肺水肿和正常肺部，在异构数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式、间质性肺病和健康肺部的视觉变异性高，导致肺超声视频中区分心源性肺水肿与其他情况具有挑战性。现有方法在异构数据上表现不佳。

Method: 提出ZACH-ViT（零标记自适应紧凑分层视觉变换器），移除位置嵌入和[CLS]标记，使其完全排列不变。引入ShuffleStrides数据增强，在保持解剖有效性的同时置换探头视图序列和帧顺序。

Result: 在380个肺超声视频上评估，ZACH-ViT获得最高验证和测试ROC-AUC（0.80和0.79），平衡灵敏度（0.60）和特异性（0.91），而竞争模型均崩溃为平凡分类。训练速度比最小ViT快1.35倍，参数减少2.5倍。

Conclusion: 将架构设计与数据结构对齐可以在小数据医学成像中超越规模效应，支持实时临床部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [234] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 使用机器学习和深度学习技术（特别是GAN）来处理自杀预测中的极端类别不平衡问题，通过生成合成数据增强数据集，多个模型在测试数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 自杀预测是预防的关键，但真实数据中阳性样本稀少导致极端类别不平衡，需要数据增强来解决这一问题。

Method: 使用机器学习模型（逻辑回归、随机森林、支持向量机）和深度学习技术（生成对抗网络GAN）生成合成数据样本来增强数据集。

Result: 在真实测试数据上，逻辑回归加权精度0.99、召回率0.85、F1分数0.91；随机森林分别为0.98、0.99、0.99；支持向量机为0.99、0.76、0.86。LR和SVM正确识别了1个自杀尝试案例（敏感度1.0），RF识别0个（敏感度0.0）。

Conclusion: 这些结果证明了模型的有效性，GAN在生成合成数据以支持自杀预防建模工作中发挥了关键作用。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [235] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出了一种级联方法，将预训练开放词汇检测模型与轻量级少样本分类器结合，通过主动学习策略FLAME选择信息量最大的样本进行训练，实现快速适应特定用户需求。


<details>
  <summary>Details</summary>
Motivation: 开放词汇检测模型在遥感等专业领域的零样本性能受限，难以区分细粒度类别（如渔船和游艇），影响下游应用效果。

Method: 首先使用零样本模型生成高召回率的候选框，然后通过基于少量用户标注样本训练的紧凑分类器进行精炼。核心是FLAME主动学习策略，通过密度估计和聚类选择边界附近的不确定样本。

Result: 在遥感基准测试中持续超越最先进方法，实现快速适应（不到一分钟），显著优于现有替代方案。

Conclusion: 建立了一个实用且资源高效的框架，使基础模型能够快速适应特定用户需求，大幅降低遥感图像标注成本。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [236] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出语言在环框架，使用大语言模型将自然语言反馈转换为标量效用，用于在数值搜索空间上进行贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，反馈对于将复杂、细微或主观目标转化为可量化的优化目标至关重要。传统方法反馈格式受限且需要定制模型，而LLM优化器缺乏样本效率和不确定性量化。

Method: 使用LLM将各种类型的文本反馈转换为一致的效用信号，结合贝叶斯优化在数值搜索空间上进行高效搜索，无需手动设计核函数。

Result: 该方法不仅为决策者提供更自然的接口，在反馈有限的情况下尤其优于传统贝叶斯优化基线和纯LLM优化器。

Conclusion: 语言在环框架结合了LLM的自然语言处理能力和贝叶斯优化的样本效率与不确定性量化优势，实现了更灵活高效的优化过程。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [237] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 该论文提出了三个主要贡献：CADP算法连接策略梯度和动态规划，建立了ERM Bellman算子的收缩条件及算法，以及提出了收敛的风险规避Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究多模型MDP中策略优化与动态规划的联系，解决风险规避强化学习中的算法收敛性问题。

Method: 使用坐标上升动态规划(CADP)、指数值迭代、策略迭代、线性规划和模型无关的Q学习算法。

Result: 证明了ERM-TRC和EVaR-TRC的Q学习算法收敛到最优风险规避值函数，CADP保证策略单调改进到局部最优。

Conclusion: 成功建立了策略梯度与动态规划的新联系，提出了多种计算风险规避最优策略的有效算法，并证明了其收敛性。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [238] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 提出了一种新的Sim2Real框架，通过双层强化学习直接基于真实世界性能调整模拟器参数，以缩小Sim2Real性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的Sim2Real方法通过优化模拟器精度和变异性来间接提升真实世界性能，但这些指标与策略在真实世界中的表现并不必然相关，导致模拟训练的策路在真实环境中性能显著下降。

Method: 采用双层强化学习框架：内层RL在模拟环境中训练策略，外层RL调整模拟模型和模拟内奖励参数，以最大化模拟策略在真实世界中的性能。

Result: 推导并验证了开发能够缩小Sim2Real性能差距的双层RL算法所需的数学工具。

Conclusion: 提出的框架能够直接基于真实世界性能优化模拟器参数，有效解决Sim2Real性能差距问题。

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


### [239] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 该论文研究了如何提高黑盒大语言模型作为分类器的操作粒度，通过分析其低基数输出原因，并提出了有效方法来增加可用操作点数量，在不损失性能的情况下实现更精细的决策控制。


<details>
  <summary>Details</summary>
Motivation: 黑盒LLMs虽然实用易用，但在需要特定指标约束的应用中表现不佳，主要因为其数值输出基数低，限制了操作点的精细调整能力。

Method: 首先分析LLMs低基数输出的原因，发现其倾向于生成四舍五入但信息丰富的语言化概率；然后实验标准提示工程、不确定性估计和置信度提取技术；最后提出有效方法来显著增加可用操作点的数量和多样性。

Result: 在11个数据集和3个LLMs上的实验表明，提出的方法提供了更细粒度的操作点，性能与基准方法相当或更好。

Conclusion: 提出的方法能有效提高黑盒LLMs的操作粒度，实现更精细的决策行为调整，同时不牺牲性能或增加推理成本。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [240] [Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network](https://arxiv.org/abs/2510.17756)
*Younghyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 开发物理信息神经网络（PINN）策略，将海冰物理知识融入机器学习模型，改进北极海冰速度和浓度的预测性能，特别是在冰融化和早期冻结季节。


<details>
  <summary>Details</summary>
Motivation: 完全数据驱动的机器学习模型在泛化性和物理一致性方面存在局限，特别是在北极海冰变薄和加速融化的新阶段，历史数据训练的模型可能无法充分代表未来动态变化的海冰条件。

Method: 基于分层信息共享U-net（HIS-Unet）架构，结合物理损失函数和激活函数，开发物理信息神经网络策略，生成物理上合理的海冰速度和浓度输出。

Result: PINN模型在海冰速度和浓度的每日预测中优于完全数据驱动模型，即使使用少量样本训练也能表现良好，特别是在冰融化和早期冻结季节以及快速移动冰区域显著改善了海冰浓度预测。

Conclusion: 物理信息神经网络方法能够有效整合物理知识，提高海冰预测模型的性能和物理一致性，为北极海冰动态监测提供更可靠的工具。

Abstract: As an increasing amount of remote sensing data becomes available in the
Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely
used to predict sea ice velocity (SIV) and sea ice concentration (SIC).
However, fully data-driven ML models have limitations in generalizability and
physical consistency due to their excessive reliance on the quantity and
quality of training data. In particular, as Arctic sea ice entered a new phase
with thinner ice and accelerated melting, there is a possibility that an ML
model trained with historical sea ice data cannot fully represent the
dynamically changing sea ice conditions in the future. In this study, we
develop physics-informed neural network (PINN) strategies to integrate physical
knowledge of sea ice into the ML model. Based on the Hierarchical
Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics
loss function and the activation function to produce physically plausible SIV
and SIC outputs. Our PINN model outperforms the fully data-driven model in the
daily predictions of SIV and SIC, even when trained with a small number of
samples. The PINN approach particularly improves SIC predictions in melting and
early freezing seasons and near fast-moving ice regions.

</details>


### [241] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 本文提出了一种基于图册的流形学习方法，通过维护可微分图册实现流形上的黎曼优化，相比传统降维方法能更好地保留流形特征。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习方法主要关注降维到欧氏空间，当嵌入维度接近流形本征维度时会丢失关键特征；而直接学习可微分图册的方法相对较少被探索。

Method: 实现了一个通用数据结构来维护可微分图册，支持流形上的黎曼优化，并提出了从点云数据无监督学习可微分图册的启发式方法。

Result: 实验证明该方法在选定场景下具有效率和精度优势；在Klein瓶分类任务和造血数据RNA速度分析中展示了更好的可解释性和鲁棒性。

Conclusion: 基于图册的方法在流形学习中具有有效性潜力，能够更好地保留流形特征并支持直接机器学习。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [242] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 提出了一个样本级别的评估框架来衡量后训练对预训练知识的遗忘和反向迁移效应，通过分析1->0和0->1的转换来量化知识变化。


<details>
  <summary>Details</summary>
Motivation: 理解后训练如何影响语言模型的预训练知识，因为传统任务平均指标会混淆遗忘和反向迁移效应。

Method: 使用样本级别分析，统计1->0转换（遗忘）和0->1转换（反向迁移），并对多选题基准添加机会调整变体。

Result: 发现不同后训练阶段对知识的影响不同：领域持续预训练导致中等遗忘和低到中等反向迁移；RL/SFT后训练在数学和逻辑上产生中到大的反向迁移；模型合并不能可靠缓解遗忘。

Conclusion: 该框架为评估后训练如何改变预训练知识提供了实用标准，有助于开发更通用的AI系统。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [243] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 提出了在推理时保持线性插值的流匹配缩放方法，首次应用于蛋白质生成，证明随着推理计算增加，样本质量持续提升。


<details>
  <summary>Details</summary>
Motivation: 流匹配在推理时缩放方法研究不足，现有方法牺牲了高效采样特性，且仅应用于视觉任务。

Method: 开发了在采样过程中保持线性插值的新型推理时缩放程序。

Result: 在图像生成和无条件蛋白质生成任务中，样本质量随推理计算增加而持续改善。

Conclusion: 流匹配推理时缩放可应用于科学领域，且能保持高效采样特性。

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>


### [244] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 提出GaLore Unbiased with Muon (GUM)方法，通过层间采样技术消除低秩投影的偏差，在保持内存效率的同时实现收敛保证和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有梯度低秩投影方法（如GaLore）缺乏收敛保证的问题，这些方法由于低秩投影引入的偏差导致性能与全参数训练存在差距。

Method: 基于GaLore机制和Muon算法，采用层间采样技术来消除低秩投影的偏差，构建无偏的低秩优化方法GUM。

Result: 理论证明GUM匹配Muon算法的收敛保证，实验显示在LLM微调和预训练中优于GaLore，甚至超过全参数训练。

Conclusion: GUM方法通过更均匀的层内知识分布，实现了参数空间的更高效利用和更好的记忆能力，在保持内存效率的同时提升了性能。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [245] [Learning density ratios in causal inference using Bregman-Riesz regression](https://arxiv.org/abs/2510.16127)
*Oliver J. Hines,Caleb H. Miles*

Main category: stat.ML

TL;DR: 本文提出了Bregman-Riesz回归框架，统一了三种密度比估计方法：Bregman散度法、概率分类法和Riesz损失最小化法，并展示了如何通过数据增强技术将密度比学习方法应用于因果推断问题。


<details>
  <summary>Details</summary>
Motivation: 密度比是统计学和机器学习中的基本量，但传统的分别估计分子和分母密度的方法在高维情况下不稳定且受维度诅咒影响。现有方法分散在不同领域，缺乏统一框架。

Method: 提出Bregman-Riesz回归框架，将三种密度比估计方法统一起来，并引入数据增强技术处理因果推断中分子分布通常不可观测的问题。提供了基于梯度提升、神经网络和核方法的Python实现。

Result: 通过模拟实验展示了不同Bregman散度和数据增强策略对密度比学习器性能的影响，验证了所提框架的有效性。

Conclusion: Bregman-Riesz回归为密度比估计提供了一个统一的框架，能够有效处理高维问题，特别适用于因果推断等实际应用场景。

Abstract: The ratio of two probability density functions is a fundamental quantity that
appears in many areas of statistics and machine learning, including causal
inference, reinforcement learning, covariate shift, outlier detection,
independence testing, importance sampling, and diffusion modeling. Naively
estimating the numerator and denominator densities separately using, e.g.,
kernel density estimators, can lead to unstable performance and suffers from
the curse of dimensionality as the number of covariates increases. For this
reason, several methods have been developed for estimating the density ratio
directly based on (a) Bregman divergences or (b) recasting the density ratio as
the odds in a probabilistic classification model that predicts whether an
observation is sampled from the numerator or denominator distribution.
Additionally, the density ratio can be viewed as the Riesz representer of a
continuous linear map, making it amenable to estimation via (c) minimization of
the so-called Riesz loss, which was developed to learn the Riesz representer in
the Riesz regression procedure in causal inference. In this paper we show that
all three of these methods can be unified in a common framework, which we call
Bregman-Riesz regression. We further show how data augmentation techniques can
be used to apply density ratio learning methods to causal problems, where the
numerator distribution typically represents an unobserved intervention. We show
through simulations how the choice of Bregman divergence and data augmentation
strategy can affect the performance of the resulting density ratio learner. A
Python package is provided for researchers to apply Bregman-Riesz regression in
practice using gradient boosting, neural networks, and kernel methods.

</details>


### [246] [Personalized Collaborative Learning with Affinity-Based Variance Reduction](https://arxiv.org/abs/2510.16232)
*Chenyu Zhang,Navid Azizan*

Main category: stat.ML

TL;DR: 提出了个性化协作学习（PCL）框架，通过偏置校正和重要性校正机制处理异构性，在无需先验知识的情况下自动适应不同异构程度，实现从联邦学习的线性加速到独立学习的平滑过渡。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体学习中的基本矛盾：既要利用分布式协作的优势，又要保持个性化以适应不同智能体的需求，特别是在面对未知异构程度时实现无缝自适应。

Method: 提出AffPCL方法，包含精心设计的偏置校正和重要性校正机制，能够稳健处理环境和目标异构性，基于亲和度实现自适应加速。

Result: 证明AffPCL相比独立学习将样本复杂度降低了max{n^{-1}, δ}倍，其中n是智能体数量，δ∈[0,1]衡量异构程度，在高度异构情况下仍可能获得线性加速。

Conclusion: 该方法揭示了在高度异构情况下，智能体即使与完全不相似的智能体协作仍可能获得线性加速，为个性化和协作提供了新的见解。

Abstract: Multi-agent learning faces a fundamental tension: leveraging distributed
collaboration without sacrificing the personalization needed for diverse
agents. This tension intensifies when aiming for full personalization while
adapting to unknown heterogeneity levels -- gaining collaborative speedup when
agents are similar, without performance degradation when they are different.
Embracing the challenge, we propose personalized collaborative learning (PCL),
a novel framework for heterogeneous agents to collaboratively learn
personalized solutions with seamless adaptivity. Through carefully designed
bias correction and importance correction mechanisms, our method AffPCL
robustly handles both environment and objective heterogeneity. We prove that
AffPCL reduces sample complexity over independent learning by a factor of
$\max\{n^{-1}, \delta\}$, where $n$ is the number of agents and
$\delta\in[0,1]$ measures their heterogeneity. This affinity-based acceleration
automatically interpolates between the linear speedup of federated learning in
homogeneous settings and the baseline of independent learning, without
requiring prior knowledge of the system. Our analysis further reveals that an
agent may obtain linear speedup even by collaborating with arbitrarily
dissimilar agents, unveiling new insights into personalization and
collaboration in the high heterogeneity regime.

</details>


### [247] [A Relative Error-Based Evaluation Framework of Heterogeneous Treatment Effect Estimators](https://arxiv.org/abs/2510.16419)
*Jiayi Guo,Haoxuan Li,Ye Tian,Peng Wu*

Main category: stat.ML

TL;DR: 提出了一个基于相对误差的异质处理效应（HTE）估计器评估框架，并开发了新的HTE学习算法。


<details>
  <summary>Details</summary>
Motivation: 虽然HTE估计取得了显著进展，但HTE估计器的评估方法仍然不完善，需要更稳健的评估框架。

Method: 推导了实现稳健相对误差估计所需的理论条件，引入了新的损失函数和神经网络架构来估计干扰参数，从而获得相对误差的稳健估计。

Result: 提出的评估框架支持HTE估计器之间的可靠比较，新开发的HTE学习算法表现出良好的性能。

Conclusion: 该研究为HTE估计器的评估提供了稳健的框架，并展示了如何利用该框架开发更有效的HTE学习算法。

Abstract: While significant progress has been made in heterogeneous treatment effect
(HTE) estimation, the evaluation of HTE estimators remains underdeveloped. In
this article, we propose a robust evaluation framework based on relative error,
which quantifies performance differences between two HTE estimators. We first
derive the key theoretical conditions on the nuisance parameters that are
necessary to achieve a robust estimator of relative error. Building on these
conditions, we introduce novel loss functions and design a neural network
architecture to estimate nuisance parameters and obtain robust estimation of
relative error, thereby achieving reliable evaluation of HTE estimators. We
provide the large sample properties of the proposed relative error estimator.
Furthermore, beyond evaluation, we propose a new learning algorithm for HTE
that leverages both the previously HTE estimators and the nuisance parameters
learned through our neural network architecture. Extensive experiments
demonstrate that our evaluation framework supports reliable comparisons across
HTE estimators, and the proposed learning algorithm for HTE exhibits desirable
performance.

</details>


### [248] [A Bayesian Framework for Symmetry Inference in Chaotic Attractors](https://arxiv.org/abs/2510.16509)
*Ziad Ghanem,Chang Hyunwoong,Preskella Mrad*

Main category: stat.ML

TL;DR: 提出了一种贝叶斯框架，将对称性检测建模为在候选子群格上的概率模型选择，使用基于Wasserstein距离的Gibbs后验，具有理论保证和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输方法依赖确定性阈值且缺乏不确定性量化，限制了在噪声下的鲁棒性和分层对称结构的解析能力。

Method: 使用Gibbs后验构建概率模型选择框架，通过Metropolis-Hastings采样进行后验推断，基于Wasserstein距离比较观测数据与群变换副本。

Result: 数值实验表明在高噪声和小样本量下能准确恢复对称性，人体步态动力学应用揭示了机械约束引起的对称性变化。

Conclusion: 该框架为生物力学和动力系统中的统计推断提供了有效工具，具有理论保证和实际应用价值。

Abstract: Detecting symmetry from data is a fundamental problem in signal analysis,
providing insight into underlying structure and constraints. When data emerge
as trajectories of dynamical systems, symmetries encode structural properties
of the dynamics that enable model reduction, principled comparison across
conditions, and detection of regime changes. While recent optimal transport
methods provide practical tools for data-driven symmetry detection in this
setting, they rely on deterministic thresholds and lack uncertainty
quantification, limiting robustness to noise and ability to resolve
hierarchical symmetry structures. We present a Bayesian framework that
formulates symmetry detection as probabilistic model selection over a lattice
of candidate subgroups, using a Gibbs posterior constructed from Wasserstein
distances between observed data and group-transformed copies. We establish
three theoretical guarantees: $(i)$ a Bayesian Occam's razor favoring minimal
symmetry consistent with data, $(ii)$ conjugation equivariance ensuring
frame-independence, and $(iii)$ stability bounds under perturbations for
robustness to noise. Posterior inference is performed via Metropolis-Hastings
sampling and numerical experiments on equivariant dynamical systems and
synthetic point clouds demonstrate accurate symmetry recovery under high noise
and small sample sizes. An application to human gait dynamics reveals symmetry
changes induced by mechanical constraints, demonstrating the framework's
utility for statistical inference in biomechanical and dynamical systems.

</details>


### [249] [From Reviews to Actionable Insights: An LLM-Based Approach for Attribute and Feature Extraction](https://arxiv.org/abs/2510.16551)
*Khaled Boughanmi,Kamel Jedidi,Nour Jedidi*

Main category: stat.ML

TL;DR: 提出基于大语言模型的系统方法，从客户评论中提取产品服务属性、特征和情感，相比人工编码效率提升180倍，能识别影响客户满意度的关键因素并产生可操作的商业洞察。


<details>
  <summary>Details</summary>
Motivation: 传统人工分析客户评论耗时且难以规模化，需要一种高效的方法来提取可解释且具有管理指导意义的见解，帮助企业识别客户满意度的关键驱动因素。

Method: 基于营销理论框架，区分感知属性和可操作特征，在20,000条星巴克Yelp评论上应用8种提示变体，通过人工标注一致性和客户评分预测效度评估模型性能。

Result: LLM与人工编码员高度一致，具有强预测效度，处理每条评论仅需2秒（人工需6分钟），分析识别出对客户满意度影响最大的属性和特征，模拟显示优化关键服务特征情感可带来1-2%的单店收入增长。

Conclusion: 该方法提供了可扩展、可靠的客户评论分析解决方案，能够生成可操作的营销仪表板，帮助企业识别痛点、设计针对性干预措施，实现数据驱动的客户体验优化。

Abstract: This research proposes a systematic, large language model (LLM) approach for
extracting product and service attributes, features, and associated sentiments
from customer reviews. Grounded in marketing theory, the framework
distinguishes perceptual attributes from actionable features, producing
interpretable and managerially actionable insights. We apply the methodology to
20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a
random subset of reviews. Model performance is assessed through agreement with
human annotations and predictive validity for customer ratings. Results show
high consistency between LLMs and human coders and strong predictive validity,
confirming the reliability of the approach. Human coders required a median of
six minutes per review, whereas the LLM processed each in two seconds,
delivering comparable insights at a scale unattainable through manual coding.
Managerially, the analysis identifies attributes and features that most
strongly influence customer satisfaction and their associated sentiments,
enabling firms to pinpoint "joy points," address "pain points," and design
targeted interventions. We demonstrate how structured review data can power an
actionable marketing dashboard that tracks sentiment over time and across
stores, benchmarks performance, and highlights high-leverage features for
improvement. Simulations indicate that enhancing sentiment for key service
features could yield 1-2% average revenue gains per store.

</details>


### [250] [Multi-Marginal Schrödinger Bridge Matching](https://arxiv.org/abs/2510.16587)
*Byoungwoo Park,Juho Lee*

Main category: stat.ML

TL;DR: 提出了多边际薛定谔桥匹配(MSBM)算法，用于处理具有多个中间时间快照的轨迹推断问题，在保持计算效率的同时有效约束所有中间边际分布。


<details>
  <summary>Details</summary>
Motivation: 在发育生物学和系统医学等领域，通常只能获得离散时间快照而无法进行纵向跟踪，传统基于成对时间点的薛定谔桥方法不足以处理具有多个中间快照的系统。

Method: 扩展迭代马尔可夫拟合(IMF)方法，专门设计用于多边际薛定谔桥问题，能够有效处理多个边际约束。

Result: 在合成数据和真实单细胞RNA测序数据集上的实证验证表明，MSBM在捕捉复杂轨迹和尊重中间分布方面具有竞争力或更优性能，且计算效率显著。

Conclusion: MSBM算法为多时间快照系统的轨迹推断提供了有效的解决方案，能够在保持全局动力学连续性的同时强健地实施所有中间边际约束。

Abstract: Understanding the continuous evolution of populations from discrete temporal
snapshots is a critical research challenge, particularly in fields like
developmental biology and systems medicine where longitudinal tracking of
individual entities is often impossible. Such trajectory inference is vital for
unraveling the mechanisms of dynamic processes. While Schr\"odinger Bridge (SB)
offer a potent framework, their traditional application to pairwise time points
can be insufficient for systems defined by multiple intermediate snapshots.
This paper introduces Multi-Marginal Schr\"odinger Bridge Matching (MSBM), a
novel algorithm specifically designed for the multi-marginal SB problem. MSBM
extends iterative Markovian fitting (IMF) to effectively handle multiple
marginal constraints. This technique ensures robust enforcement of all
intermediate marginals while preserving the continuity of the learned global
dynamics across the entire trajectory. Empirical validations on synthetic data
and real-world single-cell RNA sequencing datasets demonstrate the competitive
or superior performance of MSBM in capturing complex trajectories and
respecting intermediate distributions, all with notable computational
efficiency.

</details>


### [251] [Accelerated Learning on Large Scale Screens using Generative Library Models](https://arxiv.org/abs/2510.16612)
*Eli N. Weinstein,Andrei Slabodkin,Mattia G. Gollub,Elizabeth B. Wood*

Main category: stat.ML

TL;DR: 提出优化高通量筛选的算法，在活性序列稀少时只收集阳性样本，通过生成模型校正缺失的阴性样本，实现高效的数据收集和模型训练。


<details>
  <summary>Details</summary>
Motivation: 生物机器学习常受限于数据规模不足，高通量筛选能并行测试大量蛋白质序列，但测量和测序成本限制了数据集规模。

Method: 在活性序列稀少时仅收集阳性样本，利用文库的生成模型校正缺失的阴性样本，获得对真实条件概率的一致有效估计。

Result: 在模拟和大规模抗体筛选中验证了该方法，实验与推断的协同设计显著加速了学习过程。

Conclusion: 通过优化高通量筛选策略和利用生成模型校正，能够有效解决生物机器学习中的数据瓶颈问题。

Abstract: Biological machine learning is often bottlenecked by a lack of scaled data.
One promising route to relieving data bottlenecks is through high throughput
screens, which can experimentally test the activity of $10^6-10^{12}$ protein
sequences in parallel. In this article, we introduce algorithms to optimize
high throughput screens for data creation and model training. We focus on the
large scale regime, where dataset sizes are limited by the cost of measurement
and sequencing. We show that when active sequences are rare, we maximize
information gain if we only collect positive examples of active sequences, i.e.
$x$ with $y>0$. We can correct for the missing negative examples using a
generative model of the library, producing a consistent and efficient estimate
of the true $p(y | x)$. We demonstrate this approach in simulation and on a
large scale screen of antibodies. Overall, co-design of experiments and
inference lets us accelerate learning dramatically.

</details>


### [252] [ARCO-BO: Adaptive Resource-aware COllaborative Bayesian Optimization for Heterogeneous Multi-Agent Design](https://arxiv.org/abs/2510.16652)
*Zihan Wang,Yi-Ping Chen,Tuba Dolar,Wei Chen*

Main category: stat.ML

TL;DR: ARCO-BO是一个自适应资源感知的协作贝叶斯优化框架，专门解决多智能体优化中的异构性问题，包括目标、预算和设计空间的差异。


<details>
  <summary>Details</summary>
Motivation: 现代科学和工程设计中的分布式优化面临智能体间目标、评估预算和可访问设计变量的异构性，这导致协调困难、资源浪费和信息共享无效。现有协作BO方法假设统一资源和完全共享输入空间，这在实践中很少满足。

Method: ARCO-BO结合三个组件：相似性和最优值感知的共识机制用于自适应信息共享、预算感知的异步采样策略用于资源协调、部分输入空间共享用于异构设计空间。

Result: 在合成和高维工程问题上的实验表明，ARCO-BO在复杂多智能体设置中始终优于独立BO和现有协作BO共识方法，实现了稳健高效的性能。

Conclusion: ARCO-BO通过明确考虑多智能体优化中的异构性，提供了在复杂分布式环境中实现高效协作优化的有效解决方案。

Abstract: Modern scientific and engineering design increasingly involves distributed
optimization, where agents such as laboratories, simulations, or industrial
partners pursue related goals under differing conditions. These agents often
face heterogeneities in objectives, evaluation budgets, and accessible design
variables, which complicates coordination and can lead to redundancy, poor
resource use, and ineffective information sharing. Bayesian Optimization (BO)
is a widely used decision-making framework for expensive black box functions,
but its single-agent formulation assumes centralized control and full data
sharing. Recent collaborative BO methods relax these assumptions, yet they
often require uniform resources, fully shared input spaces, and fixed task
alignment, conditions rarely satisfied in practice. To address these
challenges, we introduce Adaptive Resource Aware Collaborative Bayesian
Optimization (ARCO-BO), a framework that explicitly accounts for heterogeneity
in multi-agent optimization. ARCO-BO combines three components: a similarity
and optima-aware consensus mechanism for adaptive information sharing, a
budget-aware asynchronous sampling strategy for resource coordination, and a
partial input space sharing for heterogeneous design spaces. Experiments on
synthetic and high-dimensional engineering problems show that ARCO-BO
consistently outperforms independent BO and existing collaborative BO via
consensus approach, achieving robust and efficient performance in complex
multi-agent settings.

</details>


### [253] [Escaping Model Collapse via Synthetic Data Verification: Near-term Improvements and Long-term Convergence](https://arxiv.org/abs/2510.16657)
*Bingji Yi,Qiyuan Liu,Yuwei Cheng,Haifeng Xu*

Main category: stat.ML

TL;DR: 研究发现通过引入外部合成数据验证器可以避免模型崩溃，甚至改善模型性能，但长期来看参数估计会收敛到验证器的知识中心。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据迭代训练导致的模型崩溃问题，探索如何通过验证机制逆转这一趋势。

Method: 在基础线性回归设置中分析迭代训练过程，引入外部验证器（人类或更好模型）来筛选合成数据，并在MNIST数据上对VAE进行实验验证。

Result: 理论分析和实验表明，使用验证后的合成数据进行迭代训练可以避免模型崩溃，早期性能有所提升，但长期会收敛到验证器的知识中心。

Conclusion: 外部验证器是防止模型崩溃的关键，但验证器的可靠性决定了最终性能的上限。

Abstract: Synthetic data has been increasingly used to train frontier generative
models. However, recent study raises key concerns that iteratively retraining a
generative model on its self-generated synthetic data may keep deteriorating
model performance, a phenomenon often coined model collapse. In this paper, we
investigate ways to modify this synthetic retraining process to avoid model
collapse, and even possibly help reverse the trend from collapse to
improvement. Our key finding is that by injecting information through an
external synthetic data verifier, whether a human or a better model, synthetic
retraining will not cause model collapse. To develop principled understandings
of the above insight, we situate our analysis in the foundational linear
regression setting, showing that iterative retraining with verified synthetic
data can yield near-term improvements but ultimately drives the parameter
estimate to the verifier's "knowledge center" in the long run. Our theory hence
predicts that, unless the verifier is perfectly reliable, the early gains will
plateau and may even reverse. Indeed, these theoretical insights are further
confirmed by our experiments on both linear regression as well as Variational
Autoencoders (VAEs) trained on MNIST data.

</details>


### [254] [Infinite Neural Operators: Gaussian processes on functions](https://arxiv.org/abs/2510.16675)
*Daniel Augusto de Souza,Yuchen Zhu,Harry Jake Cunningham,Yuri Saporito,Diego Mesquita,Marc Peter Deisenroth*

Main category: stat.ML

TL;DR: 该论文将神经网络与高斯过程的连接扩展到神经算子，证明了具有高斯分布卷积核的任意深度神经算子会收敛到函数值高斯过程，并计算了这些NO-GP的协方差函数。


<details>
  <summary>Details</summary>
Motivation: 将神经网络与高斯过程的连接扩展到神经算子领域，以改进深度神经网络的不确定性量化，并揭示当前FNO架构的归纳偏置。

Method: 证明具有高斯分布卷积核的任意深度神经算子收敛到函数值高斯过程的条件，计算NO-GP的协方差函数，包括流行的傅里叶神经算子。

Result: 成功推导出神经算子高斯过程的协方差函数，并在回归场景（包括PDE解算子）中计算了这些高斯过程的后验分布。

Conclusion: 这项工作为揭示当前FNO架构的归纳偏置迈出了重要一步，并为基于核的算子学习方法开辟了引入新归纳偏置的路径。

Abstract: A variety of infinitely wide neural architectures (e.g., dense NNs, CNNs, and
transformers) induce Gaussian process (GP) priors over their outputs. These
relationships provide both an accurate characterization of the prior predictive
distribution and enable the use of GP machinery to improve the uncertainty
quantification of deep neural networks. In this work, we extend this connection
to neural operators (NOs), a class of models designed to learn mappings between
function spaces. Specifically, we show conditions for when arbitrary-depth NOs
with Gaussian-distributed convolution kernels converge to function-valued GPs.
Based on this result, we show how to compute the covariance functions of these
NO-GPs for two NO parametrizations, including the popular Fourier neural
operator (FNO). With this, we compute the posteriors of these GPs in regression
scenarios, including PDE solution operators. This work is an important step
towards uncovering the inductive biases of current FNO architectures and opens
a path to incorporate novel inductive biases for use in kernel-based operator
learning methods.

</details>


### [255] [Local regression on path spaces with signature metrics](https://arxiv.org/abs/2510.16728)
*Christian Bayer,Davit Gogolashvili,Luca Pelizzari*

Main category: stat.ML

TL;DR: 提出了一种结合签名变换和局部核回归的函数型Nadaraya-Watson估计器，用于路径值数据的非参数回归和分类，在保持计算效率的同时提供良好的统计性能。


<details>
  <summary>Details</summary>
Motivation: 解决路径值数据的回归和分类问题，传统方法在高维设置下存在计算瓶颈，需要一种能够直接比较路径并保持计算效率的方法。

Method: 将粗糙路径理论中的签名变换与局部核回归相结合，利用签名诱导的距离在经典核回归框架中进行计算，避免大规模核矩阵操作的可扩展性瓶颈。

Result: 建立了有限样本收敛界，证明了基于签名的距离在无限维设置中相比传统度量具有更优的统计特性，在合成和真实数据应用中展现出竞争性精度和显著计算优势。

Conclusion: 签名变换为序列数据提供了一种原则性的编码方式，结合核回归框架实现了计算效率和统计性能的良好平衡，在路径值数据分析中具有实用价值。

Abstract: We study nonparametric regression and classification for path-valued data. We
introduce a functional Nadaraya-Watson estimator that combines the signature
transform from rough path theory with local kernel regression. The signature
transform provides a principled way to encode sequential data through iterated
integrals, enabling direct comparison of paths in a natural metric space. Our
approach leverages signature-induced distances within the classical kernel
regression framework, achieving computational efficiency while avoiding the
scalability bottlenecks of large-scale kernel matrix operations. We establish
finite-sample convergence bounds demonstrating favorable statistical properties
of signature-based distances compared to traditional metrics in
infinite-dimensional settings. We propose robust signature variants that
provide stability against outliers, enhancing practical performance.
Applications to both synthetic and real-world data - including stochastic
differential equation learning and time series classification - demonstrate
competitive accuracy while offering significant computational advantages over
existing methods.

</details>


### [256] [Kernel-Based Nonparametric Tests For Shape Constraints](https://arxiv.org/abs/2510.16745)
*Rohan Sen*

Main category: stat.ML

TL;DR: 提出了一个再生核希尔伯特空间框架，用于非参数均值-方差优化的形状约束推断，并提供了统计理论保证和高效计算程序。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理非参数均值-方差优化中形状约束的统计推断框架，确保理论严谨性和计算可扩展性。

Method: 使用再生核希尔伯特空间框架，推导样本估计量的统计性质，基于枢轴Cholesky分解设计高效计算程序。

Result: 获得了渐近一致性、函数中心极限定理和有限样本偏差界等理论保证，经验测试验证了方法的有效性。

Conclusion: 所提出的方法在理论保证和计算效率方面表现良好，适用于大规模数据集的形状约束推断问题。

Abstract: We develop a reproducing kernel Hilbert space (RKHS) framework for
nonparametric mean-variance optimization and inference on shape constraints of
the optimal rule. We derive statistical properties of the sample estimator and
provide rigorous theoretical guarantees, such as asymptotic consistency, a
functional central limit theorem, and a finite-sample deviation bound that
matches the Monte Carlo rate up to regularization. Building on these findings,
we introduce a joint Wald-type statistic to test for shape constraints over
finite grids. The approach comes with an efficient computational procedure
based on a pivoted Cholesky factorization, facilitating scalability to large
datasets. Empirical tests suggest favorably of the proposed methodology.

</details>


### [257] [Prediction-Augmented Trees for Reliable Statistical Inference](https://arxiv.org/abs/2510.16937)
*Vikram Kher,Argyris Oikonomou,Manolis Zampetakis*

Main category: stat.ML

TL;DR: 本文提出了两种新的学习增强估计器PART和PAQ，用于在科学发现中安全使用机器学习预测进行统计分析，相比现有方法PPI和PPI++具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在预测任务中的成功应用，科学家开始将ML预测作为科学发现流程的核心组成部分。本文研究如何在统计分析中安全地使用ML预测，特别是在只有少量黄金标准标记样本和大量未标记样本的情况下。

Method: 提出了两种新估计器：基于决策树的PART（预测增强残差树）和PAQ（预测增强积分）。PART使用贪心准则构建决策树，PAQ是当PART树深度趋于无穷时的极限情况。

Result: PART在生态学、天文学和人口普查等多个真实数据集上优于现有方法，能够构建更有效的置信区间。PAQ在适当假设下，其方差缩减速率达到O(N^{-1} + n^{-4})，显著优于现有方法的O(N^{-1}+n^{-1})速率。

Conclusion: PART和PAQ估计器通过结合黄金标准样本和机器学习预测，提供了更高效的统计分析方法，在科学发现中安全使用ML预测方面具有重要价值。

Abstract: The remarkable success of machine learning (ML) in predictive tasks has led
scientists to incorporate ML predictions as a core component of the scientific
discovery pipeline. This was exemplified by the landmark achievement of
AlphaFold (Jumper et al. (2021)). In this paper, we study how ML predictions
can be safely used in statistical analysis of data towards scientific
discovery. In particular, we follow the framework introduced by Angelopoulos et
al. (2023). In this framework, we assume access to a small set of $n$
gold-standard labeled samples, a much larger set of $N$ unlabeled samples, and
a ML model that can be used to impute the labels of the unlabeled data points.
We introduce two new learning-augmented estimators: (1) Prediction-Augmented
Residual Tree (PART), and (2) Prediction-Augmented Quadrature (PAQ). Both
estimators have significant advantages over existing estimators like PPI and
PPI++ introduced by Angelopoulos et al. (2023) and Angelopoulos et al. (2024),
respectively. PART is a decision-tree based estimator built using a greedy
criterion. We first characterize PART's asymptotic distribution and demonstrate
how to construct valid confidence intervals. Then we show that PART outperforms
existing methods in real-world datasets from ecology, astronomy, and census
reports, among other domains. This leads to estimators with higher confidence,
which is the result of using both the gold-standard samples and the machine
learning predictions. Finally, we provide a formal proof of the advantage of
PART by exploring PAQ, an estimation that arises when considering the limit of
PART when the depth its tree grows to infinity. Under appropriate assumptions
in the input data we show that the variance of PAQ shrinks at rate of $O(N^{-1}
+ n^{-4})$, improving significantly on the $O(N^{-1}+n^{-1})$ rate of existing
methods.

</details>


### [258] [Adaptive Sample Sharing for Linear Regression](https://arxiv.org/abs/2510.16986)
*Hamza Cherkaoui,Hélène Halconruy,Yohan Petetin*

Main category: stat.ML

TL;DR: 提出一种基于岭回归的样本共享方法，通过数据驱动规则决定从辅助数据集中借用多少样本，以避免负迁移并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 在许多商业场景中，特定任务的标记数据稀缺或获取成本高，限制了监督学习的应用。需要利用辅助数据集同时明确防范负迁移问题。

Method: 引入基于迁移增益估计的原则性数据驱动规则，该规则估计预测误差的边际减少量，决定从辅助数据集中添加多少样本到目标训练集。

Result: 在标准条件下，该方法在能改善参数估计时借用样本，否则避免借用。在高斯特征设置下，分析了确保借用样本减少预测误差的数据集特性。在合成和真实数据集上验证了方法的有效性。

Conclusion: 该方法在强基线和单任务训练基础上获得了一致的性能提升，同时成功避免了负迁移问题。

Abstract: In many business settings, task-specific labeled data are scarce or costly to
obtain, which limits supervised learning on a specific task. To address this
challenge, we study sample sharing in the case of ridge regression: leveraging
an auxiliary data set while explicitly protecting against negative transfer. We
introduce a principled, data-driven rule that decides how many samples from an
auxiliary dataset to add to the target training set. The rule is based on an
estimate of the transfer gain i.e. the marginal reduction in the predictive
error. Building on this estimator, we derive finite-sample guaranties: under
standard conditions, the procedure borrows when it improves parameter
estimation and abstains otherwise. In the Gaussian feature setting, we analyze
which data set properties ensure that borrowing samples reduces the predictive
error. We validate the approach in synthetic and real datasets, observing
consistent gains over strong baselines and single-task training while avoiding
negative transfer.

</details>


### [259] [Mode Collapse of Mean-Field Variational Inference](https://arxiv.org/abs/2510.17063)
*Shunan Sheng,Bohan Wu,Alberto González-Sanz*

Main category: stat.ML

TL;DR: 本文首次从理论上解释了平均场变分推断(MFVI)中的模式坍塌现象，提出了ε-分离性概念来量化混合成分的分离程度，并开发了旋转变分推断(RoVI)来解决该问题。


<details>
  <summary>Details</summary>
Motivation: MFVI在逼近高维概率分布时经常出现模式坍塌问题，即当目标分布是混合分布时，MFVI优化器倾向于将大部分质量集中在单个混合成分上。目前缺乏对这一现象的理论解释。

Method: 引入ε-分离性概念来量化混合成分的分离程度，推导出当两个混合成分充分分离时MFVI优化器分配给每个成分的质量界限。提出旋转变分推断(RoVI)，通过在MFVI中加入旋转矩阵来解决模式坍塌问题。

Result: 理论分析表明模式坍塌的发生关键取决于混合成分的相对位置。数值研究支持理论发现，并证明了RoVI在缓解模式坍塌方面的优势。

Conclusion: 本文首次为MFVI中的模式坍塌现象提供了理论解释，提出的RoVI方法能有效解决该问题，为变分推断的实际应用提供了重要改进。

Abstract: Mean-field variational inference (MFVI) is a widely used method for
approximating high-dimensional probability distributions by product measures.
It has been empirically observed that MFVI optimizers often suffer from mode
collapse. Specifically, when the target measure $\pi$ is a mixture $\pi = w P_0
+ (1 - w) P_1$, the MFVI optimizer tends to place most of its mass near a
single component of the mixture. This work provides the first theoretical
explanation of mode collapse in MFVI. We introduce the notion to capture the
separatedness of the two mixture components -- called
$\varepsilon$-separateness -- and derive explicit bounds on the fraction of
mass that any MFVI optimizer assigns to each component when $P_0$ and $P_1$ are
$\varepsilon$-separated for sufficiently small $\varepsilon$. Our results
suggest that the occurrence of mode collapse crucially depends on the relative
position of the components. To address this issue, we propose the rotational
variational inference (RoVI), which augments MFVI with a rotation matrix. The
numerical studies support our theoretical findings and demonstrate the benefits
of RoVI.

</details>


### [260] [DFNN: A Deep Fréchet Neural Network Framework for Learning Metric-Space-Valued Responses](https://arxiv.org/abs/2510.17072)
*Kyum Kim,Yaqing Chen,Paromita Dubey*

Main category: stat.ML

TL;DR: 提出深度Fréchet神经网络(DFNNs)，一种端到端的深度学习框架，用于从欧几里得预测变量预测非欧几里得响应（如概率分布、网络、对称正定矩阵和组合）。


<details>
  <summary>Details</summary>
Motivation: 现代应用中非欧几里得响应（如概率分布、网络等）的回归变得越来越重要，需要能够处理这类响应的预测方法。

Method: 利用深度神经网络的表示学习能力，通过最小化Fréchet风险来近似条件Fréchet均值（度量空间中条件期望的类比）。

Result: 建立了DFNNs的通用逼近定理，在合成分布和网络值响应以及预测就业职业组成的真实应用中的实证研究表明，DFNNs始终优于现有方法。

Conclusion: DFNNs提供了一个高度灵活且强大的框架，能够有效处理各种度量空间中的非欧几里得响应预测问题。

Abstract: Regression with non-Euclidean responses -- e.g., probability distributions,
networks, symmetric positive-definite matrices, and compositions -- has become
increasingly important in modern applications. In this paper, we propose deep
Fr\'echet neural networks (DFNNs), an end-to-end deep learning framework for
predicting non-Euclidean responses -- which are considered as random objects in
a metric space -- from Euclidean predictors. Our method leverages the
representation-learning power of deep neural networks (DNNs) to the task of
approximating conditional Fr\'echet means of the response given the predictors,
the metric-space analogue of conditional expectations, by minimizing a
Fr\'echet risk. The framework is highly flexible, accommodating diverse metrics
and high-dimensional predictors. We establish a universal approximation theorem
for DFNNs, advancing the state-of-the-art of neural network approximation
theory to general metric-space-valued responses without making model
assumptions or relying on local smoothing. Empirical studies on synthetic
distributional and network-valued responses, as well as a real-world
application to predicting employment occupational compositions, demonstrate
that DFNNs consistently outperform existing methods.

</details>


### [261] [Optimal Best Arm Identification under Differential Privacy](https://arxiv.org/abs/2510.17348)
*Marc Jourdan,Achraf Azize*

Main category: stat.ML

TL;DR: 本文研究了在全局差分隐私约束下的最佳臂识别问题，通过引入新的信息论量和改进的采样策略，显著缩小了隐私保护BAI问题的上下界差距。


<details>
  <summary>Details</summary>
Motivation: 在数据敏感应用（如自适应临床试验）中部署BAI算法时存在隐私担忧，需要研究在差分隐私约束下的最佳臂识别问题。现有方法在全局DP设置下的上下界差距较大，需要缩小这一差距。

Method: 1) 提出更紧的下界，用新的信息论量替换KL散度；2) 基于传输成本的停止规则和臂依赖几何批处理的私有均值估计器；3) 基于传输成本的Top Two采样规则。

Result: 对于任何隐私预算ε，算法在预期样本复杂度上的渐近上界与下界匹配，乘性常数小于8，优于现有的δ-正确和ε-全局DP BAI算法。

Conclusion: 本文显著缩小了隐私保护BAI问题的上下界差距，为数据敏感应用中的隐私保护决策提供了更高效的算法。

Abstract: Best Arm Identification (BAI) algorithms are deployed in data-sensitive
applications, such as adaptive clinical trials or user studies. Driven by the
privacy concerns of these applications, we study the problem of
fixed-confidence BAI under global Differential Privacy (DP) for Bernoulli
distributions. While numerous asymptotically optimal BAI algorithms exist in
the non-private setting, a significant gap remains between the best lower and
upper bounds in the global DP setting. This work reduces this gap to a small
multiplicative constant, for any privacy budget $\epsilon$. First, we provide a
tighter lower bound on the expected sample complexity of any $\delta$-correct
and $\epsilon$-global DP strategy. Our lower bound replaces the
Kullback-Leibler (KL) divergence in the transportation cost used by the
non-private characteristic time with a new information-theoretic quantity that
optimally trades off between the KL divergence and the Total Variation distance
scaled by $\epsilon$. Second, we introduce a stopping rule based on these
transportation costs and a private estimator of the means computed using an
arm-dependent geometric batching. En route to proving the correctness of our
stopping rule, we derive concentration results of independent interest for the
Laplace distribution and for the sum of Bernoulli and Laplace distributions.
Third, we propose a Top Two sampling rule based on these transportation costs.
For any budget $\epsilon$, we show an asymptotic upper bound on its expected
sample complexity that matches our lower bound to a multiplicative constant
smaller than $8$. Our algorithm outperforms existing $\delta$-correct and
$\epsilon$-global DP BAI algorithms for different values of $\epsilon$.

</details>


### [262] [Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs](https://arxiv.org/abs/2510.17472)
*Paula Cordero-Encinar,Andrew B. Duncan*

Main category: stat.ML

TL;DR: 提出了一个统一框架用于大语言模型的可认证推理，证明多数投票提供自一致性的统计保证，并引入自适应停止规则MMC。揭示了无标签后训练方法通过指数倾斜机制锐化答案分布，提出了优化锐度与偏差权衡的新目标。


<details>
  <summary>Details</summary>
Motivation: 自一致性和测试时强化学习等无监督方法提高了LLM的可靠性，但其机制和统计保证仍不清楚。需要建立统一框架来理解这些方法的理论基础。

Method: 开发了可认证推理的统一框架，推导有限样本和任意时间有效的集中界限，引入MMC顺序停止规则，分析后训练方法如何锐化答案分布，并提出新的优化目标。

Result: 证明了多数投票与模型终端分布的众数高度一致，后训练方法通过指数倾斜机制减少认证所需样本量，新目标能优化锐度与偏差的权衡。

Conclusion: 该框架统一解释了自一致性和TTRL这两种核心测试时扩展策略，为推理LLM的无标签可认证可靠性提供了统计基础。

Abstract: Recent advances such as self-consistency and test-time reinforcement learning
(TTRL) improve the reliability of large language models (LLMs) without
additional supervision, yet their underlying mechanisms and statistical
guarantees remain poorly understood. We present a unified framework for
certifiable inference in LLMs, showing that majority voting provides a
statistical certificate of self-consistency: under mild assumptions, the
aggregated answer coincides with the mode of the model's terminal distribution
with high probability. We derive finite-sample and anytime-valid concentration
bounds that quantify this confidence, and introduce the Martingale Majority
Certificate (MMC), a sequential stopping rule that adaptively determines when
sufficient samples have been drawn. We further prove that label-free
post-training methods such as TTRL implicitly sharpen the answer distribution
by exponentially tilting it toward its mode, thereby reducing the number of
samples required for certification. Building on this insight, we propose new
post-training objectives that explicitly optimise this trade-off between
sharpness and bias. Together, these results explain and connect two central
test-time scaling strategies, self-consistency and TTRL, within a single
statistical framework for label-free, certifiable reliability in reasoning
LLMs.

</details>


### [263] [Non-asymptotic error bounds for probability flow ODEs under weak log-concavity](https://arxiv.org/abs/2510.17608)
*Gitte Kremling,Francesco Iafrate,Mahsa Taheri,Johannes Lederer*

Main category: stat.ML

TL;DR: 该论文为概率流ODE的得分生成模型建立了非渐近收敛边界，在弱对数凹性和得分函数Lipschitz连续性的较弱假设下，扩展了扩散生成模型的理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有的收敛保证依赖于对目标分布的严格正则性假设，如强对数凹性或有界支撑。本文旨在为更现实的数据分布和实际ODE求解器建立收敛理论。

Method: 使用概率流ODE和指数积分器离散化方案，明确考虑初始化误差、得分近似误差和离散化效应。

Result: 在2-Wasserstein距离下建立了非渐近收敛边界，适用于非对数凹分布（如高斯混合），并为采样算法的效率和正确性提供了具体保证。

Conclusion: 该工作弥合了扩散生成建模中的关键理论挑战，将收敛理论扩展到更现实的数据分布，并为超参数选择提供了明确的速率指导。

Abstract: Score-based generative modeling, implemented through probability flow ODEs,
has shown impressive results in numerous practical settings. However, most
convergence guarantees rely on restrictive regularity assumptions on the target
distribution -- such as strong log-concavity or bounded support. This work
establishes non-asymptotic convergence bounds in the 2-Wasserstein distance for
a general class of probability flow ODEs under considerably weaker assumptions:
weak log-concavity and Lipschitz continuity of the score function. Our
framework accommodates non-log-concave distributions, such as Gaussian
mixtures, and explicitly accounts for initialization errors, score
approximation errors, and effects of discretization via an exponential
integrator scheme. Bridging a key theoretical challenge in diffusion-based
generative modeling, our results extend convergence theory to more realistic
data distributions and practical ODE solvers. We provide concrete guarantees
for the efficiency and correctness of the sampling algorithm, complementing the
empirical success of diffusion models with rigorous theory. Moreover, from a
practical perspective, our explicit rates might be helpful in choosing
hyperparameters, such as the step size in the discretization.

</details>
