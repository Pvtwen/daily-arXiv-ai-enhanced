<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [cs.LG](#cs.LG) [Total: 83]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [A Synoptic Review of High-Frequency Oscillations as a Biomarker in Neurodegenerative Disease](https://arxiv.org/abs/2508.18712)
*Samin Yaser,Mahad Ali,Laura J. Brattain,Yang Jiang,VP Nguyen,Jing Xiang*

Main category: eess.SP

TL;DR: 这篇综述论文系统分析了用于神经退行性疾病HFO研究的公开EEG数据集，通过文献计量分析和数据集比较，为利用公开数据推进HFO作为阿尔茨海默病跨疾病生物标志物提供指导。


<details>
  <summary>Details</summary>
Motivation: 高频振荡（HFOs）作为癫痫组织的特异性生物标志物，最近发现在阿尔茨海默病中也存在，反映了网络过度兴奋性，为早期诊断和疾病追踪提供了有前景的非侵入性工具。需要系统评估现有公开EEG数据集的技术适用性。

Method: 进行了1222篇文章的文献计量分析，系统分析和比较关键公开数据集，评估参与者队列、数据采集参数和可访问性，特别关注其对HFO分析的技术适用性。

Result: 发现数据集间存在显著的方法学异质性，特别是在采样频率和记录范式方面，这对跨研究验证构成挑战，但也为稳健性测试提供了机会。

Conclusion: 通过整合分散信息、澄清命名法并提供详细方法学框架，本综述为研究人员利用公开数据推进HFO作为AD及相关疾病跨疾病生物标志物的作用提供了指南。

Abstract: High Frequency Oscillations (HFOs), rapid bursts of brain activity above 80
Hz, have emerged as a highly specific biomarker for epileptogenic tissue.
Recent evidence suggests that HFOs are also present in Alzheimer's Disease
(AD), reflecting underlying network hyperexcitability and offering a promising,
noninvasive tool for early diagnosis and disease tracking. This synoptic review
provides a comprehensive analysis of publicly available electroencephalography
(EEG) datasets relevant to HFO research in neurodegenerative disorders. We
conducted a bibliometric analysis of 1,222 articles, revealing a significant
and growing research interest in HFOs, particularly within the last ten years.
We then systematically profile and compare key public datasets, evaluating
their participant cohorts, data acquisition parameters, and accessibility, with
a specific focus on their technical suitability for HFO analysis. Our
comparative synthesis highlights critical methodological heterogeneity across
datasets, particularly in sampling frequency and recording paradigms, which
poses challenges for cross-study validation, but also offers opportunities for
robustness testing. By consolidating disparate information, clarifying
nomenclature, and providing a detailed methodological framework, this review
serves as a guide for researchers aiming to leverage public data to advance the
role of HFOs as a cross-disease biomarker for AD and related conditions.

</details>


### [2] [SkyTrust: Blockchain-Enhanced UAV Security for NTNs with Dynamic Trust and Energy-Aware Consensus](https://arxiv.org/abs/2508.18735)
*Afan Ali,Irfanullah Khan*

Main category: eess.SP

TL;DR: 这篇论文提出了一种动态信任分数调整机制DTSAM-EAC，通过结合许可链和联邦学习来提高基于UAV的非地面网络的安全性和能源效率。


<details>
  <summary>Details</summary>
Motivation: UAV基站的非地面网络由于其分布式和动态性，极易受到安全攻击和恶意节点的威胁。

Method: 整合了许可语言链Hyperledger Fabric和联邦学习，通过动态信任评估机制和能源感知共识机制来保护网络安全。

Result: 模拟结果显示该框架实现了94%的信任分数预测准确率和96%的恶意UAV检测率，在隐私、能源效率和可靠性方面超过了传统方案。

Conclusion: 该方案符合6G的分布式智能和可持续性要求，是一种能源效率高、可扩展的NTN安全解决方案。

Abstract: Non-Terrestrial Networks (NTNs) based on Unmanned Aerial Vehicles (UAVs) as
base stations are extremely susceptible to security attacks due to their
distributed and dynamic nature, which makes them vulnerable to rogue nodes. In
this paper, a new Dynamic Trust Score Adjustment Mechanism with Energy-Aware
Consensus (DTSAM-EAC) is proposed to enhance security in UAV-based NTNs. The
proposed framework integrates a permissioned Hyperledger Fabric blockchain with
Federated Learning (FL) to support privacy-preserving trust evaluation. Trust
ratings are updated continuously through weighted aggregation of past trust,
present behavior, and energy contribution, thus making the system adaptive to
changing network conditions. An energy-aware consensus mechanism prioritizes
UAVs with greater available energy for block validation, ensuring efficient use
of resources under resource-constrained environments. FL aggregation with
trust-weighting further increases the resilience of the global trust model.
Simulation results verify the designed framework achieves 94\% trust score
prediction accuracy and 96\% rogue UAV detection rate while outperforming
centralized and static baselines of trust-based solutions on privacy, energy
efficiency, and reliability. It complies with 6G requirements in terms of
distributed intelligence and sustainability and is an energy-efficient and
scalable solution to secure NTNs.

</details>


### [3] [Near-Field Challenges in Ultra-Wideband ISAC: Beamforming Strategies and System Insights](https://arxiv.org/abs/2508.18810)
*Yonghwi Kim,Sang-Hyun Park,Siyun Yang,Kai-Kit Wong,Linglong Dai,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: 这篇论文提出了一种应对6G网络中近场超宽带ISAC系统挑战的实用波束形成策略，解决大天线数组和超宽带宽度带来的近场效应和波束偏移问题。


<details>
  <summary>Details</summary>
Motivation: 6G网络中集成感知与通信(ISAC)成为核心技术，但大天线数组和超宽带宽度导致近场传播效应和波束偏移，经典远场设计无法满足要求。虽然真时延迟单元(TTD)可以解决，但成本高且硬件复杂。

Method: 探索模拟和数字域的码本设计，通过大规模系统级模拟验证，包括基于真实城市3D地图的评估。

Result: 细心设计的波束形成技术可以在严重近场条件下平衡通信吞吐量和感知性能，实现可靠覆盖和高效资源利用。

Conclusion: 文章指出了硬件、算法和系统集成方面的开放性挑战，为6G完备ISAC网络的部署指明了研究方向。

Abstract: The shift toward sixth-generation (6G) wireless networks places integrated
sensing and communications (ISAC) at the core of future applications such as
autonomous driving, extended reality, and smart manufacturing. However, the
combination of large antenna arrays and ultra-wide bandwidths brings near-field
propagation effects and beam squint to the forefront, fundamentally challenging
traditional far-field designs. True time delay units (TTDs) offer a potential
solution, but their cost and hardware complexity limit scalability. In this
article, we present practical beamforming strategies for near-field
ultra-wideband ISAC systems. We explore codebook designs across analog and
digital domains that mitigate beam squint, ensure reliable user coverage, and
enhance sensing accuracy. We further validate these approaches through
large-scale system-level simulations, including 3D map-based evaluations that
reflect real-world urban environments. Our results demonstrate how carefully
designed beamforming can balance communication throughput with sensing
performance, achieving reliable coverage and efficient resource use even under
severe near-field conditions. We conclude by highlighting open challenges in
hardware, algorithms, and system integration, pointing toward research
directions that will shape the deployment of 6G-ready ISAC networks.

</details>


### [4] [DIFNet: Decentralized Information Filtering Fusion Neural Network with Unknown Correlation in Sensor Measurement Noises](https://arxiv.org/abs/2508.18854)
*Ruifeng Dong,Ming Wang,Ning Liu,Tong Guo,Jiayi Kang,Xiaojing Shen,Yao Mao*

Main category: eess.SP

TL;DR: 提出DIFNet数据驱动方法，通过神经网络学习测量噪声中的未知相关性，提升去中心化传感器网络的融合性能


<details>
  <summary>Details</summary>
Motivation: 去中心化传感器网络在状态估计中具有鲁棒性和可扩展性优势，但未知噪声相关性会降低融合精度，需要解决这一问题

Method: 基于结构化观测模型分解全局状态估计问题，使用神经网络学习离散时间非线性状态空间模型中测量噪声的未知相关性

Result: 数值模拟显示DIFNet相比传统滤波方法具有更优的融合性能，在时变噪声等复杂场景下表现出鲁棒特性

Conclusion: DIFNet能有效学习未知噪声相关性，提升去中心化传感器网络的状态估计性能，适用于复杂实际应用场景

Abstract: In recent years, decentralized sensor networks have garnered significant
attention in the field of state estimation owing to enhanced robustness,
scalability, and fault tolerance. Optimal fusion performance can be achieved
under fully connected communication and known noise correlation structures. To
mitigate communication overhead, the global state estimation problem is
decomposed into local subproblems through structured observation model. This
ensures that even when the communication network is not fully connected, each
sensor can achieve locally optimal estimates of its observable state
components. To address the degradation of fusion accuracy induced by unknown
correlations in measurement noise, this paper proposes a data-driven method,
termed Decentralized Information Filter Neural Network (DIFNet), to learn
unknown noise correlations in data for discrete-time nonlinear state space
models with cross-correlated measurement noises. Numerical simulations
demonstrate that DIFNet achieves superior fusion performance compared to
conventional filtering methods and exhibits robust characteristics in more
complex scenarios, such as the presence of time-varying noise. The source code
used in our numerical experiment can be found online at
https://wisdom-estimation.github.io/DIFNet_Demonstrate/.

</details>


### [5] [Beyond-Diagonal RIS: Adversarial Channels and Optimality of Low-Complexity Architectures](https://arxiv.org/abs/2508.19000)
*Atso Iivanainen,Robin Rajamäki,Visa Koivunen*

Main category: eess.SP

TL;DR: 本文分析了两种低复杂度BD-RIS架构（群连接和树连接）在最坏情况下的性能，发现了对抗性信道会导致显著性能损失，并揭示了两种架构之间的新联系。


<details>
  <summary>Details</summary>
Motivation: 现有BD-RIS研究主要关注平均性能和架构复杂度的权衡，但忽视了最坏情况性能分析，特别是在对抗性信道条件下的鲁棒性。

Method: 通过理论分析表征了针对群连接和树连接BD-RIS架构的对抗性信道集合，并进行了数值验证。

Result: 发现对抗性信道会导致接收信号功率的显著性能损失，并揭示了两种低复杂度BD-RIS架构之间的新联系。

Conclusion: 研究结果为设计对对抗性传播条件和恶意攻击具有鲁棒性的高效BD-RIS架构奠定了基础。

Abstract: Beyond-diagonal reconfigurable intelligent surfaces (BD-RISs) have recently
gained attention as an enhancement to conventional RISs. BD-RISs allow
optimizing not only the phase, but also the amplitude responses of their
discrete surface elements by introducing adjustable inter-element couplings.
Various BD-RIS architectures have been proposed to optimally trade off between
average performance and complexity of the architecture. However, little
attention has been paid to worst-case performance. This paper characterizes
novel sets of adversarial channels for which certain low-complexity BD-RIS
architectures have suboptimal performance in terms of received signal power at
an intended communications user. Specifically, we consider two recent BD-RIS
models: the so-called group-connected and tree-connected architecture. The
derived adversarial channel sets reveal new surprising connections between the
two architectures. We validate our analytical results numerically,
demonstrating that adversarial channels can cause a significant performance
loss. Our results pave the way towards efficient BD-RIS designs that are robust
to adversarial propagation conditions and malicious attacks.

</details>


### [6] [mmKey: Channel-Aware Beam Shaping for Reliable Key Generation in mmWave Wireless Networks](https://arxiv.org/abs/2508.19010)
*Poorya Mollahosseini,Yasaman Ghasempour*

Main category: eess.SP

TL;DR: mmKey是一个针对毫米波网络的物理层密钥生成框架，利用多天线注入随机性来克服毫米波信道稀疏性、相位噪声和路径损耗等挑战，通过遗传算法优化波束成形权重来抑制LOS分量，在安全性和鲁棒性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 毫米波频段的物理层密钥生成面临信道稀疏性、高相位噪声和高路径损耗等根本性挑战，这些因素破坏了安全密钥生成所需的随机性和互易性。

Method: 利用毫米波无线节点的多天线在准静态无线信道中注入随机性，采用遗传算法逐步演化初始权重向量种群，在考虑信道稀疏性和信噪比的条件下抑制LOS分量。

Result: 广泛仿真显示，mmKey相比随机波束成形平均提升39.4%的安全间隙，相比零陷波束成形提升34.0%，优于传统方案。

Conclusion: mmKey框架成功解决了毫米波物理层密钥生成的安全性和鲁棒性平衡问题，为下一代无线网络安全提供了有效解决方案。

Abstract: Physical-layer key generation (PLKG) has emerged as a promising technique to
secure next-generation wireless networks by exploiting the inherent properties
of the wireless channel. However, PLKG faces fundamental challenges in the
millimeter wave (mmWave) regime due to channel sparsity, higher phase noise,
and higher path loss, which undermine both the randomness and reciprocity
required for secure key generation. In this paper, we present mmKey, a novel
PLKG framework that capitalizes on the availability of multiple antennas at
mmWave wireless nodes to inject randomness into an otherwise quasi-static
wireless channel. Different from prior works that sacrifice either the secrecy
of the key generation or the robustness, mmKey balances these two requirements.
In particular, mmKey leverages a genetic algorithm to gradually evolve the
initial weight vector population toward configurations that suppress the LOS
component while taking into account the channel conditions, specifically, the
sparsity and the signal-to-noise ratio (SNR). Extensive simulations show that
mmKey improves the secrecy gap by an average of 39.4% over random beamforming
and 34.0% over null beamforming, outperforming conventional schemes.

</details>


### [7] [Fast Vortex Beam Alignment for OAM Mode Multiplexing in LOS MIMO Networks](https://arxiv.org/abs/2508.19034)
*Poorya Mollahosseini,Yasaman Ghasempour*

Main category: eess.SP

TL;DR: OrthoVortex是一个基于轨道角动量(OAM)的通信系统对准框架，通过估计失准角度和相位校正来恢复模态正交性，提高链路容量和信号干扰比。


<details>
  <summary>Details</summary>
Motivation: OAM通信系统在视距场景中提供高容量复用，但对节点失准敏感，会破坏模态正交性并阻碍数据复用增益。现有方法依赖不切实际的数字阵列或穷举波束扫描，缺乏实用性。

Method: 提出OrthoVortex框架，利用交叉模态相位作为独特特征来识别失准角度，实现少样本对准。基于OAM传播物理原理进行角度估计和相位校正，恢复模态正交性。

Result: 在120 GHz使用低成本超表面进行仿真和空中测量，实现快速精确的失准估计（方位角平均绝对误差0.69°，仰角2.54°）。可降低模态间干扰，信号干扰比提高12 dB以上，链路容量提升4.5倍以上。

Conclusion: OrthoVortex是首个经过实验验证的OAM波束对准技术，解决了实际部署中的对准难题，为OAM通信系统的实际应用提供了可行解决方案。

Abstract: Orbital Angular Momentum (OAM)-based communication systems offer
high-capacity multiplexing in line-of-sight (LOS) scenarios; yet, their
performance is sensitive to nodal misalignment, which disrupts modal
orthogonality, hindering the data multiplexing gain. To tackle this challenge,
we present OrthoVortex, a novel framework that estimates the misalignment
angles and applies the appropriate phase correction to restore orthogonality
between modes. Unlike purely theoretical prior efforts that rely on impractical
fully digital arrays or exhaustive beam scans, OrthoVortex introduces and
leverages the cross-modal phase, as a unique signature for identifying the
misalignment angles. OrthoVortex is a few-shot alignment technique, making it
feasible for real-world implementations. Our key contributions include: (i) a
robust angle estimation and phase correction framework based on the physics of
OAM propagation that estimates the misalignment and restores modal
orthogonality, (ii) the first-ever experimental validation of OAM beam
alignment with RF transceivers, and (iii) a comprehensive analysis of practical
constraints, including the impact of antenna count and bandwidth. Simulations
and over-the-air measurements using low-cost, rapidly prototyped metasurfaces
operating at 120 GHz demonstrate that OrthoVortex achieves fast and precise
misalignment estimation (mean absolute error of $0.69^{\circ}$ for azimuth and
$2.54^{\circ}$ for elevation angle). Further, OrthoVortex can mitigate the
inter-modal interference, yielding more than 12 dB increase in
signal-to-interference ratio and more than 4.5-fold improvement in link
capacity.

</details>


### [8] [Space-Time Coded RIS-Assisted Wireless Systems with Practical Reflection Models: Error Rate Analysis and Negative Moment-Based Optimization with Saddle Point Approximation](https://arxiv.org/abs/2508.19129)
*Tayfun Yilmaz,Haci Ilhan,Ibrahim Hokelek*

Main category: eess.SP

TL;DR: 本文提出了RIS辅助多天线系统在实用硬件约束下的理论误码率分析框架，针对OSTBC编码，考虑了幅度相关和量化相位的实际反射模型。


<details>
  <summary>Details</summary>
Motivation: RIS辅助通信在挑战性环境中增强无线性能受到广泛关注，但在实际硬件约束下进行准确的误码分析对未来多天线系统至关重要。

Method: 利用级联信道f的Gramian结构，推导小规模RIS的非零特征值精确MGF表达式；对于大规模RIS，采用鞍点近似来近似特征值分布；基于这些结果推导统一的SER表达式。

Result: 广泛的蒙特卡洛仿真验证了所提SER表达式的准确性，在所有配置下都表现出非常接近的一致性。

Conclusion: 该理论框架为任意RIS尺寸、相位配置以及相同/非相同幅度响应的RIS辅助系统提供了有效的误码率分析工具。

Abstract: RIS-assisted communication has recently attracted significant attention for
enhancing wireless performance in challenging environments, making accurate
error analysis under practical hardware constraints crucial for future
multi-antenna systems. This paper presents a theoretical framework for SER
analysis of RIS-assisted multiple antenna systems employing OSTBC under
practical reflection models with amplitude-dependent and quantized phase
responses. By exploiting the Gramian structure of the cascaded channel f, we
derive exact MGF expressions of the nonzero eigenvalue of f'f for small RIS
sizes. For large-scale RIS deployments, where closed-form analysis becomes
intractable, we employ Saddle Point Approximation to approximate the eigenvalue
distribution. Using these results, we derive unified SER expressions using
exact and SPA-based MGF formulations, applicable to arbitrary RIS sizes, phase
configuration, and both identical and non-identical amplitude responses.
Extensive Monte Carlo simulations confirm the accuracy of the proposed SER
expressions, demonstrating very close agreement for all configurations.

</details>


### [9] [Instantaneous Polarimetry with Zak-OTFS](https://arxiv.org/abs/2508.19185)
*Nishant Mehrotra,Sandesh Rao Mattu,Robert Calderbank*

Main category: eess.SP

TL;DR: 提出了一种基于Zak-OTFS调制的瞬时极化测量方法，通过在正交极化上同时传输相互无偏的载波波形和扩展载波波形，实现单帧传输内完成全极化响应估计，计算复杂度仅为时间带宽积的次线性。


<details>
  <summary>Details</summary>
Motivation: 极化测量对提升无线通信和雷达系统性能至关重要，但现有瞬时极化测量方法计算复杂度高（时间带宽积的二次方），需要开发更高效的方法。

Method: 利用Zak-OTFS调制，在正交极化上同时传输相互无偏的载波波形和扩展载波波形，利用波形间的相互无偏性从单帧接收信号中估计散射环境的全极化响应。

Result: 数值仿真显示该方法能够实现理想的极化目标检测和参数估计，在性能和计算复杂度方面均优于可比基线方法。

Conclusion: 所提出的Zak-OTFS调制方法实现了高效的瞬时极化测量，计算复杂度显著降低（次线性于时间带宽积），为无线通信和雷达系统提供了实用的极化测量解决方案。

Abstract: Polarimetry, which is the ability to measure the scattering response of the
environment across orthogonal polarizations, is fundamental to enhancing
wireless communication and radar system performance. In this paper, we utilize
the Zak-OTFS modulation to enable instantaneous polarimetry within a single
transmission frame. We transmit a Zak-OTFS carrier waveform and a spread
carrier waveform mutually unbiased to it simultaneously over orthogonal
polarizations. The mutual unbiasedness of the two waveforms enables the
receiver to estimate the full polarimetric response of the scattering
environment from a single received frame. Unlike existing methods for
instantaneous polarimetry with computational complexity quadratic in the
time-bandwidth product, the proposed method enables instantaneous polarimetry
at complexity that is only sublinear in the time-bandwidth product. Via
numerical simulations, we show ideal polarimetric target detection and
parameter estimation results with the proposed method, with improvements in
performance and computational complexity over comparable baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs](https://arxiv.org/abs/2508.18279)
*Jeesu Jung,Sangkeun Jung*

Main category: cs.LG

TL;DR: 通过计算教师模型思维追踪中的离散步骤数量来定义思维深度(DoT)，并使用浅到深的课程排序进行大语言模型训练


<details>
  <summary>Details</summary>
Motivation: 需要一种与理性思维对齐、可扩展且可解释的难度信号来支持课程学习

Method: 将难度定义为思维深度(DoT)，通过计算教师模型的思维追踪(如链式思维)中的离散步骤数量来实现

Result: 提出了三个可测试假设：DoT与理性测试难度相关、DoT排序课程效果更好、难度在不同教师模型间稳健

Conclusion: 该方法期望向基于认知基础、可解释的课程排序方法发展，以支持以理性为中心的训练

Abstract: Curriculum learning for training LLMs requires a difficulty signal that
aligns with reasoning while remaining scalable and interpretable. We propose a
simple premise: tasks that demand deeper depth of thought for humans should
also be harder for models. Accordingly, we define difficulty as depth of
thought (DoT) and operationalize it by counting the discrete steps in a teacher
model's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow
to deep curriculum ordered by this DoT and outline how to derive, validate, and
schedule it at scale. Our position yields three testable hypotheses: (i) DoT
correlates with conventional difficulty on reasoning benchmarks, (ii)
DoT-ordered curricula outperform length- or judge-scored curricula under
matched budgets, and (iii) the difficulty is robust across teacher models given
light formatting controls. We propose an evaluation framework and discuss
threats to validity (teacher style, length confounds) alongside practical
mitigations. Taken together, we aim to move toward cognitively grounded,
interpretable curricula for reasoning-centric training.

</details>


### [11] [Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models](https://arxiv.org/abs/2508.18284)
*Rahmat K. Adesunkanmi,Alexander W. Brandt,Masoud Deylami,Gustavo A. Giraldo Echeverri,Hamidreza Karbasian,Adel Alaeddini*

Main category: cs.LG

TL;DR: 本文提出了一种多模态机器学习框架，通过结合句子Transformer嵌入和关注机制来预测海上浮体物的偏移，在搜救操作等时间敏感场景中实现了更准确的预测效果。


<details>
  <summary>Details</summary>
Motivation: 准确预测海上浮体物偏移对搜救操作至关重要，但现有方法遇到挑战。需要一种能够处理多源数据（环境、物理特征、文本描述）并提高预测准确性的方法。

Method: 采用多模态机器学习框架：首先通过实验收集环境和物理数据，使用Navier-Stokes模型训练卷积神经网络估计拉力和升力系数，然后将物理力、环境速度、物体特征和文本描述组合，使用关注机制的LSTM和Transformer模型预测偏移轨迹。

Result: 在多个时间距离（1、3、5、10秒）上评估，该多模态模型与传统机器学习方法表现相当，同时能够进行长期预测而非单步预测，并在不同物体间展现良好的泛化能力。

Conclusion: 多模态建模策略能够在动态海上环境中提供准确且适应性强的浮体物偏移预测，为搜救操作等应用场景提供有效支持。

Abstract: Accurately predicting the drift (displacement) of leeway objects in maritime
environments remains a critical challenge, particularly in time-sensitive
scenarios such as search and rescue operations. In this study, we propose a
multi-modal machine learning framework that integrates Sentence Transformer
embeddings with attention-based sequence-to-sequence architectures to predict
the drift of leeway objects in water. We begin by experimentally collecting
environmental and physical data, including water current and wind velocities,
object mass, and surface area, for five distinct leeway objects. Using
simulated data from a Navier-Stokes-based model to train a convolutional neural
network on geometrical image representations, we estimate drag and lift
coefficients of the leeway objects. These coefficients are then used to derive
the net forces responsible for driving the objects' motion. The resulting time
series, comprising physical forces, environmental velocities, and
object-specific features, combined with textual descriptions encoded via a
language model, are inputs to attention-based sequence-to-sequence
long-short-term memory and Transformer models, to predict future drift
trajectories. We evaluate the framework across multiple time horizons ($1$,
$3$, $5$, and $10$ seconds) and assess its generalization across different
objects. We compare our approach against a fitted physics-based model and
traditional machine learning methods, including recurrent neural networks and
temporal convolutional neural networks. Our results show that these multi-modal
models perform comparably to traditional models while also enabling longer-term
forecasting in place of single-step prediction. Overall, our findings
demonstrate the ability of a multi-modal modeling strategy to provide accurate
and adaptable predictions of leeway object drift in dynamic maritime
conditions.

</details>


### [12] [Data-driven models for production forecasting and decision supporting in petroleum reservoirs](https://arxiv.org/abs/2508.18289)
*Mateus A. Fernandes,Michael M. Furlanetti,Eduardo Gildin,Marcio A. Sampaio*

Main category: cs.LG

TL;DR: 基于机器学习的石油产量预测方法，使用生产和注入数据而非地质模型信息，通过回归和神经网络实现快速可靠的油藏动态预测


<details>
  <summary>Details</summary>
Motivation: 石油储层工程面临可靠预测产量和预判岩-流体系统行为变化的主要挑战，需要不依赖地质模型和流体属性的数据驱动方法

Method: 采用监督学习方法（回归和神经网络），进行变量相关性分析和数据预处理，特别关注概念漂移问题，使用合成数据和巴西盐下油田实际数据进行验证

Result: 开发出可靠的油藏动态预测器，具有快速响应能力，能处理井口限制和加工单元约束等实际问题

Conclusion: 该方法可用于支持储层管理决策，包括预测有害行为、优化生产和注入参数、分析概率事件影响，最终实现石油采收率最大化

Abstract: Forecasting production reliably and anticipating changes in the behavior of
rock-fluid systems are the main challenges in petroleum reservoir engineering.
This project proposes to deal with this problem through a data-driven approach
and using machine learning methods. The objective is to develop a methodology
to forecast production parameters based on simple data as produced and injected
volumes and, eventually, gauges located in wells, without depending on
information from geological models, fluid properties or details of well
completions and flow systems. Initially, we performed relevance analyses of the
production and injection variables, as well as conditioning the data to suit
the problem. As reservoir conditions change over time, concept drift is a
priority concern and require special attention to those observation windows and
the periodicity of retraining, which are also objects of study. For the
production forecasts, we study supervised learning methods, such as those based
on regressions and Neural Networks, to define the most suitable for our
application in terms of performance and complexity. In a first step, we
evaluate the methodology using synthetic data generated from the UNISIM III
compositional simulation model. Next, we applied it to cases of real plays in
the Brazilian pre-salt. The expected result is the design of a reliable
predictor for reproducing reservoir dynamics, with rapid response, capability
of dealing with practical difficulties such as restrictions in wells and
processing units, and that can be used in actions to support reservoir
management, including the anticipation of deleterious behaviors, optimization
of production and injection parameters and the analysis of the effects of
probabilistic events, aiming to maximize oil recovery.

</details>


### [13] [A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach](https://arxiv.org/abs/2508.18301)
*Md Sabbir Ahmed,Nova Ahmed*

Main category: cs.LG

TL;DR: 开发了一个快速抑郁症检测系统，仅需1秒收集7天应用使用数据，使用机器学习模型识别抑郁症，准确率达82.4%，适用于资源有限地区。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症检测系统需要长期数据收集，无法满足早期快速检测的需求，特别是在资源有限的欠发达和发展中地区。

Method: 开发快速工具收集7天应用使用数据（平均0.31秒），使用多种机器学习模型和特征选择方法（包括stable FS和Boruta），对100名孟加拉学生进行研究。

Result: 轻量级梯度提升机模型使用stable FS选择的重要特征，正确识别82.4%的抑郁症学生（精确度75%，F1分数78.5%）；简约堆叠模型使用Boruta选择的约5个特征，最大精确度达77.4%（平衡准确率77.9%）。

Conclusion: 该系统快速简约的特点使其在欠发达和发展中地区具有应用价值，研究发现有助于开发资源消耗更少的系统来更好地理解抑郁症学生。

Abstract: Background: Existing robust, pervasive device-based systems developed in
recent years to detect depression require data collected over a long period and
may not be effective in cases where early detection is crucial.
  Objective: Our main objective was to develop a minimalistic system to
identify depression using data retrieved in the fastest possible time.
  Methods: We developed a fast tool that retrieves the past 7 days' app usage
data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from
Bangladesh participated in our study, and our tool collected their app usage
data. To identify depressed and nondepressed students, we developed a diverse
set of ML models. We selected important features using the stable approach,
along with 3 main types of feature selection (FS) approaches.
  Results: Leveraging only the app usage data retrieved in 1 second, our light
gradient boosting machine model used the important features selected by the
stable FS approach and correctly identified 82.4% (n=42) of depressed students
(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we
presented a parsimonious stacking model where around 5 features selected by the
all-relevant FS approach Boruta were used in each iteration of validation and
showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis
of our best models presented behavioral markers that were related to
depression.
  Conclusions: Due to our system's fast and minimalistic nature, it may make a
worthwhile contribution to identifying depression in underdeveloped and
developing regions. In addition, our detailed discussion about the implication
of our findings can facilitate the development of less resource-intensive
systems to better understand students who are depressed.

</details>


### [14] [Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder](https://arxiv.org/abs/2508.18303)
*Jueqi Wang,Zachary Jacokes,John Darrell Van Horn,Michael C. Schatz,Kevin A. Pelphrey,Archana Venkataraman*

Main category: cs.LG

TL;DR: NeuroPathX是一个可解释的深度学习框架，通过交叉注意力机制融合脑部MRI结构变异和遗传学通路数据，在自闭症和阿尔茨海默病研究中优于传统方法，揭示了生物学上合理的关联。


<details>
  <summary>Details</summary>
Motivation: 传统成像遗传学方法局限于简单线性模型或缺乏可解释性的黑盒技术，无法有效捕捉脑部结构与遗传变异之间的复杂相互作用。

Method: 采用早期融合策略和交叉注意力机制，引入两个注意力矩阵损失函数：稀疏性损失聚焦最显著相互作用，通路相似性损失确保队列间表示一致性。

Result: 在自闭症谱系障碍和阿尔茨海默病验证中，NeuroPathX优于基线方法，揭示了与疾病相关的生物学合理关联。

Conclusion: NeuroPathX有潜力推动对复杂脑部疾病的理解，代码已开源。

Abstract: While imaging-genetics holds great promise for unraveling the complex
interplay between brain structure and genetic variation in neurological
disorders, traditional methods are limited to simplistic linear models or to
black-box techniques that lack interpretability. In this paper, we present
NeuroPathX, an explainable deep learning framework that uses an early fusion
strategy powered by cross-attention mechanisms to capture meaningful
interactions between structural variations in the brain derived from MRI and
established biological pathways derived from genetics data. To enhance
interpretability and robustness, we introduce two loss functions over the
attention matrix - a sparsity loss that focuses on the most salient
interactions and a pathway similarity loss that enforces consistent
representations across the cohort. We validate NeuroPathX on both autism
spectrum disorder and Alzheimer's disease. Our results demonstrate that
NeuroPathX outperforms competing baseline approaches and reveals biologically
plausible associations linked to the disorder. These findings underscore the
potential of NeuroPathX to advance our understanding of complex brain
disorders. Code is available at https://github.com/jueqiw/NeuroPathX .

</details>


### [15] [ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation](https://arxiv.org/abs/2508.18318)
*Yang Li,Hanjie Wang,Yuanzheng Li,Jiazheng Li,Zhaoyang Dong*

Main category: cs.LG

TL;DR: ZTFed-MAS2S是一个零信任联邦学习框架，用于风电数据缺失值填补，结合了可验证差分隐私、零知识证明和动态信任聚合机制


<details>
  <summary>Details</summary>
Motivation: 风电数据常因传感器故障和传输不稳定而缺失，传统联邦学习在开放工业环境中易受异常更新和隐私泄露攻击，需要零信任机制

Method: 集成多头注意力序列到序列填补模型，采用可验证差分隐私和非交互式零知识证明，结合动态信任感知聚合机制和压缩技术

Result: 在真实风电场数据集上的实验验证了ZTFed-MAS2S在联邦学习性能和缺失数据填补方面的优越性

Conclusion: 该框架为能源领域实际应用提供了一个安全高效的解决方案

Abstract: Wind power data often suffers from missing values due to sensor faults and
unstable transmission at edge sites. While federated learning enables
privacy-preserving collaboration without sharing raw data, it remains
vulnerable to anomalous updates and privacy leakage during parameter exchange.
These challenges are amplified in open industrial environments, necessitating
zero-trust mechanisms where no participant is inherently trusted. To address
these challenges, this work proposes ZTFed-MAS2S, a zero-trust federated
learning framework that integrates a multi-head attention-based
sequence-to-sequence imputation model. ZTFed integrates verifiable differential
privacy with non-interactive zero-knowledge proofs and a confidentiality and
integrity verification mechanism to ensure verifiable privacy preservation and
secure model parameters transmission. A dynamic trust-aware aggregation
mechanism is employed, where trust is propagated over similarity graphs to
enhance robustness, and communication overhead is reduced via sparsity- and
quantization-based compression. MAS2S captures long-term dependencies in wind
power data for accurate imputation. Extensive experiments on real-world wind
farm datasets validate the superiority of ZTFed-MAS2S in both federated
learning performance and missing data imputation, demonstrating its
effectiveness as a secure and efficient solution for practical applications in
the energy sector.

</details>


### [16] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 提出了SALMAN框架，通过距离映射失真(DMD)度量来评估Transformer语言模型的局部鲁棒性，无需修改模型参数或复杂扰动启发式


<details>
  <summary>Details</summary>
Motivation: 随着预训练Transformer模型规模增大和部署增多，其输入扰动下的鲁棒性成为紧迫问题。现有方法在小参数和大规模模型间存在差异，且通常依赖劳动密集型的样本特定对抗设计

Method: 提出统一局部鲁棒性框架SALMAN，采用新颖的距离映射失真(DMD)度量，通过比较输入到输出的距离映射来评估样本易感性，具有近线性复杂度

Result: 在攻击效率和鲁棒训练方面取得显著提升

Conclusion: 该框架可作为实用的模型无关工具，提升基于Transformer的NLP系统的可靠性

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [17] [Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods](https://arxiv.org/abs/2508.18307)
*Mahishanka Withanachchi*

Main category: cs.LG

TL;DR: 提出了一个结合算子值再生核希尔伯特空间(OV-RKHS)和基于核的Koopman算子方法的统一框架，用于学习向量值函数的时空动力学，实现非参数化数据驱动的复杂时变向量场估计。


<details>
  <summary>Details</summary>
Motivation: 需要开发一种能够同时保持时空结构、支持高维非线性系统高效降阶建模和长期预测的理论基础工具，用于时空机器学习中的预测、控制和不确定性量化。

Method: 结合算子值再生核希尔伯特空间(OV-RKHS)与基于核的Koopman算子方法，建立时间依赖OV-RKHS插值的表示定理，推导光滑向量场的Sobolev型逼近边界，并提供核Koopman算子近似的谱收敛保证。

Result: 开发了一个统一的非参数化数据驱动框架，能够有效估计复杂时变向量场，同时保持时空结构，支持高维系统的降阶建模和长期预测。

Conclusion: 该框架为时空机器学习提供了理论基础的预测、控制和不确定性量化工具，在保持时空结构的同时实现了复杂系统的高效建模和预测。

Abstract: We introduce a unified framework for learning the spatio-temporal dynamics of
vector valued functions by combining operator valued reproducing kernel Hilbert
spaces (OV-RKHS) with kernel based Koopman operator methods. The approach
enables nonparametric and data driven estimation of complex time evolving
vector fields while preserving both spatial and temporal structure. We
establish representer theorems for time dependent OV-RKHS interpolation, derive
Sobolev type approximation bounds for smooth vector fields, and provide
spectral convergence guarantees for kernel Koopman operator approximations.
This framework supports efficient reduced order modeling and long term
prediction of high dimensional nonlinear systems, offering theoretically
grounded tools for forecasting, control, and uncertainty quantification in
spatio-temporal machine learning.

</details>


### [18] [CoPE: A Lightweight Complex Positional Encoding](https://arxiv.org/abs/2508.18308)
*Avinash Amballa*

Main category: cs.LG

TL;DR: CoPE是一种轻量级复数位置编码方法，用复数嵌入替代传统位置编码，实部捕获语义内容，虚部编码位置信息，在GLUE基准测试中表现优异且计算复杂度更低


<details>
  <summary>Details</summary>
Motivation: 位置编码在Transformer架构中已被证明有效，但传统方法存在长期衰减问题且计算复杂度较高，需要一种更高效的位置编码方案

Method: 提出CoPE复数位置编码架构，使用复数嵌入（实部为语义内容，虚部为位置信息），在第一层引入相位感知注意力机制捕获位置依赖模式，后续层使用标准注意力

Result: 在GLUE基准测试中表现优于RoPE、正弦和可学习位置编码，且计算复杂度更低，不出现长期衰减现象，兼容线性注意力

Conclusion: CoPE提供了一种高效的位置编码解决方案，通过复数表示同时编码内容和位置信息，在保持性能的同时降低了计算复杂度

Abstract: Recent studies have demonstrated the effectiveness of position encoding in
transformer architectures. By incorporating positional information, this
approach provides essential guidance for modeling dependencies between elements
across different sequence positions. We introduce CoPE (a lightweight Complex
Positional Encoding), a novel architecture that leverages complex-valued
encoding to encode both content and positional information. Our approach
replaces traditional positional encodings with complex embeddings where the
real part captures semantic content and the imaginary part encodes positional
information. We introduce phase-aware attention in the first layer of the
transformer model to capture position-dependent patterns, followed by standard
attention layers for higher-levels. We show that CoPE doesn't exhibit long term
decay and is compatible with linear attention. Experimental evaluation on the
GLUE benchmark suggest that our approach achieves superior performance with
less computational complexity, compared to RoPE, Sinusoidal and Learned
positional encodings.

</details>


### [19] [What Matters in Data for DPO?](https://arxiv.org/abs/2508.18312)
*Yu Pan,Zhongze Cai,Guanting Chen,Huaiyang Zhong,Chonghuan Wang*

Main category: cs.LG

TL;DR: DPO性能主要取决于被选中回答的质量，而被拒绝回答的质量影响相对有限。理论分析显示对比性主要通过提升选中样本质量来帮助优化，在线DPO设置实际上简化为对选中回答的监督微调。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO被广泛采用，但偏好数据分布如何影响其性能的基本问题仍未解决。需要系统研究偏好数据特征对DPO的影响，为构建高效对齐数据集提供指导。

Method: 从理论和实证角度系统研究偏好数据分布对DPO的影响，包括理论分析最优响应分布、在线DPO设置研究，以及在多样化任务上进行广泛实验验证。

Result: 实验证实提升选中回答质量能持续提升性能，无论被拒绝回答质量如何。混合在线策略数据也有益处，这些发现解释了广泛采用策略的机制。

Conclusion: 研究揭示了DPO工作机制，为构建高效LLM对齐偏好数据集提供了实用见解，强调应优先提升选中回答质量而非过度关注对比性。

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
approach for aligning large language models (LLMs) with human preferences,
bypassing the need for a learned reward model. Despite its growing adoption, a
fundamental question remains open: what characteristics of preference data are
most critical for DPO performance? In this work, we provide a systematic study
of how preference data distribution influences DPO, from both theoretical and
empirical perspectives. We show that the quality of chosen responses plays a
dominant role in optimizing the DPO objective, while the quality of rejected
responses may have relatively limited impact. Our theoretical analysis
characterizes the optimal response distribution under DPO and reveals how
contrastiveness between responses helps primarily by improving the chosen
samples. We further study an online DPO setting and show it effectively reduces
to supervised fine-tuning on the chosen responses. Extensive experiments across
diverse tasks confirm our findings: improving the quality of chosen responses
consistently boosts performance regardless of the quality of the rejected
responses. We also investigate the benefit of mixing the on-policy data. Our
results interpret the mechanism behind some widely adopted strategies and offer
practical insights for constructing high-impact preference datasets for LLM
alignment.

</details>


### [20] [Estimating oil recovery factor using machine learning: Applications of XGBoost classification](https://arxiv.org/abs/2210.16345)
*Alireza Roustazadeh,Behzad Ghanbarian,Frank Male,Mohammad B. Shadmand,Vahid Taslimitehrani,Larry W. Lake*

Main category: cs.LG

TL;DR: 使用XGBoost分类算法通过机器学习预测石油取出回收因子类别，最高准确率达到0.49，但模型效果依赖于训练数据库。


<details>
  <summary>Details</summary>
Motivation: 在石油开发的早期阶段，缺乏足够数据来准确估算最终取出回收因子，需要通过机器学习方法利用易获得的特征进行预测。

Method: 采用XGBoost分类算法，将回收因子划分为10个类别。合并三个数据库形成四种数据组合，使用10折交叉验证评估模型效果，并通过准确率、邻近准确率和f1指标进行评估。

Result: 模型在训练数据集上准确率最高为0.49，测试数据集为0.34，独立数据库为0.2。最重要的特征是储量、油藏面积和嵌度。

Conclusion: XGBoost分类算法可以合理地估计石油取出回收因子类别，但模型的可靠性依赖于训练数据库的质量和组成。

Abstract: In petroleum engineering, it is essential to determine the ultimate recovery
factor, RF, particularly before exploitation and exploration. However,
accurately estimating requires data that is not necessarily available or
measured at early stages of reservoir development. We, therefore, applied
machine learning (ML), using readily available features, to estimate oil RF for
ten classes defined in this study. To construct the ML models, we applied the
XGBoost classification algorithm. Classification was chosen because recovery
factor is bounded from 0 to 1, much like probability. Three databases were
merged, leaving us with four different combinations to first train and test the
ML models and then further evaluate them using an independent database
including unseen data. The cross-validation method with ten folds was applied
on the training datasets to assess the effectiveness of the models. To evaluate
the accuracy and reliability of the models, the accuracy, neighborhood
accuracy, and macro averaged f1 score were determined. Overall, results showed
that the XGBoost classification algorithm could estimate the RF class with
reasonable accuracies as high as 0.49 in the training datasets, 0.34 in the
testing datasets and 0.2 in the independent databases used. We found that the
reliability of the XGBoost model depended on the data in the training dataset
meaning that the ML models were database dependent. The feature importance
analysis and the SHAP approach showed that the most important features were
reserves and reservoir area and thickness.

</details>


### [21] [ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions](https://arxiv.org/abs/2508.18313)
*Zi Cai,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: ProtoEHR是一个可解释的分层原型学习框架，充分利用EHR数据的多层次结构来提升医疗预测性能，在多个临床任务上表现优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往只关注电子健康记录(EHR)数据的孤立组成部分，限制了预测性能和可解释性。需要开发能够充分利用EHR丰富多层次结构的框架。

Method: 利用大语言模型提取医学代码的语义关系构建医学知识图谱，设计分层表示学习框架捕获三个层次(医学代码、医院就诊、患者)的上下文表示，并在每个层次融入原型信息来捕捉内在相似性和提升泛化能力。

Result: 在两个公开数据集上对五个临床重要任务进行评估，包括死亡率预测、再入院预测、住院时长预测、药物推荐和表型预测，结果显示ProtoEHR相比基线方法能够做出更准确、鲁棒和可解释的预测。

Conclusion: ProtoEHR框架能够充分利用EHR数据的多层次结构，在提升预测性能的同时提供代码、就诊和患者三个层次的可解释性洞察，有助于医疗预测决策。

Abstract: Digital healthcare systems have enabled the collection of mass healthcare
data in electronic healthcare records (EHRs), allowing artificial intelligence
solutions for various healthcare prediction tasks. However, existing studies
often focus on isolated components of EHR data, limiting their predictive
performance and interpretability. To address this gap, we propose ProtoEHR, an
interpretable hierarchical prototype learning framework that fully exploits the
rich, multi-level structure of EHR data to enhance healthcare predictions. More
specifically, ProtoEHR models relationships within and across three
hierarchical levels of EHRs: medical codes, hospital visits, and patients. We
first leverage large language models to extract semantic relationships among
medical codes and construct a medical knowledge graph as the knowledge source.
Building on this, we design a hierarchical representation learning framework
that captures contextualized representations across three levels, while
incorporating prototype information within each level to capture intrinsic
similarities and improve generalization. To perform a comprehensive assessment,
we evaluate ProtoEHR in two public datasets on five clinically significant
tasks, including prediction of mortality, prediction of readmission, prediction
of length of stay, drug recommendation, and prediction of phenotype. The
results demonstrate the ability of ProtoEHR to make accurate, robust, and
interpretable predictions compared to baselines in the literature. Furthermore,
ProtoEHR offers interpretable insights on code, visit, and patient levels to
aid in healthcare prediction.

</details>


### [22] [Federated Learning with Heterogeneous and Private Label Sets](https://arxiv.org/abs/2508.18774)
*Adam Breitholtz,Edvin Listo Zec,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本文研究联邦学习中客户端标签集异构性问题，比较了公开标签和私有标签设置对模型性能的影响，提出了适应私有标签集的联邦学习方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中客户端标签集异构性很常见但研究较少，特别是当客户端只与中央服务器共享私有标签集时，这会增加学习算法的约束，是一个更难解决的问题。

Method: 应用分类器组合问题的经典方法进行联邦学习中心化调优，调整常见联邦学习方法以适应私有标签集设置，并在实际假设下讨论两种方法的合理性。

Result: 实验显示减少每个客户端可用标签数量会显著损害所有方法的性能。中心化调优客户端模型进行表示对齐可以改善性能，但通常以更高方差为代价。提出的标准联邦学习方法适应版本在私有标签设置中表现良好，性能与标准方法在公开设置中相当。

Conclusion: 客户端可以在几乎不损失模型准确性的情况下享受更高的隐私保护，私有标签集联邦学习是可行的解决方案。

Abstract: Although common in real-world applications, heterogeneous client label sets
are rarely investigated in federated learning (FL). Furthermore, in the cases
they are, clients are assumed to be willing to share their entire label sets
with other clients. Federated learning with private label sets, shared only
with the central server, adds further constraints on learning algorithms and
is, in general, a more difficult problem to solve. In this work, we study the
effects of label set heterogeneity on model performance, comparing the public
and private label settings -- when the union of label sets in the federation is
known to clients and when it is not. We apply classical methods for the
classifier combination problem to FL using centralized tuning, adapt common FL
methods to the private label set setting, and discuss the justification of both
approaches under practical assumptions. Our experiments show that reducing the
number of labels available to each client harms the performance of all methods
substantially. Centralized tuning of client models for representational
alignment can help remedy this, but often at the cost of higher variance.
Throughout, our proposed adaptations of standard FL methods perform well,
showing similar performance in the private label setting as the standard
methods achieve in the public setting. This shows that clients can enjoy
increased privacy at little cost to model accuracy.

</details>


### [23] [Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing](https://arxiv.org/abs/2508.18316)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 基于民联学习框架的机器学习模型，通过早期学业表现和数字参与模式预测远程教育中的高风险学生，达到了85% ROC AUC的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 远程教育中高的退学和失败率对学术机构构成重大挑战，需要提前识别高风险学生以提供及时支持。同时需要解决数据隐私和机构数据孤岛等实践挑战。

Method: 使用OULAD大规模数据集，基于早期学业表现和数字参与模式建立机器学习模型。采用民联学习（FL）框架解决数据隐私问题，比较了逻辑回归和深度神经网络两种模型复杂度，以及不同的数据平衡方法。

Result: 最终的民联学习模型显示出强大的预测能力，在识别高风险学生方面达到了约85%的ROC AUC分数。

Conclusion: 民联学习方法为学术机构提供了一种实用且可扩展的解决方案，能够建立有效的早期预警系统，在本质上尊重数据隐私的同时支持主动学生支持。

Abstract: High dropout and failure rates in distance education pose a significant
challenge for academic institutions, making the proactive identification of
at-risk students crucial for providing timely support. This study develops and
evaluates a machine learning model based on early academic performance and
digital engagement patterns from the large-scale OULAD dataset to predict
student risk at a UK university. To address the practical challenges of data
privacy and institutional silos that often hinder such initiatives, we
implement the model using a Federated Learning (FL) framework. We compare model
complexity (Logistic Regression vs. a Deep Neural Network) and data balancing.
The final federated model demonstrates strong predictive capability, achieving
an ROC AUC score of approximately 85% in identifying at-risk students. Our
findings show that this federated approach provides a practical and scalable
solution for institutions to build effective early-warning systems, enabling
proactive student support while inherently respecting data privacy.

</details>


### [24] [Composition and Alignment of Diffusion Models using Constrained Learning](https://arxiv.org/abs/2508.19104)
*Shervin Khalafi,Ignacio Hounie,Dongsheng Ding,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出了一种约束优化框架，统一扩散模型的alignment和composition，通过强制满足奖励约束和保持与预训练模型的接近性来解决多奖励优化中的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化多个奖励或组合多个模型时存在权衡问题，无法保证生成样本具有所有期望属性。需要一种能够有效满足约束条件的方法。

Method: 提出了约束优化框架，通过拉格朗日对偶训练算法来近似解决约束对齐和组合问题，确保对齐模型满足奖励约束并保持与预训练模型的接近性。

Result: 在图像生成任务中验证了方法的有效性，显示约束满足效果良好，且在同等权重方法基础上有所改进。

Conclusion: 该框架为扩散模型的alignment和composition提供了一种有效的约束优化解决方案，能够更好地满足多属性需求。

Abstract: Diffusion models have become prevalent in generative modeling due to their
ability to sample from complex distributions. To improve the quality of
generated samples and their compliance with user requirements, two commonly
used methods are: (i) Alignment, which involves fine-tuning a diffusion model
to align it with a reward; and (ii) Composition, which combines several
pre-trained diffusion models, each emphasizing a desirable attribute in the
generated outputs. However, trade-offs often arise when optimizing for multiple
rewards or combining multiple models, as they can often represent competing
properties. Existing methods cannot guarantee that the resulting model
faithfully generates samples with all the desired properties. To address this
gap, we propose a constrained optimization framework that unifies alignment and
composition of diffusion models by enforcing that the aligned model satisfies
reward constraints and/or remains close to (potentially multiple) pre-trained
models. We provide a theoretical characterization of the solutions to the
constrained alignment and composition problems and develop a Lagrangian-based
primal-dual training algorithm to approximate these solutions. Empirically, we
demonstrate the effectiveness and merits of our proposed approach in image
generation, applying it to alignment and composition, and show that our aligned
or composed model satisfies constraints effectively, and improves on the
equally-weighted approach. Our implementation can be found at
https://github.com/shervinkhalafi/constrained_comp_align.

</details>


### [25] [Understanding Tool-Integrated Reasoning](https://arxiv.org/abs/2508.19201)
*Heng Lin,Zhongwen Xu*

Main category: cs.LG

TL;DR: 本文首次从理论上证明了工具集成推理(TIR)能够严格扩展大语言模型的能力边界，通过Python解释器等工具解锁纯文本模型无法实现的问题解决策略，并提出ASPO算法来优化工具使用行为。


<details>
  <summary>Details</summary>
Motivation: 虽然工具集成的大语言模型显示出巨大潜力，但缺乏解释为什么这种范式有效的理论原理。本文旨在填补这一空白，提供TIR有效性的首个形式化证明。

Method: 引入优势塑造策略优化(ASPO)算法直接修改优势函数来引导策略行为，在数学基准测试中使用Python解释器作为外部工具进行综合实验。

Result: TIR模型在pass@k指标上显著优于纯文本模型，优势不仅限于计算密集型问题，还扩展到需要抽象洞察力的问题。ASPO带来了更早的代码调用和更多交互轮次的改进工具使用行为。

Conclusion: 本研究首次为TIR的成功提供了原理性解释，将关注点从工具是否有效转向了工具如何以及为什么能够实现更强大的推理能力。

Abstract: We study why Tool-Integrated Reasoning (TIR) makes Large Language Models
(LLMs) more capable. While LLMs integrated with tools like Python code
interpreters show great promise, a principled theory explaining why this
paradigm is effective has been missing. This work provides the first formal
proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that
tools enable a strict expansion of the model's empirical and feasible support,
breaking the capability ceiling of pure-text models by unlocking
problem-solving strategies that are otherwise impossible or intractably
verbose. To guide model behavior without compromising training stability and
performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a
novel algorithm that directly modifies the advantage function to guide the
policy behavior. We conduct comprehensive experiments on challenging
mathematical benchmarks, leveraging a Python interpreter as the external tool.
Our results show that the TIR model decisively outperforms its pure-text
counterpart on the pass@k metric. Crucially, this advantage is not confined to
computationally-intensive problems but extends to those requiring significant
abstract insight. We further identify the emergent cognitive patterns that
illustrate how models learn to think with tools. Finally, we report improved
tool usage behavior with early code invocation and much more interactive turns
with ASPO. Overall, our work provides the first principled explanation for
TIR's success, shifting the focus from the mere fact that tools work to why and
how they enable more powerful reasoning.

</details>


### [26] [Linear cost mutual information estimation and independence test of similar performance as HSIC](https://arxiv.org/abs/2508.18338)
*Jarek Duda,Jagoda Bracha,Adrian Przybysz*

Main category: cs.LG

TL;DR: HSIC方法计算复杂度高，HCR提出线性成本替代方案，通过混合矩特征描述依赖关系，提供联合分布模型和互信息近似


<details>
  <summary>Details</summary>
Motivation: HSIC方法在处理大规模数据时计算复杂度过高（O(n^2.37)），需要开发更高效的统计依赖性评估方法

Method: 提出HCR（分层相关重建）方法，使用混合矩特征描述数据依赖关系，从相关性和同方差性开始，单特征计算时间为线性O(n)

Result: HCR不仅计算成本低（线性复杂度），而且在测试中表现出比HSIC更高的依赖性敏感性，同时提供实际的联合分布模型

Conclusion: HCR是HSIC的高效替代方案，能够以线性成本处理大规模数据，通过混合矩特征有效描述统计依赖关系并近似互信息

Abstract: Evaluation of statistical dependencies between two data samples is a basic
problem of data science/machine learning, and HSIC (Hilbert-Schmidt Information
Criterion)~\cite{HSIC} is considered the state-of-art method. However, for size
$n$ data sample it requires multiplication of $n\times n$ matrices, what
currently needs $\sim O(n^{2.37})$ computational complexity~\cite{mult}, making
it impractical for large data samples. We discuss HCR (Hierarchical Correlation
Reconstruction) as its linear cost practical alternative of even higher
dependence sensitivity in tests, and additionally providing actual joint
distribution model by description of dependencies through features being mixed
moments, starting with correlation and homoscedasticity, also allowing to
approximate mutual information as just sum of squares of such nontrivial mixed
moments between two data samples. Such single dependence describing feature is
calculated in $O(n)$ linear time. Their number to test varies with dimension
$d$ - requiring $O(d^2)$ for pairwise dependencies, $O(d^3)$ if wanting to also
consider more subtle triplewise, and so on.

</details>


### [27] [DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction](https://arxiv.org/abs/2508.18376)
*Weilin Cai,Le Qin,Shwai He,Junwei Cui,Ang Li,Jiayi Huang*

Main category: cs.LG

TL;DR: DualSparse-MoE通过后训练专家分区和动态张量级计算丢弃技术，在保持模型精度的同时显著提升MoE模型的推理效率


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然通过稀疏激活减少计算量，但仍面临大规模计算和不可预测激活模式的挑战，需要更高效的部署方案

Method: 提出后训练专家分区方法诱导张量级稀疏性，结合动态张量级计算丢弃和静态神经元级重建的双重稀疏技术

Result: 25%丢弃率下平均精度仅下降0.08%-0.28%，计算速度提升与丢弃程度成正比，负载均衡感知实现1.41倍加速且精度下降仅0.5%

Conclusion: 双重稀疏方法有效解决了MoE模型部署的效率挑战，在最小精度损失下实现了显著的计算加速

Abstract: Mixture of Experts (MoE) has become a mainstream architecture for building
Large Language Models (LLMs) by reducing per-token computation while enabling
model scaling. It can be viewed as partitioning a large Feed-Forward Network
(FFN) at the tensor level into fine-grained sub-FFNs, or experts, and
activating only a sparse subset for each input. While this sparsity improves
efficiency, MoE still faces substantial challenges due to their massive
computational scale and unpredictable activation patterns.
  To enable efficient MoE deployment, we identify dual sparsity at the tensor
and neuron levels in pre-trained MoE modules as a key factor for both accuracy
and efficiency. Unlike prior work that increases tensor-level sparsity through
finer-grained expert design during pre-training, we introduce post-training
expert partitioning to induce such sparsity without retraining. This preserves
the mathematical consistency of model transformations and enhances both
efficiency and accuracy in subsequent fine-tuning and inference. Building upon
this, we propose DualSparse-MoE, an inference system that integrates dynamic
tensor-level computation dropping with static neuron-level reconstruction to
deliver significant efficiency gains with minimal accuracy loss.
  Experimental results show that enforcing an approximate 25% drop rate with
our approach reduces average accuracy by only 0.08%-0.28% across three
prevailing MoE models, while nearly all degrees of computation dropping
consistently yield proportional computational speedups. Furthermore,
incorporating load-imbalance awareness into expert parallelism achieves a 1.41x
MoE module speedup with just 0.5% average accuracy degradation.

</details>


### [28] [Low-Rank Tensor Decompositions for the Theory of Neural Networks](https://arxiv.org/abs/2508.18408)
*Ricardo Borsoi,Konstantin Usevich,Marianne Clausel*

Main category: cs.LG

TL;DR: 本文综述了低秩张量分解在深度神经网络理论中的基础作用，包括表达能力、可学习性、计算复杂性、泛化能力和可识别性等方面的理论解释。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络性能突破促使数学理论基础的建立，低秩张量分解因其与神经网络的紧密联系和丰富理论结果而特别适合此任务。

Method: 通过系统综述不同社区（计算机科学、数学等）的方法，以统一连贯的方式展示低秩张量分解如何解释深度神经网络的各种理论方面。

Result: 低秩张量分解为深度神经网络的理论理解提供了重要工具，支持了表达能力、学习算法、计算复杂性等多个关键方面的理论进展。

Conclusion: 低秩张量分解在深度神经网络理论中发挥着基础性作用，本文旨在以统一视角综述现有方法，并为该领域开辟更广阔的研究视角。

Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge
of interest in providing a mathematical basis to deep learning theory. Low-rank
tensor decompositions are specially befitting for this task due to their close
connection to NNs and their rich theoretical results. Different tensor
decompositions have strong uniqueness guarantees, which allow for a direct
interpretation of their factors, and polynomial time algorithms have been
proposed to compute them. Through the connections between tensors and NNs, such
results supported many important advances in the theory of NNs. In this review,
we show how low-rank tensor methods--which have been a core tool in the signal
processing and machine learning communities--play a fundamental role in
theoretically explaining different aspects of the performance of deep NNs,
including their expressivity, algorithmic learnability and computational
hardness, generalization, and identifiability. Our goal is to give an
accessible overview of existing approaches (developed by different communities,
ranging from computer science to mathematics) in a coherent and unified way,
and to open a broader perspective on the use of low-rank tensor decompositions
for the theory of deep NNs.

</details>


### [29] [LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning](https://arxiv.org/abs/2508.18420)
*André Quadros,Cassio Silva,Ronnie Alves*

Main category: cs.LG

TL;DR: 结合VSIMR和LLM内在奖励策略，在稀疏奖励环境中显著提升强化学习代理的性能和采样效率


<details>
  <summary>Details</summary>
Motivation: 解决极端稀疏奖励环境中传统强化学习因正反馈稀少而学习困难的问题

Method: 集成VSIMR（使用VAE奖励状态新颖性）和基于LLM的内在奖励方法，在MiniGrid DoorKey环境中使用A2C代理实现

Result: 组合策略相比单独使用任一策略或标准A2C代理，显著提高了代理性能和采样效率

Conclusion: VSIMR驱动新状态探索，LLM奖励促进目标导向的渐进式利用，两者互补有效提升学习效果

Abstract: This paper explores the combination of two intrinsic motivation strategies to
improve the efficiency of reinforcement learning (RL) agents in environments
with extreme sparse rewards, where traditional learning struggles due to
infrequent positive feedback. We propose integrating Variational State as
Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward
state novelty, with an intrinsic reward approach derived from Large Language
Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward
signals based on environment and goal descriptions, guiding the agent. We
implemented this combined approach with an Actor-Critic (A2C) agent in the
MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical
results show that this combined strategy significantly increases agent
performance and sampling efficiency compared to using each strategy
individually or a standard A2C agent, which failed to learn. Analysis of
learning curves indicates that the combination effectively complements
different aspects of the environment and task: VSIMR drives exploration of new
states, while the LLM-derived rewards facilitate progressive exploitation
towards goals.

</details>


### [30] [Enhancing Trust-Region Bayesian Optimization via Newton Methods](https://arxiv.org/abs/2508.18423)
*Quanlin Chen,Yiyu Chen,Jing Huo,Tianyu Ding,Yang Gao,Yuetong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于全局高斯过程的多局部二次模型方法，通过梯度和Hessian矩阵构建局部模型，解决了高维贝叶斯优化中的采样效率问题，提升了TuRBO方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在高维空间中面临挑战，TuRBO方法使用局部高斯过程虽然能避免过度探索，但采样效率相比全局高斯过程有所降低。需要一种既能保持异质建模又能提高采样效率的方法。

Method: 使用全局高斯过程的梯度和Hessian矩阵构建多个局部二次模型，通过求解边界约束二次规划问题来选择新的采样点，同时解决了高维空间中高斯过程梯度消失的问题。

Result: 通过收敛性分析和实验验证，该方法显著提升了TuRBO的效能，在合成函数和实际应用中均优于多种高维贝叶斯优化技术。

Conclusion: 提出的多局部二次模型方法成功解决了高维贝叶斯优化的采样效率问题，在保持异质建模能力的同时显著提升了优化性能，为高维黑盒函数优化提供了有效解决方案。

Abstract: Bayesian Optimization (BO) has been widely applied to optimize expensive
black-box functions while retaining sample efficiency. However, scaling BO to
high-dimensional spaces remains challenging. Existing literature proposes
performing standard BO in multiple local trust regions (TuRBO) for
heterogeneous modeling of the objective function and avoiding over-exploration.
Despite its advantages, using local Gaussian Processes (GPs) reduces sampling
efficiency compared to a global GP. To enhance sampling efficiency while
preserving heterogeneous modeling, we propose to construct multiple local
quadratic models using gradients and Hessians from a global GP, and select new
sample points by solving the bound-constrained quadratic program. Additionally,
we address the issue of vanishing gradients of GPs in high-dimensional spaces.
We provide a convergence analysis and demonstrate through experimental results
that our method enhances the efficacy of TuRBO and outperforms a wide range of
high-dimensional BO techniques on synthetic functions and real-world
applications.

</details>


### [31] [VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning](https://arxiv.org/abs/2508.18462)
*Fu Teng,Miao Pan,Xuhong Zhang,Zhezhi He,Yiyao Yang,Xinyi Chai,Mengnan Qi,Liqiang Lu,Jianwei Yin*

Main category: cs.LG

TL;DR: 提出了一个针对Verilog代码生成的强化学习框架，包含高质量数据集构建、奖励机制改进和平衡学习策略，在硬件描述语言生成任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 硬件描述语言（如Verilog）由于并发语义、语法刚性和仿真复杂性，在代码生成领域研究不足，需要专门的方法来解决这些挑战。

Method: 构建Veribench-53K高质量数据集；提出基于回溯的重新评分机制处理稀疏噪声奖励信号；引入样本平衡权重策略防止灾难性遗忘；建立迭代RL管道共同演化策略和奖励模型。

Result: 在Verilog生成任务上实现了最先进的性能，在测试通过率、功能正确性和编译鲁棒性方面都有显著提升。

Conclusion: 强化学习方法在硬件领域的结构化代码生成中具有巨大潜力，使用较小但高质量的数据集结合RL优化可以超越依赖大规模闭源模型蒸馏的方法。

Abstract: Recent advancements in code generation have shown remarkable success across
software domains, yet hardware description languages (HDLs) such as Verilog
remain underexplored due to their concurrency semantics, syntactic rigidity,
and simulation complexity. In this work, we address these challenges by
introducing a reinforcement learning (RL) framework tailored for Verilog code
generation. We first construct Veribench-53K, a high-quality dataset curated
from over 700K Verilog problems, enriched with structured prompts, complexity
labels, and diverse testbenches. To tackle the problem of sparse and noisy
reward signals, we propose a Trace-back based Rescore mechanism that leverages
reasoning paths and iterative refinement to enhance feedback reliability and
support reward model training. Furthermore, to mitigate catastrophic forgetting
and overfitting during RL fine-tuning, we introduce a sample-balanced weighting
strategy that adaptively balances learning dynamics based on reward-probability
distributions. These innovations are integrated into an iterative RL pipeline
that co-evolves the policy and reward models. In contrast to recent work such
as CraftRTL, which relies on large-scale closed-source model distillation, and
DeepSeek-style approaches that struggle with sparse feedback, our method
demonstrates superior performance using a smaller but high-quality dataset
combined with RL optimization. Experiments on Verilog generation tasks
demonstrate state-of-the-art performance, with substantial gains in test pass
rate, functional correctness, and compilation robustness. Our findings
highlight the potential of RL-driven approaches for structured code generation
in hardware-centric domains. VERIRL is publicly available at
https://github.com/omniAI-Lab/VeriRL.

</details>


### [32] [DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection](https://arxiv.org/abs/2508.18474)
*Bahareh Golchin,Banafsheh Rekabdar,Kunpeng Liu*

Main category: cs.LG

TL;DR: 提出基于强化学习的异常检测框架DRTA，结合动态奖励塑造、VAE和主动学习，在低标签系统中有效检测时间序列异常


<details>
  <summary>Details</summary>
Motivation: 传统方法在有限标签数据、高误报率和泛化新异常类型方面存在困难，需要更有效的异常检测解决方案

Method: 使用强化学习框架，集成动态奖励塑造机制（平衡VAE重构误差和分类奖励）、变分自编码器和主动学习

Result: 在Yahoo A1和A2基准数据集上表现优于最先进的无监督和半监督方法，保持高精度和召回率

Conclusion: 该框架是现实世界异常检测任务的可扩展高效解决方案

Abstract: Anomaly detection in time series data is important for applications in
finance, healthcare, sensor networks, and industrial monitoring. Traditional
methods usually struggle with limited labeled data, high false-positive rates,
and difficulty generalizing to novel anomaly types. To overcome these
challenges, we propose a reinforcement learning-based framework that integrates
dynamic reward shaping, Variational Autoencoder (VAE), and active learning,
called DRTA. Our method uses an adaptive reward mechanism that balances
exploration and exploitation by dynamically scaling the effect of VAE-based
reconstruction error and classification rewards. This approach enables the
agent to detect anomalies effectively in low-label systems while maintaining
high precision and recall. Our experimental results on the Yahoo A1 and Yahoo
A2 benchmark datasets demonstrate that the proposed method consistently
outperforms state-of-the-art unsupervised and semi-supervised approaches. These
findings show that our framework is a scalable and efficient solution for
real-world anomaly detection tasks.

</details>


### [33] [Data Augmentation Improves Machine Unlearning](https://arxiv.org/abs/2508.18502)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: cs.LG

TL;DR: 本研究探讨了数据增强策略对机器遗忘性能的影响，发现适当的增强设计能显著提升遗忘效果，TrivialAug增强可将平均差距指标降低40.12%。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在从训练模型中移除特定数据的影响，但系统性的数据增强设计在遗忘中的作用尚未得到充分研究。

Method: 在CIFAR-10和CIFAR-100数据集上，使用不同遗忘率测试了SalUn、随机标签和微调等遗忘方法，并比较了不同数据增强策略的效果。

Result: 实验表明，适当的数据增强设计能显著改善遗忘效果，使用TrivialAug增强时平均差距指标最多降低40.12%，缩小了与重新训练模型的性能差距。

Conclusion: 数据增强不仅有助于减少记忆效应，还在实现隐私保护和高效遗忘方面发挥关键作用，系统性的增强设计对机器遗忘性能有重要影响。

Abstract: Machine Unlearning (MU) aims to remove the influence of specific data from a
trained model while preserving its performance on the remaining data. Although
a few works suggest connections between memorisation and augmentation, the role
of systematic augmentation design in MU remains under-investigated. In this
work, we investigate the impact of different data augmentation strategies on
the performance of unlearning methods, including SalUn, Random Label, and
Fine-Tuning. Experiments conducted on CIFAR-10 and CIFAR-100, under varying
forget rates, show that proper augmentation design can significantly improve
unlearning effectiveness, reducing the performance gap to retrained models.
Results showed a reduction of up to 40.12% of the Average Gap unlearning
Metric, when using TrivialAug augmentation. Our results suggest that
augmentation not only helps reduce memorization but also plays a crucial role
in achieving privacy-preserving and efficient unlearning.

</details>


### [34] [Breaking Through Barren Plateaus: Reinforcement Learning Initializations for Deep Variational Quantum Circuits](https://arxiv.org/abs/2508.18514)
*Yifeng Peng,Xinyi Li,Zhemin Zhang,Samuel Yen-Chi Chen,Zhiding Liang,Ying Wang*

Main category: cs.LG

TL;DR: 提出基于强化学习的参数初始化策略，通过预训练避免梯度消失区域，显著提升变分量子算法的收敛速度和最终解质量


<details>
  <summary>Details</summary>
Motivation: 变分量子算法(VQAs)在近期量子设备应用中面临梯度消失问题(barren plateau)，导致训练困难，需要有效方法来缓解这一问题

Method: 使用多种强化学习算法(DPG、SAC、PPO等)生成电路参数作为动作，在标准梯度优化前最小化VQAs成本函数进行预训练

Result: 在各种噪声条件和任务下的数值实验表明，RL初始化方法显著提高了收敛速度和最终解质量，多种RL算法都能获得可比性能提升

Conclusion: RL驱动的参数初始化为量子算法设计提供了有前景的集成机器学习技术途径，能够加速VQAs的可扩展性和实际部署

Abstract: Variational Quantum Algorithms (VQAs) have gained prominence as a viable
framework for exploiting near-term quantum devices in applications ranging from
optimization and chemistry simulation to machine learning. However, the
effectiveness of VQAs is often constrained by the so-called barren plateau
problem, wherein gradients diminish exponentially as system size or circuit
depth increases, thereby hindering training. In this work, we propose a
reinforcement learning (RL)-based initialization strategy to alleviate the
barren plateau issue by reshaping the initial parameter landscape to avoid
regions prone to vanishing gradients. In particular, we explore several RL
algorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal
Policy Optimization, etc.) to generate the circuit parameters (treated as
actions) that minimize the VQAs cost function before standard gradient-based
optimization. By pre-training with RL in this manner, subsequent optimization
using methods such as gradient descent or Adam proceeds from a more favorable
initial state. Extensive numerical experiments under various noise conditions
and tasks consistently demonstrate that the RL-based initialization method
significantly enhances both convergence speed and final solution quality.
Moreover, comparisons among different RL algorithms highlight that multiple
approaches can achieve comparable performance gains, underscoring the
flexibility and robustness of our method. These findings shed light on a
promising avenue for integrating machine learning techniques into quantum
algorithm design, offering insights into how RL-driven parameter initialization
can accelerate the scalability and practical deployment of VQAs. Opening up a
promising path for the research community in machine learning for quantum,
especially barren plateau problems in VQAs.

</details>


### [35] [Quantifying The Limits of AI Reasoning: Systematic Neural Network Representations of Algorithms](https://arxiv.org/abs/2508.18526)
*Anastasis Kratsios,Dennis Zvigelsky,Bradd Hart*

Main category: cs.LG

TL;DR: 该论文提出了一种将任意电路转换为ReLU神经网络的方法，证明神经网络可以精确模拟任何推理任务，无需近似或舍入。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在完美训练后能执行何种形式推理的核心问题，量化神经网络的推理能力。

Method: 提出元算法，通过迭代将电路中的每个门替换为规范的ReLU MLP模拟器，构建前馈神经网络来精确模拟原电路。

Result: 神经网络可以精确模拟任何数字电路，包括布尔逻辑、动态规划、数学表示等推理任务，参数量与电路复杂度成正比。

Conclusion: 神经网络能够模拟任何推理任务，结果比经典通用逼近定理更强大，形式化了神经网络用空间复杂度换取算法运行时间的权衡。

Abstract: A main open question in contemporary AI research is quantifying the forms of
reasoning neural networks can perform when perfectly trained. This paper
answers this by interpreting reasoning tasks as circuit emulation, where the
gates define the type of reasoning; e.g. Boolean gates for predicate logic,
tropical circuits for dynamic programming, arithmetic and analytic gates for
symbolic mathematical representation, and hybrids thereof for deeper reasoning;
e.g. higher-order logic.
  We present a systematic meta-algorithm that converts essentially any circuit
into a feedforward neural network (NN) with ReLU activations by iteratively
replacing each gate with a canonical ReLU MLP emulator. We show that, on any
digital computer, our construction emulates the circuit exactly--no
approximation, no rounding, modular overflow included--demonstrating that no
reasoning task lies beyond the reach of neural networks. The number of neurons
in the resulting network (parametric complexity) scales with the circuit's
complexity, and the network's computational graph (structure) mirrors that of
the emulated circuit. This formalizes the folklore that NNs networks trade
algorithmic run-time (circuit runtime) for space complexity (number of
neurons).
  We derive a range of applications of our main result, from emulating
shortest-path algorithms on graphs with cubic--size NNs, to simulating stopped
Turing machines with roughly quadratically--large NNs, and even the emulation
of randomized Boolean circuits. Lastly, we demonstrate that our result is
strictly more powerful than a classical universal approximation theorem: any
universal function approximator can be encoded as a circuit and directly
emulated by a NN.

</details>


### [36] [BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration](https://arxiv.org/abs/2508.18551)
*Jun Hou,Le Wang,Xuan Wang*

Main category: cs.LG

TL;DR: BTW是一个无需额外参数的双层权重框架，通过KL散度和互信息动态调整多模态学习中各模态的重要性，有效处理噪声模态问题


<details>
  <summary>Details</summary>
Motivation: 现有方法如部分信息分解难以扩展到两个以上模态，且缺乏实例级控制能力，无法有效处理引入噪声而非补充信息的额外模态

Method: 结合实例级KL散度（衡量单模态与多模态预测差异）和模态级互信息（估计全局对齐）的双层非参数权重框架，可应用于任意数量模态

Result: 在情感回归和临床分类任务上的大量实验表明，该方法显著提升了回归性能和多类分类准确率

Conclusion: BTW框架有效解决了多模态学习中噪声模态问题，提供了可扩展的实例级模态重要性动态调整方案

Abstract: Mixture-of-Experts (MoE) models have become increasingly powerful in
multimodal learning by enabling modular specialization across modalities.
However, their effectiveness remains unclear when additional modalities
introduce more noise than complementary information. Existing approaches, such
as the Partial Information Decomposition, struggle to scale beyond two
modalities and lack the resolution needed for instance-level control. We
propose Beyond Two-modality Weighting (BTW), a bi-level, non-parametric
weighting framework that combines instance-level Kullback-Leibler (KL)
divergence and modality-level mutual information (MI) to dynamically adjust
modality importance during training. Our method does not require additional
parameters and can be applied to an arbitrary number of modalities.
Specifically, BTW computes per-example KL weights by measuring the divergence
between each unimodal and the current multimodal prediction, and modality-wide
MI weights by estimating global alignment between unimodal and multimodal
outputs. Extensive experiments on sentiment regression and clinical
classification demonstrate that our method significantly improves regression
performance and multiclass classification accuracy.

</details>


### [37] [Enhancing Chemical Explainability Through Counterfactual Masking](https://arxiv.org/abs/2508.18561)
*Łukasz Janisiów,Marek Kochańczyk,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: 提出了一种名为counterfactual masking的新框架，用于分子属性预测的可解释性分析，通过用生成模型生成的合理化学片段替换被遮蔽的子结构，提供更真实和可操作的解释。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释性方法通常使用遮蔽策略来评估重要性，但这些方法往往不符合分子分布，产生不直观的解释。需要一种能够保持化学合理性的解释方法。

Method: 使用生成模型训练完成分子图，采样化学合理的片段来替换被遮蔽的子结构，而不是简单地置零。通过评估与从数据分布中抽取的反事实分子的相对预测来提供解释。

Result: 该方法在多个数据集和属性预测任务中表现出色，能够提供更稳健、分布一致的解释，并产生有意义的反事实，直接指示结构修改如何影响预测属性。

Conclusion: counterfactual masking方法弥合了可解释性与分子设计之间的差距，为化学中的可解释机器学习提供了一条原则性和生成性的路径。

Abstract: Molecular property prediction is a crucial task that guides the design of new
compounds, including drugs and materials. While explainable artificial
intelligence methods aim to scrutinize model predictions by identifying
influential molecular substructures, many existing approaches rely on masking
strategies that remove either atoms or atom-level features to assess importance
via fidelity metrics. These methods, however, often fail to adhere to the
underlying molecular distribution and thus yield unintuitive explanations. In
this work, we propose counterfactual masking, a novel framework that replaces
masked substructures with chemically reasonable fragments sampled from
generative models trained to complete molecular graphs. Rather than evaluating
masked predictions against implausible zeroed-out baselines, we assess them
relative to counterfactual molecules drawn from the data distribution. Our
method offers two key benefits: (1) molecular realism underpinning robust and
distribution-consistent explanations, and (2) meaningful counterfactuals that
directly indicate how structural modifications may affect predicted properties.
We demonstrate that counterfactual masking is well-suited for benchmarking
model explainers and yields more actionable insights across multiple datasets
and property prediction tasks. Our approach bridges the gap between
explainability and molecular design, offering a principled and generative path
toward explainable machine learning in chemistry.

</details>


### [38] [A Note on Graphon-Signal Analysis of Graph Neural Networks](https://arxiv.org/abs/2508.18564)
*Levi Rauchwerger,Ron Levie*

Main category: cs.LG

TL;DR: 这篇论文对Levie的图神经网络分析进行了五项重要扩展：多维信号扩展、带readout的Lipschitz连续性、更优的汇聚界、非对称图论扩展，提高了理论分析的实践适用性。


<details>
  <summary>Details</summary>
Motivation: 充实图神经网络的图论分析理论，解决Levie论文在实践应用中的限制，使理论更加适用于实际图机器学习场景。

Method: 通过四个方向扩展：1)将图论-信号扩展到多维信号空间 2)扩展带readout的MPNN在cut distance下的Lipschitz连续性 3)利用稳健性汇聚界改进汇聚性界 4)将分析扩展到非对称图论和内核。

Result: 构建了更完整的图神经网络图论分析框架，提供了更强的汇聚性保障和更广泛的应用范围，为实践图机器学习提供了更坚实的理论基础。

Conclusion: 该研究完善了图神经网络的图论分析理论，解决了原有方法的实践限制，为更广泛的图机器学习应用场景提供了理论支撑和分析工具。

Abstract: A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', by
Levie, analyzed message passing graph neural networks (MPNNs) by embedding the
input space of MPNNs, i.e., attributed graphs (graph-signals), to a space of
attributed graphons (graphon-signals). Based on extensions of standard results
in graphon analysis to graphon-signals, the paper proved a generalization bound
and a sampling lemma for MPNNs. However, there are some missing ingredients in
that paper, limiting its applicability in practical settings of graph machine
learning. In the current paper, we introduce several refinements and extensions
to existing results that address these shortcomings. In detail, 1) we extend
the main results in the paper to graphon-signals with multidimensional signals
(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs with
readout with respect to cut distance (rather than MPNNs without readout with
respect to cut metric), 3) we improve the generalization bound by utilizing
robustness-type generalization bounds, and 4) we extend the analysis to
non-symmetric graphons and kernels.

</details>


### [39] [Improving Long-term Autoregressive Spatiotemporal Predictions: A Proof of Concept with Fluid Dynamics](https://arxiv.org/abs/2508.18565)
*Hao Zhou,Sibo Cheng*

Main category: cs.LG

TL;DR: SPF框架通过随机推送策略结合真实数据和模型预测，在保持单步训练的同时实现多步学习，解决了自回归方法内存消耗大和长期精度下降的问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值预测方法计算成本高，数据驱动方法虽然推理快但长期精度会下降，自回归训练需要大量GPU内存且可能牺牲短期性能。

Method: 提出随机推送前向（SPF）框架，通过从模型预测构建补充数据集，采用随机采集策略结合真实数据，在训练期间预计算多步预测以避免存储完整展开序列。

Result: 在Burgers方程和浅水基准测试中，SPF比自回归方法获得更高的长期精度，同时显著降低内存需求。

Conclusion: SPF框架在资源受限和复杂模拟场景中具有良好应用前景，能够平衡短期和长期性能并减少过拟合。

Abstract: Data-driven methods are emerging as efficient alternatives to traditional
numerical forecasting, offering fast inference and lower computational cost.
Yet, for complex systems, long-term accuracy often deteriorates due to error
accumulation, and autoregressive training (though effective) demands large GPU
memory and may sacrifice short-term performance. We propose the Stochastic
PushForward (SPF) framework, which retains one-step-ahead training while
enabling multi-step learning. SPF builds a supplementary dataset from model
predictions and combines it with ground truth via a stochastic acquisition
strategy, balancing short- and long-term performance while reducing
overfitting. Multi-step predictions are precomputed between epochs, keeping
memory usage stable without storing full unrolled sequences. Experiments on the
Burgers' equation and the Shallow Water benchmark show that SPF achieves higher
long-term accuracy than autoregressive methods while lowering memory
requirements, making it promising for resource-limited and complex simulations.

</details>


### [40] [Sparse Autoencoders for Low-$N$ Protein Function Prediction and Design](https://arxiv.org/abs/2508.18567)
*Darin Tsui,Kunal Talreja,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: 稀疏自编码器(SAEs)在低数据量蛋白质功能预测中表现优于ESM2基线，仅需24个序列即可实现更好的适应度预测，并在83%的情况下生成更高适应度的蛋白质变体


<details>
  <summary>Details</summary>
Motivation: 解决在数据稀缺情况下从氨基酸序列预测蛋白质功能的挑战，特别是在只有少量标记序列功能数据时的机器学习引导蛋白质设计问题

Method: 使用在微调ESM2嵌入上训练的稀疏自编码器(SAEs)，评估其在多样化适应度外推和蛋白质工程任务中的表现

Result: SAEs在适应度预测中始终优于或与ESM2基线竞争，稀疏潜在空间编码了紧凑且具有生物学意义的表示，能够从有限数据中更有效地泛化

Conclusion: SAEs为低数据量蛋白质功能预测和设计提供了有效解决方案，通过利用pLM表示中的生物基序，显著提高了蛋白质设计的成功率

Abstract: Predicting protein function from amino acid sequence remains a central
challenge in data-scarce (low-$N$) regimes, limiting machine learning-guided
protein design when only small amounts of assay-labeled sequence-function data
are available. Protein language models (pLMs) have advanced the field by
providing evolutionary-informed embeddings and sparse autoencoders (SAEs) have
enabled decomposition of these embeddings into interpretable latent variables
that capture structural and functional features. However, the effectiveness of
SAEs for low-$N$ function prediction and protein design has not been
systematically studied. Herein, we evaluate SAEs trained on fine-tuned ESM2
embeddings across diverse fitness extrapolation and protein engineering tasks.
We show that SAEs, with as few as 24 sequences, consistently outperform or
compete with their ESM2 baselines in fitness prediction, indicating that their
sparse latent space encodes compact and biologically meaningful representations
that generalize more effectively from limited data. Moreover, steering
predictive latents exploits biological motifs in pLM representations, yielding
top-fitness variants in 83% of cases compared to designing with ESM2 alone.

</details>


### [41] [DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model](https://arxiv.org/abs/2508.18579)
*Mohammadreza Ghaffarzadeh-Esfahani,Ali Motahharynia,Nahid Yousefian,Navid Mazrouei,Jafar Ghaisari,Yousof Gheisari*

Main category: cs.LG

TL;DR: DrugReasoner是一个基于LLaMA架构的推理型大语言模型，通过GRPO微调来预测小分子药物批准可能性，在保持竞争力的预测准确性的同时提供可解释的推理过程。


<details>
  <summary>Details</summary>
Motivation: 药物发现过程复杂且资源密集，需要早期预测批准结果来优化研究投资。传统机器学习和深度学习方法在药物批准预测中表现出潜力，但可解释性有限限制了其影响。

Method: 基于LLaMA架构构建DrugReasoner模型，使用GRPO进行微调，整合分子描述符与结构相似化合物的比较推理，生成预测结果的同时提供逐步推理过程和置信度评分。

Result: 在验证集上AUC为0.732，F1分数为0.729；测试集上AUC为0.725，F1分数为0.718。在外部独立数据集上AUC为0.728，F1分数为0.774，优于传统基线方法和ChemAP模型。

Conclusion: DrugReasoner不仅提供竞争力的预测准确性，还通过推理输出增强透明度，解决了AI辅助药物发现中的关键瓶颈，展示了推理增强型LLM作为可解释有效药物决策工具的潜力。

Abstract: Drug discovery is a complex and resource-intensive process, making early
prediction of approval outcomes critical for optimizing research investments.
While classical machine learning and deep learning methods have shown promise
in drug approval prediction, their limited interpretability constraints their
impact. Here, we present DrugReasoner, a reasoning-based large language model
(LLM) built on the LLaMA architecture and fine-tuned with group relative policy
optimization (GRPO) to predict the likelihood of small-molecule approval.
DrugReasoner integrates molecular descriptors with comparative reasoning
against structurally similar approved and unapproved compounds, generating
predictions alongside step-by-step rationales and confidence scores.
DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score
of 0.729 on the validation set and 0.725 and 0.718 on the test set,
respectively. These results outperformed conventional baselines, including
logistic regression, support vector machine, and k-nearest neighbors and had
competitive performance relative to XGBoost. On an external independent
dataset, DrugReasoner outperformed both baseline and the recently developed
ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while
maintaining high precision and balanced sensitivity, demonstrating robustness
in real-world scenarios. These findings demonstrate that DrugReasoner not only
delivers competitive predictive accuracy but also enhances transparency through
its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug
discovery. This study highlights the potential of reasoning-augmented LLMs as
interpretable and effective tools for pharmaceutical decision-making.

</details>


### [42] [History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL](https://arxiv.org/abs/2508.18588)
*Jingkai He,Tianjian Li,Erhu Feng,Dong Du,Qian Liu,Tao Liu,Yubin Xia,Haibo Chen*

Main category: cs.LG

TL;DR: RhymeRL是一个针对大语言模型强化学习的系统，通过历史相似性驱动的推测解码和两级调度策略，解决了GPU利用率低的问题，实现了2.6倍的性能提升且不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 当前RL系统存在严重的GPU利用率不足问题，主要原因是rollout阶段占主导地位以及rollout长度不平衡导致的GPU气泡。传统异步执行和截断方法虽然能部分缓解，但可能牺牲训练准确性。

Method: 提出RhymeRL系统，包含两个核心创新：1) HistoSpec推测解码推理引擎，利用历史rollout token序列的相似性生成准确草稿；2) HistoPipe两级调度策略，利用历史rollout分布的相似性平衡rollout工作负载。

Result: 在真实生产环境中评估，展示了从几十到数千个GPU的可扩展性。实验结果表明，RhymeRL相比现有方法实现了2.6倍的性能提升，且不损害准确性或修改RL范式。

Conclusion: 基于历史相似性的洞察为LLM RL训练提供了有效的加速方案，RhymeRL系统在保持训练准确性的同时显著提升了GPU利用率和整体性能。

Abstract: With the rapid advancement of large language models (LLMs), reinforcement
learning (RL) has emerged as a pivotal methodology for enhancing the reasoning
capabilities of LLMs. Unlike traditional pre-training approaches, RL
encompasses multiple stages: rollout, reward, and training, which necessitates
collaboration among various worker types. However, current RL systems continue
to grapple with substantial GPU underutilization, due to two primary factors:
(1) The rollout stage dominates the overall RL process due to test-time
scaling; (2) Imbalances in rollout lengths (within the same batch) result in
GPU bubbles. While prior solutions like asynchronous execution and truncation
offer partial relief, they may compromise training accuracy for efficiency.
  Our key insight stems from a previously overlooked observation: rollout
responses exhibit remarkable similarity across adjacent training epochs. Based
on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate
RL training with two key innovations. First, to enhance rollout generation, we
present HistoSpec, a speculative decoding inference engine that utilizes the
similarity of historical rollout token sequences to obtain accurate drafts.
Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier
scheduling strategy that leverages the similarity of historical rollout
distributions to balance workload among rollout workers. We have evaluated
RhymeRL within a real production environment, demonstrating scalability from
dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL
achieves a 2.6x performance improvement over existing methods, without
compromising accuracy or modifying the RL paradigm.

</details>


### [43] [Linear Trading Position with Sparse Spectrum](https://arxiv.org/abs/2508.18596)
*Zhao-Rong Lai,Haisheng Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The principal portfolio approach is an emerging method in signal-based
trading. However, these principal portfolios may not be diversified to explore
the key features of the prediction matrix or robust to different situations. To
address this problem, we propose a novel linear trading position with sparse
spectrum that can explore a larger spectral region of the prediction matrix. We
also develop a Krasnosel'ski\u \i-Mann fixed-point algorithm to optimize this
trading position, which possesses the descent property and achieves a linear
convergence rate in the objective value. This is a new theoretical result for
this type of algorithms. Extensive experiments show that the proposed method
achieves good and robust performance in various situations.

</details>


### [44] [Uncertainty Awareness on Unsupervised Domain Adaptation for Time Series Data](https://arxiv.org/abs/2508.18630)
*Weide Liu,Xiaoyang Zhong,Lu Wang,Jingwen Hou,Yuemei Luo,Jiebin Yan,Yuming Fang*

Main category: cs.LG

TL;DR: 本文提出了一种结合多尺度特征提取和不确定性估计的无监督域自适应方法，用于处理时间序列数据中的分布偏移问题，在多个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据中训练和测试数据集之间经常出现分布偏移，传统无监督域自适应方法在处理这类问题时泛化能力有限，需要提高模型在不同域间的泛化性和鲁棒性。

Method: 采用多尺度混合输入架构捕获不同尺度的特征，增加训练多样性并减少域间特征差异；基于证据学习引入不确定性感知机制，通过对标签施加Dirichlet先验来实现目标预测和不确定性估计。

Result: 该方法在目标域取得了显著的性能提升，不确定性感知模型表现出更低的预期校准误差(ECE)，表明预测置信度校准更好，在多个基准数据集上达到了最先进的性能。

Conclusion: 多尺度混合输入架构与不确定性感知机制的结合方法在时间序列数据的无监督域自适应中非常有效，能够显著提升模型在不同域间的泛化能力和鲁棒性。

Abstract: Unsupervised domain adaptation methods seek to generalize effectively on
unlabeled test data, especially when encountering the common challenge in time
series data that distribution shifts occur between training and testing
datasets. In this paper, we propose incorporating multi-scale feature
extraction and uncertainty estimation to improve the model's generalization and
robustness across domains. Our approach begins with a multi-scale mixed input
architecture that captures features at different scales, increasing training
diversity and reducing feature discrepancies between the training and testing
domains. Based on the mixed input architecture, we further introduce an
uncertainty awareness mechanism based on evidential learning by imposing a
Dirichlet prior on the labels to facilitate both target prediction and
uncertainty estimation. The uncertainty awareness mechanism enhances domain
adaptation by aligning features with the same labels across different domains,
which leads to significant performance improvements in the target domain.
Additionally, our uncertainty-aware model demonstrates a much lower Expected
Calibration Error (ECE), indicating better-calibrated prediction confidence.
Our experimental results show that this combined approach of mixed input
architecture with the uncertainty awareness mechanism achieves state-of-the-art
performance across multiple benchmark datasets, underscoring its effectiveness
in unsupervised domain adaptation for time series data.

</details>


### [45] [STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning](https://arxiv.org/abs/2508.18635)
*Yue Jiang,Chenxi Liu,Yile Chen,Qin Chao,Shuai Liu,Gao Cong*

Main category: cs.LG

TL;DR: STRATA-TS是一个针对时间序列数据稀缺城市的预测框架，通过选择性迁移学习和检索增强推理来改善城市预测性能


<details>
  <summary>Details</summary>
Motivation: 城市预测面临严重的数据不平衡问题：少数城市有密集的长期记录，而许多其他城市只有短期或不完整的历史数据。直接迁移学习不可靠，因为只有有限的源模式真正有益于目标域，而不加选择的迁移可能引入噪声和负迁移

Method: STRATA-TS结合领域自适应检索和推理能力强大的大模型，使用基于补丁的时间编码器识别与目标查询语义和动态对齐的源子序列，然后将检索到的样本注入检索引导的推理阶段，由LLM对目标输入和检索支持进行结构化推理，最后通过监督微调将推理过程蒸馏到紧凑的开放模型中

Result: 在新加坡、诺丁汉和格拉斯哥的三个停车可用性数据集上的广泛实验表明，STRATA-TS始终优于强大的预测和迁移基线，同时提供可解释的知识迁移路径

Conclusion: STRATA-TS框架通过选择性迁移学习和检索增强推理，有效解决了城市预测中的数据稀缺问题，在保持可解释性的同时显著提升了预测性能

Abstract: Urban forecasting models often face a severe data imbalance problem: only a
few cities have dense, long-span records, while many others expose short or
incomplete histories. Direct transfer from data-rich to data-scarce cities is
unreliable because only a limited subset of source patterns truly benefits the
target domain, whereas indiscriminate transfer risks introducing noise and
negative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware
retrieval for Time Series), a framework that combines domain-adapted retrieval
with reasoning-capable large models to improve forecasting in scarce data
regimes. STRATA-TS employs a patch-based temporal encoder to identify source
subsequences that are semantically and dynamically aligned with the target
query. These retrieved exemplars are then injected into a retrieval-guided
reasoning stage, where an LLM performs structured inference over target inputs
and retrieved support. To enable efficient deployment, we distill the reasoning
process into a compact open model via supervised fine-tuning. Extensive
experiments on three parking availability datasets across Singapore,
Nottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms
strong forecasting and transfer baselines, while providing interpretable
knowledge transfer pathways.

</details>


### [46] [Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance](https://arxiv.org/abs/2508.18638)
*Ifrah Tariq,Ernest Fraenkel*

Main category: cs.LG

TL;DR: BDVAE是一种新型深度生成模型，通过整合转录组和基因组数据，利用模块化编码器和变分推理学习生物学有意义的潜在特征，准确预测免疫检查点抑制剂治疗反应并揭示耐药机制。


<details>
  <summary>Details</summary>
Motivation: 免疫检查点抑制剂(ICIs)治疗反应存在高度变异性，现有机器学习模型缺乏可解释性且未能有效利用多组学数据的生物学结构，需要开发能够揭示耐药生物学机制的可解释预测模型。

Method: 提出生物学解缠变分自编码器(BDVAE)，使用模态特异性和通路特异性编码器整合转录组和基因组数据，通过变分推理学习与免疫、基因组和代谢过程相关的生物学有意义的潜在特征。

Result: 在366名跨4种癌症类型的患者队列中，BDVAE准确预测治疗反应(测试数据AUC-ROC=0.94)，揭示了免疫抑制、代谢转变和神经元信号等关键耐药机制，发现耐药存在于连续生物学谱系而非严格二元状态。

Conclusion: BDVAE证明了生物学结构化机器学习在阐明复杂耐药模式和指导精准免疫治疗策略方面的价值，能够生成可解释且临床相关的生物学见解。

Abstract: Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet
patient responses remain highly variable, and the biological mechanisms
underlying resistance are poorly understood. While machine learning models hold
promise for predicting responses to ICIs, most existing methods lack
interpretability and do not effectively leverage the biological structure
inherent to multi-omics data. Here, we introduce the Biologically Disentangled
Variational Autoencoder (BDVAE), a deep generative model that integrates
transcriptomic and genomic data through modality- and pathway-specific
encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a
modular encoder architecture combined with variational inference to learn
biologically meaningful latent features associated with immune, genomic, and
metabolic processes. Applied to a pan-cancer cohort of 366 patients across four
cancer types treated with ICIs, BDVAE accurately predicts treatment response
(AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance
mechanisms, including immune suppression, metabolic shifts, and neuronal
signaling. Importantly, BDVAE reveals that resistance spans a continuous
biological spectrum rather than strictly binary states, reflecting gradations
of tumor dysfunction. Several latent features correlate with survival outcomes
and known clinical subtypes, demonstrating BDVAE's capability to generate
interpretable, clinically relevant insights. These findings underscore the
value of biologically structured machine learning in elucidating complex
resistance patterns and guiding precision immunotherapy strategies.

</details>


### [47] [The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for Forecasting Market Volatility and Enhancing Market Interpretability](https://arxiv.org/abs/2508.18653)
*Xiaoliang Chen,Xin Yu,Le Chang,Teng Jing,Jiashuai He,Ze Wang,Yangjun Luo,Xingyu Chen,Jiayue Liang,Yuchen Wang,Jiaying Xie*

Main category: cs.LG

TL;DR: 提出了一种新颖的多模态金融风险评估框架，结合文本情感分析和基于声学物理模型的执行层声音特征分析，能够有效预测股票波动性而非方向性收益


<details>
  <summary>Details</summary>
Motivation: 金融市场信息不对称问题严重，传统文本分析在战略性的企业叙事面前效果有限，需要从多模态角度捕捉更真实的情绪信号

Method: 开发了物理信息声学模型(PIAM)，应用非线性声学原理从原始电话会议音频中提取情绪特征，将声学和文本情绪状态映射到三维情感状态标签空间(紧张度、稳定性和唤醒度)

Result: 基于1,795次财报电话会议数据，多模态特征能够解释高达43.8%的30天实现波动率样本外方差，特别是在执行层从脚本演讲转向即兴问答时的情绪动态变化具有最强预测力

Conclusion: 多模态方法显著优于纯财务基准，声学和文本模态具有互补性，为投资者和监管机构提供了识别隐藏企业不确定性的强大工具

Abstract: Information asymmetry in financial markets, often amplified by strategically
crafted corporate narratives, undermines the effectiveness of conventional
textual analysis. We propose a novel multimodal framework for financial risk
assessment that integrates textual sentiment with paralinguistic cues derived
from executive vocal tract dynamics in earnings calls. Central to this
framework is the Physics-Informed Acoustic Model (PIAM), which applies
nonlinear acoustics to robustly extract emotional signatures from raw
teleconference sound subject to distortions such as signal clipping. Both
acoustic and textual emotional states are projected onto an interpretable
three-dimensional Affective State Label (ASL) space-Tension, Stability, and
Arousal. Using a dataset of 1,795 earnings calls (approximately 1,800 hours),
we construct features capturing dynamic shifts in executive affect between
scripted presentation and spontaneous Q&A exchanges. Our key finding reveals a
pronounced divergence in predictive capacity: while multimodal features do not
forecast directional stock returns, they explain up to 43.8% of the
out-of-sample variance in 30-day realized volatility. Importantly, volatility
predictions are strongly driven by emotional dynamics during executive
transitions from scripted to spontaneous speech, particularly reduced textual
stability and heightened acoustic instability from CFOs, and significant
arousal variability from CEOs. An ablation study confirms that our multimodal
approach substantially outperforms a financials-only baseline, underscoring the
complementary contributions of acoustic and textual modalities. By decoding
latent markers of uncertainty from verifiable biometric signals, our
methodology provides investors and regulators a powerful tool for enhancing
market interpretability and identifying hidden corporate uncertainty.

</details>


### [48] [FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge](https://arxiv.org/abs/2508.18663)
*Gang Hu,Yinglei Teng,Pengfei Wu,Nan Wang*

Main category: cs.LG

TL;DR: 提出了FFT MoE框架，用稀疏专家混合（MoE）适配器替代LoRA，解决联邦微调中的异构性和非IID数据问题，通过门控网络和异构感知辅助损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型向AGI发展，在隐私和资源约束下进行微调变得至关重要。联邦学习通过联邦微调实现协作模型适配，但现有LoRA方法在异构FL环境中存在结构不兼容和非IID数据适应性问题。

Method: 提出FFT MoE框架：1）用MoE适配器替换LoRA；2）客户端训练轻量级门控网络选择专家子集；3）引入异构感知辅助损失动态正则化路由分布，确保专家多样性和平衡利用。

Result: 在IID和非IID条件下的广泛实验表明，FFT MoE在泛化性能和训练效率方面持续优于最先进的联邦微调基线方法。

Conclusion: FFT MoE框架有效解决了异构联邦学习环境中的结构兼容性和非IID数据适应性问题，通过MoE适配器和智能路由机制实现了更好的收敛性和泛化能力。

Abstract: As FMs drive progress toward Artificial General Intelligence (AGI),
fine-tuning them under privacy and resource constraints has become increasingly
critical particularly when highquality training data resides on distributed
edge devices. Federated Learning (FL) offers a compelling solution through
Federated Fine-Tuning (FFT), which enables collaborative model adaptation
without sharing raw data. Recent approaches incorporate Parameter-Efficient
Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce
computational overhead. However, LoRA-based FFT faces two major limitations in
heterogeneous FL environments: structural incompatibility across clients with
varying LoRA configurations and limited adaptability to non-IID data
distributions, which hinders convergence and generalization. To address these
challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with
sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight
gating network to selectively activate a personalized subset of experts,
enabling fine-grained adaptation to local resource budgets while preserving
aggregation compatibility. To further combat the expert load imbalance caused
by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary
loss that dynamically regularizes the routing distribution to ensure expert
diversity and balanced utilization. Extensive experiments spanning both IID and
non-IID conditions demonstrate that FFT MoE consistently outperforms state of
the art FFT baselines in generalization performance and training efficiency.

</details>


### [49] [Auditing Approximate Machine Unlearning for Differentially Private Models](https://arxiv.org/abs/2508.18671)
*Yuechun Gu,Jiajie He,Keke Chen*

Main category: cs.LG

TL;DR: 本文研究发现现有的近似机器学习遗忘算法可能损害差分隐私模型中保留样本的隐私，需要开发差分隐私遗忘算法


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法假设保留数据不受影响，但隐私洋葱效应表明这种假设可能不成立，特别是在差分隐私模型下，需要全面审计遗忘和保留样本的隐私风险

Method: 提出基于差分隐私和成员推理攻击的隐私标准，开发高效MIA方法A-LiRA，利用数据增强减少影子模型训练成本

Result: 实验发现现有近似机器学习遗忘算法可能无意中损害差分隐私模型中保留样本的隐私

Conclusion: 需要开发差分隐私遗忘算法来确保保留样本的隐私保护

Abstract: Approximate machine unlearning aims to remove the effect of specific data
from trained models to ensure individuals' privacy. Existing methods focus on
the removed records and assume the retained ones are unaffected. However,
recent studies on the \emph{privacy onion effect} indicate this assumption
might be incorrect. Especially when the model is differentially private, no
study has explored whether the retained ones still meet the differential
privacy (DP) criterion under existing machine unlearning methods. This paper
takes a holistic approach to auditing both unlearned and retained samples'
privacy risks after applying approximate unlearning algorithms. We propose the
privacy criteria for unlearned and retained samples, respectively, based on the
perspectives of DP and membership inference attacks (MIAs). To make the
auditing process more practical, we also develop an efficient MIA, A-LiRA,
utilizing data augmentation to reduce the cost of shadow model training. Our
experimental findings indicate that existing approximate machine unlearning
algorithms may inadvertently compromise the privacy of retained samples for
differentially private models, and we need differentially private unlearning
algorithms. For reproducibility, we have pubished our code:
https://anonymous.4open.science/r/Auditing-machine-unlearning-CB10/README.md

</details>


### [50] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: 本文研究了MoE稀疏性对记忆和推理能力的影响，发现在固定计算预算下，记忆能力随总参数增加而提升，而推理能力会饱和甚至衰退。


<details>
  <summary>Details</summary>
Motivation: 现有的经验缩放定律主要针对密集模型，而MoE模型引入了新的稀疏维度，需要研究稀疏性如何影响不同能力机制（记忆vs推理）。

Method: 训练了系统变化总参数、激活参数和top-k路由的MoE Transformer家族，在固定计算预算下记录预训练损失、下游任务损失和准确率。

Result: 记忆基准随总参数单调改善，而推理性能会饱和甚至衰退；改变top-k路由对性能影响很小；强化学习和额外测试计算无法挽救过度稀疏模型的推理缺陷。

Conclusion: MoE稀疏性对记忆和推理能力有不同影响，推理能力对过度稀疏更敏感，需要重新思考MoE模型的缩放定律和优化策略。

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


### [51] [Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding](https://arxiv.org/abs/2508.18676)
*Chufan Gao,Jintai Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: LRTab是一种新颖的提示学习方法，通过从训练数据中检索相关提示条件来提升表格推理性能，结合了微调的数据特定学习和零样本提示的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的表格推理方法存在局限性：微调方法缺乏泛化性，而零样本提示方法未能充分利用训练数据。需要一种既能利用训练数据又能保持泛化能力的方法。

Method: 1) 在训练数据上使用提示获取思维链响应；2) 对错误响应生成提示条件以避免错误；3) 使用验证数据验证提示条件有效性；4) 推理时检索最相关的提示条件作为额外上下文。

Result: 在WikiTQ和Tabfact数据集上的实验表明，LRTab方法具有可解释性、成本效益高，并且在表格推理任务上超越了之前的基线方法。

Conclusion: LRTab成功地将训练数据的学习与提示方法的泛化能力相结合，为表格理解提供了一种有效且高效的解决方案。

Abstract: Automated tabular understanding and reasoning are essential tasks for data
scientists. Recently, Large language models (LLMs) have become increasingly
prevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning
LLMs using labeled data or (2) Training-free prompting LLM agents using
chain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost
of generalizability. Training-free prompting is highly generalizable but does
not take full advantage of training data. In this paper, we propose a novel
prompting-based reasoning approach, Learn then Retrieve: LRTab, which
integrates the benefits of both by retrieving relevant information learned from
training data. We first use prompting to obtain CoT responses over the training
data. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to
avoid the error, learning insights from the data. We validate the effectiveness
of Prompt Conditions using validation data. Finally, at inference time, we
retrieve the most relevant Prompt Conditions for additional context for table
understanding. We provide comprehensive experiments on WikiTQ and Tabfact,
showing that LRTab is interpretable, cost-efficient, and can outperform
previous baselines in tabular reasoning.

</details>


### [52] [End to End Autoencoder MLP Framework for Sepsis Prediction](https://arxiv.org/abs/2508.18688)
*Hejiang Cai,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: 提出了一种端到端深度学习框架，结合无监督自编码器和多层感知器分类器，用于ICU脓毒症早期检测，在三个ICU队列中准确率分别达到74.6%、80.6%和93.5%，优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法依赖手动特征工程，难以处理电子健康记录中不规则、不完整的时间序列数据，需要开发更鲁棒的脓毒症早期检测方法。

Method: 使用无监督自编码器进行自动特征提取，结合多层感知器分类器；采用定制下采样策略提取高信息密度片段，以及非重叠动态滑动窗口机制进行实时推理；预处理时间序列数据为固定维度向量并包含缺失值指示器。

Result: 在三个ICU队列中分别获得74.6%、80.6%和93.5%的准确率，持续优于传统机器学习基线方法（朴素贝叶斯、SVM、随机森林、XGBoost）。

Conclusion: 该端到端框架在异质ICU环境中展现出优越的鲁棒性、泛化能力和临床实用性，适用于脓毒症的早期检测。

Abstract: Sepsis is a life threatening condition that requires timely detection in
intensive care settings. Traditional machine learning approaches, including
Naive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often
rely on manual feature engineering and struggle with irregular, incomplete
time-series data commonly present in electronic health records. We introduce an
end-to-end deep learning framework integrating an unsupervised autoencoder for
automatic feature extraction with a multilayer perceptron classifier for binary
sepsis risk prediction. To enhance clinical applicability, we implement a
customized down sampling strategy that extracts high information density
segments during training and a non-overlapping dynamic sliding window mechanism
for real-time inference. Preprocessed time series data are represented as fixed
dimension vectors with explicit missingness indicators, mitigating bias and
noise. We validate our approach on three ICU cohorts. Our end-to-end model
achieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,
respectively, consistently outperforming traditional machine learning
baselines. These results demonstrate the framework's superior robustness,
generalizability, and clinical utility for early sepsis detection across
heterogeneous ICU environments.

</details>


### [53] [Natural Image Classification via Quasi-Cyclic Graph Ensembles and Random-Bond Ising Models at the Nishimori Temperature](https://arxiv.org/abs/2508.18717)
*V. S. Usatyuk,D. A. Sapoznikov,S. I. Egorov*

Main category: cs.LG

TL;DR: 提出结合统计物理、编码理论和代数拓扑的统一框架，通过将高维特征向量解释为稀疏LDPC图上的自旋，在Nishimori温度下运行随机键Ising模型，实现高效多类图像分类。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个理论严谨且计算高效的框架，通过物理启发的嵌入方法实现特征压缩，同时保持分类性能，解决高维特征处理中的计算效率问题。

Method: 使用冻结的MobileNetV2主干提取特征，构建MET-QC-LDPC图表示随机键Ising模型，在Nishimori温度下运行，利用拓扑指导设计球形和环形图集成来抑制有害陷阱集。

Result: 在ImageNet-10上达到98.7%准确率，ImageNet-100上达到82.7%准确率，特征维度从1280压缩到32或64维（40倍参数减少），计算速度比二分法快6倍。

Conclusion: 拓扑指导的图设计能够产生高效、物理启发的嵌入，在保持最先进性能的同时实现大规模特征压缩，证明了统计物理和代数拓扑在机器学习中的有效结合。

Abstract: We present a unified framework combining statistical physics, coding theory,
and algebraic topology for efficient multi-class image classification.
High-dimensional feature vectors from a frozen MobileNetV2 backbone are
interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC
(MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this
RBIM at its Nishimori temperature, $\beta_N$, where the smallest eigenvalue of
the Bethe-Hessian matrix vanishes, maximizing class separability.
  Our theoretical contribution establishes a correspondence between local
trapping sets in the code's graph and topological invariants (Betti numbers,
bordism classes) of the feature manifold. A practical algorithm estimates
$\beta_N$ efficiently with a quadratic interpolant and Newton correction,
achieving a six-fold speed-up over bisection.
  Guided by topology, we design spherical and toroidal MET-QC-LDPC graph
ensembles, using permanent bounds to suppress harmful trapping sets. This
compresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and
-100 subsets. Despite massive compression (40x fewer parameters), we achieve
98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that
topology-guided graph design yields highly efficient, physics-inspired
embeddings with state-of-the-art performance.

</details>


### [54] [Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning](https://arxiv.org/abs/2508.18730)
*Yi Liu,Hongji Zhang,Yiwen Wang,Dimitris Tsaras,Lei Chen,Mingxuan Yuan,Qiang Xu*

Main category: cs.LG

TL;DR: StructRTL是一个基于控制数据流图的结构感知自监督学习框架，用于改进RTL设计质量估计，通过结合结构学习和跨阶段监督，在各项质量估计任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的方法忽略了RTL代码的结构语义，而控制数据流图能更明确地展示设计结构特征，为表示学习提供更丰富的线索。

Method: 提出结构感知图自监督学习框架StructRTL，从CDFG学习结构感知表示，并采用知识蒸馏策略将后映射网表的低级洞察转移到CDFG预测器中。

Result: 实验表明该方法在各项质量估计任务中建立了新的最先进结果，显著优于先前技术。

Conclusion: 结合结构学习和跨阶段监督的方法在RTL设计质量估计中表现出色，证明了结构感知表示学习的重要性。

Abstract: Estimating the quality of register transfer level (RTL) designs is crucial in
the electronic design automation (EDA) workflow, as it enables instant feedback
on key metrics like area and delay without the need for time-consuming logic
synthesis. While recent approaches have leveraged large language models (LLMs)
to derive embeddings from RTL code and achieved promising results, they
overlook the structural semantics essential for accurate quality estimation. In
contrast, the control data flow graph (CDFG) view exposes the design's
structural characteristics more explicitly, offering richer cues for
representation learning. In this work, we introduce a novel structure-aware
graph self-supervised learning framework, StructRTL, for improved RTL design
quality estimation. By learning structure-informed representations from CDFGs,
our method significantly outperforms prior art on various quality estimation
tasks. To further boost performance, we incorporate a knowledge distillation
strategy that transfers low-level insights from post-mapping netlists into the
CDFG predictor. Experiments show that our approach establishes new
state-of-the-art results, demonstrating the effectiveness of combining
structural learning with cross-stage supervision.

</details>


### [55] [FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks](https://arxiv.org/abs/2508.18737)
*Enrique Mármol Campos,Aurora González Vidal,José Luis Hernández Ramos,Antonio Skarmeta*

Main category: cs.LG

TL;DR: FLAegis是一个两阶段防御框架，使用符号时间序列变换和谱聚类检测拜占庭客户端，并通过FFT聚合函数增强联邦学习系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性限制了训练过程的可见性，依赖客户端诚实性，使恶意拜占庭客户端可能通过提交虚假模型更新来毒化训练过程。

Method: 采用两阶段防御框架：1）使用符号时间序列变换（SAX）放大良性模型与恶意模型之间的差异；2）使用谱聚类准确检测对抗行为；3）集成基于FFT的鲁棒聚合函数作为最终防护层。

Result: 在五种投毒攻击（从简单标签翻转到自适应优化策略）的严格评估中，该方法在检测精度和最终模型准确性方面均优于最先进的防御方法，即使在强对抗条件下也能保持持续高性能。

Conclusion: FLAegis框架有效提升了联邦学习系统对拜占庭攻击的防御能力，通过多阶段检测和聚合机制确保了系统的安全性和鲁棒性。

Abstract: Federated Learning (FL) has become a powerful technique for training Machine
Learning (ML) models in a decentralized manner, preserving the privacy of the
training datasets involved. However, the decentralized nature of FL limits the
visibility of the training process, relying heavily on the honesty of
participating clients. This assumption opens the door to malicious third
parties, known as Byzantine clients, which can poison the training process by
submitting false model updates. Such malicious clients may engage in poisoning
attacks, manipulating either the dataset or the model parameters to induce
misclassification. In response, this study introduces FLAegis, a two-stage
defensive framework designed to identify Byzantine clients and improve the
robustness of FL systems. Our approach leverages symbolic time series
transformation (SAX) to amplify the differences between benign and malicious
models, and spectral clustering, which enables accurate detection of
adversarial behavior. Furthermore, we incorporate a robust FFT-based
aggregation function as a final layer to mitigate the impact of those Byzantine
clients that manage to evade prior defenses. We rigorously evaluate our method
against five poisoning attacks, ranging from simple label flipping to adaptive
optimization-based strategies. Notably, our approach outperforms
state-of-the-art defenses in both detection precision and final model accuracy,
maintaining consistently high performance even under strong adversarial
conditions.

</details>


### [56] [Stability and Generalization for Bellman Residuals](https://arxiv.org/abs/2508.18741)
*Enoch H. Kang,Kyoungseok Jang*

Main category: cs.LG

TL;DR: 本文分析了离线强化学习和离线逆强化学习中Bellman残差最小化(BRM)方法的统计行为，提出了新的稳定性分析和风险界限


<details>
  <summary>Details</summary>
Motivation: 当前离线强化学习和离线逆强化学习实践中难以有效实施Bellman一致性约束，虽然已有基于随机梯度下降-上升的全局收敛方法，但其在离线设置下的统计行为尚未得到充分研究

Method: 引入单个Lyapunov势函数来耦合相邻数据集上的SGDA运行，获得O(1/n)的平均参数稳定性界限，并将该稳定性常数转化为BRM的O(1/n)超额风险界限

Result: 获得了比现有凸-凹鞍点问题最佳样本复杂度指数高一倍的稳定性界限，且无需方差缩减、额外正则化或对minibatch采样的限制性独立假设

Conclusion: 该方法适用于标准神经网络参数化和minibatch SGD，为离线BRM提供了理论保证

Abstract: Offline reinforcement learning and offline inverse reinforcement learning aim
to recover near-optimal value functions or reward models from a fixed batch of
logged trajectories, yet current practice still struggles to enforce Bellman
consistency. Bellman residual minimization (BRM) has emerged as an attractive
remedy, as a globally convergent stochastic gradient descent-ascent based
method for BRM has been recently discovered. However, its statistical behavior
in the offline setting remains largely unexplored. In this paper, we close this
statistical gap. Our analysis introduces a single Lyapunov potential that
couples SGDA runs on neighbouring datasets and yields an O(1/n) on-average
argument-stability bound-doubling the best known sample-complexity exponent for
convex-concave saddle problems. The same stability constant translates into the
O(1/n) excess risk bound for BRM, without variance reduction, extra
regularization, or restrictive independence assumptions on minibatch sampling.
The results hold for standard neural-network parameterizations and minibatch
SGD.

</details>


### [57] [Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming](https://arxiv.org/abs/2508.18742)
*Jiajun Li,Ran Hou,Yu Ding,Yixuan Li,Shisi Guan,Jiahui Duan,Xiongwei Han,Tao Zhong,Vincent Chau,Weiwei Wu,Wanyuan Wang*

Main category: cs.LG

TL;DR: 提出基于约束缩减的MILP模型简化新方法，通过识别关键紧约束并转为等式来加速求解，相比现有方法提升解质量50%+，减少计算时间17.47%


<details>
  <summary>Details</summary>
Motivation: 现有模型简化方法主要基于变量缩减，而约束缩减这一对偶视角被忽视。约束缩减通过将不等式约束转为等式来降低MILP复杂度，但面临识别关键约束和高效预测的挑战

Method: 首先标记最优解处的紧约束作为潜在关键约束，设计启发式规则选择关键子集；提出多模态表示技术，利用实例级和抽象级MILP公式信息来学习关键紧约束

Result: 实验结果表明，相比最先进方法，本方法提升解质量超过50%，减少计算时间17.47%

Conclusion: 约束基模型缩减方法有效解决了MILP加速求解问题，证明了约束缩减视角的重要价值，为大规模MILP问题提供了高效的解决方案

Abstract: Model reduction, which aims to learn a simpler model of the original mixed
integer linear programming (MILP), can solve large-scale MILP problems much
faster. Most existing model reduction methods are based on variable reduction,
which predicts a solution value for a subset of variables. From a dual
perspective, constraint reduction that transforms a subset of inequality
constraints into equalities can also reduce the complexity of MILP, but has
been largely ignored. Therefore, this paper proposes a novel constraint-based
model reduction approach for the MILP. Constraint-based MILP reduction has two
challenges: 1) which inequality constraints are critical such that reducing
them can accelerate MILP solving while preserving feasibility, and 2) how to
predict these critical constraints efficiently. To identify critical
constraints, we first label these tight-constraints at the optimal solution as
potential critical constraints and design a heuristic rule to select a subset
of critical tight-constraints. To learn the critical tight-constraints, we
propose a multi-modal representation technique that leverages information from
both instance-level and abstract-level MILP formulations. The experimental
results show that, compared to the state-of-the-art methods, our method
improves the quality of the solution by over 50\% and reduces the computation
time by 17.47\%.

</details>


### [58] [UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning](https://arxiv.org/abs/2508.18756)
*Zihao Huang,Yu Bao,Qiyang Min,Siyan Chen,Ran Guo,Hongzhi Huang,Defa Zhu,Yutao Zeng,Banggu Wu,Xun Zhou,Siyuan Qiao*

Main category: cs.LG

TL;DR: UltraMemV2是一种改进的内存层架构，通过五项关键改进实现了与8专家MoE模型相当的性能，同时显著降低了内存访问成本，在内存密集型任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 混合专家(MoE)模型虽然通过激活参数子集实现了高效性，但在推理过程中存在高内存访问成本的问题。之前的内存层架构如UltraMem只能匹配2专家MoE模型的性能，远不及最先进的8专家配置。

Method: 提出了五项关键改进：1)在每个transformer块中集成内存层；2)使用单线性投影简化值扩展；3)采用基于FFN的值处理；4)实施原则性参数初始化；5)重新平衡内存到FFN的计算比例。

Result: UltraMemV2在相同计算和参数条件下实现了与8专家MoE模型的性能相当，但内存访问显著降低。在内存密集型任务上表现优异：长上下文记忆+1.6分，多轮记忆+6.2分，上下文学习+7.9分。验证了激活密度比总稀疏参数数量对性能影响更大。

Conclusion: UltraMemV2使内存层架构达到了与最先进MoE模型相当的性能水平，为高效稀疏计算提供了一个有吸引力的替代方案。

Abstract: While Mixture of Experts (MoE) models achieve remarkable efficiency by
activating only subsets of parameters, they suffer from high memory access
costs during inference. Memory-layer architectures offer an appealing
alternative with very few memory access, but previous attempts like UltraMem
have only matched the performance of 2-expert MoE models, falling significantly
short of state-of-the-art 8-expert configurations. We present UltraMemV2, a
redesigned memory-layer architecture that closes this performance gap. Our
approach introduces five key improvements: integrating memory layers into every
transformer block, simplifying value expansion with single linear projections,
adopting FFN-based value processing from PEER, implementing principled
parameter initialization, and rebalancing memory-to-FFN computation ratios.
Through extensive evaluation, we demonstrate that UltraMemV2 achieves
performance parity with 8-expert MoE models under same computation and
parameters but significantly low memory access. Notably, UltraMemV2 shows
superior performance on memory-intensive tasks, with improvements of +1.6
points on long-context memorization, +6.2 points on multi-round memorization,
and +7.9 points on in-context learning. We validate our approach at scale with
models up to 2.5B activated parameters from 120B total parameters, and
establish that activation density has greater impact on performance than total
sparse parameter count. Our work brings memory-layer architectures to
performance parity with state-of-the-art MoE models, presenting a compelling
alternative for efficient sparse computation.

</details>


### [59] [Governance-as-a-Service: A Multi-Agent Framework for AI System Compliance and Policy Enforcement](https://arxiv.org/abs/2508.18765)
*Helen Pervez,Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: 提出了Governance-as-a-Service (GaaS)框架，作为分布式AI系统的运行时治理层，通过声明式规则和信任评分机制来强制执行策略，而不需要修改模型内部结构或依赖代理合作。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统向分布式生态系统发展，现有的监管机制反应式、脆弱且嵌入在代理架构中，缺乏可扩展、解耦的治理方案，存在结构性风险。

Method: GaaS采用模块化、策略驱动的执行层，使用声明式规则和信任因子机制对代理进行评分，支持强制、规范和自适应干预，实现分级执行和动态信任调节。

Result: 在三个模拟场景（无治理、GaaS执行策略、对抗性代理测试）中，GaaS可靠地阻止或重定向高风险行为，同时保持吞吐量，信任分数能够跟踪规则遵守情况并隔离不可信组件。

Conclusion: GaaS将治理定位为运行时服务，为可互操作的代理生态系统建立基础设施级别的对齐，不教授代理伦理而是强制执行伦理规范。

Abstract: As AI systems evolve into distributed ecosystems with autonomous execution,
asynchronous reasoning, and multi-agent coordination, the absence of scalable,
decoupled governance poses a structural risk. Existing oversight mechanisms are
reactive, brittle, and embedded within agent architectures, making them
non-auditable and hard to generalize across heterogeneous deployments.
  We introduce Governance-as-a-Service (GaaS): a modular, policy-driven
enforcement layer that regulates agent outputs at runtime without altering
model internals or requiring agent cooperation. GaaS employs declarative rules
and a Trust Factor mechanism that scores agents based on compliance and
severity-weighted violations. It enables coercive, normative, and adaptive
interventions, supporting graduated enforcement and dynamic trust modulation.
  To evaluate GaaS, we conduct three simulation regimes with open-source models
(LLaMA3, Qwen3, DeepSeek-R1) across content generation and financial
decision-making. In the baseline, agents act without governance; in the second,
GaaS enforces policies; in the third, adversarial agents probe robustness. All
actions are intercepted, evaluated, and logged for analysis. Results show that
GaaS reliably blocks or redirects high-risk behaviors while preserving
throughput. Trust scores track rule adherence, isolating and penalizing
untrustworthy components in multi-agent systems.
  By positioning governance as a runtime service akin to compute or storage,
GaaS establishes infrastructure-level alignment for interoperable agent
ecosystems. It does not teach agents ethics; it enforces them.

</details>


### [60] [Predicting Drug-Drug Interactions Using Heterogeneous Graph Neural Networks: HGNN-DDI](https://arxiv.org/abs/2508.18766)
*Hongbo Liu,Siyi Li,Zheng Yu*

Main category: cs.LG

TL;DR: HGNN-DDI是一个基于异构图神经网络的药物相互作用预测模型，通过整合多源药物数据，在预测准确性和鲁棒性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用(DDIs)是临床实践中的主要关注点，可能导致疗效降低或严重不良反应。传统计算方法难以捕捉药物、靶点和生物实体之间的复杂关系。

Method: 提出HGNN-DDI异构图神经网络模型，整合多种药物相关数据源，利用图表示学习建模异质生物医学网络，实现跨不同节点和边类型的有效信息传播。

Result: 在基准DDI数据集上的实验结果表明，HGNN-DDI在预测准确性和鲁棒性方面优于最先进的基线方法。

Conclusion: HGNN-DDI显示出支持更安全药物开发和精准医学的潜力，为药物相互作用预测提供了有效的解决方案。

Abstract: Drug-drug interactions (DDIs) are a major concern in clinical practice, as
they can lead to reduced therapeutic efficacy or severe adverse effects.
Traditional computational approaches often struggle to capture the complex
relationships among drugs, targets, and biological entities. In this work, we
propose HGNN-DDI, a heterogeneous graph neural network model designed to
predict potential DDIs by integrating multiple drug-related data sources.
HGNN-DDI leverages graph representation learning to model heterogeneous
biomedical networks, enabling effective information propagation across diverse
node and edge types. Experimental results on benchmark DDI datasets demonstrate
that HGNN-DDI outperforms state-of-the-art baselines in prediction accuracy and
robustness, highlighting its potential to support safer drug development and
precision medicine.

</details>


### [61] [SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation](https://arxiv.org/abs/2508.18826)
*Junyu Yan,Feng Chen,Yuyang Xue,Yuning Du,Konstantinos Vilouras,Sotirios A. Tsaftaris,Steven McDonagh*

Main category: cs.LG

TL;DR: SWiFT是一种高效的机器学习模型去偏框架，通过软掩码权重微调技术，只需少量外部数据和少量训练轮次即可显著提升模型公平性，同时保持诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法需要原始训练数据和大量重新训练，且在公平性和性能之间存在权衡。医疗等敏感领域需要更高效、低成本的去偏解决方案。

Method: 首先识别模型参数对偏差和预测性能的相对贡献，然后采用两步微调过程，根据参数贡献定义不同的梯度流来更新每个参数。

Result: 在三个偏差敏感属性（性别、肤色、年龄）和六个医学数据集上的实验表明，SWiFT能持续减少模型偏差，在公平性和准确性指标上达到或超越最先进方法，并在分布外数据集上表现出更好的泛化能力。

Conclusion: SWiFT提供了一个高效、低成本的模型去偏框架，能够在保持诊断准确性的同时显著提升模型公平性和泛化能力，适用于医疗等敏感领域的实际应用。

Abstract: Recent studies have shown that Machine Learning (ML) models can exhibit bias
in real-world scenarios, posing significant challenges in ethically sensitive
domains such as healthcare. Such bias can negatively affect model fairness,
model generalization abilities and further risks amplifying social
discrimination. There is a need to remove biases from trained models. Existing
debiasing approaches often necessitate access to original training data and
need extensive model retraining; they also typically exhibit trade-offs between
model fairness and discriminative performance. To address these challenges, we
propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that
efficiently improves fairness while preserving discriminative performance with
much less debiasing costs. Notably, SWiFT requires only a small external
dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to
first find the relative, and yet distinct, contributions of model parameters to
both bias and predictive performance. Then, a two-step fine-tuning process
updates each parameter with different gradient flows defined by its
contribution. Extensive experiments with three bias sensitive attributes
(gender, skin tone, and age) across four dermatological and two chest X-ray
datasets demonstrate that SWiFT can consistently reduce model bias while
achieving competitive or even superior diagnostic accuracy under common
fairness and accuracy metrics, compared to the state-of-the-art. Specifically,
we demonstrate improved model generalization ability as evidenced by superior
performance on several out-of-distribution (OOD) datasets.

</details>


### [62] [DRMD: Deep Reinforcement Learning for Malware Detection under Concept Drift](https://arxiv.org/abs/2508.18839)
*Shae McFadden,Myles Foley,Mario D'Onghia,Chris Hicks,Vasilios Mavroudis,Nicola Paoletti,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习的恶意软件检测方法DRMD，通过马尔可夫决策过程同时优化分类性能和样本拒绝策略，在Android恶意软件检测中显著提升了对抗概念漂移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统恶意软件检测分类器难以应对概念漂移问题，无法优化何时将决策推迟到手动标记和适应。现有的检测管道结合主动学习和拒绝机制来缓解概念漂移的影响，但仍有改进空间。

Method: 将恶意软件检测建模为一步马尔可夫决策过程，训练深度强化学习代理DRMD，同时优化样本分类性能和拒绝高风险样本进行手动标记的策略。

Result: 在Android恶意软件数据集上的时间感知评估显示，DRMD代理相比标准分类方法在AUT性能上分别提升了5.18±5.44、14.49±12.86和10.06±10.81个百分点，表现出更好的概念漂移鲁棒性。

Conclusion: 深度强化学习能够有效促进恶意软件检测，并在Android恶意软件领域的动态环境中提高对概念漂移的弹性，这是首次证明DRL在该领域的有效性。

Abstract: Malware detection in real-world settings must deal with evolving threats,
limited labeling budgets, and uncertain predictions. Traditional classifiers,
without additional mechanisms, struggle to maintain performance under concept
drift in malware domains, as their supervised learning formulation cannot
optimize when to defer decisions to manual labeling and adaptation. Modern
malware detection pipelines combine classifiers with monthly active learning
(AL) and rejection mechanisms to mitigate the impact of concept drift. In this
work, we develop a novel formulation of malware detection as a one-step Markov
Decision Process and train a deep reinforcement learning (DRL) agent,
simultaneously optimizing sample classification performance and rejecting
high-risk samples for manual labeling. We evaluated the joint detection and
drift mitigation policy learned by the DRL-based Malware Detection (DRMD) agent
through time-aware evaluations on Android malware datasets subject to realistic
drift requiring multi-year performance stability. The policies learned under
these conditions achieve a higher Area Under Time (AUT) performance compared to
standard classification approaches used in the domain, showing improved
resilience to concept drift. Specifically, the DRMD agent achieved a
$5.18\pm5.44$, $14.49\pm12.86$, and $10.06\pm10.81$ average AUT performance
improvement for the classification only, classification with rejection, and
classification with rejection and AL settings, respectively. Our results
demonstrate for the first time that DRL can facilitate effective malware
detection and improved resiliency to concept drift in the dynamic environment
of the Android malware domain.

</details>


### [63] [Recycling History: Efficient Recommendations from Contextual Dueling Bandits](https://arxiv.org/abs/2508.18841)
*Suryanarayana Sankagiri,Jalal Etesami,Pouria Fatemi,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 提出了一种新的上下文决斗bandit模型，算法推荐一个物品后，用户将其与历史消费物品进行比较，无需额外遗憾成本，实现了O(√T)的遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 现有上下文决斗bandit模型只捕获用户在选择时的隐式偏好，但用户在消费物品后能提供更可靠的反馈，因此需要新的模型来利用这种消费后比较的优势。

Method: 算法每步推荐一个物品，用户消费后将其与历史物品进行比较。通过初始随机探索阶段积累丰富历史，利用矩阵浓度界限证明历史多样性条件，从而构造信息丰富的查询。

Result: 理论分析证明了O(√T)的遗憾上界，模拟实验显示重用历史物品进行比较比仅比较同时推荐物品能显著降低遗憾。

Conclusion: 提出的新bandit模型通过利用用户消费后的比较反馈和历史物品重用，在无需额外遗憾成本的情况下实现了更好的性能，为推荐系统提供了更有效的反馈利用机制。

Abstract: The contextual duelling bandit problem models adaptive recommender systems,
where the algorithm presents a set of items to the user, and the user's choice
reveals their preference. This setup is well suited for implicit choices users
make when navigating a content platform, but does not capture other possible
comparison queries. Motivated by the fact that users provide more reliable
feedback after consuming items, we propose a new bandit model that can be
described as follows. The algorithm recommends one item per time step; after
consuming that item, the user is asked to compare it with another item chosen
from the user's consumption history. Importantly, in our model, this comparison
item can be chosen without incurring any additional regret, potentially leading
to better performance. However, the regret analysis is challenging because of
the temporal dependency in the user's history. To overcome this challenge, we
first show that the algorithm can construct informative queries provided the
history is rich, i.e., satisfies a certain diversity condition. We then show
that a short initial random exploration phase is sufficient for the algorithm
to accumulate a rich history with high probability. This result, proven via
matrix concentration bounds, yields $O(\sqrt{T})$ regret guarantees.
Additionally, our simulations show that reusing past items for comparisons can
lead to significantly lower regret than only comparing between simultaneously
recommended items.

</details>


### [64] [C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning](https://arxiv.org/abs/2508.18860)
*Wei Li,Hangjie Yuan,Zixiang Zhao,Yifan Zhu,Aojun Lu,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: C-Flat是一种针对持续学习的平坦损失景观优化方法，通过促进更平坦的最小值来提高模型稳定性和性能，并提供了高效的C-Flat++变体。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法依赖零阶锐度可能在某些情况下偏好更尖锐的最小值，导致解决方案不够鲁棒和次优，需要一种专门为持续学习设计的平坦损失景观优化方法。

Method: 提出C-Flat方法，促进持续学习中更平坦的损失景观，具有即插即用兼容性；并开发C-Flat++框架，利用选择性平坦性驱动提升来显著降低更新成本。

Result: 在多种持续学习方法、数据集和场景上的广泛实验表明，C-Flat能持续提升性能，C-Flat++在保持有效性的同时显著降低了更新成本。

Conclusion: C-Flat和C-Flat++为持续学习提供了有效的平坦最小化解决方案，在保持学习效率的同时提高了模型的鲁棒性和性能表现。

Abstract: Balancing sensitivity to new tasks and stability for retaining past knowledge
is crucial in continual learning (CL). Recently, sharpness-aware minimization
has proven effective in transfer learning and has also been adopted in
continual learning (CL) to improve memory retention and learning efficiency.
However, relying on zeroth-order sharpness alone may favor sharper minima over
flatter ones in certain settings, leading to less robust and potentially
suboptimal solutions. In this paper, we propose \textbf{C}ontinual
\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter loss
landscapes tailored for CL. C-Flat offers plug-and-play compatibility, enabling
easy integration with minimal modifications to the code pipeline. Besides, we
present a general framework that integrates C-Flat into all major CL paradigms
and conduct comprehensive comparisons with loss-minima optimizers and
flat-minima-based CL methods. Our results show that C-Flat consistently
improves performance across a wide range of settings. In addition, we introduce
C-Flat++, an efficient yet effective framework that leverages selective
flatness-driven promotion, significantly reducing the update cost required by
C-Flat. Extensive experiments across multiple CL methods, datasets, and
scenarios demonstrate the effectiveness and efficiency of our proposed
approaches. Code is available at https://github.com/WanNaa/C-Flat.

</details>


### [65] [MOCHA: Discovering Multi-Order Dynamic Causality in Temporal Point Processes](https://arxiv.org/abs/2508.18873)
*Yunyang Cao,Juekai Lin,Wenhao Li,Bo Jin*

Main category: cs.LG

TL;DR: MOCHA是一个用于在时间点过程中发现多阶动态因果关系的框架，通过建模时变有向无环图和端到端可微分学习，实现了准确的事件预测和可解释的因果结构发现。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖静态或一阶因果结构，忽略了因果关系的多阶性和时变特性，无法充分建模现实世界事件序列中的复杂因果依赖关系。

Method: 提出MOCHA框架，将多阶影响建模为潜在时变图上的多跳因果路径，使用时变有向无环图(DAG)和可学习结构权重，通过无环性和稀疏性约束确保结构有效性，设计端到端可微分框架联合建模因果发现和时间点过程动态。

Result: 在真实世界数据集上的大量实验表明，MOCHA不仅在事件预测方面达到了最先进的性能，而且揭示了有意义且可解释的因果结构。

Conclusion: MOCHA成功解决了时间点过程中多阶动态因果关系的发现问题，为建模复杂事件序列提供了有效的框架，在预测性能和结构可解释性方面都表现出色。

Abstract: Discovering complex causal dependencies in temporal point processes (TPPs) is
critical for modeling real-world event sequences. Existing methods typically
rely on static or first-order causal structures, overlooking the multi-order
and time-varying nature of causal relationships. In this paper, we propose
MOCHA, a novel framework for discovering multi-order dynamic causality in TPPs.
MOCHA characterizes multi-order influences as multi-hop causal paths over a
latent time-evolving graph. To model such dynamics, we introduce a time-varying
directed acyclic graph (DAG) with learnable structural weights, where
acyclicity and sparsity constraints are enforced to ensure structural validity.
We design an end-to-end differentiable framework that jointly models causal
discovery and TPP dynamics, enabling accurate event prediction and revealing
interpretable structures. Extensive experiments on real-world datasets
demonstrate that MOCHA not only achieves state-of-the-art performance in event
prediction, but also reveals meaningful and interpretable causal structures.

</details>


### [66] [HAEPO: History-Aggregated Exploratory Policy Optimization](https://arxiv.org/abs/2508.18884)
*Gaurish Trivedi,Alakh Sharma,Kartikey Singh Bhandari,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: HAEPO是一种新的探索性策略优化方法，通过历史聚合和轨迹级概率压缩来增强长时域任务的探索能力，在收敛速度、探索广度和奖励对齐方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如DPO和GRPO在长时域任务中探索能力有限，需要一种能够更好利用完整轨迹历史并促进广泛探索的新方法。

Method: HAEPO将每个轨迹压缩为对数概率之和，应用Plackett-Luce softmax获得与回报成正比的归一化权重，并加入熵正则化和软KL惩罚来稳定训练。

Result: 实验表明HAEPO收敛快、探索充分、与真实奖励对齐紧密，在多样化任务中表现优于或等同于PPO、GRPO和DPO。

Conclusion: HAEPO提供了一个稳定且可解释的框架，通过显式利用完整轨迹历史来平衡探索和稳定性。

Abstract: Exploration is essential in modern learning, from reinforcement learning
environments with small neural policies to large language models (LLMs).
Existing work, such as DPO, leverages full sequence log-likelihoods to capture
an entire trajectory of the model's decisions, while methods like GRPO
aggregate per-token ratios into a trajectory-level update. However, both often
limit exploration on long-horizon tasks. We introduce History-Aggregated
Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to
combat these shortcomings. HAEPO compresses each trajectory into the sum of its
logarithmic probabilities (a cumulative logarithmic likelihood), and applies a
Plackett-Luce softmax across trajectories to obtain normalized weights
proportional to their returns, thus encouraging broader exploration. We add
entropy regularization to stabilize the aggressive updates to prevent premature
collapse and a soft KL penalty relative to a frozen copy of the previous
(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,
aligns closely with true rewards, and demonstrates robust learning behavior
better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO
provides a stable and interpretable framework by explicitly leveraging
full-trajectory history while balancing exploration and stability.

</details>


### [67] [pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data](https://arxiv.org/abs/2508.18891)
*Zhijin Wang,Senzhen Wu,Yue Hu,Xiufeng Liu*

Main category: cs.LG

TL;DR: pyFAST是一个基于PyTorch的时间序列分析框架，通过解耦数据处理与模型计算来支持复杂场景，提供多源数据加载、稀疏数据处理和多种深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 现有Python时间序列库在模块化、不规则数据、多源数据和稀疏数据支持方面存在局限，需要更灵活高效的框架来促进研究。

Method: 采用PyTorch构建，明确分离数据处理和模型计算，支持多源加载、蛋白质序列处理、动态归一化、掩码建模，集成LLM架构进行稀疏数据融合。

Result: 开发了一个紧凑而强大的平台，提供经典和深度学习模型（线性、CNN、RNN、Transformer、GNN），支持批量流式聚合和设备协同优化。

Conclusion: pyFAST作为MIT许可的开源框架，为时间序列研究和应用提供了模块化、可扩展的高效解决方案，特别适合复杂数据处理场景。

Abstract: Modern time series analysis demands frameworks that are flexible, efficient,
and extensible. However, many existing Python libraries exhibit limitations in
modularity and in their native support for irregular, multi-source, or sparse
data. We introduce pyFAST, a research-oriented PyTorch framework that
explicitly decouples data processing from model computation, fostering a
cleaner separation of concerns and facilitating rapid experimentation. Its data
engine is engineered for complex scenarios, supporting multi-source loading,
protein sequence handling, efficient sequence- and patch-level padding, dynamic
normalization, and mask-based modeling for both imputation and forecasting.
pyFAST integrates LLM-inspired architectures for the alignment-free fusion of
sparse data sources and offers native sparse metrics, specialized loss
functions, and flexible exogenous data fusion. Training utilities include
batch-based streaming aggregation for evaluation and device synergy to maximize
computational efficiency. A comprehensive suite of classical and deep learning
models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a
modular architecture that encourages extension. Released under the MIT license
at GitHub, pyFAST provides a compact yet powerful platform for advancing time
series research and applications.

</details>


### [68] [Distance-informed Neural Processes](https://arxiv.org/abs/2508.18903)
*Aishwarya Venkataramanan,Joachim Denzler*

Main category: cs.LG

TL;DR: DNP是一种改进的神经过程变体，通过结合全局和距离感知的局部潜在结构来提升不确定性估计能力，解决了标准神经过程在校准不确定性和捕捉局部数据依赖方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准神经过程主要依赖全局潜在变量，在校准不确定性和捕捉局部数据依赖方面存在困难，需要一种能够同时建模任务级变化和输入相似性的方法。

Method: 引入全局潜在变量建模任务级变化，局部潜在变量在距离保持的潜在空间中捕捉输入相似性，通过bi-Lipschitz正则化约束输入关系的扭曲并保持相对距离。

Result: DNP在回归和分类任务中实现了强大的预测性能和改进的不确定性校准，能够更有效地区分分布内和分布外数据。

Conclusion: DNP通过结合全局和局部潜在结构以及距离保持机制，显著提升了神经过程的不确定性估计能力和数据依赖建模效果。

Abstract: We propose the Distance-informed Neural Process (DNP), a novel variant of
Neural Processes that improves uncertainty estimation by combining global and
distance-aware local latent structures. Standard Neural Processes (NPs) often
rely on a global latent variable and struggle with uncertainty calibration and
capturing local data dependencies. DNP addresses these limitations by
introducing a global latent variable to model task-level variations and a local
latent variable to capture input similarity within a distance-preserving latent
space. This is achieved through bi-Lipschitz regularization, which bounds
distortions in input relationships and encourages the preservation of relative
distances in the latent space. This modeling approach allows DNP to produce
better-calibrated uncertainty estimates and more effectively distinguish in-
from out-of-distribution data. Empirical results demonstrate that DNP achieves
strong predictive performance and improved uncertainty calibration across
regression and classification tasks.

</details>


### [69] [Enhancing Model Privacy in Federated Learning with Random Masking and Quantization](https://arxiv.org/abs/2508.18911)
*Zhibo Xu,Jianhao Zhu,Jingwen Xu,Changze Lv,Zisu Huang,Xiaohua Wang,Muling Wu,Qi Qian,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.LG

TL;DR: 该方法在联邦学习中保持强模型性能的同时，相比基线方法提供了更好的模型参数保护


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中模型性能与参数隐私保护之间的平衡问题

Method: 未在摘要中明确说明具体方法

Result: 在各种模型和任务上的实验结果表明，该方法在联邦学习环境中保持强模型性能，同时相比基线方法实现了增强的模型参数保护

Conclusion: 提出的方法在联邦学习场景中成功实现了模型性能保持和参数隐私保护的平衡

Abstract: Experimental results across various models and tasks demonstrate that our
approach not only maintains strong model performance in federated learning
settings but also achieves enhanced protection of model parameters compared to
baseline methods.

</details>


### [70] [Generalization Bound for a General Class of Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.18920)
*Madhusudan Verma,Manoj Kumar*

Main category: cs.LG

TL;DR: 本文首次推导了具有一般非线性动力学函数的神经ODE的泛化误差界，分析了时间相关和时间无关两种情况，并研究了过参数化和域约束对泛化界的影响。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注神经ODE中动力学函数的线性情况，或者提供依赖于采样间隔的边界。本文旨在分析更广泛的非线性动力学函数类别，填补这一研究空白。

Method: 在动力学函数关于状态变量Lipschitz连续的条件下，证明了神经ODE解的有界变差性质，基于此建立了时间相关和时间无关情况下的泛化界。

Result: 证明了在Lipschitz条件下神经ODE解具有有界变差，并成功推导出了非线性动力学神经ODE的泛化误差边界。

Conclusion: 这是首次对具有一般非线性动力学的神经ODE推导泛化界的工作，为理解这类连续深度架构模型的泛化性能提供了理论基础。

Abstract: Neural ordinary differential equations (neural ODEs) are a popular type of
deep learning model that operate with continuous-depth architectures. To assess
how well such models perform on unseen data, it is crucial to understand their
generalization error bounds. Previous research primarily focused on the linear
case for the dynamics function in neural ODEs - Marion, P. (2023), or provided
bounds for Neural Controlled ODEs that depend on the sampling interval
Bleistein et al. (2023). In this work, we analyze a broader class of neural
ODEs where the dynamics function is a general nonlinear function, either time
dependent or time independent, and is Lipschitz continuous with respect to the
state variables. We showed that under this Lipschitz condition, the solutions
to neural ODEs have solutions with bounded variations. Based on this
observation, we establish generalization bounds for both time-dependent and
time-independent cases and investigate how overparameterization and domain
constraints influence these bounds. To our knowledge, this is the first
derivation of generalization bounds for neural ODEs with general nonlinear
dynamics.

</details>


### [71] [HierCVAE: Hierarchical Attention-Driven Conditional Variational Autoencoders for Multi-Scale Temporal Modeling](https://arxiv.org/abs/2508.18922)
*Yao Wu*

Main category: cs.LG

TL;DR: HierCVAE是一种结合分层注意力机制和条件变分自编码器的新架构，用于复杂系统的时间建模，在预测精度和不确定性量化方面显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 复杂系统中的时间建模需要捕获多个时间尺度的依赖关系并管理固有不确定性，现有方法在这方面存在不足

Method: 采用三层注意力结构（局部、全局、跨时间）结合多模态条件编码，在潜在空间中使用ResFormer块，并通过预测头提供显式不确定性量化

Result: 在能源消耗数据集上评估显示，预测精度提高15-40%，不确定性校准优于最先进方法，在长期预测和复杂多变量依赖方面表现优异

Conclusion: HierCVAE通过分层注意力机制和条件变分自编码器的集成，有效解决了复杂时间建模中的多尺度依赖和不确定性管理问题

Abstract: Temporal modeling in complex systems requires capturing dependencies across
multiple time scales while managing inherent uncertainties. We propose
HierCVAE, a novel architecture that integrates hierarchical attention
mechanisms with conditional variational autoencoders to address these
challenges. HierCVAE employs a three-tier attention structure (local, global,
cross-temporal) combined with multi-modal condition encoding to capture
temporal, statistical, and trend information. The approach incorporates
ResFormer blocks in the latent space and provides explicit uncertainty
quantification via prediction heads. Through evaluations on energy consumption
datasets, HierCVAE demonstrates a 15-40% improvement in prediction accuracy and
superior uncertainty calibration compared to state-of-the-art methods,
excelling in long-term forecasting and complex multi-variate dependencies.

</details>


### [72] [Energy-Based Flow Matching for Generating 3D Molecular Structure](https://arxiv.org/abs/2508.18949)
*Wenyin Zhou,Christopher Iliffe Sprague,Vsevolod Viliuga,Matteo Tadiello,Arne Elofsson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 本文提出了一种基于能量视角的流匹配方法，用于分子结构生成，通过迭代映射随机配置到目标结构，在蛋白质对接和骨架生成任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分子结构生成在分子对接、蛋白质折叠和分子设计等生物应用中至关重要。现有生成模型如扩散模型和流匹配已取得进展，但需要从能量角度改进训练和推理过程。

Method: 采用基于能量的视角，通过深度网络学习映射函数，迭代地将源分布中的随机配置映射到数据流形中的目标结构，构建概念简单且理论合理的流匹配框架。

Result: 在蛋白质对接和蛋白质骨架生成实验中，该方法在相同计算预算下优于现有的任务相关流匹配和扩散模型基线。

Conclusion: 基于能量视角的流匹配方法为分子结构生成提供了有效的解决方案，具有理论合理性和实证有效性，与AlphaFold中的结构精炼等技术有有趣联系。

Abstract: Molecular structure generation is a fundamental problem that involves
determining the 3D positions of molecules' constituents. It has crucial
biological applications, such as molecular docking, protein folding, and
molecular design. Recent advances in generative modeling, such as diffusion
models and flow matching, have made great progress on these tasks by modeling
molecular conformations as a distribution. In this work, we focus on flow
matching and adopt an energy-based perspective to improve training and
inference of structure generation models. Our view results in a mapping
function, represented by a deep network, that is directly learned to
\textit{iteratively} map random configurations, i.e. samples from the source
distribution, to target structures, i.e. points in the data manifold. This
yields a conceptually simple and empirically effective flow matching setup that
is theoretically justified and has interesting connections to fundamental
properties such as idempotency and stability, as well as the empirically useful
techniques such as structure refinement in AlphaFold. Experiments on protein
docking as well as protein backbone generation consistently demonstrate the
method's effectiveness, where it outperforms recent baselines of
task-associated flow matching and diffusion models, using a similar
computational budget.

</details>


### [73] [Estimating Conditional Covariance between labels for Multilabel Data](https://arxiv.org/abs/2508.18951)
*Laurence A. F. Park,Jesse Read*

Main category: cs.LG

TL;DR: 本文比较了三种模型（多元Probit、多元Bernoulli和分阶段Logit）在估计多标签条件标签协方差方面的性能，发现多元Probit模型在错误率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 多标签数据分析需要先检验标签间的依赖性，但由于标签值依赖于协变量，独立性不能直接从标签值测量，需要通过条件标签协方差来评估。

Method: 使用三种模型（多元Probit、多元Bernoulli和分阶段Logit）来估计恒定和依赖的多标签条件标签协方差，并通过实验观察各模型对条件协方差的测量能力。

Result: 所有模型在测量恒定和依赖协方差方面表现相当，取决于协方差强度，但都会错误地将恒定协方差检测为依赖协方差。其中多元Probit模型的错误率最低。

Conclusion: 多元Probit模型在多标签条件协方差估计中具有最低错误率，但所有模型都存在将恒定协方差误判为依赖协方差的问题，需要谨慎使用这些模型进行标签依赖性分析。

Abstract: Multilabel data should be analysed for label dependence before applying
multilabel models. Independence between multilabel data labels cannot be
measured directly from the label values due to their dependence on the set of
covariates $\vec{x}$, but can be measured by examining the conditional label
covariance using a multivariate Probit model. Unfortunately, the multivariate
Probit model provides an estimate of its copula covariance, and so might not be
reliable in estimating constant covariance and dependent covariance. In this
article, we compare three models (Multivariate Probit, Multivariate Bernoulli
and Staged Logit) for estimating the constant and dependent multilabel
conditional label covariance. We provide an experiment that allows us to
observe each model's measurement of conditional covariance. We found that all
models measure constant and dependent covariance equally well, depending on the
strength of the covariance, but the models all falsely detect that dependent
covariance is present for data where constant covariance is present. Of the
three models, the Multivariate Probit model had the lowest error rate.

</details>


### [74] [On the Generalisation of Koopman Representations for Chaotic System Control](https://arxiv.org/abs/2508.18954)
*Kyriakos Hjikakou,Juan Diego Cardenas Cartagena,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 本文研究了Koopman表示在混沌动力系统中的泛化能力，重点考察其在预测和控制任务间的可迁移性。使用Lorenz系统作为测试平台，提出了三阶段方法：通过自编码学习Koopman嵌入、在下一状态预测上预训练transformer、为安全关键控制进行微调。


<details>
  <summary>Details</summary>
Motivation: 研究Koopman表示在混沌动力系统中的泛化能力，探索其在多任务学习中的潜力，特别是在预测和控制任务之间的可迁移性。

Method: 三阶段方法：1）通过自编码学习Koopman嵌入；2）在下一状态预测任务上预训练transformer模型；3）针对安全关键控制任务进行微调。使用Lorenz系统作为测试平台。

Result: Koopman嵌入在准确性和数据效率方面均优于标准和物理信息PCA基线。在微调过程中固定预训练transformer权重不会导致性能下降，表明学习到的表示捕获了可重用的动力学结构而非任务特定模式。

Conclusion: Koopman嵌入可作为物理信息机器学习中多任务学习的基础表示，其学习到的动力学结构具有良好的可迁移性和泛化能力。

Abstract: This paper investigates the generalisability of Koopman-based representations
for chaotic dynamical systems, focusing on their transferability across
prediction and control tasks. Using the Lorenz system as a testbed, we propose
a three-stage methodology: learning Koopman embeddings through autoencoding,
pre-training a transformer on next-state prediction, and fine-tuning for
safety-critical control. Our results show that Koopman embeddings outperform
both standard and physics-informed PCA baselines, achieving accurate and
data-efficient performance. Notably, fixing the pre-trained transformer weights
during fine-tuning leads to no performance degradation, indicating that the
learned representations capture reusable dynamical structure rather than
task-specific patterns. These findings support the use of Koopman embeddings as
a foundation for multi-task learning in physics-informed machine learning. A
project page is available at https://kikisprdx.github.io/.

</details>


### [75] [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
*Tim Kreuzer,Jelena Zdravkovic,Panagiotis Papapetrou*

Main category: cs.LG

TL;DR: PAX-TS是一种模型无关的后处理算法，用于解释时间序列预测模型及其预测结果，通过局部输入扰动生成多粒度解释，并能表征多元时间序列的跨通道相关性。


<details>
  <summary>Details</summary>
Motivation: 现代预测模型通常不透明且不提供预测解释，而现有的后处理可解释性方法（如LIME）不适用于预测场景，因此需要专门的时间序列预测可解释性方法。

Method: 基于局部输入扰动的模型无关后处理算法，通过时间步相关性矩阵分析，能够生成多粒度解释并捕捉跨通道相关性。

Result: 在7种算法和10个不同数据集的基准测试中，PAX-TS能够有效区分高性能和低性能算法的行为差异，识别出6类重复出现的模式，这些模式是性能的指标，不同类别之间存在明显的预测误差差异。

Conclusion: PAX-TS能够以不同详细程度说明时间序列预测模型的机制，其解释可用于回答关于预测的实际问题，为时间序列预测提供了有效的可解释性解决方案。

Abstract: Time series forecasting has seen considerable improvement during the last
years, with transformer models and large language models driving advancements
of the state of the art. Modern forecasting models are generally opaque and do
not provide explanations for their forecasts, while well-known post-hoc
explainability methods like LIME are not suitable for the forecasting context.
We propose PAX-TS, a model-agnostic post-hoc algorithm to explain time series
forecasting models and their forecasts. Our method is based on localized input
perturbations and results in multi-granular explanations. Further, it is able
to characterize cross-channel correlations for multivariate time series
forecasts. We clearly outline the algorithmic procedure behind PAX-TS,
demonstrate it on a benchmark with 7 algorithms and 10 diverse datasets,
compare it with two other state-of-the-art explanation algorithms, and present
the different explanation types of the method. We found that the explanations
of high-performing and low-performing algorithms differ on the same datasets,
highlighting that the explanations of PAX-TS effectively capture a model's
behavior. Based on time step correlation matrices resulting from the benchmark,
we identify 6 classes of patterns that repeatedly occur across different
datasets and algorithms. We found that the patterns are indicators of
performance, with noticeable differences in forecasting error between the
classes. Lastly, we outline a multivariate example where PAX-TS demonstrates
how the forecasting model takes cross-channel correlations into account. With
PAX-TS, time series forecasting models' mechanisms can be illustrated in
different levels of detail, and its explanations can be used to answer
practical questions on forecasts.

</details>


### [76] [FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning](https://arxiv.org/abs/2508.19009)
*Md Anwar Hossen,Fatema Siddika,Wensheng Zhang,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: FedProtoKD是一种异构联邦学习方法，通过双知识蒸馏机制和对比学习来解决原型聚合中的边缘收缩问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的原型异构联邦学习方法在服务器端使用加权平均聚合原型，导致全局知识次优和原型边缘收缩问题，特别是在模型异构和数据分布极度非独立同分布的场景下影响模型性能。

Method: 提出FedProtoKD方法，采用增强的双知识蒸馏机制（结合客户端logits和原型特征表示），使用基于对比学习的可训练服务器原型和类别自适应原型边缘来解决原型边缘收缩问题，并通过样本原型与类别代表原型的接近度评估公共样本重要性。

Result: 在各种设置下平均准确率提升1.13%至34.13%，显著优于现有的最先进异构联邦学习方法。

Conclusion: FedProtoKD通过创新的双知识蒸馏和对比学习机制有效解决了原型边缘收缩问题，为异构联邦学习提供了性能优越的解决方案。

Abstract: Heterogeneous Federated Learning (HFL) has gained attention for its ability
to accommodate diverse models and heterogeneous data across clients.
Prototype-based HFL methods emerge as a promising solution to address
statistical heterogeneity and privacy challenges, paving the way for new
advancements in HFL research. This method focuses on sharing only
class-representative prototypes among heterogeneous clients. However, these
prototypes are often aggregated on the server using weighted averaging, leading
to sub-optimal global knowledge; these cause the shrinking of aggregated
prototypes, which negatively affects the model performance in scenarios when
models are heterogeneous and data distributions are extremely non-IID. We
propose FedProtoKD in a Heterogeneous Federated Learning setting, using an
enhanced dual-knowledge distillation mechanism to improve the system
performance with clients' logits and prototype feature representation. We aim
to resolve the prototype margin-shrinking problem using a contrastive
learning-based trainable server prototype by leveraging a class-wise adaptive
prototype margin. Furthermore, we assess the importance of public samples using
the closeness of the sample's prototype to its class representative prototypes,
which enhances learning performance. FedProtoKD achieved average improvements
of 1.13% up to 34.13% accuracy across various settings and significantly
outperforms existing state-of-the-art HFL methods.

</details>


### [77] [STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems](https://arxiv.org/abs/2508.19011)
*Gary Simethy,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.LG

TL;DR: STDiff是一种基于条件去噪扩散模型的工业时间序列插补方法，通过逐步生成缺失值来学习系统动态演化，相比固定窗口方法在长间隙缺失情况下表现更优


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法假设固定时间窗口内的模式完成，但在工业系统中动态受控制动作驱动、高度非平稳且可能存在长时间连续缺失，这种假设往往失效

Method: 使用条件去噪扩散模型，具有与控制理论一致的自回归偏差，基于最近已知状态和相关控制或环境输入逐步生成缺失值

Result: 在模拟缺失块的公共污水处理数据集上始终实现最低误差，优势随间隙长度增加而增强；在真实工业数据集上产生动态合理的轨迹，而窗口模型倾向于平坦化或过度平滑

Conclusion: 支持动态感知、显式条件插补作为工业时间序列的稳健方法，讨论了计算权衡和扩展到更广泛领域的可能性

Abstract: Most deep learning methods for imputing missing values treat the task as
completing patterns within a fixed time window. This assumption often fails in
industrial systems, where dynamics are driven by control actions, are highly
non-stationary, and can experience long, uninterrupted gaps. We propose STDiff,
which reframes imputation as learning how the system evolves from one state to
the next. STDiff uses a conditional denoising diffusion model with a causal
bias aligned to control theory, generating missing values step-by-step based on
the most recent known state and relevant control or environmental inputs. On a
public wastewater treatment dataset with simulated missing blocks, STDiff
consistently achieves the lowest errors, with its advantage increasing for
longer gaps. On a raw industrial dataset with substantial real gaps, it
produces trajectories that remain dynamically plausible, in contrast to
window-based models that tend to flatten or over-smooth. These results support
dynamics-aware, explicitly conditioned imputation as a robust approach for
industrial time series, and we discuss computational trade-offs and extensions
to broader domains.

</details>


### [78] [Learning with springs and sticks](https://arxiv.org/abs/2508.19015)
*Luis Mantilla Calderón,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: 本文提出了一种使用弹簧和棍棒组成的简单动力系统来近似任意连续函数，通过物理系统的能量最小化过程实现机器学习功能，性能与多层感知机相当，并发现了热力学学习障碍现象。


<details>
  <summary>Details</summary>
Motivation: 从物理角度研究学习过程，探索使用简单的机械系统（弹簧和棍棒）来模拟函数逼近和机器学习任务，旨在从物理学视角更好地理解学习系统的本质。

Method: 使用棍棒模拟分段线性函数近似，利用弹簧势能编码均方误差损失函数，通过耗散过程收敛到最小能量配置，构建了一个物理模拟系统来处理回归任务。

Result: 系统在回归任务上的性能与多层感知机相当，发现了系统的热力学特性与学习能力之间的关系，实证发现了由环境波动引起的热力学学习障碍现象。

Conclusion: 这个简单的物理模型为从物理学角度理解学习系统提供了新的视角，揭示了热力学限制对学习能力的影响，表明学习过程确实具有深刻的物理基础。

Abstract: Learning is a physical process. Here, we aim to study a simple dynamical
system composed of springs and sticks capable of arbitrarily approximating any
continuous function. The main idea of our work is to use the sticks to mimic a
piecewise-linear approximation of the given function, use the potential energy
of springs to encode a desired mean squared error loss function, and converge
to a minimum-energy configuration via dissipation. We apply the proposed
simulation system to regression tasks and show that its performance is
comparable to that of multi-layer perceptrons. In addition, we study the
thermodynamic properties of the system and find a relation between the free
energy change of the system and its ability to learn an underlying data
distribution. We empirically find a \emph{thermodynamic learning barrier} for
the system caused by the fluctuations of the environment, whereby the system
cannot learn if its change in free energy hits such a barrier. We believe this
simple model can help us better understand learning systems from a physical
point of view.

</details>


### [79] [Working My Way Back to You: Resource-Centric Next-Activity Prediction](https://arxiv.org/abs/2508.19016)
*Kelly Kurowski,Xixi Lu,Hajo A Reijers*

Main category: cs.LG

TL;DR: 这篇论文研究了从资源角度出发的下一活动预测，通过测试四种模型和三种编码策略在四个实际数据集上的表现，发现LightGBM和Transformer模型在2-gram活动迁移编码下表现最佳，而随机森林在结合2-gram迁移和活动重复特征的编码下表现最好。


<details>
  <summary>Details</summary>
Motivation: 现有的预测过程监控研究主要从控制流角度出发，本文研究从资源角度出发进行下一活动预测，这种方法能够帮助改善工作组织、平衡工作负荷、预测容量，并提供个性化的员工支持。

Method: 研究评估了四种预测模型（LightGBM、Transformer、随机森林等）和三种编码策略（包括2-gram活动迁移、活动重复特征等），在四个实际生产数据集上进行测试对比。

Result: 结果显示LightGBM和Transformer模型在2-gram活动迁移编码下表现最好，随机森林在结合2-gram迁移和活动重复特征的编码下表现最优。结合编码策略获得了最高的平均准确率。

Conclusion: 资源角度的下一活动预测具有重要价值，能够支持更智能的资源分配、战略性劳动力规划和个性化员工支持，为预测过程监控预测领域开启了新的研究方向。

Abstract: Predictive Process Monitoring (PPM) aims to train models that forecast
upcoming events in process executions. These predictions support early
bottleneck detection, improved scheduling, proactive interventions, and timely
communication with stakeholders. While existing research adopts a control-flow
perspective, we investigate next-activity prediction from a resource-centric
viewpoint, which offers additional benefits such as improved work organization,
workload balancing, and capacity forecasting. Although resource information has
been shown to enhance tasks such as process performance analysis, its role in
next-activity prediction remains unexplored. In this study, we evaluate four
prediction models and three encoding strategies across four real-life datasets.
Compared to the baseline, our results show that LightGBM and Transformer models
perform best with an encoding based on 2-gram activity transitions, while
Random Forest benefits most from an encoding that combines 2-gram transitions
and activity repetition features. This combined encoding also achieves the
highest average accuracy. This resource-centric approach could enable smarter
resource allocation, strategic workforce planning, and personalized employee
support by analyzing individual behavior rather than case-level progression.
The findings underscore the potential of resource-centric next-activity
prediction, opening up new venues for research on PPM.

</details>


### [80] [Metric Matters: A Formal Evaluation of Similarity Measures in Active Learning for Cyber Threat Intelligence](https://arxiv.org/abs/2508.19019)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.LG

TL;DR: 主动学习框架通过相似性搜索提升APT异常检测的准确性和标签效率，相似度函数的选择对模型性能有显著影响


<details>
  <summary>Details</summary>
Motivation: 解决APT攻击的潜伏性和检测数据集中极端类别不平衡的挑战

Method: 基于注意力自动编码器的主动学习框架，利用特征空间相似性识别正常和异常实例

Result: 在多样化数据集上证明相似度函数的选择对模型收敛、异常检测准确性和标签效率有显著影响

Conclusion: 为选择适合主动学习流程的相似度函数提供了可操作的见解，特别适用于威胁情报和网络防御

Abstract: Advanced Persistent Threats (APTs) pose a severe challenge to cyber defense
due to their stealthy behavior and the extreme class imbalance inherent in
detection datasets. To address these issues, we propose a novel active
learning-based anomaly detection framework that leverages similarity search to
iteratively refine the decision space. Built upon an Attention-Based
Autoencoder, our approach uses feature-space similarity to identify normal-like
and anomaly-like instances, thereby enhancing model robustness with minimal
oracle supervision. Crucially, we perform a formal evaluation of various
similarity measures to understand their influence on sample selection and
anomaly ranking effectiveness. Through experiments on diverse datasets,
including DARPA Transparent Computing APT traces, we demonstrate that the
choice of similarity metric significantly impacts model convergence, anomaly
detection accuracy, and label efficiency. Our results offer actionable insights
for selecting similarity functions in active learning pipelines tailored for
threat intelligence and cyber defense.

</details>


### [81] [GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling](https://arxiv.org/abs/2508.19028)
*Arash Jamshidi,Lauri Seppäläinen,Katsiaryna Haitsiukevich,Hoang Phuc Hau Luu,Anton Björklund,Kai Puolamäki*

Main category: cs.LG

TL;DR: GradStop是一种新颖的随机早停方法，仅利用梯度信息来防止过拟合，无需使用验证集，特别适用于数据有限场景。


<details>
  <summary>Details</summary>
Motivation: 传统早停方法需要保留验证集，减少了训练数据量。本文旨在开发一种仅利用梯度信息（梯度下降算法自然产生）的早停方法，避免数据浪费。

Method: 通过梯度信息估计贝叶斯后验分布，将早停问题定义为从该后验中采样，并使用近似后验获得停止准则。

Result: 实证评估显示GradStop在测试数据上实现较小损失，性能优于基于验证集的停止准则，特别在数据有限场景（如迁移学习）中表现优异。

Conclusion: GradStop是一种计算开销小、可集成到梯度下降库中的有效早停方法，能够充分利用全部数据进行训练，在数据稀缺情况下具有显著优势。

Abstract: Machine learning models are often learned by minimising a loss function on
the training data using a gradient descent algorithm. These models often suffer
from overfitting, leading to a decline in predictive performance on unseen
data. A standard solution is early stopping using a hold-out validation set,
which halts the minimisation when the validation loss stops decreasing.
However, this hold-out set reduces the data available for training. This paper
presents {\sc gradstop}, a novel stochastic early stopping method that only
uses information in the gradients, which are produced by the gradient descent
algorithm ``for free.'' Our main contributions are that we estimate the
Bayesian posterior by the gradient information, define the early stopping
problem as drawing sample from this posterior, and use the approximated
posterior to obtain a stopping criterion. Our empirical evaluation shows that
{\sc gradstop} achieves a small loss on test data and compares favourably to a
validation-set-based stopping criterion. By leveraging the entire dataset for
training, our method is particularly advantageous in data-limited settings,
such as transfer learning. It can be incorporated as an optional feature in
gradient descent libraries with only a small computational overhead. The source
code is available at https://github.com/edahelsinki/gradstop.

</details>


### [82] [When recalling in-context, Transformers are not SSMs](https://arxiv.org/abs/2508.19029)
*Destiny Okpekpe,Antonio Orvieto*

Main category: cs.LG

TL;DR: 这篇论文深入分析了现代重复模型和Transformer在联想回忆任务上的性能差异，发现学习率选择对重复模型致关重要，并揭示了不同网络结构在扩缩方向上的对比特点。


<details>
  <summary>Details</summary>
Motivation: 识别现代重复模型如状态空间模型(SSMs)与Transformer在理解和记忆任务上的性能差异，并深入研究联想回忆任务中的扩缩特性和优化问题。

Method: 通过对比学习率效果、网络宽度与深度扩缩、以及网络结构消融实验，分析Transformer和Mamba模型的性能和优化稳定性。

Result: 发现重复模型对学习率效果敏感，关注度模型在单层限制下无法解决AR任务，但单层Transformer的训练动态仍会形成引导头机制。

Conclusion: 现代重复模型需要更多研究来稳定训练过程，不同网络结构在扩缩方向上有明显差异，这为深度学习模型的设计提供了重要见解。

Abstract: Despite the advantageous subquadratic complexity of modern recurrent deep
learning models -- such as state-space models (SSMs) -- recent studies have
highlighted their potential shortcomings compared to transformers on reasoning
and memorization tasks. In this paper, we dive deeper into one of such
benchmarks: associative recall (AR), which has been shown to correlate well
with language modeling performance, and inspect in detail the effects of
scaling and optimization issues in recently proposed token mixing strategies.
We first demonstrate that, unlike standard transformers, the choice of learning
rate plays a critical role in the performance of modern recurrent models: an
issue that can severely affect reported performance in previous works and
suggests further research is needed to stabilize training. Next, we show that
recurrent and attention-based models exhibit contrasting benefits when scaling
in width as opposed to depth, with attention being notably unable to solve AR
when limited to a single layer. We then further inspect 1-layer transformers,
revealing that despite their poor performance, their training dynamics
surprisingly resemble the formation of induction heads, a phenomenon previously
observed only in their 2-layer counterparts. Finally, through architectural
ablations, we study how components affects Transformer and Mamba's performance
and optimization stability.

</details>


### [83] [Breaking the Black Box: Inherently Interpretable Physics-Informed Machine Learning for Imbalanced Seismic Data](https://arxiv.org/abs/2508.19031)
*Vemula Sreenath,Filippo Gatti,Pierre Jehel*

Main category: cs.LG

TL;DR: 使用透明的机器学习架构和HazBinLoss函数开发地震地动模型，解决传统黑盒模型的可解释性问题和数据不平衡问题


<details>
  <summary>Details</summary>
Motivation: 传统机器学习地震地动模型如同"黑盒"难以解释和信任，同时地震数据库存在严重的数据不平衡问题，近断层大震级记录较少而远场小震级记录估过多

Method: 设计透明的ML架构，每个输入参数独立处理并线性相加得到输出，确保每个因子的贡献可解释；使用HazBinLoss函数在训练中给临界近场大震级记录赋予更高权重，以避免对最具破坏性场景的预测不足

Result: 该模型能够捐描已知的地震学原理，与现有地震地动模型达到相似的性能水平，同时保持了模型的透明性

Conclusion: 该框架促进了基于机器学习的风险评估和灾害规划方法的更广泛采用，解决了黑盒模型的可解释性和数据偏料问题

Abstract: Ground motion models (GMMs) predict how strongly the ground will shake during
an earthquake. They are essential for structural analysis, seismic design, and
seismic risk assessment studies. Traditional machine learning (ML) approaches
are popular to develop GMMs, due to large earthquake databases worldwide.
However, they operate as "black boxes," which are hard to interpret and trust,
limiting their use in high-stake decisions. Additionally, these databases
suffer from significant data imbalances: fewer large, critically damaging
records near the fault compared to abundant, less severely damaging distant
records. These two limitations are addressed in this work by developing a
transparent ML architecture using the HazBinLoss function. Each input (e.g.,
magnitude, distance, their interaction term, etc.) is processed separately and
added linearly to obtain the output, resulting in exact contribution of each
term. The HazBinLoss function assigns higher weights to critical near-field
large magnitude records and lower weights to less-critical far-field smaller
magnitude records, during training to prevent underprediction of the most
damaging scenarios. Our model captures known seismological principles and
achieves comparable performance with established GMMs while maintaining
transparency. This framework enables broader adoption of ML-based approaches
for risk assessment studies and disaster planning.

</details>


### [84] [Automated discovery of finite volume schemes using Graph Neural Networks](https://arxiv.org/abs/2508.19052)
*Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: GNNs不仅能够近似物理系统解，还能通过符号回归生成数值格式，包括一阶和二阶有限体积格式，甚至在无监督情况下仅使用残差损失重新发现标准数值格式。


<details>
  <summary>Details</summary>
Motivation: 探索GNN在传统近似角色之外的能力，特别是在外推和数值格式生成方面的潜力，解决GNN在训练域外泛化能力不确定的问题。

Method: 使用GNN结合符号回归方法，在包含两节点图的数据集上训练，通过残差损失（类似PINNs）在无监督情况下重新发现有限体积格式。

Result: GNN能够外推出一阶热方程有限体积格式，误差为O(ε)；通过符号回归重新发现标准一阶格式；在无监督情况下恢复一阶格式；2-hop和2层GNN分别发现二阶修正项和中点格式。

Conclusion: GNN不仅是强大的近似器，还能主动贡献于新型数值方法的开发，为科学计算开辟了新范式。

Abstract: Graph Neural Networks (GNNs) have deeply modified the landscape of numerical
simulations by demonstrating strong capabilities in approximating solutions of
physical systems. However, their ability to extrapolate beyond their training
domain (\textit{e.g.} larger or structurally different graphs) remains
uncertain. In this work, we establish that GNNs can serve purposes beyond their
traditional role, and be exploited to generate numerical schemes, in
conjunction with symbolic regression. First, we show numerically and
theoretically that a GNN trained on a dataset consisting solely of two-node
graphs can extrapolate a first-order Finite Volume (FV) scheme for the heat
equation on out-of-distribution, unstructured meshes. Specifically, if a GNN
achieves a loss $\varepsilon$ on such a dataset, it implements the FV scheme
with an error of $\mathcal{O}(\varepsilon)$. Using symbolic regression, we show
that the network effectively rediscovers the exact analytical formulation of
the standard first-order FV scheme. We then extend this approach to an
unsupervised context: the GNN recovers the first-order FV scheme using only a
residual loss similar to Physics-Informed Neural Networks (PINNs) with no
access to ground-truth data. Finally, we push the methodology further by
considering higher-order schemes: we train (i) a 2-hop and (ii) a 2-layers GNN
using the same PINN loss, that autonomously discover (i) a second-order
correction term to the initial scheme using a 2-hop stencil, and (ii) the
classic second-order midpoint scheme. These findings follows a recent paradigm
in scientific computing: GNNs are not only strong approximators, but can be
active contributors to the development of novel numerical methods.

</details>


### [85] [Tackling Federated Unlearning as a Parameter Estimation Problem](https://arxiv.org/abs/2508.19065)
*Antonio Balordi,Lorenzo Manini,Fabio Stella,Alessio Merlo*

Main category: cs.LG

TL;DR: 基于信息论的敏感参数重置策略，通过二阶Hessian信息识别和重置最受删除数据影响的参数，并进行最小化联邦重训练，实现高效联邦忘却。


<details>
  <summary>Details</summary>
Motivation: 隐私规定要求从深度学习模型中删除数据，而在联邦学习中这一挑战更为复杂，因为数据保留在客户端且完全重训练或协调更新往往不可行。

Method: 使用二阶Hessian信息来识别和选择性重置那些对要忘却数据最敏感的参数，然后进行最小化的联邦重训练。该方法模型无关，不需要服务器在初始信息聚合后访问原始客户数据。

Result: 在标准数据集上评估显示了强大的隐私保护（MIA攻击成功率接近随机猜测，类别知识被彻底删除）和高性能（与重训练基准比较的标准化准确率约为0.9）。同时在目标后门攻击场景中，该框架有效中和了恶意触发器，恢复了模型完整性。

Conclusion: 该框架为联邦学习中的数据忘却提供了一种实用解决方案，通过信息论建模和敏感参数重置策略，在保持高性能的同时实现了强大的隐私保护和模型完整性。

Abstract: Privacy regulations require the erasure of data from deep learning models.
This is a significant challenge that is amplified in Federated Learning, where
data remains on clients, making full retraining or coordinated updates often
infeasible. This work introduces an efficient Federated Unlearning framework
based on information theory, modeling leakage as a parameter estimation
problem. Our method uses second-order Hessian information to identify and
selectively reset only the parameters most sensitive to the data being
forgotten, followed by minimal federated retraining. This model-agnostic
approach supports categorical and client unlearning without requiring server
access to raw client data after initial information aggregation. Evaluations on
benchmark datasets demonstrate strong privacy (MIA success near random,
categorical knowledge erased) and high performance (Normalized Accuracy against
re-trained benchmarks of $\approx$ 0.9), while aiming for increased efficiency
over complete retraining. Furthermore, in a targeted backdoor attack scenario,
our framework effectively neutralizes the malicious trigger, restoring model
integrity. This offers a practical solution for data forgetting in FL.

</details>


### [86] [Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks](https://arxiv.org/abs/2508.19071)
*Hugo Attali,Thomas Papastergiou,Nathalie Pernelle,Fragkiskos D. Malliaros*

Main category: cs.LG

TL;DR: TRIGON是一个新颖的图重布线框架，通过从多视图学习选择相关三角形来构建丰富的非平面三角剖分，有效解决了GNN中的过挤压和过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络(GNN)的性能受到图拓扑固有问题的限制，特别是过挤压(oversquashing)和过平滑(oversmoothing)问题，需要新的图重布线方法来改善信息传播效果。

Method: TRIGON框架通过联合优化三角形选择和下游分类性能，从多个图视图中学习选择相关三角形，构建非平面三角剖分来重布线图结构。

Result: 实验结果表明，TRIGON在节点分类任务上优于现有最先进方法，在多种同质性和异质性基准测试中表现优异，产生的重布线图具有更小的直径、更大的谱间隙和更低的有效电阻。

Conclusion: TRIGON通过创新的三角剖分学习方法有效改善了图拓扑结构，为解决GNN中的过挤压和过平滑问题提供了有效的解决方案，在多个基准测试中展现了优越性能。

Abstract: Graph Neural Networks (GNNs) have emerged as the leading paradigm for
learning over graph-structured data. However, their performance is limited by
issues inherent to graph topology, most notably oversquashing and
oversmoothing. Recent advances in graph rewiring aim to mitigate these
limitations by modifying the graph topology to promote more effective
information propagation. In this work, we introduce TRIGON, a novel framework
that constructs enriched, non-planar triangulations by learning to select
relevant triangles from multiple graph views. By jointly optimizing triangle
selection and downstream classification performance, our method produces a
rewired graph with markedly improved structural properties such as reduced
diameter, increased spectral gap, and lower effective resistance compared to
existing rewiring methods. Empirical results demonstrate that TRIGON
outperforms state-of-the-art approaches on node classification tasks across a
range of homophilic and heterophilic benchmarks.

</details>


### [87] [APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration](https://arxiv.org/abs/2508.19087)
*Shaobo Ma,Chao Fang,Haikuo Shao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: APT-LLM是一个针对任意精度大语言模型的全方位加速方案，通过新型数据格式、矩阵乘法方法、内存管理系统和内核映射优化，在GPU上实现了显著的推理加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算需求巨大，现有量化方法在GPU上支持有限，特别是在超低位宽任意精度量化方面存在GPU Tensor Core支持不足、内存管理低效和内核优化不灵活等问题。

Method: 提出APT-LLM方案：1）双极INT数据格式实现高效无损转换和并行计算；2）位级矩阵分解重组方法支持任意精度；3）基于数据恢复的内存管理系统；4）动态内核映射优化超参数选择。

Result: 在RTX 3090上相比FP16基线实现3.99倍加速，相比NVIDIA CUTLASS INT4实现2.16倍加速；在RTX 4090和H800上分别实现2.44倍和1.65倍加速。

Conclusion: APT-LLM有效解决了任意精度LLM在GPU上的加速挑战，为超低位宽量化模型的部署提供了高效解决方案。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
enormous computational demands severely limit deployment and real-time
performance. Quantization methods can help reduce computational costs, however,
attaining the extreme efficiency associated with ultra-low-bit quantized LLMs
at arbitrary precision presents challenges on GPUs. This is primarily due to
the limited support for GPU Tensor Cores, inefficient memory management, and
inflexible kernel optimizations. To tackle these challenges, we propose a
comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.
Firstly, we introduce a novel data format, bipolar-INT, which allows for
efficient and lossless conversion with signed INT, while also being more
conducive to parallel computation. We also develop a matrix multiplication
(MatMul) method allowing for arbitrary precision by dismantling and
reassembling matrices at the bit level. This method provides flexible precision
and optimizes the utilization of GPU Tensor Cores. In addition, we propose a
memory management system focused on data recovery, which strategically employs
fast shared memory to substantially increase kernel execution speed and reduce
memory access latency. Finally, we develop a kernel mapping method that
dynamically selects the optimal configurable hyperparameters of kernels for
varying matrix sizes, enabling optimal performance across different LLM
architectures and precision settings. In LLM inference, APT-LLM achieves up to
a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup
over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,
APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup
over CUTLASS integer baselines.

</details>


### [88] [Active Query Selection for Crowd-Based Reinforcement Learning](https://arxiv.org/abs/2508.19132)
*Jonathan Erskine,Taku Yamagata,Raúl Santos-Rodríguez*

Main category: cs.LG

TL;DR: 提出结合概率众包建模和主动学习的新框架，处理多标注者噪声反馈，通过熵基查询选择优先获取信息量最大的动作反馈，在多个环境中验证有效性


<details>
  <summary>Details</summary>
Motivation: 基于偏好的强化学习依赖人类反馈，但高质量人类输入成本高且稀缺，特别是在专家反馈稀少或错误代价高的领域需要更高效的反馈利用方法

Method: 扩展Advise算法支持多训练器，在线估计其可靠性，结合熵基查询选择来指导反馈请求，使用概率众包建模处理噪声多标注者反馈

Result: 在Taxi、Pacman、Frozen Lake等2D游戏和血糖控制任务中，对不确定轨迹进行反馈的智能体学习速度更快，在血糖控制任务中优于基线方法

Conclusion: 结合概率众包建模和主动学习的框架能有效处理噪声人类反馈，提高基于偏好强化学习的样本效率，在医疗等关键领域具有应用潜力

Abstract: Preference-based reinforcement learning has gained prominence as a strategy
for training agents in environments where the reward signal is difficult to
specify or misaligned with human intent. However, its effectiveness is often
limited by the high cost and low availability of reliable human input,
especially in domains where expert feedback is scarce or errors are costly. To
address this, we propose a novel framework that combines two complementary
strategies: probabilistic crowd modelling to handle noisy, multi-annotator
feedback, and active learning to prioritize feedback on the most informative
agent actions. We extend the Advise algorithm to support multiple trainers,
estimate their reliability online, and incorporate entropy-based query
selection to guide feedback requests. We evaluate our approach in a set of
environments that span both synthetic and real-world-inspired settings,
including 2D games (Taxi, Pacman, Frozen Lake) and a blood glucose control task
for Type 1 Diabetes using the clinically approved UVA/Padova simulator. Our
preliminary results demonstrate that agents trained with feedback on uncertain
trajectories exhibit faster learning in most tasks, and we outperform the
baselines for the blood glucose control task.

</details>


### [89] [Saddle Hierarchy in Dense Associative Memory](https://arxiv.org/abs/2508.19151)
*Robin Thériault,Daniele Tantari*

Main category: cs.LG

TL;DR: 本文研究了基于三层玻尔兹曼机的密集联想记忆模型，通过统计力学分析推导了鞍点方程，提出了新的正则化方案使训练更稳定，并开发了基于鞍点层次结构的网络增长算法来降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 密集联想记忆模型因其对对抗样本的鲁棒性以及与transformer注意力机制、生成扩散模型等前沿机器学习范式的紧密联系而受到关注，需要深入研究其理论特性和训练方法。

Method: 使用三层玻尔兹曼机构建DAM模型，通过统计力学分析推导鞍点方程，提出新的正则化方案，并开发基于鞍点层次结构的网络增长算法。

Result: 模型能够学习到可解释的监督和无监督分类解决方案，提出的正则化使训练显著更稳定，网络增长算法大幅降低了训练计算成本。

Conclusion: DAM模型具有重要的理论价值和实用潜力，通过统计力学分析和算法创新可以有效解决训练稳定性和计算效率问题，为密集联想记忆的进一步发展提供了理论基础和方法支持。

Abstract: Dense associative memory (DAM) models have been attracting renewed attention
since they were shown to be robust to adversarial examples and closely related
to state-of-the-art machine learning paradigms, such as the attention
mechanisms in transformers and generative diffusion models. We study a DAM
built upon a three-layer Boltzmann machine with Potts hidden units, which
represent data clusters and classes. Through a statistical mechanics analysis,
we derive saddle-point equations that characterize both the stationary points
of DAMs trained on real data and the fixed points of DAMs trained on synthetic
data within a teacher-student framework. Based on these results, we propose a
novel regularization scheme that makes training significantly more stable.
Moreover, we show empirically that our DAM learns interpretable solutions to
both supervised and unsupervised classification problems. Pushing our
theoretical analysis further, we find that the weights learned by relatively
small DAMs correspond to unstable saddle points in larger DAMs. We implement a
network-growing algorithm that leverages this saddle-point hierarchy to
drastically reduce the computational cost of training dense associative memory.

</details>


### [90] [Get Global Guarantees: On the Probabilistic Nature of Perturbation Robustness](https://arxiv.org/abs/2508.19183)
*Wenchuan Mu,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 本文提出塔鲁棒性(tower robustness)作为新的鲁棒性评估指标，基于假设检验来定量评估概率鲁棒性，解决了现有方法在计算成本和测量精度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的深度学习应用中，现有鲁棒性评估方法存在计算成本与测量精度之间的显著权衡，限制了实际应用价值。

Method: 对现有鲁棒性定义和评估方法进行全面比较分析，提出基于假设检验的塔鲁棒性指标来定量评估概率鲁棒性。

Result: 广泛的比较评估证明了所提方法的优势和适用性，能够实现更严格和高效的部署前评估。

Conclusion: 该方法推进了对安全关键深度学习应用中模型鲁棒性的系统理解和增强。

Abstract: In safety-critical deep learning applications, robustness measures the
ability of neural models that handle imperceptible perturbations in input data,
which may lead to potential safety hazards. Existing pre-deployment robustness
assessment methods typically suffer from significant trade-offs between
computational cost and measurement precision, limiting their practical utility.
To address these limitations, this paper conducts a comprehensive comparative
analysis of existing robustness definitions and associated assessment
methodologies. We propose tower robustness to evaluate robustness, which is a
novel, practical metric based on hypothesis testing to quantitatively evaluate
probabilistic robustness, enabling more rigorous and efficient pre-deployment
assessments. Our extensive comparative evaluation illustrates the advantages
and applicability of our proposed approach, thereby advancing the systematic
understanding and enhancement of model robustness in safety-critical deep
learning applications.

</details>


### [91] [Emotions as Ambiguity-aware Ordinal Representations](https://arxiv.org/abs/2508.19193)
*Jingyao Wu,Matthew Barthet,David Melhart,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: 本文提出了一种新的模糊感知序数情感表示框架，能够同时捕捉情感标注的模糊性和情感轨迹的时间动态特性。


<details>
  <summary>Details</summary>
Motivation: 现有连续情感识别方法要么忽略情感的模糊性，要么将模糊性视为独立且静态的变量，无法有效处理情感固有的模糊性和动态特性。

Method: 通过情感变化率来建模情感模糊性，提出模糊感知序数情感表示方法，在RECOLA和GameVibe两个情感语料库上进行评估。

Result: 对于无界标签（如参与度），序数表示优于传统模糊感知模型，获得最高的CCC和SDA分数；对于有界轨迹，序数表示在SDA方面表现优异。

Conclusion: 序数表示能够有效捕捉标注情感轨迹的相对变化，在处理情感模糊性和动态特性方面具有显著优势。

Abstract: Emotions are inherently ambiguous and dynamic phenomena, yet existing
continuous emotion recognition approaches either ignore their ambiguity or
treat ambiguity as an independent and static variable over time. Motivated by
this gap in the literature, in this paper we introduce \emph{ambiguity-aware
ordinal} emotion representations, a novel framework that captures both the
ambiguity present in emotion annotation and the inherent temporal dynamics of
emotional traces. Specifically, we propose approaches that model emotion
ambiguity through its rate of change. We evaluate our framework on two
affective corpora -- RECOLA and GameVibe -- testing our proposed approaches on
both bounded (arousal, valence) and unbounded (engagement) continuous traces.
Our results demonstrate that ordinal representations outperform conventional
ambiguity-aware models on unbounded labels, achieving the highest Concordance
Correlation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,
highlighting their effectiveness in modeling the traces' dynamics. For bounded
traces, ordinal representations excel in SDA, revealing their superior ability
to capture relative changes of annotated emotion traces.

</details>


### [92] [Predicting the Order of Upcoming Tokens Improves Language Modeling](https://arxiv.org/abs/2508.19228)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TL;DR: 提出Token Order Prediction (TOP)方法替代Multi-Token Prediction (MTP)，通过排序学习损失来预测token的顺序而非精确token，在多个NLP基准测试中表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: MTP作为辅助目标在语言模型训练中表现不一致，在标准NLP基准测试中表现不佳，因为精确预测未来token过于困难

Method: 提出Token Order Prediction (TOP)，使用学习排序损失训练模型按接近程度对即将到来的token进行排序，相比MTP仅需单个额外的unembedding层

Result: 在340M、1.8B和7B参数规模的模型预训练中，TOP在8个标准NLP基准测试中整体优于NTP和MTP，即使在较大规模下也保持优势

Conclusion: TOP是一种有效的辅助训练目标，通过降低预测难度（从精确token预测到顺序预测）来提升语言模型性能，计算开销更小且效果更好

Abstract: Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to
improve next-token prediction (NTP) in language model training but shows
inconsistent improvements, underperforming in standard NLP benchmarks. We argue
that MTP's exact future token prediction is too difficult as an auxiliary loss.
Instead, we propose Token Order Prediction (TOP), which trains models to order
upcoming tokens by their proximity using a learning-to-rank loss. TOP requires
only a single additional unembedding layer compared to MTP's multiple
transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using
NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show
that TOP overall outperforms both NTP and MTP even at scale. Our code is
available at https://github.com/zaydzuhri/token-order-prediction

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [93] [Deterministic Coreset Construction via Adaptive Sensitivity Trimming](https://arxiv.org/abs/2508.18340)
*Faruk Alpay,Taylan Alpay*

Main category: stat.ML

TL;DR: 提出了ADUWT算法，通过修剪最低敏感度边界点并应用数据依赖的均匀权重来构建核心集，为ERM目标提供均匀的相对误差近似


<details>
  <summary>Details</summary>
Motivation: 为经验风险最小化（ERM）问题开发一个严格的理论框架，构建确定性核心集以提升计算效率

Method: 自适应确定性均匀权重修剪（ADUWT）算法，包括敏感度边界计算、点修剪和均匀权重分配

Result: 获得了整个假设空间上ERM目标的均匀(1±ε)相对误差近似，证明了自适应权重的最优性，并提供了可处理的敏感度预言机

Conclusion: ADUWT算法在理论和实验上都取得了成功，为ERM核心集构建提供了有效解决方案，并提出了未来研究方向

Abstract: We develop a rigorous framework for deterministic coreset construction in
empirical risk minimization (ERM). Our central contribution is the Adaptive
Deterministic Uniform-Weight Trimming (ADUWT) algorithm, which constructs a
coreset by excising points with the lowest sensitivity bounds and applying a
data-dependent uniform weight to the remainder. The method yields a uniform
$(1\pm\varepsilon)$ relative-error approximation for the ERM objective over the
entire hypothesis space. We provide complete analysis, including (i) a minimax
characterization proving the optimality of the adaptive weight, (ii) an
instance-dependent size analysis in terms of a \emph{Sensitivity Heterogeneity
Index}, and (iii) tractable sensitivity oracles for kernel ridge regression,
regularized logistic regression, and linear SVM. Reproducibility is supported
by precise pseudocode for the algorithm, sensitivity oracles, and evaluation
pipeline. Empirical results align with the theory. We conclude with open
problems on instance-optimal oracles, deterministic streaming, and
fairness-constrained ERM.

</details>


### [94] [Revisiting Follow-the-Perturbed-Leader with Unbounded Perturbations in Bandit Problems](https://arxiv.org/abs/2508.18604)
*Jongyeong Lee,Junya Honda,Shinji Ito,Min-hwan Oh*

Main category: stat.ML

TL;DR: 该论文研究了FTPL（Follow-the-Perturbed-Leader）算法在非对称无界Fréchet型扰动下的Best-of-Both-Worlds（BOBW）性能，扩展了FTPL的理论基础，并探讨了对称Fréchet型扰动在多臂赌博机中的局限性。


<details>
  <summary>Details</summary>
Motivation: FTRL策略通过混合正则化器已在多个设置中实现BOBW结果，但FTPL的类似结果由于分析挑战而有限。论文旨在推进FTPL的分析基础，建立更广泛的BOBW保证。

Method: 重新审视无界扰动的FTRL-FTPL对偶性，研究非对称无界Fréchet型扰动家族（包括混合Gumbel型和Fréchet型尾部的扰动），并分析对称Fréchet型扰动在双臂和多臂赌博机中的行为。

Result: 建立了FTPL在非对称无界Fréchet型扰动下的BOBW结果，在双臂设置中首次实现了对称无界扰动的BOBW保证，但在多臂设置中发现对称Fréchet型扰动违反标准BOBW分析的关键条件。

Conclusion: 研究扩展了FTPL的BOBW结果，提供了与混合正则化方法竞争的新FTPL策略设计见解，但对称Fréchet型扰动在多臂设置中的局限性表明需要进一步研究以全面理解FTPL在更广泛设置中的行为。

Abstract: Follow-the-Regularized-Leader (FTRL) policies have achieved
Best-of-Both-Worlds (BOBW) results in various settings through hybrid
regularizers, whereas analogous results for Follow-the-Perturbed-Leader (FTPL)
remain limited due to inherent analytical challenges. To advance the analytical
foundations of FTPL, we revisit classical FTRL-FTPL duality for unbounded
perturbations and establish BOBW results for FTPL under a broad family of
asymmetric unbounded Fr\'echet-type perturbations, including hybrid
perturbations combining Gumbel-type and Fr\'echet-type tails. These results not
only extend the BOBW results of FTPL but also offer new insights into designing
alternative FTPL policies competitive with hybrid regularization approaches.
Motivated by earlier observations in two-armed bandits, we further investigate
the connection between the $1/2$-Tsallis entropy and a Fr\'echet-type
perturbation. Our numerical observations suggest that it corresponds to a
symmetric Fr\'echet-type perturbation, and based on this, we establish the
first BOBW guarantee for symmetric unbounded perturbations in the two-armed
setting. In contrast, in general multi-armed bandits, we find an instance in
which symmetric Fr\'echet-type perturbations violate the key condition for
standard BOBW analysis, which is a problem not observed with asymmetric or
nonnegative Fr\'echet-type perturbations. Although this example does not rule
out alternative analyses achieving BOBW results, it suggests the limitations of
directly applying the relationship observed in two-armed cases to the general
case and thus emphasizes the need for further investigation to fully understand
the behavior of FTPL in broader settings.

</details>


### [95] [Efficient Best-of-Both-Worlds Algorithms for Contextual Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.18768)
*Mengmeng Li,Philipp Schneider,Jelisaveta Aleksić,Daniel Kuhn*

Main category: stat.ML

TL;DR: 提出了首个上下文组合半赌博的最佳两界算法，在对抗性环境下实现√T遗憾，在受污染随机环境下实现对数遗憾，并通过KKT条件将高维投影问题转化为单变量求根问题，大幅加速计算


<details>
  <summary>Details</summary>
Motivation: 解决上下文组合半赌博问题中同时保证对抗性和随机性环境下的最优遗憾界，并克服FTRL框架中高维投影计算瓶颈

Method: 基于FTRL框架，使用香农熵正则化器，利用KKT条件将K维凸投影问题转化为单变量求根问题

Result: 算法在对抗性环境下达到O(√T)遗憾，在受污染随机环境下达到O(ln T)遗憾，并实现显著的每轮计算加速

Conclusion: 该方法不仅获得了最佳两界算法的吸引人遗憾界，还提供了实质性的计算加速，适用于大规模实时应用

Abstract: We introduce the first best-of-both-worlds algorithm for contextual
combinatorial semi-bandits that simultaneously guarantees
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret in the adversarial regime and
$\widetilde{\mathcal{O}}(\ln T)$ regret in the corrupted stochastic regime. Our
approach builds on the Follow-the-Regularized-Leader (FTRL) framework equipped
with a Shannon entropy regularizer, yielding a flexible method that admits
efficient implementations. Beyond regret bounds, we tackle the practical
bottleneck in FTRL (or, equivalently, Online Stochastic Mirror Descent) arising
from the high-dimensional projection step encountered in each round of
interaction. By leveraging the Karush-Kuhn-Tucker conditions, we transform the
$K$-dimensional convex projection problem into a single-variable root-finding
problem, dramatically accelerating each round. Empirical evaluations
demonstrate that this combined strategy not only attains the attractive regret
bounds of best-of-both-worlds algorithms but also delivers substantial
per-round speed-ups, making it well-suited for large-scale, real-time
applications.

</details>


### [96] [Sparse minimum Redundancy Maximum Relevance for feature selection](https://arxiv.org/abs/2508.18901)
*Peter Naylor,Benjamin Poignard,Héctor Climente-González,Makoto Yamada*

Main category: stat.ML

TL;DR: 提出了一种结合特征-特征和特征-目标关系的特征筛选方法，使用惩罚性最小冗余最大相关性(mRMR)程序识别非活跃特征，并通过knockoff滤波器控制错误发现率。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法往往只考虑特征与目标的关系，忽略了特征间的相互关系。需要一种能够同时考虑特征-特征和特征-目标关系，并能控制错误发现率的特征筛选方法。

Method: 使用惩罚性mRMR方法（非凸正则化的连续版本），通过多阶段knockoff滤波器程序来丢弃非活跃特征，零系数代表非活跃特征集。

Result: 方法性能与HSIC-LASSO相当，但在选择特征数量上更加保守，只需设置FDR阈值而不需指定保留特征数量。在模拟和真实数据集上验证了有效性。

Conclusion: 该方法提供了一种有效的特征筛选解决方案，能够准确识别非活跃特征并控制错误发现率，具有实际应用价值。

Abstract: We propose a feature screening method that integrates both feature-feature
and feature-target relationships. Inactive features are identified via a
penalized minimum Redundancy Maximum Relevance (mRMR) procedure, which is the
continuous version of the classic mRMR penalized by a non-convex regularizer,
and where the parameters estimated as zero coefficients represent the set of
inactive features. We establish the conditions under which zero coefficients
are correctly identified to guarantee accurate recovery of inactive features.
We introduce a multi-stage procedure based on the knockoff filter enabling the
penalized mRMR to discard inactive features while controlling the false
discovery rate (FDR). Our method performs comparably to HSIC-LASSO but is more
conservative in the number of selected features. It only requires setting an
FDR threshold, rather than specifying the number of features to retain. The
effectiveness of the method is illustrated through simulations and real-world
datasets. The code to reproduce this work is available on the following GitHub:
https://github.com/PeterJackNaylor/SmRMR.

</details>


### [97] [Echoes of the past: A unified perspective on fading memory and echo states](https://arxiv.org/abs/2508.19145)
*Juan-Pablo Ortega,Florian Rossmannek*

Main category: stat.ML

TL;DR: 本文统一了RNN中各种记忆概念（稳态、回声状态、状态遗忘、输入遗忘、衰减记忆）的关系，提供了新的等价性证明和替代证明，澄清了这些概念之间的联系。


<details>
  <summary>Details</summary>
Motivation: RNN在处理时序数据时表现出不同的记忆行为，但现有的各种记忆概念（如稳态、回声状态、遗忘等）虽然经常被互换使用，但它们之间的精确关系尚不清楚，需要统一的理论框架来澄清这些概念。

Method: 采用统一的数学语言来描述各种记忆概念，推导这些概念之间的新含义和等价关系，并为一些现有结果提供替代证明方法。

Result: 建立了不同记忆概念之间的明确关系，证明了某些概念之间的等价性，为理解RNN的记忆机制提供了更清晰的理论基础。

Conclusion: 通过统一各种记忆概念的语言和关系，这项工作深化了对RNN时序信息处理能力的理解，为RNN的理论研究提供了更坚实的框架。

Abstract: Recurrent neural networks (RNNs) have become increasingly popular in
information processing tasks involving time series and temporal data. A
fundamental property of RNNs is their ability to create reliable input/output
responses, often linked to how the network handles its memory of the
information it processed. Various notions have been proposed to conceptualize
the behavior of memory in RNNs, including steady states, echo states, state
forgetting, input forgetting, and fading memory. Although these notions are
often used interchangeably, their precise relationships remain unclear. This
work aims to unify these notions in a common language, derive new implications
and equivalences between them, and provide alternative proofs to some existing
results. By clarifying the relationships between these concepts, this research
contributes to a deeper understanding of RNNs and their temporal information
processing capabilities.

</details>
