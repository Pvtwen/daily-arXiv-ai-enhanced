<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 20]
- [cs.LG](#cs.LG) [Total: 98]
- [stat.ML](#stat.ML) [Total: 2]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [FAS-ARIS: Turning Multipath Challenges Into Localization Opportunities](https://arxiv.org/abs/2509.12348)
*Hua Chen,Tao Gong,Tuo Wu,Maged Elkashlan,Baiyang Liu,Chan-Byoung Chae,Kin-Fai Tong,Kai-Kit Wong*

Main category: eess.SP

TL;DR: 提出了一种新颖的FAS-ARIS框架，将多径效应从障碍转化为资源，通过信号解耦策略和几何三角测量实现高精度的3D定位，无需依赖ToA或频率分集。


<details>
  <summary>Details</summary>
Motivation: 传统SISO系统在3D定位中存在空间自由度有限和多径传播负面影响的问题，需要一种能够利用多径效应而不是被其限制的新方法。

Method: 结合ARIS的信号放大能力和FAS的空间分集，设计专门的UE导频序列和ARIS相位配置来分离LoS和NLoS信道，采用多阶段估计算法（MUSIC算法估计AoA，最大似然估计恢复级联信道参数），最后通过最小二乘估计进行几何三角测量。

Result: 仿真结果表明，该框架在丰富多径环境中实现了接近最优的定位精度，并保持了鲁棒性，成功将传统定位挑战转化为优势。

Conclusion: FAS-ARIS框架通过创新性地利用多径效应，为3D定位提供了一种有效且稳健的解决方案，突破了传统方法的局限性。

Abstract: Traditional single-input single-output (SISO) systems face fundamental
limitations in achieving accurate three-dimensional (3D) localization due to
limited spatial degrees of freedom (DoF) and the adverse impact of multipath
propagation. This paper proposes a novel fluid antenna system (FAS)-active
reconfigurable intelligent surface (ARIS) framework that transforms multipath
effects from a hindrance into a resource for enhanced localization. By
synergistically combining the signal amplification capabilities of ARIS with
the spatial diversity enabled by FAS, the proposed system achieves robust 3D
user equipment (UE) positioning -- without relying on auxiliary information
such as time-of-arrival (ToA) or frequency diversity. The system exploits both
line-of-sight (LoS) and non-line-of-sight (NLoS) components through a tailored
signal decoupling strategy. We design novel UE pilot sequences and ARIS phase
configurations to effectively separate LoS and NLoS channels, enabling
independent parameter estimation. A multi-stage estimation algorithm is then
applied: the multiple signal classification (MUSIC) algorithm estimates
angle-of-arrival (AoA) from the direct path, while maximum likelihood
estimation with interior-point refinement recovers cascaded channel parameters
from the reflected path. Finally, geometric triangulation using least-squares
estimation determines the UE's 3D position based on the extracted AoA
information. Comprehensive performance analysis, including the derivation of
Cram\'{e}r-Rao bounds for both channel and position estimation, establishes
theoretical benchmarks. Simulation results confirm that the proposed FAS-ARIS
framework achieves near-optimal localization accuracy while maintaining
robustness in rich multipath environments -- effectively turning conventional
localization challenges into advantages.

</details>


### [2] [Partial Secrecy Analysis in Wireless Systems: Diversity-Enhanced PLS over Generalized Fading Channels](https://arxiv.org/abs/2509.12359)
*Henry Carvajal Mora,Nathaly Orozco,Fernando Almeida García,José Vega-Sánchez,Felipe Grijalva,Edgar Benitez Olivo*

Main category: eess.SP

TL;DR: 该论文分析了广义多簇波动双射线(MFTR)衰落模型下的部分保密性能，推导了关键保密指标的精确和闭式近似表达式，并通过蒙特卡洛仿真验证了结果。


<details>
  <summary>Details</summary>
Motivation: 未来移动网络中信息安全的挑战，特别是对于计算资源有限的设备。当无法实现完全保密时，部分保密机制提供了一个现实的替代方案。

Method: 研究包含发射机(A)、合法接收机(B)和窃听者(E)的系统，B和E都使用最大比合并(MRC)天线阵列，在独立非同分布(i.n.i.d.)衰落条件下，基于广义多簇波动双射线(MFTR)衰落模型进行分析。

Result: 推导了广义保密中断概率(GSOP)、平均分数模糊度(AFE)和平均信息泄漏率(AILR)的精确和闭式近似表达式，结果复杂度与分集阶数无关，并通过蒙特卡洛仿真验证。

Conclusion: MFTR模型的灵活性使得能够全面评估不同衰落条件下的保密性能，结果显示B端更多MRC分支能够增强保密性能，但这取决于A-E链路的特性。

Abstract: Securing information in future mobile networks is challenging, especially for
devices with limited computational resources. Physical layer security (PLS)
offers a viable solution by leveraging wireless channel randomness. When full
secrecy is unattainable, the partial secrecy regime provides a realistic
alternative. This work analyzes partial secrecy performance under the
generalized multicluster fluctuating two-ray (MFTR) fading model, which
subsumes many classical fading cases. We study a system with a transmitter (A),
legitimate receiver (B), and eavesdropper (E), both B and E using antenna
arrays with maximal ratio combining (MRC), under i.n.i.d. fading. Exact and
closed-form approximations are derived for key secrecy metrics: generalized
secrecy outage probability (GSOP), average fractional equivocation (AFE), and
average information leakage rate (AILR). The results, validated by Monte Carlo
simulations, retain constant complexity regardless of diversity order. The MFTR
model's flexibility enables comprehensive assessment across fading conditions,
showing that more MRC branches at B enhance secrecy performance depending on
the A-E link characteristics.

</details>


### [3] [Self-Supervised and Topological Signal-Quality Assessment for Any PPG Device](https://arxiv.org/abs/2509.12510)
*Wei Shao,Ruoyu Zhang,Zequan Liang,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun*

Main category: eess.SP

TL;DR: 提出了首个完全无监督的腕部PPG信号质量评估管道，结合自监督学习和拓扑数据分析，实现跨设备的信号质量分类


<details>
  <summary>Details</summary>
Motivation: 可穿戴光电容积描记(PPG)信号易受运动、灌注损失和环境光干扰，现有信号质量评估方法要么依赖脆弱的启发式规则，要么需要大量标注数据

Method: 两阶段方法：第一阶段使用对比学习的1-D ResNet-18在276小时无标注数据上训练，获得光学发射器和运动不变的嵌入表示；第二阶段通过持久同调将512维嵌入转换为4维拓扑特征，用HDBSCAN聚类生成二进制信号质量指数

Result: 在不重新调优的情况下，在10,000个窗口的分层样本上获得轮廓系数0.72、Davies-Bouldin指数0.34和Calinski-Harabasz指数6173的优异聚类性能

Conclusion: 提出的SSL-TDA混合框架为PPG信号提供了一个即插即用、可扩展、跨设备的信号质量门控解决方案

Abstract: Wearable photoplethysmography (PPG) is embedded in billions of devices, yet
its optical waveform is easily corrupted by motion, perfusion loss, and ambient
light, jeopardizing downstream cardiometric analytics. Existing signal-quality
assessment (SQA) methods rely either on brittle heuristics or on data-hungry
supervised models. We introduce the first fully unsupervised SQA pipeline for
wrist PPG. Stage 1 trains a contrastive 1-D ResNet-18 on 276 h of raw,
unlabeled data from heterogeneous sources (varying in device and sampling
frequency), yielding optical-emitter- and motion-invariant embeddings (i.e.,
the learned representation is stable across differences in LED wavelength,
drive intensity, and device optics, as well as wrist motion). Stage 2 converts
each 512-D encoder embedding into a 4-D topological signature via persistent
homology (PH) and clusters these signatures with HDBSCAN. To produce a binary
signal-quality index (SQI), the acceptable PPG signals are represented by the
densest cluster while the remaining clusters are assumed to mainly contain
poor-quality PPG signals. Without re-tuning, the SQI attains Silhouette,
Davies-Bouldin, and Calinski-Harabasz scores of 0.72, 0.34, and 6173,
respectively, on a stratified sample of 10,000 windows. In this study, we
propose a hybrid self-supervised-learning--topological-data-analysis (SSL--TDA)
framework that offers a drop-in, scalable, cross-device quality gate for PPG
signals.

</details>


### [4] [Rapid Adaptation of SpO2 Estimation to Wearable Devices via Transfer Learning on Low-Sampling-Rate PPG](https://arxiv.org/abs/2509.12515)
*Zequan Liang,Ruoyu Zhang,Wei Shao,krishna Karthik,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun*

Main category: eess.SP

TL;DR: 提出基于迁移学习的低功耗可穿戴设备血氧饱和度估计算法，使用25Hz双通道PPG信号，无需临床校准，能耗降低40%，精度优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统血氧饱和度监测方法需要复杂的临床校准，不适合低功耗可穿戴设备应用，需要开发无需校准的快速适应方案

Method: 使用双向LSTM加自注意力机制的迁移学习框架，先在公共临床数据集上预训练，然后在自研可穿戴设备数据上微调

Result: 在公共数据集上MAE为2.967%，私有数据集上MAE为2.624%，瞬时预测MAE为3.284%，功耗相比100Hz降低40%

Conclusion: 该方法实现了无需临床校准的快速准确血氧监测，适合低功耗可穿戴设备应用，能有效捕捉血氧快速波动

Abstract: Blood oxygen saturation (SpO2) is a vital marker for healthcare monitoring.
Traditional SpO2 estimation methods often rely on complex clinical calibration,
making them unsuitable for low-power, wearable applications. In this paper, we
propose a transfer learning-based framework for the rapid adaptation of SpO2
estimation to energy-efficient wearable devices using low-sampling-rate (25Hz)
dual-channel photoplethysmography (PPG). We first pretrain a bidirectional Long
Short-Term Memory (BiLSTM) model with self-attention on a public clinical
dataset, then fine-tune it using data collected from our wearable We-Be band
and an FDA-approved reference pulse oximeter. Experimental results show that
our approach achieves a mean absolute error (MAE) of 2.967% on the public
dataset and 2.624% on the private dataset, significantly outperforming
traditional calibration and non-transferred machine learning baselines.
Moreover, using 25Hz PPG reduces power consumption by 40% compared to 100Hz,
excluding baseline draw. Our method also attains an MAE of 3.284% in
instantaneous SpO2 prediction, effectively capturing rapid fluctuations. These
results demonstrate the rapid adaptation of accurate, low-power SpO2 monitoring
on wearable devices without the need for clinical calibration.

</details>


### [5] [Generalizable Blood Pressure Estimation from Multi-Wavelength PPG Using Curriculum-Adversarial Learning](https://arxiv.org/abs/2509.12518)
*Zequan Liang,Ruoyu Zhang,Wei Shao,Mahdi Pirayesh Shirazi Nejad,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun*

Main category: eess.SP

TL;DR: 提出基于课程对抗学习的通用血压估计框架，结合多波长PPG数据，通过课程学习和域对抗训练实现主体无关特征提取，在严格主体级分割下取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 准确且可泛化的血压估计对心血管疾病早期检测和管理至关重要，需要解决现有方法在主体级泛化方面的不足。

Method: 采用主体级数据分割，结合课程学习（从高血压分类过渡到血压回归）和域对抗训练（混淆主体身份以学习主体不变特征），使用多波长PPG数据进行多通道融合。

Result: 在四波长PPG数据集上，收缩压MAE为14.2mmHg，舒张压MAE为6.4mmHg，多通道融合始终优于单通道模型，消融研究验证了课程和对抗组件的有效性。

Conclusion: 多波长PPG的互补信息和课程对抗策略相结合，为准确稳健的血压估计提供了有前景的解决方案，具有重要的临床应用潜力。

Abstract: Accurate and generalizable blood pressure (BP) estimation is vital for the
early detection and management of cardiovascular diseases. In this study, we
enforce subject-level data splitting on a public multi-wavelength
photoplethysmography (PPG) dataset and propose a generalizable BP estimation
framework based on curriculum-adversarial learning. Our approach combines
curriculum learning, which transitions from hypertension classification to BP
regression, with domain-adversarial training that confuses subject identity to
encourage the learning of subject-invariant features. Experiments show that
multi-channel fusion consistently outperforms single-channel models. On the
four-wavelength PPG dataset, our method achieves strong performance under
strict subject-level splitting, with mean absolute errors (MAE) of 14.2mmHg for
systolic blood pressure (SBP) and 6.4mmHg for diastolic blood pressure (DBP).
Additionally, ablation studies validate the effectiveness of both the
curriculum and adversarial components. These results highlight the potential of
leveraging complementary information in multi-wavelength PPG and
curriculum-adversarial strategies for accurate and robust BP estimation.

</details>


### [6] [Kalman Filtering of Stationary Graph Signals](https://arxiv.org/abs/2509.12605)
*Yang Chen,Yeonju Lee,Yao Shi,Qiyu Sun*

Main category: eess.SP

TL;DR: 本文提出了基于对称图位移的平稳图信号新定义，证明了其可通过多项式图通道生成，并展示了卡尔曼滤波在多项式系统下保持信号平稳性的优势


<details>
  <summary>Details</summary>
Motivation: 现有图信号处理缺乏对平稳性的明确定义，需要建立图信号平稳性理论框架并开发有效的滤波方法

Method: 提出基于对称图位移的平稳图信号定义，使用多项式图通道生成信号，并应用卡尔曼滤波处理多项式状态和观测矩阵的动态系统

Result: 证明了平稳图信号可通过多项式通道生成且保持平稳性，卡尔曼滤波相比静态逆滤波和零信号策略能提供更准确、自适应的信号估计

Conclusion: 建立了图信号平稳性的理论基础，展示了卡尔曼滤波在图信号处理中的鲁棒性和适用性，为动态图信号处理提供了有效工具

Abstract: In this paper, we propose a novel definition of stationary graph signals,
formulated with respect to a symmetric graph shift, such as the graph
Laplacian. We show that stationary graph signals can be generated by
transmitting white noise through polynomial graph channels, and that their
stationarity is preserved under polynomial channel transmission.
  In this paper, we also investigate Kalman filtering to dynamical systems
characterized by polynomial state and observation matrices. We demonstrate that
Kalman filtering maintains the stationarity of graph signals, while effectively
incorporating both system dynamics and noise structure. In comparison to the
static inverse filtering method and naive zero-signal strategy, the Kalman
filtering procedure yields more accurate and adaptive signal estimates,
highlighting its robustness and versatility in graph signal processing.

</details>


### [7] [Low-Altitude UAV Tracking via Sensing-Assisted Predictive Beamforming](https://arxiv.org/abs/2509.12698)
*Yifan Jiang,Qingqing Wu,Hongxun Hui,Wen Chen,Derrick Wing Kwan Ng*

Main category: eess.SP

TL;DR: 本文研究了感知辅助预测波束成形在无人机跟踪中的中断容量最大化问题，提出了基于EKF的跟踪方案和两种高效算法，显著提升了通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注频谱效率提升，而感知辅助波束成形技术对通信可靠性的影响尚未充分探索，需要填补这一研究空白。

Method: 提出基于扩展卡尔曼滤波的蜂窝连接无人机跟踪方案，联合优化预测轨迹、感知时长比和目标恒定接收信噪比；开发了基于二分搜索和连续凸逼近的高效算法，以及基于交替优化的低复杂度算法。

Result: 仿真验证了中断概率近似的准确性，算法有效性，以及相比基准方案的显著中断容量提升，同时揭示了路径损耗减小与宽波束覆盖之间的权衡关系。

Conclusion: 该研究为感知辅助预测波束成形在无人机通信可靠性方面的应用提供了有效解决方案，通过联合优化和高效算法实现了显著的中断容量提升。

Abstract: Sensing-assisted predictive beamforming, as one of the enabling technologies
for emerging integrated sensing and communication (ISAC) paradigm, shows
significant promise for enhancing various future unmanned aerial vehicle (UAV)
applications. However, current works predominately emphasized on spectral
efficiency enhancement, while the impact of such beamforming techniques on the
communication reliability was largely unexplored and challenging to
characterize. To fill this research gap and tackle this issue, this paper
investigates outage capacity maximization for UAV tracking under the
sensing-assisted predictive beamforming scheme. Specifically, a
cellular-connected UAV tracking scheme is proposed leveraging extended Kalman
filtering (EKF), where the predicted UAV trajectory, sensing duration ratio,
and target constant received signal-to-noise ratio (SNR) are jointly optimized
to maximize the outage capacity at each time slot. To address the implicit
nature of the objective function, closed-form approximations of the outage
probabilities (OPs) at both prediction and measurement stages of each time slot
are proposed based on second-order Taylor expansions, providing an efficient
and full characterization of outage capacity. Subsequently, an efficient
algorithm is proposed based on a combination of bisection search and successive
convex approximation (SCA) to address the non-convex optimization problem with
guaranteed convergence. To further reduce computational complexity, a second
efficient algorithm is developed based on alternating optimization (AO).
Simulation results validate the accuracy of the derived OP approximations, the
effectiveness of the proposed algorithms, and the significant outage capacity
enhancement over various benchmarks, while also indicating a trade-off between
decreasing path loss and enjoying wide beam coverage for outage capacity
maximization.

</details>


### [8] [Data Fusion for BS-UE Cooperative MIMO-OFDM ISAC](https://arxiv.org/abs/2509.12646)
*Yixin Ding,Haoyu Jiang,Xiaoli Xu,Yanan Liang,Yong Zeng*

Main category: eess.SP

TL;DR: 本文提出了一种新的基站(BS)和用户设备(UE)协作感知模式，通过融合BS和UE的感知结果来提高多目标位置和速度估计精度。


<details>
  <summary>Details</summary>
Motivation: 为了进一步增强无线网络的感知能力，在3GPP定义的六种基本感知操作模式基础上，提出BS-UE协作感知的新模式。

Method: UE在解码通信数据后进一步处理接收信号提取目标感知信息，提出利用BS、UE和目标间几何关系以及预期感知质量的融合算法，结合BS单站和BS-UE双站感知。

Result: 所提出的协作感知数据融合方法能有效提高多目标位置和速度估计精度。

Conclusion: BS-UE协作感知为感知模式的扩展提供了新途径，能显著提升无线网络的感知性能。

Abstract: Integrated sensing and communication (ISAC) is a promising technique for
expanding the functionalities of wireless networks with enhanced spectral
efficiency. The 3rd Generation Partnership Project (3GPP) has defined six basic
sensing operation modes in wireless networks. To further enhance the sensing
capability of wireless networks, this paper proposes a new sensing operation
mode, i.e., the base station (BS) and user equipment (UE) cooperative sensing.
Specifically, after decoding the communication data, the UE further processes
the received signal to extract the target sensing information. We propose an
efficient algorithm for fusing the sensing results obtained by the BS and UE,
by exploiting the geometric relationship among BS, UE and targets as well as
the expected sensing quality in the BS monostatic and BS-UE bistatic sensing.
The results show that the proposed data fusion method for cooperative sensing
can effectively improve the position and velocity estimation accuracy of
multiple targets, and provide a new approach on the expansion of the sensing
pattern.

</details>


### [9] [Sustainable LSTM-Based Precoding for RIS-Aided mmWave MIMO Systems with Implicit CSI](https://arxiv.org/abs/2509.12658)
*Po-Heng Chou,Jiun-Jia Wu,Wan-Jen Huang,Ronald Y. Chang*

Main category: eess.SP

TL;DR: 提出基于LSTM的可持续预编码框架，用于RIS辅助毫米波MIMO系统，通过隐式学习信道特性减少导频开销和计算复杂度，实现接近穷举搜索90%的频谱效率但仅需2.2%的计算时间。


<details>
  <summary>Details</summary>
Motivation: 解决RIS辅助毫米波MIMO系统中显式CSI估计带来的高导频开销和计算复杂度问题，同时考虑实际硬件约束，为可持续6G无线网络提供实用且节能的解决方案。

Method: 采用LSTM网络隐式学习信道特性，结合RIS元件的相位相关幅度模型，使用多标签训练策略提高鲁棒性，避免显式CSI估计。

Result: 仿真显示该方法达到穷举搜索90%以上的频谱效率，计算时间仅为2.2%，能耗降低近两个数量级，在分布失配情况下表现稳健，可扩展至更大RIS阵列。

Conclusion: 该框架是6G无线网络中实用且节能的解决方案，通过隐式学习和多标签训练实现了高性能与低复杂度的平衡，具有良好的扩展性和鲁棒性。

Abstract: In this paper, we propose a sustainable long short-term memory (LSTM)-based
precoding framework for reconfigurable intelligent surface (RIS)-assisted
millimeter-wave (mmWave) MIMO systems. Instead of explicit channel state
information (CSI) estimation, the framework exploits uplink pilot sequences to
implicitly learn channel characteristics, reducing both pilot overhead and
inference complexity. Practical hardware constraints are addressed by
incorporating the phase-dependent amplitude model of RIS elements, while a
multi-label training strategy improves robustness when multiple near-optimal
codewords yield comparable performance. Simulations show that the proposed
design achieves over 90% of the spectral efficiency of exhaustive search (ES)
with only 2.2% of its computation time, cutting energy consumption by nearly
two orders of magnitude. The method also demonstrates resilience under
distribution mismatch and scalability to larger RIS arrays, making it a
practical and energy-efficient solution for sustainable 6G wireless networks.

</details>


### [10] [NEFT: A Unified Transformer Framework for Efficient Near-Field CSI Feedback in XL-MIMO Systems](https://arxiv.org/abs/2509.12748)
*Haiyang Li,Tianqi Mao,Pengyu Wang,Ruiqi Liu,Shunyu Li,Zhaocheng Wang*

Main category: eess.SP

TL;DR: 提出NEFT系列Transformer模型，用于超大规模MIMO系统的近场CSI反馈，在保持高精度的同时显著降低计算复杂度，适用于不同硬件平台部署。


<details>
  <summary>Details</summary>
Motivation: 超大规模MIMO系统在近场区域面临CSI反馈挑战，现有深度学习方法难以捕捉近场CSI复杂结构且计算开销过大，无法在移动设备上实用部署。

Method: 基于分层Vision Transformer架构，开发NEFT系列变体：NEFT-Compact采用多级知识蒸馏降低复杂度；NEFT-Hybrid和NEFT-Edge分别通过无注意力编码和知识蒸馏解决编码器和边缘设备约束问题。

Result: NEFT相比现有方法在NMSE指标上提升15-21dB；NEFT-Compact和NEFT-Edge减少总FLOPs 25-36%且精度损失可忽略；NEFT-Hybrid降低编码器复杂度达64%。

Conclusion: NEFT为XL-MIMO系统的近场CSI反馈提供了实用且可扩展的解决方案，能够适应不同硬件平台的部署约束。

Abstract: Extremely large-scale multiple-input multiple-output (XL-MIMO) systems,
operating in the near-field region due to their massive antenna arrays, are a
key enabler of next-generation wireless communications but face significant
challenges in channel state information (CSI) feedback. Deep learning has
emerged as a powerful tool by learning compact CSI representations for
feedback. However, existing methods struggle to capture the intricate structure
of near-field CSI while incurring prohibitive computational overhead on
practical mobile devices. To overcome these limitations, we propose the
Near-Field Efficient Feedback Transformer (NEFT) family for accurate and
efficient near-field CSI feedback across diverse hardware platforms. Built on a
hierarchical Vision Transformer backbone, NEFT is extended with lightweight
variants to meet various deployment constraints: NEFT-Compact applies
multi-level knowledge distillation (KD) to reduce complexity while maintaining
accuracy, and NEFT-Hybrid and NEFT-Edge address encoder- and edge-constrained
scenarios via attention-free encoding and KD. Extensive simulations show that
NEFT achieves a 15--21 dB improvement in normalized mean-squared error (NMSE)
over state-of-the-art methods, while NEFT-Compact and NEFT-Edge reduce total
FLOPs by 25--36% with negligible accuracy loss. Moreover, NEFT-Hybrid lowers
encoder-side complexity by up to 64%, enabling deployment in highly asymmetric
device scenarios. These results establish NEFT as a practical and scalable
solution for near-field CSI feedback in XL-MIMO systems.

</details>


### [11] [EMC Limit Level Guidelines for In-System Interference with GPS Receivers](https://arxiv.org/abs/2509.12770)
*Giorgi Tsintsadze,Haran Manoharan,Aaron Harmon,Daniel Commerou,Connor Buneta,Brian Booth,Daryl Beetner*

Main category: eess.SP

TL;DR: 本文提出了针对GPS接收机附近电子设备的EMC限值指南，通过建立载噪比退化理论模型来评估电磁干扰对GPS性能的影响，为汽车等应用提供更精细的排放评估方法。


<details>
  <summary>Details</summary>
Motivation: GPS信号较弱，附近电子设备的无意辐射发射容易造成电磁干扰，影响GPS接收机性能。现有排放标准使用的简单限值线方法不够精确，需要更细致的评估指南。

Method: 建立理论模型预测射频干扰下的载噪比退化，使用实际噪声源验证模型，基于噪声频率、带宽和幅度制定评估指南。

Result: 开发出能够准确预测GPS接收机性能退化的理论模型，并验证了其有效性，为电子设备排放影响评估提供了量化工具。

Conclusion: 提出的EMC限值指南比传统简单限值线方法更加精细和准确，能够更好地评估电子设备无意排放对附近GPS接收机的影响，特别适用于汽车等集成应用场景。

Abstract: Because GPS signals are weak, electronic systems and components that are
placed near GPS receivers can easily cause disruptive electromagnetic
interference through their unintended radiated emissions. In this paper, EMC
limit level guidelines are presented for electronics that are intended to be
placed near to GPS receivers, as often happens in automotive and other
applications. One of the challenges of defining limit-levels for systems
intended to be integrated with GPS receivers is that the impact of noise at the
input of the receiver may vary substantially depending on the form of the noise
due to the correlator function implemented by GPS receiver. The quality of the
correlated signal is typically represented using the carrier-to-noise ratio ($C
/ N_0$). A theoretical model predicting the degredation of the carrier-to-noise
ratio with radio frequency interference is presented in this paper and is
validated with realistic noise sources. The model is then used to develop
guidelines to assess the impact of unintended emissions from electronic devices
on nearby GPS receivers based on the frequency, bandwidth, and magnitude of the
noise. These guidelines provide a more nuanced method of evaluating emissions
than simple limit lines that are used by many emissions standards.

</details>


### [12] [A Statistical Benchmark for Diffusion Posterior Sampling Algorithms](https://arxiv.org/abs/2509.12821)
*Martin Zach,Youssef Haouchat,Michael Unser*

Main category: eess.SP

TL;DR: 提出了一个用于评估扩散后验采样（DPS）算法的统计基准，通过合成稀疏Lévy过程先验的信号，使用高效的Gibbs方法获得黄金标准后验样本，并与DPS算法结果进行比较。


<details>
  <summary>Details</summary>
Motivation: 为贝叶斯线性逆问题中的扩散后验采样算法建立一个可靠的统计评估基准，以准确衡量算法性能并隔离似然得分近似带来的误差。

Method: 使用稀疏Lévy过程先验合成信号，利用Gibbs方法获得标准后验样本，通过最小均方误差最优性差距和后验覆盖测试来评估DPS算法。

Result: 在去噪、解卷积、插补和部分傅里叶测量重建等逆问题上对流行DPS算法进行了数值实验，并提供了开源基准代码。

Conclusion: 该基准为DPS算法提供了可靠的评估框架，具有简单的插件接口和配置驱动运行，便于研究人员添加新算法并进行评估。

Abstract: We propose a statistical benchmark for diffusion posterior sampling (DPS)
algorithms for Bayesian linear inverse problems. The benchmark synthesizes
signals from sparse L\'evy-process priors whose posteriors admit efficient
Gibbs methods. These Gibbs methods can be used to obtain gold-standard
posterior samples that can be compared to the samples obtained by the DPS
algorithms. By using the Gibbs methods for the resolution of the denoising
problems in the reverse diffusion, the framework also isolates the error that
arises from the approximations to the likelihood score. We instantiate the
benchmark with the minimum-mean-squared-error optimality gap and posterior
coverage tests and provide numerical experiments for popular DPS algorithms on
the inverse problems of denoising, deconvolution, imputation, and
reconstruction from partial Fourier measurements. We release the benchmark code
at https://github.com/zacmar/dps-benchmark. The repository exposes simple
plug-in interfaces, reference scripts, and config-driven runs so that new
algorithms can be added and evaluated with minimal effort. We invite
researchers to contribute and report results.

</details>


### [13] [Bayesian Signal Separation via Plug-and-Play Diffusion-Within-Gibbs Sampling](https://arxiv.org/abs/2509.12857)
*Yi Zhang,Rui Guo,Yonina C. Eldar*

Main category: eess.SP

TL;DR: 提出了一种结合吉布斯采样和PnP扩散先验的后验采样算法，用于从噪声叠加中估计多个独立源信号，无需重新训练即可灵活组合单独学习的源先验，并在完美扩散模型训练假设下可证明从后验分布采样。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的信号分离方法通常需要联合训练源先验，缺乏灵活性。本文旨在开发一种能够单独学习源先验并灵活组合的方法，同时保证理论上的后验采样性质。

Method: 结合吉布斯采样方法和即插即用(PnP)扩散先验，通过后验采样算法实现信号分离。源先验可以单独学习，无需重新训练即可组合使用。

Result: 在心跳信号与合成运动伪影混合的提取任务实验中，该方法表现出优于现有方法的性能。

Conclusion: 提出的算法提供了灵活且理论保证的信号分离方案，能够有效处理多源信号分离问题，特别是在医学信号处理等应用中具有潜力。

Abstract: We propose a posterior sampling algorithm for the problem of estimating
multiple independent source signals from their noisy superposition. The
proposed algorithm is a combination of Gibbs sampling method and plug-and-play
(PnP) diffusion priors. Unlike most existing diffusion-model-based approaches
for signal separation, our method allows source priors to be learned separately
and flexibly combined without retraining. Moreover, under the assumption of
perfect diffusion model training, the proposed method provably produces samples
from the posterior distribution. Experiments on the task of heartbeat
extraction from mixtures with synthetic motion artifacts demonstrate the
superior performance of our method over existing approaches.

</details>


### [14] [Towards personalized, precise and survey-free environment recognition: AI-enhanced sensor fusion without pre-deployment](https://arxiv.org/abs/2509.12870)
*Ruichen Wang,Zhikang Ni,Pengzhou Wang,Xiya Cao,Zhi Li,Bao Zhang*

Main category: eess.SP

TL;DR: 一种不需覆盛调查的定位框架，通过多源依据建立个人化指纹库，采用AI增强动态时间扩张匹配算法，通过云端强化学习从人类反馈优化策略，在室内外环境中将网络切换延迟降低32-65%。


<details>
  <summary>Details</summary>
Motivation: 传统指纹定位需要程序复杂的现场调查且缺乏个人化适配能力，影响室内定位和连接优化的无缝体验。

Method: 构建个人化轻量级多源指纹数据库（PDR、WiFi/细胞、GNSS、交互时间标签），采用AI增强动态时间扩张模块（AIDTW）对凶噪异步序列进行匹配，通过云端强化学习从人类反馈（RLHF）循环使用近端策略优化（PPO）来不断改进策略。

Result: 在室内外场景中，系统与传统基准线相比，将网络切换延迟（TTS）降低了32-65%，且无需现场特定的预部署。

Conclusion: 该框架提供了一种无需调查、个人化适配、持续进化的环境识别方案，在保证准确性的同时显著提升了网络连接性能。

Abstract: Accurate and personalized environment recognition is essential for seamless
indoor positioning and optimized connectivity, yet traditional fingerprinting
requires costly site surveys and lacks user-level adaptation. We present a
survey-free, on-device sensor-fusion framework that builds a personalized,
lightweight multi-source fingerprint (FP) database from pedestrian dead
reckoning (PDR), WiFi/cellular, GNSS, and interaction time tags. Matching is
performed by an AI-enhanced dynamic time warping module (AIDTW) that aligns
noisy, asynchronous sequences. To turn perception into continually improving
actions, a cloud-edge online Reinforcement Learning from Human Feedback (RLHF)
loop aggregates desensitized summaries and human feedback in the cloud to
optimize a policy via proximal policy optimization (PPO), and periodically
distills updates to devices. Across indoor/outdoor scenarios, our system
reduces network-transition latency (measured by time-to-switch, TTS) by 32-65%
in daily environments compared with conventional baselines, without
site-specific pre-deployment.

</details>


### [15] [Next-Generation Backscatter Networks for Integrated Communications and RF Sensing](https://arxiv.org/abs/2509.12954)
*Traian E. Abrudan,Kartik Patel,John Kimionis,Tara Esmaeilbeig,Eleftherios Kampianakis,Sahan Damith Liyanaarachchi,Michael Eggleston*

Main category: eess.SP

TL;DR: 本文为集成了RF定位感知能力的下一代反向散射网络提供了全面的分析和理论基础，包括端到端系统建模、实验验证和性能基准测试。


<details>
  <summary>Details</summary>
Motivation: 传统反向散射网络主要关注通信功能，而下一代网络需要集成RF定位感知能力，以支持大规模部署的机器类型通信网络，同时保持能效。

Method: 推导了宽带OFDM反向散射系统的端到端系统模型，包括传播信道、接收机链损伤、RF标签操作和未同步节点的详细表征；提出了实用的双基距离测量方法；推导了Cramér-Rao下界(CRLB)来展示可实现的性能极限。

Result: 通过实际硬件进行实验评估验证了理论系统模型的准确性；实验结果表明了系统在通信、RF感知和测距方面的性能，并与理论极限进行了基准测试。

Conclusion: 该分析框架和实验验证为未来大规模部署、能效高的分布式未同步反向散射系统建立了基础理解，为机器类型通信网络的发展提供了重要支撑。

Abstract: This paper provides a comprehensive analysis and theoretical foundation for
next-generation backscatter networks that move beyond communication and
integrate RF location sensing capabilities. An end-to-end system model for
wideband OFDM backscatter systems is derived, including detailed
characterization of propagation channels, receiver chain impairments, RF tag
operation, and unsynchronized network nodes. The theoretical system model is
validated through experimental evaluation using actual hardware, demonstrating
the detailed model's accuracy. A practical bistatic ranging method that can
operate with unsynchronized nodes is presented, along with the Cram\'er-Rao
Lower Bound (CRLB) derived to show the achievable performance limits. Our
experimental results demonstrate the system performance for communication, RF
sensing, and ranging, while also benchmarking against the derived theoretical
limits. This analytical framework and experimental validation establish
fundamental understanding of distributed, unsynchronized backscatter systems
for future machine-type communication networks that are deployed in massive
scale, while remaining energy-efficient.

</details>


### [16] [Difference-Based Recovery for Modulo Sampling: Tightened Bounds and Robustness Guarantees](https://arxiv.org/abs/2509.12971)
*Wenyi Yan,Zeyuan Li,Lu Gan,Honqing Liu,Guoquan Li*

Main category: eess.SP

TL;DR: 本文改进了基于差分的模数转换恢复方法，在无噪声条件下将过采样因子从2πe降低到π，建立了噪声感知采样条件，并证明了在采样抖动下的鲁棒性，通过FPGA硬件原型验证了高达108倍幅度扩展的可靠重建。


<details>
  <summary>Details</summary>
Motivation: 传统ADC在信号超过输入范围时会削波，而现有的模数采样恢复方法要么计算复杂，要么需要高采样率，且都没有考虑实际中不可避免的采样抖动问题。

Method: 采用基于差分的高阶恢复方法，推导了无噪声和噪声条件下的采样理论保证，特别针对二阶差分恢复分析了非均匀采样情况下的鲁棒性，并通过FPGA硬件原型进行验证。

Result: 理论证明高阶差分可将过采样因子从2πe降至π，建立了稳定的噪声感知恢复条件，在采样抖动下仍能保持鲁棒性，硬件实验实现了高达108倍的幅度扩展重建。

Conclusion: 该方法提供了一种简单而鲁棒的无限传感恢复管道，显著提高了模数转换的性能和实用性，解决了传统ADC的削波限制问题。

Abstract: Conventional analog-to-digital converters (ADCs) clip when signals exceed
their input range. Modulo (unlimited) sampling overcomes this limitation by
folding the signal before digitization, but existing recovery methods are
either computationally intensive or constrained by loose oversampling bounds
that demand high sampling rates. In addition, none account for sampling jitter,
which is unavoidable in practice. This paper revisits difference-based recovery
and establishes new theoretical and practical guarantees. In the noiseless
setting, we prove that arbitrarily high difference order reduces the sufficient
oversampling factor from $2\pi e$ to $\pi$, substantially tightening classical
bounds. For fixed order $N$, we derive a noise-aware sampling condition that
guarantees stable recovery. For second-order difference-based recovery ($N=2$),
we further extend the analysis to non-uniform sampling, proving robustness
under bounded jitter. An FPGA-based hardware prototype demonstrates reliable
reconstruction with amplitude expansion up to $\rho = 108$, confirming the
feasibility of high-performance unlimited sensing with a simple and robust
recovery pipeline.

</details>


### [17] [RF-Powered Batteryless Plant Movement Sensor for Precision Agriculture](https://arxiv.org/abs/2509.13004)
*Jona Cappelle,Jarne Van Mulders,Sarah Goossens,Thomas Reher,Liesbet Van der Perre,Lieven De Strycker,Bram Van de Poel,Gilles Callebaut*

Main category: eess.SP

TL;DR: 设计了一种无电池、仅靠射频能量供电的轻量级植物运动传感器，用于监测CEA环境中植物叶片运动，通过消除电池减少生态足迹和维护需求。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要非侵入式、节能和可持续的植物监测方案，传统电池供电传感器存在生态足迹大、重量重和维护需求高的问题。

Method: 利用射频能量为惯性测量单元(IMU)供电，监测叶片运动，采用基于能量可用性的自适应读取调度，最小化电路复杂度。

Result: 实现了无电池的植物运动监测方案，能够根据能量可用性灵活调整读取计划，减少了生态影响和维护成本。

Conclusion: 该无电池传感器为精准农业提供了可持续的监测解决方案，未来可扩展多天线供电和网络化传感器同步功能。

Abstract: Precision agriculture demands non-invasive, energy-efficient, and sustainable
plant monitoring solutions. In this work, we present the design and
implementation of a lightweight, batteryless plant movement sensor powered
solely by RF energy. This sensor targets Controlled Environment Agriculture
(CEA) and utilizes inertial measurements units (IMUs) to monitor leaf motion,
which correlates with plant physiological responses to environmental stress. By
eliminating the battery, we reduce the ecological footprint, weight, and
maintenance requirements, transitioning from lifetime-based to operation-based
energy storage. Our design minimizes circuit complexity while enabling
flexible, adaptive readout scheduling based on energy availability and sensor
data. We detail the energy requirements, RF power transfer considerations,
integration constraints, and outline future directions, including multi-antenna
power delivery and networked sensor synchronization.

</details>


### [18] [Deep Tensor Learning for Reliable Channel Charting from Incomplete and Noisy Measurements](https://arxiv.org/abs/2509.13030)
*Ge Chen,Panqi Chen,Lei Cheng*

Main category: eess.SP

TL;DR: 提出一种基于深度张量学习的方法，用于从噪声和不完整的无线信道数据中提取低维特征（信道图），解决了现有方法在真实世界场景中的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有信道图方法主要基于模拟或室内测量数据，假设信道数据干净完整，但真实基站收集的信道数据通常因频率跳变而不完整，且在小区边缘噪声严重，导致现有方法性能大幅下降。

Method: 采用深度张量学习方法，利用无线信道固有的张量结构，从噪声和不完整的测量数据中有效提取信息丰富的低维特征（信道图）。

Result: 实验结果表明，该方法在具有挑战性的场景中表现出可靠性和有效性。

Conclusion: 所提出的深度张量学习方法能够有效处理真实世界中的噪声和不完整信道数据，为信道图技术在实际应用中的可靠性提供了解决方案。

Abstract: Channel charting has emerged as a powerful tool for user equipment
localization and wireless environment sensing. Its efficacy lies in mapping
high-dimensional channel data into low-dimensional features that preserve the
relative similarities of the original data. However, existing channel charting
methods are largely developed using simulated or indoor measurements, often
assuming clean and complete channel data across all frequency bands. In
contrast, real-world channels collected from base stations are typically
incomplete due to frequency hopping and are significantly noisy, particularly
at cell edges. These challenging conditions greatly degrade the performance of
current methods. To address this, we propose a deep tensor learning method that
leverages the inherent tensor structure of wireless channels to effectively
extract informative while low-dimensional features (i.e., channel charts) from
noisy and incomplete measurements. Experimental results demonstrate the
reliability and effectiveness of the proposed approach in these challenging
scenarios.

</details>


### [19] [Scatterer Localization Using Multi-Bounce Paths](https://arxiv.org/abs/2509.13071)
*Yuan Liu,Linlong Wu,Xuesong Cai,M. R. Bhavani Shankar*

Main category: eess.SP

TL;DR: 本文提出了一种基于图论的GM-SAGE算法，用于解决室内多径环境感知中的多跳效应、球面波前和近场空间非平稳性问题。


<details>
  <summary>Details</summary>
Motivation: 室内无线传感面临多跳效应、球面波前和近场空间非平稳性等挑战，传统方法难以有效处理这些复杂传播特性。

Method: 使用图论建模近场多跳传播，将反射体/散射体建模为传播图中的顶点，多跳路径建模为连接顶点的边。将距离和角度等多径参数转换为散射体坐标，并改进SAGE算法为GM-SAGE算法。

Result: 通过测量校准的射线追踪验证，GM-SAGE算法能够有效处理多跳信道，在复杂室内办公环境中表现出良好性能。

Conclusion: 基于图论的GM-SAGE算法为室内多径环境感知提供了一种有效的解决方案，能够处理近场多跳传播的复杂特性。

Abstract: Indoor sensing is challenging because of the multi-bounce effect, spherical
wavefront, and spatial nonstationarity (SNS) of the near-field effect. This
paper addresses radio-based environment sensing considering these issues.
Specifically, graph theory (GT) is used to model the multi-bounce propagation
of the near field. In this manner, indoor reflectors/scatterers are modeled as
vertices in a propagation graph, the multi-bounce paths are modeled by the
edges linking the vertices. Besides, the coupled multipath parameters in the
near field, i.e., range and angles, are denoted directly by the coordinates of
vertices. Then, the space-alternating generalized expectation-maximization
(SAGE) algorithm is adapted to the proposed Graph theory-based dictionary-aided
Multi-bounce SAGE (GM-SAGE), where the searching parameters including range and
angle of departure/arrival (AoD/AoA) are transformed to the coordinates of
scatterers in the graph. The proposed algorithm is validated through
measurement-calibrated ray tracing (RT) in a complex indoor office. The results
demonstrate that the proposed GM-SAGE can deal with multi-bounce channels.

</details>


### [20] [Transmitter Subspace-Aware Target Detection in Two-Channel Passive Radars with Inter-Receiver Collaboration](https://arxiv.org/abs/2509.13287)
*Nandan Sriranga,Haodong Yang,Pramod K. Varshney*

Main category: eess.SP

TL;DR: 分布式双通道被动雷达在单延迟多普勒单元中的目标检测方法，通过协作交换和线性组合相关测量来节省带宽并提升检测性能


<details>
  <summary>Details</summary>
Motivation: 解决在未知机会照射源(IO)条件下，使用空间分布式双通道被动雷达进行目标检测的问题，特别是在带宽受限的协作通信场景中

Method: 接收机将参考和监视信号转换到IO子空间并进行噪声白化处理，获得互相关测量；通过协作交换和线性组合互相关输出，仅子集传输到融合中心；基于融合中心测量的矩设计协作权重

Result: 提出了带宽高效的协作检测框架，通过优化协作权重来增强检测性能

Conclusion: 该方法能够在带宽受限的多址接入信道中有效实现分布式被动雷达目标检测，提升系统检测性能

Abstract: We address target detection in a single Delay-Doppler cell using spatially
distributed two-channel passive radars. An unknown illuminator of opportunity
(IO) is assumed to emit a waveform lying in a known low-dimensional subspace
(e.g., OFDM). Each receiver transforms its reference and surveillance signals
onto the IO subspace after noise-whitening, to obtain cross-correlation (CC)
measurements. To save bandwidth, receivers collaboratively exchange and
linearly combine the CC output, and only a subset transmits them to a fusion
center (FC) over a multiple-access channel (MAC). Collaboration weights are
designed using the moments of the FC measurement to enhance detection
performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis](https://arxiv.org/abs/2509.12212)
*Xinyu He,Chenhan Xiao,Haoran Li,Ruizhong Qiu,Zhe Xu,Yang Weng,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: PowerGrow是一个用于生成电网结构和节点动态的协同生成框架，通过依赖分解和分层图扩散过程，显著降低计算成本的同时保持运行有效性。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统日益动态化，但由于安全顾虑和匿名化工作的复杂性，公开测试案例稀缺，需要能够联合生成电网结构和节点动态的工具。

Method: 采用依赖分解方法将复杂联合分布分解为条件分布链，使用分层图beta扩散过程进行结构合成，配合时间自编码器将时间序列数据嵌入紧凑潜在空间。

Result: 实验表明PowerGrow在保真度和多样性方面优于现有扩散模型，达到98.9%的潮流收敛率和改进的N-1事故恢复能力。

Conclusion: PowerGrow能够生成运行有效且现实的电网场景，解决了电网生成中的计算成本和物理可行性挑战。

Abstract: Modern power systems are becoming increasingly dynamic, with changing
topologies and time-varying loads driven by renewable energy variability,
electric vehicle adoption, and active grid reconfiguration. Despite these
changes, publicly available test cases remain scarce, due to security concerns
and the significant effort required to anonymize real systems. Such limitations
call for generative tools that can jointly synthesize grid structure and nodal
dynamics. However, modeling the joint distribution of network topology, branch
attributes, bus properties, and dynamic load profiles remains a major
challenge, while preserving physical feasibility and avoiding prohibitive
computational costs. We present PowerGrow, a co-generative framework that
significantly reduces computational overhead while maintaining operational
validity. The core idea is dependence decomposition: the complex joint
distribution is factorized into a chain of conditional distributions over
feasible grid topologies, time-series bus loads, and other system attributes,
leveraging their mutual dependencies. By constraining the generation process at
each stage, we implement a hierarchical graph beta-diffusion process for
structural synthesis, paired with a temporal autoencoder that embeds
time-series data into a compact latent space, improving both training stability
and sample fidelity. Experiments across benchmark settings show that PowerGrow
not only outperforms prior diffusion models in fidelity and diversity but also
achieves a 98.9\% power flow convergence rate and improved N-1 contingency
resilience. This demonstrates its ability to generate operationally valid and
realistic power grid scenarios.

</details>


### [22] [Scaling Up Data Parallelism in Decentralized Deep Learning](https://arxiv.org/abs/2509.12213)
*Bing Xie,Junqi Yin,Zhenyu Zhou,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 本文提出了DBench基准测试框架和Ada自适应方法，发现在大规模去中心化DNN训练中，模型精度与通信图连接数和参数张量方差相关，并证明Ada方法在1008 GPU规模上能达到与中心化训练相当的精度


<details>
  <summary>Details</summary>
Motivation: 去中心化学习虽然在理论上被广泛探索，但由于缺乏稳定性、可扩展性和通用性，尚未在生产环境中广泛应用。本研究旨在探索大规模去中心化数据并行训练的实际应用

Method: 引入DBench基准测试框架，比较中心化和去中心化DNN训练；提出Ada自适应方法，在去中心化SGD训练过程中动态调整通信图

Result: 发现去中心化训练存在可扩展性和通用性问题；模型精度与通信图连接数相关；对参数张量方差异常敏感；Ada方法在1008 GPU规模训练ResNet50时能达到与中心化训练相当的收敛速度和精度

Conclusion: Ada方法通过动态调整通信图，有效解决了大规模去中心化DNN训练的问题，为去中心化学习在生产环境中的应用提供了可行方案

Abstract: Although it has been extensively explored in theory, decentralized learning
is not yet green-lighted for production use, largely due to a lack of
stability, scalability, and generality in large scale DNN training. To shed
light on the production use of decentralized learning, this work studies
decentralized data parallel training at scale. To this end, we introduce a
benchmarking framework, namely DBench, to host both centralized and
decentralized DNN training. Building upon DBench, we introduce a benchmarking
methodology to uncover the correlations between model accuracy and the
variances of parameter tensors by varying communication graphs and training
scales. Based on the benchmarking results, we observe that, (1) Similar to
centralized learning, decentralized data parallel training also presents the
issues of scalability and generality when the training scales up; (2) The model
accuracy of decentralized learning is correlated to the number of connections
in a communication graph; (3) The model accuracy of decentralized learning is
surprisingly sensitive to the variance of parameter tensors across model
replicas. Built upon the observations, we propose Ada, a decentralized adaptive
approach that performs large scale DNN training following a decentralized SGD
method and adapting the communication graph in use dynamically throughout
training iterations. We apply Ada on large scale training and observe that Ada
can obtain the best convergence rates consistently in decentralized DNN
training, and delivers equally or comparably good model accuracy for all sample
applications as centralized learning does, even when training ResNet50 for
ImageNet-1K on the scale of 1008 GPUs.

</details>


### [23] [A Physics-Informed Neural Networks-Based Model Predictive Control Framework for $SIR$ Epidemics](https://arxiv.org/abs/2509.12226)
*Aiping Zhong,Baike She,Philip E. Paré*

Main category: cs.LG

TL;DR: 提出基于物理信息神经网络(PINNs)的模型预测控制(MPC)框架，用于SIR传染病模型，能够同时估计状态和参数并生成最优控制策略


<details>
  <summary>Details</summary>
Motivation: 现有MPC研究要么假设动态状态可测量（学习参数），要么假设模型参数已知（学习状态），需要解决在仅有噪声感染状态数据下联合实时估计状态和参数的问题

Method: 提出三种PINNs算法：MPC-PINNs（用于带控制的SIR模型）、MPC-LS-PINNs（对数缩放损失函数提高抗噪性）、MPC-SI-PINNs（利用积分算子和状态耦合重建完整状态信息），并扩展到不同假设场景

Result: 实验结果表明所提方法在不同设置下均有效

Conclusion: 成功开发了能够同时估计流行病状态和参数并生成最优控制策略的MPC-PINNs框架，为传染病控制提供了有效的解决方案

Abstract: This work introduces a physics-informed neural networks (PINNs)-based model
predictive control (MPC) framework for susceptible-infected-recovered ($SIR$)
spreading models. Existing studies in MPC design for epidemic control often
assume either 1) measurable states of the dynamics, where the parameters are
learned, or 2) known parameters of the model, where the states are learned. In
this work, we address the joint real-time estimation of states and parameters
within the MPC framework using only noisy infected states, under the assumption
that 1) only the recovery rate is known, or 2) only the basic reproduction
number is known. Under the first assumption, we propose MPC-PINNs and two novel
PINNs algorithms, all of which are integrated into the MPC framework. First, we
introduce MPC-PINNs, which are designed for $SIR$ models with control. We then
propose log-scaled PINNs (MPC-LS-PINNs), which incorporate a log-scaled loss
function to improve robustness against noise. Next, we present split-integral
PINNs (MPC-SI-PINNs), which leverage integral operators and state coupling in
the neural network training process to effectively reconstruct the complete
epidemic state information. Building upon these methods, we further extend our
framework for the second assumption. We establish the necessary conditions and
extend our PINNs algorithms, where MPC-SI-PINNs are simplified as split-PINNs
(MPC-S-PINNs). By incorporating these algorithms into the MPC framework, we
simultaneously estimate the epidemic states and parameters while generating
optimal control strategies. Experiment results demonstrate the effectiveness of
the proposed methods under different settings.

</details>


### [24] [Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization](https://arxiv.org/abs/2509.12387)
*Mohamed Zayaan S*

Main category: cs.LG

TL;DR: CSML是一个新颖的因果符号元学习框架，通过感知、因果归纳和推理三个模块学习任务分布的潜在因果结构，在需要真正因果推理的任务上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型依赖虚假相关性，泛化能力差且需要大量数据。人类智能的鲁棒性和样本效率源于对因果机制的理解

Method: 包含三个关键模块：感知模块将原始输入映射到解耦的符号表示；可微因果归纳模块发现符号间的因果图；基于图的推理模块利用因果图进行预测

Result: 在CausalWorld新基准测试中，CSML在需要真正因果推理的任务上显著优于最先进的元学习和神经符号基线方法

Conclusion: 通过跨任务分布元学习共享因果世界模型，CSML能够从少量样本快速适应新任务，包括需要干预和反事实推理的任务

Abstract: Modern deep learning models excel at pattern recognition but remain
fundamentally limited by their reliance on spurious correlations, leading to
poor generalization and a demand for massive datasets. We argue that a key
ingredient for human-like intelligence-robust, sample-efficient learning-stems
from an understanding of causal mechanisms. In this work, we introduce
Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer
the latent causal structure of a task distribution. CSML comprises three key
modules: a perception module that maps raw inputs to disentangled symbolic
representations; a differentiable causal induction module that discovers the
underlying causal graph governing these symbols and a graph-based reasoning
module that leverages this graph to make predictions. By meta-learning a shared
causal world model across a distribution of tasks, CSML can rapidly adapt to
novel tasks, including those requiring reasoning about interventions and
counterfactuals, from only a handful of examples. We introduce CausalWorld, a
new physics-based benchmark designed to test these capabilities. Our
experiments show that CSML dramatically outperforms state-of-the-art
meta-learning and neuro-symbolic baselines, particularly on tasks demanding
true causal inference.

</details>


### [25] [MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors](https://arxiv.org/abs/2509.12221)
*Xin Tong,Zhi Lin,Jingya Wang,Meng Han,Bo Jin*

Main category: cs.LG

TL;DR: MEUV框架将单一拒绝方向分解为多个主题对齐、近乎正交的解锁向量，实现细粒度的主题级能力激活，在保持高攻击成功率的同时大幅减少跨主题泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的安全对齐机制会同时拒绝恶意请求和合法的高风险场景使用，而之前的单一拒绝方向编辑方法缺乏语义控制，会无差别解锁所有危险主题。

Method: 提出相互排斥解锁向量(MEUV)框架，通过多任务目标学习主题对齐的正交向量，包含差分消融边际、跨主题惩罚、正交性惩罚等辅助项，单epoch训练完成。

Result: 在双语恶意提示基准测试中，MEUV在多个模型上达到不低于87%的攻击成功率，同时将跨主题泄漏减少高达90%，且中英文向量可几乎无损迁移。

Conclusion: MEUV证明了细粒度主题级能力激活的可行性，为安全敏感领域的受控LLM部署铺平了道路，具有最小效用损失。

Abstract: Large language models (LLMs) enforce safety alignment to reliably refuse
malicious requests, yet the same blanket safeguards also block legitimate uses
in policing, defense, and other high-stakes settings. Earlier
"refusal-direction" edits can bypass those layers, but they rely on a single
vector that indiscriminately unlocks all hazardous topics, offering no semantic
control. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight
framework that factorizes the monolithic refusal direction into topic-aligned,
nearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is
learned in a single epoch with a multi-task objective that blends a
differential-ablation margin, cross-topic and orthogonality penalties, and
several auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV
achieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B,
and Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best
single-direction baseline. Vectors trained in Chinese transfer almost unchanged
to English (and vice versa), suggesting a language-agnostic refusal subspace.
The results show that fine-grained, topic-level capability activation is
achievable with minimal utility loss, paving the way for controlled LLMs
deployment in security-sensitive domains.

</details>


### [26] [Bayesian Parametric Matrix Models: Principled Uncertainty Quantification for Spectral Learning](https://arxiv.org/abs/2509.12406)
*Mohammad Nooraiepour*

Main category: cs.LG

TL;DR: 提出了贝叶斯参数矩阵模型（B-PMMs），为科学机器学习中的谱方法提供不确定性量化，解决了传统确定性方法在安全关键应用中缺乏置信度估计的问题。


<details>
  <summary>Details</summary>
Motivation: 当前谱学习方法仅提供点估计而无不确定性量化，限制了其在需要预测置信度的安全关键应用中的使用。参数矩阵模型虽然性能优异，但其确定性特性限制了在不确定性量化应用中的部署。

Method: 采用自适应谱分解与正则化矩阵扰动边界来表征特征值不确定性传播；使用流形感知的矩阵变量高斯后验进行结构化变分推断，保持Hermitian约束；提供有限样本校准保证。

Result: 在5x5到500x500矩阵维度上实验验证，B-PMMs实现了优异的不确定性校准（ECE < 0.05），保持良好扩展性，在谱病态条件下表现优雅退化，即使在近退化区域也能提供可靠的不确定性估计。

Conclusion: B-PMMs为不确定性关键领域的稳健谱学习提供了支持，为更广泛的贝叶斯谱机器学习奠定了基础，解决了标准贝叶斯方法因谱分解几何约束而失效的核心挑战。

Abstract: Scientific machine learning increasingly uses spectral methods to understand
physical systems. Current spectral learning approaches provide only point
estimates without uncertainty quantification, limiting their use in
safety-critical applications where prediction confidence is essential.
Parametric matrix models have emerged as powerful tools for scientific machine
learning, achieving exceptional performance by learning governing equations.
However, their deterministic nature limits deployment in uncertainty
quantification applications. We introduce Bayesian parametric matrix models
(B-PMMs), a principled framework that extends PMMs to provide uncertainty
estimates while preserving their spectral structure and computational
efficiency. B-PMM addresses the fundamental challenge of quantifying
uncertainty in matrix eigenvalue problems where standard Bayesian methods fail
due to the geometric constraints of spectral decomposition. The theoretical
contributions include: (i) adaptive spectral decomposition with regularized
matrix perturbation bounds that characterize eigenvalue uncertainty
propagation, (ii) structured variational inference algorithms using
manifold-aware matrix-variate Gaussian posteriors that respect Hermitian
constraints, and (iii) finite-sample calibration guarantees with explicit
dependence on spectral gaps and problem conditioning. Experimental validation
across matrix dimensions from 5x5 to 500x500 with perfect convergence rates
demonstrates that B-PMMs achieve exceptional uncertainty calibration (ECE <
0.05) while maintaining favorable scaling. The framework exhibits graceful
degradation under spectral ill-conditioning and provides reliable uncertainty
estimates even in near-degenerate regimes. The proposed framework supports
robust spectral learning in uncertainty-critical domains and lays the
groundwork for broader Bayesian spectral machine learning.

</details>


### [27] [Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO Satellite Systems](https://arxiv.org/abs/2509.12222)
*Binquan Guo,Junteng Cao,Marie Siew,Binbin Chen,Tony Q. S. Quek,Zhu Han*

Main category: cs.LG

TL;DR: 论文提出了一种基于离散时间图的按需调度框架，用于优化卫星网络中的联邦学习，通过动态分配通信资源显著减少了训练轮次时间。


<details>
  <summary>Details</summary>
Motivation: 大规模低地球轨道卫星系统能够实现广域数据交换，但由于隐私和监管限制，原始数据无法集中处理。联邦学习虽然能保护隐私，但卫星网络的动态拓扑和有限带宽会导致参数聚合延迟，延长训练时间。

Method: 采用离散时间图建模的按需调度框架，动态分配通信资源来加速联邦学习过程。

Result: 仿真结果显示，与传统统计复用策略相比，该方法将整体轮次时间减少了14.20%至41.48%，且模型越大、客户端越多，加速效果越明显。

Conclusion: 所提出的调度框架能有效解决卫星网络中联邦学习的通信瓶颈问题，具有良好的可扩展性，特别适用于大规模模型和客户端场景。

Abstract: Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued
for their ability to enable rapid and wide-area data exchange, thereby
facilitating the collaborative training of artificial intelligence (AI) models
across geographically distributed regions. Due to privacy concerns and
regulatory constraints, raw data collected at remote clients cannot be
centrally aggregated, posing a major obstacle to traditional AI training
methods. Federated learning offers a privacy-preserving alternative by training
local models on distributed devices and exchanging only model parameters.
However, the dynamic topology and limited bandwidth of satellite systems will
hinder timely parameter aggregation and distribution, resulting in prolonged
training times. To address this challenge, we investigate the problem of
scheduling federated learning over satellite networks and identify key
bottlenecks that impact the overall duration of each training round. We propose
a discrete temporal graph-based on-demand scheduling framework that dynamically
allocates communication resources to accelerate federated learning. Simulation
results demonstrate that the proposed approach achieves significant performance
gains over traditional statistical multiplexing-based model exchange
strategies, reducing overall round times by 14.20% to 41.48%. Moreover, the
acceleration effect becomes more pronounced for larger models and higher
numbers of clients, highlighting the scalability of the proposed approach.

</details>


### [28] [Spatiotemporal graph neural process for reconstruction, extrapolation, and classification of cardiac trajectories](https://arxiv.org/abs/2509.12953)
*Jaume Banus,Augustin C. Ogier,Roger Hullin,Philippe Meyer,Ruud B. van Heeswijk,Jonas Richiardi*

Main category: cs.LG

TL;DR: 提出了一种基于概率框架的时空动态建模方法，整合神经ODE、图神经网络和神经过程，用于从稀疏观测中重建和预测心脏运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 为了解决从稀疏观测中建模结构化时空动态（特别是心脏运动）的挑战，需要能够同时处理不确定性、时间连续性和解剖结构的方法。

Method: 使用时空多重图表示动态系统，通过GNN参数化的向量场建模潜在轨迹，基于节点和边级别的稀疏上下文观测推断潜在初始状态和控制变量的分布。

Result: 在三个合成动力系统和两个真实心脏影像数据集上验证，能够准确重建轨迹、从单个观测周期外推未来心脏周期，在ACDC分类任务上达到99%准确率，在UK Biobank房颤检测中达到67%准确率。

Conclusion: 该方法为分析心脏运动提供了灵活的方法，并为基于图学习的结构化生物医学时空时间序列数据分析奠定了基础。

Abstract: We present a probabilistic framework for modeling structured spatiotemporal
dynamics from sparse observations, focusing on cardiac motion. Our approach
integrates neural ordinary differential equations (NODEs), graph neural
networks (GNNs), and neural processes into a unified model that captures
uncertainty, temporal continuity, and anatomical structure. We represent
dynamic systems as spatiotemporal multiplex graphs and model their latent
trajectories using a GNN-parameterized vector field. Given the sparse context
observations at node and edge levels, the model infers a distribution over
latent initial states and control variables, enabling both interpolation and
extrapolation of trajectories. We validate the method on three synthetic
dynamical systems (coupled pendulum, Lorenz attractor, and Kuramoto
oscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UK
Biobank (N=526) - demonstrating accurate reconstruction, extrapolation, and
disease classification capabilities. The model accurately reconstructs
trajectories and extrapolates future cardiac cycles from a single observed
cycle. It achieves state-of-the-art results on the ACDC classification task (up
to 99% accuracy), and detects atrial fibrillation in UK Biobank subjects with
competitive performance (up to 67% accuracy). This work introduces a flexible
approach for analyzing cardiac motion and offers a foundation for graph-based
learning in structured biomedical spatiotemporal time-series data.

</details>


### [29] [Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design](https://arxiv.org/abs/2509.12527)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文提出了信息提升证书的理论框架，通过PAC-Bayes子伽马分析和骨架敏感性定理，为选择性分类提供形式化保证，在多个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生看似合理但错误的输出，现有启发式方法如HallBayes缺乏形式化保证，需要开发具有理论保证的可靠性验证方法。

Method: 开发了信息提升证书的综合理论，包括：(i)PAC-Bayes子伽马分析扩展标准Bernstein边界；(ii)显式骨架敏感性定理量化错误指定的鲁棒性；(iii)假设违反下的故障模式保证；(iv)骨架构建的原则性变分方法。

Result: 在六个数据集和多个模型族上验证假设，在相同风险下减少12-15%的弃权率，运行时开销保持在20%以下（通过批处理进一步降低）。

Conclusion: 该理论框架为选择性分类提供了首个全面的信息提升证书理论，具有形式化保证和实际应用价值，能够有效提高模型输出的可靠性。

Abstract: Large language models often produce plausible but incorrect outputs. Existing
heuristics such as HallBayes lack formal guarantees. We develop the first
comprehensive theory of \emph{information-lift certificates} under selective
classification. Our contributions are: (i) a PAC-Bayes \emph{sub-gamma}
analysis extending beyond standard Bernstein bounds; (ii) explicit skeleton
sensitivity theorems quantifying robustness to misspecification; (iii)
failure-mode guarantees under assumption violations; and (iv) a principled
variational method for skeleton construction. Across six datasets and multiple
model families, we validate assumptions empirically, reduce abstention by
12--15\% at the same risk, and maintain runtime overhead below 20\% (further
reduced via batching).

</details>


### [30] [TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks](https://arxiv.org/abs/2509.12224)
*Parsa Vatani,Mohamed Elrefaie,Farhad Nazarpour,Faez Ahmed*

Main category: cs.LG

TL;DR: TripOptimizer是一个完全可微分的深度学习框架，使用三平面隐式神经表示从点云数据进行快速气动分析和形状优化，在DrivAerNet++数据集上训练，可实现高达11.8%的阻力系数降低。


<details>
  <summary>Details</summary>
Motivation: 传统基于CFD的气动外形优化计算成本高昂，严重限制了设计空间探索，需要开发更高效的优化方法。

Method: 采用变分自编码器，包含基于三平面的隐式神经表示进行高保真3D几何重建和阻力系数预测头，通过修改编码器参数子集来引导初始几何朝向目标阻力值。

Result: 优化设计实现了高达11.8%的阻力系数降低，结果通过超过1.5亿网格单元的高保真CFD模拟验证。

Conclusion: 该框架实现了更敏捷的气动外形优化工作流程，减少了对计算密集型CFD模拟的依赖，特别适用于早期设计阶段，且对非水密网格具有鲁棒性。

Abstract: The computational cost of traditional Computational Fluid Dynamics-based
Aerodynamic Shape Optimization severely restricts design space exploration.
This paper introduces TripOptimizer, a fully differentiable deep learning
framework for rapid aerodynamic analysis and shape optimization directly from
vehicle point cloud data. TripOptimizer employs a Variational Autoencoder
featuring a triplane-based implicit neural representation for high-fidelity 3D
geometry reconstruction and a drag coefficient prediction head. Trained on
DrivAerNet++, a large-scale dataset of 8,000 unique vehicle geometries with
corresponding drag coefficients computed via Reynolds-Averaged Navier-Stokes
simulations, the model learns a latent representation that encodes
aerodynamically salient geometric features. We propose an optimization strategy
that modifies a subset of the encoder parameters to steer an initial geometry
towards a target drag value, and demonstrate its efficacy in case studies where
optimized designs achieved drag coefficient reductions up to 11.8\%. These
results were subsequently validated by using independent, high-fidelity
Computational Fluid Dynamics simulations with more than 150 million cells. A
key advantage of the implicit representation is its inherent robustness to
geometric imperfections, enabling optimization of non-watertight meshes, a
significant challenge for traditional adjoint-based methods. The framework
enables a more agile Aerodynamic Shape Optimization workflow, reducing reliance
on computationally intensive CFD simulations, especially during early design
stages.

</details>


### [31] [Reversible Deep Equilibrium Models](https://arxiv.org/abs/2509.12917)
*Sam McCallum,Kamran Arora,James Foster*

Main category: cs.LG

TL;DR: 提出了可逆深度平衡模型（RevDEQs），解决了传统DEQs梯度计算近似的问题，通过精确梯度计算实现了更稳定的训练和更好的性能


<details>
  <summary>Details</summary>
Motivation: 传统深度平衡模型（DEQs）虽然在大规模任务中表现出色，但其梯度计算是近似的，导致训练不稳定，需要正则化或大量函数评估来修正

Method: 引入可逆深度平衡模型（RevDEQs），允许进行精确的梯度计算，无需正则化，且比DEQs需要更少的函数评估

Result: RevDEQs在语言建模和图像分类任务上实现了最先进的性能，优于可比的隐式和显式模型

Conclusion: RevDEQs通过精确梯度计算解决了DEQs的训练稳定性问题，在保持高性能的同时显著减少了计算需求

Abstract: Deep Equilibrium Models (DEQs) are an interesting class of implicit model
where the model output is implicitly defined as the fixed point of a learned
function. These models have been shown to outperform explicit (fixed-depth)
models in large-scale tasks by trading many deep layers for a single layer that
is iterated many times. However, gradient calculation through DEQs is
approximate. This often leads to unstable training dynamics and requires
regularisation or many function evaluations to fix. Here, we introduce
Reversible Deep Equilibrium Models (RevDEQs) that allow for exact gradient
calculation, no regularisation and far fewer function evaluations than DEQs. We
show that RevDEQs achieve state-of-the-art performance on language modelling
and image classification tasks against comparable implicit and explicit models.

</details>


### [32] [Causal Discovery via Quantile Partial Effect](https://arxiv.org/abs/2509.12981)
*Yikang Chen,Xingzhe Sun,Dehui Du*

Main category: cs.LG

TL;DR: 本文提出了一种基于分位数部分效应(QPE)的因果发现方法，通过在观测分布中利用形状特征的不对称性来识别因果方向，无需考虑机制、噪声或马尔可夫假设。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法通常需要假设噪声模型或函数形式，限制了应用范围。本文旨在开发一种更直接基于观测分布形状特征的因果识别方法。

Method: 利用QPE统计量，假设因果效应位于有限线性空间中，通过基函数检验估计的QPE来区分因果方向。多变量情况下使用Fisher信息作为统计量来确定因果顺序。

Result: 在大量双变量因果发现数据集上的实验证明该方法有效。在多变量合成和真实数据集上验证了使用Fisher信息识别因果顺序的可行性。

Conclusion: QPE方法提供了一种新的因果发现框架，直接从观测分布的形状不对称性中识别因果关系，避免了传统方法对噪声模型和函数形式的依赖。

Abstract: Quantile Partial Effect (QPE) is a statistic associated with conditional
quantile regression, measuring the effect of covariates at different levels.
Our theory demonstrates that when the QPE of cause on effect is assumed to lie
in a finite linear span, cause and effect are identifiable from their
observational distribution. This generalizes previous identifiability results
based on Functional Causal Models (FCMs) with additive, heteroscedastic noise,
etc. Meanwhile, since QPE resides entirely at the observational level, this
parametric assumption does not require considering mechanisms, noise, or even
the Markov assumption, but rather directly utilizes the asymmetry of shape
characteristics in the observational distribution. By performing basis function
tests on the estimated QPE, causal directions can be distinguished, which is
empirically shown to be effective in experiments on a large number of bivariate
causal discovery datasets. For multivariate causal discovery, leveraging the
close connection between QPE and score functions, we find that Fisher
Information is sufficient as a statistical measure to determine causal order
when assumptions are made about the second moment of QPE. We validate the
feasibility of using Fisher Information to identify causal order on multiple
synthetic and real-world multivariate causal discovery datasets.

</details>


### [33] [Learning to Route: Per-Sample Adaptive Routing for Multimodal Multitask Prediction](https://arxiv.org/abs/2509.12227)
*Marzieh Ajirak,Oded Bein,Ellen Rose Bowen,Dora Kanellopoulos,Avital Falk,Faith M. Gunning,Nili Solomonov,Logan Grosenick*

Main category: cs.LG

TL;DR: 提出了一种用于多任务多模态预测的自适应路由统一框架，能够根据样本特性动态选择模态处理路径和任务共享策略，在心理治疗应用中表现出色


<details>
  <summary>Details</summary>
Motivation: 解决心理治疗领域结构化评估和非结构化临床笔记共存、数据部分缺失、结果相关性等数据异质性问题，实现个性化医疗中的自适应信息处理

Method: 基于路由的架构，定义多种模态路径（原始和融合的文本数值特征表示），学习为每个输入选择最信息丰富的专家组合，任务特定预测根据路由决策由共享或独立头生成，端到端训练

Result: 在合成数据和真实心理治疗笔记数据上评估，方法 consistently优于固定多任务或单任务基线，学习到的路由策略提供了模态相关性和任务结构的可解释性洞察

Conclusion: 该框架解决了个性化医疗中的关键挑战，通过考虑数据异质性和任务相关性的逐主体自适应信息处理，可改善心理健康结果、提高治疗分配精度和临床成本效益

Abstract: We propose a unified framework for adaptive routing in multitask, multimodal
prediction settings where data heterogeneity and task interactions vary across
samples. Motivated by applications in psychotherapy where structured
assessments and unstructured clinician notes coexist with partially missing
data and correlated outcomes, we introduce a routing-based architecture that
dynamically selects modality processing pathways and task-sharing strategies on
a per-sample basis. Our model defines multiple modality paths, including raw
and fused representations of text and numeric features and learns to route each
input through the most informative expert combination. Task-specific
predictions are produced by shared or independent heads depending on the
routing decision, and the entire system is trained end-to-end. We evaluate the
model on both synthetic data and real-world psychotherapy notes predicting
depression and anxiety outcomes. Our experiments show that our method
consistently outperforms fixed multitask or single-task baselines, and that the
learned routing policy provides interpretable insights into modality relevance
and task structure. This addresses critical challenges in personalized
healthcare by enabling per-subject adaptive information processing that
accounts for data heterogeneity and task correlations. Applied to
psychotherapy, this framework could improve mental health outcomes, enhance
treatment assignment precision, and increase clinical cost-effectiveness
through personalized intervention strategies.

</details>


### [34] [Single-stream Policy Optimization](https://arxiv.org/abs/2509.13232)
*Zhongwen Xu,Zihan Ding*

Main category: cs.LG

TL;DR: SPO是一种单流策略优化方法，通过持久化KL自适应值跟踪器和全局优势归一化，解决了传统分组方法中的信号丢失和可扩展性问题，在数学推理任务上显著优于GRPO。


<details>
  <summary>Details</summary>
Motivation: 现有基于分组的策略梯度方法（如GRPO）存在两个关键问题：频繁出现的退化组会消除学习信号，同步障碍阻碍了可扩展性。需要一种更稳定、高效的方法来优化大型语言模型。

Method: 提出单流策略优化（SPO），使用持久化的KL自适应值跟踪器替代每组的基线，并在整个批次中全局归一化优势，为每个样本提供稳定、低方差的学习信号。

Result: 在五个困难的数学基准测试中，SPO比GRPO平均maj@32提高了3.4个百分点，在BRUMO 25上提升7.3pp，AIME 25上提升4.4pp，HMMT 25上提升3.3pp。

Conclusion: SPO的成功挑战了为RL算法添加附带复杂性的主流趋势，表明基本原则而非架构变通方法将推动LLM推理的下一个进步浪潮。

Abstract: We revisit policy-gradient optimization for Large Language Models (LLMs) from
a single-stream perspective. Prevailing group-based methods like GRPO reduce
variance with on-the-fly baselines but suffer from critical flaws: frequent
degenerate groups erase learning signals, and synchronization barriers hinder
scalability. We introduce Single-stream Policy Optimization (SPO), which
eliminates these issues by design. SPO replaces per-group baselines with a
persistent, KL-adaptive value tracker and normalizes advantages globally across
the batch, providing a stable, low-variance learning signal for every sample.
Being group-free, SPO enables higher throughput and scales effectively in
long-horizon or tool-integrated settings where generation times vary.
Furthermore, the persistent value tracker naturally enables an adaptive
curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO
converges more smoothly and attains higher accuracy than GRPO, while
eliminating computation wasted on degenerate groups. Ablation studies confirm
that SPO's gains stem from its principled approach to baseline estimation and
advantage normalization, offering a more robust and efficient path for LLM
reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the
average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial
absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,
+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain
in pass@$k$ across the evaluated $k$ values. SPO's success challenges the
prevailing trend of adding incidental complexity to RL algorithms, highlighting
a path where fundamental principles, not architectural workarounds, drive the
next wave of progress in LLM reasoning.

</details>


### [35] [Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study](https://arxiv.org/abs/2509.12229)
*MSR Avinash*

Main category: cs.LG

TL;DR: 在8GB显存的消费级GPU上对Qwen2.5-1.5B模型进行LoRA/QLoRA微调的系统性能分析，发现分页优化器可提升25%吞吐量，bf16效率低于fp16，2048序列长度可行


<details>
  <summary>Details</summary>
Motivation: 探索在严格8GB显存限制的消费级GPU上进行参数高效微调（LoRA/QLoRA）的实际效率，为资源受限的研究者和从业者提供实用指导

Method: 使用NVIDIA RTX 4060对Qwen2.5-1.5B-Instruct模型进行控制性性能分析，系统变化批次大小、序列长度、优化器选择（AdamW vs PagedAdamW）和精度（fp16 vs bf16）

Result: 分页优化器提升吞吐量达25%（628 tok/s vs 500 tok/s基准），bf16效率低于fp16，在8GB限制下可实现2048 tokens的序列长度

Conclusion: 这是首个在消费级GPU上系统研究LLM微调效率的案例研究，提供了可复现的基准测试和实用指南，证明在有限硬件条件下进行高效微调的可行性

Abstract: Fine-tuning large language models (LLMs) with parameter-efficient techniques
such as LoRA and QLoRA has enabled adaptation of foundation models on modest
hardware. Yet the efficiency of such training on consumer-grade GPUs,
especially under strict 8 GB VRAM limits, remains underexplored. We present a
controlled profiling study of LoRA/QLoRA fine-tuning using the
Qwen2.5-1.5B-Instruct model on a single NVIDIA RTX 4060. Across three
representative configurations, we systematically vary batch size, sequence
length, optimizer choice (AdamW vs. PagedAdamW), and precision (fp16 vs. bf16).
We report throughput (tokens/s), time per 10k tokens, and VRAM footprint,
alongside energy estimates derived from GPU board power limits. Our results
show that paged optimizers improve throughput by up to 25% (628 tok/s vs. 500
tok/s baseline), while bf16 degrades efficiency relative to fp16. Despite 8 GB
constraints, sequence lengths up to 2048 tokens were feasible using
parameter-efficient strategies. To our knowledge, this is the first systematic
case study of LLM fine-tuning efficiency on consumer GPUs, providing
reproducible benchmarks and practical guidelines for resource-constrained
researchers and practitioners.

</details>


### [36] [Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction](https://arxiv.org/abs/2509.12234)
*Benjamin Burns,Yuan Xue,Douglas W. Scharre,Xia Ning*

Main category: cs.LG

TL;DR: PerM-MoE是一种新型稀疏混合专家方法，使用独立的路由器处理每个模态，在阿尔茨海默病多模态神经影像数据缺失情况下显著提升预测性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在临床环境中经常面临多个模态缺失的问题，导致预测准确性下降，需要提高模型在高模态缺失率下的灵活性

Method: 提出PerM-MoE方法，为每个模态使用独立的路由器替代传统的单一路由器，使用T1加权MRI、FLAIR、淀粉样蛋白PET和tau PET等多模态神经影像数据

Result: PerM-MoE在大多数模态缺失变化情况下优于现有最先进方法Flex-MoE，并展示了更有效的专家利用效率

Conclusion: PerM-MoE方法能够有效处理临床环境中常见的多模态缺失问题，在阿尔茨海默病进展预测中表现出优越性能

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disease with high
inter-patient variance in rate of cognitive decline. AD progression prediction
aims to forecast patient cognitive decline and benefits from incorporating
multiple neuroimaging modalities. However, existing multimodal models fail to
make accurate predictions when many modalities are missing during inference, as
is often the case in clinical settings. To increase multimodal model
flexibility under high modality missingness, we introduce PerM-MoE, a novel
sparse mixture-of-experts method that uses independent routers for each
modality in place of the conventional, single router. Using T1-weighted MRI,
FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art
Flex-MoE, and unimodal neuroimaging models on predicting two-year change in
Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of
modality missingness. PerM-MoE outperforms the state of the art in most
variations of modality missingness and demonstrates more effective utility of
experts than Flex-MoE.

</details>


### [37] [RL Fine-Tuning Heals OOD Forgetting in SFT](https://arxiv.org/abs/2509.12235)
*Hangzhan Jin,Sitao Luan,Sicheng Lyu,Guillaume Rabusseau,Reihaneh Rabbany,Doina Precup,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: 研究发现SFT-RL两阶段微调中，SFT早期OOD性能最佳但随后下降，RL主要作用是恢复SFT中丢失的OOD能力而非创造新能力，奇异向量旋转是性能变化的关键机制


<details>
  <summary>Details</summary>
Motivation: 探索SFT和RL两阶段微调范式背后的协同机制，验证"SFT记忆、RL泛化"这一简化说法的准确性

Method: 通过SVD分析参数矩阵，手动编辑参数并观察性能影响，分析不同训练阶段的OOD性能变化

Result: 发现奇异值在微调过程中稳定，OOD行为与奇异向量旋转强相关；RL只能在一定范围内恢复SFT丢失的能力

Conclusion: 重新定义了SFT和RL在两阶段微调中的角色，发现奇异向量旋转是性能变化的关键机制，挑战了传统认知

Abstract: The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed
by Reinforcement Learning (RL) has empirically shown better reasoning
performance than one-stage SFT for the post-training of Large Language Models
(LLMs). However, the evolution and mechanism behind the synergy of SFT and RL
are still under-explored and inconclusive. In our study, we find the well-known
claim "SFT memorizes, RL generalizes" is over-simplified, and discover that:
(1) OOD performance peaks at the early stage of SFT and then declines (OOD
forgetting), the best SFT checkpoint cannot be captured by training/test loss;
(2) the subsequent RL stage does not generate fundamentally better OOD
capability, instead it plays an \textbf{OOD restoration} role, recovering the
lost reasoning ability during SFT; (3) The recovery ability has boundaries,
\ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the
lost OOD ability;} (4) To uncover the underlying mechanisms behind the
forgetting and restoration process, we employ SVD analysis on parameter
matrices, manually edit them, and observe their impacts on model performance.
Unlike the common belief that the shift of model capacity mainly results from
the changes of singular values, we find that they are actually quite stable
throughout fine-tuning. Instead, the OOD behavior strongly correlates with the
\textbf{rotation of singular vectors}. Our findings re-identify the roles of
SFT and RL in the two-stage fine-tuning and discover the rotation of singular
vectors as the key mechanism. %reversing the rotations induced by SFT, which
shows recovery from forgetting, whereas imposing the SFT parameter directions
onto a RL-tuned model results in performance degradation. Code is available at
https://github.com/xiaodanguoguo/RL_Heals_SFT

</details>


### [38] [Neural Diffeomorphic-Neural Operator for Residual Stress-Induced Deformation Prediction](https://arxiv.org/abs/2509.12237)
*Changqing Liu,Kaining Dai,Zhiwei Zhao,Tianyi Wu,Yingguang Li*

Main category: cs.LG

TL;DR: 提出基于微分同胚嵌入神经算子的新框架NDNO，通过将复杂三维几何映射到统一参考域，实现不同几何形状下残余应力引起变形的快速准确预测。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法计算残余应力与变形耦合时计算成本高，特别是处理多样几何形状时。神经算子虽能高效求解偏微分方程，但在变化几何域中直接应用存在理论和实践限制。

Method: 使用受平滑性和可逆性约束的微分同胚神经网络将复杂三维几何显式映射到公共参考域，然后在参考域上训练神经算子学习残余应力引起的变形场。

Result: 方法能够准确预测主方向和多方位的变形场，在不同几何形状（包括组件类型、尺寸和特征）的零件上实现了高精度和高效率。

Conclusion: NDNO框架为变化几何形状结构部件的变形预测提供了有效且计算高效的解决方案，实现了快速适应不同几何形状的能力。

Abstract: Accurate prediction of machining deformation in structural components is
essential for ensuring dimensional precision and reliability. Such deformation
often originates from residual stress fields, whose distribution and influence
vary significantly with geometric complexity. Conventional numerical methods
for modeling the coupling between residual stresses and deformation are
computationally expensive, particularly when diverse geometries are considered.
Neural operators have recently emerged as a powerful paradigm for efficiently
solving partial differential equations, offering notable advantages in
accelerating residual stress-deformation analysis. However, their direct
application across changing geometric domains faces theoretical and practical
limitations. To address this challenge, a novel framework based on
diffeomorphic embedding neural operators named neural diffeomorphic-neural
operator (NDNO) is introduced. Complex three-dimensional geometries are
explicitly mapped to a common reference domain through a diffeomorphic neural
network constrained by smoothness and invertibility. The neural operator is
then trained on this reference domain, enabling efficient learning of
deformation fields induced by residual stresses. Once trained, both the
diffeomorphic neural network and the neural operator demonstrate efficient
prediction capabilities, allowing rapid adaptation to varying geometries. The
proposed method thus provides an effective and computationally efficient
solution for deformation prediction in structural components subject to varying
geometries. The proposed method is validated to predict both main-direction and
multi-direction deformation fields, achieving high accuracy and efficiency
across parts with diverse geometries including component types, dimensions and
features.

</details>


### [39] [Interpretable Data Mining of Follicular Thyroid Cancer Ultrasound Features Using Enhanced Association Rules](https://arxiv.org/abs/2509.12238)
*Songlin Zhou,Tao Zhou,Xin Li,Stephen Shing-Toung Yau*

Main category: cs.LG

TL;DR: 基于改进的关联规则挖掘方法分析滤泡性甲状腺癌临床数据，发现结节内结节模式、小梁模式、低TSH评分等新恶性关联指标


<details>
  <summary>Details</summary>
Motivation: 滤泡性甲状腺癌缺乏特异性超声征象，术前诊断困难，相关临床研究较少，需要新的数据分析方法来识别有助于术前诊断的临床指标

Method: 改进传统关联规则挖掘方法，结合可解释机器学习SHAP方法思想，提出新的恶性关联分析指标，对2010-2023年1673例病例数据进行回顾性分析

Result: 除了不规则边界、不均匀晕环等常见指标外，发现结节内结节模式、小梁模式、低TSH评分等具有强恶性关联的新指标，合并桥本甲状腺炎也可能有强恶性关联

Conclusion: 滤泡性甲状腺癌术前诊断应综合考虑多种临床指标，本研究发现的多样化恶性关联指标可为临床医生提供参考

Abstract: Purpose: Thyroid cancer has been a common cancer. Papillary thyroid cancer
and follicular thyroid cancer are the two most common types of thyroid cancer.
Follicular thyroid cancer lacks distinctive ultrasound signs and is more
difficult to diagnose preoperatively than the more prevalent papillary thyroid
cancer, and the clinical studies associated with it are less well established.
We aimed to analyze the clinical data of follicular thyroid cancer based on a
novel data mining tool to identify some clinical indications that may help in
preoperative diagnosis. Methods: We performed a retrospective analysis based on
case data collected by the Department of General Surgery of Peking University
Third Hospital between 2010 and 2023. Unlike traditional statistical methods,
we improved the association rule mining, a classical data mining method, and
proposed new analytical metrics reflecting the malignant association between
clinical indications and cancer with the help of the idea of SHAP method in
interpretable machine learning. Results: The dataset was preprocessed to
contain 1673 cases (in terms of nodes rather than patients), of which 1414 were
benign and 259 were malignant nodes. Our analysis pointed out that in addition
to some common indicators (e.g., irregular or lobulated nodal margins, uneven
thickness halo, hypoechogenicity), there were also some indicators with strong
malignant associations, such as nodule-in-nodule pattern, trabecular pattern,
and low TSH scores. In addition, our results suggest that the combination of
Hashimoto's thyroiditis may also have a strong malignant association.
Conclusion: In the preoperative diagnosis of nodules suspected of follicular
thyroid cancer, multiple clinical indications should be considered for a more
accurate diagnosis. The diverse malignant associations identified in our study
may serve as a reference for clinicians in related fields.

</details>


### [40] [InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation](https://arxiv.org/abs/2509.12239)
*Sanyam Jain,Khuram Naveed,Illia Oleksiienko,Alexandros Iosifidis,Ruben Pauwels*

Main category: cs.LG

TL;DR: InJecteD是一个用于解释去噪扩散概率模型(DDPMs)的框架，通过分析2D点云生成过程中的样本轨迹来增强模型透明度。


<details>
  <summary>Details</summary>
Motivation: 提高DDPMs的可解释性，支持人类与AI协作，使从业者能够调试和改进生成模型。

Method: 使用简化的DDPM架构，分析去噪过程中的轨迹特性（位移、速度、聚类、漂移场动态），采用Wasserstein距离和余弦相似度等统计指标。

Result: 实验揭示了不同的去噪阶段：初始噪声探索、快速形状形成和最终细化，不同数据集表现出特定行为。傅里叶基嵌入提高了轨迹稳定性和重建质量。

Conclusion: InJecteD框架成功提供了DDPMs的可解释性分析，有助于理解生成过程并改进模型设计。

Abstract: This work introduces InJecteD, a framework for interpreting Denoising
Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during
the denoising process of 2D point cloud generation. We apply this framework to
three datasets from the Datasaurus Dozen bullseye, dino, and circle using a
simplified DDPM architecture with customizable input and time embeddings. Our
approach quantifies trajectory properties, including displacement, velocity,
clustering, and drift field dynamics, using statistical metrics such as
Wasserstein distance and cosine similarity. By enhancing model transparency,
InJecteD supports human AI collaboration by enabling practitioners to debug and
refine generative models. Experiments reveal distinct denoising phases: initial
noise exploration, rapid shape formation, and final refinement, with
dataset-specific behaviors example, bullseyes concentric convergence vs. dinos
complex contour formation. We evaluate four model configurations, varying
embeddings and noise schedules, demonstrating that Fourier based embeddings
improve trajectory stability and reconstruction quality

</details>


### [41] [Why and How Auxiliary Tasks Improve JEPA Representations](https://arxiv.org/abs/2509.12249)
*Jiacan Yu,Siyi Chen,Mingrui Liu,Nono Horiuchi,Vladimir Braverman,Zicheng Xu,Dan Haramati,Randall Balestriero*

Main category: cs.LG

TL;DR: 该论文从理论上分析了带有辅助回归头的JEPA变体，证明了在确定性MDP中，当训练使潜在转移一致性损失和辅助回归损失趋近于零时，表示不会发生不健康的崩溃，非等价观测必须映射到不同的潜在表示。


<details>
  <summary>Details</summary>
Motivation: JEPA在视觉表示学习和基于模型的RL中应用日益广泛，但其行为机制仍缺乏理论理解，需要对其表示学习特性进行理论分析。

Method: 提出一个简单的实用JEPA变体，包含与潜在动态联合训练的辅助回归头，在确定性MDP框架下进行理论分析，并通过计数环境中的控制消融实验验证理论。

Result: 证明了"无病态表示崩溃定理"，辅助任务锚定了表示必须保留的区别，联合训练比单独训练产生更丰富的表示。

Conclusion: 指出了改进JEPA编码器的路径：通过训练具有辅助函数和转移动态的编码器来编码正确的等价关系。

Abstract: Joint-Embedding Predictive Architecture (JEPA) is increasingly used for
visual representation learning and as a component in model-based RL, but its
behavior remains poorly understood. We provide a theoretical characterization
of a simple, practical JEPA variant that has an auxiliary regression head
trained jointly with latent dynamics. We prove a No Unhealthy Representation
Collapse theorem: in deterministic MDPs, if training drives both the
latent-transition consistency loss and the auxiliary regression loss to zero,
then any pair of non-equivalent observations, i.e., those that do not have the
same transition dynamics or auxiliary label, must map to distinct latent
representations. Thus, the auxiliary task anchors which distinctions the
representation must preserve. Controlled ablations in a counting environment
corroborate the theory and show that training the JEPA model jointly with the
auxiliary head generates a richer representation than training them separately.
Our work indicates a path to improve JEPA encoders: training them with an
auxiliary function that, together with the transition dynamics, encodes the
right equivalence relations.

</details>


### [42] [Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE](https://arxiv.org/abs/2509.12255)
*Mihir Tare,Clemens Rattasits,Yiming Wu,Euan Wielewski*

Main category: cs.LG

TL;DR: 本文展示了GraphSAGE图神经网络在银行交易网络中的实际应用，能够处理动态异构数据，生成可解释的节点嵌入，并在洗钱检测任务中提升高风险账户识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统图嵌入方法难以处理银行动态交易数据，金融机构需要可扩展的工具来分析复杂的交易网络。GraphSAGE的归纳学习特性使其能够处理未见节点，适合随时间演化的交易数据。

Method: 使用匿名化的客户和商户交易数据构建交易网络，训练GraphSAGE模型生成节点嵌入，并通过下游分类任务验证嵌入的有效性。

Result: 嵌入结果显示出与地理和人口统计特征对齐的可解释聚类，在洗钱检测模型中提高了高风险账户的优先级排序效果。

Conclusion: GraphSAGE框架具有适应性、可扩展性和可解释性，为金融机构利用图机器学习从交易生态系统中获取可操作洞察提供了蓝图。

Abstract: Financial institutions increasingly require scalable tools to analyse complex
transactional networks, yet traditional graph embedding methods struggle with
dynamic, real-world banking data. This paper demonstrates the practical
application of GraphSAGE, an inductive Graph Neural Network framework, to
non-bipartite heterogeneous transaction networks within a banking context.
Unlike transductive approaches, GraphSAGE scales well to large networks and can
generalise to unseen nodes which is critical for institutions working with
temporally evolving transactional data. We construct a transaction network
using anonymised customer and merchant transactions and train a GraphSAGE model
to generate node embeddings. Our exploratory work on the embeddings reveals
interpretable clusters aligned with geographic and demographic attributes.
Additionally, we illustrate their utility in downstream classification tasks by
applying them to a money mule detection model where using these embeddings
improves the prioritisation of high-risk accounts. Beyond fraud detection, our
work highlights the adaptability of this framework to banking-scale networks,
emphasising its inductive capability, scalability, and interpretability. This
study provides a blueprint for financial organisations to harness graph machine
learning for actionable insights in transactional ecosystems.

</details>


### [43] [Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for Diabetes Risk Prediction](https://arxiv.org/abs/2509.12259)
*Kenneth G. Young II*

Main category: cs.LG

TL;DR: QISICGM是一个量子启发的机器学习框架，通过集成多种模型和量子启发技术，在糖尿病风险预测中实现了高精度和效率，F1分数达0.8933，AUC为0.8699。


<details>
  <summary>Details</summary>
Motivation: 解决糖尿病风险预测中的类别不平衡问题，并利用量子启发技术提升模型性能和计算效率，为临床分诊提供可靠的AI辅助工具。

Method: 使用PIMA印第安人糖尿病数据集（含2000个合成样本），集成自改进概念图和堆叠集成模型（包括随机森林、Extra Trees、变换器、CNN和FFNN），采用量子启发的相位特征映射和邻域序列建模。

Result: 在OOF评估中获得F1分数0.8933和AUC 0.8699，CPU推理效率达8.5行/秒，优于传统方法。

Conclusion: QISICGM通过校准、可解释性和开源可复现性，强调了可信AI的重要性，为糖尿病及其他领域的临床分诊提供了潜在基准。

Abstract: The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an
innovative machine learning framework that harnesses quantum-inspired
techniques to predict diabetes risk with exceptional accuracy and efficiency.
Utilizing the PIMA Indians Diabetes dataset augmented with 2,000 synthetic
samples to mitigate class imbalance (total: 2,768 samples, 1,949 positives),
QISICGM integrates a self-improving concept graph with a stacked ensemble
comprising Random Forests (RF), Extra Trees (ET), transformers, convolutional
neural networks (CNNs), and feed-forward neural networks (FFNNs). This approach
achieves an out-of-fold (OOF) F1 score of 0.8933 and an AUC of 0.8699,
outperforming traditional methods. Quantum inspired elements, such as phase
feature mapping and neighborhood sequence modeling, enrich feature
representations, enabling CPU-efficient inference at 8.5 rows per second. This
paper presents a detailed architecture, theoretical foundations, code insights,
and performance evaluations, including visualizations from the outputs
subfolder. The open-source implementation (v1.0.0) is available at
https://github.com/keninayoung/QISICGM, positioning QISICGM as a potential
benchmark for AI-assisted clinical triage in diabetes and beyond. Ultimately,
this work emphasizes trustworthy AI through calibration, interpretability, and
open-source reproducibility.

</details>


### [44] [Explainable Fraud Detection with GNNExplainer and Shapley Values](https://arxiv.org/abs/2509.12262)
*Ngoc Hieu Dao*

Main category: cs.LG

TL;DR: 开发可解释的欺诈检测系统以满足透明度和可理解性要求


<details>
  <summary>Details</summary>
Motivation: 随着数字支付的普及，金融欺诈风险增加，社会和监管机构对AI系统的透明度要求提高，欺诈分析师也需要简洁易懂的解释来提高调查效率

Method: 专注于开发可解释的欺诈检测器

Result: 未在摘要中明确说明具体结果

Conclusion: 需要开发可解释的欺诈检测系统来解决透明度和可理解性挑战

Abstract: The risk of financial fraud is increasing as digital payments are used more
and more frequently. Although the use of artificial intelligence systems for
fraud detection is widespread, society and regulators have raised the standards
for these systems' transparency for reliability verification purposes. To
increase their effectiveness in conducting fraud investigations, fraud analysts
also profit from having concise and understandable explanations. To solve these
challenges, the paper will concentrate on developing an explainable fraud
detector.

</details>


### [45] [Soft Gradient Boosting with Learnable Feature Transforms for Sequential Regression](https://arxiv.org/abs/2509.12920)
*Huseyin Karaca,Suleyman Serdar Kozat*

Main category: cs.LG

TL;DR: 提出一种软梯度提升框架，在提升过程中嵌入可学习的线性特征变换，特别适用于高维数据稀缺场景，通过端到端优化提升性能并避免过拟合


<details>
  <summary>Details</summary>
Motivation: 解决高维数据稀缺场景下传统梯度提升方法难以发现最相关输入表示的问题，通过联合学习特征变换和提升过程来提高性能

Method: 在每个提升迭代中训练软决策树并同时学习线性输入特征变换Q，可扩展到可微非线性变换

Result: 在合成和真实数据集上证明方法能有效提升性能，通过端到端优化避免过拟合

Conclusion: 该方法成功实现了特征选择/变换与提升的联合优化，在高维数据稀缺场景下表现优异，代码已公开以支持复现和后续工作

Abstract: We propose a soft gradient boosting framework for sequential regression that
embeds a learnable linear feature transform within the boosting procedure. At
each boosting iteration, we train a soft decision tree and learn a linear input
feature transform Q together. This approach is particularly advantageous in
high-dimensional, data-scarce scenarios, as it discovers the most relevant
input representations while boosting. We demonstrate, using both synthetic and
real-world datasets, that our method effectively and efficiently increases the
performance by an end-to-end optimization of feature selection/transform and
boosting while avoiding overfitting. We also extend our algorithm to
differentiable non-linear transforms if overfitting is not a problem. To
support reproducibility and future work, we share our code publicly.

</details>


### [46] [Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning](https://arxiv.org/abs/2509.12269)
*Jinmeiyang Wang,Jing Dong,Li Zhou*

Main category: cs.LG

TL;DR: MT-DQN模型融合Transformer、时序图神经网络和深度Q网络，在短视频推荐中显著优于传统方法，F1分数提升10.97%，NDCG@5提升8.3%，相比Vanilla-DQN的MSE降低34.8%，但存在计算成本和延迟敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 解决短视频环境中用户行为预测和推荐策略优化的挑战，传统串联模型和经典强化学习方法在性能和效果上存在不足。

Method: 提出MT-DQN模型，整合Transformer、时序图神经网络(TGNN)和深度Q网络(DQN)三种技术，用于捕捉用户行为序列的时序依赖关系和图结构信息。

Result: 实验显示MT-DQN显著优于传统方法：平均F1-score提升10.97%，NDCG@5提升8.3%；相比Vanilla-DQN，MSE降低34.8%，MAE降低26.5%。

Conclusion: MT-DQN在推荐性能上取得显著改进，但面临实际部署中的计算成本和在线推理延迟挑战，需要通过未来架构优化来解决。

Abstract: This paper proposes the MT-DQN model, which integrates a Transformer,
Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address the
challenges of predicting user behavior and optimizing recommendation strategies
in short-video environments. Experiments demonstrated that MT-DQN consistently
outperforms traditional concatenated models, such as Concat-Modal, achieving an
average F1-score improvement of 10.97% and an average NDCG@5 improvement of
8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQN
reduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognize
challenges in deploying MT-DQN in real-world scenarios, such as its
computational cost and latency sensitivity during online inference, which will
be addressed through future architectural optimization.

</details>


### [47] [CoVariance Filters and Neural Networks over Hilbert Spaces](https://arxiv.org/abs/2509.13178)
*Claudio Battiloro,Andrea Cavallo,Elvin Isufi*

Main category: cs.LG

TL;DR: 该论文提出了希尔伯特协方差网络(HVNs)，这是一种针对无限维希尔伯特空间中信号的卷积学习框架，通过协方差算子实现鲁棒性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有的协方差神经网络(VNNs)主要处理有限维希尔伯特空间，但缺乏对无限维空间的扩展研究。本文旨在将协方差卷积方法推广到无限维希尔伯特空间。

Method: 构建希尔伯特协方差滤波器(HVFs)和希尔伯特协方差网络(HVNs)，采用滤波器组堆叠和非线性激活，提出原则性离散化程序，证明HVFs能够恢复功能主成分分析(FPCA)。

Result: 在合成和真实世界时间序列分类任务上验证了HVNs的性能，相比MLP和FPCA分类器表现出更强的鲁棒性。

Conclusion: 该框架为无限维希尔伯特空间中的信号处理提供了有效的卷积学习方法，具有广泛的应用潜力，从多变量实值函数到再生核希尔伯特空间。

Abstract: CoVariance Neural Networks (VNNs) perform graph convolutions on the empirical
covariance matrix of signals defined over finite-dimensional Hilbert spaces,
motivated by robustness and transferability properties. Yet, little is known
about how these arguments extend to infinite-dimensional Hilbert spaces. In
this work, we take a first step by introducing a novel convolutional learning
framework for signals defined over infinite-dimensional Hilbert spaces,
centered on the (empirical) covariance operator. We constructively define
Hilbert coVariance Filters (HVFs) and design Hilbert coVariance Networks (HVNs)
as stacks of HVF filterbanks with nonlinear activations. We propose a
principled discretization procedure, and we prove that empirical HVFs can
recover the Functional PCA (FPCA) of the filtered signals. We then describe the
versatility of our framework with examples ranging from multivariate
real-valued functions to reproducing kernel Hilbert spaces. Finally, we
validate HVNs on both synthetic and real-world time-series classification
tasks, showing robust performance compared to MLP and FPCA-based classifiers.

</details>


### [48] [Deriving the Scaled-Dot-Function via Maximum Likelihood Estimation and Maximum Entropy Approach](https://arxiv.org/abs/2509.12285)
*Jiyong Ma*

Main category: cs.LG

TL;DR: 提出基于最大似然估计的transformer模型value向量确定方法，将序列建模为高斯分布，为scaled-dot-product和softmax函数提供新解释


<details>
  <summary>Details</summary>
Motivation: 为transformer架构中的scaled-dot-product函数和softmax函数提供新的理论解释，通过概率建模方法理解attention机制

Method: 使用最大似然估计方法，将value向量、key向量和query向量序列建模为高斯分布序列，其中方差依赖于时间步、key向量和query向量，均值依赖于时间步和value向量

Result: 提出了transformer模型中value向量确定的新概率框架，为attention机制提供了基于高斯分布的理论解释

Conclusion: 该方法为transformer架构的核心组件提供了新的概率视角解释，可能启发更深入的理论理解和模型改进

Abstract: In this paper, we present a maximum likelihood estimation approach to
determine the value vector in transformer models. We model the sequence of
value vectors, key vectors, and the query vector as a sequence of Gaussian
distributions. The variance in each Gaussian distribution depends on the time
step, the corresponding key vector, and the query vector. The mean value in
each Gaussian distribution depends on the time step, and the corresponding
value vector. This analysis may offer a new explanation of the
scaled-dot-product function or softmax function used in transformer
architectures [1]. Another explanation, inspired by [4], is based on the
maximum entropy approach in natural language processing [5]. In this approach,
a query vector and key vectors are used to derive the feature functions for the
maximum entropy model.

</details>


### [49] [Prediction of Stocks Index Price using Quantum GANs](https://arxiv.org/abs/2509.12286)
*Sangram Deshpande,Gopal Ramesh Dahale,Sai Nandan Morapakula,Uday Wad*

Main category: cs.LG

TL;DR: 量子生成对抗网络(QGANs)应用于股票价格预测，相比传统LSTM和GAN模型在收敛速度和预测精度方面表现更优


<details>
  <summary>Details</summary>
Motivation: 金融市场具有高度复杂性和波动性，传统模型难以捕捉其复杂模式，量子计算为金融预测提供了新的可能性

Method: 使用AWS Braket SV1模拟器训练QGAN电路，针对股票指数价格数据进行定制化QGAN模型实现

Result: QGANs能够生成与真实市场行为高度相似的合成数据，在收敛速度和预测准确性方面超越经典LSTM和GAN模型

Conclusion: 量子计算在金融预测领域具有重要应用前景，为交易员、金融分析师和研究人员提供了先进的市場分析工具

Abstract: This paper investigates the application of Quantum Generative Adversarial
Networks (QGANs) for stock price prediction. Financial markets are inherently
complex, marked by high volatility and intricate patterns that traditional
models often fail to capture. QGANs, leveraging the power of quantum computing,
offer a novel approach by combining the strengths of generative models with
quantum machine learning techniques. We implement a QGAN model tailored for
stock price prediction and evaluate its performance using historical stock
market data. Our results demonstrate that QGANs can generate synthetic data
closely resembling actual market behavior, leading to enhanced prediction
accuracy. The experiment was conducted using the Stocks index price data and
the AWS Braket SV1 simulator for training the QGAN circuits. The
quantum-enhanced model outperforms classical Long Short-Term Memory (LSTM) and
GAN models in terms of convergence speed and prediction accuracy. This research
represents a key step toward integrating quantum computing in financial
forecasting, offering potential advantages in speed and precision over
traditional methods. The findings suggest important implications for traders,
financial analysts, and researchers seeking advanced tools for market analysis.

</details>


### [50] [C3DE: Causal-Aware Collaborative Neural Controlled Differential Equation for Long-Term Urban Crowd Flow Prediction](https://arxiv.org/abs/2509.12289)
*Yuting Liu,Qiang Zhou,Hanzhe Li,Chenqi Gong,Jingjing Gu*

Main category: cs.LG

TL;DR: 提出C3DE模型，通过神经控制微分方程和因果感知机制解决长期城市人流预测中的累积采样误差和虚假相关性问题


<details>
  <summary>Details</summary>
Motivation: 长期城市人流预测面临累积采样误差和POI动态影响的挑战，特别是多时间尺度异步动态和潜在虚假因果关系

Method: 使用双路径NCDE作为主干网络捕捉多时间尺度协作信号，设计动态校正机制和反事实因果效应估计器量化POI对人流的因果影响

Result: 在三个真实数据集上的实验表明C3DE表现优异，特别是在人流波动显著的城市中

Conclusion: C3DE通过因果感知协作机制有效解决了长期城市人流预测的关键挑战，为时空预测提供了新思路

Abstract: Long-term urban crowd flow prediction suffers significantly from cumulative
sampling errors, due to increased sequence lengths and sampling intervals,
which inspired us to leverage Neural Controlled Differential Equations (NCDEs)
to mitigate this issue. However, regarding the crucial influence of Points of
Interest (POIs) evolution on long-term crowd flow, the multi-timescale
asynchronous dynamics between crowd flow and POI distribution, coupled with
latent spurious causality, poses challenges to applying NCDEs for long-term
urban crowd flow prediction. To this end, we propose Causal-aware Collaborative
neural CDE (C3DE) to model the long-term dynamic of crowd flow. Specifically,
we introduce a dual-path NCDE as the backbone to effectively capture the
asynchronous evolution of collaborative signals across multiple time scales.
Then, we design a dynamic correction mechanism with the counterfactual-based
causal effect estimator to quantify the causal impact of POIs on crowd flow and
minimize the accumulation of spurious correlations. Finally, we leverage a
predictor for long-term prediction with the fused collaborative signals of POI
and crowd flow. Extensive experiments on three real-world datasets demonstrate
the superior performance of C3DE, particularly in cities with notable flow
fluctuations.

</details>


### [51] [Spontaneous Kolmogorov-Arnold Geometry in Shallow MLPs](https://arxiv.org/abs/2509.12326)
*Michael Freedman,Michael Mulligan*

Main category: cs.LG

TL;DR: 研究发现传统单隐藏层神经网络训练中会自发产生Kolmogorov-Arnold几何结构，通过分析Jacobian矩阵的统计特性来量化这种几何特征。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何有机地学习为下游处理准备输入数据，并研究KA几何结构的出现机制以优化网络超参数。

Method: 通过分析Jacobian矩阵J(x)的外部幂统计特性，包括零行数量和子式统计量，来量化KA几何结构。

Result: 在传统单隐藏层神经网络训练中经常观察到KA几何结构的自发产生，并对其在函数复杂度和模型超参数空间中的出现位置有了初步理解。

Conclusion: 这项研究是KA-Networks（KANs）的"另一面"，不是将KA工程化到神经网络中，而是观察KA在浅层MLP中的自然涌现。

Abstract: The Kolmogorov-Arnold (KA) representation theorem constructs universal, but
highly non-smooth inner functions (the first layer map) in a single
(non-linear) hidden layer neural network. Such universal functions have a
distinctive local geometry, a "texture," which can be characterized by the
inner function's Jacobian $J({\mathbf{x}})$, as $\mathbf{x}$ varies over the
data. It is natural to ask if this distinctive KA geometry emerges through
conventional neural network optimization. We find that indeed KA geometry often
is produced when training vanilla single hidden layer neural networks. We
quantify KA geometry through the statistical properties of the exterior powers
of $J(\mathbf{x})$: number of zero rows and various observables for the minor
statistics of $J(\mathbf{x})$, which measure the scale and axis alignment of
$J(\mathbf{x})$. This leads to a rough understanding for where KA geometry
occurs in the space of function complexity and model hyperparameters. The
motivation is first to understand how neural networks organically learn to
prepare input data for later downstream processing and, second, to learn enough
about the emergence of KA geometry to accelerate learning through a timely
intervention in network hyperparameters. This research is the "flip side" of
KA-Networks (KANs). We do not engineer KA into the neural network, but rather
watch KA emerge in shallow MLPs.

</details>


### [52] [Integrating Attention-Enhanced LSTM and Particle Swarm Optimization for Dynamic Pricing and Replenishment Strategies in Fresh Food Supermarkets](https://arxiv.org/abs/2509.12339)
*Xianchen Liu,Tianhui Zhang,Xinyu Zhang,Lingmin Hou,Zhen Guo,Yuanhao Tian,Yang Liu*

Main category: cs.LG

TL;DR: 结合LSTM和PSO的新鲜食品超市定价补货优化方法，通过注意力机制增强预测，实现利润最大化和食品浪费减少


<details>
  <summary>Details</summary>
Motivation: 解决新鲜食品零售中动态定价和库存管理的挑战，平衡利润最大化和食品浪费减少的需求

Method: 使用带注意力机制的LSTM网络预测7天内的销量、价格趋势和损耗率，然后通过PSO算法优化定价和补货策略，结合成本加成定价实现动态调整

Result: 该方法不仅最大化利润，还减少食品浪费，提高决策准确性，为易腐商品行业提供可扩展解决方案

Conclusion: 该框架成功弥合了预测建模与优化之间的差距，为新鲜食品零售和其他易腐商品行业提供了有效的动态定价和库存管理解决方案

Abstract: This paper presents a novel approach to optimizing pricing and replenishment
strategies in fresh food supermarkets by combining Long Short-Term Memory
(LSTM) networks with Particle Swarm Optimization (PSO). The LSTM model,
enhanced with an attention mechanism, is used to predict sales volumes, pricing
trends, and spoilage rates over a seven-day period. The predictions generated
by the LSTM model serve as inputs for the PSO algorithm, which iteratively
optimizes pricing and replenishment strategies to maximize profitability while
adhering to inventory constraints. The integration of cost-plus pricing allows
for dynamic adjustments based on fixed and variable costs, ensuring real-time
adaptability to market fluctuations. The framework not only maximizes profits
but also reduces food waste, contributing to more sustainable supermarket
operations. The attention mechanism enhances the interpretability of the LSTM
model by identifying key time points and factors influencing sales, improving
decision-making accuracy. This methodology bridges the gap between predictive
modeling and optimization, offering a scalable solution for dynamic pricing and
inventory management in fresh food retail and other industries dealing with
perishable goods.

</details>


### [53] [FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning](https://arxiv.org/abs/2509.12344)
*Arth Sojitra,Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: 提出了FEDONet，通过在DeepONet架构中引入傅里叶嵌入主干网络，使用随机傅里叶特征映射来增强空间表示能力，显著提升了PDE求解精度


<details>
  <summary>Details</summary>
Motivation: 标准DeepONet使用全连接线性层的主干网络在捕捉复杂空间结构方面存在局限性，需要改进空间表示能力

Method: 在DeepONet架构中引入傅里叶嵌入主干网络，利用随机傅里叶特征映射来丰富空间表示能力

Result: 在多个PDE数据集上表现优于传统DeepONet，平均相对L2性能提升2-3倍

Conclusion: 傅里叶嵌入能有效增强神经算子学习，为PDE代理建模提供了强大且广泛适用的方法

Abstract: Deep Operator Networks (DeepONets) have recently emerged as powerful
data-driven frameworks for learning nonlinear operators, particularly suited
for approximating solutions to partial differential equations (PDEs). Despite
their promising capabilities, the standard implementation of DeepONets, which
typically employs fully connected linear layers in the trunk network, can
encounter limitations in capturing complex spatial structures inherent to
various PDEs. To address this, we introduce Fourier-embedded trunk networks
within the DeepONet architecture, leveraging random Fourier feature mappings to
enrich spatial representation capabilities. Our proposed Fourier-embedded
DeepONet, FEDONet demonstrates superior performance compared to the traditional
DeepONet across a comprehensive suite of PDE-driven datasets, including the
two-dimensional Poisson equation, Burgers' equation, the Lorenz-63 chaotic
system, Eikonal equation, Allen-Cahn equation, Kuramoto-Sivashinsky equation,
and the Lorenz-96 system. Empirical evaluations of FEDONet consistently show
significant improvements in solution reconstruction accuracy, with average
relative L2 performance gains ranging between 2-3x compared to the DeepONet
baseline. This study highlights the effectiveness of Fourier embeddings in
enhancing neural operator learning, offering a robust and broadly applicable
methodology for PDE surrogate modeling.

</details>


### [54] [Linear Dimensionality Reduction for Word Embeddings in Tabular Data Classification](https://arxiv.org/abs/2509.12346)
*Liam Ressel,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 本文研究了PCA和LDA两种线性降维方法在工程师薪资预测任务中的应用，提出了分区LDA方法，在有限训练样本下显著提升了词嵌入在表格数据分类中的性能。


<details>
  <summary>Details</summary>
Motivation: 工程师薪资预测挑战需要基于表格数据将薪资分为三类，其中职位描述表示为300维词嵌入，大幅增加了维度，而训练样本有限使得分类具有挑战性。词嵌入的线性降维在表格数据分类中尚未充分探索。

Method: 研究PCA和LDA两种线性降维方法。提出Partitioned-LDA方法，将词嵌入分成等大小的块，在每个块上分别执行LDA，从而减小协方差矩阵的规模。结合收缩正则化技术。

Result: PCA在适当子空间维度下优于原始嵌入；无正则化的LDA性能较差，但应用收缩后性能显著提升；Partitioned-LDA优于常规LDA，结合收缩后在竞赛公开排行榜上达到top-10准确率。

Conclusion: Partitioned-LDA方法能有效提升有限训练样本下词嵌入在表格数据分类中的性能表现。

Abstract: The Engineers' Salary Prediction Challenge requires classifying salary
categories into three classes based on tabular data. The job description is
represented as a 300-dimensional word embedding incorporated into the tabular
features, drastically increasing dimensionality. Additionally, the limited
number of training samples makes classification challenging. Linear
dimensionality reduction of word embeddings for tabular data classification
remains underexplored. This paper studies Principal Component Analysis (PCA)
and Linear Discriminant Analysis (LDA). We show that PCA, with an appropriate
subspace dimension, can outperform raw embeddings. LDA without regularization
performs poorly due to covariance estimation errors, but applying shrinkage
improves performance significantly, even with only two dimensions. We propose
Partitioned-LDA, which splits embeddings into equal-sized blocks and performs
LDA separately on each, thereby reducing the size of the covariance matrices.
Partitioned-LDA outperforms regular LDA and, combined with shrinkage, achieves
top-10 accuracy on the competition public leaderboard. This method effectively
enhances word embedding performance in tabular data classification with limited
training samples.

</details>


### [55] [Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields](https://arxiv.org/abs/2509.12358)
*Hong Sun,Joshua A. Vita,Amit Samanta,Vincenzo Lordi*

Main category: cs.LG

TL;DR: 提出MEAGraph模型，一种无监督方法，用于分析原子数据集并消除采样偏差，通过多核变换和注意力机制有效聚类高维原子环境。


<details>
  <summary>Details</summary>
Motivation: 计算化学和材料科学中，常见的数据集生成技术容易在势能面上过采样，传统聚类和剪枝方法由于原子描述符的高维性难以有效识别不同区域，导致信息丢失或偏差去除不彻底。

Method: 开发Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph)模型，结合多线性核变换和基于注意力的消息传递，无需标签或大量训练即可捕获几何敏感性并进行有效数据集剪枝。

Result: 在铌、钽和铁数据集上的应用表明，MEAGraph能有效分组相似原子环境，使基础剪枝技术能够成功移除采样偏差。

Conclusion: MEAGraph为表示学习和聚类提供了有效方法，可用于数据分析、异常检测和数据集优化，解决了高维原子描述符带来的聚类挑战。

Abstract: Constructing a chemically diverse dataset while avoiding sampling bias is
critical to training efficient and generalizable force fields. However, in
computational chemistry and materials science, many common dataset generation
techniques are prone to oversampling regions of the potential energy surface.
Furthermore, these regions can be difficult to identify and isolate from each
other or may not align well with human intuition, making it challenging to
systematically remove bias in the dataset. While traditional clustering and
pruning (down-sampling) approaches can be useful for this, they can often lead
to information loss or a failure to properly identify distinct regions of the
potential energy surface due to difficulties associated with the high
dimensionality of atomic descriptors. In this work, we introduce the
Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph) model, an
unsupervised approach for analyzing atomic datasets. MEAGraph combines multiple
linear kernel transformations with attention-based message passing to capture
geometric sensitivity and enable effective dataset pruning without relying on
labels or extensive training. Demonstrated applications on niobium, tantalum,
and iron datasets show that MEAGraph efficiently groups similar atomic
environments, allowing for the use of basic pruning techniques for removing
sampling bias. This approach provides an effective method for representation
learning and clustering that can be used for data analysis, outlier detection,
and dataset optimization.

</details>


### [56] [Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture](https://arxiv.org/abs/2509.12363)
*Ritesh Janga,Rushit Dave*

Main category: cs.LG

TL;DR: 提出基于联邦学习的智能农业框架，用于明尼苏达州农场的作物病害检测，在保护数据隐私的同时实现高精度分类


<details>
  <summary>Details</summary>
Motivation: 解决农业数据驱动决策需求与农场数据隐私保护之间的矛盾，为明尼苏达州农场提供安全高效的病害检测方案

Method: 采用联邦学习框架，包括本地数据收集、深度学习算法应用、迁移学习和中央聚合服务器进行模型精炼

Result: 预期实现病害检测精度提升、良好泛化能力、降低通信和训练成本、早期病害识别干预

Conclusion: 该框架填补了先进机器学习技术与农民实际隐私需求之间的空白，有望革新智能农业系统

Abstract: The agricultural sector is undergoing a transformation with the integration
of advanced technologies, particularly in data-driven decision-making. This
work proposes a federated learning framework for smart farming, aiming to
develop a scalable, efficient, and secure solution for crop disease detection
tailored to the environmental and operational conditions of Minnesota farms. By
maintaining sensitive farm data locally and enabling collaborative model
updates, our proposed framework seeks to achieve high accuracy in crop disease
classification without compromising data privacy. We outline a methodology
involving data collection from Minnesota farms, application of local deep
learning algorithms, transfer learning, and a central aggregation server for
model refinement, aiming to achieve improved accuracy in disease detection,
good generalization across agricultural scenarios, lower costs in communication
and training time, and earlier identification and intervention against diseases
in future implementations. We outline a methodology and anticipated outcomes,
setting the stage for empirical validation in subsequent studies. This work
comes in a context where more and more demand for data-driven interpretations
in agriculture has to be weighed with concerns about privacy from farms that
are hesitant to share their operational data. This will be important to provide
a secure and efficient disease detection method that can finally revolutionize
smart farming systems and solve local agricultural problems with data
confidentiality. In doing so, this paper bridges the gap between advanced
machine learning techniques and the practical, privacy-sensitive needs of
farmers in Minnesota and beyond, leveraging the benefits of federated learning.

</details>


### [57] [Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder](https://arxiv.org/abs/2509.12372)
*Konstantinos Vasili,Zachery T. Dahm,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 提出基于LSTM自编码器和双重注意力机制的异常检测方法，用于核反应堆辐射监测系统的异常事件检测与定位


<details>
  <summary>Details</summary>
Motivation: 新一代核反应堆产生大量多变量时间序列数据，需要开发远程自主控制系统，而准确诊断模块是实现此类系统的关键第一步。现有ML/DL方法存在可解释性差、缺乏真实数据等问题

Method: 使用无监督的LSTM自编码器，结合特征注意力和时间注意力双重机制。特征注意力识别异常传感器，时间注意力定位异常时间点

Result: 在PUR-1研究反应堆的真实数据集上进行评估，能够同时检测异常事件、定位受影响传感器和确定异常持续时间

Conclusion: 该框架通过统一的网络实现了异常检测和定位，为核能安全关键领域的ML/DL应用提供了更好的可解释性

Abstract: The nuclear industry is advancing toward more new reactor designs, with
next-generation reactors expected to be smaller in scale and power output.
These systems have the potential to produce large volumes of information in the
form of multivariate time-series data, which could be used for enhanced
real-time monitoring and control. In this context, the development of remote
autonomous or semi-autonomous control systems for reactor operation has gained
significant interest. A critical first step toward such systems is an accurate
diagnostics module capable of detecting and localizing anomalies within the
reactor system. Recent studies have proposed various ML and DL approaches for
anomaly detection in the nuclear domain. Despite promising results, key
challenges remain, including limited to no explainability, lack of access to
real-world data, and scarcity of abnormal events, which impedes benchmarking
and characterization. Most existing studies treat these methods as black boxes,
while recent work highlights the need for greater interpretability of ML/DL
outputs in safety-critical domains. Here, we propose an unsupervised
methodology based on an LSTM autoencoder with a dual attention mechanism for
characterization of abnormal events in a real-world reactor radiation area
monitoring system. The framework includes not only detection but also
localization of the event and was evaluated using real-world datasets of
increasing complexity from the PUR-1 research reactor. The attention mechanisms
operate in both the feature and temporal dimensions, where the feature
attention assigns weights to radiation sensors exhibiting abnormal patterns,
while time attention highlights the specific timesteps where irregularities
occur, thus enabling localization. By combining the results, the framework can
identify both the affected sensors and the duration of each anomaly within a
single unified network.

</details>


### [58] [Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data](https://arxiv.org/abs/2509.12375)
*Julian Ripper,Ousama Esbel,Rafael Fietzek,Max Mühlhäuser,Thomas Kreutz*

Main category: cs.LG

TL;DR: 本文提出了一种结合自回归和非自回归技术的混合生成方法，使用去噪扩散概率模型(DDPM)来生成汽车CAN总线时间序列数据，既能生成高质量的合成数据，又能修复损坏样本。


<details>
  <summary>Details</summary>
Motivation: 在包含损坏样本的小型时间序列数据集上训练深度学习模型具有挑战性，需要一种能够生成真实合成数据并修复损坏样本的有效方法。

Method: 提出混合生成方法结合自回归和非自回归技术，应用DDPM模型到车辆CAN数据集，改进了两种最新的DDPM架构，并提出了三个评估指标来量化物理正确性和测试轨道遵循度。

Result: 最佳模型在物理正确性方面甚至超过了训练数据，显示出合理的驾驶行为，并成功修复了训练数据中物理上不合理的区域，提高了数据质量。

Conclusion: DDPM模型能够有效解决小型时间序列数据集的生成和修复问题，特别是在汽车时间序列数据应用中表现出色，为数据增强和损坏数据修复提供了有效解决方案。

Abstract: Training deep learning methods on small time series datasets that also
include corrupted samples is challenging. Diffusion models have shown to be
effective to generate realistic and synthetic data, and correct corrupted
samples through imputation. In this context, this paper focuses on generating
synthetic yet realistic samples of automotive time series data. We show that
denoising diffusion probabilistic models (DDPMs) can effectively solve this
task by applying them to a challenging vehicle CAN-dataset with long-term data
and a limited number of samples. Therefore, we propose a hybrid generative
approach that combines autoregressive and non-autoregressive techniques. We
evaluate our approach with two recently proposed DDPM architectures for time
series generation, for which we propose several improvements. To evaluate the
generated samples, we propose three metrics that quantify physical correctness
and test track adherence. Our best model is able to outperform even the
training data in terms of physical correctness, while showing plausible driving
behavior. Finally, we use our best model to successfully impute physically
implausible regions in the training data, thereby improving the data quality.

</details>


### [59] [Evaluating the printability of stl files with ML](https://arxiv.org/abs/2509.12392)
*Janik Henn,Adrian Hauptmannl,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 开发AI模型检测3D模型中的常见打印问题，帮助经验不足的用户在打印前识别可能导致失败的几何特征


<details>
  <summary>Details</summary>
Motivation: 3D打印技术从专业人士转向大众市场，需要更易用的工具来帮助缺乏经验的用户避免打印失败

Method: 训练AI模型来检测3D模型中可能导致打印失败的几何特征问题

Result: 提出了一种在切片软件中增加AI辅助检测功能的新方法

Conclusion: AI辅助检测可以为3D打印新手提供有价值的支持，在打印开始前识别潜在问题

Abstract: 3D printing has long been a technology for industry professionals and
enthusiasts willing to tinker or even build their own machines. This stands in
stark contrast to today's market, where recent developments have prioritized
ease of use to attract a broader audience. Slicing software nowadays has a few
ways to sanity check the input file as well as the output gcode. Our approach
introduces a novel layer of support by training an AI model to detect common
issues in 3D models. The goal is to assist less experienced users by
identifying features that are likely to cause print failures due to difficult
to print geometries before printing even begins.

</details>


### [60] [Adaptive Spatial Goodness Encoding: Advancing and Scaling Forward-Forward Learning Without Backpropagation](https://arxiv.org/abs/2509.12394)
*Qingchun Gong,Robert Bogdan Staszewski,Kai Xu*

Main category: cs.LG

TL;DR: 提出了ASGE（自适应空间优良性编码）框架，针对CNN的前向-前向算法进行改进，解决了通道维度爆炸问题，在多个数据集上取得了优于其他无反向传播方法的性能，并首次成功应用于ImageNet。


<details>
  <summary>Details</summary>
Motivation: 现有的前向-前向算法扩展虽然改进了原始算法并适应了CNN，但由于通道维度爆炸问题，存在表示能力有限和在大规模数据集上扩展性差的问题。

Method: 提出自适应空间优良性编码（ASGE）框架，利用特征图计算每层的空间感知优良性表示，实现逐层监督，将分类复杂度与通道维度解耦。

Result: 在多个基准测试中优于所有其他基于前向-前向的方法：MNIST 99.65%、FashionMNIST 93.41%、CIFAR-10 90.62%、CIFAR-100 65.42%，首次在ImageNet上实现Top-1 26.21%和Top-5 47.49%的准确率。

Conclusion: ASGE框架完全消除了反向传播，显著缩小了与BP训练模型的性能差距，为可扩展的无反向传播CNN训练奠定了可行基础。

Abstract: The Forward-Forward (FF) algorithm offers a promising alternative to
backpropagation (BP). Despite advancements in recent FF-based extensions, which
have enhanced the original algorithm and adapted it to convolutional neural
networks (CNNs), they often suffer from limited representational capacity and
poor scalability to large-scale datasets, primarily due to exploding channel
dimensionality. In this work, we propose adaptive spatial goodness encoding
(ASGE), a new FF-based training framework tailored for CNNs. ASGE leverages
feature maps to compute spatially-aware goodness representations at each layer,
enabling layer-wise supervision. Crucially, this approach decouples
classification complexity from channel dimensionality, thereby addressing the
issue of channel explosion and achieving competitive performance compared to
other BP-free methods. ASGE outperforms all other FF-based approaches across
multiple benchmarks, delivering test accuracies of 99.65% on MNIST, 93.41% on
FashionMNIST, 90.62% on CIFAR-10, and 65.42% on CIFAR-100. Moreover, we present
the first successful application of FF-based training to ImageNet, with Top-1
and Top-5 accuracies of 26.21% and 47.49%. By entirely eliminating BP and
significantly narrowing the performance gap with BP-trained models, the ASGE
framework establishes a viable foundation toward scalable BP-free CNN training.

</details>


### [61] [Surrogate Representation Inference for Noisy Text and Image Annotations](https://arxiv.org/abs/2509.12416)
*Kentaro Nakamura*

Main category: cs.LG

TL;DR: 本文提出了替代表示推理(SRI)方法，通过假设非结构化数据完全中介人类标注与结构化变量之间的关系，学习低维表示来减少标准误差，无需无错误的人工标注，并能校正非差分测量误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在机器学习和LLM标注非结构化数据时存在标准误差大、需要无错误人工标注的问题，需要开发更有效的偏差校正方法。

Method: 提出SRI方法，假设非结构化数据完全中介标注关系，设计神经网络架构学习满足替代假设的低维表示，建立识别条件和半参数有效估计策略。

Result: 模拟研究和实际应用显示，SRI在机器学习预测精度中等时可将标准误差降低50%以上，即使在人类标注存在非差分测量误差时也能提供有效推断。

Conclusion: SRI方法通过替代表示推理有效解决了现有标注偏差校正方法的局限性，显著提高了统计推断的效率和准确性。

Abstract: As researchers increasingly rely on machine learning models and LLMs to
annotate unstructured data, such as texts or images, various approaches have
been proposed to correct bias in downstream statistical analysis. However,
existing methods tend to yield large standard errors and require some
error-free human annotation. In this paper, I introduce Surrogate
Representation Inference (SRI), which assumes that unstructured data fully
mediate the relationship between human annotations and structured variables.
The assumption is guaranteed by design provided that human coders rely only on
unstructured data for annotation. Under this setting, I propose a neural
network architecture that learns a low-dimensional representation of
unstructured data such that the surrogate assumption remains to be satisfied.
When multiple human annotations are available, SRI can further correct
non-differential measurement errors that may exist in human annotations.
Focusing on text-as-outcome settings, I formally establish the identification
conditions and semiparametric efficient estimation strategies that enable
learning and leveraging such a low-dimensional representation. Simulation
studies and a real-world application demonstrate that SRI reduces standard
errors by over 50% when machine learning prediction accuracy is moderate and
provides valid inference even when human annotations contain non-differential
measurement errors.

</details>


### [62] [On the Regularity and Fairness of Combinatorial Multi-Armed Bandit](https://arxiv.org/abs/2509.12457)
*Xiaoyi Wu,Bin Li*

Main category: cs.LG

TL;DR: 提出了一种参数化的正则公平学习算法，用于组合多臂老虎机问题，同时优化累积奖励、保证公平性和奖励规律性。


<details>
  <summary>Details</summary>
Motivation: 受无线网络应用启发，需要同时最大化累积奖励、保证各臂的公平性（最小平均奖励要求）和确保奖励规律性（各臂获得奖励的频率）。

Method: 算法线性组合虚拟队列长度（跟踪公平性违反）、时间-自上次奖励（TSLR）指标和上置信界（UCB）估计，利用Lyapunov函数进行理论分析。

Result: 理论分析表明算法可实现零累积公平性违反、良好的奖励规律性和累积遗憾性能，并通过两个真实数据集验证。

Conclusion: 所提算法能有效平衡探索-利用权衡，同时满足公平性和规律性约束，在无线网络等应用中具有实用价值。

Abstract: The combinatorial multi-armed bandit model is designed to maximize cumulative
rewards in the presence of uncertainty by activating a subset of arms in each
round. This paper is inspired by two critical applications in wireless
networks, where it's not only essential to maximize cumulative rewards but also
to guarantee fairness among arms (i.e., the minimum average reward required by
each arm) and ensure reward regularity (i.e., how often each arm receives the
reward). In this paper, we propose a parameterized regular and fair learning
algorithm to achieve these three objectives. In particular, the proposed
algorithm linearly combines virtual queue-lengths (tracking the fairness
violations), Time-Since-Last-Reward (TSLR) metrics, and Upper Confidence Bound
(UCB) estimates in its weight measure. Here, TSLR is similar to
age-of-information and measures the elapsed number of rounds since the last
time an arm received a reward, capturing the reward regularity performance, and
UCB estimates are utilized to balance the tradeoff between exploration and
exploitation in online learning. By exploring a key relationship between
virtual queue-lengths and TSLR metrics and utilizing several non-trivial
Lyapunov functions, we analytically characterize zero cumulative fairness
violation, reward regularity, and cumulative regret performance under our
proposed algorithm. These theoretical outcomes are verified by simulations
based on two real-world datasets.

</details>


### [63] [Nonlocal Neural Tangent Kernels via Parameter-Space Interactions](https://arxiv.org/abs/2509.12467)
*Sriram Nagaraj,Vishakh Hari*

Main category: cs.LG

TL;DR: 提出了非局部神经正切核(NNTK)，用参数空间中的非局部相互作用近似替代局部梯度，将NTK理论扩展到非光滑函数、随机估计器和更广泛的模型类别。


<details>
  <summary>Details</summary>
Motivation: 传统神经正切核(NTK)框架依赖于网络对参数可微的假设，这在考虑非光滑目标函数或表现出不可微行为的参数化模型时会失效。

Method: 提出非局部神经正切核(NNTK)，用非局部梯度替代局部梯度，探索了固定核和基于注意力的非局部算子公式。

Result: 通过数值研究验证了新公式的有效性，使NTK理论能够处理更广泛的函数类别和模型。

Conclusion: NNTK框架成功扩展了NTK理论的应用范围，使其能够处理非光滑函数和更广泛的模型类别，为深度学习理论分析提供了新工具。

Abstract: The Neural Tangent Kernel (NTK) framework has provided deep insights into the
training dynamics of neural networks under gradient flow. However, it relies on
the assumption that the network is differentiable with respect to its
parameters, an assumption that breaks down when considering non-smooth target
functions or parameterized models exhibiting non-differentiable behavior. In
this work, we propose a Nonlocal Neural Tangent Kernel (NNTK) that replaces the
local gradient with a nonlocal interaction-based approximation in parameter
space. Nonlocal gradients are known to exist for a wider class of functions
than the standard gradient. This allows NTK theory to be extended to nonsmooth
functions, stochastic estimators, and broader families of models. We explore
both fixed-kernel and attention-based formulations of this nonlocal operator.
We illustrate the new formulation with numerical studies.

</details>


### [64] [Comparative Analysis of Wave Scattering Numerical Modeling Using the Boundary Element Method and Physics-Informed Neural Networks](https://arxiv.org/abs/2509.12483)
*Oscar Rincón-Cardeno,Gregorio Pérez Bernal,Silvana Montoya Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

TL;DR: 本研究比较了边界元法(BEM)和物理信息神经网络(PINNs)在求解二维Helmholtz方程波散射问题中的性能，发现在相同精度下PINNs训练时间比BEM长42倍，但评估速度快204倍，且PINNs在训练域外泛化能力较差。


<details>
  <summary>Details</summary>
Motivation: 评估BEM和PINNs在相同条件下求解Helmholtz方程波散射问题的性能表现，为波动传播问题的研究方法选择提供定量数据支持。

Method: 使用BEM进行边界离散化求解，同时通过超参数优化配置PINNs并最小化控制方程和边界条件的残差来训练神经网络，比较两种方法的求解精度、计算时间和泛化能力。

Result: 在相当精度下，PINNs训练时间比BEM长42倍，但评估速度快204倍；PINNs在训练域外相对误差从7.46×10⁻²增加到8.22，而BEM在扩展区域保持相似误差水平。

Conclusion: 研究为Helmholtz方程的数值求解方法选择提供了直接比较和定量数据，指出了PINNs在训练效率和泛化能力方面的挑战，为未来波传播问题研究确立了发展方向。

Abstract: Purpose - This study compares the Boundary Element Method (BEM) and
Physics-Informed Neural Networks (PINNs) for solving the two-dimensional
Helmholtz equation in wave scattering problems. The objective is to evaluate
the performance of both methods under the same conditions.
  Design/methodology/approach - We solve the Helmholtz equation using BEM and
PINNs for the same scattering problem. The PINNs are trained by minimizing the
residual of the governing equations and boundary conditions, with their
configuration determined through hyperparameter optimization, while the BEM is
applied using boundary discretization. Both methods are evaluated in terms of
solution accuracy, computation time, and generalization capacity.
  Findings - Numerical experiments were conducted by varying the number of
integration points for BEM and the number of layers and neurons per layer for
PINNs. Hyperparameter tuning provided further insight into suitable
configurations for wave scattering problems. At comparable accuracy, PINNs
produced consistent solutions but required training times approximately 42
times longer than BEM. However, once trained, PINNs achieved evaluation times
up to 204 times faster. The generalization capacity was also assessed outside
the PINN training domain, where the relative error increased from $7.46 \times
10^{-2}$ to 8.22, while BEM maintained a similar error level in the extended
region.
  Originality/value - This work presents a direct comparison between PINNs and
BEM for the Helmholtz equation. The analysis provides quantitative data on the
performance of both methods, supporting their selection in future research on
wave propagation problems and establishing future challenges and directions.

</details>


### [65] [Finite-Agent Stochastic Differential Games on Large Graphs: II. Graph-Based Architectures](https://arxiv.org/abs/2509.12484)
*Ruimeng Hu,Jihao Long,Haosheng Zhou*

Main category: cs.LG

TL;DR: 提出了一种名为NTM的新型神经网络架构，用于计算图结构随机微分博弈中的纳什均衡。该架构通过图引导的稀疏化技术，在神经网络中嵌入固定的非可训练组件，显著减少参数数量并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 图结构多智能体系统在金融、机器人、能源等领域广泛应用，但现有方法在大规模稀疏设置下计算效率低且缺乏可解释性。需要一种既能保持性能又能提高效率的解决方案。

Method: 设计NTM架构，在图结构上施加稀疏化约束，嵌入固定的非可训练组件。将NTM集成到Direct Parameterization和Deep BSDE两种先进博弈求解器中，形成NTM-DP和NTM-DBSDE变体。

Result: 理论证明NTM在静态图博弈中具有通用逼近性质。数值实验表明，基于NTM的方法在三种随机微分博弈中性能与全可训练对应方法相当，但计算效率更高。

Conclusion: NTM架构通过图引导的稀疏化有效减少了可训练参数数量，在保持性能的同时提高了计算效率和模型可解释性，为大规模图结构博弈问题提供了实用解决方案。

Abstract: We propose a novel neural network architecture, called Non-Trainable
Modification (NTM), for computing Nash equilibria in stochastic differential
games (SDGs) on graphs. These games model a broad class of graph-structured
multi-agent systems arising in finance, robotics, energy, and social dynamics,
where agents interact locally under uncertainty. The NTM architecture imposes a
graph-guided sparsification on feedforward neural networks, embedding fixed,
non-trainable components aligned with the underlying graph topology. This
design enhances interpretability and stability, while significantly reducing
the number of trainable parameters in large-scale, sparse settings. We
theoretically establish a universal approximation property for NTM in static
games on graphs and numerically validate its expressivity and robustness
through supervised learning tasks. Building on this foundation, we incorporate
NTM into two state-of-the-art game solvers, Direct Parameterization and Deep
BSDE, yielding their sparse variants (NTM-DP and NTM-DBSDE). Numerical
experiments on three SDGs across various graph structures demonstrate that
NTM-based methods achieve performance comparable to their fully trainable
counterparts, while offering improved computational efficiency.

</details>


### [66] [Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model](https://arxiv.org/abs/2509.12497)
*Alessandro Crimi,Andrea Brovelli*

Main category: cs.LG

TL;DR: 本研究评估了基础模型在fMRI脑信号预测和因果发现中的表现，与传统Wiener-Granger因果方法相比，基础模型在零样本设置下展现出竞争优势和更精确的因果交互检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的兴起，需要了解其在神经科学时间序列预测和因果发现方面与传统方法的比较，以及是否能在零样本设置下应用。

Method: 使用合成时间序列（逻辑映射耦合和Ornstein-Uhlenbeck过程）验证方法，测试基础模型在零样本和微调设置下的预测能力，并将模型的类Granger因果估计与标准Granger因果进行比较。

Result: 基础模型在零样本预测fMRI时间序列方面表现竞争性（对照组MAPE为0.55，患者组为0.27），虽然标准Granger因果未显示模型间的明显定量差异，但基础模型提供了更精确的因果交互检测。

Conclusion: 基础模型具有多功能性、强大的零样本性能，在时间序列数据的预测和因果发现方面具有潜在应用价值。

Abstract: Time-series forecasting and causal discovery are central in neuroscience, as
predicting brain activity and identifying causal relationships between neural
populations and circuits can shed light on the mechanisms underlying cognition
and disease. With the rise of foundation models, an open question is how they
compare to traditional methods for brain signal forecasting and causality
analysis, and whether they can be applied in a zero-shot setting. In this work,
we evaluate a foundation model against classical methods for inferring
directional interactions from spontaneous brain activity measured with
functional magnetic resonance imaging (fMRI) in humans. Traditional approaches
often rely on Wiener-Granger causality. We tested the forecasting ability of
the foundation model in both zero-shot and fine-tuned settings, and assessed
causality by comparing Granger-like estimates from the model with standard
Granger causality. We validated the approach using synthetic time series
generated from ground-truth causal models, including logistic map coupling and
Ornstein-Uhlenbeck processes. The foundation model achieved competitive
zero-shot forecasting fMRI time series (mean absolute percentage error of 0.55
in controls and 0.27 in patients). Although standard Granger causality did not
show clear quantitative differences between models, the foundation model
provided a more precise detection of causal interactions.
  Overall, these findings suggest that foundation models offer versatility,
strong zero-shot performance, and potential utility for forecasting and causal
discovery in time-series data.

</details>


### [67] [Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time](https://arxiv.org/abs/2509.12521)
*Yifan Lan,Yuanpu Cao,Weitong Zhang,Lu Lin,Jinghui Chen*

Main category: cs.LG

TL;DR: 本文发现多模态大语言模型(MLLMs)存在新的安全风险：通过精心优化的图像可以任意操纵MLLMs的输出偏好，生成具有偏见但难以检测的响应。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的广泛应用，其安全性问题日益凸显。作者发现MLLMs的输出偏好可以被恶意图像操纵，这种攻击生成看似合理但带有偏见的响应，难以被检测。

Method: 提出Preference Hijacking (Phi)方法，通过在推理时使用偏好劫持图像来操纵MLLM响应偏好，无需修改模型。还引入了通用劫持扰动，可嵌入不同图像中实现偏好劫持。

Result: 在各种任务上的实验结果表明该方法具有有效性，能够成功劫持MLLMs的输出偏好。

Conclusion: 该研究揭示了MLLMs存在严重的安全漏洞，攻击者可以通过优化图像来操纵模型输出偏好，这对MLLMs的安全部署提出了重要挑战。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have gained significant
attention across various domains. However, their widespread adoption has also
raised serious safety concerns. In this paper, we uncover a new safety risk of
MLLMs: the output preference of MLLMs can be arbitrarily manipulated by
carefully optimized images. Such attacks often generate contextually relevant
yet biased responses that are neither overtly harmful nor unethical, making
them difficult to detect. Specifically, we introduce a novel method, Preference
Hijacking (Phi), for manipulating the MLLM response preferences using a
preference hijacked image. Our method works at inference time and requires no
model modifications. Additionally, we introduce a universal hijacking
perturbation -- a transferable component that can be embedded into different
images to hijack MLLM responses toward any attacker-specified preferences.
Experimental results across various tasks demonstrate the effectiveness of our
approach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi.

</details>


### [68] [Graph Homophily Booster: Rethinking the Role of Discrete Features on Heterophilic Graphs](https://arxiv.org/abs/2509.12530)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: GRAPHITE是一个通过图变换直接提高图同配性来解决异配图问题的新框架，在异配图上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有GNN在异配图上表现不佳，甚至不如简单MLP，需要超越架构设计的新方法来直接解决异配性问题

Method: 通过创建特征节点来促进具有相似特征节点间的同配性消息传递，直接转换图结构提高同配性

Result: 在挑战性数据集上显著优于最先进方法，同时在同配图上达到可比精度，理论证明能显著提高异配图的同配性

Conclusion: GRAPHITE提供了一个直接通过图变换解决异配性问题的新范式，是首个显式转换图来提高同配性的方法

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling
graph-structured data. However, existing GNNs often struggle with heterophilic
graphs, where connected nodes tend to have dissimilar features or labels. While
numerous methods have been proposed to address this challenge, they primarily
focus on architectural designs without directly targeting the root cause of the
heterophily problem. These approaches still perform even worse than the
simplest MLPs on challenging heterophilic datasets. For instance, our
experiments show that 21 latest GNNs still fall behind the MLP on the Actor
dataset. This critical challenge calls for an innovative approach to addressing
graph heterophily beyond architectural designs. To bridge this gap, we propose
and study a new and unexplored paradigm: directly increasing the graph
homophily via a carefully designed graph transformation. In this work, we
present a simple yet effective framework called GRAPHITE to address graph
heterophily. To the best of our knowledge, this work is the first method that
explicitly transforms the graph to directly improve the graph homophily.
Stemmed from the exact definition of homophily, our proposed GRAPHITE creates
feature nodes to facilitate homophilic message passing between nodes that share
similar features. Furthermore, we both theoretically and empirically show that
our proposed GRAPHITE significantly increases the homophily of originally
heterophilic graphs, with only a slight increase in the graph size. Extensive
experiments on challenging datasets demonstrate that our proposed GRAPHITE
significantly outperforms state-of-the-art methods on heterophilic graphs while
achieving comparable accuracy with state-of-the-art methods on homophilic
graphs.

</details>


### [69] [Cross-Modal Deep Metric Learning for Time Series Anomaly Detection](https://arxiv.org/abs/2509.12540)
*Wei Li,Zheze Yang*

Main category: cs.LG

TL;DR: 提出基于跨模态深度度量学习的时间序列异常检测方法，通过构建特征聚类模型、计算聚类中心距离、使用随机梯度下降优化，结合vMF分布描述数据方向特征，实现高精度快速异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列异常检测中灵敏度低和耗时高的问题，提高检测效率和准确性。

Method: 构建跨模态深度度量学习特征聚类模型，包含输入层、三元组选择层和损失函数计算层；计算聚类中心平方欧氏距离，使用随机梯度下降优化；应用vMF分布描述时间序列数据方向特征，通过主成分方向向量内积作为异常度量指标。

Result: 实验结果表明该方法能准确分类不同属性的时间序列数据，对异常具有高灵敏度，检测精度高、速度快、鲁棒性强。

Conclusion: 所提出的跨模态深度度量学习方法有效解决了时间序列异常检测的敏感性和效率问题，具有优异的检测性能和实用价值。

Abstract: To effectively address the issues of low sensitivity and high time
consumption in time series anomaly detection, we propose an anomaly detection
method based on cross-modal deep metric learning. A cross-modal deep metric
learning feature clustering model is constructed, composed of an input layer, a
triplet selection layer, and a loss function computation layer. The squared
Euclidean distances between cluster centers are calculated, and a stochastic
gradient descent strategy is employed to optimize the model and classify
different time series features. The inner product of principal component
direction vectors is used as a metric for anomaly measurement. The von
Mises-Fisher (vMF) distribution is applied to describe the directional
characteristics of time series data, and historical data is used to train and
obtain evaluation parameters. By comparing the principal component direction
vector of actual time series data with the threshold, anomaly detection is
performed. Experimental results demonstrate that the proposed method accurately
classifies time series data with different attributes, exhibits high
sensitivity to anomalies, and achieves high detection accuracy, fast detection
speed, and strong robustness.

</details>


### [70] [iCD: A Implicit Clustering Distillation Mathod for Structural Information Mining](https://arxiv.org/abs/2509.12553)
*Xiang Xue,Yatu Ji,Qing-dao-er-ji Ren,Bao Shi,Min Lu,Nier Wu,Xufei Zhuang,Haiteng Xu,Gan-qi-qi-ge Cha*

Main category: cs.LG

TL;DR: 提出了隐式聚类蒸馏(iCD)方法，通过Gram矩阵挖掘和迁移可解释的结构知识，无需真实标签或特征对齐，在细粒度分类任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 解决Logit知识蒸馏方法决策过程可解释性有限的问题，希望在不依赖中间特征对齐的情况下提升模型的可解释性

Method: 利用解耦的局部logit表示的Gram矩阵，使学生模型能够学习潜在的语义结构模式，无需真实标签或特征空间对齐

Result: 在基准数据集上的广泛实验显示iCD在不同师生架构中均有效，在细粒度分类任务中表现突出，最高比基线提升5.08%

Conclusion: iCD是一种简单有效的可解释知识蒸馏方法，能够成功挖掘和迁移结构知识，特别适用于细粒度分类场景

Abstract: Logit Knowledge Distillation has gained substantial research interest in
recent years due to its simplicity and lack of requirement for intermediate
feature alignment; however, it suffers from limited interpretability in its
decision-making process. To address this, we propose implicit Clustering
Distillation (iCD): a simple and effective method that mines and transfers
interpretable structural knowledge from logits, without requiring ground-truth
labels or feature-space alignment. iCD leverages Gram matrices over decoupled
local logit representations to enable student models to learn latent semantic
structural patterns. Extensive experiments on benchmark datasets demonstrate
the effectiveness of iCD across diverse teacher-student architectures, with
particularly strong performance in fine-grained classification tasks --
achieving a peak improvement of +5.08% over the baseline. The code is available
at: https://github.com/maomaochongaa/iCD.

</details>


### [71] [No Need for "Learning" to Defer? A Training Free Deferral Framework to Multiple Experts through Conformal Prediction](https://arxiv.org/abs/2509.12573)
*Tim Bary,Benoît Macq,Louis Petit*

Main category: cs.LG

TL;DR: 提出了一种基于共形预测的无训练、模型无关的专家延迟框架，通过预测集识别标签不确定性并选择最具区分度的专家，在CIFAR10-H和ImageNet16-H上实现了99.57%和99.40%的准确率，同时将专家工作量减少最多11倍。


<details>
  <summary>Details</summary>
Motivation: 现有学习延迟(L2D)方法对专家组成变化敏感，需要大量重新训练。需要一种无需重新训练、模型和专家无关的解决方案来改善人机协作决策。

Method: 使用共形预测器生成的预测集来识别标签特定不确定性，通过可分离性标准选择最能区分剩余可能标签的专家。

Result: 在CIFAR10-H和ImageNet16-H数据集上，方法始终优于独立模型和最强者专家，准确率达到99.57±0.10%和99.40±0.52%，专家工作量减少最多11倍。在专家性能下降和低信息设置下保持鲁棒性。

Conclusion: 该方法为现实世界人机协作提供了可扩展、无需重新训练的L2D替代方案，显著提升了决策准确性和效率。

Abstract: AI systems often fail to deliver reliable predictions across all inputs,
prompting the need for hybrid human-AI decision-making. Existing Learning to
Defer (L2D) approaches address this by training deferral models, but these are
sensitive to changes in expert composition and require significant retraining
if experts change. We propose a training-free, model- and expert-agnostic
framework for expert deferral based on conformal prediction. Our method uses
the prediction set generated by a conformal predictor to identify
label-specific uncertainty and selects the most discriminative expert using a
segregativity criterion, measuring how well an expert distinguishes between the
remaining plausible labels. Experiments on CIFAR10-H and ImageNet16-H show that
our method consistently outperforms both the standalone model and the strongest
expert, with accuracies attaining $99.57\pm0.10\%$ and $99.40\pm0.52\%$, while
reducing expert workload by up to a factor of $11$. The method remains robust
under degraded expert performance and shows a gradual performance drop in
low-information settings. These results suggest a scalable, retraining-free
alternative to L2D for real-world human-AI collaboration.

</details>


### [72] [Exploring Training Data Attribution under Limited Access Constraints](https://arxiv.org/abs/2509.12581)
*Shiyuan Zhang,Junwei Deng,Juhan Bae,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本文系统研究了在模型访问受限和计算资源有限条件下训练数据归因(TDA)方法的可行性，提出了使用代理模型等解决方案，并证明无需目标数据集预训练的模型也能获得有意义的归因分数。


<details>
  <summary>Details</summary>
Motivation: 现有的梯度基TDA方法(如影响函数)虽然性能优越，但在实际应用中面临两大挑战：商业模型不公开访问和计算资源有限，这限制了TDA方法的广泛应用。

Method: 研究了不同访问约束级别下的TDA可行性，设计了代理模型等解决方案，验证了无需目标数据集预训练的模型在多种任务中仍能提供有信息的归因分数。

Result: 证明了在有限访问条件下TDA的可行性，归因分数在不同约束场景下仍保持有效性，为实际部署提供了实用指导。

Conclusion: 该研究为在现实环境中部署TDA方法提供了可行性方案，提高了在有限访问条件下的实用性和效率，推动了TDA技术在实际应用中的广泛采纳。

Abstract: Training data attribution (TDA) plays a critical role in understanding the
influence of individual training data points on model predictions.
Gradient-based TDA methods, popularized by \textit{influence function} for
their superior performance, have been widely applied in data selection, data
cleaning, data economics, and fact tracing. However, in real-world scenarios
where commercial models are not publicly accessible and computational resources
are limited, existing TDA methods are often constrained by their reliance on
full model access and high computational costs. This poses significant
challenges to the broader adoption of TDA in practical applications.
  In this work, we present a systematic study of TDA methods under various
access and resource constraints. We investigate the feasibility of performing
TDA under varying levels of access constraints by leveraging appropriately
designed solutions such as proxy models. Besides, we demonstrate that
attribution scores obtained from models without prior training on the target
dataset remain informative across a range of tasks, which is useful for
scenarios where computational resources are limited. Our findings provide
practical guidance for deploying TDA in real-world environments, aiming to
improve feasibility and efficiency under limited access.

</details>


### [73] [A Multimodal Foundation Model to Enhance Generalizability and Data Efficiency for Pan-cancer Prognosis Prediction](https://arxiv.org/abs/2509.12600)
*Huajun Zhou,Fengtao Zhou,Jiabo Ma,Yingxue Xu,Xi Wang,Xiuming Zhang,Li Liang,Zhenhui Li,Hao Chen*

Main category: cs.LG

TL;DR: MICE是一个多模态基础模型，通过功能多样化的专家模块整合病理图像、临床报告和基因组数据，在泛癌预后预测中显著优于现有方法，展现出优异的泛化能力和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型难以充分利用多模态数据的丰富信息，提取的表示泛化性差，需要开发能够有效整合病理图像、临床报告和基因组数据的泛癌预后预测方法。

Method: 采用多专家协作框架，使用功能多样化的专家模块全面捕捉跨癌症和癌症特异性信息，结合对比学习和监督学习增强模型泛化性，基于11,799名患者30种癌症类型的数据进行训练。

Result: MICE在内部队列上C-index提升3.8%-11.2%，在独立队列上提升5.8%-8.8%，显著优于单模态和最先进的多专家多模态模型，并展现出卓越的数据效率。

Conclusion: MICE为泛癌预后预测建立了有效且可扩展的基础框架，具有个性化定制治疗和改善治疗结果的强大潜力。

Abstract: Multimodal data provides heterogeneous information for a holistic
understanding of the tumor microenvironment. However, existing AI models often
struggle to harness the rich information within multimodal data and extract
poorly generalizable representations. Here we present MICE (Multimodal data
Integration via Collaborative Experts), a multimodal foundation model that
effectively integrates pathology images, clinical reports, and genomics data
for precise pan-cancer prognosis prediction. Instead of conventional
multi-expert modules, MICE employs multiple functionally diverse experts to
comprehensively capture both cross-cancer and cancer-specific insights.
Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE's
generalizability by coupling contrastive and supervised learning. MICE
outperformed both unimodal and state-of-the-art multi-expert-based multimodal
models, demonstrating substantial improvements in C-index ranging from 3.8% to
11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,
respectively. Moreover, it exhibited remarkable data efficiency across diverse
clinical scenarios. With its enhanced generalizability and data efficiency,
MICE establishes an effective and scalable foundation for pan-cancer prognosis
prediction, holding strong potential to personalize tailored therapies and
improve treatment outcomes.

</details>


### [74] [High-Energy Concentration for Federated Learning in Frequency Domain](https://arxiv.org/abs/2509.12630)
*Haozhi Shi,Weiying Xie,Hangyu Ye,Daixun Li,Jitao Ma,Leyuan Fang*

Main category: cs.LG

TL;DR: 提出FedFD方法，通过频域高能量集中特性过滤冗余高频信息，在降低联邦学习通信成本的同时提升性能


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中合成数据方法在空间域设计存在冗余信息和噪声的问题，这些冗余增加了通信负担

Method: 基于离散余弦变换的高能量集中特性，设计二进制掩码保留低频分量，通过频域分布对齐和真实数据驱动的合成分类损失来优化低频分量质量

Result: 在5个图像和语音数据集上优于最先进方法，在CIFAR-10数据集上通信成本降低37.78%，性能提升10.88%

Conclusion: 频域感知的联邦学习方法FedFD能有效降低通信成本并提升模型性能，为联邦学习提供了新的优化方向

Abstract: Federated Learning (FL) presents significant potential for collaborative
optimization without data sharing. Since synthetic data is sent to the server,
leveraging the popular concept of dataset distillation, this FL framework
protects real data privacy while alleviating data heterogeneity. However, such
methods are still challenged by the redundant information and noise in entire
spatial-domain designs, which inevitably increases the communication burden. In
this paper, we propose a novel Frequency-Domain aware FL method with
high-energy concentration (FedFD) to address this problem. Our FedFD is
inspired by the discovery that the discrete cosine transform predominantly
distributes energy to specific regions, referred to as high-energy
concentration. The principle behind FedFD is that low-energy like
high-frequency components usually contain redundant information and noise, thus
filtering them helps reduce communication costs and optimize performance. Our
FedFD is mathematically formulated to preserve the low-frequency components
using a binary mask, facilitating an optimal solution through frequency-domain
distribution alignment. In particular, real data-driven synthetic
classification is imposed into the loss to enhance the quality of the
low-frequency components. On five image and speech datasets, FedFD achieves
superior performance than state-of-the-art methods while reducing communication
costs. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\alpha
= 0.01$, FedFD achieves a minimum reduction of 37.78\% in the communication
cost, while attaining a 10.88\% performance gain.

</details>


### [75] [Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection](https://arxiv.org/abs/2509.12650)
*Chan Sik Han,Keon Myung Lee*

Main category: cs.LG

TL;DR: TimeRep是一种新颖的时间序列异常检测方法，利用时间序列基础模型的中间层表示来计算异常分数，通过距离度量而非传统的重构或预测误差，在UCR异常档案数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法主要依赖基础模型的最后一层表示，通过重构或预测误差计算异常分数，忽略了中间层可能包含的丰富信息。

Method: 选择预训练TSFM中最具信息量的中间层和补丁标记位置，构建参考表示集合并应用核心集策略缩减规模，通过距离度量计算异常分数，并集成适应机制处理概念漂移。

Result: 在包含250个单变量时间序列的UCR异常档案数据集上，TimeRep持续优于包括非深度学习、深度学习和基础模型方法在内的广泛最先进基线。

Conclusion: 利用时间序列基础模型的中间层表示进行异常检测是有效的，TimeRep方法在多个基准测试中表现出卓越性能，为时间序列异常检测提供了新思路。

Abstract: Detecting anomalies in time series data is essential for the reliable
operation of many real-world systems. Recently, time series foundation models
(TSFMs) have emerged as a powerful tool for anomaly detection. However,
existing methods typically rely on the final layer's representations of TSFMs,
computing the anomaly score as a reconstruction or forecasting error via a
task-specific head. Instead, we propose TimeRep, a novel anomaly detection
approach that leverages the intermediate layer's representations of TSFMs,
computing the anomaly score as the distance between these representations.
Given a pre-trained TSFM, TimeRep selects the intermediate layer and
patch-token position that yield the most informative representation. TimeRep
forms a reference collection of intermediate representations from the training
data and applies a core-set strategy to reduce its size while maintaining
distributional coverage. During inference, TimeRep computes the anomaly score
for incoming data by measuring the distance between its intermediate
representations and those of the collection. To address concept drift, TimeRep
integrates an adaptation mechanism that, at inference time, augments the
collection exclusively with non-redundant intermediate representations from
incoming data. We conducted extensive experiments on the UCR Anomaly Archive,
which contains 250 univariate time series. TimeRep consistently outperforms a
broad spectrum of state-of-the-art baselines, including non-DL, DL, and
foundation model-based methods.

</details>


### [76] [Instance-level Randomization: Toward More Stable LLM Evaluations](https://arxiv.org/abs/2509.12678)
*Yiyang Li,Yonghuang Wu,Ying Luo,Liangtai Sun,Zishu Qin,Lin Qiu,Xuezhi Cao,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出实例级随机化(ILR)方法来解决大语言模型评估中的不稳定性问题，通过为每个实例随机化所有影响因素并多次实验取平均，减少方差并增强模型比较的公平性


<details>
  <summary>Details</summary>
Motivation: 大语言模型评估存在不稳定性，随机因素(如few-shot示例)的微小变化会导致评分剧烈波动甚至模型排名变化，不同模型对特定随机因素设置有不同偏好，固定设置可能导致不公平比较

Method: 提出实例级随机化(ILR)方法：为每个实例随机化所有影响评估分数的因素，进行多次实验并报告平均分数，而不是在整个基准测试中使用固定设置

Result: 理论分析和实证结果表明，ILR可以减少随机因素引起的方差和不公平比较，并且以不到一半的计算成本达到与先前方法相似的鲁棒性水平

Conclusion: ILR方法能有效解决LLM评估中的不稳定性问题，提供更公平可靠的模型比较，同时显著降低计算成本

Abstract: Evaluations of large language models (LLMs) suffer from instability, where
small changes of random factors such as few-shot examples can lead to drastic
fluctuations of scores and even model rankings. Moreover, different LLMs can
have different preferences for a certain setting of random factors. As a
result, using a fixed setting of random factors, which is often adopted as the
paradigm of current evaluations, can lead to potential unfair comparisons
between LLMs. To mitigate the volatility of evaluations, we first theoretically
analyze the sources of variance induced by changes in random factors. Targeting
these specific sources, we then propose the instance-level randomization (ILR)
method to reduce variance and enhance fairness in model comparisons. Instead of
using a fixed setting across the whole benchmark in a single experiment, we
randomize all factors that affect evaluation scores for every single instance,
run multiple experiments and report the averaged score. Theoretical analyses
and empirical results demonstrate that ILR can reduce the variance and unfair
comparisons caused by random factors, as well as achieve similar robustness
level with less than half computational cost compared with previous methods.

</details>


### [77] [Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry](https://arxiv.org/abs/2509.12679)
*Oliver Knitter,Dan Zhao,Stefan Leichenauer,Shravan Veerapaneni*

Main category: cs.LG

TL;DR: 该研究探索了基于Transformer的神经量子态(NQS)在量子化学应用中的缩放定律，发现模型大小与训练时间的关系高度依赖于损失度量和波函数拟设，与语言模型的线性关系不同。


<details>
  <summary>Details</summary>
Motivation: 随着神经量子态(NQS)越来越多地采用基于大语言模型(LLM)的组件，研究者希望理解NQS的缩放定律，从而揭示NQS波函数拟设的可扩展性和最优性能-资源权衡。

Method: 识别基于Transformer的NQS在二次量子化量子化学应用中的缩放定律，通过计算约束优化获得的参数曲线来分析性能与问题大小的关系。

Result: 研究发现模型大小与训练时间的关系高度依赖于损失度量(绝对误差和V-score)和波函数拟设，并不遵循语言模型中发现的近似线性关系。

Conclusion: NQS的缩放行为与语言模型存在显著差异，需要针对量子化学应用开发专门的缩放定律来指导模型设计和资源分配。

Abstract: Scaling laws have been used to describe how large language model (LLM)
performance scales with model size, training data size, or amount of
computational resources. Motivated by the fact that neural quantum states (NQS)
has increasingly adopted LLM-based components, we seek to understand NQS
scaling laws, thereby shedding light on the scalability and optimal
performance--resource trade-offs of NQS ansatze. In particular, we identify
scaling laws that predict the performance, as measured by absolute error and
V-score, for transformer-based NQS as a function of problem size in
second-quantized quantum chemistry applications. By performing analogous
compute-constrained optimization of the obtained parametric curves, we find
that the relationship between model size and training time is highly dependent
on loss metric and ansatz, and does not follow the approximately linear
relationship found for language models.

</details>


### [78] [ZTree: A Subgroup Identification Based Decision Tree Learning Framework](https://arxiv.org/abs/2509.12688)
*Eric Cheng,Jie Cheng*

Main category: cs.LG

TL;DR: ZTree是一个基于统计假设检验的新型决策树学习框架，用统计显著的子群识别替代传统的纯度分裂方法，通过交叉验证控制多重检验，简化参数调优并提升小数据场景性能。


<details>
  <summary>Details</summary>
Motivation: 传统CART决策树基于纯度分裂缺乏统计严谨性，需要复杂的剪枝过程和参数调优。ZTree旨在提供统计原理驱动的分裂准则，简化模型复杂度控制并提升解释性。

Method: 在每个节点应用假设检验（z检验、t检验、Mann-Whitney U、log-rank等）评估候选子群与补集的统计差异，使用交叉验证方法处理多重检验问题，仅需一个z阈值参数控制树复杂度。

Result: 在5个大规模UCI数据集上的实验表明，ZTree在小数据场景下表现优异，相比CART能生成更简单的树而不牺牲性能。

Conclusion: ZTree提供了基于假设检验的统计严谨决策树框架，通过简化参数调优和避免剪枝需求，实现了高效灵活的树学习方案。

Abstract: Decision trees are a commonly used class of machine learning models valued
for their interpretability and versatility, capable of both classification and
regression. We propose ZTree, a novel decision tree learning framework that
replaces CART's traditional purity based splitting with statistically
principled subgroup identification. At each node, ZTree applies hypothesis
testing (e.g., z-tests, t-tests, Mann-Whitney U, log-rank) to assess whether a
candidate subgroup differs meaningfully from the complement. To adjust for the
complication of multiple testing, we employ a cross-validation-based approach
to determine if further node splitting is needed. This robust stopping
criterion eliminates the need for post-pruning and makes the test threshold
(z-threshold) the only parameter for controlling tree complexity. Because of
the simplicity of the tree growing procedure, once a detailed tree is learned
using the most lenient z-threshold, all simpler trees can be derived by simply
removing nodes that do not meet the larger z-thresholds. This makes parameter
tuning intuitive and efficient. Furthermore, this z-threshold is essentially a
p-value, allowing users to easily plug in appropriate statistical tests into
our framework without adjusting the range of parameter search. Empirical
evaluation on five large-scale UCI datasets demonstrates that ZTree
consistently delivers strong performance, especially at low data regimes.
Compared to CART, ZTree also tends to grow simpler trees without sacrificing
performance. ZTree introduces a statistically grounded alternative to
traditional decision tree splitting by leveraging hypothesis testing and a
cross-validation approach to multiple testing correction, resulting in an
efficient and flexible framework.

</details>


### [79] [Soft Graph Transformer for MIMO Detection](https://arxiv.org/abs/2509.12694)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: 提出了Soft Graph Transformer (SGT)用于MIMO检测，结合图注意力机制和消息传递，支持软输入软输出，接近最大似然性能


<details>
  <summary>Details</summary>
Motivation: 传统最大似然检测计算复杂度高，消息传递算法依赖大系统渐近性假设，现有Transformer检测器无法利用MIMO因子图结构和解码器软信息

Method: 将消息传递直接集成到图感知注意力机制中，通过软输入嵌入支持解码器信息更新

Result: 作为独立检测器，SGT接近最大似然性能，超越现有Transformer方法

Conclusion: SGT设计实现了有效的软输出生成，同时保持计算效率，适用于迭代检测解码应用

Abstract: We propose the Soft Graph Transformer (SGT), a Soft-Input-Soft-Output neural
architecture tailored for MIMO detection. While Maximum Likelihood (ML)
detection achieves optimal accuracy, its prohibitive exponential complexity
renders it impractical for real-world systems. Conventional message passing
algorithms offer tractable alternatives but rely on large-system asymptotics
and random matrix assumptions, both of which break down under practical
implementations. Prior Transformer-based detectors, on the other hand, fail to
incorporate the MIMO factor graph structure and cannot utilize decoder-side
soft information, limiting their standalone performance and their applicability
in iterative detection-decoding (IDD). To overcome these limitations, SGT
integrates message passing directly into a graph-aware attention mechanism and
supports decoder-informed updates through soft-input embeddings. This design
enables effective soft-output generation while preserving computational
efficiency. As a standalone detector, SGT closely approaches ML performance and
surpasses prior Transformer-based approaches.

</details>


### [80] [Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach](https://arxiv.org/abs/2509.12697)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang*

Main category: cs.LG

TL;DR: 提出了一种双层个性化联邦学习框架，用于在基础模型上进行联邦微调，通过客户端个性化微调和基于任务向量的服务器端个性化聚合来解决小规模用户群体的个性化与联邦学习之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 联邦基础模型需要在小规模新用户或专业场景中进行微调，这些场景数据有限，个性化与联邦学习之间的权衡变得更为敏感。

Method: 采用双层个性化框架：客户端使用私有数据进行个性化微调，服务器端使用基于任务向量相似度的用户分组进行个性化聚合，以减少非IID数据中不相关或兴趣冲突客户的干扰。

Result: 在基准数据集上的广泛实验分析证明了所提算法的有效性。

Conclusion: 该框架能够有效获取群体层面的个性化信息，同时减轻非IID数据带来的干扰，为小规模用户群体的联邦基础模型微调提供了可行的解决方案。

Abstract: Federated foundation models represent a new paradigm to jointly fine-tune
pre-trained foundation models across clients. It is still a challenge to
fine-tune foundation models for a small group of new users or specialized
scenarios, which typically involve limited data compared to the large-scale
data used in pre-training. In this context, the trade-off between
personalization and federation becomes more sensitive. To tackle these, we
proposed a bi-level personalization framework for federated fine-tuning on
foundation models. Specifically, we conduct personalized fine-tuning on the
client-level using its private data, and then conduct a personalized
aggregation on the server-level using similar users measured by client-specific
task vectors. Given the personalization information gained from client-level
fine-tuning, the server-level personalized aggregation can gain group-wise
personalization information while mitigating the disturbance of irrelevant or
interest-conflict clients with non-IID data. The effectiveness of the proposed
algorithm has been demonstrated by extensive experimental analysis in benchmark
datasets.

</details>


### [81] [NORA: A Nephrology-Oriented Representation Learning Approach Towards Chronic Kidney Disease Classification](https://arxiv.org/abs/2509.12704)
*Mohammad Abdul Hafeez Khan,Twisha Bhattacharyya,Omar Khan,Noorah Khan,Alina Aziz Fatima Khan,Mohammed Qutub Khan,Sujoy Ghosh Hajra*

Main category: cs.LG

TL;DR: 提出NORA方法，结合监督对比学习和随机森林分类器，利用常规非肾脏临床变量进行慢性肾病分类，在早期检测方面表现优异


<details>
  <summary>Details</summary>
Motivation: 慢性肾病早期检测困难，特别是在门诊环境中缺乏实验室肾脏生物标志物，需要探索常规收集的非肾脏临床变量的预测潜力

Method: NORA方法：监督对比学习+非线性随机森林分类器，首先从表格化电子健康记录数据中提取判别性患者表示，然后用于下游CKD分类

Result: NORA提高了类别可分性和整体分类性能，特别增强了早期CKD的F1分数，在UCI CKD数据集上也表现出良好的泛化能力

Conclusion: NORA方法能有效利用常规临床数据进行CKD风险分层，在不同患者群体中都具有良好的预测效果

Abstract: Chronic Kidney Disease (CKD) affects millions of people worldwide, yet its
early detection remains challenging, especially in outpatient settings where
laboratory-based renal biomarkers are often unavailable. In this work, we
investigate the predictive potential of routinely collected non-renal clinical
variables for CKD classification, including sociodemographic factors, comorbid
conditions, and urinalysis findings. We introduce the Nephrology-Oriented
Representation leArning (NORA) approach, which combines supervised contrastive
learning with a nonlinear Random Forest classifier. NORA first derives
discriminative patient representations from tabular EHR data, which are then
used for downstream CKD classification. We evaluated NORA on a clinic-based EHR
dataset from Riverside Nephrology Physicians. Our results demonstrated that
NORA improves class separability and overall classification performance,
particularly enhancing the F1-score for early-stage CKD. Additionally, we
assessed the generalizability of NORA on the UCI CKD dataset, demonstrating its
effectiveness for CKD risk stratification across distinct patient cohorts.

</details>


### [82] [Spatio-temporal DeepKriging in PyTorch: A Supplementary Application to Precipitation Data for Interpolation and Probabilistic Forecasting](https://arxiv.org/abs/2509.12708)
*Pratik Nag*

Main category: cs.LG

TL;DR: 提出了一个基于PyTorch的时空深度克里金(STDK)框架，用于欧洲降水数据的插值和预测应用，能够处理时空不规则性并生成高分辨率插值及多步预测。


<details>
  <summary>Details</summary>
Motivation: 需要处理欧洲降水数据中的时空不规则性，同时实现高精度插值和多步预测，为气候数据分析提供有效工具。

Method: 使用PyTorch平台实现时空深度克里金(STDK)框架，开发了可复现的代码模块，包括插值和预测两个独立的PyTorch实现。

Result: 通过对日降水测量数据的广泛评估，证明了该方法在预测性能和鲁棒性方面的有效性。

Conclusion: 该STDK框架能够有效处理时空不规则数据，提供高质量的插值和预测结果，其开源代码便于在类似气候数据集上的广泛应用。

Abstract: A detailed analysis of precipitation data over Europe is presented, with a
focus on interpolation and forecasting applications. A Spatio-temporal
DeepKriging (STDK) framework has been implemented using the PyTorch platform to
achieve these objectives. The proposed model is capable of handling
spatio-temporal irregularities while generating high-resolution interpolations
and multi-step forecasts. Reproducible code modules have been developed as
standalone PyTorch implementations for the
interpolation\footnote[2]{Interpolation -
https://github.com/pratiknag/Spatio-temporalDeepKriging-Pytorch.git} and
forecasting\footnote[3]{Forecasting -
https://github.com/pratiknag/pytorch-convlstm.git}, facilitating broader
application to similar climate datasets. The effectiveness of this approach is
demonstrated through extensive evaluation on daily precipitation measurements,
highlighting predictive performance and robustness.

</details>


### [83] [Unbiased Online Curvature Approximation for Regularized Graph Continual Learning](https://arxiv.org/abs/2509.12727)
*Jie Yin,Ke Sun,Han Wu*

Main category: cs.LG

TL;DR: 提出了一个基于Fisher信息矩阵的图持续学习正则化框架，并开发了一种在线曲率近似方法，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决图持续学习中灾难性遗忘问题，特别是在无回放、类增量设置下，现有方法如EWC使用对角近似的Fisher信息矩阵存在局限性

Method: 建立基于Fisher信息矩阵曲率参数空间的一般正则化框架，提出无偏在线曲率近似方法，直接估计正则化项而不显式计算和存储Fisher信息矩阵

Result: 在三个图数据集上的大量实验表明，该方法显著优于现有的基于正则化的方法，在稳定性（保留旧知识）和可塑性（获取新知识）之间实现了更好的平衡

Conclusion: 所提出的在线曲率近似方法能够更好地捕捉学习新任务时的损失景观，同时保留从先前任务中学到的知识，为图持续学习提供了有效的解决方案

Abstract: Graph continual learning (GCL) aims to learn from a continuous sequence of
graph-based tasks. Regularization methods are vital for preventing catastrophic
forgetting in GCL, particularly in the challenging replay-free,
class-incremental setting, where each task consists of a set of unique classes.
In this work, we first establish a general regularization framework for GCL
based on the curved parameter space induced by the Fisher information matrix
(FIM). We show that the dominant Elastic Weight Consolidation (EWC) and its
variants are a special case within this framework, using a diagonal
approximation of the empirical FIM based on parameters from previous tasks. To
overcome their limitations, we propose a new unbiased online curvature
approximation of the full FIM based on the model's current learning state. Our
method directly estimates the regularization term in an online manner without
explicitly evaluating and storing the FIM itself. This enables the model to
better capture the loss landscape during learning new tasks while retaining the
knowledge learned from previous tasks. Extensive experiments on three graph
datasets demonstrate that our method significantly outperforms existing
regularization-based methods, achieving a superior trade-off between stability
(retaining old knowledge) and plasticity (acquiring new knowledge).

</details>


### [84] [A Graph Machine Learning Approach for Detecting Topological Patterns in Transactional Graphs](https://arxiv.org/abs/2509.12730)
*Francesco Zola,Jon Ander Medina,Andrea Venturi,Amaia Gil,Raul Orduna*

Main category: cs.LG

TL;DR: 提出了一种结合图机器学习和网络分析的方法，通过四步预处理框架和Graph Autoencoders来检测金融交易图中的拓扑模式，以改进传统规则系统对复杂金融犯罪的检测能力。


<details>
  <summary>Details</summary>
Motivation: 数字生态系统的发展使金融部门面临日益复杂的犯罪手段，传统规则系统缺乏适应性，需要分析参与者交互来发现可疑活动和提取作案手法。

Method: 四步预处理框架：提取图结构、考虑数据时间性管理大节点集、社区检测、自动标注生成弱标签；然后使用三种不同的Graph Autoencoder变体进行拓扑模式识别。

Result: 初步结果显示这种基于模式、拓扑驱动的方法能有效检测复杂金融犯罪方案。

Conclusion: 该方法为传统规则检测系统提供了有前景的替代方案，能够更好地适应不断演变的金融犯罪模式。

Abstract: The rise of digital ecosystems has exposed the financial sector to evolving
abuse and criminal tactics that share operational knowledge and techniques both
within and across different environments (fiat-based, crypto-assets, etc.).
Traditional rule-based systems lack the adaptability needed to detect
sophisticated or coordinated criminal behaviors (patterns), highlighting the
need for strategies that analyze actors' interactions to uncover suspicious
activities and extract their modus operandi. For this reason, in this work, we
propose an approach that integrates graph machine learning and network analysis
to improve the detection of well-known topological patterns within
transactional graphs. However, a key challenge lies in the limitations of
traditional financial datasets, which often provide sparse, unlabeled
information that is difficult to use for graph-based pattern analysis.
Therefore, we firstly propose a four-step preprocessing framework that involves
(i) extracting graph structures, (ii) considering data temporality to manage
large node sets, (iii) detecting communities within, and (iv) applying
automatic labeling strategies to generate weak ground-truth labels. Then, once
the data is processed, Graph Autoencoders are implemented to distinguish among
the well-known topological patterns. Specifically, three different GAE variants
are implemented and compared in this analysis. Preliminary results show that
this pattern-focused, topology-driven method is effective for detecting complex
financial crime schemes, offering a promising alternative to conventional
rule-based detection systems.

</details>


### [85] [A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression](https://arxiv.org/abs/2509.12732)
*Rishab Parthasarathy,Achintya Bhowmik*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于人工智能的端到端框架，通过时间序列机器学习模型和递归神经网络预测癌症严重程度和突变进展，并推荐治疗方案，避免依赖传统的耐耗时的湿实验数据。


<details>
  <summary>Details</summary>
Motivation: 癌症仍是美国第二大死因，年死亡人数超过60万。虽然通路分析领域很有前景，但仍依赖人工获取的湿实验数据，获取耐时。需要一种更高效、成本效益更好的分析框架。

Method: 使用TCGA数据库中的突变序列，通过新的预处理算法按突变频率筛选关键突变。将数据输入递归神经网络(RNN)预测癌症严重程度，然后概率性地结合RNN预测结果、预处理算法和多个药物目标数据库来预测未来突变和推荐治疗方案。

Result: 框架达到了稳健的结果，ROC曲线准确率超过60%，与现有癌症诊断方法相似。预处理步骤在隐称关键突变方面发挥了重要作用，证明每个癌症阶段可能包含数百个关键驱动突变。生成了基于预测基因频率的热力图，突出显示了各种癌症中的关键突变。

Conclusion: 这是首个提出高效、成本效益好的端到端框架，能够预测癌症进展并提供治疗建议，而无需依赖费时费力的湿实验工作。该方法为癌症诊断和治疗提供了一种有前景的新途径。

Abstract: Despite significant medical advancements, cancer remains the second leading
cause of death, with over 600,000 deaths per year in the US. One emerging
field, pathway analysis, is promising but still relies on manually derived wet
lab data, which is time-consuming to acquire. This work proposes an efficient,
effective end-to-end framework for Artificial Intelligence (AI) based pathway
analysis that predicts both cancer severity and mutation progression, thus
recommending possible treatments. The proposed technique involves a novel
combination of time-series machine learning models and pathway analysis. First,
mutation sequences were isolated from The Cancer Genome Atlas (TCGA) Database.
Then, a novel preprocessing algorithm was used to filter key mutations by
mutation frequency. This data was fed into a Recurrent Neural Network (RNN)
that predicted cancer severity. Then, the model probabilistically used the RNN
predictions, information from the preprocessing algorithm, and multiple
drug-target databases to predict future mutations and recommend possible
treatments. This framework achieved robust results and Receiver Operating
Characteristic (ROC) curves (a key statistical metric) with accuracies greater
than 60%, similar to existing cancer diagnostics. In addition, preprocessing
played an instrumental role in isolating important mutations, demonstrating
that each cancer stage studied may contain on the order of a few-hundred key
driver mutations, consistent with current research. Heatmaps based on predicted
gene frequency were also generated, highlighting key mutations in each cancer.
Overall, this work is the first to propose an efficient, cost-effective
end-to-end framework for projecting cancer progression and providing possible
treatments without relying on expensive, time-consuming wet lab work.

</details>


### [86] [Similarity-Distance-Magnitude Activations](https://arxiv.org/abs/2509.12760)
*Allen Schmaltz*

Main category: cs.LG

TL;DR: 提出了一种改进的softmax激活函数SDM，通过增加相似性和距离感知来提高鲁棒性和可解释性，特别适用于选择性分类任务。


<details>
  <summary>Details</summary>
Motivation: 标准softmax函数在处理协变量偏移和分布外输入时缺乏鲁棒性，且可解释性不足。需要一种能够同时考虑输出幅度、相似性和距离感知的激活函数。

Method: 在softmax基础上引入相似性感知（正确预测的深度匹配）和距离训练分布感知，形成相似性-距离-幅度（SDM）激活函数。

Result: SDM激活函数比softmax对协变量偏移和分布外输入更鲁棒，提供基于范例的可解释性，并能通过类别经验CDF划分来保护选择性分类中的类别召回率。

Conclusion: SDM激活函数在选择性分类中优于softmax，即使考虑后校准方法，仍具有更好的鲁棒性和可解释性。

Abstract: We introduce a more robust and interpretable formulation of the standard
softmax activation function commonly used with neural networks by adding
Similarity (i.e., correctly predicted depth-matches into training) awareness
and Distance-to-training-distribution awareness to the existing output
Magnitude (i.e., decision-boundary) awareness. When used as the final-layer
activation with language models, the resulting Similarity-Distance-Magnitude
(SDM) activation function is more robust than the softmax function to
co-variate shifts and out-of-distribution inputs in high-probability regions,
and provides interpretability-by-exemplar via dense matching. Complementing the
prediction-conditional estimates, the SDM activation enables a partitioning of
the class-wise empirical CDFs to guard against low class-wise recall among
selective classifications. These properties make it preferable for selective
classification, even when considering post-hoc calibration methods over the
softmax.

</details>


### [87] [EmbeddedML: A New Optimized and Fast Machine Learning Library](https://arxiv.org/abs/2509.12774)
*Halil Hüseyin Çalışkan,Talha Koruk*

Main category: cs.LG

TL;DR: EmbeddedML是一个经过数学重写和优化的机器学习库，相比scikit-learn在训练速度上有显著提升，在回归和分类任务中保持准确率的同时大幅减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习库如scikit-learn在处理大型数据集时训练时间过长，需要优化训练效率。

Method: 通过数学重写Multiple Linear Regression、Logistic Regression和SVM等算法，进行训练时间优化和数学增强。

Result: Multiple Linear Regression速度提升约数倍；SVM在小数据集上训练时间减少2倍，大数据集减少800倍；Logistic Regression训练时间减少4倍，且准确率无损失。

Conclusion: EmbeddedML提供了经过数学优化和重写的回归、分类、聚类和降维算法，能显著减少训练时间，是高效的机器学习解决方案。

Abstract: Machine learning models and libraries can train datasets of different sizes
and perform prediction and classification operations, but machine learning
models and libraries cause slow and long training times on large datasets. This
article introduces EmbeddedML, a training-time-optimized and mathematically
enhanced machine learning library. The speed was increased by approximately
times compared to scikit-learn without any loss in terms of accuracy in
regression models such as Multiple Linear Regression. Logistic Regression and
Support Vector Machines (SVM) algorithms have been mathematically rewritten to
reduce training time and increase accuracy in classification models. With the
applied mathematical improvements, training time has been reduced by
approximately 2 times for SVM on small datasets and by around 800 times on
large datasets, and by approximately 4 times for Logistic Regression, compared
to the scikit-learn implementation. In summary, the EmbeddedML library offers
regression, classification, clustering, and dimensionality reduction algorithms
that are mathematically rewritten and optimized to reduce training time.

</details>


### [88] [Energy-Efficient Quantized Federated Learning for Resource-constrained IoT devices](https://arxiv.org/abs/2509.12814)
*Wilfrid Sougrinoma Compaoré,Yaya Etiabi,El Mehdi Amhoud,Mohamad Assaad*

Main category: cs.LG

TL;DR: 提出了一种面向物联网的联邦学习框架，通过有限块长传输、模型量化和误差感知聚合机制，显著提升能效和通信可靠性，在保持模型精度的同时降低能耗75%。


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源受限，存在能量有限、通信不可靠和无限块长传输不切实际等挑战，需要设计高效的联邦学习方案。

Method: 集成有限块长传输、模型量化和误差感知聚合机制，并优化上行传输功率以平衡节能和模型性能。

Result: 仿真结果显示，相比标准联邦学习模型，能耗降低高达75%，同时保持稳健的模型精度。

Conclusion: 该框架为实际物联网部署中高效可靠的联邦学习实现提供了可行解决方案，推动了联邦学习在资源受限物联网环境中的应用。

Abstract: Federated Learning (FL) has emerged as a promising paradigm for enabling
collaborative machine learning while preserving data privacy, making it
particularly suitable for Internet of Things (IoT) environments. However,
resource-constrained IoT devices face significant challenges due to limited
energy,unreliable communication channels, and the impracticality of assuming
infinite blocklength transmission. This paper proposes a federated learning
framework for IoT networks that integrates finite blocklength transmission,
model quantization, and an error-aware aggregation mechanism to enhance energy
efficiency and communication reliability. The framework also optimizes uplink
transmission power to balance energy savings and model performance. Simulation
results demonstrate that the proposed approach significantly reduces energy
consumption by up to 75\% compared to a standard FL model, while maintaining
robust model accuracy, making it a viable solution for FL in real-world IoT
scenarios with constrained resources. This work paves the way for efficient and
reliable FL implementations in practical IoT deployments. Index Terms:
Federated learning, IoT, finite blocklength, quantization, energy efficiency.

</details>


### [89] [Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?](https://arxiv.org/abs/2509.12833)
*Hannah Markgraf,Shamburaj Sawant,Hanna Krasowski,Lukas Schäfer,Sebastien Gros,Matthias Althoff*

Main category: cs.LG

TL;DR: 本文对基于投影的安全过滤器在强化学习中的两种集成策略（SE-RL和SP-RL）进行了理论比较，重点分析了动作别名现象对策略梯度的影响，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于投影的安全过滤器在安全关键场景中广泛应用，但对SE-RL和SP-RL两种集成策略的差异缺乏形式化理解，需要理论分析来指导方法选择和改进。

Method: 提出了统一的actor-critic算法形式化框架，理论分析了两种方法的策略梯度估计，研究了动作别名现象的影响，并比较了包括新颖惩罚基改进在内的缓解策略。

Result: 实证结果表明动作别名对SP-RL的损害更大，但通过适当的改进策略，SP-RL可以在各种环境中匹配或超越改进的SE-RL。

Conclusion: 研究结果为根据任务特性选择和优化基于投影的安全RL方法提供了可操作的见解，SP-RL在适当改进后具有竞争优势。

Abstract: Projection-based safety filters, which modify unsafe actions by mapping them
to the closest safe alternative, are widely used to enforce safety constraints
in reinforcement learning (RL). Two integration strategies are commonly
considered: Safe environment RL (SE-RL), where the safeguard is treated as part
of the environment, and safe policy RL (SP-RL), where it is embedded within the
policy through differentiable optimization layers. Despite their practical
relevance in safety-critical settings, a formal understanding of their
differences is lacking. In this work, we present a theoretical comparison of
SE-RL and SP-RL. We identify a key distinction in how each approach is affected
by action aliasing, a phenomenon in which multiple unsafe actions are projected
to the same safe action, causing information loss in the policy gradients. In
SE-RL, this effect is implicitly approximated by the critic, while in SP-RL, it
manifests directly as rank-deficient Jacobians during backpropagation through
the safeguard. Our contributions are threefold: (i) a unified formalization of
SE-RL and SP-RL in the context of actor-critic algorithms, (ii) a theoretical
analysis of their respective policy gradient estimates, highlighting the role
of action aliasing, and (iii) a comparative study of mitigation strategies,
including a novel penalty-based improvement for SP-RL that aligns with
established SE-RL practices. Empirical results support our theoretical
predictions, showing that action aliasing is more detrimental for SP-RL than
for SE-RL. However, with appropriate improvement strategies, SP-RL can match or
outperform improved SE-RL across a range of environments. These findings
provide actionable insights for choosing and refining projection-based safe RL
methods based on task characteristics.

</details>


### [90] [Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use](https://arxiv.org/abs/2509.12867)
*Yabo Zhang,Yihan Zeng,Qingyun Li,Zhen Hu,Kavin Han,Wangmeng Zuo*

Main category: cs.LG

TL;DR: Tool-R1是一个基于强化学习的框架，通过生成可执行Python代码使大语言模型能够进行通用、组合式和多步骤的工具使用，在GAIA基准测试中准确性和鲁棒性显著提升


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理需要最新知识、精确操作或专业工具使用的现实任务时的局限性

Method: 提出强化学习框架，支持用户自定义工具和标准库集成，采用基于结果的奖励函数（结合LLM答案判断和代码执行成功），使用动态样本队列缓存高质量轨迹以提高训练效率

Result: 在GAIA基准测试中准确性和鲁棒性显著提升，比强基线提高约10%，在复杂多步骤任务上提升更大

Conclusion: Tool-R1具有在现实应用中实现可靠高效工具增强推理的潜力

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning, yet they remain limited when tackling
real-world tasks that require up-to-date knowledge, precise operations, or
specialized tool use. To address this, we propose Tool-R1, a reinforcement
learning framework that enables LLMs to perform general, compositional, and
multi-step tool use by generating executable Python code. Tool-R1 supports
integration of user-defined tools and standard libraries, with variable sharing
across steps to construct coherent workflows. An outcome-based reward function,
combining LLM-based answer judgment and code execution success, guides policy
optimization. To improve training efficiency, we maintain a dynamic sample
queue to cache and reuse high-quality trajectories, reducing the overhead of
costly online sampling. Experiments on the GAIA benchmark show that Tool-R1
substantially improves both accuracy and robustness, achieving about 10\% gain
over strong baselines, with larger improvements on complex multi-step tasks.
These results highlight the potential of Tool-R1 for enabling reliable and
efficient tool-augmented reasoning in real-world applications. Our code will be
available at https://github.com/YBYBZhang/Tool-R1.

</details>


### [91] [TimeCluster with PCA is Equivalent to Subspace Identification of Linear Dynamical Systems](https://arxiv.org/abs/2509.12895)
*Christian L. Hines,Samuel Spillard,Daniel P. Martin*

Main category: cs.LG

TL;DR: TimeCluster视觉分析技术与经典线性子空间识别方法在数学上等价，两者通过滑动窗口矩阵和PCA/SVD提取相同的低维线性子空间


<details>
  <summary>Details</summary>
Motivation: 探索TimeCluster视觉分析技术与传统子空间系统识别方法之间的数学等价性，以建立跨学科的理论联系

Method: 通过理论分析证明TimeCluster的滑动窗口矩阵形成Hankel矩阵，应用PCA（通过SVD）可恢复与子空间识别相同的主方向

Result: 在合成和真实动态信号上的实验证实两种嵌入方法完全一致，聚类坐标重合

Conclusion: 这种等价性为未来研究开辟了新机会，包括状态空间预测、流式处理、外部输入整合以及噪声数据中的趋势可视化

Abstract: TimeCluster is a visual analytics technique for discovering structure in long
multivariate time series by projecting overlapping windows of data into a
low-dimensional space. We show that, when Principal Component Analysis (PCA) is
chosen as the dimensionality reduction technique, this procedure is
mathematically equivalent to classical linear subspace identification
(block-Hankel matrix plus Singular Vector Decomposition (SVD)). In both
approaches, the same low-dimensional linear subspace is extracted from the time
series data. We first review the TimeCluster method and the theory of subspace
system identification. Then we show that forming the sliding-window matrix of a
time series yields a Hankel matrix, so applying PCA (via SVD) to this matrix
recovers the same principal directions as subspace identification. Thus the
cluster coordinates from TimeCluster coincide with the subspace identification
methods. We present experiments on synthetic and real dynamical signals
confirming that the two embeddings coincide. Finally, we explore and discuss
future opportunities enabled by this equivalence, including forecasting from
the identified state space, streaming/online extensions, incorporating and
visualising external inputs and robust techniques for displaying underlying
trends in corrupted data.

</details>


### [92] [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)
*Denis Janiak,Julia Moska,Dawid Motyka,Karolina Seweryn,Paweł Walkowiak,Bartosz Żuk,Arkadiusz Janz*

Main category: cs.LG

TL;DR: 提出了一个统一的评估框架，比较PPO、DPO、ORPO、KTO等LLM对齐方法在事实性、安全性、简洁性、主动性和多样性五个维度上的表现，发现不同方法在不同维度各有优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单一技术或特定维度，缺乏对LLM对齐方法内在权衡的整体评估，需要建立一个全面的评估框架来指导更平衡可靠的LLM开发。

Method: 使用统一的评估框架，在分布内和分布外数据集上比较PPO、DPO、ORPO、KTO等对齐方法，采用经过人工研究验证的LLM-as-Judge提示进行多维度评估。

Result: DPO和KTO在事实准确性方面表现最佳，PPO和DPO在安全性方面领先，PPO在简洁性与主动性之间达到最佳平衡。

Conclusion: 研究揭示了常见对齐方法的内在权衡关系，为开发更平衡可靠的LLM提供了重要指导，不同方法在不同应用场景下各有优势。

Abstract: Large language models (LLMs) require careful alignment to balance competing
objectives - factuality, safety, conciseness, proactivity, and diversity.
Existing studies focus on individual techniques or specific dimensions, lacking
a holistic assessment of the inherent trade-offs. We propose a unified
evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)
across these five axes, using both in-distribution and out-of-distribution
datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human
studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead
in safety, and PPO best balances conciseness with proactivity. Our findings
provide insights into trade-offs of common alignment methods, guiding the
development of more balanced and reliable LLMs.

</details>


### [93] [Sy-FAR: Symmetry-based Fair Adversarial Robustness](https://arxiv.org/abs/2509.12939)
*Haneen Najjar,Eyal Ronen,Mahmood Sharif*

Main category: cs.LG

TL;DR: 该论文提出了Sy-FAR方法，通过追求对称性而非完美公平性来提升对抗性鲁棒性的公平性，在面部识别等安全关键任务中取得了显著效果


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性鲁棒性方法往往导致不公平的鲁棒性——某些类别或群体更容易受到攻击。在现实世界的公平关键任务中，实现完美公平往往不可行，而对称性（攻击从类别i到j的成功率与从j到i相同）是更可行的目标

Method: 开发了Sy-FAR技术，通过鼓励对称性同时优化对抗性鲁棒性。该方法在五个数据集上使用三种模型架构进行了广泛评估，包括针对定向和非定向的现实攻击

Result: Sy-FAR相比最先进方法显著提高了公平对抗性鲁棒性，运行速度更快且结果更一致。同时还能缓解另一种不公平性——对抗样本容易被分类到的目标类别的脆弱性显著降低

Conclusion: 在现实世界的公平关键任务中，追求对称性比追求完美公平性更可行且有效。Sy-FAR方法在提升对抗性鲁棒性的同时改善了公平性，为安全关键的机器学习系统提供了更好的保护

Abstract: Security-critical machine-learning (ML) systems, such as face-recognition
systems, are susceptible to adversarial examples, including real-world
physically realizable attacks. Various means to boost ML's adversarial
robustness have been proposed; however, they typically induce unfair
robustness: It is often easier to attack from certain classes or groups than
from others. Several techniques have been developed to improve adversarial
robustness while seeking perfect fairness between classes. Yet, prior work has
focused on settings where security and fairness are less critical. Our insight
is that achieving perfect parity in realistic fairness-critical tasks, such as
face recognition, is often infeasible -- some classes may be highly similar,
leading to more misclassifications between them. Instead, we suggest that
seeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful
as from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable
because class resemblance is a symmetric relation in most domains.
Additionally, as we prove theoretically, symmetry between individuals induces
symmetry between any set of sub-groups, in contrast to other fairness notions
where group-fairness is often elusive. We develop Sy-FAR, a technique to
encourage symmetry while also optimizing adversarial robustness and extensively
evaluate it using five datasets, with three model architectures, including
against targeted and untargeted realistic attacks. The results show Sy-FAR
significantly improves fair adversarial robustness compared to state-of-the-art
methods. Moreover, we find that Sy-FAR is faster and more consistent across
runs. Notably, Sy-FAR also ameliorates another type of unfairness we discover
in this work -- target classes that adversarial examples are likely to be
classified into become significantly less vulnerable after inducing symmetry.

</details>


### [94] [BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning](https://arxiv.org/abs/2509.12964)
*Honghong Zeng,Jiong Lou,Zhe Wang,Hefeng Zhou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.LG

TL;DR: 本文提出了BAPFL，首个专门针对原型联邦学习(PFL)框架的后门攻击方法，通过原型投毒策略和触发器优化机制，在保持主任务精度的同时显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 原型联邦学习(PFL)因其独特的原型学习机制和数据异构性，对现有后门攻击具有天然抵抗性，但其安全性尚未得到充分探索。

Method: BAPFL结合原型投毒策略和触发器优化机制：原型投毒策略操纵全局原型轨迹误导良性客户端；触发器优化机制为每个目标标签学习独特隐蔽的触发器。

Result: 在多个数据集和PFL变体上的实验表明，BAPFL相比传统后门攻击实现了35%-75%的攻击成功率提升，同时保持了主任务精度。

Conclusion: BAPFL证明了PFL框架存在安全漏洞，该方法具有高效性、隐蔽性和适应性，为PFL安全性研究提供了重要启示。

Abstract: Prototype-based federated learning (PFL) has emerged as a promising paradigm
to address data heterogeneity problems in federated learning, as it leverages
mean feature vectors as prototypes to enhance model generalization. However,
its robustness against backdoor attacks remains largely unexplored. In this
paper, we identify that PFL is inherently resistant to existing backdoor
attacks due to its unique prototype learning mechanism and local data
heterogeneity. To further explore the security of PFL, we propose BAPFL, the
first backdoor attack method specifically designed for PFL frameworks. BAPFL
integrates a prototype poisoning strategy with a trigger optimization
mechanism. The prototype poisoning strategy manipulates the trajectories of
global prototypes to mislead the prototype training of benign clients, pushing
their local prototypes of clean samples away from the prototypes of
trigger-embedded samples. Meanwhile, the trigger optimization mechanism learns
a unique and stealthy trigger for each potential target label, and guides the
prototypes of trigger-embedded samples to align closely with the global
prototype of the target label. Experimental results across multiple datasets
and PFL variants demonstrate that BAPFL achieves a 35\%-75\% improvement in
attack success rate compared to traditional backdoor attacks, while preserving
main task accuracy. These results highlight the effectiveness, stealthiness,
and adaptability of BAPFL in PFL.

</details>


### [95] [Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder](https://arxiv.org/abs/2509.12991)
*Ya Zhou,Yujie Yang,Xiaohan Fan,Wei Zhao*

Main category: cs.LG

TL;DR: 提出了一种简单有效的后训练方法，显著提升了ECG基础模型ECGFounder在PTB-XL基准测试上的性能表现，特别是在样本效率方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的ECG基础模型虽然在大规模ECG数据上进行了预训练，但在临床应用中仍存在性能差距，主要原因是缺乏有效的后训练策略。

Method: 提出了一种后训练方法，包含随机深度和预览线性探测等关键组件，用于增强ECGFounder模型的性能。

Result: 在PTB-XL基准测试中，相比基线微调策略，宏观AUROC提升1.2%-3.3%，宏观AUPRC提升5.3%-20.9%；使用仅10%训练数据时，宏观AUROC提升9.1%，宏观AUPRC提升34.9%。

Conclusion: 后训练策略具有显著提升ECG基础模型性能的潜力，该方法更加稳定且样本效率更高，有望推动ECG领域基础模型的持续发展。

Abstract: ECG foundation models are increasingly popular due to their adaptability
across various tasks. However, their clinical applicability is often limited by
performance gaps compared to task-specific models, even after pre-training on
large ECG datasets and fine-tuning on target data. This limitation is likely
due to the lack of an effective post-training strategy. In this paper, we
propose a simple yet effective post-training approach to enhance ECGFounder, a
state-of-the-art ECG foundation model pre-trained on over 7 million ECG
recordings. Experiments on the PTB-XL benchmark show that our approach improves
the baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% in
macro AUPRC. Additionally, our method outperforms several recent
state-of-the-art approaches, including task-specific and advanced
architectures. Further evaluation reveals that our method is more stable and
sample-efficient compared to the baseline, achieving a 9.1% improvement in
macro AUROC and a 34.9% improvement in macro AUPRC using just 10% of the
training data. Ablation studies identify key components, such as stochastic
depth and preview linear probing, that contribute to the enhanced performance.
These findings underscore the potential of post-training strategies to improve
ECG foundation models, and we hope this work will contribute to the continued
development of foundation models in the ECG domain.

</details>


### [96] [Ensemble Visualization With Variational Autoencoder](https://arxiv.org/abs/2509.13000)
*Cenyang Wu,Qinhan Yu,Liang Zhou*

Main category: cs.LG

TL;DR: 提出基于变分自编码器(VAE)的数据集成可视化方法，通过潜在空间构建结构化概率表示，实现置信区间分析和密度估计


<details>
  <summary>Details</summary>
Motivation: 传统数据集成可视化方法难以有效处理高维空间特征和概率分布分析，需要一种能够将空间特征转换为结构化潜在表示的方法

Method: 使用变分自编码器(VAE)进行特征空间转换和无监督学习，将空间特征映射到遵循多元标准高斯分布的潜在空间

Result: 在天气预测集成数据上的初步结果表明该方法具有有效性和通用性，能够进行置信区间分析和生成数据集的概率分布密度估计

Conclusion: 该方法为数据集成可视化提供了一种新的结构化概率表示框架，特别适用于需要概率分析和不确定性可视化的应用场景

Abstract: We present a new method to visualize data ensembles by constructing
structured probabilistic representations in latent spaces, i.e.,
lower-dimensional representations of spatial data features. Our approach
transforms the spatial features of an ensemble into a latent space through
feature space conversion and unsupervised learning using a variational
autoencoder (VAE). The resulting latent spaces follow multivariate standard
Gaussian distributions, enabling analytical computation of confidence intervals
and density estimation of the probabilistic distribution that generates the
data ensemble. Preliminary results on a weather forecasting ensemble
demonstrate the effectiveness and versatility of our method.

</details>


### [97] [ReTrack: Data Unlearning in Diffusion Models through Redirecting the Denoising Trajectory](https://arxiv.org/abs/2509.13007)
*Qitan Shi,Cheng Jin,Jiawei Zhang,Yuantao Gu*

Main category: cs.LG

TL;DR: ReTrack是一种针对扩散模型的高效数据遗忘方法，通过重要性采样和保留主导项来构建优化损失，将去噪轨迹重定向到k近邻，在保持生成质量的同时实现有效遗忘。


<details>
  <summary>Details</summary>
Motivation: 扩散模型存在训练数据记忆化问题，带来隐私和安全风险。数据遗忘技术可以在不从头训练的情况下移除特定数据的影响，但需要更高效的方法。

Method: 使用重要性采样构建高效的微调损失函数，通过保留主导项来近似优化目标，将去噪轨迹重定向到k近邻数据点。

Result: 在MNIST T-Shirt、CelebA-HQ、CIFAR-10和Stable Diffusion等数据集上达到最先进性能，在遗忘强度和生成质量保持之间取得最佳平衡。

Conclusion: ReTrack提供了一种快速有效的扩散模型数据遗忘解决方案，解决了隐私保护问题，同时保持了模型的生成能力。

Abstract: Diffusion models excel at generating high-quality, diverse images but suffer
from training data memorization, raising critical privacy and safety concerns.
Data unlearning has emerged to mitigate this issue by removing the influence of
specific data without retraining from scratch. We propose ReTrack, a fast and
effective data unlearning method for diffusion models. ReTrack employs
importance sampling to construct a more efficient fine-tuning loss, which we
approximate by retaining only dominant terms. This yields an interpretable
objective that redirects denoising trajectories toward the $k$-nearest
neighbors, enabling efficient unlearning while preserving generative quality.
Experiments on MNIST T-Shirt, CelebA-HQ, CIFAR-10, and Stable Diffusion show
that ReTrack achieves state-of-the-art performance, striking the best trade-off
between unlearning strength and generation quality preservation.

</details>


### [98] [Spiking Vocos: An Energy-Efficient Neural Vocoder](https://arxiv.org/abs/2509.13049)
*Yukun Chen,Zhaoxi Mu,Andong Li,Peilin Li,Xinyu Yang*

Main category: cs.LG

TL;DR: 提出Spiking Vocos，一种基于SNN的超低能耗神经声码器，通过Spiking ConvNeXt模块和振幅捷径路径解决SNN信息瓶颈问题，使用自架构蒸馏策略提升性能，能耗仅为ANN版本的14.7%但性能相当。


<details>
  <summary>Details</summary>
Motivation: 传统神经声码器虽然合成速度和保真度显著提升，但高能耗限制了在计算资源受限的边缘设备上的实际部署。SNN因其事件驱动特性具有高能效优势，适合低资源场景。

Method: 基于高效Vocos框架构建SNN声码器，设计Spiking ConvNeXt模块减少MAC操作，加入振幅捷径路径保持信号动态特性，采用自架构蒸馏策略进行知识迁移，集成轻量级时序移位模块增强时域信息融合。

Result: 模型性能与ANN版本相当，UTMOS和PESQ分数分别达到3.74和3.45，同时能耗仅为ANN的14.7%。

Conclusion: Spiking Vocos成功实现了高性能与超低能耗的平衡，为边缘设备上的神经声码器部署提供了有效解决方案。

Abstract: Despite the remarkable progress in the synthesis speed and fidelity of neural
vocoders, their high energy consumption remains a critical barrier to practical
deployment on computationally restricted edge devices. Spiking Neural Networks
(SNNs), widely recognized for their high energy efficiency due to their
event-driven nature, offer a promising solution for low-resource scenarios. In
this paper, we propose Spiking Vocos, a novel spiking neural vocoder with
ultra-low energy consumption, built upon the efficient Vocos framework. To
mitigate the inherent information bottleneck in SNNs, we design a Spiking
ConvNeXt module to reduce Multiply-Accumulate (MAC) operations and incorporate
an amplitude shortcut path to preserve crucial signal dynamics. Furthermore, to
bridge the performance gap with its Artificial Neural Network (ANN)
counterpart, we introduce a self-architectural distillation strategy to
effectively transfer knowledge. A lightweight Temporal Shift Module is also
integrated to enhance the model's ability to fuse information across the
temporal dimension with negligible computational overhead. Experiments
demonstrate that our model achieves performance comparable to its ANN
counterpart, with UTMOS and PESQ scores of 3.74 and 3.45 respectively, while
consuming only 14.7% of the energy. The source code is available at
https://github.com/pymaster17/Spiking-Vocos.

</details>


### [99] [Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning in Spiking Neural Networks](https://arxiv.org/abs/2509.13053)
*Lorenzo Pes,Bojian Yin,Sander Stuijk,Federico Corradi*

Main category: cs.LG

TL;DR: 提出了Traces Propagation (TP)方法，一种前向传播、内存高效、可扩展且完全局部的学习规则，用于解决脉冲神经网络训练中的时空信用分配问题，无需辅助矩阵，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决SNN训练中BPTT方法不符合生物神经系统时空局部性、计算内存需求高的问题，以及现有局部学习规则需要辅助矩阵导致内存开销大和可扩展性差的问题。

Method: 结合资格迹和分层对比损失，提出Traces Propagation (TP)方法，无需辅助分层矩阵，实现完全局部的前向传播学习。

Result: 在NMNIST和SHD数据集上优于其他完全局部学习规则，在DVS-GESTURE和DVS-CIFAR10等复杂数据集上表现竞争性，可扩展到VGG-9等深层架构，内存扩展性优于现有方法。

Conclusion: TP方法为边缘设备上的高效学习提供了可行方案，特别适合实际微调任务如关键词识别，展示了在嵌入式设备上应用的潜力。

Abstract: Spiking Neural Networks (SNNs) provide an efficient framework for processing
dynamic spatio-temporal signals and for investigating the learning principles
underlying biological neural systems. A key challenge in training SNNs is to
solve both spatial and temporal credit assignment. The dominant approach for
training SNNs is Backpropagation Through Time (BPTT) with surrogate gradients.
However, BPTT is in stark contrast with the spatial and temporal locality
observed in biological neural systems and leads to high computational and
memory demands, limiting efficient training strategies and on-device learning.
Although existing local learning rules achieve local temporal credit assignment
by leveraging eligibility traces, they fail to address the spatial credit
assignment without resorting to auxiliary layer-wise matrices, which increase
memory overhead and hinder scalability, especially on embedded devices. In this
work, we propose Traces Propagation (TP), a forward-only, memory-efficient,
scalable, and fully local learning rule that combines eligibility traces with a
layer-wise contrastive loss without requiring auxiliary layer-wise matrices. TP
outperforms other fully local learning rules on NMNIST and SHD datasets. On
more complex datasets such as DVS-GESTURE and DVS-CIFAR10, TP showcases
competitive performance and scales effectively to deeper SNN architectures such
as VGG-9, while providing favorable memory scaling compared to prior fully
local scalable rules, for datasets with a significant number of classes.
Finally, we show that TP is well suited for practical fine-tuning tasks, such
as keyword spotting on the Google Speech Commands dataset, thus paving the way
for efficient learning at the edge.

</details>


### [100] [When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.13079)
*Mengyi Deng,Xin Li,Tingyu Zhu,Zhicheng Yang,Zhijiang Guo,Wei Wang*

Main category: cs.LG

TL;DR: 通过反转s1k数据集构建r1k逆向推理数据集，研究发现SFT在r1k上比s1k提升1.6%-6.8%准确率，但混合正逆向数据会削弱方向区分性，DPO只能部分恢复这种区分性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单向监督微调，忽视了不同推理模式之间的复杂相互作用，需要研究双向推理目标下SFT和DPO对模型对齐的影响。

Method: 构建r1k逆向推理数据集（通过反转s1k中的1000个正向样本），分别进行SFT和DPO训练，评估双向推理目标下的对齐效果。

Result: SFT在r1k上比s1k提升1.6%-6.8%准确率；混合正逆向数据会削弱方向区分性；DPO只能部分恢复区分性但会抑制不太偏好的推理路径。

Conclusion: 混合推理数据会引入冲突的监督信号，需要开发鲁棒且方向感知的对齐策略来处理双向推理任务。

Abstract: Existing work has shown that o1-level performance can be achieved with
limited data distillation, but most existing methods focus on unidirectional
supervised fine-tuning (SFT), overlooking the intricate interplay between
diverse reasoning patterns. In this paper, we construct r1k, a high-quality
reverse reasoning dataset derived by inverting 1,000 forward examples from s1k,
and examine how SFT and Direct Preference Optimization (DPO) affect alignment
under bidirectional reasoning objectives. SFT on r1k yields a 1.6%--6.8%
accuracy improvement over s1k across evaluated benchmarks. However, naively
mixing forward and reverse data during SFT weakens the directional distinction.
Although DPO can partially recover this distinction, it also suppresses less
preferred reasoning paths by shifting the probability mass toward irrelevant
outputs. These findings suggest that mixed reasoning data introduce conflicting
supervision signals, underscoring the need for robust and direction-aware
alignment strategies.

</details>


### [101] [Discovering Mathematical Equations with Diffusion Language Model](https://arxiv.org/abs/2509.13136)
*Xiaoxu Han,Chengzhen Ning,Jinghui Zhong,Fubiao Yang,Yu Wang,Xin Mu*

Main category: cs.LG

TL;DR: DiffuSR是一个基于连续状态扩散语言模型的符号回归预训练框架，通过扩散过程将数学符号映射到连续潜在空间，利用交叉注意力机制注入数值数据指导，在标准基准测试中达到与最先进自回归方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中发现有效且有意义的数学方程在科学发现中至关重要，但符号回归任务由于搜索空间巨大和精度与复杂度之间的权衡而具有挑战性。

Method: 使用可训练的嵌入层在扩散过程中将离散数学符号映射到连续潜在空间，通过迭代去噪将初始噪声序列转换为符号方程，利用交叉注意力机制注入数值数据指导，并设计有效的推理策略将logit先验注入遗传编程。

Result: 在标准符号回归基准测试中，DiffuSR达到了与最先进自回归方法竞争的性能，并生成更具可解释性和多样性的数学表达式。

Conclusion: DiffuSR框架通过扩散模型有效建模方程分布，为符号回归任务提供了一种有竞争力的解决方案，能够生成高质量且多样化的数学表达式。

Abstract: Discovering valid and meaningful mathematical equations from observed data
plays a crucial role in scientific discovery. While this task, symbolic
regression, remains challenging due to the vast search space and the trade-off
between accuracy and complexity. In this paper, we introduce DiffuSR, a
pre-training framework for symbolic regression built upon a continuous-state
diffusion language model. DiffuSR employs a trainable embedding layer within
the diffusion process to map discrete mathematical symbols into a continuous
latent space, modeling equation distributions effectively. Through iterative
denoising, DiffuSR converts an initial noisy sequence into a symbolic equation,
guided by numerical data injected via a cross-attention mechanism. We also
design an effective inference strategy to enhance the accuracy of the
diffusion-based equation generator, which injects logit priors into genetic
programming. Experimental results on standard symbolic regression benchmarks
demonstrate that DiffuSR achieves competitive performance with state-of-the-art
autoregressive methods and generates more interpretable and diverse
mathematical expressions.

</details>


### [102] [Curriculum Learning for Mesh-based simulations](https://arxiv.org/abs/2509.13138)
*Paul Garnier,Vincent Lannelongue,Elie Hachem*

Main category: cs.LG

TL;DR: 提出了一种从粗到细的课程学习方法，通过在粗网格上预训练再逐步引入高分辨率数据，加速图神经网络在CFD模拟中的训练，减少50%训练时间并突破性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统方法在高分辨率非结构化网格（多达30万个节点）上训练图神经网络计算流体动力学模型计算成本过高，需要更高效的训练策略。

Method: 采用课程学习策略，模型结构保持不变，仅改变训练数据的分辨率：先在极粗网格上训练，然后逐步引入中等到高分辨率数据。

Result: 在保持相当泛化精度的同时，总训练时间减少高达50%，在模型容量不足的数据集上能够突破性能平台。

Conclusion: 从粗到细的课程学习是加速高分辨率CFD图神经网络训练的有效方法，无需改变模型架构即可显著提升训练效率。

Abstract: Graph neural networks (GNNs) have emerged as powerful surrogates for
mesh-based computational fluid dynamics (CFD), but training them on
high-resolution unstructured meshes with hundreds of thousands of nodes remains
prohibitively expensive. We study a \emph{coarse-to-fine curriculum} that
accelerates convergence by first training on very coarse meshes and then
progressively introducing medium and high resolutions (up to \(3\times10^5\)
nodes). Unlike multiscale GNN architectures, the model itself is unchanged;
only the fidelity of the training data varies over time. We achieve comparable
generalization accuracy while reducing total wall-clock time by up to 50\%.
Furthermore, on datasets where our model lacks the capacity to learn the
underlying physics, using curriculum learning enables it to break through
plateaus.

</details>


### [103] [Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges](https://arxiv.org/abs/2509.13139)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 该论文研究了图异质性对消息传递图神经网络性能的影响，通过添加自循环和平行边来更新异质图，分析图拉普拉斯特征值变化与GCN性能趋势的关系，建立了图谱与低通滤波器性能之间的联系。


<details>
  <summary>Details</summary>
Motivation: 图异质性对MP-GNNs性能构成挑战，特别是低通滤波器如GCN在异质图上性能下降，需要深入分析其性能表现并建立与图内在特性的联系。

Method: 通过在异质图上添加自循环和平行边来更新图结构，观察图拉普拉斯特征值的变化，并在多个基准异质网络上研究GCN的性能趋势，建立图谱与性能的关系。

Result: 研究发现添加自循环和平行边分别会降低和增加图拉普拉斯特征值，GCN性能呈现相应的增加或下降趋势，成功建立了图谱特性与低通滤波器性能的关联。

Conclusion: 通过观察低通滤波器的性能趋势可以无缝评估图谱和图形特性，无需进行昂贵的特征值分解，为理解图神经网络在异质图上的行为提供了新的视角。

Abstract: Graph heterophily poses a formidable challenge to the performance of
Message-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filters
like Graph Convolutional Networks (GCNs) face performance degradation, which
can be attributed to the blending of the messages from dissimilar neighboring
nodes. The performance of the low-pass filters on heterophilic graphs still
requires an in-depth analysis. In this context, we update the heterophilic
graphs by adding a number of self-loops and parallel edges. We observe that
eigenvalues of the graph Laplacian decrease and increase respectively by
increasing the number of self-loops and parallel edges. We conduct several
studies regarding the performance of GCN on various benchmark heterophilic
networks by adding either self-loops or parallel edges. The studies reveal that
the GCN exhibited either increasing or decreasing performance trends on adding
self-loops and parallel edges. In light of the studies, we established
connections between the graph spectra and the performance trends of the
low-pass filters on the heterophilic graphs. The graph spectra characterize the
essential intrinsic properties of the input graph like the presence of
connected components, sparsity, average degree, cluster structures, etc. Our
work is adept at seamlessly evaluating graph spectrum and properties by
observing the performance trends of the low-pass filters without pursuing the
costly eigenvalue decomposition. The theoretical foundations are also discussed
to validate the impact of adding self-loops and parallel edges on the graph
spectrum.

</details>


### [104] [FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning](https://arxiv.org/abs/2509.13160)
*Liang Hu,Jianpeng Jiao,Jiashuo Liu,Yanle Ren,Zhoufutu Wen,Kaiyuan Zhang,Xuanliang Zhang,Xiang Gao,Tianci He,Fei Hu,Yali Liao,Zaiyuan Wang,Chenghao Yang,Qianyu Yang,Mingren Yin,Zhiyuan Zeng,Ge Zhang,Xinyi Zhang,Xiying Zhao,Zhenwei Zhu,Hongseok Namkoong,Wenhao Huang,Yuwen Tang*

Main category: cs.LG

TL;DR: FinSearchComp是首个开源的金融搜索智能体基准测试，包含三个真实金融分析师工作流程任务，由70位金融专家标注，评估了21个模型在635个问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有金融数据集缺乏评估端到端智能体数据搜索能力的功能，金融领域需要处理复杂、多步骤、时效性强的搜索任务，是评估搜索能力和知识推理的理想场景。

Method: 构建包含时间敏感数据获取、简单历史查询和复杂历史调查三个任务的基准测试，采用70位金融专家进行标注，实施严格的多阶段质量保证流程，涵盖全球和大中华区市场。

Result: Grok 4在全局子集上表现最佳，接近专家级准确率；DouBao在大中华区子集领先。实验表明配备网络搜索和金融插件能显著提升性能，模型和工具的国家来源对性能有显著影响。

Conclusion: FinSearchComp通过与真实分析师任务对齐并提供端到端评估，为复杂金融搜索和推理提供了一个专业、高难度的测试平台。

Abstract: Search has emerged as core infrastructure for LLM-based agents and is widely
viewed as critical on the path toward more general intelligence. Finance is a
particularly demanding proving ground: analysts routinely conduct complex,
multi-step searches over time-sensitive, domain-specific data, making it ideal
for assessing both search proficiency and knowledge-grounded reasoning. Yet no
existing open financial datasets evaluate data searching capability of
end-to-end agents, largely because constructing realistic, complicated tasks
requires deep financial expertise and time-sensitive data is hard to evaluate.
We present FinSearchComp, the first fully open-source agent benchmark for
realistic, open-domain financial search and reasoning. FinSearchComp comprises
three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and
Complex Historical Investigation -- closely reproduce real-world financial
analyst workflows. To ensure difficulty and reliability, we engage 70
professional financial experts for annotation and implement a rigorous
multi-stage quality-assurance pipeline. The benchmark includes 635 questions
spanning global and Greater China markets, and we evaluate 21 models (products)
on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.
DouBao (web) leads on the Greater China subset. Experimental analyses show that
equipping agents with web search and financial plugins substantially improves
results on FinSearchComp, and the country origin of models and tools impact
performance significantly.By aligning with realistic analyst tasks and
providing end-to-end evaluation, FinSearchComp offers a professional,
high-difficulty testbed for complex financial search and reasoning.

</details>


### [105] [On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models](https://arxiv.org/abs/2509.13165)
*Alessandro Antonucci,Eric Rossetto,Ivan Duvnjak*

Main category: cs.LG

TL;DR: 本文研究生成概率分类器的个体公平性，分析后验推断对私有特征扰动的鲁棒性，发现鲁棒性与预测准确性存在相关性，并提出新方法缓解公平性与准确性的传统权衡


<details>
  <summary>Details</summary>
Motivation: 研究生成概率分类器中个体公平性问题，分析后验推断对私有特征扰动的鲁棒性，探索鲁棒性与预测准确性之间的相关性

Method: 使用贝叶斯网络作为基础生成模型，在14个具有公平性问题的基准数据集上进行实证评估。为解决多私有特征鲁棒性分析的计算复杂度问题，将问题重新表述为辅助马尔可夫随机场中的最可能解释任务

Result: 实验证实了鲁棒性与预测准确性之间存在相关性的假设，鲁棒性更强的实例更可能被准确分类

Conclusion: 研究结果提出了缓解公平性与准确性之间传统权衡的新方向，为生成概率分类器的公平性分析提供了新见解

Abstract: We investigate individual fairness in generative probabilistic classifiers by
analysing the robustness of posterior inferences to perturbations in private
features. Building on established results in robustness analysis, we
hypothesise a correlation between robustness and predictive accuracy,
specifically, instances exhibiting greater robustness are more likely to be
classified accurately. We empirically assess this hypothesis using a benchmark
of fourteen datasets with fairness concerns, employing Bayesian networks as the
underlying generative models. To address the computational complexity
associated with robustness analysis over multiple private features with
Bayesian networks, we reformulate the problem as a most probable explanation
task in an auxiliary Markov random field. Our experiments confirm the
hypothesis about the correlation, suggesting novel directions to mitigate the
traditional trade-off between fairness and accuracy.

</details>


### [106] [Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification with Limited Entropy](https://arxiv.org/abs/2509.13185)
*Yunchuan Guan,Yu Liu,Ke Zhou,Zhiqi Shen,Jenq-Neng Hwang,Serge Belongie,Lei Li*

Main category: cs.LG

TL;DR: 该论文通过理论分析和实验验证，证明了元学习在有限熵监督设置下比全类训练具有更紧的泛化边界，对标签噪声和异构任务更鲁棒，并提出了MINO元学习框架来提升无监督性能。


<details>
  <summary>Details</summary>
Motivation: 针对近期研究表明全类训练策略在少样本分类任务中可以达到与元学习相当的性能，为了证明元学习的价值，建立公平比较的熵限制监督设置。

Method: 通过理论分析和实验验证元学习的泛化边界优势；提出MINO元学习框架，使用DBSCAN自适应聚类算法和动态头进行无监督任务构建，以及基于稳定性的元缩放器来抵抗标签噪声。

Result: 理论分析显示元学习具有更紧的泛化边界；实验验证元学习在有限熵下更高效，对标签噪声和异构任务更鲁棒；MINO框架在多个无监督少样本和零样本任务中表现出色。

Conclusion: 元学习在有限熵设置下具有理论优势，对无监督任务更适用；MINO框架有效提升了无监督性能，证明了元学习在复杂场景下的价值。

Abstract: Meta-learning is a powerful paradigm for tackling few-shot tasks. However,
recent studies indicate that models trained with the whole-class training
strategy can achieve comparable performance to those trained with meta-learning
in few-shot classification tasks. To demonstrate the value of meta-learning, we
establish an entropy-limited supervised setting for fair comparisons. Through
both theoretical analysis and experimental validation, we establish that
meta-learning has a tighter generalization bound compared to whole-class
training. We unravel that meta-learning is more efficient with limited entropy
and is more robust to label noise and heterogeneous tasks, making it
well-suited for unsupervised tasks. Based on these insights, We propose MINO, a
meta-learning framework designed to enhance unsupervised performance. MINO
utilizes the adaptive clustering algorithm DBSCAN with a dynamic head for
unsupervised task construction and a stability-based meta-scaler for robustness
against label noise. Extensive experiments confirm its effectiveness in
multiple unsupervised few-shot and zero-shot tasks.

</details>


### [107] [TRUST-FS: Tensorized Reliable Unsupervised Multi-View Feature Selection for Incomplete Data](https://arxiv.org/abs/2509.13192)
*Minghui Lu,Yanyong Huang,Minbo Ma,Dongjie Wang,Xiuwen Yi,Tianrui Li*

Main category: cs.LG

TL;DR: TRUST-FS是一种新的多视图无监督特征选择方法，通过张量分解框架同时处理特征选择、缺失变量填补和视图权重学习，解决了现有方法在处理缺失变量和相似性图不准确方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的多视图无监督特征选择方法在处理不完整多视图数据时存在三个主要挑战：1) 只能处理缺失视图，无法处理更一般的缺失变量情况；2) 填补和特征选择过程相互独立；3) 缺失数据导致相似性图不准确。

Method: 提出TRUST-FS方法，采用自适应加权CP分解，在统一的张量分解框架中同时进行特征选择、缺失变量填补和视图权重学习，并利用主观逻辑获取可信的跨视图相似性信息来构建可靠的相似性图。

Result: 综合实验结果表明，该方法在效果和性能上优于现有的最先进方法。

Conclusion: TRUST-FS通过统一的张量分解框架有效解决了多视图无监督特征选择中的缺失变量处理问题，提高了特征选择的准确性和可靠性。

Abstract: Multi-view unsupervised feature selection (MUFS), which selects informative
features from multi-view unlabeled data, has attracted increasing research
interest in recent years. Although great efforts have been devoted to MUFS,
several challenges remain: 1) existing methods for incomplete multi-view data
are limited to handling missing views and are unable to address the more
general scenario of missing variables, where some features have missing values
in certain views; 2) most methods address incomplete data by first imputing
missing values and then performing feature selection, treating these two
processes independently and overlooking their interactions; 3) missing data can
result in an inaccurate similarity graph, which reduces the performance of
feature selection. To solve this dilemma, we propose a novel MUFS method for
incomplete multi-view data with missing variables, termed Tensorized Reliable
UnSupervised mulTi-view Feature Selection (TRUST-FS). TRUST-FS introduces a new
adaptive-weighted CP decomposition that simultaneously performs feature
selection, missing-variable imputation, and view weight learning within a
unified tensor factorization framework. By utilizing Subjective Logic to
acquire trustworthy cross-view similarity information, TRUST-FS facilitates
learning a reliable similarity graph, which subsequently guides feature
selection and imputation. Comprehensive experimental results demonstrate the
effectiveness and superiority of our method over state-of-the-art methods.

</details>


### [108] [B-TGAT: A Bi-directional Temporal Graph Attention Transformer for Clustering Multivariate Spatiotemporal Data](https://arxiv.org/abs/2509.13202)
*Francis Ndikum Nji,Vandana Janaja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出了一种结合双向时序图注意力变换器(B-TGAT)的时间分布混合U-Net自编码器，用于高维多变量时空气候数据的高效时序聚类，在三个不同数据集上表现出优越的聚类分离性和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理高维时空气候数据时难以同时捕获局部和全局时序关系并保持空间上下文，需要新的方法来应对复杂的时序依赖、演化的空间交互和非平稳动态。

Method: 使用时间分布混合U-Net自编码器，编码器和解码器配备ConvLSTM2D模块提取联合时空特征，通过跳跃连接保持多尺度空间细节。在瓶颈处集成B-TGAT，结合基于图的空间建模和注意力驱动的时序编码。

Result: 在三个不同的时空气候数据集上实验表明，该方法相比最先进的基线方法具有优越的聚类分离性、时间稳定性，并且与已知的气候转变对齐。

Conclusion: ConvLSTM2D、U-Net跳跃连接和B-TGAT的集成提升了时序聚类性能，同时为复杂的时空变异性提供了可解释的见解，推动了方法学发展和气候科学应用。

Abstract: Clustering high-dimensional multivariate spatiotemporal climate data is
challenging due to complex temporal dependencies, evolving spatial
interactions, and non-stationary dynamics. Conventional clustering methods,
including recurrent and convolutional models, often struggle to capture both
local and global temporal relationships while preserving spatial context. We
present a time-distributed hybrid U-Net autoencoder that integrates a
Bi-directional Temporal Graph Attention Transformer (B-TGAT) to guide efficient
temporal clustering of multidimensional spatiotemporal climate datasets. The
encoder and decoder are equipped with ConvLSTM2D modules that extract joint
spatial--temporal features by modeling localized dynamics and spatial
correlations over time, and skip connections that preserve multiscale spatial
details during feature compression and reconstruction. At the bottleneck,
B-TGAT integrates graph-based spatial modeling with attention-driven temporal
encoding, enabling adaptive weighting of temporal neighbors and capturing both
short and long-range dependencies across regions. This architecture produces
discriminative latent embeddings optimized for clustering. Experiments on three
distinct spatiotemporal climate datasets demonstrate superior cluster
separability, temporal stability, and alignment with known climate transitions
compared to state-of-the-art baselines. The integration of ConvLSTM2D, U-Net
skip connections, and B-TGAT enhances temporal clustering performance while
providing interpretable insights into complex spatiotemporal variability,
advancing both methodological development and climate science applications.

</details>


### [109] [HAM: Hierarchical Adapter Merging for Scalable Continual Learning](https://arxiv.org/abs/2509.13211)
*Eric Nuertey Coleman,Luigi Quarantiello,Samrat Mukherjee,Julio Hurtado,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: HAM是一种新颖的分层适配器合并框架，通过动态合并不同任务的适配器来解决持续学习中的灾难性遗忘问题，在视觉基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中新知识干扰旧知识导致的灾难性遗忘问题，特别是当面对新数据分布时，现有参数高效微调方法在动态学习场景和长任务序列中面临扩展性挑战。

Method: 提出分层适配器合并(HAM)框架：维护固定组别层次化整合新适配器；为每个任务训练低秩适配器和重要性标量；基于适配器相似性动态分组；在组内进行适配器剪枝、缩放和合并。

Result: 在三个视觉基准测试上的广泛实验表明，HAM显著优于最先进的方法，特别是在任务数量增加时表现更加突出。

Conclusion: HAM框架通过动态合并适配器有效解决了持续学习中的扩展性问题，能够管理比竞争基线更多的任务，并提高了效率。

Abstract: Continual learning is an essential capability of human cognition, yet it
poses significant challenges for current deep learning models. The primary
issue is that new knowledge can interfere with previously learned information,
causing the model to forget earlier knowledge in favor of the new, a phenomenon
known as catastrophic forgetting. Although large pre-trained models can
partially mitigate forgetting by leveraging their existing knowledge and
over-parameterization, they often struggle when confronted with novel data
distributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,
enable efficient adaptation to new knowledge. However, they still face
challenges in scaling to dynamic learning scenarios and long sequences of
tasks, as maintaining one adapter per task introduces complexity and increases
the potential for interference. In this paper, we introduce Hierarchical
Adapters Merging (HAM), a novel framework that dynamically combines adapters
from different tasks during training. This approach enables HAM to scale
effectively, allowing it to manage more tasks than competing baselines with
improved efficiency. To achieve this, HAM maintains a fixed set of groups that
hierarchically consolidate new adapters. For each task, HAM trains a low-rank
adapter along with an importance scalar, then dynamically groups tasks based on
adapter similarity. Within each group, adapters are pruned, scaled and merge,
facilitating transfer learning between related tasks. Extensive experiments on
three vision benchmarks show that HAM significantly outperforms
state-of-the-art methods, particularly as the number of tasks increases.

</details>


### [110] [Density-Aware Farthest Point Sampling](https://arxiv.org/abs/2509.13213)
*Paolo Climaco,Jochen Garcke*

Main category: cs.LG

TL;DR: 提出一种基于密度感知的最远点采样方法(DA-FPS)，用于在标注数据有限的情况下选择训练集，通过最小化加权填充距离来降低回归模型的预测误差上界。


<details>
  <summary>Details</summary>
Motivation: 在机器学习回归任务中，由于计算限制或标注成本高昂，标注训练数据往往有限。因此需要从无标注数据中选择合适的训练集来平衡性能与效率。

Method: 推导了Lipschitz连续回归模型预测误差的上界，该上界与训练集的加权填充距离线性相关。提出了DA-FPS采样方法，该方法通过数据驱动的方式近似最小化加权填充距离。

Result: 在三个数据集上使用两种回归模型进行实验，结果显示DA-FPS相比其他采样策略显著降低了平均绝对预测误差。

Conclusion: DA-FPS是一种有效的被动、模型无关的采样方法，能够通过优化数据特征表示来提升回归模型在有限标注数据下的性能。

Abstract: We focus on training machine learning regression models in scenarios where
the availability of labeled training data is limited due to computational
constraints or high labeling costs. Thus, selecting suitable training sets from
unlabeled data is essential for balancing performance and efficiency. For the
selection of the training data, we focus on passive and model-agnostic sampling
methods that only consider the data feature representations. We derive an upper
bound for the expected prediction error of Lipschitz continuous regression
models that linearly depends on the weighted fill distance of the training set,
a quantity we can estimate simply by considering the data features. We
introduce "Density-Aware Farthest Point Sampling" (DA-FPS), a novel sampling
method. We prove that DA-FPS provides approximate minimizers for a data-driven
estimation of the weighted fill distance, thereby aiming at minimizing our
derived bound. We conduct experiments using two regression models across three
datasets. The results demonstrate that DA-FPS significantly reduces the mean
absolute prediction error compared to other sampling strategies.

</details>


### [111] [FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data](https://arxiv.org/abs/2509.13218)
*J. Cha,J. Lee,J. Cho,J. Shin*

Main category: cs.LG

TL;DR: FOSSIL是一个统一的样本权重框架，通过单一可解释公式整合类别不平衡校正、难度感知课程、数据增强惩罚和预热动态，在小样本不平衡数据场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决小样本不平衡数据场景（如罕见疾病成像、基因组学等）中现有方法（过采样、焦点损失等）脆弱或复杂的问题，需要一个统一且理论保证的框架。

Method: 提出FOSSIL框架，通过样本敏感重要性学习，将类别不平衡校正、难度感知课程学习、数据增强惩罚和预热动态整合到单一可解释的权重公式中，无需改变模型架构。

Result: 在合成和真实数据集上，FOSSIL consistently优于ERM、课程学习和元加权基线方法，并提供基于遗憾的理论保证。

Conclusion: FOSSIL提供了一个理论保证的统一框架，有效解决了小样本不平衡学习问题，在多个领域具有应用潜力。

Abstract: Imbalanced and small data regimes are pervasive in domains such as rare
disease imaging, genomics, and disaster response, where labeled samples are
scarce and naive augmentation often introduces artifacts. Existing solutions
such as oversampling, focal loss, or meta-weighting address isolated aspects of
this challenge but remain fragile or complex. We introduce FOSSIL (Flexible
Optimization via Sample Sensitive Importance Learning), a unified weighting
framework that seamlessly integrates class imbalance correction,
difficulty-aware curricula, augmentation penalties, and warmup dynamics into a
single interpretable formula. Unlike prior heuristics, the proposed framework
provides regret-based theoretical guarantees and achieves consistent empirical
gains over ERM, curriculum, and meta-weighting baselines on synthetic and
real-world datasets, while requiring no architectural changes.

</details>


### [112] [On the Out-of-Distribution Backdoor Attack for Federated Learning](https://arxiv.org/abs/2509.13219)
*Jiahao Xu,Zikai Zhang,Rui Hu*

Main category: cs.LG

TL;DR: 提出了一种新的联邦学习后门攻击方法OBA，使用分布外数据作为毒化样本和触发器，并通过SoDa技术增强隐蔽性；同时开发了BNGuard防御方法，利用批归一化层统计偏差检测恶意模型更新。


<details>
  <summary>Details</summary>
Motivation: 传统后门攻击依赖可见触发器和物理修改目标对象，应用场景受限。需要开发更隐蔽、实用的联邦学习后门攻击方法，并相应提出有效防御方案。

Method: OBA攻击使用分布外(OOD)数据同时作为毒化样本和触发器；SoDa技术通过正则化恶意本地模型的幅度和方向来增强隐蔽性；BNGuard防御通过检测批归一化层运行统计的显著偏差来识别恶意模型更新。

Result: OBA能有效绕过现有先进防御方法，同时保持主任务高准确率；BNGuard在各种设置下都能有效防御SoDa攻击，增强联邦学习的后门鲁棒性。

Conclusion: 该工作提出了更实用的联邦学习后门攻击范式，并开发了相应的有效防御机制，为联邦学习系统安全提供了新的攻击和防御视角。

Abstract: Traditional backdoor attacks in federated learning (FL) operate within
constrained attack scenarios, as they depend on visible triggers and require
physical modifications to the target object, which limits their practicality.
To address this limitation, we introduce a novel backdoor attack prototype for
FL called the out-of-distribution (OOD) backdoor attack ($\mathtt{OBA}$), which
uses OOD data as both poisoned samples and triggers simultaneously. Our
approach significantly broadens the scope of backdoor attack scenarios in FL.
To improve the stealthiness of $\mathtt{OBA}$, we propose $\mathtt{SoDa}$,
which regularizes both the magnitude and direction of malicious local models
during local training, aligning them closely with their benign versions to
evade detection. Empirical results demonstrate that $\mathtt{OBA}$ effectively
circumvents state-of-the-art defenses while maintaining high accuracy on the
main task.
  To address this security vulnerability in the FL system, we introduce
$\mathtt{BNGuard}$, a new server-side defense method tailored against
$\mathtt{SoDa}$. $\mathtt{BNGuard}$ leverages the observation that OOD data
causes significant deviations in the running statistics of batch normalization
layers. This allows $\mathtt{BNGuard}$ to identify malicious model updates and
exclude them from aggregation, thereby enhancing the backdoor robustness of FL.
Extensive experiments across various settings show the effectiveness of
$\mathtt{BNGuard}$ on defending against $\mathtt{SoDa}$. The code is available
at https://github.com/JiiahaoXU/SoDa-BNGuard.

</details>


### [113] [Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors](https://arxiv.org/abs/2509.13237)
*Aniket Didolkar,Nicolas Ballas,Sanjeev Arora,Anirudh Goyal*

Main category: cs.LG

TL;DR: 该论文提出了一种将重复推理片段转换为可重用"行为"的方法，通过LLM的元认知分析创建行为手册，在推理时提供相关行为来减少token使用并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决多步问题时经常重复推导相同的中间步骤，导致token使用和延迟增加，上下文窗口饱和限制了探索能力。

Method: 通过模型的元认知分析将重复推理片段转换为简洁可重用的"行为"（名称+指令），存储在行为手册中，在推理时提供相关行为或通过监督微调将其蒸馏到参数中。

Result: 在三种设置下取得改进：1)行为条件推理减少推理token达46%，同时保持或提高准确性；2)行为引导自改进无需参数更新即可提高10%准确性；3)行为条件SFT比普通SFT更有效地将非推理模型转换为推理模型。

Conclusion: 将缓慢推导转换为快速程序提示使LLM能够记住如何推理，而不仅仅是记住要得出什么结论。

Abstract: Large language models (LLMs) now solve multi-step problems by emitting
extended chains of thought. During the process, they often re-derive the same
intermediate steps across problems, inflating token usage and latency. This
saturation of the context window leaves less capacity for exploration. We study
a simple mechanism that converts recurring reasoning fragments into concise,
reusable "behaviors" (name + instruction) via the model's own metacognitive
analysis of prior traces. These behaviors are stored in a "behavior handbook"
which supplies them to the model in-context at inference or distills them into
parameters via supervised fine-tuning. This approach achieves improved
test-time reasoning across three different settings - 1) Behavior-conditioned
inference: Providing the LLM relevant behaviors in-context during reasoning
reduces number of reasoning tokens by up to 46% while matching or improving
baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter
updates, the model improves its own future reasoning by leveraging behaviors
from its own past problem solving attempts. This yields up to 10% higher
accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned
SFT: SFT on behavior-conditioned reasoning traces is more effective at
converting non-reasoning models into reasoning models as compared to vanilla
SFT. Together, these results indicate that turning slow derivations into fast
procedural hints enables LLMs to remember how to reason, not just what to
conclude.

</details>


### [114] [Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning](https://arxiv.org/abs/2509.13240)
*Bo Yin,Xingyi Yang,Xinchao Wang*

Main category: cs.LG

TL;DR: NoRA是首个直接适配预训练Transformer模型中非线性激活函数的参数高效微调框架，通过可学习有理函数和结构化低秩更新，仅需更新0.4%参数即可达到或超越全量微调效果。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法主要调整权重矩阵而保持激活函数固定，忽略了激活函数作为模型适应的重要对象。

Method: 用可学习有理函数替换固定激活函数，对分子和分母系数应用结构化低秩更新，采用分组设计实现局部适应和稳定性提升。

Result: 在CIFAR数据集上达到+0.17%-0.27%准确率提升；与LoRA结合(NoRA++)在LLaMA3-8B指令微调中平均MMLU提升0.3%-0.8%，STEM任务提升1.6%。

Conclusion: 激活空间调优是权重基PEFT的补充性高效替代方案，将激活函数确立为模型适应的首要对象。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt
weight matrices while keeping activation functions fixed. We introduce
\textbf{NoRA}, the first PEFT framework that directly adapts nonlinear
activation functions in pretrained transformer-based models. NoRA replaces
fixed activations with learnable rational functions and applies structured
low-rank updates to numerator and denominator coefficients, with a group-wise
design that localizes adaptation and improves stability at minimal cost. On
vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds
full fine-tuning while updating only 0.4\% of parameters (0.02M), achieving
accuracy gains of +0.17\% and +0.27\%. When combined with LoRA
(\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets
by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++
consistently improves generation quality, yielding average MMLU gains of
+0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. We
further show that NoRA constrains adaptation to a low-dimensional functional
subspace, implicitly regularizing update magnitude and direction. These results
establish activation-space tuning as a complementary and highly
parameter-efficient alternative to weight-based PEFT, positioning activation
functions as first-class objects for model adaptation.

</details>


### [115] [Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning](https://arxiv.org/abs/2509.13262)
*Zhizhong Zhao,Ke Chen*

Main category: cs.LG

TL;DR: 提出了一种后处理单前向传播框架SPC-UQ，通过分割点分析(SPA)同时捕捉任意性和认知不确定性，无需重新训练预训练模型，在回归和分类任务中实现了高效且准确的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法要么计算成本高（如贝叶斯或集成方法），要么只能提供部分任务特定的估计（如单前向传播技术），需要一种既高效又能全面捕捉不确定性的方法。

Method: 使用分割点分析(SPA)将预测残差分解为上下子集，计算每侧的平均绝对残差(MARs)，通过自一致性差异分数(SDS)进行细粒度认知估计。回归任务使用分位数回归获得预测区间，分类任务基于SPA校准softmax输出。

Result: 在多个回归和分类基准测试中，该方法匹配或超越了多种最先进的不确定性量化方法，同时计算开销极小。

Conclusion: SPC-UQ框架提供了一种高效、全面的不确定性量化解决方案，能够在保持低计算成本的同时，为深度学习的可信度提供可靠的不确定性估计。

Abstract: Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet
existing methods are either computationally intensive, such as Bayesian or
ensemble methods, or provide only partial, task-specific estimates, such as
single-forward-pass techniques. In this paper, we propose a post-hoc
single-forward-pass framework that jointly captures aleatoric and epistemic
uncertainty without modifying or retraining pretrained models. Our method
applies \emph{Split-Point Analysis} (SPA) to decompose predictive residuals
into upper and lower subsets, computing \emph{Mean Absolute Residuals} (MARs)
on each side. We prove that, under ideal conditions, the total MAR equals the
harmonic mean of subset MARs; deviations define a novel \emph{Self-consistency
Discrepancy Score} (SDS) for fine-grained epistemic estimation across
regression and classification. For regression, side-specific quantile
regression yields prediction intervals with improved empirical coverage, which
are further calibrated via SDS. For classification, when calibration data are
available, we apply SPA-based calibration identities to adjust the softmax
outputs and then compute predictive entropy on these calibrated probabilities.
Extensive experiments on diverse regression and classification benchmarks
demonstrate that our framework matches or exceeds several state-of-the-art UQ
methods while incurring minimal overhead.
  Our source code is available at https://github.com/zzz0527/SPC-UQ.

</details>


### [116] [JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks](https://arxiv.org/abs/2509.13266)
*Jiahao Zhang,Xiaobing Pei,Zhaokun Zhong,Wenqiang Hao,Zhenghao Tang*

Main category: cs.LG

TL;DR: 提出了JANUS框架，通过局部特征流形对齐和全局结构语义一致性来提升节点注入攻击的隐蔽性，使用强化学习进行优化，在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有节点注入攻击方法主要依赖间接代理指标实现隐蔽性，缺乏对注入内容本质特征的考虑，或仅关注局部结构模仿，存在局部近视问题。

Method: 提出双约束隐蔽节点注入框架JANUS：局部层面使用特征流形对齐策略实现几何一致性；全局层面引入结构化潜变量并最大化与生成结构的互信息；将注入攻击建模为序列决策过程，用强化学习优化。

Result: 在多个标准数据集上的实验表明，JANUS框架在攻击有效性和隐蔽性方面均显著优于现有方法。

Conclusion: JANUS框架通过同时考虑局部和全局结构一致性，有效解决了节点注入攻击的隐蔽性问题，为图神经网络安全提供了新的解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable performance across
various applications, yet they are vulnerable to sophisticated adversarial
attacks, particularly node injection attacks. The success of such attacks
heavily relies on their stealthiness, the ability to blend in with the original
graph and evade detection. However, existing methods often achieve stealthiness
by relying on indirect proxy metrics, lacking consideration for the fundamental
characteristics of the injected content, or focusing only on imitating local
structures, which leads to the problem of local myopia. To overcome these
limitations, we propose a dual-constraint stealthy node injection framework,
called Joint Alignment of Nodal and Universal Structures (JANUS). At the local
level, we introduce a local feature manifold alignment strategy to achieve
geometric consistency in the feature space. At the global level, we incorporate
structured latent variables and maximize the mutual information with the
generated structures, ensuring the injected structures are consistent with the
semantic patterns of the original graph. We model the injection attack as a
sequential decision process, which is optimized by a reinforcement learning
agent. Experiments on multiple standard datasets demonstrate that the JANUS
framework significantly outperforms existing methods in terms of both attack
effectiveness and stealthiness.

</details>


### [117] [LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt](https://arxiv.org/abs/2509.13268)
*Rodrigo M Carrillo-Larco*

Main category: cs.LG

TL;DR: 开源大语言模型通过思维链提示和参数高效微调，仅基于文本描述就能准确预测食物营养含量，为低负担的饮食监测提供了新方法


<details>
  <summary>Details</summary>
Motivation: 现有AI营养估算工具多依赖图像输入，研究探索大语言模型是否能仅通过文本描述准确预测营养值，以简化饮食监测流程

Method: 使用NHANES青少年24小时饮食回顾数据，采用10-shot思维链提示的开源量化LLM预测能量和五种宏量营养素，并应用参数高效微调(PEFT)提升准确性

Result: 原始LLM预测效果差(能量MAE 652.08，Lin's CCC <0.46)，但微调后模型表现显著改善(能量MAE 171-191，Lin's CCC >0.89)

Conclusion: 经过思维链提示和PEFT微调的开源LLM能够仅基于文本输入准确预测饮食回顾中的营养值，为文本基础的饮食监测工具提供了可行方案

Abstract: BACKGROUND: Most artificial intelligence tools used to estimate nutritional
content rely on image input. However, whether large language models (LLMs) can
accurately predict nutritional values based solely on text descriptions of
foods consumed remains unknown. If effective, this approach could enable
simpler dietary monitoring without the need for photographs. METHODS: We used
24-hour dietary recalls from adolescents aged 12-19 years in the National
Health and Nutrition Examination Survey (NHANES). An open-source quantized LLM
was prompted using a 10-shot, chain-of-thought approach to estimate energy and
five macronutrients based solely on text strings listing foods and their
quantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate
whether predictive accuracy improved. NHANES-calculated values served as the
ground truth for energy, proteins, carbohydrates, total sugar, dietary fiber
and total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,
mean age 15.4 years), the vanilla LLM yielded poor predictions. The mean
absolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across
endpoints. In contrast, the fine-tuned model performed substantially better,
with energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC
exceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a
chain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed
solely to text input can accurately predict energy and macronutrient values
from 24-hour dietary recalls. This approach holds promise for low-burden,
text-based dietary monitoring tools.

</details>


### [118] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Yida Zhao,Liwen Zhang,Litu Ou,Dingchu Zhang,Xixi Wu,Jialong Wu,Xinyu Wang,Zile Qiao,Zhen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.LG

TL;DR: WebSailor是一种后训练方法，通过生成高不确定性任务、RFT冷启动和DUPO算法，使开源模型在复杂信息搜索任务中达到与专有代理相当的性能。


<details>
  <summary>Details</summary>
Motivation: 超越人类认知限制是LLM训练的关键前沿。专有代理系统在复杂信息搜索基准上展现出超人类能力，其成功关键在于系统性地减少极端不确定性的推理模式，这是开源模型所缺乏的。

Method: 提出WebSailor完整后训练方法：1）通过结构化采样和信息模糊化生成新颖的高不确定性任务；2）RFT冷启动；3）高效的代理RL训练算法DUPO（重复采样策略优化）。

Result: WebSailor在复杂信息搜索任务中显著超越所有开源代理，匹配专有代理的性能，缩小了能力差距。

Conclusion: 该方法成功地将专有代理的关键推理能力移植到开源模型中，证明了通过系统性后训练可以弥补开源模型与专有系统之间的性能差距。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all open-source agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [119] [PBPK-iPINNs : Inverse Physics-Informed Neural Networks for Physiologically Based Pharmacokinetic Brain Models](https://arxiv.org/abs/2509.12666)
*Charuka D. Wickramasinghe,Krishanthi C. Weerasinghe,Pradeep K. Ranaweera*

Main category: stat.ML

TL;DR: 提出了PBPK-iPINN方法，使用逆物理信息神经网络来估计PBPK脑区室模型中的药物特异性参数和药物浓度分布，通过适当加权损失函数和精细调参实现收敛。


<details>
  <summary>Details</summary>
Motivation: 传统PBPK模型基于ODE系统，需要估计药物特异性或患者特异性参数。物理信息神经网络(PINNs)能够将机器学习与微分方程结合，确保预测符合物理规律，为PBPK参数估计提供新方法。

Method: 开发PBPK-iPINN方法，使用逆PINNs解决PBPK脑区室模型的参数估计问题。关键包括适当加权数据损失、初始条件损失和残差损失，以及精细调整网络层数、神经元数、激活函数、学习率、优化器和配点等参数。

Result: 方法能够收敛到正确解，成功估计药物特异性参数和浓度分布。通过与已建立的传统数值和统计方法比较验证了性能。

Conclusion: PBPK-iPINN为PBPK建模提供了一种有效的参数估计方法，通过适当的损失函数加权和参数调优可以实现良好的收敛性能，在药物动力学研究中具有应用潜力。

Abstract: Physics-Informed Neural Networks (PINNs) leverage machine learning with
differential equations to solve direct and inverse problems, ensuring
predictions follow physical laws. Physiologically based pharmacokinetic (PBPK)
modeling advances beyond classical compartmental approaches by using a
mechanistic, physiology focused framework. A PBPK model is based on a system of
ODEs, with each equation representing the mass balance of a drug in a
compartment, such as an organ or tissue. These ODEs include parameters that
reflect physiological, biochemical, and drug-specific characteristics to
simulate how the drug moves through the body. In this paper, we introduce
PBPK-iPINN, a method to estimate drug-specific or patient-specific parameters
and drug concentration profiles in PBPK brain compartment models using inverse
PINNs. We demonstrate that, for the inverse problem to converge to the correct
solution, the loss function components (data loss, initial conditions loss, and
residual loss) must be appropriately weighted, and parameters (including number
of layers, number of neurons, activation functions, learning rate, optimizer,
and collocation points) must be carefully tuned. The performance of the
PBPK-iPINN approach is then compared with established traditional numerical and
statistical methods.

</details>


### [120] [SURGIN: SURrogate-guided Generative INversion for subsurface multiphase flow with quantified uncertainty](https://arxiv.org/abs/2509.13189)
*Zhao Feng,Bicheng Yan,Luanxiao Zhao,Xianda Shen,Renyu Zhao,Wenhao Wang,Fengshou Zhang*

Main category: stat.ML

TL;DR: SURGIN是一种零样本条件生成的地下多相流数据同化框架，结合U-Net增强的傅里叶神经算子和基于分数的生成模型，无需任务特定重训练即可实时同化未见监测数据。


<details>
  <summary>Details</summary>
Motivation: 现有反演方法需要针对每个新的观测配置进行调整，缺乏实时同化未见监测数据的能力，需要开发一种无需重训练的零样本条件生成方法。

Method: 首先以自监督方式预训练无条件SGM捕获地质先验，然后利用可微分的U-FNO代理模型进行后验采样，实现基于未见观测的高效前向评估。

Result: 数值实验表明SURGIN能够很好地推断异质地层场并预测时空流动动力学，在不同测量设置下提供量化不确定性。

Conclusion: SURGIN通过将生成学习与代理引导的贝叶斯推断相结合，为参数函数空间中的反演建模和不确定性量化建立了新范式。

Abstract: We present a direct inverse modeling method named SURGIN, a SURrogate-guided
Generative INversion framework tailed for subsurface multiphase flow data
assimilation. Unlike existing inversion methods that require adaptation for
each new observational configuration, SURGIN features a zero-shot conditional
generation capability, enabling real-time assimilation of unseen monitoring
data without task-specific retraining. Specifically, SURGIN synergistically
integrates a U-Net enhanced Fourier Neural Operator (U-FNO) surrogate with a
score-based generative model (SGM), framing the conditional generation as a
surrogate prediction-guidance process in a Bayesian perspective. Instead of
directly learning the conditional generation of geological parameters, an
unconditional SGM is first pretrained in a self-supervised manner to capture
the geological prior, after which posterior sampling is performed by leveraging
a differentiable U-FNO surrogate to enable efficient forward evaluations
conditioned on unseen observations. Extensive numerical experiments demonstrate
SURGIN's capability to decently infer heterogeneous geological fields and
predict spatiotemporal flow dynamics with quantified uncertainty across diverse
measurement settings. By unifying generative learning with surrogate-guided
Bayesian inference, SURGIN establishes a new paradigm for inverse modeling and
uncertainty quantification in parametric functional spaces.

</details>
