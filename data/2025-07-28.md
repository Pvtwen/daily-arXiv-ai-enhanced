<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [cs.LG](#cs.LG) [Total: 52]
- [stat.ML](#stat.ML) [Total: 3]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Real-time rail vehicle localisation using spatially resolved magnetic field measurements](https://arxiv.org/abs/2507.19327)
*Niklas Dieckow,Katharina Ostaszewski,Philip Heinisch,Henriette Struckmann,Hendrik Ranocha*

Main category: eess.SP

TL;DR: 论文提出了两种基于磁场测量的实时铁路车辆定位方法，分别使用粒子滤波和序列对齐技术，并通过实验验证了其性能和适用场景。


<details>
  <summary>Details</summary>
Motivation: 为铁路车辆提供准确、稳健的实时定位，适用于安全关键应用。

Method: 1. 粒子滤波方法：通过磁相似性重加权，使用重尾非高斯核增强稳定性。2. 序列对齐方法：将实时磁信号转换为空间域并与地图匹配。

Result: 粒子滤波在21.6公里范围内实现亚5米精度，但在低速和冷启动时性能下降；序列对齐方法在冷启动场景表现优异，92%测试中定位误差小于30米。

Conclusion: 混合方法结合两种技术优势，提供适用于安全关键铁路应用的实时定位解决方案。

Abstract: This work presents two complementary real-time rail vehicle localization
methods based on magnetic field measurements and a pre-recorded magnetic map.
The first uses a particle filter reweighted via magnetic similarity, employing
a heavy-tailed non-Gaussian kernel for enhanced stability. The second is a
stateless sequence alignment technique that transforms real-time magnetic
signals into the spatial domain and matches them to the map using a similarity
measure. Experiments with operational train data show that the particle filter
achieves track-selective, sub-5-meter accuracy over 21.6 km, though its
performance degrades at low speeds and during cold starts. Accuracy tests were
constrained by the GNSS-based reference system. In contrast, the
alignment-based method excels in cold-start scenarios, localizing within 30 m
in 92 % of tests (100 % using top-3 matches). A hybrid approach combines both
methods$\unicode{x2014}$alignment-based initialization followed by particle
filter tracking. Runtime analysis confirms real-time capability on
consumer-grade hardware. The system delivers accurate, robust localization
suitable for safety-critical rail applications.

</details>


### [2] [Design and Implementation of Parametrized Look-Up Tables for Post-Correction of Oversampling Low-Resolution ADCs](https://arxiv.org/abs/2507.18673)
*Morriel Kasher,Michael Tinston,Predrag Spasojevic*

Main category: eess.SP

TL;DR: 提出了一种用于设计、优化和实现查找表（LUT）的框架，用于恢复噪声、过采样和量化信号，通过后量化数字解决方案模拟预量化抖动的频谱效果。


<details>
  <summary>Details</summary>
Motivation: 解决量化信号恢复中的频谱不纯问题，提供一种无需训练的模型驱动方法。

Method: 将LUT设计问题分解为四个阶段，采用三种抖动方法改善频谱纯度，并提出两种新颖的索引方案以减少内存开销。

Result: 在3位量化信号中，SFDR提高了19 dBc，内存占用仅324字节，且保持3位定点精度。

Conclusion: 该方法适用于低分辨率宽带设备，具有超低延迟特性。

Abstract: We propose a framework for the design, optimization, and implementation of
Look-Up Tables (LUTs) used to recover noisy, oversampled, quantized signals
given a parametric input model. The LUTs emulate the spectral effects of
pre-quantization dithering through an all-digital solution applied after
quantization. This methodology decomposes the intractable LUT design problem
into four distinct stages, each of which is addressed analytically using a
model-driven approach without reliance on training. Three dithering methods are
studied to improve spectral purity metrics. Two novel indexing schemes are
proposed to limit the LUT memory overhead shown to compress the LUT size by
over four orders of magnitude with marginal performance loss. The LUT design is
tested with an oversampled noisy sinusoidal input quantized to 3 bits and shown
to improve its Spurious-Free Dynamic Range (SFDR) by over 19 dBc with only 324
bytes of memory while maintaining the same 3-bit fixed-point precision at the
digital output. This correction can be implemented using two-level
combinational logic ensuring ultra-low latency and, hence, suitable for
low-resolution wideband devices.

</details>


### [3] [Exploiting Movable Antennas in NOMA Networks: Joint Beamforming, Power Allocation and Antenna Position Optimization](https://arxiv.org/abs/2507.18730)
*Yufeng Zhou,Wen Chen,Qingqing Wu,Xusheng Zhu,Zhendong Li,Kunlun Wang,Qiong Wu*

Main category: eess.SP

TL;DR: 论文研究了可移动天线（MA）辅助的下行链路非正交多址（NOMA）网络，以最大化系统吞吐量。通过联合优化基站波束成形、功率分配和天线位置，提出了一种高效的交替优化算法。


<details>
  <summary>Details</summary>
Motivation: 通过利用可移动天线的自由度，提升非正交多址网络的系统吞吐量。

Method: 采用交替优化框架，将问题分解为三个子问题，并利用SPCA和SCA技术处理非凸约束。

Result: 数值结果表明，所提系统在吞吐量上显著优于基准方法。

Conclusion: 通过联合优化天线位置和其他参数，可移动天线技术能显著提升NOMA网络的性能。

Abstract: This paper investigates the movable antenna (MA)- assisted downlink
non-orthogonal multiple access (NOMA) network to maximize system throughput. In
the considered scenario, both the base station (BS) and users are equipped with
MA, and a predetermined successive interference cancellation (SIC) decoding
order is adopted. Based on the field-response channel model, we formulate a
complex, non-convex problem to jointly optimize the BS beamforming, power
allocation, and MA positions at both the transmitter and receivers. To address
this, we propose an efficient algorithm based on an alternating optimization
(AO) framework, which decomposes the original problem into three distinct
subproblems. By employing sequential parametric convex approximation (SPCA) and
successive convex approximation (SCA) techniques, the non-convex constraints
within each subproblem are transformed into tractable. This methodology ensures
the algorithm converges to a stable, locally optimal solution. Numerical
results validate that the proposed system, which fully exploits the degrees of
freedom from antenna mobility at both ends, significantly outperforms
benchmarks in terms of throughput.

</details>


### [4] [Max-Min Rate Optimization for Multigroup Multicast MISO Systems Via Novel Transmissive RIS Transceiver](https://arxiv.org/abs/2507.18733)
*Yuan Guo,Wen Chen,Qingqing Wu,Yanze Zhu,Yang Liu,Zhendong Li,Ying Wang*

Main category: eess.SP

TL;DR: 论文研究了基于透射式可重构智能表面（RIS）收发器架构的多组多播下行通信系统，提出三种算法优化用户最小速率，并比较其性能与复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决多组多播通信中最大化用户最小速率的优化问题，同时降低计算复杂度。

Method: 1. 使用SCA和惩罚函数法的迭代解法；2. 基于WMMSE框架的SOCP方法；3. 结合平滑近似理论和MM方法的低复杂度无求解器算法。

Result: SOCP方法在最小速率和计算复杂度上优于惩罚法，低复杂度算法显著降低复杂度但性能略有下降。

Conclusion: 提出的三种算法有效解决了优化问题，SOCP方法性能最优，低复杂度算法适合资源受限场景。

Abstract: This paper investigates a novel transmissive reconfigurable intelligent
surface (RIS) transceiver architectureenabled multigroup multicast downlink
communication system. Under this setup, an optimization problem is formulated
to maximize the minimum rate of users across all groups, subject to the maximum
available power of each RIS unit. Due to the nondifferentiable nature of the
objective function, the max-min rate problem is challenging to solve. To tackle
this difficult problem, we develop an iterative solution by leveraging the
successive convex approximation (SCA) and the penalty function method. However,
the above approach has high computational complexity and may lead to
compromised performance. To overcome these drawbacks, we design an efficient
second-order cone programming (SOCP)-based method using the weighted minimum
mean squared error (WMMSE) framework to reduce computational complexity.
Furthermore, to further reduce the computational complexity, we also propose a
low-complexity and solver-free algorithm that analytically updates all
variables by combining the smooth approximation theory and the
majorization-minimization (MM) method. Numerical results are provided to verify
the convergence and effectiveness of our proposed three algorithms. It is also
demonstrated that the SOCP-based method outperforms the penalty-based algorithm
in terms of both the achieved min rate and the computational complexity. In
contrast, the lowcomplexity design achieves significantly lower complexity with
only slightly degraded performance.

</details>


### [5] [Max-Min Fairness-Oriented Beamforming Design in HAPS-Enabled ISAC for 6G Networks](https://arxiv.org/abs/2507.18764)
*Parisa Kanani,Mohammad Javad Omidi,Mahmoud Modarres-Hashemi,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: 本文提出了一种基于高空平台站（HAPS）的集成感知与通信（ISAC）系统，用于6G网络，通过优化资源分配实现公平性和高效性。


<details>
  <summary>Details</summary>
Motivation: 解决6G网络中服务公平分配的需求，同时提升通信和感知性能。

Method: 采用非凸优化方法，通过最大最小公平性平衡感知波束增益和通信用户的SINR需求，并满足功率约束。

Result: 仿真结果表明，HAPS-ISAC框架能实现高效资源分配、可靠覆盖和更高感知精度。

Conclusion: HAPS-ISAC是6G网络和集成通信感知系统的关键推动者。

Abstract: This paper presents a high-altitude platform station (HAPS)-enabled
integrated sensing and communication (ISAC) system designed for
sixth-generation (6G) networks. Positioned in the stratosphere, HAPS serves as
a super-macro base station, leveraging advanced beamforming techniques to
enable communication and sensing simultaneously. This research addresses the
need for equitable service distribution in 6G networks by focusing on fairness
within the HAPS-ISAC system. It tackles a non-convex optimization problem that
balances sensing beampattern gain and signal-to-interference-plus-noise ratio
(SINR) requirements among communication users (CUs) using a max-min fairness
approach while adhering to power constraints. The proposed HAPS-ISAC framework
ensures efficient resource allocation, reliable coverage, and improved sensing
accuracy. Simulation results validate the potential of HAPS-ISAC as a pivotal
enabler for 6G networks and integrated communication-sensing systems.

</details>


### [6] [Flexible Intelligent Metasurfaces in High-Mobility MIMO Integrated Sensing and Communications](https://arxiv.org/abs/2507.18793)
*Kuranage Roche Rayan Ranasinghe,Jiancheng An,Iván Alexander Morales Sandoval,Hyeon Seok Rou,Giuseppe Thadeu Freitas de Abreu,Chau Yuen,Mérouane Debbah*

Main category: eess.SP

TL;DR: 提出了一种新型的双色散MIMO信道模型，结合柔性智能超表面（FIM），适用于高移动性场景中的集成感知与通信（ISAC）。


<details>
  <summary>Details</summary>
Motivation: 针对高移动性场景中的ISAC需求，研究如何利用FIM技术优化信道模型和波形设计。

Method: 提出FIM参数化的双色散信道模型（FPDD），并将其应用于OFDM、OTFS和AFDM波形，通过梯度上升算法最大化可实现速率。

Result: 数值结果表明，FIM技术对可实现速率有显著影响，参数化设计对优化ISAC性能至关重要。

Conclusion: FIM技术在双色散信道中具有潜力，需精心设计参数以实现高性能ISAC。

Abstract: We propose a novel doubly-dispersive (DD) multiple-input multiple-output
(MIMO) channel model incorporating flexible intelligent metasurfaces (FIMs),
which is suitable for integrated sensing and communications (ISAC) in
high-mobility scenarios. We then discuss how the proposed FIM-parameterized DD
(FPDD) channel model can be applied in a logical manner to ISAC waveforms that
are known to perform well in DD environments, namely, orthogonal frequency
division multiplexing (OFDM), orthogonal time frequency space (OTFS), and
affine frequency division multiplexing (AFDM). Leveraging the proposed model,
we formulate an achievable rate maximization problem with a strong sensing
constraint for all the aforementioned waveforms, which we then solve via a
gradient ascent algorithm with closed-form gradients presented as a bonus. Our
numerical results indicate that the achievable rate is significantly impacted
by the emerging FIM technology with careful parametrization essential in
obtaining strong ISAC performance across all waveforms suitable to mitigating
the effects of DD channels.

</details>


### [7] [A Fingerprint Database Generation Method for RIS-Assisted Indoor Positioning](https://arxiv.org/abs/2507.18927)
*Xin Cheng,Yu He,Menglu Li,Ruoguang Li,Feng Shu,Guangjie Han*

Main category: eess.SP

TL;DR: 提出了一种生成RIS辅助RSS指纹数据库的新方法，通过扩展的基于簇的信道建模和RIS/Tx的物理电磁特性，解决了空间一致性问题，并评估了定位性能。


<details>
  <summary>Details</summary>
Motivation: 解决RIS辅助室内定位中缺乏真实且空间一致的信道建模方法的问题。

Method: 采用扩展的基于簇的信道建模，结合RIS和Tx的物理电磁特性，模拟空间一致性的指纹数据收集。

Result: 通过仿真验证了方法的有效性，并分析了KNN和DNN在数据库上的定位性能。

Conclusion: 该方法为RIS辅助室内定位系统设计提供了有价值的参考。

Abstract: Reconfigurable intelligent surface (RIS) has emerged as a promising
technology to enhance indoor wireless communication and sensing performance.
However, the construction of reliable received signal strength (RSS)-based
fingerprint databases for RIS-assisted indoor positioning remains an open
challenge due to the lack of realistic and spatially consistent channel
modeling methods. In this paper, we propose a novel method with open-source
codes for generating RIS-assisted RSS fingerprint databases. Our method
captures the complex RIS-assisted multipath behaviors by extended cluster-based
channel modeling and the physical and electromagnetic properties of RIS and
transmitter (Tx). And the spatial consistency is incorporated when simulating
the fingerprint data collection across neighboring positions. Furthermore, the
proposed method offers exceptional flexibility in configuring RIS and Tx
parameters. Extensive simulations are conducted to evaluate the fingerprint
database generated by the proposed method. Moreover, the positioning
performance on the database using K-nearest neighbors (KNN) and deep neural
network (DNN) is analyzed, providing valuable insights for the system design.

</details>


### [8] [Assessing the Reliability and Validity of a Balance Mat for Measuring Postural Stability: A Combined Robot-Human Approach](https://arxiv.org/abs/2507.18943)
*Abishek Shrestha,Damith Herath,Angie Fearon,Maryam Ghahramani*

Main category: eess.SP

TL;DR: 研究评估了一种低成本便携式平衡垫（BM）的可靠性和有效性，作为实验室标准力板（FP）的替代方案。通过机器人实验和人体实验验证了BM的性能，结果显示BM在适当校准后具有良好的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 力板（FP）作为实验室标准设备存在便携性和专业要求高的限制，需要一种低成本便携的替代方案。

Method: 研究分为机器人实验和人体实验。机器人实验使用UR10机械臂生成受控摆动模式评估BM的可靠性和敏感性；人体实验通过51名健康年轻参与者在BM和FP上完成平衡任务，比较摆动指标。

Result: 机器人实验显示BM在单腿和双腿站立中具有良好到优秀的可靠性（ICC>0.75）。人体实验显示BM与FP在摆动路径和范围上具有中等到强相关性，但BM会高估摆动指标，校准后一致性提高。

Conclusion: BM在适当校准后表现出可靠的摆动测量能力，可作为FP的有效替代方案。

Abstract: Postural sway assessment is important for detecting balance problems and
identifying people at risk of falls. Force plates (FP) are considered the gold
standard postural sway assessment method in laboratory conditions, but their
lack of portability and requirement of high-level expertise limit their
widespread usage. This study evaluates the reliability and validity of a novel
Balance Mat (BM) device, a low-cost portable alternative that uses optical
fibre technology. The research includes two studies: a robot study and a human
study. In the robot study, a UR10 robotic arm was used to obtain controlled
sway patterns to assess the reliability and sensitivity of the BM. In the human
study, 51 healthy young participants performed balance tasks on the BM in
combination with an FP to evaluate the BM's validity. Sway metrics such as sway
mean, sway absolute mean, sway root mean square (RMS), sway path, sway range,
and sway velocity were calculated from both BM and FP and compared. Reliability
was evaluated using the intra-class correlation coefficient (ICC), where values
greater than 0.9 were considered excellent and values between 0.75 and 0.9 were
considered good. Results from the robot study demonstrated good to excellent
ICC values in both single and double-leg stances. The human study showed
moderate to strong correlations for sway path and range. Using Bland-Altman
plots for agreement analysis revealed proportional bias between the BM and the
FP where the BM overestimated sway metrics compared to the FP. Calibration was
used to improve the agreement between the devices. The device demonstrated
consistent sway measurement across varied stance conditions, establishing both
reliability and validity following appropriate calibration.

</details>


### [9] [Max-Min Beamforming for Large-Scale Cell-Free Massive MIMO: A Randomized ADMM Algorithm](https://arxiv.org/abs/2507.18980)
*Bin Wang,Jun Fang,Yue Xiao,Martin Haardt*

Main category: eess.SP

TL;DR: 提出了一种随机ADMM算法，用于解决大规模最大-最小波束成形问题，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有确定性优化方法在问题规模增大时计算效率低，需要更高效的解决方案。

Method: 将可行性检查问题转化为线性约束优化问题，并开发随机ADMM算法，每次迭代仅需解决少量子问题。

Result: 算法具有O(1/\bar{t})收敛速度，数值结果显示其复杂度显著优于现有方法。

Conclusion: 随机ADMM算法在大规模MMB问题中具有高效性和实用性。

Abstract: We consider the problem of max-min beamforming (MMB) for cell-free massive
multi-input multi-output (MIMO) systems, where the objective is to maximize the
minimum achievable rate among all users. Existing MMB methods are mainly based
on deterministic optimization methods, which are computationally inefficient
when the problem size grows large. To address this issue, we, in this paper,
propose a randomized alternating direction method of multiplier (ADMM)
algorithm for large-scale MMB problems. We first propose a novel formulation
that transforms the highly challenging feasibility-checking problem into a
linearly constrained optimization problem. An efficient randomized ADMM is then
developed for solving the linearly constrained problem. Unlike standard ADMM,
randomized ADMM only needs to solve a small number of subproblems at each
iteration to ensure convergence, thus achieving a substantial complexity
reduction. Our theoretical analysis reveals that the proposed algorithm
exhibits an O(1/\bar{t}) convergence rate (\bar{t} represents the number of
iterations), which is on the same order as its deterministic counterpart.
Numerical results show that the proposed algorithm offers a significant
complexity advantage over existing methods in solving the MMB problem.

</details>


### [10] [Machine Learning based Radio Environment Map Estimation for Indoor Visible Light Communication](https://arxiv.org/abs/2507.19149)
*Helena Serpi,Christina,Politi*

Main category: eess.SP

TL;DR: 提出了一种基于机器学习的创新方法，用于光无线通信中的无线电地图估计，替代传统仿真技术。


<details>
  <summary>Details</summary>
Motivation: 传统仿真技术复杂且耗时，需要更高效、准确的方法来实时估计室内可见光通信（VLC）系统的信号传播。

Method: 采用多层感知器（MLP）表示室内VLC系统，通过调整MLP参数（如样本量、训练周期和批次大小）优化性能。

Result: 该方法在仿真和性能预测上准确、快速，且训练样本需求较少，适合实时估计。

Conclusion: 通过调整MLP参数，可在推理精度和训练时间之间取得平衡，满足实时需求。

Abstract: An innovative method for radio map estimation in optical wireless
communications is proposed that is based on Machine Learning rather than
simulation techniques. Multi-Layer Perceptron (MLP) representation of indoor
Visible Light Communication (VLC) systems is suggested, and signal propagation
is estimated. The simulation and performance predictions are accurate, fast and
require a reduced set of training sample size with respect to other
counterparts, making this solution very suitable for real time estimation of an
indoor VLC system. It is shown that by tweaking MLP parameters, such as sample
size, number of epochs and batch size, one can balance the desired level of
inference accuracy with training time and optimize the model's performance to
meet real-time requirements.

</details>


### [11] [High-Fidelity RF Mapping: Assessing Environmental Modeling in 6G Network Digital Twins](https://arxiv.org/abs/2507.19173)
*Lorenzo Cazzella,Francesco Linsalata,Damiano Badini,Matteo Matteucci,Maurizio Magarini,Umberto Spagnolini*

Main category: eess.SP

TL;DR: 论文提出了两种度量方法（HRT和CRT）来比较不同环境建模精度下的射线追踪模拟结果，并通过高保真数字孪生模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 评估不同环境建模精度对数字孪生（DT）模拟的影响，以确定模型特征对高效准确模拟的重要性。

Method: 提出HRT和CRT两种度量方法，结合NVIDIA Sionna RT和SUMO交通模拟器，在28 GHz频段进行网格和车辆射线追踪模拟。

Result: HRT和CRT成功识别了因环境变化导致的模拟差异，车辆轨迹模拟揭示了传播特征的动态变化。

Conclusion: 提出的度量方法能有效评估环境建模精度对数字孪生模拟的影响，为优化建模提供了工具。

Abstract: The design of accurate Digital Twins (DTs) of electromagnetic environments
strictly depends on the fidelity of the underlying environmental modeling.
Evaluating the differences among diverse levels of modeling accuracy is key to
determine the relevance of the model features towards both efficient and
accurate DT simulations. In this paper, we propose two metrics, the Hausdorff
ray tracing (HRT) and chamfer ray tracing (CRT) distances, to consistently
compare the temporal, angular and power features between two ray tracing
simulations performed on 3D scenarios featured by environmental changes. To
evaluate the introduced metrics, we considered a high-fidelity digital twin
model of an area of Milan, Italy and we enriched it with two different types of
environmental changes: (i) the inclusion of parked vehicles meshes, and (ii)
the segmentation of the buildings facade faces to separate the windows mesh
components from the rest of the building. We performed grid-based and vehicular
ray tracing simulations at 28 GHz carrier frequency on the obtained scenarios
integrating the NVIDIA Sionna RT ray tracing simulator with the SUMO vehicular
traffic simulator. Both the HRT and CRT metrics highlighted the areas of the
scenarios where the simulated radio propagation features differ owing to the
introduced mesh integrations, while the vehicular ray tracing simulations
allowed to uncover the distance patterns arising along realistic vehicular
trajectories.

</details>


### [12] [Bespoke multiresolution analysis of graph signals](https://arxiv.org/abs/2507.19181)
*Giacomo Elefante,Gianluca Giacchi,Michael Multerer,Jacopo Quizi*

Main category: eess.SP

TL;DR: 提出了一种基于图信号的多分辨率分析框架，利用样本变换（samplet transform）实现高效压缩和分析。


<details>
  <summary>Details</summary>
Motivation: 传统Haar小波方法在图信号分析中存在局限性，需要一种更通用的框架来支持更广泛的信号类型。

Method: 将图划分为多个子图，嵌入欧几里得空间构建样本变换，再映射回图结构，确保正交性、局部性和消失矩性质。

Result: 方法在压缩效率和多分辨率保真度上显著优于传统Haar小波，适用于光滑流形上的图信号。

Conclusion: 样本变换为图信号分析提供了更灵活和高效的框架，扩展了可压缩信号的范围。

Abstract: We present a novel framework for discrete multiresolution analysis of graph
signals. The main analytical tool is the samplet transform, originally defined
in the Euclidean framework as a discrete wavelet-like construction, tailored to
the analysis of scattered data. The first contribution of this work is defining
samplets on graphs. To this end, we subdivide the graph into a fixed number of
patches, embed each patch into a Euclidean space, where we construct samplets,
and eventually pull the construction back to the graph. This ensures
orthogonality, locality, and the vanishing moments property with respect to
properly defined polynomial spaces on graphs. Compared to classical Haar
wavelets, this framework broadens the class of graph signals that can
efficiently be compressed and analyzed. Along this line, we provide a
definition of a class of signals that can be compressed using our construction.
We support our findings with different examples of signals defined on graphs
whose vertices lie on smooth manifolds. For efficient numerical implementation,
we combine heavy edge clustering, to partition the graph into meaningful
patches, with landmark \texttt{Isomap}, which provides low-dimensional
embeddings for each patch. Our results demonstrate the method's robustness,
scalability, and ability to yield sparse representations with controllable
approximation error, significantly outperforming traditional Haar wavelet
approaches in terms of compression efficiency and multiresolution fidelity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Diffusion Models for Solving Inverse Problems via Posterior Sampling with Piecewise Guidance](https://arxiv.org/abs/2507.18654)
*Saeed Mohseni-Sehdeh,Walid Saad,Kei Sakaguchi,Tao Yu*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的分段引导框架，用于解决逆问题，平衡计算效率与准确性，适用于多种任务且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高维数据时表现优异，但现有方法在解决逆问题时通常需要针对特定任务重新训练，且未充分考虑测量噪声。本文旨在提出一种通用且高效的方法。

Method: 引入分段引导机制，根据扩散时间步长定义引导项，分别在高噪声和低噪声阶段采用不同近似方法，同时显式纳入测量噪声。

Result: 在图像修复任务（如修复和超分辨率）中，推理时间显著减少（25%和23-24%），且PSNR和SSIM损失可忽略。

Conclusion: 提出的分段引导框架在解决逆问题时具有高效性和通用性，适用于多种任务且无需重新训练，同时保持高准确性。

Abstract: Diffusion models are powerful tools for sampling from high-dimensional
distributions by progressively transforming pure noise into structured data
through a denoising process. When equipped with a guidance mechanism, these
models can also generate samples from conditional distributions. In this paper,
a novel diffusion-based framework is introduced for solving inverse problems
using a piecewise guidance scheme. The guidance term is defined as a piecewise
function of the diffusion timestep, facilitating the use of different
approximations during high-noise and low-noise phases. This design is shown to
effectively balance computational efficiency with the accuracy of the guidance
term. Unlike task-specific approaches that require retraining for each problem,
the proposed method is problem-agnostic and readily adaptable to a variety of
inverse problems. Additionally, it explicitly incorporates measurement noise
into the reconstruction process. The effectiveness of the proposed framework is
demonstrated through extensive experiments on image restoration tasks,
specifically image inpainting and super-resolution. Using a class conditional
diffusion model for recovery, compared to the \pgdm baseline, the proposed
framework achieves a reduction in inference time of \(25\%\) for inpainting
with both random and center masks, and \(23\%\) and \(24\%\) for \(4\times\)
and \(8\times\) super-resolution tasks, respectively, while incurring only
negligible loss in PSNR and SSIM.

</details>


### [14] [Efficient Knowledge Tracing Leveraging Higher-Order Information in Integrated Graphs](https://arxiv.org/abs/2507.18668)
*Donghee Han,Daehee Kim,Minjun Lee,Daeyoung Roh,Keejun Han,Mun Yong Yi*

Main category: cs.LG

TL;DR: DGAKT是一种基于双图注意力机制的知识追踪模型，通过子图处理提高计算效率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法在处理大规模图和长学习序列时计算成本高，DGAKT旨在解决这一问题。

Method: 采用图神经网络模型，利用学生-习题-知识点的子图关系，通过子图处理减少计算资源需求。

Result: 实验表明DGAKT在性能和资源效率上均优于现有模型。

Conclusion: DGAKT为知识追踪领域提供了高效且性能优越的新方法。

Abstract: The rise of online learning has led to the development of various knowledge
tracing (KT) methods. However, existing methods have overlooked the problem of
increasing computational cost when utilizing large graphs and long learning
sequences. To address this issue, we introduce Dual Graph Attention-based
Knowledge Tracing (DGAKT), a graph neural network model designed to leverage
high-order information from subgraphs representing student-exercise-KC
relationships. DGAKT incorporates a subgraph-based approach to enhance
computational efficiency. By processing only relevant subgraphs for each target
interaction, DGAKT significantly reduces memory and computational requirements
compared to full global graph models. Extensive experimental results
demonstrate that DGAKT not only outperforms existing KT models but also sets a
new standard in resource efficiency, addressing a critical need that has been
largely overlooked by prior KT approaches.

</details>


### [15] [Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling](https://arxiv.org/abs/2507.18671)
*Ning Liao,Xiaoxing Wang,Zehao Lin,Weiyang Guo,Feng Hong,Shixiang Song,Geng Yu,Zihua Zhao,Sitao Xie,Longxuan Wei,Xiangqi Jin,Xiaohan Qin,Jiale Ma,Kai Chen,Jiangchao Yao,Zhouhan Lin,Junchi Yan,Zhiyu Li,Feiyu Xiong,Yanfeng Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: Innovator通过将预训练的密集LLM升级为细粒度Mixtures-of-Experts模型，解决了科学数据持续预训练中的灾难性遗忘问题，并在科学任务中表现优异，同时保留通用能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在科学数据持续预训练中出现的灾难性遗忘问题，同时保持通用任务能力。

Method: 采用四阶段训练范式：科学专家诱导、细粒度专家拆分、科学感知路由预热、通用-科学专家整合训练。

Result: 在30项科学任务中平均提升25%，通用任务性能保留99%；推理增强版Innovator-Reason在复杂科学问题中表现提升30%。

Conclusion: Innovator通过专家模型设计有效分离和整合通用与科学知识，显著提升科学任务性能且不影响通用能力。

Abstract: A large language model (LLM) with knowledge in both scientific and general
tasks is the foundation of science general intelligence. However, directly
continued pretraining an LLM using science data usually leads to catastrophic
forgetting, which indicates severe degradation in general ability. In this
report, we present Innovator, which solves this problem by upcycling a
pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during
continued pretraining, where different experts are expected to learn science
knowledge in different disciplines, and a shared expert is utilized for general
tasks. Innovator introduces a four-stage upcycle training paradigm: (1)
Scientific Expert Induction on discipline-specific data, (2) Fine-grained
Expert Splitting via FFN dimension decomposition, (3) Science-Aware Routing
warmup, and (4) Generalist-Scientist Integration training on hybrid datasets.
Such a paradigm enables knowledge in the general domain, and different
scientific disciplines can be decoupled, avoiding the negative influence among
knowledge in different domains. With 53.3B total parameters and 13.3B
activated, Innovator extends Qwen2.5-7B using a shared general expert and 64
specialized scientific experts with 8 activated. Trained on 300B tokens with
tri-level quality-controlled data, Innovator achieves 25% average improvement
across 30 scientific tasks with a win rate as 70%, while retaining 99%
performance in general tasks. Furthermore, Innovator-Reason, which is
post-trained from Innovator for reasoning boosting, exhibits excellent
reasoning performance in solving complex scientific problems with improvements
over 30%.

</details>


### [16] [Market Making Strategies with Reinforcement Learning](https://arxiv.org/abs/2507.18680)
*Óscar Fernández Vicente*

Main category: cs.LG

TL;DR: 该论文研究了如何利用强化学习（尤其是深度强化学习）开发自适应且盈利的做市策略，解决了库存风险、竞争和非稳态市场动态等挑战。


<details>
  <summary>Details</summary>
Motivation: 做市商在提供流动性方面扮演重要角色，但面临库存风险、竞争和非稳态市场动态等挑战。研究旨在探索强化学习如何帮助开发自适应且盈利的做市策略。

Method: 研究将做市任务建模为强化学习问题，设计了单智能体和多智能体环境下的策略，并采用奖励工程和多目标强化学习（MORL）管理库存。此外，提出了一种基于折扣汤普森采样的POW-dTS算法，以应对非稳态问题。

Result: 实验结果表明，基于强化学习的方法在多种性能指标上显著优于传统和基线算法策略。

Conclusion: 该研究为设计稳健、高效且自适应的做市智能体提供了新方法和见解，强化了强化学习在复杂金融系统中变革算法交易的潜力。

Abstract: This thesis presents the results of a comprehensive research project focused
on applying Reinforcement Learning (RL) to the problem of market making in
financial markets. Market makers (MMs) play a fundamental role in providing
liquidity, yet face significant challenges arising from inventory risk,
competition, and non-stationary market dynamics. This research explores how RL,
particularly Deep Reinforcement Learning (DRL), can be employed to develop
autonomous, adaptive, and profitable market making strategies.
  The study begins by formulating the MM task as a reinforcement learning
problem, designing agents capable of operating in both single-agent and
multi-agent settings within a simulated financial environment. It then
addresses the complex issue of inventory management using two complementary
approaches: reward engineering and Multi-Objective Reinforcement Learning
(MORL). While the former uses dynamic reward shaping to guide behavior, the
latter leverages Pareto front optimization to explicitly balance competing
objectives.
  To address the problem of non-stationarity, the research introduces POW-dTS,
a novel policy weighting algorithm based on Discounted Thompson Sampling. This
method allows agents to dynamically select and combine pretrained policies,
enabling continual adaptation to shifting market conditions.
  The experimental results demonstrate that the proposed RL-based approaches
significantly outperform traditional and baseline algorithmic strategies across
various performance metrics. Overall, this research thesis contributes new
methodologies and insights for the design of robust, efficient, and adaptive
market making agents, reinforcing the potential of RL to transform algorithmic
trading in complex financial systems.

</details>


### [17] [Concept Probing: Where to Find Human-Defined Concepts (Extended Version)](https://arxiv.org/abs/2507.18681)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.LG

TL;DR: 提出了一种自动确定神经网络中哪一层最适合用于概念探测的方法，基于表示的信息量和规律性。


<details>
  <summary>Details</summary>
Motivation: 概念探测的性能高度依赖于所探测的内部表示层，因此需要一种方法来自动识别最适合的层。

Method: 基于表示的信息量和规律性，自动选择适合探测给定概念的神经网络层。

Result: 通过在不同模型和数据集上的实证分析验证了方法的有效性。

Conclusion: 该方法能够有效识别适合概念探测的神经网络层，提升探测性能。

Abstract: Concept probing has recently gained popularity as a way for humans to peek
into what is encoded within artificial neural networks. In concept probing,
additional classifiers are trained to map the internal representations of a
model into human-defined concepts of interest. However, the performance of
these probes is highly dependent on the internal representations they probe
from, making identifying the appropriate layer to probe an essential task. In
this paper, we propose a method to automatically identify which layer's
representations in a neural network model should be considered when probing for
a given human-defined concept of interest, based on how informative and regular
the representations are with respect to the concept. We validate our findings
through an exhaustive empirical analysis over different neural network models
and datasets.

</details>


### [18] [The Right to be Forgotten in Pruning: Unveil Machine Unlearning on Sparse Models](https://arxiv.org/abs/2507.18725)
*Yang Xiao,Gen Li,Jie Ji,Ruimeng Ye,Xiaolong Ma,Bo Hui*

Main category: cs.LG

TL;DR: 本文提出了一种称为“un-pruning”的新方法，用于消除稀疏模型中删除数据对剪枝拓扑的影响，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏模型中删除数据对剪枝拓扑的影响，并解决“被遗忘权”问题。

Method: 定义“un-pruning”术语，提出一种近似保留数据驱动的剪枝拓扑的算法，并与现有遗忘算法结合。

Result: 实验表明，MIA评估不可靠，设计了新的性能指标验证un-pruning的有效性。

Conclusion: un-pruning算法在理论和实验中均表现良好，适用于多种稀疏模型和遗忘算法。

Abstract: Machine unlearning aims to efficiently eliminate the memory about deleted
data from trained models and address the right to be forgotten. Despite the
success of existing unlearning algorithms, unlearning in sparse models has not
yet been well studied. In this paper, we empirically find that the deleted data
has an impact on the pruned topology in a sparse model. Motivated by the
observation and the right to be forgotten, we define a new terminology
``un-pruning" to eliminate the impact of deleted data on model pruning. Then we
propose an un-pruning algorithm to approximate the pruned topology driven by
retained data. We remark that any existing unlearning algorithm can be
integrated with the proposed un-pruning workflow and the error of un-pruning is
upper-bounded in theory. Also, our un-pruning algorithm can be applied to both
structured sparse models and unstructured sparse models. In the experiment, we
further find that Membership Inference Attack (MIA) accuracy is unreliable for
assessing whether a model has forgotten deleted data, as a small change in the
amount of deleted data can produce arbitrary MIA results. Accordingly, we
devise new performance metrics for sparse models to evaluate the success of
un-pruning. Lastly, we conduct extensive experiments to verify the efficacy of
un-pruning with various pruning methods and unlearning algorithms. Our code is
released at https://anonymous.4open.science/r/UnlearningSparseModels-FBC5/.

</details>


### [19] [Exploitation Over Exploration: Unmasking the Bias in Linear Bandit Recommender Offline Evaluation](https://arxiv.org/abs/2507.18756)
*Pedro R. Pires,Gregorio F. Azevedo,Pietro L. Campos,Rafael T. Sereicikas,Tiago A. Almeida*

Main category: cs.LG

TL;DR: 研究发现，在多臂老虎机（MAB）算法的离线评估中，纯利用策略（贪婪线性模型）在90%的数据集中表现优于或等同于探索策略，揭示了离线评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨MAB算法中探索与利用的权衡，并评估离线评估方法是否能准确反映探索行为的有效性。

Method: 通过广泛的离线实证比较，分析多种线性MAB算法的性能，包括贪婪线性模型和不同探索策略的变体。

Result: 贪婪线性模型在大多数数据集中表现最佳，超参数优化也倾向于最小化探索的配置。

Conclusion: 离线评估方法在反映探索行为方面存在显著不足，亟需开发更鲁棒的评估框架。

Abstract: Multi-Armed Bandit (MAB) algorithms are widely used in recommender systems
that require continuous, incremental learning. A core aspect of MABs is the
exploration-exploitation trade-off: choosing between exploiting items likely to
be enjoyed and exploring new ones to gather information. In contextual linear
bandits, this trade-off is particularly central, as many variants share the
same linear regression backbone and differ primarily in their exploration
strategies. Despite its prevalent use, offline evaluation of MABs is
increasingly recognized for its limitations in reliably assessing exploration
behavior. This study conducts an extensive offline empirical comparison of
several linear MABs. Strikingly, across over 90% of various datasets, a greedy
linear model, with no type of exploration, consistently achieves top-tier
performance, often outperforming or matching its exploratory counterparts. This
observation is further corroborated by hyperparameter optimization, which
consistently favors configurations that minimize exploration, suggesting that
pure exploitation is the dominant strategy within these evaluation settings.
Our results expose significant inadequacies in offline evaluation protocols for
bandits, particularly concerning their capacity to reflect true exploratory
efficacy. Consequently, this research underscores the urgent necessity for
developing more robust assessment methodologies, guiding future investigations
into alternative evaluation frameworks for interactive learning in recommender
systems.

</details>


### [20] [CLEAR: Unlearning Spurious Style-Content Associations with Contrastive LEarning with Anti-contrastive Regularization](https://arxiv.org/abs/2507.18794)
*Minghui Sun,Benjamin A. Goldstein,Matthew M. Engelhard*

Main category: cs.LG

TL;DR: CLEAR框架通过对比学习和反对比正则化分离任务相关和无关特征，提升测试时特征变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在医疗等应用中，确保模型特征不受种族、性别等无关特征影响，以实现公平和泛化。

Method: 提出CLEAR框架，结合对比学习和Pair-Switching正则化，最小化无关特征与任务标签的互信息。

Result: CLEAR-VAE能交换和插值内容与风格，并在新特征组合下提升分类性能。

Conclusion: CLEAR有效分离任务相关与无关特征，提升模型泛化能力。

Abstract: Learning representations unaffected by superficial characteristics is
important to ensure that shifts in these characteristics at test time do not
compromise downstream prediction performance. For instance, in healthcare
applications, we might like to learn features that contain information about
pathology yet are unaffected by race, sex, and other sources of physiologic
variability, thereby ensuring predictions are equitable and generalizable
across all demographics. Here we propose Contrastive LEarning with
Anti-contrastive Regularization (CLEAR), an intuitive and easy-to-implement
framework that effectively separates essential (i.e., task-relevant)
characteristics from superficial (i.e., task-irrelevant) characteristics during
training, leading to better performance when superficial characteristics shift
at test time. We begin by supposing that data representations can be
semantically separated into task-relevant content features, which contain
information relevant to downstream tasks, and task-irrelevant style features,
which encompass superficial attributes that are irrelevant to these tasks, yet
may degrade performance due to associations with content present in training
data that do not generalize. We then prove that our anti-contrastive penalty,
which we call Pair-Switching (PS), minimizes the Mutual Information between the
style attributes and content labels. Finally, we instantiate CLEAR in the
latent space of a Variational Auto-Encoder (VAE), then perform experiments to
quantitatively and qualitatively evaluate the resulting CLEAR-VAE over several
image datasets. Our results show that CLEAR-VAE allows us to: (a) swap and
interpolate content and style between any pair of samples, and (b) improve
downstream classification performance in the presence of previously unseen
combinations of content and style. Our code will be made publicly available.

</details>


### [21] [Ralts: Robust Aggregation for Enhancing Graph Neural Network Resilience on Bit-flip Errors](https://arxiv.org/abs/2507.18804)
*Wencheng Zou,Nan Wu*

Main category: cs.LG

TL;DR: 该论文研究了图神经网络（GNNs）在硬件故障（如位翻转错误）下的鲁棒性，并提出了一种轻量级解决方案Ralts，通过图相似性度量增强GNN的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于GNN在金融和医疗等安全关键领域的广泛应用，硬件故障可能导致严重后果，而现有研究多关注软件层面的威胁，硬件故障的研究较少。

Method: 论文首先分析了GNN对位翻转错误的鲁棒性，然后提出Ralts，利用图相似性度量过滤异常并恢复受损的图拓扑，将保护技术直接集成到聚合函数中。

Result: 实验表明，Ralts显著提升了GNN的鲁棒性，在不同模型、数据集和错误模式下，预测准确性平均提高至少10%-20%。

Conclusion: Ralts是一种高效且通用的解决方案，能够增强GNN对硬件故障的鲁棒性，同时保持与现有框架相当的执行效率。

Abstract: Graph neural networks (GNNs) have been widely applied in safety-critical
applications, such as financial and medical networks, in which compromised
predictions may cause catastrophic consequences. While existing research on GNN
robustness has primarily focused on software-level threats, hardware-induced
faults and errors remain largely underexplored. As hardware systems progress
toward advanced technology nodes to meet high-performance and energy efficiency
demands, they become increasingly susceptible to transient faults, which can
cause bit flips and silent data corruption, a prominent issue observed by major
technology companies (e.g., Meta and Google). In response, we first present a
comprehensive analysis of GNN robustness against bit-flip errors, aiming to
reveal system-level optimization opportunities for future reliable and
efficient GNN systems. Second, we propose Ralts, a generalizable and
lightweight solution to bolster GNN resilience to bit-flip errors.
Specifically, Ralts exploits various graph similarity metrics to filter out
outliers and recover compromised graph topology, and incorporates these
protective techniques directly into aggregation functions to support any
message-passing GNNs. Evaluation results demonstrate that Ralts effectively
enhances GNN robustness across a range of GNN models, graph datasets, error
patterns, and both dense and sparse architectures. On average, under a BER of
$3\times10^{-5}$, these robust aggregation functions improve prediction
accuracy by at least 20\% when errors present in model weights or node
embeddings, and by at least 10\% when errors occur in adjacency matrices. Ralts
is also optimized to deliver execution efficiency comparable to built-in
aggregation functions in PyTorch Geometric.

</details>


### [22] [Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator](https://arxiv.org/abs/2507.18807)
*YuXin Li,Felix Dangel,Derek Tam,Colin Raffel*

Main category: cs.LG

TL;DR: 论文提出了一种名为“Squisher”的方法，通过复用训练过程中已计算的平方梯度累加器来近似Fisher对角矩阵，以减少计算成本。实验表明其性能与Fisher对角矩阵相当，且优于基线方法。


<details>
  <summary>Details</summary>
Motivation: Fisher对角矩阵常用于衡量参数敏感性，但其计算成本较高。自适应梯度方法（如Adam）在训练过程中已计算平方梯度的移动平均，因此探索是否可以利用这些信息近似Fisher对角矩阵。

Method: 提出“Squisher”方法，复用训练中已计算的平方梯度累加器作为Fisher对角矩阵的近似。通过五个应用实验验证其有效性。

Result: 实验表明，Squisher在性能上与Fisher对角矩阵相似，且优于基线方法。同时量化了Squisher与Fisher对角矩阵的具体差异及其影响。

Conclusion: Squisher提供了一种计算高效的Fisher对角矩阵近似方法，性能接近原始方法，适用于多种应用场景。

Abstract: The diagonal of a model's Fisher Information Matrix (the "Fisher diagonal")
has frequently been used as a way to measure parameter sensitivity. Typically,
the Fisher diagonal is estimated via squared sampled gradients of the model's
likelihood with respect to its parameters, averaged over a few hundred or
thousand examples -- a process which incurs nontrivial computational costs. At
the same time, adaptive gradient methods like the ubiquitous Adam optimizer
compute a moving average of the squared gradient over the course of training.
This paper therefore explores whether an approximation of the Fisher diagonal
can be obtained "for free" by recycling the squared gradient accumulator that
has already been computed over the course of training. Through a comprehensive
set of experiments covering five applications of the Fisher diagonal, we
demonstrate that the "Squisher" (SQUared gradient accumulator as an
approximation of the FISHER) consistently performs similarly to the Fisher
diagonal while outperforming baseline methods. Additionally, we clarify the
exact differences between the Squisher and the Fisher diagonal and provide
empirical quantification of their respective impact.

</details>


### [23] [Test-time Offline Reinforcement Learning on Goal-related Experience](https://arxiv.org/abs/2507.18809)
*Marco Bagatella,Mert Albaba,Jonas Hübotter,Georg Martius,Andreas Krause*

Main category: cs.LG

TL;DR: 论文提出了一种基于目标条件的测试时训练（GC-TTT）算法，通过在测试时选择与当前目标相关的离线数据微调策略，显著提升了离线强化学习的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何通过测试时训练提升离线强化学习算法的性能，类似于基础模型在测试时的优化方法。

Method: 方法包括提出一种自监督数据选择标准，选择与当前状态和评估目标相关的过渡数据，并在测试时进行少量梯度步的微调。

Result: 实验结果表明，GC-TTT在高维运动导航和操作任务中显著优于标准离线预训练方法。

Conclusion: 结论表明，测试时训练在离线强化学习中具有潜力，且计算资源分配对性能提升至关重要。

Abstract: Foundation models compress a large amount of information in a single, large
neural network, which can then be queried for individual tasks. There are
strong parallels between this widespread framework and offline goal-conditioned
reinforcement learning algorithms: a universal value function is trained on a
large number of goals, and the policy is evaluated on a single goal in each
test episode. Extensive research in foundation models has shown that
performance can be substantially improved through test-time training,
specializing the model to the current goal. We find similarly that test-time
offline reinforcement learning on experience related to the test goal can lead
to substantially better policies at minimal compute costs. We propose a novel
self-supervised data selection criterion, which selects transitions from an
offline dataset according to their relevance to the current state and quality
with respect to the evaluation goal. We demonstrate across a wide range of
high-dimensional loco-navigation and manipulation tasks that fine-tuning a
policy on the selected data for a few gradient steps leads to significant
performance gains over standard offline pre-training. Our goal-conditioned
test-time training (GC-TTT) algorithm applies this routine in a
receding-horizon fashion during evaluation, adapting the policy to the current
trajectory as it is being rolled out. Finally, we study compute allocation at
inference, demonstrating that, at comparable costs, GC-TTT induces performance
gains that are not achievable by scaling model size.

</details>


### [24] [Even Faster Simulations with Flow Matching: A Study of Zero Degree Calorimeter Responses](https://arxiv.org/abs/2507.18811)
*Maksymilian Wojnar*

Main category: cs.LG

TL;DR: 利用流匹配（FM）技术开发了ALICE实验中零度量热器的快速仿真替代模型，显著降低了计算成本并提高了仿真保真度。


<details>
  <summary>Details</summary>
Motivation: 高能物理（HEP）模拟的计算需求日益增长，需要高效且低成本的仿真方法。

Method: 采用流匹配技术，提出了一种有效的训练策略，训练参数极少的快速生成模型。

Result: 在ZN和ZP探测器仿真中达到最先进的保真度，计算成本大幅降低，推理时间显著缩短。

Conclusion: 该方法为高能物理实验提供了一种高效、低成本的仿真解决方案，具有广泛应用潜力。

Abstract: Recent advances in generative neural networks, particularly flow matching
(FM), have enabled the generation of high-fidelity samples while significantly
reducing computational costs. A promising application of these models is
accelerating simulations in high-energy physics (HEP), helping research
institutions meet their increasing computational demands. In this work, we
leverage FM to develop surrogate models for fast simulations of zero degree
calorimeters in the ALICE experiment. We present an effective training strategy
that enables the training of fast generative models with an exceptionally low
number of parameters. This approach achieves state-of-the-art simulation
fidelity for both neutron (ZN) and proton (ZP) detectors, while offering
substantial reductions in computational costs compared to existing methods. Our
FM model achieves a Wasserstein distance of 1.27 for the ZN simulation with an
inference time of 0.46 ms per sample, compared to the current best of 1.20 with
an inference time of approximately 109 ms. The latent FM model further improves
the inference speed, reducing the sampling time to 0.026 ms per sample, with a
minimal trade-off in accuracy. Similarly, our approach achieves a Wasserstein
distance of 1.30 for the ZP simulation, outperforming the current best of 2.08.
The source code is available at https://github.com/m-wojnar/faster_zdc.

</details>


### [25] [Scale-Consistent Learning for Partial Differential Equations](https://arxiv.org/abs/2507.18813)
*Zongyi Li,Samuel Lanthaler,Catherine Deng,Michael Chen,Yixuan Wang,Kamyar Azizzadenesheli,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种基于尺度一致性的数据增强方法和尺度感知神经算子，用于解决机器学习模型在PDE求解中泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统ML模型在PDE求解中无法泛化到训练数据之外（如固定Reynolds数或预定义域），限制了其应用。

Method: 利用PDE的尺度一致性特性设计数据增强方案和尺度感知神经算子，引入尺度一致性损失函数。

Result: 在多个PDE问题上验证，模型在Re=1000训练后可泛化到Re=250-10000，平均误差降低34%。

Conclusion: 尺度一致性方法和神经算子显著提升了ML模型在PDE求解中的泛化能力和准确性。

Abstract: Machine learning (ML) models have emerged as a promising approach for solving
partial differential equations (PDEs) in science and engineering. Previous ML
models typically cannot generalize outside the training data; for example, a
trained ML model for the Navier-Stokes equations only works for a fixed
Reynolds number ($Re$) on a pre-defined domain. To overcome these limitations,
we propose a data augmentation scheme based on scale-consistency properties of
PDEs and design a scale-informed neural operator that can model a wide range of
scales. Our formulation leverages the facts: (i) PDEs can be rescaled, or more
concretely, a given domain can be re-scaled to unit size, and the parameters
and the boundary conditions of the PDE can be appropriately adjusted to
represent the original solution, and (ii) the solution operators on a given
domain are consistent on the sub-domains. We leverage these facts to create a
scale-consistency loss that encourages matching the solutions evaluated on a
given domain and the solution obtained on its sub-domain from the rescaled PDE.
Since neural operators can fit to multiple scales and resolutions, they are the
natural choice for incorporating scale-consistency loss during training of
neural PDE solvers. We experiment with scale-consistency loss and the
scale-informed neural operator model on the Burgers' equation, Darcy Flow,
Helmholtz equation, and Navier-Stokes equations. With scale-consistency, the
model trained on $Re$ of 1000 can generalize to $Re$ ranging from 250 to 10000,
and reduces the error by 34% on average of all datasets compared to baselines.

</details>


### [26] [Explainable AI guided unsupervised fault diagnostics for high-voltage circuit breakers](https://arxiv.org/abs/2507.19168)
*Chi-Ching Hsu,Gaëtan Frusque,Florent Forest,Felipe Macedo,Christian M. Franck,Olga Fink*

Main category: cs.LG

TL;DR: 提出了一种基于振动和声学信号的无监督故障检测与分割框架，用于高压断路器在线状态监测，无需故障标签即可诊断故障。


<details>
  <summary>Details</summary>
Motivation: 现有断路器状态监测方法依赖直接观测参数，仅覆盖少量故障机制且需离线操作，非侵入式信号（如振动、声学）虽可用于在线监测，但现有研究多依赖有监督方法，实际应用中故障标签难以获取。

Method: 提出无监督故障检测与分割框架，仅需健康数据训练，结合可解释人工智能（XAI）进行故障诊断。

Result: 实验验证了框架在高压断路器上的有效性，能检测故障并聚类不同故障类型，提供潜在故障组件指示。

Conclusion: 该框架为断路器在线状态监测提供了无需故障标签的可靠解决方案，提升了系统运行可靠性。

Abstract: Commercial high-voltage circuit breaker (CB) condition monitoring systems
rely on directly observable physical parameters such as gas filling pressure
with pre-defined thresholds. While these parameters are crucial, they only
cover a small subset of malfunctioning mechanisms and usually can be monitored
only if the CB is disconnected from the grid. To facilitate online condition
monitoring while CBs remain connected, non-intrusive measurement techniques
such as vibration or acoustic signals are necessary. Currently, CB condition
monitoring studies using these signals typically utilize supervised methods for
fault diagnostics, where ground-truth fault types are known due to artificially
introduced faults in laboratory settings. This supervised approach is however
not feasible in real-world applications, where fault labels are unavailable. In
this work, we propose a novel unsupervised fault detection and segmentation
framework for CBs based on vibration and acoustic signals. This framework can
detect deviations from the healthy state. The explainable artificial
intelligence (XAI) approach is applied to the detected faults for fault
diagnostics. The specific contributions are: (1) we propose an integrated
unsupervised fault detection and segmentation framework that is capable of
detecting faults and clustering different faults with only healthy data
required during training (2) we provide an unsupervised explainability-guided
fault diagnostics approach using XAI to offer domain experts potential
indications of the aged or faulty components, achieving fault diagnostics
without the prerequisite of ground-truth fault labels. These contributions are
validated using an experimental dataset from a high-voltage CB under healthy
and artificially introduced fault conditions, contributing to more reliable CB
system operation.

</details>


### [27] [Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach to Elicit Optimal Policy in Strong Models](https://arxiv.org/abs/2507.18858)
*Ruimeng Ye,Zihan Wang,Xiao Yang,Zinan Ling,Manling Li,Bo Hui*

Main category: cs.LG

TL;DR: 本文提出了一种弱到强泛化（W2SG）方法，通过弱模型的监督来激发强模型的潜力，并将其扩展到复杂的交互决策环境中。通过构建轨迹树和结合蒙特卡洛树搜索（MCTS），显著提升了模型的推理和决策能力。


<details>
  <summary>Details</summary>
Motivation: 受人类学习过程的启发，研究旨在不仅从成功经验中学习，还从失败经验中提取知识，以更全面地激发强模型的潜力。

Method: 提出了一种基于轨迹树的层次化表示方法，结合MCTS优化强模型，并通过理论分析验证其有效性。

Result: 实验证明，该方法在多种任务领域中显著提升了模型的推理和决策能力，验证了框架的可扩展性和鲁棒性。

Conclusion: 本文提出的方法通过结合轨迹树和MCTS，有效提升了W2SG的性能，为复杂决策任务提供了新的解决方案。

Abstract: Weak-to-Strong generalization (W2SG) is a new trend to elicit the full
capabilities of a strong model with supervision from a weak model. While
existing W2SG studies focus on simple tasks like binary classification, we
extend this paradigm to complex interactive decision-making environments.
Specifically, we fine-tune a strong model with trajectories of intermediate
actions generated by a weak model. Motivated by the human learning process, we
propose to generalize not only success knowledge but also failure experience so
that the strong model can learn from failed trajectories accumulated by weak
models. To effectively and efficiently elicit the potential of strong agents,
we further construct ``trajectory trees," a hierarchical representation that
organizes weak model-generated action trajectories, coupled with Monte Carlo
Tree Search (MCTS) to optimize the strong model. Through theoretical analysis,
we provide formal guarantees for the effectiveness of our method in improving
W2SG performance. Our empirical evaluations demonstrate substantial
improvements in reasoning and decision-making capabilities across diverse task
domains, validating the scalability and robustness of our proposed framework.
Our code is available at: https://github.com/yeruimeng/TraTree

</details>


### [28] [Early Mortality Prediction in ICU Patients with Hypertensive Kidney Disease Using Interpretable Machine Learning](https://arxiv.org/abs/2507.18866)
*Yong Si,Junyi Fan,Li Sun,Shuheng Chen,Minoo Ahmadi,Elham Pishgar,Kamiar Alaei,Greg Placencia,Maryam Pishgar*

Main category: cs.LG

TL;DR: 开发了一种基于机器学习的框架，用于预测ICU中高血压肾病患者的30天院内死亡率，CatBoost模型表现最佳，AUROC为0.88。


<details>
  <summary>Details</summary>
Motivation: 高血压肾病患者在ICU中短期死亡率高，但缺乏定制化的风险预测工具，早期识别高风险患者对临床决策至关重要。

Method: 使用MIMIC-IV v2.2数据库的早期临床数据，通过随机森林和互信息筛选18个临床特征，训练并比较多种模型，CatBoost表现最优。

Result: CatBoost在独立测试集上AUROC为0.88，敏感性0.811，特异性0.798，模型依赖有意义的预测因子如意识状态、血管加压药使用和凝血状态。

Conclusion: 提出了一种可解释的机器学习流程，结合高预测性能和不确定性量化，支持个体化分诊和透明临床决策，适合临床部署并需外部验证。

Abstract: Background: Hypertensive kidney disease (HKD) patients in intensive care
units (ICUs) face high short-term mortality, but tailored risk prediction tools
are lacking. Early identification of high-risk individuals is crucial for
clinical decision-making. Methods: We developed a machine learning framework to
predict 30-day in-hospital mortality among ICU patients with HKD using early
clinical data from the MIMIC-IV v2.2 database. A cohort of 1,366 adults was
curated with strict criteria, excluding malignancy cases. Eighteen clinical
features-including vital signs, labs, comorbidities, and therapies-were
selected via random forest importance and mutual information filtering. Several
models were trained and compared with stratified five-fold cross-validation;
CatBoost demonstrated the best performance. Results: CatBoost achieved an AUROC
of 0.88 on the independent test set, with sensitivity of 0.811 and specificity
of 0.798. SHAP values and Accumulated Local Effects (ALE) plots showed the
model relied on meaningful predictors such as altered consciousness,
vasopressor use, and coagulation status. Additionally, the DREAM algorithm was
integrated to estimate patient-specific posterior risk distributions, allowing
clinicians to assess both predicted mortality and its uncertainty. Conclusions:
We present an interpretable machine learning pipeline for early, real-time risk
assessment in ICU patients with HKD. By combining high predictive performance
with uncertainty quantification, our model supports individualized triage and
transparent clinical decisions. This approach shows promise for clinical
deployment and merits external validation in broader critical care populations.

</details>


### [29] [Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning via Incorporating Generalized Human Expertise](https://arxiv.org/abs/2507.18867)
*Xuefei Wu,Xiao Yin,Yuanyang Zhu,Chunlin Chen*

Main category: cs.LG

TL;DR: LIGHT框架通过结合人类专家知识和MARL算法，设计个体内在奖励，提升稀疏奖励环境下的多智能体探索效率。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习（MARL）中团队奖励稀疏导致的探索效率低下问题，避免依赖手工设计的低效个体奖励。

Method: 提出LIGHT框架，整合人类专家知识，通过个体动作分布和人类偏好分布设计内在奖励，优化Q学习。

Result: 实验表明LIGHT在性能和知识复用性上优于基线方法，适用于复杂稀疏奖励任务。

Conclusion: LIGHT有效结合人类知识与MARL，提升探索效率和任务表现。

Abstract: Efficient exploration in multi-agent reinforcement learning (MARL) is a
challenging problem when receiving only a team reward, especially in
environments with sparse rewards. A powerful method to mitigate this issue
involves crafting dense individual rewards to guide the agents toward efficient
exploration. However, individual rewards generally rely on manually engineered
shaping-reward functions that lack high-order intelligence, thus it behaves
ineffectively than humans regarding learning and generalization in complex
problems. To tackle these issues, we combine the above two paradigms and
propose a novel framework, LIGHT (Learning Individual Intrinsic reward via
Incorporating Generalized Human experTise), which can integrate human knowledge
into MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid
unnecessary exploration by considering both individual action distribution and
human expertise preference distribution. Then, LIGHT designs individual
intrinsic rewards for each agent based on actionable representational
transformation relevant to Q-learning so that the agents align their action
preferences with the human expertise while maximizing the joint action value.
Experimental results demonstrate the superiority of our method over
representative baselines regarding performance and better knowledge reusability
across different sparse-reward tasks on challenging scenarios.

</details>


### [30] [Geometric Multi-color Message Passing Graph Neural Networks for Blood-brain Barrier Permeability Prediction](https://arxiv.org/abs/2507.18926)
*Trung Nguyen,Md Masud Rana,Farjana Tasnim Mukta,Chang-Guo Zhan,Duc Duy Nguyen*

Main category: cs.LG

TL;DR: 论文提出了一种结合几何特征的多色消息传递图神经网络（GMC-MPNN），用于预测血脑屏障渗透性（BBBP），在分类和回归任务中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测BBBP对中枢神经系统药物开发至关重要，但现有图神经网络（GNNs）常忽略分子几何信息。

Method: GMC-MPNN通过引入原子级几何特征和长程相互作用，构建加权彩色子图来捕捉空间关系和化学背景。

Result: 在多个基准数据集上，GMC-MPNN表现优异（分类AUC-ROC达0.9704，回归RMSE为0.4609）。

Conclusion: GMC-MPNN通过整合空间几何信息，为药物发现提供了更准确和通用的工具。

Abstract: Accurate prediction of blood-brain barrier permeability (BBBP) is essential
for central nervous system (CNS) drug development. While graph neural networks
(GNNs) have advanced molecular property prediction, they often rely on
molecular topology and neglect the three-dimensional geometric information
crucial for modeling transport mechanisms. This paper introduces the geometric
multi-color message-passing graph neural network (GMC-MPNN), a novel framework
that enhances standard message-passing architectures by explicitly
incorporating atomic-level geometric features and long-range interactions. Our
model constructs weighted colored subgraphs based on atom types to capture the
spatial relationships and chemical context that govern BBB permeability. We
evaluated GMC-MPNN on three benchmark datasets for both classification and
regression tasks, using rigorous scaffold-based splitting to ensure a robust
assessment of generalization. The results demonstrate that GMC-MPNN
consistently outperforms existing state-of-the-art models, achieving superior
performance in both classifying compounds as permeable/non-permeable (AUC-ROC
of 0.9704 and 0.9685) and in regressing continuous permeability values (RMSE of
0.4609, Pearson correlation of 0.7759). An ablation study further quantified
the impact of specific atom-pair interactions, revealing that the model's
predictive power derives from its ability to learn from both common and rare,
but chemically significant, functional motifs. By integrating spatial geometry
into the graph representation, GMC-MPNN sets a new performance benchmark and
offers a more accurate and generalizable tool for drug discovery pipelines.

</details>


### [31] [Secure Best Arm Identification in the Presence of a Copycat](https://arxiv.org/abs/2507.18975)
*Asaf Cohen,Onur Günlü*

Main category: cs.LG

TL;DR: 论文研究了带安全约束的最佳臂识别问题，提出了一种无需密钥或加密原语的算法，通过编码臂实现安全性和性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 在随机线性多臂老虎机中，玩家需在T次拉臂后识别最佳臂，同时防止观察者（如模仿者Chloe）获取最佳臂信息。现有算法要么暴露最佳臂，要么性能较差。

Method: 提出了一种使用编码臂的安全算法，无需密钥或加密原语，通过巧妙设计隐藏最佳臂信息。

Result: 算法实现了Ω(T/log²(d))的错误指数，同时几乎不泄露最佳臂信息。

Conclusion: 该算法在安全性和性能之间取得了平衡，为带安全约束的最佳臂识别问题提供了有效解决方案。

Abstract: Consider the problem of best arm identification with a security constraint.
Specifically, assume a setup of stochastic linear bandits with $K$ arms of
dimension $d$. In each arm pull, the player receives a reward that is the sum
of the dot product of the arm with an unknown parameter vector and independent
noise. The player's goal is to identify the best arm after $T$ arm pulls.
Moreover, assume a copycat Chloe is observing the arm pulls. The player wishes
to keep Chloe ignorant of the best arm.
  While a minimax--optimal algorithm identifies the best arm with an
$\Omega\left(\frac{T}{\log(d)}\right)$ error exponent, it easily reveals its
best-arm estimate to an outside observer, as the best arms are played more
frequently. A naive secure algorithm that plays all arms equally results in an
$\Omega\left(\frac{T}{d}\right)$ exponent. In this paper, we propose a secure
algorithm that plays with \emph{coded arms}. The algorithm does not require any
key or cryptographic primitives, yet achieves an
$\Omega\left(\frac{T}{\log^2(d)}\right)$ exponent while revealing almost no
information on the best arm.

</details>


### [32] [KASPER: Kolmogorov Arnold Networks for Stock Prediction and Explainable Regimes](https://arxiv.org/abs/2507.18983)
*Vidhi Oad,Param Pathak,Nouhaila Innan,Shalini D,Muhammad Shafique*

Main category: cs.LG

TL;DR: 论文提出了一种名为KASPER的新框架，结合了Kolmogorov-Arnold网络和可解释性技术，用于金融市场的自适应预测。


<details>
  <summary>Details</summary>
Motivation: 金融市场的非线性和状态依赖性使得传统深度学习模型难以适应变化的市场条件，需要更自适应且可解释的方法。

Method: KASPER框架通过Gumbel-Softmax机制检测市场状态，使用稀疏样条激活的Kolmogorov-Arnold网络进行状态特定预测，并通过符号学习提取可解释规则。

Result: 在真实金融数据上，模型R²得分为0.89，夏普比率为12.02，均方误差低至0.0001，优于现有方法。

Conclusion: 该研究为金融市场的状态感知、透明且稳健的预测提供了新方向。

Abstract: Forecasting in financial markets remains a significant challenge due to their
nonlinear and regime-dependent dynamics. Traditional deep learning models, such
as long short-term memory networks and multilayer perceptrons, often struggle
to generalize across shifting market conditions, highlighting the need for a
more adaptive and interpretable approach. To address this, we introduce
Kolmogorov-Arnold networks for stock prediction and explainable regimes
(KASPER), a novel framework that integrates regime detection, sparse
spline-based function modeling, and symbolic rule extraction. The framework
identifies hidden market conditions using a Gumbel-Softmax-based mechanism,
enabling regime-specific forecasting. For each regime, it employs
Kolmogorov-Arnold networks with sparse spline activations to capture intricate
price behaviors while maintaining robustness. Interpretability is achieved
through symbolic learning based on Monte Carlo Shapley values, which extracts
human-readable rules tailored to each regime. Applied to real-world financial
time series from Yahoo Finance, the model achieves an $R^2$ score of 0.89, a
Sharpe Ratio of 12.02, and a mean squared error as low as 0.0001, outperforming
existing methods. This research establishes a new direction for regime-aware,
transparent, and robust forecasting in financial markets.

</details>


### [33] [Differentiated Thyroid Cancer Recurrence Classification Using Machine Learning Models and Bayesian Neural Networks with Varying Priors: A SHAP-Based Interpretation of the Best Performing Model](https://arxiv.org/abs/2507.18987)
*HMNS Kumari,HMLS Kumari,UMMPK Nawarathne*

Main category: cs.LG

TL;DR: 该研究提出了一个用于甲状腺癌复发的分类框架，结合了机器学习模型和贝叶斯神经网络，以提高准确性和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 甲状腺癌复发是一个重要的公共卫生问题，需要既准确又可解释且能量化不确定性的分类模型。

Method: 研究使用了383名患者的16个临床和病理变量，首先比较了11种机器学习模型，随后通过Boruta算法进行特征选择，并应用贝叶斯神经网络（BNN）量化不确定性。

Result: 支持向量机（SVM）在完整数据集上准确率最高（0.9481），逻辑回归（LR）在特征选择后准确率最高（0.9611）。BNN模型（Normal 0,10先验分布）在特征选择前后分别达到0.9740和0.9870的准确率。

Conclusion: 贝叶斯神经网络在甲状腺癌复发分类中表现出更高的准确性和不确定性量化能力，优于传统机器学习模型。

Abstract: Differentiated thyroid cancer DTC recurrence is a major public health
concern, requiring classification and predictive models that are not only
accurate but also interpretable and uncertainty aware. This study introduces a
comprehensive framework for DTC recurrence classification using a dataset
containing 383 patients and 16 clinical and pathological variables. Initially,
11 machine learning ML models were employed using the complete dataset, where
the Support Vector Machines SVM model achieved the highest accuracy of 0.9481.
To reduce complexity and redundancy, feature selection was carried out using
the Boruta algorithm, and the same ML models were applied to the reduced
dataset, where it was observed that the Logistic Regression LR model obtained
the maximum accuracy of 0.9611. However, these ML models often lack uncertainty
quantification, which is critical in clinical decision making. Therefore, to
address this limitation, the Bayesian Neural Networks BNN with six varying
prior distributions, including Normal 0,1, Normal 0,10, Laplace 0,1, Cauchy
0,1, Cauchy 0,2.5, and Horseshoe 1, were implemented on both the complete and
reduced datasets. The BNN model with Normal 0,10 prior distribution exhibited
maximum accuracies of 0.9740 and 0.9870 before and after feature selection,
respectively.

</details>


### [34] [GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units](https://arxiv.org/abs/2507.18989)
*Maxence Bouvier,Ryan Amaudruz,Felix Arnold,Renzo Andri,Lukas Cavigelli*

Main category: cs.LG

TL;DR: GENIAL是一种基于机器学习的框架，用于自动生成和优化算术单元（如乘法器），通过Transformer代理模型高效搜索最优设计，显著降低功耗。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载的增加，优化算术单元以减少数字系统占用空间变得至关重要。传统设计方法难以全面探索设计空间。

Method: GENIAL采用两阶段训练的Transformer代理模型（自监督预训练和监督微调），通过反转模型搜索最优操作数编码，最小化功耗。

Result: 实验表明，GENIAL比其他方法更高效，能快速收敛到优化设计，并在乘法器中实现高达18%的开关活动节省。

Conclusion: GENIAL为数字系统的自动化组合电路生成迈出了重要一步，展示了其在广泛逻辑功能中的适用性。

Abstract: As AI workloads proliferate, optimizing arithmetic units is becoming
increasingly important to reduce the footprint of digital systems. Conventional
design flows, which often rely on manual or heuristics-based optimization, are
limited in their ability to thoroughly explore the vast design space. In this
paper, we introduce GENIAL, a machine learning-based framework for the
automatic generation and optimization of arithmetic units, more specifically
multipliers.
  At the core of GENIAL is a Transformer-based surrogate model trained in two
stages, involving self-supervised pretraining followed by supervised
finetuning, to robustly forecast key hardware metrics such as power and area
from abstracted design representations. By inverting the surrogate model,
GENIAL efficiently searches for new operand encodings that directly minimize
power consumption in arithmetic units for specific input data distributions.
Extensive experiments on large datasets demonstrate that GENIAL is consistently
more sample efficient than other methods, and converges faster towards
optimized designs. This enables to deploy a high-effort logic synthesis
optimization flow in the loop, improving the accuracy of the surrogate model.
Notably, GENIAL automatically discovers encodings that achieve up to 18%
switching activity savings within multipliers on representative AI workloads
compared with the conventional two's complement. We also demonstrate the
versatility of our approach by achieving significant improvements on Finite
State Machines, highlighting GENIAL's applicability for a wide spectrum of
logic functions. Together, these advances mark a significant step toward
automated Quality-of-Results-optimized combinational circuit generation for
digital systems.

</details>


### [35] [Reinforcement Learning via Conservative Agent for Environments with Random Delays](https://arxiv.org/abs/2507.18992)
*Jongsoo Lee,Jangwon Kim,Jiseok Jeong,Soohee Han*

Main category: cs.LG

TL;DR: 提出了一种保守智能体方法，将随机延迟环境转化为恒定延迟环境，使现有方法可直接应用，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实中的强化学习常因环境反馈延迟而受限，随机延迟环境尤其缺乏研究，亟需解决方案。

Method: 通过保守智能体将随机延迟环境转化为恒定延迟等效环境，无需修改算法结构。

Result: 在连续控制任务中，该方法显著优于基线算法，表现更优且样本效率更高。

Conclusion: 保守智能体方法为随机延迟环境提供了一种简单而有效的解决方案。

Abstract: Real-world reinforcement learning applications are often hindered by delayed
feedback from environments, which violates the Markov assumption and introduces
significant challenges. Although numerous delay-compensating methods have been
proposed for environments with constant delays, environments with random delays
remain largely unexplored due to their inherent variability and
unpredictability. In this study, we propose a simple yet robust agent for
decision-making under random delays, termed the conservative agent, which
reformulates the random-delay environment into its constant-delay equivalent.
This transformation enables any state-of-the-art constant-delay method to be
directly extended to the random-delay environments without modifying the
algorithmic structure or sacrificing performance. We evaluate the conservative
agent-based algorithm on continuous control tasks, and empirical results
demonstrate that it significantly outperforms existing baseline algorithms in
terms of asymptotic performance and sample efficiency.

</details>


### [36] [Adapting to Fragmented and Evolving Data: A Fisher Information Perspective](https://arxiv.org/abs/2507.18996)
*Behraj Khan,Tahir Qasim Syed,Nouman Muhammad Durrani*

Main category: cs.LG

TL;DR: FADE是一个轻量级框架，用于在动态环境中处理序列协变量偏移（SCS），通过Fisher信息几何和正则化机制实现鲁棒学习。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中输入分布随时间变化而条件分布稳定的问题，提出无需任务边界或目标监督的在线学习方法。

Method: 使用基于Fisher信息几何的移位感知正则化机制和Cramer-Rao移位信号检测分布变化。

Result: 在七个基准测试中，FADE在严重偏移下准确率提升高达19%，优于TENT和DIW等方法。

Conclusion: FADE在理论和实验上均表现出色，适用于多模态和联邦学习场景。

Abstract: Modern machine learning systems operating in dynamic environments often face
\textit{sequential covariate shift} (SCS), where input distributions evolve
over time while the conditional distribution remains stable. We introduce FADE
(Fisher-based Adaptation to Dynamic Environments), a lightweight and
theoretically grounded framework for robust learning under SCS. FADE employs a
shift-aware regularization mechanism anchored in Fisher information geometry,
guiding adaptation by modulating parameter updates based on sensitivity and
stability. To detect significant distribution changes, we propose a
Cramer-Rao-informed shift signal that integrates KL divergence with temporal
Fisher dynamics. Unlike prior methods requiring task boundaries, target
supervision, or experience replay, FADE operates online with fixed memory and
no access to target labels. Evaluated on seven benchmarks spanning vision,
language, and tabular data, FADE achieves up to 19\% higher accuracy under
severe shifts, outperforming methods such as TENT and DIW. FADE also
generalizes naturally to federated learning by treating heterogeneous clients
as temporally fragmented environments, enabling scalable and stable adaptation
in decentralized settings. Theoretical analysis guarantees bounded regret and
parameter consistency, while empirical results demonstrate FADE's robustness
across modalities and shift intensities.

</details>


### [37] [A diffusion-based generative model for financial time series via geometric Brownian motion](https://arxiv.org/abs/2507.19003)
*Gihun Kim,Sun-Yong Choi,Yeoneung Kim*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的金融时间序列生成框架，结合几何布朗运动（GBM），改进传统分数模型，更真实地模拟金融数据的异方差特性。


<details>
  <summary>Details</summary>
Motivation: 传统分数模型将价格轨迹视为普通数值序列，未考虑金融时间序列的异方差性，因此需要一种更贴合金融数据特性的生成方法。

Method: 在正向噪声过程中按资产价格比例注入噪声，平衡漂移和扩散项，生成对数价格过程；使用基于Transformer的架构进行反向时间生成训练。

Result: 模型在历史股票数据上更真实地再现了厚尾收益分布、波动率聚类和杠杆效应等关键特征。

Conclusion: 该方法在金融时间序列生成中表现出更优的模拟能力，优于传统扩散模型。

Abstract: We propose a novel diffusion-based generative framework for financial time
series that incorporates geometric Brownian motion (GBM), the foundation of the
Black--Scholes theory, into the forward noising process. Unlike standard
score-based models that treat price trajectories as generic numerical
sequences, our method injects noise proportionally to asset prices at each time
step, reflecting the heteroskedasticity observed in financial time series. By
accurately balancing the drift and diffusion terms, we show that the resulting
log-price process reduces to a variance-exploding stochastic differential
equation, aligning with the formulation in score-based generative models. The
reverse-time generative process is trained via denoising score matching using a
Transformer-based architecture adapted from the Conditional Score-based
Diffusion Imputation (CSDI) framework. Empirical evaluations on historical
stock data demonstrate that our model reproduces key stylized facts
heavy-tailed return distributions, volatility clustering, and the leverage
effect more realistically than conventional diffusion models.

</details>


### [38] [MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster](https://arxiv.org/abs/2507.19017)
*Laingjun Feng,Chenyi Pan,Xinjie Guo,Fei Mei,Benzhe Ning,Jianxiang Zhang,Xinyang Liu,Beirong Zhou,Zeng Shu,Chang Liu,Guang Yang,Zhenyu Han,Jiangben Wang,Bo Wang*

Main category: cs.LG

TL;DR: MindSpeed RL是一种高效的大规模强化学习训练系统，通过分布式视角优化数据依赖关系，提升集群扩展性和内存利用率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习训练系统因节点间依赖性强，导致集群扩展性差和内存利用率低。

Method: 采用分布式传输码头策略和allgather-swap策略优化样本流和重分片流，并整合并行化和加速技术。

Result: 在多个模型上的实验表明，MindSpeed RL的吞吐量提升1.42~3.97倍。

Conclusion: MindSpeed RL显著提升了强化学习训练效率，并在Ascend超算上验证了其性能和可靠性。

Abstract: Reinforcement learning (RL) is a paradigm increasingly used to align large
language models. Popular RL algorithms utilize multiple workers and can be
modeled as a graph, where each node is the status of a worker and each edge
represents dataflow between nodes. Owing to the heavy cross-node dependencies,
the RL training system usually suffers from poor cluster scalability and low
memory utilization. In this article, we introduce MindSpeed RL, an effective
and efficient system for large-scale RL training. Unlike existing centralized
methods, MindSpeed RL organizes the essential data dependencies in RL training,
i.e., sample flow and resharding flow, from a distributed view. On the one
hand, a distributed transfer dock strategy, which sets controllers and
warehouses on the basis of the conventional replay buffer, is designed to
release the dispatch overhead in the sample flow. A practical allgather--swap
strategy is presented to eliminate redundant memory usage in resharding flow.
In addition, MindSpeed RL further integrates numerous parallelization
strategies and acceleration techniques for systematic optimization. Compared
with existing state-of-the-art systems, comprehensive experiments on the RL
training of popular Qwen2.5-Dense-7B/32B, Qwen3-MoE-30B, and
DeepSeek-R1-MoE-671B show that MindSpeed RL increases the throughput by 1.42 ~
3.97 times. Finally, we open--source MindSpeed RL and perform all the
experiments on a super pod of Ascend with 384 neural processing units (NPUs) to
demonstrate the powerful performance and reliability of Ascend.

</details>


### [39] [ProGMLP: A Progressive Framework for GNN-to-MLP Knowledge Distillation with Efficient Trade-offs](https://arxiv.org/abs/2507.19031)
*Weigang Lu,Ziyu Guan,Wei Zhao,Yaming Yang,Yujie Sun,Zheng Liang,Yibing Zhan,Dapeng Tao*

Main category: cs.LG

TL;DR: ProGMLP是一种灵活的GNN-to-MLP知识蒸馏框架，通过渐进式训练和知识蒸馏，动态调整推理成本和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有G2M方法无法动态调整推理成本和准确性，限制了其在资源多变环境中的应用。

Method: ProGMLP采用渐进式训练结构（PTS）、渐进式知识蒸馏（PKD）和渐进式混合增强（PMA）来优化蒸馏过程。

Result: 在八个真实图数据集上的实验表明，ProGMLP能保持高准确性并动态适应不同运行场景。

Conclusion: ProGMLP适用于多样化应用场景，提供高效的推理成本与准确性权衡。

Abstract: GNN-to-MLP (G2M) methods have emerged as a promising approach to accelerate
Graph Neural Networks (GNNs) by distilling their knowledge into simpler
Multi-Layer Perceptrons (MLPs). These methods bridge the gap between the
expressive power of GNNs and the computational efficiency of MLPs, making them
well-suited for resource-constrained environments. However, existing G2M
methods are limited by their inability to flexibly adjust inference cost and
accuracy dynamically, a critical requirement for real-world applications where
computational resources and time constraints can vary significantly. To address
this, we introduce a Progressive framework designed to offer flexible and
on-demand trade-offs between inference cost and accuracy for GNN-to-MLP
knowledge distillation (ProGMLP). ProGMLP employs a Progressive Training
Structure (PTS), where multiple MLP students are trained in sequence, each
building on the previous one. Furthermore, ProGMLP incorporates Progressive
Knowledge Distillation (PKD) to iteratively refine the distillation process
from GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhance
generalization by progressively generating harder mixed samples. Our approach
is validated through comprehensive experiments on eight real-world graph
datasets, demonstrating that ProGMLP maintains high accuracy while dynamically
adapting to varying runtime scenarios, making it highly effective for
deployment in diverse application settings.

</details>


### [40] [Neural Ordinary Differential Equations for Learning and Extrapolating System Dynamics Across Bifurcations](https://arxiv.org/abs/2507.19036)
*Eva van Tegelen,George van Voorn,Ioannis Athanasiadis,Peter van Heijster*

Main category: cs.LG

TL;DR: 论文提出使用神经常微分方程（Neural ODEs）从时间序列数据中学习系统动力学，成功预测了局部和全局分岔结构，并在有限和噪声数据条件下验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多局限于离散时间方法和局部分岔，无法全面捕捉复杂系统的动力学行为。本文旨在通过连续的数据驱动框架解决这些限制。

Method: 采用神经常微分方程（Neural ODEs）作为连续框架，从时间序列数据中学习参数依赖的向量场，并应用于具有局部和全局分岔的捕食者-猎物系统。

Result: 实验表明，Neural ODEs能够直接从数据中恢复分岔结构，并能预测超出训练数据参数范围的分岔行为。模型性能更依赖于数据质量而非数据量。

Conclusion: 神经常微分方程为复杂系统的分岔预测提供了一种有效的数据驱动方法，尤其在有限和噪声数据条件下表现良好。

Abstract: Forecasting system behaviour near and across bifurcations is crucial for
identifying potential shifts in dynamical systems. While machine learning has
recently been used to learn critical transitions and bifurcation structures
from data, most studies remain limited as they exclusively focus on
discrete-time methods and local bifurcations. To address these limitations, we
use Neural Ordinary Differential Equations which provide a continuous,
data-driven framework for learning system dynamics. We apply our approach to a
predator-prey system that features both local and global bifurcations,
presenting a challenging test case. Our results show that Neural Ordinary
Differential Equations can recover underlying bifurcation structures directly
from timeseries data by learning parameter-dependent vector fields. Notably, we
demonstrate that Neural Ordinary Differential Equations can forecast
bifurcations even beyond the parameter regions represented in the training
data. We also assess the method's performance under limited and noisy data
conditions, finding that model accuracy depends more on the quality of
information that can be inferred from the training data, than on the amount of
data available.

</details>


### [41] [Dynamics-Informed Reservoir Computing with Visibility Graphs](https://arxiv.org/abs/2507.19046)
*Charlotte Geier,Merten Stender*

Main category: cs.LG

TL;DR: 提出了一种基于动态信息的储层计算框架（DyRC-VG），通过可见性图技术直接从训练数据中推断储层网络结构，避免了昂贵的超参数调优，提高了预测质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 复杂非线性时间序列的准确预测是一个挑战性问题，传统储层计算（RC）的随机储层网络结构常导致次优和过大的网络。

Method: 采用可见性图（VG）技术将时间序列数据转换为网络，直接利用训练数据的VG网络构建储层，避免超参数调优。

Result: 在Duffing振荡器的预测任务中，DyRC-VG比相同规模的Erdős-Rényi图表现出更高的预测质量和一致性。

Conclusion: DyRC-VG框架通过动态信息直接构建储层网络，显著提升了预测性能，为复杂时间序列预测提供了高效解决方案。

Abstract: Accurate prediction of complex and nonlinear time series remains a
challenging problem across engineering and scientific disciplines. Reservoir
computing (RC) offers a computationally efficient alternative to traditional
deep learning by training only the read-out layer while employing a randomly
structured and fixed reservoir network. Despite its advantages, the largely
random reservoir graph architecture often results in suboptimal and oversized
networks with poorly understood dynamics. Addressing this issue, we propose a
novel Dynamics-Informed Reservoir Computing (DyRC) framework that
systematically infers the reservoir network structure directly from the input
training sequence. This work proposes to employ the visibility graph (VG)
technique, which converts time series data into networks by representing
measurement points as nodes linked by mutual visibility. The reservoir network
is constructed by directly adopting the VG network from a training data
sequence, leveraging the parameter-free visibility graph approach to avoid
expensive hyperparameter tuning. This process results in a reservoir that is
directly informed by the specific dynamics of the prediction task under study.
We assess the DyRC-VG method through prediction tasks involving the canonical
nonlinear Duffing oscillator, evaluating prediction accuracy and consistency.
Compared to an Erd\H{o}s-R\'enyi graph of the same size, spectral radius, and
comparable density, we observe higher prediction quality and more consistent
performance over repeated implementations in the DyRC-VG.

</details>


### [42] [Exploring molecular assembly as a biosignature using mass spectrometry and machine learning](https://arxiv.org/abs/2507.19057)
*Lindsay A. Rutter,Abhishek Sharma,Ian Seet,David Obeh Alobo,An Goto,Leroy Cronin*

Main category: cs.LG

TL;DR: 分子组装是一种无需结构解析即可通过质谱数据检测生命的方法，满足无偏生命探测的需求。机器学习模型能高精度预测分子组装，但需标准化质谱数据库。


<details>
  <summary>Details</summary>
Motivation: 为在太阳系任务中无偏探测生命，需一种不依赖地球生命假设的方法。分子组装因其可解释性和实验可测性成为理想候选。

Method: 开发机器学习模型预测分子组装，并通过模拟数据评估其性能及标准化需求。

Result: 模型将预测误差降低三倍，但仪器不一致性可能使误差翻倍，凸显标准化的重要性。

Conclusion: 标准化质谱数据库可支持分子组装预测，为未来天体生物学任务提供概念验证。

Abstract: Molecular assembly offers a promising path to detect life beyond Earth, while
minimizing assumptions based on terrestrial life. As mass spectrometers will be
central to upcoming Solar System missions, predicting molecular assembly from
their data without needing to elucidate unknown structures will be essential
for unbiased life detection. An ideal agnostic biosignature must be
interpretable and experimentally measurable. Here, we show that molecular
assembly, a recently developed approach to measure objects that have been
produced by evolution, satisfies both criteria. First, it is interpretable for
life detection, as it reflects the assembly of molecules with their bonds as
building blocks, in contrast to approaches that discount construction history.
Second, it can be determined without structural elucidation, as it can be
physically measured by mass spectrometry, a property that distinguishes it from
other approaches that use structure-based information measures for molecular
complexity. Whilst molecular assembly is directly measurable using mass
spectrometry data, there are limits imposed by mission constraints. To address
this, we developed a machine learning model that predicts molecular assembly
with high accuracy, reducing error by three-fold compared to baseline models.
Simulated data shows that even small instrumental inconsistencies can double
model error, emphasizing the need for standardization. These results suggest
that standardized mass spectrometry databases could enable accurate molecular
assembly prediction, without structural elucidation, providing a
proof-of-concept for future astrobiology missions.

</details>


### [43] [Clustering-Oriented Generative Attribute Graph Imputation](https://arxiv.org/abs/2507.19085)
*Mulin Chen,Bocheng Wang,Jiaxin Zhong,Zongcheng Miao,Xuelong Li*

Main category: cs.LG

TL;DR: CGIR模型通过生成对抗模块和边缘注意力网络，解决了属性缺失图聚类中语义信息捕获不足和冗余属性干扰的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在属性缺失图聚类中，未能充分捕获类相关语义信息，且优化策略忽略了冗余属性的干扰。

Method: CGIR模型通过估计子簇分布约束生成对抗模块的采样空间，并利用边缘注意力网络识别类相关属性。

Result: 实验证明CGIR优于现有方法。

Conclusion: CGIR通过子簇搜索与合并，实现了节点填补和嵌入优化的统一框架。

Abstract: Attribute-missing graph clustering has emerged as a significant unsupervised
task, where only attribute vectors of partial nodes are available and the graph
structure is intact. The related models generally follow the two-step paradigm
of imputation and refinement. However, most imputation approaches fail to
capture class-relevant semantic information, leading to sub-optimal imputation
for clustering. Moreover, existing refinement strategies optimize the learned
embedding through graph reconstruction, while neglecting the fact that some
attributes are uncorrelated with the graph. To remedy the problems, we
establish the Clustering-oriented Generative Imputation with reliable
Refinement (CGIR) model. Concretely, the subcluster distributions are estimated
to reveal the class-specific characteristics precisely, and constrain the
sampling space of the generative adversarial module, such that the imputation
nodes are impelled to align with the correct clusters. Afterwards, multiple
subclusters are merged to guide the proposed edge attention network, which
identifies the edge-wise attributes for each class, so as to avoid the
redundant attributes in graph reconstruction from disturbing the refinement of
overall embedding. To sum up, CGIR splits attribute-missing graph clustering
into the search and mergence of subclusters, which guides to implement node
imputation and refinement within a unified framework. Extensive experiments
prove the advantages of CGIR over state-of-the-art competitors.

</details>


### [44] [GCL-GCN: Graphormer and Contrastive Learning Enhanced Attributed Graph Clustering Network](https://arxiv.org/abs/2507.19095)
*Binxiong Li,Xu Xiang,Xue Li,Binyu Zhao,Yujie Liu,Huijie Tang,Benhan Yang,Zhixuan Chen*

Main category: cs.LG

TL;DR: 提出了一种名为GCL-GCN的新型深度图聚类模型，通过结合中心性编码和空间关系的Graphormer模块以及对比学习模块，显著提升了稀疏和异构图数据的聚类质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在处理稀疏和异构图数据时难以捕捉局部依赖和复杂结构的局限性。

Method: 设计了Graphormer模块结合中心性编码和空间关系，以及对比学习模块增强特征表示。

Result: 在六个数据集上实验表明，GCL-GCN在聚类质量和鲁棒性上优于14种先进方法，如在Cora数据集上ACC、NMI和ARI分别提升4.94%、13.01%和10.97%。

Conclusion: GCL-GCN通过创新的模块设计显著提升了图聚类的性能，适用于复杂图数据分析。

Abstract: Attributed graph clustering holds significant importance in modern data
analysis. However, due to the complexity of graph data and the heterogeneity of
node attributes, leveraging graph information for clustering remains
challenging. To address this, we propose a novel deep graph clustering model,
GCL-GCN, specifically designed to address the limitations of existing models in
capturing local dependencies and complex structures when dealing with sparse
and heterogeneous graph data. GCL-GCN introduces an innovative Graphormer
module that combines centrality encoding and spatial relationships, effectively
capturing both global and local information between nodes, thereby enhancing
the quality of node representations. Additionally, we propose a novel
contrastive learning module that significantly enhances the discriminative
power of feature representations. In the pre-training phase, this module
increases feature distinction through contrastive learning on the original
feature matrix, ensuring more identifiable initial representations for
subsequent graph convolution and clustering tasks. Extensive experimental
results on six datasets demonstrate that GCL-GCN outperforms 14 advanced
methods in terms of clustering quality and robustness. Specifically, on the
Cora dataset, it improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%,
respectively, compared to the primary comparison method MBN.

</details>


### [45] [Graph Structure Learning with Privacy Guarantees for Open Graph Data](https://arxiv.org/abs/2507.19116)
*Muhao Guo,Jiaqi Wu,Yang Weng,Yizheng Liao,Shengzhe Chen*

Main category: cs.LG

TL;DR: 提出了一种基于高斯差分隐私（GDP）的隐私保护图数据发布框架，解决了传统方法在数据发布阶段隐私保护的不足。


<details>
  <summary>Details</summary>
Motivation: 在大规模开放数据集中确保隐私（如GDPR要求）具有挑战性，现有方法难以平衡隐私与实用性，尤其是在数据发布者和用户分离的情况下。

Method: 利用高斯差分隐私（GDP）和结构化噪声注入机制，提出了一种隐私保护估计框架，专注于图恢复问题。

Result: 实验结果表明，该方法在图学习中表现稳健，能够无偏恢复图结构，同时满足差分隐私。

Conclusion: 该框架为隐私敏感的图分析提供了可行解决方案，并扩展了差分隐私在离散变量图中的应用。

Abstract: Ensuring privacy in large-scale open datasets is increasingly challenging
under regulations such as the General Data Protection Regulation (GDPR). While
differential privacy (DP) provides strong theoretical guarantees, it primarily
focuses on noise injection during model training, neglecting privacy
preservation at the data publishing stage. Existing privacy-preserving data
publishing (PPDP) approaches struggle to balance privacy and utility,
particularly when data publishers and users are distinct entities. To address
this gap, we focus on the graph recovery problem and propose a novel
privacy-preserving estimation framework for open graph data, leveraging
Gaussian DP (GDP) with a structured noise-injection mechanism. Unlike
traditional methods that perturb gradients or model updates, our approach
ensures unbiased graph structure recovery while enforcing DP at the data
publishing stage. Moreover, we provide theoretical guarantees on estimation
accuracy and extend our method to discrete-variable graphs, a setting often
overlooked in DP research. Experimental results in graph learning demonstrate
robust performance, offering a viable solution for privacy-conscious graph
analysis.

</details>


### [46] [Solar Photovoltaic Assessment with Large Language Model](https://arxiv.org/abs/2507.19144)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 论文提出PVAL框架，利用大语言模型（LLMs）改进卫星图像中太阳能光伏板的检测与定位，解决现有方法的透明性、泛化性和数据需求问题。


<details>
  <summary>Details</summary>
Motivation: 现有太阳能光伏板检测方法缺乏透明性、依赖高质量训练数据且难以泛化，阻碍大规模部署和电网优化。

Method: 提出PVAL框架，结合任务分解、输出标准化、少样本提示和微调，提升LLMs在光伏板检测中的表现。

Result: PVAL框架提高了检测的透明性、可扩展性和适应性，同时减少计算开销。

Conclusion: PVAL为大规模可再生能源集成和电网优化提供了自动化、可复现的解决方案。

Abstract: Accurate detection and localization of solar photovoltaic (PV) panels in
satellite imagery is essential for optimizing microgrids and active
distribution networks (ADNs), which are critical components of renewable energy
systems. Existing methods lack transparency regarding their underlying
algorithms or training datasets, rely on large, high-quality PV training data,
and struggle to generalize to new geographic regions or varied environmental
conditions without extensive re-training. These limitations lead to
inconsistent detection outcomes, hindering large-scale deployment and
data-driven grid optimization. In this paper, we investigate how large language
models (LLMs) can be leveraged to overcome these challenges. Despite their
promise, LLMs face several challenges in solar panel detection, including
difficulties with multi-step logical processes, inconsistent output formatting,
frequent misclassification of visually similar objects (e.g., shadows, parking
lots), and low accuracy in complex tasks such as spatial localization and
quantification. To overcome these issues, we propose the PV Assessment with
LLMs (PVAL) framework, which incorporates task decomposition for more efficient
workflows, output standardization for consistent and scalable formatting,
few-shot prompting to enhance classification accuracy, and fine-tuning using
curated PV datasets with detailed annotations. PVAL ensures transparency,
scalability, and adaptability across heterogeneous datasets while minimizing
computational overhead. By combining open-source accessibility with robust
methodologies, PVAL establishes an automated and reproducible pipeline for
solar panel detection, paving the way for large-scale renewable energy
integration and optimized grid management.

</details>


### [47] [Automatic Cough Analysis for Non-Small Cell Lung Cancer Detection](https://arxiv.org/abs/2507.19174)
*Chiara Giangregorio,Cristina Maria Licciardello,Vanja Miskovic,Leonardo Provenzano,Alessandra Laura Giulia Pedrocchi,Andra Diana Dumitrascu,Arsela Prelaj,Marina Chiara Garassino,Emilia Ambrosini,Simona Ferrante*

Main category: cs.LG

TL;DR: 该研究探索了自动咳嗽分析作为非小细胞肺癌（NSCLC）早期筛查工具的潜力，使用机器学习和深度学习技术，发现CNN表现最佳，但SVM在计算资源有限的情况下也适用。


<details>
  <summary>Details</summary>
Motivation: 早期检测NSCLC对改善患者预后至关重要，需要新方法辅助早期诊断。

Method: 收集227名受试者的咳嗽音频，使用SVM、XGBoost、CNN和VGG16等算法分析，并通过SHAP增强模型可解释性。

Result: CNN测试集准确率达0.83，SVM稍低（验证集0.76，测试集0.78）。公平性分析显示年龄差异略大于性别差异。

Conclusion: 需更大、更多样化的数据集验证结果，尤其需纳入高风险和早期患者。

Abstract: Early detection of non-small cell lung cancer (NSCLC) is critical for
improving patient outcomes, and novel approaches are needed to facilitate early
diagnosis. In this study, we explore the use of automatic cough analysis as a
pre-screening tool for distinguishing between NSCLC patients and healthy
controls. Cough audio recordings were prospectively acquired from a total of
227 subjects, divided into NSCLC patients and healthy controls. The recordings
were analyzed using machine learning techniques, such as support vector machine
(SVM) and XGBoost, as well as deep learning approaches, specifically
convolutional neural networks (CNN) and transfer learning with VGG16. To
enhance the interpretability of the machine learning model, we utilized Shapley
Additive Explanations (SHAP). The fairness of the models across demographic
groups was assessed by comparing the performance of the best model across
different age groups (less than or equal to 58y and higher than 58y) and gender
using the equalized odds difference on the test set. The results demonstrate
that CNN achieves the best performance, with an accuracy of 0.83 on the test
set. Nevertheless, SVM achieves slightly lower performances (accuracy of 0.76
in validation and 0.78 in the test set), making it suitable in contexts with
low computational power. The use of SHAP for SVM interpretation further
enhances model transparency, making it more trustworthy for clinical
applications. Fairness analysis shows slightly higher disparity across age
(0.15) than gender (0.09) on the test set. Therefore, to strengthen our
findings' reliability, a larger, more diverse, and unbiased dataset is needed
-- particularly including individuals at risk of NSCLC and those in early
disease stages.

</details>


### [48] [WACA-UNet: Weakness-Aware Channel Attention for Static IR Drop Prediction in Integrated Circuit Design](https://arxiv.org/abs/2507.19197)
*Youngmin Seo,Yunhyeong Kwon,Younghun Park,HwiRyong Kim,Seungho Eum,Jinha Kim,Taigon Song,Juho Kim,Unsang Park*

Main category: cs.LG

TL;DR: 论文提出了一种基于注意力机制的Weakness-Aware Channel Attention (WACA)方法，用于改进VLSI设计中的IR drop预测，显著降低了误差并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 传统模拟方法计算成本高且难以扩展，而现有学习方法未能充分利用输入通道的差异性。

Method: 将IR drop预测重新定义为像素级回归任务，并提出WACA机制，通过两阶段门控策略增强弱特征通道并抑制过强通道。

Result: 在ICCAD-2023基准测试中，平均绝对误差降低61.1%，F1分数提高71.0%。

Conclusion: 通道间异质性是VLSI物理布局分析的关键归纳偏置，WACA方法显著提升了预测性能。

Abstract: Accurate spatial prediction of power integrity issues, such as IR drop, is
critical for reliable VLSI design. However, traditional simulation-based
solvers are computationally expensive and difficult to scale. We address this
challenge by reformulating IR drop estimation as a pixel-wise regression task
on heterogeneous multi-channel physical maps derived from circuit layouts.
Prior learning-based methods treat all input layers (e.g., metal, via, and
current maps) equally, ignoring their varying importance to prediction
accuracy. To tackle this, we propose a novel Weakness-Aware Channel Attention
(WACA) mechanism, which recursively enhances weak feature channels while
suppressing over-dominant ones through a two-stage gating strategy. Integrated
into a ConvNeXtV2-based attention U-Net, our approach enables adaptive and
balanced feature representation. On the public ICCAD-2023 benchmark, our method
outperforms the ICCAD-2023 contest winner by reducing mean absolute error by
61.1% and improving F1-score by 71.0%. These results demonstrate that
channel-wise heterogeneity is a key inductive bias in physical layout analysis
for VLSI.

</details>


### [49] [Physics-Informed Graph Neural Networks for Transverse Momentum Estimation in CMS Trigger Systems](https://arxiv.org/abs/2507.19205)
*Md Abrar Jahin,Shahriar Soudeep,M. F. Mridha,Muhammad Mostafa Monowar,Md. Abdul Hamid*

Main category: cs.LG

TL;DR: 提出了一种物理信息图神经网络框架，通过四种图构建策略和新型消息传递层，实现了高效且准确的粒子横向动量估计。


<details>
  <summary>Details</summary>
Motivation: 高能物理中实时粒子横向动量估计需要高效且准确的算法，现有静态机器学习模型在高堆积条件下性能下降，通用图神经网络缺乏领域结构优化。

Method: 采用四种图构建策略（站作为节点、特征作为节点、弯曲角度中心和伪快度中心表示）结合新型消息传递层和领域特定损失函数。

Result: 在CMS触发数据集上验证，站信息EdgeConv模型达到0.8525的MAE，参数比基线少55%；伪快度中心配置也表现出更高的准确性。

Conclusion: 物理引导的图神经网络在资源受限的触发系统中具有应用潜力。

Abstract: Real-time particle transverse momentum ($p_T$) estimation in high-energy
physics demands algorithms that are both efficient and accurate under strict
hardware constraints. Static machine learning models degrade under high pileup
and lack physics-aware optimization, while generic graph neural networks (GNNs)
often neglect domain structure critical for robust $p_T$ regression. We propose
a physics-informed GNN framework that systematically encodes detector geometry
and physical observables through four distinct graph construction strategies
that systematically encode detector geometry and physical observables:
station-as-node, feature-as-node, bending angle-centric, and pseudorapidity
($\eta$)-centric representations. This framework integrates these tailored
graph structures with a novel Message Passing Layer (MPL), featuring
intra-message attention and gated updates, and domain-specific loss functions
incorporating $p_{T}$-distribution priors. Our co-design methodology yields
superior accuracy-efficiency trade-offs compared to existing baselines.
Extensive experiments on the CMS Trigger Dataset validate the approach: a
station-informed EdgeConv model achieves a state-of-the-art MAE of 0.8525 with
$\ge55\%$ fewer parameters than deep learning baselines, especially TabNet,
while an $\eta$-centric MPL configuration also demonstrates improved accuracy
with comparable efficiency. These results establish the promise of
physics-guided GNNs for deployment in resource-constrained trigger systems.

</details>


### [50] [Dependency-aware synthetic tabular data generation](https://arxiv.org/abs/2507.19211)
*Chaithra Umesh,Kristian Schultz,Manjunath Mahendra,Saptarshi Bej,Olaf Wolkenhauer*

Main category: cs.LG

TL;DR: HFGF框架通过分层生成特征，显著提升了合成表格数据中功能依赖（FDs）和逻辑依赖（LDs）的保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在隐私敏感领域（如医疗）中难以保留特征间的确定性关联（FDs和LDs），导致合成数据质量不足。

Method: 提出HFGF框架，先独立生成特征，再基于预定义的FD和LD规则重建依赖特征。

Result: 在四个基准数据集上验证，HFGF显著提升了六种生成模型（如CTGAN、TVAE）对FDs和LDs的保留能力。

Conclusion: HFGF能显著提升合成数据的结构保真度和下游应用价值。

Abstract: Synthetic tabular data is increasingly used in privacy-sensitive domains such
as health care, but existing generative models often fail to preserve
inter-attribute relationships. In particular, functional dependencies (FDs) and
logical dependencies (LDs), which capture deterministic and rule-based
associations between features, are rarely or often poorly retained in synthetic
datasets. To address this research gap, we propose the Hierarchical Feature
Generation Framework (HFGF) for synthetic tabular data generation. We created
benchmark datasets with known dependencies to evaluate our proposed HFGF. The
framework first generates independent features using any standard generative
model, and then reconstructs dependent features based on predefined FD and LD
rules. Our experiments on four benchmark datasets with varying sizes, feature
imbalance, and dependency complexity demonstrate that HFGF improves the
preservation of FDs and LDs across six generative models, including CTGAN,
TVAE, and GReaT. Our findings demonstrate that HFGF can significantly enhance
the structural fidelity and downstream utility of synthetic tabular data.

</details>


### [51] [Component-Based Machine Learning for Indoor Flow and Temperature Fields Prediction Latent Feature Aggregation and Flow Interaction](https://arxiv.org/abs/2507.19233)
*Shaofan Wang,Nils Thuerey,Philipp Geyer*

Main category: cs.LG

TL;DR: 提出了一种基于组件的机器学习（CBML）替代模型，用于快速预测室内流速和温度场，替代传统CFD模拟。


<details>
  <summary>Details</summary>
Motivation: 传统CFD模拟计算量大，难以实时或迭代设计中使用，需高效替代方法。

Method: 采用三个神经网络：CAER提取压缩流特征，MLP映射入口速度到潜在表示，CNN聚合单入口特征到双入口场景。

Result: CBML模型在训练和测试数据上均能快速准确预测双组分流速和温度场。

Conclusion: CBML方法为室内气流和温度预测提供了一种高效替代方案。

Abstract: Accurate and efficient prediction of indoor airflow and temperature
distributions is essential for building energy optimization and occupant
comfort control. However, traditional CFD simulations are computationally
intensive, limiting their integration into real-time or design-iterative
workflows. This study proposes a component-based machine learning (CBML)
surrogate modeling approach to replace conventional CFD simulation for fast
prediction of indoor velocity and temperature fields. The model consists of
three neural networks: a convolutional autoencoder with residual connections
(CAER) to extract and compress flow features, a multilayer perceptron (MLP) to
map inlet velocities to latent representations, and a convolutional neural
network (CNN) as an aggregator to combine single-inlet features into dual-inlet
scenarios. A two-dimensional room with varying left and right air inlet
velocities is used as a benchmark case, with CFD simulations providing training
and testing data. Results show that the CBML model accurately and fast predicts
two-component aggregated velocity and temperature fields across both training
and testing datasets.

</details>


### [52] [A Markov Categorical Framework for Language Modeling](https://arxiv.org/abs/2507.19247)
*Yifan Zhang*

Main category: cs.LG

TL;DR: 论文通过马尔可夫范畴（MCs）框架分析自回归语言模型的生成过程和负对数似然（NLL）目标，揭示了其成功的信息理论和几何学原理。


<details>
  <summary>Details</summary>
Motivation: 尽管自回归语言模型在实践中表现出色，但其理论基础的深入理解仍不足。本文旨在通过MCs框架填补这一空白。

Method: 使用MCs和统计散度分解生成过程，分析信息流和学习到的几何结构。

Result: 揭示了NLL目标的隐含对比学习性质，解释了现代解码方法的成功，并量化了隐藏状态的信息盈余。

Conclusion: NLL训练通过信息几何学隐含地学习结构化表示空间，为现代语言模型的有效性提供了理论基础。

Abstract: Auto-regressive language models factorize sequence probabilities and are
trained by minimizing the negative log-likelihood (NLL) objective. While
empirically powerful, a deep theoretical understanding of why this simple
objective yields such versatile representations remains elusive. This work
introduces a unifying analytical framework using Markov Categories (MCs) to
deconstruct the AR generation process and the NLL objective. We model the
single-step generation map as a composition of Markov kernels in the category
Stoch. This compositional view, when enriched with statistical divergences,
allows us to dissect information flow and learned geometry. Our framework makes
three main contributions. First, we provide a formal, information-theoretic
rationale for the success of modern speculative decoding methods like EAGLE,
quantifying the information surplus in hidden states that these methods
exploit. Second, we formalize how NLL minimization forces the model to learn
not just the next token, but the data's intrinsic conditional stochasticity, a
process we analyze using categorical entropy. Third, and most centrally, we
prove that NLL training acts as an implicit form of spectral contrastive
learning. By analyzing the information geometry of the model's prediction head,
we show that NLL implicitly forces the learned representation space to align
with the eigenspectrum of a predictive similarity operator, thereby learning a
geometrically structured space without explicit contrastive pairs. This
compositional and information-geometric perspective reveals the deep structural
principles underlying the effectiveness of modern LMs. Project Page:
https://github.com/asiresearch/lm-theory

</details>


### [53] [Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs](https://arxiv.org/abs/2507.19334)
*Shuo Yang,Zheyu Zhang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: SPADA提出了一种轻量级生成框架，通过稀疏依赖图解决表格数据增强中的偏差和计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 高质量表格数据稀缺，现有方法依赖密集建模导致偏差和计算成本高。

Method: SPADA利用LLM诱导的稀疏依赖图，通过非参数和条件归一化流方法合成数据。

Result: 实验表明，SPADA减少约束违规4%，生成速度比基线快9,500倍。

Conclusion: SPADA是一种高效且轻量的表格数据增强方法。

Abstract: Tabular data is critical across diverse domains, yet high-quality datasets
remain scarce due to privacy concerns and the cost of collection. Contemporary
approaches adopt large language models (LLMs) for tabular augmentation, but
exhibit two major limitations: (1) dense dependency modeling among tabular
features that can introduce bias, and (2) high computational overhead in
sampling. To address these issues, we propose SPADA for SPArse
Dependency-driven Augmentation, a lightweight generative framework that
explicitly captures sparse dependencies via an LLM-induced graph. We treat each
feature as a node and synthesize values by traversing the graph, conditioning
each feature solely on its parent nodes. We explore two synthesis strategies: a
non-parametric method using Gaussian kernel density estimation, and a
conditional normalizing flow model that learns invertible mappings for
conditional density estimation. Experiments on four datasets show that SPADA
reduces constraint violations by 4% compared to diffusion-based methods and
accelerates generation by nearly 9,500 times over LLM-based baselines.

</details>


### [54] [Short-Form Video Recommendations with Multimodal Embeddings: Addressing Cold-Start and Bias Challenges](https://arxiv.org/abs/2507.19346)
*Andrii Dzhoha,Katya Mirylenka,Egor Malykh,Marco-Andrea Buchmann,Francesca Catino*

Main category: cs.LG

TL;DR: 论文探讨了在电商平台引入短视频内容时面临的推荐系统挑战，并提出了一种基于多模态视觉语言模型的视频检索方法，优于传统监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 随着短视频平台的流行，电商平台开始引入短视频内容以提升用户参与度，但推荐系统面临数据有限、位置偏差和时长偏差等挑战。

Method: 采用基于多模态视觉语言模型的视频检索系统，克服数据不足和偏差问题。

Result: 在线实验表明，该方法比传统监督学习方法更有效。

Conclusion: 多模态视觉语言模型在短视频推荐中具有潜力，能有效解决新内容引入时的挑战。

Abstract: In recent years, social media users have spent significant amounts of time on
short-form video platforms. As a result, established platforms in other
domains, such as e-commerce, have begun introducing short-form video content to
engage users and increase their time spent on the platform. The success of
these experiences is due not only to the content itself but also to a unique UI
innovation: instead of offering users a list of choices to click, platforms
actively recommend content for users to watch one at a time. This creates new
challenges for recommender systems, especially when launching a new video
experience. Beyond the limited interaction data, immersive feed experiences
introduce stronger position bias due to the UI and duration bias when
optimizing for watch-time, as models tend to favor shorter videos. These
issues, together with the feedback loop inherent in recommender systems, make
it difficult to build effective solutions. In this paper, we highlight the
challenges faced when introducing a new short-form video experience and present
our experience showing that, even with sufficient video interaction data, it
can be more beneficial to leverage a video retrieval system using a fine-tuned
multimodal vision-language model to overcome these challenges. This approach
demonstrated greater effectiveness compared to conventional supervised learning
methods in online experiments conducted on our e-commerce platform.

</details>


### [55] [Reconstruction of Sparse Urban Wireless Signals via Group Equivariant Non-Expansive Operators](https://arxiv.org/abs/2507.19349)
*Lorenzo Mario Amorosa,Francesco Conti,Nicola Quercioli,Flavio Zabini,Tayebeh Lotfi Mahyari,Yiqun Ge,Patrizio Frosini*

Main category: cs.LG

TL;DR: 本文提出了一种基于群等变非扩张算子（GENEOs）的方法，用于从稀疏测量中重建6G无线网络中的空间信号（如SINR地图），相比传统神经网络具有低复杂性和高效性。


<details>
  <summary>Details</summary>
Motivation: 在6G无线网络中，高分辨率获取空间信号（如SINR地图）成本高昂，而稀疏测量下的信号重建是一个关键挑战。GENEOs作为一种数学工具，能够结合应用特定的不变性，减少参数数量并缓解数据稀缺问题。

Method: 利用GENEOs的数学框架，通过稀疏采样重建SINR地图。该方法通过代数与几何约束反映对称性，减少参数需求。

Result: 实验表明，该方法在稀疏采样下能准确重建空间信号，性能与传统方法相当。

Conclusion: GENEOs为稀疏数据下的空间信号重建提供了一种高效且低复杂性的解决方案，适用于6G网络资源管理。

Abstract: In emerging communication systems such as sixth generation (6G) wireless
networks, efficient resource management and service delivery rely on accurate
knowledge of spatially-varying quantities like signal-to-interference-noise
ratio (SINR) maps, which are costly to acquire at high resolution. This work
explores the reconstruction of such spatial signals from sparse measurements
using Group Equivariant Non-Expansive Operators (GENEOs), offering a
low-complexity alternative to traditional neural networks. The concept of
GENEO, which originated in topological data analysis (TDA), is a mathematical
tool used in machine learning to represent agents modelled as functional
operators acting on data while incorporating application-specific invariances.
Leveraging these invariances reduces the number of parameters with respect to
traditional neural networks and mitigates data scarcity by enforcing known
algebraic and geometric constraints that reflect symmetries in the agents'
actions. In this paper, we introduce a novel GENEO-based approach for SINR map
reconstruction in urban wireless communication networks using extremely sparse
sampling. We demonstrate that this mathematical framework achieves competitive
performance compared to established methods. Our evaluation, conducted using
both statistical and TDA metrics, highlights the advantages of our approach in
accurately reconstructing spatial signals under severe data limitations on the
number of samples.

</details>


### [56] [A Data-Driven Approach to Estimate LEO Orbit Capacity Models](https://arxiv.org/abs/2507.19365)
*Braden Stock,Maddox McVarthy,Simone Servadio*

Main category: cs.LG

TL;DR: 结合SINDy算法和LSTM神经网络，提出了一种轻量化的低精度模型，用于预测LEO中的卫星和碎片传播。


<details>
  <summary>Details</summary>
Motivation: 解决高精度模型计算成本高的问题，提供快速准确的预测方法。

Method: 利用SINDy算法和LSTM神经网络，基于MOCAT-MC高精度模型的数据集构建低精度模型。

Result: 实现了对LEO中卫星和碎片传播的准确预测，且计算时间更短。

Conclusion: 该方法为空间物体传播预测提供了一种高效且准确的解决方案。

Abstract: Utilizing the Sparse Identification of Nonlinear Dynamics algorithm (SINDy)
and Long Short-Term Memory Recurrent Neural Networks (LSTM), the population of
resident space objects, divided into Active, Derelict, and Debris, in LEO can
be accurately modeled to predict future satellite and debris propagation. This
proposed approach makes use of a data set coming from a computational expensive
high-fidelity model, the MOCAT-MC, to provide a light, low-fidelity counterpart
that provides accurate forecasting in a shorter time frame.

</details>


### [57] [Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided Latent Space Manipulation](https://arxiv.org/abs/2507.19368)
*Julia Siekiera,Stefan Kramer*

Main category: cs.LG

TL;DR: 论文探讨了在医学图像分析中，通过结合变分自编码器（VAE）和和积网络（SPN）生成可解释的反事实解释，以解决深度学习模型的黑箱问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分析中表现优异，但其黑箱特性引发了可靠性和可解释性的担忧。反事实解释提供了一种理解模型决策的方法，但生成符合相似性约束且易于人类理解的解释仍具挑战性。

Method: 通过将半监督VAE的潜在空间建模为SPN，利用SPN的双重角色（潜在空间描述符和分类器），优化潜在空间中的反事实解释，使其既接近原始数据分布又与目标类别分布对齐。

Result: 在cheXpert数据集上的实验表明，SPN引导的潜在空间操作优于神经网络基线，同时分析了潜在变量正则化与反事实质量之间的权衡。

Conclusion: 结合VAE和SPN的方法能够生成高质量且可解释的反事实解释，为深度学习模型的可解释性提供了有效解决方案。

Abstract: Artificial intelligence is increasingly leveraged across various domains to
automate decision-making processes that significantly impact human lives. In
medical image analysis, deep learning models have demonstrated remarkable
performance. However, their inherent complexity makes them black box systems,
raising concerns about reliability and interpretability. Counterfactual
explanations provide comprehensible insights into decision processes by
presenting hypothetical "what-if" scenarios that alter model classifications.
By examining input alterations, counterfactual explanations provide patterns
that influence the decision-making process. Despite their potential, generating
plausible counterfactuals that adhere to similarity constraints providing
human-interpretable explanations remains a challenge. In this paper, we
investigate this challenge by a model-specific optimization approach. While
deep generative models such as variational autoencoders (VAEs) exhibit
significant generative power, probabilistic models like sum-product networks
(SPNs) efficiently represent complex joint probability distributions. By
modeling the likelihood of a semi-supervised VAE's latent space with an SPN, we
leverage its dual role as both a latent space descriptor and a classifier for a
given discrimination task. This formulation enables the optimization of latent
space counterfactuals that are both close to the original data distribution and
aligned with the target class distribution. We conduct experimental evaluation
on the cheXpert dataset. To evaluate the effectiveness of the integration of
SPNs, our SPN-guided latent space manipulation is compared against a neural
network baseline. Additionally, the trade-off between latent variable
regularization and counterfactual quality is analyzed.

</details>


### [58] [FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report](https://arxiv.org/abs/2507.19402)
*Matteo Cardaioli,Luca Marangoni,Giada Martini,Francesco Mazzolin,Luca Pajola,Andrea Ferretto Parodi,Alessandra Saitta,Maria Chiara Vernillo*

Main category: cs.LG

TL;DR: 该报告比较了经典、量子和量子混合机器学习模型在金融欺诈检测中的效果，发现经典树模型（如随机森林）表现最佳，量子模型中QSVM最有潜力。


<details>
  <summary>Details</summary>
Motivation: 金融交易复杂性和数量的增加对传统欺诈检测系统提出了挑战，需要探索更高效的模型。

Method: 开发行为特征工程框架，评估经典模型（如随机森林）与量子模型（如QSVM、VQC、HQNN）在AML数据集上的表现，并提出FD4QC系统架构。

Result: 随机森林表现最佳（准确率97.34%），量子模型中QSVM精度最高（77.15%），但计算开销大。

Conclusion: 报告为金融应用提供了基准，指出量子机器学习当前局限性，并展望未来研究方向。

Abstract: The increasing complexity and volume of financial transactions pose
significant challenges to traditional fraud detection systems. This technical
report investigates and compares the efficacy of classical, quantum, and
quantum-hybrid machine learning models for the binary classification of
fraudulent financial activities.
  As of our methodology, first, we develop a comprehensive behavioural feature
engineering framework to transform raw transactional data into a rich,
descriptive feature set. Second, we implement and evaluate a range of models on
the IBM Anti-Money Laundering (AML) dataset. The classical baseline models
include Logistic Regression, Decision Tree, Random Forest, and XGBoost. These
are compared against three hybrid classic quantum algorithms architectures: a
Quantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC),
and a Hybrid Quantum Neural Network (HQNN).
  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a
practical, API-driven system architecture designed for real-world deployment,
featuring a classical-first, quantum-enhanced philosophy with robust fallback
mechanisms.
  Our results demonstrate that classical tree-based models, particularly
\textit{Random Forest}, significantly outperform the quantum counterparts in
the current setup, achieving high accuracy (\(97.34\%\)) and F-measure
(\(86.95\%\)). Among the quantum models, \textbf{QSVM} shows the most promise,
delivering high precision (\(77.15\%\)) and a low false-positive rate
(\(1.36\%\)), albeit with lower recall and significant computational overhead.
  This report provides a benchmark for a real-world financial application,
highlights the current limitations of quantum machine learning in this domain,
and outlines promising directions for future research.

</details>


### [59] [On Arbitrary Predictions from Equally Valid Models](https://arxiv.org/abs/2507.19408)
*Sarah Lockfisch,Kristian Schwethelm,Martin Menten,Rickmer Braren,Daniel Rueckert,Alexander Ziller,Georgios Kaissis*

Main category: cs.LG

TL;DR: 模型多重性指存在多个机器学习模型对数据描述相同但预测结果可能不同，医学中可能导致对同一患者产生冲突预测。研究发现小规模集成模型可减少预测多重性，并建议在模型共识不足时由专家审核。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解并解决医学领域中模型多重性带来的预测不一致问题，以提高诊断可靠性。

Method: 通过实证分析不同医学任务和模型架构中的预测多重性，研究采用小规模集成模型和弃权策略来减少预测差异。

Result: 结果显示标准验证指标无法识别唯一最优模型，预测结果常受模型开发中任意选择影响。集成模型可减少预测多重性，高精度模型能进一步降低差异。

Conclusion: 结论强调临床中需考虑模型多重性，推荐使用集成策略提高诊断可靠性，并在模型共识不足时由专家介入。

Abstract: Model multiplicity refers to the existence of multiple machine learning
models that describe the data equally well but may produce different
predictions on individual samples. In medicine, these models can admit
conflicting predictions for the same patient -- a risk that is poorly
understood and insufficiently addressed.
  In this study, we empirically analyze the extent, drivers, and ramifications
of predictive multiplicity across diverse medical tasks and model
architectures, and show that even small ensembles can mitigate/eliminate
predictive multiplicity in practice. Our analysis reveals that (1) standard
validation metrics fail to identify a uniquely optimal model and (2) a
substantial amount of predictions hinges on arbitrary choices made during model
development. Using multiple models instead of a single model reveals instances
where predictions differ across equally plausible models -- highlighting
patients that would receive arbitrary diagnoses if any single model were used.
In contrast, (3) a small ensemble paired with an abstention strategy can
effectively mitigate measurable predictive multiplicity in practice;
predictions with high inter-model consensus may thus be amenable to automated
classification. While accuracy is not a principled antidote to predictive
multiplicity, we find that (4) higher accuracy achieved through increased model
capacity reduces predictive multiplicity.
  Our findings underscore the clinical importance of accounting for model
multiplicity and advocate for ensemble-based strategies to improve diagnostic
reliability. In cases where models fail to reach sufficient consensus, we
recommend deferring decisions to expert review.

</details>


### [60] [SILS: Strategic Influence on Liquidity Stability and Whale Detection in Concentrated-Liquidity DEXs](https://arxiv.org/abs/2507.19411)
*Ali RajabiNekoo,Laleh Rasoul,Amirfarhad Farhadi,Azadeh Zamanifar*

Main category: cs.LG

TL;DR: SILS框架通过动态、影响导向的方法识别高影响力流动性提供者（LPs），取代传统静态、基于规模的评估。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖名义资本规模或表面活动，导致风险分析不准确。SILS旨在提供更详细、动态的LP影响评估。

Method: 利用链上事件日志和智能合约执行痕迹计算指数时间加权流动性（ETWL），并应用无监督异常检测，定义流动性稳定性影响评分（LSIS）。

Result: SILS能更准确地识别高影响力LPs，减少误报和漏报，支持DeFi生态系统的风险管理。

Conclusion: SILS为DeFi协议提供了主动风险管理机制，显著提升市场稳定性和透明度。

Abstract: Traditional methods for identifying impactful liquidity providers (LPs) in
Concentrated Liquidity Market Makers (CLMMs) rely on broad measures, such as
nominal capital size or surface-level activity, which often lead to inaccurate
risk analysis. The SILS framework offers a significantly more detailed
approach, characterizing LPs not just as capital holders but as dynamic
systemic agents whose actions directly impact market stability. This represents
a fundamental paradigm shift from the static, volume-based analysis to a
dynamic, impact-focused understanding. This advanced approach uses on-chain
event logs and smart contract execution traces to compute Exponential
Time-Weighted Liquidity (ETWL) profiles and apply unsupervised anomaly
detection. Most importantly, it defines an LP's functional importance through
the Liquidity Stability Impact Score (LSIS), a counterfactual metric that
measures the potential degradation of the market if the LP withdraws. This
combined approach provides a more detailed and realistic characterization of an
LP's impact, moving beyond the binary and often misleading classifications used
by existing methods. This impact-focused and comprehensive approach enables
SILS to accurately identify high-impact LPs-including those missed by
traditional methods and supports essential applications like a protective
oracle layer and actionable trader signals, thereby significantly enhancing
DeFi ecosystem. The framework provides unprecedented transparency into the
underlying liquidity structure and associated risks, effectively reducing the
common false positives and uncovering critical false negatives found in
traditional models. Therefore, SILS provides an effective mechanism for
proactive risk management, transforming how DeFi protocols safeguard their
ecosystems against asymmetric liquidity behavior.

</details>


### [61] [Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding](https://arxiv.org/abs/2507.19427)
*StepFun,:,Bin Wang,Bojun Wang,Changyi Wan,Guanzhe Huang,Hanpeng Hu,Haonan Jia,Hao Nie,Mingliang Li,Nuo Chen,Siyu Chen,Song Yuan,Wuxun Xie,Xiaoniu Song,Xing Chen,Xingping Yang,Xuelin Zhang,Yanbo Yu,Yaoyu Wang,Yibo Zhu,Yimin Jiang,Yu Zhou,Yuanwei Lu,Houyi Li,Jingcheng Hu,Ka Man Lo,Ailin Huang,Binxing Jiao,Bo Li,Boyu Chen,Changxin Miao,Chang Lou,Chen Hu,Chen Xu,Chenfeng Yu,Chengyuan Yao,Daokuan Lv,Dapeng Shi,Deshan Sun,Ding Huang,Dingyuan Hu,Dongqing Pang,Enle Liu,Fajie Zhang,Fanqi Wan,Gulin Yan,Han Zhang,Han Zhou,Hanghao Wu,Hangyu Guo,Hanqi Chen,Hanshan Zhang,Hao Wu,Haocheng Zhang,Haolong Yan,Haoran Lv,Haoran Wei,Hebin Zhou,Heng Wang,Heng Wang,Hongxin Li,Hongyu Zhou,Hongyuan Wang,Huiyong Guo,Jia Wang,Jiahao Gong,Jialing Xie,Jian Zhou,Jianjian Sun,Jiaoren Wu,Jiaran Zhang,Jiayu Liu,Jie Cheng,Jie Luo,Jie Yan,Jie Yang,Jieyi Hou,Jinguang Zhang,Jinlan Cao,Jisheng Yin,Junfeng Liu,Junhao Huang,Junzhe Lin,Kaijun Tan,Kaixiang Li,Kang An,Kangheng Lin,Kenkun Liu,Lei Yang,Liang Zhao,Liangyu Chen,Lieyu Shi,Liguo Tan,Lin Lin,Lin Zhang,Lina Chen,Liwen Huang,Liying Shi,Longlong Gu,Mei Chen,Mengqiang Ren,Ming Li,Mingzhe Chen,Na Wang,Nan Wu,Qi Han,Qian Zhao,Qiang Zhang,Qianni Liu,Qiaohui Chen,Qiling Wu,Qinglin He,Qinyuan Tan,Qiufeng Wang,Qiuping Wu,Qiuyan Liang,Quan Sun,Rui Li,Ruihang Miao,Ruosi Wan,Ruyan Guo,Shangwu Zhong,Shaoliang Pang,Shengjie Fan,Shijie Shang,Shilei Jiang,Shiliang Yang,Shiming Hao,Shuli Gao,Siming Huang,Siqi Liu,Tiancheng Cao,Tianhao Cheng,Tianhao Peng,Wang You,Wei Ji,Wen Sun,Wenjin Deng,Wenqing He,Wenzhen Zheng,Xi Chen,Xiangwen Kong,Xianzhen Luo,Xiaobo Yang,Xiaojia Liu,Xiaoxiao Ren,Xin Han,Xin Li,Xin Wu,Xu Zhao,Yanan Wei,Yang Li,Yangguang Li,Yangshijie Xu,Yanming Xu,Yaqiang Shi,Yeqing Shen,Yi Yang,Yifei Yang,Yifeng Gong,Yihan Chen,Yijing Yang,Yinmin Zhang,Yizhuang Zhou,Yuanhao Ding,Yuantao Fan,Yuanzhen Yang,Yuchu Luo,Yue Peng,Yufan Lu,Yuhang Deng,Yuhe Yin,Yujie Liu,Yukun Chen,Yuling Zhao,Yun Mou,Yunlong Li,Yunzhou Ju,Yusheng Li,Yuxiang Yang,Yuxiang Zhang,Yuyang Chen,Zejia Weng,Zhe Xie,Zheng Ge,Zheng Gong,Zhenyi Lu,Zhewei Huang,Zhichao Chang,Zhiguo Huang,Zhirui Wang,Zidong Yang,Zili Wang,Ziqi Wang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Heung-Yeung Shum,Xiangyu Zhang*

Main category: cs.LG

TL;DR: Step-3是一种硬件感知的模型-系统协同设计，通过创新的注意力机制和分布式推理系统显著降低解码成本，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在解码过程中硬件效率低下的问题，尤其是在长上下文推理任务中。

Method: 采用Multi-Matrix Factorization Attention（MFA）减少KV缓存和计算量，以及Attention-FFN Disaggregation（AFD）分布式推理系统。

Result: Step-3在解码成本上显著优于DeepSeek-V3和Qwen3 MoE 235B，并在Hopper GPU上实现更高的解码吞吐量。

Conclusion: Step-3通过硬件对齐的设计，为LLM解码设定了新的Pareto前沿，展示了成本效益的关键因素。

Abstract: Large language models (LLMs) face low hardware efficiency during decoding,
especially for long-context reasoning tasks. This paper introduces Step-3, a
321B-parameter VLM with hardware-aware model-system co-design optimized for
minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel
Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces
both KV cache size and computation while maintaining high attention
expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed
inference system that decouples attention and Feed-Forward Network (FFN) layers
into specialized subsystems. This co-design achieves unprecedented cost
efficiency: Step-3 significantly reduces theoretical decoding costs compared
with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at
longer context. Step-3 achieves low cost while activating 38B parameters per
token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that
hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are
critical to cost-effectiveness. We perform a head-to-head comparison with
DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs
achieves a decoding throughput of up to 4,039 tokens per second per GPU under
50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324
in the same setup and sets a new Pareto frontier for LLM decoding.

</details>


### [62] [Observations Meet Actions: Learning Control-Sufficient Representations for Robust Policy Generalization](https://arxiv.org/abs/2507.19437)
*Yuliang Gu,Hongpeng Cao,Marco Caccamo,Naira Hovakimyan*

Main category: cs.LG

TL;DR: 论文提出了一种基于上下文的双重推理-控制问题框架，通过区分观察充分性和控制充分性，设计了一种新的上下文证据下界目标，并提出了BCPO算法，在连续控制任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习（RL）智能体在训练范围之外部署时捕获潜在变化（上下文）的问题。

Method: 将上下文RL重新定义为双重推理-控制问题，提出观察充分性和控制充分性的概念，并设计了一个分离表示学习和策略学习的上下文ELBO目标，通过BCPO算法实现优化。

Result: 在连续控制基准测试中，BCPO算法在样本效率和性能上优于其他基线方法，并在训练范围之外保持良好表现。

Conclusion: 该框架为基于上下文的RL提供了理论、诊断和实践的统一方法。

Abstract: Capturing latent variations ("contexts") is key to deploying
reinforcement-learning (RL) agents beyond their training regime. We recast
context-based RL as a dual inference-control problem and formally characterize
two properties and their hierarchy: observation sufficiency (preserving all
predictive information) and control sufficiency (retaining decision-making
relevant information). Exploiting this dichotomy, we derive a contextual
evidence lower bound(ELBO)-style objective that cleanly separates
representation learning from policy learning and optimizes it with Bottlenecked
Contextual Policy Optimization (BCPO), an algorithm that places a variational
information-bottleneck encoder in front of any off-policy policy learner. On
standard continuous-control benchmarks with shifting physical parameters, BCPO
matches or surpasses other baselines while using fewer samples and retaining
performance far outside the training regime. The framework unifies theory,
diagnostics, and practice for context-based RL.

</details>


### [63] [Forest-Guided Clustering -- Shedding Light into the Random Forest Black Box](https://arxiv.org/abs/2507.19455)
*Lisa Barros de Andrade e Sousa,Gregor Miller,Ronan Le Gleut,Dominik Thalmeier,Helena Pelin,Marie Piraud*

Main category: cs.LG

TL;DR: Forest-Guided Clustering (FGC) 是一种针对随机森林的模型解释方法，通过分组实例揭示局部和全局结构，提供可解释的聚类和特征重要性评分。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在敏感领域的应用增加，对可解释和可信决策的需求上升。随机森林因其集成特性难以解释。

Method: FGC 通过分组实例共享的决策路径，生成与模型内部逻辑一致的可解释聚类，并计算特征重要性评分。

Result: FGC 在基准数据集上准确恢复潜在子类结构，优于传统聚类和后解释方法；在AML转录组数据中揭示生物学相关亚群。

Conclusion: FGC 在性能和可解释性之间架起桥梁，提供超越特征级归因的结构感知洞察。

Abstract: As machine learning models are increasingly deployed in sensitive application
areas, the demand for interpretable and trustworthy decision-making has
increased. Random Forests (RF), despite their widespread use and strong
performance on tabular data, remain difficult to interpret due to their
ensemble nature. We present Forest-Guided Clustering (FGC), a model-specific
explainability method that reveals both local and global structure in RFs by
grouping instances according to shared decision paths. FGC produces
human-interpretable clusters aligned with the model's internal logic and
computes cluster-specific and global feature importance scores to derive
decision rules underlying RF predictions. FGC accurately recovered latent
subclass structure on a benchmark dataset and outperformed classical clustering
and post-hoc explanation methods. Applied to an AML transcriptomic dataset, FGC
uncovered biologically coherent subpopulations, disentangled disease-relevant
signals from confounders, and recovered known and novel gene expression
patterns. FGC bridges the gap between performance and interpretability by
providing structure-aware insights that go beyond feature-level attribution.

</details>


### [64] [Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts](https://arxiv.org/abs/2507.19477)
*Sang-Woo Lee,Sohee Yang,Donghyun Kwak,Noah Y. Siegel*

Main category: cs.LG

TL;DR: 本文探讨了利用LLMs进行事件预测的研究现状，指出近期技术进步使其接近超级预测者水平，并提出了大规模训练的研究方向和具体方法。


<details>
  <summary>Details</summary>
Motivation: 早期研究的方法问题引发了对LLMs用于事件预测的质疑，但近期改进的评估方法显示LLMs性能提升，加上强化学习和推理模型的成功，表明技术已成熟，适合开展大规模研究。

Method: 讨论了训练方法和数据获取两个方向，包括解决噪声稀疏性、知识截止和简单奖励结构问题的技术，以及利用市场、公共和爬取数据的方法。

Result: 近期技术进步为LLMs达到超级预测者水平提供了可能，技术改进可推动AI在社会更广领域提供预测智能。

Conclusion: 本文提出了实现超级预测者水平AI的具体路径，呼吁研究者关注这些方向。

Abstract: Many recent papers have studied the development of superforecaster-level
event forecasting LLMs. While methodological problems with early studies cast
doubt on the use of LLMs for event forecasting, recent studies with improved
evaluation methods have shown that state-of-the-art LLMs are gradually reaching
superforecaster-level performance, and reinforcement learning has also been
reported to improve future forecasting. Additionally, the unprecedented success
of recent reasoning models and Deep Research-style models suggests that
technology capable of greatly improving forecasting performance has been
developed. Therefore, based on these positive recent trends, we argue that the
time is ripe for research on large-scale training of superforecaster-level
event forecasting LLMs. We discuss two key research directions: training
methods and data acquisition. For training, we first introduce three
difficulties of LLM-based event forecasting training: noisiness-sparsity,
knowledge cut-off, and simple reward structure problems. Then, we present
related ideas to mitigate these problems: hypothetical event Bayesian networks,
utilizing poorly-recalled and counterfactual events, and auxiliary reward
signals. For data, we propose aggressive use of market, public, and crawling
datasets to enable large-scale training and evaluation. Finally, we explain how
these technical advances could enable AI to provide predictive intelligence to
society in broader areas. This position paper presents promising specific paths
and considerations for getting closer to superforecaster-level AI technology,
aiming to call for researchers' interest in these directions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [65] [Central limit theorems for the eigenvalues of graph Laplacians on data clouds](https://arxiv.org/abs/2507.18803)
*Chenghui Li,Nicolás García Trillos,Housen Li,Leo Suchan*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given i.i.d.\ samples $X_n =\{ x_1, \dots, x_n \}$ from a distribution
supported on a low dimensional manifold ${M}$ embedded in Eucliden space, we
consider the graph Laplacian operator $\Delta_n$ associated to an
$\varepsilon$-proximity graph over $X_n$ and study the asymptotic fluctuations
of its eigenvalues around their means. In particular, letting
$\hat{\lambda}_l^\varepsilon$ denote the $l$-th eigenvalue of $\Delta_n$, and
under suitable assumptions on the data generating model and on the rate of
decay of $\varepsilon$, we prove that $\sqrt{n } (\hat{\lambda}_{l}^\varepsilon
- \mathbb{E}[\hat{\lambda}_{l}^\varepsilon] )$ is asymptotically Gaussian with
a variance that we can explicitly characterize. A formal argument allows us to
interpret this asymptotic variance as the dissipation of a gradient flow of a
suitable energy with respect to the Fisher-Rao geometry. This geometric
interpretation allows us to give, in turn, a statistical interpretation of the
asymptotic variance in terms of a Cramer-Rao lower bound for the estimation of
the eigenvalues of certain weighted Laplace-Beltrami operator. The latter
interpretation suggests a form of asymptotic statistical efficiency for the
eigenvalues of the graph Laplacian. We also present CLTs for multiple
eigenvalues and through several numerical experiments explore the validity of
our results when some of the assumptions that we make in our theoretical
analysis are relaxed.

</details>


### [66] [Probably Approximately Correct Causal Discovery](https://arxiv.org/abs/2507.18903)
*Mian Wei,Somesh Jha,David Page*

Main category: stat.ML

TL;DR: 论文提出了一个基于资源约束的因果发现框架PACC，扩展了PAC学习理论，为多种因果方法提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现实应用中因果发现受限于数据和计算资源，需要高效且高准确性的方法。

Method: 扩展PAC学习理论，提出PACC框架，应用于倾向得分和工具变量等方法。

Result: PACC框架为SCCS等方法提供了理论保证，提升了资源约束下的因果发现效率。

Conclusion: PACC框架为资源受限的因果发现提供了理论基础和实际应用价值。

Abstract: The discovery of causal relationships is a foundational problem in artificial
intelligence, statistics, epidemiology, economics, and beyond. While elegant
theories exist for accurate causal discovery given infinite data, real-world
applications are inherently resource-constrained. Effective methods for
inferring causal relationships from observational data must perform well under
finite data and time constraints, where "performing well" implies achieving
high, though not perfect accuracy. In his seminal paper A Theory of the
Learnable, Valiant highlighted the importance of resource constraints in
supervised machine learning, introducing the concept of Probably Approximately
Correct (PAC) learning as an alternative to exact learning. Inspired by
Valiant's work, we propose the Probably Approximately Correct Causal (PACC)
Discovery framework, which extends PAC learning principles to the causal field.
This framework emphasizes both computational and sample efficiency for
established causal methods such as propensity score techniques and instrumental
variable approaches. Furthermore, we show that it can also provide theoretical
guarantees for other widely used methods, such as the Self-Controlled Case
Series (SCCS) method, which had previously lacked such guarantees.

</details>


### [67] [Perfect Clustering in Very Sparse Diverse Multiplex Networks](https://arxiv.org/abs/2507.19423)
*Marianna Pensky*

Main category: stat.ML

TL;DR: 论文研究了DIMPLE-SGRDPG网络模型，提出了一种基于张量的方法，能够在更稀疏的网络中实现完美聚类。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要网络足够密集，而新方法旨在通过信息整合在稀疏网络中实现完美聚类。

Method: 提出了一种基于张量的方法，整合所有层的信息，替代传统的逐层分析。

Result: 新方法在稀疏条件下实现了完美聚类，且理论结果与计算下界一致。

Conclusion: 该方法在稀疏网络中表现优异，为多层网络聚类提供了新思路。

Abstract: The paper studies the DIverse MultiPLEx Signed Generalized Random Dot Product
Graph (DIMPLE-SGRDPG) network model (Pensky (2024)), where all layers of the
network have the same collection of nodes. In addition, all layers can be
partitioned into groups such that the layers in the same group are embedded in
the same ambient subspace but otherwise matrices of connection probabilities
can be all different. This setting includes majority of multilayer network
models as its particular cases. The key task in this model is to recover the
groups of layers with unique subspace structures, since the case where all
layers of the network are embedded in the same subspace has been fairly well
studied. Until now, clustering of layers in such networks was based on the
layer-per-layer analysis, which required the multilayer network to be
sufficiently dense. Nevertheless, in this paper we succeeded in pooling
information in all layers together and providing a tensor-based methodology
that ensures perfect clustering for a much sparser network. Our theoretical
results, established under intuitive non-restrictive assumptions, assert that
the new technique achieves perfect clustering under sparsity conditions that,
up to logarithmic factors, coincide with the computational lower bound derived
for a much simpler model.

</details>
