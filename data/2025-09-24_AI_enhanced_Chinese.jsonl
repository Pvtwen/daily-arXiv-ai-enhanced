{"id": "2509.18310", "categories": ["eess.SP", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.18310", "abs": "https://arxiv.org/abs/2509.18310", "authors": ["Bahar Kor", "Bipin Gaikwad", "Abani Patra", "Eric L. Miller"], "title": "On Multi-entity, Multivariate Quickest Change Point Detection", "comment": null, "summary": "We propose a framework for online Change Point Detection (CPD) from\nmulti-entity, multivariate time series data, motivated by applications in crowd\nmonitoring where traditional sensing methods (e.g., video surveillance) may be\ninfeasible. Our approach addresses the challenge of detecting system-wide\nbehavioral shifts in complex, dynamic environments where the number and\nbehavior of individual entities may be uncertain or evolve. We introduce the\nconcept of Individual Deviation from Normality (IDfN), computed via a\nreconstruction-error-based autoencoder trained on normal behavior. We aggregate\nthese individual deviations using mean, variance, and Kernel Density Estimates\n(KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or\nabrupt changes, we apply statistical deviation metrics and the Cumulative Sum\n(CUSUM) technique to these scores. Our unsupervised approach eliminates the\nneed for labeled data or feature extraction, enabling real-time operation on\nstreaming input. Evaluations on both synthetic datasets and crowd simulations,\nexplicitly designed for anomaly detection in group behaviors, demonstrate that\nour method accurately detects significant system-level changes, offering a\nscalable and privacy-preserving solution for monitoring complex multi-agent\nsystems. In addition to this methodological contribution, we introduce new,\nchallenging multi-entity multivariate time series datasets generated from crowd\nsimulations in Unity and coupled nonlinear oscillators. To the best of our\nknowledge, there is currently no publicly available dataset of this type\ndesigned explicitly to evaluate CPD in complex collective and interactive\nsystems, highlighting an essential gap that our work addresses.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u591a\u5b9e\u4f53\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5728\u7ebf\u53d8\u70b9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e2a\u4f53\u504f\u79bb\u6b63\u5e38\u6027\u6982\u5ff5\u548c\u7cfb\u7edf\u7ea7\u5f02\u5e38\u8bc4\u5206\u6765\u68c0\u6d4b\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7cfb\u7edf\u7ea7\u884c\u4e3a\u53d8\u5316", "motivation": "\u89e3\u51b3\u5728\u4eba\u7fa4\u76d1\u63a7\u7b49\u5e94\u7528\u4e2d\u4f20\u7edf\u4f20\u611f\u65b9\u6cd5\u4e0d\u53ef\u884c\u65f6\u7684\u6311\u6218\uff0c\u68c0\u6d4b\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7cfb\u7edf\u7ea7\u884c\u4e3a\u53d8\u5316\uff0c\u5176\u4e2d\u4e2a\u4f53\u5b9e\u4f53\u7684\u6570\u91cf\u548c\u884c\u4e3a\u53ef\u80fd\u4e0d\u786e\u5b9a\u6216\u6f14\u5316", "method": "\u4f7f\u7528\u57fa\u4e8e\u91cd\u6784\u8bef\u5dee\u7684\u81ea\u7f16\u7801\u5668\u8ba1\u7b97\u4e2a\u4f53\u504f\u79bb\u6b63\u5e38\u6027\uff0c\u901a\u8fc7\u5747\u503c\u3001\u65b9\u5dee\u548c\u6838\u5bc6\u5ea6\u4f30\u8ba1\u805a\u5408\u5f97\u5230\u7cfb\u7edf\u7ea7\u5f02\u5e38\u8bc4\u5206\uff0c\u5e94\u7528\u7edf\u8ba1\u504f\u5dee\u5ea6\u91cf\u548c\u7d2f\u79ef\u548c\u6280\u672f\u68c0\u6d4b\u6301\u7eed\u6027\u6216\u7a81\u7136\u53d8\u5316", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u4eba\u7fa4\u6a21\u62df\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u68c0\u6d4b\u663e\u8457\u7684\u7cfb\u7edf\u7ea7\u53d8\u5316\uff0c\u4e3a\u76d1\u63a7\u590d\u6742\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u53ef\u6269\u5c55\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6807\u8bb0\u6570\u636e\u6216\u7279\u5f81\u63d0\u53d6\uff0c\u652f\u6301\u5b9e\u65f6\u6d41\u5f0f\u5904\u7406\uff0c\u540c\u65f6\u586b\u8865\u4e86\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u96c6\u4f53\u4ea4\u4e92\u7cfb\u7edf\u4e2d\u53d8\u70b9\u68c0\u6d4b\u7684\u516c\u5f00\u6570\u636e\u96c6\u7a7a\u767d"}}
{"id": "2509.18381", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18381", "abs": "https://arxiv.org/abs/2509.18381", "authors": ["Nicholas L. K. Goradia", "Harpreet S. Dhillon", "R. Michael Buehrer"], "title": "Multi-Target Detection for Cognitive MIMO Radar Networks", "comment": "12 pages, 16 figures", "summary": "In this work, we develop centralized and decentralized signal fusion\ntechniques for constant false alarm rate (CFAR) multi-target detection with a\ncognitive radar network in unknown noise and clutter distributions. Further, we\nfirst develop a detection statistic for co-located monostatic MIMO radar in\nunknown noise and clutter distributions which is asymptotically CFAR as the\nnumber of received pulses over all antennas grows large, and we provide\nconditions under which this detection statistic is valid. We leverage\nreinforcement learning (RL) for improved multi-target detection performance,\nwhere the radar learns likely target locations in a search area. These results\nare then generalized to the setting of cognitive radar networks, where radars\ncollaborate to learn where targets are likely to appear in a search area. We\nshow a fundamental tradeoff between the spatial and temporal domain for CFAR\ndetection in unknown noise and clutter distributions; in other words, we show a\ntradeoff between the number of radar antennas and the number of temporal\nsamples. We show the benefits and tradeoffs with centralized and decentralized\ndetection with a network of cognitive radars.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u4fe1\u53f7\u878d\u5408\u6280\u672f\uff0c\u7528\u4e8e\u5728\u672a\u77e5\u566a\u58f0\u548c\u6742\u6ce2\u5206\u5e03\u4e0b\u7684CFAR\u591a\u76ee\u6807\u68c0\u6d4b\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u7a7a\u95f4\u57df\u548c\u65f6\u95f4\u57df\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u89e3\u51b3\u5728\u672a\u77e5\u566a\u58f0\u548c\u6742\u6ce2\u5206\u5e03\u73af\u5883\u4e0b\uff0c\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\u8fdb\u884c\u591a\u76ee\u6807\u68c0\u6d4b\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5982\u4f55\u5b9e\u73b0\u6052\u865a\u8b66\u7387\u68c0\u6d4b\u5e76\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u5171\u5740\u5355\u57fa\u5730MIMO\u96f7\u8fbe\u7684\u68c0\u6d4b\u7edf\u8ba1\u91cf\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8ba9\u96f7\u8fbe\u5b66\u4e60\u76ee\u6807\u53ef\u80fd\u4f4d\u7f6e\uff0c\u5e76\u5c06\u65b9\u6cd5\u63a8\u5e7f\u5230\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\u4e2d\u7684\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u68c0\u6d4b\u573a\u666f\u3002", "result": "\u63d0\u51fa\u7684\u68c0\u6d4b\u7edf\u8ba1\u91cf\u5728\u63a5\u6536\u8109\u51b2\u6570\u8db3\u591f\u5927\u65f6\u6e10\u8fd1CFAR\uff0c\u5c55\u793a\u4e86\u96f7\u8fbe\u5929\u7ebf\u6570\u91cf\u4e0e\u65f6\u95f4\u6837\u672c\u6570\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u9a8c\u8bc1\u4e86\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u77e5\u73af\u5883\u4e0b\u7684\u591a\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2509.18426", "categories": ["eess.SP", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2509.18426", "abs": "https://arxiv.org/abs/2509.18426", "authors": ["Ziad Hatab", "Michael Ernst Gadringer", "Arash Arsanjani", "Wolfgang Boesch"], "title": "Automatic Model Extraction of the Match Standard in Symmetric--Reciprocal--Match Calibration", "comment": "https://github.com/ZiadHatab/srm-calibration", "summary": "This paper addresses the modeling of parasitics of the match standard in the\nsymmetric-reciprocal-match (SRM) calibration method of vector network analyzers\n(VNAs). In the general SRM procedure, the match standard is assumed to be fully\nknown. Here, we demonstrate that the match can be modeled with an arbitrary\nfrequency-dependent model using a non-linear global optimization procedure. To\nhighlight the validity of the suggested approach, numerical tests were\nconducted, demonstrating the ability to recover the match standard parasitic\nmodel down to software numerical precision. Additionally, we performed\nmicrostrip line measurements to compare the SRM calibration with match modeling\nto the multiline thru-reflect-line (TRL) calibration one, showing that\nautomatic model extraction can achieve accuracy similar to using a match\nstandard defined through multiline TRL calibration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u77e2\u91cf\u7f51\u7edc\u5206\u6790\u4eea\u5bf9\u79f0\u4e92\u6613\u5339\u914d\u6821\u51c6\u65b9\u6cd5\u4e2d\u5bf9\u5339\u914d\u6807\u51c6\u5bc4\u751f\u53c2\u6570\u8fdb\u884c\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u5168\u5c40\u4f18\u5316\u7a0b\u5e8f\u5b9e\u73b0\u4efb\u610f\u9891\u7387\u76f8\u5173\u6a21\u578b\u7684\u81ea\u52a8\u63d0\u53d6\u3002", "motivation": "\u4f20\u7edfSRM\u6821\u51c6\u65b9\u6cd5\u5047\u8bbe\u5339\u914d\u6807\u51c6\u5b8c\u5168\u5df2\u77e5\uff0c\u4f46\u5b9e\u9645\u4e0a\u5339\u914d\u6807\u51c6\u7684\u5bc4\u751f\u53c2\u6570\u4f1a\u5f71\u54cd\u6821\u51c6\u7cbe\u5ea6\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u5168\u5c40\u4f18\u5316\u7a0b\u5e8f\u5bf9\u5339\u914d\u6807\u51c6\u8fdb\u884c\u4efb\u610f\u9891\u7387\u76f8\u5173\u6a21\u578b\u5efa\u6a21\uff0c\u901a\u8fc7\u6570\u503c\u6d4b\u8bd5\u548c\u5fae\u5e26\u7ebf\u6d4b\u91cf\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u6570\u503c\u6d4b\u8bd5\u663e\u793a\u80fd\u591f\u4ee5\u8f6f\u4ef6\u6570\u503c\u7cbe\u5ea6\u6062\u590d\u5339\u914d\u6807\u51c6\u5bc4\u751f\u6a21\u578b\uff0c\u5fae\u5e26\u7ebf\u6d4b\u91cf\u8868\u660e\u8be5\u65b9\u6cd5\u7cbe\u5ea6\u4e0e\u4f7f\u7528\u591a\u7ebfTRL\u6821\u51c6\u5b9a\u4e49\u7684\u5339\u914d\u6807\u51c6\u76f8\u5f53\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u52a8\u6a21\u578b\u63d0\u53d6\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8SRM\u6821\u51c6\u7cbe\u5ea6\uff0c\u8fbe\u5230\u4e0e\u591a\u7ebfTRL\u6821\u51c6\u76f8\u5f53\u7684\u51c6\u786e\u5ea6\u3002"}}
{"id": "2509.18555", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18555", "abs": "https://arxiv.org/abs/2509.18555", "authors": ["Ping Wang", "Zulin Wang", "Yuanfang Ma", "Xiaosi Tian", "Yuanhan Ni"], "title": "A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems", "comment": "6 pages, 5 figures, 2025 IEEE International Conference on\n  Communications", "summary": "This paper introduces a secure affine frequency division multiplexing\n(SE-AFDM) for wireless communication systems to enhance communication security.\nBesides configuring the parameter c1 to obtain communication reliability under\ndoubly selective channels, we also utilize the time-varying parameter c2 to\nimprove the security of the communications system. The derived input-output\nrelation shows that the legitimate receiver can eliminate the nonlinear impact\nintroduced by the time-varying c2 without losing the bit error rate (BER)\nperformance. Moreover, it is theoretically proved that the eavesdropper cannot\nseparate the time-varying c2 and random information symbols, such that the BER\nperformance of the eavesdropper is severely deteriorated. Meanwhile, the\nanalysis of the effective signal-to-interference-plus-noise ratio (SINR) of the\neavesdropper illustrates that the SINR decreases as the value range of c2\nexpands. Numerical results verify that the proposed SE-AFDM waveform has\nsignificant security while maintaining good BER performance in high-mobility\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u4eff\u5c04\u9891\u5206\u590d\u7528\uff08SE-AFDM\uff09\u6280\u672f\uff0c\u901a\u8fc7\u65f6\u53d8\u53c2\u6570c2\u589e\u5f3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u6cd5\u63a5\u6536\u8005\u7684\u826f\u597d\u8bef\u7801\u7387\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u7684\u901a\u4fe1\u5b89\u5168\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u4fdd\u8bc1\u53ef\u9760\u6027\u7684\u540c\u65f6\u96be\u4ee5\u6709\u6548\u9632\u6b62\u7a83\u542c\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9002\u5e94\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u53c8\u80fd\u63d0\u4f9b\u5b89\u5168\u6027\u7684\u65b0\u578b\u6ce2\u5f62\u6280\u672f\u3002", "method": "\u5728AFDM\u57fa\u7840\u4e0a\u5f15\u5165\u65f6\u53d8\u53c2\u6570c2\u4f5c\u4e3a\u5b89\u5168\u673a\u5236\u3002\u5408\u6cd5\u63a5\u6536\u8005\u53ef\u4ee5\u6d88\u9664c2\u5f15\u5165\u7684\u975e\u7ebf\u6027\u5f71\u54cd\uff0c\u800c\u7a83\u542c\u8005\u65e0\u6cd5\u5206\u79bbc2\u548c\u968f\u673a\u4fe1\u606f\u7b26\u53f7\uff0c\u5bfc\u81f4\u5176\u8bef\u7801\u7387\u6027\u80fd\u4e25\u91cd\u6076\u5316\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cSE-AFDM\u5728\u4fdd\u6301\u826f\u597d\u8bef\u7801\u7387\u6027\u80fd\u7684\u540c\u65f6\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u7a83\u542c\u8005\u7684\u6709\u6548\u4fe1\u5e72\u566a\u6bd4\uff0c\u6269\u5c55c2\u7684\u53d6\u503c\u8303\u56f4\u53ef\u8fdb\u4e00\u6b65\u589e\u5f3a\u5b89\u5168\u6027\u3002", "conclusion": "SE-AFDM\u6ce2\u5f62\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u901a\u4fe1\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u7684\u6709\u6548\u5e73\u8861\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18155", "categories": ["stat.ML", "cs.LG", "physics.data-an", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.18155", "abs": "https://arxiv.org/abs/2509.18155", "authors": ["Aaron Pim", "Tristan Pryer"], "title": "Surrogate Modelling of Proton Dose with Monte Carlo Dropout Uncertainty Quantification", "comment": "21 pages, 23 figures", "summary": "Accurate proton dose calculation using Monte Carlo (MC) is computationally\ndemanding in workflows like robust optimisation, adaptive replanning, and\nprobabilistic inference, which require repeated evaluations. To address this,\nwe develop a neural surrogate that integrates Monte Carlo dropout to provide\nfast, differentiable dose predictions along with voxelwise predictive\nuncertainty. The method is validated through a series of experiments, starting\nwith a one-dimensional analytic benchmark that establishes accuracy,\nconvergence, and variance decomposition. Two-dimensional bone-water phantoms,\ngenerated using TOPAS Geant4, demonstrate the method's behavior under domain\nheterogeneity and beam uncertainty, while a three-dimensional water phantom\nconfirms scalability for volumetric dose prediction. Across these settings, we\nseparate epistemic (model) from parametric (input) contributions, showing that\nepistemic variance increases under distribution shift, while parametric\nvariance dominates at material boundaries. The approach achieves significant\nspeedups over MC while retaining uncertainty information, making it suitable\nfor integration into robust planning, adaptive workflows, and uncertainty-aware\noptimisation in proton therapy.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u96c6\u6210\u8499\u7279\u5361\u6d1bdropout\u7684\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u5feb\u901f\u3001\u53ef\u5fae\u5206\u7684\u8d28\u5b50\u5242\u91cf\u9884\u6d4b\u548c\u4f53\u7d20\u7ea7\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u663e\u8457\u52a0\u901f\u8499\u7279\u5361\u6d1b\u8ba1\u7b97\u5e76\u4fdd\u7559\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002", "motivation": "\u8499\u7279\u5361\u6d1b\u8d28\u5b50\u5242\u91cf\u8ba1\u7b97\u5728\u9c81\u68d2\u4f18\u5316\u3001\u81ea\u9002\u5e94\u91cd\u89c4\u5212\u548c\u6982\u7387\u63a8\u65ad\u7b49\u9700\u8981\u91cd\u590d\u8bc4\u4f30\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u8ba1\u7b97\u91cf\u5de8\u5927\uff0c\u9700\u8981\u5f00\u53d1\u5feb\u901f\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u96c6\u6210\u8499\u7279\u5361\u6d1bdropout\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u4e00\u7ef4\u89e3\u6790\u57fa\u51c6\u3001\u4e8c\u7ef4\u9aa8\u6c34\u6a21\u4f53\u548c\u4e09\u7ef4\u6c34\u6a21\u4f53\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u6536\u655b\u6027\u548c\u65b9\u5dee\u5206\u89e3\u80fd\u529b\u3002", "result": "\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u663e\u793a\u8ba4\u77e5\u65b9\u5dee\u589e\u52a0\uff0c\u5728\u6750\u6599\u8fb9\u754c\u53c2\u6570\u65b9\u5dee\u5360\u4e3b\u5bfc\uff0c\u76f8\u6bd4\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u540c\u65f6\u4fdd\u7559\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u8d28\u5b50\u6cbb\u7597\u4e2d\u7684\u9c81\u68d2\u89c4\u5212\u3001\u81ea\u9002\u5e94\u5de5\u4f5c\u6d41\u7a0b\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4f18\u5316\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18103", "categories": ["cs.LG", "math.NT"], "pdf": "https://arxiv.org/pdf/2509.18103", "abs": "https://arxiv.org/abs/2509.18103", "authors": ["Jennifer Dodgson", "Michael Joedhitya", "Adith Ramdas", "Surender Suresh Kumar", "Adarsh Singh Chauhan", "Akira Rafhael", "Wang Mingshu", "Nordine Lotfi"], "title": "Machine Learnability as a Measure of Order in Aperiodic Sequences", "comment": null, "summary": "Research on the distribution of prime numbers has revealed a dual character:\ndeterministic in definition yet exhibiting statistical behavior reminiscent of\nrandom processes. In this paper we show that it is possible to use an\nimage-focused machine learning model to measure the comparative regularity of\nprime number fields at specific regions of an Ulam spiral. Specifically, we\ndemonstrate that in pure accuracy terms, models trained on blocks extracted\nfrom regions of the spiral in the vicinity of 500m outperform models trained on\nblocks extracted from the region representing integers lower than 25m. This\nimplies existence of more easily learnable order in the former region than in\nthe latter. Moreover, a detailed breakdown of precision and recall scores seem\nto imply that the model is favouring a different approach to classification in\ndifferent regions of the spiral, focusing more on identifying prime patterns at\nlower numbers and more on eliminating composites at higher numbers. This aligns\nwith number theory conjectures suggesting that at higher orders of magnitude we\nshould see diminishing noise in prime number distributions, with averages\n(density, AP equidistribution) coming to dominate, while local randomness\nregularises after scaling by log x. Taken together, these findings point toward\nan interesting possibility: that machine learning can serve as a new\nexperimental instrument for number theory. Notably, the method shows potential\n1 for investigating the patterns in strong and weak primes for cryptographic\npurposes.", "AI": {"tldr": "\u4f7f\u7528\u56fe\u50cf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u6790Ulam\u87ba\u65cb\u4e2d\u4e0d\u540c\u533a\u57df\u7684\u8d28\u6570\u5206\u5e03\u89c4\u5f8b\u6027\uff0c\u53d1\u73b0\u9ad8\u6570\u503c\u533a\u57df\uff08\u7ea65\u4ebf\u9644\u8fd1\uff09\u6bd4\u4f4e\u6570\u503c\u533a\u57df\uff08\u4f4e\u4e8e2500\u4e07\uff09\u66f4\u5bb9\u6613\u5b66\u4e60\uff0c\u8868\u660e\u9ad8\u6570\u503c\u533a\u57df\u5b58\u5728\u66f4\u6709\u5e8f\u7684\u8d28\u6570\u6a21\u5f0f\u3002", "motivation": "\u8d28\u6570\u5206\u5e03\u5177\u6709\u786e\u5b9a\u6027\u5b9a\u4e49\u4f46\u8868\u73b0\u51fa\u7c7b\u4f3c\u968f\u673a\u8fc7\u7a0b\u7684\u7edf\u8ba1\u884c\u4e3a\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u80fd\u5426\u4f5c\u4e3a\u6570\u8bba\u7684\u65b0\u5b9e\u9a8c\u5de5\u5177\uff0c\u7279\u522b\u662f\u7528\u4e8e\u5206\u6790\u8d28\u6570\u5728\u4e0d\u540c\u6570\u503c\u533a\u57df\u7684\u89c4\u5f8b\u6027\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u56fe\u50cf\u805a\u7126\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4eceUlam\u87ba\u65cb\u7684\u4e0d\u540c\u533a\u57df\u63d0\u53d6\u6570\u636e\u5757\u8fdb\u884c\u8bad\u7ec3\uff0c\u6bd4\u8f83\u6a21\u578b\u5728\u4f4e\u6570\u503c\u533a\u57df\uff08<25m\uff09\u548c\u9ad8\u6570\u503c\u533a\u57df\uff08~500m\uff09\u7684\u5b66\u4e60\u6548\u679c\u3002", "result": "\u5728500m\u9644\u8fd1\u533a\u57df\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u7eaf\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u572825m\u4ee5\u4e0b\u533a\u57df\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8868\u660e\u9ad8\u6570\u503c\u533a\u57df\u5b58\u5728\u66f4\u6613\u5b66\u4e60\u7684\u79e9\u5e8f\u3002\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u5206\u6790\u663e\u793a\u6a21\u578b\u5728\u4e0d\u540c\u533a\u57df\u91c7\u7528\u4e0d\u540c\u5206\u7c7b\u7b56\u7565\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u53ef\u4f5c\u4e3a\u6570\u8bba\u7814\u7a76\u7684\u65b0\u5b9e\u9a8c\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u7814\u7a76\u5f3a\u8d28\u6570\u548c\u5f31\u8d28\u6570\u6a21\u5f0f\u65b9\u9762\u5177\u6709\u5bc6\u7801\u5b66\u5e94\u7528\u6f5c\u529b\uff0c\u652f\u6301\u4e86\u6570\u8bba\u4e2d\u5173\u4e8e\u9ad8\u6570\u91cf\u7ea7\u8d28\u6570\u5206\u5e03\u566a\u58f0\u51cf\u5c11\u7684\u731c\u60f3\u3002"}}
{"id": "2509.18727", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18727", "abs": "https://arxiv.org/abs/2509.18727", "authors": ["Yasaman Ettefagh", "Sharief Saleh", "Musa Furkan Keskin", "Hui Chen", "Gonzalo Seco-Granados", "Henk Wymeersch"], "title": "Integrated Cellular and LEO-based Positioning and Synchronization under User Mobility", "comment": null, "summary": "This paper investigates the localization, synchronization, and speed\nestimation of a mobile user equipment (UE) leveraging integrated terrestrial\nand non-terrestrial networks (NTNs), in particular low Earth orbit (LEO)\nsatellites. We focus on a minimal setup in which the UE received signal from\nonly one base station (BS) and one LEO satellite. We derive a generic signal\nmodel accounting for mobility, clock and frequency offsets, based on which a\nhierarchy of simplified models are proposed and organized by computational\ncomplexity. Estimation algorithms are developed for each model to facilitate\nefficient and accurate parameter recovery. Rigorous simulations validate the\neffectiveness of the proposed models, demonstrating their suitability across\ndiverse scenarios. The findings highlight how the trade-off between complexity\nand performance can be optimized for varying deployment environments and\napplication requirements, offering valuable insights for 6G positioning and\nsynchronization systems under user mobility.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5229\u7528\u5730\u9762\u548c\u975e\u5730\u9762\u7f51\u7edc\uff08\u7279\u522b\u662f\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\uff09\u5bf9\u79fb\u52a8\u7528\u6237\u8bbe\u5907\u8fdb\u884c\u5b9a\u4f4d\u3001\u540c\u6b65\u548c\u901f\u5ea6\u4f30\u8ba1\uff0c\u91cd\u70b9\u5206\u6790\u5355\u57fa\u7ad9\u5355\u536b\u661f\u6700\u5c0f\u914d\u7f6e\u4e0b\u7684\u4fe1\u53f7\u6a21\u578b\u548c\u4f30\u8ba1\u7b97\u6cd5\u3002", "motivation": "\u968f\u77406G\u7f51\u7edc\u53d1\u5c55\uff0c\u9700\u8981\u89e3\u51b3\u79fb\u52a8\u7528\u6237\u5728\u6df7\u5408\u5730\u9762-\u536b\u661f\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u7cbe\u786e\u5b9a\u4f4d\u3001\u540c\u6b65\u548c\u901f\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u6700\u5c0f\u914d\u7f6e\u573a\u666f\u4e0b\u3002", "method": "\u5efa\u7acb\u8003\u8651\u79fb\u52a8\u6027\u3001\u65f6\u949f\u548c\u9891\u7387\u504f\u79fb\u7684\u901a\u7528\u4fe1\u53f7\u6a21\u578b\uff0c\u63d0\u51fa\u6309\u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u5c42\u7684\u7b80\u5316\u6a21\u578b\u7cfb\u5217\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6a21\u578b\u5f00\u53d1\u76f8\u5e94\u7684\u4f30\u8ba1\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\uff0c\u5c55\u793a\u4e86\u590d\u6742\u5ea6\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u4f18\u5316\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a6G\u79fb\u52a8\u7528\u6237\u5b9a\u4f4d\u548c\u540c\u6b65\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u8868\u660e\u53ef\u4ee5\u6839\u636e\u90e8\u7f72\u73af\u5883\u548c\u5e94\u7528\u9700\u6c42\u4f18\u5316\u590d\u6742\u5ea6\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2509.18349", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18349", "abs": "https://arxiv.org/abs/2509.18349", "authors": ["Saptati Datta", "Nicolas W. Hengartner", "Yulia Pimonova", "Natalie E. Klein", "Nicholas Lubbers"], "title": "Statistical Insight into Meta-Learning via Predictor Subspace Characterization and Quantification of Task Diversity", "comment": null, "summary": "Meta-learning has emerged as a powerful paradigm for leveraging information\nacross related tasks to improve predictive performance on new tasks. In this\npaper, we propose a statistical framework for analyzing meta-learning through\nthe lens of predictor subspace characterization and quantification of task\ndiversity. Specifically, we model the shared structure across tasks using a\nlatent subspace and introduce a measure of diversity that captures\nheterogeneity across task-specific predictors. We provide both simulation-based\nand theoretical evidence indicating that achieving the desired prediction\naccuracy in meta-learning depends on the proportion of predictor variance\naligned with the shared subspace, as well as on the accuracy of subspace\nestimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u5668\u5b50\u7a7a\u95f4\u8868\u5f81\u548c\u4efb\u52a1\u591a\u6837\u6027\u91cf\u5316\u6765\u5206\u6790\u5143\u5b66\u4e60\uff0c\u8868\u660e\u9884\u6d4b\u7cbe\u5ea6\u53d6\u51b3\u4e8e\u9884\u6d4b\u5668\u65b9\u5dee\u4e0e\u5171\u4eab\u5b50\u7a7a\u95f4\u7684\u5bf9\u9f50\u6bd4\u4f8b\u4ee5\u53ca\u5b50\u7a7a\u95f4\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5143\u5b66\u4e60\u5df2\u6210\u4e3a\u5229\u7528\u76f8\u5173\u4efb\u52a1\u4fe1\u606f\u6765\u63d0\u9ad8\u65b0\u4efb\u52a1\u9884\u6d4b\u6027\u80fd\u7684\u5f3a\u5927\u8303\u5f0f\uff0c\u4f46\u9700\u8981\u7edf\u8ba1\u6846\u67b6\u6765\u5206\u6790\u5176\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u5b50\u7a7a\u95f4\u5efa\u6a21\u4efb\u52a1\u95f4\u7684\u5171\u4eab\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u8861\u91cf\u4efb\u52a1\u7279\u5b9a\u9884\u6d4b\u5668\u5f02\u8d28\u6027\u7684\u591a\u6837\u6027\u5ea6\u91cf\u3002", "result": "\u6a21\u62df\u548c\u7406\u8bba\u8bc1\u636e\u8868\u660e\uff0c\u5143\u5b66\u4e60\u8fbe\u5230\u9884\u671f\u9884\u6d4b\u7cbe\u5ea6\u4f9d\u8d56\u4e8e\u9884\u6d4b\u5668\u65b9\u5dee\u4e0e\u5171\u4eab\u5b50\u7a7a\u95f4\u7684\u5bf9\u9f50\u6bd4\u4f8b\u548c\u5b50\u7a7a\u95f4\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "conclusion": "\u4efb\u52a1\u591a\u6837\u6027\u548c\u5171\u4eab\u5b50\u7a7a\u95f4\u4f30\u8ba1\u7cbe\u5ea6\u662f\u5f71\u54cd\u5143\u5b66\u4e60\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2509.18104", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18104", "abs": "https://arxiv.org/abs/2509.18104", "authors": ["Wenqian Li", "Youjia Yang", "Ruoxi Jia", "Yan Pang"], "title": "Data Valuation and Selection in a Federated Model Marketplace", "comment": null, "summary": "In the era of Artificial Intelligence (AI), marketplaces have become\nessential platforms for facilitating the exchange of data products to foster\ndata sharing. Model transactions provide economic solutions in data\nmarketplaces that enhance data reusability and ensure the traceability of data\nownership. To establish trustworthy data marketplaces, Federated Learning (FL)\nhas emerged as a promising paradigm to enable collaborative learning across\nsiloed datasets while safeguarding data privacy. However, effective data\nvaluation and selection from heterogeneous sources in the FL setup remain key\nchallenges. This paper introduces a comprehensive framework centered on a\nWasserstein-based estimator tailored for FL. The estimator not only predicts\nmodel performance across unseen data combinations but also reveals the\ncompatibility between data heterogeneity and FL aggregation algorithms. To\nensure privacy, we propose a distributed method to approximate Wasserstein\ndistance without requiring access to raw data. Furthermore, we demonstrate that\nmodel performance can be reliably extrapolated under the neural scaling law,\nenabling effective data selection without full-scale training. Extensive\nexperiments across diverse scenarios, such as label skew, mislabeled, and\nunlabeled sources, show that our approach consistently identifies\nhigh-performing data combinations, paving the way for more reliable FL-based\nmodel marketplaces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6570\u636e\u5e02\u573a\u4e2d\u7684\u5f02\u6784\u6570\u636e\u4f30\u503c\u548c\u9009\u62e9\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u65b9\u6cd5\u8fd1\u4f3cWasserstein\u8ddd\u79bb\u6765\u4fdd\u62a4\u9690\u79c1\uff0c\u5e76\u5229\u7528\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728AI\u65f6\u4ee3\uff0c\u6570\u636e\u5e02\u573a\u9700\u8981\u53ef\u4fe1\u7684\u6a21\u578b\u4ea4\u6613\u65b9\u6848\u3002\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff0c\u4f46\u5728\u5f02\u6784\u6570\u636e\u6e90\u7684\u4f30\u503c\u548c\u9009\u62e9\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u5e76\u786e\u4fdd\u6570\u636e\u517c\u5bb9\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u4f30\u8ba1\u5668\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u65b9\u6cd5\u8fd1\u4f3cWasserstein\u8ddd\u79bb\u800c\u4e0d\u9700\u8981\u539f\u59cb\u6570\u636e\u8bbf\u95ee\uff0c\u5229\u7528\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u5916\u63a8\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u9009\u62e9\u3002", "result": "\u5728\u6807\u7b7e\u504f\u659c\u3001\u9519\u8bef\u6807\u7b7e\u548c\u65e0\u6807\u7b7e\u7b49\u591a\u79cd\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4e00\u81f4\u5730\u8bc6\u522b\u9ad8\u6027\u80fd\u6570\u636e\u7ec4\u5408\uff0c\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u5e02\u573a\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u6a21\u578b\u5e02\u573a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u4f30\u503c\u548c\u9009\u62e9\u65b9\u6848\uff0c\u901a\u8fc7Wasserstein\u8ddd\u79bb\u4f30\u8ba1\u548c\u6027\u80fd\u9884\u6d4b\u89e3\u51b3\u4e86\u5f02\u6784\u6570\u636e\u6e90\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.18753", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18753", "abs": "https://arxiv.org/abs/2509.18753", "authors": ["Hao Wu", "Xinyuan Yao", "Rui Ni", "Chen Gong", "Kaibin Huang"], "title": "Detection Capability Comparison Between Intensity Detection and Splitting Detection for Rydberg-Atomic Sensors", "comment": null, "summary": "Rydberg atomic quantum receivers have been seen as novel radio frequency\nmeasurements and the high sensitivity to a large range of frequencies makes it\nattractive for communications reception. However, their unique physical\ncharacteristics enable two fundamental signal readout schemes: intensity-based\ndetection and splitting-based detection. The former measures the electric\nfields through laser intensity, while the latter utilizes Autler-Townes\nsplitting. In this work, we systematically categorize and model existing signal\nreadout methods, classifying them into these two paradigms. Then, we derive the\nmaximum likelihood estimation procedures and corresponding Cram\\'er-Rao lower\nbounds (CRLB) for each detection modality. Through the analysis of the CRLB, we\npropose strategy for both readout schemes to enhance sensitivity and minimize\nestimation variance: acquiring data in regions with maximal slope magnitudes.\nWhile this approach has been implemented in intensity-based detection (e.g.,\nsuperheterodyne schemes), its application to splitting-based detection remains\nunexplored. Implementation of non-uniform frequency scanning, with preferential\nsampling at regions exhibiting maximum peak slopes combined with our proposed\nmaximum likelihood splitting estimation method, achieves significantly reduced\nestimation variance compared to conventional polynomial fitting. The\ncomparative analysis reveals the optimal detection performance of the two\ndetection schemes. This work also contributes to enhancing the accuracy of\nmicrowave calibration. Numerical results reveal that both fundamental signal\nreadout methods achieve lower estimation variance based on our proposed maximum\nlikelihood estimation approach.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u91cc\u5fb7\u5821\u539f\u5b50\u91cf\u5b50\u63a5\u6536\u5668\u7684\u4e24\u79cd\u4fe1\u53f7\u8bfb\u51fa\u65b9\u6848\uff08\u5f3a\u5ea6\u68c0\u6d4b\u548c\u5206\u88c2\u68c0\u6d4b\uff09\uff0c\u63d0\u51fa\u4e86\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u65b9\u6cd5\u548c\u514b\u62c9\u7f8e-\u7f57\u4e0b\u754c\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u5728\u6700\u5927\u659c\u7387\u533a\u57df\u4f18\u5148\u91c7\u6837\u6765\u4f18\u5316\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u91cc\u5fb7\u5821\u539f\u5b50\u91cf\u5b50\u63a5\u6536\u5668\u5728\u5c04\u9891\u6d4b\u91cf\u4e2d\u5177\u6709\u9ad8\u7075\u654f\u5ea6\uff0c\u4f46\u5176\u4e24\u79cd\u57fa\u672c\u4fe1\u53f7\u8bfb\u51fa\u65b9\u6848\uff08\u5f3a\u5ea6\u68c0\u6d4b\u548c\u5206\u88c2\u68c0\u6d4b\uff09\u7684\u6027\u80fd\u4f18\u5316\u7b56\u7565\u5c1a\u672a\u7cfb\u7edf\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u5206\u88c2\u68c0\u6d4b\u4e2d\u6700\u5927\u659c\u7387\u533a\u57df\u91c7\u6837\u7684\u5e94\u7528\u5c1a\u672a\u63a2\u7d22\u3002", "method": "1. \u7cfb\u7edf\u5206\u7c7b\u548c\u5efa\u6a21\u73b0\u6709\u4fe1\u53f7\u8bfb\u51fa\u65b9\u6cd5\uff1b2. \u63a8\u5bfc\u4e24\u79cd\u68c0\u6d4b\u6a21\u5f0f\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7a0b\u5e8f\u548c\u514b\u62c9\u7f8e-\u7f57\u4e0b\u754c\uff1b3. \u63d0\u51fa\u5728\u6700\u5927\u659c\u7387\u5e45\u5ea6\u533a\u57df\u91c7\u96c6\u6570\u636e\u7684\u4f18\u5316\u7b56\u7565\uff1b4. \u5b9e\u73b0\u975e\u5747\u5300\u9891\u7387\u626b\u63cf\u548c\u6700\u5927\u4f3c\u7136\u5206\u88c2\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u4e24\u79cd\u57fa\u672c\u4fe1\u53f7\u8bfb\u51fa\u65b9\u6cd5\u57fa\u4e8e\u63d0\u51fa\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u65b9\u6cd5\u90fd\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u4f30\u8ba1\u65b9\u5dee\uff0c\u5206\u88c2\u68c0\u6d4b\u4e2d\u975e\u5747\u5300\u9891\u7387\u626b\u63cf\u7ed3\u5408\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u76f8\u6bd4\u4f20\u7edf\u591a\u9879\u5f0f\u62df\u5408\u663e\u8457\u51cf\u5c11\u4e86\u4f30\u8ba1\u65b9\u5dee\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4e24\u79cd\u68c0\u6d4b\u65b9\u6848\u63d0\u4f9b\u4e86\u6027\u80fd\u4f18\u5316\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u6700\u4f18\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u6709\u52a9\u4e8e\u63d0\u9ad8\u5fae\u6ce2\u6821\u51c6\u7684\u51c6\u786e\u6027\uff0c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u65b9\u6cd5\u5728\u4e24\u79cd\u68c0\u6d4b\u65b9\u6848\u4e2d\u90fd\u80fd\u6709\u6548\u964d\u4f4e\u4f30\u8ba1\u65b9\u5dee\u3002"}}
{"id": "2509.18477", "categories": ["stat.ML", "cs.LG", "62N05, 68T07"], "pdf": "https://arxiv.org/pdf/2509.18477", "abs": "https://arxiv.org/abs/2509.18477", "authors": ["Xiaogang Su"], "title": "End-Cut Preference in Survival Trees", "comment": "24 pages, 2 figures", "summary": "The end-cut preference (ECP) problem, referring to the tendency to favor\nsplit points near the boundaries of a feature's range, is a well-known issue in\nCART (Breiman et al., 1984). ECP may induce highly imbalanced and biased\nsplits, obscure weak signals, and lead to tree structures that are both\nunstable and difficult to interpret. For survival trees, we show that ECP also\narises when using greedy search to select the optimal cutoff point by\nmaximizing the log-rank test statistic. To address this issue, we propose a\nsmooth sigmoid surrogate (SSS) approach, in which the hard-threshold indicator\nfunction is replaced by a smooth sigmoid function. We further demonstrate, both\ntheoretically and through numerical illustrations, that SSS provides an\neffective remedy for mitigating or avoiding ECP.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9CART\u7b97\u6cd5\u4e2d\u7684\u7aef\u70b9\u504f\u597d\u95ee\u9898\uff0c\u63d0\u51fa\u4f7f\u7528\u5e73\u6ed1sigmoid\u66ff\u4ee3\u65b9\u6cd5\u6765\u89e3\u51b3\u751f\u5b58\u6811\u4e2d\u7684\u4e0d\u5e73\u8861\u5206\u5272\u95ee\u9898\u3002", "motivation": "\u7aef\u70b9\u504f\u597d\u95ee\u9898\u4f1a\u5bfc\u81f4\u4e0d\u5e73\u8861\u548c\u504f\u501a\u7684\u5206\u5272\uff0c\u63a9\u76d6\u5f31\u4fe1\u53f7\uff0c\u5e76\u4ea7\u751f\u4e0d\u7a33\u5b9a\u4e14\u96be\u4ee5\u89e3\u91ca\u7684\u6811\u7ed3\u6784\u3002\u5728\u751f\u5b58\u6811\u4e2d\uff0c\u4f7f\u7528\u5bf9\u6570\u79e9\u68c0\u9a8c\u7edf\u8ba1\u91cf\u8fdb\u884c\u8d2a\u5a6a\u641c\u7d22\u65f6\u4e5f\u4f1a\u51fa\u73b0\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5e73\u6ed1sigmoid\u66ff\u4ee3\u65b9\u6cd5\uff0c\u7528\u5e73\u6ed1\u7684sigmoid\u51fd\u6570\u66ff\u6362\u786c\u9608\u503c\u6307\u793a\u51fd\u6570\u3002", "result": "\u7406\u8bba\u548c\u6570\u503c\u6a21\u62df\u5747\u8bc1\u660e\uff0cSSS\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u6216\u907f\u514d\u7aef\u70b9\u504f\u597d\u95ee\u9898\u3002", "conclusion": "\u5e73\u6ed1sigmoid\u66ff\u4ee3\u65b9\u6cd5\u4e3a\u89e3\u51b3CART\u7b97\u6cd5\u4e2d\u7684\u7aef\u70b9\u504f\u597d\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18105", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18105", "abs": "https://arxiv.org/abs/2509.18105", "authors": ["Nachiket N. Naik", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand", "comment": null, "summary": "We study learning of continuous-time inventory dynamics under stochastic\ndemand and quantify when structure helps or hurts forecasting of the bullwhip\neffect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the\nentire right-hand side against a physics-informed Universal Differential\nEquation (UDE) that preserves conservation and order-up-to structure while\nlearning a small residual policy term. Classical supply chain models explain\nthe bullwhip through control/forecasting choices and information sharing, while\nrecent physics-informed and neural differential equation methods blend domain\nconstraints with learned components. It is unclear whether structural bias\nhelps or hinders forecasting under different demand regimes. We address this by\nusing a single-echelon testbed with three demand regimes - AR(1)\n(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done\non varying fractions of each trajectory, followed by evaluation of multi-step\nforecasts for inventory I, order rate O, and demand D. Across the structured\nregimes, UDE consistently generalizes better: with 90% of the training horizon,\ninventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96\nto 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the\nflexibility of NODE is better. These trends persist as train18 ing data\nshrinks, with NODE exhibiting phase drift in extrapolation while UDE remains\nstable but underreacts to rare spikes. Our results provide concrete guidance:\nenforce structure when noise is light-tailed or temporally correlated; relax\nstructure when extreme events dominate. Beyond inventory control, the results\noffer guidance for hybrid modeling in scientific and engineering systems:\nenforce known structure when conservation laws and modest noise dominate, and\nrelax structure to capture extremes in settings where rare events drive\ndynamics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u5b8c\u5168\u5b66\u4e60\u7684\u795e\u7ecfODE\u4e0e\u7269\u7406\u4fe1\u606f\u901a\u7528\u5fae\u5206\u65b9\u7a0b\u5728\u8fde\u7eed\u65f6\u95f4\u5e93\u5b58\u52a8\u529b\u5b66\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5728\u7ed3\u6784\u5316\u9700\u6c42\u673a\u5236\u4e0bUDE\u6cdb\u5316\u66f4\u597d\uff0c\u800c\u5728\u91cd\u5c3e\u9700\u6c42\u4e0bNODE\u66f4\u7075\u6d3b\u3002", "motivation": "\u7814\u7a76\u7ed3\u6784\u504f\u5dee\u5728\u4e0d\u540c\u9700\u6c42\u673a\u5236\u4e0b\u5bf9\u725b\u97ad\u6548\u5e94\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u4e3a\u79d1\u5b66\u548c\u5de5\u7a0b\u7cfb\u7edf\u4e2d\u7684\u6df7\u5408\u5efa\u6a21\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528\u5355\u7ea7\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6bd4\u8f83\u5b8c\u5168\u5b66\u4e60\u7684NODE\u548c\u4fdd\u7559\u5b88\u6052\u7ed3\u6784\u7684UDE\u5728\u4e09\u79cd\u9700\u6c42\u673a\u5236\uff08AR(1)\u3001i.i.d.\u9ad8\u65af\u3001\u91cd\u5c3e\u5bf9\u6570\u6b63\u6001\uff09\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5728\u7ed3\u6784\u5316\u9700\u6c42\u673a\u5236\u4e0b\uff0cUDE\u6cdb\u5316\u66f4\u597d\uff08\u5e93\u5b58RMSE\u4ece4.92\u964d\u81f30.26\uff09\uff1b\u5728\u91cd\u5c3e\u9700\u6c42\u4e0bNODE\u66f4\u4f18\u3002\u8bad\u7ec3\u6570\u636e\u51cf\u5c11\u65f6\uff0cNODE\u51fa\u73b0\u76f8\u4f4d\u6f02\u79fb\uff0cUDE\u4fdd\u6301\u7a33\u5b9a\u4f46\u5bf9\u7f55\u89c1\u5cf0\u503c\u53cd\u5e94\u4e0d\u8db3\u3002", "conclusion": "\u5f53\u566a\u58f0\u4e3a\u8f7b\u5c3e\u6216\u65f6\u95f4\u76f8\u5173\u65f6\u5e94\u5f3a\u5236\u7ed3\u6784\u7ea6\u675f\uff1b\u5f53\u6781\u7aef\u4e8b\u4ef6\u4e3b\u5bfc\u65f6\u5e94\u653e\u677e\u7ed3\u6784\u7ea6\u675f\u3002\u8fd9\u4e3a\u79d1\u5b66\u5de5\u7a0b\u7cfb\u7edf\u7684\u6df7\u5408\u5efa\u6a21\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2509.18799", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18799", "abs": "https://arxiv.org/abs/2509.18799", "authors": ["Sijia Cheng", "Liang Liu", "Ove Edfors", "Juan Vidal Alegria"], "title": "Highly Parallel Singular Value Decomposition for Low-Latency MIMO Processing", "comment": "5 pages, 6 figures, accepted to SiPS2025", "summary": "Singular value decomposition (SVD) is widely used in wireless systems,\nincluding multiple-input multiple-output (MIMO) processing and dimension\nreduction in distributed MIMO (D-MIMO). However, the iterative nature of\ndecomposition methods results in increased execution time as system size grows,\nposing challenges for real-time and low-latency applications. To address this,\nwe analyze the latency of state-of-art SVD methods, and highlight the\nefficiency of a 4-step highly parallel method based on Gram matrix\ntridiagonalization. Furthermore, we develop a time complexity (processing\nlatency) analysis framework with hardware profiling, allowing scalable and\nrealistic evaluation without full implementation. The numerical results\ndemonstrate the superior time efficiency of the selected parallel method,\nparticularly in massive MIMO scenarios.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86SVD\u5728\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGram\u77e9\u9635\u4e09\u5bf9\u89d2\u5316\u76844\u6b65\u5e76\u884c\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u786c\u4ef6\u5206\u6790\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u6846\u67b6\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21MIMO\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u6027\u3002", "motivation": "SVD\u5728MIMO\u5904\u7406\u7b49\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u8fed\u4ee3\u5206\u89e3\u65b9\u6cd5\u968f\u7cfb\u7edf\u89c4\u6a21\u589e\u5927\u4f1a\u5bfc\u81f4\u6267\u884c\u65f6\u95f4\u589e\u52a0\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u548c\u4f4e\u5ef6\u8fdf\u5e94\u7528\u9700\u6c42\u3002", "method": "\u5206\u6790\u4e86\u73b0\u6709SVD\u65b9\u6cd5\u7684\u5ef6\u8fdf\uff0c\u63d0\u51fa\u57fa\u4e8eGram\u77e9\u9635\u4e09\u5bf9\u89d2\u5316\u76844\u6b65\u9ad8\u5ea6\u5e76\u884c\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u7ed3\u5408\u786c\u4ef6\u5206\u6790\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u6240\u9009\u5e76\u884c\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u65f6\u95f4\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21MIMO\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u57fa\u4e8eGram\u77e9\u9635\u4e09\u5bf9\u89d2\u5316\u7684\u5e76\u884cSVD\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18484", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18484", "abs": "https://arxiv.org/abs/2509.18484", "authors": ["Yuanchen Wu", "Yubai Yuan"], "title": "Estimating Heterogeneous Causal Effect on Networks via Orthogonal Learning", "comment": null, "summary": "Estimating causal effects on networks is important for both scientific\nresearch and practical applications. Unlike traditional settings that assume\nthe Stable Unit Treatment Value Assumption (SUTVA), interference allows an\nintervention/treatment on one unit to affect the outcomes of others.\nUnderstanding both direct and spillover effects is critical in fields such as\nepidemiology, political science, and economics. Causal inference on networks\nfaces two main challenges. First, causal effects are typically heterogeneous,\nvarying with unit features and local network structure. Second, connected units\noften exhibit dependence due to network homophily, creating confounding between\nstructural correlations and causal effects. In this paper, we propose a\ntwo-stage method to estimate heterogeneous direct and spillover effects on\nnetworks. The first stage uses graph neural networks to estimate nuisance\ncomponents that depend on the complex network topology. In the second stage, we\nadjust for network confounding using these estimates and infer causal effects\nthrough a novel attention-based interference model. Our approach balances\nexpressiveness and interpretability, enabling downstream tasks such as\nidentifying influential neighborhoods and recovering the sign of spillover\neffects. We integrate the two stages using Neyman orthogonalization and\ncross-fitting, which ensures that errors from nuisance estimation contribute\nonly at higher order. As a result, our causal effect estimates are robust to\nbias and misspecification in modeling causal effects under network\ndependencies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\u6765\u4f30\u8ba1\u7f51\u7edc\u4e2d\u7684\u5f02\u8d28\u6027\u76f4\u63a5\u6548\u5e94\u548c\u6ea2\u51fa\u6548\u5e94\uff0c\u89e3\u51b3\u7f51\u7edc\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u6df7\u6dc6\u6311\u6218", "motivation": "\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u56e0\u679c\u63a8\u65ad\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u56e0\u679c\u6548\u5e94\u7684\u5f02\u8d28\u6027\uff08\u968f\u5355\u5143\u7279\u5f81\u548c\u5c40\u90e8\u7f51\u7edc\u7ed3\u6784\u53d8\u5316\uff09\u548c\u7f51\u7edc\u540c\u8d28\u6027\u5bfc\u81f4\u7684\u6df7\u6dc6\u95ee\u9898\u3002\u4f20\u7edfSUTVA\u5047\u8bbe\u4e0d\u9002\u7528\u4e8e\u5b58\u5728\u5e72\u6270\u7684\u7f51\u7edc\u573a\u666f", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u4f9d\u8d56\u590d\u6742\u7f51\u7edc\u62d3\u6251\u7684\u5e72\u6270\u9879\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u65b0\u9896\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5e72\u6270\u6a21\u578b\u8c03\u6574\u7f51\u7edc\u6df7\u6dc6\u5e76\u63a8\u65ad\u56e0\u679c\u6548\u5e94\u3002\u91c7\u7528Neyman\u6b63\u4ea4\u5316\u548c\u4ea4\u53c9\u62df\u5408\u6280\u672f", "result": "\u65b9\u6cd5\u5728\u8868\u8fbe\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u80fd\u591f\u8bc6\u522b\u6709\u5f71\u54cd\u529b\u7684\u90bb\u57df\u548c\u6062\u590d\u6ea2\u51fa\u6548\u5e94\u7684\u7b26\u53f7\u3002\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u5bf9\u7f51\u7edc\u4f9d\u8d56\u4e0b\u7684\u6a21\u578b\u8bef\u8bbe\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6d41\u884c\u75c5\u5b66\u3001\u653f\u6cbb\u79d1\u5b66\u548c\u7ecf\u6d4e\u5b66\u7b49\u9886\u57df\u9700\u8981\u540c\u65f6\u8003\u8651\u76f4\u63a5\u548c\u6ea2\u51fa\u6548\u5e94\u7684\u5e94\u7528\u573a\u666f"}}
{"id": "2509.18106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18106", "abs": "https://arxiv.org/abs/2509.18106", "authors": ["Elisa Tomassini", "Enrique Garc\u00eda-Mac\u00edas", "Filippo Ubertini"], "title": "Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks", "comment": null, "summary": "The growing use of permanent monitoring systems has increased data\navailability, offering new opportunities for structural assessment but also\nposing scalability challenges, especially across large bridge networks.\nManaging multiple structures requires tracking and comparing long-term\nbehaviour efficiently. To address this, knowledge transfer between similar\nstructures becomes essential. This study proposes a model-based transfer\nlearning approach using neural network surrogate models, enabling a model\ntrained on one bridge to be adapted to another with similar characteristics.\nThese models capture shared damage mechanisms, supporting a scalable and\ngeneralizable monitoring framework. The method was validated using real data\nfrom two bridges. The transferred model was integrated into a Bayesian\ninference framework for continuous damage assessment based on modal features\nfrom monitoring data. Results showed high sensitivity to damage location,\nseverity, and extent. This approach enhances real-time monitoring and enables\ncross-structure knowledge transfer, promoting smart monitoring strategies and\nimproved resilience at the network level.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\uff0c\u4f7f\u5728\u4e00\u4e2a\u6865\u6881\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u591f\u9002\u5e94\u5177\u6709\u76f8\u4f3c\u7279\u5f81\u7684\u53e6\u4e00\u4e2a\u6865\u6881\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u6865\u6881\u7f51\u7edc\u4e2d\u7ed3\u6784\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "motivation": "\u968f\u7740\u6c38\u4e45\u76d1\u6d4b\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6570\u636e\u53ef\u7528\u6027\u589e\u52a0\uff0c\u4e3a\u7ed3\u6784\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5728\u5927\u578b\u6865\u6881\u7f51\u7edc\u4e2d\u9762\u4e34\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u7ba1\u7406\u591a\u4e2a\u7ed3\u6784\u9700\u8981\u9ad8\u6548\u8ddf\u8e2a\u548c\u6bd4\u8f83\u957f\u671f\u884c\u4e3a\uff0c\u56e0\u6b64\u76f8\u4f3c\u7ed3\u6784\u4e4b\u95f4\u7684\u77e5\u8bc6\u8f6c\u79fb\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\u3002\u5c06\u5728\u4e00\u4e2a\u6865\u6881\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8fc1\u79fb\u5230\u5177\u6709\u76f8\u4f3c\u7279\u5f81\u7684\u53e6\u4e00\u4e2a\u6865\u6881\u4e0a\uff0c\u8fd9\u4e9b\u6a21\u578b\u6355\u6349\u5171\u4eab\u7684\u635f\u4f24\u673a\u5236\u3002\u5c06\u8fc1\u79fb\u6a21\u578b\u96c6\u6210\u5230\u8d1d\u53f6\u65af\u63a8\u7406\u6846\u67b6\u4e2d\uff0c\u57fa\u4e8e\u76d1\u6d4b\u6570\u636e\u7684\u6a21\u6001\u7279\u5f81\u8fdb\u884c\u8fde\u7eed\u635f\u4f24\u8bc4\u4f30\u3002", "result": "\u4f7f\u7528\u4e24\u4e2a\u6865\u6881\u7684\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5bf9\u635f\u4f24\u4f4d\u7f6e\u3001\u4e25\u91cd\u7a0b\u5ea6\u548c\u8303\u56f4\u5177\u6709\u9ad8\u654f\u611f\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u5b9e\u65f6\u76d1\u6d4b\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u8de8\u7ed3\u6784\u77e5\u8bc6\u8f6c\u79fb\uff0c\u4fc3\u8fdb\u4e86\u667a\u80fd\u76d1\u6d4b\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u7f51\u7edc\u5c42\u9762\u7684\u97e7\u6027\u3002"}}
{"id": "2509.18853", "categories": ["eess.SP", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.18853", "abs": "https://arxiv.org/abs/2509.18853", "authors": ["Xiaolei Li", "Pengyu Wang", "Wenhua Song", "Yangjin Xu", "Wei Gao"], "title": "Normal mode parameters estimation by a VLA in single-shooting", "comment": null, "summary": "This paper proposes an orthogonality-constrained modal search (OCMS) method\nfor estimating modal wavenumbers and modal depth functions using a vertical\nlinear array (VLA). Under the assumption of a known sound speed profile, OCMS\nleverages the orthogonality of distinct modal depth functions to extract both\nthe modal depth functions and their corresponding wavenumbers, even when the\nVLA and a monochromatic sound source remain stationary.The performance of OCMS\nis evaluated through numerical simulations under varying signal-to-noise ratios\n(SNRs), different VLA apertures, varying numbers of VLA elements, VLA tilt and\nsound speed profile (SSP) uncertainty. The results demonstrate that OCMS is\nrobust against noise, VLA aperture variations, and changes in the number of VLA\nelements, meanwhile, the algorithm maintains reliable performance when SSP\nuncertainty < 1 m/s and VLA tilt angle <5{\\deg}. Furthermore, the effectiveness\nof OCMS is validated using SwellEx96 experimental data. The relative error\nbetween the modal wavenumbers derived from experimental data and those computed\nvia Kraken is on the order of $10^{-4}$.", "AI": {"tldr": "\u63d0\u51fa\u6b63\u4ea4\u7ea6\u675f\u6a21\u6001\u641c\u7d22\u65b9\u6cd5\uff0c\u5229\u7528\u5782\u76f4\u7ebf\u6027\u9635\u5217\u4f30\u8ba1\u6a21\u6001\u6ce2\u6570\u548c\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\uff0c\u5728\u5df2\u77e5\u58f0\u901f\u5256\u9762\u4e0b\u901a\u8fc7\u6a21\u6001\u6b63\u4ea4\u6027\u63d0\u53d6\u6a21\u6001\u53c2\u6570", "motivation": "\u89e3\u51b3\u5728\u5782\u76f4\u7ebf\u6027\u9635\u5217\u548c\u5355\u9891\u58f0\u6e90\u9759\u6b62\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u4f30\u8ba1\u6a21\u6001\u6ce2\u6570\u548c\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\u7684\u95ee\u9898", "method": "\u57fa\u4e8e\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\u7684\u6b63\u4ea4\u6027\u8bbe\u8ba1OCMS\u7b97\u6cd5\uff0c\u5229\u7528\u5df2\u77e5\u58f0\u901f\u5256\u9762\u63d0\u53d6\u6a21\u6001\u53c2\u6570", "result": "\u6570\u503c\u6a21\u62df\u663e\u793aOCMS\u5bf9\u566a\u58f0\u3001\u9635\u5217\u5b54\u5f84\u53d8\u5316\u3001\u9635\u5143\u6570\u91cf\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u58f0\u901f\u5256\u9762\u8bef\u5dee<1m/s\u548c\u9635\u5217\u503e\u659c\u89d2<5\u00b0\u65f6\u6027\u80fd\u53ef\u9760\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u6a21\u6001\u6ce2\u6570\u76f8\u5bf9\u8bef\u5dee\u7ea6\u4e3a10^-4", "conclusion": "OCMS\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u53d6\u6a21\u6001\u53c2\u6570\uff0c\u5728\u591a\u79cd\u5b9e\u9645\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u4e3a\u6c34\u58f0\u6a21\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177"}}
{"id": "2509.18739", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18739", "abs": "https://arxiv.org/abs/2509.18739", "authors": ["Christos Revelas", "Otilia Boldea", "Bas J. M. Werker"], "title": "Consistency of Selection Strategies for Fraud Detection", "comment": null, "summary": "This paper studies how insurers can chose which claims to investigate for\nfraud. Given a prediction model, typically only claims with the highest\npredicted propability of being fraudulent are investigated. We argue that this\ncan lead to inconsistent learning and propose a randomized alternative. More\ngenerally, we draw a parallel with the multi-arm bandit literature and argue\nthat, in the presence of selection, the obtained observations are not iid.\nHence, dependence on past observations should be accounted for when updating\nparameter estimates. We formalize selection in a binary regression framework\nand show that model updating and maximum-likelihood estimation can be\nimplemented as if claims were investigated at random. Then, we define\nconsistency of selection strategies and conjecture sufficient conditions for\nconsistency. Our simulations suggest that the often-used selection strategy can\nbe inconsistent while the proposed randomized alternative is consistent.\nFinally, we compare our randomized selection strategy with Thompson sampling, a\nstandard multi-arm bandit heuristic. Our simulations suggest that the latter\ncan be inefficient in learning low fraud probabilities.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4fdd\u9669\u516c\u53f8\u5982\u4f55\u9009\u62e9\u8c03\u67e5\u6b3a\u8bc8\u7d22\u8d54\u7684\u7b56\u7565\uff0c\u6307\u51fa\u4f20\u7edf\u57fa\u4e8e\u6700\u9ad8\u9884\u6d4b\u6982\u7387\u7684\u8c03\u67e5\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u4e0d\u4e00\u81f4\uff0c\u5e76\u63d0\u51fa\u968f\u673a\u5316\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u53ea\u8c03\u67e5\u9884\u6d4b\u6b3a\u8bc8\u6982\u7387\u6700\u9ad8\u7684\u7d22\u8d54\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u9009\u62e9\u504f\u5dee\u548c\u5b66\u4e60\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8c03\u67e5\u7b56\u7565\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5728\u4e8c\u5143\u56de\u5f52\u6846\u67b6\u4e2d\u5f62\u5f0f\u5316\u9009\u62e9\u8fc7\u7a0b\uff0c\u63d0\u51fa\u968f\u673a\u5316\u8c03\u67e5\u7b56\u7565\uff0c\u5e76\u4e0eThompson\u91c7\u6837\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\u4f20\u7edf\u7b56\u7565\u53ef\u80fd\u4e0d\u4e00\u81f4\uff0c\u800c\u63d0\u51fa\u7684\u968f\u673a\u5316\u7b56\u7565\u5177\u6709\u4e00\u81f4\u6027\uff1bThompson\u91c7\u6837\u5728\u5b66\u4e60\u4f4e\u6b3a\u8bc8\u6982\u7387\u65f6\u6548\u7387\u8f83\u4f4e\u3002", "conclusion": "\u968f\u673a\u5316\u8c03\u67e5\u7b56\u7565\u6bd4\u4f20\u7edf\u65b9\u6cd5\u548cThompson\u91c7\u6837\u66f4\u6709\u6548\uff0c\u80fd\u591f\u89e3\u51b3\u9009\u62e9\u504f\u5dee\u95ee\u9898\u5e76\u5b9e\u73b0\u4e00\u81f4\u7684\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2509.18107", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18107", "abs": "https://arxiv.org/abs/2509.18107", "authors": ["Huanyao Zhang", "Jiaye Lin", "Wentao Zhang", "Haitao Yuan", "Guoliang Li"], "title": "AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting", "comment": null, "summary": "Multivariate time series forecasting involves predicting future values based\non historical observations. However, existing approaches primarily rely on\npredefined single-scale patches or lack effective mechanisms for multi-scale\nfeature fusion. These limitations hinder them from fully capturing the complex\npatterns inherent in time series, leading to constrained performance and\ninsufficient generalizability. To address these challenges, we propose a novel\narchitecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers\n(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both\nGeneral Pre-trained Models (GPM) and Domain-specific Models (DSM) for\nmulti-scale feature extraction. To accommodate the heterogeneity of temporal\nfeatures, AdaMixT incorporates a gating network that dynamically allocates\nweights among different experts, enabling more accurate predictions through\nadaptive multi-scale fusion. Comprehensive experiments on eight widely used\nbenchmarks, including Weather, Traffic, Electricity, ILI, and four ETT\ndatasets, consistently demonstrate the effectiveness of AdaMixT in real-world\nscenarios.", "AI": {"tldr": "\u63d0\u51faAdaMixT\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u4e13\u5bb6\u53d8\u6362\u5668\u89e3\u51b3\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u4e0d\u8db3\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u5355\u5c3a\u5ea6\u8865\u4e01\u6216\u7f3a\u4e4f\u6709\u6548\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u590d\u6742\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3", "method": "AdaMixT\u5f15\u5165\u591a\u79cd\u8865\u4e01\uff0c\u5229\u7528\u901a\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u9886\u57df\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u901a\u8fc7\u95e8\u63a7\u7f51\u7edc\u52a8\u6001\u5206\u914d\u4e0d\u540c\u4e13\u5bb6\u7684\u6743\u91cd\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u878d\u5408", "result": "\u57288\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cAdaMixT\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u6301\u7eed\u6709\u6548\u6027", "conclusion": "AdaMixT\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u4e13\u5bb6\u878d\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2509.18918", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18918", "abs": "https://arxiv.org/abs/2509.18918", "authors": ["Hamideh-Sadat Fazael-Ardekani", "Hadi Zayyani", "Hamid Soltanian-Zadeh"], "title": "Quaternion LMS for Graph Signal Recovery", "comment": null, "summary": "This letter generalizes the Graph Signal Recovery (GSR) problem in Graph\nSignal Processing (GSP) to the Quaternion domain. It extends the Quaternion\nLeast Mean Square (QLMS) in adaptive filtering literature, and Graph LMS (GLMS)\nalgorithm in GSP literature, to an algorithm called Quaternion GLMS (QGLMS).\nThe basic adaptation formula using Quaternion-based algebra is derived.\nMoreover, mean convergence analysis and mean-square convergence analysis are\nmathematically performed. Hence, a sufficient condition on the step-size\nparameter of QGLMS is suggested. Also, simulation results demonstrate the\neffectiveness of the proposed algorithm in graph signal reconstruction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u56fe\u4fe1\u53f7\u6062\u590d\u95ee\u9898\u63a8\u5e7f\u5230\u56db\u5143\u6570\u57df\uff0c\u63d0\u51fa\u4e86\u56db\u5143\u6570\u56feLMS\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u6536\u655b\u6027\u5206\u6790\u548c\u4eff\u771f\u9a8c\u8bc1\u3002", "motivation": "\u5c06\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u56fe\u4fe1\u53f7\u6062\u590d\u95ee\u9898\u6269\u5c55\u5230\u56db\u5143\u6570\u57df\uff0c\u7ed3\u5408\u56db\u5143\u6570\u6700\u5c0f\u5747\u65b9\u7b97\u6cd5\u548c\u56feLMS\u7b97\u6cd5\u7684\u4f18\u52bf\uff0c\u5904\u7406\u66f4\u590d\u6742\u7684\u591a\u7ef4\u4fe1\u53f7\u3002", "method": "\u63a8\u5bfc\u4e86\u57fa\u4e8e\u56db\u5143\u6570\u4ee3\u6570\u7684\u81ea\u9002\u5e94\u516c\u5f0f\uff0c\u8fdb\u884c\u4e86\u5747\u503c\u6536\u655b\u5206\u6790\u548c\u5747\u65b9\u6536\u655b\u5206\u6790\uff0c\u63d0\u51fa\u4e86QGLMS\u7b97\u6cd5\u7684\u6b65\u957f\u53c2\u6570\u5145\u5206\u6761\u4ef6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684QGLMS\u7b97\u6cd5\u5728\u56fe\u4fe1\u53f7\u91cd\u6784\u4e2d\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "\u6210\u529f\u5c06\u56fe\u4fe1\u53f7\u6062\u590d\u95ee\u9898\u6269\u5c55\u5230\u56db\u5143\u6570\u57df\uff0c\u63d0\u51fa\u7684QGLMS\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19226", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19226", "abs": "https://arxiv.org/abs/2509.19226", "authors": ["Muhammad Rana", "Keaton Hamm"], "title": "Neighbor Embeddings Using Unbalanced Optimal Transport Metrics", "comment": null, "summary": "This paper proposes the use of the Hellinger--Kantorovich metric from\nunbalanced optimal transport (UOT) in a dimensionality reduction and learning\n(supervised and unsupervised) pipeline. The performance of UOT is compared to\nthat of regular OT and Euclidean-based dimensionality reduction methods on\nseveral benchmark datasets including MedMNIST. The experimental results\ndemonstrate that, on average, UOT shows improvement over both Euclidean and\nOT-based methods as verified by statistical hypothesis tests. In particular, on\nthe MedMNIST datasets, UOT outperforms OT in classification 81\\% of the time.\nFor clustering MedMNIST, UOT outperforms OT 83\\% of the time and outperforms\nboth other metrics 58\\% of the time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u964d\u7ef4\u548c\u5b66\u4e60\u6d41\u7a0b\u4e2d\u4f7f\u7528\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\uff08UOT\uff09\u4e2d\u7684Hellinger-Kantorovich\u5ea6\u91cf\uff0c\u76f8\u6bd4\u4f20\u7edfOT\u548c\u6b27\u51e0\u91cc\u5f97\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18", "motivation": "\u63a2\u7d22\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u5728\u964d\u7ef4\u548c\u76d1\u7763/\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u4e0e\u4f20\u7edf\u6700\u4f18\u4f20\u8f93\u548c\u6b27\u51e0\u91cc\u5f97\u65b9\u6cd5\u7684\u5bf9\u6bd4", "method": "\u4f7f\u7528Hellinger-Kantorovich\u5ea6\u91cf\u6784\u5efa\u964d\u7ef4\u548c\u5b66\u4e60\u6d41\u7a0b\uff0c\u5728MedMNIST\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u4e0e\u5e38\u89c4OT\u548c\u6b27\u51e0\u91cc\u5f97\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4", "result": "UOT\u5728\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u4e2d\u5e73\u5747\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u548cOT\u65b9\u6cd5\uff1a\u5728MedMNIST\u5206\u7c7b\u4efb\u52a1\u4e2d81%\u60c5\u51b5\u4e0b\u4f18\u4e8eOT\uff0c\u805a\u7c7b\u4efb\u52a1\u4e2d83%\u4f18\u4e8eOT\uff0c58%\u4f18\u4e8e\u5176\u4ed6\u6240\u6709\u65b9\u6cd5", "conclusion": "\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u5728\u964d\u7ef4\u548c\u5b66\u4e60\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa"}}
{"id": "2509.18108", "categories": ["cs.LG", "cs.AI", "I.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18108", "abs": "https://arxiv.org/abs/2509.18108", "authors": ["Adam Viktorin", "Tomas Kadavy", "Jozef Kovac", "Michal Pluhacek", "Roman Senkerik"], "title": "Solve it with EASE", "comment": "EASE framework landing paper", "summary": "This paper presents EASE (Effortless Algorithmic Solution Evolution), an\nopen-source and fully modular framework for iterative algorithmic solution\ngeneration leveraging large language models (LLMs). EASE integrates generation,\ntesting, analysis, and evaluation into a reproducible feedback loop, giving\nusers full control over error handling, analysis, and quality assessment. Its\narchitecture supports the orchestration of multiple LLMs in complementary\nroles-such as generator, analyst, and evaluator. By abstracting the complexity\nof prompt design and model management, EASE provides a transparent and\nextensible platform for researchers and practitioners to co-design algorithms\nand other generative solutions across diverse domains.", "AI": {"tldr": "EASE\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fed\u4ee3\u5f0f\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u751f\u6210\uff0c\u96c6\u6210\u4e86\u751f\u6210\u3001\u6d4b\u8bd5\u3001\u5206\u6790\u548c\u8bc4\u4f30\u529f\u80fd\u3002", "motivation": "\u4e3a\u4e86\u7b80\u5316\u7b97\u6cd5\u8bbe\u8ba1\u8fc7\u7a0b\uff0c\u901a\u8fc7\u62bd\u8c61\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u7ba1\u7406\u7684\u590d\u6742\u6027\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e00\u4e2a\u900f\u660e\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u591aLLM\u534f\u540c\u67b6\u6784\uff0c\u5c06\u4e0d\u540cLLM\u5206\u914d\u4e3a\u751f\u6210\u5668\u3001\u5206\u6790\u5e08\u548c\u8bc4\u4f30\u5668\u7b49\u4e92\u8865\u89d2\u8272\uff0c\u6784\u5efa\u53ef\u91cd\u590d\u7684\u53cd\u9988\u5faa\u73af\u3002", "result": "EASE\u6846\u67b6\u80fd\u591f\u652f\u6301\u8de8\u9886\u57df\u7684\u7b97\u6cd5\u548c\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u7528\u6237\u5bf9\u9519\u8bef\u5904\u7406\u3001\u5206\u6790\u548c\u8d28\u91cf\u8bc4\u4f30\u5177\u6709\u5b8c\u5168\u63a7\u5236\u6743\u3002", "conclusion": "EASE\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u591aLLM\u534f\u540c\u5de5\u4f5c\uff0c\u4e3a\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u7684\u8fed\u4ee3\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u63a7\u4e14\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u3002"}}
{"id": "2509.19056", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19056", "abs": "https://arxiv.org/abs/2509.19056", "authors": ["Razieh Torkamani", "Arash Amini", "Hadi Zayyani", "Mehdi Korki"], "title": "Bayesian Convolutional Neural Networks for Prior Learning in Graph Signal Recovery", "comment": null, "summary": "Graph signal recovery (GSR) is a fundamental problem in graph signal\nprocessing, where the goal is to reconstruct a complete signal defined over a\ngraph from a subset of noisy or missing observations. A central challenge in\nGSR is that the underlying statistical model of the graph signal is often\nunknown or too complex to specify analytically. To address this, we propose a\nflexible, data-driven framework that learns the signal prior directly from\ntraining samples. We develop a Bayesian convolutional neural network (BCNN)\narchitecture that models the prior distribution of graph signals using\ngraph-aware filters based on Chebyshev polynomials. By interpreting the hidden\nlayers of the CNN as Gibbs distributions and employing Gaussian mixture model\n(GMM) nonlinearities, we obtain a closed-form and expressive prior. This prior\nis integrated into a variational Bayesian (VB) inference framework to estimate\nthe posterior distribution of the signal and noise precision. Extensive\nexperiments on synthetic and real-world graph datasets demonstrate that the\nproposed BCNN-GSR algorithm achieves accurate and robust recovery across a\nvariety of signal distributions. The method generalizes well to complex,\nnon-Gaussian signal models and remains computationally efficient, making it\nsuitable for practical large-scale graph recovery tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u636e\u9a71\u52a8\u56fe\u4fe1\u53f7\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u56fe\u4fe1\u53f7\u7684\u5148\u9a8c\u5206\u5e03\u6765\u5b9e\u73b0\u5bf9\u566a\u58f0\u6216\u7f3a\u5931\u89c2\u6d4b\u7684\u9c81\u68d2\u6062\u590d\u3002", "motivation": "\u56fe\u4fe1\u53f7\u6062\u590d\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u662f\u5e95\u5c42\u7edf\u8ba1\u6a21\u578b\u901a\u5e38\u672a\u77e5\u6216\u8fc7\u4e8e\u590d\u6742\u96be\u4ee5\u89e3\u6790\u6307\u5b9a\uff0c\u9700\u8981\u4e00\u79cd\u7075\u6d3b\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6765\u5b66\u4e60\u4fe1\u53f7\u5148\u9a8c\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u7684\u56fe\u611f\u77e5\u6ee4\u6ce2\u5668\u7684\u8d1d\u53f6\u65afCNN\u67b6\u6784\uff0c\u5c06\u9690\u85cf\u5c42\u89e3\u91ca\u4e3a\u5409\u5e03\u65af\u5206\u5e03\u5e76\u4f7f\u7528GMM\u975e\u7ebf\u6027\u6fc0\u6d3b\uff0c\u6784\u5efa\u95ed\u5f0f\u8868\u8fbe\u7684\u5148\u9a8c\uff0c\u5e76\u96c6\u6210\u5230\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u7406\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBCNN-GSR\u7b97\u6cd5\u5728\u5404\u79cd\u4fe1\u53f7\u5206\u5e03\u4e0b\u90fd\u80fd\u5b9e\u73b0\u51c6\u786e\u9c81\u68d2\u7684\u6062\u590d\uff0c\u5bf9\u590d\u6742\u975e\u9ad8\u65af\u4fe1\u53f7\u6a21\u578b\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u56fe\u6062\u590d\u4efb\u52a1\uff0c\u4e3a\u56fe\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19250", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19250", "abs": "https://arxiv.org/abs/2509.19250", "authors": ["Muhammad Rana", "Abiy Tasissa", "HanQin Cai", "Yakov Gavriyelov", "Keaton Hamm"], "title": "Recovering Wasserstein Distance Matrices from Few Measurements", "comment": null, "summary": "This paper proposes two algorithms for estimating square Wasserstein distance\nmatrices from a small number of entries. These matrices are used to compute\nmanifold learning embeddings like multidimensional scaling (MDS) or Isomap, but\ncontrary to Euclidean distance matrices, are extremely costly to compute. We\nanalyze matrix completion from upper triangular samples and Nystr\\\"{o}m\ncompletion in which $\\mathcal{O}(d\\log(d))$ columns of the distance matrices\nare computed where $d$ is the desired embedding dimension, prove stability of\nMDS under Nystr\\\"{o}m completion, and show that it can outperform matrix\ncompletion for a fixed budget of sample distances. Finally, we show that\nclassification of the OrganCMNIST dataset from the MedMNIST benchmark is stable\non data embedded from the Nystr\\\"{o}m estimation of the distance matrix even\nwhen only 10\\% of the columns are computed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4ece\u5c11\u91cf\u6761\u76ee\u4f30\u8ba1\u5e73\u65b9Wasserstein\u8ddd\u79bb\u77e9\u9635\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u6d41\u5f62\u5b66\u4e60\u5d4c\u5165\uff0c\u76f8\u6bd4\u6b27\u6c0f\u8ddd\u79bb\u77e9\u9635\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002\u5206\u6790\u4e86\u4e0a\u4e09\u89d2\u91c7\u6837\u77e9\u9635\u8865\u5168\u548cNystr\u00f6m\u8865\u5168\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86MDS\u5728Nystr\u00f6m\u8865\u5168\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u5728MedMNIST\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "Wasserstein\u8ddd\u79bb\u77e9\u9635\u5728\u6d41\u5f62\u5b66\u4e60\u5d4c\u5165\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7b97\u6cd5\u4ece\u5c11\u91cf\u6837\u672c\u4e2d\u51c6\u786e\u4f30\u8ba1\u8fd9\u4e9b\u77e9\u9635\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u57fa\u4e8e\u4e0a\u4e09\u89d2\u91c7\u6837\u7684\u77e9\u9635\u8865\u5168\u548cNystr\u00f6m\u8865\u5168\uff08\u4ec5\u9700\u8ba1\u7b97O(d log d)\u5217\uff09\u3002\u5206\u6790\u4e86MDS\u5728Nystr\u00f6m\u8865\u5168\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "result": "Nystr\u00f6m\u8865\u5168\u5728\u56fa\u5b9a\u6837\u672c\u8ddd\u79bb\u9884\u7b97\u4e0b\u4f18\u4e8e\u77e9\u9635\u8865\u5168\u3002\u5728OrganCMNIST\u6570\u636e\u96c6\u4e0a\uff0c\u5373\u4f7f\u53ea\u8ba1\u7b9710%\u7684\u5217\uff0c\u57fa\u4e8eNystr\u00f6m\u4f30\u8ba1\u7684\u5d4c\u5165\u6570\u636e\u5206\u7c7b\u4e5f\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "Nystr\u00f6m\u8865\u5168\u662f\u4e00\u79cd\u9ad8\u6548\u7684Wasserstein\u8ddd\u79bb\u77e9\u9635\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u5d4c\u5165\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2509.18109", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18109", "abs": "https://arxiv.org/abs/2509.18109", "authors": ["Jonatan Katz Nielsen"], "title": "Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks", "comment": null, "summary": "Accurate recognition of vessel types from Automatic Identification System\n(AIS) tracks is essential for safety oversight and combating illegal,\nunreported, and unregulated (IUU) activity. This paper presents a strait-scale,\nmachine-learning pipeline that classifies moving vessels using only AIS data.\nWe analyze eight days of historical AIS from the Danish Maritime Authority\ncovering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After\nforward/backward filling voyage records, removing kinematic and geospatial\noutliers, and segmenting per-MMSI tracks while excluding stationary periods\n($\\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,\nSOG statistics), temporal, geospatial (Haversine distances, spans), and\nship-shape attributes computed from AIS A/B/C/D reference points (length,\nwidth, aspect ratio, bridge-position ratio). To avoid leakage, we perform\ngrouped train/test splits by MMSI and use stratified 5-fold cross-validation.\nAcross five classes (cargo, tanker, passenger, high-speed craft, fishing;\nN=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest\nwith SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall\n92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches\none-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the\nbridge-position ratio and maximum SOG as the most discriminative signals;\nprincipal errors occur between cargo and tanker, reflecting similar transit\nbehavior. We demonstrate operational value by backfilling missing ship types on\nunseen data and discuss improvements such as DBSCAN based trip segmentation and\ngradient-boosted ensembles to handle frequent-stop ferries and further lift\nperformance. The results show that lightweight features over AIS trajectories\nenable real-time vessel type classification in straits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAIS\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u7ba1\u9053\uff0c\u7528\u4e8e\u5728\u72ed\u7a84\u6d77\u57df\u5bf9\u79fb\u52a8\u8239\u53ea\u8fdb\u884c\u7c7b\u578b\u5206\u7c7b\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u7279\u5f81\u548c\u6811\u6a21\u578b\u5b9e\u73b0\u4e8692.15%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u51c6\u786e\u8bc6\u522bAIS\u8f68\u8ff9\u4e2d\u7684\u8239\u53ea\u7c7b\u578b\u5bf9\u4e8e\u5b89\u5168\u76d1\u7ba1\u548c\u6253\u51fb\u975e\u6cd5\u3001\u672a\u62a5\u544a\u548c\u65e0\u7ba1\u5236(IUU)\u6d3b\u52a8\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5386\u53f2AIS\u6570\u636e\uff0c\u7ecf\u8fc7\u6570\u636e\u9884\u5904\u7406\u540e\u63d0\u53d631\u4e2a\u8f68\u8ff9\u7ea7\u7279\u5f81\uff0c\u91c7\u7528\u57fa\u4e8e\u6811\u7684\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\uff09\u8fdb\u884c\u4e94\u7c7b\u8239\u53ea\u5206\u7c7b\uff0c\u5e76\u4f7f\u7528\u5206\u7ec4\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\u907f\u514d\u6570\u636e\u6cc4\u9732\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523092.15%\u51c6\u786e\u7387\uff0c\u5b8f\u7cbe\u5ea694.11%\uff0c\u5b8f\u53ec\u56de\u738792.51%\uff0c\u5b8fF1\u5206\u657093.27%\uff0cROC-AUC\u6700\u9ad8\u8fbe0.9897\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eAIS\u8f68\u8ff9\u7684\u8f7b\u91cf\u7ea7\u7279\u5f81\u80fd\u591f\u5728\u72ed\u7a84\u6d77\u57df\u5b9e\u73b0\u5b9e\u65f6\u8239\u53ea\u7c7b\u578b\u5206\u7c7b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.19092", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19092", "abs": "https://arxiv.org/abs/2509.19092", "authors": ["Abolfazl Zakeri", "Nhan Thanh Nguyen", "Ahmed Alkhateeb", "Markku Juntti"], "title": "Data-Free Knowledge Distillation for LiDAR-Aided Beam Tracking in MmWave Systems", "comment": "Submitted for possible publication", "summary": "Multimodal sensing reduces beam training overhead but is constrained by\nmachine learning complexity and dataset demands. To address this, we propose a\ndata-free (DF) knowledge distillation (KD) framework for efficient LiDAR-aided\nmmWave beam tracking, i.e., predicting the best current and future beams.\nSpecifically, we propose a knowledge inversion framework, where a generator\nsynthesizes LiDAR input data from random noise, guided by a loss function\ndefined on the features and outputs of a pre-trained teacher model. The student\nmodel is then trained using the synthetic data and knowledge distilled from the\nteacher. The generator loss combines three terms, called metadata loss,\nactivation loss, and entropy loss. For student training, in addition to the\nstandard Kullback-Leibler divergence loss, we also consider a mean-squared\nerror (MSE) loss between the teacher and student logits. Simulation results\nshow that the proposed DF-KD (slightly) outperforms the teacher in Top-1 and\nTop-5 accuracies. Moreover, we observe that the metadata loss contributes\nsignificantly to the generator performance, and that the MSE loss for the\nstudent can effectively replace the standard KD loss while requiring fewer\nfine-tuned hyperparameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u6570\u636e\u7684\u6570\u636e\u81ea\u7531\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8eLiDAR\u8f85\u52a9\u6beb\u7c73\u6ce2\u6ce2\u675f\u8ddf\u8e2a\uff0c\u901a\u8fc7\u751f\u6210\u5668\u5408\u6210\u6570\u636e\u5e76\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u6027\u80fd\u751a\u81f3\u7565\u4f18\u4e8e\u6559\u5e08\u6a21\u578b\u3002", "motivation": "\u591a\u6a21\u6001\u4f20\u611f\u867d\u7136\u51cf\u5c11\u4e86\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\uff0c\u4f46\u53d7\u5230\u673a\u5668\u5b66\u4e60\u590d\u6742\u6027\u548c\u6570\u636e\u96c6\u9700\u6c42\u7684\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u53cd\u6f14\u6846\u67b6\uff0c\u751f\u6210\u5668\u4ece\u968f\u673a\u566a\u58f0\u5408\u6210LiDAR\u8f93\u5165\u6570\u636e\uff0c\u901a\u8fc7\u5143\u6570\u636e\u635f\u5931\u3001\u6fc0\u6d3b\u635f\u5931\u548c\u71b5\u635f\u5931\u6307\u5bfc\uff1b\u5b66\u751f\u6a21\u578b\u4f7f\u7528\u5408\u6210\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u77e5\u8bc6\u8fdb\u884c\u8bad\u7ec3\uff0c\u7ed3\u5408KL\u6563\u5ea6\u635f\u5931\u548cMSE\u635f\u5931\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684DF-KD\u6846\u67b6\u5728Top-1\u548cTop-5\u51c6\u786e\u7387\u4e0a\u7565\u4f18\u4e8e\u6559\u5e08\u6a21\u578b\uff0c\u5143\u6570\u636e\u635f\u5931\u5bf9\u751f\u6210\u5668\u6027\u80fd\u8d21\u732e\u663e\u8457\uff0cMSE\u635f\u5931\u53ef\u6709\u6548\u66ff\u4ee3\u6807\u51c6KD\u635f\u5931\u4e14\u9700\u8981\u66f4\u5c11\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "conclusion": "\u8be5\u6570\u636e\u81ea\u7531\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u4e3aLiDAR\u8f85\u52a9\u6beb\u7c73\u6ce2\u6ce2\u675f\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u51cf\u5c11\u6570\u636e\u4f9d\u8d56\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19276", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2509.19276", "abs": "https://arxiv.org/abs/2509.19276", "authors": ["Tim Y. J. Wang", "O. Deniz Akyildiz"], "title": "A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion Models", "comment": "Accepted at the 2nd Workshop on Frontiers in Probabilistic Inference:\n  Sampling Meets Learning, 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "Solving ill-posed inverse problems requires powerful and flexible priors. We\npropose leveraging pretrained latent diffusion models for this task through a\nnew training-free approach, termed Diffusion-regularized Wasserstein Gradient\nFlow (DWGF). Specifically, we formulate the posterior sampling problem as a\nregularized Wasserstein gradient flow of the Kullback-Leibler divergence in the\nlatent space. We demonstrate the performance of our method on standard\nbenchmarks using StableDiffusion (Rombach et al., 2022) as the prior.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiffusion-regularized Wasserstein Gradient Flow (DWGF)\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u89e3\u51b3\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\u9700\u8981\u5f3a\u5927\u4e14\u7075\u6d3b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e3a\u6b64\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u57fa\u7840\u3002", "method": "\u5c06\u540e\u9a8c\u91c7\u6837\u95ee\u9898\u8868\u8ff0\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2dKullback-Leibler\u6563\u5ea6\u7684\u6b63\u5219\u5316Wasserstein\u68af\u5ea6\u6d41\uff0c\u4f7f\u7528StableDiffusion\u4f5c\u4e3a\u5148\u9a8c\u6a21\u578b\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "DWGF\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u89e3\u51b3\u9006\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2509.18110", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18110", "abs": "https://arxiv.org/abs/2509.18110", "authors": ["Mrigank Dhingra", "Romit Maulik", "Adil Rasheed", "Omer San"], "title": "Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs", "comment": null, "summary": "Neural operator learning has emerged as a powerful approach for solving\npartial differential equations (PDEs) in a data-driven manner. However,\napplying principal component analysis (PCA) to high-dimensional solution fields\nincurs significant computational overhead. To address this, we propose a\npatch-based PCA-Net framework that decomposes the solution fields into smaller\npatches, applies PCA within each patch, and trains a neural operator in the\nreduced PCA space. We investigate two different patch-based approaches that\nbalance computational efficiency and reconstruction accuracy: (1)\nlocal-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off\nbetween computational cost and accuracy is analyzed, highlighting the\nadvantages and limitations of each approach. Furthermore, within each approach,\nwe explore two refinements for the most computationally efficient method: (i)\nintroducing overlapping patches with a smoothing filter and (ii) employing a\ntwo-step process with a convolutional neural network (CNN) for refinement. Our\nresults demonstrate that patch-based PCA significantly reduces computational\ncomplexity while maintaining high accuracy, reducing end-to-end pipeline\nprocessing time by a factor of 3.7 to 4 times compared to global PCA, thefore\nmaking it a promising technique for efficient operator learning in PDE-based\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8epatch\u7684PCA-Net\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u89e3\u573a\u5206\u89e3\u4e3a\u5c0f\u5757\u5e76\u5728\u6bcf\u4e2apatch\u5185\u5e94\u7528PCA\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u76f8\u6bd4\u5168\u5c40PCA\u5c06\u7aef\u5230\u7aef\u5904\u7406\u65f6\u95f4\u51cf\u5c113.7-4\u500d\u3002", "motivation": "\u89e3\u51b3\u5728\u9ad8\u7ef4\u89e3\u573a\u4e0a\u5e94\u7528\u4e3b\u6210\u5206\u5206\u6790(PCA)\u65f6\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u5728\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e24\u79cdpatch-based PCA\u65b9\u6cd5\uff1a\u5c40\u90e8\u5230\u5168\u5c40patch PCA\u548c\u5c40\u90e8\u5230\u5c40\u90e8patch PCA\uff0c\u5e76\u63a2\u7d22\u91cd\u53e0patch\u5e73\u6ed1\u6ee4\u6ce2\u548cCNN\u7ec6\u5316\u4e24\u79cd\u6539\u8fdb\u7b56\u7565\u3002", "result": "\u57fa\u4e8epatch\u7684PCA\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u7aef\u5230\u7aef\u5904\u7406\u65f6\u95f4\u6bd4\u5168\u5c40PCA\u51cf\u5c113.7-4\u500d\u3002", "conclusion": "\u57fa\u4e8epatch\u7684PCA\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u9ad8\u6548\u7b97\u5b50\u5b66\u4e60\u6280\u672f\uff0c\u5728PDE\u7cfb\u7edf\u4e2d\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u6784\u7cbe\u5ea6\u3002"}}
{"id": "2509.19119", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19119", "abs": "https://arxiv.org/abs/2509.19119", "authors": ["Palatip Jopanya", "Diana P. M. Osorio"], "title": "Enabling Drone Detection with SWARM Repeater-Assisted MIMO ISAC", "comment": "5 pages, 2 figures", "summary": "As definitions about new architectural aspects, use cases, and standards for\nintegrated sensing and communication (ISAC) continue to appear, cellular\nsystems based on massive multiple-input multiple-output (MIMO) antenna\ntechnology are also experiencing a parallel evolution through the integration\nof novel network components. This evolution should support emerging ISAC use\ncases and services. In particular, this paper explores a recent vision for\ncost-efficient cellular network densification through the deployment of swarms\nof repeaters. Leveraging their ability to retransmit signals instantaneously,\nwe investigate how these repeaters can enhance radar sensing capabilities for\ndrone detection in a swarm repeater-assisted MIMO ISAC system. Our results\ndemonstrate that, by optimizing the gains of repeaters given a sufficient\nmaximum amplification gain, increasing the number of repeaters can lead to\ngains in sensing performance.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u7fa4\u4e2d\u7ee7\u5668\u8f85\u52a9\u7684MIMO ISAC\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u4f18\u5316\u4e2d\u7ee7\u5668\u589e\u76ca\u6765\u589e\u5f3a\u65e0\u4eba\u673a\u68c0\u6d4b\u7684\u96f7\u8fbe\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7684\u65b0\u67b6\u6784\u65b9\u9762\u3001\u7528\u4f8b\u548c\u6807\u51c6\u7684\u5b9a\u4e49\u4e0d\u65ad\u51fa\u73b0\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21MIMO\u5929\u7ebf\u6280\u672f\u7684\u8702\u7a9d\u7cfb\u7edf\u4e5f\u5728\u901a\u8fc7\u96c6\u6210\u65b0\u578b\u7f51\u7edc\u7ec4\u4ef6\u7ecf\u5386\u5e76\u884c\u6f14\u8fdb\uff0c\u4ee5\u652f\u6301\u65b0\u5174\u7684ISAC\u7528\u4f8b\u548c\u670d\u52a1\u3002", "method": "\u5229\u7528\u4e2d\u7ee7\u5668\u5373\u65f6\u91cd\u4f20\u4fe1\u53f7\u7684\u80fd\u529b\uff0c\u7814\u7a76\u8fd9\u4e9b\u4e2d\u7ee7\u5668\u5982\u4f55\u589e\u5f3a\u65e0\u4eba\u673a\u68c0\u6d4b\u7684\u96f7\u8fbe\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u4e2d\u7ee7\u5668\u589e\u76ca\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7ed9\u5b9a\u8db3\u591f\u6700\u5927\u653e\u5927\u589e\u76ca\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4f18\u5316\u4e2d\u7ee7\u5668\u589e\u76ca\uff0c\u589e\u52a0\u4e2d\u7ee7\u5668\u6570\u91cf\u53ef\u4ee5\u5e26\u6765\u611f\u77e5\u6027\u80fd\u7684\u63d0\u5347\u3002", "conclusion": "\u7fa4\u4e2d\u7ee7\u5668\u90e8\u7f72\u53ef\u4ee5\u6210\u672c\u9ad8\u6548\u5730\u589e\u5f3a\u8702\u7a9d\u7f51\u7edc\u7684\u96f7\u8fbe\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u65e0\u4eba\u673a\u68c0\u6d4b\u7b49ISAC\u5e94\u7528\u4e2d\u3002"}}
{"id": "2509.18141", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18141", "abs": "https://arxiv.org/abs/2509.18141", "authors": ["Yao Zhao", "Haoyue Sun", "Yantian Ding", "Yanxun Xu"], "title": "KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots", "comment": null, "summary": "Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots\nprovides valuable insights for evidence synthesis in clinical research.\nHowever, existing approaches often rely on manual digitization, which is\nerror-prone and lacks scalability. To address these limitations, we develop\nKM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD\ndirectly from KM plots with high accuracy, robustness, and reproducibility.\nKM-GPT integrates advanced image preprocessing, multi-modal reasoning powered\nby GPT-5, and iterative reconstruction algorithms to generate high-quality IPD\nwithout manual input or intervention. Its hybrid reasoning architecture\nautomates the conversion of unstructured information into structured data flows\nand validates data extraction from complex KM plots. To improve accessibility,\nKM-GPT is equipped with a user-friendly web interface and an integrated AI\nassistant, enabling researchers to reconstruct IPD without requiring\nprogramming expertise. KM-GPT was rigorously evaluated on synthetic and\nreal-world datasets, consistently demonstrating superior accuracy. To\nillustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer\nimmunotherapy trials, reconstructing IPD to facilitate evidence synthesis and\nbiomarker-based subgroup analyses. By automating traditionally manual processes\nand providing a scalable, web-based solution, KM-GPT transforms clinical\nresearch by leveraging reconstructed IPD to enable more informed downstream\nanalyses, supporting evidence-based decision-making.", "AI": {"tldr": "KM-GPT\u662f\u4e00\u4e2a\u5168\u81ea\u52a8AI\u9a71\u52a8\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u4eceKaplan-Meier\u56fe\u4e2d\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u4e2a\u4f53\u60a3\u8005\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u6570\u5b57\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u6570\u5b57\u5316\uff0c\u5bb9\u6613\u51fa\u9519\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u4e34\u5e8a\u7814\u7a76\u8bc1\u636e\u5408\u6210\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "KM-GPT\u6574\u5408\u4e86\u5148\u8fdb\u7684\u56fe\u50cf\u9884\u5904\u7406\u3001\u57fa\u4e8eGPT-5\u7684\u591a\u6a21\u6001\u63a8\u7406\u548c\u8fed\u4ee3\u91cd\u5efa\u7b97\u6cd5\uff0c\u91c7\u7528\u6df7\u5408\u63a8\u7406\u67b6\u6784\u81ea\u52a8\u5c06\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u6d41\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u4e25\u683c\u8bc4\u4f30\u663e\u793a\uff0cKM-GPT\u59cb\u7ec8\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u80c3\u764c\u514d\u75ab\u6cbb\u7597\u8bd5\u9a8c\u7684\u835f\u8403\u5206\u6790\u4e2d\u6210\u529f\u5e94\u7528\u3002", "conclusion": "KM-GPT\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u7edf\u624b\u52a8\u6d41\u7a0b\uff0c\u4e3a\u4e34\u5e8a\u7814\u7a76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u7f51\u7edc\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u57fa\u4e8e\u8bc1\u636e\u7684\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2509.18111", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18111", "abs": "https://arxiv.org/abs/2509.18111", "authors": ["Faizul Rakib Sayem", "Shahana Ibrahim"], "title": "Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection", "comment": null, "summary": "The reliability of artificial intelligence (AI) systems in open-world\nsettings depends heavily on their ability to flag out-of-distribution (OOD)\ninputs unseen during training. Recent advances in large-scale vision-language\nmodels (VLMs) have enabled promising few-shot OOD detection frameworks using\nonly a handful of in-distribution (ID) samples. However, existing prompt\nlearning-based OOD methods rely solely on softmax probabilities, overlooking\nthe rich discriminative potential of the feature embeddings learned by VLMs\ntrained on millions of samples. To address this limitation, we propose a novel\ncontext optimization (CoOp)-based framework that integrates subspace\nrepresentation learning with prompt tuning. Our approach improves ID-OOD\nseparability by projecting the ID features into a subspace spanned by prompt\nvectors, while projecting ID-irrelevant features into an orthogonal null space.\nTo train such OOD detection framework, we design an easy-to-handle end-to-end\nlearning criterion that ensures strong OOD detection performance as well as\nhigh ID classification accuracy. Experiments on real-world datasets showcase\nthe effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4f18\u5316\uff08CoOp\uff09\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u5b50\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u4e0e\u63d0\u793a\u8c03\u4f18\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684OOD\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u4f9d\u8d56softmax\u6982\u7387\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6570\u767e\u4e07\u6837\u672c\u4e0a\u5b66\u4e60\u5230\u7684\u4e30\u5bcc\u7279\u5f81\u5d4c\u5165\u7684\u5224\u522b\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5c06ID\u7279\u5f81\u6295\u5f71\u5230\u63d0\u793a\u5411\u91cf\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\u4e2d\uff0c\u540c\u65f6\u5c06ID\u65e0\u5173\u7279\u5f81\u6295\u5f71\u5230\u6b63\u4ea4\u96f6\u7a7a\u95f4\u4e2d\uff0c\u8bbe\u8ba1\u6613\u4e8e\u5904\u7406\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u51c6\u5219\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u5b50\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u548c\u63d0\u793a\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86ID-OOD\u7684\u53ef\u5206\u79bb\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8ID\u5206\u7c7b\u51c6\u786e\u7387\u3002"}}
{"id": "2509.19130", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19130", "abs": "https://arxiv.org/abs/2509.19130", "authors": ["Abolfazl Zakeri", "Nhan Thanh Nguyen", "Ahmed Alkhateeb", "Markku Juntti"], "title": "Deep Reinforcement Learning for Dynamic Sensing and Communications", "comment": "Under review for possible publication", "summary": "Environmental sensing can significantly enhance mmWave communications by\nassisting beam training, yet its benefits must be balanced against the\nassociated sensing costs. To this end, we propose a unified machine learning\nframework that dynamically determines when to sense and leverages sensory data\nfor beam prediction. Specifically, we formulate a joint sensing and beamforming\nproblem that maximizes the av- erage signal-to-noise ratio under an average\nsensing budget. Lyapunov optimization is employed to enforce the sensing\nconstraint, while a deep Q-Network determines the sensing slots. A pretrained\ndeep neural network then maps the sens- ing data to optimal beams in the\ncodebook. Simulations based on the real-world DeepSense dataset demonstrate\nthat the pro- posed approach substantially reduces sensing overhead while\nmaintaining satisfactory communications performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u8fdb\u884c\u73af\u5883\u611f\u77e5\u5e76\u5229\u7528\u611f\u77e5\u6570\u636e\u9884\u6d4b\u6ce2\u675f\uff0c\u5728\u6ee1\u8db3\u5e73\u5747\u611f\u77e5\u9884\u7b97\u7684\u524d\u63d0\u4e0b\u6700\u5927\u5316\u4fe1\u566a\u6bd4\u3002", "motivation": "\u73af\u5883\u611f\u77e5\u80fd\u589e\u5f3a\u6beb\u7c73\u6ce2\u901a\u4fe1\u7684\u6ce2\u675f\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u5e73\u8861\u611f\u77e5\u5e26\u6765\u7684\u6536\u76ca\u4e0e\u6210\u672c\u3002", "method": "\u4f7f\u7528Lyapunov\u4f18\u5316\u5f3a\u5236\u6267\u884c\u611f\u77e5\u7ea6\u675f\uff0c\u6df1\u5ea6Q\u7f51\u7edc\u51b3\u5b9a\u611f\u77e5\u65f6\u9699\uff0c\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5c06\u611f\u77e5\u6570\u636e\u6620\u5c04\u5230\u7801\u672c\u4e2d\u7684\u6700\u4f18\u6ce2\u675f\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u4e16\u754cDeepSense\u6570\u636e\u96c6\u7684\u4eff\u771f\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u611f\u77e5\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6ee1\u610f\u7684\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51cf\u5c11\u611f\u77e5\u5f00\u9500\u7684\u540c\u65f6\uff0c\u80fd\u591f\u7ef4\u6301\u826f\u597d\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u611f\u77e5\u6210\u672c\u4e0e\u901a\u4fe1\u589e\u76ca\u7684\u6709\u6548\u5e73\u8861\u3002"}}
{"id": "2509.18452", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML", "D.2.0; G.4; B.8.2"], "pdf": "https://arxiv.org/pdf/2509.18452", "abs": "https://arxiv.org/abs/2509.18452", "authors": ["Anton Lebedev", "Won Kyung Lee", "Soumyadip Ghosh", "Olha I. Yaman", "Vassilis Kalantzis", "Yingdong Lu", "Tomasz Nowicki", "Shashanka Ubaru", "Lior Horesh", "Vassil Alexandrov"], "title": "Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion", "comment": "8 pages, 3 figures, 1 algorithm, 1 table of experiment cases", "summary": "Large, sparse linear systems are pervasive in modern science and engineering,\nand Krylov subspace solvers are an established means of solving them. Yet\nconvergence can be slow for ill-conditioned matrices, so practical deployments\nusually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix\ninversion can generate such preconditioners and accelerate Krylov iterations,\nbut its effectiveness depends on parameters whose optima vary across matrices;\nmanual or grid search is costly. We present an AI-driven framework recommending\nMCMC parameters for a given linear system. A graph neural surrogate predicts\npreconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition\nfunction then chooses the parameter sets most likely to minimise iterations. On\na previously unseen ill-conditioned system, the framework achieves better\npreconditioning with 50\\% of the search budget of conventional methods,\nyielding about a 10\\% reduction in iterations to convergence. These results\nsuggest a route for incorporating MCMC-based preconditioners into large-scale\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4bMCMC\u9884\u6761\u4ef6\u5668\u7684\u6548\u679c\uff0c\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u91c7\u96c6\u51fd\u6570\u4f18\u5316\u53c2\u6570\u9009\u62e9\uff0c\u4ece\u800c\u52a0\u901fKrylov\u5b50\u7a7a\u95f4\u6c42\u89e3\u5668\u7684\u6536\u655b\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u4e2dKrylov\u6c42\u89e3\u5668\u5728\u75c5\u6001\u77e9\u9635\u4e0a\u6536\u655b\u7f13\u6162\u7684\u95ee\u9898\uff0c\u4f20\u7edfMCMC\u9884\u6761\u4ef6\u5668\u53c2\u6570\u4f18\u5316\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u81ea\u52a8\u5316\u53c2\u6570\u63a8\u8350\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u4ee3\u7406\u6a21\u578b\u9884\u6d4b\u9884\u6761\u4ef6\u901f\u5ea6\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u91c7\u96c6\u51fd\u6570\u9009\u62e9\u6700\u4f18MCMC\u53c2\u6570\u96c6\uff0c\u4ee5\u6700\u5c0f\u5316\u8fed\u4ee3\u6b21\u6570\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u75c5\u6001\u7cfb\u7edf\u4e0a\uff0c\u8be5\u6846\u67b6\u4ec5\u7528\u4f20\u7edf\u65b9\u6cd550%\u7684\u641c\u7d22\u9884\u7b97\u5c31\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9884\u6761\u4ef6\u6548\u679c\uff0c\u6536\u655b\u8fed\u4ee3\u6b21\u6570\u51cf\u5c11\u7ea610%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5c06MCMC\u9884\u6761\u4ef6\u5668\u96c6\u6210\u5230\u5927\u89c4\u6a21\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\uff0c\u5c55\u793a\u4e86AI\u9a71\u52a8\u53c2\u6570\u4f18\u5316\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.18112", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18112", "abs": "https://arxiv.org/abs/2509.18112", "authors": ["Sheng Wong", "Ravi Shankar", "Beth Albert", "Gabriel Davis Jones"], "title": "Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis", "comment": "Preparing for journal", "summary": "Foundation models (FMs) and large language models (LLMs) demonstrate\nremarkable capabilities across diverse domains through training on massive\ndatasets. These models have demonstrated exceptional performance in healthcare\napplications, yet their potential for electronic fetal monitoring\n(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating\nfetal well-being, remains largely underexplored. Antepartum CTG interpretation\npresents unique challenges due to the complex nature of fetal heart rate (FHR)\npatterns and uterine activity, requiring sophisticated analysis of long\ntime-series data. The assessment of CTG is heavily based on subjective clinical\ninterpretation, often leading to variability in diagnostic accuracy and\ndeviation from timely pregnancy care. This study presents the first\ncomprehensive comparison of state-of-the-art AI approaches for automated\nantepartum CTG analysis. We systematically compare time-series FMs and LLMs\nagainst established CTG-specific architectures. Our evaluation encompasses over\n500 CTG recordings of varying durations reflecting real-world clinical\nrecordings, providing robust performance benchmarks across different modelling\nparadigms. Our results demonstrate that fine-tuned LLMs achieve superior\nperformance compared to both foundation models and domain-specific approaches,\noffering a promising alternative pathway for clinical CTG interpretation. These\nfindings provide critical insights into the relative strengths of different AI\nmethodologies for fetal monitoring applications and establish a foundation for\nfuture clinical AI development in prenatal care.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u6bd4\u8f83\u4e86\u7528\u4e8e\u81ea\u52a8\u4ea7\u524dCTG\u5206\u6790\u7684\u6700\u5148\u8fdbAI\u65b9\u6cd5\uff0c\u53d1\u73b0\u5fae\u8c03\u7684LLMs\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u9886\u57df\u7279\u5b9a\u65b9\u6cd5\uff0c\u4e3a\u4e34\u5e8aCTG\u89e3\u91ca\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u9014\u5f84\u3002", "motivation": "\u7535\u5b50\u80ce\u513f\u76d1\u6d4b\uff08EFM\uff09/\u80ce\u5fc3\u76d1\u62a4\uff08CTG\uff09\u5206\u6790\u662f\u8bc4\u4f30\u80ce\u513f\u5065\u5eb7\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5f53\u524dCTG\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u4e34\u5e8a\u89e3\u91ca\uff0c\u5bfc\u81f4\u8bca\u65ad\u51c6\u786e\u6027\u5b58\u5728\u5dee\u5f02\u3002\u57fa\u7840\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728CTG\u5206\u6790\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u3001LLMs\u4e0e\u5df2\u5efa\u7acb\u7684CTG\u7279\u5b9a\u67b6\u6784\uff0c\u8bc4\u4f30\u6db5\u76d6500\u591a\u4e2a\u4e0d\u540c\u65f6\u957f\u7684CTG\u8bb0\u5f55\uff0c\u53cd\u6620\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u8bb0\u5f55\u3002", "result": "\u5fae\u8c03\u7684LLMs\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u9886\u57df\u7279\u5b9a\u65b9\u6cd5\uff0c\u4e3a\u4e34\u5e8aCTG\u89e3\u91ca\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u9014\u5f84\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u80ce\u513f\u76d1\u6d4b\u5e94\u7528\u4e2d\u4e0d\u540cAI\u65b9\u6cd5\u7684\u76f8\u5bf9\u4f18\u52bf\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u4ea7\u524d\u62a4\u7406\u4e34\u5e8aAI\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.19235", "categories": ["eess.SP", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.19235", "abs": "https://arxiv.org/abs/2509.19235", "authors": ["Wamberto J. L. Queiroz", "Hugerles S. Silva", "Higo T. P. Silva", "Alexandros-Apostolos A. Boulogeorgos"], "title": "On the Performance of THz Wireless Systems over $\u03b1$-$\\mathcal{F}$ Channels with Beam Misalignment and Mobility", "comment": null, "summary": "This paper investigates the performance of terahertz~(THz) wireless systems\nover the $\\alpha$-$\\mathcal{F}$ fading channels with beam misalignment and\nmobility. New expressions are derived for the probability density, cumulative\ndistribution, and moment generating functions, as well as higher-order moments\nof the instantaneous signal-to-noise ratio. Building upon the aforementioned\nexpressions, we extract novel formulas for the outage probability, symbol error\nprobability, and average channel capacity. Asymptotic metrics are also deduced,\nwhich provide useful insights. Monte Carlo simulations results are presented to\nsupport the derived analytical framework.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u592a\u8d6b\u5179\u65e0\u7ebf\u7cfb\u7edf\u5728\u03b1-F\u8870\u843d\u4fe1\u9053\u4e2d\u53d7\u6ce2\u675f\u5931\u51c6\u548c\u79fb\u52a8\u6027\u5f71\u54cd\u7684\u6027\u80fd\uff0c\u63a8\u5bfc\u4e86\u65b0\u7684\u7edf\u8ba1\u8868\u8fbe\u5f0f\u548c\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u7814\u7a76\u592a\u8d6b\u5179\u65e0\u7ebf\u7cfb\u7edf\u5728\u5b9e\u9645\u4fe1\u9053\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7279\u522b\u662f\u8003\u8651\u6ce2\u675f\u5931\u51c6\u548c\u79fb\u52a8\u6027\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63a8\u5bfc\u4e86\u77ac\u65f6\u4fe1\u566a\u6bd4\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u3001\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\u3001\u77e9\u751f\u6210\u51fd\u6570\u548c\u9ad8\u9636\u77e9\u7684\u65b0\u8868\u8fbe\u5f0f\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u8868\u8fbe\u5f0f\u63d0\u53d6\u4e86\u4e2d\u65ad\u6982\u7387\u3001\u7b26\u53f7\u9519\u8bef\u6982\u7387\u548c\u5e73\u5747\u4fe1\u9053\u5bb9\u91cf\u7684\u65b0\u516c\u5f0f\u3002", "result": "\u83b7\u5f97\u4e86\u6e10\u8fd1\u6027\u80fd\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u5206\u6790\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u6790\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u592a\u8d6b\u5179\u65e0\u7ebf\u7cfb\u7edf\u5728\u03b1-F\u8870\u843d\u4fe1\u9053\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2509.18469", "categories": ["cs.LG", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18469", "abs": "https://arxiv.org/abs/2509.18469", "authors": ["Han-Lin Hsieh", "Maryam M. Shanechi"], "title": "Probabilistic Geometric Principal Component Analysis with application to neural data", "comment": "Published at the International Conference on Learning Representations\n  (ICLR) 2025. Code is available at GitHub\n  https://github.com/ShanechiLab/PGPCA.git", "summary": "Dimensionality reduction is critical across various domains of science\nincluding neuroscience. Probabilistic Principal Component Analysis (PPCA) is a\nprominent dimensionality reduction method that provides a probabilistic\napproach unlike the deterministic approach of PCA and serves as a connection\nbetween PCA and Factor Analysis (FA). Despite their power, PPCA and its\nextensions are mainly based on linear models and can only describe the data in\na Euclidean coordinate system. However, in many neuroscience applications, data\nmay be distributed around a nonlinear geometry (i.e., manifold) rather than\nlying in the Euclidean space. We develop Probabilistic Geometric Principal\nComponent Analysis (PGPCA) for such datasets as a new dimensionality reduction\nalgorithm that can explicitly incorporate knowledge about a given nonlinear\nmanifold that is first fitted from these data. Further, we show how in addition\nto the Euclidean coordinate system, a geometric coordinate system can be\nderived for the manifold to capture the deviations of data from the manifold\nand noise. We also derive a data-driven EM algorithm for learning the PGPCA\nmodel parameters. As such, PGPCA generalizes PPCA to better describe data\ndistributions by incorporating a nonlinear manifold geometry. In simulations\nand brain data analyses, we show that PGPCA can effectively model the data\ndistribution around various given manifolds and outperforms PPCA for such data.\nMoreover, PGPCA provides the capability to test whether the new geometric\ncoordinate system better describes the data than the Euclidean one. Finally,\nPGPCA can perform dimensionality reduction and learn the data distribution both\naround and on the manifold. These capabilities make PGPCA valuable for\nenhancing the efficacy of dimensionality reduction for analysis of\nhigh-dimensional data that exhibit noise and are distributed around a nonlinear\nmanifold.", "AI": {"tldr": "PGPCA\u662fPPCA\u7684\u6269\u5c55\uff0c\u901a\u8fc7\u5f15\u5165\u975e\u7ebf\u6027\u6d41\u5f62\u51e0\u4f55\u6765\u66f4\u597d\u5730\u63cf\u8ff0\u5206\u5e03\u5728\u6d41\u5f62\u5468\u56f4\u7684\u6570\u636e\u5206\u5e03\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u79d1\u5b66\u7b49\u9886\u57df\u7684\u9ad8\u7ef4\u6570\u636e\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u7684PPCA\u53ca\u5176\u6269\u5c55\u4e3b\u8981\u57fa\u4e8e\u7ebf\u6027\u6a21\u578b\uff0c\u53ea\u80fd\u63cf\u8ff0\u6b27\u51e0\u91cc\u5f97\u5750\u6807\u7cfb\u4e2d\u7684\u6570\u636e\u3002\u4f46\u5728\u795e\u7ecf\u79d1\u5b66\u7b49\u5e94\u7528\u4e2d\uff0c\u6570\u636e\u53ef\u80fd\u5206\u5e03\u5728\u975e\u7ebf\u6027\u6d41\u5f62\u5468\u56f4\uff0c\u800c\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u3002", "method": "\u5f00\u53d1\u4e86\u6982\u7387\u51e0\u4f55\u4e3b\u6210\u5206\u5206\u6790\uff08PGPCA\uff09\uff0c\u901a\u8fc7\u663e\u5f0f\u5730\u878d\u5165\u4ece\u6570\u636e\u4e2d\u62df\u5408\u7684\u975e\u7ebf\u6027\u6d41\u5f62\u77e5\u8bc6\uff0c\u5e76\u63a8\u5bfc\u51fa\u51e0\u4f55\u5750\u6807\u7cfb\u6765\u6355\u6349\u6570\u636e\u4e0e\u6d41\u5f62\u7684\u504f\u5dee\u548c\u566a\u58f0\u3002\u8fd8\u63a8\u5bfc\u4e86\u6570\u636e\u9a71\u52a8\u7684EM\u7b97\u6cd5\u6765\u5b66\u4e60PGPCA\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728\u6a21\u62df\u548c\u8111\u6570\u636e\u5206\u6790\u4e2d\uff0cPGPCA\u80fd\u6709\u6548\u5efa\u6a21\u5404\u79cd\u7ed9\u5b9a\u6d41\u5f62\u5468\u56f4\u7684\u6570\u636e\u5206\u5e03\uff0c\u5e76\u4e14\u5728\u6b64\u7c7b\u6570\u636e\u4e0a\u4f18\u4e8ePPCA\u3002PGPCA\u8fd8\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u51e0\u4f55\u5750\u6807\u7cfb\u662f\u5426\u6bd4\u6b27\u51e0\u91cc\u5f97\u5750\u6807\u7cfb\u66f4\u597d\u5730\u63cf\u8ff0\u6570\u636e\u7684\u80fd\u529b\u3002", "conclusion": "PGPCA\u5c06PPCA\u63a8\u5e7f\u5230\u975e\u7ebf\u6027\u6d41\u5f62\u51e0\u4f55\uff0c\u80fd\u591f\u6267\u884c\u964d\u7ef4\u5e76\u5b66\u4e60\u6d41\u5f62\u5468\u56f4\u548c\u6d41\u5f62\u4e0a\u7684\u6570\u636e\u5206\u5e03\uff0c\u5bf9\u4e8e\u5206\u6790\u5177\u6709\u566a\u58f0\u4e14\u5206\u5e03\u5728\u975e\u7ebf\u6027\u6d41\u5f62\u5468\u56f4\u7684\u9ad8\u7ef4\u6570\u636e\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.18114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18114", "abs": "https://arxiv.org/abs/2509.18114", "authors": ["Javed I. Khan an Henry Uwabor Moye"], "title": "A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU", "comment": "12 pages, Technical Report 2025-07-01, Internetworking and Media\n  Communications Research Laboratories, Department of Computer Science, Kent\n  State University", "summary": "Autoregressive inference in large transformer-based language models (LLMs)\npresents significant challenges for runtime efficiency, particularly during the\ndecode phase where load imbalance across GPU shards can cause throughput\ndegradation and latency spikes. A DPU-assisted framework leveraged by\nBlueField-3 Data Processing Units can enable real-time detection and mitigation\nof load imbalance in multi-node tensor-parallel inference. By offloading\nmonitoring tasks to the DPU and analyzing GPU telemetry and inter-node\ncommunication patterns, the resulting system can provide actionable feedback to\ninference controllers and schedulers. The goal of this study is three-fold i)\nidentify the reported skews/imbalances/pathological conditions that arise in\nmuti-GPU execution of a) LLM tensor computing (both during training and\ninference), b) identify their impact on computational performance, and c) make\na critical assessment if those can be tracked for potential mitigation from a\nDPU network.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528BlueField-3 DPU\u6765\u5b9e\u65f6\u68c0\u6d4b\u548c\u7f13\u89e3\u591a\u8282\u70b9\u5f20\u91cf\u5e76\u884c\u63a8\u7406\u4e2d\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\u7684\u6846\u67b6\uff0c\u65e8\u5728\u8bc6\u522bLLM\u5728\u591aGPU\u6267\u884c\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\u53ca\u5176\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u578b\u53d8\u538b\u5668\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u56de\u5f52\u63a8\u7406\u5728\u89e3\u7801\u9636\u6bb5\u9762\u4e34GPU\u5206\u7247\u95f4\u8d1f\u8f7d\u4e0d\u5747\u8861\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u541e\u5410\u91cf\u4e0b\u964d\u548c\u5ef6\u8fdf\u5cf0\u503c\uff0c\u9700\u8981\u6709\u6548\u7684\u5b9e\u65f6\u76d1\u6d4b\u548c\u7f13\u89e3\u673a\u5236\u3002", "method": "\u5229\u7528DPU\u8f85\u52a9\u6846\u67b6\uff0c\u5c06\u76d1\u63a7\u4efb\u52a1\u5378\u8f7d\u5230DPU\uff0c\u901a\u8fc7\u5206\u6790GPU\u9065\u6d4b\u6570\u636e\u548c\u8282\u70b9\u95f4\u901a\u4fe1\u6a21\u5f0f\uff0c\u4e3a\u63a8\u7406\u63a7\u5236\u5668\u548c\u8c03\u5ea6\u5668\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u4e86\u591aGPU\u6267\u884cLLM\u5f20\u91cf\u8ba1\u7b97\u65f6\u51fa\u73b0\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u95ee\u9898\u5bf9\u8ba1\u7b97\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "DPU\u7f51\u7edc\u80fd\u591f\u6709\u6548\u8ddf\u8e2a\u548c\u7f13\u89e3LLM\u63a8\u7406\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u4e3a\u63d0\u5347\u63a8\u7406\u6548\u7387\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19272", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19272", "abs": "https://arxiv.org/abs/2509.19272", "authors": ["Sathwik Chadaga"], "title": "Faster-Than-Nyquist Signalling - Theoretical Limits on Capacity and Techniques to Approach Capacity", "comment": null, "summary": "Faster-Than-Nyquist (FTN) Signalling is a non-orthogonal transmission scheme\nthat violates the Nyquist zero-ISI criterion providing higher throughput and\nbetter spectral efficiency than a Nyquist transmission scheme. In this thesis,\nthe inter symbol interference (ISI) introduced by FTN signalling is studied,\nand conditions on pulse shapes and $\\tau$ (time acceleration factor) are\nderived so that the ISI can be avoided completely. Further, these conditions\nare reinforced by investigating the theoretical limits on the capacities of FTN\nsystems. Finally, the use of power allocation and adaptive loading techniques\nare explored in reducing the effect of ISI and increasing the throughput of\northogonal frequency division multiplexing (OFDM) FTN systems. The\nimplementation of these techniques and simulation results are also\ndemonstrated.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86FTN\u4fe1\u53f7\u4f20\u8f93\u4e2d\u7684\u7b26\u53f7\u95f4\u5e72\u6270\u95ee\u9898\uff0c\u63a8\u5bfc\u4e86\u907f\u514dISI\u7684\u8109\u51b2\u5f62\u72b6\u548c\u65f6\u95f4\u52a0\u901f\u56e0\u5b50\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u5bb9\u91cf\u7406\u8bba\u9a8c\u8bc1\uff0c\u63a2\u7d22\u4e86\u529f\u7387\u5206\u914d\u548c\u81ea\u9002\u5e94\u52a0\u8f7d\u6280\u672f\u6765\u63d0\u5347OFDM FTN\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "FTN\u4fe1\u53f7\u4f20\u8f93\u80fd\u591f\u63d0\u4f9b\u6bd4\u5948\u594e\u65af\u7279\u4f20\u8f93\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u548c\u9891\u8c31\u6548\u7387\uff0c\u4f46\u4f1a\u5f15\u5165\u7b26\u53f7\u95f4\u5e72\u6270\u3002\u7814\u7a76\u65e8\u5728\u627e\u5230\u5b8c\u5168\u907f\u514dISI\u7684\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6280\u672f\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63a8\u5bfc\u8109\u51b2\u5f62\u72b6\u548c\u65f6\u95f4\u52a0\u901f\u56e0\u5b50\u7684\u6570\u5b66\u6761\u4ef6\u4ee5\u907f\u514dISI\uff1b\u7814\u7a76FTN\u7cfb\u7edf\u7684\u7406\u8bba\u5bb9\u91cf\u9650\u5236\uff1b\u63a2\u7d22\u529f\u7387\u5206\u914d\u548c\u81ea\u9002\u5e94\u52a0\u8f7d\u6280\u672f\u5728OFDM FTN\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff1b\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u6280\u672f\u5b9e\u73b0\u6548\u679c\u3002", "result": "\u5efa\u7acb\u4e86\u907f\u514dFTN\u4fe1\u53f7ISI\u7684\u5145\u5206\u6761\u4ef6\uff1b\u9a8c\u8bc1\u4e86FTN\u7cfb\u7edf\u7684\u5bb9\u91cf\u7406\u8bba\u6781\u9650\uff1b\u5c55\u793a\u4e86\u529f\u7387\u5206\u914d\u548c\u81ea\u9002\u5e94\u52a0\u8f7d\u6280\u672f\u80fd\u591f\u6709\u6548\u51cf\u5c11ISI\u5f71\u54cd\u5e76\u63d0\u9ad8\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "conclusion": "FTN\u4fe1\u53f7\u4f20\u8f93\u5728\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u4ee5\u5b8c\u5168\u907f\u514dISI\uff0c\u7ed3\u5408\u529f\u7387\u4f18\u5316\u548c\u81ea\u9002\u5e94\u6280\u672f\u80fd\u591f\u663e\u8457\u63d0\u5347OFDM FTN\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u73b0\u9ad8\u901f\u9ad8\u6548\u901a\u4fe1\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.18766", "categories": ["cs.LG", "math.OC", "stat.ML", "62J07, 68T07", "G.3"], "pdf": "https://arxiv.org/pdf/2509.18766", "abs": "https://arxiv.org/abs/2509.18766", "authors": ["Rapha\u00ebl Berthier"], "title": "Diagonal Linear Networks and the Lasso Regularization Path", "comment": "29 pages, 1 figure", "summary": "Diagonal linear networks are neural networks with linear activation and\ndiagonal weight matrices. Their theoretical interest is that their implicit\nregularization can be rigorously analyzed: from a small initialization, the\ntraining of diagonal linear networks converges to the linear predictor with\nminimal 1-norm among minimizers of the training loss. In this paper, we deepen\nthis analysis showing that the full training trajectory of diagonal linear\nnetworks is closely related to the lasso regularization path. In this\nconnection, the training time plays the role of an inverse regularization\nparameter. Both rigorous results and simulations are provided to illustrate\nthis conclusion. Under a monotonicity assumption on the lasso regularization\npath, the connection is exact while in the general case, we show an approximate\nconnection.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5bf9\u89d2\u7ebf\u6027\u7f51\u7edc\u7684\u8bad\u7ec3\u8f68\u8ff9\u4e0eLASSO\u6b63\u5219\u5316\u8def\u5f84\u4e4b\u95f4\u7684\u7d27\u5bc6\u8054\u7cfb\uff0c\u5176\u4e2d\u8bad\u7ec3\u65f6\u95f4\u626e\u6f14\u4e86\u9006\u6b63\u5219\u5316\u53c2\u6570\u7684\u89d2\u8272\u3002", "motivation": "\u5bf9\u89d2\u7ebf\u6027\u7f51\u7edc\u56e0\u5176\u9690\u5f0f\u6b63\u5219\u5316\u53ef\u88ab\u4e25\u683c\u5206\u6790\u800c\u5177\u6709\u7406\u8bba\u610f\u4e49\u3002\u672c\u6587\u65e8\u5728\u6df1\u5316\u8fd9\u4e00\u5206\u6790\uff0c\u63a2\u7d22\u8bad\u7ec3\u8f68\u8ff9\u4e0eLASSO\u8def\u5f84\u7684\u786e\u5207\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\uff0c\u7814\u7a76\u5bf9\u89d2\u7ebf\u6027\u7f51\u7edc\u4ece\u8f83\u5c0f\u521d\u59cb\u5316\u5f00\u59cb\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u4e0eLASSO\u6b63\u5219\u5316\u8def\u5f84\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5728LASSO\u8def\u5f84\u5355\u8c03\u6027\u5047\u8bbe\u4e0b\uff0c\u4e24\u8005\u5173\u7cfb\u7cbe\u786e\u5bf9\u5e94\uff1b\u4e00\u822c\u60c5\u51b5\u4e0b\u5219\u5448\u73b0\u8fd1\u4f3c\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "\u5bf9\u89d2\u7ebf\u6027\u7f51\u7edc\u7684\u8bad\u7ec3\u52a8\u6001\u4e0eLASSO\u6b63\u5219\u5316\u8def\u5f84\u5bc6\u5207\u76f8\u5173\uff0c\u8bad\u7ec3\u65f6\u95f4\u53ef\u89c6\u4e3a\u9006\u6b63\u5219\u5316\u53c2\u6570\uff0c\u8fd9\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u9690\u5f0f\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.18115", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18115", "abs": "https://arxiv.org/abs/2509.18115", "authors": ["Hongyi Chen", "Xiucheng Li", "Xinyang Chen", "Jing Li", "Kehai Chen", "Liqiang Nie"], "title": "Towards Scalable and Structured Spatiotemporal Forecasting", "comment": null, "summary": "In this paper, we propose a novel Spatial Balance Attention block for\nspatiotemporal forecasting. To strike a balance between obeying spatial\nproximity and capturing global correlation, we partition the spatial graph into\na set of subgraphs and instantiate Intra-subgraph Attention to learn local\nspatial correlation within each subgraph; to capture the global spatial\ncorrelation, we further aggregate the nodes to produce subgraph representations\nand achieve message passing among the subgraphs via Inter-subgraph Attention.\nBuilding on the proposed Spatial Balance Attention block, we develop a\nmultiscale spatiotemporal forecasting model by progressively increasing the\nsubgraph scales. The resulting model is both scalable and able to produce\nstructured spatial correlation, and meanwhile, it is easy to implement. We\nevaluate its efficacy and efficiency against the existing models on real-world\nspatiotemporal datasets from medium to large sizes. The experimental results\nshow that it can achieve performance improvements up to 7.7% over the baseline\nmethods at low running costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a7a\u95f4\u5e73\u8861\u6ce8\u610f\u529b\u5757\u7528\u4e8e\u65f6\u7a7a\u9884\u6d4b\uff0c\u901a\u8fc7\u5c06\u7a7a\u95f4\u56fe\u5212\u5206\u4e3a\u5b50\u56fe\uff0c\u7ed3\u5408\u5b50\u56fe\u5185\u548c\u5b50\u56fe\u95f4\u6ce8\u610f\u529b\u673a\u5236\u6765\u5e73\u8861\u5c40\u90e8\u7a7a\u95f4\u90bb\u8fd1\u6027\u548c\u5168\u5c40\u76f8\u5173\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65f6\u7a7a\u9884\u6d4b\u4e2d\u5982\u4f55\u5728\u9075\u5faa\u7a7a\u95f4\u90bb\u8fd1\u6027\u7684\u540c\u65f6\u6709\u6548\u6355\u6349\u5168\u5c40\u7a7a\u95f4\u76f8\u5173\u6027\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5c06\u7a7a\u95f4\u56fe\u5212\u5206\u4e3a\u5b50\u56fe\uff0c\u4f7f\u7528\u5b50\u56fe\u5185\u6ce8\u610f\u529b\u5b66\u4e60\u5c40\u90e8\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u901a\u8fc7\u5b50\u56fe\u95f4\u6ce8\u610f\u529b\u5b9e\u73b0\u5b50\u56fe\u95f4\u7684\u6d88\u606f\u4f20\u9012\uff0c\u5e76\u6784\u5efa\u591a\u5c3a\u5ea6\u6a21\u578b\u9010\u6b65\u589e\u52a0\u5b50\u56fe\u89c4\u6a21\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u4e2d\u5927\u578b\u65f6\u7a7a\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe7.7%\uff0c\u4e14\u8fd0\u884c\u6210\u672c\u8f83\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e2\u80fd\u4ea7\u751f\u7ed3\u6784\u5316\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u53c8\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6613\u5b9e\u73b0\u6027\uff0c\u5728\u65f6\u7a7a\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.19275", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19275", "abs": "https://arxiv.org/abs/2509.19275", "authors": ["Junzhe Song", "Ruisi He", "Mi Yang", "Zhengyu Zhang", "Xinwen Chen", "Xiaoying Zhang", "Bo Ai"], "title": "A Novel Site-Specific Inference Model for Urban Canyon Channels: From Measurements to Modeling", "comment": null, "summary": "With the rapid development of intelligent transportation and smart city\napplications, urban canyon has become a critical scenario for the design and\nevaluation of wireless communication systems. Due to its unique environmental\nlayout, the channel characteristics in urban canyon are strongly a street\ngeometry and building distribution, thereby exhibiting significant\nsite-specific channel condition. However, this feature has not been well\ncaptured in existing channel models. In this paper, we propose a site-specific\nchannel inference model based on environmental geometry, the model is\nparameterized using sub-6GHz channel measurements. Multipath components (MPCs)\nare extracted and clustered according to geometric propagation, which are\nexplicitly derived from the influence of canyon width, thereby establishing an\ninterpretable mapping between the physical environment and statistical\ncharacteristics of MPCs. A step-by-step implementation scheme is presented.\nSubsequently, the proposed site-specific channel inference model is validated\nby comparing second-order statistics of channels, derived from the model and\nmeasurements. The results show that the proposed model achieves high accuracy\nand robustness in different urban canyon scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u51e0\u4f55\u7684\u7ad9\u70b9\u7279\u5b9a\u4fe1\u9053\u63a8\u65ad\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u4e9a6GHz\u4fe1\u9053\u6d4b\u91cf\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u901a\u8fc7\u51e0\u4f55\u4f20\u64ad\u63d0\u53d6\u548c\u805a\u7c7b\u591a\u5f84\u5206\u91cf\uff0c\u5efa\u7acb\u4e86\u7269\u7406\u73af\u5883\u4e0eMPC\u7edf\u8ba1\u7279\u6027\u4e4b\u95f4\u7684\u53ef\u89e3\u91ca\u6620\u5c04\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4ea4\u901a\u548c\u667a\u6167\u57ce\u5e02\u5e94\u7528\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u57ce\u5e02\u5ce1\u8c37\u5df2\u6210\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u548c\u8bc4\u4f30\u7684\u5173\u952e\u573a\u666f\u3002\u7531\u4e8e\u5176\u72ec\u7279\u7684\u73af\u5883\u5e03\u5c40\uff0c\u57ce\u5e02\u5ce1\u8c37\u4e2d\u7684\u4fe1\u9053\u7279\u6027\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u8857\u9053\u51e0\u4f55\u548c\u5efa\u7b51\u5206\u5e03\uff0c\u4ece\u800c\u8868\u73b0\u51fa\u663e\u8457\u7684\u7ad9\u70b9\u7279\u5b9a\u4fe1\u9053\u6761\u4ef6\u3002\u7136\u800c\uff0c\u8fd9\u4e00\u7279\u5f81\u5728\u73b0\u6709\u4fe1\u9053\u6a21\u578b\u4e2d\u5c1a\u672a\u5f97\u5230\u5f88\u597d\u7684\u6355\u6349\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u73af\u5883\u51e0\u4f55\u7684\u7ad9\u70b9\u7279\u5b9a\u4fe1\u9053\u63a8\u65ad\u6a21\u578b\uff0c\u4f7f\u7528\u4e9a6GHz\u4fe1\u9053\u6d4b\u91cf\u8fdb\u884c\u53c2\u6570\u5316\u3002\u6839\u636e\u51e0\u4f55\u4f20\u64ad\u63d0\u53d6\u548c\u805a\u7c7b\u591a\u5f84\u5206\u91cf\uff0c\u8fd9\u4e9b\u5206\u91cf\u660e\u786e\u6765\u6e90\u4e8e\u5ce1\u8c37\u5bbd\u5ea6\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5efa\u7acb\u7269\u7406\u73af\u5883\u4e0eMPC\u7edf\u8ba1\u7279\u6027\u4e4b\u95f4\u7684\u53ef\u89e3\u91ca\u6620\u5c04\u3002\u63d0\u51fa\u4e86\u9010\u6b65\u5b9e\u65bd\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u4ece\u6a21\u578b\u548c\u6d4b\u91cf\u5f97\u51fa\u7684\u4fe1\u9053\u4e8c\u9636\u7edf\u8ba1\u91cf\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u7ad9\u70b9\u7279\u5b9a\u4fe1\u9053\u63a8\u65ad\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u57ce\u5e02\u5ce1\u8c37\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ad9\u70b9\u7279\u5b9a\u4fe1\u9053\u63a8\u65ad\u6a21\u578b\u80fd\u591f\u51c6\u786e\u6355\u6349\u57ce\u5e02\u5ce1\u8c37\u73af\u5883\u4e2d\u7684\u4fe1\u9053\u7279\u6027\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2509.18964", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18964", "abs": "https://arxiv.org/abs/2509.18964", "authors": ["Xingtu Liu"], "title": "Central Limit Theorems for Asynchronous Averaged Q-Learning", "comment": null, "summary": "This paper establishes central limit theorems for Polyak-Ruppert averaged\nQ-learning under asynchronous updates. We present a non-asymptotic central\nlimit theorem, where the convergence rate in Wasserstein distance explicitly\nreflects the dependence on the number of iterations, state-action space size,\nthe discount factor, and the quality of exploration. In addition, we derive a\nfunctional central limit theorem, showing that the partial-sum process\nconverges weakly to a Brownian motion.", "AI": {"tldr": "\u672c\u6587\u4e3a\u5f02\u6b65\u66f4\u65b0\u7684Polyak-Ruppert\u5e73\u5747Q\u5b66\u4e60\u5efa\u7acb\u4e86\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\uff0c\u5305\u62ec\u975e\u6e10\u8fd1\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u548c\u51fd\u6570\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u3002", "motivation": "\u7814\u7a76\u5f02\u6b65\u66f4\u65b0\u73af\u5883\u4e0bQ\u5b66\u4e60\u7684\u7edf\u8ba1\u6027\u8d28\uff0c\u7279\u522b\u662f\u5176\u6536\u655b\u901f\u7387\u548c\u6781\u9650\u5206\u5e03\uff0c\u4e3a\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u652f\u6491\u3002", "method": "\u91c7\u7528Polyak-Ruppert\u5e73\u5747\u6280\u672f\uff0c\u5206\u6790\u5f02\u6b65Q\u5b66\u4e60\u7b97\u6cd5\u5728Wasserstein\u8ddd\u79bb\u4e0b\u7684\u6536\u655b\u6027\u8d28\uff0c\u5e76\u63a8\u5bfc\u90e8\u5206\u548c\u8fc7\u7a0b\u7684\u5f31\u6536\u655b\u6027\u3002", "result": "\u5f97\u5230\u4e86\u660e\u786e\u7684\u6536\u655b\u901f\u7387\u8868\u8fbe\u5f0f\uff0c\u53cd\u6620\u4e86\u8fed\u4ee3\u6b21\u6570\u3001\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u5927\u5c0f\u3001\u6298\u6263\u56e0\u5b50\u548c\u63a2\u7d22\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u8bc1\u660e\u4e86\u90e8\u5206\u548c\u8fc7\u7a0b\u6536\u655b\u4e8e\u5e03\u6717\u8fd0\u52a8\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f02\u6b65Q\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7edf\u8ba1\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5176\u6536\u655b\u884c\u4e3a\u7684\u672c\u8d28\u7279\u5f81\u3002"}}
{"id": "2509.18116", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18116", "abs": "https://arxiv.org/abs/2509.18116", "authors": ["Nathan Egbuna", "Saatvik Gaur", "Sunishchal Dev", "Ashwinee Panda", "Maheep Chaudhary"], "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization", "comment": null, "summary": "Test-time optimization remains impractical at scale due to prohibitive\ninference costs\\textemdash techniques like iterative refinement and multi-step\nverification can require $10$--$100\\times$ more compute per query than standard\ndecoding. Latent space test-time optimization methods like LatentSeek offer a\nmore direct approach by steering hidden representations, but still demand\nexpensive per-query optimization loops with multiple backward passes. We\npropose Amortized Latent Steering (ALS), which collapses this iterative\noptimization into a single offline-computed vector applied at constant cost\nduring inference. ALS computes the mean difference between hidden states from\nsuccessful versus unsuccessful generations, then uses this direction to\ncalibrate the model's hidden representations: when decoding drifts away from\nthe success manifold, ALS nudges activations back toward it. Across GSM8K and\nMATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative\nmethods while matching or surpassing greedy Chain-of-Thought (CoT) and\nSelf-Consistency baselines, yielding up to 101\\% improvement in\nefficiency--accuracy trade-off. These results show that much of latent\noptimization's benefit can be captured offline, making sophisticated reasoning\ntechniques viable for production deployment. Code is available\nat~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}", "AI": {"tldr": "\u63d0\u51fa\u4e86\u644a\u9500\u6f5c\u5728\u5f15\u5bfc\uff08ALS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u8ba1\u7b97\u5355\u4e2a\u5411\u91cf\u6765\u66ff\u4ee3\u6602\u8d35\u7684\u9010\u67e5\u8be2\u4f18\u5316\u5faa\u73af\uff0c\u5728\u63a8\u7406\u65f6\u4ee5\u6052\u5b9a\u6210\u672c\u5e94\u7528\uff0c\u5b9e\u73b02-5\u500d\u52a0\u901f\uff0c\u540c\u65f6\u5339\u914d\u6216\u8d85\u8d8a\u8d2a\u5a6a\u601d\u7ef4\u94fe\u548c\u81ea\u4e00\u81f4\u6027\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u4f18\u5316\u65b9\u6cd5\uff08\u5982\u8fed\u4ee3\u4f18\u5316\u548c\u591a\u6b65\u9a8c\u8bc1\uff09\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u898110-100\u500d\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u800c\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u65b9\u6cd5\u5982LatentSeek\u4ecd\u9700\u8981\u6602\u8d35\u7684\u9010\u67e5\u8be2\u4f18\u5316\u5faa\u73af\u3002", "method": "ALS\u8ba1\u7b97\u6210\u529f\u4e0e\u5931\u8d25\u751f\u6210\u9690\u85cf\u72b6\u6001\u7684\u5e73\u5747\u5dee\u5f02\u5411\u91cf\uff0c\u5728\u63a8\u7406\u65f6\u7528\u8fd9\u4e2a\u65b9\u5411\u6821\u51c6\u6a21\u578b\u7684\u9690\u85cf\u8868\u793a\uff1a\u5f53\u89e3\u7801\u504f\u79bb\u6210\u529f\u6d41\u5f62\u65f6\uff0cALS\u5c06\u6fc0\u6d3b\u503c\u63a8\u56de\u6b63\u786e\u65b9\u5411\u3002", "result": "\u5728GSM8K\u548cMATH-500\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cALS\u76f8\u6bd4\u8fed\u4ee3\u65b9\u6cd5\u5b9e\u73b02-5\u500d\u52a0\u901f\uff0c\u540c\u65f6\u5339\u914d\u6216\u8d85\u8d8a\u8d2a\u5a6aCoT\u548c\u81ea\u4e00\u81f4\u6027\u57fa\u7ebf\uff0c\u6548\u7387-\u51c6\u786e\u7387\u6743\u8861\u63d0\u5347\u9ad8\u8fbe101%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6f5c\u5728\u4f18\u5316\u7684\u4e3b\u8981\u6536\u76ca\u53ef\u4ee5\u901a\u8fc7\u79bb\u7ebf\u65b9\u5f0f\u83b7\u5f97\uff0c\u4f7f\u590d\u6742\u63a8\u7406\u6280\u672f\u80fd\u591f\u5b9e\u9645\u90e8\u7f72\u5230\u751f\u4ea7\u73af\u5883\u4e2d\u3002"}}
{"id": "2509.19281", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19281", "abs": "https://arxiv.org/abs/2509.19281", "authors": ["Xiyang Lan", "Xin Li"], "title": "STFT-AECNN: An Attention-Enhanced CNN for Efficient \u03a6-OTDR Event Recognition in IoT-Enabled Distributed Acoustic Sensing", "comment": null, "summary": "Phase-sensitive optical time-domain reflectometry ({\\Phi}-OTDR) has emerged\nas a promising sensing technology in Internet of Things (IoT) infrastructures,\nenabling large-scale distributed acoustic sensing (DAS) for smart city\nsurveillance, industrial pipeline monitoring, and critical infrastructure\nprotection. However, accurately recognizing events from massive {\\Phi}-OTDR\ndata streams remains challenging, as existing deep learning methods either\ndisrupt the inherent spatiotemporal structure of signals or incur prohibitive\ncomputational costs, limiting their applicability in resource-constrained IoT\nscenarios. To overcome these challenges, we propose a novel STFT-based\nAttention-Enhanced Convolutional Neural Network (STFT-AECNN), which represents\nmulti-channel time-series data as stacked spectrograms to fully exploit their\nspatiotemporal characteristics while enabling efficient 2D CNN processing. A\nSpatial Efficient Attention Module (SEAM) is further introduced to adaptively\nemphasize the most informative channels, and a joint Cross-Entropy and Triplet\nloss is adopted to enhance the discriminability of the learned feature space.\nExtensive experiments on the public BJTU {\\Phi}-OTDR dataset demonstrate that\nSTFT-AECNN achieves a peak accuracy of 99.94% while maintaining high\ncomputational efficiency. These results highlight its potential for real-time,\nscalable, and robust event recognition in IoT-enabled DAS systems, paving the\nway for reliable and intelligent IoT sensing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSTFT\u7684\u6ce8\u610f\u529b\u589e\u5f3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08STFT-AECNN\uff09\uff0c\u7528\u4e8e\u76f8\u4f4d\u654f\u611f\u5149\u65f6\u57df\u53cd\u5c04\u8ba1\uff08\u03a6-OTDR\uff09\u6570\u636e\u7684\u4e8b\u4ef6\u8bc6\u522b\uff0c\u5728\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u8fbe\u523099.94%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u03a6-OTDR\u6570\u636e\u65f6\u8981\u4e48\u7834\u574f\u4fe1\u53f7\u7684\u65f6\u7a7a\u7ed3\u6784\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684IoT\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5c06\u591a\u901a\u9053\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8868\u793a\u4e3a\u5806\u53e0\u7684\u9891\u8c31\u56fe\uff0c\u5229\u75282D CNN\u5904\u7406\uff1b\u5f15\u5165\u7a7a\u95f4\u9ad8\u6548\u6ce8\u610f\u529b\u6a21\u5757\uff08SEAM\uff09\u81ea\u9002\u5e94\u5f3a\u8c03\u4fe1\u606f\u91cf\u6700\u5927\u7684\u901a\u9053\uff1b\u91c7\u7528\u4ea4\u53c9\u71b5\u548c\u4e09\u5143\u7ec4\u635f\u5931\u7684\u8054\u5408\u635f\u5931\u51fd\u6570\u589e\u5f3a\u7279\u5f81\u7a7a\u95f4\u7684\u53ef\u533a\u5206\u6027\u3002", "result": "\u5728BJTU \u03a6-OTDR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTFT-AECNN\u8fbe\u5230\u4e8699.94%\u7684\u5cf0\u503c\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728IoT\u652f\u6301\u7684DAS\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u4e8b\u4ef6\u8bc6\u522b\u6f5c\u529b\uff0c\u4e3a\u53ef\u9760\u7684\u667a\u80fdIoT\u4f20\u611f\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.19104", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19104", "abs": "https://arxiv.org/abs/2509.19104", "authors": ["Sharan Sahu", "Martin T. Wells"], "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment", "comment": "70 pages, 9 figures, 3 tables", "summary": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates.", "AI": {"tldr": "DRO-REBEL\u662f\u4e00\u79cd\u9c81\u68d2\u7684\u79bb\u7ebfRLHF\u65b9\u6cd5\uff0c\u901a\u8fc7Wasserstein\u3001KL\u548c\u03c7\u00b2\u6a21\u7cca\u96c6\u89e3\u51b3\u5956\u52b1\u9519\u8bef\u89c4\u8303\u548c\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u6700\u4f18\u53c2\u6570\u5316\u6536\u655b\u7387\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebfRLHF\u65b9\u6cd5\u5b58\u5728\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u6a21\u578b\u4f1a\u8fc7\u5ea6\u62df\u5408\u5956\u52b1\u9519\u8bef\u89c4\u8303\u5e76\u504f\u79bb\u8bad\u7ec3\u671f\u95f4\u89c2\u5bdf\u5230\u7684\u504f\u597d\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684DRO-REBEL\u66f4\u65b0\u5bb6\u65cf\uff0c\u4f7f\u7528Fenchel\u5bf9\u5076\u5c06\u66f4\u65b0\u7b80\u5316\u4e3a\u76f8\u5bf9\u5956\u52b1\u56de\u5f52\uff0c\u907f\u514d\u4e86PPO\u98ce\u683c\u7684\u88c1\u526a\u548c\u8f85\u52a9\u4ef7\u503c\u7f51\u7edc\u3002", "result": "\u5728Emotion Alignment\u3001ArmoRM\u591a\u76ee\u6807\u57fa\u51c6\u548cHH-Alignment\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6700\u574f\u60c5\u51b5\u9c81\u68d2\u6027\uff0c\u03c7\u00b2-REBEL\u5728\u7ecf\u9a8c\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u7a33\u5b9a\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u65e0\u514d\u8d39\u5348\u9910\u6743\u8861\uff1a\u534a\u5f84\u6536\u7f29\u901f\u5ea6\u5feb\u4e8e\u7ecf\u9a8c\u6563\u5ea6\u96c6\u4e2d\u7387\u53ef\u5b9e\u73b0\u6700\u4f18\u53c2\u6570\u5316\u7387\u4f46\u4f1a\u4e27\u5931\u8986\u76d6\u4fdd\u8bc1\uff0c\u800c\u4fdd\u8bc1\u8986\u76d6\u7684\u534a\u5f84\u4f1a\u5e26\u6765O(n^{-1/4})\u7684\u6536\u655b\u7387\u3002"}}
{"id": "2509.18117", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18117", "abs": "https://arxiv.org/abs/2509.18117", "authors": ["Eric Petit", "Denis Ch\u00eane"], "title": "Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs", "comment": "soumis {\\`a} la conf{\\'e}rence IHM 2025", "summary": "The paper presents a machine learning approach to design digital interfaces\nthat can dynamically adapt to different users and usage strategies. The\nalgorithm uses Bayesian statistics to model users' browsing behavior, focusing\non their habits rather than group preferences. It is distinguished by its\nonline incremental learning, allowing reliable predictions even with little\ndata and in the case of a changing environment. This inference method generates\na task model, providing a graphical representation of navigation with the usage\nstatistics of the current user. The algorithm learns new tasks while preserving\nprior knowledge. The theoretical framework is described, and simulations show\nthe effectiveness of the approach in stationary and non-stationary\nenvironments. In conclusion, this research paves the way for adaptive systems\nthat improve the user experience by helping them to better navigate and act on\ntheir interface.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6570\u5b57\u754c\u9762\u81ea\u9002\u5e94\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u7edf\u8ba1\u5efa\u6a21\u7528\u6237\u6d4f\u89c8\u884c\u4e3a\uff0c\u901a\u8fc7\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u7528\u6237\u548c\u4f7f\u7528\u7b56\u7565", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u7528\u6237\u548c\u4f7f\u7528\u7b56\u7565\u7684\u6570\u5b57\u754c\u9762\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u5bfc\u822a\u548c\u64cd\u4f5c\u754c\u9762", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u7edf\u8ba1\u65b9\u6cd5\u5efa\u6a21\u7528\u6237\u6d4f\u89c8\u4e60\u60ef\uff0c\u91c7\u7528\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\u7b97\u6cd5\uff0c\u751f\u6210\u4efb\u52a1\u6a21\u578b\u4ee5\u56fe\u5f62\u5316\u8868\u793a\u7528\u6237\u5bfc\u822a\u884c\u4e3a\u548c\u4f7f\u7528\u7edf\u8ba1", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9759\u6001\u548c\u975e\u9759\u6001\u73af\u5883\u4e2d\u90fd\u6709\u6548\uff0c\u80fd\u591f\u53ef\u9760\u9884\u6d4b\u7528\u6237\u884c\u4e3a\uff0c\u5373\u4f7f\u6570\u636e\u91cf\u5c11\u6216\u73af\u5883\u53d8\u5316", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u81ea\u9002\u5e94\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u5bfc\u822a\u548c\u64cd\u4f5c\u754c\u9762\u6765\u6539\u5584\u7528\u6237\u4f53\u9a8c"}}
{"id": "2509.18653", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18653", "abs": "https://arxiv.org/abs/2509.18653", "authors": ["Paris A. Karakasis", "Nicholas D. Sidiropoulos"], "title": "Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering", "comment": "13 pages, Submitted to IEEE Transactions on Signal Processing", "summary": "We introduce a novel framework for clustering a collection of tall matrices\nbased on their column spaces, a problem we term Subspace Clustering of\nSubspaces (SCoS). Unlike traditional subspace clustering methods that assume\nvectorized data, our formulation directly models each data sample as a matrix\nand clusters them according to their underlying subspaces. We establish\nconceptual links to Subspace Clustering and Generalized Canonical Correlation\nAnalysis (GCCA), and clarify key differences that arise in this more general\nsetting. Our approach is based on a Block Term Decomposition (BTD) of a\nthird-order tensor constructed from the input matrices, enabling joint\nestimation of cluster memberships and partially shared subspaces. We provide\nthe first identifiability results for this formulation and propose scalable\noptimization algorithms tailored to large datasets. Experiments on real-world\nhyperspectral imaging datasets demonstrate that our method achieves superior\nclustering accuracy and robustness, especially under high noise and\ninterference, compared to existing subspace clustering techniques. These\nresults highlight the potential of the proposed framework in challenging\nhigh-dimensional applications where structure exists beyond individual data\nvectors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u7ef4\u77e9\u9635\u805a\u7c7b\u6846\u67b6\u2014\u2014\u5b50\u7a7a\u95f4\u805a\u7c7b\uff08SCoS\uff09\uff0c\u76f4\u63a5\u5bf9\u77e9\u9635\u6570\u636e\u8fdb\u884c\u5b50\u7a7a\u95f4\u805a\u7c7b\uff0c\u800c\u975e\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u5411\u91cf\u5316\u5904\u7406\u3002", "motivation": "\u4f20\u7edf\u5b50\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\u9700\u8981\u5c06\u77e9\u9635\u6570\u636e\u5411\u91cf\u5316\uff0c\u8fd9\u4f1a\u7834\u574f\u6570\u636e\u7684\u56fa\u6709\u7ed3\u6784\u3002\u672c\u6587\u65e8\u5728\u76f4\u63a5\u5bf9\u77e9\u9635\u6837\u672c\u8fdb\u884c\u805a\u7c7b\uff0c\u66f4\u597d\u5730\u4fdd\u7559\u9ad8\u7ef4\u6570\u636e\u7684\u5b50\u7a7a\u95f4\u7ed3\u6784\u3002", "method": "\u57fa\u4e8e\u5757\u9879\u5206\u89e3\uff08BTD\uff09\u6784\u5efa\u4e09\u9636\u5f20\u91cf\uff0c\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u8054\u5408\u4f30\u8ba1\u805a\u7c7b\u6210\u5458\u5173\u7cfb\u548c\u90e8\u5206\u5171\u4eab\u5b50\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u9ad8\u5149\u8c31\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u805a\u7c7b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5b50\u7a7a\u95f4\u805a\u7c7b\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u9ad8\u566a\u58f0\u548c\u5e72\u6270\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5904\u7406\u5177\u6709\u8d85\u8d8a\u5355\u4e2a\u6570\u636e\u5411\u91cf\u7ed3\u6784\u7684\u9ad8\u7ef4\u5e94\u7528\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u590d\u6742\u9ad8\u7ef4\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19189", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19189", "abs": "https://arxiv.org/abs/2509.19189", "authors": ["Binghui Li", "Fengling Chen", "Zixun Huang", "Lean Wang", "Lei Wu"], "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws", "comment": "52 pages, accepted by NeurIPS 2025 as a spotlight paper", "summary": "Scaling laws have played a cornerstone role in guiding the training of large\nlanguage models (LLMs). However, most existing works on scaling laws primarily\nfocus on the final-step loss, overlooking the loss dynamics during the training\nprocess and, crucially, the impact of learning rate schedule (LRS). In this\npaper, we aim to bridge this gap by studying a teacher-student kernel\nregression setup trained via online stochastic gradient descent (SGD).\nLeveraging a novel intrinsic time viewpoint and stochastic differential\nequation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),\nwhich characterizes the evolution of population risk during the training\nprocess for general LRSs. Remarkably, the impact of the LRSs is captured\nthrough an explicit convolution-type functional term, making their effects\nfully tractable. To illustrate the utility of FSL, we analyze three widely used\nLRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under\nboth data-limited and compute-limited regimes. We provide theoretical\njustification for widely adopted empirical practices in LLMs pre-training such\nas (i) higher-capacity models are more data- and compute-efficient; (ii)\nlearning rate decay can improve training efficiency; (iii) WSD-like schedules\ncan outperform direct-decay schedules. Lastly, we explore the practical\nrelevance of FSL as a surrogate model for fitting, predicting and optimizing\nthe loss curves in LLM pre-training, with experiments conducted across model\nsizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen\nthe understanding of LLM pre-training dynamics and provide insights for\nimproving large-scale model training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u529f\u80fd\u7f29\u653e\u5b9a\u5f8b\uff08FSL\uff09\uff0c\u901a\u8fc7\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u5efa\u6a21SGD\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5c06\u5b66\u4e60\u7387\u8c03\u5ea6\u7684\u5f71\u54cd\u663e\u5f0f\u5730\u7eb3\u5165\u635f\u5931\u52a8\u6001\u5206\u6790\u4e2d\uff0c\u4e3aLLM\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u635f\u5931\uff0c\u5ffd\u7565\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u635f\u5931\u52a8\u6001\u548c\u5b66\u4e60\u7387\u8c03\u5ea6\u7684\u5f71\u54cd\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5e08\u751f\u6838\u56de\u5f52\u8bbe\u7f6e\u548c\u5728\u7ebfSGD\u8bad\u7ec3\uff0c\u5f15\u5165\u5185\u5728\u65f6\u95f4\u89c6\u89d2\u548cSDE\u5efa\u6a21\uff0c\u63a8\u5bfc\u51faFSL\u6846\u67b6\u6765\u5206\u6790\u4e0d\u540c\u5b66\u4e60\u7387\u8c03\u5ea6\u4e0b\u7684\u635f\u5931\u6f14\u5316\u3002", "result": "FSL\u80fd\u591f\u51c6\u786e\u6355\u6349\u5b66\u4e60\u7387\u8c03\u5ea6\u7684\u5f71\u54cd\uff0c\u7406\u8bba\u9a8c\u8bc1\u4e86\u66f4\u9ad8\u5bb9\u91cf\u6a21\u578b\u66f4\u9ad8\u6548\u3001\u5b66\u4e60\u7387\u8870\u51cf\u63d0\u5347\u6548\u7387\u3001WSD\u8c03\u5ea6\u4f18\u4e8e\u76f4\u63a5\u8870\u51cf\u7b49\u7ecf\u9a8c\u5b9e\u8df5\u3002", "conclusion": "FSL\u6846\u67b6\u6df1\u5316\u4e86\u5bf9LLM\u9884\u8bad\u7ec3\u52a8\u6001\u7684\u7406\u89e3\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u4f18\u5316\u5de5\u5177\u3002"}}
{"id": "2509.18118", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.18118", "abs": "https://arxiv.org/abs/2509.18118", "authors": ["Marcelo Ribeiro", "Diogo Costa", "Gon\u00e7alo Moreira", "Sandro Pinto", "Tiago Gomes"], "title": "Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices", "comment": null, "summary": "Modern IoT devices increasingly rely on machine learning solutions to process\ndata locally. However, the lack of graphics processing units (GPUs) or\ndedicated accelerators on most platforms makes on-device training largely\ninfeasible, often requiring cloud-based services to perform this task. This\nprocedure often raises privacy-related concerns, and creates dependency on\nreliable and always-on connectivity. Federated Learning (FL) is a new trend\nthat addresses these issues by enabling decentralized and collaborative\ntraining directly on devices, but it requires highly efficient optimization\nalgorithms. L-SGD, a lightweight variant of stochastic gradient descent, has\nenabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).\nThis work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture\nthat still lacks robust support for on-device training. L-SGD was evaluated on\nboth Arm and RISC-V platforms using 32-bit floating-point arithmetic,\nhighlighting the performance impact of the absence of Floating-Point Units\n(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit\nquantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in\nmemory usage and a 2.2x speedup in training time, with negligible accuracy\ndegradation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u8f7b\u91cf\u7ea7\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08L-SGD\uff09\u7b97\u6cd5\u6269\u5c55\u5230RISC-V\u67b6\u6784\u7684\u5fae\u63a7\u5236\u5668\u4e0a\uff0c\u901a\u8fc78\u4f4d\u91cf\u5316\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u89e3\u51b3\u4e86RISC-V\u5e73\u53f0\u7f3a\u4e4f\u6d6e\u70b9\u5355\u5143\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3IoT\u8bbe\u5907\u9700\u8981\u672c\u5730\u673a\u5668\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u5927\u591a\u6570\u5e73\u53f0\u7f3a\u4e4fGPU\u6216\u4e13\u7528\u52a0\u901f\u5668\uff0c\u4f7f\u5f97\u8bbe\u5907\u7aef\u8bad\u7ec3\u4e0d\u53ef\u884c\u3002\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002RISC-V\u4f5c\u4e3a\u65b0\u5174\u5f00\u6e90\u67b6\u6784\uff0c\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8bbe\u5907\u7aef\u8bad\u7ec3\u7684\u7a33\u5065\u652f\u6301\u3002", "method": "\u5c06L-SGD\u7b97\u6cd5\u6269\u5c55\u5230RISC-V MCU\u5e73\u53f0\uff0c\u8bc4\u4f30\u4e8632\u4f4d\u6d6e\u70b9\u8fd0\u7b97\u7684\u6027\u80fd\u5f71\u54cd\uff0c\u5e76\u9488\u5bf9RISC-V\u7f3a\u4e4fFPU\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e868\u4f4d\u91cf\u5316\u7248\u672c\u7684L-SGD\u7b97\u6cd5\u3002", "result": "8\u4f4d\u91cf\u5316L-SGD\u5728RISC-V\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u8fd14\u500d\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472.2\u500d\uff0c\u540c\u65f6\u51c6\u786e\u7387\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8bc1\u660e\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684RISC-V MCU\u4e0a\u5b9e\u73b0\u9ad8\u6548\u8bbe\u5907\u7aef\u8bad\u7ec3\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f00\u6e90\u786c\u4ef6\u5e73\u53f0\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2509.19234", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19234", "abs": "https://arxiv.org/abs/2509.19234", "authors": ["Hesam Hosseini", "Ying Cao", "Ali H. Sayed"], "title": "Stability and Generalization of Adversarial Diffusion Training", "comment": null, "summary": "Algorithmic stability is an established tool for analyzing generalization.\nWhile adversarial training enhances model robustness, it often suffers from\nrobust overfitting and an enlarged generalization gap. Although recent work has\nestablished the convergence of adversarial training in decentralized networks,\nits generalization properties remain unexplored. This work presents a\nstability-based generalization analysis of adversarial training under the\ndiffusion strategy for convex losses. We derive a bound showing that the\ngeneralization error grows with both the adversarial perturbation strength and\nthe number of training steps, a finding consistent with single-agent case but\nnovel for decentralized settings. Numerical experiments on logistic regression\nvalidate these theoretical predictions.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7b97\u6cd5\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u7814\u7a76\u4e86\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e2d\u5bf9\u6297\u8bad\u7ec3\u5728\u6269\u6563\u7b56\u7565\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u53d1\u73b0\u6cdb\u5316\u8bef\u5dee\u968f\u5bf9\u6297\u6270\u52a8\u5f3a\u5ea6\u548c\u8bad\u7ec3\u6b65\u6570\u589e\u52a0\u800c\u589e\u957f", "motivation": "\u5bf9\u6297\u8bad\u7ec3\u867d\u7136\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4f46\u5e38\u9762\u4e34\u9c81\u68d2\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u5dee\u8ddd\u6269\u5927\u7684\u95ee\u9898\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u8bc1\u660e\u4e86\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e2d\u5bf9\u6297\u8bad\u7ec3\u7684\u6536\u655b\u6027\uff0c\u4f46\u5176\u6cdb\u5316\u7279\u6027\u5c1a\u672a\u88ab\u63a2\u7d22", "method": "\u4f7f\u7528\u7b97\u6cd5\u7a33\u5b9a\u6027\u5de5\u5177\uff0c\u5728\u51f8\u635f\u5931\u51fd\u6570\u5047\u8bbe\u4e0b\uff0c\u5bf9\u6269\u6563\u7b56\u7565\u4e0b\u7684\u5bf9\u6297\u8bad\u7ec3\u8fdb\u884c\u6cdb\u5316\u5206\u6790", "result": "\u63a8\u5bfc\u51fa\u6cdb\u5316\u8bef\u5dee\u754c\uff0c\u663e\u793a\u6cdb\u5316\u8bef\u5dee\u968f\u5bf9\u6297\u6270\u52a8\u5f3a\u5ea6\u548c\u8bad\u7ec3\u6b65\u6570\u589e\u52a0\u800c\u589e\u957f\uff0c\u8fd9\u4e00\u53d1\u73b0\u4e0e\u5355\u667a\u80fd\u4f53\u60c5\u51b5\u4e00\u81f4\uff0c\u4f46\u5728\u53bb\u4e2d\u5fc3\u5316\u8bbe\u7f6e\u4e2d\u662f\u65b0\u9896\u7684", "conclusion": "\u5728\u903b\u8f91\u56de\u5f52\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u5bf9\u6297\u8bad\u7ec3\u7684\u6cdb\u5316\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e"}}
{"id": "2509.18119", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18119", "abs": "https://arxiv.org/abs/2509.18119", "authors": ["Yifan Xu", "Xiao Liu", "Xinghan Liu", "Jiaqi Fu", "Hanchen Zhang", "Bohao Jing", "Shudan Zhang", "Yuting Wang", "Wenyi Zhao", "Yuxiao Dong"], "title": "MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents", "comment": null, "summary": "Building general-purpose graphical user interface (GUI) agents has become\nincreasingly promising with the progress in vision language models. However,\ndeveloping effective mobile GUI agents with reinforcement learning (RL) remains\nchallenging due to the heavy-tailed distribution of task difficulty and the\ninefficiency of large-scale environment sampling. We present an online agentic\nreinforcement learning framework MOBILERL to enhance GUI agents in mobile\nenvironments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)\nalgorithm. In ADAGRPO, we design difficulty-adaptive positive replay and\nfailure curriculum filtering to adapt the model to different task difficulties.\nWe introduce the shortest path reward adjustment strategy to reshape rewards\nconcerning the task length in multi-turn agentic tasks. Those strategies\njointly stabilize RL training, improve sample efficiency, and generate strong\nperformance across diverse mobile apps and tasks. We apply MOBILERL to two open\nmodels (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B\nmodel achieves state-of-the-art results in terms of success rates on both\nAndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted\nin the AutoGLM products, and also open-sourced at\nhttps://github.com/THUDM/MobileRL.", "AI": {"tldr": "MOBILERL\u662f\u4e00\u4e2a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96be\u5ea6\u81ea\u9002\u5e94\u7b97\u6cd5ADAGRPO\u63d0\u5347\u79fb\u52a8GUI\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5728AndroidWorld\u548cAndroidLab\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u5f00\u53d1\u6709\u6548\u7684\u79fb\u52a8GUI\u4ee3\u7406\u9762\u4e34\u4efb\u52a1\u96be\u5ea6\u5206\u5e03\u91cd\u5c3e\u548c\u5927\u89c4\u6a21\u73af\u5883\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faADAGRPO\u7b97\u6cd5\uff0c\u5305\u542b\u96be\u5ea6\u81ea\u9002\u5e94\u6b63\u56de\u653e\u548c\u5931\u8d25\u8bfe\u7a0b\u8fc7\u6ee4\uff0c\u4ee5\u53ca\u6700\u77ed\u8def\u5f84\u5956\u52b1\u8c03\u6574\u7b56\u7565\u3002", "result": "MOBILERL-9B\u6a21\u578b\u5728AndroidWorld\u4e0a\u8fbe\u523075.8%\u7684\u6210\u529f\u7387\uff0c\u5728AndroidLab\u4e0a\u8fbe\u523046.8%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u7a33\u5b9a\u4e86RL\u8bad\u7ec3\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5728\u591a\u6837\u5316\u79fb\u52a8\u5e94\u7528\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5df2\u88abAutoGLM\u4ea7\u54c1\u91c7\u7528\u5e76\u5f00\u6e90\u3002"}}
{"id": "2509.18120", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.18120", "abs": "https://arxiv.org/abs/2509.18120", "authors": ["Thanh Linh Nguyen", "Quoc-Viet Pham"], "title": "A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning", "comment": "Accepted in IEEE GLOBECOM 2025", "summary": "Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or\nbanks) to collaboratively train artificial intelligence (AI) models while\npreserving data privacy by keeping data local. While prior work has primarily\naddressed statistical heterogeneity across organizations, a critical challenge\narises from economic competition, where organizations may act as market rivals,\nmaking them hesitant to participate in joint training due to potential utility\nloss (i.e., reduced net benefit). Furthermore, the combined effects of\nstatistical heterogeneity and inter-organizational competition on\norganizational behavior and system-wide social welfare remain underexplored. In\nthis paper, we propose CoCoGen, a coopetitive-compatible data generation\nframework, leveraging generative AI (GenAI) and potential game theory to model,\nanalyze, and optimize collaborative learning under heterogeneous and\ncompetitive settings. Specifically, CoCoGen characterizes competition and\nstatistical heterogeneity through learning performance and utility-based\nformulations and models each training round as a weighted potential game. We\nthen derive GenAI-based data generation strategies that maximize social\nwelfare. Experimental results on the Fashion-MNIST dataset reveal how varying\nheterogeneity and competition levels affect organizational behavior and\ndemonstrate that CoCoGen consistently outperforms baseline methods.", "AI": {"tldr": "CoCoGen\u662f\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210AI\u548c\u52bf\u535a\u5f08\u7406\u8bba\u7684\u5408\u4f5c\u7ade\u4e89\u517c\u5bb9\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8de8\u7ec4\u7ec7\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\u548c\u7ecf\u6d4e\u7ade\u4e89\u95ee\u9898\u3002", "motivation": "\u8de8\u7ec4\u7ec7\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u7ec4\u7ec7\u95f4\u5b58\u5728\u7ecf\u6d4e\u7ade\u4e89\u5173\u7cfb\uff0c\u5bfc\u81f4\u53c2\u4e0e\u8054\u5408\u8bad\u7ec3\u7684\u610f\u613f\u964d\u4f4e\uff0c\u540c\u65f6\u7edf\u8ba1\u5f02\u8d28\u6027\u548c\u7ade\u4e89\u5173\u7cfb\u7684\u7efc\u5408\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "CoCoGen\u901a\u8fc7\u5b66\u4e60\u548c\u6027\u80fd\u6548\u7528\u516c\u5f0f\u523b\u753b\u7ade\u4e89\u548c\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u5c06\u6bcf\u8f6e\u8bad\u7ec3\u5efa\u6a21\u4e3a\u52a0\u6743\u52bf\u535a\u5f08\uff0c\u5e76\u57fa\u4e8e\u751f\u6210AI\u63a8\u5bfc\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u7684\u6570\u636e\u751f\u6210\u7b56\u7565\u3002", "result": "\u5728Fashion-MNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoCoGen\u5728\u4e0d\u540c\u5f02\u8d28\u6027\u548c\u7ade\u4e89\u6c34\u5e73\u4e0b\u90fd\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u4f18\u5316\u7ec4\u7ec7\u884c\u4e3a\u548c\u793e\u4f1a\u798f\u5229\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f02\u6784\u7ade\u4e89\u73af\u5883\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5efa\u6a21\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u751f\u6210AI\u548c\u535a\u5f08\u7406\u8bba\u5728\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2509.18124", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.18124", "abs": "https://arxiv.org/abs/2509.18124", "authors": ["Edmund Agyemang", "Lawrence Agbota", "Vincent Agbenyeavu", "Peggy Akabuah", "Bismark Bimpong", "Christopher Attafuah"], "title": "Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters", "comment": "13 pages, 6 figures and 4 tables", "summary": "This study explores the application of supervised machine learning algorithms\nto predict coffee ratings based on a combination of influential textual and\nnumerical attributes extracted from user reviews. Through careful data\npreprocessing including text cleaning, feature extraction using TF-IDF, and\nselection with SelectKBest, the study identifies key factors contributing to\ncoffee quality assessments. Six models (Decision Tree, KNearest Neighbors,\nMulti-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained\nand evaluated using optimized hyperparameters. Model performance was assessed\nprimarily using F1-score, Gmean, and AUC metrics. Results demonstrate that\nensemble methods (Extra Trees, Random Forest, and XGBoost), as well as\nMulti-layer Perceptron, consistently outperform simpler classifiers (Decision\nTrees and K-Nearest Neighbors) in terms of evaluation metrics such as F1\nscores, G-mean and AUC. The findings highlight the essence of rigorous feature\nselection and hyperparameter tuning in building robust predictive systems for\nsensory product evaluation, offering a data driven approach to complement\ntraditional coffee cupping by expertise of trained professionals.", "AI": {"tldr": "\u672c\u7814\u7a76\u5e94\u7528\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u57fa\u4e8e\u7528\u6237\u8bc4\u8bba\u4e2d\u7684\u6587\u672c\u548c\u6570\u503c\u5c5e\u6027\u9884\u6d4b\u5496\u5561\u8bc4\u5206\uff0c\u53d1\u73b0\u96c6\u6210\u65b9\u6cd5\uff08Extra Trees\u3001Random Forest\u3001XGBoost\uff09\u548c\u591a\u5c42\u611f\u77e5\u5668\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u7b80\u5355\u5206\u7c7b\u5668\u3002", "motivation": "\u63a2\u7d22\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6765\u8865\u5145\u4f20\u7edf\u5496\u5561\u54c1\u9274\u7684\u4e13\u4e1a\u8bc4\u4f30\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u5496\u5561\u8d28\u91cf\u8bc4\u5206\u3002", "method": "\u4f7f\u7528TF-IDF\u8fdb\u884c\u6587\u672c\u7279\u5f81\u63d0\u53d6\uff0cSelectKBest\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u8bad\u7ec3\u516d\u79cd\u6a21\u578b\uff08\u51b3\u7b56\u6811\u3001K\u8fd1\u90bb\u3001\u591a\u5c42\u611f\u77e5\u5668\u3001\u968f\u673a\u68ee\u6797\u3001Extra Trees\u3001XGBoost\uff09\u5e76\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u96c6\u6210\u65b9\u6cd5\u548c\u591a\u5c42\u611f\u77e5\u5668\u5728F1\u5206\u6570\u3001G-mean\u548cAUC\u7b49\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u7b80\u5355\u5206\u7c7b\u5668\u3002", "conclusion": "\u4e25\u683c\u7684\u7279\u5f81\u9009\u62e9\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u5bf9\u4e8e\u6784\u5efa\u7a33\u5065\u7684\u611f\u5b98\u4ea7\u54c1\u8bc4\u4f30\u9884\u6d4b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u4f20\u7edf\u4e13\u4e1a\u5496\u5561\u54c1\u9274\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u8865\u5145\u65b9\u6cd5\u3002"}}
{"id": "2509.18125", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18125", "abs": "https://arxiv.org/abs/2509.18125", "authors": ["Harsha Koduri"], "title": "NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment", "comment": null, "summary": "Healthcare systems face increasing pressure to allocate limited nursing\nresources efficiently while accounting for skill heterogeneity, patient acuity,\nstaff fatigue, and continuity of care. Traditional optimization and heuristic\nscheduling methods struggle to capture these dynamic, multi-constraint\nenvironments. I propose NurseSchedRL, a reinforcement learning framework for\nnurse-patient assignment that integrates structured state encoding, constrained\naction masking, and attention-based representations of skills, fatigue, and\ngeographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with\nfeasibility masks to ensure assignments respect real-world constraints, while\ndynamically adapting to patient arrivals and varying nurse availability. In\nsimulation with realistic nurse and patient data, NurseSchedRL achieves\nimproved scheduling efficiency, better alignment of skills to patient needs,\nand reduced fatigue compared to baseline heuristic and unconstrained RL\napproaches. These results highlight the potential of reinforcement learning for\ndecision support in complex, high-stakes healthcare workforce management.", "AI": {"tldr": "NurseSchedRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u62a4\u58eb-\u60a3\u8005\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u72b6\u6001\u7f16\u7801\u3001\u7ea6\u675f\u52a8\u4f5c\u63a9\u7801\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u8003\u8651\u6280\u80fd\u5f02\u8d28\u6027\u3001\u60a3\u8005\u75c5\u60c5\u3001\u4eba\u5458\u75b2\u52b3\u548c\u62a4\u7406\u8fde\u7eed\u6027\u7b49\u591a\u91cd\u7ea6\u675f\u4e0b\uff0c\u4f18\u5316\u62a4\u58eb\u6392\u73ed\u6548\u7387\u3002", "motivation": "\u533b\u7597\u7cfb\u7edf\u9762\u4e34\u6709\u9650\u62a4\u7406\u8d44\u6e90\u7684\u9ad8\u6548\u5206\u914d\u538b\u529b\uff0c\u4f20\u7edf\u4f18\u5316\u548c\u542f\u53d1\u5f0f\u6392\u73ed\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u52a8\u6001\u591a\u7ea6\u675f\u73af\u5883\u3002", "method": "\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u53ef\u884c\u6027\u63a9\u7801\u786e\u4fdd\u5206\u914d\u7b26\u5408\u73b0\u5b9e\u7ea6\u675f\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8868\u793a\u6280\u80fd\u3001\u75b2\u52b3\u548c\u5730\u7406\u4e0a\u4e0b\u6587\uff0c\u52a8\u6001\u9002\u5e94\u60a3\u8005\u5230\u8fbe\u548c\u62a4\u58eb\u53ef\u7528\u6027\u53d8\u5316\u3002", "result": "\u5728\u771f\u5b9e\u62a4\u58eb\u548c\u60a3\u8005\u6570\u636e\u7684\u6a21\u62df\u4e2d\uff0cNurseSchedRL\u76f8\u6bd4\u57fa\u7ebf\u542f\u53d1\u5f0f\u548c\u65e0\u7ea6\u675fRL\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6392\u73ed\u6548\u7387\u3001\u66f4\u597d\u7684\u6280\u80fd\u4e0e\u60a3\u8005\u9700\u6c42\u5339\u914d\u5ea6\u4ee5\u53ca\u66f4\u4f4e\u7684\u75b2\u52b3\u5ea6\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u9ad8\u98ce\u9669\u533b\u7597\u4eba\u529b\u7ba1\u7406\u51b3\u7b56\u652f\u6301\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.18126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18126", "abs": "https://arxiv.org/abs/2509.18126", "authors": ["Bishal K C", "Amr Hilal", "Pawan Thapa"], "title": "Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning", "comment": null, "summary": "Federated Learning (FL) is a decentralized training framework widely used in\nIoT ecosystems that preserves privacy by keeping raw data local, making it\nideal for IoT-enabled cyber-physical systems with sensing and communication\nlike Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric\nVehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle\ninfrastructure, securing these IoT-based charging stations against cyber\nthreats has become critical. Centralized Intrusion Detection Systems (IDS)\nraise privacy concerns due to sensitive network and user data, making FL a\npromising alternative. However, current FL-based IDS evaluations overlook\npractical challenges such as system heterogeneity and non-IID data. To address\nthese challenges, we conducted experiments to evaluate the performance of\nfederated learning for anomaly detection in EV charging stations under system\nand data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization\napproaches, to analyze their effectiveness in anomaly detection. Under IID\nsettings, FedAvg achieves superior performance to centralized models using the\nsame neural network. However, performance degrades with non-IID data and system\nheterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous\nsettings, showing better convergence and higher anomaly detection accuracy. Our\nresults demonstrate that FL can handle heterogeneity in IoT-based EVCS without\nsignificant performance loss, with FedAvgM as a promising solution for robust,\nprivacy-preserving EVCS security.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7cfb\u7edf\u5f02\u6784\u6027\u548c\u975eIID\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0FedAvgM\u5728\u5f02\u6784\u73af\u5883\u4e0b\u4f18\u4e8eFedAvg\uff0c\u80fd\u591f\u5728\u4e0d\u663e\u8457\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684EVCS\u5b89\u5168\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u57fa\u7840\u8bbe\u65bd\u7684\u5feb\u901f\u6269\u5f20\uff0c\u4fdd\u62a4\u57fa\u4e8e\u7269\u8054\u7f51\u7684\u5145\u7535\u7ad9\u514d\u53d7\u7f51\u7edc\u5a01\u80c1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u96c6\u4e2d\u5f0f\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u56e0\u6d89\u53ca\u654f\u611f\u7f51\u7edc\u548c\u7528\u6237\u6570\u636e\u800c\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff0c\u8054\u90a6\u5b66\u4e60\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5f53\u524d\u57fa\u4e8eFL\u7684IDS\u8bc4\u4f30\u5ffd\u89c6\u4e86\u7cfb\u7edf\u5f02\u6784\u6027\u548c\u975eIID\u6570\u636e\u7b49\u5b9e\u9645\u6311\u6218\u3002", "method": "\u4f7f\u7528FedAvg\u548cFedAvgM\u8fd9\u4e24\u79cd\u5e7f\u6cdb\u7814\u7a76\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u7cfb\u7edf\u5f02\u6784\u6027\u548c\u6570\u636e\u5f02\u6784\u6027\u6761\u4ef6\u4e0b\u8bc4\u4f30\u8054\u90a6\u5b66\u4e60\u5728EV\u5145\u7535\u7ad9\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8003\u8651\u4e86IID\u548c\u975eIID\u6570\u636e\u8bbe\u7f6e\u3002", "result": "\u5728IID\u8bbe\u7f6e\u4e0b\uff0cFedAvg\u4f7f\u7528\u76f8\u540c\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u4e86\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u6a21\u578b\u7684\u6027\u80fd\u3002\u4f46\u5728\u975eIID\u6570\u636e\u548c\u7cfb\u7edf\u5f02\u6784\u6027\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\u3002FedAvgM\u5728\u5f02\u6784\u8bbe\u7f6e\u4e2d\u59cb\u7ec8\u4f18\u4e8eFedAvg\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6536\u655b\u6027\u548c\u66f4\u9ad8\u7684\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u80fd\u591f\u5904\u7406\u7269\u8054\u7f51EVCS\u4e2d\u7684\u5f02\u6784\u6027\u800c\u4e0d\u4f1a\u663e\u8457\u635f\u5931\u6027\u80fd\uff0cFedAvgM\u662f\u6784\u5efa\u9c81\u68d2\u3001\u9690\u79c1\u4fdd\u62a4\u7684EVCS\u5b89\u5168\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18127", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18127", "abs": "https://arxiv.org/abs/2509.18127", "authors": ["Jiaqi Weng", "Han Zheng", "Hanyu Zhang", "Qinqin He", "Jialing Tao", "Hui Xue", "Zhixuan Chu", "Xiting Wang"], "title": "Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework", "comment": null, "summary": "Increasing deployment of large language models (LLMs) in real-world\napplications raises significant safety concerns. Most existing safety research\nfocuses on evaluating LLM outputs or specific safety tasks, limiting their\nability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)\nfacilitate interpretability research to clarify model behavior by explaining\nsingle-meaning atomic features decomposed from entangled signals. jHowever,\nprior applications on SAEs do not interpret features with fine-grained\nsafety-related con- cepts, thus inadequately addressing safety-critical\nbehaviors, such as generating toxic responses and violating safety regu-\nlations. For rigorous safety analysis, we must extract a rich and diverse set\nof safety-relevant features that effectively capture these high-risk behaviors,\nyet face two challenges: identifying SAEs with the greatest potential for\ngenerating safety concept-specific neurons, and the prohibitively high cost of\ndetailed feature explanation. In this paper, we pro- pose Safe-SAIL, a\nframework for interpreting SAE features within LLMs to advance mechanistic\nunderstanding in safety domains. Our approach systematically identifies SAE\nwith best concept-specific interpretability, explains safety-related neurons,\nand introduces efficient strategies to scale up the in- terpretation process.\nWe will release a comprehensive toolkit including SAE checkpoints and\nhuman-readable neuron ex- planations, which supports empirical analysis of\nsafety risks to promote research on LLM safety.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSafe-SAIL\u6846\u67b6\uff0c\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAEs)\u6765\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5b89\u5168\u76f8\u5173\u7279\u5f81\uff0c\u4ee5\u63d0\u5347\u5bf9\u6a21\u578b\u5b89\u5168\u884c\u4e3a\u7684\u673a\u5236\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bc4\u4f30LLM\u8f93\u51fa\u6216\u7279\u5b9a\u5b89\u5168\u4efb\u52a1\uff0c\u65e0\u6cd5\u5e94\u5bf9\u66f4\u5e7f\u6cdb\u7684\u672a\u5b9a\u4e49\u98ce\u9669\u3002SAEs\u867d\u7136\u6709\u52a9\u4e8e\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u672a\u80fd\u5c06\u7279\u5f81\u4e0e\u7ec6\u7c92\u5ea6\u7684\u5b89\u5168\u6982\u5ff5\u5173\u8054\uff0c\u65e0\u6cd5\u5145\u5206\u89e3\u51b3\u5b89\u5168\u5173\u952e\u884c\u4e3a\u3002", "method": "\u63d0\u51faSafe-SAIL\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc6\u522b\u5177\u6709\u6700\u4f73\u6982\u5ff5\u7279\u5b9a\u53ef\u89e3\u91ca\u6027\u7684SAE\uff0c\u89e3\u91ca\u5b89\u5168\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u5e76\u5f15\u5165\u9ad8\u6548\u7b56\u7565\u6765\u6269\u5c55\u89e3\u91ca\u8fc7\u7a0b\u3002", "result": "\u5c06\u53d1\u5e03\u5305\u542bSAE\u68c0\u67e5\u70b9\u548c\u4eba\u7c7b\u53ef\u8bfb\u795e\u7ecf\u5143\u89e3\u91ca\u7684\u5b8c\u6574\u5de5\u5177\u5305\uff0c\u652f\u6301\u5bf9\u5b89\u5168\u98ce\u9669\u7684\u5b9e\u8bc1\u5206\u6790\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u4fc3\u8fdbLLM\u5b89\u5168\u7814\u7a76\uff0c\u901a\u8fc7\u63d0\u53d6\u4e30\u5bcc\u591a\u6837\u7684\u5b89\u5168\u76f8\u5173\u7279\u5f81\u6765\u6709\u6548\u6355\u6349\u9ad8\u98ce\u9669\u884c\u4e3a\u3002"}}
{"id": "2509.18128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18128", "abs": "https://arxiv.org/abs/2509.18128", "authors": ["Amirreza Tootchi", "Xiaoping Du"], "title": "Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis", "comment": null, "summary": "Machine learning surrogates are increasingly employed to replace expensive\ncomputational models for physics-based reliability analysis. However, their use\nintroduces epistemic uncertainty from model approximation errors, which couples\nwith aleatory uncertainty in model inputs, potentially compromising the\naccuracy of reliability predictions. This study proposes a Gauss-Hermite\nquadrature approach to decouple these nested uncertainties and enable more\naccurate reliability analysis. The method evaluates conditional failure\nprobabilities under aleatory uncertainty using First and Second Order\nReliability Methods and then integrates these probabilities across realizations\nof epistemic uncertainty. Three examples demonstrate that the proposed approach\nmaintains computational efficiency while yielding more trustworthy predictions\nthan traditional methods that ignore model uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eGauss-Hermite\u6c42\u79ef\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u8026\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u4e2d\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u53ef\u9760\u6027\u5206\u6790\u7684\u51c6\u786e\u6027", "motivation": "\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u5728\u7269\u7406\u53ef\u9760\u6027\u5206\u6790\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u6a21\u578b\u8fd1\u4f3c\u8bef\u5dee\u5f15\u5165\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4f1a\u4e0e\u6a21\u578b\u8f93\u5165\u7684\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u8026\u5408\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027", "method": "\u4f7f\u7528Gauss-Hermite\u6c42\u79ef\u65b9\u6cd5\u89e3\u8026\u5d4c\u5957\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u4e00\u9636\u548c\u4e8c\u9636\u53ef\u9760\u6027\u65b9\u6cd5\u8bc4\u4f30\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6761\u4ef6\u5931\u6548\u6982\u7387\uff0c\u5e76\u5728\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5b9e\u73b0\u4e0a\u8fdb\u884c\u79ef\u5206", "result": "\u4e09\u4e2a\u793a\u4f8b\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u6bd4\u5ffd\u7565\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u4f20\u7edf\u65b9\u6cd5\u4ea7\u751f\u66f4\u53ef\u4fe1\u7684\u9884\u6d4b\u7ed3\u679c", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u8026\u5408\u95ee\u9898\uff0c\u4e3a\u53ef\u9760\u6027\u5206\u6790\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u9884\u6d4b"}}
{"id": "2509.18130", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18130", "abs": "https://arxiv.org/abs/2509.18130", "authors": ["Zijie Zhou", "Huichen Ma"], "title": "Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model", "comment": null, "summary": "In the metro intelligent transportation system, accurate transfer passenger\nflow prediction is a key link in optimizing operation plans and improving\ntransportation efficiency. To further improve the theory of metro internal\ntransfer passenger flow prediction and provide more reliable support for\nintelligent operation decisions, this paper innovatively proposes a metro\ntransfer passenger flow prediction model that integrates the Seasonal and Trend\ndecomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In\npractical application, the model first relies on the deep learning library\nKeras to complete the construction and training of the GRU model, laying the\nfoundation for subsequent prediction; then preprocesses the original metro card\nswiping data, uses the graph-based depth-first search algorithm to identify\npassengers' travel paths, and further constructs the transfer passenger flow\ntime series; subsequently adopts the STL time series decomposition algorithm to\ndecompose the constructed transfer passenger flow time series into trend\ncomponent, periodic component and residual component, and uses the 3{\\sigma}\nprinciple to eliminate and fill the outliers in the residual component, and\nfinally completes the transfer passenger flow prediction.Taking the transfer\npassenger flow data of a certain metro station as the research sample, the\nvalidity of the model is verified. The results show that compared with Long\nShort-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of\nSTL time series decomposition method and Long Short-Term Memory (STL-LSTM), the\nSTL-GRU combined prediction model significantly improves the prediction\naccuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays\nand rest days, with the mean absolute percentage error (MAPE) of the prediction\nresults reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408STL\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u548cGRU\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u5730\u94c1\u6362\u4e58\u5ba2\u6d41\u91cf\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5728\u5730\u94c1\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\uff0c\u51c6\u786e\u7684\u6362\u4e58\u5ba2\u6d41\u91cf\u9884\u6d4b\u662f\u4f18\u5316\u8fd0\u8425\u8ba1\u5212\u3001\u63d0\u9ad8\u8fd0\u8f93\u6548\u7387\u7684\u5173\u952e\u73af\u8282\u3002\u9700\u8981\u6539\u8fdb\u73b0\u6709\u9884\u6d4b\u7406\u8bba\uff0c\u4e3a\u667a\u80fd\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u652f\u6301\u3002", "method": "\u521b\u65b0\u6027\u5730\u63d0\u51faSTL-GRU\u7ec4\u5408\u9884\u6d4b\u6a21\u578b\uff1a\u9996\u5148\u4f7f\u7528Keras\u6784\u5efaGRU\u6a21\u578b\uff1b\u9884\u5904\u7406\u5730\u94c1\u5237\u5361\u6570\u636e\uff0c\u57fa\u4e8e\u56fe\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7b97\u6cd5\u8bc6\u522b\u4e58\u5ba2\u51fa\u884c\u8def\u5f84\uff1b\u91c7\u7528STL\u7b97\u6cd5\u5c06\u6362\u4e58\u5ba2\u6d41\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u8d8b\u52bf\u3001\u5468\u671f\u548c\u6b8b\u5dee\u5206\u91cf\uff1b\u4f7f\u75283\u03c3\u539f\u5219\u5904\u7406\u5f02\u5e38\u503c\uff1b\u6700\u540e\u5b8c\u6210\u9884\u6d4b\u3002", "result": "\u4ee5\u67d0\u5730\u94c1\u7ad9\u6362\u4e58\u5ba2\u6d41\u6570\u636e\u4e3a\u6837\u672c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793aSTL-GRU\u6a21\u578b\u5728\u5de5\u4f5c\u65e5\uff08\u9664\u5468\u4e94\uff09\u3001\u5468\u4e94\u548c\u4f11\u606f\u65e5\u7684\u9884\u6d4b\u7cbe\u5ea6\u5747\u663e\u8457\u4f18\u4e8eLSTM\u3001GRU\u548cSTL-LSTM\u6a21\u578b\uff0cMAPE\u5206\u522b\u964d\u4f4e\u4e86\u81f3\u5c112.3\u30011.36\u548c6.42\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "STL-GRU\u7ec4\u5408\u9884\u6d4b\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5730\u94c1\u6362\u4e58\u5ba2\u6d41\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5730\u94c1\u667a\u80fd\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.18131", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18131", "abs": "https://arxiv.org/abs/2509.18131", "authors": ["Jean-Michel Tucny", "Abhisek Ganguly", "Santosh Ansumali", "Sauro Succi"], "title": "Two ways to knowledge?", "comment": null, "summary": "It is shown that the weight matrices of transformer-based machine learning\napplications to the solution of two representative physical applications show a\nrandom-like character which bears no directly recognizable link to the physical\nand mathematical structure of the physical problem under study. This suggests\nthat machine learning and the scientific method may represent two distinct and\npotentially complementary paths to knowledge, even though a strict notion of\nexplainability in terms of direct correspondence between network parameters and\nphysical structures may remain out of reach. It is also observed that drawing a\nparallel between transformer operation and (generalized) path-integration\ntechniques may account for the random-like nature of the weights, but still\ndoes not resolve the tension with explainability. We conclude with some general\ncomments on the hazards of gleaning knowledge without the benefit of Insight.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eTransformer\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u5728\u89e3\u51b3\u7269\u7406\u95ee\u9898\u65f6\uff0c\u5176\u6743\u91cd\u77e9\u9635\u5448\u73b0\u968f\u673a\u7279\u5f81\uff0c\u4e0e\u7269\u7406\u95ee\u9898\u7684\u6570\u5b66\u7ed3\u6784\u6ca1\u6709\u76f4\u63a5\u53ef\u8bc6\u522b\u8054\u7cfb\uff0c\u8fd9\u8868\u660e\u673a\u5668\u5b66\u4e60\u4e0e\u79d1\u5b66\u65b9\u6cd5\u53ef\u80fd\u662f\u4e24\u79cd\u4e0d\u540c\u4f46\u4e92\u8865\u7684\u77e5\u8bc6\u83b7\u53d6\u8def\u5f84\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662fTransformer\uff09\u5728\u89e3\u51b3\u7269\u7406\u95ee\u9898\u65f6\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u80fd\u591f\u63d0\u4f9b\u4e0e\u79d1\u5b66\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u53ef\u89e3\u91ca\u6027\u77e5\u8bc6\u3002", "method": "\u5206\u6790Transformer\u6a21\u578b\u5728\u89e3\u51b3\u4e24\u4e2a\u4ee3\u8868\u6027\u7269\u7406\u5e94\u7528\u65f6\u7684\u6743\u91cd\u77e9\u9635\u7279\u5f81\uff0c\u5e76\u4e0e\u7269\u7406\u95ee\u9898\u7684\u6570\u5b66\u7ed3\u6784\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u53d1\u73b0\u6743\u91cd\u77e9\u9635\u5177\u6709\u968f\u673a\u6027\u7279\u5f81\uff0c\u4e0e\u7269\u7406\u95ee\u9898\u7684\u7ed3\u6784\u548c\u6570\u5b66\u539f\u7406\u6ca1\u6709\u76f4\u63a5\u5bf9\u5e94\u5173\u7cfb\uff0c\u5c3d\u7ba1Transformer\u7684\u64cd\u4f5c\u53ef\u80fd\u4e0e\u5e7f\u4e49\u8def\u5f84\u79ef\u5206\u6280\u672f\u5b58\u5728\u76f8\u4f3c\u6027\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u548c\u79d1\u5b66\u65b9\u6cd5\u53ef\u80fd\u662f\u4e92\u8865\u7684\u77e5\u8bc6\u83b7\u53d6\u9014\u5f84\uff0c\u4f46\u7eaf\u7cb9\u7684\u53c2\u6570\u53ef\u89e3\u91ca\u6027\u53ef\u80fd\u96be\u4ee5\u5b9e\u73b0\uff0c\u9700\u8981\u8b66\u60d5\u7f3a\u4e4f\u6d1e\u5bdf\u529b\u7684\u77e5\u8bc6\u83b7\u53d6\u98ce\u9669\u3002"}}
{"id": "2509.18133", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18133", "abs": "https://arxiv.org/abs/2509.18133", "authors": ["Le Huang", "Jiazheng Kang", "Cheng Hou", "Zhe Zhao", "Zhenxiang Yan", "Chuan Shi", "Ting Bai"], "title": "Self-Evolving LLMs via Continual Instruction Tuning", "comment": null, "summary": "In real-world industrial settings, large language models (LLMs) must learn\ncontinually to keep pace with diverse and evolving tasks, requiring\nself-evolution to refine knowledge under dynamic data distributions. However,\nexisting continual learning (CL) approaches, such as replay and parameter\nisolation, often suffer from catastrophic forgetting: training on new tasks\ndegrades performance on earlier ones by overfitting to the new distribution and\nweakening generalization.We propose MoE-CL, a parameter-efficient adversarial\nmixture-of-experts framework for industrial-scale, self-evolving continual\ninstruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated\nLoRA expert per task to preserve task-specific knowledge via parameter\nindependence, mitigating forgetting; and (2) a shared LoRA expert to enable\ncross-task transfer. To prevent transferring task-irrelevant noise through the\nshared pathway, we integrate a task-aware discriminator within a GAN. The\ndiscriminator encourages the shared expert to pass only task-aligned\ninformation during sequential training. Through adversarial learning, the\nshared expert acquires generalized representations that mimic the\ndiscriminator, while dedicated experts retain task-specific details, balancing\nknowledge retention and cross-task generalization and thereby supporting\nself-evolution.Extensive experiments on the public MTL5 benchmark and an\nindustrial Tencent3 benchmark validate the effectiveness of MoE-CL for\ncontinual instruction tuning. In real-world A/B testing for content compliance\nreview on the Tencent Video platform, MoE-CL reduced manual review costs by\n15.3%. These results demonstrate that MoE-CL is practical for large-scale\nindustrial deployment where continual adaptation and stable transfer are\ncritical.", "AI": {"tldr": "MoE-CL\u662f\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u5bf9\u6297\u6027\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u4e1a\u89c4\u6a21\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\uff0c\u901a\u8fc7\u53cc\u4e13\u5bb6\u8bbe\u8ba1\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6301\u7eed\u5b66\u4e60\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5373\u8bad\u7ec3\u65b0\u4efb\u52a1\u4f1a\u964d\u4f4e\u5bf9\u5148\u524d\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u53ccLoRA\u4e13\u5bb6\u8bbe\u8ba1\uff1a\u4e13\u7528\u4e13\u5bb6\u4fdd\u7559\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\uff0c\u5171\u4eab\u4e13\u5bb6\u5b9e\u73b0\u8de8\u4efb\u52a1\u8fc1\u79fb\uff1b\u96c6\u6210\u4efb\u52a1\u611f\u77e5\u5224\u522b\u5668\u8fdb\u884c\u5bf9\u6297\u5b66\u4e60\uff0c\u786e\u4fdd\u5171\u4eab\u4e13\u5bb6\u53ea\u4f20\u9012\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002", "result": "\u5728MTL5\u57fa\u51c6\u548c\u817e\u8baf3\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5728\u817e\u8baf\u89c6\u9891\u5e73\u53f0\u7684\u5185\u5bb9\u5408\u89c4\u5ba1\u67e5A/B\u6d4b\u8bd5\u4e2d\uff0c\u624b\u52a8\u5ba1\u6838\u6210\u672c\u964d\u4f4e\u4e8615.3%\u3002", "conclusion": "MoE-CL\u9002\u7528\u4e8e\u9700\u8981\u6301\u7eed\u9002\u5e94\u548c\u7a33\u5b9a\u8fc1\u79fb\u7684\u5927\u89c4\u6a21\u5de5\u4e1a\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2509.18134", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.18134", "abs": "https://arxiv.org/abs/2509.18134", "authors": ["Furan Xie", "Bing Liu", "Li Chai"], "title": "A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization", "comment": null, "summary": "This paper investigates the privacy-preserving distributed optimization\nproblem, aiming to protect agents' private information from potential attackers\nduring the optimization process. Gradient tracking, an advanced technique for\nimproving the convergence rate in distributed optimization, has been applied to\nmost first-order algorithms in recent years. We first reveal the inherent\nprivacy leakage risk associated with gradient tracking. Building upon this\ninsight, we propose a weighted gradient tracking distributed privacy-preserving\nalgorithm, eliminating the privacy leakage risk in gradient tracking using\ndecaying weight factors. Then, we characterize the convergence of the proposed\nalgorithm under time-varying heterogeneous step sizes. We prove the proposed\nalgorithm converges precisely to the optimal solution under mild assumptions.\nFinally, numerical simulations validate the algorithm's effectiveness through a\nclassical distributed estimation problem and the distributed training of a\nconvolutional neural network.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u6743\u68af\u5ea6\u8ddf\u8e2a\u7684\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u68af\u5ea6\u8ddf\u8e2a\u6280\u672f\u4e2d\u5b58\u5728\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u901a\u8fc7\u8870\u51cf\u6743\u91cd\u56e0\u5b50\u6d88\u9664\u9690\u79c1\u6cc4\u9732\uff0c\u5e76\u5728\u65f6\u53d8\u5f02\u6784\u6b65\u957f\u4e0b\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u7cbe\u786e\u6536\u655b\u6027\u3002", "motivation": "\u68af\u5ea6\u8ddf\u8e2a\u6280\u672f\u867d\u7136\u80fd\u63d0\u9ad8\u5206\u5e03\u5f0f\u4f18\u5316\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4f46\u5b58\u5728\u56fa\u6709\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u68af\u5ea6\u4fe1\u606f\u83b7\u53d6\u4ee3\u7406\u7684\u79c1\u6709\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9690\u79c1\u5b89\u5168\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u52a0\u6743\u68af\u5ea6\u8ddf\u8e2a\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u4f7f\u7528\u8870\u51cf\u6743\u91cd\u56e0\u5b50\u6765\u6d88\u9664\u68af\u5ea6\u8ddf\u8e2a\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u5728\u65f6\u53d8\u5f02\u6784\u6b65\u957f\u6761\u4ef6\u4e0b\u5206\u6790\u7b97\u6cd5\u6536\u655b\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8be5\u7b97\u6cd5\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u80fd\u7cbe\u786e\u6536\u655b\u5230\u6700\u4f18\u89e3\uff0c\u6570\u503c\u6a21\u62df\u901a\u8fc7\u5206\u5e03\u5f0f\u4f30\u8ba1\u95ee\u9898\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a0\u6743\u68af\u5ea6\u8ddf\u8e2a\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u68af\u5ea6\u8ddf\u8e2a\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18135", "abs": "https://arxiv.org/abs/2509.18135", "authors": ["Shaoxun Wang", "Xingjun Zhang", "Qianyang Li", "Jiawei Cao", "Zhendong Tan"], "title": "SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting", "comment": null, "summary": "Inter-series correlations are crucial for accurate multivariate time series\nforecasting, yet these relationships often exhibit complex dynamics across\ndifferent temporal scales. Existing methods are limited in modeling these\nmulti-scale dependencies and struggle to capture their intricate and evolving\nnature. To address this challenge, this paper proposes a novel Static-Dynamic\nGraph Fusion network (SDGF), whose core lies in capturing multi-scale\ninter-series correlations through a dual-path graph structure learning\napproach. Specifically, the model utilizes a static graph based on prior\nknowledge to anchor long-term, stable dependencies, while concurrently\nemploying Multi-level Wavelet Decomposition to extract multi-scale features for\nconstructing an adaptively learned dynamic graph to capture associations at\ndifferent scales. We design an attention-gated module to fuse these two\ncomplementary sources of information intelligently, and a multi-kernel dilated\nconvolutional network is then used to deepen the understanding of temporal\npatterns. Comprehensive experiments on multiple widely used real-world\nbenchmark datasets demonstrate the effectiveness of our proposed model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9759\u6001-\u52a8\u6001\u56fe\u878d\u5408\u7f51\u7edc(SDGF)\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u56fe\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\u6355\u6349\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u95f4\u76f8\u5173\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5efa\u6a21\u8de8\u65f6\u95f4\u5c3a\u5ea6\u7684\u590d\u6742\u52a8\u6001\u4f9d\u8d56\u5173\u7cfb\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u5e8f\u5217\u95f4\u7684\u76f8\u5173\u6027\u5bf9\u4e8e\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u4e9b\u5173\u7cfb\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8868\u73b0\u51fa\u590d\u6742\u7684\u52a8\u6001\u7279\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u591a\u5c3a\u5ea6\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u6355\u6349\u5176\u590d\u6742\u4e14\u6f14\u5316\u7684\u672c\u8d28\u3002", "method": "SDGF\u6a21\u578b\u91c7\u7528\u53cc\u8def\u5f84\u56fe\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\uff1a\u4f7f\u7528\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u7684\u9759\u6001\u56fe\u6765\u951a\u5b9a\u957f\u671f\u7a33\u5b9a\u4f9d\u8d56\u5173\u7cfb\uff0c\u540c\u65f6\u5229\u7528\u591a\u7ea7\u5c0f\u6ce2\u5206\u89e3\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u6784\u5efa\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u52a8\u6001\u56fe\u6765\u6355\u6349\u4e0d\u540c\u5c3a\u5ea6\u7684\u5173\u8054\u3002\u8bbe\u8ba1\u4e86\u6ce8\u610f\u529b\u95e8\u63a7\u6a21\u5757\u667a\u80fd\u878d\u5408\u8fd9\u4e24\u79cd\u4e92\u8865\u4fe1\u606f\u6e90\uff0c\u5e76\u4f7f\u7528\u591a\u6838\u6269\u5f20\u5377\u79ef\u7f51\u7edc\u52a0\u6df1\u5bf9\u65f6\u95f4\u6a21\u5f0f\u7684\u7406\u89e3\u3002", "result": "\u5728\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "SDGF\u6a21\u578b\u901a\u8fc7\u9759\u6001-\u52a8\u6001\u56fe\u878d\u5408\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u591a\u5c3a\u5ea6\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u7684\u6311\u6218\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u76f8\u5173\u6027\u7684\u6355\u6349\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18136", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18136", "abs": "https://arxiv.org/abs/2509.18136", "authors": ["Suqing Wang", "Zuchao Li", "Luohe Shi", "Bo Du", "Hai Zhao", "Yun Li", "Qianren Wang"], "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development", "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) have achieved remarkable success across various\ndomains, driving significant technological advancements and innovations.\nDespite the rapid growth in model scale and capability, systematic, data-driven\nresearch on how structural configurations affect performance remains scarce. To\naddress this gap, we present a large-scale dataset encompassing diverse\nopen-source LLM structures and their performance across multiple benchmarks.\nLeveraging this dataset, we conduct a systematic, data mining-driven analysis\nto validate and quantify the relationship between structural configurations and\nperformance. Our study begins with a review of the historical development of\nLLMs and an exploration of potential future trends. We then analyze how various\nstructural choices impact performance across benchmarks and further corroborate\nour findings using mechanistic interpretability techniques. By providing\ndata-driven insights into LLM optimization, our work aims to guide the targeted\ndevelopment and application of future models. We will release our dataset at\nhttps://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7ed3\u6784\u914d\u7f6e\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u6570\u636e\u6316\u6398\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u9a8c\u8bc1\u4e86\u7ed3\u6784\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u89c4\u6a21\u548c\u80fd\u529b\u4e0a\u5feb\u901f\u589e\u957f\uff0c\u4f46\u5173\u4e8e\u7ed3\u6784\u914d\u7f6e\u5982\u4f55\u5f71\u54cd\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u3001\u6570\u636e\u9a71\u52a8\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b\u591a\u6837\u5316\u5f00\u6e90LLM\u7ed3\u6784\u53ca\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7cfb\u7edf\u6027\u7684\u6570\u636e\u6316\u6398\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u9a8c\u8bc1\u53d1\u73b0\u3002", "result": "\u7814\u7a76\u91cf\u5316\u4e86\u7ed3\u6784\u914d\u7f6e\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u7ed3\u6784\u9009\u62e9\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u6280\u672f\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aLLM\u4f18\u5316\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u89c1\u89e3\uff0c\u65e8\u5728\u6307\u5bfc\u672a\u6765\u6a21\u578b\u7684\u9488\u5bf9\u6027\u5f00\u53d1\u548c\u5e94\u7528\uff0c\u76f8\u5173\u6570\u636e\u96c6\u5c06\u5728HuggingFace\u5e73\u53f0\u53d1\u5e03\u3002"}}
{"id": "2509.18137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18137", "abs": "https://arxiv.org/abs/2509.18137", "authors": ["Shaoheng Wang", "Yao Lu", "Yuqi Li", "Yaxin Gao", "Jiaqi Nie", "Shanqing Yu", "Yingli Tian", "Qi Xuan"], "title": "LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods", "comment": null, "summary": "As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation\n(LoRA) can save significant costs in storage and computing, but its strong\nadaptability to a single task is often accompanied by insufficient cross-task\ngeneralization capabilities. To improve this, existing work combines LoRA with\nmixture-of-experts (MoE) to enhance the model's adaptability through expert\nmodules and routing mechanisms. However, existing LoRA-MoE methods lack unified\nstandards in models, datasets, hyperparameters, and evaluation methods, making\nit difficult to conduct fair comparisons between different methods. To this\nend, we proposed a unified benchmark named LoRALib. Specifically, we\nstandardized datasets from $40$ downstream tasks into a unified format,\nfine-tuned them using the same hyperparameters and obtained $680$ LoRA modules\nacross $17$ model architectures. Based on this LoRA library, we conduct\nlarge-scale experiments on $3$ representative LoRA-MoE methods and different\nLoRA selection mechanisms using the open-sourced testing tool OpenCompass.\nExtensive experiments show that LoRAMoE performs best, and that prioritizing\nLoRAs relevant to the target task can further improve the performance of MoE.\nWe hope these findings will inspire future work. Our datasets and LoRA library\nare available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset\nand https://huggingface.co/YaoLuzjut/models.", "AI": {"tldr": "\u63d0\u51fa\u4e86LoRALib\u7edf\u4e00\u57fa\u51c6\uff0c\u6807\u51c6\u5316\u4e8640\u4e2a\u4e0b\u6e38\u4efb\u52a1\u7684\u6570\u636e\u96c6\u548c\u8d85\u53c2\u6570\uff0c\u751f\u6210\u4e86680\u4e2aLoRA\u6a21\u5757\uff0c\u5bf93\u79cd\u4ee3\u8868\u6027LoRA-MoE\u65b9\u6cd5\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u9a8c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709LoRA-MoE\u65b9\u6cd5\u5728\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u8d85\u53c2\u6570\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0a\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\uff0c\u96be\u4ee5\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u6784\u5efaLoRALib\u57fa\u51c6\uff0c\u6807\u51c6\u531640\u4e2a\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u76f8\u540c\u8d85\u53c2\u6570\u572817\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u751f\u6210680\u4e2aLoRA\u6a21\u5757\uff0c\u57fa\u4e8eOpenCompass\u6d4b\u8bd5\u5de5\u5177\u5bf93\u79cdLoRA-MoE\u65b9\u6cd5\u548c\u4e0d\u540cLoRA\u9009\u62e9\u673a\u5236\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLoRAMoE\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u5148\u9009\u62e9\u4e0e\u76ee\u6807\u4efb\u52a1\u76f8\u5173\u7684LoRA\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347MoE\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5c06\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u542f\u53d1\uff0c\u76f8\u5173\u6570\u636e\u96c6\u548cLoRA\u5e93\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.18138", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18138", "abs": "https://arxiv.org/abs/2509.18138", "authors": ["Tiantian Zhang"], "title": "Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts", "comment": null, "summary": "We introduce a new algorithm, \\emph{Rank-Induced Plackett--Luce Mirror\nDescent (RIPLM)}, which leverages the structural equivalence between the\n\\emph{rank benchmark} and the \\emph{distributional benchmark} established in\n\\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert\nidentities, RIPLM updates directly in the \\emph{rank-induced Plackett--Luce\n(PL)} parameterization. This ensures that the algorithm's played distributions\nremain within the class of rank-induced distributions at every round,\npreserving the equivalence with the rank benchmark. To our knowledge, RIPLM is\nthe first algorithm that is both (i) \\emph{rank-faithful} and (ii)\n\\emph{variance-adaptive} in the sleeping experts setting.", "AI": {"tldr": "RIPLM\u7b97\u6cd5\u901a\u8fc7\u5229\u7528\u6392\u540d\u57fa\u51c6\u548c\u5206\u5e03\u57fa\u51c6\u7684\u7ed3\u6784\u7b49\u4ef7\u6027\uff0c\u76f4\u63a5\u5728\u6392\u540d\u8bf1\u5bfc\u7684Plackett-Luce\u53c2\u6570\u5316\u4e2d\u66f4\u65b0\uff0c\u786e\u4fdd\u7b97\u6cd5\u5728\u6bcf\u4e00\u8f6e\u90fd\u4fdd\u6301\u5728\u6392\u540d\u8bf1\u5bfc\u5206\u5e03\u7c7b\u4e2d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e13\u5bb6\u8eab\u4efd\u4e0a\u64cd\u4f5c\uff0c\u65e0\u6cd5\u4fdd\u6301\u4e0e\u6392\u540d\u57fa\u51c6\u7684\u7b49\u4ef7\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u5fe0\u5b9e\u4e8e\u6392\u540d\u53c8\u80fd\u5728\u4f11\u7720\u4e13\u5bb6\u8bbe\u7f6e\u4e2d\u81ea\u9002\u5e94\u65b9\u5dee\u7684\u7b97\u6cd5\u3002", "method": "RIPLM\u7b97\u6cd5\u5728\u6392\u540d\u8bf1\u5bfc\u7684Plackett-Luce\u53c2\u6570\u5316\u4e2d\u76f4\u63a5\u66f4\u65b0\uff0c\u5229\u7528\u6392\u540d\u57fa\u51c6\u548c\u5206\u5e03\u57fa\u51c6\u7684\u7ed3\u6784\u7b49\u4ef7\u6027\u3002", "result": "RIPLM\u662f\u7b2c\u4e00\u4e2a\u5728\u4f11\u7720\u4e13\u5bb6\u8bbe\u7f6e\u4e2d\u65e2\u5fe0\u5b9e\u4e8e\u6392\u540d\u53c8\u81ea\u9002\u5e94\u65b9\u5dee\u7684\u7b97\u6cd5\u3002", "conclusion": "RIPLM\u901a\u8fc7\u76f4\u63a5\u5728\u6392\u540d\u8bf1\u5bfc\u53c2\u6570\u5316\u4e2d\u66f4\u65b0\uff0c\u6210\u529f\u4fdd\u6301\u4e86\u4e0e\u6392\u540d\u57fa\u51c6\u7684\u7b49\u4ef7\u6027\uff0c\u4e3a\u4f11\u7720\u4e13\u5bb6\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18139", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18139", "abs": "https://arxiv.org/abs/2509.18139", "authors": ["Akshay Murthy", "Shawn Sebastian", "Manil Shangle", "Huaduo Wang", "Sopam Dasgupta", "Gopal Gupta"], "title": "Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification", "comment": "7 pages", "summary": "Recently, the demand for Machine Learning (ML) models that can balance\naccuracy, efficiency, and interpreability has grown significantly.\nTraditionally, there has been a tradeoff between accuracy and explainability in\npredictive models, with models such as Neural Networks achieving high accuracy\non complex datasets while sacrificing internal transparency. As such, new\nrule-based algorithms such as FOLD-SE have been developed that provide tangible\njustification for predictions in the form of interpretable rule sets. The\nprimary objective of this study was to compare FOLD-SE and FOLD-R++, both\nrule-based classifiers, in binary classification and evaluate how FOLD-SE\nperforms against XGBoost, a widely used ensemble classifier, when applied to\nmulti-category classification. We hypothesized that because FOLD-SE can\ngenerate a condensed rule set in a more explainable manner, it would lose\nupwards of an average of 3 percent in accuracy and F1 score when compared with\nXGBoost and FOLD-R++ in multiclass and binary classification, respectively. The\nresearch used data collections for classification, with accuracy, F1 scores,\nand processing time as the primary performance measures. Outcomes show that\nFOLD-SE is superior to FOLD-R++ in terms of binary classification by offering\nfewer rules but losing a minor percentage of accuracy and efficiency in\nprocessing time; in tasks that involve multi-category classifications, FOLD-SE\nis more precise and far more efficient compared to XGBoost, in addition to\ngenerating a comprehensible rule set. The results point out that FOLD-SE is a\nbetter choice for both binary tasks and classifications with multiple\ncategories. Therefore, these results demonstrate that rule-based approaches\nlike FOLD-SE can bridge the gap between explainability and performance,\nhighlighting their potential as viable alternatives to black-box models in\ndiverse classification tasks.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u7c7b\u5668FOLD-SE\u4e0eFOLD-R++\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53caFOLD-SE\u4e0eXGBoost\u5728\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0FOLD-SE\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u6027\u80fd\u635f\u5931\u5f88\u5c0f\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5e73\u8861\u9700\u6c42\u7684\u589e\u957f\uff0c\u4f20\u7edf\u6a21\u578b\u5b58\u5728\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u7c7b\u5668\u5982\u4f55\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u4ee5\u51c6\u786e\u7387\u3001F1\u5206\u6570\u548c\u5904\u7406\u65f6\u95f4\u4f5c\u4e3a\u4e3b\u8981\u6027\u80fd\u6307\u6807\uff0c\u6bd4\u8f83FOLD-SE\u4e0eFOLD-R++\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53caFOLD-SE\u4e0eXGBoost\u5728\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793aFOLD-SE\u5728\u4e8c\u5206\u7c7b\u4e2d\u4f18\u4e8eFOLD-R++\uff0c\u63d0\u4f9b\u66f4\u5c11\u7684\u89c4\u5219\u4f46\u4ec5\u635f\u5931\u5c11\u91cf\u51c6\u786e\u6027\u548c\u5904\u7406\u6548\u7387\uff1b\u5728\u591a\u5206\u7c7b\u4e2d\uff0cFOLD-SE\u6bd4XGBoost\u66f4\u7cbe\u786e\u4e14\u9ad8\u6548\uff0c\u540c\u65f6\u751f\u6210\u53ef\u7406\u89e3\u7684\u89c4\u5219\u96c6\u3002", "conclusion": "FOLD-SE\u662f\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4efb\u52a1\u7684\u66f4\u597d\u9009\u62e9\uff0c\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u80fd\u591f\u5f25\u5408\u53ef\u89e3\u91ca\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6210\u4e3a\u9ed1\u76d2\u6a21\u578b\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.18140", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18140", "abs": "https://arxiv.org/abs/2509.18140", "authors": ["Iram Wajahat", "Amritpal Singh", "Fazel Keshtkar", "Syed Ahmad Chan Bukhari"], "title": "A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders", "comment": "6 pages, 6 figures", "summary": "Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent\na significant global health burden, disproportionately impacting genetically\npredisposed populations such as the Pima Indians (a Native American tribe from\nsouth central Arizona). This study introduces a novel machine learning (ML)\nframework that integrates predictive modeling with gene-agnostic pathway\nmapping to identify high-risk individuals and uncover potential therapeutic\ntargets. Using the Pima Indian dataset, logistic regression and t-tests were\napplied to identify key predictors of T2DM, yielding an overall model accuracy\nof 78.43%. To bridge predictive analytics with biological relevance, we\ndeveloped a pathway mapping strategy that links identified predictors to\ncritical signaling networks, including insulin signaling, AMPK, and PPAR\npathways. This approach provides mechanistic insights without requiring direct\nmolecular data. Building upon these connections, we propose therapeutic\nstrategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1\nmodulators, and phytochemical, further validated through pathway enrichment\nanalyses. Overall, this framework advances precision medicine by offering\ninterpretable and scalable solutions for early detection and targeted\nintervention in metabolic disorders. The key contributions of this work are:\n(1) development of an ML framework combining logistic regression and principal\ncomponent analysis (PCA) for T2DM risk prediction; (2) introduction of a\ngene-agnostic pathway mapping approach to generate mechanistic insights; and\n(3) identification of novel therapeutic strategies tailored for high-risk\npopulations.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u548c\u57fa\u56e0\u65e0\u5173\u901a\u8def\u6620\u5c04\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b2\u578b\u7cd6\u5c3f\u75c5\u9ad8\u98ce\u9669\u4e2a\u4f53\u5e76\u53d1\u73b0\u6f5c\u5728\u6cbb\u7597\u9776\u70b9\u3002\u8be5\u6846\u67b6\u5728Pima\u5370\u7b2c\u5b89\u4eba\u6570\u636e\u96c6\u4e0a\u8fbe\u523078.43%\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u901a\u8fc7\u901a\u8def\u6620\u5c04\u8bc6\u522b\u4e86\u80f0\u5c9b\u7d20\u4fe1\u53f7\u3001AMPK\u548cPPAR\u7b49\u5173\u952e\u901a\u8def\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u9ad8\u98ce\u9669\u4eba\u7fa4\u7684\u7cbe\u51c6\u6cbb\u7597\u7b56\u7565\u3002", "motivation": "2\u578b\u7cd6\u5c3f\u75c5\u7b49\u4ee3\u8c22\u6027\u75be\u75c5\u5bf9\u9057\u4f20\u6613\u611f\u4eba\u7fa4\uff08\u5982Pima\u5370\u7b2c\u5b89\u4eba\uff09\u9020\u6210\u4e25\u91cd\u5f71\u54cd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u65e9\u671f\u8bc6\u522b\u9ad8\u98ce\u9669\u4e2a\u4f53\u5e76\u63d0\u4f9b\u673a\u5236\u6027\u89c1\u89e3\u7684\u7cbe\u51c6\u533b\u7597\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u548ct\u68c0\u9a8c\u8bc6\u522bT2DM\u5173\u952e\u9884\u6d4b\u56e0\u5b50\uff0c\u7ed3\u5408\u4e3b\u6210\u5206\u5206\u6790(PCA)\u6784\u5efa\u9884\u6d4b\u6a21\u578b\uff0c\u5f00\u53d1\u57fa\u56e0\u65e0\u5173\u901a\u8def\u6620\u5c04\u7b56\u7565\u5c06\u9884\u6d4b\u56e0\u5b50\u4e0e\u5173\u952e\u4fe1\u53f7\u901a\u8def\uff08\u80f0\u5c9b\u7d20\u4fe1\u53f7\u3001AMPK\u3001PPAR\u901a\u8def\uff09\u76f8\u5173\u8054\u3002", "result": "\u6a21\u578b\u6574\u4f53\u51c6\u786e\u7387\u8fbe\u523078.43%\uff0c\u6210\u529f\u8bc6\u522b\u4e86\u4e0eT2DM\u76f8\u5173\u7684\u5173\u952e\u9884\u6d4b\u56e0\u5b50\uff0c\u5e76\u901a\u8fc7\u901a\u8def\u6620\u5c04\u63ed\u793a\u4e86\u6f5c\u5728\u7684\u673a\u5236\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u5305\u62ecGLP-1/GIP\u53d7\u4f53\u6fc0\u52a8\u5242\u3001AMPK\u6fc0\u6d3b\u5242\u3001SIRT1\u8c03\u8282\u5242\u7b49\u6cbb\u7597\u7b56\u7565\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4ee3\u8c22\u6027\u75be\u75c5\u7684\u7cbe\u51c6\u533b\u7597\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u65e9\u671f\u68c0\u6d4b\u548c\u9776\u5411\u5e72\u9884\u7684\u7ed3\u5408\uff0c\u4e3a\u9ad8\u98ce\u9669\u4eba\u7fa4\u7684\u4e2a\u6027\u5316\u6cbb\u7597\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.18144", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18144", "abs": "https://arxiv.org/abs/2509.18144", "authors": ["Yubo Yang", "Yichen Zhu", "Bo Jiang"], "title": "AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation", "comment": "9 pages", "summary": "Spatio-temporal data abounds in domain like traffic and environmental\nmonitoring. However, it often suffers from missing values due to sensor\nmalfunctions, transmission failures, etc. Recent years have seen continued\nefforts to improve spatio-temporal data imputation performance. Recently\ndiffusion models have outperformed other approaches in various tasks, including\nspatio-temporal imputation, showing competitive performance. Extracting and\nutilizing spatio-temporal dependencies as conditional information is vital in\ndiffusion-based methods. However, previous methods introduce error accumulation\nin this process and ignore the variability of the dependencies in the noisy\ndata at different diffusion steps. In this paper, we propose AdaSTI (Adaptive\nDependency Model in Diffusion-based Spatio-Temporal Imputation), a novel\nspatio-temporal imputation approach based on conditional diffusion model.\nInside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model\nfor pre-imputation with the imputed result used to extract conditional\ninformation by our designed Spatio-Temporal Conditionalizer (STC)network. We\nalso propose a Noise-Aware Spatio-Temporal (NAST) network with a gated\nattention mechanism to capture the variant dependencies across diffusion steps.\nExtensive experiments on three real-world datasets show that AdaSTI outperforms\nexisting methods in all the settings, with up to 46.4% reduction in imputation\nerror.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdaSTI\uff0c\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u65f6\u7a7a\u6570\u636e\u63d2\u8865\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411S4\u6a21\u578b\u8fdb\u884c\u9884\u63d2\u8865\uff0c\u5e76\u8bbe\u8ba1\u566a\u58f0\u611f\u77e5\u65f6\u7a7a\u7f51\u7edc\u6765\u6355\u6349\u4e0d\u540c\u6269\u6563\u6b65\u9aa4\u4e2d\u7684\u53d8\u5316\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe46.4%\u7684\u63d2\u8865\u8bef\u5dee\u964d\u4f4e\u3002", "motivation": "\u65f6\u7a7a\u6570\u636e\u5e38\u56e0\u4f20\u611f\u5668\u6545\u969c\u7b49\u539f\u56e0\u5b58\u5728\u7f3a\u5931\u503c\u3002\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u5728\u63d0\u53d6\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u606f\u65f6\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4e14\u5ffd\u7565\u4e86\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u4f9d\u8d56\u5173\u7cfb\u7684\u53d8\u5316\u6027\u3002", "method": "\u63d0\u51faAdaSTI\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u57fa\u4e8e\u53cc\u5411S4\u6a21\u578b\u7684BiS4PI\u7f51\u7edc\u8fdb\u884c\u9884\u63d2\u8865\uff1b2\uff09\u65f6\u7a7a\u6761\u4ef6\u5316\u5668\uff08STC\uff09\u7f51\u7edc\u63d0\u53d6\u6761\u4ef6\u4fe1\u606f\uff1b3\uff09\u566a\u58f0\u611f\u77e5\u65f6\u7a7a\uff08NAST\uff09\u7f51\u7edc\u901a\u8fc7\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u4e0d\u540c\u6269\u6563\u6b65\u9aa4\u7684\u53d8\u5f02\u6027\u4f9d\u8d56\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAdaSTI\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d2\u8865\u8bef\u5dee\u6700\u9ad8\u964d\u4f4e46.4%\u3002", "conclusion": "AdaSTI\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5904\u7406\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u6269\u6563\u6a21\u578b\u7684\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u6709\u6548\u5229\u7528\u6761\u4ef6\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u6570\u636e\u63d2\u8865\u6027\u80fd\u3002"}}
{"id": "2509.18145", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18145", "abs": "https://arxiv.org/abs/2509.18145", "authors": ["Syed Ahmad Chan Bukhari", "Amritpal Singh", "Shifath Hossain", "Iram Wajahat"], "title": "Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records", "comment": "7 pages, 3 Figure", "summary": "Intensive Care Unit (ICU) patients often present with complex, overlapping\nsigns of physiological deterioration that require timely escalation of care.\nTraditional early warning systems, such as SOFA or MEWS, are limited by their\nfocus on single outcomes and fail to capture the multi-dimensional nature of\nclinical decline. This study proposes a multi-label classification framework to\npredict Care Escalation Triggers (CETs), including respiratory failure,\nhemodynamic instability, renal compromise, and neurological deterioration,\nusing the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are\ndefined through rule-based criteria applied to data from hours 24 to 72 (for\nexample, oxygen saturation below 90, mean arterial pressure below 65 mmHg,\ncreatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale\nscore greater than 2). Features are extracted from the first 24 hours and\ninclude vital sign aggregates, laboratory values, and static demographics. We\ntrain and evaluate multiple classification models on a cohort of 85,242 ICU\nstays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation\nmetrics include per-label precision, recall, F1-score, and Hamming loss.\nXGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,\n0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,\noutperforming baseline models. Feature analysis shows that clinically relevant\nparameters such as respiratory rate, blood pressure, and creatinine are the\nmost influential predictors, consistent with the clinical definitions of the\nCETs. The proposed framework demonstrates practical potential for early,\ninterpretable clinical alerts without requiring complex time-series modeling or\nnatural language processing.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bICU\u60a3\u8005\u7684\u62a4\u7406\u5347\u7ea7\u89e6\u53d1\u56e0\u7d20\uff08CETs\uff09\uff0c\u5305\u62ec\u547c\u5438\u8870\u7aed\u3001\u8840\u6d41\u52a8\u529b\u5b66\u4e0d\u7a33\u5b9a\u3001\u80be\u529f\u80fd\u635f\u5bb3\u548c\u795e\u7ecf\u529f\u80fd\u6076\u5316\uff0c\u4f7f\u7528ICU\u5165\u9662\u524d24\u5c0f\u65f6\u7684\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\uff08\u5982SOFA\u6216MEWS\uff09\u5c40\u9650\u4e8e\u5355\u4e00\u7ed3\u679c\u9884\u6d4b\uff0c\u65e0\u6cd5\u6355\u6349\u4e34\u5e8a\u6076\u5316\u7684\u591a\u7ef4\u5ea6\u7279\u5f81\u3002ICU\u60a3\u8005\u5e38\u51fa\u73b0\u590d\u6742\u91cd\u53e0\u7684\u751f\u7406\u6076\u5316\u8ff9\u8c61\uff0c\u9700\u8981\u53ca\u65f6\u5347\u7ea7\u62a4\u7406\u3002", "method": "\u4f7f\u7528MIMIC-IV\u6570\u636e\u5e93\uff0c\u57fa\u4e8e\u89c4\u5219\u5b9a\u4e49CETs\uff08\u5982\u8840\u6c27\u9971\u548c\u5ea6\u4f4e\u4e8e90%\u3001\u5e73\u5747\u52a8\u8109\u538b\u4f4e\u4e8e65 mmHg\u7b49\uff09\u3002\u4eceICU\u5165\u9662\u524d24\u5c0f\u65f6\u63d0\u53d6\u7279\u5f81\uff08\u751f\u547d\u4f53\u5f81\u6c47\u603b\u3001\u5b9e\u9a8c\u5ba4\u503c\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\uff09\uff0c\u572885,242\u4f8bICU\u4f4f\u9662\u6570\u636e\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\u591a\u4e2a\u5206\u7c7b\u6a21\u578b\u3002", "result": "XGBoost\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u5206\u522b\u4e3a\uff1a\u547c\u54380.66\u3001\u8840\u6d41\u52a8\u529b\u5b660.72\u3001\u80be\u529f\u80fd0.76\u3001\u795e\u7ecf\u7cfb\u7edf0.62\u3002\u7279\u5f81\u5206\u6790\u663e\u793a\u547c\u5438\u9891\u7387\u3001\u8840\u538b\u548c\u808c\u9150\u7b49\u4e34\u5e8a\u76f8\u5173\u53c2\u6570\u662f\u6700\u6709\u5f71\u54cd\u529b\u7684\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u65e9\u671f\u3001\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u8b66\u62a5\u7684\u5b9e\u9645\u6f5c\u529b\uff0c\u65e0\u9700\u590d\u6742\u7684\u65f6\u5e8f\u5efa\u6a21\u6216\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3002"}}
{"id": "2509.18147", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18147", "abs": "https://arxiv.org/abs/2509.18147", "authors": ["Xinyu Mu", "Hui Dou", "Furao Shen", "Jian Zhao"], "title": "ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks", "comment": null, "summary": "Concept-based interpretability for Convolutional Neural Networks (CNNs) aims\nto align internal model representations with high-level semantic concepts, but\nexisting approaches largely overlook the semantic roles of individual filters\nand the dynamic propagation of concepts across layers. To address these\nlimitations, we propose ConceptFlow, a concept-based interpretability framework\nthat simulates the internal \"thinking path\" of a model by tracing how concepts\nemerge and evolve across layers. ConceptFlow comprises two key components: (i)\nconcept attentions, which associate each filter with relevant high-level\nconcepts to enable localized semantic interpretation, and (ii) conceptual\npathways, derived from a concept transition matrix that quantifies how concepts\npropagate and transform between filters. Together, these components offer a\nunified and structured view of internal model reasoning. Experimental results\ndemonstrate that ConceptFlow yields semantically meaningful insights into model\nreasoning, validating the effectiveness of concept attentions and conceptual\npathways in explaining decision behavior. By modeling hierarchical conceptual\npathways, ConceptFlow provides deeper insight into the internal logic of CNNs\nand supports the generation of more faithful and human-aligned explanations.", "AI": {"tldr": "ConceptFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u6982\u5ff5\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u8ffd\u8e2a\u6982\u5ff5\u5728CNN\u5404\u5c42\u4e2d\u7684\u51fa\u73b0\u548c\u6f14\u5316\u6765\u6a21\u62df\u6a21\u578b\u7684\u5185\u90e8\"\u601d\u8003\u8def\u5f84\"\u3002", "motivation": "\u73b0\u6709\u7684CNN\u6982\u5ff5\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u4e86\u5355\u4e2a\u8fc7\u6ee4\u5668\u7684\u8bed\u4e49\u89d2\u8272\u4ee5\u53ca\u6982\u5ff5\u5728\u5c42\u95f4\u7684\u52a8\u6001\u4f20\u64ad\uff0c\u65e0\u6cd5\u5168\u9762\u89e3\u91ca\u6a21\u578b\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "ConceptFlow\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(i)\u6982\u5ff5\u6ce8\u610f\u529b\uff0c\u5c06\u6bcf\u4e2a\u8fc7\u6ee4\u5668\u4e0e\u76f8\u5173\u9ad8\u7ea7\u6982\u5ff5\u5173\u8054\u5b9e\u73b0\u5c40\u90e8\u8bed\u4e49\u89e3\u91ca\uff1b(ii)\u6982\u5ff5\u8def\u5f84\uff0c\u901a\u8fc7\u6982\u5ff5\u8f6c\u79fb\u77e9\u9635\u91cf\u5316\u6982\u5ff5\u5728\u8fc7\u6ee4\u5668\u95f4\u7684\u4f20\u64ad\u548c\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eConceptFlow\u80fd\u591f\u4ea7\u751f\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u6a21\u578b\u63a8\u7406\u6d1e\u5bdf\uff0c\u9a8c\u8bc1\u4e86\u6982\u5ff5\u6ce8\u610f\u529b\u548c\u6982\u5ff5\u8def\u5f84\u5728\u89e3\u91ca\u51b3\u7b56\u884c\u4e3a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5efa\u6a21\u5c42\u6b21\u5316\u7684\u6982\u5ff5\u8def\u5f84\uff0cConceptFlow\u63d0\u4f9b\u4e86\u5bf9CNN\u5185\u90e8\u903b\u8f91\u7684\u66f4\u6df1\u5c42\u6b21\u7406\u89e3\uff0c\u652f\u6301\u751f\u6210\u66f4\u5fe0\u5b9e\u4e14\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5bf9\u9f50\u7684\u89e3\u91ca\u3002"}}
{"id": "2509.18150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18150", "abs": "https://arxiv.org/abs/2509.18150", "authors": ["Kean Shi", "Liang Chen", "Haozhe Zhao", "Baobao Chang"], "title": "Sparse Training Scheme for Multimodal LLM", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated outstanding\nperformance across a variety of domains. However, training MLLMs is often\ninefficient due to the significantly longer input sequences introduced by\nmultimodal data and the low utilization of inter-layer computations. To address\nthis challenge, we shift the focus to the training process itself and propose a\nnovel training-efficient framework based on sparse representations, termed the\nSparse Training Scheme (STS). This scheme consists of two key components: the\nVisual Token Compressor, which reduces the information load by compressing\nvisual tokens, and the Layer Dynamic Skipper, which mitigates the computational\noverhead by dynamically skipping unnecessary layers in the language model\nduring both forward and backward passes. Our approach is broadly applicable to\ndiverse MLLM architectures and has been extensively evaluated on multiple\nbenchmarks, demonstrating its effectiveness and efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u8868\u793a\u7684\u8bad\u7ec3\u6548\u7387\u6846\u67b6STS\uff0c\u901a\u8fc7\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u5668\u548c\u5c42\u52a8\u6001\u8df3\u8fc7\u5668\u6765\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\uff0c\u4e3b\u8981\u7531\u4e8e\u591a\u6a21\u6001\u6570\u636e\u5f15\u5165\u7684\u957f\u8f93\u5165\u5e8f\u5217\u548c\u5c42\u95f4\u8ba1\u7b97\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "STS\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u5668\uff08\u51cf\u5c11\u89c6\u89c9\u4ee4\u724c\u4fe1\u606f\u8d1f\u8f7d\uff09\u548c\u5c42\u52a8\u6001\u8df3\u8fc7\u5668\uff08\u5728\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\u4e2d\u52a8\u6001\u8df3\u8fc7\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u5fc5\u8981\u5c42\uff09\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u7a00\u758f\u8bad\u7ec3\u65b9\u6848\u9002\u7528\u4e8e\u591a\u79cdMLLM\u67b6\u6784\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2509.18151", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18151", "abs": "https://arxiv.org/abs/2509.18151", "authors": ["Jindi Lv", "Yuhao Zhou", "Yuxin Tian", "Qing Ye", "Wentao Feng", "Jiancheng Lv"], "title": "HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork", "comment": null, "summary": "Time-intensive performance evaluations significantly impede progress in\nNeural Architecture Search (NAS). To address this, neural predictors leverage\nsurrogate models trained on proxy datasets, allowing for direct performance\npredictions for new architectures. However, these predictors often exhibit poor\ngeneralization due to their limited ability to capture intricate relationships\namong various architectures. In this paper, we propose HyperNAS, a novel neural\npredictor paradigm for enhancing architecture representation learning. HyperNAS\nconsists of two primary components: a global encoding scheme and a shared\nhypernetwork. The global encoding scheme is devised to capture the\ncomprehensive macro-structure information, while the shared hypernetwork serves\nas an auxiliary task to enhance the investigation of inter-architecture\npatterns. To ensure training stability, we further develop a dynamic adaptive\nmulti-task loss to facilitate personalized exploration on the Pareto front.\nExtensive experiments across five representative search spaces, including ViTs,\ndemonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For\ninstance, HyperNAS strikes new state-of-the-art results, with 97.60\\% top-1\naccuracy on CIFAR-10 and 82.4\\% top-1 accuracy on ImageNet, using at least\n5.0$\\times$ fewer samples.", "AI": {"tldr": "HyperNAS\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u9884\u6d4b\u5668\u8303\u5f0f\uff0c\u901a\u8fc7\u5168\u5c40\u7f16\u7801\u65b9\u6848\u548c\u5171\u4eab\u8d85\u7f51\u7edc\u6765\u589e\u5f3a\u67b6\u6784\u8868\u793a\u5b66\u4e60\uff0c\u5728\u5c11\u91cf\u6837\u672c\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684NAS\u6027\u80fd\u3002", "motivation": "\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e2d\u7684\u6027\u80fd\u8bc4\u4f30\u8017\u65f6\u4e25\u91cd\u963b\u788d\u8fdb\u5c55\uff0c\u73b0\u6709\u795e\u7ecf\u9884\u6d4b\u5668\u7531\u4e8e\u96be\u4ee5\u6355\u6349\u67b6\u6784\u95f4\u590d\u6742\u5173\u7cfb\u800c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "HyperNAS\u5305\u542b\u5168\u5c40\u7f16\u7801\u65b9\u6848\uff08\u6355\u83b7\u5b8f\u89c2\u7ed3\u6784\u4fe1\u606f\uff09\u548c\u5171\u4eab\u8d85\u7f51\u7edc\uff08\u589e\u5f3a\u67b6\u6784\u95f4\u6a21\u5f0f\u7814\u7a76\uff09\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u81ea\u9002\u5e94\u591a\u4efb\u52a1\u635f\u5931\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u4ee3\u8868\u6027\u641c\u7d22\u7a7a\u95f4\uff08\u5305\u62ecViTs\uff09\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cHyperNAS\u5728\u5c11\u91cf\u6837\u672c\u573a\u666f\u4e0b\u4f18\u52bf\u660e\u663e\uff0c\u5728CIFAR-10\u4e0a\u8fbe\u523097.60% top-1\u51c6\u786e\u7387\uff0cImageNet\u4e0a\u8fbe\u523082.4% top-1\u51c6\u786e\u7387\uff0c\u6837\u672c\u4f7f\u7528\u91cf\u51cf\u5c11\u81f3\u5c115\u500d\u3002", "conclusion": "HyperNAS\u901a\u8fc7\u6539\u8fdb\u7684\u67b6\u6784\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u9884\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.18152", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18152", "abs": "https://arxiv.org/abs/2509.18152", "authors": ["Zhenyu Qi", "Qing Yu", "Jichen Wang", "Yun-Bo Zhao", "Zerui Li", "Wenjun Lv"], "title": "WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation", "comment": null, "summary": "Well-log interpretation is fundamental for subsurface characterization but\nremains challenged by heterogeneous tool responses, noisy signals, and limited\nlabels. We propose WLFM, a foundation model pretrained on multi-curve logs from\n1200 wells, comprising three stages: tokenization of log patches into\ngeological tokens, self-supervised pretraining with masked-token modeling and\nstratigraphy-aware contrastive learning, and multi-task adaptation with\nfew-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,\nachieving 0.0041 MSE in porosity estimation and 74.13\\% accuracy in lithology\nclassification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\\%\naccuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,\nlearns a reusable geological vocabulary, and reconstructs masked curves with\nreasonable fidelity, though systematic offsets are observed in shallow and\nultra-deep intervals. Although boundary detection is not explicitly evaluated\nhere, clustering analyses suggest strong potential for future extension. These\nresults establish WLFM as a scalable, interpretable, and transferable backbone\nfor geological AI, with implications for multi-modal integration of logs,\nseismic, and textual data.", "AI": {"tldr": "WLFM\u662f\u4e00\u4e2a\u7528\u4e8e\u6d4b\u4e95\u89e3\u91ca\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u9884\u8bad\u7ec3\u57281200\u53e3\u4e95\u7684\u591a\u66f2\u7ebf\u6d4b\u4e95\u6570\u636e\u4e0a\uff0c\u5728\u5b54\u9699\u5ea6\u4f30\u8ba1\u548c\u5ca9\u6027\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6d4b\u4e95\u89e3\u91ca\u9762\u4e34\u5de5\u5177\u54cd\u5e94\u5f02\u8d28\u6027\u3001\u566a\u58f0\u4fe1\u53f7\u548c\u6807\u7b7e\u6709\u9650\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u7684\u53ef\u6269\u5c55AI\u65b9\u6cd5\u3002", "method": "\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u5c06\u6d4b\u4e95\u7247\u6bb5\u6807\u8bb0\u5316\u4e3a\u5730\u8d28\u6807\u8bb0\uff1b\u4f7f\u7528\u63a9\u7801\u6807\u8bb0\u5efa\u6a21\u548c\u5730\u5c42\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff1b\u901a\u8fc7\u5c11\u6837\u672c\u5fae\u8c03\u8fdb\u884c\u591a\u4efb\u52a1\u9002\u5e94\u3002", "result": "WLFM\u5728\u5b54\u9699\u5ea6\u4f30\u8ba1\u4e0a\u8fbe\u52300.0041 MSE\uff0c\u5ca9\u6027\u5206\u7c7b\u51c6\u786e\u738774.13%\uff1b\u5fae\u8c03\u540e\u5206\u522b\u63d0\u5347\u81f30.0038 MSE\u548c78.10%\u51c6\u786e\u7387\u3002", "conclusion": "WLFM\u4f5c\u4e3a\u5730\u8d28AI\u7684\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u8fc1\u79fb\u9aa8\u5e72\uff0c\u4e3a\u6d4b\u4e95\u3001\u5730\u9707\u548c\u6587\u672c\u6570\u636e\u7684\u591a\u6a21\u6001\u96c6\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18153", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.18153", "abs": "https://arxiv.org/abs/2509.18153", "authors": ["Hanqun Cao", "Marcelo D. T. Torres", "Jingjie Zhang", "Zijun Gao", "Fang Wu", "Chunbin Gu", "Jure Leskovec", "Yejin Choi", "Cesar de la Fuente-Nunez", "Guangyong Chen", "Pheng-Ann Heng"], "title": "A deep reinforcement learning platform for antibiotic discovery", "comment": "42 pages, 16 figures", "summary": "Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths\nannually by 2050, underscoring the urgent need for new antibiotics. Here we\npresent ApexAmphion, a deep-learning framework for de novo design of\nantibiotics that couples a 6.4-billion-parameter protein language model with\nreinforcement learning. The model is first fine-tuned on curated peptide data\nto capture antimicrobial sequence regularities, then optimised with proximal\npolicy optimization against a composite reward that combines predictions from a\nlearned minimum inhibitory concentration (MIC) classifier with differentiable\nphysicochemical objectives. In vitro evaluation of 100 designed peptides showed\nlow MIC values (nanomolar range in some cases) for all candidates (100% hit\nrate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial\nactivity against at least two clinically relevant bacteria. The lead molecules\nkilled bacteria primarily by potently targeting the cytoplasmic membrane. By\nunifying generation, scoring and multi-objective optimization with deep\nreinforcement learning in a single pipeline, our approach rapidly produces\ndiverse, potent candidates, offering a scalable route to peptide antibiotics\nand a platform for iterative steering toward potency and developability within\nhours.", "AI": {"tldr": "ApexAmphion\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u540864\u4ebf\u53c2\u6570\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u7528\u4e8e\u4ece\u5934\u8bbe\u8ba1\u6297\u751f\u7d20\u80bd\u3002\u8be5\u65b9\u6cd5\u5728\u4f53\u5916\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa100%\u7684\u6210\u529f\u7387\u548c\u7eb3\u7c73\u7ea7MIC\u503c\u3002", "motivation": "\u6297\u83cc\u7d20\u8010\u836f\u6027\uff08AMR\uff09\u9884\u8ba1\u5230205\u5e74\u6bcf\u5e74\u5bfc\u81f41000\u4e07\u4eba\u6b7b\u4ea1\uff0c\u8feb\u5207\u9700\u8981\u65b0\u7684\u6297\u751f\u7d20\u3002\u73b0\u6709\u6297\u751f\u7d20\u5f00\u53d1\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u8bbe\u8ba1\u5e73\u53f0\u3002", "method": "\u4f7f\u752864\u4ebf\u53c2\u6570\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\u3002\u6a21\u578b\u5148\u5728\u7cbe\u9009\u80bd\u6570\u636e\u4e0a\u5fae\u8c03\u4ee5\u6355\u83b7\u6297\u83cc\u5e8f\u5217\u89c4\u5f8b\uff0c\u7136\u540e\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7ed3\u5408MIC\u5206\u7c7b\u5668\u9884\u6d4b\u548c\u53ef\u5fae\u5206\u7269\u7406\u5316\u5b66\u76ee\u6807\u7684\u590d\u5408\u5956\u52b1\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u4f53\u5916\u8bc4\u4f30100\u4e2a\u8bbe\u8ba1\u80bd\u663e\u793a\u6240\u6709\u5019\u9009\u7269\u90fd\u5177\u6709\u4f4eMIC\u503c\uff08\u90e8\u5206\u8fbe\u5230\u7eb3\u6469\u5c14\u8303\u56f4\uff09\uff0c100%\u547d\u4e2d\u7387\u300299/100\u5316\u5408\u7269\u5bf9\u81f3\u5c11\u4e24\u79cd\u4e34\u5e8a\u76f8\u5173\u7ec6\u83cc\u8868\u73b0\u51fa\u5e7f\u8c31\u6297\u83cc\u6d3b\u6027\u3002\u4e3b\u8981\u4f5c\u7528\u673a\u5236\u662f\u901a\u8fc7\u9776\u5411\u7ec6\u80de\u8d28\u819c\u6740\u6b7b\u7ec6\u83cc\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u751f\u6210\u3001\u8bc4\u5206\u548c\u591a\u76ee\u6807\u4f18\u5316\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7edf\u4e00\u5728\u5355\u4e00\u6d41\u7a0b\u4e2d\uff0c\u80fd\u591f\u5feb\u901f\u4ea7\u751f\u591a\u6837\u5316\u3001\u5f3a\u6548\u7684\u5019\u9009\u7269\uff0c\u4e3a\u80bd\u6297\u751f\u7d20\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u5e76\u53ef\u5728\u6570\u5c0f\u65f6\u5185\u8fed\u4ee3\u4f18\u5316\u6548\u529b\u548c\u53ef\u5f00\u53d1\u6027\u3002"}}
{"id": "2509.18154", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18154", "abs": "https://arxiv.org/abs/2509.18154", "authors": ["Tianyu Yu", "Zefan Wang", "Chongyi Wang", "Fuwei Huang", "Wenshuo Ma", "Zhihui He", "Tianchi Cai", "Weize Chen", "Yuxiang Huang", "Yuanqian Zhao", "Bokai Xu", "Junbo Cui", "Yingjing Xu", "Liqing Ruan", "Luoyuan Zhang", "Hanyu Liu", "Jingkun Tang", "Hongyuan Liu", "Qining Guo", "Wenhao Hu", "Bingxiang He", "Jie Zhou", "Jie Cai", "Ji Qi", "Zonghao Guo", "Chi Chen", "Guoyang Zeng", "Yuxuan Li", "Ganqu Cui", "Ning Ding", "Xu Han", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe", "comment": "Project Website: https://github.com/OpenBMB/MiniCPM-V", "summary": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.", "AI": {"tldr": "MiniCPM-V 4.5\u662f\u4e00\u4e2a8B\u53c2\u6570\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u67b6\u6784\u3001\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u6cd5\u7684\u6539\u8fdb\u5b9e\u73b0\u4e86\u9ad8\u6548\u6027\u548c\u5f3a\u6027\u80fd\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86GPT-4o-latest\u548cQwen2.5-VL 72B\u7b49\u66f4\u5927\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u66f4\u52a0\u6613\u4e8e\u8bbf\u95ee\u548c\u6269\u5c55\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u76843D-Resampler\u6a21\u578b\u67b6\u6784\u5b9e\u73b0\u56fe\u50cf\u548c\u89c6\u9891\u7684\u9ad8\u6548\u7f16\u7801\uff0c\u4f7f\u7528\u7edf\u4e00\u5b66\u4e60\u8303\u5f0f\u5904\u7406\u6587\u6863\u77e5\u8bc6\u548c\u6587\u672c\u8bc6\u522b\uff0c\u4ee5\u53ca\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u652f\u6301\u957f\u77ed\u63a8\u7406\u6a21\u5f0f\u3002", "result": "\u5728OpenCompass\u8bc4\u4f30\u4e2d\u8d85\u8d8aGPT-4o-latest\u548cQwen2.5-VL 72B\u7b49\u6a21\u578b\uff0c\u5728VideoMME\u57fa\u51c6\u4e0a\u4ee5\u4ec546.7%\u7684GPU\u5185\u5b58\u548c8.7%\u7684\u63a8\u7406\u65f6\u95f4\u8fbe\u523030B\u4ee5\u4e0b\u6a21\u578b\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "MiniCPM-V 4.5\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u5f3a\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.18161", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18161", "abs": "https://arxiv.org/abs/2509.18161", "authors": ["William H Patty"], "title": "Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks", "comment": null, "summary": "Activation functions in neural networks are typically selected from a set of\nempirically validated, commonly used static functions such as ReLU, tanh, or\nsigmoid. However, by optimizing the shapes of a network's activation functions,\nwe can train models that are more parameter-efficient and accurate by assigning\nmore optimal activations to the neurons. In this paper, I present and compare 9\ntraining methodologies to explore dual-optimization dynamics in neural networks\nwith parameterized linear B-spline activation functions. The experiments\nrealize up to 94% lower end model error rates in FNNs and 51% lower rates in\nCNNs compared to traditional ReLU-based models. These gains come at the cost of\nadditional development and training complexity as well as end model latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e869\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u53c2\u6570\u5316\u7ebf\u6027B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u7684\u5f62\u72b6\uff0c\u76f8\u6bd4\u4f20\u7edfReLU\u6a21\u578b\uff0c\u5728FNN\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe94%\u7684\u8bef\u5dee\u7387\u964d\u4f4e\uff0c\u5728CNN\u4e2d\u5b9e\u73b0\u4e8651%\u7684\u8bef\u5dee\u7387\u964d\u4f4e\u3002", "motivation": "\u4f20\u7edf\u7684\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\u3001tanh\u3001sigmoid\uff09\u662f\u9759\u6001\u9009\u62e9\u7684\uff0c\u901a\u8fc7\u4f18\u5316\u6fc0\u6d3b\u51fd\u6570\u7684\u5f62\u72b6\u53ef\u4ee5\u8bad\u7ec3\u51fa\u66f4\u53c2\u6570\u9ad8\u6548\u548c\u51c6\u786e\u7684\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u53c2\u6570\u5316\u7ebf\u6027B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u63d0\u51fa\u4e869\u79cd\u4e0d\u540c\u7684\u8bad\u7ec3\u65b9\u6cd5\u6765\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u53cc\u91cd\u4f18\u5316\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u57fa\u4e8eReLU\u7684\u4f20\u7edf\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728FNN\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe94%\u7684\u8bef\u5dee\u7387\u964d\u4f4e\uff0c\u5728CNN\u4e2d\u5b9e\u73b0\u4e8651%\u7684\u8bef\u5dee\u7387\u964d\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u6fc0\u6d3b\u51fd\u6570\u5f62\u72b6\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u9700\u8981\u4ed8\u51fa\u989d\u5916\u7684\u5f00\u53d1\u3001\u8bad\u7ec3\u590d\u6742\u6027\u548c\u6a21\u578b\u5ef6\u8fdf\u7684\u4ee3\u4ef7\u3002"}}
{"id": "2509.18162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18162", "abs": "https://arxiv.org/abs/2509.18162", "authors": ["Meraryslan Meraliyev", "Cemil Turan", "Shirali Kadyrov"], "title": "A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge", "comment": null, "summary": "We study last-mile delivery with one truck and one drone under explicit\nbattery management: the drone flies at twice the truck speed; each sortie must\nsatisfy an endurance budget; after every delivery the drone recharges on the\ntruck before the next launch. We introduce a hybrid reinforcement learning (RL)\nsolver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a\nsmall pointer/attention policy that schedules drone sorties. The policy decodes\nlaunch--serve--rendezvous triplets with hard feasibility masks for endurance\nand post-delivery recharge; a fast, exact timeline simulator enforces\nlaunch/recovery handling and computes the true makespan used by masked\ngreedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and\n$R{=}0.1$, the method achieves an average makespan of \\textbf{5.203}$\\pm$0.093,\nversus \\textbf{5.349}$\\pm$0.038 for ALNS and \\textbf{5.208}$\\pm$0.124 for NN --\ni.e., \\textbf{2.73\\%} better than ALNS on average and within \\textbf{0.10\\%} of\nNN. Per-seed, the RL scheduler never underperforms ALNS on the same instance\nand ties or beats NN on two of three seeds. A decomposition of the makespan\nshows the expected truck--wait trade-off across heuristics; the learned\nscheduler balances both to minimize the total completion time. We provide a\nconfig-first implementation with plotting and significance-test utilities to\nsupport replication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5361\u8f66\u548c\u65e0\u4eba\u673a\u7684\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u7cfb\u7edf\uff0c\u5728\u7535\u6c60\u7ba1\u7406\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u660e\u786e\u7535\u6c60\u7ba1\u7406\u7ea6\u675f\u4e0b\u7684\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u95ee\u9898\uff0c\u5176\u4e2d\u65e0\u4eba\u673a\u98de\u884c\u901f\u5ea6\u4e3a\u5361\u8f66\u4e24\u500d\uff0c\u6bcf\u6b21\u98de\u884c\u9700\u6ee1\u8db3\u7eed\u822a\u9884\u7b97\uff0c\u4e14\u6bcf\u6b21\u914d\u9001\u540e\u9700\u5728\u5361\u8f66\u4e0a\u5145\u7535\u3002", "method": "\u5f00\u53d1\u4e86\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6c42\u89e3\u5668\uff0c\u7ed3\u5408ALNS\u5361\u8f66\u8def\u5f84\u89c4\u5212\uff08\u4f7f\u75282/3-opt\u548cOr-opt\uff09\u548c\u6307\u9488/\u6ce8\u610f\u529b\u7b56\u7565\u6765\u8c03\u5ea6\u65e0\u4eba\u673a\u98de\u884c\u4efb\u52a1\uff0c\u5305\u62ec\u4e25\u683c\u7684\u53ef\u884c\u6027\u68c0\u67e5\u548c\u7cbe\u786e\u65f6\u95f4\u7ebf\u6a21\u62df\u3002", "result": "\u5728N=50\u3001E=0.7\u3001R=0.1\u7684\u6b27\u51e0\u91cc\u5f97\u5b9e\u4f8b\u4e0a\uff0c\u5e73\u5747\u5b8c\u5de5\u65f6\u95f4\u4e3a5.203\u00b10.093\uff0c\u6bd4ALNS\u65b9\u6cd5\u63d0\u53472.73%\uff0c\u4e0eNN\u65b9\u6cd5\u76f8\u5dee\u4ec50.10%\u3002", "conclusion": "\u5b66\u4e60\u7684\u8c03\u5ea6\u5668\u80fd\u5e73\u8861\u5361\u8f66\u7b49\u5f85\u65f6\u95f4\u4ee5\u6700\u5c0f\u5316\u603b\u5b8c\u5de5\u65f6\u95f4\uff0c\u5728\u76f8\u540c\u5b9e\u4f8b\u4e0a\u4ece\u4e0d\u900a\u4e8eALNS\uff0c\u5e76\u5728\u4e09\u5206\u4e4b\u4e8c\u7684\u79cd\u5b50\u4e0a\u4e0eNN\u6301\u5e73\u6216\u66f4\u4f18\u3002"}}
{"id": "2509.18164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18164", "abs": "https://arxiv.org/abs/2509.18164", "authors": ["Ranfei Chen", "Ming Chen"], "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns", "comment": null, "summary": "Diffusion large language models (dLLMs) have emerged as a new architecture\nfollowing auto regressive models. Their denoising process offers a powerful\ngenerative advantage, but they present significant challenges in learning and\nunderstanding numerically sensitive mathematical and order-sensitive logical\ntasks. Current training methods, including pre-training, fine-tuning, and\nreinforcement learning, focus primarily on improving general knowledge\nretention and reasoning abilities, but lack a comprehensive understanding of\nmathematical and logical patterns. We propose DSFT, a simple yet effective\nDiffusion SFT strategy, by adjusting the masking strategy and loss function,\nguiding models to understand mathematical and logical patterns. This strategy\ncan be flexibly combined with pre-training, reinforcement learning, and other\ntraining methods. Validated on models such as LLaDA and Dream series, we prove\nthat DSFT on small-scale data can achieve improvements of 5-10% and\napproximately 2% on mathematical and logical problems, respectively. This\ninspiring masking approach offers insights for future learning of specific\npatterns, which can be easily and efficiently combined with other training\nmethods and applied to various dLLMs. Our code is publicly available at\nhttps://anonymous.4open.science/r/DSFT-0FFB/", "AI": {"tldr": "\u63d0\u51faDSFT\u7b56\u7565\uff0c\u901a\u8fc7\u8c03\u6574\u63a9\u7801\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u903b\u8f91\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u5b66\u4e60\u6570\u503c\u654f\u611f\u7684\u6570\u5b66\u548c\u987a\u5e8f\u654f\u611f\u7684\u903b\u8f91\u4efb\u52a1\u65f6\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u8bad\u7ec3\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6570\u5b66\u548c\u903b\u8f91\u6a21\u5f0f\u7684\u5168\u9762\u7406\u89e3", "method": "DSFT\uff08Diffusion SFT\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u8c03\u6574\u63a9\u7801\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\u6765\u5f15\u5bfc\u6a21\u578b\u7406\u89e3\u6570\u5b66\u548c\u903b\u8f91\u6a21\u5f0f\uff0c\u53ef\u4e0e\u9884\u8bad\u7ec3\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u65b9\u6cd5\u7075\u6d3b\u7ed3\u5408", "result": "\u5728LLaDA\u548cDream\u7cfb\u5217\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u5c0f\u89c4\u6a21\u6570\u636e\u4e0a\u6570\u5b66\u95ee\u9898\u63d0\u53475-10%\uff0c\u903b\u8f91\u95ee\u9898\u63d0\u5347\u7ea62%", "conclusion": "\u8fd9\u79cd\u63a9\u7801\u65b9\u6cd5\u4e3a\u672a\u6765\u5b66\u4e60\u7279\u5b9a\u6a21\u5f0f\u63d0\u4f9b\u4e86\u601d\u8def\uff0c\u53ef\u8f7b\u677e\u9ad8\u6548\u5730\u4e0e\u5176\u4ed6\u8bad\u7ec3\u65b9\u6cd5\u7ed3\u5408\u5e76\u5e94\u7528\u4e8e\u5404\u79cd\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2509.18166", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18166", "abs": "https://arxiv.org/abs/2509.18166", "authors": ["Xiaoqian Qi", "Haoye Chai", "Yong Li"], "title": "MobiGPT: A Foundation Model for Mobile Wireless Networks", "comment": null, "summary": "With the rapid development of mobile communication technologies, future\nmobile networks will offer vast services and resources for commuting,\nproduction, daily life, and entertainment. Accurate and efficient forecasting\nof mobile data (e.g., cell traffic, user behavior, channel quality) helps\noperators monitor network state changes, orchestrate wireless resources, and\nschedule infrastructure and users, thereby improving supply efficiency and\nservice quality. However, current forecasting paradigms rely on customized\ndesigns with tailored models for exclusive data types. Such approaches increase\ncomplexity and deployment costs under large-scale, heterogeneous networks\ninvolving base stations, users, and channels. In this paper, we design a\nfoundation model for mobile data forecasting, MobiGPT, with a unified structure\ncapable of forecasting three data types: base station traffic, user app usage,\nand channel quality. We propose a soft-prompt learning method to help the model\nunderstand features of different data types, and introduce a temporal masking\nmechanism to guide the model through three forecasting tasks: short-term\nprediction, long-term prediction, and distribution generation, supporting\ndiverse optimization scenarios. Evaluations on real-world datasets with over\n100,000 samples show that MobiGPT achieves accurate multi-type forecasting.\nCompared to existing models, it improves forecasting accuracy by 27.37%,\n20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits\nsuperior zero/few-shot performance in unseen scenarios, with over 21.51%\nimprovement, validating its strong transferability as a foundation model.", "AI": {"tldr": "MobiGPT\u662f\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u6570\u636e\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u7edf\u4e00\u9884\u6d4b\u57fa\u7ad9\u6d41\u91cf\u3001\u7528\u6237\u5e94\u7528\u4f7f\u7528\u548c\u4fe1\u9053\u8d28\u91cf\u4e09\u79cd\u6570\u636e\u7c7b\u578b\uff0c\u901a\u8fc7\u8f6f\u63d0\u793a\u5b66\u4e60\u548c\u65f6\u95f4\u63a9\u7801\u673a\u5236\u5b9e\u73b0\u591a\u4efb\u52a1\u9884\u6d4b\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8\u6570\u636e\u9884\u6d4b\u65b9\u6cd5\u9700\u8981\u4e3a\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u5b9a\u5236\u4e13\u95e8\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u5927\u89c4\u6a21\u5f02\u6784\u7f51\u7edc\u4e2d\u7684\u590d\u6742\u6027\u548c\u90e8\u7f72\u6210\u672c\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\u6765\u7b80\u5316\u9884\u6d4b\u6d41\u7a0b\u3002", "method": "\u63d0\u51faMobiGPT\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u8f6f\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5e2e\u52a9\u6a21\u578b\u7406\u89e3\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u7684\u7279\u5f81\uff0c\u5f15\u5165\u65f6\u95f4\u63a9\u7801\u673a\u5236\u6307\u5bfc\u6a21\u578b\u5b8c\u6210\u77ed\u671f\u9884\u6d4b\u3001\u957f\u671f\u9884\u6d4b\u548c\u5206\u5e03\u751f\u6210\u4e09\u79cd\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u5728\u5305\u542b10\u4e07+\u6837\u672c\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cMobiGPT\u76f8\u6bd4\u73b0\u6709\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u5206\u522b\u63d0\u534727.37%\u300120.08%\u548c7.27%\uff0c\u5728\u672a\u89c1\u573a\u666f\u4e0b\u7684\u96f6\u6837\u672c/\u5c11\u6837\u672c\u6027\u80fd\u63d0\u5347\u8d85\u8fc721.51%\u3002", "conclusion": "MobiGPT\u4f5c\u4e3a\u79fb\u52a8\u6570\u636e\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8fc1\u79fb\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u591a\u6837\u5316\u7684\u4f18\u5316\u573a\u666f\uff0c\u4e3a\u672a\u6765\u79fb\u52a8\u7f51\u7edc\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18169", "categories": ["cs.LG", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18169", "abs": "https://arxiv.org/abs/2509.18169", "authors": ["Hengbo Xiao", "Jingyuan Fan", "Xin Tong", "Jingzhao Zhang", "Chao Lu", "Guannan He"], "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning", "comment": null, "summary": "Complex systems typically rely on high-precision numerical computation to\nsupport decisions, but current large language models (LLMs) cannot yet\nincorporate such computations as an intrinsic and interpretable capability with\nexisting architectures. Mainstream multi-agent approaches can leverage external\nexperts, but inevitably introduce communication overhead and suffer from\ninefficient multimodal emergent capability and limited scalability. To this\nend, we propose PiMoE (Physically-isolated Mixture of Experts), a training and\ninference architecture for integrating computation and reasoning. Instead of\nthe workflow paradigm of tool invocation, PiMoE endogenously integrates\ncomputational capabilities into neural networks after separately training\nexperts, a text-to-computation module, and a router. At inference, the router\ndirects computation and reasoning at the token level, thereby enabling\niterative alternation within a single chain of thought. We evaluate PiMoE on\ntwo reasoning-computation tasks against LLM finetuning and the multi-agent\nsystem approaches. Results show that the PiMoE architecture achieves not only\nhigher accuracy than directly finetuning LLMs but also significant improvements\nin response latency, token usage, and GPU energy consumption compared with\nmainstream multi-agent approaches. PiMoE offers an efficient, interpretable,\nand scalable paradigm for next-generation scientific or industrial intelligent\nsystems.", "AI": {"tldr": "PiMoE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u5185\u5728\u5730\u6574\u5408\u8ba1\u7b97\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u5916\u90e8\u5de5\u5177\u8c03\u7528\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u4e0e\u63a8\u7406\u7684\u9ad8\u6548\u7ed3\u5408\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5c06\u9ad8\u7cbe\u5ea6\u6570\u503c\u8ba1\u7b97\u4f5c\u4e3a\u5185\u5728\u80fd\u529b\u6574\u5408\uff0c\u800c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5b58\u5728\u901a\u4fe1\u5f00\u9500\u5927\u3001\u591a\u6a21\u6001\u80fd\u529b\u6548\u7387\u4f4e\u548c\u53ef\u6269\u5c55\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "PiMoE\u91c7\u7528\u7269\u7406\u9694\u79bb\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u5206\u522b\u8bad\u7ec3\u4e13\u5bb6\u3001\u6587\u672c\u5230\u8ba1\u7b97\u6a21\u5757\u548c\u8def\u7531\u5668\uff0c\u5728\u63a8\u7406\u65f6\u8def\u7531\u5668\u5728token\u7ea7\u522b\u6307\u5bfc\u8ba1\u7b97\u548c\u63a8\u7406\uff0c\u5b9e\u73b0\u5355\u94fe\u601d\u7ef4\u5185\u7684\u8fed\u4ee3\u4ea4\u66ff\u3002", "result": "\u5728\u63a8\u7406-\u8ba1\u7b97\u4efb\u52a1\u4e0a\uff0cPiMoE\u4e0d\u4ec5\u6bd4\u76f4\u63a5\u5fae\u8c03LLM\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u800c\u4e14\u5728\u54cd\u5e94\u5ef6\u8fdf\u3001token\u4f7f\u7528\u548cGPU\u80fd\u8017\u65b9\u9762\u76f8\u6bd4\u4e3b\u6d41\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u6709\u663e\u8457\u6539\u5584\u3002", "conclusion": "PiMoE\u4e3a\u4e0b\u4e00\u4ee3\u79d1\u5b66\u6216\u5de5\u4e1a\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u8303\u5f0f\u3002"}}
{"id": "2509.18171", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18171", "abs": "https://arxiv.org/abs/2509.18171", "authors": ["Zhanting Zhou", "KaHou Tam", "Zeqin Wu", "Pengzhao Sun", "Jinbo Wang", "Fengli Zhang"], "title": "FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification", "comment": null, "summary": "Federated Graph Learning (FGL) under domain skew -- as observed on platforms\nsuch as \\emph{Twitch Gamers} and multilingual \\emph{Wikipedia} networks --\ndrives client models toward incompatible representations, rendering naive\naggregation both unstable and ineffective. We find that the culprit is not the\nweighting scheme but the \\emph{noisy gradient signal}: empirical analysis of\nbaseline methods suggests that a vast majority of gradient dimensions can be\ndominated by domain-specific variance. We therefore shift focus from\n\"aggregation-first\" to a \\emph{projection-first} strategy that denoises client\nupdates \\emph{before} they are combined. The proposed FedIA framework realises\nthis \\underline{I}mportance-\\underline{A}ware idea through a two-stage,\nplug-and-play pipeline: (i) a server-side top-$\\rho$ mask keeps only the most\ninformative about 5% of coordinates, and (ii) a lightweight\ninfluence-regularised momentum weight suppresses outlier clients. FedIA adds\n\\emph{no extra uplink traffic and only negligible server memory}, making it\nreadily deployable. On both homogeneous (Twitch Gamers) and heterogeneous\n(Wikipedia) graphs, it yields smoother, more stable convergence and higher\nfinal accuracy than nine strong baselines. A convergence sketch further shows\nthat dynamic projection maintains the optimal\n$\\mathcal{O}(\\sigma^{2}/\\sqrt{T})$ rate.", "AI": {"tldr": "FedIA\u6846\u67b6\u901a\u8fc7\u6295\u5f71\u4f18\u5148\u7b56\u7565\u89e3\u51b3\u8054\u90a6\u56fe\u5b66\u4e60\u4e2d\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u4f7f\u7528\u91cd\u8981\u6027\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u7ba1\u9053\u6765\u964d\u566a\u5ba2\u6237\u7aef\u66f4\u65b0\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u6536\u655b\u548c\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u8054\u90a6\u56fe\u5b66\u4e60\u5728\u9886\u57df\u504f\u79fb\u4e0b\uff08\u5982Twitch Gamers\u548c\u591a\u8bed\u8a00Wikipedia\u7f51\u7edc\uff09\u4f1a\u5bfc\u81f4\u5ba2\u6237\u7aef\u6a21\u578b\u8868\u793a\u4e0d\u517c\u5bb9\uff0c\u4f7f\u7b80\u5355\u805a\u5408\u53d8\u5f97\u4e0d\u7a33\u5b9a\u548c\u4f4e\u6548\u3002\u7814\u7a76\u53d1\u73b0\u95ee\u9898\u6839\u6e90\u662f\u566a\u58f0\u68af\u5ea6\u4fe1\u53f7\u800c\u975e\u6743\u91cd\u65b9\u6848\u3002", "method": "\u63d0\u51faFedIA\u6846\u67b6\uff0c\u91c7\u7528\u6295\u5f71\u4f18\u5148\u7b56\u7565\uff1a1\uff09\u670d\u52a1\u5668\u7aeftop-\u03c1\u63a9\u7801\u4fdd\u7559\u7ea65%\u6700\u4fe1\u606f\u4e30\u5bcc\u7684\u5750\u6807\uff1b2\uff09\u8f7b\u91cf\u7ea7\u5f71\u54cd\u6b63\u5219\u5316\u52a8\u91cf\u6743\u91cd\u6291\u5236\u5f02\u5e38\u5ba2\u6237\u7aef\u3002\u8be5\u65b9\u6cd5\u4e0d\u589e\u52a0\u4e0a\u884c\u6d41\u91cf\u4e14\u670d\u52a1\u5668\u5185\u5b58\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "result": "\u5728\u540c\u8d28\uff08Twitch Gamers\uff09\u548c\u5f02\u8d28\uff08Wikipedia\uff09\u56fe\u4e0a\uff0cFedIA\u6bd49\u4e2a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u66f4\u5e73\u6ed1\u7a33\u5b9a\u7684\u6536\u655b\u548c\u66f4\u9ad8\u7684\u6700\u7ec8\u51c6\u786e\u7387\u3002\u6536\u655b\u5206\u6790\u663e\u793a\u52a8\u6001\u6295\u5f71\u4fdd\u6301\u6700\u4f18\u6536\u655b\u901f\u7387\u3002", "conclusion": "FedIA\u901a\u8fc7\u91cd\u8981\u6027\u611f\u77e5\u7684\u6295\u5f71\u4f18\u5148\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u56fe\u5b66\u4e60\u4e2d\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18172", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18172", "abs": "https://arxiv.org/abs/2509.18172", "authors": ["Wonjun Bang", "Jongseok Park", "Hongseung Yu", "Kyungmin Bin", "Kyunghan Lee"], "title": "SBVR: Summation of BitVector Representation for Efficient LLM Quantization", "comment": "9 pages, 4 figures", "summary": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime.", "AI": {"tldr": "SBVR\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u5206\u5e03\u7684\u7801\u672c\u8868\u793a\u548c\u81ea\u5b9a\u4e49CUDA\u5185\u6838\uff0c\u57284\u4f4d\u91cf\u5316\u4e0b\u5b9e\u73b02.21-3.04\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709PTQ\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1aRTN\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406LLM\u6743\u91cd\u7684\u9ad8\u65af\u5206\u5e03\u7279\u6027\uff0c\u7801\u672c\u65b9\u6cd5\u867d\u7136\u80fd\u89e3\u51b3\u5206\u5e03\u95ee\u9898\u4f46\u5b58\u5728\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4e0d\u4f73\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSBVR\u65b9\u6cd5\uff0c\u5c06\u6743\u91cd\u503c\u6620\u5c04\u5230\u975e\u5747\u5300\u8868\u793a\u70b9\uff0c\u5176\u5206\u5e03\u9075\u5faaLLM\u6743\u91cd\u7684\u5b9e\u9645\u9ad8\u65af\u5206\u5e03\uff1b\u8bbe\u8ba1\u81ea\u5b9a\u4e49CUDA\u5185\u6838\uff0c\u652f\u6301\u76f4\u63a5\u5728SBVR\u683c\u5f0f\u4e0b\u8fdb\u884c\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u800c\u65e0\u9700\u89e3\u538b\u7f29\u3002", "result": "\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cSBVR\u57284\u4f4d\u91cf\u5316\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u56f0\u60d1\u5ea6\u548c\u51c6\u786e\u7387\u57fa\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u76f8\u6bd4\u539f\u751fFP16\u6a21\u578b\u5b9e\u73b0\u4e862.21-3.04\u500d\u7684\u7aef\u5230\u7aeftoken\u751f\u6210\u52a0\u901f\u3002", "conclusion": "SBVR\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18173", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18173", "abs": "https://arxiv.org/abs/2509.18173", "authors": ["Hongyi Luo", "Qing Cheng", "Daniel Matos", "Hari Krishna Gadi", "Yanfeng Zhang", "Lu Liu", "Yongliang Wang", "Niclas Zeller", "Daniel Cremers", "Liqiu Meng"], "title": "TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route", "comment": "Accepted to EMNLP 2025 (Main). This is the camera-ready/author\n  version", "summary": "Humans can interpret geospatial information through natural language, while\nthe geospatial cognition capabilities of Large Language Models (LLMs) remain\nunderexplored. Prior research in this domain has been constrained by\nnon-quantifiable metrics, limited evaluation datasets and unclear research\nhierarchies. Therefore, we propose a large-scale benchmark and conduct a\ncomprehensive evaluation of the geospatial route cognition of LLMs. We create a\nlarge-scale evaluation dataset comprised of 36000 routes from 12 metropolises\nworldwide. Then, we introduce PathBuilder, a novel tool for converting natural\nlanguage instructions into navigation routes, and vice versa, bridging the gap\nbetween geospatial information and natural language. Finally, we propose a new\nevaluation framework and metrics to rigorously assess 11 state-of-the-art\n(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs\nexhibit limitation to reverse routes: most reverse routes neither return to the\nstarting point nor are similar to the optimal route. Additionally, LLMs face\nchallenges such as low robustness in route generation and high confidence for\ntheir incorrect answers. Code\\ \\&\\ Data available here:\n\\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5730\u7406\u7a7a\u95f4\u8def\u7ebf\u8ba4\u77e5\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5728\u8def\u7ebf\u53cd\u8f6c\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5730\u7406\u7a7a\u95f4\u8ba4\u77e5\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u73b0\u6709\u7814\u7a76\u5b58\u5728\u8bc4\u4f30\u6307\u6807\u4e0d\u53ef\u91cf\u5316\u3001\u6570\u636e\u96c6\u6709\u9650\u548c\u7814\u7a76\u5c42\u6b21\u4e0d\u6e05\u6670\u7b49\u95ee\u9898\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b\u5168\u740312\u4e2a\u5927\u90fd\u5e0236000\u6761\u8def\u7ebf\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86PathBuilder\u5de5\u5177\u5728\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u5bfc\u822a\u8def\u7ebf\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6307\u6807\u6765\u8bc4\u4f3011\u4e2aSOTA LLMs\u7684\u8def\u7ebf\u53cd\u8f6c\u80fd\u529b\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793aLLMs\u5728\u8def\u7ebf\u53cd\u8f6c\u4efb\u52a1\u4e2d\u5b58\u5728\u660e\u663e\u9650\u5236\uff1a\u5927\u591a\u6570\u53cd\u8f6c\u8def\u7ebf\u65e2\u6ca1\u6709\u8fd4\u56de\u8d77\u70b9\uff0c\u4e5f\u4e0d\u63a5\u8fd1\u6700\u4f18\u8def\u7ebf\u3002LLMs\u8fd8\u9762\u4e34\u8def\u7ebf\u751f\u6210\u9c81\u68d2\u6027\u4f4e\u548c\u5bf9\u9519\u8bef\u7b54\u6848\u7f6e\u4fe1\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "conclusion": "LLMs\u5728\u5730\u7406\u7a7a\u95f4\u8def\u7ebf\u8ba4\u77e5\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u7684\u5730\u7406\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.18200", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18200", "abs": "https://arxiv.org/abs/2509.18200", "authors": ["Yu Ti Huang"], "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought", "comment": null, "summary": "Conversational agents must translate egocentric utterances (e.g., \"on my\nright\") into allocentric orientations (N/E/S/W). This challenge is particularly\ncritical in indoor or complex facilities where GPS signals are weak and\ndetailed maps are unavailable. While chain-of-thought (CoT) prompting has\nadvanced reasoning in language and vision tasks, its application to multimodal\nspatial orientation remains underexplored. We introduce Conversational\nOrientation Reasoning (COR), a new benchmark designed for Traditional Chinese\nconversational navigation projected from real-world environments, addressing\negocentric-to-allocentric reasoning in non-English and ASR-transcribed\nscenarios. We propose a multimodal chain-of-thought (MCoT) framework, which\nintegrates ASR-transcribed speech with landmark coordinates through a\nstructured three-step reasoning process: (1) extracting spatial relations, (2)\nmapping coordinates to absolute directions, and (3) inferring user orientation.\nA curriculum learning strategy progressively builds these capabilities on\nTaiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of\nresource-constrained settings. Experiments show that MCoT achieves 100%\norientation accuracy on clean transcripts and 98.1% with ASR transcripts,\nsubstantially outperforming unimodal and non-structured baselines. Moreover,\nMCoT demonstrates robustness under noisy conversational conditions, including\nASR recognition errors and multilingual code-switching. The model also\nmaintains high accuracy in cross-domain evaluation and resilience to linguistic\nvariation, domain shift, and referential ambiguity. These findings highlight\nthe potential of structured MCoT spatial reasoning as a path toward\ninterpretable and resource-efficient embodied navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5bf9\u8bdd\u5f0f\u5bfc\u822a\u4e2d\u7684\u81ea\u6211\u4e2d\u5fc3\u5230\u5f02\u6211\u4e2d\u5fc3\u7684\u7a7a\u95f4\u65b9\u5411\u63a8\u7406\u95ee\u9898\uff0c\u5728\u4f20\u7edf\u4e2d\u6587\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u65b9\u5411\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u5ba4\u5185\u6216\u590d\u6742\u8bbe\u65bd\u4e2dGPS\u4fe1\u53f7\u5f31\u3001\u8be6\u7ec6\u5730\u56fe\u4e0d\u53ef\u7528\u65f6\uff0c\u5bf9\u8bdd\u4ee3\u7406\u9700\u8981\u5c06\u81ea\u6211\u4e2d\u5fc3\u8868\u8ff0\u8f6c\u6362\u4e3a\u5f02\u6211\u4e2d\u5fc3\u65b9\u5411\u7684\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u975e\u82f1\u8bed\u548cASR\u8f6c\u5f55\u573a\u666f\u4e0b\u7684\u7a7a\u95f4\u63a8\u7406\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u6846\u67b6\uff0c\u6574\u5408ASR\u8f6c\u5f55\u8bed\u97f3\u548c\u5730\u6807\u5750\u6807\uff0c\u901a\u8fc7\u4e09\u6b65\u63a8\u7406\u8fc7\u7a0b\uff1a\u63d0\u53d6\u7a7a\u95f4\u5173\u7cfb\u3001\u5750\u6807\u6620\u5c04\u5230\u7edd\u5bf9\u65b9\u5411\u3001\u63a8\u65ad\u7528\u6237\u671d\u5411\uff0c\u5e76\u5728Taiwan-LLM-13B-v2.0-Chat\u6a21\u578b\u4e0a\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "result": "MCoT\u5728\u5e72\u51c0\u8f6c\u5f55\u672c\u4e0a\u8fbe\u5230100%\u65b9\u5411\u51c6\u786e\u7387\uff0cASR\u8f6c\u5f55\u672c\u4e0a\u8fbe\u523098.1%\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u975e\u7ed3\u6784\u5316\u57fa\u7ebf\uff0c\u5e76\u5728\u566a\u58f0\u5bf9\u8bdd\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316MCoT\u7a7a\u95f4\u63a8\u7406\u4e3a\u53ef\u89e3\u91ca\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u5177\u8eab\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u5728\u591a\u8bed\u8a00\u3001\u566a\u58f0\u73af\u5883\u4e0b\u7684\u5f3a\u5927\u9002\u5e94\u6027\u3002"}}
{"id": "2509.18208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18208", "abs": "https://arxiv.org/abs/2509.18208", "authors": ["Boyuan Zhang", "Yingjun Du", "Xiantong Zhen", "Ling Shao"], "title": "Variational Task Vector Composition", "comment": null, "summary": "Task vectors capture how a model changes during fine-tuning by recording the\ndifference between pre-trained and task-specific weights. The composition of\ntask vectors, a key operator in task arithmetic, enables models to integrate\nknowledge from multiple tasks without incurring additional inference costs. In\nthis paper, we propose variational task vector composition, where composition\ncoefficients are taken as latent variables and estimated in a Bayesian\ninference framework. Unlike previous methods that operate at the task level,\nour framework focuses on sample-specific composition. Motivated by the\nobservation of structural redundancy in task vectors, we introduce a\nSpike-and-Slab prior that promotes sparsity and preserves only the most\ninformative components. To further address the high variance and sampling\ninefficiency in sparse, high-dimensional spaces, we develop a gated sampling\nmechanism that constructs a controllable posterior by filtering the composition\ncoefficients based on both uncertainty and importance. This yields a more\nstable and interpretable variational framework by deterministically selecting\nreliable task components, reducing sampling variance while improving\ntransparency and generalization. Experimental results demonstrate that our\nmethod consistently outperforms existing approaches across all datasets by\nselectively leveraging the most reliable and informative components in task\nvectors. These findings highlight the practical value of our approach,\nestablishing a new standard for efficient and effective task vector\ncomposition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u53d8\u5206\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u65b9\u6cd5\uff0c\u5c06\u7ec4\u5408\u7cfb\u6570\u4f5c\u4e3a\u9690\u53d8\u91cf\uff0c\u5728\u8d1d\u53f6\u65af\u63a8\u7406\u6846\u67b6\u4e0b\u8fdb\u884c\u4f30\u8ba1\uff0c\u5b9e\u73b0\u6837\u672c\u7279\u5b9a\u7684\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u3002", "motivation": "\u4f20\u7edf\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u65b9\u6cd5\u5728\u4efb\u52a1\u7ea7\u522b\u64cd\u4f5c\uff0c\u800c\u89c2\u5bdf\u5230\u4efb\u52a1\u5411\u91cf\u5b58\u5728\u7ed3\u6784\u5197\u4f59\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u6837\u672c\u7279\u5b9a\u7ec4\u5408\u673a\u5236\u3002", "method": "\u5f15\u5165Spike-and-Slab\u5148\u9a8c\u4fc3\u8fdb\u7a00\u758f\u6027\uff0c\u5f00\u53d1\u95e8\u63a7\u91c7\u6837\u673a\u5236\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u548c\u91cd\u8981\u6027\u8fc7\u6ee4\u7ec4\u5408\u7cfb\u6570\uff0c\u6784\u5efa\u53ef\u63a7\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5229\u7528\u4efb\u52a1\u5411\u91cf\u4e2d\u6700\u53ef\u9760\u548c\u4fe1\u606f\u4e30\u5bcc\u7684\u7ec4\u4ef6\u5b9e\u73b0\u66f4\u597d\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u6709\u6548\u7684\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18353", "abs": "https://arxiv.org/abs/2509.18353", "authors": ["Jakub Adamczyk", "Jakub Poziemski", "Franciszek Job", "Mateusz Kr\u00f3l", "Maciej Makowski"], "title": "MolPILE - large-scale, diverse dataset for molecular representation learning", "comment": null, "summary": "The size, diversity, and quality of pretraining datasets critically determine\nthe generalization ability of foundation models. Despite their growing\nimportance in chemoinformatics, the effectiveness of molecular representation\nlearning has been hindered by limitations in existing small molecule datasets.\nTo address this gap, we present MolPILE, large-scale, diverse, and rigorously\ncurated collection of 222 million compounds, constructed from 6 large-scale\ndatabases using an automated curation pipeline. We present a comprehensive\nanalysis of current pretraining datasets, highlighting considerable\nshortcomings for training ML models, and demonstrate how retraining existing\nmodels on MolPILE yields improvements in generalization performance. This work\nprovides a standardized resource for model training, addressing the pressing\nneed for an ImageNet-like dataset in molecular chemistry.", "AI": {"tldr": "MolPILE\u662f\u4e00\u4e2a\u5305\u542b2.22\u4ebf\u4e2a\u5316\u5408\u7269\u7684\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u3001\u4e25\u683c\u7b5b\u9009\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\u73b0\u6709\u6570\u636e\u96c6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u5316\u5b66\u4fe1\u606f\u5b66\u63d0\u4f9b\u7c7b\u4f3cImageNet\u7684\u6807\u51c6\u5316\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u5c0f\u5206\u5b50\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u4e0a\u7684\u5c40\u9650\u6027\u963b\u788d\u4e86\u5206\u5b50\u8868\u793a\u5b66\u4e60\u7684\u6548\u679c\uff0c\u9700\u8981\u6784\u5efa\u66f4\u9ad8\u8d28\u91cf\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6765\u63d0\u5347\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4ece6\u4e2a\u5927\u578b\u6570\u636e\u5e93\u901a\u8fc7\u81ea\u52a8\u5316\u7b5b\u9009\u6d41\u7a0b\u6784\u5efaMolPILE\u6570\u636e\u96c6\uff0c\u5305\u542b2.22\u4ebf\u4e2a\u5316\u5408\u7269\uff0c\u5e76\u8fdb\u884c\u5168\u9762\u7684\u6570\u636e\u96c6\u5206\u6790\u3002", "result": "\u5728MolPILE\u4e0a\u91cd\u65b0\u8bad\u7ec3\u73b0\u6709\u6a21\u578b\u80fd\u591f\u663e\u8457\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "MolPILE\u4e3a\u5206\u5b50\u5316\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bad\u7ec3\u8d44\u6e90\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u5bf9\u9ad8\u8d28\u91cf\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u8feb\u5207\u9700\u6c42\u3002"}}
{"id": "2509.18362", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18362", "abs": "https://arxiv.org/abs/2509.18362", "authors": ["Yuxuan Cai", "Xiaozhuan Liang", "Xinghua Wang", "Jin Ma", "Haijin Liang", "Jinwen Luo", "Xinyu Zuo", "Lisheng Duan", "Yuyang Yin", "Xi Chen"], "title": "FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction", "comment": null, "summary": "As large language models (LLMs) become increasingly powerful, the sequential\nnature of autoregressive generation creates a fundamental throughput bottleneck\nthat limits the practical deployment. While Multi-Token Prediction (MTP) has\ndemonstrated remarkable benefits for model training efficiency and performance,\nits inherent potential for inference acceleration remains largely unexplored.\nThis paper introduces FastMTP, a simple yet effective method that improves\nmulti-step draft quality by aligning MTP training with its inference pattern,\nsignificantly enhancing speculative decoding performance. Our approach\nfine-tunes a single MTP head with position-shared weights on self-distilled\ndata, enabling it to capture dependencies among consecutive future tokens and\nmaintain high acceptance rates across multiple recursive draft steps. By\nintegrating language-aware dynamic vocabulary compression into the MTP head, we\nfurther reduce computational overhead in the drafting process. Experimental\nresults across seven diverse benchmarks demonstrate that FastMTP achieves an\naverage of 2.03x speedup compared to standard next token prediction with\nlossless output quality, outperforming vanilla MTP by 82%. FastMTP requires\nonly lightweight training and seamlessly integrates with existing inference\nframeworks, offering a practical and rapidly deployable solution for\naccelerating LLM inference.", "AI": {"tldr": "FastMTP\u662f\u4e00\u79cd\u901a\u8fc7\u8c03\u6574\u591a\u4ee4\u724c\u9884\u6d4b\u8bad\u7ec3\u4e0e\u63a8\u7406\u6a21\u5f0f\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u63a8\u6d4b\u89e3\u7801\u6027\u80fd\uff0c\u5b9e\u73b0LLM\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u81ea\u56de\u5f52\u751f\u6210\u7684\u987a\u5e8f\u6027\u9020\u6210\u4e86\u541e\u5410\u91cf\u74f6\u9888\uff0c\u9650\u5236\u4e86LLM\u7684\u5b9e\u9645\u90e8\u7f72\u3002\u591a\u4ee4\u724c\u9884\u6d4b\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u63a8\u7406\u52a0\u901f\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u4f4d\u7f6e\u5171\u4eab\u6743\u91cd\u5728\u81ea\u84b8\u998f\u6570\u636e\u4e0a\u5fae\u8c03\u5355\u4e2aMTP\u5934\uff0c\u4f7f\u5176\u80fd\u591f\u6355\u6349\u8fde\u7eed\u672a\u6765\u4ee4\u724c\u7684\u4f9d\u8d56\u5173\u7cfb\uff1b\u96c6\u6210\u8bed\u8a00\u611f\u77e5\u52a8\u6001\u8bcd\u6c47\u538b\u7f29\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u4e03\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFastMTP\u76f8\u6bd4\u6807\u51c6\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u5b9e\u73b0\u4e862.03\u500d\u7684\u5e73\u5747\u52a0\u901f\uff0c\u8f93\u51fa\u8d28\u91cf\u65e0\u635f\uff0c\u6027\u80fd\u4f18\u4e8e\u666e\u901aMTP 82%\u3002", "conclusion": "FastMTP\u4ec5\u9700\u8f7b\u91cf\u7ea7\u8bad\u7ec3\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u63a8\u7406\u6846\u67b6\u4e2d\uff0c\u4e3a\u52a0\u901fLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u5feb\u901f\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18367", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18367", "abs": "https://arxiv.org/abs/2509.18367", "authors": ["Zhuoyu Yao", "Yue Wang", "Songyang Zhang", "Yingshu Li", "Zhipeng Cai", "Zhi Tian"], "title": "Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data", "comment": null, "summary": "Recent advances in distributed swarm learning (DSL) offer a promising\nparadigm for edge Internet of Things. Such advancements enhance data privacy,\ncommunication efficiency, energy saving, and model scalability. However, the\npresence of non-independent and identically distributed (non-i.i.d.) data pose\na significant challenge for multi-access edge computing, degrading learning\nperformance and diverging training behavior of vanilla DSL. Further, there\nstill lacks theoretical guidance on how data heterogeneity affects model\ntraining accuracy, which requires thorough investigation. To fill the gap, this\npaper first study the data heterogeneity by measuring the impact of non-i.i.d.\ndatasets under the DSL framework. This then motivates a new multi-worker\nselection design for DSL, termed M-DSL algorithm, which works effectively with\ndistributed heterogeneous data. A new non-i.i.d. degree metric is introduced\nand defined in this work to formulate the statistical difference among local\ndatasets, which builds a connection between the measure of data heterogeneity\nand the evaluation of DSL performance. In this way, our M-DSL guides effective\nselection of multiple works who make prominent contributions for global model\nupdates. We also provide theoretical analysis on the convergence behavior of\nour M-DSL, followed by extensive experiments on different heterogeneous\ndatasets and non-i.i.d. data settings. Numerical results verify performance\nimprovement and network intelligence enhancement provided by our M-DSL beyond\nthe benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5de5\u4f5c\u8005\u9009\u62e9\u7b97\u6cd5M-DSL\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u7fa4\u5b66\u4e60\u4e2d\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u6311\u6218\uff0c\u901a\u8fc7\u5f15\u5165\u975ei.i.d.\u5ea6\u5ea6\u91cf\u6765\u91cf\u5316\u6570\u636e\u5f02\u8d28\u6027\u5e76\u6307\u5bfc\u5de5\u4f5c\u8005\u9009\u62e9\u3002", "motivation": "\u5206\u5e03\u5f0f\u7fa4\u5b66\u4e60\u9762\u4e34\u975ei.i.d.\u6570\u636e\u7684\u6311\u6218\uff0c\u8fd9\u4f1a\u964d\u4f4e\u5b66\u4e60\u6027\u80fd\u5e76\u4f7f\u8bad\u7ec3\u884c\u4e3a\u53d1\u6563\u3002\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u6570\u636e\u5f02\u8d28\u6027\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u51c6\u786e\u6027\u7684\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u9996\u5148\u7814\u7a76\u6570\u636e\u5f02\u8d28\u6027\uff0c\u6d4b\u91cf\u975ei.i.d.\u6570\u636e\u96c6\u5728DSL\u6846\u67b6\u4e0b\u7684\u5f71\u54cd\u3002\u7136\u540e\u63d0\u51faM-DSL\u7b97\u6cd5\uff0c\u5f15\u5165\u65b0\u7684\u975ei.i.d.\u5ea6\u5ea6\u91cf\u6765\u5236\u5b9a\u672c\u5730\u6570\u636e\u96c6\u95f4\u7684\u7edf\u8ba1\u5dee\u5f02\uff0c\u5efa\u7acb\u6570\u636e\u5f02\u8d28\u6027\u5ea6\u91cf\u548cDSL\u6027\u80fd\u8bc4\u4f30\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "result": "\u901a\u8fc7\u5728\u4e0d\u540c\u5f02\u6784\u6570\u636e\u96c6\u548c\u975ei.i.d.\u6570\u636e\u8bbe\u7f6e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86M-DSL\u5728\u6027\u80fd\u6539\u8fdb\u548c\u7f51\u7edc\u667a\u80fd\u589e\u5f3a\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "M-DSL\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5206\u5e03\u5f0f\u5f02\u6784\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u8d85\u8d8a\u57fa\u51c6\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u5e76\u4e3a\u5206\u5e03\u5f0f\u7fa4\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u8d28\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2509.18376", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.18376", "abs": "https://arxiv.org/abs/2509.18376", "authors": ["Burouj Armgaan", "Eshan Jain", "Harsh Pandey", "Mahesh Chandran", "Sayan Ranu"], "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability", "comment": "31 pages, 20 figures, NeurIPS 2025 (Oral)", "summary": "Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GnnXemplar\uff0c\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u8303\u4f8b\u7406\u8bba\u7684\u5168\u5c40\u89e3\u91ca\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u7684\u9884\u6d4b\u51b3\u7b56\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u56fe\u6570\u636e\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709GNN\u5168\u5c40\u89e3\u91ca\u65b9\u6cd5\u4f9d\u8d56\u5c0f\u56fe\u4e2d\u7684\u6a21\u5f0f\u53d1\u73b0\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u56fe\u6570\u636e\u4e2d\u5931\u6548\uff0c\u56e0\u4e3a\u5b50\u56fe\u91cd\u590d\u7f55\u89c1\u3001\u8282\u70b9\u5c5e\u6027\u9ad8\u7ef4\u4e14\u9884\u6d4b\u6765\u81ea\u590d\u6742\u7684\u7ed3\u6784-\u5c5e\u6027\u4ea4\u4e92\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u7684\u5168\u5c40\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "GnnXemplar\u5728GNN\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8bc6\u522b\u4ee3\u8868\u6027\u8282\u70b9\uff08\u8303\u4f8b\uff09\uff0c\u5e76\u901a\u8fc7\u4ece\u8fd9\u4e9b\u8282\u70b9\u7684\u90bb\u57df\u63a8\u5bfc\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u6765\u89e3\u91ca\u9884\u6d4b\u3002\u8303\u4f8b\u9009\u62e9\u88ab\u5efa\u6a21\u4e3a\u53cd\u5411k\u8fd1\u90bb\u7684\u8986\u76d6\u6700\u5927\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u8d2a\u5fc3\u7b97\u6cd5\u8fd1\u4f3c\u6c42\u89e3\u3002\u4f7f\u7528LLM\u7684\u81ea\u4f18\u5316\u63d0\u793a\u7b56\u7565\u751f\u6210\u53ef\u89e3\u91ca\u89c4\u5219\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGnnXemplar\u5728\u4fdd\u771f\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u4eba\u7c7b\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7528\u6237\u7814\u7a76\uff0860\u540d\u53c2\u4e0e\u8005\uff09\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "GnnXemplar\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684GNN\u5168\u5c40\u89e3\u91ca\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8303\u4f8b\u7406\u8bba\u548cLLM\u6280\u672f\uff0c\u80fd\u591f\u5904\u7406\u5927\u89c4\u6a21\u590d\u6742\u56fe\u6570\u636e\u7684\u89e3\u91ca\u9700\u6c42\u3002"}}
{"id": "2509.18386", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18386", "abs": "https://arxiv.org/abs/2509.18386", "authors": ["Jonathan Kabala Mbuya", "Dieter Pfoser", "Antonios Anastasopoulos"], "title": "Graph Enhanced Trajectory Anomaly Detection", "comment": null, "summary": "Trajectory anomaly detection is essential for identifying unusual and\nunexpected movement patterns in applications ranging from intelligent\ntransportation systems to urban safety and fraud prevention.\n  Existing methods only consider limited aspects of the trajectory nature and\nits movement space by treating trajectories as sequences of sampled locations,\nwith sampling determined by positioning technology, e.g., GPS, or by high-level\nabstractions such as staypoints. Trajectories are analyzed in Euclidean space,\nneglecting the constraints and connectivity information of the underlying\nmovement network, e.g., road or transit networks.\n  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework\ntightly integrates road network topology, segment semantics, and historical\ntravel patterns to model trajectory data. GETAD uses a Graph Attention Network\nto learn road-aware embeddings that capture both physical attributes and\ntransition behavior, and augments these with graph-based positional encodings\nthat reflect the spatial layout of the road network.\n  A Transformer-based decoder models sequential movement, while a\nmultiobjective loss function combining autoregressive prediction and supervised\nlink prediction ensures realistic and structurally coherent representations.\n  To improve the robustness of anomaly detection, we introduce Confidence\nWeighted Negative Log Likelihood (CW NLL), an anomaly scoring function that\nemphasizes high-confidence deviations.\n  Experiments on real-world and synthetic datasets demonstrate that GETAD\nachieves consistent improvements over existing methods, particularly in\ndetecting subtle anomalies in road-constrained environments. These results\nhighlight the benefits of incorporating graph structure and contextual\nsemantics into trajectory modeling, enabling more precise and context-aware\nanomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86GETAD\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u9053\u8def\u7f51\u7edc\u62d3\u6251\u3001\u8def\u6bb5\u8bed\u4e49\u548c\u5386\u53f2\u51fa\u884c\u6a21\u5f0f\u6765\u68c0\u6d4b\u8f68\u8ff9\u5f02\u5e38\uff0c\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u5b66\u4e60\u9053\u8def\u611f\u77e5\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7Transformer\u89e3\u7801\u5668\u5efa\u6a21\u5e8f\u5217\u79fb\u52a8\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u8003\u8651\u8f68\u8ff9\u7684\u6709\u9650\u65b9\u9762\uff0c\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u5206\u6790\u8f68\u8ff9\uff0c\u5ffd\u7565\u4e86\u5e95\u5c42\u79fb\u52a8\u7f51\u7edc\uff08\u5982\u9053\u8def\u6216\u4ea4\u901a\u7f51\u7edc\uff09\u7684\u7ea6\u675f\u548c\u8fde\u901a\u6027\u4fe1\u606f\u3002", "method": "GETAD\u6846\u67b6\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u5b66\u4e60\u9053\u8def\u611f\u77e5\u5d4c\u5165\uff0c\u7ed3\u5408\u57fa\u4e8e\u56fe\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u91c7\u7528Transformer\u89e3\u7801\u5668\u5efa\u6a21\u5e8f\u5217\u79fb\u52a8\uff0c\u5e76\u4f7f\u7528\u7ed3\u5408\u81ea\u56de\u5f52\u9884\u6d4b\u548c\u76d1\u7763\u94fe\u63a5\u9884\u6d4b\u7684\u591a\u76ee\u6807\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGETAD\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u9053\u8def\u7ea6\u675f\u73af\u5883\u4e2d\u7684\u7ec6\u5fae\u5f02\u5e38\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5c06\u56fe\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u8bed\u4e49\u6574\u5408\u5230\u8f68\u8ff9\u5efa\u6a21\u4e2d\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u7cbe\u786e\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2509.18389", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18389", "abs": "https://arxiv.org/abs/2509.18389", "authors": ["Jiuqi Wang", "Rohan Chandra", "Shangtong Zhang"], "title": "Towards Provable Emergence of In-Context Reinforcement Learning", "comment": "NeurIPS 2025, 28 pages", "summary": "Typically, a modern reinforcement learning (RL) agent solves a task by\nupdating its neural network parameters to adapt its policy to the task.\nRecently, it has been observed that some RL agents can solve a wide range of\nnew out-of-distribution tasks without parameter updates after pretraining on\nsome task distribution. When evaluated in a new task, instead of making\nparameter updates, the pretrained agent conditions its policy on additional\ninput called the context, e.g., the agent's interaction history in the new\ntask. The agent's performance increases as the information in the context\nincreases, with the agent's parameters fixed. This phenomenon is typically\ncalled in-context RL (ICRL). The pretrained parameters of the agent network\nenable the remarkable ICRL phenomenon. However, many ICRL works perform the\npretraining with standard RL algorithms. This raises the central question this\npaper aims to address: Why can the RL pretraining algorithm generate network\nparameters that enable ICRL? We hypothesize that the parameters capable of ICRL\nare minimizers of the pretraining loss. This work provides initial support for\nthis hypothesis through a case study. In particular, we prove that when a\nTransformer is pretrained for policy evaluation, one of the global minimizers\nof the pretraining loss can enable in-context temporal difference learning.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e3a\u4ec0\u4e48\u5f3a\u5316\u5b66\u4e60\u9884\u8bad\u7ec3\u7b97\u6cd5\u80fd\u591f\u751f\u6210\u652f\u6301\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684\u7f51\u7edc\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u652f\u6301\u8fd9\u4e00\u5047\u8bbe\u3002", "motivation": "\u7814\u7a76\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u73b0\u8c61\u80cc\u540e\u7684\u673a\u5236\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48\u9884\u8bad\u7ec3\u7684\u53c2\u6570\u80fd\u591f\u5728\u65e0\u9700\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u5f53Transformer\u7f51\u7edc\u9884\u8bad\u7ec3\u7528\u4e8e\u7b56\u7565\u8bc4\u4f30\u65f6\uff0c\u9884\u8bad\u7ec3\u635f\u5931\u7684\u5168\u5c40\u6700\u5c0f\u503c\u80fd\u591f\u652f\u6301\u4e0a\u4e0b\u6587\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u635f\u5931\u7684\u5168\u5c40\u6700\u5c0f\u503c\u786e\u5b9e\u80fd\u591f\u4f7f\u7f51\u7edc\u5177\u5907\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u8868\u660e\u9884\u8bad\u7ec3\u53c2\u6570\u7684\u6700\u4f18\u89e3\u662f\u5b9e\u73b0\u8fd9\u4e00\u73b0\u8c61\u7684\u5173\u952e\u3002"}}
{"id": "2509.18396", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18396", "abs": "https://arxiv.org/abs/2509.18396", "authors": ["Do\u011fay Alt\u0131nel"], "title": "Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules", "comment": "24 pages", "summary": "Deep learning optimizers are optimization algorithms that enable deep neural\nnetworks to learn. The effectiveness of learning is highly dependent on the\noptimizer employed in the training process. Alongside the rapid advancement of\ndeep learning, a wide range of optimizers with different approaches have been\ndeveloped. This study aims to provide a review of various optimizers that have\nbeen proposed and received attention in the literature. From Stochastic\ngradient descent to the most recent ones such as Momentum, AdamW, Sophia, and\nMuon in chronological order, optimizers are examined individually, and their\ndistinctive features are highlighted in the study. The update rule of each\noptimizer is presented in detail, with an explanation of the associated\nconcepts and variables. The techniques applied by these optimizers, their\ncontributions to the optimization process, and their default hyperparameter\nsettings are also discussed. In addition, insights are offered into the open\nchallenges encountered in the optimization of deep learning models. Thus, a\ncomprehensive resource is provided both for understanding the current state of\noptimizers and for identifying potential areas of future development.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u5668\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u6309\u65f6\u95f4\u987a\u5e8f\u5206\u6790\u4e86\u4ece\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u5230\u6700\u65b0\u4f18\u5316\u5668\uff08\u5982Momentum\u3001AdamW\u3001Sophia\u3001Muon\u7b49\uff09\u7684\u53d1\u5c55\u5386\u7a0b\u3001\u6280\u672f\u7279\u70b9\u548c\u66f4\u65b0\u89c4\u5219\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u5668\u5bf9\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u5feb\u901f\u53d1\u5c55\uff0c\u51fa\u73b0\u4e86\u591a\u79cd\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u5316\u5668\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u548c\u603b\u7ed3\u73b0\u6709\u7814\u7a76\u6210\u679c\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u6309\u65f6\u95f4\u987a\u5e8f\u5206\u6790\u5404\u79cd\u4f18\u5316\u5668\u7684\u66f4\u65b0\u89c4\u5219\u3001\u6280\u672f\u7279\u70b9\u3001\u8d85\u53c2\u6570\u8bbe\u7f6e\u53ca\u5176\u5bf9\u4f18\u5316\u8fc7\u7a0b\u7684\u8d21\u732e\u3002", "result": "\u63d0\u4f9b\u4e86\u7406\u89e3\u5f53\u524d\u4f18\u5316\u5668\u53d1\u5c55\u73b0\u72b6\u7684\u5168\u9762\u8d44\u6e90\uff0c\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5404\u79cd\u4f18\u5316\u5668\u7684\u6280\u672f\u7ec6\u8282\u548c\u7279\u70b9\u3002", "conclusion": "\u672c\u6587\u4e3a\u7406\u89e3\u4f18\u5316\u5668\u5f53\u524d\u72b6\u6001\u548c\u8bc6\u522b\u672a\u6765\u53d1\u5c55\u65b9\u5411\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f18\u5316\u4e2d\u9762\u4e34\u7684\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2509.18408", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.18408", "abs": "https://arxiv.org/abs/2509.18408", "authors": ["Sarwan Ali"], "title": "Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations", "comment": "Accepted to CIKM 2025 as Short paper", "summary": "We present a novel information-preserving Chaos Game Representation (CGR)\nmethod, also called Reverse-CGR (R-CGR), for biological sequence analysis that\naddresses the fundamental limitation of traditional CGR approaches - the loss\nof sequence information during geometric mapping. Our method introduces\ncomplete sequence recovery through explicit path encoding combined with\nrational arithmetic precision control, enabling perfect sequence reconstruction\nfrom stored geometric traces. Unlike purely geometric approaches, our\nreversibility is achieved through comprehensive path storage that maintains\nboth positional and character information at each step. We demonstrate the\neffectiveness of R-CGR on biological sequence classification tasks, achieving\ncompetitive performance compared to traditional sequence-based methods while\nproviding interpretable geometric visualizations. The approach generates\nfeature-rich images suitable for deep learning while maintaining complete\nsequence information through explicit encoding, opening new avenues for\ninterpretable bioinformatics analysis where both accuracy and sequence recovery\nare essential.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4fe1\u606f\u4fdd\u7559\u6df7\u6c8c\u6e38\u620f\u8868\u793a\u65b9\u6cd5\uff08R-CGR\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCGR\u65b9\u6cd5\u5728\u51e0\u4f55\u6620\u5c04\u8fc7\u7a0b\u4e2d\u4e22\u5931\u5e8f\u5217\u4fe1\u606f\u7684\u6839\u672c\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u5b8c\u6574\u7684\u5e8f\u5217\u6062\u590d\u3002", "motivation": "\u4f20\u7edfCGR\u65b9\u6cd5\u5728\u751f\u7269\u5e8f\u5217\u5206\u6790\u4e2d\u5b58\u5728\u5e8f\u5217\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u7cbe\u786e\u5e8f\u5217\u6062\u590d\u7684\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u8def\u5f84\u7f16\u7801\u7ed3\u5408\u6709\u7406\u6570\u7b97\u672f\u7cbe\u5ea6\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u7684\u5e8f\u5217\u91cd\u6784\u3002\u8be5\u65b9\u6cd5\u5b58\u50a8\u5b8c\u6574\u7684\u8def\u5f84\u4fe1\u606f\uff0c\u5728\u6bcf\u4e00\u6b65\u90fd\u4fdd\u6301\u4f4d\u7f6e\u548c\u5b57\u7b26\u4fe1\u606f\u3002", "result": "\u5728\u751f\u7269\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u4f20\u7edf\u5e8f\u5217\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u53ef\u89c6\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u751f\u6210\u4e86\u9002\u5408\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81\u4e30\u5bcc\u56fe\u50cf\uff0c\u540c\u65f6\u901a\u8fc7\u663e\u5f0f\u7f16\u7801\u4fdd\u6301\u5b8c\u6574\u7684\u5e8f\u5217\u4fe1\u606f\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.18433", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18433", "abs": "https://arxiv.org/abs/2509.18433", "authors": ["Chang Liu", "Ladda Thiamwong", "Yanjie Fu", "Rui Xie"], "title": "Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors", "comment": "Accepted at ICMLA 2025. 8 pages, 6 figures", "summary": "Utilizing offline reinforcement learning (RL) with real-world clinical data\nis getting increasing attention in AI for healthcare. However, implementation\nposes significant challenges. Defining direct rewards is difficult, and inverse\nRL (IRL) struggles to infer accurate reward functions from expert behavior in\ncomplex environments. Offline RL also encounters challenges in aligning learned\npolicies with observed human behavior in healthcare applications. To address\nchallenges in applying offline RL to physical activity promotion for older\nadults at high risk of falls, based on wearable sensor activity monitoring, we\nintroduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse\nReinforcement Learning (KANDI). By leveraging the flexible function\napproximation in Kolmogorov-Arnold Networks, we estimate reward functions by\nlearning free-living environment behavior from low-fall-risk older adults\n(experts), while diffusion-based policies within an Actor-Critic framework\nprovide a generative approach for action refinement and efficiency in offline\nRL. We evaluate KANDI using wearable activity monitoring data in a two-arm\nclinical trial from our Physio-feedback Exercise Program (PEER) study,\nemphasizing its practical application in a fall-risk intervention program to\npromote physical activity among older adults. Additionally, KANDI outperforms\nstate-of-the-art methods on the D4RL benchmark. These results underscore\nKANDI's potential to address key challenges in offline RL for healthcare\napplications, offering an effective solution for activity promotion\nintervention strategies in healthcare.", "AI": {"tldr": "\u63d0\u51fa\u4e86KANDI\u65b9\u6cd5\uff0c\u7ed3\u5408Kolmogorov-Arnold\u7f51\u7edc\u548c\u6269\u6563\u7b56\u7565\uff0c\u7528\u4e8e\u79bb\u7ebf\u9006\u5f3a\u5316\u5b66\u4e60\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u8001\u5e74\u4eba\u8dcc\u5012\u98ce\u9669\u5e72\u9884\u4e2d\u5e94\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6311\u6218\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u533b\u7597\u4fdd\u5065\u5e94\u7528\u4e2d\u9762\u4e34\u5956\u52b1\u51fd\u6570\u5b9a\u4e49\u56f0\u96be\u548c\u7b56\u7565\u4e0e\u4eba\u7c7b\u884c\u4e3a\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8001\u5e74\u4eba\u8dcc\u5012\u98ce\u9669\u5e72\u9884\u7684\u590d\u6742\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u7528Kolmogorov-Arnold\u7f51\u7edc\u5b66\u4e60\u4f4e\u8dcc\u5012\u98ce\u9669\u8001\u5e74\u4eba\uff08\u4e13\u5bb6\uff09\u7684\u884c\u4e3a\u6765\u4f30\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u540c\u65f6\u5728Actor-Critic\u6846\u67b6\u4e2d\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u8fdb\u884c\u52a8\u4f5c\u4f18\u5316\u3002", "result": "KANDI\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u7684\u8001\u5e74\u4eba\u8dcc\u5012\u98ce\u9669\u5e72\u9884\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u5e94\u7528\u6548\u679c\u3002", "conclusion": "KANDI\u4e3a\u89e3\u51b3\u533b\u7597\u4fdd\u5065\u5e94\u7528\u4e2d\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5065\u5eb7\u4fc3\u8fdb\u5e72\u9884\u7b56\u7565\u3002"}}
{"id": "2509.18445", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.18445", "abs": "https://arxiv.org/abs/2509.18445", "authors": ["Kangzheng Liu", "Leixin Ma"], "title": "MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems", "comment": "9 pages, 7 figures", "summary": "The simulation of complex physical systems using a discretized mesh is a\ncornerstone of applied mechanics, but traditional numerical solvers are often\ncomputationally prohibitive for many-query tasks. While Graph Neural Networks\n(GNNs) have emerged as powerful surrogate models for mesh-based data, their\nstandard autoregressive application for long-term prediction is often plagued\nby error accumulation and instability. To address this, we introduce\nMeshODENet, a general framework that synergizes the spatial reasoning of GNNs\nwith the continuous-time modeling of Neural Ordinary Differential Equations. We\ndemonstrate the framework's effectiveness and versatility on a series of\nchallenging structural mechanics problems, including one- and two-dimensional\nelastic bodies undergoing large, non-linear deformations. The results\ndemonstrate that our approach significantly outperforms baseline models in\nlong-term predictive accuracy and stability, while achieving substantial\ncomputational speed-ups over traditional solvers. This work presents a powerful\nand generalizable approach for developing data-driven surrogates to accelerate\nthe analysis and modeling of complex structural systems.", "AI": {"tldr": "MeshODENet\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6807\u51c6\u81ea\u56de\u5f52GNN\u957f\u671f\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\u7684\u95ee\u9898\uff0c\u5728\u7ed3\u6784\u529b\u5b66\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u6a21\u62df\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u6807\u51c6\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u6a21\u578b\u3002", "method": "\u63d0\u51faMeshODENet\u6846\u67b6\uff0c\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u8fde\u7eed\u65f6\u95f4\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u5904\u7406\u975e\u7ebf\u6027\u5927\u53d8\u5f62\u7ed3\u6784\u529b\u5b66\u95ee\u9898\u3002", "result": "\u5728\u4e00\u7ef4\u548c\u4e8c\u7ef4\u5f39\u6027\u4f53\u5927\u53d8\u5f62\u95ee\u9898\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u76f8\u6bd4\u4f20\u7edf\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "conclusion": "MeshODENet\u4e3a\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u66ff\u4ee3\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u52a0\u901f\u590d\u6742\u7ed3\u6784\u7cfb\u7edf\u7684\u5206\u6790\u548c\u5efa\u6a21\u3002"}}
{"id": "2509.18457", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18457", "abs": "https://arxiv.org/abs/2509.18457", "authors": ["Ebrahim Farahmand", "Reza Rahimi Azghan", "Nooshin Taheri Chatrudi", "Velarie Yaa Ansu-Baidoo", "Eric Kim", "Gautham Krishna Gudur", "Mohit Malu", "Owen Krueger", "Edison Thomaz", "Giulia Pedrielli", "Pavan Turaga", "Hassan Ghasemzadeh"], "title": "GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting", "comment": null, "summary": "This paper proposes GluMind, a transformer-based multimodal framework\ndesigned for continual and long-term blood glucose forecasting. GluMind devises\ntwo attention mechanisms, including cross-attention and multi-scale attention,\nwhich operate in parallel and deliver accurate predictive performance.\nCross-attention effectively integrates blood glucose data with other\nphysiological and behavioral signals such as activity, stress, and heart rate,\naddressing challenges associated with varying sampling rates and their adverse\nimpacts on robust prediction. Moreover, the multi-scale attention mechanism\ncaptures long-range temporal dependencies. To mitigate catastrophic forgetting,\nGluMind incorporates a knowledge retention technique into the transformer-based\nforecasting model. The knowledge retention module not only enhances the model's\nability to retain prior knowledge but also boosts its overall forecasting\nperformance. We evaluate GluMind on the recently released AIREADI dataset,\nwhich contains behavioral and physiological data collected from healthy people,\nindividuals with prediabetes, and those with type 2 diabetes. We examine the\nperformance stability and adaptability of GluMind in learning continuously as\nnew patient cohorts are introduced. Experimental results show that GluMind\nconsistently outperforms other state-of-the-art forecasting models, achieving\napproximately 15% and 9% improvements in root mean squared error (RMSE) and\nmean absolute error (MAE), respectively.", "AI": {"tldr": "GluMind\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u6301\u7eed\u548c\u957f\u671f\u8840\u7cd6\u9884\u6d4b\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u751f\u7406\u548c\u884c\u4e3a\u4fe1\u53f7\uff0c\u5e76\u91c7\u7528\u77e5\u8bc6\u4fdd\u7559\u6280\u672f\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u89e3\u51b3\u8840\u7cd6\u9884\u6d4b\u4e2d\u591a\u6a21\u6001\u4fe1\u53f7\u91c7\u6837\u7387\u4e0d\u540c\u3001\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u56f0\u96be\u4ee5\u53ca\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u884c\u5de5\u4f5c\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6574\u5408\u8840\u7cd6\u6570\u636e\u4e0e\u5176\u4ed6\u751f\u7406\u884c\u4e3a\u4fe1\u53f7\uff1b\u5f15\u5165\u77e5\u8bc6\u4fdd\u7559\u6a21\u5757\u589e\u5f3a\u6a21\u578b\u8bb0\u5fc6\u80fd\u529b\u3002", "result": "\u5728AIREADI\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cGluMind\u5728RMSE\u548cMAE\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f18\u6a21\u578b\u63d0\u5347\u7ea615%\u548c9%\uff0c\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "conclusion": "GluMind\u6846\u67b6\u5728\u6301\u7eed\u8840\u7cd6\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\u7684\u6311\u6218\u3002"}}
{"id": "2509.18470", "categories": ["cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18470", "abs": "https://arxiv.org/abs/2509.18470", "authors": ["Xiaozhou Tan", "Minghui Zhao", "Mattias Cross", "Anton Ragni"], "title": "Discrete-time diffusion-like models for speech synthesis", "comment": null, "summary": "Diffusion models have attracted a lot of attention in recent years. These\nmodels view speech generation as a continuous-time process. For efficient\ntraining, this process is typically restricted to additive Gaussian noising,\nwhich is limiting. For inference, the time is typically discretized, leading to\nthe mismatch between continuous training and discrete sampling conditions.\nRecently proposed discrete-time processes, on the other hand, usually do not\nhave these limitations, may require substantially fewer inference steps, and\nare fully consistent between training/inference conditions. This paper explores\nsome diffusion-like discrete-time processes and proposes some new variants.\nThese include processes applying additive Gaussian noise, multiplicative\nGaussian noise, blurring noise and a mixture of blurring and Gaussian noises.\nThe experimental results suggest that discrete-time processes offer comparable\nsubjective and objective speech quality to their widely popular continuous\ncounterpart, with more efficient and consistent training and inference schemas.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u6269\u6563\u6a21\u578b\u7684\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\u53d8\u4f53\uff0c\u5305\u62ec\u52a0\u6027\u9ad8\u65af\u566a\u58f0\u3001\u4e58\u6027\u9ad8\u65af\u566a\u58f0\u3001\u6a21\u7cca\u566a\u58f0\u53ca\u5176\u6df7\u5408\u5f62\u5f0f\uff0c\u7ed3\u679c\u8868\u660e\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\u5728\u8bed\u97f3\u8d28\u91cf\u4e0a\u4e0e\u8fde\u7eed\u5bf9\u5e94\u7269\u76f8\u5f53\uff0c\u4f46\u5177\u6709\u66f4\u9ad8\u6548\u548c\u4e00\u81f4\u7684\u8bad\u7ec3\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5c06\u8bed\u97f3\u751f\u6210\u89c6\u4e3a\u8fde\u7eed\u65f6\u95f4\u8fc7\u7a0b\uff0c\u5b58\u5728\u8bad\u7ec3\u9650\u5236\uff08\u901a\u5e38\u9650\u4e8e\u52a0\u6027\u9ad8\u65af\u566a\u58f0\uff09\u548c\u8bad\u7ec3/\u91c7\u6837\u6761\u4ef6\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\u53ef\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u4fdd\u6301\u8bad\u7ec3/\u63a8\u7406\u6761\u4ef6\u7684\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u5e76\u63a2\u7d22\u4e86\u591a\u79cd\u6269\u6563\u5f0f\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\u53d8\u4f53\uff1a\u52a0\u6027\u9ad8\u65af\u566a\u58f0\u3001\u4e58\u6027\u9ad8\u65af\u566a\u58f0\u3001\u6a21\u7cca\u566a\u58f0\u4ee5\u53ca\u6a21\u7cca\u4e0e\u9ad8\u65af\u566a\u58f0\u7684\u6df7\u5408\u5f62\u5f0f\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u79bb\u6563\u65f6\u95f4\u6846\u67b6\u4e0b\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u4e0a\u4e0e\u5e7f\u6cdb\u6d41\u884c\u7684\u8fde\u7eed\u5bf9\u5e94\u7269\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u4e00\u81f4\u7684\u8bad\u7ec3\u4e0e\u63a8\u7406\u65b9\u6848\u3002", "conclusion": "\u79bb\u6563\u65f6\u95f4\u6269\u6563\u8fc7\u7a0b\u662f\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u6a21\u578b\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u63d0\u4f9b\u76f8\u5f53\u7684\u8bed\u97f3\u8d28\u91cf\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u8fde\u7eed\u6a21\u578b\u5728\u8bad\u7ec3\u9650\u5236\u548c\u6761\u4ef6\u4e0d\u5339\u914d\u65b9\u9762\u7684\u95ee\u9898\u3002"}}
{"id": "2509.18471", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.18471", "abs": "https://arxiv.org/abs/2509.18471", "authors": ["Mariano Tepper", "Ted Willke"], "title": "Individualized non-uniform quantization for vector search", "comment": null, "summary": "Embedding vectors are widely used for representing unstructured data and\nsearching through it for semantically similar items. However, the large size of\nthese vectors, due to their high-dimensionality, creates problems for modern\nvector search techniques: retrieving large vectors from memory/storage is\nexpensive and their footprint is costly. In this work, we present NVQ\n(non-uniform vector quantization), a new vector compression technique that is\ncomputationally and spatially efficient in the high-fidelity regime. The core\nin NVQ is to use novel parsimonious and computationally efficient\nnonlinearities for building non-uniform vector quantizers. Critically, these\nquantizers are \\emph{individually} learned for each indexed vector. Our\nexperimental results show that NVQ exhibits improved accuracy compared to the\nstate of the art with a minimal computational cost.", "AI": {"tldr": "NVQ\u662f\u4e00\u79cd\u65b0\u7684\u5411\u91cf\u538b\u7f29\u6280\u672f\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u975e\u5747\u5300\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\uff0c\u5728\u9ad8\u4fdd\u771f\u5ea6\u4e0b\u5b9e\u73b0\u8ba1\u7b97\u548c\u7a7a\u95f4\u6548\u7387\u7684\u63d0\u5347", "motivation": "\u9ad8\u7ef4\u5d4c\u5165\u5411\u91cf\u7684\u5927\u5c3a\u5bf8\u7ed9\u73b0\u4ee3\u5411\u91cf\u641c\u7d22\u6280\u672f\u5e26\u6765\u95ee\u9898\uff1a\u4ece\u5185\u5b58/\u5b58\u50a8\u68c0\u7d22\u5927\u5411\u91cf\u6210\u672c\u9ad8\uff0c\u5b58\u50a8\u5360\u7528\u6602\u8d35", "method": "\u4f7f\u7528\u65b0\u9896\u7684\u7b80\u7ea6\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u975e\u7ebf\u6027\u65b9\u6cd5\u6784\u5efa\u975e\u5747\u5300\u5411\u91cf\u91cf\u5316\u5668\uff0c\u5173\u952e\u662f\u4e3a\u6bcf\u4e2a\u7d22\u5f15\u5411\u91cf\u5355\u72ec\u5b66\u4e60\u91cf\u5316\u5668", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aNVQ\u5728\u6700\u5c0f\u8ba1\u7b97\u6210\u672c\u4e0b\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u7387", "conclusion": "NVQ\u662f\u4e00\u79cd\u5728\u9ad8\u4fdd\u771f\u5ea6\u4e0b\u5177\u6709\u6539\u8fdb\u51c6\u786e\u6027\u548c\u6700\u5c0f\u8ba1\u7b97\u6210\u672c\u7684\u5411\u91cf\u538b\u7f29\u6280\u672f"}}
{"id": "2509.18480", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.18480", "abs": "https://arxiv.org/abs/2509.18480", "authors": ["Yuyang Wang", "Jiarui Lu", "Navdeep Jaitly", "Josh Susskind", "Miguel Angel Bautista"], "title": "SimpleFold: Folding Proteins is Simpler than You Think", "comment": "28 pages, 11 figures, 13 tables", "summary": "Protein folding models have achieved groundbreaking results typically via a\ncombination of integrating domain knowledge into the architectural blocks and\ntraining pipelines. Nonetheless, given the success of generative models across\ndifferent but related problems, it is natural to question whether these\narchitectural designs are a necessary condition to build performant models. In\nthis paper, we introduce SimpleFold, the first flow-matching based protein\nfolding model that solely uses general purpose transformer blocks. Protein\nfolding models typically employ computationally expensive modules involving\ntriangular updates, explicit pair representations or multiple training\nobjectives curated for this specific domain. Instead, SimpleFold employs\nstandard transformer blocks with adaptive layers and is trained via a\ngenerative flow-matching objective with an additional structural term. We scale\nSimpleFold to 3B parameters and train it on approximately 9M distilled protein\nstructures together with experimental PDB data. On standard folding benchmarks,\nSimpleFold-3B achieves competitive performance compared to state-of-the-art\nbaselines, in addition SimpleFold demonstrates strong performance in ensemble\nprediction which is typically difficult for models trained via deterministic\nreconstruction objectives. Due to its general-purpose architecture, SimpleFold\nshows efficiency in deployment and inference on consumer-level hardware.\nSimpleFold challenges the reliance on complex domain-specific architectures\ndesigns in protein folding, opening up an alternative design space for future\nprogress.", "AI": {"tldr": "SimpleFold\u662f\u9996\u4e2a\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u86cb\u767d\u8d28\u6298\u53e0\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u901a\u7528Transformer\u6a21\u5757\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u4f9d\u8d56\u590d\u6742\u9886\u57df\u7279\u5b9a\u67b6\u6784\u7684\u8bbe\u8ba1\u7406\u5ff5\u3002", "motivation": "\u8d28\u7591\u86cb\u767d\u8d28\u6298\u53e0\u6a21\u578b\u4e2d\u590d\u6742\u9886\u57df\u7279\u5b9a\u67b6\u6784\u7684\u5fc5\u8981\u6027\uff0c\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u4f7f\u7528\u901a\u7528\u751f\u6210\u6a21\u578b\u67b6\u6784\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6807\u51c6Transformer\u6a21\u5757\u914d\u5408\u81ea\u9002\u5e94\u5c42\uff0c\u901a\u8fc7\u751f\u6210\u6d41\u5339\u914d\u76ee\u6807\u548c\u7ed3\u6784\u9879\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728900\u4e07\u84b8\u998f\u86cb\u767d\u8d28\u7ed3\u6784\u548c\u5b9e\u9a8cPDB\u6570\u636e\u4e0a\u8bad\u7ec330\u4ebf\u53c2\u6570\u6a21\u578b\u3002", "result": "\u5728\u6807\u51c6\u6298\u53e0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSimpleFold-3B\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u57fa\u7ebf\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u5728\u96c6\u6210\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u90e8\u7f72\u548c\u63a8\u7406\u6548\u7387\u9ad8\u3002", "conclusion": "SimpleFold\u6311\u6218\u4e86\u86cb\u767d\u8d28\u6298\u53e0\u5bf9\u590d\u6742\u9886\u57df\u7279\u5b9a\u67b6\u6784\u8bbe\u8ba1\u7684\u4f9d\u8d56\uff0c\u4e3a\u672a\u6765\u8fdb\u5c55\u5f00\u8f9f\u4e86\u66ff\u4ee3\u8bbe\u8ba1\u7a7a\u95f4\u3002"}}
{"id": "2509.18483", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18483", "abs": "https://arxiv.org/abs/2509.18483", "authors": ["Abhijit Sen", "Illya V. Lukin", "Kurt Jacobs", "Lev Kaplan", "Andrii G. Sotnikov", "Denys I. Bondar"], "title": "Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints", "comment": null, "summary": "The prediction of quantum dynamical responses lies at the heart of modern\nphysics. Yet, modeling these time-dependent behaviors remains a formidable\nchallenge because quantum systems evolve in high-dimensional Hilbert spaces,\noften rendering traditional numerical methods computationally prohibitive.\nWhile large language models have achieved remarkable success in sequential\nprediction, quantum dynamics presents a fundamentally different challenge:\nforecasting the entire temporal evolution of quantum systems rather than merely\nthe next element in a sequence. Existing neural architectures such as recurrent\nand convolutional networks often require vast training datasets and suffer from\nspurious oscillations that compromise physical interpretability. In this work,\nwe introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)\naugmented with physics-informed loss functions that enforce the Ehrenfest\ntheorems. Our method achieves superior accuracy with significantly less\ntraining data: it requires only 5.4 percent of the samples (200) compared to\nTemporal Convolution Networks (3,700). We further introduce the Chain of KANs,\na novel architecture that embeds temporal causality directly into the model\ndesign, making it particularly well-suited for time series modeling. Our\nresults demonstrate that physics-informed KANs offer a compelling advantage\nover conventional black-box models, maintaining both mathematical rigor and\nphysical consistency while dramatically reducing data requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKolmogorov Arnold Networks\uff08KANs\uff09\u548c\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u91cf\u5b50\u52a8\u529b\u5b66\u54cd\u5e94\uff0c\u76f8\u6bd4\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u91cf\u5b50\u7cfb\u7edf\u5728\u9ad8\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u6f14\u5316\u4f7f\u5f97\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u5b58\u5728\u865a\u5047\u632f\u8361\u95ee\u9898\uff0c\u5f71\u54cd\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528Kolmogorov Arnold Networks\uff08KANs\uff09\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\uff08\u5f3a\u5236\u6267\u884cEhrenfest\u5b9a\u7406\uff09\uff0c\u5e76\u5f15\u5165Chain of KANs\u67b6\u6784\u76f4\u63a5\u5d4c\u5165\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u4ec5\u9700200\u4e2a\u6837\u672c\uff08\u4f20\u7edfTemporal Convolution Networks\u9700\u89813,700\u4e2a\u6837\u672c\uff09\uff0c\u53735.4%\u7684\u8bad\u7ec3\u6570\u636e\u91cf\uff0c\u5c31\u80fd\u8fbe\u5230\u66f4\u4f18\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7269\u7406\u4fe1\u606fKANs\u76f8\u6bd4\u4f20\u7edf\u9ed1\u76d2\u6a21\u578b\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u6570\u5b66\u4e25\u8c28\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6570\u636e\u9700\u6c42\u3002"}}
{"id": "2509.18499", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18499", "abs": "https://arxiv.org/abs/2509.18499", "authors": ["Rachel Chung", "Pratyush Nidhi Sharma", "Mikko Siponen", "Rohit Vadodaria", "Luke Smith"], "title": "Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models", "comment": "Presented at the Association of Certified Fraud Examiners (ACFE)\n  Research Institute Annual Meeting, Las Vegas, NV, (2024)", "summary": "Money laundering is a critical global issue for financial institutions.\nAutomated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),\ncan be trained to identify illicit transactions in real time. A major issue for\ndeveloping such models is the lack of access to training data due to privacy\nand confidentiality concerns. Synthetically generated data that mimics the\nstatistical properties of real data but preserves privacy and confidentiality\nhas been proposed as a solution. However, training AML models on purely\nsynthetic datasets presents its own set of challenges. This article proposes\nthe use of hybrid datasets to augment the utility of synthetic datasets by\nincorporating publicly available, easily accessible, and real-world features.\nThese additions demonstrate that hybrid datasets not only preserve privacy but\nalso improve model utility, offering a practical pathway for financial\ninstitutions to enhance AML systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6df7\u5408\u6570\u636e\u96c6\u6765\u589e\u5f3a\u5408\u6210\u6570\u636e\u96c6\u5728\u53cd\u6d17\u94b1\uff08AML\uff09\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6548\u7528\uff0c\u901a\u8fc7\u7ed3\u5408\u516c\u5f00\u53ef\u7528\u7684\u771f\u5b9e\u4e16\u754c\u7279\u5f81\uff0c\u65e2\u4fdd\u62a4\u9690\u79c1\u53c8\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u53cd\u6d17\u94b1\u662f\u91d1\u878d\u673a\u6784\u9762\u4e34\u7684\u5168\u7403\u6027\u5173\u952e\u95ee\u9898\uff0c\u4f46\u5f00\u53d1\u81ea\u52a8\u5316AML\u6a21\u578b\u65f6\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\uff0c\u56e0\u4e3a\u9690\u79c1\u548c\u4fdd\u5bc6\u95ee\u9898\u9650\u5236\u4e86\u771f\u5b9e\u6570\u636e\u7684\u8bbf\u95ee\u3002\u5408\u6210\u6570\u636e\u867d\u7136\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u7eaf\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3AML\u6a21\u578b\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u6df7\u5408\u6570\u636e\u96c6\u65b9\u6cd5\uff0c\u5c06\u5408\u6210\u6570\u636e\u4e0e\u516c\u5f00\u53ef\u7528\u3001\u6613\u4e8e\u83b7\u53d6\u7684\u771f\u5b9e\u4e16\u754c\u7279\u5f81\u76f8\u7ed3\u5408\uff0c\u4ee5\u589e\u5f3a\u5408\u6210\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\u3002", "result": "\u6df7\u5408\u6570\u636e\u96c6\u4e0d\u4ec5\u80fd\u591f\u4fdd\u62a4\u9690\u79c1\uff0c\u8fd8\u80fd\u63d0\u9ad8\u6a21\u578b\u6548\u7528\uff0c\u4e3a\u91d1\u878d\u673a\u6784\u589e\u5f3aAML\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002", "conclusion": "\u6df7\u5408\u6570\u636e\u96c6\u65b9\u6cd5\u4e3a\u89e3\u51b3AML\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u9690\u79c1\u548c\u6548\u7528\u5e73\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18521", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18521", "abs": "https://arxiv.org/abs/2509.18521", "authors": ["Yuzhen Zhou", "Jiajun Li", "Yusheng Su", "Gowtham Ramesh", "Zilin Zhu", "Xiang Long", "Chenyang Zhao", "Jin Pan", "Xiaodong Yu", "Ze Wang", "Kangrui Du", "Jialian Wu", "Ximeng Sun", "Jiang Liu", "Qiaolin Yu", "Hao Chen", "Zicheng Liu", "Emad Barsoum"], "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation", "comment": null, "summary": "Reinforcement learning (RL) has become a cornerstone in advancing large-scale\npre-trained language models (LLMs). Successive generations, including GPT-o\nseries, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale\nRL training to enhance reasoning and coding capabilities. To meet the\ncommunity's growing RL needs, numerous RL frameworks have been proposed. Most\nof these frameworks primarily rely on inference engines for rollout generation\nand training engines for policy updates. However, RL training remains\ncomputationally expensive, with rollout generation accounting for more than 90%\nof total runtime. In addition, its efficiency is often constrained by the\nlong-tail distribution of rollout response lengths, where a few lengthy\nresponses stall entire batches, leaving GPUs idle and underutilized. As model\nand rollout sizes continue to grow, this bottleneck increasingly limits\nscalability. To address this challenge, we propose Active Partial Rollouts in\nReinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the\nrollout phase, APRIL over-provisions rollout requests, terminates once the\ntarget number of responses is reached, and recycles incomplete responses for\ncontinuation in future steps. This strategy ensures that no rollouts are\ndiscarded while substantially reducing GPU idle time. Experiments show that\nAPRIL improves rollout throughput by at most 44% across commonly used RL\nalgorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%\nhigher final accuracy across tasks. Moreover, APRIL is both framework and\nhardware agnostic, already integrated into the slime RL framework, and\ndeployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies\nsystem-level and algorithmic considerations in proposing APRIL, with the aim of\nadvancing RL training efficiency and inspiring further optimizations in RL\nsystems.", "AI": {"tldr": "APRIL\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u52a8\u90e8\u5206rollout\u7b56\u7565\u89e3\u51b3\u957f\u5c3e\u54cd\u5e94\u5206\u5e03\u5bfc\u81f4\u7684GPU\u7a7a\u95f2\u95ee\u9898\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5f53\u524dRL\u8bad\u7ec3\u4e2drollout\u751f\u6210\u536090%\u4ee5\u4e0a\u8fd0\u884c\u65f6\u95f4\uff0c\u4e14\u957f\u5c3e\u54cd\u5e94\u5206\u5e03\u5bfc\u81f4GPU\u7a7a\u95f2\u548c\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "APRIL\u5728rollout\u9636\u6bb5\u8d85\u989d\u914d\u7f6e\u8bf7\u6c42\uff0c\u8fbe\u5230\u76ee\u6807\u54cd\u5e94\u6570\u91cf\u540e\u7ec8\u6b62\uff0c\u5e76\u5c06\u672a\u5b8c\u6210\u7684\u54cd\u5e94\u56de\u6536\u7528\u4e8e\u540e\u7eed\u6b65\u9aa4\u7ee7\u7eed\u5904\u7406\u3002", "result": "APRIL\u5c06rollout\u541e\u5410\u91cf\u6700\u591a\u63d0\u534744%\uff0c\u52a0\u901f\u6536\u655b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6700\u591a8%\u7684\u6700\u7ec8\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "APRIL\u7edf\u4e00\u4e86\u7cfb\u7edf\u7ea7\u548c\u7b97\u6cd5\u7ea7\u8003\u8651\uff0c\u53ef\u63d0\u9ad8RL\u8bad\u7ec3\u6548\u7387\uff0c\u4e14\u4e0e\u6846\u67b6\u548c\u786c\u4ef6\u65e0\u5173\uff0c\u5df2\u96c6\u6210\u5230slime RL\u6846\u67b6\u4e2d\u3002"}}
{"id": "2509.18529", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2509.18529", "abs": "https://arxiv.org/abs/2509.18529", "authors": ["Mingqian Ma"], "title": "Reverse-Complement Consistency for DNA Language Models", "comment": null, "summary": "A fundamental property of DNA is that the reverse complement (RC) of a\nsequence often carries identical biological meaning. However, state-of-the-art\nDNA language models frequently fail to capture this symmetry, producing\ninconsistent predictions for a sequence and its RC counterpart, which\nundermines their reliability. In this work, we introduce Reverse-Complement\nConsistency Regularization (RCCR), a simple and model-agnostic fine-tuning\nobjective that directly penalizes the divergence between a model's prediction\non a sequence and the aligned prediction on its reverse complement. We evaluate\nRCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,\nDNABERT-2) on a wide range of genomic tasks, including sequence classification,\nscalar regression, and profile prediction. Our experiments show that RCCR\nsubstantially improves RC robustness by dramatically reducing prediction flips\nand errors, all while maintaining or improving task accuracy compared to\nbaselines such as RC data augmentation and test-time averaging. By integrating\na key biological prior directly into the learning process, RCCR produces a\nsingle, intrinsically robust, and computationally efficient model fine-tuning\nrecipe for diverse biology tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53cd\u5411\u4e92\u8865\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08RCCR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5aDNA\u5e8f\u5217\u4e0e\u5176\u53cd\u5411\u4e92\u8865\u5e8f\u5217\u9884\u6d4b\u5dee\u5f02\uff0c\u63d0\u5347DNA\u8bed\u8a00\u6a21\u578b\u7684\u751f\u7269\u5b66\u5bf9\u79f0\u6027\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u73b0\u6709DNA\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u65e0\u6cd5\u6355\u6349DNA\u5e8f\u5217\u4e0e\u5176\u53cd\u5411\u4e92\u8865\u5e8f\u5217\u7684\u751f\u7269\u5b66\u5bf9\u79f0\u6027\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165RCCR\u6b63\u5219\u5316\u76ee\u6807\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u60e9\u7f5a\u6a21\u578b\u5bf9\u5e8f\u5217\u53ca\u5176\u53cd\u5411\u4e92\u8865\u5e8f\u5217\u9884\u6d4b\u7684\u5dee\u5f02\uff0c\u8be5\u65b9\u6cd5\u4e0e\u6a21\u578b\u67b6\u6784\u65e0\u5173\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u9aa8\u5e72\u7f51\u7edc\u548c\u591a\u79cd\u57fa\u56e0\u7ec4\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0cRCCR\u663e\u8457\u63d0\u5347\u4e86\u53cd\u5411\u4e92\u8865\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u9884\u6d4b\u7ffb\u8f6c\u548c\u9519\u8bef\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "RCCR\u901a\u8fc7\u5c06\u5173\u952e\u751f\u7269\u5b66\u5148\u9a8c\u76f4\u63a5\u6574\u5408\u5230\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u4e3a\u591a\u6837\u5316\u751f\u7269\u5b66\u4efb\u52a1\u63d0\u4f9b\u4e86\u5355\u4e00\u3001\u5185\u5728\u9c81\u68d2\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u6a21\u578b\u5fae\u8c03\u65b9\u6848\u3002"}}
{"id": "2509.18542", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18542", "abs": "https://arxiv.org/abs/2509.18542", "authors": ["Qi Wang", "Hanyang Peng", "Yue Yu"], "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) models enable scalable performance by activating\nlarge parameter sets sparsely, minimizing computational overhead. To circumvent\nthe prohibitive cost of training MoEs from scratch, recent work employs\nupcycling, reusing a single pre-trained dense model by replicating its\nfeed-forward network (FFN) layers into experts. However, this limits expert\ndiversity, as all experts originate from a single pre-trained dense model. This\npaper addresses this limitation by constructing powerful MoE models using\nexperts sourced from multiple identically-architected but disparate pre-trained\nmodels (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact\nthat these source models occupy disparate, dissonant regions of the parameter\nspace, making direct upcycling prone to severe performance degradation. To\novercome this, we propose Symphony-MoE, a novel two-stage framework designed to\nharmonize these models into a single, coherent expert mixture. First, we\nestablish this harmony in a training-free manner: we construct a shared\nbackbone via a layer-aware fusion strategy and, crucially, alleviate parameter\nmisalignment among experts using activation-based functional alignment.\nSubsequently, a single lightweight stage of router training coordinates the\nentire architecture. Experiments demonstrate that our method successfully\nintegrates experts from heterogeneous sources, achieving an MoE model that\nsignificantly surpasses baselines in multi-domain tasks and out-of-distribution\ngeneralization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Symphony-MoE\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u5f02\u6784\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e13\u5bb6\u6765\u6784\u5efa\u66f4\u5f3a\u5927\u7684MoE\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfupcycling\u65b9\u6cd5\u4e13\u5bb6\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfMoE\u6a21\u578b\u901a\u8fc7\u590d\u5236\u5355\u4e2a\u9884\u8bad\u7ec3\u5bc6\u96c6\u6a21\u578b\u7684FFN\u5c42\u6765\u6784\u5efa\u4e13\u5bb6\uff0c\u9650\u5236\u4e86\u4e13\u5bb6\u591a\u6837\u6027\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u591a\u4e2a\u5f02\u6784\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u6784\u5efa\u66f4\u5f3a\u5927\u7684MoE\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u8bad\u7ec3\u81ea\u7531\u9636\u6bb5\uff1a\u901a\u8fc7\u5c42\u611f\u77e5\u878d\u5408\u7b56\u7565\u6784\u5efa\u5171\u4eab\u9aa8\u5e72\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6fc0\u6d3b\u7684\u529f\u80fd\u5bf9\u9f50\u7f13\u89e3\u53c2\u6570\u9519\u4f4d\uff1b2\uff09\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u8bad\u7ec3\u9636\u6bb5\u534f\u8c03\u6574\u4e2a\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6210\u529f\u6574\u5408\u4e86\u5f02\u6784\u6e90\u4e13\u5bb6\uff0c\u5728\u591a\u9879\u4efb\u52a1\u548c\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Symphony-MoE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6e90\u4e13\u5bb6\u878d\u5408\u7684\u6311\u6218\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684MoE\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18552", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18552", "abs": "https://arxiv.org/abs/2509.18552", "authors": ["Kiril Bangachev", "Guy Bresler", "Iliyas Noman", "Yury Polyanskiy"], "title": "Global Minimizers of Sigmoid Contrastive Loss", "comment": "Author names listed in alphabetical order. NeurIPS 2025", "summary": "The meta-task of obtaining and aligning representations through contrastive\npretraining is steadily gaining importance since its introduction in CLIP and\nALIGN. In this paper we theoretically explain the advantages of synchronizing\nwith trainable inverse temperature and bias under the sigmoid loss, as\nimplemented in the recent SigLIP and SigLIP2 models of Google DeepMind.\nTemperature and bias can drive the loss function to zero for a rich class of\nconfigurations that we call $(\\mathsf{m},\n\\mathsf{b}_{\\mathsf{rel}})$-Constellations. $(\\mathsf{m},\n\\mathsf{b}_{\\mathsf{rel}})$-Constellations are a novel combinatorial object\nrelated to spherical codes and are parametrized by a margin $\\mathsf{m}$ and\nrelative bias $\\mathsf{b}_{\\mathsf{rel}}$. We use our characterization of\nconstellations to theoretically justify the success of SigLIP on retrieval, to\nexplain the modality gap present in SigLIP, and to identify the necessary\ndimension for producing high-quality representations. Finally, we propose a\nreparameterization of the sigmoid loss with explicit relative bias, which\nimproves training dynamics in experiments with synthetic data.", "AI": {"tldr": "\u672c\u6587\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86SigLIP\u548cSigLIP2\u6a21\u578b\u4e2d\u53ef\u8bad\u7ec3\u9006\u6e29\u5ea6\u548c\u504f\u7f6e\u53c2\u6570\u5728sigmoid\u635f\u5931\u51fd\u6570\u4e0b\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86(m, b_rel)-Constellations\u6982\u5ff5\u6765\u89e3\u91ca\u6a21\u578b\u6210\u529f\u7684\u539f\u56e0\u3002", "motivation": "\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u5728CLIP\u548cALIGN\u7b49\u6a21\u578b\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u4ece\u7406\u8bba\u4e0a\u89e3\u91caSigLIP\u6a21\u578b\u4e2d\u6e29\u5ea6\u548c\u504f\u7f6e\u53c2\u6570\u540c\u6b65\u8bad\u7ec3\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa(m, b_rel)-Constellations\u8fd9\u4e00\u65b0\u7684\u7ec4\u5408\u5bf9\u8c61\uff0c\u7528\u4e8e\u7406\u8bba\u5206\u6790sigmoid\u635f\u5931\u51fd\u6570\u7684\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u5e26\u6709\u663e\u5f0f\u76f8\u5bf9\u504f\u7f6e\u7684sigmoid\u635f\u5931\u91cd\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u89e3\u91ca\u4e86SigLIP\u5728\u68c0\u7d22\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u3001\u6a21\u6001\u95f4\u9699\u7684\u5b58\u5728\u4ee5\u53ca\u4ea7\u751f\u9ad8\u8d28\u91cf\u8868\u793a\u6240\u9700\u7684\u5fc5\u8981\u7ef4\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5e26\u6709\u663e\u5f0f\u76f8\u5bf9\u504f\u7f6e\u7684sigmoid\u635f\u5931\u91cd\u53c2\u6570\u5316\u80fd\u591f\u6539\u5584\u8bad\u7ec3\u52a8\u6001\u3002"}}
{"id": "2509.18568", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18568", "abs": "https://arxiv.org/abs/2509.18568", "authors": ["Niharika Tewari", "Nguyen Linh Dan Le", "Mujie Liu", "Jing Ren", "Ziqi Xu", "Tabinda Sarwar", "Veeky Baths", "Feng Xia"], "title": "Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia", "comment": null, "summary": "Dementia is a progressive neurodegenerative disorder with multiple\netiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal\ndementia, and vascular dementia. Its clinical and biological heterogeneity\nmakes diagnosis and subtype differentiation highly challenging. Graph Neural\nNetworks (GNNs) have recently shown strong potential in modeling brain\nconnectivity, but their limited robustness, data scarcity, and lack of\ninterpretability constrain clinical adoption. Explainable Graph Neural Networks\n(XGNNs) have emerged to address these barriers by combining graph-based\nlearning with interpretability, enabling the identification of disease-relevant\nbiomarkers, analysis of brain network disruptions, and provision of transparent\ninsights for clinicians. This paper presents the first comprehensive review\ndedicated to XGNNs in dementia research. We examine their applications across\nAlzheimer's disease, Parkinson's disease, mild cognitive impairment, and\nmulti-disease diagnosis. A taxonomy of explainability methods tailored for\ndementia-related tasks is introduced, alongside comparisons of existing models\nin clinical scenarios. We also highlight challenges such as limited\ngeneralizability, underexplored domains, and the integration of Large Language\nModels (LLMs) for early detection. By outlining both progress and open\nproblems, this review aims to guide future work toward trustworthy, clinically\nmeaningful, and scalable use of XGNNs in dementia research.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u53ef\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u75f4\u5446\u75c7\u7814\u7a76\u4e2d\u7684\u9996\u6b21\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u5e15\u91d1\u68ee\u75c5\u7b49\u591a\u79cd\u75f4\u5446\u4e9a\u578b\u7684\u8bca\u65ad\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u75f4\u5446\u75c7\u4efb\u52a1\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5206\u7c7b\u4f53\u7cfb\u3002", "motivation": "\u75f4\u5446\u75c7\u5177\u6709\u4e34\u5e8a\u548c\u751f\u7269\u5b66\u5f02\u8d28\u6027\uff0c\u8bca\u65ad\u548c\u4e9a\u578b\u533a\u5206\u6781\u5177\u6311\u6218\u6027\u3002\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u9c81\u68d2\u6027\u5dee\u3001\u6570\u636e\u7a00\u7f3a\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u91c7\u7528\u3002\u53ef\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u7ed3\u5408\u56fe\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u8bc6\u522b\u75be\u75c5\u76f8\u5173\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u900f\u660e\u89c1\u89e3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9\u53ef\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u75f4\u5446\u75c7\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u5168\u9762\u5206\u6790\uff0c\u5305\u62ec\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u5e15\u91d1\u68ee\u75c5\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u548c\u591a\u75be\u75c5\u8bca\u65ad\u7b49\u573a\u666f\u3002\u63d0\u51fa\u4e86\u4e13\u95e8\u9488\u5bf9\u75f4\u5446\u75c7\u4efb\u52a1\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u5bf9\u73b0\u6709\u6a21\u578b\u8fdb\u884c\u4e34\u5e8a\u573a\u666f\u6bd4\u8f83\u3002", "result": "\u603b\u7ed3\u4e86\u53ef\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u75f4\u5446\u75c7\u7814\u7a76\u4e2d\u7684\u8fdb\u5c55\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5982\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3001\u672a\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\uff0c\u4ee5\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65e9\u671f\u68c0\u6d4b\u4e2d\u7684\u6574\u5408\u673a\u4f1a\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u65e8\u5728\u6307\u5bfc\u672a\u6765\u7814\u7a76\u671d\u7740\u53ef\u4fe1\u8d56\u3001\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u4e14\u53ef\u6269\u5c55\u7684\u53ef\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u75f4\u5446\u75c7\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u65b9\u5411\u53d1\u5c55\uff0c\u4e3a\u89e3\u51b3\u75f4\u5446\u75c7\u8bca\u65ad\u548c\u6cbb\u7597\u7684\u4e34\u5e8a\u6311\u6218\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2509.18573", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18573", "abs": "https://arxiv.org/abs/2509.18573", "authors": ["Dong Chen", "Jian Liu", "Chun-Long Chen", "Guo-Wei Wei"], "title": "Interaction Topological Transformer for Multiscale Learning in Porous Materials", "comment": "4 figures, 2 tables", "summary": "Porous materials exhibit vast structural diversity and support critical\napplications in gas storage, separations, and catalysis. However, predictive\nmodeling remains challenging due to the multiscale nature of structure-property\nrelationships, where performance is governed by both local chemical\nenvironments and global pore-network topology. These complexities, combined\nwith sparse and unevenly distributed labeled data, hinder generalization across\nmaterial families. We propose the Interaction Topological Transformer (ITT), a\nunified data-efficient framework that leverages novel interaction topology to\ncapture materials information across multiple scales and multiple levels,\nincluding structural, elemental, atomic, and pairwise-elemental organization.\nITT extracts scale-aware features that reflect both compositional and\nrelational structure within complex porous frameworks, and integrates them\nthrough a built-in Transformer architecture that supports joint reasoning\nacross scales. Trained using a two-stage strategy, i.e., self-supervised\npretraining on 0.6 million unlabeled structures followed by supervised\nfine-tuning, ITT achieves state-of-the-art, accurate, and transferable\npredictions for adsorption, transport, and stability properties. This framework\nprovides a principled and scalable path for learning-guided discovery in\nstructurally and chemically diverse porous materials.", "AI": {"tldr": "\u63d0\u51fa\u4e86Interaction Topological Transformer (ITT)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4ea4\u4e92\u62d3\u6251\u7ed3\u6784\u6765\u9884\u6d4b\u591a\u5b54\u6750\u6599\u7684\u5438\u9644\u3001\u4f20\u8f93\u548c\u7a33\u5b9a\u6027\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u9ad8\u6548\u548c\u53ef\u8fc1\u79fb\u7684\u9884\u6d4b\u3002", "motivation": "\u591a\u5b54\u6750\u6599\u5177\u6709\u7ed3\u6784\u591a\u6837\u6027\uff0c\u4f46\u5728\u9884\u6d4b\u5efa\u6a21\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u7684\u591a\u5c3a\u5ea6\u7279\u6027\u4ee5\u53ca\u6807\u8bb0\u6570\u636e\u7a00\u758f\u4e14\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4ea4\u4e92\u62d3\u6251\u53d8\u6362\u5668(ITT)\u6846\u67b6\uff0c\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff08\u7ed3\u6784\u3001\u5143\u7d20\u3001\u539f\u5b50\u548c\u6210\u5bf9\u5143\u7d20\u7ec4\u7ec7\uff09\uff0c\u901a\u8fc7Transformer\u67b6\u6784\u8fdb\u884c\u8de8\u5c3a\u5ea6\u8054\u5408\u63a8\u7406\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u81ea\u76d1\u7763\u9884\u8bad\u7ec3+\u76d1\u7763\u5fae\u8c03\uff09\u3002", "result": "ITT\u572860\u4e07\u672a\u6807\u8bb0\u7ed3\u6784\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u540e\uff0c\u5728\u5438\u9644\u3001\u4f20\u8f93\u548c\u7a33\u5b9a\u6027\u6027\u80fd\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7ed3\u6784\u591a\u6837\u6027\u548c\u5316\u5b66\u591a\u6837\u6027\u591a\u5b54\u6750\u6599\u7684\u5b66\u4e60\u5f15\u5bfc\u53d1\u73b0\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2509.18584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18584", "abs": "https://arxiv.org/abs/2509.18584", "authors": ["Mingchun Sun", "Rongqiang Zhao", "Jie Liu"], "title": "DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation", "comment": null, "summary": "Diffusion models are the mainstream approach for time series generation\ntasks. However, existing diffusion models for time series generation require\nretraining the entire framework to introduce specific conditional guidance.\nThere also exists a certain degree of distributional bias between the generated\ndata and the real data, which leads to potential model biases in downstream\ntasks. Additionally, the complexity of diffusion models and the latent spaces\nleads to an uninterpretable inference process. To address these issues, we\npropose the data style-guided diffusion model (DS-Diffusion). In the\nDS-Diffusion, a diffusion framework based on style-guided kernels is developed\nto avoid retraining for specific conditions. The time-information based\nhierarchical denoising mechanism (THD) is developed to reduce the\ndistributional bias between the generated data and the real data. Furthermore,\nthe generated samples can clearly indicate the data style from which they\noriginate. We conduct comprehensive evaluations using multiple public datasets\nto validate our approach. Experimental results show that, compared to the\nstate-of-the-art model such as ImagenTime, the predictive score and the\ndiscriminative score decrease by 5.56% and 61.55%, respectively. The\ndistributional bias between the generated data and the real data is further\nreduced, the inference process is also more interpretable. Moreover, by\neliminating the need to retrain the diffusion model, the flexibility and\nadaptability of the model to specific conditions are also enhanced.", "AI": {"tldr": "DS-Diffusion\u662f\u4e00\u79cd\u6570\u636e\u98ce\u683c\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u98ce\u683c\u5f15\u5bfc\u6838\u548c\u5206\u5c42\u53bb\u566a\u673a\u5236\u89e3\u51b3\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6269\u6563\u6a21\u578b\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u3001\u5206\u5e03\u504f\u5dee\u5927\u548c\u63a8\u7406\u8fc7\u7a0b\u4e0d\u53ef\u89e3\u91ca\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6269\u6563\u6a21\u578b\u9700\u8981\u4e3a\u7279\u5b9a\u6761\u4ef6\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6846\u67b6\uff0c\u751f\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u5206\u5e03\u504f\u5dee\uff0c\u4e14\u63a8\u7406\u8fc7\u7a0b\u4e0d\u53ef\u89e3\u91ca\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u98ce\u683c\u5f15\u5bfc\u6838\u7684\u6269\u6563\u6846\u67b6\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\uff0c\u5f00\u53d1\u57fa\u4e8e\u65f6\u95f4\u4fe1\u606f\u7684\u5206\u5c42\u53bb\u566a\u673a\u5236(THD)\u51cf\u5c11\u5206\u5e03\u504f\u5dee\uff0c\u751f\u6210\u6837\u672c\u80fd\u6e05\u6670\u663e\u793a\u6570\u636e\u6765\u6e90\u98ce\u683c\u3002", "result": "\u76f8\u6bd4ImagenTime\u7b49\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u9884\u6d4b\u5206\u6570\u548c\u5224\u522b\u5206\u6570\u5206\u522b\u964d\u4f4e5.56%\u548c61.55%\uff0c\u5206\u5e03\u504f\u5dee\u8fdb\u4e00\u6b65\u51cf\u5c0f\uff0c\u63a8\u7406\u8fc7\u7a0b\u66f4\u53ef\u89e3\u91ca\u3002", "conclusion": "DS-Diffusion\u901a\u8fc7\u6d88\u9664\u91cd\u65b0\u8bad\u7ec3\u9700\u6c42\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u7279\u5b9a\u6761\u4ef6\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.18607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18607", "abs": "https://arxiv.org/abs/2509.18607", "authors": ["Qiuhai Zeng", "Sarvesh Rajkumar", "Di Wang", "Narendra Gyanchandani", "Wenbo Yan"], "title": "Reflect before Act: Proactive Error Correction in Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ninteractive decision-making tasks, but existing methods often struggle with\nerror accumulation and lack robust self-correction mechanisms. We introduce\n\"Reflect before Act\" (REBACT), a novel approach that enhances LLM-based\ndecision-making by introducing a critical reflect step prior to taking the next\naction. This approach allows for immediate error correction, ensuring smooth\naction path and adaptibity to environment feedback. We evaluate REBACT on three\ndiverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results\ndemonstrate that REBACT significantly outperforms strong baselines, improving\nsuccess rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld\n(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using\nClaude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's\nperformance improvements are achieved with only a few modification steps,\ndemonstrating its computational efficiency.", "AI": {"tldr": "REBACT\u662f\u4e00\u79cd\u589e\u5f3aLLM\u51b3\u7b56\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u91c7\u53d6\u884c\u52a8\u524d\u589e\u52a0\u53cd\u601d\u6b65\u9aa4\u6765\u5b9e\u73b0\u5373\u65f6\u9519\u8bef\u7ea0\u6b63\uff0c\u5728\u591a\u4e2a\u4ea4\u4e92\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709LLM\u51b3\u7b56\u65b9\u6cd5\u5b58\u5728\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\u548c\u7f3a\u4e4f\u9c81\u68d2\u7684\u81ea\u7ea0\u6b63\u673a\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5373\u65f6\u7ea0\u6b63\u9519\u8bef\u3001\u786e\u4fdd\u884c\u52a8\u8def\u5f84\u5e73\u6ed1\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa'\u884c\u52a8\u524d\u53cd\u601d'(REBACT)\u65b9\u6cd5\uff0c\u5728LLM\u91c7\u53d6\u4e0b\u4e00\u4e2a\u884c\u52a8\u4e4b\u524d\u5f15\u5165\u5173\u952e\u7684\u53cd\u601d\u6b65\u9aa4\uff0c\u5141\u8bb8\u57fa\u4e8e\u73af\u5883\u53cd\u9988\u8fdb\u884c\u5373\u65f6\u9519\u8bef\u7ea0\u6b63\u3002", "result": "\u5728ALFWorld\u3001WebShop\u548cTextCraft\u4e09\u4e2a\u4ea4\u4e92\u73af\u5883\u4e2d\uff0cREBACT\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1aWebShop\u6210\u529f\u7387\u63d0\u534724%\u8fbe\u523061%\uff0cALFWorld\u63d0\u53476.72%\u8fbe\u523098.51%\uff0cTextCraft\u63d0\u53470.5%\u8fbe\u523099.5%\u3002", "conclusion": "REBACT\u901a\u8fc7\u5c11\u91cf\u4fee\u6539\u6b65\u9aa4\u5c31\u80fd\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u8ba1\u7b97\u6548\u7387\uff0c\u4e3aLLM\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u7ea0\u6b63\u673a\u5236\u3002"}}
{"id": "2509.18611", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18611", "abs": "https://arxiv.org/abs/2509.18611", "authors": ["Zituo Chen", "Sili Deng"], "title": "Flow marching for a generative PDE foundation model", "comment": null, "summary": "Pretraining on large-scale collections of PDE-governed spatiotemporal\ntrajectories has recently shown promise for building generalizable models of\ndynamical systems. Yet most existing PDE foundation models rely on\ndeterministic Transformer architectures, which lack generative flexibility for\nmany science and engineering applications. We propose Flow Marching, an\nalgorithm that bridges neural operator learning with flow matching motivated by\nan analysis of error accumulation in physical dynamical systems, and we build a\ngenerative PDE foundation model on top of it. By jointly sampling the noise\nlevel and the physical time step between adjacent states, the model learns a\nunified velocity field that transports a noisy current state toward its clean\nsuccessor, reducing long-term rollout drift while enabling uncertainty-aware\nensemble generations. Alongside this core algorithm, we introduce a\nPhysics-Pretrained Variational Autoencoder (P2VAE) to embed physical states\ninto a compact latent space, and an efficient Flow Marching Transformer (FMT)\nthat combines a diffusion-forcing scheme with latent temporal pyramids,\nachieving up to 15x greater computational efficiency than full-length video\ndiffusion models and thereby enabling large-scale pretraining at substantially\nreduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE\nfamilies and train suites of P2VAEs and FMTs at multiple scales. On downstream\nevaluation, we benchmark on unseen Kolmogorov turbulence with few-shot\nadaptation, demonstrate long-term rollout stability over deterministic\ncounterparts, and present uncertainty-stratified ensemble results, highlighting\nthe importance of generative PDE foundation models for real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86Flow Marching\u7b97\u6cd5\uff0c\u5c06\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u4e0e\u6d41\u5339\u914d\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u751f\u6210\u5f0fPDE\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u91c7\u6837\u566a\u58f0\u6c34\u5e73\u548c\u7269\u7406\u65f6\u95f4\u6b65\u957f\u6765\u51cf\u5c11\u957f\u671f\u6eda\u52a8\u6f02\u79fb\u5e76\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u96c6\u6210\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684PDE\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u786e\u5b9a\u6027Transformer\u67b6\u6784\uff0c\u7f3a\u4e4f\u751f\u6210\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8bb8\u591a\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "1) Flow Marching\u7b97\u6cd5\u8054\u5408\u91c7\u6837\u566a\u58f0\u6c34\u5e73\u548c\u7269\u7406\u65f6\u95f4\u6b65\u957f\uff1b2) P2VAE\u5c06\u7269\u7406\u72b6\u6001\u5d4c\u5165\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff1b3) FMT\u7ed3\u5408\u6269\u6563\u5f3a\u5236\u65b9\u6848\u548c\u6f5c\u5728\u65f6\u95f4\u91d1\u5b57\u5854\uff0c\u8ba1\u7b97\u6548\u7387\u6bd4\u5b8c\u6574\u89c6\u9891\u6269\u6563\u6a21\u578b\u9ad815\u500d\u3002", "result": "\u572812\u4e2a\u4e0d\u540cPDE\u5bb6\u65cf\u7684\u7ea6250\u4e07\u6761\u8f68\u8ff9\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u672a\u89c1\u8fc7\u7684Kolmogorov\u6e4d\u6d41\u4e0a\u8fdb\u884c\u4e86\u5c11\u6837\u672c\u9002\u5e94\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u957f\u671f\u6eda\u52a8\u7a33\u5b9a\u6027\u4f18\u4e8e\u786e\u5b9a\u6027\u5bf9\u5e94\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0d\u786e\u5b9a\u6027\u5206\u5c42\u7684\u96c6\u6210\u7ed3\u679c\u3002", "conclusion": "\u751f\u6210\u5f0fPDE\u57fa\u7840\u6a21\u578b\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0cFlow Marching\u65b9\u6cd5\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2509.18629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18629", "abs": "https://arxiv.org/abs/2509.18629", "authors": ["Abel Gurung", "Joseph Campbell"], "title": "HyperAdapt: Simple High-Rank Adaptation", "comment": null, "summary": "Foundation models excel across diverse tasks, but adapting them to\nspecialized applications often requires fine-tuning, an approach that is memory\nand compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate\nthis by updating only a small subset of weights. In this paper, we introduce\nHyperAdapt, a parameter-efficient fine-tuning method that significantly reduces\nthe number of trainable parameters compared to state-of-the-art methods like\nLoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying\nrow- and column-wise scaling through diagonal matrices, thereby inducing a\nhigh-rank update while requiring only $n+m$ trainable parameters for an $n\n\\times m$ matrix. Theoretically, we establish an upper bound on the rank of\nHyperAdapt's updates, and empirically, we confirm that it consistently induces\nhigh-rank transformations across model layers. Experiments on GLUE, arithmetic\nreasoning, and commonsense reasoning benchmarks with models up to 14B\nparameters demonstrate that HyperAdapt matches or nearly matches the\nperformance of full fine-tuning and state-of-the-art PEFT methods while using\norders of magnitude fewer trainable parameters.", "AI": {"tldr": "HyperAdapt\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u884c\u5217\u7f29\u653e\u6280\u672f\u5b9e\u73b0\u9ad8\u79e9\u66f4\u65b0\uff0c\u4ec5\u9700n+m\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\u5373\u53ef\u9002\u914dn\u00d7m\u77e9\u9635\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9002\u5e94\u4e13\u4e1a\u5316\u5e94\u7528\u901a\u5e38\u9700\u8981\u5fae\u8c03\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u901a\u8fc7\u4ec5\u66f4\u65b0\u5c11\u91cf\u6743\u91cd\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HyperAdapt\u901a\u8fc7\u5bf9\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\u5e94\u7528\u884c\u5217\u5bf9\u89d2\u77e9\u9635\u7f29\u653e\uff0c\u8bf1\u5bfc\u9ad8\u79e9\u66f4\u65b0\u3002\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u66f4\u65b0\u79e9\u7684\u4e0a\u754c\uff0c\u5b9e\u9a8c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u80fd\u5728\u6a21\u578b\u5404\u5c42\u4e00\u81f4\u4ea7\u751f\u9ad8\u79e9\u53d8\u6362\u3002", "result": "\u5728GLUE\u3001\u7b97\u672f\u63a8\u7406\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u9ad8\u8fbe140\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0cHyperAdapt\u5728\u6027\u80fd\u4e0a\u5339\u914d\u6216\u63a5\u8fd1\u5168\u91cf\u5fae\u8c03\u53ca\u6700\u5148\u8fdb\u7684PEFT\u65b9\u6cd5\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u6570\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "HyperAdapt\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53c2\u6570\u5fae\u8c03\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u4e3a\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u9002\u5e94\u6027\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18703", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18703", "abs": "https://arxiv.org/abs/2509.18703", "authors": ["Jakub Adamczyk"], "title": "Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology", "comment": null, "summary": "This research focuses on rational pesticide design, using graph machine\nlearning to accelerate the development of safer, eco-friendly agrochemicals,\ninspired by in silico methods in drug discovery. With an emphasis on\necotoxicology, the initial contributions include the creation of ApisTox, the\nlargest curated dataset on pesticide toxicity to honey bees. We conducted a\nbroad evaluation of machine learning (ML) models for molecular graph\nclassification, including molecular fingerprints, graph kernels, GNNs, and\npretrained transformers. The results show that methods successful in medicinal\nchemistry often fail to generalize to agrochemicals, underscoring the need for\ndomain-specific models and benchmarks. Future work will focus on developing a\ncomprehensive benchmarking suite and designing ML models tailored to the unique\nchallenges of pesticide discovery.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u56fe\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u7406\u6027\u519c\u836f\u8bbe\u8ba1\uff0c\u5f00\u53d1\u66f4\u5b89\u5168\u73af\u4fdd\u7684\u519c\u7528\u5316\u5b66\u54c1\uff0c\u501f\u9274\u4e86\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u8ba1\u7b97\u673a\u6a21\u62df\u65b9\u6cd5\u3002", "motivation": "\u53d7\u836f\u7269\u53d1\u73b0\u4e2d\u8ba1\u7b97\u673a\u6a21\u62df\u65b9\u6cd5\u7684\u542f\u53d1\uff0c\u65e8\u5728\u52a0\u901f\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u73af\u4fdd\u7684\u519c\u7528\u5316\u5b66\u54c1\uff0c\u7279\u522b\u5173\u6ce8\u751f\u6001\u6bd2\u7406\u5b66\u3002", "method": "\u521b\u5efa\u4e86\u6700\u5927\u7684\u871c\u8702\u519c\u836f\u6bd2\u6027\u6570\u636e\u96c6ApisTox\uff0c\u5e76\u5e7f\u6cdb\u8bc4\u4f30\u4e86\u5206\u5b50\u56fe\u5206\u7c7b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ec\u5206\u5b50\u6307\u7eb9\u3001\u56fe\u6838\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u9884\u8bad\u7ec3\u53d8\u6362\u5668\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5728\u836f\u7269\u5316\u5b66\u4e2d\u6210\u529f\u7684\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u63a8\u5e7f\u5230\u519c\u7528\u5316\u5b66\u54c1\u9886\u57df\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u9886\u57df\u7279\u5b9a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u4e13\u6ce8\u4e8e\u5f00\u53d1\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5e76\u8bbe\u8ba1\u9488\u5bf9\u519c\u836f\u53d1\u73b0\u72ec\u7279\u6311\u6218\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002"}}
{"id": "2509.18714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18714", "abs": "https://arxiv.org/abs/2509.18714", "authors": ["Zhenyu Tao", "Wei Xu", "Xiaohu You"], "title": "A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications", "comment": "This paper is accepted by the 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "The bisimulation metric (BSM) is a powerful tool for computing state\nsimilarities within a Markov decision process (MDP), revealing that states\ncloser in BSM have more similar optimal value functions. While BSM has been\nsuccessfully utilized in reinforcement learning (RL) for tasks like state\nrepresentation learning and policy exploration, its application to multiple-MDP\nscenarios, such as policy transfer, remains challenging. Prior work has\nattempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis\nof its mathematical properties has limited further theoretical progress. In\nthis work, we formally establish a generalized bisimulation metric (GBSM)\nbetween pairs of MDPs, which is rigorously proven with the three fundamental\nproperties: GBSM symmetry, inter-MDP triangle inequality, and the distance\nbound on identical state spaces. Leveraging these properties, we theoretically\nanalyse policy transfer, state aggregation, and sampling-based estimation in\nMDPs, obtaining explicit bounds that are strictly tighter than those derived\nfrom the standard BSM. Additionally, GBSM provides a closed-form sample\ncomplexity for estimation, improving upon existing asymptotic results based on\nBSM. Numerical results validate our theoretical findings and demonstrate the\neffectiveness of GBSM in multi-MDP scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5e7f\u4e49\u53cc\u6a21\u62df\u5ea6\u91cf\uff08GBSM\uff09\uff0c\u7528\u4e8e\u8ba1\u7b97\u4e24\u4e2a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u4e4b\u95f4\u7684\u72b6\u6001\u76f8\u4f3c\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfBSM\u5728\u591aMDP\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e25\u683c\u8bc1\u660e\u4e86\u5176\u6570\u5b66\u6027\u8d28\u3002", "motivation": "\u4f20\u7edf\u53cc\u6a21\u62df\u5ea6\u91cf\uff08BSM\uff09\u5728\u5355\u4e2aMDP\u5185\u6709\u6548\uff0c\u4f46\u5728\u591aMDP\u573a\u666f\uff08\u5982\u7b56\u7565\u8fc1\u79fb\uff09\u4e2d\u5e94\u7528\u53d7\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5e7f\u4e49BSM\u6570\u5b66\u6027\u8d28\u7684\u4e25\u683c\u5206\u6790\uff0c\u9650\u5236\u4e86\u7406\u8bba\u8fdb\u5c55\u3002", "method": "\u6b63\u5f0f\u5efa\u7acbMDP\u5bf9\u4e4b\u95f4\u7684\u5e7f\u4e49\u53cc\u6a21\u62df\u5ea6\u91cf\uff08GBSM\uff09\uff0c\u4e25\u683c\u8bc1\u660e\u5176\u4e09\u4e2a\u57fa\u672c\u6027\u8d28\uff1a\u5bf9\u79f0\u6027\u3001MDP\u95f4\u4e09\u89d2\u4e0d\u7b49\u5f0f\u548c\u76f8\u540c\u72b6\u6001\u7a7a\u95f4\u4e0a\u7684\u8ddd\u79bb\u754c\u9650\u3002", "result": "GBSM\u5728\u7b56\u7565\u8fc1\u79fb\u3001\u72b6\u6001\u805a\u5408\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u4f30\u8ba1\u7b49\u65b9\u9762\u83b7\u5f97\u4e86\u6bd4\u6807\u51c6BSM\u66f4\u4e25\u683c\u7684\u7406\u8bba\u754c\u9650\uff0c\u5e76\u63d0\u4f9b\u4e86\u95ed\u5f0f\u6837\u672c\u590d\u6742\u5ea6\u4f30\u8ba1\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "GBSM\u4e3a\u591aMDP\u573a\u666f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u5728\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u90fd\u4f18\u4e8e\u4f20\u7edfBSM\u65b9\u6cd5\u3002"}}
{"id": "2509.18719", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18719", "abs": "https://arxiv.org/abs/2509.18719", "authors": ["Bo Qu", "Zhurong Wang", "Daisuke Yagi", "Zhen Xu", "Yang Zhao", "Yinan Shan", "Frank Zahradnik"], "title": "LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection", "comment": "12 pages, 12 figures, ACL 2025 industry track", "summary": "This paper presents a novel approach to e-commerce payment fraud detection by\nintegrating reinforcement learning (RL) with Large Language Models (LLMs). By\nframing transaction risk as a multi-step Markov Decision Process (MDP), RL\noptimizes risk detection across multiple payment stages. Crafting effective\nreward functions, essential for RL model success, typically requires\nsignificant human expertise due to the complexity and variability in design.\nLLMs, with their advanced reasoning and coding capabilities, are well-suited to\nrefine these functions, offering improvements over traditional methods. Our\napproach leverages LLMs to iteratively enhance reward functions, achieving\nbetter fraud detection accuracy and demonstrating zero-shot capability.\nExperiments with real-world data confirm the effectiveness, robustness, and\nresilience of our LLM-enhanced RL framework through long-term evaluations,\nunderscoring the potential of LLMs in advancing industrial RL applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u65b0\u578b\u7535\u5b50\u5546\u52a1\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4ea4\u6613\u98ce\u9669\u5efa\u6a21\u4e3a\u591a\u6b65\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5229\u7528LLM\u8fed\u4ee3\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6b3a\u8bc8\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\u5728\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u4e0a\u9700\u8981\u5927\u91cf\u4eba\u5de5\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800cLLM\u5177\u6709\u5148\u8fdb\u7684\u63a8\u7406\u548c\u7f16\u7801\u80fd\u529b\uff0c\u53ef\u4ee5\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u5c06\u4ea4\u6613\u98ce\u9669\u5efa\u6a21\u4e3a\u591a\u6b65\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u9636\u6bb5\u652f\u4ed8\u98ce\u9669\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7LLM\u8fed\u4ee3\u6539\u8fdb\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u5f39\u6027\uff0c\u957f\u671f\u8bc4\u4f30\u663e\u793aLLM\u589e\u5f3a\u7684RL\u6846\u67b6\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86LLM\u5728\u63a8\u8fdb\u5de5\u4e1a\u7ea7\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u573a\u666f\u4e2d\u3002"}}
{"id": "2509.18744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18744", "abs": "https://arxiv.org/abs/2509.18744", "authors": ["Yuqing Liu"], "title": "Theory of periodic convolutional neural network", "comment": null, "summary": "We introduce a novel convolutional neural network architecture, termed the\n\\emph{periodic CNN}, which incorporates periodic boundary conditions into the\nconvolutional layers. Our main theoretical contribution is a rigorous\napproximation theorem: periodic CNNs can approximate ridge functions depending\non $d-1$ linear variables in a $d$-dimensional input space, while such\napproximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer\nvariables). This result establishes a sharp characterization of the expressive\npower of periodic CNNs. Beyond the theory, our findings suggest that periodic\nCNNs are particularly well-suited for problems where data naturally admits a\nridge-like structure of high intrinsic dimension, such as image analysis on\nwrapped domains, physics-informed learning, and materials science. The work\nthus both expands the mathematical foundation of CNN approximation theory and\nhighlights a class of architectures with surprising and practically relevant\napproximation capabilities.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5468\u671f\u6027CNN\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\uff0c\u8bc1\u660e\u4e86\u5176\u5728d\u7ef4\u8f93\u5165\u7a7a\u95f4\u4e2d\u80fd\u591f\u903c\u8fd1d-1\u4e2a\u7ebf\u6027\u53d8\u91cf\u7684\u810a\u51fd\u6570\uff0c\u800c\u4f4e\u7ef4\u810a\u8bbe\u7f6e\u65e0\u6cd5\u5b9e\u73b0\u8fd9\u79cd\u903c\u8fd1\u3002", "motivation": "\u4f20\u7edfCNN\u5728\u5904\u7406\u5177\u6709\u5468\u671f\u6027\u7ed3\u6784\u7684\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u7136\u5904\u7406\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u7684\u67b6\u6784\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u5206\u6790\u3001\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u548c\u6750\u6599\u79d1\u5b66\u7b49\u9886\u57df\u3002", "method": "\u63d0\u51fa\u5468\u671f\u6027CNN\u67b6\u6784\uff0c\u5c06\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u878d\u5165\u5377\u79ef\u5c42\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u7684\u903c\u8fd1\u5b9a\u7406\u8bc1\u660e\u5176\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5468\u671f\u6027CNN\u80fd\u591f\u903c\u8fd1\u9ad8\u5185\u5728\u7ef4\u5ea6\u7684\u810a\u72b6\u7ed3\u6784\u51fd\u6570\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5728\u4f4e\u7ef4\u8bbe\u7f6e\u4e0b\u65e0\u6cd5\u5b9e\u73b0\u8fd9\u79cd\u903c\u8fd1\u3002", "conclusion": "\u5468\u671f\u6027CNN\u4e0d\u4ec5\u6269\u5c55\u4e86CNN\u903c\u8fd1\u7406\u8bba\u7684\u6570\u5b66\u57fa\u7840\uff0c\u8fd8\u5c55\u793a\u4e86\u4e00\u7c7b\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u7684\u67b6\u6784\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u5468\u671f\u6027\u7ed3\u6784\u7684\u6570\u636e\u5206\u6790\u95ee\u9898\u3002"}}
{"id": "2509.18751", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18751", "abs": "https://arxiv.org/abs/2509.18751", "authors": ["Samuel Yoon", "Jongwon Kim", "Juyoung Ha", "Young Myoung Ko"], "title": "MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model", "comment": null, "summary": "Recently reconstruction-based deep models have been widely used for time\nseries anomaly detection, but as their capacity and representation capability\nincrease, these models tend to over-generalize, often reconstructing unseen\nanomalies accurately. Prior works have attempted to mitigate this by\nincorporating a memory architecture that stores prototypes of normal patterns.\nNevertheless, these approaches suffer from high training costs and have yet to\nbe effectively integrated with time series foundation models (TFMs). To address\nthese challenges, we propose \\textbf{MOMEMTO}, a TFM for anomaly detection,\nenhanced with a patch-based memory module to mitigate over-generalization. The\nmemory module is designed to capture representative normal patterns from\nmultiple domains and enables a single model to be jointly fine-tuned across\nmultiple datasets through a multi-domain training strategy. MOMEMTO initializes\nmemory items with latent representations from a pre-trained encoder, organizes\nthem into patch-level units, and updates them via an attention mechanism. We\nevaluate our method using 23 univariate benchmark datasets. Experimental\nresults demonstrate that MOMEMTO, as a single model, achieves higher scores on\nAUC and VUS metrics compared to baseline methods, and further enhances the\nperformance of its backbone TFM, particularly in few-shot learning scenarios.", "AI": {"tldr": "MOMEMTO\u662f\u4e00\u4e2a\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TFM\uff09\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8865\u4e01\u7ea7\u5185\u5b58\u6a21\u5757\u6765\u7f13\u89e3\u6a21\u578b\u8fc7\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u80fd\u591f\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8054\u5408\u5fae\u8c03\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u91cd\u6784\u7684\u6df1\u5ea6\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5bb9\u6613\u8fc7\u5ea6\u6cdb\u5316\uff0c\u51c6\u786e\u91cd\u6784\u672a\u89c1\u8fc7\u7684\u5f02\u5e38\uff1b\u800c\u73b0\u6709\u5185\u5b58\u67b6\u6784\u65b9\u6cd5\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u4e0e\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u6709\u6548\u96c6\u6210\u3002", "method": "\u63d0\u51faMOMEMTO\u65b9\u6cd5\uff0c\u5305\u542b\u8865\u4e01\u7ea7\u5185\u5b58\u6a21\u5757\uff0c\u4ece\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u521d\u59cb\u5316\u5185\u5b58\u9879\uff0c\u7ec4\u7ec7\u4e3a\u8865\u4e01\u7ea7\u5355\u5143\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u66f4\u65b0\uff0c\u91c7\u7528\u591a\u9886\u57df\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u5355\u4e00\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8054\u5408\u5fae\u8c03\u3002", "result": "\u572823\u4e2a\u5355\u53d8\u91cf\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMOMEMTO\u5728AUC\u548cVUS\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9aa8\u5e72TFM\u7684\u6027\u80fd\u3002", "conclusion": "MOMEMTO\u901a\u8fc7\u5185\u5b58\u6a21\u5757\u6709\u6548\u7f13\u89e3\u4e86\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8fc7\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u591a\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\u7684\u9ad8\u6548\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.18810", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18810", "abs": "https://arxiv.org/abs/2509.18810", "authors": ["Arman Mohammadi", "Mattias Krysander", "Daniel Jung", "Erik Frisk"], "title": "Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems", "comment": null, "summary": "Deep neural networks has been increasingly applied in fault diagnostics,\nwhere it uses historical data\n  to capture systems behavior, bypassing the need for high-fidelity physical\nmodels.\n  However, despite their competence in prediction tasks, these models often\nstruggle with\n  the evaluation of their confidence. This matter is particularly\n  important in consistency-based diagnosis where decision logic is highly\nsensitive to false alarms.\n  To address this challenge, this work presents a diagnostic framework that\nuses\n  ensemble probabilistic machine learning to\n  improve diagnostic characteristics of data driven consistency based diagnosis\n  by quantifying and automating the prediction uncertainty.\n  The proposed method is evaluated across several case studies using both\nablation\n  and comparative analyses, showing consistent improvements across a range of\ndiagnostic metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u6982\u7387\u673a\u5668\u5b66\u4e60\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u548c\u81ea\u52a8\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u6539\u8fdb\u6570\u636e\u9a71\u52a8\u7684\u4e00\u81f4\u6027\u8bca\u65ad\u7279\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6545\u969c\u8bca\u65ad\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5c3d\u7ba1\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd9\u4e9b\u6a21\u578b\u5f80\u5f80\u96be\u4ee5\u8bc4\u4f30\u5176\u7f6e\u4fe1\u5ea6\u3002\u5728\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u8bca\u65ad\u4e2d\uff0c\u51b3\u7b56\u903b\u8f91\u5bf9\u8bef\u62a5\u9ad8\u5ea6\u654f\u611f\uff0c\u56e0\u6b64\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u96c6\u6210\u6982\u7387\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u6539\u8fdb\u6570\u636e\u9a71\u52a8\u7684\u4e00\u81f4\u6027\u8bca\u65ad\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u6d88\u878d\u5206\u6790\u548c\u5bf9\u6bd4\u5206\u6790\u5728\u591a\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5728\u4e00\u7cfb\u5217\u8bca\u65ad\u6307\u6807\u4e0a\u90fd\u6709\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u6570\u636e\u9a71\u52a8\u4e00\u81f4\u6027\u8bca\u65ad\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.18811", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.18811", "abs": "https://arxiv.org/abs/2509.18811", "authors": ["Thomas Savary", "Fran\u00e7ois Rozet", "Gilles Louppe"], "title": "Training-Free Data Assimilation with GenCast", "comment": null, "summary": "Data assimilation is widely used in many disciplines such as meteorology,\noceanography, and robotics to estimate the state of a dynamical system from\nnoisy observations. In this work, we propose a lightweight and general method\nto perform data assimilation using diffusion models pre-trained for emulating\ndynamical systems. Our method builds on particle filters, a class of data\nassimilation algorithms, and does not require any further training. As a\nguiding example throughout this work, we illustrate our methodology on GenCast,\na diffusion-based model that generates global ensemble weather forecasts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u901a\u7528\u6570\u636e\u540c\u5316\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5e94\u7528\u4e8e\u5929\u6c14\u9884\u6d4b\u7b49\u9886\u57df", "motivation": "\u6570\u636e\u540c\u5316\u5728\u6c14\u8c61\u5b66\u3001\u6d77\u6d0b\u5b66\u548c\u673a\u5668\u4eba\u5b66\u7b49\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u590d\u6742\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u9700\u8981\u66f4\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u57fa\u4e8e\u7c92\u5b50\u6ee4\u6ce2\u5668\u7b97\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6570\u636e\u540c\u5316\uff0c\u4ee5GenCast\u5929\u6c14\u9884\u6d4b\u6a21\u578b\u4e3a\u4f8b\u8fdb\u884c\u9a8c\u8bc1", "result": "\u5f00\u53d1\u51fa\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u6570\u636e\u540c\u5316\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u636e\u540c\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u52a8\u6001\u7cfb\u7edf\u4eff\u771f"}}
{"id": "2509.18826", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18826", "abs": "https://arxiv.org/abs/2509.18826", "authors": ["Wenlong Lyu", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "title": "Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective", "comment": "39 pages, 20 figures", "summary": "The well-known graph-based clustering methods, including spectral clustering,\nsymmetric non-negative matrix factorization, and doubly stochastic\nnormalization, can be viewed as relaxations of the kernel $k$-means approach.\nHowever, we posit that these methods excessively relax their inherent low-rank,\nnonnegative, doubly stochastic, and orthonormal constraints to ensure numerical\nfeasibility, potentially limiting their clustering efficacy. In this paper,\nguided by our theoretical analyses, we propose \\textbf{Lo}w-\\textbf{R}ank\n\\textbf{D}oubly stochastic clustering (\\textbf{LoRD}), a model that only\nrelaxes the orthonormal constraint to derive a probabilistic clustering\nresults. Furthermore, we theoretically establish the equivalence between\northogonality and block diagonality under the doubly stochastic constraint. By\nintegrating \\textbf{B}lock diagonal regularization into LoRD, expressed as the\nmaximization of the Frobenius norm, we propose \\textbf{B-LoRD}, which further\nenhances the clustering performance. To ensure numerical solvability, we\ntransform the non-convex doubly stochastic constraint into a linear convex\nconstraint through the introduction of a class probability parameter. We\nfurther theoretically demonstrate the gradient Lipschitz continuity of our LoRD\nand B-LoRD enables the proposal of a globally convergent projected gradient\ndescent algorithm for their optimization. Extensive experiments validate the\neffectiveness of our approaches. The code is publicly available at\nhttps://github.com/lwl-learning/LoRD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LoRD\u548cB-LoRD\u4e24\u79cd\u56fe\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u7ea6\u675f\u677e\u5f1b\u6765\u63d0\u5347\u805a\u7c7b\u6548\u679c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5168\u5c40\u6536\u655b\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u805a\u7c7b\u65b9\u6cd5\uff08\u5982\u8c31\u805a\u7c7b\u3001\u5bf9\u79f0\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u7b49\uff09\u8fc7\u5ea6\u677e\u5f1b\u4e86\u4f4e\u79e9\u3001\u975e\u8d1f\u3001\u53cc\u968f\u673a\u548c\u6b63\u4ea4\u7ea6\u675f\uff0c\u53ef\u80fd\u9650\u5236\u4e86\u805a\u7c7b\u6027\u80fd\u3002", "method": "\u63d0\u51faLoRD\u6a21\u578b\u4ec5\u677e\u5f1b\u6b63\u4ea4\u7ea6\u675f\uff0cB-LoRD\u5728\u6b64\u57fa\u7840\u4e0a\u52a0\u5165\u5757\u5bf9\u89d2\u6b63\u5219\u5316\uff1b\u5c06\u975e\u51f8\u53cc\u968f\u673a\u7ea6\u675f\u8f6c\u5316\u4e3a\u7ebf\u6027\u51f8\u7ea6\u675f\uff0c\u8bbe\u8ba1\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u6b63\u4ea4\u6027\u4e0e\u5757\u5bf9\u89d2\u6027\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u66f4\u7cbe\u786e\u7684\u7ea6\u675f\u5efa\u6a21\u548c\u6709\u6548\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u805a\u7c7b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.18842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18842", "abs": "https://arxiv.org/abs/2509.18842", "authors": ["Nikolas Chatzis", "Ioannis Kordonis", "Manos Theodosis", "Petros Maragos"], "title": "Shared-Weights Extender and Gradient Voting for Neural Network Expansion", "comment": "5 pages, 3 figures", "summary": "Expanding neural networks during training is a promising way to augment\ncapacity without retraining larger models from scratch. However, newly added\nneurons often fail to adjust to a trained network and become inactive,\nproviding no contribution to capacity growth. We propose the Shared-Weights\nExtender (SWE), a novel method explicitly designed to prevent inactivity of new\nneurons by coupling them with existing ones for smooth integration. In\nparallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based\nmethod for allocating neurons across layers during deep network expansion. Our\nextensive benchmarking on four datasets shows that our method can effectively\nsuppress neuron inactivity and achieve better performance compared to other\nexpanding methods and baselines.", "AI": {"tldr": "\u63d0\u51faSWE\u548cSVoD\u65b9\u6cd5\uff0c\u901a\u8fc7\u6743\u91cd\u5171\u4eab\u548c\u68af\u5ea6\u5206\u914d\u7b56\u7565\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u6269\u5c55\u4e2d\u65b0\u795e\u7ecf\u5143\u5931\u6d3b\u95ee\u9898\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027", "motivation": "\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6269\u5c55\u5bb9\u91cf\u65f6\uff0c\u65b0\u52a0\u5165\u7684\u795e\u7ecf\u5143\u5f80\u5f80\u65e0\u6cd5\u9002\u5e94\u5df2\u8bad\u7ec3\u7f51\u7edc\u800c\u53d8\u5f97\u4e0d\u6d3b\u8dc3\uff0c\u65e0\u6cd5\u771f\u6b63\u589e\u52a0\u6a21\u578b\u5bb9\u91cf", "method": "Shared-Weights Extender (SWE) \u901a\u8fc7\u5c06\u65b0\u795e\u7ecf\u5143\u4e0e\u73b0\u6709\u795e\u7ecf\u5143\u8026\u5408\u5b9e\u73b0\u5e73\u6ed1\u96c6\u6210\uff1bSteepest Voting Distributor (SVoD) \u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u7b56\u7565\u5728\u6df1\u5c42\u7f51\u7edc\u6269\u5c55\u4e2d\u5206\u914d\u795e\u7ecf\u5143", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6291\u5236\u795e\u7ecf\u5143\u5931\u6d3b\uff0c\u76f8\u6bd4\u5176\u4ed6\u6269\u5c55\u65b9\u6cd5\u548c\u57fa\u7ebf\u83b7\u5f97\u66f4\u597d\u6027\u80fd", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u6269\u5c55\u4e2d\u7684\u795e\u7ecf\u5143\u5931\u6d3b\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u5bb9\u91cf\u589e\u957f\u548c\u6027\u80fd\u63d0\u5347"}}
{"id": "2509.18851", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18851", "abs": "https://arxiv.org/abs/2509.18851", "authors": ["Gongrui Nan", "Siye Chen", "Jing Huang", "Mengyu Lu", "Dexun Wang", "Chunmei Xie", "Weiqi Xiong", "Xianzhou Zeng", "Qixuan Zhou", "Yadong Li", "Xingzhong Xu"], "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization", "comment": null, "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)\nacross various tasks. However, GRPO, a representative RLVR algorithm, suffers\nfrom a critical limitation: when all responses within a group are either\nentirely correct or entirely incorrect, the model fails to learn from these\nhomogeneous responses. This is particularly problematic for homogeneously\nincorrect groups, where GRPO's advantage function yields a value of zero,\nleading to null gradients and the loss of valuable learning signals. To\novercome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy\nOptimization), an algorithm designed to convert homogeneous errors into robust\nlearning signals. First, NGRPO introduces Advantage Calibration. This mechanism\nhypothesizes the existence of a virtual maximum-reward sample during advantage\ncalculation, thereby altering the mean and variance of rewards within a group\nand ensuring that the advantages for homogeneously incorrect samples are no\nlonger zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the\nupdate magnitude for positive samples while imposing stricter constraints on\nthat of negative samples. This serves to stabilize the exploration pressure\nintroduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B\ndemonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,\nDAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and\nAIME2025. These results validate NGRPO's ability to learn from homogeneous\nerrors, leading to stable and substantial improvements in mathematical\nreasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.", "AI": {"tldr": "NGRPO\u7b97\u6cd5\u901a\u8fc7\u4f18\u52bf\u6821\u51c6\u548c\u975e\u5bf9\u79f0\u526a\u88c1\u89e3\u51b3\u4e86GRPO\u5728\u54cd\u5e94\u7ec4\u5b8c\u5168\u6b63\u786e\u6216\u5b8c\u5168\u9519\u8bef\u65f6\u65e0\u6cd5\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "GRPO\u7b97\u6cd5\u5728\u54cd\u5e94\u7ec4\u5b8c\u5168\u6b63\u786e\u6216\u5b8c\u5168\u9519\u8bef\u65f6\u65e0\u6cd5\u4ea7\u751f\u6709\u6548\u5b66\u4e60\u4fe1\u53f7\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5b8c\u5168\u9519\u8bef\u7684\u54cd\u5e94\u7ec4\uff0c\u4f18\u52bf\u51fd\u6570\u503c\u4e3a\u96f6\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\uff0c\u635f\u5931\u4e86\u5b9d\u8d35\u7684\u5b66\u4e60\u673a\u4f1a", "method": "1. \u4f18\u52bf\u6821\u51c6\uff1a\u5728\u4f18\u52bf\u8ba1\u7b97\u4e2d\u5047\u8bbe\u5b58\u5728\u865a\u62df\u6700\u5927\u5956\u52b1\u6837\u672c\uff0c\u6539\u53d8\u7ec4\u5185\u5956\u52b1\u7684\u5747\u503c\u548c\u65b9\u5dee\uff0c\u786e\u4fdd\u5b8c\u5168\u9519\u8bef\u6837\u672c\u7684\u4f18\u52bf\u503c\u4e0d\u518d\u4e3a\u96f6\uff1b2. \u975e\u5bf9\u79f0\u526a\u88c1\uff1a\u653e\u677e\u6b63\u6837\u672c\u7684\u66f4\u65b0\u5e45\u5ea6\uff0c\u540c\u65f6\u5bf9\u8d1f\u6837\u672c\u65bd\u52a0\u66f4\u4e25\u683c\u7684\u7ea6\u675f\uff0c\u7a33\u5b9a\u63a2\u7d22\u538b\u529b", "result": "\u5728Qwen2.5-Math-7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNGRPO\u5728MATH500\u3001AMC23\u548cAIME2025\u7b49\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8ePPO\u3001GRPO\u3001DAPO\u548cPSR-NSR\u7b49\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "NGRPO\u80fd\u591f\u4ece\u5b8c\u5168\u9519\u8bef\u7684\u54cd\u5e94\u4e2d\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u7a33\u5b9a\u548c\u663e\u8457\u63d0\u5347"}}
{"id": "2509.18893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18893", "abs": "https://arxiv.org/abs/2509.18893", "authors": ["Qinhan Hou", "Yilun Zheng", "Xichun Zhang", "Sitao Luan", "Jing Tang"], "title": "Exploring Heterophily in Graph-level Tasks", "comment": "Accectped by NeurIPS 2025 Workshop, New Perspectives in Advancing\n  Graph Machine Learning (NPGML)", "summary": "While heterophily has been widely studied in node-level tasks, its impact on\ngraph-level tasks remains unclear. We present the first analysis of heterophily\nin graph-level learning, combining theoretical insights with empirical\nvalidation. We first introduce a taxonomy of graph-level labeling schemes, and\nfocus on motif-based tasks within local structure labeling, which is a popular\nlabeling scheme. Using energy-based gradient flow analysis, we reveal a key\ninsight: unlike frequency-dominated regimes in node-level tasks, motif\ndetection requires mixed-frequency dynamics to remain flexible across multiple\nspectral components. Our theory shows that motif objectives are inherently\nmisaligned with global frequency dominance, demanding distinct architectural\nconsiderations. Experiments on synthetic datasets with controlled heterophily\nand real-world molecular property prediction support our findings, showing that\nfrequency-adaptive model outperform frequency-dominated models. This work\nestablishes a new theoretical understanding of heterophily in graph-level\nlearning and offers guidance for designing effective GNN architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5206\u6790\u4e86\u5f02\u8d28\u6027\u5728\u56fe\u7ea7\u5b66\u4e60\u4e2d\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u53d1\u73b0\uff0c\u4e0e\u8282\u70b9\u7ea7\u4efb\u52a1\u4e0d\u540c\uff0c\u56fe\u7ea7\u4efb\u52a1\u4e2d\u7684motif\u68c0\u6d4b\u9700\u8981\u6df7\u5408\u9891\u7387\u52a8\u6001\uff0c\u800c\u975e\u9891\u7387\u4e3b\u5bfc\u673a\u5236\u3002", "motivation": "\u867d\u7136\u5f02\u8d28\u6027\u5728\u8282\u70b9\u7ea7\u4efb\u52a1\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u5bf9\u56fe\u7ea7\u4efb\u52a1\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5efa\u7acb\u56fe\u7ea7\u5b66\u4e60\u4e2d\u5f02\u8d28\u6027\u7684\u7406\u8bba\u7406\u89e3\u3002", "method": "\u5f15\u5165\u56fe\u7ea7\u6807\u7b7e\u65b9\u6848\u7684\u5206\u7c7b\u6cd5\uff0c\u805a\u7126\u4e8e\u5c40\u90e8\u7ed3\u6784\u6807\u7b7e\u4e2d\u7684motif\u4efb\u52a1\uff1b\u4f7f\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u68af\u5ea6\u6d41\u5206\u6790\u63ed\u793amotif\u68c0\u6d4b\u7684\u52a8\u6001\u7279\u6027\uff1b\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660emotif\u76ee\u6807\u4e0e\u5168\u5c40\u9891\u7387\u4e3b\u5bfc\u673a\u5236\u5b58\u5728\u56fa\u6709\u9519\u4f4d\uff1b\u5b9e\u9a8c\u8bc1\u660e\u9891\u7387\u81ea\u9002\u5e94\u6a21\u578b\u4f18\u4e8e\u9891\u7387\u4e3b\u5bfc\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u56fe\u7ea7\u5b66\u4e60\u4e2d\u7684\u5f02\u8d28\u6027\u5efa\u7acb\u4e86\u65b0\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u6709\u6548\u7684GNN\u67b6\u6784\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.18904", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18904", "abs": "https://arxiv.org/abs/2509.18904", "authors": ["Zhaoxin Wang", "Handing Wang", "Cong Tian", "Yaochu Jin"], "title": "Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction", "comment": null, "summary": "Federated learning allows multiple participants to collaboratively train a\ncentral model without sharing their private data. However, this distributed\nnature also exposes new attack surfaces. In particular, backdoor attacks allow\nattackers to implant malicious behaviors into the global model while\nmaintaining high accuracy on benign inputs. Existing attacks usually rely on\nfixed patterns or adversarial perturbations as triggers, which tightly couple\nthe main and backdoor tasks. This coupling makes them vulnerable to dilution by\nhonest updates and limits their persistence under federated defenses. In this\nwork, we propose an approach to decouple the backdoor task from the main task\nby dynamically optimizing the backdoor trigger within a min-max framework. The\ninner layer maximizes the performance gap between poisoned and benign samples,\nensuring that the contributions of benign users have minimal impact on the\nbackdoor. The outer process injects the adaptive triggers into the local model.\nWe evaluate our method on both computer vision and natural language tasks, and\ncompare it with six backdoor attack methods under six defense algorithms.\nExperimental results show that our method achieves good attack performance and\ncan be easily integrated into existing backdoor attack techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u52a8\u6001\u4f18\u5316\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f-\u6700\u5927\u6846\u67b6\u89e3\u8026\u4e3b\u4efb\u52a1\u548c\u540e\u95e8\u4efb\u52a1\uff0c\u63d0\u9ad8\u653b\u51fb\u7684\u6301\u4e45\u6027\u548c\u9690\u853d\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u7279\u6027\u66b4\u9732\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u73b0\u6709\u540e\u95e8\u653b\u51fb\u4f9d\u8d56\u56fa\u5b9a\u6a21\u5f0f\u6216\u5bf9\u6297\u6027\u6270\u52a8\u4f5c\u4e3a\u89e6\u53d1\u5668\uff0c\u5bfc\u81f4\u4e3b\u4efb\u52a1\u548c\u540e\u95e8\u4efb\u52a1\u7d27\u5bc6\u8026\u5408\uff0c\u5bb9\u6613\u88ab\u8bda\u5b9e\u66f4\u65b0\u7a00\u91ca\u4e14\u96be\u4ee5\u5728\u8054\u90a6\u9632\u5fa1\u4e0b\u6301\u7eed\u5b58\u5728\u3002", "method": "\u91c7\u7528\u52a8\u6001\u4f18\u5316\u540e\u95e8\u89e6\u53d1\u5668\u7684\u6700\u5c0f-\u6700\u5927\u6846\u67b6\uff1a\u5185\u5c42\u6700\u5927\u5316\u4e2d\u6bd2\u6837\u672c\u4e0e\u826f\u6027\u6837\u672c\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u786e\u4fdd\u826f\u6027\u7528\u6237\u66f4\u65b0\u5bf9\u540e\u95e8\u5f71\u54cd\u6700\u5c0f\uff1b\u5916\u5c42\u5c06\u81ea\u9002\u5e94\u89e6\u53d1\u5668\u6ce8\u5165\u672c\u5730\u6a21\u578b\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u4e0e6\u79cd\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u548c6\u79cd\u9632\u5fa1\u7b97\u6cd5\u6bd4\u8f83\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u653b\u51fb\u6027\u80fd\uff0c\u5e76\u80fd\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u540e\u95e8\u653b\u51fb\u6280\u672f\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u540e\u95e8\u653b\u51fb\u7684\u6301\u4e45\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u89e6\u53d1\u5668\u5b9e\u73b0\u4e86\u4e3b\u4efb\u52a1\u4e0e\u540e\u95e8\u4efb\u52a1\u7684\u89e3\u8026\uff0c\u63d0\u5347\u4e86\u653b\u51fb\u7684\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.18930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18930", "abs": "https://arxiv.org/abs/2509.18930", "authors": ["Alex Schutz", "Victor-Alexandru Darvariu", "Efimia Panagiotaki", "Bruno Lacerda", "Nick Hawes"], "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning", "comment": null, "summary": "Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks\nto execute classic algorithms by supervised learning. Despite its successes,\nimportant limitations remain: inability to construct valid solutions without\npost-processing and to reason about multiple correct ones, poor performance on\ncombinatorial NP-hard problems, and inapplicability to problems for which\nstrong algorithms are not yet known. To address these limitations, we reframe\nthe problem of learning algorithm trajectories as a Markov Decision Process,\nwhich imposes structure on the solution construction procedure and unlocks the\npowerful tools of imitation and reinforcement learning (RL). We propose the\nGNARL framework, encompassing the methodology to translate problem formulations\nfrom NAR to RL and a learning architecture suitable for a wide range of\ngraph-based problems. We achieve very high graph accuracy results on several\nCLRS-30 problems, performance matching or exceeding much narrower NAR\napproaches for NP-hard problems and, remarkably, applicability even when\nlacking an expert algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GNARL\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u7b97\u6cd5\u63a8\u7406\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u89e3\u51b3\u4f20\u7edfNAR\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u6cd5\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u65e0\u6cd5\u6784\u9020\u6709\u6548\u89e3\u3001\u96be\u4ee5\u5904\u7406NP\u96be\u95ee\u9898\u3001\u4f9d\u8d56\u4e13\u5bb6\u7b97\u6cd5\u7b49\u5c40\u9650\u6027\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5c06\u7b97\u6cd5\u8f68\u8ff9\u5b66\u4e60\u95ee\u9898\u91cd\u6784\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51faGNARL\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u56fe\u57fa\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2aCLRS-30\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u5f88\u9ad8\u7684\u56fe\u7cbe\u5ea6\uff0c\u5728NP\u96be\u95ee\u9898\u4e0a\u6027\u80fd\u5339\u914d\u6216\u8d85\u8fc7\u4f20\u7edfNAR\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u7f3a\u4e4f\u4e13\u5bb6\u7b97\u6cd5\u65f6\u4e5f\u9002\u7528\u3002", "conclusion": "GNARL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfNAR\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7b97\u6cd5\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18949", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18949", "abs": "https://arxiv.org/abs/2509.18949", "authors": ["Niccol\u00f2 Rocchi", "Fabio Stella", "Cassio de Campos"], "title": "Towards Privacy-Aware Bayesian Networks: A Credal Approach", "comment": "Accepted at ECAI2025 conference, 20 pages, 1 figure", "summary": "Bayesian networks (BN) are probabilistic graphical models that enable\nefficient knowledge representation and inference. These have proven effective\nacross diverse domains, including healthcare, bioinformatics and economics. The\nstructure and parameters of a BN can be obtained by domain experts or directly\nlearned from available data. However, as privacy concerns escalate, it becomes\nincreasingly critical for publicly released models to safeguard sensitive\ninformation in training data. Typically, released models do not prioritize\nprivacy by design. In particular, tracing attacks from adversaries can combine\nthe released BN with auxiliary data to determine whether specific individuals\nbelong to the data from which the BN was learned. State-of-the-art protection\ntecniques involve introducing noise into the learned parameters. While this\noffers robust protection against tracing attacks, it significantly impacts the\nmodel's utility, in terms of both the significance and accuracy of the\nresulting inferences. Hence, high privacy may be attained at the cost of\nreleasing a possibly ineffective model. This paper introduces credal networks\n(CN) as a novel solution for balancing the model's privacy and utility. After\nadapting the notion of tracing attacks, we demonstrate that a CN enables the\nmasking of the learned BN, thereby reducing the probability of successful\nattacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve\nmeaningful inferences while safeguarding privacy. Moreover, we identify key\nlearning information that must be concealed to prevent attackers from\nrecovering the underlying BN. Finally, we conduct a set of numerical\nexperiments to analyze how privacy gains can be modulated by tuning the CN\nhyperparameters. Our results confirm that CNs provide a principled, practical,\nand effective approach towards the development of privacy-aware probabilistic\ngraphical models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u7f6e\u4fe1\u7f51\u7edc\uff08CN\uff09\u4f5c\u4e3a\u8d1d\u53f6\u65af\u7f51\u7edc\uff08BN\uff09\u7684\u9690\u79c1\u4fdd\u62a4\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u6a21\u7cca\u5316\u800c\u975e\u6dfb\u52a0\u566a\u58f0\u7684\u65b9\u5f0f\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u9632\u6b62\u8ffd\u8e2a\u653b\u51fb\u3002", "motivation": "\u968f\u7740\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u516c\u5f00\u53d1\u5e03\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u6a21\u578b\u9700\u8981\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u3002\u73b0\u6709\u7684\u566a\u58f0\u6dfb\u52a0\u65b9\u6cd5\u867d\u7136\u63d0\u4f9b\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u4f46\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u7f6e\u4fe1\u7f51\u7edc\uff08CN\uff09\u6765\u63a9\u76d6\u5b66\u4e60\u5230\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u901a\u8fc7\u8c03\u6574CN\u8d85\u53c2\u6570\u6765\u8c03\u8282\u9690\u79c1\u4fdd\u62a4\u7a0b\u5ea6\uff0c\u540c\u65f6\u8bc6\u522b\u5fc5\u987b\u9690\u85cf\u7684\u5173\u952e\u5b66\u4e60\u4fe1\u606f\u4ee5\u9632\u6b62\u653b\u51fb\u8005\u6062\u590d\u5e95\u5c42BN\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u7f6e\u4fe1\u7f51\u7edc\u80fd\u591f\u6709\u6548\u964d\u4f4e\u6210\u529f\u653b\u51fb\u7684\u6982\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u9690\u79c1\u548c\u6548\u7528\u4e4b\u95f4\u5b9e\u73b0\u4e86\u5e73\u8861\u3002", "conclusion": "\u7f6e\u4fe1\u7f51\u7edc\u4e3a\u5f00\u53d1\u9690\u79c1\u611f\u77e5\u7684\u6982\u7387\u56fe\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u3001\u5b9e\u7528\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u7ef4\u6301\u6a21\u578b\u7684\u63a8\u7406\u6548\u7528\u3002"}}
{"id": "2509.18962", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18962", "abs": "https://arxiv.org/abs/2509.18962", "authors": ["Kirsten K\u00f6bschall", "Sebastian Buschj\u00e4ger", "Raphael Fischer", "Lisa Hartung", "Stefan Kramer"], "title": "Lift What You Can: Green Online Learning with Heterogeneous Ensembles", "comment": null, "summary": "Ensemble methods for stream mining necessitate managing multiple models and\nupdating them as data distributions evolve. Considering the calls for more\nsustainability, established methods are however not sufficiently considerate of\nensemble members' computational expenses and instead overly focus on predictive\ncapabilities. To address these challenges and enable green online learning, we\npropose heterogeneous online ensembles (HEROS). For every training step, HEROS\nchooses a subset of models from a pool of models initialized with diverse\nhyperparameter choices under resource constraints to train. We introduce a\nMarkov decision process to theoretically capture the trade-offs between\npredictive performance and sustainability constraints. Based on this framework,\nwe present different policies for choosing which models to train on incoming\ndata. Most notably, we propose the novel $\\zeta$-policy, which focuses on\ntraining near-optimal models at reduced costs. Using a stochastic model, we\ntheoretically prove that our $\\zeta$-policy achieves near optimal performance\nwhile using fewer resources compared to the best performing policy. In our\nexperiments across 11 benchmark datasets, we find empiric evidence that our\n$\\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating\nhighly accurate performance, in some cases even outperforming competitors, and\nsimultaneously being much more resource-friendly.", "AI": {"tldr": "HEROS\u662f\u4e00\u79cd\u5f02\u6784\u5728\u7ebf\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u9009\u62e9\u90e8\u5206\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u6301\u7eed\u6027\uff0c\u63d0\u51fa\u03b6\u7b56\u7565\u5b9e\u73b0\u8fd1\u6700\u4f18\u6027\u80fd\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u96c6\u6210\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u9884\u6d4b\u80fd\u529b\u800c\u5ffd\u89c6\u8ba1\u7b97\u6210\u672c\uff0c\u9700\u8981\u66f4\u53ef\u6301\u7eed\u7684\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5f02\u6784\u5728\u7ebf\u96c6\u6210\u6846\u67b6HEROS\uff0c\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u6027\u80fd\u4e0e\u53ef\u6301\u7eed\u6027\u6743\u8861\uff0c\u5f15\u5165\u03b6\u7b56\u7565\u9009\u62e9\u8bad\u7ec3\u6a21\u578b\u5b50\u96c6\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u03b6\u7b56\u7565\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\uff0c\u6709\u65f6\u751a\u81f3\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HEROS\u6846\u67b6\u548c\u03b6\u7b56\u7565\u4e3a\u7eff\u8272\u5728\u7ebf\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u548c\u53ef\u6301\u7eed\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.18968", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18968", "abs": "https://arxiv.org/abs/2509.18968", "authors": ["Zhanglu Yan", "Jiayi Mao", "Qianhui Liu", "Fanfan Li", "Gang Pan", "Tao Luo", "Bowen Zhu", "Weng-Fai Wong"], "title": "Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding", "comment": null, "summary": "Spiking neural networks (SNNs) promise high energy efficiency, particularly\nwith time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting\nat most one spike per neuron. However, such energy advantage is often\nunrealized because inference requires evaluating a temporal decay function and\nsubsequent multiplication with the synaptic weights. This paper challenges this\ncostly approach by repurposing a physical hardware `bug', namely, the natural\nsignal decay in optoelectronic devices, as the core computation of TTFS. We\nfabricated a custom indium oxide optoelectronic synapse, showing how its\nnatural physical decay directly implements the required temporal function. By\ntreating the device's analog output as the fused product of the synaptic weight\nand temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates\nthese expensive digital operations. To use the Otters paradigm in complex\narchitectures like the transformer, which are challenging to train directly due\nto the sparsity issue, we introduce a novel quantized neural network-to-SNN\nconversion algorithm. This complete hardware-software co-design enables our\nmodel to achieve state-of-the-art accuracy across seven GLUE benchmark datasets\nand demonstrates a 1.77$\\times$ improvement in energy efficiency over previous\nleading SNNs, based on a comprehensive analysis of compute, data movement, and\nmemory access costs using energy measurements from a commercial 22nm process.\nOur work thus establishes a new paradigm for energy-efficient SNNs, translating\nfundamental device physics directly into powerful computational primitives. All\ncodes and data are open source.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOtters\u7684\u65b0\u578b\u5149\u7535\u7a81\u89e6\u8bbe\u8ba1\uff0c\u5229\u7528\u5149\u7535\u8bbe\u5907\u7684\u81ea\u7136\u4fe1\u53f7\u8870\u51cf\u7279\u6027\u6765\u5b9e\u73b0\u65f6\u95f4\u5230\u9996\u6b21\u8109\u51b2\u7f16\u7801\uff0c\u4ece\u800c\u6d88\u9664\u6602\u8d35\u7684\u6570\u5b57\u8fd0\u7b97\uff0c\u663e\u8457\u63d0\u9ad8\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u80fd\u6548\u3002", "motivation": "\u4f20\u7edf\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7279\u522b\u662f\u65f6\u95f4\u5230\u9996\u6b21\u8109\u51b2\uff08TTFS\uff09\u7f16\u7801\u867d\u7136\u7406\u8bba\u4e0a\u5177\u6709\u9ad8\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u5728\u5b9e\u9645\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9700\u8981\u8bc4\u4f30\u65f6\u95f4\u8870\u51cf\u51fd\u6570\u5e76\u4e0e\u7a81\u89e6\u6743\u91cd\u76f8\u4e58\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u6210\u672c\u9ad8\u6602\uff0c\u5bfc\u81f4\u80fd\u6548\u4f18\u52bf\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "1. \u8bbe\u8ba1\u5e76\u5236\u9020\u4e86\u5b9a\u5236\u6c27\u5316\u94df\u5149\u7535\u7a81\u89e6\uff0c\u5229\u7528\u5176\u81ea\u7136\u7269\u7406\u8870\u51cf\u7279\u6027\u76f4\u63a5\u5b9e\u73b0\u6240\u9700\u7684\u65f6\u95f4\u51fd\u6570\n2. \u5c06\u8bbe\u5907\u7684\u6a21\u62df\u8f93\u51fa\u89c6\u4e3a\u7a81\u89e6\u6743\u91cd\u548c\u65f6\u95f4\u8870\u51cf\u7684\u878d\u5408\u4e58\u79ef\uff0c\u63d0\u51faOtters\u8303\u5f0f\n3. \u9488\u5bf9\u590d\u6742\u67b6\u6784\uff08\u5982transformer\uff09\u5f15\u5165\u65b0\u578b\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u5230SNN\u7684\u8f6c\u6362\u7b97\u6cd5\n4. \u91c7\u7528\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5", "result": "\u5728\u4e03\u4e2aGLUE\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u57fa\u4e8e\u5546\u4e1a22nm\u5de5\u827a\u7684\u80fd\u8017\u5206\u6790\u663e\u793a\uff0c\u76f8\u6bd4\u4e4b\u524d\u9886\u5148\u7684SNNs\u5b9e\u73b0\u4e861.77\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684\u80fd\u6548SNN\u8303\u5f0f\uff0c\u5c06\u57fa\u7840\u5668\u4ef6\u7269\u7406\u7279\u6027\u76f4\u63a5\u8f6c\u5316\u4e3a\u5f3a\u5927\u7684\u8ba1\u7b97\u539f\u8bed\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.18990", "categories": ["cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2509.18990", "abs": "https://arxiv.org/abs/2509.18990", "authors": ["Carson Dudley", "Marisa Eisenberg"], "title": "Learning From Simulators: A Theory of Simulation-Grounded Learning", "comment": null, "summary": "Simulation-Grounded Neural Networks (SGNNs) are predictive models trained\nentirely on synthetic data from mechanistic simulations. They have achieved\nstate-of-the-art performance in domains where real-world labels are limited or\nunobserved, but lack a formal underpinning.\n  We present the foundational theory of simulation-grounded learning. We show\nthat SGNNs implement amortized Bayesian inference under a simulation prior and\nconverge to the Bayes-optimal predictor. We derive generalization bounds under\nmodel misspecification and prove that SGNNs can learn unobservable scientific\nquantities that empirical methods provably cannot. We also formalize a novel\nform of mechanistic interpretability uniquely enabled by SGNNs: by attributing\npredictions to the simulated mechanisms that generated them, SGNNs yield\nposterior-consistent, scientifically grounded explanations.\n  We provide numerical experiments to validate all theoretical predictions.\nSGNNs recover latent parameters, remain robust under mismatch, and outperform\nclassical tools: in a model selection task, SGNNs achieve half the error of AIC\nin distinguishing mechanistic dynamics. These results establish SGNNs as a\nprincipled and practical framework for scientific prediction in data-limited\nregimes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4eff\u771f\u57fa\u7840\u795e\u7ecf\u7f51\u7edc\uff08SGNNs\uff09\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u5176\u5728\u4eff\u771f\u5148\u9a8c\u4e0b\u5b9e\u73b0\u644a\u9500\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u6536\u655b\u5230\u8d1d\u53f6\u65af\u6700\u4f18\u9884\u6d4b\u5668\uff0c\u5e76\u80fd\u5728\u6a21\u578b\u9519\u8bef\u8bbe\u5b9a\u4e0b\u5b66\u4e60\u7ecf\u9a8c\u65b9\u6cd5\u65e0\u6cd5\u89c2\u6d4b\u7684\u79d1\u5b66\u91cf\u3002", "motivation": "SGNNs\u5728\u771f\u5b9e\u6807\u7b7e\u6709\u9650\u6216\u4e0d\u53ef\u89c2\u6d4b\u7684\u9886\u57df\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u3002\u672c\u6587\u65e8\u5728\u4e3a\u4eff\u771f\u57fa\u7840\u5b66\u4e60\u5efa\u7acb\u5f62\u5f0f\u5316\u7406\u8bba\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660eSGNNs\u5b9e\u73b0\u644a\u9500\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u63a8\u5bfc\u6a21\u578b\u9519\u8bef\u8bbe\u5b9a\u4e0b\u7684\u6cdb\u5316\u8fb9\u754c\uff0c\u5e76\u5f62\u5f0f\u5316SGNNs\u7279\u6709\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u6709\u7406\u8bba\u9884\u6d4b\uff1aSGNNs\u80fd\u591f\u6062\u590d\u6f5c\u5728\u53c2\u6570\uff0c\u5728\u5931\u914d\u60c5\u51b5\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5728\u6a21\u578b\u9009\u62e9\u4efb\u52a1\u4e2dAIC\u8bef\u5dee\u51cf\u534a\u3002", "conclusion": "SGNNs\u4e3a\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u7684\u79d1\u5b66\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u5b9e\u7528\u7684\u6846\u67b6\u3002"}}
{"id": "2509.18993", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18993", "abs": "https://arxiv.org/abs/2509.18993", "authors": ["Boao Kong", "Junzhu Liang", "Yuxi Liu", "Renjia Deng", "Kun Yuan"], "title": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure", "comment": "32 pages", "summary": "Low-rank architectures have become increasingly important for efficient large\nlanguage model (LLM) pre-training, providing substantial reductions in both\nparameter complexity and memory/computational demands. Despite these\nadvantages, current low-rank methods face three critical shortcomings: (1)\ncompromised model performance, (2) considerable computational overhead, and (3)\nlimited activation memory savings. To address these limitations, we propose\nCross-layer Low-Rank residual Network (CR-Net), an innovative\nparameter-efficient framework inspired by our discovery that inter-layer\nactivation residuals possess low-rank properties. CR-Net implements this\ninsight through a dual-path architecture that efficiently reconstructs layer\nactivations by combining previous-layer outputs with their low-rank\ndifferences, thereby maintaining high-rank information with minimal parameters.\nWe further develop a specialized activation recomputation strategy tailored for\nCR-Net that dramatically reduces memory requirements. Extensive pre-training\nexperiments across model scales from 60M to 7B parameters demonstrate that\nCR-Net consistently outperforms state-of-the-art low-rank frameworks while\nrequiring fewer computational resources and less memory.", "AI": {"tldr": "\u63d0\u51faCR-Net\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5c42\u95f4\u6fc0\u6d3b\u6b8b\u5dee\u7684\u4f4e\u79e9\u7279\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u53c2\u6570\u590d\u6742\u5ea6\u548c\u5185\u5b58\u9700\u6c42", "motivation": "\u89e3\u51b3\u73b0\u6709\u4f4e\u79e9\u65b9\u6cd5\u5728\u6a21\u578b\u6027\u80fd\u3001\u8ba1\u7b97\u5f00\u9500\u548c\u5185\u5b58\u8282\u7701\u65b9\u9762\u7684\u4e09\u4e2a\u5173\u952e\u7f3a\u9677", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\uff0c\u7ed3\u5408\u524d\u4e00\u5c42\u8f93\u51fa\u548c\u4f4e\u79e9\u5dee\u5f02\u6765\u9ad8\u6548\u91cd\u6784\u5c42\u6fc0\u6d3b\uff0c\u5e76\u5f00\u53d1\u4e13\u95e8\u7684\u6fc0\u6d3b\u91cd\u8ba1\u7b97\u7b56\u7565", "result": "\u572860M\u52307B\u53c2\u6570\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u5b9e\u9a8c\u4e2d\uff0cCR-Net\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4f4e\u79e9\u6846\u67b6\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u5185\u5b58", "conclusion": "CR-Net\u901a\u8fc7\u521b\u65b0\u7684\u4f4e\u79e9\u6b8b\u5dee\u7f51\u7edc\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u79e9\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9ad8\u6548LLM\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.18997", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18997", "abs": "https://arxiv.org/abs/2509.18997", "authors": ["Pascal Esser", "Maximilian Fleissner", "Debarghya Ghoshdastidar"], "title": "Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization", "comment": null, "summary": "Representation learning from unlabeled data has been extensively studied in\nstatistics, data science and signal processing with a rich literature on\ntechniques for dimension reduction, compression, multi-dimensional scaling\namong others. However, current deep learning models use new principles for\nunsupervised representation learning that cannot be easily analyzed using\nclassical theories. For example, visual foundation models have found tremendous\nsuccess using self-supervision or denoising/masked autoencoders, which\neffectively learn representations from massive amounts of unlabeled data.\nHowever, it remains difficult to characterize the representations learned by\nthese models and to explain why they perform well for diverse prediction tasks\nor show emergent behavior. To answer these questions, one needs to combine\nmathematical tools from statistics and optimization. This paper provides an\noverview of recent theoretical advances in representation learning from\nunlabeled data and mentions our contributions in this direction.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7684\u6700\u65b0\u7406\u8bba\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4f5c\u8005\u5728\u8be5\u65b9\u5411\u4e0a\u7684\u8d21\u732e\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f7f\u7528\u81ea\u76d1\u7763\u6216\u53bb\u566a/\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7b49\u65b0\u539f\u7406\u8fdb\u884c\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u96be\u4ee5\u7528\u7ecf\u5178\u7406\u8bba\u5206\u6790\uff0c\u9700\u8981\u7ed3\u5408\u7edf\u8ba1\u548c\u4f18\u5316\u7684\u6570\u5b66\u5de5\u5177\u6765\u7406\u89e3\u5176\u8868\u5f81\u7279\u6027\u3002", "method": "\u7ed3\u5408\u7edf\u8ba1\u5b66\u548c\u4f18\u5316\u7684\u6570\u5b66\u5de5\u5177\uff0c\u5bf9\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7684\u7406\u8bba\u6846\u67b6\u8fdb\u884c\u5206\u6790\uff0c\u7279\u522b\u662f\u9488\u5bf9\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\u81ea\u76d1\u7763\u548c\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7b49\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7684\u6700\u65b0\u7406\u8bba\u8fdb\u5c55\u6982\u8ff0\uff0c\u9610\u660e\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8868\u5f81\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u53d1\u5c55\u6570\u5b66\u7406\u8bba\u5de5\u5177\u6765\u6df1\u5165\u7406\u89e3\u6df1\u5ea6\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u6a21\u578b\u7684\u8868\u5f81\u7279\u6027\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.19017", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19017", "abs": "https://arxiv.org/abs/2509.19017", "authors": ["Hazem Dewidar", "Elena Umili"], "title": "Fully Learnable Neural Reward Machines", "comment": null, "summary": "Non-Markovian Reinforcement Learning (RL) tasks present significant\nchallenges, as agents must reason over entire trajectories of state-action\npairs to make optimal decisions. A common strategy to address this is through\nsymbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which\nprovide a structured way to express temporally extended objectives. However,\nthese approaches often rely on restrictive assumptions -- such as the\navailability of a predefined Symbol Grounding (SG) function mapping raw\nobservations to high-level symbolic representations, or prior knowledge of the\ntemporal task. In this work, we propose a fully learnable version of Neural\nReward Machines (NRM), which can learn both the SG function and the automaton\nend-to-end, removing any reliance on prior knowledge. Our approach is therefore\nas easily applicable as classic deep RL (DRL) approaches, while being far more\nexplainable, because of the finite and compact nature of automata. Furthermore,\nwe show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,\nour method outperforms previous approaches based on Recurrent Neural Networks\n(RNNs).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53ef\u5b66\u4e60\u7684\u795e\u7ecf\u5956\u52b1\u673a\uff08FLNRM\uff09\uff0c\u80fd\u591f\u7aef\u5230\u7aef\u5b66\u4e60\u7b26\u53f7\u63a5\u5730\u51fd\u6570\u548c\u81ea\u52a8\u673a\uff0c\u6d88\u9664\u4e86\u5bf9\u5148\u9a8c\u77e5\u8bc6\u7684\u4f9d\u8d56\uff0c\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u975e\u9a6c\u5c14\u53ef\u592b\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b26\u53f7\u63a5\u5730\u51fd\u6570\u6216\u5148\u9a8c\u77e5\u8bc6\u7684\u9650\u5236\uff0c\u4f7f\u65b9\u6cd5\u66f4\u6613\u5e94\u7528\u4e14\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u5b8c\u5168\u53ef\u5b66\u4e60\u7684\u795e\u7ecf\u5956\u52b1\u673a\uff08FLNRM\uff09\uff0c\u96c6\u6210\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u7aef\u5230\u7aef\u5b66\u4e60\u7b26\u53f7\u63a5\u5730\u548c\u81ea\u52a8\u673a\u7ed3\u6784\u3002", "result": "FLNRM\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u7684\u5148\u524d\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u81ea\u52a8\u673a\u7684\u6709\u9650\u6027\u548c\u7d27\u51d1\u6027\u5e26\u6765\u7684\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\u3002", "conclusion": "FLNRM\u4e3a\u975e\u9a6c\u5c14\u53ef\u592b\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u6613\u4e8e\u5e94\u7528\u53c8\u5177\u6709\u53ef\u89e3\u91ca\u6027\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u5373\u53ef\u5b66\u4e60\u590d\u6742\u7684\u65f6\u95f4\u6269\u5c55\u76ee\u6807\u3002"}}
{"id": "2509.19018", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19018", "abs": "https://arxiv.org/abs/2509.19018", "authors": ["Teng Xiao", "Zuchao Li", "Lefei Zhang"], "title": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment", "comment": null, "summary": "Recent advances in multimodal large language models (LLMs) have led to\nsignificant progress in understanding, generation, and retrieval tasks.\nHowever, current solutions often treat these tasks in isolation or require\ntraining LLMs from scratch, resulting in high computational costs and limited\ngeneralization across modalities. In this work, we present OmniBridge, a\nunified and modular multimodal framework that supports vision-language\nunderstanding, generation, and retrieval within a unified architecture.\nOmniBridge adopts a language-centric design that reuses pretrained LLMs and\nintroduces a lightweight bidirectional latent alignment module. To address the\nchallenge of task interference, we propose a two-stage decoupled training\nstrategy: supervised fine-tuning and latent space alignment for aligning LLM\nbehavior with multimodal reasoning, and semantic-guided diffusion training to\nalign cross-modal latent spaces via learnable query embeddings. Extensive\nexperiments across a wide range of benchmarks demonstrate that OmniBridge\nachieves competitive or state-of-the-art performance in all three tasks.\nMoreover, our results highlight the effectiveness of latent space alignment for\nunifying multimodal modeling under a shared representation space. Code and\nmodels are released at https://github.com/xiao-xt/OmniBridge.", "AI": {"tldr": "OmniBridge\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u652f\u6301\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u3001\u751f\u6210\u548c\u68c0\u7d22\u4efb\u52a1\uff0c\u91c7\u7528\u8bed\u8a00\u4e2d\u5fc3\u8bbe\u8ba1\u548c\u8f7b\u91cf\u7ea7\u53cc\u5411\u6f5c\u5728\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u89e3\u8026\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u591a\u4efb\u52a1\u7edf\u4e00\u5efa\u6a21\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u5f80\u5f80\u5b64\u7acb\u5904\u7406\u4e0d\u540c\u4efb\u52a1\u6216\u9700\u8981\u4ece\u5934\u8bad\u7ec3\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u91c7\u7528\u8bed\u8a00\u4e2d\u5fc3\u8bbe\u8ba1\u91cd\u7528\u9884\u8bad\u7ec3LLM\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u53cc\u5411\u6f5c\u5728\u5bf9\u9f50\u6a21\u5757\uff1b\u63d0\u51fa\u4e24\u9636\u6bb5\u89e3\u8026\u8bad\u7ec3\u7b56\u7565\uff1a\u76d1\u7763\u5fae\u8c03\u548c\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u7528\u4e8e\u5bf9\u9f50LLM\u884c\u4e3a\u4e0e\u591a\u6a21\u6001\u63a8\u7406\uff0c\u8bed\u4e49\u5f15\u5bfc\u6269\u6563\u8bad\u7ec3\u901a\u8fc7\u53ef\u5b66\u4e60\u67e5\u8be2\u5d4c\u5165\u5bf9\u9f50\u8de8\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOmniBridge\u5728\u6240\u6709\u4e09\u4e2a\u4efb\u52a1\u4e0a\u90fd\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u5728\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u4e0b\u7edf\u4e00\u591a\u6a21\u6001\u5efa\u6a21\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.19032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19032", "abs": "https://arxiv.org/abs/2509.19032", "authors": ["Kashaf Ul Emaan"], "title": "Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling", "comment": null, "summary": "Detection of credit card fraud is an acute issue of financial security\nbecause transaction datasets are highly lopsided, with fraud cases being only a\ndrop in the ocean. Balancing datasets using the most popular methods of\ntraditional oversampling such as the Synthetic Minority Oversampling Technique\n(SMOTE) generally create simplistic synthetic samples that are not readily\napplicable to complex fraud patterns. Recent industry advances that include\nConditional Tabular Generative Adversarial Networks (CTGAN) and Tabular\nVariational Autoencoders (TVAE) have demonstrated increased efficiency in\ntabular synthesis, yet all these models still exhibit issues with\nhigh-dimensional dependence modelling. Now we will present our hybrid approach\nwhere we use a Generative Adversarial Network (GAN) with a Transformer encoder\nblock to produce realistic fraudulent transactions samples. The GAN\narchitecture allows training realistic generators adversarial, and the\nTransformer allows the model to learn rich feature interactions by\nself-attention. Such a hybrid strategy overcomes the limitations of SMOTE,\nCTGAN, and TVAE by producing a variety of high-quality synthetic minority\nclasses samples. We test our algorithm on the publicly-available Credit Card\nFraud Detection dataset and compare it to conventional and generative\nresampling strategies with a variety of classifiers, such as Logistic\nRegression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and\nSupport Vector Machine (SVM). Findings indicate that our Transformer-based GAN\nshows substantial gains in Recall, F1-score and Area Under the Receiver\nOperating Characteristic Curve (AUC), which indicates that it is effective in\novercoming the severe class imbalance inherent in the task of fraud detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7f16\u7801\u5668\u7684GAN\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u6b3a\u8bc8\u4ea4\u6613\u6837\u672c\uff0c\u4ee5\u89e3\u51b3\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u9762\u4e34\u4e25\u91cd\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u8fc7\u91c7\u6837\u65b9\u6cd5\u5982SMOTE\u751f\u6210\u7684\u6837\u672c\u8fc7\u4e8e\u7b80\u5355\uff0c\u800c\u73b0\u6709\u7684\u751f\u6210\u6a21\u578b\u5982CTGAN\u548cTVAE\u5728\u9ad8\u7ef4\u4f9d\u8d56\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u4e0eTransformer\u7f16\u7801\u5668\u5757\u7684\u6df7\u5408\u67b6\u6784\uff0cGAN\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u751f\u6210\u771f\u5b9e\u6837\u672c\uff0cTransformer\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u4e30\u5bcc\u7684\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u5728\u516c\u5f00\u7684\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u4e0e\u591a\u79cd\u5206\u7c7b\u5668\u7ed3\u5408\uff0c\u76f8\u6bd4\u4f20\u7edf\u548c\u751f\u6210\u91cd\u91c7\u6837\u65b9\u6cd5\uff0c\u5728\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUC\u6307\u6807\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8eTransformer\u7684GAN\u65b9\u6cd5\u80fd\u6709\u6548\u514b\u670d\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5c11\u6570\u7c7b\u6837\u672c\u3002"}}
{"id": "2509.19044", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19044", "abs": "https://arxiv.org/abs/2509.19044", "authors": ["Yang Li", "Chenyu Wang", "Tingrui Wang", "Yongwei Wang", "Haonan Li", "Zhunga Liu", "Quan Pan"], "title": "Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks", "comment": null, "summary": "Black-box adversarial attacks remain challenging due to limited access to\nmodel internals. Existing methods often depend on specific network\narchitectures or require numerous queries, resulting in limited\ncross-architecture transferability and high query costs. To address these\nlimitations, we propose JAD, a latent diffusion model framework for black-box\nadversarial attacks. JAD generates adversarial examples by leveraging a latent\ndiffusion model guided by attention maps distilled from both a convolutional\nneural network (CNN) and a Vision Transformer (ViT) models. By focusing on\nimage regions that are commonly sensitive across architectures, this approach\ncrafts adversarial perturbations that transfer effectively between different\nmodel types. This joint attention distillation strategy enables JAD to be\narchitecture-agnostic, achieving superior attack generalization across diverse\nmodels. Moreover, the generative nature of the diffusion framework yields high\nadversarial sample generation efficiency by reducing reliance on iterative\nqueries. Experiments demonstrate that JAD offers improved attack\ngeneralization, generation efficiency, and cross-architecture transferability\ncompared to existing methods, providing a promising and effective paradigm for\nblack-box adversarial attacks.", "AI": {"tldr": "JAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u6ce8\u610f\u529b\u84b8\u998f\u7b56\u7565\u5b9e\u73b0\u8de8\u67b6\u6784\u653b\u51fb\u6cdb\u5316", "motivation": "\u89e3\u51b3\u73b0\u6709\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u7f51\u7edc\u67b6\u6784\u3001\u67e5\u8be2\u6b21\u6570\u591a\u3001\u8de8\u67b6\u6784\u8fc1\u79fb\u6027\u5dee\u7684\u95ee\u9898", "method": "\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u901a\u8fc7\u4eceCNN\u548cViT\u6a21\u578b\u4e2d\u84b8\u998f\u6ce8\u610f\u529b\u56fe\u6765\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u805a\u7126\u4e8e\u8de8\u67b6\u6784\u654f\u611f\u7684\u56fe\u50cf\u533a\u57df", "result": "JAD\u5728\u653b\u51fb\u6cdb\u5316\u6027\u3001\u751f\u6210\u6548\u7387\u548c\u8de8\u67b6\u6784\u8fc1\u79fb\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "JAD\u4e3a\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6709\u6548\u8303\u5f0f"}}
{"id": "2509.19063", "categories": ["cs.LG", "cs.AI", "68T07"], "pdf": "https://arxiv.org/pdf/2509.19063", "abs": "https://arxiv.org/abs/2509.19063", "authors": ["Przemys\u0142aw Spyra"], "title": "Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training", "comment": null, "summary": "The rising computational and energy demands of deep neural networks (DNNs),\ndriven largely by backpropagation (BP), challenge sustainable AI development.\nThis paper rigorously investigates three BP-free training methods: the\nForward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)\nalgorithms, tracing their progression from foundational concepts to a\ndemonstrably superior solution.\n  A robust comparative framework was established: each algorithm was\nimplemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and\nbenchmarked against an equivalent BP-trained model. Hyperparameters were\noptimized with Optuna, and consistent early stopping criteria were applied\nbased on validation performance, ensuring all models were optimally tuned\nbefore comparison.\n  Results show that MF not only competes with but consistently surpasses BP in\nclassification accuracy on its native MLPs. Its superior generalization stems\nfrom converging to a more favorable minimum in the validation loss landscape,\nchallenging the assumption that global optimization is required for\nstate-of-the-art results. Measured at the hardware level using the NVIDIA\nManagement Library (NVML) API, MF reduces energy consumption by up to 41% and\nshortens training time by up to 34%, translating to a measurably smaller carbon\nfootprint as estimated by CodeCarbon.\n  Beyond this primary result, we present a hardware-level analysis that\nexplains the efficiency gains: exposing FF's architectural inefficiencies,\nvalidating MF's computationally lean design, and challenging the assumption\nthat all BP-free methods are inherently more memory-efficient. By documenting\nthe evolution from FF's conceptual groundwork to MF's synthesis of accuracy and\nsustainability, this work offers a clear, data-driven roadmap for future\nenergy-efficient deep learning.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e09\u79cd\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u7b97\u6cd5\uff08FF\u3001CaFo\u3001MF\uff09\uff0c\u53d1\u73b0MF\u7b97\u6cd5\u5728MLP\u67b6\u6784\u4e0a\u4e0d\u4ec5\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u4e86BP\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e8641%\u7684\u80fd\u8017\u548c34%\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u7684\u8ba1\u7b97\u548c\u80fd\u8017\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u4e3b\u8981\u53d7\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u9a71\u52a8\uff0c\u8fd9\u5bf9\u53ef\u6301\u7eedAI\u53d1\u5c55\u6784\u6210\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u66f4\u8282\u80fd\u9ad8\u6548\u7684\u66ff\u4ee3\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u6bd4\u8f83\u6846\u67b6\uff0c\u5728\u5404\u81ea\u539f\u751f\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e09\u79cdBP-free\u7b97\u6cd5\uff08FF\u548cMF\u7528MLP\uff0cCaFo\u7528CNN\uff09\uff0c\u4f7f\u7528Optuna\u4f18\u5316\u8d85\u53c2\u6570\uff0c\u57fa\u4e8e\u9a8c\u8bc1\u6027\u80fd\u5e94\u7528\u4e00\u81f4\u7684\u65e9\u505c\u6807\u51c6\uff0c\u5e76\u901a\u8fc7NVML API\u6d4b\u91cf\u786c\u4ef6\u7ea7\u80fd\u8017\u3002", "result": "MF\u7b97\u6cd5\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u4e0d\u4ec5\u4e0eBP\u76f8\u5f53\uff0c\u751a\u81f3\u66f4\u4f18\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u80fd\u8017\u964d\u4f4e41%\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed34%\uff0c\u78b3\u8db3\u8ff9\u663e\u8457\u51cf\u5c0f\u3002", "conclusion": "MF\u7b97\u6cd5\u901a\u8fc7\u6536\u655b\u5230\u9a8c\u8bc1\u635f\u5931\u666f\u89c2\u4e2d\u66f4\u6709\u5229\u7684\u6700\u5c0f\u503c\uff0c\u6311\u6218\u4e86\u5168\u5c40\u4f18\u5316\u5fc5\u8981\u6027\u7684\u5047\u8bbe\uff0c\u4e3a\u672a\u6765\u80fd\u6548\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u6570\u636e\u9a71\u52a8\u8def\u7ebf\u56fe\u3002"}}
{"id": "2509.19078", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19078", "abs": "https://arxiv.org/abs/2509.19078", "authors": ["Jian Xu", "Qibin Zhao", "John Paisley", "Delu Zeng"], "title": "Diffusion Bridge Variational Inference for Deep Gaussian Processes", "comment": null, "summary": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian\nmodeling but pose substantial challenges for posterior inference, especially\nover inducing variables. Denoising diffusion variational inference (DDVI)\naddresses this by modeling the posterior as a time-reversed diffusion from a\nsimple Gaussian prior. However, DDVI's fixed unconditional starting\ndistribution remains far from the complex true posterior, resulting in\ninefficient inference trajectories and slow convergence. In this work, we\npropose Diffusion Bridge Variational Inference (DBVI), a principled extension\nof DDVI that initiates the reverse diffusion from a learnable, data-dependent\ninitial distribution. This initialization is parameterized via an amortized\nneural network and progressively adapted using gradients from the ELBO\nobjective, reducing the posterior gap and improving sample efficiency. To\nenable scalable amortization, we design the network to operate on the inducing\ninputs, which serve as structured, low-dimensional summaries of the dataset and\nnaturally align with the inducing variables' shape. DBVI retains the\nmathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time\nSDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We\nderive a tractable training objective under this formulation and implement DBVI\nfor scalable inference in large-scale DGPs. Across regression, classification,\nand image reconstruction tasks, DBVI consistently outperforms DDVI and other\nvariational baselines in predictive accuracy, convergence speed, and posterior\nquality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DBVI\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u3001\u6570\u636e\u4f9d\u8d56\u7684\u521d\u59cb\u5206\u5e03\u6765\u6539\u8fdbDDVI\uff0c\u51cf\u5c11\u540e\u9a8c\u5dee\u8ddd\u5e76\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u3002", "motivation": "DDVI\u7684\u56fa\u5b9a\u65e0\u6761\u4ef6\u8d77\u59cb\u5206\u5e03\u4e0e\u590d\u6742\u771f\u5b9e\u540e\u9a8c\u5dee\u8ddd\u8f83\u5927\uff0c\u5bfc\u81f4\u63a8\u7406\u8f68\u8ff9\u6548\u7387\u4f4e\u4e0b\u548c\u6536\u655b\u7f13\u6162\u3002", "method": "DBVI\u4f7f\u7528\u644a\u9500\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u521d\u59cb\u5206\u5e03\uff0c\u901a\u8fc7ELBO\u76ee\u6807\u7684\u68af\u5ea6\u9010\u6b65\u9002\u5e94\uff0c\u8bbe\u8ba1\u7f51\u7edc\u5728\u8bf1\u5bfc\u8f93\u5165\u4e0a\u64cd\u4f5c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u644a\u9500\u3002", "result": "\u5728\u56de\u5f52\u3001\u5206\u7c7b\u548c\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\u4e2d\uff0cDBVI\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u540e\u9a8c\u8d28\u91cf\u65b9\u9762\u6301\u7eed\u4f18\u4e8eDDVI\u548c\u5176\u4ed6\u53d8\u5206\u57fa\u7ebf\u3002", "conclusion": "DBVI\u4fdd\u7559\u4e86DDVI\u7684\u6570\u5b66\u4f18\u96c5\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u521d\u59cb\u5206\u5e03\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2509.19084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19084", "abs": "https://arxiv.org/abs/2509.19084", "authors": ["Asela Hevapathige"], "title": "Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success across\nvarious graph-based tasks. However, they face some fundamental limitations:\nfeature oversmoothing can cause node representations to become\nindistinguishable in deeper networks, they struggle to effectively manage\nheterogeneous relationships where connected nodes differ significantly, and\nthey process entire feature vectors as indivisible units, which limits\nflexibility. We seek to address these limitations. We propose AxelGNN, a novel\nGNN architecture inspired by Axelrod's cultural dissemination model that\naddresses these limitations through a unified framework. AxelGNN incorporates\nsimilarity-gated probabilistic interactions that adaptively promote convergence\nor divergence based on node similarity, implements trait-level copying\nmechanisms for fine-grained feature aggregation at the segment level, and\nmaintains global polarization to preserve node distinctiveness across multiple\nrepresentation clusters. The model's bistable convergence dynamics naturally\nhandle both homophilic and heterophilic graphs within a single architecture.\nExtensive experiments on node classification and influence estimation\nbenchmarks demonstrate that AxelGNN consistently outperforms or matches\nstate-of-the-art GNN methods across diverse graph structures with varying\nhomophily-heterophily characteristics.", "AI": {"tldr": "AxelGNN\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7075\u611f\u6765\u81eaAxelrod\u6587\u5316\u4f20\u64ad\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u95e8\u63a7\u6982\u7387\u4ea4\u4e92\u3001\u7279\u5f81\u7ea7\u590d\u5236\u673a\u5236\u548c\u5168\u5c40\u6781\u5316\u4fdd\u6301\u6765\u89e3\u51b3\u4f20\u7edfGNN\u7684\u7279\u5f81\u8fc7\u5ea6\u5e73\u6ed1\u3001\u5f02\u6784\u5173\u7cfb\u5904\u7406\u56f0\u96be\u548c\u7279\u5f81\u5411\u91cf\u5904\u7406\u4e0d\u7075\u6d3b\u7b49\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfGNN\u5b58\u5728\u4e09\u4e2a\u57fa\u672c\u9650\u5236\uff1a\u6df1\u5c42\u7f51\u7edc\u4e2d\u8282\u70b9\u8868\u793a\u53d8\u5f97\u96be\u4ee5\u533a\u5206\u7684\u7279\u5f81\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff1b\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fde\u63a5\u8282\u70b9\u5dee\u5f02\u663e\u8457\u7684\u5f02\u6784\u5173\u7cfb\uff1b\u5c06\u6574\u4e2a\u7279\u5f81\u5411\u91cf\u4f5c\u4e3a\u4e0d\u53ef\u5206\u5272\u5355\u5143\u5904\u7406\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u3002", "method": "AxelGNN\u91c7\u7528\u76f8\u4f3c\u6027\u95e8\u63a7\u6982\u7387\u4ea4\u4e92\uff0c\u6839\u636e\u8282\u70b9\u76f8\u4f3c\u6027\u81ea\u9002\u5e94\u4fc3\u8fdb\u6536\u655b\u6216\u53d1\u6563\uff1b\u5b9e\u73b0\u7279\u5f81\u7ea7\u590d\u5236\u673a\u5236\uff0c\u5728\u7247\u6bb5\u7ea7\u522b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7279\u5f81\u805a\u5408\uff1b\u4fdd\u6301\u5168\u5c40\u6781\u5316\u4ee5\u8de8\u591a\u4e2a\u8868\u793a\u7c07\u4fdd\u6301\u8282\u70b9\u72ec\u7279\u6027\u3002", "result": "\u5728\u8282\u70b9\u5206\u7c7b\u548c\u5f71\u54cd\u529b\u4f30\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAxelGNN\u5728\u4e0d\u540c\u540c\u8d28\u6027-\u5f02\u8d28\u6027\u7279\u5f81\u7684\u591a\u6837\u5316\u56fe\u7ed3\u6784\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6216\u5339\u914d\u6700\u5148\u8fdb\u7684GNN\u65b9\u6cd5\u3002", "conclusion": "AxelGNN\u7684\u53cc\u7a33\u6001\u6536\u655b\u52a8\u529b\u5b66\u81ea\u7136\u5730\u5728\u5355\u4e00\u67b6\u6784\u5185\u5904\u7406\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u56fe\uff0c\u4e3a\u89e3\u51b3\u4f20\u7edfGNN\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2509.19098", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.19098", "abs": "https://arxiv.org/abs/2509.19098", "authors": ["Adrien Prevost", "Timothee Mathieu", "Odalric-Ambrym Maillard"], "title": "Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning", "comment": null, "summary": "We study the non-contextual multi-armed bandit problem in a transfer learning\nsetting: before any pulls, the learner is given N'_k i.i.d. samples from each\nsource distribution nu'_k, and the true target distributions nu_k lie within a\nknown distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first\nderive a problem-dependent asymptotic lower bound on cumulative regret that\nextends the classical Lai-Robbins result to incorporate the transfer parameters\n(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that\nmatches this new bound in the Gaussian case. Finally, we validate our approach\nvia simulations, showing that KL-UCB-Transfer significantly outperforms the\nno-prior baseline when source and target distributions are sufficiently close.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u975e\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u5728\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86KL-UCB-Transfer\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u6e90\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\u8db3\u591f\u63a5\u8fd1\u65f6\u663e\u8457\u4f18\u4e8e\u65e0\u5148\u9a8c\u57fa\u7ebf\u3002", "motivation": "\u5728\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\uff0c\u8fc1\u79fb\u5b66\u4e60\u53ef\u4ee5\u5229\u7528\u6e90\u5206\u5e03\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u6539\u5584\u76ee\u6807\u5206\u5e03\u7684\u51b3\u7b56\u6027\u80fd\u3002\u5f53\u6e90\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\u8db3\u591f\u63a5\u8fd1\u65f6\uff0c\u8fc1\u79fb\u5b66\u4e60\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u7d2f\u79ef\u9057\u61be\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u4e86\u5305\u542b\u8fc1\u79fb\u53c2\u6570\uff08\u8ddd\u79bb\u8fb9\u754c\u3001\u6837\u672c\u6570\u91cf\uff09\u7684\u6e10\u8fdb\u4e0b\u754c\uff0c\u7136\u540e\u63d0\u51fa\u4e86KL-UCB-Transfer\u7d22\u5f15\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5728\u9ad8\u65af\u60c5\u51b5\u4e0b\u80fd\u591f\u5339\u914d\u8fd9\u4e2a\u65b0\u8fb9\u754c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eKL-UCB-Transfer\u7b97\u6cd5\u80fd\u591f\u8fbe\u5230\u6e10\u8fdb\u6700\u4f18\u6027\u80fd\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u5f53\u6e90\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\u8db3\u591f\u63a5\u8fd1\u65f6\uff0c\u8be5\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u65e0\u5148\u9a8c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\u4e0b\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0cKL-UCB-Transfer\u7b97\u6cd5\u4e3a\u8fd9\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6e90\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\u76f8\u4f3c\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2509.19100", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19100", "abs": "https://arxiv.org/abs/2509.19100", "authors": ["Alexander Robey"], "title": "Algorithms for Adversarially Robust Deep Learning", "comment": "PhD thesis", "summary": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u6db5\u76d6\u4e86\u5bf9\u6297\u6027\u793a\u4f8b\u3001\u9886\u57df\u6cdb\u5316\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u72f1\u4e09\u4e2a\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u786e\u4fdd\u6a21\u578b\u51b3\u7b56\u80fd\u591f\u62b5\u5fa1\u5bf9\u6297\u6027\u653b\u51fb\u5177\u6709\u6839\u672c\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u65b0\u7684\u6280\u672f\u7ed3\u679c\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u8ba4\u8bc1\u7b97\u6cd5\u6765\u5904\u7406\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5bf9\u6297\u6027\u793a\u4f8b\uff1b\u5f00\u53d1\u4e86\u5728\u533b\u5b66\u6210\u50cf\u3001\u5206\u5b50\u8bc6\u522b\u548c\u56fe\u50cf\u5206\u7c7b\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6cdb\u5316\u7684\u65b0\u7b97\u6cd5\uff1b\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u72f1\u95ee\u9898\u63d0\u51fa\u4e86\u65b0\u7684\u653b\u51fb\u548c\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u5728\u5bf9\u6297\u6027\u793a\u4f8b\u3001\u9886\u57df\u6cdb\u5316\u548c\u8bed\u8a00\u6a21\u578b\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u524d\u6cbf\u8fdb\u5c55\uff0c\u4e3a\u8bbe\u8ba1\u7a33\u5065\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4ee3\u8868\u4e86\u5728\u8bbe\u8ba1\u5177\u6709\u7406\u60f3\u9c81\u68d2\u6027\u5c5e\u6027\u7684\u7b97\u6cd5\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u4fdd\u969c\u3002"}}
{"id": "2509.19112", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19112", "abs": "https://arxiv.org/abs/2509.19112", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation", "comment": "Accepted at NeuRIPS2025 Workshop on Structured Probabilistic\n  Inference and Generative Modeling", "summary": "Understanding causality in event sequences where outcome labels such as\ndiseases or system failures arise from preceding events like symptoms or error\ncodes is critical. Yet remains an unsolved challenge across domains like\nhealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label\ncausal discovery method for sparse, high-dimensional event sequences comprising\nof thousands of unique event types. Using two pretrained causal Transformers as\ndomain-specific foundation models for event sequences. CARGO infers in\nparallel, per sequence one-shot causal graphs and aggregates them using an\nadaptive frequency fusion to reconstruct the global Markov boundaries of\nlabels. This two-stage approach enables efficient probabilistic reasoning at\nscale while bypassing the intractable cost of full-dataset conditional\nindependence testing. Our results on a challenging real-world automotive fault\nprediction dataset with over 29,100 unique event types and 474 imbalanced\nlabels demonstrate CARGO's ability to perform structured reasoning.", "AI": {"tldr": "CARGO\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u6807\u7b7e\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u7a00\u758f\u9ad8\u7ef4\u4e8b\u4ef6\u5e8f\u5217\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u56e0\u679cTransformer\u548c\u81ea\u9002\u5e94\u9891\u7387\u878d\u5408\u6765\u63a8\u65ad\u56e0\u679c\u56fe", "motivation": "\u7406\u89e3\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u5bf9\u4e8e\u533b\u7597\u3001\u8f66\u8f86\u8bca\u65ad\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218", "method": "\u4f7f\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684\u56e0\u679cTransformer\u4f5c\u4e3a\u9886\u57df\u7279\u5b9a\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u884c\u63a8\u65ad\u6bcf\u4e2a\u5e8f\u5217\u7684\u56e0\u679c\u56fe\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u9891\u7387\u878d\u5408\u805a\u5408\u6765\u91cd\u5efa\u6807\u7b7e\u7684\u5168\u5c40\u9a6c\u5c14\u53ef\u592b\u8fb9\u754c", "result": "\u5728\u5305\u542b29,100\u4e2a\u72ec\u7279\u4e8b\u4ef6\u7c7b\u578b\u548c474\u4e2a\u4e0d\u5e73\u8861\u6807\u7b7e\u7684\u771f\u5b9e\u4e16\u754c\u6c7d\u8f66\u6545\u969c\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CARGO\u7684\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b", "conclusion": "CARGO\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u80fd\u591f\u5728\u89c4\u6a21\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u6982\u7387\u63a8\u7406\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5168\u6570\u636e\u96c6\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u7684\u96be\u4ee5\u5904\u7406\u6210\u672c"}}
{"id": "2509.19120", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19120", "abs": "https://arxiv.org/abs/2509.19120", "authors": ["Ferdinand Kahenga", "Antoine Bagula", "Sajal K. Das", "Patrick Sello"], "title": "FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI", "comment": null, "summary": "Federated Learning (FL) has emerged as a powerful paradigm for\nprivacy-preserving model training, yet deployments in sensitive domains such as\nhealthcare face persistent challenges from non-IID data, client unreliability,\nand adversarial manipulation. This paper introduces FedFiTS, a trust and\nfairness-aware selective FL framework that advances the FedFaSt line by\ncombining fitness-based client election with slotted aggregation. FedFiTS\nimplements a three-phase participation strategy-free-for-all training, natural\nselection, and slotted team participation-augmented with dynamic client\nscoring, adaptive thresholding, and cohort-based scheduling to balance\nconvergence efficiency with robustness. A theoretical convergence analysis\nestablishes bounds for both convex and non-convex objectives under standard\nassumptions, while a communication-complexity analysis shows reductions\nrelative to FedAvg and other baselines. Experiments on diverse datasets-medical\nimaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular\nagricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently\noutperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and\nresilience to poisoning attacks. By integrating trust-aware aggregation with\nfairness-oriented client selection, FedFiTS advances scalable and secure FL,\nmaking it well suited for real-world healthcare and cross-domain deployments.", "AI": {"tldr": "FedFiTS\u662f\u4e00\u4e2a\u7ed3\u5408\u4fe1\u4efb\u548c\u516c\u5e73\u610f\u8bc6\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u9002\u5e94\u5ea6\u7684\u5ba2\u6237\u7aef\u9009\u4e3e\u548c\u5206\u69fd\u805a\u5408\u6765\u63d0\u5347FedFaSt\u65b9\u6cd5\uff0c\u5728\u975eIID\u6570\u636e\u3001\u5ba2\u6237\u7aef\u4e0d\u53ef\u9760\u6027\u548c\u5bf9\u6297\u6027\u653b\u51fb\u7b49\u6311\u6218\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\u90e8\u7f72\u9762\u4e34\u975eIID\u6570\u636e\u3001\u5ba2\u6237\u7aef\u4e0d\u53ef\u9760\u6027\u548c\u5bf9\u6297\u6027\u64cd\u7eb5\u7b49\u6301\u7eed\u6311\u6218\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u516c\u5e73\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FedFiTS\u91c7\u7528\u4e09\u9636\u6bb5\u53c2\u4e0e\u7b56\u7565\uff08\u81ea\u7531\u8bad\u7ec3\u3001\u81ea\u7136\u9009\u62e9\u3001\u5206\u69fd\u56e2\u961f\u53c2\u4e0e\uff09\uff0c\u7ed3\u5408\u52a8\u6001\u5ba2\u6237\u7aef\u8bc4\u5206\u3001\u81ea\u9002\u5e94\u9608\u503c\u548c\u57fa\u4e8e\u961f\u5217\u7684\u8c03\u5ea6\uff0c\u5e73\u8861\u6536\u655b\u6548\u7387\u4e0e\u9c81\u68d2\u6027\u3002", "result": "\u5728\u533b\u5b66\u5f71\u50cf\u3001\u89c6\u89c9\u57fa\u51c6\u548c\u8868\u683c\u519c\u4e1a\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedFiTS\u5728\u51c6\u786e\u6027\u3001\u8fbe\u5230\u76ee\u6807\u65f6\u95f4\u548c\u5bf9\u6295\u6bd2\u653b\u51fb\u7684\u97e7\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8eFedAvg\u3001FedRand\u548cFedPow\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u4fe1\u4efb\u611f\u77e5\u805a\u5408\u548c\u516c\u5e73\u5bfc\u5411\u7684\u5ba2\u6237\u7aef\u9009\u62e9\uff0cFedFiTS\u63a8\u8fdb\u4e86\u53ef\u6269\u5c55\u548c\u5b89\u5168\u7684\u8054\u90a6\u5b66\u4e60\uff0c\u7279\u522b\u9002\u5408\u73b0\u5b9e\u4e16\u754c\u7684\u533b\u7597\u548c\u8de8\u9886\u57df\u90e8\u7f72\u3002"}}
{"id": "2509.19122", "categories": ["cs.LG", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.19122", "abs": "https://arxiv.org/abs/2509.19122", "authors": ["Chunming Ye", "Wenquan Tian", "Yalan Gao", "Songzhou Li"], "title": "Analysis on distribution and clustering of weight", "comment": "14page,16 figures", "summary": "The study on architecture and parameter characteristics remains the hot topic\nin the research of large language models. In this paper we concern with the\ncharacteristics of weight which are used to analyze the correlations and\ndifferences between models. Two kinds of vectors-standard deviation vector and\nclustering vector-are proposed to describe features of models. In the first\ncase, the weights are assumed to follow normal distribution. The standard\ndeviation values of projection matrices are normalized to form\nStandard-Deviation Vector, representing the distribution characteristics of\nmodels. In the second case, the singular values from each weight projection\nmatrix are extracted and grouped by K-Means algorithm. The grouped data with\nthe same type matrix are combined as Clustering Vector to represent the\ncorrelation characteristics of models' weights. The study reveals that these\ntwo vectors can effectively distinguish between different models and clearly\nshow the similarities among models of the same family. Moreover, after\nconducting LoRA fine-tuning with different datasets and models, it is found\nthat the distribution of weights represented by standard deviation vector is\ndirectly influenced by the dataset, but the correlations between different\nweights represented by clustering vector remain unaffected and maintain a high\nconsistency with the pre-trained model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u5411\u91cf\uff08\u6807\u51c6\u5dee\u5411\u91cf\u548c\u805a\u7c7b\u5411\u91cf\uff09\u6765\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6743\u91cd\u7279\u5f81\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u4e0d\u540c\u6a21\u578b\u5e76\u663e\u793a\u540c\u65cf\u6a21\u578b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u7814\u7a76\u53d1\u73b0LoRA\u5fae\u8c03\u540e\uff0c\u6807\u51c6\u5dee\u5411\u91cf\u53d7\u6570\u636e\u96c6\u76f4\u63a5\u5f71\u54cd\uff0c\u800c\u805a\u7c7b\u5411\u91cf\u4fdd\u6301\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u7684\u67b6\u6784\u548c\u53c2\u6570\u7279\u5f81\uff0c\u7279\u522b\u662f\u6743\u91cd\u7279\u5f81\uff0c\u4ee5\u5206\u6790\u6a21\u578b\u95f4\u7684\u76f8\u5173\u6027\u548c\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u6807\u51c6\u5dee\u5411\u91cf\uff08\u5047\u8bbe\u6743\u91cd\u670d\u4ece\u6b63\u6001\u5206\u5e03\uff0c\u5bf9\u6295\u5f71\u77e9\u9635\u6807\u51c6\u5dee\u8fdb\u884c\u5f52\u4e00\u5316\uff09\u548c\u805a\u7c7b\u5411\u91cf\uff08\u5bf9\u6743\u91cd\u6295\u5f71\u77e9\u9635\u5947\u5f02\u503c\u8fdb\u884cK-Means\u805a\u7c7b\u5206\u7ec4\uff09\uff0c\u7528\u4e8e\u63cf\u8ff0\u6a21\u578b\u7279\u5f81\u3002", "result": "\u4e24\u79cd\u5411\u91cf\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u6a21\u578b\uff0c\u6e05\u6670\u663e\u793a\u540c\u65cf\u6a21\u578b\u76f8\u4f3c\u6027\u3002LoRA\u5fae\u8c03\u540e\uff0c\u6807\u51c6\u5dee\u5411\u91cf\u53d7\u6570\u636e\u96c6\u5f71\u54cd\uff0c\u805a\u7c7b\u5411\u91cf\u4fdd\u6301\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u4e00\u81f4\u6027\u3002", "conclusion": "\u6807\u51c6\u5dee\u5411\u91cf\u548c\u805a\u7c7b\u5411\u91cf\u662f\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u7279\u5f81\u7684\u6709\u6548\u5de5\u5177\uff0c\u80fd\u63ed\u793a\u6a21\u578b\u95f4\u7684\u5206\u5e03\u7279\u6027\u548c\u76f8\u5173\u6027\u7279\u5f81\u3002"}}
{"id": "2509.19128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19128", "abs": "https://arxiv.org/abs/2509.19128", "authors": ["Alexandre Pich\u00e9", "Ehsan Kamaloo", "Rafael Pardinas", "Dzmitry Bahdanau"], "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio", "comment": null, "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning\ncapabilities of Large Language Models (LLMs). However, effectively scaling\nthese RL methods presents significant challenges, primarily due to the\ndifficulty in maintaining high AI accelerator utilization without generating\nstale, off-policy data that harms common RL algorithms. This paper introduces\nPipelineRL, an approach designed to achieve a superior trade-off between\nhardware efficiency and data on-policyness for LLM training. PipelineRL employs\nconcurrent asynchronous data generation and model training, distinguished by\nthe novel in-flight weight updates. This mechanism allows the LLM generation\nengine to receive updated model weights with minimal interruption during the\ngeneration of token sequences, thereby maximizing both the accelerator\nutilization and the freshness of training data. Experiments conducted on\nlong-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL\nachieves approximately $\\sim 2x$ faster learning compared to conventional RL\nbaselines while maintaining highly on-policy training data. A scalable and\nmodular open-source implementation of PipelineRL is also released as a key\ncontribution.", "AI": {"tldr": "PipelineRL\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u53d1\u5f02\u6b65\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u7ed3\u5408\u98de\u884c\u4e2d\u6743\u91cd\u66f4\u65b0\u673a\u5236\uff0c\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u786c\u4ef6\u6548\u7387\u4e0e\u6570\u636e\u7b56\u7565\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5728\u6269\u5c55LLM\u63a8\u7406\u80fd\u529b\u65f6\u9762\u4e34\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u548c\u4ea7\u751f\u8fc7\u65f6\u6570\u636e\u7684\u95ee\u9898\uff0c\u8fd9\u4f1a\u5f71\u54cdRL\u7b97\u6cd5\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u5e76\u53d1\u5f02\u6b65\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u5f15\u5165\u98de\u884c\u4e2d\u6743\u91cd\u66f4\u65b0\u673a\u5236\uff0c\u4f7fLLM\u751f\u6210\u5f15\u64ce\u80fd\u5728\u751f\u6210\u4ee4\u724c\u5e8f\u5217\u65f6\u6700\u5c0f\u4e2d\u65ad\u5730\u63a5\u6536\u66f4\u65b0\u540e\u7684\u6a21\u578b\u6743\u91cd\u3002", "result": "\u5728128\u4e2aH100 GPU\u4e0a\u8fdb\u884c\u7684\u957f\u5f62\u5f0f\u63a8\u7406\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cPipelineRL\u6bd4\u4f20\u7edfRL\u57fa\u7ebf\u5b66\u4e60\u901f\u5ea6\u5feb\u7ea62\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5ea6\u7b56\u7565\u6027\u7684\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "PipelineRL\u5728\u786c\u4ef6\u6548\u7387\u548c\u6570\u636e\u7b56\u7565\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6a21\u5757\u5316\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2509.19135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19135", "abs": "https://arxiv.org/abs/2509.19135", "authors": ["Wenying Luo", "Zhiyuan Lin", "Wenhao Xu", "Minghao Liu", "Zhi Li"], "title": "GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding", "comment": null, "summary": "Human mobility traces, often recorded as sequences of check-ins, provide a\nunique window into both short-term visiting patterns and persistent lifestyle\nregularities. In this work we introduce GSTM-HMU, a generative spatio-temporal\nframework designed to advance mobility analysis by explicitly modeling the\nsemantic and temporal complexity of human movement. The framework consists of\nfour key innovations. First, a Spatio-Temporal Concept Encoder (STCE)\nintegrates geographic location, POI category semantics, and periodic temporal\nrhythms into unified vector representations. Second, a Cognitive Trajectory\nMemory (CTM) adaptively filters historical visits, emphasizing recent and\nbehaviorally salient events in order to capture user intent more effectively.\nThird, a Lifestyle Concept Bank (LCB) contributes structured human preference\ncues, such as activity types and lifestyle patterns, to enhance\ninterpretability and personalization. Finally, task-oriented generative heads\ntransform the learned representations into predictions for multiple downstream\ntasks. We conduct extensive experiments on four widely used real-world\ndatasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate\nperformance on three benchmark tasks: next-location prediction, trajectory-user\nidentification, and time estimation. The results demonstrate consistent and\nsubstantial improvements over strong baselines, confirming the effectiveness of\nGSTM-HMU in extracting semantic regularities from complex mobility data. Beyond\nraw performance gains, our findings also suggest that generative modeling\nprovides a promising foundation for building more robust, interpretable, and\ngeneralizable systems for human mobility intelligence.", "AI": {"tldr": "GSTM-HMU\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u65f6\u7a7a\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4eba\u7c7b\u79fb\u52a8\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u590d\u6742\u6027\u6765\u63a8\u8fdb\u79fb\u52a8\u6027\u5206\u6790\u3002\u8be5\u6846\u67b6\u5305\u542b\u56db\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u65f6\u7a7a\u6982\u5ff5\u7f16\u7801\u5668\u3001\u8ba4\u77e5\u8f68\u8ff9\u8bb0\u5fc6\u3001\u751f\u6d3b\u65b9\u5f0f\u6982\u5ff5\u5e93\u548c\u4efb\u52a1\u5bfc\u5411\u751f\u6210\u5934\u3002\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4e09\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002", "motivation": "\u4eba\u7c7b\u79fb\u52a8\u8f68\u8ff9\u8bb0\u5f55\u4e86\u7b7e\u5230\u5e8f\u5217\uff0c\u4e3a\u7814\u7a76\u77ed\u671f\u8bbf\u95ee\u6a21\u5f0f\u548c\u6301\u4e45\u751f\u6d3b\u65b9\u5f0f\u89c4\u5f8b\u63d0\u4f9b\u4e86\u72ec\u7279\u7a97\u53e3\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5efa\u6a21\u79fb\u52a8\u6570\u636e\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6846\u67b6\u6765\u63d0\u53d6\u8bed\u4e49\u89c4\u5f8b\u3002", "method": "1) \u65f6\u7a7a\u6982\u5ff5\u7f16\u7801\u5668\u6574\u5408\u5730\u7406\u4f4d\u7f6e\u3001POI\u7c7b\u522b\u8bed\u4e49\u548c\u5468\u671f\u6027\u65f6\u95f4\u8282\u594f\uff1b2) \u8ba4\u77e5\u8f68\u8ff9\u8bb0\u5fc6\u81ea\u9002\u5e94\u8fc7\u6ee4\u5386\u53f2\u8bbf\u95ee\uff0c\u5f3a\u8c03\u8fd1\u671f\u548c\u884c\u4e3a\u663e\u8457\u4e8b\u4ef6\uff1b3) \u751f\u6d3b\u65b9\u5f0f\u6982\u5ff5\u5e93\u63d0\u4f9b\u7ed3\u6784\u5316\u4eba\u7c7b\u504f\u597d\u7ebf\u7d22\uff1b4) \u4efb\u52a1\u5bfc\u5411\u751f\u6210\u5934\u5c06\u5b66\u4e60\u8868\u793a\u8f6c\u6362\u4e3a\u4e0b\u6e38\u4efb\u52a1\u9884\u6d4b\u3002", "result": "\u5728Gowalla\u3001WeePlace\u3001Brightkite\u548cFourSquare\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cGSTM-HMU\u5728\u4e0b\u4e00\u4e2a\u4f4d\u7f6e\u9884\u6d4b\u3001\u8f68\u8ff9\u7528\u6237\u8bc6\u522b\u548c\u65f6\u95f4\u4f30\u8ba1\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u751f\u6210\u5f0f\u5efa\u6a21\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u4eba\u7c7b\u79fb\u52a8\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u3002GSTM-HMU\u80fd\u591f\u6709\u6548\u4ece\u590d\u6742\u79fb\u52a8\u6570\u636e\u4e2d\u63d0\u53d6\u8bed\u4e49\u89c4\u5f8b\uff0c\u5c55\u793a\u4e86\u751f\u6210\u65b9\u6cd5\u5728\u79fb\u52a8\u5206\u6790\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.19159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19159", "abs": "https://arxiv.org/abs/2509.19159", "authors": ["Qingfeng Lan", "Gautham Vasan", "A. Rupam Mahmood"], "title": "Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions", "comment": "Code release: https://github.com/qlan3/ENN", "summary": "Catastrophic forgetting has remained a significant challenge for efficient\nreinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While\nrecent works have proposed effective methods to mitigate this issue, they\nmainly focus on the algorithmic side. Meanwhile, we do not fully understand\nwhat architectural properties of neural networks lead to catastrophic\nforgetting. This study aims to fill this gap by studying the role of activation\nfunctions in the training dynamics of neural networks and their impact on\ncatastrophic forgetting in reinforcement learning setup. Our study reveals\nthat, besides sparse representations, the gradient sparsity of activation\nfunctions also plays an important role in reducing forgetting. Based on this\ninsight, we propose a new class of activation functions, elephant activation\nfunctions, that can generate both sparse outputs and sparse gradients. We show\nthat by simply replacing classical activation functions with elephant\nactivation functions in the neural networks of value-based algorithms, we can\nsignificantly improve the resilience of neural networks to catastrophic\nforgetting, thus making reinforcement learning more sample-efficient and\nmemory-efficient.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6fc0\u6d3b\u51fd\u6570\u5728\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u52a8\u6001\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u5bf9\u5f3a\u5316\u5b66\u4e60\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6fc0\u6d3b\u51fd\u6570\u7c7b\u522b\u2014\u2014\u5927\u8c61\u6fc0\u6d3b\u51fd\u6570\uff0c\u80fd\u591f\u4ea7\u751f\u7a00\u758f\u8f93\u51fa\u548c\u7a00\u758f\u68af\u5ea6\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u707e\u96be\u6027\u9057\u5fd8\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7b97\u6cd5\u5c42\u9762\uff0c\u800c\u5bf9\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7279\u6027\u5982\u4f55\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u7684\u7406\u89e3\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7279\u522b\u5173\u6ce8\u6fc0\u6d3b\u51fd\u6570\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u6fc0\u6d3b\u51fd\u6570\u5728\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u52a8\u6001\u4e2d\u7684\u89d2\u8272\uff0c\u53d1\u73b0\u68af\u5ea6\u7a00\u758f\u6027\u5bf9\u51cf\u5c11\u9057\u5fd8\u7684\u91cd\u8981\u6027\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u5927\u8c61\u6fc0\u6d3b\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u80fd\u540c\u65f6\u4ea7\u751f\u7a00\u758f\u8f93\u51fa\u548c\u7a00\u758f\u68af\u5ea6\uff0c\u5e76\u5728\u57fa\u4e8e\u4ef7\u503c\u7684\u7b97\u6cd5\u4e2d\u66ff\u6362\u4f20\u7edf\u6fc0\u6d3b\u51fd\u6570\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4ec5\u901a\u8fc7\u5c06\u4f20\u7edf\u6fc0\u6d3b\u51fd\u6570\u66ff\u6362\u4e3a\u5927\u8c61\u6fc0\u6d3b\u51fd\u6570\uff0c\u5c31\u80fd\u663e\u8457\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u5bf9\u707e\u96be\u6027\u9057\u5fd8\u7684\u62b5\u6297\u80fd\u529b\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u66f4\u52a0\u6837\u672c\u9ad8\u6548\u548c\u5185\u5b58\u9ad8\u6548\u3002", "conclusion": "\u6fc0\u6d3b\u51fd\u6570\u7684\u68af\u5ea6\u7a00\u758f\u6027\u5728\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u5927\u8c61\u6fc0\u6d3b\u51fd\u6570\u4e3a\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u67b6\u6784\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19197", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19197", "abs": "https://arxiv.org/abs/2509.19197", "authors": ["Abdul-Rauf Nuhu", "Parham Kebria", "Vahid Hemmati", "Benjamin Lartey", "Mahmoud Nabil Mahmoud", "Abdollah Homaifar", "Edward Tunstel"], "title": "A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness", "comment": null, "summary": "Data-driven models, especially deep learning classifiers often demonstrate\ngreat success on clean datasets. Yet, they remain vulnerable to common data\ndistortions such as adversarial and common corruption perturbations. These\nperturbations can significantly degrade performance, thereby challenging the\noverall reliability of the models. Traditional robustness validation typically\nrelies on perturbed test datasets to assess and improve model performance. In\nour framework, however, we propose a validation approach that extracts \"weak\nrobust\" samples directly from the training dataset via local robustness\nanalysis. These samples, being the most susceptible to perturbations, serve as\nan early and sensitive indicator of the model's vulnerabilities. By evaluating\nmodels on these challenging training instances, we gain a more nuanced\nunderstanding of its robustness, which informs targeted performance\nenhancement. We demonstrate the effectiveness of our approach on models trained\nwith CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation\nguided by weak robust samples can drive meaningful improvements in model\nreliability under adversarial and common corruption scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5c40\u90e8\u9c81\u68d2\u6027\u5206\u6790\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\"\u5f31\u9c81\u68d2\"\u6837\u672c\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u6837\u672c\u4f5c\u4e3a\u6a21\u578b\u8106\u5f31\u6027\u7684\u65e9\u671f\u654f\u611f\u6307\u6807\uff0c\u7528\u4e8e\u6307\u5bfc\u9488\u5bf9\u6027\u6027\u80fd\u63d0\u5347", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u5728\u5e72\u51c0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u5bf9\u6297\u6027\u6270\u52a8\u548c\u5e38\u89c1\u6570\u636e\u5931\u771f\u5f88\u8106\u5f31\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6270\u52a8\u6d4b\u8bd5\u96c6\u8bc4\u4f30\u9c81\u68d2\u6027\uff0c\u4f46\u4e0d\u591f\u9ad8\u6548", "method": "\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u6700\u6613\u53d7\u6270\u52a8\u5f71\u54cd\u7684\u5f31\u9c81\u68d2\u6837\u672c\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5728\u8fd9\u4e9b\u6311\u6218\u6027\u8bad\u7ec3\u5b9e\u4f8b\u4e0a\u7684\u8868\u73b0\u6765\u7406\u89e3\u5176\u9c81\u68d2\u6027", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u57fa\u4e8e\u5f31\u9c81\u68d2\u6837\u672c\u7684\u9c81\u68d2\u6027\u9a8c\u8bc1\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u5bf9\u6297\u6027\u548c\u5e38\u89c1\u5931\u771f\u573a\u666f\u4e0b\u7684\u53ef\u9760\u6027"}}
{"id": "2509.19215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19215", "abs": "https://arxiv.org/abs/2509.19215", "authors": ["Juntong Ni", "Saurabh Kataria", "Shengpu Tang", "Carl Yang", "Xiao Hu", "Wei Jin"], "title": "PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation", "comment": "Accepted at NeurIPS 2025 Workshop on Learning from Time Series for\n  Health", "summary": "Photoplethysmography (PPG) is widely used in wearable health monitoring, yet\nlarge PPG foundation models remain difficult to deploy on resource-limited\ndevices. We present PPG-Distill, a knowledge distillation framework that\ntransfers both global and local knowledge through prediction-, feature-, and\npatch-level distillation. PPG-Distill incorporates morphology distillation to\npreserve local waveform patterns and rhythm distillation to capture inter-patch\ntemporal structures. On heart rate estimation and atrial fibrillation\ndetection, PPG-Distill improves student performance by up to 21.8% while\nachieving 7X faster inference and reducing memory usage by 19X, enabling\nefficient PPG analysis on wearables", "AI": {"tldr": "PPG-Distill\u662f\u4e00\u4e2a\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u3001\u7279\u5f81\u548c\u8865\u4e01\u7ea7\u522b\u7684\u84b8\u998f\uff0c\u5c06\u5168\u5c40\u548c\u5c40\u90e8\u77e5\u8bc6\u4ece\u5927\u578bPPG\u57fa\u7840\u6a21\u578b\u8f6c\u79fb\u5230\u5c0f\u578b\u6a21\u578b\uff0c\u5b9e\u73b0\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548PPG\u5206\u6790", "motivation": "PPG\u5728\u53ef\u7a7f\u6234\u5065\u5eb7\u76d1\u6d4b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5927\u578bPPG\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5", "method": "\u63d0\u51faPPG-Distill\u6846\u67b6\uff0c\u5305\u542b\u5f62\u6001\u84b8\u998f\uff08\u4fdd\u7559\u5c40\u90e8\u6ce2\u5f62\u6a21\u5f0f\uff09\u548c\u8282\u5f8b\u84b8\u998f\uff08\u6355\u6349\u8865\u4e01\u95f4\u65f6\u95f4\u7ed3\u6784\uff09\uff0c\u901a\u8fc7\u9884\u6d4b\u7ea7\u3001\u7279\u5f81\u7ea7\u548c\u8865\u4e01\u7ea7\u84b8\u998f\u5b9e\u73b0\u77e5\u8bc6\u8f6c\u79fb", "result": "\u5728\u5fc3\u7387\u548c\u623f\u98a4\u68c0\u6d4b\u4efb\u52a1\u4e0a\uff0cPPG-Distill\u5c06\u5b66\u751f\u6a21\u578b\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe21.8%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53477\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1119\u500d", "conclusion": "PPG-Distill\u80fd\u591f\u5b9e\u73b0\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548PPG\u5206\u6790\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.19220", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19220", "abs": "https://arxiv.org/abs/2509.19220", "authors": ["Ferdinand Kahenga", "Antoine Bagula", "Patrick Sello", "Sajal K. Das"], "title": "FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity", "comment": null, "summary": "Federated learning in practice must contend with heterogeneous feature\nspaces, severe non-IID data, and scarce labels across clients. We present\nFedFusion, a federated transfer-learning framework that unifies domain\nadaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,\nDivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via\nconfidence-filtered pseudo-labels and domain-adaptive transfer, while clients\nmaintain personalised encoders tailored to local data. To preserve global\ncoherence under heterogeneity, FedFusion employs similarity-weighted classifier\ncoupling (with optional cluster-wise averaging), mitigating dominance by\ndata-rich sites and improving minority-client performance. The frugal-labelling\npipeline combines self-/semi-supervised pretext training with selective\nfine-tuning, reducing annotation demands without sharing raw data. Across\ntabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,\nFedFusion consistently outperforms state-of-the-art baselines in accuracy,\nrobustness, and fairness while maintaining comparable communication and\ncomputation budgets. These results show that harmonising personalisation,\ndomain adaptation, and label efficiency is an effective recipe for robust\nfederated learning under real-world constraints.", "AI": {"tldr": "FedFusion\u662f\u4e00\u4e2a\u8054\u90a6\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u57df\u9002\u5e94\u548c\u8282\u4fed\u6807\u6ce8\u6280\u672f\uff0c\u7ed3\u5408\u591a\u6837\u6027/\u805a\u7c7b\u611f\u77e5\u7f16\u7801\u5668\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5f02\u6784\u7279\u5f81\u7a7a\u95f4\u3001\u975eIID\u6570\u636e\u548c\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u5f02\u6784\u7279\u5f81\u7a7a\u95f4\u3001\u4e25\u91cd\u975eIID\u6570\u636e\u548c\u8de8\u5ba2\u6237\u7aef\u6807\u7b7e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u7684\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002", "method": "FedFusion\u4f7f\u7528\u6807\u8bb0\u7684\u6559\u5e08\u5ba2\u6237\u7aef\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u7684\u4f2a\u6807\u7b7e\u548c\u57df\u81ea\u9002\u5e94\u8fc1\u79fb\u6307\u5bfc\u5b66\u4e60\u8005\u5ba2\u6237\u7aef\uff0c\u540c\u65f6\u5ba2\u6237\u7aef\u7ef4\u62a4\u9488\u5bf9\u672c\u5730\u6570\u636e\u5b9a\u5236\u7684\u4e2a\u6027\u5316\u7f16\u7801\u5668\u3002\u91c7\u7528\u76f8\u4f3c\u6027\u52a0\u6743\u5206\u7c7b\u5668\u8026\u5408\uff08\u53ef\u9009\u805a\u7c7b\u5e73\u5747\uff09\u6765\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u6570\u636e\u4e30\u5bcc\u7ad9\u70b9\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002\u8282\u4fed\u6807\u6ce8\u7ba1\u9053\u7ed3\u5408\u81ea\u76d1\u7763/\u534a\u76d1\u7763\u9884\u8bad\u7ec3\u4e0e\u9009\u62e9\u6027\u5fae\u8c03\u3002", "result": "\u5728\u8868\u683c\u548c\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFedFusion\u5728IID\u3001\u975eIID\u548c\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\uff0c\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u9884\u7b97\u3002", "conclusion": "\u534f\u8c03\u4e2a\u6027\u5316\u3001\u57df\u9002\u5e94\u548c\u6807\u7b7e\u6548\u7387\u662f\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7ea6\u675f\u4e0b\u9c81\u68d2\u8054\u90a6\u5b66\u4e60\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.19222", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19222", "abs": "https://arxiv.org/abs/2509.19222", "authors": ["Julien Delavande", "Regis Pierrard", "Sasha Luccioni"], "title": "Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models", "comment": "10 pages. Accepted as an oral presentation at the NeurIPS 2025\n  NextVid Workshop (San Diego, December 6, 2025)", "summary": "Recent advances in text-to-video (T2V) generation have enabled the creation\nof high-fidelity, temporally coherent clips from natural language prompts. Yet\nthese systems come with significant computational costs, and their energy\ndemands remain poorly understood. In this paper, we present a systematic study\nof the latency and energy consumption of state-of-the-art open-source T2V\nmodels. We first develop a compute-bound analytical model that predicts scaling\nlaws with respect to spatial resolution, temporal length, and denoising steps.\nWe then validate these predictions through fine-grained experiments on\nWAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and\nlinear scaling with the number of denoising steps. Finally, we extend our\nanalysis to six diverse T2V models, comparing their runtime and energy profiles\nunder default settings. Our results provide both a benchmark reference and\npractical insights for designing and deploying more sustainable generative\nvideo systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u6700\u5148\u8fdb\u5f00\u6e90\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u7279\u6027\uff0c\u5efa\u7acb\u4e86\u8ba1\u7b97\u53d7\u9650\u7684\u5206\u6790\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u9884\u6d4b\u7ed3\u679c\u3002", "motivation": "\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u867d\u7136\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u80fd\u8017\u7279\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u80fd\u8017\u5206\u6790\u3002", "method": "\u9996\u5148\u5f00\u53d1\u4e86\u57fa\u4e8e\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u65f6\u95f4\u957f\u5ea6\u548c\u53bb\u566a\u6b65\u6570\u7684\u8ba1\u7b97\u53d7\u9650\u5206\u6790\u6a21\u578b\uff0c\u7136\u540e\u5728WAN2.1-T2V\u6a21\u578b\u4e0a\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6700\u540e\u6269\u5c55\u5230\u516d\u4e2a\u4e0d\u540c\u7684T2V\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u80fd\u8017\u968f\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u968f\u53bb\u566a\u6b65\u6570\u7ebf\u6027\u589e\u957f\uff0c\u4e3a\u4e0d\u540c\u6a21\u578b\u63d0\u4f9b\u4e86\u8fd0\u884c\u65f6\u548c\u80fd\u8017\u57fa\u51c6\u6570\u636e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u548c\u90e8\u7f72\u66f4\u53ef\u6301\u7eed\u7684\u751f\u6210\u5f0f\u89c6\u9891\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u51c6\u53c2\u8003\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.19233", "categories": ["cs.LG", "I.2.0; I.2.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.19233", "abs": "https://arxiv.org/abs/2509.19233", "authors": ["Milad Leyli-abadi", "Antoine Marot", "J\u00e9r\u00f4me Picault"], "title": "Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation", "comment": "Accepted at ECML PKDD ML4SPS 2025 workshop", "summary": "In the context of the energy transition, with increasing integration of\nrenewable sources and cross-border electricity exchanges, power grids are\nencountering greater uncertainty and operational risk. Maintaining grid\nstability under varying conditions is a complex task, and power flow simulators\nare commonly used to support operators by evaluating potential actions before\nimplementation. However, traditional physical solvers, while accurate, are\noften too slow for near real-time use. Machine learning models have emerged as\nfast surrogates, and to improve their adherence to physical laws (e.g.,\nKirchhoff's laws), they are often trained with embedded constraints which are\nalso known as physics-informed or hybrid models. This paper presents an\nablation study to demystify hybridization strategies, ranging from\nincorporating physical constraints as regularization terms or unsupervised\nlosses, and exploring model architectures from simple multilayer perceptrons to\nadvanced graph-based networks enabling the direct optimization of physics\nequations. Using our custom benchmarking pipeline for hybrid models called\nLIPS, we evaluate these models across four dimensions: accuracy, physical\ncompliance, industrial readiness, and out-of-distribution generalization. The\nresults highlight how integrating physical knowledge impacts performance across\nthese criteria. All the implementations are reproducible and provided in the\ncorresponding Github page.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u5206\u6790\u4e86\u7535\u529b\u7cfb\u7edf\u4e2d\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u7269\u7406\u7ea6\u675f\u96c6\u6210\u7b56\u7565\uff0c\u8bc4\u4f30\u4e86\u4ece\u7b80\u5355\u591a\u5c42\u611f\u77e5\u5668\u5230\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u540c\u67b6\u6784\u5728\u51c6\u786e\u6027\u3001\u7269\u7406\u5408\u89c4\u6027\u3001\u5de5\u4e1a\u51c6\u5907\u5ea6\u548c\u5206\u5e03\u5916\u6cdb\u5316\u56db\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u96c6\u6210\u548c\u8de8\u5883\u7535\u529b\u4ea4\u6362\u589e\u52a0\uff0c\u7535\u7f51\u9762\u4e34\u66f4\u5927\u4e0d\u786e\u5b9a\u6027\u548c\u8fd0\u884c\u98ce\u9669\u3002\u4f20\u7edf\u7269\u7406\u6c42\u89e3\u5668\u867d\u7136\u51c6\u786e\u4f46\u901f\u5ea6\u6162\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u5feb\u901f\u66ff\u4ee3\u65b9\u6848\u9700\u8981\u66f4\u597d\u5730\u9075\u5b88\u7269\u7406\u5b9a\u5f8b\u3002", "method": "\u4f7f\u7528\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\u7ba1\u9053LIPS\uff0c\u6bd4\u8f83\u4e86\u5c06\u7269\u7406\u7ea6\u675f\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\u6216\u65e0\u76d1\u7763\u635f\u5931\u7684\u6df7\u5408\u7b56\u7565\uff0c\u4ee5\u53ca\u4ece\u591a\u5c42\u611f\u77e5\u5668\u5230\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u540c\u67b6\u6784\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u7269\u7406\u77e5\u8bc6\u96c6\u6210\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u7269\u7406\u5408\u89c4\u6027\u3001\u5de5\u4e1a\u51c6\u5907\u5ea6\u548c\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u6709\u5b9e\u73b0\u90fd\u662f\u53ef\u590d\u73b0\u7684\uff0c\u5e76\u5728\u76f8\u5e94\u7684Github\u9875\u9762\u4e0a\u63d0\u4f9b\uff0c\u4e3a\u7535\u529b\u7cfb\u7edf\u6df7\u5408\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2509.19284", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19284", "abs": "https://arxiv.org/abs/2509.19284", "authors": ["Yunzhen Feng", "Julia Kempe", "Cheng Zhang", "Parag Jain", "Anthony Hartshorn"], "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT", "comment": null, "summary": "Large reasoning models (LRMs) spend substantial test-time compute on long\nchain-of-thought (CoT) traces, but what *characterizes* an effective CoT\nremains unclear. While prior work reports gains from lengthening CoTs and\nincreasing review (revisiting earlier steps) via appended *wait* tokens, recent\nstudies suggest that shorter thinking can outperform longer traces. We\ntherefore conduct a systematic evaluation across ten LRMs on math and\nscientific reasoning. Contrary to the \"longer-is-better\" narrative, we find\nthat both naive CoT lengthening and increased review are associated with\n*lower* accuracy.\n  As CoT unfolds step by step, token-level metrics can conflate verbosity with\nprocess quality. We introduce a graph view of CoT to extract structure and\nidentify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of\nsteps in abandoned branches-that consistently outpredicts length and review\nratio for correctness across models. To probe causality, we design two\ninterventions. First, we rank candidate CoTs by each metric at test time, where\nFSF yields the largest pass@1 gains; second, we edit CoTs to remove failed\nbranches, which significantly improves accuracy, indicating that failed\nbranches bias subsequent reasoning. Taken together, these results characterize\neffective CoTs as those that *fail less* and support *structure-aware*\ntest-time scaling over indiscriminately generating long CoT.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\uff0c\u66f4\u957f\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u66f4\u591a\u7684\u56de\u987e\u6b65\u9aa4\u53cd\u800c\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86\u5931\u8d25\u6b65\u9aa4\u6bd4\u4f8b\uff08FSF\uff09\u4f5c\u4e3a\u8bc4\u4f30CoT\u6709\u6548\u6027\u7684\u5173\u952e\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u7528\u4e8e\u957f\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u4f46\u4ec0\u4e48\u6784\u6210\u6709\u6548\u7684\u601d\u7ef4\u94fe\u4ecd\u4e0d\u6e05\u695a\u3002\u73b0\u6709\u7814\u7a76\u5b58\u5728\u77db\u76fe\u89c2\u70b9\uff1a\u4e00\u4e9b\u8ba4\u4e3a\u66f4\u957f\u7684\u601d\u7ef4\u94fe\u66f4\u597d\uff0c\u53e6\u4e00\u4e9b\u5219\u8ba4\u4e3a\u77ed\u601d\u7ef4\u94fe\u66f4\u4f18\u3002", "method": "\u572810\u4e2a\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e0a\u5bf9\u6570\u5b66\u548c\u79d1\u5b66\u63a8\u7406\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff1b\u5f15\u5165\u601d\u7ef4\u94fe\u7684\u56fe\u7ed3\u6784\u89c6\u56fe\u6765\u63d0\u53d6\u7ed3\u6784\u7279\u5f81\uff1b\u63d0\u51fa\u5931\u8d25\u6b65\u9aa4\u6bd4\u4f8b\uff08FSF\uff09\u6307\u6807\uff1b\u8bbe\u8ba1\u4e24\u79cd\u5e72\u9884\u5b9e\u9a8c\uff1a\u57fa\u4e8e\u6307\u6807\u6392\u540d\u5019\u9009\u601d\u7ef4\u94fe\u548c\u7f16\u8f91\u601d\u7ef4\u94fe\u79fb\u9664\u5931\u8d25\u5206\u652f\u3002", "result": "\u53d1\u73b0\u601d\u7ef4\u94fe\u957f\u5ea6\u548c\u56de\u987e\u6bd4\u4f8b\u4e0e\u51c6\u786e\u6027\u8d1f\u76f8\u5173\uff1bFSF\u6307\u6807\u5728\u9884\u6d4b\u6b63\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u957f\u5ea6\u548c\u56de\u987e\u6bd4\u4f8b\uff1b\u79fb\u9664\u5931\u8d25\u5206\u652f\u7684\u7f16\u8f91\u64cd\u4f5c\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u6709\u6548\u7684\u601d\u7ef4\u94fe\u662f\u90a3\u4e9b\u5931\u8d25\u6b65\u9aa4\u66f4\u5c11\u7684\u601d\u7ef4\u94fe\uff0c\u652f\u6301\u57fa\u4e8e\u7ed3\u6784\u611f\u77e5\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\uff0c\u800c\u975e\u76f2\u76ee\u751f\u6210\u957f\u601d\u7ef4\u94fe\u3002"}}
