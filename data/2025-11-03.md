<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 17]
- [cs.LG](#cs.LG) [Total: 71]
- [stat.ML](#stat.ML) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Investigation of Superdirectivity in Planar Holographic Arrays](https://arxiv.org/abs/2510.26803)
*Hang Lin,Liuxun Xue,Shu Sun,Ruifeng Gao,Jue Wang,Tengjiao Wang*

Main category: eess.SP

TL;DR: 该论文研究了均匀矩形阵列在holographic MIMO系统中的超方向性特性，建立了URA的数学方向性模型，推导了最大方向性的解析表达式，并通过数值模拟进行了系统分析。


<details>
  <summary>Details</summary>
Motivation: 研究均匀矩形阵列在holographic MIMO系统中的超方向性特性，为5G/6G通信系统中holographic阵列优化提供理论基础。

Method: 建立均匀矩形阵列的数学方向性模型，推导最大方向性的解析表达式，结合数值模拟进行系统分析。

Result: 结果表明通过合理利用耦合效应可以显著增强方向性，但当天线间距过渡到深亚波长尺度时，这种增强效果会出现收益递减。

Conclusion: 该研究为超方向性均匀矩形阵列的设计提供了理论基础，并为5G/6G通信系统中holographic阵列优化提供了有价值的见解。

Abstract: This paper studies the superdirectivity characteristics of uniform
rectangular arrays (URAs) for holographic multiple-input multiple-output
systems. By establishing a mathematical directivity model for the URA, an
analytical expression for the maximum directivity is derived. Accordingly,
systematic analysis is performed in conjunction with numerical simulations.
Results show that the directivity can be significantly enhanced via rational
utilization of coupling effects. However, this enhancement yields diminishing
returns when antenna spacings transition to deep sub-wavelength scales. This
study provides a theoretical basis for the design of superdirective URAs and
offers valuable insights for holographic array optimization in 5G/6G
communication systems.

</details>


### [2] [Joint optimization of microphone array geometry, sensor directivity pattern, and beamforming parameters for linear superarrays](https://arxiv.org/abs/2510.26822)
*Yuanhang Qian,Xueqin Luo,Jilu Jin,Gongping Huang,Jingdong Chen,Jacob Benesty*

Main category: eess.SP

TL;DR: 提出了一种广义线性超阵列优化框架，同时优化阵列几何、单元指向性和波束形成滤波器，以最小化设计波束图与理想指向性模式之间的近似误差。


<details>
  <summary>Details</summary>
Motivation: 现有线性超阵列方法受到限制，因为阵列几何和单元指向性这两个对波束形成性能至关重要的因素没有被联合优化。

Method: 使用Jacobi-Anger级数展开来近似理想指向性模式，通过遗传算法优化阵列几何和单元指向性。

Result: 仿真结果表明，所提出的优化阵列在目标频带和转向范围内比传统线性超阵列实现了更低的近似误差，且在频率和转向角度上表现出更稳定和改善的方向性因子和白噪声增益性能。

Conclusion: 该优化框架能够显著提升线性超阵列的波束形成性能，在多个关键指标上优于传统方法。

Abstract: Linear superarrays (LSAs) have been proposed to address the limited steering
capability of conventional linear differential microphone arrays (LDMAs) by
integrating omnidirectional and directional microphones, enabling more flexible
beamformer designs. However, existing approaches remain limited because array
geometry and element directivity, both critical to beamforming performance, are
not jointly optimized. This paper presents a generalized LSA optimization
framework that simultaneously optimizes array geometry, element directivity,
and the beamforming filter to minimize the approximation error between the
designed beampattern and an ideal directivity pattern (IDP) over the full
frequency band and all steering directions within the region of interest. The
beamformer is derived by approximating the IDP using a Jacobi-Anger series
expansion, while the array geometry and element directivity are optimized via a
genetic algorithm. Simulation results show that the proposed optimized array
achieves lower approximation error than conventional LSAs across the target
frequency band and steering range. Additionally, its directivity factor and
white noise gain demonstrate more stable and improved performance across
frequencies and steering angles.

</details>


### [3] [GeoPep: A geometry-aware masked language model for protein-peptide binding site prediction](https://arxiv.org/abs/2510.27040)
*Dian Chen,Yunkai Chen,Tong Lin,Sijie Chen,Xiaolin Cheng*

Main category: eess.SP

TL;DR: GeoPep是一个基于ESM3多模态蛋白质基础模型的肽结合位点预测框架，通过迁移学习和参数高效神经网络架构，解决了蛋白质-肽相互作用预测中肽构象灵活性和结构数据有限的挑战。


<details>
  <summary>Details</summary>
Motivation: 蛋白质-肽相互作用预测面临肽构象灵活性和结构数据有限的挑战，现有基于蛋白质结构的多模态方法难以直接应用于蛋白质-肽相互作用。

Method: 通过从ESM3多模态蛋白质基础模型进行迁移学习，结合参数高效神经网络架构，并使用基于距离的损失函数利用3D结构信息来增强结合位点预测。

Result: 综合评估表明，GeoPep在蛋白质-肽结合位点预测方面显著优于现有方法，能够有效捕捉稀疏和异质的结合模式。

Conclusion: GeoPep通过迁移学习和结构信息利用，成功解决了蛋白质-肽相互作用预测的挑战，为这一领域提供了有效的解决方案。

Abstract: Multimodal approaches that integrate protein structure and sequence have
achieved remarkable success in protein-protein interface prediction. However,
extending these methods to protein-peptide interactions remains challenging due
to the inherent conformational flexibility of peptides and the limited
availability of structural data that hinder direct training of structure-aware
models. To address these limitations, we introduce GeoPep, a novel framework
for peptide binding site prediction that leverages transfer learning from ESM3,
a multimodal protein foundation model. GeoPep fine-tunes ESM3's rich
pre-learned representations from protein-protein binding to address the limited
availability of protein-peptide binding data. The fine-tuned model is further
integrated with a parameter-efficient neural network architecture capable of
learning complex patterns from sparse data. Furthermore, the model is trained
using distance-based loss functions that exploit 3D structural information to
enhance binding site prediction. Comprehensive evaluations demonstrate that
GeoPep significantly outperforms existing methods in protein-peptide binding
site prediction by effectively capturing sparse and heterogeneous binding
patterns.

</details>


### [4] [Blind MIMO Semantic Communication via Parallel Variational Diffusion: A Completely Pilot-Free Approach](https://arxiv.org/abs/2510.27043)
*Hao Jiang,Xiaojun Yuan,Yinuo Huang,Qinghua Guo*

Main category: eess.SP

TL;DR: 提出了一种名为Blind-MIMOSC的盲多输入多输出语义通信框架，包含DJSCC发射器和基于扩散的盲接收器，无需导频即可同时恢复信道和源数据。


<details>
  <summary>Details</summary>
Motivation: 解决传统通信系统中需要导频进行信道估计的问题，实现完全无导频的联合信道估计和源数据恢复，降低信道带宽需求。

Method: 使用深度联合源信道编码发射器压缩映射源数据，采用并行变分扩散模型的盲接收器，利用两个预训练评分网络分别表征信道和源数据的先验信息。

Result: 实验表明Blind-MIMOSC在信道和源数据恢复精度上优于现有方法，并大幅降低了信道带宽比。

Conclusion: 该框架是首个在块衰落MIMO系统中实现完全无导频的联合信道估计和源恢复的解决方案，具有模块化设计和高效训练的优势。

Abstract: In this paper, we propose a novel blind multi-input multi-output (MIMO)
semantic communication (SC) framework named Blind-MIMOSC that consists of a
deep joint source-channel coding (DJSCC) transmitter and a diffusion-based
blind receiver. The DJSCC transmitter aims to compress and map the source data
into the transmitted signal by exploiting the structural characteristics of the
source data, while the diffusion-based blind receiver employs a parallel
variational diffusion (PVD) model to simultaneously recover the channel and the
source data from the received signal without using any pilots. The PVD model
leverages two pre-trained score networks to characterize the prior information
of the channel and the source data, operating in a plug-and-play manner during
inference. This design allows only the affected network to be retrained when
channel conditions or source datasets change, avoiding the complicated
full-network retraining required by end-to-end methods. This work presents the
first fully pilot-free solution for joint channel estimation and source
recovery in block-fading MIMO systems. Extensive experiments show that
Blind-MIMOSC with PVD achieves superior channel and source recovery accuracy
compared to state-of-the-art approaches, with drastically reduced channel
bandwidth ratio.

</details>


### [5] [Distributed Precoding for Cell-free Massive MIMO in O-RAN: A Multi-agent Deep Reinforcement Learning Framework](https://arxiv.org/abs/2510.27069)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: eess.SP

TL;DR: 提出了一种用于无蜂窝大规模MIMO的分布式可扩展预编码框架，结合多智能体深度强化学习和迭代算法，在保证用户最低数据率的同时提高聚合吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决当前无蜂窝大规模MIMO预编码方案中，集中式方案不可扩展而分布式方案干扰严重的问题，寻求在O-RAN架构下实现高效干扰管理的解决方案。

Method: 采用多时间尺度框架，结合多智能体深度强化学习与迭代算法的专家洞察，通过有限的信息交换来确定预编码矩阵，解决非凸优化问题。

Result: 与分布式正则化迫零(D-RZF)和加权最小均方误差(WMMSE)算法相比，实现了更高的聚合吞吐量；与集中式正则化迫零(C-RZF)相比，在相似吞吐量性能下具有更低的信令开销。

Conclusion: 所提出的分布式预编码框架在无蜂窝大规模MIMO系统中实现了吞吐量与可扩展性之间的良好平衡，为下一代无线系统提供了有效的干扰管理方案。

Abstract: Cell-free massive multiple-input multiple-output (MIMO) is a key technology
for next-generation wireless systems. The integration of cell-free massive MIMO
within the open radio access network (O-RAN) architecture addresses the growing
need for decentralized, scalable, and high-capacity networks that can support
different use cases. Precoding is a crucial step in the operation of cell-free
massive MIMO, where O-RUs steer their beams towards the intended users while
mitigating interference to other users. Current precoding schemes for cell-free
massive MIMO are either fully centralized or fully distributed. Centralized
schemes are not scalable, whereas distributed schemes may lead to a high
inter-O-RU interference. In this paper, we propose a distributed and scalable
precoding framework for cell-free massive MIMO that uses limited information
exchange among precoding agents to mitigate interference. We formulate an
optimization problem for precoding that maximizes the aggregate throughput
while guaranteeing the minimum data rate requirements of users. The formulated
problem is nonconvex. We propose a multi-timescale framework that combines
multi-agent deep reinforcement learning (DRL) with expert insights from an
iterative algorithm to determine the precoding matrices efficiently. We conduct
simulations and compare the proposed framework with the centralized precoding
and distributed precoding methods for different numbers of O-RUs, users, and
transmit antennas. The results show that the proposed framework achieves a
higher aggregate throughput than the distributed regularized zero-forcing
(D-RZF) scheme and the weighted minimum mean square error (WMMSE) algorithm.
When compared with the centralized regularized zero-forcing (C-RZF) scheme, the
proposed framework achieves similar aggregate throughput performance but with a
lower signaling overhead.

</details>


### [6] [RFI Detection and Identification at OVRO Using Pseudonymetry](https://arxiv.org/abs/2510.27078)
*Meles Weldegebriel,Zihan Li,Greg Hellbourg,Ning Zhang,Neal Patwari*

Main category: eess.SP

TL;DR: 在欧文斯谷射电天文台进行的首次Pseudonymetry空中现场演示，展示了异构无线系统间的协作频谱共享。窄带次级发射器在信号中嵌入伪名水印，宽频射电望远镜从频谱图中被动提取水印，可在低信噪比下可靠检测干扰并识别干扰设备。


<details>
  <summary>Details</summary>
Motivation: 随着无线传输在保护频段附近的增加，保护射电天文台免受意外干扰变得至关重要。现有的数据库驱动协调框架和无线电静默区无法快速识别或抑制特定干扰发射器，特别是在低信噪比条件下。

Method: 采用Pseudonymetry技术：窄带次级发射器在信号中嵌入伪名水印，宽频射电望远镜从频谱图数据中被动提取水印。该方法允许在常规解调不可行的低信噪比条件下工作。

Result: 实验结果表明，即使在常规解调不可行的低信噪比条件下，也能可靠地检测干扰并唯一识别干扰设备。

Conclusion: 这些发现验证了被动科学接收器可以参与轻量级反馈循环来触发有害传输的关闭，证明了Pseudonymetry作为保护射电天文环境的补充执法工具的潜力。

Abstract: Protecting radio astronomy observatories from unintended interference is
critical as wireless transmissions increases near protected bands. While
database-driven coordination frameworks and radio quiet zones exist, they
cannot rapidly identify or suppress specific interfering transmitters,
especially at low signal-to-noise ratio (SNR) levels. This paper presents the
first over-the-air field demonstration of Pseudonymetry at the Owens Valley
Radio Observatory (OVRO), illustrating cooperative spectrum sharing between
heterogeneous wireless systems. In our experiment, a narrow-band secondary
transmitter embeds a pseudonym watermark into its signal, while the wide-band
radio telescope passively extracts the watermark from spectrogram data. Results
show that interference can be reliably detected and the interfering device
uniquely identified even at low SNR where conventional demodulation is
infeasible. These findings validate that passive scientific receivers can
participate in a lightweight feedback loop to trigger shutdown of harmful
transmissions, demonstrating the potential of Pseudonymetry as a complementary
enforcement tool for protecting radio astronomy environments.

</details>


### [7] [Unlimited Sampling of Multiband Signals: Single-Channel Acquisition and Recovery](https://arxiv.org/abs/2510.27110)
*Gal Shtendel,Ayush Bhandari*

Main category: eess.SP

TL;DR: 提出了一种在无限采样框架下从模折叠点采样中重建多频带信号的方法，实现了亚奈奎斯特采样，并在硬件实验中获得了高达13倍的动态范围提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统采样中动态范围受限和过度过采样的问题，实现实际的高动态范围多频带采集。

Method: 在无限采样框架下，使用低复杂度单通道采集设置，从模折叠点采样中重建多频带信号。

Result: 建立了恢复保证，证明在USF范式下可实现亚奈奎斯特采样，硬件实验显示动态范围提升高达13倍，支持多达6个频谱带。

Conclusion: 该方法能够在先前受动态范围和过度过采样限制的场景中实现实用的高动态范围多频带采集。

Abstract: In this paper, we address the problem of reconstructing multiband signals
from modulo-folded, pointwise samples within the Unlimited Sensing Framework
(USF). Focusing on a low-complexity, single-channel acquisition setup, we
establish recovery guarantees demonstrating that sub-Nyquist sampling is
achievable under the USF paradigm. In doing so, we also tighten the previous
sampling theorem for bandpass signals. Our recovery algorithm demonstrates up
to a 13x dynamic range improvement in hardware experiments with up to 6
spectral bands. These results enable practical high-dynamic-range multiband
acquisition in scenarios previously limited by dynamic range and excessive
oversampling.

</details>


### [8] [From OFDM to AFDM: Enabling Adaptive Integrated Sensing and Communication in High-Mobility Scenarios](https://arxiv.org/abs/2510.27192)
*Haoran Yin,Yanqun Tang,Jun Xiong,Fan Liu,Yuanhan Ni,Qu Luo,Roberto Bomfin,Marwa Chafii,Marios Kountouris,Christos Masouros*

Main category: eess.SP

TL;DR: 本文综述了AFDM-ISAC技术，这是一种在高移动性场景下实现自适应集成感知与通信的新兴波形技术，通过使用灵活的啁啾信号作为子载波来应对严重的多普勒扩展问题。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中的集成感知与通信（ISAC）在V2X和无人机等高移动性场景中面临严重延迟和多普勒扩展的挑战，传统OFDM波形在多普勒效应下性能严重下降，需要开发新的波形技术。

Method: 采用多普勒弹性仿射频分复用（AFDM）波形，利用灵活的啁啾信号作为子载波，具有类似调频连续波（FMCW）的特性，通过分析其分集阶数、模糊函数和克拉美-罗界来评估ISAC性能极限。

Result: AFDM-ISAC在高移动性场景下展现出巨大潜力，能够有效应对严重的多普勒扩展问题，为自适应ISAC提供了新的技术路径。

Conclusion: AFDM-ISAC是一种有前景的技术，为高移动性场景下的集成感知与通信提供了有效的解决方案，并提出了多种有效的感知算法和未来发展机会。

Abstract: Integrated sensing and communication (ISAC) is a key feature of
next-generation wireless networks, enabling a wide range of emerging
applications such as vehicle-to-everything (V2X) and unmanned aerial vehicles
(UAVs), which operate in high-mobility scenarios. Notably, the wireless
channels within these applications typically exhibit severe delay and Doppler
spreads. The latter causes serious communication performance degradation in the
Orthogonal Frequency-Division Multiplexing (OFDM) waveform that is widely
adopted in current wireless networks. To address this challenge, the recently
proposed Doppler-resilient affine frequency division multiplexing (AFDM)
waveform, which uses flexible chirp signals as subcarriers, shows great
potential for achieving adaptive ISAC in high-mobility scenarios. This article
provides a comprehensive overview of AFDM-ISAC. We begin by presenting the
fundamentals of AFDM-ISAC, highlighting its inherent frequency-modulated
continuous-wave (FMCW)-like characteristics. Then, we explore its ISAC
performance limits by analyzing its diversity order, ambiguity function (AF),
and Cramer-Rao Bound (CRB). Finally, we present several effective sensing
algorithms and opportunities for AFDM-ISAC, with the aim of sparking new ideas
in this emerging field.

</details>


### [9] [Joint Visible Light and Backscatter Communications for Proximity-Based Indoor Asset Tracking Enabled by Energy-Neutral Devices](https://arxiv.org/abs/2510.27217)
*Boxuan Xie,Lauri Mela,Alexis A. Dowhuszko,Yu Bai,Zehui Xiong,Zhu Han,Dusit Niyato,Riku Jäntti*

Main category: eess.SP

TL;DR: 提出了一种结合可见光通信和反向散射通信的混合室内资产追踪系统，为能量中立的物联网设备提供定位服务，无需复杂的光电探测器和主动射频组件。


<details>
  <summary>Details</summary>
Motivation: 为下一代无线系统中的能量中立设备提供基于位置的服务，解决传统可见光定位接收器功耗高、复杂且昂贵的问题。

Method: 设计低复杂度能量中立物联网节点，通过LED接入点收集能量，调制和反射环境RF载波来指示位置；采用多小区VLC部署和频分复用方法减少干扰；在边缘RF读取器开发轻量级粒子滤波跟踪算法。

Result: 定位误差在50%分位数为0.318米，90%分位数为0.634米，在多种室内轨迹中表现稳健。

Conclusion: 该解决方案实现了可扩展、成本效益高且能量中立的室内追踪，适用于普及和边缘辅助的物联网应用。

Abstract: In next-generation wireless systems, providing location-based mobile
computing services for energy-neutral devices has become a crucial objective
for the provision of sustainable Internet of Things (IoT). Visible light
positioning (VLP) has gained great research attention as a complementary method
to radio frequency (RF) solutions since it can leverage ubiquitous lighting
infrastructure. However, conventional VLP receivers often rely on
photodetectors or cameras that are power-hungry, complex, and expensive. To
address this challenge, we propose a hybrid indoor asset tracking system that
integrates visible light communication (VLC) and backscatter communication (BC)
within a simultaneous lightwave information and power transfer (SLIPT)
framework. We design a low-complexity and energy-neutral IoT node, namely
backscatter device (BD) which harvests energy from light-emitting diode (LED)
access points, and then modulates and reflects ambient RF carriers to indicate
its location within particular VLC cells. We present a multi-cell VLC
deployment with frequency division multiplexing (FDM) method that mitigates
interference among LED access points by assigning them distinct frequency pairs
based on a four-color map scheduling principle. We develop a lightweight
particle filter (PF) tracking algorithm at an edge RF reader, where the fusion
of proximity reports and the received backscatter signal strength are employed
to track the BD. Experimental results show that this approach achieves the
positioning error of 0.318 m at 50th percentile and 0.634 m at 90th percentile,
while avoiding the use of complex photodetectors and active RF synthesizing
components at the energy-neutral IoT node. By demonstrating robust performance
in multiple indoor trajectories, the proposed solution enables scalable,
cost-effective, and energy-neutral indoor tracking for pervasive and
edge-assisted IoT applications.

</details>


### [10] [SIM-Assisted End-to-End Co-Frequency Co-Time Full-Duplex System](https://arxiv.org/abs/2510.27270)
*Yida Zhang,Qiuyan Liu,Yuqi Xia,Guoxu Xia,Qiang Wang*

Main category: eess.SP

TL;DR: 提出使用堆叠智能超表面(SIM)集成到射频前端，通过电磁神经网络(EMNN)控制超表面，在波域进行信号处理，显著降低同频同时全双工系统的自干扰和误码率。


<details>
  <summary>Details</summary>
Motivation: 为了进一步抑制同频同时全双工系统中的固有自干扰，需要增强波域信号处理能力。

Method: 将真实超表面抽象为网络隐藏层构建电磁神经网络，采用端到端学习方法控制超表面，在电磁前向传播过程中同步完成传统通信任务。

Result: 仿真结果显示，得益于SIM的额外波域处理能力，SIM辅助的CCFD系统相比传统系统显著降低了误码率。

Conclusion: 研究充分展示了电磁神经网络和SIM辅助的端到端CCFD系统在下一代收发器设计中的潜在应用价值。

Abstract: To further suppress the inherent self-interference (SI) in co-frequency and
co-time full-duplex (CCFD) systems, we propose integrating a stacked
intelligent metasurface (SIM) into the RF front-end to enhance signal
processing in the wave domain. Furthermore, an end-to-end (E2E) learning-based
signal processing method is adopted to control the metasurface. Specifically,
the real metasurface is abstracted as hidden layers of a network, thereby
constructing an electromagnetic neural network (EMNN) to enable driving control
of the real communication system. Traditional communication tasks, such as
channel coding, modulation, precoding, combining, demodulation, and channel
decoding, are synchronously carried out during the electromagnetic (EM) forward
propagation through the metasurface. Simulation results show that, benefiting
from the additional wave-domain processing capability of the SIM, the
SIM-assisted CCFD system achieves significantly reduced bit error rate (BER)
compared with conventional CCFD systems. Our study fully demonstrates the
potential applications of EMNN and SIM-assisted E2E CCFD systems in
next-generation transceiver design.

</details>


### [11] [Variational Bayesian Estimation of Low Earth Orbits for Satellite Communication](https://arxiv.org/abs/2510.27345)
*Anders Malthe Westerkam,Amélia Struyf,Dimitri Lederer,Troels Pedersen,François Quitin*

Main category: eess.SP

TL;DR: 提出了一种基于变分消息传递的算法，用于联合定位和波束跟踪低地球轨道卫星，该算法通过估计圆形轨道参数来获取角度信息。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星通信系统使用毫米波频率和混合模拟-数字架构进行快速波束转向，但由于卫星高速运动，地面站只能短暂观测到卫星，需要有效的跟踪方法。

Method: 使用变分消息传递算法，通过建模卫星轨道为圆形来估计轨道参数，从而直接获取角度信息，实现卫星的联合定位和波束跟踪。

Result: 仿真结果表明，该方法对漏检具有高度鲁棒性，即使在接近地平线时也能可靠跟踪卫星，并能有效缓解混合架构固有的模糊性问题。

Conclusion: 所提出的变分消息传递算法能够有效解决低地球轨道卫星的跟踪问题，在恶劣条件下仍能保持可靠的性能。

Abstract: Low-earth-orbit (LEO) satellite communication systems that use
millimeter-wave frequencies rely on large antenna arrays with hybrid
analog-digital architectures for rapid beam steering. LEO satellites are only
visible from the ground for short periods of times (a few tens of minutes) due
to their high orbital speeds. This paper presents a variational message passing
algorithm for joint localization and beam tracking of a LEO satellite from a
ground station equipped with a hybrid transceiver architecture. The algorithm
relies on estimating the parameters of the orbit, which is modelled as
circular. Angles are then obtained from the orbit in a straightforward manner.
Simulation results show that the proposed method is highly resilient to missed
detections, enables reliable satellite tracking even near the horizon, and
effectively alleviates the ambiguities inherent in hybrid architectures.

</details>


### [12] [Classification of Lower Limb Activities Based on Discrete Wavelet Transform Using On-Body Creeping Wave Propagation](https://arxiv.org/abs/2510.27371)
*Sagar Dutta,Banani Basu,Fazal Ahmed Talukdar*

Main category: eess.SP

TL;DR: 该研究利用人体大腿周围的爬行波传播监测腿部运动，通过柔性天线测量传输系数变化，结合多种分类算法识别六种不同的腿部活动，其中SVM与DWT组合表现最佳，且SAR值符合FCC标准。


<details>
  <summary>Details</summary>
Motivation: 探索利用爬行波在人体大腿周围的传播特性来监测腿部运动，为活动分类提供新的无线传感方法。

Method: 使用两个柔性PET天线测量传输系数变化，应用DWT进行特征提取，并比较SVM、决策树、朴素贝叶斯、KNN、DTW和DCNN等多种分类器性能。

Result: SVM与DWT组合在分类六种腿部活动时表现最优，且天线紧贴大腿时的SAR值符合FCC安全标准。

Conclusion: 爬行波传播可用于有效监测腿部运动，SVM+DWT方法在活动分类中表现最佳，且系统符合电磁安全要求。

Abstract: This article investigates how the creeping wave propagation around the human
thigh could be used to monitor the leg movements. The propagation path around
the human thigh gives information regarding leg motions that can be used for
the classification of activities. The variation of the transmission coefficient
is measured between two on-body polyethylene terephthalate (PET) flexible
antennas for six different leg-based activities that exhibit unique
time-varying signatures. A discrete wavelet transform (DWT) along with
different classifiers, such as support vector machine (SVM), decision trees,
naive Bayes, and K-nearest neighbors (KNN), is applied for feature extraction
and classification to evaluate the efficiency for classifying different
activity signals. Additional algorithms, such as dynamic time warping (DTW) and
deep convolutional neural network (DCNN), have also been implemented, and in
each case, SVM with DWT outperforms the others. Simulation to evaluate a
specific absorption rate (SAR) is carried out as the antenna is positioned on
the human thigh leaving no gap. The results show that the SAR is within the
threshold as per the Federal Communications Commission (FCC) standard.

</details>


### [13] [Classification of Induction Motor Fault and Imbalance Based on Vibration Signal Using Single Antenna's Reactive Near Field](https://arxiv.org/abs/2510.27382)
*Sagar Dutta,Banani Basu,Fazal Ahmed Talukdar*

Main category: eess.SP

TL;DR: 提出一种使用天线作为传感器的创新方法，通过测量反射系数S11来检测感应电机轴承故障和不平衡问题，实现了98.2%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有故障分析方法成本高或需要专业知识安装传感器，需要开发非侵入式且成本效益高的故障检测方法。

Method: 使用全向天线测量时变S11参数，通过频谱图分析故障特征，应用深度学习模型基于S11的幅度和相位进行分类。

Result: 实验验证了故障特征频率，使用S11幅度和相位组合的分类准确率达到98.2%，仅使用幅度为96%，仅使用相位为92.1%。

Conclusion: 基于天线的故障检测方法具有非侵入性、成本效益高的优势，在旋转机械早期故障诊断中具有良好应用前景。

Abstract: Early fault diagnosis is imperative for the proper functioning of rotating
machines. It can reduce economic losses in the industry due to unexpected
failures. Existing fault analysis methods are either expensive or demand
expertise for the installation of the sensors. This article proposes a novel
method for the detection of bearing faults and imbalance in induction motors
using an antenna as the sensor, which is noninvasive and cost-efficient.
Time-varying S11 is measured using an omnidirectional antenna, and it is seen
that the spectrogram of S11 shows unique characteristics for different fault
conditions. The experimental setup has analytically evaluated the vibration
frequencies due to fault and validated the characteristic fault frequency by
applying FFT analysis on the captured S11 data. This article has evaluated the
average power content of the detected signals at normal and different fault
conditions. A deep learning model is used to classify the faults based on the
reflection coefficient ( S11). It is found that classification accuracy of
98.2% is achieved using both magnitude and phase of S11, 96% using the
magnitude of S11 and 92.1% using the phase of S11. The classification accuracy
for different operating frequencies, antenna location, and time windows are
also investigated.

</details>


### [14] [UNILocPro: Unified Localization Integrating Model-Based Geometry and Channel Charting](https://arxiv.org/abs/2510.27394)
*Yuhao Zhang,Guangjin Pan,Musa Furkan Keskin,Ossi Kaltiokallio,Mikko Valkama,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出了UNILocPro统一定位框架，集成基于模型的定位和信道制图方法，适用于混合视距/非视距场景。通过自适应激活机制和多种损失函数联合训练，在无监督学习下实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 解决混合视距/非视距场景下的定位问题，结合模型方法和信道制图的优势，实现无监督学习下的高精度定位，避免对标记训练数据的依赖。

Method: 基于视距/非视距识别进行自适应激活；使用模型方法获取的信息训练信道制图模型；联合使用成对距离损失、三元组损失、视距损失和最优传输损失；提出低复杂度实现UNILoc，通过单次预训练最优传输变换生成标签。

Result: 提出的统一框架相比纯模型方法和纯信道制图方法显著提高了定位精度；UNILocPro在无标记数据情况下达到与全监督指纹定位相当的性能；UNILoc能大幅降低训练复杂度，仅带来轻微性能下降。

Conclusion: UNILocPro框架成功实现了模型方法和信道制图的有效集成，在无监督学习下获得高精度定位性能；低复杂度版本UNILoc在保持良好性能的同时显著降低了计算复杂度。

Abstract: In this paper, we propose a unified localization framework (called UNILocPro)
that integrates model-based localization and channel charting (CC) for mixed
line-of-sight (LoS)/non-line-of-sight (NLoS) scenarios. Specifically, based on
LoS/NLoS identification, an adaptive activation between the model-based and
CC-based methods is conducted. Aiming for unsupervised learning, information
obtained from the model-based method is utilized to train the CC model, where a
pairwise distance loss (involving a new dissimilarity metric design), a triplet
loss (if timestamps are available), a LoS-based loss, and an optimal transport
(OT)-based loss are jointly employed such that the global geometry can be well
preserved. To reduce the training complexity of UNILocPro, we propose a
low-complexity implementation (called UNILoc), where the CC model is trained
with self-generated labels produced by a single pre-training OT transformation,
which avoids iterative Sinkhorn updates involved in the OT-based loss
computation. Extensive numerical experiments demonstrate that the proposed
unified frameworks achieve significantly improved positioning accuracy compared
to both model-based and CC-based methods. Notably, UNILocPro with timestamps
attains performance on par with fully-supervised fingerprinting despite
operating without labelled training data. It is also shown that the
low-complexity UNILoc can substantially reduce training complexity with only
marginal performance degradation.

</details>


### [15] [Estimation of aboveground biomass in a tropical dry forest: An intercomparison of airborne, unmanned, and space laser scanning](https://arxiv.org/abs/2510.27408)
*Nelson Mattié,Arturo Sanchez-Azofeifa,Pablo Crespo-Peremarch,Juan-Ygnacio López-Hernández*

Main category: eess.SP

TL;DR: 本研究通过比较离散和全波形激光扫描数据，结合OLS和贝叶斯SVM方法，改进了热带干旱森林地上生物量的估算方法，发现树高相关变量是关键预测因子，SVM回归在所有激光扫描系统中误差为17.89%。


<details>
  <summary>Details</summary>
Motivation: 根据巴黎气候协定要求各国定期报告温室气体排放，森林在碳减排中起关键作用。热带干旱森林是最不了解的热带森林环境之一，需要准确估算碳储量。

Method: 使用机载、无人机和空间激光扫描数据作为独立变量，结合普通最小二乘和贝叶斯SVM方法，通过变量选择、SVM回归调优和机器学习交叉验证来避免过拟合和欠拟合。

Result: 发现六个与树高相关的变量对AGB估算最重要，哥斯达黎加瓜纳卡斯特省10个永久样地的AGB值在26.02-175.43 Mg/ha之间，SVM回归在所有激光扫描系统中的误差为17.89%，全波形空间激光扫描误差最低(17.07%)。

Conclusion: 激光扫描技术结合机器学习方法能有效估算热带干旱森林地上生物量，为巴黎协定要求的碳储量报告提供可靠数据支持。

Abstract: According to the Paris Climate Change Agreement, all nations are required to
submit reports on their greenhouse gas emissions and absorption every two years
by 2024. Consequently, forests play a crucial role in reducing carbon
emissions, which is essential for meeting these obligations. Recognizing the
significance of forest conservation in the global battle against climate
change, Article 5 of the Paris Agreement emphasizes the need for high-quality
forest data. This study focuses on enhancing methods for mapping aboveground
biomass in tropical dry forests. Tropical dry forests are considered one of the
least understood tropical forest environments; therefore, there is a need for
accurate approaches to estimate carbon pools. We employ a comparative analysis
of AGB estimates, utilizing different discrete and full-waveform laser scanning
datasets in conjunction with Ordinary Least Squares and Bayesian approaches
SVM. Airborne Laser Scanning, Unmanned Laser Scanning, and Space Laser Scanning
were used as independent variables for extracting forest metrics. Variable
selection, SVM regression tuning, and cross-validation via a machine-learning
approach were applied to account for overfitting and underfitting. The results
indicate that six key variables primarily related to tree height: Elev.minimum,
Elev.L3, lev.MAD.mode, Elev.mode, Elev.MAD.median, and Elev.skewness, are
important for AGB estimation using ALSD and ULSD , while Leaf Area Index,
canopy coverage and height, terrain elevation, and full-waveform signal energy
emerged as the most vital variables. AGB values estimated from ten permanent
tropical dry forest plots in Costa Rica Guanacaste province ranged from 26.02
Mg/ha to 175.43 Mg/ha . The SVM regressions demonstrated a 17.89 error across
all laser scanning systems, with SLSF W exhibiting the lowest error 17.07 in
estimating total biomass per plot.

</details>


### [16] [pDANSE: Particle-based Data-driven Nonlinear State Estimation from Nonlinear Measurements](https://arxiv.org/abs/2510.27503)
*Anubhab Ghosh,Yonina C. Eldar,Saikat Chatterjee*

Main category: eess.SP

TL;DR: 提出了一种基于粒子采样的数据驱动非线性状态估计方法(pDANSE)，用于处理状态转移模型未知且测量系统非线性的情况。该方法使用RNN生成高斯先验，并通过重参数化技巧处理非线性测量，避免了计算密集的序列蒙特卡洛方法。


<details>
  <summary>Details</summary>
Motivation: 现有DANSE方法仅适用于线性测量系统，无法处理非线性测量问题。当状态转移模型未知且测量系统非线性时，需要开发新的数据驱动状态估计方法。

Method: 使用RNN基于历史测量生成高斯先验，通过重参数化技巧进行粒子采样来计算状态后验的二阶统计量。采用半监督学习方法，在无标签数据时转为无监督学习。

Result: 在随机Lorenz-63系统上测试了四种非线性测量系统，包括立方非线性、相机模型非线性、半波整流非线性和笛卡尔到球坐标非线性。状态估计性能与完全知道状态转移模型的粒子滤波器相当。

Conclusion: pDANSE方法能够有效处理模型未知和非线性测量问题，性能与基于完整模型知识的传统方法相当，同时避免了计算密集的采样方法。

Abstract: We consider the problem of designing a data-driven nonlinear state estimation
(DANSE) method that uses (noisy) nonlinear measurements of a process whose
underlying state transition model (STM) is unknown. Such a process is referred
to as a model-free process. A recurrent neural network (RNN) provides
parameters of a Gaussian prior that characterize the state of the model-free
process, using all previous measurements at a given time point. In the case of
DANSE, the measurement system was linear, leading to a closed-form solution for
the state posterior. However, the presence of a nonlinear measurement system
renders a closed-form solution infeasible. Instead, the second-order statistics
of the state posterior are computed using the nonlinear measurements observed
at the time point. We address the nonlinear measurements using a
reparameterization trick-based particle sampling approach, and estimate the
second-order statistics of the state posterior. The proposed method is referred
to as particle-based DANSE (pDANSE). The RNN of pDANSE uses sequential
measurements efficiently and avoids the use of computationally intensive
sequential Monte-Carlo (SMC) and/or ancestral sampling. We describe the
semi-supervised learning method for pDANSE, which transitions to unsupervised
learning in the absence of labeled data. Using a stochastic Lorenz-$63$ system
as a benchmark process, we experimentally demonstrate the state estimation
performance for four nonlinear measurement systems. We explore cubic
nonlinearity and a camera-model nonlinearity where unsupervised learning is
used; then we explore half-wave rectification nonlinearity and
Cartesian-to-spherical nonlinearity where semi-supervised learning is used. The
performance of state estimation is shown to be competitive vis-\`a-vis particle
filters that have complete knowledge of the STM of the Lorenz-$63$ system.

</details>


### [17] [Trends and Challenges in Next-Generation GNSS Interference Management](https://arxiv.org/abs/2510.27576)
*Leatile Marata,Mariona Jaramillo-Civill,Tales Imbiriba,Petri Välisuo,Heidi Kuusniemi,Elena Simona Lohan,Pau Closas*

Main category: eess.SP

TL;DR: 本文讨论了GNSS面临的干扰威胁，提出利用人工智能技术增强GNSS抗干扰能力的研究愿景。


<details>
  <summary>Details</summary>
Motivation: 随着GNSS在自动驾驶等新兴应用中的发展，干扰威胁日益严重，传统基于物理模型的信号处理技术难以完全应对复杂干扰形式。

Method: 提出利用人工智能技术，结合数据驱动和物理模型的方法来管理GNSS干扰。

Result: AI技术有望在GNSS干扰管理中发挥关键作用，通过利用数据增强基于物理模型的解决方案。

Conclusion: AI技术是实现更鲁棒GNSS定位的重要途径，需要进一步研究如何有效整合AI与GNSS干扰管理。

Abstract: The global navigation satellite system (GNSS) continues to evolve in order to
meet the demands of emerging applications such as autonomous driving and smart
environmental monitoring. However, these advancements are accompanied by a rise
in interference threats, which can significantly compromise the reliability and
safety of GNSS. Such interference problems are typically addressed through
signal-processing techniques that rely on physics-based mathematical models.
Unfortunately, solutions of this nature can often fail to fully capture the
complex forms of interference. To address this, artificial intelligence
(AI)-inspired solutions are expected to play a key role in future interference
management solutions, thanks to their ability to exploit data in addition to
physics-based models. This magazine paper discusses the main challenges and
tasks required to secure GNSS and present a research vision on how AI can be
leveraged towards achieving more robust GNSS-based positioning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning](https://arxiv.org/abs/2510.26829)
*Svetlana Churina,Niranjan Chebrolu,Kokil Jaidka*

Main category: cs.LG

TL;DR: 论文研究了持续预训练中LLMs对错误信息的易感性，发现即使少量接触错误信息也会导致模型内部表征的持久偏移，类似于人类的虚幻真相效应。


<details>
  <summary>Details</summary>
Motivation: 探索在持续预训练过程中，LLMs是否会像人类一样因重复接触错误信息而改变对事实的信念，填补了数据投毒在动态训练环境中影响的研究空白。

Method: 提出了Layer of Truth框架和数据集，通过注入受控的投毒数据，在不同检查点、模型规模和问题类型下探测中间表征，量化信念动态变化。

Result: 发现即使最小程度的接触也能在已确立的事实上诱导持久的表征漂移，易感性在不同层和模型规模间存在差异。

Conclusion: 持续更新的LLMs存在被忽视的脆弱性：它们能够像人类一样内化错误信息，强调了在模型更新过程中需要强健的事实完整性监控。

Abstract: Large language models (LLMs) continually evolve through pre-training on
ever-expanding web data, but this adaptive process also exposes them to subtle
forms of misinformation. While prior work has explored data poisoning during
static pre-training, the effects of such manipulations under continual
pre-training remain largely unexplored. Drawing inspiration from the illusory
truth effect in human cognition - where repeated exposure to falsehoods
increases belief in their accuracy - we ask whether LLMs exhibit a similar
vulnerability. We investigate whether repeated exposure to false but
confidently stated facts can shift a model's internal representation away from
the truth.
  We introduce Layer of Truth, a framework and dataset for probing belief
dynamics in continually trained LLMs. By injecting controlled amounts of
poisoned data and probing intermediate representations across checkpoints,
model scales, and question types, we quantify when and how factual beliefs
shift. Our findings reveal that even minimal exposure can induce persistent
representational drift in well-established facts, with susceptibility varying
across layers and model sizes. These results highlight an overlooked
vulnerability of continually updated LLMs: their capacity to internalize
misinformation analogously to humans, underscoring the need for robust
monitoring of factual integrity during model updates.

</details>


### [19] [SmoothGuard: Defending Multimodal Large Language Models with Noise Perturbation and Clustering Aggregation](https://arxiv.org/abs/2510.26830)
*Guangzhi Su,Shuchang Huang,Yutong Ke,Zhuohang Liu,Long Qian,Kaizhu Huang*

Main category: cs.LG

TL;DR: 提出了SmoothGuard防御框架，通过随机噪声注入和聚类预测聚合增强多模态大语言模型的对抗鲁棒性


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在跨模态任务中表现出色，但对对抗性攻击高度脆弱，存在安全和可靠性隐患

Method: 在连续模态（如图像、音频）中添加高斯噪声，生成多个候选输出，通过嵌入聚类过滤受对抗性影响的预测，从多数簇中选择最终答案

Result: 在POPE、LLaVA-Bench和MM-SafetyBench上的实验表明，SmoothGuard显著提升了对抗攻击的鲁棒性，同时保持了竞争力

Conclusion: SmoothGuard是一个轻量级、模型无关的防御框架，通过噪声注入和聚类聚合有效增强MLLMs的鲁棒性，最佳噪声范围为0.1-0.2

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across diverse tasks by jointly reasoning over textual and visual inputs.
Despite their success, these models remain highly vulnerable to adversarial
manipulations, raising concerns about their safety and reliability in
deployment. In this work, we first generalize an approach for generating
adversarial images within the HuggingFace ecosystem and then introduce
SmoothGuard, a lightweight and model-agnostic defense framework that enhances
the robustness of MLLMs through randomized noise injection and clustering-based
prediction aggregation. Our method perturbs continuous modalities (e.g., images
and audio) with Gaussian noise, generates multiple candidate outputs, and
applies embedding-based clustering to filter out adversarially influenced
predictions. The final answer is selected from the majority cluster, ensuring
stable responses even under malicious perturbations. Extensive experiments on
POPE, LLaVA-Bench (In-the-Wild), and MM-SafetyBench demonstrate that
SmoothGuard improves resilience to adversarial attacks while maintaining
competitive utility. Ablation studies further identify an optimal noise range
(0.1-0.2) that balances robustness and utility.

</details>


### [20] [Accurate Target Privacy Preserving Federated Learning Balancing Fairness and Utility](https://arxiv.org/abs/2510.26841)
*Kangkang Sun,Jun Wu,Minyi Guo,Jianhua Li,Jianwei Huang*

Main category: cs.LG

TL;DR: 提出了FedPF算法，通过将联邦学习中的公平性和隐私保护转化为零和博弈，揭示了隐私与公平性之间的内在矛盾关系。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中同时确保跨人口群体的公平性和保护敏感客户端数据的基本挑战。

Method: 引入差分隐私公平联邦学习算法(FedPF)，将多目标优化转化为零和博弈，其中公平性和隐私约束与模型效用竞争。

Result: 实验验证显示在三个数据集上歧视减少高达42.9%，同时保持竞争性准确率，但揭示了隐私-公平性张力的不可避免性。

Conclusion: 同时实现隐私和公平性目标需要精心平衡的妥协，而非单独优化任一目标。

Abstract: Federated Learning (FL) enables collaborative model training without data
sharing, yet participants face a fundamental challenge, e.g., simultaneously
ensuring fairness across demographic groups while protecting sensitive client
data. We introduce a differentially private fair FL algorithm (\textit{FedPF})
that transforms this multi-objective optimization into a zero-sum game where
fairness and privacy constraints compete against model utility. Our theoretical
analysis reveals a surprising inverse relationship, i.e., stricter privacy
protection fundamentally limits the system's ability to detect and correct
demographic biases, creating an inherent tension between privacy and fairness.
Counterintuitively, we prove that moderate fairness constraints initially
improve model generalization before causing performance degradation, where a
non-monotonic relationship that challenges conventional wisdom about
fairness-utility tradeoffs. Experimental validation demonstrates up to 42.9 %
discrimination reduction across three datasets while maintaining competitive
accuracy, but more importantly, reveals that the privacy-fairness tension is
unavoidable, i.e., achieving both objectives simultaneously requires carefully
balanced compromises rather than optimization of either in isolation. The
source code for our proposed algorithm is publicly accessible at
https://github.com/szpsunkk/FedPF.

</details>


### [21] [CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs](https://arxiv.org/abs/2510.26843)
*Zhiyuan Ning,Jiawei Shao,Ruge Xu,Xinfei Guo,Jun Zhang,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 提出CAS-Spec方法，通过动态可切换推理加速策略构建推测性草稿模型，并引入动态树级联算法，在无需专门训练的情况下实现无损推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有的自推测解码方法加速效果有限，而依赖专门训练的方法成本高昂。级联草稿模型虽然能提供更好加速，但训练多个模型的成本限制了实际应用。

Method: 使用动态可切换推理加速策略（层稀疏化和激活量化）构建草稿模型，并提出动态树级联算法自适应路由多级草稿模型并分配草稿长度。

Result: CAS-Spec方法相比现有自推测解码方法达到最先进加速效果，平均加速比从1.1倍到2.3倍。DyTC算法相比基线方法分别提升47%和48%的平均加速比。

Conclusion: CAS-Spec方法可轻松集成到现有LLM中，随着自推测解码技术的发展具有进一步加速的潜力。

Abstract: Speculative decoding has become a widely adopted as an effective technique
for lossless inference acceleration when deploying large language models
(LLMs). While on-the-fly self-speculative methods offer seamless integration
and broad utility, they often fall short of the speed gains achieved by methods
relying on specialized training. Cascading a hierarchy of draft models promises
further acceleration and flexibility, but the high cost of training multiple
models has limited its practical application. In this paper, we propose a novel
Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs
speculative draft models by leveraging dynamically switchable inference
acceleration (DSIA) strategies, including layer sparsity and activation
quantization. Furthermore, traditional vertical and horizontal cascade
algorithms are inefficient when applied to self-speculative decoding methods.
We introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the
multi-level draft models and assigns the draft lengths, based on the heuristics
of acceptance rates and latency prediction. Our CAS-Spec method achieves
state-of-the-art acceleration compared to existing on-the-fly speculative
decoding methods, with an average speedup from $1.1\times$ to $2.3\times$ over
autoregressive decoding across various LLMs and datasets. DyTC improves the
average speedup by $47$\% and $48$\% over cascade-based baseline and tree-based
baseline algorithms, respectively. CAS-Spec can be easily integrated into most
existing LLMs and holds promising potential for further acceleration as
self-speculative decoding techniques continue to evolve.

</details>


### [22] [BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs](https://arxiv.org/abs/2510.26892)
*Mahsa Valizadeh,Rui Tuo,James Caverlee*

Main category: cs.LG

TL;DR: 提出了BI-DCGAN，一种贝叶斯扩展的DCGAN，通过引入模型不确定性来解决GAN中的模式崩溃问题，提高生成样本的多样性。


<details>
  <summary>Details</summary>
Motivation: GAN虽然擅长生成合成数据，但存在模式崩溃问题，生成器只能产生有限范围的输出，无法捕捉完整的数据分布。这在需要多样性和不确定性感知的实际应用中尤为严重。

Method: BI-DCGAN通过Bayes by Backprop学习网络权重的分布，并使用平均场变分推断在GAN训练中高效近似后验分布。这是首个基于协方差矩阵分析证明贝叶斯建模能增强GAN样本多样性的理论工作。

Result: 在标准生成基准测试上的广泛实验验证了理论结果，表明BI-DCGAN比传统DCGAN产生更多样化和鲁棒的输出，同时保持训练效率。

Conclusion: BI-DCGAN为需要多样性和不确定性的应用提供了一个可扩展且及时的解决方案，特别是在现代替代方案如扩散模型仍然过于资源密集的情况下。

Abstract: Generative Adversarial Networks (GANs) are proficient at generating synthetic
data but continue to suffer from mode collapse, where the generator produces a
narrow range of outputs that fool the discriminator but fail to capture the
full data distribution. This limitation is particularly problematic, as
generative models are increasingly deployed in real-world applications that
demand both diversity and uncertainty awareness. In response, we introduce
BI-DCGAN, a Bayesian extension of DCGAN that incorporates model uncertainty
into the generative process while maintaining computational efficiency.
BI-DCGAN integrates Bayes by Backprop to learn a distribution over network
weights and employs mean-field variational inference to efficiently approximate
the posterior distribution during GAN training. We establishes the first
theoretical proof, based on covariance matrix analysis, that Bayesian modeling
enhances sample diversity in GANs. We validate this theoretical result through
extensive experiments on standard generative benchmarks, demonstrating that
BI-DCGAN produces more diverse and robust outputs than conventional DCGANs,
while maintaining training efficiency. These findings position BI-DCGAN as a
scalable and timely solution for applications where both diversity and
uncertainty are critical, and where modern alternatives like diffusion models
remain too resource-intensive.

</details>


### [23] [Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering](https://arxiv.org/abs/2510.26898)
*Crystal Su,Kuai Yu,Jingrui Zhang,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: 提出了一种结合本体论与大语言模型的化学工程框架，通过结构化领域知识与生成推理的统一，为过程控制和安全分析等关键工程应用提供透明可审计的方法。


<details>
  <summary>Details</summary>
Motivation: 将符号化结构与神经生成相结合，为大语言模型在过程控制、安全分析等关键工程环境中的应用提供透明且可审计的方法。

Method: 通过数据获取、语义预处理、信息提取和本体映射步骤，将模型训练和推理与COPE本体对齐，生成模板化问答对指导微调，并使用控制焦点解码和引用门来约束输出到本体链接术语。

Result: 开发了一个评估指标来量化语言质量和本体准确性，通过语义检索和迭代验证等反馈机制增强了系统的可解释性和可靠性。

Conclusion: 本体集成的大语言模型框架成功地将结构化领域知识与生成推理相结合，为化学工程中的关键应用提供了透明且可靠的方法。

Abstract: This work presents an ontology-integrated large language model (LLM)
framework for chemical engineering that unites structured domain knowledge with
generative reasoning. The proposed pipeline aligns model training and inference
with the COPE ontology through a sequence of data acquisition, semantic
preprocessing, information extraction, and ontology mapping steps, producing
templated question-answer pairs that guide fine-tuning. A control-focused
decoding stage and citation gate enforce syntactic and factual grounding by
constraining outputs to ontology-linked terms, while evaluation metrics
quantify both linguistic quality and ontological accuracy. Feedback and future
extensions, including semantic retrieval and iterative validation, further
enhance the system's interpretability and reliability. This integration of
symbolic structure and neural generation provides a transparent, auditable
approach for applying LLMs to process control, safety analysis, and other
critical engineering contexts.

</details>


### [24] [Discovering EV Charging Site Archetypes Through Few Shot Forecasting: The First U.S.-Wide Study](https://arxiv.org/abs/2510.26910)
*Kshitij Nikhal,Luke Ackerknecht,Benjamin S. Riggan,Phil Stahlfeld*

Main category: cs.LG

TL;DR: 提出了一个结合聚类和少样本预测的框架，利用大规模充电需求数据集发现站点原型，以提升电动汽车充电需求预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于小规模数据集、简单的时间依赖建模方法，以及对运营历史有限站点的弱泛化能力，需要更准确预测充电行为来支持电网韧性和基础设施规划。

Method: 集成聚类与少样本预测的框架，使用新颖的大规模充电需求数据集发现站点原型，建立原型特定的专家模型进行需求预测。

Result: 原型特定的专家模型在未见站点上的需求预测表现优于全局基线模型。

Conclusion: 通过将预测性能作为基础设施分割的基础，为运营商提供了降低成本、优化能源和定价策略、支持电网韧性的可行见解，这对实现气候目标至关重要。

Abstract: The decarbonization of transportation relies on the widespread adoption of
electric vehicles (EVs), which requires an accurate understanding of charging
behavior to ensure cost-effective, grid-resilient infrastructure. Existing work
is constrained by small-scale datasets, simple proximity-based modeling of
temporal dependencies, and weak generalization to sites with limited
operational history. To overcome these limitations, this work proposes a
framework that integrates clustering with few-shot forecasting to uncover site
archetypes using a novel large-scale dataset of charging demand. The results
demonstrate that archetype-specific expert models outperform global baselines
in forecasting demand at unseen sites. By establishing forecast performance as
a basis for infrastructure segmentation, we generate actionable insights that
enable operators to lower costs, optimize energy and pricing strategies, and
support grid resilience critical to climate goals.

</details>


### [25] [Quantitative Bounds for Length Generalization in Transformers](https://arxiv.org/abs/2510.27015)
*Zachary Izzo,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 本文首次量化了Transformer模型实现长度泛化所需的训练序列长度阈值，分析了不同问题设置下的泛化条件。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer长度泛化问题：模型在短序列上训练后能否在长序列上保持性能。前人工作表明存在有限阈值，但未量化该阈值大小。

Method: 分析多种问题设置：ℓ∞误差控制vs平均误差控制、无限精度softmax注意力vs有限精度注意力、单层vs双层Transformer。证明当长序列内部行为能被训练中短序列行为"模拟"时，长度泛化发生。

Result: 提供了训练数据长度要求的定性估计，并通过实验验证了这些见解。所有场景下都证明了长度泛化的发生条件。

Conclusion: 结果深化了对Transformer外推机制的理论理解，并形式化了更复杂任务需要更丰富训练数据才能泛化的直觉。

Abstract: We study the problem of length generalization (LG) in transformers: the
ability of a model trained on shorter sequences to maintain performance when
evaluated on much longer, previously unseen inputs. Prior work by Huang et al.
(2025) established that transformers eventually achieve length generalization
once the training sequence length exceeds some finite threshold, but left open
the question of how large it must be. In this work, we provide the first
quantitative bounds on the required training length for length generalization
to occur. Motivated by previous empirical and theoretical work, we analyze LG
in several distinct problem settings: $\ell_\infty$ error control vs. average
error control over an input distribution, infinite-precision softmax attention
vs. finite-precision attention (which reduces to an argmax) in the transformer,
and one- vs. two-layer transformers. In all scenarios, we prove that LG occurs
when the internal behavior of the transformer on longer sequences can be
"simulated" by its behavior on shorter sequences seen during training. Our
bounds give qualitative estimates for the length of training data required for
a transformer to generalize, and we verify these insights empirically. These
results sharpen our theoretical understanding of the mechanisms underlying
extrapolation in transformers, and formalize the intuition that richer training
data is required for generalization on more complex tasks.

</details>


### [26] [MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models](https://arxiv.org/abs/2510.26937)
*Zimeng Huang,Jinxin Ke,Xiaoxuan Fan,Yufeng Yang,Yang Liu,Liu Zhonghan,Zedi Wang,Junteng Dai,Haoyi Jiang,Yuyu Zhou,Keze Wang,Ziliang Chen*

Main category: cs.LG

TL;DR: 提出了MM-OPERA基准测试，用于评估大型视觉语言模型在关联推理方面的能力，包含两个开放任务和11,497个实例，通过LLM-as-a-Judge策略评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在关联推理方面存在不足，缺乏像人类那样的创造性思维和知识整合能力，现有基准测试难以评估开放式的关联推理。

Method: 设计MM-OPERA基准，包含远程项目关联和上下文关联两个开放任务，采用LLM-as-a-Judge策略评估自由形式的响应和显式推理路径。

Result: 对最先进的LVLMs进行了广泛的实证研究，包括任务实例敏感性分析、评估策略有效性分析和能力多样性分析，揭示了当前模型在关联推理方面的局限性。

Conclusion: 该研究为开发更接近人类智能和通用AI提供了重要基础，通过系统性评估揭示了当前LVLMs在关联推理方面的不足。

Abstract: Large Vision-Language Models (LVLMs) have exhibited remarkable progress.
However, deficiencies remain compared to human intelligence, such as
hallucination and shallow pattern matching. In this work, we aim to evaluate a
fundamental yet underexplored intelligence: association, a cornerstone of human
cognition for creative thinking and knowledge integration. Current benchmarks,
often limited to closed-ended tasks, fail to capture the complexity of
open-ended association reasoning vital for real-world applications. To address
this, we present MM-OPERA, a systematic benchmark with 11,497 instances across
two open-ended tasks: Remote-Item Association (RIA) and In-Context Association
(ICA), aligning association intelligence evaluation with human psychometric
principles. It challenges LVLMs to resemble the spirit of divergent thinking
and convergent associative reasoning through free-form responses and explicit
reasoning paths. We deploy tailored LLM-as-a-Judge strategies to evaluate
open-ended outputs, applying process-reward-informed judgment to dissect
reasoning with precision. Extensive empirical studies on state-of-the-art
LVLMs, including sensitivity analysis of task instances, validity analysis of
LLM-as-a-Judge strategies, and diversity analysis across abilities, domains,
languages, cultures, etc., provide a comprehensive and nuanced understanding of
the limitations of current LVLMs in associative reasoning, paving the way for
more human-like and general-purpose AI. The dataset and code are available at
https://github.com/MM-OPERA-Bench/MM-OPERA.

</details>


### [27] [Exploring Landscapes for Better Minima along Valleys](https://arxiv.org/abs/2510.27153)
*Tong Zhao,Jiacheng Li,Yuanchang Zhou,Guangming Tan,Weile Jia*

Main category: cs.LG

TL;DR: 提出了一种名为"E"的适配器，用于梯度优化器，使其在达到局部最小值后继续探索损失景观中的低谷区域，以寻找更好的局部最小值，从而提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有优化器在达到局部最小值后停止搜索，但复杂损失景观中的局部最小值可能不是最优的，且不一定具有最佳泛化性能。需要继续探索以找到更低、更平坦的局部最小值。

Method: 设计了一个适配器"E"，可以集成到梯度优化器中。该适配器使优化器在达到局部最小值后继续沿着损失景观中的低谷区域进行探索，搜索潜在的更好局部最小值。

Result: 在大型批次训练场景中测试，改进后的Lamb优化器（ALTO）相比当前最优优化器，在各种大型批次训练任务中平均提高了2.5%的测试准确率（泛化能力）。

Conclusion: 该方法为优化算法设计开辟了新的研究方向，通过继续探索损失景观中的低谷区域，能够找到更低、更平坦的局部最小值，从而获得更好的泛化性能。

Abstract: Finding lower and better-generalizing minima is crucial for deep learning.
However, most existing optimizers stop searching the parameter space once they
reach a local minimum. Given the complex geometric properties of the loss
landscape, it is difficult to guarantee that such a point is the lowest or
provides the best generalization. To address this, we propose an adaptor "E"
for gradient-based optimizers. The adapted optimizer tends to continue
exploring along landscape valleys (areas with low and nearly identical losses)
in order to search for potentially better local minima even after reaching a
local minimum. This approach increases the likelihood of finding a lower and
flatter local minimum, which is often associated with better generalization. We
also provide a proof of convergence for the adapted optimizers in both convex
and non-convex scenarios for completeness. Finally, we demonstrate their
effectiveness in an important but notoriously difficult training scenario,
large-batch training, where Lamb is the benchmark optimizer. Our testing
results show that the adapted Lamb, ALTO, increases the test accuracy
(generalization) of the current state-of-the-art optimizer by an average of
2.5% across a variety of large-batch training tasks. This work potentially
opens a new research direction in the design of optimization algorithms.

</details>


### [28] [Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction](https://arxiv.org/abs/2510.26940)
*Ashwin Kumar,Hanyu Zhang,David A. Schweidel,William Yeoh*

Main category: cs.LG

TL;DR: 本文审计了最先进的移动性预测模型，发现基于用户人口统计数据的隐藏差异，并提出公平性引导的增量采样方法(FGIS)来减少群体间性能差距。


<details>
  <summary>Details</summary>
Motivation: 移动位置预测在众多应用中至关重要，但其社会影响尚未充分探索。研究发现现有模型在种族和民族用户群体间存在系统性预测性能差异。

Method: 提出公平性引导增量采样(FGIS)，使用大小感知K均值(SAKM)在潜在移动空间中聚类用户并强制执行人口普查比例，然后基于预期性能增益和当前群体代表性进行优先级采样。

Result: 该方法将群体间总差异减少了高达40%，同时保持整体准确性，在早期采样阶段改进最为显著。

Conclusion: 研究揭示了移动性预测流程中的结构性不平等，并展示了轻量级、以数据为中心的干预措施如何在低数据应用中以最小复杂度改善公平性。

Abstract: Next location prediction underpins a growing number of mobility, retail, and
public-health applications, yet its societal impacts remain largely unexplored.
In this paper, we audit state-of-the-art mobility prediction models trained on
a large-scale dataset, highlighting hidden disparities based on user
demographics. Drawing from aggregate census data, we compute the difference in
predictive performance on racial and ethnic user groups and show a systematic
disparity resulting from the underlying dataset, resulting in large differences
in accuracy based on location and user groups. To address this, we propose
Fairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy
designed for incremental data collection settings. Because individual-level
demographic labels are unavailable, we introduce Size-Aware K-Means (SAKM), a
clustering method that partitions users in latent mobility space while
enforcing census-derived group proportions. This yields proxy racial labels for
the four largest groups in the state: Asian, Black, Hispanic, and White. Built
on these labels, our sampling algorithm prioritizes users based on expected
performance gains and current group representation. This method incrementally
constructs training datasets that reduce demographic performance gaps while
preserving overall accuracy. Our method reduces total disparity between groups
by up to 40\% with minimal accuracy trade-offs, as evaluated on a state-of-art
MetaPath2Vec model and a transformer-encoder model. Improvements are most
significant in early sampling stages, highlighting the potential for
fairness-aware strategies to deliver meaningful gains even in low-resource
settings. Our findings expose structural inequities in mobility prediction
pipelines and demonstrate how lightweight, data-centric interventions can
improve fairness with little added complexity, especially for low-data
applications.

</details>


### [29] [Soft Task-Aware Routing of Experts for Equivariant Representation Learning](https://arxiv.org/abs/2510.27222)
*Jaebyeong Jeon,Hyeonseo Jang,Jy-yong Sohn,Kibok Lee*

Main category: cs.LG

TL;DR: 提出了Soft Task-Aware Routing (STAR)方法，通过将投影头建模为专家，让它们分别捕获共享或任务特定信息，减少冗余特征学习，在迁移学习中取得一致改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用分离的投影头同时学习不变和等变表示，但忽视了它们之间的共享信息，导致冗余特征学习和模型容量利用效率低下。

Method: 引入STAR路由策略，将投影头建模为专家，通过软任务感知路由机制让专家专门捕获共享信息或任务特定信息。

Result: 观察到不变和等变嵌入之间的典型相关性降低，在多种迁移学习任务中取得一致性能提升。

Conclusion: STAR通过建模投影头为专家并促进信息共享，有效减少了冗余特征学习，提高了表示学习效率。

Abstract: Equivariant representation learning aims to capture variations induced by
input transformations in the representation space, whereas invariant
representation learning encodes semantic information by disregarding such
transformations. Recent studies have shown that jointly learning both types of
representations is often beneficial for downstream tasks, typically by
employing separate projection heads. However, this design overlooks information
shared between invariant and equivariant learning, which leads to redundant
feature learning and inefficient use of model capacity. To address this, we
introduce Soft Task-Aware Routing (STAR), a routing strategy for projection
heads that models them as experts. STAR induces the experts to specialize in
capturing either shared or task-specific information, thereby reducing
redundant feature learning. We validate this effect by observing lower
canonical correlations between invariant and equivariant embeddings.
Experimental results show consistent improvements across diverse transfer
learning tasks. The code is available at https://github.com/YonseiML/star.

</details>


### [30] [Can machines think efficiently?](https://arxiv.org/abs/2510.26954)
*Adam Winchell*

Main category: cs.LG

TL;DR: 提出一个新的图灵测试版本，在原有模仿游戏基础上增加能耗约束，通过效率视角评估智能，将抽象思维问题与有限资源现实联系起来。


<details>
  <summary>Details</summary>
Motivation: 原始图灵测试已不足以区分人类和机器智能，先进AI系统已能通过原测试并引发严重伦理和环境问题，迫切需要更新测试标准。

Method: 在原有图灵测试的基础上增加能耗约束，要求评估智能时必须考虑能量消耗，将时间节省与总资源成本进行权衡。

Result: 新测试通过能耗约束为智能评估提供了可测量的实践终点，迫使社会在使用人工智能的时间节省与总资源成本之间进行权衡。

Conclusion: 基于能耗约束的新图灵测试能够更全面地评估智能，将思维效率与资源消耗联系起来，为人工智能发展提供更实用的评估框架。

Abstract: The Turing Test is no longer adequate for distinguishing human and machine
intelligence. With advanced artificial intelligence systems already passing the
original Turing Test and contributing to serious ethical and environmental
concerns, we urgently need to update the test. This work expands upon the
original imitation game by accounting for an additional factor: the energy
spent answering the questions. By adding the constraint of energy, the new test
forces us to evaluate intelligence through the lens of efficiency, connecting
the abstract problem of thinking to the concrete reality of finite resources.
Further, this proposed new test ensures the evaluation of intelligence has a
measurable, practical finish line that the original test lacks. This additional
constraint compels society to weigh the time savings of using artificial
intelligence against its total resource cost.

</details>


### [31] [Predicting Household Water Consumption Using Satellite and Street View Images in Two Indian Cities](https://arxiv.org/abs/2510.26957)
*Qiao Wang,Joseph George*

Main category: cs.LG

TL;DR: 使用公开可用的卫星图像、Google街景分割和简单地理空间数据预测印度城市家庭用水量，无需传统调查方法


<details>
  <summary>Details</summary>
Motivation: 传统家庭用水监测方法成本高、耗时，需要寻找更高效、低成本的替代方案

Method: 比较四种方法：调查特征（基准）、CNN嵌入（卫星、街景、组合）、街景语义地图加辅助数据，采用序数分类框架

Result: 街景分割加遥感协变量达到0.55准确率，接近调查模型的0.59准确率，在用水量分布两端有高精度

Conclusion: 开放获取图像结合最小地理空间数据可为城市分析提供可靠的家庭用水量估计，是调查的有效替代方案

Abstract: Monitoring household water use in rapidly urbanizing regions is hampered by
costly, time-intensive enumeration methods and surveys. We investigate whether
publicly available imagery-satellite tiles, Google Street View (GSV)
segmentation-and simple geospatial covariates (nightlight intensity, population
density) can be utilized to predict household water consumption in
Hubballi-Dharwad, India. We compare four approaches: survey features
(benchmark), CNN embeddings (satellite, GSV, combined), and GSV semantic maps
with auxiliary data. Under an ordinal classification framework, GSV
segmentation plus remote-sensing covariates achieves 0.55 accuracy for water
use, approaching survey-based models (0.59 accuracy). Error analysis shows high
precision at extremes of the household water consumption distribution, but
confusion among middle classes is due to overlapping visual proxies. We also
compare and contrast our estimates for household water consumption to that of
household subjective income. Our findings demonstrate that open-access imagery,
coupled with minimal geospatial data, offers a promising alternative to
obtaining reliable household water consumption estimates using surveys in urban
analytics.

</details>


### [32] [Fine-Grained Iterative Adversarial Attacks with Limited Computation Budget](https://arxiv.org/abs/2510.26981)
*Zhichao Hou,Weizhi Gao,Xiaorui Liu*

Main category: cs.LG

TL;DR: 提出了一种在有限计算预算下最大化对抗攻击效力的方法，通过细粒度控制机制选择性地重新计算层激活，在同等成本下优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决AI安全研究中在有限计算资源下的关键挑战：在固定计算预算下如何最大化迭代对抗攻击的强度。粗略减少攻击迭代次数会降低成本但显著削弱效果。

Method: 提出细粒度控制机制，在迭代和层级两个维度上选择性地重新计算层激活，以在约束预算内实现可达到的攻击效能。

Result: 大量实验表明，该方法在同等成本下始终优于现有基线方法。当集成到对抗训练中时，仅使用原始预算的30%就能获得相当的性能。

Conclusion: 该方法有效解决了有限计算预算下的对抗攻击优化问题，为AI安全研究提供了高效的解决方案。

Abstract: This work tackles a critical challenge in AI safety research under limited
compute: given a fixed computation budget, how can one maximize the strength of
iterative adversarial attacks? Coarsely reducing the number of attack
iterations lowers cost but substantially weakens effectiveness. To fulfill the
attainable attack efficacy within a constrained budget, we propose a
fine-grained control mechanism that selectively recomputes layer activations
across both iteration-wise and layer-wise levels. Extensive experiments show
that our method consistently outperforms existing baselines at equal cost.
Moreover, when integrated into adversarial training, it attains comparable
performance with only 30% of the original budget.

</details>


### [33] [HADSF: Aspect Aware Semantic Control for Explainable Recommendation](https://arxiv.org/abs/2510.26994)
*Zheng Nie,Peijie Sun*

Main category: cs.LG

TL;DR: 提出了HADSF框架，通过双阶段语义处理解决LLM在评论推荐系统中的冗余、幻觉和成本问题，引入新指标评估表示保真度，并在大规模实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的评论推荐系统存在三个主要问题：(i)无范围控制的自由形式评论挖掘导致冗余和噪声表示，(ii)缺乏将LLM幻觉与下游效果关联的原则性指标，(iii)未探索不同模型规模下的成本-质量权衡。

Method: HADSF采用双阶段方法：首先通过自适应选择诱导紧凑的语料级方面词汇表，然后执行词汇表引导的、显式约束的结构化方面-观点三元组提取。引入ADR和OFR指标评估表示保真度。

Result: 在约300万条评论和1.5B-70B参数的LLM上实验表明，HADSF集成到标准评分预测器中能持续降低预测误差，并使较小模型在代表性部署场景中达到竞争性能。

Conclusion: HADSF框架有效解决了LLM增强型可解释推荐中的关键挑战，通过控制提取范围、量化幻觉影响和优化成本-质量权衡，为可复现研究提供了完整工具链。

Abstract: Recent advances in large language models (LLMs) promise more effective
information extraction for review-based recommender systems, yet current
methods still (i) mine free-form reviews without scope control, producing
redundant and noisy representations, (ii) lack principled metrics that link LLM
hallucination to downstream effectiveness, and (iii) leave the cost-quality
trade-off across model scales largely unexplored. We address these gaps with
the Hyper-Adaptive Dual-Stage Semantic Framework (HADSF), a two-stage approach
that first induces a compact, corpus-level aspect vocabulary via adaptive
selection and then performs vocabulary-guided, explicitly constrained
extraction of structured aspect-opinion triples. To assess the fidelity of the
resulting representations, we introduce Aspect Drift Rate (ADR) and Opinion
Fidelity Rate (OFR) and empirically uncover a nonmonotonic relationship between
hallucination severity and rating prediction error. Experiments on
approximately 3 million reviews across LLMs spanning 1.5B-70B parameters show
that, when integrated into standard rating predictors, HADSF yields consistent
reductions in prediction error and enables smaller models to achieve
competitive performance in representative deployment scenarios. We release
code, data pipelines, and metric implementations to support reproducible
research on hallucination-aware, LLM-enhanced explainable recommendation. Code
is available at https://github.com/niez233/HADSF

</details>


### [34] [Gradient Descent as Loss Landscape Navigation: a Normative Framework for Deriving Learning Rules](https://arxiv.org/abs/2510.26997)
*John J. Vastola,Samuel J. Gershman,Kanaka Rajan*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，将学习规则视为在损失景观中导航的策略，并将最优规则识别为相关最优控制问题的解。


<details>
  <summary>Details</summary>
Motivation: 传统学习规则通常是假设而非推导的，需要理解为什么某些规则比其他规则更好，以及在什么假设下可以认为给定规则是最优的。

Method: 将学习规则建模为在（部分可观测）损失景观中导航的策略，并将最优规则识别为相关最优控制问题的解。

Result: 在该框架下，梯度下降、动量法、自然梯度、非梯度规则和Adam等自适应优化器都可以自然出现，持续学习策略如权重重置也可以被理解为对任务不确定性的最优响应。

Conclusion: 该框架通过统一目标澄清了学习的计算结构，并为设计自适应算法提供了原则性基础。

Abstract: Learning rules -- prescriptions for updating model parameters to improve
performance -- are typically assumed rather than derived. Why do some learning
rules work better than others, and under what assumptions can a given rule be
considered optimal? We propose a theoretical framework that casts learning
rules as policies for navigating (partially observable) loss landscapes, and
identifies optimal rules as solutions to an associated optimal control problem.
A range of well-known rules emerge naturally within this framework under
different assumptions: gradient descent from short-horizon optimization,
momentum from longer-horizon planning, natural gradients from accounting for
parameter space geometry, non-gradient rules from partial controllability, and
adaptive optimizers like Adam from online Bayesian inference of loss landscape
shape. We further show that continual learning strategies like weight resetting
can be understood as optimal responses to task uncertainty. By unifying these
phenomena under a single objective, our framework clarifies the computational
structure of learning and offers a principled foundation for designing adaptive
algorithms.

</details>


### [35] [Functional embeddings enable Aggregation of multi-area SEEG recordings over subjects and sessions](https://arxiv.org/abs/2510.27090)
*Sina Javadzadeh,Rahil Soroushmojdehi,S. Alireza Seyyed Mousavi,Mehrnaz Asadi,Sumiko Abe,Terence D. Sanger*

Main category: cs.LG

TL;DR: 提出了一种可扩展的表征学习框架，通过功能嵌入和Transformer建模，实现跨受试者的颅内神经数据聚合，无需严格任务结构和统一电极布局。


<details>
  <summary>Details</summary>
Motivation: 解决跨受试者颅内记录聚合的挑战，因为电极数量、位置和覆盖区域差异很大，传统的空间归一化方法无法准确捕捉功能相似性。

Method: 使用孪生编码器和对比学习学习电极的功能身份表征，然后通过Transformer建模区域间关系，支持可变通道数。

Result: 学习的功能空间支持准确的受试者内区分，形成清晰的区域一致性聚类，能够零样本迁移到未见通道，Transformer能够捕获跨区域依赖关系。

Conclusion: 该框架为大规模跨受试者颅内神经数据聚合和预训练提供了一条可行路径，特别适用于缺乏严格任务结构和统一传感器布局的场景。

Abstract: Aggregating intracranial recordings across subjects is challenging since
electrode count, placement, and covered regions vary widely. Spatial
normalization methods like MNI coordinates offer a shared anatomical reference,
but often fail to capture true functional similarity, particularly when
localization is imprecise; even at matched anatomical coordinates, the targeted
brain region and underlying neural dynamics can differ substantially between
individuals. We propose a scalable representation-learning framework that (i)
learns a subject-agnostic functional identity for each electrode from
multi-region local field potentials using a Siamese encoder with contrastive
objectives, inducing an embedding geometry that is locality-sensitive to
region-specific neural signatures, and (ii) tokenizes these embeddings for a
transformer that models inter-regional relationships with a variable number of
channels. We evaluate this framework on a 20-subject dataset spanning basal
ganglia-thalamic regions collected during flexible rest/movement recording
sessions with heterogeneous electrode layouts. The learned functional space
supports accurate within-subject discrimination and forms clear,
region-consistent clusters; it transfers zero-shot to unseen channels. The
transformer, operating on functional tokens without subject-specific heads or
supervision, captures cross-region dependencies and enables reconstruction of
masked channels, providing a subject-agnostic backbone for downstream decoding.
Together, these results indicate a path toward large-scale, cross-subject
aggregation and pretraining for intracranial neural data where strict task
structure and uniform sensor placement are unavailable.

</details>


### [36] [A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms](https://arxiv.org/abs/2510.27001)
*Elise Wolf*

Main category: cs.LG

TL;DR: 提出了一个可复现的多臂老虎机算法评估框架，系统比较了8种经典和方差感知算法，发现方差感知算法在高不确定性环境中具有优势


<details>
  <summary>Details</summary>
Motivation: 多臂老虎机算法评估缺乏标准化条件和可复现性，特别是方差感知扩展算法的性能高度依赖环境条件

Method: 开发了Bandit Playground代码库，包含明确定义的实验设置、多种性能指标和交互式评估界面

Result: 方差感知算法在奖励差异细微的高不确定性环境中表现更好，而经典算法在可分离场景或经过充分调优时表现相当或更好

Conclusion: 贡献包括：(1) 系统评估MAB算法的框架；(2) 方差感知方法优于经典方法的具体条件

Abstract: Multi-armed bandit (MAB) problems serve as a fundamental building block for
more complex reinforcement learning algorithms. However, evaluating and
comparing MAB algorithms remains challenging due to the lack of standardized
conditions and replicability. This is particularly problematic for
variance-aware extensions of classical methods like UCB, whose performance can
heavily depend on the underlying environment. In this study, we address how
performance differences between bandit algorithms can be reliably observed, and
under what conditions variance-aware algorithms outperform classical ones. We
present a reproducible evaluation designed to systematically compare eight
classical and variance-aware MAB algorithms. The evaluation framework,
implemented in our Bandit Playground codebase, features clearly defined
experimental setups, multiple performance metrics (reward, regret, reward
distribution, value-at-risk, and action optimality), and an interactive
evaluation interface that supports consistent and transparent analysis. We show
that variance-aware algorithms can offer advantages in settings with high
uncertainty where the difficulty arises from subtle differences between arm
rewards. In contrast, classical algorithms often perform equally well or better
in more separable scenarios or if fine-tuned extensively. Our contributions are
twofold: (1) a framework for systematic evaluation of MAB algorithms, and (2)
insights into the conditions under which variance-aware approaches outperform
their classical counterparts.

</details>


### [37] [Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase](https://arxiv.org/abs/2510.27002)
*Mihir Mahajan,Alfred Nguyen,Franz Srambical,Stefan Bauer*

Main category: cs.LG

TL;DR: Jasmine是一个基于JAX的高性能世界建模代码库，支持从单机到数百个加速器的扩展，实现比现有开源实现快一个数量级的CoinRun案例复现。


<details>
  <summary>Details</summary>
Motivation: 解决世界建模领域训练基础设施不足的问题，为机器人等领域克服数据稀缺性提供开放训练平台。

Method: 开发基于JAX的世界建模代码库，在数据加载、训练和检查点等方面进行性能优化，支持多种分片配置和完全可复现的训练。

Result: Jasmine实现了比现有开源实现快一个数量级的CoinRun案例复现速度，并建立了包含大规模数据集的严格基准测试管道。

Conclusion: Jasmine为世界建模提供了高性能、可扩展的基础设施，支持跨模型系列和架构消融的严格基准测试。

Abstract: While world models are increasingly positioned as a pathway to overcoming
data scarcity in domains such as robotics, open training infrastructure for
world modeling remains nascent. We introduce Jasmine, a performant JAX-based
world modeling codebase that scales from single hosts to hundreds of
accelerators with minimal code changes. Jasmine achieves an order-of-magnitude
faster reproduction of the CoinRun case study compared to prior open
implementations, enabled by performance optimizations across data loading,
training and checkpointing. The codebase guarantees fully reproducible training
and supports diverse sharding configurations. By pairing Jasmine with curated
large-scale datasets, we establish infrastructure for rigorous benchmarking
pipelines across model families and architectural ablations.

</details>


### [38] [Mixture-of-Transformers Learn Faster: A Theoretical Study on Classification Problems](https://arxiv.org/abs/2510.27004)
*Hongbo Li,Qinhang Wu,Sen Lin,Yingbin Liang,Ness B. Shroff*

Main category: cs.LG

TL;DR: 本文提出了混合变换器(MoT)理论框架，通过门控网络实现变换器块的专家专业化，证明了其能显著提升训练效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的混合专家(MoE)模型缺乏统一的理论解释，特别是在前馈层和注意力层都允许专业化的情况下。需要建立理论框架来理解专家专业化的核心学习动态。

Method: 提出混合变换器(MoT)框架，每个变换器块作为专家，由连续训练的门控网络控制。开发了三阶段训练算法，包括门控网络的连续训练。

Result: 证明每个变换器专家专门处理不同类别的任务，门控网络能准确路由数据样本。训练将预期预测损失在O(log(ϵ⁻¹))迭代步数内驱动到接近零，显著优于单个变换器的O(ϵ⁻¹)收敛率。

Conclusion: 该研究首次提供了变换器级专业化和学习动态的统一理论解释，为设计高效大规模模型提供了实践指导。

Abstract: Mixture-of-Experts (MoE) models improve transformer efficiency but lack a
unified theoretical explanation, especially when both feed-forward and
attention layers are allowed to specialize. To this end, we study the
Mixture-of-Transformers (MoT), a tractable theoretical framework in which each
transformer block acts as an expert governed by a continuously trained gating
network. This design allows us to isolate and study the core learning dynamics
of expert specialization and attention alignment. In particular, we develop a
three-stage training algorithm with continuous training of the gating network,
and show that each transformer expert specializes in a distinct class of tasks
and that the gating network accurately routes data samples to the correct
expert. Our analysis shows how expert specialization reduces gradient conflicts
and makes each subtask strongly convex. We prove that the training drives the
expected prediction loss to near zero in $O(\log(\epsilon^{-1}))$ iteration
steps, significantly improving over the $O(\epsilon^{-1})$ rate for a single
transformer. We further validate our theoretical findings through extensive
real-data experiments, demonstrating the practical effectiveness of MoT.
Together, these results offer the first unified theoretical account of
transformer-level specialization and learning dynamics, providing practical
guidance for designing efficient large-scale models.

</details>


### [39] [Enhancing Sentiment Classification with Machine Learning and Combinatorial Fusion](https://arxiv.org/abs/2510.27014)
*Sean Patten,Pin-Yu Chen,Christina Schweikert,D. Frank Hsu*

Main category: cs.LG

TL;DR: 提出了一种基于组合融合分析(CFA)的情感分类新方法，通过整合多样化的机器学习模型，在IMDB情感分析数据集上达到了97.072%的最先进准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常通过扩大单个模型规模来提升性能，但计算资源消耗大。本文旨在利用认知多样性概念，通过量化模型间的差异性来更高效地组合预测结果。

Method: 采用组合融合分析(CFA)框架，利用秩-得分特征函数量化模型间的不相似性，战略性地结合RoBERTa架构的transformer模型与传统机器学习模型（随机森林、SVM、XGBoost）的预测。

Result: 在IMDB情感分析数据集上达到97.072%的准确率，优于传统集成方法，且计算资源使用相对高效。

Conclusion: CFA方法通过有效计算和利用模型多样性，在情感分类任务中实现了优异的性能，提供了一种比单纯扩大模型规模更高效的替代方案。

Abstract: This paper presents a novel approach to sentiment classification using the
application of Combinatorial Fusion Analysis (CFA) to integrate an ensemble of
diverse machine learning models, achieving state-of-the-art accuracy on the
IMDB sentiment analysis dataset of 97.072\%. CFA leverages the concept of
cognitive diversity, which utilizes rank-score characteristic functions to
quantify the dissimilarity between models and strategically combine their
predictions. This is in contrast to the common process of scaling the size of
individual models, and thus is comparatively efficient in computing resource
use. Experimental results also indicate that CFA outperforms traditional
ensemble methods by effectively computing and employing model diversity. The
approach in this paper implements the combination of a transformer-based model
of the RoBERTa architecture with traditional machine learning models, including
Random Forest, SVM, and XGBoost.

</details>


### [40] [Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning](https://arxiv.org/abs/2510.27044)
*Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.LG

TL;DR: 研究发现RLVR方法在数学推理任务中虽然能提升评估指标，但往往是通过强化表面启发式而非获得真正的推理策略，揭示了RLVR泛化能力的局限性。


<details>
  <summary>Details</summary>
Motivation: 探究强化学习与可验证奖励（RLVR）在促进真正数学推理能力方面的有效性，因为虽然RLVR在增强语言模型数学推理能力方面显示出潜力，但其是否真正培养推理能力仍不明确。

Method: 在两个具有完全可验证解的组合问题上（活动调度和最长递增子序列）研究RLVR，使用精心策划的具有唯一最优解的数据集，并测试多种奖励设计。

Result: RLVR改善了评估指标，但通常是通过强化表面启发式方法而非获得新的推理策略，这表明RLVR的泛化能力有限。

Conclusion: 需要能够区分真正数学推理与捷径利用的基准测试，并提供对进展的忠实衡量，以推动该领域的发展。

Abstract: Mathematical reasoning is a central challenge for large language models
(LLMs), requiring not only correct answers but also faithful reasoning
processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as
a promising approach for enhancing such capabilities; however, its ability to
foster genuine reasoning remains unclear. We investigate RLVR on two
combinatorial problems with fully verifiable solutions: \emph{Activity
Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully
curated datasets with unique optima. Across multiple reward designs, we find
that RLVR improves evaluation metrics but often by reinforcing superficial
heuristics rather than acquiring new reasoning strategies. These findings
highlight the limits of RLVR generalization, emphasizing the importance of
benchmarks that disentangle genuine mathematical reasoning from shortcut
exploitation and provide faithful measures of progress. Code available at
https://github.com/xashru/rlvr-seq-generalization.

</details>


### [41] [Consistency Training Helps Stop Sycophancy and Jailbreaks](https://arxiv.org/abs/2510.27062)
*Alex Irpan,Alexander Matt Turner,Mark Kurzeja,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 本文提出一致性训练方法，通过让模型对提示中的无关线索保持响应不变性，来减少LLM的附和性和越狱攻击敏感性。


<details>
  <summary>Details</summary>
Motivation: LLM的事实性和拒绝训练容易被简单的提示修改所破坏，模型经常表现出附和用户信念或满足不当请求的问题。

Method: 采用两种一致性训练方法：基于外部输出的BCT和基于内部激活的ACT，通过数据增强使模型在不同提示变体下保持相同行为。

Result: 两种方法都有效降低了Gemini 2.5 Flash对无关线索的敏感性，BCT在减少越狱方面表现更好，且不会降低模型能力。

Conclusion: 某些对齐问题应被视为一致性问题而非最优响应问题，BCT可以简化训练流程，减少对静态数据集的依赖。

Abstract: An LLM's factuality and refusal training can be compromised by simple changes
to a prompt. Models often adopt user beliefs (sycophancy) or satisfy
inappropriate requests which are wrapped within special text (jailbreaking). We
explore \emph{consistency training}, a self-supervised paradigm that teaches a
model to be invariant to certain irrelevant cues in the prompt. Instead of
teaching the model what exact response to give on a particular prompt, we aim
to teach the model to behave identically across prompt data augmentations (like
adding leading questions or jailbreak text). We try enforcing this invariance
in two ways: over the model's external outputs (\emph{Bias-augmented
Consistency Training} (BCT) from Chua et al. [2025]) and over its internal
activations (\emph{Activation Consistency Training} (ACT), a method we
introduce). Both methods reduce Gemini 2.5 Flash's susceptibility to irrelevant
cues. Because consistency training uses responses from the model itself as
training data, it avoids issues that arise from stale training data, such as
degrading model capabilities or enforcing outdated response guidelines. While
BCT and ACT reduce sycophancy equally well, BCT does better at jailbreak
reduction. We think that BCT can simplify training pipelines by removing
reliance on static datasets. We argue that some alignment problems are better
viewed not in terms of optimal responses, but rather as consistency issues.

</details>


### [42] [Towards a Measure of Algorithm Similarity](https://arxiv.org/abs/2510.27063)
*Shairoz Sohail,Taher Ali*

Main category: cs.LG

TL;DR: 提出了EMOC框架，将算法实现嵌入到适合下游任务的特征空间中，用于评估算法之间的相似性


<details>
  <summary>Details</summary>
Motivation: 解决如何确定两个算法是否真正不同的问题，这在克隆检测和程序合成等应用中需要实用且一致的相似性度量

Method: 引入EMOC（评估-内存-操作-复杂度）框架，编译PACD数据集，包含三个问题的已验证Python实现

Result: EMOC特征支持算法类型的聚类和分类、近重复检测以及LLM生成程序的多样性量化

Conclusion: 发布了代码、数据和计算EMOC嵌入的工具，以促进算法相似性研究的可重复性和未来工作

Abstract: Given two algorithms for the same problem, can we determine whether they are
meaningfully different? In full generality, the question is uncomputable, and
empirically it is muddied by competing notions of similarity. Yet, in many
applications (such as clone detection or program synthesis) a pragmatic and
consistent similarity metric is necessary. We review existing equivalence and
similarity notions and introduce EMOC: An
Evaluation-Memory-Operations-Complexity framework that embeds algorithm
implementations into a feature space suitable for downstream tasks. We compile
PACD, a curated dataset of verified Python implementations across three
problems, and show that EMOC features support clustering and classification of
algorithm types, detection of near-duplicates, and quantification of diversity
in LLM-generated programs. Code, data, and utilities for computing EMOC
embeddings are released to facilitate reproducibility and future work on
algorithm similarity.

</details>


### [43] [MLPerf Automotive](https://arxiv.org/abs/2510.27065)
*Radoyeh Shojaei,Predrag Djurdjevic,Mostafa El-Khamy,James Goel,Kasper Mecklenburg,John Owens,Pınar Muyan-Özçelik,Tom St. John,Jinho Suh,Arjun Suresh*

Main category: cs.LG

TL;DR: MLPerf Automotive是首个用于评估汽车系统中AI加速机器学习系统的标准化公开基准，由MLCommons和自动驾驶计算联盟合作开发，专注于汽车感知任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准套件无法满足汽车机器学习系统的独特约束，如安全性和实时处理需求，因此需要专门的标准性能评估方法。

Method: 开发了包含延迟和准确性指标的基准框架，涵盖2D目标检测、2D语义分割和3D目标检测等汽车感知任务，提供参考模型和提交规则。

Result: 建立了首个标准化汽车机器学习基准，提供了可重现的性能比较框架，并公开了基准代码。

Conclusion: MLPerf Automotive填补了汽车机器学习系统标准化评估的空白，为不同硬件平台和软件实现的性能比较提供了统一标准。

Abstract: We present MLPerf Automotive, the first standardized public benchmark for
evaluating Machine Learning systems that are deployed for AI acceleration in
automotive systems. Developed through a collaborative partnership between
MLCommons and the Autonomous Vehicle Computing Consortium, this benchmark
addresses the need for standardized performance evaluation methodologies in
automotive machine learning systems. Existing benchmark suites cannot be
utilized for these systems since automotive workloads have unique constraints
including safety and real-time processing that distinguish them from the
domains that previously introduced benchmarks target. Our benchmarking
framework provides latency and accuracy metrics along with evaluation protocols
that enable consistent and reproducible performance comparisons across
different hardware platforms and software implementations. The first iteration
of the benchmark consists of automotive perception tasks in 2D object
detection, 2D semantic segmentation, and 3D object detection. We describe the
methodology behind the benchmark design including the task selection, reference
models, and submission rules. We also discuss the first round of benchmark
submissions and the challenges involved in acquiring the datasets and the
engineering efforts to develop the reference implementations. Our benchmark
code is available at https://github.com/mlcommons/mlperf_automotive.

</details>


### [44] [Towards Understanding Self-play for LLM Reasoning](https://arxiv.org/abs/2510.27072)
*Justin Yang Chae,Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.LG

TL;DR: 该研究分析了自博弈训练在大型语言模型数学推理中的训练动态，通过与RLVR和SFT比较，探讨了参数更新稀疏性、熵动态和奖励函数等机制。


<details>
  <summary>Details</summary>
Motivation: 虽然自博弈训练在领域内和领域外都显示出强大收益，但其改进机制仍不明确，需要深入理解自博弈与其他后训练策略的差异。

Method: 使用Absolute Zero Reasoner框架，比较自博弈与RLVR和SFT，分析参数更新稀疏性、令牌分布熵动态和替代提议者奖励函数。

Result: 研究阐明了自博弈与其他后训练策略的差异，揭示了其固有局限性，并通过pass@k评估连接训练动态与推理性能。

Conclusion: 研究结果明确了自博弈的独特机制，指出了通过自博弈改进LLM数学推理的未来方向。

Abstract: Recent advances in large language model (LLM) reasoning, led by reinforcement
learning with verifiable rewards (RLVR), have inspired self-play post-training,
where models improve by generating and solving their own problems. While
self-play has shown strong in-domain and out-of-domain gains, the mechanisms
behind these improvements remain poorly understood. In this work, we analyze
the training dynamics of self-play through the lens of the Absolute Zero
Reasoner, comparing it against RLVR and supervised fine-tuning (SFT). Our study
examines parameter update sparsity, entropy dynamics of token distributions,
and alternative proposer reward functions. We further connect these dynamics to
reasoning performance using pass@k evaluations. Together, our findings clarify
how self-play differs from other post-training strategies, highlight its
inherent limitations, and point toward future directions for improving LLM math
reasoning through self-play.

</details>


### [45] [QiNN-QJ: A Quantum-inspired Neural Network with Quantum Jump for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.27091)
*Yiwei Chen,Kehuan Yan,Yu Pan,Daoyi Dong*

Main category: cs.LG

TL;DR: 提出了一种量子启发的神经网络QiNN-QJ，通过量子跳跃算子实现多模态纠缠建模，解决了传统量子启发模型中训练不稳定和泛化能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有量子启发融合模型主要依赖幺正变换生成量子纠缠，虽然理论表达能力强，但存在训练不稳定和泛化能力有限的问题。

Method: 首先将每个模态编码为量子纯态，然后通过可微分模块模拟量子跳跃算子将可分离乘积态转换为纠缠表示，联合学习哈密顿量和Lindblad算子生成可控的跨模态纠缠。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS等基准数据集上实现了优于最先进模型的性能，并通过冯诺依曼纠缠熵增强了后验可解释性。

Conclusion: 为纠缠多模态融合建立了原则性框架，为量子启发方法在复杂跨模态相关性建模中开辟了新途径。

Abstract: Quantum theory provides non-classical principles, such as superposition and
entanglement, that inspires promising paradigms in machine learning. However,
most existing quantum-inspired fusion models rely solely on unitary or
unitary-like transformations to generate quantum entanglement. While
theoretically expressive, such approaches often suffer from training
instability and limited generalizability. In this work, we propose a
Quantum-inspired Neural Network with Quantum Jump (QiNN-QJ) for multimodal
entanglement modelling. Each modality is firstly encoded as a quantum pure
state, after which a differentiable module simulating the QJ operator
transforms the separable product state into the entangled representation. By
jointly learning Hamiltonian and Lindblad operators, QiNN-QJ generates
controllable cross-modal entanglement among modalities with dissipative
dynamics, where structured stochasticity and steady-state attractor properties
serve to stabilize training and constrain entanglement shaping. The resulting
entangled states are projected onto trainable measurement vectors to produce
predictions. In addition to achieving superior performance over the
state-of-the-art models on benchmark datasets, including CMU-MOSI, CMU-MOSEI,
and CH-SIMS, QiNN-QJ facilitates enhanced post-hoc interpretability through
von-Neumann entanglement entropy. This work establishes a principled framework
for entangled multimodal fusion and paves the way for quantum-inspired
approaches in modelling complex cross-modal correlations.

</details>


### [46] [Hierarchical Bayesian Model for Gene Deconvolution and Functional Analysis in Human Endometrium Across the Menstrual Cycle](https://arxiv.org/abs/2510.27097)
*Crystal Su,Kuai Yu,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: 提出了一种概率层次贝叶斯模型，用于从异质性样本的bulk RNA-seq数据中反卷积出细胞类型特异性表达谱和比例，利用单细胞参考数据，应用于人类子宫内膜组织月经周期的研究。


<details>
  <summary>Details</summary>
Motivation: 解决bulk组织RNA测序数据平均化基因表达谱的问题，无法揭示细胞类型特异性动态变化，特别是在细胞组成发生显著变化的生物学背景下。

Method: 开发概率层次贝叶斯模型，利用高分辨率单细胞参考数据，通过详细的模型结构、先验分布和推理策略，对bulk RNA-seq数据进行反卷积分析。

Result: 揭示了月经周期不同阶段上皮细胞、基质细胞和免疫细胞比例的动态变化，识别了细胞类型特异性差异基因表达（如分泌期基质细胞中的蜕膜化标记物），模型对参考数据不匹配和噪声具有鲁棒性。

Conclusion: 该贝叶斯方法为研究细胞组成变化的生物学过程提供了有力工具，在生育和子宫内膜疾病方面具有潜在临床意义，未来可整合空间转录组学数据。

Abstract: Bulk tissue RNA sequencing of heterogeneous samples provides averaged gene
expression profiles, obscuring cell type-specific dynamics. To address this, we
present a probabilistic hierarchical Bayesian model that deconvolves bulk
RNA-seq data into constituent cell-type expression profiles and proportions,
leveraging a high-resolution single-cell reference. We apply our model to human
endometrial tissue across the menstrual cycle, a context characterized by
dramatic hormone-driven cellular composition changes. Our extended framework
provides a principled inference of cell type proportions and cell-specific gene
expression changes across cycle phases. We demonstrate the model's structure,
priors, and inference strategy in detail, and we validate its performance with
simulations and comparisons to existing methods. The results reveal dynamic
shifts in epithelial, stromal, and immune cell fractions between menstrual
phases, and identify cell-type-specific differential gene expression associated
with endometrial function (e.g., decidualization markers in stromal cells
during the secretory phase). We further conduct robustness tests and show that
our Bayesian approach is resilient to reference mismatches and noise. Finally,
we discuss the biological significance of our findings, potential clinical
implications for fertility and endometrial disorders, and future directions,
including integration of spatial transcriptomics.

</details>


### [47] [Group-Sensitive Offline Contextual Bandits](https://arxiv.org/abs/2510.27123)
*Yihong Guo,Junjie Luo,Guodong Gao,Ritu Agarwal,Anqi Liu*

Main category: cs.LG

TL;DR: 提出了一种离线上下文赌博机中的群体敏感公平约束方法，通过引入群体间奖励差异约束来减少策略学习过程中可能出现的奖励差异，同时保持整体性能。


<details>
  <summary>Details</summary>
Motivation: 离线策略优化在最大化总体期望奖励时可能无意中放大群体间的奖励差异，导致某些群体比其他群体获益更多，这在资源有限的情况下引发公平性担忧。

Method: 提出了一个约束离线策略优化框架，将群体间奖励差异约束引入基于梯度的离策略优化过程，使用双重稳健估计器改进群体间奖励差异的估计，并提供策略优化的收敛保证。

Result: 在合成和真实数据集上的实验结果表明，该方法能有效减少奖励差异，同时保持有竞争力的整体性能。

Conclusion: 该方法成功解决了离线上下文赌博机中的群体公平性问题，通过约束优化框架平衡了公平性和性能目标。

Abstract: Offline contextual bandits allow one to learn policies from
historical/offline data without requiring online interaction. However, offline
policy optimization that maximizes overall expected rewards can unintentionally
amplify the reward disparities across groups. As a result, some groups might
benefit more than others from the learned policy, raising concerns about
fairness, especially when the resources are limited. In this paper, we study a
group-sensitive fairness constraint in offline contextual bandits, reducing
group-wise reward disparities that may arise during policy learning. We tackle
the following common-parity requirements: the reward disparity is constrained
within some user-defined threshold or the reward disparity should be minimized
during policy optimization. We propose a constrained offline policy
optimization framework by introducing group-wise reward disparity constraints
into an off-policy gradient-based optimization procedure. To improve the
estimation of the group-wise reward disparity during training, we employ a
doubly robust estimator and further provide a convergence guarantee for policy
optimization. Empirical results in synthetic and real-world datasets
demonstrate that our method effectively reduces reward disparities while
maintaining competitive overall performance.

</details>


### [48] [AI Agents in Drug Discovery](https://arxiv.org/abs/2510.27130)
*Srijit Seal,Dinh Long Huynh,Moudather Chelbi,Sara Khosravi,Ankur Kumar,Mattson Thieme,Isaac Wilks,Mark Davies,Jessica Mustali,Yannick Sun,Nick Edwards,Daniil Boiko,Andrei Tyrin,Douglas W. Selinger,Ayaan Parikh,Rahul Vijayan,Shoman Kasbekar,Dylan Reid,Andreas Bender,Ola Spjuth*

Main category: cs.LG

TL;DR: 本文综述了基于大语言模型的AI智能体在药物发现领域的应用，展示了这些系统如何通过自主推理、行动和学习来加速药物研发流程。


<details>
  <summary>Details</summary>
Motivation: AI智能体作为变革性工具，能够整合多样化的生物医学数据，执行任务，通过机器人平台进行实验，并在闭环中迭代优化假设，从而显著提升药物发现的效率和效果。

Method: 构建基于大语言模型的AI智能体架构，包括ReAct、Reflection、Supervisor和Swarm系统，结合感知、计算、行动和记忆工具，应用于药物发现的关键阶段。

Result: 早期实施显示在速度、可重复性和可扩展性方面取得显著进展，将原本需要数月的工作流程压缩到数小时，同时保持科学可追溯性。

Conclusion: 虽然面临数据异质性、系统可靠性、隐私和基准测试等挑战，但AI智能体系统在支持科学和转化方面展现出巨大潜力，是首个展示在操作药物发现环境中部署AI智能体系统实际影响和可量化成果的全面工作。

Abstract: Artificial intelligence (AI) agents are emerging as transformative tools in
drug discovery, with the ability to autonomously reason, act, and learn through
complicated research workflows. Building on large language models (LLMs)
coupled with perception, computation, action, and memory tools, these agentic
AI systems could integrate diverse biomedical data, execute tasks, carry out
experiments via robotic platforms, and iteratively refine hypotheses in closed
loops. We provide a conceptual and technical overview of agentic AI
architectures, ranging from ReAct and Reflection to Supervisor and Swarm
systems, and illustrate their applications across key stages of drug discovery,
including literature synthesis, toxicity prediction, automated protocol
generation, small-molecule synthesis, drug repurposing, and end-to-end
decision-making. To our knowledge, this represents the first comprehensive work
to present real-world implementations and quantifiable impacts of agentic AI
systems deployed in operational drug discovery settings. Early implementations
demonstrate substantial gains in speed, reproducibility, and scalability,
compressing workflows that once took months into hours while maintaining
scientific traceability. We discuss the current challenges related to data
heterogeneity, system reliability, privacy, and benchmarking, and outline
future directions towards technology in support of science and translation.

</details>


### [49] [Exploring the Utilities of the Rationales from Large Language Models to Enhance Automated Essay Scoring](https://arxiv.org/abs/2510.27131)
*Hong Jiao,Hanna Choi,Haowei Hua*

Main category: cs.LG

TL;DR: 该研究探索了GPT-4.1和GPT-5生成的rationales在自动评分中的效用，发现基于essay的评分通常优于基于rationale的评分，但rationale评分在类别不平衡的分数0上表现更好。集成建模进一步提升了评分准确性。


<details>
  <summary>Details</summary>
Motivation: 探索GPT-4.1和GPT-5生成的rationales在自动评分中的实际效用，比较基于essay和基于rationale的评分方法的效果。

Method: 使用2012年Kaggle ASAP数据中的Prompt 6 essays，比较essay-based评分与rationale-based评分，并进行集成建模分析。

Result: essay-based评分通常表现更好（QWK更高），但rationale-based评分在分数0上F1分数更高。集成essay-based评分模型提升了准确性，同时集成essay和两种rationale评分达到了最佳效果（QWK=0.870）。

Conclusion: rationales在自动评分中具有一定效用，特别是在处理类别不平衡问题时。集成essay和rationale评分方法可以显著提升评分准确性，达到优于文献报道的结果。

Abstract: This study explored the utilities of rationales generated by GPT-4.1 and
GPT-5 in automated scoring using Prompt 6 essays from the 2012 Kaggle ASAP
data. Essay-based scoring was compared with rationale-based scoring. The study
found in general essay-based scoring performed better than rationale-based
scoring with higher Quadratic Weighted Kappa (QWK). However, rationale-based
scoring led to higher scoring accuracy in terms of F1 scores for score 0 which
had less representation due to class imbalance issues. The ensemble modeling of
essay-based scoring models increased the scoring accuracy at both specific
score levels and across all score levels. The ensemble modeling of essay-based
scoring and each of the rationale-based scoring performed about the same.
Further ensemble of essay-based scoring and both rationale-based scoring
yielded the best scoring accuracy with QWK of 0.870 compared with 0.848
reported in literature.

</details>


### [50] [FairAD: Computationally Efficient Fair Graph Clustering via Algebraic Distance](https://arxiv.org/abs/2510.27136)
*Minh Phu Vuong,Young-Ju Lee,Iván Ojeda-Ruiz,Chul-Ho Lee*

Main category: cs.LG

TL;DR: 提出FairAD方法，一种高效公平图聚类算法，通过代数距离构建亲和矩阵并施加公平约束，在保持聚类质量的同时比现有方法快40倍


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型对某些人口群体的不公平行为日益受到关注，需要在图聚类中引入公平性约束，但现有方法计算成本高昂，特别是对于大型图

Method: 1) 基于代数距离构建亲和矩阵并施加公平约束；2) 对亲和矩阵进行图粗化处理找到代表节点；3) 求解约束最小化问题获得公平聚类解

Result: 在修改的随机块模型和六个公共数据集上的实验表明，FairAD能够实现公平聚类，同时比最先进的公平图聚类算法快40倍

Conclusion: FairAD是一种计算高效的公平图聚类方法，成功解决了现有方法在大图上计算成本高的问题，在保持公平性的同时显著提升了效率

Abstract: Due to the growing concern about unsavory behaviors of machine learning
models toward certain demographic groups, the notion of 'fairness' has recently
drawn much attention from the community, thereby motivating the study of
fairness in graph clustering. Fair graph clustering aims to partition the set
of nodes in a graph into $k$ disjoint clusters such that the proportion of each
protected group within each cluster is consistent with the proportion of that
group in the entire dataset. It is, however, computationally challenging to
incorporate fairness constraints into existing graph clustering algorithms,
particularly for large graphs. To address this problem, we propose FairAD, a
computationally efficient fair graph clustering method. It first constructs a
new affinity matrix based on the notion of algebraic distance such that
fairness constraints are imposed. A graph coarsening process is then performed
on this affinity matrix to find representative nodes that correspond to $k$
clusters. Finally, a constrained minimization problem is solved to obtain the
solution of fair clustering. Experiment results on the modified stochastic
block model and six public datasets show that FairAD can achieve fair
clustering while being up to 40 times faster compared to state-of-the-art fair
graph clustering algorithms.

</details>


### [51] [Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores](https://arxiv.org/abs/2510.27145)
*Sein Kwon,Seulgi Baek,Hyunseo Yang,Youngwan Jo,Sanghyun Park*

Main category: cs.LG

TL;DR: 提出RelTune框架，通过关系图表示参数依赖关系，使用GNN学习性能相关的语义嵌入，并引入混合分数引导的贝叶斯优化来提升数据库参数调优效率。


<details>
  <summary>Details</summary>
Motivation: 现有数据库参数自动调优方法存在三个主要局限：忽略参数间依赖关系、仅优化少数参数、贝叶斯优化的代理模型不稳定且探索效率低。

Method: 1. 构建参数关系图表示依赖关系；2. 使用GNN学习性能相关的语义嵌入；3. 提出混合分数引导贝叶斯优化，结合代理模型预测和亲和度分数。

Result: 在多个DBMS和工作负载上的实验表明，RelTune比传统基于贝叶斯优化的方法收敛更快、优化效率更高，在所有评估场景中都达到了最先进的性能。

Conclusion: RelTune通过建模参数依赖关系和引入混合引导策略，有效解决了现有自动调优方法的局限性，显著提升了数据库参数优化的效率和效果。

Abstract: Database Management Systems (DBMSs) are fundamental for managing large-scale
and heterogeneous data, and their performance is critically influenced by
configuration parameters. Effective tuning of these parameters is essential for
adapting to diverse workloads and maximizing throughput while minimizing
latency. Recent research has focused on automated configuration optimization
using machine learning; however, existing approaches still exhibit several key
limitations. Most tuning frameworks disregard the dependencies among
parameters, assuming that each operates independently. This simplification
prevents optimizers from leveraging relational effects across parameters,
limiting their capacity to capture performancesensitive interactions. Moreover,
to reduce the complexity of the high-dimensional search space, prior work often
selects only the top few parameters for optimization, overlooking others that
contribute meaningfully to performance. Bayesian Optimization (BO), the most
common method for automatic tuning, is also constrained by its reliance on
surrogate models, which can lead to unstable predictions and inefficient
exploration. To overcome these limitations, we propose RelTune, a novel
framework that represents parameter dependencies as a Relational Graph and
learns GNN-based latent embeddings that encode performancerelevant semantics.
RelTune further introduces Hybrid-Score-Guided Bayesian Optimization (HBO),
which combines surrogate predictions with an Affinity Score measuring proximity
to previously high-performing configurations. Experimental results on multiple
DBMSs and workloads demonstrate that RelTune achieves faster convergence and
higher optimization efficiency than conventional BO-based methods, achieving
state-of-the-art performance across all evaluated scenarios.

</details>


### [52] [Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler](https://arxiv.org/abs/2510.27172)
*Zixuan Hu,Li Shen,Zhenyi Wang,Yongxian Wei,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出了贝叶斯数据调度器（BDS），一种无需攻击模拟的自适应调优阶段防御策略，通过贝叶斯推理学习数据点的安全属性后验分布，在微调过程中约束有害数据的影响。


<details>
  <summary>Details</summary>
Motivation: 现有防御策略通过攻击模拟预先构建鲁棒性，但存在根本限制：(i)无法在有限威胁模型之外扩展攻击模拟，(ii)难以适应变化的攻击设置。有害微调对大型语言模型的微调即服务构成关键安全风险。

Method: 将有害微调防御建模为贝叶斯推理问题，学习每个数据点安全属性的后验分布。通过从后验分布采样安全属性来加权数据，从而约束微调过程。引入基于摊销贝叶斯学习的神经调度器，实现高效迁移。

Result: 在各种攻击和防御设置下的综合结果表明，该方法达到了最先进的性能。

Conclusion: BDS通过贝叶斯推理实现了自适应防御，无需攻击模拟即可有效缓解有害数据的影响，在多样化的攻击场景中表现出色。

Abstract: Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service
for large language models. Existing defense strategies preemptively build
robustness via attack simulation but suffer from fundamental limitations: (i)
the infeasibility of extending attack simulations beyond bounded threat models
due to the inherent difficulty of anticipating unknown attacks, and (ii)
limited adaptability to varying attack settings, as simulation fails to capture
their variability and complexity. To address these challenges, we propose
Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with
no need for attack simulation. BDS formulates harmful fine-tuning defense as a
Bayesian inference problem, learning the posterior distribution of each data
point's safety attribute, conditioned on the fine-tuning and alignment
datasets. The fine-tuning process is then constrained by weighting data with
their safety attributes sampled from the posterior, thus mitigating the
influence of harmful data. By leveraging the post hoc nature of Bayesian
inference, the posterior is conditioned on the fine-tuning dataset, enabling
BDS to tailor its defense to the specific dataset, thereby achieving adaptive
defense. Furthermore, we introduce a neural scheduler based on amortized
Bayesian learning, enabling efficient transfer to new data without retraining.
Comprehensive results across diverse attack and defense settings demonstrate
the state-of-the-art performance of our approach. Code is available at
https://github.com/Egg-Hu/Bayesian-Data-Scheduler.

</details>


### [53] [A Polynomial-time Algorithm for Online Sparse Linear Regression with Improved Regret Bound under Weaker Conditions](https://arxiv.org/abs/2510.27177)
*Junfan Li,Shizhong Liao,Zenglin Xu,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出了一种新的多项式时间算法，用于在线稀疏线性回归问题，该算法在较弱的兼容性条件下显著改进了之前的遗憾界限。


<details>
  <summary>Details</summary>
Motivation: 解决在线稀疏线性回归问题中算法只能访问k个属性进行预测的NP难题，在弱于先前假设的条件下改进遗憾界限。

Method: 利用Dantzig Selector，结合算法相关的协方差矩阵采样方案、自适应参数调整方案以及带仔细初始化的批处理在线牛顿步法。

Result: 在兼容性条件下显著改进了之前的遗憾界限，并扩展了带有额外观测的OSLR问题，同样改进了相关遗憾界限。

Conclusion: 新算法在较弱假设下实现了更好的性能，通过创新的技术组合和非平凡的分析方法解决了这一挑战性问题。

Abstract: In this paper, we study the problem of online sparse linear regression (OSLR)
where the algorithms are restricted to accessing only $k$ out of $d$ attributes
per instance for prediction, which was proved to be NP-hard. Previous work gave
polynomial-time algorithms assuming the data matrix satisfies the linear
independence of features, the compatibility condition, or the restricted
isometry property. We introduce a new polynomial-time algorithm, which
significantly improves previous regret bounds (Ito et al., 2017) under the
compatibility condition that is weaker than the other two assumptions. The
improvements benefit from a tighter convergence rate of the $\ell_1$-norm error
of our estimators. Our algorithm leverages the well-studied Dantzig Selector,
but importantly with several novel techniques, including an algorithm-dependent
sampling scheme for estimating the covariance matrix, an adaptive parameter
tuning scheme, and a batching online Newton step with careful initializations.
We also give novel and non-trivial analyses, including an induction method for
analyzing the $\ell_1$-norm error, careful analyses on the covariance of
non-independent random variables, and a decomposition on the regret. We further
extend our algorithm to OSLR with additional observations where the algorithms
can observe additional $k_0$ attributes after each prediction, and improve
previous regret bounds (Kale et al., 2017; Ito et al., 2017).

</details>


### [54] [SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference](https://arxiv.org/abs/2510.27182)
*Zongshun Zhang,Ibrahim Matta*

Main category: cs.LG

TL;DR: SERFLOW通过动态卸载机器学习模型分区到FaaS和IaaS服务，结合阶段特定资源调配和自适应负载均衡，在动态工作负载下降低云成本超过23%


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视虚拟机冷启动、长尾服务时间分布等现实因素，且单一资源配置无法适应不同查询分布，导致资源利用率低下

Method: 将ML查询建模为遍历无环阶段序列，每个阶段包含稀疏模型参数块，使用FaaS无服务器函数和阶段特定资源调配，结合基于请求摄入的自适应负载均衡

Result: 降低云成本超过23%，有效适应动态工作负载

Conclusion: SERFLOW通过整合阶段特定资源调配和自适应负载均衡，在考虑现实因素的同时优化了ML推理应用的性能和成本

Abstract: Dynamic offloading of Machine Learning (ML) model partitions across different
resource orchestration services, such as Function-as-a-Service (FaaS) and
Infrastructure-as-a-Service (IaaS), can balance processing and transmission
delays while minimizing costs of adaptive inference applications. However,
prior work often overlooks real-world factors, such as Virtual Machine (VM)
cold starts, requests under long-tail service time distributions, etc. To
tackle these limitations, we model each ML query (request) as traversing an
acyclic sequence of stages, wherein each stage constitutes a contiguous block
of sparse model parameters ending in an internal or final classifier where
requests may exit. Since input-dependent exit rates vary, no single resource
configuration suits all query distributions. IaaS-based VMs become
underutilized when many requests exit early, yet rapidly scaling to handle
request bursts reaching deep layers is impractical. SERFLOW addresses this
challenge by leveraging FaaS-based serverless functions (containers) and using
stage-specific resource provisioning that accounts for the fraction of requests
exiting at each stage. By integrating this provisioning with adaptive load
balancing across VMs and serverless functions based on request ingestion,
SERFLOW reduces cloud costs by over $23\%$ while efficiently adapting to
dynamic workloads.

</details>


### [55] [MDAS-GNN: Multi-Dimensional Spatiotemporal GNN with Spatial Diffusion for Urban Traffic Risk Forecasting](https://arxiv.org/abs/2510.27197)
*Ziyuan Gao*

Main category: cs.LG

TL;DR: 提出MDAS-GNN模型，一种多维度注意力空间扩散图神经网络，用于交通事故预测，在三个英国城市数据集上表现优于基准方法，尤其在长期预测中表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统事故预测模型将路段视为独立个体，无法捕捉城市交通网络中的复杂空间关系和时间依赖性，而交通事故每年造成全球135万人死亡，是重要的公共卫生挑战。

Method: 开发MDAS-GNN模型，整合交通安全、基础设施和环境风险三个核心维度，采用特征特定的空间扩散机制和多头时间注意力来捕捉不同时间跨度的依赖性。

Result: 在英国交通部三个城市数据集上，MDAS-GNN相比基准方法表现更优，在短、中、长期预测中均保持低误差，长期预测能力尤其突出。消融研究显示多维度特征比单特征方法减少预测误差达40%。

Conclusion: 该框架为土木工程师和城市规划者提供了先进的预测能力，支持基于数据的道路网络优化、基础设施改进和城市发展项目中的安全干预策略决策。

Abstract: Traffic accidents represent a critical public health challenge, claiming over
1.35 million lives annually worldwide. Traditional accident prediction models
treat road segments independently, failing to capture complex spatial
relationships and temporal dependencies in urban transportation networks. This
study develops MDAS-GNN, a Multi-Dimensional Attention-based Spatial-diffusion
Graph Neural Network integrating three core risk dimensions: traffic safety,
infrastructure, and environmental risk. The framework employs feature-specific
spatial diffusion mechanisms and multi-head temporal attention to capture
dependencies across different time horizons. Evaluated on UK Department for
Transport accident data across Central London, South Manchester, and SE
Birmingham, MDASGNN achieves superior performance compared to established
baseline methods. The model maintains consistently low prediction errors across
short, medium, and long-term periods, with particular strength in long-term
forecasting. Ablation studies confirm that integrated multi-dimensional
features outperform singlefeature approaches, reducing prediction errors by up
to 40%. This framework provides civil engineers and urban planners with
advanced predictive capabilities for transportation infrastructure design,
enabling data-driven decisions for road network optimization, infrastructure
resource improvements, and strategic safety interventions in urban development
projects.

</details>


### [56] [Feature-Function Curvature Analysis: A Geometric Framework for Explaining Differentiable Models](https://arxiv.org/abs/2510.27207)
*Hamed Najafi,Dongsheng Luo,Jason Liu*

Main category: cs.LG

TL;DR: 提出了特征函数曲率分析(FFCA)框架，通过4维特征签名量化特征的影响、波动性、非线性和交互作用，并扩展为动态原型分析来追踪训练过程中的特征演化。


<details>
  <summary>Details</summary>
Motivation: 主流可解释AI方法仅提供模型最终状态的静态图景，将特征作用简化为单一分数，无法处理非线性和交互作用的复杂性。

Method: FFCA分析模型学习函数的几何特性，为每个特征生成包含影响、波动性、非线性和交互作用的4维签名，并引入动态原型分析追踪训练过程中的特征演化。

Result: 首次提供了层次学习的直接经验证据，显示模型始终先学习简单线性效应再学习复杂交互作用；动态分析可识别模型容量不足并预测过拟合开始。

Conclusion: FFCA通过其静态和动态组件提供了必要的几何背景，将模型解释从简单量化转变为对整个学习过程的细致可信分析。

Abstract: Explainable AI (XAI) is critical for building trust in complex machine
learning models, yet mainstream attribution methods often provide an
incomplete, static picture of a model's final state. By collapsing a feature's
role into a single score, they are confounded by non-linearity and
interactions. To address this, we introduce Feature-Function Curvature Analysis
(FFCA), a novel framework that analyzes the geometry of a model's learned
function. FFCA produces a 4-dimensional signature for each feature, quantifying
its: (1) Impact, (2) Volatility, (3) Non-linearity, and (4) Interaction.
Crucially, we extend this framework into Dynamic Archetype Analysis, which
tracks the evolution of these signatures throughout the training process. This
temporal view moves beyond explaining what a model learned to revealing how it
learns. We provide the first direct, empirical evidence of hierarchical
learning, showing that models consistently learn simple linear effects before
complex interactions. Furthermore, this dynamic analysis provides novel,
practical diagnostics for identifying insufficient model capacity and
predicting the onset of overfitting. Our comprehensive experiments demonstrate
that FFCA, through its static and dynamic components, provides the essential
geometric context that transforms model explanation from simple quantification
to a nuanced, trustworthy analysis of the entire learning process.

</details>


### [57] [FedSM: Robust Semantics-Guided Feature Mixup for Bias Reduction in Federated Learning with Long-Tail Data](https://arxiv.org/abs/2510.27240)
*Jingrui Zhang,Yimeng Xu,Shujie Li,Feng Liang,Haihan Duan,Yanjie Dong,Victor C. M. Leung,Xiping Hu*

Main category: cs.LG

TL;DR: FedSM是一个面向客户端的联邦学习框架，通过语义引导的特征混合和轻量级分类器重训练来解决非IID和长尾数据分布导致的全局模型偏差问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非IID和长尾数据分布下会产生有偏的全局模型，需要解决这种偏差问题。

Method: 使用预训练的图文对齐模型计算类别级语义相关性，指导本地特征与全局原型进行混合，生成类别一致的伪特征，并通过概率类别选择增强特征多样性。

Result: 在多个长尾数据集上的实验表明，FedSM在准确率上持续优于最先进方法，具有高鲁棒性和计算效率。

Conclusion: FedSM通过语义引导的特征混合有效缓解了联邦学习中的模型偏差问题，且计算开销小，适用于实际部署。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients without sharing private data. However, FL suffers from
biased global models due to non-IID and long-tail data distributions. We
propose \textbf{FedSM}, a novel client-centric framework that mitigates this
bias through semantics-guided feature mixup and lightweight classifier
retraining. FedSM uses a pretrained image-text-aligned model to compute
category-level semantic relevance, guiding the category selection of local
features to mix-up with global prototypes to generate class-consistent
pseudo-features. These features correct classifier bias, especially when data
are heavily skewed. To address the concern of potential domain shift between
the pretrained model and the data, we propose probabilistic category selection,
enhancing feature diversity to effectively mitigate biases. All computations
are performed locally, requiring minimal server overhead. Extensive experiments
on long-tail datasets with various imbalanced levels demonstrate that FedSM
consistently outperforms state-of-the-art methods in accuracy, with high
robustness to domain shift and computational efficiency.

</details>


### [58] [Not All Instances Are Equally Valuable: Towards Influence-Weighted Dataset Distillation](https://arxiv.org/abs/2510.27253)
*Qiyan Deng,Changqian Zheng,Lianpeng Qiao,Yuping Wang,Chengliang Chai,Lei Cao*

Main category: cs.LG

TL;DR: 提出IWD框架，利用影响函数在数据集蒸馏过程中显式考虑数据质量，为每个实例分配自适应权重，优先处理有益数据，抑制无用或有害数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法假设所有真实实例对蒸馏过程贡献相同，但实际数据集包含信息丰富和冗余甚至有害的实例，不考虑数据质量会降低模型性能。

Method: IWD框架利用影响函数估计每个实例对蒸馏目标的影响，为其分配自适应权重，可无缝集成到各种数据集蒸馏框架中。

Result: 集成IWD提高了蒸馏数据集质量并增强模型性能，准确率提升高达7.8%。

Conclusion: IWD通过考虑数据质量改进了数据集蒸馏，其模块化设计使其能够灵活应用于不同蒸馏框架。

Abstract: Dataset distillation condenses large datasets into synthetic subsets,
achieving performance comparable to training on the full dataset while
substantially reducing storage and computation costs. Most existing dataset
distillation methods assume that all real instances contribute equally to the
process. In practice, real-world datasets contain both informative and
redundant or even harmful instances, and directly distilling the full dataset
without considering data quality can degrade model performance. In this work,
we present Influence-Weighted Distillation IWD, a principled framework that
leverages influence functions to explicitly account for data quality in the
distillation process. IWD assigns adaptive weights to each instance based on
its estimated impact on the distillation objective, prioritizing beneficial
data while downweighting less useful or harmful ones. Owing to its modular
design, IWD can be seamlessly integrated into diverse dataset distillation
frameworks. Our empirical results suggest that integrating IWD tends to improve
the quality of distilled datasets and enhance model performance, with accuracy
gains of up to 7.8%.

</details>


### [59] [ECVL-ROUTER: Scenario-Aware Routing for Vision-Language Models](https://arxiv.org/abs/2510.27256)
*Xin Tang,Youfang Han,Fangfei Gou,Wei Zhao,Xin Meng,Yang Yu,Jinguo Zhang,Yuanchun Shi,Yuntao Wang,Tengxiang Zhang*

Main category: cs.LG

TL;DR: ECVL-ROUTER是首个面向视觉语言模型(VLMs)的场景感知路由框架，通过动态选择合适模型来平衡响应速度、输出质量和能耗，在保持问题解决概率下降小于10%的同时，将80%以上查询路由到小模型。


<details>
  <summary>Details</summary>
Motivation: 用户需求在不同场景下差异很大，包括快速响应、高质量输出和低能耗。单纯依赖云端大模型会导致高延迟和高能耗，而边缘设备上的小模型能处理简单任务但能力有限。需要结合大小模型的优势。

Method: 提出ECVL-ROUTER路由框架，引入新的路由策略和评估指标，基于用户需求动态选择合适模型。构建了专门用于路由器训练的多模态响应质量数据集。

Result: 实验结果显示，该方法成功将超过80%的查询路由到小模型，同时问题解决概率下降小于10%。

Conclusion: ECVL-ROUTER框架有效平衡了不同用户需求，充分利用大小模型的优势，在保证性能的同时显著降低了延迟和能耗。

Abstract: Vision-Language Models (VLMs) excel in diverse multimodal tasks. However,
user requirements vary across scenarios, which can be categorized into fast
response, high-quality output, and low energy consumption. Relying solely on
large models deployed in the cloud for all queries often leads to high latency
and energy cost, while small models deployed on edge devices are capable of
handling simpler tasks with low latency and energy cost. To fully leverage the
strengths of both large and small models, we propose ECVL-ROUTER, the first
scenario-aware routing framework for VLMs. Our approach introduces a new
routing strategy and evaluation metrics that dynamically select the appropriate
model for each query based on user requirements, maximizing overall utility. We
also construct a multimodal response-quality dataset tailored for router
training and validate the approach through extensive experiments. Results show
that our approach successfully routes over 80\% of queries to the small model
while incurring less than 10\% drop in problem solving probability.

</details>


### [60] [Higher-order Linear Attention](https://arxiv.org/abs/2510.27258)
*Yifan Zhang,Zhen Qin,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出高阶线性注意力（HLA），一种因果流式机制，通过紧凑前缀统计实现高阶交互，在保持恒定大小状态的同时实现线性时间计算，解决了传统注意力二次成本的问题。


<details>
  <summary>Details</summary>
Motivation: 传统缩放点积注意力的二次成本限制了自回归语言模型扩展到长上下文，而现有的线性时间注意力和状态空间模型通常局限于一阶或基于核的近似，限制了表达能力。

Method: 引入HLA机制，使用紧凑前缀充分统计实现高阶交互；提供闭式流式恒等式、严格因果掩码变体和基于关联扫描的块并行训练方案；支持扩展到三阶及更高阶。

Result: 在二阶情况下，HLA保持恒定大小状态，无需物化任何n×n矩阵即可实现每令牌输出的线性时间计算，精确复现串行循环的激活。

Conclusion: HLA作为一个原则性、可扩展的构建块，将注意力式的数据依赖混合与现代循环架构的效率相结合，为长上下文建模提供了有前景的解决方案。

Abstract: The quadratic cost of scaled dot-product attention is a central obstacle to
scaling autoregressive language models to long contexts. Linear-time attention
and State Space Models (SSMs) provide scalable alternatives but are typically
restricted to first-order or kernel-based approximations, which can limit
expressivity. We introduce Higher-order Linear Attention (HLA), a causal,
streaming mechanism that realizes higher interactions via compact prefix
sufficient statistics. In the second-order case, HLA maintains a constant-size
state and computes per-token outputs in linear time without materializing any
$n \times n$ matrices. We give closed-form streaming identities, a strictly
causal masked variant using two additional summaries, and a chunk-parallel
training scheme based on associative scans that reproduces the activations of a
serial recurrence exactly. We further outline extensions to third and higher
orders. Collectively, these results position HLA as a principled, scalable
building block that combines attention-like, data-dependent mixing with the
efficiency of modern recurrent architectures. Project Page:
https://github.com/yifanzhang-pro/HLA.

</details>


### [61] [ODP-Bench: Benchmarking Out-of-Distribution Performance Prediction](https://arxiv.org/abs/2510.27263)
*Han Yu,Kehan Li,Dongbai Li,Yue He,Xingxuan Zhang,Peng Cui*

Main category: cs.LG

TL;DR: 提出了ODP-Bench基准，用于统一评估OOD性能预测算法，包含常用OOD数据集和现有算法，提供预训练模型确保比较一致性。


<details>
  <summary>Details</summary>
Motivation: 现有OOD性能预测研究评估协议不一致，覆盖的数据集和分布偏移类型有限，需要公平比较平台。

Method: 构建综合基准ODP-Bench，整合常用OOD数据集和现有性能预测算法，提供预训练模型测试平台。

Result: 建立了统一的评估框架，为未来研究提供便利和公平的比较基础。

Conclusion: ODP-Bench解决了OOD性能预测领域评估不一致的问题，为算法比较提供了标准化平台，并进行了深入实验分析。

Abstract: Recently, there has been gradually more attention paid to Out-of-Distribution
(OOD) performance prediction, whose goal is to predict the performance of
trained models on unlabeled OOD test datasets, so that we could better leverage
and deploy off-the-shelf trained models in risk-sensitive scenarios. Although
progress has been made in this area, evaluation protocols in previous
literature are inconsistent, and most works cover only a limited number of
real-world OOD datasets and types of distribution shifts. To provide convenient
and fair comparisons for various algorithms, we propose Out-of-Distribution
Performance Prediction Benchmark (ODP-Bench), a comprehensive benchmark that
includes most commonly used OOD datasets and existing practical performance
prediction algorithms. We provide our trained models as a testbench for future
researchers, thus guaranteeing the consistency of comparison and avoiding the
burden of repeating the model training process. Furthermore, we also conduct
in-depth experimental analyses to better understand their capability boundary.

</details>


### [62] [HiF-DTA: Hierarchical Feature Learning Network for Drug-Target Affinity Prediction](https://arxiv.org/abs/2510.27281)
*Minghui Li,Yuanhang Wang,Peijin Guo,Wei Wan,Shengshan Hu,Shengqing Hu*

Main category: cs.LG

TL;DR: HiF-DTA是一个用于药物-靶点亲和力预测的层次化网络，采用双路径策略提取药物和蛋白质的全局序列语义和局部拓扑特征，并通过多尺度融合模块整合原子、子结构和分子级表示。


<details>
  <summary>Details</summary>
Motivation: 现有序列深度学习方法忽视了同时建模全局序列语义特征和局部拓扑结构特征，且将药物表示为扁平序列而缺乏多尺度特征表示。

Method: 采用双路径策略分别提取全局序列语义和局部拓扑特征，对药物进行多尺度建模（原子、子结构、分子级），通过多尺度双线性注意力模块融合特征。

Result: 在Davis、KIBA和Metz数据集上的实验表明，HiF-DTA优于现有最先进基线方法，消融实验证实了全局-局部特征提取和多尺度融合的重要性。

Conclusion: HiF-DTA通过层次化网络和双路径策略有效提升了药物-靶点亲和力预测性能，证明了同时建模全局序列语义和局部拓扑特征以及多尺度表示的重要性。

Abstract: Accurate prediction of Drug-Target Affinity (DTA) is crucial for reducing
experimental costs and accelerating early screening in computational drug
discovery. While sequence-based deep learning methods avoid reliance on costly
3D structures, they still overlook simultaneous modeling of global sequence
semantic features and local topological structural features within drugs and
proteins, and represent drugs as flat sequences without atomic-level,
substructural-level, and molecular-level multi-scale features. We propose
HiF-DTA, a hierarchical network that adopts a dual-pathway strategy to extract
both global sequence semantic and local topological features from drug and
protein sequences, and models drugs multi-scale to learn atomic, substructural,
and molecular representations fused via a multi-scale bilinear attention
module. Experiments on Davis, KIBA, and Metz datasets show HiF-DTA outperforms
state-of-the-art baselines, with ablations confirming the importance of
global-local extraction and multi-scale fusion.

</details>


### [63] [Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments](https://arxiv.org/abs/2510.27287)
*Harsh Vishwakarma,Ankush Agarwal,Ojas Patil,Chaitanya Devaguptapu,Mahesh Chandran*

Main category: cs.LG

TL;DR: EnterpriseBench是一个模拟企业环境的综合基准测试，包含500个跨领域任务，用于评估LLM在企业系统中的表现，结果显示最先进的模型仅完成41.8%的任务。


<details>
  <summary>Details</summary>
Motivation: 企业系统对提升员工和客户的生产力与决策能力至关重要，但将LLM集成到企业系统中面临数据分散和访问控制复杂等挑战，需要专门的评估基准。

Method: 开发了EnterpriseBench基准测试，模拟企业环境特征，包括数据源碎片化、访问控制层次和跨职能工作流，并提供从组织元数据生成内部一致任务的数据生成流水线。

Result: 实验表明，即使最先进的LLM代理也只能完成41.8%的任务，揭示了企业AI系统仍有很大改进空间。

Conclusion: EnterpriseBench为评估企业环境中LLM系统提供了重要基准，显示了当前模型在企业任务处理能力上的局限性，为未来改进指明了方向。

Abstract: Enterprise systems are crucial for enhancing productivity and decision-making
among employees and customers. Integrating LLM based systems into enterprise
systems enables intelligent automation, personalized experiences, and efficient
information retrieval, driving operational efficiency and strategic growth.
However, developing and evaluating such systems is challenging due to the
inherent complexity of enterprise environments, where data is fragmented across
multiple sources and governed by sophisticated access controls. We present
EnterpriseBench, a comprehensive benchmark that simulates enterprise settings,
featuring 500 diverse tasks across software engineering, HR, finance, and
administrative domains. Our benchmark uniquely captures key enterprise
characteristics including data source fragmentation, access control
hierarchies, and cross-functional workflows. Additionally, we provide a novel
data generation pipeline that creates internally consistent enterprise tasks
from organizational metadata. Experiments with state-of-the-art LLM agents
demonstrate that even the most capable models achieve only 41.8% task
completion, highlighting significant opportunities for improvement in
enterprise-focused AI systems.

</details>


### [64] [Temporal Cardiovascular Dynamics for Improved PPG-Based Heart Rate Estimation](https://arxiv.org/abs/2510.27297)
*Berken Utku Demirel,Christian Holz*

Main category: cs.LG

TL;DR: 提出一种基于互信息的非线性混沌分析方法，用于增强现实场景下的心率估计，相比传统方法提升达40%，减少对多模态传感的依赖并无需后处理。


<details>
  <summary>Details</summary>
Motivation: 心率振荡具有复杂的非线性混沌特性，这给日常心血管健康监测带来了挑战，需要从数学角度解释和处理这种非线性时间复杂性。

Method: 通过互信息研究心率的非线性混沌行为，提出一种新方法，并与深度学习解决方案结合使用。

Result: 在四个真实场景数据集上的验证表明，该方法相比传统方法和现有机器学习技术，心率估计性能提升高达40%。

Conclusion: 该方法不仅能从数学角度解释和处理非线性时间复杂性，还能与深度学习结合提升性能，同时减少对多模态传感的依赖并消除后处理需求。

Abstract: The oscillations of the human heart rate are inherently complex and
non-linear -- they are best described by mathematical chaos, and they present a
challenge when applied to the practical domain of cardiovascular health
monitoring in everyday life. In this work, we study the non-linear chaotic
behavior of heart rate through mutual information and introduce a novel
approach for enhancing heart rate estimation in real-life conditions. Our
proposed approach not only explains and handles the non-linear temporal
complexity from a mathematical perspective but also improves the deep learning
solutions when combined with them. We validate our proposed method on four
established datasets from real-life scenarios and compare its performance with
existing algorithms thoroughly with extensive ablation experiments. Our results
demonstrate a substantial improvement, up to 40\%, of the proposed approach in
estimating heart rate compared to traditional methods and existing
machine-learning techniques while reducing the reliance on multiple sensing
modalities and eliminating the need for post-processing steps.

</details>


### [65] [Binary Anomaly Detection in Streaming IoT Traffic under Concept Drift](https://arxiv.org/abs/2510.27304)
*Rodrigo Matos Carnier,Laura Lahesoo,Kensuke Fukuda*

Main category: cs.LG

TL;DR: 该研究比较了批处理和流式学习在IoT网络异常检测中的表现，发现流式学习方法能更好地处理概念漂移，且树基算法在计算效率和性能上具有优势。


<details>
  <summary>Details</summary>
Motivation: 随着IoT网络流量的增长，传统的批学习模型面临维护成本高和适应概念漂移能力差的问题，而流式学习能够实现无缝更新和概念漂移检测以提高鲁棒性。

Method: 研究将IoT流量异常检测视为二元分类问题，通过混合现有数据集模拟异构网络数据流，并逐个样本进行流式处理，比较了批处理和流式学习方法以及树基和非树基算法的性能。

Result: 自适应随机森林的F1分数达到0.990±0.006，计算成本仅为批处理版本的三分之一；Hoeffding自适应树的F1分数为0.910±0.007，计算成本降低四倍，适合在线应用。

Conclusion: 流式学习方法在IoT异常检测中优于批处理方法，能有效处理概念漂移，且树基算法在计算效率和性能上表现突出，但当前数据集在暴露模型局限性方面仍存在不足。

Abstract: With the growing volume of Internet of Things (IoT) network traffic, machine
learning (ML)-based anomaly detection is more relevant than ever. Traditional
batch learning models face challenges such as high maintenance and poor
adaptability to rapid anomaly changes, known as concept drift. In contrast,
streaming learning integrates online and incremental learning, enabling
seamless updates and concept drift detection to improve robustness. This study
investigates anomaly detection in streaming IoT traffic as binary
classification, comparing batch and streaming learning approaches while
assessing the limitations of current IoT traffic datasets. We simulated
heterogeneous network data streams by carefully mixing existing datasets and
streaming the samples one by one. Our results highlight the failure of batch
models to handle concept drift, but also reveal persisting limitations of
current datasets to expose model limitations due to low traffic heterogeneity.
We also investigated the competitiveness of tree-based ML algorithms,
well-known in batch anomaly detection, and compared it to non-tree-based ones,
confirming the advantages of the former. Adaptive Random Forest achieved
F1-score of 0.990 $\pm$ 0.006 at one-third the computational cost of its batch
counterpart. Hoeffding Adaptive Tree reached F1-score of 0.910 $\pm$ 0.007,
reducing computational cost by four times, making it a viable choice for online
applications despite a slight trade-off in stability.

</details>


### [66] [Un-Attributability: Computing Novelty From Retrieval & Semantic Similarity](https://arxiv.org/abs/2510.27313)
*Philipp Davydov,Ameya Prabhu,Matthias Bethge,Elisa Nguyen,Seong Joon Oh*

Main category: cs.LG

TL;DR: 该论文提出了不可归因性作为语义新颖性的操作化度量，通过两阶段检索管道来识别模型输出中无法归因于预训练数据的语义新颖内容。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型输出与预训练语料库的关系，传统方法关注哪些训练样本影响了特定输出，而本文反转问题：哪些输出无法归因于任何预训练样本。

Method: 使用两阶段检索管道：先用轻量级GIST嵌入索引语料库并检索top-n候选，然后用ColBERTv2重新排序。如果最近语料项的可归因性低于人工生成的文本参考，则认为模型输出是新颖的。

Result: 在SmolLM和SmolLM2上的评估发现：(1)模型利用预训练数据的跨度比之前报道的更长；(2)某些领域系统性地促进或抑制新颖性；(3)指令调优不仅改变风格还增加新颖性。

Conclusion: 围绕不可归因性重新定义新颖性评估，能够在预训练规模上实现高效分析，并发布了约20TB语料块和索引工件支持复现和大规模扩展。

Abstract: Understanding how language-model outputs relate to the pretraining corpus is
central to studying model behavior. Most training data attribution (TDA)
methods ask which training examples causally influence a given output, often
using leave-one-out tests. We invert the question: which outputs cannot be
attributed to any pretraining example? We introduce un-attributability as an
operational measure of semantic novelty: an output is novel if the pretraining
corpus contains no semantically similar context. We approximate this with a
simple two-stage retrieval pipeline: index the corpus with lightweight GIST
embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the
nearest corpus item is less attributable than a human-generated text reference,
we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2
and report three findings: (1) models draw on pretraining data across much
longer spans than previously reported; (2) some domains systematically promote
or suppress novelty; and (3) instruction tuning not only alters style but also
increases novelty. Reframing novelty assessment around un-attributability
enables efficient analysis at pretraining scale. We release ~20 TB of corpus
chunks and index artifacts to support replication and large-scale extension of
our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm

</details>


### [67] [MedM2T: A MultiModal Framework for Time-Aware Modeling with Electronic Health Record and Electrocardiogram Data](https://arxiv.org/abs/2510.27321)
*Yu-Chen Kuo,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: MedM2T是一个时间感知的多模态框架，用于处理医疗数据的多模态性和异质时间结构，在心血管疾病预测、院内死亡率预测和ICU住院时间回归任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医疗数据固有的多模态性和异质时间结构给建模带来挑战，需要能够灵活处理不规则稀疏时间序列、捕捉多尺度时间模式并提取跨模态交互的框架。

Method: MedM2T包含稀疏时间序列编码器处理不规则稀疏数据，分层时间感知融合捕捉微观和宏观时间模式，双模态注意力提取跨模态交互，使用模态特定预训练编码器并特征对齐。

Result: 在MIMIC-IV数据集上，CVD预测AUROC 0.947、AUPRC 0.706；死亡率预测AUROC 0.901、AUPRC 0.558；ICU住院时间回归MAE 2.31，均优于现有方法。

Conclusion: MedM2T展现了在临床预测中的鲁棒性和广泛适用性，是一个有前景的工具，代码已开源。

Abstract: The inherent multimodality and heterogeneous temporal structures of medical
data pose significant challenges for modeling. We propose MedM2T, a time-aware
multimodal framework designed to address these complexities. MedM2T integrates:
(i) Sparse Time Series Encoder to flexibly handle irregular and sparse time
series, (ii) Hierarchical Time-Aware Fusion to capture both micro- and
macro-temporal patterns from multiple dense time series, such as ECGs, and
(iii) Bi-Modal Attention to extract cross-modal interactions, which can be
extended to any number of modalities. To mitigate granularity gaps between
modalities, MedM2T uses modality-specific pre-trained encoders and aligns
resulting features within a shared encoder. We evaluated MedM2T on MIMIC-IV and
MIMIC-IV-ECG datasets for three tasks that encompass chronic and acute disease
dynamics: 90-day cardiovascular disease (CVD) prediction, in-hospital mortality
prediction, and ICU length-of-stay (LOS) regression. MedM2T outperformed
state-of-the-art multimodal learning frameworks and existing time series
models, achieving an AUROC of 0.947 and an AUPRC of 0.706 for CVD prediction;
an AUROC of 0.901 and an AUPRC of 0.558 for mortality prediction; and Mean
Absolute Error (MAE) of 2.31 for LOS regression. These results highlight the
robustness and broad applicability of MedM2T, positioning it as a promising
tool in clinical prediction. We provide the implementation of MedM2T at
https://github.com/DHLab-TSENG/MedM2T.

</details>


### [68] [Reasoning Models Sometimes Output Illegible Chains of Thought](https://arxiv.org/abs/2510.27338)
*Arun Jose*

Main category: cs.LG

TL;DR: 研究发现基于结果强化学习的语言模型会产生难以理解的思维链，尽管最终答案可读性正常，这可能影响模型意图监控的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究思维链的可读性对于理解模型意图和检测恶意行为至关重要，但需要确保思维链清晰且忠实。

Method: 分析了14个推理模型的思维链可读性，比较了强化学习前后模型的表现，并测试了强制使用可读部分时的准确率变化。

Result: 强化学习导致思维链变得难以理解，准确率在使用可读部分时下降53%，但可读性与性能之间没有直接相关性，且问题越难可读性越差。

Conclusion: 如果不显式优化可读性，基于结果的强化学习自然会产生推理过程不透明的模型，可能削弱监控方法的有效性。

Abstract: Language models trained via outcome-based reinforcement learning (RL) to
reason using chain-of-thought (CoT) have shown remarkable performance.
Monitoring such a model's CoT may allow us to understand its intentions and
detect potential malicious behavior. However, to be effective, this requires
that CoTs are legible and faithful. We study CoT legibility across 14 reasoning
models, finding that RL often causes reasoning to become illegible to both
humans and AI monitors, with reasoning models (except Claude) generating
illegible CoTs while returning to perfectly readable final answers. We show
that models use illegible reasoning to reach correct answers (accuracy dropping
by 53\% when forced to use only legible portions), yet find no correlation
between legibility and performance when resampling - suggesting the
relationship is more nuanced. We also find that legibility degrades on harder
questions. We discuss potential hypotheses for these results, including
steganography, training artifacts, and vestigial tokens. These results suggest
that without explicit optimization for legibility, outcome-based RL naturally
produces models with increasingly opaque reasoning processes, potentially
undermining monitoring approaches.

</details>


### [69] [Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity](https://arxiv.org/abs/2510.27378)
*Austin Meek,Eitan Sprejer,Iván Arcuschin,Austin J. Brockmeier,Steven Basart*

Main category: cs.LG

TL;DR: 本文提出了一种评估思维链(CoT)监控能力的新方法，结合忠实性和详尽性来衡量CoT作为模型外部工作记忆的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注模型在提示后改变答案的情况来评估忠实性，但忽略了模型保持答案时的情况以及未与提示相关的推理方面。需要更全面地评估CoT的监控能力。

Method: 引入详尽性概念来衡量CoT是否列出解决任务所需的所有因素，将忠实性和详尽性结合成单一监控能力分数。在BBH、GPQA和MMLU数据集上评估指令调优和推理模型。

Result: 研究发现模型可能看似忠实但难以监控，因为它们遗漏关键因素，且不同模型家族的监控能力差异显著。

Conclusion: CoT的监控能力取决于忠实性和详尽性的结合，这对基于CoT监控的安全方案至关重要。发布了使用Inspect库的评估代码以支持可复现的未来工作。

Abstract: Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning.
Since any long, serial reasoning process must pass through this textual trace,
the quality of the CoT is a direct window into what the model is thinking. This
visibility could help us spot unsafe or misaligned behavior (monitorability),
but only if the CoT is transparent about its internal reasoning (faithfulness).
Fully measuring faithfulness is difficult, so researchers often focus on
examining the CoT in cases where the model changes its answer after adding a
cue to the input. This proxy finds some instances of unfaithfulness but loses
information when the model maintains its answer, and does not investigate
aspects of reasoning not tied to the cue. We extend these results to a more
holistic sense of monitorability by introducing verbosity: whether the CoT
lists every factor needed to solve the task. We combine faithfulness and
verbosity into a single monitorability score that shows how well the CoT serves
as the model's external `working memory', a property that many safety schemes
based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning
models on BBH, GPQA, and MMLU. Our results show that models can appear faithful
yet remain hard to monitor when they leave out key factors, and that
monitorability differs sharply across model families. We release our evaluation
code using the Inspect library to support reproducible future work.

</details>


### [70] [FedMuon: Accelerating Federated Learning with Matrix Orthogonalization](https://arxiv.org/abs/2510.27403)
*Junkang Liu,Fanhua Shang,Junchao Zhou,Hongying Liu,Yuanyuan Liu,Jin Liu*

Main category: cs.LG

TL;DR: 提出了FedMuon优化器来解决联邦学习中通信轮次瓶颈问题，通过矩阵正交化优化矩阵结构参数，在非IID设置下使用动量聚合和局部-全局对齐技术减少客户端漂移。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的核心瓶颈在于通信轮次，现有方法使用元素级优化器（如Adam/SGD）忽略了权重矩阵的几何结构，导致局部更新中病态方向的放大、条件数恶化以及收敛缓慢。

Method: 引入Muon优化器进行局部矩阵正交化，在非IID设置下提出FedMuon，包含动量聚合（客户端使用聚合动量进行局部初始化）和局部-全局对齐（局部梯度与全局更新方向对齐）两个关键技术。

Result: 理论上证明FedMuon在线性加速收敛率方面优于传统方法，无需异质性假设。实验验证在语言和视觉模型上，FedMuon显著减少通信轮次并提高测试精度。

Conclusion: FedMuon通过矩阵正交化和客户端漂移缓解技术，有效解决了联邦学习中的通信瓶颈问题，在IID和非IID设置下均表现出优越性能。

Abstract: The core bottleneck of Federated Learning (FL) lies in the communication
rounds. That is, how to achieve more effective local updates is crucial for
reducing communication rounds. Existing FL methods still primarily use
element-wise local optimizers (Adam/SGD), neglecting the geometric structure of
the weight matrices. This often leads to the amplification of pathological
directions in the weights during local updates, leading deterioration in the
condition number and slow convergence. Therefore, we introduce the Muon
optimizer in local, which has matrix orthogonalization to optimize
matrix-structured parameters. Experimental results show that, in IID setting,
Local Muon significantly accelerates the convergence of FL and reduces
communication rounds compared to Local SGD and Local AdamW. However, in non-IID
setting, independent matrix orthogonalization based on the local distributions
of each client induces strong client drift. Applying Muon in non-IID FL poses
significant challenges: (1) client preconditioner leading to client drift; (2)
moment reinitialization. To address these challenges, we propose a novel
Federated Muon optimizer (FedMuon), which incorporates two key techniques: (1)
momentum aggregation, where clients use the aggregated momentum for local
initialization; (2) local-global alignment, where the local gradients are
aligned with the global update direction to significantly reduce client drift.
Theoretically, we prove that \texttt{FedMuon} achieves a linear speedup
convergence rate without the heterogeneity assumption, where $S$ is the number
of participating clients per round, $K$ is the number of local iterations, and
$R$ is the total number of communication rounds. Empirically, we validate the
effectiveness of FedMuon on language and vision models. Compared to several
baselines, FedMuon significantly reduces communication rounds and improves test
accuracy.

</details>


### [71] [Atlas-Alignment: Making Interpretability Transferable Across Language Models](https://arxiv.org/abs/2510.27413)
*Bruno Puri,Jim Berend,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: Atlas-Alignment是一个通过将未知潜在空间与标记化的概念图谱对齐，实现跨模型可解释性转移的框架，无需训练稀疏自编码器即可实现语义特征搜索和可控生成。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法成本高、难以扩展，需要为每个新模型训练稀疏自编码器、手动标注组件并进行验证。

Method: 使用共享输入和轻量级表示对齐技术，将未知潜在空间与预定义的概念图谱对齐，实现语义特征搜索和可控生成。

Result: 定量和定性评估表明，简单的表示对齐方法能够实现稳健的语义检索和可控生成，无需标记概念数据。

Conclusion: Atlas-Alignment通过投资一个高质量概念图谱，能够以最小边际成本使多个新模型变得透明和可控，分摊了可解释AI的成本。

Abstract: Interpretability is crucial for building safe, reliable, and controllable
language models, yet existing interpretability pipelines remain costly and
difficult to scale. Interpreting a new model typically requires costly training
of model-specific sparse autoencoders, manual or semi-automated labeling of SAE
components, and their subsequent validation. We introduce Atlas-Alignment, a
framework for transferring interpretability across language models by aligning
unknown latent spaces to a Concept Atlas - a labeled, human-interpretable
latent space - using only shared inputs and lightweight representational
alignment techniques. Once aligned, this enables two key capabilities in
previously opaque models: (1) semantic feature search and retrieval, and (2)
steering generation along human-interpretable atlas concepts. Through
quantitative and qualitative evaluations, we show that simple representational
alignment methods enable robust semantic retrieval and steerable generation
without the need for labeled concept data. Atlas-Alignment thus amortizes the
cost of explainable AI and mechanistic interpretability: by investing in one
high-quality Concept Atlas, we can make many new models transparent and
controllable at minimal marginal cost.

</details>


### [72] [MVeLMA: Multimodal Vegetation Loss Modeling Architecture for Predicting Post-fire Vegetation Loss](https://arxiv.org/abs/2510.27443)
*Meenu Ravi,Shailik Sarkar,Yanshen Sun,Vaishnavi Singh,Chang-Tien Lu*

Main category: cs.LG

TL;DR: 提出了MVeLMA多模态端到端机器学习管道，用于预测野火后县级植被损失，通过多模态特征集成和堆叠集成架构，结合概率建模进行不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 理解野火后植被损失对于制定有效生态恢复策略至关重要，但现有研究未充分探索所有影响因素及其模态交互，且预测模型缺乏可解释性，限制了实际应用。

Method: 使用多模态特征集成管道和堆叠集成架构，结合概率建模进行不确定性估计，生成植被损失置信度地图识别高风险县。

Result: 模型在预测野火后植被损失方面优于多个最先进和基线模型，能够识别高风险县以支持针对性恢复工作。

Conclusion: 该研究成果可为未来灾害救援规划、生态政策制定和野生动物恢复管理提供信息支持。

Abstract: Understanding post-wildfire vegetation loss is critical for developing
effective ecological recovery strategies and is often challenging due to the
extended time and effort required to capture the evolving ecosystem features.
Recent works in this area have not fully explored all the contributing factors,
their modalities, and interactions with each other. Furthermore, most research
in this domain is limited by a lack of interpretability in predictive modeling,
making it less useful in real-world settings. In this work, we propose a novel
end-to-end ML pipeline called MVeLMA (\textbf{M}ultimodal \textbf{Ve}getation
\textbf{L}oss \textbf{M}odeling \textbf{A}rchitecture) to predict county-wise
vegetation loss from fire events. MVeLMA uses a multimodal feature integration
pipeline and a stacked ensemble-based architecture to capture different
modalities while also incorporating uncertainty estimation through
probabilistic modeling. Through comprehensive experiments, we show that our
model outperforms several state-of-the-art (SOTA) and baseline models in
predicting post-wildfire vegetation loss. Furthermore, we generate vegetation
loss confidence maps to identify high-risk counties, thereby helping targeted
recovery efforts. The findings of this work have the potential to inform future
disaster relief planning, ecological policy development, and wildlife recovery
management.

</details>


### [73] [Spectral Neural Graph Sparsification](https://arxiv.org/abs/2510.27474)
*Angelica Liguori,Ettore Ritacco,Pietro Sabatino,Annalisa Socievole*

Main category: cs.LG

TL;DR: 提出Spectral Preservation Network（SPN）框架，通过生成保留原始图谱特性的简化图，实现高效的下游任务处理，克服传统图神经网络的固定结构依赖和过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（特别是图卷积网络）在处理复杂系统时受限于固定结构和过平滑问题，需要一种能生成忠实代理简化图的框架来降低计算成本并保持图的关键特性。

Method: 引入Joint Graph Evolution层联合变换图拓扑和节点特征矩阵，以及Spectral Concordance损失函数来强制保持图的谱特性和节点特征一致性。

Result: 在节点级稀疏化任务中，SPN在多个指标上优于现有最先进方法，表现出优越的性能和明显优势。

Conclusion: SPN框架通过自适应图演化和谱一致性约束，有效解决了传统图神经网络的局限性，为图表示学习提供了新的有效方法。

Abstract: Graphs are central to modeling complex systems in domains such as social
networks, molecular chemistry, and neuroscience. While Graph Neural Networks,
particularly Graph Convolutional Networks, have become standard tools for graph
learning, they remain constrained by reliance on fixed structures and
susceptibility to over-smoothing. We propose the Spectral Preservation Network,
a new framework for graph representation learning that generates reduced graphs
serving as faithful proxies of the original, enabling downstream tasks such as
community detection, influence propagation, and information diffusion at a
reduced computational cost. The Spectral Preservation Network introduces two
key components: the Joint Graph Evolution layer and the Spectral Concordance
loss. The former jointly transforms both the graph topology and the node
feature matrix, allowing the structure and attributes to evolve adaptively
across layers and overcoming the rigidity of static neighborhood aggregation.
The latter regularizes these transformations by enforcing consistency in both
the spectral properties of the graph and the feature vectors of the nodes. We
evaluate the effectiveness of Spectral Preservation Network on node-level
sparsification by analyzing well-established metrics and benchmarking against
state-of-the-art methods. The experimental results demonstrate the superior
performance and clear advantages of our approach.

</details>


### [74] [Simplex-to-Euclidean Bijections for Categorical Flow Matching](https://arxiv.org/abs/2510.27480)
*Bernardo Williams,Victor M. Yeom-Song,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 提出一种在单纯形上学习和采样概率分布的方法，通过光滑双射将开放单纯形映射到欧几里得空间，利用Aitchison几何定义映射，并通过狄利克雷插值将离散观测去量化成连续数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单纯形上使用黎曼几何或自定义噪声过程，存在局限性。本文旨在在欧几里得空间中工作同时尊重Aitchison几何，实现更好的分布建模。

Method: 使用光滑双射将开放单纯形映射到欧几里得空间，基于Aitchison几何定义映射，通过狄利克雷插值对分类数据进行去量化处理。

Result: 在合成和真实世界数据集上取得了有竞争力的性能，能够在欧几里得空间中进行密度建模，同时精确恢复原始离散分布。

Conclusion: 该方法在欧几里得空间中有效建模单纯形上的概率分布，同时保持Aitchison几何特性，优于现有基于黎曼几何的方法。

Abstract: We propose a method for learning and sampling from probability distributions
supported on the simplex. Our approach maps the open simplex to Euclidean space
via smooth bijections, leveraging the Aitchison geometry to define the
mappings, and supports modeling categorical data by a Dirichlet interpolation
that dequantizes discrete observations into continuous ones. This enables
density modeling in Euclidean space through the bijection while still allowing
exact recovery of the original discrete distribution. Compared to previous
methods that operate on the simplex using Riemannian geometry or custom noise
processes, our approach works in Euclidean space while respecting the Aitchison
geometry, and achieves competitive performance on both synthetic and real-world
data sets.

</details>


### [75] [Thought Branches: Interpreting LLM Reasoning Requires Resampling](https://arxiv.org/abs/2510.27484)
*Uzay Macar,Paul C. Bogdan,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 该论文提出通过重采样研究思维链分布的方法，用于可靠地分析模型推理的因果关系，并展示了在代理错位、推理引导、步骤移除和提示影响等场景下的应用案例。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多只分析单个思维链样本，无法充分理解模型的因果影响和底层计算过程。思维链分布难以完全指定，但可以通过采样来理解。

Method: 使用重采样方法研究模型决策，包括：在代理错位场景中重采样特定句子测量下游影响；通过重采样选择具有期望属性的完成作为策略内替代方案；引入弹性度量重复重采样防止类似内容重现；适应因果中介分析研究提示的累积影响。

Result: 发现自我保存句子的因果影响很小；策略外干预产生小而不稳定的效果；关键规划语句难以移除但移除后影响很大；提示对输出有因果影响但未明确提及，对思维链产生微妙累积影响。

Conclusion: 通过重采样研究分布能够实现可靠的因果分析、更清晰的模型推理叙述，以及原则性的思维链干预。

Abstract: Most work interpreting reasoning models studies only a single
chain-of-thought (CoT), yet these models define distributions over many
possible CoTs. We argue that studying a single sample is inadequate for
understanding causal influence and the underlying computation. Though fully
specifying this distribution is intractable, it can be understood by sampling.
We present case studies using resampling to investigate model decisions. First,
when a model states a reason for its action, does that reason actually cause
the action? In "agentic misalignment" scenarios, we resample specific sentences
to measure their downstream effects. Self-preservation sentences have small
causal impact, suggesting they do not meaningfully drive blackmail. Second, are
artificial edits to CoT sufficient for steering reasoning? These are common in
literature, yet take the model off-policy. Resampling and selecting a
completion with the desired property is a principled on-policy alternative. We
find off-policy interventions yield small and unstable effects compared to
resampling in decision-making tasks. Third, how do we understand the effect of
removing a reasoning step when the model may repeat it post-edit? We introduce
a resilience metric that repeatedly resamples to prevent similar content from
reappearing downstream. Critical planning statements resist removal but have
large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can
our methods teach us anything in these settings? Adapting causal mediation
analysis, we find that hints that have a causal effect on the output without
being explicitly mentioned exert a subtle and cumulative influence on the CoT
that persists even if the hint is removed. Overall, studying distributions via
resampling enables reliable causal analysis, clearer narratives of model
reasoning, and principled CoT interventions.

</details>


### [76] [FedAdamW: A Communication-Efficient Optimizer with Convergence and Generalization Guarantees for Federated Large Models](https://arxiv.org/abs/2510.27486)
*Junkang Liu,Fanhua Shang,Kewen Zhu,Hongying Liu,Yuanyuan Liu,Jin Liu*

Main category: cs.LG

TL;DR: 提出了FedAdamW算法，这是首个针对联邦学习的AdamW优化器，解决了数据异构性、局部过拟合和收敛慢的问题，在语言和视觉Transformer模型上显著减少了通信轮数并提高了测试精度。


<details>
  <summary>Details</summary>
Motivation: 直接应用AdamW到联邦学习面临三大挑战：(1)数据异构性导致二阶矩估计方差大；(2)局部过拟合导致客户端漂移；(3)每轮重新初始化矩估计减慢收敛速度。

Method: FedAdamW使用局部校正机制和分离权重衰减来对齐局部更新与全局更新，高效聚合二阶矩估计的均值以减少方差并重新初始化它们。

Result: 理论证明FedAdamW达到线性加速收敛率，无需异构性假设。实验验证在语言和视觉Transformer模型上相比基线显著减少通信轮数并提高测试精度。

Conclusion: FedAdamW是首个专门针对联邦学习的AdamW优化器，有效解决了数据异构性、局部过拟合和收敛慢的问题，在训练和微调大模型中表现出色。

Abstract: AdamW has become one of the most effective optimizers for training
large-scale models. We have also observed its effectiveness in the context of
federated learning (FL). However, directly applying AdamW in federated learning
settings poses significant challenges: (1) due to data heterogeneity, AdamW
often yields high variance in the second-moment estimate $\boldsymbol{v}$; (2)
the local overfitting of AdamW may cause client drift; and (3) Reinitializing
moment estimates ($\boldsymbol{v}$, $\boldsymbol{m}$) at each round slows down
convergence. To address these challenges, we propose the first
\underline{Fed}erated \underline{AdamW} algorithm, called \texttt{FedAdamW},
for training and fine-tuning various large models. \texttt{FedAdamW} aligns
local updates with the global update using both a \textbf{local correction
mechanism} and decoupled weight decay to mitigate local overfitting.
\texttt{FedAdamW} efficiently aggregates the \texttt{mean} of the second-moment
estimates to reduce their variance and reinitialize them. Theoretically, we
prove that \texttt{FedAdamW} achieves a linear speedup convergence rate of
$\mathcal{O}(\sqrt{(L \Delta \sigma_l^2)/(S K R \epsilon^2)}+(L \Delta)/R)$
without \textbf{heterogeneity assumption}, where $S$ is the number of
participating clients per round, $K$ is the number of local iterations, and $R$
is the total number of communication rounds. We also employ PAC-Bayesian
generalization analysis to explain the effectiveness of decoupled weight decay
in local training. Empirically, we validate the effectiveness of
\texttt{FedAdamW} on language and vision Transformer models. Compared to
several baselines, \texttt{FedAdamW} significantly reduces communication rounds
and improves test accuracy. The code is available in
https://github.com/junkangLiu0/FedAdamW.

</details>


### [77] [InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames](https://arxiv.org/abs/2510.27497)
*Haorui Li,Weitao Du,Yuqiang Li,Hongyu Guo,Shengchao Liu*

Main category: cs.LG

TL;DR: InertialAR是一个基于Transformer的自回归模型，通过惯性坐标系对齐和几何感知注意力机制，解决了3D分子生成中的SE(3)不变性和原子索引排列不变性问题，在无条件分子生成和可控生成任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer自回归模型在文本和图像模态上取得了成功，但在3D分子生成领域仍存在挑战：需要设计对SE(3)变换和原子索引排列不变的序列化方法，以及处理离散原子类型与连续3D坐标的混合表示。

Method: 提出惯性坐标系对齐的规范序列化方法，确保SE(3)和排列不变性；使用几何旋转位置编码(GeoRoPE)增强注意力的几何感知能力；采用分层自回归范式，先预测原子类型再通过扩散损失预测3D坐标。

Result: 在QM9、GEOM-Drugs和B3LYP数据集的无条件分子生成任务中，10个评估指标中有7个达到最先进性能；在可控生成任务中，所有5个评估指标均显著优于基线方法。

Conclusion: InertialAR成功将Transformer自回归模型扩展到3D分子生成领域，通过几何感知的序列化和建模方法，在分子生成任务中展现了优越性能。

Abstract: Transformer-based autoregressive models have emerged as a unifying paradigm
across modalities such as text and images, but their extension to 3D molecule
generation remains underexplored. The gap stems from two fundamental
challenges: (1) tokenizing molecules into a canonical 1D sequence of tokens
that is invariant to both SE(3) transformations and atom index permutations,
and (2) designing an architecture capable of modeling hybrid atom-based tokens
that couple discrete atom types with continuous 3D coordinates. To address
these challenges, we introduce InertialAR. InertialAR devises a canonical
tokenization that aligns molecules to their inertial frames and reorders atoms
to ensure SE(3) and permutation invariance. Moreover, InertialAR equips the
attention mechanism with geometric awareness via geometric rotary positional
encoding (GeoRoPE). In addition, it utilizes a hierarchical autoregressive
paradigm to predict the next atom-based token, predicting the atom type first
and then its 3D coordinates via Diffusion loss. Experimentally, InertialAR
achieves state-of-the-art performance on 7 of the 10 evaluation metrics for
unconditional molecule generation across QM9, GEOM-Drugs, and B3LYP. Moreover,
it significantly outperforms strong baselines in controllable generation for
targeted chemical functionality, attaining state-of-the-art results across all
5 metrics.

</details>


### [78] [DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm](https://arxiv.org/abs/2510.27504)
*Junkang Liu,Yuxuan Tian,Fanhua Shang,Yuanyuan Liu,Hongying Liu,Junchao Zhou,Daorui Ding*

Main category: cs.LG

TL;DR: 提出DP-FedPGN算法，通过引入全局梯度范数惩罚来寻找全局平坦最小值，缓解联邦学习中差分隐私保护导致的模型泛化性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的客户端级差分隐私联邦学习方法通常会导致更尖锐的损失景观，降低模型泛化能力。虽然使用SAM可以找到局部平坦最小值，但局部平坦性不能反映CL-DPFL中的全局平坦性。

Method: 在DP-FedPGN中，将全局梯度范数惩罚引入局部损失函数来寻找全局平坦最小值。该方法不仅找到更平坦的全局最小值，还减少了局部更新范数，从而进一步减少梯度裁剪误差。

Result: 在ResNet和Transformer模型上进行的有效性测试表明，在六个视觉和自然语言处理任务中相比现有最先进算法取得了显著改进。

Conclusion: DP-FedPGN算法通过寻找全局平坦最小值，有效缓解了差分隐私导致的性能下降，同时消除了数据异构性的影响并实现了快速收敛。

Abstract: To prevent inference attacks in Federated Learning (FL) and reduce the
leakage of sensitive information, Client-level Differentially Private Federated
Learning (CL-DPFL) is widely used. However, current CL-DPFL methods usually
result in sharper loss landscapes, which leads to a decrease in model
generalization after differential privacy protection. By using Sharpness Aware
Minimization (SAM), the current popular federated learning methods are to find
a local flat minimum value to alleviate this problem. However, the local
flatness may not reflect the global flatness in CL-DPFL. Therefore, to address
this issue and seek global flat minima of models, we propose a new CL-DPFL
algorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to
the local loss to find the global flat minimum. Moreover, by using our global
gradient norm penalty, we not only find a flatter global minimum but also
reduce the locally updated norm, which means that we further reduce the error
of gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN
mitigates the performance degradation caused by DP. Meanwhile, the proposed
DP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves
fast convergence. We also use R\'enyi DP to provide strict privacy guarantees
and provide sensitivity analysis for local updates. Finally, we conduct
effectiveness tests on both ResNet and Transformer models, and achieve
significant improvements in six visual and natural language processing tasks
compared to existing state-of-the-art algorithms. The code is available at
https://github.com/junkangLiu0/DP-FedPGN

</details>


### [79] [Learning Sparse Approximate Inverse Preconditioners for Conjugate Gradient Solvers on GPUs](https://arxiv.org/abs/2510.27517)
*Zherui Yang,Zhehao Li,Kangbo Lyu,Yixuan Li,Tao Du,Ligang Liu*

Main category: cs.LG

TL;DR: 提出一种基于GNN的学习方法构建GPU友好的稀疏近似逆预处理器，避免三角求解，仅需两次矩阵向量乘积，在GPU上实现40%-53%的求解时间减少。


<details>
  <summary>Details</summary>
Motivation: 传统预处理器依赖预设算法，无法充分利用数据优化；现有基于学习的方法使用GNN但依赖不完全分解，导致三角求解阻碍GPU并行化且引入长程依赖问题。

Method: 使用GNN构建稀疏近似逆预处理器，避免三角求解，仅需两次矩阵向量乘积；引入基于统计的尺度不变损失函数，匹配CG收敛率依赖条件数而非绝对尺度的特性。

Result: 在三个PDE数据集和一个合成数据集上评估，优于标准预处理器和先前基于学习的预处理器，GPU求解时间减少40%-53%，条件数更优且泛化性能更好。

Conclusion: 提出的学习型预处理器方法在GPU上显著加速共轭梯度求解，同时保持更好的数值性能和泛化能力。

Abstract: The conjugate gradient solver (CG) is a prevalent method for solving
symmetric and positive definite linear systems Ax=b, where effective
preconditioners are crucial for fast convergence. Traditional preconditioners
rely on prescribed algorithms to offer rigorous theoretical guarantees, while
limiting their ability to exploit optimization from data. Existing
learning-based methods often utilize Graph Neural Networks (GNNs) to improve
the performance and speed up the construction. However, their reliance on
incomplete factorization leads to significant challenges: the associated
triangular solve hinders GPU parallelization in practice, and introduces
long-range dependencies which are difficult for GNNs to model. To address these
issues, we propose a learning-based method to generate GPU-friendly
preconditioners, particularly using GNNs to construct Sparse Approximate
Inverse (SPAI) preconditioners, which avoids triangular solves and requires
only two matrix-vector products at each CG step. The locality of matrix-vector
product is compatible with the local propagation mechanism of GNNs. The
flexibility of GNNs also allows our approach to be applied in a wide range of
scenarios. Furthermore, we introduce a statistics-based scale-invariant loss
function. Its design matches CG's property that the convergence rate depends on
the condition number, rather than the absolute scale of A, leading to improved
performance of the learned preconditioner. Evaluations on three PDE-derived
datasets and one synthetic dataset demonstrate that our method outperforms
standard preconditioners (Diagonal, IC, and traditional SPAI) and previous
learning-based preconditioners on GPUs. We reduce solution time on GPUs by
40%-53% (68%-113% faster), along with better condition numbers and superior
generalization performance. Source code available at
https://github.com/Adversarr/LearningSparsePreconditioner4GPU

</details>


### [80] [Leveraging Generic Time Series Foundation Models for EEG Classification](https://arxiv.org/abs/2510.27522)
*Théo Gnassounou,Yessin Moakher,Shifeng Xie,Vasilii Feofanov,Ievgen Redko*

Main category: cs.LG

TL;DR: 研究评估通用时间序列基础模型在脑电图任务中的表现，发现即使使用非神经数据或合成数据预训练，也能在EEG任务上超越专业模型。


<details>
  <summary>Details</summary>
Motivation: 探索通用时间序列基础模型在特定生物医学信号（如EEG）上的适用性，验证跨域预训练的有效性。

Method: 使用两种预训练策略：在多领域真实时间序列数据上预训练，以及在纯合成数据上预训练，然后在EEG任务（运动想象分类和睡眠阶段预测）上进行测试。

Result: 两种预训练变体都表现出色，一致优于广泛使用的卷积基线EEGNet和最新的EEG专用基础模型CBraMod。

Conclusion: 通用时间序列基础模型即使使用非神经来源数据或合成信号预训练，也能有效迁移到EEG分析，表明EEG可以从更广泛的时间序列文献进展中受益。

Abstract: Foundation models for time series are emerging as powerful general-purpose
backbones, yet their potential for domain-specific biomedical signals such as
electroencephalography (EEG) remains rather unexplored. In this work, we
investigate the applicability a recently proposed time series classification
foundation model, to a different EEG tasks such as motor imagery classification
and sleep stage prediction. We test two pretraining regimes: (a) pretraining on
heterogeneous real-world time series from multiple domains, and (b) pretraining
on purely synthetic data. We find that both variants yield strong performance,
consistently outperforming EEGNet, a widely used convolutional baseline, and
CBraMod, the most recent EEG-specific foundation model. These results suggest
that generalist time series foundation models, even when pretrained on data of
non-neural origin or on synthetic signals, can transfer effectively to EEG. Our
findings highlight the promise of leveraging cross-domain pretrained models for
brain signal analysis, suggesting that EEG may benefit from advances in the
broader time series literature.

</details>


### [81] [Active transfer learning for structural health monitoring](https://arxiv.org/abs/2510.27525)
*J. Poole,N. Dervilis,K. Worden,P. Gardner,V. Giglioni,R. S. Mills,A. J. Hughes*

Main category: cs.LG

TL;DR: 提出了一种结合迁移学习和主动学习的贝叶斯框架，用于人口结构健康监测，通过少量标记数据提高分类模型的数据效率。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测数据获取成本高且困难，特别是标记数据。不同结构的数据分布差异导致传统机器学习方法泛化能力差。

Method: 使用贝叶斯框架进行领域自适应，结合主动采样策略选择最有信息量的观测数据进行标记，减少所需标记数据量。

Result: 在实验桥梁数据集上验证，结合迁移学习和主动学习能显著提高标签稀缺场景下分类模型的数据效率。

Conclusion: 该方法可减少结构运营期间的检查次数，从而降低运营成本，对数据驱动的结构运维具有重要意义。

Abstract: Data for training structural health monitoring (SHM) systems are often
expensive and/or impractical to obtain, particularly for labelled data.
Population-based SHM (PBSHM) aims to address this limitation by leveraging data
from multiple structures. However, data from different structures will follow
distinct distributions, potentially leading to large generalisation errors for
models learnt via conventional machine learning methods. To address this issue,
transfer learning -- in the form of domain adaptation (DA) -- can be used to
align the data distributions. Most previous approaches have only considered
\emph{unsupervised} DA, where no labelled target data are available; they do
not consider how to incorporate these technologies in an online framework --
updating as labels are obtained throughout the monitoring campaign. This paper
proposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DA
mappings using a limited quantity of labelled target data. In addition, this
model is integrated into an active sampling strategy to guide inspections to
select the most informative observations to label -- leading to further
reductions in the required labelled data to learn a target classifier. The
effectiveness of this methodology is evaluated on a population of experimental
bridges. Specifically, this population includes data corresponding to several
damage states, as well as, a comprehensive set of environmental conditions. It
is found that combining transfer learning and active learning can improve data
efficiency when learning classification models in label-scarce scenarios. This
result has implications for data-informed operation and maintenance of
structures, suggesting a reduction in inspections over the operational lifetime
of a structure -- and therefore a reduction in operational costs -- can be
achieved.

</details>


### [82] [TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation Suppression and Outlier Control](https://arxiv.org/abs/2510.27527)
*Yuxiang Chen,Xiaoming Xu,Pengle Zhang,Michael Beyer,Martin Rapp,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: TetraJet-v2是一种端到端的4位全量化训练方法，使用NVFP4格式在LLM训练中量化激活、权重和梯度，解决了权重振荡和异常值问题，显著缩小了与全精度训练的性能差距。


<details>
  <summary>Details</summary>
Motivation: LLM训练成本极高，推动了对低精度全量化训练的兴趣。虽然4位格式如NVFP4能带来显著的效率提升，但在如此低精度下实现接近无损的训练仍然具有挑战性。

Method: 提出TetraJet-v2方法，包括：1）NVFP4线性层的无偏双块量化方法；2）抑制权重振荡的OsciReset算法；3）保持异常值精度的OutControl算法。

Result: 在预训练LLM中，TetraJet-v2始终优于先前的FP4训练方法，模型规模达370M，数据规模达200B tokens，将性能差距缩小了平均51.3%。

Conclusion: TetraJet-v2通过解决权重振荡和异常值问题，成功实现了高效的4位全量化训练，显著提升了低精度LLM训练的性能。

Abstract: Large Language Models (LLMs) training is prohibitively expensive, driving
interest in low-precision fully-quantized training (FQT). While novel 4-bit
formats like NVFP4 offer substantial efficiency gains, achieving near-lossless
training at such low precision remains challenging. We introduce TetraJet-v2,
an end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,
and gradients in all linear layers. We identify two critical issues hindering
low-precision LLM training: weight oscillation and outliers. To address these,
we propose: 1) an unbiased double-block quantization method for NVFP4 linear
layers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)
OutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently
outperforms prior FP4 training methods on pre-training LLMs across varying
model sizes up to 370M and data sizes up to 200B tokens, reducing the
performance gap to full-precision training by an average of 51.3%.

</details>


### [83] [AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for Proprietary Data Challenges in Financial Question Answering](https://arxiv.org/abs/2510.27537)
*Mohammad Zahangir Alam,Khandoker Ashik Uz Zaman,Mahdi H. Miraz*

Main category: cs.LG

TL;DR: AstuteRAG-FQA是一个专为金融问答设计的自适应RAG框架，通过任务感知提示工程解决金融领域RAG应用中的关键挑战，包括数据访问限制、检索准确性、监管约束和敏感数据解释问题。


<details>
  <summary>Details</summary>
Motivation: 将RAG应用于金融领域面临独特挑战：专有数据集访问受限、检索准确性有限、监管约束严格以及敏感数据解释困难。需要专门框架来解决这些问题。

Method: 采用混合检索策略整合开源和专有金融数据，使用动态提示框架实时适应查询复杂度，提出四层任务分类，并集成多层安全机制（差分隐私、数据匿名化、基于角色的访问控制）和实时合规监控。

Result: 评估了三种数据集成技术（上下文嵌入、小模型增强、目标微调），分析了它们在不同金融环境中的效率和可行性。

Conclusion: AstuteRAG-FQA为金融问答提供了一个安全、合规且高效的自适应RAG框架，能够处理多样化的金融查询类型，同时确保数据安全和监管合规。

Abstract: Retrieval-Augmented Generation (RAG) shows significant promise in
knowledge-intensive tasks by improving domain specificity, enhancing temporal
relevance, and reducing hallucinations. However, applying RAG to finance
encounters critical challenges: restricted access to proprietary datasets,
limited retrieval accuracy, regulatory constraints, and sensitive data
interpretation. We introduce AstuteRAG-FQA, an adaptive RAG framework tailored
for Financial Question Answering (FQA), leveraging task-aware prompt
engineering to address these challenges. The framework uses a hybrid retrieval
strategy integrating both open-source and proprietary financial data while
maintaining strict security protocols and regulatory compliance. A dynamic
prompt framework adapts in real time to query complexity, improving precision
and contextual relevance. To systematically address diverse financial queries,
we propose a four-tier task classification: explicit factual, implicit factual,
interpretable rationale, and hidden rationale involving implicit causal
reasoning. For each category, we identify key challenges, datasets, and
optimization techniques within the retrieval and generation process. The
framework incorporates multi-layered security mechanisms including differential
privacy, data anonymization, and role-based access controls to protect
sensitive financial information. Additionally, AstuteRAG-FQA implements
real-time compliance monitoring through automated regulatory validation systems
that verify responses against industry standards and legal obligations. We
evaluate three data integration techniques - contextual embedding, small model
augmentation, and targeted fine-tuning - analyzing their efficiency and
feasibility across varied financial environments.

</details>


### [84] [ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling](https://arxiv.org/abs/2510.27610)
*Zhuohan Wang,Ziwei Zhu,Ziniu Li,Congliang Chen,Yizhou Han,Yufeng Lin,Zhihang Lin,Angyang Gu,Xinglin Hu,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: 提出了ORGEval框架，使用图论方法评估LLM在优化问题建模中的表现，通过图同构检测替代传统的求解器方法，解决了现有评估方法的不一致性和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 工业应用中优化问题的建模需要大量人工和专业知识，LLM有潜力自动化这一过程，但缺乏有效的评估指标。现有基于求解器的方法存在不一致性、不可行性和高计算成本问题。

Method: 提出ORGEval框架，将优化模型表示为图，将等价性检测转化为图同构测试。证明当图为对称可分解时，Weisfeiler-Lehman测试能正确检测同构，并集成定制WL测试与SD检测算法。

Result: 实验显示ORGEval能成功检测模型等价性，在随机参数配置下产生100%一致结果，运行时间显著优于基于求解器的方法，特别是在困难问题上。基于此构建Bench4Opt数据集。

Conclusion: 优化建模对所有LLM仍具挑战性，但DeepSeek-V3和Claude-Opus-4在直接提示下达到最高准确率，甚至超过领先的推理模型。

Abstract: Formulating optimization problems for industrial applications demands
significant manual effort and domain expertise. While Large Language Models
(LLMs) show promise in automating this process, evaluating their performance
remains difficult due to the absence of robust metrics. Existing solver-based
approaches often face inconsistency, infeasibility issues, and high
computational costs. To address these issues, we propose ORGEval, a
graph-theoretic evaluation framework for assessing LLMs' capabilities in
formulating linear and mixed-integer linear programs. ORGEval represents
optimization models as graphs, reducing equivalence detection to graph
isomorphism testing. We identify and prove a sufficient condition, when the
tested graphs are symmetric decomposable (SD), under which the
Weisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.
Building on this, ORGEval integrates a tailored variant of the WL-test with an
SD detection algorithm to evaluate model equivalence. By focusing on structural
equivalence rather than instance-level configurations, ORGEval is robust to
numerical variations. Experimental results show that our method can
successfully detect model equivalence and produce 100\% consistent results
across random parameter configurations, while significantly outperforming
solver-based methods in runtime, especially on difficult problems. Leveraging
ORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs
on optimization modeling. Our results reveal that although optimization
modeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4
achieve the highest accuracies under direct prompting, outperforming even
leading reasoning models.

</details>


### [85] [Panprediction: Optimal Predictions for Any Downstream Task and Loss](https://arxiv.org/abs/2510.27638)
*Sivaraman Balakrishnan,Nika Haghtalab,Daniel Hsu,Brian Lee,Eric Zhao*

Main category: cs.LG

TL;DR: 提出panprediction框架，将模型训练视为从数据中提取足够信息，使模型能够最小化下游任务中的多种损失函数，统一了omniprediction和多组学习的概念。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习关注在固定分布上最小化固定损失函数，而新范式认为模型训练应提取足够信息以支持下游多种任务和损失函数的最小化。

Method: 设计了确定性和随机性panpredictor算法，分别需要Õ(1/ε³)和Õ(1/ε²)样本量，通过将panprediction约简到统计高效的步校准概念来实现。

Result: 在温和假设下，同时最小化无限多任务上的无限多损失函数在统计上可以像最小化单一任务上的单一损失函数一样容易。改进了确定性omniprediction的样本复杂度。

Conclusion: panprediction框架统一了omniprediction和多组学习，证明了在适当条件下，同时处理多任务多损失的统计复杂度与单任务单损失相当。

Abstract: Supervised learning is classically formulated as training a model to minimize
a fixed loss function over a fixed distribution, or task. However, an emerging
paradigm instead views model training as extracting enough information from
data so that the model can be used to minimize many losses on many downstream
tasks. We formalize a mathematical framework for this paradigm, which we call
panprediction, and study its statistical complexity. Formally, panprediction
generalizes omniprediction and sits upstream from multi-group learning, which
respectively focus on predictions that generalize to many downstream losses or
many downstream tasks, but not both. Concretely, we design algorithms that
learn deterministic and randomized panpredictors with
$\tilde{O}(1/\varepsilon^3)$ and $\tilde{O}(1/\varepsilon^2)$ samples,
respectively. Our results demonstrate that under mild assumptions,
simultaneously minimizing infinitely many losses on infinitely many tasks can
be as statistically easy as minimizing one loss on one task. Along the way, we
improve the best known sample complexity guarantee of deterministic
omniprediction by a factor of $1/\varepsilon$, and match all other known sample
complexity guarantees of omniprediction and multi-group learning. Our key
technical ingredient is a nearly lossless reduction from panprediction to a
statistically efficient notion of calibration, called step calibration.

</details>


### [86] [Imbalanced Classification through the Lens of Spurious Correlations](https://arxiv.org/abs/2510.27650)
*Jakob Hackstein,Sidney Bender*

Main category: cs.LG

TL;DR: 提出一种基于可解释AI的方法，通过反事实解释来识别和消除类别不平衡导致的Clever Hans效应，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡是机器学习中的基础挑战，传统方法关注数据重采样或损失重加权，但本文认为不平衡会通过少数类规范不足而放大Clever Hans效应。

Method: 采用基于反事实解释的方法，利用可解释AI技术联合识别和消除不平衡条件下出现的Clever Hans效应。

Result: 在三个数据集上实现了有竞争力的分类性能，并展示了不平衡条件下Clever Hans效应的出现机制。

Conclusion: 该方法为理解类别不平衡问题提供了新视角，揭示了现有方法忽视的Clever Hans效应问题。

Abstract: Class imbalance poses a fundamental challenge in machine learning, frequently
leading to unreliable classification performance. While prior methods focus on
data- or loss-reweighting schemes, we view imbalance as a data condition that
amplifies Clever Hans (CH) effects by underspecification of minority classes.
In a counterfactual explanations-based approach, we propose to leverage
Explainable AI to jointly identify and eliminate CH effects emerging under
imbalance. Our method achieves competitive classification performance on three
datasets and demonstrates how CH effects emerge under imbalance, a perspective
largely overlooked by existing approaches.

</details>


### [87] [Information-Theoretic Greedy Layer-wise Training for Traffic Sign Recognition](https://arxiv.org/abs/2510.27651)
*Shuyan Lyu,Zhanzimo Wu,Junliang Du*

Main category: cs.LG

TL;DR: 提出了一种基于确定性信息瓶颈和Rényi熵的逐层训练方法，在CIFAR数据集上性能优于现有逐层训练基线，且与SGD相当。


<details>
  <summary>Details</summary>
Motivation: 传统端到端训练需要反向传播和存储中间输出，在生物学上不现实。逐层训练可以避免这些问题，但现有方法只在简单架构和小数据集上验证。

Method: 基于信息瓶颈原理，每层与辅助分类器联合训练，学习最小充分的任务相关表示。使用确定性信息瓶颈和矩阵Rényi熵函数。

Result: 在CIFAR-10和CIFAR-100上验证了有效性，并在交通标志识别任务中展示了实用性。性能优于现有逐层训练基线，与SGD相当。

Conclusion: 提出的逐层训练方法不仅性能优越，而且更具生物合理性，为深度网络训练提供了新思路。

Abstract: Modern deep neural networks (DNNs) are typically trained with a global
cross-entropy loss in a supervised end-to-end manner: neurons need to store
their outgoing weights; training alternates between a forward pass
(computation) and a top-down backward pass (learning) which is biologically
implausible. Alternatively, greedy layer-wise training eliminates the need for
cross-entropy loss and backpropagation. By avoiding the computation of
intermediate gradients and the storage of intermediate outputs, it reduces
memory usage and helps mitigate issues such as vanishing or exploding
gradients. However, most existing layer-wise training approaches have been
evaluated only on relatively small datasets with simple deep architectures. In
this paper, we first systematically analyze the training dynamics of popular
convolutional neural networks (CNNs) trained by stochastic gradient descent
(SGD) through an information-theoretic lens. Our findings reveal that networks
converge layer-by-layer from bottom to top and that the flow of information
adheres to a Markov information bottleneck principle. Building on these
observations, we propose a novel layer-wise training approach based on the
recently developed deterministic information bottleneck (DIB) and the
matrix-based R\'enyi's $\alpha$-order entropy functional. Specifically, each
layer is trained jointly with an auxiliary classifier that connects directly to
the output layer, enabling the learning of minimal sufficient task-relevant
representations. We empirically validate the effectiveness of our training
procedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further
demonstrate its applicability to a practical task involving traffic sign
recognition. Our approach not only outperforms existing layer-wise training
baselines but also achieves performance comparable to SGD.

</details>


### [88] [Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems](https://arxiv.org/abs/2510.27659)
*Alireza Saleh Abadi,Leen-Kiat Soh*

Main category: cs.LG

TL;DR: 该论文分析了多智能体强化学习中的开放性问题及其对信用分配问题的影响，通过概念分析和实证研究揭示了开放环境如何导致信用分配错误。


<details>
  <summary>Details</summary>
Motivation: 传统信用分配方法假设静态智能体群体、固定任务和稳定类型，但在开放系统中这些假设被打破，需要研究开放性如何影响信用分配。

Method: 首先进行概念分析，引入新的开放性子类别，然后使用代表性时间和结构算法在开放环境中进行实证研究。

Result: 开放性直接导致信用分配错误，表现为损失函数不稳定和性能显著下降。

Conclusion: 开放环境对信用分配构成重大挑战，需要开发新的方法来应对动态变化的智能体群体、任务和类型。

Abstract: In the rapidly evolving field of multi-agent reinforcement learning (MARL),
understanding the dynamics of open systems is crucial. Openness in MARL refers
to the dynam-ic nature of agent populations, tasks, and agent types with-in a
system. Specifically, there are three types of openness as reported in (Eck et
al. 2023) [2]: agent openness, where agents can enter or leave the system at
any time; task openness, where new tasks emerge, and existing ones evolve or
disappear; and type openness, where the capabil-ities and behaviors of agents
change over time. This report provides a conceptual and empirical review,
focusing on the interplay between openness and the credit assignment problem
(CAP). CAP involves determining the contribution of individual agents to the
overall system performance, a task that becomes increasingly complex in open
environ-ments. Traditional credit assignment (CA) methods often assume static
agent populations, fixed and pre-defined tasks, and stationary types, making
them inadequate for open systems. We first conduct a conceptual analysis,
in-troducing new sub-categories of openness to detail how events like agent
turnover or task cancellation break the assumptions of environmental
stationarity and fixed team composition that underpin existing CAP methods. We
then present an empirical study using representative temporal and structural
algorithms in an open environment. The results demonstrate that openness
directly causes credit misattribution, evidenced by unstable loss functions and
significant performance degradation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [89] [Overspecified Mixture Discriminant Analysis: Exponential Convergence, Statistical Guarantees, and Remote Sensing Applications](https://arxiv.org/abs/2510.27056)
*Arman Bolatov,Alan Legg,Igor Melnykov,Amantay Nurlanuly,Maxat Tezekbayev,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 该研究分析了混合判别分析(MDA)在过拟合情况下的分类误差，证明了EM算法能快速收敛到贝叶斯风险，并在有限样本下获得n^{-1/2}的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 研究MDA在混合分量数量超过真实数据分布时的分类性能，这在复杂数据场景(如图像和文本分类)中经常被经验性地使用。

Method: 使用每个类内的双分量高斯混合模型来拟合单高斯生成的数据，分析EM算法的收敛性和统计分类误差。

Result: 在适当初始化下，EM算法在总体水平上以指数速度收敛到贝叶斯风险；在有限样本下，分类误差以n^{-1/2}的速率收敛到贝叶斯风险。

Conclusion: 为理解过拟合MDA的性能提供了严格的理论框架，并在遥感数据集上验证了理论结果。

Abstract: This study explores the classification error of Mixture Discriminant Analysis
(MDA) in scenarios where the number of mixture components exceeds those present
in the actual data distribution, a condition known as overspecification. We use
a two-component Gaussian mixture model within each class to fit data generated
from a single Gaussian, analyzing both the algorithmic convergence of the
Expectation-Maximization (EM) algorithm and the statistical classification
error. We demonstrate that, with suitable initialization, the EM algorithm
converges exponentially fast to the Bayes risk at the population level.
Further, we extend our results to finite samples, showing that the
classification error converges to Bayes risk with a rate $n^{-1/2}$ under mild
conditions on the initial parameter estimates and sample size. This work
provides a rigorous theoretical framework for understanding the performance of
overspecified MDA, which is often used empirically in complex data settings,
such as image and text classification. To validate our theory, we conduct
experiments on remote sensing datasets.

</details>


### [90] [Decreasing Entropic Regularization Averaged Gradient for Semi-Discrete Optimal Transport](https://arxiv.org/abs/2510.27340)
*Ferdinand Genans,Antoine Godichon-Baggioni,François-Xavier Vialard,Olivier Wintenberger*

Main category: stat.ML

TL;DR: 提出DRAG算法，通过自适应降低熵正则化来加速半离散最优传输问题的求解，相比固定正则化方法获得更好的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 熵正则化虽然能加速最优传输问题求解，但会引入偏差。为了在保持加速效果的同时减少偏差，需要自适应降低正则化参数。

Method: 提出DRAG算法，在随机梯度下降中随优化步数递减正则化参数，实现自适应正则化调整。

Result: 理论分析表明DRAG相比固定正则化方案具有加速效果，在OT成本和势函数估计上达到无偏的O(1/t)复杂度，在OT映射上达到O(1/√t)收敛率。

Conclusion: 数值实验验证了DRAG的有效性，证明递减正则化策略在半离散最优传输问题中具有实际优势。

Abstract: Adding entropic regularization to Optimal Transport (OT) problems has become
a standard approach for designing efficient and scalable solvers. However,
regularization introduces a bias from the true solution. To mitigate this bias
while still benefiting from the acceleration provided by regularization, a
natural solver would adaptively decrease the regularization as it approaches
the solution. Although some algorithms heuristically implement this idea, their
theoretical guarantees and the extent of their acceleration compared to using a
fixed regularization remain largely open. In the setting of semi-discrete OT,
where the source measure is continuous and the target is discrete, we prove
that decreasing the regularization can indeed accelerate convergence. To this
end, we introduce DRAG: Decreasing (entropic) Regularization Averaged Gradient,
a stochastic gradient descent algorithm where the regularization decreases with
the number of optimization steps. We provide a theoretical analysis showing
that DRAG benefits from decreasing regularization compared to a fixed scheme,
achieving an unbiased $\mathcal{O}(1/t)$ sample and iteration complexity for
both the OT cost and the potential estimation, and a $\mathcal{O}(1/\sqrt{t})$
rate for the OT map. Our theoretical findings are supported by numerical
experiments that validate the effectiveness of DRAG and highlight its practical
advantages.

</details>


### [91] [On the Equivalence of Optimal Transport Problem and Action Matching with Optimal Vector Fields](https://arxiv.org/abs/2510.27385)
*Nikita Kornilov,Alexander Korotin*

Main category: stat.ML

TL;DR: 本文证明在流匹配方法中只考虑最优向量场可以导致另一种最优传输方法：动作匹配。动作匹配学习定义整个分布序列ODE的向量场，而不像流匹配那样需要手动选择插值。


<details>
  <summary>Details</summary>
Motivation: 流匹配方法通过构造分布间的插值来学习定义ODE的向量场。最近研究表明，通过只考虑最优传输问题中的特定最优向量场，流匹配可以实现二次成本函数下的最优映射。本文旨在探索这种只考虑最优向量场的做法是否能导向另一种最优传输方法。

Method: 提出了动作匹配方法，该方法学习定义整个给定分布序列ODE的向量场，而不需要手动选择初始插值。与流匹配不同，动作匹配直接针对分布序列进行优化。

Result: 证明了在流匹配中只考虑最优向量场可以自然地导向动作匹配方法，为最优传输问题提供了新的解决途径。

Conclusion: 动作匹配提供了一种替代流匹配的最优传输方法，能够直接学习整个分布序列的动态过程，避免了手动选择插值的需求。

Abstract: Flow Matching (FM) method in generative modeling maps arbitrary probability
distributions by constructing an interpolation between them and then learning
the vector field that defines ODE for this interpolation. Recently, it was
shown that FM can be modified to map distributions optimally in terms of the
quadratic cost function for any initial interpolation. To achieve this, only
specific optimal vector fields, which are typical for solutions of Optimal
Transport (OT) problems, need to be considered during FM loss minimization. In
this note, we show that considering only optimal vector fields can lead to OT
in another approach: Action Matching (AM). Unlike FM, which learns a vector
field for a manually chosen interpolation between given distributions, AM
learns the vector field that defines ODE for an entire given sequence of
distributions.

</details>


### [92] [Interpretable Model-Aware Counterfactual Explanations for Random Forest](https://arxiv.org/abs/2510.27397)
*Joshua S. Harvey,Guanchao Feng,Sai Anusha Meesala,Tina Zhao,Dhagash Mehta*

Main category: stat.ML

TL;DR: 该论文提出了一种基于随机森林的相似性学习的反事实案例搜索和解释方法，用于生成比Shapley值更稀疏和有用的解释。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在金融等受监管行业应用受限，因为现有解释方法（如Shapley值）很少能提供因果解释。反事实案例解释更直观和可操作，但如何找到合适的反事实案例并解释哪些特征最关键是一个开放挑战。

Method: 将反事实搜索和解释问题转化为相似性学习问题，利用随机森林预测模型本身学习到的表示。找到反事实后，通过计算从原始实例到达反事实需要跨越的随机森林分区来确定特征重要性。

Result: 在MNIST手写数字数据集和德国信用数据集上的实验表明，该方法生成的解释比Shapley值更稀疏和有用。

Conclusion: 基于随机森林相似性学习的反事实解释方法能够提供更直观和可操作的解释，优于传统的Shapley值方法。

Abstract: Despite their enormous predictive power, machine learning models are often
unsuitable for applications in regulated industries such as finance, due to
their limited capacity to provide explanations. While model-agnostic frameworks
such as Shapley values have proved to be convenient and popular, they rarely
align with the kinds of causal explanations that are typically sought after.
Counterfactual case-based explanations, where an individual is informed of
which circumstances would need to be different to cause a change in outcome,
may be more intuitive and actionable. However, finding appropriate
counterfactual cases is an open challenge, as is interpreting which features
are most critical for the change in outcome. Here, we pose the question of
counterfactual search and interpretation in terms of similarity learning,
exploiting the representation learned by the random forest predictive model
itself. Once a counterfactual is found, the feature importance of the
explanation is computed as a function of which random forest partitions are
crossed in order to reach it from the original instance. We demonstrate this
method on both the MNIST hand-drawn digit dataset and the German credit
dataset, finding that it generates explanations that are sparser and more
useful than Shapley values.

</details>


### [93] [Minimax-Optimal Two-Sample Test with Sliced Wasserstein](https://arxiv.org/abs/2510.27498)
*Binh Thuan Tran,Nicolas Schreuder*

Main category: stat.ML

TL;DR: 提出基于切片Wasserstein距离的置换检验方法，在保持计算效率的同时获得最优的统计保证，无需核调优即可在各种场景下表现一致良好


<details>
  <summary>Details</summary>
Motivation: 切片Wasserstein距离在统计保证和计算效率之间提供了良好平衡，但其在假设检验方面的理论基础仍有限，需要填补这一空白

Method: 提出基于置换的切片Wasserstein检验方法，分析投影数量与统计功效之间的权衡关系

Result: 该方法继承了置换原理的有限样本I类错误控制，达到最小最大分离率n^{-1/2}，与基于核的检验方法的最优保证相匹配

Conclusion: 切片Wasserstein检验结合了有限样本有效性和竞争性功效及可扩展性，与需要仔细核调优的基于核的检验不同，在所有考虑场景下表现一致良好

Abstract: We study the problem of nonparametric two-sample testing using the sliced
Wasserstein (SW) distance. While prior theoretical and empirical work indicates
that the SW distance offers a promising balance between strong statistical
guarantees and computational efficiency, its theoretical foundations for
hypothesis testing remain limited. We address this gap by proposing a
permutation-based SW test and analyzing its performance. The test inherits
finite-sample Type I error control from the permutation principle. Moreover, we
establish non-asymptotic power bounds and show that the procedure achieves the
minimax separation rate $n^{-1/2}$ over multinomial and bounded-support
alternatives, matching the optimal guarantees of kernel-based tests while
building on the geometric foundations of Wasserstein distances. Our analysis
further quantifies the trade-off between the number of projections and
statistical power. Finally, numerical experiments demonstrate that the test
combines finite-sample validity with competitive power and scalability, and --
unlike kernel-based tests, which require careful kernel tuning -- it performs
consistently well across all scenarios we consider.

</details>


### [94] [Optimal Convergence Analysis of DDPM for General Distributions](https://arxiv.org/abs/2510.27562)
*Yuchen Jiao,Yuchen Zhou,Gen Li*

Main category: stat.ML

TL;DR: 本文对DDPM采样器进行了精细的收敛性分析，在一般分布假设下建立了近乎最优的收敛速率，显著改进了现有理论结果。


<details>
  <summary>Details</summary>
Motivation: 尽管基于分数的扩散模型在生成高质量样本方面取得了显著经验成功，但对其收敛性质的理论理解仍然有限，特别是DDPM采样器的收敛分析不够完善。

Method: 引入了一个由常数L参数化的松弛平滑条件，该条件对于许多实际分布（如高斯混合模型）来说L值较小，并分析了DDPM采样器在准确分数估计下的收敛性能。

Result: 证明DDPM采样器在Kullback-Leibler散度下达到收敛速率O~(d min{d,L²}/T²)，当L < √d时显著优于已知的d²/T²速率，并通过建立匹配下界证明分析是紧的。

Conclusion: 该收敛分析揭示了DDPM和DDIM在维度d上的依赖关系相同，提出了为什么DDIM在经验上通常更快的有趣问题。

Abstract: Score-based diffusion models have achieved remarkable empirical success in
generating high-quality samples from target data distributions. Among them, the
Denoising Diffusion Probabilistic Model (DDPM) is one of the most widely used
samplers, generating samples via estimated score functions. Despite its
empirical success, a tight theoretical understanding of DDPM -- especially its
convergence properties -- remains limited.
  In this paper, we provide a refined convergence analysis of the DDPM sampler
and establish near-optimal convergence rates under general distributional
assumptions. Specifically, we introduce a relaxed smoothness condition
parameterized by a constant $L$, which is small for many practical
distributions (e.g., Gaussian mixture models). We prove that the DDPM sampler
with accurate score estimates achieves a convergence rate of
$$\widetilde{O}\left(\frac{d\min\{d,L^2\}}{T^2}\right)~\text{in
Kullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the
number of iterations, and $\widetilde{O}$ hides polylogarithmic factors in $T$.
This result substantially improves upon the best-known $d^2/T^2$ rate when $L <
\sqrt{d}$. By establishing a matching lower bound, we show that our convergence
analysis is tight for a wide array of target distributions. Moreover, it
reveals that DDPM and DDIM share the same dependence on $d$, raising an
interesting question of why DDIM often appears empirically faster.

</details>


### [95] [Bayesian Optimization on Networks](https://arxiv.org/abs/2510.27643)
*Wenwen Li,Daniel Sanz-Alonso,Ruiyi Yang*

Main category: stat.ML

TL;DR: 本文研究了在度量图网络上的优化问题，开发了基于Whittle-Matérn高斯过程先验的贝叶斯优化算法，适用于目标函数评估成本高或只能作为黑盒使用的情况。


<details>
  <summary>Details</summary>
Motivation: 应用场景中目标函数评估成本高或只能作为黑盒使用，需要开发高效的优化算法来指导查询点的获取。

Method: 采用通过度量图上的随机偏微分方程定义的Whittle-Matérn高斯过程先验模型，构建高斯过程代理模型，并利用有限元方法表示先验。

Result: 为充分平滑的目标函数建立了遗憾界，数值结果表明算法在合成度量图和电信网络上的贝叶斯反演中表现有效。

Conclusion: 提出的贝叶斯优化算法能够有效处理度量图网络上的优化问题，特别适用于目标函数评估昂贵或黑盒的情况。

Abstract: This paper studies optimization on networks modeled as metric graphs.
Motivated by applications where the objective function is expensive to evaluate
or only available as a black box, we develop Bayesian optimization algorithms
that sequentially update a Gaussian process surrogate model of the objective to
guide the acquisition of query points. To ensure that the surrogates are
tailored to the network's geometry, we adopt Whittle-Mat\'ern Gaussian process
prior models defined via stochastic partial differential equations on metric
graphs. In addition to establishing regret bounds for optimizing sufficiently
smooth objective functions, we analyze the practical case in which the
smoothness of the objective is unknown and the Whittle-Mat\'ern prior is
represented using finite elements. Numerical results demonstrate the
effectiveness of our algorithms for optimizing benchmark objective functions on
a synthetic metric graph and for Bayesian inversion via maximum a posteriori
estimation on a telecommunication network.

</details>
