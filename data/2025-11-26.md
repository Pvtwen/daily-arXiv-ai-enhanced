<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 17]
- [cs.LG](#cs.LG) [Total: 120]
- [stat.ML](#stat.ML) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Latent-space metrics for Complex-Valued VAE out-of-distribution detection under radar clutter](https://arxiv.org/abs/2511.19805)
*Y. A. Rouzoumka,E. Terreaux,C. Morisseau,J. -P. Ovarlez,C. Ren*

Main category: eess.SP

TL;DR: 研究复杂值变分自编码器(CVAE)在复杂雷达环境中的离群分布检测，提出了多种检测指标并与经典ANMF-Tyler检测器进行比较。


<details>
  <summary>Details</summary>
Motivation: 在复杂雷达环境中进行有效的离群分布检测，探索基于深度学习的检测方法在雷达信号处理中的应用。

Method: 使用复杂值变分自编码器(CVAE)，提出重建误差(CVAE-MSE)、潜在空间评分(马氏距离、KL散度)等多种检测指标，并与传统ANMF-Tyler检测器进行对比。

Result: 在合成和实验雷达数据上分析了所有检测器的性能，展示了每种检测器的优势和弱点。

Conclusion: 复杂值变分自编码器为雷达离群分布检测提供了有效的深度学习方法，不同检测指标在不同场景下各有优劣。

Abstract: We investigate complex-valued Variational AutoEncoders (CVAE) for radar Out-Of-Distribution (OOD) detection in complex radar environments. We proposed several detection metrics: the reconstruction error of CVAE (CVAE-MSE), the latent-based scores (Mahalanobis, Kullback-Leibler divergence (KLD)), and compared their performance against the classical ANMF-Tyler detector (ANMF-FP). The performance of all these detectors is analyzed on synthetic and experimental radar data, showing the advantages and the weaknesses of each detector.

</details>


### [2] [White-Box Modeling of V2X Link Performance Using Stabilized Symbolic Regression](https://arxiv.org/abs/2511.19809)
*Rahul Gulia,Feyisayo Favour Popoola,Ashish Sheikh*

Main category: eess.SP

TL;DR: 提出一种稳定的符号回归框架，用于车辆到一切无线网络中的块错误率建模，在保持高预测精度的同时提供可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法（如深度神经网络）虽然预测精度高，但缺乏可解释性且计算成本高，限制了在实时资源受限环境中的应用。

Method: 使用稳定的符号回归框架，从真实的V2X仿真数据中推导紧凑、可解析解释的块错误率预测表达式。

Result: 最终符号表达式仅包含158个节点，在测试集上达到R²=0.8684和MSE=2.08×10⁻²，优于传统固定形式回归，并与神经网络精度相当但完全可解释。

Conclusion: 所提出的稳定符号回归框架结合了预测性能、物理保真度和计算效率，为实时V2X通信系统设计提供了强大工具。

Abstract: Reliable modeling of block error rate in vehicle-to-everything wireless networks is critical for designing robust communication systems under dynamic mobility and diverse channel conditions. Traditional machine learning approaches, such as deep neural networks, achieve high predictive accuracy but lack interpretability and impose significant computational costs, limiting their applicability in real-time, resource-constrained environments. In this work, we propose a stabilized symbolic regression framework to derive compact, analytically interpretable expressions for block error rate prediction. Trained on realistic vehicle-to-everything simulation data, the symbolic regression framework for vehicle-to-everything model accurately captures nonlinear dependencies on key system parameters, including signal-to-noise ratio, relative velocity, modulation and coding schemes, number of demodulation reference signal symbols, and environmental factors (line of sight/non-line of sight). Our final symbolic expression comprises only 158 nodes, enabling ultra-fast inference suitable for embedded deployment. On the test set, the symbolic regression framework for vehicle-to-everything model achieves a coefficient of determination $R^2 = 0.8684$ and mean squared error $= 2.08 \times 10^{-2}$ in the original block error rate domain, outperforming conventional fixed-form regressions and offering comparable accuracy to neural networks while remaining fully interpretable. Overall, the proposed Stabilized Symbolic Regression Framework for V2X combines predictive performance, physical fidelity, and computational efficiency thus providing a powerful tool for real-time V2X communication system design, adaptive resource allocation, and rapid scenario evaluation.

</details>


### [3] [Parallel Delay-Doppler Estimation via Order-Reversed Two-Stage Prony Method](https://arxiv.org/abs/2511.19866)
*Yutaka Jitsumatsu,Liangchen Sun*

Main category: eess.SP

TL;DR: 提出基于Prony的并行两阶段方法用于OTFS系统中的时延-多普勒估计，通过并行执行时延优先和多普勒优先估计并融合结果，解决路径特性相似导致的模糊问题


<details>
  <summary>Details</summary>
Motivation: 解决OTFS系统中由于路径特性相似导致的时延-多普勒估计模糊问题，为V2V和ISAC等未来应用提供可靠解决方案

Method: 基于Prony的并行两阶段方法：并行执行时延优先估计和多普勒优先估计，然后融合两种估计结果

Result: 仿真结果表明该方法在各种条件下具有优越的准确性和鲁棒性

Conclusion: 该方法为V2V和ISAC等未来应用提供了有前景的解决方案

Abstract: This paper proposes a Prony-based parallel two-stage method for delay-Doppler estimation in OTFS systems. By performing delay-first and Doppler-first estimations in parallel and fusing the results, the method resolves ambiguities caused by similar path characteristics. The simulation results demonstrate the superior accuracy and robustness of the proposed method under various conditions. This method provides a promising solution for future applications such as Vehicle-to-Vehicle (V2V) and Integrated Sensing and Communication (ISAC).

</details>


### [4] [Digital Twin-Assisted High-Precision Massive MIMO Localization in Urban Canyons](https://arxiv.org/abs/2511.20453)
*Ziqin Zhou,Hui Chen,Gerhard Steinböck,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出了一种结合数字孪生和RANSAC算法的三阶段无线定位方法，有效利用NLOS路径信息提高城市峡谷环境下的定位精度


<details>
  <summary>Details</summary>
Motivation: 解决城市峡谷环境中无线定位面临的测量噪声和严重NLOS传播问题，降低对直接LOS路径的依赖并减少系统部署成本

Method: 三阶段算法：1) 利用数字孪生模型进行几何路径关联；2) 使用RANSAC算法识别可靠的LOS和单次反射NLOS路径，排除多次反射异常值；3) 对筛选出的内点集进行最终优化估计用户位置和时钟偏差

Result: 仿真验证表明，通过数字孪生将NLOS路径转化为有价值的几何信息，该方法实现了精确的定位，降低了对直接LOS的依赖，显著降低了系统部署成本

Conclusion: 该方法适用于实际部署，能够有效解决城市峡谷环境下的无线定位挑战

Abstract: High-precision wireless localization in urban canyons is challenged by noisy measurements and severe non-line-of-sight (NLOS) propagation. This paper proposes a robust three-stage algorithm synergizing a digital twin (DT) model with the random sample consensus (RANSAC) algorithm to overcome these limitations. The method leverages the DT for geometric path association and employs RANSAC to identify reliable line-of-sight (LOS) and single-bounce NLOS paths while rejecting multi-bounce outliers. A final optimization on the resulting inlier set estimates the user's position and clock bias. Simulations validate that by effectively turning NLOS paths into valuable geometric information via the DT, the approach enables accurate localization, reduces reliance on direct LOS, and significantly lowers system deployment costs, making it suitable for practical deployment.

</details>


### [5] [Joint Classification and Regression Deep Learning Model for Universal Phase-based Ranging in Multiple Environments](https://arxiv.org/abs/2511.19891)
*Pantelis Stefanakis,Ming Shen*

Main category: eess.SP

TL;DR: 本研究提出了一种新颖的2NN模型用于相位测距，该模型通过两个神经网络分别进行环境分类和距离预测，在多种环境中均优于传统非神经网络方法。


<details>
  <summary>Details</summary>
Motivation: 相位测距技术具有高精度和无需天线阵列的优势，但需要适应不同环境。本研究旨在探索神经网络模型在多种环境下的相位测距性能，并与传统方法进行比较。

Method: 提出2NN模型，包含两个神经网络：一个用于环境分类，另一个用于距离预测。在Openfield、Office和Near Buildings三种环境中进行测试，并与非神经网络方法比较。使用RMSE和最大预测误差作为评估指标。

Result: 2NN模型在所有环境中均表现最佳，获得了最低的平均RMSE和最大误差。即使考虑环境误分类的影响，过滤后的2NN模型仍能达到最低的RMSE和最大误差。

Conclusion: 神经网络模型在多样化环境中能够提供稳健、高精度的测距性能，优于非神经网络方法，证明了其作为通用相位测距解决方案的潜力。

Abstract: Phase-Based Ranging (PBR) offers several advantages for estimating distances between wirelessly connected devices, including high accuracy over large distances and the removal of the need for antenna arrays at each transceiver. This study investigates the use of Neural Network (NN)-based models for accurate PBR in three distinct environments: Openfield, Office, and Near Buildings, comparing their performance with established non-NN methods. A novel 2NN Model is proposed, integrating two neural networks: one to classify the environment and another to predict distances. Performance was evaluated over 20 trials for each method and dataset using root mean square error (RMSE) and maximum prediction error.
  Results show that the 2NN Model consistently outperformed other methods, frequently ranking among the top methods in minimizing both RMSE and maximum error. In addition, the 2NN Model achieved the best average RMSE and the lowest maximum error. To assess the effect of environment misclassification, filtered versions of the NN models were evaluated by omitting misclassified measurements prior to RMSE calculation. Although unsuitable for production use, the filtered models revealed that misclassifications in the 2NN Model had a significant impact. Its filtered variant achieved the lowest RMSE and maximum error across all datasets, and ranked first in the frequency of attaining the lowest maximum error over 20 trials.
  Overall, the findings show that NN models deliver robust, high-accuracy ranging across diverse environments, outperforming non-NN methods and reinforcing their potential as universal PBR solutions when trained on comprehensive distance datasets.

</details>


### [6] [AI/ML based Joint Source and Channel Coding for HARQ-ACK Payload](https://arxiv.org/abs/2511.19943)
*Akash Doshi,Pinar Sen,Kirill Ivanov,Wei Yang,June Namgoong,Runxin Wang,Rachel Wang,Taesang Yoo,Jing Jiang,Tingfang Ji*

Main category: eess.SP

TL;DR: 本文提出了一种基于Transformer的编码器和改进的解码器设计，用于处理非均匀分布的HARQ-ACK比特，在5G NR上行链路中实现显著的功率节省和覆盖增益。


<details>
  <summary>Details</summary>
Motivation: 从2G到5G的信道编码都假设物理层输入比特均匀分布，但上行链路中的HARQ-ACK比特本质上是非均匀分布的。对于此类信源，采用联合信源信道编码可以获得显著的性能提升。

Method: 使用新颖的"免费午餐"训练算法学习基于Transformer的编码器，并提出每码字功率整形来利用信源先验；开发了Neyman-Pearson测试的扩展版本，在解码器中实现NACK比特相对于ACK比特的不等错误保护。

Result: 在5G NR兼容的上行链路设置中，与NR基线相比，实现目标错误率所需的平均发射功率降低了3-6 dB，最大发射功率降低了2-3 dB。

Conclusion: 所提出的编码器和解码器设计在衰落信道下提供了显著的覆盖增益和功率节省，证明了深度学习方法在非均匀信源编码中的有效性。

Abstract: Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed. However, hybrid automatic repeat request acknowledgement (HARQ-ACK) bits transmitted in the uplink are inherently non-uniformly distributed. For such sources, significant performance gains could be obtained by employing joint source channel coding, aided by deep learning-based techniques. In this paper, we learn a transformer-based encoder using a novel "free-lunch" training algorithm and propose per-codeword power shaping to exploit the source prior at the encoder whilst being robust to small changes in the HARQ-ACK distribution. Furthermore, any HARQ-ACK decoder has to achieve a low negative acknowledgement (NACK) error rate to avoid radio link failures resulting from multiple NACK errors. We develop an extension of the Neyman-Pearson test to a coded bit system with multiple information bits to achieve Unequal Error Protection of NACK over ACK bits at the decoder. Finally, we apply the proposed encoder and decoder designs to a 5G New Radio (NR) compliant uplink setup under a fading channel, describing the optimal receiver design and a low complexity coherent approximation to it. Our results demonstrate 3-6 dB reduction in the average transmit power required to achieve the target error rates compared to the NR baseline, while also achieving a 2-3 dB reduction in the maximum transmit power, thus providing for significant coverage gains and power savings.

</details>


### [7] [Cross-Modal Semantic Communication for Heterogeneous Collaborative Perception](https://arxiv.org/abs/2511.20000)
*Mingyi Lu,Guowei Liu,Le Liang,Chongtao Guo,Hao Ye,Shi Jin*

Main category: eess.SP

TL;DR: 提出跨模态语义通信框架CMSC，解决自动驾驶车辆间传感器异构性问题，通过统一语义空间实现不同传感器数据的有效融合，在低信噪比环境下显著提升感知性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的协同感知系统面临车辆间传感器异构性的挑战，不同传感器的感知数据难以统一处理和有效融合，阻碍了实际部署。

Method: 提出跨模态语义通信框架，首先将不同传感器模态的异构感知特征转换到统一的标准化语义空间，然后在该空间内进行编码、传输和解码，实现无缝信息融合。

Result: 大量实验表明，CMSC在感知性能上显著优于现有方法，特别是在低信噪比环境下表现突出。

Conclusion: CMSC框架有效解决了自动驾驶车辆间传感器异构性问题，为协同感知系统的实际部署提供了可行方案。

Abstract: Collaborative perception, an emerging paradigm in autonomous driving, has been introduced to mitigate the limitations of single-vehicle systems, such as limited sensor range and occlusion. To improve the robustness of inter-vehicle data sharing, semantic communication has recently further been integrated into collaborative perception systems to enhance overall performance. However, practical deployment of such systems is challenged by the heterogeneity of sensors across different connected autonomous vehicles (CAVs). This diversity in perceptual data complicates the design of a unified communication framework and impedes the effective fusion of shared information. To address this challenge, we propose a novel cross-modal semantic communication (CMSC) framework to facilitate effective collaboration among CAVs with disparate sensor configurations. Specifically, the framework first transforms heterogeneous perceptual features from different sensor modalities into a unified and standardized semantic space. Subsequently, encoding, transmission, and decoding are performed within this semantic space, enabling seamless and effective information fusion. Extensive experiments demonstrate that CMSC achieves significantly stronger perception performance than existing methods, particularly in low signal-to-noise ratio (SNR) regimes.

</details>


### [8] [Redefining Radar Segmentation: Simultaneous Static-Moving Segmentation and Ego-Motion Estimation using Radar Point Clouds](https://arxiv.org/abs/2511.20003)
*Simin Zhu,Satish Ravindran,Alexander Yarovoy,Francesco Fioranelli*

Main category: eess.SP

TL;DR: 提出了一种基于神经网络的方法，能够同时从雷达点云中分割静态和动态物体，并估计移动平台的瞬时2D速度，无需复杂的中间信号处理步骤。


<details>
  <summary>Details</summary>
Motivation: 传统雷达分割研究主要关注学习不同运动物体的类别标签，但雷达与光学传感器的根本差异导致类别标签预测的可靠性不同。在汽车雷达感知任务中，确定物体是静态还是动态是大多数任务的先决条件。

Method: 使用多层感知机(MLPs)和循环神经网络(RNNs)等简单有效的构建块进行特征提取，直接从原始点云中提取双任务所需信息，无需云聚合、多普勒补偿、运动补偿等中间处理步骤。

Result: 在RadarScenes真实世界雷达数据集上的测试表明，该方法在双任务上表现良好，并在其他雷达感知任务中具有广泛的应用潜力。

Conclusion: 该方法首次在文献中实现了从雷达点云同时分割静态/动态物体和估计平台速度的双任务，证明了直接从原始点云提取所需信息的可行性，为雷达感知任务提供了新的解决方案。

Abstract: Conventional radar segmentation research has typically focused on learning category labels for different moving objects. Although fundamental differences between radar and optical sensors lead to differences in the reliability of predicting accurate and consistent category labels, a review of common radar perception tasks in automotive reveals that determining whether an object is moving or static is a prerequisite for most tasks. To fill this gap, this study proposes a neural network based solution that can simultaneously segment static and moving objects from radar point clouds. Furthermore, since the measured radial velocity of static objects is correlated with the motion of the radar, this approach can also estimate the instantaneous 2D velocity of the moving platform or vehicle (ego motion). However, despite performing dual tasks, the proposed method employs very simple yet effective building blocks for feature extraction: multi layer perceptrons (MLPs) and recurrent neural networks (RNNs). In addition to being the first of its kind in the literature, the proposed method also demonstrates the feasibility of extracting the information required for the dual task directly from unprocessed point clouds, without the need for cloud aggregation, Doppler compensation, motion compensation, or any other intermediate signal processing steps. To measure its performance, this study introduces a set of novel evaluation metrics and tests the proposed method using a challenging real world radar dataset, RadarScenes. The results show that the proposed method not only performs well on the dual tasks, but also has broad application potential in other radar perception tasks.

</details>


### [9] [Sparse MIMO-OFDM Channel Estimation via RKHS Regularization](https://arxiv.org/abs/2511.20082)
*James Delfeld,Gian Marti,Chris Dick*

Main category: eess.SP

TL;DR: 提出了一种基于RKHS正则化优化的MIMO-OFDM信道估计方法，利用延迟-波束空间的带稀疏性，通过FBS算法实现准线性计算复杂度，并通过深度展开扩展提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统线性信道估计方法（如LMMSE）在MIMO-OFDM系统中性能有限，需要利用无线信道在延迟-波束空间的带稀疏特性来提升估计精度。

Method: 在RKHS中构建正则化优化问题，通过表示定理转化为有限维问题，使用FBS算法求解，并引入深度展开扩展以加速收敛。

Result: 在SionnaRT生成的射线追踪信道上的实验表明，该方法在原始估计精度和下游性能方面均显著优于基于聚合信道统计的LMMSE方法。

Conclusion: 所提出的RKHS正则化优化方法能够有效利用信道稀疏性，在保持准线性计算复杂度的同时显著提升MIMO-OFDM系统的信道估计性能。

Abstract: We propose a method for channel estimation in multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) wireless communication systems. The method exploits the band-sparsity of wireless channels in the delay-beamspace domain by solving a regularized optimization problem in a reproducing kernel Hilbert space (RKHS). A suitable representer theorem allows us to transform the infinite-dimensional optimization problem into a finite-dimensional one, which we then approximate with a low-dimensional surrogate. We solve the resulting optimization problem using a forward-backward splitting (FBS)-based algorithm. By exploiting the problem's modulation structure, we achieve a computational complexity per iteration that is quasi-linear in the number of unknown variables. We also propose a data-driven deep-unfolding based extension to improve the performance at a reduced number of iterations. We evaluate our channel estimators on ray-traced channels generated with SionnaRT. The results show that our methods significantly outperform linear methods such as linear minimum mean squared error (LMMSE) channel estimation based on aggregate channel statistics, both in terms of raw estimation accuracy as well as in downstream performance.

</details>


### [10] [Joint Bit-Partitioning and Modulation Design for Digital AirComp](https://arxiv.org/abs/2511.20113)
*Xiaojing Yan,Carlo Fischione*

Main category: eess.SP

TL;DR: 该论文提出两种比特分割方法（均匀分割和重要性自适应分割）来改进ChannelComp框架，通过将输入比特序列分组映射到调制符号，在多时隙传输中实现数字空中计算，显著降低了计算误差。


<details>
  <summary>Details</summary>
Motivation: 为了降低数字空中计算中调制设计的复杂度并提高计算可靠性，需要开发有效的比特分割策略来优化多接入信道上的函数计算性能。

Method: 提出两种比特分割方法：1）均匀比特分割，通过最大最小优化设计调制，使用CCCP求解二阶锥规划子问题；2）重要性自适应比特分割，根据比特位置的重要性自适应分配比特，联合优化调制和分割，外层使用模拟退火更新分割，内层使用CCCP进行调制设计。

Result: 数值结果表明两种方法在噪声信道中都能提供鲁棒的计算，重要性自适应比特分割相比顺序调制空中计算最多可降低5 dB的计算误差，特别在乘积计算中表现优异。

Conclusion: 比特分割策略能有效提升数字空中计算的性能，重要性自适应比特分割方法在计算精度方面具有显著优势，为多接入信道上的函数计算提供了可靠的解决方案。

Abstract: For digital over-the-air computation, the ChannelComp framework has recently been proposed to design digital modulations to compute any arbitrary function over a multiple access channel. To reduce modulation design complexity while increasing computation reliability, this paper integrates a bit-partitioning procedure into ChannelComp. The key process is to partition the input bit sequence into several groups, map each group to a single modulation symbol and transmit the encoded symbol sequence across multiple time slots. With the objective to maximize a worst-case constellation distance, we develop two bit-partitioning methods. In uniform bit-partitioning, bits are evenly distributed across groups and modulation is designed via a max-min optimization, which is handled by a CCCP that solves a sequence of second-order cone programming subproblems. In importance-adaptive bit-partitioning (IABP), the bit allocation is adapted to the significance of individual bit positions, and the modulation and partitioning are jointly optimized. To keep the overall complexity manageable, simulated annealing is employed in the outer loop to update the partitioning, while a CCCP-based solver is used in the inner loop for modulation design. Numerical results show that both methods provide robust computation in noisy channels, and IABP achieves up to a 5 dB reduction in computation error compared to Sequential Modulation for AirComp, especially for product computation.

</details>


### [11] [Optimal Waveform Design for Continuous Aperture Array (CAPA)-aided ISAC Systems](https://arxiv.org/abs/2511.20203)
*Junjie Ye,Zhaolin Wang,Yuanwei Liu,Peichang Zhang,Lei Huang,Arumugam Nallanathan*

Main category: eess.SP

TL;DR: 提出了基于连续孔径阵列的集成感知与通信框架，通过优化波形设计实现多目标感知波束成形并抑制多用户干扰。


<details>
  <summary>Details</summary>
Motivation: 传统离散阵列在集成感知与通信中存在性能限制，连续孔径阵列能够提供更好的波束成形能力和性能增益。

Method: 基于格林函数推导CAPA方向性波束图，通过波数域优化获得参考感知波形，采用拉格朗日变换和变分法求解加权函数规划问题。

Result: 数值结果表明CAPA在感知精度和通信可靠性方面相比传统离散阵列设计具有显著性能提升。

Conclusion: 连续孔径阵列为集成感知与通信系统提供了有效的解决方案，能够同时优化感知波束匹配和通信干扰抑制。

Abstract: A novel continuous-aperture-array (CAPA)-aided integrated sensing and communication (ISAC) framework is proposed. Specifically, an optimal continuous ISAC waveform is designed to form a directive beampattern for multi-target sensing while suppressing the multi-user interference (MUI). To achieve the goal of optimal waveform design, the directional beampattern of CAPA is first derived based on Green's function, whereafter a reference sensing waveform is obtained through wavenumber-domain optimization. Based on the reference sensing waveform, a weighted functional programming on the tradeoff between sensing beampattern mismatch and MUI is formulated. To solve the resulting problem, an optimal CAPA-ISAC waveform structure is analytically derived using a Lagrangian-transformation and calculus-of-variations method, where the Lagrangian multiplier associated with the optimal waveform structure is determined via Bisection search. The obtained optimal waveform reveals that it is concurrently affected by the reference sensing waveform, the channel correlations and the channel-symbol correlations. Finally, numerical results validate the effectiveness of the proposed system and waveform design, demonstrating that CAPA can achieve significant performance gains against the ISAC designs based on conventional spatially discrete array in both sensing accuracy and communication reliability.

</details>


### [12] [Rectified Flow for Vision-Aided mmWave V2I Beam Prediction](https://arxiv.org/abs/2511.20265)
*Can Zheng,Jiguang He,Chung G. Kang,Guofa Cai,Chongwen Huang,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出基于整流流的流匹配框架，用于V2I链路的视觉辅助波束预测，通过ODE向量场学习连续潜在流，实现平滑波束轨迹和快速采样。


<details>
  <summary>Details</summary>
Motivation: 传统方法建模离散波束索引序列存在局限性，需要更平滑的波束轨迹和更快的采样速度。

Method: 使用基于ODE的向量场学习连续潜在流，引入终端流约束确保有限步积分下的全局一致性，稳定长期预测。

Result: 相比RNN和LSTM基线显著提升top-K准确率，接近基于大语言模型方法的性能，在GPU和CPU上分别实现10倍和10^4倍的推理加速。

Conclusion: 流匹配框架在视觉辅助波束预测中表现出色，兼具高准确率和显著加速效果。

Abstract: This paper proposes a flow matching (FM) framework based on rectified flow for vision-aided beam prediction in vehicle-to-infrastructure (V2I) links. Instead of modeling discrete beam index sequences, the method learns a continuous latent flow governed by an ordinary differential equation (ODE)-based vector field, enabling smooth beam trajectories and fast sampling. A terminal flow constraint enforces global consistency under finite-step integration, stabilizing long-term prediction. The resulting FM-based model significantly improves top-K accuracy over RNN and LSTM baselines, approaches the performance of large language model-based approaches, and achieves inference speedups on the order of 10 x and 10^4 x on identical GPU and CPU deployments, respectively.

</details>


### [13] [Log-Mu Fading Process: Second-Order Statistics for Diversity-Combining Techniques](https://arxiv.org/abs/2511.20298)
*Godfred Kumi Tenkorang,Michel Daoud Yacoub*

Main category: eess.SP

TL;DR: 本文推导了Log-mu衰落信道上分集合并技术的二阶统计特性，包括PSC的闭合表达式以及EGC和MRC的多维积分表达式，并通过仿真验证了理论结果的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究Log-mu衰落信道上的分集合并技术，因为Log-mu分布能够准确建模各种无线通信场景中的衰落特性，而现有的二阶统计特性分析还不够完善。

Method: 推导了PSC的闭合形式表达式，以及EGC和MRC的精确多维积分表达式，考虑了M个不平衡、独立且非同分布的Log-mu衰落信道，并通过蒙特卡洛仿真验证理论结果。

Result: 获得了各种分集合并技术的二阶统计特性表达式，蒙特卡洛仿真显示理论结果与仿真结果高度一致，验证了所提表达式的准确性。

Conclusion: 成功推导了Log-mu衰落信道上分集合并技术的二阶统计特性，为无线通信系统的性能评估提供了理论工具，仿真结果证实了理论分析的有效性。

Abstract: This paper derives second-order statistics for diversity-combining techniques over Log-mu fading channels. Closed-form expressions for the level crossing rate (LCR) and average fading duration (AFD) are derived for pure selection combining (PSC), while exact multidimensional integral expressions are obtained for equal gain combining (EGC) and maximal ratio combining (MRC). The analysis considers M unbalanced, independent, and non-identically distributed (i.n.i.d.) Log-mu fading channels. Monte Carlo simulations are conducted to validate the theoretical results, demonstrating excellent agreement and confirming the accuracy of the proposed expressions.

</details>


### [14] [Next-Generation MIMO Transceivers for Integrated Sensing and Communications: Unique Security Vulnerabilities and Solutions](https://arxiv.org/abs/2511.20309)
*Kawon Han,Christos Masouros,Taneli Riihonen,Moeness G. Amin*

Main category: eess.SP

TL;DR: 本文综述了MIMO ISAC收发器设计的最新进展，重点关注ISAC特有的安全威胁和防护措施，包括安全信令设计、干扰利用和对抗条件下的收发器优化。


<details>
  <summary>Details</summary>
Motivation: ISAC作为6G关键使能技术，在实现智能、可持续、连接无线网络的同时，引入了超越传统物理层安全的新型安全漏洞，如高功率传感传输可能增强窃听者能力，被动拦截ISAC回波可能泄露敏感信息。

Method: 从发射机视角、接收机架构和全双工实现三个方面分析MIMO ISAC收发器设计，研究在ISAC特有安全威胁下的收发器设计方法。

Result: 提出了针对ISAC安全威胁的防护措施，包括安全信令设计、干扰利用和对抗条件下的收发器优化等新兴对策。

Conclusion: 讨论了在下一代无线网络中开发安全ISAC系统面临的挑战和研究机遇，强调了安全收发器设计在ISAC系统中的重要性。

Abstract: Integrated sensing and communications (ISAC), which is recognized as a key enabler for sixth generation (6G), has brought new opportunities for intelligent, sustainable, and connected wireless networks. Multiple-input multiple-output (MIMO) transceiver technology lies at the core of this paradigm, providing the degrees of freedom required for simultaneous data transmission and accurate radar sensing. The tight integration of sensing and communication introduces unique security vulnerabilities that extend beyond conventional physical-layer security (PLS). In particular, high-power transmissions directed at sensing targets may empower adversarial eavesdroppers, whereas passive interception of ISAC echoes can reveal sensitive information such as target locations and mobility patterns. This article presents an overview of recent advances in MIMO ISAC transceiver design, considering transmitter perspectives, receiver architectures, and full-duplex implementations. We examine MIMO transceiver designs under unique security threats specific to ISAC and highlight emerging countermeasures, including secure signaling design, interference exploitation, and transceiver optimization under adversarial conditions. Finally, we discuss challenges and research opportunities for developing secure ISAC systems in next-generation wireless networks.

</details>


### [15] [Bridging the Educational Divide: A Delay-Tolerant Networking Approach for Equitable Digital Learning in Rural Areas](https://arxiv.org/abs/2511.20334)
*Salah Abdeljabar,Mohamed-Slim Alouini*

Main category: eess.SP

TL;DR: 提出基于延迟容忍网络(DTN)的数字学习平台框架，为网络连接有限的农村地区提供教育机会，利用现有交通基础设施实现可持续解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决农村地区因互联网连接有限或不存在而导致的教育不平等问题，促进教育公平和数字包容。

Method: 采用延迟容忍网络(DTN)技术，利用现有交通基础设施构建数字学习平台，不依赖持续互联网连接。

Result: 在大学校园测试的原型证明了DTN用于教育传递的可行性。

Conclusion: 该框架通过解决数字鸿沟问题，与全球包容性教育和可持续发展目标保持一致，为服务不足社区提供可持续的教育解决方案。

Abstract: Access to quality education remains unequal, particularly in rural areas where Internet connectivity is limited or nonexistent. This paper introduces a framework for a digital learning platform that uses Delay Tolerant Networking (DTN) to extend educational opportunities to underserved communities. Unlike conventional models that rely on continuous Internet access, DTN offers an affordable and sustainable solution by leveraging existing transportation infrastructure. Beyond its technical contributions, the framework addresses ethical imperatives by promoting educational equity and digital inclusion. We present a prototype tested on a university campus, demonstrating the feasibility of DTN for educational delivery. By addressing the digital divide, this framework aligns with global goals of inclusive education and sustainable development.

</details>


### [16] [Time-Domain Linear Model-based Framework for Passive Acoustic Mapping of Cavitation Activity](https://arxiv.org/abs/2511.20551)
*Tatiana Gelvez-Barrera,Barbara Nicolas,Denis Kouamé,Bruno Gilles,Adrian Basarab*

Main category: eess.SP

TL;DR: 提出了一种基于线性模型的时域波束形成框架，用于被动声学映射，通过时空正则化技术提高空化活动映射的质量和数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统波束形成方法在轴向分辨率上存在限制，频域方法需要长信号进行准确估计，时域方法通常空间分辨率较低。

Method: 构建线性前向模型，将离散化的时空空化活动分布与探头记录的时域信号相关联，明确考虑采集几何决定的飞行时间延迟，并使用时空正则化技术进行模型反演。

Result: 实验结果表明，该框架仅使用频域方法通常所需数据的20%，就能实现增强或具有竞争力的空化图质量。

Conclusion: 该方法在数据效率和时空正则化灵活性方面显著优于现有技术，能够适应不同的被动空化场景。

Abstract: Passive acoustic mapping enables the spatial mapping and temporal monitoring of cavitation activity, playing a crucial role in therapeutic ultrasound applications. Most conventional beamforming methods, whether implemented in the time or frequency domains, suffer from limited axial resolution due to the absence of a reference emission onset time. While frequency-domain methods, the most efficient of which are based on the cross-spectral matrix, require long signals for accurate estimation, time-domain methods typically achieve lower spatial resolution. To address these limitations, we propose a linear model-based beamforming framework fully formulated in the time domain. The linear forward model relates a discretized spatiotemporal distribution of cavitation activity to the temporal signals recorded by a probe, explicitly accounting for time-of-flight delays dictated by the acquisition geometry. This model is then inverted using regularization techniques that exploit prior knowledge of cavitation activity in both spatial and temporal domains. Experimental results show that the proposed framework achieves enhanced or competitive cavitation map quality while using only 20\% of the data typically required by frequency-domain methods. This highlights the substantial gain in data efficiency and the flexibility of our spatiotemporal regularization to adapt to diverse passive cavitation scenarios, outperforming state-of-the-art techniques.

</details>


### [17] [Near-Field Multipath MIMO Channels: Modeling Reflectors and Exploiting NLOS Paths](https://arxiv.org/abs/2511.20572)
*Mohamadreza Delbari,George C. Alexandropoulos,Robert Schober,H. Vincent Poor,Vahid Jamali*

Main category: eess.SP

TL;DR: 本文提出了一个广义的统计近场MIMO信道模型，将点散射框架扩展到考虑大表面的非完美反射，并证明在近场多用户场景中仅依赖LOS链路可能无法实现复用增益，必须利用NLOS链路。


<details>
  <summary>Details</summary>
Motivation: 现有近场通信研究主要关注LOS链路，但NLOS分量在厘米波和毫米波频率下通常不可忽略，且对于实现MIMO系统的复用增益至关重要。现有基于点散射假设的NLOS信道模型不适用于墙壁、天花板等大反射体。

Method: 开发了一个广义的统计近场MIMO信道模型，扩展了点散射框架以考虑大表面的非完美反射，并利用该模型分析反射体物理特性对近场MIMO信道的影响。

Result: 仿真结果验证了所提模型的准确性，并显示在许多实际场景中NLOS分量的贡献不可忽略，必须在系统设计中仔细考虑。

Conclusion: 即使在近场区域，仅依赖LOS近场链路可能不足以实现复用增益，因此利用NLOS链路变得至关重要。所提出的信道模型能够准确捕捉大反射体对近场MIMO信道的影响。

Abstract: Near-field (NF) communications is receiving renewed interest in the context of multiple-input multiple-output (MIMO) systems involving large physical apertures with respect to the signal wavelength. While line-of-sight (LOS) links are typically expected to dominate in NF scenarios, the impact of non-LOS (NLOS) components at both in centimeter- and millimeter-wave frequencies may be in general non-negligible. Moreover, although weaker than the LOS path, NLOS links may be essential for achieving multiplexing gains in MIMO systems. The commonly used NF channel models for NLOS links in the literature are based on the point scattering assumption, which is not valid for large reflectors such as walls, ceilings, and the ground. In this paper, we develop a generalized statistical NF MIMO channel model that extends the widely adopted point scattering framework to account for imperfect reflections from large surfaces. This model is then leveraged to investigate how the physical characteristics of these reflectors influence the resulting NF MIMO channel. In addition, using the proposed channel model, we analytically demonstrate for a multi-user scenario that, even when users are located within the NF regime, relying solely on LOS NF links may be insufficient to achieve multiplexing gains, thus exploiting NLOS links becomes essential. Our simulation results validate the accuracy of the proposed model and show that, in many practical settings, the contribution of NLOS components is non-negligible and must be carefully accounted for in the system design.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [iRadioDiff: Physics-Informed Diffusion Model for Indoor Radio Map Construction and Localization](https://arxiv.org/abs/2511.20015)
*Xiucheng Wang,Tingwei Yuan,Yang Cao,Nan Cheng,Ruijin Sun,Weihua Zhuang*

Main category: cs.LG

TL;DR: iRadioDiff是一个基于扩散模型的室内无线电地图构建框架，通过融合物理先验知识（如衍射点、传输边界、视距轮廓）来生成高保真度的电磁场分布图，解决了传统电磁求解器延迟高和学习方法依赖稀疏测量的问题。


<details>
  <summary>Details</summary>
Motivation: 构建高保真室内无线电地图面临挑战，传统电磁求解器计算延迟高，而学习方法通常依赖稀疏测量或假设材料均匀，这与室内环境的异质性和多径丰富特性不符。

Method: 提出iRadioDiff框架，基于扩散模型，条件输入包括接入点位置、物理提示（材料反射和传输系数），并融入多径关键先验（衍射点、强传输边界、视距轮廓）来指导生成过程。

Result: 实验表明iRadioDiff在室内无线电地图构建和基于接收信号强度的室内定位方面达到最先进性能，能够有效泛化到不同布局和材料配置。

Conclusion: iRadioDiff通过物理引导的扩散模型成功解决了室内无线电地图构建的挑战，实现了准确的非平稳场不连续性建模和物理一致的无线电地图高效构建。

Abstract: Radio maps (RMs) serve as environment-aware electromagnetic (EM) representations that connect scenario geometry and material properties to the spatial distribution of signal strength, enabling localization without costly in-situ measurements. However, constructing high-fidelity indoor RMs remains challenging due to the prohibitive latency of EM solvers and the limitations of learning-based methods, which often rely on sparse measurements or assumptions of homogeneous material, which are misaligned with the heterogeneous and multipath-rich nature of indoor environments. To overcome these challenges, we propose iRadioDiff, a sampling-free diffusion-based framework for indoor RM construction. iRadioDiff is conditioned on access point (AP) positions, and physics-informed prompt encoded by material reflection and transmission coefficients. It further incorporates multipath-critical priors, including diffraction points, strong transmission boundaries, and line-of-sight (LoS) contours, to guide the generative process via conditional channels and boundary-weighted objectives. This design enables accurate modeling of nonstationary field discontinuities and efficient construction of physically consistent RMs. Experiments demonstrate that iRadioDiff achieves state-of-the-art performance in indoor RM construction and received signal strength based indoor localization, which offers effective generalization across layouts and material configurations. Code is available at https://github.com/UNIC-Lab/iRadioDiff.

</details>


### [19] [Hidden markov model to predict tourists visited place](https://arxiv.org/abs/2511.19465)
*Theo Demessance,Chongke Bi,Sonia Djebali,Guillaume Guerard*

Main category: cs.LG

TL;DR: 基于社交网络数据，使用机器学习语法推断算法预测游客移动行为，并针对大数据环境进行了算法适配。


<details>
  <summary>Details</summary>
Motivation: 社交网络上的游客数字痕迹为分析旅游行为提供了大量数据，预测游客移动对旅游营销和决策支持至关重要。

Method: 采用机器学习语法推断算法构建隐马尔可夫模型，表示游客群体的移动模式，模型具有灵活性和可编辑性。

Result: 以巴黎为例验证了方法的有效性，能够学习并预测游客的移动行为。

Conclusion: 提出的方法能够有效利用社交网络大数据预测游客移动，为旅游营销提供决策支持。

Abstract: Nowadays, social networks are becoming a popular way of analyzing tourist behavior, thanks to the digital traces left by travelers during their stays on these networks. The massive amount of data generated; by the propensity of tourists to share comments and photos during their trip; makes it possible to model their journeys and analyze their behavior. Predicting the next movement of tourists plays a key role in tourism marketing to understand demand and improve decision support. In this paper, we propose a method to understand and to learn tourists' movements based on social network data analysis to predict future movements. The method relies on a machine learning grammatical inference algorithm. A major contribution in this paper is to adapt the grammatical inference algorithm to the context of big data. Our method produces a hidden Markov model representing the movements of a group of tourists. The hidden Markov model is flexible and editable with new data. The capital city of France, Paris is selected to demonstrate the efficiency of the proposed methodology.

</details>


### [20] [Communication-Efficient Learning for Satellite Constellations](https://arxiv.org/abs/2511.20220)
*Ruxandra-Stefania Tudose,Moritz H. W. Grüss,Grace Ra Kim,Karl H. Johansson,Nicola Bastianello*

Main category: cs.LG

TL;DR: 提出了一种用于卫星星座联邦学习的新型通信高效算法，通过本地训练、压缩和误差反馈机制减少与地面站的通信次数和大小，在真实空间场景中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星星座的普及，需要解决在这些卫星上执行学习任务的问题，特别是采用联邦学习方法，其中卫星收集并本地处理数据，地面站聚合本地模型。

Method: 设计了一种通信高效的联邦学习算法，结合本地训练减少通信次数、压缩减少通信大小，并提出了增强精度的误差反馈机制，该机制还可作为算法无关的方案更广泛应用。

Result: 分析了所提出算法的收敛性，并在真实空间场景中与现有技术进行比较，展示了优越的性能表现。

Conclusion: 提出的通信高效联邦学习算法在卫星星座环境中有效减少了通信开销，同时保持了模型准确性，误差反馈机制具有更广泛的适用性。

Abstract: Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.

</details>


### [21] [Quantifying Modality Contributions via Disentangling Multimodal Representations](https://arxiv.org/abs/2511.19470)
*Padegal Amit,Omkar Mahesh Kashyap,Namitha Rayasam,Nidhi Shekhar,Surabhi Narayan*

Main category: cs.LG

TL;DR: 提出了基于部分信息分解的框架，量化多模态模型中各模态的贡献，区分独特、冗余和协同信息，无需重新训练即可分析内部表征。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基于准确性的评估，混淆了模态贡献的本质，无法区分模态本身的信息价值还是与其他模态交互产生的价值，特别是在跨注意力架构中。

Method: 基于部分信息分解框架，开发了基于迭代比例拟合程序的算法，分解内部嵌入中的预测信息为独特、冗余和协同分量，实现可扩展的仅推理分析。

Result: 提供了原则性的表征层面多模态行为视图，比基于结果的指标提供更清晰和可解释的见解。

Conclusion: 该框架能够更准确地量化模态贡献，区分不同信息类型，为多模态模型分析提供了新的视角。

Abstract: Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.

</details>


### [22] [PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer](https://arxiv.org/abs/2511.19472)
*Ruogu Ding,Xin Ning,Ulf Schlichtmann,Weikang Qian*

Main category: cs.LG

TL;DR: PrefixGPT是一个基于GPT的生成式预训练Transformer模型，能够从零开始直接生成优化的前缀加法器，通过将加法器拓扑表示为二维坐标序列并应用合法性掩码，确保所有设计在构建时都是有效的。


<details>
  <summary>Details</summary>
Motivation: 设计优化的前缀加法器具有挑战性，因为存在严格的设计规则和指数级大的设计空间。现有方法难以高效探索这一复杂空间并找到最优设计。

Method: 使用自定义的解码器专用Transformer架构，首先在随机合成的有效前缀加法器语料库上进行预训练以学习设计规则，然后进行微调以在设计空间中导航优化设计质量。

Result: 与现有工作相比，PrefixGPT不仅找到了一个面积延迟积(ADP)提高7.7%的新最优设计，而且表现出优越的探索质量，将平均ADP降低了高达79.1%。

Conclusion: 这表明GPT风格模型具有首先掌握复杂硬件设计原理，然后将其应用于更高效设计优化的潜力。

Abstract: Prefix adders are widely used in compute-intensive applications for their high speed. However, designing optimized prefix adders is challenging due to strict design rules and an exponentially large design space. We introduce PrefixGPT, a generative pre-trained Transformer (GPT) that directly generates optimized prefix adders from scratch. Our approach represents an adder's topology as a two-dimensional coordinate sequence and applies a legality mask during generation, ensuring every design is valid by construction. PrefixGPT features a customized decoder-only Transformer architecture. The model is first pre-trained on a corpus of randomly synthesized valid prefix adders to learn design rules and then fine-tuned to navigate the design space for optimized design quality. Compared with existing works, PrefixGPT not only finds a new optimal design with a 7.7% improved area-delay product (ADP) but exhibits superior exploration quality, lowering the average ADP by up to 79.1%. This demonstrates the potential of GPT-style models to first master complex hardware design principles and then apply them for more efficient design optimization.

</details>


### [23] [The Generalized Proximity Forest](https://arxiv.org/abs/2511.19487)
*Ben Shaw,Adam Rustad,Sofia Pelagalli Maia,Jake S. Rhodes,Kevin R. Moon*

Main category: cs.LG

TL;DR: 本文提出了广义邻近森林模型，将随机森林邻近性扩展到所有基于距离的监督机器学习场景，并引入了回归任务的变体，以及作为元学习框架用于监督插补。


<details>
  <summary>Details</summary>
Motivation: 随机森林邻近性在多种监督学习任务中很有用，但其效果依赖于RF模型本身的成功，而RF并非在所有场景下都是理想模型。需要将RF邻近性扩展到更广泛的基于距离的监督学习场景。

Method: 提出了广义邻近森林模型，将RF邻近性扩展到所有基于距离的监督机器学习场景；引入了PF模型的回归变体；将广义PF模型作为元学习框架，为任何预训练分类器提供监督插补能力。

Result: 实验证明，广义PF模型相比随机森林模型和k近邻模型具有独特优势。

Conclusion: 广义邻近森林模型成功扩展了RF邻近性的应用范围，为基于距离的监督学习提供了更灵活和强大的工具，特别是在元学习框架下的监督插补应用方面表现出色。

Abstract: Recent work has demonstrated the utility of Random Forest (RF) proximities for various supervised machine learning tasks, including outlier detection, missing data imputation, and visualization. However, the utility of the RF proximities depends upon the success of the RF model, which itself is not the ideal model in all contexts. RF proximities have recently been extended to time series by means of the distance-based Proximity Forest (PF) model, among others, affording time series analysis with the benefits of RF proximities. In this work, we introduce the generalized PF model, thereby extending RF proximities to all contexts in which supervised distance-based machine learning can occur. Additionally, we introduce a variant of the PF model for regression tasks. We also introduce the notion of using the generalized PF model as a meta-learning framework, extending supervised imputation capability to any pre-trained classifier. We experimentally demonstrate the unique advantages of the generalized PF model compared with both the RF model and the $k$-nearest neighbors model.

</details>


### [24] [Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition](https://arxiv.org/abs/2511.20612)
*Yujin Kim,Sarah Dean*

Main category: cs.LG

TL;DR: 提出了Stochastic NODE-DMD方法，这是DMD的概率扩展，能够建模连续时间非线性动态，同时保持可解释性，支持任意坐标的连续时空重建和预测不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统（如风场和洋流）动态复杂难以建模，传统DMD方法受限于稀疏/噪声观测、线性近似依赖以及缺乏不确定性量化。

Method: 基于DMD的概率扩展，结合神经ODE建模连续时间非线性动态，实现连续时空重建和不确定性量化。

Result: 在四个基准测试中，仅使用10%观测密度时重建精度超过基线，能够恢复动态结构并与真实模式对齐，在多实现数据集上学习校准的潜在动态分布。

Conclusion: 该方法在稀疏观测条件下仍能准确重建动态系统，提供有意义的不确定性量化，并保持集合变异性而非跨机制平均。

Abstract: Many consequential real-world systems, like wind fields and ocean currents, are dynamic and hard to model. Learning their governing dynamics remains a central challenge in scientific machine learning. Dynamic Mode Decomposition (DMD) provides a simple, data-driven approximation, but practical use is limited by sparse/noisy observations from continuous fields, reliance on linear approximations, and the lack of principled uncertainty quantification. To address these issues, we introduce Stochastic NODE-DMD, a probabilistic extension of DMD that models continuous-time, nonlinear dynamics while remaining interpretable. Our approach enables continuous spatiotemporal reconstruction at arbitrary coordinates and quantifies predictive uncertainty. Across four benchmarks, a synthetic setting and three physics-based flows, it surpasses a baseline in reconstruction accuracy when trained from only 10% observation density. It further recovers the dynamical structure by aligning learned modes and continuous-time eigenvalues with ground truth. Finally, on datasets with multiple realizations, our method learns a calibrated distribution over latent dynamics that preserves ensemble variability rather than averaging across regimes. Our code is available at: https://github.com/sedan-group/Stochastic-NODE-DMD

</details>


### [25] [WavefrontDiffusion: Dynamic Decoding Schedule or Improved Reasoning](https://arxiv.org/abs/2511.19473)
*Haojin Yang,Rui Hu,Zequn Sun,Rui Zhou,Yujun Cai,Yiwei Wang*

Main category: cs.LG

TL;DR: WavefrontDiffusion是一种动态解码方法，通过从已确定位置向外扩展活动标记波前来改进扩散语言模型的去噪策略，在推理和代码生成任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 主流去噪策略存在缺陷：标准扩散会导致过早结束序列，块扩散会破坏语义连贯性。需要一种能保持语义结构自然流动的自适应方法。

Method: 提出WavefrontDiffusion，动态扩展活动标记波前，从已确定位置向外传播，保持计算成本与基于块的方法相当。

Result: 在四个推理和代码生成基准测试中达到最先进性能，产生具有更高语义保真度的输出。

Conclusion: 自适应调度对于实现更连贯和高效的生成具有重要价值，WavefrontDiffusion展示了这一优势。

Abstract: Diffusion Language Models (DLMs) have shown strong potential for text generation and are becoming a competitive alternative to autoregressive models. The denoising strategy plays an important role in determining the quality of their outputs. Mainstream denoising strategies include Standard Diffusion and BlockDiffusion. Standard Diffusion performs global denoising without restricting the update range, often finalizing incomplete context and causing premature end-of-sequence predictions. BlockDiffusion updates fixed-size blocks in a preset order, but its rigid structure can break apart coherent semantic units and disrupt reasoning. We present WavefrontDiffusion, a dynamic decoding approach that expands a wavefront of active tokens outward from finalized positions. This adaptive process follows the natural flow of semantic structure while keeping computational cost equal to block-based methods. Across four benchmarks in reasoning and code generation, WavefrontDiffusion achieves state-of-the-art performance while producing outputs with higher semantic fidelity, showing the value of adaptive scheduling for more coherent and efficient generation.

</details>


### [26] [RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression](https://arxiv.org/abs/2511.19493)
*Chris Kuchar*

Main category: cs.LG

TL;DR: RFX v1.0 是一个生产就绪的随机森林Python实现，通过QLORA压缩和TriBlock存储等技术解决了邻近矩阵内存瓶颈，使随机森林分析能够处理超过20万样本的大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林分析受限于邻近矩阵的内存瓶颈，只能处理约6万样本的数据集，这限制了随机森林在大规模数据分析中的应用。

Method: 提出了四种解决方案：(1) QLORA压缩用于GPU邻近矩阵，实现12,500倍压缩；(2) CPU TriBlock邻近矩阵，结合上三角存储和块稀疏阈值；(3) SM感知GPU批处理大小；(4) GPU加速3D MDS可视化。

Result: QLORA压缩将100k样本的邻近矩阵内存从80GB减少到6.4MB，保持99%几何结构；CPU TriBlock实现2.7倍无损内存减少；GPU比CPU在500+树时实现1.4倍加速。

Conclusion: RFX v1.0消除了邻近矩阵内存瓶颈，使基于邻近的随机森林分析能够处理比以往可行数据集大几个数量级的数据集。

Abstract: RFX (Random Forests X), where X stands for compression or quantization, presents a production-ready implementation of Breiman and Cutler's Random Forest classification methodology in Python. RFX v1.0 provides complete classification: out-of-bag error estimation, overall and local importance measures, proximity matrices with QLORA compression, case-wise analysis, and interactive visualization (rfviz)--all with CPU and GPU acceleration. Regression, unsupervised learning, CLIQUE importance, and RF-GAP proximity are planned for v2.0.
  This work introduces four solutions addressing the proximity matrix memory bottleneck limiting Random Forest analysis to ~60,000 samples: (1) QLORA (Quantized Low-Rank Adaptation) compression for GPU proximity matrices, reducing memory from 80GB to 6.4MB for 100k samples (12,500x compression with INT8 quantization) while maintaining 99% geometric structure preservation, (2) CPU TriBlock proximity--combining upper-triangle storage with block-sparse thresholding--achieving 2.7x memory reduction with lossless quality, (3) SM-aware GPU batch sizing achieving 95% GPU utilization, and (4) GPU-accelerated 3D MDS visualization computing embeddings directly from low-rank factors using power iteration.
  Validation across four implementation modes (GPU/CPU x case-wise/non-case-wise) demonstrates correct implementation. GPU achieves 1.4x speedup over CPU for overall importance with 500+ trees. Proximity computation scales from 1,000 to 200,000+ samples (requiring GPU QLORA), with CPU TriBlock filling the gap for medium-scale datasets (10K-50K samples). RFX v1.0 eliminates the proximity memory bottleneck, enabling proximity-based Random Forest analysis on datasets orders of magnitude larger than previously feasible. Open-source production-ready classification following Breiman and Cutler's original methodology.

</details>


### [27] [Exploiting the Experts: Unauthorized Compression in MoE-LLMs](https://arxiv.org/abs/2511.19480)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.LG

TL;DR: 本文系统研究了MoE-LLMs在任务特定使用下的可修剪性，揭示了知识损失与恢复的权衡，并提出了防御策略以防止未经授权的模型压缩和微调。


<details>
  <summary>Details</summary>
Motivation: MoE架构的模块化结构引入了独特的安全漏洞：攻击者可以通过修剪专家并廉价微调剩余部分来绕过许可和安全约束，压缩或重新利用模型。

Method: 开发了专家归因框架识别对特定任务最关键的专家子集，评估修剪和重新对齐这些专家的性能权衡，使用主动学习驱动的微调方法。

Result: 研究发现存在关键的知识损失-恢复权衡：虽然可以隔离某些专家来保持任务准确性，但如果没有针对性的重新对齐，会出现显著性能下降。

Conclusion: 通过将专家修剪定位为威胁向量和防御目标，这项工作突出了MoE模块化的双重用途性质，并为MoE-LLMs的安全专业化提供了首个系统评估框架。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.

</details>


### [28] [Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma](https://arxiv.org/abs/2511.19504)
*Subramanyam Sahoo,Aman Chadha,Vinija Jain,Divya Chaudhary*

Main category: cs.LG

TL;DR: 本文提出了对齐三难困境：RLHF系统无法同时实现跨多样人类价值观的代表性、多项式计算复杂度、以及对对抗性扰动和分布偏移的鲁棒性。通过复杂性理论分析证明，实现全球规模的代表性和鲁棒性需要超多项式计算量。


<details>
  <summary>Details</summary>
Motivation: RLHF在实践中的矛盾：提高安全性会降低公平性，扩展到多样化群体计算不可行，增强鲁棒性会放大多数偏见。需要形式化这些根本性权衡。

Method: 通过整合统计学习理论和鲁棒优化的复杂性理论分析，证明在上下文维度上实现代表性和鲁棒性需要Ω(2^{d_context})操作。

Result: 当前RLHF实现通过牺牲代表性来解决三难困境，仅从同质标注者池收集10^3-10^4样本，而真实全球代表性需要10^7-10^8样本。

Conclusion: 框架统一解释了RLHF的病理现象，如偏好崩溃、谄媚和系统性偏见放大，并提出了通过战略性地放宽对齐要求来导航这些根本权衡的具体方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.

</details>


### [29] [Quality analysis and evaluation prediction of RAG retrieval based on machine learning algorithms](https://arxiv.org/abs/2511.19481)
*Ruoxin Zhang,Zhizhao Wen,Chao Wang,Chenchen Tang,Puyang Xu,Yifan Jiang*

Main category: cs.LG

TL;DR: 提出基于特征工程和粒子群优化的XGBoost回归模型，用于提升检索增强生成系统中检索模块的质量，从而改善生成内容的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理表格特征方面存在性能瓶颈，且检索结果的相关性和质量直接影响生成内容的准确性，需要优化检索质量来提升RAG系统效果。

Method: 采用XGBoost机器学习回归模型，结合特征工程和粒子群优化算法，并与决策树、AdaBoost等模型进行比较。

Result: VMD PSO BiLSTM模型在所有评估指标上表现最优，MSE、RMSE、MAE和MAPE显著降低，R2值更高，表明预测精度、稳定性和数据解释能力更突出。

Conclusion: 该成果为优化检索质量和提升RAG系统生成效果提供了有效路径，对促进相关技术实施应用具有重要价值。

Abstract: With the rapid evolution of large language models, retrieval enhanced generation technology has been widely used due to its ability to integrate external knowledge to improve output accuracy. However, the performance of the system is highly dependent on the quality of the retrieval module. If the retrieval results have low relevance to user needs or contain noisy information, it will directly lead to distortion of the generated content. In response to the performance bottleneck of existing models in processing tabular features, this paper proposes an XGBoost machine learning regression model based on feature engineering and particle swarm optimization. Correlation analysis shows that answer_quality is positively correlated with doc_delevance by 0.66, indicating that document relevance has a significant positive effect on answer quality, and improving document relevance may enhance answer quality; The strong negative correlations between semantic similarity, redundancy, and diversity were -0.89 and -0.88, respectively, indicating a tradeoff between semantic similarity, redundancy, and diversity. In other words, as the former two increased, diversity significantly decreased. The experimental results comparing decision trees, AdaBoost, etc. show that the VMD PSO BiLSTM model is superior in all evaluation indicators, with significantly lower MSE, RMSE, MAE, and MAPE compared to the comparison model. The R2 value is higher, indicating that its prediction accuracy, stability, and data interpretation ability are more outstanding. This achievement provides an effective path for optimizing the retrieval quality and improving the generation effect of RAG system, and has important value in promoting the implementation and application of related technologies.

</details>


### [30] [Shortcut Invariance: Targeted Jacobian Regularization in Disentangled Latent Space](https://arxiv.org/abs/2511.19525)
*Shivam Pal,Sakshi Varshney,Piyush Rai*

Main category: cs.LG

TL;DR: 提出一种简单有效的训练方法，通过功能不变性而非表示不变性来解决深度神经网络中的捷径学习问题，在解耦的潜在空间中识别并抑制捷径特征，实现最先进的分布外泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易学习训练数据中的捷径相关性，导致分布外泛化严重失败。现有方法学习鲁棒表示通常复杂、脆弱且难以扩展，需要不同的解决方案。

Method: 在解耦的潜在空间中，通过强标签相关性识别候选捷径特征，然后在训练期间注入有针对性的各向异性潜在噪声，使分类器对这些特征不敏感，相当于目标雅可比正则化。

Result: 在已建立的捷径学习基准测试中实现了最先进的分布外性能。

Conclusion: 通过功能不变性而非表示不变性，可以简单有效地解决捷径学习问题，使分类器忽略虚假特征，依赖更复杂的核心语义信号。

Abstract: Deep neural networks are prone to learning shortcuts, spurious and easily learned correlations in training data that cause severe failures in out-of-distribution (OOD) generalization. A dominant line of work seeks robustness by learning a robust representation, often explicitly partitioning the latent space into core and spurious components; this approach can be complex, brittle, and difficult to scale. We take a different approach, instead of a robust representation, we learn a robust function. We present a simple and effective training method that renders the classifier functionally invariant to shortcut signals. Our method operates within a disentangled latent space, which is essential as it isolates spurious and core features into distinct dimensions. This separation enables the identification of candidate shortcut features by their strong correlation with the label, used as a proxy for semantic simplicity. The classifier is then desensitized to these features by injecting targeted, anisotropic latent noise during training. We analyze this as targeted Jacobian regularization, which forces the classifier to ignore spurious features and rely on more complex, core semantic signals. The result is state-of-the-art OOD performance on established shortcut learning benchmarks.

</details>


### [31] [OmniTFT: Omni Target Forecasting for Vital Signs and Laboratory Result Trajectories in Multi Center ICU Data](https://arxiv.org/abs/2511.19485)
*Wanzhe Xu,Yutong Dai,Yitao Yang,Martin Loza,Weihang Zhang,Yang Cui,Xin Zeng,Sung Joon Park,Kenta Nakai*

Main category: cs.LG

TL;DR: OmniTFT是一个基于Temporal Fusion Transformer的深度学习框架，用于联合预测ICU中的高频生命体征和稀疏采样的实验室结果，通过四种创新策略提升性能并保持跨机构泛化能力。


<details>
  <summary>Details</summary>
Motivation: ICU中生命体征存在噪声和快速波动，实验室结果存在缺失值、测量延迟和设备偏差等问题，使得综合预测极具挑战性。

Method: 采用滑动窗口均衡采样平衡生理状态，频率感知嵌入收缩稳定罕见类别表示，分层变量选择引导模型关注信息特征簇，影响对齐注意力校准增强生理突变时的鲁棒性。

Result: 在MIMIC-III、MIMIC-IV和eICU数据集上，OmniTFT在生命体征和实验室结果的预测任务中都取得了显著性能提升。

Conclusion: OmniTFT能够统一建模多个异质临床目标，其注意力模式可解释且与已知病理生理学一致，在临床护理中具有定量决策支持的潜在应用价值。

Abstract: Accurate multivariate time-series prediction of vital signs and laboratory results is crucial for early intervention and precision medicine in intensive care units (ICUs). However, vital signs are often noisy and exhibit rapid fluctuations, while laboratory tests suffer from missing values, measurement lags, and device-specific bias, making integrative forecasting highly challenging. To address these issues, we propose OmniTFT, a deep learning framework that jointly learns and forecasts high-frequency vital signs and sparsely sampled laboratory results based on the Temporal Fusion Transformer (TFT). Specifically, OmniTFT implements four novel strategies to enhance performance: sliding window equalized sampling to balance physiological states, frequency-aware embedding shrinkage to stabilize rare-class representations, hierarchical variable selection to guide model attention toward informative feature clusters, and influence-aligned attention calibration to enhance robustness during abrupt physiological changes. By reducing the reliance on target-specific architectures and extensive feature engineering, OmniTFT enables unified modeling of multiple heterogeneous clinical targets while preserving cross-institutional generalizability. Across forecasting tasks, OmniTFT achieves substantial performance improvement for both vital signs and laboratory results on the MIMIC-III, MIMIC-IV, and eICU datasets. Its attention patterns are interpretable and consistent with known pathophysiology, underscoring its potential utility for quantitative decision support in clinical care.

</details>


### [32] [ModHiFi: Identifying High Fidelity predictive components for Model Modification](https://arxiv.org/abs/2511.19566)
*Dhruva Kashyap,Chaitanya Murti,Pranav K Nayak,Tanay Narshana,Chiranjib Bhattacharyya*

Main category: cs.LG

TL;DR: 提出了ModHiFi算法，无需训练数据或损失函数即可进行模型修改，包括结构化剪枝和类别遗忘，在ImageNet和语言模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 开放权重模型通常不提供训练数据或损失函数，使得模型修改（如剪枝、遗忘）受到限制。现有方法需要梯度或真实标签，在计算资源有限的情况下不可行。

Method: 利用局部重构误差来量化组件重要性，提出子集保真度指标。在特征不相关的情况下，通过子集保真度选择组件是最优的，据此提出ModHiFi算法。

Result: ModHiFi-P在ImageNet模型上比现有技术快11%，在语言模型上表现有竞争力；ModHiFi-U在CIFAR-10上实现完全遗忘而无需微调，在Swin Transformers上表现良好。

Conclusion: 证明了全局重构误差可由局部重构误差线性界定，为无需训练数据或损失函数的模型修改提供了理论基础和实用方法。

Abstract: Open weight models, which are ubiquitous, rarely provide access to their training data or loss function. This makes modifying such models for tasks such as pruning or unlearning constrained by this unavailability an active area of research. Existing techniques typically require gradients or ground-truth labels, rendering them infeasible in settings with limited computational resources. In this work, we investigate the fundamental question of identifying components that are critical to the model's predictive performance, without access to either gradients or the loss function, and with only distributional access such as synthetic data. We theoretically demonstrate that the global reconstruction error is linearly bounded by local reconstruction errors for Lipschitz-continuous networks such as CNNs and well-trained Transformers (which, contrary to existing literature, we find exhibit Lipschitz continuity). This motivates using the locally reconstructive behavior of component subsets to quantify their global importance, via a metric that we term Subset Fidelity. In the uncorrelated features setting, selecting individual components via their Subset Fidelity scores is optimal, which we use to propose ModHiFi, an algorithm for model modification that requires no training data or loss function access. ModHiFi-P, for structured pruning, achieves an 11% speedup over the current state of the art on ImageNet models and competitive performance on language models. ModHiFi-U, for classwise unlearning, achieves complete unlearning on CIFAR-10 without fine-tuning and demonstrates competitive performance on Swin Transformers.

</details>


### [33] [Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning then Rectification](https://arxiv.org/abs/2511.19486)
*Lei Wang,Zikun Ye,Jinglong Zhao*

Main category: cs.LG

TL;DR: 提出结合微调和校正的两阶段框架，通过优化分配有限标注样本，最小化预测误差方差而非均方误差，提升LLM在市场研究和社科应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要单独使用微调或校正来提升LLM性能，但缺乏将两者结合并优化资源分配的框架。

Method: 开发结合微调和校正的两阶段框架，提出最小化预测误差方差作为微调目标，并基于经验缩放定律优化样本分配策略。

Result: 实证分析验证了该框架相比单独使用微调或校正方法，在估计和推断性能上均有提升。

Conclusion: 提出的两阶段框架能有效结合微调和校正的优势，通过优化样本分配策略显著提升LLM预测性能。

Abstract: Driven by recent advances in artificial intelligence (AI), a growing body of work demonstrates the potential of using large language models (LLMs) to generate human-like responses in market research and social science applications. Two primary approaches can be applied to improve the performance of LLMs: fine-tuning, which aligns LLM predictions more closely with human responses, and rectification, which corrects biases in LLM outputs. In this paper, we develop a framework that combines fine-tuning and rectification, and optimally allocates limited labeled samples across the two stages. Unlike the conventional objective that minimizes the mean squared prediction errors, we propose to minimize the variance of the prediction errors as the fine-tuning objective, which is optimal for the downstream rectification stage. Building on this insight, we leverage empirical scaling laws to develop a data-driven method for optimally splitting samples between the fine-tuning and rectification stages. Empirical analysis validates our framework, demonstrating improved estimation and inference performance compared to using either fine-tuning or rectification alone.

</details>


### [34] [Neural Tractability via Structure: Learning-Augmented Algorithms for Graph Combinatorial Optimization](https://arxiv.org/abs/2511.19573)
*Jialiang Li,Weitong Chen,Mingyu Guo*

Main category: cs.LG

TL;DR: 提出结合神经模型推理效率与参数化算法最优性保证的新框架，通过神经模型处理结构困难部分，参数化算法搜索剩余简单部分，实现优于纯神经求解器的解质量。


<details>
  <summary>Details</summary>
Motivation: 神经模型在图组合优化问题上推理快但解质量不如经典搜索算法，经典算法虽慢但有最优性保证。需要结合两者优势。

Method: 使用参数化算法识别实例的结构简单部分，神经模型处理结构困难部分并提供指导信号，参数化算法整合指导信号系统搜索剩余部分。

Result: 在多个组合优化任务上，该框架获得优于纯神经求解器的解质量，与商业求解器相当，且具有更好的分布外泛化能力。

Conclusion: 该框架成功结合神经模型与参数化算法优势，既保持推理效率又提供解质量保证，解决了现有神经求解器的关键限制。

Abstract: Neural models have shown promise in solving NP-hard graph combinatorial optimization (CO) problems. Once trained, they offer fast inference and reasonably high-quality solutions for in-distribution testing instances, but they generally fall short in terms of absolute solution quality compared to classical search-based algorithms that are admittedly slower but offer optimality guarantee once search finishes.
  We propose a novel framework that combines the inference efficiency and exploratory power of neural models with the solution quality guarantee of search-based algorithms. In particular, we use parameterized algorithms (PAs) as the search component. PAs are dedicated to identifying easy instances of generally NP-hard problems, and allow for practically efficient search by exploiting structural simplicity (of the identified easy instances). Under our framework, we use parameterized analysis to identify the structurally hard parts of a CO instance. The neural model handles the hard parts by generating advisory signals based on its data-driven understanding. The PA-based search component then integrates the advisory signals to systematically and efficiently searches through the remaining structurally easy parts. Notably, our framework is agnostic to the choice of neural model and produces strictly better solutions than neural solvers alone.
  We examine our framework on multiple CO tasks. Empirical results show that it achieves superior solution quality, competitive with that of commercial solvers. Furthermore, by using the neural model only for exploratory advisory signals, our framework exhibits improved out-of-distribution generalization, addressing a key limitation of existing neural CO solvers.

</details>


### [35] [Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles](https://arxiv.org/abs/2511.19656)
*Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文针对双层优化的下界问题，在光滑非凸-强凸设置下开发了新的困难实例，分别在确定性和随机一阶oracle模型下得到了非平凡的下界。


<details>
  <summary>Details</summary>
Motivation: 虽然双层优化的上界保证已被广泛研究，但由于双层结构的复杂性，下界的进展有限。本文旨在填补这一空白，为双层优化建立更严格的下界。

Method: 开发新的困难实例，在光滑非凸-强凸设置下分析确定性和随机一阶oracle模型。使用零尊重算法框架来证明下界。

Result: 在确定性情况下，证明任何一阶零尊重算法至少需要Ω(κ^{3/2}ε^{-2})次oracle调用才能找到ε-精确稳定点；在随机情况下，至少需要Ω(κ^{5/2}ε^{-4})次随机oracle调用。

Conclusion: 结果揭示了当前双层优化上下界之间的显著差距，表明即使是简化情况（如二次下层目标）也需要进一步研究以理解标准一阶oracle下双层优化的最优复杂度。

Abstract: Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $Ω(κ^{3/2}ε^{-2})$ oracle calls to find an $ε$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $Ω(κ^{5/2}ε^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.

</details>


### [36] [Generative Model-Aided Continual Learning for CSI Feedback in FDD mMIMO-OFDM Systems](https://arxiv.org/abs/2511.19490)
*Guijun Liu,Yuwen Cao,Tomoaki Ohtsuki,Jiguang He,Shahid Mumtaz*

Main category: cs.LG

TL;DR: 提出基于GAN的CSI反馈方法，通过生成器作为记忆单元解决动态环境中现有模型需要重新训练和灾难性遗忘的问题，提升DAE框架的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有CSI反馈模型难以适应由用户移动性引起的动态环境变化，遇到新CSI分布时需要重新训练，且返回先前环境时因灾难性遗忘导致性能下降。

Method: 使用GAN生成器作为记忆单元，保留过去环境的知识，确保在不同场景下保持高性能而不遗忘。

Result: 仿真结果表明，该方法增强了DAE框架的泛化能力，同时保持低内存开销，并能与其他先进CSI反馈模型无缝集成。

Conclusion: 所提出的GAN学习方法具有鲁棒性和适应性，能有效解决CSI反馈中的持续学习问题。

Abstract: Deep autoencoder (DAE) frameworks have demonstrated their effectiveness in reducing channel state information (CSI) feedback overhead in massive multiple-input multiple-output (mMIMO) orthogonal frequency division multiplexing (OFDM) systems. However, existing CSI feedback models struggle to adapt to dynamic environments caused by user mobility, requiring retraining when encountering new CSI distributions. Moreover, returning to previously encountered environments often leads to performance degradation due to catastrophic forgetting. Continual learning involves enabling models to incorporate new information while maintaining performance on previously learned tasks. To address these challenges, we propose a generative adversarial network (GAN)-based learning approach for CSI feedback. By using a GAN generator as a memory unit, our method preserves knowledge from past environments and ensures consistently high performance across diverse scenarios without forgetting. Simulation results show that the proposed approach enhances the generalization capability of the DAE framework while maintaining low memory overhead. Furthermore, it can be seamlessly integrated with other advanced CSI feedback models, highlighting its robustness and adaptability.

</details>


### [37] [Demystifying Diffusion Objectives: Reweighted Losses are Better Variational Bounds](https://arxiv.org/abs/2511.19664)
*Jiaxin Shi,Michalis K. Titsias*

Main category: cs.LG

TL;DR: 本文提出了一种对扩散模型重加权损失的理论新解释，通过构建时间相关的变分下界级联来改进标准证据下界，从而降低数据-模型KL散度。


<details>
  <summary>Details</summary>
Motivation: 为广泛使用的扩散模型重加权损失提供理论依据，改进标准训练目标以提升模型性能。

Method: 构建时间相关的变分下界级联，推导出适用于连续高斯扩散和掩码扩散的重加权目标函数。

Result: 在掩码扩散模型中显著提升了像素空间图像建模性能，样本质量接近连续扩散模型水平。

Conclusion: 该方法为掩码图像模型中广泛使用的简单加权方案提供了理论支持，并能有效提升扩散模型性能。

Abstract: We derive a new theoretical interpretation of the reweighted losses that are widely used for training diffusion models. Our method is based on constructing a cascade of time-dependent variational lower bounds on the data log-likelihood, that provably improves upon the standard evidence lower bound and results in reduced data-model KL-divergences. Combining such bounds gives rise to reweighted objectives that can be applied to any generative diffusion model including both continuous Gaussian diffusion and masked (discrete) diffusion models. Then, we showcase this framework in masked diffusion and report significant improvements over previous training losses in pixel-space image modeling, approaching sample quality comparable to continuous diffusion models. Our results also provide a theoretical justification for the simple weighting scheme widely used in masked image models.

</details>


### [38] [OpenCML: End-to-End Framework of Open-world Machine Learning to Learn Unknown Classes Incrementally](https://arxiv.org/abs/2511.19491)
*Jitendra Parmar,Praveen Singh Thakur*

Main category: cs.LG

TL;DR: 提出了一种开放世界机器学习模型，通过发现未知类别和增量学习新类别来实现持续学习，在开放世界学习和持续学习任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型遵循封闭世界假设，难以保留先前学到的知识用于未来任务，而自动化智能系统需要学习新类别和已知任务。

Method: 模型包含两个相互连接的任务：首先发现数据中的未知类别并创建新类别，然后为每个新类别执行增量学习，从而实现持续学习。

Result: 模型在开放世界学习中优于现有方法，在持续学习中四个迭代的平均准确率达到82.54%，最低准确率为65.87%。

Conclusion: 该模型能够在开放和持续学习环境中有效扩展对数据的理解，并随时间改进性能。

Abstract: Open-world machine learning is an emerging technique in artificial intelligence, where conventional machine learning models often follow closed-world assumptions, which can hinder their ability to retain previously learned knowledge for future tasks. However, automated intelligence systems must learn about novel classes and previously known tasks. The proposed model offers novel learning classes in an open and continuous learning environment. It consists of two different but connected tasks. First, it discovers unknown classes in the data and creates novel classes; next, it learns how to perform class incrementally for each new class. Together, they enable continual learning, allowing the system to expand its understanding of the data and improve over time. The proposed model also outperformed existing approaches in open-world learning. Furthermore, it demonstrated strong performance in continuous learning, achieving a highest average accuracy of 82.54% over four iterations and a minimum accuracy of 65.87%.

</details>


### [39] [When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements](https://arxiv.org/abs/2511.19794)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出一个保守的评估协议，用于在有限计算预算下判断机器学习中1-2%的性能提升是否真实，避免因随机性导致的虚假改进声明。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习论文通常报告1-2个百分点的基准测试改进，但这些增益对随机种子、数据顺序和实现细节高度敏感，且很少提供不确定性估计或显著性检验，难以区分真实算法进步与噪声。

Method: 基于配对多种子运行、偏差校正和加速(BCa)自助法置信区间，以及对每种子差异的符号翻转置换检验的简单评估协议。

Result: 在CIFAR-10、CIFAR-10N和AG News上的实验显示，单次运行和非配对t检验经常对0.6-2.0点的改进给出显著结果，而使用三个种子的配对协议在这些设置下从未声明显著性。

Conclusion: 在有限预算下，这种保守的评估方法对于小增益是更安全的默认选择，可作为防止过度声明的防护措施。

Abstract: Recent machine learning papers often report 1-2 percentage point improvements from a single run on a benchmark. These gains are highly sensitive to random seeds, data ordering, and implementation details, yet are rarely accompanied by uncertainty estimates or significance tests. It is therefore unclear when a reported +1-2% reflects a real algorithmic advance versus noise.
  We revisit this problem under realistic compute budgets, where only a few runs are affordable. We propose a simple, PC-friendly evaluation protocol based on paired multi-seed runs, bias-corrected and accelerated (BCa) bootstrap confidence intervals, and a sign-flip permutation test on per-seed deltas. The protocol is intentionally conservative and is meant as a guardrail against over-claiming.
  We instantiate it on CIFAR-10, CIFAR-10N, and AG News using synthetic no-improvement, small-gain, and medium-gain scenarios. Single runs and unpaired t-tests often suggest significant gains for 0.6-2.0 point improvements, especially on text. With only three seeds, our paired protocol never declares significance in these settings. We argue that such conservative evaluation is a safer default for small gains under tight budgets.

</details>


### [40] [Terminal Velocity Matching](https://arxiv.org/abs/2511.19797)
*Linqi Zhou,Mathias Parger,Ayaan Haque,Jiaming Song*

Main category: cs.LG

TL;DR: TVM是一种流匹配的泛化方法，支持高保真的一步和少步生成建模，通过终端速度匹配和最小架构修改实现稳定训练，在ImageNet上达到最先进的少步生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法在终端时间行为正则化不足，且扩散变换器缺乏Lipschitz连续性，限制了少步生成模型的效果和训练稳定性。

Method: 提出终端速度匹配(TVM)方法，建模任意两个扩散时间步之间的转换；引入最小架构修改确保Lipschitz连续性；开发融合注意力核支持Jacobian-向量积的反向传播。

Result: 在ImageNet-256x256上：1步生成FID 3.29，4步生成FID 1.99；在ImageNet-512x512上：1步生成FID 4.32，4步生成FID 2.94，达到最先进的少步生成性能。

Conclusion: TVM通过终端正则化和架构改进，实现了高效稳定的少步生成建模，在图像生成任务上取得了最先进的性能表现。

Abstract: We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.

</details>


### [41] [A Systematic Study of Compression Ordering for Large Language Models](https://arxiv.org/abs/2511.19495)
*Shivansh Chhawri,Rahul Mahadik,Suparna Rooj*

Main category: cs.LG

TL;DR: 系统研究LLM压缩技术（知识蒸馏、结构化剪枝、低比特量化）的组合顺序对Qwen2.5 3B模型的影响，发现P-KD-Q序列效果最佳，达到3.68倍压缩比。


<details>
  <summary>Details</summary>
Motivation: LLM需要大量计算资源，在受限环境中部署需要模型压缩。虽然单个压缩技术效果已有研究，但它们的交互作用和最优顺序仍不明确。

Method: 评估Qwen2.5 3B模型上的多种压缩流水线，包括单技术和三技术序列，使用困惑度、G-Eval、清晰度、提示对齐和压缩比作为指标。

Result: 量化提供最大独立压缩，剪枝引入中等质量下降。技术顺序显著影响最终质量：P-KD-Q序列效果最佳，保持强指令跟随和语言理解能力。早期应用量化的流水线因不可逆信息损失而性能严重下降。

Conclusion: 为在资源受限环境中部署LLM，设计有效的、顺序感知的压缩流水线提供了实用指导。

Abstract: Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.

</details>


### [42] [Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM](https://arxiv.org/abs/2511.19496)
*Yang Liu,Xiaolong Zhong,Ling Jiang*

Main category: cs.LG

TL;DR: Xmodel-2.5是一个13亿参数的小型语言模型，作为即插即用的智能体核心，通过μP训练方法、1.4T token的课程学习以及AdamW到Muon的优化器切换，在保持计算效率的同时提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具备强大的推理和工具使用能力，但计算需求高，不适合边缘或成本敏感的应用场景。

Method: 使用最大更新参数化(μP)训练方法，在20M参数代理模型上调优的超参数可直接迁移到完整模型；采用1.4T token的Warmup-Stable-Decay课程学习；在衰减阶段从AdamW切换到Muon优化器；使用FP8混合精度训练。

Result: 在衰减阶段从AdamW切换到Muon优化器，使13个任务的推理平均性能提升了4.58%，同时保持其他所有超参数不变。

Conclusion: Xmodel-2.5证明了早期使用AdamW的稳定性与后期使用Muon的锐化能力可以结合，从而获得更好的下游性能，为边缘和成本敏感部署提供了高效的解决方案。

Abstract: Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \emph{drop-in agent core}. Training with maximal-update parameterization ($μ$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\,\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\footnote{https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints).} Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.

</details>


### [43] [Provably Outlier-resistant Semi-parametric Regression for Transferable Calibration of Low-cost Air-quality Sensors](https://arxiv.org/abs/2511.19810)
*Divyansh Chaurasia,Manoj Daram,Roshan Kumar,Nihal Thukarama Rao,Vipul Sangode,Pranjal Srivastava,Avnish Tripathi,Shoubhik Chakraborty,Akanksha,Ambasht Kumar,Davender Sethi,Sachchida Nand Tripathi,Purushottam Kar*

Main category: cs.LG

TL;DR: 提出了RESPIRE技术用于校准低成本空气质量CO传感器，该技术在校准性能、抗异常值和可解释性方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 低成本空气质量传感器在建立密集监测网络中起关键作用，但校准过程昂贵且耗时，特别是在大规模地理分布部署中。

Method: RESPIRE技术提供抗异常值的训练算法和可解释模型，能够检测模型过拟合情况，在跨站点、跨季节和跨传感器设置中表现更优。

Result: 基于四个站点、两个季节和六个传感器包的大规模部署数据，RESPIRE在校准性能上取得了改进。

Conclusion: RESPIRE为大规模低成本空气质量传感器网络提供了一种有效的校准解决方案，代码已开源。

Abstract: We present a case study for the calibration of Low-cost air-quality (LCAQ) CO sensors from one of the largest multi-site-multi-season-multi-sensor-multi-pollutant mobile air-quality monitoring network deployments in India. LCAQ sensors have been shown to play a critical role in the establishment of dense, expansive air-quality monitoring networks and combating elevated pollution levels. The calibration of LCAQ sensors against regulatory-grade monitors is an expensive, laborious and time-consuming process, especially when a large number of sensors are to be deployed in a geographically diverse layout. In this work, we present the RESPIRE technique to calibrate LCAQ sensors to detect ambient CO (Carbon Monoxide) levels. RESPIRE offers specific advantages over baseline calibration methods popular in literature, such as improved prediction in cross-site, cross-season, and cross-sensor settings. RESPIRE offers a training algorithm that is provably resistant to outliers and an explainable model with the ability to flag instances of model overfitting. Empirical results are presented based on data collected during an extensive deployment spanning four sites, two seasons and six sensor packages. RESPIRE code is available at https://github.com/purushottamkar/respire.

</details>


### [44] [PeriodNet: Boosting the Potential of Attention Mechanism for Time Series Forecasting](https://arxiv.org/abs/2511.19497)
*Bowen Zhao,Huanlai Xing,Zhiwen Xiao,Jincheng Peng,Li Feng,Xinhan Wang,Rong Qu,Hui Li*

Main category: cs.LG

TL;DR: PeriodNet提出了一种新的时间序列预测网络结构，通过周期注意力机制和迭代分组机制，在单变量和多变量时间序列预测中都优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 注意力机制在自然语言处理中表现出色，但在时间序列预测中尚未达到预期效果，需要探索更好的网络结构。

Method: 采用周期注意力和稀疏周期注意力机制分析相邻周期，引入迭代分组机制减少跨变量冗余，重新设计Transformer架构并添加周期扩散器进行多周期预测。

Result: 在8个数据集上的实验表明，PeriodNet在单变量和多变量时间序列预测中都优于6个最先进模型，在预测720长度时间序列时相对改进达到22%。

Conclusion: PeriodNet通过创新的网络结构设计，显著提升了时间序列预测的性能，特别是在长序列预测方面表现突出。

Abstract: The attention mechanism has demonstrated remarkable potential in sequence modeling, exemplified by its successful application in natural language processing with models such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT). Despite these advancements, its utilization in time series forecasting (TSF) has yet to meet expectations. Exploring a better network structure for attention in TSF holds immense significance across various domains. In this paper, we present PeriodNet with a brand new structure to forecast univariate and multivariate time series. PeriodNet incorporates period attention and sparse period attention mechanism for analyzing adjacent periods. It enhances the mining of local characteristics, periodic patterns, and global dependencies. For efficient cross-variable modeling, we introduce an iterative grouping mechanism which can directly reduce the cross-variable redundancy. To fully leverage the extracted features on the encoder side, we redesign the entire architecture of the vanilla Transformer and propose a period diffuser for precise multi-period prediction. Through comprehensive experiments conducted on eight datasets, we demonstrate that PeriodNet outperforms six state-of-the-art models in both univariate and multivariate TSF scenarios in terms of mean square error and mean absolute error. In particular, PeriodNet achieves a relative improvement of 22% when forecasting time series with a length of 720, in comparison to other models based on the conventional encoder-decoder Transformer architecture.

</details>


### [45] [Cisco Time Series Model Technical Report](https://arxiv.org/abs/2511.19841)
*Liang Gou,Archit Khare,Praneet Pabolu,Prachi Patel,Joseph Ross,Hercy Shen,Yuhan,Song,Jingze Sun,Kristal Curtis,Vedant Dharnidharka,Abhinav Mathur,Hao Yang*

Main category: cs.LG

TL;DR: Cisco Time Series Model是一个单变量零样本预测器，通过多分辨率输入架构创新，在300B+数据点上训练，在可观测性数据集上表现优异，同时保持通用预测基准的相似性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理多分辨率输入的时间序列基础模型，特别针对可观测性领域，同时保持通用预测能力。

Method: 对TimesFM模型进行架构创新，使其能够接受多分辨率输入，使用超过300B个数据点进行训练，其中一半以上来自可观测性领域。

Result: 模型在可观测性数据集上实现卓越性能，在通用预测基准上保持相似性能，多分辨率结构使模型在长上下文输入上预测更准确。

Conclusion: 多分辨率解码器模型在保持通用预测能力的同时，显著提升了可观测性领域的预测性能，长上下文预测能力得到改善。

Abstract: We introduce the Cisco Time Series Model, a univariate zero-shot forecaster. This time series foundation model is the result of a general architectural innovation to a time series model enabling it to accept multiresolution input, applied to a popular decoder-only time series model (TimesFM). The resulting multiresolution decoder-only model is trained on over 300B unique data points, with more than half coming from the observability domain. Quantitative and qualitative evaluations demonstrate that the resulting model achieves superior performance on observability datasets while retaining very similar performance on a standard general-purpose forecasting benchmark (GIFT-Eval), and suggest that the multiresolution structure enables the model to make more accurate predictions on long context input.

</details>


### [46] [Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data](https://arxiv.org/abs/2511.19498)
*Yi Zhang,Tianxiang Xu,Zijian Li,Chao Zhang,Kunyu Zhang,Zhan Gao,Meinuo Li,Xiaohan Zhang,Qichao Qi,Bing Chen*

Main category: cs.LG

TL;DR: 提出了一个分层双策略框架，用于选择性知识遗忘，在医疗领域精确移除专业知识同时保留基础医学能力，仅需修改0.1%参数即可实现82.7%遗忘率和88.5%知识保留。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在医疗场景中的隐私风险，特别是训练数据记忆化问题，需要在不损害基础医学能力的前提下精确移除敏感专业知识。

Method: 分层双策略框架，结合几何约束梯度更新和概念感知token级干预，通过统一的四级医学概念层次区分保护关键token和遗忘目标token。

Result: 在MedMCQA（外科）和MHQA（焦虑、抑郁、创伤）数据集上评估，达到82.7%遗忘率和88.5%知识保留，仅需修改0.1%参数。

Conclusion: 该框架在保持强大隐私保证的同时，满足了临床研究中监管合规性、可审计性和伦理标准的关键需求。

Abstract: Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.

</details>


### [47] [SX-GeoTree: Self-eXplaining Geospatial Regression Tree Incorporating the Spatial Similarity of Feature Attributions](https://arxiv.org/abs/2511.19845)
*Chaogui Kang,Lijian Luo,Qingfeng Guan,Yu Liu*

Main category: cs.LG

TL;DR: SX-GeoTree是一种自解释地理空间回归树，通过结合三个耦合目标（MSE、全局Moran's I和模块度最大化）来改善传统决策树在空间依赖性和解释稳定性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统决策树在表格预测中表现良好，但存在两个主要问题：(i)难以捕捉空间依赖性，(ii)无法产生局部稳定的解释。需要开发能够同时处理空间依赖性和解释稳定性的方法。

Method: 在递归分割过程中集成三个目标：MSE（杂质减少）、全局Moran's I（空间残差控制）、以及通过模块度最大化实现解释鲁棒性。模块度基于地理加权回归系数距离和SHAP归因距离构建的共识相似性网络。

Result: 在两个任务上的实验表明：SX-GeoTree保持竞争力的预测精度（与决策树R²相差0.01以内），同时改善了残差空间均匀性，并将归因共识度提高了一倍（模块度：福建0.19 vs 0.09；西雅图0.10 vs 0.05）。

Conclusion: 该框架展示了如何将空间相似性（通过GWR衍生的局部关系扩展到几何邻近性之外）嵌入可解释模型中，推进了可信地理空间机器学习，并为领域感知可解释性提供了可转移模板。

Abstract: Decision trees remain central for tabular prediction but struggle with (i) capturing spatial dependence and (ii) producing locally stable (robust) explanations. We present SX-GeoTree, a self-explaining geospatial regression tree that integrates three coupled objectives during recursive splitting: impurity reduction (MSE), spatial residual control (global Moran's I), and explanation robustness via modularity maximization on a consensus similarity network formed from (a) geographically weighted regression (GWR) coefficient distances (stimulus-response similarity) and (b) SHAP attribution distances (explanatory similarity). We recast local Lipschitz continuity of feature attributions as a network community preservation problem, enabling scalable enforcement of spatially coherent explanations without per-sample neighborhood searches. Experiments on two exemplar tasks (county-level GDP in Fujian, n=83; point-wise housing prices in Seattle, n=21,613) show SX-GeoTree maintains competitive predictive accuracy (within 0.01 $R^{2}$ of decision trees) while improving residual spatial evenness and doubling attribution consensus (modularity: Fujian 0.19 vs 0.09; Seattle 0.10 vs 0.05). Ablation confirms Moran's I and modularity terms are complementary; removing either degrades both spatial residual structure and explanation stability. The framework demonstrates how spatial similarity - extended beyond geometric proximity through GWR-derived local relationships - can be embedded in interpretable models, advancing trustworthy geospatial machine learning and offering a transferable template for domain-aware explainability.

</details>


### [48] [Beyond Binary Classification: A Semi-supervised Approach to Generalized AI-generated Image Detection](https://arxiv.org/abs/2511.19499)
*Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: 提出TriDetect检测器，通过发现假图像中的潜在架构模式来增强跨生成器泛化能力，解决GAN和扩散模型架构差异导致的检测器泛化问题。


<details>
  <summary>Details</summary>
Motivation: 当前检测器在跨生成器（特别是跨架构如GAN到扩散模型）时泛化能力差，这源于不同架构产生的伪影存在根本差异。

Method: 提出TriDetect半监督方法，通过Sinkhorn-Knopp算法进行平衡聚类分配和跨视图一致性机制，学习基础架构差异。

Result: 在两个标准基准和三个真实数据集上评估，与13个基线方法相比，展示了在未见生成器上的泛化能力。

Conclusion: 通过理论分析和实验验证，TriDetect能够有效识别不同生成架构的固有模式，提升跨生成器检测的泛化性能。

Abstract: The rapid advancement of generators (e.g., StyleGAN, Midjourney, DALL-E) has produced highly realistic synthetic images, posing significant challenges to digital media authenticity. These generators are typically based on a few core architectural families, primarily Generative Adversarial Networks (GANs) and Diffusion Models (DMs). A critical vulnerability in current forensics is the failure of detectors to achieve cross-generator generalization, especially when crossing architectural boundaries (e.g., from GANs to DMs). We hypothesize that this gap stems from fundamental differences in the artifacts produced by these \textbf{distinct architectures}. In this work, we provide a theoretical analysis explaining how the distinct optimization objectives of the GAN and DM architectures lead to different manifold coverage behaviors. We demonstrate that GANs permit partial coverage, often leading to boundary artifacts, while DMs enforce complete coverage, resulting in over-smoothing patterns. Motivated by this analysis, we propose the \textbf{Tri}archy \textbf{Detect}or (TriDetect), a semi-supervised approach that enhances binary classification by discovering latent architectural patterns within the "fake" class. TriDetect employs balanced cluster assignment via the Sinkhorn-Knopp algorithm and a cross-view consistency mechanism, encouraging the model to learn fundamental architectural distincts. We evaluate our approach on two standard benchmarks and three in-the-wild datasets against 13 baselines to demonstrate its generalization capability to unseen generators.

</details>


### [49] [Adaptivity and Universality: Problem-dependent Universal Regret for Online Convex Optimization](https://arxiv.org/abs/2511.19937)
*Peng Zhao,Yu-Hu Yan,Hang Yu,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 提出了UniGrad方法，实现了通用在线学习同时具备问题自适应性，能够根据梯度变化V_T自适应调整遗憾界，包括UniGrad.Correct和UniGrad.Bregman两个变体，以及计算效率更高的UniGrad++。


<details>
  <summary>Details</summary>
Motivation: 现有通用在线学习方法虽然能同时获得凸函数、指数凹函数和强凸函数的最优遗憾界，但缺乏问题自适应性，特别是无法根据梯度变化V_T进行自适应调整，而V_T在随机优化和博弈快速收敛中起着关键作用。

Method: 提出了UniGrad方法，包含两个变体：UniGrad.Correct保持RVU属性，UniGrad.Bregman通过新设计实现最优遗憾界。两者都使用元算法和O(log T)个基础学习器。还提出了UniGrad++通过代理优化将每轮梯度查询减少到1次。

Result: UniGrad.Correct对强凸函数达到O(log V_T)遗憾，对指数凹函数达到O(d log V_T)遗憾，对凸函数达到O(√(V_T log V_T))遗憾；UniGrad.Bregman对凸函数达到最优的O(√V_T)遗憾。UniGrad++保持相同遗憾界但计算效率更高。

Conclusion: UniGrad方法首次在通用在线学习中实现了问题自适应性，能够根据梯度变化自适应调整遗憾界，填补了现有方法的空白，并在计算效率上进行了优化。

Abstract: Universal online learning aims to achieve optimal regret guarantees without requiring prior knowledge of the curvature of online functions. Existing methods have established minimax-optimal regret bounds for universal online learning, where a single algorithm can simultaneously attain $\mathcal{O}(\sqrt{T})$ regret for convex functions, $\mathcal{O}(d \log T)$ for exp-concave functions, and $\mathcal{O}(\log T)$ for strongly convex functions, where $T$ is the number of rounds and $d$ is the dimension of the feasible domain. However, these methods still lack problem-dependent adaptivity. In particular, no universal method provides regret bounds that scale with the gradient variation $V_T$, a key quantity that plays a crucial role in applications such as stochastic optimization and fast-rate convergence in games. In this work, we introduce UniGrad, a novel approach that achieves both universality and adaptivity, with two distinct realizations: UniGrad.Correct and UniGrad.Bregman. Both methods achieve universal regret guarantees that adapt to gradient variation, simultaneously attaining $\mathcal{O}(\log V_T)$ regret for strongly convex functions and $\mathcal{O}(d \log V_T)$ regret for exp-concave functions. For convex functions, the regret bounds differ: UniGrad.Correct achieves an $\mathcal{O}(\sqrt{V_T \log V_T})$ bound while preserving the RVU property that is crucial for fast convergence in online games, whereas UniGrad.Bregman achieves the optimal $\mathcal{O}(\sqrt{V_T})$ regret bound through a novel design. Both methods employ a meta algorithm with $\mathcal{O}(\log T)$ base learners, which naturally requires $\mathcal{O}(\log T)$ gradient queries per round. To enhance computational efficiency, we introduce UniGrad++, which retains the regret while reducing the gradient query to just $1$ per round via surrogate optimization. We further provide various implications.

</details>


### [50] [How to Purchase Labels? A Cost-Effective Approach Using Active Learning Markets](https://arxiv.org/abs/2511.20605)
*Xiwen Huang,Pierre Pinson*

Main category: cs.LG

TL;DR: 提出并分析主动学习市场，通过优化问题形式化市场清算，整合预算约束和改进阈值，在房地产定价和能源预测等实际应用中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在分析师需要获取额外数据来改进模型拟合或训练预测分析模型的情况下，现有方法主要关注购买特征和样本，而本文专注于购买标签的市场机制。

Method: 采用单一买家-多卖家设置，提出两种主动学习策略（基于方差和基于查询委员会），配合不同定价机制，并与随机采样基准进行比较。

Result: 在真实世界数据集上的验证表明，该方法具有鲁棒性，相比传统方法能用更少的标签获得更优性能。

Conclusion: 提出的方法为资源受限环境中的数据采集优化提供了一个易于实现的实用解决方案。

Abstract: We introduce and analyse active learning markets as a way to purchase labels, in situations where analysts aim to acquire additional data to improve model fitting, or to better train models for predictive analytics applications. This comes in contrast to the many proposals that already exist to purchase features and examples. By originally formalising the market clearing as an optimisation problem, we integrate budget constraints and improvement thresholds into the label acquisition process. We focus on a single-buyer-multiple-seller setup and propose the use of two active learning strategies (variance based and query-by-committee based), paired with distinct pricing mechanisms. They are compared to a benchmark random sampling approach. The proposed strategies are validated on real-world datasets from two critical application domains: real estate pricing and energy forecasting. Results demonstrate the robustness of our approach, consistently achieving superior performance with fewer labels acquired compared to conventional methods. Our proposal comprises an easy-to-implement practical solution for optimising data acquisition in resource-constrained environments.

</details>


### [51] [Profile Generators: A Link between the Narrative and the Binary Matrix Representation](https://arxiv.org/abs/2511.19506)
*Raoul H. Kutil,Georg Zimmermann,Barbara Strasser-Kirchweger,Christian Borgelt*

Main category: cs.LG

TL;DR: 开发了一种症状配置文件生成器来替代二元矩阵表示法，用于处理复杂认知障碍的相似性分析，解决了传统方法无法处理大量症状组合的问题。


<details>
  <summary>Details</summary>
Motivation: DSM-5中认知障碍的复杂症状组合使得传统的二元矩阵表示法不可行，需要一种能够自动生成有效症状组合的替代表示方法。

Method: 创建症状配置文件生成器，使用列表、集合和数字的严格预定义格式来表示复杂的诊断路径，通过条件生成器操作实现特定MPCS值的计算。

Result: 成功将多个精神障碍表示为生成器形式，证明矩阵表示法对于复杂障碍过于庞大而无法管理，开发了基于生成器操作的配置文件缩减方法。

Conclusion: 症状配置文件生成器提供了一种可读、适应性强且全面的二元矩阵替代方案，能够处理复杂认知障碍的相似性分析问题。

Abstract: Mental health disorders, particularly cognitive disorders defined by deficits in cognitive abilities, are described in detail in the DSM-5, which includes definitions and examples of signs and symptoms. A simplified, machine-actionable representation was developed to assess the similarity and separability of these disorders, but it is not suited for the most complex cases. Generating or applying a full binary matrix for similarity calculations is infeasible due to the vast number of symptom combinations. This research develops an alternative representation that links the narrative form of the DSM-5 with the binary matrix representation and enables automated generation of valid symptom combinations. Using a strict pre-defined format of lists, sets, and numbers with slight variations, complex diagnostic pathways involving numerous symptom combinations can be represented. This format, called the symptom profile generator (or simply generator), provides a readable, adaptable, and comprehensive alternative to a binary matrix while enabling easy generation of symptom combinations (profiles). Cognitive disorders, which typically involve multiple diagnostic criteria with several symptoms, can thus be expressed as lists of generators. Representing several psychotic disorders in generator form and generating all symptom combinations showed that matrix representations of complex disorders become too large to manage. The MPCS (maximum pairwise cosine similarity) algorithm cannot handle matrices of this size, prompting the development of a profile reduction method using targeted generator manipulation to find specific MPCS values between disorders. The generators allow easier creation of binary representations for large matrices and make it possible to calculate specific MPCS cases between complex disorders through conditional generators.

</details>


### [52] [TouchFormer: A Robust Transformer-based Framework for Multimodal Material Perception](https://arxiv.org/abs/2511.19509)
*Kailin Lyu,Long Xiao,Jianing Zeng,Junhao Dong,Xuexin Liu,Zhuojun Zou,Haoyue Yang,Lin Shu,Jie Hao*

Main category: cs.LG

TL;DR: 提出了TouchFormer框架，通过模态自适应门控机制和跨模态注意力机制解决多模态融合中的噪声、缺失模态和动态重要性问题，在材料感知任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统视觉方法在视觉受损条件下性能下降严重，现有多模态方法简单融合模态输入，忽视了模态特定噪声、缺失模态和模态动态重要性等关键挑战。

Method: 使用模态自适应门控机制和内外模态注意力机制自适应整合跨模态特征，引入跨实例嵌入正则化策略提升细粒度分类精度。

Result: 在SSMC和USMC任务上分别提升分类准确率2.48%和6.83%，机器人实验验证了在真实环境中的有效性。

Conclusion: TouchFormer框架为机器人在安全关键应用中的部署提供了有效解决方案，代码和数据集将开源。

Abstract: Traditional vision-based material perception methods often experience substantial performance degradation under visually impaired conditions, thereby motivating the shift toward non-visual multimodal material perception. Despite this, existing approaches frequently perform naive fusion of multimodal inputs, overlooking key challenges such as modality-specific noise, missing modalities common in real-world scenarios, and the dynamically varying importance of each modality depending on the task. These limitations lead to suboptimal performance across several benchmark tasks. In this paper, we propose a robust multimodal fusion framework, TouchFormer. Specifically, we employ a Modality-Adaptive Gating (MAG) mechanism and intra- and inter-modality attention mechanisms to adaptively integrate cross-modal features, enhancing model robustness. Additionally, we introduce a Cross-Instance Embedding Regularization(CER) strategy, which significantly improves classification accuracy in fine-grained subcategory material recognition tasks. Experimental results demonstrate that, compared to existing non-visual methods, the proposed TouchFormer framework achieves classification accuracy improvements of 2.48% and 6.83% on SSMC and USMC tasks, respectively. Furthermore, real-world robotic experiments validate TouchFormer's effectiveness in enabling robots to better perceive and interpret their environment, paving the way for its deployment in safety-critical applications such as emergency response and industrial automation. The code and datasets will be open-source, and the videos are available in the supplementary materials.

</details>


### [53] [Row-stochastic matrices can provably outperform doubly stochastic matrices in decentralized learning](https://arxiv.org/abs/2511.19513)
*Bing Liu,Boao Kong,Limin Lu,Kun Yuan,Chengcheng Zhao*

Main category: cs.LG

TL;DR: 本文通过加权希尔伯特空间框架分析去中心化学习中两种权重处理策略的收敛差异，发现行随机矩阵设计在特定条件下收敛更快。


<details>
  <summary>Details</summary>
Motivation: 澄清去中心化学习中两种权重处理策略（嵌入本地损失vs使用行随机矩阵）的本质差异，现有欧几里得分析无法充分解释其收敛行为差异。

Method: 建立加权希尔伯特空间框架L²(λ;ℝᵈ)，分析两种策略在该几何空间中的收敛特性，使用Rayleigh商和Loewner序特征值比较方法。

Result: 在加权希尔伯特空间中，行随机矩阵变为自伴算子而双随机矩阵不是，产生额外惩罚项放大共识误差；行随机设计在特定拓扑条件下收敛更快。

Conclusion: 收敛差异不仅源于谱间隙，还来自惩罚项；行随机设计在满足特定拓扑条件时可获得收敛优势，为拓扑设计提供实用指导。

Abstract: Decentralized learning often involves a weighted global loss with heterogeneous node weights $λ$. We revisit two natural strategies for incorporating these weights: (i) embedding them into the local losses to retain a uniform weight (and thus a doubly stochastic matrix), and (ii) keeping the original losses while employing a $λ$-induced row-stochastic matrix. Although prior work shows that both strategies yield the same expected descent direction for the global loss, it remains unclear whether the Euclidean-space guarantees are tight and what fundamentally differentiates their behaviors. To clarify this, we develop a weighted Hilbert-space framework $L^2(λ;\mathbb{R}^d)$ and obtain convergence rates that are strictly tighter than those from Euclidean analysis. In this geometry, the row-stochastic matrix becomes self-adjoint whereas the doubly stochastic one does not, creating additional penalty terms that amplify consensus error, thereby slowing convergence. Consequently, the difference in convergence arises not only from spectral gaps but also from these penalty terms. We then derive sufficient conditions under which the row-stochastic design converges faster even with a smaller spectral gap. Finally, by using a Rayleigh-quotient and Loewner-order eigenvalue comparison, we further obtain topology conditions that guarantee this advantage and yield practical topology-design guidelines.

</details>


### [54] [Automating Deception: Scalable Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2511.19517)
*Adarsh Kumarappan,Ananya Mujoo*

Main category: cs.LG

TL;DR: 本文提出了一种自动生成心理驱动多轮越狱数据集的方法，评估了不同LLM在对抗FITD攻击时的表现，发现GPT系列模型对对话历史高度敏感，而Gemini 2.5 Flash表现出卓越的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 多轮对话攻击利用心理学原理（如得寸进尺）绕过LLM的安全对齐，现有防御方法依赖难以扩展的手动数据集创建，阻碍了防御进展。

Method: 开发自动化流水线，将FITD技术系统化为可复现模板，创建包含1,500个场景的基准数据集，评估7个模型在多轮和单轮条件下的表现。

Result: GPT系列模型对对话历史高度敏感，攻击成功率最高增加32个百分点；Gemini 2.5 Flash几乎免疫这些攻击；Claude 3 Haiku表现出强但不完美的抵抗力。

Conclusion: 当前安全架构在处理对话上下文时存在关键差异，需要能够抵抗基于叙事操纵的防御机制。

Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.

</details>


### [55] [Learning to Solve Weighted Maximum Satisfiability with a Co-Training Architecture](https://arxiv.org/abs/2511.19544)
*Kaidi Wan,Minghao Liu,Yong Lai*

Main category: cs.LG

TL;DR: SplitGNN是一种基于图神经网络的加权最大可满足性问题求解方法，通过协同训练架构和新的边分裂因子图表示，实现了更快的收敛速度和更好的预测性能，在大型加权MaxSAT基准测试中超越了现代启发式求解器。


<details>
  <summary>Details</summary>
Motivation: 解决加权最大可满足性问题，特别是在处理具有挑战性的加权实例时，传统方法存在局限性，需要开发能够利用图结构信息并提高求解效率的新方法。

Method: 提出SplitGNN方法，包含监督消息传递机制和无监督解增强层的协同训练架构，引入基于生成树和边分类的边分裂因子图表示，并实现GPU加速的高效分数计算和基于松弛的优化层。

Result: 实验显示SplitGNN比其他基于GNN的架构收敛速度快3倍，预测效果更好，在更大更难的加权MaxSAT基准测试中成功找到超越现代启发式求解器的解，并在不同结构实例上展现出优异的泛化能力。

Conclusion: SplitGNN通过创新的图表示和协同训练架构，有效解决了加权MaxSAT问题，在求解效率和泛化能力方面都取得了显著提升，为复杂组合优化问题提供了新的解决方案。

Abstract: Wepropose SplitGNN, a graph neural network (GNN)-based
  approach that learns to solve weighted maximum satisfiabil ity (MaxSAT) problem. SplitGNN incorporates a co-training
  architecture consisting of supervised message passing mech anism and unsupervised solution boosting layer. A new graph
  representation called edge-splitting factor graph is proposed
  to provide more structural information for learning, which is
  based on spanning tree generation and edge classification. To
  improve the solutions on challenging and weighted instances,
  we implement a GPU-accelerated layer applying efficient
  score calculation and relaxation-based optimization. Exper iments show that SplitGNN achieves 3* faster convergence
  and better predictions compared with other GNN-based ar chitectures. More notably, SplitGNN successfully finds solu tions that outperform modern heuristic MaxSAT solvers on
  much larger and harder weighted MaxSAT benchmarks, and
  demonstrates exceptional generalization abilities on diverse
  structural instances.

</details>


### [56] [When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics](https://arxiv.org/abs/2511.19548)
*Yiven,Zhu*

Main category: cs.LG

TL;DR: 本文提出了一个基于模型的框架，将神经信号、计算决策模型和规范福利标准联系起来，分析神经数据何时能合法地用于政策福利判断。


<details>
  <summary>Details</summary>
Motivation: 神经经济学承诺基于神经和计算证据进行福利分析，但政策制定者和商业机构越来越多地使用神经数据来证明家长式监管、"基于大脑"的干预和新福利措施的合理性。本文旨在探讨神经数据在什么条件下可以合法地为政策福利判断提供信息，而不仅仅是描述行为。

Method: 开发了一个非经验性的、基于模型的框架，在演员-评论家强化学习模型中，将从神经活动到潜在价值和预测误差，再到福利主张的推理路径形式化。

Result: 研究表明，只有当神经-计算映射得到充分验证，决策模型识别出"真实"利益与情境依赖的错误，并且福利标准被明确指定和辩护时，神经证据才能约束福利判断。

Conclusion: 分析将大脑和人工智能体视为价值学习系统，同时表明内部奖励信号（无论是生物的还是人工的）都是计算量，没有明确的规范模型就不能被视为福利衡量标准。提出了神经经济福利推理清单供监管者和神经AI系统设计者使用。

Abstract: Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, "brain-based" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies "true" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.

</details>


### [57] [Online Sparse Feature Selection in Data Streams via Differential Evolution](https://arxiv.org/abs/2511.19555)
*Ruiyang Xu*

Main category: cs.LG

TL;DR: 提出ODESFS方法解决在线稀疏流特征选择中的数据缺失问题，通过潜在因子分析和差分进化实现缺失值填补和特征重要性评估，在六个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有在线稀疏流特征选择方法在特征评估方面存在显著局限性，导致性能下降，需要改进特征选择和缺失数据处理能力。

Method: 结合潜在因子分析模型进行缺失值填补，使用差分进化算法评估特征重要性，实现在线稀疏流特征选择。

Result: 在六个真实数据集上的实验表明，ODESFS能够选择最优特征子集并获得更高的准确率，优于现有的OSFS和OS2FS方法。

Conclusion: ODESFS通过创新的缺失值填补和特征评估方法，有效提升了在线稀疏流特征选择的性能，为高维流数据处理提供了更优解决方案。

Abstract: The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.

</details>


### [58] [Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport](https://arxiv.org/abs/2511.19561)
*Zecheng Pan,Zhikang Chen,Ding Li,Min Zhang,Sen Cui,Hongshuo Jin,Luqi Tao,Yi Yang,Deheng Ye,Yu Zhang,Tingting Zhu,Tianling Ren*

Main category: cs.LG

TL;DR: OTMF是一个基于最优传输理论的模型融合框架，通过语义几何对齐解决参数插值带来的分布偏移问题，支持持续融合多个任务模型。


<details>
  <summary>Details</summary>
Motivation: 现有的模型融合方法主要依赖权重空间的参数插值，这会导致特征空间的显著分布偏移并削弱任务特定知识。

Method: OTMF通过最优传输计划发现应用于任务向量的共同掩码，选择性提取可迁移和任务无关的组件，同时保留每个任务的独特结构特性。

Result: 在多个视觉和语言基准测试中，OTMF在准确性和效率方面都达到了最先进的性能。

Conclusion: OTMF为模型融合提供了实用且理论上有价值的方法，能够有效解决分布偏移问题并支持持续融合。

Abstract: Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.

</details>


### [59] [An Invariant Latent Space Perspective on Language Model Inversion](https://arxiv.org/abs/2511.19569)
*Wentao Ye,Jiaqi Hu,Haobo Wang,Xinpeng Ti,Zhiqing Xiao,Hao Chen,Liyao Li,Lei Feng,Sai Wu,Junbo Zhao*

Main category: cs.LG

TL;DR: 提出Inv^2A方法，通过利用LLM的潜在空间不变性假设来改进语言模型反演攻击，在9个数据集上平均BLEU得分提升4.77%，同时减少对大反演语料库的依赖。


<details>
  <summary>Details</summary>
Motivation: 语言模型反演(LMI)对用户隐私和系统安全构成威胁，现有方法依赖大量反演语料库且效果有限，需要更有效的方法来恢复隐藏提示。

Method: 提出不变潜在空间假设(ILSH)，将LLM视为不变解码器，仅学习轻量级反演编码器。采用两阶段训练：对比对齐(源不变性)和监督强化(循环不变性)，可选邻域搜索优化局部性能。

Result: 在9个数据集上Inv^2A优于基线方法，平均BLEU得分提升4.77%，同时减少了对大反演语料库的依赖。分析显示现有防御措施保护有限。

Conclusion: Inv^2A通过利用LLM潜在空间不变性有效提升了语言模型反演性能，揭示了当前防御措施的不足，需要更强的安全策略。

Abstract: Language model inversion (LMI), i.e., recovering hidden prompts from outputs, emerges as a concrete threat to user privacy and system security. We recast LMI as reusing the LLM's own latent space and propose the Invariant Latent Space Hypothesis (ILSH): (1) diverse outputs from the same source prompt should preserve consistent semantics (source invariance), and (2) input<->output cyclic mappings should be self-consistent within a shared latent space (cyclic invariance). Accordingly, we present Inv^2A, which treats the LLM as an invariant decoder and learns only a lightweight inverse encoder that maps outputs to a denoised pseudo-representation. When multiple outputs are available, they are sparsely concatenated at the representation layer to increase information density. Training proceeds in two stages: contrastive alignment (source invariance) and supervised reinforcement (cyclic invariance). An optional training-free neighborhood search can refine local performance. Across 9 datasets covering user and system prompt scenarios, Inv^2A outperforms baselines by an average of 4.77% BLEU score while reducing dependence on large inverse corpora. Our analysis further shows that prevalent defenses provide limited protection, underscoring the need for stronger strategies. The source code and data involved in this paper can be found in https://github.com/yyy01/Invariant_Attacker.

</details>


### [60] [Learning Massively Multitask World Models for Continuous Control](https://arxiv.org/abs/2511.19584)
*Nicklas Hansen,Hao Su,Xiaolong Wang*

Main category: cs.LG

TL;DR: 提出了一个包含200个多样化任务的基准测试，并开发了Newt模型，通过大规模预训练和在线强化学习实现跨任务和具身的通用控制。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究主要关注单任务或离线学习，缺乏能在多个任务和不同具身之间进行在线交互的通用智能体。

Method: 首先在演示数据上进行预训练以获取任务感知表示和动作先验，然后通过在线交互在所有任务上进行联合优化。

Result: Newt在多任务性能和数据效率方面优于强基线方法，展现出强大的开环控制能力，并能快速适应未见过的任务。

Conclusion: 证明了通过大规模预训练和在线强化学习的组合可以训练出跨多个任务和具身的通用控制智能体。

Abstract: General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.

</details>


### [61] [Many Ways to be Right: Rashomon Sets for Concept-Based Neural Networks](https://arxiv.org/abs/2511.19636)
*Shihan Feng,Cheng Zhang,Michael Xi,Ethan Hsu,Lesia Semenova,Chudi Zhong*

Main category: cs.LG

TL;DR: 该论文提出了Rashomon概念瓶颈模型框架，通过轻量级适配器模块和多样性正则化训练目标，学习多个准确但通过不同人类可理解概念进行推理的神经网络，揭示同等性能解决方案中的推理多样性。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络存在Rashomon效应，即多个模型可以达到相同性能但依赖不同特征或推理模式。然而，在深度架构中发现这种多样性具有挑战性，因为连续参数空间包含无数数值不同但行为相似的近最优解。

Method: 结合轻量级适配器模块与多样性正则化训练目标，构建多样化的基于概念的深度模型，无需从头重新训练。该方法学习多个准确但通过不同人类可理解概念进行推理的神经网络。

Result: 生成的网络为相同预测提供根本不同的推理过程，揭示了概念依赖和决策制定在同等性能解决方案中的变化。

Conclusion: 该框架能够系统性地探索深度模型中数据驱动推理的多样性，为审计、比较和对齐同等准确解决方案提供了新机制。

Abstract: Modern neural networks rarely have a single way to be right. For many tasks, multiple models can achieve identical performance while relying on different features or reasoning patterns, a property known as the Rashomon Effect. However, uncovering this diversity in deep architectures is challenging as their continuous parameter spaces contain countless near-optimal solutions that are numerically distinct but often behaviorally similar. We introduce Rashomon Concept Bottleneck Models, a framework that learns multiple neural networks which are all accurate yet reason through distinct human-understandable concepts. By combining lightweight adapter modules with a diversity-regularized training objective, our method constructs a diverse set of deep concept-based models efficiently without retraining from scratch. The resulting networks provide fundamentally different reasoning processes for the same predictions, revealing how concept reliance and decision making vary across equally performing solutions. Our framework enables systematic exploration of data-driven reasoning diversity in deep models, offering a new mechanism for auditing, comparison, and alignment across equally accurate solutions.

</details>


### [62] [Structured Noise Modeling for Enhanced Time-Series Forecasting](https://arxiv.org/abs/2511.19657)
*Sepideh Koohfar*

Main category: cs.LG

TL;DR: 提出了一种预测-模糊-去噪框架，通过结构化噪声建模提高时间序列预测的保真度。该方法包含可学习的高斯过程模块生成平滑相关扰动，鼓励预测主干捕捉长程结构，同时专用细化模型恢复高分辨率时间细节。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的时间序列预测面临挑战，因为时间模式在多个尺度上运作，从广泛上下文趋势到驱动关键决策的快速细粒度波动。现有神经网络模型难以表示这些交互动态，导致预测不稳定和下游应用可靠性降低。

Method: 采用预测-模糊-去噪框架，包含可学习高斯过程模块生成平滑相关扰动，鼓励预测主干捕捉长程结构，同时专用细化模型恢复高分辨率时间细节。通过联合训练组件实现自然能力分工，避免各向同性破坏方法常见的人工痕迹。

Result: 在电力、交通和太阳能数据集上的实验显示，多时间范围准确性和稳定性方面均获得一致提升。模块化设计还允许模糊-去噪层作为预训练模型的轻量级增强，支持在有限数据场景下的高效适应。

Conclusion: 通过增强细粒度时间预测的可靠性和可解释性，该框架有助于构建更值得信赖的AI系统，应用于能源、基础设施和其他时间关键领域的预测驱动决策支持。

Abstract: Time-series forecasting remains difficult in real-world settings because temporal patterns operate at multiple scales, from broad contextual trends to fast, fine-grained fluctuations that drive critical decisions. Existing neural models often struggle to represent these interacting dynamics, leading to unstable predictions and reduced reliability in downstream applications. This work introduces a forecast-blur-denoise framework that improves temporal fidelity through structured noise modeling. The approach incorporates a learnable Gaussian Process module that generates smooth, correlated perturbations, encouraging the forecasting backbone to capture long-range structure while a dedicated refinement model restores high-resolution temporal detail. Training the components jointly enables natural competence division and avoids the artifacts commonly produced by isotropic corruption methods. Experiments across electricity, traffic, and solar datasets show consistent gains in multi-horizon accuracy and stability. The modular design also allows the blur-denoise layer to operate as a lightweight enhancement for pretrained models, supporting efficient adaptation in limited-data scenarios. By strengthening the reliability and interpretability of fine-scale temporal predictions, this framework contributes to more trustworthy AI systems used in forecasting-driven decision support across energy, infrastructure, and other time-critical domains.

</details>


### [63] [TREASURE: A Transformer-Based Foundation Model for High-Volume Transaction Understanding](https://arxiv.org/abs/2511.19693)
*Chin-Chia Michael Yeh,Uday Singh Saini,Xin Dai,Xiran Fan,Shubham Jain,Yujie Fan,Jiarui Sun,Junpeng Wang,Menghai Pan,Yingtong Dou,Yuzhong Chen,Vineeth Rakesh,Liang Wang,Yan Zheng,Mahashweta Das*

Main category: cs.LG

TL;DR: TREASURE是一个基于Transformer的交易数据基础模型，能同时捕捉消费者行为和支付网络信号，在异常行为检测和推荐系统方面显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 支付网络产生大量交易记录，正确建模这些数据可以实现异常行为检测和个性化体验等应用，最终改善人们的生活。

Method: 使用多用途Transformer基础模型，包含静态和动态属性的专用输入模块，以及预测高基数分类属性的高效训练范式。

Result: 作为独立模型将异常行为检测性能提升111%，作为嵌入提供者将推荐模型性能提升104%。

Conclusion: TREASURE展示了作为交易数据通用表示编码器的有效性，为准确推荐系统和异常行为检测等应用提供了全面信息。

Abstract: Payment networks form the backbone of modern commerce, generating high volumes of transaction records from daily activities. Properly modeling this data can enable applications such as abnormal behavior detection and consumer-level insights for hyper-personalized experiences, ultimately improving people's lives. In this paper, we present TREASURE, TRansformer Engine As Scalable Universal transaction Representation Encoder, a multipurpose transformer-based foundation model specifically designed for transaction data. The model simultaneously captures both consumer behavior and payment network signals (such as response codes and system flags), providing comprehensive information necessary for applications like accurate recommendation systems and abnormal behavior detection. Verified with industry-grade datasets, TREASURE features three key capabilities: 1) an input module with dedicated sub-modules for static and dynamic attributes, enabling more efficient training and inference; 2) an efficient and effective training paradigm for predicting high-cardinality categorical attributes; and 3) demonstrated effectiveness as both a standalone model that increases abnormal behavior detection performance by 111% over production systems and an embedding provider that enhances recommendation models by 104%. We present key insights from extensive ablation studies, benchmarks against production models, and case studies, highlighting valuable knowledge gained from developing TREASURE.

</details>


### [64] [TiCT: A Synthetically Pre-Trained Foundation Model for Time Series Classification](https://arxiv.org/abs/2511.19694)
*Chin-Chia Michael Yeh,Uday Singh Saini,Junpeng Wang,Xin Dai,Xiran Fan,Jiarui Sun,Yujie Fan,Yan Zheng*

Main category: cs.LG

TL;DR: TiCT是一个基于Transformer的时间序列基础模型，仅使用合成数据进行预训练，能够通过上下文学习进行时间序列分类，无需微调即可在UCR Archive上达到与有监督方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据普遍存在，但标注数据成本高昂。现有的时间序列基础模型主要关注预测任务，缺乏无需微调即可进行通用分类的模型。

Method: 提出TiCT模型：1）新颖架构，包括可扩展的基于比特的标签编码和特殊输出注意力机制处理任意类别数；2）合成预训练框架，结合Mixup启发的过程和数据增强来促进泛化和噪声不变性。

Result: 在UCR Archive上的广泛评估显示，TiCT仅使用上下文示例在推理时就能达到与最先进有监督方法竞争的性能，且无需更新任何模型权重。

Conclusion: TiCT证明了仅使用合成数据预训练的模型能够通过上下文学习有效处理时间序列分类任务，为构建通用时间序列基础模型提供了可行路径。

Abstract: The ubiquity of time series data creates a strong demand for general-purpose foundation models, yet developing them for classification remains a significant challenge, largely due to the high cost of labeled data. Foundation models capable of in-context learning (ICL) offer a powerful solution, adapting to new tasks with minimal examples and reducing the need for extensive retraining. However, prior work on large-scale time series models has predominantly focused on forecasting, leaving a critical gap for versatile, fine-tuning-free classification. To address this, we introduce TiCT (Time-series in-Context Transformer), a transformer-based model pre-trained exclusively on synthetic data to perform in-context classification. We make two primary technical contributions: 1) a novel architecture featuring a scalable bit-based label encoding and a special output attention mechanism to handle an arbitrary number of classes; and 2) a synthetic pre-training framework that combines a Mixup-inspired process with data augmentation to foster generalization and noise invariance. Extensive evaluations on the UCR Archive show that TiCT achieves competitive performance against state-of-the-art supervised methods. Crucially, this is accomplished using only in-context examples at inference time, without updating a single model weight.

</details>


### [65] [CafeQ: Calibration-free Quantization via Learned Transformations and Adaptive Rounding](https://arxiv.org/abs/2511.19705)
*Ziteng Sun,Adrian Benton,Samuel Kushnir,Asher Trockman,Vikas Singh,Suhas Diggavi,Ananda Theertha Suresh*

Main category: cs.LG

TL;DR: 提出无需校准数据的后训练量化方法，通过优化变换和自适应舍入来减少大语言模型量化误差，在Gemma 2模型上实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 标准后训练量化方法因权重中的异常值而产生大误差，现有缓解机制依赖校准数据，但在实际场景中校准数据可能不可用或受隐私限制。

Method: 设计无校准数据的量化损失代理函数，使用结构化矩阵变换处理单个矩阵，对计算图中直接交互的配对权重使用双矩阵变换和自适应舍入方法。

Result: 在Gemma 2 9B模型上，4位量化平均基准分数从61.9提升到62.4，3位量化从52.0提升到60.6，计算开销增加不到3%。

Conclusion: 该方法无需校准数据即可达到与需要校准数据的GPTQ方法相当的性能，为实际部署场景提供了可行的量化解决方案。

Abstract: Post-training quantization is an effective method for reducing the serving cost of large language models, where the standard approach is to use a round-to-nearest quantization level scheme. However, this often introduces large errors due to outliers in the weights. Proposed mitigation mechanisms include applying adaptive rounding, random rotation transformations or committing to a post-training target using calibration data. Unfortunately, this reliance on calibration data can be severely limiting in some real-world scenarios as such data may be unavailable or subject to privacy regulations. In this paper, we propose algorithms to optimize transformations and adaptive rounding without access to any calibration data. The optimization is achieved by designing a suitable proxy function for the quantization loss without calibration data. To maintain inference efficiency, we perform structured matrix transformations for single matrices. For paired weights that interact directly in the computation graph, we use dual matrix transformations and adaptive rounding methods. We conduct experiments on Gemma 2 models, and observe consistent improvement over the baselines. For Gemma 2 9B quantization, our method improves the average benchmark score from 61.9 to 62.4 for 4-bit quantization and from 52.0 to 60.6 for 3-bit quantization, while adding less than 3% of computation overhead. Furthermore, our method achieves performance comparable to the commonly used GPTQ method, which requires calibration data.

</details>


### [66] [Training-Free Active Learning Framework in Materials Science with Large Language Models](https://arxiv.org/abs/2511.19730)
*Hongchen Wang,Rafael Espinosa Castañeda,Jay R. Werber,Yao Fehlis,Edward Kim,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 提出LLM-AL框架，使用大语言模型进行主动学习，在材料科学四个数据集上相比传统机器学习模型减少70%实验次数，性能更优且更具探索性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习主动学习方法存在冷启动问题和领域特定特征工程限制，大语言模型凭借预训练知识和通用表示能力可直接从文本描述提出实验建议。

Method: 构建LLM-AL框架，采用迭代少样本设置，探索两种提示策略：简洁数值输入和扩展描述性文本，适应不同特征类型的材料科学数据集。

Result: 在所有数据集上，LLM-AL能将达到最佳候选所需的实验次数减少70%以上，始终优于传统机器学习模型，进行更广泛探索性搜索且更快达到最优解。

Conclusion: LLM-AL可作为传统主动学习管道的通用替代方案，实现更高效、可解释的实验选择，并具有LLM驱动自主发现的潜力。

Abstract: Active learning (AL) accelerates scientific discovery by prioritizing the most informative experiments, but traditional machine learning (ML) models used in AL suffer from cold-start limitations and domain-specific feature engineering, restricting their generalizability. Large language models (LLMs) offer a new paradigm by leveraging their pretrained knowledge and universal token-based representations to propose experiments directly from text-based descriptions. Here, we introduce an LLM-based active learning framework (LLM-AL) that operates in an iterative few-shot setting and benchmark it against conventional ML models across four diverse materials science datasets. We explored two prompting strategies: one using concise numerical inputs suited for datasets with more compositional and structured features, and another using expanded descriptive text suited for datasets with more experimental and procedural features to provide additional context. Across all datasets, LLM-AL could reduce the number of experiments needed to reach top-performing candidates by over 70% and consistently outperformed traditional ML models. We found that LLM-AL performs broader and more exploratory searches while still reaching the optima with fewer iterations. We further examined the stability boundaries of LLM-AL given the inherent non-determinism of LLMs and found its performance to be broadly consistent across runs, within the variability range typically observed for traditional ML approaches. These results demonstrate that LLM-AL can serve as a generalizable alternative to conventional AL pipelines for more efficient and interpretable experiment selection and potential LLM-driven autonomous discovery.

</details>


### [67] [DISCO: A Browser-Based Privacy-Preserving Framework for Distributed Collaborative Learning](https://arxiv.org/abs/2511.19750)
*Julien T. T. Vignoud,Valérian Rousset,Hugo El Guedj,Ignacio Aleman,Walid Bennaceur,Batuhan Faik Derinbay,Eduard Ďurech,Damien Gengler,Lucas Giordano,Felix Grimberg,Franziska Lippoldt,Christina Kopidaki,Jiafan Liu,Lauris Lopata,Nathan Maire,Paul Mansat,Martin Milenkoski,Emmanuel Omont,Güneş Özgün,Mina Petrović,Francesco Posa,Morgan Ridel,Giorgio Savini,Marcel Torne,Lucas Trognon,Alyssa Unell,Olena Zavertiaieva,Sai Praneeth Karimireddy,Tahseen Rabbani,Mary-Anne Hartley,Martin Jaggi*

Main category: cs.LG

TL;DR: DISCO是一个开源的分布式协作学习平台，允许非技术用户在不共享原始数据或编程知识的情况下协作构建机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 数据由于隐私、知识产权和法律限制等原因往往无法共享，这不仅分散了预测模型的统计能力，还造成了可访问性偏见，使得准确性不公平地分配给那些有资源克服这些问题的机构。

Method: DISCO的Web应用程序直接在浏览器中进行本地模型训练，支持跨平台使用（包括智能手机）。其模块化设计提供联邦学习和去中心化范式的选择，具有不同级别的隐私保证和多种权重聚合策略，支持模型个性化和偏差弹性。

Result: 开发了一个开源平台，代码库位于https://github.com/epfml/disco，展示性Web界面位于https://discolab.ai。

Conclusion: DISCO为数据共享受限的环境提供了一个实用的解决方案，通过分布式协作学习实现了在不共享原始数据的情况下构建机器学习模型，提高了模型的可访问性和公平性。

Abstract: Data is often impractical to share for a range of well considered reasons, such as concerns over privacy, intellectual property, and legal constraints. This not only fragments the statistical power of predictive models, but creates an accessibility bias, where accuracy becomes inequitably distributed to those who have the resources to overcome these concerns. We present DISCO: an open-source DIStributed COllaborative learning platform accessible to non-technical users, offering a means to collaboratively build machine learning models without sharing any original data or requiring any programming knowledge. DISCO's web application trains models locally directly in the browser, making our tool cross-platform out-of-the-box, including smartphones. The modular design of \disco offers choices between federated and decentralized paradigms, various levels of privacy guarantees and several approaches to weight aggregation strategies that allow for model personalization and bias resilience in the collaborative training. Code repository is available at https://github.com/epfml/disco and a showcase web interface at https://discolab.ai

</details>


### [68] [Scalable Data Attribution via Forward-Only Test-Time Inference](https://arxiv.org/abs/2511.19803)
*Sibo Ma,Julian Nyarko*

Main category: cs.LG

TL;DR: 提出了一种高效的数据归因方法，通过训练期间的短时梯度传播模拟训练样本的参数影响，在推理时仅需前向评估即可获得归因结果，大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统影响函数方法需要昂贵的反向传播或Hessian矩阵求逆，在现代网络中不实用。需要一种既能保持理论原则又能在实际部署中高效运行的数据归因方法。

Method: 在训练期间通过短时梯度传播模拟每个训练样本的参数影响，然后在推理时仅使用前向评估读取归因结果，将计算从推理阶段转移到模拟阶段。

Result: 在标准MLP基准测试中，该方法在LOO和LDS等标准归因指标上达到或超过TRAK等最先进基线，同时推理成本降低数个数量级。

Conclusion: 该方法结合了影响函数的理论保真度和一阶方法的可扩展性，为大型预训练模型提供了实用、实时的数据归因理论框架。

Abstract: Data attribution seeks to trace model behavior back to the training examples that shaped it, enabling debugging, auditing, and data valuation at scale. Classical influence-function methods offer a principled foundation but remain impractical for modern networks because they require expensive backpropagation or Hessian inversion at inference. We propose a data attribution method that preserves the same first-order counterfactual target while eliminating per-query backward passes. Our approach simulates each training example's parameter influence through short-horizon gradient propagation during training and later reads out attributions for any query using only forward evaluations. This design shifts computation from inference to simulation, reflecting real deployment regimes where a model may serve billions of user queries but originate from a fixed, finite set of data sources (for example, a large language model trained on diverse corpora while compensating a specific publisher such as the New York Times). Empirically, on standard MLP benchmarks, our estimator matches or surpasses state-of-the-art baselines such as TRAK on standard attribution metrics (LOO and LDS) while offering orders-of-magnitude lower inference cost. By combining influence-function fidelity with first-order scalability, our method provides a theoretical framework for practical, real-time data attribution in large pretrained models.

</details>


### [69] [Learning to Clean: Reinforcement Learning for Noisy Label Correction](https://arxiv.org/abs/2511.19808)
*Marzi Heidari,Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TL;DR: 提出RLNLC框架，将噪声标签校正建模为强化学习问题，通过策略网络迭代修正噪声标签并训练预测模型，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 噪声标签会严重降低机器学习模型的预测性能，需要有效的方法来处理噪声标签问题。

Method: 将噪声标签校正定义为强化学习问题，构建状态空间（数据和标签）、动作空间（标签修正）和奖励机制，使用actor-critic方法训练深度特征表示策略网络进行标签修正。

Result: 在多个基准数据集上的实验表明，RLNLC方法在噪声标签学习任务中持续优于现有的最先进技术。

Conclusion: 将噪声标签校正建模为强化学习问题是有效的，RLNLC框架能够显著提升噪声标签环境下的模型性能。

Abstract: The challenge of learning with noisy labels is significant in machine learning, as it can severely degrade the performance of prediction models if not addressed properly. This paper introduces a novel framework that conceptualizes noisy label correction as a reinforcement learning (RL) problem. The proposed approach, Reinforcement Learning for Noisy Label Correction (RLNLC), defines a comprehensive state space representing data and their associated labels, an action space that indicates possible label corrections, and a reward mechanism that evaluates the efficacy of label corrections. RLNLC learns a deep feature representation based policy network to perform label correction through reinforcement learning, utilizing an actor-critic method. The learned policy is subsequently deployed to iteratively correct noisy training labels and facilitate the training of the prediction model. The effectiveness of RLNLC is demonstrated through extensive experiments on multiple benchmark datasets, where it consistently outperforms existing state-of-the-art techniques for learning with noisy labels.

</details>


### [70] [Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models](https://arxiv.org/abs/2511.19822)
*Wentao Hu,Mingkuan Zhao,Shuangyong Song,Xiaoyan Zhu,Xin Lai,Jiayin Wang*

Main category: cs.LG

TL;DR: Mosaic Pruning (MoP) 是一种新的稀疏专家混合模型剪枝方法，通过结构化聚类和选择过程构建功能全面的专家集合，解决了现有方法在跨域应用时性能急剧下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有后训练剪枝方法通常基于单一通用语料库制定剪枝标准，导致剪枝模型在其他领域应用时出现灾难性性能下降，需要为每个新领域进行昂贵的重新剪枝。

Method: MoP采用"聚类-选择"结构过程，利用跨任务域的专家性能相似性度量进行功能聚类，然后基于提出的激活变异性评分从每个聚类中选择最具代表性的专家。

Result: 在各种MoE模型上的广泛实验表明，MoP显著优于先前工作，在通用任务上获得7.24%的性能提升，在数学推理和代码生成等专业任务上获得8.92%的提升。

Conclusion: Mosaic Pruning确保剪枝模型保留功能互补的专家集合，使模型能够处理多样化的下游任务，解决了剪枝模型的泛化问题。

Abstract: Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select" process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model's capabilities, enabling it to handle diverse downstream tasks.Extensive experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\% gain on general tasks and 8.92\% on specialized tasks like math reasoning and code generation.

</details>


### [71] [GED-Consistent Disentanglement of Aligned and Unaligned Substructures for Graph Similarity Learning](https://arxiv.org/abs/2511.19837)
*Zhentao Zhan,Xiaoliang Xu,Jingjing Wang,Junmei Wang*

Main category: cs.LG

TL;DR: GCGSim是一个图相似度计算框架，通过图级匹配和子结构级编辑成本来解决现有节点中心化方法的局限性，在四个基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的图编辑距离方法采用节点中心化匹配范式，这与GED的核心原则存在不匹配，导致无法捕捉全局结构对应关系和错误归因编辑成本。

Method: 提出GCGSim框架，专注于图级匹配和子结构级编辑成本，通过三个核心技术贡献实现GED一致的图相似度学习。

Result: 在四个基准数据集上的广泛实验表明，GCGSim实现了最先进的性能，框架有效学习了解缠结且语义有意义的子结构表示。

Conclusion: GCGSim通过图级匹配和子结构级编辑成本的方法，成功解决了节点中心化范式与GED原则之间的不匹配问题，为图相似度计算提供了更准确和语义有意义的解决方案。

Abstract: Graph Similarity Computation (GSC) is a fundamental graph related task where Graph Edit Distance (GED) serves as a prevalent metric. GED is determined by an optimal alignment between a pair of graphs that partitions each into aligned (zero-cost) and unaligned (cost-incurring) substructures. Due to NP-hard nature of exact GED computation, GED approximations based on Graph Neural Network(GNN) have emerged. Existing GNN-based GED approaches typically learn node embeddings for each graph and then aggregate pairwise node similarities to estimate the final similarity. Despite their effectiveness, we identify a mismatch between this prevalent node-centric matching paradigm and the core principles of GED. This discrepancy leads to two critical limitations: (1) a failure to capture the global structural correspondence for optimal alignment, and (2) a misattribution of edit costs driven by spurious node level signals. To address these limitations, we propose GCGSim, a GED-consistent graph similarity learning framework centering on graph-level matching and substructure-level edit costs. Specifically, we make three core technical contributions. Extensive experiments on four benchmark datasets show that GCGSim achieves state-of-the-art performance. Our comprehensive analyses further validate that the framework effectively learns disentangled and semantically meaningful substructure representations.

</details>


### [72] [Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization](https://arxiv.org/abs/2511.19851)
*Kun Guo,Xuefei Li,Xijun Wang,Howard H. Yang,Wei Feng,Tony Q. S. Quek*

Main category: cs.LG

TL;DR: 本文提出了一种加速混合分割与联邦学习(HSFL)的方法，通过联合优化学习模式选择、批次大小以及通信和计算资源来减少整体学习延迟。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)支持低延迟并行训练但可能收敛到较低精度模型，而分割学习(SL)通过顺序训练实现更高精度但延迟增加。HSFL结合两者的优势，但需要解决学习模式选择、批次大小优化和资源分配问题来加速训练。

Method: 首先分析收敛性，揭示学习模式和批次大小之间的相互作用；然后制定延迟最小化问题，提出两阶段解决方案：使用块坐标下降法求解松弛问题获得局部最优解，再通过舍入算法恢复整数批次大小以实现接近最优性能。

Result: 实验结果表明，与现有方法相比，该方法在达到目标精度方面显著加速了收敛过程。

Conclusion: 通过联合优化学习模式、批次大小和资源分配，HSFL能够有效平衡精度和延迟，实现更快的模型训练收敛。

Abstract: Federated learning (FL) and split learning (SL) are two effective distributed learning paradigms in wireless networks, enabling collaborative model training across mobile devices without sharing raw data. While FL supports low-latency parallel training, it may converge to less accurate model. In contrast, SL achieves higher accuracy through sequential training but suffers from increased delay. To leverage the advantages of both, hybrid split and federated learning (HSFL) allows some devices to operate in FL mode and others in SL mode. This paper aims to accelerate HSFL by addressing three key questions: 1) How does learning mode selection affect overall learning performance? 2) How does it interact with batch size? 3) How can these hyperparameters be jointly optimized alongside communication and computational resources to reduce overall learning delay? We first analyze convergence, revealing the interplay between learning mode and batch size. Next, we formulate a delay minimization problem and propose a two-stage solution: a block coordinate descent method for a relaxed problem to obtain a locally optimal solution, followed by a rounding algorithm to recover integer batch sizes with near-optimal performance. Experimental results demonstrate that our approach significantly accelerates convergence to the target accuracy compared to existing methods.

</details>


### [73] [Frailty-Aware Transformer for Recurrent Survival Modeling of Driver Retention in Ride-Hailing Platforms](https://arxiv.org/abs/2511.19893)
*Shuoyan Xu,Yu Zhang,Eric J. Miller*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的生存分析框架FACT，用于建模网约车司机的空闲行为，在时间依赖性C指数和Brier得分方面优于传统和深度学习生存模型。


<details>
  <summary>Details</summary>
Motivation: 网约车平台具有高频、行为驱动的特点，虽然生存分析已应用于其他领域的重复事件，但在建模网约车司机行为方面仍未被充分探索。

Method: 使用大规模平台数据将空闲行为建模为重复生存过程，提出基于Transformer的框架，通过因果掩码捕捉长期时间依赖性，并融入司机特定嵌入来建模潜在异质性。

Result: 在多伦多网约车数据上的结果显示，提出的FACT模型在时间依赖性C指数和Brier得分方面表现最佳，超越了经典和深度学习生存模型。

Conclusion: 该方法能够提供更准确的风险估计，支持平台留存策略，并提供政策相关的洞察。

Abstract: Ride-hailing platforms are characterized by high-frequency, behavior-driven environments. Although survival analysis has been applied to recurrent events in other domains, its use in modeling ride-hailing driver behavior remains largely unexplored. This study formulates idle behavior as a recurrent survival process using large-scale platform data and proposes a Transformer-based framework that captures long-term temporal dependencies with causal masking and incorporates driver-specific embeddings to model latent heterogeneity. Results on Toronto ride-hailing data demonstrate that the proposed Frailty-Aware Cox Transformer (FACT) achieves the highest time-dependent C-indices and lowest Brier Scores, outperforming classical and deep learning survival models. This approach enables more accurate risk estimation, supports platform retention strategies, and provides policy-relevant insights.

</details>


### [74] [EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning](https://arxiv.org/abs/2511.19935)
*Songlin Zhao,Michael Pitts,Zhuwei Qin*

Main category: cs.LG

TL;DR: EfficientXpert是一个轻量级领域剪枝框架，通过传播感知剪枝准则和高效适配器更新算法，将通用预训练模型一步转换为稀疏的领域适配专家模型，在40%稀疏度下仍能保持98%的密集模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域应用需求增长，但其大尺寸在资源受限环境中部署困难，现有压缩方法要么跨领域泛化能力差，要么开销过高。

Method: 结合传播感知剪枝准则（Foresight Mask）和高效适配器更新算法（Partial Brain Surgeon），集成到LoRA微调过程中，实现一步转换。

Result: 在健康和法律任务中，40%稀疏度下仍能保持98%的密集模型性能，优于现有最先进方法。分析显示领域依赖的结构变化会降低通用剪枝掩码的有效性。

Conclusion: 需要针对每个领域采用自适应、领域感知的剪枝策略，EfficientXpert为此提供了有效的解决方案。

Abstract: The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.

</details>


### [75] [Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning](https://arxiv.org/abs/2511.19941)
*Shenjun Zhong,Zhifeng Chen,Zhaolin Chen*

Main category: cs.LG

TL;DR: 使用强化学习优化磁共振指纹成像中的翻转角调度，提高指纹可区分性，可能减少重复时间并加速采集


<details>
  <summary>Details</summary>
Motivation: 磁共振指纹成像依赖于可调采集参数产生的瞬态信号动态，而翻转角等关键参数的优化是一个复杂的高维顺序决策问题，需要自动化参数选择方法来最大化指纹在参数空间中的可区分性

Method: 引入强化学习框架来优化磁共振指纹成像中的翻转角调度，通过RL算法自动选择参数，学习表现出非周期性模式的调度方案

Result: RL优化的调度方案展现出非周期性模式，增强了指纹的可分离性，并且可能减少重复时间，从而加速磁共振指纹成像采集

Conclusion: 强化学习为磁共振指纹成像序列优化提供了有效方法，能够自动学习出增强指纹可区分性的非周期性翻转角调度，有望提高采集效率

Abstract: Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.

</details>


### [76] [Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning](https://arxiv.org/abs/2511.19942)
*Jingchu Gai,Guanning Zeng,Huaqing Zhang,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 本文提出了一个名为"差分平滑"的原理性方法，通过理论分析和实验证明，该方法能够有效解决RL微调大语言模型时的多样性崩溃问题，同时提升正确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法解决RL微调导致的多样性崩溃问题时存在启发式、临时性的缺陷，经常需要在正确性和多样性之间权衡，且效果在不同任务间不稳定。

Method: 提出差分平滑方法，基于对正确轨迹应用奖励修改的关键观察，通过理论证明该方法能够同时改善正确性和多样性。

Result: 在1B到7B参数的模型上，在CountDown和真实世界数学推理等领域的广泛实验显示，差分平滑方法在Pass@1和Pass@k指标上均有提升，在AIME24数据集上最高提升6.7%。

Conclusion: 差分平滑方法在理论上优于现有启发式方法，实验证明其能够一致性地提升RL微调大语言模型的正确性和多样性。

Abstract: It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\% improvements on AIME24 dataset.

</details>


### [77] [Hierarchical Spatio-Temporal Attention Network with Adaptive Risk-Aware Decision for Forward Collision Warning in Complex Scenarios](https://arxiv.org/abs/2511.19952)
*Haoran Hu,Junren Shi,Shuo Jiang,Kun Cheng,Xia Yang,Changhao Piao*

Main category: cs.LG

TL;DR: 提出了一种集成前向碰撞预警框架，结合分层时空注意力网络和动态风险阈值调整算法，解决了计算复杂性和建模不足的双重挑战，在保持实时性的同时提高了预警准确性。


<details>
  <summary>Details</summary>
Motivation: 当前前向碰撞预警系统存在计算成本高、交互模型简化导致不可靠、静态阈值预警误报率高等问题，需要平衡多智能体交互建模精度与实时决策适应性。

Method: 采用分层时空注意力网络（HSTAN）进行解耦架构设计：图注意力网络处理空间交互，级联GRU与自注意力处理时序依赖；结合保形分位数回归生成预测区间，通过动态风险阈值调整算法实现自适应预警。

Result: 在NGSIM数据集上，推理时间仅需12.3ms（比Transformer方法快73%），平均位移误差降至0.73m（比Social_LSTM提升42.2%）；系统F1分数达0.912，误报率8.2%，预警提前时间2.8秒。

Conclusion: 该框架在复杂环境中展现出优越性能和实际部署可行性，有效平衡了计算效率与预警准确性，为车辆安全和自动驾驶提供了可靠的碰撞预警解决方案。

Abstract: Forward Collision Warning systems are crucial for vehicle safety and autonomous driving, yet current methods often fail to balance precise multi-agent interaction modeling with real-time decision adaptability, evidenced by the high computational cost for edge deployment and the unreliability stemming from simplified interaction models.To overcome these dual challenges-computational complexity and modeling insufficiency-along with the high false alarm rates of traditional static-threshold warnings, this paper introduces an integrated FCW framework that pairs a Hierarchical Spatio-Temporal Attention Network with a Dynamic Risk Threshold Adjustment algorithm. HSTAN employs a decoupled architecture (Graph Attention Network for spatial, cascaded GRU with self-attention for temporal) to achieve superior performance and efficiency, requiring only 12.3 ms inference time (73% faster than Transformer methods) and reducing the Average Displacement Error (ADE) to 0.73m (42.2% better than Social_LSTM) on the NGSIM dataset. Furthermore, Conformalized Quantile Regression enhances reliability by generating prediction intervals (91.3% coverage at 90% confidence), which the DTRA module then converts into timely warnings via a physics-informed risk potential function and an adaptive threshold mechanism inspired by statistical process control.Tested across multi-scenario datasets, the complete system demonstrates high efficacy, achieving an F1 score of 0.912, a low false alarm rate of 8.2%, and an ample warning lead time of 2.8 seconds, validating the framework's superior performance and practical deployment feasibility in complex environments.

</details>


### [78] [Prompt Fairness: Sub-group Disparities in LLMs](https://arxiv.org/abs/2511.19956)
*Meiyu Zhong,Noel Teku,Ravi Tandon*

Main category: cs.LG

TL;DR: 该论文研究了LLM的提示公平性问题，发现不同用户/风格的提示语会导致不同的模型响应质量，提出了信息论指标来量化偏见，并通过多数投票和提示中性化等干预措施减少响应差异。


<details>
  <summary>Details</summary>
Motivation: LLM在不同用户/风格的提示语下会产生不同的响应质量，这种提示公平性问题需要被量化和解决，以确保模型对所有用户群体都公平。

Method: 使用信息论指标（子组敏感性和跨组一致性）来量化偏见，并提出多数投票和提示中性化等干预措施来减少响应差异。

Result: 实验显示，在缓解前跨组差异值达到0.28，通常在0.14-0.22范围内；应用中性化和多代策略后，差异持续减少，最大差距降至0.22，许多距离降至0.17或以下。

Conclusion: 通过提出的干预措施可以有效减少LLM在不同用户群体间的响应差异，提高模型的公平性和稳定性。

Abstract: Large Language Models (LLMs), though shown to be effective in many applications, can vary significantly in their response quality. In this paper, we investigate this problem of prompt fairness: specifically, the phrasing of a prompt by different users/styles, despite the same question being asked in principle, may elicit different responses from an LLM. To quantify this disparity, we propose to use information-theoretic metrics that can capture two dimensions of bias: subgroup sensitivity, the variability of responses within a subgroup and cross group consistency, the variability of responses across subgroups. Our analysis reveals that certain subgroups exhibit both higher internal variability and greater divergence from others. Our empirical analysis reveals that certain demographic sub groups experience both higher internal variability and greater divergence from others, indicating structural inequities in model behavior. To mitigate these disparities, we propose practical interventions, including majority voting across multiple generations and prompt neutralization, which together improve response stability and enhance fairness across user populations. In the experiments, we observe clear prompt sensitivity disparities across demographic subgroups: before mitigation, cross-group divergence values reach 0.28 and typically fall in the from 0.14 to 0.22 range. After applying our neutralization and multi generation strategy, these divergences consistently decrease, with the largest gap reduced to 0.22 and many distances falling to 0.17 or below, indicating more stable and consistent outputs across subgroups.

</details>


### [79] [ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models](https://arxiv.org/abs/2511.19959)
*Yujia Wang,Yuanpu Cao,Jinghui Chen*

Main category: cs.LG

TL;DR: ParaBlock是一种用于联邦学习大语言模型的新方法，通过并行通信和计算线程来提高通信效率，同时保持与标准联邦块坐标下降方法相同的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习大语言模型时代，即使单个块也包含大量参数，给资源受限的客户端带来显著的通信延迟挑战。

Method: 建立两个并行线程分别处理通信和计算，以增强通信效率。

Result: 在通用指令跟随和数学推理任务上的实验表明，ParaBlock不仅保持强大性能，还显著提高通信效率。

Conclusion: ParaBlock成功解决了联邦训练/微调大语言模型中的通信效率问题，同时保持理论收敛性能。

Abstract: Federated learning (FL) has been extensively studied as a privacy-preserving training paradigm. Recently, federated block coordinate descent scheme has become a popular option in training large-scale models, as it allows clients to train only a subset of the model locally instead of the entire model. However, in the era of large language models (LLMs), even a single block can contain a significant number of parameters, posing substantial communication latency, particularly for resource-constrained clients. To address this challenge in federated training/fine-tuning LLMs, we propose ParaBlock, a novel approach that establishes two parallel threads for communication and computation to enhance communication efficiency. We theoretically prove that the proposed ParaBlock achieves the same convergence rate as the standard federated block coordinate descent methods. Empirical evaluations on fine-tuning LLMs on general instruction following and mathematical reasoning confirm that ParaBlock not only maintains strong performance but also significantly improves communication efficiency.

</details>


### [80] [Stragglers Can Contribute More: Uncertainty-Aware Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.19966)
*Yujia Wang,Fenglong Ma,Jinghui Chen*

Main category: cs.LG

TL;DR: FedEcho是一个异步联邦学习框架，通过不确定性感知蒸馏技术解决过时更新和数据异质性带来的挑战，在不访问私有客户端数据的情况下提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习虽然提高了效率和可扩展性，但面临过时更新和快速客户端主导学习过程的偏差问题，现有方法往往只能解决其中一个问题。

Method: 采用不确定性感知蒸馏技术，服务器评估滞后客户端预测的可靠性，根据估计的不确定性动态调整这些预测的影响力。

Result: FedEcho在大量异步延迟和数据异质性条件下，始终优于现有的异步联邦学习基线方法。

Conclusion: FedEcho通过优先处理更确定的预测同时利用所有客户端的多样化信息，有效减轻了过时更新和数据异质性的负面影响。

Abstract: Asynchronous federated learning (FL) has recently gained attention for its enhanced efficiency and scalability, enabling local clients to send model updates to the server at their own pace without waiting for slower participants. However, such a design encounters significant challenges, such as the risk of outdated updates from straggler clients degrading the overall model performance and the potential bias introduced by faster clients dominating the learning process, especially under heterogeneous data distributions. Existing methods typically address only one of these issues, creating a conflict where mitigating the impact of outdated updates can exacerbate the bias created by faster clients, and vice versa. To address these challenges, we propose FedEcho, a novel framework that incorporates uncertainty-aware distillation to enhance the asynchronous FL performances under large asynchronous delays and data heterogeneity. Specifically, uncertainty-aware distillation enables the server to assess the reliability of predictions made by straggler clients, dynamically adjusting the influence of these predictions based on their estimated uncertainty. By prioritizing more certain predictions while still leveraging the diverse information from all clients, FedEcho effectively mitigates the negative impacts of outdated updates and data heterogeneity. Through extensive experiments, we demonstrate that FedEcho consistently outperforms existing asynchronous federated learning baselines, achieving robust performance without requiring access to private client data.

</details>


### [81] [Rethinking Semi-Supervised Node Classification with Self-Supervised Graph Clustering](https://arxiv.org/abs/2511.19976)
*Songbo Wang,Renchi Yang,Yurui Lai,Xiaoyang Lin,Tsz Nam Chan*

Main category: cs.LG

TL;DR: NCGC框架将自监督图聚类与半监督分类结合，通过软正交GNN生成节点表示，利用聚类模块为未标记节点提供自监督训练，在多任务目标下提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实图中的节点倾向于形成紧密社区/簇，这些簇包含丰富信号可补偿半监督节点分类中的标签稀缺，但现有方法未充分利用这一特性。

Method: 提出NCGC框架：1）理论统一GNN和谱图聚类的优化目标，开发软正交GNN；2）包含自监督图聚类模块，使用两个聚类目标和Sinkhorn-Knopp归一化；3）通过多任务目标结合分类和聚类损失。

Result: 在七个真实图上进行广泛实验，NCGC框架在使用各种经典GNN骨干时，始终显著优于流行的GNN模型和近期基线。

Conclusion: NCGC通过整合自监督聚类和半监督分类，促进了二者之间的协同作用，增强了模型能力，为半监督节点分类提供了有效解决方案。

Abstract: The emergence of graph neural networks (GNNs) has offered a powerful tool for semi-supervised node classification tasks. Subsequent studies have achieved further improvements through refining the message passing schemes in GNN models or exploiting various data augmentation techniques to mitigate limited supervision. In real graphs, nodes often tend to form tightly-knit communities/clusters, which embody abundant signals for compensating label scarcity in semi-supervised node classification but are not explored in prior methods.
  Inspired by this, this paper presents NCGC that integrates self-supervised graph clustering and semi-supervised classification into a unified framework. Firstly, we theoretically unify the optimization objectives of GNNs and spectral graph clustering, and based on that, develop soft orthogonal GNNs (SOGNs) that leverage a refined message passing paradigm to generate node representations for both classification and clustering. On top of that, NCGC includes a self-supervised graph clustering module that enables the training of SOGNs for learning representations of unlabeled nodes in a self-supervised manner. Particularly, this component comprises two non-trivial clustering objectives and a Sinkhorn-Knopp normalization that transforms predicted cluster assignments into balanced soft pseudo-labels. Through combining the foregoing clustering module with the classification model using a multi-task objective containing the supervised classification loss on labeled data and self-supervised clustering loss on unlabeled data, NCGC promotes synergy between them and achieves enhanced model capacity. Our extensive experiments showcase that the proposed NCGC framework consistently and considerably outperforms popular GNN models and recent baselines for semi-supervised node classification on seven real graphs, when working with various classic GNN backbones.

</details>


### [82] [Operator Learning at Machine Precision](https://arxiv.org/abs/2511.19980)
*Aras Bacho,Aleksei G. Sorokin,Xianjin Yang,Théo Bourdais,Edoardo Calvello,Matthieu Darcy,Alexander Hsu,Bamdad Hosseini,Houman Owhadi*

Main category: cs.LG

TL;DR: CHONKNORIS是一种基于牛顿-康托罗维奇方法的神经算子学习范式，通过回归Tikhonov正则化牛顿更新的Cholesky因子，而非直接逼近解算子，能够达到机器精度。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子学习方法即使增加复杂度也难以显著提高精度，与更简单的核方法和降阶模型相当，需要解决这一局限性。

Method: 回归与Tikhonov正则化牛顿-康托罗维奇更新相关的椭圆算子的Cholesky因子，通过展开迭代构建神经架构，利用收缩映射实现机器精度。

Result: 在非线性椭圆方程、Burgers方程、非线性达西流、Calderón问题、逆波散射问题和地震成像等多个非线性正反问题中表现出色。

Conclusion: CHONKNORIS能够达到机器精度，并提出了基础模型变体FONKNORIS，能够准确求解未见过的非线性PDE如Klein-Gordon和Sine-Gordon方程。

Abstract: Neural operator learning methods have garnered significant attention in scientific computing for their ability to approximate infinite-dimensional operators. However, increasing their complexity often fails to substantially improve their accuracy, leaving them on par with much simpler approaches such as kernel methods and more traditional reduced-order models. In this article, we set out to address this shortcoming and introduce CHONKNORIS (Cholesky Newton--Kantorovich Neural Operator Residual Iterative System), an operator learning paradigm that can achieve machine precision. CHONKNORIS draws on numerical analysis: many nonlinear forward and inverse PDE problems are solvable by Newton-type methods. Rather than regressing the solution operator itself, our method regresses the Cholesky factors of the elliptic operator associated with Tikhonov-regularized Newton--Kantorovich updates. The resulting unrolled iteration yields a neural architecture whose machine-precision behavior follows from achieving a contractive map, requiring far lower accuracy than end-to-end approximation of the solution operator. We benchmark CHONKNORIS on a range of nonlinear forward and inverse problems, including a nonlinear elliptic equation, Burgers' equation, a nonlinear Darcy flow problem, the Calderón problem, an inverse wave scattering problem, and a problem from seismic imaging. We also present theoretical guarantees for the convergence of CHONKNORIS in terms of the accuracy of the emulated Cholesky factors. Additionally, we introduce a foundation model variant, FONKNORIS (Foundation Newton--Kantorovich Neural Operator Residual Iterative System), which aggregates multiple pre-trained CHONKNORIS experts for diverse PDEs to emulate the solution map of a novel nonlinear PDE. Our FONKNORIS model is able to accurately solve unseen nonlinear PDEs such as the Klein--Gordon and Sine--Gordon equations.

</details>


### [83] [Rethinking Message Passing Neural Networks with Diffusion Distance-guided Stress Majorization](https://arxiv.org/abs/2511.19984)
*Haoran Zheng,Renchi Yang,Yubo Zhou,Jianliang Xu*

Main category: cs.LG

TL;DR: 提出了DDSM模型，这是一种基于优化框架的新型消息传递神经网络，通过应力优化和正交正则化解决过平滑和过相关问题，并在同质和异质图上显著优于15个基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统MPNN模型由于最小化Dirichlet能量和邻域聚合操作，存在严重的过平滑和过相关问题，需要新的优化框架来克服这些限制。

Method: 基于应力优化和正交正则化的优化框架，引入扩散距离指导新的消息传递操作，并开发了高效的距离近似算法。

Result: DDSM在同质和异质图上一致且显著地优于15个强基线模型。

Conclusion: DDSM通过新的优化框架有效解决了MPNN中的过平滑和过相关问题，在多种图结构上表现出优越性能。

Abstract: Message passing neural networks (MPNNs) have emerged as go-to models for learning on graph-structured data in the past decade. Despite their effectiveness, most of such models still incur severe issues such as over-smoothing and -correlation, due to their underlying objective of minimizing the Dirichlet energy and the derived neighborhood aggregation operations. In this paper, we propose the DDSM, a new MPNN model built on an optimization framework that includes the stress majorization and orthogonal regularization for overcoming the above issues. Further, we introduce the diffusion distances for nodes into the framework to guide the new message passing operations and develop efficient algorithms for distance approximations, both backed by rigorous theoretical analyses. Our comprehensive experiments showcase that DDSM consistently and considerably outperforms 15 strong baselines on both homophilic and heterophilic graphs.

</details>


### [84] [On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices](https://arxiv.org/abs/2511.19986)
*Lianming Huang,Haibo Hu,Qiao Li,Nan Guan,Chun Jason Xue*

Main category: cs.LG

TL;DR: 提出了一种按需多任务稀疏框架，通过最大化参数重用和块粒度分解来最小化任务切换成本，在自动驾驶平台上实现了6.6倍的任务切换加速。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏方法针对单个任务优化，忽略了频繁任务切换时的显著I/O开销，需要专门设计来最小化切换成本。

Method: 将权重分解为可重用的块粒度单元，跨任务对齐稀疏结构以最大化重叠，动态加载下一个任务所需的小型差异块集。

Result: 在真实自动驾驶平台上的实验表明，相比现有稀疏方法，该框架平均加速任务切换超过6.6倍。

Conclusion: 该按需多任务稀疏框架通过参数重用和动态块加载，有效缓解了传统单体方法的冷启动延迟，显著提升了任务切换效率。

Abstract: Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.

</details>


### [85] [RankOOD -- Class Ranking-based Out-of-Distribution Detection](https://arxiv.org/abs/2511.19996)
*Dishanika Denipitiyage,Naveen Karunanayake,Suranga Seneviratne,Sanjay Chawla*

Main category: cs.LG

TL;DR: RankOOD是一种基于排序的OOD检测方法，通过Plackett-Luce损失训练模型，利用ID类预测中的排序模式来识别OOD样本。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习中交叉熵损失训练模型时，ID类预测会诱导出每个ID类的排序模式，OOD样本可能被分配高概率但不太可能遵循排序分类。

Method: 首先使用初始分类器提取每个类的排序列表，然后使用Plackett-Luce损失进行第二轮训练，其中类排序作为预测变量。

Result: 在TinyImageNet近OOD评估基准上达到SOTA性能，将FPR95降低了4.3%。

Conclusion: RankOOD框架有效利用排序模式进行OOD检测，在基准测试中表现出优越性能。

Abstract: We propose RankOOD, a rank-based Out-of-Distribution (OOD) detection approach based on training a model with the Placket-Luce loss, which is now extensively used for preference alignment tasks in foundational models. Our approach is based on the insight that with a deep learning model trained using the Cross Entropy Loss, in-distribution (ID) class prediction induces a ranking pattern for each ID class prediction. The RankOOD framework formalizes the insight by first extracting a rank list for each class using an initial classifier and then uses another round of training with the Plackett-Luce loss, where the class rank, a fixed permutation for each class, is the predicted variable. An OOD example may get assigned with high probability to an ID example, but the probability of it respecting the ranking classification is likely to be small. RankOOD, achieves SOTA performance on the near-ODD TinyImageNet evaluation benchmark, reducing FPR95 by 4.3%.

</details>


### [86] [REWA: Witness-Overlap Theory -- Foundations for Composable Binary Similarity Systems](https://arxiv.org/abs/2511.19998)
*Nikit Phadke*

Main category: cs.LG

TL;DR: REWA提出了一种基于见证集重叠的通用相似性理论，可将各种相似性定义统一编码为紧凑的位表示，并保持top-k排序的数学保证。


<details>
  <summary>Details</summary>
Motivation: 现有相似性方法（如Bloom过滤器、minhash等）缺乏统一的理论框架，REWA旨在为基于见证集重叠的相似性系统提供理论基础，实现模块化设计和可组合性。

Method: 使用有限见证集、半随机位分配和重叠单调性三个组件构建系统，证明在重叠间隙条件下，仅需O(log(|V|/δ))位即可保持top-k排序。

Result: REWA统一了多种相似性方法，提供了对数复杂度的编码方案，支持从结构、时序、因果等多种变换构建模块化相似性系统。

Conclusion: REWA为相似性系统提供了基于见证集重叠的数学基础，实现了从哈希函数工程到理论指导的转变，具有广泛的应用前景和扩展性。

Abstract: REWA introduces a general theory of similarity based on witness-overlap structures. We show that whenever similarity between concepts can be expressed as monotone witness overlap -- whether arising from graph neighborhoods, causal relations, temporal structure, topological features, symbolic patterns, or embedding-based neighborhoods -- it admits a reduction to compact encodings with provable ranking preservation guarantees. REWA systems consist of: (1) finite witness sets $W(v)$, (2) semi-random bit assignments generated from each witness, and (3) monotonicity of expected similarity in the overlap $Δ(u, v) = |W(u) \cap W(v)|$. We prove that under an overlap-gap condition on the final witness sets -- independent of how they were constructed -- top-$k$ rankings are preserved using $m = O(\log(|V|/δ))$ bits. The witness-set formulation is compositional: any sequence of structural, temporal, causal, topological, information-theoretic, or learned transformations can be combined into pipelines that terminate in discrete witness sets. The theory applies to the final witness overlap, enabling modular construction of similarity systems from reusable primitives. This yields a vast design space: millions of composable similarity definitions inherit logarithmic encoding complexity. REWA subsumes and unifies Bloom filters, minhash, LSH bitmaps, random projections, sketches, and hierarchical filters as special cases. It provides a principled foundation for similarity systems whose behavior is governed by witness overlap rather than hash-function engineering. This manuscript presents the axioms, the main reducibility theorem, complete proofs with explicit constants, and a detailed discussion of compositional design, limitations, and future extensions including multi-bit encodings, weighted witnesses, and non-set representations.

</details>


### [87] [Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting](https://arxiv.org/abs/2511.20004)
*Peining Zhang,Hongchen Qin,Haochen Zhang,Ziqi Guo,Guiling Wang,Jinbo Bi*

Main category: cs.LG

TL;DR: 零样本时间序列基础模型在LAI预测中表现优异，当输入上下文窗口足够长时，Sundial模型能超越专门训练的LSTM模型。


<details>
  <summary>Details</summary>
Motivation: 研究时间序列基础模型在农业监测中零样本预测叶面积指数(LAI)的能力，探索通用基础模型是否能在不进行任务特定调优的情况下超越专门监督模型。

Method: 使用HiQ数据集(美国，2000-2022)，系统比较统计基线、全监督LSTM和Sundial基础模型在多种评估协议下的表现。

Result: Sundial在零样本设置下，当输入上下文窗口覆盖超过1-2个完整季节周期时，能够超越全训练LSTM模型。

Conclusion: 通用基础模型首次在遥感时间序列预测中超越专门监督模型，无需任务特定调优，展现了预训练时间序列基础模型在农业环境应用中作为即插即用预测器的巨大潜力。

Abstract: This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.

</details>


### [88] [Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering](https://arxiv.org/abs/2511.20030)
*Haoran Zheng,Renchi Yang,Hongtao Wang,Jianliang Xu*

Main category: cs.LG

TL;DR: 提出Dual Graph Filtering (DGF)方案，用于多模态属性图的聚类，通过特征去噪和三交叉对比学习克服现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法过度依赖视图间高相关性，忽略了多模态属性中低模态相关性和强特征噪声的问题，导致聚类性能不佳。

Method: 提出DGF方案，包含特征去噪组件和三交叉对比训练策略，通过跨模态、邻域和社区的实例级对比学习获得鲁棒节点表示。

Result: 在8个基准MMAG数据集上的实验表明，DGF在聚类质量上显著优于多种最先进的基线方法。

Conclusion: DGF通过有效处理多模态属性中的噪声和低相关性，在多模态图聚类任务中取得了优越性能。

Abstract: Multimodal Attributed Graphs (MMAGs) are an expressive data model for representing the complex interconnections among entities that associate attributes from multiple data modalities (text, images, etc.). Clustering over such data finds numerous practical applications in real scenarios, including social community detection, medical data analytics, etc. However, as revealed by our empirical studies, existing multi-view clustering solutions largely rely on the high correlation between attributes across various views and overlook the unique characteristics (e.g., low modality-wise correlation and intense feature-wise noise) of multimodal attributes output by large pre-trained language and vision models in MMAGs, leading to suboptimal clustering performance.
  Inspired by foregoing empirical observations and our theoretical analyses with graph signal processing, we propose the Dual Graph Filtering (DGF) scheme, which innovatively incorporates a feature-wise denoising component into node representation learning, thereby effectively overcoming the limitations of traditional graph filters adopted in the extant multi-view graph clustering approaches. On top of that, DGF includes a tri-cross contrastive training strategy that employs instance-level contrastive learning across modalities, neighborhoods, and communities for learning robust and discriminative node representations. Our comprehensive experiments on eight benchmark MMAG datasets exhibit that DGF is able to outperform a wide range of state-of-the-art baselines consistently and significantly in terms of clustering quality measured against ground-truth labels.

</details>


### [89] [RED-F: Reconstruction-Elimination based Dual-stream Contrastive Forecasting for Multivariate Time Series Anomaly Prediction](https://arxiv.org/abs/2511.20044)
*PengYu Chen,Xiaohou Shi,Yuan Chang,Yan Sun,Sajal K. Das*

Main category: cs.LG

TL;DR: 提出了RED-F框架，通过重构消除和双流对比预测来增强微弱异常前兆信号的检测能力，解决现有方法在预测时被正常模式主导的问题。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法在训练时仅使用正常数据，导致预测时倾向于重构正常模式，淹没微弱的异常前兆信号，难以实现有效的异常预测。

Method: 提出RED-F框架，包含重构消除模型(REM)和双流对比预测模型(DFM)。REM使用时频混合机制消除前兆生成纯净基线，DFM通过对比两条预测流的差异来放大前兆信号，并采用多序列预测目标增强预测敏感性。

Result: 在六个真实世界数据集上的广泛实验表明，RED-F在异常预测任务中表现出优越性能。

Conclusion: RED-F通过将困难的绝对信号检测转化为更简单的相对轨迹比较，有效解决了微弱异常前兆预测的挑战，显著提升了异常预测能力。

Abstract: The proactive prediction of anomalies (AP) in multivariate time series (MTS) is a critical challenge to ensure system dependability. The difficulty lies in identifying subtle anomaly precursors concealed within normal signals. However, existing unsupervised methods, trained exclusively on normal data, demonstrate a fundamental propensity to reconstruct normal patterns. Consequently, when confronted with weak precursors, their predictions are dominated by the normal pattern, submerging the very signal required for prediction. To contend with the limitation, we propose RED-F, a Reconstruction-Elimination based Dual-stream Contrastive Forecasting framework, comprising the Reconstruction-Elimination Model (REM) and the Dual-stream Contrastive Forecasting Model (DFM). The REM utilizes a hybrid time-frequency mechanism to mitigate the precursor, generating a purified, normal-pattern baseline. The DFM then receives this purified baseline and the original sequence which retains the precursor as parallel inputs. At the core of our framework, RED-F employs a contrastive forecast that transforms the difficult task of absolute signal detection into a simpler, more robust task of relative trajectory comparison by computing the divergence between these two predictive streams. This contrastive mechanism serves to amplify the faint precursor signal. Furthermore, the DFM is trained with a novel Multi-Series Prediction (MSP) objective, which leverages distant future context to enhance its predictive sensitivity. Extensive experiments on six real-world datasets demonstrate the superior capability of RED-F in anomaly prediction tasks.

</details>


### [90] [SOMBRL: Scalable and Optimistic Model-Based RL](https://arxiv.org/abs/2511.20066)
*Bhavya Sukhija,Lenart Treven,Carmelo Sferrazza,Florian Dörfler,Pieter Abbeel,Andreas Krause*

Main category: cs.LG

TL;DR: SOMBRL是一种基于不确定性乐观原则的模型强化学习方法，通过不确定性感知的动态模型和贪婪优化外部奖励与认知不确定性的加权和来实现高效探索。


<details>
  <summary>Details</summary>
Motivation: 解决基于模型的强化学习（MBRL）中系统动态未知且必须从在线交互中学习时的有效探索挑战。

Method: 学习不确定性感知的动态模型，贪婪地最大化外部奖励和智能体认知不确定性的加权和，兼容任何策略优化器或规划器。

Result: 在非线性动态系统的有限时域、折扣无限时域和非情景设置中具有次线性遗憾，在状态和视觉控制环境中表现优于基线方法，在动态RC汽车硬件上超越现有最优方法。

Conclusion: SOMBRL为基于模型的强化学习提供了灵活、可扩展的原则性探索解决方案，展示了原则性探索在MBRL中的优势。

Abstract: We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.

</details>


### [91] [QiMeng-CRUX: Narrowing the Gap between Natural Language and Verilog via Core Refined Understanding eXpression](https://arxiv.org/abs/2511.20099)
*Lei Huang,Rui Zhang,Jiaming Guo,Yang Zhang,Di Huang,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 提出了CRUX结构化中间表示空间和两阶段训练框架CRUX-V，用于改进从自然语言到Verilog代码的生成，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言的硬件描述方法存在模糊、冗余和非结构化问题，难以生成精确的Verilog代码，需要弥合开放自然语言空间与受限目标空间之间的差距。

Method: 引入CRUX结构化中间表示空间捕获用户意图语义，设计包含联合表达建模和双空间优化的两阶段训练框架CRUX-V。

Result: 在多个Verilog生成基准测试中达到最先进性能，特别是在复杂设计任务中表现优异，CRUX表示对其他代码模型也具有可迁移性。

Conclusion: CRUX有效缩小了自由形式自然语言描述与精确Verilog生成之间的差距，结构化中间表示空间具有重要价值。

Abstract: Large language models (LLMs) have shown promising capabilities in hardware description language (HDL) generation. However, existing approaches often rely on free-form natural language descriptions that are often ambiguous, redundant, and unstructured, which poses significant challenges for downstream Verilog code generation. We treat hardware code generation as a complex transformation from an open-ended natural language space to a domain-specific, highly constrained target space. To bridge this gap, we introduce Core Refined Understanding eXpression (CRUX), a structured intermediate space that captures the essential semantics of user intent while organizing the expression for precise Verilog code generation. We further design a two-stage training framework, comprising Joint Expression Modeling and Dual-Space Optimization, to enhance the quality of both CRUX and Verilog code. Experiments across multiple Verilog generation benchmarks demonstrate that our model, CRUX-V, achieves state-of-the-art performance among general models, particularly under challenging design tasks. Furthermore, the CRUX space proves transferable and beneficial when used as input prompts for other code models, highlighting its effectiveness in narrowing the gap between free-form natural language descriptions and precise Verilog generation.

</details>


### [92] [The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs](https://arxiv.org/abs/2511.20104)
*Craig Dickson*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed "emergent misalignment" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales.
  We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%.
  We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's 'degrees of freedom' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.

</details>


### [93] [Multivariate Forecasting of Bitcoin Volatility with Gradient Boosting: Deterministic, Probabilistic, and Feature Importance Perspectives](https://arxiv.org/abs/2511.20105)
*Grzegorz Dudek,Mateusz Kasprzyk,Paweł Pełka*

Main category: cs.LG

TL;DR: 本研究应用LGBM模型对比特币已实现波动率进行确定性和概率性预测，使用69个市场、行为和宏观经济指标作为预测因子，并比较了计量经济学和机器学习基准模型。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场具有非线性和高方差特性，需要有效的波动率预测方法来捕捉这些复杂动态，同时提供对波动驱动因素的可解释见解。

Method: 使用LGBM模型进行确定性和概率性预测，探索两种分位数方法：基于pinball损失函数的直接分位数回归，以及将点预测转换为预测分布的残差模拟方法。采用增益基和置换特征重要性技术识别主要波动驱动因素。

Result: LGBM模型有效捕捉了加密货币市场的非线性和高方差特性，交易量、滞后波动率指标、投资者关注度和市值被一致识别为重要波动驱动因素。

Conclusion: LGBM模型在比特币波动率预测中表现优异，不仅能提供准确的预测，还能通过特征重要性分析提供对波动动态的可解释见解。

Abstract: This study investigates the application of the Light Gradient Boosting Machine (LGBM) model for both deterministic and probabilistic forecasting of Bitcoin realized volatility. Utilizing a comprehensive set of 69 predictors -- encompassing market, behavioral, and macroeconomic indicators -- we evaluate the performance of LGBM-based models and compare them with both econometric and machine learning baselines. For probabilistic forecasting, we explore two quantile-based approaches: direct quantile regression using the pinball loss function, and a residual simulation method that transforms point forecasts into predictive distributions. To identify the main drivers of volatility, we employ gain-based and permutation feature importance techniques, consistently highlighting the significance of trading volume, lagged volatility measures, investor attention, and market capitalization. The results demonstrate that LGBM models effectively capture the nonlinear and high-variance characteristics of cryptocurrency markets while providing interpretable insights into the underlying volatility dynamics.

</details>


### [94] [CLIMATEAGENT: Multi-Agent Orchestration for Complex Climate Data Science Workflows](https://arxiv.org/abs/2511.20109)
*Hyeonjae Kim,Chenyue Li,Wen Deng,Mengxi Jin,Wen Huang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: ClimateAgent是一个自主多代理框架，通过协调多个专业代理来端到端执行气候数据分析工作流，在85个真实世界气候任务中实现100%完成率和8.32的报告质量得分


<details>
  <summary>Details</summary>
Motivation: 通用LLM代理和静态脚本管道缺乏气候特定上下文和灵活性，在实际应用中表现不佳，需要专门的气候数据分析自动化解决方案

Method: 采用多代理架构：Orchestrate-Agent和Plan-Agent协调任务分解，Data-Agent动态内省API合成下载脚本，Coding-Agent生成Python代码、可视化和报告，并包含自校正循环

Result: 在Climate-Agent-Bench-85基准测试中，ClimateAgent实现100%任务完成率和8.32报告质量得分，显著优于GitHub-Copilot(6.27)和GPT-5基线(3.26)

Conclusion: 多代理协调、动态API感知和自校正执行显著提升了气候科学分析任务的可靠端到端自动化能力

Abstract: Climate science demands automated workflows to transform comprehensive questions into data-driven statements across massive, heterogeneous datasets. However, generic LLM agents and static scripting pipelines lack climate-specific context and flexibility, thus, perform poorly in practice. We present ClimateAgent, an autonomous multi-agent framework that orchestrates end-to-end climate data analytic workflows. ClimateAgent decomposes user questions into executable sub-tasks coordinated by an Orchestrate-Agent and a Plan-Agent; acquires data via specialized Data-Agents that dynamically introspect APIs to synthesize robust download scripts; and completes analysis and reporting with a Coding-Agent that generates Python code, visualizations, and a final report with a built-in self-correction loop. To enable systematic evaluation, we introduce Climate-Agent-Bench-85, a benchmark of 85 real-world tasks spanning atmospheric rivers, drought, extreme precipitation, heat waves, sea surface temperature, and tropical cyclones. On Climate-Agent-Bench-85, ClimateAgent achieves 100% task completion and a report quality score of 8.32, outperforming GitHub-Copilot (6.27) and a GPT-5 baseline (3.26). These results demonstrate that our multi-agent orchestration with dynamic API awareness and self-correcting execution substantially advances reliable, end-to-end automation for climate science analytic tasks.

</details>


### [95] [IDAP++: Advancing Divergence-Based Pruning via Filter-Level and Layer-Level Optimization](https://arxiv.org/abs/2511.20141)
*Aleksei Samarin,Artem Nazarenko,Egor Kotenko,Valentin Malykh,Alexander Savelev,Aleksei Toropov*

Main category: cs.LG

TL;DR: 提出基于信息流分析的神经网络压缩统一框架，通过张量流散度量化信息转换，实现滤波器级和架构级冗余消除的两阶段优化。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在滤波器和架构层面的冗余问题，为不同架构提供统一的压缩度量标准，适应资源受限环境的部署需求。

Method: 两阶段优化：第一阶段使用迭代散度感知剪枝移除冗余滤波器；第二阶段分析层间信息传播贡献，选择性消除对性能影响最小的整个层。

Result: 在多种现代架构和数据集上实现显著模型压缩，同时保持竞争力精度，参数减少效果与最先进方法相当且在卷积网络到变换器等架构上表现更优。

Conclusion: 流散度作为滤波器级和层级优化的有效指导原则，为资源受限环境部署提供实用价值。

Abstract: This paper presents a novel approach to neural network compression that addresses redundancy at both the filter and architectural levels through a unified framework grounded in information flow analysis. Building on the concept of tensor flow divergence, which quantifies how information is transformed across network layers, we develop a two-stage optimization process. The first stage employs iterative divergence-aware pruning to identify and remove redundant filters while preserving critical information pathways. The second stage extends this principle to higher-level architecture optimization by analyzing layer-wise contributions to information propagation and selectively eliminating entire layers that demonstrate minimal impact on network performance. The proposed method naturally adapts to diverse architectures, including convolutional networks, transformers, and hybrid designs, providing a consistent metric for comparing the structural importance across different layer types. Experimental validation across multiple modern architectures and datasets reveals that this combined approach achieves substantial model compression while maintaining competitive accuracy. The presented approach achieves parameter reduction results that are globally comparable to those of state-of-the-art solutions and outperforms them across a wide range of modern neural network architectures, from convolutional models to transformers. The results demonstrate how flow divergence serves as an effective guiding principle for both filter-level and layer-level optimization, offering practical benefits for deployment in resource-constrained environments.

</details>


### [96] [On the Limits of Momentum in Decentralized and Federated Optimization](https://arxiv.org/abs/2511.20168)
*Riccardo Zaccone,Sai Praneeth Karimireddy,Carlo Masone*

Main category: cs.LG

TL;DR: 该论文分析了联邦学习中动量方法在循环客户端参与下的表现，证明动量无法克服统计异质性问题，且步长衰减快于Θ(1/t)会导致收敛到依赖初始化和异质性边界的常数值。


<details>
  <summary>Details</summary>
Motivation: 探索动量方法在联邦学习分布式SGD中的应用，特别是在统计异质性环境下，动量是否能够保证收敛性能。

Method: 理论分析动量方法在循环客户端参与场景下的收敛行为，并通过数值实验和深度学习实验验证理论结果。

Result: 动量方法在统计异质性下仍然受到影响，步长衰减快于Θ(1/t)会导致收敛到依赖初始化和异质性边界的常数值。

Conclusion: 动量方法无法完全解决联邦学习中的统计异质性问题，需要新的方法来处理这种挑战。

Abstract: Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $Θ\left(1/t\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.

</details>


### [97] [AdaCap: An Adaptive Contrastive Approach for Small-Data Neural Networks](https://arxiv.org/abs/2511.20170)
*Bruno Belucci,Karim Lounici,Katia Meziani*

Main category: cs.LG

TL;DR: AdaCap是一种结合排列对比损失和Tikhonov闭式输出映射的训练方案，在小样本表格数据上显著提升神经网络性能，特别是残差模型。


<details>
  <summary>Details</summary>
Motivation: 神经网络在小样本表格数据集上表现不佳，而树模型仍占主导地位，需要改进神经网络在此类场景下的性能。

Method: 提出AdaCap训练方案，结合排列对比损失和Tikhonov闭式输出映射，通过元预测器基于数据集特征预测AdaCap的适用性。

Result: 在85个真实世界回归数据集上，AdaCap在小样本场景下带来一致且统计显著的改进，特别是对残差模型效果更佳。

Conclusion: AdaCap作为一种针对性正则化机制，能有效强化神经网络在最脆弱环节的表现。

Abstract: Neural networks struggle on small tabular datasets, where tree-based models remain dominant. We introduce Adaptive Contrastive Approach (AdaCap), a training scheme that combines a permutation-based contrastive loss with a Tikhonov-based closed-form output mapping. Across 85 real-world regression datasets and multiple architectures, AdaCap yields consistent and statistically significant improvements in the small-sample regime, particularly for residual models. A meta-predictor trained on dataset characteristics (size, skewness, noise) accurately anticipates when AdaCap is beneficial. These results show that AdaCap acts as a targeted regularization mechanism, strengthening neural networks precisely where they are most fragile. All results and code are publicly available at https://github.com/BrunoBelucci/adacap.

</details>


### [98] [Learning Subgroups with Maximum Treatment Effects without Causal Heuristics](https://arxiv.org/abs/2511.20189)
*Lincen Yang,Zhong Li,Matthijs van Leeuwen,Saber Salehkaleybar*

Main category: cs.LG

TL;DR: 本文提出在结构因果模型框架下进行最优亚组发现，证明该方法可转化为标准监督学习问题，并使用CART算法实现，相比现有方法能更准确地识别最大处理效应的亚组。


<details>
  <summary>Details</summary>
Motivation: 现有亚组发现方法存在两个主要问题：一是依赖点状条件处理效应估计，将亚组发现问题转化为更困难的点状估计问题；二是使用缺乏严格理论依据的启发式方法。本文旨在直接在结构因果模型框架下解决这些问题。

Method: 在基于分区的模型假设下，将最优亚组发现问题转化为数据生成模型恢复问题，从而转化为标准监督学习问题。使用CART算法实现亚组发现，避免了因果启发式方法。

Result: 在大量合成和半合成数据集上的实验表明，该方法相比多种基线方法能更准确地识别具有最大处理效应的亚组。

Conclusion: 在结构因果模型框架下，最优亚组发现问题可以转化为标准监督学习问题，使用CART等分区方法能够有效避免因果启发式方法的问题，更准确地发现最优亚组。

Abstract: Discovering subgroups with the maximum average treatment effect is crucial for targeted decision making in domains such as precision medicine, public policy, and education. While most prior work is formulated in the potential outcome framework, the corresponding structural causal model (SCM) for this task has been largely overlooked. In practice, two approaches dominate. The first estimates pointwise conditional treatment effects and then fits a tree on those estimates, effectively turning subgroup estimation into the harder problem of accurate pointwise estimation. The second constructs decision trees or rule sets with ad-hoc 'causal' heuristics, typically without rigorous justification for why a given heuristic may be used or whether such heuristics are necessary at all. We address these issues by studying the problem directly under the SCM framework. Under the assumption of a partition-based model, we show that optimal subgroup discovery reduces to recovering the data-generating models and hence a standard supervised learning problem (regression or classification). This allows us to adopt any partition-based methods to learn the subgroup from data. We instantiate the approach with CART, arguably one of the most widely used tree-based methods, to learn the subgroup with maximum treatment effect. Finally, on a large collection of synthetic and semi-synthetic datasets, we compare our method against a wide range of baselines and find that our approach, which avoids such causal heuristics, more accurately identifies subgroups with maximum treatment effect. Our source code is available at https://github.com/ylincen/causal-subgroup.

</details>


### [99] [In-Context Compositional Learning via Sparse Coding Transformer](https://arxiv.org/abs/2511.20194)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.LG

TL;DR: 本文提出了一种基于稀疏编码原理的注意力机制改进方法，通过将注意力块重新解释为编码字典和解码字典的投影，增强Transformer在组合任务中的能力。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在处理组合学习任务时存在挑战，因为它们并非天生设计用于处理组合任务，且提供的结构归纳偏置有限。需要增强其在组合任务中的能力。

Method: 将注意力块重新解释为输入到输出的映射，通过投影到两组学习到的字典原子上：编码字典和解码字典。对编码系数施加稀疏性约束，并通过上下文示例的系数线性组合来估计目标问题的系数。

Result: 在S-RAVEN和RAVEN数据集上验证了方法的有效性。对于某些组合泛化任务，即使标准Transformer失败时，该方法仍能保持性能。

Conclusion: 提出的基于稀疏编码的注意力机制能够有效学习和应用组合规则，在组合泛化任务中表现出色。

Abstract: Transformer architectures have achieved remarkable success across language, vision, and multimodal tasks, and there is growing demand for them to address in-context compositional learning tasks. In these tasks, models solve the target problems by inferring compositional rules from context examples, which are composed of basic components structured by underlying rules. However, some of these tasks remain challenging for Transformers, which are not inherently designed to handle compositional tasks and offer limited structural inductive bias. In this work, inspired by the principle of sparse coding, we propose a reformulation of the attention to enhance its capability for compositional tasks. In sparse coding, data are represented as sparse combinations of dictionary atoms with coefficients that capture their compositional rules. Specifically, we reinterpret the attention block as a mapping of inputs into outputs through projections onto two sets of learned dictionary atoms: an encoding dictionary and a decoding dictionary. The encoding dictionary decomposes the input into a set of coefficients, which represent the compositional structure of the input. To enhance structured representations, we impose sparsity on these coefficients. The sparse coefficients are then used to linearly combine the decoding dictionary atoms to generate the output. Furthermore, to assist compositional generalization tasks, we propose estimating the coefficients of the target problem as a linear combination of the coefficients obtained from the context examples. We demonstrate the effectiveness of our approach on the S-RAVEN and RAVEN datasets. For certain compositional generalization tasks, our method maintains performance even when standard Transformers fail, owing to its ability to learn and apply compositional rules.

</details>


### [100] [Decoupling and Damping: Structurally-Regularized Gradient Matching for Multimodal Graph Condensation](https://arxiv.org/abs/2511.20222)
*Lian Shen,Zhendan Chen,Yinhui jiang,Meijia Song,Ziming Su,Juan Liu,Xiangrong Liu*

Main category: cs.LG

TL;DR: 针对多模态图神经网络训练的计算负担问题，提出SR-GM框架，通过梯度解耦和结构阻尼正则化解决模态间梯度冲突和图结构噪声放大问题，显著提升压缩图的质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 在多模态图应用中，现有图压缩方法在多模态设置下表现不佳，主要面临两个挑战：模态间语义不对齐导致的梯度冲突，以及图神经网络消息传递架构对梯度噪声的放大效应。

Method: 提出结构正则化梯度匹配(SR-GM)框架，包含两个协同组件：1) 梯度解耦机制，通过正交投影解决模态间梯度冲突；2) 结构阻尼正则化器，利用图的Dirichlet能量将拓扑结构从噪声放大器转变为稳定力。

Result: 实验表明SR-GM相比基线方法显著提高了准确率并加速了收敛，压缩后的多模态图展现出强大的跨架构泛化能力，有望加速神经架构搜索等应用。

Conclusion: 该研究为资源受限环境下的多模态图学习提供了可扩展的方法论，证明同时解决梯度冲突和结构放大问题对于实现卓越性能至关重要。

Abstract: In critical web applications such as e-commerce and recommendation systems, multimodal graphs integrating rich visual and textual attributes are increasingly central, yet their large scale introduces substantial computational burdens for training Graph Neural Networks (GNNs). While Graph Condensation (GC) offers a promising solution by synthesizing smaller datasets, existing methods falter in the multimodal setting. We identify a dual challenge causing this failure: (1) conflicting gradients arising from semantic misalignments between modalities, and (2) the GNN's message-passing architecture pathologically amplifying this gradient noise across the graph structure. To address this, we propose Structurally-Regularized Gradient Matching (SR-GM), a novel condensation framework tailored for multimodal graphs. SR-GM introduces two synergistic components: first, a gradient decoupling mechanism that resolves inter-modality conflicts at their source via orthogonal projection; and second, a structural damping regularizer that acts directly on the gradient field. By leveraging the graph's Dirichlet energy, this regularizer transforms the topology from a noise amplifier into a stabilizing force during optimization. Extensive experiments demonstrate that SR-GM significantly improves accuracy and accelerates convergence compared to baseline methods. Ablation studies confirm that addressing both gradient conflict and structural amplification in tandem is essential for achieving superior performance. Moreover, the condensed multimodal graphs exhibit strong cross-architecture generalization and promise to accelerate applications like Neural Architecture Search. This research provides a scalable methodology for multimodal graph-based learning in resource-constrained environments.

</details>


### [101] [DiCaP: Distribution-Calibrated Pseudo-labeling for Semi-Supervised Multi-Label Learning](https://arxiv.org/abs/2511.20225)
*Bo Han,Zhuoming Li,Xiaoyu Wang,Yaxin Hou,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 提出DiCaP框架，通过估计后验精度来校准伪标签权重，结合双阈值机制区分置信和模糊样本，在半监督多标签学习中取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有半监督多标签学习方法对所有伪标签赋予相同权重，忽略了伪标签质量差异，可能放大噪声预测的影响

Method: 基于理论验证伪标签最优权重应反映其正确性概率，提出DiCaP框架估计后验精度校准权重，使用双阈值机制分离置信和模糊区域

Result: 在多个基准数据集上实验验证，方法取得一致改进，超越最先进方法达4.27%

Conclusion: DiCaP通过校准伪标签权重和双阈值机制，有效提升了半监督多标签学习的性能

Abstract: Semi-supervised multi-label learning (SSMLL) aims to address the challenge of limited labeled data in multi-label learning (MLL) by leveraging unlabeled data to improve the model's performance. While pseudo-labeling has become a dominant strategy in SSMLL, most existing methods assign equal weights to all pseudo-labels regardless of their quality, which can amplify the impact of noisy or uncertain predictions and degrade the overall performance. In this paper, we theoretically verify that the optimal weight for a pseudo-label should reflect its correctness likelihood. Empirically, we observe that on the same dataset, the correctness likelihood distribution of unlabeled data remains stable, even as the number of labeled training samples varies. Building on this insight, we propose Distribution-Calibrated Pseudo-labeling (DiCaP), a correctness-aware framework that estimates posterior precision to calibrate pseudo-label weights. We further introduce a dual-thresholding mechanism to separate confident and ambiguous regions: confident samples are pseudo-labeled and weighted accordingly, while ambiguous ones are explored by unsupervised contrastive learning. Experiments conducted on multiple benchmark datasets verify that our method achieves consistent improvements, surpassing state-of-the-art methods by up to 4.27%.

</details>


### [102] [Leveraging weights signals -- Predicting and improving generalizability in reinforcement learning](https://arxiv.org/abs/2511.20234)
*Olivier Moulin,Vincent Francois-lavet,Paul Elbers,Mark Hoogendoorn*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络内部权重预测RL智能体泛化能力的方法，并改进了PPO损失函数来提升智能体的泛化性能


<details>
  <summary>Details</summary>
Motivation: 解决RL智能体在训练环境上过拟合、泛化能力差的问题

Method: 基于智能体神经网络内部权重预测泛化分数，并改进PPO损失函数

Result: 实验结果表明改进的PPO算法能训练出具有更强泛化能力的智能体

Conclusion: 提出的方法能有效提升RL智能体的泛化能力

Abstract: Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.

</details>


### [103] [Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling](https://arxiv.org/abs/2511.20257)
*Zhiguo Zhang,Xiaoliang Ma,Daniel Schlesinger*

Main category: cs.LG

TL;DR: 提出了一种物理引导、可解释的时空学习框架，用于空气污染预测，在保持高性能的同时提供时空可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有空气污染预测模型在性能和可解释性之间存在权衡，需要既能准确预测又能提供透明解释的解决方案。

Method: 将污染物浓度的时空行为分解为两个透明加性模块：物理引导的传输核（考虑风和地理条件的平流）和可解释的注意力机制（学习局部响应并归因于历史滞后和外部驱动因素）。

Result: 在斯德哥尔摩地区数据集上的评估表明，该模型在多个预测时间范围内持续优于最先进的基线方法。

Conclusion: 该模型将高预测性能与时空可解释性相结合，为实际空气质量管理应用提供了更可靠的基础。

Abstract: Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.

</details>


### [104] [Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits](https://arxiv.org/abs/2511.20273)
*Areeb Ahmad,Abhinav Joshi,Ashutosh Modi*

Main category: cs.LG

TL;DR: 该论文提出了一种更细粒度的视角，将Transformer的注意力头和MLP层分解为正交奇异方向，揭示单个组件内叠加的独立计算。


<details>
  <summary>Details</summary>
Motivation: 现有机制可解释性方法将注意力头和MLP层视为不可分割单元，忽视了它们内部可能学习到的功能子结构。

Method: 将Transformer组件分解为正交奇异方向，在IOI、GP、GT等标准任务上验证该视角。

Result: 发现先前识别的规范功能头（如名称移动器）编码了多个与不同奇异方向对齐的重叠子功能，计算图中的节点在特定低秩方向上表现出强激活。

Conclusion: Transformer计算比先前假设的更分布式、结构化和组合化，这为细粒度机制可解释性和模型内部理解开辟了新途径。

Abstract: Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.

</details>


### [105] [HVAdam: A Full-Dimension Adaptive Optimizer](https://arxiv.org/abs/2511.20277)
*Yiheng Zhang,Shaowu Wu,Yuanzhuo Xu,Jiajun Wu,Shang Xu,Steve Drew,Xiaoguang Niu*

Main category: cs.LG

TL;DR: 提出了Anon优化器，通过可调节的自适应机制在SGD和Adam之间插值甚至外推，结合增量延迟更新技术确保收敛，在多个任务上超越现有优化器。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器如Adam在大规模模型训练中成功，但在经典架构上泛化能力不如非自适应方法如SGD，主要原因是预调节器的自适应限制了优化器适应不同优化环境的能力。

Method: 提出Anon优化器，具有连续可调的自适应机制，能够插值SGD和Adam行为；引入增量延迟更新机制，比AMSGrad的硬最大跟踪策略更灵活，增强对梯度噪声的鲁棒性。

Result: 在图像分类、扩散模型和语言建模等代表性任务上，Anon持续优于最先进的优化器；理论证明在凸和非凸设置下都有收敛保证。

Conclusion: 自适应可以作为有价值的可调设计原则，Anon提供了第一个统一可靠的框架，能够弥合经典和现代优化器之间的差距，并超越它们的优势特性。

Abstract: Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.

</details>


### [106] [Geometry of Decision Making in Language Models](https://arxiv.org/abs/2511.20315)
*Abhinav Joshi,Divyanshu Bhatt,Ashutosh Modi*

Main category: cs.LG

TL;DR: 本研究通过内在维度分析LLMs在多项选择题回答中的决策过程，发现模型在早期层使用低维流形，中间层扩展维度，后期层压缩维度并收敛到决策相关表示。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在各种任务上表现出强大的泛化能力，但其内部决策过程仍不透明。本研究旨在通过几何视角理解LLMs的隐藏表示结构。

Method: 对28个开源Transformer模型进行大规模研究，使用多种估计器估计各层的内在维度，并量化每层在MCQA任务上的性能。

Result: 发现跨模型的一致ID模式：早期层在低维流形上操作，中间层扩展空间，后期层压缩空间并收敛到决策相关表示。

Conclusion: LLMs隐式学习将语言输入投影到与任务特定决策对齐的结构化低维流形上，为语言模型中泛化和推理的出现提供了新的几何见解。

Abstract: Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.

</details>


### [107] [MXtalTools: A Toolkit for Machine Learning on Molecular Crystals](https://arxiv.org/abs/2511.20327)
*Michael Kilgour,Mark E. Tuckerman,Jutta Rogal*

Main category: cs.LG

TL;DR: MXtalTools是一个用于分子晶体数据驱动建模的灵活Python包，支持分子固态的机器学习研究，包含数据集处理、模型训练、晶体参数化、结构采样等模块化功能。


<details>
  <summary>Details</summary>
Motivation: 为分子晶体的数据驱动建模提供灵活的工具包，促进分子固态的机器学习研究，解决现有工作流程中缺乏集成工具的问题。

Method: 开发包含多个功能模块的Python包：数据集合成与整理、集成工作流、晶体参数化与表示、结构采样与优化、端到端可微分晶体处理。

Result: 成功开发了MXtalTools包，支持CUDA加速的高通量晶体建模，代码在GitHub开源并提供详细文档。

Conclusion: MXtalTools提供了一个模块化、灵活的分子晶体建模工具包，可以集成到现有工作流或构建新的建模流程，推动了分子固态的机器学习研究。

Abstract: We present MXtalTools, a flexible Python package for the data-driven modelling of molecular crystals, facilitating machine learning studies of the molecular solid state. MXtalTools comprises several classes of utilities: (1) synthesis, collation, and curation of molecule and crystal datasets, (2) integrated workflows for model training and inference, (3) crystal parameterization and representation, (4) crystal structure sampling and optimization, (5) end-to-end differentiable crystal sampling, construction and analysis. Our modular functions can be integrated into existing workflows or combined and used to build novel modelling pipelines. MXtalTools leverages CUDA acceleration to enable high-throughput crystal modelling. The Python code is available open-source on our GitHub page, with detailed documentation on ReadTheDocs.

</details>


### [108] [Soft Adaptive Policy Optimization](https://arxiv.org/abs/2511.20347)
*Chang Gao,Chujie Zheng,Xiong-Hui Chen,Kai Dang,Shixuan Liu,Bowen Yu,An Yang,Shuai Bai,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: 提出了Soft Adaptive Policy Optimization (SAPO)方法，通过软门控机制替代硬裁剪，在保持序列级一致性的同时自适应地衰减离策略更新，提高了强化学习训练大型语言模型的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于分组的策略优化方法（如GSPO和GRPO）使用硬裁剪处理重要性比率的高方差问题，但难以同时保持稳定性和有效学习。特别是在专家混合模型中，令牌级重要性比率的高方差会导致更新不稳定。

Method: SAPO使用平滑的温度控制门替代硬裁剪，自适应地衰减离策略更新，同时保留有用的学习信号。该方法既保持序列级一致性，又具有令牌自适应性，形成连续的信任区域。

Result: 在数学推理基准测试中，SAPO在相同训练预算下表现出更好的训练稳定性和更高的Pass@1性能。使用SAPO训练Qwen3-VL模型系列，在不同任务和模型规模上都获得了持续的性能提升。

Conclusion: SAPO为LLMs的强化学习训练提供了更可靠、可扩展和有效的优化策略，解决了现有方法在稳定性和学习效率之间的权衡问题。

Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.

</details>


### [109] [Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning](https://arxiv.org/abs/2511.20349)
*M. E. A. Kherchouche,F. Galpin,T. Dumas,F. Schnitzler,D. Menard,L. Zhang*

Main category: cs.LG

TL;DR: 提出了两种机器学习方法来加速VVC帧内分区中的穷举搜索，包括基于回归的RD成本预测方法和基于强化学习的决策方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决VVC帧内分区中RDO过程的计算复杂度问题，需要加速穷举搜索过程。

Method: 1. 基于回归的方法：预测CU的归一化RD成本；2. 基于强化学习的方法：使用DQN算法从CU决策轨迹中学习，将分区决策建模为MDP问题。两种方法都使用预定义阈值来选择合适的分区。

Result: 方法具有尺寸无关性，并整合了相邻块的RD成本作为输入特征，与现有方法不同。

Conclusion: 提出的两种机器学习技术能够有效加速VVC帧内分区的RDO过程，其中强化学习方法利用了分区决策的马尔可夫特性。

Abstract: In this paper, a complexity study is conducted for Versatile Video Codec (VVC) intra partitioning to accelerate the exhaustive search involved in Rate-Distortion Optimization (RDO) process. To address this problem, two main machine learning techniques are proposed and compared. Unlike existing methods, the proposed approaches are size independent and incorporate the Rate-Distortion (RD) costs of neighboring blocks as input features. The first method is a regression based technique that predicts normalized RD costs of a given Coding Unit (CU). As partitioning possesses the Markov property, the associated decision-making problem can be modeled as a Markov Decision Process (MDP) and solved by Reinforcement Learning (RL). The second approach is a RL agent learned from trajectories of CU decision across two depths with Deep Q-Network (DQN) algorithm. Then a pre-determined thresholds are applied for both methods to select a suitable split for the current CU.

</details>


### [110] [PRISM: Periodic Representation with multIscale and Similarity graph Modelling for enhanced crystal structure property prediction](https://arxiv.org/abs/2511.20362)
*Àlex Solé,Albert Mosella-Montoro,Joan Cardona,Daniel Aravena,Silvia Gómez-Coca,Eliseo Ruiz,Javier Ruiz-Hidalgo*

Main category: cs.LG

TL;DR: PRISM是一个图神经网络框架，通过集成多尺度表示和周期性特征编码来改进晶体结构预测，在晶体性质预测基准测试中显著提升了最先进方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前方法往往忽略了晶体结构中固有的周期性边界条件和多尺度相互作用，这些对于准确表示晶体结构至关重要。

Method: PRISM采用一组专家模块，每个模块专门编码周期性系统的不同结构和化学方面，明确集成多尺度表示和周期性特征编码。

Result: 在晶体结构基准测试中的广泛实验表明，PRISM提高了最先进的预测准确性。

Conclusion: PRISM显著增强了晶体性质预测能力，为晶体结构的图表示学习提供了更有效的框架。

Abstract: Crystal structures are characterised by repeating atomic patterns within unit cells across three-dimensional space, posing unique challenges for graph-based representation learning. Current methods often overlook essential periodic boundary conditions and multiscale interactions inherent to crystalline structures. In this paper, we introduce PRISM, a graph neural network framework that explicitly integrates multiscale representations and periodic feature encoding by employing a set of expert modules, each specialised in encoding distinct structural and chemical aspects of periodic systems. Extensive experiments across crystal structure-based benchmarks demonstrate that PRISM improves state-of-the-art predictive accuracy, significantly enhancing crystal property prediction.

</details>


### [111] [MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers](https://arxiv.org/abs/2511.20382)
*Audrey Pei-Hsuan Chen*

Main category: cs.LG

TL;DR: MoRE是一个多组学表示嵌入框架，通过重用预训练transformer模型，将异质检测数据对齐到共享潜在空间中，采用参数高效微调策略实现跨样本和跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 多组学数据表示学习面临极端维度、模态异质性和批次效应的挑战，而预训练transformer在多组学整合中的应用仍未被充分探索。

Method: MoRE在冻结的预训练骨干网络上附加轻量级模态特定适配器和任务自适应融合层，联合优化掩码建模目标与监督对比和批次不变对齐损失。

Result: MoRE在批次鲁棒性和生物保守性方面表现优异，同时相比完全微调模型显著减少了可训练参数数量。

Conclusion: MoRE是迈向通用组学基础模型的实用步骤，能够生成在未见细胞类型和平台上具有良好泛化能力的结构保持嵌入。

Abstract: Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with scArches, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.

</details>


### [112] [Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI](https://arxiv.org/abs/2511.20395)
*M. C. Schoppema,B. H. M. van der Velden,A. Hürriyetoğlu,M. D. Klijnstra,E. J. Faassen,A. Gerssen,H. J. van der Fels-Klerx*

Main category: cs.LG

TL;DR: 开发了一个可解释的深度学习模型来预测荷兰Zeeland河口双壳类软体动物中的河豚毒素污染，识别出日照时间、全球辐射、水温和氯离子浓度是主要影响因素。


<details>
  <summary>Details</summary>
Motivation: 自2012年以来，欧洲温带水域的双壳类软体动物中发现河豚毒素污染，这带来了食品安全风险和经济损失。早期预测河豚毒素污染对食品行业和主管部门至关重要。

Method: 开发了一个基于深度学习的可解释模型，输入气象和水文特征，输出河豚毒素污染的存在或缺失。

Result: 结果显示日出时间、日落时间、全球辐射、水温和氯离子浓度对河豚毒素污染贡献最大，有效日照时间（由日长和全球辐射代表）是重要驱动因素。

Conclusion: 可解释的深度学习模型识别出日照时间、全球辐射、水温和氯离子浓度与双壳类软体动物中的河豚毒素污染相关，该方法可作为减轻海洋毒素风险的有价值工具。

Abstract: Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.
  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.
  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.
  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.

</details>


### [113] [Model-Based Learning of Whittle indices](https://arxiv.org/abs/2511.20397)
*Joël Charles-Rebuffé,Nicolas Gast,Bruno Gaujal*

Main category: cs.LG

TL;DR: BLINQ是一种基于模型的新算法，用于学习可索引、通信且单链马尔可夫决策过程的Whittle指数，在样本效率和计算成本方面显著优于现有Q学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于Q学习的方法在样本效率和计算成本方面存在不足，需要开发更高效的算法来学习Whittle指数。

Method: 通过构建MDP的经验估计，然后使用扩展版的最先进算法计算其Whittle指数。

Result: BLINQ在样本数量上显著优于现有Q学习方法，且总计算成本更低，即使Q学习方法使用预训练神经网络加速也保持优势。

Conclusion: BLINQ提供了一种高效学习Whittle指数的方法，在样本效率和计算复杂度方面都有显著改进。

Abstract: We present BLINQ, a new model-based algorithm that learns the Whittle indices of an indexable, communicating and unichain Markov Decision Process (MDP). Our approach relies on building an empirical estimate of the MDP and then computing its Whittle indices using an extended version of a state-of-the-art existing algorithm. We provide a proof of convergence to the Whittle indices we want to learn as well as a bound on the time needed to learn them with arbitrary precision. Moreover, we investigate its computational complexity. Our numerical experiments suggest that BLINQ significantly outperforms existing Q-learning approaches in terms of the number of samples needed to get an accurate approximation. In addition, it has a total computational cost even lower than Q-learning for any reasonably high number of samples. These observations persist even when the Q-learning algorithms are speeded up using pre-trained neural networks to predict Q-values.

</details>


### [114] [Short-Range Oversquashing](https://arxiv.org/abs/2511.20406)
*Yaaqov Mishayev,Yonatan Sverdlov,Tal Amir,Nadav Dym*

Main category: cs.LG

TL;DR: 本文发现MPNN中的过压缩问题不仅存在于长距离任务，也出现在短距离任务中，揭示了过压缩的两个不同机制：瓶颈现象和梯度消失现象。


<details>
  <summary>Details</summary>
Motivation: 研究MPNN在处理图数据时的过压缩问题，特别是挑战传统认为过压缩只与长距离信息传递相关的观点。

Method: 通过理论分析和实验验证，区分过压缩的两个不同机制：短距离的瓶颈现象和长距离的梯度消失现象。

Result: 发现短距离瓶颈效应无法被现有过压缩理论解释，虚拟节点方法也无法解决此问题，而Transformer模型能成功处理此类任务。

Conclusion: Transformer相比专门的MPNN是解决过压缩问题更有效的方案，特别是在处理短距离瓶颈效应方面。

Abstract: Message Passing Neural Networks (MPNNs) are widely used for learning on graphs, but their ability to process long-range information is limited by the phenomenon of oversquashing. This limitation has led some researchers to advocate Graph Transformers as a better alternative, whereas others suggest that it can be mitigated within the MPNN framework, using virtual nodes or other rewiring techniques.
  In this work, we demonstrate that oversquashing is not limited to long-range tasks, but can also arise in short-range problems. This observation allows us to disentangle two distinct mechanisms underlying oversquashing: (1) the bottleneck phenomenon, which can arise even in low-range settings, and (2) the vanishing gradient phenomenon, which is closely associated with long-range tasks.
  We further show that the short-range bottleneck effect is not captured by existing explanations for oversquashing, and that adding virtual nodes does not resolve it. In contrast, transformers do succeed in such tasks, positioning them as the more compelling solution to oversquashing, compared to specialized MPNNs.

</details>


### [115] [Tight Margin-Based Generalization Bounds for Voting Classifiers over Finite Hypothesis Sets](https://arxiv.org/abs/2511.20407)
*Kasper Green Larsen,Natascha Schalburg*

Main category: cs.LG

TL;DR: 提出了第一个基于间隔的投票分类器泛化边界，在假设集大小、间隔、具有给定间隔的训练点比例、训练样本数量和失败概率之间的权衡方面是渐近紧致的。


<details>
  <summary>Details</summary>
Motivation: 为投票分类器建立一个严格的理论泛化边界，填补现有理论在间隔分析方面的不足，提供更精确的泛化性能保证。

Method: 通过数学证明方法，推导出基于间隔的泛化边界，该边界在多个关键参数（假设集大小、间隔值、训练数据等）的权衡中达到渐近最优。

Result: 成功证明了第一个针对投票分类器的基于间隔的泛化边界，该边界在理论上是渐近紧致的，能够更准确地描述分类器的泛化能力。

Conclusion: 该工作为投票分类器的理论分析提供了重要的理论支撑，建立的泛化边界在多个维度上都是最优的，对机器学习理论发展具有重要意义。

Abstract: We prove the first margin-based generalization bound for voting classifiers, that is asymptotically tight in the tradeoff between the size of the hypothesis set, the margin, the fraction of training points with the given margin, the number of training samples and the failure probability.

</details>


### [116] [Diffusion for Fusion: Designing Stellarators with Generative AI](https://arxiv.org/abs/2511.20445)
*Misha Padidar,Teresa Huang,Andrew Giuliani,Marina Spivak*

Main category: cs.LG

TL;DR: 使用条件扩散模型快速生成具有理想特性的准对称仿星器设计，在未见过的特性组合上表现良好，准对称性偏差小于5%。


<details>
  <summary>Details</summary>
Motivation: 仿星器设计通常需要数小时在计算集群上求解，开发快速设计方法对聚变研究至关重要。机器学习方法因大型优化仿星器数据集的出现而成为潜在解决方案。

Method: 在QUASR数据库上训练条件扩散模型，生成具有特定特性（纵横比和平均旋转变换）的准对称仿星器设计，并将其应用于训练期间未见过的特性组合。

Result: 生成的许多仿星器表现出良好性能：准对称性和目标特性的偏差小于5%。准对称性的适度偏差表明有机会达到低于1%的目标。

Conclusion: 生成建模在推进仿星器设计方面具有多个有前景的应用方向，为机器学习社区提供了一个开放的逆向问题。

Abstract: Stellarators are a prospective class of fusion-based power plants that confine a hot plasma with three-dimensional magnetic fields. Typically framed as a PDE-constrained optimization problem, stellarator design is a time-consuming process that can take hours to solve on a computing cluster. Developing fast methods for designing stellarators is crucial for advancing fusion research. Given the recent development of large datasets of optimized stellarators, machine learning approaches have emerged as a potential candidate. Motivated by this, we present an open inverse problem to the machine learning community: to rapidly generate high-quality stellarator designs which have a set of desirable characteristics. As a case study in the problem space, we train a conditional diffusion model on data from the QUASR database to generate quasisymmetric stellarator designs with desirable characteristics (aspect ratio and mean rotational transform). The diffusion model is applied to design stellarators with characteristics not seen during training. We provide evaluation protocols and show that many of the generated stellarators exhibit solid performance: less than 5% deviation from quasisymmetry and the target characteristics. The modest deviation from quasisymmetry highlights an opportunity to reach the sub 1% target. Beyond the case study, we share multiple promising avenues for generative modeling to advance stellarator design.

</details>


### [117] [Towards Trustworthy Wi-Fi Sensing: Systematic Evaluation of Deep Learning Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2511.20456)
*Shreevanth Krishnaa Gopalakrishnan,Stephen Hailes*

Main category: cs.LG

TL;DR: 本文系统评估了CSI深度学习模型在不同威胁模型下的鲁棒性，发现小模型虽然高效但在对抗攻击下明显更脆弱，物理可实现扰动比无约束特征空间攻击更难成功，对抗训练能有效缓解这些漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着基于CSI的无线感知系统在设备无关活动识别和身份检测等应用中的重要性增加，模型决策可能被细微扰动影响，这引发了安全和可靠性担忧。在真实环境部署前，量化理解模型鲁棒性至关重要。

Method: 建立了评估框架，在三个公共数据集上比较紧凑时间自编码器模型与大型深度架构，评估模型规模、训练机制和物理约束对鲁棒性的影响，涵盖白盒、黑盒/迁移和通用扰动等多种威胁模型。

Result: 实验表明，小模型在干净数据上表现相当但鲁棒性显著较差；物理可实现信号空间扰动相比无约束特征空间攻击成功率显著降低；对抗训练能提高平均鲁棒准确率，仅适度降低干净数据性能。

Conclusion: 这些发现为鲁棒性评估提供了量化基准，并为设计安全可信的人本感知系统提供了指导原则，有助于无线感知向可靠跨域操作发展。

Abstract: Machine learning has become integral to Channel State Information (CSI)-based human sensing systems and is expected to power applications such as device-free activity recognition and identity detection in future cellular and Wi-Fi generations. However, these systems rely on models whose decisions can be subtly perturbed, raising concerns for security and reliability in ubiquitous sensing. Quantifying and understanding the robustness of such models, defined as their ability to maintain accurate predictions under adversarial perturbations, is therefore critical before wireless sensing can be safely deployed in real-world environments.
  This work presents a systematic evaluation of the robustness of CSI deep learning models under diverse threat models (white-box, black-box/transfer, and universal perturbations) and varying degrees of attack realism. We establish a framework to compare compact temporal autoencoder models with larger deep architectures across three public datasets, quantifying how model scale, training regime, and physical constraints influence robustness. Our experiments show that smaller models, while efficient and equally performant on clean data, are markedly less robust. We further confirm that physically realizable signal-space perturbations, designed to be feasible in real wireless channels, significantly reduce attack success compared to unconstrained feature-space attacks. Adversarial training mitigates these vulnerabilities, improving mean robust accuracy with only moderate degradation in clean performance across both model classes. As wireless sensing advances towards reliable, cross-domain operation, these findings provide quantitative baselines for robustness estimation and inform design principles for secure and trustworthy human-centered sensing systems.

</details>


### [118] [NVIDIA Nemotron Parse 1.1](https://arxiv.org/abs/2511.20478)
*Kateryna Chumachenko,Amala Sanjay Deshmukh,Jarno Seppanen,Ilia Karmanov,Chia-Chih Chen,Lukas Voegtle,Philipp Fischer,Marek Wawrzos,Saeid Motiian,Roman Ageev,Kedi Wu,Alexandre Milesi,Maryam Moosaei,Krzysztof Pawelec,Padmavathy Subramanian,Mehrzad Samadi,Xin Yu,Celina Dear,Sarah Stoddard,Jenna Diamond,Jesse Oliver,Leanna Chraghchian,Patrick Skelly,Tom Balough,Yao Xu,Jane Polak Scowcroft,Daniel Korzekwa,Darragh Hanley,Sandip Bhaskar,Timo Roman,Karan Sapra,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron-Parse-1.1是一个轻量级文档解析和OCR模型，相比前代在通用OCR、Markdown格式化、结构化表格解析等方面有显著提升，支持更长的输出序列，公开发布了模型权重和优化容器。


<details>
  <summary>Details</summary>
Motivation: 改进前代Nemoretriever-Parse-1.0的能力，提供更好的文档解析和OCR功能，特别是在处理视觉密集文档时。

Method: 采用编码器-解码器架构，包含8.85亿参数，其中语言解码器为2.56亿参数，支持文本段边界框提取和语义分类。

Result: 在公共基准测试中达到有竞争力的准确率，成为强大的轻量级OCR解决方案，同时发布了优化版本Nemotron-Parse-1.1-TC，速度提升20%且质量损失最小。

Conclusion: Nemotron-Parse-1.1是一个高效的轻量级文档解析模型，在多个OCR任务上表现出色，并通过公开发布促进了社区使用。

Abstract: We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.

</details>


### [119] [Ranking-Enhanced Anomaly Detection Using Active Learning-Assisted Attention Adversarial Dual AutoEncoders](https://arxiv.org/abs/2511.20480)
*Sidahmed Benabderrahmane,James Cheney,Talal Rahwan*

Main category: cs.LG

TL;DR: 提出了一种结合自编码器无监督异常检测和主动学习的创新方法，用于检测高级持续性威胁(APTs)，通过选择性标注不确定样本来最小化标注成本并提高检测率。


<details>
  <summary>Details</summary>
Motivation: APT攻击具有隐蔽性和长期性，传统监督学习方法需要大量标注数据，但在实际网络安全环境中标注数据稀缺。

Method: 基于注意力对抗双自编码器的异常检测框架，结合主动学习循环，选择性查询不确定样本的标签来迭代改进模型。

Result: 在DARPA透明计算项目的真实不平衡溯源跟踪数据库上评估，APT攻击仅占数据的0.004%，结果显示主动学习期间检测率显著提升，性能优于现有方法。

Conclusion: 该方法能够以最小标注成本有效检测APT异常，在真实网络安全环境中具有实用价值。

Abstract: Advanced Persistent Threats (APTs) pose a significant challenge in cybersecurity due to their stealthy and long-term nature. Modern supervised learning methods require extensive labeled data, which is often scarce in real-world cybersecurity environments. In this paper, we propose an innovative approach that leverages AutoEncoders for unsupervised anomaly detection, augmented by active learning to iteratively improve the detection of APT anomalies. By selectively querying an oracle for labels on uncertain or ambiguous samples, we minimize labeling costs while improving detection rates, enabling the model to improve its detection accuracy with minimal data while reducing the need for extensive manual labeling. We provide a detailed formulation of the proposed Attention Adversarial Dual AutoEncoder-based anomaly detection framework and show how the active learning loop iteratively enhances the model. The framework is evaluated on real-world imbalanced provenance trace databases produced by the DARPA Transparent Computing program, where APT-like attacks constitute as little as 0.004\% of the data. The datasets span multiple operating systems, including Android, Linux, BSD, and Windows, and cover two attack scenarios. The results have shown significant improvements in detection rates during active learning and better performance compared to other existing approaches.

</details>


### [120] [MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology](https://arxiv.org/abs/2511.20490)
*Kiril Vasilev,Alexandre Misrahi,Eeshaan Jain,Phil F Cheng,Petros Liakopoulos,Olivier Michielin,Michael Moor,Charlotte Bunne*

Main category: cs.LG

TL;DR: MTBBench是一个模拟分子肿瘤委员会(MTB)决策过程的基准测试，用于评估多模态大语言模型在临床肿瘤学中的多智能体、多模态和纵向推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要评估单模态、去情境化的问答，忽略了真实临床工作流程的复杂性，特别是MTB这种需要整合异质数据和随时间演变的洞察的多智能体决策环境。

Method: 开发了MTBBench基准测试，包含临床挑战性、多模态和纵向的肿瘤学问题，并通过与临床医生共同开发的应用验证真实标注。同时提供了一个基于基础模型的工具框架来增强多模态和纵向推理。

Result: 测试显示现有LLM在可靠性方面存在不足，经常产生幻觉，难以处理时间分辨数据，无法协调冲突证据或不同模态。使用提出的工具框架后，任务级性能分别提升了9.0%和11.2%。

Conclusion: MTBBench为推进多模态LLM在精准肿瘤学MTB环境中的推理、可靠性和工具使用提供了一个具有挑战性和现实性的测试平台。

Abstract: Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.

</details>


### [121] [From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection](https://arxiv.org/abs/2511.20500)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.LG

TL;DR: 提出了一种混合迁移学习框架，结合迁移学习、可解释AI、对比学习和孪生网络，用于改进APT检测的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习检测器在处理类别不平衡、高维特征和真实世界痕迹稀缺时表现不佳，缺乏在新型攻击场景中的可迁移性。

Method: 使用基于注意力的自编码器支持跨域知识迁移，SHAP选择稳定信息特征降维，孪生编码器通过对比学习对齐源域和目标域表示。

Result: 在DARPA透明计算项目的真实世界痕迹和合成攻击场景上评估，相比经典和深度基线方法，在源域到目标域迁移中获得了改进的检测分数。

Conclusion: 该方法为APT检测提供了一个可扩展、可解释且可迁移的解决方案。

Abstract: Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.

</details>


### [122] [DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning](https://arxiv.org/abs/2511.20509)
*Mihaela Hudişteanu,Edwige Cyffers,Nikita P. Kalinin*

Main category: cs.LG

TL;DR: 提出了DP-MicroAdam，一种内存高效且支持稀疏性的自适应差分隐私优化器，在多个基准测试中优于现有自适应DP优化器，并与DP-SGD竞争或超越其性能。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器在非私有训练中是事实标准，但差分隐私训练仍主要使用DP-SGD，需要大量计算和超参数调优，因此需要开发更好的自适应DP优化器。

Method: 开发了DP-MicroAdam优化器，具有内存高效和稀疏感知特性，在随机非凸优化中证明以最优速率收敛。

Result: 在CIFAR-10、大规模ImageNet训练和预训练transformer的私有微调等基准测试中，DP-MicroAdam优于现有自适应DP优化器，与DP-SGD竞争或表现更优。

Conclusion: 自适应优化可以在差分隐私下同时提高性能和稳定性。

Abstract: Adaptive optimizers are the de facto standard in non-private training as they often enable faster convergence and improved performance. In contrast, differentially private (DP) training is still predominantly performed with DP-SGD, typically requiring extensive compute and hyperparameter tuning. We propose DP-MicroAdam, a memory-efficient and sparsity-aware adaptive DP optimizer. We prove that DP-MicroAdam converges in stochastic non-convex optimization at the optimal $\mathcal{O}(1/\sqrt{T})$ rate, up to privacy-dependent constants. Empirically, DP-MicroAdam outperforms existing adaptive DP optimizers and achieves competitive or superior accuracy compared to DP-SGD across a range of benchmarks, including CIFAR-10, large-scale ImageNet training, and private fine-tuning of pretrained transformers. These results demonstrate that adaptive optimization can improve both performance and stability under differential privacy.

</details>


### [123] [Adam Simplified: Bias Correction Simplified](https://arxiv.org/abs/2511.20516)
*Sam Laing,Antonio Orvieto*

Main category: cs.LG

TL;DR: 本文通过系统消融实验发现，Adam优化器中的偏差校正组件在最优超参数配置下对最终测试性能没有提升，有时甚至有害，挑战了该组件必须包含的传统观点。


<details>
  <summary>Details</summary>
Motivation: Adam优化器是现代深度学习的基石，但其各个组件的经验必要性往往被视为理所当然。本文特别关注偏差校正这一贡献未被充分理解的功能。

Method: 在视觉和语言建模任务上进行一系列系统消融实验，分析偏差校正在不同超参数配置下的影响。

Result: 在最优超参数配置下，包含偏差校正不会改善最终测试性能；除非实施适当的学习率调度，否则偏差校正有时会对性能产生负面影响。

Conclusion: 偏差校正可被重新解释为一种隐式学习率调度，其行为强烈依赖于平滑超参数β1、β2的选择。研究结果挑战了该组件必须普遍包含的传统观点。

Abstract: The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.

</details>


### [124] [Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media](https://arxiv.org/abs/2511.20543)
*Alhasan Abdellatif,Hannah P. Menke,Ahmed H. Elsheikh,Florian Doster,Kamaljit Singh*

Main category: cs.LG

TL;DR: UFNO-FiLM通过FiLM层解耦标量输入和空间特征，并使用空间加权损失函数，相比UFNO在多相流预测中减少了21%的气体饱和度MAE。


<details>
  <summary>Details</summary>
Motivation: UFNO虽然通过并行UNet通路提升了预测精度，但将标量输入作为空间分布场处理效率低下，且标准损失函数未考虑误差敏感性的空间变化。

Method: 引入FiLM层解耦标量输入和空间特征，避免在傅里叶变换中引入恒定信号；采用空间加权损失函数优先学习关键区域。

Result: 在地下多相流实验中，相比UFNO实现了21%的气体饱和度平均绝对误差减少。

Conclusion: UFNO-FiLM通过解耦标量输入和空间加权损失有效提升了预测精度，特别是在物理重要性高的区域。

Abstract: The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.

</details>


### [125] [E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems](https://arxiv.org/abs/2511.20564)
*Rui Xue,Shichao Zhu,Liang Qin,Guangmou Pan,Yang Song,Tianfu Wu*

Main category: cs.LG

TL;DR: 提出了E2E-GRec端到端训练框架，将GNN训练与推荐系统统一，解决了传统两阶段方法的高计算开销和缺乏联合优化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统工业部署采用两阶段流水线：GNN离线预训练生成节点嵌入，然后作为静态特征用于下游推荐系统。这种解耦范式导致高计算开销和缺乏联合优化，使得GNN对推荐任务的信息性不足。

Method: 提出E2E-GRec框架，包含三个关键组件：(i)高效子图采样确保训练可扩展性；(ii)图特征自编码器作为辅助自监督任务；(iii)两级特征融合机制结合基于Gradnorm的动态损失平衡。

Result: 离线评估、在线A/B测试（如停留时长相对提升0.133%，用户跳过视频平均数量减少0.3171%）和理论分析表明，E2E-GRec在多个推荐指标上均显著优于传统方法。

Conclusion: E2E-GRec框架通过端到端训练统一GNN与推荐系统，有效解决了传统两阶段方法的局限性，在推荐性能上取得了显著提升。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-trained offline to generate node embeddings, which are then used as static features for downstream recommender systems. This decoupled paradigm leads to two key limitations: (1) high computational overhead, since large-scale GNN inference must be repeatedly executed to refresh embeddings; and (2) lack of joint optimization, as the gradient from the recommender system cannot directly influence the GNN learning process, causing the GNN to be suboptimally informative for the recommendation task. In this paper, we propose E2E-GRec, a novel end-to-end training framework that unifies GNN training with the recommender system. Our framework is characterized by three key components: (i) efficient subgraph sampling from a large-scale cross-domain heterogeneous graph to ensure training scalability and efficiency; (ii) a Graph Feature Auto-Encoder (GFAE) serving as an auxiliary self-supervised task to guide the GNN to learn structurally meaningful embeddings; and (iii) a two-level feature fusion mechanism combined with Gradnorm-based dynamic loss balancing, which stabilizes graph-aware multi-task end-to-end training. Extensive offline evaluations, online A/B tests (e.g., a +0.133% relative improvement in stay duration, a 0.3171% reduction in the average number of videos a user skips) on large-scale production data, together with theoretical analysis, demonstrate that E2E-GRec consistently surpasses traditional approaches, yielding significant gains across multiple recommendation metrics.

</details>


### [126] [MSTN: Fast and Efficient Multivariate Time Series Model](https://arxiv.org/abs/2511.20577)
*Sumit S Shevtekar,Chandresh K Maurya,Gourab Sil*

Main category: cs.LG

TL;DR: MSTN是一种多尺度时间网络，通过层次化多尺度编码、序列建模和门控融合机制，自适应地建模从毫秒级到长期依赖的时间模式，在时间序列预测、插补和分类任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列数据具有高度非平稳性和多时间尺度动态特性，现有基于固定尺度结构先验的模型往往过度正则化时间动态，难以适应性地建模全谱时间变化，特别是在处理突发高幅事件时表现不佳。

Method: 提出多尺度时间网络(MSTN)，包含：(i)多尺度卷积编码器构建层次特征金字塔；(ii)序列建模组件处理长期时间依赖；(iii)结合SE和多头时间注意力的门控融合机制进行动态特征集成。

Result: 在时间序列长期预测、插补、分类和泛化性研究中，MSTN在32个基准数据集的24个上达到新的SOTA性能，超越了EMTSF、LLM4TS、HiMTM、TIME-LLM、MTST、SOFTS、iTransformer、TimesNet和PatchTST等当代方法。

Conclusion: MSTN通过统一框架自适应建模多尺度时间模式，在多样化时间任务中展现出持续优异的性能，为时间序列分析提供了灵活且强大的基础架构。

Abstract: Real-world time-series data is highly non stationary and complex in dynamics that operate across multiple timescales, ranging from fast, short-term changes to slow, long-term trends. Most existing models rely on fixed-scale structural priors, such as patch-based tokenization, fixed frequency transformations, or frozen backbone architectures. This often leads to over-regularization of temporal dynamics, which limits their ability to adaptively model the full spectrum of temporal variations and impairs their performance on unpredictable, Sudden, high-magnitude events. To address this, we introduce the Multi-scale Temporal Network (MSTN), a novel deep learning architecture founded on a hierarchical multi-scale and sequence modeling principle. The MSTN framework integrates: (i) a multi-scale convolutional encoder that constructs a hierarchical feature pyramid for local patterns (ii) a sequence modeling component for long-range temporal dependencies. We empirically validate this with BiLSTM and Transformer variants, establishing a flexible foundation for future architectural advancements. and (iii) a gated fusion mechanism augmented with squeeze-and-excitation (SE) and multi-head temporal attention (MHTA) for dynamic, context-aware feature integration. This design enables MSTN to adaptively model temporal patterns from milliseconds to long-range dependencies within a unified framework. Extensive evaluations across time-series long-horizon forecasting, imputation, classification and generalizability study demonstrate that MSTN achieves competitive state-of-the-art (SOTA) performance, showing improvements over contemporary approaches including EMTSF, LLM4TS, HiMTM, TIME-LLM, MTST, SOFTS, iTransformer, TimesNet, and PatchTST. In total, MSTN establishes new SOTA performance on 24 of 32 benchmark datasets, demonstrating its consistent performance across diverse temporal tasks.

</details>


### [127] [A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent](https://arxiv.org/abs/2511.20584)
*Shuo Xie,Tianhao Wang,Beining Wu,Zhiyuan Li*

Main category: cs.LG

TL;DR: 自适应优化器与归一化最速下降法在仅适应当前梯度时具有紧密联系，但分析基于不同几何概念。自适应优化器受自适应平滑性控制，而NSD依赖标准平滑性。本文扩展自适应平滑性理论到非凸设置，证明其精确刻画自适应优化器收敛性，并在凸设置中实现Nesterov动量加速。


<details>
  <summary>Details</summary>
Motivation: 揭示自适应优化器与归一化最速下降法之间的理论联系，理解不同几何条件（如平滑性概念）在优化算法分析中的作用差异。

Method: 扩展自适应平滑性理论到非凸设置，建立自适应平滑性与自适应优化器收敛性的精确对应关系，并研究Nesterov动量在自适应平滑性条件下的加速效果。

Result: 证明了自适应平滑性精确刻画自适应优化器在非凸设置中的收敛性；在凸设置中，自适应平滑性使得自适应优化器能够实现Nesterov动量加速，这在某些非欧几里得几何下是标准平滑性无法保证的。

Conclusion: 自适应平滑性是理解自适应优化器性能的关键几何条件，它提供了比标准平滑性更强的理论保证，并支持在某些非欧几里得几何下的加速收敛。

Abstract: Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.

</details>


### [128] [Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models](https://arxiv.org/abs/2511.20587)
*Karim Kadry,Abdallah Abdelwahed,Shoaib Goraya,Ajay Manicka,Naravich Chutisilp,Farhad Nezami,Elazer Edelman*

Main category: cs.LG

TL;DR: Anatomica是一个推理时框架，用于生成具有局部几何拓扑控制的多类解剖体素图，通过可微分惩罚函数和持久同调技术实现对几何和拓扑特征的控制。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够灵活控制解剖结构几何和拓扑特征的框架，为虚拟试验或机器学习工作流程生成合成数据集。

Method: 使用不同维度的立方体控制域切片相关子结构，通过体素矩控制几何特征，通过持久同调控制拓扑特征，并在潜在扩散模型中实现部分子结构提取。

Result: Anatomica能够灵活应用于不同解剖系统，在任意维度和坐标系下组合约束来控制复杂结构。

Conclusion: 该框架实现了解剖属性的高效控制，支持合成数据集的理性设计。

Abstract: We present Anatomica: an inference-time framework for generating multi-class anatomical voxel maps with localized geo-topological control. During generation, we use cuboidal control domains of varying dimensionality, location, and shape to slice out relevant substructures. These local substructures are used to compute differentiable penalty functions that steer the sample towards target constraints. We control geometric features such as size, shape, and position through voxel-wise moments, while topological features such as connected components, loops, and voids are enforced through persistent homology. Lastly, we implement Anatomica for latent diffusion models, where neural field decoders partially extract substructures, enabling the efficient control of anatomical properties. Anatomica applies flexibly across diverse anatomical systems, composing constraints to control complex structures over arbitrary dimensions and coordinate systems, thereby enabling the rational design of synthetic datasets for virtual trials or machine learning workflows.

</details>


### [129] [Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning](https://arxiv.org/abs/2511.20591)
*Charlotte Beylier,Hannah Selder,Arthur Fleig,Simon M. Hofmann,Nico Scherf*

Main category: cs.LG

TL;DR: 提出注意力导向指标(ATOMs)来研究强化学习智能体在训练过程中的注意力发展，通过三个Pong游戏变体实验验证了ATOMs能有效区分不同训练环境的注意力模式，并发现注意力发展呈现阶段性特征。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的学习过程除了数学公式外仍缺乏深入理解，需要新的方法来研究其注意力发展过程。

Method: 引入ATOMs指标，在三个设计不同的Pong游戏变体上进行控制实验，结合行为评估分析智能体注意力模式。

Result: ATOMs成功区分了不同游戏变体训练的智能体注意力模式，这些注意力差异转化为行为差异，且注意力发展呈现一致的阶段性特征。

Conclusion: ATOMs有助于提高对强化学习智能体学习过程的理解，更好地理解注意力与学习之间的关系。

Abstract: The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.

</details>


### [130] [Latent Diffusion Inversion Requires Understanding the Latent Space](https://arxiv.org/abs/2511.20592)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 该研究发现潜在扩散模型在潜在空间中存在非均匀记忆现象，并提出基于解码器回拉度量的维度排序方法，显著提升了成员推理攻击性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型反演技术主要关注数据域扩散模型，而忽略了潜在空间生成模型中的编码器/解码器对和潜在代码对记忆化的影响。

Method: 提出基于解码器回拉度量的潜在维度排序方法，识别对记忆化贡献最大的维度，并在计算攻击统计量时移除较少记忆的维度。

Result: 在CIFAR-10、CelebA、ImageNet-1K等多个数据集上，该方法使成员推理攻击的AUROC平均提升2.7%，TPR@1%FPR显著提高6.42%。

Conclusion: 研究揭示了自编码器几何结构对LDM记忆化的被忽视影响，为分析基于扩散的生成模型的隐私风险提供了新视角。

Abstract: The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\% and substantial increases in TPR@1\%FPR (6.42\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pokémon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.

</details>


### [131] [BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents](https://arxiv.org/abs/2511.20597)
*Kaiyuan Zhang,Mark Tenenholtz,Kyle Polley,Jerry Ma,Denis Yarats,Ninghui Li*

Main category: cs.LG

TL;DR: 本研究构建了一个包含真实HTML负载的提示注入攻击基准，评估现有防御措施的有效性，并提出多层防御策略来保护Web智能体免受提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: AI智能体集成到Web浏览器中带来了超越传统Web应用威胁模型的安全挑战，现有研究对提示注入攻击在真实环境中的影响理解不足。

Method: 构建包含复杂真实HTML负载的提示注入攻击基准，强调影响实际行为的注入攻击，并对前沿AI模型进行全面实证评估。

Result: 开发了具有现实复杂性和干扰频率的攻击基准，评估了现有防御措施的有效性。

Conclusion: 提出了包含架构和模型防御的多层防御策略，为设计实用安全的Web智能体提供了深度防御蓝图。

Abstract: The integration of artificial intelligence (AI) agents into web browsers introduces security challenges that go beyond traditional web application threat models. Prior work has identified prompt injection as a new attack vector for web agents, yet the resulting impact within real-world environments remains insufficiently understood.
  In this work, we examine the landscape of prompt injection attacks and synthesize a benchmark of attacks embedded in realistic HTML payloads. Our benchmark goes beyond prior work by emphasizing injections that can influence real-world actions rather than mere text outputs, and by presenting attack payloads with complexity and distractor frequency similar to what real-world agents encounter. We leverage this benchmark to conduct a comprehensive empirical evaluation of existing defenses, assessing their effectiveness across a suite of frontier AI models. We propose a multi-layered defense strategy comprising both architectural and model-based defenses to protect against evolving prompt injection attacks. Our work offers a blueprint for designing practical, secure web agents through a defense-in-depth approach.

</details>


### [132] [The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting](https://arxiv.org/abs/2511.20601)
*Heman Shakeri*

Main category: cs.LG

TL;DR: 深度序列模型在血糖预测中未能有效利用胰岛素、饮食和活动等临床驱动因素，这种现象被称为"驱动因素盲视"，表现为多变量模型相比单变量基线的性能增益Δ_drivers接近零。


<details>
  <summary>Details</summary>
Motivation: 尽管血糖生理机制已被充分理解，但现有深度序列模型未能有效整合临床驱动因素（胰岛素、饮食、活动），导致模型性能受限。

Method: 通过定义Δ_drivers指标量化驱动因素盲视程度，分析其三个成因：架构偏向自相关、数据保真度差距和生理异质性，并提出生理特征编码器、因果正则化和个性化等缓解策略。

Result: 文献分析显示Δ_drivers通常接近零，表明当前模型确实存在驱动因素盲视问题。

Conclusion: 建议未来研究常规报告Δ_drivers指标，防止驱动因素盲视模型被误认为最先进技术。

Abstract: Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Δ_{\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Δ_{\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Δ_{\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.

</details>


### [133] [Adaptive Hopfield Network: Rethinking Similarities in Associative Memory](https://arxiv.org/abs/2511.20609)
*Shurong Wang,Yuqi Pan,Zhuoyang Shen,Meng Zhang,Hongwei Wang,Guoqi Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的关联记忆模型，通过定义变体分布和自适应相似性机制，解决了现有模型无法保证检索正确性的问题，并在多种任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有关联记忆模型基于邻近度评估检索质量，无法保证检索到的模式与查询具有最强关联，缺乏正确性保证。

Method: 提出变体分布建模查询生成过程，开发自适应相似性机制来近似存储模式生成查询的似然，并构建自适应Hopfield网络（A-Hop）。

Result: 理论证明在噪声、掩码和偏置三种变体类型下实现最优正确检索，在记忆检索、表格分类、图像分类和多实例学习等任务中达到最先进性能。

Conclusion: 通过将查询视为存储记忆模式的生成变体，并采用自适应相似性机制，能够实现正确且高效的关联记忆检索。

Abstract: Associative memory models are content-addressable memory systems fundamental to biological intelligence and are notable for their high interpretability. However, existing models evaluate the quality of retrieval based on proximity, which cannot guarantee that the retrieved pattern has the strongest association with the query, failing correctness. We reframe this problem by proposing that a query is a generative variant of a stored memory pattern, and define a variant distribution to model this subtle context-dependent generative process. Consequently, correct retrieval should return the memory pattern with the maximum a posteriori probability of being the query's origin. This perspective reveals that an ideal similarity measure should approximate the likelihood of each stored pattern generating the query in accordance with variant distribution, which is impossible for fixed and pre-defined similarities used by existing associative memories. To this end, we develop adaptive similarity, a novel mechanism that learns to approximate this insightful but unknown likelihood from samples drawn from context, aiming for correct retrieval. We theoretically prove that our proposed adaptive similarity achieves optimal correct retrieval under three canonical and widely applicable types of variants: noisy, masked, and biased. We integrate this mechanism into a novel adaptive Hopfield network (A-Hop), and empirical results show that it achieves state-of-the-art performance across diverse tasks, including memory retrieval, tabular classification, image classification, and multiple instance learning.

</details>


### [134] [Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning](https://arxiv.org/abs/2511.20613)
*Panayiotis Danassis,Naman Goel*

Main category: cs.LG

TL;DR: LLM生成的代码在真实世界物流优化问题上表现不佳，人类编写的代理在竞争中明显优于LLM生成的代理，即使提供最佳人类解决方案，LLM也无法改进反而使其更差。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过于强调单元测试通过率和语法正确性，低估了需要规划、优化和战略交互的真实世界问题的难度。

Method: 基于真实世界物流优化问题（拍卖、取货和配送问题）构建多智能体推理驱动基准，评估40个LLM编码代理与17个人类编码代理在12场双循环锦标赛中的表现。

Result: 人类编码代理始终占据前5名；40个LLM代理中有33个被简单基线击败；即使提供最佳人类解决方案，LLM也无法改进反而使其更差。

Conclusion: LLM在生成能在真实世界竞争性工作的代码方面存在能力差距，需要新的强调推理驱动代码合成的评估方法。

Abstract: The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.

</details>


### [135] [DiFR: Inference Verification Despite Nondeterminism](https://arxiv.org/abs/2511.20621)
*Adam Karvonen,Daniel Reuter,Roy Rinberg,Luke Marks,Adrià Garriga-Alonso,Keri Warr*

Main category: cs.LG

TL;DR: Token-DiFR是一种通过比较生成令牌与可信参考实现预测来验证LLM推理输出的方法，使用采样种子同步来约束有效输出，使输出令牌本身可作为正确性的可审计证据。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理需求增长，需要验证推理过程是否正确执行，但由于良性数值噪声导致重复运行相同推理会产生不同结果，难以区分合法变化与实际问题。

Method: Token-DiFR：通过采样种子同步，将生成令牌与可信参考实现基于相同随机种子的预测进行比较。Activation-DiFR：使用随机正交投影将激活压缩为紧凑指纹进行验证。

Result: Token-DiFR可靠识别采样错误、模拟bug和模型量化，在300个输出令牌内检测4位量化的AUC>0.999。Activation-DiFR仅用2个输出令牌即可检测4位量化，AUC>0.999，通信开销比现有方法减少25-75%。

Conclusion: 提出的DiFR方法能有效验证LLM推理正确性，Token-DiFR以零额外成本提供可审计证据，Activation-DiFR在样本效率验证方面表现优异，已开源集成vLLM以加速可验证推理的实际部署。

Abstract: As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.

</details>


### [136] [ROOT: Robust Orthogonalized Optimizer for Neural Network Training](https://arxiv.org/abs/2511.20626)
*Wei He,Kai Han,Hang Zhou,Hanting Chen,Zhicheng Liu,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 提出了ROOT优化器，通过维度鲁棒的正交化方案和优化鲁棒的近端优化框架，解决了大语言模型训练中的正交化精度问题和异常值噪声问题，在噪声和非凸场景下实现了更快的收敛和更好的最终性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型优化面临算法不精确和训练不稳定的挑战，现有基于动量正交化的优化器存在维度脆弱性和异常值噪声敏感性两个关键鲁棒性限制。

Method: 1. 维度鲁棒正交化方案：使用自适应牛顿迭代和针对特定矩阵尺寸的细粒度系数；2. 优化鲁棒框架：通过近端优化抑制异常值噪声同时保留有意义的梯度方向。

Result: ROOT在噪声和非凸场景下相比Muon和Adam优化器实现了显著改进的鲁棒性、更快的收敛速度和更优的最终性能。

Conclusion: ROOT为开发能够处理现代大规模模型训练复杂性的鲁棒精确优化器建立了新范式。

Abstract: The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.

</details>


### [137] [Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model](https://arxiv.org/abs/2511.20636)
*Ziyue Wang,Yayati Jadhav,Peter Pak,Amir Barati Farimani*

Main category: cs.LG

TL;DR: Image2Gcode是一个端到端的数据驱动框架，可直接从2D图像生成3D打印机的G代码，绕过了传统的CAD建模阶段。


<details>
  <summary>Details</summary>
Motivation: 传统机械设计制造流程依赖CAD建模，这是主要瓶颈：构建特定对象的3D几何模型速度慢，不适合快速原型制作。即使小的设计变更也需要在CAD软件中手动更新，迭代耗时且难以扩展。

Method: 使用去噪扩散概率模型(DDPM)处理G代码序列，从图像中提取切片结构线索，通过迭代去噪将高斯噪声转换为可执行的打印移动轨迹和挤出参数。

Result: 建立了从视觉输入到原生工具路径的直接映射，无需CAD或STL中间文件，降低了增材制造的入门门槛，加速了从设计到制造的周期。

Conclusion: 该框架支持从简单草图或视觉参考进行按需原型制作，并与上游2D到3D重建模块集成，实现了从概念到物理工件的自动化流程，提高了设计迭代、修复工作流程和分布式制造的可访问性。

Abstract: Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [138] [FAST: Topology-Aware Frequency-Domain Distribution Matching for Coreset Selection](https://arxiv.org/abs/2511.19476)
*Jin Cui,Boran Zhao,Jiajun Xu,Jiaqi Guo,Shuo Guan,Pengju Ren*

Main category: stat.ML

TL;DR: FAST是一个基于谱图理论和特征函数距离的无DNN分布匹配核心集选择框架，通过渐进式采样策略显著提升性能并降低能耗


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法要么依赖DNN参数引入架构偏差，要么基于缺乏理论保证的启发式方法，且都无法显式约束分布等价性，传统度量也无法准确捕捉高阶矩差异

Method: 将核心集选择建模为图约束优化问题，使用特征函数距离在频域捕获完整分布信息，引入衰减相位解耦CFD解决梯度消失问题，采用渐进式差异感知采样策略

Result: 在所有评估基准上显著优于现有方法，平均准确率提升9.12%，功耗降低96.57%，速度提升2.2倍

Conclusion: FAST通过频域分布匹配和渐进采样策略，实现了高效、节能且性能优越的核心集选择，为大规模深度学习训练提供了实用解决方案

Abstract: Coreset selection compresses large datasets into compact, representative subsets, reducing the energy and computational burden of training deep neural networks. Existing methods are either: (i) DNN-based, which are tied to model-specific parameters and introduce architectural bias; or (ii) DNN-free, which rely on heuristics lacking theoretical guarantees. Neither approach explicitly constrains distributional equivalence, largely because continuous distribution matching is considered inapplicable to discrete sampling. Moreover, prevalent metrics (e.g., MSE, KL, MMD, CE) cannot accurately capture higher-order moment discrepancies, leading to suboptimal coresets. In this work, we propose FAST, the first DNN-free distribution-matching coreset selection framework that formulates the coreset selection task as a graph-constrained optimization problem grounded in spectral graph theory and employs the Characteristic Function Distance (CFD) to capture full distributional information in the frequency domain. We further discover that naive CFD suffers from a "vanishing phase gradient" issue in medium and high-frequency regions; to address this, we introduce an Attenuated Phase-Decoupled CFD. Furthermore, for better convergence, we design a Progressive Discrepancy-Aware Sampling strategy that progressively schedules frequency selection from low to high, preserving global structure before refining local details and enabling accurate matching with fewer frequencies while avoiding overfitting. Extensive experiments demonstrate that FAST significantly outperforms state-of-the-art coreset selection methods across all evaluated benchmarks, achieving an average accuracy gain of 9.12%. Compared to other baseline coreset methods, it reduces power consumption by 96.57% and achieves a 2.2x average speedup, underscoring its high performance and energy efficiency.

</details>


### [139] [Optimization and Regularization Under Arbitrary Objectives](https://arxiv.org/abs/2511.19628)
*Jared N. Lakhani,Etienne Pienaar*

Main category: stat.ML

TL;DR: 本研究探讨了MCMC方法在任意目标函数应用中的局限性，重点关注两区块MCMC框架的性能受似然函数锐度的影响，并通过强化学习任务验证了理论分析。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析MCMC方法在任意目标函数上的适用性限制，特别是两区块MCMC框架（交替使用Metropolis-Hastings和Gibbs采样）中似然函数锐度对性能的关键影响。

Method: 引入锐度参数并探索与目标函数成比例的替代似然公式，通过强化学习任务（导航问题和井字游戏）进行实证应用，并在黑杰克游戏中用迭代优化步骤替换MCMC框架的第一个区块。

Result: 研究表明似然曲率同时控制样本内性能和训练数据推断的正则化程度，当似然锐度过高时，后验质量会坍缩到单一主导模式上。

Conclusion: 混合方法（用优化步骤替换部分MCMC）实现了与原MCMC框架几乎相同的性能，证实了过度锐化的似然函数会导致后验集中在单一模式上。

Abstract: This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.

</details>


### [140] [Clustering Approaches for Mixed-Type Data: A Comparative Study](https://arxiv.org/abs/2511.19755)
*Badih Ghattas,Alvaro Sanchez San-Benito*

Main category: stat.ML

TL;DR: 本文综述了混合类型数据聚类方法，比较了k-prototypes、PDQ、凸k-means、KAMILA、MBNs和LCM等方法在不同实验条件下的表现，发现KAMILA、LCM和k-prototypes在调整兰德指数方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 混合类型数据的聚类仍然是一个挑战，因为现有的方法很少适合这项任务。本研究旨在提供不同方法在各种场景下行为的见解。

Method: 使用各种模拟模型比较距离基方法（k-prototypes、PDQ、凸k-means）和概率方法（KAMILA、MBNs、LCM），通过改变聚类数量、聚类重叠度、样本大小、维度、连续变量比例和聚类分布等实验因素。

Result: 聚类重叠度、连续变量比例和样本大小对性能有显著影响。当变量间存在强相互作用且明确依赖于聚类成员关系时，所有评估方法均未表现出满意性能。在实验中，KAMILA、LCM和k-prototypes在调整兰德指数方面表现最佳。

Conclusion: KAMILA、LCM和k-prototypes是混合类型数据聚类中表现最好的方法，所有方法都在R中可用。聚类重叠度和连续变量比例是影响聚类性能的关键因素。

Abstract: Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R.

</details>


### [141] [A Fully Probabilistic Tensor Network for Regularized Volterra System Identification](https://arxiv.org/abs/2511.20457)
*Afra Kilic,Kim Batselier*

Main category: stat.ML

TL;DR: BTN-V使用贝叶斯张量网络框架进行Volterra系统辨识，通过规范多分量分解表示Volterra核，将模型复杂度从O(I^D)降低到O(DIR)，并提供预测不确定性估计。


<details>
  <summary>Details</summary>
Motivation: Volterra级数建模非线性系统时，核系数数量随模型阶数呈指数增长，这带来了计算挑战。

Method: 将Volterra核表示为规范多分量分解，将所有张量分量和超参数视为随机变量，使用稀疏诱导分层先验实现自动秩确定和衰减记忆行为学习。

Result: 经验结果显示具有竞争力的准确性、增强的不确定性量化能力和降低的计算成本。

Conclusion: BTN-V框架在保持精度的同时显著降低了计算复杂度，并提供了更好的可解释性和过拟合预防能力。

Abstract: Modeling nonlinear systems with Volterra series is challenging because the number of kernel coefficients grows exponentially with the model order. This work introduces Bayesian Tensor Network Volterra kernel machines (BTN-V), extending the Bayesian Tensor Network framework to Volterra system identification. BTN-V represents Volterra kernels using canonical polyadic decomposition, reducing model complexity from O(I^D) to O(DIR). By treating all tensor components and hyperparameters as random variables, BTN-V provides predictive uncertainty estimation at no additional computational cost. Sparsity-inducing hierarchical priors enable automatic rank determination and the learning of fading-memory behavior directly from data, improving interpretability and preventing overfitting. Empirical results demonstrate competitive accuracy, enhanced uncertainty quantification, and reduced computational cost.

</details>


### [142] [Generative Modeling with Manifold Percolation](https://arxiv.org/abs/2511.20503)
*Rui Tong*

Main category: stat.ML

TL;DR: 本文提出使用连续渗流理论分析生成模型的数据支撑结构，建立了随机几何图的拓扑相变与高维数据流形之间的同构关系，并开发了可微分的渗流损失函数来指导训练，防止流形收缩并实现超泛化。


<details>
  <summary>Details</summary>
Motivation: 从观察者视角看，生成建模需要将几何支撑与概率分布解耦。传统统计指标无法捕捉结构病理（如隐式模式崩溃），需要新的几何分析方法。

Method: 利用连续渗流理论，将高维密度估计转化为支撑上的几何计数问题。建立随机几何图拓扑相变与数据流形的同构关系，开发渗流偏移度量，并转化为可微分损失函数。

Result: 实验证实该方法能防止流形收缩，驱动模型达到超泛化状态，在保持良好保真度的同时实现拓扑扩展。渗流偏移度量能捕捉统计指标无法检测的结构问题。

Conclusion: 连续渗流为生成模型的支撑分析提供了独特视角，拓扑方法能有效解决传统统计指标的局限性，实现更好的生成质量。

Abstract: Generative modeling is typically framed as learning mapping rules, but from an observer's perspective without access to these rules, the task manifests as disentangling the geometric support from the probability distribution. We propose that Continuum Percolation is uniquely suited for this support analysis, as the sampling process effectively projects high-dimensional density estimation onto a geometric counting problem on the support. In this work, we establish a rigorous isomorphism between the topological phase transitions of Random Geometric Graphs and the underlying data manifold in high-dimensional space. By analyzing the relationship between our proposed Percolation Shift metric and FID, we demonstrate that our metric captures structural pathologies (such as implicit mode collapse) where statistical metrics fail. Finally, we translate this topological phenomenon into a differentiable loss function to guide training. Experimental results confirm that this approach not only prevents manifold shrinkage but drives the model toward a state of "Hyper-Generalization," achieving good fidelity and verified topological expansion.

</details>


### [143] [Spatio-Temporal Hierarchical Causal Models](https://arxiv.org/abs/2511.20558)
*Xintong Li,Haoran Zhang,Xiao Zhou*

Main category: stat.ML

TL;DR: 提出了时空层次因果模型（ST-HCMs），通过时空折叠定理处理未观测的时空混杂因子，实现稳健的因果推断。


<details>
  <summary>Details</summary>
Motivation: 现有时空因果推断方法通常假设所有混杂因子都被观测到，这在实践中往往不成立，特别是存在未观测的、随时间不变的单元级混杂因子时。

Method: 引入时空层次因果模型（ST-HCMs）图形框架，基于时空折叠定理，证明随着子单元数据增加，复杂ST-HCM会收敛到更简单的平面因果模型。

Result: 在合成和真实数据集上验证了框架的有效性，展示了在复杂动态系统中进行稳健因果推断的潜力。

Conclusion: ST-HCMs能够处理未观测的时空混杂因子，为复杂动态系统中的因果推断提供了有效解决方案。

Abstract: The abundance of fine-grained spatio-temporal data, such as traffic sensor networks, offers vast opportunities for scientific discovery. However, inferring causal relationships from such observational data remains challenging, particularly due to unobserved confounders that are specific to units (e.g., geographical locations) yet influence outcomes over time. Most existing methods for spatio-temporal causal inference assume that all confounders are observed, an assumption that is often violated in practice. In this paper, we introduce Spatio-Temporal Hierarchical Causal Models (ST-HCMs), a novel graphical framework that extends hierarchical causal modeling to the spatio-temporal domain. At the core of our approach is the Spatio-Temporal Collapse Theorem, which shows that a complex ST-HCM converges to a simpler flat causal model as the amount of subunit data increases. This theoretical result enables a general procedure for causal identification, allowing ST-HCMs to recover causal effects even in the presence of unobserved, time-invariant unit-level confounders, a scenario where standard non-hierarchical models fail. We validate the effectiveness of our framework on both synthetic and real-world datasets, demonstrating its potential for robust causal inference in complex dynamic systems.

</details>
