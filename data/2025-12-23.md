<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 32]
- [cs.LG](#cs.LG) [Total: 130]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [An extensive analysis and calibration of the Modular Aggregation Algorithm across three categories of for GNSS trajectories data sources](https://arxiv.org/abs/2512.17919)
*Marie-Dominique van Damme,Yann Méneroux,Ana-Maria Olteanu-Raimond*

Main category: eess.SP

TL;DR: 这是一份技术报告，旨在补充会议论文，提供额外的实验和细节


<details>
  <summary>Details</summary>
Motivation: 由于会议论文篇幅限制，无法包含所有实验细节和额外内容，因此撰写此技术报告进行补充

Method: 通过技术报告的形式，提供会议论文中未包含的额外实验、详细实现细节、参数设置等补充信息

Result: 提供了更全面的实验数据和技术细节，增强了原始会议论文的完整性和可复现性

Conclusion: 技术报告作为会议论文的有价值补充，为研究社区提供了更详细的技术信息和实验验证

Abstract: This technical report aims to complement the conference paper (https://doi.org/10.1145/3678717.3691325) by providing additional experiments or further details that could not be included in the paper.

</details>


### [2] [Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach](https://arxiv.org/abs/2512.17928)
*Dongdong Yang,Bin Li,Jiguang He,Yicheng Yan,Xiaoyu Zhang,Chongwen Huang*

Main category: eess.SP

TL;DR: 提出基于梯度元学习（GML）的轻量级神经网络框架，用于高效优化STAR-RIS辅助通信系统的波束成形和相移矩阵，显著降低计算复杂度并实现近线性的复杂度增长。


<details>
  <summary>Details</summary>
Motivation: STAR-RIS技术能实现全空间覆盖并提升频谱效率，但其联合优化问题具有高维度、强非凸、NP-hard特性。传统交替优化方法计算复杂度高、可扩展性差，而现有深度学习方案需要昂贵的预训练和大规模网络模型。

Method: 开发梯度元学习（GML）框架，直接将优化梯度输入轻量级神经网络，无需预训练即可快速适应。针对独立相位和耦合相位两种STAR-RIS模型，设计了专门的GML方案，有效处理各自的幅度和相位约束。

Result: 所提方法在加权和速率性能上接近交替优化基准，同时大幅降低计算开销。当基站天线数和STAR-RIS单元数增加时，复杂度呈近线性增长，相比交替优化实现高达10倍的运行速度提升。

Conclusion: GML方法为大规模STAR-RIS辅助通信提供了可扩展且实用的解决方案，在保持性能的同时显著降低了计算复杂度，验证了该框架在大规模系统中的适用性。

Abstract: Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged as a promising technology to realize full-space coverage and boost spectral efficiency in next-generation wireless networks. Yet, the joint design of the base station precoding matrix as well as the STAR-RIS transmission and reflection coefficient matrices leads to a high-dimensional, strongly nonconvex, and NP-hard optimization problem. Conventional alternating optimization (AO) schemes typically involve repeated large-scale matrix inversion operations, resulting in high computational complexity and poor scalability, while existing deep learning approaches often rely on expensive pre-training and large network models. In this paper, we develop a gradient-based meta learning (GML) framework that directly feeds optimization gradients into lightweight neural networks, thereby removing the need for pre-training and enabling fast adaptation. Specifically, we design dedicated GML-based schemes for both independent-phase and coupled-phase STAR-RIS models, effectively handling their respective amplitude and phase constraints while achieving weighted sum-rate performance very close to that of AO-based benchmarks. Extensive simulations demonstrate that, for both phase models, the proposed methods substantially reduce computational overhead, with complexity growing nearly linearly when the number of BS antennas and STAR-RIS elements grows, and yielding up to 10 times runtime speedup over AO, which confirms the scalability and practicality of the proposed GML method for large-scale STAR-RIS-assisted communications.

</details>


### [3] [Deep Learning Surrogate for Fast CIR Prediction in Reactive Molecular Diffusion Advection Channels](https://arxiv.org/abs/2512.18071)
*Meysam Ghanbari,Mohammad Taghi Dabiri,Mazen Hasna,Tanvir Alam,Khalid Qaraqe*

Main category: eess.SP

TL;DR: 开发基于深度学习的分子通信信道冲激响应（CIR）替代模型，用于高效替代计算昂贵的反应扩散-对流偏微分方程求解


<details>
  <summary>Details</summary>
Motivation: 分子通信中准确建模信道冲激响应需要求解耦合的反应扩散-对流方程，在大规模参数扫描或设计循环中计算成本极高，需要开发高效的替代模型

Method: 基于物理的PDE-ODE模型生成大规模CIR数据集，训练神经网络直接映射传输、反应和几何参数到CIR，构建深度学习替代模型

Result: 替代模型在独立测试集上表现优异：90%测试信道的预测误差低于0.15（NRMSE），且对单个参数依赖较弱，计算效率显著提升

Conclusion: 深度学习替代模型为分子通信系统分析和设计提供了准确且计算高效的CIR评估方案，可替代重复的PDE求解

Abstract: Accurate channel impulse response (CIR) modeling in molecular communication (MC) often requires solving coupled reactive diffusion-advection equations, which is computationally expensive for large parameter sweeps or design loops. We develop a deep-learning surrogate for a three-dimensional duct MC channel with reactive diffusion-advection transport and reversible ligand-receptor binding on a finite ring receiver. Using a physics-based partial differential equation (PDE)-ordinary differential equation (ODE) model, we generate a large CIR dataset across broad transport, reaction, and geometric ranges and train a neural network that maps these parameters directly to the CIR. On an independent test set, the surrogate closely matches reference CIRs both qualitatively and quantitatively: the empirical cumulative distribution function (CDF) of the normalized root mean square error (NRMSE) shows that 90% of test channels are predicted with error below 0.15, with only weak dependence on individual parameters. The surrogate therefore offers an accurate and computationally efficient replacement for repeated PDE-based CIR evaluations in MC system analysis and design.

</details>


### [4] [Robust Beamforming for Pinching-Antenna Systems](https://arxiv.org/abs/2512.18075)
*Mingjun Sun,Chongjun Ouyang,Shaochuan Wu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出针对Pinching-antenna系统的鲁棒波束成形框架，解决信道不确定性问题，在损耗和无损耗波导中分别采用SOCP和闭式解，通过位置优化提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多假设完美信道状态信息，忽略了信道不确定性对Pinching-antenna系统性能的影响，需要开发鲁棒波束成形方案来应对实际信道条件。

Method: 针对损耗和无损耗波导分别设计：损耗情况采用二阶锥规划求解基带波束成形，无损耗情况通过最大比传输获得闭式解；使用Gauss-Seidel方法优化pinching antennas的位置。

Result: 数值结果验证了算法的有效性，表明PASS相比传统固定天线系统对信道不确定性具有更强的鲁棒性，其最差可达速率甚至能超过完美CSI下的固定天线基准。

Conclusion: 提出的鲁棒波束成形框架成功解决了PASS系统中的信道不确定性问题，显著提升了系统鲁棒性和性能，为实际部署提供了有效解决方案。

Abstract: Pinching-antenna system (PASS) mitigates large-scale path loss by enabling flexible placement of pinching antennas (PAs) along the dielectric waveguide. However, most existing studies assume perfect channel state information (CSI), overlooking the impact of channel uncertainty. This paper addresses this gap by proposing a robust beamforming framework for both lossy and lossless waveguides. For baseband beamforming, the lossy case yields an second-order cone programming-based solution, while the lossless case admits a closed-form solution via maximum ratio transmission. The PAs' positions in both cases are optimized through the Gauss-Seidel-based method. Numerical results validate the effectiveness of the proposed algorithm and demonstrate that PASS exhibits superior robustness against channel uncertainty compared with conventional fixed-antenna systems. Notably, its worst-case achievable rate can even exceed the fixed-antenna baseline under perfect CSI.

</details>


### [5] [AI Assisted Next Gen Outdoor Optical Networks: Camera Sensing for Monitoring and User Localization](https://arxiv.org/abs/2512.18087)
*Meysam Ghanbari,Mohammad Taghi Dabiri,Rula Ammuri,Mazen Hasna,Khalid Qaraqe*

Main category: eess.SP

TL;DR: 提出了一种用于室外光学接入点的用户定位监控系统，通过光电探测器阵列的空间强度测量推断用户位置，采用光学模型与轻量级数据驱动校准的混合方法，实现高精度低计算成本。


<details>
  <summary>Details</summary>
Motivation: 室外光学接入点虽然提供高数据速率和强物理层安全性，但实际部署仍存在漏洞和滥用模式，需要专门的监控层来确保网络安全。

Method: 提出混合方法：结合光学信息前向模型和稀疏模型反演，加上轻量级数据驱动校准阶段。该方法保持基于模型重建的可解释性和稳定性，同时利用学习吸收残余非理想性和设备特定失真。

Result: 在相同硬件和训练条件下（50万样本），混合方法比通用深度学习基线获得更低的均方误差，同时使用更少的训练时间和计算资源。精度随阵列分辨率提高，在60×60-80×80左右达到饱和，显示良好的精度-复杂度权衡。

Conclusion: 该系统可实现连续监控、异常检测（如潜在窃听）和访问控制，为室外光接入网络提供实用的定位监控解决方案，平衡了精度、计算成本和实时部署需求。

Abstract: We consider outdoor optical access points (OAPs), which, enabled by recent advances in metasurface technology, have attracted growing interest. While OAPs promise high data rates and strong physical-layer security, practical deployments still expose vulnerabilities and misuse patterns that necessitate a dedicated monitoring layer - the focus of this work. We therefore propose a user positioning and monitoring system that infers locations from spatial intensity measurements on a photodetector (PD) array. Specifically, our hybrid approach couples an optics-informed forward model and sparse, model-based inversion with a lightweight data-driven calibration stage, yielding high accuracy at low computational cost. This design preserves the interpretability and stability of model-based reconstruction while leveraging learning to absorb residual nonidealities and device-specific distortions. Under identical hardware and training conditions (both with 5 x 10^5 samples), the hybrid method attains consistently lower mean-squared error than a generic deep-learning baseline while using substantially less training time and compute. Accuracy improves with array resolution and saturates around 60 x 60-80 x 80, indicating a favorable accuracy-complexity trade-off for real-time deployment. The resulting position estimates can be cross-checked with real-time network logs to enable continuous monitoring, anomaly detection (e.g., potential eavesdropping), and access control in outdoor optical access networks.

</details>


### [6] [CV Quantum Communications with Angular Rejection Filtering: Modeling and Security Analysis](https://arxiv.org/abs/2512.18097)
*Mohammad Taghi Dabiri,Meysam Ghanbari,Rula Ammuri,Saif Al-Kuwari,Mazen Hasna,Khalid Qaraqe*

Main category: eess.SP

TL;DR: 该论文提出在自由空间CVQKD系统中使用角度拒绝滤波器创建安全区，以减轻湍流、指向误差和角度泄漏带来的安全威胁，通过系统建模和参数优化显著提升密钥安全率。


<details>
  <summary>Details</summary>
Motivation: 自由空间连续变量量子密钥分发面临湍流、指向误差和角度泄漏等安全威胁，这些漏洞可能被窃听者利用，需要有效方法来增强系统安全性。

Method: 开发了包含湍流、失准和安全区效应的系统与信道模型，推导信息论度量评估安全性，使用角度拒绝滤波器在接收端定义安全区，阻挡来自期望锥形区域外的信号。

Result: 仿真结果表明安全区能显著减少信息泄漏，光束腰、角度阈值和孔径尺寸的精细调优对最大化密钥安全率至关重要，较大孔径提升性能但增加接收器尺寸，长距离链路需要低于100微弧度的对准精度。

Conclusion: 安全区实施和参数优化是实用且安全的CV-QKD系统的有效策略，为自由空间量子通信提供了重要的安全保障方案。

Abstract: Continuous-variable quantum key distribution (CVQKD) over free-space optical links is a promising approach for secure communication, but its performance is limited by turbulence, pointing errors, and angular leakage that can be exploited by an eavesdropper. To mitigate this, we consider an angular rejection filter that defines a safe-zone at the receiver and blocks signals from outside the desired cone. A system and channel model is developed including turbulence, misalignment, and safe-zone effects, and information theoretic metrics are derived to evaluate security. Simulation results show that the safe zone significantly reduces information leakage and that careful tuning of beam waist, angular threshold, and aperture size is essential for maximizing the secret key rate. Larger apertures improve performance but increase receiver size, while longer links require sub 100 urad alignment accuracy. These results highlight safe-zone enforcement and parameter optimization as effective strategies for practical and secure CV-QKD.

</details>


### [7] [Two-Stage Signal Reconstruction for Amplitude-Phase-Time Block Modulation-based Communications](https://arxiv.org/abs/2512.18326)
*Meidong Xia,Min Fan,Wei Xu,Haiming Wang,Xiaohu You*

Main category: eess.SP

TL;DR: 本文提出了一种两阶段信号重建算法，通过粗重建和精重建阶段分别处理非线性失真的主导和残余分量，显著降低了功率放大器的输入回退需求，提高了效率。


<details>
  <summary>Details</summary>
Motivation: 功率放大器在低输入回退下工作可提高效率，但会引入严重的非线性失真。现有的APTBM重建方法仅启发式和统计地应用约束，限制了可实现的IBO降低和PA效率提升。

Method: 将非线性失真分解为主导和残余分量，开发两阶段信号重建算法：粗重建阶段利用APTBM块结构和PA非线性特性消除主导失真；精重建阶段通过非凸优化问题最小化残余失真，使用低复杂度迭代变量替换方法将其松弛为一系列可闭式求解的信任域子问题。

Result: 仿真中实现高达4 dB的IBO降低，实验中实现高达2 dB的IBO降低，同时保持传输性能，相比现有方法分别提高PA效率59.1%和33.9%。

Conclusion: 提出的两阶段算法通过系统处理非线性失真，显著提高了功率放大器的效率，为低IBO操作提供了有效的解决方案。

Abstract: Operating power amplifiers (PAs) at lower input back-off (IBO) levels is an effective way to improve PA efficiency, but often introduces severe nonlinear distortion that degrades transmission performance. Amplitude-phase-time block modulation (APTBM) has recently emerged as an effective solution to this problem. By leveraging the intrinsic amplitude and phase constraints of each APTBM block, PA-induced nonlinear distortion can be mitigated through constraint-guided signal reconstruction. However, existing reconstruction methods apply these constraints only heuristically and statistically, limiting the achievable IBO reduction and PA efficiency improvement. This paper addresses this limitation by decomposing the nonlinear distortion into dominant and residual components, and accordingly develops a novel two-stage signal reconstruction algorithm consisting of coarse and fine reconstruction stages. The coarse reconstruction stage eliminates the dominant distortion by jointly exploiting the APTBM block structure and PA nonlinear characteristics. The fine reconstruction stage minimizes the residual distortion by formulating a nonconvex optimization problem that explicitly enforces the APTBM constraints. To handle this problem efficiently, a low-complexity iterative variable substitution method is introduced, which relaxes the problem into a sequence of trust-region subproblems, each solvable in closed form. The proposed algorithm is validated through comprehensive numerical simulations and testbed experiments. Results show that it achieves up to 4 dB IBO reduction in simulations and up to 2 dB IBO reduction in experiments while maintaining transmission performance, corresponding to PA efficiency improvements of 59.1\% and 33.9\%, respectively, over existing methods.

</details>


### [8] [Cognitive Inference based Feature Pyramid Network for Sentimental Analysis using EEG Signals](https://arxiv.org/abs/2512.18346)
*Vishesh Bhardwaj,Aman Yadav,Srikireddy Dhanunjay Reddy,Tharun Kumar Reddy Bollu*

Main category: eess.SP

TL;DR: 提出一种基于特征金字塔网络(FPN)和门控循环单元(GRU)的编码器模型，用于EEG信号的情感分析，在ZUCO 2.0数据集上相比现有方法获得6.88%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 利用脑电图(EEG)信号进行情感分析能更深入地理解人的情绪状态，实时监测情绪波动，在心理健康监测、情感计算和个性化用户体验等领域有重要应用价值。

Method: 提出基于特征金字塔网络(FPN)的编码器模型，将FPN适配于EEG传感器数据以进行多尺度特征提取，捕获局部和全局的情感相关模式。提取的特征再输入门控循环单元(GRU)建模时序依赖，增强情感分类准确性。使用ZUCO 2.0数据集（128通道）进行训练和评估。

Result: 提出的架构在ZUCO 2.0数据集上相比现有方法获得6.88%的性能提升。此外，在DEAP和SEED验证数据集上也证明了框架的有效性。

Conclusion: 基于FPN和GRU的EEG情感分析模型能有效提取多尺度时空特征，显著提升情感分类性能，为EEG信号的情感分析提供了有效的深度学习框架。

Abstract: Sentiment analysis using Electroencephalography (EEG) sensor signals provides a deeper behavioral understanding of a person's emotional state, offering insights into real-time mood fluctuations. This approach takes advantage of brain electrical activity, making it a promising tool for various applications, including mental health monitoring, affective computing, and personalised user experiences. An encoder-based model for EEG-to-sentiment analysis, utilizing the ZUCO 2.0 dataset and incorporating a Feature Pyramid Network (FPN), is proposed to enhance this process. FPNs are adapted here for EEG sensor data, enabling multiscale feature extraction to capture local and global sentiment-related patterns. The raw EEG sensor data from the ZUCO 2.0 dataset is pre-processed and passed through the FPN, which extracts hierarchical features. In addition, extracted features are passed to a Gated Recurrent Unit (GRU) to model temporal dependencies, thereby enhancing the accuracy of sentiment classification. The ZUCO 2.0 dataset is utilized for its clear and detailed representation in 128 channels, offering rich spatial and temporal resolution. The experimental metric results show that the proposed architecture achieves a 6.88\% performance gain compared to the existing methods. Furthermore, the proposed framework demonstrated its efficacy on the validation datasets DEAP and SEED.

</details>


### [9] [RIS-Aided Spatial Nulling: Algorithms, Analysis, and Nulling Limits](https://arxiv.org/abs/2512.18426)
*Xinrui Li,R. Michael Buehrer,Steven W. Ellingson*

Main category: eess.SP

TL;DR: 该论文研究利用可重构智能表面(RIS)辅助反射面天线进行空间干扰抑制，针对连续相位和离散相位情况分别提出高效算法，并分析RIS辅助反射器的干扰抑制能力极限。


<details>
  <summary>Details</summary>
Motivation: 随着卫星和地面发射器密度增加，需要更快速收敛和性能更好的算法来抑制旁瓣干扰。RIS作为动态塑造无线传播环境的手段，在大型地面反射面天线中可作为空间干扰抑制的辅助机制，提升系统性能。

Method: 针对RIS辅助反射面天线，研究特定方向和频率下的旁瓣抑制算法。连续相位情况：采用梯度投影(GP)和交替投影(AP)算法提高可扩展性，并提出闭式近似最优解降低复杂度。离散相位情况：使用惩罚方法重新表述问题，通过主最小化求解，优于早期工作的启发式方法。分析多个干扰方向和频率下的电场特性，量化RIS辅助反射器的抑制能力。

Result: 仿真结果表明所提方法有效，并验证了理论抑制极限。连续相位情况下，闭式近似最优解能以显著降低的复杂度实现满意的抑制性能。离散相位情况下，新方法优于早期工作的启发式方法。同时识别出实现完美抑制的单模权重存在的简单判据。

Conclusion: RIS辅助反射面天线能有效实现空间干扰抑制，所提出的连续相位和离散相位算法在性能和复杂度之间取得良好平衡，为实际系统设计提供了理论指导和实用解决方案。

Abstract: Reconfigurable Intelligent Surfaces (RIS) have recently gained attention as a means to dynamically shape the wireless propagation environment through programmable reflection control. Among the numerous applications, an important emerging use case is employing RIS as an auxiliary mechanism for spatial interference nulling, particularly in large ground-based reflector antennas where sidelobe interference can significantly degrade the system performance. With the growing density of satellites and terrestrial emitters, algorithms with faster convergence speed and better performance are needed. This work investigates RIS-equipped reflector antennas as a representative example of RIS-assisted spatial nulling and develop algorithms for sidelobe cancellation at specific directions and frequencies under various constraints. For the continuous-phase case, we adapt the gradient projection (GP) and alternating projection (AP) algorithms for scalability and propose a closed-form near-optimal solution that achieves satisfactory nulling performance with significantly reduced complexity. For the discrete-phase case, we reformulate the problem using a penalty method and solve it via majorization-minimization, outperforming the heuristic methods from our earlier work. Further, we analyze the electric field characteristics across multiple interference directions and frequencies to quantify the nulling capability of the RIS-aided reflectors, and identify a simple criterion for the existence of unimodular weights enabling perfect nulls. Simulation results demonstrate the effectiveness of the proposed methods and confirm the theoretical nulling limits.

</details>


### [10] [On the Limits of Coherent Time-Domain Cancellation of Radio Frequency Interference](https://arxiv.org/abs/2512.18427)
*Xinrui Li,R. Michael Buehrer*

Main category: eess.SP

TL;DR: 该论文提出了一种基于解调-重调制的射频干扰消除方法，通过解调未知干扰信号、创建无噪声干扰副本并相干相减来消除干扰，推导了干扰抑制比的理论表达式，并通过仿真验证了该方法在不同场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 在许多传感和雷达应用中，目标信号的带宽更宽或功率更弱，使其难以从背景噪声中区分。被动雷达、认知无线电、低截获概率雷达和射电天文等应用需要消除射频干扰才能检测到目标信号。

Method: 采用解调-重调制干扰消除框架：解调未知干扰信号，创建无噪声干扰副本，然后相干地从接收信号中减去该副本。使用干扰抑制比作为性能指标，推导了IRR与信号参数最优估计方差之间的解析表达式。

Result: 仿真结果验证了IRR解析表达式的准确性，表明该方法在足够高的干扰噪声比下能显著抑制干扰，增强目标信号的检测和提取。理论分析还扩展到目标信号高于噪声底的情况，并与其他时域消除方法进行了比较。

Conclusion: 解调-重调制技术能有效消除射频干扰，特别是在干扰噪声比足够高时。通过与其他方法的比较，确定了各种方法适用的条件，为不同场景下的干扰抑制提供了实用指导。

Abstract: In many sensing (viz., radio astronomy) and radar applications, the received signal of interest (SOI) exhibits a significantly wider bandwidth or weaker power than the interference signal, rendering it indistinguishable from the background noise. Such scenarios arise frequently in applications such as passive radar, cognitive radio, low-probability-of-intercept (LPI) radar, and planetary radar for radio astronomy, where canceling the radio frequency interference (RFI) is critical for uncovering the SOI. In this work, we examine the Demodulation-Remodulation (Demod-Remod) based interference cancellation framework for the RFI. This approach demodulates the unknown interference, creates a noise-free interference replica, and coherently subtracts it from the received signal. To evaluate the performance limits, we employ the performance metric termed \textit{interference rejection ratio} (IRR), which quantifies the interference canceled. We derive the analytical expressions of IRR as a function of the optimal estimation variances of the signal parameters. Simulation results confirm the accuracy of the analytical expression for both single-carrier and multi-carrier interference signals and demonstrate that the method can substantially suppress the interference at a sufficient interference-to-noise ratio (INR), enabling enhanced detection and extraction of the SOI. We further extend the analysis to the scenario where the SOI is above the noise floor, and confirm the validity of the theoretical IRR expression in this scenario. Lastly, we compare the Demod-Remod technique to other time-domain cancellation methods. The result of the comparison identifies the conditions under which each method is preferred, offering practical guidelines for interference mitigation under different scenarios.

</details>


### [11] [The Choice of Line Lengths in Multiline Thru-Reflect-Line Calibration](https://arxiv.org/abs/2512.18641)
*Ziad Hatab,Michael Gadringer,Wolfgang Bösch*

Main category: eess.SP

TL;DR: 本文提出了一种确定多线TRL校准中线标准最优长度的分析和严格程序，通过非线性约束优化求解，并提出基于预定义稀疏尺度的简化近优长度选择方法。


<details>
  <summary>Details</summary>
Motivation: 在多线TRL校准中，线标准长度的选择对校准精度和带宽有重要影响。现有方法可能无法提供最优长度，导致校准不确定性较高。需要一种系统化方法来确定最优线长，以提高矢量网络分析仪的校准精度。

Method: 1. 通过非线性约束优化解决多线TRL校准中的特征值问题来确定最优线长
2. 提出基于预定义稀疏尺度的简化方法进行近优长度选择
3. 讨论满足带宽要求所需的线数
4. 通过蒙特卡洛模拟数值评估所提程序的灵敏度

Result: 1. 推导出的线长比现有行业标准具有更低的不确定性
2. 灵敏度分析验证了所提方法的鲁棒性
3. 提供了包括有耗线、色散线和波导带通解决方案在内的各种实际应用示例

Conclusion: 本文提出的系统化方法能够确定多线TRL校准中的最优线长，相比现有标准具有更低的校准不确定性。该方法适用于各种实际应用场景，为矢量网络分析仪的高精度校准提供了有效解决方案。

Abstract: This paper presents an analysis and rigorous procedure for determining the optimal lengths of line standards in multiline thru-reflect-line (TRL) calibration of vector network analyzers (VNAs). The solution is obtained through nonlinear constrained optimization of the eigenvalue problem in multiline TRL calibration. Additionally, we propose a simplified approach for near-optimal length selection based on predefined sparse rulers. Alongside the length calculation, we discuss the required number of lines to meet bandwidth requirements. The sensitivity of the proposed procedure is evaluated numerically via Monte Carlo simulations, demonstrating that the derived lengths have lower uncertainty than those from existing industry standards. Practical examples are provided for various applications, including lossy and dispersive lines, as well as banded solutions for waveguides.

</details>


### [12] [Multi-Waveguide Pinching Antenna Placement Optimization for Rate Maximization](https://arxiv.org/abs/2512.18711)
*Yue Zhang,Yaru Fu,Pei Liu,Yalin Liu,Kevin Hung*

Main category: eess.SP

TL;DR: 本文提出了一种基于分数规划和投影梯度上升的算法，用于优化多波导Pinching天线系统的天线布局，以最大化平均每用户数据率，同时满足严格的最小间距约束。


<details>
  <summary>Details</summary>
Motivation: Pinching天线系统能够大规模移动天线元件，为下一代无线网络带来显著的性能提升潜力。然而，优化天线布局面临复杂挑战，包括耦合的分数目标函数和非凸约束。

Method: 采用分数规划框架将非凸的速率最大化问题转化为更易处理的形式，并设计基于投影梯度上升的迭代算法来求解转化后的问题。

Result: 仿真结果表明，所提出的方案显著优于各种几何布局基准方法，通过主动减轻多用户干扰实现了更优的每用户数据率。

Conclusion: 该研究为Pinching天线系统的天线布局优化提供了有效的解决方案，能够显著提升无线网络性能，特别是在多用户干扰管理方面表现出色。

Abstract: Pinching antenna systems (PASS) have emerged as a technology that enables the large-scale movement of antenna elements, offering significant potential for performance gains in next-generation wireless networks. This paper investigates the problem of maximizing the average per-user data rate by optimizing the antenna placement of a multi-waveguide PASS, subject to a stringent physical minimum spacing constraint. To address this complex challenge, which involves a coupled fractional objective and a non-convex constraint, we employ the fractional programming (FP) framework to transform the non-convex rate maximization problem into a more tractable one, and devise a projected gradient ascent (PGA)-based algorithm to iteratively solve the transformed problem. Simulation results demonstrate that our proposed scheme significantly outperforms various geometric placement baselines, achieving superior per-user data rates by actively mitigating multi-user interference.

</details>


### [13] [DeepGuard: Defending Deep Joint Source-Channel Coding Against Eavesdropping at Physical-Layer](https://arxiv.org/abs/2512.18715)
*Kaiyi Chi,Yinghui He,Qianqian Yang,Yuanchao Shu,Zhiqin Wang,Jun Luo,Jiming Chen*

Main category: eess.SP

TL;DR: DeepGuard是首个针对DeepJSCC窃听攻击的物理层防御框架，通过前导码扰动机制在保护合法用户通信的同时显著降低窃听性能。


<details>
  <summary>Details</summary>
Motivation: DeepJSCC虽然高效，但其内在特性增加了被窃听的脆弱性。现有防御方法存在计算开销大或影响合法用户性能的问题，且现有窃听攻击研究仅限于理想信道仿真。

Method: 提出DeepGuard防御框架：1）识别并实现四种代表性的OFDM系统窃听攻击；2）设计新颖的前导码扰动机制，仅修改合法收发器共享的前导码；3）理论分析扰动对窃听信号的影响；4）开发端到端扰动优化算法。

Result: 通过SDR原型和广泛的空中实验验证，DeepGuard能有效缓解窃听威胁，在保护合法用户可靠通信的同时显著降低窃听性能。

Conclusion: DeepGuard是首个针对DeepJSCC窃听攻击的物理层防御框架，通过前导码扰动机制实现了安全与性能的良好平衡，为实际部署提供了可行方案。

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a promising paradigm for efficient and robust information transmission. However, its intrinsic characteristics also pose new security challenges, notably an increased vulnerability to eavesdropping attacks. Existing studies on defending against eavesdropping attacks in DeepJSCC, while demonstrating certain effectiveness, often incur considerable computational overhead or introduce performance trade-offs that may adversely affect legitimate users. In this paper, we present DeepGuard, to the best of our knowledge, the first physical-layer defense framework for DeepJSCC against eavesdropping attacks, validated through over-the-air experiments using software-defined radios (SDRs). Considering that existing eavesdropping attacks against DeepJSCC are limited to simulation under ideal channels, we take a step further by identifying and implementing four representative types of attacks under various configurations in orthogonal frequency-division multiplexing systems. These attacks are evaluated over-the-air under diverse scenarios, allowing us to comprehensively characterize the real-world threat landscape. To mitigate these threats, DeepGuard introduces a novel preamble perturbation mechanism that modifies the preamble shared only between legitimate transceivers. To realize it, we first conduct a theoretical analysis of the perturbation's impact on the signals intercepted by the eavesdropper. Building upon this, we develop an end-to-end perturbation optimization algorithm that significantly degrades eavesdropping performance while preserving reliable communication for legitimate users. We prototype DeepGuard using SDRs and conduct extensive over-the-air experiments in practical scenarios. Extensive experiments demonstrate that DeepGuard effectively mitigates eavesdropping threats.

</details>


### [14] [Decentralized GNSS at Global Scale via Graph-Aware Diffusion Adaptation](https://arxiv.org/abs/2512.18773)
*Xue Xian Zheng,Xing Liu,Tareq Y. Al-Naffouri*

Main category: eess.SP

TL;DR: 提出一种基于图神经网络和梯度追踪扩散策略的分布式GNSS架构，实现厘米级自定位和全网卫星校正产品共识，性能媲美集中式方法但收敛更快、通信开销更低。


<details>
  <summary>Details</summary>
Motivation: 当前GNSS系统依赖集中式处理中心，存在可扩展性、鲁棒性和延迟方面的限制。需要一种去中心化架构来提升系统性能。

Method: 将接收器网络建模为时变图，使用深度线性神经网络学习拓扑感知的混合调度，采用梯度追踪扩散策略实现局部推理和简洁消息交换。

Result: 方法在精度上匹配集中式基准，同时在收敛速度和通信开销方面显著优于现有去中心化方法，支持PPP和PPP-RTK精确定位。

Conclusion: 通过将去中心化GNSS重构为网络信号处理问题，为整合去中心化优化、共识推理和图感知学习到实际卫星导航系统中开辟了新途径。

Abstract: Network-based Global Navigation Satellite Systems (GNSS) underpin critical infrastructure and autonomous systems, yet typically rely on centralized processing hubs that limit scalability, resilience, and latency. Here we report a global-scale, decentralized GNSS architecture spanning hundreds of ground stations. By modeling the receiver network as a time-varying graph, we employ a deep linear neural network approach to learn topology-aware mixing schedules that optimize information exchange. This enables a gradient tracking diffusion strategy wherein stations execute local inference and exchange succinct messages to achieve two concurrent objectives: centimeter-level self-localization and network-wide consensus on satellite correction products. The consensus products are broadcast to user receivers as corrections, supporting precise point positioning (PPP) and precise point positioning-real-time kinematic (PPP-RTK). Numerical results demonstrate that our method matches the accuracy of centralized baselines while significantly outperforming existing decentralized methods in convergence speed and communication overhead. By reframing decentralized GNSS as a networked signal processing problem, our results pave the way for integrating decentralized optimization, consensus-based inference, and graph-aware learning as effective tools in operational satellite navigation.

</details>


### [15] [Domain Adaptation in Structural Health Monitoring of Civil Infrastructure: A Systematic Review](https://arxiv.org/abs/2512.18780)
*Yifeng Zhang,Xiao Liang*

Main category: eess.SP

TL;DR: 本文全面回顾了基于振动的结构健康监测中的领域自适应技术，分析了60多项代表性研究，总结了方法演进、应用贡献与局限，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型在土木结构评估中应用日益广泛，但跨几何、材料和环境条件转移知识仍面临重大挑战。领域自适应提供系统方法来解决这些差异，对齐模拟、实验室和现场域的特征分布，同时保持损伤相关信息的敏感性。

Method: 基于60多项代表性研究，分析SHM中DA方法的演进，包括：统计对齐、对抗学习和子域学习、物理信息自适应、以及模拟到真实转移的生成建模。总结桥梁和建筑应用中的贡献与局限。

Result: DA显著改善了泛化能力，但仍存在关键挑战：管理域差异、解决数据稀缺、增强模型可解释性、适应多源和时变条件。揭示了现有泛化和可信度方面的障碍。

Conclusion: 未来研究方向包括：将物理约束整合到学习目标中、开发物理一致的生成框架以增强数据真实性、建立可解释和可认证的DA系统、推进多源和终身自适应以实现可扩展监测。最终目标是实现透明、物理感知和自适应监测系统，支持土木基础设施的长期韧性。

Abstract: This study provides a comprehensive review of domain adaptation (DA) techniques in vibration-based structural health monitoring (SHM). As data-driven models increasingly support the assessment of civil structures, the persistent challenge of transferring knowledge across varying geometries, materials, and environmental conditions remains a major obstacle. DA offers a systematic approach to mitigate these discrepancies by aligning feature distributions between simulated, laboratory, and field domains while preserving the sensitivity of damage-related information. Drawing on more than sixty representative studies, this paper analyzes the evolution of DA methods for SHM, including statistical alignment, adversarial and subdomain learning, physics-informed adaptation, and generative modeling for simulation-to-real transfer. The review summarizes their contributions and limitations across bridge and building applications, revealing that while DA has improved generalization significantly, key challenges persist: managing domain discrepancy, addressing data scarcity, enhancing model interpretability, and enabling adaptability to multiple sources and time-varying conditions. Future research directions emphasize integrating physical constraints into learning objectives, developing physics-consistent generative frameworks to enhance data realism, establishing interpretable and certifiable DA systems for engineering practice, and advancing multi-source and lifelong adaptation for scalable monitoring. Overall, this review consolidates the methodological foundation of DA for SHM, identifies existing barriers to generalization and trust, and outlines the technological trajectory toward transparent, physics-aware, and adaptive monitoring systems that support the long-term resilience of civil infrastructure.

</details>


### [16] [RIS-Enabled Smart Wireless Environments: Fundamentals and Distributed Optimization](https://arxiv.org/abs/2512.18788)
*George C. Alexandropoulos,Kostantinos D. Katsanos,George Stamatelis,Ioannis Gavras*

Main category: eess.SP

TL;DR: 该论文综述了基于可重构智能表面的智能无线环境概念，介绍了可编程超表面的工作原理和硬件架构，讨论了RIS使能的SWE的关键性能指标和应用场景，并提出了两种基于超越对角RIS的分布式设计方案。


<details>
  <summary>Details</summary>
Motivation: 随着可重构智能表面技术的兴起，需要探索如何利用RIS创建智能无线环境，以提升无线通信系统的性能，包括频谱效率、能量效率、物理层安全等关键指标。

Method: 首先介绍可编程超表面的工作原理和硬件架构，然后提出两种基于超越对角RIS的分布式设计方案：1）基于多分支注意力卷积神经网络、参数共享和神经进化训练的混合分布式机器学习框架；2）针对宽带干扰MISO广播信道的协同优化框架，联合设计基站预编码器和所有超表面的可调电容和开关矩阵。

Result: 分布式优化的RIS使能SWE实现了接近最优的和速率性能，且在线计算复杂度低；设计的RIS使能SWE在多小区MISO网络中相比基准方案（非协同配置和传统对角超表面）展现出优越的和速率性能。

Conclusion: 基于超越对角RIS的智能无线环境设计能够显著提升无线通信系统性能，分布式优化方法在保证性能的同时降低了计算复杂度，为未来智能无线环境的发展提供了有效解决方案。

Abstract: This chapter overviews the concept of Smart Wireless Environments (SWEs) motivated by the emerging technology of Reconfigurable Intelligent Surfaces (RISs). The operating principles and state-of-the-art hardware architectures of programmable metasurfaces are first introduced. Subsequently, key performance objectives and use cases of RIS-enabled SWEs, including spectral and energy efficiency, physical-layer security, integrated sensing and communications, as well as the emerging paradigm of over-the-air computing, are discussed. Focusing on the recent trend of Beyond-Diagonal (BD) RISs, two distributed designs of respective SWEs are presented. The first deals with a multi-user Multiple-Input Single-Output (MISO) system operating within the area of influence of a SWE comprising multiple BD-RISs. A hybrid distributed and fusion machine learning framework based on multi-branch attention-based convolutional Neural Networks (NNs), NN parameter sharing, and neuroevolutionary training is presented, which enables online mapping of channel realizations to the BD-RIS configurations as well as the multi-user transmit precoder. Performance evaluation results showcase that the distributedly optimized RIS-enabled SWE achieves near-optimal sum-rate performance with low online computational complexity. The second design focuses on the wideband interference MISO broadcast channel, where each base station exclusively controls one BD-RIS to serve its assigned group of users. A cooperative optimization framework that jointly designs the base station transmit precoders as well as the tunable capacitances and switch matrices of all metasurfaces is presented. Numerical results demonstrating the superior sum-rate performance of the designed RIS-enabled SWE for multi-cell MISO networks over benchmark schemes, considering non-cooperative configuration and conventional diagonal metasurfaces, are presented.

</details>


### [17] [A 100-GHz CMOS-Compatible RIS-on-Chip Based on Phase-Delay Lines for 6G Applications](https://arxiv.org/abs/2512.18854)
*Xiarui Su,Xihui Teng,Yiyang Yu,Yiming Yang,Atif Shamim*

Main category: eess.SP

TL;DR: 提出了一种CMOS兼容的片上可重构智能表面，首次实现波束赋形，通过VO2相变材料实现180度相位差，测量显示ON/OFF状态间有27.1dB增强。


<details>
  <summary>Details</summary>
Motivation: 片上可重构智能表面对6G通信系统至关重要，需要开发低损耗、CMOS兼容的波束赋形解决方案。

Method: 设计结合缝隙、VO2相位延迟线和接地的单元结构，VO2两种状态在中心频率产生180度相位差，反射幅度优于-1.2dB，并设计60×60 RIS阵列。

Result: 原型测量显示ON/OFF状态间有27.1dB增强，单元在中心频率保持180度相位差，反射幅度优于-1.2dB，验证了波束赋形能力。

Conclusion: 提出的RIS具有低损耗、CMOS兼容优势，为未来6G应用提供了基础，首次实现了片上波束赋形能力。

Abstract: On-chip reconfigurable intelligent surfaces (RIS) are expected to play a vital role in future 6G communication systems. This work proposed a CMOS-compatible on-chip RIS capable of achieving beam steering for the first time. The proposed unit cell design is a combination of a slot, a phase-delay line with VO2, and a ground. Under the two states of the VO2, the unit cell has a 180 deg phase difference at the center frequency, while maintaining reflection magnitudes better than -1.2 dB. Moreover, a 60by60 RIS array based on the present novel unit is designed, demonstrating the beam-steering capability. Finally, to validate the design concept, a prototype is fabricated, and the detailed fabrication process is presented. The measurement result demonstrates a 27.1 dB enhancement between ON and OFF states. The proposed RIS has the advantages of low loss, CMOS-compatibility, providing a foundation for future 6G applications.

</details>


### [18] [Decentralized Cooperative Beamforming for Networked LEO Satellites with Statistical CSI](https://arxiv.org/abs/2512.18890)
*Yuchen Zhang,Eva Lagunas,Xue Xian Zheng,Symeon Chatzinotas,Tareq Y. Al-Naffouri*

Main category: eess.SP

TL;DR: 提出了一种完全去中心化的LEO卫星协同波束成形框架，通过局部化基准算法和全局耦合变量交换，在任意连接的星间网络中实现可扩展的分布式执行。


<details>
  <summary>Details</summary>
Motivation: LEO卫星星座正朝着支持星座级协作的网络化架构发展，但集中式波束成形设计存在可扩展性限制，而大型LEO星座又有严格的计算和信令约束，需要解决这些实际挑战。

Method: 首先推导了集中式加权最小均方误差(WMMSE)解作为性能基准，然后提出了拓扑无关的去中心化波束成形算法，通过局部化基准并交换一组全局耦合变量，在任意连接的星间网络中实现共识，支持卫星间完全并行执行。

Result: 仿真结果表明，所提出的去中心化方案在实际星间拓扑下接近集中式性能，同时显著降低了计算复杂度和信令开销，实现了大型LEO星座的可扩展协同波束成形。

Conclusion: 该研究开发了一种完全去中心化的协同波束成形框架，通过局部化处理和低复杂度更新规则，解决了大型LEO星座中集中式设计的可扩展性问题，为实现可扩展的星座级协作提供了实用解决方案。

Abstract: Inter-satellite-link-enabled low-Earth-orbit (LEO) satellite constellations are evolving toward networked architectures that support constellation-level cooperation, enabling multiple satellites to jointly serve user terminals through cooperative beamforming. While such cooperation can substantially enhance link budgets and achievable rates, its practical realization is challenged by the scalability limitations of centralized beamforming designs and the stringent computational and signaling constraints of large LEO constellations. This paper develops a fully decentralized cooperative beamforming framework for networked LEO satellite downlinks. Using an ergodic-rate-based formulation, we first derive a centralized weighted minimum mean squared error (WMMSE) solution as a performance benchmark. Building on this formulation, we propose a topology-agnostic decentralized beamforming algorithm by localizing the benchmark and exchanging a set of globally coupled variables whose dimensions are independent of the antenna number and enforcing consensus over arbitrary connected inter-satellite networks. The resulting algorithm admits fully parallel execution across satellites. To further enhance scalability, we eliminate the consensus-related auxiliary variables in closed form and derive a low-complexity per-satellite update rule that is optimal to local iteration and admits a quasi-closed-form solution via scalar line search. Simulation results show that the proposed decentralized schemes closely approach centralized performance under practical inter-satellite topologies, while significantly reducing computational complexity and signaling overhead, enabling scalable cooperative beamforming for large LEO constellations.

</details>


### [19] [FAS-RIS for V2X: Unlocking Realistic Performance Analysis with Finite Elements](https://arxiv.org/abs/2512.18970)
*Tuo Wu,Xiazhi Lai,Maged Elkashlan,Naofal Al-Dhahir,Matthew C. Valenti,Fumiyuki Adachi*

Main category: eess.SP

TL;DR: 本文提出了一种针对有限元FAS-RIS系统的现实性能分析框架，使用Gamma分布近似推导出中断概率的闭式表达式，比传统基于中心极限定理的方法更准确，特别适用于小规模RIS的实际场景。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统(FAS)和可重构智能表面(RIS)的协同有望实现稳健的车联网(V2X)通信，但现有分析主要依赖中心极限定理(CLT)，这只适用于大量RIS元素的情况，无法代表受成本和城市基础设施限制的实际有限规模部署，导致理论预测与实际性能之间存在关键差距。

Method: 提出了一种新颖的分析框架，采用Gamma分布近似方法，推导出有限元FAS-RIS系统中断概率的易处理闭式表达式，为实际部署提供了更准确的性能表征。

Result: 数值结果验证了所提方法的有效性，表明该方法比传统的基于CLT的方法提供了显著更准确的性能表征，特别是在小规模RIS的实际场景中表现尤为突出。

Conclusion: 这项工作为可靠FAS-RIS辅助车联网的设计和部署提供了关键基础，填补了有限规模RIS系统理论分析与实际性能之间的差距。

Abstract: The synergy of fluid antenna systems (FAS) and reconfigurable intelligent surfaces (RIS) is poised to unlock robust Vehicle-to-Everything (V2X) communications. However, a critical gap persists between theoretical predictions and real-world performance. Existing analyses predominantly rely on the Central Limit Theorem (CLT), an assumption valid only for a large number of RIS elements, which fails to represent practical, finite-sized deployments constrained by cost and urban infrastructure. This paper bridges this gap by presenting a novel framework that unlocks a realistic performance analysis for FAS-RIS systems with finite elements. Leveraging a Gamma distribution approximation, we derive a new, tractable closed-form expression for the outage probability. Numerical results validate our approach, demonstrating that it offers a significantly more accurate performance characterization than conventional CLT-based methods, particularly in the practical regime of small-scale RIS. This work provides a crucial foundation for the design and deployment of reliable FAS-RIS-aided vehicular networks.

</details>


### [20] [An Fluid Antenna Array-Enabled DOA Estimation Method: End-Fire Effect Suppression](https://arxiv.org/abs/2512.18981)
*Jiaji Ren,Ye Tian,Baiyang Liu,Tuo Wu,Wei Liu,Kai-Kit Wong,Kin-Fai Tong,Kwai-Man Luk*

Main category: eess.SP

TL;DR: 提出基于流体天线的阵列架构，通过位置空间可重构性调制阵列导向矢量，结合MUSIC算法实现高精度DOA估计，有效解决端射效应问题


<details>
  <summary>Details</summary>
Motivation: DOA估计是未来智能泛在通信系统的关键感知技术，但固定天线阵列固有的端射效应问题尚未得到充分解决，需要新的解决方案

Method: 提出由流体天线组成的新型阵列架构，利用其位置空间可重构性来等效调制阵列导向矢量，并与经典MUSIC算法相结合

Result: 仿真结果表明，该方法即使在极具挑战性的端射区域也能提供出色的估计性能

Conclusion: 基于流体天线的可重构阵列架构为解决DOA估计中的端射效应问题提供了有效方案，实现了高精度估计性能

Abstract: Direction of Arrival (DOA) estimation serves as a critical sensing technology poised to play a vital role in future intelligent and ubiquitous communication systems. Despite the development of numerous mature super-resolution algorithms, the inherent end-fire effect problem in fixed antenna arrays remains inadequately addressed. This work proposed a novel array architecture composed of fluid antennas. By exploiting the spatial reconfigurability of their positions to equivalently modulate the array steering vector and integrating it with the classical MUSIC algorithm, this approach achieved high-precision DOA estimation. Simulation results demonstrated that the proposed method delivers outstanding estimation performance even in highly challenging end-fire regions.

</details>


### [21] [Reimagining Wireless Connectivity: The FAS-RIS Synergy for 6G Smart Cities](https://arxiv.org/abs/2512.18982)
*Tuo Wu,Kai-Kit Wong,Jie Tang,Junteng Yao,Baiyang Liu,Kin-Fai Tong,Chan-Byoung Chae,Matthew C. Valenti,Kwai-Man Luk*

Main category: eess.SP

TL;DR: 本文提出了一种创新的FAS-RIS集成架构，将流体天线系统的可重构性与智能反射面的环境控制能力相结合，为未来智慧城市网络提供新的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统智能反射面(RIS)存在双路径损耗和信号处理能力有限的问题，而流体天线系统(FAS)虽能提供新的空间自由度，但缺乏环境控制能力。将两者集成可以克服各自局限，为下一代无线网络提供更强大的可重构性和环境适应性。

Method: 提出FAS-RIS集成架构，包含五种关键应用：FAS使能的基站大规模波束成形、FAS装备的用户设备、流体RIS、FAS嵌入RIS作为主动中继、以及利用表面波重建视距通信的巨型FAS。采用双时间尺度控制机制协调网络级波束成形和设备级快速适应。

Result: 仿真结果表明FAS-RIS系统具有鲁棒性和有效性。该架构能够支持从同时无线信息和能量传输到集成感知与通信等多种应用，显著提升网络性能。

Conclusion: FAS-RIS集成架构代表了未来智能城市网络的变革性方向，通过结合FAS的适应性和RIS的环境控制能力，为下一代可重构无线系统开辟了新的设计空间和优化机会，但仍面临协同设计、信道建模和优化等挑战。

Abstract: Fluid antenna system (FAS) represents the concept of treating antenna as a reconfigurable physical-layer resource to broaden system design and network optimization and inspire next-generation reconfigurable antennas. FAS can unleash new degree of freedom (DoF) via antenna reconfigurations for novel spatial diversity. Reconfigurable intelligent surfaces (RISs) on the other hand can reshape wireless propagation environments but often face limitations from double path-loss and minimal signal processing capability when operating independently. This article envisions a transformative FAS-RIS integrated architecture for future smart city networks, uniting the adaptability of FAS with the environmental control of RIS. The proposed framework has five key applications: FAS-enabled base stations (BSs) for large-scale beamforming, FAS-equipped user devices with finest spatial diversity, and three novel RIS paradigms -- fluid RIS (FRIS) with reconfigurable elements, FAS-embedded RIS as active relays, and enormous FAS (E-FAS) exploiting surface waves on facades to re-establish line-of-sight (LoS) communication. A two-timescale control mechanism coordinates network-level beamforming with rapid, device-level adaptation. Applications spanning from simultaneous wireless information and power transfer (SWIPT) to integrated sensing and communications (ISAC), with challenges in co-design, channel modeling, and optimization, are discussed. This article concludes with simulation results demonstrating the robustness and effectiveness of the FAS-RIS system.

</details>


### [22] [PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation](https://arxiv.org/abs/2512.19010)
*Devi Yuliarti,Ravi Prakash,Hiu Ching Cheung,Amy Strong,Patrick J. Codd,Shan Lin*

Main category: eess.SP

TL;DR: PalpAid是一种用于机器人辅助手术的多模态气动触觉传感器，通过麦克风和压力传感器将接触力转换为内部压力差，帮助外科医生识别肿瘤和组织边界。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术虽然价值巨大，但导致外科医生只能依赖视觉信息，缺乏触觉反馈。现有传感器通常体积庞大、复杂且与手术流程不兼容，无法有效解决这一感官缺失问题。

Method: 开发了PalpAid多模态气动触觉传感器，配备麦克风和压力传感器。压力传感器作为事件检测器，麦克风捕获的听觉特征用于组织分界。展示了传感器的设计、制造、组装，并通过特性测试验证了其使用鲁棒性、充放气循环以及与机器人系统的集成能力。

Result: 传感器能够成功分类具有不同填充密度的3D打印硬物体和离体软组织，展示了其在组织识别方面的有效性。

Conclusion: PalpAid旨在智能地填补机器人辅助手术中的感官空白，通过提供触觉反馈来改善临床决策制定。

Abstract: The tactile properties of tissue, such as elasticity and stiffness, often play an important role in surgical oncology when identifying tumors and pathological tissue boundaries. Though extremely valuable, robot-assisted surgery comes at the cost of reduced sensory information to the surgeon; typically, only vision is available. Sensors proposed to overcome this sensory desert are often bulky, complex, and incompatible with the surgical workflow. We present PalpAid, a multimodal pneumatic tactile sensor equipped with a microphone and pressure sensor, converting contact force into an internal pressure differential. The pressure sensor acts as an event detector, while the auditory signature captured by the microphone assists in tissue delineation. We show the design, fabrication, and assembly of sensory units with characterization tests to show robustness to use, inflation-deflation cycles, and integration with a robotic system. Finally, we show the sensor's ability to classify 3D-printed hard objects with varying infills and soft ex vivo tissues. Overall, PalpAid aims to fill the sensory gap intelligently and allow improved clinical decision-making.

</details>


### [23] [The MIMO-ME-MS Channel: Analysis and Algorithm for Secure MIMO Integrated Sensing and Communications](https://arxiv.org/abs/2512.19013)
*Seongkyu Jung,Namyoon Lee,Jeonghun Park*

Main category: eess.SP

TL;DR: 本文研究了MIMO集成感知与通信(ISAC)中的安全预编码设计，提出了MIMO-ME-MS信道模型，开发了基于子空间分解和功率分配的两阶段迭代算法，在安全通信和感知性能之间取得了显著平衡。


<details>
  <summary>Details</summary>
Motivation: 在MIMO集成感知与通信(ISAC)系统中，需要同时服务合法接收器、防止窃听器窃听、并支持感知接收器进行传感。传统的窃听信道或ISAC预编码设计在这种三方设置中可能不是最优的，因此需要研究专门的安全MIMO ISAC预编码设计。

Method: 1) 引入MIMO-ME-MS信道模型；2) 使用感知互信息作为感知度量；3) 基于子空间分解进行高信噪比分析，识别"有用子空间"；4) 开发两阶段迭代算法：交替进行顺序基构造和通过凸差规划进行功率分配。

Result: 高信噪比分析揭示了最大可达加权自由度，并表明准最优预编码必须跨越"有用子空间"。数值结果显示，所提方法能够捕获分析预测的理想预编码结构，在MIMO-ME-MS信道中产生显著性能增益。

Conclusion: 本文为安全MIMO ISAC系统提供了理论分析和实用算法框架，证明了在传统方法基础上需要专门设计来优化安全通信和感知性能的联合权衡，所提方法在实际信道条件下表现优异。

Abstract: This paper studies precoder design for secure MIMO integrated sensing and communications (ISAC) by introducing the MIMO-ME-MS channel, where a multi-antenna transmitter serves a legitimate multi-antenna receiver in the presence of a multi-antenna eavesdropper while simultaneously enabling sensing via a multi-antenna sensing receiver. Using sensing mutual information as the sensing metric, we formulate a nonconvex weighted objective that jointly captures secure communication (via secrecy rate) and sensing performance. A high-SNR analysis based on subspace decomposition characterizes the maximum achievable weighted degrees of freedom and reveals that a quasi-optimal precoder must span a "useful subspace," highlighting why straightforward extensions of classical wiretap/ISAC precoders can be suboptimal in this tripartite setting. Motivated by these insights, we develop a practical two-stage iterative algorithm that alternates between sequential basis construction and power allocation via a difference-of-convex program. Numerical results show that the proposed approach captures the desirable precoding structure predicted by the analysis and yields substantial gains in the MIMO-ME-MS channel.

</details>


### [24] [AI-Driven Subcarrier-Level CQI Feedback](https://arxiv.org/abs/2512.19054)
*Chengyong Jiang,Jiajia Guo,Yuqing Hua,Chao-Kai Wen,Shi Jin*

Main category: eess.SP

TL;DR: 提出AI驱动的子载波级CQI反馈框架，包括CQInet压缩重建和SR-CQInet超分辨率推断，显著提升6G系统频谱效率


<details>
  <summary>Details</summary>
Motivation: 传统子带CQI方法粒度粗糙，无法捕捉精细频率选择性变化，导致资源分配次优；AI在CSI反馈方面已有进展，但CQI反馈仍待探索

Method: 1) CQInet：基于自编码器的方案，在用户设备压缩子载波级CQI，在基站重建；2) SR-CQInet：利用超分辨率从稀疏CSI-RS推断精细子载波CQI

Result: CQInet在等效反馈开销下比传统子带CQI提升7.6%有效数据率；SR-CQInet将CSI-RS开销降至CQInet的3.5%，同时保持可比吞吐量

Conclusion: AI驱动的子载波级CQI反馈可显著提升未来无线网络的频谱效率和可靠性，为6G系统提供有效解决方案

Abstract: The Channel Quality Indicator (CQI) is a fundamental component of channel state information (CSI) that enables adaptive modulation and coding by selecting the optimal modulation and coding scheme to meet a target block error rate. While AI-enabled CSI feedback has achieved significant advances, especially in precoding matrix index feedback, AI-based CQI feedback remains underexplored. Conventional subband-based CQI approaches, due to coarse granularity, often fail to capture fine frequency-selective variations and thus lead to suboptimal resource allocation. In this paper, we propose an AI-driven subcarrier-level CQI feedback framework tailored for 6G and NextG systems. First, we introduce CQInet, an autoencoder-based scheme that compresses per-subcarrier CQI at the user equipment and reconstructs it at the base station, significantly reducing feedback overhead without compromising CQI accuracy. Simulation results show that CQInet increases the effective data rate by 7.6% relative to traditional subband CQI under equivalent feedback overhead. Building on this, we develop SR-CQInet, which leverages super-resolution to infer fine-grained subcarrier CQI from sparsely reported CSI reference signals (CSI-RS). SR-CQInet reduces CSI-RS overhead to 3.5% of CQInet's requirements while maintaining comparable throughput. These results demonstrate that AI-driven subcarrier-level CQI feedback can substantially enhance spectral efficiency and reliability in future wireless networks.

</details>


### [25] [Wireless sEMG-IMU Wearable for Real-Time Squat Kinematics and Muscle Activation](https://arxiv.org/abs/2512.19089)
*Marie Jose Perez Peralta,Daniela Flores Casillas,Benjamin Wilson,Cristian Aviles Medina,Yira Itzae Rendon Hernandez,Vladimir Orrante Bracho*

Main category: eess.SP

TL;DR: 开发了一个无线可穿戴系统，结合表面肌电（sEMG）和惯性测量单元（IMU）来分析自由体重深蹲动作，实现了低成本、便携式的肌肉激活与运动学集成分析。


<details>
  <summary>Details</summary>
Motivation: 需要一种低成本、便携式的系统来集成分析肌肉激活（通过sEMG）和运动学数据（通过IMU），特别是在体育表现和康复等生物力学应用中，同时减少电气风险。

Method: 设计并实现了一个无线可穿戴系统，包括：1）双通道sEMG前端（差分仪表放大、5-500Hz带通滤波、60Hz陷波、高增益、整流积分输出）；2）两个MPU6050 IMU用于估计膝关节角度、角速度和角加速度；3）ESP32微控制器进行数字化和无线传输（ESP-NOW协议）；4）Python GUI实时显示数据并导出到Excel；5）电池供电以减少电气风险。

Result: 成功开发出一个原型系统，能够同时记录优势腿股外侧肌和半腱肌的肌电信号，并估计膝关节的运动学参数。系统实现了实时数据显示和会话数据导出，证明了低成本、便携式EMG-IMU仪器用于肌肉激活与深蹲运动学集成分析的可行性。

Conclusion: 该原型系统展示了低成本、便携式EMG-IMU仪器在肌肉激活与深蹲运动学集成分析方面的可行性，为未来体育表现和康复领域的生物力学应用提供了一个平台。

Abstract: This work presents the design and implementation of a wireless, wearable system that combines surface electromyography (sEMG) and inertial measurement units (IMUs) to analyze a single lower-limb functional task: the free bodyweight squat in a healthy adult. The system records bipolar EMG from one agonist and one antagonist muscle of the dominant leg (vastus lateralis and semitendinosus) while simultaneously estimating knee joint angle, angular velocity, and angular acceleration using two MPU6050 IMUs. A custom dual-channel EMG front end with differential instrumentation preamplification, analog filtering (5-500 Hz band-pass and 60 Hz notch), high final gain, and rectified-integrated output was implemented on a compact 10 cm x 12 cm PCB. Data are digitized by an ESP32 microcontroller and transmitted wirelessly via ESP-NOW to a second ESP32 connected to a PC. A Python-based graphical user interface (GUI) displays EMG and kinematic signals in real time, manages subject metadata, and exports a summary of each session to Excel. The complete system is battery-powered to reduce electrical risk during human use. The resulting prototype demonstrates the feasibility of low-cost, portable EMG-IMU instrumentation for integrated analysis of muscle activation and squat kinematics and provides a platform for future biomechanical applications in sports performance and rehabilitation.

</details>


### [26] [Intelligent Sky Mirrors: SAC-Driven MF-RIS Optimization for Secure NOMA in Low-Altitude Economy](https://arxiv.org/abs/2512.19109)
*Sai Zhao,Fanjin Kong,Dong Tang,Tuo Wu,Shunxing Yang,Kai-Kit Wong,Kin-Fai Tong,Kwai-Man Luk*

Main category: eess.SP

TL;DR: 本文研究低空经济中利用智能天空镜（UAV-MF-RIS）辅助NOMA系统的安全能效最大化问题，通过双层优化方案显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 低空经济已成为智慧城市和经济增长的关键驱动力，但面临频谱效率和通信安全挑战。需要解决低空通信中的安全能效问题。

Method: 采用智能天空镜（UAV-MF-RIS）辅助NOMA系统，这些空中镜智能放大合法信号同时生成对窃听者的干扰。提出双层优化方案：第一层使用SAC算法优化无人机轨迹和基站功率分配，第二层基于信道对齐优化RIS相位。

Result: 仿真结果表明，所提出的方案在安全能效方面显著优于基准方法，有效解决了低空通信中的安全和效率问题。

Conclusion: 智能天空镜辅助NOMA系统结合双层优化方案能够有效提升低空经济中的安全能效，为智慧城市通信提供可靠解决方案。

Abstract: Low-altitude economy (LAE) has become a key driving force for smart cities and economic growth. To address spectral efficiency and communication security challenges in LAE, this paper investigates secure energy efficiency (SEE) maximization using intelligent sky mirrors, UAV-mounted multifunctional reconfigurable intelligent surfaces (MF-RIS) assisting nonorthogonal multiple access (NOMA) systems. These aerial mirrors intelligently amplify legitimate signals while simultaneously generating jamming against eavesdroppers. We formulate a joint optimization problem encompassing UAV trajectory, base station power allocation, RIS phase shifts, amplification factors, and scheduling matrices. Given the fractional SEE objective and dynamic UAV scenarios, we propose a two-layer optimization scheme: SAC-driven first layer for trajectory and power management, and channel alignment-based second layer for phase optimization. Simulations demonstrate that our proposed scheme significantly outperforms benchmark approaches.

</details>


### [27] [Specific Multi-emitter Identification: Theoretical Limits and Low-complexity Design](https://arxiv.org/abs/2512.19127)
*Yuhao Chen,Boxiang He,Junshan Luo,Shilian Wang,Lei Yao,Jing Lei*

Main category: eess.SP

TL;DR: 提出特定多发射器识别（SMEI）框架，通过多标签学习直接从重叠信号中解码发射器状态，解决传统单发射器识别在多发射器场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统特定发射器识别（SEI）技术主要针对单发射器场景，而在分布式无线网络中，多个发射器同时传输导致信号重叠，传统方法无法有效处理这一问题。

Method: 提出SMEI框架，将识别问题转化为多标签学习问题，将输出维度从指数级降低到线性级；进一步提出改进的I-SMEI，引入多头注意力机制捕捉相关信号组合的特征。

Result: SMEI实现了高识别精度且具有线性计算复杂度；I-SMEI在各种重叠场景下相比SMEI和其他先进方法显著提高了识别准确率。

Conclusion: SMEI框架有效解决了多发射器重叠信号的识别问题，I-SMEI通过注意力机制进一步提升了性能，为分布式无线网络中的发射器识别提供了有效解决方案。

Abstract: Specific emitter identification (SEI) distinguishes emitters by utilizing hardware-induced signal imperfections. However, conventional SEI techniques are primarily designed for single-emitter scenarios. This poses a fundamental limitation in distributed wireless networks, where simultaneous transmissions from multiple emitters result in overlapping signals that conventional single-emitter identification methods cannot effectively handle. To overcome this limitation, we present a specific multi-emitter identification (SMEI) framework via multi-label learning, treating identification as a problem of directly decoding emitter states from overlapping signals. Theoretically, we establish performance bounds using Fano's inequality. Methodologically, the multi-label formulation reduces output dimensionality from exponential to linear scale, thereby substantially decreasing computational complexity. Additionally, we propose an improved SMEI (I-SMEI), which incorporates multi-head attention to effectively capture features in correlated signal combinations. Experimental results demonstrate that SMEI achieves high identification accuracy with a linear computational complexity. Furthermore, the proposed I-SMEI scheme significantly improves identification accuracy across various overlapping scenarios compared to the proposed SMEI and other advanced methods.

</details>


### [28] [Energy Optimization for Time-of-Arrival Based Tracking](https://arxiv.org/abs/2512.19166)
*Luca Reggiani,Arnaldo Spalvieri*

Main category: eess.SP

TL;DR: 论文研究移动目标跟踪中的能量分配问题，通过带宽受限信号的到达时间进行定位，在保证跟踪性能约束下最小化总发射能量。


<details>
  <summary>Details</summary>
Motivation: 在移动目标跟踪系统中，多个锚点通过发射信号来估计目标位置，但能量消耗是需要优化的重要资源。现有方法在能量分配与跟踪性能之间的权衡优化方面存在不足，需要更高效的优化方法。

Method: 使用已知符号序列的信号，允许不同锚点的信号幅度和持续时间不同。主要创新是推导了后验克拉美罗界（PCRB）关于总能量变化的一阶变化闭式表达式，并提出了两种数值算法实现固定幅度、可变持续时间信号的约束优化。

Result: 推导出了PCRB随能量变化的闭式表达式，使得在跟踪的每一步都能高效计算能量分配对跟踪性能的影响。提出的数值算法能够在时分复用方案中实现能量最小化同时满足跟踪性能约束。

Conclusion: 该研究为移动目标跟踪系统的能量分配提供了理论框架和实用算法，通过闭式表达式显著提高了优化效率，在保证跟踪精度的同时最小化能量消耗。

Abstract: The paper analyzes energy allocation in a scenario where the position of a moving target is tracked by exploiting the Time-of-Arrivals of bandwidth-constrained signals received by or transmitted from a fixed number of anchors located at known positions. The signal of each anchor is generated by transmitting a sequence of known symbols, allowing for amplitude and duration (number of symbols) to be different from anchor to anchor. The problem is the minimization of the sum of the energies of the transmitted signals imposing a constraint on the performance of the tracking procedure. Specifically, the constraint is the Posterior Cramer-Rao Bound, below the mean square error achieved by any unbiased estimator. The main improvement over the previous literature is the derivation of a formula that, at each step of the tracking, allows to calculate in closed form the first-order variation of the Posterior Cramer-Rao Bound as a function of the variation of the total energy. To concretely show the application of our approach, we present also two numerical algorithms that implement the constrained optimization in the case of signals of fixed amplitude and variable duration transmitted from the anchors in a time division multiplexing scheme.

</details>


### [29] [How is remifentanil dosed without dedicated indicator?](https://arxiv.org/abs/2512.19220)
*Bob Aubouin-Pairault,Mazen Alamir,Benjamin Meyer,Rémi Wolf,Kaouther Moussa*

Main category: eess.SP

TL;DR: 研究基于回顾性临床数据，分析麻醉医师调整瑞芬太尼剂量的决策模式，发现动脉血压特征是唯一与剂量调整一致相关的指标，为闭环控制算法开发奠定基础。


<details>
  <summary>Details</summary>
Motivation: 瑞芬太尼作为广泛使用的镇痛药，在术中给药存在挑战，因为缺乏普遍接受的镇痛指标。研究旨在探索麻醉医师如何根据患者状态变化调整瑞芬太尼目标浓度。

Method: 使用数据驱动方法，分析来自韩国VitalDB和法国PREDIMED两个数据库的回顾性临床数据，研究患者状态变化与麻醉医师调整瑞芬太尼目标浓度的关联性。

Result: 结果显示，只有动脉血压衍生的特征与瑞芬太尼目标浓度变化一致相关。瑞芬太尼剂量增加与短期（1-2分钟）内血压升高或上升相关，而剂量减少与长期（5-7分钟）内血压低、稳定或下降相关。

Conclusion: 通过捕捉麻醉医师的给药策略，为未来闭环控制算法开发奠定基础。提出的特征生成和稀疏拟合方法可应用于其他需要解释人类决策的领域。

Abstract: This study investigates the paradigm of intraoperative analgesic dosage using a data-driven approach based on retrospective clinical data. Remifentanil, an analgesic widely used during anesthesia, presents a dosing challenge due to the absence of an universally accepted indicator of analgesia. To examine how changes in patient state correlate with adjustments in remifentanil target concentration triggered by the practitioner, we analyzed data from two sources: VitalDB (Seoul, Korea) and PREDIMED (Grenoble, France). Results show that only features derived from arterial pressure are consistently associated with changes in remifentanil targets. This finding is robust across both datasets despite variations in specific thresholds. In particular, increases in remifentanil targets are associated with high or rising arterial pressure over short periods (1--2 minutes), whereas decreases are linked to low, stable, or declining arterial pressure over longer periods (5--7 minutes). By capturing anesthesiologists' dosing strategies we provide a foundation for the future development of closed-loop control algorithms. Beyond the specific example of remifentanil's change prediction, the proposed feature generation and associated sparse fitting approach can be applied to other domain where human decision can be viewed as sensors interpretation.

</details>


### [30] [Anti-Malicious ISAC: How to Jointly Monitor and Disrupt Your Foes?](https://arxiv.org/abs/2512.19263)
*Zonghan Wang,Zahra Mobini,Hien Quoc Ngo,Hyundong Shin,Michail Matthaiou*

Main category: eess.SP

TL;DR: 提出一种无线主动监控范式，通过认知干扰增强监控成功率，同时保护合法目标免受恶意ISAC系统的检测或利用。


<details>
  <summary>Details</summary>
Motivation: 随着恶意ISAC系统的出现，授权方需要合法监控可疑通信链路，保护合法目标免受恶意对手的检测或利用。

Method: 1) 推导用户、感知接入点和主动监控器的SINR闭式表达式；2) 提出优化技术，通过优化干扰功率分配最小化合法目标的成功检测概率；3) 提出自适应功率分配方案，在满足感知SINR阈值和监控要求的同时最小化监控器总发射功率。

Result: 数值结果表明，所提算法显著损害了恶意ISAC的感知和通信性能。

Conclusion: 提出的无线主动监控范式能有效增强监控成功率，保护合法目标，同时通过优化功率分配降低监控器被发现的风险。

Abstract: Integrated sensing and communication (ISAC) systems are key enablers of future networks but raise significant security concerns. In this realm, the emergence of malicious ISAC systems has amplified the need for authorized parties to legitimately monitor suspicious communication links and protect legitimate targets from potential detection or exploitation by malicious foes. In this paper, we propose a new wireless proactive monitoring paradigm, where a legitimate monitor intercepts a suspicious communication link while performing cognitive jamming to enhance the monitoring success probability (MSP) and simultaneously safeguard the target. To this end, we derive closed-form expressions of the signal-to-interference-plus-noise-ratio (SINR) at the user (UE), sensing access points (S-APs), and an approximating expression of the SINR at the proactive monitor. Moreover, we propose an optimization technique under which the legitimate monitor minimizes the success detection probability (SDP) of the legitimate target, by optimizing the jamming power allocation over both communication and sensing channels subject to total power constraints and monitoring performance requirement. To enhance the monitor's longevity and reduce the risk of detection by malicious ISAC systems, we further propose an adaptive power allocation scheme aimed at minimizing the total transmit power at the monitor while meeting a pre-selected sensing SINR threshold and ensuring successful monitoring. Our numerical results show that the proposed algorithm significantly compromises the sensing and communication performance of malicious ISAC.

</details>


### [31] [Real-Time Streamable Generative Speech Restoration with Flow Matching](https://arxiv.org/abs/2512.19442)
*Simon Welker,Bunlong Lay,Maris Hillemann,Tal Peer,Timo Gerkmann*

Main category: eess.SP

TL;DR: Stream.FM：一种帧因果流生成模型，具有32ms算法延迟和48ms总延迟，实现实时通信中的生成式语音处理，支持多种语音处理任务。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的生成模型在语音处理领域表现出高自然度，但由于计算量大、需要多次调用大型DNN，在实时通信应用中滞后。需要开发低延迟的生成式语音处理模型。

Method: 提出Stream.FM：帧因果流生成模型，采用缓冲流式推理方案和优化的DNN架构；使用学习型少步数值求解器提升固定计算预算下的输出质量；探索模型权重压缩以找到计算/质量权衡的有利点；为语音增强任务提供24ms总延迟的变体。

Result: Stream.FM在多种语音处理任务中实现流式处理：语音增强、去混响、编解码器后滤波、带宽扩展、STFT相位恢复和Mel声码器。通过综合评估和MUSHRA听力测试验证，在生成式流式语音恢复方面达到最先进水平，与非流式变体相比质量下降合理，在生成式流式语音增强方面优于之前的Diffusion Buffer工作且延迟更低。

Conclusion: Stream.FM展示了高质量流式生成式语音处理可以在当前消费级GPU上实现，为实时通信中的生成式语音处理铺平道路，超越了理论延迟限制，实现了实际可用的低延迟解决方案。

Abstract: Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.
  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.
  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency.

</details>


### [32] [Towards Reliable Connectivity: Measurement-Driven Assessment of Starlink and OneWeb Non-Terrestrial and 5G Terrestrial Networks](https://arxiv.org/abs/2512.19639)
*Alejandro Ramírez-Arroyo,O. S. Peñaherrera-Pulla,Preben Mogensen*

Main category: eess.SP

TL;DR: 该研究评估了星链、OneWeb等非地面卫星网络与地面蜂窝网络的性能指标，探索了多连接技术在不同环境下的优势，发现卫星与地面网络集成能显著提升覆盖、性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前通信系统高度依赖地面网络，容易在自然灾害等情况下出现基础设施故障。需要评估非地面卫星网络（如星链、OneWeb）作为地面网络补充的性能，以增强通信覆盖和可靠性。

Method: 联合评估两个非地面卫星网络（星链和OneWeb）和两个地面蜂窝网络的关键性能指标（KPIs），在三种不同环境（城市、郊区、森林）中进行测试，并探索多连接技术（同时使用两种技术）的优势。

Result: 在城市环境中，星链和OneWeb的断线概率从约12-21%降至2%（当两种解决方案同时使用时）。综合分析表明，卫星与地面网络集成能增强覆盖、改善性能并提高可靠性。

Conclusion: 非地面卫星网络与地面蜂窝网络的集成在覆盖扩展、性能提升和可靠性增强方面具有显著优势，特别是在城市、郊区和森林等不同环境中，多连接技术能有效减少通信中断概率。

Abstract: The emergence of commercial satellite communications networks, such as Starlink and OneWeb, has significantly transformed the communications landscape over the last years. As a complement to terrestrial cellular networks, non-terrestrial systems enable coverage extension and reliability enhancement beyond the limits of conventional infrastructure. Currently, the high reliance on terrestrial networks exposes communications to vulnerabilities in the event of terrestrial infrastructure failures, e.g., due to natural disasters. Therefore, this work proposes the joint evaluation of Key Performance Indicators (KPIs) for two non-terrestrial satellite networks (Starlink and OneWeb) and two terrestrial cellular networks to assess the current performance of these technologies across three different environments: (i) urban, (ii) suburban, and (iii) forest scenarios. Additionally, multi-connectivity techniques are explored to determine the benefits in connectivity when two technologies are used simultaneously. For instance, the outage probability of Starlink and OneWeb in urban areas is reduced from approximately 12-21\% to 2\% when both solutions are employed together. Finally, the joint analysis of KPIs in both terrestrial and non-terrestrial networks demonstrates that their integration enhances coverage, improves performance, and increases reliability, highlighting the benefits of combining satellite and terrestrial systems in the analyzed environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems](https://arxiv.org/abs/2512.18317)
*Vincent Bezold,Patrick Wagner,Jakob Hofmann,Marco Huber,Alexander Sauer*

Main category: cs.LG

TL;DR: 提出一种可信赖的强化学习方法用于工业压缩空气系统控制，通过多级可解释性管道确保安全高效运行，相比现有控制器节省约4%能耗


<details>
  <summary>Details</summary>
Motivation: 工业压缩空气系统能耗巨大，现有控制器效率有限，需要开发安全、高效且可解释的强化学习控制方法，以支持在工业能源系统中的可信赖部署

Method: 开发可信赖强化学习框架，结合输入扰动测试、基于梯度的敏感性分析和SHAP特征归因的多级可解释性管道，在多种压缩机配置下进行实证评估

Result: 学习到的策略具有物理合理性，能预测未来需求并始终尊重系统边界，相比工业控制器减少不必要过压，节省约4%能耗，无需显式物理模型

Conclusion: 系统压力和预测信息主导策略决策，压缩机级输入起次要作用，效率提升、预测行为和透明验证的结合支持强化学习在工业能源系统中的可信赖部署

Abstract: This paper presents a trustworthy reinforcement learning approach for the control of industrial compressed air systems. We develop a framework that enables safe and energy-efficient operation under realistic boundary conditions and introduce a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP (SHapley Additive exPlanations) feature attribution. An empirical evaluation across multiple compressor configurations shows that the learned policy is physically plausible, anticipates future demand, and consistently respects system boundaries. Compared to the installed industrial controller, the proposed approach reduces unnecessary overpressure and achieves energy savings of approximately 4\,\% without relying on explicit physics models. The results further indicate that system pressure and forecast information dominate policy decisions, while compressor-level inputs play a secondary role. Overall, the combination of efficiency gains, predictive behavior, and transparent validation supports the trustworthy deployment of reinforcement learning in industrial energy systems.

</details>


### [34] [Why Most Optimism Bandit Algorithms Have the Same Regret Analysis: A Simple Unifying Theorem](https://arxiv.org/abs/2512.18409)
*Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的分析框架，用于证明乐观型随机bandit算法的对数遗憾界，通过识别最小核心条件简化了经典算法的证明。


<details>
  <summary>Details</summary>
Motivation: 尽管UCB、UCB-V、线性UCB和有限臂GP-UCB等乐观型随机bandit算法都实现了对数遗憾，但它们的证明虽然表面不同，本质上遵循相同结构。作者希望提取这些分析中的最小要素，为经典算法提供统一、简洁的证明框架。

Method: 作者分离出分析中的最小要素：单个高概率浓度条件（对估计量的集中性要求），然后通过两个简短的确定性引理（半径坍缩和乐观强制偏差）推导出对数遗憾。这个框架统一了经典算法的证明，并能自然扩展到许多现代bandit变体。

Result: 该框架为UCB、UCB-V、线性UCB和有限臂GP-UCB等经典乐观型bandit算法提供了统一、接近最小化的对数遗憾证明，简化了分析过程，并展示了向当代bandit变体的自然扩展能力。

Conclusion: 乐观型随机bandit算法的对数遗憾分析可以基于一个简单的高概率浓度条件和两个确定性引理来统一完成，这为经典算法提供了简洁的证明框架，并能扩展到更广泛的bandit问题中。

Abstract: Several optimism-based stochastic bandit algorithms -- including UCB, UCB-V, linear UCB, and finite-arm GP-UCB -- achieve logarithmic regret using proofs that, despite superficial differences, follow essentially the same structure. This note isolates the minimal ingredients behind these analyses: a single high-probability concentration condition on the estimators, after which logarithmic regret follows from two short deterministic lemmas describing radius collapse and optimism-forced deviations. The framework yields unified, near-minimal proofs for these classical algorithms and extends naturally to many contemporary bandit variants.

</details>


### [35] [Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment](https://arxiv.org/abs/2512.18566)
*Ruiqi Chen,Giacomo Vedovati,Todd Braver,ShiNung Ching*

Main category: cs.LG

TL;DR: DFORM框架通过非线性坐标变换对齐两个动力系统的状态空间，解决比较学习动力学的两大挑战：坐标系统不匹配和高维非线性模型中低维动力学模式的识别困难。


<details>
  <summary>Details</summary>
Motivation: 在理论神经科学中，循环神经网络等动力学系统模型用于假设生成和数据分析，但评估学习到的动力学面临两大挑战：1）不同模型坐标系统不匹配导致难以比较；2）在高维非线性模型中识别机制上重要的低维模式（如极限集）非常困难。

Method: DFORM学习两个动力系统状态空间之间的非线性坐标变换，以最大化一对一的方式对齐它们的轨迹。该方法能够评估两个模型是否具有拓扑等价性，并作为副产品定位高维系统中嵌入的低维流形上的动力学模式。

Result: DFORM能够识别线性和非线性坐标变换，验证了其在规范拓扑等价系统、RNN和非线性流相关系统中的能力。该方法还能量化拓扑不同系统之间的相似性，定位高维模型中的重要动力学模式（如不变流形和鞍极限集），并在基于人类fMRI数据训练的RNN模型中成功识别极限环。

Conclusion: DFORM提供了一个全面的框架，解决了比较学习动力学和识别高维模型中低维动力学模式的关键挑战，为理论神经科学中的模型分析和假设检验提供了有力工具。

Abstract: Dynamical systems models such as recurrent neural networks (RNNs) are increasingly popular in theoretical neuroscience for hypothesis-generation and data analysis. Evaluating the dynamics in such models is key to understanding their learned generative mechanisms. However, such evaluation is impeded by two major challenges: First, comparison of learned dynamics across models is difficult because there is no enforced equivalence of their coordinate systems. Second, identification of mechanistically important low-dimensional motifs (e.g., limit sets) is intractable in high-dimensional nonlinear models such as RNNs. Here, we propose a comprehensive framework to address these two issues, termed Diffeomorphic vector field alignment FOR learned Models (DFORM). DFORM learns a nonlinear coordinate transformation between the state spaces of two dynamical systems, which aligns their trajectories in a maximally one-to-one manner. In so doing, DFORM enables an assessment of whether two models exhibit topological equivalence, i.e., similar mechanisms despite differences in coordinate systems. A byproduct of this method is a means to locate dynamical motifs on low-dimensional manifolds embedded within higher-dimensional systems. We verified DFORM's ability to identify linear and nonlinear coordinate transformations using canonical topologically equivalent systems, RNNs, and systems related by nonlinear flows. DFORM was also shown to provide a quantification of similarity between topologically distinct systems. We then demonstrated that DFORM can locate important dynamical motifs including invariant manifolds and saddle limit sets within high-dimensional models. Finally, using a set of RNN models trained on human functional MRI (fMRI) recordings, we illustrated that DFORM can identify limit cycles from high-dimensional data-driven models, which agreed well with prior numerical analysis.

</details>


### [36] [OPBO: Order-Preserving Bayesian Optimization](https://arxiv.org/abs/2512.18980)
*Wei Peng,Jianchen Hu,Kang Liu,Qiaozhu Zhai*

Main category: cs.LG

TL;DR: 提出OPBO方法，用保持顺序而非数值的神经网络替代高斯过程，解决高维贝叶斯优化中GP失效的问题


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化使用高斯过程作为代理模型，但在高维空间（如超过500维）中会失效，因为GP依赖精确数值拟合导致计算复杂度过高

Method: 提出顺序保持贝叶斯优化（OPBO）方法：1）使用保持目标函数顺序而非数值的OP神经网络作为代理模型；2）从顺序集中选择足够好的解而非最优解来降低计算成本

Result: 实验结果表明，对于高维（超过500维）黑盒优化问题，OPBO显著优于基于回归神经网络和高斯过程的传统BO方法

Conclusion: OPBO通过改变代理模型的学习目标（从数值拟合到顺序保持），有效解决了高维贝叶斯优化的计算复杂度和性能问题

Abstract: Bayesian optimization is an effective method for solving expensive black-box optimization problems. Most existing methods use Gaussian processes (GP) as the surrogate model for approximating the black-box objective function, it is well-known that it can fail in high-dimensional space (e.g., dimension over 500). We argue that the reliance of GP on precise numerical fitting is fundamentally ill-suited in high-dimensional space, where it leads to prohibitive computational complexity. In order to address this, we propose a simple order-preserving Bayesian optimization (OPBO) method, where the surrogate model preserves the order, instead of the value, of the black-box objective function. Then we can use a simple but effective OP neural network (NN) to replace GP as the surrogate model. Moreover, instead of searching for the best solution from the acquisition model, we select good-enough solutions in the ordinal set to reduce computational cost. The experimental results show that for high-dimensional (over 500) black-box optimization problems, the proposed OPBO significantly outperforms traditional BO methods based on regression NN and GP. The source code is available at https://github.com/pengwei222/OPBO.

</details>


### [37] [Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States](https://arxiv.org/abs/2512.17934)
*Soheil Hashtarkhani,Brianna M. White,Benyamin Hoseini,David L. Schwartz,Arash Shaban-Nejad*

Main category: cs.LG

TL;DR: 使用随机森林、梯度提升回归和线性回归预测美国县级肺癌死亡率，随机森林表现最佳，吸烟率是最重要预测因子，空间分析显示中东部地区存在显著高死亡率集群。


<details>
  <summary>Details</summary>
Motivation: 肺癌是美国癌症相关死亡的主要原因，准确预测肺癌死亡率对于指导针对性干预和解决健康差异至关重要。传统回归模型虽然常用，但可解释的机器学习模型可能提供更高的预测准确性和对影响因素更深入的理解。

Method: 研究应用了三种模型：随机森林(RF)、梯度提升回归(GBR)和线性回归(LR)来预测美国县级肺癌死亡率。使用R平方和均方根误差(RMSE)评估模型性能。使用SHAP值确定变量重要性及其方向性影响。通过Getis-Ord (Gi*)热点分析分析地理差异。

Result: 随机森林模型表现最佳，R2值为41.9%，RMSE为12.8。SHAP分析确定吸烟率是最重要的预测因子，其次是房屋中位数价值和西班牙裔人口百分比。空间分析显示美国中东部县存在显著的肺癌死亡率升高集群。

Conclusion: 随机森林模型在预测肺癌死亡率方面表现出优越的预测性能，强调了吸烟流行率、房屋价值和西班牙裔人口百分比的关键作用。这些发现为设计针对性干预措施、促进筛查和解决美国肺癌影响最严重地区的健康差异提供了有价值的可操作见解。

Abstract: Lung cancer (LC) is a leading cause of cancer-related mortality in the United States. Accurate prediction of LC mortality rates is crucial for guiding targeted interventions and addressing health disparities. Although traditional regression-based models have been commonly used, explainable machine learning models may offer enhanced predictive accuracy and deeper insights into the factors influencing LC mortality. This study applied three models: random forest (RF), gradient boosting regression (GBR), and linear regression (LR) to predict county-level LC mortality rates across the United States. Model performance was evaluated using R-squared and root mean squared error (RMSE). Shapley Additive Explanations (SHAP) values were used to determine variable importance and their directional impact. Geographic disparities in LC mortality were analyzed through Getis-Ord (Gi*) hotspot analysis. The RF model outperformed both GBR and LR, achieving an R2 value of 41.9% and an RMSE of 12.8. SHAP analysis identified smoking rate as the most important predictor, followed by median home value and the percentage of the Hispanic ethnic population. Spatial analysis revealed significant clusters of elevated LC mortality in the mid-eastern counties of the United States. The RF model demonstrated superior predictive performance for LC mortality rates, emphasizing the critical roles of smoking prevalence, housing values, and the percentage of Hispanic ethnic population. These findings offer valuable actionable insights for designing targeted interventions, promoting screening, and addressing health disparities in regions most affected by LC in the United States.

</details>


### [38] [What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD](https://arxiv.org/abs/2512.17945)
*Petr Koklev*

Main category: cs.LG

TL;DR: 本文量化了信用风险模型中单调性约束的性能成本（单调性代价），发现在大数据集上成本几乎为零，在小数据集上约为2-3%。


<details>
  <summary>Details</summary>
Motivation: 金融机构在部署信用风险机器学习模型时面临预测准确性与可解释性的权衡。单调性约束能确保模型行为符合领域知识，但其性能成本（单调性代价）尚未得到充分量化。

Method: 使用五个公共数据集和三个库，对单调约束与无约束梯度提升模型进行基准测试。定义单调性代价（PoM）为从无约束模型转向约束模型时标准性能指标的相对变化，通过配对比较和bootstrap不确定性进行估计。

Result: 实验中，AUC的单调性代价范围从几乎为零到约2.9%。在大数据集上约束几乎无成本（通常低于0.2%，常与零无区别），而在约束覆盖广泛的小数据集上成本最高（约2-3%）。

Conclusion: 适当指定的单调性约束通常能以较小的准确性损失提供可解释性，特别是在大规模信用投资组合中。

Abstract: Financial institutions face a trade-off between predictive accuracy and interpretability when deploying machine learning models for credit risk. Monotonicity constraints align model behavior with domain knowledge, but their performance cost - the price of monotonicity - is not well quantified. This paper benchmarks monotone-constrained versus unconstrained gradient boosting models for credit probability of default across five public datasets and three libraries. We define the Price of Monotonicity (PoM) as the relative change in standard performance metrics when moving from unconstrained to constrained models, estimated via paired comparisons with bootstrap uncertainty. In our experiments, PoM in AUC ranges from essentially zero to about 2.9 percent: constraints are almost costless on large datasets (typically less than 0.2 percent, often indistinguishable from zero) and most costly on smaller datasets with extensive constraint coverage (around 2-3 percent). Thus, appropriately specified monotonicity constraints can often deliver interpretability with small accuracy losses, particularly in large-scale credit portfolios.

</details>


### [39] [Convolutional-neural-operator-based transfer learning for solving PDEs](https://arxiv.org/abs/2512.17969)
*Peng Fan,Guofei Pang*

Main category: cs.LG

TL;DR: 该论文将卷积神经算子扩展到少样本学习场景，通过预训练和参数调整策略，发现神经元线性变换策略在求解多种PDE时具有最高的代理精度。


<details>
  <summary>Details</summary>
Motivation: 卷积神经算子虽然在PDE求解算子学习中表现出色，但尚未在少样本学习场景中得到验证。研究者希望探索如何将这种结构保持的连续-离散等价模型应用于数据稀缺的情况。

Method: 首先使用源数据集预训练卷积神经算子，然后仅使用少量目标数据集调整训练好的神经算子参数。研究了三种参数调整策略：微调、低秩适应和神经元线性变换。

Result: 在求解Kuramoto-Sivashinsky方程、Brusselator扩散反应系统和Navier-Stokes方程等PDE时，神经元线性变换策略获得了最高的代理精度。

Conclusion: 卷积神经算子可以成功扩展到少样本学习场景，其中神经元线性变换是最有效的参数调整策略，为数据稀缺情况下的PDE求解算子学习提供了有效方法。

Abstract: Convolutional neural operator is a CNN-based architecture recently proposed to enforce structure-preserving continuous-discrete equivalence and enable the genuine, alias-free learning of solution operators of PDEs. This neural operator was demonstrated to outperform for certain cases some baseline models such as DeepONet, Fourier neural operator, and Galerkin transformer in terms of surrogate accuracy. The convolutional neural operator, however, seems not to be validated for few-shot learning. We extend the model to few-shot learning scenarios by first pre-training a convolutional neural operator using a source dataset and then adjusting the parameters of the trained neural operator using only a small target dataset. We investigate three strategies for adjusting the parameters of a trained neural operator, including fine-tuning, low-rank adaption, and neuron linear transformation, and find that the neuron linear transformation strategy enjoys the highest surrogate accuracy in solving PDEs such as Kuramoto-Sivashinsky equation, Brusselator diffusion-reaction system, and Navier-Stokes equations.

</details>


### [40] [CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs](https://arxiv.org/abs/2512.17970)
*Gunho Park,Jeongin Bae,Byeongwook Kim,Baeseong park,Jiwon Ryu,Hoseung Kim,Se Jung Kwon,Dongsoo Lee*

Main category: cs.LG

TL;DR: CodeGEMM：一种面向码本量化的GEMM内核，通过预计算质心与激活的内积来避免反量化开销，在2比特量化下相比现有方法获得显著加速


<details>
  <summary>Details</summary>
Motivation: 现有码本量化方法在极低位宽（如2比特）下表现良好，但其内核依赖反量化操作，需要重复获取质心并重构权重，导致显著的延迟和缓存压力

Method: 提出CodeGEMM内核，用预计算的质心与激活内积（存储在轻量级Psumbook中）替代反量化。推理时，码索引直接收集这些部分和，消除逐元素查找并减少片上占用

Result: 在Llama-3模型上，2比特配置下CodeGEMM相比最先进的码本量化方法获得1.83倍（8B）和8.93倍（70B）加速，同时保持可比精度，并提升计算效率和内存子系统利用率

Conclusion: CodeGEMM通过码本中心的GEMM内核设计，系统性地探索延迟-内存-精度权衡，为极低位宽量化提供了高效的推理解决方案

Abstract: Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency-memory-accuracy trade-offs under a unified implementation. On Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization.

</details>


### [41] [Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models](https://arxiv.org/abs/2512.17983)
*Irina Seregina,Philippe Lalanda,German Vega*

Main category: cs.LG

TL;DR: 本文研究使用LoRA和QLoRA等参数高效微调技术，替代完整的模型微调，用于人类活动识别任务，在保持性能的同时显著减少可训练参数、内存使用和训练时间。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别是普适计算的基础任务。尽管自监督学习和Transformer架构显著提升了HAR性能，但在目标设备计算资源有限的情况下，将大型预训练模型适应到新领域仍然具有实际挑战。

Method: 提出基于掩码自编码器骨干的适应框架，评估LoRA和量化LoRA在五个公开HAR数据集上的性能，采用Leave-One-Dataset-Out验证协议。

Result: 实验表明LoRA和QLoRA都能匹配完整微调的识别性能，同时显著减少可训练参数、内存使用和训练时间。LoRA在有限监督下保持稳健性能，适配器秩提供了准确性与效率的可控权衡。QLoRA通过量化进一步减少冻结权重的内存占用，对分类质量影响最小。

Conclusion: 参数高效微调技术（特别是LoRA和QLoRA）为HAR任务提供了可扩展的解决方案，能够在资源受限的设备上实现高性能的模型适应，平衡准确性和效率。

Abstract: Human Activity Recognition is a foundational task in pervasive computing. While recent advances in self-supervised learning and transformer-based architectures have significantly improved HAR performance, adapting large pretrained models to new domains remains a practical challenge due to limited computational resources on target devices. This papers investigates parameter-efficient fine-tuning techniques, specifically Low-Rank Adaptation (LoRA) and Quantized LoRA, as scalable alternatives to full model fine-tuning for HAR. We propose an adaptation framework built upon a Masked Autoencoder backbone and evaluate its performance under a Leave-One-Dataset-Out validation protocol across five open HAR datasets. Our experiments demonstrate that both LoRA and QLoRA can match the recognition performance of full fine-tuning while significantly reducing the number of trainable parameters, memory usage, and training time. Further analyses reveal that LoRA maintains robust performance even under limited supervision and that the adapter rank provides a controllable trade-off between accuracy and efficiency. QLoRA extends these benefits by reducing the memory footprint of frozen weights through quantization, with minimal impact on classification quality.

</details>


### [42] [TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates](https://arxiv.org/abs/2512.18129)
*Maxmillan Ries,Sohan Seth*

Main category: cs.LG

TL;DR: TraCeR：基于Transformer的生存分析框架，用于处理纵向协变量，无需比例风险假设，并评估模型校准


<details>
  <summary>Details</summary>
Motivation: 现有深度学习生存分析模型主要关注横截面特征，难以有效纳入纵向协变量，且评估时主要关注区分度而忽视校准性

Method: 基于因子化自注意力架构的Transformer框架，从测量序列估计风险函数，自然捕捉时间协变量交互，无需数据生成过程假设，支持删失数据和竞争事件

Result: 在多个真实世界数据集上，TraCeR相比最先进方法取得显著且统计显著的性能提升，同时评估了模型校准性

Conclusion: TraCeR成功解决了生存分析中纵向协变量纳入和校准评估的关键挑战，为时间到事件数据建模提供了更全面的框架

Abstract: Survival analysis is a critical tool for modeling time-to-event data. Recent deep learning-based models have reduced various modeling assumptions including proportional hazard and linearity. However, a persistent challenge remains in incorporating longitudinal covariates, with prior work largely focusing on cross-sectional features, and in assessing calibration of these models, with research primarily focusing on discrimination during evaluation. We introduce TraCeR, a transformer-based survival analysis framework for incorporating longitudinal covariates. Based on a factorized self-attention architecture, TraCeR estimates the hazard function from a sequence of measurements, naturally capturing temporal covariate interactions without assumptions about the underlying data-generating process. The framework is inherently designed to handle censored data and competing events. Experiments on multiple real-world datasets demonstrate that TraCeR achieves substantial and statistically significant performance improvements over state-of-the-art methods. Furthermore, our evaluation extends beyond discrimination metrics and assesses model calibration, addressing a key oversight in literature.

</details>


### [43] [A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations](https://arxiv.org/abs/2512.17984)
*Mohammadmahdi Rahimiasl,Ynte Vanderhoydonc,Siegfried Mercelis*

Main category: cs.LG

TL;DR: HINT提出了一种混合归纳-转导网络，结合速度的转导信号和流量的归纳学习，通过空间变换器、扩散GCN和校准层，在三个真实数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 交通流量估算面临挑战：环形检测器精确但稀疏，探针车辆速度广泛可用但与流量相关性弱，相邻路段流量尺度差异大（如匝道与主线），这打破了标准GNN的假设。

Method: 提出HINT混合归纳-转导网络，包含：(1) 归纳空间变换器学习节点特征的相似性驱动长程交互；(2) 基于FiLM条件化的扩散GCN，利用丰富静态上下文（OSM属性和交通仿真）；(3) 节点级校准层纠正尺度偏差。训练采用掩码重建、逐轮节点采样、困难节点挖掘和可见流量噪声注入。

Result: 在三个真实数据集（MOW、UTD19-Torino、UTD19-Essen）上，HINT持续超越最先进的归纳基线。相比KITS，在MOW上MAE降低约42%（基础仿真）和50%（校准仿真）；在Torino降低约22%；在Essen降低约12%。即使没有仿真，HINT在MOW和Torino上仍表现优异。

Conclusion: 结合归纳流量估算与转导速度、交通仿真和外部地理空间信息，能显著提高交通流量估算的准确性，HINT方法为未感知位置的流量估算提供了有效解决方案。

Abstract: Accurately imputing traffic flow at unsensed locations is difficult: loop detectors provide precise but sparse measurements, speed from probe vehicles is widely available yet only weakly correlated with flow, and nearby links often exhibit strong heterophily in the scale of traffic flow (e.g., ramps vs. mainline), which breaks standard GNN assumptions. We propose HINT, a Hybrid INductive-Transductive Network, and an INDU-TRANSDUCTIVE training strategy that treats speed as a transductive, network-wide signal while learning flow inductively to generalize to unseen locations. HINT couples (i) an inductive spatial transformer that learns similarity-driven, long-range interactions from node features with (ii) a diffusion GCN conditioned by FiLM on rich static context (OSM-derived attributes and traffic simulation), and (iii) a node-wise calibration layer that corrects scale biases per segment. Training uses masked reconstruction with epoch-wise node sampling, hard-node mining to emphasize difficult sensors, and noise injection on visible flows to prevent identity mapping, while graph structure is built from driving distances.
  Across three real-world datasets, MOW (Antwerp, Belgium), UTD19-Torino, and UTD19-Essen, HINT consistently surpasses state-of-the-art inductive baselines. Relative to KITS, HINT reduces MAE on MOW by $\approx42$% with basic simulation and $\approx50$% with calibrated simulation; on Torino by $\approx22$%, and on Essen by $\approx12$%. Even without simulation, HINT remains superior on MOW and Torino, while simulation is crucial on Essen. These results show that combining inductive flow imputation with transductive speed, traffic simulations and external geospatial improves accuracy for the task described above.

</details>


### [44] [Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale](https://arxiv.org/abs/2512.18373)
*Ansh Nagwekar*

Main category: cs.LG

TL;DR: 该论文系统分析了神经网络优化算法的演变，从经典一阶方法到现代高阶技术，揭示了如何通过原则性算法设计来理解训练过程，并提供了将先进优化方法整合到现代深度学习工作流中的实践指导。


<details>
  <summary>Details</summary>
Motivation: 神经网络优化是现代AI研究中至关重要但理解不足的挑战。虽然SGD及其变体已成为训练深度网络的事实标准，但它们在过参数化环境中的成功更多是经验性的而非原则性的。本文旨在通过追溯优化算法的演变来揭示原则性算法设计如何帮助理解训练过程。

Method: 从SGD和自适应梯度方法的基本原理出发，逐步分析这些传统方法在面对真实世界数据各向异性时的局限性。这些局限性促使探索基于曲率信息的复杂替代方案：二阶近似技术、分层预处理、自适应学习率等。同时研究了优化算法与更广泛的神经网络训练工具（如最大更新参数化、学习率调度、指数移动平均）之间的相互作用。

Result: 揭示了传统优化方法在真实世界数据各向异性条件下的局限性，展示了基于曲率信息的高级优化技术的优势，并阐明了优化算法与其他训练工具之间的重要协同作用。

Conclusion: 通过原则性算法设计可以更好地理解和改进神经网络训练过程。论文提供了将先进优化方法整合到现代深度学习工作流中的实践处方和实现策略，弥合了理论理解与实际部署之间的差距。

Abstract: Neural network optimization remains one of the most consequential yet poorly understood challenges in modern AI research, where improvements in training algorithms can lead to enhanced feature learning in foundation models, order-of-magnitude reductions in training time, and improved interpretability into how networks learn. While stochastic gradient descent (SGD) and its variants have become the de facto standard for training deep networks, their success in these over-parameterized regimes often appears more empirical than principled. This thesis investigates this apparent paradox by tracing the evolution of optimization algorithms from classical first-order methods to modern higher-order techniques, revealing how principled algorithmic design can demystify the training process. Starting from first principles with SGD and adaptive gradient methods, the analysis progressively uncovers the limitations of these conventional approaches when confronted with anisotropy that is representative of real-world data. These breakdowns motivate the exploration of sophisticated alternatives rooted in curvature information: second-order approximation techniques, layer-wise preconditioning, adaptive learning rates, and more. Next, the interplay between these optimization algorithms and the broader neural network training toolkit, which includes prior and recent developments such as maximal update parametrization, learning rate schedules, and exponential moving averages, emerges as equally essential to empirical success. To bridge the gap between theoretical understanding and practical deployment, this paper offers practical prescriptions and implementation strategies for integrating these methods into modern deep learning workflows.

</details>


### [45] [MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar & Unfamiliar Movements](https://arxiv.org/abs/2512.17985)
*Ruichen Tan,Jiawei Xue,Kota Tsubouchi,Takahiro Yabe,Satish V. Ukkusuri*

Main category: cs.LG

TL;DR: MoE-TransMov：基于Transformer和混合专家架构的模型，通过区分熟悉与陌生区域的不同移动模式来提升POI预测精度


<details>
  <summary>Details</summary>
Motivation: 现有POI预测方法未能有效区分用户在熟悉区域和陌生区域的移动模式差异，而研究表明用户在这两种情境下的POI选择行为存在显著不同，需要专门建模

Method: 提出MoE-TransMov模型，结合Transformer自注意力机制和混合专家架构，将用户移动分为熟悉和陌生两类，通过自适应门控网络动态选择最相关的专家模型进行预测

Result: 在两个真实数据集（Foursquare NYC和Kyoto）上的实验表明，MoE-TransMov在Top-1、Top-5、Top-10准确率和平均倒数排名等指标上均优于现有最佳基线方法

Conclusion: 通过区分不同移动情境并采用混合专家架构，能够有效提升移动性预测性能，从而增强推荐系统的个性化程度并推动各种城市应用的发展

Abstract: Accurate prediction of the next point of interest (POI) within human mobility trajectories is essential for location-based services, as it enables more timely and personalized recommendations. In particular, with the rise of these approaches, studies have shown that users exhibit different POI choices in their familiar and unfamiliar areas, highlighting the importance of incorporating user familiarity into predictive models. However, existing methods often fail to distinguish between the movements of users in familiar and unfamiliar regions. To address this, we propose MoE-TransMov, a Transformer-based model with a Transformer model with a Mixture-of-Experts (MoE) architecture designed to use one framework to capture distinct mobility patterns across different moving contexts without requiring separate training for certain data. Using user-check-in data, we classify movements into familiar and unfamiliar categories and develop a specialized expert network to improve prediction accuracy. Our approach integrates self-attention mechanisms and adaptive gating networks to dynamically select the most relevant expert models for different mobility contexts. Experiments on two real-world datasets, including the widely used but small open-source Foursquare NYC dataset and the large-scale Kyoto dataset collected with LY Corporation (Yahoo Japan Corporation), show that MoE-TransMov outperforms state-of-the-art baselines with notable improvements in Top-1, Top-5, Top-10 accuracy, and mean reciprocal rank (MRR). Given the results, we find that by using this approach, we can efficiently improve mobility predictions under different moving contexts, thereby enhancing the personalization of recommendation systems and advancing various urban applications.

</details>


### [46] [The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?](https://arxiv.org/abs/2512.18390)
*Vassilis Digalakis,Christophe Pérignon,Sébastien Saurin,Flore Sentenac*

Main category: cs.LG

TL;DR: 研究组织何时应该用依赖新特征的新模型替换现有模型，提出结合经济与统计的框架，分析学习曲线、数据获取成本等因素，开发三种实用算法并在信用评分数据上验证。


<details>
  <summary>Details</summary>
Motivation: 随着新数据源不断出现，组织面临何时用依赖新特征的新模型替换现有训练好的模型的问题。需要综合考虑学习曲线动态、数据获取和重新训练成本、未来收益折现等因素，做出经济上合理的决策。

Method: 1. 建立统一的经济统计框架，分析学习曲线动态、数据获取成本、重新训练成本和未来收益折现；2. 在简化场景下推导最优切换时间的闭式解；3. 提出三种实用算法：一次性基线方法、贪婪序列方法和前瞻序列方法；4. 在真实信用评分数据集上验证，该数据集包含逐渐到达的替代数据。

Result: 1. 最优切换时间系统地随成本参数和学习曲线行为变化；2. 前瞻序列方法优于其他方法，能够接近具有完全预见性的预言机价值；3. 建立了有限样本保证，包括前瞻序列方法相对于预言机实现次线性遗憾的条件。

Conclusion: 研究结果为新数据源可用时经济上合理的模型转换提供了操作蓝图，提出的框架和算法帮助组织在模型更新决策中平衡学习收益与切换成本。

Abstract: We study the problem of deciding whether, and when an organization should replace a trained incumbent model with a challenger relying on newly available features. We develop a unified economic and statistical framework that links learning-curve dynamics, data-acquisition and retraining costs, and discounting of future gains. First, we characterize the optimal switching time in stylized settings and derive closed-form expressions that quantify how horizon length, learning-curve curvature, and cost differentials shape the optimal decision. Second, we propose three practical algorithms: a one-shot baseline, a greedy sequential method, and a look-ahead sequential method. Using a real-world credit-scoring dataset with gradually arriving alternative data, we show that (i) optimal switching times vary systematically with cost parameters and learning-curve behavior, and (ii) the look-ahead sequential method outperforms other methods and is able to approach in value an oracle with full foresight. Finally, we establish finite-sample guarantees, including conditions under which the sequential look-ahead method achieve sublinear regret relative to that oracle. Our results provide an operational blueprint for economically sound model transitions as new data sources become available.

</details>


### [47] [FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability](https://arxiv.org/abs/2512.17986)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: FedOAED是一种新颖的联邦学习算法，通过客户端设备上的自动编码器去噪器来缓解客户端漂移和部分客户端参与带来的方差问题，在非独立同分布数据设置下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然联邦学习（FL）在数据隐私保护方面有优势，能够绕过GDPR和HIPAA等数据共享法规限制，但实际应用中仍面临数据异构性带来的挑战，包括梯度噪声、客户端漂移以及部分客户端参与导致的方差增加等问题。

Method: FedOAED算法在客户端侧集成了设备上的自动编码器去噪器，旨在缓解由多个本地训练更新引起的客户端漂移，以及部分客户端参与导致的方差问题，特别针对异构数据在有限客户端可用性下的场景。

Result: 在多个视觉数据集上的非独立同分布（Non-IID）设置实验中，FedOAED始终优于最先进的基线方法。

Conclusion: FedOAED通过创新的客户端去噪机制有效解决了联邦学习中由数据异构性和部分参与带来的关键挑战，为实际部署提供了有前景的解决方案。

Abstract: Over the last few decades, machine learning (ML) and deep learning (DL) solutions have demonstrated their potential across many applications by leveraging large amounts of high-quality data. However, strict data-sharing regulations such as the General Data Protection Regulation (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA) have prevented many data-driven applications from being realised. Federated Learning (FL), in which raw data never leaves local devices, has shown promise in overcoming these limitations. Although FL has grown rapidly in recent years, it still struggles with heterogeneity, which produces gradient noise, client-drift, and increased variance from partial client participation. In this paper, we propose FedOAED, a novel federated learning algorithm designed to mitigate client-drift arising from multiple local training updates and the variance induced by partial client participation. FedOAED incorporates an on-device autoencoder denoiser on the client side to mitigate client-drift and variance resulting from heterogeneous data under limited client availability. Experiments on multiple vision datasets under Non-IID settings demonstrate that FedOAED consistently outperforms state-of-the-art baselines.

</details>


### [48] [A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients](https://arxiv.org/abs/2512.18031)
*Sarah Nassar,Nooshin Maghsoodi,Sophia Mannina,Shamel Addas,Stephanie Sibley,Gabor Fichtinger,David Pichora,David Maslove,Purang Abolmaesumi,Parvin Mousavi*

Main category: cs.LG

TL;DR: 该研究发布了ICU房颤检测的标注数据集和基准测试，比较了三种AI方法，发现ECG基础模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 房颤是ICU患者最常见的心律失常，可能导致不良健康后果。目前缺乏ICU环境下房颤检测的标注数据集和AI方法比较基准

Method: 使用加拿大ICU和PhysioNet/Computing in Cardiology Challenge 2021的ECG数据，比较三种AI方法：基于特征的分类器、深度学习和ECG基础模型，测试多种训练配置（从零样本推理到迁移学习）

Result: ECG基础模型平均表现最佳，其次是深度学习，最后是基于特征的分类器。在ICU测试集上，通过迁移学习的ECG-FM获得最高F1分数（0.89）

Conclusion: AI在构建自动患者监测系统方面具有潜力，通过发布标注ICU数据集和性能基准，推动ICU房颤检测研究进展

Abstract: Objective: Atrial fibrillation (AF) is the most common cardiac arrhythmia experienced by intensive care unit (ICU) patients and can cause adverse health effects. In this study, we publish a labelled ICU dataset and benchmarks for AF detection. Methods: We compared machine learning models across three data-driven artificial intelligence (AI) approaches: feature-based classifiers, deep learning (DL), and ECG foundation models (FMs). This comparison addresses a critical gap in the literature and aims to pinpoint which AI approach is best for accurate AF detection. Electrocardiograms (ECGs) from a Canadian ICU and the 2021 PhysioNet/Computing in Cardiology Challenge were used to conduct the experiments. Multiple training configurations were tested, ranging from zero-shot inference to transfer learning. Results: On average and across both datasets, ECG FMs performed best, followed by DL, then feature-based classifiers. The model that achieved the top F1 score on our ICU test set was ECG-FM through a transfer learning strategy (F1=0.89). Conclusion: This study demonstrates promising potential for using AI to build an automatic patient monitoring system. Significance: By publishing our labelled ICU dataset (LinkToBeAdded) and performance benchmarks, this work enables the research community to continue advancing the state-of-the-art in AF detection in the ICU.

</details>


### [49] [Secret mixtures of experts inside your LLM](https://arxiv.org/abs/2512.18452)
*Enric Boix-Adsera*

Main category: cs.LG

TL;DR: 研究发现Transformer中的MLP层实际上执行稀疏计算，可近似为稀疏激活的MoE层，这解释了MoE模型的有效性并提出了基于低秩路由器的更高效MoE架构设计方向。


<details>
  <summary>Details</summary>
Motivation: 尽管MLP是Transformer架构中最早出现的组件之一，但由于其密集计算和难以可视化，人们对它的理解仍然有限。本文旨在理解大型语言模型中MLP层的工作原理，提出这些层实际上执行稀疏计算的假设。

Method: 建立了MoE模型与激活空间中稀疏自编码器结构之间的理论联系，并在预训练LLM上进行了实证验证。特别关注了激活分布的重要性，对比了高斯数据和神经网络激活的结构化分布。

Result: 实证验证了MLP层可以很好地近似为稀疏激活的MoE层，但这种结果仅存在于神经网络激活的结构化分布中，不适用于高斯数据。激活分布的结构对这种现象至关重要。

Conclusion: 揭示了LLM中MLP层工作的一般原理，解释了现代基于MoE的Transformer的有效性。实验探索为基于低秩路由器的更高效MoE架构设计提供了新方向。

Abstract: Despite being one of the earliest neural network layers, the Multilayer Perceptron (MLP) is arguably one of the least understood parts of the transformer architecture due to its dense computation and lack of easy visualization. This paper seeks to understand the MLP layers in dense LLM models by hypothesizing that these layers secretly approximately perform a sparse computation -- namely, that they can be well approximated by sparsely-activating Mixture of Experts (MoE) layers.
  Our hypothesis is based on a novel theoretical connection between MoE models and Sparse Autoencoder (SAE) structure in activation space. We empirically validate the hypothesis on pretrained LLMs, and demonstrate that the activation distribution matters -- these results do not hold for Gaussian data, but rather rely crucially on structure in the distribution of neural network activations.
  Our results shine light on a general principle at play in MLP layers inside LLMs, and give an explanation for the effectiveness of modern MoE-based transformers. Additionally, our experimental explorations suggest new directions for more efficient MoE architecture design based on low-rank routers.

</details>


### [50] [Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models](https://arxiv.org/abs/2512.18035)
*Wei Qian,Chenxu Zhao,Yangyi Li,Mengdi Huai*

Main category: cs.LG

TL;DR: 该论文提出了首个选择性遗忘（机器去学习）隐私漏洞的全面基准测试，系统评估了不同数据、攻击方法、去学习技术和模型架构下的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在关键领域部署，确保隐私和符合人类价值观变得至关重要。选择性遗忘虽然对隐私保护和数据移除有前景，但本身也引发新的隐私担忧。现有研究对各种去学习诱导的隐私攻击评估不一致，缺乏标准化基准，可能导致不公平比较和过于乐观的评估。

Method: 构建首个全面的选择性遗忘隐私漏洞基准测试框架，广泛调查机器去学习技术的隐私漏洞，跨多种受害数据、最先进的去学习隐私攻击方法、去学习技术和模型架构进行隐私泄露基准测试，系统评估和识别与去学习诱导隐私泄露相关的关键因素。

Result: 通过系统评估发现了去学习诱导隐私泄露的关键因素，为从业者提供了标准化的评估工具，能够更准确地评估定制化去学习应用的隐私风险。

Conclusion: 该研究填补了选择性遗忘领域缺乏标准化隐私评估工具的空白，为部署定制化去学习应用提供了可靠的隐私评估基准，有助于推动该领域的健康发展。

Abstract: The rapid advancements in artificial intelligence (AI) have primarily focused on the process of learning from data to acquire knowledgeable learning systems. As these systems are increasingly deployed in critical areas, ensuring their privacy and alignment with human values is paramount. Recently, selective forgetting (also known as machine unlearning) has shown promise for privacy and data removal tasks, and has emerged as a transformative paradigm shift in the field of AI. It refers to the ability of a model to selectively erase the influence of previously seen data, which is especially important for compliance with modern data protection regulations and for aligning models with human values. Despite its promise, selective forgetting raises significant privacy concerns, especially when the data involved come from sensitive domains. While new unlearning-induced privacy attacks are continuously proposed, each is shown to outperform its predecessors using different experimental settings, which can lead to overly optimistic and potentially unfair assessments that may disproportionately favor one particular attack over the others. In this work, we present the first comprehensive benchmark for evaluating privacy vulnerabilities in selective forgetting. We extensively investigate privacy vulnerabilities of machine unlearning techniques and benchmark privacy leakage across a wide range of victim data, state-of-the-art unlearning privacy attacks, unlearning methods, and model architectures. We systematically evaluate and identify critical factors related to unlearning-induced privacy leakage. With our novel insights, we aim to provide a standardized tool for practitioners seeking to deploy customized unlearning applications with faithful privacy assessments.

</details>


### [51] [From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers](https://arxiv.org/abs/2512.18634)
*Ryotaro Kawata,Yujin Song,Alberto Bietti,Naoki Nishikawa,Taiji Suzuki,Samuel Vaiter,Denny Wu*

Main category: cs.LG

TL;DR: 研究显示，Transformer在预训练数据分布影响下，会在通用算法（如归纳头）和位置捷径之间切换。当输入序列多样性足够（触发距离的"最大和"比率低）时，模型学习归纳头并泛化；反之则依赖位置捷径且无法泛化。研究还揭示了预训练上下文长度与OOD泛化之间的权衡。


<details>
  <summary>Details</summary>
Motivation: Transformer可以学习通用算法（如归纳头）或简单的位置捷径。本研究旨在探索预训练数据分布如何引导浅层Transformer选择不同行为机制，理解数据分布对模型学习算法偏好的影响。

Method: 聚焦于最小触发-输出预测任务（复制特殊触发器第二次出现后的标记），对单层Transformer进行梯度训练的理论分析。在无限和有限样本情况下，通过"最大和"比率衡量触发距离多样性，分析模型机制转变。还推导了最小化计算成本的优化预训练分布。

Result: 理论证明：当输入序列多样性足够（触发距离的"最大和"比率低）时，模型学习归纳头并实现OOD泛化；当比率高时，模型依赖位置捷径且无法泛化。揭示了预训练上下文长度与OOD泛化之间的权衡，并推导了最优预训练分布。合成实验验证了理论预测。

Conclusion: 预训练数据分布是控制Transformer学习行为的关键因素。通过设计具有足够多样性的数据分布，可以引导模型学习通用算法而非位置捷径，从而实现更好的OOD泛化。这为数据驱动的Transformer行为控制提供了概念性指导。

Abstract: Transformers can implement both generalizable algorithms (e.g., induction heads) and simple positional shortcuts (e.g., memorizing fixed output positions). In this work, we study how the choice of pretraining data distribution steers a shallow transformer toward one behavior or the other. Focusing on a minimal trigger-output prediction task -- copying the token immediately following a special trigger upon its second occurrence -- we present a rigorous analysis of gradient-based training of a single-layer transformer. In both the infinite and finite sample regimes, we prove a transition in the learned mechanism: if input sequences exhibit sufficient diversity, measured by a low ``max-sum'' ratio of trigger-to-trigger distances, the trained model implements an induction head and generalizes to unseen contexts; by contrast, when this ratio is large, the model resorts to a positional shortcut and fails to generalize out-of-distribution (OOD). We also reveal a trade-off between the pretraining context length and OOD generalization, and derive the optimal pretraining distribution that minimizes computational cost per sample. Finally, we validate our theoretical predictions with controlled synthetic experiments, demonstrating that broadening context distributions robustly induces induction heads and enables OOD generalization. Our results shed light on the algorithmic biases of pretrained transformers and offer conceptual guidelines for data-driven control of their learned behaviors.

</details>


### [52] [Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics](https://arxiv.org/abs/2512.18056)
*Daniel David*

Main category: cs.LG

TL;DR: 提出概率数字孪生框架，用潜在随机状态建模用户行为，通过变分自编码器学习可解释、不确定性感知的用户表示


<details>
  <summary>Details</summary>
Motivation: 现有用户建模方法（确定性嵌入或黑盒预测模型）缺乏不确定性量化和对潜在表示内容的洞察，需要更可解释、概率性的用户表示框架

Method: 提出概率数字孪生框架，将用户建模为生成观测行为数据的潜在随机状态；使用摊销变分推断学习，通过变分自编码器实现；引入统计解释流程，将潜在维度与可观测行为模式关联

Result: 用户结构主要是连续而非离散聚类的，少数主导潜在轴出现弱但有意义的结构；特定维度对应可解释特征（如观点强度和决断力）；验证了潜在维度与行为模式的统计显著差异

Conclusion: 概率数字孪生能提供超越确定性用户嵌入的可解释、不确定性感知表示，为个性化、推荐和决策支持应用提供更好的用户建模方法

Abstract: Understanding user identity and behavior is central to applications such as personalization, recommendation, and decision support. Most existing approaches rely on deterministic embeddings or black-box predictive models, offering limited uncertainty quantification and little insight into what latent representations encode. We propose a probabilistic digital twin framework in which each user is modeled as a latent stochastic state that generates observed behavioral data. The digital twin is learned via amortized variational inference, enabling scalable posterior estimation while retaining a fully probabilistic interpretation. We instantiate this framework using a variational autoencoder (VAE) applied to a user-response dataset designed to capture stable aspects of user identity. Beyond standard reconstruction-based evaluation, we introduce a statistically grounded interpretation pipeline that links latent dimensions to observable behavioral patterns. By analyzing users at the extremes of each latent dimension and validating differences using nonparametric hypothesis tests and effect sizes, we demonstrate that specific dimensions correspond to interpretable traits such as opinion strength and decisiveness. Empirically, we find that user structure is predominantly continuous rather than discretely clustered, with weak but meaningful structure emerging along a small number of dominant latent axes. These results suggest that probabilistic digital twins can provide interpretable, uncertainty-aware representations that go beyond deterministic user embeddings.

</details>


### [53] [A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage](https://arxiv.org/abs/2512.19142)
*Francis Bach*

Main category: cs.LG

TL;DR: 提出基于Choquet积分的凸损失函数，用于集合预测，可优化条件概率覆盖与集合"大小"之间的权衡，优于边际覆盖方法。


<details>
  <summary>Details</summary>
Motivation: 在监督学习中，集合预测能提供明确的不确定性估计。现有方法通常关注边际覆盖，但需要更灵活的方法来平衡条件概率覆盖与集合大小，并适应不对称损失场景。

Method: 使用Choquet积分（Lovász扩展）为实值函数水平集定义凸损失函数；提出多个扩展模拟不对称损失的二元分类损失函数；开发基于随机梯度下降或重加权最小二乘的高效优化算法。

Result: 在合成数据集上的分类和回归任务实验显示，该方法在优化条件覆盖方面优于追求边际覆盖的方法，能有效平衡覆盖率和集合大小。

Conclusion: 提出的基于Choquet积分的凸损失函数框架为集合预测提供了灵活的工具，能优化条件覆盖并适应不对称损失场景，在不确定性估计方面优于边际覆盖方法。

Abstract: We consider supervised learning problems in which set predictions provide explicit uncertainty estimates. Using Choquet integrals (a.k.a. Lov{á}sz extensions), we propose a convex loss function for nondecreasing subset-valued functions obtained as level sets of a real-valued function. This loss function allows optimal trade-offs between conditional probabilistic coverage and the ''size'' of the set, measured by a non-decreasing submodular function. We also propose several extensions that mimic loss functions and criteria for binary classification with asymmetric losses, and show how to naturally obtain sets with optimized conditional coverage. We derive efficient optimization algorithms, either based on stochastic gradient descent or reweighted least-squares formulations, and illustrate our findings with a series of experiments on synthetic datasets for classification and regression tasks, showing improvements over approaches that aim for marginal coverage.

</details>


### [54] [Microstructure-based Variational Neural Networks for Robust Uncertainty Quantification in Materials Digital Twins](https://arxiv.org/abs/2512.18104)
*Andreas E. Robertson,Samuel B. Inman,Ashley T. Lenau,Ricardo A. Lebensohn,Dongil Shin,Brad L. Boyce,Remi M. Dingreville*

Main category: cs.LG

TL;DR: VDMN是一种物理信息代理模型，通过嵌入变分分布捕捉微观结构不确定性，支持高效概率前向和逆向预测，用于构建不确定性鲁棒的材料数字孪生。


<details>
  <summary>Details</summary>
Motivation: 解决材料数字孪生中固有的不确定性挑战，包括微观结构形态、组成行为和加工条件的不可变异性，这些不确定性对开发鲁棒的数字孪生构成主要障碍。

Method: 提出变分深度材料网络（VDMN），在其分层机制架构中嵌入变分分布，采用基于泰勒级数展开和自动微分的解析传播方案，高效传播训练和预测过程中的不确定性。

Result: 在两个数字孪生应用中验证：1）作为不确定性感知材料数字孪生，预测并实验验证了增材制造聚合物复合材料的非线性力学变异性；2）作为逆向校准引擎，解耦并定量识别了组成特性中的重叠不确定性来源。

Conclusion: VDMN为构建不确定性鲁棒的材料数字孪生提供了基础框架，能够有效处理材料系统中的固有变异性，支持概率前向预测和逆向不确定性量化。

Abstract: Aleatoric uncertainties - irremovable variability in microstructure morphology, constituent behavior, and processing conditions - pose a major challenge to developing uncertainty-robust digital twins. We introduce the Variational Deep Material Network (VDMN), a physics-informed surrogate model that enables efficient and probabilistic forward and inverse predictions of material behavior. The VDMN captures microstructure-induced variability by embedding variational distributions within its hierarchical, mechanistic architecture. Using an analytic propagation scheme based on Taylor-series expansion and automatic differentiation, the VDMN efficiently propagates uncertainty through the network during training and prediction. We demonstrate its capabilities in two digital-twin-driven applications: (1) as an uncertainty-aware materials digital twin, it predicts and experimentally validates the nonlinear mechanical variability in additively manufactured polymer composites; and (2) as an inverse calibration engine, it disentangles and quantitatively identifies overlapping sources of uncertainty in constituent properties. Together, these results establish the VDMN as a foundation for uncertainty-robust materials digital twins.

</details>


### [55] [Toward Scalable and Valid Conditional Independence Testing with Spectral Representations](https://arxiv.org/abs/2512.19510)
*Alek Frohlich,Vladimir Kostic,Karim Lounici,Daniel Perazzo,Massimiliano Pontil*

Main category: cs.LG

TL;DR: 提出一种基于表示学习的条件独立性检验方法，通过偏协方差算子的奇异值分解学习表示，构建类似HSIC的检验统计量，并设计双层对比学习算法，实现可扩展的条件独立性检验。


<details>
  <summary>Details</summary>
Motivation: 条件独立性在因果推断、特征选择和图模型中至关重要，但在许多设置中无法检验。现有方法依赖限制性结构条件，核方法存在适应性有限、收敛慢和可扩展性差的问题。探索表示学习是否能解决这些限制。

Method: 基于偏协方差算子的奇异值分解学习表示，构建类似HSIC的检验统计量，并提出双层对比学习算法来学习这些表示。

Result: 理论分析将表示学习误差与检验性能联系起来，建立了渐近有效性和功效保证。初步实验表明该方法为可扩展的条件独立性检验提供了实用且统计基础良好的路径。

Conclusion: 该方法通过表示学习桥接了基于核的理论与现代表示学习，为解决条件独立性检验的可扩展性和适应性限制提供了有前景的方向。

Abstract: Conditional independence (CI) is central to causal inference, feature selection, and graphical modeling, yet it is untestable in many settings without additional assumptions. Existing CI tests often rely on restrictive structural conditions, limiting their validity on real-world data. Kernel methods using the partial covariance operator offer a more principled approach but suffer from limited adaptivity, slow convergence, and poor scalability. In this work, we explore whether representation learning can help address these limitations. Specifically, we focus on representations derived from the singular value decomposition of the partial covariance operator and use them to construct a simple test statistic, reminiscent of the Hilbert-Schmidt Independence Criterion (HSIC). We also introduce a practical bi-level contrastive algorithm to learn these representations. Our theory links representation learning error to test performance and establishes asymptotic validity and power guarantees. Preliminary experiments suggest that this approach offers a practical and statistically grounded path toward scalable CI testing, bridging kernel-based theory with modern representation learning.

</details>


### [56] [Learning Generalizable Neural Operators for Inverse Problems](https://arxiv.org/abs/2512.18120)
*Adam J. Thorpe,Stepan Tretiakov,Dibakar Roy Sarkar,Krishna Kumar,Ufuk Topcu*

Main category: cs.LG

TL;DR: B2B⁻¹框架通过分离函数表示与逆映射学习，解决反问题中神经算子面临的连续性、唯一性和稳定性挑战，能够处理不同病态程度的反问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子架构在处理反问题时面临挑战，因为病态的反映射违反了连续性、唯一性和稳定性假设，需要新的框架来处理这些限制。

Method: 提出B2B⁻¹框架，将函数表示与逆映射解耦：首先为输入和输出空间学习神经基函数，然后在得到的系数空间上训练逆模型。这种结构允许在单一框架内学习确定性、可逆和概率模型。

Result: 在六个反PDE基准测试（包括两个新数据集）上评估，相比现有可逆神经算子基线表现更好。学习的概率模型能够捕捉不确定性和输入变异性，并由于系数计算中的隐式去噪而对测量噪声保持鲁棒。

Conclusion: 通过分离表示与反演，该框架为反问题提供了可扩展的代理模型，能够跨实例、领域和病态程度泛化，在不同病态程度下保持一致的重新模拟性能。

Abstract: Inverse problems challenge existing neural operator architectures because ill-posed inverse maps violate continuity, uniqueness, and stability assumptions. We introduce B2B${}^{-1}$, an inverse basis-to-basis neural operator framework that addresses this limitation. Our key innovation is to decouple function representation from the inverse map. We learn neural basis functions for the input and output spaces, then train inverse models that operate on the resulting coefficient space. This structure allows us to learn deterministic, invertible, and probabilistic models within a single framework, and to choose models based on the degree of ill-posedness. We evaluate our approach on six inverse PDE benchmarks, including two novel datasets, and compare against existing invertible neural operator baselines. We learn probabilistic models that capture uncertainty and input variability, and remain robust to measurement noise due to implicit denoising in the coefficient calculation. Our results show consistent re-simulation performance across varying levels of ill-posedness. By separating representation from inversion, our framework enables scalable surrogate models for inverse problems that generalize across instances, domains, and degrees of ill-posedness.

</details>


### [57] [Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection](https://arxiv.org/abs/2512.18133)
*Jie Yang,Rui Zhang,Ziyang Cheng,Dawei Cheng,Guang Yang,Bo Wang*

Main category: cs.LG

TL;DR: 提出Grad模型，通过关系扩散图增强和对比学习解决金融图欺诈检测中的自适应伪装问题


<details>
  <summary>Details</summary>
Motivation: 现实金融场景中，有组织的犯罪集团采用自适应伪装策略，模仿良性用户行为特征，使得欺诈者与正常用户在平台数据库中的行为特征差异变小，导致现有图欺诈检测模型失效

Method: 提出基于关系扩散的图增强模型Grad：1）使用监督图对比学习模块增强欺诈-良性差异；2）采用引导关系扩散生成器从头生成辅助同质关系；3）在聚合过程中增强弱欺诈信号

Result: 在微信支付提供的两个真实数据集和三个公开数据集上实验，Grad在多种场景下优于SOTA方法，AUC和AP分别提升最多11.10%和43.95%

Conclusion: Grad模型能有效应对金融欺诈中的自适应伪装问题，通过图增强和对比学习显著提升欺诈检测性能

Abstract: Nowadays, Graph Fraud Detection (GFD) in financial scenarios has become an urgent research topic to protect online payment security. However, as organized crime groups are becoming more professional in real-world scenarios, fraudsters are employing more sophisticated camouflage strategies. Specifically, fraudsters disguise themselves by mimicking the behavioral data collected by platforms, ensuring that their key characteristics are consistent with those of benign users to a high degree, which we call Adaptive Camouflage. Consequently, this narrows the differences in behavioral traits between them and benign users within the platform's database, thereby making current GFD models lose efficiency. To address this problem, we propose a relation diffusion-based graph augmentation model Grad. In detail, Grad leverages a supervised graph contrastive learning module to enhance the fraud-benign difference and employs a guided relation diffusion generator to generate auxiliary homophilic relations from scratch. Based on these, weak fraudulent signals would be enhanced during the aggregation process, thus being obvious enough to be captured. Extensive experiments have been conducted on two real-world datasets provided by WeChat Pay, one of the largest online payment platforms with billions of users, and three public datasets. The results show that our proposed model Grad outperforms SOTA methods in both various scenarios, achieving at most 11.10% and 43.95% increases in AUC and AP, respectively. Our code is released at https://github.com/AI4Risk/antifraud and https://github.com/Muyiiiii/WWW25-Grad.

</details>


### [58] [Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation](https://arxiv.org/abs/2512.18174)
*Lena Libon,Meghana Bhange,Rushabh Solanki,Elliot Creager,Ulrich Aïvodji*

Main category: cs.LG

TL;DR: 论文探讨LLM推理过程中的中间计算应被视为用户个人数据，并提出基于"有意识数据贡献"框架，让低效用社区聚合知识蒸馏出更符合自身目标的替代模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展强调大规模数据集训练大型模型，催生了LLM聊天机器人等新产品，但也引发数据隐私和用户选择权的担忧。论文关注在LLM使用思维链推理时，数据可移植性和用户自主权的问题。

Method: 首先从数据隐私和可移植性法律角度论证中间计算应被视为用户个人数据。然后基于"有意识数据贡献"框架，展示低效用社区如何聚合和蒸馏共享知识，创建更符合自身目标的替代模型。最后通过实证研究验证该方法，并分析社区多样性、推理粒度和社区规模对蒸馏性能的影响。

Result: 通过实证验证了所提方法的有效性，并研究了社区多样性、推理粒度和社区规模对知识蒸馏性能的具体影响。

Conclusion: LLM推理过程中的中间计算应被视为用户个人数据，社区可以通过聚合和蒸馏共享知识来创建更符合自身目标的替代模型，这为数据隐私、用户自主权和模型对齐提供了新的解决方案。

Abstract: The current era of AI development places a heavy emphasis on training large models on increasingly scaled-up datasets. This paradigm has catalyzed entirely new product categories, such as LLM chatbots, while also raising concerns about data privacy and consumer choice. In this paper, we consider questions of data portability and user autonomy in the context of LLMs that "reason" using chain-of-thought (CoT) traces, computing intermediate text artifacts from user input before producing a final output. We first interpret recent data privacy and portability law to argue that these intermediate computations qualify as users' personal data. Then, building on the existing framework of Conscious Data Contribution, we show how communities who receive low utility from an available model can aggregate and distill their shared knowledge into an alternate model better aligned with their goals. We verify this approach empirically and investigate the effects of community diversity, reasoning granularity, and community size on distillation performance.

</details>


### [59] [FairExpand: Individual Fairness on Graphs with Partial Similarity Information](https://arxiv.org/abs/2512.18180)
*Rebecca Salganik,Yibin Wang,Guillaume Salha-Galvan,Jian Kang*

Main category: cs.LG

TL;DR: FairExpand是一个灵活的框架，用于在仅有部分节点对相似性信息的情况下促进图表示学习中的个体公平性，通过交替优化节点表示和传播相似性信息来实现公平性扩展。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习中的个体公平性方法需要所有节点对的预定义相似性信息，这在现实中往往不切实际，阻碍了这些方法在实际应用中的部署。

Method: FairExpand采用两步流水线：1) 使用骨干模型（如图神经网络）优化节点表示；2) 逐步传播相似性信息，使公平性约束能够有效扩展到整个图。

Result: 大量实验表明，FairExpand能持续增强个体公平性，同时保持模型性能，为具有部分相似性信息的实际应用提供了实用的图基个体公平性解决方案。

Conclusion: FairExpand是一个实用的框架，能够在仅有部分节点对相似性信息的现实场景中实现图表示学习的个体公平性，解决了现有方法需要完整相似性信息的限制。

Abstract: Individual fairness, which requires that similar individuals should be treated similarly by algorithmic systems, has become a central principle in fair machine learning. Individual fairness has garnered traction in graph representation learning due to its practical importance in high-stakes Web areas such as user modeling, recommender systems, and search. However, existing methods assume the existence of predefined similarity information over all node pairs, an often unrealistic requirement that prevents their operationalization in practice. In this paper, we assume the similarity information is only available for a limited subset of node pairs and introduce FairExpand, a flexible framework that promotes individual fairness in this more realistic partial information scenario. FairExpand follows a two-step pipeline that alternates between refining node representations using a backbone model (e.g., a graph neural network) and gradually propagating similarity information, which allows fairness enforcement to effectively expand to the entire graph. Extensive experiments show that FairExpand consistently enhances individual fairness while preserving performance, making it a practical solution for enabling graph-based individual fairness in real-world applications with partial similarity information.

</details>


### [60] [When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics](https://arxiv.org/abs/2512.18209)
*Yizhou Zhang*

Main category: cs.LG

TL;DR: 论文提出在广义分辨率壳层动力学框架下，学习过程的幂律标度行为源于特定的结构条件，包括梯度传播有界、初始化弱功能非相干性、雅可比矩阵演化受控以及对数平移不变性，这些条件共同迫使重正化速度场呈现幂律形式。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习中广泛观察到经验性的幂律标度现象，但其理论起源和适用范围仍不完全清楚。论文旨在理解在广义分辨率壳层动力学框架下，何时以及为何会出现幂律标度行为。

Method: 采用广义分辨率壳层动力学框架，将学习过程建模为谱能量在对数分辨率壳层间的传输。提出一组充分条件，包括计算图中梯度传播的有界性、初始化的弱功能非相干性、训练过程中雅可比矩阵演化的受控性，以及重正化壳层耦合的对数平移不变性。证明这些条件共同导致重正化速度场呈现幂律形式。

Result: 识别出GRSD壳层动力学允许重正化粗粒度描述的一组充分条件。证明幂律标度并非仅源于重正化性，而是作为刚性结果出现：将对数平移不变性与梯度流固有的时间重标度协变性结合，会迫使重正化GRSD速度场呈现幂律形式。

Conclusion: 深度学习中观察到的幂律标度行为需要特定的结构条件，这些条件约束了学习配置的多个层面。幂律标度是重正化性和时间协变性共同作用的刚性结果，为理解深度学习系统的标度行为提供了理论框架。

Abstract: Empirical power--law scaling has been widely observed across modern deep learning systems, yet its theoretical origins and scope of validity remain incompletely understood. The Generalized Resolution--Shell Dynamics (GRSD) framework models learning as spectral energy transport across logarithmic resolution shells, providing a coarse--grained dynamical description of training. Within GRSD, power--law scaling corresponds to a particularly simple renormalized shell dynamics; however, such behavior is not automatic and requires additional structural properties of the learning process.
  In this work, we identify a set of sufficient conditions under which the GRSD shell dynamics admits a renormalizable coarse--grained description. These conditions constrain the learning configuration at multiple levels, including boundedness of gradient propagation in the computation graph, weak functional incoherence at initialization, controlled Jacobian evolution along training, and log--shift invariance of renormalized shell couplings. We further show that power--law scaling does not follow from renormalizability alone, but instead arises as a rigidity consequence: once log--shift invariance is combined with the intrinsic time--rescaling covariance of gradient flow, the renormalized GRSD velocity field is forced into a power--law form.

</details>


### [61] [Stable and Efficient Single-Rollout RL for Multimodal Reasoning](https://arxiv.org/abs/2512.18215)
*Rui Liu,Dian Yu,Lei Ke,Haolin Liu,Yujun Zhou,Zhenwen Liang,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.LG

TL;DR: MSSR是一种用于多模态大语言模型的强化学习框架，通过熵基优势整形机制实现单次采样下的稳定训练，在计算效率和推理性能上优于传统的分组方法。


<details>
  <summary>Details</summary>
Motivation: 现有分组式RLVR方法需要多次采样，计算成本高；而单次采样变体在多模态场景中不稳定，容易导致训练崩溃。需要解决训练效率与稳定性之间的权衡问题。

Method: 提出MSSR（多模态稳定单次采样）框架，采用基于熵的优势整形机制，自适应地正则化优势幅度，防止训练崩溃并保持稳定性。

Result: 在分布内评估中，MSSR仅用一半训练步数就达到与分组基线相当的验证准确率；相同步数下性能超越基线，并在五个推理密集型基准测试中展现出一致的泛化改进。

Conclusion: MSSR为复杂多模态推理任务提供了稳定、计算高效且有效的RLVR解决方案，解决了单次采样在多模态场景中的稳定性问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.

</details>


### [62] [Offline Behavioral Data Selection](https://arxiv.org/abs/2512.18246)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: 该论文发现离线行为数据存在显著的数据饱和现象，并提出SDR方法从大规模数据集中提取紧凑且信息丰富的子集。


<details>
  <summary>Details</summary>
Motivation: 离线行为克隆方法从专家演示中学习策略，但大规模离线行为数据集在下游任务中会导致计算密集型训练。研究发现离线行为数据存在数据饱和现象：策略性能在仅使用数据集一小部分时就会迅速饱和，这表明策略性能与测试损失之间的弱对齐关系，通过数据选择有大幅改进空间。

Method: 提出Stepwise Dual Ranking (SDR)方法，基于两个关键原则：(1) stepwise clip：优先考虑早期阶段数据；(2) dual ranking：选择同时具有高动作价值排名和低状态密度排名的样本。

Result: 在D4RL基准测试上的大量实验和消融研究表明，SDR显著提升了离线行为数据的选择效果。

Conclusion: 离线行为数据存在数据饱和现象，通过SDR方法可以有效从大规模离线行为数据集中提取紧凑且信息丰富的子集，显著改善数据选择效果。

Abstract: Behavioral cloning is a widely adopted approach for offline policy learning from expert demonstrations. However, the large scale of offline behavioral datasets often results in computationally intensive training when used in downstream tasks. In this paper, we uncover the striking data saturation in offline behavioral data: policy performance rapidly saturates when trained on a small fraction of the dataset. We attribute this effect to the weak alignment between policy performance and test loss, revealing substantial room for improvement through data selection. To this end, we propose a simple yet effective method, Stepwise Dual Ranking (SDR), which extracts a compact yet informative subset from large-scale offline behavioral datasets. SDR is build on two key principles: (1) stepwise clip, which prioritizes early-stage data; and (2) dual ranking, which selects samples with both high action-value rank and low state-density rank. Extensive experiments and ablation studies on D4RL benchmarks demonstrate that SDR significantly enhances data selection for offline behavioral data.

</details>


### [63] [On the Convergence Rate of LoRA Gradient Descent](https://arxiv.org/abs/2512.18248)
*Siqiao Mu,Diego Klabjan*

Main category: cs.LG

TL;DR: 首次对原始LoRA梯度下降算法进行非渐近收敛分析，证明其以O(1/log T)速率收敛到平稳点，无需Lipschitz光滑性假设。


<details>
  <summary>Details</summary>
Motivation: LoRA算法因其优异性能和低计算需求而流行，但其收敛性缺乏理论理解。现有理论要么考虑渐近行为，要么假设强有界条件人为强制Lipschitz光滑性，无法反映实际应用情况。

Method: 采用三个关键步骤：i) 将问题重新表述为堆叠适配器矩阵外积形式；ii) 为"类Lipschitz"重参数化函数建立改进的下降引理；iii) 控制步长。通过这些方法分析原始LoRA梯度下降算法。

Result: 证明LoRA梯度下降以O(1/log T)的速率收敛到平稳点，其中T是迭代次数。这是首次对原始LoRA算法进行非渐近收敛分析，无需Lipschitz光滑性假设。

Conclusion: 首次为原始LoRA梯度下降算法提供了非渐近收敛保证，填补了理论空白，为实际应用提供了理论基础，证明算法在缺乏经典Lipschitz光滑性条件下仍能收敛。

Abstract: The low-rank adaptation (LoRA) algorithm for fine-tuning large models has grown popular in recent years due to its remarkable performance and low computational requirements. LoRA trains two ``adapter" matrices that form a low-rank representation of the model parameters, thereby massively reducing the number of parameters that need to be updated at every step. Although LoRA is simple, its convergence is poorly understood due to the lack of Lipschitz smoothness, a key condition for classic convergence analyses. As a result, current theoretical results only consider asymptotic behavior or assume strong boundedness conditions which artificially enforce Lipschitz smoothness. In this work, we provide for the first time a non-asymptotic convergence analysis of the \textit{original LoRA gradient descent} algorithm, which reflects widespread practice, without such assumptions. Our work relies on three key steps: i) reformulating the problem in terms of the outer product of the stacked adapter matrices, ii) a modified descent lemma for the ``Lipschitz-like" reparametrized function, and iii) controlling the step size. With this approach, we prove that LoRA gradient descent converges to a stationary point at rate $O(\frac{1}{\log T})$, where $T$ is the number of iterations.

</details>


### [64] [LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform](https://arxiv.org/abs/2512.18266)
*Lizhi Ma,Yi-Xiang Hu,Yuke Wang,Yifang Zhao,Yihui Ren,Jian-Xiang Liao,Feng Wu,Xiang-Yang Li*

Main category: cs.LG

TL;DR: LeJOT是一个智能作业成本编排框架，利用机器学习预测执行时间，通过求解器优化模型进行实时资源分配，在Databricks平台上平均降低20%云计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着大数据技术发展，Databricks平台成为企业和研究机构的核心工具，但作业执行的操作成本不断上升。现有解决方案依赖静态配置或被动调整，无法适应动态工作负载变化，需要更智能的成本管理方法。

Method: LeJOT结合机器学习进行执行时间预测和基于求解器的优化模型进行实时资源分配。它主动预测工作负载需求，动态分配计算资源，在满足性能要求的同时最小化成本。

Result: 在实际Databricks工作负载上的实验表明，LeJOT在分钟级调度时间范围内平均降低20%云计算成本，优于传统的静态分配策略。

Conclusion: LeJOT为数据湖仓环境提供了一个可扩展且自适应的成本高效作业调度解决方案，能够有效管理动态工作负载并显著降低运营成本。

Abstract: With the rapid advancements in big data technologies, the Databricks platform has become a cornerstone for enterprises and research institutions, offering high computational efficiency and a robust ecosystem. However, managing the escalating operational costs associated with job execution remains a critical challenge. Existing solutions rely on static configurations or reactive adjustments, which fail to adapt to the dynamic nature of workloads. To address this, we introduce LeJOT, an intelligent job cost orchestration framework that leverages machine learning for execution time prediction and a solver-based optimization model for real-time resource allocation. Unlike conventional scheduling techniques, LeJOT proactively predicts workload demands, dynamically allocates computing resources, and minimizes costs while ensuring performance requirements are met. Experimental results on real-world Databricks workloads demonstrate that LeJOT achieves an average 20% reduction in cloud computing costs within a minute-level scheduling timeframe, outperforming traditional static allocation strategies. Our approach provides a scalable and adaptive solution for cost-efficient job scheduling in Data Lakehouse environments.

</details>


### [65] [FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation](https://arxiv.org/abs/2512.18275)
*Runze You,Shi Pu*

Main category: cs.LG

TL;DR: FedSUM算法家族支持任意客户端参与模式，无需额外数据异质性假设，通过两种延迟指标建模参与变化性，提供统一收敛保证


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法通常针对特定客户端参与模式设计，限制了在实际部署中的适用性。实际场景中客户端参与模式多样且不可预测，需要更通用的解决方案

Method: 提出FedSUM算法家族，包括FedSUM-B（基础版）、FedSUM（标准版）和FedSUM-CR（通信减少版）。使用最大延迟τ_max和平均延迟τ_avg两个指标建模客户端参与变化性，支持任意参与模式

Result: 提供了统一的收敛保证，证明FedSUM算法在各种参与模式下的有效性，扩展了联邦学习在实际场景中的适用性

Conclusion: FedSUM算法家族通过支持任意客户端参与模式，无需额外数据异质性假设，显著提高了联邦学习的实际部署能力，为现实世界应用提供了更灵活的解决方案

Abstract: Federated Learning (FL) methods are often designed for specific client participation patterns, limiting their applicability in practical deployments. We introduce the FedSUM family of algorithms, which supports arbitrary client participation without additional assumptions on data heterogeneity. Our framework models participation variability with two delay metrics, the maximum delay $τ_{\max}$ and the average delay $τ_{\text{avg}}$. The FedSUM family comprises three variants: FedSUM-B (basic version), FedSUM (standard version), and FedSUM-CR (communication-reduced version). We provide unified convergence guarantees demonstrating the effectiveness of our approach across diverse participation patterns, thereby broadening the applicability of FL in real-world scenarios.

</details>


### [66] [AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning](https://arxiv.org/abs/2512.18295)
*Xuling Zhang,Jindong Li,Yifei Zhang,Menglin Yang*

Main category: cs.LG

TL;DR: AL GNN是一种无需反向传播和回放缓冲区的持续图学习框架，通过解析学习理论实现递归最小二乘优化，在保护隐私的同时提升性能并减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有基于经验回放的持续图学习方法存在隐私风险、效率低下等问题，需要存储历史图数据来缓解灾难性遗忘，这在实际应用中存在显著限制。

Method: AL GNN基于解析学习理论，将学习过程建模为递归最小二乘优化，通过闭式分类器更新和正则化特征自相关矩阵来维护和更新模型知识，实现单次训练任务。

Result: 在多个动态图分类基准测试中，AL GNN达到或优于现有方法性能，如在CoraFull上平均性能提升10%，在Reddit上遗忘减少超过30%，同时由于无反向传播设计训练时间减少近50%。

Conclusion: AL GNN提供了一种高效、隐私保护的持续图学习解决方案，通过消除反向传播和回放缓冲区需求，在性能、遗忘减少和训练效率方面均取得显著改进。

Abstract: Continual graph learning (CGL) aims to enable graph neural networks to incrementally learn from a stream of graph structured data without forgetting previously acquired knowledge. Existing methods particularly those based on experience replay typically store and revisit past graph data to mitigate catastrophic forgetting. However, these approaches pose significant limitations, including privacy concerns, inefficiency. In this work, we propose AL GNN, a novel framework for continual graph learning that eliminates the need for backpropagation and replay buffers. Instead, AL GNN leverages principles from analytic learning theory to formulate learning as a recursive least squares optimization process. It maintains and updates model knowledge analytically through closed form classifier updates and a regularized feature autocorrelation matrix. This design enables efficient one pass training for each task, and inherently preserves data privacy by avoiding historical sample storage. Extensive experiments on multiple dynamic graph classification benchmarks demonstrate that AL GNN achieves competitive or superior performance compared to existing methods. For instance, it improves average performance by 10% on CoraFull and reduces forgetting by over 30% on Reddit, while also reducing training time by nearly 50% due to its backpropagation free design.

</details>


### [67] [Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings](https://arxiv.org/abs/2512.18309)
*Harsh Rathva,Ojas Srivastava,Pruthwik Mishra*

Main category: cs.LG

TL;DR: ESAI是一个多智能体强化学习理论框架，通过可微分内部对齐嵌入将安全约束直接嵌入智能体内部表示中，使用反事实推理预测外部伤害并通过注意力机制调节策略更新。


<details>
  <summary>Details</summary>
Motivation: 传统的外部奖励塑造或事后安全约束方法存在局限性，需要一种将安全对齐约束直接嵌入智能体内部表示的理论框架，以实现更本质的安全对齐机制。

Method: ESAI框架整合了四种机制：基于软参考分布的可微分反事实对齐惩罚、对齐加权的感知注意力、支持时序信用分配的赫布联想记忆，以及带有偏差缓解控制的相似性加权图扩散。

Result: 分析了在Lipschitz连续性和谱约束下有界内部嵌入的稳定性条件，讨论了计算复杂度，并研究了收缩行为和公平性-性能权衡等理论特性。

Conclusion: ESAI作为多智能体系统中可微分对齐机制的概念性贡献，提出了关于收敛保证、嵌入维度和扩展到高维环境等开放理论问题，实证评估留待未来工作。

Abstract: We introduce Embedded Safety-Aligned Intelligence (ESAI), a theoretical framework for multi-agent reinforcement learning that embeds alignment constraints directly into agents internal representations using differentiable internal alignment embeddings. Unlike external reward shaping or post-hoc safety constraints, internal alignment embeddings are learned latent variables that predict externalized harm through counterfactual reasoning and modulate policy updates toward harm reduction through attention and graph-based propagation.
  The ESAI framework integrates four mechanisms: differentiable counterfactual alignment penalties computed from soft reference distributions, alignment-weighted perceptual attention, Hebbian associative memory supporting temporal credit assignment, and similarity-weighted graph diffusion with bias mitigation controls. We analyze stability conditions for bounded internal embeddings under Lipschitz continuity and spectral constraints, discuss computational complexity, and examine theoretical properties including contraction behavior and fairness-performance tradeoffs.
  This work positions ESAI as a conceptual contribution to differentiable alignment mechanisms in multi-agent systems. We identify open theoretical questions regarding convergence guarantees, embedding dimensionality, and extension to high-dimensional environments. Empirical evaluation is left to future work.

</details>


### [68] [MoE Pathfinder: Trajectory-driven Expert Pruning](https://arxiv.org/abs/2512.18425)
*Xican Yang,Yuanhe Tian,Yan Song*

Main category: cs.LG

TL;DR: 提出基于专家激活轨迹的MoE剪枝方法，将专家选择转化为全局最优路径规划问题，实现跨层非均匀剪枝，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MoE架构在大语言模型中表现出色，但面临部署复杂和激活效率低的实际问题。现有专家剪枝方法依赖局部重要性指标和均匀层间剪枝，未能充分利用评估信号且忽视了专家在不同层的异质性贡献。

Method: 将MoE视为加权计算图，基于专家在层间的激活轨迹，将专家选择转化为全局最优路径规划问题。在轨迹层面整合重构误差、路由概率和激活强度等互补重要性信号，实现跨层非均匀专家保留。

Result: 实验表明，该方法在几乎所有任务上都优于现有大多数剪枝方法，取得了优越的剪枝性能。

Conclusion: 提出的基于专家激活轨迹的全局剪枝方法有效解决了现有方法的局限性，通过考虑专家在不同层的异质性贡献，实现了更优的MoE模型压缩和部署简化。

Abstract: Mixture-of-experts (MoE) architectures used in large language models (LLMs) achieve state-of-the-art performance across diverse tasks yet face practical challenges such as deployment complexity and low activation efficiency. Expert pruning has thus emerged as a promising solution to reduce computational overhead and simplify the deployment of MoE models. However, existing expert pruning approaches conventionally rely on local importance metrics and often apply uniform layer-wise pruning, leveraging only partial evaluation signals and overlooking the heterogeneous contributions of experts across layers. To address these limitations, we propose an expert pruning approach based on the trajectory of activated experts across layers, which treats MoE as a weighted computation graph and casts expert selection as a global optimal path planning problem. Within this framework, we integrate complementary importance signals from reconstruction error, routing probabilities, and activation strength at the trajectory level, which naturally yields non-uniform expert retention across layers. Experiments show that our approach achieves superior pruning performance on nearly all tasks compared with most existing approaches.

</details>


### [69] [On the Universality of Transformer Architectures; How Much Attention Is Enough?](https://arxiv.org/abs/2512.18445)
*Amirreza Abbasi,Mohsen Hooshmand*

Main category: cs.LG

TL;DR: 该论文综述了Transformer架构的普适性问题，回顾了近期进展，包括结构最小性和逼近率等架构改进，旨在澄清Transformer表达能力的现状，区分稳健与脆弱的理论保证，并指出未来理论研究的关键方向。


<details>
  <summary>Details</summary>
Motivation: Transformer在多个AI领域（如大语言模型、计算机视觉、强化学习）中至关重要，其重要性源于该架构相对于其他方案的普适性和可扩展性。然而，对Transformer普适性问题的系统理解仍然不足，需要澄清其表达能力的理论边界。

Method: 采用文献综述方法，系统性地：1）审视Transformer的普适性问题；2）回顾近期进展，包括结构最小性和逼近率等架构改进；3）调查最先进的研究进展，为理论和实践理解提供信息。

Result: 通过系统综述，澄清了当前对Transformer表达能力的认识，区分了稳健的理论保证与脆弱的理论保证，为理解Transformer的普适性提供了理论框架。

Conclusion: 该综述明确了Transformer普适性研究的现状，指出了未来理论研究的关键方向，有助于推动对Transformer架构表达能力的深入理解，并为实际应用提供理论指导。

Abstract: Transformers are crucial across many AI fields, such as large language models, computer vision, and reinforcement learning. This prominence stems from the architecture's perceived universality and scalability compared to alternatives. This work examines the problem of universality in Transformers, reviews recent progress, including architectural refinements such as structural minimality and approximation rates, and surveys state-of-the-art advances that inform both theoretical and practical understanding. Our aim is to clarify what is currently known about Transformers expressiveness, separate robust guarantees from fragile ones, and identify key directions for future theoretical research.

</details>


### [70] [Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring](https://arxiv.org/abs/2512.19309)
*Keivan Faghih Niresi,Jun Qing,Mengjie Zhao,Olga Fink*

Main category: cs.LG

TL;DR: 提出Time-Vertex Machine Learning (TVML)框架，整合图信号处理、时域分析和机器学习，用于结构健康监测中的传感器优化布置，减少冗余同时保持关键信息。


<details>
  <summary>Details</summary>
Motivation: 随着传感器网络规模和复杂性的增加，需要在降低部署成本的同时不损害监测质量。传统图信号处理方法往往忽略了结构行为的时域动态特性。

Method: 提出TVML框架，整合图信号处理(GSP)、时域分析和机器学习，通过识别代表性节点来实现可解释且高效的传感器布置，最小化冗余同时保留关键信息。

Result: 在两个桥梁数据集上评估了损伤检测和时变图信号重建任务，结果表明该方法能有效增强SHM系统，提供鲁棒、自适应且高效的传感器布置方案。

Conclusion: TVML框架为结构健康监测系统提供了一种新颖的传感器优化布置方法，通过同时考虑空间相关性和时域动态特性，实现了成本效益和监测质量的平衡。

Abstract: Structural Health Monitoring (SHM) plays a crucial role in maintaining the safety and resilience of infrastructure. As sensor networks grow in scale and complexity, identifying the most informative sensors becomes essential to reduce deployment costs without compromising monitoring quality. While Graph Signal Processing (GSP) has shown promise by leveraging spatial correlations among sensor nodes, conventional approaches often overlook the temporal dynamics of structural behavior. To overcome this limitation, we propose Time-Vertex Machine Learning (TVML), a novel framework that integrates GSP, time-domain analysis, and machine learning to enable interpretable and efficient sensor placement by identifying representative nodes that minimize redundancy while preserving critical information. We evaluate the proposed approach on two bridge datasets for damage detection and time-varying graph signal reconstruction tasks. The results demonstrate the effectiveness of our approach in enhancing SHM systems by providing a robust, adaptive, and efficient solution for sensor placement.

</details>


### [71] [NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic](https://arxiv.org/abs/2512.18453)
*Jayant Lohia*

Main category: cs.LG

TL;DR: NOVA框架通过数值优化发现稳定的分数插值点，解决了Winograd卷积在低精度计算中的数值不稳定问题，使大尺寸瓦片卷积在FP16/Int8下可用。


<details>
  <summary>Details</summary>
Motivation: 传统Winograd卷积使用整数插值点，随着瓦片尺寸增大（如F(6,3), F(8,3)），变换矩阵的条件数急剧恶化（F(8,3)达2×10^5），导致在FP16或Int8精度下完全失效，限制了高效卷积在现代低精度硬件上的应用。

Method: 提出NOVA框架，将Winograd插值点选择视为连续优化问题：1）在R^n-1流形上通过进化策略搜索；2）将候选点规整为简单有理数；3）通过符号验证保证正确性。该方法突破了传统整数插点的限制，发现了稳定的分数配置如{±5/6, ±7/6, ±3/5}。

Result: NOVA将F(8,3)的1D条件数提升415倍，2D卷积提升172,484倍。在FP16 ImageNet推理中，标准变换准确率崩溃至4.7%（VGG16），而NOVA点恢复至75-78%准确率，无需重训练、校准或学习参数即可恢复超过70个百分点。

Conclusion: NOVA发现的变换可作为即插即用替代方案，有效解锁了大尺寸瓦片Winograd卷积在下一代低精度硬件上的效率潜力，解决了数十年来整数插值传统带来的数值稳定性瓶颈。

Abstract: Winograd convolution is the standard algorithm for efficient inference, reducing arithmetic complexity by 2.25x for 3x3 kernels. However, it faces a critical barrier in the modern era of low precision computing: numerical instability. As tiles scale to maximize efficiency (e.g., F(6,3), F(8,3)), the condition numbers of standard integer based transforms explode, reaching kappa = 2 x 10^5 for F(8,3), rendering them unusable in FP16 or Int8. We introduce NOVA (Numerical Optimization of Vandermonde Arithmetic), a discovery framework that breaks the decades old convention of integer interpolation. Treating Winograd point selection as a continuous optimization problem, NOVA searches the manifold R^n-1 via Evolution Strategy, snaps candidates to simple rationals, and guarantees correctness via symbolic verification. This process uncovers a hidden landscape of stable, fractional configurations such as {+-5/6, +-7/6, +-3/5} that defy traditional vocabulary constraints. The impact is transformative: NOVA improves the conditioning of F(8,3) by 415x in 1D, which squares to a 172,484x improvement for 2D convolution. In real world FP16 ImageNet inference, where standard transforms collapse to random chance (e.g., 4.7 percent accuracy on VGG16), NOVA's points restore full accuracy (75 to 78 percent), recovering over 70 percentage points without retraining, calibration, or learned parameters. These discovered transforms act as drop in replacements, effectively unlocking the efficiency of large tile Winograd convolution for next generation hardware.

</details>


### [72] [Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs](https://arxiv.org/abs/2512.18454)
*David Graber,Victor Armegioiu,Rebecca Buller,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出基于扩散模型的概率OOD检测框架，用于处理结合连续几何和分类特征的复杂3D图数据，通过统一连续扩散和轨迹统计特征实现高效OOD识别。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在分布外数据上性能下降，但现有方法难以处理结合连续几何和分类特征的复杂3D图数据，需要可靠的OOD检测框架。

Method: 提出基于扩散模型的概率OOD检测框架：1）统一连续扩散处理3D坐标和离散特征；2）分类特征通过交叉熵嵌入连续空间；3）使用概率流ODE产生样本对数似然；4）结合多尺度轨迹统计特征（路径弯曲度、流刚度、向量场不稳定性）增强检测。

Result: 在蛋白质-配体复合物数据集上验证：1）PF-ODE似然能识别保留的蛋白质家族为OOD；2）与独立结合亲和力模型预测误差强相关；3）结合轨迹统计特征显著提升检测灵敏度，优于仅使用似然的基线方法。

Conclusion: 该框架为几何深度学习提供了无标签的OOD量化工作流，通过统一连续扩散和轨迹统计特征，实现了对复杂3D图数据的可靠分布外检测。

Abstract: Predictive machine learning models generally excel on in-distribution data, but their performance degrades on out-of-distribution (OOD) inputs. Reliable deployment therefore requires robust OOD detection, yet this is particularly challenging for irregular 3D graphs that combine continuous geometry with categorical identities and are unordered by construction. Here, we present a probabilistic OOD detection framework for complex 3D graph data built on a diffusion model that learns a density of the training distribution in a fully unsupervised manner. A key ingredient we introduce is a unified continuous diffusion over both 3D coordinates and discrete features: categorical identities are embedded in a continuous space and trained with cross-entropy, while the corresponding diffusion score is obtained analytically via posterior-mean interpolation from predicted class probabilities. This yields a single self-consistent probability-flow ODE (PF-ODE) that produces per-sample log-likelihoods, providing a principled typicality score for distribution shift. We validate the approach on protein-ligand complexes and construct strict OOD datasets by withholding entire protein families from training. PF-ODE likelihoods identify held-out families as OOD and correlate strongly with prediction errors of an independent binding-affinity model (GEMS), enabling a priori reliability estimates on new complexes. Beyond scalar likelihoods, we show that multi-scale PF-ODE trajectory statistics - including path tortuosity, flow stiffness, and vector-field instability - provide complementary OOD information. Modeling the joint distribution of these trajectory features yields a practical, high-sensitivity detector that improves separation over likelihood-only baselines, offering a label-free OOD quantification workflow for geometric deep learning.

</details>


### [73] [Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review](https://arxiv.org/abs/2512.18466)
*Oraib Almegdadi,João Marcelino,Sarah Fakhreddine,João Manso,Nuno C. Marques*

Main category: cs.LG

TL;DR: 本文综述了自组织映射（SOM）在湖泊水库水质评估中的应用，探讨了该无监督AI技术如何应对数据稀疏性、异质性和非线性关系等挑战，并总结了其在生态评估、营养状态分类、藻华监测等方面的应用。


<details>
  <summary>Details</summary>
Motivation: 可持续水质对生态平衡和水安全至关重要，但湖泊水库评估面临数据稀疏、异质性和参数间非线性关系等挑战。随着环境数据（现场传感器、遥感影像、物联网技术、历史记录）的日益丰富，需要有效分析复杂数据集的方法，特别是在标记数据有限的情况下。

Method: 本文采用文献综述方法，系统分析自组织映射（SOM）这一无监督AI技术在水质评估中的应用。重点研究了参数选择、时空采样策略和聚类方法，探讨SOM如何处理多维数据并揭示隐藏模式。

Result: SOM在分析复杂数据集方面表现出色，特别是在标记数据有限的情况下。它能够实现高维数据可视化，促进隐藏生态模式的检测，并识别不同水质指标间的关键相关性。该技术在生态评估、营养状态分类、藻华监测和集水区影响评估中展现出多功能性。

Conclusion: SOM为湖泊水库水质评估提供了有效的无监督分析方法，能够处理多维复杂数据并揭示隐藏模式。该综述为现有方法提供了全面见解，支持未来研究和实际应用，有助于改善湖泊水库生态系统的监测和可持续管理。

Abstract: Sustainable water quality underpins ecological balance and water security. Assessing and managing lakes and reservoirs is difficult due to data sparsity, heterogeneity, and nonlinear relationships among parameters. This review examines how Self-Organizing Map (SOM), an unsupervised AI technique, is applied to water quality assessment. It synthesizes research on parameter selection, spatial and temporal sampling strategies, and clustering approaches. Emphasis is placed on how SOM handles multidimensional data and uncovers hidden patterns to support effective water management. The growing availability of environmental data from in-situ sensors, remote sensing imagery, IoT technologies, and historical records has significantly expanded analytical opportunities in environmental monitoring. SOM has proven effective in analysing complex datasets, particularly when labelled data are limited or unavailable. It enables high-dimensional data visualization, facilitates the detection of hidden ecological patterns, and identifies critical correlations among diverse water quality indicators. This review highlights SOMs versatility in ecological assessments, trophic state classification, algal bloom monitoring, and catchment area impact evaluations. The findings offer comprehensive insights into existing methodologies, supporting future research and practical applications aimed at improving the monitoring and sustainable management of lake and reservoir ecosystems.

</details>


### [74] [The Geometry of Abstraction: Continual Learning via Recursive Quotienting](https://arxiv.org/abs/2512.18471)
*Xin Li*

Main category: cs.LG

TL;DR: 论文提出递归度量收缩作为解决持续学习中平坦流形问题的几何方法，通过拓扑形变实现有界容量嵌入和线性可分性，解决灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 固定维度空间中的持续学习系统面临平坦流形问题：欧几里得空间中的线性轨迹导致测地距离随时间线性增长，迫使覆盖数发散，在固定维度硬件中必然导致轨迹重叠和灾难性干扰。

Method: 提出递归度量收缩的几何解决方案，将抽象形式化为拓扑形变：商映射在验证的时间邻域内收缩度量张量，使局部子流形直径趋近于零。通过正交流形划分实现稳定性。

Result: 1) 有界容量定理：递归商映射允许任意长轨迹嵌入有界表示体积；2) 拓扑坍缩可分性定理：递归商化使非线性可分时序序列在极限下线性可分；3) 奇偶划分稳定性定理：正交流形划分确保存储记忆的稳定性。

Conclusion: 递归度量收缩为持续学习提供了几何解决方案，揭示了神经网络中的token在物理上可实现为连接时间流形中远点的奇点或虫洞，解决了灾难性遗忘问题。

Abstract: Continual learning systems operating in fixed-dimensional spaces face a fundamental geometric barrier: the flat manifold problem. When experience is represented as a linear trajectory in Euclidean space, the geodesic distance between temporal events grows linearly with time, forcing the required covering number to diverge. In fixed-dimensional hardware, this volume expansion inevitably forces trajectory overlap, manifesting as catastrophic interference. In this work, we propose a geometric resolution to this paradox based on Recursive Metric Contraction. We formalize abstraction not as symbolic grouping, but as a topological deformation: a quotient map that collapses the metric tensor within validated temporal neighborhoods, effectively driving the diameter of local sub-manifolds to zero. We substantiate our framework with four rigorous results. First, the Bounded Capacity Theorem establishes that recursive quotient maps allow the embedding of arbitrarily long trajectories into bounded representational volumes, trading linear metric growth for logarithmic topological depth. Second, the Topological Collapse Separability Theorem, derived via Urysohn's Lemma, proves that recursive quotienting renders non-linearly separable temporal sequences linearly separable in the limit, bypassing the need for infinite-dimensional kernel projections. Third, the Parity-Partitioned Stability Theorem solves the catastrophic forgetting problem by proving that if the state space is partitioned into orthogonal flow and scaffold manifolds, the metric deformations of active learning do not disturb the stability of stored memories. Our analysis reveals that tokens in neural architectures are physically realizable as singularities or wormholes, regions of extreme positive curvature that bridge distant points in the temporal manifold.

</details>


### [75] [APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification](https://arxiv.org/abs/2512.18473)
*Khaled Berkani*

Main category: cs.LG

TL;DR: APC-GNN++是一种自适应患者中心图神经网络，用于糖尿病分类，通过上下文感知边注意力、置信度引导的特征融合和邻域一致性正则化来捕捉患者间临床关系，并采用小图方法处理新患者，在真实糖尿病数据集上优于传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够更好捕捉患者间临床意义关系、处理新患者且具有可解释性的糖尿病分类模型，为医疗专业人员提供交互式工具。

Method: 1. 上下文感知边注意力机制；2. 置信度引导的节点特征与图表示融合；3. 邻域一致性正则化；4. 小图方法处理新患者（利用最近邻而不重新训练全局模型）；5. 集成到Tkinter GUI中供医疗专业人员使用。

Result: 在阿尔及利亚地区医院收集的真实糖尿病数据集上，APC-GNN++在测试准确率和宏观F1分数上均优于传统机器学习模型（MLP、随机森林、XGBoost）和普通GCN。节点级置信度分析揭示了模型如何在不同患者群体中平衡自身信息和基于图的证据。

Conclusion: APC-GNN++通过自适应患者中心设计有效提升了糖尿病分类性能，提供了可解释的患者中心洞察，并通过小图方法实现了对新患者的实时可解释预测，系统已集成到GUI中供医疗专业人员交互使用。

Abstract: We propose APC-GNN++, an adaptive patient-centric Graph Neural Network for diabetes classification. Our model integrates context-aware edge attention, confidence-guided blending of node features and graph representations, and neighborhood consistency regularization to better capture clinically meaningful relationships between patients. To handle unseen patients, we introduce a mini-graph approach that leverages the nearest neighbors of the new patient, enabling real-time explainable predictions without retraining the global model. We evaluate APC-GNN++ on a real-world diabetes dataset collected from a regional hospital in Algeria and show that it outperforms traditional machine learning models (MLP, Random Forest, XGBoost) and a vanilla GCN, achieving higher test accuracy and macro F1- score. The analysis of node-level confidence scores further reveals how the model balances self-information and graph-based evidence across different patient groups, providing interpretable patient-centric insights. The system is also embedded in a Tkinter-based graphical user interface (GUI) for interactive use by healthcare professionals .

</details>


### [76] [Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts](https://arxiv.org/abs/2512.18522)
*Hatim M. E. Geli,Islam Omar,Mona Y. Elshinawy,David W. DuBios,Lara Prehodko,Kelly H Smith,Abdel-Hameed A. Badawy*

Main category: cs.LG

TL;DR: 该研究应用机器学习技术，结合干旱指数和历史干旱影响记录，生成短期干旱影响预测，为干旱早期预警系统提供支持。


<details>
  <summary>Details</summary>
Motivation: 干旱是影响生态和人类系统的复杂自然灾害，近年来干旱严重程度、频率和持续时间增加，需要有效的监测和缓解策略。预测干旱影响（而非仅干旱条件）可为早期预警系统和主动决策提供机会。

Method: 使用机器学习技术将干旱指数（DSCI和ESI）与历史干旱影响记录（2005-2024年）结合，采用eXtreme Gradient Boosting (XGBoost)模型，利用前8周数据预测未来8周的干旱影响，重点关注新墨西哥州的县级和州级预测。

Result: 火灾和救援影响预测准确率最高，其次是农业和水资源影响，而植物和社会影响预测变异性较大。模型成功为大多数影响类别生成了提前8周的预测，支持新墨西哥州生态干旱信息通信系统（EcoDri）的开发。

Conclusion: 该研究展示了机器学习在干旱影响预测中的潜力，可为利益相关者、土地管理者和决策者制定更有效的干旱缓解和适应策略提供支持，并具有在类似干旱易发地区广泛应用的潜力。

Abstract: Drought is a complex natural hazard that affects ecological and human systems, often resulting in substantial environmental and economic losses. Recent increases in drought severity, frequency, and duration underscore the need for effective monitoring and mitigation strategies. Predicting drought impacts rather than drought conditions alone offers opportunities to support early warning systems and proactive decision-making. This study applies machine learning techniques to link drought indices with historical drought impact records (2005:2024) to generate short-term impact forecasts. By addressing key conceptual and data-driven challenges regarding temporal scale and impact quantification, the study aims to improve the predictability of drought impacts at actionable lead times. The Drought Severity and Coverage Index (DSCI) and the Evaporative Stress Index (ESI) were combined with impact data from the Drought Impact Reporter (DIR) to model and forecast weekly drought impacts. Results indicate that Fire and Relief impacts were predicted with the highest accuracy, followed by Agriculture and Water, while forecasts for Plants and Society impacts showed greater variability. County and state level forecasts for New Mexico were produced using an eXtreme Gradient Boosting (XGBoost) model that incorporated both DSCI and ESI. The model successfully generated forecasts up to eight weeks in advance using the preceding eight weeks of data for most impact categories. This work supports the development of an Ecological Drought Information Communication System (EcoDri) for New Mexico and demonstrates the potential for broader application in similar drought-prone regions. The findings can aid stakeholders, land managers, and decision-makers in developing and implementing more effective drought mitigation and adaptation strategies.

</details>


### [77] [Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study](https://arxiv.org/abs/2512.18524)
*Janek Dyer,Jagdeep Ahluwalia,Javad Zarrin*

Main category: cs.LG

TL;DR: 该研究提出了一种结合图神经网络与图论特征的混合方法，用于区分五种生成图模型，在大型合成数据集上实现了高达98.5%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 区分生成图模型对于理解合成图和真实世界结构的复杂模式至关重要。虽然图神经网络在图分类任务中效果显著，但很少有研究探索其与可解释图论特征的结合。

Method: 生成包含五种代表性生成图家族的大型合成数据集，提取全面的节点和图级特征，使用随机森林进行特征选择，将特征集成到六种GNN架构中（GCN、GAT、GATv2、GIN、GraphSAGE、GTN），并使用Optuna进行超参数优化。

Result: GraphSAGE和GTN达到最高的分类性能（98.5%准确率），t-SNE和UMAP可视化显示强类别分离。GCN和GIN表现良好，GAT类模型因捕获全局结构能力有限而表现较差。SVM基线证实了消息传递功能对性能提升的重要性。

Conclusion: 结合图神经网络与图论特征的混合方法能有效区分生成图模型，GraphSAGE和GTN表现最佳，消息传递机制对分类性能至关重要，为理解复杂图结构提供了有效工具。

Abstract: The ability to discriminate between generative graph models is critical to understanding complex structural patterns in both synthetic graphs and the real-world structures that they emulate. While Graph Neural Networks (GNNs) have seen increasing use to great effect in graph classification tasks, few studies explore their integration with interpretable graph theoretic features. This paper investigates the classification of synthetic graph families using a hybrid approach that combines GNNs with engineered graph-theoretic features. We generate a large and structurally diverse synthetic dataset comprising graphs from five representative generative families, Erdos-Renyi, Watts-Strogatz, Barab'asi-Albert, Holme-Kim, and Stochastic Block Model. These graphs range in size up to 1x10^4 nodes, containing up to 1.1x10^5 edges. A comprehensive range of node and graph level features is extracted for each graph and pruned using a Random Forest based feature selection pipeline. The features are integrated into six GNN architectures: GCN, GAT, GATv2, GIN, GraphSAGE and GTN. Each architecture is optimised for hyperparameter selection using Optuna. Finally, models were compared against a baseline Support Vector Machine (SVM) trained solely on the handcrafted features. Our evaluation demonstrates that GraphSAGE and GTN achieve the highest classification performance, with 98.5% accuracy, and strong class separation evidenced by t-SNE and UMAP visualisations. GCN and GIN also performed well, while GAT-based models lagged due to limitations in their ability to capture global structures. The SVM baseline confirmed the importance of the message passing functionality for performance gains and meaningful class separation.

</details>


### [78] [Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing](https://arxiv.org/abs/2512.18575)
*Effiong Blessing,Chiung-Yi Tseng,Somshubhra Roy,Junaid Rehman,Isaac Nkrumah*

Main category: cs.LG

TL;DR: 首次对SNN记忆机制进行跨模态消融研究，发现Hopfield网络在视觉任务表现优异(97.68%)但在听觉任务较差(76.15%)，而监督对比学习表现更均衡，揭示了记忆机制的任务特异性而非普适性。


<details>
  <summary>Details</summary>
Motivation: 尽管记忆增强型SNN有望实现节能神经形态计算，但其在不同感官模态间的泛化能力尚未被探索。本研究旨在填补这一空白，系统评估不同记忆机制在跨模态任务中的表现。

Method: 对五种架构进行系统评估：Hopfield网络、分层门控循环网络(HGRN)和监督对比学习(SCL)，在视觉(N-MNIST)和听觉(SHD)神经形态数据集上进行跨模态消融研究，包括联合多模态训练和定量记忆痕迹分析。

Result: 发现显著的模态依赖性：Hopfield网络视觉任务97.68%但听觉仅76.15%(21.53点差距)，SCL表现更均衡(96.72%视觉，82.16%听觉，14.56点差距)。HGRN联合多模态训练达到94.41%视觉和79.37%听觉准确率。记忆痕迹分析显示跨模态对齐度低(0.038相似度)。

Conclusion: 记忆机制表现出任务特异性而非普适性，需要针对特定模态进行优化。研究首次为神经形态系统中的模态特异性记忆优化提供了实证证据，实现了603倍于传统神经网络的能量效率。

Abstract: Memory-augmented spiking neural networks (SNNs) promise energy-efficient neuromorphic computing, yet their generalization across sensory modalities remains unexplored. We present the first comprehensive cross-modal ablation study of memory mechanisms in SNNs, evaluating Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL) across visual (N-MNIST) and auditory (SHD) neuromorphic datasets. Our systematic evaluation of five architectures reveals striking modality-dependent performance patterns: Hopfield networks achieve 97.68% accuracy on visual tasks but only 76.15% on auditory tasks (21.53 point gap), revealing severe modality-specific specialization, while SCL demonstrates more balanced cross-modal performance (96.72% visual, 82.16% audio, 14.56 point gap). These findings establish that memory mechanisms exhibit task-specific benefits rather than universal applicability. Joint multi-modal training with HGRN achieves 94.41% visual and 79.37% audio accuracy (88.78% average), matching parallel HGRN performance through unified deployment. Quantitative engram analysis confirms weak cross-modal alignment (0.038 similarity), validating our parallel architecture design. Our work provides the first empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving 603x energy efficiency over traditional neural networks.

</details>


### [79] [SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models](https://arxiv.org/abs/2512.18583)
*Pengcheng Li,Qiang Fang,Tong Zhao,Yixing Lan,Xin Xu*

Main category: cs.LG

TL;DR: SD2AIL利用扩散模型生成合成演示数据增强对抗模仿学习，通过优先专家演示回放策略提升性能


<details>
  <summary>Details</summary>
Motivation: 对抗模仿学习通常需要大量专家演示，但在某些场景中收集这些演示很困难。扩散模型在数据生成方面的成功启发了使用合成演示来增强专家数据的方法

Method: 1. 在判别器中使用扩散模型生成合成演示作为伪专家数据；2. 引入优先专家演示回放策略(PEDR)，从大量(伪)专家演示中选择最有价值的演示进行回放

Result: 在模拟任务中证明了方法的有效性和鲁棒性。在Hopper任务中，平均回报达到3441，比最先进方法提升了89

Conclusion: SD2AIL通过扩散模型生成合成演示和优先回放策略，有效解决了对抗模仿学习中专家演示不足的问题，显著提升了性能

Abstract: Adversarial Imitation Learning (AIL) is a dominant framework in imitation learning that infers rewards from expert demonstrations to guide policy optimization. Although providing more expert demonstrations typically leads to improved performance and greater stability, collecting such demonstrations can be challenging in certain scenarios. Inspired by the success of diffusion models in data generation, we propose SD2AIL, which utilizes synthetic demonstrations via diffusion models. We first employ a diffusion model in the discriminator to generate synthetic demonstrations as pseudo-expert data that augment the expert demonstrations. To selectively replay the most valuable demonstrations from the large pool of (pseudo-) expert demonstrations, we further introduce a prioritized expert demonstration replay strategy (PEDR). The experimental results on simulation tasks demonstrate the effectiveness and robustness of our method. In particular, in the Hopper task, our method achieves an average return of 3441, surpassing the state-of-the-art method by 89. Our code will be available at https://github.com/positron-lpc/SD2AIL.

</details>


### [80] [Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows](https://arxiv.org/abs/2512.18595)
*Runze Mao,Rui Zhang,Xuan Bai,Tianhao Wu,Teng Zhang,Zhenyi Chen,Minqi Lin,Bocheng Zeng,Yangchen Xu,Yingxuan Xiang,Haoze Zhang,Shubham Goswami,Pierre A. Dawe,Yifan Xu,Zhenhua An,Mengtao Yan,Xiaoyi Lu,Yi Wang,Rongbo Bai,Haobu Gao,Xiaohang Fang,Han Li,Hao Sun,Zhi X. Chen*

Main category: cs.LG

TL;DR: REALM框架系统评估神经代理模型在真实多物理场问题中的表现，揭示了当前模型在复杂场景下的局限性


<details>
  <summary>Details</summary>
Motivation: 当前神经代理模型评估过于依赖简化低维代理问题，无法暴露模型在真实多物理场场景中的脆弱性，需要建立更严格的基准测试框架

Method: 提出REALM基准框架，包含11个高保真数据集（从经典多物理场问题到复杂推进和火灾安全场景），标准化端到端训练评估协议，包含多物理场感知预处理和鲁棒推演策略

Result: 系统评估了十多个代表性代理模型家族，发现三个稳健趋势：1) 受维度、刚度和网格不规则性共同控制的缩放障碍；2) 性能主要由架构归纳偏置而非参数数量控制；3) 名义精度指标与物理可信行为之间存在持续差距

Conclusion: REALM暴露了当前神经代理模型在真实多物理场流动中的局限性，为开发下一代物理感知架构提供了严格的测试平台

Abstract: Predicting multiphysics dynamics is computationally expensive and challenging due to the severe coupling of multi-scale, heterogeneous physical processes. While neural surrogates promise a paradigm shift, the field currently suffers from an "illusion of mastery", as repeatedly emphasized in top-tier commentaries: existing evaluations overly rely on simplified, low-dimensional proxies, which fail to expose the models' inherent fragility in realistic regimes. To bridge this critical gap, we present REALM (REalistic AI Learning for Multiphysics), a rigorous benchmarking framework designed to test neural surrogates on challenging, application-driven reactive flows. REALM features 11 high-fidelity datasets spanning from canonical multiphysics problems to complex propulsion and fire safety scenarios, alongside a standardized end-to-end training and evaluation protocol that incorporates multiphysics-aware preprocessing and a robust rollout strategy. Using this framework, we systematically benchmark over a dozen representative surrogate model families, including spectral operators, convolutional models, Transformers, pointwise operators, and graph/mesh networks, and identify three robust trends: (i) a scaling barrier governed jointly by dimensionality, stiffness, and mesh irregularity, leading to rapidly growing rollout errors; (ii) performance primarily controlled by architectural inductive biases rather than parameter count; and (iii) a persistent gap between nominal accuracy metrics and physically trustworthy behavior, where models with high correlations still miss key transient structures and integral quantities. Taken together, REALM exposes the limits of current neural surrogates on realistic multiphysics flows and offers a rigorous testbed to drive the development of next-generation physics-aware architectures.

</details>


### [81] [EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture](https://arxiv.org/abs/2512.18596)
*Quanxi Zhou,Wencan Mao,Yilei Liang,Manabu Tsukada,Yunling Liu,Jon Crowcroft*

Main category: cs.LG

TL;DR: 提出EIA-SEC框架解决多无人机智能农业系统中的轨迹规划问题，通过精英模仿和共享集成批评器提升性能


<details>
  <summary>Details</summary>
Motivation: 无线通信技术推动智能农业发展，无人机在其中扮演多功能角色。多无人机系统需要协同执行数据收集、图像采集和通信任务，但轨迹规划面临挑战。

Method: 将多无人机轨迹规划建模为马尔可夫决策过程，提出精英模仿演员-共享集成批评器（EIA-SEC）框架：智能体从精英智能体自适应学习以减少试错成本，共享集成批评器与各智能体的本地批评器协作，确保无偏目标值估计并防止高估。

Result: 实验结果表明，EIA-SEC在奖励性能、训练稳定性和收敛速度方面优于最先进的基线方法。

Conclusion: EIA-SEC框架有效解决了多无人机智能农业系统的轨迹规划问题，通过精英模仿学习和共享批评器机制提升了多智能体强化学习的性能。

Abstract: The widespread application of wireless communication technology has promoted the development of smart agriculture, where unmanned aerial vehicles (UAVs) play a multifunctional role. We target a multi-UAV smart agriculture system where UAVs cooperatively perform data collection, image acquisition, and communication tasks. In this context, we model a Markov decision process to solve the multi-UAV trajectory planning problem. Moreover, we propose a novel Elite Imitation Actor-Shared Ensemble Critic (EIA-SEC) framework, where agents adaptively learn from the elite agent to reduce trial-and-error costs, and a shared ensemble critic collaborates with each agent's local critic to ensure unbiased objective value estimates and prevent overestimation. Experimental results demonstrate that EIA-SEC outperforms state-of-the-art baselines in terms of reward performance, training stability, and convergence speed.

</details>


### [82] [Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning](https://arxiv.org/abs/2512.18604)
*Wencan Mao,Quanxi Zhou,Tomas Couso Coddou,Manabu Tsukada,Yunling Liu,Yusheng Ji*

Main category: cs.LG

TL;DR: 本文提出了一种基于模仿的三重深度Q网络（ITDQN）算法，用于解决无人机在智慧农业中的轨迹规划问题，该算法通过精英模仿机制降低探索成本，并使用调解器Q网络加速训练并提高性能。


<details>
  <summary>Details</summary>
Motivation: 无人机在智慧农业中具有广阔应用前景，可用于杂草检测识别和无线传感器数据收集。然而，由于环境高度不确定性、部分观测以及无人机电池容量有限，轨迹规划面临挑战。

Method: 将轨迹规划问题建模为马尔可夫决策过程（MDP），采用多智能体强化学习（MARL）求解。提出ITDQN算法，包含精英模仿机制降低探索成本，并在双深度Q网络（DDQN）基础上增加调解器Q网络以加速训练并提升性能。

Result: 在模拟和真实环境中的实验结果表明该方案有效。ITDQN算法相比DDQN在杂草识别率上提升4.43%，在数据收集率上提升6.94%。

Conclusion: 提出的ITDQN算法能有效解决无人机在智慧农业中的轨迹规划问题，通过精英模仿和调解器Q网络机制显著提升了性能表现。

Abstract: Unmanned aerial vehicles (UAVs) have emerged as a promising auxiliary platform for smart agriculture, capable of simultaneously performing weed detection, recognition, and data collection from wireless sensors. However, trajectory planning for UAV-based smart agriculture is challenging due to the high uncertainty of the environment, partial observations, and limited battery capacity of UAVs. To address these issues, we formulate the trajectory planning problem as a Markov decision process (MDP) and leverage multi-agent reinforcement learning (MARL) to solve it. Furthermore, we propose a novel imitation-based triple deep Q-network (ITDQN) algorithm, which employs an elite imitation mechanism to reduce exploration costs and utilizes a mediator Q-network over a double deep Q-network (DDQN) to accelerate and stabilize training and improve performance. Experimental results in both simulated and real-world environments demonstrate the effectiveness of our solution. Moreover, our proposed ITDQN outperforms DDQN by 4.43\% in weed recognition rate and 6.94\% in data collection rate.

</details>


### [83] [The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation](https://arxiv.org/abs/2512.18607)
*Huiqi Deng,Qihan Ren,Zhuofan Chen,Zhenyuan Cui,Wen Shen,Peng Zhang,Hongbin Pei,Quanshi Zhang*

Main category: cs.LG

TL;DR: DNNs存在普遍的中阶交互瓶颈：容易学习低阶和高阶交互，但持续欠表示中阶交互，这源于中阶交互的高上下文变异性导致梯度方差大、难以学习


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络（DNNs）能表示何种合作结构仍是一个基础但尚未充分理解的问题。本研究将交互作为此类结构的基本单元，探究DNNs在不同上下文复杂度水平下如何编码交互，以及这些微观交互模式如何塑造宏观表示能力

Method: 使用多阶交互量化上下文复杂度，每阶反映评估变量对联合交互效用所需的上下文信息量。基于此进行分层分析，包括：(i) 实证发现普遍交互瓶颈；(ii) 理论解释中阶交互的高上下文变异性导致高梯度方差；(iii) 通过引入损失函数调节瓶颈，引导模型关注特定阶数的交互

Result: 实证发现DNNs普遍存在中阶交互瓶颈；理论证明中阶交互具有最高上下文变异性，导致大梯度方差；通过损失函数可调节瓶颈；微观交互结构与宏观表示行为相关：低阶强调模型展现更强泛化性和鲁棒性，高阶强调模型具有更强结构建模和拟合能力

Conclusion: 揭示了现代DNNs固有的表示偏置，建立了交互阶数作为解释和指导深度表示的强大视角，为理解和调控神经网络表示能力提供了新框架

Abstract: Understanding what kinds of cooperative structures deep neural networks (DNNs) can represent remains a fundamental yet insufficiently understood problem. In this work, we treat interactions as the fundamental units of such structure and investigate a largely unexplored question: how DNNs encode interactions under different levels of contextual complexity, and how these microscopic interaction patterns shape macroscopic representation capacity. To quantify this complexity, we use multi-order interactions [57], where each order reflects the amount of contextual information required to evaluate the joint interaction utility of a variable pair. This formulation enables a stratified analysis of cooperative patterns learned by DNNs. Building on this formulation, we develop a comprehensive study of interaction structure in DNNs. (i) We empirically discover a universal interaction bottleneck: across architectures and tasks, DNNs easily learn low-order and high-order interactions but consistently under-represent mid-order ones. (ii) We theoretically explain this bottleneck by proving that mid-order interactions incur the highest contextual variability, yielding large gradient variance and making them intrinsically difficult to learn. (iii) We further modulate the bottleneck by introducing losses that steer models toward emphasizing interactions of selected orders. Finally, we connect microscopic interaction structures with macroscopic representational behavior: low-order-emphasized models exhibit stronger generalization and robustness, whereas high-order-emphasized models demonstrate greater structural modeling and fitting capability. Together, these results uncover an inherent representational bias in modern DNNs and establish interaction order as a powerful lens for interpreting and guiding deep representations.

</details>


### [84] [The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss](https://arxiv.org/abs/2512.18610)
*Rongyao Cai,Yuxi Wan,Kexin Zhang,Ming Jin,Hao Wang,Zhiqiang Ge,Daoyi Dong,Yong Liu,Qingsong Wen*

Main category: cs.LG

TL;DR: 论文提出期望优化偏差(EOB)理论，量化点损失函数在时间序列建模中的偏差，并提出基于序列长度缩减和结构正交化的去偏方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列建模中，基于点损失函数（如MSE）的优化方法存在根本缺陷，因为它们依赖于点独立同分布(i.i.d.)假设，忽视了时间序列的因果时间结构。虽然这个问题逐渐受到关注，但缺乏正式的理论基础。

Method: 1. 在协方差平稳性下，从第一性原理分析期望优化偏差(EOB)，将其信息论形式化为真实联合分布与错误i.i.d.对应分布之间的差异。2. 推导线性和非线性系统中非确定性EOB的闭式量化。3. 提出基于序列长度缩减和结构正交化的去偏程序，通过DFT或DWT同时实现这两个原则。4. 提出新颖的调和ℓ_p范数框架来修正高方差序列的梯度问题。

Result: 1. 揭示了基本范式悖论：时间序列越确定性和结构化，点损失函数造成的偏差越严重。2. 证明EOB是内在数据属性，仅由序列长度和提出的结构信噪比(SSNR)决定。3. 广泛实验验证了EOB理论的普适性和去偏程序的优越性能。

Conclusion: 论文建立了时间序列建模中点损失函数偏差的正式理论基础，提出了理论诊断和原则性去偏方法，为解决时间序列优化中的基本问题提供了新视角和实用解决方案。

Abstract: Optimizing time series models via point-wise loss functions (e.g., MSE) relying on a flawed point-wise independent and identically distributed (i.i.d.) assumption that disregards the causal temporal structure, an issue with growing awareness yet lacking formal theoretical grounding. Focusing on the core independence issue under covariance stationarity, this paper aims to provide a first-principles analysis of the Expectation of Optimization Bias (EOB), formalizing it information-theoretically as the discrepancy between the true joint distribution and its flawed i.i.d. counterpart. Our analysis reveals a fundamental paradigm paradox: the more deterministic and structured the time series, the more severe the bias by point-wise loss function. We derive the first closed-form quantification for the non-deterministic EOB across linear and non-linear systems, and prove EOB is an intrinsic data property, governed exclusively by sequence length and our proposed Structural Signal-to-Noise Ratio (SSNR). This theoretical diagnosis motivates our principled debiasing program that eliminates the bias through sequence length reduction and structural orthogonalization. We present a concrete solution that simultaneously achieves both principles via DFT or DWT. Furthermore, a novel harmonized $\ell_p$ norm framework is proposed to rectify gradient pathologies of high-variance series. Extensive experiments validate EOB Theory's generality and the superior performance of debiasing program.

</details>


### [85] [ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs](https://arxiv.org/abs/2512.18633)
*Han-Seul Jeong,Youngjoon Park,Hyungseok Song,Woohyung Lim*

Main category: cs.LG

TL;DR: ARC框架通过解耦属性表示实现跨VRP问题的学习，将属性分解为固有语义嵌入和上下文交互嵌入，利用类比一致性实现零样本泛化


<details>
  <summary>Details</summary>
Motivation: 现实世界中的车辆路径问题具有多样化属性，需要能够跨问题变体高效泛化的学习方法

Method: 提出ARC框架，将属性表示解耦为固有属性嵌入（IAE）和上下文交互嵌入（CIE），通过强制嵌入空间中的类比一致性来保持属性语义转换在不同问题上下文中的不变性

Result: 在分布内、零样本泛化、少样本适应和真实世界基准测试中均达到最先进性能

Conclusion: ARC通过解耦属性表示和类比一致性学习，能够跨VRP问题变体高效泛化，支持零样本组合和知识重用

Abstract: Vehicle Routing Problems (VRPs) with diverse real-world attributes have driven recent interest in cross-problem learning approaches that efficiently generalize across problem variants. We propose ARC (Attribute Representation via Compositional Learning), a cross-problem learning framework that learns disentangled attribute representations by decomposing them into two complementary components: an Intrinsic Attribute Embedding (IAE) for invariant attribute semantics and a Contextual Interaction Embedding (CIE) for attribute-combination effects. This disentanglement is achieved by enforcing analogical consistency in the embedding space to ensure the semantic transformation of adding an attribute (e.g., a length constraint) remains invariant across different problem contexts. This enables our model to reuse invariant semantics across trained variants and construct representations for unseen combinations. ARC achieves state-of-the-art performance across in-distribution, zero-shot generalization, few-shot adaptation, and real-world benchmarks.

</details>


### [86] [Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2512.18670)
*Xue Yang,Michael Schukat,Junlin Lu,Patrick Mannion,Karl Mason,Enda Howley*

Main category: cs.LG

TL;DR: 提出DGCRL方法，通过外部自演化的演示库直接指导RL探索和适应，解决持续强化学习中的稳定性-可塑性困境


<details>
  <summary>Details</summary>
Motivation: 传统RL在动态环境中表现不佳，持续强化学习面临稳定性（保留先验知识）与可塑性（获取新知识）的平衡挑战。现有方法主要通过优化机制间接影响知识，很少直接影响智能体行为，这阻碍了有效的知识重用和高效学习。

Method: 提出演示引导的持续强化学习（DGCRL），将先验知识存储在外部自演化的演示库中，直接指导RL探索和适应。智能体为每个任务动态选择最相关的演示，并遵循基于课程学习的策略，从演示引导探索逐步过渡到完全自主探索。

Result: 在2D导航和MuJoCo运动任务上的大量实验表明，该方法具有优越的平均性能、增强的知识迁移能力、减轻遗忘效果和训练效率。敏感性分析和消融研究进一步验证了其有效性。

Conclusion: DGCRL通过外部演示库直接指导RL行为，有效解决了持续强化学习中的稳定性-可塑性困境，实现了更好的知识重用和学习效率。

Abstract: Reinforcement learning (RL) excels in various applications but struggles in dynamic environments where the underlying Markov decision process evolves. Continual reinforcement learning (CRL) enables RL agents to continually learn and adapt to new tasks, but balancing stability (preserving prior knowledge) and plasticity (acquiring new knowledge) remains challenging. Existing methods primarily address the stability-plasticity dilemma through mechanisms where past knowledge influences optimization but rarely affects the agent's behavior directly, which may hinder effective knowledge reuse and efficient learning. In contrast, we propose demonstration-guided continual reinforcement learning (DGCRL), which stores prior knowledge in an external, self-evolving demonstration repository that directly guides RL exploration and adaptation. For each task, the agent dynamically selects the most relevant demonstration and follows a curriculum-based strategy to accelerate learning, gradually shifting from demonstration-guided exploration to fully self-exploration. Extensive experiments on 2D navigation and MuJoCo locomotion tasks demonstrate its superior average performance, enhanced knowledge transfer, mitigation of forgetting, and training efficiency. The additional sensitivity analysis and ablation study further validate its effectiveness.

</details>


### [87] [Improving Pattern Recognition of Scheduling Anomalies through Structure-Aware and Semantically-Enhanced Graphs](https://arxiv.org/abs/2512.18673)
*Ning Lyu,Junjie Jiang,Lu Chang,Chihui Shao,Feng Chen,Chong Zhang*

Main category: cs.LG

TL;DR: 提出一种结构感知驱动的调度图建模方法，通过结构引导的调度图构建和多尺度图语义聚合，提升复杂系统调度行为异常识别的准确性和表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有调度行为异常识别方法在复杂系统（多任务并发、资源竞争、阶段转换等场景）中难以准确捕捉全局调度关系和异常特征，需要更有效的建模方法来提升异常检测性能。

Method: 1. 结构引导的调度图构建机制：整合任务执行阶段、资源节点状态和调度路径信息，构建动态演化的调度行为图；2. 多尺度图语义聚合模块：通过局部邻接语义集成和全局拓扑对齐，实现调度特征的语义一致性建模。

Result: 在真实调度数据集上实验，设置多种调度扰动路径模拟不同类型异常（结构偏移、资源变化、任务延迟）。模型在多个指标上表现出显著性能优势，对结构扰动和语义偏移敏感。可视化分析显示调度行为图具有更强的异常可分性和模式表征能力。

Conclusion: 该方法通过结构引导和语义聚合的协同作用，有效提升了调度异常检测的准确性和适应性，验证了在复杂调度场景中异常识别任务的有效性。

Abstract: This paper proposes a structure-aware driven scheduling graph modeling method to improve the accuracy and representation capability of anomaly identification in scheduling behaviors of complex systems. The method first designs a structure-guided scheduling graph construction mechanism that integrates task execution stages, resource node states, and scheduling path information to build dynamically evolving scheduling behavior graphs, enhancing the model's ability to capture global scheduling relationships. On this basis, a multi-scale graph semantic aggregation module is introduced to achieve semantic consistency modeling of scheduling features through local adjacency semantic integration and global topology alignment, thereby strengthening the model's capability to capture abnormal features in complex scenarios such as multi-task concurrency, resource competition, and stage transitions. Experiments are conducted on a real scheduling dataset with multiple scheduling disturbance paths set to simulate different types of anomalies, including structural shifts, resource changes, and task delays. The proposed model demonstrates significant performance advantages across multiple metrics, showing a sensitive response to structural disturbances and semantic shifts. Further visualization analysis reveals that, under the combined effect of structure guidance and semantic aggregation, the scheduling behavior graph exhibits stronger anomaly separability and pattern representation, validating the effectiveness and adaptability of the method in scheduling anomaly detection tasks.

</details>


### [88] [Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding](https://arxiv.org/abs/2512.18689)
*Xiangrui Cai,Shaocheng Ma,Lei Cao,Jie Li,Tianyu Liu,Yilin Dong*

Main category: cs.LG

TL;DR: 提出EEG-CSANet模型，通过多分支并行架构和集中式稀疏注意力网络处理EEG信号的时空异质性，在五个公开数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号解码是脑机接口和智能交互的关键技术，但EEG信号存在固有的时空异质性，需要有效处理这种复杂特征。

Method: 采用多分支并行架构，每个时间尺度配备独立的空间特征提取模块；提出集中式稀疏注意力网络(EEG-CSANet)，包含主分支（多尺度自注意力建模核心时空模式）和辅助分支（稀疏交叉注意力促进高效局部交互）。

Result: 在五个公开数据集上达到SOTA性能：BCIC-IV-2A(88.54%)、BCIC-IV-2B(91.09%)、HGD(99.43%)、SEED(96.03%)、SEED-VIG(90.56%)，表现出强大的适应性和鲁棒性。

Conclusion: EEG-CSANet在多种EEG解码任务中表现出色，具有成为EEG信号解码领域基准模型的潜力，未来可作为该领域的有前景的基线模型。

Abstract: Electroencephalography (EEG) signal decoding is a key technology that translates brain activity into executable commands, laying the foundation for direct brain-machine interfacing and intelligent interaction. To address the inherent spatiotemporal heterogeneity of EEG signals, this paper proposes a multi-branch parallel architecture, where each temporal scale is equipped with an independent spatial feature extraction module. To further enhance multi-branch feature fusion, we propose a Fusion of Multiscale Features via Centralized Sparse-attention Network (EEG-CSANet), a centralized sparse-attention network. It employs a main-auxiliary branch architecture, where the main branch models core spatiotemporal patterns via multiscale self-attention, and the auxiliary branch facilitates efficient local interactions through sparse cross-attention. Experimental results show that EEG-CSANet achieves state-of-the-art (SOTA) performance across five public datasets (BCIC-IV-2A, BCIC-IV-2B, HGD, SEED, and SEED-VIG), with accuracies of 88.54%, 91.09%, 99.43%, 96.03%, and 90.56%, respectively. Such performance demonstrates its strong adaptability and robustness across various EEG decoding tasks. Moreover, extensive ablation studies are conducted to enhance the interpretability of EEG-CSANet. In the future, we hope that EEG-CSANet could serve as a promising baseline model in the field of EEG signal decoding. The source code is publicly available at: https://github.com/Xiangrui-Cai/EEG-CSANet

</details>


### [89] [Generating Risky Samples with Conformity Constraints via Diffusion Models](https://arxiv.org/abs/2512.18722)
*Han Yu,Hao Zou,Xingxuan Zhang,Zhengyi Wang,Yue He,Kehan Li,Peng Cui*

Main category: cs.LG

TL;DR: RiskyDiff：一种利用扩散模型生成高风险样本的方法，通过文本和图像嵌入作为隐式约束，并设计符合性评分来增强类别一致性，同时引入嵌入筛选和风险梯度指导机制提升样本风险度。


<details>
  <summary>Details</summary>
Motivation: 现有方法发现高风险样本的多样性受限于现有数据集的覆盖范围，而基于扩散模型的方法在生成样本与预期类别的一致性方面存在问题，可能引入标签噪声并限制应用效果。

Method: 提出RiskyDiff方法：1) 结合文本和图像嵌入作为类别一致性的隐式约束；2) 设计符合性评分来显式强化类别一致性；3) 引入嵌入筛选机制；4) 采用风险梯度指导来提升生成样本的风险程度。

Result: 实验表明RiskyDiff在风险程度、生成质量和类别一致性方面显著优于现有方法，并且通过使用高一致性生成样本进行数据增强，可以提升模型的泛化能力。

Conclusion: RiskyDiff通过结合隐式和显式约束机制，有效解决了扩散模型生成高风险样本时的类别一致性问题，为模型风险评估和数据增强提供了有效工具。

Abstract: Although neural networks achieve promising performance in many tasks, they may still fail when encountering some examples and bring about risks to applications. To discover risky samples, previous literature attempts to search for patterns of risky samples within existing datasets or inject perturbation into them. Yet in this way the diversity of risky samples is limited by the coverage of existing datasets. To overcome this limitation, recent works adopt diffusion models to produce new risky samples beyond the coverage of existing datasets. However, these methods struggle in the conformity between generated samples and expected categories, which could introduce label noise and severely limit their effectiveness in applications. To address this issue, we propose RiskyDiff that incorporates the embeddings of both texts and images as implicit constraints of category conformity. We also design a conformity score to further explicitly strengthen the category conformity, as well as introduce the mechanisms of embedding screening and risky gradient guidance to boost the risk of generated samples. Extensive experiments reveal that RiskyDiff greatly outperforms existing methods in terms of the degree of risk, generation quality, and conformity with conditioned categories. We also empirically show the generalization ability of the models can be enhanced by augmenting training data with generated samples of high conformity.

</details>


### [90] [ML Inference Scheduling with Predictable Latency](https://arxiv.org/abs/2512.18725)
*Haidong Zhao,Nikolaos Georgantas*

Main category: cs.LG

TL;DR: 现有机器学习推理服务系统的干扰预测方法存在粒度粗和模型静态的问题，限制了调度效果，本文评估了这些限制并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 机器学习推理服务系统需要调度请求以提高GPU利用率并满足SLO或截止时间要求，但并发任务对GPU资源的竞争会产生干扰，影响调度可预测性，而现有干扰预测方法存在局限性。

Method: 评估现有干扰预测方法的局限性，包括粒度粗（忽略运行时共置动态）和静态预测模型（难以适应不同工作负载特性），并概述正在进行的改进工作。

Result: 识别出现有干扰预测方法在准确性和适应性方面的不足，这些限制影响了调度效果和SLO/截止时间满足。

Conclusion: 需要开发更细粒度、自适应的干扰预测方法来实现高效的机器学习推理调度，本文为这一方向提供了研究基础。

Abstract: Machine learning (ML) inference serving systems can schedule requests to improve GPU utilization and to meet service level objectives (SLOs) or deadlines. However, improving GPU utilization may compromise latency-sensitive scheduling, as concurrent tasks contend for GPU resources and thereby introduce interference. Given that interference effects introduce unpredictability in scheduling, neglecting them may compromise SLO or deadline satisfaction. Nevertheless, existing interference prediction approaches remain limited in several respects, which may restrict their usefulness for scheduling. First, they are often coarse-grained, which ignores runtime co-location dynamics and thus restricts their accuracy in interference prediction. Second, they tend to use a static prediction model, which may not effectively cope with different workload characteristics. To this end, we evaluate the potential limitations of existing interference prediction approaches and outline our ongoing work toward achieving efficient ML inference scheduling.

</details>


### [91] [A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models](https://arxiv.org/abs/2512.18730)
*Zhiquan Tan,Yinrong Hong*

Main category: cs.LG

TL;DR: 论文利用KL正则化强化学习的最优策略的闭式能量模型结构，为LLM提供统一的变分分析，证明指令调优模型满足细致平衡条件，推理模型的目标等价于向最优推理分布的期望KL最小化。


<details>
  <summary>Details</summary>
Motivation: 尽管通过KL正则化强化学习训练的大语言模型展现出强大的指令跟随、自我纠正和推理能力，但其理论基础仍然有限。本文旨在填补这一理论空白。

Method: 利用KL正则化最优策略的闭式能量模型结构，提供统一的变分分析框架。对于指令调优模型，在奖励势函数和预训练对称性的自然假设下，证明转移核满足细致平衡条件。对于推理模型，分析其目标函数结构。

Result: 1) 指令调优模型的转移核相对于编码响应质量的标量势函数满足细致平衡，导致单调KL收敛到高质量平稳分布，有界到达优越状态的命中时间，以及由谱隙控制的指数混合。2) 推理模型的目标等价于向最优推理分布的期望KL最小化，次优性间隙简化为目标与当前准确率之间的伯努利KL散度。

Conclusion: 该理论框架为理解KL正则化强化学习训练的大语言模型提供了统一的分析工具，解释了指令调优模型的收敛特性和推理模型的熵-准确率权衡现象。

Abstract: Large language models (LLMs) trained via KL-regularized reinforcement learning demonstrate strong instruction following, self-correction, and reasoning abilities. Yet their theoretical underpinnings remain limited. We exploit the closed-form energy-based model (EBM) structure of the optimal KL-regularized policy to provide a unified variational analysis of LLMs.
  For instruction-tuned models, under natural assumptions on reward potentials and pretraining symmetry, we prove that the transition kernel satisfies detailed balance with respect to a scalar potential encoding response quality. This yields monotonic KL convergence to a high-quality stationary distribution, bounded hitting times to superior states, and exponential mixing governed by the spectral gap.
  For reasoning models trained with verifiable rewards (RLVR), we show the objective is equivalent to expected KL minimization toward an optimal reasoning distribution, with the suboptimality gap reducing to the Bernoulli KL between target and current accuracies along the natural gradient flow. This helps explain empirical entropy-accuracy trade-offs.

</details>


### [92] [Is Your Conditional Diffusion Model Actually Denoising?](https://arxiv.org/abs/2512.18736)
*Daniel Pfrommer,Zehao Dou,Christopher Scarvelis,Max Simchowitz,Ali Jadbabaie*

Main category: cs.LG

TL;DR: 扩散模型在条件生成时会出现与理想去噪过程的偏差，这种偏差与模型容量或训练数据量无关，源于条件空间不同部分去噪流难以桥接的平滑性归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 研究条件扩散模型（如文本到图像生成和观测条件控制策略）的归纳偏置，发现这些模型在条件查询时与理想去噪过程存在系统性偏差，导致不同采样算法产生不一致结果。

Method: 提出"计划偏差"作为衡量偏离标准去噪过程的量化指标，并提供计算方法。通过理论分析表明这种现象源于模型在条件空间不同部分桥接去噪流时面临的困难，以及平滑性归纳偏置的影响。

Result: 条件扩散模型在生成时确实会偏离理想去噪过程，这种偏差与模型容量或训练数据量无关。DDPM、DDIM等流行采样算法在这种条件下会产生不一致的结果。

Conclusion: 条件扩散模型存在系统性偏差，源于条件空间不同部分去噪流难以桥接的平滑性归纳偏置。这一发现对理解扩散模型工作机制和设计更鲁棒的采样算法具有重要意义。

Abstract: We study the inductive biases of diffusion models with a conditioning-variable, which have seen widespread application as both text-conditioned generative image models and observation-conditioned continuous control policies. We observe that when these models are queried conditionally, their generations consistently deviate from the idealized "denoising" process upon which diffusion models are formulated, inducing disagreement between popular sampling algorithms (e.g. DDPM, DDIM). We introduce Schedule Deviation, a rigorous measure which captures the rate of deviation from a standard denoising process, and provide a methodology to compute it. Crucially, we demonstrate that the deviation from an idealized denoising process occurs irrespective of the model capacity or amount of training data. We posit that this phenomenon occurs due to the difficulty of bridging distinct denoising flows across different parts of the conditioning space and show theoretically how such a phenomenon can arise through an inductive bias towards smoothness.

</details>


### [93] [PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation](https://arxiv.org/abs/2512.18737)
*Zichuan Lin,Xiaokai Huang,Jiate Liu,Yuxuan Han,Jia Chen,Xiapeng Wu,Deheng Ye*

Main category: cs.LG

TL;DR: PIPCFR通过引入治疗后变量改进伪结果插补，提升个体治疗效果估计精度


<details>
  <summary>Details</summary>
Motivation: 现有方法大多忽视治疗后变量对结果的影响，导致反事实预测方差增加，无法完全捕捉结果变异性

Method: 提出PIPCFR方法，结合治疗后变量改进伪结果插补，学习有效表示保留信息成分同时减轻偏差

Result: 在真实世界和模拟数据集上，PIPCFR相比现有方法显著降低个体治疗效果估计误差

Conclusion: 治疗后变量对个体治疗效果估计至关重要，PIPCFR通过理论分析和实证验证展示了其有效性

Abstract: The estimation of individual treatment effects (ITE) focuses on predicting the outcome changes that result from a change in treatment. A fundamental challenge in observational data is that while we need to infer outcome differences under alternative treatments, we can only observe each individual's outcome under a single treatment. Existing approaches address this limitation either by training with inferred pseudo-outcomes or by creating matched instance pairs. However, recent work has largely overlooked the potential impact of post-treatment variables on the outcome. This oversight prevents existing methods from fully capturing outcome variability, resulting in increased variance in counterfactual predictions. This paper introduces Pseudo-outcome Imputation with Post-treatment Variables for Counterfactual Regression (PIPCFR), a novel approach that incorporates post-treatment variables to improve pseudo-outcome imputation. We analyze the challenges inherent in utilizing post-treatment variables and establish a novel theoretical bound for ITE risk that explicitly connects post-treatment variables to ITE estimation accuracy. Unlike existing methods that ignore these variables or impose restrictive assumptions, PIPCFR learns effective representations that preserve informative components while mitigating bias. Empirical evaluations on both real-world and simulated datasets demonstrate that PIPCFR achieves significantly lower ITE errors compared to existing methods.

</details>


### [94] [Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning](https://arxiv.org/abs/2512.18763)
*Minh Vu,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 论文提出GMM-QFs，将高斯混合模型作为Q函数损失的直接替代，而非传统的概率密度估计器，通过黎曼优化在策略评估中学习参数，在多个基准任务中表现优异且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中高斯混合模型主要用于概率密度函数估计，本文探索其作为函数逼近器的新角色，特别是作为Q函数损失的直接替代，以提供更强的表示能力和更小的计算开销。

Method: 提出GMM-QFs模型，将高斯混合模型嵌入贝尔曼残差中，通过黎曼流形上的优化学习混合权重、高斯均值向量和协方差矩阵等参数，并将此几何视角整合到标准策略迭代框架的策略评估步骤中。

Result: 理论证明GMM-QFs在广泛函数类上是通用逼近器，实验显示即使在没有经验数据的情况下，GMM-QFs在多个基准RL任务中表现竞争力强，有时甚至优于最先进方法，同时计算开销显著小于依赖经验数据的深度学习方法。

Conclusion: GMM-QFs作为Q函数损失的新型函数逼近器，结合黎曼优化方法，在强化学习中提供了强大的表示能力、优异的性能和较低的计算成本，为函数逼近开辟了新方向。

Abstract: Unlike their conventional use as estimators of probability density functions in reinforcement learning (RL), this paper introduces a novel function-approximation role for Gaussian mixture models (GMMs) as direct surrogates for Q-function losses. These parametric models, termed GMM-QFs, possess substantial representational capacity, as they are shown to be universal approximators over a broad class of functions. They are further embedded within Bellman residuals, where their learnable parameters -- a fixed number of mixing weights, together with Gaussian mean vectors and covariance matrices -- are inferred from data via optimization on a Riemannian manifold. This geometric perspective on the parameter space naturally incorporates Riemannian optimization into the policy-evaluation step of standard policy-iteration frameworks. Rigorous theoretical results are established, and supporting numerical tests show that, even without access to experience data, GMM-QFs deliver competitive performance and, in some cases, outperform state-of-the-art approaches across a range of benchmark RL tasks, all while maintaining a significantly smaller computational footprint than deep-learning methods that rely on experience data.

</details>


### [95] [Label-Informed Outlier Detection Based on Granule Density](https://arxiv.org/abs/2512.18774)
*Baiyang Chen,Zhong Yuan,Dezhong Peng,Hongmei Chen,Xiaomin Song,Huiming Zheng*

Main category: cs.LG

TL;DR: 提出基于粒计算和模糊集的标签信息异构数据异常检测方法GDOF，通过标签信息模糊粒化表示异构数据类型，结合属性相关性进行异常评分，在少量标记异常样本下实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 现有半监督异常检测方法通常将数据视为纯数值且确定性处理，忽略了现实世界复杂数据集中固有的异质性和不确定性。需要一种能够处理异构数据并利用有限标记异常信息的方法。

Method: 提出Granule Density-based Outlier Factor (GDOF)方法：1) 使用标签信息模糊粒化有效表示各种数据类型；2) 开发粒度密度进行精确密度估计；3) 通过评估属性相关性（利用少量标记异常）整合各属性的粒度密度进行异常评分。

Result: 在多个真实世界数据集上的实验结果表明，GDOF在异构数据异常检测方面表现突出，仅需少量标记异常样本即可实现高效检测。模糊集与粒计算的结合为复杂多样数据类型的异常检测提供了实用框架。

Conclusion: GDOF通过整合模糊集和粒计算，提供了一种处理异构数据异常检测的有效方法，能够利用有限标记信息在复杂数据集中准确识别异常。所有相关数据集和源代码已公开供进一步研究。

Abstract: Outlier detection, crucial for identifying unusual patterns with significant implications across numerous applications, has drawn considerable research interest. Existing semi-supervised methods typically treat data as purely numerical and} in a deterministic manner, thereby neglecting the heterogeneity and uncertainty inherent in complex, real-world datasets. This paper introduces a label-informed outlier detection method for heterogeneous data based on Granular Computing and Fuzzy Sets, namely Granule Density-based Outlier Factor (GDOF). Specifically, GDOF first employs label-informed fuzzy granulation to effectively represent various data types and develops granule density for precise density estimation. Subsequently, granule densities from individual attributes are integrated for outlier scoring by assessing attribute relevance with a limited number of labeled outliers. Experimental results on various real-world datasets show that GDOF stands out in detecting outliers in heterogeneous data with a minimal number of labeled outliers. The integration of Fuzzy Sets and Granular Computing in GDOF offers a practical framework for outlier detection in complex and diverse data types. All relevant datasets and source codes are publicly available for further research. This is the author's accepted manuscript of a paper published in IEEE Transactions on Fuzzy Systems. The final version is available at https://doi.org/10.1109/TFUZZ.2024.3514853

</details>


### [96] [Controllable Probabilistic Forecasting with Stochastic Decomposition Layers](https://arxiv.org/abs/2512.18815)
*John S. Schreck,William E. Chapman,Charlie Becker,David John Gagne,Dhamma Kimpara,Nihanth Cherukuru,Judith Berner,Kirsten J. Mayer,Negin Sobhani*

Main category: cs.LG

TL;DR: 提出Stochastic Decomposition Layers (SDL)，通过分层噪声注入将确定性ML天气模型转换为概率集合系统，相比传统方法大幅降低计算成本，实现良好校准的集合预测。


<details>
  <summary>Details</summary>
Motivation: 当前基于CRPS的集合方法训练策略和噪声注入机制各异，大多通过条件归一化全局注入噪声，这增加了训练成本且限制了随机扰动的物理可解释性。

Method: 引入Stochastic Decomposition Layers (SDL)，借鉴StyleGAN的分层噪声注入思想，在三个解码器尺度上通过潜在驱动调制、逐像素噪声和通道缩放应用学习到的扰动。

Result: 应用于WXFormer时，SDL仅需基线模型训练计算成本的不到2%。集合成员从紧凑潜在张量(5MB)生成，实现完美可重现性。在2022年ERA5再分析数据上评估，集合的spread-skill比接近1，rank直方图在中程预报中趋于均匀，校准性能与业务IFS-ENS相当。

Conclusion: SDL提供了一种高效、可解释的集合预测方法，多尺度实验揭示了分层不确定性：粗层调制天气尺度模式，细层控制中尺度变率，为业务预报和气候应用提供可解释的不确定性量化。

Abstract: AI weather prediction ensembles with latent noise injection and optimized with the continuous ranked probability score (CRPS) have produced both accurate and well-calibrated predictions with far less computational cost compared with diffusion-based methods. However, current CRPS ensemble approaches vary in their training strategies and noise injection mechanisms, with most injecting noise globally throughout the network via conditional normalization. This structure increases training expense and limits the physical interpretability of the stochastic perturbations. We introduce Stochastic Decomposition Layers (SDL) for converting deterministic machine learning weather models into probabilistic ensemble systems. Adapted from StyleGAN's hierarchical noise injection, SDL applies learned perturbations at three decoder scales through latent-driven modulation, per-pixel noise, and channel scaling. When applied to WXFormer via transfer learning, SDL requires less than 2\% of the computational cost needed to train the baseline model. Each ensemble member is generated from a compact latent tensor (5 MB), enabling perfect reproducibility and post-inference spread adjustment through latent rescaling. Evaluation on 2022 ERA5 reanalysis shows ensembles with spread-skill ratios approaching unity and rank histograms that progressively flatten toward uniformity through medium-range forecasts, achieving calibration competitive with operational IFS-ENS. Multi-scale experiments reveal hierarchical uncertainty: coarse layers modulate synoptic patterns while fine layers control mesoscale variability. The explicit latent parameterization provides interpretable uncertainty quantification for operational forecasting and climate applications.

</details>


### [97] [Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection](https://arxiv.org/abs/2512.18826)
*Souhail Abdelmouaiz Sadat,Mohamed Yacine Touahria Miliani,Khadidja Hab El Hames,Hamida Seba,Mohammed Haddad*

Main category: cs.LG

TL;DR: 该综述回顾了双曲图嵌入模型，并在异常检测任务上评估它们，突出了相比欧几里得方法在捕捉复杂结构方面的优势。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得方法在处理复杂图结构数据时存在局限性，特别是在异常检测任务中。双曲空间因其能够更好地表示层次结构和复杂关系而显示出潜力，需要系统评估双曲图嵌入模型在异常检测中的表现。

Method: 对多种双曲图嵌入模型（包括HGCAE、\(\mathcal{P}\)-VAE、HGCN等）进行系统性评估，并与欧几里得方法（如DOMINANT、GraphSage）在异常检测任务上进行对比分析。

Result: 双曲模型表现优异：\(\mathcal{P}\)-VAE在Elliptic数据集上达到94%的F1分数，HGCAE在Cora数据集上达到80%。相比之下，欧几里得方法在复杂数据上表现较差。

Conclusion: 双曲空间在异常检测中具有显著优势，能够更好地处理复杂图结构。研究提供了开源库以促进该领域的进一步研究。

Abstract: This survey reviews hyperbolic graph embedding models, and evaluate them on anomaly detection, highlighting their advantages over Euclidean methods in capturing complex structures. Evaluating models like \textit{HGCAE}, \textit{\(\mathcal{P}\)-VAE}, and \textit{HGCN} demonstrates high performance, with \textit{\(\mathcal{P}\)-VAE} achieving an F1-score of 94\% on the \textit{Elliptic} dataset and \textit{HGCAE} scoring 80\% on \textit{Cora}. In contrast, Euclidean methods like \textit{DOMINANT} and \textit{GraphSage} struggle with complex data. The study emphasizes the potential of hyperbolic spaces for improving anomaly detection, and provides an open-source library to foster further research in this field.

</details>


### [98] [Generative Modeling through Spectral Analysis of Koopman Operator](https://arxiv.org/abs/2512.18837)
*Yuanchao Xu,Fengyi Li,Masahiro Fujisawa,Youssef Marzouk,Isao Ishikawa*

Main category: cs.LG

TL;DR: 提出KSWGD生成模型框架，结合算子理论谱分析与最优传输，通过Koopman算子近似从轨迹数据直接估计谱结构，无需目标势函数显式知识或神经网络训练，实现加速Wasserstein梯度下降。


<details>
  <summary>Details</summary>
Motivation: 传统Wasserstein梯度下降方法需要目标势函数的显式知识或神经网络训练，限制了应用范围。本文旨在开发一种直接从轨迹数据中学习谱结构的方法，实现更高效、更通用的生成建模。

Method: 结合Koopman算子理论与最优传输，通过轨迹数据近似Koopman算子来估计谱结构，用于加速Wasserstein梯度下降。该方法建立了与Feynman-Kac理论的联系，提供了概率基础。

Result: 在多种设置下（包括紧致流形采样、亚稳态多阱系统、图像生成和高维随机偏微分方程）的实验表明，KSWGD相比现有方法收敛更快，同时保持高质量样本生成。

Conclusion: KSWGD提供了一个无需显式目标势函数或神经网络训练的高效生成建模框架，通过算子理论谱分析与最优传输的结合，实现了加速收敛和高样本质量，具有广泛的应用潜力。

Abstract: We propose Koopman Spectral Wasserstein Gradient Descent (KSWGD), a generative modeling framework that combines operator-theoretic spectral analysis with optimal transport. The novel insight is that the spectral structure required for accelerated Wasserstein gradient descent can be directly estimated from trajectory data via Koopman operator approximation which can eliminate the need for explicit knowledge of the target potential or neural network training. We provide rigorous convergence analysis and establish connection to Feynman-Kac theory that clarifies the method's probabilistic foundation. Experiments across diverse settings, including compact manifold sampling, metastable multi-well systems, image generation, and high dimensional stochastic partial differential equation, demonstrate that KSWGD consistently achieves faster convergence than other existing methods while maintaining high sample quality.

</details>


### [99] [Merging of Kolmogorov-Arnold networks trained on disjoint datasets](https://arxiv.org/abs/2512.18921)
*Andrew Polar,Michael Poluektov*

Main category: cs.LG

TL;DR: 论文提出在不相交数据集上训练KAN网络可加速训练，结合牛顿-卡奇马兹方法和分段线性基函数是目前最快组合


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究主要关注分布式节点的训练收敛性，而本文专门针对训练加速问题，这在不相交数据集训练中尤为关键

Method: 采用牛顿-卡奇马兹优化方法和分段线性基函数，在不相交数据集（或训练数据集的子集）上进行训练

Result: 实验证明在不相交数据集上训练能进一步提升性能，提供了实验比较结果

Conclusion: 不相交数据集训练是加速KAN网络训练的有效方法，公开了所有相关代码

Abstract: Training on disjoint datasets can serve two primary goals: accelerating data processing and enabling federated learning. It has already been established that Kolmogorov-Arnold networks (KANs) are particularly well suited for federated learning and can be merged through simple parameter averaging. While the federated learning literature has mostly focused on achieving training convergence across distributed nodes, the present paper specifically targets acceleration of the training, which depends critically on the choice of an optimisation method and the type of the basis functions. To the best knowledge of the authors, the fastest currently-available combination is the Newton-Kaczmarz method and the piecewise-linear basis functions. Here, it is shown that training on disjoint datasets (or disjoint subsets of the training dataset) can further improve the performance. Experimental comparisons are provided, and all corresponding codes are publicly available.

</details>


### [100] [The Ensemble Schr{ö}dinger Bridge filter for Nonlinear Data Assimilation](https://arxiv.org/abs/2512.18928)
*Feng Bao,Hui Sun*

Main category: cs.LG

TL;DR: 提出一种新的非线性最优滤波器——集成薛定谔桥非线性滤波器，将标准预测过程与扩散生成建模结合，实现无模型误差、无需导数、无需训练且高度并行化的滤波算法。


<details>
  <summary>Details</summary>
Motivation: 传统非线性滤波器（如集成卡尔曼滤波和粒子滤波）在处理高度非线性系统时存在局限性，特别是在高维混沌环境中。需要一种更有效的方法来处理这类复杂滤波问题。

Method: 将标准预测过程与扩散生成建模相结合，通过薛定谔桥框架实现滤波步骤。该方法无结构模型误差，无需导数计算，无需训练过程，且具有高度并行化特性。

Result: 在高达40维或更高的混沌环境中，算法对高度非线性动力学表现良好。在多种非线性程度测试中，性能优于集成卡尔曼滤波和粒子滤波等经典方法。

Conclusion: 提出的集成薛定谔桥非线性滤波器是一种有效的非线性滤波方法，未来工作将扩展到实际气象应用并建立严格的收敛性分析。

Abstract: This work puts forward a novel nonlinear optimal filter namely the Ensemble Schr{ö}dinger Bridge nonlinear filter. The proposed filter finds marriage of the standard prediction procedure and the diffusion generative modeling for the analysis procedure to realize one filtering step. The designed approach finds no structural model error, and it is derivative free, training free and highly parallizable. Experimental results show that the designed algorithm performs well given highly nonlinear dynamics in (mildly) high dimension up to 40 or above under a chaotic environment. It also shows better performance than classical methods such as the ensemble Kalman filter and the Particle filter in numerous tests given different level of nonlinearity. Future work will focus on extending the proposed approach to practical meteorological applications and establishing a rigorous convergence analysis.

</details>


### [101] [DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems](https://arxiv.org/abs/2512.18932)
*Sarwan Ali*

Main category: cs.LG

TL;DR: DPSR是一个三阶段去噪框架，通过利用评分矩阵的稀疏性、低秩特性和协同模式，在保护差分隐私的同时显著提升推荐质量，甚至在某些情况下超越非隐私基线。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私机制面临隐私-效用权衡的根本挑战：随着隐私预算收紧，推荐质量不可避免地下降。需要一种方法在保护隐私的同时保持甚至提升推荐质量。

Method: DPSR采用三阶段去噪框架：1) 信息论噪声校准，自适应减少高信息评分的噪声；2) 基于协同过滤的去噪，利用物品相似性去除隐私噪声；3) 低秩矩阵补全，利用潜在结构恢复信号。所有去噪操作在噪声注入后执行，通过后处理免疫定理保持差分隐私。

Result: 在隐私预算ε=0.1到10.0范围内，DPSR比最先进的拉普拉斯和高斯机制实现5.57%到9.23%的RMSE提升（所有改进统计显著，p<0.05，大多数p<0.001）。在ε=1.0时，DPSR的RMSE为0.9823，甚至优于非隐私基线（1.0983）。

Conclusion: DPSR通过创新的三阶段去噪框架，不仅解决了差分隐私中的隐私-效用权衡问题，还能作为有效的正则化器去除数据噪声，在某些情况下实现比非隐私方法更好的推荐质量。

Abstract: Differential privacy (DP) has emerged as the gold standard for protecting user data in recommender systems, but existing privacy-preserving mechanisms face a fundamental challenge: the privacy-utility tradeoff inevitably degrades recommendation quality as privacy budgets tighten. We introduce DPSR (Differentially Private Sparse Reconstruction), a novel three-stage denoising framework that fundamentally addresses this limitation by exploiting the inherent structure of rating matrices -- sparsity, low-rank properties, and collaborative patterns.
  DPSR consists of three synergistic stages: (1) \textit{information-theoretic noise calibration} that adaptively reduces noise for high-information ratings, (2) \textit{collaborative filtering-based denoising} that leverages item-item similarities to remove privacy noise, and (3) \textit{low-rank matrix completion} that exploits latent structure for signal recovery. Critically, all denoising operations occur \textit{after} noise injection, preserving differential privacy through the post-processing immunity theorem while removing both privacy-induced and inherent data noise.
  Through extensive experiments on synthetic datasets with controlled ground truth, we demonstrate that DPSR achieves 5.57\% to 9.23\% RMSE improvement over state-of-the-art Laplace and Gaussian mechanisms across privacy budgets ranging from $\varepsilon=0.1$ to $\varepsilon=10.0$ (all improvements statistically significant with $p < 0.05$, most $p < 0.001$). Remarkably, at $\varepsilon=1.0$, DPSR achieves RMSE of 0.9823, \textit{outperforming even the non-private baseline} (1.0983), demonstrating that our denoising pipeline acts as an effective regularizer that removes data noise in addition to privacy noise.

</details>


### [102] [When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models](https://arxiv.org/abs/2512.18934)
*Michael S. Zhang,Rishi A. Ruia,Arnav Kewalram,Saathvik Dharmapuram,Utkarsh Sharma,Kevin Zhu*

Main category: cs.LG

TL;DR: 量化（INT8/INT4）在持续学习中反而优于FP16精度，量化噪声起到正则化作用，小规模回放缓冲区（0.1-10%）能显著提升知识保留


<details>
  <summary>Details</summary>
Motivation: 研究量化精度与持续学习中灾难性遗忘的相互作用，挑战"精度越高越好"的传统观念，为部署高效压缩模型提供指导

Method: 系统研究不同量化精度（FP16、INT8、INT4）与回放缓冲区策略在大语言模型中的表现，分析量化噪声的正则化效应

Result: 量化模型在后续任务中反超FP16（INT4在代码生成上达到40% vs FP16的20%），小回放缓冲区（0.1%）显著提升知识保留，INT8在可塑性与知识保留间达到最佳平衡

Conclusion: INT8量化在计算效率和持续学习性能上均优于高精度模型，量化噪声起到正则化作用防止过拟合，为实际部署提供实用指南

Abstract: Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.

</details>


### [103] [Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement](https://arxiv.org/abs/2512.18950)
*Saman Forouzandeh,Wei Peng,Parham Moradi,Xinghuo Yu,Mahdi Jalili*

Main category: cs.LG

TL;DR: MACLA框架通过外部分层程序记忆实现推理与学习的解耦，无需更新LLM参数，在多个基准测试中表现优异且高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体通常需要微调LLM参数，这既计算成本高又缺乏可解释性。MACLA旨在通过外部记忆系统实现样本高效、可解释且持续改进的智能体，避免LLM参数更新。

Method: MACLA保持LLM冻结，在外部分层程序记忆中执行所有适应。它从轨迹中提取可重用程序，通过贝叶斯后验跟踪可靠性，通过期望效用评分选择动作，并通过对比成功与失败来精炼程序。

Result: 在四个基准测试(ALFWorld、WebShop、TravelPlanner、InterCodeSQL)中平均性能达78.1%，优于所有基线。在ALFWorld未见任务上达到90.3%性能，有3.1%正向泛化。系统在56秒内构建记忆，比最先进的LLM参数训练基线快2800倍，将2851条轨迹压缩为187个程序。

Conclusion: 具有贝叶斯选择和对比精炼的结构化外部记忆能够实现样本高效、可解释且持续改进的智能体，无需LLM参数更新，为LLM智能体学习提供了新范式。

Abstract: We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.

</details>


### [104] [Learning Through Little Eyes: Attribute Discrimination Beyond Objects](https://arxiv.org/abs/2512.18951)
*Patrick Batsell,Tsutsui Satoshi,Bihan Wen*

Main category: cs.LG

TL;DR: 研究比较了婴儿视角对比学习模型CVCL与CLIP在属性识别上的差异，发现CVCL在大小识别上更好，CLIP在颜色识别上更优，两者在纹理识别上都存在视觉与语言空间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 婴儿在前两年不仅能识别物体类别，还能识别颜色、大小、纹理等细粒度属性。现有CVCL模型只关注类别识别，不清楚婴儿尺度的学习是否支持属性识别，因此需要系统评估。

Method: 引入一个系统变化颜色、大小和纹理的基准测试，用于控制性测试类别内属性识别。比较CVCL（基于婴儿自我中心视频训练的CLIP风格模型）与CLIP在属性识别上的表现。

Result: CVCL在大小识别上表现更好，CLIP在颜色识别上准确率更高。两个模型都能在图像嵌入中表示纹理特征，但都无法将纹理特征与语言空间对应起来。

Conclusion: 婴儿尺度的学习确实支持属性识别，但不同属性在不同模型中的表现存在差异。视觉与语言空间在纹理识别上存在鸿沟，需要进一步研究如何弥合这一差距。

Abstract: Infants learn to recognize not only object categories but also fine grained attributes such as color, size, and texture within their first two years of life. Prior work explores Childs View for Contrastive Learning (CVCL), a CLIP style model trained on infant egocentric video as a computational model of early infant learning, but it focuses only on class level recognition. This leaves it unclear whether infant scale learning also supports attribute discrimination. To address this, we introduce a benchmark that systematically varies color, size, and texture, allowing controlled tests of within class attribute recognition. Comparing CVCL with CLIP shows clear differences. CVCL is better at size discrimination, while CLIP achieves higher accuracy on color discrimination. Both models represent texture in image embeddings but fail to ground texture linguistically, suggesting a gap between visual and language spaces.

</details>


### [105] [Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation](https://arxiv.org/abs/2512.18957)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 提出一种在线分布鲁棒强化学习算法，无需先验模型或离线数据，通过与环境交互学习最优鲁棒策略，适用于高维任务。


<details>
  <summary>Details</summary>
Motivation: 实际应用中，训练环境与部署环境不匹配会导致RL代理性能下降。现有分布鲁棒RL方法通常需要大量先验知识（如生成模型或大型离线数据集），且主要局限于表格方法，难以扩展到复杂领域。

Method: 提出在线DR-RL算法，采用通用函数逼近，仅通过与环境交互学习最优鲁棒策略，无需先验模型或离线数据。基于总变差不确定性集进行理论分析。

Result: 建立了接近最优的次线性遗憾界，证明了方法的样本效率和有效性，能够部署到高维任务中。

Conclusion: 该方法克服了现有DR-RL的局限性，实现了无需先验知识的在线学习，为复杂领域中的鲁棒RL部署提供了可行方案。

Abstract: The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncertainty set of transition dynamics. However, existing work typically relies on substantial prior knowledge-such as access to a generative model or a large offline dataset-and largely focuses on tabular methods that do not scale to complex domains. We overcome these limitations by proposing an online DR-RL algorithm with general function approximation that learns an optimal robust policy purely through interaction with the environment, without requiring prior models or offline data, enabling deployment in high-dimensional tasks. We further provide a theoretical analysis establishing a near-optimal sublinear regret bound under a total variation uncertainty set, demonstrating the sample efficiency and effectiveness of our method.

</details>


### [106] [Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling](https://arxiv.org/abs/2512.18965)
*Sutashu Tomonaga,Kenji Doya,Noboru Murata*

Main category: cs.LG

TL;DR: 提出了一种基于滞后算子的离散时间状态空间模型直接构建框架，避免了传统连续时间建模和离散化的复杂过程，提供了更直观、模块化的设计方法。


<details>
  <summary>Details</summary>
Motivation: 结构化状态空间模型（SSMs）是Mamba架构的核心，但其理论基础依赖于复杂的连续时间建模和离散化过程，这可能会模糊直觉理解。需要一种更直接、基于第一原理的框架来构建离散时间SSMs。

Method: 引入了一种新颖的滞后算子，通过几何方式推导离散时间递推关系，测量系统基函数在时间步之间的"滑动"和变化。状态矩阵通过该算子的单次内积计算，提供了结合不同基函数和时间扭曲方案的模块化设计空间。

Result: 特定实例精确恢复了有影响力的HiPPO模型的递推关系。数值模拟验证了推导的正确性，为设计灵活和鲁棒的序列模型提供了新的理论工具。

Conclusion: 提出了一种直接、基于第一原理的离散时间SSMs构建框架，通过滞后算子提供了更直观、模块化的设计方法，能够精确恢复现有模型并支持新模型的创建。

Abstract: Structured State Space Models (SSMs), which are at the heart of the recently popular Mamba architecture, are powerful tools for sequence modeling. However, their theoretical foundation relies on a complex, multi-stage process of continuous-time modeling and subsequent discretization, which can obscure intuition. We introduce a direct, first-principles framework for constructing discrete-time SSMs that is both flexible and modular. Our approach is based on a novel lag operator, which geometrically derives the discrete-time recurrence by measuring how the system's basis functions "slide" and change from one timestep to the next. The resulting state matrices are computed via a single inner product involving this operator, offering a modular design space for creating novel SSMs by flexibly combining different basis functions and time-warping schemes. To validate our approach, we demonstrate that a specific instance exactly recovers the recurrence of the influential HiPPO model. Numerical simulations confirm our derivation, providing new theoretical tools for designing flexible and robust sequence models.

</details>


### [107] [Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets](https://arxiv.org/abs/2512.18977)
*Baiyang Chen,Zhong Yuan,Dezhong Peng,Xiaoliang Chen,Hongmei Chen*

Main category: cs.LG

TL;DR: 提出一种基于模糊粗糙集理论的半监督异常检测算法COD，用于处理异构数据，通过标签信息构建模糊相似关系，结合分类一致性和异常因子进行检测。


<details>
  <summary>Details</summary>
Motivation: 当前半监督异常检测方法主要针对数值数据，忽视了数据信息的异构性，需要一种能处理异构数据并利用部分标签监督的方法来降低误报率。

Method: 1) 利用少量标记异常构建标签信息模糊相似关系；2) 引入模糊决策系统一致性评估属性对知识分类的贡献；3) 基于模糊相似类定义异常因子，结合分类一致性和异常因子预测异常。

Result: 在15个新提出的数据集上进行广泛评估，实验结果表明COD优于或与领先的异常检测器相当。

Conclusion: 提出的COD算法能有效处理异构数据的半监督异常检测问题，通过模糊粗糙集理论和一致性指导，在降低误报率方面表现优异。

Abstract: Outlier detection aims to find samples that behave differently from the majority of the data. Semi-supervised detection methods can utilize the supervision of partial labels, thus reducing false positive rates. However, most of the current semi-supervised methods focus on numerical data and neglect the heterogeneity of data information. In this paper, we propose a consistency-guided outlier detection algorithm (COD) for heterogeneous data with the fuzzy rough set theory in a semi-supervised manner. First, a few labeled outliers are leveraged to construct label-informed fuzzy similarity relations. Next, the consistency of the fuzzy decision system is introduced to evaluate attributes' contributions to knowledge classification. Subsequently, we define the outlier factor based on the fuzzy similarity class and predict outliers by integrating the classification consistency and the outlier factor. The proposed algorithm is extensively evaluated on 15 freshly proposed datasets. Experimental results demonstrate that COD is better than or comparable with the leading outlier detectors. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.asoc.2024.112070

</details>


### [108] [Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy](https://arxiv.org/abs/2512.18978)
*Baiyang Chen,Zhong Yuan,Zheng Liu,Dezhong Peng,Yongxiang Li,Chang Liu,Guiduo Duan*

Main category: cs.LG

TL;DR: 提出基于模糊粗糙集的半监督离群点检测方法FROD，有效处理混合属性数据的不确定性和异质性


<details>
  <summary>Details</summary>
Motivation: 现有半监督离群点检测方法通常忽略真实世界混合属性数据的不确定性和异质性，需要更有效的方法来处理这些挑战

Method: 1) 使用少量标记数据构建模糊决策系统，基于模糊近似引入属性分类精度评估属性集贡献；2) 利用未标记数据计算模糊相对熵，从不确定性角度表征离群点；3) 结合属性分类精度和模糊相对熵开发检测算法

Result: 在16个公共数据集上的实验结果表明，FROD与领先的检测算法相当或更好

Conclusion: FROD方法能有效处理混合属性数据的不确定性和异质性，在离群点检测任务中表现出色

Abstract: Outlier detection is a critical task in data mining, aimed at identifying objects that significantly deviate from the norm. Semi-supervised methods improve detection performance by leveraging partially labeled data but typically overlook the uncertainty and heterogeneity of real-world mixed-attribute data. This paper introduces a semi-supervised outlier detection method, namely fuzzy rough sets-based outlier detection (FROD), to effectively handle these challenges. Specifically, we first utilize a small subset of labeled data to construct fuzzy decision systems, through which we introduce the attribute classification accuracy based on fuzzy approximations to evaluate the contribution of attribute sets in outlier detection. Unlabeled data is then used to compute fuzzy relative entropy, which provides a characterization of outliers from the perspective of uncertainty. Finally, we develop the detection algorithm by combining attribute classification accuracy with fuzzy relative entropy. Experimental results on 16 public datasets show that FROD is comparable with or better than leading detection algorithms. All datasets and source codes are accessible at https://github.com/ChenBaiyang/FROD. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.ijar.2025.109373

</details>


### [109] [R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression](https://arxiv.org/abs/2512.18986)
*Kun Zhao,Siyuan Dai,Yingying Zhang,Guodong Liu,Pengfei Gu,Chenghua Lin,Paul M. Thompson,Alex Leow,Heng Huang,Lifang He,Liang Zhan,Haoteng Tang*

Main category: cs.LG

TL;DR: R-GenIMA是一个可解释的多模态大语言模型，通过结合ROI-wise视觉变换器和遗传提示，联合建模结构MRI和SNP变异，在阿尔茨海默病早期检测中实现最先进的分类性能，并提供生物学解释。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期检测需要整合宏观神经解剖学改变和微观遗传易感性的模型，但现有多模态方法难以对齐这些异质信号。

Method: 提出R-GenIMA模型，将解剖分割的脑区表示为视觉token，SNP谱编码为结构化文本，通过跨模态注意力机制连接区域萎缩模式与潜在遗传因素。

Result: 在ADNI队列中实现NC、SMC、MCI、AD四分类的最先进性能；识别出阶段特异性脑区和基因特征；注意力归因揭示与GWAS支持的AD风险位点一致的基因富集。

Conclusion: 可解释的多模态AI能够整合影像和遗传学数据揭示机制洞察，为临床可部署工具奠定基础，支持早期风险分层和精准治疗策略。

Abstract: Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.

</details>


### [110] [The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results](https://arxiv.org/abs/2512.19007)
*Konstantin Kaulen,Tobias Ladner,Stanley Bak,Christopher Brix,Hai Duong,Thomas Flinkow,Taylor T. Johnson,Lukas Koller,Edoardo Manino,ThanhVu H Nguyen,Haoze Wu*

Main category: cs.LG

TL;DR: VNN-COMP 2025是第六届神经网络验证竞赛，在SAIV/CAV会议上举办，旨在公平比较神经网络验证工具、标准化接口并促进社区交流。8个团队在16个常规和9个扩展基准上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 该竞赛旨在促进神经网络验证工具的公平客观比较，推动工具接口标准化，并团结神经网络验证社区。通过建立标准化格式和公平的评估环境，为领域发展提供基准。

Method: 采用标准化网络格式（ONNX）和规范格式（VNN-LIB），使用AWS实例的自动评估管道确保硬件成本平等，参赛者在最终测试集公开前选择工具参数。8个团队在16个常规和9个扩展基准上进行评估。

Result: 8个团队参与了2025年的竞赛，在多样化的基准集上进行了测试。报告总结了竞赛规则、基准、参与工具、结果和经验教训，为神经网络验证领域提供了重要的性能比较和标准化参考。

Conclusion: VNN-COMP竞赛成功促进了神经网络验证工具的比较和标准化，通过公平的评估环境和社区参与，推动了该领域的技术发展和标准化进程。

Abstract: This report summarizes the 6th International Verification of Neural Networks Competition (VNN-COMP 2025), held as a part of the 8th International Symposium on AI Verification (SAIV), that was collocated with the 37th International Conference on Computer-Aided Verification (CAV). VNN-COMP is held annually to facilitate the fair and objective comparison of state-of-the-art neural network verification tools, encourage the standardization of tool interfaces, and bring together the neural network verification community. To this end, standardized formats for networks (ONNX) and specification (VNN-LIB) were defined, tools were evaluated on equal-cost hardware (using an automatic evaluation pipeline based on AWS instances), and tool parameters were chosen by the participants before the final test sets were made public. In the 2025 iteration, 8 teams participated on a diverse set of 16 regular and 9 extended benchmarks. This report summarizes the rules, benchmarks, participating tools, results, and lessons learned from this iteration of this competition.

</details>


### [111] [Optimizer Dynamics at the Edge of Stability with Differential Privacy](https://arxiv.org/abs/2512.19019)
*Ayana Hussain,Ricky Fang*

Main category: cs.LG

TL;DR: DP训练改变神经网络优化动态，但边缘稳定性模式仍然存在，尽管DP通常会降低锐度并阻止优化器完全达到经典稳定性阈值。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私(DP)如何改变神经网络优化动态，特别是梯度裁剪和高斯噪声对训练稳定性的影响，理解DP训练中是否保持经典优化器的稳定性模式。

Method: 比较梯度下降(GD)和Adam及其隐私保护变体，分析DP训练中梯度裁剪和噪声如何改变锐度和损失演化，研究不同学习率和隐私预算下的稳定性行为。

Result: DP训练虽然降低锐度并可能阻止优化器完全达到经典稳定性阈值，但边缘稳定性模式仍然存在，最大学习率和最大隐私预算会接近甚至有时超过这些阈值。

Conclusion: DP在神经网络优化中引入了不可预测性，但经典优化器的稳定性模式在DP训练中仍然持续存在，尽管有所改变。

Abstract: Deep learning models can reveal sensitive information about individual training examples, and while differential privacy (DP) provides guarantees restricting such leakage, it also alters optimization dynamics in poorly understood ways. We study the training dynamics of neural networks under DP by comparing Gradient Descent (GD), and Adam to their privacy-preserving variants. Prior work shows that these optimizers exhibit distinct stability dynamics: full-batch methods train at the Edge of Stability (EoS), while mini-batch and adaptive methods exhibit analogous edge-of-stability behavior. At these regimes, the training loss and the sharpness--the maximum eigenvalue of the training loss Hessian--exhibit certain characteristic behavior. In DP training, per-example gradient clipping and Gaussian noise modify the update rule, and it is unclear whether these stability patterns persist. We analyze how clipping and noise change sharpness and loss evolution and show that while DP generally reduces the sharpness and can prevent optimizers from fully reaching the classical stability thresholds, patterns from EoS and analogous adaptive methods stability regimes persist, with the largest learning rates and largest privacy budgets approaching, and sometimes exceeding, these thresholds. These findings highlight the unpredictability introduced by DP in neural network optimization.

</details>


### [112] [A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development](https://arxiv.org/abs/2512.19031)
*Yuan Fang,Fabian Waschkowski,Maximilian Reissmann,Richard D. Sandberg,Takuo Oda,Koichi Tanimoto*

Main category: cs.LG

TL;DR: 提出了一种结合代理模型的符号CFD驱动训练框架，通过实时学习ML生成模型的误差来大幅降低训练成本，同时保持与原CFD驱动方法相当的预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统CFD驱动训练需要将每个ML生成的候选模型嵌入CFD求解器进行评估，对于复杂流动需要数百到数千次高保真模拟，计算成本过高。需要一种方法来降低这种训练成本。

Method: 提出扩展框架，将代理模型集成到符号CFD驱动训练中：1）代理模型学习基于先前CFD评估的ML生成模型误差；2）新模型先用代理评估，只有预测误差小或不确定性高的才进行完整CFD模拟；3）将离散符号表达式映射到连续空间；4）扩展为多输出代理模型支持多目标训练；5）基于概率输出选择最优训练设置。

Result: 在一维和二维统计流动（包括单表达式和多表达式模型优化）中，该框架显著降低了训练成本，同时保持了与原CFD驱动方法相当的预测精度。

Conclusion: 提出的代理增强CFD驱动训练框架能够有效降低训练计算成本，同时保持物理一致性模型的预测准确性，为复杂流动的ML-CFD集成建模提供了实用解决方案。

Abstract: Computational Fluid Dynamics (CFD)-driven training combines machine learning (ML) with CFD solvers to develop physically consistent closure models with improved predictive accuracy. In the original framework, each ML-generated candidate model is embedded in a CFD solver and evaluated against reference data, requiring hundreds to thousands of high-fidelity simulations and resulting in prohibitive computational cost for complex flows. To overcome this limitation, we propose an extended framework that integrates surrogate modeling into symbolic CFD-driven training in real time to reduce training cost. The surrogate model learns to approximate the errors of ML-generated models based on previous CFD evaluations and is continuously refined during training. Newly generated models are first assessed using the surrogate, and only those predicted to yield small errors or high uncertainty are subsequently evaluated with full CFD simulations. Discrete expressions generated by symbolic regression are mapped into a continuous space using averaged input-symbol values as inputs to a probabilistic surrogate model. To support multi-objective model training, particularly when fixed weighting of competing quantities is challenging, the surrogate is extended to a multi-output formulation by generalizing the kernel to a matrix form, providing one mean and variance prediction per training objective. Selection metrics based on these probabilistic outputs are used to identify an optimal training setup. The proposed surrogate-augmented CFD-driven training framework is demonstrated across a range of statistically one- and two-dimensional flows, including both single- and multi-expression model optimization. In all cases, the framework substantially reduces training cost while maintaining predictive accuracy comparable to that of the original CFD-driven approach.

</details>


### [113] [Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building](https://arxiv.org/abs/2512.19038)
*Liping Sun,Yucheng Guo,Siliang Lu,Zhenzhen Li*

Main category: cs.LG

TL;DR: 开发时间序列预测模型，预测美国建筑区域空气温度，支持2周时间范围的预测，用于HVAC系统智能控制和混合建筑能源建模。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化导致极端天气频发，需要更节能、灵活的HVAC系统控制来维持室内舒适环境并减少建筑对气候变化的贡献，这要求快速建模和预测建筑区域空气温度。

Method: 采用结合物理模型和数据驱动的混合方法，开发时间序列预测模型，专注于短期和长期预测（2周时间范围）。

Result: 开发了能够预测美国建筑区域空气温度的模型，支持2周时间范围的预测。

Conclusion: 该模型可进一步改进以支持HVAC系统的智能控制和运行（需求灵活性），并可用于混合建筑能源建模。

Abstract: With the press of global climate change, extreme weather and sudden weather changes are becoming increasingly common. To maintain a comfortable indoor environment and minimize the contribution of the building to climate change as much as possible, higher requirements are placed on the operation and control of HVAC systems, e.g., more energy-efficient and flexible to response to the rapid change of weather. This places demands on the rapid modeling and prediction of zone air temperatures of buildings. Compared to the traditional simulation-based approach such as EnergyPlus and DOE2, a hybrid approach combined physics and data-driven is more suitable. Recently, the availability of high-quality datasets and algorithmic breakthroughs have driven a considerable amount of work in this field. However, in the niche of short- and long-term predictions, there are still some gaps in existing research. This paper aims to develop a time series forecast model to predict the zone air temperature in a building located in America on a 2-week horizon. The findings could be further improved to support intelligent control and operation of HVAC systems (i.e. demand flexibility) and could also be used as hybrid building energy modeling.

</details>


### [114] [Efficient Personalization of Generative Models via Optimal Experimental Design](https://arxiv.org/abs/2512.19057)
*Guy Schacht,Ziyad Sheebaelhamd,Riccardo De Santi,Mojmír Mutný,Andreas Krause*

Main category: cs.LG

TL;DR: 提出基于最优实验设计的偏好查询选择方法ED-PBRL，通过最大化潜在偏好模型信息来高效选择人类反馈查询，减少对齐生成模型所需的人工标注成本


<details>
  <summary>Details</summary>
Motivation: 人类反馈获取成本高且耗时，需要数据高效的查询选择方法来对齐生成模型与用户需求

Method: 将偏好查询选择问题形式化为最大化潜在偏好模型信息，提出凸优化公式，开发统计和计算高效的ED-PBRL算法，支持图像、文本等结构化查询构建

Result: 通过个性化文本到图像生成模型到用户特定风格的实验，证明该方法相比随机查询选择需要更少的偏好查询

Conclusion: 提出的最优实验设计框架能够高效选择信息量最大的偏好查询，显著减少对齐生成模型所需的人类反馈数据量

Abstract: Preference learning from human feedback has the ability to align generative models with the needs of end-users. Human feedback is costly and time-consuming to obtain, which creates demand for data-efficient query selection methods. This work presents a novel approach that leverages optimal experimental design to ask humans the most informative preference queries, from which we can elucidate the latent reward function modeling user preferences efficiently. We formulate the problem of preference query selection as the one that maximizes the information about the underlying latent preference model. We show that this problem has a convex optimization formulation, and introduce a statistically and computationally efficient algorithm ED-PBRL that is supported by theoretical guarantees and can efficiently construct structured queries such as images or text. We empirically present the proposed framework by personalizing a text-to-image generative model to user-specific styles, showing that it requires less preference queries compared to random query selection.

</details>


### [115] [Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation](https://arxiv.org/abs/2512.19061)
*Chi Liu*

Main category: cs.LG

TL;DR: 提出基于图的欺诈检测框架，通过硬链接（身份关系）和软链接（行为关联）区分，采用图变换技术合并硬链接组件为超节点，构建加权软链接图进行嵌入和聚类，显著提升检测覆盖率。


<details>
  <summary>Details</summary>
Motivation: 协同欺诈中多个欺诈账户形成复杂网络结构，传统方法仅依赖高置信度身份链接导致覆盖有限，而使用所有链接又造成图碎片化降低聚类效果，需要解决大规模异构图聚类的挑战。

Method: 区分硬链接（电话、信用卡、身份证）和软链接（设备指纹、cookie、IP地址）；通过图变换技术：先识别硬链接连通组件并合并为超节点，再重建加权软链接图；使用LINE进行表示学习，HDBSCAN进行密度聚类。

Result: 在真实支付平台数据集上：图规模从2500万节点减少到770万节点；检测覆盖率相比仅用硬链接的基线方法翻倍；在识别的欺诈集群中保持高精度。

Conclusion: 该框架为工业级欺诈检测系统提供了可扩展的实用解决方案，通过链接变换有效处理大规模异构图聚类问题，显著提升协同欺诈检测效果。

Abstract: Collaborative fraud, where multiple fraudulent accounts coordinate to exploit online payment systems, poses significant challenges due to the formation of complex network structures. Traditional detection methods that rely solely on high-confidence identity links suffer from limited coverage, while approaches using all available linkages often result in fragmented graphs with reduced clustering effectiveness. In this paper, we propose a novel graph-based fraud detection framework that addresses the challenge of large-scale heterogeneous graph clustering through a principled link transformation approach. Our method distinguishes between \emph{hard links} (high-confidence identity relationships such as phone numbers, credit cards, and national IDs) and \emph{soft links} (behavioral associations including device fingerprints, cookies, and IP addresses). We introduce a graph transformation technique that first identifies connected components via hard links, merges them into super-nodes, and then reconstructs a weighted soft-link graph amenable to efficient embedding and clustering. The transformed graph is processed using LINE (Large-scale Information Network Embedding) for representation learning, followed by HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for density-based cluster discovery. Experiments on a real-world payment platform dataset demonstrate that our approach achieves significant graph size reduction (from 25 million to 7.7 million nodes), doubles the detection coverage compared to hard-link-only baselines, and maintains high precision across identified fraud clusters. Our framework provides a scalable and practical solution for industrial-scale fraud detection systems.

</details>


### [116] [DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale](https://arxiv.org/abs/2512.19097)
*Danny Dongyeop Han,Yonghyeon Gwon,Ahhyun Lucy Lee,Taeyang Lee,Seong Jin Lee,Jubin Choi,Sebin Lee,Jihyun Bang,Seungju Lee,David Keetae Park,Shinjae Yoo,Chun Kee Chung,Jiook Cha*

Main category: cs.LG

TL;DR: DIVER-1是迄今为止最大的EEG和iEEG基础模型家族，在5.3k小时iEEG和54k小时EEG数据上训练，参数达18.2亿，首次系统分析了该领域的缩放规律，发现数据受限的缩放规律：在给定数据和计算资源下，较小模型长时间训练优于大模型短期训练。


<details>
  <summary>Details</summary>
Motivation: 现有电生理学基础模型规模有限，尽管有明确证据表明缩放能提升性能。EEG和iEEG信号对神经科学、脑机接口和临床应用至关重要，需要更大规模、更多样化的模型。

Method: 构建了迄今为止最大最多样化的电生理学数据集（5.3k小时iEEG和54k小时EEG，来自1.77万+受试者）。设计了架构创新：任意变量注意力、滑动时间条件位置编码、多域重建。首次系统分析了电生理学领域的缩放规律。

Result: DIVER-1 iEEG和EEG模型在各自基准测试中均达到最先进性能。发现了数据受限的缩放规律：在给定数据和计算资源下，较小模型长时间训练优于大模型短期训练，这与之前强调模型大小而非训练时长的做法形成对比。

Conclusion: DIVER-1为电生理学基础模型开发提供了高效的缩放和资源分配指导原则。模型家族在各自领域达到SOTA性能，为未来电生理学基础模型的发展提供了重要参考。

Abstract: Electrophysiology signals such as EEG and iEEG are central to neuroscience, brain-computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of EEG and iEEG foundation models trained on the largest and most diverse corpus to date-5.3k hours of iEEG and 54k hours of EEG (1.6M channel-hours from over 17.7k subjects)-and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: for a given amount of data and compute, smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we also design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 iEEG and EEG models each achieve state-of-the-art performance on their respective benchmarks, establishing a concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.

</details>


### [117] [Dual Model Deep Learning for Alzheimer Prognostication](https://arxiv.org/abs/2512.19099)
*Alireza Moayedikia,Sara Fin,Uffe Kock Wiil*

Main category: cs.LG

TL;DR: PROGRESS是一个深度学习框架，使用单次脑脊液生物标志物评估预测阿尔茨海默病进展，无需纵向数据，提供不确定性量化，显著优于传统生存预测方法。


<details>
  <summary>Details</summary>
Motivation: 当前阿尔茨海默病治疗需要精确的时间决策，但现有预测模型需要纵向观察且缺乏不确定性量化，在关键的首诊时无法提供实用指导。

Method: 开发了PROGRESS双模型深度学习框架：概率轨迹网络预测个体化认知衰退并量化不确定性；深度生存模型估计从轻度认知障碍到痴呆的转化时间。使用NACC数据库中43个中心的3000多名参与者数据。

Result: PROGRESS在生存预测上显著优于Cox比例风险、随机生存森林和梯度提升方法。风险分层显示患者组间转化率有7倍差异。留一中心验证显示跨站点稳健泛化能力。

Conclusion: PROGRESS通过结合优越的生存预测和可信的轨迹不确定性量化，填补了生物标志物测量与个体化临床决策之间的空白。

Abstract: Disease modifying therapies for Alzheimer's disease demand precise timing decisions, yet current predictive models require longitudinal observations and provide no uncertainty quantification, rendering them impractical at the critical first visit when treatment decisions must be made. We developed PROGRESS (PRognostic Generalization from REsting Static Signatures), a dual-model deep learning framework that transforms a single baseline cerebrospinal fluid biomarker assessment into actionable prognostic estimates without requiring prior clinical history. The framework addresses two complementary clinical questions: a probabilistic trajectory network predicts individualized cognitive decline with calibrated uncertainty bounds achieving near-nominal coverage, enabling honest prognostic communication; and a deep survival model estimates time to conversion from mild cognitive impairment to dementia. Using data from over 3,000 participants across 43 Alzheimer's Disease Research Centers in the National Alzheimer's Coordinating Center database, PROGRESS substantially outperforms Cox proportional hazards, Random Survival Forests, and gradient boosting methods for survival prediction. Risk stratification identifies patient groups with seven-fold differences in conversion rates, enabling clinically meaningful treatment prioritization. Leave-one-center-out validation demonstrates robust generalizability, with survival discrimination remaining strong across held-out sites despite heterogeneous measurement conditions spanning four decades of assay technologies. By combining superior survival prediction with trustworthy trajectory uncertainty quantification, PROGRESS bridges the gap between biomarker measurement and personalized clinical decision-making.

</details>


### [118] [Timely Parameter Updating in Over-the-Air Federated Learning](https://arxiv.org/abs/2512.19103)
*Jiaqi Zhu,Zhongyuan Zhao,Xiao Li,Ruihao Du,Shi Jin,Howard H. Yang*

Main category: cs.LG

TL;DR: 提出FAIR-k算法，结合Round-Robin和Top-k策略，在联邦学习的空中计算框架中平衡参数新鲜度和梯度重要性，解决正交波形有限与高维模型不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中空中计算(OAC)能缓解通信瓶颈，但实际系统中正交波形数量有限，与现代深度学习模型的高维度不匹配，需要选择最重要的梯度子集进行空中更新。

Method: 提出FAIR-k算法，每轮通信选择最具影响力的梯度子集进行空中更新。算法结合Round-Robin（保证新鲜度）和Top-k（基于梯度重要性）的优势，使用马尔可夫分析工具刻画参数陈旧性分布。

Result: 建立了FAIR-k在OAC-FL中的收敛率分析，揭示了数据异构性、信道噪声和参数陈旧性对训练效率的联合影响。FAIR-k促进新鲜参数更新，不仅加速收敛，还能通过延长本地训练时间提升通信效率。

Conclusion: FAIR-k算法在有限正交波形条件下有效平衡参数更新新鲜度和重要性，为OAC-FL系统提供了理论保证和实用解决方案，显著提升训练效率和通信效率。

Abstract: Incorporating over-the-air computations (OAC) into the model training process of federated learning (FL) is an effective approach to alleviating the communication bottleneck in FL systems. Under OAC-FL, every client modulates its intermediate parameters, such as gradient, onto the same set of orthogonal waveforms and simultaneously transmits the radio signal to the edge server. By exploiting the superposition property of multiple-access channels, the edge server can obtain an automatically aggregated global gradient from the received signal. However, the limited number of orthogonal waveforms available in practical systems is fundamentally mismatched with the high dimensionality of modern deep learning models. To address this issue, we propose Freshness Freshness-mAgnItude awaRe top-k (FAIR-k), an algorithm that selects, in each communication round, the most impactful subset of gradients to be updated over the air. In essence, FAIR-k combines the complementary strengths of the Round-Robin and Top-k algorithms, striking a delicate balance between timeliness (freshness of parameter updates) and importance (gradient magnitude). Leveraging tools from Markov analysis, we characterize the distribution of parameter staleness under FAIR-k. Building on this, we establish the convergence rate of OAC-FL with FAIR-k, which discloses the joint effect of data heterogeneity, channel noise, and parameter staleness on the training efficiency. Notably, as opposed to conventional analyses that assume a universal Lipschitz constant across all the clients, our framework adopts a finer-grained model of the data heterogeneity. The analysis demonstrates that since FAIR-k promotes fresh (and fair) parameter updates, it not only accelerates convergence but also enhances communication efficiency by enabling an extended period of local training without significantly affecting overall training efficiency.

</details>


### [119] [HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction](https://arxiv.org/abs/2512.19114)
*Haoyu Jiang,Boan Qu,Junjie Zhu,Fanjie Zeng,Xiaojie Lin,Wei Zhong*

Main category: cs.LG

TL;DR: HyperLoad：基于大语言模型的跨模态框架，解决绿色数据中心小样本负载预测问题，通过知识对齐和多尺度特征建模提升预测精度


<details>
  <summary>Details</summary>
Motivation: 人工智能快速发展导致数据中心能耗和碳排放激增，绿色数据中心需要实现分钟级可再生能源、储能和负载协调，但现有方法难以应对冷启动、负载失真、数据碎片化和分布偏移等小样本场景

Method: 提出HyperLoad跨模态框架：1) 跨模态知识对齐阶段，将文本先验和时间序列数据映射到共同潜在空间；2) 多尺度特征建模阶段，通过自适应前缀调优注入领域对齐先验，使用增强全局交互注意力机制捕获跨设备时序依赖

Result: 在公开数据集DCData上测试，无论在数据充足还是数据稀缺场景下，HyperLoad均超越现有最佳基线方法，展示了其在可持续绿色数据中心管理中的实用性

Conclusion: HyperLoad通过利用预训练大语言模型克服数据稀缺问题，为绿色数据中心负载预测提供了有效的跨模态解决方案，有助于实现可持续数据中心管理

Abstract: The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.

</details>


### [120] [A Composable Channel-Adaptive Architecture for Seizure Classification](https://arxiv.org/abs/2512.19123)
*Francesco Carzaniga,Michael Hersche,Kaspar Schindler,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出通道自适应架构，可处理任意通道数的多变量时间序列（特别是iEEG），通过单通道处理+向量符号融合+长时记忆实现分类，支持跨受试者预训练和快速个性化微调。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在处理多通道iEEG时面临的通道数固定、跨受试者训练困难、以及缺乏临床相关长度时间上下文的问题。

Method: 1) 对每个通道独立应用SOTA模型处理；2) 使用向量符号算法融合特征，通过可训练标量重建空间关系；3) 在长达2分钟的长时记忆中累积特征进行分类；4) 支持跨异质受试者大规模预训练，然后通过少量数据快速微调个性化。

Result: 在癫痫检测任务中，CA-EEGWaveNet仅使用测试对象的一次癫痫发作数据训练，而基线使用除一次外的所有数据，CA模型仍超越基线（中位F1分数0.78 vs 0.76）。CA-EEGNet同样优于其基线（0.79 vs 0.74）。

Conclusion: CA模型解决了通道自适应性和跨受试者训练问题，同时将有效时间上下文扩展到临床相关长度，可作为现有模型的直接替代方案，带来更好的特性和性能。

Abstract: Objective: We develop a channel-adaptive (CA) architecture that seamlessly processes multi-variate time-series with an arbitrary number of channels, and in particular intracranial electroencephalography (iEEG) recordings. Methods: Our CA architecture first processes the iEEG signal using state-of-the-art models applied to each single channel independently. The resulting features are then fused using a vector-symbolic algorithm which reconstructs the spatial relationship using a trainable scalar per channel. Finally, the fused features are accumulated in a long-term memory of up to 2 minutes to perform the classification. Each CA-model can then be pre-trained on a large corpus of iEEG recordings from multiple heterogeneous subjects. The pre-trained model is personalized to each subject via a quick fine-tuning routine, which uses equal or lower amounts of data compared to existing state-of-the-art models, but requiring only 1/5 of the time. Results: We evaluate our CA-models on a seizure detection task both on a short-term (~20 hours) and a long-term (~2500 hours) dataset. In particular, our CA-EEGWaveNet is trained on a single seizure of the tested subject, while the baseline EEGWaveNet is trained on all but one. Even in this challenging scenario, our CA-EEGWaveNet surpasses the baseline in median F1-score (0.78 vs 0.76). Similarly, CA-EEGNet based on EEGNet, also surpasses its baseline in median F1-score (0.79 vs 0.74). Conclusion and significance: Our CA-model addresses two issues: first, it is channel-adaptive and can therefore be trained across heterogeneous subjects without loss of performance; second, it increases the effective temporal context size to a clinically-relevant length. Therefore, our model is a drop-in replacement for existing models, bringing better characteristics and performance across the board.

</details>


### [121] [RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling](https://arxiv.org/abs/2512.19147)
*Haoran Yang,Yinan Zhang,Wenjie Zhang,Dongxia Wang,Peiyu Liu,Yuqi Ye,Kexin Chen,Wenhai Wang*

Main category: cs.LG

TL;DR: 提出RP-CATE模型，结合通道注意力、循环感知机和Transformer，用于工业混合建模，解决现有方法架构单一和未充分利用数据潜在关联的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工业混合建模方法存在两个主要局限：1）主要使用单一机器学习方法处理特定任务，缺乏适合建模任务的综合架构；2）未充分利用工业数据中的潜在关联（如单调性、周期性），影响预测性能。

Method: 提出RP-CATE模型：1）用通道注意力替代自注意力机制，并加入循环感知机模块；2）提出伪图像数据类型及其生成方法；3）提出伪序列数据概念及转换方法，使模型能更好捕获数据关联。

Result: 在化学工程混合建模实验中，RP-CATE相比其他基线模型取得了最佳性能。

Conclusion: RP-CATE通过创新的架构设计和数据处理方法，有效解决了工业混合建模中的现有局限，提升了模型性能和实用性。

Abstract: Nowadays, industrial hybrid modeling which integrates both mechanistic modeling and machine learning-based modeling techniques has attracted increasing interest from scholars due to its high accuracy, low computational cost, and satisfactory interpretability. Nevertheless, the existing industrial hybrid modeling methods still face two main limitations. First, current research has mainly focused on applying a single machine learning method to one specific task, failing to develop a comprehensive machine learning architecture suitable for modeling tasks, which limits their ability to effectively represent complex industrial scenarios. Second, industrial datasets often contain underlying associations (e.g., monotonicity or periodicity) that are not adequately exploited by current research, which can degrade model's predictive performance. To address these limitations, this paper proposes the Recurrent Perceptron-based Channel Attention Transformer Encoder (RP-CATE), with three distinctive characteristics: 1: We developed a novel architecture by replacing the self-attention mechanism with channel attention and incorporating our proposed Recurrent Perceptron (RP) Module into Transformer, achieving enhanced effectiveness for industrial modeling tasks compared to the original Transformer. 2: We proposed a new data type called Pseudo-Image Data (PID) tailored for channel attention requirements and developed a cyclic sliding window method for generating PID. 3: We introduced the concept of Pseudo-Sequential Data (PSD) and a method for converting industrial datasets into PSD, which enables the RP Module to capture the underlying associations within industrial dataset more effectively. An experiment aimed at hybrid modeling in chemical engineering was conducted by using RP-CATE and the experimental results demonstrate that RP-CATE achieves the best performance compared to other baseline models.

</details>


### [122] [Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments](https://arxiv.org/abs/2512.19154)
*Geraud Nangue Tasse,Matthew Riemer,Benjamin Rosman,Tim Klinger*

Main category: cs.LG

TL;DR: 提出Adaptive Stacking元算法，通过自适应维护较小记忆堆栈来处理高度非马尔可夫环境，显著降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境中存在高度非马尔可夫依赖关系，传统帧堆叠方法需要大量历史观测，导致计算和内存成本过高。作者观察到许多环境中，环境仅因果依赖于相对较少的观测，因此需要自适应记忆管理方法。

Method: 提出Adaptive Stacking元算法，能够自适应维护较小的记忆堆栈，学习移除对未来奖励无预测性的记忆，同时保留重要经验。为MLP、LSTM和Transformer智能体提供收敛保证，并量化计算和内存节省。

Result: 在可控非马尔可夫依赖程度的内存任务实验中，验证了该元算法能够有效学习移除无关记忆，同时保持重要经验，实现显著的计算和内存节省。

Conclusion: Adaptive Stacking为处理高度非马尔可夫环境提供了一种高效解决方案，通过自适应记忆管理在保证性能的同时大幅降低资源需求，适用于多种神经网络架构。

Abstract: Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking

</details>


### [123] [Practical Quantum-Classical Feature Fusion for complex data Classification](https://arxiv.org/abs/2512.19180)
*Azadeh Alavi,Fatemeh Kouchmeshki,Abdolrahman Alavi*

Main category: cs.LG

TL;DR: 提出了一种跨注意力中间融合架构，将量子特征与经典神经网络通过注意力机制融合，相比传统混合方法在复杂数据集上表现更优


<details>
  <summary>Details</summary>
Motivation: 现有混合量子-经典学习方法通常将量子电路作为孤立特征提取器，通过直接拼接方式与经典表示合并，忽略了量子与经典分支是不同的计算模态，限制了在复杂高维表格和半结构化数据上的可靠性能

Method: 提出多模态混合学习框架，采用跨注意力中间融合架构，其中经典表示通过带有残差连接的注意力块查询量子派生的特征标记，量子分支保持在实用的NISQ预算内（最多9个量子比特）

Result: 在Wine、Breast Cancer、Forest CoverType、FashionMNIST和SteelPlatesFaults数据集上评估，纯量子模型和标准混合设计由于测量引起的信息丢失而表现不佳，而跨注意力中间融合模型在大多数情况下在更复杂数据集上表现更优

Conclusion: 量子派生信息通过原则性的多模态融合集成时最有价值，而不是孤立使用或松散附加到经典特征上

Abstract: Hybrid quantum and classical learning aims to couple quantum feature maps with the robustness of classical neural networks, yet most architectures treat the quantum circuit as an isolated feature extractor and merge its measurements with classical representations by direct concatenation. This neglects that the quantum and classical branches constitute distinct computational modalities and limits reliable performance on complex, high dimensional tabular and semi structured data, including remote sensing, environmental monitoring, and medical diagnostics. We present a multimodal formulation of hybrid learning and propose a cross attention mid fusion architecture in which a classical representation queries quantum derived feature tokens through an attention block with residual connectivity. The quantum branch is kept within practical NISQ budgets and uses up to nine qubits. We evaluate on Wine, Breast Cancer, Forest CoverType, FashionMNIST, and SteelPlatesFaults, comparing a quantum only model, a classical baseline, residual hybrid models, and the proposed mid fusion model under a consistent protocol. Pure quantum and standard hybrid designs underperform due to measurement induced information loss, while cross attention mid fusion is consistently competitive and improves performance on the more complex datasets in most cases. These findings suggest that quantum derived information becomes most valuable when integrated through principled multimodal fusion rather than used in isolation or loosely appended to classical features.

</details>


### [124] [Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning](https://arxiv.org/abs/2512.19184)
*Mahdi Mohammadigohari,Giuseppe Di Fatta,Giuseppe Nicosia,Panos M. Pardalos*

Main category: cs.LG

TL;DR: 本文提出了向量值神经网络和深度核方法的新泛化界，通过算子理论框架专注于多任务学习，结合Koopman方法和草图技术，建立了更紧的泛化保证，并提出了深度向量值再生核希尔伯特空间框架。


<details>
  <summary>Details</summary>
Motivation: 多任务学习在深度学习架构中的泛化性质相对未被充分探索，传统基于范数的泛化界不够紧，且Koopman方法存在计算挑战，需要新的理论框架来提供更好的性能保证。

Method: 1) 将Koopman方法与现有技术结合，建立更紧的泛化界；2) 引入适用于向量值神经网络的草图技术以降低计算复杂度；3) 提出深度向量值再生核希尔伯特空间框架，利用Perron-Frobenius算子增强深度核方法；4) 推导新的Rademacher泛化界，并通过核精化策略处理欠拟合和过拟合问题。

Result: 获得了比传统范数界更紧的泛化保证，在通用Lipschitz损失下得到超额风险界，适用于鲁棒回归和多分位数回归等应用，为多任务学习提供了新的理论见解。

Conclusion: 这项工作为多任务学习在深度学习架构中的泛化性质提供了新的理论框架和见解，通过算子理论方法、草图技术和深度向量值再生核希尔伯特空间框架，解决了现有方法的局限性，推动了该领域的发展。

Abstract: This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework, deep vector-valued reproducing kernel Hilbert spaces (vvRKHS), leveraging Perron Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multitask learning with deep learning architectures, an area that has been relatively unexplored until recent developments.

</details>


### [125] [Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction](https://arxiv.org/abs/2512.19194)
*Leming Zhou,Zuo Wang,Zhigang Liu*

Main category: cs.LG

TL;DR: 提出一种基于因果异构图表示学习的COPD共病风险预测方法，通过构建患者-疾病交互的异构图，结合因果推理机制，提高COPD急性加重的早期识别能力。


<details>
  <summary>Details</summary>
Motivation: 基层医疗对慢性阻塞性肺疾病（COPD）急性加重的早期识别和预警能力不足，导致患病率高、负担重但筛查率低，需要改进这一现状。

Method: 开发因果异构图表示学习（CHGRL）方法：1）构建患者与疾病交互的异构图数据集；2）设计因果感知的异构图学习架构，将因果推理机制与异构图学习结合；3）在模型设计中加入因果损失函数，在交叉熵分类损失基础上增加反事实推理学习损失和因果正则化损失。

Result: 通过实验评估，与强GNN基线模型相比，提出的模型展现出较高的检测准确率。

Conclusion: CHGRL方法能够有效提高COPD共病风险预测的准确性，有助于改善COPD急性加重的早期识别和预警能力。

Abstract: Due to the insufficient diagnosis and treatment capabilities at the grassroots level, there are still deficiencies in the early identification and early warning of acute exacerbation of Chronic obstructive pulmonary disease (COPD), often resulting in a high prevalence rate and high burden, but the screening rate is relatively low. In order to gradually improve this situation. In this paper, this study develop a Causal Heterogeneous Graph Representation Learning (CHGRL) method for COPD comorbidity risk prediction method that: a) constructing a heterogeneous Our dataset includes the interaction between patients and diseases; b) A cause-aware heterogeneous graph learning architecture has been constructed, combining causal inference mechanisms with heterogeneous graph learning, which can support heterogeneous graph causal learning for different types of relationships; and c) Incorporate the causal loss function in the model design, and add counterfactual reasoning learning loss and causal regularization loss on the basis of the cross-entropy classification loss. We evaluate our method and compare its performance with strong GNN baselines. Following experimental evaluation, the proposed model demonstrates high detection accuracy.

</details>


### [126] [On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning](https://arxiv.org/abs/2512.19199)
*Mahdi Mohammadigohari,Giuseppe Di Fatta,Giuseppe Nicosia,Panos M. Pardalos*

Main category: cs.LG

TL;DR: 论文利用算子理论技术为多任务深度神经网络建立了泛化界，通过利用权重矩阵的小条件数和引入定制的Sobolev空间作为扩展假设空间，提出了比传统基于范数方法更紧的界。


<details>
  <summary>Details</summary>
Motivation: 现有基于范数的多任务深度神经网络泛化界不够紧致，需要更精确的理论框架来理解多任务深度学习的泛化性能，特别是在核方法背景下。

Method: 采用算子理论技术，利用权重矩阵的小条件数，引入定制的Sobolev空间作为扩展假设空间，建立比传统方法更紧的泛化界。

Result: 提出的泛化界比现有基于Koopman的界更优，即使在单输出设置下也有效，保持了灵活性、独立于网络宽度等关键优势。

Conclusion: 该框架为多任务深度学习的核方法背景提供了更精确的理论理解，通过算子理论方法实现了比传统范数方法更紧的泛化界。

Abstract: The paper establishes generalization bounds for multitask deep neural networks using operator-theoretic techniques. The authors propose a tighter bound than those derived from conventional norm based methods by leveraging small condition numbers in the weight matrices and introducing a tailored Sobolev space as an expanded hypothesis space. This enhanced bound remains valid even in single output settings, outperforming existing Koopman based bounds. The resulting framework maintains key advantages such as flexibility and independence from network width, offering a more precise theoretical understanding of multitask deep learning in the context of kernel methods.

</details>


### [127] [MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning](https://arxiv.org/abs/2512.19206)
*Tao Zhang,Ziqian Zeng,Hao Peng,Huiping Zhuang,Cen Chen*

Main category: cs.LG

TL;DR: MixKVQ：一种查询感知的混合精度KV缓存量化方法，在保持复杂推理性能的同时显著减少内存占用


<details>
  <summary>Details</summary>
Motivation: 长链思维推理显著提升了LLM能力，但带来了巨大的KV缓存内存和延迟开销。现有低比特量化方法在复杂推理任务上性能下降严重，需要更智能的量化策略。

Method: 提出MixKVQ方法：1）考虑关键通道的固有量化难度和与查询的相关性；2）使用轻量级查询感知算法识别并保留需要高精度的关键通道；3）对值缓存应用逐令牌量化

Result: 在复杂推理数据集上的实验表明，MixKVQ显著优于现有低比特方法，在显著减少内存占用的同时，性能可与全精度基线相媲美

Conclusion: MixKVQ是一种即插即用的高效KV缓存量化方法，通过查询感知的混合精度策略，在保持推理性能的同时大幅降低内存开销，为长链推理的实际部署提供了可行方案

Abstract: Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.

</details>


### [128] [Phase-space entropy at acquisition reflects downstream learnability](https://arxiv.org/abs/2512.19223)
*Xiu-Cheng Wang,Jun-Jie Zhanga,Nan Cheng,Long-Gang Pang,Taijiao Du,Deyu Meng*

Main category: cs.LG

TL;DR: 提出基于仪器分辨相空间的采集级标量ΔS_B，用于量化采集过程如何保留或破坏下游学习可用的信息，无需训练即可预测下游重建/识别难度。


<details>
  <summary>Details</summary>
Motivation: 现代学习系统处理跨领域变化的数据，但都依赖于测量中已有的结构。需要一种通用的、模态无关的方法来量化采集过程本身如何保留或破坏下游学习可用的信息。

Method: 提出基于仪器分辨相空间的采集级标量ΔS_B，直接量化采集过程在仪器尺度上如何混合或移除联合空间-频率结构。该方法无需训练即可评估采样几何。

Result: ΔS_B理论上能正确识别周期性采样的相空间相干性作为混叠的物理来源，恢复经典采样定理结果。在掩码图像分类、加速MRI和大规模MIMO等实验中，|ΔS_B|能一致地排名采样几何并预测下游重建/识别难度，无需训练。

Conclusion: 采集阶段的相空间熵反映了下游可学习性，使得能够在训练前选择候选采样策略，并作为跨模态信息保存的共享概念。

Abstract: Modern learning systems work with data that vary widely across domains, but they all ultimately depend on how much structure is already present in the measurements before any model is trained. This raises a basic question: is there a general, modality-agnostic way to quantify how acquisition itself preserves or destroys the information that downstream learners could use? Here we propose an acquisition-level scalar $ΔS_{\mathcal B}$ based on instrument-resolved phase space. Unlike pixelwise distortion or purely spectral errors that often saturate under aggressive undersampling, $ΔS_{\mathcal B}$ directly quantifies how acquisition mixes or removes joint space--frequency structure at the instrument scale. We show theoretically that \(ΔS_{\mathcal B}\) correctly identifies the phase-space coherence of periodic sampling as the physical source of aliasing, recovering classical sampling-theorem consequences. Empirically, across masked image classification, accelerated MRI, and massive MIMO (including over-the-air measurements), $|ΔS_{\mathcal B}|$ consistently ranks sampling geometries and predicts downstream reconstruction/recognition difficulty \emph{without training}. In particular, minimizing $|ΔS_{\mathcal B}|$ enables zero-training selection of variable-density MRI mask parameters that matches designs tuned by conventional pre-reconstruction criteria. These results suggest that phase-space entropy at acquisition reflects downstream learnability, enabling pre-training selection of candidate sampling policies and as a shared notion of information preservation across modalities.

</details>


### [129] [Regression generation adversarial network based on dual data evaluation strategy for industrial application](https://arxiv.org/abs/2512.19232)
*Zesen Wang,Yonggang Li,Lijuan Lan*

Main category: cs.LG

TL;DR: 提出基于多任务学习的回归GAN框架，通过将回归信息融入判别器和生成器，并采用浅层共享机制，解决工业软测量中数据不足问题，同时提升生成样本质量和算法效率。


<details>
  <summary>Details</summary>
Motivation: 工业软测量中数据不足问题影响模型可靠性，传统GAN未考虑标签与特征的映射关系，现有方法未能同时兼顾性能与效率。

Method: 提出多任务学习回归GAN框架：1) 将回归信息融入判别器和生成器；2) 实现判别器与回归器的浅层共享机制；3) 设计双重数据评估策略，使GAN生成更多样化样本。

Result: 在四个经典工业软测量案例（污水处理厂、地表水、CO2吸收塔、工业燃气轮机）中验证了方法的优越性，显著提升了生成样本质量和算法运行效率。

Conclusion: 所提方法能有效解决工业软测量中的数据不足问题，通过改进GAN框架同时提升生成样本质量和算法效率，增强了后续建模的泛化能力。

Abstract: Soft sensing infers hard-to-measure data through a large number of easily obtainable variables. However, in complex industrial scenarios, the issue of insufficient data volume persists, which diminishes the reliability of soft sensing. Generative Adversarial Networks (GAN) are one of the effective solutions for addressing insufficient samples. Nevertheless, traditional GAN fail to account for the mapping relationship between labels and features, which limits further performance improvement. Although some studies have proposed solutions, none have considered both performance and efficiency simultaneously. To address these problems, this paper proposes the multi-task learning-based regression GAN framework that integrates regression information into both the discriminator and generator, and implements a shallow sharing mechanism between the discriminator and regressor. This approach significantly enhances the quality of generated samples while improving the algorithm's operational efficiency. Moreover, considering the importance of training samples and generated samples, a dual data evaluation strategy is designed to make GAN generate more diverse samples, thereby increasing the generalization of subsequent modeling. The superiority of method is validated through four classic industrial soft sensing cases: wastewater treatment plants, surface water, $CO_2$ absorption towers, and industrial gas turbines.

</details>


### [130] [From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis](https://arxiv.org/abs/2512.19246)
*Moncef Garouani,Ayah Barhrhouj*

Main category: cs.LG

TL;DR: MetaSHAP是一个可扩展的半自动化可解释AI方法，使用元学习和Shapley值分析为超参数调优提供可操作的、数据集感知的洞察，基于超过900万个评估过的机器学习管道基准。


<details>
  <summary>Details</summary>
Motivation: 超参数调优是机器学习模型优化的基础步骤，但计算成本高昂。除了优化本身，理解超参数的相对重要性和相互作用对于高效的模型开发至关重要。

Method: MetaSHAP通过元学习和Shapley值分析，从历史配置中学习代理性能模型，使用基于SHAP的分析计算超参数交互作用，并从最有影响力的超参数中推导出可解释的调优范围。

Result: 在164个分类数据集和14个分类器的多样化基准上进行了实证验证，证明MetaSHAP能够产生可靠的重要性排名，并且在指导贝叶斯优化时能够获得有竞争力的性能。

Conclusion: MetaSHAP为实践者提供了不仅能够优先考虑哪些超参数需要调优，还能理解它们的方向性和相互作用的工具，从而提供可操作的调优洞察。

Abstract: Hyperparameters tuning is a fundamental, yet computationally expensive, step in optimizing machine learning models. Beyond optimization, understanding the relative importance and interaction of hyperparameters is critical to efficient model development. In this paper, we introduce MetaSHAP, a scalable semi-automated eXplainable AI (XAI) method, that uses meta-learning and Shapley values analysis to provide actionable and dataset-aware tuning insights. MetaSHAP operates over a vast benchmark of over 09 millions evaluated machine learning pipelines, allowing it to produce interpretable importance scores and actionable tuning insights that reveal how much each hyperparameter matters, how it interacts with others and in which value ranges its influence is concentrated. For a given algorithm and dataset, MetaSHAP learns a surrogate performance model from historical configurations, computes hyperparameters interactions using SHAP-based analysis, and derives interpretable tuning ranges from the most influential hyperparameters. This allows practitioners not only to prioritize which hyperparameters to tune, but also to understand their directionality and interactions. We empirically validate MetaSHAP on a diverse benchmark of 164 classification datasets and 14 classifiers, demonstrating that it produces reliable importance rankings and competitive performance when used to guide Bayesian optimization.

</details>


### [131] [Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems](https://arxiv.org/abs/2512.19250)
*Prathamesh Devadiga*

Main category: cs.LG

TL;DR: 本文评估了小型语言模型（约10亿参数）在编译器自动并行化任务中的表现，相比传统启发式方法，在11个真实内核上平均加速6.81倍，卷积操作峰值加速达43.25倍。


<details>
  <summary>Details</summary>
Motivation: 传统自动并行化编译器依赖僵化的启发式方法，难以应对现代异构系统的复杂性。需要探索语言模型作为推理引擎在复杂编译器优化任务中的潜力。

Method: 评估了三个小型语言模型（gemma3、llama3.2、qwen2.5），使用六种推理策略，在11个真实内核（来自科学计算、图算法和机器学习）上进行测试。系统与LLVM Polly、TVM和Triton等强大编译器基线进行对比。

Result: 在376次评估中，该方法平均加速6.81倍，卷积操作峰值加速达43.25倍。分析了可扩展性，使用多种消毒工具验证正确性，并在不同编译器和硬件平台上确认了鲁棒性。

Conclusion: 小型高效的语言模型可以作为复杂编译器优化任务的强大推理引擎，为编译器优化提供了新的有效途径。

Abstract: Traditional auto-parallelizing compilers, reliant on rigid heuristics, struggle with the complexity of modern heterogeneous systems. This paper presents a comprehensive evaluation of small (approximately 1B parameter) language-model-driven compiler auto-parallelization. We evaluate three models: gemma3, llama3.2, and qwen2.5, using six reasoning strategies across 11 real-world kernels drawn from scientific computing, graph algorithms, and machine learning. Our system is benchmarked against strong compiler baselines, including LLVM Polly, TVM, and Triton. Across 376 total evaluations, the proposed approach achieves an average speedup of 6.81x and a peak performance of 43.25x on convolution operations. We analyze scalability, verify correctness using multiple sanitizers, and confirm robustness across diverse compilers and hardware platforms. Our results demonstrate that small, efficient language models can serve as powerful reasoning engines for complex compiler optimization tasks.

</details>


### [132] [Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study](https://arxiv.org/abs/2512.19253)
*Carla Crivoi,Radu Tudor Ionescu*

Main category: cs.LG

TL;DR: 首次对混合量子-经典神经网络中的机器遗忘进行实证研究，发现量子模型支持有效遗忘，但效果受电路深度、纠缠结构和任务复杂度影响


<details>
  <summary>Details</summary>
Motivation: 机器遗忘在经典深度学习中得到广泛探索，但在变分量子电路和量子增强架构中的行为仍基本未被研究，需要建立量子机器遗忘的基准实证见解

Method: 将多种遗忘方法（基于梯度、蒸馏、正则化和认证技术）适配到量子设置，并针对混合模型引入两种新的遗忘策略，在Iris、MNIST和Fashion-MNIST数据集上进行实验

Result: 浅层VQC显示出高内在稳定性，记忆最小；而深层混合模型在效用、遗忘强度和与重训练基准对齐之间存在更强的权衡。某些方法（如EU-k、LCA和认证遗忘）在各项指标上提供最佳平衡

Conclusion: 量子模型可以支持有效遗忘，但结果强烈依赖于电路深度、纠缠结构和任务复杂度。这些发现为量子机器遗忘建立了基准实证见解，并强调需要量子感知算法和理论保证

Abstract: We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.

</details>


### [133] [Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals](https://arxiv.org/abs/2512.19280)
*Chang Dong,Jianfeng Tao,Chengliang Liu*

Main category: cs.LG

TL;DR: 提出基于数字孪生的零样本故障诊断框架，利用流体噪声信号，仅需健康状态数据校准数字孪生模型，生成合成故障信号训练深度学习分类器，实现超过95%的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 轴向柱塞泵是流体动力系统的关键部件，传统数据驱动方法需要大量标注故障数据（难以获取），而基于模型的方法受参数不确定性影响。需要在数据稀缺场景下实现可靠故障诊断。

Method: 1) 仅使用健康状态数据校准高保真数字孪生模型；2) 生成合成故障信号训练深度学习分类器；3) 使用物理信息神经网络作为虚拟传感器估计流量脉动；4) 集成Grad-CAM可视化神经网络决策过程，发现匹配时域输入子序列长度的大核和时频域输入的小核能提高诊断准确率。

Result: 校准后的数字孪生模型生成的信号训练的分类器在真实基准测试中诊断准确率超过95%，而未校准模型性能显著较低，验证了框架在数据稀缺场景下的有效性。

Conclusion: 提出的数字孪生驱动零样本故障诊断框架能够仅利用健康状态数据实现高精度故障诊断，通过物理信息特征提取和模型校准，解决了传统方法在数据稀缺场景下的局限性。

Abstract: Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.

</details>


### [134] [MAGIC: Achieving Superior Model Merging via Magnitude Calibration](https://arxiv.org/abs/2512.19320)
*Yayuan Li,Jian Zhang,Jintao Guo,Zihan Cheng,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.LG

TL;DR: 提出MAGIC框架，通过特征和权重空间的幅度校准来提升模型合并性能，无需额外训练


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法主要关注特征方向对齐，忽略了幅度的重要性。幅度在合并操作中容易受到扰动，导致特征偏差和性能下降

Method: 提出MAGIC框架，包含三种变体：特征空间校准(FSC)使用少量无标签数据重新对齐特征；权重空间校准(WSC)在权重空间进行校准无需额外数据；双重空间校准(DSC)结合两者

Result: 在计算机视觉任务上平均提升4.3%（8个数据集），在NLP任务上提升8.0%（Llama模型），无需额外训练

Conclusion: 特征幅度校准是模型合并中被忽视但关键的因素，MAGIC框架能有效提升合并模型性能，具有实用价值

Abstract: The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC

</details>


### [135] [Alternative positional encoding functions for neural transformers](https://arxiv.org/abs/2512.19323)
*Ezequiel Lopez-Rubio,Macoris Decena-Gimenez,Rafael Marcos Luque-Baena*

Main category: cs.LG

TL;DR: 提出了一种用于Transformer位置编码的替代性周期函数，相比原始正弦函数在某些实验中表现更优


<details>
  <summary>Details</summary>
Motivation: Transformer架构中的位置编码模块至关重要，目前广泛使用的正弦函数虽然有效，但可能存在改进空间。作者希望探索替代性的周期函数来更好地编码位置信息。

Method: 提出了一组替代性的周期函数用于位置编码，这些函数保留了正弦函数的关键特性，但在基本方式上有所不同。通过实验对比了这些新函数与原始正弦函数的性能。

Result: 初步实验表明，提出的替代性周期函数在性能上显著优于原始的正弦函数版本，显示出更好的效果。

Conclusion: 提出的替代性周期函数有潜力在更广泛的Transformer架构中得到应用，为位置编码提供了新的选择。

Abstract: A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.

</details>


### [136] [A Logical View of GNN-Style Computation and the Role of Activation Functions](https://arxiv.org/abs/2512.19332)
*Pablo Barceló,Floris Geerts,Matthias Lanzinger,Klara Pakhomenko,Jan Van den Bussche*

Main category: cs.LG

TL;DR: MPLang语言研究：无激活函数时表达力由walk-summed特征刻画；有界激活函数（满足温和条件）下，所有最终常数激活函数具有相同表达力；首次证明在存在线性层时，无界激活函数（如ReLU）比有界激活函数（如截断ReLU）具有更强的数值查询表达力。


<details>
  <summary>Details</summary>
Motivation: 研究MPLang（一种捕获图神经网络计算的声明式语言）的数值和布尔表达力，特别关注激活函数类型对表达力的影响，旨在理解不同激活函数在图神经网络中的表达能力差异。

Method: 1. 分析无激活函数的A-MPLang片段，用walk-summed特征刻画其表达力；2. 研究有界激活函数，证明在温和条件下所有最终常数激活函数具有相同表达力；3. 首次证明在存在线性层时，无界激活函数（如ReLU）比有界激活函数（如截断ReLU）具有更强的数值查询表达力。

Result: 1. A-MPLang的表达力可由walk-summed特征完全刻画；2. 有界激活函数中，所有最终常数激活函数具有相同的数值和布尔表达力；3. 首次建立了无界激活函数和有界激活函数在存在线性层时的表达力分离：ReLU比最终常数激活函数（如截断ReLU）具有更强的数值查询表达力。

Conclusion: 激活函数类型对图神经网络的表达力有重要影响：无激活函数时表达力有限；有界激活函数中最终常数激活函数具有相同表达力；但在存在线性层时，无界激活函数（如ReLU）比有界激活函数具有更强的表达力，这对图神经网络的设计有重要启示。

Abstract: We study the numerical and Boolean expressiveness of MPLang, a declarative language that captures the computation of graph neural networks (GNNs) through linear message passing and activation functions. We begin with A-MPLang, the fragment without activation functions, and give a characterization of its expressive power in terms of walk-summed features. For bounded activation functions, we show that (under mild conditions) all eventually constant activations yield the same expressive power - numerical and Boolean - and that it subsumes previously established logics for GNNs with eventually constant activation functions but without linear layers. Finally, we prove the first expressive separation between unbounded and bounded activations in the presence of linear layers: MPLang with ReLU is strictly more powerful for numerical queries than MPLang with eventually constant activation functions, e.g., truncated ReLU. This hinges on subtle interactions between linear aggregation and eventually constant non-linearities, and it establishes that GNNs using ReLU are more expressive than those restricted to eventually constant activations and linear layers.

</details>


### [137] [Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation](https://arxiv.org/abs/2512.19361)
*Isshaan Singh,Divyansh Chawla,Anshu Garg,Shivin Mangal,Pallavi Gupta,Khushi Agarwal,Nimrat Singh Khalsa,Nandan Patel*

Main category: cs.LG

TL;DR: 提出结合LSTM和RNN的混合强化学习框架，用于物联网食品供应链中的实时腐败预测，强调可解释性并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代物联网驱动的食品供应链中，易腐商品对环境条件高度敏感，需要智能实时腐败预测系统。现有方法缺乏对动态条件的适应性，且无法实时优化决策。

Method: 提出混合强化学习框架，整合LSTM和RNN来捕捉传感器数据的时间依赖性。采用基于规则的分类器环境提供透明的腐败级别标注，使用可解释性驱动的指标监控模型行为。

Result: 在模拟和实时硬件数据上的广泛评估表明，基于LSTM和RNN的智能体在预测准确性和决策效率上优于其他强化学习方法，同时保持可解释性。

Conclusion: 混合深度强化学习与集成可解释性在可扩展的物联网食品监控系统中具有巨大潜力，能够实现稳健、自适应的实时腐败预测。

Abstract: The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.

</details>


### [138] [From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples](https://arxiv.org/abs/2512.19363)
*Canran Xiao,Jiabao Dou,Zhiming Lin,Zong Ke,Liwei Hou*

Main category: cs.LG

TL;DR: HCDV提出分层对比数据估值框架，将经典Data-Shapley的O(n!)复杂度降至O(TK_max log n)，通过几何感知表示学习、分层聚类和局部蒙特卡洛博弈实现高效数据估值


<details>
  <summary>Details</summary>
Motivation: 经典Data-Shapley方法复杂度高（O(n!)）且采用点对点视角，难以适应现代大规模、异构和几何结构化的数据集，需要更高效的数据估值方法

Method: 三阶段框架：1) 学习对比保持几何结构的表示；2) 构建平衡的粗到细分层聚类；3) 通过局部蒙特卡洛博弈分配Shapley式收益，预算向下传播

Result: 在四个基准测试（表格、视觉、流式、4500万样本CTR任务）和OpenDataVal套件上，HCDV提升准确率高达+5个百分点，减少估值时间高达100倍，支持增强过滤、低延迟流式更新和公平市场支付等任务

Conclusion: HCDV是高效、可扩展的数据估值框架，近似满足Shapley四公理，显著降低计算复杂度，适用于大规模异构数据集，为数据增强过滤、流式更新和公平数据市场提供实用解决方案

Abstract: How should we quantify the value of each training example when datasets are large, heterogeneous, and geometrically structured? Classical Data-Shapley answers in principle, but its O(n!) complexity and point-wise perspective are ill-suited to modern scales. We propose Hierarchical Contrastive Data Valuation (HCDV), a three-stage framework that (i) learns a contrastive, geometry-preserving representation, (ii) organizes the data into a balanced coarse-to-fine hierarchy of clusters, and (iii) assigns Shapley-style payoffs to coalitions via local Monte-Carlo games whose budgets are propagated downward. HCDV collapses the factorial burden to O(T sum_{l} K_{l}) = O(T K_max log n), rewards examples that sharpen decision boundaries, and regularizes outliers through curvature-based smoothness. We prove that HCDV approximately satisfies the four Shapley axioms with surplus loss O(eta log n), enjoys sub-Gaussian coalition deviation tilde O(1/sqrt{T}), and incurs at most k epsilon_infty regret for top-k selection. Experiments on four benchmarks--tabular, vision, streaming, and a 45M-sample CTR task--plus the OpenDataVal suite show that HCDV lifts accuracy by up to +5 pp, slashes valuation time by up to 100x, and directly supports tasks such as augmentation filtering, low-latency streaming updates, and fair marketplace payouts.

</details>


### [139] [Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture](https://arxiv.org/abs/2512.19367)
*Christian Hägg,Kathlén Kohn,Giovanni Luca Marchetti,Boris Shapiro*

Main category: cs.LG

TL;DR: Sprecher Networks (SNs) 是一种受经典Kolmogorov-Arnold-Sprecher构造启发的新型神经网络架构，使用共享可学习样条和结构化块，相比MLPs和KANs具有更好的参数效率和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络架构如MLPs和KANs存在参数效率低和内存消耗大的问题。MLPs使用固定节点激活函数，KANs使用可学习的边缘激活函数，但两者在参数数量和内存使用上都不够高效。需要一种既能保持函数逼近能力，又能显著提高参数和内存效率的新架构。

Method: SNs采用共享可学习样条（单调和通用）在结构化块中，包含显式移位参数和混合权重。单层变体直接实现Sprecher 1965年的移位样条求和公式，并扩展到更深的多层组合。还引入了可选的横向混合连接，实现输出维度间的块内通信，作为全注意力机制的高效替代方案。

Result: SNs具有O(LN + LG)的参数复杂度（vs MLPs的O(LN^2)），并通过顺序评估策略将前向中间内存峰值从O(N^2)降低到O(N)。这使得在内存约束下可以构建更宽的架构。实验证明，将这些块组合成深度网络能产生高度参数和内存高效的模型。

Conclusion: Sprecher Networks提供了一种新颖的神经网络架构，在保持强大函数逼近能力的同时，显著提高了参数和内存效率。它直接实现了经典的Sprecher公式，并扩展到深层架构，为构建更高效的大规模神经网络提供了有前景的方向。

Abstract: We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).

</details>


### [140] [OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation](https://arxiv.org/abs/2512.19379)
*Xueming Yan,Boyan Xu,Yaochu Jin,Lixian Xiao,Wenlong Ye,Runyang Cai,Zeqi Zheng,Jingfa Liu,Aimin Yang*

Main category: cs.LG

TL;DR: 提出首个印尼语多模态情感识别基准数据集IndoMER和基于Qwen2.5-Omni的多模态适应框架OmniMER，通过辅助感知任务提升情感识别性能


<details>
  <summary>Details</summary>
Motivation: 印尼语作为东南亚社交媒体主要语言，拥有超过2亿使用者，但在多模态情感识别研究中服务不足，需要专门的数据集和方法

Method: 1) 构建IndoMER数据集：包含1,944个视频片段，203位说话者，七种情感类别，具有文本、音频、视觉对齐标注；2) 提出OmniMER框架：基于Qwen2.5-Omni，通过三个辅助感知任务（文本情感关键词提取、视频面部表情分析、音频韵律分析）增强多模态融合前的特征提取

Result: OmniMER在IndoMER数据集上取得0.582的宏平均F1（情感分类）和0.454（情感识别），分别比基线模型提升7.6和22.1个绝对百分点；在中文CH-SIMS数据集上的跨语言评估也证明了框架的泛化能力

Conclusion: IndoMER填补了印尼语多模态情感识别基准的空白，OmniMER框架通过辅助感知任务有效解决了跨模态不一致和长尾分布等现实挑战，为低资源多模态情感识别提供了有效解决方案

Abstract: Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER

</details>


### [141] [Real-Time Machine Learning for Embedded Anomaly Detection](https://arxiv.org/abs/2512.19383)
*Abdelmadjid Benmachiche,Khadija Rais,Hamda Slimi*

Main category: cs.LG

TL;DR: 这篇综述论文系统分析了在资源受限的IoT边缘设备上进行实时异常检测的机器学习方法，重点关注计算效率、内存和功耗的严格约束，为工程师提供了算法选择和部署的实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着资源受限的物联网环境和嵌入式设备的普及，边缘端实时异常检测面临巨大压力。需要在严格的延迟、内存和功耗约束下实现有效的异常检测，这促使研究者探索轻量级机器学习方法。

Method: 论文采用综述研究方法，系统比较了多种轻量级异常检测算法，包括隔离森林、一类支持向量机、循环架构和统计技术等。分析重点放在这些算法在嵌入式实现中的实际表现，特别关注计算效率与检测精度的权衡。

Result: 研究发现硬件约束从根本上重新定义了算法选择，不同算法在准确性和计算效率之间存在显著权衡。论文提供了根据设备配置选择算法的实用建议，并指出了TinyML等新兴趋势如何帮助缩小检测能力与嵌入式现实之间的差距。

Conclusion: 这篇论文为在带宽受限且可能涉及安全关键应用的边缘环境中部署异常检测的工程师提供了战略路线图，强调了在严格资源约束下算法选择的重要性，并展望了TinyML等新技术的发展前景。

Abstract: The spread of a resource-constrained Internet of Things (IoT) environment and embedded devices has put pressure on the real-time detection of anomalies occurring at the edge. This survey presents an overview of machine-learning methods aimed specifically at on-device anomaly detection with extremely strict constraints for latency, memory, and power consumption. Lightweight algorithms such as Isolation Forest, One-Class SVM, recurrent architectures, and statistical techniques are compared here according to the realities of embedded implementation. Our survey brings out significant trade-offs of accuracy and computational efficiency of detection, as well as how hardware constraints end up fundamentally redefining algorithm choice. The survey is completed with a set of practical recommendations on the choice of the algorithm depending on the equipment profiles and new trends in TinyML, which can help close the gap between detection capabilities and embedded reality. The paper serves as a strategic roadmap for engineers deploying anomaly detection in edge environments that are constrained by bandwidth and may be safety-critical.

</details>


### [142] [Brain-Grounded Axes for Reading and Steering LLM States](https://arxiv.org/abs/2512.19399)
*Sandro Andric*

Main category: cs.LG

TL;DR: 使用人类大脑活动作为坐标系统来解读和调控大语言模型状态，通过脑电图数据构建词汇级大脑图谱，训练轻量级适配器将LLM隐藏状态映射到大脑轴，实现基于神经生理学的可解释控制


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型可解释性方法通常依赖文本监督，缺乏外部基础。本研究提出使用人类大脑活动作为坐标系统，为LLM状态提供神经生理学基础，实现更可靠的可解释性和控制

Method: 使用SMN4Lang MEG数据集构建词汇级大脑相位锁定值图谱，通过独立成分分析提取潜在轴。训练轻量级适配器将LLM隐藏状态映射到这些大脑轴，无需微调LLM本身。通过控制实验验证方法的稳健性

Result: 成功识别出稳健的词汇频率轴（与TinyLlama中间层相关），以及功能/内容轴（在TinyLlama、Qwen2-0.5B和GPT-2中表现一致）。大脑轴相比文本探针产生更大的对数频率偏移且困惑度更低。轴结构在不同嵌入方法下保持稳定

Conclusion: 神经生理学基础轴为LLM行为提供了可解释且可控的操作界面，支持使用大脑活动作为外部坐标系统来解读和调控语言模型状态，为可解释AI提供了新途径

Abstract: Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.

</details>


### [143] [Symplectic Reservoir Representation of Legendre Dynamics](https://arxiv.org/abs/2512.19409)
*Robert Simon Fong,Gouhei Tanaka,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 提出一种在表示层面保持辛几何约束的学习系统——辛水库（SR），通过勒让德对偶性实现哈密顿系统的辛结构保持，确保表示演化过程中保持勒让德图结构。


<details>
  <summary>Details</summary>
Motivation: 现代学习系统基于数据的内部表示，但这些表示如何编码底层物理或统计结构往往不明确。物理学中哈密顿系统的守恒律（如辛性）保证长期稳定性，现有工作主要在损失或输出层面硬编码这些约束。本文探索在表示层面本身遵守辛守恒律的意义。

Method: 通过勒让德对偶性表达辛约束：原始参数与对偶参数的配对构成表示必须保持的结构。形式化勒让德动力学为保持在勒让德图上的随机过程。几何上证明保持所有勒让德图的映射正是余切丛的辛同胚。基于此设计辛水库（SR）架构，这是一种特殊的循环神经网络，其循环核心由哈密顿系统生成。

Result: 证明SR更新具有规范形式，能够将勒让德图传输到勒让德图，在每个时间步保持勒让德对偶性。该类包括线性时不变高斯过程回归和Ornstein-Uhlenbeck动力学。

Conclusion: 辛水库实现了几何约束的、保持勒让德对偶的表示映射，将辛几何和哈密顿力学直接注入表示层面，为学习系统提供了物理启发的表示结构。

Abstract: Modern learning systems act on internal representations of data, yet how these representations encode underlying physical or statistical structure is often left implicit. In physics, conservation laws of Hamiltonian systems such as symplecticity guarantee long-term stability, and recent work has begun to hard-wire such constraints into learning models at the loss or output level. Here we ask a different question: what would it mean for the representation itself to obey a symplectic conservation law in the sense of Hamiltonian mechanics?
  We express this symplectic constraint through Legendre duality: the pairing between primal and dual parameters, which becomes the structure that the representation must preserve. We formalize Legendre dynamics as stochastic processes whose trajectories remain on Legendre graphs, so that the evolving primal-dual parameters stay Legendre dual. We show that this class includes linear time-invariant Gaussian process regression and Ornstein-Uhlenbeck dynamics.
  Geometrically, we prove that the maps that preserve all Legendre graphs are exactly symplectomorphisms of cotangent bundles of the form "cotangent lift of a base diffeomorphism followed by an exact fibre translation". Dynamically, this characterization leads to the design of a Symplectic Reservoir (SR), a reservoir-computing architecture that is a special case of recurrent neural network and whose recurrent core is generated by Hamiltonian systems that are at most linear in the momentum.
  Our main theorem shows that every SR update has this normal form and therefore transports Legendre graphs to Legendre graphs, preserving Legendre duality at each time step. Overall, SR implements a geometrically constrained, Legendre-preserving representation map, injecting symplectic geometry and Hamiltonian mechanics directly at the representational level.

</details>


### [144] [Research Program: Theory of Learning in Dynamical Systems](https://arxiv.org/abs/2512.19410)
*Elad Hazan,Shai Shalev Shwartz,Nathan Srebro*

Main category: cs.LG

TL;DR: 论文提出从下一个标记预测角度研究动力系统可学习性的框架，将可学习性视为有限样本问题，基于系统动态特性而非序列统计特性，并在线性动力系统中展示了无需系统辨识的有限观测预测。


<details>
  <summary>Details</summary>
Motivation: 现代学习系统越来越多地与随时间演化且依赖隐藏内部状态的数据交互。核心问题是：何时仅从观测就能学习这样的动力系统？需要建立基于动态特性而非序列统计特性的可学习性理论框架。

Method: 提出动力系统可学习性的研究框架，将可学习性定义为有限样本问题，关注在有限预热期后每个时间步都能保持的均匀保证。引入动态可学习性概念，捕捉系统结构（如稳定性、混合性、可观测性、谱特性）如何影响可靠预测所需的观测数量。

Result: 在线性动力系统案例中证明，通过基于谱滤波的非适当方法，无需系统辨识即可在有限观测后实现准确预测。建立了动力系统学习与经典PAC、在线和通用预测理论的关系。

Conclusion: 提出了从下一个标记预测角度研究动力系统可学习性的系统框架，强调基于动态特性的有限样本分析的重要性，为研究非线性和受控系统提供了方向，并展示了在线性系统中无需辨识即可预测的可能性。

Abstract: Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.

</details>


### [145] [Attention Is Not What You Need](https://arxiv.org/abs/2512.19428)
*Zhang Chong*

Main category: cs.LG

TL;DR: 论文提出了一种基于Grassmann流形的无注意力架构，替代传统的自注意力机制，在语言建模和自然语言推理任务上取得了与Transformer相当的性能。


<details>
  <summary>Details</summary>
Motivation: 重新审视序列建模中的基本问题：显式自注意力是否是实现强大性能和推理能力的必要条件。作者认为标准多头注意力本质上是张量提升的一种形式，虽然表达能力极强但数学上不透明，难以用少量显式不变量描述深层模型。

Method: 提出基于Grassmann流的无注意力架构：Causal Grassmann层。该方法（1）线性降维token状态，（2）通过Plucker坐标将局部token对编码为Grassmann流形上的二维子空间，（3）通过门控混合将这些几何特征融合回隐藏状态。信息通过低秩子空间在多尺度局部窗口上的受控变形传播，核心计算在有限维流形而非非结构化张量空间中进行。

Result: 在Wikitext-2语言建模基准上，纯Grassmann模型（1300-1800万参数）的验证困惑度与规模匹配的Transformer相差约10-15%。在SNLI自然语言推理任务上，基于DistilBERT的Grassmann-Plucker头略优于Transformer头，最佳验证和测试准确率分别为0.8550/0.8538 vs 0.8545/0.8511。分析显示Grassmann混合复杂度在固定秩下随序列长度线性增长。

Conclusion: 基于流形的设计为神经推理的几何和不变量解释提供了更结构化的途径，表明显式自注意力并非实现强大序列建模性能的必要条件。

Abstract: We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.
  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.
  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.

</details>


### [146] [An Inverse Scattering Inspired Fourier Neural Operator for Time-Dependent PDE Learning](https://arxiv.org/abs/2512.19439)
*Rixin Yu*

Main category: cs.LG

TL;DR: 提出IS-FNO方法，通过结合逆散射变换的可逆性和谱演化结构，改进神经算子在非线性PDE长期预测中的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子方法（如FNO和Koopman扩展）在非线性PDE的长期预测中存在稳定性问题，特别是对于混沌、刚性和长时间尺度的动力系统。需要结合物理结构来提高长期预测的鲁棒性。

Method: 提出逆散射启发的傅里叶神经算子(IS-FNO)，通过可逆神经变换强制提升和投影映射之间的近可逆配对，使用指数傅里叶层建模潜在时间演化，自然编码线性和非线性谱动力学。

Result: 在多个基准PDE（包括Michelson-Sivashinsky、Kuramoto-Sivashinsky、KdV和KP方程）上评估，IS-FNO相比基线FNO和Koopman模型获得更低的短期误差和显著改善的长期稳定性。对于可积系统，嵌入解析散射结构的简化IS-FNO变体在有限模型容量下保持竞争力的长期准确性。

Conclusion: 将物理结构（特别是可逆性和谱演化）融入神经算子设计能显著增强非线性PDE动力学的鲁棒性和长期预测保真度。

Abstract: Learning accurate and stable time-advancement operators for nonlinear partial differential equations (PDEs) remains challenging, particularly for chaotic, stiff, and long-horizon dynamical systems. While neural operator methods such as the Fourier Neural Operator (FNO) and Koopman-inspired extensions achieve good short-term accuracy, their long-term stability is often limited by unconstrained latent representations and cumulative rollout errors. In this work, we introduce an inverse scattering inspired Fourier Neural Operator(IS-FNO), motivated by the reversibility and spectral evolution structure underlying the classical inverse scattering transform. The proposed architecture enforces a near-reversible pairing between lifting and projection maps through an explicitly invertible neural transformation, and models latent temporal evolution using exponential Fourier layers that naturally encode linear and nonlinear spectral dynamics. We systematically evaluate IS-FNO against baseline FNO and Koopman-based models on a range of benchmark PDEs, including the Michelson-Sivashinsky and Kuramoto-Sivashinsky equations (in one and two dimensions), as well as the integrable Korteweg-de Vries and Kadomtsev-Petviashvili equations. The results demonstrate that IS-FNO achieves lower short-term errors and substantially improved long-horizon stability in non-stiff regimes. For integrable systems, reduced IS-FNO variants that embed analytical scattering structure retain competitive long-term accuracy despite limited model capacity. Overall, this work shows that incorporating physical structure -- particularly reversibility and spectral evolution -- into neural operator design significantly enhances robustness and long-term predictive fidelity for nonlinear PDE dynamics.

</details>


### [147] [Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm](https://arxiv.org/abs/2512.19440)
*Antonio Consolo,Andrea Manno,Edoardo Amaldi*

Main category: cs.LG

TL;DR: 本文提出了一种新的稀疏核逻辑回归方法，通过扩展Keerthi等人的训练公式，在保持良好测试精度的同时诱导稀疏性，并设计了基于二阶信息的SMO分解算法。


<details>
  <summary>Details</summary>
Motivation: 核逻辑回归（KLR）虽然能提供类别成员概率估计，但通常缺乏稀疏性。现有方法如Import Vector Machine（IVM）和ℓ₁/₂正则化存在启发式或临时性，难以在预测精度和稀疏性之间取得良好平衡。

Method: 扩展Keerthi等人的二元KLR训练公式以诱导稀疏性，设计基于二阶信息的序列最小优化（SMO）分解算法来高效求解对偶问题，并证明其全局收敛性。

Result: 在12个文献数据集上的实验表明，该方法在精度和稀疏性之间取得了竞争性平衡，优于IVM、ℓ₁/₂正则化KLR和SVM，同时保留了提供类别成员概率估计的优势。

Conclusion: 提出的稀疏二元KLR方法在保持良好预测性能的同时实现了模型稀疏化，为实际应用提供了更高效的分类器，兼具概率估计能力和计算效率。

Abstract: Kernel logistic regression (KLR) is a widely used supervised learning method for binary and multi-class classification, which provides estimates of the conditional probabilities of class membership for the data points. Unlike other kernel methods such as Support Vector Machines (SVMs), KLRs are generally not sparse. Previous attempts to deal with sparsity in KLR include a heuristic method referred to as the Import Vector Machine (IVM) and ad hoc regularizations such as the $\ell_{1/2}$-based one. Achieving a good trade-off between prediction accuracy and sparsity is still a challenging issue with a potential significant impact from the application point of view. In this work, we revisit binary KLR and propose an extension of the training formulation proposed by Keerthi et al., which is able to induce sparsity in the trained model, while maintaining good testing accuracy. To efficiently solve the dual of this formulation, we devise a decomposition algorithm of Sequential Minimal Optimization type which exploits second-order information, and for which we establish global convergence. Numerical experiments conducted on 12 datasets from the literature show that the proposed binary KLR approach achieves a competitive trade-off between accuracy and sparsity with respect to IVM, $\ell_{1/2}$-based regularization for KLR, and SVM while retaining the advantages of providing informative estimates of the class membership probabilities.

</details>


### [148] [Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications](https://arxiv.org/abs/2512.19472)
*Lorenzo Capelli,Leandro de Souza Rosa,Gianluca Setti,Mauro Mangia,Riccardo Rovatti*

Main category: cs.LG

TL;DR: MACS是一个统一的后处理框架，通过分析中间激活层生成分类图，从中推导出可用于置信度估计、检测分布偏移和对抗攻击的分数，在VGG16和ViTb16模型上超越现有方法且计算开销更小。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的黑盒特性在关键领域引发透明度和可信度担忧，现有方法要么需要重新训练模型，要么只能独立解决置信度、分布偏移或对抗攻击中的单一问题，缺乏统一且适用于已有模型的解决方案。

Method: 提出MACS（多层分析置信度评分）框架，作为后处理方法分析预训练模型的中间激活层，生成分类图，从分类图中推导出统一的置信度分数，该分数可同时用于置信度估计、分布偏移检测和对抗攻击检测。

Result: 在VGG16和ViTb16模型上的实验表明，MACS在置信度估计、分布偏移检测和对抗攻击检测三个任务上的性能均超越现有最先进方法，同时计算开销显著降低。

Conclusion: MACS提供了一个统一的后处理框架，能够在不重新训练现有模型的情况下，同时解决置信度估计、分布偏移检测和对抗攻击检测三个关键问题，具有更好的性能和更低的计算成本。

Abstract: The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.

</details>


### [149] [Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks](https://arxiv.org/abs/2512.19488)
*Hafsa Benaddi,Mohammed Jouhari,Nouha Laamech,Anas Motii,Khalil Ibrahimi*

Main category: cs.LG

TL;DR: 结合SHAP特征剪枝与知识蒸馏的Kronecker网络，实现轻量级物联网入侵检测系统，在保持高准确率的同时大幅降低模型复杂度


<details>
  <summary>Details</summary>
Motivation: 物联网设备广泛部署需要高精度的入侵检测系统，但传统深度学习IDS模型太大、计算密集，不适合边缘部署。需要在资源受限环境下实现高效检测

Method: 1. 使用高容量教师模型通过SHAP解释识别最相关特征；2. 采用Kronecker结构层构建压缩学生模型以减少参数；3. 通过知识蒸馏将软化的决策边界从教师传递给学生，提高压缩下的泛化能力

Result: 在TON_IoT数据集上，学生模型比教师模型小近三个数量级，但能保持0.986以上的宏F1分数，推理延迟达到毫秒级

Conclusion: 可解释性驱动的特征剪枝和结构化压缩可以共同实现可扩展、低延迟、高能效的物联网入侵检测系统，适用于异构物联网环境

Abstract: The widespread deployment of Internet of Things (IoT) devices requires intrusion detection systems (IDS) with high accuracy while operating under strict resource constraints. Conventional deep learning IDS are often too large and computationally intensive for edge deployment. We propose a lightweight IDS that combines SHAP-guided feature pruning with knowledge-distilled Kronecker networks. A high-capacity teacher model identifies the most relevant features through SHAP explanations, and a compressed student leverages Kronecker-structured layers to minimize parameters while preserving discriminative inputs. Knowledge distillation transfers softened decision boundaries from teacher to student, improving generalization under compression. Experiments on the TON\_IoT dataset show that the student is nearly three orders of magnitude smaller than the teacher yet sustains macro-F1 above 0.986 with millisecond-level inference latency. The results demonstrate that explainability-driven pruning and structured compression can jointly enable scalable, low-latency, and energy-efficient IDS for heterogeneous IoT environments.

</details>


### [150] [Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico](https://arxiv.org/abs/2512.19491)
*Martí Medina-Hern ández,Janos Kertész,Mihály Fazekas*

Main category: cs.LG

TL;DR: 该研究使用正例-未标记学习算法，结合传统腐败风险指标和网络特征，识别墨西哥政府采购中的腐败合同，显著优于仅使用传统指标的方法。


<details>
  <summary>Details</summary>
Motivation: 政府采购中的欺诈和腐败检测是全球性挑战。现有研究主要基于领域知识的腐败风险指标和合同网络模式，但监督学习面临缺乏确认的非腐败负例样本的问题。

Method: 使用墨西哥联邦采购公开数据和公司制裁记录，实施正例-未标记学习算法，整合基于领域知识的腐败风险指标和网络特征来识别可能腐败的合同。

Result: 最佳PU模型平均捕获已知正例的能力比随机猜测高32%，性能是随机猜测的2.3倍，显著优于仅使用传统风险指标的方法。网络特征（特别是网络核心合同和高特征向量中心性供应商）最重要。

Conclusion: 该方法可支持墨西哥执法工作，并能适应其他国家背景。网络特征在腐败检测中起关键作用，传统风险指标主要对竞争性招标合同有增强效果。

Abstract: Detecting fraud and corruption in public procurement remains a major challenge for governments worldwide. Most research to-date builds on domain-knowledge-based corruption risk indicators of individual contract-level features and some also analyzes contracting network patterns. A critical barrier for supervised machine learning is the absence of confirmed non-corrupt, negative, examples, which makes conventional machine learning inappropriate for this task. Using publicly available data on federally funded procurement in Mexico and company sanction records, this study implements positive-unlabeled (PU) learning algorithms that integrate domain-knowledge-based red flags with network-derived features to identify likely corrupt and fraudulent contracts. The best-performing PU model on average captures 32 percent more known positives and performs on average 2.3 times better than random guessing, substantially outperforming approaches based solely on traditional red flags. The analysis of the Shapley Additive Explanations reveals that network-derived features, particularly those associated with contracts in the network core or suppliers with high eigenvector centrality, are the most important. Traditional red flags further enhance model performance in line with expectations, albeit mainly for contracts awarded through competitive tenders. This methodology can support law enforcement in Mexico, and it can be adapted to other national contexts too.

</details>


### [151] [Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset](https://arxiv.org/abs/2512.19494)
*Nikita Volzhin,Soowhan Yoon*

Main category: cs.LG

TL;DR: KAGNNs在无机纳米材料数据集CHILI上超越传统GNNs，取得SOTA分类结果


<details>
  <summary>Details</summary>
Motivation: 现有KAN-based GNNs研究主要关注有机分子数据集，忽视了无机纳米材料数据集，需要填补这一空白

Method: 将Kolmogorov-Arnold图神经网络(KAGNNs)应用于大型无机纳米材料数据集CHILI，并进行适配和测试

Result: 在CHILI数据集（特别是CHILI-3K）上，KAGNNs在分类任务中显著超越传统GNNs，达到最先进水平

Conclusion: KAGNNs在无机纳米材料数据集上表现出色，扩展了KAN-based模型的应用范围，为无机材料研究提供了有效工具

Abstract: The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.

</details>


### [152] [DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast](https://arxiv.org/abs/2512.19506)
*Hongliang Li,Nong Zhang,Zhewen Xu,Xiang Li,Changzheng Liu,Chongbo Zhao,Jie Wu*

Main category: cs.LG

TL;DR: 提出DK-STN模型，结合数值天气预报和神经网络的优点，实现高效稳定的MJO预测，准确率与ECMWF相当但效率更高


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报方法资源密集、耗时且不稳定，而现有神经网络方法虽然节省资源但准确性不足，无法达到ECMWF的28天预测水平，需要开发结合两者优势的新方法

Method: 提出领域知识嵌入时空网络（DK-STN），在时空网络基础上通过两种关键方法嵌入领域知识：1）应用领域知识增强方法；2）将领域知识处理方法集成到网络训练中

Result: 使用ERA5数据评估，DK-STN能以7天气候数据为输入，在1-2秒内生成未来28天的可靠预测，不同季节误差仅为2-3天，准确率与ECMWF相当但效率和稳定性显著更优

Conclusion: DK-STN成功结合了数值天气预报和神经网络方法的优点，在保持高效率和高稳定性的同时，显著提高了神经网络方法的预测准确性，为MJO预测提供了新的有效解决方案

Abstract: Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.

</details>


### [153] [LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning](https://arxiv.org/abs/2512.19516)
*Xueming Yan,Bo Yin,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出LacaDM方法，通过潜在因果扩散模型增强多目标强化学习在动态环境中的适应性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统多目标强化学习方法在目标冲突和动态环境适应方面存在局限，特别是在大规模复杂状态-动作空间中泛化能力不足

Method: 提出潜在因果扩散模型(LacaDM)，学习环境状态与策略之间的潜在时序因果关系，并将这些因果结构嵌入扩散模型框架中

Result: 在MOGymnasium框架的各种任务上，LacaDM在超体积、稀疏性和期望效用最大化等指标上均优于现有最佳基线方法

Conclusion: LacaDM通过建模潜在因果结构，在多目标强化学习中实现了目标冲突平衡和跨场景知识迁移，在复杂任务中表现出色

Abstract: Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.

</details>


### [154] [Initialization of a Polyharmonic Cascade, Launch and Testing](https://arxiv.org/abs/2512.19524)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 提出了一种基于超八面体对称星座的通用初始化方法，能够稳定训练数百层深度网络，将线性代数简化为2D操作，在多个数据集上展示了可扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度神经网络训练中的稳定性问题，特别是对于没有跳跃连接的深层网络（可达500层），需要一种理论上推导的初始化方法来确保稳定训练。

Method: 提出基于超八面体对称星座（带中心点）的通用初始化程序，将线性代数运算简化为2D操作，在GPU上高效执行。

Result: 在MNIST上达到98.3%（无卷积或数据增强），HIGGS数据集AUC约0.885（1100万样本），Epsilon数据集AUC约0.963（2000特征），展示了可扩展性和鲁棒性。

Conclusion: 该初始化方法不仅确保深层网络的稳定训练，还显著简化计算，提供了完整的可复现性代码库。

Abstract: This paper concludes a series of studies on the polyharmonic cascade, a deep machine learning architecture theoretically derived from indifference principles and the theory of random functions. A universal initialization procedure is proposed, based on symmetric constellations in the form of hyperoctahedra with a central point. This initialization not only ensures stable training of cascades with tens and hundreds of layers (up to 500 layers without skip connections), but also radically simplifies the computations. Scalability and robustness are demonstrated on MNIST (98.3% without convolutions or augmentations), HIGGS (AUC approximately 0.885 on 11M examples), and Epsilon (AUC approximately 0.963 with 2000 features). All linear algebra is reduced to 2D operations and is efficiently executed on GPUs. A public repository and an archived snapshot are provided for full reproducibility.

</details>


### [155] [Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions](https://arxiv.org/abs/2512.19527)
*Diego Hitzges,Guillaume Sagnol*

Main category: cs.LG

TL;DR: 提出一种用于无关并行机调度的深度学习模型，能够处理任意规模的问题，在离线确定性调度中显著优于现有调度规则。


<details>
  <summary>Details</summary>
Motivation: 无关并行机调度问题具有挑战性：作业和机器数量可变，每个作业-机器对都有独特的处理时间，导致特征维度动态变化。现有基于学习的方法难以有效处理这种复杂情况。

Method: 设计了一种新颖的神经网络架构，借鉴NLP技术处理可变数量的作业和机器以及变化的特征维度。采用离线确定性调度方法，一次性考虑所有输入生成完整调度，而不是在线顺序处理作业。

Result: 在8作业4机器的实例上训练和测试，成本仅比最优解高2.51%。在所有测试配置中（最多100作业10机器），网络始终优于先进的调度规则，后者平均成本高出22.22%。

Conclusion: 该方法通过小规模实例监督训练，能很好地泛化到大规模调度环境，具有快速重新训练和适应各种调度条件的能力，有望成为无关机器调度及相关问题的标准学习型方法。

Abstract: Deep learning has been effectively applied to many discrete optimization problems. However, learning-based scheduling on unrelated parallel machines remains particularly difficult to design. Not only do the numbers of jobs and machines vary, but each job-machine pair has a unique processing time, dynamically altering feature dimensions. We propose a novel approach with a neural network tailored for offline deterministic scheduling of arbitrary sizes on unrelated machines. The goal is to minimize a complex objective function that includes the makespan and the weighted tardiness of jobs and machines. Unlike existing online approaches, which process jobs sequentially, our method generates a complete schedule considering the entire input at once. The key contribution of this work lies in the sophisticated architecture of our model. By leveraging various NLP-inspired architectures, it effectively processes any number of jobs and machines with varying feature dimensions imposed by unrelated processing times. Our approach enables supervised training on small problem instances while demonstrating strong generalization to much larger scheduling environments. Trained and tested on instances with 8 jobs and 4 machines, costs were only 2.51% above optimal. Across all tested configurations of up to 100 jobs and 10 machines, our network consistently outperformed an advanced dispatching rule, which incurred 22.22% higher costs on average. As our method allows fast retraining with simulated data and adaptation to various scheduling conditions, we believe it has the potential to become a standard approach for learning-based scheduling on unrelated machines and similar problem environments.

</details>


### [156] [Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement](https://arxiv.org/abs/2512.19530)
*Hongsheng Xing,Qiuxin Si*

Main category: cs.LG

TL;DR: 本文提出了Catechol Benchmark数据集，用于评估溶剂连续组成范围内反应产率的预测方法，并开发了一种混合GNN架构，相比传统方法实现了60%的误差降低。


<details>
  <summary>Details</summary>
Motivation: 有机合成和过程化学中，预测连续溶剂组成范围内的反应结果是一个关键挑战。传统机器学习方法通常将溶剂视为离散分类变量，这阻碍了在溶剂空间中的系统插值和外推。

Method: 提出了混合GNN架构，将图注意力网络（GATs）与差分反应指纹（DRFP）和学习到的混合物感知溶剂编码相结合。使用严格的留一溶剂出和留一混合物出协议评估各种架构。

Result: 提出的混合GNN架构实现了MSE为0.0039（±0.0003），相比竞争基线误差降低了60%，比表格集成方法提高了25倍以上。传统表格方法（MSE 0.099）和大型语言模型嵌入（MSE 0.129）表现较差。

Conclusion: 显式的分子图消息传递和连续混合物编码对于稳健的泛化至关重要。该工作为数据高效的反应预测和连续溶剂表示学习提供了完整的基准数据集和评估协议。

Abstract: Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.
  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \textbf{MSE of 0.0039} ($\pm$ 0.0003), representing a 60\% error reduction over competitive baselines and a $>25\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.

</details>


### [157] [DFORD: Directional Feedback based Online Ordinal Regression Learning](https://arxiv.org/abs/2512.19550)
*Naresh Manwani,M Elamparithy,Tanish Taneja*

Main category: cs.LG

TL;DR: 提出一种使用方向反馈的在线序数回归算法，通过探索-利用策略从弱监督信号中学习，实现O(log T)期望遗憾


<details>
  <summary>Details</summary>
Motivation: 在序数回归中，传统方法需要完整的标签信息，但实际应用中可能只能获得方向性反馈（预测标签在真实标签的左侧还是右侧）。这种弱监督设置更具实用性，需要开发相应的学习算法。

Method: 提出基于探索-利用策略的在线算法，使用方向反馈学习序数回归模型。开发核化变体处理非线性问题，采用截断技巧提高内存效率。算法在期望意义上保持阈值排序。

Result: 算法达到O(log T)的期望遗憾。在合成和真实数据集上的实验表明，使用方向反馈的方法与完整信息方法性能相当（有时更好）。

Conclusion: 方向反馈为序数回归提供了有效的弱监督学习框架，提出的在线算法在保持理论保证的同时，在实际应用中表现出色。

Abstract: In this paper, we introduce directional feedback in the ordinal regression setting, in which the learner receives feedback on whether the predicted label is on the left or the right side of the actual label. This is a weak supervision setting for ordinal regression compared to the full information setting, where the learner can access the labels. We propose an online algorithm for ordinal regression using directional feedback. The proposed algorithm uses an exploration-exploitation scheme to learn from directional feedback efficiently. Furthermore, we introduce its kernel-based variant to learn non-linear ordinal regression models in an online setting. We use a truncation trick to make the kernel implementation more memory efficient. The proposed algorithm maintains the ordering of the thresholds in the expected sense. Moreover, it achieves the expected regret of $\mathcal{O}(\log T)$. We compare our approach with a full information and a weakly supervised algorithm for ordinal regression on synthetic and real-world datasets. The proposed approach, which learns using directional feedback, performs comparably (sometimes better) to its full information counterpart.

</details>


### [158] [CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal](https://arxiv.org/abs/2512.19554)
*Yongxin Wang,Zhicheng Yang,Meng Cao,Mingfei Han,Haokun Lin,Yingying Zhu,Xiaojun Chang,Xiaodan Liang*

Main category: cs.LG

TL;DR: CARE是一个专注于失败数据的后训练框架，通过对比锚定和反思引导重采样，将错误转化为监督信号，提升多模态推理的准确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励强化学习（RLVR）在处理失败数据时效率低下：当所有轨迹都错误时梯度停滞；当有正确轨迹时，更新通常忽略其他接近但错误的轨迹，导致信用分配错误。需要一种能有效利用失败数据的方法。

Method: CARE包含两个核心组件：1) 锚定对比目标：围绕最佳轨迹形成紧凑子组和语义相近的困难负例，进行组内z-score归一化（仅负例缩放），包含全负例救援机制防止零信号批次；2) 反思引导重采样（RGR）：一次性结构化自我修复，重写代表性失败案例并用相同验证器重新评分，将接近错误转化为可用正例。

Result: 在Qwen2.5-VL-7B上，CARE在六个可验证视觉推理基准测试中比GRPO提升4.6个百分点的宏平均准确率；在Qwen3-VL-8B上，在相同评估协议下，在MathVista和MMMU-Pro上达到竞争性或最先进的结果。

Conclusion: CARE通过专注于失败数据，将错误转化为监督信号，显著提高了多模态推理的准确性和训练平滑度，同时明确增加了来自失败的学习信号比例。

Abstract: Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.

</details>


### [159] [KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning](https://arxiv.org/abs/2512.19605)
*Eric Zimmermann,Harley Wiltzer,Justin Szeto,David Alvarez-Melis,Lester Mackey*

Main category: cs.LG

TL;DR: 提出KerJEPA家族，使用核基正则化器改进自监督学习，相比现有JEPA方法具有更好的训练稳定性和设计灵活性


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法使用欧几里得表示向各向同性高斯先验的正则化，虽然能提升训练稳定性和下游泛化能力，但正则化方法有限。需要更灵活的正则化框架来改进自监督学习算法。

Method: 引入KerJEPA家族，使用核基正则化器。通过扩展可行的核函数和先验分布，计算切片最大均值差异（MMD）的闭式高维极限，开发具有改进训练稳定性和设计灵活性的替代KerJEPA变体。

Result: 开发出具有多种有利特性的替代KerJEPA算法，包括改进的训练稳定性和设计灵活性。其中一个实例对应最近提出的LeJEPA Epps-Pulley正则化器，该正则化器使用高斯先验和高斯核近似切片MMD。

Conclusion: KerJEPA家族为自监督学习提供了更灵活的正则化框架，通过核基方法扩展了JEPA架构的能力，在训练稳定性和算法设计方面具有优势。

Abstract: Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.

</details>


### [160] [The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference](https://arxiv.org/abs/2512.19643)
*Rajyasri Roy,Dibyajyoti Nayak,Somdatta Goswami*

Main category: cs.LG

TL;DR: ANCHOR提出了一种自适应混合推理框架，通过结合预训练的神经算子和经典数值求解器，使用基于残差的误差估计器在线监测和纠正误差，实现非线性时变PDE的稳定长期预测。


<details>
  <summary>Details</summary>
Motivation: 神经算子（NO）代理在参数化和函数输入上提供快速推理，但大多数自回归框架容易受到累积误差的影响，且集成平均指标对单个推理轨迹的保证有限。在实际应用中，超出训练范围后误差累积可能变得不可接受，现有方法缺乏在线监测或纠正机制。

Method: ANCHOR将预训练的神经算子作为主要推理引擎，并使用基于物理的残差误差估计器自适应地将其与经典数值求解器耦合。该方法受数值分析中自适应时间步长的启发，通过监测归一化PDE残差的指数移动平均（EMA）来检测累积误差，并在不需要真实解的情况下触发纠正性求解器干预。

Result: 在四个经典PDE（1D和2D Burgers方程、2D Allen-Cahn方程和3D热传导方程）上的评估表明，ANCHOR能够可靠地限制长期误差增长，稳定外推滚动，并显著提高相对于独立神经算子的鲁棒性，同时比高保真数值求解器效率更高。

Conclusion: ANCHOR通过自适应混合推理框架解决了神经算子在长期预测中的误差累积问题，实现了稳定可靠的PDE预测，在效率和精度之间取得了良好平衡。

Abstract: Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.

</details>


### [161] [Deep Legendre Transform](https://arxiv.org/abs/2512.19649)
*Aleksey Minabutdinov,Patrick Cheridito*

Main category: cs.LG

TL;DR: 提出一种基于深度学习计算可微凸函数凸共轭的新算法，通过隐式Fenchel公式实现高效梯度优化，能处理高维问题并提供误差估计。


<details>
  <summary>Details</summary>
Motivation: 凸共轭是凸分析中的基本运算，在优化、控制理论、物理和经济学等领域有广泛应用。传统数值方法面临维度灾难，而现有神经网络方法主要针对最优传输问题且需要解决复杂的优化问题。

Method: 采用隐式Fenchel公式表示凸共轭，构建基于梯度的优化框架来最小化近似误差，同时提供后验误差估计。还结合Kolmogorov-Arnold网络进行符号回归以获取精确凸共轭。

Result: 数值实验表明该方法能在不同高维示例中提供准确结果，并能通过符号回归获得特定凸函数的精确凸共轭。

Conclusion: 该方法为计算凸共轭提供了高效、可扩展的深度学习框架，克服了传统方法的维度限制，并能提供误差估计和精确解。

Abstract: We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method's ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.

</details>


### [162] [Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies](https://arxiv.org/abs/2512.19673)
*Yuqiao Tan,Minzheng Wang,Shizhu He,Huanxuan Liao,Chengfeng Zhao,Qiunan Lu,Tian Liang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的RL范式BuPO，通过分解大语言模型的内部层策略来优化早期训练，提升复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法将大语言模型视为单一统一策略，忽视了其内部机制。理解策略在不同层和模块间的演化对于实现更有针对性的优化和揭示复杂推理机制至关重要。

Method: 利用Transformer残差流的固有分割以及隐藏状态与解嵌入矩阵组合的等价性，将语言模型策略分解为内部层策略和内部模块策略。基于此提出Bottom-up Policy Optimization (BuPO)，在早期训练中直接优化内部层策略。

Result: 分析发现：早期层保持高熵用于探索，顶层收敛到接近零熵用于精炼；LLama在最后一层快速收敛，而Qwen系列（特别是Qwen3）展现出更类似人类的渐进结构化推理模式。BuPO在复杂推理基准测试中表现出色。

Conclusion: 通过分解和优化内部层策略，BuPO能够重建基础推理能力并实现优越性能，为理解和大语言模型优化提供了新视角。

Abstract: Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [163] [Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler](https://arxiv.org/abs/2512.17977)
*Holden Lee,Matheau Santana-Gijzen*

Main category: stat.ML

TL;DR: 提出Re-ALPS算法，通过重加权和分区函数估计改进ALPS，无需高斯假设即可在多模态分布中实现多项式时间混合


<details>
  <summary>Details</summary>
Motivation: 传统MCMC方法在多模态分布采样中面临指数级混合时间问题，需要利用额外信息（如各模态的暖启动点）来加速模态间混合

Method: 改进ALPS算法，通过重加权分布向暖启动点混合中心倾斜，在最冷层级使用暖启动点间的传送机制，无需Hessian信息而是通过蒙特卡洛估计分区函数

Result: 在一般设置下证明了首个多项式时间边界，数值实验显示在重尾分布混合上比ALPS有更好的混合性能

Conclusion: Re-ALPS算法通过分区函数估计和重加权机制，能够在更复杂几何形状的目标分布中实现高效的多模态采样，无需高斯近似假设

Abstract: Sampling from multimodal distributions is a central challenge in Bayesian inference and machine learning. In light of hardness results for sampling -- classical MCMC methods, even with tempering, can suffer from exponential mixing times -- a natural question is how to leverage additional information, such as a warm start point for each mode, to enable faster mixing across modes. To address this, we introduce Reweighted ALPS (Re-ALPS), a modified version of the Annealed Leap-Point Sampler (ALPS) that dispenses with the Gaussian approximation assumption. We prove the first polynomial-time bound that works in a general setting, under a natural assumption that each component contains significant mass relative to the others when tilted towards the corresponding warm start point. Similarly to ALPS, we define distributions tilted towards a mixture centered at the warm start points, and at the coldest level, use teleportation between warm start points to enable efficient mixing across modes. In contrast to ALPS, our method does not require Hessian information at the modes, but instead estimates component partition functions via Monte Carlo. This additional estimation step is crucial in allowing the algorithm to handle target distributions with more complex geometries besides approximate Gaussian. For the proof, we show convergence results for Markov processes when only part of the stationary distribution is well-mixing and estimation for partition functions for individual components of a mixture. We numerically evaluate our algorithm's mixing performance compared to ALPS on a mixture of heavy-tailed distributions.

</details>


### [164] [Causal Inference as Distribution Adaptation: Optimizing ATE Risk under Propensity Uncertainty](https://arxiv.org/abs/2512.18083)
*Ashley Zhang*

Main category: stat.ML

TL;DR: 将因果推断方法统一为机器学习视角下的域适应问题，提出联合稳健估计器(JRE)，通过联合训练结果模型和倾向得分模型，在倾向得分误设时获得更好的有限样本性能。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断方法（如结果回归和IPWRA）通常从缺失数据填补和识别理论的角度推导。本文希望从机器学习视角统一这些方法，将ATE估计重新定义为分布偏移下的域适应问题，并探索更有效的估计器设计。

Method: 1. 建立统一框架：将ATE估计视为域适应问题，IPWRA是重要性加权经验风险最小化；2. 分析双重稳健估计器：指出标准方法要求结果模型各自无偏是充分但不必要条件；3. 提出联合稳健估计器(JRE)：利用倾向得分的自助法不确定性量化，联合训练结果模型，优化期望ATE风险。

Result: 1. Hajek估计器是IPWRA在常数假设类下的特例；2. 标准双重稳健估计器要求过强的条件；3. JRE在结果模型误设的有限样本情况下，相比标准IPWRA可降低高达15%的MSE。

Conclusion: 从机器学习视角统一因果推断方法提供了新的理论洞察，提出的JRE通过联合优化倾向得分和结果模型，在倾向得分误设时展现出更好的稳健性和有限样本性能。

Abstract: Standard approaches to causal inference, such as Outcome Regression and Inverse Probability Weighted Regression Adjustment (IPWRA), are typically derived through the lens of missing data imputation and identification theory. In this work, we unify these methods from a Machine Learning perspective, reframing ATE estimation as a \textit{domain adaptation problem under distribution shift}. We demonstrate that the canonical Hajek estimator is a special case of IPWRA restricted to a constant hypothesis class, and that IPWRA itself is fundamentally Importance-Weighted Empirical Risk Minimization designed to correct for the covariate shift between the treated sub-population and the target population.
  Leveraging this unified framework, we critically examine the optimization objectives of Doubly Robust estimators. We argue that standard methods enforce \textit{sufficient but not necessary} conditions for consistency by requiring outcome models to be individually unbiased. We define the true "ATE Risk Function" and show that minimizing it requires only that the biases of the treated and control models structurally cancel out. Exploiting this insight, we propose the \textbf{Joint Robust Estimator (JRE)}. Instead of treating propensity estimation and outcome modeling as independent stages, JRE utilizes bootstrap-based uncertainty quantification of the propensity score to train outcome models jointly. By optimizing for the expected ATE risk over the distribution of propensity scores, JRE leverages model degrees of freedom to achieve robustness against propensity misspecification. Simulation studies demonstrate that JRE achieves up to a 15\% reduction in MSE compared to standard IPWRA in finite-sample regimes with misspecified outcome models.

</details>


### [165] [Unsupervised Feature Selection via Robust Autoencoder and Adaptive Graph Learning](https://arxiv.org/abs/2512.18720)
*Feng Yu,MD Saifur Rahman Mazumder,Ying Su,Oscar Contreras Velasco*

Main category: stat.ML

TL;DR: 提出RAEUFS模型，使用深度自编码器进行非线性特征表示学习，提高对异常值的鲁棒性，在无监督特征选择任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督特征选择方法存在两个关键局限：1）使用过于简化的线性映射，无法捕捉复杂的特征关系；2）假设均匀的聚类分布，忽略了现实数据中普遍存在的异常值。

Method: 提出鲁棒自编码器无监督特征选择（RAEUFS）模型，利用深度自编码器学习非线性特征表示，同时提高对异常值的鲁棒性，并开发了高效的优化算法。

Result: 大量实验表明，该方法在干净数据和受异常值污染的数据设置下，都优于最先进的无监督特征选择方法。

Conclusion: RAEUFS模型通过深度自编码器有效解决了现有无监督特征选择方法的局限性，在复杂特征关系和异常值处理方面表现出色。

Abstract: Effective feature selection is essential for high-dimensional data analysis and machine learning. Unsupervised feature selection (UFS) aims to simultaneously cluster data and identify the most discriminative features. Most existing UFS methods linearly project features into a pseudo-label space for clustering, but they suffer from two critical limitations: (1) an oversimplified linear mapping that fails to capture complex feature relationships, and (2) an assumption of uniform cluster distributions, ignoring outliers prevalent in real-world data. To address these issues, we propose the Robust Autoencoder-based Unsupervised Feature Selection (RAEUFS) model, which leverages a deep autoencoder to learn nonlinear feature representations while inherently improving robustness to outliers. We further develop an efficient optimization algorithm for RAEUFS. Extensive experiments demonstrate that our method outperforms state-of-the-art UFS approaches in both clean and outlier-contaminated data settings.

</details>


### [166] [On Conditional Stochastic Interpolation for Generative Nonlinear Sufficient Dimension Reduction](https://arxiv.org/abs/2512.18971)
*Shuntuo Xu,Zhou Yu,Jian Huang*

Main category: stat.ML

TL;DR: 提出GenSDR方法，利用生成模型解决非线性充分降维中低维充分结构的识别问题，具有理论保证和广泛适用性


<details>
  <summary>Details</summary>
Motivation: 非线性充分降维中识别低维充分结构是一个基础但具有挑战性的问题，现有方法在总体和样本层面缺乏识别低维结构的完备性理论保证

Method: 提出生成充分降维(GenSDR)方法，利用现代生成模型，通过集成技术扩展到非欧几里得响应场景

Result: GenSDR能够在总体和样本层面完全恢复中心σ-场的信息，在样本层面建立了条件分布视角的一致性性质，数值结果展示了出色的经验性能

Conclusion: GenSDR通过生成模型解决了充分降维中的理论完备性问题，具有广泛的实际应用潜力

Abstract: Identifying low-dimensional sufficient structures in nonlinear sufficient dimension reduction (SDR) has long been a fundamental yet challenging problem. Most existing methods lack theoretical guarantees of exhaustiveness in identifying lower dimensional structures, either at the population level or at the sample level. We tackle this issue by proposing a new method, generative sufficient dimension reduction (GenSDR), which leverages modern generative models. We show that GenSDR is able to fully recover the information contained in the central $σ$-field at both the population and sample levels. In particular, at the sample level, we establish a consistency property for the GenSDR estimator from the perspective of conditional distributions, capitalizing on the distributional learning capabilities of deep generative models. Moreover, by incorporating an ensemble technique, we extend GenSDR to accommodate scenarios with non-Euclidean responses, thereby substantially broadening its applicability. Extensive numerical results demonstrate the outstanding empirical performance of GenSDR and highlight its strong potential for addressing a wide range of complex, real-world tasks.

</details>


### [167] [Cluster-Based Generalized Additive Models Informed by Random Fourier Features](https://arxiv.org/abs/2512.19373)
*Xin Huang,Jia Li,Jun Yu*

Main category: stat.ML

TL;DR: 提出一种基于随机傅里叶特征(RFF)的混合广义可加模型(GAMs)，通过RFF嵌入进行软聚类，构建局部可解释的GAMs，在保持模型可解释性的同时提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 在机器学习应用中，黑盒模型（如深度神经网络或基于核的方法）虽然预测性能强但难以解释，而传统可解释模型性能有限。需要一种既能保持模型透明度又能提升预测性能的方法。

Method: 首先学习RFF嵌入表示，通过主成分分析压缩降维，使用高斯混合模型进行软聚类，基于聚类结果构建混合GAMs框架，每个局部GAM通过可解释的单变量平滑函数捕捉非线性效应。

Result: 在加州住房、NASA翼型自噪声和共享单车等真实世界回归基准数据集上的数值实验表明，相比经典可解释模型，该方法在预测性能上有显著提升。

Conclusion: 该方法为将表示学习与透明统计建模相结合提供了原则性框架，在保持模型可解释性的同时实现了更好的预测性能。

Abstract: Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.

</details>
