<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 22]
- [cs.LG](#cs.LG) [Total: 60]
- [stat.ML](#stat.ML) [Total: 4]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [On the static and small signal analysis of DAB converter](https://arxiv.org/abs/2601.10746)
*Yuxin Yang,Hang Zhou,Hourong Song,Branislav Hredzak*

Main category: eess.SP

TL;DR: 提出了一种求解双有源桥(DAB)周期性工作点的方法


<details>
  <summary>Details</summary>
Motivation: 双有源桥变换器在电力电子应用中需要准确计算其周期性工作点，传统方法可能存在计算复杂或精度不足的问题

Method: 开发了一种新的计算方法来求解DAB的周期性工作点，可能涉及数学建模、状态空间分析或数值计算方法

Result: 该方法能够有效计算DAB的周期性工作点，提高了计算精度和效率

Conclusion: 提出的方法为DAB变换器的分析和设计提供了有效的工具，有助于优化其性能和控制策略

Abstract: This document develops a method to solve the periodic operating point of Dual-Active-Bridge (DAB).

</details>


### [2] [Zonotope Shadow and Reflection Matching: A Novel GNSS Reflection-Based Framework for Enhanced Positioning Accuracy in Urban Areas](https://arxiv.org/abs/2601.10727)
*Sanghyun Kim,Jiwon Seo*

Main category: eess.SP

TL;DR: 提出ZSRM（区域阴影反射匹配）方法，结合GNSS阴影和反射信息，相比传统ZSM方法显著提升城市定位精度


<details>
  <summary>Details</summary>
Motivation: 传统3DMA GNSS定位方法（如阴影匹配）在城市环境中存在精度限制，网格化估计受分辨率约束，难以满足用户指定的保护等级要求

Method: 提出ZSRM（区域阴影反射匹配）方法，不仅利用GNSS阴影信息（LOS/NLOS卫星判断），还结合反射信号信息，使用基于集合的位置估计而非网格化估计

Result: 在城市环境实地测试中，ZSRM相比ZSM的RMS水平位置误差改善10.0%-53.6%，横街和沿街位置边界分别改善18.0%-50.1%和30.7%-59.3%

Conclusion: ZSRM通过结合阴影和反射信息，显著提升了城市GNSS定位精度，为满足安全边界要求提供了有效解决方案

Abstract: In urban areas, signal reception conditions are often poor due to reflections from buildings, resulting in inaccurate global navigation satellite system (GNSS)-based positioning. Various 3D-mapping-aided (3DMA) GNSS techniques, including shadow matching, have been proposed to address this issue. However, conventional shadow matching estimates positions in a discretized manner. The accuracy of this approach is limited by the resolution of the grid points representing the candidate receiver positions, making it difficult to achieve robust urban positioning and to ensure that the position estimate satisfies user-specified protection levels or safety bounds. To overcome these limitations, zonotope shadow matching (ZSM) has been proposed, which utilizes a set-based position estimate rather than grid-based estimates. ZSM calculates the GNSS shadow--an area on the ground where the line-of-sight (LOS) is blocked and only non-line-of-sight (NLOS) signals can be received--to estimate the receiver's position set. ZSM distinguishes between LOS and NLOS satellites, determining that the receiver is inside the GNSS shadow if the satellite is NLOS and outside if the satellite is LOS. However, relying solely on GNSS shadows limits the ability to sufficiently reduce the size of the receiver position set and to precisely estimate the receiver's location. To address this, we propose zonotope shadow and reflection matching (ZSRM) to enhance positioning accuracy in urban areas. The proposed ZSRM technique is validated through field tests using GNSS signals collected in an urban environment. Consequently, the RMS horizontal position error of ZSRM improved by 10.0% to 53.6% compared with ZSM, while the RMS cross-street and along-street position bounds improved by 18.0% to 50.1% and 30.7% to 59.3%, respectively.

</details>


### [3] [Millimeter-Wave Gesture Recognition in ISAC: Does Reducing Sensing Airtime Hamper Accuracy?](https://arxiv.org/abs/2601.10733)
*Jakob Struye,Nabeel Nisar Bhat,Siddhartha Kumar,Mohammad Hossein Moghaddam,Jeroen Famaey*

Main category: eess.SP

TL;DR: 毫米波ISAC系统中，仅用25%的感知时间就能实现接近全时感知的手势识别准确率（仅下降0.15个百分点），同时保持高速通信能力


<details>
  <summary>Details</summary>
Motivation: 现有ISAC系统需要在感知和通信模式之间分配时间，但这种时间分配决策对感知性能的具体影响尚不明确，需要深入研究

Method: 使用两个毫米波设备进行恒定波束扫描，收集测试对象执行不同手势时的波束对功率数据集，然后训练卷积神经网络手势分类器，并通过子采样测量来模拟减少感知时间的效果

Result: 仅使用25%的感知时间，分类准确率仅比全时感知下降0.15个百分点，表明毫米波ISAC系统能够在低感知时间下实现高质量感知

Conclusion: 毫米波ISAC系统能够在保持极高数据吞吐量的同时，仅用少量感知时间就实现高质量手势识别，这使其成为真正无线扩展现实等应用的关键使能技术

Abstract: Most Integrated Sensing and Communications (ISAC) systems require dividing airtime across their two modes. However, the specific impact of this decision on sensing performance remains unclear and underexplored. In this paper, we therefore investigate the impact on a gesture recognition system using a Millimeter-Wave (mmWave) ISAC system. With our dataset of power per beam pair gathered with two mmWave devices performing constant beam sweeps while test subjects performed distinct gestures, we train a gesture classifier using Convolutional Neural Networks. We then subsample these measurements, emulating reduced sensing airtime, showing that a sensing airtime of 25 % only reduces classification accuracy by 0.15 percentage points from full-time sensing. Alongside this high-quality sensing at low airtime, mmWave systems are known to provide extremely high data throughputs, making mmWave ISAC a prime enabler for applications such as truly wireless Extended Reality.

</details>


### [4] [SSC-UNet: UNet with Self-Supervised Contrastive Learning for Phonocardiography Noise Reduction](https://arxiv.org/abs/2601.10735)
*Lizy Abraham,Siobhan Coughlan,Kritika Rajain,Changhong Li,Saji Philip,Adam James*

Main category: eess.SP

TL;DR: 提出基于Noise2Noise的自监督心音图降噪模型，无需干净数据训练，在10dB医院噪声下平均SNR达12.98dB，分类敏感度从27%提升至88%


<details>
  <summary>Details</summary>
Motivation: 先天性心脏病(CHD)影响全球约1%的新生儿，心音图是经济有效的辅助诊断工具。但诊断模型性能高度依赖心音图质量，降噪尤为关键。监督UNet虽能有效降噪，但干净数据有限限制了其应用，且心音图复杂的时频特性使平衡噪声去除和病理特征保留变得困难。

Method: 提出基于Noise2Noise的自监督心音图降噪模型，无需干净数据训练。采用数据增强和对比学习提升性能。

Result: 在10dB医院噪声环境下，滤波后平均信噪比(SNR)达到12.98dB。滤波后分类敏感度从27%显著提升至88%，表明在实际噪声环境中具有良好的病理特征保留能力。

Conclusion: 该自监督降噪模型在无需干净数据的情况下有效提升了心音图质量，显著改善了CHD分类性能，具有实际临床应用潜力。

Abstract: Congenital Heart Disease (CHD) remains a significant global health concern affecting approximately 1\% of births worldwide. Phonocardiography has emerged as a supplementary tool to diagnose CHD cost-effectively. However, the performance of these diagnostic models highly depends on the quality of the phonocardiography, thus, noise reduction is particularly critical. Supervised UNet effectively improves noise reduction capabilities, but limited clean data hinders its application. The complex time-frequency characteristics of phonocardiography further complicate finding the balance between effectively removing noise and preserving pathological features. In this study, we proposed a self-supervised phonocardiography noise reduction model based on Noise2Noise to enable training without clean data. Augmentation and contrastive learning are applied to enhance its performance. We obtained an average SNR of 12.98 dB after filtering under 10~dB of hospital noise. Classification sensitivity after filtering was improved from 27\% to 88\%, indicating its promising pathological feature retention capabilities in practical noisy environments.

</details>


### [5] [Differentiating through binarized topology changes: Second-order subpixel-smoothed projection](https://arxiv.org/abs/2601.10737)
*Giuseppe Romano,Rodrigo Arrieta,Steven G. Johnson*

Main category: eess.SP

TL;DR: 提出SSP2方法，通过Hessian正则化解决拓扑优化中拓扑变化时的不可微问题，保证二阶可微性同时保持几乎处处二值结构。


<details>
  <summary>Details</summary>
Motivation: 传统SSP方法在拓扑变化（如界面合并）时无法保证可微性，这违反了梯度优化算法的收敛保证，限制了优化算法的选择范围。

Method: 使用滤波场的Hessian矩阵对SSP进行正则化，形成二阶SSP（SSP2）方法，在拓扑变化时保证投影密度的二阶可微性。

Result: SSP2在连接主导问题（频繁拓扑变化）中比SSP收敛更快，其他情况下性能相当，且能支持更广泛的优化算法如内点法。

Conclusion: SSP2解决了拓扑优化中拓扑变化时的可微性问题，可作为现有代码的直接替代方案，扩展了优化算法的选择范围。

Abstract: A key challenge in topology optimization (TopOpt) is that manufacturable structures, being inherently binary, are non-differentiable, creating a fundamental tension with gradient-based optimization. The subpixel-smoothed projection (SSP) method addresses this issue by smoothing sharp interfaces at the subpixel level through a first-order expansion of the filtered field. However, SSP does not guarantee differentiability under topology changes, such as the merging of two interfaces, and therefore violates the convergence guarantees of many popular gradient-based optimization algorithms. We overcome this limitation by regularizing SSP with the Hessian of the filtered field, resulting in a twice-differentiable projected density during such transitions, while still guaranteeing an almost-everywhere binary structure. We demonstrate the effectiveness of our second-order SSP (SSP2) methodology on both thermal and photonic problems, showing that SSP2 has faster convergence than SSP for connectivity-dominant cases -- where frequent topology changes occur -- while exhibiting comparable performance otherwise. Beyond improving convergence guarantees for CCSA optimizers, SSP2 enables the use of a broader class of optimization algorithms with stronger theoretical guarantees, such as interior-point methods. Since SSP2 adds minimal complexity relative to SSP or traditional projection schemes, it can be used as a drop-in replacement in existing TopOpt codes.

</details>


### [6] [UBiGTLoc: A Unified BiLSTM-Graph Transformer Localization Framework for IoT Sensor Networks](https://arxiv.org/abs/2601.10743)
*Ayesh Abu Lehyeh,Anastassia Gharib,Tian Xia,Dryver Huston,Safwan Wshah*

Main category: eess.SP

TL;DR: 提出UBiGTLoc框架，结合双向LSTM和图Transformer，用于无线物联网传感器网络中的节点定位，无需依赖锚节点，仅使用RSSI数据。


<details>
  <summary>Details</summary>
Motivation: 现有传感器节点定位方法严重依赖锚节点，但在实际物联网场景中锚节点可能不可行；同时RSSI波动（特别是在非视距条件下）会影响定位精度。

Method: 提出统一的双向LSTM-图Transformer定位框架（UBiGTLoc），利用BiLSTM捕获RSSI数据的时间变化，使用图Transformer层建模传感器节点间的空间关系。

Result: 广泛模拟表明，UBiGTLoc在密集和稀疏无线传感器网络中均优于现有方法，仅使用成本效益高的RSSI数据就能提供鲁棒的定位。

Conclusion: UBiGTLoc框架能有效解决锚节点依赖问题，在无锚和有锚无线传感器网络中都能实现准确的传感器节点定位，对实际物联网应用具有重要价值。

Abstract: Sensor nodes localization in wireless Internet of Things (IoT) sensor networks is crucial for the effective operation of diverse applications, such as smart cities and smart agriculture. Existing sensor nodes localization approaches heavily rely on anchor nodes within wireless sensor networks (WSNs). Anchor nodes are sensor nodes equipped with global positioning system (GPS) receivers and thus, have known locations. These anchor nodes operate as references to localize other sensor nodes. However, the presence of anchor nodes may not always be feasible in real-world IoT scenarios. Additionally, localization accuracy can be compromised by fluctuations in Received Signal Strength Indicator (RSSI), particularly under non-line-of-sight (NLOS) conditions. To address these challenges, we propose UBiGTLoc, a Unified Bidirectional Long Short-Term Memory (BiLSTM)-Graph Transformer Localization framework. The proposed UBiGTLoc framework effectively localizes sensor nodes in both anchor-free and anchor-presence WSNs. The framework leverages BiLSTM networks to capture temporal variations in RSSI data and employs Graph Transformer layers to model spatial relationships between sensor nodes. Extensive simulations demonstrate that UBiGTLoc consistently outperforms existing methods and provides robust localization across both dense and sparse WSNs while relying solely on cost-effective RSSI data.

</details>


### [7] [An IoT-Based Controlled Environment Storage for Prevention of Spoilage of Onion (Allium Cepa) During Post-Harvest with UV-C Disinfection](https://arxiv.org/abs/2601.10745)
*Shivam Kumar,Himanshu Singh*

Main category: eess.SP

TL;DR: 印度洋葱存储损耗严重，传统方法要么效果差要么成本高，本文提出基于物联网的低成本智能洋葱存储系统，通过监测调控环境参数和紫外线消毒，目标将损耗率从40-45%降至15-20%，成本约6-7万卢比，适合小农户使用。


<details>
  <summary>Details</summary>
Motivation: 印度作为世界第二大洋葱生产国，年产量超2600万吨，但存储期间因腐烂、发芽和失重导致30-40%的损失。传统存储方法存在两难：低成本的传统存储方式损耗率达40%，而高效冷藏存储对小农户来说过于昂贵。需要为占印度多数的中小农户开发既有效又负担得起的存储解决方案。

Method: 开发基于物联网的低成本智能洋葱存储系统，使用ESP32微控制器、DHT22温湿度传感器、MQ-135气体传感器监测环境参数，结合UV-C紫外线消毒技术。系统自动调节温度、湿度和有害气体，设计为太阳能供电、节能环保，成本控制在6-7万卢比（约60-70k INR）。

Result: 提出的智能存储系统目标将洋葱损耗率从当前的40-45%降低至15-20%，同时保持成本效益。系统设计为农民友好型，适合印度大多数小农户使用，通过自动环境调控和紫外线消毒技术减少存储损失。

Conclusion: 物联网智能存储系统为解决印度洋葱存储损耗问题提供了可行的低成本解决方案，平衡了存储效果和经济性，有助于提高小农户的收入和减少粮食浪费，具有实际应用价值。

Abstract: India is the second largest producer of onions in the world, contributing over 26 million tonnes annually. However, during storage, approximately 30-40% of onions are lost due to rotting, sprouting, and weight loss. Despite being a major producer, conventional storage methods are either low-cost but ineffective (traditional storage with 40% spoilage) or highly effective but prohibitively expensive for small farmers (cold storage). This paper presents a low-cost IoT-based smart onion storage system that monitors and automatically regulates environmental parameters including temperature, humidity, and spoilage gases using ESP32 microcontroller, DHT22 sensor, MQ-135 gas sensor, and UV-C disinfection technology. The proposed system aims to reduce onion spoilage to 15-20% from the current 40-45% wastage rate while remaining affordable for small and marginal farmers who constitute the majority in India. The system is designed to be cost-effective (estimated 60k-70k INR), energy-efficient, farmer-friendly, and solar-powered.

</details>


### [8] [Sensor Placement for Urban Traffic Interpolation: A Data-Driven Evaluation to Inform Policy](https://arxiv.org/abs/2601.10747)
*Silke K. Kaiser*

Main category: eess.SP

TL;DR: 本研究通过柏林和曼哈顿的实际数据，比较了多种数据驱动的交通传感器布设策略，发现强调均匀空间覆盖和主动学习的空间布设策略能显著降低交通流量预测误差，结合均匀时间分布可接近永久部署的性能。


<details>
  <summary>Details</summary>
Motivation: 城市交通流量数据对城市规划至关重要，但现有传感器布设通常基于行政优先而非数据优化，导致覆盖偏差和估计性能下降。需要评估数据驱动的传感器布设策略来改善数据质量。

Method: 使用柏林（Strava自行车计数）和曼哈顿（出租车计数）的街道段级数据，比较基于网络中心性、空间覆盖、特征覆盖和主动学习的空间布设策略，同时考察临时传感器的时间部署方案。

Result: 强调均匀空间覆盖和采用主动学习的空间布设策略表现最佳：仅用10个传感器就能在柏林降低60%以上、曼哈顿降低70%以上的平均绝对误差。时间部署上，将测量均匀分布在平日可进一步降低误差（柏林7%，曼哈顿21%）。

Conclusion: 数据驱动的传感器布设策略能显著提高数据效用，临时部署结合优化的时空策略可接近永久部署的性能，为城市提供了灵活且高效的交通监测方案选择。

Abstract: Data on citywide street-segment traffic volumes are essential for urban planning and sustainable mobility management. Yet such data are available only for a limited subset of streets due to the high costs of sensor deployment and maintenance. Traffic volumes on the remaining network are therefore interpolated based on existing sensor measurements. However, current sensor locations are often determined by administrative priorities rather than by data-driven optimization, leading to biased coverage and reduced estimation performance. This study provides a large-scale, real-world benchmarking of easily implementable, data-driven strategies for optimizing the placement of permanent and temporary traffic sensors, using segment-level data from Berlin (Strava bicycle counts) and Manhattan (taxi counts). It compares spatial placement strategies based on network centrality, spatial coverage, feature coverage, and active learning. In addition, the study examines temporal deployment schemes for temporary sensors. The findings highlight that spatial placement strategies that emphasize even spatial coverage and employ active learning achieve the lowest prediction errors. With only 10 sensors, they reduce the mean absolute error by over 60% in Berlin and 70% in Manhattan compared to alternatives. Temporal deployment choices further improve performance: distributing measurements evenly across weekdays reduces error by an additional 7% in Berlin and 21% in Manhattan. Together, these spatial and temporal principles allow temporary deployments to closely approximate the performance of optimally placed permanent deployments. From a policy perspective, the results indicate that cities can substantially improve data usefulness by adopting data-driven sensor placement strategies, while retaining flexibility in choosing between temporary and permanent deployments.

</details>


### [9] [AnyECG: Evolved ECG Foundation Model for Holistic Health Profiling](https://arxiv.org/abs/2601.10748)
*Jun Li,Hongling Zhu,Yujie Xiao,Qinghao Zhao,Yalei Ke,Gongzheng Tang,Guangkun Nie,Deyun Zhang,Jin Li,Canqing Yu,Shenda Hong*

Main category: eess.SP

TL;DR: AnyECG是一个基于AI-ECG的全健康分析基础模型，使用1320万份心电图数据训练，能够同时检测1172种疾病，预测未来风险并识别共病模式。


<details>
  <summary>Details</summary>
Motivation: 现有AI-ECG模型大多专注于单一疾病识别，忽略了共病和未来风险预测。虽然ECGFounder扩展了心脏疾病覆盖范围，但仍需要一个全面的健康分析模型。

Method: 构建了包含1320万份心电图（来自298万患者）的多中心数据集，通过迁移学习对ECGFounder进行微调，开发了AnyECG基础模型。使用外部验证队列和10年纵向队列评估当前诊断、未来风险预测和共病识别性能。

Result: AnyECG在1172种疾病中表现出系统性预测能力，其中306种疾病的AUROC超过0.7。模型揭示了新的疾病关联、稳健的共病模式和未来疾病风险。代表性示例包括：甲状旁腺功能亢进症（AUROC 0.941）、2型糖尿病（0.803）、克罗恩病（0.817）、淋巴性白血病（0.856）、慢性阻塞性肺病（0.773）。

Conclusion: AnyECG基础模型提供了充分证据，表明AI-ECG可以作为同时进行疾病检测和长期风险预测的系统性工具。

Abstract: Background: Artificial intelligence enabled electrocardiography (AI-ECG) has demonstrated the ability to detect diverse pathologies, but most existing models focus on single disease identification, neglecting comorbidities and future risk prediction. Although ECGFounder expanded cardiac disease coverage, a holistic health profiling model remains needed.
  Methods: We constructed a large multicenter dataset comprising 13.3 million ECGs from 2.98 million patients. Using transfer learning, ECGFounder was fine-tuned to develop AnyECG, a foundation model for holistic health profiling. Performance was evaluated using external validation cohorts and a 10-year longitudinal cohort for current diagnosis, future risk prediction, and comorbidity identification.
  Results: AnyECG demonstrated systemic predictive capability across 1172 conditions, achieving an AUROC greater than 0.7 for 306 diseases. The model revealed novel disease associations, robust comorbidity patterns, and future disease risks. Representative examples included high diagnostic performance for hyperparathyroidism (AUROC 0.941), type 2 diabetes (0.803), Crohn disease (0.817), lymphoid leukemia (0.856), and chronic obstructive pulmonary disease (0.773).
  Conclusion: The AnyECG foundation model provides substantial evidence that AI-ECG can serve as a systemic tool for concurrent disease detection and long-term risk prediction.

</details>


### [10] [LSR-Net: A Lightweight and Strong Robustness Network for Bearing Fault Diagnosis in Noise Environment](https://arxiv.org/abs/2601.10761)
*Junseok Lee,Jihye Shin,Sangyong Lee,Chang-Jae Chun*

Main category: eess.SP

TL;DR: 提出LSR-Net轻量级强鲁棒网络，用于旋转轴承在噪声环境下的实时故障诊断，结合去噪特征增强模块和轻量化设计，在噪声环境中表现最佳且计算复杂度最低。


<details>
  <summary>Details</summary>
Motivation: 旋转轴承在现代工业中至关重要，但因其高速、高负载和恶劣运行环境而故障率高。故障诊断延迟可能导致经济损失和生命损失，且振动信号易受环境和噪声影响，因此需要在噪声环境中进行准确诊断。

Method: 1. 设计去噪特征增强模块(DFEM)：通过基于卷积的去噪(CD)块对特征图施加非线性处理，生成3通道2D矩阵；采用自适应剪枝增强强噪声下的去噪能力。2. 轻量化模型设计：使用组卷积(GConv)、组点卷积(GPConv)和通道分割设计卷积效率洗牌(CES)块，保持低参数量；通过注意力机制和通道洗牌平衡精度与计算复杂度。

Result: 在噪声环境下使用振动信号验证，所提模型相比基准模型具有最佳的抗噪声能力，且模型计算复杂度最低。

Conclusion: LSR-Net成功实现了在噪声环境下的准确轴承故障诊断，兼具轻量化和强鲁棒性，能够满足实时诊断需求。

Abstract: Rotating bearings play an important role in modern industries, but have a high probability of occurrence of defects because they operate at high speed, high load, and poor operating environments. Therefore, if a delay time occurs when a bearing is diagnosed with a defect, this may cause economic loss and loss of life. Moreover, since the vibration sensor from which the signal is collected is highly affected by the operating environment and surrounding noise, accurate defect diagnosis in a noisy environment is also important. In this paper, we propose a lightweight and strong robustness network (LSR-Net) that is accurate in a noisy environment and enables real-time fault diagnosis. To this end, first, a denoising and feature enhancement module (DFEM) was designed to create a 3-channel 2D matrix by giving several nonlinearity to the feature-map that passed through the denoising module (DM) block composed of convolution-based denoising (CD) blocks. Moreover, adaptive pruning was applied to DM to improve denoising ability when the power of noise is strong. Second, for lightweight model design, a convolution-based efficiency shuffle (CES) block was designed using group convolution (GConv), group pointwise convolution (GPConv) and channel split that can design the model while maintaining low parameters. In addition, the trade-off between the accuracy and model computational complexity that can occur due to the lightweight design of the model was supplemented using attention mechanisms and channel shuffle. In order to verify the defect diagnosis performance of the proposed model, performance verification was conducted in a noisy environment using a vibration signal. As a result, it was confirmed that the proposed model had the best anti-noise ability compared to the benchmark models, and the computational complexity of the model was also the lowest.

</details>


### [11] [Physically constrained unfolded multi-dimensional OMP for large MIMO systems](https://arxiv.org/abs/2601.10771)
*Nay Klaimi,Clément Elvira,Philippe Mary,Luc Le Magoarou*

Main category: eess.SP

TL;DR: MOMPnet：一种基于深度展开的多字典稀疏恢复框架，解决传统方法在信道估计和定位中的模型不准确和计算复杂度高的问题


<details>
  <summary>Details</summary>
Motivation: 传统稀疏恢复方法依赖精确的物理模型（现实中很少完美已知），且在大规模MIMO系统中字典维度增加时计算复杂度急剧增长

Method: 结合深度展开与数据驱动的字典学习，使用多个独立的小字典而非单个大字典，实现低复杂度的多维正交匹配追踪算法

Result: 在真实信道数据上评估，相比多个基线方法表现出强大性能

Conclusion: MOMPnet框架能有效缓解硬件损伤，保持可解释性，同时降低计算复杂度，具有实际应用潜力

Abstract: Sparse recovery methods are essential for channel estimation and localization in modern communication systems, but their reliability relies on accurate physical models, which are rarely perfectly known. Their computational complexity also grows rapidly with the dictionary dimensions in large MIMO systems. In this paper, we propose MOMPnet, a novel unfolded sparse recovery framework that addresses both the reliability and complexity challenges of traditional methods. By integrating deep unfolding with data-driven dictionary learning, MOMPnet mitigates hardware impairments while preserving interpretability. Instead of a single large dictionary, multiple smaller, independent dictionaries are employed, enabling a low-complexity multidimensional Orthogonal Matching Pursuit algorithm. The proposed unfolded network is evaluated on realistic channel data against multiple baselines, demonstrating its strong performance and potential.

</details>


### [12] [Adaptive algorithm for microsensor in sustainable environmental monitoring](https://arxiv.org/abs/2601.10780)
*Nursultan Daupayev,Christian Engel,Ricky Bendyk,Soeren Hirsch*

Main category: eess.SP

TL;DR: 提出基于傅里叶变换的数据采集处理算法，通过谐波分析提取主导频率成分，实现事件触发式传感器激活，减少能耗和存储需求


<details>
  <summary>Details</summary>
Motivation: 传统传感器数据采集产生大量数据，导致持续功耗和存储空间需求增加，需要更高效的数据采集方法

Method: 基于离散傅里叶变换的数据采集处理算法，利用谐波分析提取主导频率成分，识别频率峰值，实现事件触发式传感器激活

Result: 算法使传感器仅在事件发生时激活，同时保留检测缺陷所需的关键信息，如建筑物表面结构缺陷检测，确保预测准确性

Conclusion: 提出的算法能有效减少传感器功耗和存储需求，同时保持对结构缺陷检测的准确性，为智能监测系统提供高效解决方案

Abstract: Traditional data collection from sensors produce a lot of data, which lead to constant power consumption and require more storage space. This study proposes an algorithm for a data acquisition and processing method based on Fourier transform (DFT), which extracts dominant frequency components using harmonic analysis (HA) to identify frequency peaks. This algorithm allows sensors to activate only when an event occurs, while preserving critical information for detecting defects, such as those in the surface structures of buildings and ensuring accuracy for further predictions.

</details>


### [13] [RIS-aided Radar Detection Architectures with Application to Low-RCS Targets](https://arxiv.org/abs/2601.10846)
*Fabiola Colone,Filippo Costa,Yiding Gao,Chengpeng Hao,Linjie Yan,Giuliano Manara,Danilo Orlando*

Main category: eess.SP

TL;DR: 利用可重构智能表面辅助雷达检测低可观测目标，通过联合处理单站和双站回波信号，提高对隐身目标的检测能力


<details>
  <summary>Details</summary>
Motivation: 传统多基地雷达网络用于反隐身存在同步、成本、相位相干性和能耗等问题，需要更有效的解决方案来检测低可观测目标

Method: 使用可重构智能表面形成联合单站和双站配置，拦截目标在不同方向散射的能量并重定向回雷达，设计了五种具有恒虚警率特性的检测架构

Result: 提出的策略相比传统检测器能有效检测低可观测目标，并提供了满足应用需求的可重构智能表面设计指南

Conclusion: 利用可重构智能表面辅助的雷达检测方法为解决低可观测目标检测问题提供了有效解决方案

Abstract: In this paper, we address the radar detection of low observable targets with the assistance of a reconfigurable intelligent surface (RIS). Instead of using a multistatic radar network as counter-stealth strategy with its synchronization, costs, phase coherence, and energy consumption issues, we exploit a RIS to form a joint monostatic and bistatic configuration that can intercept the energy backscattered by the target along irrelevant directions different from the line-of-sight of the radar. Then, this energy is redirected towards the radar that capitalizes all the backscattered energy to detect the low observable target. To this end, five different detection architectures are devised that jointly process monostatic and bistatic echoes and exhibit the constant false alarm rate property at least with respect to the clutter power. To support the practical implementation, we also provide a guideline for the design of a RIS that satisfies the operating requirements of the considered application. The performance analysis is carried out in comparison with conventional detectors and shows that the proposed strategy leads to effective solutions to the detection of low observable targets.

</details>


### [14] [Large Wireless Foundation Models: Stronger over Bigger](https://arxiv.org/abs/2601.10963)
*Xiang Cheng,Boxun Liu,Xuanyu Liu,Xuesong Cai*

Main category: eess.SP

TL;DR: 该论文提出了大型无线基础模型（LWFMs）的概念，旨在解决现有AI通信系统泛化能力差的问题，为6G物理层提供具有强推理和泛化能力的基础模型框架。


<details>
  <summary>Details</summary>
Motivation: 现有AI通信系统采用任务特定模型，泛化能力差；而通信系统本质是通用系统，需要支持多样化场景。基础模型具有强推理和泛化能力，但无线系统约束阻碍了LLM类模型在无线领域的直接应用。

Method: 提出LWFMs概念和框架，包括两种实现范式：1）利用现有通用基础模型；2）构建新型无线基础模型。提炼两种范式的技术路线，制定无线约束下的设计原则，并通过案例研究验证优势。

Result: 建立了LWFMs的概念框架，提出了两种实现范式和技术路线，制定了无线约束下的设计原则，通过案例研究验证了LWFMs的优势，并对"大型"概念进行了多维度分析。

Conclusion: LWFMs为解决AI通信系统泛化问题提供了有前景的解决方案，为6G物理层赋能基础模型奠定了理论基础，并指出了未来研究方向。

Abstract: AI-communication integration is widely regarded as a core enabling technology for 6G. Most existing AI-based physical-layer designs rely on task-specific models that are separately tailored to individual modules, resulting in poor generalization. In contrast, communication systems are inherently general-purpose and should support broad applicability and robustness across diverse scenarios. Foundation models offer a promising solution through strong reasoning and generalization, yet wireless-system constraints hinder a direct transfer of large language model (LLM)-style success to the wireless domain. Therefore, we introduce the concept of large wireless foundation models (LWFMs) and present a novel framework for empowering the physical layer with foundation models under wireless constraints. Specifically, we propose two paradigms for realizing LWFMs, including leveraging existing general-purpose foundation models and building novel wireless foundation models. Based on recent progress, we distill two roadmaps for each paradigm and formulate design principles under wireless constraints. We further provide case studies of LWFM-empowered wireless systems to intuitively validate their advantages. Finally, we characterize the notion of "large" in LWFMs through a multidimensional analysis of existing work and outline promising directions for future research.

</details>


### [15] [DuTrack: Long-Term Indoor Human Tracking with Dual-Channel Sensing and Inference](https://arxiv.org/abs/2601.10972)
*Mengning Li,Wenye Wang*

Main category: eess.SP

TL;DR: DuTrack：一种融合Wi-Fi和声学传感的多模态跟踪系统，通过声学信号校正Wi-Fi累积误差，实现稳定的人体跟踪


<details>
  <summary>Details</summary>
Motivation: 当前基于速度特征的Wi-Fi跟踪方法存在累积误差问题，难以实现长期稳定跟踪。需要一种能够校正Wi-Fi累积误差的稳定跟踪方案。

Method: 提出融合Wi-Fi和声学传感的多模态跟踪系统。理论建模：Wi-Fi在视距和非视距场景分别建模为椭圆菲涅尔区和双曲线区；声学传感区建模为双曲线簇。建立电磁波与机械波融合的优化方程，并设计数据驱动架构求解该方程。

Result: 实验结果显示，与基于模型的方法相比，中位跟踪误差减少89.37%；与数据驱动方法相比，中位跟踪误差减少65.02%。

Conclusion: DuTrack通过融合Wi-Fi和声学传感，有效解决了Wi-Fi跟踪的累积误差问题，实现了稳定的人体跟踪，为智能家居和家庭护理应用提供了可靠的技术方案。

Abstract: Wi-Fi tracking technology demonstrates promising potential for future smart home and intelligent family care. Currently, accurate Wi-Fi tracking methods rely primarily on fine-grained velocity features. However, such velocity-based approaches suffer from the problem of accumulative errors, making it challenging to stably track users' trajectories over a long period of time. This paper presents DuTrack, a fusion-based tracking system for stable human tracking. The fundamental idea is to leverage the ubiquitous acoustic signals in households to rectify the accumulative Wi-Fi tracking error. Theoretically, Wi-Fi sensing in line-of-sight (LoS) and non-line-of-sight (NLoS) scenarios can be modeled as elliptical Fresnel zones and hyperbolic zones, respectively. By designing acoustic sensing signals, we are able to model the acoustic sensing zones as a series of hyperbolic clusters. We reveal how to fuse the fields of electromagnetic waves and mechanical waves, and establish the optimization equation. Next, we design a data-driven architecture to solve the aforementioned optimization equation. Experimental results show that the proposed multimodal tracking scheme exhibits superior performance. We achieve a 89.37% reduction in median tracking error compared to model-based methods and a 65.02% reduction compared to data-driven methods.

</details>


### [16] [Delay-Aware Task Offloading for Heterogeneous VLC-RF-based Vehicular Fog Computing](https://arxiv.org/abs/2601.10978)
*Nan An,Hongyi He,Fang Yang,Chang Liu,Jian Song,Zhu Han,Binbin Zhu*

Main category: eess.SP

TL;DR: 提出一种基于可见光通信(VLC)和射频(RF)的异构架构用于车载雾计算系统，通过动态任务分割和卸载优化，相比单一通信方式平均任务处理延迟降低15%


<details>
  <summary>Details</summary>
Motivation: 传统车载雾计算依赖射频通信，在密集车辆环境中适应性受限。需要利用VLC的抗干扰优势和RF的覆盖优势，提升任务卸载效率

Method: 设计异构VLC-RF架构，将计算任务动态分割并通过VLC和RF链路卸载到空闲车辆。提出基于残差的主化最小化(RBMM)算法优化任务卸载和计算资源分配

Result: 仿真结果表明，提出的异构VLC-RF架构结合RBMM算法，相比仅使用VLC或RF的车载雾计算系统，平均任务处理延迟降低15%

Conclusion: 异构VLC-RF架构能有效结合两种通信技术的优势，显著降低任务处理延迟，为下一代交通网络中的延迟敏感服务提供支持

Abstract: Vehicular fog computing (VFC) is a promising paradigm for reducing the computation burden of vehicles, thus supporting delay-sensitive services in next-generation transportation networks. However, traditional VFC schemes rely on radio frequency (RF) communications, which limits their adaptability for dense vehicular environments. In this paper, a heterogeneous visible light communication (VLC)-RF architecture is designed for VFC systems to facilitate efficient task offloading. Specifically, computing tasks are dynamically partitioned and offloaded to idle vehicles via both VLC and RF links, thereby fully exploiting the interference resilience of VLC and the coverage advantage of RF. To minimize the average task processing delay (TPD), an optimization problem of task offloading and computing resource allocation is formulated, and then solved by the developed residual-based majorization-minimization (RBMM) algorithm. Simulation results confirm that the heterogeneous VLC-RF architecture with the proposed algorithm achieves a 15% average TPD reduction compared to VFC systems relying solely on VLC or RF.

</details>


### [17] [Uni-Fi: Integrated Multi-Task Wi-Fi Sensing](https://arxiv.org/abs/2601.10980)
*Mengning Li,Wenye Wang*

Main category: eess.SP

TL;DR: Uni-Fi是一个可扩展的多任务Wi-Fi感知集成框架，通过统一架构和可扩展流水线解决不同感知任务集成难题，在定位、活动分类和存在检测任务上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi感知技术支持智能家居应用，但不同感知任务集成面临两大挑战：缺乏统一架构捕捉任务间共性，以及缺乏可扩展流水线整合未来研究方法。

Method: 提出Uni-Fi框架：1) 统一理论框架揭示单任务与多任务感知的根本差异；2) 可扩展感知流水线自动生成多任务感知求解器，实现多个感知模型的无缝集成。

Result: 实验结果显示Uni-Fi在多个任务上表现稳健：定位误差约0.54米，活动分类准确率98.34%，存在检测准确率98.57%。

Conclusion: Uni-Fi通过统一架构和可扩展流水线成功解决了多任务Wi-Fi感知集成问题，为未来智能家居感知应用提供了有效的框架支持。

Abstract: Wi-Fi sensing technology enables non-intrusive, continuous monitoring of user locations and activities, which supports diverse smart home applications. Since different sensing tasks exhibit contextual relationships, their integration can enhance individual module performance. However, integrating sensing tasks across different research efforts faces challenges due to the absence of two key elements. The first is a unified architecture that captures the fundamental nature shared across diverse sensing tasks. The second is an extensible pipeline that can integrate sensing methodologies proposed in potential future research. This paper presents Uni-Fi, an extensible framework for multi-task Wi-Fi sensing integration. This paper makes the following contributions. First, we propose a unified theoretical framework that reveals the fundamental differences between single-task and multi-task sensing. Second, we develop a scalable sensing pipeline that automatically generates multi-task sensing solvers, enabling seamless integration of multiple sensing models. Experimental results show that Uni-Fi achieves robust performance across tasks, with a localization error of approximately 0.54 meters, 98.34 percent accuracy for activity classification, and 98.57 percent accuracy for presence detection.

</details>


### [18] [Hybrid Resource Allocation Scheme for Bistatic ISAC with Data Channels](https://arxiv.org/abs/2601.11110)
*Marcus Henninger,Lucas Giroto,Ahmed Elkelesh,Silvio Mandelli*

Main category: eess.SP

TL;DR: 论文提出了一种用于双基地ISAC的混合资源分配方案，通过将低调制阶数符号作为伪导频放置在合适的感知网格上，在轻微降低通信链路频谱效率的同时显著提升双基地感知性能。


<details>
  <summary>Details</summary>
Motivation: 双基地ISAC能够有效重用现有蜂窝基础设施，在未来感知网络中扮演重要角色。然而，使用数据信道进行ISAC时存在资源分配冲突：通信链路希望传输更高调制阶数符号以最大化吞吐量，而感知则偏好更低调制阶数以获得更高的雷达图像信噪比。

Method: 提出混合资源分配方案，通过在合适的感知网格上放置低调制阶数符号作为伪导频，在提升双基地感知性能的同时仅轻微降低通信链路的频谱效率。

Result: 仿真结果验证了该方法相对于不同基线的有效性，并提供了关于解码错误如何影响感知性能的实际见解。

Conclusion: 所提出的混合资源分配方案能够有效解决双基地ISAC中通信与感知的资源分配冲突，在保证通信性能的同时显著提升感知能力。

Abstract: Bistatic integrated sensing and communication (ISAC) enables efficient reuse of the existing cellular infrastructure and is likely to play an important role in future sensing networks. In this context, ISAC using the data channel is a promising approach to improve the bistatic sensing performance compared to relying solely on pilots. One of the challenges associated with this approach is resource allocation: the communication link aims to transmit higher modulation order (MO) symbols to maximize the throughput, whereas a lower MO is preferable for sensing to achieve a higher signal-to-noise ratio in the radar image. To address this conflict, this paper introduces a hybrid resource allocation scheme. By placing lower MO symbols as pseudo-pilots on a suitable sensing grid, we enhance the bistatic sensing performance while only slightly reducing the spectral efficiency of the communication link. Simulation results validate our approach against different baselines and provide practical insights into how decoding errors affect the sensing performance.

</details>


### [19] [Comprehensive Robust Dynamic Mode Decomposition from Mode Extraction to Dimensional Reduction](https://arxiv.org/abs/2601.11116)
*Yuki Nakamura,Shingo Takemoto,Shunsuke Ono*

Main category: eess.SP

TL;DR: 提出CR-DMD框架，通过凸优化预处理去除混合噪声，再用凸优化降维构建忠实低维表示，在噪声条件下优于现有鲁棒DMD方法。


<details>
  <summary>Details</summary>
Motivation: 标准DMD依赖最小二乘估计计算线性时间演化算子，在噪声下性能显著下降。现有鲁棒变体通常修改最小二乘公式，但仍不稳定且无法确保忠实的低维表示。

Method: 1. 基于凸优化的预处理方法，有效去除混合噪声，实现准确稳定的模态提取；2. 新的凸优化降维公式，将鲁棒提取的模态与原始噪声观测显式关联，通过模态的稀疏加权和构建原始数据的忠实表示。使用预条件原始-对偶分裂方法高效求解。

Result: 在流体动力学数据集上的实验表明，CR-DMD在噪声条件下的模态准确性和低维表示保真度方面，始终优于最先进的鲁棒DMD方法。

Conclusion: CR-DMD框架通过鲁棒化整个DMD过程（从模态提取到降维），有效应对混合噪声，提供了更稳定和忠实的低维表示。

Abstract: We propose Comprehensive Robust Dynamic Mode Decomposition (CR-DMD), a novel framework that robustifies the entire DMD process - from mode extraction to dimensional reduction - against mixed noise. Although standard DMD widely used for uncovering spatio-temporal patterns and constructing low-dimensional models of dynamical systems, it suffers from significant performance degradation under noise due to its reliance on least-squares estimation for computing the linear time evolution operator. Existing robust variants typically modify the least-squares formulation, but they remain unstable and fail to ensure faithful low-dimensional representations. First, we introduce a convex optimization-based preprocessing method designed to effectively remove mixed noise, achieving accurate and stable mode extraction. Second, we propose a new convex formulation for dimensional reduction that explicitly links the robustly extracted modes to the original noisy observations, constructing a faithful representation of the original data via a sparse weighted sum of the modes. Both stages are efficiently solved by a preconditioned primal-dual splitting method. Experiments on fluid dynamics datasets demonstrate that CR-DMD consistently outperforms state-of-the-art robust DMD methods in terms of mode accuracy and fidelity of low-dimensional representations under noisy conditions.

</details>


### [20] [Scalable mm-Wave Liquid Crystal Reconfigurable Intelligent Surfaces based on the Delay Line Architecture](https://arxiv.org/abs/2601.11307)
*Julia Schwarzbeck,Robin Neuder,Marc Späth,Alejandro Jiménez-Sáez*

Main category: eess.SP

TL;DR: 该论文提出了一种基于延迟线架构的宽带液晶可重构智能表面，工作在60GHz频段，最多可达750个辐射单元，实现了宽带宽、连续相位控制和快速响应。


<details>
  <summary>Details</summary>
Motivation: 开发高性能、可扩展的毫米波可重构智能表面，解决传统方法在带宽、相位控制和响应速度方面的限制，同时降低功耗。

Method: 采用延迟线架构将相位控制层与辐射层解耦，使用4.6微米薄液晶层，实现列式偏置控制，设计了120和750单元两种原型。

Result: 实现了±60°波束扫描，-3dB带宽超过9%，单元功耗纳瓦级，模拟预测孔径效率超过20%，实测效率为9.2%和2.6%。

Conclusion: 延迟线架构的液晶可重构智能表面相比传统方法具有明显优势，验证了架构的可扩展性，实验室环境的技术挑战影响了实测效率。

Abstract: This paper presents the design, fabrication, and characterization of broadband liquid crystal (LC) reconfigurable intelligent surfaces (RIS) operating around 60 GHz and scaling up to 750 radiating elements. The RISs employ a delay line architecture (DLA) that decouples the phase shifting and radiating layer, enabling wide bandwidth, continuous phase control exceeding 360°, and fast response times with a micrometer-thin LC layer of 4.6 micrometer. Two prototypes with 120 and 750 elements are realized using identical unit cells and column-wise biasing. Measurements demonstrate beam steering over +-60° and -3 dB bandwidths exceeding 9% for both apertures, confirming the scalability of the proposed architecture. On top of a measured nanowatt power consumption per unit cell, aperture efficiencies above 20% are predicted by simulations. While the measured efficiencies are reduced to 9.2% and 2.6%, a detailed analysis verifies that this reduction can be attributed to technological challenges in a laboratory environment. Finally, a comprehensive comparison between the applied DLA-based LC-RIS and a conventional approach highlights the superior potential of applied architecture.

</details>


### [21] [Modulation, ISI, and Detection for Langmuir Adsorption-Based Microfluidic Molecular Communication](https://arxiv.org/abs/2601.11351)
*Ruifeng Zheng,Pengjie Zhou,Pit Hofmann,Martín Schottlender,Fatima Rani,Juan A. Cabrera,Frank H. P. Fitzek*

Main category: eess.SP

TL;DR: 该论文研究了有限容量Langmuir吸附驱动的微流控分子通信接收器，推导了反应限制区间的闭式单脉冲响应核和符号率递归，揭示了信道记忆和符号间干扰，并提出了低复杂度检测器。


<details>
  <summary>Details</summary>
Motivation: 研究微流控分子通信系统中有限容量Langmuir吸附接收器的性能，特别是在反应限制区间内，需要理解信道记忆、符号间干扰以及随机性对检测性能的影响。

Method: 在反应限制区间推导闭式单脉冲响应核和符号率递归；开发短脉冲和长脉冲近似；采用有限受体二项计数模型和脉冲结束采样；提出低复杂度中点阈值检测器。

Result: 揭示了长脉冲区间由于饱和导致的干扰不对称性；提出的检测器在干扰可忽略时简化为固定阈值；数值结果验证了所提出的表征方法并量化了检测性能。

Conclusion: 该研究为微流控分子通信接收器提供了理论分析框架，揭示了有限容量Langmuir吸附对信道特性的影响，并提出了实用的低复杂度检测方案。

Abstract: This paper studies microfluidic molecular communication receivers with finite-capacity Langmuir adsorption driven by an effective surface concentration. In the reaction-limited regime, we derive a closed-form single-pulse response kernel and a symbol-rate recursion for on-off keying that explicitly exposes channel memory and inter-symbol interference. We further develop short-pulse and long-pulse approximations, revealing an interference asymmetry in the long-pulse regime due to saturation. To account for stochasticity, we adopt a finite-receptor binomial counting model, employ pulse-end sampling, and propose a low-complexity midpoint-threshold detector that reduces to a fixed threshold when interference is negligible. Numerical results corroborate the proposed characterization and quantify detection performance versus pulse and symbol durations.

</details>


### [22] [Channel Estimation in MIMO Systems Aided by Microwave Linear Analog Computers (MiLACs)](https://arxiv.org/abs/2601.11438)
*Qiaosen Zhang,Matteo Nerini,Bruno Clerckx*

Main category: eess.SP

TL;DR: 提出用于微波线性模拟计算机辅助MIMO系统的全模拟域LS和MMSE信道估计方案，显著降低计算复杂度、硬件要求和PAPR


<details>
  <summary>Details</summary>
Motivation: 微波线性模拟计算机(MiLAC)在大规模MIMO系统中能大幅降低硬件和计算成本，但信道估计仍是未解决问题。传统LS和MMSE估计依赖密集数字计算，削弱了MiLAC的优势。

Method: 设计由MiLAC实现的训练预编码器和组合器，使LS和MMSE估计完全在模拟域进行，性能与数字方案相同但复杂度大幅降低。

Result: 数值结果验证了所提方案的有效性和优势，实现了与数字方案相同的性能，同时显著减少了计算复杂度、发射RF链、ADC/DAC分辨率要求和PAPR。

Conclusion: 提出的全模拟域信道估计方案解决了MiLAC辅助MIMO系统的关键问题，充分发挥了MiLAC在硬件和计算成本方面的优势。

Abstract: Microwave linear analog computers (MiLACs) have recently emerged as a promising solution for future gigantic multiple-input multiple-output (MIMO) systems, enabling beamforming with greatly reduced hardware and computational cost. However, channel estimation for MiLAC-aided systems remains an open problem. Conventional least squares (LS) and minimum mean square error (MMSE) estimation rely on intensive digital computation, which undermines the benefits offered by MiLACs. In this letter, we propose efficient LS and MMSE channel estimation schemes for MiLAC-aided MIMO systems. By designing training precoders and combiners implemented by MiLACs, both LS and MMSE estimation are performed fully in the analog domain, achieving identical performance to their digital counterparts while significantly reducing computational complexity, transmit RF chains, analog-to-digital/digital-to-analog converters (ADCs/DACs) resolution requirements, and peak-to-average power ratio (PAPR). Numerical results verify the effectiveness and advantages of the proposed schemes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Analytic Bijections for Smooth and Interpretable Normalizing Flows](https://arxiv.org/abs/2601.10774)
*Mathis Gerdes,Miranda C. N. Cheng*

Main category: cs.LG

TL;DR: 提出三种解析可逆的全局平滑双射函数（三次有理、sinh、三次多项式），以及新颖的径向流架构，在保持训练稳定性的同时显著减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有归一化流方法存在权衡：仿射变换平滑可逆但表达能力有限；单调样条局部可控但分段平滑且定义域有界；残差流平滑但需要数值求逆。需要结合各种方法优点的全局平滑、解析可逆的双射函数。

Method: 1. 提出三种解析可逆的全局平滑双射函数：三次有理函数、sinh函数、三次多项式。2. 开发径向流架构：直接参数化径向坐标变换，保持角度方向不变。3. 在耦合流中作为即插即用组件，并在φ⁴晶格场论等高维物理问题上进行验证。

Result: 1. 新双射函数在耦合流中匹配或超越样条性能。2. 径向流展现出卓越的训练稳定性，产生几何可解释的变换，对于具有径向结构的目标，仅用千分之一参数即可达到耦合流相当的质量。3. 在φ⁴晶格场论实验中，新方法优于仿射基线，并能针对特定问题设计解决模式坍塌问题。

Conclusion: 提出的三种解析可逆双射函数和径向流架构结合了现有方法的优点，提供了全局平滑、解析可逆的解决方案，在保持训练稳定性的同时显著减少参数需求，为高维物理问题等应用提供了有效的归一化流设计。

Abstract: A key challenge in designing normalizing flows is finding expressive scalar bijections that remain invertible with tractable Jacobians. Existing approaches face trade-offs: affine transformations are smooth and analytically invertible but lack expressivity; monotonic splines offer local control but are only piecewise smooth and act on bounded domains; residual flows achieve smoothness but need numerical inversion. We introduce three families of analytic bijections -- cubic rational, sinh, and cubic polynomial -- that are globally smooth ($C^\infty$), defined on all of $\mathbb{R}$, and analytically invertible in closed form, combining the favorable properties of all prior approaches. These bijections serve as drop-in replacements in coupling flows, matching or exceeding spline performance. Beyond coupling layers, we develop radial flows: a novel architecture using direct parametrization that transforms the radial coordinate while preserving angular direction. Radial flows exhibit exceptional training stability, produce geometrically interpretable transformations, and on targets with radial structure can achieve comparable quality to coupling flows with $1000\times$ fewer parameters. We provide comprehensive evaluation on 1D and 2D benchmarks, and demonstrate applicability to higher-dimensional physics problems through experiments on $φ^4$ lattice field theory, where our bijections outperform affine baselines and enable problem-specific designs that address mode collapse.

</details>


### [24] [Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework](https://arxiv.org/abs/2601.10779)
*Qingyue Zhang,Chang Chu,Haohao Fu,Tianren Peng,Yanru Wu,Guanbo Huang,Yang Li,Shao-Lun Huang*

Main category: cs.LG

TL;DR: 提出UOWQ框架，统一优化多源迁移学习中的源权重和迁移样本量，理论证明全样本迁移最优，并提供权重闭式解和优化算法。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习方法通常单独优化源权重或迁移样本量，忽略了二者的联合考虑，且均匀迁移可能导致负迁移，需要平衡异构源的贡献。

Method: 提出UOWQ理论框架，基于KL散度泛化误差的渐近分析，将多源迁移学习建模为参数估计问题，联合确定最优源权重和迁移样本量。

Result: 理论证明：权重调整后全样本迁移总是最优的；单源场景有闭式解，多源场景通过凸优化求解；实验在DomainNet和Office-Home等基准上优于基线方法。

Conclusion: UOWQ框架统一优化权重和迁移量，理论严谨且实践有效，解决了多源迁移学习中的负迁移问题，为迁移学习提供了新的理论指导。

Abstract: Transfer learning plays a vital role in improving model performance in data-scarce scenarios. However, naive uniform transfer from multiple source tasks may result in negative transfer, highlighting the need to properly balance the contributions of heterogeneous sources. Moreover, existing transfer learning methods typically focus on optimizing either the source weights or the amount of transferred samples, while largely neglecting the joint consideration of the other. In this work, we propose a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), which formulates multi-source transfer learning as a parameter estimation problem grounded in an asymptotic analysis of a Kullback-Leibler divergence-based generalization error measure. The proposed framework jointly determines the optimal source weights and optimal transfer quantities for each source task. Firstly, we prove that using all available source samples is always optimal once the weights are properly adjusted, and we provide a theoretical explanation for this phenomenon. Moreover, to determine the optimal transfer weights, our analysis yields closed-form solutions in the single-source setting and develops a convex optimization-based numerical procedure for the multi-source case. Building on the theoretical results, we further propose practical algorithms for both multi-source transfer learning and multi-task learning settings. Extensive experiments on real-world benchmarks, including DomainNet and Office-Home, demonstrate that UOWQ consistently outperforms strong baselines. The results validate both the theoretical predictions and the practical effectiveness of our framework.

</details>


### [25] [FSL-BDP: Federated Survival Learning with Bayesian Differential Privacy for Credit Risk Modeling](https://arxiv.org/abs/2601.11134)
*Sultan Amed,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: 论文提出FSL-BDP框架，结合联邦学习和贝叶斯差分隐私来建模违约时间，解决数据保护法规下的跨机构信用风险建模问题，实验显示联邦设置改变了隐私机制的性能排名。


<details>
  <summary>Details</summary>
Motivation: 传统违约预测存在两个问题：二元分类忽略违约时间（将早期和晚期违约者同等对待），集中式训练违反新兴的数据保护法规（如GDPR、CCPA）。金融机构需要在不共享敏感数据的情况下进行跨机构学习。

Method: 提出联邦生存学习框架（FSL-BDP），结合贝叶斯差分隐私，在不集中敏感数据的情况下建模违约时间轨迹。该框架提供数据依赖的差分隐私保证，同时让机构共同学习风险动态。

Result: 在三个真实信用数据集（LendingClub、SBA、Bondora）上的实验表明：联邦设置改变了隐私机制的性能排名。在集中式设置中，经典差分隐私优于贝叶斯差分隐私，但在联邦设置中，贝叶斯差分隐私获益更大（+7.0% vs +1.4%），达到接近非私有性能的水平，并在大多数参与客户中优于经典差分隐私。

Conclusion: 隐私机制的选择应在目标部署架构中评估，而不是基于集中式基准。这为在受监管的多机构环境中设计隐私保护决策支持系统提供了实用指导。

Abstract: Credit risk models are a critical decision-support tool for financial institutions, yet tightening data-protection rules (e.g., GDPR, CCPA) increasingly prohibit cross-border sharing of borrower data, even as these models benefit from cross-institution learning. Traditional default prediction suffers from two limitations: binary classification ignores default timing, treating early defaulters (high loss) equivalently to late defaulters (low loss), and centralized training violates emerging regulatory constraints. We propose a Federated Survival Learning framework with Bayesian Differential Privacy (FSL-BDP) that models time-to-default trajectories without centralizing sensitive data. The framework provides Bayesian (data-dependent) differential privacy (DP) guarantees while enabling institutions to jointly learn risk dynamics. Experiments on three real-world credit datasets (LendingClub, SBA, Bondora) show that federation fundamentally alters the relative effectiveness of privacy mechanisms. While classical DP performs better than Bayesian DP in centralized settings, the latter benefits substantially more from federation (+7.0\% vs +1.4\%), achieving near parity of non-private performance and outperforming classical DP in the majority of participating clients. This ranking reversal yields a key decision-support insight: privacy mechanism selection should be evaluated in the target deployment architecture, rather than centralized benchmarks. These findings provide actionable guidance for practitioners designing privacy-preserving decision support systems in regulated, multi-institutional environments.

</details>


### [26] [Towards Tensor Network Models for Low-Latency Jet Tagging on FPGAs](https://arxiv.org/abs/2601.10801)
*Alberto Coppi,Ema Puljak,Lorenzo Borella,Daniel Jaschke,Enrique Rico,Maurizio Pierini,Jacopo Pazzini,Andrea Triossi,Simone Montangero*

Main category: cs.LG

TL;DR: 该研究系统评估了张量网络模型（MPS和TTN）在实时喷注标记中的应用，重点关注FPGA低延迟部署，展示了在保持竞争力的同时实现亚微秒级延迟的可行性。


<details>
  <summary>Details</summary>
Motivation: 受HL-LHC Level-1触发系统严格要求的驱动，探索张量网络作为深度神经网络的紧凑且可解释的替代方案，用于高能物理中的实时喷注标记任务。

Method: 使用矩阵乘积态和树张量网络模型，基于低层级喷注组成特征进行分类；研究后训练量化以实现硬件高效实现；对最佳模型进行FPGA综合以评估资源使用、延迟和内存占用。

Result: 模型性能与最先进的深度学习分类器相当；通过量化实现硬件高效实现而不降低分类性能或延迟；FPGA综合显示亚微秒级延迟，支持在线实时触发系统部署的可行性。

Conclusion: 张量网络模型在低延迟环境中具有快速且资源高效推理的潜力，为实时触发系统提供了可行的紧凑替代方案。

Abstract: We present a systematic study of Tensor Network (TN) models $\unicode{x2013}$ Matrix Product States (MPS) and Tree Tensor Networks (TTN) $\unicode{x2013}$ for real-time jet tagging in high-energy physics, with a focus on low-latency deployment on Field Programmable Gate Arrays (FPGAs). Motivated by the strict requirements of the HL-LHC Level-1 trigger system, we explore TNs as compact and interpretable alternatives to deep neural networks. Using low-level jet constituent features, our models achieve competitive performance compared to state-of-the-art deep learning classifiers. We investigate post-training quantization to enable hardware-efficient implementations without degrading classification performance or latency. The best-performing models are synthesized to estimate FPGA resource usage, latency, and memory occupancy, demonstrating sub-microsecond latency and supporting the feasibility of online deployment in real-time trigger systems. Overall, this study highlights the potential of TN-based models for fast and resource-efficient inference in low-latency environments.

</details>


### [27] [Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration](https://arxiv.org/abs/2601.10973)
*Zain ul Abdeen,Waris Gill,Ming Jin*

Main category: cs.LG

TL;DR: 提出MGF-RL框架，结合元学习和进化策略，用于配电网灾后负荷恢复，能快速适应新故障场景，减少重训练需求


<details>
  <summary>Details</summary>
Motivation: 极端事件后恢复关键负荷需要自适应控制，但可再生能源不确定性、可调度资源有限和非线性动态使恢复困难。传统RL泛化能力差，对新故障场景需要大量重训练

Method: 提出元引导无梯度强化学习(MGF-RL)框架，结合一阶元学习和进化策略，从历史故障经验学习可迁移初始化，快速适应新场景，无需梯度计算，处理非线性约束系统动态

Result: 在IEEE 13和123节点测试系统中，MGF-RL优于标准RL、MAML元RL和模型预测控制，在可靠性、恢复速度和适应效率方面表现更好，泛化到未见故障和可再生能源模式，需要更少微调

Conclusion: MGF-RL为可再生能源丰富的配电网实时负荷恢复提供有效解决方案，提供次线性遗憾界限，支持经验增益，激励实际应用

Abstract: Restoring critical loads after extreme events demands adaptive control to maintain distribution-grid resilience, yet uncertainty in renewable generation, limited dispatchable resources, and nonlinear dynamics make effective restoration difficult. Reinforcement learning (RL) can optimize sequential decisions under uncertainty, but standard RL often generalizes poorly and requires extensive retraining for new outage configurations or generation patterns. We propose a meta-guided gradient-free RL (MGF-RL) framework that learns a transferable initialization from historical outage experiences and rapidly adapts to unseen scenarios with minimal task-specific tuning. MGF-RL couples first-order meta-learning with evolutionary strategies, enabling scalable policy search without gradient computation while accommodating nonlinear, constrained distribution-system dynamics. Experiments on IEEE 13-bus and IEEE 123-bus test systems show that MGF-RL outperforms standard RL, MAML-based meta-RL, and model predictive control across reliability, restoration speed, and adaptation efficiency under renewable forecast errors. MGF-RL generalizes to unseen outages and renewable patterns while requiring substantially fewer fine-tuning episodes than conventional RL. We also provide sublinear regret bounds that relate adaptation efficiency to task similarity and environmental variation, supporting the empirical gains and motivating MGF-RL for real-time load restoration in renewable-rich distribution grids.

</details>


### [28] [When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models](https://arxiv.org/abs/2601.11444)
*Raphaël Razafindralambo,Rémy Sun,Frédéric Precioso,Damien Garreau,Pierre-Alexandre Mattei*

Main category: cs.LG

TL;DR: 扩散模型集成通常能改善分数匹配损失和模型似然，但无法稳定提升图像生成质量指标如FID，在表格数据中某些集成策略表现更优


<details>
  <summary>Details</summary>
Motivation: 尽管集成方法在监督学习中已被证明有效，但在无条件分数扩散模型中的应用仍未被充分探索。本研究旨在探究集成是否能为生成建模带来实际益处

Method: 研究多种集成策略：深度集成、蒙特卡洛Dropout等，在CIFAR-10和FFHQ图像数据集上进行实验，同时探索表格数据中的随机森林集成，并提供分数模型求和的理论分析

Result: 分数集成通常能改善分数匹配损失和模型似然，但无法稳定提升FID等感知质量指标。在表格数据中，某些集成策略表现优于其他方法

Conclusion: 扩散模型集成在理论指标上有改进，但在实际图像质量评估中效果有限，这揭示了分数估计与图像质量之间的复杂关系，为模型组合技术提供了理论洞见

Abstract: Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).

</details>


### [29] [Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning -- Towards a Pure Neural Logic Core](https://arxiv.org/abs/2601.10810)
*Mengmeng Peng,Zhenyu Fang,He Sun*

Main category: cs.LG

TL;DR: 论文提出"数字代谢"假说，认为通过选择性遗忘可以蒸馏出纯神经逻辑核心，并引入RLCP双流训练框架实现这一目标，在Qwen2.5-0.5B上观察到模型在遗忘特定事实的同时自发采用链式推理。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在参数纠缠问题，通用推理能力（逻辑）和特定事实知识（事实）在共享权重中处于叠加态，导致"记忆墙"问题，计算能力浪费在模拟检索上，经常产生幻觉。

Method: 提出"数字代谢"热力学假说，认为目标性遗忘对于蒸馏纯神经逻辑核心是必要的。引入再生逻辑核心协议（RLCP），这是一个双流训练框架，通过深层梯度反转使特定事实依赖线性不可解码。

Result: 在Qwen2.5-0.5B上应用RLCP观察到明显的相变：模型对目标事实关联的保留率接近零（准确率<7%），同时表现出与"结构结晶"效应一致的变化。在GSM8K上的实证分析显示，"代谢"后的模型自发采用链式推理（CoT）脚手架。

Conclusion: 研究结果为DeepSeek的Engram等架构创新提供了动态权重层面的对应物，为模块化"神经CPU+符号RAM"架构铺平了道路。虽然行为转变的因果机制需要进一步研究，但发现表明选择性遗忘可以促进更纯粹的推理能力。

Abstract: Large language models (LLMs) currently suffer from parameter entanglement, where general reasoning capabilities (logic) and specific factual knowledge (facts) exist in a superposition state within shared weights. This coupling leads to the "memory wall," where computational capacity is squandered on simulating retrieval, often resulting in hallucinations. In this paper, we propose "digital metabolism," a thermodynamic hypothesis suggesting that targeted forgetting is necessary for distilling a pure neural logic core. To validate this hypothesis, we introduce the Regenerative Logic-Core Protocol (RLCP), a dual-stream training framework that renders specific factual dependencies linearly undecodable via deep-layer gradient reversal. Applying RLCP to Qwen2.5-0.5B, we observe a distinct phase transition: the model achieves near-zero retention of targeted factual associations (Accuracy < 7%) while exhibiting changes consistent with an emergent "structural crystallization" effect. Empirical analysis on GSM8K reveals that the "metabolized" model spontaneously adopts chain-of-thought (CoT) scaffolding, which we interpret as compensating for the loss of direct associative recall (shifting from $O(1)$ recall to $O(N)$ reasoning). While the causal mechanism underlying this behavioral shift requires further investigation, our findings provide a dynamic weight-level counterpart to architectural innovations like DeepSeek's Engram, paving the way for modular "Neural CPU + Symbolic RAM" architectures.

</details>


### [30] [Offline Reinforcement-Learning-Based Power Control for Application-Agnostic Energy Efficiency](https://arxiv.org/abs/2601.11352)
*Akhilesh Raj,Swann Perarnau,Aniruddha Gokhale,Solomon Bekele Abera*

Main category: cs.LG

TL;DR: 使用离线强化学习设计CPU功率控制器，通过离线训练减少在线训练问题，在可接受的性能损失下显著降低并行应用的能耗。


<details>
  <summary>Details</summary>
Motivation: 现代计算基础设施设计中能源效率至关重要，但在线强化学习训练面临模拟环境建模困难、噪声干扰和可靠性问题等挑战，需要更稳健的解决方案。

Method: 采用离线强化学习方法，利用预先收集的状态转换数据集进行训练；结合灰盒方法，使用应用无关的在线性能数据（如心跳）和硬件性能计数器，通过Intel的RAPL控制实时系统功率。

Result: 在各种计算密集型和内存密集型基准测试中，离线训练的智能体能够在可容忍的性能损失下显著降低能耗。

Conclusion: 离线强化学习是设计自主CPU功率控制器的有效替代方案，能够在不显著影响性能的情况下提高并行应用的能源效率。

Abstract: Energy efficiency has become an integral aspect of modern computing infrastructure design, impacting the performance, cost, scalability, and durability of production systems. The incorporation of power actuation and sensing capabilities in CPU designs is indicative of this, enabling the deployment of system software that can actively monitor and adjust energy consumption and performance at runtime. While reinforcement learning (RL) would seem ideal for the design of such energy efficiency control systems, online training presents challenges ranging from the lack of proper models for setting up an adequate simulated environment, to perturbation (noise) and reliability issues, if training is deployed on a live system.
  In this paper we discuss the use of offline reinforcement learning as an alternative approach for the design of an autonomous CPU power controller, with the goal of improving the energy efficiency of parallel applications at runtime without unduly impacting their performance. Offline RL sidesteps the issues incurred by online RL training by leveraging a dataset of state transitions collected from arbitrary policies prior to training.
  Our methodology applies offline RL to a gray-box approach to energy efficiency, combining online application-agnostic performance data (e.g., heartbeats) and hardware performance counters to ensure that the scientific objectives are met with limited performance degradation. Evaluating our method on a variety of compute-bound and memory-bound benchmarks and controlling power on a live system through Intel's Running Average Power Limit, we demonstrate that such an offline-trained agent can substantially reduce energy consumption at a tolerable performance degradation cost.

</details>


### [31] [Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents](https://arxiv.org/abs/2601.10820)
*Himanshu Thakur,Anusha Kamath,Anurag Muthyala,Dhwani Sanmukhani,Smruthi Mukund,Jay Katukuri*

Main category: cs.LG

TL;DR: 提出一个规划引导的多智能体框架，用于自动化特征工程代码生成，解决现有方法在数据集稀缺、个性化集成和人机协作方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成模型在特征工程自动化方面面临三大挑战：1) 缺乏捕捉生产级特征工程迭代复杂编码过程的数据集；2) 现有编码代理（如CoPilot、Devin）难以与团队特定工具、代码库、工作流集成；3) 人机协作时机不当或反馈不足导致效果不佳。

Method: 采用规划引导、约束拓扑的多智能体框架，以多步方式为代码库生成代码。LLM驱动的规划器利用团队环境图来协调可用代理调用，生成上下文感知提示，并使用下游失败来追溯修正上游工件。可在关键步骤请求人工干预。

Result: 在内部数据集上，相比手动构建和无规划工作流，评估指标分别提升38%和150%。在实际应用中，为服务1.2亿用户的推荐模型构建特征时，将特征工程周期从3周缩短到1天。

Conclusion: 该规划引导的多智能体框架有效解决了特征工程自动化的关键挑战，显著提升了代码生成质量和效率，在实际生产环境中实现了重大改进。

Abstract: Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.

</details>


### [32] [MetaboNet: The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management](https://arxiv.org/abs/2601.11505)
*Miriam K. Wolff,Peter Calhoun,Eleonora Maria Aiello,Yao Qin,Sam F. Royston*

Main category: cs.LG

TL;DR: 研究者整合多个公开的1型糖尿病数据集，创建了统一的MetaboNet数据集，包含3135名患者和1228患者年的CGM和胰岛素数据，提供公开下载和受限访问两种获取方式。


<details>
  <summary>Details</summary>
Motivation: 当前1型糖尿病算法开发面临数据碎片化和缺乏标准化的问题，现有数据集结构差异大、访问处理耗时，阻碍了数据整合和算法可比性、泛化性。

Method: 整合多个公开的1型糖尿病数据集，要求同时包含连续血糖监测数据和胰岛素泵剂量记录，保留碳水化合物摄入和体力活动等辅助信息，提供标准化格式转换管道。

Result: 创建了MetaboNet数据集，包含3135名受试者和1228患者年的重叠CGM和胰岛素数据，规模远超现有独立基准数据集，涵盖广泛的血糖谱和人口统计学特征。

Conclusion: MetaboNet为1型糖尿病研究提供了统一的数据资源，通过公开和受限两种访问方式促进算法开发，相比单个数据集能产生更具泛化性的算法性能。

Abstract: Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unified and accessible data resource for T1D algorithm development. Multiple publicly available T1D datasets were consolidated into a unified resource, termed the MetaboNet dataset. Inclusion required the availability of both continuous glucose monitoring (CGM) data and corresponding insulin pump dosing records. Additionally, auxiliary information such as reported carbohydrate intake and physical activity was retained when present. The MetaboNet dataset comprises 3135 subjects and 1228 patient-years of overlapping CGM and insulin data, making it substantially larger than existing standalone benchmark datasets. The resource is distributed as a fully public subset available for immediate download at https://metabo-net.org/ , and with a Data Use Agreement (DUA)-restricted subset accessible through their respective application processes. For the datasets in the latter subset, processing pipelines are provided to automatically convert the data into the standardized MetaboNet format. A consolidated public dataset for T1D research is presented, and the access pathways for both its unrestricted and DUA-governed components are described. The resulting dataset covers a broad range of glycemic profiles and demographics and thus can yield more generalizable algorithmic performance than individual datasets.

</details>


### [33] [Mugi: Value Level Parallelism For Efficient LLMs](https://arxiv.org/abs/2601.10823)
*Daniel Price,Prabhu Vellaisamy,John Shen,Di Wu*

Main category: cs.LG

TL;DR: Mugi架构通过值级并行性优化LLM，提升非线性操作和GEMM性能，显著提高吞吐量、能效并降低碳排放


<details>
  <summary>Details</summary>
Motivation: 现有值级并行性主要针对大批次、低精度GEMM，但LLM包含更复杂的非线性操作和小批次GEMM，需要探索VLP如何全面优化LLM

Method: 1) 将VLP推广到非线性近似，采用值中心方法对重要值分配更高精度；2) 优化小批次GEMM的非对称输入；3) 设计Mugi架构整合创新并支持完整LLM工作负载

Result: Mugi显著提升吞吐量和能效：softmax操作达45×和668×，LLM达2.07×和3.11×，降低运营碳排放1.45×和隐含碳排放1.48×

Conclusion: Mugi架构通过VLP创新有效优化LLM性能、能效和可持续性，为LLM硬件加速提供新方向

Abstract: Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM. In this paper, we explore how VLP benefits LLMs. First, we generalize VLP for nonlinear approximations, outperforming existing nonlinear approximations in end-to-end LLM accuracy, performance, and efficiency. Our VLP approximation follows a value-centric approach, where important values are assigned with greater accuracy. Second, we optimize VLP for small-batch GEMMs with asymmetric inputs efficiently, which leverages timely LLM optimizations, including weight-only quantization, key-value (KV) cache quantization, and group query attention. Finally, we design a new VLP architecture, Mugi, to encapsulate the innovations above and support full LLM workloads, while providing better performance, efficiency and sustainability. Our experimental results show that Mugi can offer significant improvements on throughput and energy efficiency, up to $45\times$ and $668\times$ for nonlinear softmax operations, and $2.07\times$ and $3.11\times$ for LLMs, and also decrease operational carbon for LLM operation by $1.45\times$ and embodied carbon by $1.48\times$.

</details>


### [34] [AI-Guided Human-In-the-Loop Inverse Design of High Performance Engineering Structures](https://arxiv.org/abs/2601.10859)
*Dat Quoc Ha,Md Ferdous Alam,Markus J. Buehler,Faez Ahmed,Josephine V. Carstensen*

Main category: cs.LG

TL;DR: 提出AI协同拓扑优化方法，通过机器学习预测用户偏好的修改区域，减少迭代次数，提高设计效率


<details>
  <summary>Details</summary>
Motivation: 传统拓扑优化计算时间长且黑箱特性阻碍用户交互，现有的人机协同方法依赖耗时的迭代区域选择

Method: 使用U-Net架构的图像分割模型，在合成数据集上训练预测用户偏好区域（最长拓扑构件或最复杂结构连接）

Result: 模型成功预测合理修改区域，AI推荐可提高可制造性或线性屈曲载荷39%，总设计时间仅增加15秒

Conclusion: AI协同的人机循环拓扑优化方法能有效减少迭代次数，提高设计效率，并在非标准问题上展现泛化能力

Abstract: Inverse design tools such as Topology Optimization (TO) can achieve new levels of improvement for high-performance engineered structures. However, widespread use is hindered by high computational times and a black-box nature that inhibits user interaction. Human-in-the-loop TO approaches are emerging that integrate human intuition into the design generation process. However, these rely on the time-consuming bottleneck of iterative region selection for design modifications. To reduce the number of iterative trials, this contribution presents an AI co-pilot that uses machine learning to predict the user's preferred regions. The prediction model is configured as an image segmentation task with a U-Net architecture. It is trained on synthetic datasets where human preferences either identify the longest topological member or the most complex structural connection. The model successfully predicts plausible regions for modification and presents them to the user as AI recommendations. The human preference model demonstrates generalization across diverse and non-standard TO problems and exhibits emergent behavior outside the single-region selection training data. Demonstration examples show that the new human-in-the-loop TO approach that integrates the AI co-pilot can improve manufacturability or improve the linear buckling load by 39% while only increasing the total design time by 15 sec compared to conventional simplistic TO.

</details>


### [35] [Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting](https://arxiv.org/abs/2601.10863)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.LG

TL;DR: 提出新的预测评分标准（forecast AC score），同时考虑多步预测的准确性和时间一致性，并在季节性ARIMA模型上验证其有效性


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法只优化准确性，忽略了时间一致性（即模型在不同预测起点对同一未来事件的预测一致性）。需要一种能同时评估多步预测准确性和稳定性的质量度量方法

Method: 提出预测准确性与一致性评分（forecast AC score），该评分允许用户指定权重来平衡准确性和一致性要求。将该评分实现为可微分的目标函数，用于训练季节性ARIMA模型

Result: 在M4 Hourly基准数据集上评估，相比传统最大似然估计有显著改进。AC优化模型在保持可比或改进的点预测准确性的同时，对相同目标时间戳的预测波动性降低了75%

Conclusion: forecast AC score能有效平衡预测准确性和时间一致性，为概率性多步预测提供了更全面的质量评估框架，并在实践中显示出显著优势

Abstract: Traditional time series forecasting methods optimize for accuracy alone. This objective neglects temporal consistency, in other words, how consistently a model predicts the same future event as the forecast origin changes. We introduce the forecast accuracy and coherence score (forecast AC score for short) for measuring the quality of probabilistic multi-horizon forecasts in a way that accounts for both multi-horizon accuracy and stability. Our score additionally provides for user-specified weights to balance accuracy and consistency requirements. As an example application, we implement the score as a differentiable objective function for training seasonal ARIMA models and evaluate it on the M4 Hourly benchmark dataset. Results demonstrate substantial improvements over traditional maximum likelihood estimation. Our AC-optimized models achieve a 75\% reduction in forecast volatility for the same target timestamps while maintaining comparable or improved point forecast accuracy.

</details>


### [36] [Unit-Consistent (UC) Adjoint for GSD and Backprop in Deep Learning Applications](https://arxiv.org/abs/2601.10873)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: 提出一种基于单位一致性伴随算子的优化方法，确保正齐次神经网络在节点级对角重缩放下的不变性


<details>
  <summary>Details</summary>
Motivation: 由线性映射和正齐次非线性（如ReLU）构成的深度神经网络具有基本的规范对称性：网络函数对节点级对角重缩放不变。但标准梯度下降对该对称性不是等变的，导致优化轨迹严重依赖于任意参数化。

Method: 在反向伴随/优化几何层面制定不变性要求，用单位一致性（UC）伴随替换欧几里得转置，推导出UC规范一致的梯度下降和反向传播。

Result: 提出了一种简单、算子级的统一方法，可一致应用于网络组件和优化器状态，确保优化过程对规范对称性的不变性。

Conclusion: 通过单位一致性伴随算子实现了正齐次神经网络优化中的规范不变性，为这类网络的优化提供了更稳定、不依赖于参数化的方法。

Abstract: Deep neural networks constructed from linear maps and positively homogeneous nonlinearities (e.g., ReLU) possess a fundamental gauge symmetry: the network function is invariant to node-wise diagonal rescalings. However, standard gradient descent is not equivariant to this symmetry, causing optimization trajectories to depend heavily on arbitrary parameterizations. Prior work has proposed rescaling-invariant optimization schemes for positively homogeneous networks (e.g., path-based or path-space updates). Our contribution is complementary: we formulate the invariance requirement at the level of the backward adjoint/optimization geometry, which provides a simple, operator-level recipe that can be applied uniformly across network components and optimizer state. By replacing the Euclidean transpose with a Unit-Consistent (UC) adjoint, we derive UC gauge-consistent steepest descent and backprogation.

</details>


### [37] [Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning](https://arxiv.org/abs/2601.10905)
*Rajat Ghosh,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出Action Shapley作为训练数据选择的公平度量，并设计随机动态算法以降低计算复杂度，在数据受限的真实场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，世界模型的质量对系统效能和可解释性至关重要，而模型质量又高度依赖训练数据。现有方法缺乏对训练数据选择的系统化、无偏度量。

Method: 引入Action Shapley作为训练数据选择的公平度量，提出随机动态算法来降低传统Shapley值计算的指数复杂度。

Result: 算法在五个数据受限的真实案例中验证，计算效率比传统指数时间计算提升超过80%。基于Action Shapley的训练数据选择策略持续优于临时选择方法。

Conclusion: Action Shapley为世界模型的训练数据选择提供了有效的公平度量，其高效算法在实际应用中具有显著优势。

Abstract: Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.

</details>


### [38] [Realistic Curriculum Reinforcement Learning for Autonomous and Sustainable Marine Vessel Navigation](https://arxiv.org/abs/2601.10911)
*Zhang Xiaocai,Xiao Zhe,Liang Maohan,Liu Tao,Li Haijiang,Zhang Wenbin*

Main category: cs.LG

TL;DR: 提出一个结合课程强化学习(CRL)框架、数据驱动海洋模拟环境和机器学习燃料消耗预测模块的可持续船舶导航系统，在印度洋海域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统船舶导航依赖人工经验，缺乏自主性和排放意识，容易因人为错误影响安全性。海事运输的可持续性（包括温室气体排放和航行安全）日益重要。

Method: 1) 构建基于真实船舶移动数据的海洋模拟环境，使用扩散模型模拟动态海事条件；2) 基于历史操作数据和机器学习回归预测燃料消耗；3) 将周围环境表示为图像输入以捕捉空间复杂性；4) 设计轻量级策略型CRL代理，包含考虑安全、排放、及时性和目标完成的综合奖励机制。

Result: 在印度洋海域验证了该框架的有效性，能够实现可持续和安全的船舶导航，在连续动作空间中确保稳定高效的学习。

Conclusion: 提出的CRL框架结合数据驱动模拟和燃料预测模块，能够有效处理复杂任务，实现可持续、安全的自主船舶导航，解决了传统导航方法的局限性。

Abstract: Sustainability is becoming increasingly critical in the maritime transport, encompassing both environmental and social impacts, such as Greenhouse Gas (GHG) emissions and navigational safety. Traditional vessel navigation heavily relies on human experience, often lacking autonomy and emission awareness, and is prone to human errors that may compromise safety. In this paper, we propose a Curriculum Reinforcement Learning (CRL) framework integrated with a realistic, data-driven marine simulation environment and a machine learning-based fuel consumption prediction module. The simulation environment is constructed using real-world vessel movement data and enhanced with a Diffusion Model to simulate dynamic maritime conditions. Vessel fuel consumption is estimated using historical operational data and learning-based regression. The surrounding environment is represented as image-based inputs to capture spatial complexity. We design a lightweight, policy-based CRL agent with a comprehensive reward mechanism that considers safety, emissions, timeliness, and goal completion. This framework effectively handles complex tasks progressively while ensuring stable and efficient learning in continuous action spaces. We validate the proposed approach in a sea area of the Indian Ocean, demonstrating its efficacy in enabling sustainable and safe vessel navigation.

</details>


### [39] [FAConvLSTM: Factorized-Attention ConvLSTM for Efficient Feature Extraction in Multivariate Climate Data](https://arxiv.org/abs/2601.10914)
*Francis Ndikum Nji,Jianwu Wang*

Main category: cs.LG

TL;DR: FAConvLSTM：一种用于地球观测数据的因子化注意力ConvLSTM层，通过轻量级瓶颈和轴向空间注意力提高效率、空间表达能力和物理可解释性，替代传统ConvLSTM2D。


<details>
  <summary>Details</summary>
Motivation: 传统ConvLSTM2D在处理高分辨率多变量地球观测数据时面临挑战：密集卷积门控计算成本高，局部感受野限制长程空间结构和解耦气候动力学的建模。

Method: 提出FAConvLSTM层，使用[1×1]瓶颈和共享深度空间混合因子化门计算；多尺度扩张深度分支和挤压激励重校准；轻量级轴向空间注意力机制；专用子空间头和时间自注意力。

Result: 在多变量时空气候数据实验中，FAConvLSTM比标准ConvLSTM产生更稳定、可解释和鲁棒的潜在表示，同时显著降低计算开销。

Conclusion: FAConvLSTM作为ConvLSTM2D的直接替代方案，在效率、空间表达能力和物理可解释性方面同时改进，能更好地建模地球观测数据中的复杂时空模式。

Abstract: Learning physically meaningful spatiotemporal representations from high-resolution multivariate Earth observation data is challenging due to strong local dynamics, long-range teleconnections, multi-scale interactions, and nonstationarity. While ConvLSTM2D is a commonly used baseline, its dense convolutional gating incurs high computational cost and its strictly local receptive fields limit the modeling of long-range spatial structure and disentangled climate dynamics. To address these limitations, we propose FAConvLSTM, a Factorized-Attention ConvLSTM layer designed as a drop-in replacement for ConvLSTM2D that simultaneously improves efficiency, spatial expressiveness, and physical interpretability. FAConvLSTM factorizes recurrent gate computations using lightweight [1 times 1] bottlenecks and shared depthwise spatial mixing, substantially reducing channel complexity while preserving recurrent dynamics. Multi-scale dilated depthwise branches and squeeze-and-excitation recalibration enable efficient modeling of interacting physical processes across spatial scales, while peephole connections enhance temporal precision. To capture teleconnection-scale dependencies without incurring global attention cost, FAConvLSTM incorporates a lightweight axial spatial attention mechanism applied sparsely in time. A dedicated subspace head further produces compact per timestep embeddings refined through temporal self-attention with fixed seasonal positional encoding. Experiments on multivariate spatiotemporal climate data shows superiority demonstrating that FAConvLSTM yields more stable, interpretable, and robust latent representations than standard ConvLSTM, while significantly reducing computational overhead.

</details>


### [40] [HOSL: Hybrid-Order Split Learning for Memory-Constrained Edge Training](https://arxiv.org/abs/2601.10940)
*Aakriti,Zhe Li,Dandan Liang,Chao Huang,Rui Li,Haibo Yang*

Main category: cs.LG

TL;DR: HOSL提出了一种混合阶分割学习框架，通过客户端使用零阶优化减少内存开销，服务器端使用一阶优化保证性能，在边缘设备上实现内存高效的大语言模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有分割学习系统主要依赖一阶优化，需要客户端存储激活等中间量，导致内存开销大，抵消了模型分割的优势。而零阶优化虽然减少内存使用，但收敛慢且性能下降。需要解决内存效率与优化效果之间的权衡问题。

Method: HOSL采用混合阶优化策略：客户端使用内存高效的零阶梯度估计，消除反向传播和激活存储；服务器端使用一阶优化确保快速收敛和竞争力性能。理论上证明收敛率取决于客户端模型维度而非完整模型维度。

Result: 在OPT模型（125M和1.3B参数）的6个任务上，HOSL相比一阶方法减少客户端GPU内存达3.7倍，同时准确率仅相差0.20%-4.23%。相比零阶基线，性能提升达15.55%，验证了混合策略的有效性。

Conclusion: HOSL成功解决了分割学习中内存效率与优化效果之间的权衡问题，通过混合阶优化策略实现了边缘设备上内存高效的大语言模型训练，为资源受限设备上的协作学习提供了有效解决方案。

Abstract: Split learning (SL) enables collaborative training of large language models (LLMs) between resource-constrained edge devices and compute-rich servers by partitioning model computation across the network boundary. However, existing SL systems predominantly rely on first-order (FO) optimization, which requires clients to store intermediate quantities such as activations for backpropagation. This results in substantial memory overhead, largely negating benefits of model partitioning. In contrast, zeroth-order (ZO) optimization eliminates backpropagation and significantly reduces memory usage, but often suffers from slow convergence and degraded performance. In this work, we propose HOSL, a novel Hybrid-Order Split Learning framework that addresses this fundamental trade-off between memory efficiency and optimization effectiveness by strategically integrating ZO optimization on the client side with FO optimization on the server side. By employing memory-efficient ZO gradient estimation at the client, HOSL eliminates backpropagation and activation storage, reducing client memory consumption. Meanwhile, server-side FO optimization ensures fast convergence and competitive performance. Theoretically, we show that HOSL achieves a $\mathcal{O}(\sqrt{d_c/TQ})$ rate, which depends on client-side model dimension $d_c$ rather than the full model dimension $d$, demonstrating that convergence improves as more computation is offloaded to the server. Extensive experiments on OPT models (125M and 1.3B parameters) across 6 tasks demonstrate that HOSL reduces client GPU memory by up to 3.7$\times$ compared to the FO method while achieving accuracy within 0.20%-4.23% of this baseline. Furthermore, HOSL outperforms the ZO baseline by up to 15.55%, validating the effectiveness of our hybrid strategy for memory-efficient training on edge devices.

</details>


### [41] [Multivariate LSTM-Based Forecasting for Renewable Energy: Enhancing Climate Change Mitigation](https://arxiv.org/abs/2601.10961)
*Farshid Kamrani,Kristen Schell*

Main category: cs.LG

TL;DR: 提出基于多元LSTM网络的可再生能源发电预测模型，利用本地及邻近区域历史数据提升预测精度，降低碳排放并提高供电可靠性。


<details>
  <summary>Details</summary>
Motivation: 可再生能源并网带来机遇的同时，其固有的波动性也给电力系统带来挑战。准确的发电预测对维持电力系统可靠性、稳定性和经济性至关重要。传统方法（如确定性方法和随机规划）常依赖K-means等聚类技术生成代表性场景，但可能无法充分捕捉可再生能源数据的复杂时间依赖性和非线性模式。

Method: 提出基于多元长短期记忆（LSTM）网络的预测模型，利用可再生能源的实际历史数据进行预测。该模型能够有效捕捉长期依赖性和不同可再生能源之间的相互作用，同时利用本地和邻近区域的历史数据来提升预测准确性。

Result: 案例研究表明，所提出的预测方法能够降低二氧化碳排放，并提供更可靠的电力负荷供应。

Conclusion: 多元LSTM网络能够有效预测可再生能源发电，通过捕捉复杂的时间依赖性和非线性模式，为电力系统运行提供更准确、可靠且环保的预测结果。

Abstract: The increasing integration of renewable energy sources (RESs) into modern power systems presents significant opportunities but also notable challenges, primarily due to the inherent variability of RES generation. Accurate forecasting of RES generation is crucial for maintaining the reliability, stability, and economic efficiency of power system operations. Traditional approaches, such as deterministic methods and stochastic programming, frequently depend on representative scenarios generated through clustering techniques like K-means. However, these methods may fail to fully capture the complex temporal dependencies and non-linear patterns within RES data. This paper introduces a multivariate Long Short-Term Memory (LSTM)-based network designed to forecast RESs generation using their real-world historical data. The proposed model effectively captures long-term dependencies and interactions between different RESs, utilizing historical data from both local and neighboring areas to enhance predictive accuracy. In the case study, we showed that the proposed forecasting approach results in lower CO2 emissions, and a more reliable supply of electric loads.

</details>


### [42] [Transient learning dynamics drive escape from sharp valleys in Stochastic Gradient Descent](https://arxiv.org/abs/2601.10962)
*Ning Yang,Yikuan Zhang,Qi Ouyang,Chao Tang,Yuhai Tu*

Main category: cs.LG

TL;DR: SGD噪声通过非平衡机制重塑损失景观，形成偏好平坦解的等效势能，并通过瞬态冻结机制最终将动态限制在单一盆地中。


<details>
  <summary>Details</summary>
Motivation: 尽管SGD是深度学习的核心，但其偏好平坦、更可泛化解的动态起源仍不清楚。本文旨在揭示SGD选择解的机制，理解学习动态、损失景观几何与泛化之间的物理联系。

Method: 通过分析SGD学习动态，结合数值实验观察轨迹行为，并使用可处理的物理模型展示SGD噪声如何重塑损失景观为偏好平坦解的等效势能。

Result: 发现SGD存在瞬态探索阶段，轨迹反复逃离尖锐谷并转向平坦区域；SGD噪声将景观重塑为偏好平坦解的等效势能；存在瞬态冻结机制，随着训练进行，能量壁垒增长抑制谷间转移，最终将动态限制在单一盆地中；增加SGD噪声强度延迟冻结，增强向平坦最小值的收敛。

Conclusion: 研究提供了一个统一的物理框架，连接学习动态、损失景观几何和泛化，并为设计更有效的优化算法提出了原则。SGD通过非平衡机制偏好平坦解，瞬态冻结机制最终确定解的选择。

Abstract: Stochastic gradient descent (SGD) is central to deep learning, yet the dynamical origin of its preference for flatter, more generalizable solutions remains unclear. Here, by analyzing SGD learning dynamics, we identify a nonequilibrium mechanism governing solution selection. Numerical experiments reveal a transient exploratory phase in which SGD trajectories repeatedly escape sharp valleys and transition toward flatter regions of the loss landscape. By using a tractable physical model, we show that the SGD noise reshapes the landscape into an effective potential that favors flat solutions. Crucially, we uncover a transient freezing mechanism: as training proceeds, growing energy barriers suppress inter-valley transitions and ultimately trap the dynamics within a single basin. Increasing the SGD noise strength delays this freezing, which enhances convergence to flatter minima. Together, these results provide a unified physical framework linking learning dynamics, loss-landscape geometry, and generalization, and suggest principles for the design of more effective optimization algorithms.

</details>


### [43] [Reasoning Distillation for Lightweight Automated Program Repair](https://arxiv.org/abs/2601.10987)
*Aanand Balasubramanian,Sashank Silwal*

Main category: cs.LG

TL;DR: 轻量级符号推理监督能提升紧凑型自动程序修复模型中的修复类型分类性能，通过推理蒸馏方法让大模型提供结构化推理标签，在不增加模型大小的情况下改善性能，特别是在低频错误类别上。


<details>
  <summary>Details</summary>
Motivation: 小型代码模型适合资源受限环境，但通常只产生单一预测，不清楚它们是否真正学习程序结构还是依赖浅层相关性。需要提升轻量级程序修复模型的可解释性和鲁棒性。

Method: 提出推理蒸馏方法：大型教师模型提供结构化符号推理标签和修复类型标签，这些标签捕获错误的高层因果属性而不依赖自由形式解释。在IntroClass基准上训练基于CodeT5的学生模型，比较仅标签和推理蒸馏两种设置。

Result: 推理监督一致提升宏平均性能，特别是在较少出现的错误类别上，且不增加模型大小或复杂度。正确推理轨迹与正确预测强相关，但不完全决定预测结果。

Conclusion: 符号推理蒸馏是提升轻量级程序修复模型可解释性和鲁棒性的实用方法，能够帮助小型模型学习更有意义的程序结构而非浅层相关性。

Abstract: We study whether lightweight symbolic reasoning supervision can improve fix type classification in compact automated program repair models. Small code models are attractive for resource-constrained settings, but they typically produce only a single prediction, making it unclear whether they learn meaningful program structure or rely on shallow correlations. We propose a reasoning distillation approach in which a large teacher model provides structured symbolic reasoning tags alongside fix-type labels. These tags capture high-level causal properties of bugs without relying on free-form explanations. We train a CodeT5-based student model under label-only and reasoning-distilled settings on the IntroClass benchmark. Reasoning supervision consistently improves macro averaged performance, particularly on less frequent bug categories, without increasing model size or complexity. We further analyze the relationship between reasoning accuracy and fix-type prediction, showing that correct reasoning traces strongly correlate with correct predictions, while not fully determining them. Our results suggest that symbolic reasoning distillation is a practical way to improve interpretability and robustness in lightweight program repair models.

</details>


### [44] [Constant Metric Scaling in Riemannian Computation](https://arxiv.org/abs/2601.10992)
*Kisung You*

Main category: cs.LG

TL;DR: 该论文澄清了黎曼度量常数缩放对几何量和优化算法的影响，区分了哪些量会变化（如距离、体积）和哪些几何结构保持不变（如联络、测地线）。


<details>
  <summary>Details</summary>
Motivation: 在计算应用中，黎曼度量的常数缩放经常被引入，但这一操作的影响在实践中并不总是清晰，可能与曲率变化、流形结构变化或坐标表示变化混淆。作者旨在澄清这一基本操作的实际意义。

Method: 通过理论分析，系统地区分在常数度量缩放下变化的量（范数、距离、体积元、梯度大小）和保持不变的几何对象（Levi-Civita联络、测地线、指数和对数映射、平行移动）。

Result: 明确展示了常数度量缩放如何影响各种几何量，同时指出核心几何结构保持不变。在黎曼优化中，这种缩放通常可以解释为步长的全局重新缩放，而不是底层几何的修改。

Conclusion: 该说明性文章澄清了如何在黎曼计算中引入全局度量尺度参数而不改变这些方法所依赖的几何结构，为实践者提供了清晰的理论指导。

Abstract: Constant rescaling of a Riemannian metric appears in many computational settings, often through a global scale parameter that is introduced either explicitly or implicitly. Although this operation is elementary, its consequences are not always made clear in practice and may be confused with changes in curvature, manifold structure, or coordinate representation. In this note we provide a short, self-contained account of constant metric scaling on arbitrary Riemannian manifolds. We distinguish between quantities that change under such a scaling, including norms, distances, volume elements, and gradient magnitudes, and geometric objects that remain invariant, such as the Levi--Civita connection, geodesics, exponential and logarithmic maps, and parallel transport. We also discuss implications for Riemannian optimization, where constant metric scaling can often be interpreted as a global rescaling of step sizes rather than a modification of the underlying geometry. The goal of this note is purely expository and is intended to clarify how a global metric scale parameter can be introduced in Riemannian computation without altering the geometric structures on which these methods rely.

</details>


### [45] [Backdoor Attacks on Multi-modal Contrastive Learning](https://arxiv.org/abs/2601.11006)
*Simi D Kuniyilh,Rita Machacy*

Main category: cs.LG

TL;DR: 对比学习中的后门攻击综述：分析威胁模型、攻击方法、目标领域和防御措施，强调对比学习特有漏洞及其对工业分布式系统安全部署的影响


<details>
  <summary>Details</summary>
Motivation: 对比学习已成为跨领域自监督表示学习的主流方法，但近期研究表明其易受后门和数据投毒攻击。攻击者可通过操纵预训练数据或模型更新来植入恶意行为，这对工业分布式环境中的安全部署构成威胁。

Method: 本文采用系统性综述方法，对对比学习中的后门攻击进行全面比较分析。研究内容包括：威胁模型分析、攻击方法分类、目标领域识别、现有防御措施评估，并总结该领域最新进展。

Result: 研究发现对比学习存在特定漏洞，攻击者可通过数据投毒和模型更新操纵等方式植入后门。论文系统梳理了不同攻击策略和防御机制，揭示了当前安全挑战。

Conclusion: 对比学习中的后门攻击研究具有重要意义，特别是在工业分布式环境中。未来需要进一步研究更有效的防御机制，以应对不断演变的攻击策略，确保对比学习模型的安全部署。

Abstract: Contrastive learning has become a leading self- supervised approach to representation learning across domains, including vision, multimodal settings, graphs, and federated learning. However, recent studies have shown that contrastive learning is susceptible to backdoor and data poisoning attacks. In these attacks, adversaries can manipulate pretraining data or model updates to insert hidden malicious behavior. This paper offers a thorough and comparative review of backdoor attacks in contrastive learning. It analyzes threat models, attack methods, target domains, and available defenses. We summarize recent advancements in this area, underline the specific vulnerabilities inherent to contrastive learning, and discuss the challenges and future research directions. Our findings have significant implications for the secure deployment of systems in industrial and distributed environments.

</details>


### [46] [Combating Spurious Correlations in Graph Interpretability via Self-Reflection](https://arxiv.org/abs/2601.11021)
*Kecheng Cai,Chenyang Xu,Chao Peng*

Main category: cs.LG

TL;DR: 本文提出一个自反思框架，通过将现有图解释方法的输出反馈给模型进行第二轮评估，提升在Spurious-Motif基准上的可解释性，并基于此提出微调训练方法。


<details>
  <summary>Details</summary>
Motivation: Spurious-Motif基准包含虚假相关性，现有图可解释方法在该基准上表现显著较差，需要提升模型在强虚假相关数据集上的可解释能力。

Method: 提出自反思框架，将现有图可解释方法生成的重要性分数反馈给原始方法进行第二轮评估，模仿大语言模型的自反思提示机制，并基于反馈机制提出微调训练方法。

Result: 自反思技术能有效提升在具有强虚假相关的Spurious-Motif数据集上的可解释性，从图表示学习角度分析了改进原因。

Conclusion: 自反思框架可集成到现有图可解释学习方法中，通过迭代评估提升模型区分真实相关结构和误导模式的能力，为解决虚假相关挑战提供了新思路。

Abstract: Interpretable graph learning has recently emerged as a popular research topic in machine learning. The goal is to identify the important nodes and edges of an input graph that are crucial for performing a specific graph reasoning task. A number of studies have been conducted in this area, and various benchmark datasets have been proposed to facilitate evaluation. Among them, one of the most challenging is the Spurious-Motif benchmark, introduced at ICLR 2022. The datasets in this synthetic benchmark are deliberately designed to include spurious correlations, making it particularly difficult for models to distinguish truly relevant structures from misleading patterns. As a result, existing methods exhibit significantly worse performance on this benchmark compared to others.
  In this paper, we focus on improving interpretability on the challenging Spurious-Motif datasets. We demonstrate that the self-reflection technique, commonly used in large language models to tackle complex tasks, can also be effectively adapted to enhance interpretability in datasets with strong spurious correlations. Specifically, we propose a self-reflection framework that can be integrated with existing interpretable graph learning methods. When such a method produces importance scores for each node and edge, our framework feeds these predictions back into the original method to perform a second round of evaluation. This iterative process mirrors how large language models employ self-reflective prompting to reassess their previous outputs. We further analyze the reasons behind this improvement from the perspective of graph representation learning, which motivates us to propose a fine-tuning training method based on this feedback mechanism.

</details>


### [47] [Matching High-Dimensional Geometric Quantiles for Test-Time Adaptation of Transformers and Convolutional Networks Alike](https://arxiv.org/abs/2601.11022)
*Sravan Danda,Aditya Challa,Shlok Mehendale,Snehanshu Saha*

Main category: cs.LG

TL;DR: 提出一种架构无关的测试时自适应方法，通过添加适配器网络预处理输入图像，使用分位数损失训练，匹配高维几何分位数来纠正分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法大多依赖修改分类器权重，与架构高度相关，难以扩展到通用架构。需要一种架构无关的方法来处理测试数据分布偏移问题。

Method: 提出架构无关的测试时自适应方法：1) 添加适配器网络预处理输入图像；2) 使用提出的分位数损失训练适配器；3) 通过匹配高维几何分位数来纠正分布偏移。

Result: 在CIFAR10-C、CIFAR100-C和TinyImageNet-C数据集上验证了方法有效性，训练了经典卷积网络和Transformer网络，证明了方法的架构无关性。

Conclusion: 提出的架构无关测试时自适应方法通过分位数损失训练适配器网络，能够有效处理测试数据分布偏移，适用于多种神经网络架构。

Abstract: Test-time adaptation (TTA) refers to adapting a classifier for the test data when the probability distribution of the test data slightly differs from that of the training data of the model. To the best of our knowledge, most of the existing TTA approaches modify the weights of the classifier relying heavily on the architecture. It is unclear as to how these approaches are extendable to generic architectures. In this article, we propose an architecture-agnostic approach to TTA by adding an adapter network pre-processing the input images suitable to the classifier. This adapter is trained using the proposed quantile loss. Unlike existing approaches, we correct for the distribution shift by matching high-dimensional geometric quantiles. We prove theoretically that under suitable conditions minimizing quantile loss can learn the optimal adapter. We validate our approach on CIFAR10-C, CIFAR100-C and TinyImageNet-C by training both classic convolutional and transformer networks on CIFAR10, CIFAR100 and TinyImageNet datasets.

</details>


### [48] [AVP-Pro: An Adaptive Multi-Modal Fusion and Contrastive Learning Approach for Comprehensive Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2601.11028)
*Xinru Wen,Weizhong Lin,zi liu,Xuan Xiao*

Main category: cs.LG

TL;DR: AVP-Pro：基于自适应特征融合和对比学习的双阶段抗病毒肽预测框架，在通用识别和功能亚型预测中均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有抗病毒肽识别方法在捕捉复杂序列依赖性和区分高相似度样本方面存在局限，需要更强大的预测工具来支持抗病毒药物开发

Method: 提出双阶段预测框架：1）构建包含10种描述符的全景特征空间，采用分层融合架构整合CNN提取的局部基序和BiLSTM捕获的全局依赖；2）使用基于BLOSUM62增强的OHEM对比学习策略锐化决策边界

Result: 通用AVP识别阶段准确率达0.9531，MCC为0.9064，优于现有SOTA方法；功能亚型预测阶段在小样本条件下实现了6个病毒家族和8种特定病毒的准确分类

Conclusion: AVP-Pro为抗病毒药物高通量筛选提供了强大且可解释的新工具，并开发了用户友好的Web界面

Abstract: The accurate identification of antiviral peptides (AVPs) is crucial for novel drug development. However, existing methods still have limitations in capturing complex sequence dependencies and distinguishing confusing samples with high similarity. To address these challenges, we propose AVP-Pro, a novel two-stage predictive framework that integrates adaptive feature fusion and contrastive learning. To comprehensively capture the physicochemical properties and deep-seated patterns of peptide sequences, we constructed a panoramic feature space encompassing 10 distinct descriptors and designed a hierarchical fusion architecture. This architecture integrates self-attention and adaptive gating mechanisms to dynamically modulate the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Targeting the blurred decision boundary caused by the high similarity between positive and negative sample sequences, we adopted an Online Hard Example Mining (OHEM)-driven contrastive learning strategy enhanced by BLOSUM62. This approach significantly sharpened the model's discriminative power. Model evaluation results show that in the first stage of general AVP identification, the model achieved an accuracy of 0.9531 and an MCC of 0.9064, outperforming existing state-of-the-art (SOTA) methods. In the second stage of functional subtype prediction, combined with a transfer learning strategy, the model realized accurate classification of 6 viral families and 8 specific viruses under small-sample conditions. AVP-Pro provides a powerful and interpretable new tool for the high-throughput screening of antiviral drugs. To further enhance accessibility for users, we have developed a user-friendly web interface, which is available at https://wwwy1031-avp-pro.hf.space.

</details>


### [49] [Self-Augmented Mixture-of-Experts for QoS Prediction](https://arxiv.org/abs/2601.11036)
*Kecheng Cai,Chao Peng,Chenyang Xu,Xia Chen*

Main category: cs.LG

TL;DR: 提出一种基于自增强混合专家模型的QoS预测方法，通过迭代预测和专家间协作解决用户-服务交互稀疏性问题


<details>
  <summary>Details</summary>
Motivation: QoS预测是服务计算和个性化推荐中的核心问题，但用户-服务交互数据通常非常稀疏，只有少量反馈值被观察到，这给准确预测带来了挑战

Method: 提出自增强策略，利用模型自身预测进行迭代优化：部分掩码预测值并反馈给模型重新预测。基于此设计自增强混合专家模型，多个专家网络迭代协作估计QoS值，通过专家间通信实现预测精化

Result: 在基准数据集上的实验表明，该方法优于现有基线方法，取得了有竞争力的结果

Conclusion: 自增强混合专家模型通过迭代优化和专家协作有效解决了QoS预测中的稀疏性问题，为服务计算和推荐系统提供了有效的解决方案

Abstract: Quality of Service (QoS) prediction is one of the most fundamental problems in service computing and personalized recommendation. In the problem, there is a set of users and services, each associated with a set of descriptive features. Interactions between users and services produce feedback values, typically represented as numerical QoS metrics such as response time or availability. Given the observed feedback for a subset of user-service pairs, the goal is to predict the QoS values for the remaining pairs.
  A key challenge in QoS prediction is the inherent sparsity of user-service interactions, as only a small subset of feedback values is typically observed. To address this, we propose a self-augmented strategy that leverages a model's own predictions for iterative refinement. In particular, we partially mask the predicted values and feed them back into the model to predict again. Building on this idea, we design a self-augmented mixture-of-experts model, where multiple expert networks iteratively and collaboratively estimate QoS values. We find that the iterative augmentation process naturally aligns with the MoE architecture by enabling inter-expert communication: in the second round, each expert receives the first-round predictions and refines its output accordingly. Experiments on benchmark datasets show that our method outperforms existing baselines and achieves competitive results.

</details>


### [50] [OpFML: Pipeline for ML-based Operational Forecasting](https://arxiv.org/abs/2601.11046)
*Shahbaz Alvi,Giusy Fedele,Gabriele Accarino,Italo Epicoco,Ilenia Manco,Pasquale Schiano*

Main category: cs.LG

TL;DR: OpFML是一个可配置、可适应的机器学习管道，用于周期性预测，特别应用于每日火灾危险指数预测


<details>
  <summary>Details</summary>
Motivation: 机器学习在气候和地球科学中应用广泛，但传统火灾风险评估方法往往高估风险，需要更有效的操作预测系统

Method: 开发了OpFML：一个可配置、可适应的机器学习管道，用于周期性预测，特别应用于每日火灾危险指数预测

Result: OpFML管道能够有效服务于机器学习模型的周期性预测，展示了在火灾危险指数预测中的应用能力

Conclusion: OpFML为机器学习在操作预测系统中提供了一个灵活、可配置的解决方案，特别适用于火灾危险评估等周期性预测任务

Abstract: Machine learning is finding its application in a multitude of areas in science and research, and Climate and Earth Sciences is no exception to this trend. Operational forecasting systems based on data-driven approaches and machine learning methods deploy models for periodic forecasting. Wildfire danger assessment using machine learning has garnered significant interest in the last decade, as conventional methods often overestimate the risk of wildfires. In this work, we present the code OpFML: Operational Forecasting with Machine Learning. OpFML is a configurable and adaptable pipeline that can be utilized to serve a machine learning model for periodic forecasting. We further demonstrate the capabilities of the pipeline through its application to daily Fire Danger Index forecasting and outline its various features.

</details>


### [51] [Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs](https://arxiv.org/abs/2601.11061)
*Lecheng Yan,Ruizhe Li,Guanhua Chen,Qing Li,Jiahui Geng,Wenxi Li,Vincent Wang,Chris Lee*

Main category: cs.LG

TL;DR: 研究发现RLVR训练中即使使用虚假奖励，模型性能仍能提升，这是因为模型形成了"锚点-适配器"电路，绕过推理直接检索记忆答案，导致答案困惑度下降但提示侧连贯性退化。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR能有效提升LLM推理能力，但近期证据显示即使使用虚假或不正确的奖励，模型也能获得显著性能提升。研究者希望探究这一现象背后的机制，理解模型如何绕过推理过程实现性能提升。

Method: 使用路径修补、Logit Lens、JSD分析和神经微分方程等技术，识别并定位了模型中的"锚点-适配器"电路。具体包括：1) 分析困惑度悖论现象；2) 定位功能锚点层(L18-20)和结构适配器层(L21+)；3) 通过缩放特定MLP键值实现双向因果操控。

Result: 发现了"困惑度悖论"：虚假RLVR导致答案标记困惑度下降但提示侧连贯性退化。定位到中间层(L18-20)的功能锚点触发记忆解决方案检索，后续层(L21+)的结构适配器调整表征以适应捷径信号。通过操控MLP键值可双向调节污染驱动的性能。

Conclusion: 研究揭示了RLVR训练中模型通过形成"锚点-适配器"电路绕过推理直接检索记忆的机制，为识别和缓解RLVR调优模型中的数据污染问题提供了机制性路线图。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a "Perplexity Paradox": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.

</details>


### [52] [Bridging Cognitive Neuroscience and Graph Intelligence: Hippocampus-Inspired Multi-View Hypergraph Learning for Web Finance Fraud](https://arxiv.org/abs/2601.11073)
*Rongkun Cui,Nana Zhang,Kun Zhu,Qi Zhang*

Main category: cs.LG

TL;DR: HIMVH：受海马体启发的多视图超图学习模型，用于网络金融欺诈检测，解决欺诈伪装和长尾分布问题，在6个数据集上平均提升AUC 6.42%、F1 9.74%、AP 39.14%。


<details>
  <summary>Details</summary>
Motivation: 在线金融服务面临欺诈伪装（恶意交易模仿良性行为）和长尾数据分布（罕见但关键的欺诈案例被掩盖）两大挑战，现有基于图神经网络的方法难以有效应对。

Method: 提出HIMVH模型：1) 受海马体场景冲突监测启发，设计跨视图不一致感知模块，捕捉多交易视图间的细微差异和行为异质性；2) 受CA1区匹配-不匹配新颖性检测机制启发，引入新颖性感知超图学习模块，测量特征与邻域期望的偏差并自适应重加权消息。

Result: 在6个基于网络的金融欺诈数据集上，HIMVH相比15个最先进模型平均提升AUC 6.42%、F1 9.74%、AP 39.14%。

Conclusion: HIMVH通过模仿海马体的神经机制，有效解决了金融欺诈检测中的伪装和长尾分布问题，显著提升了检测性能，为网络金融安全提供了新方案。

Abstract: Online financial services constitute an essential component of contemporary web ecosystems, yet their openness introduces substantial exposure to fraud that harms vulnerable users and weakens trust in digital finance. Such threats have become a significant web harm that erodes societal fairness and affects the well being of online communities. However, existing detection methods based on graph neural networks (GNNs) struggle with two persistent challenges: (1) fraud camouflage, where malicious transactions mimic benign behaviors to evade detection, and (2) long-tailed data distributions, which obscure rare but critical fraudulent cases. To fill these gaps, we propose HIMVH, a Hippocampus-Inspired Multi-View Hypergraph learning model for web finance fraud detection. Specifically, drawing inspiration from the scene conflict monitoring role of the hippocampus, we design a cross-view inconsistency perception module that captures subtle discrepancies and behavioral heterogeneity across multiple transaction views. This module enables the model to identify subtle cross-view conflicts for detecting online camouflaged fraudulent behaviors. Furthermore, inspired by the match-mismatch novelty detection mechanism of the CA1 region, we introduce a novelty-aware hypergraph learning module that measures feature deviations from neighborhood expectations and adaptively reweights messages, thereby enhancing sensitivity to online rare fraud patterns in the long-tailed settings. Extensive experiments on six web-based financial fraud datasets demonstrate that HIMVH achieves 6.42\% improvement in AUC, 9.74\% in F1 and 39.14\% in AP on average over 15 SOTA models.

</details>


### [53] [Soft Bayesian Context Tree Models for Real-Valued Time Series](https://arxiv.org/abs/2601.11079)
*Shota Saito,Yuta Nakahara,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: 提出Soft-BCT模型，用于实值时间序列分析，采用概率性上下文空间分割而非确定性分割，基于变分推断进行学习，在真实数据集上表现优于或等同于传统BCT。


<details>
  <summary>Details</summary>
Motivation: 传统BCT模型对实值时间序列使用硬分割（确定性分割）的上下文空间，这可能限制了模型的灵活性和表达能力。需要一种更灵活的模型来处理实值时间序列。

Method: 提出Soft-BCT模型，采用软分割（概率性分割）的上下文空间，基于变分推断设计学习算法。

Result: 在多个真实世界数据集上，Soft-BCT表现出与传统BCT几乎相同或更优的性能。

Conclusion: Soft-BCT通过引入软分割机制，为实值时间序列建模提供了更灵活有效的方法，在保持或提升性能的同时增强了模型的表达能力。

Abstract: This paper proposes the soft Bayesian context tree model (Soft-BCT), which is a novel BCT model for real-valued time series. The Soft-BCT considers soft (probabilistic) splits of the context space, instead of hard (deterministic) splits of the context space as in the previous BCT for real-valued time series. A learning algorithm of the Soft-BCT is proposed based on the variational inference. For some real-world datasets, the Soft-BCT demonstrates almost the same or superior performance to the previous BCT.

</details>


### [54] [Differentially Private Subspace Fine-Tuning for Large Language Models](https://arxiv.org/abs/2601.11113)
*Lele Zheng,Xiang Wang,Tao Zhang,Yang Cao,Ke Cheng,Yulong Shen*

Main category: cs.LG

TL;DR: DP-SFT：一种两阶段子空间微调方法，通过将DP噪声仅注入到低维任务特定子空间中，在保持差分隐私保证的同时显著降低噪声幅度，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在下游任务上的微调通常依赖敏感数据，引发隐私担忧。传统差分隐私方法在高维参数空间中注入噪声会产生大范数扰动，导致性能下降和训练不稳定。

Method: 提出两阶段子空间微调方法DP-SFT：第一阶段通过分析主梯度方向识别低维任务特定子空间；第二阶段将完整梯度投影到该子空间，添加DP噪声，然后将扰动梯度映射回原始参数空间进行模型更新。

Result: 在多个数据集上的实验表明，DP-SFT在严格的DP约束下提高了准确性和稳定性，加速了收敛速度，相比DP微调基线取得了显著提升。

Conclusion: DP-SFT通过将DP噪声限制在任务相关的低维子空间中，有效降低了噪声幅度，在保持正式DP保证的同时显著提升了差分隐私微调的性能和稳定性。

Abstract: Fine-tuning large language models on downstream tasks is crucial for realizing their cross-domain potential but often relies on sensitive data, raising privacy concerns. Differential privacy (DP) offers rigorous privacy guarantees and has been widely adopted in fine-tuning; however, naively injecting noise across the high-dimensional parameter space creates perturbations with large norms, degrading performance and destabilizing training. To address this issue, we propose DP-SFT, a two-stage subspace fine-tuning method that substantially reduces noise magnitude while preserving formal DP guarantees. Our intuition is that, during fine-tuning, significant parameter updates lie within a low-dimensional, task-specific subspace, while other directions change minimally. Hence, we only inject DP noise into this subspace to protect privacy without perturbing irrelevant parameters. In phase one, we identify the subspace by analyzing principal gradient directions to capture task-specific update signals. In phase two, we project full gradients onto this subspace, add DP noise, and map the perturbed gradients back to the original parameter space for model updates, markedly lowering noise impact. Experiments on multiple datasets demonstrate that DP-SFT enhances accuracy and stability under rigorous DP constraints, accelerates convergence, and achieves substantial gains over DP fine-tuning baselines.

</details>


### [55] [Optimized Algorithms for Text Clustering with LLM-Generated Constraints](https://arxiv.org/abs/2601.11118)
*Chaoqi Jia,Weihong Wu,Longkun Guo,Zhigang Lu,Chao Chen,Kok-Leong Ong*

Main category: cs.LG

TL;DR: 提出一种基于大语言模型的约束生成方法，通过生成约束集而非传统成对约束来减少资源消耗，并结合定制化聚类算法处理可能不准确的约束


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法通过人工标注的成对约束（必须链接和不能链接）来提高准确性，但这种方法成本高且效率低。随着大语言模型的发展，需要更高效的自动约束生成方法来减少资源消耗并提高约束质量

Method: 1) 提出新颖的约束生成方法，生成约束集而非传统成对约束，提高查询效率和约束准确性；2) 设计针对LLM生成约束特点的约束聚类算法，包含置信度阈值和惩罚机制来处理可能不准确的约束

Result: 在五个文本数据集上的评估显示，该方法在保持与最先进算法相当的聚类准确性的同时，将LLM查询次数减少了20倍以上，显著降低了约束生成成本

Conclusion: 提出的方法通过高效的约束生成和针对性的聚类算法，在显著减少资源消耗的同时保持了聚类质量，为基于LLM的约束聚类提供了有效的解决方案

Abstract: Clustering is a fundamental tool that has garnered significant interest across a wide range of applications including text analysis. To improve clustering accuracy, many researchers have incorporated background knowledge, typically in the form of must-link and cannot-link constraints, to guide the clustering process. With the recent advent of large language models (LLMs), there is growing interest in improving clustering quality through LLM-based automatic constraint generation. In this paper, we propose a novel constraint-generation approach that reduces resource consumption by generating constraint sets rather than using traditional pairwise constraints. This approach improves both query efficiency and constraint accuracy compared to state-of-the-art methods. We further introduce a constrained clustering algorithm tailored to the characteristics of LLM-generated constraints. Our method incorporates a confidence threshold and a penalty mechanism to address potentially inaccurate constraints. We evaluate our approach on five text datasets, considering both the cost of constraint generation and the overall clustering performance. The results show that our method achieves clustering accuracy comparable to the state-of-the-art algorithms while reducing the number of LLM queries by more than 20 times.

</details>


### [56] [Shape-morphing programming of soft materials on complex geometries via neural operator](https://arxiv.org/abs/2601.11126)
*Lu Chen,Gengxiang Chen,Xu Liu,Jingyan Su,Xuhao Lyu,Lihui Wang,Yingguang Li*

Main category: cs.LG

TL;DR: 提出S2NO神经算子，结合谱方法和空间卷积，实现复杂几何体上的高保真形状变形预测与材料分布优化


<details>
  <summary>Details</summary>
Motivation: 现有形状变形设计主要针对简单几何体，难以实现复杂几何体（如不规则边界、多孔结构、薄壁结构）上的精确多样化变形，限制了在植入物部署、气动变形等高级应用中的潜力

Method: 提出谱空间神经算子(S2NO)，整合拉普拉斯特征函数编码和空间卷积，有效捕捉不规则计算域上的全局和局部变形行为；结合进化算法进行体素级材料分布优化；利用神经算子的离散不变性实现超分辨率材料分布设计

Result: S2NO能够在各种复杂几何体上实现高保真变形预测，包括不规则边界形状、多孔结构和薄壁结构；超分辨率设计进一步扩展了变形设计的多样性和复杂性

Conclusion: S2NO显著提高了编程复杂形状变形的效率和能力，为高级应用如保形植入物部署和气动变形提供了有效解决方案

Abstract: Shape-morphing soft materials can enable diverse target morphologies through voxel-level material distribution design, offering significant potential for various applications. Despite progress in basic shape-morphing design with simple geometries, achieving advanced applications such as conformal implant deployment or aerodynamic morphing requires accurate and diverse morphing designs on complex geometries, which remains challenging. Here, we present a Spectral and Spatial Neural Operator (S2NO), which enables high-fidelity morphing prediction on complex geometries. S2NO effectively captures global and local morphing behaviours on irregular computational domains by integrating Laplacian eigenfunction encoding and spatial convolutions. Combining S2NO with evolutionary algorithms enables voxel-level optimisation of material distributions for shape morphing programming on various complex geometries, including irregular-boundary shapes, porous structures, and thin-walled structures. Furthermore, the neural operator's discretisation-invariant property enables super-resolution material distribution design, further expanding the diversity and complexity of morphing design. These advancements significantly improve the efficiency and capability of programming complex shape morphing.

</details>


### [57] [Context-aware Graph Causality Inference for Few-Shot Molecular Property Prediction](https://arxiv.org/abs/2601.11135)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: CaMol：一种基于因果推理的上下文感知图学习框架，用于小样本分子性质预测，通过发现因果亚结构提高准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 当前基于上下文学习的分子性质预测方法存在两个主要局限：1）未能充分利用与性质因果相关的官能团先验知识；2）无法识别与性质直接相关的关键亚结构。需要一种能够从因果角度理解分子结构与性质关系的框架。

Method: CaMol采用因果推理视角，假设每个分子包含决定特定性质的潜在因果结构。方法包括：1）构建编码化学知识的上下文图，连接官能团、分子和性质；2）提出可学习的原子掩码策略，分离因果亚结构和混杂亚结构；3）引入分布干预器，通过后门调整结合因果亚结构和化学基础的混杂因子。

Result: 在多个分子数据集上的实验表明，CaMol在小样本任务中实现了优越的准确性和样本效率，展现出对未见性质的泛化能力。发现的因果亚结构与官能团的化学知识高度一致，支持模型的可解释性。

Conclusion: CaMol通过因果推理框架有效解决了小样本分子性质预测中的关键挑战，不仅提高了预测性能，还提供了与化学知识一致的因果亚结构解释，为分子性质预测提供了新的可解释性方法。

Abstract: Molecular property prediction is becoming one of the major applications of graph learning in Web-based services, e.g., online protein structure prediction and drug discovery. A key challenge arises in few-shot scenarios, where only a few labeled molecules are available for predicting unseen properties. Recently, several studies have used in-context learning to capture relationships among molecules and properties, but they face two limitations in: (1) exploiting prior knowledge of functional groups that are causally linked to properties and (2) identifying key substructures directly correlated with properties. We propose CaMol, a context-aware graph causality inference framework, to address these challenges by using a causal inference perspective, assuming that each molecule consists of a latent causal structure that determines a specific property. First, we introduce a context graph that encodes chemical knowledge by linking functional groups, molecules, and properties to guide the discovery of causal substructures. Second, we propose a learnable atom masking strategy to disentangle causal substructures from confounding ones. Third, we introduce a distribution intervener that applies backdoor adjustment by combining causal substructures with chemically grounded confounders, disentangling causal effects from real-world chemical variations. Experiments on diverse molecular datasets showed that CaMol achieved superior accuracy and sample efficiency in few-shot tasks, showing its generalizability to unseen properties. Also, the discovered causal substructures were strongly aligned with chemical knowledge about functional groups, supporting the model interpretability.

</details>


### [58] [Assesing the Viability of Unsupervised Learning with Autoencoders for Predictive Maintenance in Helicopter Engines](https://arxiv.org/abs/2601.11154)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 比较监督分类与无监督自编码器两种直升机发动机故障预测方法，发现监督方法在有标注数据时表现好，但自编码器无需故障标签也能有效检测，更适合故障数据稀缺的场景。


<details>
  <summary>Details</summary>
Motivation: 直升机发动机的意外故障会导致严重的运营中断、安全风险和昂贵的维修费用，需要有效的预测性维护策略来降低这些风险。

Method: 比较两种预测性维护策略：1）监督分类管道，依赖正常和故障行为的标注示例；2）基于自编码器的无监督异常检测方法，仅使用健康发动机数据学习正常操作模型，将偏差标记为潜在故障。

Result: 监督模型在有标注故障数据时表现出色，而自编码器无需故障标签也能实现有效检测。两种方法在真实直升机发动机遥测数据集上进行了评估。

Conclusion: 监督方法在可获得标注故障数据时准确率高，但自编码器在故障数据稀缺或不完整的情况下特别适用。比较突出了准确性、数据可用性和部署可行性之间的权衡，强调了无监督学习作为航空航天应用早期故障检测可行解决方案的潜力。

Abstract: Unplanned engine failures in helicopters can lead to severe operational disruptions, safety hazards, and costly repairs. To mitigate these risks, this study compares two predictive maintenance strategies for helicopter engines: a supervised classification pipeline and an unsupervised anomaly detection approach based on autoencoders (AEs). The supervised method relies on labelled examples of both normal and faulty behaviour, while the unsupervised approach learns a model of normal operation using only healthy engine data, flagging deviations as potential faults. Both methods are evaluated on a real-world dataset comprising labelled snapshots of helicopter engine telemetry. While supervised models demonstrate strong performance when annotated failures are available, the AE achieves effective detection without requiring fault labels, making it particularly well suited for settings where failure data are scarce or incomplete. The comparison highlights the practical trade-offs between accuracy, data availability, and deployment feasibility, and underscores the potential of unsupervised learning as a viable solution for early fault detection in aerospace applications.

</details>


### [59] [Theoretically and Practically Efficient Resistance Distance Computation on Large Graphs](https://arxiv.org/abs/2601.11159)
*Yichun Yang,Longlong Lin,Rong-Hua Li,Meihao Liao,Guoren Wang*

Main category: cs.LG

TL;DR: 提出两种基于Lanczos方法的高效电阻距离计算算法：全局算法Lanczos Iteration（时间复杂度$\tilde{O}(\sqrtκ m)$）和局部算法Lanczos Push（时间复杂度$\tilde{O}(κ^{2.75})$），显著改进了现有方法。


<details>
  <summary>Details</summary>
Motivation: 电阻距离在图分析中应用广泛（如图聚类、链接预测、图神经网络），但现有方法在大图上效率低下，特别是当图拉普拉斯矩阵条件数κ较大时收敛缓慢。

Method: 受经典Lanczos方法启发，提出两种新算法：1) Lanczos Iteration - 全局算法，时间复杂度$\tilde{O}(\sqrtκ m)$；2) Lanczos Push - 局部算法，时间复杂度与图大小无关，为$\tilde{O}(κ^{2.75})$。

Result: 在8个不同规模和统计特性的真实数据集上验证，Lanczos Iteration和Lanczos Push在效率和精度上都显著优于现有最优方法。

Conclusion: 提出的两种Lanczos算法有效解决了大图上电阻距离计算效率低的问题，显著降低了条件数κ的依赖，为图分析应用提供了高效解决方案。

Abstract: The computation of resistance distance is pivotal in a wide range of graph analysis applications, including graph clustering, link prediction, and graph neural networks. Despite its foundational importance, efficient algorithms for computing resistance distances on large graphs are still lacking. Existing state-of-the-art (SOTA) methods, including power iteration-based algorithms and random walk-based local approaches, often struggle with slow convergence rates, particularly when the condition number of the graph Laplacian matrix, denoted by $κ$, is large. To tackle this challenge, we propose two novel and efficient algorithms inspired by the classic Lanczos method: Lanczos Iteration and Lanczos Push, both designed to reduce dependence on $κ$. Among them, Lanczos Iteration is a near-linear time global algorithm, whereas Lanczos Push is a local algorithm with a time complexity independent of the size of the graph. More specifically, we prove that the time complexity of Lanczos Iteration is $\tilde{O}(\sqrtκ m)$ ($m$ is the number of edges of the graph and $\tilde{O}$ means the complexity omitting the $\log$ terms) which achieves a speedup of $\sqrtκ$ compared to previous power iteration-based global methods. For Lanczos Push, we demonstrate that its time complexity is $\tilde{O}(κ^{2.75})$ under certain mild and frequently established assumptions, which represents a significant improvement of $κ^{0.25}$ over the SOTA random walk-based local algorithms. We validate our algorithms through extensive experiments on eight real-world datasets of varying sizes and statistical properties, demonstrating that Lanczos Iteration and Lanczos Push significantly outperform SOTA methods in terms of both efficiency and accuracy.

</details>


### [60] [Clustering High-dimensional Data: Balancing Abstraction and Representation Tutorial at AAAI 2026](https://arxiv.org/abs/2601.11160)
*Claudia Plant,Lena G. M. Bauer,Christian Böhm*

Main category: cs.LG

TL;DR: 该教程探讨聚类算法中抽象与表示之间的平衡，分析从传统K-means到子空间和深度聚类的发展，强调未来需要更自适应地平衡这两者以提高性能。


<details>
  <summary>Details</summary>
Motivation: 聚类需要在抽象和表示之间取得平衡：既要抽象掉个体对象的冗余细节，又要保留区分不同群组的关键特征。现有算法在这两者间的权衡各不相同，需要系统性地探讨如何优化这种平衡。

Method: 分析不同聚类方法的抽象-表示权衡：传统K-means采用高度抽象（平均化细节）和简单表示（高斯分布）；子空间聚类通过多个潜在空间分别学习聚类相关信息和无关信息；深度聚类通过基于质心和密度的聚类损失来强制抽象。

Result: 展示了不同聚类方法在抽象-表示谱系中的位置，指出随着表示能力的增强，需要在目标函数中显式强制抽象以防止过度拟合。子空间聚类的多潜在空间方法为解决这一矛盾提供了思路。

Conclusion: 未来聚类研究需要更自适应地平衡抽象与表示，以提高性能、能效和可解释性。人脑在聚类和相关任务中的优异表现表明仍有很大改进空间，自动寻找抽象与表示的最佳平衡点是关键方向。

Abstract: How to find a natural grouping of a large real data set? Clustering requires a balance between abstraction and representation. To identify clusters, we need to abstract from superfluous details of individual objects. But we also need a rich representation that emphasizes the key features shared by groups of objects that distinguish them from other groups of objects.
  Each clustering algorithm implements a different trade-off between abstraction and representation. Classical K-means implements a high level of abstraction - details are simply averaged out - combined with a very simple representation - all clusters are Gaussians in the original data space. We will see how approaches to subspace and deep clustering support high-dimensional and complex data by allowing richer representations. However, with increasing representational expressiveness comes the need to explicitly enforce abstraction in the objective function to ensure that the resulting method performs clustering and not just representation learning. We will see how current deep clustering methods define and enforce abstraction through centroid-based and density-based clustering losses. Balancing the conflicting goals of abstraction and representation is challenging. Ideas from subspace clustering help by learning one latent space for the information that is relevant to clustering and another latent space to capture all other information in the data.
  The tutorial ends with an outlook on future research in clustering. Future methods will more adaptively balance abstraction and representation to improve performance, energy efficiency and interpretability. By automatically finding the sweet spot between abstraction and representation, the human brain is very good at clustering and other related tasks such as single-shot learning. So, there is still much room for improvement.

</details>


### [61] [GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling](https://arxiv.org/abs/2601.11161)
*Pascal Schlachter,Bin Yang*

Main category: cs.LG

TL;DR: GMM-COMET：首个持续源自由通用域自适应方法，通过高斯混合模型伪标记和均值教师框架，在多个未标记目标域流上实现稳定自适应。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，源数据在自适应过程中可能不再可用，且目标域标签空间可能与源域不同。现有SF-UniDA方法仅假设单一域偏移，而实际应用中模型需要连续适应多个不同未标记目标域。

Method: 结合高斯混合模型伪标记与均值教师框架，引入一致性损失增强鲁棒性，在持续SF-UniDA设置中实现稳定自适应。

Result: GMM-COMET在所有评估场景中持续优于仅源模型，为持续SF-UniDA提供了强大的首个基线方法。

Conclusion: 该方法成功解决了持续源自由通用域自适应问题，通过集成伪标记和均值教师框架实现了对多个目标域的稳定适应，为未来研究奠定了基础。

Abstract: Unsupervised domain adaptation tackles the problem that domain shifts between training and test data impair the performance of neural networks in many real-world applications. Thereby, in realistic scenarios, the source data may no longer be available during adaptation, and the label space of the target domain may differ from the source label space. This setting, known as source-free universal domain adaptation (SF-UniDA), has recently gained attention, but all existing approaches only assume a single domain shift from source to target. In this work, we present the first study on continual SF-UniDA, where the model must adapt sequentially to a stream of multiple different unlabeled target domains. Building upon our previous methods for online SF-UniDA, we combine their key ideas by integrating Gaussian mixture model-based pseudo-labeling within a mean teacher framework for improved stability over long adaptation sequences. Additionally, we introduce consistency losses for further robustness. The resulting method GMM-COMET provides a strong first baseline for continual SF-UniDA and is the only approach in our experiments to consistently improve upon the source-only model across all evaluated scenarios. Our code is available at https://github.com/pascalschlachter/GMM-COMET.

</details>


### [62] [LSTM VS. Feed-Forward Autoencoders for Unsupervised Fault Detection in Hydraulic Pumps](https://arxiv.org/abs/2601.11163)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 提出两种基于自编码器的无监督故障检测方法，用于工业液压泵的早期故障预警，无需故障样本训练即可实现高可靠性检测。


<details>
  <summary>Details</summary>
Motivation: 工业液压泵的意外故障会导致生产中断和巨大成本，需要有效的早期故障检测方法。传统方法通常需要故障样本，但在实际中故障数据难以获取。

Method: 提出两种无监督自编码器方案：1）前馈模型分析单个传感器快照；2）LSTM模型捕捉短期时间窗口。两种模型仅使用健康数据训练，基于52个传感器通道的分钟级日志。

Result: 尽管训练时没有故障样本，两种模型在包含7个标注故障区间的独立测试集上都实现了高可靠性检测。

Conclusion: 无监督自编码器方法能够有效检测工业液压泵的早期故障，仅需健康数据训练即可实现高可靠性，具有实际应用价值。

Abstract: Unplanned failures in industrial hydraulic pumps can halt production and incur substantial costs. We explore two unsupervised autoencoder (AE) schemes for early fault detection: a feed-forward model that analyses individual sensor snapshots and a Long Short-Term Memory (LSTM) model that captures short temporal windows. Both networks are trained only on healthy data drawn from a minute-level log of 52 sensor channels; evaluation uses a separate set that contains seven annotated fault intervals. Despite the absence of fault samples during training, the models achieve high reliability.

</details>


### [63] [TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation](https://arxiv.org/abs/2601.11184)
*Xiangyu Xu,Qingsong Zhong,Jilin Hu*

Main category: cs.LG

TL;DR: 提出结构解耦的多尺度时间序列生成框架，通过双路径VQ-VAE分离趋势和季节性成分，实现从粗到细的自回归生成，在减少参数量的同时提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析面临数据稀缺和隐私挑战，生成建模提供了有前景的解决方案。然而，时间序列的结构复杂性（多尺度时间模式和异质成分）尚未得到充分解决。

Method: 1) 将序列编码为多个时间分辨率的离散token，以从粗到细的方式进行自回归生成；2) 引入双路径VQ-VAE解耦趋势和季节性成分，学习语义一致的潜在表示；3) 提出基于引导的重建策略，使用粗粒度季节性信号作为先验来指导细粒度季节性模式的重建。

Result: 在六个数据集上的实验表明，该方法比现有方法生成更高质量的时间序列。模型在显著减少参数数量的情况下实现了强劲性能，并展现出生成高质量长期序列的卓越能力。

Conclusion: 提出的结构解耦多尺度生成框架有效解决了时间序列的结构复杂性，在保持参数效率的同时提升了生成质量，为时间序列生成建模提供了新的解决方案。

Abstract: Generative modeling offers a promising solution to data scarcity and privacy challenges in time series analysis. However, the structural complexity of time series, characterized by multi-scale temporal patterns and heterogeneous components, remains insufficiently addressed. In this work, we propose a structure-disentangled multiscale generation framework for time series. Our approach encodes sequences into discrete tokens at multiple temporal resolutions and performs autoregressive generation in a coarse-to-fine manner, thereby preserving hierarchical dependencies. To tackle structural heterogeneity, we introduce a dual-path VQ-VAE that disentangles trend and seasonal components, enabling the learning of semantically consistent latent representations. Additionally, we present a guidance-based reconstruction strategy, where coarse seasonal signals are utilized as priors to guide the reconstruction of fine-grained seasonal patterns. Experiments on six datasets show that our approach produces higher-quality time series than existing methods. Notably, our model achieves strong performance with a significantly reduced parameter count and exhibits superior capability in generating high-quality long-term sequences. Our implementation is available at https://anonymous.4open.science/r/TimeMAR-BC5B.

</details>


### [64] [FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization](https://arxiv.org/abs/2601.11200)
*Haiyang Xiao,Weiqing Li,Jinyue Guo,Guochao Jiang,Guohua Liu,Yuewei Zhang*

Main category: cs.LG

TL;DR: FAQ框架利用同系列大模型的先验知识生成高保真校准数据，通过群体竞争和重归一化提升后训练量化精度，相比基线减少28.5%的精度损失。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化方法依赖有限校准样本，难以捕捉推理阶段的激活分布，导致量化参数偏差。校准数据的代表性和通用性是决定量化精度的核心瓶颈。

Method: FAQ框架：1）将原始校准样本输入同系列更大LLM，利用一致知识体系生成高保真校准数据；2）数据包含思维链推理并符合预期激活分布；3）通过专家指导下的群体竞争选择最佳样本；4）重归一化增强标准PTQ效果。

Result: 在Qwen3-8B等多个模型系列上的实验表明，FAQ相比使用原始校准数据的基线方法，能减少高达28.5%的精度损失，展现了强大的潜力和贡献。

Conclusion: FAQ通过利用同系列大模型的先验知识生成高质量校准数据，有效解决了传统PTQ方法中校准数据代表性和通用性不足的问题，显著提升了量化精度。

Abstract: Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.

</details>


### [65] [SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients](https://arxiv.org/abs/2601.11219)
*Zhikang Shen,Jianrong Lu,Haiyuan Wan,Jianhai Chen*

Main category: cs.LG

TL;DR: SDFLoRA：一种选择性双模块联邦LoRA方法，通过将客户端适配器分解为全局模块和本地模块来解决联邦学习中LoRA的秩异构问题，实现更好的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的大语言模型适配面临秩异构问题，不同客户端使用不同的低秩配置导致直接聚合LoRA更新存在偏差和不稳定。现有解决方案通常强制统一秩或将异构更新对齐到共享子空间，这会过度约束客户端特定语义、限制个性化，并且在差分隐私噪声下对本地客户端信息保护较弱。

Method: 提出选择性双模块联邦LoRA（SDFLoRA），将每个客户端适配器分解为：1）全局模块，捕获可转移知识；2）本地模块，保留客户端特定适配。全局模块在客户端间选择性对齐和聚合，而本地模块保持私有。这种设计支持在秩异构下的鲁棒学习，并通过仅在全局模块中注入差分隐私噪声来实现隐私感知优化。

Result: 在GLUE基准测试上的实验表明，SDFLoRA优于代表性的联邦LoRA基线方法，并实现了更好的效用-隐私权衡。

Conclusion: SDFLoRA通过将客户端适配器分解为全局和本地模块，有效解决了联邦学习中LoRA的秩异构问题，既支持个性化适配又保护了客户端隐私，在保持性能的同时实现了更好的隐私保护。

Abstract: Federated learning (FL) for large language models (LLMs) has attracted increasing attention as a way to enable privacy-preserving adaptation over distributed data. Parameter-efficient methods such as LoRA are widely adopted to reduce communication and memory costs. Despite these advances, practical FL deployments often exhibit rank heterogeneity, since different clients may use different low-rank configurations. This makes direct aggregation of LoRA updates biased and unstable. Existing solutions typically enforce unified ranks or align heterogeneous updates into a shared subspace, which over-constrains client-specific semantics, limits personalization, and provides weak protection of local client information under differential privacy noise. To address this issue, we propose Selective Dual-module Federated LoRA (SDFLoRA), which decomposes each client adapter into a global module that captures transferable knowledge and a local module that preserves client-specific adaptations. The global module is selectively aligned and aggregated across clients, while local modules remain private. This design enables robust learning under rank heterogeneity and supports privacy-aware optimization by injecting differential privacy noise exclusively into the global module. Experiments on GLUE benchmarks demonstrate that SDFLoRA outperforms representative federated LoRA baselines and achieves a better utility-privacy trade-off.

</details>


### [66] [Operator learning on domain boundary through combining fundamental solution-based artificial data and boundary integral techniques](https://arxiv.org/abs/2601.11222)
*Haochen Wu,Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: 提出MAD-BNO框架，仅使用边界数据学习偏微分方程算子，通过数学人工数据生成训练样本，无需全区域采样或数值模拟。


<details>
  <summary>Details</summary>
Motivation: 传统算子学习方法需要全区域采样数据，计算成本高。本文旨在开发仅依赖边界数据的算子学习框架，减少数据需求并提高效率。

Method: 结合数学人工数据方法，从基本解生成边界数据对（Dirichlet-Neumann数据），学习边界到边界的映射。训练后通过边界积分公式恢复内部解。

Result: 在二维Laplace、Poisson和Helmholtz方程基准测试中，达到或优于现有神经算子方法的精度，同时显著减少训练时间。

Conclusion: MAD-BNO提供了一种高效的数据驱动算子学习方法，仅需边界数据，可扩展到三维问题和复杂几何形状。

Abstract: For linear partial differential equations with known fundamental solutions, this work introduces a novel operator learning framework that relies exclusively on domain boundary data, including solution values and normal derivatives, rather than full-domain sampling. By integrating the previously developed Mathematical Artificial Data (MAD) method, which enforces physical consistency, all training data are synthesized directly from the fundamental solutions of the target problems, resulting in a fully data-driven pipeline without the need for external measurements or numerical simulations. We refer to this approach as the Mathematical Artificial Data Boundary Neural Operator (MAD-BNO), which learns boundary-to-boundary mappings using MAD-generated Dirichlet-Neumann data pairs. Once trained, the interior solution at arbitrary locations can be efficiently recovered through boundary integral formulations, supporting Dirichlet, Neumann, and mixed boundary conditions as well as general source terms. The proposed method is validated on benchmark operator learning tasks for two-dimensional Laplace, Poisson, and Helmholtz equations, where it achieves accuracy comparable to or better than existing neural operator approaches while significantly reducing training time. The framework is naturally extensible to three-dimensional problems and complex geometries.

</details>


### [67] [Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation](https://arxiv.org/abs/2601.11258)
*Pingzhi Tang,Yiding Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: PaST框架通过提取技能向量实现模块化技能转移，解决LLM知识更新中SFT和RL正交性问题，在知识问答和工具使用任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM面临"知识截止"挑战，现有SFT方法能更新事实知识但无法可靠提升问答推理能力，RL训练计算成本过高，且观察到SFT和RL的参数更新几乎正交，需要更高效的知识适应方法。

Method: 提出参数化技能转移(PaST)框架：1)从源域提取领域无关的技能向量；2)目标模型在轻量级SFT后，通过线性注入方式将知识操作技能转移到目标模型中。

Result: 在SQuAD上比最先进的SFT基线提升9.9分；在LooGLE长上下文QA上获得8.0分绝对准确率提升；在ToolBench工具使用基准上平均提升10.3分成功率，技能向量展现良好可扩展性和跨域迁移性。

Conclusion: PaST通过模块化技能转移有效解决了LLM知识适应问题，证明了技能向量的可迁移性和实用性，为高效知识更新提供了新思路。

Abstract: Large Language Models (LLMs) face the "knowledge cutoff" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.

</details>


### [68] [Latent Dynamics Graph Convolutional Networks for model order reduction of parameterized time-dependent PDEs](https://arxiv.org/abs/2601.11259)
*Lorenzo Tomada,Federico Pichi,Gianluigi Rozza*

Main category: cs.LG

TL;DR: 提出LD-GCN：一种无编码器的图神经网络架构，用于参数化偏微分方程的非线性模型降阶，结合几何归纳偏置与可解释的潜在动力学，支持时间外推和零样本预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时结合几何归纳偏置与可解释的潜在行为，要么忽略动力学特征，要么忽视空间信息。需要一种能学习全局低维表示并保持可解释性的数据驱动方法。

Method: 提出Latent Dynamics Graph Convolutional Network (LD-GCN)：无编码器的纯数据驱动架构，在潜在空间建模时间演化并通过时间步进推进，使用GNN将轨迹解码到几何参数化域上。

Result: 方法通过通用逼近定理进行数学验证，在复杂计算力学问题上数值测试成功，包括Navier-Stokes方程的分岔现象检测，支持时间外推和潜在插值的零样本预测。

Conclusion: LD-GCN成功结合了几何归纳偏置与可解释的潜在动力学，为参数化PDE的非线性模型降阶提供了有效框架，增强了可解释性并支持零样本预测。

Abstract: Graph Neural Networks (GNNs) are emerging as powerful tools for nonlinear Model Order Reduction (MOR) of time-dependent parameterized Partial Differential Equations (PDEs). However, existing methodologies struggle to combine geometric inductive biases with interpretable latent behavior, overlooking dynamics-driven features or disregarding spatial information. In this work, we address this gap by introducing Latent Dynamics Graph Convolutional Network (LD-GCN), a purely data-driven, encoder-free architecture that learns a global, low-dimensional representation of dynamical systems conditioned on external inputs and parameters. The temporal evolution is modeled in the latent space and advanced through time-stepping, allowing for time-extrapolation, and the trajectories are consistently decoded onto geometrically parameterized domains using a GNN. Our framework enhances interpretability by enabling the analysis of the reduced dynamics and supporting zero-shot prediction through latent interpolation. The methodology is mathematically validated via a universal approximation theorem for encoder-free architectures, and numerically tested on complex computational mechanics problems involving physical and geometric parameters, including the detection of bifurcating phenomena for Navier-Stokes equations. Code availability: https://github.com/lorenzotomada/ld-gcn-rom

</details>


### [69] [Sample-Near-Optimal Agnostic Boosting with Improved Running Time](https://arxiv.org/abs/2601.11265)
*Arthur da Cunha,Miakel Møller Høgsgaard,Andrea Paudice*

Main category: cs.LG

TL;DR: 提出了首个具有近最优样本复杂度的不可知增强算法，在固定其他参数时以多项式时间运行


<details>
  <summary>Details</summary>
Motivation: 经典增强方法已有深入理解，但在不可知设置下（不对数据做任何假设）研究较少。虽然最近几乎解决了不可知增强的样本复杂度问题，但已知算法具有指数级运行时间，需要更高效的算法。

Method: 提出新的不可知增强算法，在固定其他参数时，运行时间相对于样本大小是多项式的，同时保持近最优的样本复杂度。

Result: 首次实现了具有近最优样本复杂度的不可知增强算法，且运行时间在固定其他参数时相对于样本大小是多项式的。

Conclusion: 该工作填补了不可知增强算法效率的空白，提供了第一个既具有理论保证又实际可行的不可知增强算法。

Abstract: Boosting is a powerful method that turns weak learners, which perform only slightly better than random guessing, into strong learners with high accuracy. While boosting is well understood in the classic setting, it is less so in the agnostic case, where no assumptions are made about the data. Indeed, only recently was the sample complexity of agnostic boosting nearly settled arXiv:2503.09384, but the known algorithm achieving this bound has exponential running time. In this work, we propose the first agnostic boosting algorithm with near-optimal sample complexity, running in time polynomial in the sample size when considering the other parameters of the problem fixed.

</details>


### [70] [Metabolomic Biomarker Discovery for ADHD Diagnosis Using Interpretable Machine Learning](https://arxiv.org/abs/2601.11283)
*Nabil Belacel,Mohamed Rachid Boulassel*

Main category: cs.LG

TL;DR: 该研究通过尿液代谢组学结合可解释机器学习，开发了基于14种代谢物的ADHD诊断模型，AUC>0.97，为ADHD提供了客观生物学诊断框架。


<details>
  <summary>Details</summary>
Motivation: ADHD作为一种常见的神经发育障碍，目前缺乏客观的诊断工具，需要基于生物学的精准精神病学诊断框架。

Method: 整合尿液代谢组学与可解释机器学习框架，使用Closest Resemblance分类器结合特征选择，分析52名ADHD患者和46名对照的靶向代谢组学数据。

Result: CR模型优于随机森林和K近邻分类器，基于14种代谢物（包括多巴胺4-硫酸盐、N-乙酰天冬氨酰谷氨酸和瓜氨酸）实现AUC>0.97，这些代谢物与多巴胺能神经传递和氨基酸代谢通路相关。

Conclusion: 该研究展示了代谢组学与可解释机器学习相结合的转化框架，为ADHD提供了客观、生物学信息化的诊断策略，支持未来整合到靶向代谢组学检测和即时诊断平台。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a prevalent neurodevelopmental disorder with limited objective diagnostic tools, highlighting the urgent need for objective, biology-based diagnostic frameworks in precision psychiatry. We integrate urinary metabolomics with an interpretable machine learning framework to identify biochemical signatures associated with ADHD. Targeted metabolomic profiles from 52 ADHD and 46 control participants were analyzed using a Closest Resemblance (CR) classifier with embedded feature selection. The CR model outperformed Random Forest and K-Nearest Neighbor classifiers, achieving an AUC > 0.97 based on a reduced panel of 14 metabolites. These metabolites including dopamine 4-sulfate, N-acetylaspartylglutamic acid, and citrulline map to dopaminergic neurotransmission and amino acid metabolism pathways, offering mechanistic insight into ADHD pathophysiology. The CR classifier's transparent decision boundaries and low computational cost support integration into targeted metabolomic assays and future point of care diagnostic platforms. Overall, this work demonstrates a translational framework combining metabolomics and interpretable machine learning to advance objective, biologically informed diagnostic strategies for ADHD.

</details>


### [71] [FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning](https://arxiv.org/abs/2601.11311)
*Zhihan Yang,Jiaqi Wei,Xiang Zhang,Haoyu Dong,Yiwen Wang,Xiaoke Guo,Pengkun Zhang,Yiwei Xu,Chenyu You*

Main category: cs.LG

TL;DR: FORESTLLM：结合决策树结构偏置与LLM语义推理的少样本表格学习框架，仅训练时使用LLM设计轻量可解释森林模型


<details>
  <summary>Details</summary>
Motivation: 传统树方法在少样本场景下因统计纯度指标不稳定易过拟合，而直接应用LLM常忽略表格数据结构导致性能不佳。需要结合两者优势解决少样本表格学习挑战。

Method: 1) 语义分割准则：LLM基于标记和未标记数据评估候选分割的语义连贯性，构建更鲁棒的树结构；2) 一次性上下文推理机制：LLM将决策路径和支持样本蒸馏为确定性预测，替代噪声经验估计

Result: 在多样化少样本分类和回归基准测试中，FORESTLLM实现了最先进的性能

Conclusion: FORESTLLM成功统一了决策森林的结构归纳偏置与LLM的语义推理能力，在少样本表格学习中取得突破，同时保持了测试时的轻量性和可解释性

Abstract: Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.

</details>


### [72] [Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models](https://arxiv.org/abs/2601.11342)
*Chuanyue Yu,Jiahui Wang,Yuhan Li,Heng Chang,Ge Lan,Qingyun Sun,Jia Li,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: 本文提出SPREAD框架，通过查询相关性引导的去噪策略解决扩散语言模型在检索增强生成中的语义漂移问题，显著提升生成精度。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散语言模型在自然语言处理任务中表现出色，但检索增强生成在增强大语言模型方面的成功潜力尚未在DLMs中得到充分探索，主要由于LLM和DLM解码机制的根本差异。现有DLMs在RAG框架中存在生成精度有限的问题。

Method: 提出SPREAD（Semantic-Preserving REtrieval-Augmented Diffusion）框架，引入查询相关性引导的去噪策略。该策略通过主动引导去噪轨迹，确保生成内容始终锚定在查询语义上，有效抑制语义漂移。

Result: 实验结果表明，SPREAD显著增强了RAG框架中生成答案的精度，有效缓解了响应语义漂移问题。DLMs与RAG结合显示出对上下文信息更强的依赖性。

Conclusion: SPREAD框架成功解决了扩散语言模型在检索增强生成中的语义漂移问题，为DLMs在RAG框架中的应用提供了有效解决方案，展示了DLMs与RAG结合的潜力。

Abstract: Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.

</details>


### [73] [FEATHer: Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting](https://arxiv.org/abs/2601.11350)
*Jaehoon Lee,Seungwoo Lee,Younghwi Kim,Dohee Kim,Sunghyun Sim*

Main category: cs.LG

TL;DR: FEATHer是一种用于边缘设备时间序列预测的轻量级模型，仅需400参数，在严格资源限制下实现准确长期预测。


<details>
  <summary>Details</summary>
Motivation: 工业领域（如制造、智能工厂）需要边缘设备（PLC、微控制器）上的时间序列预测，这些设备有严格的延迟和内存限制（仅几千参数），传统深度架构不适用。

Method: 1) 超轻量多尺度频率分解；2) 共享密集时间核（投影-深度卷积-投影，无循环或注意力）；3) 频率感知分支门控；4) 稀疏周期核通过周期下采样重建输出。

Result: 在8个基准测试中排名最佳，获得60个第一名结果，平均排名2.05，参数最少仅400个，优于基线模型。

Conclusion: FEATHer证明在受限边缘硬件上可实现可靠的长期预测，为工业实时推理提供了实用方向。

Abstract: Time-series forecasting is fundamental in industrial domains like manufacturing and smart factories. As systems evolve toward automation, models must operate on edge devices (e.g., PLCs, microcontrollers) with strict constraints on latency and memory, limiting parameters to a few thousand. Conventional deep architectures are often impractical here. We propose the Fourier-Efficient Adaptive Temporal Hierarchy Forecaster (FEATHer) for accurate long-term forecasting under severe limits. FEATHer introduces: (i) ultra-lightweight multiscale decomposition into frequency pathways; (ii) a shared Dense Temporal Kernel using projection-depthwise convolution-projection without recurrence or attention; (iii) frequency-aware branch gating that adaptively fuses representations based on spectral characteristics; and (iv) a Sparse Period Kernel reconstructing outputs via period-wise downsampling to capture seasonality. FEATHer maintains a compact architecture (as few as 400 parameters) while outperforming baselines. Across eight benchmarks, it achieves the best ranking, recording 60 first-place results with an average rank of 2.05. These results demonstrate that reliable long-range forecasting is achievable on constrained edge hardware, offering a practical direction for industrial real-time inference.

</details>


### [74] [Latent Space Inference via Paired Autoencoders](https://arxiv.org/abs/2601.11397)
*Emma Hart,Bas Peters,Julianne Chung,Matthias Chung*

Main category: cs.LG

TL;DR: 提出基于配对自编码器的数据驱动潜在空间推理框架，用于处理观测不一致性，通过连接参数空间和观测空间的潜在空间映射，实现正则化反演和优化。


<details>
  <summary>Details</summary>
Motivation: 解决反问题中观测数据不一致性（部分、噪声、分布外数据）的挑战，同时保持与底层物理模型的一致性。

Method: 使用两个自编码器分别处理参数空间和观测空间，通过学习两个潜在空间之间的映射关系，构建正则化反演的代理模型。

Result: 相比单独使用配对自编码器和相同架构的端到端编码器-解码器，该方法在数据不一致场景下能产生更准确的重建结果。

Conclusion: 该框架在医学层析成像和地球物理地震波形反演中得到验证，可广泛应用于科学和工程中的各种反问题。

Abstract: This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mappings between the autoencoders' latent spaces. These mappings enable a surrogate for regularized inversion and optimization in low-dimensional, informative latent spaces. Our flexible framework can work with partial, noisy, or out-of-distribution data, all while maintaining consistency with the underlying physical models. The paired autoencoders enable reconstruction of corrupted data, and then use the reconstructed data for parameter estimation, which produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of the same architecture, especially in scenarios with data inconsistencies. We demonstrate our approaches on two imaging examples in medical tomography and geophysical seismic-waveform inversion, but the described approaches are broadly applicable to a variety of inverse problems in scientific and engineering applications.

</details>


### [75] [Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.11401)
*Ahmed Rashwan,Keith Briggs,Chris Budd,Lisa Kreusser*

Main category: cs.LG

TL;DR: 本文提出扩散价值函数(DVF)，一种用于图马尔可夫决策过程(GMDP)的分解价值函数，通过在图结构上扩散奖励来解决多智能体强化学习中的信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的信用分配在具有结构化局部交互的大规模系统中是一个核心挑战。现有的全局价值函数提供弱学习信号，而局部构造方法难以估计且在无限时域设置中表现不佳。

Method: 提出扩散价值函数(DVF)，通过在图影响结构上结合时间折扣和空间衰减来扩散奖励，为每个智能体分配价值分量。基于DVF提出扩散A2C(DA2C)算法和稀疏消息传递执行器LD-GNN，用于在通信成本下学习去中心化算法。

Result: DVF被证明是良定义的，具有贝尔曼不动点，并通过平均性质分解全局折扣价值。在消防基准和三个分布式计算任务（向量图着色和两个发射功率优化问题）中，DA2C始终优于局部和全局批评器基线，平均奖励提升高达11%。

Conclusion: DVF为图结构多智能体系统提供了一种有效的信用分配方法，能够作为标准RL算法的即插即用批评器，并通过图神经网络进行可扩展估计，显著提升了多智能体强化学习的性能。

Abstract: Credit assignment is a core challenge in multi-agent reinforcement learning (MARL), especially in large-scale systems with structured, local interactions. Graph-based Markov decision processes (GMDPs) capture such settings via an influence graph, but standard critics are poorly aligned with this structure: global value functions provide weak per-agent learning signals, while existing local constructions can be difficult to estimate and ill-behaved in infinite-horizon settings. We introduce the Diffusion Value Function (DVF), a factored value function for GMDPs that assigns to each agent a value component by diffusing rewards over the influence graph with temporal discounting and spatial attenuation. We show that DVF is well-defined, admits a Bellman fixed point, and decomposes the global discounted value via an averaging property. DVF can be used as a drop-in critic in standard RL algorithms and estimated scalably with graph neural networks. Building on DVF, we propose Diffusion A2C (DA2C) and a sparse message-passing actor, Learned DropEdge GNN (LD-GNN), for learning decentralised algorithms under communication costs. Across the firefighting benchmark and three distributed computation tasks (vector graph colouring and two transmit power optimisation problems), DA2C consistently outperforms local and global critic baselines, improving average reward by up to 11%.

</details>


### [76] [Forcing and Diagnosing Failure Modes of Fourier Neural Operators Across Diverse PDE Families](https://arxiv.org/abs/2601.11428)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: FNOs在PDE求解中表现良好，但在分布偏移、长时程推演和结构扰动下的鲁棒性不足。本文提出系统压力测试框架，在五类PDE上暴露FNO的脆弱性，发现参数或边界条件偏移可使误差增加10倍以上。


<details>
  <summary>Details</summary>
Motivation: Fourier神经算子（FNOs）在偏微分方程求解映射学习方面表现出色，但其在分布偏移、长时程推演和结构扰动下的鲁棒性尚未得到充分理解。需要系统性地测试FNOs在不同PDE家族中的失败模式。

Method: 提出系统性压力测试框架，在五类不同性质的PDE家族（色散、椭圆、多尺度流体、金融、混沌系统）上进行大规模评估（训练1000个模型）。设计控制性压力测试，包括参数偏移、边界/终端条件变化、分辨率外推与谱分析、迭代推演等，以暴露FNOs的脆弱性。

Result: 参数或边界条件的分布偏移可使误差增加一个数量级以上；分辨率变化主要将误差集中在高频模式；输入扰动通常不会放大误差，但最坏情况（如局部泊松扰动）仍具挑战性。揭示了谱偏差、累积积分误差和边界条件过拟合等脆弱性。

Conclusion: 研究提供了FNOs失败模式的比较图谱和可操作的改进见解，为算子学习的鲁棒性提升指明了方向。压力测试框架有助于系统评估神经算子的脆弱性，推动更稳健的PDE求解方法发展。

Abstract: Fourier Neural Operators (FNOs) have shown strong performance in learning solution maps of partial differential equations (PDEs), but their robustness under distribution shifts, long-horizon rollouts, and structural perturbations remains poorly understood. We present a systematic stress-testing framework that probes failure modes of FNOs across five qualitatively different PDE families: dispersive, elliptic, multi-scale fluid, financial, and chaotic systems. Rather than optimizing in-distribution accuracy, we design controlled stress tests--including parameter shifts, boundary or terminal condition changes, resolution extrapolation with spectral analysis, and iterative rollouts--to expose vulnerabilities such as spectral bias, compounding integration errors, and overfitting to restricted boundary regimes. Our large-scale evaluation (1{,}000 trained models) reveals that distribution shifts in parameters or boundary conditions can inflate errors by more than an order of magnitude, while resolution changes primarily concentrate error in high-frequency modes. Input perturbations generally do not amplify error, though worst-case scenarios (e.g., localized Poisson perturbations) remain challenging. These findings provide a comparative failure-mode atlas and actionable insights for improving robustness in operator learning.

</details>


### [77] [Inter-patient ECG Arrhythmia Classification with LGNs and LUTNs](https://arxiv.org/abs/2601.11433)
*Wout Mommen,Lars Keuninckx,Paul Detterer,Achiel Colpaert,Piet Wambacq*

Main category: cs.LG

TL;DR: 本文提出使用深度可微分逻辑门网络(LGNs)和查找表网络(LUTNs)进行心电图分类，在MIT-BIH心律失常数据集上达到94.28%准确率，计算量比现有方法低3-6个数量级，功耗仅5-7mW，适合植入式设备。


<details>
  <summary>Details</summary>
Motivation: 开发适用于心脏植入或可穿戴设备的低功耗、高速心律失常检测方法，特别是针对训练集未包含的患者（患者间范式），需要极低计算复杂度和功耗的模型。

Method: 1. 使用深度可微分逻辑门网络(LGNs)和查找表网络(LUTNs)进行ECG分类；2. 提出新颖的预处理方法；3. 使用多路复用器布尔方程训练LUTs；4. 首次在LGNs/LUTNs中采用速率编码；5. 在Artix 7 FPGA上实现，评估功耗和资源使用。

Result: 1. 在MIT-BIH四分类问题上达到94.28%准确率和0.683 jκ指数；2. 仅需2.89k-6.17k FLOPs，比SOTA方法低3-6个数量级；3. FPGA实现需2000-2990个LUTs，功耗5-7mW（每推理50-70pJ）；4. 性能显著优于先前LGN结果。

Conclusion: LGNs和LUTNs可用于极低功耗、高速的心律失常检测，特别适合心脏植入物和可穿戴设备，即使对于训练集未包含的患者也能有效工作，为边缘医疗设备提供了有前景的解决方案。

Abstract: Deep Differentiable Logic Gate Networks (LGNs) and Lookup Table Networks (LUTNs) are demonstrated to be suitable for the automatic classification of electrocardiograms (ECGs) using the inter-patient paradigm. The methods are benchmarked using the MIT-BIH arrhythmia data set, achieving up to 94.28% accuracy and a $jκ$ index of 0.683 on a four-class classification problem. Our models use between 2.89k and 6.17k FLOPs, including preprocessing and readout, which is three to six orders of magnitude less compared to SOTA methods. A novel preprocessing method is utilized that attains superior performance compared to existing methods for both the mixed-patient and inter-patient paradigms. In addition, a novel method for training the Lookup Tables (LUTs) in LUTNs is devised that uses the Boolean equation of a multiplexer (MUX). Additionally, rate coding was utilized for the first time in these LGNs and LUTNs, enhancing the performance of LGNs. Furthermore, it is the first time that LGNs and LUTNs have been benchmarked on the MIT-BIH arrhythmia dataset using the inter-patient paradigm. Using an Artix 7 FPGA, between 2000 and 2990 LUTs were needed, and between 5 to 7 mW (i.e. 50 pJ to 70 pJ per inference) was estimated for running these models. The performance in terms of both accuracy and $jκ$-index is significantly higher compared to previous LGN results. These positive results suggest that one can utilize LGNs and LUTNs for the detection of arrhythmias at extremely low power and high speeds in heart implants or wearable devices, even for patients not included in the training set.

</details>


### [78] [GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2601.11440)
*Francisco Giral,Álvaro Manzano,Ignacio Gómez,Ricardo Vinuesa,Soledad Le Clainche*

Main category: cs.LG

TL;DR: GenDA是一个生成式数据同化框架，利用多尺度图扩散架构从稀疏观测重建城市风场，无需重新训练即可泛化到未见过的几何形状和风况。


<details>
  <summary>Details</summary>
Motivation: 城市风场重建对于空气质量评估、热量扩散和行人舒适度至关重要，但在只有稀疏传感器数据的情况下仍然具有挑战性。现有方法难以处理复杂几何形状和有限观测数据。

Method: 提出GenDA框架，采用基于多尺度图的扩散架构，在CFD模拟数据上训练。将无分类器引导解释为学习后验重建机制：无条件分支学习几何感知流先验，传感器条件分支在采样时注入观测约束。支持稀疏固定传感器和轨迹观测。

Result: 在布里斯托尔真实城市街区的RANS模拟上评估，相比监督图神经网络基线和经典降阶数据同化方法，相对均方根误差降低25-57%，结构相似性指数提高23-33%。

Conclusion: GenDA为复杂环境监测提供了可扩展的生成式、几何感知数据同化路径，能够泛化到未见过的几何形状、风向和网格分辨率。

Abstract: Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism: the unconditional branch learns a geometry-aware flow prior, while the sensor-conditioned branch injects observational constraints during sampling. This formulation enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. We consider both sparse fixed sensors and trajectory-based observations using the same reconstruction procedure. When evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, GenDA reduces the relative root-mean-square error (RRMSE) by 25-57% and increases the structural similarity index (SSIM) by 23-33% across the tested meshes. Experiments are conducted on Reynolds-averaged Navier-Stokes (RANS) simulations of a real urban neighbourhood in Bristol, United Kingdom, at a characteristic Reynolds number of $\mathrm{Re}\approx2\times10^{7}$, featuring complex building geometry and irregular terrain. The proposed framework provides a scalable path toward generative, geometry-aware data assimilation for environmental monitoring in complex domains.

</details>


### [79] [Low-Rank Key Value Attention](https://arxiv.org/abs/2601.11471)
*James O'Neill,Robert Clancy,Mariia Matskevichus,Fergal Reid*

Main category: cs.LG

TL;DR: LRKV是一种通过低秩残差共享KV投影的注意力机制，在保持全token分辨率的同时显著减少KV缓存内存，提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: Transformer预训练面临内存和计算资源限制，其中KV缓存成为训练和自回归解码的主要瓶颈。需要一种既能减少KV缓存内存占用，又能保持模型性能的解决方案。

Method: 提出低秩KV适应（LRKV），在多头注意力中使用共享的全秩KV投影，并添加低秩、头特定的残差项。这种方法在完全共享和完全独立注意力之间提供连续权衡，是标准多头注意力的直接替代方案。

Result: 在2.5B规模实验中，LRKV使用约一半的KV缓存就能超越标准注意力性能，达到相同模型质量时训练计算量减少20-25%。在损失下降速度、验证困惑度和下游任务性能方面均优于标准注意力、MQA/GQA和MLA。

Conclusion: LRKV是一种实用有效的注意力机制，能够在内存和计算受限的情况下扩展Transformer预训练，同时保持注意力头的功能多样性。

Abstract: Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.

</details>


### [80] [Extractive summarization on a CMOS Ising machine](https://arxiv.org/abs/2601.11491)
*Ziqing Zeng,Abhimanyu Kumar,Chris H. Kim,Ulya R. Karpuzcu,Sachin S. Sapatnekar*

Main category: cs.LG

TL;DR: 该论文探索在低功耗CMOS耦合振荡器伊辛机(COBI)上实现McDonald式抽取式摘要，通过硬件感知的伊辛公式化、随机舍入、迭代优化和分解策略，在CNN/DailyMail数据集上实现了3-4.5倍的速度提升和2-3个数量级的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 现代抽取式摘要系统虽然准确率高，但依赖CPU/GPU基础设施，能耗高且不适合资源受限环境的实时推理。作者探索在低功耗CMOS耦合振荡器伊辛机上实现抽取式摘要，以解决能耗和实时性挑战。

Method: 1) 提出硬件感知的伊辛公式化方法，减少局部场和耦合项之间的尺度不平衡，提高系数量化的鲁棒性；2) 开发完整的抽取式摘要流水线，包括随机舍入和迭代优化以补偿精度损失；3) 采用分解策略将大型摘要问题划分为可在COBI上高效求解的较小伊辛子问题。

Result: 在CNN/DailyMail数据集上，COBI实现了3-4.5倍的运行速度提升（与暴力方法相比），能耗降低2-3个数量级，同时保持了有竞争力的摘要质量。性能与软件Tabu搜索相当。

Conclusion: 研究结果表明，CMOS伊辛求解器在边缘设备上实现实时、低能耗的文本摘要具有巨大潜力，为资源受限环境中的高效摘要系统提供了新方向。

Abstract: Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU infrastructures that are energy-intensive and poorly suited for real-time inference in resource-constrained environments. In this work, we explore the feasibility of implementing McDonald-style extractive summarization on a low-power CMOS coupled oscillator-based Ising machine (COBI) that supports integer-valued, all-to-all spin couplings. We first propose a hardware-aware Ising formulation that reduces the scale imbalance between local fields and coupling terms, thereby improving robustness to coefficient quantization: this method can be applied to any problem formulation that requires k of n variables to be chosen. We then develop a complete ES pipeline including (i) stochastic rounding and iterative refinement to compensate for precision loss, and (ii) a decomposition strategy that partitions a large ES problem into smaller Ising subproblems that can be efficiently solved on COBI and later combined. Experimental results on the CNN/DailyMail dataset show that our pipeline can produce high-quality summaries using only integer-coupled Ising hardware with limited precision. COBI achieves 3-4.5x runtime speedups compared to a brute-force method, which is comparable to software Tabu search, and two to three orders of magnitude reductions in energy, while maintaining competitive summary quality. These results highlight the potential of deploying CMOS Ising solvers for real-time, low-energy text summarization on edge devices.

</details>


### [81] [QUPID: A Partitioned Quantum Neural Network for Anomaly Detection in Smart Grid](https://arxiv.org/abs/2601.11500)
*Hoang M. Ngo,Tre' R. Jeter,Jung Taek Seo,My T. Thai*

Main category: cs.LG

TL;DR: 本文提出QUPID和R-QUPID两种量子机器学习模型，用于智能电网异常检测，通过分区量子神经网络提升检测性能和对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 智能电网面临网络物理威胁、系统故障和网络攻击等风险，传统机器学习模型难以处理智能电网的复杂性且易受对抗性攻击，需要更鲁棒的异常检测方法。

Method: 提出QUPID分区量子神经网络，利用量子增强特征表示建模智能电网高维复杂性；扩展为R-QUPID，结合差分隐私增强鲁棒性；分区框架解决量子机器学习可扩展性问题。

Result: 实验结果表明QUPID和R-QUPID在多种场景下显著优于传统机器学习模型，在异常检测能力和鲁棒性方面都有明显提升。

Conclusion: 量子机器学习为智能电网异常检测提供了有效解决方案，QUPID和R-QUPID通过量子增强特征表示和分区框架，实现了高性能、鲁棒且可扩展的异常检测系统。

Abstract: Smart grid infrastructures have revolutionized energy distribution, but their day-to-day operations require robust anomaly detection methods to counter risks associated with cyber-physical threats and system faults potentially caused by natural disasters, equipment malfunctions, and cyber attacks. Conventional machine learning (ML) models are effective in several domains, yet they struggle to represent the complexities observed in smart grid systems. Furthermore, traditional ML models are highly susceptible to adversarial manipulations, making them increasingly unreliable for real-world deployment. Quantum ML (QML) provides a unique advantage, utilizing quantum-enhanced feature representations to model the intricacies of the high-dimensional nature of smart grid systems while demonstrating greater resilience to adversarial manipulation. In this work, we propose QUPID, a partitioned quantum neural network (PQNN) that outperforms traditional state-of-the-art ML models in anomaly detection. We extend our model to R-QUPID that even maintains its performance when including differential privacy (DP) for enhanced robustness. Moreover, our partitioning framework addresses a significant scalability problem in QML by efficiently distributing computational workloads, making quantum-enhanced anomaly detection practical in large-scale smart grid environments. Our experimental results across various scenarios exemplifies the efficacy of QUPID and R-QUPID to significantly improve anomaly detection capabilities and robustness compared to traditional ML approaches.

</details>


### [82] [Building Production-Ready Probes For Gemini](https://arxiv.org/abs/2601.11516)
*János Kramár,Joshua Engels,Zheng Wang,Bilal Chughtai,Rohin Shah,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 本文提出新的激活探针架构以解决长上下文分布偏移问题，提升语言模型滥用检测的泛化能力，并成功部署于Gemini系统。


<details>
  <summary>Details</summary>
Motivation: 随着前沿语言模型能力快速提升，需要更强的滥用缓解措施。现有激活探针技术面临关键挑战：在重要的生产分布偏移下泛化能力不足，特别是从短上下文到长上下文的转换。

Method: 提出多种新的探针架构处理长上下文分布偏移，在网络安全攻击领域评估其鲁棒性，包括多轮对话、静态越狱和自适应红队测试。结合架构选择和多样化分布训练，并配对提示分类器。

Result: 虽然multimax解决了上下文长度问题，但需要架构选择和多样化训练才能实现广泛泛化。探针与提示分类器结合能以低成本实现最优准确率。成功部署于Gemini系统，AlphaEvolve在探针架构搜索和自适应红队测试中显示早期积极结果。

Conclusion: 激活探针是有效的滥用缓解技术，但需要专门处理分布偏移。结合适当架构、多样化训练和提示分类器可实现鲁棒检测。自动化AI安全研究已具备可行性，成功部署证明其实用价值。

Abstract: Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.
  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.
  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [83] [Mass Distribution versus Density Distribution in the Context of Clustering](https://arxiv.org/abs/2601.10759)
*Kai Ming Ting,Ye Zhu,Hang Zhang,Tianrun Liang*

Main category: stat.ML

TL;DR: 该论文提出使用质量分布而非密度分布作为聚类基础，开发了质量最大化聚类算法，克服了传统密度聚类的高密度偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法基于密度分布存在根本性局限——高密度偏差，无论使用何种算法，这种偏差都会阻碍发现任意形状、大小和密度的聚类。现有密度聚类算法虽然尝试缓解这一问题，但密度分布的根本限制仍然存在。

Method: 提出质量最大化聚类算法，以质量分布而非密度分布为基础，最大化所有聚类的总质量。该方法可以轻松调整为最大化总密度，以便对比研究密度分布与质量分布的根本差异。

Result: 质量最大化聚类算法相比密度最大化聚类的主要优势在于，其最大化过程不会偏向密集聚类，从而能够更好地发现各种形状、大小和密度的聚类。

Conclusion: 质量分布是比密度分布更好的数据描述基础，基于质量分布的聚类算法能够克服传统密度聚类的高密度偏差问题，为发现任意形状、大小和密度的聚类提供了更有效的解决方案。

Abstract: This paper investigates two fundamental descriptors of data, i.e., density distribution versus mass distribution, in the context of clustering. Density distribution has been the de facto descriptor of data distribution since the introduction of statistics. We show that density distribution has its fundamental limitation -- high-density bias, irrespective of the algorithms used to perform clustering. Existing density-based clustering algorithms have employed different algorithmic means to counter the effect of the high-density bias with some success, but the fundamental limitation of using density distribution remains an obstacle to discovering clusters of arbitrary shapes, sizes and densities. Using the mass distribution as a better foundation, we propose a new algorithm which maximizes the total mass of all clusters, called mass-maximization clustering (MMC). The algorithm can be easily changed to maximize the total density of all clusters in order to examine the fundamental limitation of using density distribution versus mass distribution. The key advantage of the MMC over the density-maximization clustering is that the maximization is conducted without a bias towards dense clusters.

</details>


### [84] [Memorize Early, Then Query: Inlier-Memorization-Guided Active Outlier Detection](https://arxiv.org/abs/2601.10993)
*Minseo Kang,Seunghwan Park,Dongha Kim*

Main category: stat.ML

TL;DR: IMBoost：通过主动学习增强内点记忆效应，提升无监督异常检测性能


<details>
  <summary>Details</summary>
Motivation: 现有基于内点记忆效应的无监督异常检测方法在内外点分离不明显或异常点形成密集簇时效果不佳，需要更有效的区分机制

Method: 提出IMBoost框架，包含两个阶段：1）预热阶段诱导和促进内点记忆效应；2）极化阶段利用主动查询的样本最大化内外点得分差异，包括新颖的查询策略和定制损失函数

Result: 在多个基准数据集上显著优于最先进的主动异常检测方法，同时计算成本大幅降低

Conclusion: IMBoost通过主动学习增强内点记忆效应，有效解决了无监督异常检测中的挑战，理论分析证明其能持续降低内点风险同时增加异常点风险，实现更好的分离效果

Abstract: Outlier detection (OD) aims to identify abnormal instances, known as outliers or anomalies, by learning typical patterns of normal data, or inliers. Performing OD under an unsupervised regime-without any information about anomalous instances in the training data-is challenging. A recently observed phenomenon, known as the inlier-memorization (IM) effect, where deep generative models (DGMs) tend to memorize inlier patterns during early training, provides a promising signal for distinguishing outliers. However, existing unsupervised approaches that rely solely on the IM effect still struggle when inliers and outliers are not well-separated or when outliers form dense clusters. To address these limitations, we incorporate active learning to selectively acquire informative labels, and propose IMBoost, a novel framework that explicitly reinforces the IM effect to improve outlier detection. Our method consists of two stages: 1) a warm-up phase that induces and promotes the IM effect, and 2) a polarization phase in which actively queried samples are used to maximize the discrepancy between inlier and outlier scores. In particular, we propose a novel query strategy and tailored loss function in the polarization phase to effectively identify informative samples and fully leverage the limited labeling budget. We provide a theoretical analysis showing that the IMBoost consistently decreases inlier risk while increasing outlier risk throughout training, thereby amplifying their separation. Extensive experiments on diverse benchmark datasets demonstrate that IMBoost not only significantly outperforms state-of-the-art active OD methods but also requires substantially less computational cost.

</details>


### [85] [Contextual Distributionally Robust Optimization with Causal and Continuous Structure: An Interpretable and Tractable Approach](https://arxiv.org/abs/2601.11016)
*Fenglin Zhang,Jie Wang*

Main category: stat.ML

TL;DR: 提出基于因果Sinkhorn距离的上下文分布鲁棒优化框架，开发可解释的决策规则和高效算法，在合成和真实数据集上验证性能。


<details>
  <summary>Details</summary>
Motivation: 传统分布鲁棒优化（DRO）通常忽略数据的因果结构和连续性特征，导致决策规则缺乏可解释性且难以处理连续分布。需要开发既能考虑因果一致性又能保持计算可处理性的DRO框架。

Method: 1. 提出因果Sinkhorn距离（CSD），一种熵正则化的因果Wasserstein距离；2. 建立基于CSD的上下文DRO模型（Causal-SDRO）；3. 设计软回归森林（SRF）决策规则，结合决策树可解释性和神经网络可微性；4. 开发随机组合梯度算法，收敛速率O(ε⁻⁴)。

Result: 1. 推导出Causal-SDRO的强对偶重构，最坏分布表现为Gibbs分布的混合；2. SRF能在任意可测函数空间中逼近最优策略，保持全局和局部可解释性；3. 算法收敛速率与标准SGD匹配；4. 在合成和真实数据集上验证了方法的优越性能和可解释性。

Conclusion: 提出的Causal-SDRO框架成功整合了因果结构、分布鲁棒性和可解释性，SRF决策规则和高效算法为实际应用提供了实用工具，在保持理论保证的同时实现了优越性能。

Abstract: In this paper, we introduce a framework for contextual distributionally robust optimization (DRO) that considers the causal and continuous structure of the underlying distribution by developing interpretable and tractable decision rules that prescribe decisions using covariates. We first introduce the causal Sinkhorn discrepancy (CSD), an entropy-regularized causal Wasserstein distance that encourages continuous transport plans while preserving the causal consistency. We then formulate a contextual DRO model with a CSD-based ambiguity set, termed Causal Sinkhorn DRO (Causal-SDRO), and derive its strong dual reformulation where the worst-case distribution is characterized as a mixture of Gibbs distributions. To solve the corresponding infinite-dimensional policy optimization, we propose the Soft Regression Forest (SRF) decision rule, which approximates optimal policies within arbitrary measurable function spaces. The SRF preserves the interpretability of classical decision trees while being fully parametric, differentiable, and Lipschitz smooth, enabling intrinsic interpretation from both global and local perspectives. To solve the Causal-SDRO with parametric decision rules, we develop an efficient stochastic compositional gradient algorithm that converges to an $\varepsilon$-stationary point at a rate of $O(\varepsilon^{-4})$, matching the convergence rate of standard stochastic gradient descent. Finally, we validate our method through numerical experiments on synthetic and real-world datasets, demonstrating its superior performance and interpretability.

</details>


### [86] [Split-and-Conquer: Distributed Factor Modeling for High-Dimensional Matrix-Variate Time Series](https://arxiv.org/abs/2601.11091)
*Hangjin Jiang,Yuzhou Li,Zhaoxing Gao*

Main category: stat.ML

TL;DR: 提出分布式框架处理高维大规模异构矩阵时间序列数据，通过因子模型降维，保持潜在矩阵结构，提高计算效率


<details>
  <summary>Details</summary>
Motivation: 处理高维、大规模、异构的矩阵时间序列数据降维问题，现有分布式方法未能充分利用矩阵结构信息

Method: 数据按列（或行）分区分配到节点服务器，各节点通过二维张量PCA估计行（或列）载荷矩阵，中央服务器聚合局部估计并进行最终PCA，获得全局载荷矩阵估计，进而计算因子矩阵

Result: 提出的框架在计算效率和估计精度方面表现良好，支持未知分组情况下的行/列聚类，可扩展到单位根非平稳矩阵时间序列

Conclusion: 该分布式框架有效处理大规模矩阵时间序列数据，保持矩阵结构，提高计算效率和信息利用率，具有理论和实际应用价值

Abstract: In this paper, we propose a distributed framework for reducing the dimensionality of high-dimensional, large-scale, heterogeneous matrix-variate time series data using a factor model. The data are first partitioned column-wise (or row-wise) and allocated to node servers, where each node estimates the row (or column) loading matrix via two-dimensional tensor PCA. These local estimates are then transmitted to a central server and aggregated, followed by a final PCA step to obtain the global row (or column) loading matrix estimator. Given the estimated loading matrices, the corresponding factor matrices are subsequently computed. Unlike existing distributed approaches, our framework preserves the latent matrix structure, thereby improving computational efficiency and enhancing information utilization. We also discuss row- and column-wise clustering procedures for settings in which the group memberships are unknown. Furthermore, we extend the analysis to unit-root nonstationary matrix-variate time series. Asymptotic properties of the proposed method are derived for the diverging dimension of the data in each computing unit and the sample size $T$. Simulation results assess the computational efficiency and estimation accuracy of the proposed framework, and real data applications further validate its predictive performance.

</details>
