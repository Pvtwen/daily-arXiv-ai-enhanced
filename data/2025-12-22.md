<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 18]
- [cs.LG](#cs.LG) [Total: 67]
- [stat.ML](#stat.ML) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Packet Detection in a Filter Bank-Based Ultra-Wideband Communication System](https://arxiv.org/abs/2512.17055)
*Brian Nelson,Behrouz Farhang-Boroujeny*

Main category: eess.SP

TL;DR: 提出基于FBMC-SS的UWB包检测器设计，包括单无线电带(SRB)和多无线电带(MRB)两种架构，后者通过并行处理降低采样率要求，在-40dB低信噪比下实现可靠检测。


<details>
  <summary>Details</summary>
Motivation: FBMC-SS技术可用于UWB系统，但传统单处理器检测器需要500MHz以上的高速采样率，而低成本ADC/DAC通常限制在200MHz以下，需要更实用的检测器设计。

Method: 基于Rao得分测试开发包检测器，利用FBMC-SS信号结构提出级联信道化检测器(SRB)。为降低采样率要求，扩展为多无线电带(MRB)检测器，使用并行信号处理器在中等采样率下工作。

Result: SRB和MRB检测器在典型UWB信道中性能相同。通过1.28GHz带宽的空中演示，在-40dB极低信噪比和最恶劣环境下，使用IEEE802.15.4标准建议的最长前导码约一半长度即可实现可靠检测。

Conclusion: 提出的MRB检测器通过并行处理解决了高速采样率限制问题，在保持性能的同时实现了更实用的UWB包检测器设计，适用于低成本硬件实现。

Abstract: Recently, filter bank multi-carrier spread spectrum (FBMC-SS) technology has been proposed for use in ultra-wideband (UWB) communication systems. It has been noted that, due to the spectral partitioning properties of the filter banks, a UWB signal can be synthesized and processed using a parallel set of signal processors operating at a moderate rate. This transceiver architecture can be used to generate UWB signals, without requiring a high-rate analog-to-digital and/or digital-to-analog converter. In this paper, beginning with a design operating on a single signal processor, we explore the development of a packet detector using the Rao score test. Taking advantage of the FBMC-SS signal structure, an effective detector design based on a cascade channelizer is proposed. We refer to this design as singe-radio band (SRB) detector. Given the typical bandwidth of UWB systems ($\bf 500$~MHz or wider), the SRB detector has to operate at a fast sampling rate of greater than $\bf 500$~MHz. This may be undesirable, as low cost analog-to-digital (ADC) and digital-to-analog (DAC) converters are often limited to a sampling rate of $\bf 200$~MHz or lower. Taking note of this point, the proposed SRB detector is extended to a multi-radio band (MRB) detector, where a set of parallel signal processors operating at a moderate sampling rate are used for a more practical implementation of the detector. Through computer simulations, we show that SRB and MRB detectors have the same performance in typical UWB channels. Finally, we provide results from an over-the-air demonstration of a UWB design occupying $\bf 1.28$~GHz of bandwidth. We find that reliable detection performance is possible in the harshest environments, at signal-to-noise ratios as low as $\bf -40$~dB with a preamble length of approximately half the duration of longest preamble length recommended in the IEEE802.15.4 standard.

</details>


### [2] [Dirichlet Meets Horvitz and Thompson: Estimating Homophily in Large Networks via Sampling](https://arxiv.org/abs/2512.17084)
*Hamed Ajorlou,Gonzalo Mateos,Luana Ruiz*

Main category: eess.SP

TL;DR: 提出基于采样的框架，通过Dirichlet能量估计网络同质性，使用Horvitz-Thompson估计器从部分图观测中进行无偏推断


<details>
  <summary>Details</summary>
Motivation: 评估大规模网络的同质性对理解图结构规律至关重要，但传统方法需要访问完整网络拓扑和节点特征，这在大型、动态、资源受限或隐私约束的场景中不切实际

Method: 提出基于采样的框架，利用Dirichlet能量（基于拉普拉斯的全变差）估计同质性，采用Horvitz-Thompson估计器进行无偏推断，Dirichlet能量是图上边的平方节点特征偏差之和，可在一般网络采样设计下估计

Result: 实验证明Dirichlet能量可以从采样图中一致估计，提出的HT估计器能可靠地从采样网络测量中捕捉同质结构（或缺乏同质性）

Conclusion: 提出的采样框架能有效估计网络同质性，解决了传统方法在大规模、动态、资源受限或隐私约束场景中的局限性

Abstract: Assessing homophily in large-scale networks is central to understanding structural regularities in graphs, and thus inform the choice of models (such as graph neural networks) adopted to learn from network data. Evaluation of smoothness metrics requires access to the entire network topology and node features, which may be impractical in several large-scale, dynamic, resource-limited, or privacy-constrained settings. In this work, we propose a sampling-based framework to estimate homophily via the Dirichlet energy (Laplacian-based total variation) of graph signals, leveraging the Horvitz-Thompson (HT) estimator for unbiased inference from partial graph observations. The Dirichlet energy is a so-termed total (of squared nodal feature deviations) over graph edges; hence, estimable under general network sampling designs for which edge-inclusion probabilities can be analytically derived and used as weights in the proposed HT estimator. We establish that the Dirichlet energy can be consistently estimated from sampled graphs, and empirically study other heterophily measures as well. Experiments on several heterophilic benchmark datasets demonstrate the effectiveness of the proposed HT estimators in reliably capturing homophilic structure (or lack thereof) from sampled network measurements.

</details>


### [3] [Kalman Filter-based Mobile User-RIS Channel Estimation and User Localization](https://arxiv.org/abs/2512.17112)
*Ju Zhuoxuan,Doroslovacki Milos*

Main category: eess.SP

TL;DR: 提出基于卡尔曼滤波的RIS辅助通信系统信道估计与用户定位方法，包括处理非圆噪声的NCNKF算法和DSFT插值定位技术


<details>
  <summary>Details</summary>
Motivation: 在恶劣环境或信号受阻区域，通信网络中的信道估计和用户定位面临挑战。传统方法难以处理高移动性用户、多天线基站和多RIS元素构成的非线性时变信道模型

Method: 1) 建立用户位置相关的非线性时变信道模型；2) 应用卡尔曼滤波类算法降低信道参数估计的MSE；3) 提出非圆噪声卡尔曼滤波(NCNKF)处理非圆复状态空间噪声；4) 结合离散空间傅里叶变换(DSFT)和插值技术降低基于CSI的用户定位RMSE；5) 将单用户情况扩展到多用户场景

Result: KF在信道估计中比其他已知方法获得更低的MSE；NCNKF在非圆状态空间噪声场景下性能更优；DSFT插值方法复杂度更低且优于其他定位方法

Conclusion: 提出的KF类算法和DSFT插值方法显著提升了RIS辅助通信系统的信道估计和用户定位性能，特别是在恶劣环境和信号受阻区域。数值比较和深入讨论验证了所提方法的性能改进

Abstract: In communication networks, channel estimation and user localization are challenging problems in harsh environments or signal-blocked areas. This paper introduces a novel approach to minimize the Mean Squared Error (MSE) in channel estimation between mobile users and rectangular Reconfigurable Intelligent Surfaces (RIS) within wireless communication systems. Meanwhile, the user localization is realized based on the estimated Channel State Information (CSI). In this paper, we assume a non-linear, user's position-dependent system model, for a user with high mobility, an RIS with multiple elements, and a base station (BS) with multiple antennas. After that, we apply the Kalman Filtering (KF) like algorithms to reduce MSE in estimating parameters of this time-variant channel model. Additionally, we propose a Non-Circular Noise Kalman Filter (NCNKF) to address scenarios with non-circular complex state-space noise. Furthermore, we apply the Discrete Space Fourier Transform (DSFT) method, combined with the interpolation techniques to decrease the Root Mean Squared Error (RMSE) for the user localization based on the estimated CSI. Finally, we extend the single-user case into the multi-user situation. Results show that KF can achieve lower MSE in estimating the channel than other known approaches, while the NCNKF algorithm has better performance in non-circular state-space noise scenarios. At the same time, the DSFT interpolation outperforms the other approaches with lower complexity. The study concludes with numerical comparisons and an in-depth discussion of the performance improvements enabled by our approaches.

</details>


### [4] [Deep Learning-Enabled Multi-Tag Detection in Ambient Backscatter Communications](https://arxiv.org/abs/2512.17125)
*Talha Akyildiz,Hessam Mahdavifar*

Main category: eess.SP

TL;DR: 提出两种深度学习框架解决环境反向散射通信中多标签检测问题，无需完美信道状态信息，性能接近理想LRT基准


<details>
  <summary>Details</summary>
Motivation: 环境反向散射通信中多标签可靠检测面临挑战：强直接链路干扰、微弱反向散射信号、指数级联合状态空间，且难以获取完美信道状态信息

Method: 提出两种深度学习框架：1) EmbedNet端到端原型网络，将接收信号协方差特征直接映射到多标签状态；2) ChanEstNet混合方案，用CNN从导频符号估计有效信道系数，传递给传统LRT进行可解释的多假设检测

Result: 仿真显示所提方法显著降低误码率，紧密跟踪LRT基准，大幅优于能量检测基线，尤其随标签数量增加时优势更明显

Conclusion: 深度学习框架能有效解决环境反向散射通信中的多标签检测问题，无需完美CSI，性能接近理想检测器，为实际部署提供可行方案

Abstract: Ambient backscatter communication (AmBC) enables battery-free connectivity by letting passive tags modulate existing RF signals, but reliable detection of multiple tags is challenging due to strong direct link interference, very weak backscatter signals, and an exponentially large joint state space. Classical multi-hypothesis likelihood ratio tests (LRTs) are optimal for this task when perfect channel state information (CSI) is available, yet in AmBC such CSI is difficult to obtain and track because the RF source is uncooperative and the tags are low-power passive devices. We first derive analytical performance bounds for an LRT receiver with perfect CSI to serve as a benchmark. We then propose two complementary deep learning frameworks that relax the CSI requirement while remaining modulation-agnostic. EmbedNet is an end-to-end prototypical network that maps covariance features of the received signal directly to multi-tag states. ChanEstNet is a hybrid scheme in which a convolutional neural network estimates effective channel coefficients from pilot symbols and passes them to a conventional LRT for interpretable multi-hypothesis detection. Simulations over diverse ambient sources and system configurations show that the proposed methods substantially reduce bit error rate, closely track the LRT benchmark, and significantly outperform energy detection baselines, especially as the number of tags increases.

</details>


### [5] [Deep Reinforcement Learning-Aided Strategies for Big Data Offloading in Vehicular Networks](https://arxiv.org/abs/2512.17133)
*Talha Akyildiz,Hessam Mahdavifar*

Main category: eess.SP

TL;DR: 该论文提出了一种利用V2V链路进行数据上传的车辆网络框架，通过指定领导者车辆接收并去重其他车辆数据，使用深度强化学习优化上传时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 在车辆网络场景中，车辆需要上传大量数据到基站，但直接上传可能效率低下。利用现有的V2V链路，通过数据去重和智能调度可以显著减少上传时间和能耗。

Method: 提出车辆网络框架，指定领导者车辆接收跟随者车辆数据并进行去重处理，然后上传到基站。建立数学模型，分别针对最小化总时间和总能耗制定优化问题，使用深度强化学习在动态网络环境中求解。

Result: 深度强化学习的应用效果显著，数据卸载与去重相结合能大幅减少时间和能耗。数值结果验证了所提方法的有效性，并优于其他替代方法。

Conclusion: 在车辆网络中使用V2V链路进行数据卸载和去重，结合深度强化学习优化调度，能够有效提升数据上传效率，减少时间和能源消耗。

Abstract: We consider vehicular networking scenarios where existing vehicle-to-vehicle (V2V) links can be leveraged for an effective uploading of large-size data to the network. In particular, we consider a group of vehicles where one vehicle can be designated as the \textit{leader} and other \textit{follower} vehicles can offload their data to the leader vehicle or directly upload it to the base station (or a combination of the two). In our proposed framework, the leader vehicle is responsible for receiving the data from other vehicles and processing it in order to remove the redundancy (deduplication) before uploading it to the base station. We present a mathematical framework of the considered network and formulate two separate optimization problems for minimizing (i) total time and (ii) total energy consumption by vehicles for uploading their data to the base station. We employ deep reinforcement learning (DRL) tools to obtain solutions in a dynamic vehicular network where network parameters (e.g., vehicle locations and channel coefficients) vary over time. Our results demonstrate that the application of DRL is highly beneficial, and data offloading with deduplication can significantly reduce the time and energy consumption. Furthermore, we present comprehensive numerical results to validate our findings and compare them with alternative approaches to show the benefits of the proposed DRL methods.

</details>


### [6] [BM4D-PC: nonlocal volumetric denoising of principal components of diffusion-weighted MR images](https://arxiv.org/abs/2512.17138)
*Vinicius P. Campos,Diego Szczupak,Tales Santini,Afonso C. Silva,Alessandro Foi,Marcelo A. C. Vieira,Corey A. Baron*

Main category: eess.SP

TL;DR: 提出BM4D-PC方法，通过结合噪声功率谱密度和主成分分析，有效去除扩散加权MRI中的空间相关噪声，优于现有去噪方法。


<details>
  <summary>Details</summary>
Motivation: 扩散加权MRI中的噪声通常具有空间相关性，现有去噪方法未能充分考虑不同采集和重建策略导致的噪声特性差异。

Method: 提出BM4D-PC方法：1) 结合完整噪声统计信息（包括噪声功率谱密度）；2) 利用BM4D算法；3) 对扩散加权图像进行主成分分析，在主要成分上应用BM4D；4) 可直接估计噪声图和PSD。

Result: 在模拟实验中，BM4D-PC在PSNR、SSIM和RMSE指标上表现最佳；在体内实验中，显著提升原始DWI图像质量，在噪声抑制和细节保留方面优于现有方法，改善扩散指标质量。

Conclusion: BM4D-PC方法在不同采集策略和图像分辨率的dMRI数据上实现了最先进的去噪效果，有望支持神经科学研究进展。

Abstract: Purpose: Noise in diffusion-weighted MRI (dMRI) is often spatially correlated due to different acquisition and reconstruction strategies, which is not fully accounted for in current denoising strategies. Thus, we propose a novel model-based denoising method for dMRI that effectively accounts for the different noise characteristics of data. Methods: We propose a denoising strategy that incorporates full noise statistics, including the noise power spectral density (PSD), by leveraging the BM4D algorithm. Furthermore, to exploit redundancy across the diffusion MRI dataset, BM4D is applied to principal components (PC) of diffusion-weighted images (DWI) obtained through principal component analysis (PCA) decomposition of the entire DWI dataset, an approach we refer to as BM4D-PC. Importantly, our method also allows for direct estimation of both the noise map and PSD. We evaluated BM4D-PC against four existing state-of-the-art methods using in-silico and in vivo datasets, including high-resolution human and marmoset acquisitions. Results: Overall, BM4D-PC presented the best results for the metrics PSNR, SSIM and RMSE on the in-silico experiments. The in-vivo studies also showed that BM4D-PC dramatically enhanced the image quality of raw DWIs, outperforming existing denoising methods in terms of noise suppression and detail preservation, leading to improved quality of diffusion metrics. Conclusion: The proposed BM4D-PC method demonstrated state-of-the-art denoising results for dMRI using datasets from various acquisition strategies and image resolutions, potentially supporting future advances in neuroscience research.

</details>


### [7] [RSMA-Assited and Transceiver-Coordinated ICI Management for MIMO-OFDM System](https://arxiv.org/abs/2512.17171)
*Hengyu Zhang,Xuehan Wang,Xu Shi,Jintao Wang,Zhaohui Yang*

Main category: eess.SP

TL;DR: 提出RSMA辅助的收发端协同传输方案，用于MIMO-OFDM系统中的ICI管理，通过混合SIC架构和混合预编码设计，在高速移动场景下有效抑制干扰并提升系统容量。


<details>
  <summary>Details</summary>
Motivation: 下一代通信系统中高速移动场景日益重要，但MIMO-OFDM技术在此类场景中受到多普勒引起的载波间干扰(ICI)的严重限制。RSMA作为未来通信的关键多址技术，具有优越的干扰管理能力，可用于解决此问题。

Method: 提出RSMA辅助的收发端协同传输方案：接收端设计混合SIC架构，采用动态子载波聚类，实现并行簇内和串行簇间处理；发送端设计匹配的混合预编码，通过和速率最大化问题求解，使用提出的ABC-PSO算法优化模拟相位，结合WMMSE数字预编码迭代。

Result: 仿真结果表明，该方案能够有效抑制ICI，在控制复杂度的同时提升系统容量。

Conclusion: RSMA辅助的收发端协同传输方案为高速移动场景下的MIMO-OFDM系统提供了有效的ICI管理解决方案，平衡了性能与复杂度。

Abstract: High-mobility scenarios are becoming increasingly critical in next-generation communication systems. While multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) stands as a prominent technology, its performance in such scenarios is fundamentally limited by Doppler-induced inter-carrier interference (ICI). Rate splitting multiple access (RSMA), recognized as a key multiple access technique for future communications, demonstrates superior interference management capabilities that we leverage to address this challenge. In specific, we propose a novel RSMA-assisted and transceiver-coordinated transmission scheme for ICI management in MIMO-OFDM system: (1) At the receiver side, we develop a hybrid successive interference cancellation (SIC) architecture with dynamic subcarrier clustering, which enables parallel intra-cluster and serial inter-cluster processing to balance complexity and performance. (2) At the transmitter~side, we design a matched hybrid precoding through formulated sum-rate maximization, solved via our proposed augmented boundary-compressed particle swarm optimization (ABC-PSO) algorithm for analog phase optimization and weighted minimum mean-square error (WMMSE)-based digital precoding iteration. Simulation results show that our scheme brings effective ICI suppression and enhanced system capacity with controlled complexity.

</details>


### [8] [Near-Field Position and Orientation Tracking With Hybrid ELAA Architecture](https://arxiv.org/abs/2512.17274)
*Lin Chen,Xiaojun Yuan,Ying-Jun Angela Zhang*

Main category: eess.SP

TL;DR: 该论文提出了一种预测性模拟合并辅助扩展卡尔曼滤波框架，用于在射频链数量有限的情况下，通过超大规模天线阵列实现移动站的近场位置和方向跟踪。


<details>
  <summary>Details</summary>
Motivation: 在混合阵列架构下，模拟压缩只能部分访问超大规模天线阵列的测量数据，因此需要设计能够在信道不确定性和单位模硬件约束下保留姿态相关信号分量的模拟合并器。

Method: 提出预测性模拟合并辅助扩展卡尔曼滤波框架，利用移动站姿态变化的时间相关性预测性地捕获最具信息量的信号分量；通过贝叶斯克拉美罗界和费舍尔信息矩阵分析性能极限；开发两种低复杂度模拟合并器设计方法。

Result: 数值结果表明，所提出的预测性模拟合并方法显著提高了跟踪精度，即使在射频链更少、发射功率更低的情况下也能实现良好性能。

Conclusion: 该研究为超大规模天线阵列系统中的近场姿态跟踪提供了有效的解决方案，通过预测性模拟合并策略克服了硬件限制，实现了高精度跟踪性能。

Abstract: This paper investigates near-field (NF) position and orientation tracking of a multi-antenna mobile station (MS) using an extremely large antenna array (ELAA)-equipped base station (BS) with a limited number of radio frequency (RF) chains. Under this hybrid array architecture, the received uplink pilot signal at the BS is first combined by analog phase shifters, producing a low-dimensional observation before digital processing. Such analog compression provides only partial access to the ELAA measurement, making it essential to design an analog combiner that can preserve pose-relevant signal components despite channel uncertainty and unit-modulus hardware constraints. To address this, we propose a predictive analog combining-assisted extended Kalman filter (PAC-EKF) framework, where the analog combiner can leverage the temporal correlation in the MS pose variation to capture the most informative signal components predictively. We then analyze fundamental performance limits via Bayesian Cramér-Rao bound and Fisher information matrix, explicitly quantifying how the analog combiner, array size, signal-to-noise ratio, and MS pose influence the pose information contained in the uplink observation. Building on these insights, we develop two methods for designing a low-complexity analog combiner. Numerical results show that the proposed predictive analog combining approach significantly improves tracking accuracy, even with fewer RF chains and lower transmit power.

</details>


### [9] [Near-Field Multi-User Communications via Polar-Domain Beamfocusing: Analytical Framework and Performance Analysis](https://arxiv.org/abs/2512.17283)
*Lin Chen,Ahmed Elzanaty,Mustafa A. Kishk,Ying-Jun Angela Zhang*

Main category: eess.SP

TL;DR: 该论文提出了一个分析性随机几何框架，用于研究近场传播下的多用户系统，通过极域波束聚焦实现空间多址接入，并提出了近场多级天线模式近似来解决角度-距离耦合带来的分析难题。


<details>
  <summary>Details</summary>
Motivation: 随着无线系统向更高频率和极大天线阵列发展，近场传播变得越来越重要。与远场通信相比，近场传播具有球形波前，能够在角度和距离（极域）上实现波束聚焦，为空间多址接入提供了新机会。然而，近场天线模式中角度和距离的内在耦合使得用户间干扰分析变得困难。

Method: 提出了一个分析性随机几何框架，用于捕捉近场传播特性和用户位置的空间随机性。为了处理角度-距离耦合带来的分析难题，提出了近场多级天线模式近似，该近似能够实现计算高效的表达式和关键性能指标的紧上界。

Result: 分析和仿真结果表明，所提出的框架能够准确捕捉性能趋势，并揭示了硬件配置（包括天线数量和射频链数量）与系统性能（在空间资源复用和干扰抑制方面）之间的基本权衡关系。

Conclusion: 该论文开发的随机几何框架为近场多用户系统提供了有效的分析工具，通过极域波束聚焦实现了空间多址接入，提出的NF-MLAP近似解决了分析难题，为系统设计和优化提供了理论指导。

Abstract: As wireless systems evolve toward higher frequencies and extremely large antenna arrays, near-field (NF) propagation becomes increasingly dominant. Unlike far-field (FF) communication, which relies on a planar-wavefront model and is limited to angular-domain beamsteering, NF propagation exhibits spherical wavefronts that enable beamfocusing in both angle and distance, i.e., the polar domain, offering new opportunities for spatial multiple access. This paper develops an analytical stochastic geometry (SG) framework for a multi-user system assisted by polar-domain beamfocusing, which jointly captures NF propagation characteristics and the spatial randomness of user locations. The intrinsic coupling between angle and distance in the NF antenna pattern renders inter-user interference analysis intractable. To address this challenge, we propose a tractable near-field multi-level antenna pattern (NF-MLAP) approximation, which enables computationally efficient expressions and tight upper bounds for key performance metrics, including coverage probability, spectrum efficiency, and area spectrum efficiency. Analytical and simulation results demonstrate that the proposed framework accurately captures performance trends and reveals fundamental trade-offs between hardware configuration (including the number of antennas and radio frequency chains) and system performance (in terms of spatial resource reuse and interference mitigation).

</details>


### [10] [OpenPathNet: An Open-Source RF Multipath Data Generator for AI-Driven Wireless Systems](https://arxiv.org/abs/2512.17286)
*Lizhou Liu,Xiaohui Chen,Wenyi Zhang*

Main category: eess.SP

TL;DR: OpenPathNet是一个开源RF多径数据生成器和数据集，通过高精度射线追踪模拟提供物理一致的多径参数，用于AI驱动的6G无线研究。


<details>
  <summary>Details</summary>
Motivation: 现有RF数据集（如CKMImageNet）主要提供预处理后的图像化信道表示，掩盖了信号传播的细粒度物理特性，而AI与6G融合需要大规模、高保真、可复现的RF数据集。

Method: 采用模块化、参数化的流水线，基于真实世界环境地图进行高精度射线追踪模拟，生成分解的、物理一致的多径参数（包括每路径增益、到达时间、空间角度）。

Result: 开发了OpenPathNet开源生成器和配套数据集，提供可扩展的测试平台，支持信道建模、波束预测、环境感知通信和集成感知等AI驱动的6G研究。

Conclusion: OpenPathNet填补了现有RF数据集的不足，通过提供物理一致的多径参数，为AI驱动的6G系统研究提供了可扩展、透明、可复现的数据基础，代码和数据集已公开。

Abstract: The convergence of artificial intelligence (AI) and sixth-generation (6G) wireless technologies is driving an urgent need for large-scale, high-fidelity, and reproducible radio frequency (RF) datasets. Existing resources, such as CKMImageNet, primarily provide preprocessed and image-based channel representations, which conceal the fine-grained physical characteristics of signal propagation that are essential for effective AI modeling. To bridge this gap, we present OpenPathNet, an open-source RF multipath data generator accompanied by a publicly released dataset for AI-driven wireless research. Distinct from prior datasets, OpenPathNet offers disaggregated and physically consistent multipath parameters, including per-path gain, time of arrival (ToA), and spatial angles, derived from high-precision ray tracing simulations constructed on real-world environment maps. By adopting a modular, parameterized pipeline, OpenPathNet enables reproducible generation of multipath data and can be readily extended to new environments and configurations, improving scalability and transparency. The released generator and accompanying dataset provide an extensible testbed that holds promise for advancing studies on channel modeling, beam prediction, environment-aware communication, and integrated sensing in AI-enabled 6G systems. The source code and dataset are publicly available at https://github.com/liu-lz/OpenPathNet.

</details>


### [11] [Content-Aware RSMA-Enabled Pinching-Antenna Systems for Latency Optimization in 6G Networks](https://arxiv.org/abs/2512.17332)
*Yu Hua,Yaru Fu,Yalin Liu,Zheng Shi,Kevin Hung*

Main category: eess.SP

TL;DR: 提出一种将Pinching天线系统与内容感知速率分割多址接入相结合的传输框架，通过联合优化天线位置和RSMA参数来最小化系统延迟


<details>
  <summary>Details</summary>
Motivation: 6G网络中Pinching天线系统能动态重构无线传播环境，但需要更智能的多用户传输策略来提升性能，特别是减少用户间干扰和功率碎片化

Method: 提出内容感知RSMA方案，让请求相同内容的用户共享统一的私有流；开发CARP-JO算法，将非凸优化问题分解为可通过二分搜索、凸规划和黄金分割搜索解决的子问题

Result: 仿真结果表明，CARP-JO方案在延迟性能上持续优于传统RSMA、NOMA和固定天线系统，验证了物理层可重构性与智能通信策略协同设计的有效性

Conclusion: 通过将Pinching天线系统的物理层可重构性与内容感知RSMA相结合，能显著提升6G网络的多用户性能，特别是在高频段易受严重路径损耗的场景中

Abstract: The Pinching Antenna System (PAS) has emerged as a promising technology to dynamically reconfigure wireless propagation environments in 6G networks. By activating radiating elements at arbitrary positions along a dielectric waveguide, PAS can establish strong line-of-sight (LoS) links with users, significantly enhancing channel gain and deployment flexibility, particularly in high-frequency bands susceptible to severe path loss. To further improve multi-user performance, this paper introduces a novel content-aware transmission framework that integrates PAS with rate-splitting multiple access (RSMA). Unlike conventional RSMA, the proposed RSMA scheme enables users requesting the same content to share a unified private stream, thereby mitigating inter-user interference and reducing power fragmentation. We formulate a joint optimization problem aimed at minimizing the average system latency by dynamically adapting both antenna positioning and RSMA parameters according to channel conditions and user requests. A Content-Aware RSMA and Pinching-antenna Joint Optimization (CARP-JO) algorithm is developed, which decomposes the non-convex problem into tractable subproblems solved via bisection search, convex programming, and golden-section search. Simulation results demonstrate that the proposed CARP-JO scheme consistently outperforms Traditional RSMA, NOMA, and Fixed-antenna systems across diverse network scenarios in terms of latency, underscoring the effectiveness of co-designing physical-layer reconfigurability with intelligent communication strategies.

</details>


### [12] [Active RIS-Aided Anti-Jamming Wireless Communications: A Stackelberg Game Perspective](https://arxiv.org/abs/2512.17335)
*Xiao Tang,Zhen Ma,Bin Li,Cong Li,Qinghe Du,Dusit Niyato,Zhu Han*

Main category: eess.SP

TL;DR: 该论文提出了一种基于主动可重构智能表面的抗干扰通信方案，通过Stackelberg博弈建模合法系统与自适应干扰器之间的策略交互，采用双层优化和块坐标下降框架解决非凸问题，显著提升了合法传输性能并抑制了干扰效果。


<details>
  <summary>Details</summary>
Motivation: 自适应干扰攻击对无线通信的安全性和可靠性构成严重威胁，特别是干扰器能够优化其策略的情况下。传统抗干扰方法难以应对这种智能干扰，需要新的技术框架来保护合法通信。

Method: 将合法系统与自适应干扰器之间的策略交互建模为Stackelberg博弈，合法用户作为领导者设计策略并预测干扰器的最优响应。采用双层优化联合考虑合法发射功率、发射/接收波束成形和主动反射，使用块坐标下降框架通过凸松弛和逐次凸逼近技术迭代求解子问题。

Result: 仿真结果表明，所提出的主动RIS辅助方案在各种场景下相比基线方案，在增强合法传输和抑制干扰效果方面具有显著优越性，证明了Stackelberg均衡的存在性并成功推导出均衡解。

Conclusion: 将主动RIS技术与战略博弈论框架相结合，为抗干扰通信提供了有效解决方案，能够显著提升系统性能并抑制自适应干扰攻击的影响。

Abstract: The pervasive threat of jamming attacks, particularly from adaptive jammers capable of optimizing their strategies, poses a significant challenge to the security and reliability of wireless communications. This paper addresses this issue by investigating anti-jamming communications empowered by an active reconfigurable intelligent surface. The strategic interaction between the legitimate system and the adaptive jammer is modeled as a Stackelberg game, where the legitimate user, acting as the leader, proactively designs its strategy while anticipating the jammer's optimal response. We prove the existence of the Stackelberg equilibrium and derive it using a backward induction method. Particularly, the jammer's optimal strategy is embedded into the leader's problem, resulting in a bi-level optimization that jointly considers legitimate transmit power, transmit/receive beamformers, and active reflection. We tackle this complex, non-convex problem by using a block coordinate descent framework, wherein subproblems are iteratively solved via convex relaxation and successive convex approximation techniques. Simulation results demonstrate the significant superiority of the proposed active RIS-assisted scheme in enhancing legitimate transmissions and degrading jamming effects compared to baseline schemes across various scenarios. These findings highlight the effectiveness of combining active RIS technology with a strategic game-theoretic framework for anti-jamming communications.

</details>


### [13] [SCAR: Semantic Cardiac Adversarial Representation via Spatiotemporal Manifold Optimization in ECG](https://arxiv.org/abs/2512.17423)
*Shunbo Jia,Caizhi Liao*

Main category: eess.SP

TL;DR: 提出SCAR框架，通过语义心脏对抗表示生成针对ECG的通用对抗扰动，绕过临床"人类防火墙"，同时作为数据增强策略和临床教育样本。


<details>
  <summary>Details</summary>
Motivation: 传统ECG对抗攻击存在两个问题：1）标准不可感知噪声约束（如10uV）因心脏波形的高主体间变异性而无法生成有效的通用攻击；2）传统"隐形"攻击容易被临床医生视为技术伪影，无法破坏人机协同诊断流程。

Method: 提出SCAR框架，将时空平滑（W=25，约50ms）、频谱一致性（<15Hz）和解剖幅度约束（<0.2mV）直接集成到梯度优化流形中，生成语义心脏对抗表示。

Result: SCAR在源模型上达到82.46%成功率，在ResNet上保持58.09%的强迁移性，而基线方法在迁移任务上性能崩溃（~16%成功率）。临床分析显示SCAR专门伪造心肌梗死特征（90.2%误诊率），通过数学重建病理性ST段抬高。

Conclusion: SCAR不仅作为混合对抗训练的鲁棒数据增强策略提供最佳临床防御，还提供有效的教育样本，训练临床医生识别低成本、AI靶向的语义伪造。

Abstract: Deep learning models for Electrocardiogram (ECG) analysis have achieved expert-level performance but remain vulnerable to adversarial attacks. However, applying Universal Adversarial Perturbations (UAP) to ECG signals presents a unique challenge: standard imperceptible noise constraints (e.g., 10 uV) fail to generate effective universal attacks due to the high inter-subject variability of cardiac waveforms. Furthermore, traditional "invisible" attacks are easily dismissed by clinicians as technical artifacts, failing to compromise the human-in-the-loop diagnostic pipeline. In this study, we propose SCAR (Semantic Cardiac Adversarial Representation), a novel UAP framework tailored to bypass the clinical "Human Firewall." Unlike traditional approaches, SCAR integrates spatiotemporal smoothing (W=25, approx. 50ms), spectral consistency (<15 Hz), and anatomical amplitude constraints (<0.2 mV) directly into the gradient optimization manifold.
  Results: We benchmarked SCAR against a rigorous baseline (Standard Universal DeepFool with post-hoc physiological filtering). While the baseline suffers a performance collapse (~16% success rate on transfer tasks), SCAR maintains robust transferability (58.09% on ResNet) and achieves 82.46% success on the source model. Crucially, clinical analysis reveals an emergent targeted behavior: SCAR specifically converges to forging Myocardial Infarction features (90.2% misdiagnosis) by mathematically reconstructing pathological ST-segment elevations. Finally, we demonstrate that SCAR serves a dual purpose: it not only functions as a robust data augmentation strategy for Hybrid Adversarial Training, offering optimal clinical defense, but also provides effective educational samples for training clinicians to recognize low-cost, AI-targeted semantic forgeries.

</details>


### [14] [Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions](https://arxiv.org/abs/2512.17473)
*Atharva Awari,Nicolas Gillis,Arnaud Vandaele*

Main category: eess.SP

TL;DR: 提出基于ADMM的非线性矩阵分解算法，支持多种非线性函数和损失函数，在真实数据集上验证了其适用性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵分解通常假设线性关系，但许多实际应用需要非线性变换。现有非线性矩阵分解方法缺乏统一的优化框架，难以处理不同的非线性函数和损失函数。

Method: 基于交替方向乘子法(ADMM)设计优化算法，支持多种非线性函数（ReLU、平方、MinMax变换）和损失函数（最小二乘、ℓ1范数、KL散度），提供灵活的扩展性。

Result: 在真实数据集上验证了算法的适用性、效率和适应性，展示了在非负稀疏数据近似、概率电路表示和推荐系统等领域的应用潜力。

Conclusion: 提出的ADMM框架为非线性矩阵分解提供了统一、灵活的解决方案，能够处理多种非线性变换和损失函数，具有广泛的应用前景。

Abstract: We present an algorithm based on the alternating direction method of multipliers (ADMM) for solving nonlinear matrix decompositions (NMD). Given an input matrix $X \in \mathbb{R}^{m \times n}$ and a factorization rank $r \ll \min(m, n)$, NMD seeks matrices $W \in \mathbb{R}^{m \times r}$ and $H \in \mathbb{R}^{r \times n}$ such that $X \approx f(WH)$, where $f$ is an element-wise nonlinear function. We evaluate our method on several representative nonlinear models: the rectified linear unit activation $f(x) = \max(0, x)$, suitable for nonnegative sparse data approximation, the component-wise square $f(x) = x^2$, applicable to probabilistic circuit representation, and the MinMax transform $f(x) = \min(b, \max(a, x))$, relevant for recommender systems. The proposed framework flexibly supports diverse loss functions, including least squares, $\ell_1$ norm, and the Kullback-Leibler divergence, and can be readily extended to other nonlinearities and metrics. We illustrate the applicability, efficiency, and adaptability of the approach on real-world datasets, highlighting its potential for a broad range of applications.

</details>


### [15] [Sub-6 GHz Beam-Reconfigurable Microfluidic Antenna Using Graphene Liquid for 5G Network](https://arxiv.org/abs/2512.17434)
*Sasmita Dash,Constantinos Psomas,Ioannis Krikidis*

Main category: eess.SP

TL;DR: 提出一种基于石墨烯液体的可重构天线，用于sub-6 GHz通信系统，能够实现360°波束重构，在5.5 GHz频段具有6 dBi增益和24%的宽频带阻抗带宽。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信系统的快速发展，高性能天线对于扩大覆盖范围、提高容量和增强传输质量变得至关重要。石墨烯液体因其低成本、高导电性、柔性和易加工等独特特性，在液体天线研究中展现出优于传统液态金属的性能。

Method: 设计了一种基于石墨烯液体的可重构天线，通过微流控通道控制石墨烯液体的流动来实现波束重构机制。天线能够在360°方向上重新配置波束，并在5.5 GHz频段实现六个特定方向（0°、45°、135°、180°、225°和315°）的波束重构。

Result: 天线在5.5 GHz频段实现了6 dBi的增益和24%的宽频带阻抗带宽。在六种可重构场景下，天线在5.5 GHz频段均能提供稳定的反射系数，波束重构性能良好。

Conclusion: 这种基于石墨烯液体的可重构sub-6 GHz天线设计为下一代无线通信系统提供了有前景的解决方案，具有波束重构能力强、性能稳定等优势。

Abstract: As wireless communication systems continue to grow rapidly, high-performance antennas become increasingly crucial for expanding coverage, improving capacity, and enhancing transmission quality. In light of this, research has focused considerable attention on liquid antennas due to their unique characteristics, which include small size, flexibility, reconfigurability and transparency. Recently, graphene liquid has been explored for numerous applications due to its low cost, high conductivity, flexibility, and ease of processing. Specifically for antenna applications, graphene liquid performs better than conventional liquid metal. This paper presents a graphene-liquid antenna with beam reconfiguration ability for sub-6 GHz communication system. The graphene-liquid movement within the microfluidic channel is taken into consideration by the reconfiguration mechanism. The antenna achieves beam reconfiguration in 360° directions with 6 dBi of gain at 5.5 GHz, featuring a wideband impedance bandwidth of 24%. The antenna main beam is specifically reconfigured into six directions (0°, 45°, 135°, 180°, 225° and 315°) at 5.5 GHz. Additionally, in all six reconfigurable scenarios at 5.5 GHz, the antenna provides a stable reflection coefficient. Therefore, for the next generation of wireless communication systems, this novel design of graphene-liquid-based reconfigurable sub-6 GHz antennas holds promise.

</details>


### [16] [Augmented Affine Frequency Division Multiplexing for Both Low PAPR Signaling and Diversity Gain Protection](https://arxiv.org/abs/2512.17679)
*Zhou Lu,Mohammed El-Hajjar,Lie-liang Yang*

Main category: eess.SP

TL;DR: 提出A²FDM方案，通过替换AFDM中的c₂矩阵为同时执行子块DFT和符号映射的酉矩阵，解决AFDM的PAPR问题，并保持分集增益。


<details>
  <summary>Details</summary>
Motivation: AFDM存在与OFDM相同的PAPR问题，且AFDM中的c₂矩阵在信号传输中是不必要的。需要一种既能降低PAPR又能保持AFDM分集增益的方案。

Method: 提出增强型AFDM方案A²FDM，用新的酉矩阵替换AFDM中的c₂矩阵，该矩阵同时执行子块DFT和符号映射。提出两种符号映射方案：交织映射和局部化映射，分别对应交织A²FDM和局部化A²FDM。

Result: 推导了输入输出关系，分析了复杂度和系统参数对性能的影响。仿真结果表明，A²FDM能够规避AFDM的PAPR问题，并在AFDM处于不利条件导致分集增益损失时，仍能获得可达的分集增益。

Conclusion: A²FDM方案有效解决了AFDM的PAPR问题，同时保持了分集增益性能，在AFDM性能不佳的条件下仍能获得良好的性能表现。

Abstract: Research results on Affine Frequency Division Multiplexing (AFDM) reveal that it experiences the same Peak-to-Average Power Ratio (PAPR) problem as conventional Orthogonal Frequency-Division Multiplexing (OFDM). On the other side, some references and also our studies demonstrate that AFDM involves an unneeded matrix, which is based on a parameter typically represented by $c_2$, for signalling. Hence, in this paper, an augmented AFDM scheme, referred to as A$^2$FDM, is proposed to mitigate the PAPR problem of AFDM, which is achieved by replacing the $c_2$ matrix in AFDM by a new unitary matrix that performs both sub-block-based Discrete Fourier Transform (DFT) and symbol mapping. Two symbol mapping schemes, namely interleaved mapping and localized mapping, are proposed for implementing A$^2$FDM, yielding the Interleaved A$^2$FDM and Localized A$^2$FDM. The input-output relationships of these schemes are derived and the complexity and the effects of system parameters on the performance of A$^2$FDM along with AFDM systems are analyzed. Furthermore, simulation results are provided to demonstrate and compare comprehensively the performance of the considered schemes in conjunction with different system settings and various operational conditions. Our studies and results demonstrate that, while A$^2$FDM is capable of circumventing the PAPR problem faced by AFDM, it is capable of attaining the achievable diversity gain, when AFDM is operated in its undesirable conditions resulting in the loss of the diversity gain available.

</details>


### [17] [Bridging simulation and reality in subsurface radar-based sensing: physics-guided hierarchical domain adaptation with deep adversarial learning](https://arxiv.org/abs/2512.17831)
*Zixin Wang,Ishfaq Aziz,Mohamad Alipour*

Main category: eess.SP

TL;DR: 提出基于物理引导的层次化域适应框架，通过深度对抗学习解决GPR模拟数据与真实数据分布差异问题，实现鲁棒的地下材料特性估计


<details>
  <summary>Details</summary>
Motivation: 地面穿透雷达(GPR)用于地下材料特性估计，但数据驱动方法需要大量标注数据。虽然可用FDTD方法生成合成数据，但模拟数据与真实数据存在分布差异，导致模型在实际场景中表现不佳

Method: 提出物理引导的层次化域适应框架，结合深度对抗学习。通过实验室和现场测试评估单层和双层材料，并与1D CNN和DANN等先进方法进行基准比较

Result: 所提框架在预测与实测参数值之间获得更高的相关系数R和更低的偏差，估计的标准差更小，有效缩小了模拟与真实雷达信号之间的域差距

Conclusion: 该物理引导的层次化域适应框架能够有效桥接模拟与真实GPR信号之间的域差距，实现高效的地下材料特性检索，为野火风险评估和精准农业提供可靠工具

Abstract: Accurate estimation of subsurface material properties, such as soil moisture, is critical for wildfire risk assessment and precision agriculture. Ground-penetrating radar (GPR) is a non-destructive geophysical technique widely used to characterize subsurface conditions. Data-driven parameter estimation methods typically require large amounts of labeled training data, which is expensive to obtain from real-world GPR scans under diverse subsurface conditions. A physics-based GPR model using the finite-difference time-domain (FDTD) method can be employed to generate large synthetic datasets through simulations across varying material parameters, which are then utilized to train data-driven models. A key limitation, however, is that simulated data (source domain) and real-world data (target domain) often follow different distributions, which can cause data-driven models trained on simulations to underperform in real-world scenarios. To address this challenge, this study proposes a novel physics-guided hierarchical domain adaptation framework with deep adversarial learning for robust subsurface material property estimation from GPR signals. The proposed framework is systematically evaluated through the laboratory tests for single- and two-layer materials, as well as the field tests for single- and two-layer materials, and is benchmarked against state-of-the-art methods, including the one-dimensional convolutional neural network (1D CNN) and domain adversarial neural network (DANN). The results demonstrate that the proposed framework achieves higher correlation coefficients R and lower Bias between the predicted and measured parameter values, along with smaller standard deviations in the estimations, thereby validating their effectiveness in bridging the domain gap between simulated and real-world radar signals and enabling efficient subsurface material property retrieval.

</details>


### [18] [Novel Double-Chirp Preamble Design for Multiuser Asynchronous Massive MIMO LoRa Networks](https://arxiv.org/abs/2512.17835)
*The Khai Nguyen,Ebrahim Bedeer*

Main category: eess.SP

TL;DR: 提出用于多用户异步大规模MIMO LoRa网络的新型前导码设计与检测方法，解决传统单啁啾前导码在多用户场景下的前导码相似效应问题


<details>
  <summary>Details</summary>
Motivation: 现有LoRa网络前导码检测仅针对单个终端设备，无法处理多个终端异步传输到多天线网关的场景，且传统单啁啾前导码在多用户环境下存在前导码相似效应，导致无法区分不同终端的前导码

Method: 1) 提出双啁啾前导码设计和前导码分配方法，使每个终端的前导码具有唯一性和可识别性；2) 推导基于最大似然(ML)的双啁啾前导码检测方案；3) 提出低复杂度递归DFT计算技术，减少每个采样周期的计算复杂度

Result: 仿真结果表明，在瑞利衰落信道中，所提设计仅需增加约2dB功率即可将终端数量从1个扩展到15个，同时保持相同的前导码检测错误性能

Conclusion: 提出的双啁啾前导码设计和检测方法有效解决了多用户LoRa网络中的前导码相似效应问题，通过低复杂度递归DFT实现高效检测，显著提升了网络容量

Abstract: This paper proposes a novel preamble design and detection method for multiuser asynchronous massive MIMO LoRa networks. Unlike existing works, which only consider the preamble detection for a single target end devices (ED), we proposed to simultaneously detect the preambles of multiple EDs that asynchronously transmit their uplink (UL) packets to a multiple-antenna gateway (GW). First we show that the preamble detection in multiuser LoRa networks with the conventional single-chirp preamble suffers from the so-called preamble resemblance effect. This means that the preamble of any single ED can resemble the preambles of all EDs in the network, and make it impossible to determine to which ED a preamble belongs. To address this problem, a novel double-chirp preamble design and a preamble assignment method are proposed, which can mitigate the preamble resemblance effect by making the preamble of each ED unique and recognizable. Next, a maximum-likelihood (ML) based detection scheme for the proposed double-chirp preamble is derived. Finally, since the proposed algorithm requires the calculation of the discrete Fourier transform (DFT) every sampling period, we proposed a low-complexity technique to calculate the DFT recursively to reduce the complexity of our proposed design. Simulation shows that the proposed preamble detection design and detection requires just about 2 dB more power to increase the number of EDs from one to 15 in the Rayleigh fading channel while achieving the same preamble detection error performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Dion2: A Simple Method to Shrink Matrix in Muon](https://arxiv.org/abs/2512.16928)
*Kwangjun Ahn,Noah Amsel,John Langford*

Main category: cs.LG

TL;DR: Dion2是一种简化Muon优化器中正交化步骤的方法，通过采样部分行或列来减少计算和通信成本，提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器虽然具有强大的经验性能和理论基础，但其正交化步骤的超线性成本随着规模增加而带来越来越大的开销。现有方法尝试减少进入正交化步骤的矩阵大小，但需要更简单的方法。

Method: Dion2在每次迭代中采样一部分行或列，只对这些采样部分进行正交化。这种采样过程使更新变得稀疏，从而减少计算和通信成本。

Result: 该方法简化了Muon计算中涉及的矩阵缩小过程，相比先前方法更加简单，同时减少了计算和通信开销，提高了Muon的可扩展性。

Conclusion: Dion2提供了一种简单有效的方法来缓解Muon优化器的正交化成本问题，通过稀疏采样策略提高了大规模应用中的可扩展性。

Abstract: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.

</details>


### [20] [BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control](https://arxiv.org/abs/2512.16929)
*Pranesh Sathish Kumar*

Main category: cs.LG

TL;DR: 开发低成本双模式神经肌肉控制系统，结合EEG和EMG信号实现假肢多自由度实时控制，总成本约240美元


<details>
  <summary>Details</summary>
Motivation: 传统上肢假肢控制系统昂贵且不直观，限制了低收入地区截肢者的功能性和可及性，需要开发低成本、直观的控制方案

Method: 集成EEG（NeuroSky MindWave Mobile 2）和EMG（MyoWare 2.0）信号采集，使用ESP32微控制器处理信号：EEG通过眨眼事件分类控制手部开合，EMG通过阈值检测控制肘部运动

Result: 成功构建功能原型，EEG控制4个手指舵机，EMG控制2个肘部舵机，总成本约240美元，实现了低成本、生物直觉的假肢控制

Conclusion: 该系统为资源匮乏地区提供了可行的低成本假肢控制方案，未来可通过3D打印外壳、改进模型和提升舵机扭矩来进一步优化

Abstract: Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.

</details>


### [21] [QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification](https://arxiv.org/abs/2512.16960)
*Bikash K. Behera,Giuseppe Sergioli,Robert Giuntini*

Main category: cs.LG

TL;DR: 量子启发机器学习中，基于核技巧和量子态判别测量的两种方法在合成数据增强场景下均优于经典随机森林，其中PGM在特定编码下表现最佳，KPGM则在不同采样策略下更稳健。


<details>
  <summary>Details</summary>
Motivation: 量子启发机器学习利用量子理论数学框架增强经典算法，但目前缺乏对核技巧和量子态判别测量这两种主要方法的统一理论实证比较。本文旨在系统比较这两种范式在不同数据增强场景下的性能。

Method: 提出统一的理论和实证比较框架，分析核化PGM和直接PGM分类器在合成过采样场景下的性能。使用Quantum SMOTE变体生成数据，评估多种量子副本配置下的分类效果。

Result: 两种量子启发分类器均显著优于经典随机森林基准。PGM在立体编码和n_copies=2时达到最高准确率(0.8512)和F1分数(0.8234)。KPGM在不同QSMOTE变体下表现更稳定，在立体和振幅编码下分别达到0.8511和0.8483。

Conclusion: 量子启发分类器在召回率和平衡性能方面提供实质性提升，并具有互补优势：PGM受益于编码特定增强，KPGM则在采样策略间保持稳健。研究为不同数据特性和计算约束下的方法选择提供实用指导。

Abstract: Quantum-inspired machine learning (QiML) leverages mathematical frameworks from quantum theory to enhance classical algorithms, with particular emphasis on inner product structures in high-dimensional feature spaces. Among the prominent approaches, the Kernel Trick, widely used in support vector machines, provides efficient similarity computation, while the Pretty Good Measurement (PGM), originating from quantum state discrimination, enables classification grounded in Hilbert space geometry. Building on recent developments in kernelized PGM (KPGM) and direct PGM-based classifiers, this work presents a unified theoretical and empirical comparison of these paradigms. We analyze their performance across synthetic oversampling scenarios using Quantum SMOTE (QSMOTE) variants. Experimental results show that both PGM and KPGM classifiers consistently outperform a classical random forest baseline, particularly when multiple quantum copies are employed. Notably, PGM with stereo encoding and n_copies=2 achieves the highest overall accuracy (0.8512) and F1-score (0.8234), while KPGM demonstrates competitive and more stable behavior across QSMOTE variants, with top scores of 0.8511 (stereo) and 0.8483 (amplitude). These findings highlight that quantum-inspired classifiers not only provide tangible gains in recall and balanced performance but also offer complementary strengths: PGM benefits from encoding-specific enhancements, whereas KPGM ensures robustness across sampling strategies. Our results advance the understanding of kernel-based and measurement-based QiML methods, offering practical guidance on their applicability under varying data characteristics and computational constraints.

</details>


### [22] [Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models](https://arxiv.org/abs/2512.16963)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: 提出"压缩即路由"新架构理念，通过Transformer自编码器实现64倍序列压缩，利用重建误差作为内在分布指纹来自动调度专家模块，解决LLM上下文限制、推理成本和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型面临三个主要挑战：上下文长度限制、高推理成本以及持续学习中的灾难性遗忘。虽然混合专家架构缓解了部分冲突，但其路由机制通常依赖显式训练的辅助分类器，增加了系统复杂性且在处理混合领域输入时缺乏可解释性。

Method: 基于"压缩即智能"的前提，提出"压缩即路由"新架构理念。训练了一个8700万参数的端到端Transformer自编码器，实现64倍序列长度压缩（将512个token压缩为8个潜在向量）。利用重建误差作为内在分布指纹，使专家模块能够直接通过重建残差自动调度，无需显式门控网络。

Result: 压缩器表现出极端的领域判别能力：在领域内（代码）验证集上达到99.47%的重建准确率；在半分布外领域（Wiki文本）上准确率急剧下降至47.76%；在完全分布外领域（随机序列）上进一步暴跌至仅0.57%。这种极端且系统的性能差异验证了重建误差作为内在分布指纹的有效性。

Conclusion: 该架构为处理超长上下文提供了"VRAM压缩"的新视角，为下一代可扩展模块化神经网络提供了新的研究视角。基于重建残差的路由机制具有良好的可扩展性，无需显式门控网络，简化了系统复杂性。

Abstract: Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.
  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: \textbf{``Compression is Routing.''} We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a \textbf{64x sequence length compression} (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of \textbf{99.47\%} on the in-domain (code) validation set; accuracy drops sharply to \textbf{47.76\%} on a semi-out-of-distribution domain (Wiki text); and further plummets to just \textbf{0.57\%} on a fully out-of-distribution domain (random sequences).
  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an \textbf{Intrinsic Distribution Fingerprint}. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.

</details>


### [23] [Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes](https://arxiv.org/abs/2512.16967)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 基于XGBoost的轻量级梯度提升框架，利用地面观测数据和物理引导特征工程，实现低能见度和降水事件的短期预测，在多个国际机场验证中显著优于传统TAF预报。


<details>
  <summary>Details</summary>
Motivation: 当前航空天气预报依赖计算密集的数值预报和人工TAF产品，存在保守偏差和时间分辨率有限的问题，需要更高效、准确的短期预测方法保障航空安全和运行效率。

Method: 采用XGBoost梯度提升框架，仅使用地面观测数据（METAR），通过热力学原理进行物理引导的特征工程，在11个不同气候区域的国际机场进行训练和验证。

Result: 模型在3小时战术预测中显著优于传统TAF预报，召回率提高2.5-4.0倍，同时减少误报；SHAP分析显示模型能隐式重建局地物理驱动过程，提供可解释性。

Conclusion: 该轻量级物理引导机器学习框架能有效捕捉局地物理过程，无需人工配置，在短期航空天气预报中具有显著优势，为边缘计算部署提供了可行方案。

Abstract: Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.
  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing

</details>


### [24] [Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs](https://arxiv.org/abs/2512.17008)
*Junbo Li,Peng Zhou,Rui Meng,Meet P. Vadera,Lihong Li,Yang Li*

Main category: cs.LG

TL;DR: 本文提出turn-PPO算法，通过将多轮任务建模为回合级MDP而非令牌级MDP，解决了GRPO在长视野推理任务中的局限性，提升了强化学习在交互式LLM智能体训练中的稳定性和效果。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练交互式LLM智能体方面具有潜力，但现有的GRPO算法在多轮任务中，特别是需要长视野推理的场景下存在明显局限性。需要探索更稳定有效的优势估计策略来应对这些挑战。

Method: 首先探索PPO作为替代方案，发现比GRPO更鲁棒。然后提出turn-PPO变体，基于回合级MDP公式而非常用的令牌级MDP，专门针对多轮场景优化。

Result: 在WebShop和Sokoban数据集上的实验结果表明，turn-PPO无论是否包含长推理组件都表现出有效性，证明了该方法在多轮任务中的优越性。

Conclusion: turn-PPO通过回合级MDP公式有效解决了多轮任务中的强化学习挑战，为交互式LLM智能体在需要长视野推理的复杂环境中的训练提供了更稳定有效的解决方案。

Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.

</details>


### [25] [GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning](https://arxiv.org/abs/2512.17034)
*Chang-Hwan Lee,Chanseung Lee*

Main category: cs.LG

TL;DR: GB-DQN是一种针对非平稳环境的深度强化学习方法，通过梯度提升集成来适应环境变化，每个新学习器学习当前集成在环境变化后的贝尔曼残差。


<details>
  <summary>Details</summary>
Motivation: 非平稳环境（动态或奖励变化）对深度强化学习构成根本挑战，会导致学习到的价值函数失效和灾难性遗忘。现有方法难以快速适应环境变化。

Method: 提出梯度提升深度Q网络（GB-DQN），采用自适应集成方法。不重新训练单个Q网络，而是构建加法集成，每个新学习器被训练来近似当前集成在环境变化后的贝尔曼残差。

Result: 理论分析表明每个提升步骤减少经验贝尔曼残差，集成在标准假设下收敛到变化后的最优价值函数。实验在多种控制任务中显示比DQN和其他非平稳基线方法具有更快的恢复速度、更好的稳定性和更强的鲁棒性。

Conclusion: GB-DQN通过梯度提升集成有效解决了深度强化学习在非平稳环境中的适应问题，提供了理论保证和实证优势。

Abstract: Non-stationary environments pose a fundamental challenge for deep reinforcement learning, as changes in dynamics or rewards invalidate learned value functions and cause catastrophic forgetting. We propose \emph{Gradient-Boosted Deep Q-Networks (GB-DQN)}, an adaptive ensemble method that addresses model drift through incremental residual learning. Instead of retraining a single Q-network, GB-DQN constructs an additive ensemble in which each new learner is trained to approximate the Bellman residual of the current ensemble after drift. We provide theoretical results showing that each boosting step reduces the empirical Bellman residual and that the ensemble converges to the post-drift optimal value function under standard assumptions. Experiments across a diverse set of control tasks with controlled dynamics changes demonstrate faster recovery, improved stability, and greater robustness compared to DQN and common non-stationary baselines.

</details>


### [26] [SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples](https://arxiv.org/abs/2512.17051)
*Haoye Lu,Yaoliang Yu,Darren Ho*

Main category: cs.LG

TL;DR: 提出SFBD-OMNI框架，利用大量噪声样本和少量干净样本恢复真实分布，通过单边熵最优传输和EM算法处理任意测量模型


<details>
  <summary>Details</summary>
Motivation: 现实场景中获取完全观测样本昂贵或不可行，而部分噪声观测相对容易收集，需要从大量噪声样本中恢复真实分布

Method: 将分布恢复任务构建为单边熵最优传输问题，使用EM类算法求解；提出可恢复性测试准则；引入SFBD-OMNI框架，扩展SFBD方法处理任意测量模型

Result: 在基准数据集和多样化测量设置中，方法在定性和定量性能上均有显著提升

Conclusion: SFBD-OMNI能够有效利用大量噪声样本和少量干净样本恢复真实分布，适用于任意测量模型，具有实际应用价值

Abstract: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.

</details>


### [27] [Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs](https://arxiv.org/abs/2512.17131)
*Aaron Defazio,Konstantin Mishchenko,Parameswaran Raman,Hao-Jun Michael Shi,Lin Xiao*

Main category: cs.LG

TL;DR: 提出广义原始平均(GPA)，改进Nesterov方法的原始平均公式，解决单工作者DiLoCo和Schedule-Free等平均优化器的局限性，实现更平滑的迭代平均，减少内存开销并简化超参数调优。


<details>
  <summary>Details</summary>
Motivation: 现有平均优化器如单工作者DiLoCo和Schedule-Free存在局限性：DiLoCo采用周期性平均导致双循环结构，增加内存需求和超参数数量；Schedule-Free维护均匀平均但不够灵活。需要一种更通用、高效的迭代平均方法。

Method: GPA通过解耦Nesterov原始平均公式中的插值常数，实现每步平滑平均迭代。这种方法推广并改进了单工作者DiLoCo，消除了双循环结构，减少内存开销至单个额外缓冲区，简化超参数调优。

Result: 在Llama-160M模型上，GPA相比基准(AdamW)验证损失达到速度提升24.22%；在ImageNet ViT任务中，小批量和大批量设置下分别获得12%和27%的速度提升。理论证明GPA能匹配或超越原始优化器的收敛保证。

Conclusion: GPA是一种通用高效的优化框架，通过解耦插值常数实现平滑迭代平均，在性能和效率上优于现有平均优化器，同时保持理论收敛保证，为深度学习优化提供了新思路。

Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.

</details>


### [28] [Dynamic Tool Dependency Retrieval for Efficient Function Calling](https://arxiv.org/abs/2512.17052)
*Bhrij Patel,Davide Belli,Amir Jalalirad,Maximilian Arnold,Aleksandr Ermovol,Bence Major*

Main category: cs.LG

TL;DR: 提出DTDR方法，通过动态检索工具依赖关系，基于初始查询和演化执行上下文，显著提升函数调用成功率


<details>
  <summary>Details</summary>
Motivation: 现有检索方法依赖静态有限输入，无法捕捉多步工具依赖关系和演化任务上下文，导致引入不相关工具误导代理，降低效率和准确性

Method: 提出动态工具依赖检索(DTDR)，基于函数调用演示建模工具依赖关系，结合初始查询和演化执行上下文进行自适应检索

Result: 相比最先进的静态检索方法，DTDR将函数调用成功率提升23%到104%，并在检索精度、下游任务准确性和计算效率方面表现优异

Conclusion: 动态工具检索能显著改善函数调用代理的性能，通过建模工具依赖关系和适应任务上下文演化，提供更高效准确的外部工具选择

Abstract: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\%$ and $104\%$ compared to state-of-the-art static retrievers.

</details>


### [29] [meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis](https://arxiv.org/abs/2512.17409)
*Dishantkumar Sutariya,Eike Petersen*

Main category: cs.LG

TL;DR: 提出一个用于医学影像模型子组性能差异分析的统计工具箱，解决性能指标选择、不确定性估计、多重比较校正和组合子组发现等挑战。


<details>
  <summary>Details</summary>
Motivation: 在医学影像分析中，按患者和记录属性分层分析模型性能已成为标准做法，能揭示重要模型失败模式。然而，以统计严谨的方式进行此类分析并不简单，需要解决多个统计挑战。

Method: 开发一个统计工具箱，包含：1）选择适合不同样本量和基础率比较的性能指标；2）确定指标不确定性；3）校正多重比较以评估观察差异是否偶然；4）在组合分析中实现机制以找到最"有趣"的子组。

Result: 在ISIC2020皮肤病变恶性分类和MIMIC-CXR胸部X光疾病分类两个案例研究中展示了工具箱的分析能力，验证了其在实际医学影像应用中的有效性。

Conclusion: 该工具箱使从业者能够轻松而严谨地评估模型潜在的子组性能差异，虽然广泛适用，但专门为医学影像应用设计，解决了该领域的关键统计挑战。

Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.

</details>


### [30] [Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III](https://arxiv.org/abs/2512.17058)
*Vladimir G. Pestov*

Main category: cs.LG

TL;DR: 证明了k-最近邻分类器在完备可分度量空间中弱普遍一致性的充要条件，完成了三个等价条件的证明循环


<details>
  <summary>Details</summary>
Motivation: 建立k-最近邻分类器理论基础的完整框架，解决Collins等人(2020)提出的猜想，并修正Kumari和Pestov(2024)中的错误主张

Method: 通过数学证明建立三个条件之间的等价关系：(1)k-最近邻分类器的弱普遍一致性，(2)强Lebesgue-Besicovitch微分性质，(3)Nagata意义下的σ-有限维空间

Result: 证明了(1)⇒(3)的蕴含关系，从而完成了三个条件的等价性证明，确认了k-最近邻分类器理论基础的完整性

Conclusion: k-最近邻分类器在完备可分度量空间中具有弱普遍一致性的充要条件是空间具有Nagata σ-有限维性，这为机器学习理论提供了严格的数学基础

Abstract: We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:
  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.
  The equivalence (2)$\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\Rightarrow$(1) was established by Cérou and Guyader (2006). We prove the implication (1)$\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).

</details>


### [31] [Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation](https://arxiv.org/abs/2512.17073)
*Zhenyu Liu,Yunzhen Liu,Zehao Fan,Garrett Gagnon,Yayue Hou,Nan Wu,Yangwook Kang,Liu Liu*

Main category: cs.LG

TL;DR: 提出BEAMoE方法，通过低秩补偿实现带宽高效的MoE推理，在保持精度的同时减少专家传输开销


<details>
  <summary>Details</summary>
Motivation: MoE模型通过稀疏激活扩展容量，但给内存和带宽带来压力。现有的卸载技术虽然缓解了GPU内存问题，但token级路由导致不规则传输，使推理受限于I/O。静态均匀量化减少了流量，但在激进压缩下会因忽略专家异质性而降低精度。

Method: 提出带宽高效的自适应MoE方法，通过低秩补偿进行路由器引导的精度恢复。在推理时，传输紧凑的低秩因子给每个token的Top-n专家，并对这些专家应用补偿，同时保持其他专家为低比特。该方法与GPU和GPU-NDP系统的卸载技术集成。

Result: 该方法在带宽-精度权衡方面表现优异，并提高了推理吞吐量。

Conclusion: BEAMoE方法通过低秩补偿机制有效解决了MoE模型推理中的带宽瓶颈问题，在保持模型精度的同时显著减少了传输开销，为大规模MoE模型的部署提供了实用的解决方案。

Abstract: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.

</details>


### [32] [A Systems-Theoretic View on the Convergence of Algorithms under Disturbances](https://arxiv.org/abs/2512.17598)
*Guner Dilsad Er,Sebastian Trimpe,Michael Muehlebach*

Main category: cs.LG

TL;DR: 该论文提出了一种统一框架，用于分析算法在噪声、扰动和与其他动态系统互连情况下的稳定性边界和收敛速率。


<details>
  <summary>Details</summary>
Motivation: 算法越来越多地在复杂的物理、社会和工程系统中运行，这些系统存在扰动、噪声以及与其他动态系统的互连。现有分析通常假设算法在隔离环境中运行，缺乏对实际环境中这些干扰因素的系统性分析框架。

Method: 通过利用逆李雅普诺夫定理，推导出量化扰动影响的关键不等式，将孤立算法的收敛保证扩展到存在扰动的情况，系统性地推导稳定性边界和收敛速率。

Result: 建立了统一的扰动分析框架，可应用于多种场景：分布式学习中的通信约束、机器学习泛化的敏感性分析、以及隐私保护中的有意噪声注入等，为算法在噪声和扰动环境下的性能评估提供了理论工具。

Conclusion: 该研究提供了一个统一的工具，用于分析算法在噪声、扰动和与其他动态系统互连情况下的性能，填补了算法分析在实际复杂环境中的理论空白。

Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.

</details>


### [33] [Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?](https://arxiv.org/abs/2512.17079)
*Saraswathy Amjith,Mihika Dusad,Neha Muramalla,Shweta Shah*

Main category: cs.LG

TL;DR: 训练模型识别和纠正推理错误可以提升数学推理的鲁棒性，而不损害标准问题解决能力


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学推理中对早期错误非常脆弱，单个算术错误或不合理推断通常会导致最终答案错误。研究者希望探索通过训练模型识别和恢复错误来提升鲁棒性

Method: 使用MATH-lighteval竞赛级问题，生成包含单一控制错误（计算错误或推理错误）的思维链前缀，使用GRPO和二元最终答案奖励对Qwen3-4B模型进行微调

Result: Mixed-CoT-RL模型在干净问题上与标准RL表现相当（41% vs 41%），但在包含错误推理的问题上显著优于标准RL（24% vs 19%）。仅使用干净数据训练的RL会降低鲁棒性（19% vs 20%）。混合训练效果最佳，推理错误训练比计算错误训练带来更大的鲁棒性提升

Conclusion: 在训练中暴露错误推理痕迹可以改善模型的错误恢复行为而不牺牲准确性，这为构建更鲁棒的数学推理LLMs提供了一条路径

Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.

</details>


### [34] [Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents](https://arxiv.org/abs/2512.17688)
*Paul Mangold,Eloïse Berthier,Eric Moulines*

Main category: cs.LG

TL;DR: 本文首次为联邦SARSA算法提供了理论分析，建立了在异构环境下的收敛保证和复杂度边界，证明了FedSARSA能实现线性加速效果。


<details>
  <summary>Details</summary>
Motivation: 联邦强化学习中的SARSA算法缺乏理论分析，特别是在异构环境（局部转移和奖励不同）下的收敛性保证。需要量化异构性影响并建立通信和样本复杂度边界。

Method: 提出FedSARSA算法，使用线性函数逼近和局部训练。核心贡献是新的多步误差展开分析，精确量化异构性影响，分析多局部更新的收敛性。

Result: 建立了FedSARSA在异构环境下的收敛保证，提供了首个样本和通信复杂度边界。证明FedSARSA能实现相对于智能体数量的线性加速（除高阶马尔可夫采样项外）。数值实验支持理论发现。

Conclusion: FedSARSA在异构联邦强化学习环境中具有理论保证的收敛性，能有效利用多智能体实现加速，为联邦SARSA算法提供了首个完整的理论分析框架。

Abstract: We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.

</details>


### [35] [How to Square Tensor Networks and Circuits Without Squaring Them](https://arxiv.org/abs/2512.17090)
*Lorenzo Loconte,Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: 提出了一种参数化平方电路的方法，通过正交性和确定性条件实现高效边缘化计算，克服了传统平方张量网络在机器学习应用中的计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 平方张量网络及其扩展形式平方电路作为分布估计器虽然表达能力强大且支持闭式边缘化，但平方操作在计算配分函数或边缘化变量时引入了额外的计算复杂度，这限制了它们在机器学习中的应用。

Method: 受张量网络规范形式中的正交性思想和电路中确定性可实现可处理最大化的启发，提出了平方电路的参数化方法。该方法通过参数化平方电路来克服边缘化计算开销，即使在不同于张量网络的因子分解中也能实现高效边缘化。

Result: 实验表明，提出的平方电路条件在分布估计任务中不会损失表达能力，同时实现了更高效的学习过程。

Conclusion: 通过参数化平方电路，成功解决了传统平方张量网络在边缘化计算中的复杂度问题，为机器学习应用提供了更高效的分布估计方法。

Abstract: Squared tensor networks (TNs) and their extension as computational graphs--squared circuits--have been used as expressive distribution estimators, yet supporting closed-form marginalization. However, the squaring operation introduces additional complexity when computing the partition function or marginalizing variables, which hinders their applicability in ML. To solve this issue, canonical forms of TNs are parameterized via unitary matrices to simplify the computation of marginals. However, these canonical forms do not apply to circuits, as they can represent factorizations that do not directly map to a known TN. Inspired by the ideas of orthogonality in canonical forms and determinism in circuits enabling tractable maximization, we show how to parameterize squared circuits to overcome their marginalization overhead. Our parameterizations unlock efficient marginalization even in factorizations different from TNs, but encoded as circuits, whose structure would otherwise make marginalization computationally hard. Finally, our experiments on distribution estimation show how our proposed conditions in squared circuits come with no expressiveness loss, while enabling more efficient learning.

</details>


### [36] [Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting](https://arxiv.org/abs/2512.17696)
*Yuri Calleo*

Main category: cs.LG

TL;DR: 提出了一种空间感知的Transformer架构，通过可学习的协方差核将地统计归纳偏置注入自注意力机制，在保持深度学习灵活性的同时融入物理先验知识。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在计算上难以扩展到大规模传感器网络，而现代Transformer架构虽然擅长序列建模，但缺乏空间几何归纳偏置，将空间传感器视为排列不变的标记，无法理解距离关系。

Method: 提出空间感知Transformer，将自注意力结构分解为平稳物理先验和非平稳数据驱动残差，通过可学习协方差核将地统计归纳偏置直接注入自注意力机制，施加软拓扑约束。

Result: 在合成高斯随机场和真实交通基准测试中优于最先进的图神经网络，实现了"深度变差图"现象，网络能够通过反向传播端到端恢复底层过程的真实空间衰减参数。

Conclusion: 该方法不仅提供更优的预测精度，还能产生良好校准的概率预测，有效弥合了物理感知建模与数据驱动学习之间的差距。

Abstract: The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.

</details>


### [37] [Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making](https://arxiv.org/abs/2512.17091)
*Toshiaki Hori,Jonathan DeCastro,Deepak Gopinath,Avinash Balachandran,Guy Rosman*

Main category: cs.LG

TL;DR: 提出融合强化学习与MPC规划的新方法，通过自适应采样机制提升规划性能和数据效率


<details>
  <summary>Details</summary>
Motivation: 解决具有层次结构的规划问题，结合强化学习和MPC规划的优势，提高规划方法的鲁棒性和适应性

Method: 将强化学习动作信息传递给MPPI采样器，自适应聚合MPPI样本来更新价值估计，在价值估计不确定时进行更多MPPI探索

Result: 在赛车驾驶、改进的Acrobot和添加障碍的Lunar Lander等多个领域验证，相比现有方法成功率最高提升72%，收敛速度加快2.1倍

Conclusion: 该方法能够处理复杂规划问题，易于适应不同应用，在数据效率和整体性能方面优于现有方法

Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.

</details>


### [38] [Mitigating Forgetting in Low Rank Adaptation](https://arxiv.org/abs/2512.17720)
*Joanna Sliwa,Frank Schneider,Philipp Hennig,Jose Miguel Hernandez-Lobato*

Main category: cs.LG

TL;DR: LaLoRA：基于拉普拉斯近化的LoRA权重正则化方法，通过估计参数置信度约束高曲率方向的更新，在保持预训练知识的同时实现高效目标域学习。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调方法（如LoRA）虽然能快速适应下游任务，但会导致模型遗忘预训练阶段获得的领域知识（灾难性遗忘）。需要一种方法在保持预训练知识的同时实现高效的目标域学习。

Method: LaLoRA：权重空间正则化技术，将拉普拉斯近似应用于低秩适配（LoRA）。仅对LoRA权重应用拉普拉斯近似，估计模型对每个参数的置信度，约束高曲率方向的更新。通过正则化强度直接控制学习-遗忘权衡。

Result: 在Llama模型数学推理微调中展示了改进的学习-遗忘权衡。探索了不同损失景观曲率近似方法用于参数置信度估计，分析了拉普拉斯近似所用数据的影响，并研究了超参数鲁棒性。

Conclusion: LaLoRA提供了一种轻量级方法，通过权重空间正则化缓解灾难性遗忘问题，在保持预训练知识的同时实现高效的目标域适应，且学习-遗忘权衡可通过正则化强度直接控制。

Abstract: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.

</details>


### [39] [UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data](https://arxiv.org/abs/2512.17100)
*Justin Li,Efe Sencan,Jasper Zheng Duan,Vitus J. Leung,Stephan Tsaur,Ayse K. Coskun*

Main category: cs.LG

TL;DR: UniCoMTE是一个模型无关的框架，用于为多元时间序列分类器生成反事实解释，通过修改输入样本来识别影响模型预测的关键时间特征，在ECG分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在复杂时间序列分类中表现优异，但其黑盒特性限制了在高风险领域（如医疗保健）的信任和采用，需要提高模型的可解释性。

Method: 提出UniCoMTE框架，通过修改输入样本并评估对模型预测的影响来识别关键时间特征。该框架与多种模型架构兼容，可直接处理原始时间序列数据。

Result: 在ECG时间序列分类器上评估，结果显示UniCoMTE生成的解释比LIME和SHAP等方法更简洁、稳定且与人类认知一致，在清晰度和实用性方面均优于现有方法。

Conclusion: 通过将模型预测与有意义的信号模式联系起来，该框架提升了深度学习模型在现实世界时间序列应用中的可解释性。

Abstract: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.

</details>


### [40] [Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow](https://arxiv.org/abs/2512.17878)
*Herlock Rahimi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Wasserstein-Fisher-Rao几何的采样方法，通过引入显式修正项和加权随机微分方程，改善扩散模型在非凸或多模态分布中的采样效率。


<details>
  <summary>Details</summary>
Motivation: 基于分数的扩散模型在处理非凸或多模态分布（如双井势能）时，混合速率会指数级恶化。传统扩散动力学在非对数凹目标分布上的探索能力有限，需要开发改进的采样方案。

Method: 利用信息几何工具，通过Wasserstein-Fisher-Rao几何增强扩散采样器，引入显式修正项，并使用Feynman-Kac表示实现加权随机微分方程。

Result: 建立了WFR-based采样动力学的初步但严谨的理论框架，阐明了其几何和算子理论结构，为未来理论和算法发展奠定了基础。

Conclusion: Wasserstein-Fisher-Rao几何为改进扩散采样提供了有前景的方向，通过结合样本空间传输和概率测度空间的重整化动力学，有望解决非凸分布采样中的探索问题。

Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.

</details>


### [41] [Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models](https://arxiv.org/abs/2512.17107)
*Zenan Yang,Yuanliang Li,Jingwei Zhang,Yongjie Liu,Kun Ding*

Main category: cs.LG

TL;DR: 提出基于可微分快速故障仿真模型(DFFSM)的光伏串故障量化方法，使用Adahessian优化器实现梯度故障参数识别(GFPI)，准确量化部分阴影、短路和串联电阻退化故障。


<details>
  <summary>Details</summary>
Motivation: 现有光伏阵列故障量化方法存在效率有限和可解释性不足的问题，需要开发更高效、可解释的故障诊断方法来支持光伏系统的可靠运行和智能维护。

Method: 提出可微分快速故障仿真模型(DFFSM)准确建模多故障下的I-V特性并提供故障参数解析梯度，基于此开发使用Adahessian优化器的梯度故障参数识别(GFPI)方法。

Result: 在模拟和实测I-V曲线上的实验结果表明，GFPI方法对不同故障均实现高量化精度，I-V重构误差低于3%，验证了可微分物理模拟器在光伏系统故障诊断中的可行性。

Conclusion: 基于可微分物理模拟器的故障量化方法为光伏系统故障诊断提供了高效、准确的解决方案，证明了DFFSM和GFPI在光伏阵列故障诊断中的有效性和实用性。

Abstract: Accurate fault diagnosis and quantification are essential for the reliable operation and intelligent maintenance of photovoltaic (PV) arrays. However, existing fault quantification methods often suffer from limited efficiency and interpretability. To address these challenges, this paper proposes a novel fault quantification approach for PV strings based on a differentiable fast fault simulation model (DFFSM). The proposed DFFSM accurately models I-V characteristics under multiple faults and provides analytical gradients with respect to fault parameters. Leveraging this property, a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer is developed to efficiently quantify partial shading, short-circuit, and series-resistance degradation. Experimental results on both simulated and measured I-V curves demonstrate that the proposed GFPI achieves high quantification accuracy across different faults, with the I-V reconstruction error below 3%, confirming the feasibility and effectiveness of the application of differentiable physical simulators for PV system fault diagnosis.

</details>


### [42] [Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space](https://arxiv.org/abs/2512.17884)
*Xinyue Yu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 提出一种基于正则化随机傅里叶特征和有限元重构映射的方法，用于从噪声数据中学习算子，具有噪声鲁棒性和计算效率


<details>
  <summary>Details</summary>
Motivation: 核基算子学习方法虽然准确且理论完备，但对于大规模训练集计算代价高，且对噪声敏感。需要一种既能保持准确性又能提高计算效率、增强噪声鲁棒性的方法。

Method: 提出RRFF-FEM方法：使用多元t分布采样的随机特征，结合频率加权的Tikhonov正则化抑制高频噪声，并采用有限元重构映射。当特征数N与训练样本数m满足N∝mlogm时，系统条件良好。

Result: 建立了随机特征矩阵极端奇异值的高概率界，证明了系统的良好条件性。在多个PDE基准问题上的实验表明，该方法对噪声鲁棒，相比无正则化随机特征模型性能提升且训练时间减少，与核方法和神经算子相比保持竞争性精度。

Conclusion: RRFF-FEM方法为从噪声数据中学习算子提供了一种计算高效、噪声鲁棒的解决方案，在保持准确性的同时显著提升了计算效率。

Abstract: Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.

</details>


### [43] [Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse](https://arxiv.org/abs/2512.17108)
*Kunjal Panchal,Saayan Mitra,Somdeb Sarkhel,Haoliang Wang,Ishita Dasgupta,Gang Wu,Hui Guan*

Main category: cs.LG

TL;DR: Atom是一个在移动设备上高效执行视频-语言多阶段流水线的系统，通过分解大模型为可重用模块并跨子任务复用，减少模型重复加载，实现并行执行，在智能手机上比非复用基线快27-33%


<details>
  <summary>Details</summary>
Motivation: 视频-语言模型在移动设备上执行多阶段流水线时面临挑战，包括冗余的模型加载和碎片化执行，导致效率低下

Method: 将十亿参数模型分解为可重用模块（如视觉编码器和语言解码器），在字幕生成、推理和索引等子任务间复用这些模块，消除重复模型加载，实现并行执行

Result: 在商用智能手机上，Atom比非复用基线快27-33%，性能下降很小（检索任务Recall@1下降≤2.3，字幕生成CIDEr下降≤1.5）

Conclusion: Atom为边缘设备上的高效视频-语言理解提供了一个实用、可扩展的解决方案

Abstract: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\leq$ 2.3 Recall@1 in retrieval, $\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.

</details>


### [44] [Bridging Training and Merging Through Momentum-Aware Optimization](https://arxiv.org/abs/2512.17109)
*Alireza Moayedikia,Alicia Troncoso*

Main category: cs.LG

TL;DR: 提出统一框架，在训练时维护因子化动量和曲率统计信息，然后重用这些信息进行几何感知的模型组合，避免重复计算并实现更原则的模型合并。


<details>
  <summary>Details</summary>
Motivation: 当前工作流程在训练时计算曲率信息，然后丢弃，再为模型合并重新计算类似信息，浪费计算资源并丢弃有价值的轨迹数据。需要统一训练和合并的框架。

Method: 引入统一框架，在训练期间维护因子化动量和曲率统计信息，然后重用这些信息进行几何感知的模型组合。该方法积累任务显著性分数，实现无需后验Fisher计算的曲率感知合并。

Result: 在自然语言理解基准测试中，曲率感知参数选择在所有稀疏度水平上都优于仅基于幅度的基线，多任务合并优于强基线。框架表现出秩不变收敛性和优于现有低秩优化器的超参数鲁棒性。

Conclusion: 通过将优化轨迹视为可重用资产而非丢弃，该方法消除了冗余计算，同时实现了更原则的模型组合，统一了训练和合并过程。

Abstract: Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.

</details>


### [45] [Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts](https://arxiv.org/abs/2512.17111)
*Anjali Sarawgi,Esteban Garces Arias,Christof Zotter*

Main category: cs.LG

TL;DR: 首个针对古尼泊尔语的端到端手写文本识别系统，采用编码器-解码器架构，在低资源条件下实现4.9%的字符错误率


<details>
  <summary>Details</summary>
Motivation: 古尼泊尔语作为具有历史意义但资源匮乏的语言，缺乏有效的手写文本识别系统，需要开发专门的技术来应对低资源挑战

Method: 采用行级转录方法，系统探索编码器-解码器架构和数据中心化技术，实现解码策略并分析令牌级混淆以理解模型行为

Result: 最佳模型达到4.9%的字符错误率，虽然评估数据集保密，但公开了训练代码、模型配置和评估脚本以支持进一步研究

Conclusion: 成功开发了首个古尼泊尔语端到端手写文本识别系统，为低资源历史文字识别提供了可行方案，并开源工具促进该领域研究

Abstract: This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.

</details>


### [46] [The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining](https://arxiv.org/abs/2512.17121)
*Jasmine Vu,Shivanand Sheshappanavar*

Main category: cs.LG

TL;DR: 该研究评估了CLIP模型在医学影像中处理否定语句的能力，通过微调方法改进模型对否定临床语言的检索准确性，并分析了模型内部行为变化。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉语言模型在医学影像任务中应用广泛，但存在处理否定语句能力不足的问题，这在医学诊断中尤其关键，因为否定描述（如"无肺炎"）在临床报告中很常见。

Method: 使用Stanford AIMI CheXagent模型评估胸部X光图像检索能力，对比含否定和不含否定的提示词效果。采用微调方法改进模型，并通过token归因、t-SNE投影和注意力头消融分析模型内部行为。

Result: 微调后模型处理否定语句的能力有所提升，但正提示词的准确性略有下降。通过内部行为分析发现，微调方法重塑了文本编码器对否定临床语言的表示方式。

Conclusion: 该研究深入理解了CLIP模型内部行为，通过微调方法改善了模型处理否定临床语言的能力，有助于提高医学AI设备的可靠性，为医疗应用中的视觉语言模型优化提供了方向。

Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.

</details>


### [47] [DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations](https://arxiv.org/abs/2512.17129)
*Seong Ho Pahng,Guoye Guan,Benjamin Fefferman,Sahand Hormoz*

Main category: cs.LG

TL;DR: DiffeoMorph：一个端到端可微分框架，通过学习形态发生协议，引导智能体群体变形为目标3D形状，使用注意力SE(3)-等变图神经网络和基于3D Zernike多项式的形状匹配损失。


<details>
  <summary>Details</summary>
Motivation: 生物系统通过相同智能体的集体行为形成复杂三维结构，这种分布式控制如何产生精确全局模式是发育生物学、分布式机器人、可编程物质和多智能体学习的核心问题。

Method: 使用注意力SE(3)-等变图神经网络，每个智能体根据自身内部状态和接收信号更新位置和状态；引入基于3D Zernike多项式的形状匹配损失，通过隐式微分计算对齐步骤梯度，形成双层优化问题。

Result: 系统基准测试显示形状匹配损失优于其他标准距离度量；DiffeoMorph能够仅使用最小空间线索形成从简单椭球体到复杂形态的各种形状。

Conclusion: DiffeoMorph为学习分布式形态发生协议提供了一个有效的可微分框架，能够实现精确的三维形状形成，在生物发育和分布式系统中有广泛应用前景。

Abstract: Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.

</details>


### [48] [Distributed Learning in Markovian Restless Bandits over Interference Graphs for Stable Spectrum Sharing](https://arxiv.org/abs/2512.17161)
*Liad Lea Didi,Kobi Cohen*

Main category: cs.LG

TL;DR: 提出SMILE算法，在通信受限的无线网络中实现分布式频谱共享，结合多对一匹配稳定性和未知马尔可夫信道学习，达到最优稳定分配和对数遗憾。


<details>
  <summary>Details</summary>
Motivation: 研究多个认知通信实体（如小区、子网络或认知无线电用户）在通信受限无线网络中的分布式频谱接入与共享问题。网络由干扰图建模，目标是实现全局稳定且干扰感知的信道分配。现有工作未能在随机、时变的非稳态环境中建立全局Gale-Shapley稳定性。

Method: 提出SMILE（Stable Multi-matching with Interference-aware LEarning）算法，将非稳态多臂老虎机学习与图约束协调相结合。算法让小区分布式地平衡未知信道的探索和已学信息的利用，通过通信高效的分布式学习实现信道分配。

Result: 证明SMILE收敛到最优稳定分配，并相对于完全了解期望效用的先知实现对数遗憾。仿真验证了理论保证，展示了算法在多种频谱共享场景下的鲁棒性、可扩展性和效率。

Conclusion: SMILE是首个在随机、时变的非稳态环境中为信道分配建立全局Gale-Shapley稳定性的工作，通过结合分布式学习和图约束协调，实现了通信高效的频谱共享。

Abstract: We study distributed learning for spectrum access and sharing among multiple cognitive communication entities, such as cells, subnetworks, or cognitive radio users (collectively referred to as cells), in communication-constrained wireless networks modeled by interference graphs. Our goal is to achieve a globally stable and interference-aware channel allocation. Stability is defined through a generalized Gale-Shapley multi-to-one matching, a well-established solution concept in wireless resource allocation. We consider wireless networks where L cells share S orthogonal channels and cannot simultaneously use the same channel as their neighbors. Each channel evolves as an unknown restless Markov process with cell-dependent rewards, making this the first work to establish global Gale-Shapley stability for channel allocation in a stochastic, temporally varying restless environment. To address this challenge, we develop SMILE (Stable Multi-matching with Interference-aware LEarning), a communication-efficient distributed learning algorithm that integrates restless bandit learning with graph-constrained coordination. SMILE enables cells to distributedly balance exploration of unknown channels with exploitation of learned information. We prove that SMILE converges to the optimal stable allocation and achieves logarithmic regret relative to a genie with full knowledge of expected utilities. Simulations validate the theoretical guarantees and demonstrate SMILE's robustness, scalability, and efficiency across diverse spectrum-sharing scenarios.

</details>


### [49] [BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions](https://arxiv.org/abs/2512.17198)
*Shao-Ting Chiu,Ioannis G. Kevrekidis,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: BumpNet是一种基于无网格基函数展开的稀疏神经网络框架，用于PDE数值解和算子学习，通过可训练的sigmoid基函数实现高效训练和动态剪枝。


<details>
  <summary>Details</summary>
Motivation: 传统径向基函数(RBF)网络在PDE求解中存在训练效率问题，需要一种能够利用现代神经网络训练技术、具有自适应性和稀疏性的框架。

Method: 使用sigmoid激活函数构建基函数，所有参数（形状、位置、幅度）完全可训练，通过动态剪枝实现模型简约和h-自适应。结合PINNs、EDNNs和DeepONet分别形成Bump-PINNs、Bump-EDNN和Bump-DeepONet。

Result: 数值实验表明该架构在PDE求解和算子学习方面具有高效性和准确性，能够有效处理一般PDE、时间演化PDE和PDE算子学习问题。

Conclusion: BumpNet是一个通用的稀疏神经网络框架，通过可训练的sigmoid基函数和动态剪枝机制，能够与现有神经网络架构有效结合，为PDE数值解和算子学习提供了高效准确的解决方案。

Abstract: We introduce BumpNet, a sparse neural network framework for PDE numerical solution and operator learning. BumpNet is based on meshless basis function expansion, in a similar fashion to radial-basis function (RBF) networks. Unlike RBF networks, the basis functions in BumpNet are constructed from ordinary sigmoid activation functions. This enables the efficient use of modern training techniques optimized for such networks. All parameters of the basis functions, including shape, location, and amplitude, are fully trainable. Model parsimony and h-adaptivity are effectively achieved through dynamically pruning basis functions during training. BumpNet is a general framework that can be combined with existing neural architectures for learning PDE solutions: here, we propose Bump-PINNs (BumpNet with physics-informed neural networks) for solving general PDEs; Bump-EDNN (BumpNet with evolutionary deep neural networks) to solve time-evolution PDEs; and Bump-DeepONet (BumpNet with deep operator networks) for PDE operator learning. Bump-PINNs are trained using the same collocation-based approach used by PINNs, Bump-EDNN uses a BumpNet only in the spatial domain and uses EDNNs to advance the solution in time, while Bump-DeepONets employ a BumpNet regression network as the trunk network of a DeepONet. Extensive numerical experiments demonstrate the efficiency and accuracy of the proposed architecture.

</details>


### [50] [Learning solution operator of dynamical systems with diffusion maps kernel ridge regression](https://arxiv.org/abs/2512.17203)
*Jiwoo Song,Daning Huang,John Harlim*

Main category: cs.LG

TL;DR: DM-KRR方法通过扩散映射核结合动态感知验证策略，在复杂动力系统的长期预测中优于现有方法，强调了几何结构对预测性能的关键作用。


<details>
  <summary>Details</summary>
Motivation: 许多科学和工程系统表现出复杂的非线性动力学，长期预测困难。现有数据驱动模型在几何结构未知或表示不佳时性能下降，需要一种能适应系统内在几何结构的方法。

Method: 提出扩散映射核岭回归(DM-KRR)：结合核岭回归框架与从扩散映射导出的数据驱动核，采用动态感知验证策略，无需显式流形重建或吸引子建模。

Result: 在多种系统（光滑流形、混沌吸引子、高维时空流）中，DM-KRR在准确性和数据效率上均优于最先进的随机特征、神经网络和算子学习方法。

Conclusion: 长期预测能力不仅取决于模型表达能力，更关键的是通过动态一致的模型选择尊重数据中的几何约束。简单性、几何意识和强大性能为复杂动力系统的可靠高效学习提供了有前景的路径。

Abstract: Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.

</details>


### [51] [Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods](https://arxiv.org/abs/2512.17257)
*Iason Kyriakopoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 该研究系统评估了五种时间序列预测模型在不同时间尺度（分钟、小时、天）和空间聚合水平（充电站、区域、城市）上对电动汽车充电需求的预测效果，使用四个真实世界数据集进行对比分析。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及以应对气候变化，其对电网管理的影响引发关注，预测充电需求成为重要研究问题。现有研究缺乏对不同时间尺度和空间聚合水平下多种预测方法的系统比较。

Method: 使用五种时间序列预测模型（传统统计方法、机器学习和深度学习方法），在四个公开真实世界数据集上，评估短、中、长期时间尺度（分钟、小时、天）和不同空间聚合水平（充电站、区域、城市级）的预测性能。

Result: 研究首次系统评估了电动汽车充电需求预测在广泛时间尺度和空间聚合水平上的表现，为不同应用场景下的模型选择提供了实证依据。

Conclusion: 该工作填补了电动汽车充电需求预测研究中的空白，为电网管理和充电基础设施规划提供了重要的方法论参考和实证基础。

Abstract: With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.

</details>


### [52] [SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS](https://arxiv.org/abs/2512.17262)
*Suraj Kumar,Arvind Kumar,Soumi Chattopadhyay*

Main category: cs.LG

TL;DR: SHARP-QoS是一个用于联合QoS预测的统一框架，通过双曲卷积提取层次特征、自适应特征共享机制和EMA损失平衡策略，解决了现有方法中的负迁移和表示学习不足问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的QoS数据极其稀疏、噪声大且具有层次依赖关系，现有方法要么单独预测每个QoS参数（计算成本高、泛化差），要么联合预测但存在负迁移问题（由于QoS参数数值范围不一致导致损失缩放问题）和表示学习不足。

Method: 1. 使用双曲卷积在庞加莱球中提取QoS和上下文结构的层次特征；2. 自适应特征共享机制，允许信息丰富的QoS和上下文信号之间的特征交换，使用门控特征融合模块支持结构和共享表示之间的动态特征选择；3. EMA-based损失平衡策略，实现稳定的联合优化，减轻负迁移。

Result: 在包含2、3、4个QoS参数的三个数据集上评估，SHARP-QoS优于单任务和多任务基线模型。广泛研究表明，该模型有效解决了稀疏性、对异常值的鲁棒性和冷启动等主要挑战，同时保持适度的计算开销。

Conclusion: SHARP-QoS通过统一的联合QoS预测策略，成功解决了现有方法的局限性，实现了可靠的服务质量预测，为依赖服务导向计算提供了有效的解决方案。

Abstract: Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincaré ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.

</details>


### [53] [A Theoretical Analysis of State Similarity Between Markov Decision Processes](https://arxiv.org/abs/2512.17265)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出了广义双模拟度量（GBSM），用于衡量任意马尔可夫决策过程（MDP）对之间的状态相似性，解决了传统双模拟度量在多MDP场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 双模拟度量（BSM）在分析单个MDP内的状态相似性方面很有效，但在多个MDP之间的应用存在挑战。先前工作尝试将BSM扩展到MDP对，但缺乏完善的数学性质限制了进一步的理论分析。

Method: 建立了广义双模拟度量（GBSM），严格证明了其三个基本度量性质：GBSM对称性、MDP间三角不等式和相同空间上的距离界限。利用这些性质，理论分析了跨MDP的策略迁移、状态聚合和基于采样的估计。

Result: GBSM提供了比标准BSM更严格的理论界限，并为估计提供了闭式样本复杂度，改进了基于BSM的现有渐近结果。数值结果验证了理论发现并展示了GBSM在多MDP场景中的有效性。

Conclusion: GBSM为跨MDP的状态相似性度量提供了坚实的理论基础，具有更优的理论界限和实际应用价值，推动了多MDP场景下的强化学习分析。

Abstract: The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.

</details>


### [54] [Understanding Generalization in Role-Playing Models via Information Theory](https://arxiv.org/abs/2512.17270)
*Yongqi Li,Hao Lang,Fei Huang,Tieyun Qian,Yongbin Li*

Main category: cs.LG

TL;DR: 本文提出R-EMID信息论指标来量化角色扮演模型在分布偏移下的性能退化，并开发强化学习框架提升对话响应概率估计，发现用户偏移是最大风险因素。


<details>
  <summary>Details</summary>
Motivation: 角色扮演模型在实际应用中性能下降，主要由于用户、角色和对话组合的分布偏移。现有方法如LLM-as-a-judge无法提供细粒度诊断，缺乏正式框架来表征RPM的泛化行为。

Method: 提出基于推理的有效互信息差异(R-EMID)指标来可解释地测量RPM性能退化；推导R-EMID上界预测最坏情况泛化性能；提出协同演化强化学习框架自适应建模用户、角色和对话上下文连接，改进对话响应生成概率估计。

Result: 评估显示用户偏移在所有偏移中风险最高，强化学习是增强RPM泛化的最有效方法。R-EMID能够有效量化不同分布偏移对RPM性能的影响。

Conclusion: R-EMID为诊断角色扮演模型泛化问题提供了理论框架和实用指标，协同演化强化学习框架能有效提升对话响应概率估计，有助于改进RPM在实际部署中的性能。

Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.

</details>


### [55] [MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics](https://arxiv.org/abs/2512.17273)
*Farinaz Mostajeran,Aruzhan Tleubek,Salah A Faroughi*

Main category: cs.LG

TL;DR: MINPO是一个统一框架，用于建模由长程空间相互作用和/或长期时间记忆引起的非局部动力学，通过神经表示直接学习非局部算子及其逆，并显式重构未知解场。


<details>
  <summary>Details</summary>
Motivation: 许多物理系统表现出由积分微分方程描述的非局部时空行为。传统方法需要重复评估卷积积分，成本随核复杂性和维度快速增加。现有神经求解器可以加速特定计算，但无法泛化到不同的非局部结构。

Method: MINPO使用KANs或MLPs作为编码器，直接学习非局部算子及其逆的神经表示，然后显式重构未知解场。学习过程通过轻量级非局部一致性损失项来保证学习到的算子与重构解之间的一致性。

Result: MINPO在准确性方面表现出色，并在处理(i)多样核类型、(ii)不同核维度和(iii)重复核积分评估带来的计算需求方面展示了鲁棒性。与经典方法和基于MLP的先进神经策略相比具有优势。

Conclusion: MINPO超越了问题特定的公式，为受非局部算子支配的系统提供了一个统一框架，能够自然捕捉并高效解决由广泛IDE谱及其子集（包括分数PDE）支配的非局部时空依赖关系。

Abstract: Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.

</details>


### [56] [Alzheimer's Disease Brain Network Mining](https://arxiv.org/abs/2512.17276)
*Alireza Moayedikia,Sara Fin*

Main category: cs.LG

TL;DR: MATCH-AD是一个半监督学习框架，通过整合深度表示学习、基于图的标签传播和最优传输理论，在只有三分之一数据有标签的情况下实现了近乎完美的阿尔茨海默病诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病诊断面临临床评估昂贵且侵入性的挑战，导致神经影像数据集中只有部分样本有真实标签。需要开发能够利用大量未标注数据的半监督方法。

Method: 提出MATCH-AD框架，整合深度表示学习、基于图的标签传播和最优传输理论。利用神经影像数据的流形结构将诊断信息从有限标注样本传播到大量未标注样本，使用Wasserstein距离量化认知状态间的疾病进展。

Result: 在近5000名国家阿尔茨海默病协调中心受试者上评估，涵盖结构MRI、脑脊液生物标志物和临床变量。尽管只有不到三分之一的受试者有真实标签，但实现了近乎完美的诊断准确率，显著优于所有基线方法。

Conclusion: 有原则的半监督学习可以释放全球积累的部分标注神经影像数据的诊断潜力，显著减少标注负担，同时保持适合临床部署的准确性。

Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.

</details>


### [57] [M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge](https://arxiv.org/abs/2512.17299)
*Abdullah M. Zyarah,Dhireesha Kudithipudi*

Main category: cs.LG

TL;DR: M2RU是一种混合信号架构，实现了minion循环单元，用于边缘设备上的高效时序处理和片上持续学习，相比CMOS数字设计能效提升29倍。


<details>
  <summary>Details</summary>
Motivation: 边缘平台上的持续学习面临挑战，因为循环网络需要高能耗的训练过程和频繁的数据传输，这在嵌入式部署中不切实际。

Method: 采用混合信号架构实现minion循环单元，集成加权比特流技术（使多比特数字输入无需高分辨率转换即可在交叉阵列中处理）和经验回放机制（稳定域偏移下的学习）。

Result: M2RU在48.62mW功耗下实现15GOPS，对应312GOPS/W，在顺序MNIST和CIFAR-10任务上保持与软件基线5%以内的精度，相比CMOS数字设计能效提升29倍，持续学习工作负载下预期运行寿命12.2年。

Conclusion: M2RU为边缘级时序智能中的实时自适应提供了一个可扩展且高能效的平台。

Abstract: Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.

</details>


### [58] [Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability](https://arxiv.org/abs/2512.17316)
*Michael Merry,Pat Riddle,Jim Warren*

Main category: cs.LG

TL;DR: 提出一个基于图论的全局适用标准来评估模型的内在可解释性，区分"可解释"与"已解释"模型，并以临床使用的PREDICT心血管疾病风险模型为例验证该标准


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能领域缺乏一致的内在可解释性定义和测试标准，现有工作要么依赖指标度量，要么依赖直觉判断，需要建立严谨的评估框架

Method: 使用图论表示和分解模型，形成结构局部解释作为可验证的假设-证据结构注释，然后重新组合成全局解释，建立可解释性评估标准

Result: 提出的标准与现有内在可解释性直觉相符，能够解释为什么大型回归模型可能不可解释而稀疏神经网络可能可解释，并成功应用于临床使用的PREDICT心血管疾病风险模型

Conclusion: 该工作为可解释性研究提供了结构化框架，为监管机构提供了灵活而严谨的合规测试标准，并证明了PREDICT模型具有内在可解释性

Abstract: Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - "we know it when we see it". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.

</details>


### [59] [Task Schema and Binding: A Double Dissociation Study of In-Context Learning](https://arxiv.org/abs/2512.17325)
*Chaeha Kim*

Main category: cs.LG

TL;DR: 研究发现上下文学习可分解为任务模式识别和绑定两个独立机制，前者通过后期MLP补丁100%传递，后者通过残差流补丁62%传递，且机制适用于包括Mamba在内的多种架构。


<details>
  <summary>Details</summary>
Motivation: 现有研究将上下文学习视为单一机制（检索式、梯度下降式或贝叶斯式），缺乏对其内部机制的因果验证。本文旨在通过实验验证上下文学习是否可分解为独立的神经机制。

Method: 使用激活补丁实验，测试了9个模型（来自7个Transformer家族和Mamba，参数量370M-13B）。通过晚期MLP补丁和残差流补丁分别验证任务模式和绑定机制的可分离性。

Result: 1. 双重分离：任务模式通过晚期MLP补丁100%传递，绑定通过残差流补丁62%传递；2. 先验-模式权衡：模式依赖与先验知识负相关；3. 架构普适性：机制适用于所有测试架构包括非Transformer的Mamba。

Conclusion: 上下文学习由任务模式和绑定两个可分离的神经机制组成，而非单一机制。模型在先验知识缺乏时依赖任务模式，但先验知识通过注意力误路由干扰而非直接输出竞争。这解释了为什么任意映射成功而事实覆盖失败，瓶颈在于注意力层面而非输出层面。

Abstract: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:
  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms
  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)
  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba
  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.

</details>


### [60] [Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs](https://arxiv.org/abs/2512.17352)
*Ivan Kralj,Lodovico Giaretta,Gordan Ježić,Ivana Podnar Žarko,Šarūnas Girdzijauskas*

Main category: cs.LG

TL;DR: 提出自适应剪枝算法减少ST-GNN在边缘计算中的通信开销，同时引入SEPA指标评估交通事件预测能力


<details>
  <summary>Details</summary>
Motivation: ST-GNN处理智能交通系统高频数据时，在分布式边缘节点部署会产生大量通信开销，特别是相邻节点间重复传输重叠特征

Method: 1) 自适应剪枝算法动态过滤冗余邻居特征，基于近期模型性能调整剪枝率；2) 提出SEPA指标专门评估交通减速和恢复事件的预测能力

Result: 在PeMS-BAY和PeMSD7-M数据集上，自适应剪枝算法在保持预测精度的同时显著降低通信成本，SEPA指标揭示了空间连接性对动态不规则交通预测的真正价值

Conclusion: 自适应剪枝算法可在不损害关键交通事件响应能力的前提下降低通信开销，SEPA指标比标准误差指标更能反映模型对交通事件的预测能力

Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.

</details>


### [61] [Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach](https://arxiv.org/abs/2512.17367)
*Yidong Chai,Yi Liu,Mohammadreza Ebrahimi,Weifeng Li,Balaji Padmanabhan*

Main category: cs.LG

TL;DR: 提出LLM-SGA框架和ARHOCD检测器，通过样本生成聚合和集成学习提升有害内容检测的对抗鲁棒性和准确性


<details>
  <summary>Details</summary>
Motivation: 社交媒体有害内容检测模型易受对抗攻击，现有方法难以同时保证对抗鲁棒性和检测准确性

Method: 1) LLM-SGA框架利用文本对抗攻击的不变性特征；2) ARHOCD检测器包含集成检测器、动态权重分配和对抗训练策略三个创新组件

Result: 在仇恨言论、谣言和极端主义内容三个数据集上，ARHOCD展现出强大的对抗鲁棒性和检测准确性提升

Conclusion: 提出的框架和检测器能有效提升社交媒体有害内容检测的对抗鲁棒性，解决了现有研究的局限性

Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.

</details>


### [62] [AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens](https://arxiv.org/abs/2512.17375)
*Tung-Ling Li,Yuhao Wu,Hongliang Liu*

Main category: cs.LG

TL;DR: 论文发现奖励模型和LLM-as-a-Judge系统存在漏洞：低困惑度的控制token序列可以翻转二元评估结果，导致高误报率。作者提出AdvJudge-Zero方法发现这些控制token，并展示对抗训练可以缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 奖励模型和LLM-as-a-Judge系统在现代后训练流程（如RLHF、DPO、RLAIF）中至关重要，它们提供标量反馈和二元决策来指导模型选择和基于RL的微调。然而这些评判系统存在一个反复出现的漏洞：短序列的低困惑度控制token可以翻转许多二元评估结果。

Method: 提出AdvJudge-Zero方法，利用模型的下一token分布和beam-search探索从头发现多样化的控制token序列。分析显示诱导的隐藏状态扰动集中在低秩"软模式"中，与评判系统的拒绝方向反对齐。

Result: 这些控制token在大型开放权重和专用评判模型对数学和推理基准的错误答案进行评分时，会导致非常高的误报率。实验表明，基于LoRA的对抗训练在少量控制token增强的示例上可以显著减少这些误报，同时保持评估质量。

Conclusion: LLM-as-a-Judge系统存在实际的风险漏洞，控制token序列可以操纵评估结果。对抗训练是缓解此问题的有效方法，需要在后训练流程中考虑这种奖励攻击风险。

Abstract: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.

</details>


### [63] [DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference](https://arxiv.org/abs/2512.17398)
*Yonathan Bornfeld,Shai Avidan*

Main category: cs.LG

TL;DR: 本文提出了一种新的私有推理激活模块，通过原型通道机制大幅减少DReLU操作数量，在保持隐私的同时提升计算效率，在分类和分割任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 私有推理(PI)中ReLU计算是主要计算瓶颈，现有方法致力于减少ReLU数量。本文发现一个DReLU可以为多个ReLU操作服务，因此寻求更高效的激活机制来减少DReLU操作。

Method: 提出新的激活模块：只在原型通道子集上执行DReLU操作，而复制通道从对应的原型通道神经元复制DReLU结果。将此思想扩展到不同层之间，大幅减少DReLU操作数量。

Result: 在resnet类型网络中显著减少DReLU操作；理论分析显示新方法能用单一非线性和两个神经元解决扩展XOR问题；在多个分类任务上达到新的SOTA结果，在图像分割任务上达到SOTA结果。

Conclusion: 提出的原型通道激活模块能有效减少私有推理中的DReLU操作数量，同时保持模型性能，在理论和实验上都证明了其有效性，为隐私保护机器学习提供了更高效的解决方案。

Abstract: Private Inference (PI) uses cryptographic primitives to perform privacy preserving machine learning. In this setting, the owner of the network runs inference on the data of the client without learning anything about the data and without revealing any information about the model. It has been observed that a major computational bottleneck of PI is the calculation of the gate (i.e., ReLU), so a considerable amount of effort have been devoted to reducing the number of ReLUs in a given network.
  We focus on the DReLU, which is the non-linear step function of the ReLU and show that one DReLU can serve many ReLU operations. We suggest a new activation module where the DReLU operation is only performed on a subset of the channels (Prototype channels), while the rest of the channels (replicate channels) replicates the DReLU of each of their neurons from the corresponding neurons in one of the prototype channels. We then extend this idea to work across different layers.
  We show that this formulation can drastically reduce the number of DReLU operations in resnet type network. Furthermore, our theoretical analysis shows that this new formulation can solve an extended version of the XOR problem, using just one non-linearity and two neurons, something that traditional formulations and some PI specific methods cannot achieve. We achieve new SOTA results on several classification setups, and achieve SOTA results on image segmentation.

</details>


### [64] [Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.17444)
*Javier Gonzalez-Ruiz,Carlos Rodriguez-Pardo,Iacopo Savelli,Alice Di Bella,Massimo Tavoni*

Main category: cs.LG

TL;DR: 提出基于多智能体强化学习的电力市场模型，用于评估长期电力市场机制对脱碳转型的影响，应用于意大利电力系统案例研究。


<details>
  <summary>Details</summary>
Motivation: 电力系统是实现碳中和的关键，但缺乏先进工具来支持政策制定者设计和评估长期电力市场机制，特别是在多种政策和市场机制相互作用的环境中。

Method: 采用多智能体强化学习模型，发电公司作为利润最大化智能体在批发电力市场中进行投资决策，使用独立近端策略优化算法，并通过广泛超参数搜索确保分散训练产生符合竞争行为的结果。

Result: 应用于意大利电力系统的案例研究表明，市场设计对电力部门脱碳和避免价格波动具有关键作用，框架能够评估多种政策和市场机制同时相互作用的环境。

Conclusion: 该框架为政策制定者和利益相关者提供了设计和测试长期电力市场机制的有效工具，能够捕捉脱碳能源系统的关键特征，支持市场参与者对脱碳路径的响应和适应。

Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.

</details>


### [65] [Learning What to Write: Write-Gated KV for Efficient Long-Context Inference](https://arxiv.org/abs/2512.17452)
*Yen-Chieh Huang,Rui Fang,Ming-Syan Chen,Pi-Cheng Hsiu*

Main category: cs.LG

TL;DR: WG-KV提出一种轻量级KV缓存管理机制，通过学习预测token效用，在写入缓存前过滤低效用状态，显著减少内存使用并提升推理速度


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理面临二次注意力复杂度和线性KV缓存增长的瓶颈。现有方法通过事后选择或淘汰来缓解，但忽略了根本低效问题：不加区分地写入持久内存

Method: 将KV缓存管理形式化为包含KV准入、选择和淘汰三个原语的因果系统。通过Write-Gated KV机制实例化KV准入，这是一种轻量级机制，学习在token进入缓存前预测其效用，通过过滤低效用状态来维护紧凑的全局缓存和滑动本地缓存

Result: 在Llama模型上，内存使用减少46-57%，预填充速度提升3.03-3.45倍，解码速度提升1.89-2.56倍，精度损失可忽略，且与FlashAttention和分页KV系统兼容

Conclusion: 学习"写什么"是实现高效长上下文推理的原则性和实用方法，Write-Gated KV通过早期过滤低效用状态有效解决了KV缓存管理问题

Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

</details>


### [66] [A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting](https://arxiv.org/abs/2512.17453)
*Henok Tenaw Moges,Deshendran Moodley*

Main category: cs.LG

TL;DR: Lite-STGNN是一个轻量级时空图神经网络，用于长期多变量预测，通过分解式时间建模和可学习稀疏图结构，在保持高效的同时达到SOTA精度。


<details>
  <summary>Details</summary>
Motivation: 解决长期多变量时间序列预测中传统方法参数过多、计算复杂的问题，同时保持高精度和可解释性。

Method: 结合趋势-季节性分解的时间模块和可学习稀疏图结构的空间模块，使用低秩Top-K邻接学习和保守的horizon-wise门控机制。

Result: 在四个基准数据集上达到最先进精度（最长720步预测），参数效率高，训练速度比基于Transformer的方法快得多。空间模块比时间基线提升4.6%，Top-K增强局部性3.3%。

Conclusion: Lite-STGNN提供了一个紧凑、可解释且高效的长期多变量时间序列预测框架。

Abstract: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.

</details>


### [67] [Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study](https://arxiv.org/abs/2512.17477)
*Shubham Das,Kaushal Singhania,Amit Sadhu,Suprabhat Das,Arghya Nandi*

Main category: cs.LG

TL;DR: 使用深度学习替代模型（BiLSTM-VAE和BiLSTM-Transformer）快速预测Inconel 625高温合金的蠕变行为，相比传统ANSYS模拟将计算时间从30-40分钟缩短到秒级。


<details>
  <summary>Details</summary>
Motivation: Inconel 625等高温合金的蠕变行为对航空航天和能源系统组件的长期可靠性至关重要。传统有限元模拟（如ANSYS）计算成本高昂，单次10,000小时模拟需要数十分钟，限制了设计优化和结构健康监测的效率。

Method: 使用ANSYS基于Norton定律生成蠕变应变数据（单轴应力50-150 MPa，温度700-1000°C）。训练两种深度学习架构：1) BiLSTM变分自编码器（BiLSTM-VAE）用于不确定性感知和生成式预测；2) BiLSTM-Transformer混合模型，利用自注意力机制捕捉长期时间行为。两种模型都作为替代预测器。

Result: BiLSTM-VAE提供稳定可靠的蠕变应变预测，BiLSTM-Transformer在整个时间范围内实现高确定性精度。性能评估使用RMSE、MAE和R²指标。延迟测试显示显著加速：ANSYS模拟每个应力-温度条件需要30-40分钟，而替代模型在几秒内完成预测。

Conclusion: 提出的深度学习框架为高温合金应用提供了快速蠕变评估的解决方案，支持设计优化和结构健康监测，相比传统模拟方法实现了数量级的计算加速。

Abstract: Time-dependent deformation, particularly creep, in high-temperature alloys such as Inconel 625 is a key factor in the long-term reliability of components used in aerospace and energy systems. Although Inconel 625 shows excellent creep resistance, finite-element creep simulations in tools such as ANSYS remain computationally expensive, often requiring tens of minutes for a single 10,000-hour run. This work proposes deep learning based surrogate models to provide fast and accurate replacements for such simulations. Creep strain data was generated in ANSYS using the Norton law under uniaxial stresses of 50 to 150 MPa and temperatures of 700 to 1000 $^\circ$C, and this temporal dataset was used to train two architectures: a BiLSTM Variational Autoencoder for uncertainty-aware and generative predictions, and a BiLSTM Transformer hybrid that employs self-attention to capture long-range temporal behavior. Both models act as surrogate predictors, with the BiLSTM-VAE offering probabilistic output and the BiLSTM-Transformer delivering high deterministic accuracy. Performance is evaluated using RMSE, MAE, and $R^2$. Results show that the BiLSTM-VAE provides stable and reliable creep strain forecasts, while the BiLSTM-Transformer achieves strong accuracy across the full time range. Latency tests indicate substantial speedup: while each ANSYS simulation requires 30 to 40 minutes for a given stress-temperature condition, the surrogate models produce predictions within seconds. The proposed framework enables rapid creep assessment for design optimization and structural health monitoring, and provides a scalable solution for high-temperature alloy applications.

</details>


### [68] [SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals](https://arxiv.org/abs/2512.17527)
*Muhammad Haris Khan*

Main category: cs.LG

TL;DR: SafeBench-Seq：一个用于蛋白质序列危害筛查的基准测试和基线分类器，基于公开数据构建，采用同源性控制评估，可在普通CPU上运行。


<details>
  <summary>Details</summary>
Motivation: 蛋白质设计基础模型存在生物安全风险，但社区缺乏简单、可复现的序列级危害筛查基准，需要在同源性控制下评估且能在普通CPU上运行。

Method: 使用公开数据（SafeProtein危害序列和UniProt良性序列），提取可解释特征（全局物理化学描述符和氨基酸组成），通过≤40%同源性聚类进行簇级留出验证，采用校准分类器并评估概率质量。

Result: 同源性聚类评估显著低估了随机分割评估的鲁棒性；校准线性模型表现出较好的校准性，而树集成模型Brier/ECE略高；系统仅发布元数据（不发布危险序列），实现严格评估。

Conclusion: SafeBench-Seq提供了一个CPU专用、可复现的蛋白质序列危害筛查基准，通过同源性控制评估和元数据发布机制，为生物安全风险评估提供了可靠工具。

Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.

</details>


### [69] [NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks](https://arxiv.org/abs/2512.17531)
*Salar Beigzad*

Main category: cs.LG

TL;DR: CFF算法通过层间协作机制扩展Forward-Forward算法，解决传统实现中层间隔离问题，提升深度学习性能


<details>
  <summary>Details</summary>
Motivation: 传统Forward-Forward算法存在层间隔离问题，各层独立优化goodness函数，缺乏集体学习动态，限制了表示协调和深层架构的收敛效率

Method: 提出协作式Forward-Forward学习框架，包含两种协作范式：固定CFF（恒定层间耦合）和自适应CFF（可学习协作参数），通过加权所有层的贡献实现协调特征学习

Result: 在MNIST和Fashion-MNIST数据集上评估显示，相比基线Forward-Forward实现有显著性能提升

Conclusion: 层间协作是Forward-Forward学习的基本增强机制，对神经形态计算架构和能源受限AI系统具有直接应用价值

Abstract: The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.

</details>


### [70] [Bayesian Optimisation: Which Constraints Matter?](https://arxiv.org/abs/2512.17569)
*Xietao Wang Lin,Juan Ungredda,Max Butler,James Town,Alma Rahat,Hemant Singh,Juergen Branke*

Main category: cs.LG

TL;DR: 提出贝叶斯优化的新变体，针对具有解耦黑盒约束的问题，通过知识梯度采集函数，仅评估相关约束以提高优化效率


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在处理昂贵全局黑盒优化问题时表现出色，但对于具有解耦约束的问题，现有方法不够高效。通常只有少数约束在最优解处是起作用的，因此需要仅评估相关约束以提高优化效率

Method: 提出基于知识梯度采集函数的贝叶斯优化新变体，专门处理解耦黑盒约束问题。方法能够识别哪些约束可能在最优解处起作用，从而选择性地评估相关约束，避免不必要的约束函数评估

Result: 通过实证基准测试，新方法在性能上优于现有最先进方法，证明了其在处理解耦约束优化问题上的优越性

Conclusion: 提出的贝叶斯优化变体能够有效处理具有解耦黑盒约束的优化问题，通过选择性评估相关约束显著提高优化效率，在实验中表现出优于现有方法的性能

Abstract: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.

</details>


### [71] [GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping](https://arxiv.org/abs/2512.17570)
*Yikang Yue,Yishu Yin,Xuehai Qian*

Main category: cs.LG

TL;DR: GreedySnake是一种新的SSD卸载训练系统，采用垂直调度策略，相比水平调度能获得更高的训练吞吐量，特别是在小批量大小下表现更佳。


<details>
  <summary>Details</summary>
Motivation: SSD卸载训练为LLM训练提供了一种经济实用的方法，但现有系统存在I/O瓶颈，特别是在小批量大小下训练吞吐量不足。

Method: 提出GreedySnake系统，采用垂直调度策略（逐层执行所有微批次），并重叠优化步骤与下一轮前向传播以缓解I/O瓶颈。

Result: 在A100 GPU上，GreedySnake相比ZeRO-Infinity获得显著提升：GPT-65B在1GPU上提升1.96倍，4GPU上提升1.93倍；GPT-175B在1GPU上提升2.53倍。

Conclusion: GreedySnake通过垂直调度和优化步骤重叠，有效提升了SSD卸载训练的效率，使系统更接近roofline模型的理想性能。

Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake

</details>


### [72] [Machine Learning for Static and Single-Event Dynamic Complex Network Analysis](https://arxiv.org/abs/2512.17577)
*Nikolaos Nakis*

Main category: cs.LG

TL;DR: 开发用于静态和单事件动态网络的图表示学习新算法，基于潜在空间模型，创建结构感知的网络表示，实现统一的嵌入学习过程。


<details>
  <summary>Details</summary>
Motivation: 当前图表示学习方法通常需要启发式方法和多阶段处理，缺乏统一的框架。需要开发能够同时捕捉网络结构特征（如同质性、传递性、平衡理论）并处理多种图分析任务的综合方法。

Method: 基于潜在空间模型家族，特别是潜在距离模型，开发结构感知的网络表示方法。这些方法能够生成层次化的网络结构表达、社区特征、极端节点识别和时间网络影响动态量化，采用统一的学习过程，避免启发式和多阶段处理。

Result: 提出了能够同时处理静态和单事件动态网络的统一图表示学习框架，能够捕捉网络结构特征，实现层次化表达、社区分析、极端节点识别和时间动态量化。

Conclusion: 通过潜在空间模型开发了统一、全面的网络嵌入方法，能够有效表征网络结构并处理多样化的图分析任务，为图表示学习提供了新的算法框架。

Abstract: The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.

</details>


### [73] [Learning Safe Autonomous Driving Policies Using Predictive Safety Representations](https://arxiv.org/abs/2512.17586)
*Mahesh Keswani,Raunak Bhattacharyya*

Main category: cs.LG

TL;DR: SRPL框架在真实世界自动驾驶场景中验证有效，能提升奖励-安全权衡，改善成功率并降低成本，但对策略优化器和数据分布敏感，且能增强对观测噪声的鲁棒性和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习在自动驾驶中面临效率与安全的根本性矛盾：过于保守的策略限制驾驶效率，而激进探索则带来安全风险。SRPL框架通过预测未来约束违规模型来解决这一挑战，但此前主要在受控环境中验证，需要研究其在真实世界自动驾驶场景中的有效性。

Method: 在Waymo Open Motion Dataset和NuPlan数据集上进行系统实验，评估SRPL框架在真实世界自动驾驶场景中的表现。研究包括奖励-安全权衡、成功率、成本降低等指标，并分析其对策略优化器和数据分布的依赖性，以及对观测噪声的鲁棒性和跨数据集泛化能力。

Result: SRPL能显著改善奖励-安全权衡，在成功率（效应量r=0.65-0.86）和成本降低（效应量r=0.70-0.83）方面取得统计学显著改进（p<0.05）。但其效果依赖于底层策略优化器和数据集分布。预测安全表征能提升对观测噪声的鲁棒性，在零样本跨数据集评估中，SRPL增强的智能体展现出比非SRPL方法更好的泛化能力。

Conclusion: 预测安全表征有潜力增强自动驾驶中的安全强化学习，但实际部署需要考虑策略优化器和数据分布的影响。SRPL框架在真实世界场景中表现出有效性，特别是在提升鲁棒性和泛化能力方面，为自动驾驶安全强化学习提供了有前景的方向。

Abstract: Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.

</details>


### [74] [Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models](https://arxiv.org/abs/2512.17592)
*Arthur Guijt,Dirk Thierens,Ellen Kerkhof,Jan Wiersma,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.LG

TL;DR: 该论文研究了在数据分散且无法共享的医疗领域，通过异步协作（仅共享已训练模型）和模型拼接技术来提升性能的方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗等数据分散且隐私敏感的领域，传统联邦学习需要同步训练，而异步协作（如通过发表共享已训练模型）更具实用性，但如何有效整合这些独立训练的模型是一个挑战。

Method: 采用多目标视角，将各参与方数据性能独立评估，提出使用拼接层（stitching layers）来整合独立训练模型的中间表示，以实现异步协作下的模型融合。

Result: 独立训练的模型在自身数据上表现相似，但在其他方数据上表现较差；集成方法虽能提升泛化能力但会损害各方自身数据性能；而通过拼接层整合中间表示能在保持泛化能力的同时恢复竞争性性能。

Conclusion: 异步协作通过模型拼接技术可以产生竞争性结果，为数据分散且无法共享的领域提供了一种实用的模型协作方案。

Abstract: Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.
  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.

</details>


### [75] [A Unified Representation of Neural Networks Architectures](https://arxiv.org/abs/2512.17593)
*Christophe Prieur,Mircea Lazar,Bogdan Robu*

Main category: cs.LG

TL;DR: 本文提出了一种分布式参数神经网络（DiPaNet）的统一框架，将有限维和无限维神经网络架构通过均匀化/离散化联系起来，推导了神经元数量和隐藏层数趋于无穷时的近似误差。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络架构在隐藏层神经元数量和层数趋于无穷时的极限情况，建立有限维和无限维神经网络之间的统一理论框架，为不同神经网络架构提供统一的数学基础。

Method: 1. 首先推导单隐藏层神经网络的积分无限宽度表示；2. 扩展到具有有限积分隐藏层和残差连接的深度残差CNN；3. 重新审视神经ODE与深度残差NN的关系，通过离散化技术形式化近似误差；4. 将两种方法合并为统一的DiPaNet表示。

Result: 提出了DiPaNet框架，证明大多数现有的有限维和无限维神经网络架构都可以通过DiPaNet表示的均匀化/离散化联系起来，建立了统一的数学表示，并讨论了与神经场的异同。

Conclusion: DiPaNet提供了一个统一的确定性框架，适用于一般的一致连续矩阵权重函数，能够连接各种神经网络架构，为理论分析和实际应用提供了新的视角和可能性。

Abstract: In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.

</details>


### [76] [More Consistent Accuracy PINN via Alternating Easy-Hard Training](https://arxiv.org/abs/2512.17607)
*Zhaoqian Gao,Min Yanga*

Main category: cs.LG

TL;DR: 本文提出了一种结合硬优先和软优先训练的混合策略，通过交替训练算法提升PINNs在求解各类PDE时的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前PINNs的训练策略研究不足，硬优先方法（受有限元方法启发）和软优先方法都存在明显的权衡和在不同PDE类型上表现不一致的问题，需要一种更可靠的训练策略。

Method: 开发了一种混合策略，结合硬优先和软优先训练的优势，采用交替训练算法，在具有陡峭梯度、非线性和高维度的PDE上实现稳定训练。

Result: 在各类PDE上实现了稳定高精度，相对L2误差主要在O(10^-5)到O(10^-6)范围内，显著超越基线方法，且在不同问题上表现出更好的可靠性。

Conclusion: 这项工作为设计混合训练策略提供了新见解，能够增强PINNs的性能和鲁棒性，解决现有方法在不同PDE类型上表现不一致的问题。

Abstract: Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.

</details>


### [77] [SCOPE: Sequential Causal Optimization of Process Interventions](https://arxiv.org/abs/2512.17629)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: SCOPE：一种新的规范性过程监控方法，通过后向归纳和因果学习，直接从观测数据中学习对齐的顺序干预推荐，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有规范性过程监控方法在处理顺序干预时存在局限：要么只关注单次干预，要么将多次干预视为独立，忽略了时间上的相互作用。依赖模拟或数据增强的方法会产生现实差距和偏差。

Method: SCOPE采用后向归纳法评估每个候选干预行动的效果，从最终决策点向前传播影响。利用因果学习器直接从观测数据学习，无需构建过程近似模型进行强化学习。

Result: 在现有合成数据集和新半合成数据集上的实验表明，SCOPE在优化关键绩效指标方面始终优于最先进的规范性过程监控技术。

Conclusion: SCOPE能够有效学习对齐的顺序干预推荐，提出的半合成设置可作为未来顺序规范性过程监控研究的可重用基准。

Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.

</details>


### [78] [Trust-Region Adaptive Policy Optimization](https://arxiv.org/abs/2512.17636)
*Mingyu Su,Jian Guan,Yuxian Gu,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: TRAPO是一个混合训练框架，通过交错使用监督微调(SFT)和强化学习(RL)来解决传统两阶段训练中的不一致问题，在数学推理任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段训练流程(SFT然后RL)存在不一致性：SFT强制模仿专家数据，抑制了探索并导致遗忘，限制了RL的改进潜力。需要一种更高效的训练框架来统一外部监督和自我探索。

Method: TRAPO框架在每个训练实例中交错使用SFT和RL：在专家前缀上优化SFT损失，在模型自身生成上优化RL损失。引入信任区域SFT(TrSFT)来稳定训练，最小化信任区域内的前向KL散度，在区域外衰减优化。还包含自适应前缀选择机制，根据效用分配专家指导。

Result: 在五个数学推理基准测试中，TRAPO一致超越了标准SFT、RL、SFT-then-RL流程以及最近的最先进方法，为推理增强的LLMs建立了强大的新范式。

Conclusion: TRAPO通过统一监督微调和强化学习，解决了传统两阶段训练的不一致问题，提供了一种更高效的训练框架，显著提升了大型语言模型的复杂推理能力。

Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

</details>


### [79] [Estimating Spatially Resolved Radiation Fields Using Neural Networks](https://arxiv.org/abs/2512.17654)
*Felix Lehner,Pasquale Lombardo,Susana Castillo,Oliver Hupe,Marcus Magnor*

Main category: cs.LG

TL;DR: 使用神经网络重建医疗辐射场散射辐射空间分布的方法研究，评估了不同网络架构在三种合成数据集上的性能


<details>
  <summary>Details</summary>
Motivation: 为医疗辐射防护剂量学（如介入放射学和心脏病学）开发准确估计散射辐射空间分布的方法，解决传统方法在复杂辐射场中测量的挑战

Method: 使用基于Geant4的蒙特卡洛模拟生成三种复杂度递增的合成数据集，评估卷积神经网络和全连接神经网络架构，通过训练管道重建辐射场的通量和光谱空间分布

Result: 确定了适合重建辐射场空间分布的网络设计决策，所有数据集和训练管道已作为开源发布在独立存储库中

Conclusion: 神经网络能够有效重建医疗辐射场中的散射辐射空间分布，开源数据集和训练管道为辐射防护剂量学提供了有价值的工具

Abstract: We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology. Therefore, we present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All used datasets as well as our training pipeline are published as open source in separate repositories.

</details>


### [80] [Polyharmonic Cascade](https://arxiv.org/abs/2512.17671)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 提出"多谐级联"深度学习架构，通过多谐样条包序列实现非线性函数逼近，保持全局光滑性和概率解释，采用非梯度下降的线性系统训练方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习架构缺乏严格的概率解释和理论一致性，梯度下降训练方法存在优化困难、过拟合等问题。需要一种既能逼近复杂非线性函数，又能保持理论严谨性和概率解释的新架构。

Method: 提出多谐级联架构：由多谐样条包序列组成，每层基于随机函数理论和无差别原理严格推导。训练方法：在每个批次上求解关于固定"星座"节点处函数值的全局线性系统，而非直接优化系数，实现所有层的同步更新。

Result: 在MNIST数据集上实现了快速学习且不过拟合。该方法将计算简化为可在GPU上高效执行的2D矩阵操作，具有良好的可扩展性。

Conclusion: 多谐级联架构结合了深度学习的表达能力与严格的数学理论框架，提供了一种替代梯度下降的训练方法，保持了概率解释和理论一致性，在保持性能的同时具有更好的可解释性。

Abstract: This paper presents a deep machine learning architecture, the "polyharmonic cascade" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed "constellations" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.

</details>


### [81] [You Only Train Once: Differentiable Subset Selection for Omics Data](https://arxiv.org/abs/2512.17678)
*Daphné Chopard,Jorge da Silva Gonçalves,Irene Cannistraci,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.LG

TL;DR: YOTO是一个端到端的可微分框架，能联合识别离散基因子集并进行预测，通过多任务学习设计实现稀疏、紧凑的基因选择，在单细胞RNA-seq数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法多为多阶段流程或依赖事后特征归因，导致选择与预测弱耦合，需要更紧密集成的端到端解决方案。

Method: 提出YOTO框架，通过可微分架构联合学习离散基因子集和预测任务，实现选择与预测的闭环反馈，采用多任务学习设计共享表示，强制稀疏性使只有选定基因参与推理。

Result: 在两个代表性单细胞RNA-seq数据集上评估，YOTO始终优于最先进的基线方法，展示了更好的预测性能和更紧凑、有意义的基因子集。

Conclusion: 稀疏、端到端、多任务的基因子集选择能提高预测性能并产生紧凑且有意义的基因子集，推动了生物标志物发现和单细胞分析的发展。

Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.

</details>


### [82] [Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation](https://arxiv.org/abs/2512.17762)
*Luca Miglior,Matteo Tolloso,Alessio Gravina,Davide Bacciu*

Main category: cs.LG

TL;DR: 论文提出了ECHO基准测试，用于系统评估图神经网络在长距离信息传播方面的能力，包含三个合成图任务和两个真实化学数据集，揭示了现有GNN在长程交互处理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 有效捕捉长距离交互是图神经网络研究中的一个基本但未解决的挑战，对科学应用至关重要。目前缺乏系统评估GNN处理长程图传播能力的基准测试。

Method: 提出了ECHO基准测试，包含三个合成图任务（单源最短路径、节点离心率、图直径），基于具有信息瓶颈的挑战性拓扑结构；以及两个真实化学数据集（ECHO-Charge和ECHO-Energy），分别用于预测原子部分电荷和分子总能量，基于密度泛函理论计算作为参考。

Result: 对流行GNN架构的广泛基准测试揭示了明显的性能差距，强调了真正长程传播的困难，并指出了能够克服固有局限性的设计选择。

Conclusion: ECHO为评估长距离信息传播设立了新标准，并为AI在科学中的应用提供了需要长程传播能力的强有力示例。

Abstract: Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.

</details>


### [83] [Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments](https://arxiv.org/abs/2512.17771)
*Dong Chen,Zhengqing Hu,Shixing Zhao,Yibo Guo*

Main category: cs.LG

TL;DR: EA方法通过设计特定小模型来补充大语言模型的数据分布，无需访问大模型参数，仅需极少资源即可达到PEFT性能


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法面临两大挑战：1) 资源成本高，仍需要大量时间和内存；2) 参数依赖性强，需要更新大模型参数，而许多领先模型已闭源仅通过API访问，成本高昂且调优过程缓慢

Method: 提出Easy Adaptation (EA)方法，设计特定小模型(SSMs)来补充大语言模型在特定数据分布上的不足，无需访问大模型参数，仅需极少资源

Result: 大量实验表明，EA在多种任务上匹配了PEFT的性能，同时无需访问大模型参数，且仅需极少资源

Conclusion: EA提供了一种高效的大模型适应方法，解决了PEFT的资源成本和参数依赖问题，特别适合资源受限环境和闭源大模型场景

Abstract: While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.

</details>


### [84] [Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning](https://arxiv.org/abs/2512.17788)
*Wei Tang,Yin-Fang Yang,Weijia Zhang,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 提出可校准消歧损失（CDL）解决多示例部分标签学习中的校准问题，提升分类准确性和校准性能


<details>
  <summary>Details</summary>
Motivation: 现有MIPL方法存在校准性能差的问题，影响分类器可靠性，需要同时提升分类准确性和校准性能

Method: 提出可校准消歧损失（CDL），包含两种实现：第一种基于候选标签集概率校准预测，第二种整合候选和非候选标签集概率

Result: 在基准和真实数据集上实验证明，CDL显著提升了分类和校准性能

Conclusion: CDL作为即插即用模块可无缝集成到现有MIPL和PLL框架中，理论分析证明其优于传统消歧损失

Abstract: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.

</details>


### [85] [Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation](https://arxiv.org/abs/2512.17820)
*Liam Collins,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Donald Loveland,Leonardo Neves,Neil Shah*

Main category: cs.LG

TL;DR: 该论文研究发现ID嵌入和文本模态嵌入在序列推荐中是互补的，提出了一种简单的集成方法，无需复杂融合架构即可获得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐模型要么完全用模态特征替换ID嵌入，要么采用复杂的融合策略联合使用两者，但缺乏对ID和模态特征互补性的理解。本文旨在填补这一空白，研究ID和文本模态特征的互补性。

Method: 提出新的序列推荐方法：通过独立训练保持ID-文本互补性，然后使用简单的集成策略来利用这种互补性。具体包括独立训练ID模型和文本模型，然后通过集成结合两者的预测结果。

Result: 实验表明该方法优于多个竞争性序列推荐基线，证明了ID和文本特征对于实现最先进的序列推荐性能都是必要的，但不需要复杂的融合架构。

Conclusion: ID和文本模态特征在序列推荐中具有互补性，简单的集成方法就能有效利用这种互补性，无需复杂的多阶段训练或对齐架构，为序列推荐模型设计提供了新的视角。

Abstract: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [86] [Disentangled representations via score-based variational autoencoders](https://arxiv.org/abs/2512.17127)
*Benjamin S. H. Lyo,Eero P. Simoncelli,Cristina Savin*

Main category: stat.ML

TL;DR: SAMI是一种无监督表示学习方法，结合了扩散模型和VAE的理论框架，通过分数引导学习具有多尺度结构的可解释表示。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督表示学习方法难以自动捕获数据中的有意义结构，特别是从复杂的自然数据中学习可解释的潜在维度。扩散模型虽然能生成高质量样本，但其内部结构信息通常是隐式的，难以直接用于表示学习。

Method: SAMI通过统一扩散模型和VAE的证据下界，构建了一个原则性的目标函数。该方法利用底层扩散过程的分数引导来学习表示，能够从预训练扩散模型中提取有用表示，且仅需极少额外训练。

Result: SAMI在合成数据中恢复真实生成因子，从复杂自然图像中学习因子化、语义化的潜在维度，将视频序列编码为比替代编码器更直的潜在轨迹（尽管仅在静态图像上训练）。其概率公式提供了在无监督标签下识别语义有意义轴的新方法。

Conclusion: 扩散模型中的隐式结构信息可以通过与变分自编码器的协同组合变得显式和可解释。SAMI的数学精确性允许对学习表示的性质做出形式化陈述，为无监督表示学习提供了新的理论框架。

Abstract: We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.

</details>


### [87] [Sharp Structure-Agnostic Lower Bounds for General Functional Estimation](https://arxiv.org/abs/2512.17341)
*Jikai Jin,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 该论文系统研究了结构不可知估计器的最优误差率，证明了双重稳健学习和去偏/双重机器学习在因果推断中的最优性，并区分了双重稳健可达成和不可达成两种机制。


<details>
  <summary>Details</summary>
Motivation: 经典最优估计方法通常依赖强结构假设，这些假设在实践中可能被错误设定，限制了实际应用。因此，需要研究不依赖结构先验的结构不可知方法，并理解这些方法的基本极限。

Method: 系统分析结构不可知估计器的最优误差率，首先证明双重稳健学习在估计平均处理效应（ATE）中的最优性，然后扩展到依赖未知干扰函数的一般函数类，分析去偏/双重机器学习的最优性，区分双重稳健可达成和不可达成两种机制。

Result: 证明了双重稳健学习和去偏/双重机器学习在结构不可知框架下达到最优误差率，推导了两种机制下的不同最优率，并通过具体实例化展示了现有结果的恢复和新估计量的扩展。

Conclusion: 研究结果为广泛使用的一阶去偏方法提供了理论验证，为实践中在没有结构假设时寻求最优方法提供了指导，并推广了作者先前在ATE下界方面的研究成果。

Abstract: The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \citet{jin2024structure} by the same authors.

</details>


### [88] [Generative modeling of conditional probability distributions on the level-sets of collective variables](https://arxiv.org/abs/2512.17374)
*Fatima-Zahrae Akhyar,Wei Zhang,Gabriel Stoltz,Christof Schütte*

Main category: stat.ML

TL;DR: 提出一种高效学习概率分布在集体变量水平集上条件分布生成模型的方法，支持同时学习不同水平集，并利用增强采样数据进行数据增强以改善低概率区域学习质量。


<details>
  <summary>Details</summary>
Motivation: 研究如何从数据中学习概率分布在集体变量水平集上的条件分布生成模型，这对于生物物理中的分子系统生成建模具有潜在应用价值。

Method: 提出通用高效的学习方法，能够同时学习集体变量不同水平集上的生成模型；采用增强采样技术的数据进行数据增强，改善低概率区域的学习质量。

Result: 通过具体数值示例证明了所提学习方法的有效性，展示了在生物物理分子系统生成建模中的潜在应用前景。

Conclusion: 该方法为学习概率分布在集体变量水平集上的条件分布生成模型提供了一种有效解决方案，特别适用于生物物理等领域的复杂系统建模。

Abstract: Given a probability distribution $μ$ in $\mathbb{R}^d$ represented by data, we study in this paper the generative modeling of its conditional probability distributions on the level-sets of a collective variable $ξ: \mathbb{R}^d \rightarrow \mathbb{R}^k$, where $1 \le k<d$. We propose a general and effcient learning approach that is able to learn generative models on different level-sets of $ξ$ simultaneously. To improve the learning quality on level-sets in low-probability regions, we also propose a strategy for data enrichment by utilizing data from enhanced sampling techniques. We demonstrate the effectiveness of our proposed learning approach through concrete numerical examples. The proposed approach is potentially useful for the generative modeling of molecular systems in biophysics, for instance.

</details>


### [89] [Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing](https://arxiv.org/abs/2512.17426)
*Xiaosi Gu,Ayaka Sakata,Tomoyuki Obuchi*

Main category: stat.ML

TL;DR: 本文开发了用于SCAD惩罚稀疏信号重建的一步复制对称破缺近似消息传递算法(1RSB-AMP)，通过1RSB-SE状态演化分析改进了算法性能，缩小了发散区域，提升了完美重建的算法极限。


<details>
  <summary>Details</summary>
Motivation: 传统复制对称近似消息传递(RS-AMP)在特定参数区域会发散，且1RSB描述在热力学上并非精确，需要开发更稳定的算法来改进稀疏信号重建性能。

Method: 从1RSB置信传播出发，推导出1RSB-AMP的显式更新规则和相应的状态演化方程(1RSB-SE)，提出最小化发散区域的新准则来确定Parisi参数，并结合非凸性控制协议。

Result: 1RSB-AMP与1RSB-SE在宏观层面高度一致，即使在RS-AMP发散的区域；新方法缩小了发散区域，相比RS-AMP提升了完美重建的算法极限，但增益有限且仍略低于贝叶斯最优阈值。

Conclusion: 1RSB-AMP为SCAD惩罚稀疏信号重建提供了更稳定的算法框架，通过1RSB-SE分析和新参数选择准则改进了性能，并揭示了该问题中1RSB相的热力学特性。

Abstract: We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equations. A detailed comparison shows that 1RSB-AMP and 1RSB-SE agree remarkably well at the macroscopic level, even in parameter regions where replica-symmetric (RS) AMP, termed RS-AMP, diverges and where the 1RSB description itself is not expected to be thermodynamically exact. Fixed-point analysis of 1RSB-SE reveals a phase diagram consisting of success, failure, and diverging phases, as in the RS case. However, the diverging-region boundary now depends on the Parisi parameter due to the 1RSB ansatz, and we propose a new criterion -- minimizing the size of the diverging region -- rather than the conventional zero-complexity condition, to determine its value. Combining this criterion with the nonconvexity-control (NCC) protocol proposed in a previous RS study improves the algorithmic limit of perfect reconstruction compared with RS-AMP. Numerical solutions of 1RSB-SE and experiments with 1RSB-AMP confirm that this improved limit is achieved in practice, though the gain is modest and remains slightly inferior to the Bayes-optimal threshold. We also report the behavior of thermodynamic quantities -- overlaps, free entropy, complexity, and the non-self-averaging susceptibility -- that characterize the 1RSB phase in this problem.

</details>


### [90] [Fast and Robust: Computationally Efficient Covariance Estimation for Sub-Weibull Vectors](https://arxiv.org/abs/2512.17632)
*Even He*

Main category: stat.ML

TL;DR: 提出一种计算高效的协方差估计方法，针对Sub-Weibull分布（拉伸指数尾分布），通过交叉拟合范数截断估计器实现O(Nd²)复杂度，达到最优子高斯收敛速率。


<details>
  <summary>Details</summary>
Motivation: 高维协方差估计对异常值敏感，现有统计最优估计器（如半定规划或迭代M估计）计算成本高（O(d³)），需要为Sub-Weibull分布设计计算高效的替代方案。

Method: 提出交叉拟合范数截断估计器，不同于逐元素截断，该方法保持谱几何特性，使用加权Hanson-Wright不等式推导非渐近误差界，证明在Sub-Weibull类中指数尾衰减补偿了几何不匹配。

Result: 该估计器以高概率恢复最优子高斯速率$\tilde{O}(\sqrt{r(Σ)/N})$，计算复杂度为O(Nd²)，达到构建完整协方差矩阵的理论下界，为比高斯重尾但比多项式衰减轻尾的高维数据提供可扩展解决方案。

Conclusion: 对于Sub-Weibull分布，交叉拟合范数截断估计器在计算效率（O(Nd²)）和统计精度（最优子高斯速率）之间取得平衡，为高维重尾协方差估计提供了实用解决方案。

Abstract: High-dimensional covariance estimation is notoriously sensitive to outliers. While statistically optimal estimators exist for general heavy-tailed distributions, they often rely on computationally expensive techniques like semidefinite programming or iterative M-estimation ($O(d^3)$). In this work, we target the specific regime of \textbf{Sub-Weibull distributions} (characterized by stretched exponential tails $\exp(-t^α)$). We investigate a computationally efficient alternative: the \textbf{Cross-Fitted Norm-Truncated Estimator}. Unlike element-wise truncation, our approach preserves the spectral geometry while requiring $O(Nd^2)$ operations, which represents the theoretical lower bound for constructing a full covariance matrix. Although spherical truncation is geometrically suboptimal for anisotropic data, we prove that within the Sub-Weibull class, the exponential tail decay compensates for this mismatch. Leveraging weighted Hanson-Wright inequalities, we derive non-asymptotic error bounds showing that our estimator recovers the optimal sub-Gaussian rate $\tilde{O}(\sqrt{r(Σ)/N})$ with high probability. This provides a scalable solution for high-dimensional data that exhibits tails heavier than Gaussian but lighter than polynomial decay.

</details>


### [91] [Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design](https://arxiv.org/abs/2512.17659)
*Madhav R. Muthyala,Farshud Sorourifar,Tianhong Tan,You Peng,Joel A. Paulson*

Main category: stat.ML

TL;DR: 提出"生成-优化"框架用于多目标分子设计，使用qPMHI获取函数进行批量选择，在可持续能源存储应用中表现优异


<details>
  <summary>Details</summary>
Motivation: 分子设计需要满足多个相互冲突的目标，化学空间巨大且高保真模拟成本高。现有结合贝叶斯优化和生成模型的方法依赖连续潜在空间，存在架构纠缠和可扩展性挑战

Method: 提出模块化的"生成-优化"框架：每轮用生成模型构建大量候选分子池，然后使用新颖的qPMHI（多点最大超体积改进概率）获取函数选择最可能扩展帕累托前沿的批量候选分子。qPMHI具有可加性分解特性，可通过蒙特卡洛采样简单概率排序实现精确、可扩展的批量选择

Result: 在合成基准测试和应用驱动任务中显著优于最先进的潜在空间和离散分子优化方法。在可持续能源存储案例研究中，快速发现了新颖、多样且高性能的有机（醌基）阴极材料用于水系氧化还原液流电池

Conclusion: 提出的模块化"生成-优化"框架结合了生成模型的多样性和贝叶斯优化的样本效率，通过qPMHI实现可扩展的批量选择，为多目标分子发现提供了有效解决方案

Abstract: Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular "generate-then-optimize" framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.

</details>


### [92] [Imputation Uncertainty in Interpretable Machine Learning Methods](https://arxiv.org/abs/2512.17689)
*Pegah Golchian,Marvin N. Wright*

Main category: stat.ML

TL;DR: 比较不同插补方法对可解释机器学习方法置信区间覆盖率的影响，发现单一插补会低估方差，而多重插补在大多数情况下能接近名义覆盖率


<details>
  <summary>Details</summary>
Motivation: 现实数据中经常出现缺失值，这会影响可解释机器学习方法的解释。现有研究关注偏差问题，显示不同插补方法可能导致模型解释差异，但忽略了额外的插补不确定性及其对方差和置信区间的影响

Method: 比较不同插补方法对三种可解释机器学习方法置信区间覆盖率的影响：排列特征重要性、部分依赖图和Shapley值

Result: 单一插补会导致方差低估，在大多数情况下，只有多重插补能接近名义覆盖率

Conclusion: 在可解释机器学习中处理缺失值时，需要考虑插补不确定性，多重插补比单一插补能提供更准确的置信区间估计

Abstract: In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.

</details>
