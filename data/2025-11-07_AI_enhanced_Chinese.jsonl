{"id": "2511.03735", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.03735", "abs": "https://arxiv.org/abs/2511.03735", "authors": ["Valentin Mouton", "Adrien M\u00e9lot"], "title": "Friction on Demand: A Generative Framework for the Inverse Design of Metainterfaces", "comment": "Preprint", "summary": "Designing frictional interfaces to exhibit prescribed macroscopic behavior is\na challenging inverse problem, made difficult by the non-uniqueness of\nsolutions and the computational cost of contact simulations. Traditional\napproaches rely on heuristic search over low-dimensional parameterizations,\nwhich limits their applicability to more complex or nonlinear friction laws. We\nintroduce a generative modeling framework using Variational Autoencoders (VAEs)\nto infer surface topographies from target friction laws. Trained on a synthetic\ndataset composed of 200 million samples constructed from a parameterized\ncontact mechanics model, the proposed method enables efficient, simulation-free\ngeneration of candidate topographies. We examine the potential and limitations\nof generative modeling for this inverse design task, focusing on balancing\naccuracy, throughput, and diversity in the generated solutions. Our results\nhighlight trade-offs and outline practical considerations when balancing these\nobjectives. This approach paves the way for near-real-time control of\nfrictional behavior through tailored surface topographies.", "AI": {"tldr": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u7684\u751f\u6210\u5efa\u6a21\u6846\u67b6\uff0c\u4ece\u76ee\u6807\u6469\u64e6\u5b9a\u5f8b\u63a8\u65ad\u8868\u9762\u5f62\u8c8c\uff0c\u5b9e\u73b0\u6469\u64e6\u754c\u9762\u7684\u9006\u5411\u8bbe\u8ba1\u3002", "motivation": "\u8bbe\u8ba1\u5177\u6709\u7279\u5b9a\u5b8f\u89c2\u884c\u4e3a\u7684\u6469\u64e6\u754c\u9762\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u9006\u5411\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4f4e\u7ef4\u53c2\u6570\u5316\u7684\u542f\u53d1\u5f0f\u641c\u7d22\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u6216\u975e\u7ebf\u6027\u6469\u64e6\u5b9a\u5f8b\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u57fa\u4e8e\u53c2\u6570\u5316\u63a5\u89e6\u529b\u5b66\u6a21\u578b\u6784\u5efa\u5305\u542b2\u4ebf\u6837\u672c\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u8fdb\u884c\u751f\u6210\u5efa\u6a21\uff0c\u5b9e\u73b0\u65e0\u4eff\u771f\u7684\u5019\u9009\u5f62\u8c8c\u9ad8\u6548\u751f\u6210\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u3001\u65e0\u4eff\u771f\u5730\u751f\u6210\u5019\u9009\u8868\u9762\u5f62\u8c8c\uff0c\u5728\u51c6\u786e\u6027\u3001\u541e\u5410\u91cf\u548c\u751f\u6210\u89e3\u591a\u6837\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u8fc7\u5b9a\u5236\u8868\u9762\u5f62\u8c8c\u5b9e\u73b0\u6469\u64e6\u884c\u4e3a\u7684\u8fd1\u5b9e\u65f6\u63a7\u5236\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5e76\u7a81\u51fa\u4e86\u5728\u5e73\u8861\u8fd9\u4e9b\u76ee\u6807\u65f6\u7684\u6743\u8861\u548c\u5b9e\u9645\u8003\u8651\u3002"}}
{"id": "2511.03749", "categories": ["cs.LG", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.03749", "abs": "https://arxiv.org/abs/2511.03749", "authors": ["Oluwadurotimi Onibonoje", "Vuong M. Ngo", "Andrew McCarre", "Elodie Ruelle", "Bernadette O-Briend", "Mark Roantree"], "title": "Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland", "comment": "13 pages (two-columns), 7 figures, 3 tables", "summary": "Grasslands, constituting the world's second-largest terrestrial carbon sink,\nplay a crucial role in biodiversity and the regulation of the carbon cycle.\nCurrently, the Irish dairy sector, a significant economic contributor, grapples\nwith challenges related to profitability and sustainability. Presently, grass\ngrowth forecasting relies on impractical mechanistic models. In response, we\npropose deep learning models tailored for univariate datasets, presenting\ncost-effective alternatives. Notably, a temporal convolutional network designed\nfor forecasting Perennial Ryegrass growth in Cork exhibits high performance,\nleveraging historical grass height data with RMSE of 2.74 and MAE of 3.46.\nValidation across a comprehensive dataset spanning 1,757 weeks over 34 years\nprovides insights into optimal model configurations. This study enhances our\nunderstanding of model behavior, thereby improving reliability in grass growth\nforecasting and contributing to the advancement of sustainable dairy farming\npractices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8349\u5730\u751f\u957f\u9884\u6d4b\u6a21\u578b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7231\u5c14\u5170\u5976\u725b\u4e1a\u4e2d\u591a\u5e74\u751f\u9ed1\u9ea6\u8349\u7684\u751f\u957f\u9884\u6d4b\uff0c\u4f7f\u7528\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u8349\u5730\u4f5c\u4e3a\u5168\u7403\u7b2c\u4e8c\u5927\u78b3\u6c47\u5bf9\u751f\u7269\u591a\u6837\u6027\u548c\u78b3\u5faa\u73af\u8c03\u8282\u81f3\u5173\u91cd\u8981\uff0c\u7231\u5c14\u5170\u5976\u725b\u4e1a\u9762\u4e34\u76c8\u5229\u548c\u53ef\u6301\u7eed\u6027\u6311\u6218\uff0c\u73b0\u6709\u8349\u5730\u751f\u957f\u9884\u6d4b\u6a21\u578b\u4e0d\u5b9e\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u9488\u5bf9\u5355\u53d8\u91cf\u6570\u636e\u96c6\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662f\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff0c\u5229\u7528\u5386\u53f2\u8349\u9ad8\u6570\u636e\u8fdb\u884c\u591a\u5e74\u751f\u9ed1\u9ea6\u8349\u751f\u957f\u9884\u6d4b\u3002", "result": "\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u5728\u79d1\u514b\u5730\u533a\u7684\u591a\u5e74\u751f\u9ed1\u9ea6\u8349\u751f\u957f\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cRMSE\u4e3a2.74\uff0cMAE\u4e3a3.46\uff0c\u57fa\u4e8e34\u5e741,757\u5468\u7684\u6570\u636e\u9a8c\u8bc1\u4e86\u6700\u4f18\u6a21\u578b\u914d\u7f6e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u9ad8\u4e86\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u7406\u89e3\uff0c\u589e\u5f3a\u4e86\u8349\u5730\u751f\u957f\u9884\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u53ef\u6301\u7eed\u5976\u725b\u517b\u6b96\u5b9e\u8df5\u3002"}}
{"id": "2511.03837", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03837", "abs": "https://arxiv.org/abs/2511.03837", "authors": ["Sa\u00fal Fenollosa", "Narcis Cardona", "Wenfei Yang", "Jian Li"], "title": "Correlation and Temporal Consistency Analysis of Mono-static and Bi-static ISAC Channels", "comment": "6 pages, 7 figures, 2 tables. Accepted for publication at the 2025\n  IEEE Global Communications Conference (GLOBECOM), WS-26: 4th Workshop on\n  Propagation Channel Models and Evaluation Methodologies for 6G", "summary": "Integrated Sensing and Communication (ISAC) is critical for efficient\nspectrum and hardware utilization in future wireless networks like 6G. However,\nexisting channel models lack comprehensive characterization of ISAC-specific\ndynamics, particularly the relationship between mono-static (co-located Tx/Rx)\nand bi-static (separated Tx/Rx) sensing configurations. Empirical measurements\nin dynamic urban microcell (UMi) environments using a 79-GHz FMCW channel\nsounder help bridge this gap. Two key findings are demonstrated: (1)\nmono-static and bi-static channels exhibit consistently low instantaneous\ncorrelation due to divergent propagation geometries; (2) despite low\ninstantaneous correlation, both channels share unified temporal consistency,\nevolving predictably under environmental kinematics. These insights, validated\nacross seven real-world scenarios with moving targets/transceivers, inform\nrobust ISAC system design and future standardization.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc779-GHz FMCW\u4fe1\u9053\u6d4b\u91cf\uff0c\u63ed\u793a\u4e86\u5355\u9759\u6001\u548c\u53cc\u9759\u6001ISAC\u4fe1\u9053\u5728\u52a8\u6001\u57ce\u5e02\u5fae\u8702\u7a9d\u73af\u5883\u4e2d\u7684\u76f8\u5173\u7279\u6027\uff1a\u77ac\u65f6\u76f8\u5173\u6027\u4f4e\u4f46\u5177\u6709\u7edf\u4e00\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u4fe1\u9053\u6a21\u578b\u7f3a\u4e4f\u5bf9ISAC\u7279\u5b9a\u52a8\u6001\u7684\u5168\u9762\u8868\u5f81\uff0c\u7279\u522b\u662f\u5355\u9759\u6001\u548c\u53cc\u9759\u6001\u4f20\u611f\u914d\u7f6e\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u9700\u8981\u5b9e\u8bc1\u6d4b\u91cf\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5728\u52a8\u6001\u57ce\u5e02\u5fae\u8702\u7a9d\u73af\u5883\u4e2d\u4f7f\u752879-GHz FMCW\u4fe1\u9053\u6d4b\u91cf\u4eea\u8fdb\u884c\u5b9e\u8bc1\u6d4b\u91cf\uff0c\u6db5\u76d67\u4e2a\u771f\u5b9e\u573a\u666f\u548c\u79fb\u52a8\u76ee\u6807/\u6536\u53d1\u5668\u3002", "result": "\u53d1\u73b0\u5355\u9759\u6001\u548c\u53cc\u9759\u6001\u4fe1\u9053\u7531\u4e8e\u4f20\u64ad\u51e0\u4f55\u5dee\u5f02\u800c\u5448\u73b0\u6301\u7eed\u4f4e\u77ac\u65f6\u76f8\u5173\u6027\uff0c\u4f46\u4e24\u8005\u5177\u6709\u7edf\u4e00\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5728\u73af\u5883\u8fd0\u52a8\u5b66\u4e0b\u53ef\u9884\u6d4b\u5730\u6f14\u5316\u3002", "conclusion": "\u8fd9\u4e9b\u89c1\u89e3\u4e3a\u7a33\u5065\u7684ISAC\u7cfb\u7edf\u8bbe\u8ba1\u548c\u672a\u6765\u6807\u51c6\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2511.03753", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.03753", "abs": "https://arxiv.org/abs/2511.03753", "authors": ["Youssef Elmir", "Yassine Himeur", "Abbes Amira"], "title": "Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices", "comment": "06 pages, 03 figures, accepted for presentation at the 7th IEEE\n  Computing, Communications and IoT Applications Conference (ComComAp 2025)", "summary": "This study presents a federated learning (FL) framework for\nprivacy-preserving electrocardiogram (ECG) classification in Internet of Things\n(IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian\nAngular Field (GAF) images, the proposed approach enables efficient feature\nextraction through Convolutional Neural Networks (CNNs) while ensuring that\nsensitive medical data remain local to each device. This work is among the\nfirst to experimentally validate GAF-based federated ECG classification across\nheterogeneous IoT devices, quantifying both performance and communication\nefficiency. To evaluate feasibility in realistic IoT settings, we deployed the\nframework across a server, a laptop, and a resource-constrained Raspberry Pi 4,\nreflecting edge-cloud integration in IoT ecosystems. Experimental results\ndemonstrate that the FL-GAF model achieves a high classification accuracy of\n95.18% in a multi-client setup, significantly outperforming a single-client\nbaseline in both accuracy and training time. Despite the added computational\ncomplexity of GAF transformations, the framework maintains efficient resource\nutilization and communication overhead. These findings highlight the potential\nof lightweight, privacy-preserving AI for IoT-based healthcare monitoring,\nsupporting scalable and secure edge deployments in smart health systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u548cGAF\u56fe\u50cf\u8f6c\u6362\u7684\u9690\u79c1\u4fdd\u62a4ECG\u5206\u7c7b\u6846\u67b6\uff0c\u5728IoT\u533b\u7597\u73af\u5883\u4e2d\u5b9e\u73b095.18%\u7684\u9ad8\u51c6\u786e\u7387\uff0c\u540c\u65f6\u786e\u4fdd\u533b\u7597\u6570\u636e\u672c\u5730\u5316\u5904\u7406\u3002", "motivation": "\u89e3\u51b3IoT\u533b\u7597\u73af\u5883\u4e2dECG\u6570\u636e\u5206\u7c7b\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u907f\u514d\u654f\u611f\u533b\u7597\u6570\u636e\u96c6\u4e2d\u5b58\u50a8\uff0c\u540c\u65f6\u9002\u5e94\u5f02\u6784IoT\u8bbe\u5907\u7684\u8d44\u6e90\u7ea6\u675f\u3002", "method": "\u5c061D ECG\u4fe1\u53f7\u8f6c\u6362\u4e3a2D GAF\u56fe\u50cf\uff0c\u4f7f\u7528CNN\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u5728\u591a\u8bbe\u5907\u95f4\u534f\u540c\u8bad\u7ec3\uff0c\u4fdd\u6301\u6570\u636e\u672c\u5730\u5316\u3002", "result": "\u5728\u5305\u542b\u670d\u52a1\u5668\u3001\u7b14\u8bb0\u672c\u7535\u8111\u548c\u6811\u8393\u6d3e4\u7684\u591a\u5ba2\u6237\u7aef\u8bbe\u7f6e\u4e2d\uff0cFL-GAF\u6a21\u578b\u8fbe\u523095.18%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u5ba2\u6237\u7aef\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u548c\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u3001\u9690\u79c1\u4fdd\u62a4\u7684AI\u5728IoT\u533b\u7597\u76d1\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u652f\u6301\u667a\u80fd\u5065\u5eb7\u7cfb\u7edf\u4e2d\u53ef\u6269\u5c55\u4e14\u5b89\u5168\u7684\u8fb9\u7f18\u90e8\u7f72\u3002"}}
{"id": "2511.03756", "categories": ["stat.ML", "cs.LG", "physics.flu-dyn", "stat.AP", "60G60 (Primary), 68T05"], "pdf": "https://arxiv.org/pdf/2511.03756", "abs": "https://arxiv.org/abs/2511.03756", "authors": ["Aniket Jivani", "Cosmin Safta", "Beckett Y. Zhou", "Xun Huan"], "title": "Bifidelity Karhunen-Lo\u00e8ve Expansion Surrogate with Active Learning for Random Fields", "comment": null, "summary": "We present a bifidelity Karhunen-Lo\\`eve expansion (KLE) surrogate model for\nfield-valued quantities of interest (QoIs) under uncertain inputs. The approach\ncombines the spectral efficiency of the KLE with polynomial chaos expansions\n(PCEs) to preserve an explicit mapping between input uncertainties and output\nfields. By coupling inexpensive low-fidelity (LF) simulations that capture\ndominant response trends with a limited number of high-fidelity (HF)\nsimulations that correct for systematic bias, the proposed method enables\naccurate and computationally affordable surrogate construction. To further\nimprove surrogate accuracy, we form an active learning strategy that adaptively\nselects new HF evaluations based on the surrogate's generalization error,\nestimated via cross-validation and modeled using Gaussian process regression.\nNew HF samples are then acquired by maximizing an expected improvement\ncriterion, targeting regions of high surrogate error. The resulting BF-KLE-AL\nframework is demonstrated on three examples of increasing complexity: a\none-dimensional analytical benchmark, a two-dimensional convection-diffusion\nsystem, and a three-dimensional turbulent round jet simulation based on\nReynolds-averaged Navier--Stokes (RANS) and enhanced delayed detached-eddy\nsimulations (EDDES). Across these cases, the method achieves consistent\nimprovements in predictive accuracy and sample efficiency relative to\nsingle-fidelity and random-sampling approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u4fdd\u771f\u5ea6Karhunen-Lo\u00e8ve\u5c55\u5f00\uff08KLE\uff09\u4ee3\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u786e\u5b9a\u8f93\u5165\u4e0b\u7684\u573a\u503c\u611f\u5174\u8da3\u91cf\uff08QoIs\uff09\uff0c\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5355\u4fdd\u771f\u5ea6\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u53cc\u4fdd\u771f\u5ea6\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u7ed3\u5408\u4f4e\u6210\u672c\u4f4e\u4fdd\u771f\u5ea6\u6a21\u62df\u548c\u5c11\u91cf\u9ad8\u4fdd\u771f\u5ea6\u6a21\u62df\u6765\u6784\u5efa\u51c6\u786e\u4e14\u8ba1\u7b97\u8d1f\u62c5\u5c0f\u7684\u4ee3\u7406\u6a21\u578b\u3002", "method": "\u5c06KLE\u7684\u5149\u8c31\u6548\u7387\u4e0e\u591a\u9879\u5f0f\u6df7\u6c8c\u5c55\u5f00\uff08PCEs\uff09\u7ed3\u5408\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u81ea\u9002\u5e94\u9009\u62e9\u65b0\u7684\u9ad8\u4fdd\u771f\u5ea6\u8bc4\u4f30\uff0c\u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1\u548c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u4f30\u8ba1\u6cdb\u5316\u8bef\u5dee\uff0c\u6700\u5927\u5316\u671f\u671b\u6539\u8fdb\u51c6\u5219\u3002", "result": "\u5728\u4e09\u4e2a\u590d\u6742\u5ea6\u9012\u589e\u7684\u793a\u4f8b\u4e2d\uff08\u4e00\u7ef4\u5206\u6790\u57fa\u51c6\u3001\u4e8c\u7ef4\u5bf9\u6d41-\u6269\u6563\u7cfb\u7edf\u3001\u4e09\u7ef4\u6e4d\u6d41\u5706\u5c04\u6d41\u6a21\u62df\uff09\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5355\u4fdd\u771f\u5ea6\u548c\u968f\u673a\u91c7\u6837\u65b9\u6cd5\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u6837\u672c\u6548\u7387\u65b9\u9762\u5747\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "BF-KLE-AL\u6846\u67b6\u80fd\u591f\u6709\u6548\u6784\u5efa\u51c6\u786e\u4e14\u8ba1\u7b97\u8d1f\u62c5\u5c0f\u7684\u573a\u503cQoIs\u4ee3\u7406\u6a21\u578b\uff0c\u5728\u590d\u6742\u5de5\u7a0b\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.03923", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03923", "abs": "https://arxiv.org/abs/2511.03923", "authors": ["Xianhua Yu", "Dong Li", "Bowen Gu", "Liuqing Yang", "Sumei Sun", "George K. Karagiannidis"], "title": "Adaptive Phase Shift Information Compression for IRS Systems: A Prompt Conditioned Variable Rate Framework", "comment": null, "summary": "Intelligent reflecting surfaces (IRSs) have become a vital technology for\nimproving the spectrum and energy efficiency of forthcoming wireless networks.\nNevertheless, practical implementation is obstructed by the excessive overhead\nassociated with the frequent transmission of phase shift information (PSI) over\nbandwidth-constrained control lines. Current deep learning-based compression\nmethods mitigate this problem but are constrained by elevated decoder\ncomplexity, inadequate flexibility to dynamic channels, and static compression\nratios. This research presents a prompt-conditioned PSI compression system that\nintegrates prompt learning inspired by large models into the PSI compression\nprocess to address these difficulties. A hybrid prompt technique that\nintegrates soft prompt concatenation with feature-wise linear modulation (FiLM)\nfacilitates adaptive encoding across diverse signal-to-noise ratios (SNRs),\nfading kinds, and compression ratios. Furthermore, a variable rate technique\nincorporates the compression ratio into the prompt embeddings through latent\nmasking, enabling a singular model to adeptly balance reconstruction accuracy.\nAdditionally, a lightweight depthwise convolutional gating (DWCG) decoder\nfacilitates precise feature reconstruction with minimal complexity.\nComprehensive simulations indicate that the proposed framework significantly\nreduces NMSE compared to traditional autoencoder baselines, while ensuring\nrobustness across various channel circumstances and accommodating variable\ncompression ratios within a single model. These findings underscore the\nframework's promise as a scalable and efficient solution for real-time IRS\ncontrol in next-generation wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u667a\u80fd\u53cd\u5c04\u9762\u76f8\u4f4d\u4fe1\u606f\u538b\u7f29\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u63d0\u793a\u6280\u672f\u548c\u53ef\u53d8\u7387\u65b9\u6cd5\uff0c\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e94\u538b\u7f29\u548c\u7cbe\u786e\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u53cd\u5c04\u9762\u5b9e\u9645\u90e8\u7f72\u4e2d\u56e0\u9891\u7e41\u4f20\u8f93\u76f8\u4f4d\u4fe1\u606f\u5e26\u6765\u7684\u9ad8\u5f00\u9500\u95ee\u9898\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u89e3\u7801\u5668\u590d\u6742\u5ea6\u9ad8\u3001\u5bf9\u52a8\u6001\u4fe1\u9053\u9002\u5e94\u6027\u5dee\u548c\u538b\u7f29\u6bd4\u56fa\u5b9a\u7b49\u9650\u5236\u3002", "method": "\u91c7\u7528\u63d0\u793a\u5b66\u4e60\u4e0e\u7279\u5f81\u7ebf\u6027\u8c03\u5236\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u63d0\u793a\u6280\u672f\uff0c\u901a\u8fc7\u6f5c\u5728\u63a9\u7801\u5c06\u538b\u7f29\u6bd4\u5d4c\u5165\u63d0\u793a\u5411\u91cf\uff0c\u5e76\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5377\u79ef\u95e8\u63a7\u89e3\u7801\u5668\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u57fa\u7ebf\u663e\u8457\u964d\u4f4e\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff0c\u5728\u5404\u79cd\u4fe1\u9053\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5355\u4e00\u6a21\u578b\u652f\u6301\u53ef\u53d8\u538b\u7f29\u6bd4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b9e\u65f6IRS\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03757", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03757", "abs": "https://arxiv.org/abs/2511.03757", "authors": ["Xuan Ouyang", "Senan Wang", "Bouzhou Wang", "Siyuan Xiahou", "Jinrong Zhou", "Yuekang Li"], "title": "Laugh, Relate, Engage: Stylized Comment Generation for Short Videos", "comment": null, "summary": "Short-video platforms have become a central medium in the modern Internet\nlandscape, where efficient information delivery and strong interactivity are\nreshaping user engagement and cultural dissemination. Among the various forms\nof user interaction, comments play a vital role in fostering community\nparticipation and enabling content re-creation. However, generating comments\nthat are both compliant with platform guidelines and capable of exhibiting\nstylistic diversity and contextual awareness remains a significant challenge.\nWe introduce LOLGORITHM, a modular multi-agent system (MAS) designed for\ncontrollable short-video comment generation. The system integrates video\nsegmentation, contextual and affective analysis, and style-aware prompt\nconstruction. It supports six distinct comment styles: puns (homophones),\nrhyming, meme application, sarcasm (irony), plain humor, and content\nextraction. Powered by a multimodal large language model (MLLM), LOLGORITHM\ndirectly processes video inputs and achieves fine-grained style control through\nexplicit prompt markers and few-shot examples. To support development and\nevaluation, we construct a bilingual dataset using official APIs from Douyin\n(Chinese) and YouTube (English), covering five popular video genres: comedy\nskits, daily life jokes, funny animal clips, humorous commentary, and talk\nshows. Evaluation combines automated metrics originality, relevance, and style\nconformity with a large-scale human preference study involving 40 videos and\n105 participants. Results show that LOLGORITHM significantly outperforms\nbaseline models, achieving preference rates of over 90% on Douyin and 87.55% on\nYouTube. This work presents a scalable and culturally adaptive framework for\nstylized comment generation on short-video platforms, offering a promising path\nto enhance user engagement and creative interaction.", "AI": {"tldr": "LOLGORITHM\u662f\u4e00\u4e2a\u7528\u4e8e\u77ed\u89c6\u9891\u5e73\u53f0\u8bc4\u8bba\u751f\u6210\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u652f\u6301\u516d\u79cd\u4e0d\u540c\u98ce\u683c\u7684\u8bc4\u8bba\u751f\u6210\uff0c\u901a\u8fc7\u89c6\u9891\u5206\u5272\u3001\u60c5\u611f\u5206\u6790\u548c\u98ce\u683c\u611f\u77e5\u63d0\u793a\u6784\u5efa\u5b9e\u73b0\u53ef\u63a7\u8bc4\u8bba\u751f\u6210\u3002", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u4e2d\u8bc4\u8bba\u5728\u4fc3\u8fdb\u793e\u533a\u53c2\u4e0e\u548c\u5185\u5bb9\u518d\u521b\u4f5c\u65b9\u9762\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u751f\u6210\u65e2\u7b26\u5408\u5e73\u53f0\u89c4\u8303\u53c8\u5177\u6709\u98ce\u683c\u591a\u6837\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bc4\u8bba\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u96c6\u6210\u89c6\u9891\u5206\u5272\u3001\u4e0a\u4e0b\u6587\u548c\u60c5\u611f\u5206\u6790\u3001\u98ce\u683c\u611f\u77e5\u63d0\u793a\u6784\u5efa\uff0c\u652f\u6301\u516d\u79cd\u8bc4\u8bba\u98ce\u683c\uff0c\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u5904\u7406\u89c6\u9891\u8f93\u5165\u3002", "result": "\u5728\u6296\u97f3\u548cYouTube\u5e73\u53f0\u4e0a\uff0cLOLGORITHM\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u504f\u597d\u7387\u5206\u522b\u8d85\u8fc790%\u548c87.55%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u77ed\u89c6\u9891\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u6587\u5316\u9002\u5e94\u7684\u98ce\u683c\u5316\u8bc4\u8bba\u751f\u6210\u6846\u67b6\uff0c\u6709\u671b\u589e\u5f3a\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u521b\u610f\u4e92\u52a8\u3002"}}
{"id": "2511.03797", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.03797", "abs": "https://arxiv.org/abs/2511.03797", "authors": ["Aimee Maurais", "Bamdad Hosseini", "Youssef Marzouk"], "title": "Learning Paths for Dynamic Measure Transport: A Control Perspective", "comment": "To appear at NeurIPS 2025 Workshop on Frontiers of Probabilistic\n  Inference: Sampling Meets Learning", "summary": "We bring a control perspective to the problem of identifying paths of\nmeasures for sampling via dynamic measure transport (DMT). We highlight the\nfact that commonly used paths may be poor choices for DMT and connect existing\nmethods for learning alternate paths to mean-field games. Based on these\nconnections we pose a flexible family of optimization problems for identifying\ntilted paths of measures for DMT and advocate for the use of objective terms\nwhich encourage smoothness of the corresponding velocities. We present a\nnumerical algorithm for solving these problems based on recent Gaussian process\nmethods for solution of partial differential equations and demonstrate the\nability of our method to recover more efficient and smooth transport models\ncompared to those which use an untilted reference path.", "AI": {"tldr": "\u672c\u6587\u4ece\u63a7\u5236\u8bba\u89d2\u5ea6\u7814\u7a76\u52a8\u6001\u6d4b\u5ea6\u4f20\u8f93\u4e2d\u7684\u6d4b\u5ea6\u8def\u5f84\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u51fa\u4f7f\u7528\u503e\u659c\u6d4b\u5ea6\u8def\u5f84\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u6765\u83b7\u5f97\u66f4\u5e73\u6ed1\u9ad8\u6548\u7684\u4f20\u8f93\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u6d4b\u5ea6\u4f20\u8f93\u65b9\u6cd5\u4e2d\u5e38\u7528\u7684\u6d4b\u5ea6\u8def\u5f84\u9009\u62e9\u4e0d\u4f73\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u4f18\u7684\u8def\u5f84\u6765\u6539\u8fdb\u91c7\u6837\u6548\u7387\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e73\u5747\u573a\u535a\u5f08\u7684\u6d4b\u5ea6\u8def\u5f84\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u503e\u659c\u6d4b\u5ea6\u8def\u5f84\u548c\u9f13\u52b1\u901f\u5ea6\u5e73\u6ed1\u6027\u7684\u76ee\u6807\u51fd\u6570\uff0c\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\u6c42\u89e3\u76f8\u5173\u504f\u5fae\u5206\u65b9\u7a0b\u3002", "result": "\u76f8\u6bd4\u672a\u503e\u659c\u53c2\u8003\u8def\u5f84\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6062\u590d\u51fa\u66f4\u9ad8\u6548\u548c\u5e73\u6ed1\u7684\u4f20\u8f93\u6a21\u578b\u3002", "conclusion": "\u4ece\u63a7\u5236\u8bba\u89c6\u89d2\u63d0\u51fa\u7684\u503e\u659c\u6d4b\u5ea6\u8def\u5f84\u4f18\u5316\u65b9\u6cd5\u80fd\u663e\u8457\u6539\u8fdb\u52a8\u6001\u6d4b\u5ea6\u4f20\u8f93\u7684\u6027\u80fd\uff0c\u83b7\u5f97\u66f4\u597d\u7684\u91c7\u6837\u6548\u679c\u3002"}}
{"id": "2511.03967", "categories": ["eess.SP", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03967", "abs": "https://arxiv.org/abs/2511.03967", "authors": ["Wuxia Chen", "Sean Moushegian", "Vahid Tarokh", "Taposh Banerjee"], "title": "Score-Based Quickest Change Detection and Fault Identification for Multi-Stream Signals", "comment": null, "summary": "This paper introduces an approach to multi-stream quickest change detection\nand fault isolation for unnormalized and score-based statistical models.\nTraditional optimal algorithms in the quickest change detection literature\nrequire explicit pre-change and post-change distributions to calculate the\nlikelihood ratio of the observations, which can be computationally expensive\nfor higher-dimensional data and sometimes even infeasible for complex machine\nlearning models. To address these challenges, we propose the min-SCUSUM method,\na Hyvarinen score-based algorithm that computes the difference of score\nfunctions in place of log-likelihood ratios. We provide a delay and false alarm\nanalysis of the proposed algorithm, showing that its asymptotic performance\ndepends on the Fisher divergence between the pre- and post-change\ndistributions. Furthermore, we establish an upper bound on the probability of\nfault misidentification in distinguishing the affected stream from the\nunaffected ones.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eHyvarinen\u5206\u6570\u7684min-SCUSUM\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6d41\u5feb\u901f\u53d8\u5316\u68c0\u6d4b\u548c\u6545\u969c\u9694\u79bb\uff0c\u65e0\u9700\u663e\u5f0f\u5206\u5e03\u5373\u53ef\u8ba1\u7b97\u5206\u6570\u51fd\u6570\u5dee\u5f02\u66ff\u4ee3\u4f3c\u7136\u6bd4\u3002", "motivation": "\u4f20\u7edf\u6700\u4f18\u7b97\u6cd5\u9700\u8981\u663e\u5f0f\u7684\u524d\u540e\u53d8\u5316\u5206\u5e03\u6765\u8ba1\u7b97\u89c2\u6d4b\u503c\u7684\u4f3c\u7136\u6bd4\uff0c\u5bf9\u9ad8\u7ef4\u6570\u636e\u548c\u590d\u6742\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8ba1\u7b97\u6602\u8d35\u751a\u81f3\u4e0d\u53ef\u884c\u3002", "method": "min-SCUSUM\u65b9\u6cd5\uff0c\u57fa\u4e8eHyvarinen\u5206\u6570\u7684\u7b97\u6cd5\uff0c\u8ba1\u7b97\u5206\u6570\u51fd\u6570\u5dee\u5f02\u800c\u975e\u5bf9\u6570\u4f3c\u7136\u6bd4\u3002", "result": "\u7b97\u6cd5\u6e10\u8fd1\u6027\u80fd\u53d6\u51b3\u4e8e\u524d\u540e\u53d8\u5316\u5206\u5e03\u4e4b\u95f4\u7684Fisher\u6563\u5ea6\uff0c\u5efa\u7acb\u4e86\u6545\u969c\u8bef\u8bc6\u522b\u6982\u7387\u7684\u4e0a\u754c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u7edf\u8ba1\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5feb\u901f\u53d8\u5316\u68c0\u6d4b\u548c\u6545\u969c\u9694\u79bb\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03768", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03768", "abs": "https://arxiv.org/abs/2511.03768", "authors": ["Candace Ross", "Florian Bordes", "Adina Williams", "Polina Kirichenko", "Mark Ibrahim"], "title": "What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes", "comment": "10 pages, 6 figures. Accepted to NeurIPS Datasets & Benchmarks 2025", "summary": "Multimodal language models possess a remarkable ability to handle an\nopen-vocabulary's worth of objects. Yet the best models still suffer from\nhallucinations when reasoning about scenes in the real world, revealing a gap\nbetween their seemingly strong performance on existing perception benchmarks\nthat are saturating and their reasoning in the real world. To address this gap,\nwe build a novel benchmark of in-the-wild scenes that we call Common-O. With\nmore than 10.5k examples using exclusively new images not found in web training\ndata to avoid contamination, Common-O goes beyond just perception, inspired by\ncognitive tests for humans, to probe reasoning across scenes by asking \"what's\nin common?\". We evaluate leading multimodal language models, including models\nspecifically trained to perform chain-of-thought reasoning. We find that\nperceiving objects in single images is tractable for most models, yet reasoning\nacross scenes is very challenging even for the best models, including reasoning\nmodels. Despite saturating many leaderboards focusing on perception, the best\nperforming model only achieves 35% on Common-O -- and on Common-O Complex,\nconsisting of more complex scenes, the best model achieves only 1%. Curiously,\nwe find models are more prone to hallucinate when similar objects are present\nin the scene, suggesting models may be relying on object co-occurrence seen\nduring training. Among the models we evaluated, we found scale can provide\nmodest improvements while models explicitly trained with multi-image inputs\nshow bigger improvements, suggesting scaled multi-image training may offer\npromise. We make our benchmark publicly available to spur research into the\nchallenge of hallucination when reasoning across scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCommon-O\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u4f73\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u63a8\u7406\u4e2d\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u63a8\u7406\u4e2d\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u63ed\u793a\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u6784\u5efa\u5305\u542b10,500\u591a\u4e2a\u5168\u65b0\u56fe\u50cf\u7684Common-O\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\"\u5171\u540c\u70b9\u662f\u4ec0\u4e48\"\u7684\u95ee\u9898\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u8de8\u573a\u666f\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u611f\u77e5\u548c\u63a8\u7406\u4e24\u4e2a\u5c42\u9762\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728Common-O\u4e0a\u4ec5\u8fbe\u523035%\u51c6\u786e\u7387\uff0c\u5728\u66f4\u590d\u6742\u7684Common-O Complex\u4e0a\u4ec51%\uff1b\u6a21\u578b\u5728\u9762\u5bf9\u76f8\u4f3c\u7269\u4f53\u65f6\u66f4\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u53ef\u80fd\u8fc7\u5ea6\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7269\u4f53\u5171\u73b0\u6a21\u5f0f\u3002", "conclusion": "\u591a\u6a21\u6001\u6a21\u578b\u5728\u8de8\u573a\u666f\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u6311\u6218\uff0c\u591a\u56fe\u50cf\u8bad\u7ec3\u53ef\u80fd\u63d0\u4f9b\u6539\u8fdb\u65b9\u5411\uff0c\u8be5\u57fa\u51c6\u6d4b\u8bd5\u65e8\u5728\u63a8\u52a8\u89e3\u51b3\u573a\u666f\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u7814\u7a76\u3002"}}
{"id": "2511.03892", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03892", "abs": "https://arxiv.org/abs/2511.03892", "authors": ["Chiraag Kaushik", "Justin Romberg", "Vidya Muthukumar"], "title": "A general technique for approximating high-dimensional empirical kernel matrices", "comment": "32 pages", "summary": "We present simple, user-friendly bounds for the expected operator norm of a\nrandom kernel matrix under general conditions on the kernel function\n$k(\\cdot,\\cdot)$. Our approach uses decoupling results for U-statistics and the\nnon-commutative Khintchine inequality to obtain upper and lower bounds\ndepending only on scalar statistics of the kernel function and a ``correlation\nkernel'' matrix corresponding to $k(\\cdot,\\cdot)$. We then apply our method to\nprovide new, tighter approximations for inner-product kernel matrices on\ngeneral high-dimensional data, where the sample size and data dimension are\npolynomially related. Our method obtains simplified proofs of existing results\nthat rely on the moment method and combinatorial arguments while also providing\nnovel approximation results for the case of anisotropic Gaussian data. Finally,\nusing similar techniques to our approximation result, we show a tighter lower\nbound on the bias of kernel regression with anisotropic Gaussian data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u968f\u673a\u6838\u77e9\u9635\u671f\u671b\u7b97\u5b50\u8303\u6570\u7684\u7b80\u5355\u7528\u6237\u53cb\u597d\u8fb9\u754c\uff0c\u4f7f\u7528U\u7edf\u8ba1\u91cf\u7684\u89e3\u8026\u7ed3\u679c\u548c\u975e\u4ea4\u6362Khint\u8f9b\u4e0d\u7b49\u5f0f\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u6838\u51fd\u6570\u7684\u6807\u91cf\u7edf\u8ba1\u91cf\u548c\u76f8\u5173\u6838\u77e9\u9635\u3002", "motivation": "\u4e3a\u968f\u673a\u6838\u77e9\u9635\u7684\u671f\u671b\u7b97\u5b50\u8303\u6570\u63d0\u4f9b\u66f4\u7b80\u5355\u3001\u66f4\u7d27\u7684\u8fb9\u754c\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u6570\u636e\u573a\u666f\u4e0b\uff0c\u7b80\u5316\u73b0\u6709\u7ed3\u679c\u7684\u8bc1\u660e\u5e76\u63d0\u4f9b\u65b0\u7684\u8fd1\u4f3c\u7ed3\u679c\u3002", "method": "\u4f7f\u7528U\u7edf\u8ba1\u91cf\u7684\u89e3\u8026\u7ed3\u679c\u548c\u975e\u4ea4\u6362Khint\u8f9b\u4e0d\u7b49\u5f0f\uff0c\u901a\u8fc7\u6838\u51fd\u6570\u7684\u6807\u91cf\u7edf\u8ba1\u91cf\u548c\u76f8\u5173\u6838\u77e9\u9635\u6765\u83b7\u5f97\u4e0a\u4e0b\u754c\u3002", "result": "\u83b7\u5f97\u4e86\u4ec5\u4f9d\u8d56\u4e8e\u6838\u51fd\u6570\u6807\u91cf\u7edf\u8ba1\u91cf\u548c\u76f8\u5173\u6838\u77e9\u9635\u7684\u4e0a\u4e0b\u754c\uff0c\u4e3a\u9ad8\u7ef4\u5185\u79ef\u6838\u77e9\u9635\u63d0\u4f9b\u4e86\u66f4\u7d27\u7684\u8fd1\u4f3c\uff0c\u5e76\u5bf9\u5404\u5411\u5f02\u6027\u9ad8\u65af\u6570\u636e\u7684\u6838\u56de\u5f52\u504f\u5dee\u7ed9\u51fa\u4e86\u66f4\u7d27\u7684\u4e0b\u754c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u73b0\u6709\u7ed3\u679c\u7684\u8bc1\u660e\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5404\u5411\u5f02\u6027\u9ad8\u65af\u6570\u636e\u7684\u65b0\u8fd1\u4f3c\u7ed3\u679c\uff0c\u5e76\u5728\u6838\u56de\u5f52\u504f\u5dee\u5206\u6790\u4e2d\u83b7\u5f97\u4e86\u66f4\u7d27\u7684\u4e0b\u754c\u3002"}}
{"id": "2511.03984", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03984", "abs": "https://arxiv.org/abs/2511.03984", "authors": ["Hanfu Zhang", "Erwu Liu"], "title": "Joint Beamforming and Position Design for Movable Antenna Assisted LEO ISAC Systems", "comment": "13 pages, 8 figures", "summary": "Low earth orbit (LEO) satellite-assisted integrated sensing and\ncommunications (ISAC) systems have been extensively studied to achieve\nubiquitous connectivity. However, the severe signal attenuation and limited\ntransmit power at LEO satellites can degrade ISAC performance. To address this\nissue, this paper investigated movable antenna (MA)-assisted LEO ISAC systems.\nWe derive the communication signal-to-interference-plus-noise ratio (SINR) and\nthe sensing squared position error bound (SPEB) for evaluating the ISAC\nperformance. Then, we jointly optimize the transmit beamforming and the MA\npositions to minimize the SPEB under the SINR constraints, total transmit power\nconstraint, and several inherent physical constraints of the MA array. We first\nsimplify the complex problem using the semidefinite relaxation (SDR). Then, we\npresent a novel alternating optimization (AO)-based algorithm to decouple the\noriginal problem into two subproblems, consequently convexified and solved.\nSimulations demonstrate the convergence and effectiveness of the proposed\nalgorithm. Better trade-off between communication and sensing performance, and\nat least 25% gain in sensing performance are achieved, compared to the\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53ef\u79fb\u52a8\u5929\u7ebf\u8f85\u52a9\u7684\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u548c\u5929\u7ebf\u4f4d\u7f6e\uff0c\u5728\u6ee1\u8db3\u901a\u4fe1SINR\u7ea6\u675f\u548c\u603b\u53d1\u5c04\u529f\u7387\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u611f\u77e5\u6027\u80fd\u7684\u5e73\u65b9\u4f4d\u7f6e\u8bef\u5dee\u754c\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u8f85\u52a9\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u5b58\u5728\u4e25\u91cd\u7684\u4fe1\u53f7\u8870\u51cf\u548c\u6709\u9650\u7684\u53d1\u5c04\u529f\u7387\u95ee\u9898\uff0c\u8fd9\u4f1a\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63a2\u7d22\u4e86\u53ef\u79fb\u52a8\u5929\u7ebf\u6280\u672f\u7684\u5e94\u7528\u3002", "method": "\u9996\u5148\u4f7f\u7528\u534a\u5b9a\u677e\u5f1b\u7b80\u5316\u590d\u6742\u95ee\u9898\uff0c\u7136\u540e\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4ea4\u66ff\u4f18\u5316\u7684\u7b97\u6cd5\uff0c\u5c06\u539f\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u51f8\u5316\u5904\u7406\u5e76\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u6240\u63d0\u7b97\u6cd5\u5177\u6709\u6536\u655b\u6027\u548c\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5\uff0c\u5728\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6743\u8861\uff0c\u611f\u77e5\u6027\u80fd\u81f3\u5c11\u63d0\u5347\u4e8625%\u3002", "conclusion": "\u53ef\u79fb\u52a8\u5929\u7ebf\u6280\u672f\u80fd\u591f\u6709\u6548\u63d0\u5347\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u6027\u80fd\u65b9\u9762\u6709\u663e\u8457\u6539\u5584\u3002"}}
{"id": "2511.03774", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03774", "abs": "https://arxiv.org/abs/2511.03774", "authors": ["Jaden Park", "Mu Cai", "Feng Yao", "Jingbo Shang", "Soochahn Lee", "Yong Jae Lee"], "title": "Contamination Detection for VLMs using Multi-Modal Semantic Perturbation", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have achieved\nstate-of-the-art performance on numerous benchmark tasks. However, the use of\ninternet-scale, often proprietary, pretraining corpora raises a critical\nconcern for both practitioners and users: inflated performance due to test-set\nleakage. While prior works have proposed mitigation strategies such as\ndecontamination of pretraining data and benchmark redesign for LLMs, the\ncomplementary direction of developing detection methods for contaminated VLMs\nremains underexplored. To address this gap, we deliberately contaminate\nopen-source VLMs on popular benchmarks and show that existing detection\napproaches either fail outright or exhibit inconsistent behavior. We then\npropose a novel simple yet effective detection method based on multi-modal\nsemantic perturbation, demonstrating that contaminated models fail to\ngeneralize under controlled perturbations. Finally, we validate our approach\nacross multiple realistic contamination strategies, confirming its robustness\nand effectiveness. The code and perturbed dataset will be released publicly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8bed\u4e49\u6270\u52a8\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6d4b\u8bd5\u96c6\u6c61\u67d3\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5931\u6548\u6216\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5b58\u5728\u6d4b\u8bd5\u96c6\u6cc4\u9732\u5bfc\u81f4\u6027\u80fd\u865a\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u6c61\u67d3\u68c0\u6d4b\u65b9\u6cd5\u5728VLMs\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u6545\u610f\u6c61\u67d3\u5f00\u6e90VLMs\uff0c\u7136\u540e\u4f7f\u7528\u591a\u6a21\u6001\u8bed\u4e49\u6270\u52a8\u65b9\u6cd5\u8fdb\u884c\u68c0\u6d4b\uff0c\u53d1\u73b0\u53d7\u6c61\u67d3\u6a21\u578b\u5728\u53d7\u63a7\u6270\u52a8\u4e0b\u65e0\u6cd5\u6cdb\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u73b0\u5b9e\u6c61\u67d3\u7b56\u7565\u4e0b\u90fd\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4b\u51fa\u53d7\u6c61\u67d3\u7684\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u8bed\u4e49\u6270\u52a8\u68c0\u6d4b\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u4e3a\u89e3\u51b3VLMs\u6d4b\u8bd5\u96c6\u6c61\u67d3\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03952", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03952", "abs": "https://arxiv.org/abs/2511.03952", "authors": ["Aukosh Jagannath", "Taj Jones-McCormick", "Varnan Sarangian"], "title": "High-dimensional limit theorems for SGD: Momentum and Adaptive Step-sizes", "comment": null, "summary": "We develop a high-dimensional scaling limit for Stochastic Gradient Descent\nwith Polyak Momentum (SGD-M) and adaptive step-sizes. This provides a framework\nto rigourously compare online SGD with some of its popular variants. We show\nthat the scaling limits of SGD-M coincide with those of online SGD after an\nappropriate time rescaling and a specific choice of step-size. However, if the\nstep-size is kept the same between the two algorithms, SGD-M will amplify\nhigh-dimensional effects, potentially degrading performance relative to online\nSGD. We demonstrate our framework on two popular learning problems: Spiked\nTensor PCA and Single Index Models. In both cases, we also examine online SGD\nwith an adaptive step-size based on normalized gradients. In the\nhigh-dimensional regime, this algorithm yields multiple benefits: its dynamics\nadmit fixed points closer to the population minimum and widens the range of\nadmissible step-sizes for which the iterates converge to such solutions. These\nexamples provide a rigorous account, aligning with empirical motivation, of how\nearly preconditioners can stabilize and improve dynamics in settings where\nonline SGD fails.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u5e26Polyak\u52a8\u91cf\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d(SGD-M)\u7684\u9ad8\u7ef4\u7f29\u653e\u6781\u9650\uff0c\u5e76\u4e0e\u666e\u901a\u5728\u7ebfSGD\u8fdb\u884c\u6bd4\u8f83\uff0c\u53d1\u73b0SGD-M\u5728\u76f8\u540c\u6b65\u957f\u4e0b\u4f1a\u653e\u5927\u9ad8\u7ef4\u6548\u5e94\uff0c\u800c\u81ea\u9002\u5e94\u6b65\u957f\u7b97\u6cd5\u5728\u5f20\u91cfPCA\u548c\u5355\u6307\u6807\u6a21\u578b\u95ee\u9898\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u4e3a\u4e25\u683c\u6bd4\u8f83\u5728\u7ebfSGD\u53ca\u5176\u53d8\u4f53\u7b97\u6cd5\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5206\u6790SGD-M\u4e0e\u666e\u901aSGD\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u7684\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u5efa\u7acbSGD-M\u7684\u9ad8\u7ef4\u7f29\u653e\u6781\u9650\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u91cd\u7f29\u653e\u548c\u7279\u5b9a\u6b65\u957f\u9009\u62e9\uff0c\u5c06SGD-M\u7684\u7f29\u653e\u6781\u9650\u4e0e\u5728\u7ebfSGD\u5bf9\u9f50\uff0c\u5e76\u5728\u5f20\u91cfPCA\u548c\u5355\u6307\u6807\u6a21\u578b\u95ee\u9898\u4e0a\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u53d1\u73b0SGD-M\u5728\u76f8\u540c\u6b65\u957f\u4e0b\u4f1a\u653e\u5927\u9ad8\u7ef4\u6548\u5e94\uff0c\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\uff1b\u800c\u57fa\u4e8e\u5f52\u4e00\u5316\u68af\u5ea6\u7684\u81ea\u9002\u5e94\u6b65\u957f\u7b97\u6cd5\u80fd\u4ea7\u751f\u66f4\u63a5\u8fd1\u603b\u4f53\u6700\u5c0f\u503c\u7684\u56fa\u5b9a\u70b9\uff0c\u5e76\u6269\u5927\u6536\u655b\u6b65\u957f\u8303\u56f4\u3002", "conclusion": "\u65e9\u671f\u9884\u6761\u4ef6\u5668\u53ef\u4ee5\u5728\u5728\u7ebfSGD\u5931\u8d25\u7684\u573a\u666f\u4e2d\u7a33\u5b9a\u548c\u6539\u8fdb\u52a8\u6001\u6027\u80fd\uff0c\u4e3a\u81ea\u9002\u5e94\u6b65\u957f\u7b97\u6cd5\u7684\u4f18\u52bf\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2511.03998", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03998", "abs": "https://arxiv.org/abs/2511.03998", "authors": ["Abhishek Rajasekaran", "Mehdi Karbalayghareh", "Xiaoyan Ma", "David J. Love", "Christopher G. Brinton"], "title": "Optimal RIS Placement in a Multi-User MISO System with User Randomness", "comment": "6 pages, 3 figures", "summary": "It is well established that the performance of reconfigurable intelligent\nsurface (RIS)-assisted systems critically depends on the optimal placement of\nthe RIS. Previous works consider either simple coverage maximization or\nsimultaneous optimization of the placement of the RIS along with the\nbeamforming and reflection coefficients, most of which assume that the location\nof the RIS, base station (BS), and users are known. However, in practice, only\nthe spatial variation of user density and obstacle configuration are likely to\nbe known prior to deployment of the system. Thus, we formulate a non-convex\nproblem that optimizes the position of the RIS over the expected minimum\nsignal-to-interference-plus-noise ratio (SINR) of the system with user\nrandomness, assuming that the system employs joint beamforming after\ndeployment. To solve this problem, we propose a recursive coarse-to-fine\nmethodology that constructs a set of candidate locations for RIS placement\nbased on the obstacle configuration and evaluates them over multiple\ninstantiations from the user distribution. The search is recursively refined\nwithin the optimal region identified in each stage to determine the final\noptimal region for RIS deployment. Numerical results are presented to\ncorroborate our findings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7c97\u5230\u7ec6\u7684\u9012\u5f52\u65b9\u6cd5\u6765\u4f18\u5316\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(RIS)\u7684\u4f4d\u7f6e\u90e8\u7f72\uff0c\u8003\u8651\u7528\u6237\u968f\u673a\u6027\u548c\u969c\u788d\u7269\u914d\u7f6e\uff0c\u4ee5\u6700\u5927\u5316\u7cfb\u7edf\u7684\u6700\u5c0f\u671f\u671b\u4fe1\u5e72\u566a\u6bd4(SINR)\u3002", "motivation": "\u73b0\u6709RIS\u8f85\u52a9\u7cfb\u7edf\u7814\u7a76\u5927\u591a\u5047\u8bbe\u7528\u6237\u548c\u57fa\u7ad9\u4f4d\u7f6e\u5df2\u77e5\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u524d\u901a\u5e38\u53ea\u77e5\u9053\u7528\u6237\u5bc6\u5ea6\u5206\u5e03\u548c\u969c\u788d\u7269\u914d\u7f6e\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316RIS\u4f4d\u7f6e\u6765\u5e94\u5bf9\u7528\u6237\u968f\u673a\u6027\u3002", "method": "\u63d0\u51fa\u9012\u5f52\u4ece\u7c97\u5230\u7ec6\u7684\u65b9\u6cd5\uff1a\u57fa\u4e8e\u969c\u788d\u7269\u914d\u7f6e\u6784\u5efa\u5019\u9009\u4f4d\u7f6e\u96c6\u5408\uff0c\u901a\u8fc7\u591a\u8f6e\u7528\u6237\u5206\u5e03\u5b9e\u4f8b\u8bc4\u4f30\uff0c\u5728\u6bcf\u9636\u6bb5\u8bc6\u522b\u6700\u4f18\u533a\u57df\u5e76\u9012\u5f52\u7ec6\u5316\u641c\u7d22\uff0c\u6700\u7ec8\u786e\u5b9aRIS\u90e8\u7f72\u7684\u6700\u4f73\u533a\u57df\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u627e\u5230\u5728\u7528\u6237\u968f\u673a\u6027\u6761\u4ef6\u4e0b\u7684\u6700\u4f18RIS\u90e8\u7f72\u4f4d\u7f6e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e2dRIS\u4f4d\u7f6e\u4f18\u5316\u95ee\u9898\uff0c\u4e3aRIS\u8f85\u52a9\u901a\u4fe1\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03806", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03806", "abs": "https://arxiv.org/abs/2511.03806", "authors": ["Linghui Zeng", "Ruixuan Liu", "Atiquer Rahman Sarkar", "Xiaoqian Jiang", "Joyce C. Ho", "Li Xiong"], "title": "FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features", "comment": null, "summary": "Ensuring the privacy of sensitive training data is crucial in\nprivacy-preserving machine learning. However, in practical scenarios, privacy\nprotection may be required for only a subset of features. For instance, in ICU\ndata, demographic attributes like age and gender pose higher privacy risks due\nto their re-identification potential, whereas raw lab results are generally\nless sensitive. Traditional DP-SGD enforces privacy protection on all features\nin one sample, leading to excessive noise injection and significant utility\ndegradation. We propose FusionDP, a two-step framework that enhances model\nutility under feature-level differential privacy. First, FusionDP leverages\nlarge foundation models to impute sensitive features given non-sensitive\nfeatures, treating them as external priors that provide high-quality estimates\nof sensitive attributes without accessing the true values during model\ntraining. Second, we introduce a modified DP-SGD algorithm that trains models\non both original and imputed features while formally preserving the privacy of\nthe original sensitive features. We evaluate FusionDP on two modalities: a\nsepsis prediction task on tabular data from PhysioNet and a clinical note\nclassification task from MIMIC-III. By comparing against privacy-preserving\nbaselines, our results show that FusionDP significantly improves model\nperformance while maintaining rigorous feature-level privacy, demonstrating the\npotential of foundation model-driven imputation to enhance the privacy-utility\ntrade-off for various modalities.", "AI": {"tldr": "FusionDP\u662f\u4e00\u4e2a\u7279\u5f81\u7ea7\u5dee\u5206\u9690\u79c1\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u5bf9\u654f\u611f\u7279\u5f81\u8fdb\u884c\u4f30\u7b97\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfDP-SGD\u5bf9\u6240\u6709\u7279\u5f81\u7edf\u4e00\u65bd\u52a0\u9690\u79c1\u4fdd\u62a4\uff0c\u5bfc\u81f4\u566a\u58f0\u6ce8\u5165\u8fc7\u591a\u548c\u6548\u7528\u4e0b\u964d\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u53ea\u6709\u90e8\u5206\u654f\u611f\u7279\u5f81\u9700\u8981\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u4e24\u6b65\u6cd5\uff1a1) \u4f7f\u7528\u57fa\u7840\u6a21\u578b\u6839\u636e\u975e\u654f\u611f\u7279\u5f81\u4f30\u7b97\u654f\u611f\u7279\u5f81\uff1b2) \u6539\u8fdb\u7684DP-SGD\u7b97\u6cd5\uff0c\u5728\u539f\u59cb\u7279\u5f81\u548c\u4f30\u7b97\u7279\u5f81\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u62a4\u539f\u59cb\u654f\u611f\u7279\u5f81\u7684\u9690\u79c1\u3002", "result": "\u5728PhysioNet\u8d25\u8840\u75c7\u9884\u6d4b\u548cMIMIC-III\u4e34\u5e8a\u7b14\u8bb0\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0cFusionDP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FusionDP\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u9a71\u52a8\u7684\u4f30\u7b97\u65b9\u6cd5\uff0c\u6709\u6548\u6539\u5584\u4e86\u5404\u79cd\u6a21\u6001\u6570\u636e\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u3002"}}
{"id": "2511.03963", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03963", "abs": "https://arxiv.org/abs/2511.03963", "authors": ["Shinto Eguchi"], "title": "Robust inference using density-powered Stein operators", "comment": null, "summary": "We introduce a density-power weighted variant for the Stein operator, called\nthe $\\gamma$-Stein operator. This is a novel class of operators derived from\nthe $\\gamma$-divergence, designed to build robust inference methods for\nunnormalized probability models. The operator's construction (weighting by the\nmodel density raised to a positive power $\\gamma$ inherently down-weights the\ninfluence of outliers, providing a principled mechanism for robustness.\nApplying this operator yields a robust generalization of score matching that\nretains the crucial property of being independent of the model's normalizing\nconstant. We extend this framework to develop two key applications: the\n$\\gamma$-kernelized Stein discrepancy for robust goodness-of-fit testing, and\n$\\gamma$-Stein variational gradient descent for robust Bayesian posterior\napproximation. Empirical results on contaminated Gaussian and quartic potential\nmodels show our methods significantly outperform standard baselines in both\nrobustness and statistical efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u03b3-\u6563\u5ea6\u7684\u5bc6\u5ea6\u52a0\u6743Stein\u7b97\u5b50\u53d8\u4f53\u2014\u2014\u03b3-Stein\u7b97\u5b50\uff0c\u7528\u4e8e\u6784\u5efa\u975e\u5f52\u4e00\u5316\u6982\u7387\u6a21\u578b\u7684\u9c81\u68d2\u63a8\u65ad\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u9c81\u68d2\u5f97\u5206\u5339\u914d\u3001\u6838\u5316Stein\u5dee\u5f02\u548cStein\u53d8\u5206\u68af\u5ea6\u4e0b\u964d\u7b49\u5e94\u7528\u3002", "motivation": "\u73b0\u6709Stein\u7b97\u5b50\u65b9\u6cd5\u5bf9\u5f02\u5e38\u503c\u654f\u611f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u7136\u6291\u5236\u5f02\u5e38\u503c\u5f71\u54cd\u7684\u9c81\u68d2\u63a8\u65ad\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u6a21\u578b\u5f52\u4e00\u5316\u5e38\u6570\u7684\u72ec\u7acb\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u6a21\u578b\u5bc6\u5ea6\u63d0\u5347\u5230\u6b63\u5e42\u03b3\u8fdb\u884c\u52a0\u6743\uff0c\u6784\u5efa\u03b3-Stein\u7b97\u5b50\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1\u9c81\u68d2\u5f97\u5206\u5339\u914d\u3001\u03b3-\u6838\u5316Stein\u5dee\u5f02\u548c\u03b3-Stein\u53d8\u5206\u68af\u5ea6\u4e0b\u964d\u3002", "result": "\u5728\u6c61\u67d3\u9ad8\u65af\u548c\u56db\u6b21\u52bf\u80fd\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u7edf\u8ba1\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u03b3-Stein\u7b97\u5b50\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u9c81\u68d2\u63a8\u65ad\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u5e38\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5f52\u4e00\u5316\u5e38\u6570\u7684\u4e0d\u4f9d\u8d56\u6027\u3002"}}
{"id": "2511.04011", "categories": ["eess.SP", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.04011", "abs": "https://arxiv.org/abs/2511.04011", "authors": ["Higo T. P. Da Silva", "Hugerles S. Silva", "Felipe A. P. Figueiredo", "Andre A. Dos Anjos", "Rausley A. A. Souza"], "title": "A Survey on Noise-Based Communication", "comment": null, "summary": "The proliferation of sixth-generation (6G) networks and the massive Internet\nof Things (IoT) demand wireless communication technologies that are\nultra-low-power, secure, and covert. Noise-based communication has emerged as a\ntransformative paradigm that meets these demands by encoding information\ndirectly into the statistical properties of noise, rather than using\ntraditional deterministic carriers. This survey provides a comprehensive\nsynthesis of this field, systematically exploring its fundamental principles\nand key methodologies, including thermal noise modulation (TherMod), noise\nmodulation (NoiseMod) and its variants, and the Kirchhoff-law-Johnson-noise\n(KLJN) secure key exchange. We address critical practical challenges such as\nchannel estimation and hardware implementation, and highlight emerging\napplications in simultaneous wireless information and power transfer (SWIPT)\nand non-orthogonal multiple access (NOMA). Our analysis confirms that\nnoise-based systems offer unparalleled advantages in energy efficiency and\ncovertness, and we conclude by outlining future research directions to realize\ntheir potential for enabling the next generation of autonomous and secure\nwireless networks.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u566a\u58f0\u7684\u901a\u4fe1\u6280\u672f\uff0c\u8fd9\u662f\u4e00\u79cd\u57286G\u548c\u7269\u8054\u7f51\u80cc\u666f\u4e0b\u6ee1\u8db3\u8d85\u4f4e\u529f\u8017\u3001\u5b89\u5168\u548c\u9690\u853d\u9700\u6c42\u7684\u53d8\u9769\u6027\u901a\u4fe1\u8303\u5f0f\u3002", "motivation": "\u7b2c\u516d\u4ee3\u7f51\u7edc\u548c\u5927\u89c4\u6a21\u7269\u8054\u7f51\u7684\u53d1\u5c55\u9700\u8981\u8d85\u4f4e\u529f\u8017\u3001\u5b89\u5168\u548c\u9690\u853d\u7684\u65e0\u7ebf\u901a\u4fe1\u6280\u672f\uff0c\u4f20\u7edf\u786e\u5b9a\u6027\u8f7d\u6ce2\u901a\u4fe1\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u63a2\u7d22\u4e86\u57fa\u4e8e\u566a\u58f0\u901a\u4fe1\u7684\u57fa\u672c\u539f\u7406\u548c\u5173\u952e\u65b9\u6cd5\uff0c\u5305\u62ec\u70ed\u566a\u58f0\u8c03\u5236\u3001\u566a\u58f0\u8c03\u5236\u53ca\u5176\u53d8\u4f53\u3001\u57fa\u5c14\u970d\u592b\u5b9a\u5f8b-\u7ea6\u7ff0\u900a\u566a\u58f0\u5b89\u5168\u5bc6\u94a5\u4ea4\u6362\u7b49\u3002", "result": "\u5206\u6790\u786e\u8ba4\u57fa\u4e8e\u566a\u58f0\u7684\u7cfb\u7edf\u5728\u80fd\u6548\u548c\u9690\u853d\u6027\u65b9\u9762\u5177\u6709\u65e0\u4e0e\u4f26\u6bd4\u7684\u4f18\u52bf\uff0c\u5e76\u63a2\u8ba8\u4e86\u5728\u540c\u65f6\u65e0\u7ebf\u4fe1\u606f\u548c\u80fd\u91cf\u4f20\u8f93\u3001\u975e\u6b63\u4ea4\u591a\u5740\u63a5\u5165\u7b49\u65b0\u5174\u5e94\u7528\u3002", "conclusion": "\u57fa\u4e8e\u566a\u58f0\u7684\u901a\u4fe1\u4e3a\u5b9e\u73b0\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u5b89\u5168\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u89e3\u51b3\u5b9e\u9645\u6311\u6218\u5e76\u5145\u5206\u53d1\u6325\u5176\u4f18\u52bf\u3002"}}
{"id": "2511.03807", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03807", "abs": "https://arxiv.org/abs/2511.03807", "authors": ["Shivogo John"], "title": "Fair and Explainable Credit-Scoring under Concept Drift: Adaptive Explanation Frameworks for Evolving Populations", "comment": "18 pages, 14 figures", "summary": "Evolving borrower behaviors, shifting economic conditions, and changing\nregulatory landscapes continuously reshape the data distributions underlying\nmodern credit-scoring systems. Conventional explainability techniques, such as\nSHAP, assume static data and fixed background distributions, making their\nexplanations unstable and potentially unfair when concept drift occurs. This\nstudy addresses that challenge by developing adaptive explanation frameworks\nthat recalibrate interpretability and fairness in dynamically evolving credit\nmodels. Using a multi-year credit dataset, we integrate predictive modeling via\nXGBoost with three adaptive SHAP variants: (A) per-slice explanation\nreweighting that adjusts for feature distribution shifts, (B) drift-aware SHAP\nrebaselining with sliding-window background samples, and (C) online surrogate\ncalibration using incremental Ridge regression. Each method is benchmarked\nagainst static SHAP explanations using metrics of predictive performance (AUC,\nF1), directional and rank stability (cosine, Kendall tau), and fairness\n(demographic parity and recalibration). Results show that adaptive methods,\nparticularly rebaselined and surrogate-based explanations, substantially\nimprove temporal stability and reduce disparate impact across demographic\ngroups without degrading predictive accuracy. Robustness tests, including\ncounterfactual perturbations, background sensitivity analysis, and\nproxy-variable detection, confirm the resilience of adaptive explanations under\nreal-world drift conditions. These findings establish adaptive explainability\nas a practical mechanism for sustaining transparency, accountability, and\nethical reliability in data-driven credit systems, and more broadly, in any\ndomain where decision models evolve with population change.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u81ea\u9002\u5e94\u89e3\u91ca\u6846\u67b6\u6765\u89e3\u51b3\u4fe1\u7528\u8bc4\u5206\u7cfb\u7edf\u4e2d\u6982\u5ff5\u6f02\u79fb\u5bfc\u81f4\u7684\u89e3\u91ca\u4e0d\u7a33\u5b9a\u548c\u4e0d\u516c\u5e73\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u79cd\u81ea\u9002\u5e94SHAP\u53d8\u4f53\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u4f20\u7edf\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff08\u5982SHAP\uff09\u5047\u8bbe\u9759\u6001\u6570\u636e\u548c\u56fa\u5b9a\u80cc\u666f\u5206\u5e03\uff0c\u5728\u6982\u5ff5\u6f02\u79fb\u53d1\u751f\u65f6\u4f1a\u5bfc\u81f4\u89e3\u91ca\u4e0d\u7a33\u5b9a\u548c\u6f5c\u5728\u4e0d\u516c\u5e73\uff0c\u8fd9\u5f71\u54cd\u4e86\u4fe1\u7528\u8bc4\u5206\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "method": "\u6574\u5408XGBoost\u9884\u6d4b\u5efa\u6a21\u4e0e\u4e09\u79cd\u81ea\u9002\u5e94SHAP\u53d8\u4f53\uff1a(A)\u6309\u5207\u7247\u89e3\u91ca\u91cd\u52a0\u6743\uff0c(B)\u6f02\u79fb\u611f\u77e5SHAP\u91cd\u65b0\u57fa\u7ebf\u5316\uff0c(C)\u5728\u7ebf\u4ee3\u7406\u6821\u51c6\u3002\u4f7f\u7528\u591a\u5e74\u4fe1\u7528\u6570\u636e\u96c6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u81ea\u9002\u5e94\u65b9\u6cd5\u7279\u522b\u662f\u91cd\u65b0\u57fa\u7ebf\u5316\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u51cf\u5c11\u4e86\u8de8\u4eba\u53e3\u7fa4\u4f53\u7684\u5dee\u5f02\u5f71\u54cd\uff0c\u4e14\u4e0d\u964d\u4f4e\u9884\u6d4b\u51c6\u786e\u6027\u3002\u9c81\u68d2\u6027\u6d4b\u8bd5\u786e\u8ba4\u4e86\u81ea\u9002\u5e94\u89e3\u91ca\u5728\u771f\u5b9e\u6f02\u79fb\u6761\u4ef6\u4e0b\u7684\u97e7\u6027\u3002", "conclusion": "\u81ea\u9002\u5e94\u53ef\u89e3\u91ca\u6027\u662f\u5728\u6570\u636e\u9a71\u52a8\u4fe1\u7528\u7cfb\u7edf\u4e2d\u7ef4\u6301\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u548c\u4f26\u7406\u53ef\u9760\u6027\u7684\u5b9e\u7528\u673a\u5236\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u51b3\u7b56\u6a21\u578b\u968f\u4eba\u53e3\u53d8\u5316\u800c\u6f14\u53d8\u7684\u9886\u57df\u3002"}}
{"id": "2511.04275", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04275", "abs": "https://arxiv.org/abs/2511.04275", "authors": ["Jungbin Jun", "Ilsang Ohn"], "title": "Online Conformal Inference with Retrospective Adjustment for Faster Adaptation to Distribution Shift", "comment": null, "summary": "Conformal prediction has emerged as a powerful framework for constructing\ndistribution-free prediction sets with guaranteed coverage assuming only the\nexchangeability assumption. However, this assumption is often violated in\nonline environments where data distributions evolve over time. Several recent\napproaches have been proposed to address this limitation, but, typically, they\nslowly adapt to distribution shifts because they update predictions only in a\nforward manner, that is, they generate a prediction for a newly observed data\npoint while previously computed predictions are not updated. In this paper, we\npropose a novel online conformal inference method with retrospective\nadjustment, which is designed to achieve faster adaptation to distributional\nshifts. Our method leverages regression approaches with efficient leave-one-out\nupdate formulas to retroactively adjust past predictions when new data arrive,\nthereby aligning the entire set of predictions with the most recent data\ndistribution. Through extensive numerical studies performed on both synthetic\nand real-world data sets, we show that the proposed approach achieves faster\ncoverage recalibration and improved statistical efficiency compared to existing\nonline conformal prediction methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u56de\u987e\u6027\u8c03\u6574\u7684\u5728\u7ebf\u5171\u5f62\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u56de\u5f52\u65b9\u6cd5\u548c\u7559\u4e00\u6cd5\u66f4\u65b0\u516c\u5f0f\u6765\u8c03\u6574\u5386\u53f2\u9884\u6d4b\uff0c\u4ee5\u66f4\u5feb\u9002\u5e94\u6570\u636e\u5206\u5e03\u53d8\u5316\u3002", "motivation": "\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u9762\u4e34\u6570\u636e\u5206\u5e03\u968f\u65f6\u95f4\u53d8\u5316\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u5411\u524d\u66f4\u65b0\u9884\u6d4b\uff0c\u9002\u5e94\u5206\u5e03\u53d8\u5316\u8f83\u6162\u3002", "method": "\u4f7f\u7528\u56de\u5f52\u65b9\u6cd5\u548c\u9ad8\u6548\u7684\u7559\u4e00\u6cd5\u66f4\u65b0\u516c\u5f0f\uff0c\u5728\u65b0\u6570\u636e\u5230\u8fbe\u65f6\u56de\u987e\u6027\u8c03\u6574\u8fc7\u53bb\u7684\u9884\u6d4b\uff0c\u4f7f\u6240\u6709\u9884\u6d4b\u4e0e\u6700\u65b0\u6570\u636e\u5206\u5e03\u5bf9\u9f50\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u6570\u503c\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u8986\u76d6\u7387\u91cd\u65b0\u6821\u51c6\u548c\u66f4\u9ad8\u7684\u7edf\u8ba1\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u56de\u987e\u6027\u8c03\u6574\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u9002\u5e94\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u6570\u636e\u5206\u5e03\u53d8\u5316\uff0c\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.04015", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04015", "abs": "https://arxiv.org/abs/2511.04015", "authors": ["Haotian Zhang", "Shijian Gao", "Xiang Cheng"], "title": "Tiny-WiFo: A Lightweight Wireless Foundation Model for Channel Prediction via Multi-Component Adaptive Knowledge Distillation", "comment": "5 pages, 1 figures, 3 tables", "summary": "The massive scale of Wireless Foundation Models (FMs) hinders their real-time\ndeployment on edge devices. This letter moves beyond standard knowledge\ndistillation by introducing a novel Multi-Component Adaptive Knowledge\nDistillation (MCAKD) framework. Key innovations include a Cross-Attention-Based\nKnowledge Selection (CA-KS) module that selectively identifies critical\nfeatures from the teacher model, and an Autonomous Learning-Passive Learning\n(AL-PL) strategy that balances knowledge transfer with independent learning to\nachieve high training efficiency at a manageable computational cost. When\napplied to the WiFo FM, the distilled Tiny-WiFo model, with only 5.5M\nparameters, achieves a 1.6 ms inference time on edge hardware while retaining\nover 98% of WiFo's performance and its crucial zero-shot generalization\ncapability, making real-time FM deployment viable.", "AI": {"tldr": "\u63d0\u51faMCAKD\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u77e5\u8bc6\u9009\u62e9\u548c\u81ea\u4e3b-\u88ab\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u5c06WiFo\u57fa\u7840\u6a21\u578b\u538b\u7f29\u4e3a\u4ec55.5M\u53c2\u6570\u7684Tiny-WiFo\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b01.6ms\u63a8\u7406\u65f6\u95f4\uff0c\u4fdd\u630198%\u6027\u80fd\u5e76\u4fdd\u7559\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u65e0\u7ebf\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u5e9e\u5927\uff0c\u96be\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u65f6\u90e8\u7f72\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u591a\u7ec4\u4ef6\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5305\u62ec\u8de8\u6ce8\u610f\u529b\u77e5\u8bc6\u9009\u62e9\u6a21\u5757\u548c\u81ea\u4e3b-\u88ab\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u9009\u62e9\u6027\u63d0\u53d6\u6559\u5e08\u6a21\u578b\u5173\u952e\u7279\u5f81\u5e76\u5e73\u8861\u77e5\u8bc6\u8f6c\u79fb\u4e0e\u72ec\u7acb\u5b66\u4e60\u3002", "result": "Tiny-WiFo\u6a21\u578b\u4ec55.5M\u53c2\u6570\uff0c\u5728\u8fb9\u7f18\u786c\u4ef6\u4e0a\u63a8\u7406\u65f6\u95f41.6ms\uff0c\u4fdd\u6301WiFo\u6a21\u578b98%\u4ee5\u4e0a\u6027\u80fd\uff0c\u4fdd\u7559\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MCAKD\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u65e0\u7ebf\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u6548\u538b\u7f29\uff0c\u4f7f\u5b9e\u65f6\u8fb9\u7f18\u90e8\u7f72\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2511.03808", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03808", "abs": "https://arxiv.org/abs/2511.03808", "authors": ["Bo Zhao", "Berkcan Kapusuzoglu", "Kartik Balasubramaniam", "Sambit Sahu", "Supriyo Chakraborty", "Genta Indra Winata"], "title": "Optimizing Reasoning Efficiency through Prompt Difficulty Prediction", "comment": "NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Reasoning language models perform well on complex tasks but are costly to\ndeploy due to their size and long reasoning traces. We propose a routing\napproach that assigns each problem to the smallest model likely to solve it,\nreducing compute without sacrificing accuracy. Using intermediate\nrepresentations from s1.1-32B, we train lightweight predictors of problem\ndifficulty or model correctness to guide routing across a pool of reasoning\nmodels. On diverse math benchmarks, routing improves efficiency over random\nassignment and matches s1.1-32B's performance while using significantly less\ncompute. Our results demonstrate that difficulty-aware routing is effective for\ncost-efficient deployment of reasoning models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u96be\u5ea6\u611f\u77e5\u7684\u8def\u7531\u65b9\u6cd5\uff0c\u5c06\u95ee\u9898\u5206\u914d\u7ed9\u6700\u53ef\u80fd\u89e3\u51b3\u7684\u6700\u5c0f\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u89c4\u6a21\u548c\u957f\u63a8\u7406\u8f68\u8ff9\u5bfc\u81f4\u90e8\u7f72\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u51cf\u5c11\u8ba1\u7b97\u91cf\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528s1.1-32B\u7684\u4e2d\u95f4\u8868\u793a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9884\u6d4b\u5668\uff0c\u9884\u6d4b\u95ee\u9898\u96be\u5ea6\u6216\u6a21\u578b\u6b63\u786e\u6027\uff0c\u6307\u5bfc\u5728\u63a8\u7406\u6a21\u578b\u6c60\u4e2d\u8fdb\u884c\u8def\u7531\u5206\u914d\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8def\u7531\u65b9\u6cd5\u76f8\u6bd4\u968f\u673a\u5206\u914d\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5339\u914ds1.1-32B\u7684\u6027\u80fd\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "conclusion": "\u96be\u5ea6\u611f\u77e5\u8def\u7531\u5bf9\u4e8e\u63a8\u7406\u6a21\u578b\u7684\u6210\u672c\u9ad8\u6548\u90e8\u7f72\u662f\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04291", "categories": ["stat.ML", "cs.LG", "cs.NA", "eess.SP", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.04291", "abs": "https://arxiv.org/abs/2511.04291", "authors": ["Giovanni Barbarino", "Nicolas Gillis", "Subhayan Saha"], "title": "Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition", "comment": "38 pages, 4 figures", "summary": "Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used\nsuccessfully in many applications, such as hyperspectral imaging, chemical\nkinetics, spectroscopy, topic modeling, and audio source separation. However,\nits robustness to noise has been a long-standing open problem. In this paper,\nwe prove that min-vol NMF identifies the groundtruth factors in the presence of\nnoise under a condition referred to as the expanded sufficiently scattered\ncondition which requires the data points to be sufficiently well scattered in\nthe latent simplex generated by the basis vectors.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u6700\u5c0f\u4f53\u79ef\u975e\u8d1f\u77e9\u9635\u5206\u89e3(min-vol NMF)\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u80fd\u591f\u8bc6\u522b\u771f\u5b9e\u56e0\u5b50\uff0c\u524d\u63d0\u662f\u6570\u636e\u70b9\u5728\u57fa\u5411\u91cf\u751f\u6210\u7684\u6f5c\u5728\u5355\u7eaf\u5f62\u4e2d\u5145\u5206\u5206\u6563\u3002", "motivation": "\u6700\u5c0f\u4f53\u79efNMF\u5df2\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u6210\u529f\u4f7f\u7528\uff0c\u4f46\u5176\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u4e00\u76f4\u662f\u4e00\u4e2a\u957f\u671f\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u5728\u6269\u5c55\u7684\u5145\u5206\u5206\u6563\u6761\u4ef6\u4e0b\uff0c\u5206\u6790min-vol NMF\u5728\u566a\u58f0\u73af\u5883\u4e2d\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u8bc1\u660emin-vol NMF\u80fd\u591f\u5728\u566a\u58f0\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u771f\u5b9e\u56e0\u5b50\u3002", "conclusion": "min-vol NMF\u5728\u6ee1\u8db3\u6269\u5c55\u5145\u5206\u5206\u6563\u6761\u4ef6\u65f6\u5bf9\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u57fa\u7840\u56e0\u5b50\u3002"}}
{"id": "2511.04200", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04200", "abs": "https://arxiv.org/abs/2511.04200", "authors": ["Yuanhan Ni", "Fan Liu", "Haoran Yin", "Yanqun Tang", "Zulin Wang"], "title": "Ambiguity Function Analysis of AFDM Under Pulse-Shaped Random ISAC Signaling", "comment": null, "summary": "This paper investigates the ambiguity function (AF) of the emerging affine\nfrequency division multiplexing (AFDM) waveform for Integrated Sensing and\nCommunication (ISAC) signaling under a pulse shaping regime. Specifically, we\nfirst derive the closed-form expression of the average squared discrete period\nAF (DPAF) for AFDM waveform without pulse shaping, revealing that the AF\ndepends on the parameter $c_1$ and the kurtosis of random communication data,\nwhile being independent of the parameter $c_2$. As a step further, we conduct a\ncomprehensive analysis on the AFs of various waveforms, including AFDM,\northogonal frequency division multiplexing (OFDM) and orthogonal chirp-division\nmultiplexing (OCDM). Our results indicate that all three waveforms exhibit the\nsame number of regular depressions in the sidelobes of their AFs, which incurs\nperformance loss for detecting and estimating weak targets. However, the AFDM\nwaveform can flexibly control the positions of depressions by adjusting the\nparameter $c_1$, which motivates a novel design approach of the AFDM parameters\nto mitigate the adverse impact of depressions of the strong target on the weak\ntarget. Furthermore, a closed-form expression of the average squared DPAF for\npulse-shaped random AFDM waveform is derived, which demonstrates that the pulse\nshaping filter generates the shaped mainlobe along the delay axis and the rapid\nroll-off sidelobes along the Doppler axis. Numerical results verify the\neffectiveness of our theoretical analysis and proposed design methodology for\nthe AFDM modulation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7528\u4e8e\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u7684\u4eff\u5c04\u9891\u5206\u590d\u7528(AFDM)\u6ce2\u5f62\u5728\u8109\u51b2\u6210\u5f62\u4e0b\u7684\u6a21\u7cca\u51fd\u6570\u7279\u6027\uff0c\u53d1\u73b0AFDM\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u53c2\u6570c1\u7075\u6d3b\u63a7\u5236\u6a21\u7cca\u51fd\u6570\u51f9\u9677\u4f4d\u7f6e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u8bbe\u8ba1\u65b9\u6cd5\u6765\u51cf\u8f7b\u5f3a\u76ee\u6807\u5bf9\u5f31\u76ee\u6807\u68c0\u6d4b\u7684\u4e0d\u5229\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76AFDM\u6ce2\u5f62\u5728ISAC\u5e94\u7528\u4e2d\u7684\u6a21\u7cca\u51fd\u6570\u7279\u6027\uff0c\u7279\u522b\u662f\u8109\u51b2\u6210\u5f62\u5bf9\u6a21\u7cca\u51fd\u6570\u7684\u5f71\u54cd\uff0c\u4ee5\u89e3\u51b3\u5f31\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u635f\u5931\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u4e86\u65e0\u8109\u51b2\u6210\u5f62AFDM\u6ce2\u5f62\u7684\u5e73\u5747\u5e73\u65b9\u79bb\u6563\u5468\u671f\u6a21\u7cca\u51fd\u6570\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u7136\u540e\u5206\u6790\u4e86AFDM\u3001OFDM\u548cOCDM\u7b49\u591a\u79cd\u6ce2\u5f62\u7684\u6a21\u7cca\u51fd\u6570\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u53c2\u6570c1\u8c03\u6574\u7684AFDM\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e09\u79cd\u6ce2\u5f62\u5728\u6a21\u7cca\u51fd\u6570\u65c1\u74e3\u4e2d\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u89c4\u5219\u51f9\u9677\uff0c\u4f46AFDM\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u53c2\u6570c1\u7075\u6d3b\u63a7\u5236\u51f9\u9677\u4f4d\u7f6e\u3002\u8109\u51b2\u6210\u5f62\u6ee4\u6ce2\u5668\u5728\u5ef6\u8fdf\u8f74\u4e0a\u4ea7\u751f\u6210\u5f62\u4e3b\u74e3\uff0c\u5728\u591a\u666e\u52d2\u8f74\u4e0a\u4ea7\u751f\u5feb\u901f\u6eda\u964d\u7684\u65c1\u74e3\u3002", "conclusion": "AFDM\u6ce2\u5f62\u5728ISAC\u5e94\u7528\u4e2d\u5177\u6709\u72ec\u7279\u7684\u4f18\u52bf\uff0c\u53ef\u4ee5\u901a\u8fc7\u53c2\u6570\u8bbe\u8ba1\u4f18\u5316\u6a21\u7cca\u51fd\u6570\u7279\u6027\uff0c\u63d0\u9ad8\u5f31\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u548c\u6240\u63d0\u8bbe\u8ba1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.03809", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.03809", "abs": "https://arxiv.org/abs/2511.03809", "authors": ["Fran\u00e7ois Belias", "Naser Ezzati-Jivan", "Foutse Khomh"], "title": "One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA", "comment": "14 pages", "summary": "Adaptive batch size methods aim to accelerate neural network training, but\nexisting approaches apply identical adaptation strategies across all\narchitectures, assuming a one-size-fits-all solution. We introduce DEBA\n(Dynamic Efficient Batch Adaptation), an adaptive batch scheduler that monitors\ngradient variance, gradient norm variation and loss variation to guide batch\nsize adaptations. Through systematic evaluation across six architectures\n(ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on\nCIFAR-10 and CIFAR-100, with five random seeds per configuration, we\ndemonstrate that the architecture fundamentally determines adaptation efficacy.\nOur findings reveal that: (1) lightweight and medium-depth architectures\n(MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve a 45-62% training speedup\nwith simultaneous accuracy improvements of 1-7%; (2) shallow residual networks\n(ResNet-18) show consistent gains of +2.4 - 4.0% in accuracy, 36 - 43% in\nspeedup, while deep residual networks (ResNet-50) exhibit high variance and\noccasional degradation; (3) already-stable architectures (ViT-B16) show minimal\nspeedup (6%) despite maintaining accuracy, indicating that adaptation benefits\nvary with baseline optimization characteristics. We introduce a baseline\ncharacterization framework using gradient stability metrics (stability score,\ngradient norm variation) that predicts which architectures will benefit from\nadaptive scheduling. Our ablation studies reveal critical design choices often\noverlooked in prior work: sliding window statistics (vs. full history) and\nsufficient cooldown periods (5+ epochs) between adaptations are essential for\nsuccess. This work challenges the prevailing assumption that adaptive methods\ngeneralize across architectures and provides the first systematic evidence that\nbatch size adaptation requires an architecture-aware design.", "AI": {"tldr": "DEBA\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u6279\u91cf\u5927\u5c0f\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u76d1\u63a7\u68af\u5ea6\u65b9\u5dee\u3001\u68af\u5ea6\u8303\u6570\u53d8\u5316\u548c\u635f\u5931\u53d8\u5316\u6765\u6307\u5bfc\u6279\u91cf\u5927\u5c0f\u8c03\u6574\u3002\u7814\u7a76\u53d1\u73b0\u67b6\u6784\u51b3\u5b9a\u9002\u5e94\u6548\u679c\uff1a\u8f7b\u91cf\u7ea7\u548c\u4e2d\u6df1\u67b6\u6784\u83b7\u5f9745-62%\u8bad\u7ec3\u52a0\u901f\u548c1-7%\u7cbe\u5ea6\u63d0\u5347\uff0c\u800c\u6df1\u5c42\u6b8b\u5dee\u7f51\u7edc\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0cViT\u7b49\u7a33\u5b9a\u67b6\u6784\u53d7\u76ca\u6709\u9650\u3002", "motivation": "\u73b0\u6709\u81ea\u9002\u5e94\u6279\u91cf\u5927\u5c0f\u65b9\u6cd5\u5bf9\u6240\u6709\u67b6\u6784\u91c7\u7528\u76f8\u540c\u7684\u9002\u5e94\u7b56\u7565\uff0c\u5047\u8bbe\u5b58\u5728\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u67b6\u6784\u9002\u5e94\u6548\u679c\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u63d0\u51faDEBA\u81ea\u9002\u5e94\u6279\u91cf\u8c03\u5ea6\u5668\uff0c\u76d1\u63a7\u68af\u5ea6\u65b9\u5dee\u3001\u68af\u5ea6\u8303\u6570\u53d8\u5316\u548c\u635f\u5931\u53d8\u5316\uff1b\u57286\u79cd\u67b6\u6784\u4e0a\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4f7f\u7528\u68af\u5ea6\u7a33\u5b9a\u6027\u6307\u6807\uff08\u7a33\u5b9a\u6027\u5f97\u5206\u3001\u68af\u5ea6\u8303\u6570\u53d8\u5316\uff09\u9884\u6d4b\u67b6\u6784\u9002\u5e94\u6027\u3002", "result": "\u8f7b\u91cf\u7ea7\u548c\u4e2d\u6df1\u67b6\u6784\u83b7\u5f9745-62%\u8bad\u7ec3\u52a0\u901f\u548c1-7%\u7cbe\u5ea6\u63d0\u5347\uff1bResNet-18\u7cbe\u5ea6\u63d0\u53472.4-4.0%\uff0c\u52a0\u901f36-43%\uff1bResNet-50\u8868\u73b0\u4e0d\u7a33\u5b9a\uff1bViT-B16\u4ec56%\u52a0\u901f\uff1b\u6ed1\u52a8\u7a97\u53e3\u7edf\u8ba1\u548c\u8db3\u591f\u51b7\u5374\u671f\u662f\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u3002", "conclusion": "\u81ea\u9002\u5e94\u6279\u91cf\u5927\u5c0f\u65b9\u6cd5\u4e0d\u80fd\u901a\u7528\u5316\uff0c\u9700\u8981\u67b6\u6784\u611f\u77e5\u8bbe\u8ba1\uff1b\u67b6\u6784\u7684\u57fa\u7ebf\u4f18\u5316\u7279\u6027\u51b3\u5b9a\u4e86\u9002\u5e94\u6548\u679c\uff1b\u63d0\u51fa\u4e86\u9884\u6d4b\u67b6\u6784\u9002\u5e94\u6027\u7684\u57fa\u7ebf\u8868\u5f81\u6846\u67b6\u3002"}}
{"id": "2511.04301", "categories": ["stat.ML", "math.DG"], "pdf": "https://arxiv.org/pdf/2511.04301", "abs": "https://arxiv.org/abs/2511.04301", "authors": ["Frederik M\u00f6bius Rygaard", "S\u00f8ren Hauberg", "Steen Markvorsen"], "title": "Simultaneous Optimization of Geodesics and Fr\u00e9chet Means", "comment": null, "summary": "A central part of geometric statistics is to compute the Fr\\'echet mean. This\nis a well-known intrinsic mean on a Riemannian manifold that minimizes the sum\nof squared Riemannian distances from the mean point to all other data points.\nThe Fr\\'echet mean is simple to define and generalizes the Euclidean mean, but\nfor most manifolds even minimizing the Riemannian distance involves solving an\noptimization problem. Therefore, numerical computations of the Fr\\'echet mean\nrequire solving an embedded optimization problem in each iteration. We\nintroduce the GEORCE-FM algorithm to simultaneously compute the Fr\\'echet mean\nand Riemannian distances in each iteration in a local chart, making it faster\nthan previous methods. We extend the algorithm to Finsler manifolds and\nintroduce an adaptive extension such that GEORCE-FM scales to a large number of\ndata points. Theoretically, we show that GEORCE-FM has global convergence and\nlocal quadratic convergence and prove that the adaptive extension converges in\nexpectation to the Fr\\'echet mean. We further empirically demonstrate that\nGEORCE-FM outperforms existing baseline methods to estimate the Fr\\'echet mean\nin terms of both accuracy and runtime.", "AI": {"tldr": "\u63d0\u51fa\u4e86GEORCE-FM\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u540c\u65f6\u8ba1\u7b97Fr\u00e9chet\u5747\u503c\u548c\u9ece\u66fc\u8ddd\u79bb\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5feb\uff0c\u5e76\u6269\u5c55\u5230Finsler\u6d41\u5f62\u548c\u81ea\u9002\u5e94\u6269\u5c55\u4ee5\u5904\u7406\u5927\u91cf\u6570\u636e\u70b9\u3002", "motivation": "Fr\u00e9chet\u5747\u503c\u662f\u51e0\u4f55\u7edf\u8ba1\u4e2d\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u9700\u8981\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u89e3\u51b3\u5d4c\u5165\u5f0f\u4f18\u5316\u95ee\u9898\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u3002", "method": "\u5f00\u53d1\u4e86GEORCE-FM\u7b97\u6cd5\uff0c\u5728\u5c40\u90e8\u56fe\u8868\u4e2d\u540c\u65f6\u8ba1\u7b97Fr\u00e9chet\u5747\u503c\u548c\u9ece\u66fc\u8ddd\u79bb\uff0c\u907f\u514d\u4e86\u91cd\u590d\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u6269\u5c55\u7248\u672c\u3002", "result": "\u7406\u8bba\u8bc1\u660eGEORCE-FM\u5177\u6709\u5168\u5c40\u6536\u655b\u6027\u548c\u5c40\u90e8\u4e8c\u6b21\u6536\u655b\u6027\uff0c\u81ea\u9002\u5e94\u6269\u5c55\u5728\u671f\u671b\u4e0a\u6536\u655b\u5230Fr\u00e9chet\u5747\u503c\u3002\u5b9e\u9a8c\u8868\u660e\u5728\u51c6\u786e\u6027\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u90fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GEORCE-FM\u7b97\u6cd5\u4e3a\u8ba1\u7b97Fr\u00e9chet\u5747\u503c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2511.04292", "categories": ["eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.04292", "abs": "https://arxiv.org/abs/2511.04292", "authors": ["Arne Van Den Kerchove", "Hakim Si-Mohammed", "Fran\u00e7ois Cabestaing", "Marc M. Van Hulle"], "title": "BTTDA: Block-Term Tensor Discriminant Analysis for Brain-Computer Interfacing", "comment": "This archive contains 26 pages, 7 figures, 2 tables, 3 appendices and\n  3 ancillary files (erp_results.csv, mi_results.csv, block_theta_results.csv).\n  Source code is available at https://github.com/arnevdk/bttda", "summary": "Brain-computer interfaces (BCIs) allow direct communication between the brain\nand external devices, frequently using electroencephalography (EEG) to record\nneural activity. Dimensionality reduction and structured regularization are\nessential for effectively classifying task-related brain signals, including\nevent-related potentials (ERPs) and motor imagery (MI) rhythms. Current\ntensor-based approaches, such as Tucker and PARAFAC decompositions, often lack\nthe flexibility needed to fully capture the complexity of EEG data. This study\nintroduces Block-Term Tensor Discriminant Analysis (BTTDA): a novel\ntensor-based and supervised feature extraction method designed to enhance\nclassification accuracy by providing flexible multilinear dimensionality\nreduction. Extending Higher Order Discriminant Analysis (HODA), BTTDA uses a\nnovel and interpretable forward model for HODA combined with a deflation scheme\nto iteratively extract discriminant block terms, improving feature\nrepresentation for classification. BTTDA and a sum-of-rank-1-terms variant\nPARAFACDA were evaluated on publicly available ERP (second-order tensors) and\nMI (third-order tensors) EEG datasets from the MOABB benchmarking framework.\nBenchmarking revealed that BTTDA and PARAFACDA significantly outperform the\ntraditional HODA method in ERP decoding, resulting in state-of-the art\nperformance (ROC-AUC = 91.25%). For MI, decoding results of HODA, BTTDA and\nPARAFACDA were subpar, but BTTDA still significantly outperformed HODA (64.52%\n> 61.00%). The block-term structure of BTTDA enables interpretable and more\nefficient dimensionality reduction without compromising discriminative power.\nThis offers a promising and adaptable approach for feature extraction in BCI\nand broader neuroimaging applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f20\u91cf\u5224\u522b\u5206\u6790\u65b9\u6cd5BTTDA\uff0c\u7528\u4e8e\u8111\u673a\u63a5\u53e3\u4e2d\u7684EEG\u4fe1\u53f7\u5206\u7c7b\uff0c\u5728ERP\u89e3\u7801\u4e2d\u8fbe\u523091.25%\u7684ROC-AUC\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\u5982Tucker\u548cPARAFAC\u5728\u6355\u6349EEG\u6570\u636e\u590d\u6742\u6027\u65b9\u9762\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u6765\u63d0\u9ad8\u8111\u673a\u63a5\u53e3\u7684\u5206\u7c7b\u7cbe\u5ea6\u3002", "method": "BTTDA\u662f\u4e00\u79cd\u57fa\u4e8e\u5757\u9879\u5f20\u91cf\u7684\u76d1\u7763\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u9ad8\u9636\u5224\u522b\u5206\u6790\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u7684\u524d\u5411\u6a21\u578b\u548c\u7d27\u7f29\u65b9\u6848\u8fed\u4ee3\u63d0\u53d6\u5224\u522b\u5757\u9879\u3002", "result": "\u5728MOABB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBTTDA\u548cPARAFACDA\u5728ERP\u89e3\u7801\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfHODA\u65b9\u6cd5\uff0c\u8fbe\u523091.25%\u7684ROC-AUC\uff1b\u5728MI\u4efb\u52a1\u4e2dBTTDA\u4e5f\u663e\u8457\u4f18\u4e8eHODA(64.52% > 61.00%)\u3002", "conclusion": "BTTDA\u7684\u5757\u9879\u7ed3\u6784\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u964d\u7ef4\uff0c\u540c\u65f6\u4fdd\u6301\u5224\u522b\u80fd\u529b\uff0c\u4e3aBCI\u548c\u795e\u7ecf\u5f71\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9002\u5e94\u6027\u65b9\u6cd5\u3002"}}
{"id": "2511.03824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03824", "abs": "https://arxiv.org/abs/2511.03824", "authors": ["Ryien Hosseini", "Filippo Simini", "Venkatram Vishwanath", "Rebecca Willett", "Henry Hoffmann"], "title": "Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks", "comment": "To appear at NeurIPS 2025", "summary": "Graph Neural Networks learn on graph-structured data by iteratively\naggregating local neighborhood information. While this local message passing\nparadigm imparts a powerful inductive bias and exploits graph sparsity, it also\nyields three key challenges: (i) oversquashing of long-range information, (ii)\noversmoothing of node representations, and (iii) limited expressive power. In\nthis work we inject randomized global embeddings of node features, which we\nterm \\textit{Sketched Random Features}, into standard GNNs, enabling them to\nefficiently capture long-range dependencies. The embeddings are unique,\ndistance-sensitive, and topology-agnostic -- properties which we analytically\nand empirically show alleviate the aforementioned limitations when injected\ninto GNNs. Experimental results on real-world graph learning tasks confirm that\nthis strategy consistently improves performance over baseline GNNs, offering\nboth a standalone solution and a complementary enhancement to existing\ntechniques such as graph positional encodings. Our source code is available at\n\\href{https://github.com/ryienh/sketched-random-features}{https://github.com/ryienh/sketched-random-features}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u8349\u56fe\u968f\u673a\u7279\u5f81\"\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u6807\u51c6GNN\u6ce8\u5165\u968f\u673a\u5168\u5c40\u8282\u70b9\u7279\u5f81\u5d4c\u5165\uff0c\u6709\u6548\u89e3\u51b3GNN\u7684\u4e09\u5927\u6311\u6218\uff1a\u957f\u8ddd\u79bb\u4fe1\u606f\u6324\u538b\u3001\u8282\u70b9\u8868\u793a\u8fc7\u5ea6\u5e73\u6ed1\u548c\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002", "motivation": "GNN\u901a\u8fc7\u5c40\u90e8\u6d88\u606f\u4f20\u9012\u5b66\u4e60\u56fe\u7ed3\u6784\u6570\u636e\uff0c\u4f46\u9762\u4e34\u957f\u8ddd\u79bb\u4fe1\u606f\u6324\u538b\u3001\u8282\u70b9\u8868\u793a\u8fc7\u5ea6\u5e73\u6ed1\u548c\u8868\u8fbe\u80fd\u529b\u6709\u9650\u4e09\u5927\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9ad8\u6548\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u7684\u65b9\u6cd5\u3002", "method": "\u5411\u6807\u51c6GNN\u6ce8\u5165\u968f\u673a\u5168\u5c40\u8282\u70b9\u7279\u5f81\u5d4c\u5165\uff0c\u8fd9\u4e9b\u5d4c\u5165\u5177\u6709\u552f\u4e00\u6027\u3001\u8ddd\u79bb\u654f\u611f\u6027\u548c\u62d3\u6251\u65e0\u5173\u6027\uff0c\u80fd\u6709\u6548\u7f13\u89e3GNN\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u5b66\u4e60\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebfGNN\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u65e2\u53ef\u72ec\u7acb\u4f7f\u7528\u4e5f\u53ef\u4e0e\u73b0\u6709\u6280\u672f\uff08\u5982\u56fe\u4f4d\u7f6e\u7f16\u7801\uff09\u4e92\u8865\u589e\u5f3a\u3002", "conclusion": "\u8349\u56fe\u968f\u673a\u7279\u5f81\u4e3aGNN\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u4e14\u5177\u6709\u901a\u7528\u6027\u3002"}}
{"id": "2511.04403", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04403", "abs": "https://arxiv.org/abs/2511.04403", "authors": ["Sara P\u00e9rez-Vieites", "Sahel Iqbal", "Simo S\u00e4rkk\u00e4", "Dominik Baumann"], "title": "Online Bayesian Experimental Design for Partially Observed Dynamical Systems", "comment": "19 pages, 5 figures", "summary": "Bayesian experimental design (BED) provides a principled framework for\noptimizing data collection, but existing approaches do not apply to crucial\nreal-world settings such as dynamical systems with partial observability, where\nonly noisy and incomplete observations are available. These systems are\nnaturally modeled as state-space models (SSMs), where latent states mediate the\nlink between parameters and data, making the likelihood -- and thus\ninformation-theoretic objectives like the expected information gain (EIG) --\nintractable. In addition, the dynamical nature of the system requires online\nalgorithms that update posterior distributions and select designs sequentially\nin a computationally efficient manner. We address these challenges by deriving\nnew estimators of the EIG and its gradient that explicitly marginalize latent\nstates, enabling scalable stochastic optimization in nonlinear SSMs. Our\napproach leverages nested particle filters (NPFs) for efficient online\ninference with convergence guarantees. Applications to realistic models, such\nas the susceptible-infected-recovered (SIR) and a moving source location task,\nshow that our framework successfully handles both partial observability and\nonline computation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u90e8\u5206\u53ef\u89c2\u6d4b\u52a8\u6001\u7cfb\u7edf\u7684\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u5bfcEIG\u53ca\u5176\u68af\u5ea6\u7684\u65b0\u4f30\u8ba1\u5668\uff0c\u7ed3\u5408\u5d4c\u5957\u7c92\u5b50\u6ee4\u6ce2\u5668\u5b9e\u73b0\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u7684\u53ef\u6269\u5c55\u968f\u673a\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5177\u6709\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u53ea\u80fd\u83b7\u5f97\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u7684\u89c2\u6d4b\u6570\u636e\uff0c\u4e14\u4f3c\u7136\u51fd\u6570\u96be\u4ee5\u5904\u7406\uff0c\u9700\u8981\u5728\u7ebf\u7b97\u6cd5\u8fdb\u884c\u540e\u9a8c\u66f4\u65b0\u548c\u8bbe\u8ba1\u9009\u62e9\u3002", "method": "\u63a8\u5bfc\u4e86\u671f\u671b\u4fe1\u606f\u589e\u76ca\u53ca\u5176\u68af\u5ea6\u7684\u65b0\u4f30\u8ba1\u5668\uff0c\u663e\u5f0f\u5730\u8fb9\u7f18\u5316\u6f5c\u5728\u72b6\u6001\uff1b\u4f7f\u7528\u5d4c\u5957\u7c92\u5b50\u6ee4\u6ce2\u5668\u8fdb\u884c\u9ad8\u6548\u7684\u5728\u7ebf\u63a8\u7406\uff1b\u5728\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u968f\u673a\u4f18\u5316\u3002", "result": "\u5728SIR\u6a21\u578b\u548c\u79fb\u52a8\u6e90\u5b9a\u4f4d\u4efb\u52a1\u7b49\u73b0\u5b9e\u6a21\u578b\u4e2d\u6210\u529f\u5e94\u7528\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u5728\u7ebf\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5177\u6709\u6536\u655b\u4fdd\u8bc1\u7684\u9ad8\u6548\u5728\u7ebf\u7b97\u6cd5\u3002"}}
{"id": "2511.04351", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04351", "abs": "https://arxiv.org/abs/2511.04351", "authors": ["Hasan Akgul", "Mari Eplik", "Javier Rojas", "Akira Yamamoto", "Rajesh Kumar", "Maya Singh"], "title": "RCMCL: A Unified Contrastive Learning Framework for Robust Multi-Modal (RGB-D, Skeleton, Point Cloud) Action Understanding", "comment": "11 pages, 6 figures,", "summary": "Human action recognition (HAR) with multi-modal inputs (RGB-D, skeleton,\npoint cloud) can achieve high accuracy but typically relies on large labeled\ndatasets and degrades sharply when sensors fail or are noisy. We present Robust\nCross-Modal Contrastive Learning (RCMCL), a self-supervised framework that\nlearns modality-invariant representations and remains reliable under modality\ndropout and corruption. RCMCL jointly optimizes (i) a cross-modal contrastive\nobjective that aligns heterogeneous streams, (ii) an intra-modal\nself-distillation objective that improves view-invariance and reduces\nredundancy, and (iii) a degradation simulation objective that explicitly trains\nmodels to recover from masked or corrupted inputs. At inference, an Adaptive\nModality Gating (AMG) network assigns data-driven reliability weights to each\nmodality for robust fusion. On NTU RGB+D 120 (CS/CV) and UWA3D-II, RCMCL\nattains state-of-the-art accuracy in standard settings and exhibits markedly\nbetter robustness: under severe dual-modality dropout it shows only an 11.5%\ndegradation, significantly outperforming strong supervised fusion baselines.\nThese results indicate that self-supervised cross-modal alignment, coupled with\nexplicit degradation modeling and adaptive fusion, is key to deployable\nmulti-modal HAR.", "AI": {"tldr": "\u63d0\u51fa\u4e86RCMCL\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u3001\u6a21\u6001\u5185\u81ea\u84b8\u998f\u548c\u9000\u5316\u6a21\u62df\u6765\u5b66\u4e60\u6a21\u6001\u4e0d\u53d8\u8868\u793a\uff0c\u5728\u6a21\u6001\u4e22\u5931\u6216\u635f\u574f\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u52a8\u4f5c\u8bc6\u522b\u5728\u4f20\u611f\u5668\u6545\u969c\u6216\u566a\u58f0\u65f6\u6027\u80fd\u6025\u5267\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u5bf9\u6a21\u6001\u7f3a\u5931\u654f\u611f\u3002", "method": "\u8054\u5408\u4f18\u5316\u4e09\u4e2a\u76ee\u6807\uff1a\u8de8\u6a21\u6001\u5bf9\u6bd4\u5bf9\u9f50\u3001\u6a21\u6001\u5185\u81ea\u84b8\u998f\u63d0\u9ad8\u89c6\u56fe\u4e0d\u53d8\u6027\u3001\u9000\u5316\u6a21\u62df\u8bad\u7ec3\u6a21\u578b\u4ece\u635f\u574f\u8f93\u5165\u4e2d\u6062\u590d\u3002\u63a8\u7406\u65f6\u4f7f\u7528\u81ea\u9002\u5e94\u6a21\u6001\u95e8\u63a7\u7f51\u7edc\u8fdb\u884c\u9c81\u68d2\u878d\u5408\u3002", "result": "\u5728NTU RGB+D 120\u548cUWA3D-II\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u7cbe\u5ea6\uff0c\u5728\u4e25\u91cd\u53cc\u6a21\u6001\u4e22\u5931\u60c5\u51b5\u4e0b\u4ec5\u4e0b\u964d11.5%\uff0c\u663e\u8457\u4f18\u4e8e\u5f3a\u76d1\u7763\u878d\u5408\u57fa\u7ebf\u3002", "conclusion": "\u81ea\u76d1\u7763\u8de8\u6a21\u6001\u5bf9\u9f50\u7ed3\u5408\u663e\u5f0f\u9000\u5316\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u878d\u5408\u662f\u5b9e\u73b0\u53ef\u90e8\u7f72\u591a\u6a21\u6001\u52a8\u4f5c\u8bc6\u522b\u7684\u5173\u952e\u3002"}}
{"id": "2511.03828", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03828", "abs": "https://arxiv.org/abs/2511.03828", "authors": ["Lipeng Zu", "Hansong Zhou", "Xiaonan Zhang"], "title": "From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification", "comment": null, "summary": "Transitioning from offline to online reinforcement learning (RL) poses\ncritical challenges due to distributional shifts between the fixed behavior\npolicy in the offline dataset and the evolving policy during online learning.\nAlthough this issue is widely recognized, few methods attempt to explicitly\nassess or utilize the distributional structure of the offline data itself,\nleaving a research gap in adapting learning strategies to different types of\nsamples. To address this challenge, we propose an innovative method,\nEnergy-Guided Diffusion Stratification (StratDiff), which facilitates smoother\ntransitions in offline-to-online RL. StratDiff deploys a diffusion model to\nlearn prior knowledge from the offline dataset. It then refines this knowledge\nthrough energy-based functions to improve policy imitation and generate\noffline-like actions during online fine-tuning. The KL divergence between the\ngenerated action and the corresponding sampled action is computed for each\nsample and used to stratify the training batch into offline-like and\nonline-like subsets. Offline-like samples are updated using offline objectives,\nwhile online-like samples follow online learning strategies. We demonstrate the\neffectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL\nand IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff\nsignificantly outperforms existing methods, achieving enhanced adaptability and\nmore stable performance across diverse RL settings.", "AI": {"tldr": "\u63d0\u51faStratDiff\u65b9\u6cd5\u89e3\u51b3\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b66\u4e60\u79bb\u7ebf\u6570\u636e\u5148\u9a8c\uff0c\u4f7f\u7528\u80fd\u91cf\u51fd\u6570\u6539\u8fdb\u7b56\u7565\u6a21\u4eff\uff0c\u5e76\u6839\u636eKL\u6563\u5ea6\u5c06\u8bad\u7ec3\u6837\u672c\u5206\u5c42\u4e3a\u79bb\u7ebf\u7c7b\u548c\u5728\u7ebf\u7c7b\u6837\u672c\u5206\u522b\u5904\u7406\u3002", "motivation": "\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u884c\u4e3a\u7b56\u7565\u4e0e\u5728\u7ebf\u5b66\u4e60\u7b56\u7565\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5f88\u5c11\u5229\u7528\u79bb\u7ebf\u6570\u636e\u7684\u5206\u5e03\u7ed3\u6784\u6765\u9002\u5e94\u4e0d\u540c\u7c7b\u578b\u6837\u672c\u7684\u5b66\u4e60\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5b66\u4e60\u79bb\u7ebf\u6570\u636e\u5148\u9a8c\uff0c\u901a\u8fc7\u80fd\u91cf\u51fd\u6570\u6539\u8fdb\u7b56\u7565\u6a21\u4eff\uff0c\u8ba1\u7b97\u751f\u6210\u52a8\u4f5c\u4e0e\u91c7\u6837\u52a8\u4f5c\u7684KL\u6563\u5ea6\u6765\u5206\u5c42\u8bad\u7ec3\u6279\u6b21\uff0c\u79bb\u7ebf\u7c7b\u6837\u672c\u4f7f\u7528\u79bb\u7ebf\u76ee\u6807\u66f4\u65b0\uff0c\u5728\u7ebf\u7c7b\u6837\u672c\u4f7f\u7528\u5728\u7ebf\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0eCal-QL\u548cIQL\u65b9\u6cd5\u96c6\u6210\u540e\uff0cStratDiff\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u9002\u5e94\u6027\u548c\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "StratDiff\u65b9\u6cd5\u901a\u8fc7\u5206\u5c42\u5904\u7406\u4e0d\u540c\u5206\u5e03\u7279\u5f81\u7684\u6837\u672c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.04568", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.04568", "abs": "https://arxiv.org/abs/2511.04568", "authors": ["Masahiro Kato"], "title": "Riesz Regression As Direct Density Ratio Estimation", "comment": null, "summary": "Riesz regression has garnered attention as a tool in debiased machine\nlearning for causal and structural parameter estimation (Chernozhukov et al.,\n2021). This study shows that Riesz regression is closely related to direct\ndensity-ratio estimation (DRE) in important cases, including average treat-\nment effect (ATE) estimation. Specifically, the idea and objective in Riesz\nregression coincide with the one in least-squares importance fitting (LSIF,\nKanamori et al., 2009) in direct density-ratio estimation. While Riesz\nregression is general in the sense that it can be applied to Riesz representer\nestimation in a wide class of problems, the equivalence with DRE allows us to\ndirectly import exist- ing results in specific cases, including\nconvergence-rate analyses, the selection of loss functions via\nBregman-divergence minimization, and regularization techniques for flexible\nmodels, such as neural networks. Conversely, insights about the Riesz\nrepresenter in debiased machine learning broaden the applications of direct\ndensity-ratio estimation methods. This paper consolidates our prior results in\nKato (2025a) and Kato (2025b).", "AI": {"tldr": "Riesz\u56de\u5f52\u4e0e\u76f4\u63a5\u5bc6\u5ea6\u6bd4\u4f30\u8ba1(DRE)\u5728\u91cd\u8981\u573a\u666f\u4e0b\u7b49\u4ef7\uff0c\u7279\u522b\u662f\u5728\u5e73\u5747\u5904\u7406\u6548\u5e94\u4f30\u8ba1\u4e2d\u3002\u8fd9\u79cd\u7b49\u4ef7\u5173\u7cfb\u4f7f\u5f97\u53cc\u65b9\u9886\u57df\u7684\u65b9\u6cd5\u548c\u7406\u8bba\u53ef\u4ee5\u76f8\u4e92\u501f\u9274\u3002", "motivation": "\u7814\u7a76Riesz\u56de\u5f52\u4e0e\u76f4\u63a5\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4ee5\u4fc3\u8fdb\u4e24\u4e2a\u9886\u57df\u7684\u65b9\u6cd5\u548c\u7406\u8bba\u76f8\u4e92\u501f\u9274\uff0c\u6269\u5c55\u5404\u81ea\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660eRiesz\u56de\u5f52\u4e0e\u6700\u5c0f\u4e8c\u4e58\u91cd\u8981\u6027\u62df\u5408(LSIF)\u5728\u76f4\u63a5\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u4e2d\u7684\u7b49\u4ef7\u6027\uff0c\u5efa\u7acb\u4e24\u4e2a\u9886\u57df\u4e4b\u95f4\u7684\u6865\u6881\u3002", "result": "\u53d1\u73b0Riesz\u56de\u5f52\u4e0e\u76f4\u63a5\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u5728\u91cd\u8981\u6848\u4f8b\u4e2d\u5177\u6709\u7b49\u4ef7\u6027\uff0c\u4f7f\u5f97\u6536\u655b\u7387\u5206\u6790\u3001\u635f\u5931\u51fd\u6570\u9009\u62e9\u548c\u6b63\u5219\u5316\u6280\u672f\u53ef\u4ee5\u5728\u4e24\u4e2a\u9886\u57df\u95f4\u76f4\u63a5\u8fc1\u79fb\u3002", "conclusion": "Riesz\u56de\u5f52\u4e0e\u76f4\u63a5\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u7684\u7b49\u4ef7\u5173\u7cfb\u4e3a\u4e24\u4e2a\u9886\u57df\u63d0\u4f9b\u4e86\u76f8\u4e92\u501f\u9274\u7684\u673a\u4f1a\uff0c\u6269\u5c55\u4e86\u5404\u81ea\u7684\u65b9\u6cd5\u8bba\u548c\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2511.04362", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04362", "abs": "https://arxiv.org/abs/2511.04362", "authors": ["Chiara Telli", "Oleg Antropov", "Anne L\u00f6nnqvist", "Marco Lavalle"], "title": "High-Resolution Forest Mapping from L-Band Interferometric SAR Time Series using Deep Learning over Northern Spain", "comment": null, "summary": "In this study, we examine the potential of high-resolution forest mapping\nusing L-band interferometric time series datasets and deep learning modeling.\nOur SAR data are represented by a time series of nine ALOS-2 PALSAR-2 dual-pol\nSAR images acquired at near-zero spatial baseline over a study site in\nAsturias, Northern Spain. Reference data are collected using airborne laser\nscanning. We examine the performance of several candidate deep learning models\nfrom UNet-family with various combinations of input polarimetric and\ninterferometric features. In addition to basic Vanilla UNet, attention\nreinforced UNet model with squeeze-excitation blocks (SeU-Net) and advanced\nUNet model with nested structure and skip pathways are used. Studied features\ninclude dual pol interferometric observables additionally incorporating\nmodel-based derived measures. Results show that adding model-based inverted\nInSAR features or InSAR coherence layers improves retrieval accuracy compared\nto using backscatter intensity only. Use of attention mechanisms and nested\nconnection fusion provides better predictions than using Vanilla UNet or\ntraditional machine learning methods. Forest height retrieval accuracies range\nbetween 3.1-3.8 m (R2 = 0.45--0.55) at 20 m resolution when only intensity data\nare used, and improve to less than 2.8 m when both intensity and\ninterferometric coherence features are included. At 40 m and 60 m resolution,\nretrieval performance further improves, primarily due to higher SNR in both the\nintensity and interferometric layers. When using intensity at 60 m resolution,\nbest achieved RMSE is 2.2 m, while when using all suitable input features the\nachieved error is 1.95 m. We recommend this hybrid approach for L-band SAR\nretrievals also suitable for NISAR and future ROSE-L missions.", "AI": {"tldr": "\u4f7f\u7528L\u6ce2\u6bb5\u5e72\u6d89\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u68ee\u6797\u5236\u56fe\uff0c\u901a\u8fc7\u7ed3\u5408\u6781\u5316\u3001\u5e72\u6d89\u7279\u5f81\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68ee\u6797\u9ad8\u5ea6\u53cd\u6f14\u7cbe\u5ea6\u3002", "motivation": "\u63a2\u7d22\u5229\u7528L\u6ce2\u6bb5\u5e72\u6d89\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u68ee\u6797\u5236\u56fe\u7684\u6f5c\u529b\uff0c\u4e3aNISAR\u548c\u672a\u6765ROSE-L\u4efb\u52a1\u63d0\u4f9b\u9002\u7528\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528ALOS-2 PALSAR-2\u53cc\u6781\u5316SAR\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\uff0c\u7ed3\u5408UNet\u7cfb\u5217\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ecVanilla UNet\u3001SeU-Net\u548c\u5d4c\u5957\u7ed3\u6784UNet\uff09\uff0c\u5206\u6790\u4e0d\u540c\u6781\u5316\u3001\u5e72\u6d89\u7279\u5f81\u7ec4\u5408\u7684\u6027\u80fd\u3002", "result": "\u4ec5\u4f7f\u7528\u5f3a\u5ea6\u6570\u636e\u65f6\u68ee\u6797\u9ad8\u5ea6\u53cd\u6f14\u7cbe\u5ea6\u4e3a3.1-3.8\u7c73\uff08R2=0.45-0.55\uff09\uff0c\u52a0\u5165\u5e72\u6d89\u76f8\u5e72\u7279\u5f81\u540e\u7cbe\u5ea6\u63d0\u9ad8\u81f3\u5c0f\u4e8e2.8\u7c73\u3002\u572860\u7c73\u5206\u8fa8\u7387\u4e0b\uff0c\u4f7f\u7528\u6240\u6709\u5408\u9002\u7279\u5f81\u65f6\u8bef\u5dee\u53ef\u8fbe1.95\u7c73\u3002", "conclusion": "\u5efa\u8bae\u91c7\u7528\u8fd9\u79cd\u7ed3\u5408\u5f3a\u5ea6\u6570\u636e\u548c\u5e72\u6d89\u76f8\u5e72\u7279\u5f81\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eL\u6ce2\u6bb5SAR\u53cd\u6f14\uff0c\u5bf9NISAR\u548c\u672a\u6765ROSE-L\u4efb\u52a1\u5177\u6709\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2511.03831", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03831", "abs": "https://arxiv.org/abs/2511.03831", "authors": ["James Enouen", "Yujia Zheng", "Ignavier Ng", "Yan Liu", "Kun Zhang"], "title": "Higher-Order Causal Structure Learning with Additive Models", "comment": null, "summary": "Causal structure learning has long been the central task of inferring causal\ninsights from data. Despite the abundance of real-world processes exhibiting\nhigher-order mechanisms, however, an explicit treatment of interactions in\ncausal discovery has received little attention. In this work, we focus on\nextending the causal additive model (CAM) to additive models with higher-order\ninteractions. This second level of modularity we introduce to the structure\nlearning problem is most easily represented by a directed acyclic hypergraph\nwhich extends the DAG. We introduce the necessary definitions and theoretical\ntools to handle the novel structure we introduce and then provide\nidentifiability results for the hyper DAG, extending the typical Markov\nequivalence classes. We next provide insights into why learning the more\ncomplex hypergraph structure may actually lead to better empirical results. In\nparticular, more restrictive assumptions like CAM correspond to easier-to-learn\nhyper DAGs and better finite sample complexity. We finally develop an extension\nof the greedy CAM algorithm which can handle the more complex hyper DAG search\nspace and demonstrate its empirical usefulness in synthetic experiments.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u56e0\u679c\u52a0\u6027\u6a21\u578b(CAM)\u4ee5\u5904\u7406\u9ad8\u9636\u4ea4\u4e92\u4f5c\u7528\uff0c\u5f15\u5165\u4e86\u6709\u5411\u65e0\u73af\u8d85\u56fe\u6765\u8868\u793a\u8fd9\u79cd\u7ed3\u6784\uff0c\u63d0\u4f9b\u4e86\u53ef\u8bc6\u522b\u6027\u7ed3\u679c\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8bb8\u591a\u8fc7\u7a0b\u90fd\u8868\u73b0\u51fa\u9ad8\u9636\u673a\u5236\uff0c\u4f46\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u5bf9\u4ea4\u4e92\u4f5c\u7528\u7684\u663e\u5f0f\u5904\u7406\u5374\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u6269\u5c55\u56e0\u679c\u52a0\u6027\u6a21\u578b\u4ee5\u5305\u542b\u9ad8\u9636\u4ea4\u4e92\u4f5c\u7528\u3002", "method": "\u6269\u5c55\u56e0\u679c\u52a0\u6027\u6a21\u578b(CAM)\u5230\u5305\u542b\u9ad8\u9636\u4ea4\u4e92\u7684\u52a0\u6027\u6a21\u578b\uff0c\u4f7f\u7528\u6709\u5411\u65e0\u73af\u8d85\u56fe\u8868\u793a\u7ed3\u6784\uff0c\u63d0\u4f9b\u7406\u8bba\u5de5\u5177\u548c\u53ef\u8bc6\u522b\u6027\u7ed3\u679c\uff0c\u5e76\u5f00\u53d1\u4e86\u8d2a\u5a6aCAM\u7b97\u6cd5\u7684\u6269\u5c55\u7248\u672c\u3002", "result": "\u63d0\u4f9b\u4e86\u8d85DAG\u7684\u53ef\u8bc6\u522b\u6027\u7ed3\u679c\uff0c\u6269\u5c55\u4e86\u5178\u578b\u7684\u9a6c\u5c14\u53ef\u592b\u7b49\u4ef7\u7c7b\uff0c\u5e76\u8bc1\u660e\u4e86\u5b66\u4e60\u66f4\u590d\u6742\u7684\u8d85\u56fe\u7ed3\u6784\u53ef\u80fd\u5e26\u6765\u66f4\u597d\u7684\u7ecf\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u6269\u5c55\u4e86\u56e0\u679c\u52a0\u6027\u6a21\u578b\u4ee5\u5904\u7406\u9ad8\u9636\u4ea4\u4e92\u4f5c\u7528\uff0c\u5f15\u5165\u4e86\u6709\u5411\u65e0\u73af\u8d85\u56fe\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86\u6709\u6548\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u5408\u6210\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u7ecf\u9a8c\u4ef7\u503c\u3002"}}
{"id": "2511.04576", "categories": ["stat.ML", "cs.LG", "68T01"], "pdf": "https://arxiv.org/pdf/2511.04576", "abs": "https://arxiv.org/abs/2511.04576", "authors": ["Zhuo Zhang", "Xiong Xiong", "Sen Zhang", "Yuan Zhao", "Xi Yang"], "title": "Physics-Informed Neural Networks and Neural Operators for Parametric PDEs: A Human-AI Collaborative Analysis", "comment": "61 pages, 3 figures. Submitted to The 1st International Conference on\n  AI Scientists (ICAIS 2025)", "summary": "PDEs arise ubiquitously in science and engineering, where solutions depend on\nparameters (physical properties, boundary conditions, geometry). Traditional\nnumerical methods require re-solving the PDE for each parameter, making\nparameter space exploration prohibitively expensive. Recent machine learning\nadvances, particularly physics-informed neural networks (PINNs) and neural\noperators, have revolutionized parametric PDE solving by learning solution\noperators that generalize across parameter spaces. We critically analyze two\nmain paradigms: (1) PINNs, which embed physical laws as soft constraints and\nexcel at inverse problems with sparse data, and (2) neural operators (e.g.,\nDeepONet, Fourier Neural Operator), which learn mappings between\ninfinite-dimensional function spaces and achieve unprecedented generalization.\nThrough comparisons across fluid dynamics, solid mechanics, heat transfer, and\nelectromagnetics, we show neural operators can achieve computational speedups\nof $10^3$ to $10^5$ times faster than traditional solvers for multi-query\nscenarios, while maintaining comparable accuracy. We provide practical guidance\nfor method selection, discuss theoretical foundations (universal approximation,\nconvergence), and identify critical open challenges: high-dimensional\nparameters, complex geometries, and out-of-distribution generalization. This\nwork establishes a unified framework for understanding parametric PDE solvers\nvia operator learning, offering a comprehensive, incrementally updated resource\nfor this rapidly evolving field", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u53c2\u6570\u5316\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86PINNs\u548c\u795e\u7ecf\u7b97\u5b50\u4e24\u79cd\u4e3b\u8981\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u7b97\u5b50\u5728\u591a\u67e5\u8be2\u573a\u666f\u4e0b\u6bd4\u4f20\u7edf\u6c42\u89e3\u5668\u5feb10^3\u523010^5\u500d\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u53c2\u6570\u91cd\u65b0\u6c42\u89e3PDE\uff0c\u5bfc\u81f4\u53c2\u6570\u7a7a\u95f4\u63a2\u7d22\u6210\u672c\u8fc7\u9ad8\u3002\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u6cdb\u5316\u7684\u89e3\u7b97\u5b50\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u6279\u5224\u6027\u5206\u6790\u4e24\u79cd\u4e3b\u8981\u8303\u5f0f\uff1a(1) PINNs\uff1a\u5c06\u7269\u7406\u5b9a\u5f8b\u4f5c\u4e3a\u8f6f\u7ea6\u675f\u5d4c\u5165\uff0c\u64c5\u957f\u7a00\u758f\u6570\u636e\u7684\u53cd\u95ee\u9898\uff1b(2) \u795e\u7ecf\u7b97\u5b50\uff08\u5982DeepONet\u3001\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff09\uff1a\u5b66\u4e60\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u4e4b\u95f4\u7684\u6620\u5c04\uff0c\u5b9e\u73b0\u524d\u6240\u672a\u6709\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u56fa\u4f53\u529b\u5b66\u3001\u70ed\u4f20\u5bfc\u548c\u7535\u78c1\u5b66\u7b49\u9886\u57df\u7684\u6bd4\u8f83\u663e\u793a\uff0c\u795e\u7ecf\u7b97\u5b50\u80fd\u591f\u5b9e\u73b0\u6bd4\u4f20\u7edf\u6c42\u89e3\u5668\u5feb10^3\u523010^5\u500d\u7684\u8ba1\u7b97\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u5efa\u7acb\u4e86\u901a\u8fc7\u7b97\u5b50\u5b66\u4e60\u7406\u89e3\u53c2\u6570\u5316PDE\u6c42\u89e3\u5668\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4e3a\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u3001\u53ef\u589e\u91cf\u66f4\u65b0\u7684\u8d44\u6e90\uff0c\u5e76\u6307\u51fa\u4e86\u9ad8\u7ef4\u53c2\u6570\u3001\u590d\u6742\u51e0\u4f55\u548c\u5206\u5e03\u5916\u6cdb\u5316\u7b49\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2511.04448", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04448", "abs": "https://arxiv.org/abs/2511.04448", "authors": ["Chu Li", "Kevin Weinberger", "Aydin Sezgin"], "title": "A Lightweight Framework for Integrated Sensing and Communications with RIS", "comment": null, "summary": "Reconfigurable Intelligent Surfaces (RIS) have been recognized as a promising\ntechnology to enhance both communication and sensing performance in integrated\nsensing and communication (ISAC) systems for future 6G networks. However,\nexisting RIS optimization methods for improving ISAC performance are mainly\nbased on semidefinite relaxation (SDR) or iterative algorithms. The former\nsuffers from high computational complexity and limited scalability, especially\nwhen the number of RIS elements becomes large, while the latter yields\nsuboptimal solutions whose performance depends on initialization. In this work,\nwe introduce a lightweight RIS phase design framework that provides a\nclosed-form solution and explicitly accounts for the trade-off between\ncommunication and sensing, as well as proportional beam gain distribution\ntoward multiple sensing targets. The key idea is to partition the RIS\nconfiguration into two parts: the first part is designed to maximize the\ncommunication performance, while the second introduces small perturbations to\ngenerate multiple beams for multi-target sensing. Simulation results validate\nthe effectiveness of the proposed approach and demonstrate that it achieves\nperformance comparable to SDR but with significantly lower computational\ncomplexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7RIS\u76f8\u4f4d\u8bbe\u8ba1\u6846\u67b6\uff0c\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u95ed\u5f0f\u89e3\uff0c\u5e73\u8861\u901a\u4fe1\u4e0e\u611f\u77e5\u6027\u80fd\uff0c\u652f\u6301\u591a\u76ee\u6807\u611f\u77e5\u7684\u6ce2\u675f\u589e\u76ca\u5206\u5e03\u3002", "motivation": "\u73b0\u6709RIS\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eSDR\u6216\u8fed\u4ee3\u7b97\u6cd5\uff0c\u524d\u8005\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4e14\u53ef\u6269\u5c55\u6027\u6709\u9650\uff0c\u540e\u8005\u6027\u80fd\u4f9d\u8d56\u4e8e\u521d\u59cb\u5316\u4e14\u53ea\u80fd\u83b7\u5f97\u6b21\u4f18\u89e3\u3002", "method": "\u5c06RIS\u914d\u7f6e\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u7b2c\u4e00\u90e8\u5206\u6700\u5927\u5316\u901a\u4fe1\u6027\u80fd\uff0c\u7b2c\u4e8c\u90e8\u5206\u5f15\u5165\u5c0f\u6270\u52a8\u4e3a\u591a\u76ee\u6807\u611f\u77e5\u751f\u6210\u591a\u4e2a\u6ce2\u675f\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u4e0eSDR\u76f8\u5f53\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u6846\u67b6\u4e3aRIS\u5728ISAC\u7cfb\u7edf\u4e2d\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03836", "abs": "https://arxiv.org/abs/2511.03836", "authors": ["Lipeng Zu", "Hansong Zhou", "Xiaonan Zhang"], "title": "Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction", "comment": null, "summary": "Deep Q-Networks (DQNs) estimate future returns by learning from transitions\nsampled from a replay buffer. However, the target updates in DQN often rely on\nnext states generated by actions from past, potentially suboptimal, policy. As\na result, these states may not provide informative learning signals, causing\nhigh variance into the update process. This issue is exacerbated when the\nsampled transitions are poorly aligned with the agent's current policy. To\naddress this limitation, we propose the Successor-state Aggregation Deep\nQ-Network (SADQ), which explicitly models environment dynamics using a\nstochastic transition model. SADQ integrates successor-state distributions into\nthe Q-value estimation process, enabling more stable and policy-aligned value\nupdates. Additionally, it explores a more efficient action selection strategy\nwith the modeled transition structure. We provide theoretical guarantees that\nSADQ maintains unbiased value estimates while reducing training variance. Our\nextensive empirical results across standard RL benchmarks and real-world\nvector-based control tasks demonstrate that SADQ consistently outperforms DQN\nvariants in both stability and learning efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86SADQ\uff08Successor-state Aggregation Deep Q-Network\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u73af\u5883\u52a8\u6001\u6765\u6539\u8fdbDQN\uff0c\u89e3\u51b3\u4f20\u7edfDQN\u4e2d\u76ee\u6807\u66f4\u65b0\u4f9d\u8d56\u8fc7\u65f6\u7b56\u7565\u5bfc\u81f4\u7684\u9ad8\u65b9\u5dee\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfDQN\u4ece\u56de\u653e\u7f13\u51b2\u533a\u91c7\u6837\u8f6c\u6362\u65f6\uff0c\u76ee\u6807\u66f4\u65b0\u4f9d\u8d56\u4e8e\u8fc7\u53bb\u53ef\u80fd\u6b21\u4f18\u7b56\u7565\u751f\u6210\u7684\u4e0b\u4e00\u4e2a\u72b6\u6001\uff0c\u8fd9\u4e9b\u72b6\u6001\u53ef\u80fd\u65e0\u6cd5\u63d0\u4f9b\u6709\u6548\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u5bfc\u81f4\u66f4\u65b0\u8fc7\u7a0b\u65b9\u5dee\u8fc7\u9ad8\uff0c\u7279\u522b\u662f\u5728\u91c7\u6837\u8f6c\u6362\u4e0e\u5f53\u524d\u7b56\u7565\u4e0d\u5339\u914d\u65f6\u95ee\u9898\u66f4\u4e25\u91cd\u3002", "method": "SADQ\u4f7f\u7528\u968f\u673a\u8f6c\u79fb\u6a21\u578b\u663e\u5f0f\u5efa\u6a21\u73af\u5883\u52a8\u6001\uff0c\u5c06\u540e\u7ee7\u72b6\u6001\u5206\u5e03\u96c6\u6210\u5230Q\u503c\u4f30\u8ba1\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u548c\u7b56\u7565\u5bf9\u9f50\u7684\u503c\u66f4\u65b0\uff0c\u5e76\u5229\u7528\u5efa\u6a21\u7684\u8f6c\u79fb\u7ed3\u6784\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u52a8\u4f5c\u9009\u62e9\u7b56\u7565\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1SADQ\u4fdd\u6301\u65e0\u504f\u503c\u4f30\u8ba1\u7684\u540c\u65f6\u964d\u4f4e\u8bad\u7ec3\u65b9\u5dee\u3002\u5728\u6807\u51c6RL\u57fa\u51c6\u6d4b\u8bd5\u548c\u73b0\u5b9e\u4e16\u754c\u5411\u91cf\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cSADQ\u5728\u7a33\u5b9a\u6027\u548c\u5b66\u4e60\u6548\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8eDQN\u53d8\u4f53\u3002", "conclusion": "SADQ\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u73af\u5883\u52a8\u6001\u548c\u96c6\u6210\u540e\u7ee7\u72b6\u6001\u5206\u5e03\uff0c\u6709\u6548\u89e3\u51b3\u4e86DQN\u4e2d\u7684\u9ad8\u65b9\u5dee\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2511.04635", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04635", "abs": "https://arxiv.org/abs/2511.04635", "authors": ["Qingbin Li", "Jian Pang"], "title": "An Area-Efficient 20-100-GHz Phase-Invariant Switch-Type Attenuator Achieving 0.1-dB Tuning Step in 65-nm CMOS", "comment": "Accepted paper at IEEE UCMMT 2025. 3 pages, 7 figures. Uploaded as\n  preprint for open access", "summary": "This paper presents a switch-type attenuator working from 20 to 100 GHz. The\nattenuator adopts a capacitive compensation technique to reduce phase error.\nThe small resistors in this work are implemented with metal lines to reduce the\nintrinsic parasitic capacitance, which helps minimize the amplitude and phase\nerrors over a wide frequency range. Moreover, the utilization of metal lines\nalso reduces the chip area. In addition, a continuous tuning attenuation unit\nis employed to improve the overall attenuation accuracy of the attenuator. The\npassive attenuator is designed and fabricated in a standard 65nm CMOS. The\nmeasurement results reveal a relative attenuation range of 7.5 dB with a\ncontinuous tuning step within 20-100 GHz. The insertion loss is 1.6-3.8 dB\nwithin the operation band, while the return losses of all states are better\nthan 11.5 dB. The RMS amplitude and phase errors are below 0.15 dB and\n1.6{\\deg}, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5de5\u4f5c\u572820-100 GHz\u7684\u5f00\u5173\u578b\u8870\u51cf\u5668\uff0c\u91c7\u7528\u7535\u5bb9\u8865\u507f\u6280\u672f\u51cf\u5c0f\u76f8\u4f4d\u8bef\u5dee\uff0c\u4f7f\u7528\u91d1\u5c5e\u7ebf\u5b9e\u73b0\u5c0f\u7535\u963b\u4ee5\u51cf\u5c0f\u5bc4\u751f\u7535\u5bb9\uff0c\u5e76\u91c7\u7528\u8fde\u7eed\u8c03\u8c10\u8870\u51cf\u5355\u5143\u63d0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u572820-100 GHz\u5bbd\u9891\u5e26\u8303\u56f4\u5185\u5de5\u4f5c\u7684\u8870\u51cf\u5668\uff0c\u9700\u8981\u89e3\u51b3\u4f20\u7edf\u8870\u51cf\u5668\u5728\u9ad8\u9891\u4e0b\u5b58\u5728\u7684\u76f8\u4f4d\u8bef\u5dee\u5927\u3001\u5bc4\u751f\u7535\u5bb9\u5f71\u54cd\u663e\u8457\u7b49\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u8870\u51cf\u7cbe\u5ea6\u548c\u51cf\u5c0f\u82af\u7247\u9762\u79ef\u3002", "method": "\u91c7\u7528\u7535\u5bb9\u8865\u507f\u6280\u672f\u51cf\u5c0f\u76f8\u4f4d\u8bef\u5dee\uff1b\u4f7f\u7528\u91d1\u5c5e\u7ebf\u5b9e\u73b0\u5c0f\u7535\u963b\u4ee5\u51cf\u5c0f\u56fa\u6709\u5bc4\u751f\u7535\u5bb9\uff1b\u91c7\u7528\u8fde\u7eed\u8c03\u8c10\u8870\u51cf\u5355\u5143\u63d0\u9ad8\u6574\u4f53\u8870\u51cf\u7cbe\u5ea6\uff1b\u57fa\u4e8e\u6807\u51c665nm CMOS\u5de5\u827a\u8bbe\u8ba1\u548c\u5236\u9020\u3002", "result": "\u76f8\u5bf9\u8870\u51cf\u8303\u56f4\u4e3a7.5 dB\uff0c\u8fde\u7eed\u8c03\u8c10\u6b65\u957f\u572820-100 GHz\u5185\uff1b\u63d2\u5165\u635f\u8017\u4e3a1.6-3.8 dB\uff1b\u6240\u6709\u72b6\u6001\u7684\u53cd\u5c04\u635f\u8017\u4f18\u4e8e11.5 dB\uff1bRMS\u5e45\u5ea6\u548c\u76f8\u4f4d\u8bef\u5dee\u5206\u522b\u4f4e\u4e8e0.15 dB\u548c1.6\u00b0\u3002", "conclusion": "\u8be5\u8870\u51cf\u5668\u572820-100 GHz\u9891\u6bb5\u5185\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u5177\u6709\u4f4e\u63d2\u5165\u635f\u8017\u3001\u9ad8\u53cd\u5c04\u635f\u8017\u548c\u8f83\u5c0f\u7684\u5e45\u5ea6/\u76f8\u4f4d\u8bef\u5dee\uff0c\u9a8c\u8bc1\u4e86\u6240\u91c7\u7528\u6280\u672f\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.03877", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03877", "abs": "https://arxiv.org/abs/2511.03877", "authors": ["Kimia Kazemian", "Zhenzhen Liu", "Yangfanyu Yang", "Katie Z Luo", "Shuhan Gu", "Audrey Du", "Xinyu Yang", "Jack Jansons", "Kilian Q Weinberger", "John Thickstun", "Yian Yin", "Sarah Dean"], "title": "Benchmark Datasets for Lead-Lag Forecasting on Social Platforms", "comment": null, "summary": "Social and collaborative platforms emit multivariate time-series traces in\nwhich early interactions-such as views, likes, or downloads-are followed,\nsometimes months or years later, by higher impact like citations, sales, or\nreviews. We formalize this setting as Lead-Lag Forecasting (LLF): given an\nearly usage channel (the lead), predict a correlated but temporally shifted\noutcome channel (the lag). Despite the ubiquity of such patterns, LLF has not\nbeen treated as a unified forecasting problem within the time-series community,\nlargely due to the absence of standardized datasets. To anchor research in LLF,\nhere we present two high-volume benchmark datasets-arXiv (accesses -> citations\nof 2.3M papers) and GitHub (pushes/stars -> forks of 3M repositories)-and\noutline additional domains with analogous lead-lag dynamics, including\nWikipedia (page views -> edits), Spotify (streams -> concert attendance),\ne-commerce (click-throughs -> purchases), and LinkedIn profile (views ->\nmessages). Our datasets provide ideal testbeds for lead-lag forecasting, by\ncapturing long-horizon dynamics across years, spanning the full spectrum of\noutcomes, and avoiding survivorship bias in sampling. We documented all\ntechnical details of data curation and cleaning, verified the presence of\nlead-lag dynamics through statistical and classification tests, and benchmarked\nparametric and non-parametric baselines for regression. Our study establishes\nLLF as a novel forecasting paradigm and lays an empirical foundation for its\nsystematic exploration in social and usage data. Our data portal with downloads\nand documentation is available at https://lead-lag-forecasting.github.io/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9886\u5148-\u6ede\u540e\u9884\u6d4b\uff08LLF\uff09\u8fd9\u4e00\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8303\u5f0f\uff0c\u65e8\u5728\u6839\u636e\u65e9\u671f\u4f7f\u7528\u6e20\u9053\uff08\u9886\u5148\uff09\u9884\u6d4b\u76f8\u5173\u4f46\u65f6\u95f4\u504f\u79fb\u7684\u7ed3\u679c\u6e20\u9053\uff08\u6ede\u540e\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86arXiv\u548cGitHub\u4e24\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u793e\u4ea4\u548c\u534f\u4f5c\u5e73\u53f0\u4ea7\u751f\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5176\u4e2d\u65e9\u671f\u4e92\u52a8\uff08\u5982\u6d4f\u89c8\u3001\u70b9\u8d5e\uff09\u4e0e\u540e\u671f\u9ad8\u5f71\u54cd\u529b\u7ed3\u679c\uff08\u5982\u5f15\u7528\u3001\u9500\u552e\uff09\u4e4b\u95f4\u5b58\u5728\u65f6\u95f4\u5ef6\u8fdf\u5173\u7cfb\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u6765\u7cfb\u7edf\u7814\u7a76\u8fd9\u79cd\u9886\u5148-\u6ede\u540e\u9884\u6d4b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6784\u5efaarXiv\uff08\u8bbf\u95ee\u91cf\u2192\u5f15\u7528\u91cf\uff0c230\u4e07\u7bc7\u8bba\u6587\uff09\u548cGitHub\uff08\u63a8\u9001/\u661f\u6807\u2192\u5206\u652f\uff0c300\u4e07\u4e2a\u4ed3\u5e93\uff09\u4e24\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u9886\u5148-\u6ede\u540e\u52a8\u6001\u7684\u5b58\u5728\uff0c\u5e76\u5efa\u7acb\u53c2\u6570\u5316\u548c\u975e\u53c2\u6570\u5316\u56de\u5f52\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5305\u542b\u591a\u5e74\u957f\u671f\u52a8\u6001\u3001\u8986\u76d6\u5b8c\u6574\u7ed3\u679c\u8c31\u7cfb\u4e14\u907f\u514d\u5e78\u5b58\u8005\u504f\u5dee\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7edf\u8ba1\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u9886\u5148-\u6ede\u540e\u52a8\u6001\u7684\u5b58\u5728\uff0c\u4e3a\u7cfb\u7edf\u7814\u7a76LLF\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002", "conclusion": "\u672c\u7814\u7a76\u786e\u7acb\u4e86\u9886\u5148-\u6ede\u540e\u9884\u6d4b\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7684\u9884\u6d4b\u8303\u5f0f\uff0c\u4e3a\u5728\u793e\u4ea4\u548c\u4f7f\u7528\u6570\u636e\u4e2d\u7cfb\u7edf\u63a2\u7d22\u6b64\u7c7b\u95ee\u9898\u5960\u5b9a\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u76f8\u5173\u6570\u636e\u548c\u6587\u6863\u5df2\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2511.03953", "categories": ["cs.LG", "eess.SP", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03953", "abs": "https://arxiv.org/abs/2511.03953", "authors": ["Wuxia Chen", "Taposh Banerjee", "Vahid Tarokh"], "title": "Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels", "comment": null, "summary": "We address the problem of quickest change detection in Markov processes with\nunknown transition kernels. The key idea is to learn the conditional score\n$\\nabla_{\\mathbf{y}} \\log p(\\mathbf{y}|\\mathbf{x})$ directly from sample pairs\n$( \\mathbf{x},\\mathbf{y})$, where both $\\mathbf{x}$ and $\\mathbf{y}$ are\nhigh-dimensional data generated by the same transition kernel. In this way, we\navoid explicit likelihood evaluation and provide a practical way to learn the\ntransition dynamics. Based on this estimation, we develop a score-based CUSUM\nprocedure that uses conditional Hyvarinen score differences to detect changes\nin the kernel. To ensure bounded increments, we propose a truncated version of\nthe statistic. With Hoeffding's inequality for uniformly ergodic Markov\nprocesses, we prove exponential lower bounds on the mean time to false alarm.\nWe also prove asymptotic upper bounds on detection delay. These results give\nboth theoretical guarantees and practical feasibility for score-based detection\nin high-dimensional Markov models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u5206\u6570\u5b66\u4e60\u7684\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u5feb\u901f\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\uff0c\u907f\u514d\u663e\u5f0f\u4f3c\u7136\u8ba1\u7b97\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u5728\u672a\u77e5\u8f6c\u79fb\u6838\u60c5\u51b5\u4e0b\u7684\u5feb\u901f\u53d8\u5316\u68c0\u6d4b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u6570\u636e\u573a\u666f\u4e0b\u907f\u514d\u590d\u6742\u7684\u4f3c\u7136\u8ba1\u7b97\u3002", "method": "\u76f4\u63a5\u4ece\u6837\u672c\u5bf9\u5b66\u4e60\u6761\u4ef6\u5206\u6570\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1\u57fa\u4e8e\u5206\u6570\u7684CUSUM\u7a0b\u5e8f\uff0c\u4f7f\u7528\u6761\u4ef6Hyvarinen\u5206\u6570\u5dee\u5f02\u68c0\u6d4b\u6838\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u622a\u65ad\u7edf\u8ba1\u91cf\u786e\u4fdd\u6709\u754c\u589e\u91cf\u3002", "result": "\u8bc1\u660e\u4e86\u5747\u5300\u904d\u5386\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u7684\u8bef\u62a5\u65f6\u95f4\u6307\u6570\u4e0b\u754c\u548c\u68c0\u6d4b\u5ef6\u8fdf\u7684\u6e10\u8fd1\u4e0a\u754c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u7ef4\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u4e2d\u7684\u57fa\u4e8e\u5206\u6570\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2511.03911", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03911", "abs": "https://arxiv.org/abs/2511.03911", "authors": ["Sanggeon Yun", "Hyunwoo Oh", "Ryozo Masukawa", "Mohsen Imani"], "title": "DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets", "comment": "Accepted to DATE 2026", "summary": "Decomposition is a proven way to shrink deep networks without changing I/O.\nWe bring this idea to hyperdimensional computing (HDC), where footprint cuts\nusually shrink the feature axis and erode concentration and robustness. Prior\nHDC decompositions decode via fixed atomic hypervectors, which are ill-suited\nfor compressing learned class prototypes. We introduce DecoHD, which learns\ndirectly in a decomposed HDC parameterization: a small, shared set of per-layer\nchannels with multiplicative binding across layers and bundling at the end,\nyielding a large representational space from compact factors. DecoHD compresses\nalong the class axis via a lightweight bundling head while preserving native\nbind-bundle-score; training is end-to-end, and inference remains pure HDC,\naligning with in/near-memory accelerators. In evaluation, DecoHD attains\nextreme memory savings with only minor accuracy degradation under tight\ndeployment budgets. On average it stays within about 0.1-0.15% of a strong\nnon-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip\nnoise, reaches its accuracy plateau with up to ~97% fewer trainable parameters,\nand -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU\n(AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x\nover a baseline HDC ASIC.", "AI": {"tldr": "Decomposition is a proven way to shrink deep networks without changing I/O. We bring this idea to hyperdimensional computing (HDC), where footprint cuts usually shrink the feature axis and erode concentration and robustness. Prior HDC decompositions decode via fixed atomic hypervectors, which are ill-suited for compressing learned class prototypes. We introduce DecoHD, which learns directly in a decomposed HDC parameterization: a small, shared set of per-layer channels with multiplicative binding across layers and bundling at the end, yielding a large representational space from compact factors. DecoHD compresses along the class axis via a lightweight bundling head while preserving native bind-bundle-score; training is end-to-end, and inference remains pure HDC, aligning with in/near-memory accelerators. In evaluation, DecoHD attains extreme memory savings with only minor accuracy degradation under tight deployment budgets. On average it stays within about 0.1-0.15% of a strong non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters, and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU (AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x over a baseline HDC ASIC.", "motivation": "Decomposition is a proven way to shrink deep networks without changing I/O. We bring this idea to hyperdimensional computing (HDC), where footprint cuts usually shrink the feature axis and erode concentration and robustness. Prior HDC decompositions decode via fixed atomic hypervectors, which are ill-suited for compressing learned class prototypes.", "method": "We introduce DecoHD, which learns directly in a decomposed HDC parameterization: a small, shared set of per-layer channels with multiplicative binding across layers and bundling at the end, yielding a large representational space from compact factors. DecoHD compresses along the class axis via a lightweight bundling head while preserving native bind-bundle-score; training is end-to-end, and inference remains pure HDC, aligning with in/near-memory accelerators.", "result": "In evaluation, DecoHD attains extreme memory savings with only minor accuracy degradation under tight deployment budgets. On average it stays within about 0.1-0.15% of a strong non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters, and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU (AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x over a baseline HDC ASIC.", "conclusion": "Decomposition is a proven way to shrink deep networks without changing I/O. We bring this idea to hyperdimensional computing (HDC), where footprint cuts usually shrink the feature axis and erode concentration and robustness. Prior HDC decompositions decode via fixed atomic hypervectors, which are ill-suited for compressing learned class prototypes. We introduce DecoHD, which learns directly in a decomposed HDC parameterization: a small, shared set of per-layer channels with multiplicative binding across layers and bundling at the end, yielding a large representational space from compact factors. DecoHD compresses along the class axis via a lightweight bundling head while preserving native bind-bundle-score; training is end-to-end, and inference remains pure HDC, aligning with in/near-memory accelerators. In evaluation, DecoHD attains extreme memory savings with only minor accuracy degradation under tight deployment budgets. On average it stays within about 0.1-0.15% of a strong non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters, and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU (AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x over a baseline HDC ASIC."}}
{"id": "2511.03972", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.03972", "abs": "https://arxiv.org/abs/2511.03972", "authors": ["Semih Cayci"], "title": "Non-Asymptotic Optimization and Generalization Bounds for Stochastic Gauss-Newton in Overparameterized Models", "comment": null, "summary": "An important question in deep learning is how higher-order optimization\nmethods affect generalization. In this work, we analyze a stochastic\nGauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch\nsampling for training overparameterized deep neural networks with smooth\nactivations in a regression setting. Our theoretical contributions are twofold.\nFirst, we establish finite-time convergence bounds via a variable-metric\nanalysis in parameter space, with explicit dependencies on the batch size,\nnetwork width and depth. Second, we derive non-asymptotic generalization bounds\nfor SGN using uniform stability in the overparameterized regime, characterizing\nthe impact of curvature, batch size, and overparameterization on generalization\nperformance. Our theoretical results identify a favorable generalization regime\nfor SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along\nthe optimization path yields tighter stability bounds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u968f\u673a\u9ad8\u65af-\u725b\u987f\u65b9\u6cd5\u5728\u8fc7\u53c2\u6570\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u6536\u655b\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u6709\u9650\u65f6\u95f4\u6536\u655b\u754c\u548c\u6cdb\u5316\u754c\uff0c\u63ed\u793a\u4e86\u66f2\u7387\u3001\u6279\u5927\u5c0f\u548c\u8fc7\u53c2\u6570\u5316\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u9ad8\u9636\u4f18\u5316\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8fc7\u53c2\u6570\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\uff0c\u968f\u673a\u9ad8\u65af-\u725b\u987f\u65b9\u6cd5\u7684\u6536\u655b\u6027\u548c\u6cdb\u5316\u7279\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u5e26Levenberg-Marquardt\u963b\u5c3c\u548cmini-batch\u91c7\u6837\u7684\u968f\u673a\u9ad8\u65af-\u725b\u987f\u65b9\u6cd5\uff0c\u5728\u56de\u5f52\u8bbe\u7f6e\u4e2d\u8bad\u7ec3\u5177\u6709\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\u7684\u8fc7\u53c2\u6570\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u53d8\u91cf\u5ea6\u91cf\u5206\u6790\u548c\u5747\u5300\u7a33\u5b9a\u6027\u7406\u8bba\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5efa\u7acb\u4e86\u6709\u9650\u65f6\u95f4\u6536\u655b\u754c\uff0c\u660e\u786e\u4f9d\u8d56\u4e8e\u6279\u5927\u5c0f\u3001\u7f51\u7edc\u5bbd\u5ea6\u548c\u6df1\u5ea6\uff1b\u63a8\u5bfc\u4e86\u975e\u6e10\u8fd1\u6cdb\u5316\u754c\uff0c\u8bc6\u522b\u4e86\u9ad8\u65af-\u725b\u987f\u77e9\u9635\u6700\u5c0f\u7279\u5f81\u503c\u8f83\u5927\u65f6\u6cdb\u5316\u6027\u80fd\u66f4\u4f18\u7684\u6709\u5229\u673a\u5236\u3002", "conclusion": "\u968f\u673a\u9ad8\u65af-\u725b\u987f\u65b9\u6cd5\u5728\u8fc7\u53c2\u6570\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u5177\u6709\u826f\u597d\u7684\u6536\u655b\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u66f2\u7387\u7279\u6027\u3001\u6279\u5927\u5c0f\u548c\u8fc7\u53c2\u6570\u5316\u7a0b\u5ea6\u5171\u540c\u5f71\u54cd\u6cdb\u5316\u8868\u73b0\u3002"}}
{"id": "2511.03924", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03924", "abs": "https://arxiv.org/abs/2511.03924", "authors": ["Ekin U\u011furel", "Cynthia Chen", "Brian H. Y. Lee", "Filipe Rodrigues"], "title": "On Predicting Sociodemographics from Mobility Signals", "comment": "22 pages, 8 figures", "summary": "Inferring sociodemographic attributes from mobility data could help\ntransportation planners better leverage passively collected datasets, but this\ntask remains difficult due to weak and inconsistent relationships between\nmobility patterns and sociodemographic traits, as well as limited\ngeneralization across contexts. We address these challenges from three angles.\nFirst, to improve predictive accuracy while retaining interpretability, we\nintroduce a behaviorally grounded set of higher-order mobility descriptors\nbased on directed mobility graphs. These features capture structured patterns\nin trip sequences, travel modes, and social co-travel, and significantly\nimprove prediction of age, gender, income, and household structure over\nbaselines features. Second, we introduce metrics and visual diagnostic tools\nthat encourage evenness between model confidence and accuracy, enabling\nplanners to quantify uncertainty. Third, to improve generalization and sample\nefficiency, we develop a multitask learning framework that jointly predicts\nmultiple sociodemographic attributes from a shared representation. This\napproach outperforms single-task models, particularly when training data are\nlimited or when applying models across different time periods (i.e., when the\ntest set distribution differs from the training set).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u79fb\u52a8\u6570\u636e\u63a8\u65ad\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u884c\u4e3a\u5bfc\u5411\u7684\u9ad8\u9636\u79fb\u52a8\u63cf\u8ff0\u7b26\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5de5\u5177\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4ece\u79fb\u52a8\u6570\u636e\u63a8\u65ad\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u6709\u52a9\u4e8e\u4ea4\u901a\u89c4\u5212\uff0c\u4f46\u7531\u4e8e\u79fb\u52a8\u6a21\u5f0f\u4e0e\u793e\u4f1a\u4eba\u53e3\u7279\u5f81\u5173\u7cfb\u5f31\u4e14\u4e0d\u4e00\u81f4\uff0c\u4ee5\u53ca\u8de8\u60c5\u5883\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u8fd9\u4e00\u4efb\u52a1\u4ecd\u7136\u56f0\u96be\u3002", "method": "1) \u57fa\u4e8e\u6709\u5411\u79fb\u52a8\u56fe\u7684\u884c\u4e3a\u5bfc\u5411\u9ad8\u9636\u79fb\u52a8\u63cf\u8ff0\u7b26\uff1b2) \u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u51c6\u786e\u6027\u7684\u5747\u8861\u5ea6\u91cf\u548c\u53ef\u89c6\u5316\u8bca\u65ad\u5de5\u5177\uff1b3) \u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u5171\u4eab\u8868\u793a\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u793e\u4f1a\u4eba\u53e3\u5c5e\u6027\u3002", "result": "\u65b0\u7279\u5f81\u663e\u8457\u63d0\u9ad8\u4e86\u5e74\u9f84\u3001\u6027\u522b\u3001\u6536\u5165\u548c\u5bb6\u5ead\u7ed3\u6784\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff1b\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u6216\u6d4b\u8bd5\u96c6\u5206\u5e03\u4e0e\u8bad\u7ec3\u96c6\u4e0d\u540c\u65f6\u4f18\u4e8e\u5355\u4efb\u52a1\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u884c\u4e3a\u5bfc\u5411\u7279\u5f81\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u79fb\u52a8\u6570\u636e\u63a8\u65ad\u793e\u4f1a\u4eba\u53e3\u5c5e\u6027\u7684\u6311\u6218\uff0c\u4e3a\u4ea4\u901a\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5de5\u5177\u3002"}}
{"id": "2511.04000", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.04000", "abs": "https://arxiv.org/abs/2511.04000", "authors": ["Kyaw Hpone Myint", "Zhe Wu", "Alexandre G. R. Day", "Giri Iyengar"], "title": "Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations", "comment": "9 pages, 3 figures, Neurips 2025 GenAI in Finance Workshop", "summary": "Decision trees are widely used in high-stakes fields like finance and\nhealthcare due to their interpretability. This work introduces an efficient,\nscalable method for generating synthetic pre-training data to enable\nmeta-learning of decision trees. Our approach samples near-optimal decision\ntrees synthetically, creating large-scale, realistic datasets. Using the\nMetaTree transformer architecture, we demonstrate that this method achieves\nperformance comparable to pre-training on real-world data or with\ncomputationally expensive optimal decision trees. This strategy significantly\nreduces computational costs, enhances data generation flexibility, and paves\nthe way for scalable and efficient meta-learning of interpretable decision tree\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u51b3\u7b56\u6811\u7684\u5143\u5b66\u4e60\uff0c\u901a\u8fc7\u751f\u6210\u63a5\u8fd1\u6700\u4f18\u7684\u51b3\u7b56\u6811\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u51b3\u7b56\u6811\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u4f20\u7edf\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u771f\u5b9e\u6570\u636e\u6216\u8ba1\u7b97\u6602\u8d35\u7684\u4f18\u5316\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u91c7\u6837\u63a5\u8fd1\u6700\u4f18\u7684\u51b3\u7b56\u6811\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528MetaTree transformer\u67b6\u6784\u8fdb\u884c\u5143\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u6216\u6700\u4f18\u51b3\u7b56\u6811\u9884\u8bad\u7ec3\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u751f\u6210\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u51b3\u7b56\u6811\u5143\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\u3002"}}
{"id": "2511.03928", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03928", "abs": "https://arxiv.org/abs/2511.03928", "authors": ["Arthur Chen", "Victor Zhong"], "title": "SynQuE: Estimating Synthetic Dataset Quality Without Annotations", "comment": "Under review", "summary": "We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE)\nproblem: ranking synthetic datasets by their expected real-world task\nperformance using only limited unannotated real data. This addresses a critical\nand open challenge where data is scarce due to collection costs or privacy\nconstraints. We establish the first comprehensive benchmarks for this problem\nby introducing and evaluating proxy metrics that choose synthetic data for\ntraining to maximize task performance on real data. We introduce the first\nproxy metrics for SynQuE by adapting distribution and diversity-based distance\nmeasures to our context via embedding models. To address the shortcomings of\nthese metrics on complex planning tasks, we propose LENS, a novel proxy that\nleverages large language model reasoning. Our results show that SynQuE proxies\ncorrelate with real task performance across diverse tasks, including sentiment\nanalysis, Text2SQL, web navigation, and image classification, with LENS\nconsistently outperforming others on complex tasks by capturing nuanced\ncharacteristics. For instance, on text-to-SQL parsing, training on the top-3\nsynthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to\n38.4 (+8.1)% on average compared to selecting data indiscriminately. This work\nestablishes SynQuE as a practical framework for synthetic data selection under\nreal-data scarcity and motivates future research on foundation model-based data\ncharacterization and fine-grained data selection.", "AI": {"tldr": "\u63d0\u51fa\u4e86SynQuE\u95ee\u9898\uff1a\u4f7f\u7528\u6709\u9650\u7684\u672a\u6807\u6ce8\u771f\u5b9e\u6570\u636e\u6765\u8bc4\u4f30\u5408\u6210\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u5206\u5e03\u8ddd\u79bb\u3001\u591a\u6837\u6027\u548cLLM\u63a8\u7406\u7684\u4ee3\u7406\u6307\u6807\u6765\u9884\u6d4b\u5408\u6210\u6570\u636e\u5728\u771f\u5b9e\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5728\u6570\u636e\u7a00\u7f3a\uff08\u6536\u96c6\u6210\u672c\u9ad8\u6216\u9690\u79c1\u9650\u5236\uff09\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u9009\u62e9\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86LENS\u4ee3\u7406\u6307\u6807\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u540c\u65f6\u5bf9\u6bd4\u4e86\u57fa\u4e8e\u5206\u5e03\u8ddd\u79bb\u548c\u591a\u6837\u6027\u7684\u4f20\u7edf\u65b9\u6cd5\u3002", "result": "SynQuE\u4ee3\u7406\u6307\u6807\u4e0e\u771f\u5b9e\u4efb\u52a1\u6027\u80fd\u76f8\u5173\uff0c\u5728\u60c5\u611f\u5206\u6790\u3001Text2SQL\u3001\u7f51\u9875\u5bfc\u822a\u548c\u56fe\u50cf\u5206\u7c7b\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0cLENS\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\u3002\u4f8b\u5982\u5728Text2SQL\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528SynQuE\u9009\u62e9\u7684\u6570\u636e\u96c6\u53ef\u5c06\u51c6\u786e\u7387\u4ece30.4%\u63d0\u5347\u81f338.4%\u3002", "conclusion": "SynQuE\u4e3a\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u5408\u6210\u6570\u636e\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u5e76\u63a8\u52a8\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6570\u636e\u8868\u5f81\u548c\u7ec6\u7c92\u5ea6\u6570\u636e\u9009\u62e9\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2511.04160", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.04160", "abs": "https://arxiv.org/abs/2511.04160", "authors": ["Laurits Fredsgaard", "Mikkel N. Schmidt"], "title": "On Joint Regularization and Calibration in Deep Ensembles", "comment": "39 pages, 8 figures, 11 tables", "summary": "Deep ensembles are a powerful tool in machine learning, improving both model\nperformance and uncertainty calibration. While ensembles are typically formed\nby training and tuning models individually, evidence suggests that jointly\ntuning the ensemble can lead to better performance. This paper investigates the\nimpact of jointly tuning weight decay, temperature scaling, and early stopping\non both predictive performance and uncertainty quantification. Additionally, we\npropose a partially overlapping holdout strategy as a practical compromise\nbetween enabling joint evaluation and maximizing the use of data for training.\nOur results demonstrate that jointly tuning the ensemble generally matches or\nimproves performance, with significant variation in effect size across\ndifferent tasks and metrics. We highlight the trade-offs between individual and\njoint optimization in deep ensemble training, with the overlapping holdout\nstrategy offering an attractive practical solution. We believe our findings\nprovide valuable insights and guidance for practitioners looking to optimize\ndeep ensemble models. Code is available at:\nhttps://github.com/lauritsf/ensemble-optimality-gap", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8054\u5408\u8c03\u4f18\u6df1\u5ea6\u96c6\u6210\u6a21\u578b\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u90e8\u5206\u91cd\u53e0\u4fdd\u7559\u7b56\u7565\u4f5c\u4e3a\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u6df1\u5ea6\u96c6\u6210\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u4f46\u901a\u5e38\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u3002\u8bc1\u636e\u8868\u660e\u8054\u5408\u8c03\u4f18\u96c6\u6210\u53ef\u80fd\u5e26\u6765\u66f4\u597d\u6027\u80fd\u3002", "method": "\u7814\u7a76\u8054\u5408\u8c03\u4f18\u6743\u91cd\u8870\u51cf\u3001\u6e29\u5ea6\u7f29\u653e\u548c\u65e9\u505c\u5bf9\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u90e8\u5206\u91cd\u53e0\u4fdd\u7559\u7b56\u7565\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u4f7f\u7528\u548c\u8054\u5408\u8bc4\u4f30\u7684\u6298\u4e2d\u65b9\u6848\u3002", "result": "\u8054\u5408\u8c03\u4f18\u96c6\u6210\u901a\u5e38\u80fd\u5339\u914d\u6216\u6539\u8fdb\u6027\u80fd\uff0c\u4f46\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6307\u6807\u4e0a\u6548\u679c\u5dee\u5f02\u663e\u8457\u3002\u90e8\u5206\u91cd\u53e0\u4fdd\u7559\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6df1\u5ea6\u96c6\u6210\u8bad\u7ec3\u4e2d\u4e2a\u4f53\u4f18\u5316\u4e0e\u8054\u5408\u4f18\u5316\u7684\u6743\u8861\uff0c\u4e3a\u5b9e\u8df5\u8005\u4f18\u5316\u6df1\u5ea6\u96c6\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u6307\u5bfc\u3002"}}
{"id": "2511.03929", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03929", "abs": "https://arxiv.org/abs/2511.03929", "authors": ["NVIDIA", ":", "Amala Sanjay Deshmukh", "Kateryna Chumachenko", "Tuomas Rintamaki", "Matthieu Le", "Tyler Poon", "Danial Mohseni Taheri", "Ilia Karmanov", "Guilin Liu", "Jarno Seppanen", "Guo Chen", "Karan Sapra", "Zhiding Yu", "Adi Renduchintala", "Charles Wang", "Peter Jin", "Arushi Goel", "Mike Ranzinger", "Lukas Voegtle", "Philipp Fischer", "Timo Roman", "Wei Ping", "Boxin Wang", "Zhuolin Yang", "Nayeon Lee", "Shaokun Zhang", "Fuxiao Liu", "Zhiqi Li", "Di Zhang", "Greg Heinrich", "Hongxu", "Yin", "Song Han", "Pavlo Molchanov", "Parth Mannan", "Yao Xu", "Jane Polak Scowcroft", "Tom Balough", "Subhashree Radhakrishnan", "Paris Zhang", "Sean Cha", "Ratnesh Kumar", "Zaid Pervaiz Bhat", "Jian Zhang", "Darragh Hanley", "Pritam Biswas", "Jesse Oliver", "Kevin Vasques", "Roger Waleffe", "Duncan Riach", "Oluwatobi Olabiyi", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Pritam Gundecha", "Khanh Nguyen", "Alexandre Milesi", "Eugene Khvedchenia", "Ran Zilberstein", "Ofri Masad", "Natan Bagrov", "Nave Assaf", "Tomer Asida", "Daniel Afrimi", "Amit Zuker", "Netanel Haber", "Zhiyu Cheng", "Jingyu", "Xin", "Di", "Wu", "Nik Spirin", "Maryam Moosaei", "Roman Ageev", "Vanshil Atul Shah", "Yuting Wu", "Daniel Korzekwa", "Unnikrishnan Kizhakkemadam Sreekumar", "Wanli Jiang", "Padmavathy Subramanian", "Alejandra Rico", "Sandip Bhaskar", "Saeid Motiian", "Kedi Wu", "Annie Surla", "Chia-Chih Chen", "Hayden Wolff", "Matthew Feinberg", "Melissa Corpuz", "Marek Wawrzos", "Eileen Long", "Aastha Jhunjhunwala", "Paul Hendricks", "Farzan Memarian", "Benika Hall", "Xin-Yu Wang", "David Mosallanezhad", "Soumye Singhal", "Luis Vega", "Katherine Cheung", "Krzysztof Pawelec", "Michael Evans", "Katherine Luna", "Jie Lou", "Erick Galinkin", "Akshay Hazare", "Kaustubh Purandare", "Ann Guan", "Anna Warno", "Chen Cui", "Yoshi Suhara", "Shibani Likhite", "Seph Mard", "Meredith Price", "Laya Sleiman", "Saori Kaji", "Udi Karpas", "Kari Briski", "Joey Conway", "Michael Lightstone", "Jan Kautz", "Mohammad Shoeybi", "Mostofa Patwary", "Jonathen Cohen", "Oleksii Kuchaiev", "Andrew Tao", "Bryan Catanzaro"], "title": "NVIDIA Nemotron Nano V2 VL", "comment": null, "summary": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron\nvision-language series designed for strong real-world document understanding,\nlong video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers\nsignificant improvements over our previous model,\nLlama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major\nenhancements in model architecture, datasets, and training recipes. Nemotron\nNano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and\ninnovative token reduction techniques to achieve higher inference throughput in\nlong document and video scenarios. We are releasing model checkpoints in BF16,\nFP8, and FP4 formats and sharing large parts of our datasets, recipes and\ntraining code.", "AI": {"tldr": "Nemotron Nano V2 VL\u662fNemotron\u89c6\u89c9\u8bed\u8a00\u7cfb\u5217\u7684\u6700\u65b0\u6a21\u578b\uff0c\u4e13\u4e3a\u771f\u5b9e\u4e16\u754c\u6587\u6863\u7406\u89e3\u3001\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u8bbe\u8ba1\uff0c\u76f8\u6bd4\u524d\u4ee3\u6a21\u578b\u5728\u6240\u6709\u89c6\u89c9\u548c\u6587\u672c\u9886\u57df\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5728\u771f\u5b9e\u4e16\u754c\u6587\u6863\u7406\u89e3\u3001\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u65b9\u6cd5\u6765\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "method": "\u57fa\u4e8eNemotron Nano V2\u6df7\u5408Mamba-Transformer LLM\u67b6\u6784\uff0c\u91c7\u7528\u521b\u65b0\u7684token\u51cf\u5c11\u6280\u672f\u6765\u63d0\u9ad8\u957f\u6587\u6863\u548c\u89c6\u9891\u573a\u666f\u4e0b\u7684\u63a8\u7406\u541e\u5410\u91cf\u3002", "result": "\u76f8\u6bd4\u524d\u4ee3\u6a21\u578bLlama-3.1-Nemotron-Nano-VL-8B\uff0c\u5728\u6240\u6709\u89c6\u89c9\u548c\u6587\u672c\u9886\u57df\u90fd\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u5728\u957f\u6587\u6863\u548c\u89c6\u9891\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u63a8\u7406\u541e\u5410\u91cf\u3002", "conclusion": "Nemotron Nano V2 VL\u5728\u6587\u6863\u7406\u89e3\u3001\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u56e2\u961f\u53d1\u5e03\u4e86BF16\u3001FP8\u548cFP4\u683c\u5f0f\u7684\u6a21\u578b\u68c0\u67e5\u70b9\uff0c\u5e76\u5206\u4eab\u4e86\u5927\u90e8\u5206\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u65b9\u6cd5\u548c\u4ee3\u7801\u3002"}}
{"id": "2511.04445", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.04445", "abs": "https://arxiv.org/abs/2511.04445", "authors": ["Syeda Sitara Wishal Fatima", "Afshin Rahimi"], "title": "ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting", "comment": "Portions of this work were previously published in the author's\n  Master's thesis at University of Windsor (2024)", "summary": "Time series forecasting is essential across domains from finance to supply\nchain management. This paper introduces ForecastGAN, a novel decomposition\nbased adversarial framework addressing limitations in existing approaches for\nmulti-horizon predictions. Although transformer models excel in long-term\nforecasting, they often underperform in short-term scenarios and typically\nignore categorical features. ForecastGAN operates through three integrated\nmodules: a Decomposition Module that extracts seasonality and trend components;\na Model Selection Module that identifies optimal neural network configurations\nbased on forecasting horizon; and an Adversarial Training Module that enhances\nprediction robustness through Conditional Generative Adversarial Network\ntraining. Unlike conventional approaches, ForecastGAN effectively integrates\nboth numerical and categorical features. We validate our framework on eleven\nbenchmark multivariate time series datasets that span various forecasting\nhorizons. The results show that ForecastGAN consistently outperforms\nstate-of-the-art transformer models for short-term forecasting while remaining\ncompetitive for long-term horizons. This research establishes a more\ngeneralizable approach to time series forecasting that adapts to specific\ncontexts while maintaining strong performance across diverse data\ncharacteristics without extensive hyperparameter tuning.", "AI": {"tldr": "ForecastGAN\u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u89e3\u7684\u5bf9\u6297\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u65f6\u95f4\u8303\u56f4\u9884\u6d4b\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u96c6\u6210\u6570\u503c\u548c\u5206\u7c7b\u7279\u5f81\uff0c\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u4f18\u4e8etransformer\u6a21\u578b\u3002", "motivation": "\u73b0\u6709transformer\u6a21\u578b\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\u4e14\u901a\u5e38\u5ffd\u7565\u5206\u7c7b\u7279\u5f81\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u4e0d\u540c\u9884\u6d4b\u8303\u56f4\u5e76\u6709\u6548\u6574\u5408\u5404\u7c7b\u7279\u5f81\u7684\u901a\u7528\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u96c6\u6210\u6a21\u5757\uff1a\u5206\u89e3\u6a21\u5757\u63d0\u53d6\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u6210\u5206\uff1b\u6a21\u578b\u9009\u62e9\u6a21\u5757\u6839\u636e\u9884\u6d4b\u8303\u56f4\u9009\u62e9\u6700\u4f18\u795e\u7ecf\u7f51\u7edc\u914d\u7f6e\uff1b\u5bf9\u6297\u8bad\u7ec3\u6a21\u5757\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u589e\u5f3a\u9884\u6d4b\u9c81\u68d2\u6027\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cForecastGAN\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684transformer\u6a21\u578b\uff0c\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u9002\u5e94\u7279\u5b9a\u60c5\u5883\uff0c\u5728\u4e0d\u540c\u6570\u636e\u7279\u5f81\u4e0b\u4fdd\u6301\u5f3a\u6027\u80fd\uff0c\u65e0\u9700\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u4f18\u3002"}}
{"id": "2511.03938", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03938", "abs": "https://arxiv.org/abs/2511.03938", "authors": ["Sanggeon Yun", "Hyunwoo Oh", "Ryozo Masukawa", "Pietro Mercati", "Nathaniel D. Bastian", "Mohsen Imani"], "title": "LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction", "comment": "Accepted to DATE 2026", "summary": "Hyperdimensional computing (HDC) suits memory, energy, and\nreliability-constrained systems, yet the standard \"one prototype per class\"\ndesign requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior\ncompaction reduces $D$ (feature axis), improving storage/compute but weakening\nrobustness. We introduce LogHD, a logarithmic class-axis reduction that\nreplaces the $C$ per-class prototypes with $n\\!\\approx\\!\\lceil\\log_k C\\rceil$\nbundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional\nactivation space, cutting memory to $O(D\\log_k C)$ while preserving $D$. LogHD\nuses a capacity-aware codebook and profile-based decoding, and composes with\nfeature-axis sparsification. Across datasets and injected bit flips, LogHD\nattains competitive accuracy with smaller models and higher resilience at\nmatched memory. Under equal memory, it sustains target accuracy at roughly\n$2.5$-$3.0\\times$ higher bit-flip rates than feature-axis compression; an ASIC\ninstantiation delivers $498\\times$ energy efficiency and $62.6\\times$ speedup\nover an AMD Ryzen 9 9950X and $24.3\\times$/$6.58\\times$ over an NVIDIA RTX\n4090, and is $4.06\\times$ more energy-efficient and $2.19\\times$ faster than a\nfeature-axis HDC ASIC baseline.", "AI": {"tldr": "LogHD\u662f\u4e00\u79cd\u8d85\u7ef4\u8ba1\u7b97\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u7c7b\u522b\u8f74\u8fdb\u884c\u5bf9\u6570\u538b\u7f29\uff0c\u5c06\u5185\u5b58\u9700\u6c42\u4eceO(CD)\u964d\u4f4e\u5230O(D log_k C)\uff0c\u540c\u65f6\u4fdd\u6301\u7ef4\u5ea6D\u4e0d\u53d8\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u6807\u51c6\u8d85\u7ef4\u8ba1\u7b97\u7684\"\u6bcf\u7c7b\u4e00\u4e2a\u539f\u578b\"\u8bbe\u8ba1\u9700\u8981O(CD)\u5185\u5b58\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u4e3b\u8981\u51cf\u5c11\u7279\u5f81\u7ef4\u5ea6D\uff0c\u4f46\u8fd9\u4f1a\u524a\u5f31\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u538b\u7f29\u5185\u5b58\u53c8\u80fd\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5bf9\u6570\u7c7b\u522b\u8f74\u538b\u7f29\uff0c\u7528n\u2248\u2308log_k C\u2309\u4e2a\u6346\u7ed1\u8d85\u5411\u91cf\u4ee3\u66ffC\u4e2a\u6bcf\u7c7b\u539f\u578b\uff0c\u5728n\u7ef4\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89e3\u7801\uff0c\u7ed3\u5408\u5bb9\u91cf\u611f\u77e5\u7801\u672c\u548c\u57fa\u4e8e\u914d\u7f6e\u6587\u4ef6\u7684\u89e3\u7801\uff0c\u53ef\u4e0e\u7279\u5f81\u8f74\u7a00\u758f\u5316\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6ce8\u5165\u6bd4\u7279\u7ffb\u8f6c\u7684\u60c5\u51b5\u4e0b\uff0cLogHD\u5728\u8f83\u5c0f\u6a21\u578b\u4e0b\u8fbe\u5230\u7ade\u4e89\u6027\u51c6\u786e\u5ea6\uff0c\u5728\u76f8\u540c\u5185\u5b58\u4e0b\u6bd4\u7279\u5f81\u8f74\u538b\u7f29\u65b9\u6cd5\u80fd\u627f\u53d72.5-3.0\u500d\u66f4\u9ad8\u7684\u6bd4\u7279\u7ffb\u8f6c\u7387\u3002ASIC\u5b9e\u73b0\u76f8\u6bd4AMD Ryzen 9 9950X\u5b9e\u73b0498\u500d\u80fd\u6548\u548c62.6\u500d\u52a0\u901f\uff0c\u76f8\u6bd4NVIDIA RTX 4090\u5b9e\u73b024.3\u500d/6.58\u500d\u63d0\u5347\u3002", "conclusion": "LogHD\u901a\u8fc7\u7c7b\u522b\u8f74\u7684\u5bf9\u6570\u538b\u7f29\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u7ef4\u8ba1\u7b97\u7684\u5185\u5b58\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u786c\u4ef6\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04518", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML", "68T05, 62J07, 65M20, 65M60", "I.2.6; G.1.2; G.1.8"], "pdf": "https://arxiv.org/pdf/2511.04518", "abs": "https://arxiv.org/abs/2511.04518", "authors": ["Obed Amo", "Samit Ghosh", "Markus Lange-Hegermann", "Bogdan Rai\u0163\u0103", "Michael Pokojovy"], "title": "Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity", "comment": "14 pages, 2 figures", "summary": "We present a new benchmarking study comparing a boundary-constrained\nEhrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical\nfinite element method combined with Crank--Nicolson time stepping (CN-FEM) for\nsolving the two-dimensional wave equation with homogeneous Dirichlet boundary\nconditions. The B-EPGP construction leverages exponential-polynomial bases\nderived from the characteristic variety to enforce the PDE and boundary\nconditions exactly and employs penalized least squares to estimate the\ncoefficients. To ensure fairness across paradigms, we introduce a\ndegrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP\nconsistently attains lower space-time $L^2$-error and maximum-in-time\n$L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of\nmagnitude.", "AI": {"tldr": "\u6bd4\u8f83B-EPGP\u548cCN-FEM\u6c42\u89e3\u4e8c\u7ef4\u6ce2\u52a8\u65b9\u7a0b\u7684\u6027\u80fd\uff0cB-EPGP\u5728\u5339\u914d\u81ea\u7531\u5ea6\u4e0b\u7cbe\u5ea6\u63d0\u9ad8\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7", "motivation": "\u8bc4\u4f30\u8fb9\u754c\u7ea6\u675fEhrenpreis-Palamodov\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u6a21\u578b\u4e0e\u4f20\u7edf\u6709\u9650\u5143\u65b9\u6cd5\u5728\u6c42\u89e3\u4e8c\u7ef4\u6ce2\u52a8\u65b9\u7a0b\u65f6\u7684\u6027\u80fd\u5dee\u5f02", "method": "B-EPGP\u4f7f\u7528\u6307\u6570\u591a\u9879\u5f0f\u57fa\u51fd\u6570\u7cbe\u786e\u6ee1\u8db3PDE\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u91c7\u7528\u60e9\u7f5a\u6700\u5c0f\u4e8c\u4e58\u6cd5\u4f30\u8ba1\u7cfb\u6570\uff1bCN-FEM\u4f7f\u7528\u7ecf\u5178\u6709\u9650\u5143\u6cd5\u7ed3\u5408Crank-Nicolson\u65f6\u95f4\u6b65\u8fdb", "result": "\u5728\u5339\u914d\u81ea\u7531\u5ea6\u6761\u4ef6\u4e0b\uff0cB-EPGP\u5728\u7a7a\u95f4-\u65f6\u95f4L^2\u8bef\u5dee\u548c\u7a7a\u95f4\u6700\u5927\u65f6\u95f4L^2\u8bef\u5dee\u65b9\u9762\u5747\u4f18\u4e8eCN-FEM\uff0c\u7cbe\u5ea6\u63d0\u9ad8\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7", "conclusion": "B-EPGP\u65b9\u6cd5\u5728\u6c42\u89e3\u4e8c\u7ef4\u6ce2\u52a8\u65b9\u7a0b\u65f6\u6bd4\u4f20\u7edfCN-FEM\u65b9\u6cd5\u5177\u6709\u663e\u8457\u66f4\u9ad8\u7684\u7cbe\u5ea6\u4f18\u52bf"}}
{"id": "2511.03939", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03939", "abs": "https://arxiv.org/abs/2511.03939", "authors": ["Raghav Sharma", "Manan Mehta", "Sai Tiger Raina"], "title": "RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is the standard for\naligning Large Language Models (LLMs), yet recent progress has moved beyond\ncanonical text-based methods. This survey synthesizes the new frontier of\nalignment research by addressing critical gaps in multi-modal alignment,\ncultural fairness, and low-latency optimization. To systematically explore\nthese domains, we first review foundational algo- rithms, including PPO, DPO,\nand GRPO, before presenting a detailed analysis of the latest innovations. By\nproviding a comparative synthesis of these techniques and outlining open\nchallenges, this work serves as an essential roadmap for researchers building\nmore robust, efficient, and equitable AI systems.", "AI": {"tldr": "\u8be5\u8c03\u67e5\u8bba\u6587\u7cfb\u7edf\u68b3\u7406\u4e86\u8d85\u8d8a\u4f20\u7edf\u6587\u672c\u65b9\u6cd5\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60(RLHF)\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u591a\u6a21\u6001\u5bf9\u9f50\u3001\u6587\u5316\u516c\u5e73\u6027\u548c\u4f4e\u5ef6\u8fdf\u4f18\u5316\u7b49\u5173\u952e\u9886\u57df\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dRLHF\u7814\u7a76\u5728\u591a\u6a21\u6001\u5bf9\u9f50\u3001\u6587\u5316\u516c\u5e73\u6027\u548c\u4f4e\u5ef6\u8fdf\u4f18\u5316\u65b9\u9762\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u9ad8\u6548\u548c\u516c\u5e73\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u9996\u5148\u56de\u987ePPO\u3001DPO\u3001GRPO\u7b49\u57fa\u7840\u7b97\u6cd5\uff0c\u7136\u540e\u8be6\u7ec6\u5206\u6790\u6700\u65b0\u521b\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u6bd4\u8f83\u6027\u7efc\u5408\u68b3\u7406\u8fd9\u4e9b\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u9f50\u7814\u7a76\u65b0\u524d\u6cbf\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u6bd4\u8f83\u4e86\u5404\u79cd\u6280\u672f\u7684\u4f18\u52a3\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u6307\u660e\u4e86\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7814\u7a76\u4eba\u5458\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u9ad8\u6548\u548c\u516c\u5e73\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8def\u7ebf\u56fe\uff0c\u5e76\u6307\u51fa\u4e86\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2511.04666", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.04666", "abs": "https://arxiv.org/abs/2511.04666", "authors": ["Ben Sanati", "Thomas L. Lee", "Trevor McInroe", "Aidan Scannell", "Nikolay Malkin", "David Abel", "Amos Storkey"], "title": "Forgetting is Everywhere", "comment": "Project page:\n  https://ben-sanati.github.io/forgetting-is-everywhere-project/", "summary": "A fundamental challenge in developing general learning algorithms is their\ntendency to forget past knowledge when adapting to new data. Addressing this\nproblem requires a principled understanding of forgetting; yet, despite decades\nof study, no unified definition has emerged that provides insights into the\nunderlying dynamics of learning. We propose an algorithm- and task-agnostic\ntheory that characterises forgetting as a lack of self-consistency in a\nlearner's predictive distribution over future experiences, manifesting as a\nloss of predictive information. Our theory naturally yields a general measure\nof an algorithm's propensity to forget. To validate the theory, we design a\ncomprehensive set of experiments that span classification, regression,\ngenerative modelling, and reinforcement learning. We empirically demonstrate\nhow forgetting is present across all learning settings and plays a significant\nrole in determining learning efficiency. Together, these results establish a\nprincipled understanding of forgetting and lay the foundation for analysing and\nimproving the information retention capabilities of general learning\nalgorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b97\u6cd5\u548c\u4efb\u52a1\u65e0\u5173\u7684\u9057\u5fd8\u7406\u8bba\uff0c\u5c06\u9057\u5fd8\u5b9a\u4e49\u4e3a\u5b66\u4e60\u8005\u5728\u672a\u6765\u7ecf\u9a8c\u9884\u6d4b\u5206\u5e03\u4e2d\u7f3a\u4e4f\u81ea\u4e00\u81f4\u6027\uff0c\u8868\u73b0\u4e3a\u9884\u6d4b\u4fe1\u606f\u7684\u635f\u5931\u3002\u8be5\u7406\u8bba\u81ea\u7136\u5730\u4ea7\u751f\u4e86\u4e00\u4e2a\u8861\u91cf\u7b97\u6cd5\u9057\u5fd8\u503e\u5411\u7684\u901a\u7528\u6307\u6807\u3002", "motivation": "\u5f00\u53d1\u901a\u7528\u5b66\u4e60\u7b97\u6cd5\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u662f\u5b83\u4eec\u5728\u9002\u5e94\u65b0\u6570\u636e\u65f6\u503e\u5411\u4e8e\u5fd8\u8bb0\u8fc7\u53bb\u7684\u77e5\u8bc6\u3002\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u9700\u8981\u5bf9\u9057\u5fd8\u6709\u539f\u5219\u6027\u7684\u7406\u89e3\uff0c\u4f46\u5c3d\u7ba1\u7ecf\u8fc7\u51e0\u5341\u5e74\u7684\u7814\u7a76\uff0c\u8fd8\u6ca1\u6709\u51fa\u73b0\u80fd\u591f\u6d1e\u5bdf\u5b66\u4e60\u5e95\u5c42\u52a8\u6001\u7684\u7edf\u4e00\u5b9a\u4e49\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b97\u6cd5\u548c\u4efb\u52a1\u65e0\u5173\u7684\u7406\u8bba\uff0c\u5c06\u9057\u5fd8\u8868\u5f81\u4e3a\u5b66\u4e60\u8005\u5728\u672a\u6765\u7ecf\u9a8c\u9884\u6d4b\u5206\u5e03\u4e2d\u7f3a\u4e4f\u81ea\u4e00\u81f4\u6027\uff0c\u8868\u73b0\u4e3a\u9884\u6d4b\u4fe1\u606f\u7684\u635f\u5931\u3002\u8be5\u7406\u8bba\u81ea\u7136\u5730\u4ea7\u751f\u4e86\u4e00\u4e2a\u8861\u91cf\u7b97\u6cd5\u9057\u5fd8\u503e\u5411\u7684\u901a\u7528\u6307\u6807\u3002", "result": "\u901a\u8fc7\u8bbe\u8ba1\u6db5\u76d6\u5206\u7c7b\u3001\u56de\u5f52\u3001\u751f\u6210\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5168\u9762\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7406\u8bba\uff0c\u7ecf\u9a8c\u8bc1\u660e\u9057\u5fd8\u5b58\u5728\u4e8e\u6240\u6709\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u5e76\u5728\u51b3\u5b9a\u5b66\u4e60\u6548\u7387\u65b9\u9762\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5efa\u7acb\u4e86\u5bf9\u9057\u5fd8\u7684\u539f\u5219\u6027\u7406\u89e3\uff0c\u4e3a\u5206\u6790\u548c\u6539\u8fdb\u901a\u7528\u5b66\u4e60\u7b97\u6cd5\u7684\u4fe1\u606f\u4fdd\u7559\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.03966", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03966", "abs": "https://arxiv.org/abs/2511.03966", "authors": ["Mingliang Hou", "Yinuo Wang", "Teng Guo", "Zitao Liu", "Wenzhou Dou", "Jiaqi Zheng", "Renqiang Luo", "Mi Tian", "Weiqi Luo"], "title": "PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in Cognitive Diagnosis", "comment": null, "summary": "The need to remove specific student data from cognitive diagnosis (CD) models\nhas become a pressing requirement, driven by users' growing assertion of their\n\"right to be forgotten\". However, existing CD models are largely designed\nwithout privacy considerations and lack effective data unlearning mechanisms.\nDirectly applying general purpose unlearning algorithms is suboptimal, as they\nstruggle to balance unlearning completeness, model utility, and efficiency when\nconfronted with the unique heterogeneous structure of CD models. To address\nthis, our paper presents the first systematic study of the data unlearning\nproblem for CD models, proposing a novel and efficient algorithm: hierarchical\nimportanceguided forgetting (HIF). Our key insight is that parameter importance\nin CD models exhibits distinct layer wise characteristics. HIF leverages this\nvia an innovative smoothing mechanism that combines individual and layer, level\nimportance, enabling a more precise distinction of parameters associated with\nthe data to be unlearned. Experiments on three real world datasets show that\nHIF significantly outperforms baselines on key metrics, offering the first\neffective solution for CD models to respond to user data removal requests and\nfor deploying high-performance, privacy preserving AI systems", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\u7684\u6570\u636e\u9057\u5fd8\u7b97\u6cd5HIF\uff0c\u901a\u8fc7\u5206\u5c42\u91cd\u8981\u6027\u5f15\u5bfc\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u3001\u5b8c\u6574\u7684\u6570\u636e\u5220\u9664\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd", "motivation": "\u7528\u6237\u5bf9\"\u88ab\u9057\u5fd8\u6743\"\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u73b0\u6709\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\u7f3a\u4e4f\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u901a\u7528\u9057\u5fd8\u7b97\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406CD\u6a21\u578b\u7684\u5f02\u6784\u7ed3\u6784", "method": "\u63d0\u51fa\u5206\u5c42\u91cd\u8981\u6027\u5f15\u5bfc\u9057\u5fd8\u7b97\u6cd5(HIF)\uff0c\u5229\u7528\u53c2\u6570\u91cd\u8981\u6027\u7684\u5206\u5c42\u7279\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5e73\u6ed1\u673a\u5236\u7ed3\u5408\u4e2a\u4f53\u548c\u5c42\u7ea7\u91cd\u8981\u6027\uff0c\u7cbe\u786e\u8bc6\u522b\u5f85\u9057\u5fd8\u6570\u636e\u76f8\u5173\u53c2\u6570", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHIF\u5728\u5173\u952e\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "HIF\u4e3a\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u6709\u6548\u7684\u6570\u636e\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u90e8\u7f72\u9ad8\u6027\u80fd\u3001\u9690\u79c1\u4fdd\u62a4\u7684AI\u7cfb\u7edf"}}
{"id": "2511.03976", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.03976", "abs": "https://arxiv.org/abs/2511.03976", "authors": ["Xu Zou"], "title": "PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction", "comment": "preprint", "summary": "Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable\nevolutionary trajectory, characterized by the continual emergence of\nimmune-evasive variants. This poses persistent challenges to public health and\nvaccine development.\n  While large-scale generative pre-trained transformers (GPTs) have\nrevolutionized the modeling of sequential data, their direct applications to\nnoisy viral genomic sequences are limited. In this paper, we introduce\nPETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based\non evolutionary trajectories derived from phylogenetic trees rather than raw\nRNA sequences. This method effectively mitigates sequencing noise and captures\nthe hierarchical structure of viral evolution.\n  With a weighted training framework to address substantial geographical and\ntemporal imbalances in global sequence data, PETRA excels in predicting future\nSARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide\nmutations and 17.10\\% for spike amino-acid mutations, compared to 0.49% and\n6.64% respectively for the best baseline. PETRA also demonstrates its ability\nto aid in the real-time mutation prediction of major clades like 24F(XEC) and\n25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra", "AI": {"tldr": "PETRA\u662f\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u8f68\u8ff9\u800c\u975e\u539f\u59cbRNA\u5e8f\u5217\u7684\u9884\u8bad\u7ec3\u53d8\u6362\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u53d1\u80b2\u6811\u63d0\u53d6\u8fdb\u5316\u8def\u5f84\uff0c\u6709\u6548\u51cf\u5c11\u6d4b\u5e8f\u566a\u58f0\u5e76\u6355\u6349\u75c5\u6bd2\u8fdb\u5316\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5728\u9884\u6d4bSARS-CoV-2\u672a\u6765\u7a81\u53d8\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "SARS-CoV-2\u5177\u6709\u5feb\u901f\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u8fdb\u5316\u8f68\u8ff9\uff0c\u6301\u7eed\u51fa\u73b0\u514d\u75ab\u9003\u9038\u53d8\u79cd\uff0c\u5bf9\u516c\u5171\u536b\u751f\u548c\u75ab\u82d7\u5f00\u53d1\u6784\u6210\u6311\u6218\u3002\u73b0\u6709\u7684\u5927\u89c4\u6a21\u751f\u6210\u9884\u8bad\u7ec3\u53d8\u6362\u5668\u76f4\u63a5\u5e94\u7528\u4e8e\u566a\u58f0\u75c5\u6bd2\u57fa\u56e0\u7ec4\u5e8f\u5217\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faPETRA\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7cfb\u7edf\u53d1\u80b2\u6811\u63d0\u53d6\u7684\u8fdb\u5316\u8f68\u8ff9\u800c\u975e\u539f\u59cbRNA\u5e8f\u5217\uff0c\u91c7\u7528\u52a0\u6743\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u5168\u7403\u5e8f\u5217\u6570\u636e\u7684\u5730\u7406\u548c\u65f6\u95f4\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "PETRA\u5728\u9884\u6d4bSARS-CoV-2\u672a\u6765\u7a81\u53d8\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6838\u82f7\u9178\u7a81\u53d8\u7684\u52a0\u6743\u53ec\u56de\u7387@1\u8fbe\u52309.45%\uff0c\u523a\u7a81\u86cb\u767d\u6c28\u57fa\u9178\u7a81\u53d8\u4e3a17.10%\uff0c\u8fdc\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u76840.49%\u548c6.64%\u3002\u8fd8\u80fd\u5b9e\u65f6\u9884\u6d4b\u4e3b\u8981\u8fdb\u5316\u652f\u7684\u7a81\u53d8\u3002", "conclusion": "PETRA\u901a\u8fc7\u5229\u7528\u8fdb\u5316\u8f68\u8ff9\u800c\u975e\u539f\u59cb\u5e8f\u5217\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u6bd2\u57fa\u56e0\u7ec4\u6570\u636e\u4e2d\u7684\u566a\u58f0\u548c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u9884\u6d4bSARS-CoV-2\u7a81\u53d8\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u75c5\u6bd2\u8fdb\u5316\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2511.03981", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03981", "abs": "https://arxiv.org/abs/2511.03981", "authors": ["Yuxiao Wang", "Di Wu", "Feng Liu", "Zhimin Qiu", "Chenrui Hu"], "title": "Structural Priors and Modular Adapters in the Composable Fine-Tuning Algorithm of Large-Scale Models", "comment": null, "summary": "This paper proposes a composable fine-tuning method that integrates graph\nstructural priors with modular adapters to address the high computational cost\nand structural instability faced by large-scale pre-trained models in\nmulti-task adaptation. The method introduces a relation matrix to model\ndependencies among tasks, explicitly encoding correlations between nodes and\npaths into graph structural priors, which provide unified structural\nconstraints for adapter weight allocation and path selection. Modular adapters\nare embedded into different layers through low-rank mapping and a pluggable\nmechanism, enabling efficient cross-task composition and reuse under prior\nguidance. This mechanism not only improves parameter efficiency and training\nstability but also alleviates path conflicts and redundant computation in\nmulti-task scenarios. Furthermore, experiments on hyperparameter sensitivity,\nenvironmental sensitivity, and data sensitivity are conducted to systematically\nanalyze key factors such as routing temperature, gating thresholds, and\nrelation matrix regularization strength, verifying the consistency and superior\nperformance of the method under structural constraints. The results demonstrate\nthat the proposed framework significantly enhances task prediction accuracy,\nadapter weight allocation precision, and overall computational efficiency while\nmaintaining model lightweight design, highlighting the synergistic advantages\nof graph priors and modular mechanisms in composable fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u7ec4\u5408\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u5148\u9a8c\u548c\u6a21\u5757\u5316\u9002\u914d\u5668\u89e3\u51b3\u5927\u6a21\u578b\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u7ed3\u6784\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u591a\u4efb\u52a1\u9002\u5e94\u65f6\u9762\u4e34\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u7ed3\u6784\u4e0d\u7a33\u5b9a\u6027\u6311\u6218\u3002", "method": "\u5f15\u5165\u5173\u7cfb\u77e9\u9635\u5efa\u6a21\u4efb\u52a1\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5c06\u8282\u70b9\u548c\u8def\u5f84\u76f8\u5173\u6027\u7f16\u7801\u4e3a\u56fe\u7ed3\u6784\u5148\u9a8c\uff0c\u901a\u8fc7\u4f4e\u79e9\u6620\u5c04\u548c\u53ef\u63d2\u62d4\u673a\u5236\u5d4c\u5165\u6a21\u5757\u5316\u9002\u914d\u5668\u3002", "result": "\u663e\u8457\u63d0\u5347\u4efb\u52a1\u9884\u6d4b\u7cbe\u5ea6\u3001\u9002\u914d\u5668\u6743\u91cd\u5206\u914d\u7cbe\u5ea6\u548c\u6574\u4f53\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8f7b\u91cf\u5316\u8bbe\u8ba1\u3002", "conclusion": "\u56fe\u5148\u9a8c\u548c\u6a21\u5757\u5316\u673a\u5236\u5728\u53ef\u7ec4\u5408\u5fae\u8c03\u4e2d\u5177\u6709\u534f\u540c\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u53c2\u6570\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
