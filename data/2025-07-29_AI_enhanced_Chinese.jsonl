{"id": "2507.19540", "categories": ["stat.ML", "cond-mat.stat-mech", "cs.LG", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2507.19540", "abs": "https://arxiv.org/abs/2507.19540", "authors": ["Roger Guimera", "Marta Sales-Pardo"], "title": "Bayesian symbolic regression: Automated equation discovery from a physicists' perspective", "comment": null, "summary": "Symbolic regression automates the process of learning closed-form\nmathematical models from data. Standard approaches to symbolic regression, as\nwell as newer deep learning approaches, rely on heuristic model selection\ncriteria, heuristic regularization, and heuristic exploration of model space.\nHere, we discuss the probabilistic approach to symbolic regression, an\nalternative to such heuristic approaches with direct connections to information\ntheory and statistical physics. We show how the probabilistic approach\nestablishes model plausibility from basic considerations and explicit\napproximations, and how it provides guarantees of performance that heuristic\napproaches lack. We also discuss how the probabilistic approach compels us to\nconsider model ensembles, as opposed to single models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7b26\u53f7\u56de\u5f52\u7684\u6982\u7387\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6a21\u578b\u53ef\u4fe1\u5ea6\u548c\u6027\u80fd\u4fdd\u8bc1\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u7b26\u53f7\u56de\u5f52\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u9009\u62e9\u548c\u63a2\u7d22\uff0c\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u548c\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u91c7\u7528\u6982\u7387\u65b9\u6cd5\uff0c\u7ed3\u5408\u4fe1\u606f\u8bba\u548c\u7edf\u8ba1\u7269\u7406\u5b66\u7684\u7406\u8bba\uff0c\u901a\u8fc7\u57fa\u672c\u8003\u91cf\u548c\u663e\u5f0f\u8fd1\u4f3c\u5efa\u7acb\u6a21\u578b\u53ef\u4fe1\u5ea6\u3002", "result": "\u6982\u7387\u65b9\u6cd5\u63d0\u4f9b\u4e86\u542f\u53d1\u5f0f\u65b9\u6cd5\u6240\u7f3a\u4e4f\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u5e76\u4fc3\u4f7f\u8003\u8651\u6a21\u578b\u96c6\u5408\u800c\u975e\u5355\u4e00\u6a21\u578b\u3002", "conclusion": "\u6982\u7387\u65b9\u6cd5\u4e3a\u7b26\u53f7\u56de\u5f52\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u4f18\u52bf\u3002"}}
{"id": "2507.19663", "categories": ["stat.ML", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.19663", "abs": "https://arxiv.org/abs/2507.19663", "authors": ["Leo Guo", "Adwait Inamdar", "Willem D. van Driel", "GuoQi Zhang"], "title": "Adaptive Bayesian Data-Driven Design of Reliable Solder Joints for Micro-electronic Devices", "comment": "data-driven design, adaptive hyperparameters, Bayesian optimization,\n  solder joint reliability, micro-electronics", "summary": "Solder joint reliability related to failures due to thermomechanical loading\nis a critically important yet physically complex engineering problem. As a\nresult, simulated behavior is oftentimes computationally expensive. In an\nincreasingly data-driven world, the usage of efficient data-driven design\nschemes is a popular choice. Among them, Bayesian optimization (BO) with\nGaussian process regression is one of the most important representatives. The\nauthors argue that computational savings can be obtained from exploiting\nthorough surrogate modeling and selecting a design candidate based on multiple\nacquisition functions. This is feasible due to the relatively low computational\ncost, compared to the expensive simulation objective. This paper addresses the\nshortcomings in the adjacent literature by providing and implementing a novel\nheuristic framework to perform BO with adaptive hyperparameters across the\nvarious optimization iterations. Adaptive BO is subsequently compared to\nregular BO when faced with synthetic objective minimization problems. The\nresults show the efficiency of adaptive BO when compared any worst-performing\nregular Bayesian schemes. As an engineering use case, the solder joint\nreliability problem is tackled by minimizing the accumulated non-linear creep\nstrain under a cyclic thermal load. Results show that adaptive BO outperforms\nregular BO by 3% on average at any given computational budget threshold,\ncritically saving half of the computational expense budget. This practical\nresult underlines the methodological potential of the adaptive Bayesian\ndata-driven methodology to achieve better results and cut optimization-related\nexpenses. Lastly, in order to promote the reproducibility of the results, the\ndata-driven implementations are made available on an open-source basis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u7684\u81ea\u9002\u5e94\u542f\u53d1\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u710a\u70b9\u53ef\u9760\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u91c7\u96c6\u51fd\u6570\u548c\u81ea\u9002\u5e94\u8d85\u53c2\u6570\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u7ed3\u679c\u663e\u793a\u6bd4\u4f20\u7edfBO\u8282\u770150%\u8ba1\u7b97\u6210\u672c\u4e14\u6027\u80fd\u63d0\u53473%\u3002", "motivation": "\u710a\u70b9\u53ef\u9760\u6027\u95ee\u9898\u8ba1\u7b97\u590d\u6742\u4e14\u6602\u8d35\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\uff0c\u9700\u6570\u636e\u9a71\u52a8\u7684\u9ad8\u6548\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94BO\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u548c\u591a\u91c7\u96c6\u51fd\u6570\uff0c\u52a8\u6001\u8c03\u6574\u8d85\u53c2\u6570\u3002", "result": "\u81ea\u9002\u5e94BO\u5728\u5408\u6210\u95ee\u9898\u548c\u5b9e\u9645\u710a\u70b9\u95ee\u9898\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edfBO\uff0c\u8282\u770150%\u8ba1\u7b97\u6210\u672c\u4e14\u6027\u80fd\u63d0\u53473%\u3002", "conclusion": "\u81ea\u9002\u5e94BO\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u7ed3\u679c\u53ef\u590d\u73b0\u3002"}}
{"id": "2507.19774", "categories": ["stat.ML", "cs.LG", "62M45, 62H30, 62P30"], "pdf": "https://arxiv.org/pdf/2507.19774", "abs": "https://arxiv.org/abs/2507.19774", "authors": ["Agnideep Aich", "Ashit Baran Aich", "Md Monzur Murshed", "Sameera Hewage", "Bruce Wade"], "title": "Bag of Coins: A Statistical Probe into Neural Confidence Structures", "comment": null, "summary": "Modern neural networks, despite their high accuracy, often produce poorly\ncalibrated confidence scores, limiting their reliability in high-stakes\napplications. Existing calibration methods typically post-process model outputs\nwithout interrogating the internal consistency of the predictions themselves.\nIn this work, we introduce a novel, non-parametric statistical probe, the\nBag-of-Coins (BoC) test, that examines the internal consistency of a\nclassifier's logits. The BoC test reframes confidence estimation as a\nfrequentist hypothesis test: does the model's top-ranked class win 1-v-1\ncontests against random competitors at a rate consistent with its own stated\nsoftmax probability? When applied to modern deep learning architectures, this\nsimple probe reveals a fundamental dichotomy. On Vision Transformers (ViTs),\nthe BoC output serves as a state-of-the-art confidence score, achieving\nnear-perfect calibration with an ECE of 0.0212, an 88% improvement over a\ntemperature-scaled baseline. Conversely, on Convolutional Neural Networks\n(CNNs) like ResNet, the probe reveals a deep inconsistency between the model's\npredictions and its internal logit structure, a property missed by traditional\nmetrics. We posit that BoC is not merely a calibration method, but a new\ndiagnostic tool for understanding and exposing the differing ways that popular\narchitectures represent uncertainty.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBag-of-Coins (BoC)\u7684\u975e\u53c2\u6570\u7edf\u8ba1\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5206\u7c7b\u5668\u5185\u90e8\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u53d1\u73b0ViTs\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u6548\u679c\u663e\u8457\u4f18\u4e8eCNNs\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u7cbe\u5ea6\u4e0e\u5176\u7f6e\u4fe1\u5ea6\u6821\u51c6\u4e0d\u8db3\u5f62\u6210\u77db\u76fe\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u6df1\u5165\u63a2\u7a76\u9884\u6d4b\u7684\u5185\u90e8\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faBoC\u6d4b\u8bd5\uff0c\u5c06\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u8f6c\u5316\u4e3a\u9891\u7387\u5047\u8bbe\u68c0\u9a8c\uff0c\u68c0\u67e5\u6a21\u578b\u9884\u6d4b\u4e0e\u5176softmax\u6982\u7387\u7684\u4e00\u81f4\u6027\u3002", "result": "ViTs\u7684BoC\u8f93\u51fa\u8fbe\u5230\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u6821\u51c6\uff08ECE\u4e3a0.0212\uff09\uff0c\u800cCNNs\u5219\u663e\u793a\u51fa\u9884\u6d4b\u4e0e\u5185\u90e8\u7ed3\u6784\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "BoC\u4e0d\u4ec5\u662f\u6821\u51c6\u5de5\u5177\uff0c\u8fd8\u80fd\u63ed\u793a\u4e0d\u540c\u67b6\u6784\u5728\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u4e0a\u7684\u5dee\u5f02\u3002"}}
{"id": "2507.19787", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19787", "abs": "https://arxiv.org/abs/2507.19787", "authors": ["Sara M. Ichinaga", "Steven L. Brunton", "Aleksandr Y. Aravkin", "J. Nathan Kutz"], "title": "Sparse-mode Dynamic Mode Decomposition for Disambiguating Local and Global Structures", "comment": null, "summary": "The dynamic mode decomposition (DMD) is a data-driven approach that extracts\nthe dominant features from spatiotemporal data. In this work, we introduce\nsparse-mode DMD, a new variant of the optimized DMD framework that specifically\nleverages sparsity-promoting regularization in order to approximate DMD modes\nwhich have localized spatial structure. The algorithm maintains the\nnoise-robust properties of optimized DMD while disambiguating between modes\nwhich are spatially local versus global in nature. In many applications, such\nmodes are associated with discrete and continuous spectra respectively, thus\nallowing the algorithm to explicitly construct, in an unsupervised manner, the\ndistinct portions of the spectrum. We demonstrate this by analyzing synthetic\nand real-world systems, including examples from optical waveguides, quantum\nmechanics, and sea surface temperature data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u6a21\u5f0f\u52a8\u6001\u6a21\u6001\u5206\u89e3\uff08sparse-mode DMD\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u6b63\u5219\u5316\u63d0\u53d6\u5c40\u90e8\u5316\u7a7a\u95f4\u7ed3\u6784\u7684\u6a21\u6001\uff0c\u540c\u65f6\u4fdd\u6301\u566a\u58f0\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfDMD\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u5c40\u90e8\u548c\u5168\u5c40\u6a21\u6001\uff0c\u7a00\u758f\u6a21\u5f0fDMD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u660e\u786e\u6784\u5efa\u9891\u8c31\u7684\u4e0d\u540c\u90e8\u5206\u3002", "method": "\u901a\u8fc7\u7a00\u758f\u6b63\u5219\u5316\u4f18\u5316DMD\u6846\u67b6\uff0c\u63d0\u53d6\u5c40\u90e8\u5316\u7a7a\u95f4\u7ed3\u6784\u7684\u6a21\u6001\uff0c\u540c\u65f6\u4fdd\u6301\u566a\u58f0\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u5b9e\u9645\u7cfb\u7edf\uff08\u5982\u5149\u6ce2\u5bfc\u3001\u91cf\u5b50\u529b\u5b66\u548c\u6d77\u9762\u6e29\u5ea6\u6570\u636e\uff09\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7a00\u758f\u6a21\u5f0fDMD\u80fd\u591f\u65e0\u76d1\u7763\u5730\u660e\u786e\u533a\u5206\u5c40\u90e8\u548c\u5168\u5c40\u6a21\u6001\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.19510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19510", "abs": "https://arxiv.org/abs/2507.19510", "authors": ["Haoxuan Ma", "Xishun Liao", "Yifan Liu", "Chris Stanford", "Jiaqi Ma"], "title": "Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers", "comment": null, "summary": "This paper addresses a critical gap in urban mobility modeling by focusing on\nshift workers, a population segment comprising 15-20% of the workforce in\nindustrialized societies yet systematically underrepresented in traditional\ntransportation surveys and planning. This underrepresentation is revealed in\nthis study by a comparative analysis of GPS and survey data, highlighting stark\ndifferences between the bimodal temporal patterns of shift workers and the\nconventional 9-to-5 schedules recorded in surveys. To address this bias, we\nintroduce a novel transformer-based approach that leverages fragmented GPS\ntrajectory data to generate complete, behaviorally valid activity patterns for\nindividuals working non-standard hours. Our method employs periodaware temporal\nembeddings and a transition-focused loss function specifically designed to\ncapture the unique activity rhythms of shift workers and mitigate the inherent\nbiases in conventional transportation datasets. Evaluation shows that the\ngenerated data achieves remarkable distributional alignment with GPS data from\nLos Angeles County (Average JSD < 0.02 for all evaluation metrics). By\ntransforming incomplete GPS traces into complete, representative activity\npatterns, our approach provides transportation planners with a powerful data\naugmentation tool to fill critical gaps in understanding the 24/7 mobility\nneeds of urban populations, enabling precise and inclusive transportation\nplanning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff0c\u5229\u7528GPS\u8f68\u8ff9\u6570\u636e\u751f\u6210\u8f6e\u73ed\u5de5\u4eba\u7684\u5b8c\u6574\u6d3b\u52a8\u6a21\u5f0f\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u4ea4\u901a\u8c03\u67e5\u4e2d\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u8f6e\u73ed\u5de5\u4eba\u5360\u5de5\u4e1a\u5316\u793e\u4f1a\u52b3\u52a8\u529b\u768415-20%\uff0c\u4f46\u5728\u4f20\u7edf\u4ea4\u901a\u8c03\u67e5\u548c\u89c4\u5212\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5bf9\u975e\u6807\u51c6\u5de5\u4f5c\u65f6\u95f4\u4eba\u7fa4\u7684\u79fb\u52a8\u9700\u6c42\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u5468\u671f\u6027\u65f6\u95f4\u5d4c\u5165\u548c\u8fc7\u6e21\u805a\u7126\u635f\u5931\u51fd\u6570\uff0c\u4ece\u788e\u7247\u5316GPS\u6570\u636e\u751f\u6210\u5b8c\u6574\u7684\u6d3b\u52a8\u6a21\u5f0f\u3002", "result": "\u751f\u6210\u7684\u6570\u636e\u4e0e\u6d1b\u6749\u77f6\u53bf\u7684GPS\u6570\u636e\u5206\u5e03\u9ad8\u5ea6\u4e00\u81f4\uff08\u5e73\u5747JSD < 0.02\uff09\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4ea4\u901a\u89c4\u5212\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6570\u636e\u589e\u5f3a\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u3001\u7cbe\u786e\u5730\u6ee1\u8db3\u57ce\u5e02\u4eba\u53e3\u768424/7\u79fb\u52a8\u9700\u6c42\u3002"}}
{"id": "2507.19844", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19844", "abs": "https://arxiv.org/abs/2507.19844", "authors": ["Biswarup Mukherjee", "Li Zhou", "S. Gokul Krishnan", "Milad Kabirifar", "Subhash Lakshminarayana", "Charalambos Konstantinou"], "title": "VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets", "comment": "2025 IEEE International Conference on Communications, Control, and\n  Computing Technologies for Smart Grids (SmartGridComm)", "summary": "This paper introduces a model for coordinating prosumers with heterogeneous\ndistributed energy resources (DERs), participating in the local energy market\n(LEM) that interacts with the market-clearing entity. The proposed LEM scheme\nutilizes a data-driven, model-free reinforcement learning approach based on the\nmulti-agent deep deterministic policy gradient (MADDPG) framework, enabling\nprosumers to make real-time decisions on whether to buy, sell, or refrain from\nany action while facilitating efficient coordination for optimal energy trading\nin a dynamic market. In addition, we investigate a price manipulation strategy\nusing a variational auto encoder-generative adversarial network (VAE-GAN)\nmodel, which allows utilities to adjust price signals in a way that induces\nfinancial losses for the prosumers. Our results show that under adversarial\npricing, heterogeneous prosumer groups, particularly those lacking generation\ncapabilities, incur financial losses. The same outcome holds across LEMs of\ndifferent sizes. As the market size increases, trading stabilizes and fairness\nimproves through emergent cooperation among agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08MADDPG\uff09\u7684\u672c\u5730\u80fd\u6e90\u5e02\u573a\uff08LEM\uff09\u534f\u8c03\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u4e86\u4ef7\u683c\u64cd\u7eb5\u7b56\u7565\u5bf9\u4ea7\u6d88\u8005\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\uff08DERs\uff09\u5728\u52a8\u6001\u5e02\u573a\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u5e76\u7814\u7a76\u4ef7\u683c\u64cd\u7eb5\u5bf9\u4ea7\u6d88\u8005\u7684\u8d22\u52a1\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08MADDPG\uff09\u548cVAE-GAN\u6a21\u578b\u8fdb\u884c\u4ef7\u683c\u64cd\u7eb5\u5206\u6790\u3002", "result": "\u5728\u5bf9\u6297\u6027\u5b9a\u4ef7\u4e0b\uff0c\u7f3a\u4e4f\u53d1\u7535\u80fd\u529b\u7684\u4ea7\u6d88\u8005\u7fa4\u4f53\u906d\u53d7\u8d22\u52a1\u635f\u5931\uff1b\u5e02\u573a\u6269\u5927\u540e\uff0c\u4ea4\u6613\u7a33\u5b9a\u6027\u548c\u516c\u5e73\u6027\u63d0\u9ad8\u3002", "conclusion": "MADDPG\u6846\u67b6\u6709\u6548\u534f\u8c03\u4ea7\u6d88\u8005\uff0c\u4f46\u4ef7\u683c\u64cd\u7eb5\u7b56\u7565\u9700\u8b66\u60d5\uff1b\u5e02\u573a\u89c4\u6a21\u7684\u6269\u5927\u6709\u52a9\u4e8e\u5408\u4f5c\u4e0e\u516c\u5e73\u3002"}}
{"id": "2507.19546", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19546", "abs": "https://arxiv.org/abs/2507.19546", "authors": ["Yansong Du", "Yutong Deng", "Yuting Zhou", "Feiyu Jiao", "Bangyao Wang", "Zhancong Xu", "Zhaoxiang Jiang", "Xun Guan"], "title": "Multipath Interference Suppression in Indirect Time-of-Flight Imaging via a Novel Compressed Sensing Framework", "comment": "15 pages, 10 figures", "summary": "We propose a novel compressed sensing method to improve the depth\nreconstruction accuracy and multi-target separation capability of indirect\nTime-of-Flight (iToF) systems. Unlike traditional approaches that rely on\nhardware modifications, complex modulation, or cumbersome data-driven\nreconstruction, our method operates with a single modulation frequency and\nconstructs the sensing matrix using multiple phase shifts and narrow-duty-cycle\ncontinuous waves. During matrix construction, we further account for pixel-wise\nrange variation caused by lens distortion, making the sensing matrix better\naligned with actual modulation response characteristics. To enhance sparse\nrecovery, we apply K-Means clustering to the distance response dictionary and\nconstrain atom selection within each cluster during the OMP process, which\neffectively reduces the search space and improves solution stability.\nExperimental results demonstrate that the proposed method outperforms\ntraditional approaches in both reconstruction accuracy and robustness, without\nrequiring any additional hardware changes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u538b\u7f29\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u8c03\u5236\u9891\u7387\u548c\u591a\u76f8\u4f4d\u504f\u79fb\u6539\u8fdbiToF\u7cfb\u7edf\u7684\u6df1\u5ea6\u91cd\u5efa\u7cbe\u5ea6\u548c\u591a\u76ee\u6807\u5206\u79bb\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u786c\u4ef6\u4fee\u6539\u6216\u590d\u6742\u8c03\u5236\uff0c\u800c\u65b0\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u786c\u4ef6\u5373\u53ef\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5229\u7528\u591a\u76f8\u4f4d\u504f\u79fb\u548c\u7a84\u5360\u7a7a\u6bd4\u8fde\u7eed\u6ce2\u6784\u5efa\u611f\u77e5\u77e9\u9635\uff0c\u5e76\u901a\u8fc7K-Means\u805a\u7c7b\u4f18\u5316\u7a00\u758f\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u65e0\u9700\u786c\u4ef6\u6539\u52a8\u5373\u53ef\u663e\u8457\u63d0\u5347iToF\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2507.20058", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.20058", "abs": "https://arxiv.org/abs/2507.20058", "authors": ["Ran Tong", "Lanruo Wang", "Tong Wang", "Wei Yan"], "title": "Predicting Parkinson's Disease Progression Using Statistical and Neural Mixed Effects Models: A Comparative Study on Longitudinal Biomarkers", "comment": "20pages,3 figures,currently under review", "summary": "Predicting Parkinson's Disease (PD) progression is crucial, and voice\nbiomarkers offer a non-invasive method for tracking symptom severity (UPDRS\nscores) through telemonitoring. Analyzing this longitudinal data is challenging\ndue to within-subject correlations and complex, nonlinear patient-specific\nprogression patterns. This study benchmarks LMMs against two advanced hybrid\napproaches: the Generalized Neural Network Mixed Model (GNMM) (Mandel 2021),\nwhich embeds a neural network within a GLMM structure, and the Neural Mixed\nEffects (NME) model (Wortwein 2023), allowing nonlinear subject-specific\nparameters throughout the network. Using the Oxford Parkinson's telemonitoring\nvoice dataset, we evaluate these models' performance in predicting Total UPDRS\nto offer practical guidance for PD research and clinical applications.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u7ebf\u6027\u6df7\u5408\u6a21\u578b\uff08LMM\uff09\u4e0e\u4e24\u79cd\u5148\u8fdb\u6df7\u5408\u65b9\u6cd5\uff08GNMM\u548cNME\uff09\u5728\u9884\u6d4b\u5e15\u91d1\u68ee\u75c5\u8fdb\u5c55\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u7684\u8fdb\u5c55\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u58f0\u97f3\u751f\u7269\u6807\u5fd7\u7269\u63d0\u4f9b\u4e86\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\uff0c\u4f46\u7eb5\u5411\u6570\u636e\u5206\u6790\u5b58\u5728\u6311\u6218\u3002", "method": "\u4f7f\u7528\u725b\u6d25\u5e15\u91d1\u68ee\u75c5\u8fdc\u7a0b\u76d1\u6d4b\u58f0\u97f3\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LMM\u3001GNMM\u548cNME\u6a21\u578b\u5728\u9884\u6d4bUPDRS\u603b\u5206\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u4e3a\u5e15\u91d1\u68ee\u75c5\u7814\u7a76\u548c\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "GNMM\u548cNME\u6a21\u578b\u5728\u9884\u6d4b\u5e15\u91d1\u68ee\u75c5\u8fdb\u5c55\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2507.19513", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19513", "abs": "https://arxiv.org/abs/2507.19513", "authors": ["Khalid Ali", "Zineddine Bettouche", "Andreas Kassler", "Andreas Fischer"], "title": "Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting", "comment": null, "summary": "Accurate spatiotemporal traffic forecasting is vital for intelligent resource\nmanagement in 5G and beyond. However, conventional AI approaches often fail to\ncapture the intricate spatial and temporal patterns that exist, due to e.g.,\nthe mobility of users. We introduce a lightweight, dual-path Spatiotemporal\nNetwork that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling\nand a three-layer Conv3D module for spatial feature extraction. A fusion layer\nintegrates both streams into a cohesive representation, enabling robust\nforecasting. Our design improves gradient stability and convergence speed while\nreducing prediction error. Evaluations on real-world datasets show superior\nforecast performance over ConvLSTM baselines and strong generalization to\nunseen regions, making it well-suited for large-scale, next-generation network\ndeployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,\nwith a 30% improvement in model generalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u53cc\u8def\u5f84\u65f6\u7a7a\u7f51\u7edc\uff0c\u7ed3\u5408sLSTM\u548cConv3D\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "5G\u53ca\u672a\u6765\u7f51\u7edc\u4e2d\uff0c\u51c6\u786e\u7684\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\u5bf9\u667a\u80fd\u8d44\u6e90\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edfAI\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u65f6\u7a7a\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u8bbe\u8ba1\uff0csLSTM\u7528\u4e8e\u65f6\u95f4\u5efa\u6a21\uff0cConv3D\u6a21\u5757\u7528\u4e8e\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u5c42\u6574\u5408\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eConvLSTM\u57fa\u7ebf\uff0cMAE\u964d\u4f4e23%\uff0c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u63d0\u534730%\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u9002\u5408\u5927\u89c4\u6a21\u65b0\u4e00\u4ee3\u7f51\u7edc\u90e8\u7f72\uff0c\u5177\u6709\u68af\u5ea6\u7a33\u5b9a\u6027\u548c\u5feb\u901f\u6536\u655b\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.20587", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.20587", "abs": "https://arxiv.org/abs/2507.20587", "authors": ["Zhongyao Luo", "Hao Wu", "Zhao Ge", "Ming Tang"], "title": "Real-Time Distributed Optical Fiber Vibration Recognition via Extreme Lightweight Model and Cross-Domain Distillation", "comment": "12 pages, 8 figures", "summary": "Distributed optical fiber vibration sensing (DVS) systems offer a promising\nsolution for large-scale monitoring and intrusion event recognition. However,\ntheir practical deployment remains hindered by two major challenges:\ndegradation of recognition accuracy in dynamic conditions, and the\ncomputational bottleneck of real-time processing for mass sensing data. This\npaper presents a new solution to these challenges, through a FPGA-accelerated\nextreme lightweight model along with a newly proposed knowledge distillation\nframework. The proposed three-layer depthwise separable convolution network\ncontains only 4141 parameters, which is the most compact architecture in this\nfield to date, and achieves a maximum processing speed of 0.019 ms for each\nsample covering a 12.5 m fiber length over 0.256 s. This performance\ncorresponds to real-time processing capabilities for sensing fibers extending\nup to 168.68 km. To improve generalizability under changing environments, the\nproposed cross-domain distillation framework guided by physical priors is used\nhere to embed frequency-domain insights into the time-domain model. This allows\nfor time-frequency representation learning without increasing complexity and\nboosts recognition accuracy from 51.93% to 95.72% under unseen environmental\nconditions. The proposed methodology provides key advancements including a\nframework combining interpretable signal processing technique with deep\nlearning and a reference architecture for real-time processing and\nedge-computing in DVS systems, and more general distributed optical fiber\nsensing (DOFS) area. It mitigates the trade-off between sensing range and\nreal-time capability, bridging the gap between theoretical capabilities and\npractical deployment requirements. Furthermore, this work reveals a new\ndirection for building more efficient, robust and explainable artificial\nintelligence systems for DOFS technologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u52a0\u901f\u7684\u8d85\u8f7b\u91cf\u6a21\u578b\u548c\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u7684\u5206\u5e03\u5f0f\u5149\u7ea4\u632f\u52a8\u4f20\u611f\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u6761\u4ef6\u4e0b\u8bc6\u522b\u7cbe\u5ea6\u4e0b\u964d\u548c\u5b9e\u65f6\u5904\u7406\u5927\u6570\u636e\u91cf\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u5149\u7ea4\u632f\u52a8\u4f20\u611f\u7cfb\u7edf\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u8bc6\u522b\u7cbe\u5ea6\u4e0b\u964d\u548c\u5b9e\u65f6\u5904\u7406\u5927\u6570\u636e\u91cf\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7f51\u7edc\uff08\u4ec54141\u53c2\u6570\uff09\u548c\u57fa\u4e8e\u7269\u7406\u5148\u9a8c\u7684\u8de8\u57df\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u3002", "result": "\u5b9e\u73b0\u6bcf\u6837\u672c0.019 ms\u7684\u5904\u7406\u901f\u5ea6\uff0c\u652f\u6301168.68 km\u5149\u7ea4\u7684\u5b9e\u65f6\u5904\u7406\uff1b\u8bc6\u522b\u7cbe\u5ea6\u4ece51.93%\u63d0\u5347\u81f395.72%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5206\u5e03\u5f0f\u5149\u7ea4\u4f20\u611f\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u65f6\u5904\u7406\u548c\u8fb9\u7f18\u8ba1\u7b97\u7684\u53c2\u8003\u67b6\u6784\uff0c\u5e76\u63ed\u793a\u4e86\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u9c81\u68d2\u548c\u53ef\u89e3\u91caAI\u7cfb\u7edf\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.19763", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19763", "abs": "https://arxiv.org/abs/2507.19763", "authors": ["Zhuoyin Dai", "Jingran Xu", "Xiaoli Xu", "Ruoguang Li", "Yong Zeng", "Jiangbin Lyu"], "title": "Coverage Probability and Average Rate Analysis of Hybrid Cellular and Cell-free Network", "comment": null, "summary": "Cell-free wireless networks deploy distributed access points (APs) to\nsimultaneously serve user equipments (UEs) across the service region and are\nregarded as one of the most promising network architectural paradigms. Despite\nrecent advances in the performance analysis and optimization of cellfree\nwireless networks, it remains an open question whether large-scale deployment\nof APs in existing wireless networks can cost-effectively achieve communication\ncapacity growth. Besides, the realization of a cell-free network is considered\nto be a gradual long-term evolutionary process in which cell-free APs will be\nincrementally introduced into existing cellular networks, and form a hybrid\ncommunication network with the existing cellular base stations (BSs). Such a\ncollaboration will bridge the gap between the established cellular network and\nthe innovative cellfree network. Therefore, hybrid cellular and cell-free\nnetworks (HCCNs) emerge as a practical and feasible solution for advancing\ncell-free network development, and it is worthwhile to further explore its\nperformance limits. This paper presents a stochastic geometry-based hybrid\ncellular and cell-free network model to analyze the distributions of signal and\ninterference and reveal their mutual coupling. Specifically, in order to\nbenefit the UEs from both the cellular BSs and the cell-free APs, a conjugate\nbeamforming design is employed, and the aggregated signal is analyzed using\nmoment matching. Then, the coverage probability of the hybrid network is\ncharacterized by deriving the Laplace transforms and their higher-order\nderivatives of interference components. Furthermore, the average achievable\nrate of the hybrid network over channel fading is derived based on the\ninterference coupling analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u51e0\u4f55\u7684\u6df7\u5408\u8702\u7a9d\u548c\u65e0\u5c0f\u533a\u7f51\u7edc\u6a21\u578b\uff0c\u5206\u6790\u4e86\u4fe1\u53f7\u548c\u5e72\u6270\u7684\u5206\u5e03\u53ca\u5176\u76f8\u4e92\u8026\u5408\uff0c\u5e76\u63a8\u5bfc\u4e86\u8986\u76d6\u6982\u7387\u548c\u5e73\u5747\u53ef\u5b9e\u73b0\u901f\u7387\u3002", "motivation": "\u63a2\u8ba8\u5927\u89c4\u6a21\u90e8\u7f72\u65e0\u5c0f\u533a\u63a5\u5165\u70b9\uff08APs\uff09\u5728\u73b0\u6709\u65e0\u7ebf\u7f51\u7edc\u4e2d\u662f\u5426\u80fd\u7ecf\u6d4e\u9ad8\u6548\u5730\u5b9e\u73b0\u901a\u4fe1\u5bb9\u91cf\u589e\u957f\uff0c\u4ee5\u53ca\u6df7\u5408\u8702\u7a9d\u548c\u65e0\u5c0f\u533a\u7f51\u7edc\uff08HCCNs\uff09\u7684\u6027\u80fd\u6781\u9650\u3002", "method": "\u91c7\u7528\u5171\u8f6d\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u5229\u7528\u77e9\u5339\u914d\u5206\u6790\u805a\u5408\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u63a8\u5bfc\u5e72\u6270\u5206\u91cf\u7684\u62c9\u666e\u62c9\u65af\u53d8\u6362\u53ca\u5176\u9ad8\u9636\u5bfc\u6570\u6765\u8868\u5f81\u8986\u76d6\u6982\u7387\u3002", "result": "\u63ed\u793a\u4e86\u4fe1\u53f7\u548c\u5e72\u6270\u7684\u5206\u5e03\u53ca\u5176\u8026\u5408\u5173\u7cfb\uff0c\u5e76\u63a8\u5bfc\u4e86\u6df7\u5408\u7f51\u7edc\u5728\u4fe1\u9053\u8870\u843d\u4e0b\u7684\u5e73\u5747\u53ef\u5b9e\u73b0\u901f\u7387\u3002", "conclusion": "\u6df7\u5408\u8702\u7a9d\u548c\u65e0\u5c0f\u533a\u7f51\u7edc\u662f\u63a8\u52a8\u65e0\u5c0f\u533a\u7f51\u7edc\u53d1\u5c55\u7684\u5b9e\u7528\u53ef\u884c\u65b9\u6848\uff0c\u5176\u6027\u80fd\u5206\u6790\u4e3a\u672a\u6765\u7f51\u7edc\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.20560", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.20560", "abs": "https://arxiv.org/abs/2507.20560", "authors": ["Xintao Xia", "Linjun Zhang", "Zhanrui Cai"], "title": "Statistical Inference for Differentially Private Stochastic Gradient Descent", "comment": null, "summary": "Privacy preservation in machine learning, particularly through Differentially\nPrivate Stochastic Gradient Descent (DP-SGD), is critical for sensitive data\nanalysis. However, existing statistical inference methods for SGD predominantly\nfocus on cyclic subsampling, while DP-SGD requires randomized subsampling. This\npaper first bridges this gap by establishing the asymptotic properties of SGD\nunder the randomized rule and extending these results to DP-SGD. For the output\nof DP-SGD, we show that the asymptotic variance decomposes into statistical,\nsampling, and privacy-induced components. Two methods are proposed for\nconstructing valid confidence intervals: the plug-in method and the random\nscaling method. We also perform extensive numerical analysis, which shows that\nthe proposed confidence intervals achieve nominal coverage rates while\nmaintaining privacy.", "AI": {"tldr": "\u672c\u6587\u586b\u8865\u4e86\u968f\u673a\u5b50\u91c7\u6837\u4e0bSGD\u7684\u6e10\u8fd1\u6027\u8d28\u4e0eDP-SGD\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u6784\u5efa\u6709\u6548\u7f6e\u4fe1\u533a\u95f4\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7edf\u8ba1\u63a8\u65ad\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5faa\u73af\u5b50\u91c7\u6837\uff0c\u800cDP-SGD\u9700\u8981\u968f\u673a\u5b50\u91c7\u6837\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5efa\u7acb\u4e86\u968f\u673a\u89c4\u5219\u4e0bSGD\u7684\u6e10\u8fd1\u6027\u8d28\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230DP-SGD\uff0c\u63d0\u51fa\u4e86\u63d2\u4ef6\u6cd5\u548c\u968f\u673a\u7f29\u653e\u6cd5\u6784\u5efa\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "DP-SGD\u8f93\u51fa\u7684\u6e10\u8fd1\u65b9\u5dee\u5206\u89e3\u4e3a\u7edf\u8ba1\u3001\u91c7\u6837\u548c\u9690\u79c1\u8bf1\u5bfc\u4e09\u90e8\u5206\uff0c\u6570\u503c\u5206\u6790\u663e\u793a\u7f6e\u4fe1\u533a\u95f4\u8fbe\u5230\u540d\u4e49\u8986\u76d6\u7387\u5e76\u4fdd\u6301\u9690\u79c1\u3002", "conclusion": "\u672c\u6587\u4e3a\u968f\u673a\u5b50\u91c7\u6837\u4e0b\u7684DP-SGD\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u65b9\u6cd5\u5de5\u5177\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2507.19514", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19514", "abs": "https://arxiv.org/abs/2507.19514", "authors": ["Andrew Kiruluta"], "title": "Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks", "comment": null, "summary": "We introduce a fully spectral learning framework that eliminates traditional\nneural layers by operating entirely in the wavelet domain. The model applies\nlearnable nonlinear transformations, including soft-thresholding and gain-phase\nmodulation, directly to wavelet coefficients. It also includes a differentiable\nwavelet basis selection mechanism, enabling adaptive processing using families\nsuch as Haar, Daubechies, and Biorthogonal wavelets.\n  Implemented in PyTorch with full 3D support, the model maintains a spectral\npipeline without spatial convolutions or attention. On synthetic 3D denoising\nand natural language tasks from the GLUE benchmark, including SST-2 sentiment\nclassification, the model achieves 89.3 percent accuracy, close to a 4-layer\nTransformer baseline (90.1 percent), while using 72 percent fewer parameters\nand 58 percent less peak memory. Faster early convergence is observed due to\nspectral sparsity priors.\n  In contrast to the quadratic complexity of self-attention and large matrix\nmultiplications in Transformers, our approach uses linear-time wavelet\ntransforms and pointwise nonlinearities, significantly reducing inference cost.\nThis yields a compact, interpretable, and efficient alternative to neural\nmodels. Our results support the viability of principled spectral learning in\nboth vision and language tasks, offering new directions for model design\nwithout overparameterized architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u5c0f\u6ce2\u57df\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5c0f\u6ce2\u7cfb\u6570\u4e0a\u5e94\u7528\u53ef\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u66ff\u4ee3\u4f20\u7edf\u795e\u7ecf\u5c42\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u578b\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08\u5982Transformer\uff09\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u53c2\u6570\u8fc7\u591a\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5728\u5c0f\u6ce2\u57df\u4e2d\u76f4\u63a5\u5e94\u7528\u975e\u7ebf\u6027\u53d8\u6362\uff08\u5982\u8f6f\u9608\u503c\u548c\u589e\u76ca\u76f8\u4f4d\u8c03\u5236\uff09\uff0c\u5e76\u5f15\u5165\u53ef\u5fae\u5206\u7684\u5c0f\u6ce2\u57fa\u9009\u62e9\u673a\u5236\uff0c\u652f\u6301\u591a\u79cd\u5c0f\u6ce2\u65cf\uff08\u5982Haar\u3001Daubechies\uff09\u3002", "result": "\u57283D\u53bb\u566a\u548c\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\uff08\u5982GLUE\u57fa\u51c6\u4e2d\u7684SST-2\u60c5\u611f\u5206\u7c7b\uff09\u4e0a\uff0c\u6a21\u578b\u6027\u80fd\u63a5\u8fd14\u5c42Transformer\uff0889.3% vs 90.1%\uff09\uff0c\u4f46\u53c2\u6570\u51cf\u5c1172%\uff0c\u5cf0\u503c\u5185\u5b58\u964d\u4f4e58%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u8bbe\u8ba1\u65b9\u5411\uff0c\u907f\u514d\u4e86\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u67b6\u6784\u3002"}}
{"id": "2507.19785", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19785", "abs": "https://arxiv.org/abs/2507.19785", "authors": ["Gevindu Ganganath", "Pasindu Sankalpa", "Samal Punsara", "Demitha Pasindu", "Chamira U. S. Edussooriya", "Ranga Rodrigo", "Udaya S. K. P. Miriya Thanthrige"], "title": "Radar and Acoustic Sensor Fusion using a Transformer Encoder for Robust Drone Detection and Classification", "comment": "Submitted to IEEE Sensors Letters", "summary": "The use of drones in a wide range of applications is steadily increasing.\nHowever, this has also raised critical security concerns such as unauthorized\ndrone intrusions into restricted zones. Therefore, robust and accurate drone\ndetection and classification mechanisms are required despite significant\nchallenges due to small size of drones, low-altitude flight, and environmental\nnoise. In this letter, we propose a multi-modal approach combining radar and\nacoustic sensing for detecting and classifying drones. We employ radar due to\nits long-range capabilities, and robustness to different weather conditions. We\nutilize raw acoustic signals without converting them to other domains such as\nspectrograms or Mel-frequency cepstral coefficients. This enables us to use\nfewer number of parameters compared to the stateof-the-art approaches.\nFurthermore, we explore the effectiveness of the transformer encoder\narchitecture in fusing these sensors. Experimental results obtained in outdoor\nsettings verify the superior performance of the proposed approach compared to\nthe state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u96f7\u8fbe\u548c\u58f0\u5b66\u4f20\u611f\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u7684\u68c0\u6d4b\u548c\u5206\u7c7b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u65e0\u4eba\u673a\u5e7f\u6cdb\u5e94\u7528\u5e26\u6765\u5b89\u5168\u95ee\u9898\uff0c\u9700\u89e3\u51b3\u65e0\u4eba\u673a\u68c0\u6d4b\u548c\u5206\u7c7b\u7684\u6311\u6218\uff0c\u5982\u4f53\u79ef\u5c0f\u3001\u4f4e\u7a7a\u98de\u884c\u548c\u73af\u5883\u566a\u58f0\u3002", "method": "\u7ed3\u5408\u96f7\u8fbe\uff08\u8fdc\u8ddd\u79bb\u3001\u5168\u5929\u5019\uff09\u548c\u539f\u59cb\u58f0\u5b66\u4fe1\u53f7\uff08\u51cf\u5c11\u53c2\u6570\uff09\uff0c\u4f7f\u7528Transformer\u7f16\u7801\u5668\u878d\u5408\u4f20\u611f\u5668\u3002", "result": "\u6237\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2507.20941", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2507.20941", "abs": "https://arxiv.org/abs/2507.20941", "authors": ["Sacha Braun", "Eug\u00e8ne Berta", "Michael I. Jordan", "Francis Bach"], "title": "Multivariate Conformal Prediction via Conformalized Gaussian Scoring", "comment": null, "summary": "While achieving exact conditional coverage in conformal prediction is\nunattainable without making strong, untestable regularity assumptions, the\npromise of conformal prediction hinges on finding approximations to conditional\nguarantees that are realizable in practice. A promising direction for obtaining\nconditional dependence for conformal sets--in particular capturing\nheteroskedasticity--is through estimating the conditional density\n$\\mathbb{P}_{Y|X}$ and conformalizing its level sets. Previous work in this\nvein has focused on nonconformity scores based on the empirical cumulative\ndistribution function (CDF). Such scores are, however, computationally costly,\ntypically requiring expensive sampling methods. To avoid the need for sampling,\nwe observe that the CDF-based score reduces to a Mahalanobis distance in the\ncase of Gaussian scores, yielding a closed-form expression that can be directly\nconformalized. Moreover, the use of a Gaussian-based score opens the door to a\nnumber of extensions of the basic conformal method; in particular, we show how\nto construct conformal sets with missing output values, refine conformal sets\nas partial information about $Y$ becomes available, and construct conformal\nsets on transformations of the output space. Finally, empirical results\nindicate that our approach produces conformal sets that more closely\napproximate conditional coverage in multivariate settings compared to\nalternative methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u5206\u6570\u7684\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edfCDF\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u6269\u5c55\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u7684\u5171\u5f62\u96c6\u6784\u5efa\u3002", "motivation": "\u5171\u5f62\u9884\u6d4b\u4e2d\u7cbe\u786e\u7684\u6761\u4ef6\u8986\u76d6\u96be\u4ee5\u5b9e\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u9ad8\u65af\u5206\u6570\u7684\u9a6c\u6c0f\u8ddd\u79bb\u66ff\u4ee3CDF\u65b9\u6cd5\uff0c\u5b9e\u73b0\u95ed\u5f0f\u8868\u8fbe\uff0c\u5e76\u6269\u5c55\u4e86\u5171\u5f62\u9884\u6d4b\u7684\u5e94\u7528\u573a\u666f\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u53d8\u91cf\u8bbe\u7f6e\u4e0b\u66f4\u63a5\u8fd1\u6761\u4ef6\u8986\u76d6\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u9ad8\u65af\u5206\u6570\u65b9\u6cd5\u4e3a\u5171\u5f62\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19515", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.19515", "abs": "https://arxiv.org/abs/2507.19515", "authors": ["Edmund F. Agyemang", "Hansapani Rodrigo", "Vincent Agbenyeavu"], "title": "A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting", "comment": null, "summary": "Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year,\nthough this estimate is an improvement from years past due to improvements in\nsanitation, healthcare practices, and vaccination programs. In this study, we\nperform a comparative analysis of traditional and deep learning models to\npredict Influenza A outbreaks. Using historical data from January 2009 to\nDecember 2023, we compared the performance of traditional ARIMA and Exponential\nSmoothing(ETS) models with six distinct deep learning architectures: Simple\nRNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear\nsuperiority of all the deep learning models, especially the state-of-the-art\nTransformer with respective average testing MSE and MAE of 0.0433 \\pm 0.0020\nand 0.1126 \\pm 0.0016 for capturing the temporal complexities associated with\nInfluenza A data, outperforming well known traditional baseline ARIMA and ETS\nmodels. These findings of this study provide evidence that state-of-the-art\ndeep learning architectures can enhance predictive modeling for infectious\ndiseases and indicate a more general trend toward using deep learning methods\nto enhance public health forecasting and intervention planning strategies.\nFuture work should focus on how these models can be incorporated into real-time\nforecasting and preparedness systems at an epidemic level, and integrated into\nexisting surveillance systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u6a21\u578b\uff08ARIMA\u548cETS\uff09\u4e0e\u516d\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08Simple RNN\u3001LSTM\u3001GRU\u3001BiLSTM\u3001BiGRU\u548cTransformer\uff09\u5728\u9884\u6d4b\u7532\u578b\u6d41\u611f\u7206\u53d1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662fTransformer\uff09\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u7532\u578b\u6d41\u611f\u6bcf\u5e74\u5bfc\u81f4\u5927\u91cf\u6b7b\u4ea1\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u9884\u6d4b\u6a21\u578b\u6765\u63d0\u5347\u516c\u5171\u536b\u751f\u5e72\u9884\u80fd\u529b\u3002", "method": "\u4f7f\u75282009\u5e74\u81f32023\u5e74\u7684\u5386\u53f2\u6570\u636e\uff0c\u6bd4\u8f83\u4f20\u7edf\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5747\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0cTransformer\u8868\u73b0\u6700\u4f73\uff08\u6d4b\u8bd5MSE\u548cMAE\u5206\u522b\u4e3a0.0433\u548c0.1126\uff09\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u4f20\u67d3\u75c5\u9884\u6d4b\u80fd\u529b\uff0c\u672a\u6765\u5e94\u5c06\u5176\u6574\u5408\u5230\u5b9e\u65f6\u9884\u6d4b\u548c\u76d1\u6d4b\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2507.19812", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19812", "abs": "https://arxiv.org/abs/2507.19812", "authors": ["Dezhi Wang", "Chongwen Huang", "Xiaojun Yuan", "Sami Muhaidat", "Lei Liu", "Xiaoming Chen", "Zhaoyang Zhang", "Chau Yuen", "M\u00e9rouane Debbah"], "title": "Channel Estimation in Massive MIMO Systems with Orthogonal Delay-Doppler Division Multiplexing", "comment": null, "summary": "Orthogonal delay-Doppler division multiplexing~(ODDM) modulation has recently\nbeen regarded as a promising technology to provide reliable communications in\nhigh-mobility situations. Accurate and low-complexity channel estimation is one\nof the most critical challenges for massive multiple input multiple\noutput~(MIMO) ODDM systems, mainly due to the extremely large antenna arrays\nand high-mobility environments. To overcome these challenges, this paper\naddresses the issue of channel estimation in downlink massive MIMO-ODDM systems\nand proposes a low-complexity algorithm based on memory approximate message\npassing~(MAMP) to estimate the channel state information~(CSI). Specifically,\nwe first establish the effective channel model of the massive MIMO-ODDM\nsystems, where the magnitudes of the elements in the equivalent channel vector\nfollow a Bernoulli-Gaussian distribution. Further, as the number of antennas\ngrows, the elements in the equivalent coefficient matrix tend to become\ncompletely random. Leveraging these characteristics, we utilize the MAMP method\nto determine the gains, delays, and Doppler effects of the multi-path channel,\nwhile the channel angles are estimated through the discrete Fourier transform\nmethod. Finally, numerical results show that the proposed channel estimation\nalgorithm approaches the Bayesian optimal results when the number of antennas\ntends to infinity and improves the channel estimation accuracy by about 30%\ncompared with the existing algorithms in terms of the normalized mean square\nerror.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMAMP\u7684\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21MIMO-ODDM\u7cfb\u7edf\u4e2d\u7684\u4fe1\u9053\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u9ad8\u79fb\u52a8\u6027\u73af\u5883\u4e0b\u5927\u89c4\u6a21MIMO-ODDM\u7cfb\u7edf\u7684\u4fe1\u9053\u4f30\u8ba1\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5929\u7ebf\u9635\u5217\u5e9e\u5927\u548c\u79fb\u52a8\u6027\u9ad8\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u5efa\u7acb\u6709\u6548\u4fe1\u9053\u6a21\u578b\uff0c\u5229\u7528MAMP\u65b9\u6cd5\u4f30\u8ba1\u591a\u5f84\u4fe1\u9053\u7684\u589e\u76ca\u3001\u65f6\u5ef6\u548c\u591a\u666e\u52d2\u6548\u5e94\uff0c\u5e76\u901a\u8fc7\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u4f30\u8ba1\u4fe1\u9053\u89d2\u5ea6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u5929\u7ebf\u6570\u91cf\u8d8b\u4e8e\u65e0\u7a77\u65f6\u63a5\u8fd1\u8d1d\u53f6\u65af\u6700\u4f18\u7ed3\u679c\uff0c\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u6bd4\u73b0\u6709\u7b97\u6cd5\u63d0\u9ad8\u7ea630%\u3002", "conclusion": "\u63d0\u51fa\u7684MAMP\u7b97\u6cd5\u5728\u5927\u89c4\u6a21MIMO-ODDM\u7cfb\u7edf\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u79fb\u52a8\u6027\u73af\u5883\u3002"}}
{"id": "2507.20975", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20975", "abs": "https://arxiv.org/abs/2507.20975", "authors": ["Trevor Harris", "Yan Liu"], "title": "Locally Adaptive Conformal Inference for Operator Models", "comment": "9 pages, 2 figures, 2 tables", "summary": "Operator models are regression algorithms for functional data and have become\na key tool for emulating large-scale dynamical systems. Recent advances in deep\nneural operators have dramatically improved the accuracy and scalability of\noperator modeling, but lack an inherent notion of predictive uncertainty. We\nintroduce Local Spectral Conformal Inference (LSCI), a new framework for\nlocally adaptive, distribution-free uncertainty quantification for neural\noperator models. LSCI uses projection-based depth scoring and localized\nconformal inference to generate function-valued prediction sets with\nstatistical guarantees. We prove approximate finite-sample marginal coverage\nunder local exchangeability, and demonstrate significant gains in adaptivity\nand coverage across synthetic and real-world operator learning tasks.", "AI": {"tldr": "LSCI\u6846\u67b6\u4e3a\u795e\u7ecf\u7b97\u5b50\u6a21\u578b\u63d0\u4f9b\u5c40\u90e8\u81ea\u9002\u5e94\u3001\u65e0\u5206\u5e03\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u663e\u8457\u63d0\u5347\u9002\u5e94\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u795e\u7ecf\u7b97\u5b50\u6a21\u578b\u7f3a\u4e4f\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u56fa\u6709\u6982\u5ff5\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLSCI\u6846\u67b6\uff0c\u7ed3\u5408\u6295\u5f71\u6df1\u5ea6\u8bc4\u5206\u548c\u5c40\u90e8\u5171\u5f62\u63a8\u65ad\uff0c\u751f\u6210\u5177\u6709\u7edf\u8ba1\u4fdd\u8bc1\u7684\u51fd\u6570\u503c\u9884\u6d4b\u96c6\u3002", "result": "\u5728\u5408\u6210\u548c\u5b9e\u9645\u7b97\u5b50\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cLSCI\u663e\u8457\u63d0\u5347\u4e86\u9002\u5e94\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002", "conclusion": "LSCI\u4e3a\u795e\u7ecf\u7b97\u5b50\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u7edf\u8ba1\u4fdd\u8bc1\u3002"}}
{"id": "2507.19517", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19517", "abs": "https://arxiv.org/abs/2507.19517", "authors": ["Mohit Gupta", "Debjit Bhowmick", "Ben Beck"], "title": "BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation", "comment": "This paper has been accepted for publication in the Proceedings of\n  the $28^{th}$ IEEE International Conference on Intelligent Transportation\n  Systems (ITSC 2025). This is the author's version of the work", "summary": "Accurate link-level bicycle volume estimation is essential for informed urban\nand transport planning but it is challenged by extremely sparse count data in\nurban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task\nframework augmenting a Hybrid Graph Neural Network (GNN) with Variational\nAutoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing\nsparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks\n(GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model\nintricate spatial relationships in sparse networks while VAE generates\nsynthetic nodes and edges to enrich the graph structure and enhance the\nestimation performance. BikeVAE-GNN simultaneously performs - regression for\nbicycling volume estimation and classification for bicycling traffic level\ncategorization. We demonstrate the effectiveness of BikeVAE-GNN using\nOpenStreetMap data and publicly available bicycle count data within the City of\nMelbourne - where only 141 of 15,933 road segments have labeled counts\n(resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN\noutperforms machine learning and baseline GNN models, achieving a mean absolute\nerror (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99.\nAblation studies further validate the effective role of Hybrid-GNN and VAE\ncomponents. Our research advances bicycling volume estimation in sparse\nnetworks using novel and state-of-the-art approaches, providing insights for\nsustainable bicycling infrastructures.", "AI": {"tldr": "BikeVAE-GNN\u662f\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u53cc\u4efb\u52a1\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u7a00\u758f\u81ea\u884c\u8f66\u7f51\u7edc\u4e2d\u7684\u81ea\u884c\u8f66\u6d41\u91cf\u4f30\u8ba1\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u57ce\u5e02\u81ea\u884c\u8f66\u7f51\u7edc\u7684\u6570\u636e\u7a00\u758f\u6027\u4f7f\u5f97\u81ea\u884c\u8f66\u6d41\u91cf\u4f30\u8ba1\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u586b\u8865\u6570\u636e\u7a7a\u767d\u5e76\u63d0\u5347\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faBikeVAE-GNN\u6846\u67b6\uff0c\u7ed3\u5408Hybrid-GNN\uff08GCN\u3001GAT\u548cGraphSAGE\uff09\u548cVAE\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u8282\u70b9\u548c\u8fb9\u6765\u4e30\u5bcc\u56fe\u7ed3\u6784\uff0c\u540c\u65f6\u8fdb\u884c\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5728\u58a8\u5c14\u672c\u5e02\u7684\u6570\u636e\u96c6\u4e0a\uff0cBikeVAE-GNN\u7684MAE\u4e3a30.82\u8f86/\u5929\uff0c\u51c6\u786e\u7387\u548cF1-score\u5747\u8fbe\u523099%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "BikeVAE-GNN\u4e3a\u7a00\u758f\u7f51\u7edc\u4e2d\u7684\u81ea\u884c\u8f66\u6d41\u91cf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u53ef\u6301\u7eed\u81ea\u884c\u8f66\u57fa\u7840\u8bbe\u65bd\u7684\u89c4\u5212\u3002"}}
{"id": "2507.19837", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19837", "abs": "https://arxiv.org/abs/2507.19837", "authors": ["Jiacheng Wang", "Changyuan Zhao", "Zehui Xiong", "Tao Xiang", "Dusit Niyato", "Xianbin Wang", "Shiwen Mao", "Dong In Kim"], "title": "Feature Engineering for Wireless Communications and Networking: Concepts, Methodologies, and Applications", "comment": "7 pages, 5 figures", "summary": "AI-enabled wireless communications have attracted tremendous research\ninterest in recent years, particularly with the rise of novel paradigms such as\nlow-altitude integrated sensing and communication (ISAC) networks. Within these\nsystems, feature engineering plays a pivotal role by transforming raw wireless\ndata into structured representations suitable for AI models. Hence, this paper\noffers a comprehensive investigation of feature engineering techniques in\nAI-driven wireless communications. Specifically, we begin with a detailed\nanalysis of fundamental principles and methodologies of feature engineering.\nNext, we present its applications in wireless communication systems, with\nspecial emphasis on ISAC networks. Finally, we introduce a generative AI-based\nframework, which can reconstruct signal feature spectrum under malicious\nattacks in low-altitude ISAC networks. The case study shows that it can\neffectively reconstruct the signal spectrum, achieving an average structural\nsimilarity index improvement of 4%, thereby supporting downstream sensing and\ncommunication applications.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u7814\u7a76\u4e86AI\u9a71\u52a8\u7684\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u7279\u5f81\u5de5\u7a0b\u6280\u672f\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5176\u57fa\u672c\u539f\u7406\u3001\u65b9\u6cd5\u53ca\u5176\u5728\u4f4e\u7a7aISAC\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210AI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6076\u610f\u653b\u51fb\u4e0b\u91cd\u5efa\u4fe1\u53f7\u7279\u5f81\u8c31\u3002", "motivation": "\u968f\u7740AI\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7279\u5f81\u5de5\u7a0b\u6280\u672f\u6210\u4e3a\u5c06\u539f\u59cb\u6570\u636e\u8f6c\u5316\u4e3a\u9002\u5408AI\u6a21\u578b\u7684\u7ed3\u6784\u5316\u8868\u793a\u7684\u5173\u952e\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e00\u6280\u672f\u53ca\u5176\u5728\u4f4e\u7a7aISAC\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6587\u7ae0\u9996\u5148\u5206\u6790\u4e86\u7279\u5f81\u5de5\u7a0b\u7684\u57fa\u672c\u539f\u7406\u548c\u65b9\u6cd5\uff0c\u968f\u540e\u63a2\u8ba8\u4e86\u5176\u5728\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210AI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4fe1\u53f7\u7279\u5f81\u8c31\u7684\u91cd\u5efa\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u91cd\u5efa\u4fe1\u53f7\u8c31\uff0c\u5e73\u5747\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u63d0\u9ad8\u4e864%\uff0c\u652f\u6301\u4e0b\u6e38\u611f\u77e5\u548c\u901a\u4fe1\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u4e3aAI\u9a71\u52a8\u7684\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u7279\u5f81\u5de5\u7a0b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7814\u7a76\uff0c\u5e76\u901a\u8fc7\u751f\u6210AI\u6846\u67b6\u5c55\u793a\u4e86\u5176\u5728\u4f4e\u7a7aISAC\u7f51\u7edc\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.19539", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19539", "abs": "https://arxiv.org/abs/2507.19539", "authors": ["Khurram Javed", "Richard S. Sutton"], "title": "Swift-Sarsa: Fast and Robust Linear Control", "comment": "Presented at RLDM 2025", "summary": "Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD\nlearning -- SwiftTD -- that augments True Online TD($\\lambda$) with step-size\noptimization, a bound on the effective learning rate, and step-size decay. In\ntheir experiments SwiftTD outperformed True Online TD($\\lambda$) and\nTD($\\lambda$) on a variety of prediction tasks derived from Atari games, and\nits performance was robust to the choice of hyper-parameters. In this extended\nabstract we extend SwiftTD to work for control problems. We combine the key\nideas behind SwiftTD with True Online Sarsa($\\lambda$) to develop an on-policy\nreinforcement learning algorithm called $\\textit{Swift-Sarsa}$.\n  We propose a simple benchmark for linear on-policy control called the\n$\\textit{operant conditioning benchmark}$. The key challenge in the operant\nconditioning benchmark is that a very small subset of input signals are\nrelevant for decision making. The majority of the signals are noise sampled\nfrom a non-stationary distribution. To learn effectively, the agent must learn\nto differentiate between the relevant signals and the noisy signals, and\nminimize prediction errors by assigning credit to the weight parameters\nassociated with the relevant signals.\n  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to\nassign credit to the relevant signals without any prior knowledge of the\nstructure of the problem. It opens the door for solution methods that learn\nrepresentations by searching over hundreds of millions of features in parallel\nwithout performance degradation due to noisy or bad features.", "AI": {"tldr": "SwiftTD\u7b97\u6cd5\u901a\u8fc7\u4f18\u5316\u6b65\u957f\u3001\u9650\u5236\u6709\u6548\u5b66\u4e60\u7387\u548c\u6b65\u957f\u8870\u51cf\uff0c\u6539\u8fdb\u4e86True Online TD(\u03bb)\uff0c\u5e76\u5728Atari\u6e38\u620f\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u672c\u6587\u5c06\u5176\u6269\u5c55\u4e3a\u63a7\u5236\u95ee\u9898\u7684Swift-Sarsa\u7b97\u6cd5\uff0c\u5e76\u5728\u64cd\u4f5c\u6761\u4ef6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u63a7\u5236\u95ee\u9898\u4e2d\u4fe1\u53f7\u566a\u58f0\u5e72\u6270\u548c\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "method": "\u7ed3\u5408SwiftTD\u548cTrue Online Sarsa(\u03bb)\u5f00\u53d1Swift-Sarsa\u7b97\u6cd5\uff0c\u5e94\u7528\u4e8e\u64cd\u4f5c\u6761\u4ef6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b66\u4e60\u533a\u5206\u76f8\u5173\u4fe1\u53f7\u4e0e\u566a\u58f0\u3002", "result": "Swift-Sarsa\u5728\u65e0\u5148\u9a8c\u77e5\u8bc6\u4e0b\u6210\u529f\u5206\u914d\u4fe1\u7528\u7ed9\u76f8\u5173\u4fe1\u53f7\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5e76\u884c\u7279\u5f81\u641c\u7d22\u3002", "conclusion": "Swift-Sarsa\u4e3a\u5904\u7406\u566a\u58f0\u4fe1\u53f7\u548c\u5927\u89c4\u6a21\u7279\u5f81\u641c\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.19518", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19518", "abs": "https://arxiv.org/abs/2507.19518", "authors": ["Sangwoo Seo", "Jimin Seo", "Yoonho Lee", "Donghyeon Kim", "Hyejin Shin", "Banghyun Sung", "Chanyoung Park"], "title": "Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction", "comment": "ICCAD 2025", "summary": "Subgraph matching plays an important role in electronic design automation\n(EDA) and circuit verification. Traditional rule-based methods have limitations\nin generalizing to arbitrary target circuits. Furthermore, node-to-node\nmatching approaches tend to be computationally inefficient, particularly for\nlarge-scale circuits. Deep learning methods have emerged as a potential\nsolution to address these challenges, but existing models fail to efficiently\ncapture global subgraph embeddings or rely on inefficient matching matrices,\nwhich limits their effectiveness for large circuits. In this paper, we propose\nan efficient graph matching approach that utilizes Graph Neural Networks (GNNs)\nto predict regions of high probability for containing the target circuit.\nSpecifically, we construct various negative samples to enable GNNs to\naccurately learn the presence of target circuits and develop an approach to\ndirectly extracting subgraph embeddings from the entire circuit, which captures\nglobal subgraph information and addresses the inefficiency of applying GNNs to\nall candidate subgraphs. Extensive experiments demonstrate that our approach\nsignificantly outperforms existing methods in terms of time efficiency and\ntarget region prediction, offering a scalable and effective solution for\nsubgraph matching in large-scale circuits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u9ad8\u6548\u5b50\u56fe\u5339\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u548c\u7535\u8def\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\uff0c\u8282\u70b9\u5339\u914d\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u9ad8\u6548\u6355\u83b7\u5168\u5c40\u5b50\u56fe\u5d4c\u5165\u6216\u4f9d\u8d56\u4f4e\u6548\u5339\u914d\u77e9\u9635\u3002", "method": "\u5229\u7528GNN\u9884\u6d4b\u76ee\u6807\u7535\u8def\u7684\u9ad8\u6982\u7387\u533a\u57df\uff0c\u6784\u5efa\u8d1f\u6837\u672c\u4ee5\u51c6\u786e\u5b66\u4e60\u76ee\u6807\u7535\u8def\u7684\u5b58\u5728\uff0c\u5e76\u76f4\u63a5\u4ece\u6574\u4e2a\u7535\u8def\u4e2d\u63d0\u53d6\u5b50\u56fe\u5d4c\u5165\u4ee5\u6355\u83b7\u5168\u5c40\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u6548\u7387\u548c\u76ee\u6807\u533a\u57df\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u7535\u8def\u4e2d\u7684\u5b50\u56fe\u5339\u914d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19910", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19910", "abs": "https://arxiv.org/abs/2507.19910", "authors": ["Jun Wu", "Weijie Yuan", "Qingqing Cheng", "Haijia Jin"], "title": "Toward Dual-Functional LAWN: Control-Aware System Design for Aerodynamics-Aided UAV Formations", "comment": null, "summary": "Integrated sensing and communication (ISAC) has emerged as a pivotal\ntechnology for advancing low-altitude wireless networks (LAWNs), serving as a\ncritical enabler for next-generation communication systems. This paper\ninvestigates the system design for energy-saving unmanned aerial vehicle (UAV)\nformations in dual-functional LAWNs, where a ground base station (GBS)\nsimultaneously wirelessly controls multiple UAV formations and performs sensing\ntasks. To enhance flight endurance, we exploit the aerodynamic upwash effects\nand propose a distributed energy-saving formation framework based on the\nadapt-then-combine (ATC) diffusion least mean square (LMS) algorithm.\nSpecifically, each UAV updates the local position estimate by invoking the LMS\nalgorithm, followed by refining it through cooperative information exchange\nwith neighbors. This enables an optimized aerodynamic structure that minimizes\nthe formation's overall energy consumption. To ensure control stability and\nfairness, we formulate a maximum linear quadratic regulator (LQR) minimization\nproblem, which is subject to both the available power budget and the required\nsensing beam pattern gain. To address this non-convex problem, we develop a\ntwo-step approach by first deriving a closed-form expression of LQR as a\nfunction of arbitrary beamformers. Subsequently, an efficient iterative\nalgorithm that integrates successive convex approximation (SCA) and\nsemidefinite relaxation (SDR) techniques is proposed to obtain a sub-optimal\ndual-functional beamforming solution. Extensive simulation results confirm that\nthe 'V'-shaped formation is the most energy-efficient configuration and\ndemonstrate the superiority of our proposed design over benchmark schemes in\nimproving control performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eATC\u6269\u6563LMS\u7b97\u6cd5\u7684\u5206\u5e03\u5f0f\u8282\u80fd\u65e0\u4eba\u673a\u7f16\u961f\u6846\u67b6\uff0c\u7528\u4e8e\u53cc\u529f\u80fd\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff0c\u4f18\u5316\u4e86\u80fd\u91cf\u6d88\u8017\u5e76\u63d0\u5347\u4e86\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u65e0\u4eba\u673a\u7f16\u961f\u7684\u8282\u80fd\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u5347\u98de\u884c\u7eed\u822a\u80fd\u529b\u5e76\u540c\u65f6\u5b8c\u6210\u901a\u4fe1\u548c\u611f\u77e5\u4efb\u52a1\u3002", "method": "\u5229\u7528\u7a7a\u6c14\u52a8\u529b\u5b66\u4e0a\u5347\u6548\u5e94\uff0c\u63d0\u51fa\u5206\u5e03\u5f0f\u8282\u80fd\u7f16\u961f\u6846\u67b6\uff0c\u91c7\u7528ATC\u6269\u6563LMS\u7b97\u6cd5\u4f18\u5316\u4f4d\u7f6e\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u6700\u5927LQR\u6700\u5c0f\u5316\u95ee\u9898\u786e\u4fdd\u63a7\u5236\u7a33\u5b9a\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c'V'\u5f62\u7f16\u961f\u662f\u6700\u8282\u80fd\u7684\u914d\u7f6e\uff0c\u6240\u63d0\u8bbe\u8ba1\u5728\u63a7\u5236\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u4f18\u5316\u4e86\u65e0\u4eba\u673a\u7f16\u961f\u7684\u80fd\u91cf\u6d88\u8017\uff0c\u540c\u65f6\u6ee1\u8db3\u4e86\u901a\u4fe1\u548c\u611f\u77e5\u7684\u53cc\u529f\u80fd\u9700\u6c42\u3002"}}
{"id": "2507.19627", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19627", "abs": "https://arxiv.org/abs/2507.19627", "authors": ["Zhengqi Lin", "Andrzej Ruszczy\u0144ski"], "title": "Federated Calculation of the Free-Support Transportation Barycenter by Single-Loop Dual Decomposition", "comment": null, "summary": "We propose an efficient federated dual decomposition algorithm for\ncalculating the Wasserstein barycenter of several distributions, including\nchoosing the support of the solution. The algorithm does not access local data\nand uses only highly aggregated information. It also does not require repeated\nsolutions to mass transportation problems. Because of the absence of any\nmatrix-vector operations, the algorithm exhibits a very low complexity of each\niteration and significant scalability. We illustrate its virtues and compare it\nto the state-of-the-art methods on several examples of mixture models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u8054\u90a6\u5bf9\u5076\u5206\u89e3\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u591a\u4e2a\u5206\u5e03\u7684Wasserstein\u91cd\u5fc3\uff0c\u5e76\u9009\u62e9\u89e3\u7684\u652f\u6491\u96c6\u3002\u7b97\u6cd5\u4e0d\u8bbf\u95ee\u672c\u5730\u6570\u636e\uff0c\u4ec5\u4f7f\u7528\u9ad8\u5ea6\u805a\u5408\u4fe1\u606f\uff0c\u4e14\u65e0\u9700\u91cd\u590d\u89e3\u51b3\u8d28\u91cf\u4f20\u8f93\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97Wasserstein\u91cd\u5fc3\u65f6\u7684\u9ad8\u590d\u6742\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5bf9\u5076\u5206\u89e3\u7b97\u6cd5\uff0c\u907f\u514d\u77e9\u9635-\u5411\u91cf\u64cd\u4f5c\uff0c\u964d\u4f4e\u8fed\u4ee3\u590d\u6742\u5ea6\u3002", "result": "\u7b97\u6cd5\u5728\u591a\u4e2a\u6df7\u5408\u6a21\u578b\u793a\u4f8b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u8ba1\u7b97Wasserstein\u91cd\u5fc3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19519", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19519", "abs": "https://arxiv.org/abs/2507.19519", "authors": ["J. Poole", "P. Gardner", "A. J. Hughes", "N. Dervilis", "R. S. Mills", "T. A. Dardeno", "K. Worden"], "title": "Physics-informed transfer learning for SHM via feature selection", "comment": null, "summary": "Data used for training structural health monitoring (SHM) systems are\nexpensive and often impractical to obtain, particularly labelled data.\nPopulation-based SHM presents a potential solution to this issue by considering\nthe available data across a population of structures. However, differences\nbetween structures will mean the training and testing distributions will\ndiffer; thus, conventional machine learning methods cannot be expected to\ngeneralise between structures. To address this issue, transfer learning (TL),\ncan be used to leverage information across related domains. An important\nconsideration is that the lack of labels in the target domain limits data-based\nmetrics to quantifying the discrepancy between the marginal distributions.\nThus, a prerequisite for the application of typical unsupervised TL methods is\nto identify suitable source structures (domains), and a set of features, for\nwhich the conditional distributions are related to the target structure.\nGenerally, the selection of domains and features is reliant on domain\nexpertise; however, for complex mechanisms, such as the influence of damage on\nthe dynamic response of a structure, this task is not trivial. In this paper,\nknowledge of physics is leveraged to select more similar features, the modal\nassurance criterion (MAC) is used to quantify the correspondence between the\nmodes of healthy structures. The MAC is shown to have high correspondence with\na supervised metric that measures joint-distribution similarity, which is the\nprimary indicator of whether a classifier will generalise between domains. The\nMAC is proposed as a measure for selecting a set of features that behave\nconsistently across domains when subjected to damage, i.e. features with\ninvariance in the conditional distributions. This approach is demonstrated on\nnumerical and experimental case studies to verify its effectiveness in various\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u77e5\u8bc6\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u6001\u4fdd\u8bc1\u51c6\u5219\uff08MAC\uff09\u91cf\u5316\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff08SHM\uff09\u4e2d\u4e0d\u540c\u7ed3\u6784\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u5206\u5e03\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff08SHM\uff09\u4e2d\u8bad\u7ec3\u6570\u636e\u6602\u8d35\u4e14\u96be\u4ee5\u83b7\u53d6\uff0c\u5c24\u5176\u662f\u6807\u6ce8\u6570\u636e\u3002\u4e0d\u540c\u7ed3\u6784\u95f4\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u5229\u7528\u6a21\u6001\u4fdd\u8bc1\u51c6\u5219\uff08MAC\uff09\u9009\u62e9\u76f8\u4f3c\u7279\u5f81\uff0c\u91cf\u5316\u5065\u5eb7\u7ed3\u6784\u6a21\u6001\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u548c\u5b9e\u9a8c\u6848\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "MAC\u4e0e\u8861\u91cf\u8054\u5408\u5206\u5e03\u76f8\u4f3c\u6027\u7684\u76d1\u7763\u6307\u6807\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8868\u660e\u5176\u80fd\u6709\u6548\u9009\u62e9\u6761\u4ef6\u5206\u5e03\u4e0d\u53d8\u7684\u7279\u5f81\u3002", "conclusion": "MAC\u53ef\u4f5c\u4e3a\u9009\u62e9\u8de8\u57df\u4e00\u81f4\u6027\u7279\u5f81\u7684\u6709\u6548\u5ea6\u91cf\uff0c\u63d0\u5347SHM\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.19936", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19936", "abs": "https://arxiv.org/abs/2507.19936", "authors": ["Zhongnian Li", "Chao Zheng", "Jian Xiao", "Ji Wang", "Gongpu Wang", "Ming Zeng", "Octavia A. Dobre"], "title": "Deep Learning Based Joint Channel Estimation and Positioning for Sparse XL-MIMO OFDM Systems", "comment": "5 pages,8 figures", "summary": "This paper investigates joint channel estimation and positioning in\nnear-field sparse extra-large multiple-input multiple-output (XL-MIMO)\northogonal frequency division multiplexing (OFDM) systems. To achieve\ncooperative gains between channel estimation and positioning, we propose a deep\nlearning-based two-stage framework comprising positioning and channel\nestimation. In the positioning stage, the user's coordinates are predicted and\nutilized in the channel estimation stage, thereby enhancing the accuracy of\nchannel estimation. Within this framework, we propose a U-shaped Mamba\narchitecture for channel estimation and positioning, termed as CP-Mamba. This\nnetwork integrates the strengths of the Mamba model with the structural\nadvantages of U-shaped convolutional networks, enabling effective capture of\nlocal spatial features and long-range temporal dependencies of the channel.\nNumerical simulation results demonstrate that the proposed two-stage approach\nwith CP-Mamba architecture outperforms existing baseline methods. Moreover,\nsparse arrays (SA) exhibit significantly superior performance in both channel\nestimation and positioning accuracy compared to conventional compact arrays.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff08CP-Mamba\uff09\uff0c\u7528\u4e8e\u8fd1\u573a\u7a00\u758f\u8d85\u5927MIMO-OFDM\u7cfb\u7edf\u4e2d\u7684\u8054\u5408\u4fe1\u9053\u4f30\u8ba1\u4e0e\u5b9a\u4f4d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u8fd1\u573a\u7a00\u758f\u8d85\u5927MIMO-OFDM\u7cfb\u7edf\u4e2d\u4fe1\u9053\u4f30\u8ba1\u4e0e\u5b9a\u4f4d\u7684\u534f\u540c\u589e\u76ca\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u5b9a\u4f4d\u9636\u6bb5\u9884\u6d4b\u7528\u6237\u5750\u6807\uff0c\u4fe1\u9053\u4f30\u8ba1\u9636\u6bb5\u5229\u7528\u5750\u6807\u63d0\u5347\u51c6\u786e\u6027\uff1b\u91c7\u7528U\u5f62Mamba\u67b6\u6784\uff08CP-Mamba\uff09\u6355\u83b7\u4fe1\u9053\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\u548c\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0cCP-Mamba\u67b6\u6784\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1b\u7a00\u758f\u9635\u5217\u5728\u4fe1\u9053\u4f30\u8ba1\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u7d27\u51d1\u9635\u5217\u3002", "conclusion": "CP-Mamba\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8054\u5408\u4fe1\u9053\u4f30\u8ba1\u4e0e\u5b9a\u4f4d\u7684\u7cbe\u5ea6\uff0c\u7a00\u758f\u9635\u5217\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2507.19680", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19680", "abs": "https://arxiv.org/abs/2507.19680", "authors": ["Niclas Alexander G\u00f6ring", "Charles London", "Abdurrahman Hadi Erturk", "Chris Mingard", "Yoonsoo Nam", "Ard A. Louis"], "title": "Feature learning is decoupled from generalization in high capacity neural networks", "comment": null, "summary": "Neural networks outperform kernel methods, sometimes by orders of magnitude,\ne.g. on staircase functions. This advantage stems from the ability of neural\nnetworks to learn features, adapting their hidden representations to better\ncapture the data. We introduce a concept we call feature quality to measure\nthis performance improvement. We examine existing theories of feature learning\nand demonstrate empirically that they primarily assess the strength of feature\nlearning, rather than the quality of the learned features themselves.\nConsequently, current theories of feature learning do not provide a sufficient\nfoundation for developing theories of neural network generalization.", "AI": {"tldr": "\u795e\u7ecf\u7f51\u7edc\u5728\u7279\u5f81\u5b66\u4e60\u80fd\u529b\u4e0a\u4f18\u4e8e\u6838\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u4e3b\u8981\u5173\u6ce8\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\u800c\u975e\u7279\u5f81\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u4e3a\u4f55\u5728\u67d0\u4e9b\u4efb\u52a1\uff08\u5982\u9636\u68af\u51fd\u6570\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u6838\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u73b0\u6709\u7279\u5f81\u5b66\u4e60\u7406\u8bba\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u2018\u7279\u5f81\u8d28\u91cf\u2019\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u8bc4\u4f30\u73b0\u6709\u7406\u8bba\u3002", "result": "\u73b0\u6709\u7406\u8bba\u4e3b\u8981\u8861\u91cf\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\uff0c\u800c\u975e\u7279\u5f81\u8d28\u91cf\uff0c\u4e0d\u8db3\u4ee5\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u9700\u8981\u53d1\u5c55\u66f4\u5168\u9762\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u6cdb\u5316\u673a\u5236\u3002"}}
{"id": "2507.19520", "categories": ["cs.LG", "astro-ph.EP", "astro-ph.IM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19520", "abs": "https://arxiv.org/abs/2507.19520", "authors": ["Ethan Lo", "Dan C. Lo"], "title": "Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves", "comment": null, "summary": "With manual searching processes, the rate at which scientists and astronomers\ndiscover exoplanets is slow because of inefficiencies that require an extensive\ntime of laborious inspections. In fact, as of now there have been about only\n5,000 confirmed exoplanets since the late 1900s. Recently, machine learning\n(ML) has proven to be extremely valuable and efficient in various fields,\ncapable of processing massive amounts of data in addition to increasing its\naccuracy by learning. Though ML models for discovering exoplanets owned by\nlarge corporations (e.g. NASA) exist already, they largely depend on complex\nalgorithms and supercomputers. In an effort to reduce such complexities, in\nthis paper, we report the results and potential benefits of various, well-known\nML models in the discovery and validation of extrasolar planets. The ML models\nthat are examined in this study include logistic regression, k-nearest\nneighbors, and random forest. The dataset on which the models train and predict\nis acquired from NASA's Kepler space telescope. The initial results show\npromising scores for each model. However, potential biases and dataset\nimbalances necessitate the use of data augmentation techniques to further\nensure fairer predictions and improved generalization. This study concludes\nthat, in the context of searching for exoplanets, data augmentation techniques\nsignificantly improve the recall and precision, while the accuracy varies for\neach model.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u53d1\u73b0\u7cfb\u5916\u884c\u661f\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7b80\u5316\u590d\u6742\u7b97\u6cd5\uff0c\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u3001K\u8fd1\u90bb\u548c\u968f\u673a\u68ee\u6797\u7b49\u6a21\u578b\uff0c\u7ed3\u5408NASA\u7684\u5f00\u666e\u52d2\u671b\u8fdc\u955c\u6570\u636e\uff0c\u5c55\u793a\u4e86\u6570\u636e\u589e\u5f3a\u6280\u672f\u5bf9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u641c\u7d22\u7cfb\u5916\u884c\u661f\u6548\u7387\u4f4e\u4e0b\uff0c\u673a\u5668\u5b66\u4e60\u80fd\u9ad8\u6548\u5904\u7406\u5927\u91cf\u6570\u636e\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u590d\u6742\u7b97\u6cd5\u548c\u8d85\u7ea7\u8ba1\u7b97\u673a\uff0c\u672c\u7814\u7a76\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u903b\u8f91\u56de\u5f52\u3001K\u8fd1\u90bb\u548c\u968f\u673a\u68ee\u6797\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u57fa\u4e8eNASA\u5f00\u666e\u52d2\u671b\u8fdc\u955c\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u548c\u9884\u6d4b\uff0c\u5e76\u91c7\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u5404\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6570\u636e\u4e0d\u5e73\u8861\u548c\u6f5c\u5728\u504f\u5dee\u9700\u8981\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u4ee5\u63d0\u5347\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u3002", "conclusion": "\u6570\u636e\u589e\u5f3a\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u7684\u51c6\u786e\u7387\u8868\u73b0\u4e0d\u4e00\uff0c\u4e3a\u7cfb\u5916\u884c\u661f\u641c\u7d22\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19984", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19984", "abs": "https://arxiv.org/abs/2507.19984", "authors": ["Irfan Muhammad", "Priyadarshi Mukherjee", "Wee Kiat New", "Hirley Alves", "Ioannis Krikidis", "Kai-Kit Wong"], "title": "Dependability Theory-based Statistical QoS Provisioning of Fluid Antenna Systems", "comment": null, "summary": "Fluid antenna systems (FAS) have recently emerged as a promising technology\nfor next-generation wireless networks, offering real-time spatial\nreconfiguration to enhance reliability, throughput, and energy efficiency.\nNevertheless, existing studies often overlook the temporal dynamics of channel\nfading and their implications for mission-critical operations. In this paper,\nwe propose a dependability-theoretic framework for statistical\nquality-of-service (QoS) provisioning of FAS under finite blocklength (FBL)\nconstraints. Specifically, we derive new closed-form expressions for the\nlevel-crossing rate (LCR) and average fade duration (AFD) of an $N$-port FAS\nover Nakagami-$m$ fading channels. Leveraging these second-order statistics, we\ndefine two key dependability metrics such as mission reliability and mean\ntime-to-first-failure (MTTFF), to quantify the probability of uninterrupted\noperation over a defined mission duration. We further extend the classical\neffective capacity (EC) concept to incorporate mission reliability in the FBL\nregime, yielding a mission EC (mEC). To capture energy efficiency under bursty\ntraffic and latency constraints, we also develop the mission effective energy\nefficiency (mEEE) metric and formulate its maximization as a non-convex\nfractional optimization problem. This problem is then solved via a modified\nDinkelbach's method with an embedded line search. Extensive simulations uncover\ncritical trade-offs among port count, QoS exponent, signal-to-noise ratio, and\nmission duration, offering insights for the design of ultra-reliable,\nlow-latency, and energy-efficient industrial internet-of-things (IIoT) systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u9760\u6027\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6709\u9650\u5757\u957f\u5ea6\uff08FBL\uff09\u7ea6\u675f\u4e0b\u4e3a\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u63d0\u4f9b\u7edf\u8ba1\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u4fdd\u969c\u3002\u901a\u8fc7\u63a8\u5bfc\u65b0\u7684\u95ed\u5408\u8868\u8fbe\u5f0f\u548c\u5b9a\u4e49\u5173\u952e\u53ef\u9760\u6027\u6307\u6807\uff0c\u7814\u7a76\u4e86FAS\u5728\u4efb\u52a1\u5173\u952e\u64cd\u4f5c\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5e38\u5ffd\u7565\u4fe1\u9053\u8870\u843d\u7684\u65f6\u53d8\u7279\u6027\u53ca\u5176\u5bf9\u4efb\u52a1\u5173\u952e\u64cd\u4f5c\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u91cf\u5316FAS\u7684\u53ef\u9760\u6027\u3002", "method": "\u63a8\u5bfc\u4e86N\u7aef\u53e3FAS\u5728Nakagami-m\u8870\u843d\u4fe1\u9053\u4e0b\u7684\u95ed\u5408\u8868\u8fbe\u5f0f\uff0c\u5b9a\u4e49\u4e86\u4efb\u52a1\u53ef\u9760\u6027\u548c\u9996\u6b21\u6545\u969c\u5e73\u5747\u65f6\u95f4\uff08MTTFF\uff09\u7b49\u6307\u6807\uff0c\u5e76\u6269\u5c55\u4e86\u7ecf\u5178\u6709\u6548\u5bb9\u91cf\uff08EC\uff09\u6982\u5ff5\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u63ed\u793a\u4e86\u7aef\u53e3\u6570\u91cf\u3001QoS\u6307\u6570\u3001\u4fe1\u566a\u6bd4\u548c\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u4e3a\u8d85\u53ef\u9760\u3001\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u80fd\u6548\u7684\u5de5\u4e1a\u7269\u8054\u7f51\uff08IIoT\uff09\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6307\u6807\u4e3aFAS\u5728\u4efb\u52a1\u5173\u952e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.19873", "categories": ["cs.LG", "cs.CY", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19873", "abs": "https://arxiv.org/abs/2507.19873", "authors": ["Bj\u00f6rn Kischelewski", "Benjamin Guedj", "David Wahl"], "title": "RestoreAI -- Pattern-based Risk Estimation Of Remaining Explosives", "comment": null, "summary": "Landmine removal is a slow, resource-intensive process affecting over 60\ncountries. While AI has been proposed to enhance explosive ordnance (EO)\ndetection, existing methods primarily focus on object recognition, with limited\nattention to prediction of landmine risk based on spatial pattern information.\nThis work aims to answer the following research question: How can AI be used to\npredict landmine risk from landmine patterns to improve clearance time\nefficiency? To that effect, we introduce RestoreAI, an AI system for\npattern-based risk estimation of remaining explosives. RestoreAI is the first\nAI system that leverages landmine patterns for risk prediction, improving the\naccuracy of estimating the residual risk of missing EO prior to land release.\nWe particularly focus on the implementation of three instances of RestoreAI,\nrespectively, linear, curved and Bayesian pattern deminers. First, the linear\npattern deminer uses linear landmine patterns from a principal component\nanalysis (PCA) for the landmine risk prediction. Second, the curved pattern\ndeminer uses curved landmine patterns from principal curves. Finally, the\nBayesian pattern deminer incorporates prior expert knowledge by using a\nBayesian pattern risk prediction. Evaluated on real-world landmine data,\nRestoreAI significantly boosts clearance efficiency. The top-performing\npattern-based deminers achieved a 14.37 percentage point increase in the\naverage share of cleared landmines per timestep and required 24.45% less time\nthan the best baseline deminer to locate all landmines. Interestingly, linear\nand curved pattern deminers showed no significant performance difference,\nsuggesting that more efficient linear patterns are a viable option for risk\nprediction.", "AI": {"tldr": "RestoreAI\u5229\u7528\u5730\u96f7\u7a7a\u95f4\u6a21\u5f0f\u9884\u6d4b\u98ce\u9669\uff0c\u63d0\u5347\u6392\u96f7\u6548\u7387\uff0c\u7ebf\u6027\u4e0e\u66f2\u7ebf\u6a21\u5f0f\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709AI\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5730\u96f7\u8bc6\u522b\uff0c\u800c\u5ffd\u7565\u57fa\u4e8e\u7a7a\u95f4\u6a21\u5f0f\u7684\u98ce\u9669\u9884\u6d4b\uff0c\u5f71\u54cd\u6392\u96f7\u6548\u7387\u3002", "method": "\u5f00\u53d1RestoreAI\u7cfb\u7edf\uff0c\u5305\u62ec\u7ebf\u6027\u3001\u66f2\u7ebf\u548c\u8d1d\u53f6\u65af\u6a21\u5f0f\u6392\u96f7\u5668\uff0c\u5206\u522b\u5229\u7528PCA\u3001\u4e3b\u66f2\u7ebf\u548c\u4e13\u5bb6\u77e5\u8bc6\u8fdb\u884c\u98ce\u9669\u9884\u6d4b\u3002", "result": "RestoreAI\u663e\u8457\u63d0\u5347\u6392\u96f7\u6548\u7387\uff0c\u6e05\u9664\u7387\u63d0\u9ad814.37%\uff0c\u65f6\u95f4\u51cf\u5c1124.45%\uff0c\u7ebf\u6027\u4e0e\u66f2\u7ebf\u6a21\u5f0f\u6027\u80fd\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "RestoreAI\u901a\u8fc7\u6a21\u5f0f\u9884\u6d4b\u98ce\u9669\u6709\u6548\u63d0\u5347\u6392\u96f7\u6548\u7387\uff0c\u7ebf\u6027\u6a21\u5f0f\u53ef\u4f5c\u4e3a\u9ad8\u6548\u9009\u62e9\u3002"}}
{"id": "2507.19522", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19522", "abs": "https://arxiv.org/abs/2507.19522", "authors": ["Aarush Gupta", "Kendric Hsu", "Syna Mathod"], "title": "Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations", "comment": null, "summary": "Mathematical models in neural networks are powerful tools for solving complex\ndifferential equations and optimizing their parameters; that is, solving the\nforward and inverse problems, respectively. A forward problem predicts the\noutput of a network for a given input by optimizing weights and biases. An\ninverse problem finds equation parameters or coefficients that effectively\nmodel the data. A Physics-Informed Neural Network (PINN) can solve both\nproblems. PINNs inject prior analytical information about the data into the\ncost function to improve model performance outside the training set boundaries.\nThis also allows PINNs to efficiently solve problems with sparse data without\noverfitting by extrapolating the model to fit larger trends in the data. The\nprior information we implement is in the form of differential equations.\nResiduals are the differences between the left-hand and right-hand sides of\ncorresponding differential equations; PINNs minimize these residuals to\neffectively solve the differential equation and take advantage of prior\nknowledge. In this way, the solution and parameters are embedded into the loss\nfunction and optimized, allowing both the weights of the neural network and the\nmodel parameters to be found simultaneously, solving both the forward and\ninverse problems in the process. In this paper, we will create PINNs with\nresiduals of varying complexity, beginning with linear and quadratic models and\nthen expanding to fit models for the heat equation and other complex\ndifferential equations. We will mainly use Python as the computing language,\nusing the PyTorch library to aid us in our research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u5fae\u5206\u65b9\u7a0b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u540c\u65f6\u89e3\u51b3\u6b63\u5411\u548c\u9006\u5411\u95ee\u9898\uff0c\u5e76\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u5fae\u5206\u65b9\u7a0b\u7684\u6b63\u5411\u548c\u9006\u5411\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u63d0\u9ad8\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u5916\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528PINN\uff0c\u5c06\u5fae\u5206\u65b9\u7a0b\u7684\u6b8b\u5dee\u5d4c\u5165\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u540c\u65f6\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u548c\u6a21\u578b\u53c2\u6570\u3002", "result": "PINN\u80fd\u591f\u9ad8\u6548\u89e3\u51b3\u7a00\u758f\u6570\u636e\u95ee\u9898\uff0c\u907f\u514d\u8fc7\u62df\u5408\uff0c\u5e76\u9002\u7528\u4e8e\u7ebf\u6027\u3001\u4e8c\u6b21\u53ca\u590d\u6742\u5fae\u5206\u65b9\u7a0b\uff08\u5982\u70ed\u65b9\u7a0b\uff09\u3002", "conclusion": "PINN\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u540c\u65f6\u89e3\u51b3\u6b63\u5411\u548c\u9006\u5411\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u590d\u6742\u5fae\u5206\u65b9\u7a0b\u3002"}}
{"id": "2507.19996", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19996", "abs": "https://arxiv.org/abs/2507.19996", "authors": ["Saeed Razavikia", "Mohammad Bokaei", "Arash Amini", "Stefano Rini", "Carlo Fischione"], "title": "DOA Estimation via Optimal Weighted Low-Rank Matrix Completion", "comment": null, "summary": "This paper presents a novel method for estimating the direction of arrival\n(DOA) for a non-uniform and sparse linear sensor array using the weighted\nlifted structure low-rank matrix completion. The proposed method uses a single\nsnapshot sample in which a single array of data is observed. The method is\nrooted in a weighted lifted-structured low-rank matrix recovery framework. The\nmethod involves four key steps: (i) lifting the antenna samples to form a\nlow-rank stature, then (ii) designing left and right weight matrices to reflect\nthe sample informativeness, (iii) estimating a noise-free uniform array output\nthrough completion of the weighted lifted samples, and (iv) obtaining the DOAs\nfrom the restored uniform linear array samples.\n  We study the complexity of steps (i) to (iii) above, where we analyze the\nrequired sample for the array interpolation of step (iii) for DOA estimation.\nWe demonstrate that the proposed choice of weight matrices achieves a\nnear-optimal sample complexity. This complexity aligns with the problem's\ndegree of freedom, equivalent to the number of DOAs adjusted for logarithmic\nfactors. Numerical evaluations show the proposed method's superiority against\nthe non-weighted counterpart and atomic norm minimization-based methods.\nNotably, our proposed method significantly improves, with approximately a 10 dB\nreduction in normalized mean-squared error over the non-weighted method at\nlow-noise conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u63d0\u5347\u7ed3\u6784\u4f4e\u79e9\u77e9\u9635\u8865\u5168\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u975e\u5747\u5300\u7a00\u758f\u7ebf\u6027\u4f20\u611f\u5668\u9635\u5217\u7684\u5230\u8fbe\u65b9\u5411\uff08DOA\uff09\uff0c\u4ec5\u9700\u5355\u6b21\u5feb\u7167\u6837\u672c\u3002", "motivation": "\u89e3\u51b3\u975e\u5747\u5300\u7a00\u758f\u9635\u5217\u5728\u5355\u6b21\u5feb\u7167\u4e0bDOA\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u901a\u8fc7\u52a0\u6743\u4f4e\u79e9\u77e9\u9635\u8865\u5168\u63d0\u5347\u6027\u80fd\u3002", "method": "1) \u63d0\u5347\u5929\u7ebf\u6837\u672c\u5f62\u6210\u4f4e\u79e9\u7ed3\u6784\uff1b2) \u8bbe\u8ba1\u5de6\u53f3\u6743\u91cd\u77e9\u9635\u53cd\u6620\u6837\u672c\u4fe1\u606f\u91cf\uff1b3) \u8865\u5168\u52a0\u6743\u63d0\u5347\u6837\u672c\u4f30\u8ba1\u65e0\u566a\u5747\u5300\u9635\u5217\u8f93\u51fa\uff1b4) \u4ece\u6062\u590d\u7684\u5747\u5300\u9635\u5217\u6837\u672c\u4e2d\u83b7\u53d6DOA\u3002", "result": "\u6570\u503c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u566a\u58f0\u6761\u4ef6\u4e0b\u6bd4\u975e\u52a0\u6743\u65b9\u6cd5\u548c\u539f\u5b50\u8303\u6570\u6700\u5c0f\u5316\u65b9\u6cd5\u6027\u80fd\u66f4\u4f18\uff0c\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u964d\u4f4e\u7ea610 dB\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a0\u6743\u65b9\u6cd5\u5728\u6837\u672c\u590d\u6742\u5ea6\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u9635\u5217DOA\u4f30\u8ba1\u3002"}}
{"id": "2507.19968", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19968", "abs": "https://arxiv.org/abs/2507.19968", "authors": ["Yue Hu", "Zanxia Cao", "Yingchao Liu"], "title": "Dimer-Enhanced Optimization: A First-Order Approach to Escaping Saddle Points in Neural Network Training", "comment": "8 pages, 2 figures", "summary": "First-order optimization methods, such as SGD and Adam, are widely used for\ntraining large-scale deep neural networks due to their computational efficiency\nand robust performance. However, relying solely on gradient information, these\nmethods often struggle to navigate complex loss landscapes with flat regions,\nplateaus, and saddle points. Second-order methods, which use curvature\ninformation from the Hessian matrix, can address these challenges but are\ncomputationally infeasible for large models. The Dimer method, a first-order\ntechnique that constructs two closely spaced points to probe the local geometry\nof a potential energy surface, efficiently estimates curvature using only\ngradient information. Inspired by its use in molecular dynamics simulations for\nlocating saddle points, we propose Dimer-Enhanced Optimization (DEO), a novel\nframework to escape saddle points in neural network training. DEO adapts the\nDimer method to explore a broader region of the loss landscape, approximating\nthe Hessian's smallest eigenvector without computing the full matrix. By\nperiodically projecting the gradient onto the subspace orthogonal to the\nminimum curvature direction, DEO guides the optimizer away from saddle points\nand flat regions, enhancing training efficiency with non-stepwise updates.\nPreliminary experiments on a Transformer toy model show DEO achieves\ncompetitive performance compared to standard first-order methods, improving\nnavigation of complex loss landscapes. Our work repurposes physics-inspired,\nfirst-order curvature estimation to enhance neural network training in\nhigh-dimensional spaces.", "AI": {"tldr": "DEO\uff08Dimer-Enhanced Optimization\uff09\u662f\u4e00\u79cd\u65b0\u578b\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u7269\u7406\u5b66\u542f\u53d1\u7684Dimer\u65b9\u6cd5\u4f30\u8ba1\u66f2\u7387\u4fe1\u606f\uff0c\u5e2e\u52a9\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u9003\u79bb\u978d\u70b9\u548c\u5e73\u5766\u533a\u57df\u3002", "motivation": "\u4f20\u7edf\u4e00\u9636\u4f18\u5316\u65b9\u6cd5\uff08\u5982SGD\u548cAdam\uff09\u5728\u590d\u6742\u635f\u5931\u66f2\u9762\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4e8c\u9636\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002DEO\u65e8\u5728\u901a\u8fc7\u4e00\u9636\u65b9\u6cd5\u9ad8\u6548\u4f30\u8ba1\u66f2\u7387\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "method": "DEO\u57fa\u4e8eDimer\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u9020\u4e24\u4e2a\u90bb\u8fd1\u70b9\u4f30\u8ba1\u5c40\u90e8\u66f2\u7387\uff0c\u5e76\u5468\u671f\u6027\u5c06\u68af\u5ea6\u6295\u5f71\u5230\u6700\u5c0f\u66f2\u7387\u65b9\u5411\u7684\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff0c\u907f\u514d\u978d\u70b9\u548c\u5e73\u5766\u533a\u57df\u3002", "result": "\u5728Transformer\u73a9\u5177\u6a21\u578b\u4e0a\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cDEO\u6027\u80fd\u4e0e\u6807\u51c6\u4e00\u9636\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u80fd\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u635f\u5931\u66f2\u9762\u3002", "conclusion": "DEO\u6210\u529f\u5c06\u7269\u7406\u5b66\u542f\u53d1\u7684\u66f2\u7387\u4f30\u8ba1\u6280\u672f\u5e94\u7528\u4e8e\u9ad8\u7ef4\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u4e3a\u4e00\u9636\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.19523", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19523", "abs": "https://arxiv.org/abs/2507.19523", "authors": ["Xingyu Su", "Xiner Li", "Yuchao Lin", "Ziqian Xie", "Degui Zhi", "Shuiwang Ji"], "title": "Language Models for Controllable DNA Sequence Design", "comment": null, "summary": "We consider controllable DNA sequence design, where sequences are generated\nby conditioning on specific biological properties. While language models (LMs)\nsuch as GPT and BERT have achieved remarkable success in natural language\ngeneration, their application to DNA sequence generation remains largely\nunderexplored. In this work, we introduce ATGC-Gen, an Automated Transformer\nGenerator for Controllable Generation, which leverages cross-modal encoding to\nintegrate diverse biological signals. ATGC-Gen is instantiated with both\ndecoder-only and encoder-only transformer architectures, allowing flexible\ntraining and generation under either autoregressive or masked recovery\nobjectives. We evaluate ATGC-Gen on representative tasks including promoter and\nenhancer sequence design, and further introduce a new dataset based on ChIP-Seq\nexperiments for modeling protein binding specificity. Our experiments\ndemonstrate that ATGC-Gen can generate fluent, diverse, and biologically\nrelevant sequences aligned with the desired properties. Compared to prior\nmethods, our model achieves notable improvements in controllability and\nfunctional relevance, highlighting the potential of language models in\nadvancing programmable genomic design. The source code is released at\n(https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).", "AI": {"tldr": "ATGC-Gen\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684DNA\u5e8f\u5217\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7f16\u7801\u6574\u5408\u751f\u7269\u4fe1\u53f7\uff0c\u652f\u6301\u81ea\u56de\u5f52\u6216\u63a9\u7801\u6062\u590d\u76ee\u6807\uff0c\u751f\u6210\u5177\u6709\u7279\u5b9a\u751f\u7269\u5c5e\u6027\u7684\u5e8f\u5217\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728DNA\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u63a7\u6027\u548c\u529f\u80fd\u76f8\u5173\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u89e3\u7801\u5668\u548c\u7f16\u7801\u5668Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u7f16\u7801\uff0c\u652f\u6301\u81ea\u56de\u5f52\u548c\u63a9\u7801\u6062\u590d\u8bad\u7ec3\u76ee\u6807\u3002", "result": "ATGC-Gen\u751f\u6210\u7684\u5e8f\u5217\u6d41\u7545\u3001\u591a\u6837\u4e14\u5177\u6709\u751f\u7269\u76f8\u5173\u6027\uff0c\u5728\u53ef\u63a7\u6027\u548c\u529f\u80fd\u76f8\u5173\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ATGC-Gen\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u7f16\u7a0b\u57fa\u56e0\u7ec4\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.20189", "categories": ["eess.SP", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.20189", "abs": "https://arxiv.org/abs/2507.20189", "authors": ["Chengkai Wang", "Di Wu", "Yunsheng Liao", "Wenyao Zheng", "Ziyi Zeng", "Xurong Gao", "Hemmings Wu", "Zhoule Zhu", "Jie Yang", "Lihua Zhong", "Weiwei Cheng", "Yun-Hsuan Chen", "Mohamad Sawan"], "title": "NeuroCLIP: A Multimodal Contrastive Learning Method for rTMS-treated Methamphetamine Addiction Analysis", "comment": null, "summary": "Methamphetamine dependence poses a significant global health challenge, yet\nits assessment and the evaluation of treatments like repetitive transcranial\nmagnetic stimulation (rTMS) frequently depend on subjective self-reports, which\nmay introduce uncertainties. While objective neuroimaging modalities such as\nelectroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS)\noffer alternatives, their individual limitations and the reliance on\nconventional, often hand-crafted, feature extraction can compromise the\nreliability of derived biomarkers. To overcome these limitations, we propose\nNeuroCLIP, a novel deep learning framework integrating simultaneously recorded\nEEG and fNIRS data through a progressive learning strategy. This approach\noffers a robust and trustworthy biomarker for methamphetamine addiction.\nValidation experiments show that NeuroCLIP significantly improves\ndiscriminative capabilities among the methamphetamine-dependent individuals and\nhealthy controls compared to models using either EEG or only fNIRS alone.\nFurthermore, the proposed framework facilitates objective, brain-based\nevaluation of rTMS treatment efficacy, demonstrating measurable shifts in\nneural patterns towards healthy control profiles after treatment. Critically,\nwe establish the trustworthiness of the multimodal data-driven biomarker by\nshowing its strong correlation with psychometrically validated craving scores.\nThese findings suggest that biomarker derived from EEG-fNIRS data via NeuroCLIP\noffers enhanced robustness and reliability over single-modality approaches,\nproviding a valuable tool for addiction neuroscience research and potentially\nimproving clinical assessments.", "AI": {"tldr": "NeuroCLIP\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408EEG\u548cfNIRS\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u7532\u57fa\u82ef\u4e19\u80fa\u4f9d\u8d56\u8005\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u5ba2\u89c2\u8bc4\u4f30rTMS\u6cbb\u7597\u6548\u679c\u3002", "motivation": "\u7532\u57fa\u82ef\u4e19\u80fa\u4f9d\u8d56\u7684\u8bc4\u4f30\u548c\u6cbb\u7597\u6548\u679c\u5e38\u4f9d\u8d56\u4e3b\u89c2\u62a5\u544a\uff0c\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3002\u73b0\u6709\u795e\u7ecf\u5f71\u50cf\u6280\u672f\uff08\u5982EEG\u548cfNIRS\uff09\u56e0\u4e2a\u4f53\u5c40\u9650\u6027\u548c\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u5f71\u54cd\u751f\u7269\u6807\u5fd7\u7269\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faNeuroCLIP\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5b66\u4e60\u7b56\u7565\u6574\u5408EEG\u548cfNIRS\u6570\u636e\uff0c\u751f\u6210\u66f4\u53ef\u9760\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002", "result": "NeuroCLIP\u663e\u8457\u63d0\u5347\u5bf9\u4f9d\u8d56\u8005\u548c\u5065\u5eb7\u5bf9\u7167\u7684\u533a\u5206\u80fd\u529b\uff0c\u5e76\u80fd\u5ba2\u89c2\u8bc4\u4f30rTMS\u6cbb\u7597\u540e\u7684\u795e\u7ecf\u6a21\u5f0f\u53d8\u5316\u3002\u751f\u7269\u6807\u5fd7\u7269\u4e0e\u5fc3\u7406\u6d4b\u91cf\u9a8c\u8bc1\u7684\u6e34\u6c42\u8bc4\u5206\u5f3a\u76f8\u5173\u3002", "conclusion": "NeuroCLIP\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u63d0\u4f9b\u66f4\u7a33\u5065\u53ef\u9760\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4e3a\u6210\u763e\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u53ca\u4e34\u5e8a\u8bc4\u4f30\u63d0\u4f9b\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.20048", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20048", "abs": "https://arxiv.org/abs/2507.20048", "authors": ["Jesus S. Aguilar-Ruiz"], "title": "Irredundant $k$-Fold Cross-Validation", "comment": null, "summary": "In traditional k-fold cross-validation, each instance is used ($k-1$) times\nfor training and once for testing, leading to redundancy that lets many\ninstances disproportionately influence the learning phase. We introduce\nIrredundant $k$-fold cross-validation, a novel method that guarantees each\ninstance is used exactly once for training and once for testing across the\nentire validation procedure. This approach ensures a more balanced utilization\nof the dataset, mitigates overfitting due to instance repetition, and enables\nsharper distinctions in comparative model analysis. The method preserves\nstratification and remains model-agnostic, i.e., compatible with any\nclassifier. Experimental results demonstrate that it delivers consistent\nperformance estimates across diverse datasets -- comparable to $k$-fold\ncross-validation -- while providing less optimistic variance estimates because\ntraining partitions are non-overlapping, and significantly reducing the overall\ncomputational cost.", "AI": {"tldr": "\u63d0\u51faIrredundant k-fold\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u786e\u4fdd\u6bcf\u4e2a\u5b9e\u4f8b\u4ec5\u7528\u4e8e\u4e00\u6b21\u8bad\u7ec3\u548c\u4e00\u6b21\u6d4b\u8bd5\uff0c\u51cf\u5c11\u5197\u4f59\u548c\u8fc7\u62df\u5408\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfk-fold\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u5b9e\u4f8b\u91cd\u590d\u4f7f\u7528\u5bfc\u81f4\u5197\u4f59\u548c\u4e0d\u5e73\u8861\uff0c\u5f71\u54cd\u6a21\u578b\u5206\u6790\u548c\u6027\u80fd\u4f30\u8ba1\u3002", "method": "\u5f15\u5165Irredundant k-fold\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u4fdd\u8bc1\u6bcf\u4e2a\u5b9e\u4f8b\u4ec5\u7528\u4e8e\u4e00\u6b21\u8bad\u7ec3\u548c\u4e00\u6b21\u6d4b\u8bd5\uff0c\u4fdd\u6301\u5206\u5c42\u4e14\u6a21\u578b\u65e0\u5173\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u6027\u80fd\u4e0e\u4f20\u7edfk-fold\u76f8\u5f53\uff0c\u4f46\u65b9\u5dee\u4f30\u8ba1\u66f4\u51c6\u786e\uff0c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "Irredundant k-fold\u4ea4\u53c9\u9a8c\u8bc1\u662f\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u5e73\u8861\u4e14\u53ef\u9760\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002"}}
{"id": "2507.19524", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19524", "abs": "https://arxiv.org/abs/2507.19524", "authors": ["Ugo Lomoio", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "title": "Kolmogorov Arnold Network Autoencoder in Medicine", "comment": null, "summary": "Deep learning neural networks architectures such Multi Layer Perceptrons\n(MLP) and Convolutional blocks still play a crucial role in nowadays research\nadvancements. From a topological point of view, these architecture may be\nrepresented as graphs in which we learn the functions related to the nodes\nwhile fixed edges convey the information from the input to the output. A recent\nwork introduced a new architecture called Kolmogorov Arnold Networks (KAN) that\nreports how putting learnable activation functions on the edges of the neural\nnetwork leads to better performances in multiple scenarios. Multiple studies\nare focusing on optimizing the KAN architecture by adding important features\nsuch as dropout regularization, Autoencoders (AE), model benchmarking and last,\nbut not least, the KAN Convolutional Network (KCN) that introduced matrix\nconvolution with KANs learning. This study aims to benchmark multiple versions\nof vanilla AEs (such as Linear, Convolutional and Variational) against their\nKolmogorov-Arnold counterparts that have same or less number of parameters.\nUsing cardiological signals as model input, a total of five different classic\nAE tasks were studied: reconstruction, generation, denoising, inpainting and\nanomaly detection. The proposed experiments uses a medical dataset\n\\textit{AbnormalHeartbeat} that contains audio signals obtained from the\nstethoscope.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4f20\u7edf\u81ea\u7f16\u7801\u5668\uff08\u5982\u7ebf\u6027\u3001\u5377\u79ef\u548c\u53d8\u5206\uff09\u4e0eKolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u53d8\u4f53\u5728\u5fc3\u810f\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0KAN\u5728\u53c2\u6570\u66f4\u5c11\u6216\u76f8\u540c\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76KAN\u67b6\u6784\u5728\u81ea\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5fc3\u810f\u4fe1\u53f7\u5904\u7406\u9886\u57df\uff0c\u4ee5\u9a8c\u8bc1\u5176\u76f8\u5bf9\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u5fc3\u810f\u4fe1\u53f7\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u4e0eKAN\u53d8\u4f53\u5728\u91cd\u5efa\u3001\u751f\u6210\u3001\u53bb\u566a\u3001\u4fee\u590d\u548c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "KAN\u67b6\u6784\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u81ea\u7f16\u7801\u5668\uff0c\u5c24\u5176\u662f\u5728\u53c2\u6570\u6548\u7387\u65b9\u9762\u3002", "conclusion": "KAN\u67b6\u6784\u5728\u5fc3\u810f\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.20283", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20283", "abs": "https://arxiv.org/abs/2507.20283", "authors": ["Haotian Tian", "Lixiang Lian", "Jiaqi Cao", "Sijie Ji"], "title": "Information-Preserving CSI Feedback: Invertible Networks with Endogenous Quantization and Channel Error Mitigation", "comment": null, "summary": "Deep learning has emerged as a promising solution for efficient channel state\ninformation (CSI) feedback in frequency division duplex (FDD) massive MIMO\nsystems. Conventional deep learning-based methods typically rely on a deep\nautoencoder to compress the CSI, which leads to irreversible information loss\nand degrades reconstruction accuracy. This paper introduces InvCSINet, an\ninformation-preserving CSI feedback framework based on invertible neural\nnetworks (INNs). By leveraging the bijective nature of INNs, the model ensures\ninformation-preserving compression and reconstruction with shared model\nparameters. To address practical challenges such as quantization and\nchannel-induced errors, we endogenously integrate an adaptive quantization\nmodule, a differentiable bit-channel distortion module and an information\ncompensation module into the INN architecture. This design enables the network\nto learn and compensate the information loss during CSI compression,\nquantization, and noisy transmission, thereby preserving the CSI integrity\nthroughout the feedback process. Simulation results validate the effectiveness\nof the proposed scheme, demonstrating superior CSI recovery performance and\nrobustness to practical impairments with a lightweight architecture.", "AI": {"tldr": "InvCSINet\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u9006\u795e\u7ecf\u7f51\u7edc\uff08INN\uff09\u7684CSI\u53cd\u9988\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u4fdd\u7559\u538b\u7f29\u548c\u91cd\u5efa\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5e76\u6574\u5408\u4e86\u81ea\u9002\u5e94\u91cf\u5316\u6a21\u5757\u548c\u5931\u771f\u8865\u507f\u6a21\u5757\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728CSI\u53cd\u9988\u4e2d\u56e0\u4fe1\u606f\u4e22\u5931\u5bfc\u81f4\u91cd\u5efa\u7cbe\u5ea6\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u4fe1\u606f\u4fdd\u7559\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528INN\u7684\u53cc\u5c04\u7279\u6027\u5b9e\u73b0\u4fe1\u606f\u4fdd\u7559\u538b\u7f29\u548c\u91cd\u5efa\uff0c\u5e76\u6574\u5408\u81ea\u9002\u5e94\u91cf\u5316\u3001\u5931\u771f\u8865\u507f\u548c\u4fe1\u606f\u8865\u507f\u6a21\u5757\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cInvCSINet\u5728CSI\u6062\u590d\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u67b6\u6784\u8f7b\u91cf\u3002", "conclusion": "InvCSINet\u901a\u8fc7INN\u548c\u8865\u507f\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86CSI\u53cd\u9988\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.20068", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20068", "abs": "https://arxiv.org/abs/2507.20068", "authors": ["Aishwarya Mandyam", "Jason Meng", "Ge Gao", "Jiankai Sun", "Mac Schwager", "Barbara E. Engelhardt", "Emma Brunskill"], "title": "PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data", "comment": null, "summary": "Off-policy evaluation (OPE) methods aim to estimate the value of a new\nreinforcement learning (RL) policy prior to deployment. Recent advances have\nshown that leveraging auxiliary datasets, such as those synthesized by\ngenerative models, can improve the accuracy of these value estimates.\nUnfortunately, such auxiliary datasets may also be biased, and existing methods\nfor using data augmentation for OPE in RL lack principled uncertainty\nquantification. In high stakes settings like healthcare, reliable uncertainty\nestimates are important for comparing policy value estimates. In this work, we\npropose two approaches to construct valid confidence intervals for OPE when\nusing data augmentation. The first provides a confidence interval over the\npolicy performance conditioned on a particular initial state $V^{\\pi}(s_0)$--\nsuch intervals are particularly important for human-centered applications. To\ndo so we introduce a new conformal prediction method for high dimensional state\nMDPs. Second, we consider the more common task of estimating the average policy\nperformance over many initial states; to do so we draw on ideas from doubly\nrobust estimation and prediction powered inference. Across simulators spanning\nrobotics, healthcare and inventory management, and a real healthcare dataset\nfrom MIMIC-IV, we find that our methods can use augmented data and still\nconsistently produce intervals that cover the ground truth values, unlike\npreviously proposed methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u65f6\u6784\u5efa\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u4ee5\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u79bb\u7b56\u7565\u8bc4\u4f30\uff08OPE\uff09\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u53ef\u9760\u7684\u7b56\u7565\u4ef7\u503c\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u8fd9\u4e00\u80fd\u529b\u3002", "method": "1. \u9488\u5bf9\u7279\u5b9a\u521d\u59cb\u72b6\u6001\u7684\u6761\u4ef6\u7b56\u7565\u6027\u80fd\u7f6e\u4fe1\u533a\u95f4\uff1b2. \u57fa\u4e8e\u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u548c\u9884\u6d4b\u9a71\u52a8\u63a8\u65ad\u7684\u5e73\u5747\u7b56\u7565\u6027\u80fd\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u62df\u5668\u548c\u771f\u5b9e\u533b\u7597\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u65b9\u6cd5\u80fd\u8986\u76d6\u771f\u5b9e\u503c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86OPE\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.19525", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19525", "abs": "https://arxiv.org/abs/2507.19525", "authors": ["Chenchen Zhao", "Zhengyuan Shi", "Xiangyu Wen", "Chengjie Liu", "Yi Liu", "Yunhao Zhou", "Yuxiang Zhao", "Hefei Feng", "Yinan Zhu", "Gwok-Waa Wan", "Xin Cheng", "Weiyu Chen", "Yongqi Fu", "Chujie Chen", "Chenhao Xue", "Guangyu Sun", "Ying Wang", "Yibo Lin", "Jun Yang", "Ning Xu", "Xi Wang", "Qiang Xu"], "title": "MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs", "comment": "10 pages, 1 figure, 5 tables. To appear in ICCAD 2025", "summary": "The emergence of multimodal large language models (MLLMs) presents promising\nopportunities for automation and enhancement in Electronic Design Automation\n(EDA). However, comprehensively evaluating these models in circuit design\nremains challenging due to the narrow scope of existing benchmarks. To bridge\nthis gap, we introduce MMCircuitEval, the first multimodal benchmark\nspecifically designed to assess MLLM performance comprehensively across diverse\nEDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer\n(QA) pairs spanning digital and analog circuits across critical EDA stages -\nranging from general knowledge and specifications to front-end and back-end\ndesign. Derived from textbooks, technical question banks, datasheets, and\nreal-world documentation, each QA pair undergoes rigorous expert review for\naccuracy and relevance. Our benchmark uniquely categorizes questions by design\nstage, circuit type, tested abilities (knowledge, comprehension, reasoning,\ncomputation), and difficulty level, enabling detailed analysis of model\ncapabilities and limitations. Extensive evaluations reveal significant\nperformance gaps among existing LLMs, particularly in back-end design and\ncomplex computations, highlighting the critical need for targeted training\ndatasets and modeling approaches. MMCircuitEval provides a foundational\nresource for advancing MLLMs in EDA, facilitating their integration into\nreal-world circuit design workflows. Our benchmark is available at\nhttps://github.com/cure-lab/MMCircuitEval.", "AI": {"tldr": "MMCircuitEval\u662f\u9996\u4e2a\u9488\u5bf9EDA\u4efb\u52a1\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3614\u4e2aQA\u5bf9\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30MLLM\u5728\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8303\u56f4\u72ed\u7a84\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30MLLM\u5728EDA\u4e2d\u7684\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u4ece\u6559\u79d1\u4e66\u3001\u6280\u672f\u9898\u5e93\u7b49\u6765\u6e90\u7cbe\u5fc3\u7b5b\u9009QA\u5bf9\uff0c\u5e76\u6309\u8bbe\u8ba1\u9636\u6bb5\u3001\u7535\u8def\u7c7b\u578b\u7b49\u5206\u7c7b\uff0c\u6784\u5efaMMCircuitEval\u57fa\u51c6\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u73b0\u6709LLM\u5728EDA\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u540e\u7aef\u8bbe\u8ba1\u548c\u590d\u6742\u8ba1\u7b97\u65b9\u9762\u3002", "conclusion": "MMCircuitEval\u4e3aMLLM\u5728EDA\u4e2d\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u5176\u5728\u5b9e\u9645\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.20392", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20392", "abs": "https://arxiv.org/abs/2507.20392", "authors": ["Donggu Lee", "Sung Joon Maeng", "Ozgur Ozdemir", "Mani Bharathi Pandian", "Ismail Guvenc"], "title": "Reliability of Wi-Fi, LTE, and 5G-Based UAV RC Links in ISM Bands: Uplink Interference Asymmetry Analysis and HARQ Design", "comment": null, "summary": "Command and control of uncrewed aerial vehicles (UAVs) is often realized\nthrough air-to-ground (A2G) remote control (RC) links that operate in ISM\nbands. While wireless fidelity (Wi-Fi) technology is commonly used for UAV RC\nlinks, ISM-based long-term evolution (LTE) and fifth-generation (5G)\ntechnologies have also been recently considered for the same purpose. A major\nproblem for UAV RC links in the ISM bands is that other types of interference\nsources, such as legacy Wi-Fi and Bluetooth transmissions, may degrade the link\nquality. Such interference problems are a higher concern for the UAV in the air\nthan the RC unit on the ground due to the UAV being in line-of-sight (LoS) with\na larger number of interference sources. To obtain empirical evidence of the\nasymmetric interference conditions in downlink (DL) and uplink (UL), we first\nconducted a measurement campaign using a helikite platform in urban and rural\nareas at NC State University. The results from this measurement campaign show\nthat the aggregate interference can be up to 16.66 dB at higher altitudes up to\n170 m, compared with the interference observed at a ground receiver. As a\nresult of this asymmetric UL interference, lost hybrid automatic repeat request\n(HARQ) indicators (ACK/NACK) in the UL may degrade the DL throughput. To\ninvestigate this, we study various HARQ mechanisms, including HARQ Type-I with\nno combining, HARQ Type-I with chase combining, HARQ Type-III with incremental\nredundancy, and burst transmission with chase combining. To evaluate the impact\nof asymmetric UL interference on throughput performance, we consider three\nsteps of evaluation process: 1) standalone physical DL shared channel (PDSCH)\nthroughput evaluation with perfect ACK/NACK assumption; 2) standalone physical\nUL control channel (PUCCH) decoding reliability evaluation; and 3) PDSCH DL\nthroughput evaluation with asymmetric UL ACK/NACK transmission.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u4eba\u673a\u9065\u63a7\u94fe\u8def\u5728ISM\u9891\u6bb5\u4e2d\u7684\u4e0d\u5bf9\u79f0\u5e72\u6270\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u91cf\u548cHARQ\u673a\u5236\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u4e0a\u884c\u94fe\u8def\u5e72\u6270\u5bf9\u4e0b\u884c\u94fe\u8def\u541e\u5410\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u65e0\u4eba\u673a\u9065\u63a7\u94fe\u8def\u5728ISM\u9891\u6bb5\u6613\u53d7\u5e72\u6270\uff0c\u5c24\u5176\u662f\u4e0a\u884c\u94fe\u8def\u5e72\u6270\u53ef\u80fd\u5f71\u54cd\u4e0b\u884c\u94fe\u8def\u6027\u80fd\uff0c\u9700\u901a\u8fc7\u5b9e\u9a8c\u548c\u673a\u5236\u5206\u6790\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u6d3b\u52a8\uff08\u4f7f\u7528\u6c26\u6c14\u7403\u5e73\u53f0\uff09\u83b7\u53d6\u5e72\u6270\u6570\u636e\uff0c\u5e76\u5206\u6790\u591a\u79cdHARQ\u673a\u5236\uff08\u5982Type-I\u3001Type-III\u7b49\uff09\u5bf9\u541e\u5410\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u9ad8\u7a7a\u5e72\u6270\u6bd4\u5730\u9762\u9ad816.66 dB\uff0c\u4e0a\u884c\u94fe\u8def\u5e72\u6270\u5bfc\u81f4HARQ\u6307\u6807\u4e22\u5931\uff0c\u5f71\u54cd\u4e0b\u884c\u94fe\u8def\u541e\u5410\u91cf\u3002", "conclusion": "\u4e0d\u5bf9\u79f0\u4e0a\u884c\u94fe\u8def\u5e72\u6270\u663e\u8457\u5f71\u54cd\u65e0\u4eba\u673a\u9065\u63a7\u94fe\u8def\u6027\u80fd\uff0c\u9700\u4f18\u5316HARQ\u673a\u5236\u4ee5\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\u3002"}}
{"id": "2507.20088", "categories": ["cs.LG", "math-ph", "math.MP", "math.OC", "stat.ML", "G.1.6; G.1.7; G.2.2"], "pdf": "https://arxiv.org/pdf/2507.20088", "abs": "https://arxiv.org/abs/2507.20088", "authors": ["Dmitry Pasechnyuk-Vilensky", "Daniil Doroshenko"], "title": "Feed-anywhere ANN (I) Steady Discrete $\\to$ Diffusing on Graph Hidden States", "comment": "11 pages, 1 algorithm", "summary": "We propose a novel framework for learning hidden graph structures from data\nusing geometric analysis and nonlinear dynamics. Our approach: (1) Defines\ndiscrete Sobolev spaces on graphs for scalar/vector fields, establishing key\nfunctional properties; (2) Introduces gauge-equivalent nonlinear Schr\\\"odinger\nand Landau--Lifshitz dynamics with provable stable stationary solutions\nsmoothly dependent on input data and graph weights; (3) Develops a stochastic\ngradient algorithm over graph moduli spaces with sparsity regularization.\nTheoretically, we guarantee: topological correctness (homology recovery),\nmetric convergence (Gromov--Hausdorff), and efficient search space utilization.\nOur dynamics-based model achieves stronger generalization bounds than standard\nneural networks, with complexity dependent on the data manifold's topology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u5206\u6790\u548c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u9690\u85cf\u7684\u56fe\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u51e0\u4f55\u5206\u6790\u548c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. \u5728\u56fe\u4e0a\u7684\u79bb\u6563Sobolev\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u6807\u91cf/\u5411\u91cf\u573a\uff1b2. \u5f15\u5165\u89c4\u8303\u7b49\u4ef7\u7684\u975e\u7ebf\u6027Schr\u00f6dinger\u548cLandau--Lifshitz\u52a8\u529b\u5b66\uff1b3. \u5f00\u53d1\u57fa\u4e8e\u56fe\u6a21\u7a7a\u95f4\u7684\u968f\u673a\u68af\u5ea6\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u62d3\u6251\u6b63\u786e\u6027\u3001\u5ea6\u91cf\u6536\u655b\u6027\u548c\u9ad8\u6548\u641c\u7d22\u7a7a\u95f4\u5229\u7528\uff0c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u56fe\u7ed3\u6784\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u548c\u65b9\u6cd5\u652f\u6301\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u62d3\u6251\u4f9d\u8d56\u6027\u3002"}}
{"id": "2507.19526", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19526", "abs": "https://arxiv.org/abs/2507.19526", "authors": ["Jianyuan Bo", "Hao Wu", "Yuan Fang"], "title": "Quantizing Text-attributed Graphs for Semantic-Structural Integration", "comment": "Accepted at KDD'2025", "summary": "Text-attributed graphs (TAGs) have emerged as a powerful representation for\nmodeling complex relationships across diverse domains. With the rise of large\nlanguage models (LLMs), there is growing interest in leveraging their\ncapabilities for graph learning. However, current approaches face significant\nchallenges in embedding structural information into LLM-compatible formats,\nrequiring either computationally expensive alignment mechanisms or manual graph\nverbalization techniques that often lose critical structural details. Moreover,\nthese methods typically require labeled data from source domains for effective\ntransfer learning, significantly constraining their adaptability. We propose\nSTAG, a novel self-supervised framework that directly quantizes graph\nstructural information into discrete tokens using a frozen codebook. Unlike\ntraditional quantization approaches, our method employs soft assignment and KL\ndivergence guided quantization to address the unique challenges of graph data,\nwhich lacks natural tokenization structures. Our framework enables both\nLLM-based and traditional learning approaches, supporting true zero-shot\ntransfer learning without requiring labeled data even in the source domain.\nExtensive experiments demonstrate state-of-the-art performance across multiple\nnode classification benchmarks while maintaining compatibility with different\nLLM architectures, offering an elegant solution to bridging graph learning with\nLLMs.", "AI": {"tldr": "STAG\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u56fe\u7ed3\u6784\u4fe1\u606f\u4e3a\u4ee4\u724c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u7ed3\u6784\u4fe1\u606f\u5d4c\u5165LLM\u517c\u5bb9\u683c\u5f0f\u65f6\u7684\u6311\u6218\uff0c\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u56fe\u7ed3\u6784\u4fe1\u606f\u5d4c\u5165LLM\u517c\u5bb9\u683c\u5f0f\u65f6\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u4e22\u5931\u7ed3\u6784\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u6e90\u57df\u6807\u8bb0\u6570\u636e\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u3002", "method": "STAG\u901a\u8fc7\u8f6f\u5206\u914d\u548cKL\u6563\u5ea6\u5f15\u5bfc\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5c06\u56fe\u7ed3\u6784\u4fe1\u606f\u76f4\u63a5\u91cf\u5316\u4e3a\u79bb\u6563\u4ee4\u724c\uff0c\u65e0\u9700\u81ea\u7136\u4ee4\u724c\u5316\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTAG\u5728\u591a\u4e2a\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u517c\u5bb9\u4e0d\u540cLLM\u67b6\u6784\u3002", "conclusion": "STAG\u4e3a\u56fe\u5b66\u4e60\u4e0eLLM\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u4f18\u96c5\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u3002"}}
{"id": "2507.20408", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20408", "abs": "https://arxiv.org/abs/2507.20408", "authors": ["Samiul Based Shuvo", "Taufiq Hasan"], "title": "A Multi-Stage Hybrid CNN-Transformer Network for Automated Pediatric Lung Sound Classification", "comment": null, "summary": "Automated analysis of lung sound auscultation is essential for monitoring\nrespiratory health, especially in regions facing a shortage of skilled\nhealthcare workers. While respiratory sound classification has been widely\nstudied in adults, its ap plication in pediatric populations, particularly in\nchildren aged <6 years, remains an underexplored area. The developmental\nchanges in pediatric lungs considerably alter the acoustic proper ties of\nrespiratory sounds, necessitating specialized classification approaches\ntailored to this age group. To address this, we propose a multistage hybrid\nCNN-Transformer framework that combines CNN-extracted features with an\nattention-based architecture to classify pediatric respiratory diseases using\nscalogram images from both full recordings and individual breath events. Our\nmodel achieved an overall score of 0.9039 in binary event classifi cation and\n0.8448 in multiclass event classification by employing class-wise focal loss to\naddress data imbalance. At the recording level, the model attained scores of\n0.720 for ternary and 0.571 for multiclass classification. These scores\noutperform the previous best models by 3.81% and 5.94%, respectively. This\napproach offers a promising solution for scalable pediatric respiratory disease\ndiagnosis, especially in resource-limited settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN-Transformer\u6df7\u5408\u6846\u67b6\u7684\u513f\u79d1\u547c\u5438\u75be\u75c5\u5206\u7c7b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u513f\u7ae5\u547c\u5438\u97f3\u5206\u7c7b\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u6570\u636e\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u513f\u79d1\u547c\u5438\u97f3\u5206\u7c7b\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f6\u5c81\u4ee5\u4e0b\u513f\u7ae5\uff0c\u5176\u80ba\u90e8\u53d1\u80b2\u53d8\u5316\u5bfc\u81f4\u547c\u5438\u97f3\u7279\u6027\u4e0d\u540c\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6df7\u5408CNN-Transformer\u6846\u67b6\uff0c\u7ed3\u5408CNN\u63d0\u53d6\u7684\u7279\u5f81\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\uff0c\u5bf9\u547c\u5438\u97f3\u8fdb\u884c\u4e8c\u5143\u548c\u591a\u7c7b\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u5728\u4e8c\u5143\u4e8b\u4ef6\u5206\u7c7b\u4e2d\u5f97\u52060.9039\uff0c\u591a\u7c7b\u4e8b\u4ef6\u5206\u7c7b\u4e2d\u5f97\u52060.8448\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b3.81%\u548c5.94%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u513f\u79d1\u547c\u5438\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20089", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20089", "abs": "https://arxiv.org/abs/2507.20089", "authors": ["Ziyi Liang", "Annie Qu", "Babak Shahbaba"], "title": "Meta Fusion: A Unified Framework For Multimodality Fusion with Mutual Learning", "comment": null, "summary": "Developing effective multimodal data fusion strategies has become\nincreasingly essential for improving the predictive power of statistical\nmachine learning methods across a wide range of applications, from autonomous\ndriving to medical diagnosis. Traditional fusion methods, including early,\nintermediate, and late fusion, integrate data at different stages, each\noffering distinct advantages and limitations. In this paper, we introduce Meta\nFusion, a flexible and principled framework that unifies these existing\nstrategies as special cases. Motivated by deep mutual learning and ensemble\nlearning, Meta Fusion constructs a cohort of models based on various\ncombinations of latent representations across modalities, and further boosts\npredictive performance through soft information sharing within the cohort. Our\napproach is model-agnostic in learning the latent representations, allowing it\nto flexibly adapt to the unique characteristics of each modality.\nTheoretically, our soft information sharing mechanism reduces the\ngeneralization error. Empirically, Meta Fusion consistently outperforms\nconventional fusion strategies in extensive simulation studies. We further\nvalidate our approach on real-world applications, including Alzheimer's disease\ndetection and neural decoding.", "AI": {"tldr": "Meta Fusion\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u65e9\u671f\u3001\u4e2d\u671f\u548c\u665a\u671f\u878d\u5408\u7b56\u7565\uff0c\u901a\u8fc7\u8f6f\u4fe1\u606f\u5171\u4eab\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u4e14\u6027\u80fd\u66f4\u4f18\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u6df1\u5ea6\u4e92\u5b66\u4e60\u548c\u96c6\u6210\u5b66\u4e60\uff0cMeta Fusion\u6784\u5efa\u591a\u6a21\u6001\u6f5c\u5728\u8868\u793a\u7ec4\u5408\u7684\u6a21\u578b\u7fa4\uff0c\u5e76\u901a\u8fc7\u8f6f\u4fe1\u606f\u5171\u4eab\u4f18\u5316\u6027\u80fd\u3002", "result": "Meta Fusion\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5e94\u7528\uff08\u5982\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u548c\u795e\u7ecf\u89e3\u7801\uff09\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u878d\u5408\u7b56\u7565\u3002", "conclusion": "Meta Fusion\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.19527", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19527", "abs": "https://arxiv.org/abs/2507.19527", "authors": ["Yihan Wang", "Jianing Zhao"], "title": "Research on the application of graph data structure and graph neural network in node classification/clustering tasks", "comment": null, "summary": "Graph-structured data are pervasive across domains including social networks,\nbiological networks, and knowledge graphs. Due to their non-Euclidean nature,\nsuch data pose significant challenges to conventional machine learning methods.\nThis study investigates graph data structures, classical graph algorithms, and\nGraph Neural Networks (GNNs), providing comprehensive theoretical analysis and\ncomparative evaluation. Through comparative experiments, we quantitatively\nassess performance differences between traditional algorithms and GNNs in node\nclassification and clustering tasks. Results show GNNs achieve substantial\naccuracy improvements of 43% to 70% over traditional methods. We further\nexplore integration strategies between classical algorithms and GNN\narchitectures, providing theoretical guidance for advancing graph\nrepresentation learning research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u56fe\u6570\u636e\u7ed3\u6784\u3001\u7ecf\u5178\u56fe\u7b97\u6cd5\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\uff0c\u901a\u8fc7\u6bd4\u8f83\u5b9e\u9a8c\u5b9a\u91cf\u8bc4\u4f30\u4e86\u4f20\u7edf\u7b97\u6cd5\u4e0eGNNs\u5728\u8282\u70b9\u5206\u7c7b\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u7ed3\u679c\u663e\u793aGNNs\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u534743%\u81f370%\u3002", "motivation": "\u7531\u4e8e\u56fe\u6570\u636e\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u7279\u6027\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u56fe\u6570\u636e\u7ed3\u6784\u548cGNNs\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u7814\u7a76\u7ed3\u5408\u4e86\u7406\u8bba\u5206\u6790\u548c\u6bd4\u8f83\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u4f20\u7edf\u56fe\u7b97\u6cd5\u4e0eGNNs\u5728\u8282\u70b9\u5206\u7c7b\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "GNNs\u5728\u8282\u70b9\u5206\u7c7b\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u534743%\u81f370%\u3002", "conclusion": "\u8bba\u6587\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\uff0c\u5e76\u63a2\u7d22\u4e86\u7ecf\u5178\u7b97\u6cd5\u4e0eGNN\u67b6\u6784\u7684\u96c6\u6210\u7b56\u7565\u3002"}}
{"id": "2507.20489", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20489", "abs": "https://arxiv.org/abs/2507.20489", "authors": ["Sanghyeok Kim", "Jinu Gong", "Joonhyuk Kang"], "title": "Energy-Efficient Secure Communications via Joint Optimization of UAV Trajectory and Movable-Antenna Array Beamforming", "comment": "5 pages, 2 figures", "summary": "This paper investigates the potential of unmanned aerial vehicles (UAVs)\nequipped with movable-antenna (MA) arrays to strengthen security in wireless\ncommunication systems. We propose a novel framework that jointly optimizes the\nUAV trajectory and the reconfigurable beamforming of the MA array to maximize\nsecrecy energy efficiency, while ensuring reliable communication with\nlegitimate users. By exploiting the spatial degrees of freedom enabled by the\nMA array, the system can form highly directional beams and deep nulls, thereby\nsignificantly improving physical layer security. Numerical results demonstrate\nthat the proposed approach achieves superior secrecy energy efficiency,\nattributed to the enhanced spatial flexibility provided by the movable antenna\narchitecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u914d\u5907\u53ef\u79fb\u52a8\u5929\u7ebf\u9635\u5217\u7684\u65e0\u4eba\u673a\u4f18\u5316\u8f68\u8ff9\u548c\u6ce2\u675f\u6210\u5f62\u4ee5\u63d0\u9ad8\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5b89\u5168\u6027\u7684\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u65e0\u4eba\u673a\u548c\u53ef\u79fb\u52a8\u5929\u7ebf\u9635\u5217\u5728\u589e\u5f3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5b89\u5168\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u548c\u53ef\u79fb\u52a8\u5929\u7ebf\u9635\u5217\u7684\u6ce2\u675f\u6210\u5f62\uff0c\u4ee5\u6700\u5927\u5316\u4fdd\u5bc6\u80fd\u91cf\u6548\u7387\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4fdd\u5bc6\u80fd\u91cf\u6548\u7387\u3002", "conclusion": "\u53ef\u79fb\u52a8\u5929\u7ebf\u67b6\u6784\u63d0\u4f9b\u7684\u7a7a\u95f4\u7075\u6d3b\u6027\u663e\u8457\u63d0\u5347\u4e86\u7269\u7406\u5c42\u5b89\u5168\u6027\u3002"}}
{"id": "2507.20108", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20108", "abs": "https://arxiv.org/abs/2507.20108", "authors": ["Tony Shaska Sr"], "title": "Graded Transformers: A Symbolic-Geometric Approach to Structured Learning", "comment": null, "summary": "We introduce the Graded Transformer framework, a novel class of sequence\nmodels that embeds algebraic inductive biases through grading transformations\non vector spaces. Extending the theory of Graded Neural Networks (GNNs), we\npropose two architectures: the Linearly Graded Transformer (LGT) and the\nExponentially Graded Transformer (EGT). These models apply parameterized\nscaling operators-governed by fixed or learnable grading tuples and, for EGT,\nexponential factors to infuse hierarchical structure into attention and\nrepresentation layers, enhancing efficiency for structured data.\n  We derive rigorous theoretical guarantees, including universal approximation\ntheorems for continuous and Sobolev functions, reduced sample complexity via\neffective VC dimension bounds, Lipschitz continuity of graded operations, and\nrobustness to adversarial perturbations. A graded loss function ensures\ngradient stability and alignment with domain priors during optimization. By\ntreating grades as differentiable parameters, the framework enables adaptive\nfeature prioritization, overcoming limitations of fixed grades in prior work.\n  The Graded Transformer holds transformative potential for hierarchical\nlearning and neurosymbolic reasoning, with applications spanning algebraic\ngeometry (e.g., moduli spaces and zeta functions), physics (e.g., multiscale\nsimulations), natural language processing (e.g., syntactic parsing), biological\nsequence analysis (e.g., variant prediction), and emerging areas like graph\nneural networks and financial modeling. This work advances structured deep\nlearning by fusing geometric and algebraic principles with attention\nmechanisms, offering a mathematically grounded alternative to data-driven\nmodels and paving the way for interpretable, efficient systems in complex\ndomains.", "AI": {"tldr": "\u63d0\u51fa\u4e86Graded Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u6570\u5f52\u7eb3\u504f\u7f6e\u589e\u5f3a\u5e8f\u5217\u6a21\u578b\uff0c\u5305\u62ecLGT\u548cEGT\u4e24\u79cd\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u7ed3\u6784\u5316\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u878d\u5408\u51e0\u4f55\u4e0e\u4ee3\u6570\u539f\u7406\uff0c\u63d0\u5347\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u6548\u7387\u3002", "method": "\u5f15\u5165\u5206\u7ea7\u53d8\u6362\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u7f29\u653e\u64cd\u4f5c\u7b26\uff08\u56fa\u5b9a\u6216\u53ef\u5b66\u4e60\u7684\u5206\u7ea7\u5143\u7ec4\uff09\u5728\u6ce8\u610f\u529b\u548c\u8868\u793a\u5c42\u4e2d\u6ce8\u5165\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u5305\u62ec\u901a\u7528\u903c\u8fd1\u5b9a\u7406\u3001\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4e\u3001\u64cd\u4f5c\u8fde\u7eed\u6027\u53ca\u5bf9\u6297\u9c81\u68d2\u6027\uff1b\u5e94\u7528\u9886\u57df\u5e7f\u6cdb\u3002", "conclusion": "Graded Transformer\u4e3a\u7ed3\u6784\u5316\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u9ad8\u6548\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.19529", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19529", "abs": "https://arxiv.org/abs/2507.19529", "authors": ["Obumneme Nwafor", "Mohammed Abdul Majeed Al Hooti"], "title": "Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction", "comment": null, "summary": "As green hydrogen emerges as a major component of global decarbonisation,\nOman has positioned itself strategically through national auctions and\ninternational partnerships. Following two successful green hydrogen project\nrounds, the country launched its third auction (R3) in the Duqm region. While\nthis area exhibits relative geospatial homogeneity, it is still vulnerable to\nenvironmental fluctuations that pose inherent risks to productivity. Despite\ngrowing global investment in green hydrogen, operational data remains scarce,\nwith major projects like Saudi Arabia's NEOM facility not expected to commence\nproduction until 2026, and Oman's ACME Duqm project scheduled for 2028. This\nabsence of historical maintenance and performance data from large-scale\nhydrogen facilities in desert environments creates a major knowledge gap for\naccurate risk assessment for infrastructure planning and auction decisions.\nGiven this data void, environmental conditions emerge as accessible and\nreliable proxy for predicting infrastructure maintenance pressures, because\nharsh desert conditions such as dust storms, extreme temperatures, and humidity\nfluctuations are well-documented drivers of equipment degradation in renewable\nenergy systems. To address this challenge, this paper proposes an Artificial\nIntelligence decision support system that leverages publicly available\nmeteorological data to develop a predictive Maintenance Pressure Index (MPI),\nwhich predicts risk levels and future maintenance demands on hydrogen\ninfrastructure. This tool strengthens regulatory foresight and operational\ndecision-making by enabling temporal benchmarking to assess and validate\nperformance claims over time. It can be used to incorporate temporal risk\nintelligence into auction evaluation criteria despite the absence of historical\noperational benchmarks.", "AI": {"tldr": "\u963f\u66fc\u901a\u8fc7\u7eff\u8272\u6c22\u80fd\u9879\u76ee\u5e94\u5bf9\u5168\u7403\u8131\u78b3\u8d8b\u52bf\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u6c22\u8bbe\u65bd\u7684\u5386\u53f2\u6570\u636e\uff0c\u5bfc\u81f4\u98ce\u9669\u8bc4\u4f30\u56f0\u96be\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6c14\u8c61\u6570\u636e\u7684AI\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u9884\u6d4b\u7ef4\u62a4\u538b\u529b\u6307\u6570\uff08MPI\uff09\uff0c\u4ee5\u586b\u8865\u6570\u636e\u7a7a\u767d\u5e76\u4f18\u5316\u62cd\u5356\u51b3\u7b56\u3002", "motivation": "\u5168\u7403\u7eff\u8272\u6c22\u80fd\u6295\u8d44\u589e\u52a0\uff0c\u4f46\u7f3a\u4e4f\u6c99\u6f20\u73af\u5883\u4e0b\u5927\u89c4\u6a21\u6c22\u8bbe\u65bd\u7684\u8fd0\u884c\u6570\u636e\uff0c\u5bfc\u81f4\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u62cd\u5356\u51b3\u7b56\u7684\u98ce\u9669\u8bc4\u4f30\u56f0\u96be\u3002", "method": "\u5229\u7528\u516c\u5f00\u6c14\u8c61\u6570\u636e\u5f00\u53d1AI\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u751f\u6210\u7ef4\u62a4\u538b\u529b\u6307\u6570\uff08MPI\uff09\uff0c\u9884\u6d4b\u6c22\u80fd\u57fa\u7840\u8bbe\u65bd\u7684\u7ef4\u62a4\u9700\u6c42\u548c\u98ce\u9669\u6c34\u5e73\u3002", "result": "\u63d0\u51fa\u7684MPI\u5de5\u5177\u80fd\u591f\u586b\u8865\u5386\u53f2\u6570\u636e\u7a7a\u767d\uff0c\u4e3a\u62cd\u5356\u8bc4\u4f30\u548c\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u65f6\u95f4\u7ef4\u5ea6\u7684\u98ce\u9669\u667a\u80fd\u3002", "conclusion": "AI\u9a71\u52a8\u7684MPI\u7cfb\u7edf\u4e3a\u7eff\u8272\u6c22\u80fd\u9879\u76ee\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u98ce\u9669\u9884\u6d4b\u5de5\u5177\uff0c\u652f\u6301\u66f4\u660e\u667a\u7684\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u62cd\u5356\u51b3\u7b56\u3002"}}
{"id": "2507.20112", "categories": ["cs.LG", "cs.AI", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20112", "abs": "https://arxiv.org/abs/2507.20112", "authors": ["Tianyi Xu", "Yiting Chen", "Henger Li", "Zheyong Bian", "Emiliano Dall'Anese", "Zizhan Zheng"], "title": "Online Learning with Probing for Sequential User-Centric Selection", "comment": null, "summary": "We formalize sequential decision-making with information acquisition as the\nprobing-augmented user-centric selection (PUCS) framework, where a learner\nfirst probes a subset of arms to obtain side information on resources and\nrewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such\nas ridesharing, wireless scheduling, and content recommendation, in which both\nresources and payoffs are initially unknown and probing is costly. For the\noffline setting with known distributions, we present a greedy probing algorithm\nwith a constant-factor approximation guarantee $\\zeta = (e-1)/(2e-1)$. For the\nonline setting with unknown distributions, we introduce OLPA, a stochastic\ncombinatorial bandit algorithm that achieves a regret bound\n$\\mathcal{O}(\\sqrt{T} + \\ln^{2} T)$. We also prove a lower bound\n$\\Omega(\\sqrt{T})$, showing that the upper bound is tight up to logarithmic\nfactors. Experiments on real-world data demonstrate the effectiveness of our\nsolutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PUCS\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4fe1\u606f\u83b7\u53d6\u4e0b\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u5305\u62ec\u79bb\u7ebf\u8d2a\u5fc3\u7b97\u6cd5\u548c\u5728\u7ebfOLPA\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u8d44\u6e90\u4e0e\u56de\u62a5\u672a\u77e5\u4e14\u63a2\u6d4b\u6210\u672c\u9ad8\u7684\u5e94\u7528\u4e2d\uff08\u5982\u62fc\u8f66\u3001\u65e0\u7ebf\u8c03\u5ea6\u7b49\uff09\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u51b3\u7b56\u6846\u67b6\u3002", "method": "\u63d0\u51faPUCS\u6846\u67b6\uff0c\u79bb\u7ebf\u7528\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u5728\u7ebf\u7528OLPA\u7b97\u6cd5\u3002", "result": "\u79bb\u7ebf\u7b97\u6cd5\u6709\u5e38\u6570\u8fd1\u4f3c\u6bd4\uff0c\u5728\u7ebf\u7b97\u6cd5\u8fbe\u5230\u221aT\u7684\u9057\u61be\u4e0a\u754c\uff0c\u4e14\u4e0b\u754c\u8bc1\u660e\u5176\u7d27\u6027\u3002", "conclusion": "PUCS\u6846\u67b6\u53ca\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.19530", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19530", "abs": "https://arxiv.org/abs/2507.19530", "authors": ["Md Basit Azam", "Sarangthem Ibotombi Singh"], "title": "Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation", "comment": null, "summary": "Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)\nwhere hemodynamic instability can\n  rapidly progress to cardiovascular collapse. Current machine\n  learning (ML) approaches suffer from three limitations: lack of\n  external validation, absence of uncertainty quantification, and\n  inadequate data leakage prevention. This study presents the\n  first comprehensive framework with novel algorithmic leakage\n  prevention, uncertainty quantification, and cross-institutional\n  validation for electronic health records (EHRs) based BP pre dictions. Our\nmethodology implemented systematic data leakage\n  prevention, uncertainty quantification through quantile regres sion, and\nexternal validation between the MIMIC-III and eICU\n  databases. An ensemble framework combines Gradient Boosting,\n  Random Forest, and XGBoost with 74 features across five\n  physiological domains. Internal validation achieved a clinically\n  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03\n  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI\n  standards. External validation showed 30% degradation with\n  critical limitations in patients with hypotensive. Uncertainty\n  quantification generated valid prediction intervals (80.3% SBP\n  and 79.9% DBP coverage), enabling risk-stratified protocols\n  with narrow intervals (< 15 mmHg) for standard monitoring\n  and wide intervals (> 30 mmHg) for manual verification. This\n  framework provides realistic deployment expectations for cross institutional\nAI-assisted BP monitoring in critical care settings.\n  The source code is publicly available at https://github.com/\n  mdbasit897/clinical-bp-prediction-ehr.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u8840\u538b\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5916\u90e8\u9a8c\u8bc1\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6570\u636e\u6cc4\u6f0f\u9884\u9632\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u91cd\u75c7\u76d1\u62a4\u75c5\u623f\uff08ICU\uff09\u4e2d\u8840\u538b\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5916\u90e8\u9a8c\u8bc1\u4e0d\u8db3\u3001\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6570\u636e\u6cc4\u6f0f\u9884\u9632\u7b49\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7cfb\u7edf\u6027\u6570\u636e\u6cc4\u6f0f\u9884\u9632\u3001\u5206\u4f4d\u6570\u56de\u5f52\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u5728MIMIC-III\u548ceICU\u6570\u636e\u5e93\u95f4\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u3002\u7ed3\u5408\u68af\u5ea6\u63d0\u5347\u3001\u968f\u673a\u68ee\u6797\u548cXGBoost\u7684\u96c6\u6210\u6846\u67b6\uff0c\u6db5\u76d6\u4e94\u4e2a\u751f\u7406\u9886\u57df\u768474\u4e2a\u7279\u5f81\u3002", "result": "\u5185\u90e8\u9a8c\u8bc1\u8868\u73b0\u826f\u597d\uff08SBP\uff1aR\u00b2=0.86\uff0cRMSE=6.03 mmHg\uff1bDBP\uff1aR\u00b2=0.49\uff0cRMSE=7.13 mmHg\uff09\uff0c\u5916\u90e8\u9a8c\u8bc1\u663e\u793a\u6027\u80fd\u4e0b\u964d30%\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u751f\u6210\u4e86\u6709\u6548\u7684\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8de8\u673a\u6784AI\u8f85\u52a9\u8840\u538b\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u9884\u671f\uff0c\u5e76\u516c\u5f00\u4e86\u6e90\u4ee3\u7801\u3002"}}
{"id": "2507.20648", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20648", "abs": "https://arxiv.org/abs/2507.20648", "authors": ["Christos Ntemkas", "Antonios Argyriou"], "title": "RFI and Jamming Detection in Antenna Arrays with an LSTM Autoencoder", "comment": null, "summary": "Radio frequency interference (RFI) and malicious jammers are a significant\nproblem in our wireless world. Detecting RFI or jamming is typically performed\nwith model-based statistical detection or AI-empowered algorithms that use an\ninput baseband data or time-frequency representations like spectrograms. In\nthis work we depart from the previous approaches and we leverage data in\nantenna array systems. We use Fourier imaging to localize spatially the sources\nand then deploy a deep LSTM autoencoder that detects RFI and jamming as\nanomalies. Our results for different power levels of the RFI/jamming sources,\nand the signal of interest, reveal that our detector offers high performance\nwithout needing any pre-existing knowledge regarding the RFI or jamming signal.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5929\u7ebf\u9635\u5217\u6570\u636e\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5c04\u9891\u5e72\u6270\uff08RFI\uff09\u548c\u6076\u610f\u5e72\u6270\uff0c\u65e0\u9700\u9884\u77e5\u5e72\u6270\u4fe1\u53f7\u4fe1\u606f\u3002", "motivation": "\u5c04\u9891\u5e72\u6270\u548c\u6076\u610f\u5e72\u6270\u662f\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u4e3b\u8981\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7edf\u8ba1\u68c0\u6d4b\u6216\u57fa\u4e8eAI\u7684\u7b97\u6cd5\uff0c\u4f46\u9700\u8981\u8f93\u5165\u57fa\u5e26\u6570\u636e\u6216\u65f6\u9891\u8868\u793a\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u5929\u7ebf\u9635\u5217\u6570\u636e\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u5085\u91cc\u53f6\u6210\u50cf\u8fdb\u884c\u7a7a\u95f4\u6e90\u5b9a\u4f4d\uff0c\u5e76\u91c7\u7528\u6df1\u5ea6LSTM\u81ea\u7f16\u7801\u5668\u68c0\u6d4b\u5f02\u5e38\uff08RFI\u548c\u5e72\u6270\uff09\u3002", "result": "\u5728\u4e0d\u540c\u529f\u7387\u6c34\u5e73\u7684RFI/\u5e72\u6270\u548c\u4fe1\u53f7\u6761\u4ef6\u4e0b\uff0c\u68c0\u6d4b\u5668\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u9884\u77e5\u5e72\u6270\u4fe1\u53f7\u4fe1\u606f\uff0c\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.20268", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20268", "abs": "https://arxiv.org/abs/2507.20268", "authors": ["Seonghoon Yoo", "Houssem Sifaou", "Sangwoo Park", "Joonhyuk Kang", "Osvaldo Simeone"], "title": "Data-Efficient Prediction-Powered Calibration via Cross-Validation", "comment": null, "summary": "Calibration data are necessary to formally quantify the uncertainty of the\ndecisions produced by an existing artificial intelligence (AI) model. To\novercome the common issue of scarce calibration data, a promising approach is\nto employ synthetic labels produced by a (generally different) predictive\nmodel. However, fine-tuning the label-generating predictor on the inference\ntask of interest, as well as estimating the residual bias of the synthetic\nlabels, demand additional data, potentially exacerbating the calibration data\nscarcity problem. This paper introduces a novel approach that efficiently\nutilizes limited calibration data to simultaneously fine-tune a predictor and\nestimate the bias of the synthetic labels. The proposed method yields\nprediction sets with rigorous coverage guarantees for AI-generated decisions.\nExperimental results on an indoor localization problem validate the\neffectiveness and performance gains of our solution.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u6709\u9650\u6821\u51c6\u6570\u636e\u540c\u65f6\u5fae\u8c03\u9884\u6d4b\u5668\u5e76\u4f30\u8ba1\u5408\u6210\u6807\u7b7e\u7684\u504f\u5dee\uff0c\u4ee5\u89e3\u51b3\u6821\u51c6\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3AI\u6a21\u578b\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u6821\u51c6\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u540c\u65f6\u5fae\u8c03\u9884\u6d4b\u5668\u548c\u4f30\u8ba1\u5408\u6210\u6807\u7b7e\u504f\u5dee\uff0c\u9ad8\u6548\u5229\u7528\u6709\u9650\u6821\u51c6\u6570\u636e\u3002", "result": "\u5728\u5ba4\u5185\u5b9a\u4f4d\u95ee\u9898\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aAI\u751f\u6210\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8986\u76d6\u4fdd\u8bc1\u3002"}}
{"id": "2507.19534", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2; I.7"], "pdf": "https://arxiv.org/pdf/2507.19534", "abs": "https://arxiv.org/abs/2507.19534", "authors": ["Ali Shakeri", "Wei Emma Zhang", "Amin Beheshti", "Weitong Chen", "Jian Yang", "Lishan Yang"], "title": "FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings", "comment": "12 pages; Published to PAKDD'2025", "summary": "Pre-trained Language Models (PLMs) have demonstrated impressive performance\nin various NLP tasks. However, traditional fine-tuning methods for leveraging\nPLMs for downstream tasks entail significant computational overhead.\nPrompt-tuning has emerged as an efficient alternative that involves prepending\na limited number of parameters to the input sequence and only updating them\nwhile the PLM's parameters are frozen. However, this technique's prompts remain\nfixed for all inputs, reducing the model's flexibility. The Federated Learning\n(FL) technique has gained attention in recent years to address the growing\nconcerns around data privacy. However, challenges such as communication and\ncomputation limitations of clients still need to be addressed. To mitigate\nthese challenges, this paper introduces the Federated Dynamic Prompt Generator\n(FedDPG), which incorporates a dynamic prompt generator network to generate\ncontext-aware prompts based on the given input, ensuring flexibility and\nadaptability while prioritising data privacy in federated learning settings.\nOur experiments on three NLP benchmark datasets showcase that FedDPG\noutperforms the state-of-the-art parameter-efficient fine-tuning methods in\nterms of global model performance, and has significantly reduced the\ncalculation time and the number of parameters to be sent through the FL\nnetwork.", "AI": {"tldr": "FedDPG\u662f\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u63d0\u793a\u751f\u6210\u548c\u8054\u90a6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u7075\u6d3b\u6027\u5e76\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u4e14\u8054\u90a6\u5b66\u4e60\u4e2d\u5b58\u5728\u8ba1\u7b97\u548c\u901a\u4fe1\u9650\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51faFedDPG\uff0c\u901a\u8fc7\u52a8\u6001\u63d0\u793a\u751f\u6210\u7f51\u7edc\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\uff0c\u5e76\u5728\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2aNLP\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cFedDPG\u4f18\u4e8e\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u548c\u901a\u4fe1\u53c2\u6570\u3002", "conclusion": "FedDPG\u5728\u4fdd\u6301\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.20651", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20651", "abs": "https://arxiv.org/abs/2507.20651", "authors": ["Jichao Zhang", "Xiao-Lei Zhang", "Kunde Yang"], "title": "Angle-distance decomposition based on deep learning for active sonar detection", "comment": null, "summary": "Underwater target detection using active sonar constitutes a critical\nresearch area in marine sciences and engineering. However, traditional signal\nprocessing methods face significant challenges in complex underwater\nenvironments due to noise, reverberation, and interference. To address these\nissues, this paper presents a deep learning-based active sonar target detection\nmethod that decomposes the detection process into separate angle and distance\nestimation tasks. Active sonar target detection employs deep learning models to\npredict target distance and angle, with the final target position determined by\nintegrating these estimates. Limited underwater acoustic data hinders effective\nmodel training, but transfer learning and simulation offer practical solutions\nto this challenge. Experimental results verify that the method achieves\neffective and robust performance under challenging conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e3b\u52a8\u58f0\u7eb3\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u89d2\u5ea6\u548c\u8ddd\u79bb\u4f30\u8ba1\u4efb\u52a1\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548c\u4eff\u771f\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u56e0\u566a\u58f0\u3001\u6df7\u54cd\u548c\u5e72\u6270\u9762\u4e34\u6311\u6218\uff0c\u9700\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u522b\u9884\u6d4b\u76ee\u6807\u8ddd\u79bb\u548c\u89d2\u5ea6\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548c\u4eff\u771f\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u5177\u6709\u6709\u6548\u4e14\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u4e3b\u52a8\u58f0\u7eb3\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.20272", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20272", "abs": "https://arxiv.org/abs/2507.20272", "authors": ["Dharmesh Tailor", "Alvaro H. C. Correia", "Eric Nalisnick", "Christos Louizos"], "title": "Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence", "comment": "Accepted at the 13th International Conference on Learning\n  Representations (ICLR 2025)", "summary": "Uncertainty quantification is an important prerequisite for the deployment of\ndeep learning models in safety-critical areas. Yet, this hinges on the\nuncertainty estimates being useful to the extent the prediction intervals are\nwell-calibrated and sharp. In the absence of inherent uncertainty estimates\n(e.g. pretrained models predicting only point estimates), popular approaches\nthat operate post-hoc include Laplace's method and split conformal prediction\n(split-CP). However, Laplace's method can be miscalibrated when the model is\nmisspecified and split-CP requires sample splitting, and thus comes at the\nexpense of statistical efficiency. In this work, we construct prediction\nintervals for neural network regressors post-hoc without held-out data. This is\nachieved by approximating the full conformal prediction method (full-CP).\nWhilst full-CP nominally requires retraining the model for every test point and\ncandidate label, we propose to train just once and locally perturb model\nparameters using Gauss-Newton influence to approximate the effect of\nretraining. Coupled with linearization of the network, we express the absolute\nresidual nonconformity score as a piecewise linear function of the candidate\nlabel allowing for an efficient procedure that avoids the exhaustive search\nover the output space. On standard regression benchmarks and bounding box\nlocalization, we show the resulting prediction intervals are locally-adaptive\nand often tighter than those of split-CP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4fdd\u7559\u6570\u636e\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u5168\u5171\u5f62\u9884\u6d4b\uff08full-CP\uff09\u4e3a\u795e\u7ecf\u7f51\u7edc\u56de\u5f52\u5668\u6784\u5efa\u9884\u6d4b\u533a\u95f4\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5982Laplace\u65b9\u6cd5\u548csplit-CP\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982Laplace\u65b9\u6cd5\u53ef\u80fd\u6821\u51c6\u4e0d\u4f73\uff0c\u800csplit-CP\u9700\u8981\u6837\u672c\u5206\u5272\uff0c\u727a\u7272\u7edf\u8ba1\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5c40\u90e8\u6270\u52a8\u6a21\u578b\u53c2\u6570\uff08\u4f7f\u7528\u9ad8\u65af-\u725b\u987f\u5f71\u54cd\uff09\u548c\u7f51\u7edc\u7ebf\u6027\u5316\uff0c\u8fd1\u4f3c\u5168\u5171\u5f62\u9884\u6d4b\uff0c\u907f\u514d\u4e86\u9010\u70b9\u91cd\u65b0\u8bad\u7ec3\u548c\u8f93\u51fa\u7a7a\u95f4\u7684\u7a77\u4e3e\u641c\u7d22\u3002", "result": "\u5728\u6807\u51c6\u56de\u5f52\u57fa\u51c6\u548c\u8fb9\u754c\u6846\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0c\u751f\u6210\u7684\u9884\u6d4b\u533a\u95f4\u5177\u6709\u5c40\u90e8\u9002\u5e94\u6027\uff0c\u4e14\u901a\u5e38\u6bd4split-CP\u66f4\u7d27\u51d1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u540e\u5904\u7406\u573a\u666f\u3002"}}
{"id": "2507.19536", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19536", "abs": "https://arxiv.org/abs/2507.19536", "authors": ["K. -C. Ouyang", "S. -Y. Zhang", "S. -L. Liu", "J. Tian", "Y. -H. Li", "H. Tong", "H. -Y. Bai", "W. -H. Wang", "Y. -C. Hu"], "title": "Graph Learning Metallic Glass Discovery from Wikipedia", "comment": "7 figures", "summary": "Synthesizing new materials efficiently is highly demanded in various research\nfields. However, this process is usually slow and expensive, especially for\nmetallic glasses, whose formation strongly depends on the optimal combinations\nof multiple elements to resist crystallization. This constraint renders only\nseveral thousands of candidates explored in the vast material space since 1960.\nRecently, data-driven approaches armed by advanced machine learning techniques\nprovided alternative routes for intelligent materials design. Due to data\nscarcity and immature material encoding, the conventional tabular data is\nusually mined by statistical learning algorithms, giving limited model\npredictability and generalizability. Here, we propose sophisticated data\nlearning from material network representations. The node elements are encoded\nfrom the Wikipedia by a language model. Graph neural networks with versatile\narchitectures are designed to serve as recommendation systems to explore hidden\nrelationships among materials. By employing Wikipedia embeddings from different\nlanguages, we assess the capability of natural languages in materials design.\nOur study proposes a new paradigm to harvesting new amorphous materials and\nbeyond with artificial intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6750\u6599\u7f51\u7edc\u8868\u793a\u548c\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5408\u6210\u65b0\u6750\u6599\uff0c\u7279\u522b\u662f\u91d1\u5c5e\u73bb\u7483\u3002", "motivation": "\u4f20\u7edf\u6750\u6599\u5408\u6210\u8fc7\u7a0b\u7f13\u6162\u4e14\u6602\u8d35\uff0c\u5c24\u5176\u662f\u91d1\u5c5e\u73bb\u7483\u7684\u5f62\u6210\u9700\u8981\u591a\u5143\u7d20\u4f18\u5316\u7ec4\u5408\uff0c\u6570\u636e\u7a00\u7f3a\u548c\u6750\u6599\u7f16\u7801\u4e0d\u6210\u719f\u9650\u5236\u4e86\u7edf\u8ba1\u5b66\u4e60\u7b97\u6cd5\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u5229\u7528\u8bed\u8a00\u6a21\u578b\u4ece\u7ef4\u57fa\u767e\u79d1\u7f16\u7801\u8282\u70b9\u5143\u7d20\uff0c\u8bbe\u8ba1\u591a\u79cd\u67b6\u6784\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u63a8\u8350\u7cfb\u7edf\uff0c\u63a2\u7d22\u6750\u6599\u95f4\u7684\u9690\u85cf\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u591a\u8bed\u8a00\u7ef4\u57fa\u767e\u79d1\u5d4c\u5165\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u5728\u6750\u6599\u8bbe\u8ba1\u4e2d\u7684\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u65b0\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5229\u7528\u4eba\u5de5\u667a\u80fd\u63a2\u7d22\u65b0\u578b\u975e\u6676\u6750\u6599\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.20657", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20657", "abs": "https://arxiv.org/abs/2507.20657", "authors": ["Margarita Loupa", "Antonios Argyriou", "Yanwei Liu"], "title": "The micro-Doppler Attack Against AI-based Human Activity Classification from Wireless Signals", "comment": null, "summary": "A subset of Human Activity Classification (HAC) systems are based on AI\nalgorithms that use passively collected wireless signals. This paper presents\nthe micro-Doppler attack targeting HAC from wireless orthogonal frequency\ndivision multiplexing (OFDM) signals. The attack is executed by inserting\nartificial variations in a transmitted OFDM waveform to alter its micro-Doppler\nsignature when it reflects off a human target. We investigate two variants of\nour scheme that manipulate the waveform at different time scales resulting in\naltered receiver spectrograms. HAC accuracy with a deep convolutional neural\nnetwork (CNN) can be reduced to less than 10%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u57fa\u4e8e\u65e0\u7ebfOFDM\u4fe1\u53f7\u7684\u4eba\u7c7b\u6d3b\u52a8\u5206\u7c7b\u7cfb\u7edf\u7684\u5fae\u591a\u666e\u52d2\u653b\u51fb\uff0c\u901a\u8fc7\u4eba\u4e3a\u6539\u53d8\u4fe1\u53f7\u6ce2\u5f62\u964d\u4f4e\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u9488\u5bf9\u65e0\u7ebf\u4fe1\u53f7\u7684\u4eba\u7c7b\u6d3b\u52a8\u5206\u7c7b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u63ed\u793a\u5176\u6f5c\u5728\u7684\u8106\u5f31\u6027\u3002", "method": "\u901a\u8fc7\u63d2\u5165\u4eba\u4e3a\u53d8\u5316\u7684OFDM\u6ce2\u5f62\uff0c\u6539\u53d8\u5176\u5fae\u591a\u666e\u52d2\u7279\u5f81\uff0c\u5f71\u54cd\u63a5\u6536\u5668\u7684\u9891\u8c31\u56fe\u3002", "result": "\u4f7f\u7528\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684HAC\u7cfb\u7edf\u51c6\u786e\u6027\u53ef\u964d\u81f310%\u4ee5\u4e0b\u3002", "conclusion": "\u5fae\u591a\u666e\u52d2\u653b\u51fb\u5bf9\u4eba\u7c7b\u6d3b\u52a8\u5206\u7c7b\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u52a0\u5f3a\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2507.20459", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.ME", "stat.ML", "stat.TH", "62F12, 62H30, 15A69, 65Y20"], "pdf": "https://arxiv.org/pdf/2507.20459", "abs": "https://arxiv.org/abs/2507.20459", "authors": ["Liu Zhang", "Oscar Mickelin", "Sheng Xu", "Amit Singer"], "title": "Diagonally-Weighted Generalized Method of Moments Estimation for Gaussian Mixture Modeling", "comment": null, "summary": "Since Pearson [Philosophical Transactions of the Royal Society of London. A,\n185 (1894), pp. 71-110] first applied the method of moments (MM) for modeling\ndata as a mixture of one-dimensional Gaussians, moment-based estimation methods\nhave proliferated. Among these methods, the generalized method of moments (GMM)\nimproves the statistical efficiency of MM by weighting the moments\nappropriately. However, the computational complexity and storage complexity of\nMM and GMM grow exponentially with the dimension, making these methods\nimpractical for high-dimensional data or when higher-order moments are\nrequired. Such computational bottlenecks are more severe in GMM since it\nadditionally requires estimating a large weighting matrix. To overcome these\nbottlenecks, we propose the diagonally-weighted GMM (DGMM), which achieves a\nbalance among statistical efficiency, computational complexity, and numerical\nstability. We apply DGMM to study the parameter estimation problem for weakly\nseparated heteroscedastic low-rank Gaussian mixtures and design a\ncomputationally efficient and numerically stable algorithm that obtains the\nDGMM estimator without explicitly computing or storing the moment tensors. We\nimplement the proposed algorithm and empirically validate the advantages of\nDGMM: in numerical studies, DGMM attains smaller estimation errors while\nrequiring substantially shorter runtime than MM and GMM. The code and data will\nbe available upon publication at https://github.com/liu-lzhang/dgmm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u89d2\u7ebf\u52a0\u6743\u5e7f\u4e49\u77e9\u65b9\u6cd5\uff08DGMM\uff09\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u4e2d\u4f20\u7edf\u77e9\u65b9\u6cd5\uff08MM\u548cGMM\uff09\u8ba1\u7b97\u548c\u5b58\u50a8\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5f31\u5206\u79bb\u5f02\u65b9\u5dee\u4f4e\u79e9\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u77e9\u65b9\u6cd5\uff08MM\u548cGMM\uff09\u5728\u9ad8\u7ef4\u6570\u636e\u6216\u9700\u8981\u9ad8\u9636\u77e9\u65f6\u8ba1\u7b97\u548c\u5b58\u50a8\u590d\u6742\u5ea6\u5448\u6307\u6570\u589e\u957f\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002GMM\u8fd8\u9700\u4f30\u8ba1\u5927\u578b\u52a0\u6743\u77e9\u9635\uff0c\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u5bf9\u89d2\u7ebf\u52a0\u6743GMM\uff08DGMM\uff09\uff0c\u901a\u8fc7\u5408\u7406\u52a0\u6743\u77e9\uff0c\u5e73\u8861\u7edf\u8ba1\u6548\u7387\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6570\u503c\u7a33\u5b9a\u6027\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u65e0\u9700\u663e\u5f0f\u8ba1\u7b97\u6216\u5b58\u50a8\u77e9\u5f20\u91cf\u3002", "result": "\u6570\u503c\u7814\u7a76\u8868\u660e\uff0cDGMM\u5728\u4f30\u8ba1\u8bef\u5dee\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u5747\u4f18\u4e8eMM\u548cGMM\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "DGMM\u662f\u4e00\u79cd\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u77e9\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5f31\u5206\u79bb\u5f02\u65b9\u5dee\u4f4e\u79e9\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u3002"}}
{"id": "2507.20664", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20664", "abs": "https://arxiv.org/abs/2507.20664", "authors": ["Kohei Shimomura", "Chi-Hsuan Lee", "Takuya Sakamoto"], "title": "A Nonlinear Spectral Approach for Radar-Based Heartbeat Estimation via Autocorrelation of Higher Harmonics", "comment": "4 pages, 4 figures, 3 tables. This work is going to be submitted to\n  the IEEE for possible publication", "summary": "This study presents a nonlinear signal processing method for accurate\nradar-based heartbeat interval estimation by exploiting the periodicity of\nhigher-order harmonics inherent in heartbeat signals. Unlike conventional\napproaches that employ selective frequency filtering or track individual\nharmonics, the proposed method enhances the global periodic structure of the\nspectrum via nonlinear correlation processing. Specifically, smoothing and\nsecond-derivative operations are first applied to the radar displacement signal\nto suppress noise and accentuate higher-order heartbeat harmonics. Rather than\nisolating specific frequency components, we compute localized autocorrelations\nof the Fourier spectrum around the harmonic frequencies. The incoherent\nsummation of these autocorrelations yields a pseudo-spectrum in which the\nfundamental heartbeat periodicity is distinctly emphasized. This nonlinear\napproach mitigates the effects of respiratory harmonics and noise, enabling\nrobust interbeat interval estimation. Experiments with radar measurements from\nfive participants demonstrate that the proposed method reduces root-mean-square\nerror by 20% and improves the correlation coefficient by 0.20 relative to\nconventional techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u975e\u7ebf\u6027\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5fc3\u8df3\u4fe1\u53f7\u7684\u9ad8\u9636\u8c10\u6ce2\u5468\u671f\u6027\uff0c\u63d0\u9ad8\u96f7\u8fbe\u5fc3\u8df3\u95f4\u9694\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u9009\u62e9\u6027\u9891\u7387\u6ee4\u6ce2\u6216\u8ddf\u8e2a\u5355\u4e2a\u8c10\u6ce2\uff0c\u4f46\u96be\u4ee5\u6709\u6548\u6291\u5236\u547c\u5438\u8c10\u6ce2\u548c\u566a\u58f0\u3002", "method": "\u901a\u8fc7\u5e73\u6ed1\u548c\u4e8c\u9636\u5bfc\u6570\u64cd\u4f5c\u6291\u5236\u566a\u58f0\u5e76\u7a81\u51fa\u9ad8\u9636\u8c10\u6ce2\uff0c\u8ba1\u7b97\u8c10\u6ce2\u9891\u7387\u9644\u8fd1\u7684\u5c40\u90e8\u81ea\u76f8\u5173\uff0c\u5e76\u901a\u8fc7\u975e\u76f8\u5e72\u6c42\u548c\u751f\u6210\u4f2a\u9891\u8c31\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u5747\u65b9\u6839\u8bef\u5dee\u964d\u4f4e20%\uff0c\u76f8\u5173\u7cfb\u6570\u63d0\u9ad80.20\u3002", "conclusion": "\u975e\u7ebf\u6027\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u8df3\u95f4\u9694\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.20542", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20542", "abs": "https://arxiv.org/abs/2507.20542", "authors": ["Dawon Ahn", "Jun-Gi Jang", "Evangelos E. Papalexakis"], "title": "Improving Group Fairness in Tensor Completion via Imbalance Mitigating Entity Augmentation", "comment": null, "summary": "Group fairness is important to consider in tensor decomposition to prevent\ndiscrimination based on social grounds such as gender or age. Although few\nworks have studied group fairness in tensor decomposition, they suffer from\nperformance degradation. To address this, we propose STAFF(Sparse Tensor\nAugmentation For Fairness) to improve group fairness by minimizing the gap in\ncompletion errors of different groups while reducing the overall tensor\ncompletion error. Our main idea is to augment a tensor with augmented entities\nincluding sufficient observed entries to mitigate imbalance and group bias in\nthe sparse tensor. We evaluate \\method on tensor completion with various\ndatasets under conventional and deep learning-based tensor models. STAFF\nconsistently shows the best trade-off between completion error and group\nfairness; at most, it yields 36% lower MSE and 59% lower MADE than the\nsecond-best baseline.", "AI": {"tldr": "STAFF\u65b9\u6cd5\u901a\u8fc7\u7a00\u758f\u5f20\u91cf\u589e\u5f3a\u63d0\u5347\u7ec4\u516c\u5e73\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u6574\u4f53\u5f20\u91cf\u8865\u5168\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\u5728\u7ec4\u516c\u5e73\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faSTAFF\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u5f20\u91cf\u5b9e\u4f53\u4ee5\u51cf\u5c11\u4e0d\u5e73\u8861\u548c\u7ec4\u504f\u89c1\uff0c\u4f18\u5316\u8865\u5168\u8bef\u5dee\u548c\u516c\u5e73\u6027\u3002", "result": "STAFF\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0b\u8868\u73b0\u6700\u4f73\uff0cMSE\u548cMADE\u5206\u522b\u964d\u4f4e36%\u548c59%\u3002", "conclusion": "STAFF\u5728\u5f20\u91cf\u8865\u5168\u4e2d\u5b9e\u73b0\u4e86\u516c\u5e73\u6027\u4e0e\u6027\u80fd\u7684\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2507.19547", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.19547", "abs": "https://arxiv.org/abs/2507.19547", "authors": ["Pablo Peiro-Corbacho", "Long Lin", "Pablo \u00c1vila", "Alejandro Carta-Bergaz", "\u00c1ngel Arenal", "Carlos Sevilla-Salcedo", "Gonzalo R. R\u00edos-Mu\u00f1oz"], "title": "Latent Representations of Intracardiac Electrograms for Atrial Fibrillation Driver Detection", "comment": null, "summary": "Atrial Fibrillation (AF) is the most prevalent sustained arrhythmia, yet\ncurrent ablation therapies, including pulmonary vein isolation, are frequently\nineffective in persistent AF due to the involvement of non-pulmonary vein\ndrivers. This study proposes a deep learning framework using convolutional\nautoencoders for unsupervised feature extraction from unipolar and bipolar\nintracavitary electrograms (EGMs) recorded during AF in ablation studies. These\nlatent representations of atrial electrical activity enable the\ncharacterization and automation of EGM analysis, facilitating the detection of\nAF drivers.\n  The database consisted of 11,404 acquisitions recorded from 291 patients,\ncontaining 228,080 unipolar EGMs and 171,060 bipolar EGMs. The autoencoders\nsuccessfully learned latent representations with low reconstruction loss,\npreserving the morphological features. The extracted embeddings allowed\ndownstream classifiers to detect rotational and focal activity with moderate\nperformance (AUC 0.73-0.76) and achieved high discriminative performance in\nidentifying atrial EGM entanglement (AUC 0.93).\n  The proposed method can operate in real-time and enables integration into\nclinical electroanatomical mapping systems to assist in identifying\narrhythmogenic regions during ablation procedures. This work highlights the\npotential of unsupervised learning to uncover physiologically meaningful\nfeatures from intracardiac signals.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u81ea\u7f16\u7801\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5fc3\u623f\u98a4\u52a8\uff08AF\uff09\u7684\u8154\u5185\u7535\u56fe\uff08EGMs\uff09\u4e2d\u65e0\u76d1\u7763\u63d0\u53d6\u7279\u5f81\uff0c\u4ee5\u5e2e\u52a9\u68c0\u6d4bAF\u9a71\u52a8\u6e90\u3002", "motivation": "\u5f53\u524d\u6d88\u878d\u7597\u6cd5\u5bf9\u6301\u7eed\u6027AF\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u975e\u80ba\u9759\u8109\u9a71\u52a8\u6e90\u7684\u53c2\u4e0e\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u5316EGM\u5206\u6790\uff0c\u63d0\u9ad8AF\u9a71\u52a8\u6e90\u7684\u68c0\u6d4b\u6548\u7387\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u81ea\u7f16\u7801\u5668\u4ece\u5355\u6781\u548c\u53cc\u6781EGMs\u4e2d\u63d0\u53d6\u6f5c\u5728\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4e0b\u6e38\u5206\u7c7b\u5668\u68c0\u6d4b\u65cb\u8f6c\u548c\u7126\u70b9\u6d3b\u52a8\u3002", "result": "\u81ea\u7f16\u7801\u5668\u6210\u529f\u63d0\u53d6\u4e86\u4f4e\u91cd\u6784\u635f\u5931\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u5206\u7c7b\u5668\u5728\u68c0\u6d4b\u65cb\u8f6c\u548c\u7126\u70b9\u6d3b\u52a8\u65f6\u8868\u73b0\u4e2d\u7b49\uff08AUC 0.73-0.76\uff09\uff0c\u5728\u8bc6\u522b\u5fc3\u623fEGM\u7ea0\u7f20\u65f6\u8868\u73b0\u4f18\u5f02\uff08AUC 0.93\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5b9e\u65f6\u8fd0\u884c\u5e76\u96c6\u6210\u5230\u4e34\u5e8a\u7535\u89e3\u5256\u6807\u6d4b\u7cfb\u7edf\u4e2d\uff0c\u5c55\u793a\u4e86\u65e0\u76d1\u7763\u5b66\u4e60\u5728\u63d0\u53d6\u5fc3\u810f\u4fe1\u53f7\u751f\u7406\u7279\u5f81\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.20789", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20789", "abs": "https://arxiv.org/abs/2507.20789", "authors": ["Hung Nguyen-Kha", "Vu Nguyen Ha", "Ti Ti Nguyen", "Eva Lagunas", "Symeon Chatzinotas", "Joel Grotz"], "title": "DT-Aided Resource Management in Spectrum Sharing Integrated Satellite-Terrestrial Networks", "comment": null, "summary": "The integrated satellite-terrestrial networks (ISTNs) through spectrum\nsharing have emerged as a promising solution to improve spectral efficiency and\nmeet increasing wireless demand. However, this coexistence introduces\nsignificant challenges, including inter-system interference (ISI) and the low\nEarth orbit satellite (LSat) movements. To capture the actual environment for\nresource management, we propose a time-varying digital twin (DT)-aided\nframework for ISTNs incorporating 3D map that enables joint optimization of\nbandwidth (BW) allocation, traffic steering, and resource allocation, and aims\nto minimize congestion. The problem is formulated as a mixed-integer nonlinear\nprogramming (MINLP), addressed through a two-phase algorithm based on\nsuccessive convex approximation (SCA) and compressed sensing approaches.\nNumerical results demonstrate the proposed method's superior performance in\nqueue length minimization compared to benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u96c6\u6210\u536b\u661f-\u5730\u9762\u7f51\u7edc\uff08ISTN\uff09\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5e26\u5bbd\u5206\u914d\u3001\u6d41\u91cf\u5f15\u5bfc\u548c\u8d44\u6e90\u5206\u914d\u6765\u6700\u5c0f\u5316\u62e5\u585e\u3002", "motivation": "\u89e3\u51b3\u536b\u661f\u4e0e\u5730\u9762\u7f51\u7edc\u5171\u5b58\u65f6\u4ea7\u751f\u7684\u7cfb\u7edf\u95f4\u5e72\u6270\uff08ISI\uff09\u548c\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\uff08LSat\uff09\u79fb\u52a8\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u53d8\u5316\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u7ed3\u54083D\u5730\u56fe\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7b97\u6cd5\uff08\u57fa\u4e8e\u8fde\u7eed\u51f8\u8fd1\u4f3c\u548c\u538b\u7f29\u611f\u77e5\uff09\u89e3\u51b3\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\uff08MINLP\uff09\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u961f\u5217\u957f\u5ea6\u6700\u5c0f\u5316\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aISTN\u7684\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.20993", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20993", "abs": "https://arxiv.org/abs/2507.20993", "authors": ["Henri Arno", "Thomas Demeester"], "title": "Personalized Treatment Effect Estimation from Unstructured Data", "comment": null, "summary": "Existing methods for estimating personalized treatment effects typically rely\non structured covariates, limiting their applicability to unstructured data.\nYet, leveraging unstructured data for causal inference has considerable\napplication potential, for instance in healthcare, where clinical notes or\nmedical images are abundant. To this end, we first introduce an approximate\n'plug-in' method trained directly on the neural representations of unstructured\ndata. However, when these fail to capture all confounding information, the\nmethod may be subject to confounding bias. We therefore introduce two\ntheoretically grounded estimators that leverage structured measurements of the\nconfounders during training, but allow estimating personalized treatment\neffects purely from unstructured inputs, while avoiding confounding bias. When\nthese structured measurements are only available for a non-representative\nsubset of the data, these estimators may suffer from sampling bias. To address\nthis, we further introduce a regression-based correction that accounts for the\nnon-uniform sampling, assuming the sampling mechanism is known or can be\nwell-estimated. Our experiments on two benchmark datasets show that the plug-in\nmethod, directly trainable on large unstructured datasets, achieves strong\nempirical performance across all settings, despite its simplicity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u975e\u7ed3\u6784\u5316\u6570\u636e\u4f30\u8ba1\u4e2a\u6027\u5316\u6cbb\u7597\u6548\u679c\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u76f4\u63a5\u8bad\u7ec3\u7684\u8fd1\u4f3c'\u63d2\u4ef6'\u65b9\u6cd5\u548c\u4e24\u79cd\u7406\u8bba\u57fa\u7840\u7684\u4f30\u8ba1\u5668\uff0c\u89e3\u51b3\u4e86\u6df7\u6742\u504f\u5dee\u548c\u62bd\u6837\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7ed3\u6784\u5316\u534f\u53d8\u91cf\uff0c\u9650\u5236\u4e86\u5728\u975e\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u533b\u7597\u4e34\u5e8a\u8bb0\u5f55\u6216\u56fe\u50cf\uff09\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u8fd1\u4f3c'\u63d2\u4ef6'\u65b9\u6cd5\u76f4\u63a5\u8bad\u7ec3\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u795e\u7ecf\u8868\u793a\uff1b\u63d0\u51fa\u4e24\u79cd\u7406\u8bba\u57fa\u7840\u7684\u4f30\u8ba1\u5668\uff0c\u5229\u7528\u6df7\u6742\u56e0\u5b50\u7684\u7ed3\u6784\u5316\u6d4b\u91cf\u907f\u514d\u504f\u5dee\uff1b\u8fdb\u4e00\u6b65\u5f15\u5165\u56de\u5f52\u6821\u6b63\u89e3\u51b3\u62bd\u6837\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d2\u4ef6\u65b9\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u76f4\u63a5\u8bad\u7ec3\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u4e2a\u6027\u5316\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7684\u5e94\u7528\u8303\u56f4\uff0c\u89e3\u51b3\u4e86\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u7684\u6df7\u6742\u548c\u62bd\u6837\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2507.19561", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19561", "abs": "https://arxiv.org/abs/2507.19561", "authors": ["Roie Ezraty", "Menachem Stern", "Shmuel M. Rubinstein"], "title": "Harnessing intuitive local evolution rules for physical learning", "comment": "26 pages, 6 figures (with appendices). Submitted to Physical Review E", "summary": "Machine Learning, however popular and accessible, is computationally\nintensive and highly power-consuming, prompting interest in alternative\nphysical implementations of learning tasks. We introduce a training scheme for\nphysical systems that minimize power dissipation in which only boundary\nparameters (i.e. inputs and outputs) are externally controlled. Using this\nscheme, these Boundary-Enabled Adaptive State Tuning Systems (BEASTS) learn by\nexploiting local physical rules. Our scheme, BEASTAL (BEAST-Adaline), is the\nclosest analog of the Adaline algorithm for such systems. We demonstrate this\nautonomous learning in silico for regression and classification tasks. Our\napproach advances previous physical learning schemes by using intuitive, local\nevolution rules without requiring large-scale memory or complex internal\narchitectures. BEASTAL can perform any linear task, achieving best performance\nwhen the local evolution rule is non-linear.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7cfb\u7edf\u7684\u4f4e\u529f\u8017\u5b66\u4e60\u65b9\u6848BEASTAL\uff0c\u901a\u8fc7\u8fb9\u754c\u53c2\u6570\u63a7\u5236\u548c\u5c40\u90e8\u7269\u7406\u89c4\u5219\u5b9e\u73b0\u81ea\u4e3b\u5b66\u4e60\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u8ba1\u7b97\u5bc6\u96c6\u4e14\u529f\u8017\u9ad8\uff0c\u9700\u8981\u63a2\u7d22\u7269\u7406\u7cfb\u7edf\u5b9e\u73b0\u5b66\u4e60\u4efb\u52a1\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5f15\u5165BEASTAL\u8bad\u7ec3\u65b9\u6848\uff0c\u4ec5\u63a7\u5236\u8fb9\u754c\u53c2\u6570\uff0c\u5229\u7528\u5c40\u90e8\u7269\u7406\u89c4\u5219\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u81ea\u4e3b\u5b66\u4e60\u7684\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u7ebf\u6027\u4efb\u52a1\uff0c\u975e\u7ebf\u6027\u89c4\u5219\u4e0b\u6027\u80fd\u6700\u4f73\u3002", "conclusion": "BEASTAL\u65b9\u6848\u65e0\u9700\u5927\u89c4\u6a21\u5185\u5b58\u6216\u590d\u6742\u67b6\u6784\uff0c\u4e3a\u7269\u7406\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20825", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20825", "abs": "https://arxiv.org/abs/2507.20825", "authors": ["Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu"], "title": "Chirp-Permuted AFDM: A New Degree of Freedom for Next-Generation Versatile Waveform Design", "comment": null, "summary": "We present a novel multicarrier waveform, termed chirp-permuted affine\nfrequency division multiplexing (CP-AFDM), which introduces a unique\nchirp-permutation domain on top of the chirp subcarriers of the conventional\nAFDM. Rigorous analysis of the signal model and waveform properties, supported\nby numerical simulations, demonstrates that the proposed CP-AFDM preserves all\ncore characteristics of affine frequency division multiplexing (AFDM) -\nincluding robustness to doubly-dispersive channels, peak-to-average power ratio\n(PAPR), and full delay-Doppler representation - while further enhancing\nambiguity function resolution and peak-to-sidelobe ratio (PSLR) in the Doppler\ndomain. These improvements establish CP-AFDM as a highly attractive candidate\nfor emerging sixth generation (6G) use cases demanding both reliability and\nsensing-awareness. Moreover, by exploiting the vast degree of freedom in the\nchirp-permutation domain, two exemplary multifunctional applications are\nintroduced: an index modulation (IM) technique over the permutation domain\nwhich achieves significant spectral efficiency gains, and a physical-layer\nsecurity scheme that ensures practically perfect security through\npermutation-based keying, without requiring additional transmit energy or\nsignaling overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u8f7d\u6ce2\u6ce2\u5f62CP-AFDM\uff0c\u5728\u4f20\u7edfAFDM\u7684\u57fa\u7840\u4e0a\u5f15\u5165\u72ec\u7279\u7684chirp\u7f6e\u6362\u57df\uff0c\u63d0\u5347\u4e86\u591a\u666e\u52d2\u57df\u7684\u5206\u8fa8\u7387\u548c\u5cf0\u503c\u65c1\u74e3\u6bd4\uff0c\u9002\u7528\u4e8e6G\u5e94\u7528\u3002", "motivation": "\u4e3a\u6ee1\u8db36G\u5bf9\u53ef\u9760\u6027\u548c\u611f\u77e5\u80fd\u529b\u7684\u9700\u6c42\uff0c\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u591a\u8f7d\u6ce2\u6ce2\u5f62\uff0c\u4ee5\u589e\u5f3a\u6027\u80fd\u3002", "method": "\u5728\u4f20\u7edfAFDM\u7684chirp\u5b50\u8f7d\u6ce2\u4e0a\u5f15\u5165chirp\u7f6e\u6362\u57df\uff0c\u5206\u6790\u4fe1\u53f7\u6a21\u578b\u548c\u6ce2\u5f62\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u3002", "result": "CP-AFDM\u4fdd\u7559\u4e86AFDM\u7684\u6838\u5fc3\u7279\u6027\uff0c\u540c\u65f6\u5728\u591a\u666e\u52d2\u57df\u63d0\u5347\u4e86\u5206\u8fa8\u7387\u548c\u5cf0\u503c\u65c1\u74e3\u6bd4\uff0c\u5e76\u5c55\u793a\u4e86\u4e24\u79cd\u591a\u529f\u80fd\u5e94\u7528\u3002", "conclusion": "CP-AFDM\u662f\u4e00\u79cd\u9002\u7528\u4e8e6G\u7684\u9ad8\u6027\u80fd\u6ce2\u5f62\uff0c\u517c\u5177\u53ef\u9760\u6027\u548c\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u652f\u6301\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.21040", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21040", "abs": "https://arxiv.org/abs/2507.21040", "authors": ["Aditya Ravuri", "Neil D. Lawrence"], "title": "Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps: An Interpretation and Potential Improvements", "comment": "Initial version", "summary": "We propose a probabilistic interpretation of transformers as unrolled\ninference steps assuming a probabilistic Laplacian Eigenmaps model from the\nProbDR framework. Our derivation shows that at initialisation, transformers\nperform \"linear\" dimensionality reduction. We also show that within the\ntransformer block, a graph Laplacian term arises from our arguments, rather\nthan an attention matrix (which we interpret as an adjacency matrix). We\ndemonstrate that simply subtracting the identity from the attention matrix (and\nthereby taking a graph diffusion step) improves validation performance on a\nlanguage model and a simple vision transformer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06Transformer\u89e3\u91ca\u4e3a\u57fa\u4e8e\u6982\u7387\u62c9\u666e\u62c9\u65af\u7279\u5f81\u6620\u5c04\u6a21\u578b\u7684\u5c55\u5f00\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u8bc1\u660e\u521d\u59cb\u5316\u65f6Transformer\u6267\u884c\u7ebf\u6027\u964d\u7ef4\u3002\u6b64\u5916\uff0cTransformer\u5757\u4e2d\u51fa\u73b0\u7684\u56fe\u62c9\u666e\u62c9\u65af\u9879\u4f18\u4e8e\u6ce8\u610f\u529b\u77e9\u9635\uff08\u88ab\u89c6\u4e3a\u90bb\u63a5\u77e9\u9635\uff09\uff0c\u901a\u8fc7\u7b80\u5355\u8c03\u6574\u6ce8\u610f\u529b\u77e9\u9635\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22Transformer\u7684\u5e95\u5c42\u6570\u5b66\u673a\u5236\uff0c\u63ed\u793a\u5176\u4e0e\u6982\u7387\u6a21\u578b\u7684\u8054\u7cfb\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "method": "\u57fa\u4e8eProbDR\u6846\u67b6\uff0c\u5c06Transformer\u89e3\u91ca\u4e3a\u6982\u7387\u62c9\u666e\u62c9\u65af\u7279\u5f81\u6620\u5c04\u6a21\u578b\u7684\u5c55\u5f00\u63a8\u7406\u6b65\u9aa4\uff0c\u5206\u6790\u521d\u59cb\u5316\u884c\u4e3a\u53ca\u56fe\u62c9\u666e\u62c9\u65af\u9879\u7684\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8c03\u6574\u6ce8\u610f\u529b\u77e9\u9635\uff08\u51cf\u53bb\u5355\u4f4d\u77e9\u9635\uff09\u80fd\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u548c\u7b80\u5355\u89c6\u89c9Transformer\u7684\u9a8c\u8bc1\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u4e3aTransformer\u63d0\u4f9b\u4e86\u65b0\u7684\u6982\u7387\u89e3\u91ca\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u7b80\u5355\u8c03\u6574\u6ce8\u610f\u529b\u77e9\u9635\u4f18\u5316\u6027\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.20942", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20942", "abs": "https://arxiv.org/abs/2507.20942", "authors": ["Taewon Jeong", "Lucas Giroto", "Umut Utku Erdem", "Christian Karle", "Jiyeon Choi", "Thomas Zwick", "Benjamin Nuss"], "title": "Interference Analysis and Successive Interference Cancellation for Multistatic OFDM-based ISAC Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Multistatic integrated sensing and communications (ISAC) systems, which use\ndistributed transmitters and receivers, offer enhanced spatial coverage and\nsensing accuracy compared to stand-alone ISAC configurations. However, these\nsystems face challenges due to interference between co-existing ISAC nodes,\nespecially during simultaneous operation. In this paper, we analyze the impact\nof this mutual interference arising from the co-existence in a multistatic ISAC\nscenario, where a mono- and a bistatic ISAC system share the same spectral\nresources. We first classify differenct types of interference in the power\ndomain. Then, we discuss how the interference can affect both sensing and\ncommunications in terms of bit error rate (BER), error vector magnitude (EVM),\nand radar image under varied transmit power and RCS configurations through\nsimulations. Along with interfernce analysis, we propose a low-complexity\nsuccessive interference cancellation method that adaptively cancels either the\nmonostatic reflection or the bistatic line-of-sight signal based on a\nmonostatic radar image signal-to-interference-plus-noise ratio (SINR). The\nproposed framework is evaluated with both simulations and proof-of-concept\nmeasurements using an ISAC testbed with a radar echo generator for object\nemulation. The results have shown that the proposed method reduces BER and\nimproves EVM as well as radar image SINR across a wide range of SINR\nconditions. These results demonstrate that accurate component-wise cancellation\ncan be achieved with low computational overhead, making the method suitable for\npractical applications.", "AI": {"tldr": "\u591a\u9759\u6001\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u901a\u8fc7\u5206\u5e03\u5f0f\u6536\u53d1\u5668\u63d0\u5347\u7a7a\u95f4\u8986\u76d6\u548c\u611f\u77e5\u7cbe\u5ea6\uff0c\u4f46\u9762\u4e34\u5171\u5b58\u8282\u70b9\u95f4\u7684\u5e72\u6270\u95ee\u9898\u3002\u672c\u6587\u5206\u6790\u4e86\u5e72\u6270\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u5e72\u6270\u6d88\u9664\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u591a\u9759\u6001ISAC\u7cfb\u7edf\u5728\u5171\u5b58\u8282\u70b9\u95f4\u5b58\u5728\u5e72\u6270\uff0c\u5f71\u54cd\u611f\u77e5\u548c\u901a\u4fe1\u6027\u80fd\uff0c\u9700\u7814\u7a76\u5e72\u6270\u5206\u7c7b\u53ca\u6d88\u9664\u65b9\u6cd5\u3002", "method": "\u5206\u7c7b\u5e72\u6270\u7c7b\u578b\uff0c\u63d0\u51fa\u57fa\u4e8e\u5355\u9759\u6001\u96f7\u8fbe\u56fe\u50cf\u4fe1\u5e72\u566a\u6bd4\uff08SINR\uff09\u7684\u4f4e\u590d\u6742\u5ea6\u5e72\u6270\u6d88\u9664\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u964d\u4f4e\u4e86\u8bef\u7801\u7387\uff08BER\uff09\uff0c\u6539\u5584\u4e86\u8bef\u5dee\u5411\u91cf\u5e45\u5ea6\uff08EVM\uff09\u548c\u96f7\u8fbe\u56fe\u50cfSINR\u3002", "conclusion": "\u4f4e\u590d\u6742\u5ea6\u5e72\u6270\u6d88\u9664\u65b9\u6cd5\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u80fd\u6709\u6548\u63d0\u5347\u591a\u9759\u6001ISAC\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2507.19635", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.19635", "abs": "https://arxiv.org/abs/2507.19635", "authors": ["Zain Asgar", "Michelle Nguyen", "Sachin Katti"], "title": "Efficient and Scalable Agentic AI with Heterogeneous Systems", "comment": "Early access preprint", "summary": "AI agents are emerging as a dominant workload in a wide range of\napplications, promising to be the vehicle that delivers the promised benefits\nof AI to enterprises and consumers. Unlike conventional software or static\ninference, agentic workloads are dynamic and structurally complex. Often these\nagents are directed graphs of compute and IO operations that span multi-modal\ndata input and conversion), data processing and context gathering (e.g vector\nDB lookups), multiple LLM inferences, tool calls, etc. To scale AI agent usage,\nwe need efficient and scalable deployment and agent-serving infrastructure.\n  To tackle this challenge, in this paper, we present a system design for\ndynamic orchestration of AI agent workloads on heterogeneous compute\ninfrastructure spanning CPUs and accelerators, both from different vendors and\nacross different performance tiers within a single vendor. The system delivers\nseveral building blocks: a framework for planning and optimizing agentic AI\nexecution graphs using cost models that account for compute, memory, and\nbandwidth constraints of different HW; a MLIR based representation and\ncompilation system that can decompose AI agent execution graphs into granular\noperators and generate code for different HW options; and a dynamic\norchestration system that can place the granular components across a\nheterogeneous compute infrastructure and stitch them together while meeting an\nend-to-end SLA. Our design performs a systems level TCO optimization and\npreliminary results show that leveraging a heterogeneous infrastructure can\ndeliver significant TCO benefits. A preliminary surprising finding is that for\nsome workloads a heterogeneous combination of older generation GPUs with newer\naccelerators can deliver similar TCO as the latest generation homogenous GPU\ninfrastructure design, potentially extending the life of deployed\ninfrastructure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u7f16\u6392AI\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u65e8\u5728\u4f18\u5316\u5f02\u6784\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u4e0a\u7684\u6267\u884c\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u603b\u62e5\u6709\u6210\u672c\uff08TCO\uff09\u3002", "motivation": "AI\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u5177\u6709\u52a8\u6001\u6027\u548c\u7ed3\u6784\u590d\u6742\u6027\uff0c\u9700\u8981\u9ad8\u6548\u7684\u90e8\u7f72\u548c\u670d\u52a1\u57fa\u7840\u8bbe\u65bd\u4ee5\u652f\u6301\u5176\u89c4\u6a21\u5316\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u5305\u62ec\u6267\u884c\u56fe\u4f18\u5316\u6846\u67b6\u3001\u57fa\u4e8eMLIR\u7684\u8868\u793a\u548c\u7f16\u8bd1\u7cfb\u7edf\uff0c\u4ee5\u53ca\u52a8\u6001\u7f16\u6392\u7cfb\u7edf\uff0c\u652f\u6301\u5f02\u6784\u786c\u4ef6\u4e0a\u7684\u4efb\u52a1\u5206\u89e3\u4e0e\u8c03\u5ea6\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u80fd\u663e\u8457\u964d\u4f4eTCO\uff0c\u751a\u81f3\u67d0\u4e9b\u60c5\u51b5\u4e0b\u65e7GPU\u4e0e\u65b0\u52a0\u901f\u5668\u7684\u7ec4\u5408\u6027\u80fd\u4e0e\u6700\u65b0\u540c\u6784GPU\u76f8\u5f53\u3002", "conclusion": "\u5f02\u6784\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u53ef\u6709\u6548\u4f18\u5316AI\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u90e8\u7f72\u6210\u672c\uff0c\u5ef6\u957f\u73b0\u6709\u786c\u4ef6\u7684\u4f7f\u7528\u5bff\u547d\u3002"}}
{"id": "2507.20952", "categories": ["eess.SP", "94C30", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.20952", "abs": "https://arxiv.org/abs/2507.20952", "authors": ["Jimmy Fernandez Landivar", "Andrea Zanella", "Ihsane Gryech", "Sofie Pollin", "Hazem Sallouha"], "title": "Analytical Modeling of Batteryless IoT Sensors Powered by Ambient Energy Harvesting", "comment": "6 pages, 6 figures, 1 table, accepted for publication in the 36th\n  IEEE International Symposium on Personal, Indoor and Mobile Radio\n  Communications (PIMRC 2025), Istanbul, T\\\"urkiye", "summary": "This paper presents a comprehensive mathematical model to characterize the\nenergy dynamics of batteryless IoT sensor nodes powered entirely by ambient\nenergy harvesting. The model captures both the energy harvesting and\nconsumption phases, explicitly incorporating power management tasks to enable\nprecise estimation of device behavior across diverse environmental conditions.\nThe proposed model is applicable to a wide range of IoT devices and supports\nintelligent power management units designed to maximize harvested energy under\nfluctuating environmental conditions. We validated our model against a\nprototype batteryless IoT node, conducting experiments under three distinct\nillumination scenarios. Results show a strong correlation between analytical\nand measured supercapacitor voltage profiles, confirming the proposed model's\naccuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u7535\u6c60\u7269\u8054\u7f51\u4f20\u611f\u5668\u8282\u70b9\u7684\u80fd\u91cf\u52a8\u529b\u5b66\u6570\u5b66\u6a21\u578b\uff0c\u6db5\u76d6\u80fd\u91cf\u6536\u96c6\u548c\u6d88\u8017\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u65e0\u7535\u6c60\u7269\u8054\u7f51\u8bbe\u5907\u4f9d\u8d56\u73af\u5883\u80fd\u91cf\u6536\u96c6\uff0c\u9700\u8981\u7cbe\u786e\u7684\u6570\u5b66\u6a21\u578b\u6765\u9884\u6d4b\u5176\u884c\u4e3a\uff0c\u4ee5\u4f18\u5316\u80fd\u91cf\u7ba1\u7406\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7efc\u5408\u6570\u5b66\u6a21\u578b\uff0c\u6355\u6349\u80fd\u91cf\u6536\u96c6\u548c\u6d88\u8017\uff0c\u5e76\u652f\u6301\u667a\u80fd\u7535\u6e90\u7ba1\u7406\u5355\u5143\u3002\u901a\u8fc7\u539f\u578b\u8bbe\u5907\u5728\u4e09\u79cd\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5206\u6790\u5f97\u5230\u7684\u8d85\u7ea7\u7535\u5bb9\u5668\u7535\u538b\u66f2\u7ebf\u4e0e\u5b9e\u9645\u6d4b\u91cf\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u65e0\u7535\u6c60\u7269\u8054\u7f51\u8bbe\u5907\u7684\u80fd\u91cf\u884c\u4e3a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u73af\u5883\u6761\u4ef6\uff0c\u4e3a\u667a\u80fd\u7535\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.19639", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.19639", "abs": "https://arxiv.org/abs/2507.19639", "authors": ["Devroop Kar", "Zimeng Lyu", "Sheeraja Rajakrishnan", "Hao Zhang", "Alex Ororbia", "Travis Desell", "Daniel Krutz"], "title": "Directly Learning Stock Trading Strategies Through Profit Guided Loss Functions", "comment": "17 pages, 4 figures, Submitted to Neural Information Processing\n  Systems 2025", "summary": "Stock trading has always been a challenging task due to the highly volatile\nnature of the stock market. Making sound trading decisions to generate profit\nis particularly difficult under such conditions. To address this, we propose\nfour novel loss functions to drive decision-making for a portfolio of stocks.\nThese functions account for the potential profits or losses based with respect\nto buying or shorting respective stocks, enabling potentially any artificial\nneural network to directly learn an effective trading strategy. Despite the\nhigh volatility in stock market fluctuations over time, training time-series\nmodels such as transformers on these loss functions resulted in trading\nstrategies that generated significant profits on a portfolio of 50 different\nS&P 500 company stocks as compared to a benchmark reinforcment learning\ntechniques and a baseline buy and hold method. As an example, using 2021, 2022\nand 2023 as three test periods, the Crossformer model adapted with our best\nloss function was most consistent, resulting in returns of 51.42%, 51.04% and\n48.62% respectively. In comparison, the best performing state-of-the-art\nreinforcement learning methods, PPO and DDPG, only delivered maximum profits of\naround 41%, 2.81% and 41.58% for the same periods. The code is available at\nhttps://anonymous.4open.science/r/bandit-stock-trading-58C8/README.md.", "AI": {"tldr": "\u63d0\u51fa\u56db\u79cd\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u9a71\u52a8\u80a1\u7968\u6295\u8d44\u7ec4\u5408\u51b3\u7b56\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u80a1\u7968\u5e02\u573a\u9ad8\u5ea6\u6ce2\u52a8\uff0c\u4f20\u7edf\u4ea4\u6613\u51b3\u7b56\u96be\u4ee5\u76c8\u5229\uff0c\u9700\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff08\u5982Transformer\uff09\uff0c\u4f18\u5316\u4ea4\u6613\u7b56\u7565\u3002", "result": "\u572850\u53eaS&P 500\u80a1\u7968\u4e0a\u6d4b\u8bd5\uff0cCrossformer\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5e74\u56de\u62a5\u7387\u8d8548%\u3002", "conclusion": "\u65b0\u578b\u635f\u5931\u51fd\u6570\u6709\u6548\u63d0\u5347\u4ea4\u6613\u7b56\u7565\u8868\u73b0\uff0c\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2507.19822", "categories": ["cs.LG", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19822", "abs": "https://arxiv.org/abs/2507.19822", "authors": ["Youngjoon Lee", "Hyukjoon Lee", "Jinu Gong", "Yang Cao", "Joonhyuk Kang"], "title": "Debunking Optimization Myths in Federated Learning for Medical Image Classification", "comment": "Accepted to Efficient Medical AI Workshop - MICCAI 2025", "summary": "Federated Learning (FL) is a collaborative learning method that enables\ndecentralized model training while preserving data privacy. Despite its promise\nin medical imaging, recent FL methods are often sensitive to local factors such\nas optimizers and learning rates, limiting their robustness in practical\ndeployments. In this work, we revisit vanilla FL to clarify the impact of edge\ndevice configurations, benchmarking recent FL methods on colorectal pathology\nand blood cell classification task. We numerically show that the choice of\nlocal optimizer and learning rate has a greater effect on performance than the\nspecific FL method. Moreover, we find that increasing local training epochs can\neither enhance or impair convergence, depending on the FL method. These\nfindings indicate that appropriate edge-specific configuration is more crucial\nthan algorithmic complexity for achieving effective FL.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u8054\u90a6\u5b66\u4e60\u4e2d\u672c\u5730\u4f18\u5316\u5668\u548c\u5b66\u4e60\u7387\u7684\u9009\u62e9\u5bf9\u6027\u80fd\u5f71\u54cd\u5927\u4e8eFL\u65b9\u6cd5\u672c\u8eab\uff0c\u9002\u5f53\u7684\u8fb9\u7f18\u8bbe\u5907\u914d\u7f6e\u6bd4\u7b97\u6cd5\u590d\u6742\u6027\u66f4\u91cd\u8981\u3002", "motivation": "\u63a2\u8ba8\u8054\u90a6\u5b66\u4e60\u5728\u533b\u7597\u5f71\u50cf\u4e2d\u56e0\u672c\u5730\u914d\u7f6e\uff08\u5982\u4f18\u5316\u5668\u548c\u5b66\u4e60\u7387\uff09\u654f\u611f\u800c\u5f71\u54cd\u5b9e\u9645\u90e8\u7f72\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ed3\u80a0\u75c5\u7406\u548c\u8840\u7ec6\u80de\u5206\u7c7b\u4efb\u52a1\uff0c\u5bf9\u591a\u79cdFL\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u672c\u5730\u914d\u7f6e\u7684\u5f71\u54cd\u3002", "result": "\u672c\u5730\u4f18\u5316\u5668\u548c\u5b66\u4e60\u7387\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u589e\u52a0\u672c\u5730\u8bad\u7ec3\u8f6e\u6570\u53ef\u80fd\u63d0\u5347\u6216\u635f\u5bb3\u6536\u655b\u6027\uff0c\u53d6\u51b3\u4e8eFL\u65b9\u6cd5\u3002", "conclusion": "\u6709\u6548\u7684\u8054\u90a6\u5b66\u4e60\u66f4\u4f9d\u8d56\u4e8e\u9002\u5f53\u7684\u8fb9\u7f18\u8bbe\u5907\u914d\u7f6e\uff0c\u800c\u975e\u7b97\u6cd5\u590d\u6742\u6027\u3002"}}
{"id": "2507.19684", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19684", "abs": "https://arxiv.org/abs/2507.19684", "authors": ["Bermet Burkanova", "Payam Jome Yazdian", "Chuxuan Zhang", "Trinity Evans", "Paige Tutt\u00f6s\u00ed", "Angelica Lim"], "title": "Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks", "comment": "https://rosielab.github.io/compas3d", "summary": "Imagine a humanoid that can safely and creatively dance with a human,\nadapting to its partner's proficiency, using haptic signaling as a primary form\nof communication. While today's AI systems excel at text or voice-based\ninteraction with large language models, human communication extends far beyond\ntext-it includes embodied movement, timing, and physical coordination. Modeling\ncoupled interaction between two agents poses a formidable challenge: it is\ncontinuous, bidirectionally reactive, and shaped by individual variation. We\npresent CoMPAS3D, the largest and most diverse motion capture dataset of\nimprovised salsa dancing, designed as a challenging testbed for interactive,\nexpressive humanoid AI. The dataset includes 3 hours of leader-follower salsa\ndances performed by 18 dancers spanning beginner, intermediate, and\nprofessional skill levels. For the first time, we provide fine-grained salsa\nexpert annotations, covering over 2,800 move segments, including move types,\ncombinations, execution errors and stylistic elements. We draw analogies\nbetween partner dance communication and natural language, evaluating CoMPAS3D\non two benchmark tasks for synthetic humans that parallel key problems in\nspoken language and dialogue processing: leader or follower generation with\nproficiency levels (speaker or listener synthesis), and duet (conversation)\ngeneration. Towards a long-term goal of partner dance with humans, we release\nthe dataset, annotations, and code, along with a multitask SalsaAgent model\ncapable of performing all benchmark tasks, alongside additional baselines to\nencourage research in socially interactive embodied AI and creative, expressive\nhumanoid motion generation.", "AI": {"tldr": "CoMPAS3D\u662f\u4e00\u4e2a\u5927\u578b\u591a\u6837\u7684\u5373\u5174\u8428\u5c14\u8428\u821e\u52a8\u4f5c\u6355\u6349\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4e3a\u4ea4\u4e92\u5f0f\u3001\u8868\u8fbe\u6027\u4eba\u5f62AI\u63d0\u4f9b\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u63a2\u7d22\u4eba\u5f62AI\u5982\u4f55\u901a\u8fc7\u89e6\u89c9\u4fe1\u53f7\u5b89\u5168\u4e14\u521b\u9020\u6027\u5730\u4e0e\u4eba\u7c7b\u5171\u821e\uff0c\u8d85\u8d8a\u6587\u672c\u548c\u8bed\u97f3\u4ea4\u4e92\uff0c\u6a21\u62df\u53cc\u5411\u8fde\u7eed\u7684\u8eab\u4f53\u534f\u8c03\u3002", "method": "\u6536\u96c618\u540d\u821e\u80053\u5c0f\u65f6\u7684\u8428\u5c14\u8428\u821e\u6570\u636e\uff0c\u63d0\u4f9b\u7cbe\u7ec6\u7684\u52a8\u4f5c\u6ce8\u91ca\uff0c\u5e76\u5f00\u53d1\u591a\u4efb\u52a1SalsaAgent\u6a21\u578b\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b2800\u591a\u4e2a\u52a8\u4f5c\u6bb5\uff0c\u6db5\u76d6\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\uff0c\u6a21\u578b\u80fd\u5b8c\u6210\u9886\u5bfc\u8005/\u8ddf\u968f\u8005\u751f\u6210\u548c\u53cc\u4eba\u821e\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "CoMPAS3D\u4e3a\u793e\u4ea4\u4ea4\u4e92\u5f0f\u4eba\u5f62AI\u548c\u521b\u610f\u52a8\u4f5c\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2507.20426", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.20426", "abs": "https://arxiv.org/abs/2507.20426", "authors": ["Samiul Based Shuvo", "Tasnia Binte Mamun", "U Rajendra Acharya"], "title": "ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings", "comment": null, "summary": "DNA-binding proteins (DBPs) are integral to gene regulation and cellular\nprocesses, making their accurate identification essential for understanding\nbiological functions and disease mechanisms. Experimental methods for DBP\nidentification are time-consuming and costly, driving the need for efficient\ncomputational prediction techniques. In this study, we propose a novel deep\nlearning framework, ResCap-DBP, that combines a residual learning-based encoder\nwith a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly\nfrom raw protein sequences. Our architecture incorporates dilated convolutions\nwithin residual blocks to mitigate vanishing gradient issues and extract rich\nsequence features, while capsule layers with dynamic routing capture\nhierarchical and spatial relationships within the learned feature space. We\nconducted comprehensive ablation studies comparing global and local embeddings\nfrom ProteinBERT and conventional one-hot encoding. Results show that\nProteinBERT embeddings substantially outperform other representations on large\ndatasets. Although one-hot encoding showed marginal advantages on smaller\ndatasets, such as PDB186, it struggled to scale effectively. Extensive\nevaluations on four pairs of publicly available benchmark datasets demonstrate\nthat our model consistently outperforms current state-of-the-art methods. It\nachieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On\nindependent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2%\nand 83.3%, while maintaining competitive performance on larger datasets such as\nPDB20000. Notably, the model maintains a well balanced sensitivity and\nspecificity across datasets. These results demonstrate the efficacy and\ngeneralizability of integrating global protein representations with advanced\ndeep learning architectures for reliable and scalable DBP prediction in diverse\ngenomic contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6b8b\u5dee\u5b66\u4e60\u548c\u80f6\u56ca\u7f51\u7edc\u7684\u65b0\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6ResCap-DBP\uff0c\u7528\u4e8e\u76f4\u63a5\u4ece\u86cb\u767d\u8d28\u5e8f\u5217\u9884\u6d4bDNA\u7ed3\u5408\u86cb\u767d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "DNA\u7ed3\u5408\u86cb\u767d\u7684\u8bc6\u522b\u5bf9\u7406\u89e3\u751f\u7269\u529f\u80fd\u548c\u75be\u75c5\u673a\u5236\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9a8c\u65b9\u6cd5\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8ba1\u7b97\u9884\u6d4b\u6280\u672f\u3002", "method": "\u7ed3\u5408\u6b8b\u5dee\u5b66\u4e60\u7f16\u7801\u5668\u548c\u4e00\u7ef4\u80f6\u56ca\u7f51\u7edc\uff0c\u5229\u7528\u81a8\u80c0\u5377\u79ef\u63d0\u53d6\u5e8f\u5217\u7279\u5f81\uff0c\u80f6\u56ca\u5c42\u6355\u6349\u7279\u5f81\u7a7a\u95f4\u7684\u5c42\u6b21\u548c\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cAUC\u5f97\u5206\u9ad8\u8fbe98.0%\uff0c\u5e76\u4fdd\u6301\u5e73\u8861\u7684\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u3002", "conclusion": "ResCap-DBP\u5c55\u793a\u4e86\u7ed3\u5408\u5168\u5c40\u86cb\u767d\u8d28\u8868\u793a\u548c\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728DBP\u9884\u6d4b\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2507.19686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19686", "abs": "https://arxiv.org/abs/2507.19686", "authors": ["Robert Frenken", "Sidra Ghayour Bhatti", "Hanqin Zhang", "Qadeer Ahmed"], "title": "KD-GAT: Combining Knowledge Distillation and Graph Attention Transformer for a Controller Area Network Intrusion Detection System", "comment": null, "summary": "The Controller Area Network (CAN) protocol is widely adopted for in-vehicle\ncommunication but lacks inherent security mechanisms, making it vulnerable to\ncyberattacks. This paper introduces KD-GAT, an intrusion detection framework\nthat combines Graph Attention Networks (GATs) with knowledge distillation (KD)\nto enhance detection accuracy while reducing computational complexity. In our\napproach, CAN traffic is represented as graphs using a sliding window to\ncapture temporal and relational patterns. A multi-layer GAT with jumping\nknowledge aggregation acting as the teacher model, while a compact student\nGAT--only 6.32% the size of the teacher--is trained via a two-phase process\ninvolving supervised pretraining and knowledge distillation with both soft and\nhard label supervision. Experiments on three benchmark datasets--Car-Hacking,\nCar-Survival, and can-train-and-test demonstrate that both teacher and student\nmodels achieve strong results, with the student model attaining 99.97% and\n99.31% accuracy on Car-Hacking and Car-Survival, respectively. However,\nsignificant class imbalance in can-train-and-test has led to reduced\nperformance for both models on this dataset. Addressing this imbalance remains\nan important direction for future work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKD-GAT\u7684\u5165\u4fb5\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u548c\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\uff0c\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "CAN\u534f\u8bae\u7f3a\u4e4f\u5b89\u5168\u673a\u5236\uff0c\u6613\u53d7\u7f51\u7edc\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5c06CAN\u6d41\u91cf\u8868\u793a\u4e3a\u56fe\uff0c\u4f7f\u7528\u591a\u5c42GAT\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\uff09\u8bad\u7ec3\u5c0f\u578b\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5728Car-Hacking\u548cCar-Survival\u6570\u636e\u96c6\u4e0a\uff0c\u5b66\u751f\u6a21\u578b\u5206\u522b\u8fbe\u523099.97%\u548c99.31%\u7684\u51c6\u786e\u7387\uff0c\u4f46\u5728can-train-and-test\u6570\u636e\u96c6\u4e0a\u56e0\u7c7b\u522b\u4e0d\u5e73\u8861\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "KD-GAT\u6846\u67b6\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2507.21023", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21023", "abs": "https://arxiv.org/abs/2507.21023", "authors": ["Rick S. Blum", "Franziska Freytag"], "title": "On Using the Shapley Value for Anomaly Localization: A Statistical Investigation", "comment": null, "summary": "Recent publications have suggested using the Shapley value for anomaly\nlocalization for sensor data systems. Using a reasonable mathematical anomaly\nmodel for full control, experiments indicate that using a single fixed term in\nthe Shapley value calculation achieves a lower complexity anomaly localization\ntest, with the same probability of error, as a test using the Shapley value for\nall cases tested. A proof demonstrates these conclusions must be true for all\nindependent observation cases. For dependent observation cases, no proof is\navailable.", "AI": {"tldr": "\u4f7f\u7528Shapley\u503c\u8fdb\u884c\u4f20\u611f\u5668\u6570\u636e\u5f02\u5e38\u5b9a\u4f4d\uff0c\u5b9e\u9a8c\u8868\u660e\u56fa\u5b9a\u9879\u8ba1\u7b97\u53ef\u964d\u4f4e\u590d\u6742\u5ea6\u4e14\u4fdd\u6301\u76f8\u540c\u9519\u8bef\u7387\uff0c\u72ec\u7acb\u89c2\u6d4b\u60c5\u51b5\u4e0b\u7ed3\u8bba\u666e\u9002\u3002", "motivation": "\u63a2\u7d22Shapley\u503c\u5728\u4f20\u611f\u5668\u6570\u636e\u5f02\u5e38\u5b9a\u4f4d\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u7b80\u5316\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u6570\u5b66\u8bc1\u660e\uff0c\u6bd4\u8f83\u56fa\u5b9a\u9879\u4e0e\u5b8c\u6574Shapley\u503c\u8ba1\u7b97\u7684\u6027\u80fd\u3002", "result": "\u56fa\u5b9a\u9879\u8ba1\u7b97\u5728\u72ec\u7acb\u89c2\u6d4b\u4e0b\u6027\u80fd\u4e0e\u5b8c\u6574\u8ba1\u7b97\u76f8\u540c\uff0c\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "\u56fa\u5b9a\u9879\u65b9\u6cd5\u9002\u7528\u4e8e\u72ec\u7acb\u89c2\u6d4b\uff0c\u4f9d\u8d56\u89c2\u6d4b\u60c5\u51b5\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.19697", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19697", "abs": "https://arxiv.org/abs/2507.19697", "authors": ["Yazeed Alrubyli", "Omar Alomeir", "Abrar Wafa", "Di\u00e1na Hidv\u00e9gi", "Hend Alrasheed", "Mohsen Bahrami"], "title": "NAICS-Aware Graph Neural Networks for Large-Scale POI Co-visitation Prediction: A Multi-Modal Dataset and Methodology", "comment": null, "summary": "Understanding where people go after visiting one business is crucial for\nurban planning, retail analytics, and location-based services. However,\npredicting these co-visitation patterns across millions of venues remains\nchallenging due to extreme data sparsity and the complex interplay between\nspatial proximity and business relationships. Traditional approaches using only\ngeographic distance fail to capture why coffee shops attract different customer\nflows than fine dining restaurants, even when co-located. We introduce\nNAICS-aware GraphSAGE, a novel graph neural network that integrates business\ntaxonomy knowledge through learnable embeddings to predict population-scale\nco-visitation patterns. Our key insight is that business semantics, captured\nthrough detailed industry codes, provide crucial signals that pure spatial\nmodels cannot explain. The approach scales to massive datasets (4.2 billion\npotential venue pairs) through efficient state-wise decomposition while\ncombining spatial, temporal, and socioeconomic features in an end-to-end\nframework. Evaluated on our POI-Graph dataset comprising 94.9 million\nco-visitation records across 92,486 brands and 48 US states, our method\nachieves significant improvements over state-of-the-art baselines: the\nR-squared value increases from 0.243 to 0.625 (a 157 percent improvement), with\nstrong gains in ranking quality (32 percent improvement in NDCG at 10).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5546\u4e1a\u5206\u7c7b\u77e5\u8bc6\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08NAICS-aware GraphSAGE\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u5927\u89c4\u6a21\u573a\u6240\u7684\u5171\u8bbf\u6a21\u5f0f\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3\u4eba\u4eec\u8bbf\u95ee\u4e00\u4e2a\u5546\u4e1a\u573a\u6240\u540e\u7684\u53bb\u5411\u5bf9\u57ce\u5e02\u89c4\u5212\u3001\u96f6\u552e\u5206\u6790\u548c\u57fa\u4e8e\u4f4d\u7f6e\u7684\u670d\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u56e0\u6570\u636e\u7a00\u758f\u6027\u548c\u7a7a\u95f4\u4e0e\u5546\u4e1a\u5173\u7cfb\u7684\u590d\u6742\u6027\u800c\u53d7\u9650\u3002", "method": "\u63d0\u51faNAICS-aware GraphSAGE\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5d4c\u5165\u6574\u5408\u5546\u4e1a\u5206\u7c7b\u77e5\u8bc6\uff0c\u7ed3\u5408\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u793e\u4f1a\u7ecf\u6d4e\u7279\u5f81\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u9884\u6d4b\u3002", "result": "\u5728\u5305\u542b9490\u4e07\u5171\u8bbf\u8bb0\u5f55\u7684POI-Graph\u6570\u636e\u96c6\u4e0a\uff0cR-squared\u503c\u4ece0.243\u63d0\u5347\u81f30.625\uff08\u63d0\u5347157%\uff09\uff0c\u6392\u540d\u8d28\u91cf\u663e\u8457\u63d0\u9ad8\uff08NDCG@10\u63d0\u534732%\uff09\u3002", "conclusion": "\u5546\u4e1a\u8bed\u4e49\uff08\u901a\u8fc7\u884c\u4e1a\u4ee3\u7801\u6355\u83b7\uff09\u4e3a\u5171\u8bbf\u6a21\u5f0f\u9884\u6d4b\u63d0\u4f9b\u4e86\u5173\u952e\u4fe1\u53f7\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.19700", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19700", "abs": "https://arxiv.org/abs/2507.19700", "authors": ["Anton Danholt Lautrup", "Muhammad Rajabinasab", "Tobias Hyrup", "Arthur Zimek", "Peter Schneider-Kamp"], "title": "Disjoint Generative Models", "comment": null, "summary": "We propose a new framework for generating cross-sectional synthetic datasets\nvia disjoint generative models. In this paradigm, a dataset is partitioned into\ndisjoint subsets that are supplied to separate instances of generative models.\nThe results are then combined post hoc by a joining operation that works in the\nabsence of common variables/identifiers. The success of the framework is\ndemonstrated through several case studies and examples on tabular data that\nhelps illuminate some of the design choices that one may make. The principal\nbenefit of disjoint generative models is significantly increased privacy at\nonly a low utility cost. Additional findings include increased effectiveness\nand feasibility for certain model types and the possibility for mixed-model\nsynthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u79bb\u751f\u6210\u6a21\u578b\u751f\u6210\u8de8\u622a\u9762\u5408\u6210\u6570\u636e\u96c6\u7684\u65b0\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u9690\u79c1\u6027\u4e14\u6548\u7528\u635f\u5931\u4f4e\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u751f\u6210\u6a21\u578b\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u5408\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u66f4\u9ad8\u6548\u548c\u53ef\u884c\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5c06\u6570\u636e\u96c6\u5212\u5206\u4e3a\u4e0d\u76f8\u4ea4\u5b50\u96c6\uff0c\u5206\u522b\u8f93\u5165\u72ec\u7acb\u7684\u751f\u6210\u6a21\u578b\uff0c\u540e\u901a\u8fc7\u65e0\u5171\u540c\u53d8\u91cf\u7684\u8fde\u63a5\u64cd\u4f5c\u5408\u5e76\u7ed3\u679c\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\u8be5\u6846\u67b6\u5728\u9690\u79c1\u6027\u548c\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u540c\u65f6\u652f\u6301\u6df7\u5408\u6a21\u578b\u5408\u6210\u3002", "conclusion": "\u5206\u79bb\u751f\u6210\u6a21\u578b\u6846\u67b6\u4e3a\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u9ad8\u6548\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.19715", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19715", "abs": "https://arxiv.org/abs/2507.19715", "authors": ["Rahul Raja", "Arpita Vats"], "title": "Beyond Nearest Neighbors: Semantic Compression and Graph-Augmented Retrieval for Enhanced Vector Search", "comment": null, "summary": "Vector databases typically rely on approximate nearest neighbor (ANN) search\nto retrieve the top-k closest vectors to a query in embedding space. While\neffective, this approach often yields semantically redundant results, missing\nthe diversity and contextual richness required by applications such as\nretrieval-augmented generation (RAG), multi-hop QA, and memory-augmented\nagents. We introduce a new retrieval paradigm: semantic compression, which aims\nto select a compact, representative set of vectors that captures the broader\nsemantic structure around a query. We formalize this objective using principles\nfrom submodular optimization and information geometry, and show that it\ngeneralizes traditional top-k retrieval by prioritizing coverage and diversity.\nTo operationalize this idea, we propose graph-augmented vector retrieval, which\noverlays semantic graphs (e.g., kNN or knowledge-based links) atop vector\nspaces to enable multi-hop, context-aware search. We theoretically analyze the\nlimitations of proximity-based retrieval under high-dimensional concentration\nand highlight how graph structures can improve semantic coverage. Our work\noutlines a foundation for meaning-centric vector search systems, emphasizing\nhybrid indexing, diversity-aware querying, and structured semantic retrieval.\nWe make our implementation publicly available to foster future research in this\narea.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u8303\u5f0f\u2014\u2014\u8bed\u4e49\u538b\u7f29\uff0c\u901a\u8fc7\u5b50\u6a21\u4f18\u5316\u548c\u4fe1\u606f\u51e0\u4f55\u539f\u5219\u9009\u62e9\u5177\u6709\u4ee3\u8868\u6027\u7684\u5411\u91cf\u96c6\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u5728\u8bed\u4e49\u591a\u6837\u6027\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08ANN\uff09\u867d\u7136\u6709\u6548\uff0c\u4f46\u5f80\u5f80\u8fd4\u56de\u8bed\u4e49\u5197\u4f59\u7684\u7ed3\u679c\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u591a\u8df3\u95ee\u7b54\u7b49\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u538b\u7f29\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b50\u6a21\u4f18\u5316\u548c\u4fe1\u606f\u51e0\u4f55\u539f\u5219\uff0c\u5e76\u901a\u8fc7\u56fe\u589e\u5f3a\u5411\u91cf\u68c0\u7d22\uff08\u5982kNN\u6216\u77e5\u8bc6\u56fe\u8c31\uff09\u5b9e\u73b0\u591a\u8df3\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u641c\u7d22\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u9ad8\u7ef4\u96c6\u4e2d\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e\u90bb\u8fd1\u7684\u68c0\u7d22\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u56fe\u7ed3\u6784\u53ef\u4ee5\u6539\u5584\u8bed\u4e49\u8986\u76d6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4ee5\u610f\u4e49\u4e3a\u4e2d\u5fc3\u7684\u5411\u91cf\u641c\u7d22\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5f3a\u8c03\u6df7\u5408\u7d22\u5f15\u3001\u591a\u6837\u6027\u611f\u77e5\u67e5\u8be2\u548c\u7ed3\u6784\u5316\u8bed\u4e49\u68c0\u7d22\u3002"}}
{"id": "2507.19737", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.19737", "abs": "https://arxiv.org/abs/2507.19737", "authors": ["Yinzhou Tang", "Huandong Wang", "Xiaochen Fan", "Yong Li"], "title": "Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning", "comment": null, "summary": "The vulnerability of cities to natural disasters has increased with\nurbanization and climate change, making it more important to predict human\nmobility in the disaster scenarios for downstream tasks including\nlocation-based early disaster warning and pre-allocating rescue resources, etc.\nHowever, existing human mobility prediction models are mainly designed for\nnormal scenarios, and fail to adapt to disaster scenarios due to the shift of\nhuman mobility patterns under disaster. To address this issue, we introduce\n\\textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios\nthat can be integrated into existing deep mobility prediction methods by\nleveraging LLMs to model the mobility intention and transferring the common\nknowledge of how different disasters affect mobility intentions between cities.\nThis framework utilizes a RAG-Enhanced Intention Predictor to forecast the next\nintention, refines it with an LLM-based Intention Refiner, and then maps the\nintention to an exact location using an Intention-Modulated Location Predictor.\nExtensive experiments illustrate that DisasterMobLLM can achieve a 32.8\\%\nimprovement in terms of Acc@1 and a 35.0\\% improvement in terms of the F1-score\nof predicting immobility compared to the baselines. The code is available at\nhttps://github.com/tsinghua-fib-lab/DisasterMobLLM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDisasterMobLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u707e\u5bb3\u573a\u666f\u4e0b\u7684\u4eba\u7c7b\u79fb\u52a8\u6027\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548cRAG\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u57ce\u5e02\u5316\u4e0e\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u57ce\u5e02\u5bf9\u81ea\u7136\u707e\u5bb3\u7684\u8106\u5f31\u6027\uff0c\u73b0\u6709\u79fb\u52a8\u6027\u9884\u6d4b\u6a21\u578b\u5728\u707e\u5bb3\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u6846\u67b6\u5305\u62ecRAG\u589e\u5f3a\u7684\u610f\u56fe\u9884\u6d4b\u5668\u3001\u57fa\u4e8eLLM\u7684\u610f\u56fe\u7cbe\u70bc\u5668\u548c\u610f\u56fe\u8c03\u5236\u7684\u5b9a\u4f4d\u9884\u6d4b\u5668\uff0c\u6574\u5408\u4e86\u707e\u5bb3\u5bf9\u79fb\u52a8\u610f\u56fe\u7684\u5f71\u54cd\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDisasterMobLLM\u5728Acc@1\u548cF1-score\u4e0a\u5206\u522b\u63d0\u5347\u4e8632.8%\u548c35.0%\u3002", "conclusion": "DisasterMobLLM\u6709\u6548\u89e3\u51b3\u4e86\u707e\u5bb3\u573a\u666f\u4e0b\u4eba\u7c7b\u79fb\u52a8\u6027\u9884\u6d4b\u7684\u6311\u6218\uff0c\u4e3a\u707e\u5bb3\u9884\u8b66\u548c\u6551\u63f4\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2507.19755", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.19755", "abs": "https://arxiv.org/abs/2507.19755", "authors": ["Ziqi Zhang", "Shiheng Chen", "Runze Yang", "Zhisheng Wei", "Wei Zhang", "Lei Wang", "Zhanzhi Liu", "Fengshan Zhang", "Jing Wu", "Xiaoyong Pan", "Hongbin Shen", "Longbing Cao", "Zhaohong Deng"], "title": "Modeling enzyme temperature stability from sequence segment perspective", "comment": null, "summary": "Developing enzymes with desired thermal properties is crucial for a wide\nrange of industrial and research applications, and determining temperature\nstability is an essential step in this process. Experimental determination of\nthermal parameters is labor-intensive, time-consuming, and costly. Moreover,\nexisting computational approaches are often hindered by limited data\navailability and imbalanced distributions. To address these challenges, we\nintroduce a curated temperature stability dataset designed for model\ndevelopment and benchmarking in enzyme thermal modeling. Leveraging this\ndataset, we present the \\textit{Segment Transformer}, a novel deep learning\nframework that enables efficient and accurate prediction of enzyme temperature\nstability. The model achieves state-of-the-art performance with an RMSE of\n24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33,\nrespectively. These results highlight the effectiveness of incorporating\nsegment-level representations, grounded in the biological observation that\ndifferent regions of a protein sequence contribute unequally to thermal\nbehavior. As a proof of concept, we applied the Segment Transformer to guide\nthe engineering of a cutinase enzyme. Experimental validation demonstrated a\n1.64-fold improvement in relative activity following heat treatment, achieved\nthrough only 17 mutations and without compromising catalytic function.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSegment Transformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u9884\u6d4b\u9176\u7684\u6e29\u5ea6\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5b9e\u9a8c\u6d4b\u5b9a\u9176\u7684\u8010\u70ed\u6027\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u4e0d\u8db3\u548c\u4e0d\u5e73\u8861\u5206\u5e03\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7cbe\u5fc3\u6574\u7406\u7684\u6e29\u5ea6\u7a33\u5b9a\u6027\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86Segment Transformer\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u57fa\u4e8e\u86cb\u767d\u8d28\u5e8f\u5217\u4e2d\u4e0d\u540c\u533a\u57df\u5bf9\u70ed\u884c\u4e3a\u8d21\u732e\u4e0d\u5747\u7684\u751f\u7269\u5b66\u89c2\u5bdf\u3002", "result": "\u6a21\u578b\u5728RMSE\u3001MAE\u548c\u76f8\u5173\u6027\u6307\u6807\u4e0a\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u901a\u8fc717\u4e2a\u7a81\u53d8\u4f7f\u9176\u7684\u76f8\u5bf9\u6d3b\u6027\u63d0\u9ad8\u4e861.64\u500d\u3002", "conclusion": "Segment Transformer\u4e3a\u9176\u7684\u8010\u70ed\u6027\u9884\u6d4b\u548c\u5de5\u7a0b\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2507.19771", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19771", "abs": "https://arxiv.org/abs/2507.19771", "authors": ["Xin Zhang", "Lissette Iturburu", "Juan Nicolas Villamizar", "Xiaoyu Liu", "Manuel Salmeron", "Shirley J. Dyke", "Julio Ramirez"], "title": "Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation", "comment": null, "summary": "Structural drawings are widely used in many fields, e.g., mechanical\nengineering, civil engineering, etc. In civil engineering, structural drawings\nserve as the main communication tool between architects, engineers, and\nbuilders to avoid conflicts, act as legal documentation, and provide a\nreference for future maintenance or evaluation needs. They are often organized\nusing key elements such as title/subtitle blocks, scales, plan views, elevation\nview, sections, and detailed sections, which are annotated with standardized\nsymbols and line types for interpretation by engineers and contractors. Despite\nadvances in software capabilities, the task of generating a structural drawing\nremains labor-intensive and time-consuming for structural engineers. Here we\nintroduce a novel generative AI-based method for generating structural drawings\nemploying a large language model (LLM) agent. The method incorporates a\nretrieval-augmented generation (RAG) technique using externally-sourced facts\nto enhance the accuracy and reliability of the language model. This method is\ncapable of understanding varied natural language descriptions, processing these\nto extract necessary information, and generating code to produce the desired\nstructural drawing in AutoCAD. The approach developed, demonstrated and\nevaluated herein enables the efficient and direct conversion of a structural\ndrawing's natural language description into an AutoCAD drawing, significantly\nreducing the workload compared to current working process associated with\nmanual drawing production, facilitating the typical iterative process of\nengineers for expressing design ideas in a simplified way.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u9ad8\u6548\u8f6c\u6362\u4e3aAutoCAD\u7ed3\u6784\u56fe\u7eb8\u3002", "motivation": "\u7ed3\u6784\u56fe\u7eb8\u5728\u5de5\u7a0b\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u751f\u6210\u8fc7\u7a0b\u8017\u65f6\u8017\u529b\u3002\u73b0\u6709\u8f6f\u4ef6\u80fd\u529b\u867d\u5f3a\uff0c\u4f46\u4ecd\u9700\u5de5\u7a0b\u5e08\u624b\u52a8\u64cd\u4f5c\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528LLM\u7ed3\u5408RAG\u6280\u672f\uff0c\u4ece\u5916\u90e8\u8d44\u6e90\u83b7\u53d6\u4fe1\u606f\uff0c\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u3002\u6a21\u578b\u80fd\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u5e76\u751f\u6210AutoCAD\u4ee3\u7801\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5230AutoCAD\u56fe\u7eb8\u7684\u76f4\u63a5\u8f6c\u6362\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u624b\u52a8\u7ed8\u56fe\u7684\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u7ed3\u6784\u56fe\u7eb8\u751f\u6210\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u5de5\u7a0b\u5e08\u7684\u8bbe\u8ba1\u8fed\u4ee3\u6548\u7387\u3002"}}
{"id": "2507.19803", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19803", "abs": "https://arxiv.org/abs/2507.19803", "authors": ["Saram Abbas", "Naeem Soomro", "Rishad Shafik", "Rakesh Heer", "Kabita Adhikari"], "title": "AI-Based Clinical Rule Discovery for NMIBC Recurrence through Tsetlin Machines", "comment": "Submitted to ISTM 2025", "summary": "Bladder cancer claims one life every 3 minutes worldwide. Most patients are\ndiagnosed with non-muscle-invasive bladder cancer (NMIBC), yet up to 70% recur\nafter treatment, triggering a relentless cycle of surgeries, monitoring, and\nrisk of progression. Clinical tools like the EORTC risk tables are outdated and\nunreliable - especially for intermediate-risk cases.\n  We propose an interpretable AI model using the Tsetlin Machine (TM), a\nsymbolic learner that outputs transparent, human-readable logic. Tested on the\nPHOTO trial dataset (n=330), TM achieved an F1-score of 0.80, outperforming\nXGBoost (0.78), Logistic Regression (0.60), and EORTC (0.42). TM reveals the\nexact clauses behind each prediction, grounded in clinical features like tumour\ncount, surgeon experience, and hospital stay - offering accuracy and full\ntransparency. This makes TM a powerful, trustworthy decision-support tool ready\nfor real-world adoption.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTsetlin Machine\uff08TM\uff09\u7684\u53ef\u89e3\u91caAI\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u975e\u808c\u5c42\u6d78\u6da6\u6027\u8180\u80f1\u764c\uff08NMIBC\uff09\u7684\u590d\u53d1\u98ce\u9669\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8180\u80f1\u764c\u7684\u9ad8\u590d\u53d1\u7387\u548c\u73b0\u6709\u4e34\u5e8a\u5de5\u5177\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u4e2d\u7b49\u98ce\u9669\u75c5\u4f8b\u7684\u9884\u6d4b\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528Tsetlin Machine\uff08TM\uff09\u6784\u5efa\u53ef\u89e3\u91ca\u7684AI\u6a21\u578b\uff0c\u57fa\u4e8ePHOTO\u8bd5\u9a8c\u6570\u636e\u96c6\uff08n=330\uff09\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "TM\u7684F1\u5f97\u5206\u4e3a0.80\uff0c\u4f18\u4e8eXGBoost\uff080.78\uff09\u3001\u903b\u8f91\u56de\u5f52\uff080.60\uff09\u548cEORTC\uff080.42\uff09\uff0c\u5e76\u63d0\u4f9b\u900f\u660e\u9884\u6d4b\u903b\u8f91\u3002", "conclusion": "TM\u662f\u4e00\u79cd\u51c6\u786e\u4e14\u900f\u660e\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.19839", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19839", "abs": "https://arxiv.org/abs/2507.19839", "authors": ["Tiantian Peng", "Yuyang Liu", "Shuo Yang", "Qiuhe Hong", "YongHong Tian"], "title": "GNSP: Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning", "comment": null, "summary": "Contrastive Language-Image Pretraining has demonstrated remarkable zero-shot\ngeneralization by aligning visual and textual modalities in a shared embedding\nspace. However, when continuously fine-tuned on diverse tasks, CLIP suffers\nfrom catastrophic forgetting and degradation of its embedding alignment,\nundermining its zero-shot capabilities. In this work, we propose Gradient Null\nSpace Projection (GNSP), an efficient continual learning method that projects\ntask-specific gradients onto the null space of previously learned knowledge.\nThis orthogonal projection mathematically prevents interference with previous\ntasks without relying on rehearsal or architectural modification. Furthermore,\nto preserve the inherent generalization property of CLIP, we introduce\nknowledge distillation and combine it with a modality alignment preservation\nloss inspired by CLIP pre-training to stabilize the structure of the multimodal\nembedding space during fine-tuning. On the MTIL benchmark consisting of 11\ntasks, our method achieved SOTA performance on both the Average and Last key\nmetrics. More importantly, experiments show that our method successfully\nmaintains the original modality gap and cross-modal retrieval performance of\nCLIP, confirming its effectiveness in maintaining a robust visual-language\nspace throughout the continual learning process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGNSP\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u96f6\u7a7a\u95f4\u6295\u5f71\u89e3\u51b3CLIP\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u6a21\u6001\u5bf9\u9f50\u635f\u5931\u4fdd\u6301\u5176\u96f6\u6837\u672c\u80fd\u529b\u3002", "motivation": "CLIP\u5728\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\u548c\u5d4c\u5165\u5bf9\u9f50\u9000\u5316\uff0c\u5f71\u54cd\u5176\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faGNSP\u65b9\u6cd5\uff0c\u5c06\u4efb\u52a1\u7279\u5b9a\u68af\u5ea6\u6295\u5f71\u5230\u5148\u524d\u77e5\u8bc6\u7684\u96f6\u7a7a\u95f4\uff0c\u907f\u514d\u5e72\u6270\uff1b\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u6a21\u6001\u5bf9\u9f50\u635f\u5931\u4fdd\u6301\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "\u5728MTIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86CLIP\u7684\u6a21\u6001\u95f4\u9699\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "GNSP\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86CLIP\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u5176\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.19846", "categories": ["cs.LG", "cs.IR", "68T50", "I.2.7; I.2.6; H.3.3; H.4.1"], "pdf": "https://arxiv.org/pdf/2507.19846", "abs": "https://arxiv.org/abs/2507.19846", "authors": ["Harish S", "Chetana K Nayak", "Joy Bose"], "title": "A Scalable and High Availability Solution for Recommending Resolutions to Problem Tickets", "comment": "9 pages, 7 figures", "summary": "Resolution of incidents or problem tickets is a common theme in service\nindustries in any sector, including billing and charging systems in telecom\ndomain. Machine learning can help to identify patterns and suggest resolutions\nfor the problem tickets, based on patterns in the historical data of the\ntickets. However, this process may be complicated due to a variety of phenomena\nsuch as data drift and issues such as missing data, lack of data pertaining to\nresolutions of past incidents, too many similar sounding resolutions due to\nfree text and similar sounding text. This paper proposes a robust ML-driven\nsolution employing clustering, supervised learning, and advanced NLP models to\ntackle these challenges effectively. Building on previous work, we demonstrate\nclustering-based resolution identification, supervised classification with LDA,\nSiamese networks, and One-shot learning, Index embedding. Additionally, we\npresent a real-time dashboard and a highly available Kubernetes-based\nproduction deployment. Our experiments with both the open-source Bitext\ncustomer-support dataset and proprietary telecom datasets demonstrate high\nprediction accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u805a\u7c7b\u3001\u76d1\u7763\u5b66\u4e60\u548c\u9ad8\u7ea7NLP\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u7535\u4fe1\u9886\u57df\u95ee\u9898\u5de5\u5355\u7684\u590d\u6742\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u5de5\u5355\u5904\u7406\u4e2d\u7684\u6570\u636e\u6f02\u79fb\u3001\u7f3a\u5931\u6570\u636e\u548c\u6587\u672c\u76f8\u4f3c\u6027\u7b49\u95ee\u9898\uff0c\u63d0\u9ad8\u5de5\u5355\u89e3\u51b3\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u805a\u7c7b\u3001\u76d1\u7763\u5b66\u4e60\uff08LDA\u3001Siamese\u7f51\u7edc\u3001One-shot\u5b66\u4e60\uff09\u548c\u7d22\u5f15\u5d4c\u5165\u6280\u672f\uff0c\u7ed3\u5408\u5b9e\u65f6\u4eea\u8868\u76d8\u548cKubernetes\u90e8\u7f72\u3002", "result": "\u5728\u5f00\u6e90\u548c\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u590d\u6742\u5de5\u5355\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.19849", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19849", "abs": "https://arxiv.org/abs/2507.19849", "authors": ["Guanting Dong", "Hangyu Mao", "Kai Ma", "Licheng Bao", "Yifei Chen", "Zhongyuan Wang", "Zhongxia Chen", "Jiazhen Du", "Huiyang Wang", "Fuzheng Zhang", "Guorui Zhou", "Yutao Zhu", "Ji-Rong Wen", "Zhicheng Dou"], "title": "Agentic Reinforced Policy Optimization", "comment": "Working on progress", "summary": "Large-scale reinforcement learning with verifiable rewards (RLVR) has\ndemonstrated its effectiveness in harnessing the potential of large language\nmodels (LLMs) for single-turn reasoning tasks. In realistic reasoning\nscenarios, LLMs can often utilize external tools to assist in task-solving\nprocesses. However, current RL algorithms inadequately balance the models'\nintrinsic long-horizon reasoning capabilities and their proficiency in\nmulti-turn tool interactions. To bridge this gap, we propose Agentic Reinforced\nPolicy Optimization (ARPO), a novel agentic RL algorithm tailored for training\nmulti-turn LLM-based agents. Through preliminary experiments, we observe that\nLLMs tend to exhibit highly uncertain behavior, characterized by an increase in\nthe entropy distribution of generated tokens, immediately following\ninteractions with external tools. Motivated by this observation, ARPO\nincorporates an entropy-based adaptive rollout mechanism, dynamically balancing\nglobal trajectory sampling and step-level sampling, thereby promoting\nexploration at steps with high uncertainty after tool usage. By integrating an\nadvantage attribution estimation, ARPO enables LLMs to internalize advantage\ndifferences in stepwise tool-use interactions. Our experiments across 13\nchallenging benchmarks in computational reasoning, knowledge reasoning, and\ndeep search domains demonstrate ARPO's superiority over trajectory-level RL\nalgorithms. Remarkably, ARPO achieves improved performance using only half of\nthe tool-use budget required by existing methods, offering a scalable solution\nfor aligning LLM-based agents with real-time dynamic environments. Our code and\ndatasets are released at https://github.com/dongguanting/ARPO", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARPO\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684LLM\u4ee3\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u5168\u5c40\u8f68\u8ff9\u91c7\u6837\u548c\u6b65\u9aa4\u7ea7\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u672a\u80fd\u6709\u6548\u5e73\u8861LLM\u7684\u957f\u65f6\u63a8\u7406\u80fd\u529b\u548c\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ARPO\u7ed3\u5408\u4e86\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u6eda\u52a8\u673a\u5236\u548c\u4f18\u52bf\u5f52\u56e0\u4f30\u8ba1\uff0c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u7b56\u7565\u4ee5\u5e94\u5bf9\u5de5\u5177\u4f7f\u7528\u540e\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u572813\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARPO\u8868\u73b0\u4f18\u4e8e\u8f68\u8ff9\u7ea7\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e14\u4ec5\u9700\u73b0\u6709\u65b9\u6cd5\u4e00\u534a\u7684\u5de5\u5177\u4f7f\u7528\u9884\u7b97\u3002", "conclusion": "ARPO\u4e3aLLM\u4ee3\u7406\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19855", "categories": ["cs.LG", "cs.HC", "68T05, 68T07, 68T40", "I.2.6; I.2.9; I.2.7; I.2.10; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.19855", "abs": "https://arxiv.org/abs/2507.19855", "authors": ["Aditya Sharma", "Linh Nguyen", "Ananya Gupta", "Chengyu Wang", "Chiamaka Adebayo", "Jakub Kowalski"], "title": "Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning", "comment": "12 pages, 4 figures,", "summary": "Large Language Models (LLMs), despite their advanced linguistic capabilities,\nfundamentally lack an intuitive understanding of physical dynamics, which\nlimits their effectiveness in real-world scenarios that require causal\nreasoning. In this paper, we introduce Causal World Model Induction (CWMI), a\nnovel framework designed to embed an explicit model of causal physics within an\nLLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a\nnew training objective called Causal Intervention Loss, encouraging the model\nto learn cause-and-effect relationships from multimodal data. By training the\nmodel to predict the outcomes of hypothetical interventions instead of merely\ncapturing statistical correlations, CWMI develops a robust internal\nrepresentation of physical laws. Experimental results show that CWMI\nsignificantly outperforms state-of-the-art LLMs on zero-shot physical reasoning\ntasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench\ndataset. These findings demonstrate that inducing a causal world model is a\ncritical step toward more reliable and generalizable AI systems.", "AI": {"tldr": "CWMI\u6846\u67b6\u901a\u8fc7\u5d4c\u5165\u56e0\u679c\u7269\u7406\u6a21\u5757\u548c\u65b0\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u96f6\u6837\u672c\u7269\u7406\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "LLMs\u7f3a\u4e4f\u5bf9\u7269\u7406\u52a8\u6001\u7684\u76f4\u89c2\u7406\u89e3\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u56e0\u679c\u63a8\u7406\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u5f15\u5165Causal World Model Induction (CWMI)\u6846\u67b6\uff0c\u5305\u62ecCausal Physics Module (CPM)\u548cCausal Intervention Loss\u8bad\u7ec3\u76ee\u6807\uff0c\u4ece\u591a\u6a21\u6001\u6570\u636e\u4e2d\u5b66\u4e60\u56e0\u679c\u5173\u7cfb\u3002", "result": "CWMI\u5728PIQA\u548cPhysiCa-Bench\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLMs\u3002", "conclusion": "\u8bf1\u5bfc\u56e0\u679c\u4e16\u754c\u6a21\u578b\u662f\u6784\u5efa\u66f4\u53ef\u9760\u3001\u901a\u7528AI\u7cfb\u7edf\u7684\u5173\u952e\u6b65\u9aa4\u3002"}}
{"id": "2507.19887", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19887", "abs": "https://arxiv.org/abs/2507.19887", "authors": ["Shishir Muralidhara", "Didier Stricker", "Ren\u00e9 Schuster"], "title": "CLoRA: Parameter-Efficient Continual Learning with Low-Rank Adaptation", "comment": "Accepted at CoLLAs 2025", "summary": "In the past, continual learning (CL) was mostly concerned with the problem of\ncatastrophic forgetting in neural networks, that arises when incrementally\nlearning a sequence of tasks. Current CL methods function within the confines\nof limited data access, without any restrictions imposed on computational\nresources. However, in real-world scenarios, the latter takes precedence as\ndeployed systems are often computationally constrained. A major drawback of\nmost CL methods is the need to retrain the entire model for each new task. The\ncomputational demands of retraining large models can be prohibitive, limiting\nthe applicability of CL in environments with limited resources. Through CLoRA,\nwe explore the applicability of Low-Rank Adaptation (LoRA), a\nparameter-efficient fine-tuning method for class-incremental semantic\nsegmentation. CLoRA leverages a small set of parameters of the model and uses\nthe same set for learning across all tasks. Results demonstrate the efficacy of\nCLoRA, achieving performance on par with and exceeding the baseline methods. We\nfurther evaluate CLoRA using NetScore, underscoring the need to factor in\nresource efficiency and evaluate CL methods beyond task performance. CLoRA\nsignificantly reduces the hardware requirements for training, making it\nwell-suited for CL in resource-constrained environments after deployment.", "AI": {"tldr": "CLoRA\u5229\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u6301\u7eed\u5b66\u4e60\u7684\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u6bd4\u6570\u636e\u8bbf\u95ee\u9650\u5236\u66f4\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\uff0c\u4ec5\u8c03\u6574\u5c11\u91cf\u53c2\u6570\uff0c\u8de8\u4efb\u52a1\u5171\u4eab\u540c\u4e00\u7ec4\u53c2\u6570\u3002", "result": "CLoRA\u6027\u80fd\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u9700\u6c42\u3002", "conclusion": "CLoRA\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff0c\u5f3a\u8c03\u8bc4\u4f30\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u65f6\u9700\u8003\u8651\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2507.19894", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19894", "abs": "https://arxiv.org/abs/2507.19894", "authors": ["Xiaohua Feng", "Jiaming Zhang", "Fengyuan Yu", "Chengye Wang", "Li Zhang", "Kaixiang Li", "Yuyuan Li", "Chaochao Chen", "Jianwei Yin"], "title": "A Survey on Generative Model Unlearning: Fundamentals, Taxonomy, Evaluation, and Future Direction", "comment": null, "summary": "With the rapid advancement of generative models, associated privacy concerns\nhave attracted growing attention. To address this, researchers have begun\nadapting machine unlearning techniques from traditional classification models\nto generative settings. Although notable progress has been made in this area, a\nunified framework for systematically organizing and integrating existing work\nis still lacking. The substantial differences among current studies in terms of\nunlearning objectives and evaluation protocols hinder the objective and fair\ncomparison of various approaches. While some studies focus on specific types of\ngenerative models, they often overlook the commonalities and systematic\ncharacteristics inherent in Generative Model Unlearning (GenMU). To bridge this\ngap, we provide a comprehensive review of current research on GenMU and propose\na unified analytical framework for categorizing unlearning objectives,\nmethodological strategies, and evaluation metrics. In addition, we explore the\nconnections between GenMU and related techniques, including model editing,\nreinforcement learning from human feedback, and controllable generation. We\nfurther highlight the potential practical value of unlearning techniques in\nreal-world applications. Finally, we identify key challenges and outline future\nresearch directions aimed at laying a solid foundation for further advancements\nin this field. We consistently maintain the related open-source materials at\nhttps://github.com/caxLee/Generative-model-unlearning-survey.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u751f\u6210\u6a21\u578b\u9057\u5fd8\uff08GenMU\uff09\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u9057\u5fd8\u6280\u672f\u6846\u67b6\u4ee5\u6574\u5408\u73b0\u6709\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5168\u9762\u56de\u987eGenMU\u7814\u7a76\uff0c\u63d0\u51fa\u5206\u7c7b\u9057\u5fd8\u76ee\u6807\u3001\u65b9\u6cd5\u7b56\u7565\u548c\u8bc4\u4f30\u6307\u6807\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u76f8\u5173\u6280\u672f\u8054\u7cfb\u3002", "result": "\u5efa\u7acb\u4e86GenMU\u7684\u7cfb\u7edf\u5316\u5206\u7c7b\u6846\u67b6\uff0c\u660e\u786e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "GenMU\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u7edf\u4e00\u6807\u51c6\uff0c\u672a\u6765\u5e94\u5173\u6ce8\u6280\u672f\u6574\u5408\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.19964", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19964", "abs": "https://arxiv.org/abs/2507.19964", "authors": ["Kunhao Li", "Di Wu", "Jun Bai", "Jing Xu", "Lei Yang", "Ziyi Zhang", "Yiliao Song", "Wencheng Yang", "Taotao Cai", "Yan Li"], "title": "Who Owns This Sample: Cross-Client Membership Inference Attack in Federated Graph Neural Networks", "comment": null, "summary": "Graph-structured data is prevalent in many real-world applications, including\nsocial networks, financial systems, and molecular biology. Graph Neural\nNetworks (GNNs) have become the de facto standard for learning from such data\ndue to their strong representation capabilities. As GNNs are increasingly\ndeployed in federated learning (FL) settings to preserve data locality and\nprivacy, new privacy threats arise from the interaction between graph\nstructures and decentralized training. In this paper, we present the first\nsystematic study of cross-client membership inference attacks (CC-MIA) against\nnode classification tasks of federated GNNs (FedGNNs), where a malicious client\naims to infer which client owns the given data. Unlike prior\ncentralized-focused work that focuses on whether a sample was included in\ntraining, our attack targets sample-to-client attribution, a finer-grained\nprivacy risk unique to federated settings. We design a general attack framework\nthat exploits FedGNNs' aggregation behaviors, gradient updates, and embedding\nproximity to link samples to their source clients across training rounds. We\nevaluate our attack across multiple graph datasets under realistic FL setups.\nResults show that our method achieves high performance on both membership\ninference and ownership identification. Our findings highlight a new privacy\nthreat in federated graph learning-client identity leakage through structural\nand model-level cues, motivating the need for attribution-robust GNN design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8054\u90a6\u56fe\u795e\u7ecf\u7f51\u7edc\uff08FedGNNs\uff09\u4e2d\u7684\u8de8\u5ba2\u6237\u7aef\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08CC-MIA\uff09\uff0c\u63ed\u793a\u4e86\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u6837\u672c\u4e0e\u5ba2\u6237\u7aef\u5f52\u5c5e\u7684\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9690\u79c1\u5a01\u80c1\u56e0\u56fe\u7ed3\u6784\u4e0e\u5206\u6563\u8bad\u7ec3\u7684\u4ea4\u4e92\u800c\u589e\u52a0\uff0c\u9700\u7814\u7a76\u65b0\u7684\u653b\u51fb\u65b9\u5f0f\u3002", "method": "\u8bbe\u8ba1\u4e86\u5229\u7528FedGNNs\u805a\u5408\u884c\u4e3a\u3001\u68af\u5ea6\u66f4\u65b0\u548c\u5d4c\u5165\u76f8\u4f3c\u6027\u7684\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u8f6e\u6b21\u5c06\u6837\u672c\u94fe\u63a5\u5230\u6e90\u5ba2\u6237\u7aef\u3002", "result": "\u5728\u591a\u79cd\u56fe\u6570\u636e\u96c6\u548c\u5b9e\u9645FL\u8bbe\u7f6e\u4e0b\uff0c\u653b\u51fb\u65b9\u6cd5\u5728\u6210\u5458\u63a8\u7406\u548c\u6240\u6709\u6743\u8bc6\u522b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8054\u90a6\u56fe\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u8eab\u4efd\u6cc4\u9732\u7684\u65b0\u9690\u79c1\u5a01\u80c1\uff0c\u547c\u5401\u8bbe\u8ba1\u66f4\u5177\u9c81\u68d2\u6027\u7684GNN\u4ee5\u5e94\u5bf9\u6b64\u7c7b\u98ce\u9669\u3002"}}
{"id": "2507.20008", "categories": ["cs.LG", "cs.AI", "62M10 (Primary), 62H30 (Secondary)", "I.2.6; I.5.1; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.20008", "abs": "https://arxiv.org/abs/2507.20008", "authors": ["Padmavathi Moorthy"], "title": "Robust Taxi Fare Prediction Under Noisy Conditions: A Comparative Study of GAT, TimesNet, and XGBoost", "comment": "10 pages, 9 figures, prepared with LaTeX, GitHub link:\n  https://github.com/padmavathi026/Smart-Fare-Prediction", "summary": "Precise fare prediction is crucial in ride-hailing platforms and urban\nmobility systems. This study examines three machine learning models-Graph\nAttention Networks (GAT), XGBoost, and TimesNet to evaluate their predictive\ncapabilities for taxi fares using a real-world dataset comprising over 55\nmillion records. Both raw (noisy) and denoised versions of the dataset are\nanalyzed to assess the impact of data quality on model performance. The study\nevaluated the models along multiple axes, including predictive accuracy,\ncalibration, uncertainty estimation, out-of-distribution (OOD) robustness, and\nfeature sensitivity. We also explore pre-processing strategies, including KNN\nimputation, Gaussian noise injection, and autoencoder-based denoising. The\nstudy reveals critical differences between classical and deep learning models\nunder realistic conditions, offering practical guidelines for building robust\nand scalable models in urban fare prediction systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08GAT\u3001XGBoost\u548cTimesNet\uff09\u5728\u51fa\u79df\u8f66\u8d39\u7528\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u4e86\u9884\u5904\u7406\u7b56\u7565\u3002", "motivation": "\u7cbe\u786e\u7684\u8d39\u7528\u9884\u6d4b\u5bf9\u7f51\u7ea6\u8f66\u5e73\u53f0\u548c\u57ce\u5e02\u4ea4\u901a\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u5305\u542b5500\u4e07\u6761\u8bb0\u5f55\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u6bd4\u8f83GAT\u3001XGBoost\u548cTimesNet\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u5206\u6790\u6570\u636e\u9884\u5904\u7406\uff08\u5982KNN\u63d2\u8865\u3001\u9ad8\u65af\u566a\u58f0\u6ce8\u5165\u548c\u81ea\u7f16\u7801\u5668\u53bb\u566a\uff09\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u7ecf\u5178\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u5173\u952e\u5dee\u5f02\uff0c\u4e3a\u6784\u5efa\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u57ce\u5e02\u8d39\u7528\u9884\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002", "conclusion": "\u7814\u7a76\u4e3a\u57ce\u5e02\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u8d39\u7528\u9884\u6d4b\u63d0\u4f9b\u4e86\u6a21\u578b\u9009\u62e9\u548c\u9884\u5904\u7406\u7684\u5b9e\u7528\u5efa\u8bae\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.20016", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.1"], "pdf": "https://arxiv.org/pdf/2507.20016", "abs": "https://arxiv.org/abs/2507.20016", "authors": ["Liu junkang", "Yuanyuan Liu", "Fanhua Shang", "Hongying Liu", "Jin Liu", "Wei Feng"], "title": "FedSWA: Improving Generalization in Federated Learning with Highly Heterogeneous Data via Momentum-Based Stochastic Controlled Weight Averaging", "comment": "icml 2025", "summary": "For federated learning (FL) algorithms such as FedSAM, their generalization\ncapability is crucial for real-word applications. In this paper, we revisit the\ngeneralization problem in FL and investigate the impact of data heterogeneity\non FL generalization. We find that FedSAM usually performs worse than FedAvg in\nthe case of highly heterogeneous data, and thus propose a novel and effective\nfederated learning algorithm with Stochastic Weight Averaging (called\n\\texttt{FedSWA}), which aims to find flatter minima in the setting of highly\nheterogeneous data. Moreover, we introduce a new momentum-based stochastic\ncontrolled weight averaging FL algorithm (\\texttt{FedMoSWA}), which is designed\nto better align local and global models.\n  Theoretically, we provide both convergence analysis and generalization bounds\nfor \\texttt{FedSWA} and \\texttt{FedMoSWA}. We also prove that the optimization\nand generalization errors of \\texttt{FedMoSWA} are smaller than those of their\ncounterparts, including FedSAM and its variants. Empirically, experimental\nresults on CIFAR10/100 and Tiny ImageNet demonstrate the superiority of the\nproposed algorithms compared to their counterparts. Open source code at:\nhttps://github.com/junkangLiu0/FedSWA.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u6784\u6027\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7b97\u6cd5\uff08FedSWA\u548cFedMoSWA\uff09\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u8054\u90a6\u5b66\u4e60\uff08\u5982FedSAM\uff09\u5728\u9ad8\u5ea6\u5f02\u6784\u6570\u636e\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u53d1\u73b0\u5176\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faFedSWA\uff08\u57fa\u4e8e\u968f\u673a\u6743\u91cd\u5e73\u5747\uff09\u548cFedMoSWA\uff08\u52a8\u91cf\u63a7\u5236\u6743\u91cd\u5e73\u5747\uff09\u7b97\u6cd5\uff0c\u4ee5\u5bfb\u627e\u66f4\u5e73\u5766\u7684\u6700\u5c0f\u503c\u5e76\u4f18\u5316\u6a21\u578b\u5bf9\u9f50\u3002", "result": "\u7406\u8bba\u8bc1\u660eFedMoSWA\u7684\u4f18\u5316\u548c\u6cdb\u5316\u8bef\u5dee\u66f4\u5c0f\uff1b\u5b9e\u9a8c\u5728CIFAR10/100\u548cTiny ImageNet\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "FedSWA\u548cFedMoSWA\u5728\u5f02\u6784\u6570\u636e\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.20051", "categories": ["cs.LG", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.20051", "abs": "https://arxiv.org/abs/2507.20051", "authors": ["Weicong Chen", "Vikash Singh", "Zahra Rahmani", "Debargha Ganguly", "Mohsen Hariri", "Vipin Chaudhary"], "title": "$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning", "comment": null, "summary": "Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on\nerror-prone parsing, and use unrealistic evaluation protocols. We introduce\n$K^4$, an unsupervised and parser-independent framework for high-performance\nonline detection. $K^4$ transforms arbitrary log embeddings into compact\nfour-dimensional descriptors (Precision, Recall, Density, Coverage) using\nefficient k-nearest neighbor (k-NN) statistics. These descriptors enable\nlightweight detectors to accurately score anomalies without retraining. Using a\nmore realistic online evaluation protocol, $K^4$ sets a new state-of-the-art\n(AUROC: 0.995-0.999), outperforming baselines by large margins while being\norders of magnitude faster, with training under 4 seconds and inference as low\nas 4 $\\mu$s.", "AI": {"tldr": "$K^4$\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u3001\u72ec\u7acb\u4e8e\u89e3\u6790\u5668\u7684\u9ad8\u6027\u80fd\u5728\u7ebf\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u7ef4\u63cf\u8ff0\u7b26\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901f\u5ea6\u6162\u3001\u4f9d\u8d56\u6613\u51fa\u9519\u7684\u89e3\u6790\uff0c\u4e14\u8bc4\u4f30\u534f\u8bae\u4e0d\u73b0\u5b9e\u3002", "method": "$K^4$\u5c06\u65e5\u5fd7\u5d4c\u5165\u8f6c\u6362\u4e3a\u56db\u7ef4\u63cf\u8ff0\u7b26\uff08\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u3001\u5bc6\u5ea6\u3001\u8986\u76d6\u7387\uff09\uff0c\u57fa\u4e8ek-NN\u7edf\u8ba1\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u3002", "result": "$K^4$\u5728\u5728\u7ebf\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff08AUROC: 0.995-0.999\uff09\uff0c\u901f\u5ea6\u5feb\uff08\u8bad\u7ec3<4\u79d2\uff0c\u63a8\u7406\u4f4e\u81f34\u03bcs\uff09\u3002", "conclusion": "$K^4$\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2507.20057", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20057", "abs": "https://arxiv.org/abs/2507.20057", "authors": ["Clare Lyle", "Gharda Sokar", "Razvan Pascanu", "Andras Gyorgy"], "title": "What Can Grokking Teach Us About Learning Under Nonstationarity?", "comment": null, "summary": "In continual learning problems, it is often necessary to overwrite components\nof a neural network's learned representation in response to changes in the data\nstream; however, neural networks often exhibit \\primacy bias, whereby early\ntraining data hinders the network's ability to generalize on later tasks. While\nfeature-learning dynamics of nonstationary learning problems are not well\nstudied, the emergence of feature-learning dynamics is known to drive the\nphenomenon of grokking, wherein neural networks initially memorize their\ntraining data and only later exhibit perfect generalization. This work\nconjectures that the same feature-learning dynamics which facilitate\ngeneralization in grokking also underlie the ability to overwrite previous\nlearned features as well, and methods which accelerate grokking by facilitating\nfeature-learning dynamics are promising candidates for addressing primacy bias\nin non-stationary learning problems. We then propose a straightforward method\nto induce feature-learning dynamics as needed throughout training by increasing\nthe effective learning rate, i.e. the ratio between parameter and update norms.\nWe show that this approach both facilitates feature-learning and improves\ngeneralization in a variety of settings, including grokking, warm-starting\nneural network training, and reinforcement learning tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u5f81\u5b66\u4e60\u52a8\u6001\u5982\u4f55\u5e2e\u52a9\u514b\u670d\u65e9\u671f\u6570\u636e\u7684\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8c03\u6574\u6709\u6548\u5b66\u4e60\u7387\u6765\u4fc3\u8fdb\u7279\u5f81\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u4e2d\uff0c\u795e\u7ecf\u7f51\u7edc\u5bb9\u6613\u56e0\u65e9\u671f\u6570\u636e\u504f\u89c1\uff08primacy bias\uff09\u800c\u96be\u4ee5\u9002\u5e94\u540e\u7eed\u4efb\u52a1\uff0c\u800c\u7279\u5f81\u5b66\u4e60\u52a8\u6001\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4e2d\u53ef\u80fd\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u589e\u52a0\u6709\u6548\u5b66\u4e60\u7387\uff08\u53c2\u6570\u4e0e\u66f4\u65b0\u8303\u6570\u7684\u6bd4\u7387\uff09\u6765\u8bf1\u5bfc\u7279\u5f81\u5b66\u4e60\u52a8\u6001\uff0c\u4ece\u800c\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\uff08\u5982grokking\u3001\u795e\u7ecf\u7f51\u7edc\u9884\u70ed\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff09\u4e2d\u5747\u80fd\u4fc3\u8fdb\u7279\u5f81\u5b66\u4e60\u5e76\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u7279\u5f81\u5b66\u4e60\u52a8\u6001\u662f\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u65e9\u671f\u6570\u636e\u504f\u89c1\u7684\u6709\u6548\u9014\u5f84\uff0c\u8c03\u6574\u6709\u6548\u5b66\u4e60\u7387\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u901a\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.20060", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.20060", "abs": "https://arxiv.org/abs/2507.20060", "authors": ["Nomaan A. Kherani", "Urbashi Mitra"], "title": "ModShift: Model Privacy via Designed Shifts", "comment": "To appear in the 2025 Asilomar Conference on Signals, Systems and\n  Computers", "summary": "In this paper, shifts are introduced to preserve model privacy against an\neavesdropper in federated learning. Model learning is treated as a parameter\nestimation problem. This perspective allows us to derive the Fisher Information\nmatrix of the model updates from the shifted updates and drive them to\nsingularity, thus posing a hard estimation problem for Eve. The shifts are\nsecurely shared with the central server to maintain model accuracy at the\nserver and participating devices. A convergence test is proposed to detect if\nmodel updates have been tampered with and we show that our scheme passes this\ntest. Numerical results show that our scheme achieves a higher model shift when\ncompared to a noise injection scheme while requiring a lesser bandwidth secret\nchannel.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u53c2\u6570\u504f\u79fb\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u4e2d\u6a21\u578b\u9690\u79c1\u7684\u65b9\u6cd5\uff0c\u5229\u7528Fisher\u4fe1\u606f\u77e9\u9635\u4f7f\u7a83\u542c\u8005\u96be\u4ee5\u4f30\u8ba1\u6a21\u578b\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u6a21\u578b\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u9632\u6b62\u7a83\u542c\u8005\u901a\u8fc7\u6a21\u578b\u66f4\u65b0\u63a8\u65ad\u654f\u611f\u4fe1\u606f\u3002", "method": "\u5c06\u6a21\u578b\u5b66\u4e60\u89c6\u4e3a\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u6570\u504f\u79fb\u4f7fFisher\u4fe1\u606f\u77e9\u9635\u5947\u5f02\u5316\uff0c\u540c\u65f6\u5b89\u5168\u5171\u4eab\u504f\u79fb\u4ee5\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002", "result": "\u65b9\u6848\u5728\u6570\u503c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6a21\u578b\u504f\u79fb\u6548\u679c\uff0c\u4e14\u6240\u9700\u5e26\u5bbd\u66f4\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4fdd\u62a4\u9690\u79c1\uff0c\u540c\u65f6\u901a\u8fc7\u6536\u655b\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5b89\u5168\u6027\u3002"}}
{"id": "2507.20061", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.20061", "abs": "https://arxiv.org/abs/2507.20061", "authors": ["Saba Ahmadi", "Avrim Blum", "Haifeng Xu", "Fan Yao"], "title": "Strategic Filtering for Content Moderation: Free Speech or Free of Distortion?", "comment": null, "summary": "User-generated content (UGC) on social media platforms is vulnerable to\nincitements and manipulations, necessitating effective regulations. To address\nthese challenges, those platforms often deploy automated content moderators\ntasked with evaluating the harmfulness of UGC and filtering out content that\nviolates established guidelines. However, such moderation inevitably gives rise\nto strategic responses from users, who strive to express themselves within the\nconfines of guidelines. Such phenomena call for a careful balance between: 1.\nensuring freedom of speech -- by minimizing the restriction of expression; and\n2. reducing social distortion -- measured by the total amount of content\nmanipulation. We tackle the problem of optimizing this balance through the lens\nof mechanism design, aiming at optimizing the trade-off between minimizing\nsocial distortion and maximizing free speech. Although determining the optimal\ntrade-off is NP-hard, we propose practical methods to approximate the optimal\nsolution. Additionally, we provide generalization guarantees determining the\namount of finite offline data required to approximate the optimal moderator\neffectively.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u751f\u6210\u5185\u5bb9\uff08UGC\uff09\u7684\u76d1\u7ba1\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u673a\u5236\u8bbe\u8ba1\u4f18\u5316\u8a00\u8bba\u81ea\u7531\u4e0e\u5185\u5bb9\u64cd\u7eb5\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u5e76\u63d0\u4f9b\u4e86\u8fd1\u4f3c\u6700\u4f18\u89e3\u7684\u5b9e\u7528\u65b9\u6cd5\u53ca\u6cdb\u5316\u4fdd\u8bc1\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u7528\u6237\u751f\u6210\u5185\u5bb9\u5bb9\u6613\u53d7\u5230\u717d\u52a8\u548c\u64cd\u7eb5\uff0c\u9700\u8981\u6709\u6548\u76d1\u7ba1\u3002\u81ea\u52a8\u5316\u5185\u5bb9\u5ba1\u6838\u867d\u80fd\u8fc7\u6ee4\u8fdd\u89c4\u5185\u5bb9\uff0c\u4f46\u4f1a\u5f15\u53d1\u7528\u6237\u7684\u7b56\u7565\u6027\u56de\u5e94\uff0c\u9700\u5e73\u8861\u8a00\u8bba\u81ea\u7531\u4e0e\u51cf\u5c11\u793e\u4f1a\u626d\u66f2\u3002", "method": "\u901a\u8fc7\u673a\u5236\u8bbe\u8ba1\u4f18\u5316\u8a00\u8bba\u81ea\u7531\u4e0e\u5185\u5bb9\u64cd\u7eb5\u7684\u6743\u8861\uff0c\u63d0\u51fa\u8fd1\u4f3c\u6700\u4f18\u89e3\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u6cdb\u5316\u4fdd\u8bc1\u4ee5\u786e\u5b9a\u6240\u9700\u79bb\u7ebf\u6570\u636e\u91cf\u3002", "result": "\u5c3d\u7ba1\u6700\u4f18\u6743\u8861\u95ee\u9898\u662fNP\u96be\u7684\uff0c\u4f46\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u8fd1\u4f3c\u6700\u4f18\u89e3\uff0c\u5e76\u91cf\u5316\u4e86\u6240\u9700\u79bb\u7ebf\u6570\u636e\u7684\u89c4\u6a21\u3002", "conclusion": "\u7814\u7a76\u4e3a\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u4f18\u5316\u4e86\u8a00\u8bba\u81ea\u7531\u4e0e\u793e\u4f1a\u626d\u66f2\u7684\u5e73\u8861\u3002"}}
{"id": "2507.20065", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20065", "abs": "https://arxiv.org/abs/2507.20065", "authors": ["Xinyi Li", "Zongyi Li", "Nikola Kovachki", "Anima Anandkumar"], "title": "Geometric Operator Learning with Optimal Transport", "comment": null, "summary": "We propose integrating optimal transport (OT) into operator learning for\npartial differential equations (PDEs) on complex geometries. Classical\ngeometric learning methods typically represent domains as meshes, graphs, or\npoint clouds. Our approach generalizes discretized meshes to mesh density\nfunctions, formulating geometry embedding as an OT problem that maps these\nfunctions to a uniform density in a reference space. Compared to previous\nmethods relying on interpolation or shared deformation, our OT-based method\nemploys instance-dependent deformation, offering enhanced flexibility and\neffectiveness. For 3D simulations focused on surfaces, our OT-based neural\noperator embeds the surface geometry into a 2D parameterized latent space. By\nperforming computations directly on this 2D representation of the surface\nmanifold, it achieves significant computational efficiency gains compared to\nvolumetric simulation. Experiments with Reynolds-averaged Navier-Stokes\nequations (RANS) on the ShapeNet-Car and DrivAerNet-Car datasets show that our\nmethod achieves better accuracy and also reduces computational expenses in\nterms of both time and memory usage compared to existing machine learning\nmodels. Additionally, our model demonstrates significantly improved accuracy on\nthe FlowBench dataset, underscoring the benefits of employing\ninstance-dependent deformation for datasets with highly variable geometries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u878d\u5165\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7b97\u5b50\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u51e0\u4f55\u5d4c\u5165\u95ee\u9898\u8f6c\u5316\u4e3aOT\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u51e0\u4f55\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c06\u57df\u8868\u793a\u4e3a\u7f51\u683c\u3001\u56fe\u6216\u70b9\u4e91\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7OT\u65b9\u6cd5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u51e0\u4f55\u5d4c\u5165\u3002", "method": "\u5c06\u79bb\u6563\u7f51\u683c\u63a8\u5e7f\u4e3a\u7f51\u683c\u5bc6\u5ea6\u51fd\u6570\uff0c\u901a\u8fc7OT\u95ee\u9898\u5c06\u5176\u6620\u5c04\u5230\u53c2\u8003\u7a7a\u95f4\u4e2d\u7684\u5747\u5300\u5bc6\u5ea6\u3002\u57283D\u6a21\u62df\u4e2d\uff0c\u5c06\u8868\u9762\u51e0\u4f55\u5d4c\u51652D\u53c2\u6570\u5316\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728RANS\u65b9\u7a0b\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff08\u65f6\u95f4\u548c\u5185\u5b58\uff09\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u5e76\u5728FlowBench\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u57fa\u4e8eOT\u7684\u5b9e\u4f8b\u4f9d\u8d56\u53d8\u5f62\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u51e0\u4f55\u65f6\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u63d0\u5347\u7cbe\u5ea6\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.20072", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.20072", "abs": "https://arxiv.org/abs/2507.20072", "authors": ["Jiaqiang Li", "Jianbin Tan", "Xueqin Wang"], "title": "Sparse Equation Matching: A Derivative-Free Learning for General-Order Dynamical Systems", "comment": null, "summary": "Equation discovery is a fundamental learning task for uncovering the\nunderlying dynamics of complex systems, with wide-ranging applications in areas\nsuch as brain connectivity analysis, climate modeling, gene regulation, and\nphysical system simulation. However, many existing approaches rely on accurate\nderivative estimation and are limited to first-order dynamical systems,\nrestricting their applicability to real-world scenarios. In this work, we\npropose sparse equation matching (SEM), a unified framework that encompasses\nseveral existing equation discovery methods under a common formulation. SEM\nintroduces an integral-based sparse regression method using Green's functions,\nenabling derivative-free estimation of differential operators and their\nassociated driving functions in general-order dynamical systems. The\neffectiveness of SEM is demonstrated through extensive simulations,\nbenchmarking its performance against derivative-based approaches. We then apply\nSEM to electroencephalographic (EEG) data recorded during multiple oculomotor\ntasks, collected from 52 participants in a brain-computer interface experiment.\nOur method identifies active brain regions across participants and reveals\ntask-specific connectivity patterns. These findings offer valuable insights\ninto brain connectivity and the underlying neural mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7a00\u758f\u65b9\u7a0b\u5339\u914d\uff08SEM\uff09\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u5bfc\u6570\u4f30\u8ba1\u5fae\u5206\u7b97\u5b50\u53ca\u5176\u9a71\u52a8\u51fd\u6570\uff0c\u9002\u7528\u4e8e\u4e00\u822c\u9636\u52a8\u6001\u7cfb\u7edf\uff0c\u5e76\u5728\u8111\u7535\u56fe\u6570\u636e\u5206\u6790\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u7684\u5bfc\u6570\u4f30\u8ba1\u4e14\u4ec5\u9002\u7528\u4e8e\u4e00\u9636\u52a8\u6001\u7cfb\u7edf\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u79ef\u5206\u7684\u7a00\u758f\u56de\u5f52\u65b9\u6cd5\uff0c\u5229\u7528\u683c\u6797\u51fd\u6570\u5b9e\u73b0\u65e0\u5bfc\u6570\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u8111\u7535\u56fe\u6570\u636e\u5206\u6790\u9a8c\u8bc1\u4e86SEM\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u4efb\u52a1\u7279\u5b9a\u7684\u8111\u8fde\u63a5\u6a21\u5f0f\u3002", "conclusion": "SEM\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5c24\u5176\u5728\u8111\u8fde\u63a5\u5206\u6790\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2507.20078", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20078", "abs": "https://arxiv.org/abs/2507.20078", "authors": ["Adelaide Danilov", "Aria Nourbakhsh", "Christoph Schommer"], "title": "Cluster Purge Loss: Structuring Transformer Embeddings for Equivalent Mutants Detection", "comment": "11 pages, 6 figures", "summary": "Recent pre-trained transformer models achieve superior performance in various\ncode processing objectives. However, although effective at optimizing decision\nboundaries, common approaches for fine-tuning them for downstream\nclassification tasks - distance-based methods or training an additional\nclassification head - often fail to thoroughly structure the embedding space to\nreflect nuanced intra-class semantic relationships. Equivalent code mutant\ndetection is one of these tasks, where the quality of the embedding space is\ncrucial to the performance of the models. We introduce a novel framework that\nintegrates cross-entropy loss with a deep metric learning objective, termed\nCluster Purge Loss. This objective, unlike conventional approaches,\nconcentrates on adjusting fine-grained differences within each class,\nencouraging the separation of instances based on semantical equivalency to the\nclass center using dynamically adjusted borders. Employing UniXCoder as the\nbase model, our approach demonstrates state-of-the-art performance in the\ndomain of equivalent mutant detection and produces a more interpretable\nembedding space.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u6df1\u5ea6\u5ea6\u91cf\u5b66\u4e60\u76ee\u6807\u7684\u65b0\u6846\u67b6\uff08Cluster Purge Loss\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u4ee3\u7801\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5728\u7b49\u6548\u4ee3\u7801\u53d8\u5f02\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5fae\u8c03\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u65f6\uff0c\u672a\u80fd\u5145\u5206\u7ed3\u6784\u5316\u5d4c\u5165\u7a7a\u95f4\u4ee5\u53cd\u6620\u7c7b\u5185\u8bed\u4e49\u5173\u7cfb\uff0c\u800c\u7b49\u6548\u4ee3\u7801\u53d8\u5f02\u68c0\u6d4b\u4efb\u52a1\u5bf9\u5d4c\u5165\u7a7a\u95f4\u8d28\u91cf\u8981\u6c42\u8f83\u9ad8\u3002", "method": "\u63d0\u51faCluster Purge Loss\uff0c\u7ed3\u5408\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u6df1\u5ea6\u5ea6\u91cf\u5b66\u4e60\u76ee\u6807\uff0c\u52a8\u6001\u8c03\u6574\u7c7b\u5185\u8fb9\u754c\u4ee5\u4f18\u5316\u5d4c\u5165\u7a7a\u95f4\u3002\u4ee5UniXCoder\u4e3a\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728\u7b49\u6548\u53d8\u5f02\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u751f\u6210\u66f4\u53ef\u89e3\u91ca\u7684\u5d4c\u5165\u7a7a\u95f4\u3002", "conclusion": "\u65b0\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b49\u6548\u4ee3\u7801\u53d8\u5f02\u68c0\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.20096", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05"], "pdf": "https://arxiv.org/pdf/2507.20096", "abs": "https://arxiv.org/abs/2507.20096", "authors": ["Xin Gao", "Xingming Xu"], "title": "EcoTransformer: Attention without Multiplication", "comment": "8 pages, 1 figure", "summary": "The Transformer, with its scaled dot-product attention mechanism, has become\na foundational architecture in modern AI. However, this mechanism is\ncomputationally intensive and incurs substantial energy costs. We propose a new\nTransformer architecture EcoTransformer, in which the output context vector is\nconstructed as the convolution of the values using a Laplacian kernel, where\nthe distances are measured by the L1 metric between the queries and keys.\nCompared to dot-product based attention, the new attention score calculation is\nfree of matrix multiplication. It performs on par with, or even surpasses,\nscaled dot-product attention in NLP, bioinformatics, and vision tasks, while\nconsuming significantly less energy.", "AI": {"tldr": "EcoTransformer\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u548cL1\u8ddd\u79bb\u7684\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u66ff\u4ee3\u4f20\u7edf\u70b9\u79ef\u6ce8\u610f\u529b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u80fd\u8017\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edfTransformer\u7684\u70b9\u79ef\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u5bc6\u96c6\u4e14\u80fd\u8017\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Laplacian\u6838\u5377\u79ef\u503c\u5411\u91cf\uff0c\u901a\u8fc7L1\u8ddd\u79bb\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\uff0c\u907f\u514d\u77e9\u9635\u4e58\u6cd5\u3002", "result": "\u5728NLP\u3001\u751f\u7269\u4fe1\u606f\u5b66\u548c\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u6216\u4f18\u4e8e\u4f20\u7edf\u6ce8\u610f\u529b\uff0c\u540c\u65f6\u80fd\u8017\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "EcoTransformer\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684Transformer\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u80fd\u8017\u654f\u611f\u573a\u666f\u3002"}}
{"id": "2507.20114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20114", "abs": "https://arxiv.org/abs/2507.20114", "authors": ["Jianping Yao", "Son N. Tran", "Hieu Nguyen", "Samantha Sawyer", "Rocco Longo"], "title": "Wine Characterisation with Spectral Information and Predictive Artificial Intelligence", "comment": null, "summary": "The purpose of this paper is to use absorbance data obtained by human tasting\nand an ultraviolet-visible (UV-Vis) scanning spectrophotometer to predict the\nattributes of grape juice (GJ) and to classify the wine's origin, respectively.\nThe approach combined machine learning (ML) techniques with spectroscopy to\nfind a relatively simple way to apply them in two stages of winemaking and help\nimprove the traditional wine analysis methods regarding sensory data and wine's\norigins. This new technique has overcome the disadvantages of the complex\nsensors by taking advantage of spectral fingerprinting technology and forming a\ncomprehensive study of the employment of AI in the wine analysis domain. In the\nresults, Support Vector Machine (SVM) was the most efficient and robust in both\nattributes and origin prediction tasks. Both the accuracy and F1 score of the\norigin prediction exceed 91%. The feature ranking approach found that the more\ninfluential wavelengths usually appear at the lower end of the scan range, 250\nnm (nanometers) to 420 nm, which is believed to be of great help for selecting\nappropriate validation methods and sensors to extract wine data in future\nresearch. The knowledge of this research provides new ideas and early solutions\nfor the wine industry or other beverage industries to integrate big data and\nIoT in the future, which significantly promotes the development of 'Smart\nWineries'.", "AI": {"tldr": "\u8bba\u6587\u5229\u7528\u7d2b\u5916-\u53ef\u89c1\u5149\u8c31\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u9884\u6d4b\u8461\u8404\u6c41\u5c5e\u6027\u5e76\u5206\u7c7b\u8461\u8404\u9152\u4ea7\u5730\uff0c\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u548cF1\u5206\u6570\u5747\u8d85\u8fc791%\u3002", "motivation": "\u6539\u8fdb\u4f20\u7edf\u8461\u8404\u9152\u5206\u6790\u65b9\u6cd5\uff0c\u7ed3\u5408\u5149\u8c31\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\uff0c\u4e3a\u8461\u8404\u9152\u884c\u4e1a\u63d0\u4f9b\u66f4\u7b80\u5355\u3001\u9ad8\u6548\u7684\u6570\u636e\u5206\u6790\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u7d2b\u5916-\u53ef\u89c1\u5149\u8c31\u626b\u63cf\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5206\u4e24\u9636\u6bb5\u5206\u6790\u8461\u8404\u6c41\u5c5e\u6027\u548c\u8461\u8404\u9152\u4ea7\u5730\u5206\u7c7b\u3002", "result": "SVM\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u51c6\u786e\u7387\u548cF1\u5206\u6570\u5747\u8d85\u8fc791%\uff1b\u5173\u952e\u6ce2\u957f\u96c6\u4e2d\u5728250-420 nm\u8303\u56f4\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u8461\u8404\u9152\u884c\u4e1a\u6574\u5408\u5927\u6570\u636e\u548c\u7269\u8054\u7f51\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u2018\u667a\u80fd\u9152\u5e84\u2019\u53d1\u5c55\u3002"}}
{"id": "2507.20127", "categories": ["cs.LG", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.20127", "abs": "https://arxiv.org/abs/2507.20127", "authors": ["Xuanting Xie", "Bingheng Li", "Erlin Pan", "Zhao Kang", "Wenyu Chen"], "title": "Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing", "comment": "11 pages, 6 figures", "summary": "Graph Neural Networks (GNNs) have become a dominant approach to learning\ngraph representations, primarily because of their message-passing mechanisms.\nHowever, GNNs typically adopt a fixed aggregator function such as Mean, Max, or\nSum without principled reasoning behind the selection. This rigidity,\nespecially in the presence of heterophily, often leads to poor, problem\ndependent performance. Although some attempts address this by designing more\nsophisticated aggregation functions, these methods tend to rely heavily on\nlabeled data, which is often scarce in real-world tasks. In this work, we\npropose a novel unsupervised framework, \"Aggregation-aware Multilayer\nPerceptron\" (AMLP), which shifts the paradigm from directly crafting\naggregation functions to making MLP adaptive to aggregation. Our lightweight\napproach consists of two key steps: First, we utilize a graph reconstruction\nmethod that facilitates high-order grouping effects, and second, we employ a\nsingle-layer network to encode varying degrees of heterophily, thereby\nimproving the capacity and applicability of the model. Extensive experiments on\nnode clustering and classification demonstrate the superior performance of\nAMLP, highlighting its potential for diverse graph learning scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAMLP\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u4f7fMLP\u9002\u5e94\u805a\u5408\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86GNN\u4e2d\u56fa\u5b9a\u805a\u5408\u51fd\u6570\u5728\u5f02\u8d28\u6027\u4e0b\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "GNN\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u805a\u5408\u51fd\u6570\uff08\u5982Mean\u3001Max\u3001Sum\uff09\uff0c\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\uff0c\u5c24\u5176\u5728\u5f02\u8d28\u6027\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u73b0\u5b9e\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3002", "method": "AMLP\u6846\u67b6\u5206\u4e3a\u4e24\u6b65\uff1a1\uff09\u5229\u7528\u56fe\u91cd\u6784\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u9636\u5206\u7ec4\u6548\u5e94\uff1b2\uff09\u4f7f\u7528\u5355\u5c42\u7f51\u7edc\u7f16\u7801\u4e0d\u540c\u7a0b\u5ea6\u7684\u5f02\u8d28\u6027\u3002", "result": "\u5728\u8282\u70b9\u805a\u7c7b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAMLP\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "AMLP\u4e3a\u591a\u6837\u5316\u7684\u56fe\u5b66\u4e60\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20130", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.20130", "abs": "https://arxiv.org/abs/2507.20130", "authors": ["Yi He", "Ailun Wang", "Zhi Wang", "Yu Liu", "Xingyuan Xu", "Wen Yan"], "title": "Generative molecule evolution using 3D pharmacophore for efficient Structure-Based Drug Design", "comment": null, "summary": "Recent advances in generative models, particularly diffusion and\nauto-regressive models, have revolutionized fields like computer vision and\nnatural language processing. However, their application to structure-based drug\ndesign (SBDD) remains limited due to critical data constraints. To address the\nlimitation of training data for models targeting SBDD tasks, we propose an\nevolutionary framework named MEVO, which bridges the gap between billion-scale\nsmall molecule dataset and the scarce protein-ligand complex dataset, and\neffectively increase the abundance of training data for generative SBDD models.\nMEVO is composed of three key components: a high-fidelity VQ-VAE for molecule\nrepresentation in latent space, a diffusion model for pharmacophore-guided\nmolecule generation, and a pocket-aware evolutionary strategy for molecule\noptimization with physics-based scoring function. This framework efficiently\ngenerate high-affinity binders for various protein targets, validated with\npredicted binding affinities using free energy perturbation (FEP) methods. In\naddition, we showcase the capability of MEVO in designing potent inhibitors to\nKRAS$^{\\textrm{G12D}}$, a challenging target in cancer therapeutics, with\nsimilar affinity to the known highly active inhibitor evaluated by FEP\ncalculations. With high versatility and generalizability, MEVO offers an\neffective and data-efficient model for various tasks in structure-based ligand\ndesign.", "AI": {"tldr": "MEVO\u662f\u4e00\u4e2a\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5c0f\u5206\u5b50\u6570\u636e\u96c6\u548c\u7a00\u7f3a\u7684\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u7ed3\u6784\u836f\u7269\u8bbe\u8ba1\uff08SBDD\uff09\u4e2d\u7684\u6570\u636e\u9650\u5236\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u91cf\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u9650\u5236\uff0c\u751f\u6210\u6a21\u578b\u5728\u7ed3\u6784\u836f\u7269\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u53d7\u9650\uff0cMEVO\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "MEVO\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u9ad8\u4fdd\u771fVQ-VAE\u3001\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8e\u7269\u7406\u7684\u8bc4\u5206\u51fd\u6570\u7684\u8fdb\u5316\u7b56\u7565\u3002", "result": "MEVO\u6210\u529f\u751f\u6210\u4e86\u9ad8\u4eb2\u548c\u529b\u7ed3\u5408\u7269\uff0c\u5e76\u5728KRASG12D\u6291\u5236\u5242\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "MEVO\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u6570\u636e\u53cb\u597d\u7684\u7ed3\u6784\u914d\u4f53\u8bbe\u8ba1\u6a21\u578b\u3002"}}
{"id": "2507.20144", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20144", "abs": "https://arxiv.org/abs/2507.20144", "authors": ["Zeyi Liu", "Songqiao Hu", "Pengyu Han", "Jiaming Liu", "Xiao He"], "title": "Awesome-OL: An Extensible Toolkit for Online Learning", "comment": "7 pages", "summary": "In recent years, online learning has attracted increasing attention due to\nits adaptive capability to process streaming and non-stationary data. To\nfacilitate algorithm development and practical deployment in this area, we\nintroduce Awesome-OL, an extensible Python toolkit tailored for online learning\nresearch. Awesome-OL integrates state-of-the-art algorithm, which provides a\nunified framework for reproducible comparisons, curated benchmark datasets, and\nmulti-modal visualization. Built upon the scikit-multiflow open-source\ninfrastructure, Awesome-OL emphasizes user-friendly interactions without\ncompromising research flexibility or extensibility. The source code is publicly\navailable at: https://github.com/liuzy0708/Awesome-OL.", "AI": {"tldr": "Awesome-OL\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60\u7814\u7a76\u7684Python\u5de5\u5177\u5305\uff0c\u96c6\u6210\u4e86\u5148\u8fdb\u7b97\u6cd5\u3001\u57fa\u51c6\u6570\u636e\u96c6\u548c\u591a\u6a21\u6001\u53ef\u89c6\u5316\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u56e0\u5176\u5904\u7406\u6d41\u5f0f\u548c\u975e\u5e73\u7a33\u6570\u636e\u7684\u81ea\u9002\u5e94\u80fd\u529b\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u5de5\u5177\u652f\u6301\u7814\u7a76\u548c\u90e8\u7f72\u3002", "method": "\u57fa\u4e8escikit-multiflow\u5f00\u6e90\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u7684\u4ea4\u4e92\u754c\u9762\uff0c\u540c\u65f6\u4fdd\u6301\u7814\u7a76\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "Awesome-OL\u4e3a\u5728\u7ebf\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u6bd4\u8f83\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u516c\u5f00\u4e86\u6e90\u4ee3\u7801\u3002", "conclusion": "Awesome-OL\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u7075\u6d3b\u7684\u5de5\u5177\u5305\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5728\u7ebf\u5b66\u4e60\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2507.20164", "categories": ["cs.LG", "cs.AI", "68T05, 68T07, 62M45"], "pdf": "https://arxiv.org/pdf/2507.20164", "abs": "https://arxiv.org/abs/2507.20164", "authors": ["Jinwook Hong"], "title": "ASNN: Learning to Suggest Neural Architectures from Performance Distributions", "comment": "10 pages", "summary": "The architecture of a neural network (NN) plays a critical role in\ndetermining its performance. However, there is no general closed-form function\nthat maps between network structure and accuracy, making the process of\narchitecture design largely heuristic or search-based. In this study, we\npropose the Architecture Suggesting Neural Network (ASNN), a model designed to\nlearn the relationship between NN architecture and its test accuracy, and to\nsuggest improved architectures accordingly. To train ASNN, we constructed\ndatasets using TensorFlow-based models with varying numbers of layers and\nnodes. Experimental results were collected for both 2-layer and 3-layer\narchitectures across a grid of configurations, each evaluated with 10 repeated\ntrials to account for stochasticity. Accuracy values were treated as inputs,\nand architectural parameters as outputs. The trained ASNN was then used\niteratively to predict architectures that yield higher performance. In both\n2-layer and 3-layer cases, ASNN successfully suggested architectures that\noutperformed the best results found in the original training data. Repeated\nprediction and retraining cycles led to the discovery of architectures with\nimproved mean test accuracies, demonstrating the model's capacity to generalize\nthe performance-structure relationship. These results suggest that ASNN\nprovides an efficient alternative to random search for architecture\noptimization, and offers a promising approach toward automating neural network\ndesign. \"Parts of the manuscript, including text editing and expression\nrefinement, were supported by OpenAI's ChatGPT. All content was reviewed and\nverified by the authors.\"", "AI": {"tldr": "ASNN\u901a\u8fc7\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u4e0e\u6d4b\u8bd5\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u6539\u8fdb\u67b6\u6784\uff0c\u4f18\u4e8e\u968f\u673a\u641c\u7d22\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\u901a\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u641c\u7d22\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u901a\u7528\u95ed\u5f0f\u51fd\u6570\u6620\u5c04\u7ed3\u6784\u4e0e\u6027\u80fd\u5173\u7cfb\u3002", "method": "\u6784\u5efa\u6570\u636e\u96c6\u8bad\u7ec3ASNN\u6a21\u578b\uff0c\u5229\u7528\u5176\u8fed\u4ee3\u9884\u6d4b\u66f4\u9ad8\u6027\u80fd\u7684\u67b6\u6784\u3002", "result": "ASNN\u57282\u5c42\u548c3\u5c42\u67b6\u6784\u4e2d\u5747\u63d0\u51fa\u4f18\u4e8e\u539f\u59cb\u6570\u636e\u7684\u67b6\u6784\uff0c\u63d0\u9ad8\u5e73\u5747\u6d4b\u8bd5\u7cbe\u5ea6\u3002", "conclusion": "ASNN\u4e3a\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4f18\u5316\u63d0\u4f9b\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u671b\u5b9e\u73b0\u81ea\u52a8\u5316\u8bbe\u8ba1\u3002"}}
{"id": "2507.20191", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20191", "abs": "https://arxiv.org/abs/2507.20191", "authors": ["Cheng-Jun Guo", "Chuan-Xian Ren", "You-Wei Luo", "Xiao-Lin Xu", "Hong Yan"], "title": "Partial Domain Adaptation via Importance Sampling-based Shift Correction", "comment": null, "summary": "Partial domain adaptation (PDA) is a challenging task in real-world machine\nlearning scenarios. It aims to transfer knowledge from a labeled source domain\nto a related unlabeled target domain, where the support set of the source label\ndistribution subsumes the target one. Previous PDA works managed to correct the\nlabel distribution shift by weighting samples in the source domain. However,\nthe simple reweighing technique cannot explore the latent structure and\nsufficiently use the labeled data, and then models are prone to over-fitting on\nthe source domain. In this work, we propose a novel importance sampling-based\nshift correction (IS$^2$C) method, where new labeled data are sampled from a\nbuilt sampling domain, whose label distribution is supposed to be the same as\nthe target domain, to characterize the latent structure and enhance the\ngeneralization ability of the model. We provide theoretical guarantees for\nIS$^2$C by proving that the generalization error can be sufficiently dominated\nby IS$^2$C. In particular, by implementing sampling with the mixture\ndistribution, the extent of shift between source and sampling domains can be\nconnected to generalization error, which provides an interpretable way to build\nIS$^2$C. To improve knowledge transfer, an optimal transport-based independence\ncriterion is proposed for conditional distribution alignment, where the\ncomputation of the criterion can be adjusted to reduce the complexity from\n$\\mathcal{O}(n^3)$ to $\\mathcal{O}(n^2)$ in realistic PDA scenarios. Extensive\nexperiments on PDA benchmarks validate the theoretical results and demonstrate\nthe effectiveness of our IS$^2$C over existing methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u7684IS\u00b2C\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u90e8\u5206\u57df\u9002\u5e94\uff08PDA\uff09\u4e2d\u7684\u6807\u7b7e\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u90e8\u5206\u57df\u9002\u5e94\uff08PDA\uff09\u4e2d\uff0c\u7b80\u5355\u7684\u6837\u672c\u52a0\u6743\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6807\u8bb0\u6570\u636e\u5e76\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6821\u6b63\u6807\u7b7e\u5206\u5e03\u504f\u79fb\u3002", "method": "\u63d0\u51faIS\u00b2C\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u6784\u5efa\u7684\u91c7\u6837\u57df\u4e2d\u91c7\u6837\u65b0\u6807\u8bb0\u6570\u636e\uff0c\u4ee5\u5339\u914d\u76ee\u6807\u57df\u7684\u6807\u7b7e\u5206\u5e03\uff0c\u5e76\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u8fdb\u884c\u6761\u4ef6\u5206\u5e03\u5bf9\u9f50\u3002", "result": "\u7406\u8bba\u8bc1\u660eIS\u00b2C\u53ef\u4ee5\u4e3b\u5bfc\u6cdb\u5316\u8bef\u5dee\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728PDA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "IS\u00b2C\u65b9\u6cd5\u80fd\u6709\u6548\u6821\u6b63\u6807\u7b7e\u5206\u5e03\u504f\u79fb\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aPDA\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20202", "categories": ["cs.LG", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2507.20202", "abs": "https://arxiv.org/abs/2507.20202", "authors": ["Longfei Lu"], "title": "Technical Indicator Networks (TINs): An Interpretable Neural Architecture Modernizing Classic al Technical Analysis for Adaptive Algorithmic Trading", "comment": "Patent Application No. DE10202502351 filed on July 8, 2025 with DPMA", "summary": "This work proposes that a vast majority of classical technical indicators in\nfinancial analysis are, in essence, special cases of neural networks with fixed\nand interpretable weights. It is shown that nearly all such indicators, such as\nmoving averages, momentum-based oscillators, volatility bands, and other\ncommonly used technical constructs, can be reconstructed topologically as\nmodular neural network components. Technical Indicator Networks (TINs) are\nintroduced as a general neural architecture that replicates and structurally\nupgrades traditional indicators by supporting n-dimensional inputs such as\nprice, volume, sentiment, and order book data. By encoding domain-specific\nknowledge into neural structures, TINs modernize the foundational logic of\ntechnical analysis and propel algorithmic trading into a new era, bridging the\nlegacy of proven indicators with the potential of contemporary AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f20\u7edf\u91d1\u878d\u6280\u672f\u6307\u6807\u672c\u8d28\u4e0a\u662f\u5177\u6709\u56fa\u5b9a\u548c\u53ef\u89e3\u91ca\u6743\u91cd\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u4f8b\uff0c\u5e76\u5f15\u5165\u6280\u672f\u6307\u6807\u7f51\u7edc\uff08TINs\uff09\u4f5c\u4e3a\u901a\u7528\u67b6\u6784\u3002", "motivation": "\u5c06\u4f20\u7edf\u6280\u672f\u6307\u6807\u4e0e\u73b0\u4ee3AI\u7cfb\u7edf\u7ed3\u5408\uff0c\u5347\u7ea7\u6280\u672f\u5206\u6790\u7684\u57fa\u7840\u903b\u8f91\u3002", "method": "\u901a\u8fc7\u6a21\u5757\u5316\u795e\u7ecf\u7f51\u7edc\u7ec4\u4ef6\u91cd\u6784\u6280\u672f\u6307\u6807\uff0c\u652f\u6301\u591a\u7ef4\u8f93\u5165\u3002", "result": "TINs\u80fd\u591f\u590d\u5236\u5e76\u7ed3\u6784\u6027\u5730\u5347\u7ea7\u4f20\u7edf\u6307\u6807\uff0c\u63a8\u52a8\u7b97\u6cd5\u4ea4\u6613\u8fdb\u5165\u65b0\u65f6\u4ee3\u3002", "conclusion": "TINs\u901a\u8fc7\u5c06\u9886\u57df\u77e5\u8bc6\u7f16\u7801\u4e3a\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u8fde\u63a5\u4f20\u7edf\u6307\u6807\u4e0e\u73b0\u4ee3AI\u6f5c\u529b\u3002"}}
{"id": "2507.20243", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20243", "abs": "https://arxiv.org/abs/2507.20243", "authors": ["Lang Yu", "Zhangyang Gao", "Cheng Tan", "Qin Chen", "Jie Zhou", "Liang He"], "title": "Protein-SE(3): Benchmarking SE(3)-based Generative Models for Protein Structure Design", "comment": null, "summary": "SE(3)-based generative models have shown great promise in protein geometry\nmodeling and effective structure design. However, the field currently lacks a\nmodularized benchmark to enable comprehensive investigation and fair comparison\nof different methods. In this paper, we propose Protein-SE(3), a new benchmark\nbased on a unified training framework, which comprises protein scaffolding\ntasks, integrated generative models, high-level mathematical abstraction, and\ndiverse evaluation metrics. Recent advanced generative models designed for\nprotein scaffolding, from multiple perspectives like DDPM (Genie1 and Genie2),\nScore Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and\nFrameFlow) are integrated into our framework. All integrated methods are fairly\ninvestigated with the same training dataset and evaluation metrics.\nFurthermore, we provide a high-level abstraction of the mathematical\nfoundations behind the generative models, enabling fast prototyping of future\nalgorithms without reliance on explicit protein structures. Accordingly, we\nrelease the first comprehensive benchmark built upon unified training framework\nfor SE(3)-based protein structure design, which is publicly accessible at\nhttps://github.com/BruthYU/protein-se3.", "AI": {"tldr": "\u63d0\u51fa\u4e86Protein-SE(3)\u57fa\u51c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u548c\u6bd4\u8f83SE(3)\u86cb\u767d\u8d28\u7ed3\u6784\u8bbe\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6a21\u5757\u5316\u57fa\u51c6\u4ee5\u5168\u9762\u7814\u7a76\u548c\u516c\u5e73\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff0c\u6574\u5408\u591a\u79cd\u751f\u6210\u6a21\u578b\uff08\u5982DDPM\u3001Score Matching\u3001Flow Matching\uff09\u548c\u591a\u6837\u5316\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u53d1\u5e03\u4e86\u9996\u4e2a\u57fa\u4e8e\u7edf\u4e00\u6846\u67b6\u7684SE(3)\u86cb\u767d\u8d28\u8bbe\u8ba1\u57fa\u51c6\uff0c\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "Protein-SE(3)\u4e3a\u672a\u6765\u7b97\u6cd5\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6570\u5b66\u62bd\u8c61\u548c\u516c\u5e73\u6bd4\u8f83\u5e73\u53f0\u3002"}}
{"id": "2507.20263", "categories": ["cs.LG", "cs.AI", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2507.20263", "abs": "https://arxiv.org/abs/2507.20263", "authors": ["Junjie Zhao", "Chengxi Zhang", "Chenkai Wang", "Peng Yang"], "title": "Learning from Expert Factors: Trajectory-level Reward Shaping for Formulaic Alpha Mining", "comment": null, "summary": "Reinforcement learning (RL) has successfully automated the complex process of\nmining formulaic alpha factors, for creating interpretable and profitable\ninvestment strategies. However, existing methods are hampered by the sparse\nrewards given the underlying Markov Decision Process. This inefficiency limits\nthe exploration of the vast symbolic search space and destabilizes the training\nprocess. To address this, Trajectory-level Reward Shaping (TLRS), a novel\nreward shaping method, is proposed. TLRS provides dense, intermediate rewards\nby measuring the subsequence-level similarity between partially generated\nexpressions and a set of expert-designed formulas. Furthermore, a reward\ncentering mechanism is introduced to reduce training variance. Extensive\nexperiments on six major Chinese and U.S. stock indices show that TLRS\nsignificantly improves the predictive power of mined factors, boosting the Rank\nInformation Coefficient by 9.29% over existing potential-based shaping\nalgorithms. Notably, TLRS achieves a major leap in computational efficiency by\nreducing its time complexity with respect to the feature dimension from linear\nto constant, which is a significant improvement over distance-based baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTLRS\u7684\u65b0\u5956\u52b1\u5851\u9020\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u5e8f\u5217\u7ea7\u76f8\u4f3c\u6027\u63d0\u4f9b\u5bc6\u96c6\u4e2d\u95f4\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u6295\u8d44\u7b56\u7565\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u4e0b\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u7b26\u53f7\u641c\u7d22\u7a7a\u95f4\u7684\u63a2\u7d22\u5e76\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faTLRS\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u90e8\u5206\u751f\u6210\u8868\u8fbe\u5f0f\u4e0e\u4e13\u5bb6\u8bbe\u8ba1\u516c\u5f0f\u7684\u5b50\u5e8f\u5217\u7ea7\u76f8\u4f3c\u6027\u63d0\u4f9b\u5bc6\u96c6\u5956\u52b1\uff0c\u5e76\u5f15\u5165\u5956\u52b1\u4e2d\u5fc3\u5316\u673a\u5236\u964d\u4f4e\u8bad\u7ec3\u65b9\u5dee\u3002", "result": "\u5728\u516d\u79cd\u4e3b\u8981\u4e2d\u7f8e\u80a1\u7968\u6307\u6570\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTLRS\u663e\u8457\u63d0\u5347\u9884\u6d4b\u80fd\u529b\uff0cRank Information Coefficient\u63d0\u9ad89.29%\uff0c\u8ba1\u7b97\u6548\u7387\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "TLRS\u901a\u8fc7\u5bc6\u96c6\u5956\u52b1\u548c\u9ad8\u6548\u8ba1\u7b97\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u6295\u8d44\u7b56\u7565\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2507.20326", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20326", "abs": "https://arxiv.org/abs/2507.20326", "authors": ["Jiaxi Wang", "Yaosen Min", "Xun Zhu", "Miao Li", "Ji Wu"], "title": "MIPS: a Multimodal Infinite Polymer Sequence Pre-training Framework for Polymer Property Prediction", "comment": "14 pages, 8 figures, accepted by ACM Multimedia 2025 (oral)", "summary": "Polymers, composed of repeating structural units called monomers, are\nfundamental materials in daily life and industry. Accurate property prediction\nfor polymers is essential for their design, development, and application.\nHowever, existing modeling approaches, which typically represent polymers by\nthe constituent monomers, struggle to capture the whole properties of polymer,\nsince the properties change during the polymerization process. In this study,\nwe propose a Multimodal Infinite Polymer Sequence (MIPS) pre-training\nframework, which represents polymers as infinite sequences of monomers and\nintegrates both topological and spatial information for comprehensive modeling.\nFrom the topological perspective, we generalize message passing mechanism (MPM)\nand graph attention mechanism (GAM) to infinite polymer sequences. For MPM, we\ndemonstrate that applying MPM to infinite polymer sequences is equivalent to\napplying MPM on the induced star-linking graph of monomers. For GAM, we propose\nto further replace global graph attention with localized graph attention (LGA).\nMoreover, we show the robustness of the \"star linking\" strategy through Repeat\nand Shift Invariance Test (RSIT). Despite its robustness, \"star linking\"\nstrategy exhibits limitations when monomer side chains contain ring structures,\na common characteristic of polymers, as it fails the Weisfeiler-Lehman~(WL)\ntest. To overcome this issue, we propose backbone embedding to enhance the\ncapability of MPM and LGA on infinite polymer sequences. From the spatial\nperspective, we extract 3D descriptors of repeating monomers to capture spatial\ninformation. Finally, we design a cross-modal fusion mechanism to unify the\ntopological and spatial information. Experimental validation across eight\ndiverse polymer property prediction tasks reveals that MIPS achieves\nstate-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIPS\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u805a\u5408\u7269\u6027\u8d28\uff0c\u7ed3\u5408\u4e86\u62d3\u6251\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u805a\u5408\u7269\u5728\u805a\u5408\u8fc7\u7a0b\u4e2d\u7684\u6574\u4f53\u6027\u8d28\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "MIPS\u6846\u67b6\u5c06\u805a\u5408\u7269\u8868\u793a\u4e3a\u65e0\u9650\u5355\u4f53\u5e8f\u5217\uff0c\u7ed3\u5408\u62d3\u6251\uff08MPM\u548cGAM\uff09\u548c\u7a7a\u95f4\u4fe1\u606f\uff083D\u63cf\u8ff0\u7b26\uff09\uff0c\u5e76\u63d0\u51fa\u5c40\u90e8\u56fe\u6ce8\u610f\u529b\uff08LGA\uff09\u548c\u9aa8\u67b6\u5d4c\u5165\u4ee5\u89e3\u51b3\u73af\u7ed3\u6784\u95ee\u9898\u3002", "result": "\u5728\u516b\u9879\u805a\u5408\u7269\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cMIPS\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MIPS\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u521b\u65b0\u7684\u62d3\u6251\u5efa\u6a21\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u5408\u7269\u6027\u8d28\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.20335", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20335", "abs": "https://arxiv.org/abs/2507.20335", "authors": ["Siyu Song", "Wentao Liu", "Ye Lu", "Ruohua Zhang", "Tao Liu", "Jinze Lv", "Xinyun Wang", "Aimin Zhou", "Fei Tan", "Bo Jiang", "Hao Hao"], "title": "Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning", "comment": null, "summary": "The integration of large language models (LLMs) into education presents\nunprecedented opportunities for scalable personalized learning. However,\nstandard LLMs often function as generic information providers, lacking\nalignment with fundamental pedagogical principles such as helpfulness,\nstudent-centered personalization, and creativity cultivation. To bridge this\ngap, we propose EduAlign, a novel framework designed to guide LLMs toward\nbecoming more effective and responsible educational assistants. EduAlign\nconsists of two main stages. In the first stage, we curate a dataset of 8k\neducational interactions and annotate them-both manually and\nautomatically-along three key educational dimensions: Helpfulness,\nPersonalization, and Creativity (HPC). These annotations are used to train\nHPC-RM, a multi-dimensional reward model capable of accurately scoring LLM\noutputs according to these educational principles. We further evaluate the\nconsistency and reliability of this reward model. In the second stage, we\nleverage HPC-RM as a reward signal to fine-tune a pre-trained LLM using Group\nRelative Policy Optimization (GRPO) on a set of 2k diverse prompts. We then\nassess the pre- and post-finetuning models on both educational and\ngeneral-domain benchmarks across the three HPC dimensions. Experimental results\ndemonstrate that the fine-tuned model exhibits significantly improved alignment\nwith pedagogical helpfulness, personalization, and creativity stimulation. This\nstudy presents a scalable and effective approach to aligning LLMs with nuanced\nand desirable educational traits, paving the way for the development of more\nengaging, pedagogically aligned AI tutors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEduAlign\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u6ce8\u6559\u80b2\u4ea4\u4e92\u6570\u636e\u5e76\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6559\u80b2\u9886\u57df\u7684\u8868\u73b0\uff0c\u663e\u8457\u63d0\u5347\u5176\u6559\u5b66\u5e2e\u52a9\u6027\u3001\u4e2a\u6027\u5316\u548c\u521b\u9020\u529b\u3002", "motivation": "\u6807\u51c6LLM\u5728\u6559\u80b2\u4e2d\u7f3a\u4e4f\u4e0e\u6559\u5b66\u539f\u5219\uff08\u5982\u5e2e\u52a9\u6027\u3001\u4e2a\u6027\u5316\u548c\u521b\u9020\u529b\uff09\u7684\u5bf9\u9f50\uff0cEduAlign\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u6807\u6ce88k\u6559\u80b2\u4ea4\u4e92\u6570\u636e\uff0c\u8bad\u7ec3HPC-RM\u5956\u52b1\u6a21\u578b\uff1b2. \u4f7f\u7528GRPO\u65b9\u6cd5\u5fae\u8c03LLM\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u6559\u5b66\u5e2e\u52a9\u6027\u3001\u4e2a\u6027\u5316\u548c\u521b\u9020\u529b\u65b9\u9762\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "EduAlign\u4e3a\u5f00\u53d1\u66f4\u5177\u6559\u5b66\u5bf9\u9f50\u6027\u7684AI\u5bfc\u5e08\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.20349", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.20349", "abs": "https://arxiv.org/abs/2507.20349", "authors": ["Rezaur Rashid", "Gabriel Terejanu"], "title": "From Observations to Causations: A GNN-based Probabilistic Prediction Framework for Causal Discovery", "comment": null, "summary": "Causal discovery from observational data is challenging, especially with\nlarge datasets and complex relationships. Traditional methods often struggle\nwith scalability and capturing global structural information. To overcome these\nlimitations, we introduce a novel graph neural network (GNN)-based\nprobabilistic framework that learns a probability distribution over the entire\nspace of causal graphs, unlike methods that output a single deterministic\ngraph. Our framework leverages a GNN that encodes both node and edge attributes\ninto a unified graph representation, enabling the model to learn complex causal\nstructures directly from data. The GNN model is trained on a diverse set of\nsynthetic datasets augmented with statistical and information-theoretic\nmeasures, such as mutual information and conditional entropy, capturing both\nlocal and global data properties. We frame causal discovery as a supervised\nlearning problem, directly predicting the entire graph structure. Our approach\ndemonstrates superior performance, outperforming both traditional and recent\nnon-GNN-based methods, as well as a GNN-based approach, in terms of accuracy\nand scalability on synthetic and real-world datasets without further training.\nThis probabilistic framework significantly improves causal structure learning,\nwith broad implications for decision-making and scientific discovery across\nvarious fields.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u53d1\u73b0\u56e0\u679c\u5173\u7cfb\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u975eGNN\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u5927\u6570\u636e\u96c6\u548c\u590d\u6742\u5173\u7cfb\u4e0b\u96be\u4ee5\u6269\u5c55\u548c\u6355\u6349\u5168\u5c40\u7ed3\u6784\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528GNN\u7f16\u7801\u8282\u70b9\u548c\u8fb9\u5c5e\u6027\uff0c\u5b66\u4e60\u6574\u4e2a\u56e0\u679c\u56fe\u7a7a\u95f4\u7684\u6982\u7387\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u7ed3\u5408\u7edf\u8ba1\u548c\u4fe1\u606f\u8bba\u5ea6\u91cf\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u548c\u975eGNN\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6982\u7387\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u80fd\u529b\uff0c\u5bf9\u591a\u9886\u57df\u51b3\u7b56\u548c\u79d1\u5b66\u53d1\u73b0\u5177\u6709\u5e7f\u6cdb\u610f\u4e49\u3002"}}
{"id": "2507.20351", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.20351", "abs": "https://arxiv.org/abs/2507.20351", "authors": ["Ronglong Fang", "Yuesheng Xu"], "title": "Computational Advantages of Multi-Grade Deep Learning: Convergence Analysis and Performance Insights", "comment": null, "summary": "Multi-grade deep learning (MGDL) has been shown to significantly outperform\nthe standard single-grade deep learning (SGDL) across various applications.\nThis work aims to investigate the computational advantages of MGDL focusing on\nits performance in image regression, denoising, and deblurring tasks, and\ncomparing it to SGDL. We establish convergence results for the gradient descent\n(GD) method applied to these models and provide mathematical insights into\nMGDL's improved performance. In particular, we demonstrate that MGDL is more\nrobust to the choice of learning rate under GD than SGDL. Furthermore, we\nanalyze the eigenvalue distributions of the Jacobian matrices associated with\nthe iterative schemes arising from the GD iterations, offering an explanation\nfor MGDL's enhanced training stability.", "AI": {"tldr": "MGDL\u5728\u56fe\u50cf\u56de\u5f52\u3001\u53bb\u566a\u548c\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eSGDL\uff0c\u4e14\u5bf9\u5b66\u4e60\u7387\u9009\u62e9\u66f4\u9c81\u68d2\u3002", "motivation": "\u7814\u7a76MGDL\u7684\u8ba1\u7b97\u4f18\u52bf\u53ca\u5176\u5728\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u6cd5\uff08GD\uff09\u5206\u6790MGDL\u548cSGDL\u7684\u6536\u655b\u6027\u53ca\u96c5\u53ef\u6bd4\u77e9\u9635\u7279\u5f81\u503c\u5206\u5e03\u3002", "result": "MGDL\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8eSGDL\uff0c\u4e14\u5bf9\u5b66\u4e60\u7387\u9009\u62e9\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "MGDL\u56e0\u5176\u6570\u5b66\u7279\u6027\u5728\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8eSGDL\u3002"}}
{"id": "2507.20357", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20357", "abs": "https://arxiv.org/abs/2507.20357", "authors": ["Kohei Miyaguchi", "Masao Joko", "Rebekah Sheraw", "Tsuyoshi Id\u00e9"], "title": "Wafer Defect Root Cause Analysis with Partial Trajectory Regression", "comment": "Published as K. Miyaguchi, M. Joko, R. Sheraw and T. Id\\'e, \"Wafer\n  Defect Root Cause Analysis with Partial Trajectory Regression,'' Proceedings\n  of the 36th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC\n  2025), Albany, NY, USA, 2025, pp. 1-6, doi: 10.1109/ASMC64512.2025.11010733", "summary": "Identifying upstream processes responsible for wafer defects is challenging\ndue to the combinatorial nature of process flows and the inherent variability\nin processing routes, which arises from factors such as rework operations and\nrandom process waiting times. This paper presents a novel framework for wafer\ndefect root cause analysis, called Partial Trajectory Regression (PTR). The\nproposed framework is carefully designed to address the limitations of\nconventional vector-based regression models, particularly in handling\nvariable-length processing routes that span a large number of heterogeneous\nphysical processes. To compute the attribution score of each process given a\ndetected high defect density on a specific wafer, we propose a new algorithm\nthat compares two counterfactual outcomes derived from partial process\ntrajectories. This is enabled by new representation learning methods, proc2vec\nand route2vec. We demonstrate the effectiveness of the proposed framework using\nreal wafer history data from the NY CREATES fab in Albany.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPTR\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u6676\u5706\u7f3a\u9677\u7684\u6839\u56e0\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5411\u91cf\u56de\u5f52\u6a21\u578b\u5728\u5904\u7406\u53d8\u957f\u5904\u7406\u8def\u5f84\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7531\u4e8e\u6676\u5706\u5236\u9020\u8fc7\u7a0b\u7684\u7ec4\u5408\u6027\u548c\u53d8\u5f02\u6027\uff08\u5982\u8fd4\u5de5\u64cd\u4f5c\u548c\u968f\u673a\u7b49\u5f85\u65f6\u95f4\uff09\uff0c\u8bc6\u522b\u5bfc\u81f4\u7f3a\u9677\u7684\u4e0a\u6e38\u8fc7\u7a0b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528Partial Trajectory Regression (PTR)\u6846\u67b6\uff0c\u7ed3\u5408proc2vec\u548croute2vec\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u90e8\u5206\u5904\u7406\u8def\u5f84\u7684\u53cd\u4e8b\u5b9e\u7ed3\u679c\u8ba1\u7b97\u5404\u8fc7\u7a0b\u7684\u8d21\u732e\u5206\u6570\u3002", "result": "\u5728NY CREATES fab\u7684\u5b9e\u9645\u6676\u5706\u5386\u53f2\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "PTR\u6846\u67b6\u4e3a\u6676\u5706\u7f3a\u9677\u7684\u6839\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.20362", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.20362", "abs": "https://arxiv.org/abs/2507.20362", "authors": ["Hengyu Liu", "Tianyi Li", "Yuqiang He", "Kristian Torp", "Yushuai Li", "Christian S. Jensen"], "title": "MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS Data (Extended Version)", "comment": "18 pages, 4 figures", "summary": "Location-tracking data from the Automatic Identification System, much of\nwhich is publicly available, plays a key role in a range of maritime safety and\nmonitoring applications. However, the data suffers from missing values that\nhamper downstream applications. Imputing the missing values is challenging\nbecause the values of different heterogeneous attributes are updated at diverse\nrates, resulting in the occurrence of multi-scale dependencies among\nattributes. Existing imputation methods that assume similar update rates across\nattributes are unable to capture and exploit such dependencies, limiting their\nimputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based\nImputation Network that aims improve imputation accuracy by capturing\nmulti-scale dependencies. Specifically, MH-GIN first extracts multi-scale\ntemporal features for each attribute while preserving their intrinsic\nheterogeneous characteristics. Then, it constructs a multi-scale heterogeneous\ngraph to explicitly model dependencies between heterogeneous attributes to\nenable more accurate imputation of missing values through graph propagation.\nExperimental results on two real-world datasets find that MH-GIN is capable of\nan average 57% reduction in imputation errors compared to state-of-the-art\nmethods, while maintaining computational efficiency. The source code and\nimplementation details of MH-GIN are publicly available\nhttps://github.com/hyLiu1994/MH-GIN.", "AI": {"tldr": "MH-GIN\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u5f02\u6784\u56fe\u7f51\u7edc\u7684\u7f3a\u5931\u503c\u586b\u8865\u65b9\u6cd5\uff0c\u9488\u5bf9\u6d77\u4e0a\u81ea\u52a8\u8bc6\u522b\u7cfb\u7edf\u6570\u636e\u4e2d\u7684\u591a\u5c3a\u5ea6\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u586b\u8865\u7cbe\u5ea6\u3002", "motivation": "\u6d77\u4e0a\u81ea\u52a8\u8bc6\u522b\u7cfb\u7edf\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u591a\u5c3a\u5ea6\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "MH-GIN\u901a\u8fc7\u63d0\u53d6\u591a\u5c3a\u5ea6\u65f6\u95f4\u7279\u5f81\u5e76\u6784\u5efa\u591a\u5c3a\u5ea6\u5f02\u6784\u56fe\uff0c\u663e\u5f0f\u5efa\u6a21\u5f02\u8d28\u5c5e\u6027\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5229\u7528\u56fe\u4f20\u64ad\u586b\u8865\u7f3a\u5931\u503c\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cMH-GIN\u5e73\u5747\u51cf\u5c1157%\u7684\u586b\u8865\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "MH-GIN\u901a\u8fc7\u6355\u6349\u591a\u5c3a\u5ea6\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f3a\u5931\u503c\u586b\u8865\u7684\u51c6\u786e\u6027\uff0c\u4e14\u4ee3\u7801\u5f00\u6e90\u3002"}}
{"id": "2507.20364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20364", "abs": "https://arxiv.org/abs/2507.20364", "authors": ["Kohei Miyaguchi", "Masao Joko", "Rebekah Sheraw", "Tsuyoshi Id\u00e9"], "title": "Sequence-Aware Inline Measurement Attribution for Good-Bad Wafer Diagnosis", "comment": "Published as K. Miyaguchi, M. Joko, R. Sheraw and T. Id\\'e,\n  \"Sequence-Aware Inline Measurement Attribution for Good-Bad Wafer Diagnosis :\n  DM: Big Data Management and Machine Learning,\" 2025 36th Annual SEMI Advanced\n  Semiconductor Manufacturing Conference (ASMC), Albany, NY, USA, 2025, pp.\n  1-6, doi: 10.1109/ASMC64512.2025.11010308", "summary": "How can we identify problematic upstream processes when a certain type of\nwafer defect starts appearing at a quality checkpoint? Given the complexity of\nmodern semiconductor manufacturing, which involves thousands of process steps,\ncross-process root cause analysis for wafer defects has been considered highly\nchallenging. This paper proposes a novel framework called Trajectory Shapley\nAttribution (TSA), an extension of Shapley values (SV), a widely used\nattribution algorithm in explainable artificial intelligence research. TSA\novercomes key limitations of standard SV, including its disregard for the\nsequential nature of manufacturing processes and its reliance on an arbitrarily\nchosen reference point. We applied TSA to a good-bad wafer diagnosis task in\nexperimental front-end-of-line processes at the NY CREATES Albany NanoTech fab,\naiming to identify measurement items (serving as proxies for process\nparameters) most relevant to abnormal defect occurrence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrajectory Shapley Attribution (TSA)\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u6676\u5706\u7f3a\u9677\u7684\u8de8\u5de5\u827a\u6839\u672c\u539f\u56e0\u5206\u6790\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u534a\u5bfc\u4f53\u5236\u9020\u6d89\u53ca\u6570\u5343\u4e2a\u5de5\u827a\u6b65\u9aa4\uff0c\u6676\u5706\u7f3a\u9677\u7684\u8de8\u5de5\u827a\u6839\u56e0\u5206\u6790\u6781\u5177\u6311\u6218\u6027\u3002", "method": "TSA\u662fShapley\u503c\uff08SV\uff09\u7684\u6269\u5c55\uff0c\u514b\u670d\u4e86\u6807\u51c6SV\u5ffd\u7565\u5de5\u827a\u987a\u5e8f\u6027\u548c\u4f9d\u8d56\u4efb\u610f\u53c2\u8003\u70b9\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728\u5b9e\u9a8c\u6027\u524d\u7aef\u5de5\u827a\u4e2d\uff0cTSA\u6210\u529f\u8bc6\u522b\u4e86\u4e0e\u5f02\u5e38\u7f3a\u9677\u76f8\u5173\u7684\u6d4b\u91cf\u9879\u3002", "conclusion": "TSA\u4e3a\u590d\u6742\u5236\u9020\u73af\u5883\u4e2d\u7684\u7f3a\u9677\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.20369", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20369", "abs": "https://arxiv.org/abs/2507.20369", "authors": ["Ahmed Shokry", "Ayman Khalafallah"], "title": "Clustering by Attention: Leveraging Prior Fitted Transformers for Data Partitioning", "comment": null, "summary": "Clustering is a core task in machine learning with wide-ranging applications\nin data mining and pattern recognition. However, its unsupervised nature makes\nit inherently challenging. Many existing clustering algorithms suffer from\ncritical limitations: they often require careful parameter tuning, exhibit high\ncomputational complexity, lack interpretability, or yield suboptimal accuracy,\nespecially when applied to large-scale datasets. In this paper, we introduce a\nnovel clustering approach based on meta-learning. Our approach eliminates the\nneed for parameter optimization while achieving accuracy that outperforms\nstate-of-the-art clustering techniques. The proposed technique leverages a few\npre-clustered samples to guide the clustering process for the entire dataset in\na single forward pass. Specifically, we employ a pre-trained Prior-Data Fitted\nTransformer Network (PFN) to perform clustering. The algorithm computes\nattention between the pre-clustered samples and the unclustered samples,\nallowing it to infer cluster assignments for the entire dataset based on the\nlearned relation. We theoretically and empirically demonstrate that, given just\na few pre-clustered examples, the model can generalize to accurately cluster\nthe rest of the dataset. Experiments on challenging benchmark datasets show\nthat our approach can successfully cluster well-separated data without any\npre-clustered samples, and significantly improves performance when a few\nclustered samples are provided. We show that our approach is superior to the\nstate-of-the-art techniques. These results highlight the effectiveness and\nscalability of our approach, positioning it as a promising alternative to\nexisting clustering techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u65b0\u578b\u805a\u7c7b\u65b9\u6cd5\uff0c\u65e0\u9700\u53c2\u6570\u4f18\u5316\u5373\u53ef\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u805a\u7c7b\u7b97\u6cd5\u5b58\u5728\u53c2\u6570\u8c03\u4f18\u590d\u6742\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u6216\u51c6\u786e\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684Prior-Data Fitted Transformer Network\uff08PFN\uff09\uff0c\u901a\u8fc7\u5c11\u91cf\u9884\u805a\u7c7b\u6837\u672c\u5f15\u5bfc\u6574\u4e2a\u6570\u636e\u96c6\u7684\u805a\u7c7b\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u9884\u805a\u7c7b\u6837\u672c\u65f6\u8868\u73b0\u826f\u597d\uff0c\u63d0\u4f9b\u5c11\u91cf\u6837\u672c\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u662f\u73b0\u6709\u805a\u7c7b\u6280\u672f\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.20373", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20373", "abs": "https://arxiv.org/abs/2507.20373", "authors": ["Kiymet Kaya", "Elif Ak", "Sule Gunduz Oguducu"], "title": "WBHT: A Generative Attention Architecture for Detecting Black Hole Anomalies in Backbone Networks", "comment": null, "summary": "We propose the Wasserstein Black Hole Transformer (WBHT) framework for\ndetecting black hole (BH) anomalies in communication networks. These anomalies\ncause packet loss without failure notifications, disrupting connectivity and\nleading to financial losses. WBHT combines generative modeling, sequential\nlearning, and attention mechanisms to improve BH anomaly detection. It\nintegrates a Wasserstein generative adversarial network with attention\nmechanisms for stable training and accurate anomaly identification. The model\nuses long-short-term memory layers to capture long-term dependencies and\nconvolutional layers for local temporal patterns. A latent space encoding\nmechanism helps distinguish abnormal network behavior. Tested on real-world\nnetwork data, WBHT outperforms existing models, achieving significant\nimprovements in F1 score (ranging from 1.65% to 58.76%). Its efficiency and\nability to detect previously undetected anomalies make it a valuable tool for\nproactive network monitoring and security, especially in mission-critical\nnetworks.", "AI": {"tldr": "WBHT\u6846\u67b6\u7ed3\u5408\u751f\u6210\u6a21\u578b\u3001\u5e8f\u5217\u5b66\u4e60\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u68c0\u6d4b\u901a\u4fe1\u7f51\u7edc\u4e2d\u7684\u9ed1\u6d1e\u5f02\u5e38\uff0c\u663e\u8457\u63d0\u5347F1\u5206\u6570\u3002", "motivation": "\u9ed1\u6d1e\u5f02\u5e38\u5bfc\u81f4\u65e0\u901a\u77e5\u7684\u4e22\u5305\uff0c\u7834\u574f\u8fde\u63a5\u5e76\u9020\u6210\u7ecf\u6d4e\u635f\u5931\uff0c\u9700\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408Wasserstein GAN\u4e0e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u7528LSTM\u548c\u5377\u79ef\u5c42\u6355\u6349\u957f\u77ed\u671f\u4f9d\u8d56\u548c\u5c40\u90e8\u65f6\u5e8f\u6a21\u5f0f\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\u533a\u5206\u5f02\u5e38\u884c\u4e3a\u3002", "result": "\u5728\u771f\u5b9e\u7f51\u7edc\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0cWBHT\u7684F1\u5206\u6570\u63d0\u53471.65%\u81f358.76%\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "WBHT\u9ad8\u6548\u4e14\u80fd\u68c0\u6d4b\u672a\u53d1\u73b0\u5f02\u5e38\uff0c\u9002\u7528\u4e8e\u5173\u952e\u7f51\u7edc\u76d1\u63a7\u548c\u5b89\u5168\u3002"}}
{"id": "2507.20378", "categories": ["cs.LG", "astro-ph.CO"], "pdf": "https://arxiv.org/pdf/2507.20378", "abs": "https://arxiv.org/abs/2507.20378", "authors": ["Bonny Y. Wang", "Leander Thiele"], "title": "Set-based Implicit Likelihood Inference of Galaxy Cluster Mass", "comment": "5 pages, 4 figures; accepted as a spotlight talk at ICML-colocated\n  ML4Astro 2025 workshop", "summary": "We present a set-based machine learning framework that infers posterior\ndistributions of galaxy cluster masses from projected galaxy dynamics. Our\nmodel combines Deep Sets and conditional normalizing flows to incorporate both\npositional and velocity information of member galaxies to predict residual\ncorrections to the $M$-$\\sigma$ relation for improved interpretability. Trained\non the Uchuu-UniverseMachine simulation, our approach significantly reduces\nscatter and provides well-calibrated uncertainties across the full mass range\ncompared to traditional dynamical estimates.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.20424", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.20424", "abs": "https://arxiv.org/abs/2507.20424", "authors": ["Tolga Dimlioglu", "Anna Choromanska"], "title": "Communication-Efficient Distributed Training for Collaborative Flat Optima Recovery in Deep Learning", "comment": "9 pages main body, 32 pages of supplementary material for detailed\n  derivations and more experiment results", "summary": "We study centralized distributed data parallel training of deep neural\nnetworks (DNNs), aiming to improve the trade-off between communication\nefficiency and model performance of the local gradient methods. To this end, we\nrevisit the flat-minima hypothesis, which suggests that models with better\ngeneralization tend to lie in flatter regions of the loss landscape. We\nintroduce a simple, yet effective, sharpness measure, Inverse Mean Valley, and\ndemonstrate its strong correlation with the generalization gap of DNNs. We\nincorporate an efficient relaxation of this measure into the distributed\ntraining objective as a lightweight regularizer that encourages workers to\ncollaboratively seek wide minima. The regularizer exerts a pushing force that\ncounteracts the consensus step pulling the workers together, giving rise to the\nDistributed Pull-Push Force (DPPF) algorithm. Empirically, we show that DPPF\noutperforms other communication-efficient approaches and achieves better\ngeneralization performance than local gradient methods and synchronous gradient\naveraging, while significantly reducing communication overhead. In addition,\nour loss landscape visualizations confirm the ability of DPPF to locate flatter\nminima. On the theoretical side, we show that DPPF guides workers to span flat\nvalleys, with the final valley width governed by the interplay between push and\npull strengths, and that its pull-push dynamics is self-stabilizing. We further\nprovide generalization guarantees linked to the valley width and prove\nconvergence in the non-convex setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u8bad\u7ec3\u7b97\u6cd5DPPF\uff0c\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u5668Inverse Mean Valley\uff0c\u5e73\u8861\u901a\u4fe1\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u6539\u8fdb\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u901a\u4fe1\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63a2\u7d22\u5e73\u5766\u6700\u5c0f\u503c\u5047\u8bbe\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f15\u5165Inverse Mean Valley\u4f5c\u4e3a\u9510\u5ea6\u5ea6\u91cf\uff0c\u5c06\u5176\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u878d\u5165\u8bad\u7ec3\u76ee\u6807\uff0c\u63d0\u51faDPPF\u7b97\u6cd5\uff0c\u901a\u8fc7\u63a8\u62c9\u52a8\u6001\u5e73\u8861\u901a\u4fe1\u4e0e\u4f18\u5316\u3002", "result": "DPPF\u5728\u901a\u4fe1\u6548\u7387\u548c\u6cdb\u5316\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u5b9e\u5176\u80fd\u591f\u627e\u5230\u66f4\u5e73\u5766\u7684\u6700\u5c0f\u503c\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u6536\u655b\u548c\u6cdb\u5316\u4fdd\u8bc1\u3002", "conclusion": "DPPF\u7b97\u6cd5\u901a\u8fc7\u63a8\u62c9\u52a8\u6001\u6709\u6548\u5e73\u8861\u901a\u4fe1\u4e0e\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0f\u8bad\u7ec3\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u5177\u5907\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.20433", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20433", "abs": "https://arxiv.org/abs/2507.20433", "authors": ["Alessandro Capurso", "Elia Piccoli", "Davide Bacciu"], "title": "FAST: Similarity-based Knowledge Transfer for Efficient Policy Learning", "comment": "Accepted at IEEE Conference on Games (CoG) 2025", "summary": "Transfer Learning (TL) offers the potential to accelerate learning by\ntransferring knowledge across tasks. However, it faces critical challenges such\nas negative transfer, domain adaptation and inefficiency in selecting solid\nsource policies. These issues often represent critical problems in evolving\ndomains, i.e. game development, where scenarios transform and agents must\nadapt. The continuous release of new agents is costly and inefficient. In this\nwork we challenge the key issues in TL to improve knowledge transfer, agents\nperformance across tasks and reduce computational costs. The proposed\nmethodology, called FAST - Framework for Adaptive Similarity-based Transfer,\nleverages visual frames and textual descriptions to create a latent\nrepresentation of tasks dynamics, that is exploited to estimate similarity\nbetween environments. The similarity scores guides our method in choosing\ncandidate policies from which transfer abilities to simplify learning of novel\ntasks. Experimental results, over multiple racing tracks, demonstrate that FAST\nachieves competitive final performance compared to learning-from-scratch\nmethods while requiring significantly less training steps. These findings\nhighlight the potential of embedding-driven task similarity estimations.", "AI": {"tldr": "FAST\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u4f30\u8ba1\u4efb\u52a1\u76f8\u4f3c\u6027\uff0c\u4f18\u5316\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u8d1f\u8fc1\u79fb\u3001\u9886\u57df\u9002\u5e94\u548c\u6e90\u7b56\u7565\u9009\u62e9\u6548\u7387\u4f4e\u7b49\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u53d8\u5316\u7684\u6e38\u620f\u5f00\u53d1\u9886\u57df\u3002", "method": "\u5229\u7528\u89c6\u89c9\u5e27\u548c\u6587\u672c\u63cf\u8ff0\u6784\u5efa\u4efb\u52a1\u52a8\u6001\u7684\u6f5c\u5728\u8868\u793a\uff0c\u57fa\u4e8e\u76f8\u4f3c\u6027\u5206\u6570\u9009\u62e9\u5019\u9009\u7b56\u7565\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u591a\u4e2a\u8d5b\u8f66\u8d5b\u9053\u4e0a\uff0cFAST\u6027\u80fd\u4e0e\u4ece\u5934\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u8bad\u7ec3\u6b65\u6570\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u5d4c\u5165\u9a71\u52a8\u7684\u4efb\u52a1\u76f8\u4f3c\u6027\u4f30\u8ba1\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.20440", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2507.20440", "abs": "https://arxiv.org/abs/2507.20440", "authors": ["Vicente Ramos", "Sundous Hussein", "Mohamed Abdel-Hafiz", "Arunangshu Sarkar", "Weixuan Liu", "Katerina J. Kechris", "Russell P. Bowler", "Leslie Lange", "Farnoush Banaei-Kashani"], "title": "BioNeuralNet: A Graph Neural Network based Multi-Omics Network Data Analysis Tool", "comment": "6 pages, 1 figure, 2 tables; Software available on PyPI as\n  BioNeuralNet. For documentation, tutorials, and workflows see\n  https://bioneuralnet.readthedocs.io", "summary": "Multi-omics data offer unprecedented insights into complex biological\nsystems, yet their high dimensionality, sparsity, and intricate interactions\npose significant analytical challenges. Network-based approaches have advanced\nmulti-omics research by effectively capturing biologically relevant\nrelationships among molecular entities. While these methods are powerful for\nrepresenting molecular interactions, there remains a need for tools\nspecifically designed to effectively utilize these network representations\nacross diverse downstream analyses. To fulfill this need, we introduce\nBioNeuralNet, a flexible and modular Python framework tailored for end-to-end\nnetwork-based multi-omics data analysis. BioNeuralNet leverages Graph Neural\nNetworks (GNNs) to learn biologically meaningful low-dimensional\nrepresentations from multi-omics networks, converting these complex molecular\nnetworks into versatile embeddings. BioNeuralNet supports all major stages of\nmulti-omics network analysis, including several network construction\ntechniques, generation of low-dimensional representations, and a broad range of\ndownstream analytical tasks. Its extensive utilities, including diverse GNN\narchitectures, and compatibility with established Python packages (e.g.,\nscikit-learn, PyTorch, NetworkX), enhance usability and facilitate quick\nadoption. BioNeuralNet is an open-source, user-friendly, and extensively\ndocumented framework designed to support flexible and reproducible multi-omics\nnetwork analysis in precision medicine.", "AI": {"tldr": "BioNeuralNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684Python\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u7ec4\u5b66\u6570\u636e\u7684\u7aef\u5230\u7aef\u7f51\u7edc\u5206\u6790\uff0c\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u591a\u7ec4\u5b66\u6570\u636e\u7684\u9ad8\u7ef4\u5ea6\u548c\u590d\u6742\u6027\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u6709\u6548\u5229\u7528\u7f51\u7edc\u8868\u793a\u8fdb\u884c\u5206\u6790\u3002", "method": "BioNeuralNet\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4ece\u591a\u7ec4\u5b66\u7f51\u7edc\u4e2d\u5b66\u4e60\u4f4e\u7ef4\u8868\u793a\uff0c\u652f\u6301\u7f51\u7edc\u6784\u5efa\u3001\u8868\u793a\u751f\u6210\u548c\u4e0b\u6e38\u5206\u6790\u3002", "result": "BioNeuralNet\u662f\u4e00\u4e2a\u7075\u6d3b\u3001\u6a21\u5757\u5316\u4e14\u517c\u5bb9\u4e3b\u6d41Python\u5de5\u5177\u7684\u5f00\u6e90\u6846\u67b6\u3002", "conclusion": "BioNeuralNet\u4e3a\u7cbe\u51c6\u533b\u5b66\u4e2d\u7684\u591a\u7ec4\u5b66\u7f51\u7edc\u5206\u6790\u63d0\u4f9b\u4e86\u7528\u6237\u53cb\u597d\u4e14\u53ef\u590d\u73b0\u7684\u5de5\u5177\u3002"}}
{"id": "2507.20443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20443", "abs": "https://arxiv.org/abs/2507.20443", "authors": ["Hongbo Li", "Lingjie Duan", "Yingbin Liang"], "title": "Provable In-Context Learning of Nonlinear Regression with Transformers", "comment": null, "summary": "The transformer architecture, which processes sequences of input tokens to\nproduce outputs for query tokens, has revolutionized numerous areas of machine\nlearning. A defining feature of transformers is their ability to perform\npreviously unseen tasks using task-specific prompts without updating\nparameters, a phenomenon known as in-context learning (ICL). Recent research\nhas actively explored the training dynamics behind ICL, with much of the focus\non relatively simple tasks such as linear regression and binary classification.\nTo advance the theoretical understanding of ICL, this paper investigates more\ncomplex nonlinear regression tasks, aiming to uncover how transformers acquire\nin-context learning capabilities in these settings. We analyze the stage-wise\ndynamics of attention during training: attention scores between a query token\nand its target features grow rapidly in the early phase, then gradually\nconverge to one, while attention to irrelevant features decays more slowly and\nexhibits oscillatory behavior. Our analysis introduces new proof techniques\nthat explicitly characterize how the nature of general non-degenerate\nL-Lipschitz task functions affects attention weights. Specifically, we identify\nthat the Lipschitz constant L of nonlinear function classes as a key factor\ngoverning the convergence dynamics of transformers in ICL. Leveraging these\ninsights, for two distinct regimes depending on whether L is below or above a\nthreshold, we derive different time bounds to guarantee near-zero prediction\nerror. Notably, despite the convergence time depending on the underlying task\nfunctions, we prove that query tokens consistently attend to prompt tokens with\nhighly relevant features at convergence, demonstrating the ICL capability of\ntransformers for unseen functions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u5728\u590d\u6742\u975e\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u52a8\u6001\u548cLipschitz\u5e38\u6570\u5bf9\u6536\u655b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22Transformer\u5728\u590d\u6742\u975e\u7ebf\u6027\u4efb\u52a1\u4e2d\u7684ICL\u673a\u5236\uff0c\u5f25\u8865\u73b0\u6709\u7814\u7a76\u5bf9\u7b80\u5355\u4efb\u52a1\u7684\u5c40\u9650\u6027\u3002", "method": "\u5206\u6790\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ce8\u610f\u529b\u7684\u9636\u6bb5\u6027\u52a8\u6001\uff0c\u5f15\u5165\u65b0\u7684\u8bc1\u660e\u6280\u672f\uff0c\u7814\u7a76Lipschitz\u5e38\u6570\u5bf9\u6536\u655b\u7684\u5f71\u54cd\u3002", "result": "\u6ce8\u610f\u529b\u5728\u65e9\u671f\u5feb\u901f\u805a\u7126\u76ee\u6807\u7279\u5f81\uff0c\u540e\u671f\u6536\u655b\uff1bLipschitz\u5e38\u6570\u51b3\u5b9a\u6536\u655b\u65f6\u95f4\uff0c\u4f46Transformer\u59cb\u7ec8\u80fd\u5173\u6ce8\u76f8\u5173\u7279\u5f81\u3002", "conclusion": "Transformer\u5728\u590d\u6742\u975e\u7ebf\u6027\u4efb\u52a1\u4e2d\u5177\u5907ICL\u80fd\u529b\uff0cLipschitz\u5e38\u6570\u662f\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002"}}
{"id": "2507.20446", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20446", "abs": "https://arxiv.org/abs/2507.20446", "authors": ["Guanghui Zhu", "Xin Fang", "Lei Wang", "Wenzhong Chen", "Rong Gu", "Chunfeng Yuan", "Yihua Huang"], "title": "BOASF: A Unified Framework for Speeding up Automatic Machine Learning via Adaptive Successive Filtering", "comment": null, "summary": "Machine learning has been making great success in many application areas.\nHowever, for the non-expert practitioners, it is always very challenging to\naddress a machine learning task successfully and efficiently. Finding the\noptimal machine learning model or the hyperparameter combination set from a\nlarge number of possible alternatives usually requires considerable expert\nknowledge and experience. To tackle this problem, we propose a combined\nBayesian Optimization and Adaptive Successive Filtering algorithm (BOASF) under\na unified multi-armed bandit framework to automate the model selection or the\nhyperparameter optimization. Specifically, BOASF consists of multiple\nevaluation rounds in each of which we select promising configurations for each\narm using the Bayesian optimization. Then, ASF can early discard the\npoor-performed arms adaptively using a Gaussian UCB-based probabilistic model.\nFurthermore, a Softmax model is employed to adaptively allocate available\nresources for each promising arm that advances to the next round. The arm with\na higher probability of advancing will be allocated more resources.\nExperimental results show that BOASF is effective for speeding up the model\nselection and hyperparameter optimization processes while achieving robust and\nbetter prediction performance than the existing state-of-the-art automatic\nmachine learning methods. Moreover, BOASF achieves better anytime performance\nunder various time budgets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u81ea\u9002\u5e94\u8fde\u7eed\u8fc7\u6ee4\u7b97\u6cd5\uff08BOASF\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u975e\u4e13\u5bb6\u5728\u673a\u5668\u5b66\u4e60\u548c\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u51cf\u5c11\u5bf9\u4e13\u5bb6\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002", "method": "BOASF\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u81ea\u9002\u5e94\u8fde\u7eed\u8fc7\u6ee4\uff0c\u901a\u8fc7\u591a\u8f6e\u8bc4\u4f30\u548c\u8d44\u6e90\u5206\u914d\u4f18\u5316\u6a21\u578b\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBOASF\u5728\u901f\u5ea6\u548c\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e14\u5728\u4e0d\u540c\u65f6\u95f4\u9884\u7b97\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "BOASF\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5065\u7684\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\u3002"}}
{"id": "2507.20447", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20447", "abs": "https://arxiv.org/abs/2507.20447", "authors": ["Takanobu Furuhashi", "Hidekata Hontani", "Tatsuya Yokota"], "title": "WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex Envelope", "comment": "8 pages, 4 figures", "summary": "Sparse regularization is fundamental in signal processing for efficient\nsignal recovery and feature extraction. However, it faces a fundamental\ndilemma: the most powerful sparsity-inducing penalties are often\nnon-differentiable, conflicting with gradient-based optimizers that dominate\nthe field. We introduce WEEP (Weakly-convex Envelope of Piecewise Penalty), a\nnovel, fully differentiable sparse regularizer derived from the weakly-convex\nenvelope framework. WEEP provides strong, unbiased sparsity while maintaining\nfull differentiability and L-smoothness, making it natively compatible with any\ngradient-based optimizer. This resolves the conflict between statistical\nperformance and computational tractability. We demonstrate superior performance\ncompared to the L1-norm and other established non-convex sparse regularizers on\nchallenging signal and image denoising tasks.", "AI": {"tldr": "WEEP\u662f\u4e00\u79cd\u65b0\u578b\u53ef\u5fae\u7a00\u758f\u6b63\u5219\u5316\u5668\uff0c\u89e3\u51b3\u4e86\u975e\u53ef\u5fae\u60e9\u7f5a\u4e0e\u68af\u5ea6\u4f18\u5316\u5668\u7684\u51b2\u7a81\uff0c\u5728\u4fe1\u53f7\u548c\u56fe\u50cf\u53bb\u566a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7a00\u758f\u6b63\u5219\u5316\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6700\u5f3a\u7684\u7a00\u758f\u60e9\u7f5a\u901a\u5e38\u4e0d\u53ef\u5fae\uff0c\u4e0e\u68af\u5ea6\u4f18\u5316\u5668\u51b2\u7a81\u3002", "method": "\u63d0\u51faWEEP\uff08Weakly-convex Envelope of Piecewise Penalty\uff09\uff0c\u57fa\u4e8e\u5f31\u51f8\u5305\u7edc\u6846\u67b6\uff0c\u5b9e\u73b0\u5b8c\u5168\u53ef\u5fae\u4e14\u5e73\u6ed1\u7684\u7a00\u758f\u6b63\u5219\u5316\u3002", "result": "WEEP\u5728\u4fe1\u53f7\u548c\u56fe\u50cf\u53bb\u566a\u4efb\u52a1\u4e2d\u4f18\u4e8eL1\u8303\u6570\u548c\u5176\u4ed6\u975e\u51f8\u7a00\u758f\u6b63\u5219\u5316\u5668\u3002", "conclusion": "WEEP\u89e3\u51b3\u4e86\u7edf\u8ba1\u6027\u80fd\u4e0e\u8ba1\u7b97\u53ef\u884c\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4e3a\u7a00\u758f\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.20453", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20453", "abs": "https://arxiv.org/abs/2507.20453", "authors": ["Camilo Tamayo-Rousseau", "Yunjia Zhao", "Yiqun Zhang", "Randall Balestriero"], "title": "Your Attention Matters: to Improve Model Robustness to Noise and Spurious Correlations", "comment": null, "summary": "Self-attention mechanisms are foundational to Transformer architectures,\nsupporting their impressive success in a wide range of tasks. While there are\nmany self-attention variants, their robustness to noise and spurious\ncorrelations has not been well studied. This study evaluates Softmax, Sigmoid,\nLinear, Doubly Stochastic, and Cosine attention within Vision Transformers\nunder different data corruption scenarios. Through testing across the CIFAR-10,\nCIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is\nthe most robust. Our findings inform self-attention selection in contexts with\nimperfect data.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e0d\u540c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u6570\u636e\u635f\u574f\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0Doubly Stochastic\u6ce8\u610f\u529b\u6700\u7a33\u5065\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728Transformer\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5bf9\u566a\u58f0\u548c\u4f2a\u76f8\u5173\u6027\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u8bc4\u4f30\u4e86Softmax\u3001Sigmoid\u3001Linear\u3001Doubly Stochastic\u548cCosine\u6ce8\u610f\u529b\u5728Vision Transformers\u4e2d\uff0c\u9488\u5bf9CIFAR-10\u3001CIFAR-100\u548cImagenette\u6570\u636e\u96c6\u7684\u6570\u636e\u635f\u574f\u573a\u666f\u3002", "result": "Doubly Stochastic\u6ce8\u610f\u529b\u8868\u73b0\u6700\u7a33\u5065\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u6570\u636e\u4e0d\u5b8c\u7f8e\u65f6\u9009\u62e9\u81ea\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.20460", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20460", "abs": "https://arxiv.org/abs/2507.20460", "authors": ["Selahattin Akkas", "Ariful Azad"], "title": "Shapley-Value-Based Graph Sparsification for GNN Inference", "comment": "10 pages", "summary": "Graph sparsification is a key technique for improving inference efficiency in\nGraph Neural Networks by removing edges with minimal impact on predictions. GNN\nexplainability methods generate local importance scores, which can be\naggregated into global scores for graph sparsification. However, many\nexplainability methods produce only non-negative scores, limiting their\napplicability for sparsification. In contrast, Shapley value based methods\nassign both positive and negative contributions to node predictions, offering a\ntheoretically robust and fair allocation of importance by evaluating many\nsubsets of graphs. Unlike gradient-based or perturbation-based explainers,\nShapley values enable better pruning strategies that preserve influential edges\nwhile removing misleading or adversarial connections. Our approach shows that\nShapley value-based graph sparsification maintains predictive performance while\nsignificantly reducing graph complexity, enhancing both interpretability and\nefficiency in GNN inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eShapley\u503c\u7684\u56fe\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u63a8\u7406\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u7a00\u758f\u5316\u65b9\u6cd5\u901a\u5e38\u4ec5\u4f9d\u8d56\u975e\u8d1f\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002Shapley\u503c\u80fd\u63d0\u4f9b\u6b63\u8d1f\u8d21\u732e\u8bc4\u5206\uff0c\u66f4\u516c\u5e73\u5730\u5206\u914d\u91cd\u8981\u6027\u3002", "method": "\u5229\u7528Shapley\u503c\u751f\u6210\u8282\u70b9\u9884\u6d4b\u7684\u6b63\u8d1f\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u901a\u8fc7\u8bc4\u4f30\u56fe\u7684\u591a\u4e2a\u5b50\u96c6\uff0c\u5b9e\u73b0\u66f4\u4f18\u7684\u526a\u679d\u7b56\u7565\u3002", "result": "\u57fa\u4e8eShapley\u503c\u7684\u7a00\u758f\u5316\u65b9\u6cd5\u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u56fe\u7684\u590d\u6742\u5ea6\u3002", "conclusion": "Shapley\u503c\u65b9\u6cd5\u5728\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.20478", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20478", "abs": "https://arxiv.org/abs/2507.20478", "authors": ["Daiko Kishikawa", "Yuka Muto", "Shunji Kotsuki"], "title": "Conditional Diffusion Models for Global Precipitation Map Inpainting", "comment": null, "summary": "Incomplete satellite-based precipitation presents a significant challenge in\nglobal monitoring. For example, the Global Satellite Mapping of Precipitation\n(GSMaP) from JAXA suffers from substantial missing regions due to the orbital\ncharacteristics of satellites that have microwave sensors, and its current\ninterpolation methods often result in spatial discontinuities. In this study,\nwe formulate the completion of the precipitation map as a video inpainting task\nand propose a machine learning approach based on conditional diffusion models.\nOur method employs a 3D U-Net with a 3D condition encoder to reconstruct\ncomplete precipitation maps by leveraging spatio-temporal information from\ninfrared images, latitude-longitude grids, and physical time inputs. Training\nwas carried out on ERA5 hourly precipitation data from 2020 to 2023. We\ngenerated a pseudo-GSMaP dataset by randomly applying GSMaP masks to ERA maps.\nPerformance was evaluated for the calendar year 2024, and our approach produces\nmore spatio-temporally consistent inpainted precipitation maps compared to\nconventional methods. These results indicate the potential to improve global\nprecipitation monitoring using the conditional diffusion models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u586b\u8865\u536b\u661f\u964d\u6c34\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u533a\u57df\uff0c\u901a\u8fc73D U-Net\u548c\u65f6\u7a7a\u4fe1\u606f\u751f\u6210\u66f4\u4e00\u81f4\u7684\u964d\u6c34\u56fe\u3002", "motivation": "\u5168\u7403\u536b\u661f\u964d\u6c34\u76d1\u6d4b\uff08\u5982GSMaP\uff09\u56e0\u5fae\u6ce2\u4f20\u611f\u5668\u8f68\u9053\u7279\u6027\u5b58\u5728\u5927\u91cf\u7f3a\u5931\u533a\u57df\uff0c\u73b0\u6709\u63d2\u503c\u65b9\u6cd5\u5bfc\u81f4\u7a7a\u95f4\u4e0d\u8fde\u7eed\u6027\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c3D U-Net\uff0c\u7ed3\u5408\u7ea2\u5916\u56fe\u50cf\u3001\u7ecf\u7eac\u5ea6\u7f51\u683c\u548c\u7269\u7406\u65f6\u95f4\u8f93\u5165\uff0c\u5229\u7528ERA5\u964d\u6c34\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u57282024\u5e74\u7684\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u964d\u6c34\u56fe\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5177\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "conclusion": "\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6709\u671b\u6539\u5584\u5168\u7403\u964d\u6c34\u76d1\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.20490", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20490", "abs": "https://arxiv.org/abs/2507.20490", "authors": ["Yanheng Hou", "Xunkai Li", "Zhenjun Li", "Bing Zhou", "Ronghua Li", "Guoren Wang"], "title": "HIAL: A New Paradigm for Hypergraph Active Learning via Influence Maximization", "comment": null, "summary": "In recent years, Hypergraph Neural Networks (HNNs) have demonstrated immense\npotential in handling complex systems with high-order interactions. However,\nacquiring large-scale, high-quality labeled data for these models is costly,\nmaking Active Learning (AL) a critical technique. Existing Graph Active\nLearning (GAL) methods, when applied to hypergraphs, often rely on techniques\nlike \"clique expansion,\" which destroys the high-order structural information\ncrucial to a hypergraph's success, thereby leading to suboptimal performance.\nTo address this challenge, we introduce HIAL (Hypergraph Active Learning), a\nnative active learning framework designed specifically for hypergraphs. We\ninnovatively reformulate the Hypergraph Active Learning (HAL) problem as an\nInfluence Maximization task. The core of HIAL is a dual-perspective influence\nfunction that, based on our novel \"High-Order Interaction-Aware (HOI-Aware)\"\npropagation mechanism, synergistically evaluates a node's feature-space\ncoverage (via Magnitude of Influence, MoI) and its topological influence (via\nExpected Diffusion Value, EDV). We prove that this objective function is\nmonotone and submodular, thus enabling the use of an efficient greedy algorithm\nwith a formal (1-1/e) approximation guarantee. Extensive experiments on seven\npublic datasets demonstrate that HIAL significantly outperforms\nstate-of-the-art baselines in terms of performance, efficiency, generality, and\nrobustness, establishing an efficient and powerful new paradigm for active\nlearning on hypergraphs.", "AI": {"tldr": "HIAL\u662f\u4e00\u79cd\u4e13\u4e3a\u8d85\u56fe\u8bbe\u8ba1\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9ad8\u9636\u4ea4\u4e92\u611f\u77e5\u4f20\u64ad\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5728\u8d85\u56fe\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u7834\u574f\u4e86\u9ad8\u9636\u7ed3\u6784\u4fe1\u606f\uff0cHIAL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u8d85\u56fe\u4e3b\u52a8\u5b66\u4e60\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5f71\u54cd\u529b\u6700\u5927\u5316\u4efb\u52a1\uff0c\u63d0\u51fa\u53cc\u89c6\u89d2\u5f71\u54cd\u529b\u51fd\u6570\uff0c\u7ed3\u5408\u7279\u5f81\u7a7a\u95f4\u8986\u76d6\u548c\u62d3\u6251\u5f71\u54cd\u529b\u8bc4\u4f30\u3002", "result": "\u5728\u4e03\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cHIAL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HIAL\u4e3a\u8d85\u56fe\u4e3b\u52a8\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.20498", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20498", "abs": "https://arxiv.org/abs/2507.20498", "authors": ["Enjun Du", "Siyi Liu", "Yongqi Zhang"], "title": "Mixture of Length and Pruning Experts for Knowledge Graphs Reasoning", "comment": null, "summary": "Knowledge Graph (KG) reasoning, which aims to infer new facts from structured\nknowledge repositories, plays a vital role in Natural Language Processing (NLP)\nsystems. Its effectiveness critically depends on constructing informative and\ncontextually relevant reasoning paths. However, existing graph neural networks\n(GNNs) often adopt rigid, query-agnostic path-exploration strategies, limiting\ntheir ability to adapt to diverse linguistic contexts and semantic nuances. To\naddress these limitations, we propose \\textbf{MoKGR}, a mixture-of-experts\nframework that personalizes path exploration through two complementary\ncomponents: (1) a mixture of length experts that adaptively selects and weights\ncandidate path lengths according to query complexity, providing query-specific\nreasoning depth; and (2) a mixture of pruning experts that evaluates candidate\npaths from a complementary perspective, retaining the most informative paths\nfor each query. Through comprehensive experiments on diverse benchmark, MoKGR\ndemonstrates superior performance in both transductive and inductive settings,\nvalidating the effectiveness of personalized path exploration in KGs reasoning.", "AI": {"tldr": "MoKGR\u662f\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8def\u5f84\u63a2\u7d22\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u8def\u5f84\u63a2\u7d22\u7b56\u7565\u50f5\u5316\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u7684\u8bed\u8a00\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u63d0\u51faMoKGR\u6846\u67b6\uff0c\u5305\u542b\u957f\u5ea6\u4e13\u5bb6\u548c\u526a\u679d\u4e13\u5bb6\uff0c\u5206\u522b\u81ea\u9002\u5e94\u9009\u62e9\u8def\u5f84\u957f\u5ea6\u548c\u8bc4\u4f30\u8def\u5f84\u4fe1\u606f\u91cf\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoKGR\u5728\u4f20\u9012\u548c\u5f52\u7eb3\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u4e2a\u6027\u5316\u8def\u5f84\u63a2\u7d22\u80fd\u6709\u6548\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2507.20499", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20499", "abs": "https://arxiv.org/abs/2507.20499", "authors": ["Linh Le Pham Van", "Minh Hoang Nguyen", "Duc Kieu", "Hung Le", "Hung The Tran", "Sunil Gupta"], "title": "DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning", "comment": "accepted at ECAI 2025", "summary": "Cross-domain offline reinforcement learning (RL) seeks to enhance sample\nefficiency in offline RL by utilizing additional offline source datasets. A key\nchallenge is to identify and utilize source samples that are most relevant to\nthe target domain. Existing approaches address this challenge by measuring\ndomain gaps through domain classifiers, target transition dynamics modeling, or\nmutual information estimation using contrastive loss. However, these methods\noften require large target datasets, which is impractical in many real-world\nscenarios. In this work, we address cross-domain offline RL under a limited\ntarget data setting, identifying two primary challenges: (1) Dataset imbalance,\nwhich is caused by large source and small target datasets and leads to\noverfitting in neural network-based domain gap estimators, resulting in\nuninformative measurements; and (2) Partial domain overlap, where only a subset\nof the source data is closely aligned with the target domain. To overcome these\nissues, we propose DmC, a novel framework for cross-domain offline RL with\nlimited target samples. Specifically, DmC utilizes $k$-nearest neighbor\n($k$-NN) based estimation to measure domain proximity without neural network\ntraining, effectively mitigating overfitting. Then, by utilizing this domain\nproximity, we introduce a nearest-neighbor-guided diffusion model to generate\nadditional source samples that are better aligned with the target domain, thus\nenhancing policy learning with more effective source samples. Through\ntheoretical analysis and extensive experiments in diverse MuJoCo environments,\nwe demonstrate that DmC significantly outperforms state-of-the-art cross-domain\noffline RL methods, achieving substantial performance gains.", "AI": {"tldr": "\u63d0\u51faDmC\u6846\u67b6\uff0c\u901a\u8fc7k-NN\u548c\u6269\u6563\u6a21\u578b\u89e3\u51b3\u8de8\u57df\u79bb\u7ebfRL\u4e2d\u76ee\u6807\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u76ee\u6807\u6570\u636e\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002DmC\u9488\u5bf9\u76ee\u6807\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\uff0c\u89e3\u51b3\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u548c\u90e8\u5206\u57df\u91cd\u53e0\u95ee\u9898\u3002", "method": "\u4f7f\u7528k-NN\u4f30\u8ba1\u57df\u63a5\u8fd1\u5ea6\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0e\u76ee\u6807\u57df\u66f4\u5339\u914d\u7684\u6e90\u6837\u672c\u3002", "result": "\u5728MuJoCo\u73af\u5883\u4e2d\uff0cDmC\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "DmC\u4e3a\u8de8\u57df\u79bb\u7ebfRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20503", "categories": ["cs.LG", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.20503", "abs": "https://arxiv.org/abs/2507.20503", "authors": ["Cheng-Fu Yang", "Thanh Tran", "Christos Christodoulopoulos", "Weitong Ruan", "Rahul Gupta", "Kai-Wei Chang"], "title": "Customize Multi-modal RAI Guardrails with Precedent-based predictions", "comment": "Accepted to COLM 2025", "summary": "A multi-modal guardrail must effectively filter image content based on\nuser-defined policies, identifying material that may be hateful, reinforce\nharmful stereotypes, contain explicit material, or spread misinformation.\nDeploying such guardrails in real-world applications, however, poses\nsignificant challenges. Users often require varied and highly customizable\npolicies and typically cannot provide abundant examples for each custom policy.\nConsequently, an ideal guardrail should be scalable to the multiple policies\nand adaptable to evolving user standards with minimal retraining. Existing\nfine-tuning methods typically condition predictions on pre-defined policies,\nrestricting their generalizability to new policies or necessitating extensive\nretraining to adapt. Conversely, training-free methods struggle with limited\ncontext lengths, making it difficult to incorporate all the policies\ncomprehensively. To overcome these limitations, we propose to condition model's\njudgment on \"precedents\", which are the reasoning processes of prior data\npoints similar to the given input. By leveraging precedents instead of fixed\npolicies, our approach greatly enhances the flexibility and adaptability of the\nguardrail. In this paper, we introduce a critique-revise mechanism for\ncollecting high-quality precedents and two strategies that utilize precedents\nfor robust prediction. Experimental results demonstrate that our approach\noutperforms previous methods across both few-shot and full-dataset scenarios\nand exhibits superior generalization to novel policies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u2018\u5148\u4f8b\u2019\u7684\u591a\u6a21\u6001\u62a4\u680f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u7c7b\u4f3c\u8f93\u5165\u7684\u5386\u53f2\u63a8\u7406\u8fc7\u7a0b\uff0c\u589e\u5f3a\u62a4\u680f\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u65b0\u653f\u7b56\u6216\u81ea\u5b9a\u4e49\u653f\u7b56\u65f6\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u6216\u96be\u4ee5\u5168\u9762\u6574\u5408\u653f\u7b56\u3002", "method": "\u91c7\u7528\u2018\u5148\u4f8b\u2019\u800c\u975e\u56fa\u5b9a\u653f\u7b56\uff0c\u5f15\u5165\u6279\u8bc4-\u4fee\u8ba2\u673a\u5236\u6536\u96c6\u9ad8\u8d28\u91cf\u5148\u4f8b\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u5229\u7528\u5148\u4f8b\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u548c\u5168\u6570\u636e\u96c6\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5bf9\u65b0\u653f\u7b56\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5148\u4f8b\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u62a4\u680f\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u591a\u653f\u7b56\u548c\u52a8\u6001\u7528\u6237\u6807\u51c6\u3002"}}
{"id": "2507.20505", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20505", "abs": "https://arxiv.org/abs/2507.20505", "authors": ["Binxiong Li", "Yuefei Wang", "Binyu Zhao", "Heyang Gao", "Benhan Yang", "Quanzhou Luo", "Xue Li", "Xu Xiang", "Yujie Liu", "Huijie Tang"], "title": "Attributed Graph Clustering with Multi-Scale Weight-Based Pairwise Coarsening and Contrastive Learning", "comment": "The source code for this study is available at\n  https://github.com/YF-W/MPCCL", "summary": "This study introduces the Multi-Scale Weight-Based Pairwise Coarsening and\nContrastive Learning (MPCCL) model, a novel approach for attributed graph\nclustering that effectively bridges critical gaps in existing methods,\nincluding long-range dependency, feature collapse, and information loss.\nTraditional methods often struggle to capture high-order graph features due to\ntheir reliance on low-order attribute information, while contrastive learning\ntechniques face limitations in feature diversity by overemphasizing local\nneighborhood structures. Similarly, conventional graph coarsening methods,\nthough reducing graph scale, frequently lose fine-grained structural details.\nMPCCL addresses these challenges through an innovative multi-scale coarsening\nstrategy, which progressively condenses the graph while prioritizing the\nmerging of key edges based on global node similarity to preserve essential\nstructural information. It further introduces a one-to-many contrastive\nlearning paradigm, integrating node embeddings with augmented graph views and\ncluster centroids to enhance feature diversity, while mitigating feature\nmasking issues caused by the accumulation of high-frequency node weights during\nmulti-scale coarsening. By incorporating a graph reconstruction loss and KL\ndivergence into its self-supervised learning framework, MPCCL ensures\ncross-scale consistency of node representations. Experimental evaluations\nreveal that MPCCL achieves a significant improvement in clustering performance,\nincluding a remarkable 15.24% increase in NMI on the ACM dataset and notable\nrobust gains on smaller-scale datasets such as Citeseer, Cora and DBLP.", "AI": {"tldr": "MPCCL\u6a21\u578b\u901a\u8fc7\u591a\u5c3a\u5ea6\u7c97\u5316\u548c\u5bf9\u6bd4\u5b66\u4e60\u89e3\u51b3\u56fe\u805a\u7c7b\u4e2d\u7684\u957f\u7a0b\u4f9d\u8d56\u3001\u7279\u5f81\u5d29\u6e83\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u9ad8\u9636\u56fe\u7279\u5f81\uff0c\u5bf9\u6bd4\u5b66\u4e60\u8fc7\u5ea6\u4f9d\u8d56\u5c40\u90e8\u7ed3\u6784\uff0c\u800c\u56fe\u7c97\u5316\u65b9\u6cd5\u4e22\u5931\u7ec6\u7c92\u5ea6\u4fe1\u606f\u3002MPCCL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6\u7c97\u5316\u7b56\u7565\u548c\u4e00\u5bf9\u591a\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\uff0c\u7ed3\u5408\u56fe\u91cd\u5efa\u635f\u5931\u548cKL\u6563\u5ea6\u786e\u4fdd\u8282\u70b9\u8868\u793a\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728ACM\u6570\u636e\u96c6\u4e0aNMI\u63d0\u534715.24%\uff0c\u5728Citeseer\u3001Cora\u548cDBLP\u7b49\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "MPCCL\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u805a\u7c7b\u6027\u80fd\u3002"}}
{"id": "2507.20513", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20513", "abs": "https://arxiv.org/abs/2507.20513", "authors": ["Shiva Sinaei", "Chuanjun Zheng", "Kaan Ak\u015fit", "Daisuke Iwai"], "title": "Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations", "comment": null, "summary": "Ray tracing is a widely used technique for modeling optical systems,\ninvolving sequential surface-by-surface computations, which can be\ncomputationally intensive. We propose Ray2Ray, a novel method that leverages\nimplicit neural representations to model optical systems with greater\nefficiency, eliminating the need for surface-by-surface computations in a\nsingle pass end-to-end model. Ray2Ray learns the mapping between rays emitted\nfrom a given source and their corresponding rays after passing through a given\noptical system in a physically accurate manner. We train Ray2Ray on nine\noff-the-shelf optical systems, achieving positional errors on the order of\n1{\\mu}m and angular deviations on the order 0.01 degrees in the estimated\noutput rays. Our work highlights the potential of neural representations as a\nproxy for optical raytracer.", "AI": {"tldr": "Ray2Ray\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u9ad8\u6548\u5efa\u6a21\u5149\u5b66\u7cfb\u7edf\uff0c\u907f\u514d\u9010\u9762\u8ba1\u7b97\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u5efa\u6a21\u3002", "motivation": "\u4f20\u7edf\u5149\u7ebf\u8ffd\u8e2a\u9010\u9762\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u9700\u66f4\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRay2Ray\uff0c\u901a\u8fc7\u795e\u7ecf\u8868\u793a\u5b66\u4e60\u5149\u7ebf\u6620\u5c04\uff0c\u5b9e\u73b0\u5355\u6b21\u7aef\u5230\u7aef\u5efa\u6a21\u3002", "result": "\u57289\u4e2a\u73b0\u6210\u5149\u5b66\u7cfb\u7edf\u4e0a\u8bad\u7ec3\uff0c\u4f4d\u7f6e\u8bef\u5dee\u7ea61\u03bcm\uff0c\u89d2\u5ea6\u504f\u5dee\u7ea60.01\u5ea6\u3002", "conclusion": "\u795e\u7ecf\u8868\u793a\u53ef\u4f5c\u4e3a\u5149\u5b66\u5149\u7ebf\u8ffd\u8e2a\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.20533", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20533", "abs": "https://arxiv.org/abs/2507.20533", "authors": ["Rajalaxmi Rajagopalan", "Yu-Lin Wei", "Romit Roy Choudhury"], "title": "Kernel Learning for Sample Constrained Black-Box Optimization", "comment": "Accepted to AAAI 2025", "summary": "Black box optimization (BBO) focuses on optimizing unknown functions in\nhigh-dimensional spaces. In many applications, sampling the unknown function is\nexpensive, imposing a tight sample budget. Ongoing work is making progress on\nreducing the sample budget by learning the shape/structure of the function,\nknown as kernel learning. We propose a new method to learn the kernel of a\nGaussian Process. Our idea is to create a continuous kernel space in the latent\nspace of a variational autoencoder, and run an auxiliary optimization to\nidentify the best kernel. Results show that the proposed method, Kernel\nOptimized Blackbox Optimization (KOBO), outperforms state of the art by\nestimating the optimal at considerably lower sample budgets. Results hold not\nonly across synthetic benchmark functions but also in real applications. We\nshow that a hearing aid may be personalized with fewer audio queries to the\nuser, or a generative model could converge to desirable images from limited\nuser ratings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKOBO\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9ad8\u65af\u8fc7\u7a0b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u8fde\u7eed\u6838\u7a7a\u95f4\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6837\u672c\u9884\u7b97\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u4f18\u5316\u672a\u77e5\u51fd\u6570\u65f6\uff0c\u91c7\u6837\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u51cf\u5c11\u6837\u672c\u9884\u7b97\u3002", "method": "\u5728\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u521b\u5efa\u8fde\u7eed\u6838\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u8f85\u52a9\u4f18\u5316\u786e\u5b9a\u6700\u4f73\u6838\u3002", "result": "KOBO\u5728\u5408\u6210\u57fa\u51c6\u51fd\u6570\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6837\u672c\u9884\u7b97\u3002", "conclusion": "KOBO\u65b9\u6cd5\u5728\u6837\u672c\u9884\u7b97\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2507.20534", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.20534", "abs": "https://arxiv.org/abs/2507.20534", "authors": ["Kimi Team", "Yifan Bai", "Yiping Bao", "Guanduo Chen", "Jiahao Chen", "Ningxin Chen", "Ruijue Chen", "Yanru Chen", "Yuankun Chen", "Yutian Chen", "Zhuofu Chen", "Jialei Cui", "Hao Ding", "Mengnan Dong", "Angang Du", "Chenzhuang Du", "Dikang Du", "Yulun Du", "Yu Fan", "Yichen Feng", "Kelin Fu", "Bofei Gao", "Hongcheng Gao", "Peizhong Gao", "Tong Gao", "Xinran Gu", "Longyu Guan", "Haiqing Guo", "Jianhang Guo", "Hao Hu", "Xiaoru Hao", "Tianhong He", "Weiran He", "Wenyang He", "Chao Hong", "Yangyang Hu", "Zhenxing Hu", "Weixiao Huang", "Zhiqi Huang", "Zihao Huang", "Tao Jiang", "Zhejun Jiang", "Xinyi Jin", "Yongsheng Kang", "Guokun Lai", "Cheng Li", "Fang Li", "Haoyang Li", "Ming Li", "Wentao Li", "Yanhao Li", "Yiwei Li", "Zhaowei Li", "Zheming Li", "Hongzhan Lin", "Xiaohan Lin", "Zongyu Lin", "Chengyin Liu", "Chenyu Liu", "Hongzhang Liu", "Jingyuan Liu", "Junqi Liu", "Liang Liu", "Shaowei Liu", "T. Y. Liu", "Tianwei Liu", "Weizhou Liu", "Yangyang Liu", "Yibo Liu", "Yiping Liu", "Yue Liu", "Zhengying Liu", "Enzhe Lu", "Lijun Lu", "Shengling Ma", "Xinyu Ma", "Yingwei Ma", "Shaoguang Mao", "Jie Mei", "Xin Men", "Yibo Miao", "Siyuan Pan", "Yebo Peng", "Ruoyu Qin", "Bowen Qu", "Zeyu Shang", "Lidong Shi", "Shengyuan Shi", "Feifan Song", "Jianlin Su", "Zhengyuan Su", "Xinjie Sun", "Flood Sung", "Heyi Tang", "Jiawen Tao", "Qifeng Teng", "Chensi Wang", "Dinglu Wang", "Feng Wang", "Haiming Wang", "Jianzhou Wang", "Jiaxing Wang", "Jinhong Wang", "Shengjie Wang", "Shuyi Wang", "Yao Wang", "Yejie Wang", "Yiqin Wang", "Yuxin Wang", "Yuzhi Wang", "Zhaoji Wang", "Zhengtao Wang", "Zhexu Wang", "Chu Wei", "Qianqian Wei", "Wenhao Wu", "Xingzhe Wu", "Yuxin Wu", "Chenjun Xiao", "Xiaotong Xie", "Weimin Xiong", "Boyu Xu", "Jing Xu", "Jinjing Xu", "L. H. Xu", "Lin Xu", "Suting Xu", "Weixin Xu", "Xinran Xu", "Yangchuan Xu", "Ziyao Xu", "Junjie Yan", "Yuzi Yan", "Xiaofei Yang", "Ying Yang", "Zhen Yang", "Zhilin Yang", "Zonghan Yang", "Haotian Yao", "Xingcheng Yao", "Wenjie Ye", "Zhuorui Ye", "Bohong Yin", "Longhui Yu", "Enming Yuan", "Hongbang Yuan", "Mengjie Yuan", "Haobing Zhan", "Dehao Zhang", "Hao Zhang", "Wanlu Zhang", "Xiaobin Zhang", "Yangkun Zhang", "Yizhi Zhang", "Yongting Zhang", "Yu Zhang", "Yutao Zhang", "Yutong Zhang", "Zheng Zhang", "Haotian Zhao", "Yikai Zhao", "Huabin Zheng", "Shaojie Zheng", "Jianren Zhou", "Xinyu Zhou", "Zaida Zhou", "Zhen Zhu", "Weiyu Zhuang", "Xinxing Zu"], "title": "Kimi K2: Open Agentic Intelligence", "comment": "tech report of Kimi K2", "summary": "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32\nbillion activated parameters and 1 trillion total parameters. We propose the\nMuonClip optimizer, which improves upon Muon with a novel QK-clip technique to\naddress training instability while enjoying the advanced token efficiency of\nMuon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero\nloss spike. During post-training, K2 undergoes a multi-stage post-training\nprocess, highlighted by a large-scale agentic data synthesis pipeline and a\njoint reinforcement learning (RL) stage, where the model improves its\ncapabilities through interactions with real and synthetic environments.\n  Kimi K2 achieves state-of-the-art performance among open-source non-thinking\nmodels, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on\nTau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on\nSWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in\nnon-thinking settings. It also exhibits strong capabilities in coding,\nmathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6,\n49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without\nextended thinking. These results position Kimi K2 as one of the most capable\nopen-source large language models to date, particularly in software engineering\nand agentic tasks. We release our base and post-trained model checkpoints to\nfacilitate future research and applications of agentic intelligence.", "AI": {"tldr": "Kimi K2\u662f\u4e00\u4e2a\u57fa\u4e8eMixture-of-Experts (MoE)\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u670932\u4ebf\u6fc0\u6d3b\u53c2\u6570\u548c1\u4e07\u4ebf\u603b\u53c2\u6570\uff0c\u901a\u8fc7MuonClip\u4f18\u5316\u5668\u89e3\u51b3\u4e86\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9636\u6bb5\u540e\u8bad\u7ec3\u4e2d\u63d0\u5347\u4e86\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u5728\u975e\u601d\u8003\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u8f6f\u4ef6\u5de5\u7a0b\u548c\u4ee3\u7406\u4efb\u52a1\u4e2d\uff0c\u540c\u65f6\u63d0\u4f9b\u5f00\u6e90\u6a21\u578b\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002", "method": "\u4f7f\u7528MuonClip\u4f18\u5316\u5668\uff08\u57fa\u4e8eMuon\u6539\u8fdb\u7684QK-clip\u6280\u672f\uff09\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u540e\u8bad\u7ec3\u9636\u6bb5\u5305\u62ec\u5927\u89c4\u6a21\u4ee3\u7406\u6570\u636e\u5408\u6210\u548c\u8054\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3002", "result": "Kimi K2\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982Tau2-Bench\uff0866.1\uff09\u3001ACEBench\uff08En\uff09\uff0876.5\uff09\u7b49\uff0c\u8d85\u8d8a\u4e86\u5927\u591a\u6570\u5f00\u6e90\u548c\u95ed\u6e90\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "Kimi K2\u662f\u76ee\u524d\u6700\u5f3a\u5927\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e4b\u4e00\uff0c\u5c24\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u548c\u4ee3\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u68c0\u67e5\u70b9\u4ee5\u63a8\u52a8\u7814\u7a76\u3002"}}
{"id": "2507.20571", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20571", "abs": "https://arxiv.org/abs/2507.20571", "authors": ["Shuaipeng Zhang", "Lanju Kong", "Yixin Zhang", "Wei He", "Yongqing Zheng", "Han Yu", "Lizhen Cui"], "title": "DAG-AFL:Directed Acyclic Graph-based Asynchronous Federated Learning", "comment": "6 pages, IEEE International Conference on Multimedia & Expo 2025\n  conference paper", "summary": "Due to the distributed nature of federated learning (FL), the vulnerability\nof the global model and the need for coordination among many client devices\npose significant challenges. As a promising decentralized, scalable and secure\nsolution, blockchain-based FL methods have attracted widespread attention in\nrecent years. However, traditional consensus mechanisms designed for Proof of\nWork (PoW) similar to blockchain incur substantial resource consumption and\ncompromise the efficiency of FL, particularly when participating devices are\nwireless and resource-limited. To address asynchronous client participation and\ndata heterogeneity in FL, while limiting the additional resource overhead\nintroduced by blockchain, we propose the Directed Acyclic Graph-based\nAsynchronous Federated Learning (DAG-AFL) framework. We develop a tip selection\nalgorithm that considers temporal freshness, node reachability and model\naccuracy, with a DAG-based trusted verification strategy. Extensive experiments\non 3 benchmarking datasets against eight state-of-the-art approaches\ndemonstrate that DAG-AFL significantly improves training efficiency and model\naccuracy by 22.7% and 6.5% on average, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u7684\u5f02\u6b65\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff08DAG-AFL\uff09\uff0c\u4ee5\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5f02\u6b65\u53c2\u4e0e\u548c\u6570\u636e\u5f02\u6784\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u533a\u5757\u94fe\u5f15\u5165\u7684\u8d44\u6e90\u5f00\u9500\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u7279\u6027\u5bfc\u81f4\u5168\u5c40\u6a21\u578b\u6613\u53d7\u653b\u51fb\u4e14\u9700\u8981\u5927\u91cf\u5ba2\u6237\u7aef\u8bbe\u5907\u534f\u8c03\uff0c\u533a\u5757\u94fe\u867d\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4f20\u7edf\u5171\u8bc6\u673a\u5236\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u5f71\u54cd\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8003\u8651\u65f6\u95f4\u65b0\u9c9c\u5ea6\u3001\u8282\u70b9\u53ef\u8fbe\u6027\u548c\u6a21\u578b\u51c6\u786e\u6027\u7684tip\u9009\u62e9\u7b97\u6cd5\uff0c\u5e76\u91c7\u7528DAG\u53ef\u4fe1\u9a8c\u8bc1\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4\u516b\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0cDAG-AFL\u5e73\u5747\u63d0\u5347\u8bad\u7ec3\u6548\u738722.7%\u548c\u6a21\u578b\u51c6\u786e\u73876.5%\u3002", "conclusion": "DAG-AFL\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8d44\u6e90\u5f00\u9500\u3002"}}
{"id": "2507.20573", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20573", "abs": "https://arxiv.org/abs/2507.20573", "authors": ["Yaxin Xiao", "Qingqing Ye", "Li Hu", "Huadi Zheng", "Haibo Hu", "Zi Liang", "Haoyang Li", "Yijie Jiao"], "title": "Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy", "comment": "Accepted by ICCV 2025", "summary": "Machine unlearning enables the removal of specific data from ML models to\nuphold the right to be forgotten. While approximate unlearning algorithms offer\nefficient alternatives to full retraining, this work reveals that they fail to\nadequately protect the privacy of unlearned data. In particular, these\nalgorithms introduce implicit residuals which facilitate privacy attacks\ntargeting at unlearned data. We observe that these residuals persist regardless\nof model architectures, parameters, and unlearning algorithms, exposing a new\nattack surface beyond conventional output-based leakage. Based on this insight,\nwe propose the Reminiscence Attack (ReA), which amplifies the correlation\nbetween residuals and membership privacy through targeted fine-tuning\nprocesses. ReA achieves up to 1.90x and 1.12x higher accuracy than prior\nattacks when inferring class-wise and sample-wise membership, respectively. To\nmitigate such residual-induced privacy risk, we develop a dual-phase\napproximate unlearning framework that first eliminates deep-layer unlearned\ndata traces and then enforces convergence stability to prevent models from\n\"pseudo-convergence\", where their outputs are similar to retrained models but\nstill preserve unlearned residuals. Our framework works for both classification\nand generation tasks. Experimental evaluations confirm that our approach\nmaintains high unlearning efficacy, while reducing the adaptive privacy attack\naccuracy to nearly random guess, at the computational cost of 2-12% of full\nretraining from scratch.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u8fd1\u4f3c\u9057\u5fd8\u7b97\u6cd5\u672a\u80fd\u5145\u5206\u4fdd\u62a4\u88ab\u9057\u5fd8\u6570\u636e\u7684\u9690\u79c1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\uff08ReA\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u53cc\u9636\u6bb5\u9057\u5fd8\u6846\u67b6\u4ee5\u964d\u4f4e\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u53d1\u73b0\u8fd1\u4f3c\u9057\u5fd8\u7b97\u6cd5\u4e2d\u5b58\u5728\u7684\u9690\u79c1\u6f0f\u6d1e\uff0c\u5c24\u5176\u662f\u9690\u5f0f\u6b8b\u7559\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u4ee5\u4fdd\u62a4\u88ab\u9057\u5fd8\u6570\u636e\u7684\u9690\u79c1\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86Reminiscence Attack\uff08ReA\uff09\u6765\u5229\u7528\u6b8b\u7559\u4fe1\u606f\u8fdb\u884c\u9690\u79c1\u653b\u51fb\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u53cc\u9636\u6bb5\u8fd1\u4f3c\u9057\u5fd8\u6846\u67b6\u4ee5\u6d88\u9664\u6b8b\u7559\u5e76\u9632\u6b62\u4f2a\u6536\u655b\u3002", "result": "ReA\u653b\u51fb\u5728\u63a8\u65ad\u7c7b\u7ea7\u548c\u6837\u672c\u7ea7\u6210\u5458\u9690\u79c1\u65f6\u5206\u522b\u6bd4\u73b0\u6709\u653b\u51fb\u9ad81.90\u500d\u548c1.12\u500d\u7684\u51c6\u786e\u7387\u3002\u53cc\u9636\u6bb5\u6846\u67b6\u5c06\u81ea\u9002\u5e94\u9690\u79c1\u653b\u51fb\u7684\u51c6\u786e\u7387\u964d\u81f3\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\uff0c\u8ba1\u7b97\u6210\u672c\u4ec5\u4e3a\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u76842-12%\u3002", "conclusion": "\u8bba\u6587\u8868\u660e\u8fd1\u4f3c\u9057\u5fd8\u7b97\u6cd5\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u63d0\u51fa\u7684\u53cc\u9636\u6bb5\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u6548\u9057\u5fd8\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u9690\u79c1\u653b\u51fb\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2507.20576", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20576", "abs": "https://arxiv.org/abs/2507.20576", "authors": ["Alexander Barklage", "Philipp Bekemeyer"], "title": "Fusing CFD and measurement data using transfer learning", "comment": null, "summary": "Aerodynamic analysis during aircraft design usually involves methods of\nvarying accuracy and spatial resolution, which all have their advantages and\ndisadvantages. It is therefore desirable to create data-driven models which\neffectively combine these advantages. Such data fusion methods for distributed\nquantities mainly rely on proper orthogonal decomposition as of now, which is a\nlinear method. In this paper, we introduce a non-linear method based on neural\nnetworks combining simulation and measurement data via transfer learning. The\nnetwork training accounts for the heterogeneity of the data, as simulation data\nusually features a high spatial resolution, while measurement data is sparse\nbut more accurate. In a first step, the neural network is trained on simulation\ndata to learn spatial features of the distributed quantities. The second step\ninvolves transfer learning on the measurement data to correct for systematic\nerrors between simulation and measurement by only re-training a small subset of\nthe entire neural network model. This approach is applied to a multilayer\nperceptron architecture and shows significant improvements over the established\nmethod based on proper orthogonal decomposition by producing more physical\nsolutions near nonlinearities. In addition, the neural network provides\nsolutions at arbitrary flow conditions, thus making the model useful for flight\nmechanical design, structural sizing, and certification. As the proposed\ntraining strategy is very general, it can also be applied to more complex\nneural network architectures in the future.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u7ed3\u5408\u4eff\u771f\u548c\u6d4b\u91cf\u6570\u636e\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u7ebf\u6027\u65b9\u6cd5\uff08\u5982POD\uff09\u5728\u6c14\u52a8\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u6c14\u52a8\u5206\u6790\u65b9\u6cd5\uff08\u5982POD\uff09\u662f\u7ebf\u6027\u7684\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u975e\u7ebf\u6027\u95ee\u9898\uff0c\u4e14\u4eff\u771f\u6570\u636e\u5206\u8fa8\u7387\u9ad8\u4f46\u8bef\u5dee\u5927\uff0c\u6d4b\u91cf\u6570\u636e\u7a00\u758f\u4f46\u51c6\u786e\uff0c\u9700\u4e00\u79cd\u80fd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u5206\u4e24\u6b65\u8bad\u7ec3\uff1a\u5148\u5728\u4eff\u771f\u6570\u636e\u4e0a\u5b66\u4e60\u7a7a\u95f4\u7279\u5f81\uff0c\u518d\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5728\u6d4b\u91cf\u6570\u636e\u4e0a\u4fee\u6b63\u7cfb\u7edf\u8bef\u5dee\uff0c\u4ec5\u91cd\u65b0\u8bad\u7ec3\u90e8\u5206\u7f51\u7edc\u3002", "result": "\u76f8\u6bd4POD\u65b9\u6cd5\uff0c\u795e\u7ecf\u7f51\u7edc\u5728\u975e\u7ebf\u6027\u533a\u57df\u751f\u6210\u66f4\u7269\u7406\u7684\u89e3\uff0c\u4e14\u80fd\u9002\u5e94\u4efb\u610f\u6d41\u52a8\u6761\u4ef6\uff0c\u9002\u7528\u4e8e\u98de\u884c\u529b\u5b66\u8bbe\u8ba1\u3001\u7ed3\u6784\u5c3a\u5bf8\u548c\u8ba4\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u7528\u6027\u5f3a\uff0c\u672a\u6765\u53ef\u5e94\u7528\u4e8e\u66f4\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002"}}
{"id": "2507.20592", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20592", "abs": "https://arxiv.org/abs/2507.20592", "authors": ["Fei Kong", "Xiaohan Shan", "Yanwei Hu", "Jianmin Li"], "title": "PhaseNAS: Language-Model Driven Architecture Search with Dynamic Phase Adaptation", "comment": "14pages", "summary": "Neural Architecture Search (NAS) is challenged by the trade-off between\nsearch space exploration and efficiency, especially for complex tasks. While\nrecent LLM-based NAS methods have shown promise, they often suffer from static\nsearch strategies and ambiguous architecture representations. We propose\nPhaseNAS, an LLM-based NAS framework with dynamic phase transitions guided by\nreal-time score thresholds and a structured architecture template language for\nconsistent code generation. On the NAS-Bench-Macro benchmark, PhaseNAS\nconsistently discovers architectures with higher accuracy and better rank. For\nimage classification (CIFAR-10/100), PhaseNAS reduces search time by up to 86%\nwhile maintaining or improving accuracy. In object detection, it automatically\nproduces YOLOv8 variants with higher mAP and lower resource cost. These results\ndemonstrate that PhaseNAS enables efficient, adaptive, and generalizable NAS\nacross diverse vision tasks.", "AI": {"tldr": "PhaseNAS\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684NAS\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9636\u6bb5\u8f6c\u6362\u548c\u7ed3\u6784\u5316\u6a21\u677f\u8bed\u8a00\uff0c\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u6548\u7387\u548c\u67b6\u6784\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM-based NAS\u65b9\u6cd5\u4e2d\u9759\u6001\u641c\u7d22\u7b56\u7565\u548c\u6a21\u7cca\u67b6\u6784\u8868\u793a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u641c\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u52a8\u6001\u9636\u6bb5\u8f6c\u6362\uff08\u5b9e\u65f6\u5206\u6570\u9608\u503c\u5f15\u5bfc\uff09\u548c\u7ed3\u6784\u5316\u67b6\u6784\u6a21\u677f\u8bed\u8a00\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u4ee3\u7801\u751f\u6210\u3002", "result": "\u5728NAS-Bench-Macro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bCIFAR-10/100\u4e0a\u641c\u7d22\u65f6\u95f4\u51cf\u5c1186%\uff1bYOLOv8\u53d8\u4f53\u5728\u76ee\u6807\u68c0\u6d4b\u4e2dmAP\u66f4\u9ad8\u4e14\u8d44\u6e90\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "PhaseNAS\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u4e14\u901a\u7528\u7684NAS\u3002"}}
{"id": "2507.20644", "categories": ["cs.LG", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2507.20644", "abs": "https://arxiv.org/abs/2507.20644", "authors": ["Julia Siekiera", "Christian Schl\u00f6tterer", "Stefan Kramer"], "title": "Deep Generative Models of Evolution: SNP-level Population Adaptation by Genomic Linkage Incorporation", "comment": "10 pages, 5 figures", "summary": "The investigation of allele frequency trajectories in populations evolving\nunder controlled environmental pressures has become a popular approach to study\nevolutionary processes on the molecular level. Statistical models based on\nwell-defined evolutionary concepts can be used to validate different hypotheses\nabout empirical observations. Despite their popularity, classic statistical\nmodels like the Wright-Fisher model suffer from simplified assumptions such as\nthe independence of selected loci along a chromosome and uncertainty about the\nparameters. Deep generative neural networks offer a powerful alternative known\nfor the integration of multivariate dependencies and noise reduction. Due to\ntheir high data demands and challenging interpretability they have, so far, not\nbeen widely considered in the area of population genomics. To address the\nchallenges in the area of Evolve and Resequencing experiments (E&R) based on\npooled sequencing (Pool-Seq) data, we introduce a deep generative neural\nnetwork that aims to model a concept of evolution based on empirical\nobservations over time. The proposed model estimates the distribution of allele\nfrequency trajectories by embedding the observations from single nucleotide\npolymorphisms (SNPs) with information from neighboring loci. Evaluation on\nsimulated E&R experiments demonstrates the model's ability to capture the\ndistribution of allele frequency trajectories and illustrates the\nrepresentational power of deep generative models on the example of linkage\ndisequilibrium (LD) estimation. Inspecting the internally learned\nrepresentations enables estimating pairwise LD, which is typically inaccessible\nin Pool-Seq data. Our model provides competitive LD estimation in Pool-Seq data\nhigh degree of LD when compared to existing methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u548c\u4f30\u8ba1\u7b49\u4f4d\u57fa\u56e0\u9891\u7387\u8f68\u8ff9\u7684\u5206\u5e03\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7fa4\u4f53\u57fa\u56e0\u7ec4\u5b66\u4e2d\u7684Evolve and Resequencing\u5b9e\u9a8c\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\uff08\u5982Wright-Fisher\u6a21\u578b\uff09\u5728\u5047\u8bbe\u548c\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u6df1\u5ea6\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u6574\u5408\u591a\u53d8\u91cf\u4f9d\u8d56\u6027\u548c\u964d\u566a\uff0c\u4f46\u56e0\u5176\u9ad8\u6570\u636e\u9700\u6c42\u548c\u89e3\u91ca\u6027\u6311\u6218\uff0c\u5c1a\u672a\u5e7f\u6cdb\u5e94\u7528\u4e8e\u7fa4\u4f53\u57fa\u56e0\u7ec4\u5b66\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u79cd\u6df1\u5ea6\u751f\u6210\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5d4c\u5165\u5355\u6838\u82f7\u9178\u591a\u6001\u6027\uff08SNPs\uff09\u53ca\u5176\u90bb\u8fd1\u4f4d\u70b9\u7684\u4fe1\u606f\uff0c\u6a21\u62df\u57fa\u4e8e\u65f6\u95f4\u7ecf\u9a8c\u89c2\u5bdf\u7684\u8fdb\u5316\u6982\u5ff5\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u6a21\u578b\u80fd\u591f\u6355\u6349\u7b49\u4f4d\u57fa\u56e0\u9891\u7387\u8f68\u8ff9\u7684\u5206\u5e03\uff0c\u5e76\u5728\u8fde\u9501\u4e0d\u5e73\u8861\uff08LD\uff09\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6a21\u578b\u5c55\u793a\u4e86\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u7fa4\u4f53\u57fa\u56e0\u7ec4\u5b66\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406Pool-Seq\u6570\u636e\u65f6\uff0c\u80fd\u591f\u4f30\u8ba1\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u83b7\u53d6\u7684\u8fde\u9501\u4e0d\u5e73\u8861\u4fe1\u606f\u3002"}}
{"id": "2507.20678", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20678", "abs": "https://arxiv.org/abs/2507.20678", "authors": ["Filip de Roos", "Fabio Muratore"], "title": "Novel Pivoted Cholesky Decompositions for Efficient Gaussian Process Inference", "comment": null, "summary": "The Cholesky decomposition is a fundamental tool for solving linear systems\nwith symmetric and positive definite matrices which are ubiquitous in linear\nalgebra, optimization, and machine learning. Its numerical stability can be\nimproved by introducing a pivoting strategy that iteratively permutes the rows\nand columns of the matrix. The order of pivoting indices determines how\naccurately the intermediate decomposition can reconstruct the original matrix,\nthus is decisive for the algorithm's efficiency in the case of early\ntermination. Standard implementations select the next pivot from the largest\nvalue on the diagonal. In the case of Bayesian nonparametric inference, this\nstrategy corresponds to greedy entropy maximization, which is often used in\nactive learning and design of experiments. We explore this connection in detail\nand deduce novel pivoting strategies for the Cholesky decomposition. The\nresulting algorithms are more efficient at reducing the uncertainty over a data\nset, can be updated to include information about observations, and additionally\nbenefit from a tailored implementation. We benchmark the effectiveness of the\nnew selection strategies on two tasks important to Gaussian processes: sparse\nregression and inference based on preconditioned iterative solvers. Our results\nshow that the proposed selection strategies are either on par or, in most\ncases, outperform traditional baselines while requiring a negligible amount of\nadditional computation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u6539\u8fdbCholesky\u5206\u89e3\u7684\u67a2\u8f74\u7b56\u7565\uff0c\u63d0\u51fa\u65b0\u7684\u67a2\u8f74\u9009\u62e9\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u7b97\u6cd5\u6548\u7387\uff0c\u5e76\u5728\u9ad8\u65af\u8fc7\u7a0b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "Cholesky\u5206\u89e3\u5728\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u7684\u7ebf\u6027\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u67a2\u8f74\u9009\u62e9\u7b56\u7565\u53ef\u80fd\u6548\u7387\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u67a2\u8f74\u7b56\u7565\u63d0\u5347\u5176\u6570\u503c\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u67a2\u8f74\u9009\u62e9\u7b56\u7565\uff0c\u57fa\u4e8e\u8d1d\u53f6\u65af\u975e\u53c2\u6570\u63a8\u65ad\u4e2d\u7684\u71b5\u6700\u5927\u5316\u601d\u60f3\uff0c\u4f18\u5316\u5206\u89e3\u8fc7\u7a0b\u3002\u7b97\u6cd5\u652f\u6301\u52a8\u6001\u66f4\u65b0\u89c2\u6d4b\u4fe1\u606f\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u5b9e\u73b0\u5b9a\u5236\u5316\u3002", "result": "\u65b0\u7b56\u7565\u5728\u9ad8\u65af\u8fc7\u7a0b\u4efb\u52a1\uff08\u7a00\u758f\u56de\u5f52\u548c\u8fed\u4ee3\u6c42\u89e3\u5668\u63a8\u65ad\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "conclusion": "\u6539\u8fdb\u7684\u67a2\u8f74\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86Cholesky\u5206\u89e3\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u9ad8\u65af\u8fc7\u7a0b\u4efb\u52a1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.20708", "categories": ["cs.LG", "math.OC", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.20708", "abs": "https://arxiv.org/abs/2507.20708", "authors": ["Valentin Lafargue", "Adriana Laurindo Monteiro", "Emmanuelle Claeys", "Laurent Risser", "Jean-Michel Loubes"], "title": "Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks", "comment": null, "summary": "Proving the compliance of AI algorithms has become an important challenge\nwith the growing deployment of such algorithms for real-life applications.\nInspecting possible biased behaviors is mandatory to satisfy the constraints of\nthe regulations of the EU Artificial Intelligence's Act. Regulation-driven\naudits increasingly rely on global fairness metrics, with Disparate Impact\nbeing the most widely used. Yet such global measures depend highly on the\ndistribution of the sample on which the measures are computed. We investigate\nfirst how to manipulate data samples to artificially satisfy fairness criteria,\ncreating minimally perturbed datasets that remain statistically\nindistinguishable from the original distribution while satisfying prescribed\nfairness constraints. Then we study how to detect such manipulation. Our\nanalysis (i) introduces mathematically sound methods for modifying empirical\ndistributions under fairness constraints using entropic or optimal transport\nprojections, (ii) examines how an auditee could potentially circumvent fairness\ninspections, and (iii) offers recommendations to help auditors detect such data\nmanipulations. These results are validated through experiments on classical\ntabular datasets in bias detection.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6570\u636e\u64cd\u7eb5\u6ee1\u8db3\u516c\u5e73\u6027\u6807\u51c6\uff0c\u5e76\u7814\u7a76\u4e86\u5982\u4f55\u68c0\u6d4b\u8fd9\u79cd\u64cd\u7eb5\u3002", "motivation": "\u968f\u7740AI\u7b97\u6cd5\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u8bc1\u660e\u5176\u5408\u89c4\u6027\u6210\u4e3a\u91cd\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u6ee1\u8db3\u6b27\u76dfAI\u6cd5\u6848\u7684\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u6570\u5b66\u65b9\u6cd5\uff08\u71b5\u6216\u6700\u4f18\u4f20\u8f93\u6295\u5f71\uff09\u4fee\u6539\u7ecf\u9a8c\u5206\u5e03\u4ee5\u6ee1\u8db3\u516c\u5e73\u6027\u7ea6\u675f\uff0c\u5e76\u7814\u7a76\u5982\u4f55\u68c0\u6d4b\u6570\u636e\u64cd\u7eb5\u3002", "result": "\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u7ecf\u5178\u8868\u683c\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u68c0\u6d4b\u6570\u636e\u64cd\u7eb5\u7684\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5ba1\u8ba1\u8005\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u5efa\u8bae\uff0c\u4ee5\u5e94\u5bf9\u6f5c\u5728\u7684\u6570\u636e\u64cd\u7eb5\u884c\u4e3a\u3002"}}
{"id": "2507.20714", "categories": ["cs.LG", "cs.AI", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.20714", "abs": "https://arxiv.org/abs/2507.20714", "authors": ["Asma Sadia Khan", "Fariba Tasnia Khan", "Tanjim Mahmud", "Salman Karim Khan", "Rishita Chakma", "Nahed Sharmen", "Mohammad Shahadat Hossain", "Karl Andersson"], "title": "Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI", "comment": null, "summary": "Prostate cancer, the second most prevalent male malignancy, requires advanced\ndiagnostic tools. We propose an explainable AI system combining BERT (for\ntextual clinical notes) and Random Forest (for numerical lab data) through a\nnovel multimodal fusion strategy, achieving superior classification performance\non PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is\nestablished, our work demonstrates that a simple yet interpretable BERT+RF\npipeline delivers clinically significant improvements - particularly for\nintermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824\nnumerical/0.725 textual). SHAP analysis provides transparent feature importance\nrankings, while ablation studies prove textual features' complementary value.\nThis accessible approach offers hospitals a balance of high performance\n(F1=89%), computational efficiency, and clinical interpretability - addressing\ncritical needs in prostate cancer diagnostics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BERT\u548c\u968f\u673a\u68ee\u6797\u7684\u53ef\u89e3\u91caAI\u7cfb\u7edf\uff0c\u7528\u4e8e\u524d\u5217\u817a\u764c\u8bca\u65ad\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u4e34\u5e8a\u53ef\u89e3\u91ca\u3002", "motivation": "\u524d\u5217\u817a\u764c\u8bca\u65ad\u9700\u8981\u9ad8\u7ea7\u5de5\u5177\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7ed3\u5408BERT\u5904\u7406\u6587\u672c\u4e34\u5e8a\u7b14\u8bb0\u548c\u968f\u673a\u68ee\u6797\u5904\u7406\u6570\u503c\u5b9e\u9a8c\u5ba4\u6570\u636e\uff0c\u91c7\u7528\u65b0\u578b\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u3002", "result": "\u5728PLCO-NIH\u6570\u636e\u96c6\u4e0a\u8fbe\u523098%\u51c6\u786e\u7387\u548c99% AUC\uff0c\u5c24\u5176\u5728\u4e2d\u7ea7\u764c\u75c7\u9636\u6bb5\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u6ee1\u8db3\u524d\u5217\u817a\u764c\u8bca\u65ad\u9700\u6c42\u3002"}}
{"id": "2507.20718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20718", "abs": "https://arxiv.org/abs/2507.20718", "authors": ["Sungjun Lim", "Kangjun Noh", "Youngjun Choi", "Heeyoung Lee", "Kyungwoo Song"], "title": "Uncertainty-driven Embedding Convolution", "comment": null, "summary": "Text embeddings are essential components in modern NLP pipelines. While\nnumerous embedding models have been proposed, their performance varies across\ndomains, and no single model consistently excels across all tasks. This\nvariability motivates the use of ensemble techniques to combine complementary\nstrengths. However, most existing ensemble methods operate on deterministic\nembeddings and fail to account for model-specific uncertainty, limiting their\nrobustness and reliability in downstream applications. To address these\nlimitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC\nfirst transforms deterministic embeddings into probabilistic ones in a post-hoc\nmanner. It then computes adaptive ensemble weights based on embedding\nuncertainty, grounded in a Bayes-optimal solution under a surrogate loss.\nAdditionally, UEC introduces an uncertainty-aware similarity function that\ndirectly incorporates uncertainty into similarity scoring. Extensive\nexperiments on retrieval, classification, and semantic similarity benchmarks\ndemonstrate that UEC consistently improves both performance and robustness by\nleveraging principled uncertainty modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6587\u672c\u5d4c\u5165\u96c6\u6210\u65b9\u6cd5UEC\uff0c\u901a\u8fc7\u540e\u9a8c\u6982\u7387\u5316\u5d4c\u5165\u548c\u81ea\u9002\u5e94\u6743\u91cd\u8ba1\u7b97\u63d0\u5347\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5d4c\u5165\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\uff0c\u4e14\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u672a\u8003\u8651\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u9c81\u68d2\u6027\u3002", "method": "UEC\u5c06\u786e\u5b9a\u6027\u5d4c\u5165\u8f6c\u5316\u4e3a\u6982\u7387\u5d4c\u5165\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u8ba1\u7b97\u81ea\u9002\u5e94\u6743\u91cd\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u76f8\u4f3c\u6027\u8bc4\u5206\u3002", "result": "\u5728\u68c0\u7d22\u3001\u5206\u7c7b\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\u4e2d\uff0cUEC\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "UEC\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u6709\u6548\u63d0\u5347\u4e86\u5d4c\u5165\u96c6\u6210\u7684\u6548\u679c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2507.20836", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20836", "abs": "https://arxiv.org/abs/2507.20836", "authors": ["Jakob Snel", "Seong Joon Oh"], "title": "First Hallucination Tokens Are Different from Conditional Ones", "comment": "4.5 pages, 3 figures, Dataset, Knowledge Paper, Hallucination,\n  Trustworthiness", "summary": "Hallucination, the generation of untruthful content, is one of the major\nconcerns regarding foundational models. Detecting hallucinations at the token\nlevel is vital for real-time filtering and targeted correction, yet the\nvariation of hallucination signals within token sequences is not fully\nunderstood. Leveraging the RAGTruth corpus with token-level annotations and\nreproduced logits, we analyse how these signals depend on a token's position\nwithin hallucinated spans, contributing to an improved understanding of\ntoken-level hallucination. Our results show that the first hallucinated token\ncarries a stronger signal and is more detectable than conditional tokens. We\nrelease our analysis framework, along with code for logit reproduction and\nmetric computation at https://github.com/jakobsnl/RAGTruth_Xtended.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u7840\u6a21\u578b\u4e2d\u5e7b\u89c9\uff08\u751f\u6210\u4e0d\u771f\u5b9e\u5185\u5bb9\uff09\u7684\u68c0\u6d4b\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5e7b\u89c9\u4fe1\u53f7\u5728\u6807\u8bb0\u5e8f\u5217\u4e2d\u7684\u53d8\u5316\uff0c\u53d1\u73b0\u9996\u4e2a\u5e7b\u89c9\u6807\u8bb0\u7684\u4fe1\u53f7\u66f4\u5f3a\u4e14\u66f4\u6613\u68c0\u6d4b\u3002", "motivation": "\u5e7b\u89c9\u662f\u57fa\u7840\u6a21\u578b\u7684\u4e3b\u8981\u95ee\u9898\u4e4b\u4e00\uff0c\u5b9e\u65f6\u68c0\u6d4b\u548c\u7ea0\u6b63\u5e7b\u89c9\u5bf9\u6a21\u578b\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5bf9\u6807\u8bb0\u7ea7\u5e7b\u89c9\u4fe1\u53f7\u7684\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u5229\u7528RAGTruth\u8bed\u6599\u5e93\u7684\u6807\u8bb0\u7ea7\u6ce8\u91ca\u548c\u590d\u73b0\u7684logits\uff0c\u5206\u6790\u5e7b\u89c9\u4fe1\u53f7\u5982\u4f55\u4f9d\u8d56\u4e8e\u6807\u8bb0\u5728\u5e7b\u89c9\u7247\u6bb5\u4e2d\u7684\u4f4d\u7f6e\u3002", "result": "\u9996\u4e2a\u5e7b\u89c9\u6807\u8bb0\u7684\u4fe1\u53f7\u66f4\u5f3a\u4e14\u66f4\u6613\u68c0\u6d4b\uff0c\u800c\u6761\u4ef6\u6807\u8bb0\u7684\u4fe1\u53f7\u8f83\u5f31\u3002", "conclusion": "\u7814\u7a76\u6539\u8fdb\u4e86\u5bf9\u6807\u8bb0\u7ea7\u5e7b\u89c9\u7684\u7406\u89e3\uff0c\u5e76\u53d1\u5e03\u4e86\u5206\u6790\u6846\u67b6\u548c\u4ee3\u7801\u3002"}}
{"id": "2507.20838", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.20838", "abs": "https://arxiv.org/abs/2507.20838", "authors": ["Yongzheng Liu", "Yiming Wang", "Po Xu", "Yingjie Xu", "Yuntian Chen", "Dongxiao Zhang"], "title": "BuildSTG: A Multi-building Energy Load Forecasting Method using Spatio-Temporal Graph Neural Network", "comment": null, "summary": "Due to the extensive availability of operation data, data-driven methods show\nstrong capabilities in predicting building energy loads. Buildings with similar\nfeatures often share energy patterns, reflected by spatial dependencies in\ntheir operational data, which conventional prediction methods struggle to\ncapture. To overcome this, we propose a multi-building prediction approach\nusing spatio-temporal graph neural networks, comprising graph representation,\ngraph learning, and interpretation. First, a graph is built based on building\ncharacteristics and environmental factors. Next, a multi-level graph\nconvolutional architecture with attention is developed for energy prediction.\nLastly, a method interpreting the optimized graph structure is introduced.\nExperiments on the Building Data Genome Project 2 dataset confirm superior\nperformance over baselines such as XGBoost, SVR, FCNN, GRU, and Naive,\nhighlighting the method's robustness, generalization, and interpretability in\ncapturing meaningful building similarities and spatial relationships.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u5efa\u7b51\u80fd\u8017\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u3001\u5b66\u4e60\u548c\u89e3\u91ca\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5efa\u7b51\u95f4\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u800c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5efa\u7b51\u80fd\u8017\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5efa\u7b51\u7279\u5f81\u548c\u73af\u5883\u56e0\u7d20\u7684\u56fe\uff0c\u5f00\u53d1\u5e26\u6ce8\u610f\u529b\u7684\u591a\u5c42\u6b21\u56fe\u5377\u79ef\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u56fe\u7ed3\u6784\u89e3\u91ca\u65b9\u6cd5\u3002", "result": "\u5728Building Data Genome Project 2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eXGBoost\u3001SVR\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u5efa\u7b51\u95f4\u7684\u76f8\u4f3c\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u4e3a\u5efa\u7b51\u80fd\u8017\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.20840", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20840", "abs": "https://arxiv.org/abs/2507.20840", "authors": ["Udo Schlegel", "Gabriel Marques Tavares", "Thomas Seidl"], "title": "Towards Explainable Deep Clustering for Time Series Data", "comment": "14 pages, accepted at TempXAI Workshop at ECML-PKDD 2025", "summary": "Deep clustering uncovers hidden patterns and groups in complex time series\ndata, yet its opaque decision-making limits use in safety-critical settings.\nThis survey offers a structured overview of explainable deep clustering for\ntime series, collecting current methods and their real-world applications. We\nthoroughly discuss and compare peer-reviewed and preprint papers through\napplication domains across healthcare, finance, IoT, and climate science. Our\nanalysis reveals that most work relies on autoencoder and attention\narchitectures, with limited support for streaming, irregularly sampled, or\nprivacy-preserved series, and interpretability is still primarily treated as an\nadd-on. To push the field forward, we outline six research opportunities: (1)\ncombining complex networks with built-in interpretability; (2) setting up\nclear, faithfulness-focused evaluation metrics for unsupervised explanations;\n(3) building explainers that adapt to live data streams; (4) crafting\nexplanations tailored to specific domains; (5) adding human-in-the-loop methods\nthat refine clusters and explanations together; and (6) improving our\nunderstanding of how time series clustering models work internally. By making\ninterpretability a primary design goal rather than an afterthought, we propose\nthe groundwork for the next generation of trustworthy deep clustering time\nseries analytics.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u805a\u7c7b\u5728\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ca\u5176\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u516d\u4e2a\u7814\u7a76\u65b9\u5411\u4ee5\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002", "motivation": "\u6df1\u5ea6\u805a\u7c7b\u5728\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u80fd\u53d1\u73b0\u9690\u85cf\u6a21\u5f0f\u548c\u5206\u7ec4\uff0c\u4f46\u5176\u4e0d\u900f\u660e\u7684\u51b3\u7b56\u8fc7\u7a0b\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u548c\u5206\u6790\u540c\u884c\u8bc4\u5ba1\u548c\u9884\u5370\u672c\u8bba\u6587\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u5e94\u7528\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\u3001\u7269\u8054\u7f51\u548c\u6c14\u5019\u79d1\u5b66\uff09\u4e2d\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u81ea\u7f16\u7801\u5668\u548c\u6ce8\u610f\u529b\u67b6\u6784\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5bf9\u6d41\u5f0f\u3001\u4e0d\u89c4\u5219\u91c7\u6837\u6216\u9690\u79c1\u4fdd\u62a4\u5e8f\u5217\u7684\u652f\u6301\u6709\u9650\uff0c\u4e14\u53ef\u89e3\u91ca\u6027\u901a\u5e38\u4f5c\u4e3a\u9644\u52a0\u529f\u80fd\u5904\u7406\u3002", "conclusion": "\u63d0\u51fa\u4e86\u516d\u4e2a\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u5c06\u53ef\u89e3\u91ca\u6027\u4f5c\u4e3a\u8bbe\u8ba1\u76ee\u6807\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u4fe1\u8d56\u7684\u6df1\u5ea6\u805a\u7c7b\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.20853", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20853", "abs": "https://arxiv.org/abs/2507.20853", "authors": ["Saket Tiwari", "Omer Gottesman", "George Konidaris"], "title": "Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces", "comment": "Proceedings of the Thirteenth International Conference on Learning\n  Representations (ICLR 2025). arXiv admin note: text overlap with\n  arXiv:2301.00009", "summary": "Advances in reinforcement learning (RL) have led to its successful\napplication in complex tasks with continuous state and action spaces. Despite\nthese advances in practice, most theoretical work pertains to finite state and\naction spaces. We propose building a theoretical understanding of continuous\nstate and action spaces by employing a geometric lens to understand the locally\nattained set of states. The set of all parametrised policies learnt through a\nsemi-gradient based approach induces a set of attainable states in RL. We show\nthat the training dynamics of a two-layer neural policy induce a low\ndimensional manifold of attainable states embedded in the high-dimensional\nnominal state space trained using an actor-critic algorithm. We prove that,\nunder certain conditions, the dimensionality of this manifold is of the order\nof the dimensionality of the action space. This is the first result of its\nkind, linking the geometry of the state space to the dimensionality of the\naction space. We empirically corroborate this upper bound for four MuJoCo\nenvironments and also demonstrate the results in a toy environment with varying\ndimensionality. We also show the applicability of this theoretical result by\nintroducing a local manifold learning layer to the policy and value function\nnetworks to improve the performance in control environments with very high\ndegrees of freedom by changing one layer of the neural network to learn sparse\nrepresentations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u89c6\u89d2\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5c40\u90e8\u53ef\u8fbe\u72b6\u6001\u96c6\uff0c\u5e76\u8bc1\u660e\u4e86\u52a8\u4f5c\u7a7a\u95f4\u7ef4\u5ea6\u4e0e\u53ef\u8fbe\u72b6\u6001\u6d41\u5f62\u7ef4\u5ea6\u7684\u5173\u7cfb\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u7406\u8bba\u7814\u7a76\u4ecd\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6709\u9650\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u89c6\u89d2\u5206\u6790\u5c40\u90e8\u53ef\u8fbe\u72b6\u6001\u96c6\uff0c\u4f7f\u7528\u4e24\u5c42\u795e\u7ecf\u7b56\u7565\u7684\u8bad\u7ec3\u52a8\u6001\u6765\u8bc1\u660e\u53ef\u8fbe\u72b6\u6001\u6d41\u5f62\u7684\u4f4e\u7ef4\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86\u53ef\u8fbe\u72b6\u6001\u6d41\u5f62\u7684\u7ef4\u5ea6\u4e0e\u52a8\u4f5c\u7a7a\u95f4\u7ef4\u5ea6\u76f8\u5173\uff0c\u5e76\u5728MuJoCo\u73af\u5883\u548c\u73a9\u5177\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u7406\u8bba\u3002", "conclusion": "\u8be5\u7406\u8bba\u6846\u67b6\u4e3a\u9ad8\u81ea\u7531\u5ea6\u63a7\u5236\u73af\u5883\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u8868\u793a\u5b66\u4e60\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.20862", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20862", "abs": "https://arxiv.org/abs/2507.20862", "authors": ["Shomoita Jahid Mitin", "Rodrigue Rizk", "Maximilian Scherer", "Thomas Koeglsperger", "Daniel Lench", "KC Santosh", "Arun Singh"], "title": "Bi-cephalic self-attended model to classify Parkinson's disease patients with freezing of gait", "comment": "26 pages, 5944 words, 4 figures, 2 tables, European Journal of\n  Neuroscience: Special edition FOG", "summary": "Parkinson Disease (PD) often results in motor and cognitive impairments,\nincluding gait dysfunction, particularly in patients with freezing of gait\n(FOG). Current detection methods are either subjective or reliant on\nspecialized gait analysis tools. This study aims to develop an objective,\ndata-driven, and multi-modal classification model to detect gait dysfunction in\nPD patients using resting-state EEG signals combined with demographic and\nclinical variables. We utilized a dataset of 124 participants: 42 PD patients\nwith FOG (PDFOG+), 41 without FOG (PDFOG-), and 41 age-matched healthy\ncontrols. Features extracted from resting-state EEG and descriptive variables\n(age, education, disease duration) were used to train a novel Bi-cephalic\nSelf-Attention Model (BiSAM). We tested three modalities: signal-only,\ndescriptive-only, and multi-modal, across different EEG channel subsets\n(BiSAM-63, -16, -8, and -4). Signal-only and descriptive-only models showed\nlimited performance, achieving a maximum accuracy of 55% and 68%, respectively.\nIn contrast, the multi-modal models significantly outperformed both, with\nBiSAM-8 and BiSAM-4 achieving the highest classification accuracy of 88%. These\nresults demonstrate the value of integrating EEG with objective descriptive\nfeatures for robust PDFOG+ detection. This study introduces a multi-modal,\nattention-based architecture that objectively classifies PDFOG+ using minimal\nEEG channels and descriptive variables. This approach offers a scalable and\nefficient alternative to traditional assessments, with potential applications\nin routine clinical monitoring and early diagnosis of PD-related gait\ndysfunction.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u6570\u636e\u7684BiSAM\u6a21\u578b\uff0c\u7ed3\u5408EEG\u4fe1\u53f7\u548c\u4e34\u5e8a\u53d8\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e15\u91d1\u68ee\u75c5\u51bb\u7ed3\u6b65\u6001\uff08PDFOG+\uff09\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u5e15\u91d1\u68ee\u75c5\u51bb\u7ed3\u6b65\u6001\u7684\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u89c2\u6027\u5f3a\u6216\u4f9d\u8d56\u4e13\u4e1a\u8bbe\u5907\uff0c\u9700\u8981\u4e00\u79cd\u5ba2\u89c2\u3001\u6570\u636e\u9a71\u52a8\u7684\u591a\u6a21\u6001\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528124\u540d\u53c2\u4e0e\u8005\uff08PDFOG+\u3001PDFOG-\u548c\u5065\u5eb7\u5bf9\u7167\u7ec4\uff09\u7684\u9759\u606f\u6001EEG\u4fe1\u53f7\u548c\u4e34\u5e8a\u53d8\u91cf\uff0c\u8bad\u7ec3BiSAM\u6a21\u578b\uff0c\u6d4b\u8bd5\u4e86\u4fe1\u53f7\u3001\u63cf\u8ff0\u548c\u591a\u6a21\u6001\u4e09\u79cd\u6a21\u5f0f\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\uff08BiSAM-8\u548cBiSAM-4\uff09\u5206\u7c7b\u51c6\u786e\u7387\u6700\u9ad8\uff0888%\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4fe1\u53f7\uff0855%\uff09\u548c\u63cf\u8ff0\uff0868%\uff09\u5355\u72ec\u6a21\u578b\u3002", "conclusion": "\u7ed3\u5408EEG\u548c\u4e34\u5e8a\u53d8\u91cf\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u4e3aPDFOG+\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u76d1\u6d4b\u548c\u65e9\u671f\u8bca\u65ad\u3002"}}
{"id": "2507.20894", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20894", "abs": "https://arxiv.org/abs/2507.20894", "authors": ["Lara Neves", "Afonso Louren\u00e7o", "Alberto Cano", "Goreti Marreiros"], "title": "Online hierarchical partitioning of the output space in extreme multi-label data stream", "comment": "Accepted at 28th European Conference on Artificial Intelligence (ECAI\n  2025)", "summary": "Mining data streams with multi-label outputs poses significant challenges due\nto evolving distributions, high-dimensional label spaces, sparse label\noccurrences, and complex label dependencies. Moreover, concept drift affects\nnot only input distributions but also label correlations and imbalance ratios\nover time, complicating model adaptation. To address these challenges,\nstructured learners are categorized into local and global methods. Local\nmethods break down the task into simpler components, while global methods adapt\nthe algorithm to the full output space, potentially yielding better predictions\nby exploiting label correlations. This work introduces iHOMER (Incremental\nHierarchy Of Multi-label Classifiers), an online multi-label learning framework\nthat incrementally partitions the label space into disjoint, correlated\nclusters without relying on predefined hierarchies. iHOMER leverages online\ndivisive-agglomerative clustering based on \\textit{Jaccard} similarity and a\nglobal tree-based learner driven by a multivariate \\textit{Bernoulli} process\nto guide instance partitioning. To address non-stationarity, it integrates\ndrift detection mechanisms at both global and local levels, enabling dynamic\nrestructuring of label partitions and subtrees. Experiments across 23\nreal-world datasets show iHOMER outperforms 5 state-of-the-art global\nbaselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\\%, and 12 local\nbaselines, such as binary relevance transformations of kNN, EFDT, ARF, and\nADWIN bagging/boosting ensembles, by 32\\%, establishing its robustness for\nonline multi-label classification.", "AI": {"tldr": "iHOMER\u662f\u4e00\u79cd\u5728\u7ebf\u591a\u6807\u7b7e\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u805a\u7c7b\u548c\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u6807\u7b7e\u6570\u636e\u6d41\u4e2d\u7684\u5206\u5e03\u6f14\u5316\u3001\u9ad8\u7ef4\u6807\u7b7e\u7a7a\u95f4\u548c\u6807\u7b7e\u4f9d\u8d56\u6027\u95ee\u9898\u9700\u8981\u52a8\u6001\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "iHOMER\u4f7f\u7528\u5728\u7ebf\u5206\u88c2-\u805a\u5408\u805a\u7c7b\u548c\u5168\u5c40\u6811\u5b66\u4e60\u5668\uff0c\u7ed3\u5408\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\u52a8\u6001\u8c03\u6574\u6807\u7b7e\u5206\u533a\u3002", "result": "\u572823\u4e2a\u6570\u636e\u96c6\u4e0a\uff0ciHOMER\u6bd45\u79cd\u5168\u5c40\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534723%\uff0c\u6bd412\u79cd\u5c40\u90e8\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534732%\u3002", "conclusion": "iHOMER\u5728\u5728\u7ebf\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u52a8\u6001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.20919", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.20919", "abs": "https://arxiv.org/abs/2507.20919", "authors": ["Aman Shukla", "Daniel Patrick Scantlebury", "Rishabh Kumar"], "title": "Modeling User Behavior from Adaptive Surveys with Supplemental Context", "comment": "Best Paper, NewInML @ ICML 2025", "summary": "Modeling user behavior is critical across many industries where understanding\npreferences, intent, or decisions informs personalization, targeting, and\nstrategic outcomes. Surveys have long served as a classical mechanism for\ncollecting such behavioral data due to their interpretability, structure, and\nease of deployment. However, surveys alone are inherently limited by user\nfatigue, incomplete responses, and practical constraints on their length making\nthem insufficient for capturing user behavior. In this work, we present LANTERN\n(Late-Attentive Network for Enriched Response Modeling), a modular architecture\nfor modeling user behavior by fusing adaptive survey responses with\nsupplemental contextual signals. We demonstrate the architectural value of\nmaintaining survey primacy through selective gating, residual connections and\nlate fusion via cross-attention, treating survey data as the primary signal\nwhile incorporating external modalities only when relevant. LANTERN outperforms\nstrong survey-only baselines in multi-label prediction of survey responses. We\nfurther investigate threshold sensitivity and the benefits of selective\nmodality reliance through ablation and rare/frequent attribute analysis.\nLANTERN's modularity supports scalable integration of new encoders and evolving\ndatasets. This work provides a practical and extensible blueprint for behavior\nmodeling in survey-centric applications.", "AI": {"tldr": "LANTERN\u662f\u4e00\u79cd\u6a21\u5757\u5316\u67b6\u6784\uff0c\u901a\u8fc7\u878d\u5408\u81ea\u9002\u5e94\u8c03\u67e5\u54cd\u5e94\u548c\u4e0a\u4e0b\u6587\u4fe1\u53f7\u6765\u5efa\u6a21\u7528\u6237\u884c\u4e3a\uff0c\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u8c03\u67e5\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8c03\u67e5\u65b9\u6cd5\u5b58\u5728\u7528\u6237\u75b2\u52b3\u3001\u4e0d\u5b8c\u6574\u54cd\u5e94\u548c\u957f\u5ea6\u9650\u5236\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u7528\u6237\u884c\u4e3a\uff0c\u9700\u8981\u7ed3\u5408\u5176\u4ed6\u4fe1\u53f7\u63d0\u5347\u5efa\u6a21\u6548\u679c\u3002", "method": "LANTERN\u91c7\u7528\u9009\u62e9\u6027\u95e8\u63a7\u3001\u6b8b\u5dee\u8fde\u63a5\u548c\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u8c03\u67e5\u6570\u636e\u4e3a\u4e3b\u4fe1\u53f7\uff0c\u4ec5\u5728\u76f8\u5173\u65f6\u878d\u5408\u5916\u90e8\u6a21\u6001\u3002", "result": "LANTERN\u5728\u591a\u6807\u7b7e\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u8c03\u67e5\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u548c\u5c5e\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u9009\u62e9\u6027\u6a21\u6001\u4f9d\u8d56\u7684\u4f18\u52bf\u3002", "conclusion": "LANTERN\u4e3a\u4ee5\u8c03\u67e5\u4e3a\u4e2d\u5fc3\u7684\u884c\u4e3a\u5efa\u6a21\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20925", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.20925", "abs": "https://arxiv.org/abs/2507.20925", "authors": ["Hongzhi Zhang", "Zhonglie Liu", "Kun Meng", "Jiameng Chen", "Jia Wu", "Bo Du", "Di Lin", "Yan Che", "Wenbin Hu"], "title": "Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein Interaction", "comment": null, "summary": "Given the vastness of chemical space and the ongoing emergence of previously\nuncharacterized proteins, zero-shot compound-protein interaction (CPI)\nprediction better reflects the practical challenges and requirements of\nreal-world drug development. Although existing methods perform adequately\nduring certain CPI tasks, they still face the following challenges: (1)\nRepresentation learning from local or complete protein sequences often\noverlooks the complex interdependencies between subsequences, which are\nessential for predicting spatial structures and binding properties. (2)\nDependence on large-scale or scarce multimodal protein datasets demands\nsignificant training data and computational resources, limiting scalability and\nefficiency. To address these challenges, we propose a novel approach that\npretrains protein representations for CPI prediction tasks using subsequence\nreordering, explicitly capturing the dependencies between protein subsequences.\nFurthermore, we apply length-variable protein augmentation to ensure excellent\npretraining performance on small training datasets. To evaluate the model's\neffectiveness and zero-shot learning ability, we combine it with various\nbaseline methods. The results demonstrate that our approach can improve the\nbaseline model's performance on the CPI task, especially in the challenging\nzero-shot scenario. Compared to existing pre-training models, our model\ndemonstrates superior performance, particularly in data-scarce scenarios where\ntraining samples are limited. Our implementation is available at\nhttps://github.com/Hoch-Zhang/PSRP-CPI.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u86cb\u767d\u8d28\u5b50\u5e8f\u5217\u91cd\u6392\u5e8f\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u5316\u5408\u7269-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\uff08CPI\uff09\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b50\u5e8f\u5217\u4f9d\u8d56\u6027\u548c\u6570\u636e\u7a00\u7f3a\u6027\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709CPI\u9884\u6d4b\u65b9\u6cd5\u5728\u5b50\u5e8f\u5217\u4f9d\u8d56\u6027\u548c\u6570\u636e\u9700\u6c42\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u5e94\u5bf9\u96f6\u6837\u672c\u573a\u666f\u3002", "method": "\u901a\u8fc7\u5b50\u5e8f\u5217\u91cd\u6392\u5e8f\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u957f\u5ea6\u53ef\u53d8\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aCPI\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u3002"}}
{"id": "2507.20929", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.20929", "abs": "https://arxiv.org/abs/2507.20929", "authors": ["Wei Shan Lee", "Chi Kiu Althina Chau", "Kei Chon Sio", "Kam Ian Leong"], "title": "Breaking the Precision Ceiling in Physics-Informed Neural Networks: A Hybrid Fourier-Neural Architecture for Ultra-High Accuracy", "comment": null, "summary": "Physics-informed neural networks (PINNs) have plateaued at errors of\n$10^{-3}$-$10^{-4}$ for fourth-order partial differential equations, creating a\nperceived precision ceiling that limits their adoption in engineering\napplications. We break through this barrier with a hybrid Fourier-neural\narchitecture for the Euler-Bernoulli beam equation, achieving unprecedented L2\nerror of $1.94 \\times 10^{-7}$-a 17-fold improvement over standard PINNs and\n\\(15-500\\times\\) better than traditional numerical methods. Our approach\nsynergistically combines a truncated Fourier series capturing dominant modal\nbehavior with a deep neural network providing adaptive residual corrections. A\nsystematic harmonic optimization study revealed a counter-intuitive discovery:\nexactly 10 harmonics yield optimal performance, with accuracy catastrophically\ndegrading from $10^{-7}$ to $10^{-1}$ beyond this threshold. The two-phase\noptimization strategy (Adam followed by L-BFGS) and adaptive weight balancing\nenable stable ultra-precision convergence. GPU-accelerated implementation\nachieves sub-30-minute training despite fourth-order derivative complexity. By\naddressing 12 critical gaps in existing approaches-from architectural rigidity\nto optimization landscapes-this work demonstrates that ultra-precision is\nachievable through proper design, opening new paradigms for scientific\ncomputing where machine learning can match or exceed traditional numerical\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5085\u91cc\u53f6-\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86PINNs\u5728\u56db\u9636\u504f\u5fae\u5206\u65b9\u7a0b\u4e2d\u7684\u7cbe\u5ea6\uff0cL2\u8bef\u5dee\u964d\u81f31.94\u00d710^-7\u3002", "motivation": "\u89e3\u51b3PINNs\u5728\u56db\u9636\u504f\u5fae\u5206\u65b9\u7a0b\u4e2d\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7a81\u783410^-3-10^-4\u7684\u8bef\u5dee\u74f6\u9888\u3002", "method": "\u7ed3\u5408\u622a\u65ad\u5085\u91cc\u53f6\u7ea7\u6570\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff08Adam\u548cL-BFGS\uff09\u548c\u81ea\u9002\u5e94\u6743\u91cd\u5e73\u8861\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002", "result": "L2\u8bef\u5dee\u8fbe\u52301.94\u00d710^-7\uff0c\u6bd4\u6807\u51c6PINNs\u63d0\u534717\u500d\uff0c\u6bd4\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u63d0\u534715-500\u500d\u3002", "conclusion": "\u901a\u8fc7\u5408\u7406\u8bbe\u8ba1\uff0c\u673a\u5668\u5b66\u4e60\u53ef\u4ee5\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5b9e\u73b0\u8d85\u9ad8\u7cbe\u5ea6\uff0c\u751a\u81f3\u8d85\u8d8a\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u3002"}}
{"id": "2507.20936", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.20936", "abs": "https://arxiv.org/abs/2507.20936", "authors": ["Ansh Poonia", "Maeghal Jain"], "title": "Dissecting Persona-Driven Reasoning in Language Models via Activation Patching", "comment": "11 pages", "summary": "Large language models (LLMs) exhibit remarkable versatility in adopting\ndiverse personas. In this study, we examine how assigning a persona influences\na model's reasoning on an objective task. Using activation patching, we take a\nfirst step toward understanding how key components of the model encode\npersona-specific information. Our findings reveal that the early Multi-Layer\nPerceptron (MLP) layers attend not only to the syntactic structure of the input\nbut also process its semantic content. These layers transform persona tokens\ninto richer representations, which are then used by the middle Multi-Head\nAttention (MHA) layers to shape the model's output. Additionally, we identify\nspecific attention heads that disproportionately attend to racial and\ncolor-based identities.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8d4b\u4e88\u4e0d\u540c\u89d2\u8272\u65f6\u5982\u4f55\u5f71\u54cd\u5176\u5ba2\u89c2\u4efb\u52a1\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u65e9\u671fMLP\u5c42\u5904\u7406\u8bed\u4e49\u5185\u5bb9\uff0c\u800c\u4e2d\u95f4MHA\u5c42\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u751f\u6210\u8f93\u51fa\u3002", "motivation": "\u63a2\u7d22\u89d2\u8272\u5206\u914d\u5bf9\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u7406\u89e3\u6a21\u578b\u5982\u4f55\u7f16\u7801\u89d2\u8272\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u6fc0\u6d3b\u4fee\u8865\u6280\u672f\u5206\u6790\u6a21\u578b\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u7814\u7a76MLP\u548cMHA\u5c42\u7684\u4f5c\u7528\u3002", "result": "\u65e9\u671fMLP\u5c42\u5904\u7406\u8bed\u4e49\u5185\u5bb9\uff0c\u4e2d\u95f4MHA\u5c42\u751f\u6210\u8f93\u51fa\uff1b\u67d0\u4e9b\u6ce8\u610f\u529b\u5934\u5bf9\u79cd\u65cf\u548c\u80a4\u8272\u8eab\u4efd\u5173\u6ce8\u5ea6\u9ad8\u3002", "conclusion": "\u89d2\u8272\u4fe1\u606f\u901a\u8fc7\u7279\u5b9a\u5c42\u5904\u7406\u5e76\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u5904\u7406\u89d2\u8272\u4fe1\u606f\u7684\u673a\u5236\u3002"}}
{"id": "2507.20954", "categories": ["cs.LG", "cs.CE", "math.DS", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2507.20954", "abs": "https://arxiv.org/abs/2507.20954", "authors": ["David Ye", "Jan Williams", "Mars Gao", "Stefano Riva", "Matteo Tomasetto", "David Zoro", "J. Nathan Kutz"], "title": "PySHRED: A Python package for SHallow REcurrent Decoding for sparse sensing, model reduction and scientific discovery", "comment": "15 pages, 9 figures", "summary": "SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for\nmodeling high-dimensional dynamical systems and/or spatiotemporal data from\ndynamical system snapshot observations. PySHRED is a Python package that\nimplements SHRED and several of its major extensions, including for robust\nsensing, reduced order modeling and physics discovery. In this paper, we\nintroduce the version 1.0 release of PySHRED, which includes data preprocessors\nand a number of cutting-edge SHRED methods specifically designed to handle\nreal-world data that may be noisy, multi-scale, parameterized, prohibitively\nhigh-dimensional, and strongly nonlinear. The package is easy to install,\nthoroughly-documented, supplemented with extensive code examples, and\nmodularly-structured to support future additions. The entire codebase is\nreleased under the MIT license and is available at\nhttps://github.com/pyshred-dev/pyshred.", "AI": {"tldr": "PySHRED\u662f\u4e00\u4e2aPython\u5305\uff0c\u5b9e\u73b0\u4e86SHallow REcurrent Decoders (SHRED)\u53ca\u5176\u6269\u5c55\uff0c\u7528\u4e8e\u5efa\u6a21\u9ad8\u7ef4\u52a8\u6001\u7cfb\u7edf\u548c\u65f6\u7a7a\u6570\u636e\uff0c\u652f\u6301\u566a\u58f0\u3001\u591a\u5c3a\u5ea6\u3001\u53c2\u6570\u5316\u7b49\u590d\u6742\u6570\u636e\u3002", "motivation": "\u4e3a\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u6570\u636e\uff08\u5982\u566a\u58f0\u3001\u9ad8\u7ef4\u3001\u975e\u7ebf\u6027\uff09\u63d0\u4f9b\u9ad8\u6548\u5de5\u5177\u3002", "method": "\u901a\u8fc7SHRED\u53ca\u5176\u6269\u5c55\u65b9\u6cd5\uff08\u5982\u9c81\u68d2\u611f\u77e5\u3001\u964d\u9636\u5efa\u6a21\u548c\u7269\u7406\u53d1\u73b0\uff09\u5efa\u6a21\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u6570\u636e\u9884\u5904\u7406\u548c\u6a21\u5757\u5316\u7ed3\u6784\u3002", "result": "\u53d1\u5e03\u4e86PySHRED 1.0\u7248\u672c\uff0c\u5305\u542b\u591a\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0c\u6613\u4e8e\u5b89\u88c5\u3001\u6587\u6863\u5b8c\u5584\uff0c\u5e76\u652f\u6301\u672a\u6765\u6269\u5c55\u3002", "conclusion": "PySHRED\u662f\u4e00\u4e2a\u529f\u80fd\u5f3a\u5927\u4e14\u7075\u6d3b\u7684Python\u5305\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u6570\u636e\u7684\u5efa\u6a21\u548c\u5206\u6790\u3002"}}
{"id": "2507.20967", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20967", "abs": "https://arxiv.org/abs/2507.20967", "authors": ["Tianhao Wang", "Simon Klancher", "Kunal Mukherjee", "Josh Wiedemeier", "Feng Chen", "Murat Kantarcioglu", "Kangkook Jee"], "title": "PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes", "comment": null, "summary": "The rise of graph-structured data has driven interest in graph learning and\nsynthetic data generation. While successful in text and image domains,\nsynthetic graph generation remains challenging -- especially for real-world\ngraphs with complex, heterogeneous schemas. Existing research has focused\nmostly on homogeneous structures with simple attributes, limiting their\nusefulness and relevance for application domains requiring semantic fidelity.\n  In this research, we introduce ProvCreator, a synthetic graph framework\ndesigned for complex heterogeneous graphs with high-dimensional node and edge\nattributes. ProvCreator formulates graph synthesis as a sequence generation\ntask, enabling the use of transformer-based large language models. It features\na versatile graph-to-sequence encoder-decoder that 1. losslessly encodes graph\nstructure and attributes, 2. efficiently compresses large graphs for contextual\nmodeling, and 3. supports end-to-end, learnable graph generation.\n  To validate our research, we evaluate ProvCreator on two challenging domains:\nsystem provenance graphs in cybersecurity and knowledge graphs from\nIntelliGraph Benchmark Dataset. In both cases, ProvCreator captures intricate\ndependencies between structure and semantics, enabling the generation of\nrealistic and privacy-aware synthetic datasets.", "AI": {"tldr": "ProvCreator\u662f\u4e00\u4e2a\u7528\u4e8e\u590d\u6742\u5f02\u6784\u56fe\u7684\u5408\u6210\u6846\u67b6\uff0c\u5229\u7528\u5e8f\u5217\u751f\u6210\u548cTransformer\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u3001\u903c\u771f\u7684\u56fe\u6570\u636e\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u5f02\u6784\u56fe\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u56fe\u5230\u5e8f\u5217\u7684\u7f16\u7801-\u89e3\u7801\u6846\u67b6\uff0c\u7ed3\u5408Transformer\u6a21\u578b\uff0c\u652f\u6301\u65e0\u635f\u7f16\u7801\u548c\u9ad8\u6548\u538b\u7f29\u3002", "result": "\u5728\u7f51\u7edc\u5b89\u5168\u548c\u77e5\u8bc6\u56fe\u8c31\u9886\u57df\u9a8c\u8bc1\u4e86ProvCreator\u7684\u751f\u6210\u80fd\u529b\uff0c\u80fd\u591f\u6355\u6349\u7ed3\u6784\u548c\u8bed\u4e49\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "ProvCreator\u4e3a\u590d\u6742\u5f02\u6784\u56fe\u7684\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.20968", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20968", "abs": "https://arxiv.org/abs/2507.20968", "authors": ["Rongyao Cai", "Ming Jin", "Qingsong Wen", "Kexin Zhang"], "title": "From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation", "comment": null, "summary": "Domain shift poses a fundamental challenge in time series analysis, where\nmodels trained on source domain often fail dramatically when applied in target\ndomain with different yet similar distributions. While current unsupervised\ndomain adaptation (UDA) methods attempt to align cross-domain feature\ndistributions, they typically treat features as indivisible entities, ignoring\ntheir intrinsic compositions that governs domain adaptation. We introduce\nDARSD, a novel UDA framework with theoretical explainability that explicitly\nrealizes UDA tasks from the perspective of representation space decomposition.\nOur core insight is that effective domain adaptation requires not just\nalignment, but principled disentanglement of transferable knowledge from mixed\nrepresentations. DARSD consists three synergistic components: (I) An\nadversarial learnable common invariant basis that projects original features\ninto a domain-invariant subspace while preserving semantic content; (II) A\nprototypical pseudo-labeling mechanism that dynamically separates target\nfeatures based on confidence, hindering error accumulation; (III) A hybrid\ncontrastive optimization strategy that simultaneously enforces feature\nclustering and consistency while mitigating emerging distribution gaps.\nComprehensive experiments conducted on four benchmark datasets (WISDM, HAR,\nHHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms,\nachieving optimal performance in 35 out of 53 cross-domain scenarios.", "AI": {"tldr": "DARSD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u7a7a\u95f4\u5206\u89e3\u5b9e\u73b0\u57df\u81ea\u9002\u5e94\uff0c\u4f18\u4e8e\u73b0\u670912\u79cdUDA\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u57df\u504f\u79fb\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u7279\u5f81\u5185\u5728\u7ec4\u6210\uff0c\u5bfc\u81f4\u8de8\u57df\u6027\u80fd\u4e0b\u964d\u3002", "method": "DARSD\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u5bf9\u6297\u5b66\u4e60\u7684\u516c\u5171\u4e0d\u53d8\u57fa\u3001\u539f\u578b\u4f2a\u6807\u7b7e\u673a\u5236\u548c\u6df7\u5408\u5bf9\u6bd4\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDARSD\u572853\u4e2a\u8de8\u57df\u573a\u666f\u4e2d\u768435\u4e2a\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "DARSD\u901a\u8fc7\u5206\u89e3\u8868\u793a\u7a7a\u95f4\u6709\u6548\u5b9e\u73b0\u57df\u81ea\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u8de8\u57df\u6027\u80fd\u3002"}}
{"id": "2507.20973", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20973", "abs": "https://arxiv.org/abs/2507.20973", "authors": ["Chao Wu", "Zhenyi Wang", "Kangxian Xie", "Naresh Kumar Devulapally", "Vishnu Suresh Lokhande", "Mingchen Gao"], "title": "Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder", "comment": null, "summary": "Text-to-image (T2I) diffusion models often exhibit gender bias, particularly\nby generating stereotypical associations between professions and gendered\nsubjects. This paper presents SAE Debias, a lightweight and model-agnostic\nframework for mitigating such bias in T2I generation. Unlike prior approaches\nthat rely on CLIP-based filtering or prompt engineering, which often require\nmodel-specific adjustments and offer limited control, SAE Debias operates\ndirectly within the feature space without retraining or architectural\nmodifications. By leveraging a k-sparse autoencoder pre-trained on a gender\nbias dataset, the method identifies gender-relevant directions within the\nsparse latent space, capturing professional stereotypes. Specifically, a biased\ndirection per profession is constructed from sparse latents and suppressed\nduring inference to steer generations toward more gender-balanced outputs.\nTrained only once, the sparse autoencoder provides a reusable debiasing\ndirection, offering effective control and interpretable insight into biased\nsubspaces. Extensive evaluations across multiple T2I models, including Stable\nDiffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially\nreduces gender bias while preserving generation quality. To the best of our\nknowledge, this is the first work to apply sparse autoencoders for identifying\nand intervening in gender bias within T2I models. These findings contribute\ntoward building socially responsible generative AI, providing an interpretable\nand model-agnostic tool to support fairness in text-to-image generation.", "AI": {"tldr": "SAE Debias\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u51cf\u5c11\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u548c\u6291\u5236\u6027\u522b\u76f8\u5173\u65b9\u5411\u3002", "motivation": "T2I\u6269\u6563\u6a21\u578b\u5e38\u8868\u73b0\u51fa\u6027\u522b\u504f\u89c1\uff0c\u5c24\u5176\u662f\u804c\u4e1a\u4e0e\u6027\u522b\u4e4b\u95f4\u7684\u523b\u677f\u5173\u8054\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56CLIP\u8fc7\u6ee4\u6216\u63d0\u793a\u5de5\u7a0b\uff0c\u6548\u679c\u6709\u9650\u4e14\u9700\u6a21\u578b\u8c03\u6574\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684k\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u7a00\u758f\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bc6\u522b\u6027\u522b\u76f8\u5173\u65b9\u5411\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u6291\u5236\u8fd9\u4e9b\u65b9\u5411\u4ee5\u751f\u6210\u6027\u522b\u5e73\u8861\u7684\u56fe\u50cf\u3002", "result": "SAE Debias\u5728\u591a\u4e2aT2I\u6a21\u578b\uff08\u5982Stable Diffusion\u7cfb\u5217\uff09\u4e2d\u663e\u8457\u51cf\u5c11\u6027\u522b\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "SAE Debias\u662f\u9996\u4e2a\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u548c\u5e72\u9884T2I\u6a21\u578b\u4e2d\u6027\u522b\u504f\u89c1\u7684\u5de5\u4f5c\uff0c\u4e3a\u6784\u5efa\u516c\u5e73\u7684\u751f\u6210AI\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u5de5\u5177\u3002"}}
{"id": "2507.20984", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20984", "abs": "https://arxiv.org/abs/2507.20984", "authors": ["Yixin Song", "Zhenliang Xue", "Dongliang Wei", "Feiyang Chen", "Jianxiang Gao", "Junchen Liu", "Hangyu Liang", "Guangshuo Qin", "Chengrong Tian", "Bo Wen", "Longyu Zhao", "Xinrui Zheng", "Zeyu Mi", "Haibo Chen"], "title": "SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment", "comment": null, "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.", "AI": {"tldr": "SmallThinker\u662f\u4e00\u7cfb\u5217\u4e13\u4e3a\u672c\u5730\u8bbe\u5907\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7a00\u758f\u7ed3\u6784\u548c\u9884\u53d6\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u5b9e\u73b0\u5728\u666e\u901aCPU\u4e0a\u7684\u9ad8\u6548\u8fd0\u884c\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56GPU\u4e91\u57fa\u7840\u8bbe\u65bd\u7684\u8303\u5f0f\uff0c\u4e3a\u672c\u5730\u8bbe\u5907\u7684\u8ba1\u7b97\u80fd\u529b\u3001\u5185\u5b58\u548c\u5b58\u50a8\u9650\u5236\u63d0\u4f9b\u539f\u751f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u7ea7\u7a00\u758f\u7ed3\u6784\uff08MoE\u4e0e\u7a00\u758f\u524d\u9988\u7f51\u7edc\uff09\u3001\u9884\u6ce8\u610f\u529b\u8def\u7531\u5668\u548cNoPE-RoPE\u6df7\u5408\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f18\u5316\u8ba1\u7b97\u3001\u5b58\u50a8\u548c\u5185\u5b58\u6548\u7387\u3002", "result": "SmallThinker-4B-A0.6B\u548cSmallThinker-21B-A3B\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u5e76\u5728\u666e\u901aCPU\u4e0a\u5b9e\u73b0\u6bcf\u79d220\u4e2a\u4ee4\u724c\u7684\u901f\u5ea6\uff0c\u5185\u5b58\u5360\u7528\u6781\u4f4e\u3002", "conclusion": "SmallThinker\u8bc1\u660e\u4e86\u672c\u5730\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u51cf\u5c11\u4e86\u5bf9\u6602\u8d35GPU\u786c\u4ef6\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.20997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20997", "abs": "https://arxiv.org/abs/2507.20997", "authors": ["Haris Khan", "Shumaila Asif", "Sadia Asif"], "title": "Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition", "comment": "11 pages, 6 figures, 3 tables. Will be Submitted to ICLR 2025 for\n  review", "summary": "In real-world machine learning deployments, models must be continually\nupdated, composed, and when required, selectively undone. However, existing\napproaches to model merging and continual learning often suffer from task\ninterference, catastrophic forgetting, or lack of reversibility. We propose\nModular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework\nthat enables scalable, interference-free, and reversible composition of\nfine-tuned models. Each task-specific model is encoded as a delta from a shared\nbase and projected into an orthogonal subspace to eliminate conflict. These\nprojected deltas are then merged via gradient-based optimization to form a\nunified model that retains performance across tasks. Our approach supports\ncontinual integration of new models, structured unmerging for compliance such\nas GDPR requirements, and model stability via elastic weight consolidation and\nsynthetic replay. Extensive experiments on vision and natural language\nprocessing benchmarks demonstrate that MDM-OC outperforms prior baselines in\naccuracy, backward transfer, and unmerge fidelity, while remaining\nmemory-efficient and computationally tractable. This framework offers a\nprincipled solution for modular and compliant AI system design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMDM-OC\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u65e0\u5e72\u6270\u4e14\u53ef\u9006\u7684\u6a21\u578b\u5408\u5e76\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u4efb\u52a1\u5e72\u6270\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u5b66\u4e60\u90e8\u7f72\u4e2d\uff0c\u6a21\u578b\u9700\u8981\u4e0d\u65ad\u66f4\u65b0\u3001\u7ec4\u5408\u548c\u9009\u62e9\u6027\u64a4\u9500\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4efb\u52a1\u5e72\u6270\u3001\u707e\u96be\u6027\u9057\u5fd8\u6216\u7f3a\u4e4f\u53ef\u9006\u6027\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7f16\u7801\u4e3a\u5171\u4eab\u57fa\u7840\u7684\u589e\u91cf\uff0c\u5e76\u5c06\u5176\u6295\u5f71\u5230\u6b63\u4ea4\u5b50\u7a7a\u95f4\u4ee5\u907f\u514d\u51b2\u7a81\uff0c\u518d\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u5408\u5e76\u4e3a\u7edf\u4e00\u6a21\u578b\u3002", "result": "\u5728\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMDM-OC\u5728\u51c6\u786e\u6027\u3001\u5411\u540e\u8f6c\u79fb\u548c\u64a4\u9500\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u9ad8\u6548\u548c\u8ba1\u7b97\u53ef\u884c\u3002", "conclusion": "MDM-OC\u4e3a\u6a21\u5757\u5316\u548c\u5408\u89c4\u7684AI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20999", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.20999", "abs": "https://arxiv.org/abs/2507.20999", "authors": ["Yining Huang", "Bin Li", "Keke Tang", "Meilian Chen"], "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning", "comment": "10 pages", "summary": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoRA-PAR\u7684\u53cc\u7cfb\u7edfLoRA\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u533a\u6570\u636e\u548c\u53c2\u6570\u4ee5\u9002\u5e94\u5feb\u901f\u76f4\u89c9\u4efb\u52a1\uff08System 1\uff09\u548c\u591a\u6b65\u903b\u8f91\u63a8\u7406\u4efb\u52a1\uff08System 2\uff09\uff0c\u51cf\u5c11\u53c2\u6570\u4f7f\u7528\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9886\u57df\u9002\u5e94\u6216\u5206\u5c42\u5206\u914d\uff0c\u672a\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u5b9a\u5236\u6570\u636e\u548c\u53c2\u6570\u3002\u53d7\u300a\u601d\u8003\uff0c\u5feb\u4e0e\u6162\u300b\u542f\u53d1\uff0c\u63d0\u51faLLM\u53c2\u6570\u53ef\u80fd\u7c7b\u4f3c\u5730\u5206\u4e3a\u5feb\u901f\u76f4\u89c9\u548c\u6df1\u5ea6\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u578b\u89d2\u8272\u626e\u6f14\u548c\u6295\u7968\u5206\u7c7b\u4efb\u52a1\u6570\u636e\uff0c\u57fa\u4e8e\u91cd\u8981\u6027\u8bc4\u5206\u5206\u533a\u53c2\u6570\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff1aSFT\u8bad\u7ec3System 1\u4efb\u52a1\u589e\u5f3a\u76f4\u89c9\uff0cRL\u4f18\u5316System 2\u4efb\u52a1\u5f3a\u5316\u903b\u8f91\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff08SFT\u548cRL\uff09\u5728\u51cf\u5c11\u53c2\u6570\u4f7f\u7528\u7684\u540c\u65f6\uff0c\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u73b0\u6709PEFT\u57fa\u7ebf\u3002", "conclusion": "LoRA-PAR\u6846\u67b6\u901a\u8fc7\u5206\u533a\u6570\u636e\u548c\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u540c\u65f6\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u4e3aLLM\u7684\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.21004", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21004", "abs": "https://arxiv.org/abs/2507.21004", "authors": ["Fang Li"], "title": "Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability", "comment": null, "summary": "Deep Neural Networks (DNNs) deliver impressive performance but their\nblack-box nature limits deployment in high-stakes domains requiring\ntransparency. We introduce Compositional Function Networks (CFNs), a novel\nframework that builds inherently interpretable models by composing elementary\nmathematical functions with clear semantics. Unlike existing interpretable\napproaches that are limited to simple additive structures, CFNs support diverse\ncompositional patterns -- sequential, parallel, and conditional -- enabling\ncomplex feature interactions while maintaining transparency. A key innovation\nis that CFNs are fully differentiable, allowing efficient training through\nstandard gradient descent. We demonstrate CFNs' versatility across multiple\ndomains, from symbolic regression to image classification with deep\nhierarchical networks. Our empirical evaluation shows CFNs achieve competitive\nperformance against black-box models (96.24% accuracy on CIFAR-10) while\noutperforming state-of-the-art interpretable models like Explainable Boosting\nMachines. By combining the hierarchical expressiveness and efficient training\nof deep learning with the intrinsic interpretability of well-defined\nmathematical functions, CFNs offer a powerful framework for applications where\nboth performance and accountability are paramount.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6Compositional Function Networks\uff08CFNs\uff09\uff0c\u901a\u8fc7\u7ec4\u5408\u5177\u6709\u660e\u786e\u8bed\u4e49\u7684\u6570\u5b66\u51fd\u6570\u6784\u5efa\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u652f\u6301\u590d\u6742\u7279\u5f81\u4ea4\u4e92\u4e14\u4fdd\u6301\u900f\u660e\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u6027\u80fd\u4f18\u5f02\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u9650\u5236\u4e86\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u3002CFNs\u65e8\u5728\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "CFNs\u901a\u8fc7\u7ec4\u5408\u57fa\u672c\u6570\u5b66\u51fd\u6570\uff08\u652f\u6301\u987a\u5e8f\u3001\u5e76\u884c\u548c\u6761\u4ef6\u6a21\u5f0f\uff09\u6784\u5efa\u6a21\u578b\uff0c\u5b8c\u5168\u53ef\u5fae\u5206\uff0c\u652f\u6301\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u3002", "result": "CFNs\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u7b26\u53f7\u56de\u5f52\u548c\u56fe\u50cf\u5206\u7c7b\uff09\u8868\u73b0\u4f18\u5f02\uff0cCIFAR-10\u4e0a\u51c6\u786e\u7387\u8fbe96.24%\uff0c\u4f18\u4e8e\u73b0\u6709\u53ef\u89e3\u91ca\u6a21\u578b\u3002", "conclusion": "CFNs\u4e3a\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u5e76\u91cd\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u6846\u67b6\u3002"}}
{"id": "2507.21016", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.21016", "abs": "https://arxiv.org/abs/2507.21016", "authors": ["Jagruti Patel", "Mikkel Sch\u00f6ttner", "Thomas A. W. Bolton", "Patric Hagmann"], "title": "Predicting Cognition from fMRI:A Comparative Study of Graph, Transformer, and Kernel Models Across Task and Rest Conditions", "comment": "Preliminary version; a revised version will be uploaded later", "summary": "Predicting cognition from neuroimaging data in healthy individuals offers\ninsights into the neural mechanisms underlying cognitive abilities, with\npotential applications in precision medicine and early detection of\nneurological and psychiatric conditions. This study systematically benchmarked\nclassical machine learning (Kernel Ridge Regression (KRR)) and advanced deep\nlearning (DL) models (Graph Neural Networks (GNN) and Transformer-GNN (TGNN))\nfor cognitive prediction using Resting-state (RS), Working Memory, and Language\ntask fMRI data from the Human Connectome Project Young Adult dataset.\n  Our results, based on R2 scores, Pearson correlation coefficient, and mean\nabsolute error, revealed that task-based fMRI, eliciting neural responses\ndirectly tied to cognition, outperformed RS fMRI in predicting cognitive\nbehavior. Among the methods compared, a GNN combining structural connectivity\n(SC) and functional connectivity (FC) consistently achieved the highest\nperformance across all fMRI modalities; however, its advantage over KRR using\nFC alone was not statistically significant. The TGNN, designed to model\ntemporal dynamics with SC as a prior, performed competitively with FC-based\napproaches for task-fMRI but struggled with RS data, where its performance\naligned with the lower-performing GNN that directly used fMRI time-series data\nas node features. These findings emphasize the importance of selecting\nappropriate model architectures and feature representations to fully leverage\nthe spatial and temporal richness of neuroimaging data.\n  This study highlights the potential of multimodal graph-aware DL models to\ncombine SC and FC for cognitive prediction, as well as the promise of\nTransformer-based approaches for capturing temporal dynamics. By providing a\ncomprehensive comparison of models, this work serves as a guide for advancing\nbrain-behavior modeling using fMRI, SC and DL.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u7ecf\u5178\u673a\u5668\u5b66\u4e60\uff08KRR\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08GNN\u3001TGNN\uff09\u5728\u9884\u6d4b\u8ba4\u77e5\u80fd\u529b\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4efb\u52a1\u578bfMRI\u4f18\u4e8e\u9759\u606f\u6001fMRI\uff0cGNN\u7ed3\u5408SC\u548cFC\u8868\u73b0\u6700\u4f73\uff0c\u4f46TGNN\u5728\u4efb\u52a1\u578bfMRI\u4e2d\u8868\u73b0\u63a5\u8fd1FC\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u9884\u6d4b\u8ba4\u77e5\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u8ba4\u77e5\u7684\u795e\u7ecf\u673a\u5236\uff0c\u5e76\u5e94\u7528\u4e8e\u7cbe\u51c6\u533b\u7597\u548c\u65e9\u671f\u795e\u7ecf\u7cbe\u795e\u75be\u75c5\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528HCP\u5e74\u8f7b\u6210\u4eba\u6570\u636e\u96c6\uff0c\u6bd4\u8f83KRR\u3001GNN\u548cTGNN\u5728\u9759\u606f\u6001\u3001\u5de5\u4f5c\u8bb0\u5fc6\u548c\u8bed\u8a00\u4efb\u52a1fMRI\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4efb\u52a1\u578bfMRI\u4f18\u4e8e\u9759\u606f\u6001fMRI\uff1bGNN\u7ed3\u5408SC\u548cFC\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4f18\u52bf\u4e0d\u663e\u8457\uff1bTGNN\u5728\u4efb\u52a1\u578bfMRI\u4e2d\u8868\u73b0\u63a5\u8fd1FC\u65b9\u6cd5\uff0c\u4f46\u5728\u9759\u606f\u6001\u6570\u636e\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u67b6\u6784\u548c\u7279\u5f81\u8868\u793a\u5bf9\u5145\u5206\u5229\u7528\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u7684\u65f6\u7a7a\u7279\u6027\u81f3\u5173\u91cd\u8981\uff0c\u591a\u6a21\u6001\u56fe\u611f\u77e5DL\u6a21\u578b\u548cTransformer\u65b9\u6cd5\u5728\u8ba4\u77e5\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.21021", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21021", "abs": "https://arxiv.org/abs/2507.21021", "authors": ["Zhen Zhang", "Dong Sam Ha", "Gota Morota", "Sook Shin"], "title": "Behavior-Specific Filtering for Enhanced Pig Behavior Classification in Precision Livestock Farming", "comment": "11 pages, 4 tables, 3 figures", "summary": "This study proposes a behavior-specific filtering method to improve behavior\nclassification accuracy in Precision Livestock Farming. While traditional\nfiltering methods, such as wavelet denoising, achieved an accuracy of 91.58%,\nthey apply uniform processing to all behaviors. In contrast, the proposed\nbehavior-specific filtering method combines Wavelet Denoising with a Low Pass\nFilter, tailored to active and inactive pig behaviors, and achieved a peak\naccuracy of 94.73%. These results highlight the effectiveness of\nbehavior-specific filtering in enhancing animal behavior monitoring, supporting\nbetter health management and farm efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7279\u5b9a\u884c\u4e3a\u7684\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u51c6\u755c\u7267\u4e1a\u4e2d\u884c\u4e3a\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u8fc7\u6ee4\u65b9\u6cd5\u5bf9\u6240\u6709\u884c\u4e3a\u91c7\u7528\u7edf\u4e00\u5904\u7406\uff0c\u9650\u5236\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u5c0f\u6ce2\u53bb\u566a\u548c\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u9488\u5bf9\u6d3b\u8dc3\u548c\u975e\u6d3b\u8dc3\u732a\u884c\u4e3a\u8fdb\u884c\u5b9a\u5236\u5316\u5904\u7406\u3002", "result": "\u5cf0\u503c\u51c6\u786e\u7387\u8fbe\u523094.73%\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u768491.58%\u3002", "conclusion": "\u884c\u4e3a\u7279\u5b9a\u8fc7\u6ee4\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u52a8\u7269\u884c\u4e3a\u76d1\u6d4b\uff0c\u6709\u52a9\u4e8e\u5065\u5eb7\u7ba1\u7406\u548c\u519c\u573a\u6548\u7387\u3002"}}
{"id": "2507.21024", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21024", "abs": "https://arxiv.org/abs/2507.21024", "authors": ["Mayumi Nakano", "Yuya Seki", "Shuta Kikuchi", "Shu Tanaka"], "title": "Optimization Performance of Factorization Machine with Annealing under Limited Training Data", "comment": "9 pages, 4 figures", "summary": "Black-box (BB) optimization problems aim to identify an input that minimizes\nthe output of a function (the BB function) whose input-output relationship is\nunknown. Factorization machine with annealing (FMA) is a promising approach to\nthis task, employing a factorization machine (FM) as a surrogate model to\niteratively guide the solution search via an Ising machine. Although FMA has\ndemonstrated strong optimization performance across various applications, its\nperformance often stagnates as the number of optimization iterations increases.\nOne contributing factor to this stagnation is the growing number of data points\nin the dataset used to train FM. It is hypothesized that as more data points\nare accumulated, the contribution of newly added data points becomes diluted\nwithin the entire dataset, thereby reducing their impact on improving the\nprediction accuracy of FM. To address this issue, we propose a novel method for\nsequential dataset construction that retains at most a specified number of the\nmost recently added data points. This strategy is designed to enhance the\ninfluence of newly added data points on the surrogate model. Numerical\nexperiments demonstrate that the proposed FMA achieves lower-cost solutions\nwith fewer BB function evaluations compared to the conventional FMA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684FMA\u65b9\u6cd5\uff0c\u901a\u8fc7\u9650\u5236\u6570\u636e\u96c6\u5927\u5c0f\u6765\u589e\u5f3a\u65b0\u6570\u636e\u70b9\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u63d0\u5347\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfFMA\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u6027\u80fd\u505c\u6ede\uff0c\u539f\u56e0\u662f\u6570\u636e\u96c6\u589e\u5927\u5bfc\u81f4\u65b0\u6570\u636e\u70b9\u7684\u5f71\u54cd\u88ab\u7a00\u91ca\u3002", "method": "\u91c7\u7528\u5e8f\u5217\u5316\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u4ec5\u4fdd\u7559\u6700\u8fd1\u6dfb\u52a0\u7684\u6307\u5b9a\u6570\u91cf\u6570\u636e\u70b9\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u7684FMA\u80fd\u4ee5\u66f4\u5c11\u7684\u51fd\u6570\u8bc4\u4f30\u83b7\u5f97\u66f4\u4f4e\u6210\u672c\u7684\u89e3\u3002", "conclusion": "\u9650\u5236\u6570\u636e\u96c6\u5927\u5c0f\u80fd\u6709\u6548\u63d0\u5347FMA\u7684\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2507.21037", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21037", "abs": "https://arxiv.org/abs/2507.21037", "authors": ["Jinzhou Wu", "Baoping Tang", "Qikang Li", "Yi Wang", "Cheng Li", "Shujian Yu"], "title": "When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New Framework for Cross-Subject Motor Imagery Decoding", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Decoding motor imagery (MI) electroencephalogram (EEG) signals, a key\nnon-invasive brain-computer interface (BCI) paradigm for controlling external\nsystems, has been significantly advanced by deep learning. However, MI-EEG\ndecoding remains challenging due to substantial inter-subject variability and\nlimited labeled target data, which necessitate costly calibration for new\nusers. Many existing multi-source domain adaptation (MSDA) methods\nindiscriminately incorporate all available source domains, disregarding the\nlarge inter-subject differences in EEG signals, which leads to negative\ntransfer and excessive computational costs. Moreover, while many approaches\nfocus on feature distribution alignment, they often neglect the explicit\ndependence between features and decision-level outputs, limiting their ability\nto preserve discriminative structures. To address these gaps, we propose a\nnovel MSDA framework that leverages a pretrained large Brain Foundation Model\n(BFM) for dynamic and informed source subject selection, ensuring only relevant\nsources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS)\nand Conditional CS (CCS) divergences to jointly perform feature-level and\ndecision-level alignment, enhancing domain invariance while maintaining class\ndiscriminability. Extensive evaluations on two benchmark MI-EEG datasets\ndemonstrate that our framework outperforms a broad range of state-of-the-art\nbaselines. Additional experiments with a large source pool validate the\nscalability and efficiency of BFM-guided selection, which significantly reduces\ntraining time without sacrificing performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u5927\u8111\u57fa\u7840\u6a21\u578b\uff08BFM\uff09\u7684\u591a\u6e90\u57df\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u9009\u62e9\u76f8\u5173\u6e90\u57df\uff0c\u5e76\u7ed3\u5408\u7279\u5f81\u7ea7\u548c\u51b3\u7b56\u7ea7\u5bf9\u9f50\u4ee5\u63d0\u5347MI-EEG\u89e3\u7801\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3MI-EEG\u89e3\u7801\u4e2d\u56e0\u88ab\u8bd5\u95f4\u5dee\u5f02\u5927\u548c\u6807\u8bb0\u6570\u636e\u6709\u9650\u5bfc\u81f4\u7684\u8d1f\u8fc1\u79fb\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528BFM\u52a8\u6001\u9009\u62e9\u76f8\u5173\u6e90\u57df\uff0c\u4f7f\u7528Cauchy-Schwarz\u548cConditional CS\u6563\u5ea6\u8fdb\u884c\u7279\u5f81\u7ea7\u548c\u51b3\u7b56\u7ea7\u5bf9\u9f50\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6MI-EEG\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cBFM\u9009\u62e9\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u4e14\u4e0d\u727a\u7272\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MI-EEG\u89e3\u7801\u4e2d\u7684\u6311\u6218\uff0c\u5177\u6709\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.21049", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21049", "abs": "https://arxiv.org/abs/2507.21049", "authors": ["Zedong Wang", "Siyuan Li", "Dan Xu"], "title": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning", "comment": "ICCV 2025 (Highlight). Project page:\n  https://jacky1128.github.io/RepMTL/", "summary": "Despite the promise of Multi-Task Learning in leveraging complementary\nknowledge across tasks, existing multi-task optimization (MTO) techniques\nremain fixated on resolving conflicts via optimizer-centric loss scaling and\ngradient manipulation strategies, yet fail to deliver consistent gains. In this\npaper, we argue that the shared representation space, where task interactions\nnaturally occur, offers rich information and potential for operations\ncomplementary to existing optimizers, especially for facilitating the\ninter-task complementarity, which is rarely explored in MTO. This intuition\nleads to Rep-MTL, which exploits the representation-level task saliency to\nquantify interactions between task-specific optimization and shared\nrepresentation learning. By steering these saliencies through entropy-based\npenalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate\nnegative transfer by maintaining the effective training of individual tasks\ninstead pure conflict-solving, while explicitly promoting complementary\ninformation sharing. Experiments are conducted on four challenging MTL\nbenchmarks covering both task-shift and domain-shift scenarios. The results\nshow that Rep-MTL, even paired with the basic equal weighting policy, achieves\ncompetitive performance gains with favorable efficiency. Beyond standard\nperformance metrics, Power Law exponent analysis demonstrates Rep-MTL's\nefficacy in balancing task-specific learning and cross-task sharing. The\nproject page is available at HERE.", "AI": {"tldr": "Rep-MTL\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u4efb\u52a1\u663e\u8457\u6027\u6765\u91cf\u5316\u4efb\u52a1\u95f4\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e92\u8865\u4fe1\u606f\u5171\u4eab\uff0c\u800c\u975e\u4ec5\u89e3\u51b3\u51b2\u7a81\u3002", "motivation": "\u73b0\u6709\u591a\u4efb\u52a1\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u4f18\u5316\u5668\u4e3a\u4e2d\u5fc3\u7684\u635f\u5931\u7f29\u653e\u548c\u68af\u5ea6\u64cd\u4f5c\u89e3\u51b3\u4efb\u52a1\u51b2\u7a81\uff0c\u4f46\u672a\u80fd\u5b9e\u73b0\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002\u672c\u6587\u8ba4\u4e3a\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u4ea4\u4e92\u4fe1\u606f\uff0c\u53ef\u7528\u4e8e\u4fc3\u8fdb\u4efb\u52a1\u95f4\u7684\u4e92\u8865\u6027\u3002", "method": "Rep-MTL\u5229\u7528\u8868\u793a\u7ea7\u4efb\u52a1\u663e\u8457\u6027\u91cf\u5316\u4efb\u52a1\u95f4\u4ea4\u4e92\uff0c\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u60e9\u7f5a\u548c\u6837\u672c\u7ea7\u8de8\u4efb\u52a1\u5bf9\u9f50\u6765\u5f15\u5bfc\u8fd9\u4e9b\u663e\u8457\u6027\uff0c\u4ee5\u51cf\u8f7b\u8d1f\u8fc1\u79fb\u5e76\u4fc3\u8fdb\u4e92\u8865\u4fe1\u606f\u5171\u4eab\u3002", "result": "\u5728\u56db\u4e2a\u591a\u4efb\u52a1\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRep-MTL\u5373\u4f7f\u4e0e\u57fa\u672c\u7b49\u6743\u91cd\u7b56\u7565\u7ed3\u5408\uff0c\u4e5f\u80fd\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u5e73\u8861\u4efb\u52a1\u7279\u5b9a\u5b66\u4e60\u548c\u8de8\u4efb\u52a1\u5171\u4eab\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Rep-MTL\u901a\u8fc7\u8868\u793a\u7ea7\u4efb\u52a1\u663e\u8457\u6027\u64cd\u4f5c\uff0c\u6709\u6548\u4fc3\u8fdb\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u4e92\u8865\u4fe1\u606f\u5171\u4eab\uff0c\u4e3a\u591a\u4efb\u52a1\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.21053", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21053", "abs": "https://arxiv.org/abs/2507.21053", "authors": ["David McAllister", "Songwei Ge", "Brent Yi", "Chung Min Kim", "Ethan Weber", "Hongsuk Choi", "Haiwen Feng", "Angjoo Kanazawa"], "title": "Flow Matching Policy Gradients", "comment": "See our blog post: https://flowreinforce.github.io", "summary": "Flow-based generative models, including diffusion models, excel at modeling\ncontinuous distributions in high-dimensional spaces. In this work, we introduce\nFlow Policy Optimization (FPO), a simple on-policy reinforcement learning\nalgorithm that brings flow matching into the policy gradient framework. FPO\ncasts policy optimization as maximizing an advantage-weighted ratio computed\nfrom the conditional flow matching loss, in a manner compatible with the\npopular PPO-clip framework. It sidesteps the need for exact likelihood\ncomputation while preserving the generative capabilities of flow-based models.\nUnlike prior approaches for diffusion-based reinforcement learning that bind\ntraining to a specific sampling method, FPO is agnostic to the choice of\ndiffusion or flow integration at both training and inference time. We show that\nFPO can train diffusion-style policies from scratch in a variety of continuous\ncontrol tasks. We find that flow-based models can capture multimodal action\ndistributions and achieve higher performance than Gaussian policies,\nparticularly in under-conditioned settings.", "AI": {"tldr": "FPO\u662f\u4e00\u79cd\u5c06\u6d41\u5339\u914d\u5f15\u5165\u7b56\u7565\u68af\u5ea6\u6846\u67b6\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u517c\u5bb9PPO-clip\u6846\u67b6\uff0c\u65e0\u9700\u7cbe\u786e\u4f3c\u7136\u8ba1\u7b97\uff0c\u9002\u7528\u4e8e\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u3002", "motivation": "\u5c06\u6d41\u5339\u914d\u7684\u4f18\u52bf\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u5316\uff0c\u89e3\u51b3\u9ad8\u65af\u7b56\u7565\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u4f18\u52bf\u52a0\u6743\u6bd4\u8ba1\u7b97\u6761\u4ef6\u6d41\u5339\u914d\u635f\u5931\uff0c\u7ed3\u5408PPO-clip\u6846\u67b6\uff0c\u5b9e\u73b0\u7b56\u7565\u4f18\u5316\u3002", "result": "FPO\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u9ad8\u65af\u7b56\u7565\uff0c\u5c24\u5176\u5728\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u548c\u6b20\u6761\u4ef6\u8bbe\u7f6e\u4e0b\u3002", "conclusion": "FPO\u5c55\u793a\u4e86\u6d41\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
