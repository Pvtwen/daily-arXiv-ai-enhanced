<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 24]
- [cs.LG](#cs.LG) [Total: 161]
- [stat.ML](#stat.ML) [Total: 15]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Domain Adaptation-Enabled Realistic Map-Based Channel Estimation for MIMO-OFDM](https://arxiv.org/abs/2507.08974)
*Thien Hieu Hoang,Tri Nhu Do,Georges Kaddoum*

Main category: eess.SP

TL;DR: 提出了一种新的域适应方法，用于解决无线通信中动态环境下的信道估计问题，通过模拟训练和域适应技术提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统模型在动态环境中表现不佳，而机器学习方法因信道特性差异缺乏泛化能力，需解决这一问题。

Method: 提出基于模拟训练的信道估计流程和域适应方法，减少实际数据需求。

Result: 模型在有限真实信道信息下仍表现稳健。

Conclusion: 该方法为动态环境下的信道估计提供了有效解决方案。

Abstract: Accurate channel estimation is crucial for the improvement of signal
processing performance in wireless communications. However, traditional
model-based methods frequently experience difficulties in dynamic environments.
Similarly, alternative machine-learning approaches typically lack
generalization across different datasets due to variations in channel
characteristics. To address this issue, in this study, we propose a novel
domain adaptation approach to bridge the gap between the quasi-static channel
model (QSCM) and the map-based channel model (MBCM). Specifically, we first
proposed a channel estimation pipeline that takes into account realistic
channel simulation to train our foundation model. Then, we proposed domain
adaptation methods to address the estimation problem. Using simulation-based
training to reduce data requirements for effective application in practical
wireless environments, we find that the proposed strategy enables robust model
performance, even with limited true channel information.

</details>


### [2] [LNN-powered Fluid Antenna Multiple Access](https://arxiv.org/abs/2507.08821)
*Pedro D. Alvim,Hugerles S. Silva,Ugo S. Dias,Osamah S. Badarneh,Felipe A. P. Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 论文提出了一种基于液态神经网络（LNNs）的流体天线系统多标签分类方法，用于优化端口选择，降低中断概率。


<details>
  <summary>Details</summary>
Motivation: 解决流体天线多接入场景下端口选择问题，提升信号干扰噪声比。

Method: 将端口选择问题建模为多标签分类任务，利用LNNs预测最优端口，并结合α-μ衰落模型和超参数优化。

Result: 相比现有方法，提出的方法具有更低的中断概率。

Conclusion: LNNs在流体天线系统中能有效优化端口选择，提升性能。

Abstract: Fluid antenna systems represent an innovative approach in wireless
communication, recently applied in multiple access to optimize the
signal-to-interference-plus-noise ratio through port selection. This letter
frames the port selection problem as a multi-label classification task for the
first time, improving best-port selection with limited port observations. We
address this challenge by leveraging liquid neural networks (LNNs) to predict
the optimal port under emerging fluid antenna multiple access scenarios
alongside a more general $\alpha$-$\mu$ fading model. We also apply
hyperparameter optimization to refine LNN architectures for different
observation scenarios. Our approach yields lower outage probability values than
existing methods.

</details>


### [3] [Fundamental limits via CRB of semi-blind channel estimation in Massive MIMO systems](https://arxiv.org/abs/2507.08950)
*Xue Zhang,Abla Kammoun,Mohamed-Slim Alouini*

Main category: eess.SP

TL;DR: 研究了大规模MIMO系统中半盲信道估计的确定性及随机Cramér-Rao界（CRB）的渐近行为，推导了不同渐近条件下的数学表达式，并发现CRB可随传输块长度增加而无限减小，但需训练序列长度同步增长且用户数固定。


<details>
  <summary>Details</summary>
Motivation: 探讨半盲信道估计在大规模MIMO系统中的性能极限，填补现有研究中对CRB渐近行为分析的不足。

Method: 推导并分析了确定性及随机CRB在不同渐近条件下的数学表达式，通过数值结果验证理论发现。

Result: CRB可随传输块长度增加而无限减小，但需训练序列长度同步增长且用户数固定；若训练序列数与用户数成比例，则信道估计误差有非零下限。

Conclusion: 半盲信道估计能有效减少所需训练序列数，但需合理设计训练序列长度与用户数的关系。

Abstract: This paper investigates the asymptotic behavior of the deterministic and
stochastic Cram\'er-Rao Bounds (CRB) for semi-blind channel estimation in
massive multiple-input multiple-output (MIMO) systems. We derive and analyze
mathematically tractable expressions for both metrics under various asymptotic
regimes, which govern the growth rates of the number of antennas, the number of
users, the training sequence length, and the transmission block length. Unlike
the existing work, our results show that the CRB can be made arbitrarily small
as the transmission block length increases, but only when the training sequence
length grows at the same rate and the number of users remains fixed. However,
if the number of training sequences remains proportional to the number of
users, the channel estimation error is always lower-bounded by a non-vanishing
constant. Numerical results are presented to support our findings and
demonstrate the advantages of semi-blind channel estimation in reducing the
required number of training sequences.

</details>


### [4] [Hypergraph Overlapping Community Detection for Brain Networks](https://arxiv.org/abs/2507.08999)
*Duc Vu,Selin Aviyente*

Main category: eess.SP

TL;DR: 该论文提出了一种基于超图的社区检测方法，用于分析fMRI数据中的高阶脑区关系。


<details>
  <summary>Details</summary>
Motivation: 传统功能连接网络（FCNs）仅量化脑区间的成对关系，忽略了高阶依赖关系，因此需要新方法来表征多脑区的高阶关系。

Method: 首先为每个被试构建超图以捕捉高阶依赖关系，然后引入基于谱聚类的超图方法检测重叠社区结构，并在多被试中检测共识社区结构。

Result: 方法应用于人类连接组计划的静息态fMRI数据，总结了健康年轻人的重叠社区结构。

Conclusion: 该方法成功表征了脑超网络中的高阶关系，为社区检测提供了新工具。

Abstract: Functional magnetic resonance imaging (fMRI) has been commonly used to
construct functional connectivity networks (FCNs) of the human brain. TFCNs are
primarily limited to quantifying pairwise relationships between ROIs ignoring
higher order dependencies between multiple brain regions. Recently, hypergraph
construction methods from fMRI time series data have been proposed to
characterize the high-order relations among multiple ROIs. While there have
been multiple methods for constructing hypergraphs from fMRI time series, the
question of how to characterize the topology of these hypergraphs remains open.
In this paper, we make two key contributions to the field of community
detection in brain hypernetworks. First, we construct a hypergraph for each
subject capturing high order dependencies between regions. Second, we introduce
a spectral clustering based approach on hypergraphs to detect overlapping
community structure. Finally, the proposed method is implemented to detect the
consensus community structure across multiple subjects. The proposed method is
applied to resting state fMRI data from Human Connectome Project to summarize
the overlapping community structure across a group of healthy young adults.

</details>


### [5] [Time-Varying Offset Estimation for Clock-Asynchronous Bistatic ISAC Systems](https://arxiv.org/abs/2507.09215)
*Yi Wang,Keke Zu,Luping Xiang,Martin Haardt,Kun Yang*

Main category: eess.SP

TL;DR: 提出了一种针对时钟异步双基地ISAC系统的时变偏移估计（TVOE）框架，通过扩展卡尔曼滤波实时跟踪偏移，显著提升感知性能。


<details>
  <summary>Details</summary>
Motivation: 双基地ISAC系统中时钟异步导致的定时偏移和载波频率偏移会引发感知模糊，传统同步方法在无人机场景中不可靠。

Method: 利用视距路径的几何特性建模偏移为隐藏随机过程，通过扩展卡尔曼滤波实时估计和校正偏移。

Result: 仿真显示TVOE方法将估计精度提高了60%。

Conclusion: TVOE框架为时钟异步双基地ISAC系统提供了高效、无需基础设施的同步解决方案。

Abstract: The bistatic Integrated Sensing and Communication (ISAC) is poised to become
a key application for next generation communication networks (e.g., B5G/6G),
providing simultaneous sensing and communication services with minimal changes
to existing network infrastructure and hardware. However, a significant
challenge in bistatic cooperative sensing is clock asynchronism, arising from
the use of different clocks at far separated transmitters and receivers. This
asynchrony leads to Timing Offsets (TOs) and Carrier Frequency Offsets (CFOs),
potentially causing sensing ambiguity. Traditional synchronization methods
typically rely on static reference links or GNSS-based timing sources, both of
which are often unreliable or unavailable in UAVbased bistatic ISAC scenarios.
To overcome these limitations, we propose a Time-Varying Offset Estimation
(TVOE) framework tailored for clock-asynchronous bistatic ISAC systems, which
leverages the geometrically predictable characteristics of the Line-of-Sight
(LoS) path to enable robust, infrastructure-free
  synchronization. The framework treats the LoS delay and the Doppler shift as
dynamic observations and models their evolution as a hidden stochastic process.
A state-space formulation is developed to jointly estimate TO and CFO via an
Extended Kalman Filter (EKF), enabling real-time tracking of clock offsets
across successive frames. Furthermore, the estimated offsets are subsequently
applied to correct the timing misalignment of all Non-Line-of-Sight (NLoS)
components, thereby enhancing the high-resolution target sensing performance.
Extensive simulation results demonstrate that the proposed TVOE method improves
the estimation accuracy by 60%.

</details>


### [6] [Image Super-Resolution-Based Signal Enhancement in Bistatic ISAC](https://arxiv.org/abs/2507.09218)
*Yi Wang,Keke Zu,Luping Xiang,Martin Haardt,Chaochao Wang,Xianchao Zhang,Kun Yang*

Main category: eess.SP

TL;DR: 论文提出了一种基于图像超分辨率的信号增强框架（ISR-SE），用于提升双基地ISAC系统中的信号识别与恢复能力，解决了复杂环境中信号弱化的问题。


<details>
  <summary>Details</summary>
Motivation: 双基地ISAC系统在下一代通信网络中潜力巨大，但复杂环境中的高反射损耗导致信号弱化，传统自适应滤波方法难以适应复杂网络拓扑。

Method: 通过短时傅里叶变换（STFT）生成频谱图，并将其映射为RGB图像，结合UNet和扩散模型的混合架构进行图像去噪。

Result: 提出的ISR-SE框架显著提升了低信噪比条件下的信号质量和清晰度。

Conclusion: ISR-SE框架为双基地ISAC系统中的信号增强提供了有效解决方案，具有实际应用潜力。

Abstract: Bistatic Integrated Sensing and Communication (ISAC) is poised to become a
cornerstone technology in next-generation communication networks, such as
Beyond 5G (B5G) and 6G, by enabling the concurrent execution of sensing and
communication functions without requiring significant modifications to existing
infrastructure. Despite its promising potential, a major challenge in bistatic
cooperative sensing lies in the degradation of sensing accuracy, primarily
caused by the inherently weak received signals resulting from high reflection
losses in complex environments. Traditional methods have predominantly relied
on adaptive filtering techniques to enhance the Signal-to-Noise Ratio (SNR) by
dynamically adjusting the filter coefficients. However, these methods often
struggle to adapt effectively to the increasingly complex and diverse network
topologies. To address these challenges, we propose a novel Image
Super-Resolution-based Signal Enhancement (ISR-SE) framework that significantly
improves the recognition and recovery capabilities of ISAC signals.
Specifically, we first perform a time-frequency analysis by applying the
Short-Time Fourier Transform (STFT) to the received signals, generating
spectrograms that capture the frequency, magnitude, and phase components. These
components are then mapped into RGB images, where each channel represents one
of the extracted features, enabling a more intuitive and informative
visualization of the signal structure. To enhance these RGB images, we design
an improved denoising network that combines the strengths of the UNet
architecture and diffusion models. This hybrid architecture leverages UNet's
multi-scale feature extraction and the generative capacity of diffusion models
to perform effective image denoising, thereby improving the quality and clarity
of signal representations under low-SNR conditions.

</details>


### [7] [Deep Learning for sub-THz Radio Unit Selection using sub-10 GHz Channel Information and Inferred Device Beamforming](https://arxiv.org/abs/2507.09244)
*Nishant Gupta,Muris Sarajlic,Erik G. Larsson*

Main category: eess.SP

TL;DR: 论文提出了一种利用深度学习从sub-10 GHz信道特性推断合适sub-THz RU的方法，解决了beam search的高开销问题，并考虑了UE的IBBC信息。


<details>
  <summary>Details</summary>
Motivation: 未来6G应用中，sub-THz RU的密集部署虽能提供高数据率和可靠覆盖，但beam search或RU选择会带来高开销和功耗。

Method: 利用深度学习从sub-10 GHz信道特性推断sub-THz RU候选，并考虑UE的IBBC（低高频段天线模式的夹角）信息。

Result: 仿真结果表明，推断的sub-THz RU性能良好，忽略UE方向会对系统性能产生负面影响。

Conclusion: 该方法能有效减少sub-THz RU选择的开销，且无需UE共享IBBC信息。

Abstract: The dense and distributed deployment of sub-THz radio units (RUs) alongside
sub-10 GHz access point (AP) is a promising approach to provide high data rate
and reliable coverage for future 6G applications. However, beam search or RU
selection for the sub-THz RUs incurs significant overhead and high power
consumption. To address this, we introduce a method that leverages deep
learning to infer a suitable sub-THz RU candidate from a set of sub-THz RUs
using the sub-10 GHz channel characteristics. A novel aspect of this work is
the consideration of inter-band beam configuration (IBBC), defined as the
broadside angle between the low-band and high-band antenna patterns of the user
equipment (UE). Since IBBC indicates the beamforming information or UE's
orientation, it is typically not shared with the network as a part of
signalling. Therefore, we propose a solution strategy to infer a suitable
sub-THz RU even when UEs do not share their IBBC information. Simulation
results illustrate the performance of the inferred sub-THz RU and highlights
the detrimental impact of neglecting UE orientation on the systems performance.

</details>


### [8] [Matched Filtering-Based Channel Estimation for AFDM Systems in Doubly Selective Channels](https://arxiv.org/abs/2507.09268)
*Xiangjun Li,Zilong Liu,Zhengchun Zhou,Pingzhi Fan*

Main category: eess.SP

TL;DR: 本文提出了一种增强型AFDM波形，通过考虑延迟-多普勒耦合相位，研究了复杂双选择性信道中的匹配滤波辅助信道估计方案，并解决了路径模糊问题。


<details>
  <summary>Details</summary>
Motivation: AFDM作为6G波形具有后向兼容性，但在复杂信道中性能受限，需改进信道估计和路径模糊问题。

Method: 通过推导输入输出关系，分析干扰和SINR，提出MF辅助CE方案和基于GFS的MF-GFS方案。

Result: 仿真结果表明，所提方案在通信性能和复杂度上均有显著优势。

Conclusion: 增强型AFDM及MF辅助CE方案有效提升了系统性能，降低了复杂度。

Abstract: Affine frequency division multiplexing (AFDM) has recently emerged as an
excellent backward-compatible 6G waveform. In this paper, an enhanced AFDM is
proposed whereby the delay-Doppler (DD) coupling phase is considered.
Specifically, we study matched filtering (MF) assisted channel estimation (CE)
for AFDM systems in complex doubly selective channels. By deriving the complete
input-output relationship, the inter-chirp-carrier interference,
signal-to-interference-plus-noise ratio (SINR), and the effective SINR loss of
AFDM, are investigated in discrete affine Fourier transform (DAFT) domain.
Further, we look into the path ambiguity problem and show that it may lead to
severe performance deterioration in fractional-delay fractional-Doppler
channels. To address such a problem, we introduce an MF assisted CE scheme
building upon a novel pilot arrangement across two consecutive AFDM
transmissions. This allows us to sequentially estimate the parameters of each
path by exploiting the separability and approximate orthogonality of different
paths in the DAFT domain, thus leading to significantly reduced complexity.
Furthermore, based on generalized Fibonacci search (GFS), an MF-GFS scheme is
proposed to avoid significantly redundant computation, which can be extended to
typical wide-band systems. Extensive simulation results indicate that the
proposed schemes offer superior advantages in terms of their improved
communication performance and lower complexity.

</details>


### [9] [Free-running vs. Synchronous: Single-Photon Lidar for High-flux 3D Imaging](https://arxiv.org/abs/2507.09386)
*Ruangrawee Kitichotkul,Shashwath Bharadwaj,Joshua Rapp,Yanting Ma,Alexander Mehta,Vivek K Goyal*

Main category: eess.SP

TL;DR: 提出了一种高效的最大似然估计方法，结合正则化框架，提升自由运行单光子激光雷达（SPL）的精度。


<details>
  <summary>Details</summary>
Motivation: 自由运行SPL在减少直方图失真方面有优势，但现有解决方案有限，需提高其精度。

Method: 提出联合最大似然估计器计算信号通量、背景通量和深度，并结合学习的点云评分模型作为先验的正则化框架。

Result: 自由运行SPL在相同条件下比同步SPL估计误差更低，正则化进一步提升了精度。

Conclusion: 自由运行SPL结合所提方法在精度上优于同步SPL，验证了方法的有效性。

Abstract: Conventional wisdom suggests that single-photon lidar (SPL) should operate in
low-light conditions to minimize dead-time effects. Many methods have been
developed to mitigate these effects in synchronous SPL systems. However,
solutions for free-running SPL remain limited despite the advantage of reduced
histogram distortion from dead times. To improve the accuracy of free-running
SPL, we propose a computationally efficient joint maximum likelihood estimator
of the signal flux, the background flux, and the depth using only histograms,
along with a complementary regularization framework that incorporates a learned
point cloud score model as a prior. Simulations and experiments demonstrate
that free-running SPL yields lower estimation errors than its synchronous
counterpart under identical conditions, with our regularization further
improving accuracy.

</details>


### [10] [Lightweight Graph Neural Networks for Enhanced 5G NR Channel Estimation](https://arxiv.org/abs/2507.09408)
*Sajedeh Norouzi,Mostafa Rahmani,Yi Chu,Torsten Braun,Kaushik Chowdhury,Alister Burr*

Main category: eess.SP

TL;DR: GraphNet是一种基于图神经网络（GNN）的轻量级信道估计方法，旨在提升5G NR系统在动态环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统信道估计方法在复杂和动态环境中表现不佳，需要更高效且适应性强的解决方案。

Method: 采用GNN架构，降低计算开销，同时捕捉关键特征以提升估计准确性。

Result: GraphNet在稳定条件下与ChannelNet性能相当，在高动态环境中表现更优，且计算资源需求更低。

Conclusion: GraphNet展示了GNN在信道估计中的潜力，为5G技术提供了一种高效、可扩展的解决方案。

Abstract: Effective channel estimation CE is critical for optimizing the performance of
5G New Radio NR systems particularly in dynamic environments where traditional
methods struggle with complexity and adaptability This paper introduces
GraphNet a novel lightweight Graph Neural Network GNNbased estimator designed
to enhance CE in 5G NR Our proposed method utilizes a GNN architecture that
minimizes computational overhead while capturing essential features necessary
for accurate CE We evaluate GraphNet across various channel conditions from
slowvarying to highly dynamic environments and compare its performance to
ChannelNet a wellknown deep learningbased CE method GraphNet not only matches
ChannelNets performance in stable conditions but significantly outperforms it
in highvariation scenarios particularly in terms of Block Error Rate It also
includes builtin noise estimation that enhances robustness in challenging
channel conditions Furthermore its significantly lighter computational
footprint makes GraphNet highly suitable for realtime deployment especially on
edge devices with limited computational resources By underscoring the potential
of GNNs to transform CE processes GraphNet offers a scalable and robust
solution that aligns with the evolving demands of 5G technologies highlighting
its efficiency and performance as a nextgeneration solution for wireless
communication systems

</details>


### [11] [An Enregy Efficient Design of Hybrid NOMA Based on Hybrid SIC with Power Adaptation](https://arxiv.org/abs/2507.09458)
*Wang Ning,Zhang Chenyu,Sun Yanshi,Min Minghui,Liu Yuanwei,Li Shiyin*

Main category: eess.SP

TL;DR: 论文提出了一种结合混合连续干扰消除（HSIC）和功率适应（PA）的H-NOMA方案，显著提升了无线通信系统的性能，尤其在能效方面优于纯OMA。


<details>
  <summary>Details</summary>
Motivation: 为了进一步释放H-NOMA的潜力，结合HSIC和PA技术，提升数据传输速率和能效。

Method: 提出HSIC-PA辅助的H-NOMA方案，通过严格的数学推导和渐近分析验证其性能。

Result: 在高信噪比下，该方案能以更低能耗实现比纯OMA更高的数据传输速率，且概率趋近于1。

Conclusion: HSIC-PA辅助的H-NOMA方案在能效和性能上显著优于传统方案，并通过数值结果验证了其优越性。

Abstract: Recently, hybrid non-orthogonal multiple access (H-NOMA) technology, which
effectively utilizes both NOMA and orthogonal multiple access (OMA)
technologies through flexible resource allocation in a single transmission, has
demonstrated immense potential for enhancing the performance of wireless
communication systems. To further release the potential of HNOMA, this paper
proposes a novel design of H-NOMA which jointly incorporates hybrid successive
interference cancellation (HSIC) and power adaptation (PA) in the NOMA
transmission phase. To reveal the potential of the proposed HSIC-PA aided
H-NOMA scheme, closed-form expression for the probability of the event that
H-NOMA can achieve a higher data rate than pure OMA by consuming less energy is
rigorously derived. Furthermore, the asymptotic analysis demonstrates that the
probability of the proposed H-NOMA scheme approaches 1 in the high
signal-to-noise ratio (SNR) regime without any constraints on either users'
target rates or transmit power ratios. This represents a significant
improvement over conventional H-NOMA schemes, which require specific
restrictive conditions to achieve probability 1 at high SNRs as shown in
existing work. The above observation indicates that with less energy
consumption, the proposed HSIC-PA aided H-NOMA can achieve a higher data rate
than pure OMA with probability 1 at high SNRs, and hence a higher energy
efficiency. Finally, numerical results are provided to verify the accuracy of
the analysis and also demonstrate the superior performance of the proposed
H-NOMA scheme.

</details>


### [12] [Reframing SAR Target Recognition as Visual Reasoning: A Chain-of-Thought Dataset with Multimodal LLMs](https://arxiv.org/abs/2507.09535)
*Chaoran Li,Xingguo Xu,Siyuan Mu*

Main category: eess.SP

TL;DR: 论文提出了一种基于多模态大语言模型（MLLMs）的SAR目标识别方法，通过Chain-of-Thought推理增强分类效果，实验表明该方法在逻辑推理和可解释性上表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统SAR图像识别方法受限于数据固有缺陷（如弱纹理、高噪声、模糊边界），本文探索通过多模态推理任务重新定义SAR目标识别。

Method: 利用GPT-4o等MLLMs，结合候选类别和Chain-of-Thought推理，对SAR图像进行分类，并基于FAIR-CSAR构建新数据集。

Result: 实验显示MLLMs能生成逻辑连贯且可解释的推理，同时分析了模型的优势和局限性。

Conclusion: 证明了MLLMs在SAR分析中的可行性，为未来SAR视觉推理研究奠定了基础。

Abstract: In the context of Synthetic Aperture Radar (SAR) image recognition,
traditional methods often struggle with the intrinsic limitations of SAR data,
such as weak texture, high noise, and ambiguous object boundaries. This work
explores a novel perspective by reformulating SAR target recognition as a
multimodal reasoning task. We leverage multimodal large language models
(MLLMs), specifically GPT-4o, to perform target classification based on SAR
imagery, guided by candidate categories and enhanced with Chain-of-Thought
(CoT) reasoning. A new dataset is constructed based on the FAIR-CSAR benchmark,
comprising raw SAR images, structured target annotations, candidate label sets,
and GPT-generated CoT reasoning chains. Experimental results show that the
MLLMs are capable of generating logically coherent and interpretable inferences
in most scenarios. Our analysis highlights both the strengths and current
limitations of MLLMs in interpreting SAR imagery, and we provide detailed
insights into model behavior through failure case analysis. This work
demonstrates the feasibility of incorporating MLLMs into SAR analysis pipelines
and establishes a foundation for future research in SAR-oriented visual
reasoning.

</details>


### [13] [Novel Physics-Aware Attention-Based Machine Learning Approach for Mutual Coupling Modeling](https://arxiv.org/abs/2507.09561)
*Can Wang,Wei Liu,Hanzhi Ma,Xiaonan Jiang,Erping Li,Steven Gao*

Main category: eess.SP

TL;DR: 提出了一种物理感知的卷积长短期记忆网络（PC-LSTM），用于高效准确地提取偶极天线阵列的互阻抗矩阵，通过结合物理知识和注意力机制，实现了快速且高精度的阻抗提取。


<details>
  <summary>Details</summary>
Motivation: 传统全波仿真方法（如CST Microwave Studio）计算互阻抗矩阵耗时较长，需要一种更高效的替代方法。

Method: 通过物理感知神经网络重新解释格林函数，并嵌入自适应损失函数；设计注意力机制校准复数值特征；使用卷积LSTM网络处理融合后的表示。

Result: 在五个基准测试中验证了方法的有效性，提取阻抗的精度高，且速度比CST Microwave Studio快7倍。

Conclusion: PC-LSTM是一种快速且物理可解释的替代方案，适用于互耦合特性分析。

Abstract: This article presents a physics-aware convolutional long short-term memory
(PC-LSTM) network for efficient and accurate extraction of mutual impedance
matrices in dipole antenna arrays. By reinterpreting the Green's function
through a physics-aware neural network and embedding it into an adaptive loss
function, the proposed machine learning-based approach achieves enhanced
physical interpretability in mutual coupling modeling. Also, an attention
mechanism is carefully designed to calibrate complex-valued features by fusing
the real and imaginary parts of the Green's function matrix. These fused
representations are then processed by a convolutional long short-term memory
network, and the impedance matrix of the linear antenna array can be finally
derived. Validation against five benchmarks underscores the efficacy of the
proposed approach, demonstrating accurate impedance extraction with up to a 7x
speedup compared to CST Microwave Studio, making it a fast alternative to
full-wave simulations for mutual coupling characterization.

</details>


### [14] [A New Wireless Image Transmission System Using Code Index Modulation and Image Enhancement for High-Rate Next Generation Networks](https://arxiv.org/abs/2507.09713)
*Burak Ahmet Ozden,Erdogan Aydin,Ahmet Elbir,Filiz Gurkan*

Main category: eess.SP

TL;DR: 本文提出了一种基于码索引调制（CIM）的无线图像传输系统（CIM-IT），结合扩频码索引和QAM技术，通过最大似然检测器和增强滤波器优化传输效果。


<details>
  <summary>Details</summary>
Motivation: 随着无线网络技术的发展，高分辨率、高数据流量密度以及多媒体应用的普及，医疗和军事领域对高速可靠图像传输的需求推动了新型高性能无线图像传输系统的设计。

Method: 提出CIM-IT系统，将图像像素值映射为比特，通过扩频码索引和QAM技术在无线信道中传输，接收端使用解扩最大似然检测器和增强滤波器重建图像。

Result: 系统在误码性能、频谱效率、能量效率和吞吐量方面优于传统无线通信技术，并通过提出的高级滤波器进一步优化图像质量。

Conclusion: CIM-IT系统为无线图像传输提供了一种高效可靠的解决方案，尤其在医疗和军事领域具有应用潜力。

Abstract: With the development of wireless network technologies, the wireless image
transmission area has become prominent. The need for high resolution, data
traffic density, widespread use of multimedia applications, and the importance
of high rate and reliable image transmission in medical and military fields
necessitate the design of novel and high-performance wireless image
transmission systems. This paper proposes a code index modulation (CIM)-based
image transmission (CIM-IT) system that utilizes spreading code index and
quadrature amplitude modulation (QAM) symbol for image transmission over a
wireless channel. The proposed CIM-IT system maps bits to each pixel value of
the image to be transmitted and transmits these bits over a wireless channel
using a single-input and multiple-output system comprising code index
modulation and QAM techniques. At the receiver, the active spreading code index
and the selected QAM symbol are estimated using a despreading-based maximum
likelihood detector, and the corresponding bits are obtained. The image
conveyed from the transmitter is then reconstructed at the receiver side using
the pixel values corresponding to the bits. The obtained noisy image is
enhanced using important enhancement filters. In addition, an advanced filter
is proposed to improve the transmitted degraded image with optimum results.
Furthermore, error performance, spectral efficiency, energy efficiency, and
throughputof the CIM-IT system are performed and the results are compared with
traditional wireless communication techniques.

</details>


### [15] [Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing](https://arxiv.org/abs/2507.09776)
*Mihir Kavishwar,Naresh Shanbhag*

Main category: eess.SP

TL;DR: 论文提出了一种名为CACTUS的算法，通过优化ADC参数，降低模拟内存计算（AIMC）中ADC的精度需求，同时提高计算信噪比（CSNR）。


<details>
  <summary>Details</summary>
Motivation: 传统方法高估了ADC精度需求，忽略了量化误差的输入依赖性，导致AIMC的能效受限。

Method: 开发了CSNR的解析表达式，并提出CACTUS算法优化ADC参数。

Result: 在28nm CMOS工艺的SRAM AIMC模型中，CACTUS将ADC精度需求降低3位，同时CSNR提高6dB。

Conclusion: CACTUS在特定条件下优于传统SQNR优化的ADC，为AIMC能效提升提供了新思路。

Abstract: Analog in-memory computing (AIMC) is an energy-efficient alternative to
digital architectures for accelerating machine learning and signal processing
workloads. However, its energy efficiency is limited by the high energy cost of
the column analog-to-digital converters (ADCs). Reducing the ADC precision is
an effective approach to lowering its energy cost. However, doing so also
reduces the AIMC's computational accuracy thereby making it critical to
identify the minimum precision required to meet a target accuracy. Prior works
overestimate the ADC precision requirements by modeling quantization error as
input-independent noise, maximizing the signal-to-quantization-noise ratio
(SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address
these limitations by developing analytical expressions for estimating the
compute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and
propose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a
circuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we
show that for a 256-dimensional binary dot product, CACTUS reduces the ADC
precision requirements by 3b while achieving 6dB higher CSNR over prior
methods. We also delineate operating conditions under which our proposed
CSNR-optimal ADCs outperform conventional SQNR-optimal ADCs.

</details>


### [16] [Precoded Zak-OTFS for Per-Carrier Equalization](https://arxiv.org/abs/2507.09894)
*Saif Khan Mohammed,Amit Kumar Pathak,Muhammad Ubadah,Ronny Hadani,Ananthanarayanan Chockalingam,Robert Calderbank*

Main category: eess.SP

TL;DR: Zak-OTFS调制通过延迟-多普勒域中的脉冲探测信道散射环境，提出了一种新型预编码技术，显著降低了复杂度并提高了频谱效率。


<details>
  <summary>Details</summary>
Motivation: 研究Zak-OTFS调制在双扩展信道下的预编码技术，以简化信道均衡并提高频谱效率。

Method: 利用延迟-多普勒域中的脉冲探测信道散射环境，开发了一种针对二维部分响应信道的新型预编码技术。

Result: 预编码技术实现了对每个延迟-多普勒载波的独立均衡，复杂度显著降低，同时减少了保护载波的开销，提高了频谱效率。

Conclusion: Zak-OTFS调制结合新型预编码技术，为双扩展信道提供了一种高效且低复杂度的解决方案。

Abstract: In Zak-OTFS (orthogonal time frequency space) modulation the carrier waveform
is a pulse in the delay-Doppler (DD) domain, formally a quasi-periodic
localized function with specific periods along delay and Doppler. When the
channel delay spread is less than the delay period, and the channel Doppler
spread is less than the Doppler period, the response to a single Zak-OTFS
carrier provides an image of the scattering environment and can be used to
predict the effective channel at all other carriers. The image of the
scattering environment changes slowly, making it possible to employ precoding
at the transmitter. Precoding techniques were developed more than thirty years
ago for wireline modem channels (V.34 standard) defined by linear convolution
where a pulse in the time domain (TD) is used to probe the one-dimensional
partial response channel. The action of a doubly spread channel on Zak-OTFS
modulation determines a two-dimensional partial response channel defined by
twisted convolution, and we develop a novel precoding technique for this
channel. The proposed precoder leads to separate equalization of each DD
carrier which has significantly lower complexity than joint equalization of all
carriers. Further, the effective precoded channel results in non-interfering DD
carriers which significantly reduces the overhead of guard carriers separating
data and pilot carriers, which improves the spectral efficiency significantly.

</details>


### [17] [AI-Enhanced Wide-Area Data Imaging via Massive Non-Orthogonal Direct Device-to-HAPS Transmission](https://arxiv.org/abs/2507.09895)
*Hyung-Joo Moon,Chan-Byoung Chae,Kai-Kit Wong,Robert W. Heath Jr*

Main category: eess.SP

TL;DR: MAP-X框架通过HAPS和分布式传感器重建空间相关数据，提出两种AI集成方法（DNN和CNN），显著优于传统方法，并提出了地面-HAPS协作框架。


<details>
  <summary>Details</summary>
Motivation: 为延迟敏感的IoT应用提供高效的数据地图重建解决方案。

Method: 采用DNN点估计和CNN图像重建两种AI方法，结合地面-HAPS协作框架。

Result: 两种AI方法均显著优于传统IDFT线性后处理方法。

Conclusion: AI增强的MAP-X适用于灾害响应和网络管理等实际场景。

Abstract: Massive Aerial Processing for X MAP-X is an innovative framework for
reconstructing spatially correlated ground data, such as environmental or
industrial measurements distributed across a wide area, into data maps using a
single high altitude pseudo-satellite (HAPS) and a large number of distributed
sensors. With subframe-level data reconstruction, MAP-X provides a
transformative solution for latency-sensitive IoT applications. This article
explores two distinct approaches for AI integration in the post-processing
stage of MAP-X. The DNN-based pointwise estimation approach enables real-time,
adaptive reconstruction through online training, while the CNN-based image
reconstruction approach improves reconstruction accuracy through offline
training with non-real-time data. Simulation results show that both approaches
significantly outperform the conventional inverse discrete Fourier transform
(IDFT)-based linear post-processing method. Furthermore, to enable AI-enhanced
MAP-X, we propose a ground-HAPS cooperation framework, where terrestrial
stations collect, process, and relay training data to the HAPS. With its
enhanced capability in reconstructing field data, AI-enhanced MAP-X is
applicable to various real-world use cases, including disaster response and
network management.

</details>


### [18] [VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling](https://arxiv.org/abs/2507.09987)
*Zihang Zeng,Shu Sun,Meixia Tao,Yin Xu,Xianghao Yu*

Main category: eess.SP

TL;DR: 提出了一种名为VoxelRF的新方法，用于无线信道建模，通过体素网格表示和浅层MLP实现快速准确的空间频谱合成。


<details>
  <summary>Details</summary>
Motivation: 传统无线信道建模方法在准确性、效率和可扩展性方面存在挑战，而基于神经辐射场（NeRF）的方法训练和推理速度慢。

Method: VoxelRF采用体素网格表示和浅层MLP替代NeRF中的多层感知机（MLP），并引入渐进学习、空区域跳过和背景熵损失函数。

Result: 实验表明，VoxelRF在计算量和训练数据有限的情况下仍能保持高准确性，适用于实时和资源受限的无线应用。

Conclusion: VoxelRF是一种高效且实用的无线信道建模方法，显著提升了速度和资源利用率。

Abstract: Wireless channel modeling in complex environments is crucial for wireless
communication system design and deployment. Traditional channel modeling
approaches face challenges in balancing accuracy, efficiency, and scalability,
while recent neural approaches such as neural radiance field (NeRF) suffer from
long training and slow inference. To tackle these challenges, we propose
voxelized radiance field (VoxelRF), a novel neural representation for wireless
channel modeling that enables fast and accurate synthesis of spatial spectra.
VoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based
methods with trilinear interpolation of voxel grid-based representation, and
two shallow MLPs to model both propagation and transmitter-dependent effects.
To further accelerate training and improve generalization, we introduce
progressive learning, empty space skipping, and an additional background
entropy loss function. Experimental results demonstrate that VoxelRF achieves
competitive accuracy with significantly reduced computation and limited
training data, making it more practical for real-time and resource-constrained
wireless applications.

</details>


### [19] [Sparsity-Aware Extended Kalman Filter for Tracking Dynamic Graphs](https://arxiv.org/abs/2507.09999)
*Lital Dabush,Nir Shlezinger,Tirza Routtenberg*

Main category: eess.SP

TL;DR: 提出了一种基于图信号处理的动态图拓扑跟踪方法，利用非线性状态空间模型和稀疏感知的扩展卡尔曼滤波器，实现了高效且准确的动态图跟踪。


<details>
  <summary>Details</summary>
Motivation: 动态图拓扑的跟踪是图信号处理中的核心问题，涉及电力系统、脑机接口等多个领域，现有方法难以应对非线性测量和高噪声环境。

Method: 采用图基非线性状态空间模型，将图拉普拉斯矩阵参数化为边权重，并开发了稀疏感知的扩展卡尔曼滤波器和动态规划方案计算雅可比矩阵。

Result: 数值研究表明，该方法能在高噪声和非线性测量条件下，准确跟踪稀疏且时变的图拓扑，同时保持低计算复杂度。

Conclusion: 该方法为动态图拓扑跟踪提供了一种高效且鲁棒的解决方案，适用于多种实际应用场景。

Abstract: A broad range of applications involve signals with irregular structures that
can be represented as a graph. As the underlying structures can change over
time, the tracking dynamic graph topologies from observed signals is a
fundamental challenge in graph signal processing (GSP), with applications in
various domains, such as power systems, the brain-machine interface, and
communication systems. In this paper, we propose a method for tracking dynamic
changes in graph topologies. Our approach builds on a representation of the
dynamics as a graph-based nonlinear state-space model (SSM), where the
observations are graph signals generated through graph filtering, and the
underlying evolving topology serves as the latent states. In our formulation,
the graph Laplacian matrix is parameterized using the incidence matrix and edge
weights, enabling a structured representation of the state. In order to track
the evolving topology in the resulting SSM, we develop a sparsity-aware
extended Kalman filter (EKF) that integrates $\ell_1$-regularized updates
within the filtering process. Furthermore, a dynamic programming scheme to
efficiently compute the Jacobian of the graph filter is introduced. Our
numerical study demonstrates the ability of the proposed method to accurately
track sparse and time-varying graphs under realistic conditions, with highly
nonlinear measurements, various noise levels, and different change rates, while
maintaining low computational complexity.

</details>


### [20] [Deep Learning-Based Beamforming Design Using Target Beam Patterns](https://arxiv.org/abs/2507.10063)
*Hongpu Zhang,Shu Sun,Hangsong Yan,Jianhua Mo*

Main category: eess.SP

TL;DR: 提出了一种基于深度学习的波束成形设计框架，可直接将目标波束模式映射到多种天线阵列架构的最优波束成形向量。


<details>
  <summary>Details</summary>
Motivation: 解决在多天线架构（数字、模拟和混合波束成形）下，如何高效设计满足硬件约束的波束成形向量的问题。

Method: 采用轻量级编码器-解码器网络，编码器压缩复杂波束模式为低维特征向量，解码器重建波束成形向量。引入两阶段训练：离线预训练提取鲁棒特征，在线训练解码器使用复合损失函数确保主瓣形状和旁瓣抑制。

Result: 仿真结果表明，该方法在有限信道状态信息下接近全数字波束成形的频谱效率，并优于现有代表性方法。

Conclusion: 该框架为多天线架构下的波束成形设计提供了一种高效且灵活的方法。

Abstract: This paper proposes a deep learning-based beamforming design framework that
directly maps a target beam pattern to optimal beamforming vectors across
multiple antenna array architectures, including digital, analog, and hybrid
beamforming. The proposed method employs a lightweight encoder-decoder network
where the encoder compresses the complex beam pattern into a low-dimensional
feature vector and the decoder reconstructs the beamforming vector while
satisfying hardware constraints. To address training challenges under diverse
and limited channel station information (CSI) conditions, a two-stage training
process is introduced, which consists of an offline pre-training for robust
feature extraction using an auxiliary module, followed by online training of
the decoder with a composite loss function that ensures alignment between the
synthesized and target beam patterns in terms of the main lobe shape and side
lobe suppression. Simulation results based on NYUSIM-generated channels show
that the proposed method can achieve spectral efficiency close to that of fully
digital beamforming under limited CSI and outperforms representative existing
methods.

</details>


### [21] [Intrinsic frequency distribution characterises neural dynamics](https://arxiv.org/abs/2507.10145)
*Ryohei Fukuma,Yoshinobu Kawahara,Okito Yamashita,Kei Majima,Haruhiko Kishima,Takufumi Yanagisawa*

Main category: eess.SP

TL;DR: 论文提出使用动态模态分解（DMD）的固有频率分布来表征神经活动，并验证其在区分健康人与痴呆或帕金森病患者中的有效性。


<details>
  <summary>Details</summary>
Motivation: 理解、预测和控制非线性时空动态系统（如大脑）需要分解多变量时间序列的基本动态。DMD能够捕捉非平稳成分，提供比傅里叶变换更准确的分析。

Method: 利用DMD分解脑电图数据，提取固有频率分布，并与傅里叶变换的幅度谱进行比较。

Result: DMD频率分布在区分患者与健康人时显著优于傅里叶变换的幅度谱。

Conclusion: DMD频率分布可作为表征电生理信号非线性时空动态的新生物标志物。

Abstract: Decomposing multivariate time series with certain basic dynamics is crucial
for understanding, predicting and controlling nonlinear spatiotemporally
dynamic systems such as the brain. Dynamic mode decomposition (DMD) is a method
for decomposing nonlinear spatiotemporal dynamics into several basic dynamics
(dynamic modes; DMs) with intrinsic frequencies and decay rates. In particular,
unlike Fourier transform-based methods, which are used to decompose a
single-channel signal into the amplitudes of sinusoidal waves with discrete
frequencies at a regular interval, DMD can derive the intrinsic frequencies of
a multichannel signal on the basis of the available data; furthermore, it can
capture nonstationary components such as alternations between states with
different intrinsic frequencies. Here, we propose the use of the distribution
of intrinsic frequencies derived from DMDs (DM frequencies) to characterise
neural activities. The distributions of DM frequencies in the
electroencephalograms of healthy subjects and patients with dementia or
Parkinson's disease in a resting state were evaluated. By using the
distributions, these patients were distinguished from healthy subjects with
significantly greater accuracy than when using amplitude spectra derived by
discrete Fourier transform. This finding suggests that the distribution of DM
frequencies exhibits distinct behaviour from amplitude spectra, and therefore,
the distribution may serve as a new biomarker by characterising the nonlinear
spatiotemporal dynamics of electrophysiological signals.

</details>


### [22] [Pinching-Antenna Systems for Physical Layer Security](https://arxiv.org/abs/2507.10167)
*Kaidi Wang,Zhiguo Ding,Naofal Al-Dhahir*

Main category: eess.SP

TL;DR: 研究了捏合天线系统在增强物理层安全方面的潜力，通过振幅和相位调整优化合法用户信号并削弱窃听者信号，提出基于Shapley值的天线激活算法，仿真显示其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索捏合天线系统在物理层安全中的应用潜力，通过多天线协作提升合法用户的通信质量并抑制窃听者。

Method: 在波导上预装多个捏合天线，通过振幅和相位调整优化信号，建模为联盟博弈，提出基于Shapley值的天线激活算法。

Result: 仿真结果表明，该系统显著提升了保密速率，且基于Shapley值的算法优于传统联盟值方法。

Conclusion: 捏合天线系统通过多天线协作和Shapley值算法，有效提升了物理层安全性能。

Abstract: This letter investigates the potential of pinching-antenna systems for
enhancing physical layer security. By pre-installing multiple pinching antennas
at discrete positions along a waveguide, the capability of the considered
system to perform amplitude and phase adjustment is validated through the
formulation of a secrecy rate maximization problem. Specifically, amplitude
control is applied to enhance the signal quality at the legitimate user, while
phase alignment is designed to degrade the received signal quality at the
eavesdropper. This cooperation among pinching antennas is modeled as a
coalitional game, and a corresponding antenna activation algorithm is proposed.
The individual impact of each antenna is quantified based on the Shapley value
and marginal contribution, providing a fair and efficient method for
performance evaluation. Simulation results show that the considered
pinching-antenna system achieves significant improvements in secrecy rate, and
that the Shapley value based algorithm outperforms conventional coalition value
based solutions.

</details>


### [23] [Pinching-Antenna Systems with LoS Blockages](https://arxiv.org/abs/2507.10173)
*Kaidi Wang,Chongjun Ouyang,Yuanwei Liu,Zhiguo Ding*

Main category: eess.SP

TL;DR: 研究了捏合天线系统在视线（LoS）阻塞情况下构建LoS链路的能力，通过优化波导分配和天线激活，提出匹配算法提升系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 探索在LoS阻塞情况下，利用捏合天线系统动态建立LoS链路以增强信号并消除干扰。

Method: 通过联合优化波导分配和天线激活，提出基于匹配的算法，采用两种偏好设计。

Result: 仿真结果表明，该系统能有效利用LoS阻塞减少干扰，显著提高系统吞吐量。

Conclusion: 捏合天线系统及提出的解决方案能动态建立LoS链路，有效利用阻塞提升性能。

Abstract: The aim of this letter is to explore the capability of pinching-antenna
systems to construct line-of-sight (LoS) links in the presence of LoS
blockages. Specifically, pinching antennas are pre-installed at preconfigured
positions along waveguides and can be selectively activated to create LoS links
for enhancing desired signals and non-line-of-sight (NLoS) links for
eliminating inter-user interference. On this basis, a sum-rate maximization
problem is formulated by jointly optimizing waveguide assignment and antenna
activation. To solve this problem, a matching based algorithm is proposed using
two distinct preference designs. Simulation results demonstrate that the
considered pinching-antenna system and proposed solutions can dynamically
establish LoS links and effectively exploit LoS blockages to mitigate
interference, thereby significantly improving system throughput.

</details>


### [24] [Enhanced Throughput and Seamless Handover Solutions for Urban 5G-Vehicle C-Band Integrated Satellite-Terrestrial Networks](https://arxiv.org/abs/2507.10308)
*Hung Nguyen-Kha,Vu Nguyen Ha,Eva Lagunas,Symeon Chatzinotas,Joel Grotz*

Main category: eess.SP

TL;DR: 本文研究了5G卫星-地面综合网络（ISTN）中支持城市环境中移动用户（UE）的下行传输，通过联合优化功率分配和用户关联，解决高密度遮挡和动态覆盖带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 城市环境中高密度遮挡、UE移动性及LEO卫星动态覆盖对用户关联和资源分配提出了重大挑战，需优化吞吐量和无缝切换。

Method: 提出基于连续凸近似（SCA）的迭代算法和实用的预测算法，联合优化功率分配和用户关联。

Result: 仿真使用伦敦3D地图和Google导航数据，验证了算法在吞吐量和连接变化上的有效性。

Conclusion: 所提算法在吞吐量和连接变化方面优于贪婪和基准算法，适用于实际场景。

Abstract: This paper investigates downlink transmission in 5G Integrated
Satellite-Terrestrial Networks (ISTNs) supporting automotive users (UEs) in
urban environments, where base stations (BSs) and Low Earth Orbit (LEO)
satellites (LSats) cooperate to serve moving UEs over shared C-band frequency
carriers. Urban settings, characterized by dense obstructions, together with UE
mobility, and the dynamic movement and coverage of LSats pose significant
challenges to user association and resource allocation. To address these
challenges, we formulate a multi-objective optimization problem designed to
improve both throughput and seamless handover (HO). Particularly, the
formulated problem balances sum-rate (SR) maximization and connection change
(CC) minimization through a weighted trade-off by jointly optimizing power
allocation and BS-UE/LSat-UE associations over a given time window. This is a
mixed-integer and non-convex problem which is inherently difficult to solve. To
solve this problem efficiently, we propose an iterative algorithm based on the
Successive Convex Approximation (SCA) technique. Furthermore, we introduce a
practical prediction-based algorithm capable of providing efficient solutions
in real-world implementations. Especially, the simulations use a realistic 3D
map of London and UE routes obtained from the Google Navigator application to
ensure practical examination. Thanks to these realistic data, the simulation
results can show valuable insights into the link budget assessment in urban
areas due to the impact of buildings on transmission links under the blockage,
reflection, and diffraction effects. Furthermore, the numerical results
demonstrate the effectiveness of our proposed algorithms in terms of SR and the
CC-number compared to the greedy and benchmark algorithms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: 本文提出了一种名为Recurrent Expansion (RE)的新学习范式，超越了传统的机器学习和深度学习，通过分析模型自身行为的演化来实现自我迭代改进。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习主要关注静态数据表示，而RE旨在通过模型行为的动态分析实现更智能的学习系统。

Method: RE通过多次映射数据并分析内部表示与性能信号（如损失）来迭代改进模型。扩展版本包括Multiverse RE (MVRE)和Heterogeneous MVRE (HMVRE)，后者进一步引入不同架构模型的多样性。

Result: RE及其变体（如Sc-HMVRE）为深度学习引入了行为感知和自我演化的能力，为可扩展、自适应的AI奠定了基础。

Conclusion: RE标志着深度学习从静态表示学习向行为感知和自我演化系统的转变，为未来智能模型的发展提供了新方向。

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [26] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: 论文提出了一种基于可解释人工智能（XAI）的高效三重模块冗余（TMR）方法，用于提升深度神经网络（DNN）在比特翻转故障下的可靠性。通过梯度驱动的XAI技术（LRP）计算参数重要性，选择性应用TMR，显著提升了模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，DNN的可靠性至关重要。传统TMR方法开销大，需选择性应用以优化效率。XAI技术能提供参数重要性信息，为TMR的选择标准提供依据。

Method: 采用低成本的梯度驱动XAI技术（LRP）计算DNN参数的重要性分数，选择关键权重应用TMR。在VGG16和AlexNet模型上验证，使用MNIST和CIFAR-10数据集。

Result: 在比特错误率为10-4时，该方法对AlexNet模型的可靠性提升超过60%，同时保持与现有方法相同的开销。

Conclusion: 基于XAI的TMR方法能高效提升DNN的可靠性，为安全关键应用提供了可行的解决方案。

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [27] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: 提出了一种名为“行为探索”的方法，通过训练长上下文生成模型，使自主代理能够快速在线探索和适应环境，模仿专家行为并实现专家级探索。


<details>
  <summary>Details</summary>
Motivation: 现有算法依赖随机探索和缓慢的梯度更新，而人类能快速适应和学习。研究旨在赋予自主代理类似人类的能力。

Method: 利用专家演示数据集，训练长上下文生成模型预测专家行为，并结合探索性度量，实现快速在线适应和专家级探索。

Result: 在模拟运动和操作任务以及真实机器人操作中验证了方法的有效性，展示了其学习适应性和探索行为的能力。

Conclusion: 行为探索方法成功实现了快速在线适应和专家级探索，为自主代理的能力提升提供了新思路。

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [28] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: 论文提出了一种结合机器学习和人机交互的决策支持系统，帮助印度卡纳塔克邦的农民应对市场和气候波动，并通过语音界面解决识字障碍。


<details>
  <summary>Details</summary>
Motivation: 农民面临市场和气候波动，同时因识字障碍被排除在数字革命之外，需要一种包容且实用的解决方案。

Method: 结合随机森林分类器评估农艺适宜性，LSTM网络预测市场价格，并通过语音界面（卡纳达语）提供服务。

Result: 随机森林模型准确率达98.5%，LSTM模型预测价格误差低，系统显著提升农民经济韧性。

Conclusion: 该系统为边缘化农民社区提供了可扩展且高效的经济优化解决方案。

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [29] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: 论文研究了在连续状态和动作动态系统中模仿专家演示者的问题，提出了两种干预措施以缓解误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在离散场景中已取得巨大成功，但在物理场景（如自动驾驶和机器人学习）中由于误差累积问题更为复杂，需要更先进的策略或数据增强方法。

Method: 提出了两种干预措施：在开环稳定系统中使用“动作分块”（预测并执行动作序列），在可能不稳定的系统中使用“噪声注入”（在专家演示中添加噪声）。

Result: 这些干预措施有效缓解了误差累积问题，并与现代机器人学习中的流行选择一致，但其效果与设计目标不同。

Conclusion: 研究结合了控制理论和强化学习的工具，揭示了单独考虑任一领域时未出现的新问题。

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [30] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: LoRA是一种广泛用于微调大语言模型的技术，通过引入低秩权重矩阵减少参数更新量，但速度提升不一致。本文分析其性能限制因素，并提出更高效的微调方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在不同模型架构和训练设置中速度提升不一致，作者希望探究其性能限制因素。

Method: 对LoRA性能进行全面分析，提出更高效的微调方法，并进行实验评估。

Result: 新方法在性能上与LoRA相当或更优，同时提供更一致的训练速度提升。

Conclusion: 本文为资源受限下优化LLM微调提供了实用指南和见解。

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [31] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: cs.LG

TL;DR: 提出了一种基于控制理论中H²模型降阶技术的方法，用于减少线性SSM模型的参数量，实验表明该方法在保持性能的同时将参数量减少到1/32。


<details>
  <summary>Details</summary>
Motivation: 线性SSM模型在捕获长程依赖时参数量过大，难以在资源受限设备上部署。

Method: 应用控制理论中的H²模型降阶技术对线性SSM组件进行参数压缩。

Result: 在LRA基准测试中，该方法优于现有的平衡截断方法，参数量减少到1/32且性能未下降。

Conclusion: 提出的方法能有效减少线性SSM模型的参数量，适用于资源受限环境。

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [32] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络（PINN）的框架，用于模拟二维平流-扩散方程控制的污染物扩散，通过嵌入物理定律和拟合噪声合成数据，解决了传统数值方法在大规模动态海洋域中的建模难题。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理大规模动态海洋域中的污染物传输时面临复杂性和规模上的挑战，因此需要一种更高效且物理一致的建模方法。

Method: 采用物理信息神经网络（PINN）框架，结合二维平流-扩散方程，通过混合损失函数（包括PDE残差、边界/初始条件一致性及加权数据拟合项）进行训练，并利用Julia语言的高性能计算能力。

Result: 模型能够生成物理一致的预测，并通过噪声合成数据捕捉现实世界的变异性，为传统求解器提供了可扩展且灵活的替代方案。

Conclusion: PINN框架在污染物扩散模拟中表现出色，为解决复杂动态系统的建模问题提供了新思路。

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [33] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer神经网络的洗钱检测方法，通过对比学习生成时间序列表示，并结合双阈值和BH程序控制误报率，实验表明其优于基于规则或LSTM的方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统洗钱检测方法（如基于规则或LSTM）在误报率和检测能力上的不足。

Method: 1. 通过对比学习无监督生成时间序列表示；2. 利用这些表示生成洗钱评分；3. 引入双阈值和BH程序控制误报率。

Result: Transformer能有效捕捉洗钱模式，检测能力优于传统方法，同时误报率可控。

Conclusion: 新方法在洗钱检测中表现出色，尤其在减少误报和提高检测能力方面优于传统方法。

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [34] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: CompactifAI压缩方法应用于Llama 3.1 8B模型，显著减少计算资源并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 评估CompactifAI压缩方法在大语言模型Llama 3.1 8B上的性能，关注效率和准确性。

Method: 使用Codecarbon和Ragas框架分别评估能源消耗和准确性，对比压缩与完整模型。

Result: 压缩模型显著减少资源消耗，同时保持准确性，更高效、可扩展且成本效益高。

Conclusion: CompactifAI压缩方法有效提升模型效率，适合大规模应用。

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [35] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: 论文提出了一种名为wd1的新方法，通过加权似然优化扩散大语言模型的推理能力，避免了传统RL方法的计算开销和偏差问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）的推理能力提升是一个开放性问题，传统RL方法因似然函数难以处理而引入计算开销和潜在偏差。

Method: 提出wd1方法，将目标重新表述为加权似然，仅需近似当前参数化策略的似然，简化了计算。

Result: 在推理基准测试中，wd1无需监督微调或数据，性能优于现有RL方法，准确率提升16%，同时减少训练时间和梯度步数。

Conclusion: wd1因其高效性和简单性，成为优化dLLMs推理的更有效方法。

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [36] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的迁移学习方法（TAT），利用阿尔茨海默病（AD）数据增强路易体痴呆（LBD）的诊断，解决了数据稀缺和领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: LBD是一种常见但研究不足的痴呆症，诊断面临数据稀缺和领域偏移的挑战，而AD数据丰富但存在领域差异。

Method: 使用结构MRI提取的结构连接性（SC）作为训练数据，通过TAT自适应分配权重，突出疾病可转移特征，抑制领域特异性特征。

Result: 实验证明TAT有效减少了领域偏移，提高了LBD诊断的准确性。

Conclusion: TAT为数据稀缺和领域偏移条件下的罕见疾病诊断提供了新框架。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [37] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: 提出了一种名为WRCor的无训练估计代理，用于加速神经架构搜索（NAS），并通过实验验证其高效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本NAS方法的有效性、稳定性和通用性不足，需要更高效的架构估计策略。

Method: 使用加权响应相关性（WRCor）作为训练免费代理，通过响应样本间的相关系数矩阵计算代理分数。

Result: WRCor及其投票代理在代理评估中表现更优，NAS算法在不同搜索空间中优于现有方法，ImageNet-1k上仅需4 GPU小时即可发现22.1%测试错误的架构。

Conclusion: WRCor是一种高效、稳定的零样本NAS代理，显著提升了架构搜索的效率和性能。

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [38] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: FedRAS是一个通信高效的联邦推荐系统框架，通过动作共享策略减少通信开销，同时避免模型性能下降。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统（FedRecs）面临通信开销高和训练效率低的问题，现有压缩方法会导致性能下降。

Method: 采用动作共享策略，将梯度聚类为有限的动作进行通信，而非直接压缩嵌入矩阵，并结合自适应聚类机制。

Result: 实验表明，FedRAS能减少96.88%的通信负载，且不影响推荐性能。

Conclusion: FedRAS有效解决了通信和效率问题，适用于异构环境。

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [39] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: FLLL3M是一个基于联邦学习和大语言模型的隐私保护框架，用于下一位置预测，具有高精度和低资源需求。


<details>
  <summary>Details</summary>
Motivation: 解决下一位置预测中的隐私保护和资源效率问题。

Method: 通过保留用户数据本地化，并利用大语言模型的高效外积机制。

Result: 在多个数据集上取得SOTA结果，参数减少45.6%，内存使用减少52.7%。

Conclusion: FLLL3M是一种高效且隐私保护的下一位置预测解决方案。

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [40] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: 论文提出了一种动态自适应扇出优化采样器（DAFOS），通过动态调整扇出和优先处理重要节点，显著提升了GNN的训练速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的均匀邻居采样和静态扇出设置限制了GNN的可扩展性和效率，因此需要一种更高效的采样方法。

Method: DAFOS基于节点度对节点评分，动态调整扇出，并集成早停机制。

Result: 在三个基准数据集上，DAFOS显著提升了训练速度和F1分数，最高达到12.6倍加速。

Conclusion: DAFOS是一种高效且可扩展的大规模GNN训练解决方案。

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [41] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: 本文提出AMLAS-RL框架，用于为强化学习（RL）系统生成安全保证论证，弥补现有方法在RL生命周期中系统性安全保证的不足。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习（ML）在信息物理系统（CPS）中的广泛应用，RL因其适应复杂动态环境的能力而备受关注，但其安全性和保证性仍面临挑战。现有方法如Safe-RL和AMLAS未能完全解决RL的独特需求。

Method: 作者提出AMLAS-RL框架，通过迭代过程为RL系统生成安全保证论证，并以轮式车辆避障任务为例进行演示。

Result: AMLAS-RL框架成功为RL系统提供了系统性安全保证，解决了现有方法的不足。

Conclusion: AMLAS-RL为RL在安全关键应用中的安全保证提供了有效解决方案，填补了现有方法的空白。

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [42] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: 比较时间序列基础模型（TSFMs）与传统方法在共形预测中的表现，发现TSFMs在数据有限时表现更优，预测区间更可靠且校准更稳定。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在时间序列共形预测中的潜力，特别是在数据有限的情况下。

Method: 在共形预测设置下，比较TSFMs与传统统计模型和梯度提升方法的性能。

Result: TSFMs在数据有限时提供更可靠的预测区间和更稳定的校准过程，优势随数据减少而更明显。

Conclusion: 基础模型能显著提升时间序列共形预测的可靠性，尤其在数据受限场景中。

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [43] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: 论文提出了图神经网络（GNN）在求解偏微分方程（PDE）时所需消息传递迭代次数的严格下界，减少了超参数调优的需求。


<details>
  <summary>Details</summary>
Motivation: 通过将物理问题的特性与GNN的消息传递需求关联，为三类基本PDE（双曲、抛物、椭圆）推导下界，以优化GNN在求解PDE时的效率。

Method: 研究PDE的物理常数、时空离散化与GNN消息传递机制的关系，推导出迭代次数的下界。

Result: 当迭代次数低于下界时，信息传播效率低，解质量差；满足下界时，GNN能准确捕捉现象，得到高精度解。

Conclusion: 通过四个方程实例验证了下界的严格性，表明该方法能有效指导GNN在PDE求解中的应用。

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [44] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: 论文提出了一种新的业务对齐评估指标e-Profits，用于量化客户关系管理中的流失预测模型性能，优于传统指标如AUC和F1-score。


<details>
  <summary>Details</summary>
Motivation: 传统指标（如AUC和F1-score）未能反映财务结果，可能导致战略决策失误。

Method: e-Profits基于客户特定价值、保留概率和干预成本，利用Kaplan-Meier生存分析估计个性化保留率，支持细粒度评估。

Result: 在电信数据集上测试六种分类器，e-Profits重塑了模型排名，揭示了传统指标忽略的财务优势。

Conclusion: e-Profits是一种易于理解的工具，支持业务场景下的模型评估，尤其适用于利润驱动的决策。

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [45] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 论文研究了数据偏见对算法歧视的影响，提出了数据偏见配置文件（DBP）来检测和缓解偏见，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据偏见是算法歧视的主要驱动因素，但目前研究不足，阻碍了检测和缓解偏见的实践发展。

Method: 研究三种常见数据偏见的单独及联合影响，开发检测机制并整合为DBP。

Result: 发现弱势群体在训练集中的代表性不足对歧视的影响较小，而代理和标签偏见的组合更为关键。DBP能有效预测歧视风险。

Conclusion: DBP为系统记录偏见信号提供了初步框架，连接了算法公平研究和反歧视政策。

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [46] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 论文研究了无需Lipschitz平滑假设的随机洗牌梯度方法，提出新的步长策略，证明了在非凸、强凸和非强凸情况下的收敛性，并通过实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有随机洗牌梯度方法的收敛性分析大多依赖Lipschitz平滑条件，但许多机器学习模型不满足此条件，因此需要研究更弱的假设下的收敛性。

Method: 提出一种新的步长策略，分析随机洗牌梯度方法在非凸、强凸和非强凸情况下的收敛性，支持随机洗牌和任意洗牌方案。

Result: 算法在更弱的假设下收敛，且收敛速率与当前已知最优速率一致，实验验证了其实际性能。

Conclusion: 该研究扩展了随机洗牌梯度方法的适用范围，为不满足Lipschitz平滑条件的模型提供了理论支持。

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [47] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: 论文提出了一种基于反向离散化和近端映射的扩散模型（ProxDM），相比传统得分匹配方法，在理论和实践上均表现出优势。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在高维数据生成中表现出色，但传统方法依赖得分（梯度）进行采样，存在效率问题。本文探索通过反向离散化和近端映射提升性能。

Method: 使用反向离散化随机微分方程（SDE），以近端映射替代得分，结合近端匹配技术学习对数密度的近端算子，构建ProxDM。

Result: 理论证明ProxDM仅需$\widetilde{O}(d/\sqrt{\varepsilon})$步即可生成KL散度$\varepsilon$-精确的分布；实验显示其收敛速度显著快于传统方法。

Conclusion: ProxDM通过反向离散化和近端映射，在生成效率和准确性上优于传统得分匹配方法，为扩散模型提供了新思路。

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [48] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: 论文探讨了如何开发高效的AI建议系统，以提升假设生成和实验设计的质量。研究发现，小型模型结合压缩文献库和结构化推理框架，能在特定任务中超越通用语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究快速发展，但缺乏可扩展的建议系统来提供高质量反馈，以优化假设和实验设计。

Method: 研究关键因素（如模型大小、上下文长度、置信度估计和结构化推理），开发小型模型结合压缩文献库和结构化推理框架。

Result: 小型模型在ICLR 2025的自评前30%提交中表现优于通用模型，高置信度预测的接受率超过90%。

Conclusion: 该系统显著提升了假设生成和实验设计的质量与效率，代码已开源。

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [49] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: 本文分析了离散扩散模型中无分类器引导（CFG）的作用，发现早期高引导会损害生成质量，而后期引导影响更大。提出了改进方法，通过简单代码调整提升样本质量。


<details>
  <summary>Details</summary>
Motivation: 研究离散扩散模型中CFG引导时间表的作用，解释现有实现中的问题，并提出改进方案。

Method: 理论分析CFG在掩码离散扩散中的作用，提出新的引导机制，通过平滑数据分布与初始分布的传输提升质量。

Result: 实验证明改进方法在ImageNet和QM9数据集上有效提升样本质量。

Conclusion: 早期高引导有害，后期引导更有效；提出的简单改进方法显著提升离散扩散模型的生成质量。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [50] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 提出了一种基于学习的旅行需求建模框架，结合人口合成、活动生成、位置分配和大规模微观交通模拟，显著降低了建模成本并提高了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统基于活动的模型（ABMs）依赖简化规则和假设，开发成本高且难以跨区域适应。

Method: 集成人口合成、协调活动生成、位置分配和微观交通模拟的统一系统，数据驱动且可迁移。

Result: 在洛杉矶的验证中，模型复现了真实世界移动模式，性能接近传统ABMs，成本更低且可扩展性更强。

Conclusion: 该框架为旅行需求建模提供了高效、可扩展且可迁移的解决方案。

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [51] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: 论文提出了一种名为SGNNs的框架，通过利用机制模拟作为神经网络的训练数据，解决了科学建模中机制模型和机器学习模型的局限性。SGNNs在多个科学领域和任务中表现优异，并提供了新的可解释性方法。


<details>
  <summary>Details</summary>
Motivation: 科学建模中，机制模型可解释但难以应对复杂现实，而机器学习模型灵活但依赖大量标注数据且缺乏可解释性。SGNNs旨在结合两者的优势。

Method: SGNNs利用机制模拟生成合成数据训练神经网络，覆盖多样化的模型结构、参数范围和观测噪声。

Result: SGNNs在预测和推理任务中均取得领先结果，如COVID-19预测、化学反应产率预测和生态预测等。此外，SGNNs提供了新的可解释性方法。

Conclusion: SGNNs将科学理论与深度学习灵活性结合，开创了新的建模范式，使模拟成为灵活的监督来源，支持鲁棒且可解释的推理。

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [52] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: 提出了一种基于CLIP模型的语义通信框架，无需训练即可提取数据语义，并通过PPO算法优化无线网络中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于神经网络的语义编码和解码需要联合训练，而CLIP模型无需训练即可实现语义提取，同时解决无线网络中噪声和资源限制的问题。

Method: 使用CLIP模型提取语义信息，结合PPO强化学习算法优化模型架构和频谱资源分配。

Result: 仿真结果显示，该方法收敛速度提升40%，累积奖励提高4倍。

Conclusion: 基于CLIP的语义通信框架在无线网络中表现优异，显著提升了性能和效率。

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [53] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: 提出了一种改进高斯概率生成模型（GPGMs）生成效率的框架，通过识别数据快速收敛到高斯分布的特征步骤，并用闭式高斯近似替代剩余轨迹。


<details>
  <summary>Details</summary>
Motivation: GPGMs的高计算成本限制了其实际部署，需要减少生成轨迹的冗余步骤。

Method: 分析数据快速收敛到高斯分布的特征步骤，用闭式高斯近似替代剩余轨迹，避免冗余扰动。

Result: 在多种数据模态上实现了样本质量和计算效率的显著提升。

Conclusion: 该方法在保持学习动态完整性的同时，显著提高了生成效率。

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [54] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: VIPEEGNet是一种基于卷积神经网络的模型，用于通过EEG及时识别有害脑活动，性能接近人类专家，并在外部验证中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在EEG识别脑活动时存在评分者间差异、资源限制和泛化性差的问题，需要更高效的解决方案。

Method: 开发并验证了VIPEEGNet模型，使用两个独立数据集（1950名患者和1532名患者）进行训练和测试。

Result: VIPEEGNet在二元分类和多分类任务中表现出高准确性和与人类专家相当的性能，外部验证排名前2。

Conclusion: VIPEEGNet是一种高效且参数较少的模型，适用于EEG脑活动识别，具有临床应用潜力。

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [55] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: 本文提出了一种名为ODIA的新方法，通过在线用户交互数据加速LLM的函数调用，显著降低延迟并保持准确性。


<details>
  <summary>Details</summary>
Motivation: LLM函数调用的高延迟影响用户体验，需要一种高效且自动化的解决方案。

Method: 利用在线用户数据识别简单查询，并通过知识蒸馏将大模型能力迁移到小模型。

Result: 延迟降低45%（预期）和78%（中位数），小模型处理60%流量且精度损失可忽略。

Conclusion: ODIA是一种实用且自动化的解决方案，适用于生产环境。

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [56] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: 论文提出了一种基于广义投影贝尔曼误差（GPBE）的多步信用分配方法，改进了深度强化学习中的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法如半梯度TD简单高效但易发散，而GTD方法虽有收敛保证但很少用于深度RL。GPBE的引入为非线性函数逼近提供了高效支持，但仅限于单步方法，限制了信用分配速度和样本效率。

Method: 将GPBE目标扩展为支持基于λ-回报的多步信用分配，并提出了三种梯度优化方法，包括前向视图和后向视图。

Result: 在MuJoCo和MinAtar环境中，新算法优于PPO和StreamQ。

Conclusion: 多步GPBE方法显著提升了深度RL的稳定性和学习效率。

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [57] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Main category: cs.LG

TL;DR: 论文提出了一种基于哈密顿蒙特卡洛（HMC）采样的深度神经网络（DNN）概率最后一层方法（LL-HMC），以降低计算成本并提升不确定性估计能力。


<details>
  <summary>Details</summary>
Motivation: 尽管HMC是不确定性估计的金标准，但其计算成本限制了其在大规模数据和大型DNN中的应用。LL-HMC通过仅对DNN最后一层进行HMC采样，解决了这一问题。

Method: LL-HMC将HMC采样限制在DNN的最后一层，减少了计算需求。论文将其与五种最后一层概率深度学习方法（LL-PDL）在三个真实视频数据集上进行比较，评估分类性能、校准和分布外（OOD）检测。

Result: LL-HMC在分类和OOD检测上表现竞争性。额外的采样参数未提升分类性能，但改进了OOD检测。多链或不同初始位置未带来一致改进。

Conclusion: LL-HMC是一种高效的概率最后一层方法，适用于计算资源有限的场景，尤其在OOD检测方面表现优异。

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [58] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: 论文提出了一种通用的低秩分解方法，通过隐式神经信号表示框架解决连续时间向量值信号的PCA和ICA问题。


<details>
  <summary>Details</summary>
Motivation: 传统PCA和ICA方法在处理连续时间信号、点云或不规则采样信号时存在局限性，需要一种更通用的解决方案。

Method: 使用连续时间随机过程建模信号，通过对比函数项统一PCA和ICA方法，强制学习分解后的信号具有所需的统计特性（如去相关、独立性）。

Result: 该方法能够应用于传统技术无法处理的点云和不规则采样信号。

Conclusion: 提出的框架为连续域信号分解提供了通用且灵活的方法，扩展了PCA和ICA的应用范围。

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [59] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 论文提出了一种名为Fair-FLIP的后处理方法，旨在减少深度伪造检测中的偏见，同时保持检测性能。实验表明，该方法能显著提升公平性指标，且对准确率影响极小。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测方法虽然性能高，但存在对种族和性别等人口属性的偏见，威胁公众信任。

Method: 提出Fair-FLIP方法，通过重新加权训练模型的最后一层输入，减少子群体间的差异。

Result: Fair-FLIP将公平性指标提升高达30%，准确率仅下降0.25%。

Conclusion: Fair-FLIP是一种有效的公平性优化方法，适用于深度伪造检测领域。

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [60] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: 研究了特征选项（eigenoptions）在强化学习中对信用分配和探索的作用，发现预设特征选项有助于两者，而在线发现可能阻碍学习。


<details>
  <summary>Details</summary>
Motivation: 探索特征选项是否能在无模型强化学习中加速信用分配，并评估其在表格和像素网格世界中的表现。

Method: 在表格和像素网格世界中评估预设和在线发现特征选项的效果，并提出一种在非线性函数逼近下学习选项值的方法。

Result: 预设特征选项有助于信用分配和探索，而在线发现可能因偏置经验而阻碍学习。终止条件对性能有显著影响。

Conclusion: 特征选项在支持信用分配和探索方面具有潜力，但也存在复杂性，尤其是在在线发现和终止条件设计上。

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [61] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 论文提出了一种基于在线世界模型的持续强化学习方法，通过规划解决任务序列中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习（CRL）中，智能体在学习新任务时容易忘记旧任务，即灾难性遗忘。本文旨在解决这一问题。

Method: 使用在线学习的Follow-The-Leader浅层模型捕捉世界动态，并通过模型预测控制规划任务。模型具有理论保证的遗憾边界。

Result: 在设计的Continual Bench环境中，该方法优于基于深度世界模型和持续学习技术的基线方法。

Conclusion: 提出的在线世界模型方法能持续学习新任务且不遗忘旧技能，性能优于现有方法。

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [62] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Main category: cs.LG

TL;DR: TolerantECG是一种针对ECG信号的基础模型，能够抗噪声并适应部分导联缺失的情况，通过对比学习和自监督学习框架训练，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: ECG信号可能因噪声或导联缺失导致诊断错误或不确定性，因此需要一种鲁棒的模型来解决这些问题。

Method: 结合对比学习和自监督学习框架，训练模型学习ECG信号表示及其文本报告描述，同时处理噪声或导联缺失信号。

Result: 在PTB-XL数据集中表现最佳或次佳，在MIT-BIH心律失常数据库中性能最高。

Conclusion: TolerantECG是一种有效的解决方案，能够提升ECG信号分析的鲁棒性和准确性。

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [63] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: DeepX-GAN模型用于模拟超出历史记录的极端气候事件，揭示潜在风险。


<details>
  <summary>Details</summary>
Motivation: 传统方法低估了极端气候的空间依赖性和未见过的事件风险。

Method: 开发了DeepX-GAN，一种知识驱动的深度生成模型，捕捉罕见极端事件的空间结构。

Result: 模型成功模拟了未见的极端事件，揭示了脆弱地区的潜在风险。

Conclusion: 需制定空间适应性政策，以应对未来气候变暖带来的新兴风险热点。

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [64] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的跨平台广告推荐方法，通过多维建模提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 提高跨平台广告推荐的准确性，捕捉用户兴趣迁移的潜在路径。

Method: 利用用户行为数据、广告内容和平台特征进行多维建模，通过GNN捕捉兴趣迁移路径。

Result: 实验结果显示，Platform B的AUC值达到0.937，表现最佳；Platform A和C的精度和召回率略有下降。通过调整超参数，模型的适应性和鲁棒性得到提升。

Conclusion: 该方法在多平台数据上表现良好，通过超参数调整进一步优化了模型性能。

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [65] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 论文分析了联邦学习（FL）中客户端随机参与的问题，提出了对Agnostic FedAvg算法的收敛性证明，适用于非均匀参与分布，并展示了其优于加权聚合变体的性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端随机参与且参与概率未知或偏置的实际挑战，现有方法通常假设完全参与或已知均匀分布，而实际中这些假设不成立。

Method: 研究Agnostic FedAvg算法在随机且变规模客户端参与下的优化问题，并证明其在凸且可能非光滑损失函数下的收敛性。

Result: 在聚合周期T内达到标准收敛速率O(1/√T)，首次为Agnostic FedAvg在非均匀随机参与下提供收敛保证。

Conclusion: Agnostic FedAvg在未知参与分布的情况下优于加权聚合变体，具有实际应用价值。

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [66] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 提出了一种名为“warm-start model”的确定性模型，通过提供更好的初始点加速条件生成，显著减少生成过程所需的步骤。


<details>
  <summary>Details</summary>
Motivation: 传统迭代生成模型（如扩散模型和流匹配）生成高保真样本需要数百次函数评估，速度较慢。

Method: 使用条件化的高斯分布N(mu, sigma)作为初始点，而非传统的N(0, I)，并通过条件归一化技巧与标准生成模型兼容。

Result: 在图像修复等任务中，仅需11次函数评估（1次预热，10次生成）即可达到与1000步DDPM基线相当的效果。

Conclusion: 该方法简单高效，可与现有生成模型和采样技术结合，进一步加速生成过程。

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [67] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: 该论文提出了一种构造性小波神经网络（CWNN），通过频率估计和基函数增量机制优化小波基的选择和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统小波神经网络在构建精确小波基和高计算成本方面存在挑战，限制了其应用。

Method: 引入频率估计器和基函数增量机制，优先选择高能量基函数，并定义时间-频率范围以提高计算效率。

Result: 通过四个示例验证了框架的广泛适用性和实用性，包括静态映射估计、离线数据组合、时变映射识别和实时非线性依赖捕捉。

Conclusion: CWNN显著提高了小波神经网络的效率和准确性，代码已开源。

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [68] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: ToxBench是一个大规模AB-FEP数据集，用于机器学习开发，专注于人类雌激素受体α（ERα），包含8,770个复合物结构及其结合自由能。提出的DualBind模型在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在蛋白质-配体结合亲和力预测中数据不足的问题，同时弥补物理方法计算成本高的缺陷。

Method: 引入ToxBench数据集，并开发DualBind模型，采用双损失框架学习结合能函数。

Result: DualBind在基准测试中表现优于现有方法，展示了机器学习以低成本近似AB-FEP的潜力。

Conclusion: ToxBench和DualBind为药物发现提供了高效工具，推动了机器学习在结合亲和力预测中的应用。

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [69] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: TPP-SD是一种新颖的方法，通过将语言模型中的推测解码技术应用于Transformer时间点过程（TPP）采样，显著加速了采样过程。


<details>
  <summary>Details</summary>
Motivation: Transformer TPP模型虽然强大，但其采样速度较慢，限制了实际应用。TPP-SD旨在解决这一问题，通过结合推测解码技术提升采样效率。

Method: TPP-SD利用较小的草稿模型生成多个候选事件，然后由较大的目标模型并行验证，从而加速采样。

Result: 实验表明，TPP-SD在保持与自回归采样相同输出分布的同时，实现了2-6倍的加速。

Conclusion: TPP-SD成功地将Transformer TPP模型的高效采样需求与实际应用中的速度要求结合起来。

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [70] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: 论文展示了物理信息神经网络（PINNs）在模拟湍流中的潜力，无需传统计算网格或训练数据即可直接求解流体方程。


<details>
  <summary>Details</summary>
Motivation: 湍流模拟对计算资源要求极高，传统方法在高流速下难以实现。PINNs提供了一种基于物理方程的连续、无网格解决方案。

Method: 结合自适应网络架构、因果训练和先进优化方法，克服学习混沌动力学的挑战。

Result: PINNs能准确再现湍流的关键统计特性，如能谱、动能、涡度和雷诺应力。

Conclusion: 神经网络方程求解器可处理复杂混沌系统，为超越传统计算限制的湍流建模开辟新途径。

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [71] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Main category: cs.LG

TL;DR: 提出了一种量化并利用不确定性的选择性插补框架，避免不可靠插补，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域时间序列数据常因传感器断开而缺失，现有方法忽视模型不确定性或无法估计。

Method: 引入通用框架，量化不确定性并选择性插补高置信值。

Result: 在多种EHR数据集上实验显示，选择性插补减少误差并提升下游任务（如24小时死亡率预测）。

Conclusion: 将不确定性纳入时间序列插补具有实际价值。

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [72] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Main category: cs.LG

TL;DR: 论文提出了一种新的傅里叶基映射（FBM）方法，通过整合时间-频率特征解决现有傅里叶方法的局限性，并展示了其在多种神经网络模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于傅里叶变换的方法存在起始周期不一致、序列长度不一致等问题，且无法精确解释频率分量或忽略时间信息。

Method: 提出FBM方法，通过傅里叶基展开和映射整合时间-频率特征，并设计了FBM-L、FBM-NL、FBM-NP和FBM-S等模型架构及多种技术（如交互掩码、中心化等）。

Result: 在多种真实数据集上验证了FBM在长短期预测任务中的SOTA性能。

Conclusion: FBM方法有效解决了现有傅里叶方法的局限性，并在时间序列预测中表现出优越性能。

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [73] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 论文提出了一种系统性框架，通过表示引导改进扩散模型，包括两种新策略：多模态联合学习和优化训练课程，实验显示性能提升和训练加速。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型可以通过与预训练模型对齐内部表示来提升生成质量，但缺乏系统性方法。

Method: 提出两种策略：1) 多模态联合学习；2) 优化训练课程。

Result: 在图像、蛋白质序列和分子生成任务中表现优异，训练速度显著提升（如ImageNet 256×256任务中训练速度提升23.3倍）。

Conclusion: 表示引导能有效改进扩散模型，提升生成质量和训练效率。

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [74] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Main category: cs.LG

TL;DR: 论文介绍了一个开源框架，用于开发相关性核函数，特别关注用户定义和核组合，以支持代理建模。通过引入频率感知元素，该框架能有效捕捉复杂机械行为和时频动态特性。


<details>
  <summary>Details</summary>
Motivation: 传统核函数（如指数基方法）的局限性促使开发更广泛的核函数类型（如指数平方正弦和有理二次核）及其导数，以提升建模能力。

Method: 提出并验证了新的核函数方法，包括其导数，应用于Sinc测试案例、CO2浓度和航空客运流量预测，并集成到开源工具SMT 2.0中。

Result: 框架成功扩展了核函数范围，支持灵活组合，为复杂频率敏感领域提供了强大的建模工具。

Conclusion: 该开源框架为工程师和研究人员提供了灵活的工具集，未来可广泛应用于复杂系统的代理建模。

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [75] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: 论文揭示了模型排行榜可能成为大规模分发中毒模型的隐蔽渠道，提出了TrojanClimb框架，展示了其在多种模态中的有效性，并呼吁重新设计排行榜评估机制以应对安全风险。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索中毒模型如何通过模型排行榜大规模分发，填补了现有研究中这一机制的空白。

Method: 提出了TrojanClimb框架，通过在保持排行榜竞争力的同时注入恶意行为，验证了其在文本嵌入、文本生成、文本转语音和文本转图像四种模态中的有效性。

Result: 实验证明，攻击者可以成功在排行榜中取得高排名，同时嵌入任意有害功能（如后门或偏见注入）。

Conclusion: 研究揭示了机器学习生态系统的重大漏洞，呼吁重新设计排行榜评估机制以检测和过滤恶意模型，并警示社区关于使用未经验证模型的风险。

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [76] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Schedule-Free（SF）方法，用于大规模语言模型训练，无需显式衰减阶段或额外内存开销，并通过理论和实证分析验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着模型和数据集的规模迅速增长，传统的预训练策略（如固定计算预算的余弦学习率调度）已无法满足需求，需要更灵活且可扩展的替代方案。

Method: 研究重新审视了Schedule-Free（SF）方法，并通过理论和实证分析揭示其动态特性，提出了一种改进的SF变体，增强了对动量和大批量训练的鲁棒性。

Result: 改进的SF方法在无需显式衰减或额外内存的情况下，有效优化了损失函数，适用于持续扩展的训练任务。

Conclusion: SF方法是一种实用、可扩展且理论支持的语言模型训练方法，解决了现有方法的局限性。

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [77] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: 开发了一种自监督深度学习模型，从多模态信号（EEG、ECG和呼吸信号）中提取有意义模式，用于预测心血管疾病（CVD）风险。


<details>
  <summary>Details</summary>
Motivation: 利用多模态信号（EEG、ECG和呼吸信号）提取特征，提升CVD风险预测的准确性和个性化。

Method: 训练自监督深度学习模型，基于4,398名参与者的数据，通过对比有无CVD结果的个体嵌入生成投影分数，并在1,093名独立参与者中验证。

Result: 投影分数揭示了各模态的临床意义模式，结合Framingham风险评分显著提升预测性能（AUC 0.607-0.965），外部验证结果稳健。

Conclusion: 该框架可直接从PSG数据生成个体化CVD风险评分，有望整合到临床实践中，提升风险评估和个性化护理。

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [78] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Main category: cs.LG

TL;DR: 论文提出NeuTSFlow框架，通过神经算子学习连续函数族之间的转换路径，解决时间序列预测中离散观测与连续过程不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法将时间序列视为离散序列，忽略了其作为连续过程噪声采样的本质。论文提出从函数族的角度重新定义预测任务，以更准确地建模连续过程。

Method: 提出NeuTSFlow框架，利用神经算子实现流匹配，学习历史函数族到未来函数族的转换路径，直接在无限维函数空间中建模。

Result: 实验表明，NeuTSFlow在多种预测任务中表现出更高的准确性和鲁棒性。

Conclusion: 函数族视角有效提升了时间序列预测的准确性，NeuTSFlow框架为连续过程的建模提供了新思路。

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [79] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: 利用人类注视建模改进RLHF，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: RLHF虽能对齐语言模型与人类偏好，但计算成本高。

Method: 提出两种方法：注视感知奖励模型和基于注视的稀疏奖励分布。

Result: 实验显示注视信息能加速收敛并保持性能。

Conclusion: 人类注视是优化RLHF效率的有价值信号。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [80] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 论文提出PRRO方法，通过数据修剪和列重排序提升合成表格数据的监督学习性能，实验显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 合成表格数据在监督学习中性能不佳，主要由于类别不平衡和数据关系未被充分考虑。

Method: 提出PRRO方法，结合数据修剪和列重排序，优化合成数据的信号噪声比和结构对齐。

Result: PRRO在22个数据集上平均提升26.74%性能，最高达871.46%，类别分布相似性提升43%。

Conclusion: PRRO有效提升合成数据质量，促进数据合成与监督学习的无缝集成。

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [81] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: 论文指出当前大型语言模型（LLM）推理系统的评估方法存在缺陷，提出了三个关键维度的常见反模式，并提供了一个检查清单和框架以改进评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统的评估方法存在缺陷，掩盖了真实性能，阻碍科学进步。

Method: 通过系统分析近期系统，识别出基线公平性、评估设置和指标设计三个维度的反模式，并提出改进框架。

Result: 揭示了常见反模式如何导致误导性结论，并通过案例研究验证了框架的实用性。

Conclusion: 建立了一个严谨的评估方法论基础，促进LLM推理系统的真实进步。

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [82] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Main category: cs.LG

TL;DR: 论文提出了一种结合神经ODE、图注意力、小波变换和自适应频率学习的框架，用于能源需求与供应的预测，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 能源供需预测对可持续能源系统优化至关重要，但受可再生能源波动和动态消费模式的影响，预测具有挑战性。

Method: 模型整合了神经ODE、图注意力、多分辨率小波变换和自适应频率学习，采用Runge-Kutta方法求解ODE，并结合图注意力与残差连接。

Result: 在七个数据集上的实验表明，该模型在预测指标上优于现有方法，并能通过SHAP分析增强可解释性。

Conclusion: 该框架能有效捕捉复杂时间依赖，适用于可持续能源应用。

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [83] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出一种分布式预训练方法，通过训练小型结构化子网络减少内存需求，避免节点间激活通信，性能不降。


<details>
  <summary>Details</summary>
Motivation: 解决大规模模型预训练中内存需求高和节点间通信成本高的问题。

Method: 训练小型结构化子网络，避免节点间激活通信，采用两种子网络构建策略。

Result: 随机块丢弃技术表现优于宽度子网络构建，内存使用减少20-40%且性能不降。

Conclusion: 该方法在减少内存需求的同时保持性能，具有潜力。

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [84] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Main category: cs.LG

TL;DR: LoRA-MCL是一种结合多选择学习和低秩适配的训练方案，用于生成多样且合理的句子延续。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型在给定上下文时存在多解问题，需要一种方法处理这种模糊性。

Method: 结合多选择学习（MCL）和Winner-Takes-All（WTA）损失，通过低秩适配（LoRA）高效处理模糊性。

Result: 在视觉和音频字幕任务中，生成结果具有高多样性和相关性。

Conclusion: LoRA-MCL有效解决了语言模型中的多解问题，生成结果多样且合理。

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [85] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: 论文提出了一种递归MDN（R-MDN）层，用于在持续学习中消除混杂变量的影响，提升模型的公平性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 混杂变量会导致虚假相关性和预测偏差，而在持续学习中，如何保持特征表示对混杂变量的不变性仍是一个挑战。

Method: 引入R-MDN层，通过递归最小二乘法动态调整特征分布，适应数据和混杂变量的变化。

Result: 实验表明，R-MDN在静态学习和持续学习中均能减少混杂变量的影响，提升预测的公平性。

Conclusion: R-MDN是一种有效的方法，可用于任何深度学习架构，解决持续学习中的混杂变量问题。

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [86] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Main category: cs.LG

TL;DR: 本文通过最优传输视角研究分布偏移下的共形预测覆盖损失问题，提出了一种估计和缓解覆盖损失的方法。


<details>
  <summary>Details</summary>
Motivation: 共形预测在分布偏移下可能失效，现有方法需预先知道分布偏移类型，限制了其应用。

Method: 利用最优传输理论，分析并估计分布偏移对共形预测覆盖的影响。

Result: 研究表明，可以通过最优传输方法有效估计覆盖损失并采取缓解措施。

Conclusion: 最优传输为分布偏移下的共形预测提供了新的解决方案，无需预先了解偏移类型。

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [87] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: 论文提出了一种结合队列理论和注意力模型的新方法QT-SimAM，用于高精度预测航班延误，并在不同网络中验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 航班延误对航空业造成重大财务和运营问题，需要精确且通用的预测模型以改善乘客体验和减少收入损失。

Method: 结合队列理论和简单注意力模型（QT-SimAM），利用美国运输统计局和EUROCONTROL数据进行验证。

Result: QT-SimAM在美国数据上准确率为0.927，F1分数0.932；在欧盟数据上准确率为0.826，F1分数0.791。

Conclusion: QT-SimAM是一种高效的端到端方法，能高精度预测不同网络的航班延误，有助于减少乘客焦虑和优化运营决策。

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [88] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: 论文提出DejaVu攻击，利用网络延迟导致的多模态传感器时间错位，显著降低自动驾驶感知任务性能，并提出防御方法AION。


<details>
  <summary>Details</summary>
Motivation: 多模态融合（MMF）对自动驾驶感知至关重要，但其依赖严格的时间同步，易受攻击。

Method: 提出DejaVu攻击，分析传感器任务敏感性；设计AION防御，通过跨模态时间一致性监测攻击。

Result: 攻击可显著降低检测性能（如mAP降88.5%）；AION防御AUROC达0.92-0.98，误报率低。

Conclusion: AION是一种针对时间错位攻击的鲁棒且通用的防御方案。

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [89] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: 论文提出了一种基于集合到集合（S2S）的推荐框架S2SRec2，用于解决传统食谱补全方法在预测多个缺失食材时的不足。


<details>
  <summary>Details</summary>
Motivation: 在生鲜电商中，顾客常因缺乏专业知识而无法构建完整的食材组合，传统方法仅预测单一缺失食材，无法满足实际需求。

Method: S2SRec2采用Set Transformer和多任务学习，同时学习从现有食材表示中检索缺失食材和评估篮子的完整性。

Result: 实验表明，S2SRec2显著优于单目标基线方法，提升了购物体验和烹饪创意。

Conclusion: S2SRec2为生鲜电商提供了一种有效的食材推荐方法，解决了传统方法的局限性。

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [90] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: 论文提出了一种结合图提示与权重剪枝的新框架GPAWP，旨在通过减少提示数量提升图神经网络的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）在图任务中表现优异，但仍面临训练时间长、难以捕捉复杂关系等问题。图预训练和提示方法虽有潜力，但现有研究忽视了图提示对模型优化的影响及其稳定性。

Method: 提出GPAWP框架，通过重要性评估函数确定不同粒度的正负权重，并采用分层剪枝去除负提示标签。

Result: 在三个基准数据集上的实验表明，GPAWP显著减少了节点分类任务中的参数数量，同时保持了竞争力。

Conclusion: GPAWP通过优化图提示和权重剪枝，提升了GNNs的性能和效率，为相关研究提供了新思路。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [91] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: POIFormer是一种基于Transformer的新框架，用于准确高效地将用户访问归因于特定POI，解决了GPS不准确和POI空间密度高的问题。


<details>
  <summary>Details</summary>
Motivation: 由于GPS不准确和城市环境中POI的高密度，传统的基于邻近性的方法难以准确归因用户访问。

Method: POIFormer利用Transformer的自注意力机制，联合建模空间邻近性、访问时间、POI语义、用户行为和群体行为模式等多维度信号。

Result: 在真实世界移动数据集上的实验表明，POIFormer在空间噪声和高密度POI环境下显著优于现有基线。

Conclusion: POIFormer是一种实用且高效的POI归因方法，适用于多样化数据源和地理环境。

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [92] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: MolecBioNet是一种新型图框架，整合分子和生物医学知识，用于预测药物相互作用（DDI），优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图方法独立处理药物对，忽略其复杂性和上下文依赖性，且难以整合生物网络和分子结构。

Method: MolecBioNet将药物对建模为统一实体，提取生物知识图谱子图并构建分子层次图，使用图神经网络学习多尺度表示，引入两种池化策略（CASPool和AGIPool）及互信息最小化正则化。

Result: 实验显示MolecBioNet在DDI预测上优于现有方法，消融研究和嵌入可视化验证了其优势。

Conclusion: MolecBioNet通过统一建模和多尺度知识整合，提供了更准确和可解释的DDI预测。

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [93] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: XiChen是一个完全由AI驱动的全球天气预报系统，能够在17秒内完成从数据同化到中期预报的整个流程，其预报准确性可与传统NWP系统媲美。


<details>
  <summary>Details</summary>
Motivation: 传统AI天气预报模型依赖NWP系统准备初始条件，耗时且效率低，XiChen旨在实现完全独立于NWP的高效AI天气预报。

Method: XiChen基于预训练的天气预测基础模型，并通过微调作为观测算子和数据同化模型，结合四维变分知识，实现高效数据同化和预报。

Result: XiChen在17秒内完成全流程，预报准确性与NWP系统相当，预报领先时间超过8.25天。

Conclusion: XiChen展示了完全由AI驱动的天气预报系统的潜力，有望摆脱对NWP系统的依赖。

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [94] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: 论文提出两种轻量级模块（CKM和CSM），实现动态调整补丁大小，提升计算效率和预测稳定性，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 固定补丁大小限制了预算敏感的部署，需动态调整补丁大小以提升效率。

Method: 引入CKM和CSM模块，结合循环补丁大小策略，动态控制补丁大小。

Result: 在2D和3D PDE基准测试中提升了预测精度和运行效率。

Conclusion: 首次实现补丁大小动态调整，为PDE代理任务提供通用计算自适应建模基础。

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [95] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Main category: cs.LG

TL;DR: 论文提出了一种元自编码器（MAE）的概念，用于对多个自编码器进行编码和解码，适用于动态演化的多类别研究。


<details>
  <summary>Details</summary>
Motivation: 研究目标是捕捉动态演化的多类别（如物种）之间的共性和差异，为机器学习和生物学研究提供新工具。

Method: 通过构建元自编码器（MAE），对多个自编码器进行编码和解码，学习其紧凑表示。

Result: 初步定义了MAE的概念，并提供了示例，展示了其在机器学习和生物学中的应用潜力。

Conclusion: MAE为动态演化的多类别研究提供了新方法，未来可进一步探索其理论和应用。

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [96] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Main category: cs.LG

TL;DR: 提出了一种新的公平CCA方法，用于公平表示学习，确保投影特征与敏感属性无关，同时保持高相关性分析性能。


<details>
  <summary>Details</summary>
Motivation: 随着公平性在机器学习中的重要性增加，公平CCA受到关注，但现有方法常忽略对下游分类任务的影响。

Method: 提出了一种新颖的公平CCA方法，确保投影特征独立于敏感属性，以增强公平性而不牺牲准确性。

Result: 在合成数据和ADNI真实数据上验证了方法的有效性，既能保持高相关性分析性能，又能提高分类任务的公平性。

Conclusion: 该方法为神经影像研究中的公平机器学习提供了有效工具，确保无偏分析。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [97] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Main category: cs.LG

TL;DR: 提出了一种噪声条件图网络（NCGNs），通过动态调整架构以适应噪声水平，显著提升了图生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的图生成模型在噪声处理上缺乏动态适应性，限制了表达能力和性能。

Method: 引入NCGNs，提出动态消息传递（DMP），根据噪声水平调整消息传递的范围和分辨率。

Result: DMP在3D点云、时空转录组学和图像等多个领域均优于噪声无关架构。

Conclusion: NCGNs通过动态适应噪声水平，显著提升了图生成模型的表达能力和性能。

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [98] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: 研究了多头潜在注意力（MLA）在预训练中对Transformer内部容量的影响，发现旋转嵌入的应用方式对防止容量瓶颈至关重要。


<details>
  <summary>Details</summary>
Motivation: 探讨MLA压缩键/值内存时如何影响Transformer的预训练容量，特别是旋转嵌入的应用方式。

Method: 使用Marchenko-Pastur诊断分析$W_{Q}W_{K}^\top$矩阵谱，比较标准多头注意力（MHA）、MLA-PreRoPE和MLA-Decoupled三种变体。

Result: 发现容量瓶颈局部出现，旋转嵌入的解耦变体能防止谱分裂，保持广泛谱支持。

Conclusion: 旋转嵌入的应用方式与压缩位置同样重要，解耦设计能有效维持模型容量。

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [99] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Main category: cs.LG

TL;DR: 提出了一种基于缩放定律的系统方法，用于确定目标领域的最佳数据混合比例，避免了传统试错法的高成本。


<details>
  <summary>Details</summary>
Motivation: 传统的数据混合比例选择依赖试错法，在大规模预训练中不实用，需要一种更高效的方法。

Method: 利用缩放定律预测模型在不同数据混合比例下的损失，并通过小规模训练估算参数，外推至更大规模。

Result: 在LLM、NMM和LVM三种大规模预训练场景中验证了缩放定律的普适性和预测能力。

Conclusion: 该方法为确定最优数据混合比例提供了理论依据，显著降低了计算成本。

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [100] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Main category: cs.LG

TL;DR: 论文提出了一种对抗性激活修补方法，用于检测和缓解大型语言模型中的欺骗行为，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在安全对齐后仍可能表现出欺骗行为，需要一种机制来检测和缓解此类问题。

Method: 采用对抗性激活修补框架，通过修补特定层的激活来模拟和量化欺骗行为。

Result: 实验表明，对抗性修补将欺骗性输出从0%提升至23.9%，并验证了多个假设。

Conclusion: 该方法为AI安全提供了新工具，并提出了未来研究方向。

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [101] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Main category: cs.LG

TL;DR: 本文探讨了信息几何在深度学习模型压缩中的应用，强调通过定义低计算子流形和投影实现压缩，并证明迭代奇异值阈值方法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型参数量的增加，资源受限设备需要有效的压缩技术，信息几何提供了一种分析现有压缩方法的新视角。

Method: 采用信息几何分析模型压缩方法，提出通过定义低计算子流形和投影实现压缩，并证明迭代奇异值阈值方法的收敛性。

Result: 研究表明，使用信息散度在预训练模型压缩中至关重要，而在微调场景中，瓶颈模型的可训练性更为重要。

Conclusion: 信息几何视角为模型压缩提供了新思路，通过软秩减少等简单修改可提升固定压缩率下的性能。

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [102] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: DyCAST-Net是一种新型架构，结合扩张时间卷积和动态稀疏注意力机制，用于多变量时间序列中的因果发现，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列中的因果关系理解对金融和营销等领域至关重要，但传统方法难以处理复杂依赖和滞后效应。

Method: DyCAST-Net通过扩张卷积捕捉多尺度时间依赖，动态稀疏注意力机制消除虚假连接，结合统计洗牌测试验证提高鲁棒性。

Result: 在金融和营销数据集上，DyCAST-Net优于TCDF、GCFormer和CausalFormer，能更精确估计因果延迟并减少误发现。

Conclusion: DyCAST-Net通过可解释的注意力热图揭示隐藏因果关系，适用于高维动态环境，具有广泛适用性。

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [103] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Main category: cs.LG

TL;DR: 该论文研究了大型预训练变压器在上下文学习（ICL）中的机制，发现其泛化能力受限于训练数据的分布，且无法实现类似OLS回归的算法。


<details>
  <summary>Details</summary>
Motivation: 探索变压器在ICL中的学习机制，揭示其泛化能力的局限性。

Method: 通过合成线性回归任务和分布外泛化实验，分析变压器的学习行为。

Result: 变压器在分布外数据上泛化能力差，且学习行为与OLS等算法不一致；预训练语料对ICL行为有显著影响。

Conclusion: 变压器的ICL能力受限于训练数据分布，其学习机制与经典算法不同，预训练语料对其行为起关键作用。

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [104] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: 本文探讨了结合卷积神经网络（CNN）和计算热力学模型的方法，用于预测压水堆燃料棒的温度、应力和应变，以支持核电站的预测性维护（PdM）。


<details>
  <summary>Details</summary>
Motivation: 核电站的预测性维护（PdM）能减少因组件故障导致的意外停机时间，因此开发实时监测工具具有重要意义。

Method: 使用CNN架构结合计算热力学模型，基于燃料棒包壳外表面的有限温度测量数据，预测温度、应力和应变分布。数据集通过BISON和MOOSE-THM耦合模拟生成。

Result: CNN训练超过1,000轮未出现过拟合，温度分布预测准确度高，进一步用于热力学模型计算应力和应变分布。

Conclusion: 该方法为核反应堆的预测性维护提供了潜在工具，实现了对燃料棒状态的实时监测。

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [105] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Main category: cs.LG

TL;DR: 论文开发了半监督回归模型，通过家庭传感器数据估计ALS功能衰退率，比较了三种模型范式，发现迁移学习在子量表预测中表现最佳，而自注意力插值在复杂非线性模式中表现最优。


<details>
  <summary>Details</summary>
Motivation: 临床监测ALS功能衰退的间歇性评估可能遗漏关键变化，需通过连续家庭传感器数据改进预测。

Method: 比较了三种模型范式（个体批量学习、队列批量学习与增量微调迁移学习）及三种插值方法（线性斜率、立方多项式、自注意力插值）。

Result: 迁移学习在32次对比中的28次中改善了子量表预测误差（平均RMSE=0.20），自注意力插值在子量表模型中表现最优（平均RMSE=0.19）。

Conclusion: 根据功能域的同质-异质特征匹配学习技术可提高预测准确性，未来可集成自适应模型选择以实现及时干预。

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [106] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Main category: cs.LG

TL;DR: La-Proteina是一种基于部分潜在蛋白质表示的全原子蛋白质设计模型，通过固定维度的残基潜在变量捕获序列和原子细节，解决了侧链长度变化问题，并在多个生成基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成模型难以直接生成全原子结构与氨基酸序列的问题，特别是侧链长度变化带来的挑战。

Method: 采用部分潜在蛋白质表示，显式建模粗粒度主链结构，通过固定维度的残基潜在变量捕获序列和原子细节，利用流匹配建模联合分布。

Result: 在多个生成基准测试中达到最先进性能，包括全原子共设计性、多样性和结构有效性，并能生成长达800个残基的蛋白质。

Conclusion: La-Proteina在原子级蛋白质设计任务中表现出卓越的扩展性和鲁棒性，为原子结构条件设计提供了新工具。

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [107] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Main category: cs.LG

TL;DR: 提出一种基于离散微分算子的新方法，通过范德蒙系数矩阵估计导数并局部表示连续光滑函数，解决了泰勒公式的维数灾难和误差传播问题。


<details>
  <summary>Details</summary>
Motivation: 泰勒公式在函数表示中具有重要意义，但在离散情况下存在维数灾难和导数计算中的误差传播问题。

Method: 利用截断泰勒级数导出的范德蒙系数矩阵，提出一种离散微分算子，同时计算所有低于采样点数的导数阶数，减轻误差传播。

Result: 数学上严格证明了导数估计和函数表示的误差界限，实验表明该方法在导数估计和函数表示上优于有限前向差分、三次样条和线性插值。

Conclusion: 该方法在视觉表示、特征提取、流体力学和跨媒体成像等领域具有广泛适用性。

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [108] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: 论文分析了基于两个非对称价值函数的TD学习方法（QV-learning和AV-learning），发现AV-learning在控制任务中优于Q-learning，并提出新算法RDQ，性能优于Dueling DQN。


<details>
  <summary>Details</summary>
Motivation: 探讨学习两个价值函数（而非单一动作价值函数）是否具有优势及其理论依据。

Method: 分析QV-learning和AV-learning的收敛性和样本效率，并引入新算法Regularized Dueling Q-learning (RDQ)。

Result: AV-learning在控制任务中表现优于Q-learning；RDQ在MinAtar基准测试中显著优于Dueling DQN。

Conclusion: AV-learning方法在控制任务中具有优势，RDQ为实际应用提供了更高效的解决方案。

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [109] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: 论文探讨了在数据不平衡情况下评估XAI方法解释可靠性的重要性，并提出了一种针对少数类的评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的广泛应用和立法要求的增加，解释的鲁棒性成为关键，但数据不平衡（如高风险用例中常见）对AI和XAI方法提出了挑战。

Method: 提出了一种基于少数类的评估方法，包括邻域生成、解释聚合和一致性度量。

Result: 通过一个基于数值特征表格数据集的霜冻事件案例展示了方法的有效性。

Conclusion: 研究为数据不平衡情况下XAI解释的可靠性评估提供了初步见解和方法。

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [110] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Main category: cs.LG

TL;DR: 论文介绍了一个用于社交媒体用户帖子中健康维度分类的数据集，涵盖六个关键方面，并评估了传统和基于Transformer的模型。


<details>
  <summary>Details</summary>
Motivation: 旨在通过社交媒体内容评估用户的多维度健康状态，为个性化健康评估和心理健康早期干预提供支持。

Method: 开发了全面的标注框架，评估了传统机器学习和Transformer模型，使用交叉验证和性能指标（精确率、召回率、F1分数）。

Result: 数据集支持区域特定的健康评估，模型表现通过后解释确保透明度和可解释性。

Conclusion: 该数据集为社交媒体健康评估和心理健康干预提供了新工具，并公开了实验和数据集。

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [111] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Main category: cs.LG

TL;DR: DRAGD和DRAGDP攻击利用联邦学习中梯度差异重建被删除数据，揭示联邦遗忘的隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 联邦遗忘过程中梯度交换可能泄露敏感数据，需研究其隐私风险。

Method: 提出DRAGD攻击，利用遗忘前后梯度差异重建数据；DRAGDP进一步结合公开数据提升重建精度。

Result: 实验表明DRAGD和DRAGDP在数据重建上显著优于现有方法。

Conclusion: 研究揭示了联邦遗忘的隐私漏洞，并提出了提升安全性的解决方案。

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [112] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: MLoRQ是一种结合低秩近似和混合精度量化的新方法，用于在资源受限的边缘设备上部署Transformer网络，通过两阶段优化实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署Transformer网络面临挑战，现有技术如低秩近似和混合精度量化需进一步优化。

Method: MLoRQ采用两阶段优化：层内优化筛选压缩方案，层间优化分配位宽和秩；可选步骤通过自适应舍入减少误差。

Result: MLoRQ在视觉Transformer任务中表现优异，性能提升高达15%。

Conclusion: MLoRQ是一种高效且兼容性强的压缩方法，适用于边缘设备部署。

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [113] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: 论文通过大规模多语言人类研究，发现人类偏好比现有LLMs更多样化，提出负相关采样方法提升对齐效果，并开源了最大的多语言偏好数据集。


<details>
  <summary>Details</summary>
Motivation: 研究如何让LLMs服务具有不同文化、政治等冲突偏好的用户，解决现有方法在捕捉人类偏好多样性上的不足。

Method: 通过多国人类研究比较LLMs与人类偏好差异，提出负相关采样方法，并构建多语言多轮偏好数据集。

Result: 发现人类偏好比LLMs更多样化，负相关采样显著提升对齐效果，开源了最大的多语言偏好数据集。

Conclusion: 负相关采样和开源数据集有助于LLMs更好地服务全球多样化用户。

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [114] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Main category: cs.LG

TL;DR: 研究将Conformal Prediction (CP)与确定性加密数据的监督学习结合，以在不确定性量化和隐私保护机器学习之间架起桥梁。


<details>
  <summary>Details</summary>
Motivation: 旨在解决隐私保护机器学习中缺乏严格不确定性量化的问题。

Method: 使用AES加密的MNIST数据集，测试传统$p$值基和$e$值基的CP方法。

Result: 加密数据上训练的模型仍能提取有意义结构，测试准确率36.88%，$e$值基CP覆盖率达60%以上。

Conclusion: CP在加密数据中具有潜力，但需权衡预测集紧凑性和可靠性。

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [115] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Main category: cs.LG

TL;DR: 研究了在DAG中分布式学习问题，探讨了信息聚合的条件，发现DAG深度是关键参数。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习中信息聚合的问题，即如何在部分特征可见的情况下，通过DAG结构实现全局最优模型的性能。

Method: 通过DAG拓扑排序顺序学习，每个代理利用自身观察的特征和父节点的预测作为额外特征训练模型。

Result: 理论上下界表明DAG深度是关键，信息聚合在足够长的路径上可实现，但某些拓扑结构（如星型）无法实现。

Conclusion: DAG深度是信息聚合的决定因素，实验验证了理论结果。

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [116] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Main category: cs.LG

TL;DR: 本文研究了生成式和判别式LSTM文本分类模型在边缘计算中的量化表现，发现生成式模型对量化位宽和校准数据更敏感，尤其在类别不平衡时性能下降。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中低延迟和高精度的需求促使研究生成式和判别式分类模型在量化后的表现，以优化部署。

Method: 使用Brevitas量化库对生成式和判别式LSTM模型进行后训练量化，评估不同位宽和噪声条件下的性能。

Result: 判别式模型表现稳健，生成式模型对位宽、校准数据和输入噪声敏感，类别不平衡时性能显著下降。

Conclusion: 校准数据对量化效果至关重要，生成式模型在噪声环境下需谨慎部署。

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [117] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: EPT-2是Earth Physics Transformer系列的最新AI模型，显著优于前代EPT-1.5，并在预测能源相关变量上达到新水平。


<details>
  <summary>Details</summary>
Motivation: 提升地球系统预测的准确性和效率，超越现有AI和数值预报系统。

Method: 采用Transformer架构，并引入基于扰动的集合模型EPT-2e进行概率预测。

Result: EPT-2和EPT-2e在预测性能上超越Microsoft Aurora和ECMWF的IFS HRES及ENS，且计算成本更低。

Conclusion: EPT-2系列模型为地球系统预测提供了高效、准确的解决方案，并通过平台开放访问。

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [118] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Main category: cs.LG

TL;DR: 论文探讨了如何利用高分辨率遥感数据和AI工具改进大范围精细生境分类，解决了现有地图在主题和空间分辨率上的不足。


<details>
  <summary>Details</summary>
Motivation: 由于人类活动对生态系统的压力增加，高精度生境地图对保护和恢复至关重要，但现有地图在主题和空间分辨率上存在不足。

Method: 利用欧洲植被档案的植被数据，结合多光谱和合成孔径雷达影像，采用分层生境命名法和集成机器学习方法进行建模。

Result: 分层命名法解决了分类模糊问题，多源数据集成和类不平衡校正提高了分类精度。

Conclusion: 该方法可推广到其他地区，未来研究应关注动态生境的时间建模和下一代遥感数据的应用。

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [119] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: 提出了一种基于边界条件数据学习物理定律的通用物理模拟基础AI模型，无需预先编码方程。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如PINNs和有限差分法）需要显式数学方程，限制了通用性和发现潜力。

Method: 采用草图引导的扩散变换器方法，将模拟视为条件生成问题，利用空间边界条件生成稳态解。

Result: 模型实现了边界到稳态的直接映射，通用性强，SSIM > 0.8，边界精度高。

Conclusion: 该工作实现了从AI加速物理到AI发现物理的范式转变，建立了首个通用物理模拟框架。

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [120] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Main category: cs.LG

TL;DR: 研究探讨非等变CNN通过旋转增强是否能达到等变GNN的性能，并分析模型规模、数据集大小和训练时长的影响。


<details>
  <summary>Details</summary>
Motivation: 等变GNN复杂且难以训练，研究者希望探索更简单的非等变CNN是否能通过学习达到类似效果。

Method: 使用旋转增强训练非等变CNN，并通过损失分解分析预测误差和等变误差。

Result: 研究发现非等变CNN在生成任务中可以学习等变性，并分析了关键因素对性能的影响。

Conclusion: 非等变CNN通过适当训练可以匹配等变模型的性能，为分子生成提供了更简单的替代方案。

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [121] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Main category: cs.LG

TL;DR: 该论文提出了一种基于混合专家（MoE）的新型TFBS预测方法，结合多个预训练的CNN模型，并在分布内外数据上验证其性能。同时，引入ShiftSmooth技术提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 转录因子结合位点（TFBS）预测对理解基因调控至关重要，但现有方法在多样性和泛化性上存在局限。

Method: 采用混合专家（MoE）框架，整合多个预训练CNN模型，并通过ShiftSmooth技术增强可解释性。

Result: MoE模型在分布内外数据上表现优异，ShiftSmooth在解释性上优于传统方法。

Conclusion: 该研究为TFBS预测提供了高效、泛化性强且可解释的解决方案，有望推动基因组生物学的发展。

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [122] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Main category: cs.LG

TL;DR: 提出了一种结合物理监督与时空学习的框架RGPD，用于RUL和SOH估计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RUL和SOH的准确估计对工业应用中的PHM至关重要，需要结合物理约束与时空学习。

Method: 使用GCRNs和GATConv捕捉时空特征，引入SAC模块动态优化表示，并通过Q-learning动态分配物理约束权重。

Result: 在多个工业数据集上，RGPD在RUL和SOH估计中表现优于现有方法，具有强鲁棒性和预测准确性。

Conclusion: RGPD框架通过动态结合物理约束与时空学习，显著提升了工业系统中的RUL和SOH估计性能。

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [123] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkær Olsen. Mads Østergaard,Karl Ulbæk,Søren Føns Nielsen,Rasmus Malik Høegh Lindrup,Bjørn Sand Jensen,Morten Mørup*

Main category: cs.LG

TL;DR: 论文提出了一种支持早期退出的神经网络架构，用于单通道语音分离，并通过概率框架动态调整计算资源，实现高性能和可解释的退出条件。


<details>
  <summary>Details</summary>
Motivation: 解决现有语音分离模型计算和参数固定、无法适应不同设备需求的问题，特别是在嵌入式设备上的应用。

Method: 设计支持早期退出的神经网络架构，结合不确定性感知的概率框架，建模干净语音信号和误差方差，推导基于信噪比的退出条件。

Result: 实验表明，单一早期退出模型在多种计算和参数预算下，性能与当前最优模型相当。

Conclusion: 该框架实现了语音分离网络的动态计算扩展，同时保持高性能和可解释性。

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [124] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Main category: cs.LG

TL;DR: 该论文提出两种加速3D分子构象生成的训练和推理机制，通过SO(3)-Averaged Flow训练目标实现快速训练，并通过重流和蒸馏方法实现高质量少步生成。


<details>
  <summary>Details</summary>
Motivation: 快速准确地生成分子构象对计算化学和药物发现任务至关重要，但现有扩散或流模型需要大量计算资源。

Method: 提出SO(3)-Averaged Flow训练目标以加速训练，并采用重流和蒸馏方法优化推理过程。

Result: SO(3)-Averaged Flow训练模型达到最先进的构象生成质量，重流和蒸馏方法实现高质量少步生成。

Conclusion: 该工作为基于流模型的高效分子构象生成提供了可行路径。

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [125] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Main category: cs.LG

TL;DR: 论文提出两种互补方法加速分类导向的近似机器遗忘（AMU）：Blend（分布匹配数据集压缩）和A-AMU（加速损失函数优化），显著减少计算时间并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前AMU方法在处理保留数据集时计算开销大，且减少训练轮次仍具挑战性。

Method: 1. Blend：通过视觉相似图像合并减少保留集大小，速度快于现有数据集压缩方法。2. A-AMU：通过增强损失函数加速收敛，结合主损失和可微分正则化器。

Result: 实验表明，数据与损失优化的双重方法显著降低端到端遗忘延迟，同时保持模型效用和隐私。

Conclusion: 这是首个通过联合设计专用数据集压缩技术和加速损失函数系统解决遗忘效率的工作。

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [126] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Main category: cs.LG

TL;DR: LinkedIn开发了STAR系统，结合LLM和GNN解决推荐系统中的冷启动、过滤气泡和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 解决职业匹配推荐系统中的冷启动、过滤气泡和偏见等挑战。

Method: 结合LLM（处理文本数据）和GNN（捕捉关系网络），采用自适应采样和版本管理等工业级范式。

Result: 提出了构建嵌入的稳健方法、可扩展的GNN-LLM集成方案，以及实际部署的实用见解。

Conclusion: STAR系统为大规模推荐系统提供了端到端的解决方案，显著提升了推荐性能。

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [127] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Main category: cs.LG

TL;DR: 提出了一种轻量级的图感知联邦学习方法，结合了FedAvg的简单性和图学习的关键思想，有效捕捉空间关系并保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法（如FedAvg）假设客户端独立，忽略了交通预测任务中重要的空间关系，而现有的图学习方法计算开销大。

Method: 采用基本的邻域聚合原则指导参数更新，根据图连接性加权客户端模型，避免训练完整模型。

Result: 在METR-LA和PEMS-BAY数据集上表现优于标准基线和近期图联邦学习方法。

Conclusion: 该方法在保持计算效率的同时，有效捕捉空间关系，为交通预测提供了一种实用解决方案。

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [128] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Main category: cs.LG

TL;DR: 论文研究了神经网络中的压缩计算现象，发现实际训练得到的解决方案与理论构造不同，具有高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络是否能在实践中学习到理论提出的压缩计算电路，以解决Universal-AND问题。

Method: 使用一个玩具模型，限制隐藏维度以迫使模型找到计算高效的电路，即压缩计算。

Result: 训练过程发现了一种简单、全密集的解决方案，比理论构造更高效，且能自然扩展到其他布尔操作。

Conclusion: 研究揭示了神经网络偏好的电路类型和叠加表示的灵活性，对网络电路和可解释性有更广泛的理解。

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [129] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Main category: cs.LG

TL;DR: 提出一种结合动态时间规整（DTW）和神经网络的模型，适应冷启动场景并保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 神经网络在时间序列分类中表现优异，但依赖大量标注数据且缺乏可解释性；DTW在有限数据下有效且可解释，但无法利用大数据。

Method: 提出动态长度缩短算法，将时间序列转换为原型，并将DTW关系转化为等效的循环神经网络。

Result: 在低资源场景中显著优于现有方法，在高资源场景中仍具竞争力。

Conclusion: 该模型兼具DTW的可解释性和神经网络的可训练性，适应性强。

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [130] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成式认知诊断范式，通过生成建模实现无需参数重优化的认知状态推断，显著提升了诊断速度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统认知诊断模型无法对新学习者进行即时诊断且可靠性有限，需要解决这些问题。

Method: 提出了生成式诊断范式，并具体化为G-IRT和G-NCDM两种模型，通过生成过程分离认知状态推断和响应预测。

Result: 实验显示，新方法在真实数据集上表现优异，诊断速度提升100倍，解决了可扩展性和可靠性问题。

Conclusion: 生成式诊断为认知诊断应用开辟了新途径，尤其在智能模型评估和教育系统中具有潜力。

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [131] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: 论文提出了一种名为任务向量估计（TVE）的新型预训练框架，旨在解决关系数据库中任务异构性带来的挑战，通过建模任务感知表示显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 关系数据库在多个领域支撑关键基础设施，但由于任务异构性（如关系模式图、时间依赖性和SQL定义的标签逻辑），设计通用预训练策略仍具挑战性。

Method: TVE框架通过基于模式遍历图的集合聚合构建预测监督信号，显式建模下一窗口关系动态，并结合信息论视角形式化表示任务感知信息。

Result: 在RelBench基准测试中，TVE始终优于传统预训练基线，表明任务异构性和时间结构的编码对关系数据库预测建模至关重要。

Conclusion: 研究支持将任务异构性和时间结构作为关系数据库预训练目标的设计原则，以提升任务感知表示的有效性。

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [132] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 本文提出了一种新的自动提示优化（APO）框架，通过增强反馈机制（包括正负强化）和反馈多样化技术，显著提升了提示优化的效果和效率，并解决了不同LLM间提示迁移的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有APO方法主要关注错误修正，忽略了正确预测中的有价值信息，限制了其效果和效率。同时，LLM的快速迭代和多样性使得提示迁移成为实际挑战。

Method: 提出了基于正负强化的反馈机制和反馈多样化技术，以减少噪声并保留有效提示组件；同时，提出了持续提示优化（CPO）以解决提示迁移问题。

Result: 实验表明，该方法在准确率、收敛速度和计算成本上均优于基线，尤其在提示迁移场景中表现突出。

Conclusion: 该框架显著提升了提示优化的效果和效率，并解决了LLM间提示迁移的挑战。

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [133] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Main category: cs.LG

TL;DR: 论文提出了一种基于任务先验的概率空间框架，用于评估模型在所有可能下游任务上的性能，解决了当前固定评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中的评估方法依赖于固定的下游基准测试，限制了研究的进展。作者希望通过定义任务先验和任务分布，提供一个更灵活的评估框架。

Method: 通过定义任务先验和任务分布，构建一个概率空间，评估模型在所有可能下游任务上的平均性能和方差。

Result: 该框架首次能够回答模型在所有可能任务上的平均性能和方差问题，为SSL研究提供了新的评估标准。

Conclusion: 任务先验框架有望加速SSL研究的进展，并为AI评估提供更全面的视角。

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [134] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: AdaBrain-Bench是一个标准化的大规模基准测试，用于评估非侵入式脑机接口（BCI）中的基础模型，涵盖7种关键应用，并提供多维评估指标和工具。


<details>
  <summary>Details</summary>
Motivation: 非侵入式BCI信号的高噪声和有限任务数据限制了解码能力，当前缺乏全面、实用且可扩展的基准测试来评估公共基础模型的实用性。

Method: 提出AdaBrain-Bench基准测试，整合多样化的BCI解码数据集、任务适应流程和多维评估指标，评估基础模型在跨主体、多主体和少样本场景中的泛化能力。

Result: 通过AdaBrain-Bench评估了一系列公开的基础模型，并提供了在不同场景中选择合适模型的实践建议。

Conclusion: AdaBrain-Bench为促进稳健和通用的神经解码解决方案提供了一个可扩展且持续演进的平台。

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [135] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种名为scSGC的软图聚类方法，用于解决单细胞RNA测序数据中硬图构建的局限性，通过非二元边权重更准确地表征细胞间的连续相似性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的单细胞RNA测序聚类方法（如图神经网络）依赖硬图构建，导致信息丢失和聚类偏差。scSGC旨在解决这些问题。

Method: scSGC包含三个核心组件：基于ZINB的特征自编码器、双通道切割信息软图嵌入模块和基于最优传输的聚类优化模块。

Result: 在十个数据集上的实验表明，scSGC在聚类准确性、细胞类型注释和计算效率上优于13种最先进的聚类模型。

Conclusion: scSGC具有显著潜力，可推动单细胞RNA测序数据分析并深化对细胞异质性的理解。

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [136] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Main category: cs.LG

TL;DR: 论文研究了深度神经网络在过参数化情况下如何实现无限泛化，通过循环神经网络在流式奇偶任务中的学习动态，揭示了算法开发的机制。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络如何在有限训练数据下实现无限泛化的机制，特别是通过流式奇偶任务这一非线性任务。

Method: 使用循环神经网络（RNN）训练流式奇偶任务，分析其学习动态和表现。

Result: 发现RNN在足够训练后会出现完美无限泛化的相变，并通过有效理论揭示了隐式表示合并效应。

Conclusion: 研究揭示了神经网络从有限训练经验中实现无限泛化的一种机制，即通过构建有限自动机完成任务。

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [137] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: DepBERT结合了依赖树和Transformer模型，显著提升了因果关系短语提取的性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法未充分利用依赖树等语言学工具，限制了性能。

Method: 提出DepBERT，将依赖树融入Transformer框架。

Result: 在三个数据集上表现优于现有方法。

Conclusion: DepBERT证明了依赖树在监督模型中的有效性。

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [138] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Main category: cs.LG

TL;DR: 论文提出了一种解释大型语言模型（LLM）内部推理过程的新方法，以核工程领域为例，通过神经元激活模式分析和神经元沉默技术，验证了特定神经元组对任务性能的关键作用。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如核工程）中部署LLM需要理解其内部推理过程，以满足核监管框架的验证要求。

Method: 采用低秩适应技术对通用LLM进行微调，通过神经元激活模式比较和神经元沉默技术分析其行为变化。

Result: 沉默特定神经元组会显著降低任务性能，影响模型生成准确技术信息的能力。

Conclusion: 该方法提升了LLM的透明度，为核级AI验证提供了可行路径。

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [139] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 论文提出MemSinks方法，通过设计隔离记忆内容，解决大语言模型记忆重复序列的隐私和版权问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易记忆重复序列，引发隐私和版权问题，现有后处理方法效果有限。

Method: 提出MemSinks范式，利用序列标识符激活特定记忆神经元，隔离记忆内容。

Result: 在十亿参数和十亿token规模下实现有效隔离和强泛化能力。

Conclusion: MemSinks首次证明在真实数据上同时实现泛化和隔离的可行性。

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [140] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Main category: cs.LG

TL;DR: 论文提出了一种动态调整神经元数量的方法，以解决类别不平衡问题，提升少数类的识别性能。


<details>
  <summary>Details</summary>
Motivation: 受生物学启发，人脑在学习过程中会动态生成和修剪神经元，而传统深度学习模型的神经元数量固定，无法适应类别不平衡的数据集。

Method: 在训练过程中定期增加和移除神经元，动态调整网络容量，同时保持最终网络结构和大小不变。

Result: 在三个数据集和五种模型上的实验表明，该方法优于固定网络，且与其他不平衡处理技术结合效果更佳。

Conclusion: 动态网络设计能有效提升类别不平衡数据的性能，具有生物启发性。

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [141] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: 论文提出Iceberg方法，通过合成数据增强和弱标签生成，提升深度学习模型在硬件设计HLS中的泛化能力，显著提高了建模准确性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的HLS预测模型泛化能力不足，需改进以适应实际应用。

Method: 采用合成数据增强（Iceberg），结合LLM生成程序和弱标签，并集成上下文模型架构进行元学习。

Result: 在六种实际应用中，建模准确率几何平均提升86.4%，离线DSE性能提升2.47倍和1.12倍。

Conclusion: Iceberg通过合成数据和弱标签有效提升了模型的泛化能力，适用于实际硬件设计场景。

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [142] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 提出了一种新的表示学习和分类模型，用于在线招聘中的职位分类，通过嵌入分层行业类别到潜在空间，显著提升了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 在线招聘中职位分类的复杂性增加，传统文本分类方法无法充分利用行业类别的分层结构，需要更先进的模型。

Method: 结合标准职业分类系统（SOC）和内部分层分类法Carotene，将职位和分层行业类别嵌入潜在空间，捕捉图和分层关系。

Result: 在大规模职位发布数据集上的实验表明，该模型显著优于现有方法，提升了分类准确性。

Conclusion: 该研究为招聘行业提供了更准确的职位分类框架，支持更明智的决策。

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [143] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Main category: cs.LG

TL;DR: 论文提出了一种新的潜在空间距离估计方法，通过行和列距离近似潜在空间距离，并引入校正项减少噪声影响，最终提出Radial Neighborhood Estimator（RNE）提升推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统潜在空间具有低秩结构，但如何有效定义和测量潜在空间中的距离以捕捉用户和物品关系是关键挑战。

Method: 通过行和列距离近似潜在空间距离，引入基于经验方差估计的校正项，提出RNE方法，结合重叠和部分重叠用户-物品对构建邻域，并通过局部核回归平滑提升填补精度。

Result: 在模拟和真实数据集上验证，RNE优于现有协同过滤和矩阵分解方法，并缓解冷启动问题。

Conclusion: RNE为潜在空间距离估计提供了结构化方法，显著提升推荐系统性能，同时解决冷启动问题。

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [144] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Main category: cs.LG

TL;DR: 论文探讨了空间回归模型中的归纳偏差问题，提出了改进GNNWR的方法，通过引入CNN、RNN和Transformer的概念，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNNWR模型在捕捉空间非平稳性时受限于固定的距离方案和有限的归纳偏差，需要更灵活的方法。

Method: 通过结合CNN的局部感受野、RNN的序列上下文和Transformer的自注意力机制，改进GNNWR模型。

Result: 在合成数据集上，改进后的GNNWR在捕捉非线性空间关系上优于传统方法，且性能受数据特征影响显著。

Conclusion: 归纳偏差对空间建模至关重要，未来可探索可学习的空间权重函数和混合神经网络架构。

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [145] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.LG

TL;DR: TDCRL是一种结合因果推理的源自由域泛化方法，通过文本驱动生成视觉表示并提取领域不变特征，显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统域泛化方法需要多源域数据，成本高且不切实际；现有源自由域泛化方法受限于领域特异性混杂因素。

Method: TDCRL分两步：1）通过数据增强生成风格词向量，结合类别信息生成文本嵌入模拟视觉表示；2）训练因果干预网络提取领域不变特征。

Result: 在PACS、VLCS、OfficeHome和DomainNet数据集上表现优异，达到最先进水平。

Conclusion: TDCRL通过因果学习机制有效提取领域不变特征，为源自由域泛化提供了新思路。

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [146] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息高斯过程的网格自由框架，用于解决合规性最小化问题，通过多输出神经网络控制设计复杂性，并显著优于传统方法和现有ML方法。


<details>
  <summary>Details</summary>
Motivation: 现有ML方法在解决合规性最小化问题时存在特征边界不清晰、计算成本高且缺乏设计复杂性控制机制的问题。

Method: 采用物理信息高斯过程（GPs）和多输出神经网络（PGCANs）参数化设计和状态变量，通过最小化合规性、总势能和体积分数约束残差来估计参数。

Result: 方法能够生成超分辨率拓扑结构，收敛速度快，合规性和灰度区域分数优于传统数值方法，并能控制精细特征。

Conclusion: 该方法在解决合规性最小化问题上表现出色，优于现有ML方法和传统数值方法。

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [147] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Main category: cs.LG

TL;DR: 研究了图结构对神经网络性能的影响，发现具有紧密连接社区结构的网络学习能力更强。


<details>
  <summary>Details</summary>
Motivation: 探索图结构（如社区结构）对神经网络预测性能的影响，填补现有研究的空白。

Method: 使用随机网络、无标度网络和生物神经网络进行比较分析，研究结构属性对图像分类任务的影响。

Result: 具有紧密连接社区结构的网络表现出更强的学习能力。

Conclusion: 研究为网络科学和机器学习提供了新见解，可能启发更生物化的神经网络设计。

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [148] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Main category: cs.LG

TL;DR: 本研究开发了首个图神经网络（GNN）模型，用于预测亚利桑那州的谷热病发病率，结合环境因素和病例数据，揭示了关键的环境驱动因素。


<details>
  <summary>Details</summary>
Motivation: 谷热病在美国西南部流行地区是重要的公共卫生问题，需要有效的预测模型以指导预防措施。

Method: 利用图神经网络整合病例数据和环境预测因子（如土壤条件、大气变量等），探索变量间的相关性，并引入滞后效应捕捉疾病进展的延迟。

Result: GNN模型成功预测了谷热病趋势，并识别了影响发病率的关键环境因素。

Conclusion: 该模型可为早期预警系统和资源分配提供支持，有助于高风险地区的疾病预防。

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [149] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Main category: cs.LG

TL;DR: 论文探讨了LLMs在单细胞数据分析中的性能驱动因素，并提出了结合scGPT和LLMs的scMPT模型，展示了互补方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞基础模型（如scGPT）无法利用生物学中的文本信息，而LLMs作为替代方案虽表现良好，但其性能驱动因素尚不明确。

Method: 研究分析了LLMs在单细胞数据中的性能驱动因素，并开发了scMPT模型，结合scGPT和LLMs的优势。

Result: scMPT表现优于其组成部分，且在不同数据集上表现更一致。

Conclusion: LLMs可作为单细胞基础模型的补充，提升单细胞分析性能。

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [150] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Main category: cs.LG

TL;DR: 本文提出了一种训练对抗鲁棒决策树的高效流程，包括自动选择扰动大小、训练和验证鲁棒性，并发现验证时间与训练时间无关。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在工业中的快速应用，其可信赖性成为焦点，但鲁棒训练流程的效率和可持续性仍需建立。

Method: 提出三阶段流程：自动选择扰动大小、训练对抗鲁棒模型、验证鲁棒性。扰动大小可通过小模型估计以提高效率。

Result: 验证时间对整体效率至关重要，但与训练时间无关。

Conclusion: 该流程为对抗鲁棒训练提供了高效且可持续的解决方案。

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [151] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [152] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Main category: cs.LG

TL;DR: MTF-Grasp是一种多层级联邦学习方法，针对机器人抓取任务中非独立同分布（non-IID）数据导致的性能下降问题，通过选择数据质量高的机器人训练初始模型，提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在机器人抓取任务中未充分探索，且非独立同分布数据导致性能下降。

Method: 提出MTF-Grasp，选择数据质量高的机器人训练初始模型，再分发给其他机器人。

Result: 在Cornell和Jacquard数据集上，性能提升高达8%。

Conclusion: MTF-Grasp有效解决了非独立同分布数据问题，提升了机器人抓取任务的性能。

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [153] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: FedAcross+是一个高效的联邦学习框架，专注于解决实际工业场景中的客户端适应问题，通过预训练模型和适应性模块减少计算开销，并支持流式数据处理。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘设备上的实际应用面临数据标注成本高、协变量偏移和资源限制下的模型更新困难等挑战。

Method: 框架采用预训练的源模型（包括深度骨干、适应性模块和分类器），在客户端适应时冻结骨干和分类器，仅调整适应性模块以减少计算开销。

Result: 实验证明FedAcross+在低端设备上能有效适应目标域，解决域偏移问题，并支持资源受限环境下的零星模型更新。

Conclusion: FedAcross+为资源受限环境提供了一种实用且高效的联邦学习解决方案，适用于非静态环境。

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [154] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Main category: cs.LG

TL;DR: 本文旨在通过实例和直观可视化，澄清张量网络（TN）分解中TN秩的概念，帮助读者理解其选择和解释。


<details>
  <summary>Details</summary>
Motivation: TN秩在张量分解中至关重要，但其缺乏统一含义和直观解释，常被当作经验调参而非设计参数。本文希望通过领域知识和图形化方法解决这一问题。

Method: 通过实际案例（如CP和Tucker分解）展示领域知识如何指导TN秩选择，并采用图形化方法推广到任意阶张量。

Result: 揭示了TN秩与张量展开矩阵秩的关系，简化了复杂代数操作，支持领域驱动的TN设计。

Conclusion: 本文为读者提供了对TN秩的清晰统一理解，支持其在应用和教育中的选择和解释。

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [155] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Main category: cs.LG

TL;DR: 该论文探讨了使用无监督CNN-LSTM自编码器模型从低级别游戏数据中直接获取潜在表示，以减少对领域专业知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 游戏风格识别能为游戏设计提供见解并实现自适应体验，但现有方法依赖领域知识或抽象。

Method: 采用无监督CNN-LSTM自编码器模型处理MicroRTS中的低级别游戏数据。

Result: 模型在潜在空间中实现了不同游戏代理的有意义分离，减少了对领域专业知识的依赖。

Conclusion: 该方法可用于探索AI玩家的多样化游戏风格，减少领域偏见。

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [156] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 论文提出了T-GRAB基准测试，用于系统评估Temporal Graph Neural Networks（TGNNs）在时间推理上的能力，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前TGNNs是否有效捕捉周期性、因果关系和长程依赖等核心时间模式尚不明确，需要系统评估。

Method: 引入T-GRAB，一组合成的可控任务，用于隔离和测试TGNNs的关键时间推理能力。

Result: 评估了11种方法，发现它们在泛化时间模式上存在根本性不足。

Conclusion: T-GRAB揭示了传统基准测试隐藏的挑战，为开发更强时间推理能力的架构提供了方向。

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [157] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: 论文提出了一种对抗性表示学习方法，通过引入焦点熵来减少信息泄露，同时保持预测能力。


<details>
  <summary>Details</summary>
Motivation: 如何在保持用户隐私的同时学习具有高预测能力的表示。

Method: 采用对抗性表示学习方法，引入焦点熵以减少信息泄露。

Result: 在多个基准测试中展示了可行性，结果表明在适度隐私泄露下具有高目标效用。

Conclusion: 该方法在保护隐私的同时，能够有效维持预测性能。

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [158] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Main category: cs.LG

TL;DR: 通过图变量和统计充分性分析神经网络，证明在无限宽度极限下，层输出对输入具有统计充分性，并扩展到有限宽度网络。


<details>
  <summary>Details</summary>
Motivation: 结合统计充分性、图论表示和深度学习，为神经网络提供新的统计理解。

Method: 将神经网络层解释为基于图的变换，神经元作为输入与锚点间的成对函数，分析条件分布保持的条件。

Result: 在无限宽度极限下，层输出具有统计充分性；有限宽度网络通过区域分离输入分布和适当锚点构造也能实现充分性。

Conclusion: 该框架为神经网络提供了新的统计视角，适用于多种网络结构和激活函数。

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [159] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: KAPI-ELM是一种基于RBF的自适应扩展方法，结合贝叶斯优化和最小二乘优化，解决了PI-ELM在捕捉尖锐梯度时的局限性，并在多个PDE问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: PI-ELM虽然速度快，但其固定输入层限制了捕捉尖锐梯度的能力，因此需要一种更灵活的方法。

Method: 引入轻量级贝叶斯优化框架，优化输入层参数的统计分布，结合最小二乘优化输出层参数。

Result: 在多个PDE基准测试中，KAPI-ELM在正反问题中均达到最优精度，且参数更少。

Conclusion: KAPI-ELM是一种可扩展、可解释且通用的物理信息学习框架，特别适用于刚性PDE问题。

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [160] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: SAFE-T是一种基于生物上下文的条件生成化学语言模型，用于分子设计和评分，无需依赖结构信息或人工评分函数，显著提升药物发现效率。


<details>
  <summary>Details</summary>
Motivation: 现有生成化学语言模型在药物发现中因缺乏可靠奖励信号和输出可解释性而受限。

Method: SAFE-T通过建模片段分子序列在生物提示下的条件概率，支持虚拟筛选、药物-靶标相互作用预测等任务，并通过采样实现目标导向的分子生成。

Result: 在零样本评估中，SAFE-T性能优于或与现有方法相当，且速度更快；片段级归因显示其能捕捉已知结构-活性关系。

Conclusion: SAFE-T证明条件生成化学语言模型可统一评分与生成，加速早期药物发现。

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [161] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Main category: cs.LG

TL;DR: 本文研究了层次k-中值聚类问题的平均敏感性，提出了一种高效算法，并证明了其低敏感性和高质量聚类。


<details>
  <summary>Details</summary>
Motivation: 现代算法应用中，数据集通常庞大且动态，若层次聚类对数据扰动敏感，算法实用性将大幅降低。

Method: 通过测量删除随机数据点时输出的预期变化，分析算法的平均敏感性，并提出高效层次k-中值聚类算法。

Result: 理论证明该算法具有低平均敏感性和高聚类质量，同时验证了单链接聚类和CLNSS变体的高敏感性。

Conclusion: 实验验证了所提算法的鲁棒性和有效性，适用于动态数据集。

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [162] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Main category: cs.LG

TL;DR: Demenba是一种基于状态空间模型的自动痴呆分类框架，通过语音记录分析认知能力下降，优于现有方法21%。


<details>
  <summary>Details</summary>
Motivation: 早期发现痴呆对及时干预和改善患者预后至关重要，传统神经心理学测试依赖人工评分，自动化系统能更高效地完成评估。

Method: 提出Demenba框架，基于状态空间模型，内存和计算随序列长度线性增长，训练数据来自Framingham心脏研究的1000多小时认知评估录音。

Result: Demenba在细粒度痴呆分类中优于现有方法21%，且参数更少；与大型语言模型融合后性能进一步提升。

Conclusion: Demenba为痴呆评估提供了更透明、可扩展的工具，未来可结合大型语言模型进一步优化。

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [163] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Main category: cs.LG

TL;DR: 本文通过比较统计、机器学习和深度学习方法，填补了IMU运动捕捉数据缺失值的系统性评估空白，发现多变量方法在复杂缺失情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 运动捕捉数据缺失问题影响其应用，但缺乏系统性评估方法。

Method: 比较统计、机器学习和深度学习填补方法，引入公开数据集，模拟三种缺失机制。

Result: 多变量方法显著优于单变量方法，如GAIN和迭代填补器在复杂缺失情况下表现最佳。

Conclusion: 为未来研究提供基准，并建议提升运动捕捉数据完整性的实用方法。

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [164] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Main category: cs.LG

TL;DR: 本文研究了ReLU神经网络对Korobov函数的$L_p$和$W^1_p$范数逼近误差，提出了网络宽度和深度相关的超逼近误差界。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络在高维空间中逼近Korobov函数时的表现，突破维度灾难的限制。

Method: 结合稀疏网格有限元和比特提取技术，分析网络逼近误差。

Result: 在$L_p$范数下得到$2m$阶误差界，$W^1_p$范数下得到$2m-2$阶误差界，优于经典结果。

Conclusion: 神经网络的表达能力不受维度灾难影响，逼近效果显著优于传统方法。

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [165] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Main category: cs.LG

TL;DR: 提出了一种在$SO(3)$流形上加速扩散过程的算法，通过数值Picard迭代实现，实验显示速度提升达4.9倍且不影响任务奖励。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的顺序性导致去噪过程耗时，需加速以提升效率。

Method: 采用数值Picard迭代方法，适配$SO(3)$空间。

Result: 算法在现有方法上实现速度提升4.9倍，且任务奖励无显著下降。

Conclusion: 该算法显著减少了生成样本的延迟，适用于解决姿态模糊问题。

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [166] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedFD的特征蒸馏方法，用于解决模型异构联邦学习中的知识偏差问题，通过正交投影对齐特征信息，提升全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖logit蒸馏，无法解决异构模型带来的知识偏差，导致训练不稳定和性能不佳。

Method: 提出基于特征蒸馏的FedFD方法，利用正交投影层对齐客户端模型特征，减少知识偏差。

Result: 实验表明FedFD在性能上优于现有方法。

Conclusion: FedFD通过特征对齐和正交投影，有效解决了异构联邦学习中的知识偏差问题，提升了模型性能。

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [167] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: 论文提出了一种名为Temporal-Aligned Transformer (TAT)的多时间范围预测模型，用于解决电商和实体零售中需求高峰预测的挑战，特别是在促销活动期间。


<details>
  <summary>Details</summary>
Motivation: 准确的长期需求预测对供应链管理至关重要，尤其是在促销活动期间需求高峰难以预测的情况下。

Method: TAT模型结合了已知的上下文变量（如节假日和促销信息），并采用了一种新颖的Temporal Alignment Attention (TAA)机制来学习上下文依赖的对齐。

Result: 在两个大型电商数据集上的实验表明，TAT在需求高峰预测上的准确率提升了30%，同时整体性能与其他先进方法相当。

Conclusion: TAT通过上下文对齐机制显著提升了高峰需求预测的准确性，为供应链管理提供了有力支持。

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [168] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: DeepONet架构在解决一维固结问题中表现出色，特别是改进的Trunknet Fourier特征增强模型（Model 4）优于标准配置，计算效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索DeepONet在岩土工程中的应用潜力，填补现有研究的空白。

Method: 评估四种DeepONet架构，包括标准模型和物理启发式模型，并提出改进的Trunknet Fourier特征增强模型。

Result: Model 4表现最佳，计算速度提升1.5至100倍，适用于快速变化函数。

Conclusion: DeepONet在岩土工程中具有高效、通用的替代建模潜力，推动了科学机器学习在岩土领域的早期整合。

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [169] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Main category: cs.LG

TL;DR: 本文介绍了一种基于云和LLM的共享电动出行平台，集成了移动应用以提供个性化路线推荐，并通过优化模块和RAG框架在不同场景下进行评估。


<details>
  <summary>Details</summary>
Motivation: 随着智能出行和共享电动出行服务的兴起，用户对端到端解决方案的需求增长，推动了云和LLM技术的应用。

Method: 开发了一个云基LLM驱动的共享电动出行平台，结合移动应用提供个性化路线推荐，并通过优化模块和RAG框架进行评估。

Result: 优化模块在不同交通场景下评估了旅行时间和成本；RAG框架在模式级别上对用户和系统操作员查询的准确率分别为0.98和0.81。

Conclusion: 该平台展示了云和LLM技术在共享电动出行领域的潜力，为智能决策和用户交互提供了有效支持。

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [170] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Main category: cs.LG

TL;DR: 论文提出了一种基于语义标签的依赖感知Transformer模型TagBERT，用于电子商务查询重构，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子商务搜索引擎面临查询模糊、词汇不对齐等问题，传统方法未能充分利用语义标签，影响了查询重构的效果。

Method: 将查询重构视为令牌分类任务，设计依赖感知的Transformer模型TagBERT，利用令牌的语义标签学习更优的查询短语嵌入。

Result: 在大规模真实电子商务数据集上，TagBERT在重要令牌分类任务中表现优于BERT、eBERT和序列到序列Transformer模型。

Conclusion: TagBERT通过利用语义标签显著提升了查询重构的准确性，为电子商务搜索提供了更优的解决方案。

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [171] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Main category: cs.LG

TL;DR: 提出了一种针对复杂反应（如环化反应）的机制搜索策略，结合图枚举和机器学习筛选，利用神经网络势能（AIMNet2-rxn）评估反应路径。


<details>
  <summary>Details</summary>
Motivation: 复杂反应（如多键协同变化的自然产物合成关键步骤）增加了机制搜索的难度，需开发更高效的方法。

Method: 结合图枚举和机器学习筛选，使用神经网络势能（AIMNet2-rxn）评估反应路径。

Result: 神经网络势能能准确估算活化能、预测立体选择性，并复现自然产物合成的关键步骤。

Conclusion: 该策略为复杂反应机制搜索提供了高效、经济的解决方案。

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [172] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Main category: cs.LG

TL;DR: 提出了一种用于算子学习中不确定性量化的新框架——随机算子网络（SON），结合了随机神经网络（SNN）和DeepONet的概念。


<details>
  <summary>Details</summary>
Motivation: 解决算子学习中的不确定性量化问题。

Method: 通过将分支网络建模为随机微分方程（SDE），并通过伴随BSDE反向传播，用随机最大原理中的哈密顿梯度替代损失函数的梯度进行SGD更新。

Result: SON能够通过其扩散参数学习算子中的不确定性，并在2D和3D噪声算子复制中表现出有效性。

Conclusion: SON是一种有效的框架，能够量化并学习算子中的不确定性。

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [173] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Main category: cs.LG

TL;DR: 研究探讨了在AI/ML模型中平衡能源效率与性能的挑战，通过知识蒸馏训练紧凑的DeepRX学生模型，实现能源节省。


<details>
  <summary>Details</summary>
Motivation: 解决AI/ML模型在能源效率与性能之间的权衡问题，特别是在DeepRX接收器中。

Method: 使用知识蒸馏（KD）训练紧凑的DeepRX学生模型，比较不同模型大小和超参数，评估FLOPs/Watt和FLOPs/clock。

Result: 蒸馏模型在SINR水平下表现出更低的错误率，验证了KD在能源高效AI解决方案中的有效性。

Conclusion: 知识蒸馏可显著降低能源消耗，同时保持模型性能，为能源高效AI提供了可行方案。

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [174] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: CLA是一种自监督学习方法，用于在线持续学习，通过对齐当前与过去的表征减轻遗忘，加速训练收敛，并在相同计算预算下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线持续学习中，数据以小批量到达且无任务边界，现有自监督学习方法较少，需解决遗忘问题。

Method: 提出Continual Latent Alignment (CLA)，通过对齐当前与过去的表征来减少遗忘。

Result: CLA加速训练收敛，优于现有方法；早期预训练中使用CLA比全i.i.d.预训练效果更好。

Conclusion: CLA是一种有效的在线持续学习自监督策略，兼具高效性和性能优势。

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [175] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Main category: cs.LG

TL;DR: 本文通过构建一系列测试，分析了当前最先进的视觉语言模型（VLMs）在基本视觉任务中的局限性，并探讨了其设计中的潜在不足。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在复杂计算机视觉问题中表现出色，但仍缺乏一些基本的视觉理解能力。本文旨在揭示这些局限性，并推动模型的进一步改进。

Method: 通过比较VLMs的最终表现与视觉编码器、中间视觉语言投影和LLM解码器输出的性能，构建测试以分析其设计缺陷。

Result: 研究发现VLMs存在一些不足，并对其能力、鲁棒性和视觉信息处理方式提出了重要观察。

Conclusion: 本文的发现为改进VLMs提供了指导，并希望推动其进一步发展。

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [176] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Main category: cs.LG

TL;DR: 论文探讨了优化问题中梯度下降的收敛性，特别是通过Polyak-Łojasiewicz不等式（PLI）及其推广条件，分析了连续时间与离散时间LQR问题的收敛差异，并研究了梯度估计误差的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决连续时间LQR问题中收敛速率随初始条件增大而消失的现象，以及梯度估计误差对优化过程的影响。

Method: 方法包括应用PLI及其推广条件，结合输入到状态稳定性（ISS）分析梯度估计误差的影响。

Result: 结果表明，连续时间与离散时间LQR问题的收敛行为存在显著差异，推广的PLI条件有助于理解梯度误差的瞬态和渐近效应。

Conclusion: 结论指出，推广的PLI条件是理解优化问题收敛行为的关键，尤其是在存在梯度误差的情况下。

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [177] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: Target Polish是一种高效的非负矩阵和张量分解框架，通过加权中位数变换实现异常值鲁棒性，同时保持Fast-HALS算法的高效性。


<details>
  <summary>Details</summary>
Motivation: 传统加权NMF方法对异常值鲁棒但收敛慢，需要一种既能抵抗异常值又能高效计算的方法。

Method: 采用加权中位数变换自适应平滑数据，结合Fast-HALS算法的高效加性更新结构。

Result: 在图像数据集上，Target Polish在准确性和计算效率上均优于现有鲁棒NMF方法。

Conclusion: Target Polish在保持高效计算的同时，显著提升了异常值鲁棒性。

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [178] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Saúl Fenollosa*

Main category: cs.LG

TL;DR: 论文研究了弹性权重巩固（EWC）在持续学习中的作用，通过实验验证其减少遗忘的效果，并探讨了其与其他正则化方法的比较。


<details>
  <summary>Details</summary>
Motivation: 持续学习中神经网络容易遗忘旧知识（灾难性遗忘），EWC作为一种基于正则化的方法，旨在解决这一问题。

Method: 在PermutedMNIST和RotatedMNIST基准测试中，比较EWC与L2正则化和无正则化的SGD，分析其平衡知识保留和适应性的能力。

Result: EWC显著减少了遗忘，但略微降低了新任务的学习效率。同时探讨了dropout和超参数的影响。

Conclusion: EWC是神经网络终身学习的可行解决方案，但其效率和泛化能力需进一步优化。

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [179] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Main category: cs.LG

TL;DR: SplitHappens结合FSS和U形SL，提升数据隐私保护，减少计算和通信成本，同时抵御多种攻击。


<details>
  <summary>Details</summary>
Motivation: SL虽能保护客户端数据，但易受攻击，现有FSS方法未能全面解决安全问题。

Method: 扩展FSS与SL结合至U形SL，保留标签隐私，抵御模型反转和标签推断攻击。

Result: 实验表明，该方法降低训练时间和通信成本，同时保持准确性。

Conclusion: U形SL结合FSS提供更高安全性，适用于多种攻击场景。

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [180] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Main category: cs.LG

TL;DR: 论文探讨了人工智能在生物学中的应用，指出缺乏标准化跨领域基准测试的问题，并提出建立统一框架的建议。


<details>
  <summary>Details</summary>
Motivation: 解决生物学中AI模型缺乏标准化基准的问题，以提升模型的稳健性和可信度。

Method: 通过研讨会汇集专家，分析数据异质性、噪声、可重复性等瓶颈，并提出改进建议。

Result: 提出高质量数据管理、标准化工具、全面评估指标等建议，以推动AI驱动的虚拟细胞基准测试。

Conclusion: 标准化基准将促进生物学AI模型的严谨性、可重复性和生物相关性，推动新发现和细胞系统理解。

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [181] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: 研究发现，尽管强化学习（RL）能提升大语言模型（LLMs）的推理能力，但Qwen2.5模型在常见基准测试中的表现可能因数据污染而不可靠。作者提出使用合成数据集RandomCalculation验证RL方法，发现仅准确奖励信号能持续提升性能。


<details>
  <summary>Details</summary>
Motivation: 探讨RL方法在提升LLMs推理能力中的实际效果，并揭示现有基准测试可能因数据污染导致结果不可靠的问题。

Method: 引入合成数据集RandomCalculation，生成无污染的算术问题，用于验证RL方法的有效性。

Result: 在无污染数据集上，仅准确奖励信号能持续提升模型性能，而噪声或错误信号无效。

Conclusion: 建议在无污染的基准测试和多样化模型家族中评估RL方法，以确保结论的可靠性。

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [182] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lécuyer*

Main category: cs.LG

TL;DR: 论文分析了在重尾类别不平衡分布下常见私有学习优化算法的行为，发现DP-GD在低频类别学习中表现不佳，而DP-AdamBC通过消除DP偏差显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究在重尾类别不平衡分布下，私有学习优化算法的表现差异，特别是低频类别的学习效果。

Method: 通过理论模型和实验对比DP-GD与DP-AdamBC在重尾不平衡数据上的表现，后者通过消除DP偏差优化二阶信息估计。

Result: DP-AdamBC在低频类别学习中表现更优，训练准确率提升约8%（控制实验）和5%（真实数据）。

Conclusion: DP-AdamBC能有效缓解重尾类别不平衡带来的问题，提升低频类别的学习效果。

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [183] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Main category: cs.LG

TL;DR: 论文提出Graph World Model（GWM），支持处理非结构化和图结构数据，通过多模态信息实现多样化任务，性能优于领域专用基线。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型（WMs）主要处理非结构化数据，无法利用普遍存在的图结构数据；而图基础模型仅限于图学习任务，无法扩展到多模态数据和跨学科任务。

Method: GWM通过通用消息传递算法聚合结构化信息，支持多模态数据（文本或嵌入空间），并引入动作节点以链接多样化任务。

Result: 在六项跨领域任务中，GWM性能优于或匹配领域专用基线，并展现零样本/少样本能力。

Conclusion: GWM为处理多模态图结构数据提供通用解决方案，支持多样化任务，具有广泛适用性。

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [184] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Main category: cs.LG

TL;DR: 论文提出FusionBench和FusionFactory，通过多级融合框架（查询级、思维级、模型级）优化LLM路由，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有应用多依赖单一LLM，无法充分利用多样化模型的优势，导致性能和成本效率低下。

Method: 提出FusionBench路由基准和FusionFactory融合框架，包括查询级、思维级和模型级融合。

Result: FusionFactory在所有14个基准测试中均优于单一LLM，验证了融合框架的有效性。

Conclusion: 系统化LLM融合能有效利用互补优势，提升整体性能。

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>


### [185] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Main category: cs.LG

TL;DR: 论文提出了一种新的解缠方法，通过拆分嵌套规则的节点来提升神经DNF模型的性能，并在分类任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 神经DNF模型在符号翻译过程中因阈值处理导致性能下降，部分原因是未能解缠网络权重中的学习知识。

Method: 提出解缠方法，将嵌套规则的节点拆分为独立的小节点，以更好地保留模型性能。

Result: 在多种分类任务中，解缠方法提供了紧凑且可解释的逻辑表示，性能接近翻译前的模型。

Conclusion: 解缠方法有效解决了神经DNF模型在符号翻译中的性能下降问题。

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [186] [Physics-informed machine learning: A mathematical framework with applications to time series forecasting](https://arxiv.org/abs/2507.08906)
*Nathan Doumèche*

Main category: stat.ML

TL;DR: 该论文探讨了物理信息机器学习（PIML）的统计特性及其工业应用，包括理论分析和实际案例。


<details>
  <summary>Details</summary>
Motivation: 研究PIML方法的统计特性，并将其应用于实际工业问题，如能源信号预测和时间序列约束设计。

Method: 分析了物理信息神经网络（PINNs）的近似性、一致性、过拟合和收敛性，并将其问题框架化为核方法；开发了高效的GPU实现算法。

Result: 展示了PIML在电动汽车充电占用率和电力需求预测中的应用，并提出了时间序列约束框架。

Conclusion: PIML结合物理先验和机器学习，在理论和实际应用中均表现出潜力。

Abstract: Physics-informed machine learning (PIML) is an emerging framework that
integrates physical knowledge into machine learning models. This physical prior
often takes the form of a partial differential equation (PDE) system that the
regression function must satisfy. In the first part of this dissertation, we
analyze the statistical properties of PIML methods. In particular, we study the
properties of physics-informed neural networks (PINNs) in terms of
approximation, consistency, overfitting, and convergence. We then show how PIML
problems can be framed as kernel methods, making it possible to apply the tools
of kernel ridge regression to better understand their behavior. In addition, we
use this kernel formulation to develop novel physics-informed algorithms and
implement them efficiently on GPUs. The second part explores industrial
applications in forecasting energy signals during atypical periods. We present
results from the Smarter Mobility challenge on electric vehicle charging
occupancy and examine the impact of mobility on electricity demand. Finally, we
introduce a physics-constrained framework for designing and enforcing
constraints in time series, applying it to load forecasting and tourism
forecasting in various countries.

</details>


### [187] [The Bayesian Approach to Continual Learning: An Overview](https://arxiv.org/abs/2507.08922)
*Tameem Adel*

Main category: stat.ML

TL;DR: 该论文综述了贝叶斯持续学习的不同设置，包括任务增量学习和类别增量学习，探讨了其定义、相关领域联系、算法分类及最新进展，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 持续学习能够模拟人类学习方式，解决深度学习在现实问题中的扩展性挑战，而贝叶斯推理为持续学习提供了更新知识而不遗忘旧数据的理论基础。

Method: 通过分类和综述贝叶斯持续学习的算法，分析其与领域适应、迁移学习和元学习的联系，并探讨心理学类比。

Result: 提出了贝叶斯持续学习的分类法，总结了当前最先进的算法，并指出了与心理学领域的潜在联系。

Conclusion: 贝叶斯持续学习具有潜力，但仍面临挑战，未来研究可进一步探索其算法优化和应用扩展。

Abstract: Continual learning is an online paradigm where a learner continually
accumulates knowledge from different tasks encountered over sequential time
steps. Importantly, the learner is required to extend and update its knowledge
without forgetting about the learning experience acquired from the past, and
while avoiding the need to retrain from scratch. Given its sequential nature
and its resemblance to the way humans think, continual learning offers an
opportunity to address several challenges which currently stand in the way of
widening the range of applicability of deep models to further real-world
problems. The continual need to update the learner with data arriving
sequentially strikes inherent congruence between continual learning and
Bayesian inference which provides a principal platform to keep updating the
prior beliefs of a model given new data, without completely forgetting the
knowledge acquired from the old data. This survey inspects different settings
of Bayesian continual learning, namely task-incremental learning and
class-incremental learning. We begin by discussing definitions of continual
learning along with its Bayesian setting, as well as the links with related
fields, such as domain adaptation, transfer learning and meta-learning.
Afterwards, we introduce a taxonomy offering a comprehensive categorization of
algorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we
analyze the state-of-the-art while zooming in on some of the most prominent
Bayesian continual learning algorithms to date. Furthermore, we shed some light
on links between continual learning and developmental psychology, and
correspondingly introduce analogies between both fields. We follow that with a
discussion of current challenges, and finally conclude with potential areas for
future research on Bayesian continual learning.

</details>


### [188] [Fixed-Confidence Multiple Change Point Identification under Bandit Feedback](https://arxiv.org/abs/2507.08994)
*Joseph Lazzaro,Ciara Pike-Burke*

Main category: stat.ML

TL;DR: 论文提出了一种固定置信度的分段常数赌博机问题，用于快速识别函数突变点，并证明了其方法的渐近最优性。


<details>
  <summary>Details</summary>
Motivation: 分段常数函数在多个领域中描述现实现象，需要快速准确地识别其突变点。

Method: 引入固定置信度的分段常数赌博机问题，提出基于Track-and-Stop的简单高效方法。

Result: 理论证明方法在多种情况下渐近最优，实验验证其高效性。

Conclusion: 该方法在突变点识别中表现优异，具有实际应用潜力。

Abstract: Piecewise constant functions describe a variety of real-world phenomena in
domains ranging from chemistry to manufacturing. In practice, it is often
required to confidently identify the locations of the abrupt changes in these
functions as quickly as possible. For this, we introduce a fixed-confidence
piecewise constant bandit problem. Here, we sequentially query points in the
domain and receive noisy evaluations of the function under bandit feedback. We
provide instance-dependent lower bounds for the complexity of change point
identification in this problem. These lower bounds illustrate that an optimal
method should focus its sampling efforts adjacent to each of the change points,
and the number of samples around each change point should be inversely
proportional to the magnitude of the change. Building on this, we devise a
simple and computationally efficient variant of Track-and-Stop and prove that
it is asymptotically optimal in many regimes. We support our theoretical
findings with experimental results in synthetic environments demonstrating the
efficiency of our method.

</details>


### [189] [Optimal High-probability Convergence of Nonlinear SGD under Heavy-tailed Noise via Symmetrization](https://arxiv.org/abs/2507.09093)
*Aleksandar Armacki,Dragana Bajovic,Dusan Jakovetic,Soummya Kar*

Main category: stat.ML

TL;DR: 研究了在非凸优化和重尾噪声下SGD类方法的高概率收敛性，提出非线性框架和对称化梯度估计器，改进了收敛速率和适用性。


<details>
  <summary>Details</summary>
Motivation: 解决重尾噪声和非对称噪声对SGD类方法收敛性的影响，提出更通用的非线性框架和对称化方法。

Method: 提出非线性SGD（N-SGD）和两种对称化梯度估计器（SGE和MSGE），结合非线性框架处理噪声。

Result: N-SGD、N-SGE和N-MSGE在非对称噪声下均达到$\widetilde{\mathcal{O}}(t^{-1/2})$收敛速率，且具有指数衰减尾部。

Conclusion: 通过对称化方法和非线性框架，显著扩展了SGD类方法的适用性，改进了收敛速率和复杂度。

Abstract: We study convergence in high-probability of SGD-type methods in non-convex
optimization and the presence of heavy-tailed noise. To combat the heavy-tailed
noise, a general black-box nonlinear framework is considered, subsuming
nonlinearities like sign, clipping, normalization and their smooth
counterparts. Our first result shows that nonlinear SGD (N-SGD) achieves the
rate $\widetilde{\mathcal{O}}(t^{-1/2})$, for any noise with unbounded moments
and a symmetric probability density function (PDF). Crucially, N-SGD has
exponentially decaying tails, matching the performance of linear SGD under
light-tailed noise. To handle non-symmetric noise, we propose two novel
estimators, based on the idea of noise symmetrization. The first, dubbed
Symmetrized Gradient Estimator (SGE), assumes a noiseless gradient at any
reference point is available at the start of training, while the second, dubbed
Mini-batch SGE (MSGE), uses mini-batches to estimate the noiseless gradient.
Combined with the nonlinear framework, we get N-SGE and N-MSGE methods,
respectively, both achieving the same convergence rate and exponentially
decaying tails as N-SGD, while allowing for non-symmetric noise with unbounded
moments and PDF satisfying a mild technical condition, with N-MSGE additionally
requiring bounded noise moment of order $p \in (1,2]$. Compared to works
assuming noise with bounded $p$-th moment, our results: 1) are based on a novel
symmetrization approach; 2) provide a unified framework and relaxed moment
conditions; 3) imply optimal oracle complexity of N-SGD and N-SGE, strictly
better than existing works when $p < 2$, while the complexity of N-MSGE is
close to existing works. Compared to works assuming symmetric noise with
unbounded moments, we: 1) provide a sharper analysis and improved rates; 2)
facilitate state-dependent symmetric noise; 3) extend the strong guarantees to
non-symmetric noise.

</details>


### [190] [CoVAE: Consistency Training of Variational Autoencoders](https://arxiv.org/abs/2507.09103)
*Gianluigi Silvestri,Luca Ambrogioni*

Main category: stat.ML

TL;DR: CoVAE是一种单阶段生成自编码框架，通过一致性训练VAE架构，避免了传统两阶段方法的计算开销和采样时间增加，能一步生成高质量样本。


<details>
  <summary>Details</summary>
Motivation: 挑战传统两阶段生成方法的计算开销和采样效率问题，提出更高效的单阶段框架。

Method: 采用一致性模型技术训练VAE，编码器学习渐进式潜在表示，解码器使用一致性损失和变分正则化。

Result: CoVAE在一步或少量步骤内生成高质量样本，性能优于传统VAE和其他单阶段方法。

Conclusion: CoVAE为自编码和扩散式生成建模提供了统一框架，是高效一步生成的有力方案。

Abstract: Current state-of-the-art generative approaches frequently rely on a two-stage
training procedure, where an autoencoder (often a VAE) first performs
dimensionality reduction, followed by training a generative model on the
learned latent space. While effective, this introduces computational overhead
and increased sampling times. We challenge this paradigm by proposing
Consistency Training of Variational AutoEncoders (CoVAE), a novel single-stage
generative autoencoding framework that adopts techniques from consistency
models to train a VAE architecture. The CoVAE encoder learns a progressive
series of latent representations with increasing encoding noise levels,
mirroring the forward processes of diffusion and flow matching models. This
sequence of representations is regulated by a time dependent $\beta$ parameter
that scales the KL loss. The decoder is trained using a consistency loss with
variational regularization, which reduces to a conventional VAE loss at the
earliest latent time. We show that CoVAE can generate high-quality samples in
one or few steps without the use of a learned prior, significantly
outperforming equivalent VAEs and other single-stage VAEs methods. Our approach
provides a unified framework for autoencoding and diffusion-style generative
modeling and provides a viable route for one-step generative high-performance
autoencoding. Our code is publicly available at
https://github.com/gisilvs/covae.

</details>


### [191] [A Generalization Theory for Zero-Shot Prediction](https://arxiv.org/abs/2507.09128)
*Ronak Mehta,Zaid Harchaoui*

Main category: stat.ML

TL;DR: 论文提出了一种理论框架，用于理解基于预训练基础模型的零样本预测方法，并分析了其泛化能力的关键条件独立性关系。


<details>
  <summary>Details</summary>
Motivation: 研究零样本预测的理论基础，以更好地理解预训练模型在无标注数据下游任务中的泛化能力。

Method: 通过识别零样本预测的目标量及其关键条件独立性关系，构建理论框架。

Result: 明确了零样本预测的学习目标及其泛化能力的条件独立性机制。

Conclusion: 该理论框架为预训练模型在零样本预测中的应用提供了理论基础和指导。

Abstract: A modern paradigm for generalization in machine learning and AI consists of
pre-training a task-agnostic foundation model, generally obtained using
self-supervised and multimodal contrastive learning. The resulting
representations can be used for prediction on a downstream task for which no
labeled data is available. We present a theoretical framework to better
understand this approach, called zero-shot prediction. We identify the target
quantities that zero-shot prediction aims to learn, or learns in passing, and
the key conditional independence relationships that enable its generalization
ability.

</details>


### [192] [A Randomized Algorithm for Sparse PCA based on the Basic SDP Relaxation](https://arxiv.org/abs/2507.09148)
*Alberto Del Pia,Dekun Zhou*

Main category: stat.ML

TL;DR: 本文提出了一种基于SDP松弛的随机近似算法，用于解决NP难的稀疏主成分分析问题，并在理论和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 稀疏主成分分析（SPCA）是一种重要的降维技术，但其NP难特性使得高效求解具有挑战性。本文旨在提出一种高效的近似算法。

Method: 基于基本SDP松弛的随机近似算法，通过多次调用实现高概率的近似比，并在特定技术假设下进一步优化。

Result: 算法在稀疏常数和特征数对数范围内具有近似比，且在低秩或指数衰减特征值的SDP解下满足技术假设。数值实验验证了算法的有效性。

Conclusion: 该算法在理论和实际应用中均表现出色，为SPCA提供了一种高效的近似解决方案。

Abstract: Sparse Principal Component Analysis (SPCA) is a fundamental technique for
dimensionality reduction, and is NP-hard. In this paper, we introduce a
randomized approximation algorithm for SPCA, which is based on the basic SDP
relaxation. Our algorithm has an approximation ratio of at most the sparsity
constant with high probability, if called enough times. Under a technical
assumption, which is consistently satisfied in our numerical tests, the average
approximation ratio is also bounded by $\mathcal{O}(\log{d})$, where $d$ is the
number of features. We show that this technical assumption is satisfied if the
SDP solution is low-rank, or has exponentially decaying eigenvalues. We then
present a broad class of instances for which this technical assumption holds.
We also demonstrate that in a covariance model, which generalizes the spiked
Wishart model, our proposed algorithm achieves a near-optimal approximation
ratio. We demonstrate the efficacy of our algorithm through numerical results
on real-world datasets.

</details>


### [193] [Uncovering symmetric and asymmetric species associations from community and environmental data](https://arxiv.org/abs/2507.09317)
*Sara Si-Moussi,Esther Galbrun,Mickael Hedde,Giovanni Poggiato,Matthias Rohr,Wilfried Thuiller*

Main category: stat.ML

TL;DR: 提出一种机器学习框架，用于从物种群落和环境数据中提取双向关联，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 生物相互作用影响群落组装和物种空间共变，但现有模型多假设对称关系，无法捕捉不对称相互作用。

Method: 框架通过物种特定潜在嵌入建模双向关联，并结合多物种条件生成模型拟合环境驱动与生物关联的不同模式。

Result: 模拟和实证数据验证了框架能恢复已知对称和不对称关联，并优于现有模型。

Conclusion: 该框架直观、模块化，适用于多种分类群。

Abstract: There is no much doubt that biotic interactions shape community assembly and
ultimately the spatial co-variations between species. There is a hope that the
signal of these biotic interactions can be observed and retrieved by
investigating the spatial associations between species while accounting for the
direct effects of the environment. By definition, biotic interactions can be
both symmetric and asymmetric. Yet, most models that attempt to retrieve
species associations from co-occurrence or co-abundance data internally assume
symmetric relationships between species. Here, we propose and validate a
machine-learning framework able to retrieve bidirectional associations by
analyzing species community and environmental data.
  Our framework (1) models pairwise species associations as directed influences
from a source to a target species, parameterized with two species-specific
latent embeddings: the effect of the source species on the community, and the
response of the target species to the community; and (2) jointly fits these
associations within a multi-species conditional generative model with different
modes of interactions between environmental drivers and biotic associations.
Using both simulated and empirical data, we demonstrate the ability of our
framework to recover known asymmetric and symmetric associations and highlight
the properties of the learned association networks. By comparing our approach
to other existing models such as joint species distribution models and
probabilistic graphical models, we show its superior capacity at retrieving
symmetric and asymmetric interactions. The framework is intuitive, modular and
broadly applicable across various taxonomic groups.

</details>


### [194] [An Algorithm for Identifying Interpretable Subgroups With Elevated Treatment Effects](https://arxiv.org/abs/2507.09494)
*Albert Chiu*

Main category: stat.ML

TL;DR: 提出一种算法，用于识别具有显著治疗效果的易解释子群，通过规则集形式表达，平衡子群规模与效果大小。


<details>
  <summary>Details</summary>
Motivation: 现有CATE估计方法常产生高维且难以解释的结果，需补充方法以提取关键信息辅助决策。

Method: 基于规则集的子群识别算法，通过目标函数权衡子群规模与效果大小，生成Pareto最优规则集。

Result: 通过模拟和实证验证方法实用性，展示其局限性与有效性。

Conclusion: 该方法为CATE估计提供易解释的子群分析工具，适用于决策支持和科学研究。

Abstract: We introduce an algorithm for identifying interpretable subgroups with
elevated treatment effects, given an estimate of individual or conditional
average treatment effects (CATE). Subgroups are characterized by ``rule sets''
-- easy-to-understand statements of the form (Condition A AND Condition B) OR
(Condition C) -- which can capture high-order interactions while retaining
interpretability. Our method complements existing approaches for estimating the
CATE, which often produce high dimensional and uninterpretable results, by
summarizing and extracting critical information from fitted models to aid
decision making, policy implementation, and scientific understanding. We
propose an objective function that trades-off subgroup size and effect size,
and varying the hyperparameter that controls this trade-off results in a
``frontier'' of Pareto optimal rule sets, none of which dominates the others
across all criteria. Valid inference is achievable through sample splitting. We
demonstrate the utility and limitations of our method using simulated and
empirical examples.

</details>


### [195] [Signed Graph Learning: Algorithms and Theory](https://arxiv.org/abs/2507.09717)
*Abdullah Karaaslanli,Bisakh Banerjee,Tapabrata Maiti,Selin Aviyente*

Main category: stat.ML

TL;DR: 本文提出了一种从平滑有符号图信号中学习有符号图结构的方法，使用净拉普拉斯矩阵作为图移位算子，并通过非凸优化问题求解。


<details>
  <summary>Details</summary>
Motivation: 现实数据常以图结构表示，但现有研究多关注无符号图，而许多系统需用有符号图描述正负交互。本文旨在填补这一空白。

Method: 利用净拉普拉斯矩阵定义平滑信号，通过非凸优化最小化信号总变差，采用ADMM算法求解，并引入快速算法降低复杂度。

Result: 提供了算法收敛性证明和估计误差界，实验验证了方法在模拟数据和基因调控网络推断中的有效性。

Conclusion: 该方法能有效学习有符号图结构，优于现有方法，适用于复杂系统建模。

Abstract: Real-world data is often represented through the relationships between data
samples, forming a graph structure. In many applications, it is necessary to
learn this graph structure from the observed data. Current graph learning
research has primarily focused on unsigned graphs, which consist only of
positive edges. However, many biological and social systems are better
described by signed graphs that account for both positive and negative
interactions, capturing similarity and dissimilarity between samples. In this
paper, we develop a method for learning signed graphs from a set of smooth
signed graph signals. Specifically, we employ the net Laplacian as a graph
shift operator (GSO) to define smooth signed graph signals as the outputs of a
low-pass signed graph filter defined by the net Laplacian. The signed graph is
then learned by formulating a non-convex optimization problem where the total
variation of the observed signals is minimized with respect to the net
Laplacian. The proposed problem is solved using alternating direction method of
multipliers (ADMM) and a fast algorithm reducing the per-ADMM iteration
complexity from quadratic to linear in the number of nodes is introduced.
Furthermore, theoretical proofs of convergence for the algorithm and a bound on
the estimation error of the learned net Laplacian as a function of sample size,
number of nodes, and graph topology are provided. Finally, the proposed method
is evaluated on simulated data and gene regulatory network inference problem
and compared to existing signed graph learning methods.

</details>


### [196] [Discovering Governing Equations in the Presence of Uncertainty](https://arxiv.org/abs/2507.09740)
*Ridwan Olabiyi,Han Hu,Ashif Iquebal*

Main category: stat.ML

TL;DR: 提出了一种随机逆物理发现（SIP）框架，通过处理系统变异性和测量噪声，从噪声和有限数据中准确发现动力学系统的控制方程。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设固定系数确定性模型，难以应对现实系统中的输入变异性和噪声观测数据，因此需要一种新方法。

Method: SIP框架将未知系数视为随机变量，通过最小化后验样本与经验数据分布之间的Kullback-Leibler散度推断其分布。

Result: 在多个基准测试中，SIP比SINDy及其贝叶斯变体平均降低了82%的系数均方根误差，并能提供量化不确定性的可解释模型。

Conclusion: SIP为噪声、变异性和数据有限的环境下提供了一种鲁棒且数据高效的物理发现方法。

Abstract: In the study of complex dynamical systems, understanding and accurately
modeling the underlying physical processes is crucial for predicting system
behavior and designing effective interventions. Yet real-world systems exhibit
pronounced input (or system) variability and are observed through noisy,
limited data conditions that confound traditional discovery methods that assume
fixed-coefficient deterministic models. In this work, we theorize that
accounting for system variability together with measurement noise is the key to
consistently discover the governing equations underlying dynamical systems. As
such, we introduce a stochastic inverse physics-discovery (SIP) framework that
treats the unknown coefficients as random variables and infers their posterior
distribution by minimizing the Kullback-Leibler divergence between the
push-forward of the posterior samples and the empirical data distribution.
Benchmarks on four canonical problems -- the Lotka-Volterra predator-prey
system (multi- and single-trajectory), the historical Hudson Bay lynx-hare
data, the chaotic Lorenz attractor, and fluid infiltration in porous media
using low- and high-viscosity liquids -- show that SIP consistently identifies
the correct equations and lowers coefficient root-mean-square error by an
average of 82\% relative to the Sparse Identification of Nonlinear Dynamics
(SINDy) approach and its Bayesian variant. The resulting posterior
distributions yield 95\% credible intervals that closely track the observed
trajectories, providing interpretable models with quantified uncertainty. SIP
thus provides a robust, data-efficient approach for consistent physics
discovery in noisy, variable, and data-limited settings.

</details>


### [197] [Regret Analysis of Posterior Sampling-Based Expected Improvement for Bayesian Optimization](https://arxiv.org/abs/2507.09828)
*Shion Takeno,Yu Inatsu,Masayuki Karasuyama,Ichiro Takeuchi*

Main category: stat.ML

TL;DR: 论文分析了基于后验采样的随机期望改进（EI）方法，证明了其在高斯过程假设下能实现次线性贝叶斯累积遗憾界。


<details>
  <summary>Details</summary>
Motivation: 尽管期望改进（EI）在应用中表现优异，但其理论分析相对不足，本文旨在填补这一空白。

Method: 提出了一种随机化的EI变体，通过从后验样本路径的最大值评估EI。

Result: 证明了该方法在高斯过程假设下能实现次线性贝叶斯累积遗憾界。

Conclusion: 数值实验验证了所提方法的有效性。

Abstract: Bayesian optimization is a powerful tool for optimizing an
expensive-to-evaluate black-box function. In particular, the effectiveness of
expected improvement (EI) has been demonstrated in a wide range of
applications. However, theoretical analyses of EI are limited compared with
other theoretically established algorithms. This paper analyzes a randomized
variant of EI, which evaluates the EI from the maximum of the posterior sample
path. We show that this posterior sampling-based random EI achieves the
sublinear Bayesian cumulative regret bounds under the assumption that the
black-box function follows a Gaussian process. Finally, we demonstrate the
effectiveness of the proposed method through numerical experiments.

</details>


### [198] [Simulating Biases for Interpretable Fairness in Offline and Online Classifiers](https://arxiv.org/abs/2507.10154)
*Ricardo Inácio,Zafeiris Kokkinogenis,Vitor Cerqueira,Carlos Soares*

Main category: stat.ML

TL;DR: 该论文提出了一种通过合成数据集生成可控偏见的框架，并利用新型可解释性技术评估偏见缓解方法的效果。


<details>
  <summary>Details</summary>
Motivation: 预测模型常因训练数据中的偏见而强化不公平决策，因此需要方法评估和缓解这种偏见。

Method: 开发了一个基于代理的模型（ABM）生成合成数据集，模拟贷款申请中的系统性偏见，并应用分类器预测贷款结果。同时提出了一种基于二阶Shapley值的可解释性技术。

Result: 实验表明，偏见数据会导致不公平预测，而提出的框架和技术能有效评估和缓解偏见。

Conclusion: 该研究为偏见数据生成和缓解提供了实用框架，并通过可解释性技术揭示了缓解方法对分类器行为的影响。

Abstract: Predictive models often reinforce biases which were originally embedded in
their training data, through skewed decisions. In such cases, mitigation
methods are critical to ensure that, regardless of the prevailing disparities,
model outcomes are adjusted to be fair. To assess this, datasets could be
systematically generated with specific biases, to train machine learning
classifiers. Then, predictive outcomes could aid in the understanding of this
bias embedding process. Hence, an agent-based model (ABM), depicting a loan
application process that represents various systemic biases across two
demographic groups, was developed to produce synthetic datasets. Then, by
applying classifiers trained on them to predict loan outcomes, we can assess
how biased data leads to unfairness. This highlights a main contribution of
this work: a framework for synthetic dataset generation with controllable bias
injection. We also contribute with a novel explainability technique, which
shows how mitigations affect the way classifiers leverage data features, via
second-order Shapley values. In experiments, both offline and online learning
approaches are employed. Mitigations are applied at different stages of the
modelling pipeline, such as during pre-processing and in-processing.

</details>


### [199] [MF-GLaM: A multifidelity stochastic emulator using generalized lambda models](https://arxiv.org/abs/2507.10303)
*K. Giannoukou,X. Zhu,S. Marelli,B. Sudret*

Main category: stat.ML

TL;DR: 提出了一种多保真度广义λ模型（MF-GLaM），用于高效模拟高保真度随机模拟器的条件响应分布，利用低保真度模拟器数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 随机模拟器的条件概率分布模拟具有挑战性，传统确定性代理模型无法胜任，且高保真度模拟器数据获取成本高。低保真度模拟器数据可用于增强信息。

Method: 基于广义λ模型（GLaM），构建多保真度广义λ模型（MF-GLaM），利用低保真度模拟器数据高效模拟高保真度模拟器的条件响应分布。

Result: 通过合成案例和实际地震应用验证，MF-GLaM在相同成本下精度更高，或在显著降低成本下性能相当。

Conclusion: MF-GLaM为随机模拟器的条件响应分布模拟提供了一种高效、非侵入性的多保真度方法。

Abstract: Stochastic simulators exhibit intrinsic stochasticity due to unobservable,
uncontrollable, or unmodeled input variables, resulting in random outputs even
at fixed input conditions. Such simulators are common across various scientific
disciplines; however, emulating their entire conditional probability
distribution is challenging, as it is a task traditional deterministic
surrogate modeling techniques are not designed for. Additionally, accurately
characterizing the response distribution can require prohibitively large
datasets, especially for computationally expensive high-fidelity (HF)
simulators. When lower-fidelity (LF) stochastic simulators are available, they
can enhance limited HF information within a multifidelity surrogate modeling
(MFSM) framework. While MFSM techniques are well-established for deterministic
settings, constructing multifidelity emulators to predict the full conditional
response distribution of stochastic simulators remains a challenge. In this
paper, we propose multifidelity generalized lambda models (MF-GLaMs) to
efficiently emulate the conditional response distribution of HF stochastic
simulators by exploiting data from LF stochastic simulators. Our approach
builds upon the generalized lambda model (GLaM), which represents the
conditional distribution at each input by a flexible, four-parameter
generalized lambda distribution. MF-GLaMs are non-intrusive, requiring no
access to the internal stochasticity of the simulators nor multiple
replications of the same input values. We demonstrate the efficacy of MF-GLaM
through synthetic examples of increasing complexity and a realistic earthquake
application. Results show that MF-GLaMs can achieve improved accuracy at the
same cost as single-fidelity GLaMs, or comparable performance at significantly
reduced cost.

</details>


### [200] [Information Must Flow: Recursive Bootstrapping for Information Bottleneck in Optimal Transport](https://arxiv.org/abs/2507.10443)
*Xin Li*

Main category: stat.ML

TL;DR: 论文提出了一种统一的认知框架CCUP，通过信息在高低熵之间的流动建模认知，解决了信息瓶颈问题，并证明了其收敛性和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在建立一个统一的认知框架，解释信息如何在高低熵之间流动，并解决信息瓶颈问题。

Method: 采用Rao-Blackwellized变分熵最小化方法，结合双向交互循环（上下行信息流）实现信息优化。

Result: 证明了Delta收敛定理，展示了递归熵最小化在潜在空间中的稳定性，并扩展了语言作为符号传输系统的观点。

Conclusion: CCUP为个体认知和集体智能中的信息流动提供了基础原则，递归推断是心智适应和扩展的结构性通道。

Abstract: We present the Context-Content Uncertainty Principle (CCUP), a unified
framework that models cognition as the directed flow of information between
high-entropy context and low-entropy content. Inference emerges as a cycle of
bidirectional interactions, bottom-up contextual disambiguation paired with
top-down content reconstruction, which resolves the Information Bottleneck in
Optimal Transport (iBOT). Implemented via Rao-Blackwellized variational entropy
minimization, CCUP steers representations toward minimal joint uncertainty
while preserving inferential directionality. Local cycle completion underpins
temporal bootstrapping, chaining simulations to refine memory, and spatial
bootstrapping, enabling compositional hierarchical inference. We prove a Delta
Convergence Theorem showing that recursive entropy minimization yields
delta-like attractors in latent space, stabilizing perceptual schemas and motor
plans. Temporal bootstrapping through perception-action loops and sleep-wake
consolidation further transforms episodic traces into semantic knowledge.
Extending CCUP, each hierarchical level performs delta-seeded inference:
low-entropy content seeds diffuse outward along goal-constrained paths shaped
by top-down priors and external context, confining inference to task-relevant
manifolds and circumventing the curse of dimensionality. Building on this, we
propose that language emerges as a symbolic transport system, externalizing
latent content to synchronize inference cycles across individuals. Together,
these results establish iBOT as a foundational principle of information flow in
both individual cognition and collective intelligence, positioning recursive
inference as the structured conduit through which minds adapt, align, and
extend.

</details>
