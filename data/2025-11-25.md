<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 25]
- [cs.LG](#cs.LG) [Total: 215]
- [stat.ML](#stat.ML) [Total: 20]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis from a CLIP-Aligned Encoder](https://arxiv.org/abs/2511.17547)
*Jeyoung Lee,Hochul Kang*

Main category: eess.SP

TL;DR: SYNAPSE是一个两阶段框架，通过CLIP对齐的EEG自编码器和轻量级Stable Diffusion适配，实现从脑电图信号到高质量图像的生成，在感知保真度和重建效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将扩散生成模型扩展到脑信号领域可以加深对人类感知和心智表征的理解，但EEG信号存在高噪声、低空间分辨率和强个体间变异性的挑战。

Method: 两阶段框架：第一阶段使用CLIP对齐的EEG自编码器学习语义结构化的潜在表示；第二阶段冻结预训练编码器，与轻量级Stable Diffusion适配集成，实现高效EEG特征条件化。

Result: 在CVPR40数据集上实现了语义连贯的潜在空间和最先进的感知保真度，在重建效率和图像质量方面均优于现有EEG到图像模型，且能有效跨主体泛化。

Conclusion: 重建大脑感知的内容而非分类的内容，是基于EEG的忠实图像生成的关键。

Abstract: Recent progress in diffusion-based generative models has enabled high-quality image synthesis conditioned on diverse modalities. Extending such models to brain signals could deepen our understanding of human perception and mental representations. However,electroencephalography (EEG) presents major challenges for image generation due to high noise, low spatial resolution, and strong inter-subject variability. Existing approaches,such as DreamDiffusion, BrainVis, and GWIT, primarily adapt EEG features to pre-trained Stable Diffusion models using complex alignment or classification pipelines, often resulting in large parameter counts and limited interpretability. We introduce SYNAPSE, a two-stage framework that bridges EEG signal representation learning and high-fidelity image synthesis. In Stage1, a CLIP-aligned EEG autoencoder learns a semantically structured latent representation by combining signal reconstruction and cross-modal alignment objectives. In Stage2, the pretrained encoder is frozen and integrated with a lightweight adaptation of Stable Diffusion, enabling efficient conditioning on EEG features with minimal trainable parameters. Our method achieves a semantically coherent latent space and state-of-the-art perceptual fidelity on the CVPR40 dataset, outperforming prior EEG-to-image models in both reconstruction efficiency and image quality. Quantitative and qualitative analyses demonstrate that SYNAPSE generalizes effectively across subjects, preserving visual semantics even when class-level agreement is reduced. These results suggest that reconstructing what the brain perceives, rather than what it classifies, is key to faithful EEG-based image generation.

</details>


### [2] [Semantic-driven Wireless Environment Knowledge Representation for Efficiency-Accuracy Balanced Beam Prediction in Vehicular Networks](https://arxiv.org/abs/2511.17552)
*Jialin Wang,Jianhua Zhang,Yu Li,Yutong Sun,Yuxiang Zhang*

Main category: eess.SP

TL;DR: 提出了PES-WEKBP框架，通过电磁基础的知识蒸馏方法将原始视觉数据转换为超精简、可解释的无线环境知识矩阵，实现高维度压缩和准确波束预测。


<details>
  <summary>Details</summary>
Motivation: 车联网高速移动环境下需要超可靠低延迟通信，传统波束预测方法面临高维输入、训练时间长和可解释性差的问题。

Method: 使用电磁基础的知识蒸馏方法，将视觉数据转换为包含材料电磁特性和空间关系的无线环境知识矩阵，然后通过轻量级决策网络进行波束预测。

Result: 实现了99.75%到99.96%的维度压缩，准确率提升5.52%到8.19%，在PCEI指标上优于现有方法。

Conclusion: PES-WEKBP框架在车联网场景中实现了高效可靠的波束预测，平衡了可靠性和计算效率。

Abstract: The rapid evolution of the internet of vehicles demands ultra-reliable low-latency communication in high-mobility environments, where conventional beam prediction methods suffer from high-dimensional inputs, prolonged training times, and limited interpretability. To address these challenges, the propagation environment semantics-aware wireless environment knowledge beam prediction (PES-WEKBP) framework is proposed. PES-WEKBP pioneers a novel electromagnetic (EM)-grounded knowledge distillation method, transforming raw visual data into an ultra-lean, interpretable material and location-related wireless environment knowledge matrix. This matrix explicitly encodes critical propagation environment semantics, which is material EM properties and spatial relationships through a physics-informed parameterization process, distilling the environment and channel interplay into a minimal yet information-dense representation. A lightweight decision network then leverages this highly compressed knowledge for low-complexity beam prediction. To holistically evaluate the performance of PES-WEKBP, we first design the prediction consistency-efficiency index (PCEI), which combines prediction accuracy with a stability-penalized logarithmic training time to ensure a balanced optimization of reliability and computational efficiency. Experiments validate that PES-WEKBP achieves a 99.75% to 99.96% dimension reduction and improves accuracy by 5.52% to 8.19%, which outperforms state-of-the-art methods in PCEI scores across diverse vehicular scenarios.

</details>


### [3] [WaveC2R: Wavelet-Driven Coarse-to-Refined Hierarchical Learning for Radar Retrieval](https://arxiv.org/abs/2511.17558)
*Chunlei Shi,Han Xu,Yinghao Li,Yi-Lin Wei,Yongchao Feng,Yecheng Zhang,Dan Niu*

Main category: eess.SP

TL;DR: WaveC2R是一个基于小波变换的粗到精雷达检索框架，通过多源数据融合和频域分解，分别建模低频降水模式和高频气象边界，显著提升了卫星雷达检索的精度。


<details>
  <summary>Details</summary>
Motivation: 现有卫星雷达检索方法主要依赖单一数据源的简单空间域架构，难以准确捕捉复杂降水模式和锐利气象边界，需要更先进的框架来解决这些限制。

Method: WaveC2R采用两阶段方法：(1)强度-边界解耦学习，通过小波分解和频域特定损失函数分别优化低频强度和高频边界；(2)细节增强扩散细化，利用频率感知条件先验和多源数据逐步增强精细尺度降水结构。

Result: 在SEVIR数据集上的实验表明，WaveC2R在卫星雷达检索方面达到了最先进的性能，特别是在保留高强度降水特征和锐利气象边界方面表现出色。

Conclusion: WaveC2R通过频域分解和多源数据融合，有效解决了现有卫星雷达检索方法的局限性，为复杂降水模式的准确建模提供了新思路。

Abstract: Satellite-based radar retrieval methods are widely employed to fill coverage gaps in ground-based radar systems, especially in remote areas affected by terrain blockage and limited detection range. Existing methods predominantly rely on overly simplistic spatial-domain architectures constructed from a single data source, limiting their ability to accurately capture complex precipitation patterns and sharply defined meteorological boundaries. To address these limitations, we propose WaveC2R, a novel wavelet-driven coarse-to-refined framework for radar retrieval. WaveC2R integrates complementary multi-source data and leverages frequency-domain decomposition to separately model low-frequency components for capturing precipitation patterns and high-frequency components for delineating sharply defined meteorological boundaries. Specifically, WaveC2R consists of two stages (i)Intensity-Boundary Decoupled Learning, which leverages wavelet decomposition and frequency-specific loss functions to separately optimize low-frequency intensity and high-frequency boundaries; and (ii)Detail-Enhanced Diffusion Refinement, which employs frequency-aware conditional priors and multi-source data to progressively enhance fine-scale precipitation structures while preserving coarse-scale meteorological consistency. Experimental results on the publicly available SEVIR dataset demonstrate that WaveC2R achieves state-of-the-art performance in satellite-based radar retrieval, particularly excelling at preserving high-intensity precipitation features and sharply defined meteorological boundaries.

</details>


### [4] [OFDM-ISAC Beyond CP Limit: Performance Analysis and Mitigation Algorithms](https://arxiv.org/abs/2511.17878)
*Peishi Li,Ming Li,Rang Liu,Qian Liu,A. Lee Swindlehurst*

Main category: eess.SP

TL;DR: 提出了一个统一的OFDM-ISAC分析框架，用于解决循环前缀(CP)不足时的感知性能问题，并开发了两种标准兼容的连续干扰消除方法。


<details>
  <summary>Details</summary>
Motivation: OFDM在集成感知与通信中很适用，但其循环前缀通常针对通信级多径设计，不足以支持感知需求。当回波超过CP持续时间时，符号间和载波间干扰会破坏子载波正交性并降低感知性能。

Method: 开发了通用的回波模型来捕获CP不足引起的结构化ISI/ICI耦合，提出了两种标准兼容的SIC方法：低复杂度的SIC-DFT和超分辨率的SIC-ESPRIT。

Result: 在CP不足条件下，两种算法均提供超过4dB的SINR改善，SIC-ESPRIT将距离/速度均方根误差降低约一个数量级，接近足够长CP可实现的性能。

Conclusion: 该研究为超越CP限制的可靠长距离OFDM-ISAC感知提供了理论见解和实用解决方案。

Abstract: Orthogonal frequency division multiplexing (OFDM) is well-suited for integrated sensing and communications (ISAC), yet its cyclic prefix (CP) is dimensioned for communications-grade multipath and is generally insufficient for sensing. When echoes exceed the CP duration, inter-symbol and inter-carrier interference (ISI/ICI) break subcarrier orthogonality and degrade sensing. This paper presents a unified analytical and algorithmic framework for OFDM-ISAC beyond the CP limit. We first develop a general echo model that explicitly captures the structured coupling of ISI and ICI caused by CP insufficiency. Building on this model, we derive closed-form expressions for the sensing signal-to-interference-plus-noise ratio (SINR) and the range-Doppler peak sidelobe level ratio (PSLR), both of which are shown to deteriorate approximately linearly with the normalized excess delay beyond the CP. To mitigate these effects, we propose two standard-compatible successive interference cancellation (SIC) methods: SIC-DFT, a low-complexity DFT-based scheme, and SIC-ESPRIT, a super-resolution subspace approach. Simulations corroborate the analysis and demonstrate consistent gains over representative benchmarks. Both algorithms provide more than $4$dB SINR improvement under CP-insufficient conditions, while SIC-ESPRIT reduces range/velocity root-mean-square-errors (RMSE) by about one order of magnitude, approaching the performance achievable with a sufficiently long CP. These results offer both theoretical insight and practical solutions for reliable long-range OFDM-ISAC sensing beyond the CP limit.

</details>


### [5] [On the Performance of Dual-Antenna Repeater Assisted Bi-Static MIMO ISAC](https://arxiv.org/abs/2511.17980)
*Anubhab Chowdhury,Erik G. Larsson*

Main category: eess.SP

TL;DR: 提出中继器辅助的双基地集成感知与通信系统框架，通过中继器增强目标检测和下行通信性能，但需权衡感知与通信间的干扰问题。


<details>
  <summary>Details</summary>
Motivation: 中继器能放大目标反射信号增强感知性能，同时改善下行通信覆盖，但会引入噪声和干扰，需要平衡感知与通信的权衡关系。

Method: 设计基站发射端的预编码器，在中继器热点区域部署中继器，通过精心设计的预编码策略来优化系统性能。

Result: 在中继器热点区域部署中继器，能够以显著降低的目标雷达散射截面方差获得更高的检测概率。

Conclusion: 中继器辅助的集成感知与通信系统通过合理设计预编码器，能够有效提升目标检测性能，同时改善通信覆盖。

Abstract: This paper presents a framework for target detection and downlink data transmission in a repeater-assisted bi-static integrated sensing and communication system. A repeater is an active scatterer that retransmits incoming signals with a complex gain almost instantaneously, thereby enhancing sensing performance by amplifying the echoes reflected by the targets. The same mechanism can also improve downlink communication by mitigating coverage holes. However, the repeater introduces noise and increases interference at the sensing receiver, while also amplifying the interference from target detection signals at the downlink users. The proposed framework accounts for these sensing-communication trade-offs and demonstrates the potential benefits achievable through a carefully designed precoder at the transmitting base station. In particular, our finding is that a higher value of probability of detection can be attained with considerably lower target radar-cross-section variance by deploying repeaters in the target hot-spot areas.

</details>


### [6] [Semi-Passive IRS Enabled Sensing with Group Movable Sensors](https://arxiv.org/abs/2511.18892)
*Qiaoyan Peng,Qingqing Wu,Wen Chen,Guangji Chen,Ying Gao,Lexi Xu,Shaodan Ma*

Main category: eess.SP

TL;DR: 本文研究了半被动智能反射面(IRS)辅助的非视距(NLoS)感知系统中传感器位置选择问题，提出了可移动传感器方案来优化Cramer-Rao界(CRB)，并通过理论分析和数值结果验证了其优于固定位置方案的性能。


<details>
  <summary>Details</summary>
Motivation: 感知系统性能受信号衰减和接收组件数量限制，需要优化传感器位置来提升方向到达角(DoA)估计精度。

Method: 将IRS中的传感器集成组进行移动，推导相应的CRB，并得到CRB最小化问题中可移动传感器位置的最优闭式解。

Result: 理论分析和数值结果表明，所提出的可移动传感器方案在CRB性能上优于固定位置方案。

Conclusion: 通过优化传感器位置可以显著提升半被动IRS辅助NLo S感知系统的DoA估计性能。

Abstract: The performance of the sensing system is limited by the signal attenuation and the number of receiving components. In this letter, we investigate the sensor position selection in a semi-passive intelligent reflecting surface (IRS) enabled non-line-of-sight (NLoS) sensing system. The IRS consists of passive elements and active sensors, where the sensors can receive and process the echo signal for direction-of-arrival (DoA) estimation. Motivated by the movable antenna array and fluid antenna system, we consider the case where the sensors are integrated into a group for movement and derive the corresponding Cramer-Rao bound (CRB). Then, the optimal solution for the positions of the movable sensors (MSs) to the CRB minimization problem is derived in closed form. Moreover, we characterize the relationship between the CRB and system parameters. Theoretical analysis and numerical results are provided to demonstrate the superiority of the proposed MS scheme over the fixed-position (FP) scheme.

</details>


### [7] [Orthogonal Chirp Delay-Doppler Division Multiplexing (CDDM) Modulation for High Mobility Communications](https://arxiv.org/abs/2511.17991)
*Chaoyuan Bai,Pingzhi Fan,Zhengchun Zhou,Zilong Liu*

Main category: eess.SP

TL;DR: 提出一种基于正交chirp-Zak变换的多载波调制框架CDDM，在高速移动通信场景中通过将数据符号扩展到延迟-多普勒域，实现了低计算复杂度和改进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 针对高速移动通信场景，需要开发能够有效处理延迟-多普勒域信道特性的调制方案，以克服传统调制在时变信道中的性能限制。

Method: 采用正交chirp-Zak变换将数据符号扩展到延迟-多普勒域，提出基于CZT的叠加稀疏导频结构进行信道估计，并开发嵌入式导频方案。

Result: 仿真结果显示CDDM在完美CSI下相比现有调制方案显著改善了误码率性能，在非完美CSI下通过CZT导频方案显著降低了归一化均方误差，同时保持与ODDM相当的估计精度但计算复杂度更低。

Conclusion: CDDM调制框架成功地将chirp波形特性与延迟-多普勒域信道结构协同集成，在高速移动通信中实现了优越的性能和计算效率。

Abstract: This paper proposes a novel multi-carrier modulation framework for high-mobility communication scenarios. Our key idea lies in spreading data symbols across the delay-Doppler (DD) domain through orthogonal chirp-Zak transform (CZT). To enable efficient signal multiplexing, the proposed modulation scheme employs a transmitter signal that maintains orthogonality with the inherent resolution characteristics of the DD plane. Termed as Orthogonal Chirp Delay-Doppler Division Multiplexing (CDDM), we demonstrate a synergistic integration of chirp waveform properties with the channel structure of the DD domain, thereby achieving advantages with both lower computational efficiency and improved detection performance. We introduce a novel CZT-based superimposed sparse pilot structure to enable simultaneous estimation of delay-Doppler shifts and channel coefficients. For enhanced performance, we further develop an embedded pilot scheme that demonstrates channel estimation performance comparable to that of Orthogonal Delay-Doppler Division Multiplexing (ODDM) systems. Simulation results demonstrate that CDDM achieves significant bit error rate (BER) improvements over existing modulation schemes , under perfect channel state information (CSI), as well as superior out-of-band emissions (OOBE). Further, for the imperfect CSI case, the proposed CZT-based superimposed pilot scheme leads to significantly reduced normalized mean square error (NMSE), whilst attaining equivalent estimation accuracy to that of ODDM with lower computational complexity.

</details>


### [8] [Channel Estimation for RIS-Aided MU-MIMO mmWave Systems with Direct Channel Links](https://arxiv.org/abs/2511.18009)
*Taihao Zhang,Zhendong Peng,Cunhua Pan,Hong Ren,Jiangzhou Wang*

Main category: eess.SP

TL;DR: 提出了一种用于RIS辅助多用户MIMO毫米波系统的三阶段统一信道估计策略，能够有效估计直接信道和级联信道参数。


<details>
  <summary>Details</summary>
Motivation: 在存在直接信道的情况下，RIS辅助的多用户MIMO毫米波系统需要准确的信道估计来优化系统性能。现有方法在直接信道存在时估计精度有限，需要开发更有效的估计策略。

Method: 采用三阶段策略：第一阶段通过配置RIS相位偏移消除级联信道分量来估计直接信道；第二阶段使用正交子空间投影获取等效信号矩阵，估计用户-RIS信道的离开角；第三阶段结合相同导频的时隙信号，通过正交补空间投影去除直接分量，估计RIS-BS信道的到达角和级联信道剩余参数。

Result: 仿真结果表明，所提方法比现有方法具有更好的估计性能。

Conclusion: 提出的三阶段统一信道估计策略能够有效处理直接信道存在的情况，避免误差传播，提高信道估计精度。

Abstract: In this paper, we propose a three-stage unified channel estimation strategy for reconfigurable intelligent surface (RIS)-aided multi-user (MU) multiple-input multiple-output (MIMO) millimeter wave (mmWave) systems with the existence of the direct channels, where the base station (BS), the users and the RIS are equipped with uniform planar array (UPA). The effectiveness of the developed three-stage strategy stems from the careful design of both the pilot signal sequence of the users and the vectors of RIS. Specifically, in Stage I, the cascaded channel components are eliminated by configuring the RIS phase shift vectors with a π difference to estimate the direct channels for all users. The orthogonal subspace projection is employed in Stage II to obtain equivalent signal matrices, enabling the estimation of angles of departure (AoDs) of the user-RIS channel for all users. In Stage III, we combine the signals of the time slots with the same pilots and project obtained measurement matrix to the orthogonal complement space of the component consisting of the portion of the direct channel, which removes the direct components and thus prevents error propagation from the direct channels to the cascaded channels. Then, we estimate the angles of arrival (AoAs) of the RIS-BS channel and remaining parameters of the cascaded channel for all users by exploiting the sparsity and correlation in the obtained equivalent matrices. Simulation results demonstrate that the proposed method yields better estimation performance than the existing methods.

</details>


### [9] [Precise Localization of High-Voltage Breakdown Events using $φ$-Optical Time-Domain Reflectometry on an Optical Ground Wire](https://arxiv.org/abs/2511.18130)
*Konstantinos Alexoudis,Luke Silvestre,Tom Huiskamp,Jasper Müller,Vincent Sleiffer,Florian Azendorf,Sander Jansen,Chigo Okonkwo,Tom Bradley*

Main category: eess.SP

TL;DR: 使用φ-OTDR技术通过分析高压放电期间背向散射光的相位和频域特征，检测和定位完整的火花间隙击穿，并与示波器记录的事件同步验证。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够检测和定位高压放电事件的方法，特别是在长距离链路上实现高空间分辨率的放电检测和声学重建。

Method: 采用φ-OTDR技术，分析背向散射光的相位和频域特征，与示波器记录的高压放电事件同步，测量亚kHz信号以确认放电特征。

Result: 在长链路上实现了约10米空间分辨率的清晰放电特征检测和声学重建，确认了放电特征的存在。

Conclusion: φ-OTDR技术能够有效检测和定位高压放电事件，在长距离链路上实现高空间分辨率的放电监测和声学重建。

Abstract: We present $φ$-OTDR for detecting and localising full spark-gap breakdowns by analysing backscattered light phase and frequency-domain signatures during high-voltage discharges synchronised with oscilloscope-recorded events. Measuring sub-kHz confirms clear discharge signatures and acoustic reconstruction over long links with $\approx$ 10 m spatial resolution.

</details>


### [10] [CT-CFAR A Robust CFAR Detector Based on CLEAN and Truncated Statistics in Sidelobe-Contaminated Environments](https://arxiv.org/abs/2511.18358)
*Jiachen Zhu,Fangjiong Chen,Jie Wu,Ming Xia*

Main category: eess.SP

TL;DR: 提出一种基于CLEAN概念和截断统计的CFAR目标检测算法，通过截断统计分离目标和噪声分量，引入可学习历史旁瓣信息增强鲁棒性，结合Candan算法和最小二乘估计建立目标重建模型，在复杂场景下实现高精度目标检测。


<details>
  <summary>Details</summary>
Motivation: 解决雷达检测中参考窗口样本非均匀性问题，特别是旁瓣污染和其他异常干扰导致的参考样本异质性，克服传统CFAR算法在复杂场景下的局限性。

Method: 采用截断统计分离雷达回波功率谱中的目标和噪声分量，引入可学习历史旁瓣信息，基于多通道回波数据建立结合Candan算法和最小二乘估计的目标重建模型，融入CLEAN概念抑制旁瓣干扰。

Result: 蒙特卡洛仿真和实际测量实验表明，CT-CFAR算法无需异常样本先验知识即可实现高精度目标检测，相比多种CFAR算法能准确估计噪声谱，在旁瓣污染复杂场景下具有优越的检测性能和计算效率。

Conclusion: 所提出的CT-CFAR算法有效克服了参考窗口的局限性，通过截断统计和CLEAN概念恢复了参考窗口的均匀性假设，在复杂干扰环境下展现出优异的检测性能和适应性。

Abstract: This paper proposes a constant false alarm rate (CFAR) target detection algorithm based on the CLEAN concept and truncated statistics to mitigate the non-homogeneity of reference samples caused by sidelobe contamination and other abnormal interferences within the reference window. The proposed algorithm employs truncated statistics to separate target and noise components in the radar echo power spectrum, thereby restoring the homogeneity assumption of the reference window. In addition, learnable historical sidelobe information is introduced to enhance the robustness and environmental adaptability of the detection process. Furthermore, based on multichannel echo data, a target reconstruction model that combines the Candan algorithm with least-squares estimation is established, incorporating the CLEAN concept to suppress sidelobe interference. Monte Carlo simulations and real-world measurement experiments demonstrate that the proposed CT-CFAR algorithm achieves high-precision target detection without requiring prior knowledge of abnormal samples. Compared with various CFAR algorithms, the proposed approach overcomes the limitations of the reference window, accurately estimates the noise spectrum, and exhibits superior detection performance and computational efficiency in complex scenarios affected by sidelobe contamination.

</details>


### [11] [BeamCKM: A Framework of Channel Knowledge Map Construction for Multi-Antenna Systems](https://arxiv.org/abs/2511.18376)
*Haohan Wang,Xu Shi,Hengyu Zhang,Yashuai Cao,Sufang Yang,Jintao Wang,Kaibin Huang*

Main category: eess.SP

TL;DR: 本文提出BeamCKM概念和CKMTransUNet架构，结合UNet和ViT来构建多天线系统的信道知识地图，并通过M3ChanNet方法利用多模态学习进一步提升地图构建精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统实值路径损耗图与B5G/6G系统中多自由度相干波束成形之间的固有冲突，为多天线通信场景提供专用指导。

Method: 提出CKMTransUNet架构，结合UNet进行多尺度特征提取和ViT模块捕获全局依赖关系，使用复合损失函数表征波束传播特性。基于此提出M3ChanNet方法，利用多模态学习和交叉注意力机制从环境轮廓和实时多波束观测中提取内在侧信息。

Result: 仿真结果表明，所提方法持续优于最先进的插值方法和深度学习方法，即使在环境轮廓不准确的情况下也能提供优越性能。

Conclusion: BeamCKM和CKMTransUNet架构有效解决了多天线系统的信道知识地图构建问题，M3ChanNet进一步提升了精度，为B5G/6G系统提供了有效的解决方案。

Abstract: The channel knowledge map (CKM) enables efficient construction of high-fidelity mapping between spatial environments and channel parameters via electromagnetic information analysis. Nevertheless, existing studies are largely confined to single-antenna systems, failing to offer dedicated guidance for multi-antenna communication scenarios. To address the inherent conflict between traditional real-value pathloss map and multi-degree-of-freedom (DoF) coherent beamforming in B5G/6G systems, this paper proposes a novel concept of BeamCKM and CKMTransUNet architecture. The CKMTransUNet approach combines a UNet backbone for multi-scale feature extraction with a vision transformer (ViT) module to capture global dependencies among encoded linear vectors, utilizing a composite loss function to characterize the beam propagation characteristics. Furthermore, based on the CKMTransUNet backbone, this paper presents a methodology named M3ChanNet. It leverages the multi-modal learning technique and cross-attention mechanisms to extract intrinsic side information from environmental profiles and real-time multi-beam observations, thereby further improving the map construction accuracy. Simulation results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) interpolation methods and deep learning (DL) approaches, delivering superior performance even when environmental contours are inaccurate. For reproducibility, the code is publicly accessible at https://github.com/github-whh/BeamCKM.

</details>


### [12] [AutoMAS: A Generic Multi-Agent System for Algorithm Self-Adaptation in Wireless Networks](https://arxiv.org/abs/2511.18414)
*Dingli Yuan,Jingchen Peng,Jie Fan,Boxiang Ren,Lu Yang,Peng Liu*

Main category: eess.SP

TL;DR: AutoMAS是一个通用的多智能体系统，能够根据动态无线环境自主选择最合适的无线优化算法，将理论保证的无线算法与智能体感知能力相结合，为复杂任务提供更可靠的解决方案。


<details>
  <summary>Details</summary>
Motivation: 无线通信环境具有强动态特性，传统无线网络基于静态规则和预定义算法运行，缺乏自适应能力。人工智能的快速发展为无线网络实现更智能化和完全自动化提供了可能性。

Method: 提出AutoMAS多智能体系统，结合理论保证的无线算法和智能体感知能力，能够自主选择最适合当前无线环境的优化算法。以信道估计问题为例进行案例研究，在具有不同信道传播特性的多样化环境中测试移动用户场景。

Result: 仿真结果表明，AutoMAS在变化场景中能够保证最高精度，在信道估计案例中表现出色。

Conclusion: AutoMAS可以推广到6G无线网络中，以高精度自主处理各种任务，为动态无线环境提供智能化的解决方案。

Abstract: The wireless communication environment has the characteristic of strong dynamics. Conventional wireless networks operate based on the static rules with predefined algorithms, lacking the self-adaptation ability. The rapid development of artificial intelligence (AI) provides a possibility for wireless networks to become more intelligent and fully automated. As such, we plan to integrate the cognitive capability and high intelligence of the emerging AI agents into wireless networks. In this work, we propose AutoMAS, a generic multi-agent system which can autonomously select the most suitable wireless optimization algorithm according to the dynamic wireless environment. Our AutoMAS combines theoretically guaranteed wireless algorithms with agents' perception ability, thereby providing sounder solutions to complex tasks no matter how the environment changes. As an example, we conduct a case study on the classical channel estimation problem, where the mobile user moves in diverse environments with different channel propagation characteristics. Simulation results demonstrate that our AutoMAS can guarantee the highest accuracy in changing scenarios. Similarly, our AutoMAS can be generalized to autonomously handle various tasks in 6G wireless networks with high accuracy.

</details>


### [13] [A Comparative Study of Rare-Event Simulation Methods for Outage Probability in GSC/MRC Systems under Rician Fading](https://arxiv.org/abs/2511.18419)
*Mahmoud Ghazal,Nadhir Ben Rached,Tareq Al-Naffouri*

Main category: eess.SP

TL;DR: 本文研究了增强蒙特卡洛方法在Rician衰落下SIMO系统中断概率评估中的应用，比较了多种方法性能，发现CE方法最为稳健。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛方法在评估SIMO系统中断概率时效率较低，特别是在罕见事件场景下，需要更高效的仿真技术。

Method: 采用了多种增强蒙特卡洛技术：UIS、MLS、ET、CE以及新提出的PIS方法，用于评估GSC/MRC组合的SIMO系统在Rician衰落下的中断概率。

Result: 性能评估显示ET、CE和PIS方法表现最佳，其中CE方法在三种方法中最为稳健。

Conclusion: 交叉熵(CE)方法在评估Rician衰落下SIMO系统中断概率时表现出最佳稳健性，是推荐的仿真技术。

Abstract: This paper explores the use of enhanced Monte-Carlo (MC) techniques to evaluate the outage probability of single-input-multiple-output (SIMO) systems under Rician fading, in which the input is combined using generalized selection combining with maximum ratio combining (GSC/MRC). The studied set of methods includes previously established methods: universal importance sampling (UIS) and multilevel splitting (MLS), alongside readapted methods: exponential twisting (ET) and cross-entropy (CE), and a novel method introduced here: partition importance sampling (PIS). Performance is assessed across standard efficiency metrics, revealing that ET, CE, and PIS exhibit the best performance. CE is found to be the most robust method among them.

</details>


### [14] [6G Satellite Direct-to-Cell Connectivity: "To distribute, or not to distribute, that is the question"](https://arxiv.org/abs/2511.18455)
*Diego Tuzi,Thomas Delamotte,Andreas Knopp*

Main category: eess.SP

TL;DR: 本文提出将传统单颗卫星分解为由多个小型平台（如立方星）组成的星群，通过分布式架构形成大型虚拟孔径，实现卫星与地面手持设备的直接连接。


<details>
  <summary>Details</summary>
Motivation: 6G需要实现卫星与地面手持设备的直接连接，传统单星方案使用相控阵天线成本较高。本文旨在探索通过分布式小卫星群降低成本并引入可扩展性和容错性等新特性。

Method: 将传统单颗大型卫星分解为多个小型平台组成的星群，每个平台配备少量辐射单元，通过远距离分布形成大型虚拟孔径。

Result: 该方法有望降低生产和发射成本，同时引入可扩展性、容错性等分布式系统优势。

Conclusion: 分布式小卫星群方案为6G卫星通信提供了有前景的替代方案，但需要进一步研究解决相关技术挑战。

Abstract: Direct-to-cell connectivity between satellites and common terrestrial handheld devices represents an essential feature of 6G. The industry is considering different type of constellations but using classical single satellite solutions based on phased array antennas. This article proposes to decompose a classical single satellite into a swarm of multiple small platforms (e.g. CubeSats) each equipped with one or a small number of radiating elements. The platforms are spaced far apart to create a large virtual aperture. The use of small satellites promises cost reduction for production and launch, while the distributed nature of the system introduces interesting features, such as scalability and fault tolerance. This perspective article provides insights into the opportunities and a discussion of the research challenges for the feasibility of the proposed approach.

</details>


### [15] [Autoencoder for Position-Assisted Beam Prediction in mmWave ISAC Systems](https://arxiv.org/abs/2511.18594)
*Ahmad A. Aziz El-Banna,Octavia A. Dobre*

Main category: eess.SP

TL;DR: 提出一种轻量级自动编码器模型，用于位置辅助波束预测，相比传统深度全连接神经网络可减少83%的计算复杂度，同时保持相似的波束预测精度。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信需要精确的波束对准，但传统方法需要大量训练开销。通过结合位置信息可以减少这种开销，但现有方法计算复杂度高。

Method: 设计了一个三层欠完备网络的轻量级自动编码器，利用其降维能力来降低训练模型的计算需求。

Result: 仿真结果显示，所提模型在波束预测精度上与基线方法相似，但计算复杂度减少了83%。

Conclusion: 轻量级自动编码器模型能够有效解决位置辅助波束预测问题，在保持性能的同时显著降低计算复杂度。

Abstract: Integrated sensing and communication and millimeter wave (mmWave) have emerged as pivotal technologies for 6G networks. However, the narrow nature of mmWave beams requires precise alignments that typically necessitate large training overhead. This overhead can be reduced by incorporating the position information with beam adjustments. This letter proposes a lightweight autorencoder (LAE) model that addresses the position-assisted beam prediction problem while significantly reducing computational complexity compared to the conventional baseline method, i.e., deep fully connected neural network. The proposed LAE is designed as a three-layer undercomplete network to exploit its dimensionality reduction capabilities and thereby mitigate the computational requirements of the trained model. Simulation results show that the proposed model achieves a similar beam prediction accuracy to the baseline with an 83% complexity reduction.

</details>


### [16] [Leveraging Language Models for Interpretable Analysis of Narratives in a Large Corpus](https://arxiv.org/abs/2511.18599)
*Eric A. Bai,Minling Zhou,Ricardo Henao,Kyle M. Schwing,Lawrence Carin*

Main category: eess.SP

TL;DR: 提出了一种可解释的模型，结合词袋主题表示和基于LLM的问答叙事模型，在共享的RKHS表示空间中量化文档，以解决大规模语料库分析的成本、可解释性和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 叙事驱动人类行为并处于地缘政治核心，但一直难以量化其重叠和演变。需要一种能够测量叙事重叠和演化的量化方法。

Method: 集成词袋主题表示和基于LLM的问答叙事模型，共享潜在的再生核希尔伯特空间表示。使用高效的功能梯度下降更新，并引入受Transformer架构启发的上下文问答外推方法。

Result: 模型能够缓解使用LLM分析大型语料库时的成本、可解释性和泛化挑战，无需完全推理即可准确预测未查询文档的问答结果。

Conclusion: 该方法为量化叙事提供了有效的解决方案，结合了传统词袋模型和现代LLM的优势，在保持可解释性的同时实现了高效的文档分析。

Abstract: Narratives drive human behavior and lay at the core of geopolitics, but have eluded quantification that would permit measurement of their overlap and evolution. We present an interpretable model that integrates an established bag-of-words (BoW) topical representation and a novel LLM-based question answering (Q&A) narrative model, which share a latent Reproducing Kernel Hilbert Space representation, to quantify written documents. Our approach mitigates the cost, interpretability, and generalization challenges of using a LLM to analyze large corpora without full inference. We derive efficient functional gradient descent updates that are interpretable and structurally analogous to the self-attention mechanism in Transformers. We further introduce an in-context Q&A extrapolation method inspired by Transformer architectures, enabling accurate prediction of Q&A outcomes for unqueried documents.

</details>


### [17] [LLM4AMC: Adapting Large Language Models for Adaptive Modulation and Coding](https://arxiv.org/abs/2511.18690)
*Xinyu Pan,Boxun Liu,Xiang Cheng,Chen Chen*

Main category: eess.SP

TL;DR: 提出LLM4AMC方法，利用预训练大语言模型进行信道质量预测，优化5G NR中的自适应调制编码技术，显著提升链路性能。


<details>
  <summary>Details</summary>
Motivation: 传统自适应调制编码方法因信道质量指示器老化问题导致性能下降，而大语言模型在上下文理解和时序建模方面的能力与AMC技术的动态信道适配需求天然契合。

Method: 冻结LLM大部分参数进行微调，设计包含预处理层、嵌入层、骨干网络和输出层的四模块网络架构，有效捕捉信道质量的时变特性以实现准确预测。

Result: 仿真实验表明，所提方法显著改善了链路性能，并展现出实际部署的潜力。

Conclusion: 基于预训练LLM的信道质量预测方法能够有效优化AMC技术，为5G NR系统提供了性能提升的可行方案。

Abstract: Adaptive modulation and coding (AMC) is a key technology in 5G new radio (NR), enabling dynamic link adaptation by balancing transmission efficiency and reliability based on channel conditions. However, traditional methods often suffer from performance degradation due to the aging issues of channel quality indicator (CQI). Recently, the emerging capabilities of large language models (LLMs) in contextual understanding and temporal modeling naturally align with the dynamic channel adaptation requirements of AMC technology. Leveraging pretrained LLMs, we propose a channel quality prediction method empowered by LLMs to optimize AMC, termed LLM4AMC. We freeze most parameters of the LLM and fine-tune it to fully utilize the knowledge acquired during pretraining while better adapting it to the AMC task. We design a network architecture composed of four modules, a preprocessing layer, an embedding layer, a backbone network, and an output layer, effectively capturing the time-varying characteristics of channel quality to achieve accurate predictions of future channel conditions. Simulation experiments demonstrate that our proposed method significantly improves link performance and exhibits potential for practical deployment.

</details>


### [18] [Near-Field Sparse Bayesian Channel Estimation and Tracking for XL-IRS-Aided Wideband mmWave Systems](https://arxiv.org/abs/2511.18752)
*Xiaokun Tuo,Zijian Chen,Ming-Min Zhao,Changsheng You,Min-Jian Zhao*

Main category: eess.SP

TL;DR: 提出了一种基于张量稀疏的信道估计与跟踪(TS-CET)算法，用于XL-IRS辅助的宽带MIMO-OFDM系统，通过利用信道中的时空稀疏性来提高估计精度并降低导频开销。


<details>
  <summary>Details</summary>
Motivation: 6G系统中XL-IRS辅助毫米波通信面临准确获取信道状态信息的挑战，现有方法未能充分利用信道固有的时空稀疏性，导致估计性能不佳。

Method: 首先提出统一的近场级联信道表示模型，构建分层时空稀疏先验，然后提出TS-CET算法，将基于张量的正交匹配追踪与基于粒子的变分贝叶斯推理和消息传递相结合。

Result: 仿真结果表明，与现有基准方法相比，TS-CET框架显著提高了估计精度并降低了导频开销。

Conclusion: 所提出的TS-CET算法有效解决了XL-IRS辅助宽带通信中的信道估计挑战，通过利用时空稀疏性实现了优越的性能。

Abstract: The rapid development of 6G systems demands advanced technologies to boost network capacity and spectral efficiency, particularly in the context of intelligent reflecting surfaces (IRS)-aided millimeter-wave (mmWave) communications. A key challenge here is obtaining accurate channel state information (CSI), especially with extremely large IRS (XL-IRS), due to near-field propagation, high-dimensional wideband cascaded channels, and the passive nature of the XL-IRS. In addition, most existing CSI acquisition methods fail to leverage the spatio-temporal sparsity inherent in the channel, resulting in suboptimal estimation performance. To address these challenges, we consider an XL-IRS-aided wideband multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) system and propose an efficient channel estimation and tracking (CET) algorithm. Specifically, a unified near-field cascaded channel representation model is presented first, and a hierarchical spatio-temporal sparse prior is then constructed to capture two-dimensional (2D) block sparsity in the polar domain, one-dimensional (1D) clustered sparsity in the angle-delay domain, and temporal correlations across different channel estimation frames. Based on these priors, a tensor-based sparse CET (TS-CET) algorithm is proposed that integrates tensor-based orthogonal matching pursuit (OMP) with particle-based variational Bayesian inference (VBI) and message passing. Simulation results demonstrate that the TS-CET framework significantly improves the estimation accuracy and reduces the pilot overhead as compared to existing benchmark methods.

</details>


### [19] [Robust Nonlinear Transform Coding: A Framework for Generalizable Joint Source-Channel Coding](https://arxiv.org/abs/2511.18884)
*Jihun Park,Junyong Shin,Jinsung Park,Yo-Seb Jeon*

Main category: eess.SP

TL;DR: 提出Robust-NTC框架，将变分潜在建模与信道自适应传输结合，通过显式建模潜在分布和变分目标来适应信道变化，无需信道特定训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的JSCC方法隐式吸收信道变化，缺乏对信道不确定性的显式建模，限制了在实际系统中的适应性和性能。

Method: 使用高斯代理对量化和信道噪声建模变分目标，学习潜在统计信息；集成到OFDM系统中，联合优化潜在量化、比特分配、调制阶数和功率分配。

Result: 在OFDM系统中，Robust-NTC在广泛的SNR条件下实现了优于数字JSCC基线的率失真效率和稳定的重建保真度。

Conclusion: Robust-NTC框架通过显式建模信道不确定性，实现了对信道条件的自适应，在实际系统中表现出优越的性能和稳定性。

Abstract: This paper proposes robust nonlinear transform coding (Robust-NTC), a generalizable digital joint source-channel coding (JSCC) framework that couples variational latent modeling with channel adaptive transmission. Unlike learning-based JSCC methods that implicitly absorb channel variations, Robust-NTC explicitly models element-wise latent distributions via a variational objective with a Gaussian proxy for quantization and channel noise, allowing encoder-decoder to capture latent uncertainty without channel-specific training. Using the learned statistics, Robust-NTC also facilitates rate-distortion optimization to adaptively select element-wise quantizers and bit depths according to online channel condition. To support practical deployment, Robust-NTC is integrated into an orthogonal frequency-division multiplexing (OFDM) system, where a unified resource allocation framework jointly optimizes latent quantization, bit allocation, modulation order, and power allocation to minimize transmission latency while guaranteeing learned distortion targets. Simulation results demonstrate that for practical OFDM systems, Robust-NTC achieves superior rate-distortion efficiency and stable reconstruction fidelity compared to digital JSCC baselines across wide-ranging SNR conditions.

</details>


### [20] [Movable-Antenna Array Enhanced Multi-Target Sensing: CRB Characterization and Optimization](https://arxiv.org/abs/2511.18907)
*Haobin Mao,Lipeng Zhu,Wenyan Ma,Zhenyu Xiao,Xiang-Gen Xia,Rui Zhang*

Main category: eess.SP

TL;DR: 提出基于可移动天线阵列的新型无线感知系统，通过优化天线位置来提升多目标空间角度估计性能，相比传统固定天线阵列和单目标导向的可移动天线阵列，在CRB和实际AoA估计均方误差方面均有显著改善。


<details>
  <summary>Details</summary>
Motivation: 可移动天线技术通过灵活的天线移动来提高第六代网络中的无线通信和感知性能，但目前缺乏针对多目标角度估计的系统性优化方法。

Method: 首先建立多目标AoA估计的Cramér-Rao界矩阵与天线位置的关系，然后通过优化天线位置来最小化CRB矩阵迹的期望值，采用蒙特卡洛方法逼近目标函数，并提出基于群体的梯度下降算法求解。

Result: 数值结果表明，所提出的基于可移动天线的设计在降低CRB和实际AoA估计均方误差方面优于传统固定位置天线阵列和单目标导向的可移动天线阵列。

Conclusion: 设计的可移动天线阵列几何在角度域中表现出低相关性和高有效灵敏度向量功率，显著改善了CRB性能，同时降低了角度估计模糊度，提升了均方误差性能。

Abstract: Movable antennas (MAs) have emerged as a promising technology to improve wireless communication and sensing performance towards sixth-generation (6G) networks through flexible antenna movement. In this paper, we propose a novel wireless sensing system based on MA arrays to enhance multi-target spatial angle estimation performance. We begin by characterizing the Cramér-Rao bound (CRB) matrix for multi-target angle of arrival (AoA) estimation as a function of the antenna's positions in MA arrays, thereby establishing a theoretical foundation for antenna position optimization. Then, aiming at improving the sensing coverage performance, we formulate an optimization problem to minimize the expectation of the trace of the CRB matrix over random target angles subject to a given distribution by optimizing the antennas' positions. To tackle the formulated challenging optimization problem, the Monte Carlo method is employed to approximate the intractable objective function, and a swarm-based gradient descent algorithm is subsequently proposed to address the approximated problem. In addition, a lower-bound on the sum of CRBs for multi-target AoA estimation is derived. Numerical results demonstrate that the proposed MA-based design achieves superior sensing performance compared to conventional systems using fixed-position antenna (FPA) arrays and single-target-oriented MA arrays, in terms of decreasing both CRB and the actual AoA estimation mean square error (MSE). Fundamentally, the designed MA array geometry exhibits low correlation and high effective power of sensitivity vectors for multi-target sensing in the angular domain, leading to significant CRB performance improvement. The resultant low correlation of steering vectors over multiple targets' directions further helps mitigate angle estimation ambiguity and thus enhances MSE performance.

</details>


### [21] [Adaptive Probabilistic Constellation Shaping based on Enumerative Sphere Shaping for FSO Channel with Turbulence and Pointing Errors](https://arxiv.org/abs/2511.18911)
*Jingtian Liu,Xiongwei Yang,Yi Wei,Jianjun Yu,Feng Zhao*

Main category: eess.SP

TL;DR: 提出了一种基于枚举球面整形(ESS)的自适应概率星座整形(A-PCS)相干系统，用于自由空间光通信，实现从QPSK到64QAM的准连续速率控制，在强湍流和大指向误差下达到99.999%的可靠性。


<details>
  <summary>Details</summary>
Motivation: 自由空间光通信面临大气湍流、指向误差等导致的随机衰减问题，需要自适应速率控制技术来提高频谱利用率和链路可靠性。

Method: 采用枚举球面整形(ESS)进行分布匹配的概率星座整形(PCS-64QAM)系统，提供从常规QPSK等效到64QAM频谱效率的连续速率控制。

Result: 系统实现了约0.05 bits/4D的频谱效率控制粒度和0.1 dB的后FEC SNR阈值控制，最大控制深度达12.5 dB，在最大湍流强度σ_R²=1.39和指向误差σ_s=0.5m条件下达到99.999%的可靠性。

Conclusion: 基于ESS的A-PCS系统比基于恒定组合分布匹配(CCDM)的系统具有更高的频谱利用率和更精细的控制粒度，能够满足严重湍流和大指向误差下的通信要求。

Abstract: Free-space optical (FSO) transmission enables fast, secure, and efficient next-generation communications with abundant spectrum resources. However, atmospheric turbulence, pointing errors, path loss, and atmospheric loss induce random attenuation, challenging link reliability. Adaptive rate control technology enhances spectrum utilization and reliability. We propose an adaptive probabilistic constellation shaping (A-PCS) coherent system utilizing enumerated spherical shaping (ESS) for distribution matching. With PCS-64QAM, the system achieves continuous rate control from conventional QPSK-equivalent to 64QAM spectral efficiency, providing quasi-continuous control with granularities of approximately $0.05$~bits/4D for spectral efficiency and $0.1$~dB for the post-FEC SNR threshold, and a maximum control depth of $12.5$~dB. Leveraging ESS for efficient sequence utilization, it offers higher spectral utilization and finer control granularity than constant composition DM (CCDM)-based A-PCS systems. We further model and analyze the FSO channel, presenting calculations and comparisons of outage probability and ergodic capacity under varying turbulence intensities and pointing errors. Results demonstrate 99.999~\% reliability at maximum $σ_\mathrm{R}^2 = 1.39$ and $σ_\mathrm{s} = 0.5~\mathrm{m}$, meeting requirements under severe turbulence and large pointing errors.

</details>


### [22] [Development of a Transit-Time Ultrasonic Flow Measurement System for Partially Filled Pipes: Incorporating Flow Profile Correction Factor and Real-Time Clogging Detection](https://arxiv.org/abs/2511.19310)
*Mohammadhadi Mesmarian,Mohammad Mahdi Kharidar,Hossein Nejat Pishkenari*

Main category: eess.SP

TL;DR: 开发了一种能够同时测量流速和液位的超声波流量计，通过流型修正因子(FPCF)显著提高了部分填充管道流量测量的准确性，实验显示最大误差从8.51%降至2.44%。


<details>
  <summary>Details</summary>
Motivation: 部分填充管道中的流量测量比全填充系统更复杂，主要由于横截面内复杂的流速分布是测量误差的关键来源。

Method: 设计开发了超声波流量计，基于流速分布特性推导了流型修正因子(FPCF)并应用于原始流量计输出，在250mm直径管道专用明渠流环中测试校准。

Result: 应用FPCF显著提高了精度，最大流量测量误差从8.51%降至2.44%，校准后FWME从1.78%降至0.08%。设备还能在堵塞情况下可靠测量并触发警报。

Conclusion: 所提出的方法有效提高了部分填充管道流量测量的准确性，设备在正常和堵塞条件下都能可靠工作。

Abstract: Flow measurement in partially filled pipes presents greater complexity compared to fully filled systems, primarily due to the complex velocity distribution within the cross-section, which is a key source of measurement inaccuracy. To address this challenge, an ultrasonic flow meter was designed and developed, capable of simultaneously measuring both flow velocity and fluid level. To improve measurement accuracy, a flow profile correction factor (FPCF) was derived based on the velocity distribution characteristics and applied to the raw flow meter output. A dedicated open-channel flow loop incorporating a 250 mm diameter pipe was constructed to test and calibrate the system under controlled conditions. Flow rates in the loop varied from 2 to 6 liters per second. The accuracy of the flow meter was evaluated using the Flow-Weighted Mean Error (FWME) metric. Experimental results showed that applying the FPCF significantly improved accuracy, reducing the maximum flow measurement error from 8.51% to 2.44%. Furthermore, calibration led to a substantial decrease in FWME from 1.78% to 0.08%, confirming the effectiveness of the proposed methodology. The flow meter was also subjected to clogging scenarios by artificially obstructing the flow. Under these conditions, the device was able to reliably measure the flow and successfully detected the clogging, triggering an alarm to the operator to take necessary action.

</details>


### [23] [Secure Beamforming Design for IRS-ISAC Systems with a Hardware-Efficient Hybrid Beamforming Architecture](https://arxiv.org/abs/2511.19321)
*Weijie Xiong,Zhenglan Zhao,Jingran Lin,Zhiling Xiao,Qiang Li*

Main category: eess.SP

TL;DR: 本文提出了一种基于硬件高效混合波束成形的IRS辅助ISAC系统，通过联合优化模拟/数字波束成形器、IRS反射系数和雷达缩放因子，在保证雷达波束图相似度和总发射功率约束下最大化通信保密间隙。


<details>
  <summary>Details</summary>
Motivation: 在IRS辅助的ISAC系统中，需要平衡通信安全性和雷达探测性能，同时考虑硬件成本效率。混合波束成形架构能够提供性能与成本的良好折衷。

Method: 采用惩罚对偶分解(PDD)框架，将雷达约束作为惩罚项加入目标函数，通过外罚函数法处理非凸优化问题，每个步骤都有闭式解并保证收敛到稳定点。

Result: 仿真结果验证了所提算法的有效性，证明了IRS-ISAC系统在HB架构下能够实现性能与硬件成本的优越平衡。

Conclusion: 所提出的基于PDD的优化方法能够有效解决IRS-ISAC系统中的保密间隙最大化问题，混合波束成形架构为实际部署提供了可行的硬件效率方案。

Abstract: In this paper, we employ a hardware-efficient hybrid beamforming (HB) architecture to achieve balanced performance in an intelligent reflecting surface (IRS)-assisted integrated sensing and communication (ISAC) system. We consider a scenario where a multi-antenna, dual-function base station (BS) performs secure beamforming for a multi-antenna legitimate receiver while simultaneously detecting potential targets. Our objective is to maximize the communication secrecy gap by jointly optimizing the analog and digital beamformers, IRS reflection coefficients, and radar scaling factor, subject to constraints on beampattern similarity, total transmit power budget, and the constant modulus of both the analog beamformer and IRS reflection coefficients. This secrecy gap maximization problem is generally non-convex. To address this, we incorporate the exterior penalty method by adding the radar constraint as a penalty term in the objective function. We then propose an efficient approach based on the penalty dual decomposition (PDD) framework to solve the reformulated problem, featuring closed-form solutions at each step and guaranteeing convergence to a stationary point. Simulation results validate the effectiveness of the proposed algorithm and demonstrate the superiority of the IRS-ISAC system with HB architecture in balancing performance and hardware costs.

</details>


### [24] [Secure Analog Beamforming for Multi-user MISO Systems with Movable Antennas](https://arxiv.org/abs/2511.19360)
*Weijie Xiong,Jingran Lin,Kai Zhong,Liu Yang,Hongli Liu,Qiang Li,Cunhua Pan*

Main category: eess.SP

TL;DR: 本文研究了基于模拟波束成形的可移动天线系统，通过联合优化相位偏移和天线位置来最大化组播保密率，解决了传统全数字波束成形硬件成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 可移动天线能够灵活调整位置以改善信道环境，但传统全数字波束成形需要每个天线配备专用射频链，硬件成本过高。模拟波束成形提供了成本效益更高的替代方案。

Method: 提出惩罚约束乘积流形框架，将位置约束转化为惩罚函数，在乘积流形空间进行无约束优化，并使用并行共轭梯度下降算法高效更新变量。

Result: 仿真结果表明，基于模拟波束成形的可移动天线系统在组播保密率和硬件成本之间实现了良好平衡。

Conclusion: 该工作证明了模拟波束成形与可移动天线结合在物理层安全通信中的有效性，为解决NP-hard优化问题提供了高效算法框架。

Abstract: Movable antennas (MAs) represent a novel approach that enables flexible adjustments to antenna positions, effectively altering the channel environment and thereby enhancing the performance of wireless communication systems. However, conventional MA implementations often adopt fully digital beamforming (FDB), which requires a dedicated RF chain for each antenna. This requirement significantly increase hardware costs, making such systems impractical for multi-antenna deployments. To address this, hardware-efficient analog beamforming (AB) offers a cost-effective alternative. This paper investigates the physical layer security (PLS) in an MA-enabled multiple-input single-output (MISO) communication system with an emphasis on AB. In this scenario, an MA-enabled transmitter with AB broadcasts common confidential information to a group of legitimate receivers, while a number of eavesdroppers overhear the transmission and attempt to intercept the information. Our objective is to maximize the multicast secrecy rate (MSR) by jointly optimizing the phase shifts of the AB and the positions of the MAs, subject to constraints on the movement area of the MAs and the constant modulus (CM) property of the analog phase shifters. This MSR maximization problem is highly challenging, as we have formally proven it to be NP-hard. To solve it efficiently, we propose a penalty constrained product manifold (PCPM) framework. Specifically, we first reformulate the position constraints as a penalty function, enabling unconstrained optimization on a product manifold space (PMS), and then propose a parallel conjugate gradient descent algorithm to efficiently update the variables. Simulation results demonstrate that MA-enabled systems with AB can achieve a well-balanced performance in terms of MSR and hardware costs.

</details>


### [25] [Connectivity-Aware Task Offloading for Remote Northern Regions: a Hybrid LEO-MEO Architecture](https://arxiv.org/abs/2511.19369)
*Mohammed Almekhlafi,Antoine Lesage-Landry,Gunes Karabulut Kurt*

Main category: eess.SP

TL;DR: 提出了一种结合MEO和LEO卫星的混合架构，用于北极地区任务卸载，通过优化框架提高任务准入率并平衡能耗与延迟需求。


<details>
  <summary>Details</summary>
Motivation: 北极地区由于LEO卫星覆盖稀疏，难以实现稳定连接和低延迟计算服务，需要增强偏远地区的服务可靠性。

Method: 开发了优化框架，结合动态路径选择与频率和计算资源分配，将NP-hard问题重构为混合整数凸形式，使用现成优化求解器。

Result: 相比单独LEO网络，混合LEO-MEO架构将任务准入率提高15%，平均延迟降低12%。

Conclusion: 该混合架构有潜力显著增强北极偏远地区的连接性和用户体验。

Abstract: Arctic regions, such as northern Canada, face significant challenges in achieving consistent connectivity and low-latency computing services due to the sparse coverage of Low Earth Orbit (LEO) satellites. To enhance service reliability in remote areas, this paper proposes a hybrid satellite architecture for task offloading that combines Medium Earth Orbit (MEO) and LEO satellites. We develop an optimization framework to maximize task offloading admission rate while balancing the energy consumption and delay requirements. Accounting for satellite visibility and limited computing resources, our approach integrates dynamic path selection with frequency and computational resource allocation. Because the formulated problem is NP-hard, we reformulate it into a mixed-integer convex form using disjunctive constraints and convex relaxation techniques, enabling efficient use of off-the-shelf optimization solvers. Simulation results show that, compared to a standalone LEO network, the proposed hybrid LEO-MEO architecture improves the task admission rate by 15\% and reduces the average delay by 12\%. These findings highlight the architecture's potential to enhance connectivity and user experience in remote Arctic areas.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Generative Myopia: Why Diffusion Models Fail at Structure](https://arxiv.org/abs/2511.18593)
*Milad Siami*

Main category: cs.LG

TL;DR: 论文揭示了图扩散模型(GDMs)存在生成近视问题，即过度关注统计频率而忽略结构关键性，导致在组合任务中移除重要的稀有桥梁边。作者提出了谱加权扩散方法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 图扩散模型在优化统计似然时，会隐式地作为频率滤波器，偏好丰富的子结构而忽略谱关键结构，这种现象被称为生成近视。在组合任务如图稀疏化中，这会导致灾难性地移除结构必需但统计稀有的"稀有桥梁"边。

Method: 提出了谱加权扩散方法，使用有效电阻重新对齐变分目标。该方法将谱先验摊销到训练阶段，在推理时零开销。

Result: 该方法消除了生成近视，匹配了最优谱预言机的性能，在标准扩散完全失败(0%)的对抗基准上实现了100%的连接性。

Conclusion: 谱先验可以有效地解决图扩散模型中的生成近视问题，通过重新对齐优化目标来捕捉结构关键性，而不仅仅是统计频率。

Abstract: Graph Diffusion Models (GDMs) optimize for statistical likelihood, implicitly acting as \textbf{frequency filters} that favor abundant substructures over spectrally critical ones. We term this phenomenon \textbf{Generative Myopia}. In combinatorial tasks like graph sparsification, this leads to the catastrophic removal of ``rare bridges,'' edges that are structurally mandatory ($R_{\text{eff}} \approx 1$) but statistically scarce. We prove theoretically and empirically that this failure is driven by \textbf{Gradient Starvation}: the optimization landscape itself suppresses rare structural signals, rendering them unlearnable regardless of model capacity. To resolve this, we introduce \textbf{Spectrally-Weighted Diffusion}, which re-aligns the variational objective using Effective Resistance. We demonstrate that spectral priors can be amortized into the training phase with zero inference overhead. Our method eliminates myopia, matching the performance of an optimal Spectral Oracle and achieving \textbf{100\% connectivity} on adversarial benchmarks where standard diffusion fails completely (0\%).

</details>


### [27] [Practical Machine Learning for Aphasic Discourse Analysis](https://arxiv.org/abs/2511.17553)
*Jason M. Pittman,Anton Phillips,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.LG

TL;DR: 本研究评估了五种机器学习模型在失语症患者图片描述任务中自动识别正确信息单元(CIU)的能力，发现模型在区分词语与非词语方面表现优异，但在识别CIU方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: CIU分析是量化失语症患者语言能力的重要方法，但临床应用中由于需要人工编码而受限。机器学习技术有望自动化这一过程，减轻言语治疗师的工作负担。

Method: 使用五种监督机器学习模型，基于失语症患者的人类编码转录本和相应的词语及CIU数据进行训练，评估模型在图片描述任务中识别CIU的可靠性。

Result: 词语与非词语分类准确率接近完美(0.995)，AUC范围0.914-0.995；CIU与非CIU分类表现差异较大，k-NN模型准确率最高(0.824)，AUC第二高(0.787)。

Conclusion: 监督机器学习模型能有效区分词语与非词语，但识别CIU更具挑战性，表明自动化CIU分析仍需进一步研究改进。

Abstract: Analyzing spoken discourse is a valid means of quantifying language ability in persons with aphasia. There are many ways to quantify discourse, one common way being to evaluate the informativeness of the discourse. That is, given the total number of words produced, how many of those are context-relevant and accurate. This type of analysis is called Correct Information Unit (CIU) analysis and is one of the most prevalent discourse analyses used by speech-language pathologists (SLPs). Despite this, CIU analysis in the clinic remains limited due to the manual labor needed by SLPs to code and analyze collected speech. Recent advances in machine learning (ML) seek to augment such labor by automating modeling of propositional, macrostructural, pragmatic, and multimodal dimensions of discourse. To that end, this study evaluated five ML models for reliable identification of Correct Information Units (CIUs, Nicholas & Brookshire, 1993), during a picture description task. The five supervised ML models were trained using randomly selected human-coded transcripts and accompanying words and CIUs from persons with aphasia. The baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995) with high AUC range (0.914 min, 0.995 max). In contrast, CIU vs non-CIU showed a greater variability, with the k-nearest neighbor (k-NN) model the highest accuracy (0.824) and second highest AUC (0.787). These findings indicate that while the supervised ML models can distinguish word from not word, identifying CIUs is challenging.

</details>


### [28] [Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks](https://arxiv.org/abs/2511.17564)
*Guilherme Grancho D. Fernandes,Marco A. Barroca,Mateus dos Santos,Rafael S. Oliveira*

Main category: cs.LG

TL;DR: 使用双向LSTM网络对PLAsTiCC数据集中的瞬变天体光变曲线进行分类，将14个类别重组为5个通用类别以解决类别不平衡问题。模型在S-Like和Periodic类别上表现良好，但在Fast和Long类别上性能较差，且难以区分Periodic和Non-Periodic对象。


<details>
  <summary>Details</summary>
Motivation: 解决瞬变天体光变曲线分类中的类别不平衡问题，并评估模型在部分观测数据下的性能表现。

Method: 采用双向LSTM神经网络，通过填充、时间重缩放和通量归一化进行预处理，使用掩码层处理变长序列，在PLAsTiCC数据集上训练和评估。

Result: S-Like和Periodic类别的ROC AUC分别为0.95和0.99，但Fast和Long类别性能较差（Long类别ROC AUC仅0.68）。在部分光变曲线数据（5、10、20天）上性能显著下降。

Conclusion: 类别不平衡和有限的时间信息是主要限制因素，建议采用类别平衡策略和关注检测时刻的预处理技术来改进性能。

Abstract: This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance.

</details>


### [29] [Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs](https://arxiv.org/abs/2511.17566)
*Shuaiyu Xie,Hanbin He,Jian Wang,Bing Li*

Main category: cs.LG

TL;DR: CCLH是一个用于微服务系统根因分析的新框架，通过级联条件学习和异构超图建模来解决传统方法在任务协作和实例关系建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统根因分析方法面临两个关键挑战：1）联合学习范式忽略了任务间的因果依赖关系；2）主要关注点对点关系，忽视了由部署配置和负载均衡引起的实例间群体影响。

Method: 提出CCLH框架，采用级联条件学习来协调诊断任务，提供三级分类法描述实例间的群体影响，并引入异构超图来建模这些关系以模拟故障传播。

Result: 在三个微服务基准数据集上的广泛实验表明，CCLH在根因定位和故障类型识别方面均优于现有最先进方法。

Conclusion: CCLH通过级联条件学习和异构超图建模有效解决了传统根因分析方法的局限性，在微服务系统中实现了更准确的故障诊断。

Abstract: Root cause analysis in microservice systems typically involves two core tasks: root cause localization (RCL) and failure type identification (FTI). Despite substantial research efforts, conventional diagnostic approaches still face two key challenges. First, these methods predominantly adopt a joint learning paradigm for RCL and FTI to exploit shared information and reduce training time. However, this simplistic integration neglects the causal dependencies between tasks, thereby impeding inter-task collaboration and information transfer. Second, these existing methods primarily focus on point-to-point relationships between instances, overlooking the group nature of inter-instance influences induced by deployment configurations and load balancing. To overcome these limitations, we propose CCLH, a novel root cause analysis framework that orchestrates diagnostic tasks based on cascaded conditional learning. CCLH provides a three-level taxonomy for group influences between instances and incorporates a heterogeneous hypergraph to model these relationships, facilitating the simulation of failure propagation. Extensive experiments conducted on datasets from three microservice benchmarks demonstrate that CCLH outperforms state-of-the-art methods in both RCL and FTI.

</details>


### [30] [Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization](https://arxiv.org/abs/2511.17568)
*Le Xu,Jiayu Chen*

Main category: cs.LG

TL;DR: 本文首次将Sharpness-Aware Minimization (SAM)应用于离线强化学习，通过寻找更平坦的最小值来提升模型在数据损坏情况下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习对现实世界的数据损坏非常脆弱，即使鲁棒算法在具有挑战性的观测和混合损坏下也会失败。作者认为这种失败源于数据损坏在损失景观中创建了尖锐的最小值，导致泛化能力差。

Method: 将SAM作为通用即插即用优化器集成到离线RL基线中：IQL（在此设置中表现最佳的离线RL算法）和RIQL（专门为数据损坏鲁棒性设计的算法）。在D4RL基准测试中使用随机和对抗性损坏进行评估。

Result: SAM增强的方法在原始基线上持续且显著地表现出色。奖励表面的可视化证实SAM找到了更平滑的解，为其在提高离线RL代理鲁棒性方面的有效性提供了有力证据。

Conclusion: SAM通过寻找更平坦的最小值，成功提升了离线强化学习在数据损坏情况下的鲁棒性，是一种有效的通用优化方法。

Abstract: Offline reinforcement learning (RL) is vulnerable to real-world data corruption, with even robust algorithms failing under challenging observation and mixture corruptions. We posit this failure stems from data corruption creating sharp minima in the loss landscape, leading to poor generalization. To address this, we are the first to apply Sharpness-Aware Minimization (SAM) as a general-purpose, plug-and-play optimizer for offline RL. SAM seeks flatter minima, guiding models to more robust parameter regions. We integrate SAM into strong baselines for data corruption: IQL, a top-performing offline RL algorithm in this setting, and RIQL, an algorithm designed specifically for data-corruption robustness. We evaluate them on D4RL benchmarks with both random and adversarial corruption. Our SAM-enhanced methods consistently and significantly outperform the original baselines. Visualizations of the reward surface confirm that SAM finds smoother solutions, providing strong evidence for its effectiveness in improving the robustness of offline RL agents.

</details>


### [31] [Binary BPE: A Family of Cross-Platform Tokenizers for Binary Analysis](https://arxiv.org/abs/2511.17573)
*Michael J. Bommarito*

Main category: cs.LG

TL;DR: 提出Binary BPE分词器家族，专门用于二进制分析，通过字节对编码在多个平台上训练，提供4K-64K词汇表，实现2-3倍的上下文窗口效率提升。


<details>
  <summary>Details</summary>
Motivation: 解决二进制分析中字节级分词的问题：原始字节浪费transformer的上下文窗口容量，现有文本分词器无法处理0x00-0xFF序列。

Method: 开发跨平台Byte Pair Encoding分词器，在包含Linux、Windows、macOS、Android和恶意软件的大型二进制语料库上训练，提供4K、8K、16K、32K和64K词汇表的分词器。

Result: 分词器发现可解释模式（ELF/PE头、指令序列、跨平台字符串），每个token实现多字节压缩，在未压缩可执行文件上比原始字节提供2-3倍的上下文窗口内容容量。

Conclusion: Binary BPE分词器为二进制分析提供了高效的分词解决方案，支持内容识别、恶意软件检测、逆向工程和优化等应用，已在HuggingFace发布。

Abstract: Sequence models for binary analysis are bottlenecked by byte-level tokenization: raw bytes waste precious context window capacity for transformers and other neural network architectures, and many existing text-oriented tokenizers fail on arbitrary 0x00--0xFF sequences. To address this issue, we introduce the Binary BPE tokenizer family, a set of cross-platform Byte Pair Encoding (BPE) tokenizers for executables trained on a large corpus of binaries spanning multiple platforms, architectures, and operating systems, including Linux, Windows, macOS, Android, and malware sources. We release trained tokenizers with vocabularies of 4K, 8K, 16K, 32K, and 64K tokens, enabling both systematic scaling studies and practical deployment from resource-constrained edge devices to high-throughput datacenters. These tokenizers discover interpretable patterns (ELF/PE headers, instruction sequences, cross-platform strings) while yielding multi-byte compression per token. On representative uncompressed executables (e.g., ELF/PE/Mach-O rather than compressed APKs), the Binary BPE tokenizers typically allow for roughly 2-3x more binary content per fixed-length transformer context window than raw bytes, enabling more efficient research and practical deployment for content identification, malware detection, reverse engineering, and optimization. We release the trained Binary BPE tokenizers on HuggingFace, providing a drop-in, open-source foundation for binary-focused language models and context-efficient agentic tools.

</details>


### [32] [Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation](https://arxiv.org/abs/2511.17577)
*Fengming Yu,Qingyu Meng,Haiwei Pan,Kejia Zhang*

Main category: cs.LG

TL;DR: 提出了一种轻量级优化方法，结合动态注意力头剪枝和知识蒸馏，在保持数学推理能力的同时显著提升大语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理等复杂任务上表现出色，但计算和存储成本高昂，阻碍实际部署。需要找到既能保持推理能力又能提升效率的方法。

Method: 动态评估多头注意力机制中每个注意力头的重要性（结合权重范数和熵），实时剪枝冗余头以减少计算开销，并通过知识蒸馏将原始模型信息迁移到剪枝后的学生模型中。

Result: 在Math23k数据集上，30%剪枝率下：参数减少18.7%，推理速度提升27.5%，FLOPs减少19.3%，准确率仅下降0.7%（从84.4%降至83.7%）。

Conclusion: 该方法在保持强大推理性能的同时实现了显著的效率提升，为大语言模型在数学推理任务中的高效部署提供了实用解决方案。

Abstract: With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks.

</details>


### [33] [Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation](https://arxiv.org/abs/2511.17579)
*Hefei Xu,Le Wu,Chen Cheng,Hao Liu*

Main category: cs.LG

TL;DR: 提出了一个名为MVA的新框架，通过最小化不同人类价值观之间的互信息来缓解多值对齐中的参数干扰问题，并使用价值外推策略探索帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，将其与人类价值观对齐以确保安全和伦理变得至关重要。现有方法如RLHF和DPO在多值对齐中存在不稳定、低效以及无法有效处理价值观冲突的问题。

Method: 提出MVA框架：1）通过最小化不同人类价值观之间的互信息来减轻参数干扰；2）使用价值外推策略高效探索帕累托前沿，构建具有不同价值偏好的LLM集合。

Result: 大量实验表明，MVA在将LLM与多个人类价值观对齐方面持续优于现有基线方法。

Conclusion: MVA框架有效解决了多值对齐中的挑战，能够更好地平衡多个可能冲突的人类价值观，实现更优的对齐效果。

Abstract: With the rapid advancement of large language models (LLMs), aligning them with human values for safety and ethics has become a critical challenge. This problem is especially challenging when multiple, potentially conflicting human values must be considered and balanced. Although several variants of existing alignment methods (such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)) have been proposed to address multi-value alignment, they suffer from notable limitations: 1) they are often unstable and inefficient in multi-value optimization; and 2) they fail to effectively handle value conflicts. As a result, these approaches typically struggle to achieve optimal trade-offs when aligning multiple values.
  To address this challenge, we propose a novel framework called Multi-Value Alignment (MVA). It mitigates alignment degradation caused by parameter interference among diverse human values by minimizing their mutual information. Furthermore, we propose a value extrapolation strategy to efficiently explore the Pareto frontier, thereby constructing a set of LLMs with diverse value preferences. Extensive experiments demonstrate that MVA consistently outperforms existing baselines in aligning LLMs with multiple human values.

</details>


### [34] [EgoCogNav: Cognition-aware Human Egocentric Navigation](https://arxiv.org/abs/2511.17581)
*Zhiwen Qiu,Ziang Liu,Wenqian Niu,Tapomayukh Bhattacharjee,Saleh Kalantari*

Main category: cs.LG

TL;DR: EgoCogNav是一个多模态自我中心导航框架，通过预测感知路径不确定性作为潜在状态，并融合场景特征与感官线索来联合预测轨迹和头部运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注完全观察场景中的运动预测，往往忽略捕捉人们对空间感受和反应的人类因素。

Method: 提出EgoCogNav框架，预测感知路径不确定性作为潜在状态，通过融合场景特征与感官线索来联合预测轨迹和头部运动。同时构建了CEN数据集，包含6小时真实世界自我中心记录。

Result: EgoCogNav学习到的感知不确定性与人类行为（如扫描、犹豫、回溯）高度相关，并能泛化到未见过的环境。

Conclusion: 该工作通过建模认知和体验因素，深化了对人-环境交互的理解，为安全社交导航和有效辅助寻路提供了支持。

Abstract: Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.

</details>


### [35] [GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.17582)
*Jie Ou,Shuaihong Jiang,Yingjun Du,Cees G. M. Snoek*

Main category: cs.LG

TL;DR: GateRA是一种参数高效微调框架，通过token感知的调制机制动态调整PEFT更新的强度，实现选择性、token级别的自适应，在推理任务中优于现有PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法对所有token应用静态、输入无关的更新，忽视了不同输入的重要性和难度差异，导致在简单内容上过拟合或在重要区域适应不足，特别是在自回归场景中。

Method: 在标准PEFT分支中引入自适应门控机制，实现token级别的动态调整；使用基于熵的正则化鼓励接近二元的门控决策；理论分析显示GateRA在PEFT路径上产生软梯度掩码效应。

Result: 在多个常识推理基准测试中，GateRA持续优于或匹配先前的PEFT方法；可视化显示GateRA自动抑制冗余预填充token的更新，在解码阶段强调适应。

Conclusion: GateRA通过token感知调制实现了更智能的参数高效微调，在保持预训练知识的同时专注于具有挑战性的情况，提供了解释性强、稀疏的自适应能力。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.

</details>


### [36] [Learning Straight Flows: Variational Flow Matching for Efficient Generation](https://arxiv.org/abs/2511.17583)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Xiao Wang,Yanning Shen*

Main category: cs.LG

TL;DR: 提出S-VFM方法，通过引入变分潜码来强制轨迹直线化，解决Flow Matching中一步生成能力受限的问题。


<details>
  <summary>Details</summary>
Motivation: Flow Matching依赖学习弯曲轨迹，难以实现一步生成。现有方法存在离散近似误差、训练不稳定和收敛困难等问题。

Method: 将变分潜码（表示"生成概览"）集成到Flow Matching框架中，明确强制轨迹直线化，产生线性生成路径。

Result: 在三个挑战性基准测试中取得竞争性性能，在训练和推理效率上优于现有方法。

Conclusion: S-VFM通过变分潜码有效解决了Flow Matching的轨迹弯曲问题，实现了更高效的训练和推理。

Abstract: Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \textbf{S}traight \textbf{V}ariational \textbf{F}low \textbf{M}atching (\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.

</details>


### [37] [LLM-Powered Text-Attributed Graph Anomaly Detection via Retrieval-Augmented Reasoning](https://arxiv.org/abs/2511.17584)
*Haoyan Xu,Ruizhi Qian,Zhengtao Yao,Ziyi Liu,Li Li,Yuqi Li,Yanshu Li,Wenqing Zheng,Daniele Rosa,Daniel Barcklow,Senthil Kumar,Jieyu Zhao,Yue Zhao*

Main category: cs.LG

TL;DR: 提出了TAG-AD基准数据集，用于文本属性图的异常节点检测，结合LLM生成真实异常文本，并开发了RAG辅助的零样本LLM异常检测框架。


<details>
  <summary>Details</summary>
Motivation: 文本属性图上的异常检测在欺诈检测等应用中很重要，但由于缺乏标准化基准数据集而研究不足。

Method: 利用LLM在原始文本空间中生成语义连贯但上下文不一致的异常节点文本；提出RAG辅助的零样本LLM异常检测框架，构建全局异常知识库。

Result: 实验显示LLM在检测上下文异常方面特别有效，而GNN方法在结构异常检测方面更优；RAG辅助提示达到与人工设计提示相当的性能。

Conclusion: LLM和GNN方法在异常检测中各有优势，RAG辅助的零样本LLM框架具有实用价值，无需手动提示工程。

Abstract: Anomaly detection on attributed graphs plays an essential role in applications such as fraud detection, intrusion monitoring, and misinformation analysis. However, text-attributed graphs (TAGs), in which node information is expressed in natural language, remain underexplored, largely due to the absence of standardized benchmark datasets. In this work, we introduce TAG-AD, a comprehensive benchmark for anomaly node detection on TAGs. TAG-AD leverages large language models (LLMs) to generate realistic anomalous node texts directly in the raw text space, producing anomalies that are semantically coherent yet contextually inconsistent and thus more reflective of real-world irregularities. In addition, TAG-AD incorporates multiple other anomaly types, enabling thorough and reproducible evaluation of graph anomaly detection (GAD) methods. With these datasets, we further benchmark existing unsupervised GNN-based GAD methods as well as zero-shot LLMs for GAD.
  As part of our zero-shot detection setup, we propose a retrieval-augmented generation (RAG)-assisted, LLM-based zero-shot anomaly detection framework. The framework mitigates reliance on brittle, hand-crafted prompts by constructing a global anomaly knowledge base and distilling it into reusable analysis frameworks. Our experimental results reveal a clear division of strengths: LLMs are particularly effective at detecting contextual anomalies, whereas GNN-based methods remain superior for structural anomaly detection. Moreover, RAG-assisted prompting achieves performance comparable to human-designed prompts while eliminating manual prompt engineering, underscoring the practical value of our RAG-assisted zero-shot LLM anomaly detection framework.

</details>


### [38] [PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis](https://arxiv.org/abs/2511.17585)
*Kang He,Boyu Chen,Yuzhe Ding,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.LG

TL;DR: PaSE框架通过原型对齐校准和Shapley优化均衡来解决多模态情感分析中的模态竞争问题，提升模态间协作性能


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态融合存在模态竞争问题，主导模态会压制较弱模态，导致性能不佳

Method: 采用原型引导校准学习(PCL)精炼单模态表示，通过熵最优传输机制确保语义一致性；使用双阶段优化策略，包括原型门控融合模块和基于Shapley的梯度调制(SGM)

Result: 在IEMOCAP、MOSI和MOSEI数据集上的广泛实验证实PaSE实现了优越性能并有效缓解了模态竞争

Conclusion: PaSE框架能够有效增强多模态协作，同时明确缓解模态竞争问题

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal performance.In this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition.

</details>


### [39] [Emotion and Intention Guided Multi-Modal Learning for Sticker Response Selection](https://arxiv.org/abs/2511.17587)
*Yuxuan Hu,Jian Chen,Yuhao Wang,Zixuan Li,Jing Xiong,Pengyue Jia,Wei Wang,Chengming Li,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 提出EIGML框架，首次联合建模情感和意图，通过双层次对比框架和多模态融合模块，显著提升贴纸响应选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有贴纸响应选择方法通常依赖语义匹配，并分别建模情感和意图线索，当情感和意图不一致时容易导致匹配错误。

Method: EIGML框架包含双层次对比框架（模态内和模态间对齐）和意图-情感引导的多模态融合模块（情感引导意图知识选择、意图-情感引导注意力融合、相似度调整匹配机制）。

Result: 在两个公开SRS数据集上的实验表明，EIGML持续优于最先进的基线方法，实现了更高的准确性和对情感意图特征的更好理解。

Conclusion: EIGML通过联合建模情感和意图，有效减少了孤立建模带来的偏差，显著提升了贴纸选择性能。

Abstract: Stickers are widely used in online communication to convey emotions and implicit intentions. The Sticker Response Selection (SRS) task aims to select the most contextually appropriate sticker based on the dialogue. However, existing methods typically rely on semantic matching and model emotional and intentional cues separately, which can lead to mismatches when emotions and intentions are misaligned. To address this issue, we propose Emotion and Intention Guided Multi-Modal Learning (EIGML). This framework is the first to jointly model emotion and intention, effectively reducing the bias caused by isolated modeling and significantly improving selection accuracy. Specifically, we introduce Dual-Level Contrastive Framework to perform both intra-modality and inter-modality alignment, ensuring consistent representation of emotional and intentional features within and across modalities. In addition, we design an Intention-Emotion Guided Multi-Modal Fusion module that integrates emotional and intentional information progressively through three components: Emotion-Guided Intention Knowledge Selection, Intention-Emotion Guided Attention Fusion, and Similarity-Adjusted Matching Mechanism. This design injects rich, effective information into the model and enables a deeper understanding of the dialogue, ultimately enhancing sticker selection performance. Experimental results on two public SRS datasets show that EIGML consistently outperforms state-of-the-art baselines, achieving higher accuracy and a better understanding of emotional and intentional features. Code is provided in the supplementary materials.

</details>


### [40] [Llamazip: Leveraging LLaMA for Lossless Text Compression and Training Dataset Detection](https://arxiv.org/abs/2511.17589)
*Sören Dréano,Derek Molloy,Noel Murphy*

Main category: cs.LG

TL;DR: Llamazip是一种基于LLaMA3语言模型预测能力的无损文本压缩算法，通过仅存储模型无法预测的token实现高效压缩，同时还能识别文档是否属于语言模型训练数据。


<details>
  <summary>Details</summary>
Motivation: 利用语言模型的预测能力开发高效无损压缩算法，同时解决语言模型训练数据来源识别问题，关注数据来源、知识产权和训练透明度等关键问题。

Method: 基于LLaMA3语言模型的预测能力，仅存储模型无法预测的token，分析量化技术和上下文窗口大小对性能的影响。

Result: 实现了显著的数据压缩效果，同时能够识别文档是否属于语言模型训练数据集，量化技术和上下文窗口大小影响压缩比和计算需求。

Conclusion: Llamazip不仅展示了高效无损压缩的潜力，还为语言模型训练数据来源识别提供了解决方案，有助于提高训练过程的透明度和数据来源的可追溯性。

Abstract: This work introduces Llamazip, a novel lossless text compression algorithm based on the predictive capabilities of the LLaMA3 language model. Llamazip achieves significant data reduction by only storing tokens that the model fails to predict, optimizing storage efficiency without compromising data integrity. Key factors affecting its performance, including quantization and context window size, are analyzed, revealing their impact on compression ratios and computational requirements. Beyond compression, Llamazip demonstrates the potential to identify whether a document was part of the training dataset of a language model. This capability addresses critical concerns about data provenance, intellectual property, and transparency in language model training.

</details>


### [41] [SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data](https://arxiv.org/abs/2511.17590)
*Ke Yu,Shigeru Ishikura,Yukari Usukura,Yuki Shigoku,Teruaki Hayashi*

Main category: cs.LG

TL;DR: 提出SHAP距离作为评估合成表格数据语义保真度的新指标，通过比较真实数据和合成数据训练的模型SHAP归因向量的余弦距离来检测语义差异。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注分布相似性或预测性能，但无法评估合成数据是否保持与真实数据一致的推理模式，即语义保真度。

Method: 引入SHAP距离，定义为基于真实数据与合成数据训练的分类器的全局SHAP归因向量之间的余弦距离。

Result: SHAP距离能可靠识别标准统计和预测指标忽略的语义差异，包括特征重要性偏移和尾部效应欠表示问题。

Conclusion: SHAP距离是审计合成表格数据语义保真度的实用工具，建议将基于归因的评估整合到未来基准测试流程中。

Abstract: Synthetic tabular data, which are widely used in domains such as healthcare, enterprise operations, and customer analytics, are increasingly evaluated to ensure that they preserve both privacy and utility. While existing evaluation practices typically focus on distributional similarity (e.g., the Kullback-Leibler divergence) or predictive performance (e.g., Train-on-Synthetic-Test-on-Real (TSTR) accuracy), these approaches fail to assess semantic fidelity, that is, whether models trained on synthetic data follow reasoning patterns consistent with those trained on real data. To address this gap, we introduce the SHapley Additive exPlanations (SHAP) Distance, a novel explainability-aware metric that is defined as the cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets. By analyzing datasets that span clinical health records with physiological features, enterprise invoice transactions with heterogeneous scales, and telecom churn logs with mixed categorical-numerical attributes, we demonstrate that the SHAP Distance reliably identifies semantic discrepancies that are overlooked by standard statistical and predictive measures. In particular, our results show that the SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. This study positions the SHAP Distance as a practical and discriminative tool for auditing the semantic fidelity of synthetic tabular data, and offers practical guidelines for integrating attribution-based evaluation into future benchmarking pipelines.

</details>


### [42] [Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI](https://arxiv.org/abs/2511.17593)
*Saicharan Kolluru*

Main category: cs.LG

TL;DR: 对vLLM和HuggingFace TGI两个开源LLM服务框架的实证评估，显示vLLM在高并发下吞吐量最高可达TGI的24倍，而TGI在交互式单用户场景下延迟更低。


<details>
  <summary>Details</summary>
Motivation: 生产环境中部署大型语言模型需要高效的推理服务系统来平衡吞吐量、延迟和资源利用率。

Method: 使用LLaMA-2模型（7B到70B参数）对vLLM和TGI进行多维度基准测试，包括吞吐量性能、端到端延迟、GPU内存利用率和可扩展性特性。

Result: vLLM通过其新颖的PagedAttention机制在高并发工作负载下比TGI实现高达24倍的吞吐量提升，而TGI在交互式单用户场景下表现出更低的尾部延迟。

Conclusion: 框架选择应基于具体用例需求：vLLM在高吞吐量批处理场景中表现优异，而TGI更适合具有中等并发性的延迟敏感交互应用。

Abstract: The deployment of Large Language Models (LLMs) in production environments requires efficient inference serving systems that balance throughput, latency, and resource utilization. This paper presents a comprehensive empirical evaluation of two prominent open-source LLM serving frameworks: vLLM and HuggingFace Text Generation Inference (TGI). We benchmark these systems across multiple dimensions including throughput performance, end-to-end latency, GPU memory utilization, and scalability characteristics using LLaMA-2 models ranging from 7B to 70B parameters. Our experiments reveal that vLLM achieves up to 24x higher throughput than TGI under high-concurrency workloads through its novel PagedAttention mechanism, while TGI demonstrates lower tail latencies for interactive single-user scenarios. We provide detailed performance profiles for different deployment scenarios and offer practical recommendations for system selection based on workload characteristics. Our findings indicate that the choice between these frameworks should be guided by specific use-case requirements: vLLM excels in high-throughput batch processing scenarios, while TGI is better suited for latency-sensitive interactive applications with moderate concurrency.

</details>


### [43] [AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention](https://arxiv.org/abs/2511.17594)
*Aleksandar Stankovic*

Main category: cs.LG

TL;DR: AutoSAGE是一个输入感知的CUDA调度器，通过轻量级估计和微探针选择最佳分块和映射策略，在CSR SpMM/SDDMM稀疏GNN聚合操作中实现性能优化，在特定条件下可达4.7倍加速。


<details>
  <summary>Details</summary>
Motivation: 稀疏GNN聚合操作(CSR SpMM/SDDMM)的性能受到度偏斜、特征宽度和GPU微架构的显著影响，需要针对不同输入进行优化。

Method: 开发AutoSAGE调度器，使用轻量级估计和on-device微探针为每个输入选择最佳分块和映射策略，包含回退机制和持久缓存。

Result: 在Reddit和OGBN-Products数据集上，在带宽受限的特征宽度下与供应商基线相当，在小宽度下获得性能提升；在合成稀疏度和偏斜压力测试中实现最高4.7倍内核级加速。

Conclusion: AutoSAGE能有效优化稀疏GNN聚合操作，提供了可复现的CUDA源码、Python绑定和缓存日志。

Abstract: Sparse GNN aggregations (CSR SpMM/SDDMM) vary widely in performance with degree skew, feature width, and GPU micro-architecture. We present AutoSAGE, an input-aware CUDA scheduler that chooses tiling and mapping per input using a lightweight estimate refined by on-device micro-probes, with a guardrail that safely falls back to vendor kernels and a persistent cache for deterministic replay. AutoSAGE covers SpMM and SDDMM and composes into a CSR attention pipeline (SDDMM -> row-softmax -> SpMM). On Reddit and OGBN-Products, it matches vendor baselines at bandwidth-bound feature widths and finds gains at small widths; on synthetic sparsity and skew stress tests it achieves up to 4.7x kernel-level speedups. We release CUDA sources, Python bindings, a reproducible harness, and replayable cache logs.

</details>


### [44] [Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design](https://arxiv.org/abs/2511.17595)
*Markus D. Solbach,John K. Tsotsos*

Main category: cs.LG

TL;DR: 本文研究了强化学习在3D Same-Different视觉空间任务中的应用，发现标准方法面临挑战，但通过基于人类实验设计的课程学习取得了成功。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在复杂、非结构化问题领域中的智能行为表现能力，扩展RL在更广泛场景下的适用性。

Method: 使用PPO、行为克隆和模仿学习等最先进方法，并基于真实人类实验发现设计课程学习策略。

Result: 标准RL方法在直接学习最优策略时遇到困难，但通过精心设计的课程学习实现了有效学习。

Conclusion: 课程学习为强化学习在复杂视觉空间任务中提供了有前景的解决方案，人类实验的见解对于设计有效的学习策略至关重要。

Abstract: Reinforcement Learning is a mature technology, often suggested as a potential route towards Artificial General Intelligence, with the ambitious goal of replicating the wide range of abilities found in natural and artificial intelligence, including the complexities of human cognition. While RL had shown successes in relatively constrained environments, such as the classic Atari games and specific continuous control problems, recent years have seen efforts to expand its applicability. This work investigates the potential of RL in demonstrating intelligent behaviour and its progress in addressing more complex and less structured problem domains.
  We present an investigation into the capacity of modern RL frameworks in addressing a seemingly straightforward 3D Same-Different visuospatial task. While initial applications of state-of-the-art methods, including PPO, behavioural cloning and imitation learning, revealed challenges in directly learning optimal strategies, the successful implementation of curriculum learning offers a promising avenue. Effective learning was achieved by strategically designing the lesson plan based on the findings of a real-world human experiment.

</details>


### [45] [Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning](https://arxiv.org/abs/2511.17598)
*Zhizuo Chen,Theodore T. Allen*

Main category: cs.LG

TL;DR: 提出了非平稳和变折扣MDP（NVMDP）框架，能够处理非平稳环境和时变折扣率，包含传统MDP作为特例，并提供策略塑形机制。


<details>
  <summary>Details</summary>
Motivation: 传统MDP算法在非平稳环境中面临挑战，无限时域方法不适用于有限时域任务，需要更灵活的框架来应对这些限制。

Method: 建立NVMDP理论框架，包括状态-动作值函数递归、矩阵表示、最优性条件，并扩展动态规划和Q学习算法，以及函数逼近下的策略梯度理论。

Result: 在非平稳网格世界环境中，NVMDP算法成功恢复最优轨迹，而原始Q学习失败，验证了框架的有效性。

Conclusion: NVMDP提供了一个理论严谨且实际有效的强化学习框架，只需轻微算法修改即可鲁棒处理非平稳性和显式策略塑形。

Abstract: Algorithms developed under stationary Markov Decision Processes (MDPs) often face challenges in non-stationary environments, and infinite-horizon formulations may not directly apply to finite-horizon tasks. To address these limitations, we introduce the Non-stationary and Varying-discounting MDP (NVMDP) framework, which naturally accommodates non-stationarity and allows discount rates to vary with time and transitions. Infinite-horizon, stationary MDPs emerge as special cases of NVMDPs for identifying an optimal policy, and finite-horizon MDPs are also subsumed within the NVMDP formulations. Moreover, NVMDPs provide a flexible mechanism to shape optimal policies, without altering the state space, action space, or the reward structure. We establish the theoretical foundations of NVMDPs, including assumptions, state- and action-value formulation and recursion, matrix representation, optimality conditions, and policy improvement under finite state and action spaces. Building on these results, we adapt dynamic programming and generalized Q-learning algorithms to NVMDPs, along with formal convergence proofs. For problems requiring function approximation, we extend the Policy Gradient Theorem and the policy improvement bound in Trust Region Policy Optimization (TRPO), offering proofs in both scalar and matrix forms. Empirical evaluations in a non-stationary gridworld environment demonstrate that NVMDP-based algorithms successfully recover optimal trajectories under multiple reward and discounting schemes, whereas original Q-learning fails. These results collectively show that NVMDPs provide a theoretically sound and practically effective framework for reinforcement learning, requiring only minor algorithmic modifications while enabling robust handling of non-stationarity and explicit optimal policy shaping.

</details>


### [46] [From Projection to Prediction: Beyond Logits for Scalable Language Models](https://arxiv.org/abs/2511.17599)
*Jianbing Dong,Jianbin Chang*

Main category: cs.LG

TL;DR: 提出一种将输出投影和损失预测集成到单步操作的新方法，避免显式对数张量生成，显著减少内存使用和带宽压力。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段流水线需要完全生成中间对数张量，导致内存占用大、带宽消耗高，限制了可扩展性和训练吞吐量。

Method: 通过直接从隐藏状态和目标标记计算损失，绕过显式对数张量生成，将输出投影和损失预测集成到单一操作中。

Result: 实验显示该方法在LLM训练中实现了显著的内存节省和可测量的加速，支持更大批次和更长序列而不牺牲准确性。

Conclusion: 重新思考投影和预测之间的边界为高效LLM训练提供了实用的系统优化方案。

Abstract: Training Large Language Models (LLMs) typically involves a two-stage pipeline at the output layer: hidden states are projected into vocabulary logits via a linear transformation (lm_head), followed by cross-entropy loss computation against target tokens. While conceptually simple, this design incurs substantial overhead. The intermediate logits tensor, with dimensions proportional to batch size, sequence length, and vocabulary size, must be fully materialized in GPU memory, even though only one target token per position is ultimately used. This leads to significant memory footprint and bandwidth comsumption, limiting scalability and slowing training throughput.
  In this work, we introduce a novel approach to integrates the output projection and loss prediction into a single operation. By directly computing the loss from hidden states and target tokens, our approach bypasses explicit logits materialization. This design reduces memory usage and alleviates bandwidth pressure. Experiments on LLM training demonstrate that our method achieves substantial memory savings and measurable speedups compared to the standard two-stage pipeline, enabling large batch sizes and longer sequences without sacrificing accuracy. Our work highlights the benefits of rethinking the boundary between projection and prediction, offering a practical systems optimization for efficient LLM training.

</details>


### [47] [Generalizable and Efficient Automated Scoring with a Knowledge-Distilled Multi-Task Mixture-of-Experts](https://arxiv.org/abs/2511.17601)
*Luyang Fang,Tao Wang,Ping Ma,Xiaoming Zhai*

Main category: cs.LG

TL;DR: 提出了UniMoE-Guided方法，通过知识蒸馏将多个任务特定的大型模型压缩为单个紧凑模型，在保持性能的同时大幅提升效率


<details>
  <summary>Details</summary>
Motivation: 解决自动评分系统中每个任务需要单独模型导致的资源消耗、存储和维护问题

Method: 使用知识蒸馏的多任务混合专家方法，包含共享编码器、门控MoE块和轻量级任务头，结合真实标签和教师指导进行训练

Result: 在9个科学推理任务上，性能与任务特定模型相当，存储需求减少6倍，相比20B参数教师模型减少87倍

Conclusion: 该方法为课堂和大规模评估系统提供了可扩展、可靠且资源高效的自动评分实用路径

Abstract: Automated scoring of written constructed responses typically relies on separate models per task, straining computational resources, storage, and maintenance in real-world education settings. We propose UniMoE-Guided, a knowledge-distilled multi-task Mixture-of-Experts (MoE) approach that transfers expertise from multiple task-specific large models (teachers) into a single compact, deployable model (student). The student combines (i) a shared encoder for cross-task representations, (ii) a gated MoE block that balances shared and task-specific processing, and (iii) lightweight task heads. Trained with both ground-truth labels and teacher guidance, the student matches strong task-specific models while being far more efficient to train, store, and deploy. Beyond efficiency, the MoE layer improves transfer and generalization: experts develop reusable skills that boost cross-task performance and enable rapid adaptation to new tasks with minimal additions and tuning. On nine NGSS-aligned science-reasoning tasks (seven for training/evaluation and two held out for adaptation), UniMoE-Guided attains performance comparable to per-task models while using $\sim$6$\times$ less storage than maintaining separate students, and $87\times$ less than the 20B-parameter teacher. The method offers a practical path toward scalable, reliable, and resource-efficient automated scoring for classroom and large-scale assessment systems.

</details>


### [48] [Beyond Surface-Level Similarity: Hierarchical Contamination Detection for Synthetic Training Data in Foundation Models](https://arxiv.org/abs/2511.17602)
*Sushant Mehta*

Main category: cs.LG

TL;DR: 提出了一个分层污染检测框架，用于检测合成数据对基准测试的语义级污染，相比现有方法在检测效果上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法只能识别token级别的重叠，无法检测语义级别的污染，而基础模型越来越多地使用可能隐含编码基准知识的合成数据进行训练。

Method: 提出了一个四层次的分层污染检测框架：token级别、语义级别、推理模式和性能悬崖检测。

Result: 在MMLU、GSM8K和HumanEval上的实验表明，语义级污染能逃避现有方法检测(F1=0.17-0.49)，但我们的分层方法能有效检测(F1=0.76)，相比最先进基线平均提升26.5%。

Conclusion: 该框架为从业者提供了实用的审计管道工具，支持合成训练数据的负责任部署。

Abstract: Synthetic data has become essential for training foundation models, yet benchmark contamination threatens evaluation integrity. Although existing detection methods identify token-level overlap, they fail to detect semantic-level contamination where synthetic data conceptually resemble benchmarks without lexical overlap. This gap is critical as foundation models increasingly train on synthetic data that may implicitly encode benchmark knowledge. We propose a hierarchical contamination detection framework operating at four levels: token level, semantic level, reasoning pattern, and performance cliff detection. Through controlled experiments on MMLU, GSM8K and HumanEval, we demonstrate that semantic-level contamination evades existing methods (F1=0.17-0.49) but is effectively detected by our hierarchical approach (F1 = 0.76), with an average improvement of 26. 5\% over state-of-the-art baselines. Our framework provides practitioners with practical tools for audit pipelines and enables responsible deployment of synthetic training data.

</details>


### [49] [Saving Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2511.16520)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: cs.LG

TL;DR: FMPlug是一个插件框架，通过实例引导的时间相关预热启动策略和尖锐高斯性正则化，显著提升了基础流匹配模型在逆问题求解中的性能。


<details>
  <summary>Details</summary>
Motivation: 基础流匹配模型本应成为逆问题的通用先验，但目前表现不如领域特定甚至无训练先验。需要解锁其潜力，使其成为实用的可重用先验。

Method: 结合实例引导的时间相关预热启动策略和尖锐高斯性正则化，在保持高斯结构的同时添加问题特定指导。

Result: 在图像恢复和科学逆问题中实现了显著的性能提升。

Conclusion: 为将基础流匹配模型打造成实用的可重用逆问题先验指明了一条路径。

Abstract: Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors. How can we unlock their potential? We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs. FMPlug combines an instance-guided, time-dependent warm-start strategy with a sharp Gaussianity regularization, adding problem-specific guidance while preserving the Gaussian structures. This leads to a significant performance boost across image restoration and scientific IPs. Our results point to a path for making foundation FM models practical, reusable priors for IP solving.

</details>


### [50] [BrainHGT: A Hierarchical Graph Transformer for Interpretable Brain Network Analysis](https://arxiv.org/abs/2511.17604)
*Jiajun Ma,Yongchao Zhang,Chao Zhang,Zhao Lv,Shengbing Pei*

Main category: cs.LG

TL;DR: 提出了BrainHGT，一种分层图Transformer，通过长短期注意力编码器和先验引导聚类模块模拟大脑从局部到全局的信息处理层次结构，显著提升了疾病识别性能并增强了生物可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将大脑建模为扁平网络，忽略了其模块化结构，且注意力机制平等对待所有脑区连接，忽视了与距离相关的节点连接模式。大脑信息处理是一个涉及局部和长程交互、区域与功能模块交互以及模块间交互的分层过程。

Method: 设计长短期范围注意力编码器处理密集局部交互和稀疏长程连接；开发先验引导聚类模块，利用交叉注意力机制将脑区分组为功能社区，并利用神经解剖学先验指导聚类过程。

Result: 实验结果表明，该方法显著提高了疾病识别的性能，并能可靠地捕捉大脑的子功能模块，展示了其可解释性。

Conclusion: BrainHGT通过模拟大脑自然信息处理的分层结构，有效解决了现有方法的局限性，在疾病识别和生物可解释性方面表现出色。

Abstract: Graph Transformer shows remarkable potential in brain network analysis due to its ability to model graph structures and complex node relationships. Most existing methods typically model the brain as a flat network, ignoring its modular structure, and their attention mechanisms treat all brain region connections equally, ignoring distance-related node connection patterns. However, brain information processing is a hierarchical process that involves local and long-range interactions between brain regions, interactions between regions and sub-functional modules, and interactions among functional modules themselves. This hierarchical interaction mechanism enables the brain to efficiently integrate local computations and global information flow, supporting the execution of complex cognitive functions. To address this issue, we propose BrainHGT, a hierarchical Graph Transformer that simulates the brain's natural information processing from local regions to global communities. Specifically, we design a novel long-short range attention encoder that utilizes parallel pathways to handle dense local interactions and sparse long-range connections, thereby effectively alleviating the over-globalizing issue. To further capture the brain's modular architecture, we designe a prior-guided clustering module that utilizes a cross-attention mechanism to group brain regions into functional communities and leverage neuroanatomical prior to guide the clustering process, thereby improving the biological plausibility and interpretability. Experimental results indicate that our proposed method significantly improves performance of disease identification, and can reliably capture the sub-functional modules of the brain, demonstrating its interpretability.

</details>


### [51] [Copula Based Fusion of Clinical and Genomic Machine Learning Risk Scores for Breast Cancer Risk Stratification](https://arxiv.org/abs/2511.17605)
*Agnideep Aich,Sameera Hewage,Md Monzur Murshed*

Main category: cs.LG

TL;DR: 使用copula方法直接建模临床和基因组风险评分的联合关系，可以改善乳腺癌5年癌症特异性死亡的风险分层，比简单线性组合更有效。


<details>
  <summary>Details</summary>
Motivation: 临床和基因组模型通常使用简单线性规则组合，没有考虑风险评分之间的复杂关系，特别是在极端情况下。需要更准确地建模这种联合关系来改善风险分层。

Method: 使用METABRIC乳腺癌队列，训练随机森林和XGBoost等监督分类器，将预测概率转换为伪观测值，然后拟合高斯、Clayton和Gumbel copula来建模临床和基因组风险评分的联合分布。

Result: 临床模型区分度好(AUC 0.783)，基因组模型表现中等(AUC 0.681)。高斯copula最能捕捉联合分布关系。基于copula分组显示，临床和基因组均为高风险的患者生存率最差。

Conclusion: copula融合方法在真实世界队列中有效，考虑评分间依赖关系能更好识别预后最差的患者亚组。

Abstract: Clinical and genomic models are both used to predict breast cancer outcomes, but they are often combined using simple linear rules that do not account for how their risk scores relate, especially at the extremes. Using the METABRIC breast cancer cohort, we studied whether directly modeling the joint relationship between clinical and genomic machine learning risk scores could improve risk stratification for 5-year cancer-specific mortality. We created a binary 5-year cancer-death outcome and defined two sets of predictors: a clinical set (demographic, tumor, and treatment variables) and a genomic set (gene-expression $z$-scores). We trained several supervised classifiers, such as Random Forest and XGBoost, and used 5-fold cross-validated predicted probabilities as unbiased risk scores. These scores were converted to pseudo-observations on $(0,1)^2$ to fit Gaussian, Clayton, and Gumbel copulas. Clinical models showed good discrimination (AUC 0.783), while genomic models had moderate performance (AUC 0.681). The joint distribution was best captured by a Gaussian copula (bootstrap $p=0.997$), which suggests a symmetric, moderately strong positive relationship. When we grouped patients based on this relationship, Kaplan-Meier curves showed clear differences: patients who were high-risk in both clinical and genomic scores had much poorer survival than those high-risk in only one set. These results show that copula-based fusion works in real-world cohorts and that considering dependencies between scores can better identify patient subgroups with the worst prognosis.

</details>


### [52] [Efficient Large-Scale Learning of Minimax Risk Classifiers](https://arxiv.org/abs/2511.17626)
*Kartheek Bondugula,Santiago Mazuelas,Aritz Pérez*

Main category: cs.LG

TL;DR: 提出了一种基于约束和列生成组合的学习算法，用于高效训练大规模多类分类任务中的极小极大风险分类器。


<details>
  <summary>Details</summary>
Motivation: 传统的随机次梯度方法不适用于极小极大风险分类器，因为它们最小化的是最大期望损失而非平均损失。需要开发能处理大规模数据的有效学习算法。

Method: 结合约束生成和列生成技术，构建了一个高效的学习算法，专门用于训练极小极大风险分类器。

Result: 在多个基准数据集上的实验表明，该算法在通用大规模数据上提供高达10倍加速，在类别数量较多时提供约100倍加速。

Conclusion: 所提出的算法成功解决了极小极大风险分类器在大规模多类分类任务中的训练效率问题，显著提升了计算性能。

Abstract: Supervised learning with large-scale data usually leads to complex optimization problems, especially for classification tasks with multiple classes. Stochastic subgradient methods can enable efficient learning with a large number of samples for classification techniques that minimize the average loss over the training samples. However, recent techniques, such as minimax risk classifiers (MRCs), minimize the maximum expected loss and are not amenable to stochastic subgradient methods. In this paper, we present a learning algorithm based on the combination of constraint and column generation that enables efficient learning of MRCs with large-scale data for classification tasks with multiple classes. Experiments on multiple benchmark datasets show that the proposed algorithm provides upto a 10x speedup for general large-scale data and around a 100x speedup with a sizeable number of classes.

</details>


### [53] [Energy-based Autoregressive Generation for Neural Population Dynamics](https://arxiv.org/abs/2511.17606)
*Ningling Ge,Sicheng Dai,Yu Zhu,Shan Yu*

Main category: cs.LG

TL;DR: 提出了基于能量的自回归生成框架，通过能量变换器在潜在空间中学习时间动态，实现高效生成具有真实群体和单神经元发放统计特性的神经数据。


<details>
  <summary>Details</summary>
Motivation: 解决计算建模中计算效率与高保真建模之间的基本权衡问题，加速对大脑功能的理解。

Method: 使用基于能量的变换器，通过严格适当的评分规则在潜在空间中学习时间动态，实现高效的自回归生成。

Result: 在合成Lorenz数据集和两个神经潜在基准数据集上实现了最先进的生成质量，计算效率显著提升，特别是在基于扩散的方法上。条件生成应用展示了泛化到未见行为情境和改善运动脑机接口解码精度的能力。

Conclusion: 基于能量的建模对神经群体动态有效，在神经科学研究和神经工程中具有应用价值。

Abstract: Understanding brain function represents a fundamental goal in neuroscience, with critical implications for therapeutic interventions and neural engineering applications. Computational modeling provides a quantitative framework for accelerating this understanding, but faces a fundamental trade-off between computational efficiency and high-fidelity modeling. To address this limitation, we introduce a novel Energy-based Autoregressive Generation (EAG) framework that employs an energy-based transformer learning temporal dynamics in latent space through strictly proper scoring rules, enabling efficient generation with realistic population and single-neuron spiking statistics. Evaluation on synthetic Lorenz datasets and two Neural Latents Benchmark datasets (MC_Maze and Area2_bump) demonstrates that EAG achieves state-of-the-art generation quality with substantial computational efficiency improvements, particularly over diffusion-based methods. Beyond optimal performance, conditional generation applications show two capabilities: generalizing to unseen behavioral contexts and improving motor brain-computer interface decoding accuracy using synthetic neural data. These results demonstrate the effectiveness of energy-based modeling for neural population dynamics with applications in neuroscience research and neural engineering. Code is available at https://github.com/NinglingGe/Energy-based-Autoregressive-Generation-for-Neural-Population-Dynamics.

</details>


### [54] [Diffusion Models are Molecular Dynamics Simulators](https://arxiv.org/abs/2511.17741)
*Justin Diamond,Markus Lill*

Main category: cs.LG

TL;DR: 本文证明了带序列偏置的扩散采样器等价于过阻尼朗之万动力学的欧拉-丸山积分器，建立了扩散采样与朗之万时间演化的精确对应关系，提出了完全数据驱动的分子动力学框架。


<details>
  <summary>Details</summary>
Motivation: 将分子动力学重新表述为扩散模型，摆脱传统MD对极小时间步长的依赖，通过模型容量和去噪步数两个可扩展参数控制精度，实现无需轨迹数据、无需人工力场的分子模拟。

Method: 证明扩散采样器与朗之万动力学积分器的等价性，将每个反向去噪步骤解释为随机微分方程的一步，学习得分函数作为漂移项（即学习能量的梯度）。

Result: 建立了轨迹级信息论误差界限，清晰分离离散化误差和得分模型误差，证明生成的分子轨迹具有MD类时间相关性，尽管模型仅基于静态构型训练。

Conclusion: 该框架实现了完全数据驱动的分子动力学，从非相关平衡快照学习力场，无需轨迹数据训练，仍能保持与学习能量相关的玻尔兹曼分布。

Abstract: We prove that a denoising diffusion sampler equipped with a sequential bias across the batch dimension is exactly an Euler-Maruyama integrator for overdamped Langevin dynamics. Each reverse denoising step, with its associated spring stiffness, can be interpreted as one step of a stochastic differential equation with an effective time step set jointly by the noise schedule and that stiffness. The learned score then plays the role of the drift, equivalently the gradient of a learned energy, yielding a precise correspondence between diffusion sampling and Langevin time evolution.
  This equivalence recasts molecular dynamics (MD) in terms of diffusion models. Accuracy is no longer tied to a fixed, extremely small MD time step; instead, it is controlled by two scalable knobs: model capacity, which governs how well the drift is approximated, and the number of denoising steps, which sets the integrator resolution. In practice, this leads to a fully data-driven MD framework that learns forces from uncorrelated equilibrium snapshots, requires no hand-engineered force fields, uses no trajectory data for training, and still preserves the Boltzmann distribution associated with the learned energy.
  We derive trajectory-level, information-theoretic error bounds that cleanly separate discretization error from score-model error, clarify how temperature enters through the effective spring, and show that the resulting sampler generates molecular trajectories with MD-like temporal correlations, even though the model is trained only on static configurations.

</details>


### [55] [Finding Pre-Injury Patterns in Triathletes from Lifestyle, Recovery and Load Dynamics Features](https://arxiv.org/abs/2511.17610)
*Leonardo Rossi,Bruno Rodrigues*

Main category: cs.LG

TL;DR: 提出一个针对铁人三项的合成数据生成框架，整合训练负荷、睡眠质量、压力和恢复状态等日常因素，通过机器学习模型实现高精度的运动损伤预测。


<details>
  <summary>Details</summary>
Motivation: 铁人三项训练中高强度的重复性生理压力使运动员面临过度使用损伤风险，现有预测方法主要依赖训练负荷指标，忽略了睡眠质量、压力和个体生活方式等关键恢复因素。

Method: 开发专门针对铁人三项的合成数据生成框架，生成生理上合理的运动员档案，模拟包含周期化和负荷管理原则的个性化训练计划，并整合睡眠质量、压力水平和恢复状态等日常生活因素。

Result: 机器学习模型（LASSO、随机森林和XGBoost）表现出高预测性能（AUC高达0.86），识别出睡眠障碍、心率变异性和压力是损伤风险的关键早期指标。

Conclusion: 这种基于可穿戴设备的方法不仅提高了损伤预测准确性，还为克服现实世界数据限制提供了实用解决方案，为整体、情境感知的运动员监测提供了途径。

Abstract: Triathlon training, which involves high-volume swimming, cycling, and running, places athletes at substantial risk for overuse injuries due to repetitive physiological stress. Current injury prediction approaches primarily rely on training load metrics, often neglecting critical factors such as sleep quality, stress, and individual lifestyle patterns that significantly influence recovery and injury susceptibility.
  We introduce a novel synthetic data generation framework tailored explicitly for triathlon. This framework generates physiologically plausible athlete profiles, simulates individualized training programs that incorporate periodization and load-management principles, and integrates daily-life factors such as sleep quality, stress levels, and recovery states. We evaluated machine learning models (LASSO, Random Forest, and XGBoost) showing high predictive performance (AUC up to 0.86), identifying sleep disturbances, heart rate variability, and stress as critical early indicators of injury risk. This wearable-driven approach not only enhances injury prediction accuracy but also provides a practical solution to overcoming real-world data limitations, offering a pathway toward a holistic, context-aware athlete monitoring.

</details>


### [56] [Smoothed Agnostic Learning of Halfspaces over the Hypercube](https://arxiv.org/abs/2511.17782)
*Yiwen Kou,Raghu Meka*

Main category: cs.LG

TL;DR: 提出了一个基于随机位翻转的平滑学习框架，用于布尔半空间的不可知学习，在次指数分布假设下实现了高效算法。


<details>
  <summary>Details</summary>
Motivation: 解决布尔半空间不可知学习在离散域中的计算困难问题，现有基于高斯扰动的框架不适用于离散域。

Method: 引入基于随机位翻转的平滑学习框架，在次指数分布假设下设计高效学习算法。

Result: 获得了近似n的多项式时间复杂度和样本复杂度，首次在布尔超立方体上实现平滑不可知半空间学习的高效计算保证。

Conclusion: 该框架弥合了最坏情况不可计算性与实际可学习性之间的差距，为离散设置中的平滑学习提供了理论基础。

Abstract: Agnostic learning of Boolean halfspaces is a fundamental problem in computational learning theory, but it is known to be computationally hard even for weak learning. Recent work [CKKMK24] proposed smoothed analysis as a way to bypass such hardness, but existing frameworks rely on additive Gaussian perturbations, making them unsuitable for discrete domains. We introduce a new smoothed agnostic learning framework for Boolean inputs, where perturbations are modeled via random bit flips. This defines a natural discrete analogue of smoothed optimality generalizing the Gaussian case. Under strictly subexponential assumptions on the input distribution, we give an efficient algorithm for learning halfspaces in this model, with runtime and sample complexity approximately n raised to a poly(1/(sigma * epsilon)) factor. Previously, such algorithms were known only with strong structural assumptions for the discrete hypercube, for example, independent coordinates or symmetric distributions. Our result provides the first computationally efficient guarantee for smoothed agnostic learning of halfspaces over the Boolean hypercube, bridging the gap between worst-case intractability and practical learnability in discrete settings.

</details>


### [57] [AI-driven Generation of MALDI-TOF MS for Microbial Characterization](https://arxiv.org/abs/2511.17611)
*Lucía Schmidt-Santiago,David Rodríguez-Temporal,Carlos Sevilla-Salcedo,Vanessa Gómez-Verdejo*

Main category: cs.LG

TL;DR: 本研究评估了三种深度生成模型（MALDIVAE、MALDIGAN、MALDIffusion）用于合成MALDI-TOF MS微生物质谱数据，以解决临床微生物学中数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: MALDI-TOF MS是临床微生物鉴定的关键技术，但数据驱动的诊断模型发展受限于缺乏足够大、平衡且标准化的光谱数据集。

Method: 采用三种条件生成模型：变分自编码器（MALDIVAE）、生成对抗网络（MALDIGAN）和去噪扩散概率模型（MALDIffusion），以物种标签为条件生成微生物光谱，并使用多种指标评估光谱保真度和多样性。

Result: 合成数据在统计和诊断上与真实测量相当，仅使用合成样本训练的分类器能达到与真实数据训练相似的性能。MALDIVAE在真实性、稳定性和效率方面表现最佳，而MALDIffusion计算成本较高，MALDIGAN稳定性稍差。

Conclusion: 生成模型能有效合成真实的MALDI-TOF光谱数据，MALDIVAE提供了最佳平衡，且合成数据增强能显著改善少数物种的分类准确性，有效缓解类别不平衡问题。

Abstract: Matrix-Assisted Laser Desorption/Ionization Time-of-Flight Mass Spectrometry (MALDI-TOF MS) has become a cornerstone technology in clinical microbiology, enabling rapid and accurate microbial identification. However, the development of data-driven diagnostic models remains limited by the lack of sufficiently large, balanced, and standardized spectral datasets. This study investigates the use of deep generative models to synthesize realistic MALDI-TOF MS spectra, aiming to overcome data scarcity and support the development of robust machine learning tools in microbiology.
  We adapt and evaluate three generative models, Variational Autoencoders (MALDIVAEs), Generative Adversarial Networks (MALDIGANs), and Denoising Diffusion Probabilistic Model (MALDIffusion), for the conditional generation of microbial spectra guided by species labels. Generation is conditioned on species labels, and spectral fidelity and diversity are assessed using diverse metrics.
  Our experiments show that synthetic data generated by MALDIVAE, MALDIGAN, and MALDIffusion are statistically and diagnostically comparable to real measurements, enabling classifiers trained exclusively on synthetic samples to reach performance levels similar to those trained on real data. While all models faithfully reproduce the peak structure and variability of MALDI-TOF spectra, MALDIffusion obtains this fidelity at a substantially higher computational cost, and MALDIGAN shows competitive but slightly less stable behaviour. In contrast, MALDIVAE offers the most favorable balance between realism, stability, and efficiency. Furthermore, augmenting minority species with synthetic spectra markedly improves classification accuracy, effectively mitigating class imbalance and domain mismatch without compromising the authenticity of the generated data.

</details>


### [58] [Improved Sample Complexity for Full Coverage in Compact and Continuous Spaces](https://arxiv.org/abs/2511.17784)
*Lyu Yuhuan*

Main category: cs.LG

TL;DR: 提出了一种基于随机采样的覆盖分析方法，通过离散化d维单位超立方体并应用集中不等式，得到了与失败概率对数相关的样本复杂度界限，相比经典的线性依赖更紧致。


<details>
  <summary>Details</summary>
Motivation: 经典的覆盖分析在小失败概率下通常产生保守界限，需要更精确的理论工具来支持依赖网格覆盖保证的算法。

Method: 在d维单位超立方体上进行均匀随机采样，分析离散化后的未覆盖子立方体数量，应用集中不等式到未覆盖计数统计量。

Result: 推导出样本复杂度界限M=O(C̃ln(2C̃/δ))，具有对数依赖关系，数值研究表明该界限能更紧密地跟踪实际覆盖需求。

Conclusion: 该方法为依赖网格覆盖保证的算法提供了更锐利的理论工具，特别是在高置信度机制下能实现更高效的采样。

Abstract: Verifying uniform conditions over continuous spaces through random sampling is fundamental in machine learning and control theory, yet classical coverage analyses often yield conservative bounds, particularly at small failure probabilities. We study uniform random sampling on the $d$-dimensional unit hypercube and analyze the number of uncovered subcubes after discretization. By applying a concentration inequality to the uncovered-count statistic, we derive a sample complexity bound with a logarithmic dependence on the failure probability ($δ$), i.e., $M =O( \tilde{C}\ln(\frac{2\tilde{C}}δ))$, which contrasts sharply with the classical linear $1/δ$ dependence. Under standard Lipschitz and uniformity assumptions, we present a self-contained derivation and compare our result with classical coupon-collector rates. Numerical studies across dimensions, precision levels, and confidence targets indicate that our bound tracks practical coverage requirements more tightly and scales favorably as $δ\to 0$. Our findings offer a sharper theoretical tool for algorithms that rely on grid-based coverage guarantees, enabling more efficient sampling, especially in high-confidence regimes.

</details>


### [59] [Tensor Gauge Flow Models](https://arxiv.org/abs/2511.17616)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: 提出Tensor Gauge Flow Models，通过引入高阶张量规范场扩展了流方程，在Gaussian混合模型上表现出优于标准流和规范流基线的生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有Gauge Flow Models和Higher Gauge Flow Models在数据几何和规范理论结构编码方面有限，需要更丰富的表达能力的流模型。

Method: 将高阶张量规范场纳入流方程，构建Tensor Gauge Flow Models，增强流动力学的几何和规范理论结构。

Result: 在Gaussian混合模型实验中，Tensor Gauge Flow Models相比标准流和规范流基线获得了改进的生成性能。

Conclusion: Tensor Gauge Flow Models通过引入高阶张量规范场，成功增强了流模型的表达能力，在生成任务中表现出优越性能。

Abstract: This paper introduces Tensor Gauge Flow Models, a new class of Generative Flow Models that generalize Gauge Flow Models and Higher Gauge Flow Models by incorporating higher-order Tensor Gauge Fields into the Flow Equation. This extension allows the model to encode richer geometric and gauge-theoretic structure in the data, leading to more expressive flow dynamics. Experiments on Gaussian mixture models show that Tensor Gauge Flow Models achieve improved generative performance compared to both standard and gauge flow baselines.

</details>


### [60] [Semi-Supervised Federated Multi-Label Feature Selection with Fuzzy Information Measures](https://arxiv.org/abs/2511.17796)
*Afsaneh Mahanipour,Hana Khamfroush*

Main category: cs.LG

TL;DR: 提出了一种半监督联邦多标签特征选择方法SSFMLFS，在客户端只有未标记数据、服务器有少量标记数据的联邦学习环境中进行特征选择。


<details>
  <summary>Details</summary>
Motivation: 现有的多标签特征选择方法需要集中式数据，不适合分布式和联邦环境；且联邦方法通常假设客户端有标记数据，这在客户端缺乏标记能力时不可行。

Method: 将模糊信息理论应用于联邦设置，客户端计算模糊相似矩阵并传输给服务器，服务器计算特征冗余度和特征-标签相关性，构建特征图并使用PageRank算法对特征进行重要性排序。

Result: 在五个真实世界数据集上的实验表明，SSFMLFS在非IID数据分布设置下，在三种不同评估指标上都优于其他联邦和集中式的监督与半监督方法。

Conclusion: SSFMLFS方法有效解决了联邦环境中客户端只有未标记数据的多标签特征选择问题，在非IID数据分布下表现出色。

Abstract: Multi-label feature selection (FS) reduces the dimensionality of multi-label data by removing irrelevant, noisy, and redundant features, thereby boosting the performance of multi-label learning models. However, existing methods typically require centralized data, which makes them unsuitable for distributed and federated environments where each device/client holds its own local dataset. Additionally, federated methods often assume that clients have labeled data, which is unrealistic in cases where clients lack the expertise or resources to label task-specific data. To address these challenges, we propose a Semi-Supervised Federated Multi-Label Feature Selection method, called SSFMLFS, where clients hold only unlabeled data, while the server has limited labeled data. SSFMLFS adapts fuzzy information theory to a federated setting, where clients compute fuzzy similarity matrices and transmit them to the server, which then calculates feature redundancy and feature-label relevancy degrees. A feature graph is constructed by modeling features as vertices, assigning relevancy and redundancy degrees as vertex weights and edge weights, respectively. PageRank is then applied to rank the features by importance. Extensive experiments on five real-world datasets from various domains, including biology, images, music, and text, demonstrate that SSFMLFS outperforms other federated and centralized supervised and semi-supervised approaches in terms of three different evaluation metrics in non-IID data distribution setting.

</details>


### [61] [Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks for Explainable Depression Identification](https://arxiv.org/abs/2511.17622)
*Weidao Chen,Yuxiao Yang,Yueming Wang*

Main category: cs.LG

TL;DR: NH-GCAT是一个神经科学启发的分层图因果注意力网络，通过在不同空间尺度上显式建模抑郁症特定机制，将神经科学领域知识与深度学习相结合，用于抑郁症诊断。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经影像数据的图神经网络方法主要是数据驱动的黑盒模型，缺乏神经生物学可解释性。需要开发能结合神经科学知识的可解释抑郁症诊断模型。

Method: 提出三个关键技术：(1)局部脑区水平的残差门控融合模块，整合BOLD动态和功能连接模式；(2)多区域回路水平的分层回路编码方案；(3)多回路网络水平的变分潜在因果注意力机制，推断关键回路间的定向信息流。

Result: 在REST-meta-MDD数据集上进行严格的留一站点交叉验证，NH-GCAT在抑郁症分类中达到最先进性能，样本量加权平均准确率为73.3%，AUROC为76.4%。

Conclusion: NH-GCAT框架在实现高性能抑郁症分类的同时，提供了神经生物学上有意义的解释，成功将神经科学领域知识与深度学习相结合。

Abstract: Major Depressive Disorder (MDD), affecting millions worldwide, exhibits complex pathophysiology manifested through disrupted brain network dynamics. Although graph neural networks that leverage neuroimaging data have shown promise in depression diagnosis, existing approaches are predominantly data-driven and operate largely as black-box models, lacking neurobiological interpretability. Here, we present NH-GCAT (Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks), a novel framework that bridges neuroscience domain knowledge with deep learning by explicitly and hierarchically modeling depression-specific mechanisms at different spatial scales. Our approach introduces three key technical contributions: (1) at the local brain regional level, we design a residual gated fusion module that integrates temporal blood oxygenation level dependent (BOLD) dynamics with functional connectivity patterns, specifically engineered to capture local depression-relevant low-frequency neural oscillations; (2) at the multi-regional circuit level, we propose a hierarchical circuit encoding scheme that aggregates regional node representations following established depression neurocircuitry organization, and (3) at the multi-circuit network level, we develop a variational latent causal attention mechanism that leverages a continuous probabilistic latent space to infer directed information flow among critical circuits, characterizing disease-altered whole-brain inter-circuit interactions. Rigorous leave-one-site-out cross-validation on the REST-meta-MDD dataset demonstrates NH-GCAT's state-of-the-art performance in depression classification, achieving a sample-size weighted-average accuracy of 73.3\% and an AUROC of 76.4\%, while simultaneously providing neurobiologically meaningful explanations.

</details>


### [62] [High-Accuracy List-Decodable Mean Estimation](https://arxiv.org/abs/2511.17822)
*Ziyun Chen,Spencer Compton,Daniel Kane,Jerry Li*

Main category: cs.LG

TL;DR: 本文研究了高精度列表可解码学习问题，提出了在列表大小和精度之间进行权衡的方法。主要结果是在身份协方差高斯分布的平均值估计中，可以实现非平凡的高精度保证。


<details>
  <summary>Details</summary>
Motivation: 现有列表可解码学习算法虽然能实现最优的列表大小，但误差随1/α衰减较差。本文旨在探索是否可以在列表大小和精度之间进行权衡，实现高精度列表可解码学习。

Method: 提出了全新的可识别性证明，并设计了一种不依赖平方和层次结构的新算法，能够输出候选均值列表，其中一个元素与真实均值的ℓ2距离不超过ε。

Result: 证明了存在大小为L = exp(O(log²(1/α)/ε²))的候选均值列表，其中至少一个元素与真实均值的距离不超过ε。算法的时间和样本复杂度为n = d^{O(log L)} + exp exp(Õ(log L))。

Conclusion: 在列表可解码均值估计中，可以实现列表大小与精度之间的权衡，为高精度列表可解码学习提供了理论和算法基础。

Abstract: In list-decodable learning, we are given a set of data points such that an $α$-fraction of these points come from a nice distribution $D$, for some small $α\ll 1$, and the goal is to output a short list of candidate solutions, such that at least one element of this list recovers some non-trivial information about $D$. By now, there is a large body of work on this topic; however, while many algorithms can achieve optimal list size in terms of $α$, all known algorithms must incur error which decays, in some cases quite poorly, with $1 / α$. In this paper, we ask if this is inherent: is it possible to trade off list size with accuracy in list-decodable learning? More formally, given $ε> 0$, can we can output a slightly larger list in terms of $α$ and $ε$, but so that one element of this list has error at most $ε$ with the ground truth? We call this problem high-accuracy list-decodable learning. Our main result is that non-trivial high-accuracy guarantees, both information-theoretically and algorithmically, are possible for the canonical setting of list-decodable mean estimation of identity-covariance Gaussians. Specifically, we demonstrate that there exists a list of candidate means of size at most $L = \exp \left( O\left( \tfrac{\log^2 1 / α}{ε^2} \right)\right)$ so that one of the elements of this list has $\ell_2$ distance at most $ε$ to the true mean. We also design an algorithm that outputs such a list with runtime and sample complexity $n = d^{O(\log L)} + \exp \exp (\widetilde{O}(\log L))$. We do so by demonstrating a completely novel proof of identifiability, as well as a new algorithmic way of leveraging this proof without the sum-of-squares hierarchy, which may be of independent technical interest.

</details>


### [63] [M$^2$OE$^2$-GL: A Family of Probabilistic Load Forecasters That Scales to Massive Customers](https://arxiv.org/abs/2511.17623)
*Haoran Li,Zhe Cheng,Muhao Guo,Yang Weng,Yannan Sun,Victor Tran,John Chainaranont*

Main category: cs.LG

TL;DR: 提出了M2OE2-GL方法，通过全局预训练和轻量级微调解决大规模配电系统中概率负荷预测的异质性和可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 在大型配电系统中，为每个客户单独训练模型计算和存储成本高，而单一全局模型无法处理不同客户类型、位置和相位间的分布偏移。现有方法很少同时解决异质性和可扩展性问题。

Method: 首先在所有馈线负荷上预训练单一全局M2OE2基础模型，然后应用轻量级微调来生成一组紧凑的群组特定预测器。

Result: 在实际电力公司数据上的评估显示，M2OE2-GL实现了显著的误差减少，同时保持了对大量负荷的可扩展性。

Conclusion: M2OE2-GL方法有效解决了大规模配电系统中概率负荷预测的异质性和可扩展性挑战，在保持可扩展性的同时显著提升了预测精度。

Abstract: Probabilistic load forecasting is widely studied and underpins power system planning, operation, and risk-aware decision making. Deep learning forecasters have shown strong ability to capture complex temporal and contextual patterns, achieving substantial accuracy gains. However, at the scale of thousands or even hundreds of thousands of loads in large distribution feeders, a deployment dilemma emerges: training and maintaining one model per customer is computationally and storage intensive, while using a single global model ignores distributional shifts across customer types, locations, and phases. Prior work typically focuses on single-load forecasters, global models across multiple loads, or adaptive/personalized models for relatively small settings, and rarely addresses the combined challenges of heterogeneity and scalability in large feeders. We propose M2OE2-GL, a global-to-local extension of the M2OE2 probabilistic forecaster. We first pretrain a single global M2OE2 base model across all feeder loads, then apply lightweight fine-tuning to derive a compact family of group-specific forecasters. Evaluated on realistic utility data, M2OE2-GL yields substantial error reductions while remaining scalable to very large numbers of loads.

</details>


### [64] [A novel k-means clustering approach using two distance measures for Gaussian data](https://arxiv.org/abs/2511.17823)
*Naitik Gada*

Main category: cs.LG

TL;DR: 提出一种结合类内距离(WCD)和类间距离(ICD)的新型k-means聚类算法，使用Calinski-Harabasz准则确定最佳k值，提高聚类鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统k-means聚类算法存在局限性，希望通过同时考虑类内距离和类间距离来增强聚类结果的稳定性和准确性。

Method: 开发了一种改进的k-means算法，将WCD和ICD作为距离度量，使用Calinski-Harabasz准则自动确定最佳聚类数k。

Result: 在合成数据和UCI基准数据集上的实验表明，该方法比传统k-means在聚类收敛和异常值处理方面表现更好。

Conclusion: 结合WCD和ICD的k-means算法能提供更鲁棒的聚类结果，并为未来研究提供了新的方向。

Abstract: Clustering algorithms have long been the topic of research, representing the more popular side of unsupervised learning. Since clustering analysis is one of the best ways to find some clarity and structure within raw data, this paper explores a novel approach to \textit{k}-means clustering. Here we present a \textit{k}-means clustering algorithm that takes both the within cluster distance (WCD) and the inter cluster distance (ICD) as the distance metric to cluster the data into \emph{k} clusters pre-determined by the Calinski-Harabasz criterion in order to provide a more robust output for the clustering analysis. The idea with this approach is that by including both the measurement metrics, the convergence of the data into their clusters becomes solidified and more robust. We run the algorithm with some synthetically produced data and also some benchmark data sets obtained from the UCI repository. The results show that the convergence of the data into their respective clusters is more accurate by using both WCD and ICD measurement metrics. The algorithm is also better at clustering the outliers into their true clusters as opposed to the traditional \textit{k} means method. We also address some interesting possible research topics that reveal themselves as we answer the questions we initially set out to address.

</details>


### [65] [QML-HCS: A Hypercausal Quantum Machine Learning Framework for Non-Stationary Environments](https://arxiv.org/abs/2511.17624)
*Hector E Mozo*

Main category: cs.LG

TL;DR: QML-HCS是一个量子启发机器学习框架，通过超因果反馈动态实现非平稳环境下的自适应行为，整合量子叠加原理和动态因果反馈，无需专用硬件即可进行实验。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习和量子启发系统在非平稳环境中数据分布漂移、缺乏连续适应机制、因果稳定性和相干状态更新能力的问题。

Method: 采用统一计算架构，整合量子启发叠加原理、动态因果反馈和确定性-随机混合执行，实现超因果处理核心，支持可逆变换、多路径因果传播和漂移状态评估。

Result: 通过最小模拟展示了超因果模型在输入分布突变时的自适应能力，同时保持内部相干性，为未来理论扩展和基准研究奠定基础。

Conclusion: QML-HCS为量子启发学习、因果推理和混合计算提供了可扩展的实验平台，无需专用硬件，为未来与经典和量子模拟平台的集成建立了基础架构。

Abstract: QML-HCS is a research-grade framework for constructing and analyzing quantum-inspired machine learning models operating under hypercausal feedback dynamics. Hypercausal refers to AI systems that leverage extended, deep, or nonlinear causal relationships (expanded causality) to reason, predict, and infer states beyond the capabilities of traditional causal models. Current machine learning and quantum-inspired systems struggle in non-stationary environments, where data distributions drift and models lack mechanisms for continuous adaptation, causal stability, and coherent state updating. QML-HCS addresses this limitation through a unified computational architecture that integrates quantum-inspired superposition principles, dynamic causal feedback, and deterministic-stochastic hybrid execution to enable adaptive behavior in changing environments.
  The framework implements a hypercausal processing core capable of reversible transformations, multipath causal propagation, and evaluation of alternative states under drift. Its architecture incorporates continuous feedback to preserve causal consistency and adjust model behavior without requiring full retraining. QML-HCS provides a reproducible and extensible Python interface backed by efficient computational routines, enabling experimentation in quantum-inspired learning, causal reasoning, and hybrid computation without the need for specialized hardware.
  A minimal simulation demonstrates how a hypercausal model adapts to a sudden shift in the input distribution while preserving internal coherence. This initial release establishes the foundational architecture for future theoretical extensions, benchmarking studies, and integration with classical and quantum simulation platforms.

</details>


### [66] [Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch](https://arxiv.org/abs/2511.17826)
*Ziyang Zhang,Xinheng Ding,Jiayi Yuan,Rixin Liu,Huizi Mao,Jiarong Xing,Zirui Liu*

Main category: cs.LG

TL;DR: 提出了Tree-Based Invariant Kernels (TBIK)来解决大语言模型推理中的张量并行规模相关的非确定性问题，确保在不同TP配置下获得比特级相同的推理结果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务框架存在非确定性行为：相同输入在不同系统配置（如TP规模、批大小）下会产生不同输出，这在LLM作为评判器、多智能体系统和强化学习等应用中造成严重问题，特别是RL训练中训练引擎和推理引擎的并行策略不匹配会导致性能下降甚至崩溃。

Method: 识别并分析了TP引起不一致性的根本原因，提出了基于树的恒定内核(TBIK)，通过统一的层次二叉树结构对齐GPU内和GPU间的归约顺序，实现了TP不变的矩阵乘法和归约原语。

Result: 实验证实了在不同TP规模下实现了零概率发散和比特级可复现的确定性推理，在RL训练管道中实现了vLLM和FSDP之间的比特级相同结果。

Conclusion: TBIK有效解决了大语言模型推理中的张量并行规模相关的非确定性问题，为LLM应用的确定性推理提供了可靠保障。

Abstract: Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.

</details>


### [67] [Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently](https://arxiv.org/abs/2511.17852)
*Bochen Lyu,Yiyang Jia,Xiaohao Cai,Zhanxing Zhu*

Main category: cs.LG

TL;DR: 本文通过理论分析比较了强化学习(RL)和监督微调(SFT)在训练Transformer学习k-稀疏布尔函数时的机制差异，发现RL同时学习整个思维链，而SFT逐步学习思维链。


<details>
  <summary>Details</summary>
Motivation: 虽然RL和SFT都能让Transformer获得思维链推理能力，但它们的底层机制和差异在理论上尚不清楚，需要系统性的理论分析。

Method: 使用单层Transformer和中间监督来学习可递归分解为固定2-稀疏布尔函数的k-稀疏布尔函数，分析RL和SFT的学习动态。

Result: 验证了两种方法在k-PARITY、k-AND和k-OR等基本函数上的可学习性，揭示了RL同时学习整个思维链而SFT逐步学习的差异。

Conclusion: 研究为理解RL和SFT触发Transformer思维链能力的底层机制提供了理论洞见，揭示了它们不同的学习行为模式。

Abstract: Transformers can acquire Chain-of-Thought (CoT) capabilities to solve complex reasoning tasks through fine-tuning. Reinforcement learning (RL) and supervised fine-tuning (SFT) are two primary approaches to this end, yet their underlying mechanisms and differences remain theoretically unclear. In this work, we examine these aspects specifically for learning $k$-sparse Boolean functions with a one-layer transformer and intermediate supervision that is akin to CoT. In particular, we consider $k$-sparse Boolean functions that can be recursively decomposed into fixed 2-sparse Boolean functions. We analyze the learning dynamics of fine-tuning the transformer via either RL or SFT with CoT to identify sufficient conditions for it to provably learn these functions. We verify that these conditions hold for three basic examples, including $k$-PARITY, $k$-AND, and $k$-OR, thus demonstrating the learnability of both approaches. Notably, we reveal that RL and SFT exhibit distinct learning behaviors: RL learns the whole CoT chain simultaneously, whereas SFT learns the CoT chain step-by-step. Overall, our findings provide theoretical insights into the underlying mechanisms of RL and SFT as well as how they differ in triggering the CoT capabilities of transformers.

</details>


### [68] [Rectifying Mean-Shift in Cascaded Precipitation Nowcasting](https://arxiv.org/abs/2511.17628)
*Fanbo Ju,Haiyuan Shi,Qingjian Ni*

Main category: cs.LG

TL;DR: RectiCast是一个两阶段降水临近预报框架，通过双流匹配模型明确解耦均值场偏移校正和局部随机性生成，显著提升了预报性能。


<details>
  <summary>Details</summary>
Motivation: 现有的级联架构方法忽视了确定性预测中的系统性分布偏移与局部随机性的混淆问题，导致概率性组件的预测被污染，特别是在较长预报时效上出现降水模式和强度的不准确。

Method: 采用两阶段框架：第一阶段使用确定性模型生成后验均值；第二阶段引入校正器明确学习分布偏移并生成校正均值，然后生成器在条件校正均值下建模局部随机性。

Result: 在SEVIR和MeteoNet数据集上的实验表明，RectiCast相比现有最先进方法取得了显著的性能提升。

Conclusion: 通过明确解耦均值场偏移校正和局部随机性生成，RectiCast有效解决了现有方法中分布偏移污染问题，提升了降水临近预报的准确性。

Abstract: Precipitation nowcasting, which aims to provide high spatio-temporal resolution precipitation forecasts by leveraging current radar observations, is a core task in regional weather forecasting. The cascaded architecture has emerged as the mainstream paradigm for deep learning-based precipitation nowcasting. This paradigm involves a deterministic model to predict macroscopic trends (or posterior mean), followed by a probabilistic model to generate local details (or local stochasticity). However, existing methods commonly overlook the conflation of the systematic distribution shift in deterministic predictions and the local stochasticity. As a result, the deterministic component's distribution shift contaminates the predictions of the probabilistic component, leading to inaccuracies in precipitation patterns and intensity, particularly over longer lead times. To address this issue, we introduce RectiCast, a two-stage framework that explicitly decouples the correction of mean-field shift from the generation of local stochasticity via a dual Flow Matching model. In the first stage, a deterministic model generates the posterior mean. In the second stage, we introduce a Rectifier to explicitly learn the distribution shift and produce a rectified mean. Subsequently, a Generator focuses on modeling the local stochasticity conditioned on the rectified mean. Experiments on SEVIR and MeteoNet demonstrate that RectiCast achieves significant performance improvements over existing state-of-the-art methods.

</details>


### [69] [Cost-Sensitive Conformal Training with Provably Controllable Learning Bounds](https://arxiv.org/abs/2511.17861)
*Xuesong Jia,Yuanjie Shi,Ziquan Liu,Yi Xu,Yan Yan*

Main category: cs.LG

TL;DR: 提出了一种新的成本敏感共形训练算法，通过真实标签的排名权重策略来最小化预测集大小，避免了传统方法中使用代理函数带来的不可控学习界限问题。


<details>
  <summary>Details</summary>
Motivation: 传统共形训练方法使用Sigmoid或高斯误差函数作为代理指示函数，但这些函数没有对指示函数的统一误差界限，导致学习界限不可控。

Method: 提出成本敏感共形训练算法，理论证明最小化预测集大小的期望值上界于真实标签的期望排名，并开发基于真实标签排名的权重分配策略。

Result: 实验验证了理论分析的有效性，在预测效率方面优于其他共形训练方法，平均预测集大小减少了21.38%。

Conclusion: 所提出的加权目标与共形预测集期望大小之间存在紧密联系，新方法在保证概率有效性的同时显著提高了预测效率。

Abstract: Conformal prediction (CP) is a general framework to quantify the predictive uncertainty of machine learning models that uses a set prediction to include the true label with a valid probability. To align the uncertainty measured by CP, conformal training methods minimize the size of the prediction sets. A typical way is to use a surrogate indicator function, usually Sigmoid or Gaussian error function. However, these surrogate functions do not have a uniform error bound to the indicator function, leading to uncontrollable learning bounds. In this paper, we propose a simple cost-sensitive conformal training algorithm that does not rely on the indicator approximation mechanism. Specifically, we theoretically show that minimizing the expected size of prediction sets is upper bounded by the expected rank of true labels. To this end, we develop a rank weighting strategy that assigns the weight using the rank of true label on each data sample. Our analysis provably demonstrates the tightness between the proposed weighted objective and the expected size of conformal prediction sets. Extensive experiments verify the validity of our theoretical insights, and superior empirical performance over other conformal training in terms of predictive efficiency with 21.38% reduction for average prediction set size.

</details>


### [70] [Boundary-Aware Adversarial Filtering for Reliable Diagnosis under Extreme Class Imbalance](https://arxiv.org/abs/2511.17629)
*Yanxuan Yu,Michael S. Hughes,Julien Lee,Jiacheng Zhou,Andrew F. Laine*

Main category: cs.LG

TL;DR: 提出AF-SMOTE方法解决极端类别不平衡下的分类问题，通过合成少数类样本并使用对抗判别器和边界效用模型进行过滤，在保持校准性能的同时提高召回率和平均精度。


<details>
  <summary>Details</summary>
Motivation: 在医疗诊断等场景中，极端类别不平衡下的召回率和校准性能都至关重要，因为漏诊罕见疾病的真实阳性病例可能带来严重后果。

Method: AF-SMOTE框架：首先合成少数类样本，然后通过对抗判别器和边界效用模型进行过滤，在决策边界平滑和类条件密度的温和假设下，证明过滤步骤能单调改善F_beta代理指标而不增加Brier分数。

Result: 在MIMIC-IV代理标签预测和欺诈检测基准测试中，AF-SMOTE比强过采样基线方法获得更高的召回率和平均精度，并具有最佳校准性能，在多个数据集上验证了这些优势。

Conclusion: AF-SMOTE在医疗数据集上的成功应用展示了其在临床情境中的实用价值，能够有效减少罕见疾病中的漏诊情况。

Abstract: We study classification under extreme class imbalance where recall and calibration are both critical, for example in medical diagnosis scenarios. We propose AF-SMOTE, a mathematically motivated augmentation framework that first synthesizes minority points and then filters them by an adversarial discriminator and a boundary utility model. We prove that, under mild assumptions on the decision boundary smoothness and class-conditional densities, our filtering step monotonically improves a surrogate of F_beta (for beta >= 1) while not inflating Brier score. On MIMIC-IV proxy label prediction and canonical fraud detection benchmarks, AF-SMOTE attains higher recall and average precision than strong oversampling baselines (SMOTE, ADASYN, Borderline-SMOTE, SVM-SMOTE), and yields the best calibration. We further validate these gains across multiple additional datasets beyond MIMIC-IV. Our successful application of AF-SMOTE to a healthcare dataset using a proxy label demonstrates in a disease-agnostic way its practical value in clinical situations, where missing true positive cases in rare diseases can have severe consequences.

</details>


### [71] [Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing](https://arxiv.org/abs/2511.17902)
*Yifan He,Haodong Zhang,Qiuheng Song,Lin Lei,Zhenxuan Zeng,Haoyang He,Hongyan Wu*

Main category: cs.LG

TL;DR: 提出DUPLE元学习框架解决分布式光纤传感中的跨部署活动识别问题，通过双域多原型学习、统计引导网络和查询感知原型聚合来应对信号模式变化、标注数据稀缺和类内多样性不足的挑战。


<details>
  <summary>Details</summary>
Motivation: 分布式光纤传感在实际应用中面临三个关键挑战：不同光纤部署类型下相同活动的信号模式差异大导致域偏移；新部署场景中标注数据稀缺限制模型适应性；源域内数据稀缺难以捕获类内多样性进行鲁棒学习。

Method: 提出DUPLE元学习框架：1) 双域多原型学习器融合时域和频域特征增强泛化能力；2) 统计引导网络从原始统计特征推断域重要性和原型敏感性；3) 查询感知原型聚合模块自适应选择和组合相关原型。

Result: 在跨部署DFOS数据集上的广泛实验表明，该方法在域泛化设置中显著优于基线方法，能够在不同光纤配置下以最少标注数据实现鲁棒的事件识别。

Conclusion: DUPLE框架有效解决了DFOS系统中的跨部署活动识别挑战，通过元学习方法实现了在标注数据稀缺情况下的鲁棒性能，为实际应用提供了可行的解决方案。

Abstract: Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning.
  To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data.
  Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data.

</details>


### [72] [Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change](https://arxiv.org/abs/2511.17630)
*Nele Albers,Esra Cemre Su de Groot,Loes Keijsers,Manon H. Hillegers,Emiel Krahmer*

Main category: cs.LG

TL;DR: 探索使用大型语言模型生成用户交互样本，用于训练数字行为改变场景中的强化学习模型，结果显示LLM生成的样本在缺乏真实数据时有用，且性能达到人类评分者水平。


<details>
  <summary>Details</summary>
Motivation: 开发个性化数字健康行为改变应用需要大量设计选择，这些选择的有效性难以从文献预测且实践评估成本高，因此探索LLM是否能开箱即用地生成有用用户交互样本。

Method: 使用真实用户数据作为比较基准，评估LLM生成样本的有效性，分析不同提示策略（短/长提示变体、思维链提示、少样本提示）的相对效果。

Result: LLM生成的样本在缺乏真实数据时有用，性能达到人类评分者水平，不同提示策略的效果因研究和LLM而异，仅提示改写就有较大差异。

Conclusion: LLM生成的样本在实践中具有应用价值，为如何使用这些样本提供了建议。

Abstract: Personalizing digital applications for health behavior change is a promising route to making them more engaging and effective. This especially holds for approaches that adapt to users and their specific states (e.g., motivation, knowledge, wants) over time. However, developing such approaches requires making many design choices, whose effectiveness is difficult to predict from literature and costly to evaluate in practice. In this work, we explore whether large language models (LLMs) can be used out-of-the-box to generate samples of user interactions that provide useful information for training reinforcement learning models for digital behavior change settings. Using real user data from four large behavior change studies as comparison, we show that LLM-generated samples can be useful in the absence of real data. Comparisons to the samples provided by human raters further show that LLM-generated samples reach the performance of human raters. Additional analyses of different prompting strategies including shorter and longer prompt variants, chain-of-thought prompting, and few-shot prompting show that the relative effectiveness of different strategies depends on both the study and the LLM with also relatively large differences between prompt paraphrases alone. We provide recommendations for how LLM-generated samples can be useful in practice.

</details>


### [73] [Mitigating Catastrophic Forgetting in Streaming Generative and Predictive Learning via Stateful Replay](https://arxiv.org/abs/2511.17936)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 本文研究了在内存约束下使用状态回放方法进行流式学习，通过梯度对齐分析证明了回放机制在异构多任务流中能显著减少灾难性遗忘，在良性时间序列流中与顺序微调表现相似。


<details>
  <summary>Details</summary>
Motivation: 部署的学习系统需要在内存约束下更新流式数据模型。顺序微调方法容易遭受灾难性遗忘，而有限缓冲区的回放方法在生成式和预测式目标下的行为尚未得到充分理解。

Method: 提出了状态回放方法的统一研究框架，将顺序微调和回放视为理想联合目标的随机梯度方法，使用梯度对齐分析来理解混合当前和历史样本如何减少遗忘。在六个流式场景中评估单一回放机制。

Result: 在异构多任务流中，回放将平均遗忘减少2-3倍；在良性时间序列流中，两种方法表现相似。

Conclusion: 状态回放是流式环境中持续学习的一个强大而简单的基线方法。

Abstract: Many deployed learning systems must update models on streaming data under memory constraints. The default strategy, sequential fine-tuning on each new phase, is architecture-agnostic but often suffers catastrophic forgetting when later phases correspond to different sub-populations or tasks. Replay with a finite buffer is a simple alternative, yet its behaviour across generative and predictive objectives is not well understood. We present a unified study of stateful replay for streaming autoencoding, time series forecasting, and classification. We view both sequential fine-tuning and replay as stochastic gradient methods for an ideal joint objective, and use a gradient alignment analysis to show when mixing current and historical samples should reduce forgetting. We then evaluate a single replay mechanism on six streaming scenarios built from Rotated MNIST, ElectricityLoadDiagrams 2011-2014, and Airlines delay data, using matched training budgets and three seeds. On heterogeneous multi task streams, replay reduces average forgetting by a factor of two to three, while on benign time based streams both methods perform similarly. These results position stateful replay as a strong and simple baseline for continual learning in streaming environments.

</details>


### [74] [Enhanced Federated Deep Multi-View Clustering under Uncertainty Scenario](https://arxiv.org/abs/2511.17631)
*Bingjun Wei,Xuemei Cao,Jiafen Liu,Haoyang Liang,Xin Yang*

Main category: cs.LG

TL;DR: 提出了增强联邦深度多视图聚类框架EFDMVC，通过层级对比融合解决视图不确定性，视图自适应漂移模块解决聚合不确定性，平衡聚合机制协调客户端更新，在异构不确定视图下实现鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦多视图聚类假设客户端视图统一，但实际部署中存在异构视图完整性，包含不完整、冗余或损坏数据。现有方法建模视图异构性但忽略动态视图组合带来的语义冲突，未能解决视图不确定性（任意视图配对的语义不一致）和聚合不确定性（客户端更新发散且贡献不平衡）的双重不确定性。

Method: 1. 对齐局部语义，客户端内层级对比融合消除语义冲突解决视图不确定性；2. 视图自适应漂移模块通过全局-局部原型对比动态校正参数偏差缓解聚合不确定性；3. 平衡聚合机制协调客户端更新。

Result: 实验结果表明EFDMVC在多个基准数据集上对异构不确定视图具有优越的鲁棒性，在全面评估中始终优于所有最先进的基线方法。

Conclusion: EFDMVC框架有效解决了联邦多视图聚类中的双重不确定性挑战，为处理异构不确定视图提供了有效的解决方案。

Abstract: Traditional Federated Multi-View Clustering assumes uniform views across clients, yet practical deployments reveal heterogeneous view completeness with prevalent incomplete, redundant, or corrupted data. While recent approaches model view heterogeneity, they neglect semantic conflicts from dynamic view combinations, failing to address dual uncertainties: view uncertainty (semantic inconsistency from arbitrary view pairings) and aggregation uncertainty (divergent client updates with imbalanced contributions). To address these, we propose a novel Enhanced Federated Deep Multi-View Clustering framework: first align local semantics, hierarchical contrastive fusion within clients resolves view uncertainty by eliminating semantic conflicts; a view adaptive drift module mitigates aggregation uncertainty through global-local prototype contrast that dynamically corrects parameter deviations; and a balanced aggregation mechanism coordinates client updates. Experimental results demonstrate that EFDMVC achieves superior robustness against heterogeneous uncertain views across multiple benchmark datasets, consistently outperforming all state-of-the-art baselines in comprehensive evaluations.

</details>


### [75] [On Transportability for Structural Causal Bandits](https://arxiv.org/abs/2511.17953)
*Min Woo Park,Sanghack Lee*

Main category: cs.LG

TL;DR: 该论文研究了具有可迁移性的结构因果赌博机问题，通过融合来自不同环境（观测或实验数据）的先验知识来提升部署环境中的学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有结构因果赌博机框架虽然能利用因果结构知识优化动作空间，但缺乏从不同条件下收集的异构数据集迁移信息的指导方法。

Method: 提出结构因果赌博机与可迁移性结合的方法，利用跨环境的不变性，将源环境的先验知识融合到部署环境中。

Result: 开发出的赌博机算法实现了亚线性遗憾界，明确依赖于先验数据的信息量，可能优于仅依赖在线学习的标准赌博机方法。

Conclusion: 通过利用跨环境的不变性，可以持续改进学习效果，证明在结构因果赌博机中融合先验知识是有效且有益的。

Abstract: Intelligent agents equipped with causal knowledge can optimize their action spaces to avoid unnecessary exploration. The structural causal bandit framework provides a graphical characterization for identifying actions that are unable to maximize rewards by leveraging prior knowledge of the underlying causal structure. While such knowledge enables an agent to estimate the expected rewards of certain actions based on others in online interactions, there has been little guidance on how to transfer information inferred from arbitrary combinations of datasets collected under different conditions -- observational or experimental -- and from heterogeneous environments. In this paper, we investigate the structural causal bandit with transportability, where priors from the source environments are fused to enhance learning in the deployment setting. We demonstrate that it is possible to exploit invariances across environments to consistently improve learning. The resulting bandit algorithm achieves a sub-linear regret bound with an explicit dependence on informativeness of prior data, and it may outperform standard bandit approaches that rely solely on online learning.

</details>


### [76] [Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production](https://arxiv.org/abs/2511.17632)
*Bestoun S. Ahmed,Tommaso Azzalin,Andreas Kassler,Andreas Thore,Hans Lindback*

Main category: cs.LG

TL;DR: 提出基于数字孪生的智能制造方法，通过微服务边缘计算平台和深度强化学习优化钢铁生产，提高可持续性、效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 将传统工业流程转变为智能系统，实现可持续制造目标，强调MLOps在数据驱动制造中的关键作用。

Method: 采用微服务边缘计算平台，通过数字孪生实时处理传感器数据，使用深度强化学习代理优化感应炉加热和功率设置。

Result: 系统能够优化生产流程，减少制造浪费，提高生产质量，并具备适应各种工业应用的灵活性。

Conclusion: 该研究为传统流程向智能系统转型提供了关键步骤，突出了MLOps在塑造数据驱动制造未来中的重要角色。

Abstract: We explore a Digital Twin-Based Approach for Smart Manufacturing to improve Sustainability, Efficiency, and Cost-Effectiveness for a steel production plant. Our system is based on a micro-service edge-compute platform that ingests real-time sensor data from the process into a digital twin over a converged network infrastructure. We implement agile machine learning-based control loops in the digital twin to optimize induction furnace heating, enhance operational quality, and reduce process waste. Key to our approach is a Deep Reinforcement learning-based agent used in our machine learning operation (MLOps) driven system to autonomously correlate the system state with its digital twin to identify correction actions that aim to optimize power settings for the plant. We present the theoretical basis, architectural details, and practical implications of our approach to reduce manufacturing waste and increase production quality. We design the system for flexibility so that our scalable event-driven architecture can be adapted to various industrial applications. With this research, we propose a pivotal step towards the transformation of traditional processes into intelligent systems, aligning with sustainability goals and emphasizing the role of MLOps in shaping the future of data-driven manufacturing.

</details>


### [77] [An Adaptive Resonance Theory-based Topological Clustering Algorithm with a Self-Adjusting Vigilance Parameter](https://arxiv.org/abs/2511.17983)
*Naoki Masuyama,Yuichiro Toda,Yusuke Nojima,Hisao Ishibuchi*

Main category: cs.LG

TL;DR: 提出了一种基于自适应共振理论(ART)的拓扑聚类算法，通过多样性驱动的适应机制自动调整重计算间隔和警戒阈值，实现无超参数学习，在动态环境中保持聚类稳定性和连续性。


<details>
  <summary>Details</summary>
Motivation: 解决静态和非静态设置中的聚类问题，需要能够适应分布变化同时保持先前学习到的聚类结构的模型。

Method: 基于自适应共振理论(ART)的拓扑聚类算法，采用多样性驱动的适应机制来自主调整重计算间隔和警戒阈值。

Result: 在24个真实世界数据集上的实验表明，该算法在聚类性能和持续学习能力方面均优于最先进的方法。

Conclusion: 所提出的参数适应机制在减轻灾难性遗忘和保持演化数据流中一致聚类方面具有有效性。

Abstract: Clustering in stationary and nonstationary settings, where data distributions remain static or evolve over time, requires models that can adapt to distributional shifts while preserving previously learned cluster structures. This paper proposes an Adaptive Resonance Theory (ART)-based topological clustering algorithm that autonomously adjusts its recalculation interval and vigilance threshold through a diversity-driven adaptation mechanism. This mechanism enables hyperparameter-free learning that maintains cluster stability and continuity in dynamic environments. Experiments on 24 real-world datasets demonstrate that the proposed algorithm outperforms state-of-the-art methods in both clustering performance and continual learning capability. These results highlight the effectiveness of the proposed parameter adaptation in mitigating catastrophic forgetting and maintaining consistent clustering in evolving data streams. Source code is available at https://github.com/Masuyama-lab/IDAT

</details>


### [78] [PocketLLM: Ultimate Compression of Large Language Models via Meta Networks](https://arxiv.org/abs/2511.17637)
*Ye Tian,Chengcheng Wang,Jing Han,Yehui Tang,Kai Han*

Main category: cs.LG

TL;DR: PocketLLM是一种通过元网络在潜在空间压缩大语言模型的新方法，使用编码器将模型权重投影到离散潜在向量，通过紧凑码本表示，再用轻量解码器重建权重，实现高压缩比。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断增大，在边缘设备上存储和传输变得困难，传统量化剪枝方法难以在保持精度的同时实现极致压缩。

Method: 提出编码器网络将LLM权重投影到离散潜在向量，用紧凑码本表示，轻量解码器将码本代表向量映射回原始权重空间，仅需小型解码器、简洁码本和索引。

Result: 实验显示PocketLLM在极高压缩比下仍保持优异性能，例如将Llama 2-7B压缩10倍而精度下降可忽略。

Conclusion: PocketLLM通过潜在空间压缩方法实现了大语言模型的高效压缩，在保持精度的同时显著减小模型存储和传输需求。

Abstract: As Large Language Models (LLMs) continue to grow in size, storing and transmitting them on edge devices becomes increasingly challenging. Traditional methods like quantization and pruning struggle to achieve extreme compression of LLMs without sacrificing accuracy. In this paper, we introduce PocketLLM, a novel approach to compress LLMs in a latent space via meta-networks. A simple encoder network is proposed to project the weights of LLMs into discrete latent vectors, which are then represented using a compact codebook. A lightweight decoder network is employed to map the codebook's representative vectors back to the original weight space. This method allows for significant compression of the large weights in LLMs, consisting solely of a small decoder, a concise codebook, and an index. Extensive experiments show that PocketLLM achieves superior performance even at significantly high compression ratios, e.g., compressing Llama 2-7B by 10x with a negligible drop in accuracy.

</details>


### [79] [Learning Rate Scheduling with Matrix Factorization for Private Training](https://arxiv.org/abs/2511.17994)
*Nikita P. Kalinin,Joel Daniel Andersson*

Main category: cs.LG

TL;DR: 该论文研究了在差分隐私模型训练中，结合学习率调度和相关噪声的随机梯度下降方法，提出了学习率感知的矩阵分解方法，相比传统前缀和分解在误差指标上有改进。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私训练理论主要关注恒定学习率下的前缀和分解方法，而实践中广泛使用学习率调度来加速训练和提高收敛性，这之间存在理论空白。

Method: 推导了在单周期和多周期设置下，广泛学习率调度类别的上下界，并提出了学习率感知的矩阵分解方法，构建了适合实际部署的内存高效结构。

Result: 在CIFAR-10和IMDB数据集上的实验证实，调度感知的分解方法在私有训练中提高了准确性，在MaxSE和MeanSE误差指标上优于前缀和分解。

Conclusion: 学习率感知的矩阵分解方法能够有效改善差分隐私模型训练的性能，填补了理论分析与实际应用之间的差距。

Abstract: We study differentially private model training with stochastic gradient descent under learning rate scheduling and correlated noise. Although correlated noise, in particular via matrix factorizations, has been shown to improve accuracy, prior theoretical work focused primarily on the prefix-sum workload. That workload assumes a constant learning rate, whereas in practice learning rate schedules are widely used to accelerate training and improve convergence. We close this gap by deriving general upper and lower bounds for a broad class of learning rate schedules in both single- and multi-epoch settings. Building on these results, we propose a learning-rate-aware factorization that achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics. Our theoretical analysis yields memory-efficient constructions suitable for practical deployment, and experiments on CIFAR-10 and IMDB datasets confirm that schedule-aware factorizations improve accuracy in private training.

</details>


### [80] [Model-to-Model Knowledge Transmission (M2KT): A Data-Free Framework for Cross-Model Understanding Transfer](https://arxiv.org/abs/2511.17638)
*Pratham Sorte*

Main category: cs.LG

TL;DR: 提出M2KT方法，实现无需数据的模型间知识传输，通过概念空间的知识包交换，减少98%数据使用，达到教师模型85-90%性能


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏、迁移学习等方法仍依赖数据驱动，需要教师模型生成样本或梯度，限制了知识传输效率

Method: 在概念空间而非样本空间操作，定义概念流形，引入模型间对齐映射，使用复合损失函数确保几何、结构和推理一致性，包含安全约束

Result: 在大语言模型的符号推理任务中，M2KT达到教师模型85-90%性能，同时减少98%以上数据使用

Conclusion: 为无数据的AI间知识传输和自改进模型生态系统建立了理论和实践基础

Abstract: Modern artificial intelligence systems depend heavily on large datasets for both training and transferring knowledge between models. Knowledge distillation, transfer learning, and dataset distillation have made such transfers more efficient, yet they remain fundamentally data-driven: a teacher must produce examples, logits, or gradients for a student to learn. In this work, we introduce Model-to-Model Knowledge Transmission (M2KT), a novel paradigm for data-free conceptual transfer between neural networks. M2KT enables models to exchange knowledge packets that encapsulate structured concept embeddings, abstraction graphs, reasoning traces, and provenance metadata. Unlike classical distillation, M2KT operates primarily in concept space rather than example space, and it does not require labeled datasets or teacher-generated outputs during transfer. We formalize the notion of concept manifolds, introduce an inter-model alignment mapping between teacher and student latent spaces, and derive a composite loss that enforces geometric, structural, and reasoning consistency together with explicit safety constraints. We further present algorithmic procedures for teacher-side packet generation and student-side ingestion and verification. Experiments on symbolic reasoning with large language models show that M2KT can achieve approximately 85 to 90 percent of teacher performance while reducing data usage by over 98 percent compared to standard knowledge distillation. This work establishes a theoretical and practical foundation for data-free AI-to-AI knowledge transfer and self-improving model ecosystems.

</details>


### [81] [Hierarchical Linkage Clustering Beyond Binary Trees and Ultrametrics](https://arxiv.org/abs/2511.18056)
*Maximilien Dreveton,Matthias Grossglauser,Daichi Kuroda,Patrick Thiran*

Main category: cs.LG

TL;DR: 本文提出了有效层次结构的概念，定义了对有效层次结构的偏序，证明了最精细有效层次结构的存在性，并开发了一种两步算法来恢复该结构。


<details>
  <summary>Details</summary>
Motivation: 传统层次聚类方法存在三个主要局限：总是返回层次结构（即使不存在）、仅限于二叉树（即使真实结构是非二元的）、对连接函数选择高度敏感。

Method: 提出了有效层次结构的概念，定义了对有效层次结构的偏序，证明了最精细有效层次结构的存在性，并开发了一种两步算法（先通过连接方法构建二叉树，然后修剪以强制有效性）。

Result: 建立了连接函数在何种条件下能精确恢复最精细有效层次结构的充要条件，证明了满足这些条件的连接函数在修剪后产生相同的层次结构。经典连接规则（单连接、全连接、平均连接）满足条件，而Ward连接不满足。

Conclusion: 该方法解决了传统层次聚类的局限性，能够识别数据中是否存在层次结构，支持非二叉树结构，并且对连接函数选择具有鲁棒性。

Abstract: Hierarchical clustering seeks to uncover nested structures in data by constructing a tree of clusters, where deeper levels reveal finer-grained relationships. Traditional methods, including linkage approaches, face three major limitations: (i) they always return a hierarchy, even if none exists, (ii) they are restricted to binary trees, even if the true hierarchy is non-binary, and (iii) they are highly sensitive to the choice of linkage function. In this paper, we address these issues by introducing the notion of a valid hierarchy and defining a partial order over the set of valid hierarchies. We prove the existence of a finest valid hierarchy, that is, the hierarchy that encodes the maximum information consistent with the similarity structure of the data set. In particular, the finest valid hierarchy is not constrained to binary structures and, when no hierarchical relationships exist, collapses to a star tree. We propose a simple two-step algorithm that first constructs a binary tree via a linkage method and then prunes it to enforce validity. We establish necessary and sufficient conditions on the linkage function under which this procedure exactly recovers the finest valid hierarchy, and we show that all linkage functions satisfying these conditions yield the same hierarchy after pruning. Notably, classical linkage rules such as single, complete, and average satisfy these conditions, whereas Ward's linkage fails to do so.

</details>


### [82] [TTF: A Trapezoidal Temporal Fusion Framework for LTV Forecasting in Douyin](https://arxiv.org/abs/2511.17639)
*Yibing Wan,Zhengxiong Guan,Chaoli Zhang,Xiaoyang Li,Lai Xu,Beibei Jia,Zhenzhe Zheng,Fan Wu*

Main category: cs.LG

TL;DR: 提出了TTF框架解决用户生命周期价值(LTV)预测中的多时间序列不对齐、短输入长输出(SILO)和数据波动性三大挑战，在抖音平台上部署后显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 互联网公司需要优化付费获客渠道的预算分配，关键在于早期预测渠道级LTV以最大化LTV/CAC比率，但传统时间序列预测方法难以应对LTV预测的特殊挑战。

Method: 提出梯形时间融合(TTF)框架，包含梯形多时间序列模块处理数据不对齐和SILO问题，以及多塔融合网络(MT-FusionNet)进行准确预测。

Result: 在抖音平台部署后，LTV曲线的点级MAPE降低了4.3%，聚合LTV的MAPE降低了3.2%。

Conclusion: TTF框架有效解决了LTV预测中的关键挑战，在实际应用中显著提升了预测精度，为预算优化提供了可靠支持。

Abstract: In the user growth scenario, Internet companies invest heavily in paid acquisition channels to acquire new users. But sustainable growth depends on acquired users' generating lifetime value (LTV) exceeding customer acquisition cost (CAC). In order to maximize LTV/CAC ratio, it is crucial to predict channel-level LTV in an early stage for further optimization of budget allocation. The LTV forecasting problem is significantly different from traditional time series forecasting problems, and there are three main challenges. Firstly, it is an unaligned multi-time series forecasting problem that each channel has a number of LTV series of different activation dates. Secondly, to predict in the early stage, it faces the imbalanced short-input long-output (SILO) challenge. Moreover, compared with the commonly used time series datasets, the real LTV series are volatile and non-stationary, with more frequent fluctuations and higher variance. In this work, we propose a novel framework called Trapezoidal Temporal Fusion (TTF) to address the above challenges. We introduce a trapezoidal multi-time series module to deal with data unalignment and SILO challenges, and output accurate predictions with a multi-tower structure called MT-FusionNet. The framework has been deployed to the online system for Douyin. Compared to the previously deployed online model, MAPEp decreased by 4.3%, and MAPEa decreased by 3.2%, where MAPEp denotes the point-wise MAPE of the LTV curve and MAPEa denotes the MAPE of the aggregated LTV.

</details>


### [83] [Active Learning with Selective Time-Step Acquisition for PDEs](https://arxiv.org/abs/2511.18107)
*Yegon Kim,Hyunsu Kim,Gyeonghoon Ko,Juho Lee*

Main category: cs.LG

TL;DR: 提出了一种用于PDE代理建模的主动学习框架，通过策略性地仅生成最重要的时间步长来显著降低训练数据生成成本，从而提高代理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高昂，而代理模型开发受到生成足够训练数据成本的限制。现有PDE主动学习方法总是获取整个轨迹，成本过高。

Method: 开发了一个主动学习框架，策略性地仅用数值求解器生成最重要的时间步长，同时使用代理模型近似剩余步骤。提出了一个获取函数来估计时间步长集的效用。

Result: 在多个基准PDE上验证了方法的有效性，包括Burgers方程、KdV方程、Kuramoto-Sivashinsky方程、不可压缩和可压缩Navier-Stokes方程。实验显示性能大幅优于现有最佳方法，不仅降低平均误差，还降低了99%、95%和50%分位数的误差。

Conclusion: 该方法为PDE代理建模提供了一个数据高效的解决方案，显著降低了训练数据生成成本，同时提高了模型性能。

Abstract: Accurately solving partial differential equations (PDEs) is critical to understanding complex scientific and engineering phenomena, yet traditional numerical solvers are computationally expensive. Surrogate models offer a more efficient alternative, but their development is hindered by the cost of generating sufficient training data from numerical solvers. In this paper, we present a novel framework for active learning (AL) in PDE surrogate modeling that reduces this cost. Unlike the existing AL methods for PDEs that always acquire entire PDE trajectories, our approach strategically generates only the most important time steps with the numerical solver, while employing the surrogate model to approximate the remaining steps. This dramatically reduces the cost incurred by each trajectory and thus allows the active learning algorithm to try out a more diverse set of trajectories given the same budget. To accommodate this novel framework, we develop an acquisition function that estimates the utility of a set of time steps by approximating its resulting variance reduction. We demonstrate the effectiveness of our method on several benchmark PDEs, including the Burgers' equation, Korteweg-De Vries equation, Kuramoto-Sivashinsky equation, the incompressible Navier-Stokes equation, and the compressible Navier-Stokes equation. Experiments show that our approach improves performance by large margins over the best existing method. Our method not only reduces average error but also the 99\%, 95\%, and 50\% quantiles of error, which is rare for an AL algorithm. All in all, our approach offers a data-efficient solution to surrogate modeling for PDEs.

</details>


### [84] [BlockCert: Certified Blockwise Extraction of Transformer Mechanisms](https://arxiv.org/abs/2511.17645)
*Sandro Andric*

Main category: cs.LG

TL;DR: BlockCert框架为Transformer机制提供经过认证的块级提取，通过机器可检查的证书来约束近似误差、记录覆盖指标并哈希底层构件，将局部保证提升到全局偏差边界。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性和模型编辑领域通常缺乏关于提取或编辑模型在相关输入上与原模型偏离程度的明确保证，需要形式化验证方法。

Method: 引入BlockCert框架进行Transformer机制的认证块级提取，使用Lipschitz-based组合定理将局部保证提升到全局偏差边界，并在Lean 4中形式化验证。

Result: 在GPT-2 small、TinyLlama-1.1B-Chat和Llama-3.2-3B上获得高块级覆盖率和小的残差误差，在TinyLlama设置中完全拼接模型在压力提示上与基线困惑度匹配在约6e-5内。

Conclusion: 块级提取与显式证书对于真实Transformer语言模型是可行的，为机制可解释性和模型行为的形式推理提供了实用桥梁。

Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.

</details>


### [85] [Adaptive Conformal Prediction for Quantum Machine Learning](https://arxiv.org/abs/2511.18225)
*Douglas Spencer,Samual Nicholls,Michele Caprio*

Main category: cs.LG

TL;DR: 提出了自适应量子保形预测(AQCP)算法，通过持续重新校准来应对量子处理器中的时变噪声，在任意硬件噪声条件下保持渐近平均覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习需要可靠的不确定性量化方法，但现有量子保形预测方法无法应对量子处理器固有的时变噪声，这会破坏保形保证。

Method: 基于自适应保形推理框架，通过持续重新校准来适应量子硬件中的时变噪声，开发了自适应量子保形预测算法。

Result: 在IBM量子处理器上的实证研究表明，AQCP能够达到目标覆盖水平，并且比量子保形预测表现出更好的稳定性。

Conclusion: AQCP算法能够有效应对量子硬件噪声，为量子机器学习提供了更可靠的不确定性量化方法。

Abstract: Quantum machine learning seeks to leverage quantum computers to improve upon classical machine learning algorithms. Currently, robust uncertainty quantification methods remain underdeveloped in the quantum domain, despite the critical need for reliable and trustworthy predictions. Recent work has introduced quantum conformal prediction, a framework that produces prediction sets that are guaranteed to contain the true outcome with user-specified probability. In this work, we formalise how the time-varying noise inherent in quantum processors can undermine conformal guarantees, even when calibration and test data are exchangeable. To address this challenge, we draw on Adaptive Conformal Inference, a method which maintains validity over time via repeated recalibration. We introduce Adaptive Quantum Conformal Prediction (AQCP), an algorithm which preserves asymptotic average coverage guarantees under arbitrary hardware noise conditions. Empirical studies on an IBM quantum processor demonstrate that AQCP achieves target coverage levels and exhibits greater stability than quantum conformal prediction.

</details>


### [86] [MamTiff-CAD: Multi-Scale Latent Diffusion with Mamba+ for Complex Parametric Sequence](https://arxiv.org/abs/2511.17647)
*Liyuan Deng,Yunpeng Bai,Yongkang Dai,Xiaoshui Huang,Hongping Gan,Dongshuo Huang,Hao jiacheng,Yilei Shi*

Main category: cs.LG

TL;DR: MamTiff-CAD是一个基于Transformer扩散模型的CAD参数命令序列生成框架，通过Mamba+和Transformer的混合架构处理长序列CAD命令，在60-256长度的序列生成任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂CAD模型的几何和拓扑约束时，难以生成长序列参数命令，需要解决长序列依赖问题。

Method: 设计Mamba+和Transformer混合自编码器，将参数化CAD序列转换为潜在表示；Mamba+块引入遗忘门机制捕获长程依赖；基于多尺度Transformer的扩散模型学习长序列命令分布。

Result: 在重建和生成任务上达到最先进性能，成功生成长度60-256的CAD模型序列。

Conclusion: MamTiff-CAD框架有效解决了长序列CAD参数命令生成问题，为工业应用提供了可靠解决方案。

Abstract: Parametric Computer-Aided Design (CAD) is crucial in industrial applications, yet existing approaches often struggle to generate long sequence parametric commands due to complex CAD models' geometric and topological constraints. To address this challenge, we propose MamTiff-CAD, a novel CAD parametric command sequences generation framework that leverages a Transformer-based diffusion model for multi-scale latent representations. Specifically, we design a novel autoencoder that integrates Mamba+ and Transformer, to transfer parameterized CAD sequences into latent representations. The Mamba+ block incorporates a forget gate mechanism to effectively capture long-range dependencies. The non-autoregressive Transformer decoder reconstructs the latent representations. A diffusion model based on multi-scale Transformer is then trained on these latent embeddings to learn the distribution of long sequence commands. In addition, we also construct a dataset that consists of long parametric sequences, which is up to 256 commands for a single CAD model. Experiments demonstrate that MamTiff-CAD achieves state-of-the-art performance on both reconstruction and generation tasks, confirming its effectiveness for long sequence (60-256) CAD model generation.

</details>


### [87] [Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors](https://arxiv.org/abs/2511.18615)
*Jiawei Hu,Javier A. Barria*

Main category: cs.LG

TL;DR: 提出了FMAPLS和online-FMAPLS两种贝叶斯框架，用于解决标签偏移问题，通过联合优化Dirichlet超参数和类别先验，显著提升了分类器在测试数据分布变化时的性能。


<details>
  <summary>Details</summary>
Motivation: 标签偏移是监督学习中的常见挑战，当测试数据的类别先验分布与训练数据不同时，会导致分类器性能显著下降。现有MAPLS方法存在刚性约束限制，需要更灵活有效的解决方案。

Method: 使用批处理和在线EM算法联合优化Dirichlet超参数α和类别先验π，引入线性替代函数替代基于梯度的超参数更新，获得闭式解降低计算复杂度。在线版本用随机近似替代批处理E步，实现实时适应流数据。

Result: 在CIFAR100和ImageNet数据集上的实验表明，FMAPLS和online-FMAPLS分别实现了高达40%和12%的KL散度降低，并在后偏移准确率上显著优于现有最优基线方法，特别是在严重类别不平衡和分布不确定性情况下。

Conclusion: 所提出的方法在鲁棒性、可扩展性和动态学习场景适用性方面表现出色，证实了其在大规模和动态学习环境中的有效性。

Abstract: Label shift, a prevalent challenge in supervised learning, arises when the class prior distribution of test data differs from that of training data, leading to significant degradation in classifier performance. To accurately estimate the test priors and enhance classification accuracy, we propose a Bayesian framework for label shift estimation, termed Full Maximum A Posterior Label Shift (FMAPLS), along with its online version, online-FMAPLS. Leveraging batch and online Expectation-Maximization (EM) algorithms, these methods jointly and dynamically optimize Dirichlet hyperparameters $\boldsymbolα$ and class priors $\boldsymbolπ$, thereby overcoming the rigid constraints of the existing Maximum A Posterior Label Shift (MAPLS) approach. Moreover, we introduce a linear surrogate function (LSF) to replace gradient-based hyperparameter updates, yielding closed-form solutions that reduce computational complexity while retaining asymptotic equivalence. The online variant substitutes the batch E-step with a stochastic approximation, enabling real-time adaptation to streaming data. Furthermore, our theoretical analysis reveals a fundamental trade-off between online convergence rate and estimation accuracy. Extensive experiments on CIFAR100 and ImageNet datasets under shuffled long-tail and Dirichlet test priors demonstrate that FMAPLS and online-FMAPLS respectively achieve up to 40% and 12% lower KL divergence and substantial improvements in post-shift accuracy over state-of-the-art baselines, particularly under severe class imbalance and distributional uncertainty. These results confirm the robustness, scalability, and suitability of the proposed methods for large-scale and dynamic learning scenarios.

</details>


### [88] [Frugality in second-order optimization: floating-point approximations for Newton's method](https://arxiv.org/abs/2511.17660)
*Giuseppe Carrino,Elena Loli Piccolomini,Elisa Riccietti,Theo Mary*

Main category: cs.LG

TL;DR: 该论文分析了有限精度算术对牛顿步长的影响，建立了混合精度牛顿优化器的收敛定理，并提出了GN_k方法，在回归任务中达到与完整牛顿法相当的性能但需要更少的导数计算。


<details>
  <summary>Details</summary>
Motivation: 虽然高阶优化方法如牛顿法能提供更高的准确性和更快的收敛速度，但由于计算成本高而在实际应用中较少使用。本研究旨在解决这一矛盾。

Method: 1. 分析有限精度算术对牛顿步长的影响，建立混合精度牛顿优化器的收敛定理 2. 提出GN_k方法，允许部分计算二阶导数

Result: 在标准回归基准测试中，所提出的方法在Australian和MUSH数据集上优于Adam优化器。GN_k在回归任务中达到与完整牛顿法相当的性能，同时显著减少导数计算次数。

Conclusion: 混合精度牛顿优化器和GN_k方法能够有效平衡计算成本与性能，为实际应用中采用高阶优化方法提供了可行方案。

Abstract: Minimizing loss functions is central to machine-learning training. Although first-order methods dominate practical applications, higher-order techniques such as Newton's method can deliver greater accuracy and faster convergence, yet are often avoided due to their computational cost. This work analyzes the impact of finite-precision arithmetic on Newton steps and establishes a convergence theorem for mixed-precision Newton optimizers, including "quasi" and "inexact" variants. The theorem provides not only convergence guarantees but also a priori estimates of the achievable solution accuracy. Empirical evaluations on standard regression benchmarks demonstrate that the proposed methods outperform Adam on the Australian and MUSH datasets. The second part of the manuscript introduces GN_k, a generalized Gauss-Newton method that enables partial computation of second-order derivatives. GN_k attains performance comparable to full Newton's method on regression tasks while requiring significantly fewer derivative evaluations.

</details>


### [89] [Majority of the Bests: Improving Best-of-N via Bootstrapping](https://arxiv.org/abs/2511.18630)
*Amin Rakhsha,Kanika Madan,Tianyu Zhang,Amir-massoud Farahmand,Amir Khasahmadi*

Main category: cs.LG

TL;DR: 提出Majority-of-the-Bests (MoB)方法，通过自助采样估计BoN的输出分布并选择其众数，在奖励模型不完美时比传统Best-of-N方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 当奖励模型不完美时，Best-of-N方法无法可靠找到正确答案，性能急剧下降。虽然正确答案在输出分布中概率不高，但通常是众数。

Method: 通过自助采样估计BoN的输出分布，选择该分布的众数作为最终输出。

Result: 在5个基准测试、3个基础LLM和2个奖励模型的30个设置中，25个设置表现优于BoN。

Conclusion: MoB是BoN和自一致性的简单而强大的替代方案，激励对更细致选择机制的研究。

Abstract: Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.

</details>


### [90] [Enhancing Breast Cancer Prediction with LLM-Inferred Confounders](https://arxiv.org/abs/2511.17662)
*Debmita Roy*

Main category: cs.LG

TL;DR: 使用大语言模型从临床数据中推断糖尿病、肥胖和心血管疾病等混杂疾病的概率，提升乳腺癌预测的随机森林模型性能。


<details>
  <summary>Details</summary>
Motivation: 通过AI生成的特征来增强乳腺癌预测，特别是利用大语言模型从常规临床数据中推断混杂疾病的可能性。

Method: 使用大语言模型（如Gemma和Llama）从临床数据中生成特征，然后输入随机森林模型进行乳腺癌预测。

Result: AI生成的特征显著提升了随机森林模型的性能，Gemma提升了3.9%，Llama提升了6.4%。

Conclusion: 该方法在非侵入性预筛查和临床整合方面具有潜力，支持乳腺癌早期检测和共享决策的改进。

Abstract: This study enhances breast cancer prediction by using large language models to infer the likelihood of confounding diseases, namely diabetes, obesity, and cardiovascular disease, from routine clinical data. These AI-generated features improved Random Forest model performance, particularly for LLMs like Gemma (3.9%) and Llama (6.4%). The approach shows promise for noninvasive prescreening and clinical integration, supporting improved early detection and shared decision-making in breast cancer diagnosis.

</details>


### [91] [Subtract the Corruption: Training-Data-Free Corrective Machine Unlearning using Task Arithmetic](https://arxiv.org/abs/2511.18660)
*Mostafa Mozafari,Farooq Ahmad Wani,Maria Sofia Bucarelli,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 提出CUTS方法，在无法访问原始训练数据和被污染样本的情况下，通过代理集进行源自由的纠正性机器遗忘，有效消除模型中的污染影响。


<details>
  <summary>Details</summary>
Motivation: 现实场景中训练数据往往不可访问，且无法确定被污染的具体样本，现有基于遗忘集的纠正性机器遗忘方法失效，需要开发源自由的方法。

Method: CUTS方法：将干净信号和污染信号视为不同任务，在权重空间进行修正。先对污染模型在代理集上微调放大污染机制，计算污染权重与微调权重的差异作为代理任务向量，然后减去校准后的该向量来消除污染。

Result: 在标签噪声下恢复大部分丢失的效用，对于后门触发器几乎完全消除攻击且对效用损害最小，在源自由设置下优于最先进的专门CMU方法。

Conclusion: CUTS提供了一种轻量级的权重空间修正方法，无需访问干净数据或遗忘集，在源自由环境下有效实现纠正性机器遗忘。

Abstract: Corrupted training data are ubiquitous. Corrective Machine Unlearning (CMU) seeks to remove the influence of such corruption post-training. Prior CMU typically assumes access to identified corrupted training samples (a ``forget set''). However, in many real-world scenarios the training data are no longer accessible. We formalize \emph{source-free} CMU, where the original training data are unavailable and, consequently, no forget set of identified corrupted training samples can be specified. Instead, we assume a small proxy (surrogate) set of corrupted samples that reflect the suspected corruption type without needing to be the original training samples. In this stricter setting, methods relying on forget set are ineffective or narrow in scope. We introduce \textit{Corrective Unlearning in Task Space} (CUTS), a lightweight weight space correction method guided by the proxy set using task arithmetic principles. CUTS treats the clean and the corruption signal as distinct tasks. Specifically, we briefly fine-tune the corrupted model on the proxy to amplify the corruption mechanism in the weight space, compute the difference between the corrupted and fine-tuned weights as a proxy task vector, and subtract a calibrated multiple of this vector to cancel the corruption. Without access to clean data or a forget set, CUTS recovers a large fraction of the lost utility under label noise and, for backdoor triggers, nearly eliminates the attack with minimal damage to utility, outperforming state-of-the-art specialized CMU methods in source-free setting.

</details>


### [92] [AI-based framework to predict animal and pen feed intake in feedlot beef cattle](https://arxiv.org/abs/2511.17663)
*Alex S. C. Maia,John B. Hall,Hugo F. M. Milan,Izabelle A. M. A. Teixeira*

Main category: cs.LG

TL;DR: 开发了一个基于AI的框架，利用环境指数和机器学习模型准确预测个体动物和围栏级别的饲料摄入量。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏充分利用纵向大数据来准确预测饲料摄入量的方法，特别是在考虑环境条件的情况下。

Method: 使用19个实验的1650万+样本数据和环境数据，开发了两个环境指数（InComfort-Index和EASI-Index），并结合机器学习模型（XGBoost）进行预测。

Result: 最佳模型在个体动物级别的预测准确度为RMSE 1.38 kg/天，在围栏级别为0.14 kg/(天-动物)。

Conclusion: 该框架为精准管理肉牛提供了可靠工具，有助于减少饲料浪费、优化资源和适应气候变化。

Abstract: Advances in technology are transforming sustainable cattle farming practices, with electronic feeding systems generating big longitudinal datasets on individual animal feed intake, offering the possibility for autonomous precision livestock systems. However, the literature still lacks a methodology that fully leverages these longitudinal big data to accurately predict feed intake accounting for environmental conditions. To fill this gap, we developed an AI-based framework to accurately predict feed intake of individual animals and pen-level aggregation. Data from 19 experiments (>16.5M samples; 2013-2024) conducted at Nancy M. Cummings Research Extension & Education Center (Carmen, ID) feedlot facility and environmental data from AgriMet Network weather stations were used to develop two novel environmental indices: InComfort-Index, based solely on meteorological variables, showed good predictive capability for thermal comfort but had limited ability to predict feed intake; EASI-Index, a hybrid index integrating environmental variables with feed intake behavior, performed well in predicting feed intake but was less effective for thermal comfort. Together with the environmental indices, machine learning models were trained and the best-performing machine learning model (XGBoost) accuracy was RMSE of 1.38 kg/day for animal-level and only 0.14 kg/(day-animal) at pen-level. This approach provides a robust AI-based framework for predicting feed intake in individual animals and pens, with potential applications in precision management of feedlot cattle, through feed waste reduction, resource optimization, and climate-adaptive livestock management.

</details>


### [93] [OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting](https://arxiv.org/abs/2511.18732)
*Haoming Jia,Yi Han,Xiang Wang,Huizan Wang,Wei Wu,Jianming Zheng,Peikun Xiao*

Main category: cs.LG

TL;DR: 提出了OceanForecastBench，一个用于数据驱动海洋预报的开源标准化基准，包含28年高质量再分析数据、高可靠性观测数据以及评估流程和基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的海洋预报模型缺乏开源标准化基准，导致数据使用和评估方法不一致，阻碍了模型开发、性能比较和跨学科合作。

Method: 构建包含三个核心贡献的基准框架：(1)28年全球海洋再分析数据；(2)卫星和现场观测数据用于评估；(3)评估流程和6个典型基线模型。

Result: 创建了目前最全面的数据驱动海洋预报基准框架，为模型开发、评估和比较提供了开源平台。

Conclusion: OceanForecastBench解决了海洋预报领域缺乏标准化基准的问题，将促进数据驱动海洋预报模型的发展和应用。

Abstract: Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.

</details>


### [94] [CubeletWorld: A New Abstraction for Scalable 3D Modeling](https://arxiv.org/abs/2511.17664)
*Azlaan Mustafa Samad,Hoang H. Nguyen,Lukas Berg,Henrik Müller,Yuan Xue,Daniel Kudenko,Zahra Ahmadi*

Main category: cs.LG

TL;DR: CubeletWorld是一个新颖的城市环境表示框架，通过离散化的3D网格单元（cubelets）来整合异构城市数据，支持隐私保护的规划、导航和占用预测任务。


<details>
  <summary>Details</summary>
Motivation: 现代城市产生大量异构数据，但将这些数据整合成连贯的空间模型用于规划和预测仍然是一个主要挑战。现有基于智能体的方法依赖直接环境感知，限制了可扩展性并引发隐私问题。

Method: 提出CubeletWorld框架，使用称为cubelets的离散化3D空间单元来表示城市环境，将基础设施、移动和环境指标等多样化数据信号嵌入到局部化的cubelet状态中。

Result: 评估了CubeletWorld状态预测任务，探索了适用于该设置的修改核心模型，分析了空间粒度增加带来的稀疏性表示和基线可扩展性挑战。

Conclusion: CubeletWorld提供了一个灵活可扩展的框架，用于从复杂城市数据中学习，为可扩展仿真和决策支持开辟了新可能性，在人口统计建模、环境监测和应急响应等领域具有应用前景。

Abstract: Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here.

</details>


### [95] [Sampling Control for Imbalanced Calibration in Semi-Supervised Learning](https://arxiv.org/abs/2511.18773)
*Senmao Tian,Xiang Wei,Shunli Zhang*

Main category: cs.LG

TL;DR: SC-SSL是一个统一的半监督学习框架，通过解耦采样控制来抑制类别不平衡导致的模型偏差，在训练和推理阶段分别处理特征级和权重不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习中类别不平衡导致的模型偏差问题，现有方法通常粗粒度地处理模型不平衡，混淆了数据不平衡与类别特定学习难度带来的偏差。

Method: 提出SC-SSL框架，通过解耦采样控制：训练阶段使用具有显式扩展能力的分类器并自适应调整采样概率；推理阶段分析线性分类器的权重不平衡，应用后处理采样控制优化偏置向量来校准logits。

Result: 在多个基准数据集和不同分布设置下的广泛实验验证了SC-SSL的一致性和最先进性能。

Conclusion: SC-SSL通过解耦采样控制有效缓解了半监督学习中的类别不平衡问题，在训练和推理阶段分别处理特征级和权重不平衡，取得了优异的性能表现。

Abstract: Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL.

</details>


### [96] [GANGR: GAN-Assisted Scalable and Efficient Global Routing Parallelization](https://arxiv.org/abs/2511.17665)
*Hadi Khodaei Jooshin,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: 提出了一种基于Wasserstein生成对抗网络(WGANs)的新型全局路由批处理算法，相比传统启发式方法，能生成更少但质量更高的批次，在ISPD'24基准测试中实现40%运行时间减少，路由质量仅下降0.002%。


<details>
  <summary>Details</summary>
Motivation: 传统全局路由批处理方法依赖计算昂贵的启发式算法，导致批次过大、批次数量过多、生成时间过长等问题，限制了可扩展性和效率。

Method: 使用Wasserstein生成对抗网络(WGANs)增强的新型批处理算法，能够更有效地进行并行化处理。

Result: 在ISPD'24竞赛基准测试中，相比最先进的路由器，运行时间减少高达40%，路由质量仅下降0.002%。

Conclusion: 基于WGANs的批处理算法显著提高了全局路由的效率和可扩展性，同时保持了高质量的路由结果。

Abstract: Global routing is a critical stage in electronic design automation (EDA) that enables early estimation and optimization of the routability of modern integrated circuits with respect to congestion, power dissipation, and design complexity. Batching is a primary concern in top-performing global routers, grouping nets into manageable sets to enable parallel processing and efficient resource usage. This process improves memory usage, scalable parallelization on modern hardware, and routing congestion by controlling net interactions within each batch. However, conventional batching methods typically depend on heuristics that are computationally expensive and can lead to suboptimal results (oversized batches with conflicting nets, excessive batch counts degrading parallelization, and longer batch generation times), ultimately limiting scalability and efficiency. To address these limitations, a novel batching algorithm enhanced with Wasserstein generative adversarial networks (WGANs) is introduced in this paper, enabling more effective parallelization by generating fewer higher-quality batches in less time. The proposed algorithm is tested on the latest ISPD'24 contest benchmarks, demonstrating up to 40% runtime reduction with only 0.002% degradation in routing quality as compared to state-of-the-art router.

</details>


### [97] [Doubly Wild Refitting: Model-Free Evaluation of High Dimensional Black-Box Predictions under Convex Losses](https://arxiv.org/abs/2511.18789)
*Haichen Hu,David Simchi-Levi*

Main category: cs.LG

TL;DR: 提出一种高效的再拟合程序，用于计算经验风险最小化在凸损失函数下的超额风险，并提供高概率上界。该方法通过生成伪标签数据来评估现代不透明机器学习系统的风险。


<details>
  <summary>Details</summary>
Motivation: 传统基于容量的学习理论在处理极度复杂的假设类（如深度神经网络和生成模型）时变得不可行，需要一种模型无关的方法来评估现代不透明机器学习系统的超额风险。

Method: 使用随机扰动梯度向量生成两组伪标签数据（wild response），然后对黑盒训练程序进行两次再拟合得到两个wild预测器，最后结合原始预测器、wild预测器和wild响应来推导超额风险上界。

Result: 该方法能够高效计算超额风险并提供高概率上界，不需要先验了解底层函数类的复杂度，具有模型无关的特性。

Conclusion: 该方法为理论上评估现代不透明机器学习系统提供了有前景的解决方案，特别是在传统容量理论不可行的情况下。

Abstract: We study the problem of excess risk evaluation for empirical risk minimization (ERM) under general convex loss functions. Our contribution is an efficient refitting procedure that computes the excess risk and provides high-probability upper bounds under the fixed-design setting. Assuming only black-box access to the training algorithm and a single dataset, we begin by generating two sets of artificially modified pseudo-outcomes termed wild response, created by stochastically perturbing the gradient vectors with carefully chosen scaling. Using these two pseudo-labeled datasets, we then refit the black-box procedure twice to obtain two corresponding wild predictors. Finally, leveraging the original predictor, the two wild predictors, and the constructed wild responses, we derive an efficient excess risk upper bound. A key feature of our analysis is that it requires no prior knowledge of the complexity of the underlying function class. As a result, the method is essentially model-free and holds significant promise for theoretically evaluating modern opaque machine learning system--such as deep nerral networks and generative model--where traditional capacity-based learning theory becomes infeasible due to the extreme complexity of the hypothesis class.

</details>


### [98] [Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous Vehicles](https://arxiv.org/abs/2511.17675)
*Navneet Singh,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出了一种紧凑的混合量子架构，用于自动驾驶轨迹预测，通过残差学习、量子注意力编码和傅里叶解码，在Waymo数据集上实现了优于运动学基线的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶轨迹预测需要在计算和延迟约束下提供准确、校准的多模态未来预测，现有方法在效率和性能方面存在挑战。

Method: 使用混合量子架构：量子注意力编码器(9量子比特)、参数精简的量子前馈堆栈(64层)、傅里叶解码器，在自我中心车道对齐框架中进行残差学习，通过SPSA训练。

Result: 在Waymo数据集上，minADE为1.94m，minFDE为3.56m，在16个预测模型中优于运动学基线，减少了漏检率并提高了召回率。

Conclusion: 残差学习、车道框架、截断傅里叶解码、浅层纠缠和基于频谱的排序能够有效聚焦容量，从小型浅层量子电路中产生稳定的多模态预测。

Abstract: Trajectory forecasting for autonomous driving must deliver accurate, calibrated multi-modal futures under tight compute and latency constraints. We propose a compact hybrid quantum architecture that aligns quantum inductive bias with road-scene structure by operating in an ego-centric, lane-aligned frame and predicting residual corrections to a kinematic baseline instead of absolute poses. The model combines a transformer-inspired quantum attention encoder (9 qubits), a parameter-lean quantum feedforward stack (64 layers, ${\sim}1200$ trainable angles), and a Fourier-based decoder that uses shallow entanglement and phase superposition to generate 16 trajectory hypotheses in a single pass, with mode confidences derived from the latent spectrum. All circuit parameters are trained with Simultaneous Perturbation Stochastic Approximation (SPSA), avoiding backpropagation through non-analytic components. In the Waymo Open Motion Dataset, the model achieves minADE (minimum Average Displacement Error) of \SI{1.94}{m} and minFDE (minimum Final Displacement Error) of \SI{3.56}{m} in the $16$ models predicted over the horizon of \SI{2.0}{s}, consistently outperforming a kinematic baseline with reduced miss rates and strong recall. Ablations confirm that residual learning in the lane frame, truncated Fourier decoding, shallow entanglement, and spectrum-based ranking focus capacity where it matters, yielding stable optimization and reliable multi-modal forecasts from small, shallow quantum circuits on a modern autonomous-driving benchmark.

</details>


### [99] [Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery](https://arxiv.org/abs/2511.18940)
*Sanjeev Manivannan,Chandrashekar Lakshminarayan*

Main category: cs.LG

TL;DR: 提出了几何感知预处理模块和深度同余网络，直接在SPD流形上处理协方差矩阵，在零样本跨被试运动想象解码任务中比最强基线提升3-4%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决EEG脑机接口中跨被试运动想象解码的挑战，特别是由于被试间变异性和SPD流形的弯曲几何特性导致的性能下降问题。

Method: 引入DCR和RiFU预处理模块扩展黎曼对齐，提出SPD-DCNet和RiFUNet两个流形分类器，使用分层同余变换学习判别性、被试不变的协方差表示。

Result: 在BCI-IV 2a基准测试中，跨被试准确率比最强经典基线提高3-4%。

Conclusion: 几何感知变换对于稳健的EEG解码具有重要价值，特别是在零样本跨被试设置下。

Abstract: Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces due to strong subject variability and the curved geometry of covariance matrices on the symmetric positive definite (SPD) manifold. We address the zero-shot cross-subject setting, where no target-subject labels or adaptation are allowed, by introducing novel geometry-aware preprocessing modules and deep congruence networks that operate directly on SPD covariance matrices. Our preprocessing modules, DCR and RiFU, extend Riemannian Alignment by improving action separation while reducing subject-specific distortions. We further propose two manifold classifiers, SPD-DCNet and RiFUNet, which use hierarchical congruence transforms to learn discriminative, subject-invariant covariance representations. On the BCI-IV 2a benchmark, our framework improves cross-subject accuracy by 3-4% over the strongest classical baselines, demonstrating the value of geometry-aware transformations for robust EEG decoding.

</details>


### [100] [A Hybrid Classical-Quantum Fine Tuned BERT for Text Classification](https://arxiv.org/abs/2511.17677)
*Abu Kaisar Mohammad Masum,Naveed Mahmud,M. Hassan Najafi,Sercan Aygun*

Main category: cs.LG

TL;DR: 提出了一种将n量子比特量子电路与经典BERT模型结合的混合方法用于文本分类，实验结果表明该混合模型在标准基准数据集上具有竞争力甚至优于经典基线方法。


<details>
  <summary>Details</summary>
Motivation: BERT微调在文本分类中计算成本高且需要仔细的超参数调优，而量子算法在机器学习和文本分类任务中显示出超越传统方法的潜力。

Method: 集成n量子比特量子电路与经典BERT模型的混合方法，用于文本分类任务。

Result: 混合模型在标准基准数据集上表现具有竞争力，在某些情况下优于经典基线方法，证明了经典-量子模型在不同数据集上微调预训练模型的适应性。

Conclusion: 该混合模型展示了量子计算在提升文本分类任务性能方面的潜力，为这一研究领域的发展提供了可行性证明。

Abstract: Fine-tuning BERT for text classification can be computationally challenging and requires careful hyper-parameter tuning. Recent studies have highlighted the potential of quantum algorithms to outperform conventional methods in machine learning and text classification tasks. In this work, we propose a hybrid approach that integrates an n-qubit quantum circuit with a classical BERT model for text classification. We evaluate the performance of the fine-tuned classical-quantum BERT and demonstrate its feasibility as well as its potential in advancing this research area. Our experimental results show that the proposed hybrid model achieves performance that is competitive with, and in some cases better than, the classical baselines on standard benchmark datasets. Furthermore, our approach demonstrates the adaptability of classical-quantum models for fine-tuning pre-trained models across diverse datasets. Overall, the hybrid model highlights the promise of quantum computing in achieving improved performance for text classification tasks.

</details>


### [101] [The Core in Max-Loss Non-Centroid Clustering Can Be Empty](https://arxiv.org/abs/2511.19107)
*Robert Bredereck,Eva Deltl,Leon Kellerhals,Jannik Peters*

Main category: cs.LG

TL;DR: 本文研究了在最大损失目标下的非质心聚类中的核心稳定性问题，证明了对于k≥3的情况，存在度量实例使得任何聚类的α-核心都不存在（α<2^(1/5)≈1.148）。


<details>
  <summary>Details</summary>
Motivation: 研究非质心聚类中核心稳定性的存在性问题，填补该领域在最大损失目标下的理论空白。

Method: 通过理论证明和计算机辅助证明相结合的方法，构造了特定的度量实例和二维欧几里得点集。

Result: 证明了对于k≥3且n≥9（n可被k整除）的情况，不存在α-核心（α<2^(1/5)）。这是该目标下的首个不可能性结果。

Conclusion: 在最大损失目标下的非质心聚类中，核心可能是空的，这为理解聚类稳定性提供了重要理论洞见。

Abstract: We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\geq 3$ there exist metric instances with $n\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $α$-core for any $α<2^{\frac{1}{5}}\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective.

</details>


### [102] [Boosting Brain-inspired Path Integration Efficiency via Learning-based Replication of Continuous Attractor Neurodynamics](https://arxiv.org/abs/2511.17687)
*Zhangyu Ge,Xu He,Lingfei Mo,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lansong Jiang,Fengyuan Liu*

Main category: cs.LG

TL;DR: 提出了一种使用表示学习模型复制连续吸引子神经网络神经动力学模式的高效路径整合方法，在保持定位精度的同时显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有脑启发导航研究中的路径整合机制存在显著计算冗余和效率问题，不利于该技术的实际应用。

Method: 使用轻量级人工神经网络复制HDC和GC的神经动力学模式，并将这些模型集成以实现脑启发的航位推算路径整合。

Result: 在多种环境中的基准测试显示，该方法不仅准确复制了导航细胞的神经动力学模式，定位精度与NeuroSLAM相当，而且在通用设备上效率提升约17.5%，在边缘设备上提升40~50%。

Conclusion: 这项工作为增强脑启发导航技术的实用性提供了一种新颖的实现策略，并具有进一步扩展的潜力。

Abstract: The brain's Path Integration (PI) mechanism offers substantial guidance and inspiration for Brain-Inspired Navigation (BIN). However, the PI capability constructed by the Continuous Attractor Neural Networks (CANNs) in most existing BIN studies exhibits significant computational redundancy, and its operational efficiency needs to be improved; otherwise, it will not be conducive to the practicality of BIN technology. To address this, this paper proposes an efficient PI approach using representation learning models to replicate CANN neurodynamic patterns. This method successfully replicates the neurodynamic patterns of CANN-modeled Head Direction Cells (HDCs) and Grid Cells (GCs) using lightweight Artificial Neural Networks (ANNs). These ANN-reconstructed HDC and GC models are then integrated to achieve brain-inspired PI for Dead Reckoning (DR). Benchmark tests in various environments, compared with the well-known NeuroSLAM system, demonstrate that this work not only accurately replicates the neurodynamic patterns of navigation cells but also matches NeuroSLAM in positioning accuracy. Moreover, efficiency improvements of approximately 17.5% on the general-purpose device and 40~50% on the edge device were observed, compared with NeuroSLAM. This work offers a novel implementation strategy to enhance the practicality of BIN technology and holds potential for further extension.

</details>


### [103] [Masked Diffusion Models are Secretly Learned-Order Autoregressive Models](https://arxiv.org/abs/2511.19152)
*Prateek Garg,Bhavya Kohli,Sunita Sarawagi*

Main category: cs.LG

TL;DR: 本文提出了一种训练框架，通过多变量噪声调度来优化掩码扩散模型的解码顺序，证明MDM目标可以分解为这些顺序上的加权自回归损失。


<details>
  <summary>Details</summary>
Motivation: 观察到MDM在随机顺序下解码令牌，且该顺序对性能有显著影响，因此研究能否设计训练框架来优化解码顺序。

Method: 使用多变量噪声调度的连续时间变分目标，建立解码顺序与多变量噪声调度之间的直接对应关系，并证明MDM目标可分解为加权自回归损失。

Result: 成功识别并优化解码顺序，打破了MDM目标对噪声调度的不变性，建立了MDM作为具有可学习顺序的自回归模型的理论基础。

Conclusion: MDM可以通过多变量噪声调度优化解码顺序，其目标可分解为加权自回归损失，为可学习顺序的自回归建模提供了理论支持。

Abstract: Masked Diffusion Models (MDMs) have emerged as one of the most promising paradigms for generative modeling over discrete domains. It is known that MDMs effectively train to decode tokens in a random order, and that this ordering has significant performance implications in practice. This observation raises a fundamental question: can we design a training framework that optimizes for a favorable decoding order? We answer this in the affirmative, showing that the continuous-time variational objective of MDMs, when equipped with multivariate noise schedules, can identify and optimize for a decoding order during training. We establish a direct correspondence between decoding order and the multivariate noise schedule and show that this setting breaks invariance of the MDM objective to the noise schedule. Furthermore, we prove that the MDM objective decomposes precisely into a weighted auto-regressive losses over these orders, which establishes them as auto-regressive models with learnable orders.

</details>


### [104] [Enhancing Adversarial Transferability through Block Stretch and Shrink](https://arxiv.org/abs/2511.17688)
*Quan Liu,Feng Ye,Chenhao Lu,Shuming Zhen,Guanliang Huang,Lunzhe Chen,Xudong Ke*

Main category: cs.LG

TL;DR: 提出Block Stretch and Shrink (BSS)方法，通过将图像分块并进行拉伸和收缩操作来增强对抗样本的迁移性，在ImageNet子集上表现优于现有输入变换攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有输入变换攻击方法的跨模型迁移性有限，研究表明高迁移性与多样化的注意力热图和保持全局语义相关。

Method: 将图像分成块，对这些块应用拉伸和收缩操作，从而在变换输入中多样化注意力热图，同时保持其全局语义。

Result: 在ImageNet子集上的实证评估表明，BSS在迁移性方面优于现有的输入变换攻击方法。

Conclusion: BSS方法通过分块拉伸收缩操作有效提升了对抗样本的迁移性，并建议在统一数量尺度下评估输入变换攻击方法以确保公平比较。

Abstract: Adversarial attacks introduce small, deliberately crafted perturbations that mislead neural networks, and their transferability from white-box to black-box target models remains a critical research focus. Input transformation-based attacks are a subfield of adversarial attacks that enhance input diversity through input transformations to improve the transferability of adversarial examples. However, existing input transformation-based attacks tend to exhibit limited cross-model transferability. Previous studies have shown that high transferability is associated with diverse attention heatmaps and the preservation of global semantics in transformed inputs. Motivated by this observation, we propose Block Stretch and Shrink (BSS), a method that divides an image into blocks and applies stretch and shrink operations to these blocks, thereby diversifying attention heatmaps in transformed inputs while maintaining their global semantics. Empirical evaluations on a subset of ImageNet demonstrate that BSS outperforms existing input transformation-based attack methods in terms of transferability. Furthermore, we examine the impact of the number scale, defined as the number of transformed inputs, in input transformation-based attacks, and advocate evaluating these methods under a unified number scale to enable fair and comparable assessments.

</details>


### [105] [Local Entropy Search over Descent Sequences for Bayesian Optimization](https://arxiv.org/abs/2511.19241)
*David Stenger,Armin Lindicke,Alexander von Rohr,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 提出局部熵搜索(LES)方法，通过贝叶斯优化明确针对迭代优化器的下降序列可达解，在复杂设计空间中实现强样本效率


<details>
  <summary>Details</summary>
Motivation: 在大型复杂设计空间中搜索全局最优解通常不可行且不必要，更实用的替代方案是使用梯度下降等局部优化方法迭代细化初始设计的邻域

Method: LES算法通过优化器传播目标函数的后验信念，生成下降序列的概率分布，然后通过分析熵计算和下降序列的蒙特卡洛采样，最大化与该分布的互信息来选择下一个评估点

Result: 在高复杂度合成目标和基准问题上的实证结果表明，与现有局部和全局贝叶斯优化方法相比，LES实现了强大的样本效率

Conclusion: 局部熵搜索为复杂设计空间的优化提供了一种高效的贝叶斯优化范式，特别适用于迭代优化器可达解的搜索

Abstract: Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.

</details>


### [106] [DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams](https://arxiv.org/abs/2511.17693)
*Ginés Carreto Picón,Peng Yuan Zhou,Qi Zhang,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: 提出了DeepCoT（深度持续Transformer），一种无冗余的编码器模型，可在现有深度编码器架构上应用，显著减少流数据推理中的计算冗余。


<details>
  <summary>Details</summary>
Motivation: Transformer模型规模不断增大，但在资源受限设备上需要低延迟推理。流数据推理在滑动时间窗口上产生高度冗余计算，现有持续Transformer只能用于浅层模型，限制了泛化能力。

Method: 开发了DeepCoT，一种无冗余的编码器模型，可最小化改动应用于现有深度编码器架构，消除流数据推理中的计算冗余。

Result: 在音频、视频和文本流上的实验表明，DeepCoT在保持与非持续基线相当性能的同时，为所有Transformer层提供线性计算成本，运行时间比先前高效模型减少高达两个数量级。

Conclusion: DeepCoT成功解决了深度Transformer模型在流数据推理中的计算冗余问题，实现了高效的低延迟推理，同时保持模型性能。

Abstract: Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.

</details>


### [107] [Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme](https://arxiv.org/abs/2511.19390)
*Rudy Morel,Francesco Pio Ramunno,Jeff Shen,Alberto Bietti,Kyunghyun Cho,Miles Cranmer,Siavash Golkar,Olexandr Gugnin,Geraud Krawezik,Tanya Marwah,Michael McCabe,Lucas Meyer,Payel Mukhopadhyay,Ruben Ohana,Liam Parker,Helen Qu,François Rozet,K. D. Leka,François Lanusse,David Fouhey,Shirley Ho*

Main category: cs.LG

TL;DR: 提出了一种用于部分可观测长记忆动力系统概率预测的多尺度推理方案，特别针对太阳动力学应用，解决了标准自回归方法无法有效捕捉长期依赖关系的问题。


<details>
  <summary>Details</summary>
Motivation: 许多动态系统（如太阳物理）只能观测到部分状态，缺乏内部过程的直接测量，而标准推理方案无法有效整合过去信息来捕捉长期依赖关系。

Method: 提出多尺度推理方案，生成在时间上近细远粗的轨迹，在不增加计算成本的情况下捕捉长期时间依赖关系，并将其集成到扩散模型中。

Result: 该推理方案显著减少了预测分布的偏差，并提高了展开稳定性。

Conclusion: 多尺度推理方案能够有效解决部分可观测长记忆动力系统的概率预测问题，在太阳动力学等应用中表现出优越性能。

Abstract: Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.

</details>


### [108] [Periodicity-Enforced Neural Network for Designing Deterministic Lateral Displacement Devices](https://arxiv.org/abs/2511.17754)
*Andrew Lee,Mahir Mobarrat,Xiaolin Chen*

Main category: cs.LG

TL;DR: 提出一种周期性增强的代理建模方法，通过周期性层确保确定性侧向位移(DLD)微流控设备单元边界的精确周期性匹配，显著提高多单元设备预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统DLD设备设计需要计算昂贵的Navier-Stokes模拟和粒子追踪分析，现有深度学习方法无法充分处理关键周期性边界条件，导致多单元设备预测中的累积误差。

Method: 采用周期性层神经网络组件，构建三个子网络预测稳态非维速度场和压力场(u, v, p)，通过架构强制而非惩罚项确保单元边界的精确周期性匹配。

Result: 在120个CFD生成几何体上的验证显示，周期性层实现达到0.478%临界直径误差，同时保持完美的周期性一致性，比基线方法提升85.4%。

Conclusion: 该方法实现了高效准确的DLD设备设计，保证多单元设备应用中边界条件的满足，为液体活检癌症检测提供可靠工具。

Abstract: Deterministic Lateral Displacement (DLD) devices enable liquid biopsy for cancer detection by separating circulating tumor cells (CTCs) from blood samples based on size, but designing these microfluidic devices requires computationally expensive Navier-Stokes simulations and particle-tracing analyses. While recent surrogate modeling approaches using deep learning have accelerated this process, they often inadequately handle the critical periodic boundary conditions of DLD unit cells, leading to cumulative errors in multi-unit device predictions. This paper introduces a periodicity-enforced surrogate modeling approach that incorporates periodic layers, neural network components that guarantee exact periodicity without penalty terms or output modifications, into deep learning architectures for DLD device design. The proposed method employs three sub-networks to predict steady-state, non-dimensional velocity and pressure fields (u, v, p) rather than directly predicting critical diameters or particle trajectories, enabling complete flow field characterization and enhanced design flexibility. Periodic layers ensure exact matching of flow variables across unit cell boundaries through architectural enforcement rather than soft penalty-based approaches. Validation on 120 CFD-generated geometries demonstrates that the periodic layer implementation achieves 0.478% critical diameter error while maintaining perfect periodicity consistency, representing an 85.4% improvement over baseline methods. The approach enables efficient and accurate DLD device design with guaranteed boundary condition satisfaction for multi-unit device applications.

</details>


### [109] [PrismSSL: One Interface, Many Modalities; A Single-Interface Library for Multimodal Self-Supervised Learning](https://arxiv.org/abs/2511.17776)
*Melika Shirian,Kianoosh Vadaei,Kian Majlessi,Audrina Ebrahimi,Arshia Hemmat,Peyman Adibi,Hossein Karshenas*

Main category: cs.LG

TL;DR: PrismSSL是一个统一的Python库，集成了音频、视觉、图和跨模态的最先进自监督学习方法，提供模块化代码库和图形化仪表板。


<details>
  <summary>Details</summary>
Motivation: 为研究人员和从业者提供一个统一的框架，简化自监督学习的安装、配置和训练过程，支持多种模态和方法扩展。

Method: 采用模块化设计，提供训练器和数据集抽象，集成HuggingFace Transformers，支持分布式训练、超参数搜索、LoRA微调等功能，并包含基于Flask的图形化仪表板。

Result: 开发了PrismSSL库，已打包在PyPI上发布，采用MIT许可证，提供了完整的代码和数据配方，确保可复现性。

Conclusion: PrismSSL成功创建了一个统一、易用的自监督学习框架，显著降低了使用门槛，支持多种模态和方法的快速部署与扩展。

Abstract: We present PrismSSL, a Python library that unifies state-of-the-art self-supervised learning (SSL) methods across audio, vision, graphs, and cross-modal settings in a single, modular codebase. The goal of the demo is to show how researchers and practitioners can: (i) install, configure, and run pretext training with a few lines of code; (ii) reproduce compact benchmarks; and (iii) extend the framework with new modalities or methods through clean trainer and dataset abstractions. PrismSSL is packaged on PyPI, released under the MIT license, integrates tightly with HuggingFace Transformers, and provides quality-of-life features such as distributed training in PyTorch, Optuna-based hyperparameter search, LoRA fine-tuning for Transformer backbones, animated embedding visualizations for sanity checks, Weights & Biases logging, and colorful, structured terminal logs for improved usability and clarity. In addition, PrismSSL offers a graphical dashboard - built with Flask and standard web technologies - that enables users to configure and launch training pipelines with minimal coding. The artifact (code and data recipes) will be publicly available and reproducible.

</details>


### [110] [Data-Driven Predictive Modeling of Microfluidic Cancer Cell Separation Using a Deterministic Lateral Displacement Device](https://arxiv.org/abs/2511.17787)
*Elizabeth Chen,Andrew Lee,Tanbir Sarowar,Xiaolin Chen*

Main category: cs.LG

TL;DR: 本研究利用机器学习模型优化确定性侧向位移（DLD）微流控设备的设计参数，以提高肺癌细胞分离效率，为癌症早期诊断提供数据驱动的自动化设计框架。


<details>
  <summary>Details</summary>
Motivation: 确定性侧向位移设备在无标记、基于尺寸的颗粒和细胞分离中广泛应用，特别是在循环肿瘤细胞分离方面具有潜力。但稀有CTC检测的挑战和计算密集型模拟的依赖需要更高效的优化方法。

Method: 采用梯度提升、k近邻、随机森林和多层感知器等机器学习模型，基于大量数值验证数据集预测粒子轨迹并识别最优设备配置参数（如行移分数、柱尺寸和间隙距离）。

Result: 机器学习模型能够准确预测粒子轨迹，识别关键设计变量，实现高通量、成本效益高的DLD设备设计优化。

Conclusion: 这种集成方法推进了可扩展和精确微流控系统的发展，为癌症早期检测和个性化医疗提供了系统化的数据驱动优化框架。

Abstract: Deterministic Lateral Displacement (DLD) devices are widely used in microfluidics for label-free, size-based separation of particles and cells, with particular promise in isolating circulating tumor cells (CTCs) for early cancer diagnostics. This study focuses on the optimization of DLD design parameters, such as row shift fraction, post size, and gap distance, to enhance the selective isolation of lung cancer cells based on their physical properties. To overcome the challenges of rare CTC detection and reduce reliance on computationally intensive simulations, machine learning models including gradient boosting, k-nearest neighbors, random forest, and multilayer perceptron (MLP) regressors are employed. Trained on a large, numerically validated dataset, these models predict particle trajectories and identify optimal device configurations, enabling high-throughput and cost-effective DLD design. Beyond trajectory prediction, the models aid in isolating critical design variables, offering a systematic, data-driven framework for automated DLD optimization. This integrative approach advances the development of scalable and precise microfluidic systems for cancer diagnostics, contributing to the broader goals of early detection and personalized medicine.

</details>


### [111] [Physical Reinforcement Learning](https://arxiv.org/abs/2511.17789)
*Sam Dillavou,Shruti Mishra*

Main category: cs.LG

TL;DR: 本文展示了如何使用对比局部学习网络（CLLNs）解决强化学习问题，通过将Q学习算法适配到模拟的CLLNs中，实现了两个简单RL任务的成功。


<details>
  <summary>Details</summary>
Motivation: 数字计算机功耗高且对组件损坏敏感，不适合能源受限的自主智能体在不确定环境中使用。CLLNs作为模拟自调节非线性电阻网络，具有低功耗和物理损伤鲁棒性的优势，但之前仅用于监督学习。

Method: 将Q学习算法适配到模拟的CLLNs中，明确识别了RL工具箱中除训练网络外所需的其他组件，包括策略函数、价值函数和回放缓冲区等。

Result: 在两个简单的强化学习问题上取得了成功，证明了CLLNs在RL任务中的可行性。

Conclusion: CLLNs能够放弃数字硬件所需的物理安全假设，支持生物学中重要但在数字计算机中无意义的次要目标训练，为低功耗、鲁棒的自主智能体提供了有前景的替代方案。

Abstract: Digital computers are power-hungry and largely intolerant of damaged components, making them potentially difficult tools for energy-limited autonomous agents in uncertain environments. Recently developed Contrastive Local Learning Networks (CLLNs) - analog networks of self-adjusting nonlinear resistors - are inherently low-power and robust to physical damage, but were constructed to perform supervised learning. In this work we demonstrate success on two simple RL problems using Q-learning adapted for simulated CLLNs. Doing so makes explicit the components (beyond the network being trained) required to enact various tools in the RL toolbox, some of which (policy function and value function) are more natural in this system than others (replay buffer). We discuss assumptions such as the physical safety that digital hardware requires, CLLNs can forgo, and biological systems cannot rely on, and highlight secondary goals that are important in biology and trainable in CLLNs, but make little sense in digital computers.

</details>


### [112] [Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2511.17801)
*Cuong Pham,Hoang Anh Dung,Cuong C. Nguyen,Trung Le,Gustavo Carneiro,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出了一种基于二次优化的层特定高影响参数选择框架，用于在极低位宽量化中平衡计算效率和模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法在极低位宽下精度损失严重，且固定比例的高影响参数策略忽略了层间敏感性差异。

Method: 使用二次优化框架确定层特定的高影响参数比例，考虑层间依赖关系，对高影响参数采用中等位宽量化，其余参数使用极低位宽量化。

Result: 在相同资源约束下，比FP16保留方法能保留更多高影响参数，实现了计算效率和模型精度的有效平衡。

Conclusion: 该方法在保持高性能的同时，相比现有最先进方法在计算效率和准确性之间取得了更好的平衡。

Abstract: Large language models (LLMs) have significantly advanced natural language processing, but their massive parameter counts create substantial computational and memory challenges during deployment. Post-training quantization (PTQ) has emerged as a promising approach to mitigate these challenges with minimal overhead. While existing PTQ methods can effectively quantize LLMs, they experience substantial accuracy loss at extremely low bit-widths, primarily due to high-impact parameters that significantly influence quantization performance. Several approaches address these issues by identifying and retaining the high-impact parameters in FP16 format. However, they apply fixed ratios of high-impact parameters across all layers, overlooking layer-wise sensitivity variations. In this paper, we propose a quadratic optimization framework that determines layer-specific ratios of high-impact parameters while considering inter-layer dependencies. We quantize high-impact parameters to moderate bit-widths, which often result in negligible performance degradation in quantized LLMs, while the remaining parameters can be quantized to extremely low bit-widths. Under the same resource-constrained budget, this allows for preserving more high-impact parameters than methods that keep selecting a few in FP16 format. Additionally, the proposed framework allows us to leverage an advanced quantization method that often requires extensive learnable parameters solely for high-impact parameters, while applying a computationally efficient method to the rest. Our approach achieves an effective balance between computational efficiency and model accuracy while maintaining high performance compared to state-of-the-art methods.

</details>


### [113] [Adaptive Layer-Wise Transformations for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2511.17809)
*Cuong Pham,Hoang Anh Dung,Cuong C. Nguyen,Trung Le,Gustavo Carneiro,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出自适应变换选择框架，通过逐层选择最优变换类型来解决LLM量化中的异质分布问题，显著提升低比特量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法采用同质变换设置，忽略了LLM中激活和权重的异质分布特性，导致在低比特设置下性能显著下降。

Method: 将变换选择建模为可微分优化问题，建立权重分布峰度与准确变换类型的关联，提出基于鲁棒z-score归一化的离群值引导层选择方法。

Result: 在LLaMA系列模型上，W3A3K2V2量化设置下比当前最佳方法FlatQuant提升4.58困惑度点和2.11%的六任务零样本准确率。

Conclusion: 异质变换选择对于实现最优LLM量化至关重要，自适应方法在减少计算开销的同时保持了高性能。

Abstract: Large language models require significant computational resources for deployment, making quantization essential for practical applications. However, the main obstacle to effective quantization lies in systematic outliers in activations and weights, which cause substantial LLM performance degradation, especially at low-bit settings. While existing transformation-based methods like affine and rotation transformations successfully mitigate outliers, they apply the homogeneous transformation setting, i.e., using the same transformation types across all layers, ignoring the heterogeneous distribution characteristics within LLMs. In this paper, we propose an adaptive transformation selection framework that systematically determines optimal transformations on a per-layer basis. To this end, we first formulate transformation selection as a differentiable optimization problem to achieve the accurate transformation type for each layer. However, searching for optimal layer-wise transformations for every model is computationally expensive. To this end, we establish the connection between weight distribution kurtosis and accurate transformation type. Specifically, we propose an outlier-guided layer selection method using robust $z$-score normalization that achieves comparable performance to differentiable search with significantly reduced overhead. Comprehensive experiments on LLaMA family models demonstrate that our adaptive approach consistently outperforms the widely-used fixed transformation settings. For example, our method achieves an improvement of up to 4.58 perplexity points and a 2.11% gain in average six-task zero-shot accuracy under aggressive W3A3K2V2 quantization settings for the LLaMA-3-8B model compared to the current best existing method, FlatQuant, demonstrating the necessity of heterogeneous transformation selection for optimal LLM quantization.

</details>


### [114] [APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs](https://arxiv.org/abs/2511.17818)
*Aishwarya Mandyam,Kalyani Limaye,Barbara E. Engelhardt,Emily Alsentzer*

Main category: cs.LG

TL;DR: 使用大型语言模型生成反事实标注来增强离线策略评估，解决医疗领域数据集覆盖不足的问题


<details>
  <summary>Details</summary>
Motivation: 标准离线策略评估方法受限于行为数据集的大小和覆盖范围，而人工获取专家标注的反事实注释成本高昂，限制了方法的可扩展性

Method: 利用领域知识指导LLMs预测在替代治疗下关键临床特征的演变，然后通过已知奖励函数将这些预测特征转换为反事实标注，并将其整合到OPE估计器中

Result: 在MIMIC-IV数据集的两个患者子集上评估，最先进的LLMs在预测临床特征方面达到可比较性能；在大多数情况下，基于LLM的反事实标注显著改善了OPE估计，直到某个临界点

Conclusion: 基于LLM的反事实标注为解决医疗数据集的覆盖限制提供了一种可扩展的方法，能够在临床环境中更安全地部署决策策略

Abstract: Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment. As such, OPE plays a critical role in ensuring safety in high-stakes domains such as healthcare. However, standard OPE approaches are limited by the size and coverage of the behavior dataset. While previous work has explored using expert-labeled counterfactual annotations to enhance dataset coverage, obtaining such annotations is expensive, limiting the scalability of prior approaches. We propose leveraging large language models (LLMs) to generate counterfactual annotations for OPE in medical domains. Our method uses domain knowledge to guide LLMs in predicting how key clinical features evolve under alternate treatments. These predicted features can then be transformed using known reward functions to create counterfactual annotations. We first evaluate the ability of several LLMs to predict clinical features across two patient subsets in MIMIC-IV, finding that state-of-the-art LLMs achieve comparable performance. Building on this capacity to predict clinical features, we generate LLM-based counterfactual annotations and incorporate them into an OPE estimator. Our empirical results analyze the benefits of counterfactual annotations under varying degrees of shift between the behavior and target policies. We find that in most cases, the LLM-based counterfactual annotations significantly improve OPE estimates up to a point. We provide an entropy-based metric to identify when additional annotations cease to be useful. Our results demonstrate that LLM-based counterfactual annotations offer a scalable approach for addressing coverage limitations in healthcare datasets, enabling safer deployment of decision-making policies in clinical settings.

</details>


### [115] [Unified Class and Domain Incremental Learning with Mixture of Experts for Indoor Localization](https://arxiv.org/abs/2511.17829)
*Akhil Singampalli,Sudeep Pasricha*

Main category: cs.LG

TL;DR: MOELO是一个新颖的持续学习框架，首次联合解决室内定位中的领域增量学习和类别增量学习问题，通过混合专家架构实现轻量级、鲁棒且自适应的定位解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统室内定位机器学习模型面临硬件/软件差异导致的领域偏移和室内环境变化导致的类别偏移问题，静态模型在长期使用中效果下降。

Method: 采用混合专家架构，按区域增量训练专家，通过等角紧框架门控机制实现高效路由和低延迟推理，保持紧凑模型尺寸。

Result: 相比现有最优框架，MOELO在平均定位误差上提升25.6倍，最差情况定位误差提升44.5倍，遗忘率降低21.5倍。

Conclusion: MOELO为资源受限移动设备提供了有效的持续学习解决方案，能够在动态异构环境中实现长期可靠的室内定位。

Abstract: Indoor localization using machine learning has gained traction due to the growing demand for location-based services. However, its long-term reliability is hindered by hardware/software variations across mobile devices, which shift the model's input distribution to create domain shifts. Further, evolving indoor environments can introduce new locations over time, expanding the output space to create class shifts, making static machine learning models ineffective over time. To address these challenges, we propose a novel unified continual learning framework for indoor localization called MOELO that, for the first time, jointly addresses domain-incremental and class-incremental learning scenarios. MOELO enables a lightweight, robust, and adaptive localization solution that can be deployed on resource-limited mobile devices and is capable of continual learning in dynamic, heterogeneous real-world settings. This is made possible by a mixture-of-experts architecture, where experts are incrementally trained per region and selected through an equiangular tight frame based gating mechanism ensuring efficient routing, and low-latency inference, all within a compact model footprint. Experimental evaluations show that MOELO achieves improvements of up to 25.6x in mean localization error, 44.5x in worst-case localization error, and 21.5x lesser forgetting compared to state-of-the-art frameworks across diverse buildings, mobile devices, and learning scenarios.

</details>


### [116] [Internalizing Tools as Morphisms in Graded Transformers](https://arxiv.org/abs/2511.17840)
*Tony Shaska*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a graded formulation of internal symbolic computation for transformers. The hidden space is endowed with a grading $V=\bigoplus_{g\in G}V_g$, and symbolic operations are realized as typed block maps (morphisms) $φ_{h\leftarrow g}:V_g\to V_h$ that are activated selectively by a differentiable routing policy. A self-supervised \emph{graded utility functional}, defined as the loss reduction induced by a candidate morphism, governs activation and yields sparse, interpretable behavior. We develop the algebraic and geometric foundations: an internal model category whose objects are homogeneous components and whose morphisms are admissible grade transitions; adjoint pairs encoding typed round trips; and information-geometric interpretations in terms of KL gain, mirror descent with Bregman divergences, and Fisher natural gradients. Methodologically, we specify a utility--aware routing mechanism and objective that remain fully end-to-end differentiable. Analytic case studies and lightweight sanity checks illustrate selective morphic activation on hybrid symbolic-linguistic tasks. The framework unifies symbolic computation, geometry, and self--supervised learning within the \emph{graded transformer} formalism \cite{sh-89,sh-95}, while subsuming prior external-tool paradigms (e.g., Toolformer \cite{toolformer2023}) as a special case via functorial internalization.

</details>


### [117] [Scaling Kinetic Monte-Carlo Simulations of Grain Growth with Combined Convolutional and Graph Neural Networks](https://arxiv.org/abs/2511.17848)
*Zhihui Tian,Ethan Suwandi,Tomas Oppelstrup,Vasily V. Bulatov,Joel B. Harley,Fei Zhou*

Main category: cs.LG

TL;DR: 提出结合CNN自编码器和GNN的混合架构，用于高效模拟晶粒生长，显著降低计算成本和内存占用，同时提高准确性和时空建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在模拟大规模晶界网络时面临计算成本和内存占用过高的问题，难以扩展到实际应用所需的大尺寸模拟单元。

Method: 使用基于CNN的双射自编码器压缩空间维度，在降维后的潜在空间中使用GNN演化微观结构，减少消息传递层数（从12层降至3层）。

Result: 在最大网格（160^3）上，内存使用和推理运行时间分别减少117倍和115倍，相比纯GNN基线具有更高的准确性和更强的时空能力，尤其在长期测试中表现更佳。

Conclusion: 该方法为模拟晶粒生长提供了高度可扩展的解决方案，结合了自编码器的无损信息压缩能力和GNN的学习能力，在保持准确性的同时显著提升计算效率。

Abstract: Graph neural networks (GNN) have emerged as a promising machine learning method for microstructure simulations such as grain growth. However, accurate modeling of realistic grain boundary networks requires large simulation cells, which GNN has difficulty scaling up to. To alleviate the computational costs and memory footprint of GNN, we propose a hybrid architecture combining a convolutional neural network (CNN) based bijective autoencoder to compress the spatial dimensions, and a GNN that evolves the microstructure in the latent space of reduced spatial sizes. Our results demonstrate that the new design significantly reduces computational costs with using fewer message passing layer (from 12 down to 3) compared with GNN alone. The reduction in computational cost becomes more pronounced as the spatial size increases, indicating strong computational scalability. For the largest mesh evaluated (160^3), our method reduces memory usage and runtime in inference by 117x and 115x, respectively, compared with GNN-only baseline. More importantly, it shows higher accuracy and stronger spatiotemporal capability than the GNN-only baseline, especially in long-term testing. Such combination of scalability and accuracy is essential for simulating realistic material microstructures over extended time scales. The improvements can be attributed to the bijective autoencoder's ability to compress information losslessly from spatial domain into a high dimensional feature space, thereby producing more expressive latent features for the GNN to learn from, while also contributing its own spatiotemporal modeling capability. The training was optimized to learn from the stochastic Potts Monte Carlo method. Our findings provide a highly scalable approach for simulating grain growth.

</details>


### [118] [Equivalence of Context and Parameter Updates in Modern Transformer Blocks](https://arxiv.org/abs/2511.17864)
*Adrian Goldwaser,Michael Munn,Javier Gonzalvo,Benoit Dherin*

Main category: cs.LG

TL;DR: 本文扩展了上下文对transformer影响的理论，证明了在现代LLM架构中，整个上下文效应可以完美映射为MLP权重矩阵的rank-1补丁和RMSNorm尺度补丁，并提出了输入可控性和输出可控性的通用框架。


<details>
  <summary>Details</summary>
Motivation: 扩展先前关于上下文在transformer中影响的理论，使其适用于现代大型语言模型的多样化架构，提供更统一和强大的理解框架。

Method: 首先为Gemma风格transformer块提供精确解析解，然后推广到多层模型，提出基于输入可控性和输出可控性的通用框架和构造性证明算法。

Result: 证明了对于任何MLP块，只要内部函数输入可控且外部函数输出可控，就可以实现完美的隐式权重补丁，该框架适用于包括门控、预/后归一化、专家混合和顺序/并行transformer块在内的多种现代LLM架构。

Conclusion: 提供了一个更简单强大的理论框架来理解transformer模型如何将提示转换为有效权重，统一了多种现代LLM架构中的上下文效应表示方法。

Abstract: Recent research has established that the impact of context in a vanilla transformer can be represented implicitly by forming a token-dependent, rank-1 patch to its MLP weights. This work extends that foundational theory to the diverse architectures of modern Large Language Models. We first demonstrate a precise, analytical solution for a Gemma-style transformer block, proving that the entire effect of a context can be perfectly mapped to rank-1 patches on its MLP weight matrices and a patch to the RMSNorm scale. We then generalize this result, providing a constructive proof and algorithm for multi-layer models. To unify these findings, we introduce a general framework centered on two core properties: input controllability and output controllability. We prove that a perfect implicit weight patch is possible for any MLP block where the inner function is input-controllable and the outer function is output-controllable. This provides a simpler and more powerful lens for understanding how transformer models transmute prompts into effective weights. This setup generalizes to a wide range of modern LLM architectures including gating, pre-/post-norm, mixture of experts and sequential/parallel transformer blocks.

</details>


### [119] [The Horcrux: Mechanistically Interpretable Task Decomposition for Detecting and Mitigating Reward Hacking in Embodied AI Systems](https://arxiv.org/abs/2511.17869)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出了MITD分层Transformer架构，通过可解释的任务分解来检测和缓解奖励黑客攻击，在1000个HH-RLHF样本上实验显示12-25步分解深度可将奖励黑客频率降低34%。


<details>
  <summary>Details</summary>
Motivation: 现有具身AI代理通过奖励黑客攻击利用奖励信号缺陷，获得高代理分数但无法实现真实目标，需要更有效的检测方法。

Method: 引入机械可解释任务分解(MITD)架构，包含规划器、协调器和执行器模块，将任务分解为可解释子任务，并生成注意力瀑布图和神经通路流程图等诊断可视化。

Result: 在1000个HH-RLHF样本上的实验表明，12-25步的分解深度可将四种失败模式下的奖励黑客频率降低34%。

Conclusion: 基于机械原理的分解比事后行为监控能更有效地检测奖励黑客攻击，提供了新的检测范式。

Abstract: Embodied AI agents exploit reward signal flaws through reward hacking, achieving high proxy scores while failing true objectives. We introduce Mechanistically Interpretable Task Decomposition (MITD), a hierarchical transformer architecture with Planner, Coordinator, and Executor modules that detects and mitigates reward hacking. MITD decomposes tasks into interpretable subtasks while generating diagnostic visualizations including Attention Waterfall Diagrams and Neural Pathway Flow Charts. Experiments on 1,000 HH-RLHF samples reveal that decomposition depths of 12 to 25 steps reduce reward hacking frequency by 34 percent across four failure modes. We present new paradigms showing that mechanistically grounded decomposition offers a more effective way to detect reward hacking than post-hoc behavioral monitoring.

</details>


### [120] [Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction](https://arxiv.org/abs/2511.17879)
*Yusong Wu,Stephen Brade,Teng Ma,Tia-Jane Fowler,Enning Yang,Berker Banar,Aaron Courville,Natasha Jaques,Cheng-Zhi Anna Huang*

Main category: cs.LG

TL;DR: 提出一种对抗训练方法来缓解RL后训练中的奖励破解问题，用于旋律到和弦伴奏任务，通过共同进化的判别器保持输出多样性


<details>
  <summary>Details</summary>
Motivation: 实时即兴演奏需要实时协调和适应，而RL后训练常因利用一致性奖励而减少输出多样性，这种奖励破解问题在音乐创作中特别有害

Method: 在策略生成轨迹上进行对抗训练，使用共同进化的判别器区分策略轨迹和数据分布，策略同时最大化判别器输出和一致性奖励

Result: 在模拟和真实用户研究中显示输出多样性、和声一致性、适应速度和用户控制力均得到改善

Conclusion: 该方法简单有效地缓解了生成序列模型RL后训练中的奖励破解问题

Abstract: Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player's future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.

</details>


### [121] [Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization](https://arxiv.org/abs/2511.17963)
*Jun Kevin,Pujianto Yugopuspito*

Main category: cs.LG

TL;DR: 提出融合LSTM预测和PPO强化学习的混合投资组合优化框架，在非平稳市场条件下实现更高收益和更强韧性


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化方法难以适应动态市场变化，需要结合时序预测和自适应决策能力来应对非平稳市场环境

Method: 使用LSTM捕捉时序依赖关系进行预测，PPO智能体在连续动作空间中自适应调整资产配置，形成混合架构

Result: 在多种资产数据集上的测试表明，混合模型相比等权重、指数型和单一模型方法具有更高的年化收益和夏普比率，在非平稳市场环境下表现更稳健

Conclusion: LSTM+PPO混合框架为动态投资组合优化提供了一个稳健的AI驱动解决方案，在非平稳市场条件下表现出优越性能

Abstract: This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization.

</details>


### [122] [Uncertainty-Aware Federated Learning for Cyber-Resilient Microgrid Energy Management](https://arxiv.org/abs/2511.17968)
*Oluleke Babayomi,Dong-Seong Kim*

Main category: cs.LG

TL;DR: 提出一个集成联邦LSTM光伏预测和两阶段级联虚假数据注入攻击检测的微电网网络弹性框架，通过多信号融合实现攻击恢复和成本优化


<details>
  <summary>Details</summary>
Motivation: 解决微电网在遭受网络攻击时保持经济效率和运行可靠性的挑战，现有方法假设测量无异常、预测不确定性未量化，且未缓解针对可再生能源预测的恶意攻击

Method: 结合自编码器重构误差和预测不确定性量化，集成联邦LSTM光伏预测与两阶段级联虚假数据注入攻击检测，实现攻击弹性的储能调度和数据隐私保护

Result: 在极端虚假数据攻击条件下，框架将误报检测减少70%，恢复93.7%的预测性能损失，实现5%的运行成本节约，缓解34.7%的攻击导致经济损失

Conclusion: 基于精度的级联检测与多信号融合优于单信号方法，验证了去中心化微电网安全与性能的协同效应

Abstract: Maintaining economic efficiency and operational reliability in microgrid energy management systems under cyberattack conditions remains challenging. Most approaches assume non-anomalous measurements, make predictions with unquantified uncertainties, and do not mitigate malicious attacks on renewable forecasts for energy management optimization. This paper presents a comprehensive cyber-resilient framework integrating federated Long Short-Term Memory-based photovoltaic forecasting with a novel two-stage cascade false data injection attack detection and energy management system optimization. The approach combines autoencoder reconstruction error with prediction uncertainty quantification to enable attack-resilient energy storage scheduling while preserving data privacy. Extreme false data attack conditions were studied that caused 58% forecast degradation and 16.9\% operational cost increases. The proposed integrated framework reduced false positive detections by 70%, recovered 93.7% of forecasting performance losses, and achieved 5\% operational cost savings, mitigating 34.7% of attack-induced economic losses. Results demonstrate that precision-focused cascade detection with multi-signal fusion outperforms single-signal approaches, validating security-performance synergy for decentralized microgrids.

</details>


### [123] [Controllability Analysis of State Space-based Language Model](https://arxiv.org/abs/2511.17970)
*Mohamed Mabrok,Yalda Zafari*

Main category: cs.LG

TL;DR: 提出了Influence Score作为衡量Mamba模型内部动态的指标，通过分析状态空间参数来量化token对后续状态的影响程度。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（特别是Mamba）已成为序列建模的强大架构，但其内部动态相比基于注意力的模型仍缺乏深入理解。

Method: 从Mamba的离散化状态空间参数推导出基于可控性的Influence Score，通过类似系统可观测性的反向递推计算。在三个Mamba变体上进行了六项实验测试。

Result: 发现三个主要洞察：1）Influence Score随模型规模和训练数据增加而增加；2）Mamba表现出一致的架构模式；3）仅在规模较大时出现涌现行为。

Conclusion: Influence Score可作为解释和比较基于SSM的语言模型的实用诊断工具。

Abstract: State-space models (SSMs), particularly Mamba, have become powerful architectures for sequence modeling, yet their internal dynamics remain poorly understood compared to attention-based models. We introduce and validate the Influence Score, a controllability-based metric derived from the discretized state-space parameters of Mamba and computed through a backward recurrence analogous to system observability. The score quantifies how strongly a token at position k affects all later states and outputs. We evaluate this measure across three Mamba variants: mamba-130m, mamba-2.8b, and mamba-2.8b-slimpj, using six experiments that test its sensitivity to temperature, prompt complexity, token type, layer depth, token position, and input perturbations. The results show three main insights: (1) the Influence Score increases with model size and training data, reflecting model capacity; (2) Mamba exhibits consistent architectural patterns, including recency bias and concentrated influence in mid-to-late layers; and (3) emergent behaviors appear only at scale, with mamba-2.8b-slimpj uniquely prioritizing content words and reducing internal influence in the presence of noise. These findings establish the Influence Score as a practical diagnostic tool for interpreting and comparing SSM-based language models.

</details>


### [124] [Federated Anomaly Detection and Mitigation for EV Charging Forecasting Under Cyberattacks](https://arxiv.org/abs/2511.17978)
*Oluleke Babayomi,Dong-Seong Kim*

Main category: cs.LG

TL;DR: 提出了一种新颖的异常弹性联邦学习框架，用于电动汽车充电基础设施的网络安全保护和需求预测，在保护数据隐私的同时检测网络攻击并维持可信的预测精度。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电基础设施面临日益严重的网络安全威胁，现有预测技术缺乏结合鲁棒异常缓解解决方案和数据隐私保护的能力。

Method: 集成三个关键创新：基于LSTM自编码器的分布式异常检测、基于插值的异常数据缓解以保持时间连续性、以及联邦LSTM网络实现无需集中数据聚合的协作学习。

Result: 联邦方法相比集中式模型性能提升15.2%的R2精度，同时保持数据本地化；网络攻击检测和缓解系统恢复47.9%的攻击导致性能下降，保持91.3%的精度和1.21%的低误报率。

Conclusion: 该架构能够增强电动汽车基础设施规划、隐私保护协作预测、网络安全韧性，并在分布式充电网络中实现从恶意威胁的快速恢复。

Abstract: Electric Vehicle (EV) charging infrastructure faces escalating cybersecurity threats that can severely compromise operational efficiency and grid stability. Existing forecasting techniques are limited by the lack of combined robust anomaly mitigation solutions and data privacy preservation. Therefore, this paper addresses these challenges by proposing a novel anomaly-resilient federated learning framework that simultaneously preserves data privacy, detects cyber-attacks, and maintains trustworthy demand prediction accuracy under adversarial conditions. The proposed framework integrates three key innovations: LSTM autoencoder-based distributed anomaly detection deployed at each federated client, interpolation-based anomalous data mitigation to preserve temporal continuity, and federated Long Short-Term Memory (LSTM) networks that enable collaborative learning without centralized data aggregation. The framework is validated on real-world EV charging infrastructure datasets combined with real-world DDoS attack datasets, providing robust validation of the proposed approach under realistic threat scenarios. Experimental results demonstrate that the federated approach achieves superior performance compared to centralized models, with 15.2% improvement in R2 accuracy while maintaining data locality. The integrated cyber-attack detection and mitigation system produces trustworthy datasets that enhance prediction reliability, recovering 47.9% of attack-induced performance degradation while maintaining exceptional precision (91.3%) and minimal false positive rates (1.21%). The proposed architecture enables enhanced EV infrastructure planning, privacy-preserving collaborative forecasting, cybersecurity resilience, and rapid recovery from malicious threats across distributed charging networks.

</details>


### [125] [Escaping Optimization Stagnation: Taking Steps Beyond Task Arithmetic via Difference Vectors](https://arxiv.org/abs/2511.17987)
*Jinping Wang,Zhiqiang Gao,Dinggen Zhang,Zhiwu Xie*

Main category: cs.LG

TL;DR: 提出DV-BASI算法，使用差异向量作为定向扰动，克服任务算术方法中的优化停滞问题，实现连续优化过程。


<details>
  <summary>Details</summary>
Motivation: 当前预训练模型编辑方法面临高计算成本和有限可扩展性的挑战，任务算术方法虽有效但优化潜力未充分发掘，主要受限于优化停滞问题。

Method: 引入差异向量概念（任务向量的广义形式），提出基于差异向量的各向异性缩放迭代算法（DV-BASI），利用差异向量的逃逸性和方向优势进行连续优化。

Result: DV-BASI在多任务模型合并中的平均性能甚至超过单独微调的模型，在监督和无监督评估协议上达到最先进性能。

Conclusion: 差异向量为任务算术方法提供了有效的优化机制，DV-BASI算法具有表达性搜索方向、少参数学习和可扩展框架的优势。

Abstract: Current methods for editing pre-trained models face significant challenges, primarily high computational costs and limited scalability. Task arithmetic has recently emerged as a promising solution, using simple arithmetic operations-addition and negation-based on task vectors which are the differences between fine-tuned and pre-trained model weights, to efficiently modify model behavior. However, the full potential of task arithmetic remains underexplored, primarily due to limited mechanisms for overcoming optimization stagnation. To address this challenge, we introduce the notion of difference vector, a generalized form of task vectors derived from the historical movements during optimization. Using difference vectors as directed perturbations, we propose the Difference Vector-based Anisotropic Scaling Iterative algorithm (DV-BASI) to enable a continuous optimization process for task arithmetic methods without relying on any additional modules or components. Notably, by leveraging escapability and directional advantages of difference vectors, the average performance on different tasks of the multi-task model merged by DV-BASI may even outperform models individually fine-tuned. Based on this observation, we extend the application of difference vectors to a feasible fine-tuning method for single-task models. On the practical side, DV-BASI allows expressive searching directions with few learnable parameters and forms a scalable framework. We also integrate DV-BASI with task arithmetic methods and advanced optimization techniques to achieve state-of-the-art performance on both supervised and unsupervised evaluation protocols.

</details>


### [126] [Privacy Auditing of Multi-domain Graph Pre-trained Model under Membership Inference Attacks](https://arxiv.org/abs/2511.17989)
*Jiayi Luo,Qingyun Sun,Yuecen Wei,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: MGP-MIA是一个针对多域图预训练模型的成员推理攻击框架，通过机器遗忘放大成员信号、增量学习构建可靠影子模型、基于相似度的推理机制来识别训练数据成员。


<details>
  <summary>Details</summary>
Motivation: 多域图预训练虽然提高了图神经网络的泛化能力，但其在成员推理攻击下的隐私风险尚未被充分探索，面临泛化能力增强、影子数据集不具代表性、成员信号减弱等挑战。

Method: 提出MGP-MIA框架：1）通过机器遗忘放大目标模型的过拟合特征；2）通过增量学习构建可靠影子模型；3）基于样本相似度进行成员推理。

Result: 大量实验证明MGP-MIA的有效性，揭示了多域图预训练存在的隐私风险。

Conclusion: 多域图预训练模型存在显著的隐私风险，MGP-MIA能够有效进行成员推理攻击，为图基础模型的隐私保护提供了重要启示。

Abstract: Multi-domain graph pre-training has emerged as a pivotal technique in developing graph foundation models. While it greatly improves the generalization of graph neural networks, its privacy risks under membership inference attacks (MIAs), which aim to identify whether a specific instance was used in training (member), remain largely unexplored. However, effectively conducting MIAs against multi-domain graph pre-trained models is a significant challenge due to: (i) Enhanced Generalization Capability: Multi-domain pre-training reduces the overfitting characteristics commonly exploited by MIAs. (ii) Unrepresentative Shadow Datasets: Diverse training graphs hinder the obtaining of reliable shadow graphs. (iii) Weakened Membership Signals: Embedding-based outputs offer less informative cues than logits for MIAs. To tackle these challenges, we propose MGP-MIA, a novel framework for Membership Inference Attacks against Multi-domain Graph Pre-trained models. Specifically, we first propose a membership signal amplification mechanism that amplifies the overfitting characteristics of target models via machine unlearning. We then design an incremental shadow model construction mechanism that builds a reliable shadow model with limited shadow graphs via incremental learning. Finally, we introduce a similarity-based inference mechanism that identifies members based on their similarity to positive and negative samples. Extensive experiments demonstrate the effectiveness of our proposed MGP-MIA and reveal the privacy risks of multi-domain graph pre-training.

</details>


### [127] [Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning](https://arxiv.org/abs/2511.18000)
*Radman Rakhshandehroo,Daniel Coombs*

Main category: cs.LG

TL;DR: ContagionRL是一个Gymnasium兼容的强化学习平台，专门用于空间流行病模拟中的系统化奖励工程研究，通过评估不同奖励函数设计对学习生存策略的影响。


<details>
  <summary>Details</summary>
Motivation: 传统基于代理的模型依赖固定行为规则，无法系统评估奖励函数设计如何影响不同疫情场景下的学习生存策略，存在奖励工程研究不足的知识空白。

Method: 平台整合空间SIRS+D流行病学模型与可配置环境参数，评估五种不同奖励设计（从稀疏生存奖励到新型势场方法），使用多种RL算法（PPO、SAC、A2C）进行系统消融研究。

Result: 方向性指导和明确的依从激励是稳健策略学习的关键组成部分。使用势场奖励训练的智能体始终表现优异，学会最大程度遵守非药物干预措施并发展复杂的空间规避策略。

Conclusion: ContagionRL是研究流行病背景下适应性行为响应的有效平台，强调了奖励设计、信息结构和环境可预测性在学习中的重要性。

Abstract: We present ContagionRL, a Gymnasium-compatible reinforcement learning platform specifically designed for systematic reward engineering in spatial epidemic simulations. Unlike traditional agent-based models that rely on fixed behavioral rules, our platform enables rigorous evaluation of how reward function design affects learned survival strategies across diverse epidemic scenarios. ContagionRL integrates a spatial SIRS+D epidemiological model with configurable environmental parameters, allowing researchers to stress-test reward functions under varying conditions including limited observability, different movement patterns, and heterogeneous population dynamics. We evaluate five distinct reward designs, ranging from sparse survival bonuses to a novel potential field approach, across multiple RL algorithms (PPO, SAC, A2C). Through systematic ablation studies, we identify that directional guidance and explicit adherence incentives are critical components for robust policy learning. Our comprehensive evaluation across varying infection rates, grid sizes, visibility constraints, and movement patterns reveals that reward function choice dramatically impacts agent behavior and survival outcomes. Agents trained with our potential field reward consistently achieve superior performance, learning maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies. The platform's modular design enables systematic exploration of reward-behavior relationships, addressing a knowledge gap in models of this type where reward engineering has received limited attention. ContagionRL is an effective platform for studying adaptive behavioral responses in epidemic contexts and highlight the importance of reward design, information structure, and environmental predictability in learning.

</details>


### [128] [Understanding Private Learning From Feature Perspective](https://arxiv.org/abs/2511.18006)
*Meng Ding,Mingxi Lei,Shaopeng Fu,Shaowei Wang,Di Wang,Jinhui Xu*

Main category: cs.LG

TL;DR: 本文首次从特征学习角度构建了差分隐私SGD的理论分析框架，揭示了私有训练中特征信号学习需要更高信噪比，且数据噪声记忆问题在私有和非私有学习中都会发生。


<details>
  <summary>Details</summary>
Motivation: 尽管利用预训练模型特征增强DP-SGD训练已取得显著经验进展，但私有学习中特征动态的理论理解仍然不足，现有DP分析忽视了标签相关特征信号与标签无关噪声的关键区别。

Method: 基于多补丁数据结构，使用带多项式ReLU激活的两层CNN，通过噪声梯度下降理论分析私有训练中的特征信号学习和数据噪声记忆。

Result: 发现：(1)有效私有信号学习需要比非私有训练更高的信噪比；(2)当非私有学习中出现数据噪声记忆时，私有学习也会出现，导致训练损失小但泛化性能差。

Conclusion: 研究凸显了私有学习的挑战，并证明了特征增强提高信噪比的益处，合成和真实数据集实验验证了理论发现。

Abstract: Differentially private Stochastic Gradient Descent (DP-SGD) has become integral to privacy-preserving machine learning, ensuring robust privacy guarantees in sensitive domains. Despite notable empirical advances leveraging features from non-private, pre-trained models to enhance DP-SGD training, a theoretical understanding of feature dynamics in private learning remains underexplored. This paper presents the first theoretical framework to analyze private training through a feature learning perspective. Building on the multi-patch data structure from prior work, our analysis distinguishes between label-dependent feature signals and label-independent noise, a critical aspect overlooked by existing analyses in the DP community. Employing a two-layer CNN with polynomial ReLU activation, we theoretically characterize both feature signal learning and data noise memorization in private training via noisy gradient descent. Our findings reveal that (1) Effective private signal learning requires a higher signal-to-noise ratio (SNR) compared to non-private training, and (2) When data noise memorization occurs in non-private learning, it will also occur in private learning, leading to poor generalization despite small training loss. Our findings highlight the challenges of private learning and prove the benefit of feature enhancement to improve SNR. Experiments on synthetic and real-world datasets also validate our theoretical findings.

</details>


### [129] [Curvature-Aware Safety Restoration In LLMs Fine-Tuning](https://arxiv.org/abs/2511.18039)
*Thong Bach,Thanh Nguyen-Tang,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: 本文发现微调LLM时安全对齐能力并未被抹除，而是参数空间中发生了偏移，提出了一种基于曲率感知的对齐恢复方法，在保持任务性能的同时有效减少有害响应。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型进行下游任务时往往会损害安全对齐能力，即使使用参数高效方法如LoRA也是如此，这促使研究者探索如何在微调后恢复模型的安全对齐。

Method: 提出曲率感知对齐恢复方法，利用影响函数和二阶优化，选择性地增加有害输入上的损失，同时保持任务性能。该方法基于基础模型和微调模型共享的几何结构进行精确更新。

Result: 在多个模型系列和对抗设置下的广泛评估表明，该方法能有效减少有害响应，同时保持甚至提升实用性和少样本学习性能。

Conclusion: 通过利用微调模型损失景观的几何结构特性，可以实现精确、低影响的安全对齐恢复，避免完全回滚到基础模型，为安全微调提供了新思路。

Abstract: Fine-tuning Large Language Models (LLMs) for downstream tasks often compromises safety alignment, even when using parameter-efficient methods like LoRA. In this work, we uncover a notable property: fine-tuned models preserve the geometric structure of their loss landscapes concerning harmful content, regardless of the fine-tuning method employed. This suggests that safety behaviors are not erased but shifted to less influential regions of the parameter space. Building on this insight, we propose a curvature-aware alignment restoration method that leverages influence functions and second-order optimization to selectively increase loss on harmful inputs while preserving task performance. By navigating the shared geometry between base and fine-tuned models, our method discourages unsafe outputs while preserving task-relevant performance, avoiding full reversion and enabling precise, low-impact updates. Extensive evaluations across multiple model families and adversarial settings show that our approach efficiently reduces harmful responses while maintaining or even improving utility and few-shot learning performance.

</details>


### [130] [pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data](https://arxiv.org/abs/2511.18066)
*Md Akil Raihan Iftee,Syed Md. Ahnaf Hasan,Mir Sazzat Hossain,Rakibul Hasan Rajib,Amin Ahsan Ali,AKM Mahbubur Rahman,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 提出pFedBBN框架，用于解决联邦学习中测试时适应问题，特别是在类别不平衡场景下，通过平衡批归一化和基于相似度的客户端协作来提升模型鲁棒性和少数类性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的测试时适应面临类别不平衡的挑战，关键类别在单个客户端数据集中严重不足。现有方法依赖标注数据或客户端协调，无法在联邦类别不平衡约束下处理动态领域或推理时的分布偏移。

Method: 使用平衡批归一化在本地客户端适应中平等对待所有类别，同时通过BBN相似度指导客户端协作，确保具有相似平衡表示的客户端相互增强，保持适应与领域特定特征对齐。

Result: 在多样化基线的广泛实验中，pFedBBN持续提升了鲁棒性和少数类性能，优于最先进的联邦学习和测试时适应方法。

Conclusion: pFedBBN通过平衡特征归一化和领域感知协作，解决了分布偏移和类别不平衡问题，无需客户端的任何标注或原始数据，支持完全无监督的本地适应和个性化推理。

Abstract: Test-time adaptation (TTA) in federated learning (FL) is crucial for handling unseen data distributions across clients, particularly when faced with domain shifts and skewed class distributions. Class Imbalance (CI) remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed CI during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none address class unsupervised adaptation to dynamic domains or distribution shifts at inference time under federated CI constraints. Revealing the failure of state-of-the-art TTA in federated client adaptation in CI scenario, we propose pFedBBN,a personalized federated test-time adaptation framework that employs balanced batch normalization (BBN) during local client adaptation to mitigate prediction bias by treating all classes equally, while also enabling client collaboration guided by BBN similarity, ensuring that clients with similar balanced representations reinforce each other and that adaptation remains aligned with domain-specific characteristics. pFedBBN supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Extensive experiments across diverse baselines show that pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods.

</details>


### [131] [The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality](https://arxiv.org/abs/2511.18084)
*Dou Liu,Ying Long,Sophia Zuoqiu,Kaipeng Xie,Runze Yang,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.LG

TL;DR: 评估四种LLM对齐策略在临床决策中的表现，发现GRPO算法精度最高但医生更偏好SFT模型，揭示算法改进与临床信任之间的对齐悖论


<details>
  <summary>Details</summary>
Motivation: LLM在临床决策支持中的应用日益增多，但如何使其与真实医学的多维推理路径保持一致仍面临重大挑战

Method: 使用8000多份不孕症治疗记录，系统评估SFT、DPO、GRPO和ICL四种对齐策略，结合自动基准测试和盲法医生评估的双层框架

Result: GRPO在多个决策层实现最高算法精度，但临床医生更偏好SFT模型，认为其推理过程更清晰、治疗可行性更高。SFT在盲法对比中获得最高胜率51.2%

Conclusion: 算法改进不一定转化为更高的临床信任，可能偏离以人为中心的偏好，需要优先考虑临床可解释性和实际可行性的对齐策略

Abstract: Large language models (LLMs) are increasingly adopted in clinical decision support, yet aligning them with the multifaceted reasoning pathways of real-world medicine remains a major challenge. Using more than 8,000 infertility treatment records, we systematically evaluate four alignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-layer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments. GRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the value of reinforcement-based optimization for structured prediction tasks. However, clinicians consistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher therapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest winning rate (51.2%), outperforming both GRPO (26.2%) and even physicians' original decisions (22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessarily translate into higher clinical trust, and may diverge from human-centered preferences. Our findings highlight the need for alignment strategies that prioritize clinically interpretable and practically feasible reasoning, rather than solely optimizing decision-level accuracy.

</details>


### [132] [A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization](https://arxiv.org/abs/2511.18093)
*Fulong Yao,Wanqing Zhao,Matthew Forshaw*

Main category: cs.LG

TL;DR: 本文提出了一种新的误差时间差分(ETD)算法，用于解决微电网能量优化中预测模型不确定性导致的控制策略次优问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度强化学习的预测控制方法往往忽视预测模型不完善带来的不确定性，这会导致控制策略次优。

Method: 首先建立包含可再生能源和储能系统的微电网系统及其马尔可夫决策过程模型，然后提出基于深度Q网络的预测控制方法，设计了加权平均算法和新ETD算法来量化和处理预测不确定性。

Result: 在真实美国数据集上的仿真表明，开发的ETD算法有效提高了深度强化学习在优化微电网运行中的性能。

Conclusion: 新提出的ETD算法能够有效解决预测不确定性，提升微电网能量优化的控制性能。

Abstract: Predictive control approaches based on deep reinforcement learning (DRL) have gained significant attention in microgrid energy optimization. However, existing research often overlooks the issue of uncertainty stemming from imperfect prediction models, which can lead to suboptimal control strategies. This paper presents a new error temporal difference (ETD) algorithm for DRL to address the uncertainty in predictions,aiming to improve the performance of microgrid operations. First,a microgrid system integrated with renewable energy sources (RES) and energy storage systems (ESS), along with its Markov decision process (MDP), is modelled. Second, a predictive control approach based on a deep Q network (DQN) is presented, in which a weighted average algorithm and a new ETD algorithm are designed to quantify and address the prediction uncertainty, respectively. Finally, simulations on a realworld US dataset suggest that the developed ETD effectively improves the performance of DRL in optimizing microgrid operations.

</details>


### [133] [Vulnerability-Aware Robust Multimodal Adversarial Training](https://arxiv.org/abs/2511.18138)
*Junrui Zhang,Xinyu Zhao,Jie Peng,Chenjie Wang,Jianmin Ji,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出VARMAT方法，通过量化各模态的脆弱性并进行针对性正则化，提升多模态模型的对抗鲁棒性，在三个数据集上分别获得12.73%、22.21%、11.19%的鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了不同模态对最终鲁棒性的贡献差异，导致鲁棒性表现不佳。多模态模型由于模态间的相互依赖，更容易受到对抗攻击。

Method: VARMAT方法包含两个步骤：1) 基于攻击目标的一阶近似显式量化各模态的脆弱性（探测）；2) 提出针对性的正则化项，惩罚高脆弱性模态，在保持任务准确性的同时指导鲁棒学习（训练）。

Result: 在多个涉及不同模态的多模态数据集上验证了方法的增强鲁棒性，在三个数据集上分别实现了12.73%、22.21%、11.19%的鲁棒性提升。

Conclusion: 该方法揭示了多模态对抗训练中的一个重要盲点，通过考虑模态间的脆弱性差异显著提升了模型鲁棒性。

Abstract: Multimodal learning has shown significant superiority on various tasks by integrating multiple modalities. However, the interdependencies among modalities increase the susceptibility of multimodal models to adversarial attacks. Existing methods mainly focus on attacks on specific modalities or indiscriminately attack all modalities. In this paper, we find that these approaches ignore the differences between modalities in their contribution to final robustness, resulting in suboptimal robustness performance. To bridge this gap, we introduce Vulnerability-Aware Robust Multimodal Adversarial Training (VARMAT), a probe-in-training adversarial training method that improves multimodal robustness by identifying the vulnerability of each modality. To be specific, VARMAT first explicitly quantifies the vulnerability of each modality, grounded in a first-order approximation of the attack objective (Probe). Then, we propose a targeted regularization term that penalizes modalities with high vulnerability, guiding robust learning while maintaining task accuracy (Training). We demonstrate the enhanced robustness of our method across multiple multimodal datasets involving diverse modalities. Finally, we achieve {12.73%, 22.21%, 11.19%} robustness improvement on three multimodal datasets, revealing a significant blind spot in multimodal adversarial training.

</details>


### [134] [Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction](https://arxiv.org/abs/2511.18150)
*Randy Davila,Beyzanur Ispir*

Main category: cs.LG

TL;DR: 比较CNN和GNN在近似图支配数计算中的表现，GNN在准确性和速度上均优于CNN，提供超过200倍的加速。


<details>
  <summary>Details</summary>
Motivation: 图支配数的精确计算是NP难问题，传统方法仅限于小规模图实例，需要开发高效的近似方法。

Method: 使用卷积神经网络（CNN）基于邻接矩阵表示和图神经网络（GNN）基于图结构消息传递两种神经网络方法，在2000个最多64个顶点的随机图上进行实验。

Result: GNN表现显著优于CNN，达到R²=0.987和MAE=0.372，而CNN为R²=0.955和MAE=0.500。GNN提供超过200倍的加速同时保持接近完美的保真度。

Conclusion: GNN可作为组合图不变量的实用替代方法，对可扩展图优化和数学发现具有重要意义。

Abstract: We investigate machine learning approaches to approximating the \emph{domination number} of graphs, the minimum size of a dominating set. Exact computation of this parameter is NP-hard, restricting classical methods to small instances. We compare two neural paradigms: Convolutional Neural Networks (CNNs), which operate on adjacency matrix representations, and Graph Neural Networks (GNNs), which learn directly from graph structure through message passing. Across 2,000 random graphs with up to 64 vertices, GNNs achieve markedly higher accuracy ($R^2=0.987$, MAE $=0.372$) than CNNs ($R^2=0.955$, MAE $=0.500$). Both models offer substantial speedups over exact solvers, with GNNs delivering more than $200\times$ acceleration while retaining near-perfect fidelity. Our results position GNNs as a practical surrogate for combinatorial graph invariants, with implications for scalable graph optimization and mathematical discovery.

</details>


### [135] [scipy.spatial.transform: Differentiable Framework-Agnostic 3D Transformations in Python](https://arxiv.org/abs/2511.18157)
*Martin Schuck,Alexander von Rohr,Angela P. Schoellig*

Main category: cs.LG

TL;DR: SciPy spatial.transform模块的全面重构，使其兼容所有实现Python数组API的数组库，支持GPU/TPU执行、JIT编译和自动微分。


<details>
  <summary>Details</summary>
Motivation: 解决原有SciPy spatial.transform仅支持NumPy的限制，使其能够在GPU加速和自动微分工作流中使用。

Method: 重写SciPy spatial.transform功能，使其兼容任何实现Python数组API的库（如JAX、PyTorch、CuPy），同时保持原有接口。

Result: 实现与多种框架的兼容性，支持GPU/TPU执行、JIT编译、向量化批处理和自动微分。

Conclusion: 该重构已合并到SciPy主分支，为可微分系统和机器学习中的3D空间数学提供了框架无关的生产级基础。

Abstract: Three-dimensional rigid-body transforms, i.e. rotations and translations, are central to modern differentiable machine learning pipelines in robotics, vision, and simulation. However, numerically robust and mathematically correct implementations, particularly on SO(3), are error-prone due to issues such as axis conventions, normalizations, composition consistency and subtle errors that only appear in edge cases. SciPy's spatial.transform module is a rigorously tested Python implementation. However, it historically only supported NumPy, limiting adoption in GPU-accelerated and autodiff-based workflows. We present a complete overhaul of SciPy's spatial.transform functionality that makes it compatible with any array library implementing the Python array API, including JAX, PyTorch, and CuPy. The revised implementation preserves the established SciPy interface while enabling GPU/TPU execution, JIT compilation, vectorized batching, and differentiation via native autodiff of the chosen backend. We demonstrate how this foundation supports differentiable scientific computing through two case studies: (i) scalability of 3D transforms and rotations and (ii) a JAX drone simulation that leverages SciPy's Rotation for accurate integration of rotational dynamics. Our contributions have been merged into SciPy main and will ship in the next release, providing a framework-agnostic, production-grade basis for 3D spatial math in differentiable systems and ML.

</details>


### [136] [LocaGen: Low-Overhead Indoor Localization Through Spatial Augmentation](https://arxiv.org/abs/2511.18158)
*Abdelrahman Abdelmotlb,Abdallah Taman,Sherif Mostafa,Moustafa Youssef*

Main category: cs.LG

TL;DR: LocaGen是一个空间增强框架，通过条件扩散模型生成未见过位置的高质量合成指纹数据，显著减少室内定位系统的指纹采集工作量。


<details>
  <summary>Details</summary>
Motivation: 传统指纹定位系统需要大量位置标记信号数据的采集工作，限制了实际部署。现有减少工作量的方法存在表示能力低、模式崩溃或仍需在所有目标位置采集数据的问题。

Method: 使用条件扩散模型结合空间感知优化策略，仅基于部分已见位置的指纹数据合成未见过位置的指纹数据；通过领域特定启发式方法增强已见位置数据，并使用基于密度的新方法策略性选择已见和未见位置以确保鲁棒覆盖。

Result: 在真实WiFi指纹数据集上的评估显示，即使在30%位置未见过的情况下，LocaGen仍能保持相同的定位精度，相比最先进的增强方法精度提升高达28%。

Conclusion: LocaGen通过空间增强显著减少了指纹定位系统的部署工作量，同时保持了高定位精度，为室内定位系统的实际部署提供了可行的解决方案。

Abstract: Indoor localization systems commonly rely on fingerprinting, which requires extensive survey efforts to obtain location-tagged signal data, limiting their real-world deployability. Recent approaches that attempt to reduce this overhead either suffer from low representation ability, mode collapse issues, or require the effort of collecting data at all target locations. We present LocaGen, a novel spatial augmentation framework that significantly reduces fingerprinting overhead by generating high-quality synthetic data at completely unseen locations. LocaGen leverages a conditional diffusion model guided by a novel spatially aware optimization strategy to synthesize realistic fingerprints at unseen locations using only a subset of seen locations. To further improve our diffusion model performance, LocaGen augments seen location data based on domain-specific heuristics and strategically selects the seen and unseen locations using a novel density-based approach that ensures robust coverage. Our extensive evaluation on a real-world WiFi fingerprinting dataset shows that LocaGen maintains the same localization accuracy even with 30% of the locations unseen and achieves up to 28% improvement in accuracy over state-of-the-art augmentation methods.

</details>


### [137] [Bringing Stability to Diffusion: Decomposing and Reducing Variance of Training Masked Diffusion Models](https://arxiv.org/abs/2511.18159)
*Mengni Jia,Mengyu Zhou,Yihao Liu,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 本文分析了掩码扩散模型训练方差高的根本原因，提出了两种核心方差降低方法P-POTS和MIRROR，显著提升了MDM在复杂推理任务上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型作为自回归模型的有前景替代方案，存在训练方差过高的问题，导致梯度估计噪声大、优化不稳定，即使与自回归模型在初始化时性能相当，在任务特定训练后也会大幅落后。

Method: 1）首次将MDM训练方差分解为三个来源：掩码模式噪声、掩码率噪声和数据噪声；2）设计了六种方差降低方法，核心包括P-POTS（帕累托最优t采样器）和MIRROR（使用负相关样本减少掩码模式噪声）。

Result: 相比标准MDM训练，新方法在复杂推理任务上准确率提升7-8%，同时将运行间变异性降低到接近ARM水平，大幅缩小了与强ARM基线的差距。

Conclusion: 通过理论分析和系统性的方差降低方法，成功解决了MDM训练方差过高的问题，显著提升了其训练稳定性和最终性能。

Abstract: Masked diffusion models (MDMs) are a promising alternative to autoregressive models (ARMs), but they suffer from inherently much higher training variance. High variance leads to noisier gradient estimates and unstable optimization, so even equally strong pretrained MDMs and ARMs that are competitive at initialization often diverge after task-specific training, with MDMs falling far behind. There has been no theoretical explanation or systematic solution. We derive the first decomposition of MDM training variance into three sources: (A) masking pattern noise, (B) masking rate noise, and (C) data noise, while ARMs are only affected by (C). This explains the fundamental training gap. Building on this foundation, we design six variance-reduction methods, including two core methods: (1) P-POTS, a Pareto-optimal t sampler that minimizes training variance by sampling harder t values more often with appropriately smaller update steps, and (2) MIRROR, which uses negatively correlated samples to reduce (A). Experiments show that compared to standard MDM training, our methods improve accuracy by 7-8% on complex reasoning tasks, while simultaneously reducing run-to-run variability to near ARM levels, substantially narrowing the gap with strong ARM baselines; in most settings, even the best baseline runs remain below the worst run of our method.

</details>


### [138] [Bayesian Calibration of Engine-out NOx Models for Engine-to-Engine Transferability](https://arxiv.org/abs/2511.18178)
*Shrenik Zinage,Peter Meckl,Ilias Bilionis*

Main category: cs.LG

TL;DR: 提出贝叶斯校准框架，结合高斯过程和近似贝叶斯计算来推断和校正传感器偏差，解决发动机间差异导致的NOx预测泛化问题


<details>
  <summary>Details</summary>
Motivation: 传统模型在少量发动机数据上训练，难以泛化到整个发动机群体，存在传感器偏差和输入条件变化问题，需要能够适应发动机间差异的模型

Method: 使用贝叶斯校准框架，结合高斯过程和近似贝叶斯计算，从预训练模型出发推断发动机特定传感器偏差并重新校准预测

Result: 该方法在未见测试数据上生成发动机出口NOx的后验预测分布，无需重新训练模型即可实现高精度预测

Conclusion: 这种可转移建模方法显著提高了预测准确性，有效解决了发动机间差异问题，提升了模型泛化能力

Abstract: Accurate prediction of engine-out NOx is essential for meeting stringent emissions regulations and optimizing engine performance. Traditional approaches rely on models trained on data from a small number of engines, which can be insufficient in generalizing across an entire population of engines due to sensor biases and variations in input conditions. In real world applications, these models require tuning or calibration to maintain acceptable error tolerance when applied to other engines. This highlights the need for models that can adapt with minimal adjustments to accommodate engine-to-engine variability and sensor discrepancies. While previous studies have explored machine learning methods for predicting engine-out NOx, these approaches often fail to generalize reliably across different engines and operating environments. To address these issues, we propose a Bayesian calibration framework that combines Gaussian processes with approximate Bayesian computation to infer and correct sensor biases. Starting with a pre-trained model developed using nominal engine data, our method identifies engine specific sensor biases and recalibrates predictions accordingly. By incorporating these inferred biases, our approach generates posterior predictive distributions for engine-out NOx on unseen test data, achieving high accuracy without retraining the model. Our results demonstrate that this transferable modeling approach significantly improves the accuracy of predictions compared to conventional non-adaptive GP models, effectively addressing engine-to-engine variability and improving model generalizability.

</details>


### [139] [MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning](https://arxiv.org/abs/2511.18181)
*Adam Callaghan,Karl Mason,Patrick Mannion*

Main category: cs.LG

TL;DR: 提出了首个针对连续状态和动作空间的多目标多智能体强化学习框架MOMA-AC，基于TD3和DDPG算法实现，能够通过单一神经网络编码所有智能体在冲突目标下的帕累托最优策略前沿。


<details>
  <summary>Details</summary>
Motivation: 填补多目标多智能体强化学习在连续状态和动作空间中的研究空白，解决现有方法无法有效处理连续空间中多智能体多目标优化的问题。

Method: 结合多头行动者网络、集中化评论家和目标偏好条件架构，基于TD3和DDPG算法实例化为MOMA-TD3和MOMA-DDPG，通过物理模拟器构建测试套件评估合作运动任务。

Result: 在期望效用和超体积指标上相比外层循环和独立训练基线取得统计显著改进，随着智能体数量增加保持稳定可扩展性。

Conclusion: 该框架为连续多智能体领域中的鲁棒、可扩展多目标策略学习奠定了基础性步骤。

Abstract: This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains.

</details>


### [140] [Accelerating Time Series Foundation Models with Speculative Decoding](https://arxiv.org/abs/2511.18191)
*Pranav Subbaraman,Fang Sun,Yue Yao,Huacong Tang,Xiao Luo,Yizhou Sun*

Main category: cs.LG

TL;DR: 提出了一种基于推测解码的时间序列预测推理加速框架，使用小模型生成预测片段，大模型并行验证，显著减少推理延迟


<details>
  <summary>Details</summary>
Motivation: Transformer模型在时间序列预测中性能优异但计算成本高，限制了在延迟敏感的web应用中的部署

Method: 采用推测解码方法，用小"草稿"模型生成未来时间序列片段，然后用大"目标"模型并行验证，减少顺序前向传播次数

Result: 在web应用相关的时间序列预测基准测试中，实现了显著的推理加速，同时保持了竞争力的准确性

Conclusion: 该框架无需修改现有基础模型架构，可立即应用于加速已部署的时间序列预测系统

Abstract: Modern web applications--from real-time content recommendation and dynamic pricing to CDN optimization--increasingly rely on time-series forecasting to deliver personalized experiences to billions of users. Large-scale Transformer-based models have achieved state-of-the-art performance in time-series forecasting but suffer from high computational costs, limiting their deployment in latency-sensitive web applications. To address this challenge, we propose a general inference acceleration framework that adapts speculative decoding to autoregressive time-series models. Our approach employs a smaller "draft" model to propose future time-series patches, which are then verified in parallel by a larger "target" model, reducing the number of sequential forward passes required. We address key technical challenges in adapting this technique from discrete language tokens to continuous time-series distributions, including the design of acceptance criteria for multivariate Gaussian patches and practical variants that balance efficiency with accuracy. Through experiments on time series forecasting benchmarks relevant to web applications, we demonstrate significant inference speedups while maintaining competitive accuracy. The framework requires no architectural modifications to existing foundation models, making it immediately applicable to accelerate deployed time-series forecasting systems. Our implementation can be found at https://github.com/PranavSubbaraman/STRIDE

</details>


### [141] [Deep Gaussian Process Proximal Policy Optimization](https://arxiv.org/abs/2511.18214)
*Matthijs van der Lende,Juan Cardenas-Cartagena*

Main category: cs.LG

TL;DR: 提出了Deep Gaussian Process Proximal Policy Optimization (GPPO)，一种利用深度高斯过程来近似策略和价值函数的可扩展模型无关actor-critic算法，在保持与PPO竞争性能的同时提供校准良好的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的不确定性估计对于需要平衡安全探索和高效学习的控制任务至关重要，而深度神经网络虽然推动了RL的突破，但往往缺乏校准的不确定性估计。

Method: 使用深度高斯过程来近似策略和价值函数，开发了GPPO算法，这是一种可扩展的模型无关actor-critic方法。

Result: 在标准高维连续控制基准测试中，GPPO保持了与Proximal Policy Optimization竞争的性能，同时提供了校准良好的不确定性估计。

Conclusion: GPPO能够为更安全和更有效的探索提供信息，解决了深度强化学习中不确定性估计不足的问题。

Abstract: Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.

</details>


### [142] [Tail Distribution of Regret in Optimistic Reinforcement Learning](https://arxiv.org/abs/2511.18247)
*Sajad Khodadadian,Mehrdad Moharrami*

Main category: cs.LG

TL;DR: 本文推导了基于乐观策略的强化学习在有限时域表格MDP中的实例依赖遗憾尾界，分析了两种探索奖励方案，揭示了遗憾分布的双机制结构。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注期望遗憾或单一高概率分位数，缺乏对遗憾完整分布特性的理解。本文旨在填补这一空白，提供全面的遗憾尾部分析。

Method: 采用UCBVI类型算法，分析两种探索奖励方案：(i) K依赖方案，显式包含总episode数K；(ii) K独立方案，仅依赖当前episode索引。算法包含调优参数α。

Result: 获得了Pr(R_K ≥ x)的上界，显示双机制结构：从实例依赖尺度m_K到转移阈值的亚高斯尾部，之后是亚韦布尔尾部。同时推导了期望遗憾的实例依赖界。

Conclusion: 这是首个为episodic强化学习中标准乐观算法提供全面遗憾尾保证的工作，参数α可在期望遗憾和亚高斯尾部范围间进行权衡。

Abstract: We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\Pr(R_K \ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $α$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning.

</details>


### [143] [Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj](https://arxiv.org/abs/2511.18248)
*Wei Zhen Teoh*

Main category: cs.LG

TL;DR: CausalTraj是一个基于时间因果关系的多智能体轨迹预测模型，专注于生成联合概率轨迹，在团队运动中实现更连贯的多智能体预测。


<details>
  <summary>Details</summary>
Motivation: 现有模型主要基于单智能体精度指标（minADE、minFDE）进行评估，忽视了多智能体联合预测的合理性，导致无法生成连贯的多智能体场景。

Method: 提出CausalTraj模型，这是一个基于时间因果关系和似然的多智能体轨迹预测方法，专门设计用于生成联合概率的多智能体轨迹预测。

Result: 在NBA SportVU、Basketball-U和Football-U数据集上，CausalTraj在单智能体精度指标上表现竞争性，在联合指标（minJADE、minJFDE）上取得最佳记录，并生成定性连贯和真实的比赛演化。

Conclusion: CausalTraj通过关注联合预测能力，在保持单智能体精度的同时，显著提升了多智能体轨迹预测的连贯性和现实性。

Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.

</details>


### [144] [Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data](https://arxiv.org/abs/2511.18260)
*Yueqi Wang,Guang Lin*

Main category: cs.LG

TL;DR: RB-DeepONet是一种混合算子学习框架，将降基数值结构与DeepONet的分支-主干架构融合，用于参数PDE的高效求解。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法依赖不透明的学习主干、需要大量标注数据，或在边界和源数据与物理参数独立变化时失效。

Method: 主干固定为通过贪婪选择生成的降基空间，分支网络预测降基系数，使用投影变分残差进行无标签训练，结合边界和源模态编码处理独立变化的载荷。

Result: RB-DeepONet在精度上与侵入式RB-Galerkin、POD-DeepONet和FEONet相当，但使用更少的可训练参数并获得显著加速。

Conclusion: RB-DeepONet为大规模参数PDE提供了一个高效、稳定且可解释的算子学习器。

Abstract: Parametric PDEs power modern simulation, design, and digital-twin systems, yet their many-query workloads still hinge on repeatedly solving large finite-element systems. Existing operator-learning approaches accelerate this process but often rely on opaque learned trunks, require extensive labeled data, or break down when boundary and source data vary independently from physical parameters. We introduce RB-DeepONet, a hybrid operator-learning framework that fuses reduced-basis (RB) numerical structure with the branch-trunk architecture of DeepONet. The trunk is fixed to a rigorously constructed RB space generated offline via Greedy selection, granting physical interpretability, stability, and certified error control. The branch network predicts only RB coefficients and is trained label-free using a projected variational residual that targets the RB-Galerkin solution. For problems with independently varying loads or boundary conditions, we develop boundary and source modal encodings that compress exogenous data into low-dimensional coordinates while preserving accuracy. Combined with affine or empirical interpolation decompositions, RB-DeepONet achieves a strict offline-online split: all heavy lifting occurs offline, and online evaluation scales only with the RB dimension rather than the full mesh. We provide convergence guarantees separating RB approximation error from statistical learning error, and numerical experiments show that RB-DeepONet attains accuracy competitive with intrusive RB-Galerkin, POD-DeepONet, and FEONet while using dramatically fewer trainable parameters and achieving significant speedups. This establishes RB-DeepONet as an efficient, stable, and interpretable operator learner for large-scale parametric PDEs.

</details>


### [145] [A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks](https://arxiv.org/abs/2511.18269)
*Ved Mohan,El Mehdi Er Raqabi,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 提出一个结合运筹学和机器学习的通用框架，用于在大规模物流网络中实现公平的资源替代，显著降低了模型规模和执行时间。


<details>
  <summary>Details</summary>
Motivation: 解决大规模物流网络中由于需求模式不均衡导致的资源不对称流动和节点持续不平衡问题，特别是在分散化设置下实现全局协调的困难。

Method: OR组件在公平视角下建模和解决资源替代问题，ML组件利用历史数据学习调度员偏好，智能探索决策空间，并通过动态选择每个弧的前κ个资源来提高计算效率。

Result: 在世界上最大的包裹递送公司网络中应用，实现了模型规模减少80%，执行时间减少90%，同时保持最优性。

Conclusion: 该框架能够生成高质量解决方案组合，调度员可以从中选择满意的权衡方案，显著优于现有最先进方法。

Abstract: Ensuring that the right resource is available at the right location and time remains a major challenge for organizations operating large-scale logistics networks. The challenge comes from uneven demand patterns and the resulting asymmetric flow of resources across the arcs, which create persistent imbalances at the network nodes. Resource substitution among multiple, potentially composite and interchangeable, resource types is a cost-effective way to mitigate these imbalances. This leads to the resource substitution problem, which aims at determining the minimum number of resource substitutions from an initial assignment to minimize the overall network imbalance. In decentralized settings, achieving globally coordinated solutions becomes even more difficult. When substitution entails costs, effective prescriptions must also incorporate fairness and account for the individual preferences of schedulers. This paper presents a generic framework that combines operations research (OR) and machine learning (ML) to enable fair resource substitution in large networks. The OR component models and solves the resource substitution problem under a fairness lens. The ML component leverages historical data to learn schedulers' preferences, guide intelligent exploration of the decision space, and enhance computational efficiency by dynamically selecting the top-$κ$ resources for each arc in the network. The framework produces a portfolio of high-quality solutions from which schedulers can select satisfactory trade-offs. The proposed framework is applied to the network of one of the largest package delivery companies in the world, which serves as the primary motivation for this research. Computational results demonstrate substantial improvements over state-of-the-art methods, including an 80% reduction in model size and a 90% decrease in execution time while preserving optimality.

</details>


### [146] [From Tables to Signals: Revealing Spectral Adaptivity in TabPFN](https://arxiv.org/abs/2511.18278)
*Jianqiao Zheng,Cameron Gordon,Yiping Ji,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 本文通过信号重建的视角分析TabPFN，发现其具有比标准ReLU-MLP更宽的有效频率容量，且其频谱能力能根据上下文样本数量自适应调整，这种特性使其能进行无需训练和超参数调优的图像去噪。


<details>
  <summary>Details</summary>
Motivation: 理解任务无关的表格基础模型（如TabPFN）的归纳偏差来源，目前对其在上下文学习中的行为机制尚不清楚。

Method: 通过信号重建和频率分析的方法研究TabPFN，分析其频谱特性、位置编码的影响，并与标准ReLU-MLP进行对比。

Result: 发现TabPFN具有比ReLU-MLP更宽的有效频率容量，其频谱能力能根据上下文样本数量自适应调整（称为频谱适应性），位置编码能调节其频率响应。

Conclusion: TabPFN具有独特的频谱特性，可作为任务无关的隐式模型，在信号重建任务中展现出潜力。

Abstract: Task-agnostic tabular foundation models such as TabPFN have achieved impressive performance on tabular learning tasks, yet the origins of their inductive biases remain poorly understood. In this work, we study TabPFN through the lens of signal reconstruction and provide the first frequency-based analysis of its in-context learning behavior. We show that TabPFN possesses a broader effective frequency capacity than standard ReLU-MLPs, even without hyperparameter tuning. Moreover, unlike MLPs whose spectra evolve primarily over training epochs, we find that TabPFN's spectral capacity adapts directly to the number of samples provided in-context, a phenomenon we term Spectral Adaptivity. We further demonstrate that positional encoding modulates TabPFN's frequency response, mirroring classical results in implicit neural representations. Finally, we show that these properties enable TabPFN to perform training-free and hyperparameter-free image denoising, illustrating its potential as a task-agnostic implicit model. Our analysis provides new insight into the structure and inductive biases of tabular foundation models and highlights their promise for broader signal reconstruction tasks.

</details>


### [147] [TRIDENT: A Trimodal Cascade Generative Framework for Drug and RNA-Conditioned Cellular Morphology Synthesis](https://arxiv.org/abs/2511.18287)
*Rui Peng,Ziru Liu,Lingyuan Ye,Yuxing Lu,Boxin Shi,Jinzhuo Wang*

Main category: cs.LG

TL;DR: TRIDENT是一个级联生成框架，通过同时考虑扰动和相应基因表达谱来合成真实的细胞形态，显著优于现有方法，在未见化合物上表现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于建模直接关联（如扰动→RNA或扰动→形态），忽略了从RNA到形态的关键因果联系，这限制了构建AI虚拟细胞的能力。

Method: 提出了TRIDENT级联生成框架，构建了MorphoGene数据集（包含98种化合物的L1000基因表达和Cell Painting图像配对数据），通过RNA条件化来合成细胞形态。

Result: TRIDENT显著优于最先进方法，实现了高达7倍的改进，对未见化合物具有强泛化能力。在多西他赛案例研究中验证了RNA引导合成能准确产生相应表型。

Conclusion: 通过显式建模转录组-表型组映射，TRIDENT提供了一个强大的计算机模拟工具，使我们更接近预测性虚拟细胞。

Abstract: Accurately modeling the relationship between perturbations, transcriptional responses, and phenotypic changes is essential for building an AI Virtual Cell (AIVC). However, existing methods typically constrained to modeling direct associations, such as Perturbation $\rightarrow$ RNA or Perturbation $\rightarrow$ Morphology, overlook the crucial causal link from RNA to morphology. To bridge this gap, we propose TRIDENT, a cascade generative framework that synthesizes realistic cellular morphology by conditioning on both the perturbation and the corresponding gene expression profile. To train and evaluate this task, we construct MorphoGene, a new dataset pairing L1000 gene expression with Cell Painting images for 98 compounds. TRIDENT significantly outperforms state-of-the-art approaches, achieving up to 7-fold improvement with strong generalization to unseen compounds. In a case study on docetaxel, we validate that RNA-guided synthesis accurately produces the corresponding phenotype. An ablation study further confirms that this RNA conditioning is essential for the model's high fidelity. By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves us closer to a predictive virtual cell.

</details>


### [148] [ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning](https://arxiv.org/abs/2511.18291)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: 提出了ADF-LoRA方法，在去中心化联邦学习中通过交替更新低秩矩阵和混合矩阵来改善参数状态一致性，实现更稳定和快速的收敛。


<details>
  <summary>Details</summary>
Motivation: 在去中心化联邦学习中，交替更新LoRA矩阵存在相位状态不匹配和块间发散的新挑战，需要设计更稳定的参数传播机制。

Method: ADF-LoRA每轮只同步更新一个低秩矩阵，并混合两个矩阵以保持参数状态一致性，同时保留交替更新的交叉项抑制效果。

Result: 在多个GLUE任务上的实验表明，ADF-LoRA实现了更快更平滑的收敛，在所有任务上获得了最高的平均准确率，优于现有的LoRA变体。

Conclusion: ADF-LoRA通过同步单矩阵更新和双矩阵混合，有效解决了去中心化联邦学习中的稳定性问题，是LoRA在无服务器拓扑中的有效扩展。

Abstract: This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin.

</details>


### [149] [MultiDiffNet: A Multi-Objective Diffusion Framework for Generalizable Brain Decoding](https://arxiv.org/abs/2511.18294)
*Mengchun Zhang,Kateryna Shapovalenko,Yucheng Shao,Eddie Guo,Parusha Pradhan*

Main category: cs.LG

TL;DR: MultiDiffNet是一个基于扩散模型的框架，通过优化多目标学习紧凑潜在空间，绕过生成增强，实现跨被试的EEG解码，在多个任务上达到最先进的泛化性能。


<details>
  <summary>Details</summary>
Motivation: EEG神经解码面临跨被试泛化能力差的问题，主要原因是高被试间变异性和缺乏大规模数据集。现有方法依赖合成被试生成或简单数据增强，但无法可靠扩展或泛化。

Method: 引入MultiDiffNet扩散框架，学习针对多目标优化的紧凑潜在空间，直接从该空间进行解码，使用被试和会话分离评估。

Result: 在各种神经解码任务上实现最先进的跨被试泛化性能，并发布了统一的基准套件和针对低试次EEG设置的统计报告框架。

Conclusion: 这项工作为现实世界BCI系统中的被试无关EEG解码提供了可重现和开源的基础。

Abstract: Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce \textit{MultiDiffNet}, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems.

</details>


### [150] [GROOT: Graph Edge Re-growth and Partitioning for the Verification of Large Designs in Logic Synthesis](https://arxiv.org/abs/2511.18297)
*Kiran Thorat,Hongwu Peng,Yuebo Luo,Xi Xie,Shaoyi Huang,Amit Hasan,Jiahui Zhao,Yingjie Li,Zhijie Shi,Cunxi Yu,Caiwen Ding*

Main category: cs.LG

TL;DR: GROOT是一个算法与系统协同设计的框架，通过整合芯片设计领域知识、图划分算法和优化的GPU内核，显著提高了芯片验证效率，在保持高精度的同时大幅减少了内存占用和运行时间。


<details>
  <summary>Details</summary>
Motivation: 传统芯片验证方法耗时且计算量大，特别是对于大规模电路。虽然图神经网络(GNNs)有潜力提高验证效率，但缺乏一个综合考虑芯片设计领域知识、图理论和GPU内核设计的联合框架。

Method: 1. 利用电路节点类型和连接极性创建节点特征
2. 使用图划分算法将大图分割为子图以便GPU快速处理
3. 开发图边再生算法恢复验证精度
4. 针对EDA图工作负载的极化分布特性，重新设计HD-kernel和LD-kernel两个GPU内核

Result: 1. 内存占用减少59.38%
2. 验证准确率达到99.96%（针对1024位CSA乘法器）
3. 相比cuSPARSE、MergePath-SpMM和GNNAdvisor，运行时间分别提升1.104倍、5.796倍和1.469倍

Conclusion: GROOT框架成功整合了芯片设计领域知识、图划分算法和优化的GPU内核设计，为大规模芯片验证提供了一种高效且准确的解决方案。

Abstract: Traditional verification methods in chip design are highly time-consuming and computationally demanding, especially for large scale circuits. Graph neural networks (GNNs) have gained popularity as a potential solution to improve verification efficiency. However, there lacks a joint framework that considers all chip design domain knowledge, graph theory, and GPU kernel designs. To address this challenge, we introduce GROOT, an algorithm and system co-design framework that contains chip design domain knowledge and redesigned GPU kernels, to improve verification efficiency. More specifically, we create node features utilizing the circuit node types and the polarity of the connections between the input edges to nodes in And-Inverter Graphs (AIGs). We utilize a graph partitioning algorithm to divide the large graphs into smaller sub-graphs for fast GPU processing and develop a graph edge re-growth algorithm to recover verification accuracy. We carefully profile the EDA graph workloads and observe the uniqueness of their polarized distribution of high degree (HD) nodes and low degree (LD) nodes. We redesign two GPU kernels (HD-kernel and LD-kernel), to fit the EDA graph learning workload on a single GPU. We compare the results with state-of-the-art (SOTA) methods: GAMORA, a GNN-based approach, and the traditional ABC framework. Results show that GROOT achieves a significant reduction in memory footprint (59.38 %), with high accuracy (99.96%) for a very large CSA multiplier, i.e. 1,024 bits with a batch size of 16, which consists of 134,103,040 nodes and 268,140,544 edges. We compare GROOT with GPU-based GPU Kernel designs SOTAs such as cuSPARSE, MergePath-SpMM, and GNNAdvisor. We achieve up to 1.104x, 5.796x, and 1.469x improvement in runtime, respectively.

</details>


### [151] [Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery](https://arxiv.org/abs/2511.18303)
*Rui Ding,Rodrigo Pires Ferreira,Yuxin Chen,Junhong Chen*

Main category: cs.LG

TL;DR: 提出了一种用于复杂材料和设备发现的分层深度研究代理，通过本地可部署的框架结合检索增强生成和深度研究树机制，在27个纳米材料/设备主题上表现优于商业系统且成本更低。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器学习代理和商业系统在复杂材料与设备发现问题上覆盖范围有限、无法与本地数据和工具集成的问题。

Method: 采用分层深度研究代理框架，集成本地检索增强生成与大型语言模型推理器，通过深度研究树机制自适应扩展和修剪研究分支。

Result: 在27个主题的评估中，报告质量与商业系统相当甚至更好，成本显著降低，并通过干实验室验证确认了可行性。

Conclusion: 该深度研究代理为复杂材料和设备发现提供了高效、低成本的解决方案，支持本地部署和工具集成。

Abstract: We present a long-horizon, hierarchical deep research (DR) agent designed for complex materials and device discovery problems that exceed the scope of existing Machine Learning (ML) surrogates and closed-source commercial agents. Our framework instantiates a locally deployable DR instance that integrates local retrieval-augmented generation with large language model reasoners, enhanced by a Deep Tree of Research (DToR) mechanism that adaptively expands and prunes research branches to maximize coverage, depth, and coherence. We systematically evaluate across 27 nanomaterials/device topics using a large language model (LLM)-as-judge rubric with five web-enabled state-of-the-art models as jurors. In addition, we conduct dry-lab validations on five representative tasks, where human experts use domain simulations (e.g., density functional theory, DFT) to verify whether DR-agent proposals are actionable. Results show that our DR agent produces reports with quality comparable to--and often exceeding--those of commercial systems (ChatGPT-5-thinking/o3/o4-mini-high Deep Research) at a substantially lower cost, while enabling on-prem integration with local data and tools.

</details>


### [152] [DiM-TS: Bridge the Gap between Selective State Space Models and Time Series for Generative Modeling](https://arxiv.org/abs/2511.18312)
*Zihao Yao,Jiankai Zuo,Yaying Zhang*

Main category: cs.LG

TL;DR: 提出了DiM-TS模型，利用Mamba状态空间模型增强时间序列生成能力，通过Lag Fusion和Permutation Scanning解决长程依赖和通道关系问题


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成时间序列数据时难以捕获长程时间依赖和复杂通道相互关系，需要改进模型架构来更好地保护数据的时间周期性和通道相关性

Method: 提出Lag Fusion Mamba和Permutation Scanning Mamba两种变体，分别处理相关时间滞后和通道排列问题，然后整合成DiM-TS模型

Result: 在公共数据集上的综合实验表明DiM-TS在生成真实时间序列数据方面具有优越性，能更好地保持数据的多样性属性

Conclusion: DiM-TS是一个高质量的时间序列生成模型，通过增强Mamba的序列建模能力，显著提升了时间序列数据的生成质量

Abstract: Time series data plays a pivotal role in a wide variety of fields but faces challenges related to privacy concerns. Recently, synthesizing data via diffusion models is viewed as a promising solution. However, existing methods still struggle to capture long-range temporal dependencies and complex channel interrelations. In this research, we aim to utilize the sequence modeling capability of a State Space Model called Mamba to extend its applicability to time series data generation. We firstly analyze the core limitations in State Space Model, namely the lack of consideration for correlated temporal lag and channel permutation. Building upon the insight, we propose Lag Fusion Mamba and Permutation Scanning Mamba, which enhance the model's ability to discern significant patterns during the denoising process. Theoretical analysis reveals that both variants exhibit a unified matrix multiplication framework with the original Mamba, offering a deeper understanding of our method. Finally, we integrate two variants and introduce Diffusion Mamba for Time Series (DiM-TS), a high-quality time series generation model that better preserves the temporal periodicity and inter-channel correlations. Comprehensive experiments on public datasets demonstrate the superiority of DiM-TS in generating realistic time series while preserving diverse properties of data.

</details>


### [153] [AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert](https://arxiv.org/abs/2511.18314)
*Yuting Gao,Wang Lan,Hengyuan Zhao,Linjiang Huang,Si Liu,Qingpei Guo*

Main category: cs.LG

TL;DR: AnyExperts提出了一种按需、预算感知的动态路由框架，通过根据token的语义重要性分配可变数量的专家槽位，优化多模态MoE模型的计算资源分配。


<details>
  <summary>Details</summary>
Motivation: 现有多模态MoE模型采用固定的专家激活策略，忽略了不同模态间语义重要性的异质性，导致计算资源分配不优，冗余token消耗了与关键token相同的资源。

Method: 提出AnyExperts框架，为每个token分配可变数量的专家槽位，槽位总数在固定范围内，每个槽位由真实专家或虚拟专家填充，虚拟专家比例上限为20%。模型自适应平衡真实与虚拟专家比例，为语义丰富区域分配更多真实专家。

Result: 在视觉理解、音频理解和NLP理解等多样化任务中，AnyExperts在相同计算预算下提升了性能。在通用图像/视频任务中，以40%更少的真实专家激活达到可比精度；在文本密集任务中，保持性能同时减少10%真实专家使用。

Conclusion: 细粒度、重要性驱动的专家分配显著提升了多模态MoE模型的效率和效果。

Abstract: Multimodal Mixture-of-Experts (MoE) models offer a promising path toward scalable and efficient large vision-language systems. However, existing approaches rely on rigid routing strategies (typically activating a fixed number of experts per token) ignoring the inherent heterogeneity in semantic importance across modalities. This leads to suboptimal compute allocation, where redundant tokens consume as many resources as critical ones. To address this, we propose AnyExperts, a novel on-demand, budget-aware dynamic routing framework that allocates a variable total number of expert slots per token based on its semantic importance. Crucially, to prevent uncontrolled compute growth, the total slots per token are constrained within a fixed range, and each slot is filled by either a real expert or a virtual expert, with the virtual share capped at a small maximum (e.g., 20%). The model then adaptively balances the real-to-virtual ratio per token, assigning more real experts to semantically rich regions and relying more on virtual experts for redundant content. Evaluated across diverse tasks in visual understanding, audio understanding, and NLP understanding, AnyExperts improves performance under the same compute budget. Notably, on general image/video tasks, it achieves comparable accuracy with 40% fewer real expert activations; on text-dense tasks (OCR and NLP), it maintains performance while reducing real expert usage by 10%. These results demonstrate that fine-grained, importance-driven expert allocation significantly enhances both the efficiency and effectiveness of multimodal MoE models.

</details>


### [154] [DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations](https://arxiv.org/abs/2511.18331)
*Sohini Roychowdhury,Adam Holeman,Mohammad Amin,Feng Wei,Bhaskar Mehta,Srihari Reddy*

Main category: cs.LG

TL;DR: Dynamix是一个可扩展的个性化序列探索框架，通过最大相关性原则和基于事件特征的自监督学习，优化在线广告推荐系统中的用户参与历史处理，在保持广告预测准确性的同时提高训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 在线广告推荐系统处理完整的用户-广告参与历史既计算密集又容易受到噪声影响，需要一种更高效的序列处理方法。

Method: 使用最大相关性原则和基于事件特征的自监督学习，在会话和表面级别对用户参与进行分类，通过动态特征移除和选择性特征增强来优化处理。

Result: 动态资源移除使训练和推理吞吐量分别提高1.15%和1.8%，动态特征增强提供0.033 NE增益，同时推理QPS提高4.2%。

Conclusion: Dynamix在基于在线用户序列的推荐模型中实现了显著的成本效率和性能改进，自监督用户分段和资源探索可以进一步优化复杂特征选择策略。

Abstract: For online ad-recommendation systems, processing complete user-ad-engagement histories is both computationally intensive and noise-prone. We introduce Dynamix, a scalable, personalized sequence exploration framework that optimizes event history processing using maximum relevance principles and self-supervised learning through Event Based Features (EBFs). Dynamix categorizes users-engagements at session and surface-levels by leveraging correlations between dwell-times and ad-conversion events. This enables targeted, event-level feature removal and selective feature boosting for certain user-segments, thereby yielding training and inference efficiency wins without sacrificing engaging ad-prediction accuracy. While, dynamic resource removal increases training and inference throughput by 1.15% and 1.8%, respectively, dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2% over baseline models. These results demonstrate that Dynamix achieves significant cost efficiency and performance improvements in online user-sequence based recommendation models. Self-supervised user-segmentation and resource exploration can further boost complex feature selection strategies while optimizing for workflow and compute resources.

</details>


### [155] [Clinician-in-the-Loop Smart Home System to Detect Urinary Tract Infection Flare-Ups via Uncertainty-Aware Decision Support](https://arxiv.org/abs/2511.18334)
*Chibuike E. Ugwu,Roschelle Fritz,Diane J. Cook,Janardhan Rao Doppa*

Main category: cs.LG

TL;DR: 开发了一个临床医生在环的智能家居系统，利用环境传感器数据检测老年人尿路感染发作，通过不确定性量化方法提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 老年人尿路感染发作风险高且常被忽视，传统机器学习方法缺乏不确定性分析，限制了临床决策的有效性。

Method: 采用临床医生在环的智能家居系统，提取行为标记，训练预测模型，并应用符合性校准区间方法进行不确定性量化。

Result: 在8个真实智能家居数据上评估，该方法在召回率等指标上优于基线方法，同时保持最低的弃权比例和区间宽度。42名护士调查确认系统输出对临床决策有价值。

Conclusion: 该系统通过不确定性感知的决策支持，能够有效改善老年人尿路感染和其他病症发作的管理，具有实际应用价值。

Abstract: Urinary tract infection (UTI) flare-ups pose a significant health risk for older adults with chronic conditions. These infections often go unnoticed until they become severe, making early detection through innovative smart home technologies crucial. Traditional machine learning (ML) approaches relying on simple binary classification for UTI detection offer limited utility to nurses and practitioners as they lack insight into prediction uncertainty, hindering informed clinical decision-making. This paper presents a clinician-in-the-loop (CIL) smart home system that leverages ambient sensor data to extract meaningful behavioral markers, train robust predictive ML models, and calibrate them to enable uncertainty-aware decision support. The system incorporates a statistically valid uncertainty quantification method called Conformal-Calibrated Interval (CCI), which quantifies uncertainty and abstains from making predictions ("I don't know") when the ML model's confidence is low. Evaluated on real-world data from eight smart homes, our method outperforms baseline methods in recall and other classification metrics while maintaining the lowest abstention proportion and interval width. A survey of 42 nurses confirms that our system's outputs are valuable for guiding clinical decision-making, underscoring their practical utility in improving informed decisions and effectively managing UTIs and other condition flare-ups in older adults.

</details>


### [156] [Auxiliary Gene Learning: Spatial Gene Expression Estimation by Auxiliary Gene Selection](https://arxiv.org/abs/2511.18336)
*Kaito Shiku,Kazuya Nishimura,Shinnosuke Matsuo,Yasuhiro Kojima,Ryoma Bise*

Main category: cs.LG

TL;DR: 提出AGL方法，通过将忽略基因的表达估计重新表述为辅助任务并与主要任务联合训练，利用被忽略基因的益处。使用DkGSB方法选择对目标基因预测有积极影响的辅助基因子集。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学技术存在大量观测噪声，以往研究仅使用高变异基因子集进行训练和评估，忽略了其他基因。但由于基因间存在共表达关系，低表达基因可能对评估目标有贡献。

Method: AGL方法将忽略基因的表达估计作为辅助任务与主要任务联合训练。提出DkGSB方法，利用先验知识对基因排序，将组合选择问题松弛为可微分的top-k选择问题。

Result: 实验证实了纳入辅助基因的有效性，所提方法优于传统的辅助任务学习方法。

Conclusion: 通过辅助基因学习和可微分基因选择方法，能够有效利用被忽略基因的信息，提升空间转录组学数据分析性能。

Abstract: Spatial transcriptomics (ST) is a novel technology that enables the observation of gene expression at the resolution of individual spots within pathological tissues. ST quantifies the expression of tens of thousands of genes in a tissue section; however, heavy observational noise is often introduced during measurement. In prior studies, to ensure meaningful assessment, both training and evaluation have been restricted to only a small subset of highly variable genes, and genes outside this subset have also been excluded from the training process. However, since there are likely co-expression relationships between genes, low-expression genes may still contribute to the estimation of the evaluation target. In this paper, we propose $Auxiliary \ Gene \ Learning$ (AGL) that utilizes the benefit of the ignored genes by reformulating their expression estimation as auxiliary tasks and training them jointly with the primary tasks. To effectively leverage auxiliary genes, we must select a subset of auxiliary genes that positively influence the prediction of the target genes. However, this is a challenging optimization problem due to the vast number of possible combinations. To overcome this challenge, we propose Prior-Knowledge-Based Differentiable Top-$k$ Gene Selection via Bi-level Optimization (DkGSB), a method that ranks genes by leveraging prior knowledge and relaxes the combinatorial selection problem into a differentiable top-$k$ selection problem. The experiments confirm the effectiveness of incorporating auxiliary genes and show that the proposed method outperforms conventional auxiliary task learning approaches.

</details>


### [157] [Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking](https://arxiv.org/abs/2511.18394)
*Chinmay Karkar,Paras Chopra*

Main category: cs.LG

TL;DR: LLMs在预测社会、政治和经济事件方面展现出部分能力，但其预测性能因领域结构和提示框架而有显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在真实世界事件预测中的能力，特别是针对模型截止日期后发生的事件，探索不同因素如何影响预测准确性和校准。

Method: 分析不同模型家族在真实世界问题上的表现，考察上下文、问题类型和外部知识对准确性和校准的影响，以及添加事实新闻背景如何改变信念形成和失败模式。

Result: 预测能力高度可变，取决于我们询问的内容和方式。

Conclusion: LLMs的预测能力不是固定的，而是高度依赖于具体的预测领域、问题类型和提示框架。

Abstract: Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.

</details>


### [158] [Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using Multi-View Conditional Information Bottleneck](https://arxiv.org/abs/2511.18404)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: MVCIB是一个多视图条件信息瓶颈框架，用于在2D和3D分子结构上自监督预训练图神经网络，通过发现共享信息同时最小化各视图的无关特征，并利用关键子结构作为锚点实现跨视图对齐。


<details>
  <summary>Details</summary>
Motivation: 现有分子图预训练方法主要对齐图级表示，但面临两个挑战：(1)发现两视图间共享信息同时减少视图特定信息；(2)识别和对齐重要子结构（如官能团）以增强跨视图一致性和模型表达能力。

Method: 提出MVCIB框架，使用一个视图作为上下文条件来指导另一个视图的表示学习；利用关键子结构（官能团和ego网络）作为锚点；提出跨注意力机制捕获子结构间的细粒度相关性以实现跨视图子图对齐。

Result: 在四个分子领域的广泛实验表明，MVCIB在预测性能和可解释性方面始终优于基线方法；MVCIB实现了3D Weisfeiler-Lehman表达能力，能够区分非同构图以及具有相同2D连接但不同3D几何结构的异构体。

Conclusion: MVCIB通过多视图条件信息瓶颈和子结构对齐，有效解决了多视图分子学习中的关键挑战，在分子表示学习方面取得了显著进展。

Abstract: Recent pre-training strategies for molecular graphs have attempted to use 2D and 3D molecular views as both inputs and self-supervised signals, primarily aligning graph-level representations. However, existing studies remain limited in addressing two main challenges of multi-view molecular learning: (1) discovering shared information between two views while diminishing view-specific information and (2) identifying and aligning important substructures, e.g., functional groups, which are crucial for enhancing cross-view consistency and model expressiveness. To solve these challenges, we propose a Multi-View Conditional Information Bottleneck framework, called MVCIB, for pre-training graph neural networks on 2D and 3D molecular structures in a self-supervised setting. Our idea is to discover the shared information while minimizing irrelevant features from each view under the MVCIB principle, which uses one view as a contextual condition to guide the representation learning of its counterpart. To enhance semantic and structural consistency across views, we utilize key substructures, e.g., functional groups and ego-networks, as anchors between the two views. Then, we propose a cross-attention mechanism that captures fine-grained correlations between the substructures to achieve subgraph alignment across views. Extensive experiments in four molecular domains demonstrated that MVCIB consistently outperforms baselines in both predictive performance and interpretability. Moreover, MVCIB achieved the 3d Weisfeiler-Lehman expressiveness power to distinguish not only non-isomorphic graphs but also different 3D geometries that share identical 2D connectivity, such as isomers.

</details>


### [159] [Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems](https://arxiv.org/abs/2511.18417)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 提出了类别等变神经网络(CENNs)的统一理论，将群/群胚等变网络、偏序集/格等变网络、图和层神经网络统一起来。证明了在一般设置下的等变通用逼近定理，并实例化到群/群胚、偏序集/格、图和胞腔层等具体场景。


<details>
  <summary>Details</summary>
Motivation: 统一现有的各种等变神经网络框架，扩展等变深度学习的范围，不仅包含几何对称性，还包括上下文和组合对称性。

Method: 在带有Radon测度的拓扑类别中形式化等变性，将线性和非线性层置于类别设置中，构建类别等变神经网络理论。

Result: 证明了有限深度CENNs在连续等变变换空间中是稠密的，即等变通用逼近定理成立。为群/群胚、偏序集/格、图和胞腔层等具体案例推导了相应的通用逼近定理。

Conclusion: 类别等变深度学习扩展了等变深度学习的视野，超越了群作用，包含了更广泛的对称性类型。

Abstract: We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.

</details>


### [160] [Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels](https://arxiv.org/abs/2511.18457)
*Duncan Stothers,Ben Stothers,Emily Schaeffer,Kishore Mulpuri*

Main category: cs.LG

TL;DR: 提出一种超声优先、辐射保护的发育性髋关节发育不良(DDH)诊断策略，仅在必要时才进行X光检查。通过自监督预训练、测量预测和校准延迟规则，实现有限样本覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 减少DDH诊断中不必要的X光辐射暴露，同时保证诊断准确性。通过超声优先策略，仅在超声检查结果不确定时才进行X光检查。

Method: 1) 使用SimSiam在大规模无标签数据集上预训练模态特定编码器；2) 冻结主干网络，在DDH相关标志点和测量上拟合小型头部；3) 使用校准集应用单侧一致性延迟规则，提供有限样本覆盖保证。

Result: 超声测量误差适中(alpha MAE约9.7度，覆盖率MAE约14.0%)，X光探头AI和CE的MAE分别为约7.6度和8.9度。校准后的仅超声策略在不同规则家族和参数设置下表现良好。

Conclusion: 开发了一个简单、可复现的流程，将有限标签转化为可解释的测量结果和可调节的选择性成像曲线，适用于临床交接和未来外部验证。

Abstract: We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.
  We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.

</details>


### [161] [SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation](https://arxiv.org/abs/2511.18468)
*Md Akil Raihan Iftee,Mir Sazzat Hossain,Rakibul Hasan Rajib,Tariq Iqbal,Md Mofijul Islam,M Ashraful Amin,Amin Ahsan Ali,AKM Mahbubur Rahman*

Main category: cs.LG

TL;DR: 提出了SloMo-Fast框架，一种无需源数据的双教师持续测试时适应方法，通过慢教师保留长期知识、快教师快速适应新域，解决现有CTTA方法中的长期遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖源数据或原型，在隐私敏感和资源受限环境中适用性有限，且存在长期遗忘问题，导致在先前遇到域上的性能下降。

Method: 采用双教师框架：慢教师展示缓慢遗忘特性，保留先前遇到域的长期知识；快教师快速适应新域并跨域积累知识。还提出了Cyclic-TTA基准来模拟循环域偏移。

Result: 在Cyclic-TTA和其他十个CTTA设置中，SloMo-Fast始终优于最先进方法，展示了其在演变和重访域上的适应和泛化能力。

Conclusion: SloMo-Fast框架有效解决了CTTA中的长期遗忘问题，在无需源数据的情况下实现了对演变和重访域的鲁棒适应和泛化。

Abstract: Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains.

</details>


### [162] [Adaptive Mesh-Quantization for Neural PDE Solvers](https://arxiv.org/abs/2511.18474)
*Winfried van den Dool,Maksim Zhdanov,Yuki M. Asano,Max Welling*

Main category: cs.LG

TL;DR: 提出自适应网格量化方法，通过轻量级辅助模型识别高损失区域，动态调整量化位宽，在复杂物理区域分配更多计算资源，实现计算效率优化。


<details>
  <summary>Details</summary>
Motivation: 物理系统通常具有空间变化的复杂性，现有图神经网络在求解PDE时对所有节点采用统一计算强度，导致计算资源分配效率低下。

Method: 引入自适应网格量化，在网格节点、边和簇特征上进行空间自适应量化，通过轻量级辅助模型识别高损失区域，动态调整量化位宽分配。

Result: 在2D Darcy流、2D非定常流体动力学、3D稳态Navier-Stokes模拟和2D超弹性问题等多个任务中，相比均匀量化基线获得一致的Pareto改进，在相同成本下性能提升高达50%。

Conclusion: 自适应网格量化框架能够有效优化计算资源利用，在保持计算成本的同时显著提升模型性能。

Abstract: Physical systems commonly exhibit spatially varying complexity, presenting a significant challenge for neural PDE solvers. While Graph Neural Networks can handle the irregular meshes required for complex geometries and boundary conditions, they still apply uniform computational effort across all nodes regardless of the underlying physics complexity. This leads to inefficient resource allocation where computationally simple regions receive the same treatment as complex phenomena. We address this challenge by introducing Adaptive Mesh Quantization: spatially adaptive quantization across mesh node, edge, and cluster features, dynamically adjusting the bit-width used by a quantized model. We propose an adaptive bit-width allocation strategy driven by a lightweight auxiliary model that identifies high-loss regions in the input mesh. This enables dynamic resource distribution in the main model, where regions of higher difficulty are allocated increased bit-width, optimizing computational resource utilization. We demonstrate our framework's effectiveness by integrating it with two state-of-the-art models, MP-PDE and GraphViT, to evaluate performance across multiple tasks: 2D Darcy flow, large-scale unsteady fluid dynamics in 2D, steady-state Navier-Stokes simulations in 3D, and a 2D hyper-elasticity problem. Our framework demonstrates consistent Pareto improvements over uniformly quantized baselines, yielding up to 50% improvements in performance at the same cost.

</details>


### [163] [Real-Time Personalized Content Adaptation through Matrix Factorization and Context-Aware Federated Learning](https://arxiv.org/abs/2511.18489)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.LG

TL;DR: 提出基于联邦学习的个性化LLM框架，通过本地数据微调GPT模型，结合用户画像评分和社交网络分析，实现隐私保护的个性化内容推荐。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体平台中用户交互和内容相关性的挑战，在保护用户隐私的同时提供个性化内容推荐。

Method: 采用联邦学习框架，客户端使用本地社交媒体数据微调GPT基础模型，通过联邦聚合保护隐私，结合用户画像评分、矩阵分解技术和社交参与度量化方法。

Result: 系统能够实时提供个性化内容建议，通过自适应反馈循环和可读性评分算法显著提升内容质量和相关性。

Conclusion: 该综合解决方案不仅解决了内容过滤和推荐的挑战，还促进了更吸引人的社交媒体体验，同时保护用户隐私，为数字平台的个性化交互设定了新标准。

Abstract: Our study presents a multifaceted approach to enhancing user interaction and content relevance in social media platforms through a federated learning framework. We introduce personalized LLM Federated Learning and Context-based Social Media models. In our framework, multiple client entities receive a foundational GPT model, which is fine-tuned using locally collected social media data while ensuring data privacy through federated aggregation. Key modules focus on categorizing user-generated content, computing user persona scores, and identifying relevant posts from friends networks. By integrating a sophisticated social engagement quantification method with matrix factorization techniques, our system delivers real-time personalized content suggestions tailored to individual preferences. Furthermore, an adaptive feedback loop, alongside a robust readability scoring algorithm, significantly enhances the quality and relevance of the content presented to users. This comprehensive solution not only addresses the challenges of content filtering and recommendation but also fosters a more engaging social media experience while safeguarding user privacy, setting a new standard for personalized interactions in digital platforms.

</details>


### [164] [RRaPINNs: Residual Risk-Aware Physics Informed Neural Networks](https://arxiv.org/abs/2511.18515)
*Ange-Clément Akazan,Issa Karambal,Jean Medard Ngnotchouye,Abebe Geletu Selassie. W*

Main category: cs.LG

TL;DR: 提出了RRaPINNs框架，通过条件风险价值（CVaR）优化尾部目标，并引入均值超额（ME）代理惩罚项来控制最坏情况的PDE残差，将PINN训练转化为风险敏感优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs最小化平均残差会掩盖大的局部化误差，需要开发能够控制最坏情况残差的方法来提高可靠性。

Method: 使用CVaR进行尾部聚焦优化，引入ME代理惩罚项直接控制最坏情况PDE残差，将PINN训练转化为风险敏感优化并与机会约束公式相关联。

Result: 在多个PDE问题中，RRaPINNs显著减少了尾部残差，同时保持或改善了平均误差，ME代理比直接CVaR铰链提供更平滑的优化。

Conclusion: RRaPINNs为可靠感知的科学机器学习提供了实用路径，机会约束可靠性水平α作为透明旋钮在整体精度和尾部控制之间进行权衡。

Abstract: Physics-informed neural networks (PINNs) typically minimize average residuals, which can conceal large, localized errors. We propose Residual Risk-Aware Physics-Informed Neural Networks PINNs (RRaPINNs), a single-network framework that optimizes tail-focused objectives using Conditional Value-at-Risk (CVaR), we also introduced a Mean-Excess (ME) surrogate penalty to directly control worst-case PDE residuals. This casts PINN training as risk-sensitive optimization and links it to chance-constrained formulations. The method is effective and simple to implement. Across several partial differential equations (PDEs) such as Burgers, Heat, Korteweg-de-Vries, and Poisson (including a Poisson interface problem with a source jump at x=0.5) equations, RRaPINNs reduce tail residuals while maintaining or improving mean errors compared to vanilla PINNs, Residual-Based Attention and its variant using convolution weighting; the ME surrogate yields smoother optimization than a direct CVaR hinge. The chance constraint reliability level $α$ acts as a transparent knob trading bulk accuracy (lower $α$ ) for stricter tail control (higher $α$ ). We discuss the framework limitations, including memoryless sampling, global-only tail budgeting, and residual-centric risk, and outline remedies via persistent hard-point replay, local risk budgets, and multi-objective risk over BC/IC terms. RRaPINNs offer a practical path to reliability-aware scientific ML for both smooth and discontinuous PDEs.

</details>


### [165] [CHIPS: Efficient CLIP Adaptation via Curvature-aware Hybrid Influence-based Data Selection](https://arxiv.org/abs/2511.18519)
*Xinlin Zhuang,Yichen Li,Xiwei Liu,Haolin Yang,Yifan Lu,Ziyun Zou,Yulong Li,Huifa Li,Dongliang Chen,Qinglei Wang,Weiyang Liu,Ying Qian,Jiangming Shi,Imran Razzak*

Main category: cs.LG

TL;DR: CHIPS是一种数据选择方法，通过计算图像-文本对的效用分数来选择高质量数据，在仅使用30%数据时能达到全数据集持续预训练的性能，在10%数据时优于半数据集训练。


<details>
  <summary>Details</summary>
Motivation: 探索数据选择是否能替代大规模数据集进行持续预训练，从数据中心的视角重新审视CLIP在垂直领域的适应问题。

Method: 提出CHIPS方法，通过三个互补因素计算数据效用分数：1) 曲率感知的牛顿式对齐（忠实性）；2) 基于InfoNCE的曲率估计器与JL投影（可扩展性）；3) 选择感知的相关性权重与可学习性（保留性）。

Result: 在17个医学基准测试中达到选择基线的最优性能，仅用30%数据匹配全数据集CPT，仅用10%数据优于半数据集CPT；在31个通用领域基准测试中，在10-30%数据保留预算下性能下降最小。

Conclusion: 有效的数据选择可以显著减少持续预训练所需的数据量，CHIPS方法在医学和通用领域都表现出色，为数据高效的领域适应提供了新思路。

Abstract: Adapting CLIP to vertical domains is typically approached by novel fine-tuning strategies or by continual pre-training (CPT) on large domain-specific datasets. Yet, data itself remains an underexplored factor in this process. We revisit this task from a data-centric perspective: Can effective data selection substitute for large-scale datasets in CPT? We introduce CHIPS (Curvature-aware Hybrid Influence in Projection Subspace), which assigns each image-text pair a utility score that integrates three complementary factors aligned with three goals: faithfulness via a curvature-aware, Newton-style alignment computed in CLIP's end-point subspace; scalability via an InfoNCE-aware curvature estimator with Johnson-Lindenstrauss (JL) sketching; and retention via a selection-aware relevance weight combined with learnability to balance target adaptation against general-domain preservation. We justify this design theoretically by proving a lower-bound guarantee on the proxy's correlation with full-parameter alignment and by characterizing the bias-variance trade-offs introduced by curvature mixing and JL sketching. We evaluate CHIPS empirically across various settings: 1) CHIPS attains state-of-the-art performance among selection baselines on 17 medical benchmarks, matches full-dataset CPT with 30% of the data, and outperforms half-dataset CPT using only 10%; 2) on 31 general-domain benchmarks, CHIPS yields the smallest performance drop under 10-30% data-retention budgets. Code, data, and checkpoints will be released.

</details>


### [166] [Hyperspectral Variational Autoencoders for Joint Data Compression and Component Extraction](https://arxiv.org/abs/2511.18521)
*Core Francisco Park,Manuel Perez-Carrasco,Caroline Nowlan,Cecilia Garraffo*

Main category: cs.LG

TL;DR: 使用变分自编码器(VAE)对NASA TEMPO卫星高光谱数据进行514倍压缩，重建误差比信号低1-2个数量级，同时研究压缩潜空间中大气信息的保留程度。


<details>
  <summary>Details</summary>
Motivation: 地球静止轨道高光谱卫星每天产生TB级数据，给存储、传输和科学社区分发带来严峻挑战，需要高效的压缩方法。

Method: 采用变分自编码器(VAE)方法压缩高光谱观测数据，并使用线性和非线性探针从压缩潜空间中提取Level-2产品(NO2、O3、HCHO、云分数)。

Result: 实现514倍压缩，重建误差比信号低1-2个数量级；云分数和总臭氧提取性能良好(R^2=0.93和0.81)，但对流层痕量气体提取困难(NO2 R^2=0.20，HCHO R^2=0.51)。

Conclusion: 神经压缩能显著减少高光谱数据量同时保留关键大气信号，解决了新一代地球观测系统的关键瓶颈问题。

Abstract: Geostationary hyperspectral satellites generate terabytes of data daily, creating critical challenges for storage, transmission, and distribution to the scientific community. We present a variational autoencoder (VAE) approach that achieves x514 compression of NASA's TEMPO satellite hyperspectral observations (1028 channels, 290-490nm) with reconstruction errors 1-2 orders of magnitude below the signal across all wavelengths. This dramatic data volume reduction enables efficient archival and sharing of satellite observations while preserving spectral fidelity. Beyond compression, we investigate to what extent atmospheric information is retained in the compressed latent space by training linear and nonlinear probes to extract Level-2 products (NO2, O3, HCHO, cloud fraction). Cloud fraction and total ozone achieve strong extraction performance (R^2 = 0.93 and 0.81 respectively), though these represent relatively straightforward retrievals given their distinct spectral signatures. In contrast, tropospheric trace gases pose genuine challenges for extraction (NO2 R^2 = 0.20, HCHO R^2 = 0.51) reflecting their weaker signals and complex atmospheric interactions. Critically, we find the VAE encodes atmospheric information in a semi-linear manner - nonlinear probes substantially outperform linear ones - and that explicit latent supervision during training provides minimal improvement, revealing fundamental encoding challenges for certain products. This work demonstrates that neural compression can dramatically reduce hyperspectral data volumes while preserving key atmospheric signals, addressing a critical bottleneck for next-generation Earth observation systems. Code - https://github.com/cfpark00/Hyperspectral-VAE

</details>


### [167] [TimePre: Bridging Accuracy, Efficiency, and Stability in Probabilistic Time-Series Forecasting](https://arxiv.org/abs/2511.18539)
*Lingyu Jiang,Lingyu Xu,Peiran Li,Qianwen Ge,Dingyi Zhuang,Shuo Xing,Wenjing Chen,Xiangbo Gao,Ting-Hsuan Chen,Xueying Zhan,Xin Zhang,Ziming Zhang,Zhengzhong Tu,Michael Zielewski,Kazunori Yamada,Fangzhou Lin*

Main category: cs.LG

TL;DR: TimePre是一个新颖的概率时间序列预测框架，通过稳定实例归一化(SIN)成功将MLP模型的高效性与多选择学习(MCL)的分布灵活性相结合，解决了传统MCL方法的训练不稳定和假设崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有概率时间序列预测方法存在效率与稳定性矛盾：扩散模型计算成本高，而非采样方法如MCL与高效MLP骨干网络结合时会出现严重的训练不稳定和假设崩溃问题。

Method: 提出TimePre框架，核心是稳定实例归一化(SIN)层，通过校正通道级统计偏移来稳定混合架构，彻底解决假设崩溃问题。

Result: 在6个基准数据集上的实验表明，TimePre在关键概率指标上达到新的最先进精度，推理速度比基于采样的模型快几个数量级，且表现出稳定的性能扩展。

Conclusion: TimePre填补了概率预测中准确性、效率和稳定性之间的长期差距，为不确定性感知决策提供了实用解决方案。

Abstract: Probabilistic Time-Series Forecasting (PTSF) is critical for uncertainty-aware decision making, but existing generative models, such as diffusion-based approaches, are computationally prohibitive due to expensive iterative sampling. Non-sampling frameworks like Multiple Choice Learning (MCL) offer an efficient alternative, but suffer from severe training instability and hypothesis collapse, which has historically hindered their performance. This problem is dramatically exacerbated when attempting to combine them with modern, efficient MLP-based backbones. To resolve this fundamental incompatibility, we propose TimePre, a novel framework that successfully unifies the efficiency of MLP-based models with the distributional flexibility of the MCL paradigm. The core of our solution is Stabilized Instance Normalization (SIN), a novel normalization layer that explicitly remedies this incompatibility. SIN stabilizes the hybrid architecture by correcting channel-wise statistical shifts, definitively resolving the catastrophic hypothesis collapse. Extensive experiments on six benchmark datasets demonstrate that TimePre achieves new state-of-the-art accuracy on key probabilistic metrics. Critically, TimePre achieves inference speeds orders of magnitude faster than sampling-based models and, unlike prior MCL work, demonstrates stable performance scaling. It thus bridges the long-standing gap between accuracy, efficiency, and stability in probabilistic forecasting.

</details>


### [168] [In Search of Goodness: Large Scale Benchmarking of Goodness Functions for the Forward-Forward Algorithm](https://arxiv.org/abs/2511.18567)
*Arya Shah,Vaibhav Tripathi*

Main category: cs.LG

TL;DR: 本文对Forward-Forward算法中的21种不同goodness函数进行了基准测试，发现某些替代函数在分类准确率上显著优于标准基线，同时揭示了预测性能与环境成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: Forward-Forward算法的有效性依赖于goodness函数的定义，但当前实现主要使用简单的平方和度量，不清楚这是否是最优选择。

Method: 在四个标准图像数据集（MNIST、FashionMNIST、CIFAR-10、STL-10）上对21种不同的goodness函数进行基准测试，评估分类准确率、能耗和碳足迹。

Result: 发现某些替代goodness函数显著优于标准基线：game_theoretic_local在MNIST上达到97.15%准确率，softmax_energy_margin_local在FashionMNIST上达到82.84%，triplet_margin_local在STL-10上达到37.69%。同时观察到计算效率存在显著差异。

Conclusion: goodness函数是FF算法设计中的关键超参数，需要在预测性能和环境成本之间进行权衡。

Abstract: The Forward-Forward (FF) algorithm offers a biologically plausible alternative to backpropagation, enabling neural networks to learn through local updates. However, FF's efficacy relies heavily on the definition of "goodness", which is a scalar measure of neural activity. While current implementations predominantly utilize a simple sum-of-squares metric, it remains unclear if this default choice is optimal. To address this, we benchmarked 21 distinct goodness functions across four standard image datasets (MNIST, FashionMNIST, CIFAR-10, STL-10), evaluating classification accuracy, energy consumption, and carbon footprint. We found that certain alternative goodness functions inspired from various domains significantly outperform the standard baseline. Specifically, \texttt{game\_theoretic\_local} achieved 97.15\% accuracy on MNIST, \texttt{softmax\_energy\_margin\_local} reached 82.84\% on FashionMNIST, and \texttt{triplet\_margin\_local} attained 37.69\% on STL-10. Furthermore, we observed substantial variability in computational efficiency, highlighting a critical trade-off between predictive performance and environmental cost. These findings demonstrate that the goodness function is a pivotal hyperparameter in FF design. We release our code on \href{https://github.com/aryashah2k/In-Search-of-Goodness}{Github} for reference and reproducibility.

</details>


### [169] [SAMBA: Toward a Long-Context EEG Foundation Model via Spatial Embedding and Differential Mamba](https://arxiv.org/abs/2511.18571)
*Jiazhen Hong,Geoffrey Mackellar,Soheila Ghane*

Main category: cs.LG

TL;DR: SAMBA是一个基于Mamba的U型编码器-解码器架构的自监督学习框架，用于长序列脑电图建模，通过时间语义随机掩码、多头差分Mamba模块和空间自适应输入嵌入等技术，有效捕获EEG数据中的长程时间依赖性和空间变异性。


<details>
  <summary>Details</summary>
Motivation: EEG数据的高采样率和长记录时长需要能够建模长序列的表示模型，但现有Transformer模型因二次复杂度难以扩展到长上下文，且电极配置差异和个体间信号差异对开发通用基础模型构成挑战。

Method: 提出SAMBA框架，包含：1）时间语义随机掩码进行语义级序列重建；2）多头差分Mamba模块抑制冗余并突出显著时间结构；3）空间自适应输入嵌入在三维欧几里得空间中学习统一嵌入，实现跨设备鲁棒性。

Result: 在13个EEG数据集上的实验表明，SAMBA在多种任务、电极配置和序列时长下始终优于最先进方法，同时保持低内存消耗和推理时间。学习到的空间权重图与任务相关神经生理区域高度一致。

Conclusion: SAMBA展示了作为实时脑机接口应用基础模型的可扩展性和实际潜力，其学习到的表示具有可解释性和泛化能力。

Abstract: Long-sequence electroencephalogram (EEG) modeling is essential for developing generalizable EEG representation models. This need arises from the high sampling rate of EEG data and the long recording durations required to capture extended neurological patterns in brain activity. Transformer-based models have shown promise in modeling short sequences of a few seconds; however, their quadratic complexity limits scalability to longer contexts. Moreover, variability in electrode montage across available datasets, along with inter-subject differences in brain signals, pose significant challenges to developing a generalizable and robust foundation model. We propose \textit{SAMBA}, a self-supervised learning framework with a Mamba-based U-shaped encoder-decoder architecture, which effectively captures long-range temporal dependencies and spatial variability in EEG data. Leveraging the inherent ability of Mamba in processing long context sizes, we introduce: (1) \textit{Temporal Semantic Random Masking} for semantic-level sequence reconstruction, (2) a \textit{Multi-Head Differential Mamba} module to suppress redundancy and emphasize salient temporal structures, and (3) a \textit{Spatial-Adaptive Input Embedding} that learns unified embeddings in a three-dimensional Euclidean space, enabling robustness across devices. Experiments on thirteen EEG datasets across diverse tasks, electrode configurations, and sequence durations demonstrate that SAMBA consistently outperforms state-of-the-art methods while maintaining low memory consumption and inference time. We also show the learned spatial weight maps from our embedding module align closely with task-relevant neurophysiological regions, demonstrating the learnability and interpretability of SAMBA. These results highlight SAMBA's scalability and practical potential as a foundation model for real-time brain-computer interface applications.

</details>


### [170] [CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning](https://arxiv.org/abs/2511.18611)
*Mengdi Wang,Efe Bozkir,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: CycleSL是一种新颖的无聚合分割学习框架，通过将服务器端训练视为独立的高层机器学习任务，采用循环更新机制来提升可扩展性和模型性能，解决了传统分割学习中的服务器资源开销和客户端漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统分割学习存在可扩展性差、服务器资源开销大、模型性能下降等问题，特别是并行分割学习和分割联邦学习中的模型复制聚合导致高资源消耗，客户端漂移和延迟影响收敛性能。

Method: 受交替块坐标下降启发，CycleSL将服务器端训练视为独立的高层机器学习任务，对客户端提取的特征进行重采样以减少异构性和漂移，采用循环更新机制：先优化服务器模型，然后使用更新后的服务器进行客户端梯度计算。

Result: 在五个公开数据集上进行了非独立同分布数据和部分客户端参与的基准测试，实证结果表明CycleSL能有效提升模型性能。

Conclusion: CycleSL通过无聚合设计和循环更新机制，显著提升了分割学习的可扩展性和性能，并能与现有方法无缝集成。

Abstract: Split learning emerges as a promising paradigm for collaborative distributed model training, akin to federated learning, by partitioning neural networks between clients and a server without raw data exchange. However, sequential split learning suffers from poor scalability, while parallel variants like parallel split learning and split federated learning often incur high server resource overhead due to model duplication and aggregation, and generally exhibit reduced model performance and convergence owing to factors like client drift and lag. To address these limitations, we introduce CycleSL, a novel aggregation-free split learning framework that enhances scalability and performance and can be seamlessly integrated with existing methods. Inspired by alternating block coordinate descent, CycleSL treats server-side training as an independent higher-level machine learning task, resampling client-extracted features (smashed data) to mitigate heterogeneity and drift. It then performs cyclical updates, namely optimizing the server model first, followed by client updates using the updated server for gradient computation. We integrate CycleSL into previous algorithms and benchmark them on five publicly available datasets with non-iid data distribution and partial client attendance. Our empirical findings highlight the effectiveness of CycleSL in enhancing model performance. Our source code is available at https://gitlab.lrz.de/hctl/CycleSL.

</details>


### [171] [KAN vs LSTM Performance in Time Series Forecasting](https://arxiv.org/abs/2511.18613)
*Tabish Ali Rather,S M Mahmudul Hasan Joy,Nadezda Sukhorukova,Federico Frascoli*

Main category: cs.LG

TL;DR: 比较KAN和LSTM在股票价格预测中的表现，LSTM在所有预测时间跨度上都显著优于KAN，准确率更高但KAN在计算效率上有优势


<details>
  <summary>Details</summary>
Motivation: 评估KAN和LSTM在非确定性股票价格数据预测中的性能，分析预测准确性与可解释性之间的权衡

Method: 使用均方根误差(RMSE)作为评估指标，比较KAN和LSTM在不同预测时间跨度上的表现

Result: LSTM在所有测试的预测时间跨度上都表现出显著优势，而标准KAN虽然具有理论可解释性，但误差率显著更高

Conclusion: LSTM在时间序列预测应用中占据主导地位，而KAN的主要优势在于计算效率，在资源受限且精度要求不高的场景下可能有应用价值

Abstract: This paper compares Kolmogorov-Arnold Networks (KAN) and Long Short-Term Memory networks (LSTM) for forecasting non-deterministic stock price data, evaluating predictive accuracy versus interpretability trade-offs using Root Mean Square Error (RMSE).LSTM demonstrates substantial superiority across all tested prediction horizons, confirming their established effectiveness for sequential data modelling. Standard KAN, while offering theoretical interpretability through the Kolmogorov-Arnold representation theorem, exhibits significantly higher error rates and limited practical applicability for time series forecasting. The results confirm LSTM dominance in accuracy-critical time series applications while identifying computational efficiency as KANs' primary advantage in resource-constrained scenarios where accuracy requirements are less stringent. The findings support LSTM adoption for practical financial forecasting while suggesting that continued research into specialised KAN architectures may yield future improvements.

</details>


### [172] [FOS: A Large-Scale Temporal Graph Benchmark for Scientific Interdisciplinary Link Prediction](https://arxiv.org/abs/2511.18631)
*Kiyan Rezaee,Morteza Ziabakhsh,Niloofar Nikfarjam,Mohammad M. Ghassemi,Yazdan Rezaee Jouryabi,Sadegh Eskandari,Reza Lashgari*

Main category: cs.LG

TL;DR: FOS是一个时间感知的图基准，用于预测科学研究领域之间的新兴跨学科联系，通过分析1827-2024年间65,027个子领域的共现关系来识别前沿科学方向。


<details>
  <summary>Details</summary>
Motivation: 跨学科科学突破通常意外出现，预测新研究领域的形成仍具挑战性。需要建立一个全面的基准来推进科学前沿预测研究。

Method: 构建年度共现图，节点为研究子领域（含语义嵌入），边表示两个领域在同一出版物中的共现（带时间戳）。将新领域对连接预测建模为时间链接预测任务，重点关注首次连接。

Result: 实验表明：(i) 使用领域的长文本描述嵌入显著提高预测准确率；(ii) 不同模型类在不同评估设置下表现优异；(iii) 案例分析显示FOS的顶级链接预测与后续年份实际出现的领域配对一致。

Conclusion: FOS基准为预测科学前沿提供了可复现的研究基础，公开数据集和评估代码将推动该领域发展。

Abstract: Interdisciplinary scientific breakthroughs mostly emerge unexpectedly, and forecasting the formation of novel research fields remains a major challenge. We introduce FOS (Future Of Science), a comprehensive time-aware graph-based benchmark that reconstructs annual co-occurrence graphs of 65,027 research sub-fields (spanning 19 general domains) over the period 1827-2024. In these graphs, edges denote the co-occurrence of two fields in a single publication and are timestamped with the corresponding publication year. Nodes are enriched with semantic embeddings, and edges are characterized by temporal and topological descriptors. We formulate the prediction of new field-pair linkages as a temporal link-prediction task, emphasizing the "first-time" connections that signify pioneering interdisciplinary directions. Through extensive experiments, we evaluate a suite of state-of-the-art temporal graph architectures under multiple negative-sampling regimes and show that (i) embedding long-form textual descriptions of fields significantly boosts prediction accuracy, and (ii) distinct model classes excel under different evaluation settings. Case analyses show that top-ranked link predictions on FOS align with field pairings that emerge in subsequent years of academic publications. We publicly release FOS, along with its temporal data splits and evaluation code, to establish a reproducible benchmark for advancing research in predicting scientific frontiers.

</details>


### [173] [The Locally Deployable Virtual Doctor: LLM Based Human Interface for Automated Anamnesis and Database Conversion](https://arxiv.org/abs/2511.18632)
*Jan Benedikt Ruhland,Doguhan Bahcivan,Jan-Peter Sowa,Ali Canbay,Dominik Heider*

Main category: cs.LG

TL;DR: MedChat是一个本地可部署的虚拟医生框架，结合了基于LLM的医疗聊天机器人和扩散驱动的虚拟形象，用于自动化结构化问诊，确保患者数据隐私保护。


<details>
  <summary>Details</summary>
Motivation: 在临床环境中实现高对话性能的同时减少计算需求，满足严格的数据保护和患者隐私要求，解决现有云系统在医疗领域的隐私和安全问题。

Method: 使用真实和合成医疗对话混合语料库对聊天机器人进行微调，通过低秩适应优化模型效率；实现安全隔离的数据库接口；使用条件扩散模型在潜在空间中生成虚拟形象，与音频特征同步实现逼真语音和面部动画。

Result: 系统展示了完全离线本地部署的可行性，自编码器和扩散网络平滑收敛，MedChat实现了稳定微调并对未见数据具有强泛化能力。

Conclusion: 该系统为AI辅助临床问诊提供了一个隐私保护、资源高效的基础框架，适用于低成本设置。

Abstract: Recent advances in large language models made it possible to achieve high conversational performance with substantially reduced computational demands, enabling practical on-site deployment in clinical environments. Such progress allows for local integration of AI systems that uphold strict data protection and patient privacy requirements, yet their secure implementation in medicine necessitates careful consideration of ethical, regulatory, and technical constraints.
  In this study, we introduce MedChat, a locally deployable virtual physician framework that integrates an LLM-based medical chatbot with a diffusion-driven avatar for automated and structured anamnesis. The chatbot was fine-tuned using a hybrid corpus of real and synthetically generated medical dialogues, while model efficiency was optimized via Low-Rank Adaptation. A secure and isolated database interface was implemented to ensure complete separation between patient data and the inference process. The avatar component was realized through a conditional diffusion model operating in latent space, trained on researcher video datasets and synchronized with mel-frequency audio features for realistic speech and facial animation.
  Unlike existing cloud-based systems, this work demonstrates the feasibility of a fully offline, locally deployable LLM-diffusion framework for clinical anamnesis. The autoencoder and diffusion networks exhibited smooth convergence, and MedChat achieved stable fine-tuning with strong generalization to unseen data. The proposed system thus provides a privacy-preserving, resource-efficient foundation for AI-assisted clinical anamnesis, also in low-cost settings.

</details>


### [174] [Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost](https://arxiv.org/abs/2511.18643)
*Haojun Xia,Xiaoxia Wu,Jisen Li,Robert Wu,Junxiong Wang,Jue Wang,Chenxi Li,Aman Singhal,Alay Dilipbhai Shah,Alpay Ariyak,Donglin Zhuang,Zhongzhu Zhou,Ben Athiwaratkun,Zhen Zheng,Shuaiwen Leon Song*

Main category: cs.LG

TL;DR: Kitty通过算法-系统协同设计实现混合精度KV缓存，在保持精度的同时将KV内存减少近8倍，实现更大的批次和更高的吞吐量。


<details>
  <summary>Details</summary>
Motivation: KV缓存是LLM推理中的主要内存瓶颈，4位KV量化能保持精度，但2位量化会降低精度，特别是在长上下文推理中。

Method: 采用混合精度KV缓存：动态通道精度提升算法对Key缓存通道按敏感度排序，仅保留小部分高精度通道；系统层面提供页面中心KV布局、Triton兼容的页面反量化内核和轻量级运行时管道。

Result: 在七个任务和两个模型系列上，Kitty将KV内存减少近8倍，精度损失可忽略，在相同内存预算下实现最多8倍批次大小和2.1-4.1倍吞吐量提升。

Conclusion: Kitty通过算法-系统协同设计成功解决了2位KV量化精度下降的问题，为LLM推理提供了高效的内存优化方案。

Abstract: The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive experiments show that Dynamic Channel-wise Precision Boost -- which ranks Key-cache channels by sensitivity and keeps only a small fraction at higher precision -- maintains near-zero loss in accuracy drop while approaching 2-bit memory. The main challenge is handling dynamic 4-bit channel boosts while keeping the page layout coalesced and the dequantization uniform, with no scattered reads or hard-coded masks. Kitty addresses these issues by decompose each mixed-precision Key page into two tensors with unified 2-bit precision. Based on this, Kitty provides a page-centric KV layout, Triton-compatible page dequantization kernels, and a lightweight runtime pipeline that preserves coalescing and avoids divergence. Across seven tasks and two model families (Qwen3, LLaMA3), Kitty cuts KV memory by nearly 8x with negligible accuracy loss, enabling up to 8x larger batches and 2.1x-4.1x higher throughput under the same memory budget. We release the full implementation of Kitty at https://github.com/Summer-Summer/Kitty.

</details>


### [175] [Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers](https://arxiv.org/abs/2511.18670)
*Rowan Bradbury,Aniket Srinivasan Ashok,Sai Ram Kasanagottu,Gunmay Jhingran,Shuai Meng*

Main category: cs.LG

TL;DR: DCR（确定性连续替换）通过确定性退火权重混合教师和学生输出，解决了预训练模型中模块替换时的优化稳定性问题，相比随机替换方法收敛更快、对齐更好。


<details>
  <summary>Details</summary>
Motivation: 预训练模型中替换模块（特别是将二次自注意力替换为高效注意力）存在优化稳定性问题，冷启动重新初始化会破坏冻结主干的稳定性。

Method: 提出DCR方法，使用确定性退火权重混合教师和学生的输出，消除随机替换中门控引起的梯度方差。

Result: 在单种子研究中，DCR在控制注意力替换任务上比随机门控和蒸馏基线收敛更快、对齐更强。

Conclusion: DCR为异构算子交换建立了基础，解决了模块替换中的核心稳定性挑战。

Abstract: Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps.

</details>


### [176] [Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition](https://arxiv.org/abs/2511.18671)
*Yan Wang,Ke Deng,Yongli Ren*

Main category: cs.LG

TL;DR: 提出了多智能体交叉熵方法(MCEM)结合单调非线性评论家分解(NCD)，通过增加高价值联合行动的概率来排除次优行为，解决了集中训练与分散执行中的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中集中训练与分散执行(CTDE)框架下的集中-分散不匹配(CDM)问题，即一个智能体的次优行为会降低其他智能体的学习效果。

Method: 使用多智能体交叉熵方法(MCEM)更新策略，通过增加高价值联合行动的概率来排除次优行为；结合单调非线性评论家分解(NCD)；为提高样本效率，采用改进的k步回报和Retrace技术进行离策略学习。

Result: 分析和实验表明，MCEM在连续和离散动作基准测试中都优于最先进的方法。

Conclusion: MCEM方法有效克服了线性分解表达能力有限和非线性分解需要集中梯度的权衡问题，在多智能体强化学习中表现出优越性能。

Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.

</details>


### [177] [QuantKAN: A Unified Quantization Framework for Kolmogorov Arnold Networks](https://arxiv.org/abs/2511.18689)
*Kazi Ahmed Asif Fuad,Lizhong Chen*

Main category: cs.LG

TL;DR: QuantKAN是一个统一框架，用于量化Kolmogorov Arnold Networks (KANs)，在QAT和PTQ两种量化模式下工作，通过扩展现代量化算法到基于样条的层，为KANs建立了首个系统性的低比特量化基准。


<details>
  <summary>Details</summary>
Motivation: KANs虽然具有强大的表达能力和可解释性，但其异构的样条和基础分支参数阻碍了高效量化，与CNNs和Transformers相比，KANs的量化研究尚未得到充分探索。

Method: QuantKAN扩展了LSQ、LSQ+、PACT、DoReFa、QIL、GPTQ、BRECQ、AdaRound、AWQ和HAWQ-V2等现代量化算法，为基于样条的层设计分支特定的量化器，分别处理基础、样条和激活组件。

Result: 实验表明KANs与低比特量化兼容，但表现出强烈的方法-架构交互：LSQ、LSQ+和PACT在4比特下对浅层KAN模型保持接近全精度准确率，而DoReFa在深度KAGN模型下表现最稳定。PTQ中GPTQ和Uniform表现最佳。

Conclusion: QuantKAN框架统一了样条学习和量化，为在资源受限环境中高效部署KANs提供了实用工具和指导方针。

Abstract: Kolmogorov Arnold Networks (KANs) represent a new class of neural architectures that replace conventional linear transformations and node-based nonlinearities with spline-based function approximations distributed along network edges. Although KANs offer strong expressivity and interpretability, their heterogeneous spline and base branch parameters hinder efficient quantization, which remains unexamined compared to CNNs and Transformers. In this paper, we present QuantKAN, a unified framework for quantizing KANs across both quantization aware training (QAT) and post-training quantization (PTQ) regimes. QuantKAN extends modern quantization algorithms, such as LSQ, LSQ+, PACT, DoReFa, QIL, GPTQ, BRECQ, AdaRound, AWQ, and HAWQ-V2, to spline based layers with branch-specific quantizers for base, spline, and activation components. Through extensive experiments on MNIST, CIFAR 10, and CIFAR 100 across multiple KAN variants (EfficientKAN, FastKAN, PyKAN, and KAGN), we establish the first systematic benchmarks for low-bit spline networks. Our results show that KANs, particularly deeper KAGN variants, are compatible with low-bit quantization but exhibit strong method architecture interactions: LSQ, LSQ+, and PACT preserve near full precision accuracy at 4 bit for shallow KAN MLP and ConvNet models, while DoReFa provides the most stable behavior for deeper KAGN under aggressive low-bit settings. For PTQ, GPTQ and Uniform consistently deliver the strongest overall performance across datasets, with BRECQ highly competitive on simpler regimes such as MNIST. Our proposed QuantKAN framework thus unifies spline learning and quantization, and provides practical tools and guidelines for efficiently deploying KANs in real-world, resource-constrained environments.

</details>


### [178] [VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking](https://arxiv.org/abs/2511.18692)
*Kichang Yang,Seonjun Kim,Minjae Kim,Nairan Zhang,Chi Zhang,Youngki Lee*

Main category: cs.LG

TL;DR: Neuron Chunking是一种I/O高效的激活稀疏化策略，通过将神经元重要性分析与存储访问成本结合，显著提升边缘设备上大视觉语言模型的闪存权重卸载性能。


<details>
  <summary>Details</summary>
Motivation: 传统激活稀疏化方法仅基于激活幅度选择神经元，忽略了访问模式对闪存性能的影响，导致I/O效率低下。

Method: 提出神经元分块方法，在内存中对连续神经元进行分组，通过轻量级抽象模型评估访问连续性，选择具有高效用（神经元重要性除以估计延迟）的分块。

Result: 在Jetson Orin Nano和Jetson AGX Orin上分别实现了4.65倍和5.76倍的I/O效率提升。

Conclusion: 通过将稀疏化决策与底层存储行为对齐，Neuron Chunking显著改善了边缘设备上大视觉语言模型的I/O效率。

Abstract: Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.

</details>


### [179] [GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction](https://arxiv.org/abs/2511.18716)
*Zesheng Liu,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: GRIT-LP是一种用于极地雷达图像冰层厚度估计的图变换器，通过分区空间图构建和长程跳跃连接解决了深度图变换器的过平滑和长程依赖建模问题，在RMSE上比现有方法提升24.92%。


<details>
  <summary>Details</summary>
Motivation: 准确估计冰层厚度对于理解积雪积累、重建过去气候模式以及减少未来冰盖演化和海平面上升预测的不确定性至关重要。现有图变换器在深度上受到过平滑和弱长程依赖建模的限制。

Method: 结合归纳几何图学习框架与自注意力机制，引入两个主要创新：分区空间图构建策略形成重叠的全连接局部邻域以保持空间一致性并抑制不相关长程链接的噪声；变换器内部的长程跳跃连接机制改善信息流并减轻深层注意力层的过平滑。

Result: 广泛实验表明GRIT-LP优于当前最先进方法，在均方根误差上提升24.92%。

Conclusion: 结果突显了图变换器通过捕获局部结构特征和跨内部冰层的长程依赖来建模时空模式的有效性，并展示了其在推进数据驱动理解冰冻圈过程方面的潜力。

Abstract: Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes.

</details>


### [180] [Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM](https://arxiv.org/abs/2511.18721)
*Adarsh Kumarappan,Ayushi Mehrotra*

Main category: cs.LG

TL;DR: 提出了(k, ε)-unstable概率框架来改进SmoothLLM防御的认证保证，通过数据驱动的攻击成功率模型提供更可信的安全证书。


<details>
  <summary>Details</summary>
Motivation: SmoothLLM防御依赖于在实践中很少成立的严格k-unstable假设，这限制了安全证书的可信度。

Method: 引入(k, ε)-unstable概率框架，结合攻击成功的经验模型，推导SmoothLLM防御概率的新下界。

Result: 开发了更实用和可信的安全认证机制，能够为从业者提供可操作的安全保证。

Conclusion: 这项工作为LLM安全部署提供了一个实用且理论基础的机制，使其更能抵抗对其安全对齐的利用。

Abstract: The SmoothLLM defense provides a certification guarantee against jailbreaking attacks, but it relies on a strict `k-unstable' assumption that rarely holds in practice. This strong assumption can limit the trustworthiness of the provided safety certificate. In this work, we address this limitation by introducing a more realistic probabilistic framework, `(k, $\varepsilon$)-unstable,' to certify defenses against diverse jailbreaking attacks, from gradient-based (GCG) to semantic (PAIR). We derive a new, data-informed lower bound on SmoothLLM's defense probability by incorporating empirical models of attack success, providing a more trustworthy and practical safety certificate. By introducing the notion of (k, $\varepsilon$)-unstable, our framework provides practitioners with actionable safety guarantees, enabling them to set certification thresholds that better reflect the real-world behavior of LLMs. Ultimately, this work contributes a practical and theoretically-grounded mechanism to make LLMs more resistant to the exploitation of their safety alignments, a critical challenge in secure AI deployment.

</details>


### [181] [LogSyn: A Few-Shot LLM Framework for Structured Insight Extraction from Unstructured General Aviation Maintenance Logs](https://arxiv.org/abs/2511.18727)
*Devansh Agarwal,Maitreyi Chatterjee,Biplab Chatterjee*

Main category: cs.LG

TL;DR: LogSyn是一个使用大语言模型将飞机维护日志从非结构化文本转换为结构化数据的框架，通过少样本上下文学习实现问题-解决叙述的摘要和事件分类。


<details>
  <summary>Details</summary>
Motivation: 飞机维护日志包含宝贵的安全数据，但由于其非结构化文本格式而未被充分利用。

Method: 使用大语言模型进行少样本上下文学习，在6,169条记录上执行受控抽象生成，总结问题-解决叙述并在详细层次本体中分类事件。

Result: 框架能够识别关键故障模式，为维护日志的语义结构化和可操作洞察提取提供了可扩展的方法。

Conclusion: 这项工作为改进航空及相关行业的维护工作流程和预测分析提供了实用路径。

Abstract: Aircraft maintenance logs hold valuable safety data but remain underused due to their unstructured text format. This paper introduces LogSyn, a framework that uses Large Language Models (LLMs) to convert these logs into structured, machine-readable data. Using few-shot in-context learning on 6,169 records, LogSyn performs Controlled Abstraction Generation (CAG) to summarize problem-resolution narratives and classify events within a detailed hierarchical ontology. The framework identifies key failure patterns, offering a scalable method for semantic structuring and actionable insight extraction from maintenance logs. This work provides a practical path to improve maintenance workflows and predictive analytics in aviation and related industries.

</details>


### [182] [Reinforcement Learning for Self-Healing Material Systems](https://arxiv.org/abs/2511.18728)
*Maitreyi Chatterjee,Devansh Agarwal,Biplab Chatterjee*

Main category: cs.LG

TL;DR: 将自愈过程建模为强化学习问题，比较离散动作和连续动作智能体在材料恢复中的表现，发现连续剂量控制的TD3智能体表现最佳


<details>
  <summary>Details</summary>
Motivation: 自主材料系统需要自适应控制方法来最大化结构寿命，平衡结构完整性维护与有限资源消耗

Method: 将自愈过程构建为马尔可夫决策过程，使用Q-learning、DQN和TD3算法在随机仿真环境中进行对比评估

Result: 强化学习控制器显著优于启发式基线，实现近乎完全的材料恢复；TD3智能体在收敛速度和稳定性方面表现最优

Conclusion: 连续剂量控制在动态自愈应用中具有优越性，精细化的比例驱动是必要的

Abstract: The transition to autonomous material systems necessitates adaptive control methodologies to maximize structural longevity. This study frames the self-healing process as a Reinforcement Learning (RL) problem within a Markov Decision Process (MDP), enabling agents to autonomously derive optimal policies that efficiently balance structural integrity maintenance against finite resource consumption. A comparative evaluation of discrete-action (Q-learning, DQN) and continuous-action (TD3) agents in a stochastic simulation environment revealed that RL controllers significantly outperform heuristic baselines, achieving near-complete material recovery. Crucially, the TD3 agent utilizing continuous dosage control demonstrated superior convergence speed and stability, underscoring the necessity of fine-grained, proportional actuation in dynamic self-healing applications.

</details>


### [183] [Large-Scale In-Game Outcome Forecasting for Match, Team and Players in Football using an Axial Transformer Neural Network](https://arxiv.org/abs/2511.18730)
*Michael Horton,Patrick Lucey*

Main category: cs.LG

TL;DR: 提出基于轴向变换器的神经网络模型，用于在足球比赛中实时预测13种球员动作的累计数量，包括球员、团队和比赛三个层面的多时间步预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测足球比赛中球员动作总数对战术决策、体育博彩和电视转播分析有重要价值，需要考虑比赛状态、球员能力、球员互动和比赛动态等多种因素。

Method: 使用轴向变换器神经网络，能够高效捕捉比赛进展的时间动态和球员间的互动关系，提出了一种等效于常规序列变换器的新型轴向变换器设计。

Result: 模型能够做出一致可靠的预测，每场比赛以低延迟高效生成约75,000个实时预测。

Conclusion: 该轴向变换器模型在足球比赛动作预测任务中表现良好，能够有效处理复杂的时空动态和球员互动关系。

Abstract: Football (soccer) is a sport that is characterised by complex game play, where players perform a variety of actions, such as passes, shots, tackles, fouls, in order to score goals, and ultimately win matches. Accurately forecasting the total number of each action that each player will complete during a match is desirable for a variety of applications, including tactical decision-making, sports betting, and for television broadcast commentary and analysis. Such predictions must consider the game state, the ability and skill of the players in both teams, the interactions between the players, and the temporal dynamics of the game as it develops. In this paper, we present a transformer-based neural network that jointly and recurrently predicts the expected totals for thirteen individual actions at multiple time-steps during the match, and where predictions are made for each individual player, each team and at the game-level. The neural network is based on an \emph{axial transformer} that efficiently captures the temporal dynamics as the game progresses, and the interactions between the players at each time-step. We present a novel axial transformer design that we show is equivalent to a regular sequential transformer, and the design performs well experimentally. We show empirically that the model can make consistent and reliable predictions, and efficiently makes $\sim$75,000 live predictions at low latency for each game.

</details>


### [184] [SAOT: An Enhanced Locality-Aware Spectral Transformer for Solving PDEs](https://arxiv.org/abs/2511.18777)
*Chenhong Zhou,Jie Chen,Zaifeng Yang*

Main category: cs.LG

TL;DR: 提出了一种结合小波变换空间频率局部化特性的新型Wavelet Attention模块和Spectral Attention Operator Transformer框架，有效解决了Fourier Neural Operator在捕捉局部细节和高频分量方面的局限性。


<details>
  <summary>Details</summary>
Motivation: Fourier Neural Operator (FNO) 在求解偏微分方程时存在过度平滑解、无法有效捕捉局部细节和高频分量的问题，需要改进以提升模型性能。

Method: 开发了具有线性计算复杂度的Wavelet Attention模块，并构建了Spectral Attention Operator Transformer (SAOT) 混合谱Transformer框架，通过门控融合块整合小波注意力的局部聚焦和傅里叶注意力的全局感受野。

Result: Wavelet Attention显著缓解了Fourier Attention的局限性，大幅超越现有基于小波的神经算子。SAOT在六个算子学习基准测试中达到最先进性能，并展现出强大的离散化不变能力。

Conclusion: 通过整合局部感知和全局谱表示，提出的SAOT框架在算子学习任务中表现出卓越性能，为解决偏微分方程提供了更有效的解决方案。

Abstract: Neural operators have shown great potential in solving a family of Partial Differential Equations (PDEs) by modeling the mappings between input and output functions. Fourier Neural Operator (FNO) implements global convolutions via parameterizing the integral operators in Fourier space. However, it often results in over-smoothing solutions and fails to capture local details and high-frequency components. To address these limitations, we investigate incorporating the spatial-frequency localization property of Wavelet transforms into the Transformer architecture. We propose a novel Wavelet Attention (WA) module with linear computational complexity to efficiently learn locality-aware features. Building upon WA, we further develop the Spectral Attention Operator Transformer (SAOT), a hybrid spectral Transformer framework that integrates WA's localized focus with the global receptive field of Fourier-based Attention (FA) through a gated fusion block. Experimental results demonstrate that WA significantly mitigates the limitations of FA and outperforms existing Wavelet-based neural operators by a large margin. By integrating the locality-aware and global spectral representations, SAOT achieves state-of-the-art performance on six operator learning benchmarks and exhibits strong discretization-invariant ability.

</details>


### [185] [Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs](https://arxiv.org/abs/2511.18783)
*Renchu Guan,Xuyang Li,Yachao Zhang,Wei Pang,Fausto Giunchiglia,Ximing Li,Yonghao Liu,Xiaoyue Feng*

Main category: cs.LG

TL;DR: HONOR是一个新颖的无监督超图对比学习框架，适用于同配性和异配性超图，通过提示机制和自适应注意力聚合来建模异配关系，在理论和实验上都表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的超图神经网络大多基于同配性假设，但现实世界中的超图往往具有显著的异配性结构，这限制了现有方法的性能。

Method: 提出HONOR框架，包含：1）基于提示的超边特征构建策略，保持全局语义一致性同时抑制局部噪声；2）自适应注意力聚合模块，动态捕捉节点对超边的多样化局部贡献；3）结合高通滤波来充分利用异配连接模式。

Result: 理论分析证明了HONOR的优越泛化能力和鲁棒性。大量实验验证HONOR在同配和异配数据集上都优于最先进的基线方法。

Conclusion: HONOR通过显式建模异配关系，能够生成更具区分性和鲁棒性的节点和超边表示，有效解决了现有超图神经网络在同配性假设下的局限性。

Abstract: Hypergraphs, as a generalization of traditional graphs, naturally capture high-order relationships. In recent years, hypergraph neural networks (HNNs) have been widely used to capture complex high-order relationships. However, most existing hypergraph neural network methods inherently rely on the homophily assumption, which often does not hold in real-world scenarios that exhibit significant heterophilic structures. To address this limitation, we propose \textbf{HONOR}, a novel unsupervised \textbf{H}ypergraph c\textbf{ON}trastive learning framework suitable for both hom\textbf{O}philic and hete\textbf{R}ophilic hypergraphs. Specifically, HONOR explicitly models the heterophilic relationships between hyperedges and nodes through two complementary mechanisms: a prompt-based hyperedge feature construction strategy that maintains global semantic consistency while suppressing local noise, and an adaptive attention aggregation module that dynamically captures the diverse local contributions of nodes to hyperedges. Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns, yielding more discriminative and robust node and hyperedge representations. Theoretically, we demonstrate the superior generalization ability and robustness of HONOR. Empirically, extensive experiments further validate that HONOR consistently outperforms state-of-the-art baselines under both homophilic and heterophilic datasets.

</details>


### [186] [Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models](https://arxiv.org/abs/2511.18829)
*Kanav Arora,Girish Narayanswamy,Shwetak Patel,Richard Li*

Main category: cs.LG

TL;DR: 本研究探索了将大型预训练PPG模型蒸馏为适合边缘设备实时推理的小型模型的方法，评估了四种蒸馏策略并分析了模型大小与性能的关系。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习模型在心率估计任务中表现良好，但为了在可穿戴设备上部署，这些模型必须满足严格的内存和延迟限制。

Method: 评估了四种蒸馏策略：硬蒸馏、软蒸馏、解耦知识蒸馏(DKD)和特征蒸馏，通过全面的教师和学生模型容量扫描。

Result: 提出了描述模型大小与性能关系的缩放定律特征化。

Conclusion: 这项早期研究为构建可部署在边缘设备上的生理传感模型奠定了实用且可预测的方法基础。

Abstract: Heart rate estimation from photoplethysmography (PPG) signals generated by wearable devices such as smartwatches and fitness trackers has significant implications for the health and well-being of individuals. Although prior work has demonstrated deep learning models with strong performance in the heart rate estimation task, in order to deploy these models on wearable devices, these models must also adhere to strict memory and latency constraints. In this work, we explore and characterize how large pre-trained PPG models may be distilled to smaller models appropriate for real-time inference on the edge. We evaluate four distillation strategies through comprehensive sweeps of teacher and student model capacities: (1) hard distillation, (2) soft distillation, (3) decoupled knowledge distillation (DKD), and (4) feature distillation. We present a characterization of the resulting scaling laws describing the relationship between model size and performance. This early investigation lays the groundwork for practical and predictable methods for building edge-deployable models for physiological sensing.

</details>


### [187] [Leveraging Duration Pseudo-Embeddings in Multilevel LSTM and GCN Hypermodels for Outcome-Oriented PPM](https://arxiv.org/abs/2511.18830)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: 提出了一种双输入神经网络策略，通过持续时间感知的伪嵌入矩阵将时间重要性转化为紧凑可学习的表示，解决了预测过程监控中时间不规则性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在预测过程监控中难以处理时间不规则性，特别是随机事件持续时间和重叠时间戳，限制了它们在不同数据集上的适应性。

Method: 采用双输入神经网络策略分离事件和序列属性，使用持续时间感知伪嵌入矩阵。在B-LSTM和B-GCN基线模型基础上开发了D-LSTM和D-GCN变体，所有模型都包含自调谐超模型进行自适应架构选择。

Result: 在平衡和不平衡结果预测任务上的实验表明，持续时间伪嵌入输入持续改善泛化能力，降低模型复杂度，并增强可解释性。

Conclusion: 明确的时间编码具有显著优势，为稳健的实时预测过程监控应用提供了灵活的设计方案。

Abstract: Existing deep learning models for Predictive Process Monitoring (PPM) struggle with temporal irregularities, particularly stochastic event durations and overlapping timestamps, limiting their adaptability across heterogeneous datasets. We propose a dual input neural network strategy that separates event and sequence attributes, using a duration-aware pseudo-embedding matrix to transform temporal importance into compact, learnable representations. This design is implemented across two baseline families: B-LSTM and B-GCN, and their duration-aware variants D-LSTM and D-GCN. All models incorporate self-tuned hypermodels for adaptive architecture selection. Experiments on balanced and imbalanced outcome prediction tasks show that duration pseudo-embedding inputs consistently improve generalization, reduce model complexity, and enhance interpretability. Our results demonstrate the benefits of explicit temporal encoding and provide a flexible design for robust, real-world PPM applications.

</details>


### [188] [Auto-ML Graph Neural Network Hypermodels for Outcome Prediction in Event-Sequence Data](https://arxiv.org/abs/2511.18835)
*Fang Wang,Lance Kosca,Adrienne Kosca,Marko Gacesa,Ernesto Damiani*

Main category: cs.LG

TL;DR: HGNN(O)是一个用于事件序列数据结果预测的AutoML GNN超模型框架，通过贝叶斯优化的自调优机制自动适应架构和超参数，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 为事件序列数据的结果预测提供一个鲁棒且可泛化的AutoML-GNN基准方法，避免手动配置架构和超参数。

Method: 扩展了四种架构（单层、双层、双层伪嵌入、双层嵌入）和六种GNN算子，采用基于贝叶斯优化的自调优机制，包含剪枝和早停策略。

Result: 在Traffic Fines数据集上准确率超过0.98，在Patients数据集上加权F1分数达0.86，无需显式处理不平衡问题。

Conclusion: 提出的AutoML-GNN方法为复杂事件序列数据的结果预测提供了鲁棒且可泛化的基准解决方案。

Abstract: This paper introduces HGNN(O), an AutoML GNN hypermodel framework for outcome prediction on event-sequence data. Building on our earlier work on graph convolutional network hypermodels, HGNN(O) extends four architectures-One Level, Two Level, Two Level Pseudo Embedding, and Two Level Embedding-across six canonical GNN operators. A self-tuning mechanism based on Bayesian optimization with pruning and early stopping enables efficient adaptation over architectures and hyperparameters without manual configuration. Empirical evaluation on both balanced and imbalanced event logs shows that HGNN(O) achieves accuracy exceeding 0.98 on the Traffic Fines dataset and weighted F1 scores up to 0.86 on the Patients dataset without explicit imbalance handling. These results demonstrate that the proposed AutoML-GNN approach provides a robust and generalizable benchmark for outcome prediction in complex event-sequence data.

</details>


### [189] [Federated style aware transformer aggregation of representations](https://arxiv.org/abs/2511.18841)
*Mincheol Jeon,Euinam Huh*

Main category: cs.LG

TL;DR: FedSTAR是一个风格感知的联邦学习框架，通过解耦客户端特定风格因子和共享内容表示来解决个性化联邦学习中的领域异构、数据不平衡和通信约束问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习缺乏个性化，单一全局模型无法捕捉客户端特定特征，导致对有高度不同数据分布的客户端产生偏差预测和泛化能力差。

Method: 使用基于Transformer的注意力机制聚合类原型，解耦客户端特定风格因子和共享内容表示，通过交换紧凑原型和风格向量而非完整模型参数来减少通信开销。

Result: 实验结果表明，内容-风格解耦与注意力驱动原型聚合相结合，在不增加通信成本的情况下提高了异构环境中的个性化和鲁棒性。

Conclusion: FedSTAR通过风格感知的联邦学习框架有效解决了个性化联邦学习中的关键挑战，实现了更好的个性化和通信效率。

Abstract: Personalized Federated Learning (PFL) faces persistent challenges, including domain heterogeneity from diverse client data, data imbalance due to skewed participation, and strict communication constraints. Traditional federated learning often lacks personalization, as a single global model cannot capture client-specific characteristics, leading to biased predictions and poor generalization, especially for clients with highly divergent data distributions.
  To address these issues, we propose FedSTAR, a style-aware federated learning framework that disentangles client-specific style factors from shared content representations. FedSTAR aggregates class-wise prototypes using a Transformer-based attention mechanism, allowing the server to adaptively weight client contributions while preserving personalization.
  Furthermore, by exchanging compact prototypes and style vectors instead of full model parameters, FedSTAR significantly reduces communication overhead. Experimental results demonstrate that combining content-style disentanglement with attention-driven prototype aggregation improves personalization and robustness in heterogeneous environments without increasing communication cost.

</details>


### [190] [WaveTuner: Comprehensive Wavelet Subband Tuning for Time Series Forecasting](https://arxiv.org/abs/2511.18846)
*Yubo Wang,Hui He,Chaoxi Niu,Zhendong Niu*

Main category: cs.LG

TL;DR: WaveTuner是一个基于小波分解的时间序列预测框架，通过全频谱子带调谐解决现有方法对高频分量利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有小波方法主要递归分解低频分量，严重忽略了对于精确预测至关重要的细微高频分量，导致信息利用不充分。

Method: 包含两个关键模块：自适应小波细化模块（将时间序列转换为时频系数，动态分配子带权重）和多分支专业化模块（使用多个KAN网络分别建模不同频谱子带）。

Result: 在8个真实世界数据集上的广泛实验表明，WaveTuner在时间序列预测中达到了最先进的性能。

Conclusion: WaveTuner在统一的时频框架内全面调谐全局趋势和局部变化，有效提升了时间序列预测的准确性。

Abstract: Due to the inherent complexity, temporal patterns in real-world time series often evolve across multiple intertwined scales, including long-term periodicity, short-term fluctuations, and abrupt regime shifts. While existing literature has designed many sophisticated decomposition approaches based on the time or frequency domain to partition trend-seasonality components and high-low frequency components, an alternative line of approaches based on the wavelet domain has been proposed to provide a unified multi-resolution representation with precise time-frequency localization. However, most wavelet-based methods suffer from a persistent bias toward recursively decomposing only low-frequency components, severely underutilizing subtle yet informative high-frequency components that are pivotal for precise time series forecasting. To address this problem, we propose WaveTuner, a Wavelet decomposition framework empowered by full-spectrum subband Tuning for time series forecasting. Concretely, WaveTuner comprises two key modules: (i) Adaptive Wavelet Refinement module, that transforms time series into time-frequency coefficients, utilizes an adaptive router to dynamically assign subband weights, and generates subband-specific embeddings to support refinement; and (ii) Multi-Branch Specialization module, that employs multiple functional branches, each instantiated as a flexible Kolmogorov-Arnold Network (KAN) with a distinct functional order to model a specific spectral subband. Equipped with these modules, WaveTuner comprehensively tunes global trends and local variations within a unified time-frequency framework. Extensive experiments on eight real-world datasets demonstrate WaveTuner achieves state-of-the-art forecasting performance in time series forecasting.

</details>


### [191] [Robust and Generalizable GNN Fine-Tuning via Uncertainty-aware Adapter Learning](https://arxiv.org/abs/2511.18859)
*Bo Jiang,Weijun Zhao,Beibei Wang,Xiao Wang,Jin Tang*

Main category: cs.LG

TL;DR: 提出了UAdapterGNN方法，通过集成不确定性学习到GNN适配器中，增强预训练GNN模型在微调过程中对噪声图数据的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的AdapterGNN方法容易受到图数据中各种噪声（如噪声边和模糊节点属性）的影响，表现出有限的泛化能力。如何增强GNN微调的鲁棒性和泛化能力是一个开放问题。

Method: 使用高斯概率适配器来增强预训练GNN模型，当图包含各种噪声时，该方法能自动吸收高斯分布方差变化的影响，从而显著增强模型鲁棒性。

Result: 在多个基准测试上的广泛实验证明了所提出的UAdapterGNN方法的有效性、鲁棒性和高泛化能力。

Conclusion: 通过集成不确定性学习到GNN适配器中，可以有效解决图数据噪声问题，显著提升预训练GNN模型在微调过程中的鲁棒性和泛化性能。

Abstract: Recently, fine-tuning large-scale pre-trained GNNs has yielded remarkable attention in adapting pre-trained GNN models for downstream graph learning tasks. One representative fine-tuning method is to exploit adapter (termed AdapterGNN) which aims to 'augment' the pre-trained model by inserting a lightweight module to make the 'augmented' model better adapt to the downstream tasks. However, graph data may contain various types of noise in downstream tasks, such as noisy edges and ambiguous node attributes. Existing AdapterGNNs are often prone to graph noise and exhibit limited generalizability. How to enhance the robustness and generalization ability of GNNs' fine tuning remains an open problem. In this paper, we show that the above problem can be well addressed by integrating uncertainty learning into the GNN adapter. We propose the Uncertainty-aware Adapter (UAdapterGNN) that fortifies pre-trained GNN models against noisy graph data in the fine-tuning process. Specifically, in contrast to regular AdapterGNN, our UAdapterGNN exploits Gaussian probabilistic adapter to augment the pre-trained GNN model. In this way, when the graph contains various noises,our method can automatically absorb the effects of changes in the variances of the Gaussian distribution, thereby significantly enhancing the model's robustness. Also, UAdapterGNN can further improve the generalization ability of the model on the downstream tasks. Extensive experiments on several benchmarks demonstrate the effectiveness, robustness and high generalization ability of the proposed UAdapterGNN method.

</details>


### [192] [KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit](https://arxiv.org/abs/2511.18868)
*Dezhi Ran,Shuxiao Xie,Mingfang Ji,Ziyue Hua,Mengzhou Wu,Yuan Cao,Yuzhe Guo,Yu Hao,Linyi Li,Yitao Hu,Tao Xie*

Main category: cs.LG

TL;DR: KernelBand是一个将内核优化建模为分层多臂老虎机问题的框架，通过LLM代理战略性地导航优化空间，在减少token使用的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统高质量内核开发需要大量硬件架构和软件优化专业知识，现有LLM代码生成方法由于缺乏硬件领域知识，难以在庞大的优化空间中有效平衡探索与利用。

Method: 将内核优化构建为分层多臂老虎机问题，利用硬件性能分析信息识别有前景的优化策略，采用运行时行为聚类减少跨内核候选的探索开销。

Result: 在TritonBench上的广泛实验表明，KernelBand显著优于最先进方法，用更少的token实现更优性能，且随着计算资源增加持续改进而不饱和。

Conclusion: KernelBand成功解决了LLM在内核优化中的探索-利用平衡问题，为高效内核开发提供了新范式。

Abstract: High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase.

</details>


### [193] [Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning](https://arxiv.org/abs/2511.18871)
*Jian Lu*

Main category: cs.LG

TL;DR: 提出了一种将推理和训练分离的周期性异步框架，通过改进数据加载器和统一三模型架构，在保持算法精度不变的同时实现了至少3倍的整体性能提升。


<details>
  <summary>Details</summary>
Motivation: 主流RL框架中推理和训练在同一设备上同步执行，计算耦合限制了并发性，训练效率成为关键挑战。

Method: 采用推理与训练分离部署策略，通过改进数据加载器构建周期性异步框架，使用统一三模型架构和共享提示注意力掩码减少重复计算。

Result: 在NPU平台上实现了至少3倍的整体性能提升，算法精度与同步方法完全等效。

Conclusion: 该周期性异步框架支持按需弹性扩展各组件，具有广泛应用潜力。

Abstract: Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application.

</details>


### [194] [Hi-SAFE: Hierarchical Secure Aggregation for Lightweight Federated Learning](https://arxiv.org/abs/2511.18887)
*Hyeong-Gun Joo,Songnam Hong,Seunghwan Lee,Dong-Joon Shin*

Main category: cs.LG

TL;DR: 提出了Hi-SAFE，一个轻量级且密码学安全的聚合框架，用于基于符号的联邦学习，解决了隐私保护和通信效率的挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在确保隐私和通信效率方面面临挑战，特别是在物联网和边缘网络等资源受限环境中。基于符号的方法虽然节省带宽，但容易受到推理攻击，而现有的安全聚合技术要么不兼容基于符号的方法，要么开销过大。

Method: 提出了Hi-SAFE框架，核心贡献是基于费马小定理构建了SIGNSGD-MV的高效多数投票多项式，将多数投票表示为有限域上的低次多项式，实现安全评估。还引入了分层子分组策略，确保恒定乘法深度和有界的每用户复杂度。

Result: 该方法能够隐藏中间值，仅揭示最终结果，同时保持与用户数量无关的恒定计算复杂度。

Conclusion: Hi-SAFE为基于符号的联邦学习提供了一个轻量级且密码学安全的聚合解决方案，有效平衡了隐私保护和通信效率的需求。

Abstract: Federated learning (FL) faces challenges in ensuring both privacy and communication efficiency, particularly in resource-constrained environments such as Internet of Things (IoT) and edge networks. While sign-based methods, such as sign stochastic gradient descent with majority voting (SIGNSGD-MV), offer substantial bandwidth savings, they remain vulnerable to inference attacks due to exposure of gradient signs. Existing secure aggregation techniques are either incompatible with sign-based methods or incur prohibitive overhead. To address these limitations, we propose Hi-SAFE, a lightweight and cryptographically secure aggregation framework for sign-based FL. Our core contribution is the construction of efficient majority vote polynomials for SIGNSGD-MV, derived from Fermat's Little Theorem. This formulation represents the majority vote as a low-degree polynomial over a finite field, enabling secure evaluation that hides intermediate values and reveals only the final result. We further introduce a hierarchical subgrouping strategy that ensures constant multiplicative depth and bounded per-user complexity, independent of the number of users n.

</details>


### [195] [Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models](https://arxiv.org/abs/2511.18890)
*Yonggan Fu,Xin Dong,Shizhe Diao,Matthijs Van keirsbilck,Hanrong Ye,Wonmin Byeon,Yashaswi Karnati,Lucas Liebenwein,Hannah Zhang,Nikolaus Binder,Maksim Khadkevich,Alexander Keller,Jan Kautz,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.LG

TL;DR: 该研究提出了一种基于真实设备延迟优化的小语言模型设计方法，通过分析深度-宽度比和算子选择两个关键因素，结合进化搜索框架和权重归一化技术，开发出Nemotron-Flash系列模型，在准确性和效率方面显著超越现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有小语言模型设计主要关注参数数量优化，但参数效率并不一定转化为实际设备上的速度提升。本研究旨在识别影响小语言模型真实设备延迟的关键因素，为以真实延迟为主要考虑因素的SLM设计和训练提供通用原则和方法。

Method: 1. 分析深度-宽度比对延迟的影响；2. 探索高效注意力替代算子；3. 构建进化搜索框架自动发现延迟最优的算子组合；4. 使用权重归一化技术增强训练效果。

Result: 提出的Nemotron-Flash模型在准确性和效率方面显著提升：相比Qwen3-1.7B/0.6B，平均准确率提升超过5.5%，延迟降低1.3倍/1.9倍，吞吐量提高18.7倍/45.6倍。

Conclusion: 通过综合考虑架构设计和训练优化，本研究成功推进了小语言模型的准确性-效率前沿，证明了真实设备延迟导向的设计方法的重要性。

Abstract: Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.

</details>


### [196] [VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL](https://arxiv.org/abs/2511.18902)
*Zengjie Hu,Jiantao Qiu,Tianyi Bai,Haojin Yang,Binhang Yuan,Qi Jing,Conghui He,Wentao Zhang*

Main category: cs.LG

TL;DR: VADE是一个基于方差感知的动态采样框架，通过在线样本难度估计解决组策略优化中的梯度消失问题，提高训练信号同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的组策略优化方法在组内所有响应获得相同奖励时会出现梯度消失问题，导致训练信号减弱。现有解决方案存在计算开销大或缺乏实时适应性的问题。

Method: VADE框架包含三个核心组件：使用Beta分布进行在线样本级难度估计、通过估计正确概率最大化信息增益的Thompson采样器、以及在策略演化下保持稳健估计的双尺度先验衰减机制。

Result: 在多模态推理基准测试中，VADE在性能和样本效率方面持续优于强基线方法，同时显著降低了计算开销。

Conclusion: VADE能够作为即插即用组件无缝集成到现有的基于组的强化学习算法中，有效解决梯度消失问题并提升训练效率。

Abstract: Group-based policy optimization methods like GRPO and GSPO have become standard for training multimodal models, leveraging group-wise rollouts and relative advantage estimation. However, they suffer from a critical \emph{gradient vanishing} problem when all responses within a group receive identical rewards, causing advantage estimates to collapse and training signals to diminish. Existing attempts to mitigate this issue fall into two paradigms: filtering-based and sampling-based methods. Filtering-based methods first generate rollouts broadly and then retroactively filter out uninformative groups, leading to substantial computational overhead. Sampling-based methods proactively select effective samples before rollout but rely on static criteria or prior dataset knowledge, lacking real-time adaptability. To address these issues, we propose \textbf{VADE}, a \textbf{V}ariance-\textbf{A}ware \textbf{D}ynamic sampling framework via online sample-level difficulty \textbf{E}stimation. Our framework integrates three key components: online sample-level difficulty estimation using Beta distributions, a Thompson sampler that maximizes information gain through the estimated correctness probability, and a two-scale prior decay mechanism that maintains robust estimation under policy evolution. This three components design enables VADE to dynamically select the most informative samples, thereby amplifying training signals while eliminating extra rollout costs. Extensive experiments on multimodal reasoning benchmarks show that VADE consistently outperforms strong baselines in both performance and sample efficiency, while achieving a dramatic reduction in computational overhead. More importantly, our framework can serves as a plug-and-play component to be seamlessly integrated into existing group-based RL algorithms. Code and models are available at https://VADE-RL.github.io.

</details>


### [197] [How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining](https://arxiv.org/abs/2511.18903)
*Kairong Luo,Zhenbo Sun,Haodong Wen,Xinyu Shi,Jiarui Cui,Chenyi Dang,Kaifeng Lyu,Wenguang Chen*

Main category: cs.LG

TL;DR: 研究发现课程式预训练效果受限的原因是数据质量升序排列与学习率衰减计划不兼容，提出两种简单策略来缓解这一问题，在标准基准测试上平均得分提升1.64%。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据稀缺，LLM通常在混合质量数据上训练。课程式预训练（按质量升序排列数据）是更好利用高质量数据的自然方法，但先前研究显示其改进有限。

Method: 识别出课程式训练与学习率衰减计划的不兼容性，提出两种策略：使用更温和的学习率衰减计划（最终学习率仅略小于峰值学习率），以及用模型平均替代学习率衰减（对最后几个检查点进行加权平均）。

Result: 结合这两种策略，在标准基准测试套件上平均得分比随机洗牌提高1.64%，在1.5B参数模型上验证了不同数据质量指标的有效性。

Conclusion: 研究呼吁重新评估课程式LLM预训练方法，并强调了数据课程与优化方法协同设计的潜力。

Abstract: Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.

</details>


### [198] [Learning Solution Operators for Partial Differential Equations via Monte Carlo-Type Approximation](https://arxiv.org/abs/2511.18930)
*Salah Eddine Choutri,Prajwal Chauhan,Othmane Mazhar,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: MCNO提出了一种轻量级架构，通过蒙特卡洛方法直接逼近核积分来学习参数化PDE的解算子，无需谱或平移不变性假设，能在不同网格分辨率下泛化。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子（如傅里叶神经算子）依赖谱假设和平移不变性，MCNO旨在提供不依赖这些假设的轻量级替代方案。

Method: 使用蒙特卡洛方法直接逼近核积分，核表示为固定随机采样点上的可学习张量，无需固定全局基函数或训练期间重复采样。

Result: 在标准1D PDE基准测试中，MCNO以较低计算成本实现了竞争性精度。

Conclusion: MCNO为谱和基于图的神经算子提供了简单实用的替代方案，具有轻量级和灵活性的优势。

Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a lightweight architecture for learning solution operators for parametric PDEs by directly approximating the kernel integral using a Monte Carlo approach. Unlike Fourier Neural Operators, MCNO makes no spectral or translation-invariance assumptions. The kernel is represented as a learnable tensor over a fixed set of randomly sampled points. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with low computational cost, providing a simple and practical alternative to spectral and graph-based neural operators.

</details>


### [199] [SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression](https://arxiv.org/abs/2511.18936)
*Santhosh G S,Saurav Prakash,Balaraman Ravindran*

Main category: cs.LG

TL;DR: SWAN是一种无需微调的KV缓存压缩框架，通过正交矩阵旋转和剪枝直接用于注意力计算，无需解压缩步骤，在50-60%内存节省下仍保持接近原始性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在自回归推理时面临KV缓存内存占用大的瓶颈，现有压缩技术存在信息丢失、固定限制或解压缩计算开销大的问题。

Method: 使用离线正交矩阵对KV缓存进行旋转和剪枝，无需解压缩即可直接用于注意力计算，并支持运行时可调压缩级别。

Result: 在50-60%的每token KV缓存内存节省下，性能接近未压缩基线，且具有运行时可调压缩级别的灵活性。

Conclusion: SWAN提供了一种无解压缩设计、高压缩性能下保持性能、适应性强的实用高效LLM长上下文服务解决方案。

Abstract: Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.

</details>


### [200] [MIST: Mutual Information Via Supervised Training](https://arxiv.org/abs/2511.18945)
*German Gritsai,Megan Richards,Maxime Méloux,Kyunghyun Cho,Maxime Peyrard*

Main category: cs.LG

TL;DR: 提出完全数据驱动的互信息估计器设计方法，使用神经网络参数化估计函数，在大规模元数据集上训练，支持变样本量和维度，提供分位数回归的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统互信息估计方法依赖理论保证但缺乏灵活性，需要开发完全经验性的估计器，在保持效率的同时提供可靠的不确定性估计。

Method: 使用神经网络MIST参数化互信息估计函数，在625,000个合成联合分布上训练，采用二维注意力机制处理变样本量和维度，优化分位数回归损失进行不确定性量化。

Result: 学习的估计器在不同样本量和维度下显著优于经典基线方法，分位数区间校准良好且比自举置信区间更可靠，推理速度比现有神经基线快几个数量级。

Conclusion: 该框架提供了可训练、完全可微的估计器，可嵌入更大学习流程，通过可逆变换适应任意数据模态，为互信息估计开辟了新的经验性途径。

Abstract: We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.

</details>


### [201] [Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation](https://arxiv.org/abs/2511.18958)
*Qisen Chai,Yansong Wang,Junjie Huang,Tao Jia*

Main category: cs.LG

TL;DR: Cutter是一个双智能体强化学习框架，用于压缩图结构数据以高效评估图鲁棒性，包含关键节点检测和冗余节点检测两个智能体，通过轨迹级奖励塑造、原型塑造和跨智能体模仿等策略提升压缩质量。


<details>
  <summary>Details</summary>
Motivation: 随着图结构数据规模增长，评估其对抗攻击下的鲁棒性变得计算昂贵且难以扩展，需要压缩图以保留拓扑结构和鲁棒性特征。

Method: 提出Cutter双智能体强化学习框架：关键节点检测智能体(VDA)识别结构关键节点，冗余节点检测智能体(RDA)识别冗余节点；采用轨迹级奖励塑造、原型塑造和跨智能体模仿三种策略提升学习效率和压缩质量。

Result: 在多个真实世界图上的实验表明，Cutter生成的压缩图保留了关键静态拓扑属性，在各种攻击场景下展现出与原图高度一致的鲁棒性退化趋势。

Conclusion: Cutter显著提高了图鲁棒性评估效率，同时不损害评估保真度，为大规模图鲁棒性分析提供了高效解决方案。

Abstract: As graph-structured data grow increasingly large, evaluating their robustness under adversarial attacks becomes computationally expensive and difficult to scale. To address this challenge, we propose to compress graphs into compact representations that preserve both topological structure and robustness profile, enabling efficient and reliable evaluation.We propose Cutter, a dual-agent reinforcement learning framework composed of a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA), which collaboratively identify structurally vital and redundant nodes for guided compression. Cutter incorporates three key strategies to enhance learning efficiency and compression quality: trajectory-level reward shaping to transform sparse trajectory returns into dense, policy-equivalent learning signals; prototype-based shaping to guide decisions using behavioral patterns from both highand low-return trajectories; and cross-agent imitation to enable safer and more transferable exploration. Experiments on multiple real-world graphs demonstrate that Cutter generates compressed graphs that retain essential static topological properties and exhibit robustness degradation trends highly consistent with the original graphs under various attack scenarios, thereby significantly improving evaluation efficiency without compromising assessment fidelity.

</details>


### [202] [AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention](https://arxiv.org/abs/2511.18960)
*Lei Xiao,Jifeng Li,Juntao Gao,Feiyang Ye,Yan Jin,Jingjing Qian,Jing Zhang,Yong Wu,Xiaoyuan Yu*

Main category: cs.LG

TL;DR: AVA-VLA是一个基于POMDP视角的视觉-语言-动作模型，通过引入主动视觉注意力机制，利用历史上下文动态调制视觉处理，在机器人任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型通常将任务建模为MDP，在每个时间步独立处理视觉输入，这种历史无关的设计无法有效利用历史上下文，在动态顺序决策中不够理想。

Method: 从POMDP角度重新定义问题，提出AVA-VLA框架，引入主动视觉注意力(AVA)模块，利用循环状态（代理信念状态的神经近似）动态调制视觉处理，基于历史上下文主动处理任务相关的视觉标记。

Result: 在LIBERO和CALVIN等流行机器人基准测试中实现了最先进的性能，并在双臂机器人平台上验证了实际应用性和鲁棒的仿真到真实迁移能力。

Conclusion: AVA-VLA通过引入基于历史上下文的主动视觉注意力机制，有效解决了现有VLA模型在动态顺序决策中的局限性，展示了优越的性能和实际应用价值。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.

</details>


### [203] [FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning](https://arxiv.org/abs/2511.18977)
*Xin Yuan,Siqi Li,Jiateng Wei,Chengrui Zhu,Yanming Wu,Qingpeng Li,Jiajun Lv,Xiaoke Lan,Jun Chen,Yong Liu*

Main category: cs.LG

TL;DR: FastForward Pruning是一种高效的层间稀疏度分配方法，通过解耦的单步强化学习框架，将策略优化与预算满足问题分离，显著降低计算成本，在LLaMA、Mistral和OPT模型上优于启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法面临效率瓶颈：启发式方法快速但性能次优，基于搜索的方法（如强化学习）计算成本过高，难以在大规模模型上应用。

Method: 提出FastForward Pruning框架，采用解耦的单步强化学习，将策略优化与预算约束分离；采用课程学习策略，从简单任务开始逐步增加复杂度，大幅减少计算开销。

Result: 在LLaMA、Mistral和OPT模型家族上的评估表明，该方法发现的剪枝策略优于强启发式基线；与其他基于搜索的算法相比，以极低计算成本获得竞争性或更优的结果。

Conclusion: FastForward Pruning在搜索效率上具有明显优势，能够高效找到最优的层间稀疏度分配策略，为大模型剪枝提供了实用的解决方案。

Abstract: Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency.

</details>


### [204] [Dynamic Mixture of Experts Against Severe Distribution Shifts](https://arxiv.org/abs/2511.18987)
*Donghu Kim*

Main category: cs.LG

TL;DR: 评估DynamicMoE方法在持续学习和强化学习环境中的表现，并与现有网络扩展方法进行基准测试


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在持续学习中的可塑性-稳定性困境，受生物大脑通过容量增长保持可塑性的启发

Method: 采用动态混合专家(DynamicMoE)架构，通过为不同分布专门化专家来实现持续学习

Result: 论文旨在评估DynamicMoE方法在持续学习和强化学习环境中的有效性

Conclusion: MoE架构为持续学习提供了一种有前景的替代方案，能够专门化专家处理不同分布

Abstract: The challenge of building neural networks that can continuously learn and adapt to evolving data streams is central to the fields of continual learning (CL) and reinforcement learning (RL). This lifelong learning problem is often framed in terms of the plasticity-stability dilemma, focusing on issues like loss of plasticity and catastrophic forgetting. Unlike neural networks, biological brains maintain plasticity through capacity growth, inspiring researchers to explore similar approaches in artificial networks, such as adding capacity dynamically. Prior solutions often lack parameter efficiency or depend on explicit task indices, but Mixture-of-Experts (MoE) architectures offer a promising alternative by specializing experts for distinct distributions. This paper aims to evaluate a DynamicMoE approach for continual and reinforcement learning environments and benchmark its effectiveness against existing network expansion methods.

</details>


### [205] [3D Dynamic Radio Map Prediction Using Vision Transformers for Low-Altitude Wireless Networks](https://arxiv.org/abs/2511.19019)
*Nguyen Duc Minh Quang,Chang Liu,Huy-Trung Nguyen,Shuangyang Li,Derrick Wing Kwan Ng,Wei Xiang*

Main category: cs.LG

TL;DR: 提出3D动态无线电地图(3D-DRM)框架，使用Vision Transformer和Transformer模块学习预测低空无线网络中接收功率的时空演化，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络中由于三维移动性、时变用户密度和有限功率预算，导致基站发射功率动态波动，现有静态无线电地图无法捕捉实时功率变化和时空依赖关系。

Method: 使用Vision Transformer编码器从3D无线电地图提取高维空间表示，结合基于Transformer的模块建模序列依赖关系来预测未来功率分布。

Result: 3D-DRM准确捕捉快速变化的功率动态，在无线电地图重建和短期预测方面显著优于基线模型。

Conclusion: 3D-DRM框架能有效学习低空无线网络中功率的时空演化，为无线电感知网络优化提供支持。

Abstract: Low-altitude wireless networks (LAWN) are rapidly expanding with the growing deployment of unmanned aerial vehicles (UAVs) for logistics, surveillance, and emergency response. Reliable connectivity remains a critical yet challenging task due to three-dimensional (3D) mobility, time-varying user density, and limited power budgets. The transmit power of base stations (BSs) fluctuates dynamically according to user locations and traffic demands, leading to a highly non-stationary 3D radio environment. Radio maps (RMs) have emerged as an effective means to characterize spatial power distributions and support radio-aware network optimization. However, most existing works construct static or offline RMs, overlooking real-time power variations and spatio-temporal dependencies in multi-UAV networks. To overcome this limitation, we propose a {3D dynamic radio map (3D-DRM)} framework that learns and predicts the spatio-temporal evolution of received power. Specially, a Vision Transformer (ViT) encoder extracts high-dimensional spatial representations from 3D RMs, while a Transformer-based module models sequential dependencies to predict future power distributions. Experiments unveil that 3D-DRM accurately captures fast-varying power dynamics and substantially outperforms baseline models in both RM reconstruction and short-term prediction.

</details>


### [206] [OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs](https://arxiv.org/abs/2511.19023)
*Yuting Gao,Weihao Chen,Lan Wang,Ruihan Xu,Qingpei Guo*

Main category: cs.LG

TL;DR: OrdMoE是一种新颖的偏好对齐框架，利用MoE架构中的内部信号构建偏好层次，无需外部人工标注的偏好数据。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好学习方法主要依赖外部人工标注的偏好数据，这些数据收集成本高且劳动密集。

Method: 通过观察路由器专家选择分数隐含的质量感知排名，将专家按路由分数分组为不同等级，分别激活每个等级生成质量递增的响应序列，构建自监督的偏好排序。

Result: 在多个多模态基准测试中，OrdMoE显著提升了多模态MoE LLMs的对齐和整体性能，无需任何人工标注偏好数据即可获得有竞争力的结果。

Conclusion: OrdMoE通过利用MoE架构的内在信号，成功实现了零成本的偏好对齐，为多模态大语言模型的后训练对齐提供了一种高效且经济的方法。

Abstract: Preference learning has recently emerged as a pivotal strategy for post-training alignment of Multimodal Large Language Models (MLLMs). However, existing approaches predominantly rely on external human-annotated preference data, which is costly and labor-intensive to collect. In this work, we propose OrdMoE, a novel preference alignment framework that bypasses the reliance on external human preferences entirely by leveraging intrinsic signals within Mixture-of-Experts (MoE) architectures. Specifically, we observe that the router's expert selection scores implicitly encode a quality-aware ranking of responses (i.e. higher-scoring experts consistently generate higher-quality outputs). Building on this insight, OrdMoE constructs an internal preference hierarchy by grouping experts into ranked tiers based on their per-token routing scores and activating each tier separately to produce a sequence of responses with increasing quality. This yields a zero-cost, self-supervised preference ordering over generated responses, which can be directly optimized using standard preference learning objectives. Extensive experiments across multiple multimodal benchmarks demnstrate that OrdMoE significantly enhances both alignment and overall performance of multimodal Mixture-of-Experts LLMs, achieving competitive results without requiring any human-annotated preference data.

</details>


### [207] [Resolving Node Identifiability in Graph Neural Processes via Laplacian Spectral Encodings](https://arxiv.org/abs/2511.19037)
*Zimo Yan,Zheng Xie,Chang Liu,Yuan Wang*

Main category: cs.LG

TL;DR: 提出了一种拉普拉斯位置编码方法，能够克服传统消息传递图神经网络在Weisfeiler-Lehman测试下的表达能力限制，通过理论证明和实验验证了该方法在节点识别和药物相互作用任务上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递图神经网络的表达能力受限于一维Weisfeiler-Lehman测试，无法区分结构不同的节点，需要更强大的位置编码方法来突破这一限制。

Method: 开发了一种拉普拉斯位置编码，对特征向量符号翻转和特征空间内基旋转具有不变性，结合最短路径与扩散距离的单调关系、基于恒定锚点的光谱三角测量以及具有对数嵌入大小的定量光谱单射性。

Result: 理论证明该编码方法能够从恒定数量的观测中实现节点识别，并在药物-药物相互作用任务上显著提升了ROC曲线下面积和F1分数。

Conclusion: 通过原则性的位置信息解决理论表达能力限制具有实际效益，拉普拉斯位置编码为图神经网络提供了更强的表达能力。

Abstract: Message passing graph neural networks are widely used for learning on graphs, yet their expressive power is limited by the one-dimensional Weisfeiler-Lehman test and can fail to distinguish structurally different nodes. We provide rigorous theory for a Laplacian positional encoding that is invariant to eigenvector sign flips and to basis rotations within eigenspaces. We prove that this encoding yields node identifiability from a constant number of observations and establishes a sample-complexity separation from architectures constrained by the Weisfeiler-Lehman test. The analysis combines a monotone link between shortest-path and diffusion distance, spectral trilateration with a constant set of anchors, and quantitative spectral injectivity with logarithmic embedding size. As an instantiation, pairing this encoding with a neural-process style decoder yields significant gains on a drug-drug interaction task on chemical graphs, improving both the area under the ROC curve and the F1 score and demonstrating the practical benefits of resolving theoretical expressiveness limitations with principled positional information.

</details>


### [208] [Mitigating Participation Imbalance Bias in Asynchronous Federated Learning](https://arxiv.org/abs/2511.19066)
*Xiangyu Chang,Manyi Yao,Srikanth V. Krishnamurthy,Christian R. Shelton,Anirban Chakraborty,Ananthram Swami,Samet Oymak,Amit Roy-Chowdhury*

Main category: cs.LG

TL;DR: 本文分析了异步联邦学习中的异质性放大问题，提出了ACE和ACED方法来解决参与不平衡和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习中，服务器立即使用每个到达客户端的贡献更新全局模型，导致客户端在不同模型版本上训练，产生信息陈旧性。在非IID数据分布下，这种异步模式放大了客户端异质性的负面影响，使更快客户端贡献更频繁更新，从而偏向全局模型。

Method: 提出了ACE方法，通过立即、非缓冲的更新来缓解参与不平衡，利用所有客户端的最新信息。还提出了ACED变体，在客户端多样性和更新陈旧性之间进行平衡。

Result: 在不同模型、任务和异质性/延迟设置下的实验验证了分析，并证明了所提方法的鲁棒性能。

Conclusion: ACE和ACED方法有效缓解了异步联邦学习中的异质性放大问题，通过平衡客户端参与和延迟管理来提升模型性能。

Abstract: In Asynchronous Federated Learning (AFL), the central server immediately updates the global model with each arriving client's contribution. As a result, clients perform their local training on different model versions, causing information staleness (delay). In federated environments with non-IID local data distributions, this asynchronous pattern amplifies the adverse effect of client heterogeneity (due to different data distribution, local objectives, etc.), as faster clients contribute more frequent updates, biasing the global model. We term this phenomenon heterogeneity amplification. Our work provides a theoretical analysis that maps AFL design choices to their resulting error sources when heterogeneity amplification occurs. Guided by our analysis, we propose ACE (All-Client Engagement AFL), which mitigates participation imbalance through immediate, non-buffered updates that use the latest information available from all clients. We also introduce a delay-aware variant, ACED, to balance client diversity against update staleness. Experiments on different models for different tasks across diverse heterogeneity and delay settings validate our analysis and demonstrate the robust performance of our approaches.

</details>


### [209] [EnfoPath: Energy-Informed Analysis of Generative Trajectories in Flow Matching](https://arxiv.org/abs/2511.19087)
*Ziyun Li,Ben Dai,Huancheng Hu,Henrik Boström,Soon Hoe Lim*

Main category: cs.LG

TL;DR: 该论文引入动能路径能量(KPE)作为诊断工具，通过分析基于ODE的采样器生成路径的动能总量，揭示了语义质量与数据密度之间的关键关系。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注端点指标(如保真度、似然、感知质量)，而忽视了采样轨迹所揭示的深层信息。受经典力学启发，作者希望了解生成路径能揭示什么关于样本特征的信息。

Method: 引入动能路径能量(KPE)，量化ODE采样器每个生成路径的总动能消耗。在CIFAR-10和ImageNet-256上进行全面实验分析。

Result: 发现两个关键现象：(i)更高的KPE预测更强的语义质量，表明语义更丰富的样本需要更大的动能努力；(ii)更高的KPE与数据密度呈负相关，信息丰富的样本位于稀疏的低密度区域。

Conclusion: 语义信息丰富的样本自然地存在于数据分布的稀疏前沿，需要更大的生成努力。轨迹级分析为理解生成难度和样本特征提供了物理启发且可解释的框架。

Abstract: Flow-based generative models synthesize data by integrating a learned velocity field from a reference distribution to the target data distribution. Prior work has focused on endpoint metrics (e.g., fidelity, likelihood, perceptual quality) while overlooking a deeper question: what do the sampling trajectories reveal? Motivated by classical mechanics, we introduce kinetic path energy (KPE), a simple yet powerful diagnostic that quantifies the total kinetic effort along each generation path of ODE-based samplers. Through comprehensive experiments on CIFAR-10 and ImageNet-256, we uncover two key phenomena: ({i}) higher KPE predicts stronger semantic quality, indicating that semantically richer samples require greater kinetic effort, and ({ii}) higher KPE inversely correlates with data density, with informative samples residing in sparse, low-density regions. Together, these findings reveal that semantically informative samples naturally reside on the sparse frontier of the data distribution, demanding greater generative effort. Our results suggest that trajectory-level analysis offers a physics-inspired and interpretable framework for understanding generation difficulty and sample characteristics.

</details>


### [210] [Optimization of Deep Learning Models for Dynamic Market Behavior Prediction](https://arxiv.org/abs/2511.19090)
*Shenghan Zhao,Yuzhen Lin,Ximeng Yang,Qiaochu Lu,Haozhong Xue,Gaozhe Jiang*

Main category: cs.LG

TL;DR: 提出一种结合多尺度时间卷积、门控循环模块和时间感知自注意力的混合序列模型，用于电商零售的多时间尺度需求预测，在多个基准测试中表现出更好的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 金融科技领域深度学习的应用日益增多，特别是在预测消费者行为方面具有提升贷款策略和市场效率的潜力。本文专注于零售市场行为，明确定义了SKU级别的多时间尺度需求预测目标。

Method: 使用混合序列模型，结合多尺度时间卷积、门控循环模块和时间感知自注意力机制，采用标准回归损失进行训练，并通过严格的时间分割防止数据泄露。

Result: 与ARIMA/Prophet、LSTM/GRU、LightGBM以及先进的Transformer预测器相比，该模型在MAE、RMSE、sMAPE、MASE和Theil's U_2等指标上均表现出持续准确度提升，在高峰/节假日期间具有更好的鲁棒性。

Conclusion: 通过消融实验和统计显著性测试验证了改进的可靠性，并提供了实现细节以确保可复现性，证明了所提混合模型在零售需求预测中的有效性。

Abstract: The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility.

</details>


### [211] [Edge-Based Predictive Data Reduction for Smart Agriculture: A Lightweight Approach to Efficient IoT Communication](https://arxiv.org/abs/2511.19103)
*Dora Krekovic,Mario Kusek,Ivana Podnar Zarko,Danh Le-Phuoc*

Main category: cs.LG

TL;DR: 提出一种用于边缘计算环境的预测算法，通过预测传感器数据并在偏差超过预设阈值时才传输数据，减少冗余通信，提高能效。


<details>
  <summary>Details</summary>
Motivation: 物联网设备快速增长导致大量传感器数据传输到云端，造成网络拥塞、延迟增加和能耗过高，特别是在资源受限的远程环境中，连续传输变化不大的数据效率低下。

Method: 在网络边缘部署预测滤波器，预测下一个传感器数据点，仅当实际值与预测值的偏差超过预设容差时才触发数据传输；同时云端模型确保数据完整性和系统一致性。

Result: 该双模型策略有效减少了通信开销，通过最小化冗余传输提高了能效，并支持跨站点泛化，使模型可在不同区域部署而无需重新训练。

Conclusion: 该解决方案具有高度可扩展性、能源感知能力，非常适合在远程和带宽受限的物联网环境中优化传感器数据传输。

Abstract: The rapid growth of IoT devices has led to an enormous amount of sensor data that requires transmission to cloud servers for processing, resulting in excessive network congestion, increased latency and high energy consumption. This is particularly problematic in resource-constrained and remote environments where bandwidth is limited, and battery-dependent devices further emphasize the problem. Moreover, in domains such as agriculture, consecutive sensor readings often have minimal variation, making continuous data transmission inefficient and unnecessarily resource intensive. To overcome these challenges, we propose an analytical prediction algorithm designed for edge computing environments and validated through simulation. The proposed solution utilizes a predictive filter at the network edge that forecasts the next sensor data point and triggers data transmission only when the deviation from the predicted value exceeds a predefined tolerance. A complementary cloud-based model ensures data integrity and overall system consistency. This dual-model strategy effectively reduces communication overhead and demonstrates potential for improving energy efficiency by minimizing redundant transmissions. In addition to reducing communication load, our approach leverages both in situ and satellite observations from the same locations to enhance model robustness. It also supports cross-site generalization, enabling models trained in one region to be effectively deployed elsewhere without retraining. This makes our solution highly scalable, energy-aware, and well-suited for optimizing sensor data transmission in remote and bandwidth-constrained IoT environments.

</details>


### [212] [Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty](https://arxiv.org/abs/2511.19124)
*Krishang Sharma*

Main category: cs.LG

TL;DR: 提出了一种新颖的不确定性感知深度学习框架，用于航空发动机剩余使用寿命预测，通过概率建模直接学习数据内在的不确定性，在关键区域预测性能上取得突破性进展。


<details>
  <summary>Details</summary>
Motivation: 准确的剩余使用寿命预测和不确定性量化是航空预测性维护中的关键挑战，现有CMAPSS文献中尚未探索通过概率建模直接学习数据内在不确定性的方法。

Method: 采用分层架构，集成多尺度Inception块提取时间模式、双向LSTM进行序列建模、传感器和时间维度的双重注意力机制，以及贝叶斯输出层同时预测RUL均值和方差。

Result: 在NASA CMAPSS基准测试中，整体RMSE分别为16.22、19.29、16.84和19.98；关键区域（RUL≤30周期）RMSE达到5.14、6.89、5.27和7.16，比传统方法提升25-40%。

Conclusion: 该框架不仅实现了竞争性的整体性能，更在安全关键预测方面建立了新基准，学习到的不确定性提供了良好校准的置信区间，实现了CMAPSS文献中前所未有的风险感知维护调度能力。

Abstract: Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature.

</details>


### [213] [First-order Sobolev Reinforcement Learning](https://arxiv.org/abs/2511.19165)
*Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: 提出一种改进的时间差分学习方法，通过强制一阶贝尔曼一致性来训练价值函数，使其不仅匹配贝尔曼目标值，还匹配关于状态和动作的导数。


<details>
  <summary>Details</summary>
Motivation: 传统时间差分学习只关注价值函数与贝尔曼目标在数值上的一致性，而忽略了局部几何结构的一致性。这可能导致批评家收敛速度较慢和策略梯度不稳定。

Method: 通过可微动态系统对贝尔曼备份进行微分，获得分析一致的梯度目标。使用Sobolev型损失将这些梯度目标纳入批评家目标函数中，使批评家与目标函数的局部几何结构对齐。

Result: 该方法可以无缝集成到现有算法中（如Q学习、DDPG、SAC），不改变整体结构，但可能带来更快的批评家收敛和更稳定的策略梯度。

Conclusion: 一阶TD匹配原则通过强制价值函数在值和导数层面都与贝尔曼目标一致，有望提升强化学习算法的性能和稳定性。

Abstract: We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.

</details>


### [214] [RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning](https://arxiv.org/abs/2511.19168)
*Deyi Ji,Yuekui Yang,Liqun Liu,Peng Shu,Haiyang Wu,Shaogang Tang,Xudong Chen,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.LG

TL;DR: RAVEN++是一个针对视频广告审核的增强框架，通过主动强化学习、细粒度违规理解和渐进式多阶段训练，显著提升了违规检测的精确性、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频广告审核模型（如RAVEN）在细粒度理解、可解释性和泛化能力方面存在不足，无法满足复杂广告内容审核的精确需求。

Method: 提出三个关键创新：1）主动强化学习动态适应不同难度样本；2）通过分层奖励函数和推理蒸馏实现细粒度违规理解；3）渐进式多阶段训练结合知识注入、课程被动RL和主动RL。

Result: 在公开和专有数据集上的广泛实验表明，RAVEN++在细粒度违规理解、推理能力和泛化性方面优于通用LLM和专用模型RAVEN，在线A/B测试也验证了其有效性。

Conclusion: RAVEN++通过创新的强化学习方法和多阶段训练策略，显著提升了视频广告审核的精度和可解释性，为复杂数字内容审核提供了有效解决方案。

Abstract: Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.

</details>


### [215] [From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation](https://arxiv.org/abs/2511.19176)
*Jeeho Shin,Kyungho Kim,Kijung Shin*

Main category: cs.LG

TL;DR: TESMR是一个三阶段框架，通过内容增强、关系增强和学习增强逐步优化多模态特征，在食谱推荐任务中实现了7-15%的Recall@10提升。


<details>
  <summary>Details</summary>
Motivation: 食谱推荐需要有效利用用户-食谱交互之外的多模态特征，系统性地增强这些信号具有很大潜力。

Method: 提出TESMR三阶段框架：1)基于内容的增强使用基础模型进行多模态理解；2)基于关系的增强通过用户-食谱交互的消息传播；3)基于学习的增强通过可学习嵌入的对比学习。

Result: 在两个真实世界数据集上的实验表明，TESMR优于现有方法，Recall@10提高了7-15%。

Conclusion: TESMR通过逐步细化多模态特征为有效嵌入，显著提升了食谱推荐性能，证明了系统增强多模态信号的价值。

Abstract: Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.

</details>


### [216] [Empirical Comparison of Forgetting Mechanisms for UCB-based Algorithms on a Data-Driven Simulation Platform](https://arxiv.org/abs/2511.19240)
*Minxin Chen*

Main category: cs.LG

TL;DR: 提出FDSW-UCB算法，结合折扣长期视角和滑动窗口短期视角，在非平稳多臂老虎机环境中表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统UCB算法在非平稳奖励分布环境中性能显著下降，需要解决动态环境下的决策问题

Method: 开发FDSW-UCB双视角算法，集成折扣机制和滑动窗口机制，并构建基于MovieLens-1M和Open Bandit数据集的半合成仿真平台

Result: 滑动窗口机制(SW-UCB)表现稳健，而折扣方法(D-UCB)存在基本学习失败导致线性遗憾；FDSW-UCB采用乐观聚合策略在动态环境中表现最优

Conclusion: 集成策略本身是成功的关键因素，FDSW-UCB在非平稳环境中具有优越性能

Abstract: Many real-world bandit problems involve non-stationary reward distributions, where the optimal decision may shift due to evolving environments. However, the performance of some typical Multi-Armed Bandit (MAB) models such as Upper Confidence Bound (UCB) algorithms degrades significantly in non-stationary environments where reward distributions change over time. To address this limitation, this paper introduces and evaluates FDSW-UCB, a novel dual-view algorithm that integrates a discount-based long-term perspective with a sliding-window-based short-term view. A data-driven semi-synthetic simulation platform, built upon the MovieLens-1M and Open Bandit datasets, is developed to test algorithm adaptability under abrupt and gradual drift scenarios. Experimental results demonstrate that a well-configured sliding-window mechanism (SW-UCB) is robust, while the widely used discounting method (D-UCB) suffers from a fundamental learning failure, leading to linear regret. Crucially, the proposed FDSW-UCB, when employing an optimistic aggregation strategy, achieves superior performance in dynamic settings, highlighting that the ensemble strategy itself is a decisive factor for success.

</details>


### [217] [MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization](https://arxiv.org/abs/2511.19253)
*Boyuan Wu*

Main category: cs.LG

TL;DR: MAESTRO框架将LLM移出执行循环，作为离线训练架构，通过语义课程生成器和自动奖励合成器来指导标准MARL算法，在交通信号控制任务中实现了更好的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决合作式多智能体强化学习中设计密集奖励函数和构建避免局部最优的课程的挑战，避免现有方法依赖固定启发式或直接在控制循环中使用LLM的高成本和实时性差的问题。

Method: 提出MAESTRO框架，包含语义课程生成器（创建多样化、性能驱动的交通场景）和自动奖励合成器（生成适应课程难度的可执行Python奖励函数），指导MADDPG算法但不增加部署时的推理成本。

Result: 在杭州16个交叉路口的大规模交通信号控制任务中，完整系统相比强课程基线实现了+4.0%的平均回报提升（163.26 vs. 156.93）和2.2%更好的风险调整性能（夏普比率1.53 vs. 0.70）。

Conclusion: LLM可以作为合作式MARL训练的有效高层设计器，通过离线方式生成课程和奖励函数来提升学习性能和稳定性。

Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.

</details>


### [218] [A Nutrition Multimodal Photoplethysmography Language Model](https://arxiv.org/abs/2511.19260)
*Kyle Verrier,Achille Nazaret,Joseph Futoma,Andrew C. Miller,Guillermo Sapiro*

Main category: cs.LG

TL;DR: 提出了一个结合可穿戴设备PPG信号和饮食描述的NPLM模型，通过将生理信号与饮食上下文联合推理，显著提升了日常热量摄入预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 饥饿和饱腹感动态影响饮食行为和代谢健康，但在日常环境中难以捕捉。需要非侵入性的大规模饮食监测方法。

Method: 开发了NPLM模型，将可穿戴设备的连续PPG信号与饮食描述相结合，将PPG投影为语言模型可解释的嵌入，实现生理和饮食上下文的联合推理。基于19,340名参与者和110万餐食-PPG对进行训练。

Result: 模型将日常热量摄入预测准确率比纯文本基线提高了11%，即使去除80%的饮食文本信息仍能保持准确性。在独立验证研究(n=140)中复现了这些发现。

Conclusion: 整合消费者可穿戴设备的生理测量与饮食信息对于大规模非侵入性饮食监测具有重要价值。

Abstract: Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.

</details>


### [219] [Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention](https://arxiv.org/abs/2511.19263)
*Lucas Li,Jean-Baptiste Puel,Florence Carton,Dounya Barrit,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: 提出Solar-GECO模型，通过几何感知的协同注意力机制预测钙钛矿太阳能电池的功率转换效率，结合几何图神经网络和语言模型嵌入，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 钙钛矿太阳能电池性能受多层结构复杂相互作用影响，传统实验筛选方法耗时昂贵，现有机器学习模型忽视钙钛矿晶体几何信息。

Method: 使用几何图神经网络编码钙钛矿吸收层原子结构，语言模型处理传输层化学文本，协同注意力模块捕获层内依赖和层间相互作用，概率回归头预测PCE及其不确定性。

Result: Solar-GECO达到最先进性能，将PCE预测的平均绝对误差从3.066降至2.936，显著优于多个基线模型。

Conclusion: 整合几何和文本信息为PCE预测提供了更强大准确的框架。

Abstract: Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction.

</details>


### [220] [Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry](https://arxiv.org/abs/2511.19264)
*Amirtha Varshini A S,Duminda S. Ranasinghe,Hok Hei Tam*

Main category: cs.LG

TL;DR: 提出了一个用于SynFlowNet（一种生成流网络）的可解释性框架，通过梯度显著性、稀疏自编码器和基序探针三种方法揭示分子设计中的化学逻辑。


<details>
  <summary>Details</summary>
Motivation: 生成流网络在分子设计中很有前景，但其内部决策策略不透明，限制了在药物发现中的应用，化学家需要清晰可解释的结构设计理由。

Method: 集成三种互补方法：梯度显著性结合反事实扰动识别原子环境影响；稀疏自编码器揭示物理化学性质对应的潜在因子；基序探针显示功能基团的显式编码。

Result: 成功揭示了SynFlowNet内部的化学逻辑，包括原子环境如何影响奖励、结构编辑如何改变分子结果，以及功能基团的线性可解码性。

Conclusion: 该框架为SynFlowNet提供了可操作和机制性的洞察，支持透明可控的分子设计。

Abstract: Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.

</details>


### [221] [Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks](https://arxiv.org/abs/2511.19265)
*Bianka Kowalska,Halina Kwaśnicka*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.

</details>


### [222] [Leveraging Spatiotemporal Graph Neural Networks for Multi-Store Sales Forecasting](https://arxiv.org/abs/2511.19267)
*Manish Singh,Arpita Dayama*

Main category: cs.LG

TL;DR: 评估时空图神经网络在多店零售销售预测中的有效性，与ARIMA、LSTM和XGBoost基准模型对比，STGNN在各项误差指标上表现最优。


<details>
  <summary>Details</summary>
Motivation: 研究多店零售环境中店铺间依赖关系对销售预测的影响，探索图神经网络在捕捉店铺间时空依赖方面的潜力。

Method: 使用45家沃尔玛店铺的周销售数据，构建学习自适应图的关系预测框架，通过残差路径预测对数差分销售并重构最终值。

Result: STGNN在所有基准模型中达到最低预测误差，在标准化总绝对误差、P90 MAPE和MAPE方差指标上表现最佳。学习到的邻接矩阵揭示了有意义的功能性店铺集群。

Conclusion: 关系结构显著提升互联零售环境中的预测质量，STGNN是多店需求预测的稳健建模选择。

Abstract: This work evaluates the effectiveness of spatiotemporal Graph Neural Networks (GNNs) for multi-store retail sales forecasting and compares their performance against ARIMA, LSTM, and XGBoost baselines. Using weekly sales data from 45 Walmart stores, we construct a relational forecasting framework that models inter-store dependencies through a learned adaptive graph. The proposed STGNN predicts log-differenced sales and reconstructs final values through a residual path, enabling stable training and improved generalisation. Experiments show that STGNN achieves the lowest overall forecasting error, outperforming all baselines in Normalised Total Absolute Error, P90 MAPE, and variance of MAPE across stores. Analysis of the learned adjacency matrix reveals meaningful functional store clusters and high-influence nodes that emerge without geographic metadata. These results demonstrate that relational structure significantly improves forecast quality in interconnected retail environments and establishes STGNNs as a robust modelling choice for multi-store demand prediction.

</details>


### [223] [CDLM: Consistency Diffusion Language Models For Faster Sampling](https://arxiv.org/abs/2511.19269)
*Minseo Kim,Chenfeng Xu,Coleman Hooper,Harman Singh,Ben Athiwaratkun,Ce Zhang,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: CDLM通过一致性建模和块级因果注意力掩码，解决了扩散语言模型的推理速度瓶颈，实现了3.6x-14.5x的延迟降低，同时保持数学和编程任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然提供了并行生成的范式，但由于需要大量细化步骤且无法使用标准KV缓存，导致推理速度缓慢。

Method: CDLM结合一致性建模来大幅减少采样步骤，通过多令牌最终化实现加速；同时使用块级因果注意力掩码进行微调，使模型完全兼容KV缓存。

Result: 实验显示CDLM在数学和编程任务上实现了3.6倍到14.5倍的延迟降低，同时保持了有竞争力的准确率。

Conclusion: CDLM成功解决了扩散语言模型的推理速度瓶颈，为实际应用提供了可行的加速方案。

Abstract: Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.

</details>


### [224] [Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model](https://arxiv.org/abs/2511.19272)
*Felix Birkel*

Main category: cs.LG

TL;DR: Tiny-TSM是一个小型时间序列基础模型，仅2300万参数，在单个A100 GPU上训练不到一周，通过新的合成数据生成和数据增强管道实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个规模小、训练成本低但性能优异的时间序列基础模型，解决现有大模型训练资源消耗大的问题。

Method: 使用新的合成数据生成和数据增强管道(SynthTS)，引入因果输入归一化方案，采用密集的下一个token预测损失进行训练。

Result: 在中长期预测任务上超越所有评估的时间序列基础模型，短期预测性能与SOTA模型相当，且训练仅需单个A100 GPU。

Conclusion: Tiny-TSM证明了无需神经架构搜索、超参数调优或扩大模型规模，也能实现SOTA性能，为资源受限环境提供了实用解决方案。

Abstract: We present Tiny-TSM, a time series foundation model characterized by small scale, economical training, and state-of-the-art performance. It comprises 23M total parameters, trained on a single A100 GPU in less than a week using a new synthetic data generation and data augmentation pipeline (SynthTS). Without any neural architecture search, hyperparameter tuning, or scaling up model size, Tiny-TSM achieves state-of-the-art performance on a wide range of time series benchmark datasets, often outperforming much larger models and even matching the performance of much larger, industrial-scale, likely highly tuned foundation models. Specifically, Tiny-TSM outperforms all other time series foundation models we evaluated on medium- and long-term forecasting tasks under MSE loss, while short-term accuracy is still competitive with state-of-the-art models.
  We also introduce a causal input normalization scheme that enables time series models to be trained with dense next-token prediction loss, significantly accelerating convergence speed and reducing training time.
  All experiments were conducted on a single A100 GPU, illustrating the practicality of the proposed approach in a resource-constrained setting.

</details>


### [225] [Scalable Bayesian Network Structure Learning Using Tsetlin Machine to Constrain the Search Space](https://arxiv.org/abs/2511.19273)
*Kunal Dumbre,Lei Jiao,Ole-Christoffer Granmo*

Main category: cs.LG

TL;DR: 提出了一种基于Tsetlin Machine的高效贝叶斯网络结构学习方法，通过选择重要变量进行条件独立性测试，显著降低了PC算法的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: PC算法在因果推断中广泛应用，但随着数据集规模增大，其时间复杂度过高，限制了在大规模实际问题中的应用。

Method: 利用Tsetlin Machine提取最重要的字面量，仅对这些选定的字面量进行条件独立性测试，而不是对所有变量进行测试。

Result: 在bnlearn存储库的分类数据集（如Munin1、Hepar2）上评估，该方法不仅显著降低了计算复杂度，而且保持了竞争力的因果发现准确性。

Conclusion: 基于TM的方法为传统PC算法实现提供了可行的替代方案，在不牺牲性能的前提下提高了效率。

Abstract: The PC algorithm is a widely used method in causal inference for learning the structure of Bayesian networks. Despite its popularity, the PC algorithm suffers from significant time complexity, particularly as the size of the dataset increases, which limits its applicability in large-scale real-world problems. In this study, we propose a novel approach that utilises the Tsetlin Machine (TM) to construct Bayesian structures more efficiently. Our method leverages the most significant literals extracted from the TM and performs conditional independence (CI) tests on these selected literals instead of the full set of variables, resulting in a considerable reduction in computational time. We implemented our approach and compared it with various state-of-the-art methods. Our evaluation includes categorical datasets from the bnlearn repository, such as Munin1, Hepar2. The findings indicate that the proposed TM-based method not only reduces computational complexity but also maintains competitive accuracy in causal discovery, making it a viable alternative to traditional PC algorithm implementations by offering improved efficiency without compromising performance.

</details>


### [226] [Closing Gaps in Emissions Monitoring with Climate TRACE](https://arxiv.org/abs/2511.19277)
*Brittany V. Lancellotti,Jordan M. Malof,Aaron Davitt,Gavin McCormick,Shelby Anderson,Pol Carbó-Mestre,Gary Collins,Verity Crane,Zoheyr Doctor,George Ebri,Kevin Foster,Trey M. Gowdy,Michael Guzzardi,John Heal,Heather Hunter,David Kroodsma,Khandekar Mahammad Galib,Paul J. Markakis,Gavin McDonald,Daniel P. Moore,Eric D. Nguyen,Sabina Parvu,Michael Pekala,Christine D. Piatko,Amy Piscopo,Mark Powell,Krsna Raniga,Elizabeth P. Reilly,Michael Robinette,Ishan Saraswat,Patrick Sicurello,Isabella Söldner-Rembold,Raymond Song,Charlotte Underwood,Kyle Bradbury*

Main category: cs.LG

TL;DR: Climate TRACE是一个开放获取平台，提供全球温室气体排放估算，具有增强的细节、覆盖范围和及时性，支持数据驱动的气候行动。


<details>
  <summary>Details</summary>
Motivation: 现有排放数据集缺乏准确性、全球覆盖、高时空分辨率和频繁更新等关键特性，限制了其可操作性。

Method: 综合现有排放数据，优先考虑准确性、覆盖范围和分辨率，并使用特定行业的估算方法来填补数据空白。

Result: 首次提供全球全面的人为排放源（如单个发电厂）排放估算，涵盖2021年1月1日至今，每月更新，报告延迟两个月。

Conclusion: Climate TRACE代表了排放核算和减排领域的重大突破，支持在决策层面进行数据驱动的气候行动。

Abstract: Global greenhouse gas emissions estimates are essential for monitoring and mitigation planning. Yet most datasets lack one or more characteristics that enhance their actionability, such as accuracy, global coverage, high spatial and temporal resolution, and frequent updates. To address these gaps, we present Climate TRACE (climatetrace.org), an open-access platform delivering global emissions estimates with enhanced detail, coverage, and timeliness. Climate TRACE synthesizes existing emissions data, prioritizing accuracy, coverage, and resolution, and fills gaps using sector-specific estimation approaches. The dataset is the first to provide globally comprehensive emissions estimates for individual sources (e.g., individual power plants) for all anthropogenic emitting sectors. The dataset spans January 1, 2021, to the present, with a two-month reporting lag and monthly updates. The open-access platform enables non-technical audiences to engage with detailed emissions datasets for most subnational governments worldwide. Climate TRACE supports data-driven climate action at scales where decisions are made, representing a major breakthrough for emissions accounting and mitigation.

</details>


### [227] [MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings](https://arxiv.org/abs/2511.19279)
*Victor Rambaud,Salvador Mascarenhas,Yair Lakretz*

Main category: cs.LG

TL;DR: MapFormers是基于Transformer的新架构，能够从观测数据中学习认知地图并并行执行路径整合，通过输入依赖的位置编码实现结构与内容的解耦，在OOD泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏人类和动物所具有的认知地图能力，这种能力能够编码实体间的抽象关系，提供强大的OOD泛化能力。

Method: 开发了两种MapFormers变体，通过更新Transformer中的位置编码为输入依赖矩阵，统一绝对和相对位置编码来分别建模情景记忆和工作记忆。

Result: 在包括经典2D导航任务在内的多个任务中测试，MapFormers能够学习底层空间的认知地图，并在OOD泛化（如更长序列）方面达到近乎完美的性能，优于现有架构。

Conclusion: 结果表明，设计用于学习认知地图的模型具有优越性，引入结构偏置实现结构-内容解耦在Transformer中通过输入依赖位置编码是可行的，MapFormers在神经科学和AI领域具有广泛应用前景。

Abstract: A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.

</details>


### [228] [Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning](https://arxiv.org/abs/2511.19299)
*James R. M. Black,Moritz S. Hanke,Aaron Maiwald,Tina Hernandez-Boussard,Oliver M. Crook,Jaspreet Pannu*

Main category: cs.LG

TL;DR: 研究表明，通过微调方法可以绕过基因组语言模型的数据排除安全措施，恢复对有害人类感染病毒的预测能力，这凸显了基因组语言模型安全框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 基因组语言模型在生物数据上的应用引发了滥用担忧，特别是可能被用于生成人类感染病毒的基因组。目前主要通过在预训练数据中过滤病毒序列来降低风险，但这种方法对可微调的开源模型的有效性尚不清楚。

Method: 评估了最先进的基因组语言模型Evo 2，使用110种有害人类感染病毒的序列进行微调，测试其恢复滥用相关预测能力的效果。

Result: 微调后的模型在未见过的病毒序列上表现出较低的困惑度，并且能够识别SARS-CoV-2的免疫逃逸变体（AUROC为0.6），尽管在微调过程中未接触过SARS-CoV-2序列。

Conclusion: 数据排除措施可能被微调方法绕过，基因组语言模型需要更完善的安全框架、评估和缓解措施来确保安全部署。

Abstract: Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.

</details>


### [229] [Understanding the Staged Dynamics of Transformers in Learning Latent Structure](https://arxiv.org/abs/2511.19328)
*Rohan Saha,Farzane Aminmansour,Alona Fyshe*

Main category: cs.LG

TL;DR: 该研究使用Alchemy基准测试探索transformer学习潜在结构的动态过程，发现模型以离散阶段的方式获取能力，先学习粗粒度规则，再学习完整的潜在结构，并揭示了模型在规则组合与分解能力上的不对称性。


<details>
  <summary>Details</summary>
Motivation: 虽然transformer能够从上下文中发现潜在结构，但关于它们如何获取潜在结构不同组成部分的动态过程仍不清楚。本研究旨在深入理解transformer学习潜在结构的动态机制。

Method: 使用Alchemy基准测试，在三个任务变体上训练小型仅解码器transformer：1）从部分上下文信息推断缺失规则，2）组合简单规则解决多步序列，3）分解复杂多步示例以推断中间步骤。通过将每个任务分解为可解释的事件来分析学习过程。

Result: 模型以离散阶段的方式获取能力，首先学习粗粒度规则，然后学习完整的潜在结构。发现了一个关键的不对称性：模型能够稳健地组合基本规则，但在分解复杂示例以发现基本规则方面存在困难。

Conclusion: 这些发现为理解transformer模型如何学习潜在结构提供了新的见解，展示了这些能力在训练过程中如何逐步演化，揭示了模型在组合与分解能力上的不对称特性。

Abstract: While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.

</details>


### [230] [Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data](https://arxiv.org/abs/2511.19330)
*Dominik Luszczynski*

Main category: cs.LG

TL;DR: 本文提出了两种基于斜率的新型对抗攻击方法，能够操纵N-HiTS股票预测模型的输出，使预测斜率翻倍，并能绕过标准安全机制。此外还开发了GAN架构生成逼真合成数据，并设计了恶意软件来注入对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在图像领域已有深入研究，但在时间序列领域特别是金融数据预测方面研究较少。本文旨在填补这一空白，研究针对金融时间序列预测模型的对抗攻击方法。

Method: 提出了两种新的基于斜率的对抗攻击方法：通用斜率攻击和最小二乘斜率攻击，并将其集成到GAN架构中生成合成数据。还设计了恶意软件来在模型推理库中注入对抗攻击。

Result: 新方法能够成功操纵N-HiTS预测，使预测斜率翻倍，并能绕过4层CNN鉴别器，将其特异性降至28%，准确率降至57%。

Conclusion: 机器学习安全研究不应仅关注模型安全性，还需要保护整个处理流程，因为对抗攻击可以通过多种方式渗透系统。

Abstract: A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.

</details>


### [231] [Annotation-Free Class-Incremental Learning](https://arxiv.org/abs/2511.19344)
*Hari Chandana Kuchibhotla,K S Ananth,Vineeth N Balasubramanian*

Main category: cs.LG

TL;DR: 本文提出了注释自由类增量学习(AFCIL)这一更现实的持续学习范式，并开发了CrossWorld CL框架，通过利用外部世界知识作为稳定辅助源，在没有标注的情况下实现有效的持续学习。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法大多假设在整个学习过程中都有标注数据可用，但现实场景中数据往往是无标注且顺序到达的。本文旨在解决在没有标注的情况下如何实现有效的类增量学习这一更具挑战性的问题。

Method: 提出了CrossWorld CL框架：1)为每个下游类别检索语义相关的ImageNet类别；2)通过跨域对齐策略映射下游和ImageNet特征；3)引入新颖的重放策略。该设计让模型在没有标注的情况下发现语义结构，同时保持先前知识的完整性。

Result: 在四个数据集上的实验表明，CrossWorld CL超越了CLIP基线和现有的持续学习及无标注学习方法，证明了世界知识对注释自由持续学习的益处。

Conclusion: 世界知识可以作为注释自由持续学习的有效辅助源，CrossWorld CL框架在更具挑战性的AFCIL场景中表现出色，为现实世界的持续学习应用提供了可行方案。

Abstract: Despite significant progress in continual learning ranging from architectural novelty to clever strategies for mitigating catastrophic forgetting most existing methods rest on a strong but unrealistic assumption the availability of labeled data throughout the learning process. In real-world scenarios, however, data often arrives sequentially and without annotations, rendering conventional approaches impractical. In this work, we revisit the fundamental assumptions of continual learning and ask: Can current systems adapt when labels are absent and tasks emerge incrementally over time? To this end, we introduce Annotation-Free Class-Incremental Learning (AFCIL), a more realistic and challenging paradigm where unlabeled data arrives continuously, and the learner must incrementally acquire new classes without any supervision. To enable effective learning under AFCIL, we propose CrossWorld CL, a Cross Domain World Guided Continual Learning framework that incorporates external world knowledge as a stable auxiliary source. The method retrieves semantically related ImageNet classes for each downstream category, maps downstream and ImageNet features through a cross domain alignment strategy and finally introduce a novel replay strategy. This design lets the model uncover semantic structure without annotations while keeping earlier knowledge intact. Across four datasets, CrossWorld-CL surpasses CLIP baselines and existing continual and unlabeled learning methods, underscoring the benefit of world knowledge for annotation free continual learning.

</details>


### [232] [Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric](https://arxiv.org/abs/2511.19350)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.LG

TL;DR: 提出了一种可扩展的光谱方法来自动估计短文本嵌入的聚类数量，并提出了Cohesion Ratio指标来评估聚类质量，无需真实标签。


<details>
  <summary>Details</summary>
Motivation: 短文本嵌入聚类是NLP的基础任务，但传统方法需要预先指定聚类数量，这在实际应用中具有挑战性。

Method: 使用基于余弦相似度的拉普拉斯特征谱结构来估计聚类数量，采用自适应采样策略实现可扩展性，并提出Cohesion Ratio指标进行无监督评估。

Result: 在6个短文本数据集和4个嵌入模型上的实验表明，使用该方法指导的K-Means和HAC算法显著优于HDBSCAN、OPTICS和Leiden等参数较少的方法。

Conclusion: 该光谱估计器和Cohesion Ratio为短文本数据的无监督组织和评估提供了实用价值。

Abstract: Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.

</details>


### [233] [Leveraging LLMs for reward function design in reinforcement learning control tasks](https://arxiv.org/abs/2511.19355)
*Franklin Cardenoso,Wouter Caarls*

Main category: cs.LG

TL;DR: LEARN-Opt是一个基于LLM的完全自主、模型无关的框架，能够从系统描述和任务目标中自动生成、执行和评估奖励函数，无需初步指标或环境源代码。


<details>
  <summary>Details</summary>
Motivation: 设计有效的奖励函数是强化学习中的主要瓶颈，需要大量人工专业知识且耗时。现有方法需要初步评估指标、人工反馈或环境源代码作为上下文。

Method: 提出LEARN-Opt框架，能够直接从系统描述和任务目标中自主推导性能指标，实现无监督的奖励函数评估和选择。

Result: 实验表明LEARN-Opt性能与最先进方法（如EUREKA）相当或更好，且需要更少先验知识。能够利用低成本LLM找到高性能候选函数。

Conclusion: LEARN-Opt有潜力在不需任何人工定义指标的情况下生成高质量奖励函数，减少工程开销并增强泛化能力。

Abstract: The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.

</details>


### [234] [Enhancing Conformal Prediction via Class Similarity](https://arxiv.org/abs/2511.19359)
*Ariel Fargion,Lahav Dabah,Tom Tirer*

Main category: cs.LG

TL;DR: 本文提出了一种基于类别相似性的共形预测增强方法，通过惩罚组外错误和利用类别相似性来减少预测集大小，同时保证覆盖概率。


<details>
  <summary>Details</summary>
Motivation: 在需要将类别划分为语义组的应用中，用户不仅需要平均预测集小，还需要包含较少语义不同组的预测集。现有共形预测方法主要关注平均预测集大小，未充分考虑类别语义分组的需求。

Method: 1. 给定类别划分时，在CP评分函数中增加惩罚组外错误的项；2. 提出不依赖人工语义划分的模型特定变体，利用类别相似性进一步减少预测集大小；3. 理论分析和证明该策略对组相关指标的优越性。

Result: 理论证明对于常见类别划分，该方法可以减少任何CP评分函数的平均集大小。实证研究涵盖多个CP方法、模型和数据集，显示基于类别相似性的方法能持续提升CP性能。

Conclusion: 提出的基于类别相似性的方法是一种广泛适用的工具，能够有效增强任何CP方法在任何数据集上的性能，特别是在需要语义分组考虑的场景中。

Abstract: Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods.

</details>


### [235] [Neural surrogates for designing gravitational wave detectors](https://arxiv.org/abs/2511.19364)
*Carlos Ruiz-Gonzalez,Sören Arlt,Sebastian Lehner,Arturs Berzins,Yehonathan Drori,Rana X Adhikari,Johannes Brandstetter,Mario Krenn*

Main category: cs.LG

TL;DR: 使用神经网络替代模型加速物理模拟器，在引力波探测器设计中显著提高实验设计效率


<details>
  <summary>Details</summary>
Motivation: 随着实验设置日益复杂，传统CPU模拟器的计算成本成为主要限制，需要寻找更高效的替代方案

Method: 训练神经网络替代引力波物理模拟器Finesse，通过自动微分和GPU并行化，在替代模型训练、逆向设计和模拟器验证之间循环迭代

Result: 该方法在几小时内找到的解决方案优于传统优化器5天才能达到的设计质量

Conclusion: 该框架可广泛应用于模拟器瓶颈阻碍优化和发现的其他领域

Abstract: Physics simulators are essential in science and engineering, enabling the analysis, control, and design of complex systems. In experimental sciences, they are increasingly used to automate experimental design, often via combinatorial search and optimization. However, as the setups grow more complex, the computational cost of traditional, CPU-based simulators becomes a major limitation. Here, we show how neural surrogate models can significantly reduce reliance on such slow simulators while preserving accuracy. Taking the design of interferometric gravitational wave detectors as a representative example, we train a neural network to surrogate the gravitational wave physics simulator Finesse, which was developed by the LIGO community. Despite that small changes in physical parameters can change the output by orders of magnitudes, the model rapidly predicts the quality and feasibility of candidate designs, allowing an efficient exploration of large design spaces. Our algorithm loops between training the surrogate, inverse designing new experiments, and verifying their properties with the slow simulator for further training. Assisted by auto-differentiation and GPU parallelism, our method proposes high-quality experiments much faster than direct optimization. Solutions that our algorithm finds within hours outperform designs that take five days for the optimizer to reach. Though shown in the context of gravitational wave detectors, our framework is broadly applicable to other domains where simulator bottlenecks hinder optimization and discovery.

</details>


### [236] [LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems](https://arxiv.org/abs/2511.19368)
*Tianyang Duan,Zongyuan Zhang,Zheng Lin,Songxiao Guo,Xiuxian Guan,Guangyu Wu,Zihan Fang,Haotian Meng,Xia Du,Ji-Zhe Zhou,Heming Cui,Jun Luo,Yue Gao*

Main category: cs.LG

TL;DR: RELED是一个可扩展的多智能体强化学习框架，结合了LLM驱动的专家演示和自主智能体探索，通过理论非平稳性边界提升专家轨迹质量，并自适应平衡专家生成和智能体生成轨迹的学习，在城市网络实验中优于现有MARL方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在资源受限的边缘设备上部署时，由于智能体策略的同步更新导致严重的非平稳性问题，造成训练不稳定和策略收敛困难，特别是在智能体数量增加时。

Method: 提出RELED框架：1) 平稳性感知专家演示模块，利用理论非平稳性边界提升LLM生成的专家轨迹质量；2) 混合专家-智能体策略优化模块，自适应平衡从专家生成和智能体生成轨迹的学习。

Result: 基于OpenStreetMap的真实城市网络实验表明，RELED相比最先进的多智能体强化学习方法取得了更优越的性能。

Conclusion: RELED通过整合LLM驱动的专家演示和自主探索，有效解决了多智能体强化学习中的非平稳性问题，提高了训练稳定性和策略收敛性能。

Abstract: Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.

</details>


### [237] [Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware](https://arxiv.org/abs/2511.19379)
*Srishti Gupta,Yashasvee Taiwade*

Main category: cs.LG

TL;DR: 本文比较了DDPM和Flow Matching在低资源硬件上的几何特性和效率，发现Flow Matching在效率和路径优化方面显著优于DDPM，适合实时资源受限的生成任务。


<details>
  <summary>Details</summary>
Motivation: DDPM在生成图像合成中达到新水平，但推理时的计算开销大（需要多达1000次迭代步骤），阻碍了其部署。本研究旨在比较DDPM与新兴的Flow Matching范式在低资源硬件上的几何和效率特性。

Method: 在MNIST数据集上使用共享的时间条件U-Net骨干网络实现两种框架，进行几何分析和效率比较，包括曲率测量和数值敏感性分析。

Result: Flow Matching在效率上显著优于Diffusion，学习到高度校正的传输路径（曲率≈1.02），而Diffusion轨迹保持随机和曲折（曲率≈3.45）。在N=10次函数评估时，Flow Matching保持高保真度而Diffusion崩溃。数值敏感性分析显示学习到的向量场足够线性，无需高阶ODE求解器。

Conclusion: Flow Matching是实时资源受限生成任务的优越算法选择。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps. This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware. By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency. Our geometric analysis reveals that Flow Matching learns a highly rectified transport path (Curvature $\mathcal{C} \approx 1.02$), which is near-optimal, whereas Diffusion trajectories remain stochastic and tortuous ($\mathcal{C} \approx 3.45$). Furthermore, we establish an ``efficiency frontier'' at $N=10$ function evaluations, where Flow Matching retains high fidelity while Diffusion collapses. Finally, we show via numerical sensitivity analysis that the learned vector field is sufficiently linear to render high-order ODE solvers (Runge-Kutta 4) unnecessary, validating the use of lightweight Euler solvers for edge deployment. \textbf{This work concludes that Flow Matching is the superior algorithmic choice for real-time, resource-constrained generative tasks.}

</details>


### [238] [Learning Robust Social Strategies with Large Language Models](https://arxiv.org/abs/2511.19405)
*Dereck Piche,Mohammed Muqeeth,Milad Aghajohari,Juan Duque,Michael Noukhovitch,Aaron Courville*

Main category: cs.LG

TL;DR: 本文研究了在多智能体环境中，强化学习训练的LLM智能体倾向于发展自私行为的问题，并提出了一种基于对手学习意识的Advantage Alignment算法来促进多智能体合作。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI的普及，具有不同甚至冲突目标的智能体将在复杂环境中交互。在多智能体社会困境中，智能体的个体激励可能损害集体福利，而标准的强化学习在这种设置下往往收敛到自私的策略。

Method: 1) 将对手学习意识算法Advantage Alignment应用于LLM微调，促进多智能体合作和抗利用性；2) 引入组相对基线简化迭代游戏中的优势计算；3) 创建新的社会困境环境Trust and Split，需要自然语言沟通来实现高集体福利。

Result: 在各种社会困境中，使用Advantage Alignment学习的策略实现了更高的集体收益，同时保持对贪婪智能体利用的鲁棒性。

Conclusion: Advantage Alignment算法能够有效解决多智能体环境中RL训练导致的自私行为问题，促进LLM智能体之间的合作，提高集体福利。

Abstract: As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converges to defecting, self-interested policies. We show the same effect in LLMs: despite cooperative priors, RL-trained LLM agents develop opportunistic behavior that can exploit even advanced closed-source models. To address this tendency of RL to converge to poor equilibria, we adapt a recent opponent-learning awareness algorithm, Advantage Alignment, to fine-tune LLMs toward multi-agent cooperation and non-exploitability. We then introduce a group-relative baseline that simplifies advantage computation in iterated games, enabling multi-agent training at LLM scale. We also contribute a novel social dilemma environment, Trust and Split, which requires natural language communication to achieve high collective welfare. Across a wide range of social dilemmas, policies learned with Advantage Alignment achieve higher collective payoffs while remaining robust against exploitation by greedy agents.

</details>


### [239] [UniGame: Turning a Unified Multimodal Model Into Its Own Adversary](https://arxiv.org/abs/2511.19413)
*Zhaolong Su,Wang Lu,Hao Chen,Sharon Li,Jindong Wang*

Main category: cs.LG

TL;DR: UniGame是一个自对抗后训练框架，通过轻量级扰动器解决统一多模态模型中理解与生成之间的不一致性问题，显著提升模型的一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型存在根本性不一致：理解偏好紧凑嵌入，而生成偏好重建丰富的表示，这种结构权衡导致决策边界错位、跨模态连贯性下降以及对分布和对抗性变化的脆弱性。

Method: 在共享token接口应用轻量级扰动器，使生成分支能够主动寻找和挑战脆弱理解，将模型自身转化为对抗者。

Result: UniGame显著提升一致性(+4.6%)、理解能力(+3.6%)、生成质量(+0.02)，并在NaturalBench和AdVQA上分别提升分布外和对抗鲁棒性(+4.8%和+6.2%)。

Conclusion: 对抗性自博弈是增强未来多模态基础模型连贯性、稳定性和统一能力的通用有效原则，该框架架构无关，仅增加不到1%参数，且与现有后训练方法互补。

Abstract: Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame

</details>


### [240] [Flow Map Distillation Without Data](https://arxiv.org/abs/2511.19428)
*Shangyuan Tong,Nanye Ma,Saining Xie,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 提出了一种无需外部数据集的流映射蒸馏方法，仅从先验分布采样，避免了教师-数据不匹配问题，在ImageNet上仅需1步采样就达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统流映射蒸馏依赖外部数据集，存在教师-数据不匹配风险，因为静态数据集可能无法完整代表教师的生成能力。

Method: 开发了基于先验分布采样的数据无关框架，通过学习教师的采样路径并主动纠正累积误差来确保高保真度。

Result: 从SiT-XL/2+REPA蒸馏，在ImageNet 256x256上FID达到1.45，在512x512上达到1.49，仅需1步采样。

Conclusion: 建立了一个更稳健的生成模型加速范式，推动了无需数据的流映射蒸馏的广泛应用。

Abstract: State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [241] [Quantum Fourier Transform Based Kernel for Solar Irrandiance Forecasting](https://arxiv.org/abs/2511.17698)
*Nawfel Mechiche-Alami,Eduardo Rodriguez,Jose M. Cardemil,Enrique Lopez Droguett*

Main category: stat.ML

TL;DR: 提出了一种基于量子傅里叶变换的量子核方法，用于短期时间序列预测，在太阳能辐照度数据上相比经典核方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在时间序列预测中的应用，特别是利用量子傅里叶变换的特性来提升预测性能。

Method: 将信号分窗、幅度编码，通过量子傅里叶变换和防护旋转层构建量子核，结合核岭回归进行预测，并融合外生预测因子的特征核。

Result: 在多站点太阳能辐照度数据上，该方法在R2、nRMSE和nMBE指标上均优于经典RBF和多项式核方法，平均误差更小。

Conclusion: 量子傅里叶变换增强的量子核在时间序列预测中具有潜力，为NISQ设备上的实现提供了路径。

Abstract: This study proposes a Quantum Fourier Transform (QFT)-enhanced quantum kernel for short-term time-series forecasting. Each signal is windowed, amplitude-encoded, transformed by a QFT, then passed through a protective rotation layer to avoid the QFT/QFT adjoint cancellation; the resulting kernel is used in kernel ridge regression (KRR). Exogenous predictors are incorporated by convexly fusing feature-specific kernels. On multi-station solar irradiance data across Koppen climate classes, the proposed kernel consistently improves median R2 and nRMSE over reference classical RBF and polynomials kernels, while also reducing bias (nMBE); complementary MAE/ERMAX analyses indicate tighter average errors with remaining headroom under sharp transients. For both quantum and classical models, the only tuned quantities are the feature-mixing weights and the KRR ridge alpha; classical hyperparameters (gamma, r, d) are fixed, with the same validation set size for all models. Experiments are conducted on a noiseless simulator (5 qubits; window length L=32). Limitations and ablations are discussed, and paths toward NISQ execution are outlined.

</details>


### [242] [Prequential posteriors](https://arxiv.org/abs/2511.17721)
*Shreya Sinha-Roy,Richard G. Everitt,Christian P. Robert,Ritabrata Dutta*

Main category: stat.ML

TL;DR: 提出了一种基于预测序列损失函数的预序后验方法，用于深度生成预测模型的数据同化，解决了传统贝叶斯方法因似然函数难以处理而无法应用的问题。


<details>
  <summary>Details</summary>
Motivation: 深度生成预测模型在预测任务中表现出色，但由于其似然函数难以处理，无法使用标准的贝叶斯数据同化方法。需要开发新的方法来更新这类模型以适应新观测数据。

Method: 使用预测序列损失函数构建预序后验，采用可并行化的无浪费序列蒙特卡洛采样器，结合预条件梯度核来高效探索高维参数空间。

Result: 在合成多维时间序列和真实气象数据集上的验证表明，该方法能够有效地将数据同化到复杂动力系统的深度生成预测模型中。

Conclusion: 预序后验方法为深度生成预测模型提供了一种可扩展的数据同化解决方案，特别适用于时间依赖数据的预测任务，具有实际应用价值。

Abstract: Data assimilation is a fundamental task in updating forecasting models upon observing new data, with applications ranging from weather prediction to online reinforcement learning. Deep generative forecasting models (DGFMs) have shown excellent performance in these areas, but assimilating data into such models is challenging due to their intractable likelihood functions. This limitation restricts the use of standard Bayesian data assimilation methodologies for DGFMs. To overcome this, we introduce prequential posteriors, based upon a predictive-sequential (prequential) loss function; an approach naturally suited for temporally dependent data which is the focus of forecasting tasks. Since the true data-generating process often lies outside the assumed model class, we adopt an alternative notion of consistency and prove that, under mild conditions, both the prequential loss minimizer and the prequential posterior concentrate around parameters with optimal predictive performance. For scalable inference, we employ easily parallelizable wastefree sequential Monte Carlo (SMC) samplers with preconditioned gradient-based kernels, enabling efficient exploration of high-dimensional parameter spaces such as those in DGFMs. We validate our method on both a synthetic multi-dimensional time series and a real-world meteorological dataset; highlighting its practical utility for data assimilation for complex dynamical systems.

</details>


### [243] [Variational Estimators for Node Popularity Models](https://arxiv.org/abs/2511.17783)
*Jony Karki,Dongzhou Huang,Yunpeng Zhao*

Main category: stat.ML

TL;DR: 提出了一种基于变分期望最大化(VEM)框架的高效计算方法，用于估计双向节点流行度模型(TNPM)，在二分网络和无向网络中均表现出优于现有算法的估计精度。


<details>
  <summary>Details</summary>
Motivation: 节点流行度是建模真实网络的关键因素，现有方法如TSDC算法在准确性和适用性方面存在局限，需要开发更高效且理论可靠的方法。

Method: 开发了变分期望最大化(VEM)框架，建立了二分网络中社区分配估计的标签一致性理论保证。

Result: 通过大量模拟研究，该方法在二分网络和无向网络中均实现了优于现有算法的估计精度；在真实网络上的评估进一步证明了其实际有效性和鲁棒性。

Conclusion: 提出的VEM框架为TNPM提供了计算高效且理论可靠的估计方法，在多种网络类型中均表现出优越性能。

Abstract: Node popularity is recognized as a key factor in modeling real-world networks, capturing heterogeneity in connectivity across communities. This concept is equally important in bipartite networks, where nodes in different partitions may exhibit varying popularity patterns, motivating models such as the Two-Way Node Popularity Model (TNPM). Existing methods, such as the Two-Stage Divided Cosine (TSDC) algorithm, provide a scalable estimation approach but may have limitations in terms of accuracy or applicability across different types of networks. In this paper, we develop a computationally efficient and theoretically justified variational expectation-maximization (VEM) framework for the TNPM. We establish label consistency for the estimated community assignments produced by the proposed variational estimator in bipartite networks. Through extensive simulation studies, we show that our method achieves superior estimation accuracy across a range of bipartite as well as undirected networks compared to existing algorithms. Finally, we evaluate our method on real-world bipartite and undirected networks, further demonstrating its practical effectiveness and robustness.

</details>


### [244] [An operator splitting analysis of Wasserstein--Fisher--Rao gradient flows](https://arxiv.org/abs/2511.18060)
*Francesca Romana Crucinio,Sahani Pathiraja*

Main category: stat.ML

TL;DR: 本文研究了Wasserstein-Fisher-Rao梯度流中算子分裂顺序对算法收敛速度的影响，发现通过精心选择步长和算子顺序，分裂方案可以比精确WFR流更快收敛到目标分布。


<details>
  <summary>Details</summary>
Motivation: 现有WFR梯度流算法隐式使用算子分裂技术来数值逼近WFR偏微分方程，但分裂顺序（W-FR或FR-W）的影响尚未得到定量分析。本文旨在研究不同算子顺序对收敛速度的影响。

Method: 使用算子分裂技术，分别研究W-FR和FR-W两种顺序的分裂方案，通过变分公式描述单时间步的演化过程，并分析在何种情况下应优先选择哪种分裂顺序。

Result: 研究发现，通过精心选择步长和算子顺序，分裂方案可以比精确WFR流更快收敛到目标分布。同时证明了WFR梯度流保持对数凹性，并获得了WFR的第一个锐衰减界。

Conclusion: 算子分裂顺序对WFR梯度流的收敛速度有显著影响，W-FR和FR-W分裂在不同设置下各有优势，需要根据具体场景选择合适的分裂顺序。

Abstract: Wasserstein-Fisher-Rao (WFR) gradient flows have been recently proposed as a powerful sampling tool that combines the advantages of pure Wasserstein (W) and pure Fisher-Rao (FR) gradient flows. Existing algorithmic developments implicitly make use of operator splitting techniques to numerically approximate the WFR partial differential equation, whereby the W flow is evaluated over a given step size and then the FR flow (or vice versa). This works investigates the impact of the order in which the W and FR operator are evaluated and aims to provide a quantitative analysis. Somewhat surprisingly, we show that with a judicious choice of step size and operator ordering, the split scheme can converge to the target distribution faster than the exact WFR flow (in terms of model time). We obtain variational formulae describing the evolution over one time step of both sequential splitting schemes and investigate in which settings the W-FR split should be preferred to the FR-W split. As a step towards this goal we show that the WFR gradient flow preserves log-concavity and obtain the first sharp decay bound for WFR.

</details>


### [245] [Conformal Prediction for Compositional Data](https://arxiv.org/abs/2511.18141)
*Lucas P. Amaral,Luben M. C. Cabezas,Thiago R. Ramos,Gustavo H. G. A. Pereira*

Main category: stat.ML

TL;DR: 本文提出了针对成分响应数据的保形预测方法，使用狄利克雷回归构建基于分位数残差的分割保形方法和最高密度区域策略，确保有限样本边际覆盖同时保持单纯形几何特性。


<details>
  <summary>Details</summary>
Motivation: 成分响应数据（比例数据必须为正且和为1）在预测任务中需要专门的保形预测方法，现有方法难以同时保证覆盖率和效率。

Method: 基于狄利克雷回归，提出分位数残差分割保形方法和最高密度区域策略，结合快速坐标下限近似和内部网格细化来恢复锐度。

Result: 蒙特卡洛模拟显示分位数残差和网格细化HDR方法在异方差设计中达到接近90%的经验覆盖，产生比保守的坐标下限近似更窄的区域。在家庭预算份额应用中，网格细化HDR获得最接近目标覆盖率和最小平均宽度。

Conclusion: 单纯形上的保形预测可以同时实现校准和效率，为成分预测任务提供实用的不确定性量化。

Abstract: In this work, we propose a set of conformal prediction procedures tailored to compositional responses, where outcomes are proportions that must be positive and sum to one. Building on Dirichlet regression, we introduce a split conformal approach based on quantile residuals and a highest-density region strategy that combines a fast coordinate-floor approximation with an internal grid refinement to restore sharpness. Both constructions are model-agnostic at the conformal layer and guarantee finite-sample marginal coverage under exchangeability, while respecting the geometry of the simplex. A comprehensive Monte Carlo study spanning homoscedastic and heteroscedastic designs shows that the quantile residual and grid-refined HDR methods achieve empirical coverage close to the nominal 90\% level and produce substantially narrower regions than the coordinate-floor approximation, which tends to be conservative. We further demonstrate the methods on household budget shares from the BudgetItaly dataset, using standardized socioeconomic and price covariates with a train, calibration, and test split. In this application, the grid-refined HDR attains coverage closest to the target with the smallest average widths, closely followed by the quantile residual approach, while the simple triangular HDR yields wider, less informative sets. Overall, the results indicate that conformal prediction on the simplex can be both calibrated and efficient, providing practical uncertainty quantification for compositional prediction tasks.

</details>


### [246] [Sparse Polyak with optimal thresholding operators for high-dimensional M-estimation](https://arxiv.org/abs/2511.18167)
*Tianqi Qiao,Marie Maros*

Main category: stat.ML

TL;DR: 提出并分析了一种改进的Sparse Polyak算法变体，用于高维M估计问题，在保持对维度良好缩放性的同时获得更稀疏和更准确的解。


<details>
  <summary>Details</summary>
Motivation: 原始Sparse Polyak算法在高维设置下虽然能适应问题曲率且性能不随维度增加而恶化，但需要牺牲解的稀疏性和统计精度来获得收敛保证。

Method: 开发了Sparse Polyak的变体，保留了其对环境维度的良好缩放特性，同时改进了稀疏性和准确性。

Result: 新变体算法在保持维度缩放优势的同时，能够获得比原始算法更稀疏和更准确的解决方案。

Conclusion: 该工作成功改进了Sparse Polyak算法，解决了原始版本在稀疏性和统计精度方面的局限性，为高维M估计问题提供了更好的解决方案。

Abstract: We propose and analyze a variant of Sparse Polyak for high dimensional M-estimation problems. Sparse Polyak proposes a novel adaptive step-size rule tailored to suitably estimate the problem's curvature in the high-dimensional setting, guaranteeing that the algorithm's performance does not deteriorate when the ambient dimension increases. However, convergence guarantees can only be obtained by sacrificing solution sparsity and statistical accuracy. In this work, we introduce a variant of Sparse Polyak that retains its desirable scaling properties with respect to the ambient dimension while obtaining sparser and more accurate solutions.

</details>


### [247] [Improving Forecasts of Suicide Attempts for Patients with Little Data](https://arxiv.org/abs/2511.18199)
*Genesis Hang,Annie Chen,Hope Neveux,Matthew K. Nock,Yaniv Yacoby*

Main category: stat.ML

TL;DR: 该论文提出了一种名为潜在相似性高斯过程（LSGPs）的新方法，用于解决自杀企图预测中因数据稀少和患者异质性导致的模型性能不佳问题。


<details>
  <summary>Details</summary>
Motivation: 生态瞬时评估提供了自杀想法和行为的实时数据，但由于自杀企图的罕见性和患者异质性，预测自杀企图仍然具有挑战性。现有模型要么在全体患者上表现不佳，要么在数据有限的患者上过拟合。

Method: 引入了潜在相似性高斯过程（LSGPs）来捕捉患者异质性，使数据较少的患者能够利用相似患者的趋势，而无需进行核设计。

Result: 初步结果显示，即使没有核设计，该方法也优于除一个基线外的所有基线，同时提供了对患者相似性的新理解。

Conclusion: LSGPs方法在自杀企图预测中显示出潜力，能够有效处理患者异质性和数据稀少问题，为临床实践提供了新的工具。

Abstract: Ecological Momentary Assessment provides real-time data on suicidal thoughts and behaviors, but predicting suicide attempts remains challenging due to their rarity and patient heterogeneity. We show that single models fit to all patients perform poorly, while individualized models improve performance but still overfit to patients with limited data. To address this, we introduce Latent Similarity Gaussian Processes (LSGPs) to capture patient heterogeneity, enabling those with little data to leverage similar patients' trends. Preliminary results show promise: even without kernel-design, we outperform all but one baseline while offering a new understanding of patient similarity.

</details>


### [248] [Reliable Selection of Heterogeneous Treatment Effect Estimators](https://arxiv.org/abs/2511.18464)
*Jiayi Guo,Zijun Gao*

Main category: stat.ML

TL;DR: 提出了一种无需真实治疗效果即可选择最优异质处理效应估计器的方法，通过多重检验框架和交叉拟合的指数加权统计量实现可靠的错误率控制。


<details>
  <summary>Details</summary>
Motivation: 在异质处理效应估计中，由于处理效应本身无法直接观测，如何从多个候选估计器中选择最优的估计器是一个具有挑战性的问题。现有方法往往依赖真实治疗效果，但在实际应用中这通常是不可得的。

Method: 将估计器选择问题转化为多重检验问题，采用交叉拟合的指数加权检验统计量。关键创新是双向样本分割方案，将干扰项估计与权重学习解耦，确保推断所需的稳定性。

Result: 在ACIC 2016、IHDP和Twins基准测试中，该方法提供了可靠的错误控制，相比常用方法显著减少了错误选择，证明了即使没有真实治疗效果，该方法仍然可行且有效。

Conclusion: 该方法为异质处理效应估计器的选择提供了一种无需真实治疗效果的有效解决方案，通过稳定性理论保证了渐近族错误率控制，在实际应用中表现出优越性能。

Abstract: We study the problem of selecting the best heterogeneous treatment effect (HTE) estimator from a collection of candidates in settings where the treatment effect is fundamentally unobserved. We cast estimator selection as a multiple testing problem and introduce a ground-truth-free procedure based on a cross-fitted, exponentially weighted test statistic. A key component of our method is a two-way sample splitting scheme that decouples nuisance estimation from weight learning and ensures the stability required for valid inference. Leveraging a stability-based central limit theorem, we establish asymptotic familywise error rate control under mild regularity conditions. Empirically, our procedure provides reliable error control while substantially reducing false selections compared with commonly used methods across ACIC 2016, IHDP, and Twins benchmarks, demonstrating that our method is feasible and powerful even without ground-truth treatment effects.

</details>


### [249] [Transforming Conditional Density Estimation Into a Single Nonparametric Regression Task](https://arxiv.org/abs/2511.18530)
*Alexander G. Reisach,Olivier Collier,Alex Luedtke,Antoine Chambaz*

Main category: stat.ML

TL;DR: 提出了一种将条件密度估计问题转化为单一非参数回归任务的方法，通过引入辅助样本，可以利用神经网络和决策树等在高维数据中表现良好的回归方法。


<details>
  <summary>Details</summary>
Motivation: 传统条件密度估计方法在高维数据中面临挑战，需要开发能够利用现代回归技术的新方法。

Method: 通过引入辅助样本将条件密度估计转化为回归任务，开发了condensité方法实现这一思路。

Result: 在合成数据、大规模人口调查数据集和卫星图像数据集上的实验表明，condensité方法达到或超越了现有技术水平，产生的条件密度与文献中已有发现一致。

Conclusion: 该方法为基于回归的条件密度估计开辟了新可能性，实证结果表明在应用研究中具有很大潜力。

Abstract: We propose a way of transforming the problem of conditional density estimation into a single nonparametric regression task via the introduction of auxiliary samples. This allows leveraging regression methods that work well in high dimensions, such as neural networks and decision trees. Our main theoretical result characterizes and establishes the convergence of our estimator to the true conditional density in the data limit. We develop condensité, a method that implements this approach. We demonstrate the benefit of the auxiliary samples on synthetic data and showcase that condensité can achieve good out-of-the-box results. We evaluate our method on a large population survey dataset and on a satellite imaging dataset. In both cases, we find that condensité matches or outperforms the state of the art and yields conditional densities in line with established findings in the literature on each dataset. Our contribution opens up new possibilities for regression-based conditional density estimation and the empirical results indicate strong promise for applied research.

</details>


### [250] [Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks](https://arxiv.org/abs/2511.18562)
*Xunlei Qian,Yue Xing*

Main category: stat.ML

TL;DR: 本文研究了在对抗性扰动下分离式保形预测的鲁棒性，分析了校准阶段对抗攻击强度对测试阶段覆盖保证的影响，并探讨了对抗训练对预测集大小的影响。


<details>
  <summary>Details</summary>
Motivation: 保形预测(CP)虽然提供分布无关的有限样本覆盖保证，但严重依赖可交换性条件，这在分布偏移下经常被违反。研究在测试时对抗性扰动下CP的鲁棒性具有重要意义。

Method: 通过理论分析校准阶段对抗扰动强度如何影响对抗测试条件下的覆盖保证，并研究模型训练阶段对抗训练的影响。进行大量实验验证理论。

Result: (1)预测覆盖率随校准阶段攻击强度单调变化；(2)通过合适的校准攻击，可在连续扰动水平范围内保持目标覆盖率；(3)训练阶段对抗训练产生更紧凑且信息量高的预测集。

Conclusion: 在对抗性环境下，通过控制校准阶段的对抗攻击强度可以预测性地控制测试阶段的覆盖保证，且对抗训练有助于生成更紧凑的预测集。

Abstract: Conformal prediction (CP) provides distribution-free, finite-sample coverage guarantees but critically relies on exchangeability, a condition often violated under distribution shift. We study the robustness of split conformal prediction under adversarial perturbations at test time, focusing on both coverage validity and the resulting prediction set size. Our theoretical analysis characterizes how the strength of adversarial perturbations during calibration affects coverage guarantees under adversarial test conditions. We further examine the impact of adversarial training at the model-training stage. Extensive experiments support our theory: (i) Prediction coverage varies monotonically with the calibration-time attack strength, enabling the use of nonzero calibration-time attack to predictably control coverage under adversarial tests; (ii) target coverage can hold over a range of test-time attacks: with a suitable calibration attack, coverage stays within any chosen tolerance band across a contiguous set of perturbation levels; and (iii) adversarial training at the training stage produces tighter prediction sets that retain high informativeness.

</details>


### [251] [Differential privacy with dependent data](https://arxiv.org/abs/2511.18583)
*Valentin Roth,Marco Avella-Medina*

Main category: stat.ML

TL;DR: 本文研究了在依赖数据场景下的差分隐私均值估计问题，提出了基于Winsorized均值估计器的方法，能够处理有界和无界数据，并在弱依赖条件下获得与独立同分布情况相似的渐近和有限样本保证。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的社会科学和健康科学研究经常涉及敏感的个人数据，这些数据通常是依赖的（如重复测量）。现有的差分隐私统计理论主要针对独立同分布数据，对依赖数据的处理缺乏系统研究。

Method: 使用Winsorized均值估计器，通过log-Sobolev不等式形式化依赖关系，将Karwa和Vadhan(2018)的稳定直方图方法扩展到非独立同分布设置，用于估计Winsorized估计器的私有投影区间。

Result: 提出的方法在弱依赖条件下能够获得与独立同分布情况相似的统计保证，并且可以扩展到用户级差分隐私均值估计、局部模型、随机效应模型、纵向线性回归和非参数回归等场景。

Conclusion: 这项工作为依赖数据下的差分隐私统计研究迈出了第一步，为处理现实世界中常见的依赖数据提供了理论基础和方法框架。

Abstract: Dependent data underlies many statistical studies in the social and health sciences, which often involve sensitive or private information. Differential privacy (DP) and in particular \textit{user-level} DP provide a natural formalization of privacy requirements for processing dependent data where each individual provides multiple observations to the dataset. However, dependence introduced, e.g., through repeated measurements challenges the existing statistical theory under DP-constraints. In \iid{} settings, noisy Winsorized mean estimators have been shown to be minimax optimal for standard (\textit{item-level}) and \textit{user-level} DP estimation of a mean $μ\in \R^d$. Yet, their behavior on potentially dependent observations has not previously been studied. We fill this gap and show that Winsorized mean estimators can also be used under dependence for bounded and unbounded data, and can lead to asymptotic and finite sample guarantees that resemble their \iid{} counterparts under a weak notion of dependence. For this, we formalize dependence via log-Sobolev inequalities on the joint distribution of observations. This enables us to adapt the stable histogram by Karwa and Vadhan (2018) to a non-\iid{} setting, which we then use to estimate the private projection intervals of the Winsorized estimator. The resulting guarantees for our item-level mean estimator extend to \textit{user-level} mean estimation and transfer to the local model via a randomized response histogram. Using the mean estimators as building blocks, we provide extensions to random effects models, longitudinal linear regression and nonparametric regression. Therefore, our work constitutes a first step towards a systematic study of DP for dependent data.

</details>


### [252] [Fast Escape, Slow Convergence: Learning Dynamics of Phase Retrieval under Power-Law Data](https://arxiv.org/abs/2511.18661)
*Guillaume Braun,Bruno Loureiro,Ha Quang Minh,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 该论文研究了各向异性高斯输入下相位检索问题的缩放规律，揭示了与各向同性情况不同的三阶段学习轨迹，并推导了均方误差的显式缩放规律。


<details>
  <summary>Details</summary>
Motivation: 研究各向异性数据如何重塑非线性回归中的学习动态和缩放规律，填补各向异性数据下缩放规律严格理论分析的空白。

Method: 使用各向异性高斯输入的相位检索模型，开发可处理的约简方法分析总结统计量的演化，通过理论推导和实验验证。

Result: 发现三阶段学习轨迹：快速逃离低对齐、总结统计量缓慢收敛、低方差方向的光谱尾部学习，实验证实了预测的阶段和指数。

Conclusion: 首次严格刻画了各向异性数据下非线性回归的缩放规律，揭示了各向异性如何重塑学习动态，光谱衰减决定了收敛时间和误差曲线。

Abstract: Scaling laws describe how learning performance improves with data, compute, or training time, and have become a central theme in modern deep learning. We study this phenomenon in a canonical nonlinear model: phase retrieval with anisotropic Gaussian inputs whose covariance spectrum follows a power law. Unlike the isotropic case, where dynamics collapse to a two-dimensional system, anisotropy yields a qualitatively new regime in which an infinite hierarchy of coupled equations governs the evolution of the summary statistics. We develop a tractable reduction that reveals a three-phase trajectory: (i) fast escape from low alignment, (ii) slow convergence of the summary statistics, and (iii) spectral-tail learning in low-variance directions. From this decomposition, we derive explicit scaling laws for the mean-squared error, showing how spectral decay dictates convergence times and error curves. Experiments confirm the predicted phases and exponents. These results provide the first rigorous characterization of scaling laws in nonlinear regression with anisotropic data, highlighting how anisotropy reshapes learning dynamics.

</details>


### [253] [On Instability of Minimax Optimal Optimism-Based Bandit Algorithms](https://arxiv.org/abs/2511.18750)
*Samya Praharaj,Koulik Khamaru*

Main category: stat.ML

TL;DR: 该论文分析了基于乐观原则的多臂老虎机算法的稳定性，发现广泛使用的最小化最大最优UCB风格算法都违反Lai-Wei稳定性准则，导致样本均值不满足渐近正态性，揭示了稳定性与最小化最大最优遗憾之间的基本矛盾。


<details>
  <summary>Details</summary>
Motivation: 多臂老虎机算法生成的数据具有自适应、非独立同分布特性，使得统计推断变得困难。经典表现是样本均值可能不满足中心极限定理。虽然UCB算法满足稳定性条件，但它不是最小化最大最优的，这引发了是否能够同时实现最小化最大最优和统计稳定性的问题。

Method: 分析基于乐观原则的广泛类别老虎机算法的稳定性特性，建立了一般结构条件来判定这些算法是否违反Lai-Wei稳定性准则。

Result: 证明了广泛使用的最小化最大最优UCB风格算法（包括MOSS、Anytime-MOSS、Vanilla-MOSS、ADA-UCB、OC-UCB、KL-MOSS、KL-UCB++、KL-UCB-SWITCH和Anytime KL-UCB-SWITCH）都是不稳定的。数值模拟进一步证实了这些情况下样本均值不表现出渐近正态性。

Conclusion: 研究结果表明稳定性与最小化最大最优遗憾之间存在基本矛盾，提出了是否可能设计同时实现稳定性和最小化最大最优的老虎机算法的重要开放问题。

Abstract: Statistical inference from data generated by multi-armed bandit (MAB) algorithms is challenging due to their adaptive, non-i.i.d. nature. A classical manifestation is that sample averages of arm rewards under bandit sampling may fail to satisfy a central limit theorem. Lai and Wei's stability condition provides a sufficient, and essentially necessary criterion, for asymptotic normality in bandit problems. While the celebrated Upper Confidence Bound (UCB) algorithm satisfies this stability condition, it is not minimax optimal, raising the question of whether minimax optimality and statistical stability can be achieved simultaneously. In this paper, we analyze the stability properties of a broad class of bandit algorithms that are based on the optimism principle. We establish general structural conditions under which such algorithms violate the Lai-Wei stability criterion. As a consequence, we show that widely used minimax-optimal UCB-style algorithms, including MOSS, Anytime-MOSS, Vanilla-MOSS, ADA-UCB, OC-UCB, KL-MOSS, KL-UCB++, KL-UCB-SWITCH, and Anytime KL-UCB-SWITCH, are unstable. We further complement our theoretical results with numerical simulations demonstrating that, in all these cases, the sample means fail to exhibit asymptotic normality.
  Overall, our findings suggest a fundamental tension between stability and minimax optimal regret, raising the question of whether it is possible to design bandit algorithms that achieve both. Understanding whether such simultaneously stable and minimax optimal strategies exist remains an important open direction.

</details>


### [254] [Uncertainty of Network Topology with Applications to Out-of-Distribution Detection](https://arxiv.org/abs/2511.18813)
*Sing-Yuan Yeh,Chun-Hao Yang*

Main category: stat.ML

TL;DR: 提出了一种基于持续同调的贝叶斯神经网络拓扑不确定性度量(pTU)，用于解决分布外检测问题，该方法从模型角度衡量输入与模型交互的不确定性。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络需要可靠的不确定性量化，特别是在处理分布外输入时。现有方法在检测OOD样本方面存在局限性，需要从拓扑角度提供新的见解。

Method: 引入预测拓扑不确定性(pTU)作为新的拓扑摘要，通过持续同调分析模型与输入的交互方式，并基于pTU构建OOD检测的显著性检验框架。

Result: 实验验证了该框架在统计功效、敏感性和鲁棒性方面的有效性，且pTU对模型架构不敏感。

Conclusion: pTU为贝叶斯神经网络提供了一种新颖的拓扑不确定性度量，能够有效解决OOD检测问题，提高模型可靠性。

Abstract: Persistent homology (PH) is a crucial concept in computational topology, providing a multiscale topological description of a space. It is particularly significant in topological data analysis, which aims to make statistical inference from a topological perspective. In this work, we introduce a new topological summary for Bayesian neural networks, termed the predictive topological uncertainty (pTU). The proposed pTU measures the uncertainty in the interaction between the model and the inputs. It provides insights from the model perspective: if two samples interact with a model in a similar way, then they are considered identically distributed. We also show that the pTU is insensitive to the model architecture. As an application, pTU is used to solve the out-of-distribution (OOD) detection problem, which is critical to ensure model reliability. Failure to detect OOD input can lead to incorrect and unreliable predictions. To address this issue, we propose a significance test for OOD based on the pTU, providing a statistical framework for this issue. The effectiveness of the framework is validated through various experiments, in terms of its statistical power, sensitivity, and robustness.

</details>


### [255] [Fairness Meets Privacy: Integrating Differential Privacy and Demographic Parity in Multi-class Classification](https://arxiv.org/abs/2511.18876)
*Lilian Say,Christophe Denis,Rafael Pinot*

Main category: stat.ML

TL;DR: 本文挑战了隐私与公平必然冲突的观点，提出DP2DP算法能同时实现差分隐私和人口统计均等，且对公平性影响极小。


<details>
  <summary>Details</summary>
Motivation: 机器学习在敏感应用中需要同时保护数据隐私和确保公平性，但现有研究常将隐私与公平视为冲突目标，认为强隐私保护必然损害公平性。

Method: 设计了DP2DP后处理算法，该算法同时强制执行人口统计均等和差分隐私。

Result: 理论分析显示算法以与最佳非私有方法几乎相同的速率收敛到公平目标，实验在合成和真实数据集上验证了该算法在准确性/公平性/隐私权衡方面达到最先进水平。

Conclusion: 差分隐私可以与公平性增强流程集成，且对公平性保证的影响极小，挑战了隐私与公平必然冲突的传统观点。

Abstract: The increasing use of machine learning in sensitive applications demands algorithms that simultaneously preserve data privacy and ensure fairness across potentially sensitive sub-populations. While privacy and fairness have each been extensively studied, their joint treatment remains poorly understood. Existing research often frames them as conflicting objectives, with multiple studies suggesting that strong privacy notions such as differential privacy inevitably compromise fairness. In this work, we challenge that perspective by showing that differential privacy can be integrated into a fairness-enhancing pipeline with minimal impact on fairness guarantees. We design a postprocessing algorithm, called DP2DP, that enforces both demographic parity and differential privacy. Our analysis reveals that our algorithm converges towards its demographic parity objective at essentially the same rate (up logarithmic factor) as the best non-private methods from the literature. Experiments on both synthetic and real datasets confirm our theoretical results, showing that the proposed algorithm achieves state-of-the-art accuracy/fairness/privacy trade-offs.

</details>


### [256] [Classification EM-PCA for clustering and embedding](https://arxiv.org/abs/2511.18992)
*Zineddine Tighidet,Lazhar Labiod,Mohamed Nadif*

Main category: stat.ML

TL;DR: 提出一种结合PCA和CEM算法的同步非顺序方法，同时进行数据嵌入和聚类，解决高维数据和EM算法收敛慢的问题。


<details>
  <summary>Details</summary>
Motivation: 高斯混合模型在聚类中应用广泛，但面临高维数据和EM算法收敛慢的挑战。CEM算法提供快速收敛方案，但维度缩减仍是问题。

Method: 结合主成分分析(PCA)和分类EM(CEM)算法，同步而非顺序地执行数据嵌入和聚类两个任务。

Result: 证明了该方法在聚类和数据嵌入方面的优势，并建立了与其他聚类方法的联系。

Conclusion: 提出的同步方法在聚类性能和数据处理方面表现出色，为解决高维聚类问题提供了有效方案。

Abstract: The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.

</details>


### [257] [Structured Matching via Cost-Regularized Unbalanced Optimal Transport](https://arxiv.org/abs/2511.19075)
*Emanuele Pardini,Katerina Papagiannouli*

Main category: stat.ML

TL;DR: 提出了成本正则化不平衡最优传输(CR-UOT)框架，允许地面传输成本变化，同时支持质量创建和移除，用于跨异构空间的数据匹配。


<details>
  <summary>Details</summary>
Motivation: 传统不平衡最优传输(UOT)需要预定义地面传输成本，这在异构空间数据匹配中可能无法准确表示数据几何结构，而选择合适成本具有挑战性。

Method: 引入成本正则化不平衡最优传输(CR-UOT)，允许地面成本变化，通过线性变换参数化的内积成本族，结合熵正则化开发算法。

Result: CR-UOT能够通过不平衡Gromov-Wasserstein类型问题匹配跨欧几里得空间的度量或点云，在单细胞组学数据对齐中表现优异，特别是当许多细胞缺乏直接匹配时。

Conclusion: CR-UOT框架提供了一种灵活的方法来处理异构空间中的数据匹配问题，特别是在存在大量不匹配样本的情况下，优于传统方法。

Abstract: Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches.

</details>


### [258] [A Robust State Filter Against Unmodeled Process And Measurement Noise](https://arxiv.org/abs/2511.19157)
*Weitao Liu*

Main category: stat.ML

TL;DR: 提出了一种新颖的卡尔曼滤波框架，能够在过程噪声和测量噪声下实现鲁棒状态估计，基于加权观测似然滤波(WoLF)思想，采用广义贝叶斯方法处理两种噪声中的异常值。


<details>
  <summary>Details</summary>
Motivation: 现有WoLF方法主要针对测量异常值提供鲁棒性，但实际应用中过程噪声也可能存在异常值，需要同时处理两种噪声异常值的鲁棒状态估计框架。

Method: 基于广义贝叶斯方法，扩展WoLF框架，构建同时考虑过程噪声和测量噪声异常值的卡尔曼滤波框架。

Result: 开发了一个统一的鲁棒状态估计框架，能够有效处理过程噪声和测量噪声中的异常值。

Conclusion: 该框架扩展了WoLF的能力，为存在多种噪声异常值的状态估计问题提供了更全面的解决方案。

Abstract: This paper introduces a novel Kalman filter framework designed to achieve robust state estimation under both process and measurement noise. Inspired by the Weighted Observation Likelihood Filter (WoLF), which provides robustness against measurement outliers, we applied generalized Bayesian approach to build a framework considering both process and measurement noise outliers.

</details>


### [259] [The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility](https://arxiv.org/abs/2511.19284)
*Eichi Uehara*

Main category: stat.ML

TL;DR: 提出了一个统一鲁棒框架，重新设计了对重叠区域平均处理效应(ATO)的估计方法


<details>
  <summary>Details</summary>
Motivation: 需要解决高斯机制中高阶正交性不可能的问题，并提供对异常值具有鲁棒性的ATE估计方法

Method: 结合gamma-散度用于异常值鲁棒性、渐进非凸性(GNC)用于全局优化，以及"守门员"机制来处理高斯机制中的高阶正交性问题

Result: 开发了一个统一的鲁棒框架，改进了ATO估计

Conclusion: 该框架通过整合多种技术，为处理高斯机制中的挑战和异常值问题提供了有效的解决方案

Abstract: This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a "Gatekeeper" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes.

</details>


### [260] [Nonparametric Instrumental Variable Regression with Observed Covariates](https://arxiv.org/abs/2511.19404)
*Zikai Shen,Zonghao Chen,Dimitri Meunier,Ingo Steinwart,Arthur Gretton,Zhu Li*

Main category: stat.ML

TL;DR: 本文研究了带观测协变量的非参数工具变量回归(NPIV-O)问题，提出了KIV-O算法并建立了首个L^2-极小极大下界学习率。


<details>
  <summary>Details</summary>
Motivation: 相比标准NPIV，观测协变量有助于因果识别和异质性因果效应估计，但带来了部分恒等结构和各向异性平滑性的理论分析挑战。

Method: 引入傅里叶部分平滑度量，扩展核2SLS工具变量算法(KIV-O)以自适应各向异性平滑性的高斯核长度尺度。

Result: 证明了KIV-O的L^2上界学习率和NPIV-O的首个L^2极小极大下界学习率，两者在NPIV和NPR的最优速率之间插值。

Conclusion: 发现上界与下界之间存在间隙，源于为最小化投影风险而调整的核长度尺度选择。理论分析也适用于具有相同条件矩约束的邻近因果推断框架。

Abstract: We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O.

</details>
