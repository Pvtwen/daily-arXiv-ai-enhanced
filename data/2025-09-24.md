<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 17]
- [cs.LG](#cs.LG) [Total: 125]
- [stat.ML](#stat.ML) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [On Multi-entity, Multivariate Quickest Change Point Detection](https://arxiv.org/abs/2509.18310)
*Bahar Kor,Bipin Gaikwad,Abani Patra,Eric L. Miller*

Main category: eess.SP

TL;DR: 提出一种用于多实体多元时间序列数据的在线变点检测框架，通过个体偏离正常性概念和系统级异常评分来检测复杂动态环境中的系统级行为变化


<details>
  <summary>Details</summary>
Motivation: 解决在人群监控等应用中传统传感方法不可行时的挑战，检测复杂动态环境中系统级行为变化，其中个体实体的数量和行为可能不确定或演化

Method: 使用基于重构误差的自编码器计算个体偏离正常性，通过均值、方差和核密度估计聚合得到系统级异常评分，应用统计偏差度量和累积和技术检测持续性或突然变化

Result: 在合成数据集和人群模拟上的评估表明，该方法能准确检测显著的系统级变化，为监控复杂多智能体系统提供可扩展和隐私保护的解决方案

Conclusion: 该方法无需标记数据或特征提取，支持实时流式处理，同时填补了用于评估复杂集体交互系统中变点检测的公开数据集空白

Abstract: We propose a framework for online Change Point Detection (CPD) from
multi-entity, multivariate time series data, motivated by applications in crowd
monitoring where traditional sensing methods (e.g., video surveillance) may be
infeasible. Our approach addresses the challenge of detecting system-wide
behavioral shifts in complex, dynamic environments where the number and
behavior of individual entities may be uncertain or evolve. We introduce the
concept of Individual Deviation from Normality (IDfN), computed via a
reconstruction-error-based autoencoder trained on normal behavior. We aggregate
these individual deviations using mean, variance, and Kernel Density Estimates
(KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or
abrupt changes, we apply statistical deviation metrics and the Cumulative Sum
(CUSUM) technique to these scores. Our unsupervised approach eliminates the
need for labeled data or feature extraction, enabling real-time operation on
streaming input. Evaluations on both synthetic datasets and crowd simulations,
explicitly designed for anomaly detection in group behaviors, demonstrate that
our method accurately detects significant system-level changes, offering a
scalable and privacy-preserving solution for monitoring complex multi-agent
systems. In addition to this methodological contribution, we introduce new,
challenging multi-entity multivariate time series datasets generated from crowd
simulations in Unity and coupled nonlinear oscillators. To the best of our
knowledge, there is currently no publicly available dataset of this type
designed explicitly to evaluate CPD in complex collective and interactive
systems, highlighting an essential gap that our work addresses.

</details>


### [2] [Multi-Target Detection for Cognitive MIMO Radar Networks](https://arxiv.org/abs/2509.18381)
*Nicholas L. K. Goradia,Harpreet S. Dhillon,R. Michael Buehrer*

Main category: eess.SP

TL;DR: 该论文开发了集中式和分布式信号融合技术，用于在未知噪声和杂波分布下的CFAR多目标检测，利用强化学习提升检测性能，并揭示了空间域和时间域之间的基本权衡关系。


<details>
  <summary>Details</summary>
Motivation: 解决在未知噪声和杂波分布环境下，认知雷达网络进行多目标检测时面临的挑战，特别是如何实现恒虚警率检测并提高检测性能。

Method: 开发了适用于共址单基地MIMO雷达的检测统计量，利用强化学习让雷达学习目标可能位置，并将方法推广到认知雷达网络中的集中式和分布式检测场景。

Result: 提出的检测统计量在接收脉冲数足够大时渐近CFAR，展示了雷达天线数量与时间样本数之间的权衡关系，验证了认知雷达网络在检测性能上的优势。

Conclusion: 该研究为未知环境下的多目标检测提供了有效的技术方案，揭示了空间-时间权衡关系，为认知雷达网络的实际应用提供了理论支持。

Abstract: In this work, we develop centralized and decentralized signal fusion
techniques for constant false alarm rate (CFAR) multi-target detection with a
cognitive radar network in unknown noise and clutter distributions. Further, we
first develop a detection statistic for co-located monostatic MIMO radar in
unknown noise and clutter distributions which is asymptotically CFAR as the
number of received pulses over all antennas grows large, and we provide
conditions under which this detection statistic is valid. We leverage
reinforcement learning (RL) for improved multi-target detection performance,
where the radar learns likely target locations in a search area. These results
are then generalized to the setting of cognitive radar networks, where radars
collaborate to learn where targets are likely to appear in a search area. We
show a fundamental tradeoff between the spatial and temporal domain for CFAR
detection in unknown noise and clutter distributions; in other words, we show a
tradeoff between the number of radar antennas and the number of temporal
samples. We show the benefits and tradeoffs with centralized and decentralized
detection with a network of cognitive radars.

</details>


### [3] [Automatic Model Extraction of the Match Standard in Symmetric--Reciprocal--Match Calibration](https://arxiv.org/abs/2509.18426)
*Ziad Hatab,Michael Ernst Gadringer,Arash Arsanjani,Wolfgang Boesch*

Main category: eess.SP

TL;DR: 本文提出了一种在矢量网络分析仪对称互易匹配校准方法中对匹配标准寄生参数进行建模的新方法，通过非线性全局优化程序实现任意频率相关模型的自动提取。


<details>
  <summary>Details</summary>
Motivation: 传统SRM校准方法假设匹配标准完全已知，但实际上匹配标准的寄生参数会影响校准精度，需要更准确的建模方法。

Method: 采用非线性全局优化程序对匹配标准进行任意频率相关模型建模，通过数值测试和微带线测量验证方法的有效性。

Result: 数值测试显示能够以软件数值精度恢复匹配标准寄生模型，微带线测量表明该方法精度与使用多线TRL校准定义的匹配标准相当。

Conclusion: 所提出的自动模型提取方法能够有效提高SRM校准精度，达到与多线TRL校准相当的准确度。

Abstract: This paper addresses the modeling of parasitics of the match standard in the
symmetric-reciprocal-match (SRM) calibration method of vector network analyzers
(VNAs). In the general SRM procedure, the match standard is assumed to be fully
known. Here, we demonstrate that the match can be modeled with an arbitrary
frequency-dependent model using a non-linear global optimization procedure. To
highlight the validity of the suggested approach, numerical tests were
conducted, demonstrating the ability to recover the match standard parasitic
model down to software numerical precision. Additionally, we performed
microstrip line measurements to compare the SRM calibration with match modeling
to the multiline thru-reflect-line (TRL) calibration one, showing that
automatic model extraction can achieve accuracy similar to using a match
standard defined through multiline TRL calibration.

</details>


### [4] [A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems](https://arxiv.org/abs/2509.18555)
*Ping Wang,Zulin Wang,Yuanfang Ma,Xiaosi Tian,Yuanhan Ni*

Main category: eess.SP

TL;DR: 本文提出了一种安全仿射频分复用（SE-AFDM）技术，通过时变参数c2增强无线通信系统的安全性，同时保持合法接收者的良好误码率性能。


<details>
  <summary>Details</summary>
Motivation: 针对高移动性场景下的通信安全问题，传统方法在保证可靠性的同时难以有效防止窃听。需要一种既能适应双选择性信道又能提供安全性的新型波形技术。

Method: 在AFDM基础上引入时变参数c2作为安全机制。合法接收者可以消除c2引入的非线性影响，而窃听者无法分离c2和随机信息符号，导致其误码率性能严重恶化。

Result: 理论分析和数值结果表明，SE-AFDM在保持良好误码率性能的同时，能显著降低窃听者的有效信干噪比，扩展c2的取值范围可进一步增强安全性。

Conclusion: SE-AFDM波形在高移动性场景下实现了通信安全性和可靠性的有效平衡，为无线通信系统提供了一种实用的安全解决方案。

Abstract: This paper introduces a secure affine frequency division multiplexing
(SE-AFDM) for wireless communication systems to enhance communication security.
Besides configuring the parameter c1 to obtain communication reliability under
doubly selective channels, we also utilize the time-varying parameter c2 to
improve the security of the communications system. The derived input-output
relation shows that the legitimate receiver can eliminate the nonlinear impact
introduced by the time-varying c2 without losing the bit error rate (BER)
performance. Moreover, it is theoretically proved that the eavesdropper cannot
separate the time-varying c2 and random information symbols, such that the BER
performance of the eavesdropper is severely deteriorated. Meanwhile, the
analysis of the effective signal-to-interference-plus-noise ratio (SINR) of the
eavesdropper illustrates that the SINR decreases as the value range of c2
expands. Numerical results verify that the proposed SE-AFDM waveform has
significant security while maintaining good BER performance in high-mobility
scenarios.

</details>


### [5] [Integrated Cellular and LEO-based Positioning and Synchronization under User Mobility](https://arxiv.org/abs/2509.18727)
*Yasaman Ettefagh,Sharief Saleh,Musa Furkan Keskin,Hui Chen,Gonzalo Seco-Granados,Henk Wymeersch*

Main category: eess.SP

TL;DR: 本文研究利用地面和非地面网络（特别是低地球轨道卫星）对移动用户设备进行定位、同步和速度估计，重点分析单基站单卫星最小配置下的信号模型和估计算法。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络发展，需要解决移动用户在混合地面-卫星网络环境下的精确定位、同步和速度估计问题，特别是在资源受限的最小配置场景下。

Method: 建立考虑移动性、时钟和频率偏移的通用信号模型，提出按计算复杂度分层的简化模型系列，并为每个模型开发相应的估计算法。

Result: 通过严格仿真验证了所提模型的有效性，证明其在不同场景下的适用性，展示了复杂度与性能之间的优化权衡。

Conclusion: 研究结果为6G移动用户定位和同步系统提供了有价值的见解，表明可以根据部署环境和应用需求优化复杂度与性能的平衡。

Abstract: This paper investigates the localization, synchronization, and speed
estimation of a mobile user equipment (UE) leveraging integrated terrestrial
and non-terrestrial networks (NTNs), in particular low Earth orbit (LEO)
satellites. We focus on a minimal setup in which the UE received signal from
only one base station (BS) and one LEO satellite. We derive a generic signal
model accounting for mobility, clock and frequency offsets, based on which a
hierarchy of simplified models are proposed and organized by computational
complexity. Estimation algorithms are developed for each model to facilitate
efficient and accurate parameter recovery. Rigorous simulations validate the
effectiveness of the proposed models, demonstrating their suitability across
diverse scenarios. The findings highlight how the trade-off between complexity
and performance can be optimized for varying deployment environments and
application requirements, offering valuable insights for 6G positioning and
synchronization systems under user mobility.

</details>


### [6] [Detection Capability Comparison Between Intensity Detection and Splitting Detection for Rydberg-Atomic Sensors](https://arxiv.org/abs/2509.18753)
*Hao Wu,Xinyuan Yao,Rui Ni,Chen Gong,Kaibin Huang*

Main category: eess.SP

TL;DR: 本文系统分析了里德堡原子量子接收器的两种信号读出方案（强度检测和分裂检测），提出了最大似然估计方法和克拉美-罗下界分析，并通过在最大斜率区域优先采样来优化检测性能。


<details>
  <summary>Details</summary>
Motivation: 里德堡原子量子接收器在射频测量中具有高灵敏度，但其两种基本信号读出方案（强度检测和分裂检测）的性能优化策略尚未系统研究，特别是在分裂检测中最大斜率区域采样的应用尚未探索。

Method: 1. 系统分类和建模现有信号读出方法；2. 推导两种检测模式的最大似然估计程序和克拉美-罗下界；3. 提出在最大斜率幅度区域采集数据的优化策略；4. 实现非均匀频率扫描和最大似然分裂估计方法。

Result: 数值结果表明，两种基本信号读出方法基于提出的最大似然估计方法都实现了更低的估计方差，分裂检测中非均匀频率扫描结合最大似然估计相比传统多项式拟合显著减少了估计方差。

Conclusion: 本研究为两种检测方案提供了性能优化策略，揭示了最优检测性能，同时有助于提高微波校准的准确性，最大似然估计方法在两种检测方案中都能有效降低估计方差。

Abstract: Rydberg atomic quantum receivers have been seen as novel radio frequency
measurements and the high sensitivity to a large range of frequencies makes it
attractive for communications reception. However, their unique physical
characteristics enable two fundamental signal readout schemes: intensity-based
detection and splitting-based detection. The former measures the electric
fields through laser intensity, while the latter utilizes Autler-Townes
splitting. In this work, we systematically categorize and model existing signal
readout methods, classifying them into these two paradigms. Then, we derive the
maximum likelihood estimation procedures and corresponding Cram\'er-Rao lower
bounds (CRLB) for each detection modality. Through the analysis of the CRLB, we
propose strategy for both readout schemes to enhance sensitivity and minimize
estimation variance: acquiring data in regions with maximal slope magnitudes.
While this approach has been implemented in intensity-based detection (e.g.,
superheterodyne schemes), its application to splitting-based detection remains
unexplored. Implementation of non-uniform frequency scanning, with preferential
sampling at regions exhibiting maximum peak slopes combined with our proposed
maximum likelihood splitting estimation method, achieves significantly reduced
estimation variance compared to conventional polynomial fitting. The
comparative analysis reveals the optimal detection performance of the two
detection schemes. This work also contributes to enhancing the accuracy of
microwave calibration. Numerical results reveal that both fundamental signal
readout methods achieve lower estimation variance based on our proposed maximum
likelihood estimation approach.

</details>


### [7] [Highly Parallel Singular Value Decomposition for Low-Latency MIMO Processing](https://arxiv.org/abs/2509.18799)
*Sijia Cheng,Liang Liu,Ove Edfors,Juan Vidal Alegria*

Main category: eess.SP

TL;DR: 论文分析了SVD在无线系统中的延迟问题，提出了一种基于Gram矩阵三对角化的4步并行方法，并开发了硬件分析的时间复杂度框架，证明该方法在大规模MIMO场景下的高效性。


<details>
  <summary>Details</summary>
Motivation: SVD在MIMO处理等无线系统中广泛应用，但传统迭代分解方法随系统规模增大会导致执行时间增加，难以满足实时和低延迟应用需求。

Method: 分析了现有SVD方法的延迟，提出基于Gram矩阵三对角化的4步高度并行方法，并开发了结合硬件分析的时间复杂度评估框架。

Result: 数值结果表明所选并行方法具有优越的时间效率，特别是在大规模MIMO场景下表现突出。

Conclusion: 基于Gram矩阵三对角化的并行SVD方法能有效解决大规模无线系统中的延迟问题，为实时应用提供了可行的解决方案。

Abstract: Singular value decomposition (SVD) is widely used in wireless systems,
including multiple-input multiple-output (MIMO) processing and dimension
reduction in distributed MIMO (D-MIMO). However, the iterative nature of
decomposition methods results in increased execution time as system size grows,
posing challenges for real-time and low-latency applications. To address this,
we analyze the latency of state-of-art SVD methods, and highlight the
efficiency of a 4-step highly parallel method based on Gram matrix
tridiagonalization. Furthermore, we develop a time complexity (processing
latency) analysis framework with hardware profiling, allowing scalable and
realistic evaluation without full implementation. The numerical results
demonstrate the superior time efficiency of the selected parallel method,
particularly in massive MIMO scenarios.

</details>


### [8] [Normal mode parameters estimation by a VLA in single-shooting](https://arxiv.org/abs/2509.18853)
*Xiaolei Li,Pengyu Wang,Wenhua Song,Yangjin Xu,Wei Gao*

Main category: eess.SP

TL;DR: 提出正交约束模态搜索方法，利用垂直线性阵列估计模态波数和模态深度函数，在已知声速剖面下通过模态正交性提取模态参数


<details>
  <summary>Details</summary>
Motivation: 解决在垂直线性阵列和单频声源静止情况下，准确估计模态波数和模态深度函数的问题

Method: 基于模态深度函数的正交性设计OCMS算法，利用已知声速剖面提取模态参数

Result: 数值模拟显示OCMS对噪声、阵列孔径变化、阵元数量变化具有鲁棒性，在声速剖面误差<1m/s和阵列倾斜角<5°时性能可靠；实验验证显示模态波数相对误差约为10^-4

Conclusion: OCMS方法能够有效提取模态参数，在多种实际条件下保持稳定性能，为水声模态分析提供了可靠工具

Abstract: This paper proposes an orthogonality-constrained modal search (OCMS) method
for estimating modal wavenumbers and modal depth functions using a vertical
linear array (VLA). Under the assumption of a known sound speed profile, OCMS
leverages the orthogonality of distinct modal depth functions to extract both
the modal depth functions and their corresponding wavenumbers, even when the
VLA and a monochromatic sound source remain stationary.The performance of OCMS
is evaluated through numerical simulations under varying signal-to-noise ratios
(SNRs), different VLA apertures, varying numbers of VLA elements, VLA tilt and
sound speed profile (SSP) uncertainty. The results demonstrate that OCMS is
robust against noise, VLA aperture variations, and changes in the number of VLA
elements, meanwhile, the algorithm maintains reliable performance when SSP
uncertainty < 1 m/s and VLA tilt angle <5{\deg}. Furthermore, the effectiveness
of OCMS is validated using SwellEx96 experimental data. The relative error
between the modal wavenumbers derived from experimental data and those computed
via Kraken is on the order of $10^{-4}$.

</details>


### [9] [Quaternion LMS for Graph Signal Recovery](https://arxiv.org/abs/2509.18918)
*Hamideh-Sadat Fazael-Ardekani,Hadi Zayyani,Hamid Soltanian-Zadeh*

Main category: eess.SP

TL;DR: 该论文将图信号处理中的图信号恢复问题推广到四元数域，提出了四元数图LMS算法，并进行了收敛性分析和仿真验证。


<details>
  <summary>Details</summary>
Motivation: 将图信号处理中的图信号恢复问题扩展到四元数域，结合四元数最小均方算法和图LMS算法的优势，处理更复杂的多维信号。

Method: 推导了基于四元数代数的自适应公式，进行了均值收敛分析和均方收敛分析，提出了QGLMS算法的步长参数充分条件。

Result: 仿真结果表明所提出的QGLMS算法在图信号重构中具有有效性。

Conclusion: 成功将图信号恢复问题扩展到四元数域，提出的QGLMS算法在理论和实验上都表现出良好的性能。

Abstract: This letter generalizes the Graph Signal Recovery (GSR) problem in Graph
Signal Processing (GSP) to the Quaternion domain. It extends the Quaternion
Least Mean Square (QLMS) in adaptive filtering literature, and Graph LMS (GLMS)
algorithm in GSP literature, to an algorithm called Quaternion GLMS (QGLMS).
The basic adaptation formula using Quaternion-based algebra is derived.
Moreover, mean convergence analysis and mean-square convergence analysis are
mathematically performed. Hence, a sufficient condition on the step-size
parameter of QGLMS is suggested. Also, simulation results demonstrate the
effectiveness of the proposed algorithm in graph signal reconstruction.

</details>


### [10] [Bayesian Convolutional Neural Networks for Prior Learning in Graph Signal Recovery](https://arxiv.org/abs/2509.19056)
*Razieh Torkamani,Arash Amini,Hadi Zayyani,Mehdi Korki*

Main category: eess.SP

TL;DR: 提出了一种基于贝叶斯卷积神经网络的数据驱动图信号恢复框架，通过学习图信号的先验分布来实现对噪声或缺失观测的鲁棒恢复。


<details>
  <summary>Details</summary>
Motivation: 图信号恢复中的核心挑战是底层统计模型通常未知或过于复杂难以解析指定，需要一种灵活的数据驱动方法来学习信号先验。

Method: 开发了基于切比雪夫多项式的图感知滤波器的贝叶斯CNN架构，将隐藏层解释为吉布斯分布并使用GMM非线性激活，构建闭式表达的先验，并集成到变分贝叶斯推理框架中。

Result: 在合成和真实图数据集上的实验表明，BCNN-GSR算法在各种信号分布下都能实现准确鲁棒的恢复，对复杂非高斯信号模型具有良好的泛化能力。

Conclusion: 该方法计算高效，适用于大规模图恢复任务，为图信号处理提供了一种有效的数据驱动解决方案。

Abstract: Graph signal recovery (GSR) is a fundamental problem in graph signal
processing, where the goal is to reconstruct a complete signal defined over a
graph from a subset of noisy or missing observations. A central challenge in
GSR is that the underlying statistical model of the graph signal is often
unknown or too complex to specify analytically. To address this, we propose a
flexible, data-driven framework that learns the signal prior directly from
training samples. We develop a Bayesian convolutional neural network (BCNN)
architecture that models the prior distribution of graph signals using
graph-aware filters based on Chebyshev polynomials. By interpreting the hidden
layers of the CNN as Gibbs distributions and employing Gaussian mixture model
(GMM) nonlinearities, we obtain a closed-form and expressive prior. This prior
is integrated into a variational Bayesian (VB) inference framework to estimate
the posterior distribution of the signal and noise precision. Extensive
experiments on synthetic and real-world graph datasets demonstrate that the
proposed BCNN-GSR algorithm achieves accurate and robust recovery across a
variety of signal distributions. The method generalizes well to complex,
non-Gaussian signal models and remains computationally efficient, making it
suitable for practical large-scale graph recovery tasks.

</details>


### [11] [Data-Free Knowledge Distillation for LiDAR-Aided Beam Tracking in MmWave Systems](https://arxiv.org/abs/2509.19092)
*Abolfazl Zakeri,Nhan Thanh Nguyen,Ahmed Alkhateeb,Markku Juntti*

Main category: eess.SP

TL;DR: 提出了一种无需真实数据的数据自由知识蒸馏框架，用于LiDAR辅助毫米波波束跟踪，通过生成器合成数据并训练学生模型，性能甚至略优于教师模型。


<details>
  <summary>Details</summary>
Motivation: 多模态传感虽然减少了波束训练开销，但受到机器学习复杂性和数据集需求的限制，需要开发更高效的方法。

Method: 采用知识反演框架，生成器从随机噪声合成LiDAR输入数据，通过元数据损失、激活损失和熵损失指导；学生模型使用合成数据和教师模型知识进行训练，结合KL散度损失和MSE损失。

Result: 仿真结果显示，提出的DF-KD框架在Top-1和Top-5准确率上略优于教师模型，元数据损失对生成器性能贡献显著，MSE损失可有效替代标准KD损失且需要更少的超参数调优。

Conclusion: 该数据自由知识蒸馏框架为LiDAR辅助毫米波波束跟踪提供了一种高效的解决方案，在减少数据依赖的同时实现了优异的性能。

Abstract: Multimodal sensing reduces beam training overhead but is constrained by
machine learning complexity and dataset demands. To address this, we propose a
data-free (DF) knowledge distillation (KD) framework for efficient LiDAR-aided
mmWave beam tracking, i.e., predicting the best current and future beams.
Specifically, we propose a knowledge inversion framework, where a generator
synthesizes LiDAR input data from random noise, guided by a loss function
defined on the features and outputs of a pre-trained teacher model. The student
model is then trained using the synthetic data and knowledge distilled from the
teacher. The generator loss combines three terms, called metadata loss,
activation loss, and entropy loss. For student training, in addition to the
standard Kullback-Leibler divergence loss, we also consider a mean-squared
error (MSE) loss between the teacher and student logits. Simulation results
show that the proposed DF-KD (slightly) outperforms the teacher in Top-1 and
Top-5 accuracies. Moreover, we observe that the metadata loss contributes
significantly to the generator performance, and that the MSE loss for the
student can effectively replace the standard KD loss while requiring fewer
fine-tuned hyperparameters.

</details>


### [12] [Enabling Drone Detection with SWARM Repeater-Assisted MIMO ISAC](https://arxiv.org/abs/2509.19119)
*Palatip Jopanya,Diana P. M. Osorio*

Main category: eess.SP

TL;DR: 本文探讨了在群中继器辅助的MIMO ISAC系统中，通过优化中继器增益来增强无人机检测的雷达感知能力。


<details>
  <summary>Details</summary>
Motivation: 随着集成感知与通信（ISAC）的新架构方面、用例和标准的定义不断出现，基于大规模MIMO天线技术的蜂窝系统也在通过集成新型网络组件经历并行演进，以支持新兴的ISAC用例和服务。

Method: 利用中继器即时重传信号的能力，研究这些中继器如何增强无人机检测的雷达感知能力，并通过优化中继器增益来提升性能。

Result: 结果表明，在给定足够最大放大增益的情况下，通过优化中继器增益，增加中继器数量可以带来感知性能的提升。

Conclusion: 群中继器部署可以成本高效地增强蜂窝网络的雷达感知能力，特别是在无人机检测等ISAC应用中。

Abstract: As definitions about new architectural aspects, use cases, and standards for
integrated sensing and communication (ISAC) continue to appear, cellular
systems based on massive multiple-input multiple-output (MIMO) antenna
technology are also experiencing a parallel evolution through the integration
of novel network components. This evolution should support emerging ISAC use
cases and services. In particular, this paper explores a recent vision for
cost-efficient cellular network densification through the deployment of swarms
of repeaters. Leveraging their ability to retransmit signals instantaneously,
we investigate how these repeaters can enhance radar sensing capabilities for
drone detection in a swarm repeater-assisted MIMO ISAC system. Our results
demonstrate that, by optimizing the gains of repeaters given a sufficient
maximum amplification gain, increasing the number of repeaters can lead to
gains in sensing performance.

</details>


### [13] [Deep Reinforcement Learning for Dynamic Sensing and Communications](https://arxiv.org/abs/2509.19130)
*Abolfazl Zakeri,Nhan Thanh Nguyen,Ahmed Alkhateeb,Markku Juntti*

Main category: eess.SP

TL;DR: 提出一个统一的机器学习框架，动态决定何时进行环境感知并利用感知数据预测波束，在满足平均感知预算的前提下最大化信噪比。


<details>
  <summary>Details</summary>
Motivation: 环境感知能增强毫米波通信的波束训练，但需要平衡感知带来的收益与成本。

Method: 使用Lyapunov优化强制执行感知约束，深度Q网络决定感知时隙，预训练的深度神经网络将感知数据映射到码本中的最优波束。

Result: 基于真实世界DeepSense数据集的仿真表明，该方法显著降低了感知开销，同时保持了满意的通信性能。

Conclusion: 所提出的方法在减少感知开销的同时，能够维持良好的通信性能，实现了感知成本与通信增益的有效平衡。

Abstract: Environmental sensing can significantly enhance mmWave communications by
assisting beam training, yet its benefits must be balanced against the
associated sensing costs. To this end, we propose a unified machine learning
framework that dynamically determines when to sense and leverages sensory data
for beam prediction. Specifically, we formulate a joint sensing and beamforming
problem that maximizes the av- erage signal-to-noise ratio under an average
sensing budget. Lyapunov optimization is employed to enforce the sensing
constraint, while a deep Q-Network determines the sensing slots. A pretrained
deep neural network then maps the sens- ing data to optimal beams in the
codebook. Simulations based on the real-world DeepSense dataset demonstrate
that the pro- posed approach substantially reduces sensing overhead while
maintaining satisfactory communications performance.

</details>


### [14] [On the Performance of THz Wireless Systems over $α$-$\mathcal{F}$ Channels with Beam Misalignment and Mobility](https://arxiv.org/abs/2509.19235)
*Wamberto J. L. Queiroz,Hugerles S. Silva,Higo T. P. Silva,Alexandros-Apostolos A. Boulogeorgos*

Main category: eess.SP

TL;DR: 本文研究了太赫兹无线系统在α-F衰落信道中受波束失准和移动性影响的性能，推导了新的统计表达式和性能指标。


<details>
  <summary>Details</summary>
Motivation: 研究太赫兹无线系统在实际信道条件下的性能表现，特别是考虑波束失准和移动性对系统性能的影响。

Method: 推导了瞬时信噪比的概率密度函数、累积分布函数、矩生成函数和高阶矩的新表达式，并基于这些表达式提取了中断概率、符号错误概率和平均信道容量的新公式。

Result: 获得了渐近性能指标，并通过蒙特卡洛仿真验证了所提出的分析框架的有效性。

Conclusion: 提出的分析框架能够有效评估太赫兹无线系统在α-F衰落信道中的性能，为系统设计提供了理论依据。

Abstract: This paper investigates the performance of terahertz~(THz) wireless systems
over the $\alpha$-$\mathcal{F}$ fading channels with beam misalignment and
mobility. New expressions are derived for the probability density, cumulative
distribution, and moment generating functions, as well as higher-order moments
of the instantaneous signal-to-noise ratio. Building upon the aforementioned
expressions, we extract novel formulas for the outage probability, symbol error
probability, and average channel capacity. Asymptotic metrics are also deduced,
which provide useful insights. Monte Carlo simulations results are presented to
support the derived analytical framework.

</details>


### [15] [Faster-Than-Nyquist Signalling - Theoretical Limits on Capacity and Techniques to Approach Capacity](https://arxiv.org/abs/2509.19272)
*Sathwik Chadaga*

Main category: eess.SP

TL;DR: 本文研究了FTN信号传输中的符号间干扰问题，推导了避免ISI的脉冲形状和时间加速因子条件，并通过容量理论验证，探索了功率分配和自适应加载技术来提升OFDM FTN系统的性能。


<details>
  <summary>Details</summary>
Motivation: FTN信号传输能够提供比奈奎斯特传输更高的吞吐量和频谱效率，但会引入符号间干扰。研究旨在找到完全避免ISI的条件，并通过优化技术提升系统性能。

Method: 推导脉冲形状和时间加速因子的数学条件以避免ISI；研究FTN系统的理论容量限制；探索功率分配和自适应加载技术在OFDM FTN系统中的应用；通过仿真验证技术实现效果。

Result: 建立了避免FTN信号ISI的充分条件；验证了FTN系统的容量理论极限；展示了功率分配和自适应加载技术能够有效减少ISI影响并提高系统吞吐量。

Conclusion: FTN信号传输在满足特定条件下可以完全避免ISI，结合功率优化和自适应技术能够显著提升OFDM FTN系统的性能，为实现高速高效通信提供了理论和技术支持。

Abstract: Faster-Than-Nyquist (FTN) Signalling is a non-orthogonal transmission scheme
that violates the Nyquist zero-ISI criterion providing higher throughput and
better spectral efficiency than a Nyquist transmission scheme. In this thesis,
the inter symbol interference (ISI) introduced by FTN signalling is studied,
and conditions on pulse shapes and $\tau$ (time acceleration factor) are
derived so that the ISI can be avoided completely. Further, these conditions
are reinforced by investigating the theoretical limits on the capacities of FTN
systems. Finally, the use of power allocation and adaptive loading techniques
are explored in reducing the effect of ISI and increasing the throughput of
orthogonal frequency division multiplexing (OFDM) FTN systems. The
implementation of these techniques and simulation results are also
demonstrated.

</details>


### [16] [A Novel Site-Specific Inference Model for Urban Canyon Channels: From Measurements to Modeling](https://arxiv.org/abs/2509.19275)
*Junzhe Song,Ruisi He,Mi Yang,Zhengyu Zhang,Xinwen Chen,Xiaoying Zhang,Bo Ai*

Main category: eess.SP

TL;DR: 本文提出了一种基于环境几何的站点特定信道推断模型，该模型使用亚6GHz信道测量进行参数化，通过几何传播提取和聚类多径分量，建立了物理环境与MPC统计特性之间的可解释映射。


<details>
  <summary>Details</summary>
Motivation: 随着智能交通和智慧城市应用的快速发展，城市峡谷已成为无线通信系统设计和评估的关键场景。由于其独特的环境布局，城市峡谷中的信道特性强烈依赖于街道几何和建筑分布，从而表现出显著的站点特定信道条件。然而，这一特征在现有信道模型中尚未得到很好的捕捉。

Method: 提出基于环境几何的站点特定信道推断模型，使用亚6GHz信道测量进行参数化。根据几何传播提取和聚类多径分量，这些分量明确来源于峡谷宽度的影响，从而建立物理环境与MPC统计特性之间的可解释映射。提出了逐步实施方案。

Result: 通过比较从模型和测量得出的信道二阶统计量，验证了所提出的站点特定信道推断模型。结果表明，该模型在不同城市峡谷场景中实现了高精度和鲁棒性。

Conclusion: 所提出的站点特定信道推断模型能够准确捕捉城市峡谷环境中的信道特性，为无线通信系统设计和评估提供了有效的工具。

Abstract: With the rapid development of intelligent transportation and smart city
applications, urban canyon has become a critical scenario for the design and
evaluation of wireless communication systems. Due to its unique environmental
layout, the channel characteristics in urban canyon are strongly a street
geometry and building distribution, thereby exhibiting significant
site-specific channel condition. However, this feature has not been well
captured in existing channel models. In this paper, we propose a site-specific
channel inference model based on environmental geometry, the model is
parameterized using sub-6GHz channel measurements. Multipath components (MPCs)
are extracted and clustered according to geometric propagation, which are
explicitly derived from the influence of canyon width, thereby establishing an
interpretable mapping between the physical environment and statistical
characteristics of MPCs. A step-by-step implementation scheme is presented.
Subsequently, the proposed site-specific channel inference model is validated
by comparing second-order statistics of channels, derived from the model and
measurements. The results show that the proposed model achieves high accuracy
and robustness in different urban canyon scenarios.

</details>


### [17] [STFT-AECNN: An Attention-Enhanced CNN for Efficient Φ-OTDR Event Recognition in IoT-Enabled Distributed Acoustic Sensing](https://arxiv.org/abs/2509.19281)
*Xiyang Lan,Xin Li*

Main category: eess.SP

TL;DR: 提出了一种基于STFT的注意力增强卷积神经网络（STFT-AECNN），用于相位敏感光时域反射计（Φ-OTDR）数据的事件识别，在保持高计算效率的同时达到99.94%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在处理Φ-OTDR数据时要么破坏信号的时空结构，要么计算成本过高，限制了在资源受限的IoT场景中的应用。

Method: 将多通道时间序列数据表示为堆叠的频谱图，利用2D CNN处理；引入空间高效注意力模块（SEAM）自适应强调信息量最大的通道；采用交叉熵和三元组损失的联合损失函数增强特征空间的可区分性。

Result: 在BJTU Φ-OTDR数据集上的实验表明，STFT-AECNN达到了99.94%的峰值准确率，同时保持高计算效率。

Conclusion: 该方法在IoT支持的DAS系统中具有实时、可扩展和鲁棒的事件识别潜力，为可靠的智能IoT传感应用铺平了道路。

Abstract: Phase-sensitive optical time-domain reflectometry ({\Phi}-OTDR) has emerged
as a promising sensing technology in Internet of Things (IoT) infrastructures,
enabling large-scale distributed acoustic sensing (DAS) for smart city
surveillance, industrial pipeline monitoring, and critical infrastructure
protection. However, accurately recognizing events from massive {\Phi}-OTDR
data streams remains challenging, as existing deep learning methods either
disrupt the inherent spatiotemporal structure of signals or incur prohibitive
computational costs, limiting their applicability in resource-constrained IoT
scenarios. To overcome these challenges, we propose a novel STFT-based
Attention-Enhanced Convolutional Neural Network (STFT-AECNN), which represents
multi-channel time-series data as stacked spectrograms to fully exploit their
spatiotemporal characteristics while enabling efficient 2D CNN processing. A
Spatial Efficient Attention Module (SEAM) is further introduced to adaptively
emphasize the most informative channels, and a joint Cross-Entropy and Triplet
loss is adopted to enhance the discriminability of the learned feature space.
Extensive experiments on the public BJTU {\Phi}-OTDR dataset demonstrate that
STFT-AECNN achieves a peak accuracy of 99.94% while maintaining high
computational efficiency. These results highlight its potential for real-time,
scalable, and robust event recognition in IoT-enabled DAS systems, paving the
way for reliable and intelligent IoT sensing applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: 使用图像机器学习模型分析Ulam螺旋中不同区域的质数分布规律性，发现高数值区域（约5亿附近）比低数值区域（低于2500万）更容易学习，表明高数值区域存在更有序的质数模式。


<details>
  <summary>Details</summary>
Motivation: 质数分布具有确定性定义但表现出类似随机过程的统计行为。研究旨在探索机器学习能否作为数论的新实验工具，特别是用于分析质数在不同数值区域的规律性差异。

Method: 采用图像聚焦的机器学习模型，从Ulam螺旋的不同区域提取数据块进行训练，比较模型在低数值区域（<25m）和高数值区域（~500m）的学习效果。

Result: 在500m附近区域训练的模型在纯准确率上优于在25m以下区域训练的模型，表明高数值区域存在更易学习的秩序。精度和召回率分析显示模型在不同区域采用不同分类策略。

Conclusion: 机器学习可作为数论研究的新实验工具，特别是在研究强质数和弱质数模式方面具有密码学应用潜力，支持了数论中关于高数量级质数分布噪声减少的猜想。

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [19] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: 本文提出了一个基于Wasserstein距离的联邦学习框架，用于解决数据市场中的异构数据估值和选择问题，通过分布式方法近似Wasserstein距离来保护隐私，并利用神经缩放定律预测模型性能。


<details>
  <summary>Details</summary>
Motivation: 在AI时代，数据市场需要可信的模型交易方案。联邦学习虽然能保护数据隐私，但在异构数据源的估值和选择方面仍面临挑战，需要一种能预测模型性能并确保数据兼容性的方法。

Method: 提出基于Wasserstein距离的估计器框架，采用分布式方法近似Wasserstein距离而不需要原始数据访问，利用神经缩放定律外推模型性能，实现高效数据选择。

Result: 在标签偏斜、错误标签和无标签等多种场景下的实验表明，该方法能一致地识别高性能数据组合，提升联邦学习模型市场的可靠性。

Conclusion: 该框架为基于联邦学习的模型市场提供了有效的数据估值和选择方案，通过Wasserstein距离估计和性能预测解决了异构数据源的关键挑战。

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [20] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 该研究比较了完全学习的神经ODE与物理信息通用微分方程在连续时间库存动力学预测中的表现，发现在结构化需求机制下UDE泛化更好，而在重尾需求下NODE更灵活。


<details>
  <summary>Details</summary>
Motivation: 研究结构偏差在不同需求机制下对牛鞭效应预测的影响，为科学和工程系统中的混合建模提供指导。

Method: 使用单级测试平台，比较完全学习的NODE和保留守恒结构的UDE在三种需求机制（AR(1)、i.i.d.高斯、重尾对数正态）下的表现。

Result: 在结构化需求机制下，UDE泛化更好（库存RMSE从4.92降至0.26）；在重尾需求下NODE更优。训练数据减少时，NODE出现相位漂移，UDE保持稳定但对罕见峰值反应不足。

Conclusion: 当噪声为轻尾或时间相关时应强制结构约束；当极端事件主导时应放松结构约束。这为科学工程系统的混合建模提供了具体指导。

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [21] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: 该研究提出了一种基于模型的迁移学习方法，使用神经网络代理模型，使在一个桥梁上训练的模型能够适应具有相似特征的另一个桥梁，以解决大型桥梁网络中结构评估的可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着永久监测系统的广泛应用，数据可用性增加，为结构评估提供了新机会，但在大型桥梁网络中面临可扩展性挑战。管理多个结构需要高效跟踪和比较长期行为，因此相似结构之间的知识转移变得至关重要。

Method: 采用基于模型的迁移学习方法，使用神经网络代理模型。将在一个桥梁上训练的模型迁移到具有相似特征的另一个桥梁上，这些模型捕捉共享的损伤机制。将迁移模型集成到贝叶斯推理框架中，基于监测数据的模态特征进行连续损伤评估。

Result: 使用两个桥梁的真实数据进行验证，结果显示该方法对损伤位置、严重程度和范围具有高敏感性。

Conclusion: 该方法增强了实时监测能力，实现了跨结构知识转移，促进了智能监测策略，提高了网络层面的韧性。

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [22] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: 提出AdaMixT架构，通过自适应多尺度专家变换器解决多元时间序列预测中多尺度特征融合不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖预定义的单尺度补丁或缺乏有效的多尺度特征融合机制，无法充分捕捉时间序列中的复杂模式，导致性能受限和泛化能力不足

Method: AdaMixT引入多种补丁，利用通用预训练模型和领域特定模型进行多尺度特征提取，并通过门控网络动态分配不同专家的权重，实现自适应多尺度融合

Result: 在8个广泛使用的基准数据集上的综合实验表明，AdaMixT在真实场景中具有持续有效性

Conclusion: AdaMixT通过自适应多尺度专家融合机制，显著提升了多元时间序列预测的性能和泛化能力

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [23] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE是一个开源、模块化的框架，利用大语言模型进行迭代式算法解决方案生成，集成了生成、测试、分析和评估功能。


<details>
  <summary>Details</summary>
Motivation: 为了简化算法设计过程，通过抽象提示设计和模型管理的复杂性，为研究人员和从业者提供一个透明可扩展的平台。

Method: 采用多LLM协同架构，将不同LLM分配为生成器、分析师和评估器等互补角色，构建可重复的反馈循环。

Result: EASE框架能够支持跨领域的算法和生成解决方案的协同设计，用户对错误处理、分析和质量评估具有完全控制权。

Conclusion: EASE通过模块化设计和多LLM协同工作，为算法解决方案的迭代生成提供了一个高效、可控且可扩展的平台。

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [24] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: 本文提出了一种基于AIS数据的机器学习管道，用于在狭窄海域对移动船只进行类型分类，通过轨迹级特征和树模型实现了92.15%的准确率。


<details>
  <summary>Details</summary>
Motivation: 准确识别AIS轨迹中的船只类型对于安全监管和打击非法、未报告和无管制(IUU)活动至关重要。

Method: 使用历史AIS数据，经过数据预处理后提取31个轨迹级特征，采用基于树的模型（如随机森林）进行五类船只分类，并使用分组训练/测试分割避免数据泄露。

Result: 随机森林模型在测试集上达到92.15%准确率，宏精度94.11%，宏召回率92.51%，宏F1分数93.27%，ROC-AUC最高达0.9897。

Conclusion: 研究表明，基于AIS轨迹的轻量级特征能够在狭窄海域实现实时船只类型分类，具有实际应用价值。

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [25] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: 提出基于patch的PCA-Net框架，通过将解场分解为小块并在每个patch内应用PCA，显著降低计算复杂度，相比全局PCA将端到端处理时间减少3.7-4倍。


<details>
  <summary>Details</summary>
Motivation: 解决在高维解场上应用主成分分析(PCA)时计算开销过大的问题，提高神经算子学习在偏微分方程求解中的计算效率。

Method: 提出两种patch-based PCA方法：局部到全局patch PCA和局部到局部patch PCA，并探索重叠patch平滑滤波和CNN细化两种改进策略。

Result: 基于patch的PCA显著降低计算复杂度，同时保持高精度，端到端处理时间比全局PCA减少3.7-4倍。

Conclusion: 基于patch的PCA是一种有前景的高效算子学习技术，在PDE系统中平衡了计算效率和重构精度。

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [26] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT是一个全自动AI驱动的管道，用于从Kaplan-Meier图中高精度重建个体患者数据，解决了手动数字化方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动数字化，容易出错且缺乏可扩展性，需要自动化解决方案来提高临床研究证据合成的效率和准确性。

Method: KM-GPT整合了先进的图像预处理、基于GPT-5的多模态推理和迭代重建算法，采用混合推理架构自动将非结构化信息转换为结构化数据流。

Result: 在合成和真实数据集上的严格评估显示，KM-GPT始终表现出卓越的准确性，并在胃癌免疫治疗试验的荟萃分析中成功应用。

Conclusion: KM-GPT通过自动化传统手动流程，为临床研究提供可扩展的基于网络的解决方案，支持基于证据的决策制定。

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [27] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 提出了一种基于上下文优化（CoOp）的新框架，将子空间表示学习与提示调优相结合，用于改进大规模视觉语言模型的OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示学习的OOD检测方法仅依赖softmax概率，忽略了视觉语言模型在数百万样本上学习到的丰富特征嵌入的判别潜力。

Method: 通过将ID特征投影到提示向量张成的子空间中，同时将ID无关特征投影到正交零空间中，设计易于处理的端到端学习准则。

Result: 在真实世界数据集上的实验证明了该方法的有效性。

Conclusion: 该框架通过整合子空间表示学习和提示调优，显著提升了ID-OOD的可分离性，同时保持了高ID分类准确率。

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [28] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: 提出了一种AI驱动的框架，通过图神经网络预测MCMC预条件器的效果，并使用贝叶斯采集函数优化参数选择，从而加速Krylov子空间求解器的收敛。


<details>
  <summary>Details</summary>
Motivation: 解决大型稀疏线性系统中Krylov求解器在病态矩阵上收敛缓慢的问题，传统MCMC预条件器参数优化成本高昂，需要自动化参数推荐方法。

Method: 使用图神经网络构建代理模型预测预条件速度，结合贝叶斯采集函数选择最优MCMC参数集，以最小化迭代次数。

Result: 在未见过的病态系统上，该框架仅用传统方法50%的搜索预算就实现了更好的预条件效果，收敛迭代次数减少约10%。

Conclusion: 该研究为将MCMC预条件器集成到大规模系统提供了一条可行路径，展示了AI驱动参数优化的有效性。

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [29] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 本文首次全面比较了用于自动产前CTG分析的最先进AI方法，发现微调的LLMs在性能上优于基础模型和领域特定方法，为临床CTG解释提供了有前景的替代途径。


<details>
  <summary>Details</summary>
Motivation: 电子胎儿监测（EFM）/胎心监护（CTG）分析是评估胎儿健康的关键技术，但当前CTG评估主要依赖主观临床解释，导致诊断准确性存在差异。基础模型和大型语言模型在医疗领域表现出色，但在CTG分析方面的潜力尚未充分探索。

Method: 系统比较时间序列基础模型、LLMs与已建立的CTG特定架构，评估涵盖500多个不同时长的CTG记录，反映了真实世界的临床记录。

Result: 微调的LLMs在性能上优于基础模型和领域特定方法，为临床CTG解释提供了有前景的替代途径。

Conclusion: 这些发现为胎儿监测应用中不同AI方法的相对优势提供了关键见解，并为未来产前护理临床AI发展奠定了基础。

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [30] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: PGPCA是PPCA的扩展，通过引入非线性流形几何来更好地描述分布在流形周围的数据分布，适用于神经科学等领域的高维数据分析。


<details>
  <summary>Details</summary>
Motivation: 传统的PPCA及其扩展主要基于线性模型，只能描述欧几里得坐标系中的数据。但在神经科学等应用中，数据可能分布在非线性流形周围，而非欧几里得空间。

Method: 开发了概率几何主成分分析（PGPCA），通过显式地融入从数据中拟合的非线性流形知识，并推导出几何坐标系来捕捉数据与流形的偏差和噪声。还推导了数据驱动的EM算法来学习PGPCA模型参数。

Result: 在模拟和脑数据分析中，PGPCA能有效建模各种给定流形周围的数据分布，并且在此类数据上优于PPCA。PGPCA还提供了测试几何坐标系是否比欧几里得坐标系更好地描述数据的能力。

Conclusion: PGPCA将PPCA推广到非线性流形几何，能够执行降维并学习流形周围和流形上的数据分布，对于分析具有噪声且分布在非线性流形周围的高维数据具有重要价值。

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [31] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: 本文提出了一种利用BlueField-3 DPU来实时检测和缓解多节点张量并行推理中负载不均衡问题的框架，旨在识别LLM在多GPU执行中的负载不均衡问题及其对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 大型变压器语言模型的自回归推理在解码阶段面临GPU分片间负载不均衡的问题，导致吞吐量下降和延迟峰值，需要有效的实时监测和缓解机制。

Method: 利用DPU辅助框架，将监控任务卸载到DPU，通过分析GPU遥测数据和节点间通信模式，为推理控制器和调度器提供可操作的反馈。

Result: 研究识别了多GPU执行LLM张量计算时出现的负载不均衡问题，并评估了这些问题对计算性能的影响。

Conclusion: DPU网络能够有效跟踪和缓解LLM推理中的负载不均衡问题，为提升推理效率提供了可行的解决方案。

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [32] [Diagonal Linear Networks and the Lasso Regularization Path](https://arxiv.org/abs/2509.18766)
*Raphaël Berthier*

Main category: cs.LG

TL;DR: 本文揭示了对角线性网络的训练轨迹与LASSO正则化路径之间的紧密联系，其中训练时间扮演了逆正则化参数的角色。


<details>
  <summary>Details</summary>
Motivation: 对角线性网络因其隐式正则化可被严格分析而具有理论意义。本文旨在深化这一分析，探索训练轨迹与LASSO路径的确切关系。

Method: 通过理论分析和数值模拟，研究对角线性网络从较小初始化开始的训练过程，并与LASSO正则化路径进行对比。

Result: 在LASSO路径单调性假设下，两者关系精确对应；一般情况下则呈现近似对应关系。

Conclusion: 对角线性网络的训练动态与LASSO正则化路径密切相关，训练时间可视为逆正则化参数，这为理解神经网络隐式正则化提供了新视角。

Abstract: Diagonal linear networks are neural networks with linear activation and
diagonal weight matrices. Their theoretical interest is that their implicit
regularization can be rigorously analyzed: from a small initialization, the
training of diagonal linear networks converges to the linear predictor with
minimal 1-norm among minimizers of the training loss. In this paper, we deepen
this analysis showing that the full training trajectory of diagonal linear
networks is closely related to the lasso regularization path. In this
connection, the training time plays the role of an inverse regularization
parameter. Both rigorous results and simulations are provided to illustrate
this conclusion. Under a monotonicity assumption on the lasso regularization
path, the connection is exact while in the general case, we show an approximate
connection.

</details>


### [33] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出了一种新颖的空间平衡注意力块用于时空预测，通过将空间图划分为子图，结合子图内和子图间注意力机制来平衡局部空间邻近性和全局相关性。


<details>
  <summary>Details</summary>
Motivation: 为了解决时空预测中如何在遵循空间邻近性的同时有效捕捉全局空间相关性这一挑战。

Method: 将空间图划分为子图，使用子图内注意力学习局部空间相关性，通过子图间注意力实现子图间的消息传递，并构建多尺度模型逐步增加子图规模。

Result: 在真实世界的中大型时空数据集上，相比基线方法性能提升最高达7.7%，且运行成本较低。

Conclusion: 该方法既能产生结构化的空间相关性，又具有可扩展性和易实现性，在时空预测任务中表现出色。

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [34] [Central Limit Theorems for Asynchronous Averaged Q-Learning](https://arxiv.org/abs/2509.18964)
*Xingtu Liu*

Main category: cs.LG

TL;DR: 本文为异步更新的Polyak-Ruppert平均Q学习建立了中心极限定理，包括非渐近中心极限定理和函数中心极限定理。


<details>
  <summary>Details</summary>
Motivation: 研究异步更新环境下Q学习的统计性质，特别是其收敛速率和极限分布，为理论分析和实际应用提供理论支撑。

Method: 采用Polyak-Ruppert平均技术，分析异步Q学习算法在Wasserstein距离下的收敛性质，并推导部分和过程的弱收敛性。

Result: 得到了明确的收敛速率表达式，反映了迭代次数、状态-动作空间大小、折扣因子和探索质量的影响，并证明了部分和过程收敛于布朗运动。

Conclusion: 该研究为异步Q学习提供了严格的统计理论基础，揭示了其收敛行为的本质特征。

Abstract: This paper establishes central limit theorems for Polyak-Ruppert averaged
Q-learning under asynchronous updates. We present a non-asymptotic central
limit theorem, where the convergence rate in Wasserstein distance explicitly
reflects the dependence on the number of iterations, state-action space size,
the discount factor, and the quality of exploration. In addition, we derive a
functional central limit theorem, showing that the partial-sum process
converges weakly to a Brownian motion.

</details>


### [35] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 提出了摊销潜在引导（ALS）方法，通过离线计算单个向量来替代昂贵的逐查询优化循环，在推理时以恒定成本应用，实现2-5倍加速，同时匹配或超越贪婪思维链和自一致性基线。


<details>
  <summary>Details</summary>
Motivation: 现有测试时优化方法（如迭代优化和多步验证）推理成本过高，需要10-100倍的计算开销，而潜在空间优化方法如LatentSeek仍需要昂贵的逐查询优化循环。

Method: ALS计算成功与失败生成隐藏状态的平均差异向量，在推理时用这个方向校准模型的隐藏表示：当解码偏离成功流形时，ALS将激活值推回正确方向。

Result: 在GSM8K和MATH-500基准测试中，ALS相比迭代方法实现2-5倍加速，同时匹配或超越贪婪CoT和自一致性基线，效率-准确率权衡提升高达101%。

Conclusion: 研究表明潜在优化的主要收益可以通过离线方式获得，使复杂推理技术能够实际部署到生产环境中。

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [36] [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)
*Sharan Sahu,Martin T. Wells*

Main category: cs.LG

TL;DR: DRO-REBEL是一种鲁棒的离线RLHF方法，通过Wasserstein、KL和χ²模糊集解决奖励错误规范和过优化问题，实现最优参数化收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有离线RLHF方法存在过优化问题，模型会过度拟合奖励错误规范并偏离训练期间观察到的偏好行为。

Method: 提出统一的DRO-REBEL更新家族，使用Fenchel对偶将更新简化为相对奖励回归，避免了PPO风格的裁剪和辅助价值网络。

Result: 在Emotion Alignment、ArmoRM多目标基准和HH-Alignment上表现出强大的最坏情况鲁棒性，χ²-REBEL在经验性能上表现最稳定。

Conclusion: 研究验证了无免费午餐权衡：半径收缩速度快于经验散度集中率可实现最优参数化率但会丧失覆盖保证，而保证覆盖的半径会带来O(n^{-1/4})的收敛率。

Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for
aligning Large Language Models (LLMs) with human intent. However, existing
offline RLHF approaches suffer from overoptimization, where models overfit to
reward misspecification and drift from preferred behaviors observed during
training. We introduce DRO-REBEL, a unified family of robust REBEL updates with
type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality,
each update reduces to a simple relative-reward regression, preserving
scalability and avoiding PPO-style clipping or auxiliary value networks. Under
standard linear-reward and log-linear policy classes with a data-coverage
condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants
than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$
rate via a localized Rademacher complexity analysis. The same analysis closes
the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal
parametric rates. We derive practical SGD algorithms for all three divergences:
gradient regularization (Wasserstein), importance weighting (KL), and a fast
1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale
ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong
worst-case robustness across unseen preference mixtures, model sizes, and data
scales, with $\chi^2$-REBEL showing consistently strong empirical performance.
A controlled radius--coverage study validates a no-free-lunch trade-off: radii
shrinking faster than empirical divergence concentration rates achieve
minimax-optimal parametric rates but forfeit coverage, while
coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

</details>


### [37] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: 提出一种基于机器学习的数字界面自适应设计方法，使用贝叶斯统计建模用户浏览行为，通过在线增量学习实现动态适应不同用户和使用策略


<details>
  <summary>Details</summary>
Motivation: 设计能够动态适应不同用户和使用策略的数字界面，提升用户体验，帮助用户更好地导航和操作界面

Method: 使用贝叶斯统计方法建模用户浏览习惯，采用在线增量学习算法，生成任务模型以图形化表示用户导航行为和使用统计

Result: 仿真实验表明该方法在静态和非静态环境中都有效，能够可靠预测用户行为，即使数据量少或环境变化

Conclusion: 这项研究为自适应系统开辟了新途径，通过帮助用户更好地导航和操作界面来改善用户体验

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [38] [Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering](https://arxiv.org/abs/2509.18653)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: 本文提出了一种新的高维矩阵聚类框架——子空间聚类（SCoS），直接对矩阵数据进行子空间聚类，而非传统方法中的向量化处理。


<details>
  <summary>Details</summary>
Motivation: 传统子空间聚类方法需要将矩阵数据向量化，这会破坏数据的固有结构。本文旨在直接对矩阵样本进行聚类，更好地保留高维数据的子空间结构。

Method: 基于块项分解（BTD）构建三阶张量，通过张量分解联合估计聚类成员关系和部分共享子空间，并提出了可扩展的优化算法。

Result: 在真实高光谱成像数据集上的实验表明，该方法在聚类精度和鲁棒性方面优于现有子空间聚类技术，特别是在高噪声和干扰环境下表现优异。

Conclusion: 该框架在处理具有超越单个数据向量结构的高维应用方面具有巨大潜力，为复杂高维数据分析提供了新的解决方案。

Abstract: We introduce a novel framework for clustering a collection of tall matrices
based on their column spaces, a problem we term Subspace Clustering of
Subspaces (SCoS). Unlike traditional subspace clustering methods that assume
vectorized data, our formulation directly models each data sample as a matrix
and clusters them according to their underlying subspaces. We establish
conceptual links to Subspace Clustering and Generalized Canonical Correlation
Analysis (GCCA), and clarify key differences that arise in this more general
setting. Our approach is based on a Block Term Decomposition (BTD) of a
third-order tensor constructed from the input matrices, enabling joint
estimation of cluster memberships and partially shared subspaces. We provide
the first identifiability results for this formulation and propose scalable
optimization algorithms tailored to large datasets. Experiments on real-world
hyperspectral imaging datasets demonstrate that our method achieves superior
clustering accuracy and robustness, especially under high noise and
interference, compared to existing subspace clustering techniques. These
results highlight the potential of the proposed framework in challenging
high-dimensional applications where structure exists beyond individual data
vectors.

</details>


### [39] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: 本文提出了功能缩放定律（FSL），通过随机微分方程建模SGD训练过程，将学习率调度的影响显式地纳入损失动态分析中，为LLM预训练提供了理论指导。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究主要关注最终损失，忽略了训练过程中的损失动态和学习率调度的影响，本文旨在填补这一空白。

Method: 使用师生核回归设置和在线SGD训练，引入内在时间视角和SDE建模，推导出FSL框架来分析不同学习率调度下的损失演化。

Result: FSL能够准确捕捉学习率调度的影响，理论验证了更高容量模型更高效、学习率衰减提升效率、WSD调度优于直接衰减等经验实践。

Conclusion: FSL框架深化了对LLM预训练动态的理解，为大规模模型训练提供了理论指导和优化工具。

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


### [40] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: 该论文将轻量级随机梯度下降（L-SGD）算法扩展到RISC-V架构的微控制器上，通过8位量化技术显著提升了训练效率，解决了RISC-V平台缺乏浮点单元的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现代IoT设备需要本地机器学习能力，但大多数平台缺乏GPU或专用加速器，使得设备端训练不可行。联邦学习虽然能实现去中心化训练，但需要高效的优化算法。RISC-V作为新兴开源架构，目前缺乏对设备端训练的稳健支持。

Method: 将L-SGD算法扩展到RISC-V MCU平台，评估了32位浮点运算的性能影响，并针对RISC-V缺乏FPU的问题，开发了8位量化版本的L-SGD算法。

Result: 8位量化L-SGD在RISC-V平台上实现了内存使用减少近4倍，训练速度提升2.2倍，同时准确率损失可忽略不计。

Conclusion: 该工作证明了在资源受限的RISC-V MCU上实现高效设备端训练的可行性，为开源硬件平台的机器学习应用提供了重要支持。

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [41] [Stability and Generalization of Adversarial Diffusion Training](https://arxiv.org/abs/2509.19234)
*Hesam Hosseini,Ying Cao,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文通过算法稳定性分析，研究了去中心化网络中对抗训练在扩散策略下的泛化性能，发现泛化误差随对抗扰动强度和训练步数增加而增长


<details>
  <summary>Details</summary>
Motivation: 对抗训练虽然增强模型鲁棒性，但常面临鲁棒过拟合和泛化差距扩大的问题。尽管已有研究证明了去中心化网络中对抗训练的收敛性，但其泛化特性尚未被探索

Method: 使用算法稳定性工具，在凸损失函数假设下，对扩散策略下的对抗训练进行泛化分析

Result: 推导出泛化误差界，显示泛化误差随对抗扰动强度和训练步数增加而增长，这一发现与单智能体情况一致，但在去中心化设置中是新颖的

Conclusion: 在逻辑回归上的数值实验验证了理论预测，为去中心化对抗训练的泛化性能提供了理论依据

Abstract: Algorithmic stability is an established tool for analyzing generalization.
While adversarial training enhances model robustness, it often suffers from
robust overfitting and an enlarged generalization gap. Although recent work has
established the convergence of adversarial training in decentralized networks,
its generalization properties remain unexplored. This work presents a
stability-based generalization analysis of adversarial training under the
diffusion strategy for convex losses. We derive a bound showing that the
generalization error grows with both the adversarial perturbation strength and
the number of training steps, a finding consistent with single-agent case but
novel for decentralized settings. Numerical experiments on logistic regression
validate these theoretical predictions.

</details>


### [42] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL是一个在线强化学习框架，通过难度自适应算法ADAGRPO提升移动GUI代理的性能，在AndroidWorld和AndroidLab上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 开发有效的移动GUI代理面临任务难度分布重尾和大规模环境采样效率低下的挑战。

Method: 提出ADAGRPO算法，包含难度自适应正回放和失败课程过滤，以及最短路径奖励调整策略。

Result: MOBILERL-9B模型在AndroidWorld上达到75.8%的成功率，在AndroidLab上达到46.8%的成功率。

Conclusion: 该框架稳定了RL训练，提高了样本效率，在多样化移动应用和任务中表现出色，已被AutoGLM产品采用并开源。

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [43] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: CoCoGen是一个基于生成AI和势博弈理论的合作竞争兼容数据生成框架，用于解决跨组织联邦学习中的统计异质性和经济竞争问题。


<details>
  <summary>Details</summary>
Motivation: 跨组织联邦学习中，组织间存在经济竞争关系，导致参与联合训练的意愿降低，同时统计异质性和竞争关系的综合影响尚未得到充分研究。

Method: CoCoGen通过学习和性能效用公式刻画竞争和统计异质性，将每轮训练建模为加权势博弈，并基于生成AI推导最大化社会福利的数据生成策略。

Result: 在Fashion-MNIST数据集上的实验表明，CoCoGen在不同异质性和竞争水平下都能优于基线方法，有效优化组织行为和社会福利。

Conclusion: 该框架为异构竞争环境下的联邦学习提供了有效的建模和优化方法，证明了生成AI和博弈理论在解决此类问题中的价值。

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [44] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: 本研究应用监督机器学习算法，基于用户评论中的文本和数值属性预测咖啡评分，发现集成方法（Extra Trees、Random Forest、XGBoost）和多层感知器在性能上优于简单分类器。


<details>
  <summary>Details</summary>
Motivation: 探索数据驱动方法来补充传统咖啡品鉴的专业评估，通过机器学习预测咖啡质量评分。

Method: 使用TF-IDF进行文本特征提取，SelectKBest进行特征选择，训练六种模型（决策树、K近邻、多层感知器、随机森林、Extra Trees、XGBoost）并优化超参数。

Result: 集成方法和多层感知器在F1分数、G-mean和AUC等评估指标上表现优于简单分类器。

Conclusion: 严格的特征选择和超参数调优对于构建稳健的感官产品评估预测系统至关重要，为传统专业咖啡品鉴提供了数据驱动的补充方法。

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [45] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: NurseSchedRL是一个基于强化学习的护士-患者分配框架，通过结构化状态编码、约束动作掩码和注意力机制，在考虑技能异质性、患者病情、人员疲劳和护理连续性等多重约束下，优化护士排班效率。


<details>
  <summary>Details</summary>
Motivation: 医疗系统面临有限护理资源的高效分配压力，传统优化和启发式排班方法难以处理动态多约束环境。

Method: 使用近端策略优化（PPO）算法，结合可行性掩码确保分配符合现实约束，通过注意力机制表示技能、疲劳和地理上下文，动态适应患者到达和护士可用性变化。

Result: 在真实护士和患者数据的模拟中，NurseSchedRL相比基线启发式和无约束RL方法，实现了更高的排班效率、更好的技能与患者需求匹配度以及更低的疲劳度。

Conclusion: 强化学习在复杂高风险医疗人力管理决策支持中具有巨大潜力。

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [46] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: 该论文评估了联邦学习在电动汽车充电站异常检测中的性能，特别是在系统异构性和非IID数据条件下的表现。研究发现FedAvgM在异构环境下优于FedAvg，能够在不显著损失性能的情况下实现隐私保护的EVCS安全。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车基础设施的快速扩张，保护基于物联网的充电站免受网络威胁变得至关重要。集中式入侵检测系统因涉及敏感网络和用户数据而存在隐私问题，联邦学习成为有前景的替代方案。然而，当前基于FL的IDS评估忽视了系统异构性和非IID数据等实际挑战。

Method: 使用FedAvg和FedAvgM这两种广泛研究的优化方法，在系统异构性和数据异构性条件下评估联邦学习在EV充电站异常检测中的性能。实验考虑了IID和非IID数据设置。

Result: 在IID设置下，FedAvg使用相同神经网络实现了优于集中式模型的性能。但在非IID数据和系统异构性条件下性能下降。FedAvgM在异构设置中始终优于FedAvg，表现出更好的收敛性和更高的异常检测准确率。

Conclusion: 联邦学习能够处理物联网EVCS中的异构性而不会显著损失性能，FedAvgM是构建鲁棒、隐私保护的EVCS安全的有前景解决方案。

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [47] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: 本文提出Safe-SAIL框架，利用稀疏自编码器(SAEs)来解释大语言模型中的安全相关特征，以提升对模型安全行为的机制理解。


<details>
  <summary>Details</summary>
Motivation: 现有安全研究主要关注评估LLM输出或特定安全任务，无法应对更广泛的未定义风险。SAEs虽然有助于解释模型行为，但之前的研究未能将特征与细粒度的安全概念关联，无法充分解决安全关键行为。

Method: 提出Safe-SAIL框架，系统识别具有最佳概念特定可解释性的SAE，解释安全相关神经元，并引入高效策略来扩展解释过程。

Result: 将发布包含SAE检查点和人类可读神经元解释的完整工具包，支持对安全风险的实证分析。

Conclusion: 该框架能够促进LLM安全研究，通过提取丰富多样的安全相关特征来有效捕捉高风险行为。

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [48] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: 提出一种基于Gauss-Hermite求积的方法，用于解耦机器学习代理模型中的认知不确定性和偶然不确定性，从而提高可靠性分析的准确性


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型在物理可靠性分析中应用广泛，但其模型近似误差引入的认知不确定性会与模型输入的偶然不确定性耦合，影响可靠性预测的准确性

Method: 使用Gauss-Hermite求积方法解耦嵌套不确定性，通过一阶和二阶可靠性方法评估偶然不确定性下的条件失效概率，并在认知不确定性实现上进行积分

Result: 三个示例表明该方法在保持计算效率的同时，比忽略模型不确定性的传统方法产生更可信的预测结果

Conclusion: 所提出的方法能够有效处理机器学习代理模型中的不确定性耦合问题，为可靠性分析提供更准确的预测

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [49] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: 本文提出了一种结合STL时间序列分解和GRU神经网络的模型，用于预测地铁换乘客流量，相比传统方法显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 在地铁智能交通系统中，准确的换乘客流量预测是优化运营计划、提高运输效率的关键环节。需要改进现有预测理论，为智能运营决策提供更可靠支持。

Method: 创新性地提出STL-GRU组合预测模型：首先使用Keras构建GRU模型；预处理地铁刷卡数据，基于图的深度优先搜索算法识别乘客出行路径；采用STL算法将换乘客流时间序列分解为趋势、周期和残差分量；使用3σ原则处理异常值；最后完成预测。

Result: 以某地铁站换乘客流数据为样本验证，结果显示STL-GRU模型在工作日（除周五）、周五和休息日的预测精度均显著优于LSTM、GRU和STL-LSTM模型，MAPE分别降低了至少2.3、1.36和6.42个百分点。

Conclusion: STL-GRU组合预测模型能够有效提高地铁换乘客流预测的准确性，为地铁智能运营决策提供了可靠的技术支持。

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [50] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: 研究表明，基于Transformer的机器学习应用在解决物理问题时，其权重矩阵呈现随机特征，与物理问题的数学结构没有直接可识别联系，这表明机器学习与科学方法可能是两种不同但互补的知识获取路径。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习模型（特别是Transformer）在解决物理问题时的内部工作机制，以及这些模型是否能够提供与科学方法相媲美的可解释性知识。

Method: 分析Transformer模型在解决两个代表性物理应用时的权重矩阵特征，并与物理问题的数学结构进行对比。

Result: 发现权重矩阵具有随机性特征，与物理问题的结构和数学原理没有直接对应关系，尽管Transformer的操作可能与广义路径积分技术存在相似性。

Conclusion: 机器学习和科学方法可能是互补的知识获取途径，但纯粹的参数可解释性可能难以实现，需要警惕缺乏洞察力的知识获取风险。

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [51] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: MoE-CL是一个参数高效的对抗性混合专家框架，用于工业规模的大语言模型持续指令调优，通过双专家设计解决灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，大语言模型需要持续学习以适应动态变化的任务，但现有持续学习方法存在灾难性遗忘问题，即训练新任务会降低对先前任务的性能。

Method: 采用双LoRA专家设计：专用专家保留任务特定知识，共享专家实现跨任务迁移；集成任务感知判别器进行对抗学习，确保共享专家只传递任务相关信息。

Result: 在MTL5基准和腾讯3基准上的实验验证了有效性，在腾讯视频平台的内容合规审查A/B测试中，手动审核成本降低了15.3%。

Conclusion: MoE-CL适用于需要持续适应和稳定迁移的大规模工业部署场景。

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [52] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: 本文提出了一种加权梯度跟踪的分布式隐私保护算法，解决了梯度跟踪技术中存在的隐私泄露风险，通过衰减权重因子消除隐私泄露，并在时变异构步长下证明了算法的精确收敛性。


<details>
  <summary>Details</summary>
Motivation: 梯度跟踪技术虽然能提高分布式优化的收敛速度，但存在固有的隐私泄露风险，攻击者可能通过梯度信息获取代理的私有数据。本文旨在解决这一隐私安全问题。

Method: 提出加权梯度跟踪分布式隐私保护算法，使用衰减权重因子来消除梯度跟踪中的隐私泄露风险，并在时变异构步长条件下分析算法收敛性。

Result: 理论证明该算法在温和假设下能精确收敛到最优解，数值模拟通过分布式估计问题和卷积神经网络训练验证了算法的有效性。

Conclusion: 所提出的加权梯度跟踪算法成功解决了梯度跟踪中的隐私泄露问题，为隐私保护的分布式优化提供了有效解决方案。

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [53] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: 该论文提出了一种静态-动态图融合网络(SDGF)，通过双路径图结构学习方法捕捉多尺度时间序列间相关性，解决了现有方法难以建模跨时间尺度的复杂动态依赖关系的问题。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测中，序列间的相关性对于准确性至关重要，但这些关系在不同时间尺度上表现出复杂的动态特性。现有方法在建模多尺度依赖关系方面存在局限，难以捕捉其复杂且演化的本质。

Method: SDGF模型采用双路径图结构学习方法：使用基于先验知识的静态图来锚定长期稳定依赖关系，同时利用多级小波分解提取多尺度特征构建自适应学习的动态图来捕捉不同尺度的关联。设计了注意力门控模块智能融合这两种互补信息源，并使用多核扩张卷积网络加深对时间模式的理解。

Result: 在多个广泛使用的真实世界基准数据集上的综合实验证明了所提出模型的有效性。

Conclusion: SDGF模型通过静态-动态图融合机制成功解决了多元时间序列预测中多尺度依赖关系建模的挑战，为复杂动态相关性的捕捉提供了有效解决方案。

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [54] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: 本文提出了一个大规模数据集，系统分析了大语言模型（LLMs）的结构配置与性能之间的关系，通过数据挖掘和机制可解释性技术验证了结构选择对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在规模和能力上快速增长，但关于结构配置如何影响性能的系统性、数据驱动研究仍然稀缺。本文旨在填补这一空白。

Method: 构建包含多样化开源LLM结构及其在多个基准测试中性能的大规模数据集，采用系统性的数据挖掘分析方法，并结合机制可解释性技术验证发现。

Result: 研究量化了结构配置与性能之间的关系，分析了不同结构选择在各种基准测试中的影响，并通过可解释性技术进一步证实了这些发现。

Conclusion: 这项工作为LLM优化提供了数据驱动的见解，旨在指导未来模型的针对性开发和应用，相关数据集将在HuggingFace平台发布。

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [55] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: 提出了LoRALib统一基准，标准化了40个下游任务的数据集和超参数，生成了680个LoRA模块，对3种代表性LoRA-MoE方法进行了大规模实验评估。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA-MoE方法在模型、数据集、超参数和评估方法上缺乏统一标准，难以进行公平比较。

Method: 构建LoRALib基准，标准化40个任务的数据集，使用相同超参数在17种模型架构上生成680个LoRA模块，基于OpenCompass测试工具对3种LoRA-MoE方法和不同LoRA选择机制进行大规模实验。

Result: 实验表明LoRAMoE方法表现最佳，优先选择与目标任务相关的LoRA可以进一步提升MoE性能。

Conclusion: 这些发现将为未来工作提供启发，相关数据集和LoRA库已开源。

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [56] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: RIPLM算法通过利用排名基准和分布基准的结构等价性，直接在排名诱导的Plackett-Luce参数化中更新，确保算法在每一轮都保持在排名诱导分布类中。


<details>
  <summary>Details</summary>
Motivation: 现有方法在专家身份上操作，无法保持与排名基准的等价性。需要一种既忠实于排名又能在休眠专家设置中自适应方差的算法。

Method: RIPLM算法在排名诱导的Plackett-Luce参数化中直接更新，利用排名基准和分布基准的结构等价性。

Result: RIPLM是第一个在休眠专家设置中既忠实于排名又自适应方差的算法。

Conclusion: RIPLM通过直接在排名诱导参数化中更新，成功保持了与排名基准的等价性，为休眠专家问题提供了新的解决方案。

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [57] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: 本文比较了基于规则的分类器FOLD-SE与FOLD-R++在二分类任务中的表现，以及FOLD-SE与XGBoost在多分类任务中的性能。研究发现FOLD-SE在保持可解释性的同时，性能损失很小，能够有效平衡准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着对机器学习模型在准确性、效率和可解释性方面平衡需求的增长，传统模型存在准确性与可解释性之间的权衡。本研究旨在探索基于规则的分类器如何弥合这一差距。

Method: 使用分类数据集，以准确率、F1分数和处理时间作为主要性能指标，比较FOLD-SE与FOLD-R++在二分类任务中的表现，以及FOLD-SE与XGBoost在多分类任务中的性能。

Result: 结果显示FOLD-SE在二分类中优于FOLD-R++，提供更少的规则但仅损失少量准确性和处理效率；在多分类中，FOLD-SE比XGBoost更精确且高效，同时生成可理解的规则集。

Conclusion: FOLD-SE是二分类和多分类任务的更好选择，基于规则的方法能够弥合可解释性与性能之间的差距，成为黑盒模型的可行替代方案。

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [58] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: 本研究开发了一个结合机器学习预测模型和基因无关通路映射的新框架，用于识别2型糖尿病高风险个体并发现潜在治疗靶点。该框架在Pima印第安人数据集上达到78.43%的预测准确率，并通过通路映射识别了胰岛素信号、AMPK和PPAR等关键通路，提出了针对高风险人群的精准治疗策略。


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病等代谢性疾病对遗传易感人群（如Pima印第安人）造成严重影响，需要开发能够早期识别高风险个体并提供机制性见解的精准医疗方法。

Method: 使用逻辑回归和t检验识别T2DM关键预测因子，结合主成分分析(PCA)构建预测模型，开发基因无关通路映射策略将预测因子与关键信号通路（胰岛素信号、AMPK、PPAR通路）相关联。

Result: 模型整体准确率达到78.43%，成功识别了与T2DM相关的关键预测因子，并通过通路映射揭示了潜在的机制联系，提出了包括GLP-1/GIP受体激动剂、AMPK激活剂、SIRT1调节剂等治疗策略。

Conclusion: 该框架为代谢性疾病的精准医疗提供了可解释和可扩展的解决方案，实现了早期检测和靶向干预的结合，为高风险人群的个性化治疗提供了新途径。

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [59] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: 本文提出AdaSTI，一种基于条件扩散模型的新型时空数据插补方法，通过双向S4模型进行预插补，并设计噪声感知时空网络来捕捉不同扩散步骤中的变化依赖关系，在三个真实数据集上实现了高达46.4%的插补误差降低。


<details>
  <summary>Details</summary>
Motivation: 时空数据常因传感器故障等原因存在缺失值。现有扩散模型方法在提取时空依赖关系作为条件信息时存在误差累积问题，且忽略了不同噪声水平下依赖关系的变化性。

Method: 提出AdaSTI框架，包含：1）基于双向S4模型的BiS4PI网络进行预插补；2）时空条件化器（STC）网络提取条件信息；3）噪声感知时空（NAST）网络通过门控注意力机制捕捉不同扩散步骤的变异性依赖。

Result: 在三个真实世界数据集上的广泛实验表明，AdaSTI在所有设置下均优于现有方法，插补误差最高降低46.4%。

Conclusion: AdaSTI通过自适应地处理时空依赖关系，在扩散模型的每个步骤中有效利用条件信息，显著提升了时空数据插补性能。

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [60] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: 本研究提出了一个多标签分类框架，用于预测ICU患者的护理升级触发因素（CETs），包括呼吸衰竭、血流动力学不稳定、肾功能损害和神经功能恶化，使用ICU入院前24小时的数据。


<details>
  <summary>Details</summary>
Motivation: 传统的早期预警系统（如SOFA或MEWS）局限于单一结果预测，无法捕捉临床恶化的多维度特征。ICU患者常出现复杂重叠的生理恶化迹象，需要及时升级护理。

Method: 使用MIMIC-IV数据库，基于规则定义CETs（如血氧饱和度低于90%、平均动脉压低于65 mmHg等）。从ICU入院前24小时提取特征（生命体征汇总、实验室值、人口统计学数据），在85,242例ICU住院数据上训练和评估多个分类模型。

Result: XGBoost表现最佳，F1分数分别为：呼吸0.66、血流动力学0.72、肾功能0.76、神经系统0.62。特征分析显示呼吸频率、血压和肌酐等临床相关参数是最有影响力的预测因子。

Conclusion: 该框架展示了早期、可解释的临床警报的实际潜力，无需复杂的时序建模或自然语言处理。

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [61] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: ConceptFlow是一个基于概念的可解释性框架，通过追踪概念在CNN各层中的出现和演化来模拟模型的内部"思考路径"。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN概念可解释性方法大多忽视了单个过滤器的语义角色以及概念在层间的动态传播，无法全面解释模型的内部推理过程。

Method: ConceptFlow包含两个关键组件：(i)概念注意力，将每个过滤器与相关高级概念关联实现局部语义解释；(ii)概念路径，通过概念转移矩阵量化概念在过滤器间的传播和转换。

Result: 实验结果表明ConceptFlow能够产生语义上有意义的模型推理洞察，验证了概念注意力和概念路径在解释决策行为方面的有效性。

Conclusion: 通过建模层次化的概念路径，ConceptFlow提供了对CNN内部逻辑的更深层次理解，支持生成更忠实且与人类认知对齐的解释。

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [62] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: 提出了一种基于稀疏表示的训练效率框架STS，通过视觉令牌压缩器和层动态跳过器来减少多模态大语言模型的训练计算开销。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型训练效率低下，主要由于多模态数据引入的长输入序列和层间计算利用率低的问题。

Method: STS框架包含两个关键组件：视觉令牌压缩器（减少视觉令牌信息负载）和层动态跳过器（在前向和后向传播中动态跳过语言模型的不必要层）。

Result: 该方法在多个基准测试中进行了广泛评估，证明了其有效性和效率。

Conclusion: 该稀疏训练方案适用于多种MLLM架构，能够显著提升训练效率。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [63] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperNAS提出了一种新的神经预测器范式，通过全局编码方案和共享超网络来增强架构表示学习，在少量样本情况下实现最先进的NAS性能。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索（NAS）中的性能评估耗时严重阻碍进展，现有神经预测器由于难以捕捉架构间复杂关系而泛化能力差。

Method: HyperNAS包含全局编码方案（捕获宏观结构信息）和共享超网络（增强架构间模式研究），并采用动态自适应多任务损失确保训练稳定性。

Result: 在五个代表性搜索空间（包括ViTs）上的实验显示，HyperNAS在少量样本场景下优势明显，在CIFAR-10上达到97.60% top-1准确率，ImageNet上达到82.4% top-1准确率，样本使用量减少至少5倍。

Conclusion: HyperNAS通过改进的架构表示学习方法，显著提升了神经预测器的性能，特别是在数据稀缺场景下表现出色。

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [64] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: WLFM是一个用于测井解释的基础模型，通过多阶段预训练在1200口井的多曲线测井数据上，在孔隙度估计和岩性分类任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 测井解释面临工具响应异质性、噪声信号和标签有限等挑战，需要开发能够处理这些问题的可扩展AI方法。

Method: 三阶段方法：将测井片段标记化为地质标记；使用掩码标记建模和地层感知对比学习进行自监督预训练；通过少样本微调进行多任务适应。

Result: WLFM在孔隙度估计上达到0.0041 MSE，岩性分类准确率74.13%；微调后分别提升至0.0038 MSE和78.10%准确率。

Conclusion: WLFM作为地质AI的可扩展、可解释和可迁移骨干，为测井、地震和文本数据的多模态集成奠定了基础。

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [65] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: ApexAmphion是一个深度学习框架，通过结合64亿参数蛋白质语言模型和强化学习，用于从头设计抗生素肽。该方法在体外评估中显示出100%的成功率和纳米级MIC值。


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性（AMR）预计到205年每年导致1000万人死亡，迫切需要新的抗生素。现有抗生素开发方法效率低下，需要更快速、可扩展的设计平台。

Method: 使用64亿参数蛋白质语言模型，通过强化学习进行优化。模型先在精选肽数据上微调以捕获抗菌序列规律，然后用近端策略优化结合MIC分类器预测和可微分物理化学目标的复合奖励进行优化。

Result: 体外评估100个设计肽显示所有候选物都具有低MIC值（部分达到纳摩尔范围），100%命中率。99/100化合物对至少两种临床相关细菌表现出广谱抗菌活性。主要作用机制是通过靶向细胞质膜杀死细菌。

Conclusion: 该方法通过将生成、评分和多目标优化与深度强化学习统一在单一流程中，能够快速产生多样化、强效的候选物，为肽抗生素提供了可扩展的路径，并可在数小时内迭代优化效力和可开发性。

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [66] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5是一个8B参数的多模态大语言模型，通过架构、数据和训练方法的改进实现了高效性和强性能，在多项基准测试中超越了GPT-4o-latest和Qwen2.5-VL 72B等更大模型。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型训练和推理效率低下的问题，使其更加易于访问和扩展。

Method: 采用统一的3D-Resampler模型架构实现图像和视频的高效编码，使用统一学习范式处理文档知识和文本识别，以及混合强化学习策略支持长短推理模式。

Result: 在OpenCompass评估中超越GPT-4o-latest和Qwen2.5-VL 72B等模型，在VideoMME基准上以仅46.7%的GPU内存和8.7%的推理时间达到30B以下模型的最优性能。

Conclusion: MiniCPM-V 4.5证明了在保持强性能的同时显著提升效率的可行性，为多模态大语言模型的高效发展提供了新方向。

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [67] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: 本文提出并比较了9种训练方法，用于优化神经网络中参数化线性B样条激活函数的形状，相比传统ReLU模型，在FNN中实现了高达94%的误差率降低，在CNN中实现了51%的误差率降低。


<details>
  <summary>Details</summary>
Motivation: 传统的激活函数（如ReLU、tanh、sigmoid）是静态选择的，通过优化激活函数的形状可以训练出更参数高效和准确的模型。

Method: 使用参数化线性B样条激活函数，并提出了9种不同的训练方法来探索神经网络中的双重优化动态。

Result: 实验结果显示，与基于ReLU的传统模型相比，该方法在FNN中实现了高达94%的误差率降低，在CNN中实现了51%的误差率降低。

Conclusion: 通过优化激活函数形状可以显著提高模型性能，但需要付出额外的开发、训练复杂性和模型延迟的代价。

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [68] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: 本文提出了一种混合强化学习方法，用于优化卡车和无人机的最后一公里配送系统，在电池管理约束下实现了比传统方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 研究在明确电池管理约束下的最后一公里配送问题，其中无人机飞行速度为卡车两倍，每次飞行需满足续航预算，且每次配送后需在卡车上充电。

Method: 开发了混合强化学习求解器，结合ALNS卡车路径规划（使用2/3-opt和Or-opt）和指针/注意力策略来调度无人机飞行任务，包括严格的可行性检查和精确时间线模拟。

Result: 在N=50、E=0.7、R=0.1的欧几里得实例上，平均完工时间为5.203±0.093，比ALNS方法提升2.73%，与NN方法相差仅0.10%。

Conclusion: 学习的调度器能平衡卡车等待时间以最小化总完工时间，在相同实例上从不逊于ALNS，并在三分之二的种子上与NN持平或更优。

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [69] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: 提出DSFT策略，通过调整掩码策略和损失函数，提升扩散大语言模型在数学和逻辑任务上的性能


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在生成方面有优势，但在学习数值敏感的数学和顺序敏感的逻辑任务时面临挑战，现有训练方法缺乏对数学和逻辑模式的全面理解

Method: DSFT（Diffusion SFT）策略，通过调整掩码策略和损失函数来引导模型理解数学和逻辑模式，可与预训练、强化学习等方法灵活结合

Result: 在LLaDA和Dream系列模型上验证，小规模数据上数学问题提升5-10%，逻辑问题提升约2%

Conclusion: 这种掩码方法为未来学习特定模式提供了思路，可轻松高效地与其他训练方法结合并应用于各种扩散大语言模型

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [70] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: MobiGPT是一个用于移动数据预测的基础模型，能够统一预测基站流量、用户应用使用和信道质量三种数据类型，通过软提示学习和时间掩码机制实现多任务预测，在真实数据集上表现出优异的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前移动数据预测方法需要为不同数据类型定制专门模型，增加了大规模异构网络中的复杂性和部署成本，需要一种统一的基础模型来简化预测流程。

Method: 提出MobiGPT基础模型，采用软提示学习方法帮助模型理解不同数据类型的特征，引入时间掩码机制指导模型完成短期预测、长期预测和分布生成三种预测任务。

Result: 在包含10万+样本的真实数据集上，MobiGPT相比现有模型预测准确率分别提升27.37%、20.08%和7.27%，在未见场景下的零样本/少样本性能提升超过21.51%。

Conclusion: MobiGPT作为移动数据预测的基础模型，具有强大的泛化能力和迁移能力，能够有效支持多样化的优化场景，为未来移动网络资源管理提供高效解决方案。

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [71] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: PiMoE提出了一种新的训练和推理架构，通过在神经网络中内在地整合计算能力，而不是依赖外部工具调用，实现了计算与推理的高效结合。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型无法将高精度数值计算作为内在能力整合，而多智能体方法存在通信开销大、多模态能力效率低和可扩展性有限的问题。

Method: PiMoE采用物理隔离的专家混合架构，分别训练专家、文本到计算模块和路由器，在推理时路由器在token级别指导计算和推理，实现单链思维内的迭代交替。

Result: 在推理-计算任务上，PiMoE不仅比直接微调LLM获得更高准确率，而且在响应延迟、token使用和GPU能耗方面相比主流多智能体方法有显著改善。

Conclusion: PiMoE为下一代科学或工业智能系统提供了一个高效、可解释和可扩展的范式。

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [72] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: FedIA框架通过投影优先策略解决联邦图学习中的领域偏移问题，使用重要性感知的两阶段管道来降噪客户端更新，实现更稳定收敛和更高准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习在领域偏移下（如Twitch Gamers和多语言Wikipedia网络）会导致客户端模型表示不兼容，使简单聚合变得不稳定和低效。研究发现问题根源是噪声梯度信号而非权重方案。

Method: 提出FedIA框架，采用投影优先策略：1）服务器端top-ρ掩码保留约5%最信息丰富的坐标；2）轻量级影响正则化动量权重抑制异常客户端。该方法不增加上行流量且服务器内存开销可忽略。

Result: 在同质（Twitch Gamers）和异质（Wikipedia）图上，FedIA比9个强基线方法实现更平滑稳定的收敛和更高的最终准确率。收敛分析显示动态投影保持最优收敛速率。

Conclusion: FedIA通过重要性感知的投影优先策略有效解决了联邦图学习中的领域偏移问题，提供了一种高效可部署的解决方案。

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [73] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: SBVR是一种新型的大语言模型量化方法，通过高斯分布的码本表示和自定义CUDA内核，在4位量化下实现2.21-3.04倍的端到端加速，同时保持最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法存在局限性：RTN方法无法处理LLM权重的高斯分布特性，码本方法虽然能解决分布问题但存在内存访问模式不佳导致推理速度下降的问题。

Method: 提出SBVR方法，将权重值映射到非均匀表示点，其分布遵循LLM权重的实际高斯分布；设计自定义CUDA内核，支持直接在SBVR格式下进行矩阵向量乘法而无需解压缩。

Result: 在各种模型上的评估显示，SBVR在4位量化下实现了最先进的困惑度和准确率基准性能，同时相比原生FP16模型实现了2.21-3.04倍的端到端token生成加速。

Conclusion: SBVR方法成功解决了现有量化方法的局限性，在保持高精度的同时显著提升了推理速度，为大语言模型的高效部署提供了有效的解决方案。

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [74] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: 该论文提出了一个大规模基准测试，用于评估大语言模型的地理空间路线认知能力，发现LLMs在路线反转任务中存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过自然语言理解地理空间信息，但大语言模型的地理空间认知能力尚未得到充分探索，现有研究存在评估指标不可量化、数据集有限和研究层次不清晰等问题。

Method: 创建了包含全球12个大都市36000条路线的大规模评估数据集，开发了PathBuilder工具在自然语言指令和导航路线之间进行转换，并提出了新的评估框架和指标来评估11个SOTA LLMs的路线反转能力。

Result: 基准测试显示LLMs在路线反转任务中存在明显限制：大多数反转路线既没有返回起点，也不接近最优路线。LLMs还面临路线生成鲁棒性低和对错误答案置信度高的问题。

Conclusion: LLMs在地理空间路线认知方面存在显著局限性，需要进一步改进模型的地理空间理解能力。

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [75] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: 本文提出了多模态思维链（MCoT）框架，用于解决对话式导航中的自我中心到异我中心的空间方向推理问题，在传统中文环境中实现了高精度的方向识别。


<details>
  <summary>Details</summary>
Motivation: 解决室内或复杂设施中GPS信号弱、详细地图不可用时，对话代理需要将自我中心表述转换为异我中心方向的关键挑战，特别是在非英语和ASR转录场景下的空间推理问题。

Method: 提出多模态思维链框架，整合ASR转录语音和地标坐标，通过三步推理过程：提取空间关系、坐标映射到绝对方向、推断用户朝向，并在Taiwan-LLM-13B-v2.0-Chat模型上采用课程学习策略。

Result: MCoT在干净转录本上达到100%方向准确率，ASR转录本上达到98.1%，显著优于单模态和非结构化基线，并在噪声对话条件下表现出鲁棒性。

Conclusion: 结构化MCoT空间推理为可解释和资源高效的具身导航提供了一条有前景的路径，展示了在多语言、噪声环境下的强大适应性。

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [76] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: 本文提出变分任务向量组合方法，将组合系数作为隐变量，在贝叶斯推理框架下进行估计，实现样本特定的任务向量组合。


<details>
  <summary>Details</summary>
Motivation: 传统任务向量组合方法在任务级别操作，而观察到任务向量存在结构冗余，需要更精细的样本特定组合机制。

Method: 引入Spike-and-Slab先验促进稀疏性，开发门控采样机制基于不确定性和重要性过滤组合系数，构建可控后验分布。

Result: 实验结果表明该方法在所有数据集上一致优于现有方法，通过选择性利用任务向量中最可靠和信息丰富的组件实现更好性能。

Conclusion: 该方法为高效有效的任务向量组合建立了新标准，具有实际应用价值。

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [77] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: MolPILE是一个包含2.22亿个化合物的大规模、多样化、严格筛选的数据集，旨在解决分子表示学习中现有数据集不足的问题，为化学信息学提供类似ImageNet的标准化资源。


<details>
  <summary>Details</summary>
Motivation: 现有小分子数据集在规模、多样性和质量上的局限性阻碍了分子表示学习的效果，需要构建更高质量的大规模数据集来提升基础模型的泛化能力。

Method: 从6个大型数据库通过自动化筛选流程构建MolPILE数据集，包含2.22亿个化合物，并进行全面的数据集分析。

Result: 在MolPILE上重新训练现有模型能够显著提升泛化性能，证明了该数据集的有效性。

Conclusion: MolPILE为分子化学领域提供了标准化的训练资源，解决了该领域对高质量大规模数据集的迫切需求。

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [78] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: FastMTP是一种通过调整多令牌预测训练与推理模式对齐的方法，显著提升推测解码性能，实现LLM推理加速。


<details>
  <summary>Details</summary>
Motivation: 自回归生成的顺序性造成了吞吐量瓶颈，限制了LLM的实际部署。多令牌预测在训练效率和性能方面表现出色，但其推理加速潜力尚未充分探索。

Method: 通过位置共享权重在自蒸馏数据上微调单个MTP头，使其能够捕捉连续未来令牌的依赖关系；集成语言感知动态词汇压缩以减少计算开销。

Result: 在七个不同基准测试中，FastMTP相比标准下一个令牌预测实现了2.03倍的平均加速，输出质量无损，性能优于普通MTP 82%。

Conclusion: FastMTP仅需轻量级训练，可无缝集成到现有推理框架中，为加速LLM推理提供了实用且快速部署的解决方案。

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [79] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: 本文提出了一种新的多工作者选择算法M-DSL，用于解决分布式群学习中的非独立同分布数据挑战，通过引入非i.i.d.度度量来量化数据异质性并指导工作者选择。


<details>
  <summary>Details</summary>
Motivation: 分布式群学习面临非i.i.d.数据的挑战，这会降低学习性能并使训练行为发散。目前缺乏关于数据异质性如何影响模型训练准确性的理论指导。

Method: 首先研究数据异质性，测量非i.i.d.数据集在DSL框架下的影响。然后提出M-DSL算法，引入新的非i.i.d.度度量来制定本地数据集间的统计差异，建立数据异质性度量和DSL性能评估之间的联系。

Result: 通过在不同异构数据集和非i.i.d.数据设置上的广泛实验，数值结果验证了M-DSL在性能改进和网络智能增强方面的优势。

Conclusion: M-DSL算法能够有效处理分布式异构数据，提供了超越基准的性能改进，并为分布式群学习中的数据异质性挑战提供了理论指导。

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [80] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: 该论文提出了GnnXemplar，一种基于认知科学范例理论的全局解释方法，用于解释图神经网络(GNN)的预测决策，解决了现有方法在大规模真实图数据中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN全局解释方法依赖小图中的模式发现，但在大规模真实图数据中失效，因为子图重复罕见、节点属性高维且预测来自复杂的结构-属性交互。需要开发能够处理这些挑战的全局解释方法。

Method: GnnXemplar在GNN嵌入空间中识别代表性节点（范例），并通过从这些节点的邻域推导自然语言规则来解释预测。范例选择被建模为反向k近邻的覆盖最大化问题，使用贪心算法近似求解。使用LLM的自优化提示策略生成可解释规则。

Result: 在多个基准测试上的实验表明，GnnXemplar在保真度、可扩展性和人类可解释性方面显著优于现有方法，用户研究（60名参与者）验证了其有效性。

Conclusion: GnnXemplar提供了一种新颖且有效的GNN全局解释框架，结合了范例理论和LLM技术，能够处理大规模复杂图数据的解释需求。

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [81] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: 提出了GETAD框架，通过整合道路网络拓扑、路段语义和历史出行模式来检测轨迹异常，使用图注意力网络学习道路感知嵌入，并通过Transformer解码器建模序列移动。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹异常检测方法仅考虑轨迹的有限方面，在欧几里得空间中分析轨迹，忽略了底层移动网络（如道路或交通网络）的约束和连通性信息。

Method: GETAD框架使用图注意力网络学习道路感知嵌入，结合基于图的位置编码，采用Transformer解码器建模序列移动，并使用结合自回归预测和监督链接预测的多目标损失函数。

Result: 在真实世界和合成数据集上的实验表明，GETAD相比现有方法取得了持续改进，特别是在检测道路约束环境中的细微异常方面表现突出。

Conclusion: 将图结构和上下文语义整合到轨迹建模中，能够实现更精确和上下文感知的异常检测。

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [82] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文探讨了为什么强化学习预训练算法能够生成支持上下文强化学习的网络参数，并通过理论证明支持这一假设。


<details>
  <summary>Details</summary>
Motivation: 研究上下文强化学习现象背后的机制，解释为什么预训练的参数能够在无需更新的情况下适应新任务。

Method: 通过理论分析，证明当Transformer网络预训练用于策略评估时，预训练损失的全局最小值能够支持上下文时序差分学习。

Result: 理论证明表明，预训练损失的全局最小值确实能够使网络具备上下文强化学习的能力。

Conclusion: 研究为上下文强化学习的有效性提供了理论支持，表明预训练参数的最优解是实现这一现象的关键。

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [83] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: 本文对深度学习优化器进行了全面综述，按时间顺序分析了从随机梯度下降到最新优化器（如Momentum、AdamW、Sophia、Muon等）的发展历程、技术特点和更新规则。


<details>
  <summary>Details</summary>
Motivation: 深度学习优化器对神经网络学习效果至关重要，随着深度学习快速发展，出现了多种不同方法的优化器，需要系统梳理和总结现有研究成果。

Method: 采用文献综述方法，按时间顺序分析各种优化器的更新规则、技术特点、超参数设置及其对优化过程的贡献。

Result: 提供了理解当前优化器发展现状的全面资源，详细介绍了各种优化器的技术细节和特点。

Conclusion: 本文为理解优化器当前状态和识别未来发展方向提供了有价值的参考，同时指出了深度学习模型优化中面临的开放挑战。

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [84] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: 提出了一种新型的信息保留混沌游戏表示方法（R-CGR），解决了传统CGR方法在几何映射过程中丢失序列信息的根本限制，实现了完整的序列恢复。


<details>
  <summary>Details</summary>
Motivation: 传统CGR方法在生物序列分析中存在序列信息丢失的问题，这限制了其在需要精确序列恢复的应用中的使用。

Method: 通过显式路径编码结合有理数算术精度控制，实现了完美的序列重构。该方法存储完整的路径信息，在每一步都保持位置和字符信息。

Result: 在生物序列分类任务中表现出与传统序列方法相竞争的性能，同时提供可解释的几何可视化。

Conclusion: 该方法生成了适合深度学习的特征丰富图像，同时通过显式编码保持完整的序列信息，为可解释的生物信息学分析开辟了新途径。

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [85] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: 提出了KANDI方法，结合Kolmogorov-Arnold网络和扩散策略，用于离线逆强化学习，旨在解决在老年人跌倒风险干预中应用离线强化学习的挑战。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在医疗保健应用中面临奖励函数定义困难和策略与人类行为对齐的挑战，特别是在老年人跌倒风险干预的复杂环境中。

Method: 使用Kolmogorov-Arnold网络学习低跌倒风险老年人（专家）的行为来估计奖励函数，同时在Actor-Critic框架中采用基于扩散的策略进行动作优化。

Result: KANDI在D4RL基准测试中优于现有最先进方法，并在实际的老年人跌倒风险干预临床试验中表现出良好的应用效果。

Conclusion: KANDI为解决医疗保健应用中离线强化学习的关键挑战提供了有效解决方案，特别适用于健康促进干预策略。

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [86] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: MeshODENet结合图神经网络和神经常微分方程，解决了传统数值求解器计算成本高和标准自回归GNN长期预测误差累积的问题，在结构力学问题上实现了更高的长期预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器在复杂物理系统模拟中计算成本过高，而标准图神经网络在长期预测中存在误差累积和不稳定问题，需要开发更有效的替代模型。

Method: 提出MeshODENet框架，将图神经网络的空间推理能力与神经常微分方程的连续时间建模相结合，用于处理非线性大变形结构力学问题。

Result: 在一维和二维弹性体大变形问题上，该方法显著优于基线模型，在长期预测精度和稳定性方面表现优异，同时相比传统求解器实现了显著的计算加速。

Conclusion: MeshODENet为开发数据驱动的替代模型提供了一种强大且通用的方法，可加速复杂结构系统的分析和建模。

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [87] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GluMind是一个基于Transformer的多模态框架，用于持续和长期血糖预测，通过交叉注意力和多尺度注意力机制整合生理和行为信号，并采用知识保留技术防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决血糖预测中多模态信号采样率不同、长期时间依赖关系建模困难以及持续学习中的灾难性遗忘问题。

Method: 设计并行工作的交叉注意力和多尺度注意力机制，整合血糖数据与其他生理行为信号；引入知识保留模块增强模型记忆能力。

Result: 在AIREADI数据集上评估，GluMind在RMSE和MAE指标上分别比现有最优模型提升约15%和9%，表现出稳定的性能和适应性。

Conclusion: GluMind框架在持续血糖预测任务中表现出色，有效解决了多模态数据融合和长期依赖建模的挑战。

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [88] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: 本文探索了扩散模型的离散时间过程变体，包括加性高斯噪声、乘性高斯噪声、模糊噪声及其混合形式，结果表明离散时间过程在语音质量上与连续对应物相当，但具有更高效和一致的训练推理模式。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型将语音生成视为连续时间过程，存在训练限制（通常限于加性高斯噪声）和训练/采样条件不匹配的问题。离散时间过程可以克服这些限制，减少推理步骤，并保持训练/推理条件的一致性。

Method: 提出并探索了多种扩散式离散时间过程变体：加性高斯噪声、乘性高斯噪声、模糊噪声以及模糊与高斯噪声的混合形式。这些方法在离散时间框架下进行建模。

Result: 实验结果表明，离散时间过程在主观和客观语音质量评估上与广泛流行的连续对应物相当，同时提供了更高效和一致的训练与推理方案。

Conclusion: 离散时间扩散过程是连续时间扩散模型的有效替代方案，能够提供相当的语音质量，同时解决了连续模型在训练限制和条件不匹配方面的问题。

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [89] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: NVQ是一种新的向量压缩技术，通过非线性非均匀向量量化方法，在高保真度下实现计算和空间效率的提升


<details>
  <summary>Details</summary>
Motivation: 高维嵌入向量的大尺寸给现代向量搜索技术带来问题：从内存/存储检索大向量成本高，存储占用昂贵

Method: 使用新颖的简约且计算高效的非线性方法构建非均匀向量量化器，关键是为每个索引向量单独学习量化器

Result: 实验结果显示NVQ在最小计算成本下相比现有技术表现出更高的准确率

Conclusion: NVQ是一种在高保真度下具有改进准确性和最小计算成本的向量压缩技术

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [90] [SimpleFold: Folding Proteins is Simpler than You Think](https://arxiv.org/abs/2509.18480)
*Yuyang Wang,Jiarui Lu,Navdeep Jaitly,Josh Susskind,Miguel Angel Bautista*

Main category: cs.LG

TL;DR: SimpleFold是首个基于流匹配的蛋白质折叠模型，仅使用通用Transformer模块，挑战了传统依赖复杂领域特定架构的设计理念。


<details>
  <summary>Details</summary>
Motivation: 质疑蛋白质折叠模型中复杂领域特定架构的必要性，探索是否可以使用通用生成模型架构实现竞争性性能。

Method: 使用标准Transformer模块配合自适应层，通过生成流匹配目标和结构项进行训练，在900万蒸馏蛋白质结构和实验PDB数据上训练30亿参数模型。

Result: 在标准折叠基准测试中，SimpleFold-3B达到与最先进基线竞争的性能，在集成预测方面表现优异，且在消费级硬件上部署和推理效率高。

Conclusion: SimpleFold挑战了蛋白质折叠对复杂领域特定架构设计的依赖，为未来进展开辟了替代设计空间。

Abstract: Protein folding models have achieved groundbreaking results typically via a
combination of integrating domain knowledge into the architectural blocks and
training pipelines. Nonetheless, given the success of generative models across
different but related problems, it is natural to question whether these
architectural designs are a necessary condition to build performant models. In
this paper, we introduce SimpleFold, the first flow-matching based protein
folding model that solely uses general purpose transformer blocks. Protein
folding models typically employ computationally expensive modules involving
triangular updates, explicit pair representations or multiple training
objectives curated for this specific domain. Instead, SimpleFold employs
standard transformer blocks with adaptive layers and is trained via a
generative flow-matching objective with an additional structural term. We scale
SimpleFold to 3B parameters and train it on approximately 9M distilled protein
structures together with experimental PDB data. On standard folding benchmarks,
SimpleFold-3B achieves competitive performance compared to state-of-the-art
baselines, in addition SimpleFold demonstrates strong performance in ensemble
prediction which is typically difficult for models trained via deterministic
reconstruction objectives. Due to its general-purpose architecture, SimpleFold
shows efficiency in deployment and inference on consumer-level hardware.
SimpleFold challenges the reliance on complex domain-specific architectures
designs in protein folding, opening up an alternative design space for future
progress.

</details>


### [91] [Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints](https://arxiv.org/abs/2509.18483)
*Abhijit Sen,Illya V. Lukin,Kurt Jacobs,Lev Kaplan,Andrii G. Sotnikov,Denys I. Bondar*

Main category: cs.LG

TL;DR: 本文提出了一种基于Kolmogorov Arnold Networks（KANs）和物理信息损失函数的新方法，用于预测量子动力学响应，相比传统神经网络方法显著减少了训练数据需求并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 量子系统在高维希尔伯特空间中的演化使得传统数值方法计算成本高昂，现有神经网络架构需要大量训练数据且存在虚假振荡问题，影响物理可解释性。

Method: 使用Kolmogorov Arnold Networks（KANs）结合物理信息损失函数（强制执行Ehrenfest定理），并引入Chain of KANs架构直接嵌入时间因果关系。

Result: 该方法仅需200个样本（传统Temporal Convolution Networks需要3,700个样本），即5.4%的训练数据量，就能达到更优的准确性。

Conclusion: 物理信息KANs相比传统黑盒模型具有明显优势，在保持数学严谨性和物理一致性的同时大幅降低数据需求。

Abstract: The prediction of quantum dynamical responses lies at the heart of modern
physics. Yet, modeling these time-dependent behaviors remains a formidable
challenge because quantum systems evolve in high-dimensional Hilbert spaces,
often rendering traditional numerical methods computationally prohibitive.
While large language models have achieved remarkable success in sequential
prediction, quantum dynamics presents a fundamentally different challenge:
forecasting the entire temporal evolution of quantum systems rather than merely
the next element in a sequence. Existing neural architectures such as recurrent
and convolutional networks often require vast training datasets and suffer from
spurious oscillations that compromise physical interpretability. In this work,
we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)
augmented with physics-informed loss functions that enforce the Ehrenfest
theorems. Our method achieves superior accuracy with significantly less
training data: it requires only 5.4 percent of the samples (200) compared to
Temporal Convolution Networks (3,700). We further introduce the Chain of KANs,
a novel architecture that embeds temporal causality directly into the model
design, making it particularly well-suited for time series modeling. Our
results demonstrate that physics-informed KANs offer a compelling advantage
over conventional black-box models, maintaining both mathematical rigor and
physical consistency while dramatically reducing data requirements.

</details>


### [92] [Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models](https://arxiv.org/abs/2509.18499)
*Rachel Chung,Pratyush Nidhi Sharma,Mikko Siponen,Rohit Vadodaria,Luke Smith*

Main category: cs.LG

TL;DR: 本文提出使用混合数据集来增强合成数据集在反洗钱（AML）模型训练中的效用，通过结合公开可用的真实世界特征，既保护隐私又提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 反洗钱是金融机构面临的全球性关键问题，但开发自动化AML模型时缺乏训练数据，因为隐私和保密问题限制了真实数据的访问。合成数据虽然能保护隐私，但纯合成数据集训练AML模型存在挑战。

Method: 提出使用混合数据集方法，将合成数据与公开可用、易于获取的真实世界特征相结合，以增强合成数据集的实用性。

Result: 混合数据集不仅能够保护隐私，还能提高模型效用，为金融机构增强AML系统提供了实用途径。

Conclusion: 混合数据集方法为解决AML模型训练中的数据隐私和效用平衡问题提供了有效的解决方案，具有实际应用价值。

Abstract: Money laundering is a critical global issue for financial institutions.
Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),
can be trained to identify illicit transactions in real time. A major issue for
developing such models is the lack of access to training data due to privacy
and confidentiality concerns. Synthetically generated data that mimics the
statistical properties of real data but preserves privacy and confidentiality
has been proposed as a solution. However, training AML models on purely
synthetic datasets presents its own set of challenges. This article proposes
the use of hybrid datasets to augment the utility of synthetic datasets by
incorporating publicly available, easily accessible, and real-world features.
These additions demonstrate that hybrid datasets not only preserve privacy but
also improve model utility, offering a practical pathway for financial
institutions to enhance AML systems.

</details>


### [93] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: APRIL是一种强化学习训练优化方法，通过主动部分rollout策略解决长尾响应分布导致的GPU空闲问题，提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前RL训练中rollout生成占90%以上运行时间，且长尾响应分布导致GPU空闲和利用率低下，限制了模型的可扩展性。

Method: APRIL在rollout阶段超额配置请求，达到目标响应数量后终止，并将未完成的响应回收用于后续步骤继续处理。

Result: APRIL将rollout吞吐量最多提升44%，加速收敛，并在多个任务上实现最多8%的最终准确率提升。

Conclusion: APRIL统一了系统级和算法级考虑，可提高RL训练效率，且与框架和硬件无关，已集成到slime RL框架中。

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [94] [Reverse-Complement Consistency for DNA Language Models](https://arxiv.org/abs/2509.18529)
*Mingqian Ma*

Main category: cs.LG

TL;DR: 本文提出了反向互补一致性正则化（RCCR）方法，通过惩罚DNA序列与其反向互补序列预测差异，提升DNA语言模型的生物学对称性捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有DNA语言模型经常无法捕捉DNA序列与其反向互补序列的生物学对称性，导致预测不一致，影响模型可靠性。

Method: 引入RCCR正则化目标，在微调过程中直接惩罚模型对序列及其反向互补序列预测的差异，该方法与模型架构无关。

Result: 在三个不同骨干网络和多种基因组任务上验证，RCCR显著提升了反向互补鲁棒性，减少预测翻转和错误，同时保持或提升任务准确率。

Conclusion: RCCR通过将关键生物学先验直接整合到学习过程中，为多样化生物学任务提供了单一、内在鲁棒且计算高效的模型微调方案。

Abstract: A fundamental property of DNA is that the reverse complement (RC) of a
sequence often carries identical biological meaning. However, state-of-the-art
DNA language models frequently fail to capture this symmetry, producing
inconsistent predictions for a sequence and its RC counterpart, which
undermines their reliability. In this work, we introduce Reverse-Complement
Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning
objective that directly penalizes the divergence between a model's prediction
on a sequence and the aligned prediction on its reverse complement. We evaluate
RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,
DNABERT-2) on a wide range of genomic tasks, including sequence classification,
scalar regression, and profile prediction. Our experiments show that RCCR
substantially improves RC robustness by dramatically reducing prediction flips
and errors, all while maintaining or improving task accuracy compared to
baselines such as RC data augmentation and test-time averaging. By integrating
a key biological prior directly into the learning process, RCCR produces a
single, intrinsically robust, and computationally efficient model fine-tuning
recipe for diverse biology tasks.

</details>


### [95] [Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts](https://arxiv.org/abs/2509.18542)
*Qi Wang,Hanyang Peng,Yue Yu*

Main category: cs.LG

TL;DR: 本文提出了Symphony-MoE框架，通过融合多个异构预训练模型的专家来构建更强大的MoE模型，解决了传统upcycling方法专家多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型通过复制单个预训练密集模型的FFN层来构建专家，限制了专家多样性。本文旨在利用多个异构预训练模型来构建更强大的MoE模型。

Method: 提出两阶段框架：1）训练自由阶段：通过层感知融合策略构建共享骨干，并使用基于激活的功能对齐缓解参数错位；2）轻量级路由器训练阶段协调整个架构。

Result: 实验表明该方法成功整合了异构源专家，在多项任务和分布外泛化方面显著超越基线方法。

Conclusion: Symphony-MoE框架有效解决了多源专家融合的挑战，为构建更强大的MoE模型提供了新思路。

Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.

</details>


### [96] [Global Minimizers of Sigmoid Contrastive Loss](https://arxiv.org/abs/2509.18552)
*Kiril Bangachev,Guy Bresler,Iliyas Noman,Yury Polyanskiy*

Main category: cs.LG

TL;DR: 本文从理论上分析了SigLIP和SigLIP2模型中可训练逆温度和偏置参数在sigmoid损失函数下的优势，提出了(m, b_rel)-Constellations概念来解释模型成功的原因。


<details>
  <summary>Details</summary>
Motivation: 对比学习预训练在CLIP和ALIGN等模型中的重要性日益增加，需要从理论上解释SigLIP模型中温度和偏置参数同步训练的优势。

Method: 提出(m, b_rel)-Constellations这一新的组合对象，用于理论分析sigmoid损失函数的行为，并提出了带有显式相对偏置的sigmoid损失重参数化方法。

Result: 理论分析解释了SigLIP在检索任务上的成功、模态间隙的存在以及产生高质量表示所需的必要维度。

Conclusion: 通过理论分析和实验验证，证明了带有显式相对偏置的sigmoid损失重参数化能够改善训练动态。

Abstract: The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.

</details>


### [97] [Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia](https://arxiv.org/abs/2509.18568)
*Niharika Tewari,Nguyen Linh Dan Le,Mujie Liu,Jing Ren,Ziqi Xu,Tabinda Sarwar,Veeky Baths,Feng Xia*

Main category: cs.LG

TL;DR: 这篇论文是关于可解释图神经网络在痴呆症研究中的首次全面综述，涵盖了阿尔茨海默病、帕金森病等多种痴呆亚型的诊断应用，并提出了针对痴呆症任务的可解释性方法分类体系。


<details>
  <summary>Details</summary>
Motivation: 痴呆症具有临床和生物学异质性，诊断和亚型区分极具挑战性。传统图神经网络存在鲁棒性差、数据稀缺和可解释性不足等问题，限制了临床采用。可解释图神经网络能够结合图学习和可解释性，识别疾病相关生物标志物，为临床医生提供透明见解。

Method: 通过系统综述方法，对可解释图神经网络在痴呆症研究中的应用进行全面分析，包括阿尔茨海默病、帕金森病、轻度认知障碍和多疾病诊断等场景。提出了专门针对痴呆症任务的可解释性方法分类体系，并对现有模型进行临床场景比较。

Result: 总结了可解释图神经网络在痴呆症研究中的进展，识别了当前面临的挑战，如泛化能力有限、未充分探索的领域，以及大语言模型在早期检测中的整合机会。

Conclusion: 该综述旨在指导未来研究朝着可信赖、具有临床意义且可扩展的可解释图神经网络在痴呆症研究中的应用方向发展，为解决痴呆症诊断和治疗的临床挑战提供支持。

Abstract: Dementia is a progressive neurodegenerative disorder with multiple
etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal
dementia, and vascular dementia. Its clinical and biological heterogeneity
makes diagnosis and subtype differentiation highly challenging. Graph Neural
Networks (GNNs) have recently shown strong potential in modeling brain
connectivity, but their limited robustness, data scarcity, and lack of
interpretability constrain clinical adoption. Explainable Graph Neural Networks
(XGNNs) have emerged to address these barriers by combining graph-based
learning with interpretability, enabling the identification of disease-relevant
biomarkers, analysis of brain network disruptions, and provision of transparent
insights for clinicians. This paper presents the first comprehensive review
dedicated to XGNNs in dementia research. We examine their applications across
Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and
multi-disease diagnosis. A taxonomy of explainability methods tailored for
dementia-related tasks is introduced, alongside comparisons of existing models
in clinical scenarios. We also highlight challenges such as limited
generalizability, underexplored domains, and the integration of Large Language
Models (LLMs) for early detection. By outlining both progress and open
problems, this review aims to guide future work toward trustworthy, clinically
meaningful, and scalable use of XGNNs in dementia research.

</details>


### [98] [Interaction Topological Transformer for Multiscale Learning in Porous Materials](https://arxiv.org/abs/2509.18573)
*Dong Chen,Jian Liu,Chun-Long Chen,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出了Interaction Topological Transformer (ITT)框架，通过多尺度交互拓扑结构来预测多孔材料的吸附、传输和稳定性性能，实现了数据高效和可迁移的预测。


<details>
  <summary>Details</summary>
Motivation: 多孔材料具有结构多样性，但在预测建模方面面临挑战，主要由于结构-性能关系的多尺度特性以及标记数据稀疏且分布不均的问题。

Method: 使用交互拓扑变换器(ITT)框架，提取多尺度特征（结构、元素、原子和成对元素组织），通过Transformer架构进行跨尺度联合推理，采用两阶段训练策略（自监督预训练+监督微调）。

Result: ITT在60万未标记结构上进行预训练后，在吸附、传输和稳定性性能预测方面达到了最先进的准确性和可迁移性。

Conclusion: 该框架为结构多样性和化学多样性多孔材料的学习引导发现提供了原则性和可扩展的路径。

Abstract: Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.

</details>


### [99] [DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation](https://arxiv.org/abs/2509.18584)
*Mingchun Sun,Rongqiang Zhao,Jie Liu*

Main category: cs.LG

TL;DR: DS-Diffusion是一种数据风格引导的扩散模型，通过风格引导核和分层去噪机制解决现有时间序列生成扩散模型需要重新训练、分布偏差大和推理过程不可解释的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成扩散模型需要为特定条件重新训练整个框架，生成数据与真实数据存在分布偏差，且推理过程不可解释。

Method: 提出基于风格引导核的扩散框架避免重新训练，开发基于时间信息的分层去噪机制(THD)减少分布偏差，生成样本能清晰显示数据来源风格。

Result: 相比ImagenTime等最先进模型，预测分数和判别分数分别降低5.56%和61.55%，分布偏差进一步减小，推理过程更可解释。

Conclusion: DS-Diffusion通过消除重新训练需求，增强了模型对特定条件的灵活性和适应性，同时提高了生成质量和可解释性。

Abstract: Diffusion models are the mainstream approach for time series generation
tasks. However, existing diffusion models for time series generation require
retraining the entire framework to introduce specific conditional guidance.
There also exists a certain degree of distributional bias between the generated
data and the real data, which leads to potential model biases in downstream
tasks. Additionally, the complexity of diffusion models and the latent spaces
leads to an uninterpretable inference process. To address these issues, we
propose the data style-guided diffusion model (DS-Diffusion). In the
DS-Diffusion, a diffusion framework based on style-guided kernels is developed
to avoid retraining for specific conditions. The time-information based
hierarchical denoising mechanism (THD) is developed to reduce the
distributional bias between the generated data and the real data. Furthermore,
the generated samples can clearly indicate the data style from which they
originate. We conduct comprehensive evaluations using multiple public datasets
to validate our approach. Experimental results show that, compared to the
state-of-the-art model such as ImagenTime, the predictive score and the
discriminative score decrease by 5.56% and 61.55%, respectively. The
distributional bias between the generated data and the real data is further
reduced, the inference process is also more interpretable. Moreover, by
eliminating the need to retrain the diffusion model, the flexibility and
adaptability of the model to specific conditions are also enhanced.

</details>


### [100] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: REBACT是一种增强LLM决策能力的新方法，通过在采取行动前增加反思步骤来实现即时错误纠正，在多个交互环境中显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM决策方法存在错误累积问题和缺乏鲁棒的自纠正机制，需要一种能够即时纠正错误、确保行动路径平滑的方法。

Method: 提出'行动前反思'(REBACT)方法，在LLM采取下一个行动之前引入关键的反思步骤，允许基于环境反馈进行即时错误纠正。

Result: 在ALFWorld、WebShop和TextCraft三个交互环境中，REBACT显著优于基线方法：WebShop成功率提升24%达到61%，ALFWorld提升6.72%达到98.51%，TextCraft提升0.5%达到99.5%。

Conclusion: REBACT通过少量修改步骤就能实现性能提升，证明了其计算效率，为LLM决策任务提供了一种有效的自纠正机制。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [101] [Flow marching for a generative PDE foundation model](https://arxiv.org/abs/2509.18611)
*Zituo Chen,Sili Deng*

Main category: cs.LG

TL;DR: 提出了Flow Marching算法，将神经算子学习与流匹配相结合，构建了一个生成式PDE基础模型，通过联合采样噪声水平和物理时间步长来减少长期滚动漂移并实现不确定性感知的集成生成。


<details>
  <summary>Details</summary>
Motivation: 现有的PDE基础模型主要基于确定性Transformer架构，缺乏生成灵活性，无法满足许多科学和工程应用的需求。

Method: 1) Flow Marching算法联合采样噪声水平和物理时间步长；2) P2VAE将物理状态嵌入紧凑潜在空间；3) FMT结合扩散强制方案和潜在时间金字塔，计算效率比完整视频扩散模型高15倍。

Result: 在12个不同PDE家族的约250万条轨迹上训练模型，在未见过的Kolmogorov湍流上进行了少样本适应评估，展示了长期滚动稳定性优于确定性对应模型，并提供了不确定性分层的集成结果。

Conclusion: 生成式PDE基础模型对于实际应用具有重要意义，Flow Marching方法在减少计算成本的同时提高了模型的生成能力和不确定性感知能力。

Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.

</details>


### [102] [HyperAdapt: Simple High-Rank Adaptation](https://arxiv.org/abs/2509.18629)
*Abel Gurung,Joseph Campbell*

Main category: cs.LG

TL;DR: HyperAdapt是一种参数高效微调方法，通过行列缩放技术实现高秩更新，仅需n+m个可训练参数即可适配n×m矩阵，在保持性能的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 基础模型在多样化任务中表现出色，但适应专业化应用通常需要微调，这种方法内存和计算成本高昂。参数高效微调方法通过仅更新少量权重来缓解这一问题。

Method: HyperAdapt通过对预训练权重矩阵应用行列对角矩阵缩放，诱导高秩更新。理论分析建立了更新秩的上界，实验证实该方法能在模型各层一致产生高秩变换。

Result: 在GLUE、算术推理和常识推理基准测试中，使用高达140亿参数的模型进行实验，HyperAdapt在性能上匹配或接近全量微调及最先进的PEFT方法，同时使用的可训练参数数量减少数个数量级。

Conclusion: HyperAdapt提供了一种高效的参数微调方案，在保持模型性能的同时显著降低了计算和内存需求，为大型基础模型的适应性应用提供了实用解决方案。

Abstract: Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.

</details>


### [103] [Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology](https://arxiv.org/abs/2509.18703)
*Jakub Adamczyk*

Main category: cs.LG

TL;DR: 该研究利用图机器学习进行理性农药设计，开发更安全环保的农用化学品，借鉴了药物发现中的计算机模拟方法。


<details>
  <summary>Details</summary>
Motivation: 受药物发现中计算机模拟方法的启发，旨在加速开发更安全、环保的农用化学品，特别关注生态毒理学。

Method: 创建了最大的蜜蜂农药毒性数据集ApisTox，并广泛评估了分子图分类的机器学习模型，包括分子指纹、图核、图神经网络和预训练变换器。

Result: 结果显示在药物化学中成功的方法往往无法推广到农用化学品领域，强调了需要领域特定模型和基准测试。

Conclusion: 未来工作将专注于开发全面的基准测试套件，并设计针对农药发现独特挑战的机器学习模型。

Abstract: This research focuses on rational pesticide design, using graph machine
learning to accelerate the development of safer, eco-friendly agrochemicals,
inspired by in silico methods in drug discovery. With an emphasis on
ecotoxicology, the initial contributions include the creation of ApisTox, the
largest curated dataset on pesticide toxicity to honey bees. We conducted a
broad evaluation of machine learning (ML) models for molecular graph
classification, including molecular fingerprints, graph kernels, GNNs, and
pretrained transformers. The results show that methods successful in medicinal
chemistry often fail to generalize to agrochemicals, underscoring the need for
domain-specific models and benchmarks. Future work will focus on developing a
comprehensive benchmarking suite and designing ML models tailored to the unique
challenges of pesticide discovery.

</details>


### [104] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出了广义双模拟度量（GBSM），用于计算两个马尔可夫决策过程（MDP）之间的状态相似性，解决了传统BSM在多MDP场景下的局限性，并严格证明了其数学性质。


<details>
  <summary>Details</summary>
Motivation: 传统双模拟度量（BSM）在单个MDP内有效，但在多MDP场景（如策略迁移）中应用受限。现有方法缺乏对广义BSM数学性质的严格分析，限制了理论进展。

Method: 正式建立MDP对之间的广义双模拟度量（GBSM），严格证明其三个基本性质：对称性、MDP间三角不等式和相同状态空间上的距离界限。

Result: GBSM在策略迁移、状态聚合和基于采样的估计等方面获得了比标准BSM更严格的理论界限，并提供了闭式样本复杂度估计。数值结果验证了理论发现。

Conclusion: GBSM为多MDP场景提供了理论基础和实用工具，在理论分析和实际应用中都优于传统BSM方法。

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [105] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: 本文提出了一种将强化学习与大型语言模型相结合的新型电子商务支付欺诈检测方法，通过将交易风险建模为多步马尔可夫决策过程，利用LLM迭代优化奖励函数，实现了更好的欺诈检测准确性和零样本能力。


<details>
  <summary>Details</summary>
Motivation: 传统的欺诈检测方法在奖励函数设计上需要大量人工专业知识，而LLM具有先进的推理和编码能力，可以优化这一过程，提高检测效果。

Method: 将交易风险建模为多步马尔可夫决策过程，利用强化学习优化多阶段支付风险检测，并通过LLM迭代改进奖励函数设计。

Result: 在真实世界数据上的实验证实了该方法的有效性、鲁棒性和弹性，长期评估显示LLM增强的RL框架具有显著优势。

Conclusion: 该方法展示了LLM在推进工业级强化学习应用方面的潜力，特别是在复杂奖励函数设计场景中。

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [106] [Theory of periodic convolutional neural network](https://arxiv.org/abs/2509.18744)
*Yuqing Liu*

Main category: cs.LG

TL;DR: 本文介绍了一种新型的周期性CNN架构，通过引入周期性边界条件，证明了其在d维输入空间中能够逼近d-1个线性变量的脊函数，而低维脊设置无法实现这种逼近。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在处理具有周期性结构的数据时存在局限性，需要开发能够自然处理周期性边界条件的架构，特别是在图像分析、物理信息学习和材料科学等领域。

Method: 提出周期性CNN架构，将周期性边界条件融入卷积层，并通过严格的逼近定理证明其表达能力。

Result: 理论证明周期性CNN能够逼近高内在维度的脊状结构函数，而传统方法在低维设置下无法实现这种逼近。

Conclusion: 周期性CNN不仅扩展了CNN逼近理论的数学基础，还展示了一类具有实际应用价值的架构，特别适用于具有周期性结构的数据分析问题。

Abstract: We introduce a novel convolutional neural network architecture, termed the
\emph{periodic CNN}, which incorporates periodic boundary conditions into the
convolutional layers. Our main theoretical contribution is a rigorous
approximation theorem: periodic CNNs can approximate ridge functions depending
on $d-1$ linear variables in a $d$-dimensional input space, while such
approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer
variables). This result establishes a sharp characterization of the expressive
power of periodic CNNs. Beyond the theory, our findings suggest that periodic
CNNs are particularly well-suited for problems where data naturally admits a
ridge-like structure of high intrinsic dimension, such as image analysis on
wrapped domains, physics-informed learning, and materials science. The work
thus both expands the mathematical foundation of CNN approximation theory and
highlights a class of architectures with surprising and practically relevant
approximation capabilities.

</details>


### [107] [MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model](https://arxiv.org/abs/2509.18751)
*Samuel Yoon,Jongwon Kim,Juyoung Ha,Young Myoung Ko*

Main category: cs.LG

TL;DR: MOMEMTO是一个基于时间序列基础模型（TFM）的异常检测方法，通过引入补丁级内存模块来缓解模型过度泛化问题，能够在多个数据集上联合微调并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于重构的深度模型在时间序列异常检测中容易过度泛化，准确重构未见过的异常；而现有内存架构方法训练成本高且难以与时间序列基础模型有效集成。

Method: 提出MOMEMTO方法，包含补丁级内存模块，从预训练编码器初始化内存项，组织为补丁级单元，通过注意力机制更新，采用多领域训练策略实现单一模型在多个数据集上的联合微调。

Result: 在23个单变量基准数据集上的实验表明，MOMEMTO在AUC和VUS指标上优于基线方法，特别是在少样本学习场景中显著提升了骨干TFM的性能。

Conclusion: MOMEMTO通过内存模块有效缓解了时间序列异常检测中的过度泛化问题，实现了多数据集联合训练的高效异常检测，在少样本场景下表现优异。

Abstract: Recently reconstruction-based deep models have been widely used for time
series anomaly detection, but as their capacity and representation capability
increase, these models tend to over-generalize, often reconstructing unseen
anomalies accurately. Prior works have attempted to mitigate this by
incorporating a memory architecture that stores prototypes of normal patterns.
Nevertheless, these approaches suffer from high training costs and have yet to
be effectively integrated with time series foundation models (TFMs). To address
these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,
enhanced with a patch-based memory module to mitigate over-generalization. The
memory module is designed to capture representative normal patterns from
multiple domains and enables a single model to be jointly fine-tuned across
multiple datasets through a multi-domain training strategy. MOMEMTO initializes
memory items with latent representations from a pre-trained encoder, organizes
them into patch-level units, and updates them via an attention mechanism. We
evaluate our method using 23 univariate benchmark datasets. Experimental
results demonstrate that MOMEMTO, as a single model, achieves higher scores on
AUC and VUS metrics compared to baseline methods, and further enhances the
performance of its backbone TFM, particularly in few-shot learning scenarios.

</details>


### [108] [Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems](https://arxiv.org/abs/2509.18810)
*Arman Mohammadi,Mattias Krysander,Daniel Jung,Erik Frisk*

Main category: cs.LG

TL;DR: 本文提出了一种基于集成概率机器学习的诊断框架，通过量化和自动化预测不确定性来改进数据驱动的一致性诊断特性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在故障诊断中应用日益广泛，但尽管在预测任务中表现出色，这些模型往往难以评估其置信度。在基于一致性的诊断中，决策逻辑对误报高度敏感，因此置信度评估尤为重要。

Method: 使用集成概率机器学习方法，通过量化预测不确定性来改进数据驱动的一致性诊断框架。

Result: 通过消融分析和对比分析在多个案例研究中评估，结果显示在一系列诊断指标上都有持续改进。

Conclusion: 所提出的方法能够有效提升数据驱动一致性诊断的性能，特别是在处理预测不确定性方面表现出色。

Abstract: Deep neural networks has been increasingly applied in fault diagnostics,
where it uses historical data
  to capture systems behavior, bypassing the need for high-fidelity physical
models.
  However, despite their competence in prediction tasks, these models often
struggle with
  the evaluation of their confidence. This matter is particularly
  important in consistency-based diagnosis where decision logic is highly
sensitive to false alarms.
  To address this challenge, this work presents a diagnostic framework that
uses
  ensemble probabilistic machine learning to
  improve diagnostic characteristics of data driven consistency based diagnosis
  by quantifying and automating the prediction uncertainty.
  The proposed method is evaluated across several case studies using both
ablation
  and comparative analyses, showing consistent improvements across a range of
diagnostic metrics.

</details>


### [109] [Training-Free Data Assimilation with GenCast](https://arxiv.org/abs/2509.18811)
*Thomas Savary,François Rozet,Gilles Louppe*

Main category: cs.LG

TL;DR: 提出一种基于预训练扩散模型的轻量级通用数据同化方法，无需额外训练，应用于天气预测等领域


<details>
  <summary>Details</summary>
Motivation: 数据同化在气象学、海洋学和机器人学等领域广泛应用，但现有方法需要复杂训练过程，需要更轻量级的解决方案

Method: 基于粒子滤波器算法，利用预训练的扩散模型进行数据同化，以GenCast天气预测模型为例进行验证

Result: 开发出无需额外训练的数据同化框架，可直接应用于预训练的扩散模型

Conclusion: 该方法为数据同化提供了一种轻量级且通用的解决方案，特别适用于基于扩散模型的动态系统仿真

Abstract: Data assimilation is widely used in many disciplines such as meteorology,
oceanography, and robotics to estimate the state of a dynamical system from
noisy observations. In this work, we propose a lightweight and general method
to perform data assimilation using diffusion models pre-trained for emulating
dynamical systems. Our method builds on particle filters, a class of data
assimilation algorithms, and does not require any further training. As a
guiding example throughout this work, we illustrate our methodology on GenCast,
a diffusion-based model that generates global ensemble weather forecasts.

</details>


### [110] [Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective](https://arxiv.org/abs/2509.18826)
*Wenlong Lyu,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: 本文提出了LoRD和B-LoRD两种图聚类方法，通过减少约束松弛来提升聚类效果，并设计了全局收敛的优化算法。


<details>
  <summary>Details</summary>
Motivation: 现有图聚类方法（如谱聚类、对称非负矩阵分解等）过度松弛了低秩、非负、双随机和正交约束，可能限制了聚类性能。

Method: 提出LoRD模型仅松弛正交约束，B-LoRD在此基础上加入块对角正则化；将非凸双随机约束转化为线性凸约束，设计投影梯度下降算法进行优化。

Result: 理论证明了正交性与块对角性的等价关系，实验验证了方法的有效性。

Conclusion: 通过更精确的约束建模和有效的优化算法，提出的方法在聚类性能上优于现有方法。

Abstract: The well-known graph-based clustering methods, including spectral clustering,
symmetric non-negative matrix factorization, and doubly stochastic
normalization, can be viewed as relaxations of the kernel $k$-means approach.
However, we posit that these methods excessively relax their inherent low-rank,
nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical
feasibility, potentially limiting their clustering efficacy. In this paper,
guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank
\textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only
relaxes the orthonormal constraint to derive a probabilistic clustering
results. Furthermore, we theoretically establish the equivalence between
orthogonality and block diagonality under the doubly stochastic constraint. By
integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the
maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further
enhances the clustering performance. To ensure numerical solvability, we
transform the non-convex doubly stochastic constraint into a linear convex
constraint through the introduction of a class probability parameter. We
further theoretically demonstrate the gradient Lipschitz continuity of our LoRD
and B-LoRD enables the proposal of a globally convergent projected gradient
descent algorithm for their optimization. Extensive experiments validate the
effectiveness of our approaches. The code is publicly available at
https://github.com/lwl-learning/LoRD.

</details>


### [111] [Shared-Weights Extender and Gradient Voting for Neural Network Expansion](https://arxiv.org/abs/2509.18842)
*Nikolas Chatzis,Ioannis Kordonis,Manos Theodosis,Petros Maragos*

Main category: cs.LG

TL;DR: 提出SWE和SVoD方法，通过权重共享和梯度分配策略解决神经网络扩展中新神经元失活问题，在四个数据集上验证了有效性


<details>
  <summary>Details</summary>
Motivation: 神经网络训练过程中扩展容量时，新加入的神经元往往无法适应已训练网络而变得不活跃，无法真正增加模型容量

Method: Shared-Weights Extender (SWE) 通过将新神经元与现有神经元耦合实现平滑集成；Steepest Voting Distributor (SVoD) 使用基于梯度的策略在深层网络扩展中分配神经元

Result: 在四个数据集上的广泛基准测试表明，该方法能有效抑制神经元失活，相比其他扩展方法和基线获得更好性能

Conclusion: 所提出的方法能够有效解决神经网络扩展中的神经元失活问题，实现更好的容量增长和性能提升

Abstract: Expanding neural networks during training is a promising way to augment
capacity without retraining larger models from scratch. However, newly added
neurons often fail to adjust to a trained network and become inactive,
providing no contribution to capacity growth. We propose the Shared-Weights
Extender (SWE), a novel method explicitly designed to prevent inactivity of new
neurons by coupling them with existing ones for smooth integration. In
parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based
method for allocating neurons across layers during deep network expansion. Our
extensive benchmarking on four datasets shows that our method can effectively
suppress neuron inactivity and achieve better performance compared to other
expanding methods and baselines.

</details>


### [112] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: NGRPO算法通过优势校准和非对称剪裁解决了GRPO在响应组完全正确或完全错误时无法学习的问题，显著提升了数学推理能力


<details>
  <summary>Details</summary>
Motivation: GRPO算法在响应组完全正确或完全错误时无法产生有效学习信号，特别是对于完全错误的响应组，优势函数值为零导致梯度消失，损失了宝贵的学习机会

Method: 1. 优势校准：在优势计算中假设存在虚拟最大奖励样本，改变组内奖励的均值和方差，确保完全错误样本的优势值不再为零；2. 非对称剪裁：放松正样本的更新幅度，同时对负样本施加更严格的约束，稳定探索压力

Result: 在Qwen2.5-Math-7B模型上的实验表明，NGRPO在MATH500、AMC23和AIME2025等数学基准测试中显著优于PPO、GRPO、DAPO和PSR-NSR等基线方法

Conclusion: NGRPO能够从完全错误的响应中学习，实现了数学推理能力的稳定和显著提升

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [113] [Exploring Heterophily in Graph-level Tasks](https://arxiv.org/abs/2509.18893)
*Qinhan Hou,Yilun Zheng,Xichun Zhang,Sitao Luan,Jing Tang*

Main category: cs.LG

TL;DR: 该论文首次分析了异质性在图级学习中的影响，通过理论分析和实验验证发现，与节点级任务不同，图级任务中的motif检测需要混合频率动态，而非频率主导机制。


<details>
  <summary>Details</summary>
Motivation: 虽然异质性在节点级任务中已被广泛研究，但其对图级任务的影响尚不清楚。本文旨在填补这一空白，建立图级学习中异质性的理论理解。

Method: 引入图级标签方案的分类法，聚焦于局部结构标签中的motif任务；使用基于能量的梯度流分析揭示motif检测的动态特性；在合成数据集和真实分子属性预测数据集上进行实验验证。

Result: 理论分析表明motif目标与全局频率主导机制存在固有错位；实验证明频率自适应模型优于频率主导模型。

Conclusion: 这项工作为图级学习中的异质性建立了新的理论理解，并为设计有效的GNN架构提供了指导。

Abstract: While heterophily has been widely studied in node-level tasks, its impact on
graph-level tasks remains unclear. We present the first analysis of heterophily
in graph-level learning, combining theoretical insights with empirical
validation. We first introduce a taxonomy of graph-level labeling schemes, and
focus on motif-based tasks within local structure labeling, which is a popular
labeling scheme. Using energy-based gradient flow analysis, we reveal a key
insight: unlike frequency-dominated regimes in node-level tasks, motif
detection requires mixed-frequency dynamics to remain flexible across multiple
spectral components. Our theory shows that motif objectives are inherently
misaligned with global frequency dominance, demanding distinct architectural
considerations. Experiments on synthetic datasets with controlled heterophily
and real-world molecular property prediction support our findings, showing that
frequency-adaptive model outperform frequency-dominated models. This work
establishes a new theoretical understanding of heterophily in graph-level
learning and offers guidance for designing effective GNN architectures.

</details>


### [114] [Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction](https://arxiv.org/abs/2509.18904)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出了一种联邦学习中的动态优化后门攻击方法，通过最小-最大框架解耦主任务和后门任务，提高攻击的持久性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的分布式特性暴露了新的攻击面，现有后门攻击依赖固定模式或对抗性扰动作为触发器，导致主任务和后门任务紧密耦合，容易被诚实更新稀释且难以在联邦防御下持续存在。

Method: 采用动态优化后门触发器的最小-最大框架：内层最大化中毒样本与良性样本的性能差距，确保良性用户更新对后门影响最小；外层将自适应触发器注入本地模型。

Result: 在计算机视觉和自然语言任务上评估，与6种后门攻击方法和6种防御算法比较，实验结果显示该方法具有良好的攻击性能，并能轻松集成到现有后门攻击技术中。

Conclusion: 该方法有效解决了联邦学习中后门攻击的持久性问题，通过动态优化触发器实现了主任务与后门任务的解耦，提升了攻击的隐蔽性和鲁棒性。

Abstract: Federated learning allows multiple participants to collaboratively train a
central model without sharing their private data. However, this distributed
nature also exposes new attack surfaces. In particular, backdoor attacks allow
attackers to implant malicious behaviors into the global model while
maintaining high accuracy on benign inputs. Existing attacks usually rely on
fixed patterns or adversarial perturbations as triggers, which tightly couple
the main and backdoor tasks. This coupling makes them vulnerable to dilution by
honest updates and limits their persistence under federated defenses. In this
work, we propose an approach to decouple the backdoor task from the main task
by dynamically optimizing the backdoor trigger within a min-max framework. The
inner layer maximizes the performance gap between poisoned and benign samples,
ensuring that the contributions of benign users have minimal impact on the
backdoor. The outer process injects the adaptive triggers into the local model.
We evaluate our method on both computer vision and natural language tasks, and
compare it with six backdoor attack methods under six defense algorithms.
Experimental results show that our method achieves good attack performance and
can be easily integrated into existing backdoor attack techniques.

</details>


### [115] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: 本文提出了GNARL框架，将神经算法推理重新定义为马尔可夫决策过程，结合模仿学习和强化学习来解决传统NAR方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统神经算法推理方法存在无法构造有效解、难以处理NP难问题、依赖专家算法等局限性，需要新的框架来克服这些问题。

Method: 将算法轨迹学习问题重构为马尔可夫决策过程，提出GNARL框架，结合模仿学习和强化学习方法，适用于广泛的图基问题。

Result: 在多个CLRS-30问题上实现了很高的图精度，在NP难问题上性能匹配或超过传统NAR方法，甚至在缺乏专家算法时也适用。

Conclusion: GNARL框架成功解决了传统NAR方法的局限性，为算法学习提供了更灵活和强大的解决方案。

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [116] [Towards Privacy-Aware Bayesian Networks: A Credal Approach](https://arxiv.org/abs/2509.18949)
*Niccolò Rocchi,Fabio Stella,Cassio de Campos*

Main category: cs.LG

TL;DR: 本文提出使用置信网络（CN）作为贝叶斯网络（BN）的隐私保护替代方案，通过模糊化而非添加噪声的方式，在保持模型效用的同时防止追踪攻击。


<details>
  <summary>Details</summary>
Motivation: 随着隐私问题日益严重，公开发布的贝叶斯网络模型需要保护训练数据中的敏感信息。现有的噪声添加方法虽然提供强大的隐私保护，但显著降低了模型的实用性。

Method: 采用置信网络（CN）来掩盖学习到的贝叶斯网络，通过调整CN超参数来调节隐私保护程度，同时识别必须隐藏的关键学习信息以防止攻击者恢复底层BN。

Result: 数值实验证实，置信网络能够有效降低成功攻击的概率，同时保持有意义的推理能力，在隐私和效用之间实现了平衡。

Conclusion: 置信网络为开发隐私感知的概率图模型提供了一种原则性、实用且有效的方法，能够在保护隐私的同时维持模型的推理效用。

Abstract: Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.

</details>


### [117] [Lift What You Can: Green Online Learning with Heterogeneous Ensembles](https://arxiv.org/abs/2509.18962)
*Kirsten Köbschall,Sebastian Buschjäger,Raphael Fischer,Lisa Hartung,Stefan Kramer*

Main category: cs.LG

TL;DR: HEROS是一种异构在线集成方法，通过在资源约束下选择部分模型进行训练，平衡预测性能和可持续性，提出ζ策略实现近最优性能并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有集成方法过度关注预测能力而忽视计算成本，需要更可持续的在线学习方法。

Method: 提出异构在线集成框架HEROS，使用马尔可夫决策过程建模性能与可持续性权衡，引入ζ策略选择训练模型子集。

Result: 在11个基准数据集上实验表明，ζ策略在保持高精度的同时显著减少资源消耗，有时甚至优于现有方法。

Conclusion: HEROS框架和ζ策略为绿色在线学习提供了有效解决方案，在性能和可持续性之间取得了良好平衡。

Abstract: Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.

</details>


### [118] [Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding](https://arxiv.org/abs/2509.18968)
*Zhanglu Yan,Jiayi Mao,Qianhui Liu,Fanfan Li,Gang Pan,Tao Luo,Bowen Zhu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Otters的新型光电突触设计，利用光电设备的自然信号衰减特性来实现时间到首次脉冲编码，从而消除昂贵的数字运算，显著提高脉冲神经网络的能效。


<details>
  <summary>Details</summary>
Motivation: 传统脉冲神经网络（SNNs）特别是时间到首次脉冲（TTFS）编码虽然理论上具有高能效优势，但在实际推理过程中需要评估时间衰减函数并与突触权重相乘，这些操作成本高昂，导致能效优势难以实现。

Method: 1. 设计并制造了定制氧化铟光电突触，利用其自然物理衰减特性直接实现所需的时间函数
2. 将设备的模拟输出视为突触权重和时间衰减的融合乘积，提出Otters范式
3. 针对复杂架构（如transformer）引入新型量化神经网络到SNN的转换算法
4. 采用硬件-软件协同设计方法

Result: 在七个GLUE基准数据集上达到最先进的准确率，基于商业22nm工艺的能耗分析显示，相比之前领先的SNNs实现了1.77倍的能效提升。

Conclusion: 该工作建立了一种新的能效SNN范式，将基础器件物理特性直接转化为强大的计算原语，所有代码和数据均已开源。

Abstract: Spiking neural networks (SNNs) promise high energy efficiency, particularly
with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting
at most one spike per neuron. However, such energy advantage is often
unrealized because inference requires evaluating a temporal decay function and
subsequent multiplication with the synaptic weights. This paper challenges this
costly approach by repurposing a physical hardware `bug', namely, the natural
signal decay in optoelectronic devices, as the core computation of TTFS. We
fabricated a custom indium oxide optoelectronic synapse, showing how its
natural physical decay directly implements the required temporal function. By
treating the device's analog output as the fused product of the synaptic weight
and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates
these expensive digital operations. To use the Otters paradigm in complex
architectures like the transformer, which are challenging to train directly due
to the sparsity issue, we introduce a novel quantized neural network-to-SNN
conversion algorithm. This complete hardware-software co-design enables our
model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets
and demonstrates a 1.77$\times$ improvement in energy efficiency over previous
leading SNNs, based on a comprehensive analysis of compute, data movement, and
memory access costs using energy measurements from a commercial 22nm process.
Our work thus establishes a new paradigm for energy-efficient SNNs, translating
fundamental device physics directly into powerful computational primitives. All
codes and data are open source.

</details>


### [119] [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
*Carson Dudley,Marisa Eisenberg*

Main category: cs.LG

TL;DR: 本文提出了仿真基础神经网络（SGNNs）的理论基础，证明其在仿真先验下实现摊销贝叶斯推断，收敛到贝叶斯最优预测器，并能在模型错误设定下学习经验方法无法观测的科学量。


<details>
  <summary>Details</summary>
Motivation: SGNNs在真实标签有限或不可观测的领域取得了最先进性能，但缺乏理论基础。本文旨在为仿真基础学习建立形式化理论框架。

Method: 通过理论分析证明SGNNs实现摊销贝叶斯推断，推导模型错误设定下的泛化边界，并形式化SGNNs特有的机制可解释性方法。

Result: 数值实验验证了所有理论预测：SGNNs能够恢复潜在参数，在失配情况下保持鲁棒性，在模型选择任务中AIC误差减半。

Conclusion: SGNNs为数据受限场景下的科学预测提供了一个原则性和实用的框架。

Abstract: Simulation-Grounded Neural Networks (SGNNs) are predictive models trained
entirely on synthetic data from mechanistic simulations. They have achieved
state-of-the-art performance in domains where real-world labels are limited or
unobserved, but lack a formal underpinning.
  We present the foundational theory of simulation-grounded learning. We show
that SGNNs implement amortized Bayesian inference under a simulation prior and
converge to the Bayes-optimal predictor. We derive generalization bounds under
model misspecification and prove that SGNNs can learn unobservable scientific
quantities that empirical methods provably cannot. We also formalize a novel
form of mechanistic interpretability uniquely enabled by SGNNs: by attributing
predictions to the simulated mechanisms that generated them, SGNNs yield
posterior-consistent, scientifically grounded explanations.
  We provide numerical experiments to validate all theoretical predictions.
SGNNs recover latent parameters, remain robust under mismatch, and outperform
classical tools: in a model selection task, SGNNs achieve half the error of AIC
in distinguishing mechanistic dynamics. These results establish SGNNs as a
principled and practical framework for scientific prediction in data-limited
regimes.

</details>


### [120] [CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)
*Boao Kong,Junzhu Liang,Yuxi Liu,Renjia Deng,Kun Yuan*

Main category: cs.LG

TL;DR: 提出CR-Net框架，通过利用层间激活残差的低秩特性，在保持高性能的同时显著降低参数复杂度和内存需求


<details>
  <summary>Details</summary>
Motivation: 解决现有低秩方法在模型性能、计算开销和内存节省方面的三个关键缺陷

Method: 采用双路径架构，结合前一层输出和低秩差异来高效重构层激活，并开发专门的激活重计算策略

Result: 在60M到7B参数规模的预训练实验中，CR-Net持续优于最先进的低秩框架，同时需要更少的计算资源和内存

Conclusion: CR-Net通过创新的低秩残差网络设计，有效解决了低秩方法的局限性，为高效LLM预训练提供了有前景的解决方案

Abstract: Low-rank architectures have become increasingly important for efficient large
language model (LLM) pre-training, providing substantial reductions in both
parameter complexity and memory/computational demands. Despite these
advantages, current low-rank methods face three critical shortcomings: (1)
compromised model performance, (2) considerable computational overhead, and (3)
limited activation memory savings. To address these limitations, we propose
Cross-layer Low-Rank residual Network (CR-Net), an innovative
parameter-efficient framework inspired by our discovery that inter-layer
activation residuals possess low-rank properties. CR-Net implements this
insight through a dual-path architecture that efficiently reconstructs layer
activations by combining previous-layer outputs with their low-rank
differences, thereby maintaining high-rank information with minimal parameters.
We further develop a specialized activation recomputation strategy tailored for
CR-Net that dramatically reduces memory requirements. Extensive pre-training
experiments across model scales from 60M to 7B parameters demonstrate that
CR-Net consistently outperforms state-of-the-art low-rank frameworks while
requiring fewer computational resources and less memory.

</details>


### [121] [Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization](https://arxiv.org/abs/2509.18997)
*Pascal Esser,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: 本文综述了无监督表示学习的最新理论进展，特别是深度学习方法与传统统计方法之间的差异，并介绍了作者在该方向上的贡献。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型使用自监督或去噪/掩码自编码器等新原理进行无监督表示学习，但这些方法难以用经典理论分析，需要结合统计和优化的数学工具来理解其表征特性。

Method: 结合统计学和优化的数学工具，对无监督表示学习的理论框架进行分析，特别是针对视觉基础模型中自监督和掩码自编码器等方法。

Result: 提供了无监督表示学习的最新理论进展概述，阐明了深度学习模型表征学习的理论基础。

Conclusion: 需要进一步发展数学理论工具来深入理解深度无监督表示学习模型的表征特性和性能表现。

Abstract: Representation learning from unlabeled data has been extensively studied in
statistics, data science and signal processing with a rich literature on
techniques for dimension reduction, compression, multi-dimensional scaling
among others. However, current deep learning models use new principles for
unsupervised representation learning that cannot be easily analyzed using
classical theories. For example, visual foundation models have found tremendous
success using self-supervision or denoising/masked autoencoders, which
effectively learn representations from massive amounts of unlabeled data.
However, it remains difficult to characterize the representations learned by
these models and to explain why they perform well for diverse prediction tasks
or show emergent behavior. To answer these questions, one needs to combine
mathematical tools from statistics and optimization. This paper provides an
overview of recent theoretical advances in representation learning from
unlabeled data and mentions our contributions in this direction.

</details>


### [122] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: 本文提出了一种完全可学习的神经奖励机（FLNRM），能够端到端学习符号接地函数和自动机，消除了对先验知识的依赖，在非马尔可夫强化学习任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决非马尔可夫强化学习任务中传统方法依赖预定义符号接地函数或先验知识的限制，使方法更易应用且更具可解释性。

Method: 提出完全可学习的神经奖励机（FLNRM），集成深度强化学习，端到端学习符号接地和自动机结构。

Result: FLNRM方法优于基于循环神经网络（RNN）的先前方法，同时保持了自动机的有限性和紧凑性带来的可解释性优势。

Conclusion: FLNRM为非马尔可夫强化学习提供了一种既易于应用又具有可解释性的有效解决方案，无需先验知识即可学习复杂的时间扩展目标。

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [123] [OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment](https://arxiv.org/abs/2509.19018)
*Teng Xiao,Zuchao Li,Lefei Zhang*

Main category: cs.LG

TL;DR: OmniBridge是一个统一的多模态框架，支持视觉语言理解、生成和检索任务，采用语言中心设计和轻量级双向潜在对齐模块，通过两阶段解耦训练策略实现多任务统一建模。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型解决方案往往孤立处理不同任务或需要从头训练，导致计算成本高且跨模态泛化能力有限。

Method: 采用语言中心设计重用预训练LLM，引入轻量级双向潜在对齐模块；提出两阶段解耦训练策略：监督微调和潜在空间对齐用于对齐LLM行为与多模态推理，语义引导扩散训练通过可学习查询嵌入对齐跨模态潜在空间。

Result: 在广泛基准测试中，OmniBridge在所有三个任务上都达到竞争性或最先进的性能。

Conclusion: 潜在空间对齐在共享表示空间下统一多模态建模方面具有有效性，代码和模型已开源。

Abstract: Recent advances in multimodal large language models (LLMs) have led to
significant progress in understanding, generation, and retrieval tasks.
However, current solutions often treat these tasks in isolation or require
training LLMs from scratch, resulting in high computational costs and limited
generalization across modalities. In this work, we present OmniBridge, a
unified and modular multimodal framework that supports vision-language
understanding, generation, and retrieval within a unified architecture.
OmniBridge adopts a language-centric design that reuses pretrained LLMs and
introduces a lightweight bidirectional latent alignment module. To address the
challenge of task interference, we propose a two-stage decoupled training
strategy: supervised fine-tuning and latent space alignment for aligning LLM
behavior with multimodal reasoning, and semantic-guided diffusion training to
align cross-modal latent spaces via learnable query embeddings. Extensive
experiments across a wide range of benchmarks demonstrate that OmniBridge
achieves competitive or state-of-the-art performance in all three tasks.
Moreover, our results highlight the effectiveness of latent space alignment for
unifying multimodal modeling under a shared representation space. Code and
models are released at https://github.com/xiao-xt/OmniBridge.

</details>


### [124] [Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling](https://arxiv.org/abs/2509.19032)
*Kashaf Ul Emaan*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer编码器的GAN混合方法，用于生成高质量欺诈交易样本，以解决信用卡欺诈检测中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测面临严重的数据不平衡问题，传统的过采样方法如SMOTE生成的样本过于简单，而现有的生成模型如CTGAN和TVAE在高维依赖建模方面存在不足。

Method: 采用生成对抗网络（GAN）与Transformer编码器块的混合架构，GAN通过对抗训练生成真实样本，Transformer通过自注意力机制学习丰富的特征交互。

Result: 在公开的信用卡欺诈检测数据集上测试，与多种分类器结合，相比传统和生成重采样方法，在召回率、F1分数和AUC指标上均有显著提升。

Conclusion: 基于Transformer的GAN方法能有效克服欺诈检测中的严重类别不平衡问题，生成高质量的少数类样本。

Abstract: Detection of credit card fraud is an acute issue of financial security
because transaction datasets are highly lopsided, with fraud cases being only a
drop in the ocean. Balancing datasets using the most popular methods of
traditional oversampling such as the Synthetic Minority Oversampling Technique
(SMOTE) generally create simplistic synthetic samples that are not readily
applicable to complex fraud patterns. Recent industry advances that include
Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular
Variational Autoencoders (TVAE) have demonstrated increased efficiency in
tabular synthesis, yet all these models still exhibit issues with
high-dimensional dependence modelling. Now we will present our hybrid approach
where we use a Generative Adversarial Network (GAN) with a Transformer encoder
block to produce realistic fraudulent transactions samples. The GAN
architecture allows training realistic generators adversarial, and the
Transformer allows the model to learn rich feature interactions by
self-attention. Such a hybrid strategy overcomes the limitations of SMOTE,
CTGAN, and TVAE by producing a variety of high-quality synthetic minority
classes samples. We test our algorithm on the publicly-available Credit Card
Fraud Detection dataset and compare it to conventional and generative
resampling strategies with a variety of classifiers, such as Logistic
Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and
Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN
shows substantial gains in Recall, F1-score and Area Under the Receiver
Operating Characteristic Curve (AUC), which indicates that it is effective in
overcoming the severe class imbalance inherent in the task of fraud detection.

</details>


### [125] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: JAD是一个基于潜在扩散模型的黑盒对抗攻击框架，通过联合注意力蒸馏策略实现跨架构攻击泛化


<details>
  <summary>Details</summary>
Motivation: 解决现有黑盒对抗攻击方法依赖特定网络架构、查询次数多、跨架构迁移性差的问题

Method: 利用潜在扩散模型生成对抗样本，通过从CNN和ViT模型中蒸馏注意力图来指导生成过程，聚焦于跨架构敏感的图像区域

Result: JAD在攻击泛化性、生成效率和跨架构迁移性方面优于现有方法

Conclusion: JAD为黑盒对抗攻击提供了一个有前景的有效范式

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


### [126] [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)
*Przemysław Spyra*

Main category: cs.LG

TL;DR: 本文系统比较了三种无需反向传播的深度学习训练算法（FF、CaFo、MF），发现MF算法在MLP架构上不仅达到甚至超越了BP的准确率，同时显著降低了41%的能耗和34%的训练时间。


<details>
  <summary>Details</summary>
Motivation: 深度学习网络的计算和能耗需求不断增长，主要受反向传播算法驱动，这对可持续AI发展构成挑战。研究旨在寻找更节能高效的替代训练方法。

Method: 建立了严格的比较框架，在各自原生架构上实现三种BP-free算法（FF和MF用MLP，CaFo用CNN），使用Optuna优化超参数，基于验证性能应用一致的早停标准，并通过NVML API测量硬件级能耗。

Result: MF算法在分类准确率上不仅与BP相当，甚至更优，其泛化能力更强，能耗降低41%，训练时间缩短34%，碳足迹显著减小。

Conclusion: MF算法通过收敛到验证损失景观中更有利的最小值，挑战了全局优化必要性的假设，为未来能效深度学习提供了明确的数据驱动路线图。

Abstract: The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.

</details>


### [127] [Diffusion Bridge Variational Inference for Deep Gaussian Processes](https://arxiv.org/abs/2509.19078)
*Jian Xu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 本文提出了DBVI方法，通过可学习的、数据依赖的初始分布来改进DDVI，减少后验差距并提高采样效率。


<details>
  <summary>Details</summary>
Motivation: DDVI的固定无条件起始分布与复杂真实后验差距较大，导致推理轨迹效率低下和收敛缓慢。

Method: DBVI使用摊销神经网络参数化初始分布，通过ELBO目标的梯度逐步适应，设计网络在诱导输入上操作以实现可扩展摊销。

Result: 在回归、分类和图像重建任务中，DBVI在预测准确性、收敛速度和后验质量方面持续优于DDVI和其他变分基线。

Conclusion: DBVI保留了DDVI的数学优雅性，同时通过可学习的初始分布显著提高了推理效率和质量。

Abstract: Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian
modeling but pose substantial challenges for posterior inference, especially
over inducing variables. Denoising diffusion variational inference (DDVI)
addresses this by modeling the posterior as a time-reversed diffusion from a
simple Gaussian prior. However, DDVI's fixed unconditional starting
distribution remains far from the complex true posterior, resulting in
inefficient inference trajectories and slow convergence. In this work, we
propose Diffusion Bridge Variational Inference (DBVI), a principled extension
of DDVI that initiates the reverse diffusion from a learnable, data-dependent
initial distribution. This initialization is parameterized via an amortized
neural network and progressively adapted using gradients from the ELBO
objective, reducing the posterior gap and improving sample efficiency. To
enable scalable amortization, we design the network to operate on the inducing
inputs, which serve as structured, low-dimensional summaries of the dataset and
naturally align with the inducing variables' shape. DBVI retains the
mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time
SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We
derive a tractable training objective under this formulation and implement DBVI
for scalable inference in large-scale DGPs. Across regression, classification,
and image reconstruction tasks, DBVI consistently outperforms DDVI and other
variational baselines in predictive accuracy, convergence speed, and posterior
quality.

</details>


### [128] [Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying](https://arxiv.org/abs/2509.19084)
*Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelGNN是一种新颖的图神经网络架构，灵感来自Axelrod文化传播模型，通过相似性门控概率交互、特征级复制机制和全局极化保持来解决传统GNN的特征过度平滑、异构关系处理困难和特征向量处理不灵活等问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN存在三个基本限制：深层网络中节点表示变得难以区分的特征过度平滑问题；难以有效处理连接节点差异显著的异构关系；将整个特征向量作为不可分割单元处理，限制了灵活性。

Method: AxelGNN采用相似性门控概率交互，根据节点相似性自适应促进收敛或发散；实现特征级复制机制，在片段级别进行细粒度特征聚合；保持全局极化以跨多个表示簇保持节点独特性。

Result: 在节点分类和影响力估计基准测试上的广泛实验表明，AxelGNN在不同同质性-异质性特征的多样化图结构上始终优于或匹配最先进的GNN方法。

Conclusion: AxelGNN的双稳态收敛动力学自然地在单一架构内处理同质性和异质性图，为解决传统GNN的局限性提供了统一框架。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.

</details>


### [129] [Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning](https://arxiv.org/abs/2509.19098)
*Adrien Prevost,Timothee Mathieu,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: 本文研究了非上下文多臂老虎机问题在迁移学习场景下的表现，提出了KL-UCB-Transfer算法，该算法在源分布和目标分布足够接近时显著优于无先验基线。


<details>
  <summary>Details</summary>
Motivation: 在多臂老虎机问题中，迁移学习可以利用源分布的先验知识来改善目标分布的决策性能。当源分布和目标分布足够接近时，迁移学习可以显著减少累积遗憾。

Method: 首先推导了包含迁移参数（距离边界、样本数量）的渐进下界，然后提出了KL-UCB-Transfer索引策略，该策略在高斯情况下能够匹配这个新边界。

Result: 理论分析表明KL-UCB-Transfer算法能够达到渐进最优性能，仿真验证了当源分布和目标分布足够接近时，该算法显著优于无先验基线方法。

Conclusion: 迁移学习框架下的多臂老虎机问题具有实际应用价值，KL-UCB-Transfer算法为这类问题提供了有效的解决方案，特别是在源分布和目标分布相似的情况下。

Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning
setting: before any pulls, the learner is given N'_k i.i.d. samples from each
source distribution nu'_k, and the true target distributions nu_k lie within a
known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first
derive a problem-dependent asymptotic lower bound on cumulative regret that
extends the classical Lai-Robbins result to incorporate the transfer parameters
(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that
matches this new bound in the Gaussian case. Finally, we validate our approach
via simulations, showing that KL-UCB-Transfer significantly outperforms the
no-prior baseline when source and target distributions are sufficiently close.

</details>


### [130] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: 该论文讨论了深度学习模型在安全关键应用中的鲁棒性问题，涵盖了对抗性示例、领域泛化和大型语言模型越狱三个方面的最新进展。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型在安全关键应用中的广泛使用，确保模型决策能够抵御对抗性攻击具有根本重要性。

Method: 提出了新的技术结果、训练范式和认证算法来处理计算机视觉中的对抗性示例；开发了在医学成像、分子识别和图像分类中实现最先进泛化的新算法；针对大型语言模型越狱问题提出了新的攻击和防御方法。

Result: 在对抗性示例、领域泛化和语言模型鲁棒性方面取得了前沿进展，为设计稳健的深度学习模型提供了新的解决方案。

Conclusion: 该研究代表了在设计具有理想鲁棒性属性的算法方面的最新进展，为安全关键应用中深度学习模型的可靠性提供了重要保障。

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [131] [Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation](https://arxiv.org/abs/2509.19112)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: CARGO是一个可扩展的多标签因果发现方法，用于处理稀疏高维事件序列，通过预训练的因果Transformer和自适应频率融合来推断因果图


<details>
  <summary>Details</summary>
Motivation: 理解事件序列中的因果关系对于医疗、车辆诊断等领域至关重要，但目前仍是一个未解决的挑战

Method: 使用两个预训练的因果Transformer作为领域特定基础模型，并行推断每个序列的因果图，并通过自适应频率融合聚合来重建标签的全局马尔可夫边界

Result: 在包含29,100个独特事件类型和474个不平衡标签的真实世界汽车故障预测数据集上验证了CARGO的结构化推理能力

Conclusion: CARGO的两阶段方法能够在规模上实现高效的概率推理，同时避免了全数据集条件独立性测试的难以处理成本

Abstract: Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.

</details>


### [132] [FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI](https://arxiv.org/abs/2509.19120)
*Ferdinand Kahenga,Antoine Bagula,Sajal K. Das,Patrick Sello*

Main category: cs.LG

TL;DR: FedFiTS是一个结合信任和公平意识的联邦学习框架，通过基于适应度的客户端选举和分槽聚合来提升FedFaSt方法，在非IID数据、客户端不可靠性和对抗性攻击等挑战下实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗等敏感领域部署面临非IID数据、客户端不可靠性和对抗性操纵等持续挑战，需要更鲁棒和公平的解决方案。

Method: FedFiTS采用三阶段参与策略（自由训练、自然选择、分槽团队参与），结合动态客户端评分、自适应阈值和基于队列的调度，平衡收敛效率与鲁棒性。

Result: 在医学影像、视觉基准和表格农业数据上的实验表明，FedFiTS在准确性、达到目标时间和对投毒攻击的韧性方面持续优于FedAvg、FedRand和FedPow。

Conclusion: 通过整合信任感知聚合和公平导向的客户端选择，FedFiTS推进了可扩展和安全的联邦学习，特别适合现实世界的医疗和跨领域部署。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

</details>


### [133] [Analysis on distribution and clustering of weight](https://arxiv.org/abs/2509.19122)
*Chunming Ye,Wenquan Tian,Yalan Gao,Songzhou Li*

Main category: cs.LG

TL;DR: 该论文提出了两种向量（标准差向量和聚类向量）来分析大语言模型的权重特征，能够有效区分不同模型并显示同族模型间的相似性。研究发现LoRA微调后，标准差向量受数据集直接影响，而聚类向量保持与预训练模型的高度一致性。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型的架构和参数特征，特别是权重特征，以分析模型间的相关性和差异。

Method: 提出标准差向量（假设权重服从正态分布，对投影矩阵标准差进行归一化）和聚类向量（对权重投影矩阵奇异值进行K-Means聚类分组），用于描述模型特征。

Result: 两种向量能有效区分不同模型，清晰显示同族模型相似性。LoRA微调后，标准差向量受数据集影响，聚类向量保持与预训练模型一致性。

Conclusion: 标准差向量和聚类向量是分析大语言模型权重特征的有效工具，能揭示模型间的分布特性和相关性特征。

Abstract: The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.

</details>


### [134] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: PipelineRL是一种新的强化学习方法，通过并发异步数据生成和模型训练，结合飞行中权重更新机制，解决了LLM训练中硬件效率与数据策略性之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在扩展LLM推理能力时面临硬件利用率低和产生过时数据的问题，这会影响RL算法的效果。

Method: 采用并发异步数据生成和模型训练，引入飞行中权重更新机制，使LLM生成引擎能在生成令牌序列时最小中断地接收更新后的模型权重。

Result: 在128个H100 GPU上进行的长形式推理任务实验中，PipelineRL比传统RL基线学习速度快约2倍，同时保持高度策略性的训练数据。

Conclusion: PipelineRL在硬件效率和数据策略性之间实现了优越的平衡，并提供了可扩展的模块化开源实现。

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [135] [GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](https://arxiv.org/abs/2509.19135)
*Wenying Luo,Zhiyuan Lin,Wenhao Xu,Minghao Liu,Zhi Li*

Main category: cs.LG

TL;DR: GSTM-HMU是一个生成式时空框架，通过显式建模人类移动的语义和时间复杂性来推进移动性分析。该框架包含四个关键创新：时空概念编码器、认知轨迹记忆、生活方式概念库和任务导向生成头。在四个真实数据集上的实验表明，该模型在三个基准任务上均优于强基线。


<details>
  <summary>Details</summary>
Motivation: 人类移动轨迹记录了签到序列，为研究短期访问模式和持久生活方式规律提供了独特窗口。现有方法未能充分建模移动数据的语义和时间复杂性，需要更先进的框架来提取语义规律。

Method: 1) 时空概念编码器整合地理位置、POI类别语义和周期性时间节奏；2) 认知轨迹记忆自适应过滤历史访问，强调近期和行为显著事件；3) 生活方式概念库提供结构化人类偏好线索；4) 任务导向生成头将学习表示转换为下游任务预测。

Result: 在Gowalla、WeePlace、Brightkite和FourSquare四个数据集上的实验显示，GSTM-HMU在下一个位置预测、轨迹用户识别和时间估计三个任务上均取得一致且显著的性能提升，优于强基线方法。

Conclusion: 生成式建模为构建更鲁棒、可解释和可泛化的人类移动智能系统提供了有前景的基础。GSTM-HMU能够有效从复杂移动数据中提取语义规律，展示了生成方法在移动分析中的优势。

Abstract: Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.

</details>


### [136] [Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions](https://arxiv.org/abs/2509.19159)
*Qingfeng Lan,Gautham Vasan,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: 该论文研究了激活函数在神经网络训练动态中的作用及其对强化学习中灾难性遗忘的影响，提出了一种新的激活函数类别——大象激活函数，能够产生稀疏输出和稀疏梯度，从而显著减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是强化学习中长期存在的挑战，现有研究主要关注算法层面，而对神经网络架构特性如何导致灾难性遗忘的理解不足。本研究旨在填补这一空白，特别关注激活函数的作用。

Method: 通过研究激活函数在神经网络训练动态中的角色，发现梯度稀疏性对减少遗忘的重要性。基于此提出大象激活函数，该函数能同时产生稀疏输出和稀疏梯度，并在基于价值的算法中替换传统激活函数进行验证。

Result: 研究表明，仅通过将传统激活函数替换为大象激活函数，就能显著提高神经网络对灾难性遗忘的抵抗能力，使强化学习更加样本高效和内存高效。

Conclusion: 激活函数的梯度稀疏性在减少灾难性遗忘中起关键作用，大象激活函数为缓解这一问题提供了有效的架构解决方案。

Abstract: Catastrophic forgetting has remained a significant challenge for efficient
reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While
recent works have proposed effective methods to mitigate this issue, they
mainly focus on the algorithmic side. Meanwhile, we do not fully understand
what architectural properties of neural networks lead to catastrophic
forgetting. This study aims to fill this gap by studying the role of activation
functions in the training dynamics of neural networks and their impact on
catastrophic forgetting in reinforcement learning setup. Our study reveals
that, besides sparse representations, the gradient sparsity of activation
functions also plays an important role in reducing forgetting. Based on this
insight, we propose a new class of activation functions, elephant activation
functions, that can generate both sparse outputs and sparse gradients. We show
that by simply replacing classical activation functions with elephant
activation functions in the neural networks of value-based algorithms, we can
significantly improve the resilience of neural networks to catastrophic
forgetting, thus making reinforcement learning more sample-efficient and
memory-efficient.

</details>


### [137] [A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness](https://arxiv.org/abs/2509.19197)
*Abdul-Rauf Nuhu,Parham Kebria,Vahid Hemmati,Benjamin Lartey,Mahmoud Nabil Mahmoud,Abdollah Homaifar,Edward Tunstel*

Main category: cs.LG

TL;DR: 提出一种通过局部鲁棒性分析从训练数据中提取"弱鲁棒"样本的验证方法，这些样本作为模型脆弱性的早期敏感指标，用于指导针对性性能提升


<details>
  <summary>Details</summary>
Motivation: 深度学习分类器在干净数据集上表现良好，但对对抗性扰动和常见数据失真很脆弱，传统方法依赖扰动测试集评估鲁棒性，但不够高效

Method: 从训练数据中提取最易受扰动影响的弱鲁棒样本，通过评估模型在这些挑战性训练实例上的表现来理解其鲁棒性

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上验证了方法的有效性

Conclusion: 基于弱鲁棒样本的鲁棒性验证能够显著提升模型在对抗性和常见失真场景下的可靠性

Abstract: Data-driven models, especially deep learning classifiers often demonstrate
great success on clean datasets. Yet, they remain vulnerable to common data
distortions such as adversarial and common corruption perturbations. These
perturbations can significantly degrade performance, thereby challenging the
overall reliability of the models. Traditional robustness validation typically
relies on perturbed test datasets to assess and improve model performance. In
our framework, however, we propose a validation approach that extracts "weak
robust" samples directly from the training dataset via local robustness
analysis. These samples, being the most susceptible to perturbations, serve as
an early and sensitive indicator of the model's vulnerabilities. By evaluating
models on these challenging training instances, we gain a more nuanced
understanding of its robustness, which informs targeted performance
enhancement. We demonstrate the effectiveness of our approach on models trained
with CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation
guided by weak robust samples can drive meaningful improvements in model
reliability under adversarial and common corruption scenarios.

</details>


### [138] [PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation](https://arxiv.org/abs/2509.19215)
*Juntong Ni,Saurabh Kataria,Shengpu Tang,Carl Yang,Xiao Hu,Wei Jin*

Main category: cs.LG

TL;DR: PPG-Distill是一个知识蒸馏框架，通过预测、特征和补丁级别的蒸馏，将全局和局部知识从大型PPG基础模型转移到小型模型，实现资源受限设备上的高效PPG分析


<details>
  <summary>Details</summary>
Motivation: PPG在可穿戴健康监测中广泛应用，但大型PPG基础模型难以在资源受限设备上部署，需要开发高效的模型压缩方法

Method: 提出PPG-Distill框架，包含形态蒸馏（保留局部波形模式）和节律蒸馏（捕捉补丁间时间结构），通过预测级、特征级和补丁级蒸馏实现知识转移

Result: 在心率和房颤检测任务上，PPG-Distill将学生模型性能提升高达21.8%，推理速度提升7倍，内存使用减少19倍

Conclusion: PPG-Distill能够实现可穿戴设备上的高效PPG分析，为资源受限环境下的健康监测提供了可行的解决方案

Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables

</details>


### [139] [FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity](https://arxiv.org/abs/2509.19220)
*Ferdinand Kahenga,Antoine Bagula,Patrick Sello,Sajal K. Das*

Main category: cs.LG

TL;DR: FedFusion是一个联邦迁移学习框架，通过统一域适应和节俭标注技术，结合多样性/聚类感知编码器，解决联邦学习中的异构特征空间、非IID数据和标签稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现实中的联邦学习面临异构特征空间、严重非IID数据和跨客户端标签稀缺的挑战，需要一种能够同时处理这些问题的鲁棒解决方案。

Method: FedFusion使用标记的教师客户端通过置信度过滤的伪标签和域自适应迁移指导学习者客户端，同时客户端维护针对本地数据定制的个性化编码器。采用相似性加权分类器耦合（可选聚类平均）来保持全局一致性，减少数据丰富站点的主导地位。节俭标注管道结合自监督/半监督预训练与选择性微调。

Result: 在表格和图像基准测试中，FedFusion在IID、非IID和标签稀缺情况下，在准确性、鲁棒性和公平性方面始终优于最先进的基线方法，同时保持相当的通信和计算预算。

Conclusion: 协调个性化、域适应和标签效率是应对现实世界约束下鲁棒联邦学习的有效方法。

Abstract: Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.

</details>


### [140] [Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models](https://arxiv.org/abs/2509.19222)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 本文系统研究了最先进开源文本到视频生成模型的延迟和能耗特性，建立了计算受限的分析模型，并在多个模型上验证了预测结果。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成系统虽然能够生成高质量视频，但计算成本高昂且能耗特性尚未被充分理解，需要系统性的能耗分析。

Method: 首先开发了基于空间分辨率、时间长度和去噪步数的计算受限分析模型，然后在WAN2.1-T2V模型上进行细粒度实验验证，最后扩展到六个不同的T2V模型进行比较分析。

Result: 实验表明能耗随空间和时间维度呈二次增长，随去噪步数线性增长，为不同模型提供了运行时和能耗基准数据。

Conclusion: 研究结果为设计和部署更可持续的生成式视频系统提供了基准参考和实践指导。

Abstract: Recent advances in text-to-video (T2V) generation have enabled the creation
of high-fidelity, temporally coherent clips from natural language prompts. Yet
these systems come with significant computational costs, and their energy
demands remain poorly understood. In this paper, we present a systematic study
of the latency and energy consumption of state-of-the-art open-source T2V
models. We first develop a compute-bound analytical model that predicts scaling
laws with respect to spatial resolution, temporal length, and denoising steps.
We then validate these predictions through fine-grained experiments on
WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and
linear scaling with the number of denoising steps. Finally, we extend our
analysis to six diverse T2V models, comparing their runtime and energy profiles
under default settings. Our results provide both a benchmark reference and
practical insights for designing and deploying more sustainable generative
video systems.

</details>


### [141] [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](https://arxiv.org/abs/2509.19233)
*Milad Leyli-abadi,Antoine Marot,Jérôme Picault*

Main category: cs.LG

TL;DR: 本文通过消融研究分析了电力系统中混合机器学习模型的物理约束集成策略，评估了从简单多层感知器到图神经网络的不同架构在准确性、物理合规性、工业准备度和分布外泛化四个维度的表现。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源集成和跨境电力交换增加，电网面临更大不确定性和运行风险。传统物理求解器虽然准确但速度慢，机器学习模型作为快速替代方案需要更好地遵守物理定律。

Method: 使用自定义基准测试管道LIPS，比较了将物理约束作为正则化项或无监督损失的混合策略，以及从多层感知器到图神经网络的不同架构。

Result: 研究结果揭示了物理知识集成如何影响模型在准确性、物理合规性、工业准备度和分布外泛化方面的性能。

Conclusion: 所有实现都是可复现的，并在相应的Github页面上提供，为电力系统混合模型的开发提供了系统性的评估框架。

Abstract: In the context of the energy transition, with increasing integration of
renewable sources and cross-border electricity exchanges, power grids are
encountering greater uncertainty and operational risk. Maintaining grid
stability under varying conditions is a complex task, and power flow simulators
are commonly used to support operators by evaluating potential actions before
implementation. However, traditional physical solvers, while accurate, are
often too slow for near real-time use. Machine learning models have emerged as
fast surrogates, and to improve their adherence to physical laws (e.g.,
Kirchhoff's laws), they are often trained with embedded constraints which are
also known as physics-informed or hybrid models. This paper presents an
ablation study to demystify hybridization strategies, ranging from
incorporating physical constraints as regularization terms or unsupervised
losses, and exploring model architectures from simple multilayer perceptrons to
advanced graph-based networks enabling the direct optimization of physics
equations. Using our custom benchmarking pipeline for hybrid models called
LIPS, we evaluate these models across four dimensions: accuracy, physical
compliance, industrial readiness, and out-of-distribution generalization. The
results highlight how integrating physical knowledge impacts performance across
these criteria. All the implementations are reproducible and provided in the
corresponding Github page.

</details>


### [142] [What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT](https://arxiv.org/abs/2509.19284)
*Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn*

Main category: cs.LG

TL;DR: 本文研究发现，在大型推理模型中，更长的思维链（CoT）和更多的回顾步骤反而会降低准确性，提出了失败步骤比例（FSF）作为评估CoT有效性的关键指标。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在测试时需要大量计算资源用于长思维链推理，但什么构成有效的思维链仍不清楚。现有研究存在矛盾观点：一些认为更长的思维链更好，另一些则认为短思维链更优。

Method: 在10个大型推理模型上对数学和科学推理进行系统评估；引入思维链的图结构视图来提取结构特征；提出失败步骤比例（FSF）指标；设计两种干预实验：基于指标排名候选思维链和编辑思维链移除失败分支。

Result: 发现思维链长度和回顾比例与准确性负相关；FSF指标在预测正确性方面优于长度和回顾比例；移除失败分支的编辑操作显著提高准确性。

Conclusion: 有效的思维链是那些失败步骤更少的思维链，支持基于结构感知的测试时扩展策略，而非盲目生成长思维链。

Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [143] [Surrogate Modelling of Proton Dose with Monte Carlo Dropout Uncertainty Quantification](https://arxiv.org/abs/2509.18155)
*Aaron Pim,Tristan Pryer*

Main category: stat.ML

TL;DR: 开发了一种集成蒙特卡洛dropout的神经代理模型，用于快速、可微分的质子剂量预测和体素级预测不确定性估计，显著加速蒙特卡洛计算并保留不确定性信息。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛质子剂量计算在鲁棒优化、自适应重规划和概率推断等需要重复评估的工作流程中计算量巨大，需要开发快速替代方法。

Method: 使用集成蒙特卡洛dropout的神经网络模型，通过一维解析基准、二维骨水模体和三维水模体实验验证方法的准确性、收敛性和方差分解能力。

Result: 方法在分布偏移下显示认知方差增加，在材料边界参数方差占主导，相比蒙特卡洛方法实现显著加速同时保留不确定性信息。

Conclusion: 该方法适用于质子治疗中的鲁棒规划、自适应工作流程和不确定性感知优化，具有实际应用价值。

Abstract: Accurate proton dose calculation using Monte Carlo (MC) is computationally
demanding in workflows like robust optimisation, adaptive replanning, and
probabilistic inference, which require repeated evaluations. To address this,
we develop a neural surrogate that integrates Monte Carlo dropout to provide
fast, differentiable dose predictions along with voxelwise predictive
uncertainty. The method is validated through a series of experiments, starting
with a one-dimensional analytic benchmark that establishes accuracy,
convergence, and variance decomposition. Two-dimensional bone-water phantoms,
generated using TOPAS Geant4, demonstrate the method's behavior under domain
heterogeneity and beam uncertainty, while a three-dimensional water phantom
confirms scalability for volumetric dose prediction. Across these settings, we
separate epistemic (model) from parametric (input) contributions, showing that
epistemic variance increases under distribution shift, while parametric
variance dominates at material boundaries. The approach achieves significant
speedups over MC while retaining uncertainty information, making it suitable
for integration into robust planning, adaptive workflows, and uncertainty-aware
optimisation in proton therapy.

</details>


### [144] [Statistical Insight into Meta-Learning via Predictor Subspace Characterization and Quantification of Task Diversity](https://arxiv.org/abs/2509.18349)
*Saptati Datta,Nicolas W. Hengartner,Yulia Pimonova,Natalie E. Klein,Nicholas Lubbers*

Main category: stat.ML

TL;DR: 本文提出了一个统计框架，通过预测器子空间表征和任务多样性量化来分析元学习，表明预测精度取决于预测器方差与共享子空间的对齐比例以及子空间估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 元学习已成为利用相关任务信息来提高新任务预测性能的强大范式，但需要统计框架来分析其性能。

Method: 使用潜在子空间建模任务间的共享结构，并引入衡量任务特定预测器异质性的多样性度量。

Result: 模拟和理论证据表明，元学习达到预期预测精度依赖于预测器方差与共享子空间的对齐比例和子空间估计准确性。

Conclusion: 任务多样性和共享子空间估计精度是影响元学习性能的关键因素。

Abstract: Meta-learning has emerged as a powerful paradigm for leveraging information
across related tasks to improve predictive performance on new tasks. In this
paper, we propose a statistical framework for analyzing meta-learning through
the lens of predictor subspace characterization and quantification of task
diversity. Specifically, we model the shared structure across tasks using a
latent subspace and introduce a measure of diversity that captures
heterogeneity across task-specific predictors. We provide both simulation-based
and theoretical evidence indicating that achieving the desired prediction
accuracy in meta-learning depends on the proportion of predictor variance
aligned with the shared subspace, as well as on the accuracy of subspace
estimation.

</details>


### [145] [End-Cut Preference in Survival Trees](https://arxiv.org/abs/2509.18477)
*Xiaogang Su*

Main category: stat.ML

TL;DR: 本文针对CART算法中的端点偏好问题，提出使用平滑sigmoid替代方法来解决生存树中的不平衡分割问题。


<details>
  <summary>Details</summary>
Motivation: 端点偏好问题会导致不平衡和偏倚的分割，掩盖弱信号，并产生不稳定且难以解释的树结构。在生存树中，使用对数秩检验统计量进行贪婪搜索时也会出现这一问题。

Method: 提出平滑sigmoid替代方法，用平滑的sigmoid函数替换硬阈值指示函数。

Result: 理论和数值模拟均证明，SSS方法能有效缓解或避免端点偏好问题。

Conclusion: 平滑sigmoid替代方法为解决CART算法中的端点偏好问题提供了有效的解决方案。

Abstract: The end-cut preference (ECP) problem, referring to the tendency to favor
split points near the boundaries of a feature's range, is a well-known issue in
CART (Breiman et al., 1984). ECP may induce highly imbalanced and biased
splits, obscure weak signals, and lead to tree structures that are both
unstable and difficult to interpret. For survival trees, we show that ECP also
arises when using greedy search to select the optimal cutoff point by
maximizing the log-rank test statistic. To address this issue, we propose a
smooth sigmoid surrogate (SSS) approach, in which the hard-threshold indicator
function is replaced by a smooth sigmoid function. We further demonstrate, both
theoretically and through numerical illustrations, that SSS provides an
effective remedy for mitigating or avoiding ECP.

</details>


### [146] [Estimating Heterogeneous Causal Effect on Networks via Orthogonal Learning](https://arxiv.org/abs/2509.18484)
*Yuanchen Wu,Yubai Yuan*

Main category: stat.ML

TL;DR: 提出一种两阶段方法来估计网络中的异质性直接效应和溢出效应，解决网络因果推断中的异质性和混淆挑战


<details>
  <summary>Details</summary>
Motivation: 网络环境下的因果推断面临两个主要挑战：因果效应的异质性（随单元特征和局部网络结构变化）和网络同质性导致的混淆问题。传统SUTVA假设不适用于存在干扰的网络场景

Method: 两阶段方法：第一阶段使用图神经网络估计依赖复杂网络拓扑的干扰项；第二阶段通过新颖的基于注意力的干扰模型调整网络混淆并推断因果效应。采用Neyman正交化和交叉拟合技术

Result: 方法在表达性和可解释性之间取得平衡，能够识别有影响力的邻域和恢复溢出效应的符号。因果效应估计对网络依赖下的模型误设具有鲁棒性

Conclusion: 提出的方法为网络环境下的因果推断提供了有效的解决方案，特别适用于流行病学、政治科学和经济学等领域需要同时考虑直接和溢出效应的应用场景

Abstract: Estimating causal effects on networks is important for both scientific
research and practical applications. Unlike traditional settings that assume
the Stable Unit Treatment Value Assumption (SUTVA), interference allows an
intervention/treatment on one unit to affect the outcomes of others.
Understanding both direct and spillover effects is critical in fields such as
epidemiology, political science, and economics. Causal inference on networks
faces two main challenges. First, causal effects are typically heterogeneous,
varying with unit features and local network structure. Second, connected units
often exhibit dependence due to network homophily, creating confounding between
structural correlations and causal effects. In this paper, we propose a
two-stage method to estimate heterogeneous direct and spillover effects on
networks. The first stage uses graph neural networks to estimate nuisance
components that depend on the complex network topology. In the second stage, we
adjust for network confounding using these estimates and infer causal effects
through a novel attention-based interference model. Our approach balances
expressiveness and interpretability, enabling downstream tasks such as
identifying influential neighborhoods and recovering the sign of spillover
effects. We integrate the two stages using Neyman orthogonalization and
cross-fitting, which ensures that errors from nuisance estimation contribute
only at higher order. As a result, our causal effect estimates are robust to
bias and misspecification in modeling causal effects under network
dependencies.

</details>


### [147] [Consistency of Selection Strategies for Fraud Detection](https://arxiv.org/abs/2509.18739)
*Christos Revelas,Otilia Boldea,Bas J. M. Werker*

Main category: stat.ML

TL;DR: 本文研究保险公司如何选择调查欺诈索赔的策略，指出传统基于最高预测概率的调查方法可能导致学习不一致，并提出随机化替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统方法只调查预测欺诈概率最高的索赔，这可能导致选择偏差和学习不一致问题，需要更有效的调查策略。

Method: 将问题建模为多臂老虎机问题，在二元回归框架中形式化选择过程，提出随机化调查策略，并与Thompson采样进行比较。

Result: 模拟实验表明传统策略可能不一致，而提出的随机化策略具有一致性；Thompson采样在学习低欺诈概率时效率较低。

Conclusion: 随机化调查策略比传统方法和Thompson采样更有效，能够解决选择偏差问题并实现一致的学习效果。

Abstract: This paper studies how insurers can chose which claims to investigate for
fraud. Given a prediction model, typically only claims with the highest
predicted propability of being fraudulent are investigated. We argue that this
can lead to inconsistent learning and propose a randomized alternative. More
generally, we draw a parallel with the multi-arm bandit literature and argue
that, in the presence of selection, the obtained observations are not iid.
Hence, dependence on past observations should be accounted for when updating
parameter estimates. We formalize selection in a binary regression framework
and show that model updating and maximum-likelihood estimation can be
implemented as if claims were investigated at random. Then, we define
consistency of selection strategies and conjecture sufficient conditions for
consistency. Our simulations suggest that the often-used selection strategy can
be inconsistent while the proposed randomized alternative is consistent.
Finally, we compare our randomized selection strategy with Thompson sampling, a
standard multi-arm bandit heuristic. Our simulations suggest that the latter
can be inefficient in learning low fraud probabilities.

</details>


### [148] [Neighbor Embeddings Using Unbalanced Optimal Transport Metrics](https://arxiv.org/abs/2509.19226)
*Muhammad Rana,Keaton Hamm*

Main category: stat.ML

TL;DR: 本文提出在降维和学习流程中使用不平衡最优传输（UOT）中的Hellinger-Kantorovich度量，相比传统OT和欧几里得方法在多个基准数据集上表现更优


<details>
  <summary>Details</summary>
Motivation: 探索不平衡最优传输在降维和监督/无监督学习中的潜力，特别是与传统最优传输和欧几里得方法的对比

Method: 使用Hellinger-Kantorovich度量构建降维和学习流程，在MedMNIST等基准数据集上进行实验，并与常规OT和欧几里得方法进行对比

Result: UOT在统计假设检验中平均优于欧几里得和OT方法：在MedMNIST分类任务中81%情况下优于OT，聚类任务中83%优于OT，58%优于其他所有方法

Conclusion: 不平衡最优传输在降维和学习任务中具有显著优势，特别是在医学图像数据集上表现突出

Abstract: This paper proposes the use of the Hellinger--Kantorovich metric from
unbalanced optimal transport (UOT) in a dimensionality reduction and learning
(supervised and unsupervised) pipeline. The performance of UOT is compared to
that of regular OT and Euclidean-based dimensionality reduction methods on
several benchmark datasets including MedMNIST. The experimental results
demonstrate that, on average, UOT shows improvement over both Euclidean and
OT-based methods as verified by statistical hypothesis tests. In particular, on
the MedMNIST datasets, UOT outperforms OT in classification 81\% of the time.
For clustering MedMNIST, UOT outperforms OT 83\% of the time and outperforms
both other metrics 58\% of the time.

</details>


### [149] [Recovering Wasserstein Distance Matrices from Few Measurements](https://arxiv.org/abs/2509.19250)
*Muhammad Rana,Abiy Tasissa,HanQin Cai,Yakov Gavriyelov,Keaton Hamm*

Main category: stat.ML

TL;DR: 提出了两种从少量条目估计平方Wasserstein距离矩阵的算法，用于流形学习嵌入，相比欧氏距离矩阵计算成本更低。分析了上三角采样矩阵补全和Nyström补全方法，证明了MDS在Nyström补全下的稳定性，并在MedMNIST数据集上验证了分类性能。


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离矩阵在流形学习嵌入中很重要，但计算成本极高。需要开发高效算法从少量样本中准确估计这些矩阵。

Method: 提出了两种算法：基于上三角采样的矩阵补全和Nyström补全（仅需计算O(d log d)列）。分析了MDS在Nyström补全下的稳定性。

Result: Nyström补全在固定样本距离预算下优于矩阵补全。在OrganCMNIST数据集上，即使只计算10%的列，基于Nyström估计的嵌入数据分类也保持稳定。

Conclusion: Nyström补全是一种高效的Wasserstein距离矩阵估计方法，能够在显著减少计算量的同时保持嵌入质量和分类性能。

Abstract: This paper proposes two algorithms for estimating square Wasserstein distance
matrices from a small number of entries. These matrices are used to compute
manifold learning embeddings like multidimensional scaling (MDS) or Isomap, but
contrary to Euclidean distance matrices, are extremely costly to compute. We
analyze matrix completion from upper triangular samples and Nystr\"{o}m
completion in which $\mathcal{O}(d\log(d))$ columns of the distance matrices
are computed where $d$ is the desired embedding dimension, prove stability of
MDS under Nystr\"{o}m completion, and show that it can outperform matrix
completion for a fixed budget of sample distances. Finally, we show that
classification of the OrganCMNIST dataset from the MedMNIST benchmark is stable
on data embedded from the Nystr\"{o}m estimation of the distance matrix even
when only 10\% of the columns are computed.

</details>


### [150] [A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion Models](https://arxiv.org/abs/2509.19276)
*Tim Y. J. Wang,O. Deniz Akyildiz*

Main category: stat.ML

TL;DR: 提出了一种名为Diffusion-regularized Wasserstein Gradient Flow (DWGF)的无训练方法，利用预训练的潜在扩散模型解决不适定逆问题。


<details>
  <summary>Details</summary>
Motivation: 解决不适定逆问题需要强大且灵活的先验知识，预训练的潜在扩散模型为此提供了良好的基础。

Method: 将后验采样问题表述为潜在空间中Kullback-Leibler散度的正则化Wasserstein梯度流，使用StableDiffusion作为先验模型。

Result: 在标准基准测试中展示了方法的性能。

Conclusion: DWGF方法能够有效利用预训练扩散模型解决逆问题，无需额外训练。

Abstract: Solving ill-posed inverse problems requires powerful and flexible priors. We
propose leveraging pretrained latent diffusion models for this task through a
new training-free approach, termed Diffusion-regularized Wasserstein Gradient
Flow (DWGF). Specifically, we formulate the posterior sampling problem as a
regularized Wasserstein gradient flow of the Kullback-Leibler divergence in the
latent space. We demonstrate the performance of our method on standard
benchmarks using StableDiffusion (Rombach et al., 2022) as the prior.

</details>
