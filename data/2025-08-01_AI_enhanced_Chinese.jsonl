{"id": "2507.22906", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.22906", "abs": "https://arxiv.org/abs/2507.22906", "authors": ["Bin Deng", "Jiatong Bai", "Feilong Zhao", "Zuming Xie", "Maolin Li", "Yan Wang", "Feng Shu"], "title": "DNN-based Methods of Jointly Sensing Number and Directions of Targets via a Green Massive H2AD MIMO Receiver", "comment": null, "summary": "As a green MIMO structure, the heterogeneous hybrid analog-digital H2AD MIMO\narchitecture has been shown to own a great potential to replace the massive or\nextremely large-scale fully-digital MIMO in the future wireless networks to\naddress the three challenging problems faced by the latter: high energy\nconsumption, high circuit cost, and high complexity. However, how to\nintelligently sense the number and direction of multi-emitters via such a\nstructure is still an open hard problem. To address this, we propose a\ntwo-stage sensing framework that jointly estimates the number and direction\nvalues of multiple targets. Specifically, three target number sensing methods\nare designed: an improved eigen-domain clustering (EDC) framework, an enhanced\ndeep neural network (DNN) based on five key statistical features, and an\nimproved one-dimensional convolutional neural network (1D-CNN) utilizing full\neigenvalues. Subsequently, a low-complexity and high-accuracy DOA estimation is\nachieved via the introduced online micro-clustering (OMC-DOA) method.\nFurthermore, we derive the Cram\\'er-Rao lower bound (CRLB) for the H2AD under\nmultiple-source conditions as a theoretical performance benchmark. Simulation\nresults show that the developed three methods achieve 100\\% number of targets\nsensing at moderate-to-high SNRs, while the improved 1D-CNN exhibits superior\nunder extremely-low SNR conditions. The introduced OMC-DOA outperforms existing\nclustering and fusion-based DOA methods in multi-source environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u5f02\u6784\u6df7\u5408\u6a21\u62df-\u6570\u5b57H2AD MIMO\u67b6\u6784\u4e2d\u591a\u53d1\u5c04\u6e90\u7684\u6570\u91cf\u548c\u65b9\u5411\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u9ad8\u80fd\u8017\u3001\u9ad8\u6210\u672c\u548c\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u5168\u6570\u5b57MIMO\u9762\u4e34\u7684\u9ad8\u80fd\u8017\u3001\u9ad8\u6210\u672c\u548c\u590d\u6742\u6027\u6311\u6218\uff0c\u540c\u65f6\u89e3\u51b3\u5f02\u6784\u6df7\u5408\u6a21\u62df-\u6570\u5b57H2AD MIMO\u67b6\u6784\u4e2d\u591a\u53d1\u5c04\u6e90\u6570\u91cf\u548c\u65b9\u5411\u611f\u77e5\u7684\u96be\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u611f\u77e5\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u79cd\u76ee\u6807\u6570\u91cf\u611f\u77e5\u65b9\u6cd5\uff08\u6539\u8fdb\u7684EDC\u3001\u589e\u5f3a\u7684DNN\u548c1D-CNN\uff09\u548c\u4f4e\u590d\u6742\u5ea6\u9ad8\u7cbe\u5ea6\u7684OMC-DOA\u65b9\u5411\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u591a\u6e90\u6761\u4ef6\u4e0b\u7684CRLB\u3002", "result": "\u4e09\u79cd\u76ee\u6807\u6570\u91cf\u611f\u77e5\u65b9\u6cd5\u5728\u4e2d\u9ad8\u4fe1\u566a\u6bd4\u4e0b\u8fbe\u5230100%\u51c6\u786e\u7387\uff0c1D-CNN\u5728\u6781\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u8868\u73b0\u4f18\u5f02\uff1bOMC-DOA\u5728\u591a\u6e90\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86H2AD MIMO\u67b6\u6784\u4e2d\u7684\u591a\u6e90\u611f\u77e5\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.22909", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22909", "abs": "https://arxiv.org/abs/2507.22909", "authors": ["Yin Zhang", "Jiayi Zhang", "Bokai Xu", "Yuanbin Chen", "Zhilong Liu", "Jiakang Zheng", "Enyu Shi", "Ziheng Liu", "Tierui Gong", "Wei E. I. Sha", "Chau Yuen", "Shi Jin", "Bo Ai"], "title": "Rydberg Atomic Receivers for Wireless Communications: Fundamentals, Potential, Applications, and Challenges", "comment": null, "summary": "Rydberg atomic receivers (RARs) leverage the quantum coherence of highly\nexcited atoms to overcome the intrinsic physical limitations of conventional\nradio frequency receivers (RFRs), particularly in sensitivity, and bandwidth.\nThis innovative technology represents a paradigm shift in wireless\ncommunication systems. This paper systematically explains the fundamental\nsensing mechanisms of RARs, contrasts their differences from RFRs in working\nprinciples and architectures. We explore their advantages in emerging wireless\ncommunication scenarios, such as integrated sensing and communications, quantum\nRydberg radar, and quantum space communications. Practical challenges, such as\nlimited instantaneous bandwidth and nonlinear distortion, are identified. To\naddress these issues, mitigation strategies and future research directions are\nalso outlined, supporting the advancement of RAR-aided wireless systems.", "AI": {"tldr": "Rydberg\u539f\u5b50\u63a5\u6536\u5668\uff08RARs\uff09\u5229\u7528\u9ad8\u6fc0\u53d1\u539f\u5b50\u7684\u91cf\u5b50\u76f8\u5e72\u6027\uff0c\u514b\u670d\u4f20\u7edf\u5c04\u9891\u63a5\u6536\u5668\uff08RFRs\uff09\u7684\u56fa\u6709\u7269\u7406\u9650\u5236\uff0c\u63d0\u5347\u7075\u654f\u5ea6\u548c\u5e26\u5bbd\u3002\u672c\u6587\u7cfb\u7edf\u9610\u8ff0RARs\u7684\u4f20\u611f\u673a\u5236\uff0c\u5bf9\u6bd4\u5176\u4e0eRFRs\u7684\u5de5\u4f5c\u539f\u7406\u548c\u67b6\u6784\u5dee\u5f02\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u65e0\u7ebf\u901a\u4fe1\u573a\u666f\u4e2d\u7684\u4f18\u52bf\u3002\u540c\u65f6\u6307\u51fa\u5b9e\u9645\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u7b56\u7565\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u5c04\u9891\u63a5\u6536\u5668\uff08RFRs\uff09\u5728\u7075\u654f\u5ea6\u548c\u5e26\u5bbd\u65b9\u9762\u5b58\u5728\u56fa\u6709\u7269\u7406\u9650\u5236\uff0c\u800cRydberg\u539f\u5b50\u63a5\u6536\u5668\uff08RARs\uff09\u901a\u8fc7\u91cf\u5b50\u76f8\u5e72\u6027\u63d0\u4f9b\u4e86\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5e26\u6765\u9769\u65b0\u3002", "method": "\u7cfb\u7edf\u5206\u6790RARs\u7684\u4f20\u611f\u673a\u5236\uff0c\u5bf9\u6bd4\u5176\u4e0eRFRs\u7684\u5de5\u4f5c\u539f\u7406\u548c\u67b6\u6784\u5dee\u5f02\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\u3001\u91cf\u5b50Rydberg\u96f7\u8fbe\u548c\u91cf\u5b50\u7a7a\u95f4\u901a\u4fe1\u7b49\u65b0\u5174\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "result": "RARs\u5728\u7075\u654f\u5ea6\u548c\u5e26\u5bbd\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5728\u77ac\u65f6\u5e26\u5bbd\u548c\u975e\u7ebf\u6027\u5931\u771f\u7b49\u65b9\u9762\u5b58\u5728\u5b9e\u9645\u6311\u6218\u3002", "conclusion": "RARs\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5e26\u6765\u9769\u65b0\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u89e3\u51b3\u5b9e\u9645\u6311\u6218\uff0c\u4ee5\u63a8\u52a8\u5176\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2507.23057", "categories": ["eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.23057", "abs": "https://arxiv.org/abs/2507.23057", "authors": ["Triet M. Tran", "Sina Khanmohammadi"], "title": "Neural Energy Landscapes Predict Working Memory Decline After Brain Tumor Resection", "comment": null, "summary": "Surgical resection is the primary treatment option for brain tumor patients,\nbut it carries the risk of postoperative cognitive dysfunction. This study\ninvestigates how tumor-induced alterations in presurgical neural dynamics\nrelate to postoperative working memory decline. We analyzed functional magnetic\nresonance imaging (fMRI) of brain tumor patients before surgery and extracted\nenergy landscapes of high-order brain interactions. We then examined the\nrelation between these energy features and postoperative working memory\nperformance using statistical and machine learning (random forest) models.\nPatients with lower postoperative working memory scores exhibited fewer but\nmore extreme transitions between local energy minima and maxima, whereas\npatients with higher scores showed more frequent but less extreme shifts.\nFurthermore, the presurgical high-order energy features were able to accurately\npredict postoperative working memory decline with a mean accuracy of 90\\%, F1\nscore of 87.5\\%, and an AUC of 0.95. Our study suggests that the brain\ntumor-induced disruptions in high-order neural dynamics before surgery are\npredictive of postoperative working memory decline. Our findings pave the path\nfor personalized surgical planning and targeted interventions to mitigate\ncognitive risks associated with brain tumor resection.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u8111\u80bf\u7624\u60a3\u8005\u672f\u524d\u795e\u7ecf\u52a8\u529b\u5b66\u53d8\u5316\u4e0e\u672f\u540e\u5de5\u4f5c\u8bb0\u5fc6\u8870\u9000\u7684\u5173\u7cfb\uff0c\u901a\u8fc7fMRI\u5206\u6790\u9ad8\u9636\u8111\u4ea4\u4e92\u80fd\u91cf\u7279\u5f81\uff0c\u5e76\u5229\u7528\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u672f\u540e\u8ba4\u77e5\u529f\u80fd\u4e0b\u964d\u3002", "motivation": "\u8111\u80bf\u7624\u624b\u672f\u5207\u9664\u53ef\u80fd\u5bfc\u81f4\u672f\u540e\u8ba4\u77e5\u529f\u80fd\u969c\u788d\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u672f\u524d\u795e\u7ecf\u52a8\u529b\u5b66\u7279\u5f81\u9884\u6d4b\u672f\u540e\u5de5\u4f5c\u8bb0\u5fc6\u8870\u9000\uff0c\u4e3a\u4e2a\u6027\u5316\u624b\u672f\u89c4\u5212\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u5206\u6790\u672f\u524dfMRI\u6570\u636e\uff0c\u63d0\u53d6\u9ad8\u9636\u8111\u4ea4\u4e92\u80fd\u91cf\u7279\u5f81\uff0c\u4f7f\u7528\u7edf\u8ba1\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u8bc4\u4f30\u4e0e\u672f\u540e\u5de5\u4f5c\u8bb0\u5fc6\u8868\u73b0\u7684\u5173\u7cfb\u3002", "result": "\u672f\u540e\u5de5\u4f5c\u8bb0\u5fc6\u8f83\u4f4e\u7684\u60a3\u8005\u8868\u73b0\u51fa\u66f4\u5c11\u4f46\u66f4\u6781\u7aef\u7684\u80fd\u91cf\u6781\u503c\u8f6c\u6362\uff0c\u800c\u9ad8\u5206\u60a3\u8005\u8f6c\u6362\u66f4\u9891\u7e41\u4f46\u5e45\u5ea6\u8f83\u5c0f\u3002\u6a21\u578b\u9884\u6d4b\u672f\u540e\u8870\u9000\u7684\u51c6\u786e\u7387\u8fbe90%\uff0cAUC\u4e3a0.95\u3002", "conclusion": "\u672f\u524d\u9ad8\u9636\u795e\u7ecf\u52a8\u529b\u5b66\u7279\u5f81\u53ef\u9884\u6d4b\u672f\u540e\u5de5\u4f5c\u8bb0\u5fc6\u8870\u9000\uff0c\u4e3a\u4e2a\u6027\u5316\u624b\u672f\u548c\u5e72\u9884\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.23235", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.23235", "abs": "https://arxiv.org/abs/2507.23235", "authors": ["Mohammad Roueinfar", "Masoud Ardini"], "title": "In-Orbit Cosmo-SkyMed antenna pattern estimation by a narrowband sweeper receiver", "comment": null, "summary": "This paper introduces a novel method for antenna pattern estimation in\nsatellites equipped with Synthetic Aperture Radar (SAR), utilizing a Narrowband\nSweeper Receiver (NSR). By accurately measuring power across individual\nfrequencies within SAR's inherently broadband spectrum, the NSR significantly\nenhances antenna pattern extraction accuracy. Analytical models and practical\nexperiments conducted using the Cosmo-SkyMed satellite validate the receiver's\nperformance, demonstrating superior signal-to-noise ratio (SNR) compared to\nconventional receivers. This research represents a key advancement in SAR\ntechnology, offering a robust framework for future satellite calibration and\nverification methodologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7a84\u5e26\u626b\u63cf\u63a5\u6536\u5668\uff08NSR\uff09\u4f30\u8ba1\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u536b\u661f\u5929\u7ebf\u65b9\u5411\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728SAR\u5bbd\u5e26\u9891\u8c31\u4e2d\u63d0\u53d6\u5929\u7ebf\u65b9\u5411\u56fe\u65f6\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7NSR\u5728SAR\u9891\u8c31\u4e2d\u9010\u4e2a\u9891\u7387\u7cbe\u786e\u6d4b\u91cf\u529f\u7387\uff0c\u7ed3\u5408\u5206\u6790\u6a21\u578b\u548cCosmo-SkyMed\u536b\u661f\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "NSR\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u63a5\u6536\u5668\u7684\u4fe1\u566a\u6bd4\uff08SNR\uff09\uff0c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aSAR\u6280\u672f\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u4e3a\u672a\u6765\u536b\u661f\u6821\u51c6\u548c\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u9760\u6846\u67b6\u3002"}}
{"id": "2507.23017", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.23017", "abs": "https://arxiv.org/abs/2507.23017", "authors": ["Tyler Maunu", "Gabriel Abreu"], "title": "A Smoothing Newton Method for Rank-one Matrix Recovery", "comment": "12 pages, 4 figures", "summary": "We consider the phase retrieval problem, which involves recovering a rank-one\npositive semidefinite matrix from rank-one measurements. A recently proposed\nalgorithm based on Bures-Wasserstein gradient descent (BWGD) exhibits\nsuperlinear convergence, but it is unstable, and existing theory can only prove\nlocal linear convergence for higher rank matrix recovery. We resolve this gap\nby revealing that BWGD implements Newton's method with a nonsmooth and\nnonconvex objective. We develop a smoothing framework that regularizes the\nobjective, enabling a stable method with rigorous superlinear convergence\nguarantees. Experiments on synthetic data demonstrate this superior stability\nwhile maintaining fast convergence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u6ed1\u6846\u67b6\u7684\u7a33\u5b9a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86Bures-Wasserstein\u68af\u5ea6\u4e0b\u964d\uff08BWGD\uff09\u5728\u76f8\u4f4d\u6062\u590d\u95ee\u9898\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u8d85\u7ebf\u6027\u6536\u655b\u3002", "motivation": "BWGD\u7b97\u6cd5\u5728\u76f8\u4f4d\u6062\u590d\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8d85\u7ebf\u6027\u6536\u655b\uff0c\u4f46\u4e0d\u7a33\u5b9a\uff0c\u4e14\u73b0\u6709\u7406\u8bba\u53ea\u80fd\u8bc1\u660e\u9ad8\u9636\u77e9\u9635\u6062\u590d\u7684\u5c40\u90e8\u7ebf\u6027\u6536\u655b\u3002", "method": "\u901a\u8fc7\u63ed\u793aBWGD\u5b9e\u73b0\u4e86\u975e\u5149\u6ed1\u3001\u975e\u51f8\u76ee\u6807\u7684\u725b\u987f\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5e73\u6ed1\u6846\u67b6\u6765\u6b63\u5219\u5316\u76ee\u6807\uff0c\u4ece\u800c\u83b7\u5f97\u7a33\u5b9a\u7684\u8d85\u7ebf\u6027\u6536\u655b\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5feb\u901f\u6536\u655b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e73\u6ed1\u6846\u67b6\u89e3\u51b3\u4e86BWGD\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8d85\u7ebf\u6027\u6536\u655b\u4fdd\u8bc1\u3002"}}
{"id": "2507.22954", "categories": ["cs.LG", "eess.IV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.22954", "abs": "https://arxiv.org/abs/2507.22954", "authors": ["Ridvan Yesiloglu", "Wei Peng", "Md Tauhidul Islam", "Ehsan Adeli"], "title": "Neural Autoregressive Modeling of Brain Aging", "comment": "Accepted at Deep Generative Models Workshop @ MICCAI 2025", "summary": "Brain aging synthesis is a critical task with broad applications in clinical\nand computational neuroscience. The ability to predict the future structural\nevolution of a subject's brain from an earlier MRI scan provides valuable\ninsights into aging trajectories. Yet, the high-dimensionality of data, subtle\nchanges of structure across ages, and subject-specific patterns constitute\nchallenges in the synthesis of the aging brain. To overcome these challenges,\nwe propose NeuroAR, a novel brain aging simulation model based on generative\nautoregressive transformers. NeuroAR synthesizes the aging brain by\nautoregressively estimating the discrete token maps of a future scan from a\nconvenient space of concatenated token embeddings of a previous and future\nscan. To guide the generation, it concatenates into each scale the subject's\nprevious scan, and uses its acquisition age and the target age at each block\nvia cross-attention. We evaluate our approach on both the elderly population\nand adolescent subjects, demonstrating superior performance over\nstate-of-the-art generative models, including latent diffusion models (LDM) and\ngenerative adversarial networks, in terms of image fidelity. Furthermore, we\nemploy a pre-trained age predictor to further validate the consistency and\nrealism of the synthesized images with respect to expected aging patterns.\nNeuroAR significantly outperforms key models, including LDM, demonstrating its\nability to model subject-specific brain aging trajectories with high fidelity.", "AI": {"tldr": "NeuroAR\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u81ea\u56de\u5f52\u53d8\u6362\u5668\u7684\u65b0\u578b\u8111\u8001\u5316\u6a21\u62df\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u4f30\u8ba1\u672a\u6765\u626b\u63cf\u7684\u79bb\u6563\u6807\u8bb0\u56fe\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u548c\u4e2a\u4f53\u7279\u5f02\u6027\u6a21\u5f0f\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u8111\u8001\u5316\u5408\u6210\u5728\u4e34\u5e8a\u548c\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9ad8\u7ef4\u6570\u636e\u3001\u7ec6\u5fae\u7ed3\u6784\u53d8\u5316\u548c\u4e2a\u4f53\u7279\u5f02\u6027\u6a21\u5f0f\u662f\u5176\u5408\u6210\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "NeuroAR\u5229\u7528\u751f\u6210\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u4f30\u8ba1\u672a\u6765\u626b\u63cf\u7684\u79bb\u6563\u6807\u8bb0\u56fe\uff0c\u5e76\u7ed3\u5408\u5148\u524d\u7684\u626b\u63cf\u548c\u76ee\u6807\u5e74\u9f84\u8fdb\u884c\u5f15\u5bfc\u751f\u6210\u3002", "result": "NeuroAR\u5728\u8001\u5e74\u548c\u9752\u5c11\u5e74\u7fa4\u4f53\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u751f\u6210\u6a21\u578b\uff08\u5982LDM\u548cGAN\uff09\uff0c\u56fe\u50cf\u4fdd\u771f\u5ea6\u66f4\u9ad8\u3002", "conclusion": "NeuroAR\u80fd\u591f\u9ad8\u4fdd\u771f\u5730\u6a21\u62df\u4e2a\u4f53\u7279\u5f02\u6027\u8111\u8001\u5316\u8f68\u8ff9\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5173\u952e\u6a21\u578b\u3002"}}
{"id": "2507.23236", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.23236", "abs": "https://arxiv.org/abs/2507.23236", "authors": ["Zhuoyin Dai", "Di Wu", "Yong Zeng", "Xiaoli Xu", "Xinyi Wang", "Zesong Fei"], "title": "BS-1-to-N: Diffusion-Based Environment-Aware Cross-BS Channel Knowledge Map Generation for Cell-Free Networks", "comment": null, "summary": "Channel knowledge map (CKM) inference across base stations (BSs) is the key\nto achieving efficient environmentaware communications. This paper proposes an\nenvironmentaware cross-BS CKM inference method called BS-1-to-N based on the\ngenerative diffusion model. To this end, we first design the BS location\nembedding (BSLE) method tailored for cross-BS CKM inference to embed BS\nlocation information in the feature vector of CKM. Further, we utilize the\ncross- and self-attention mechanism for the proposed BS-1-to-N model to\nrespectively learn the relationships between source and target BSs, as well as\nthat among target BSs. Therefore, given the locations of the source and target\nBSs, together with the source CKMs as control conditions, cross-BS CKM\ninference can be performed for an arbitrary number of source and target BSs.\nSpecifically, in architectures with massive distributed nodes like cell-free\nnetworks, traditional methods of sequentially traversing each BS for CKM\nconstruction are prohibitively costly. By contrast, the proposed BS-1-to-N\nmodel is able to achieve efficient CKM inference for a target BS at any\npotential location based on the CKMs of source BSs. This is achieved by\nexploiting the fact that within a given area, different BSs share the same\nwireless environment that leads to their respective CKMs. Therefore, similar to\nmulti-view synthesis, CKMs of different BSs are representations of the same\nwireless environment from different BS locations. By mining the implicit\ncorrelation between CKM and BS location based on the wireless environment, the\nproposed BS-1-to-N method achieves efficient CKM inference across BSs. We\nprovide extensive comparisons of CKM inference between the proposed BS-1-to-N\ngenerative model versus benchmarking schemes, and provide one use case study to\ndemonstrate its practical application for the optimization of BS deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u73af\u5883\u611f\u77e5\u8de8\u57fa\u7ad9CKM\u63a8\u7406\u65b9\u6cd5BS-1-to-N\uff0c\u901a\u8fc7BSLE\u5d4c\u5165\u57fa\u7ad9\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5229\u7528\u4ea4\u53c9\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u8282\u70b9\u67b6\u6784\u4e2d\u6784\u5efaCKM\u6210\u672c\u9ad8\u6602\uff0c\u800c\u4e0d\u540c\u57fa\u7ad9\u7684CKM\u5171\u4eab\u76f8\u540c\u7684\u65e0\u7ebf\u73af\u5883\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u73af\u5883\u611f\u77e5\u8de8\u57fa\u7ad9CKM\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86BSLE\u65b9\u6cd5\u5d4c\u5165\u57fa\u7ad9\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5229\u7528\u4ea4\u53c9\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u6e90\u4e0e\u76ee\u6807\u57fa\u7ad9\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u76ee\u6807\u57fa\u7ad9\u95f4\u7684\u5173\u7cfb\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6e90CKM\u548c\u4f4d\u7f6e\u4fe1\u606f\u7684\u8de8\u57fa\u7ad9CKM\u63a8\u7406\u3002", "result": "\u63d0\u51fa\u7684BS-1-to-N\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u5730\u4e3a\u4efb\u610f\u6f5c\u5728\u4f4d\u7f6e\u7684\u57fa\u7ad9\u63a8\u65adCKM\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u5728\u57fa\u7ad9\u90e8\u7f72\u4f18\u5316\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "BS-1-to-N\u65b9\u6cd5\u901a\u8fc7\u6316\u6398CKM\u4e0e\u57fa\u7ad9\u4f4d\u7f6e\u7684\u9690\u542b\u5173\u8054\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8de8\u57fa\u7ad9CKM\u63a8\u7406\uff0c\u4e3a\u73af\u5883\u611f\u77e5\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.23349", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23349", "abs": "https://arxiv.org/abs/2507.23349", "authors": ["Wenhai Cui", "Xiaoting Ji", "Wen Su", "Xiaodong Yan", "Xingqiu Zhao"], "title": "Optimal Transport Learning: Balancing Value Optimization and Fairness in Individualized Treatment Rules", "comment": null, "summary": "Individualized treatment rules (ITRs) have gained significant attention due\nto their wide-ranging applications in fields such as precision medicine,\nridesharing, and advertising recommendations. However, when ITRs are influenced\nby sensitive attributes such as race, gender, or age, they can lead to outcomes\nwhere certain groups are unfairly advantaged or disadvantaged. To address this\ngap, we propose a flexible approach based on the optimal transport theory,\nwhich is capable of transforming any optimal ITR into a fair ITR that ensures\ndemographic parity. Recognizing the potential loss of value under fairness\nconstraints, we introduce an ``improved trade-off ITR,\" designed to balance\nvalue optimization and fairness while accommodating varying levels of fairness\nthrough parameter adjustment. To maximize the value of the improved trade-off\nITR under specific fairness levels, we propose a smoothed fairness constraint\nfor estimating the adjustable parameter. Additionally, we establish a\ntheoretical upper bound on the value loss for the improved trade-off ITR. We\ndemonstrate performance of the proposed method through extensive simulation\nstudies and application to the Next 36 entrepreneurial program dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u7684\u7075\u6d3b\u65b9\u6cd5\uff0c\u5c06\u4efb\u4f55\u6700\u4f18\u4e2a\u4f53\u5316\u6cbb\u7597\u89c4\u5219\uff08ITR\uff09\u8f6c\u5316\u4e3a\u516c\u5e73\u7684ITR\uff0c\u786e\u4fdd\u4eba\u53e3\u7edf\u8ba1\u5b66\u5e73\u7b49\u3002\u901a\u8fc7\u5f15\u5165\u201c\u6539\u8fdb\u7684\u6743\u8861ITR\u201d\u5e73\u8861\u4ef7\u503c\u4f18\u5316\u4e0e\u516c\u5e73\u6027\uff0c\u5e76\u901a\u8fc7\u5e73\u6ed1\u516c\u5e73\u7ea6\u675f\u4f18\u5316\u53c2\u6570\u8c03\u6574\u3002", "motivation": "\u89e3\u51b3ITR\u4e2d\u56e0\u654f\u611f\u5c5e\u6027\uff08\u5982\u79cd\u65cf\u3001\u6027\u522b\u3001\u5e74\u9f84\uff09\u5bfc\u81f4\u7684\u4e0d\u516c\u5e73\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\uff0c\u63d0\u51fa\u6539\u8fdb\u7684\u6743\u8861ITR\u548c\u5e73\u6ed1\u516c\u5e73\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u548c\u5b9e\u9645\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5efa\u7acb\u4e86\u7406\u8bba\u4e0a\u7684\u4ef7\u503c\u635f\u5931\u4e0a\u9650\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u4ef7\u503c\u4f18\u5316\u4e0e\u516c\u5e73\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.22956", "categories": ["cs.LG", "cs.HC", "K.3.1"], "pdf": "https://arxiv.org/pdf/2507.22956", "abs": "https://arxiv.org/abs/2507.22956", "authors": ["Dong Hyun Roh", "Rajesh Kumar", "An Ngo"], "title": "LLM-Assisted Cheating Detection in Korean Language via Keystrokes", "comment": "This paper has 11 pages, 6 figures, 2 tables, and has been accepted\n  for publication at IEEE-IJCB 2025", "summary": "This paper presents a keystroke-based framework for detecting LLM-assisted\ncheating in Korean, addressing key gaps in prior research regarding language\ncoverage, cognitive context, and the granularity of LLM involvement. Our\nproposed dataset includes 69 participants who completed writing tasks under\nthree conditions: Bona fide writing, paraphrasing ChatGPT responses, and\ntranscribing ChatGPT responses. Each task spans six cognitive processes defined\nin Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and\ncreate). We extract interpretable temporal and rhythmic features and evaluate\nmultiple classifiers under both Cognition-Aware and Cognition-Unaware settings.\nTemporal features perform well under Cognition-Aware evaluation scenarios,\nwhile rhythmic features generalize better under cross-cognition scenarios.\nMoreover, detecting bona fide and transcribed responses was easier than\nparaphrased ones for both the proposed models and human evaluators, with the\nmodels significantly outperforming the humans. Our findings affirm that\nkeystroke dynamics facilitate reliable detection of LLM-assisted writing across\nvarying cognitive demands and writing strategies, including paraphrasing and\ntranscribing LLM-generated responses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51fb\u952e\u52a8\u6001\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u97e9\u8bed\u4e2dLLM\u8f85\u52a9\u7684\u4f5c\u5f0a\u884c\u4e3a\uff0c\u586b\u8865\u4e86\u8bed\u8a00\u8986\u76d6\u3001\u8ba4\u77e5\u4e0a\u4e0b\u6587\u548cLLM\u53c2\u4e0e\u7c92\u5ea6\u7b49\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u8bed\u8a00\u8986\u76d6\u4e0d\u8db3\u3001\u8ba4\u77e5\u4e0a\u4e0b\u6587\u5ffd\u7565\u4ee5\u53caLLM\u53c2\u4e0e\u7c92\u5ea6\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5305\u542b69\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u96c6\uff0c\u5206\u4e3a\u771f\u5b9e\u5199\u4f5c\u3001\u6539\u5199ChatGPT\u56de\u7b54\u548c\u8f6c\u5f55ChatGPT\u56de\u7b54\u4e09\u79cd\u6761\u4ef6\uff0c\u6db5\u76d6Bloom\u5206\u7c7b\u6cd5\u4e2d\u7684\u516d\u79cd\u8ba4\u77e5\u8fc7\u7a0b\u3002\u63d0\u53d6\u65f6\u95f4\u548c\u8282\u594f\u7279\u5f81\uff0c\u5e76\u5728\u8ba4\u77e5\u611f\u77e5\u548c\u8ba4\u77e5\u65e0\u611f\u77e5\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u5206\u7c7b\u5668\u3002", "result": "\u65f6\u95f4\u7279\u5f81\u5728\u8ba4\u77e5\u611f\u77e5\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u8282\u594f\u7279\u5f81\u5728\u8de8\u8ba4\u77e5\u573a\u666f\u4e2d\u66f4\u5177\u6cdb\u5316\u6027\u3002\u68c0\u6d4b\u771f\u5b9e\u548c\u8f6c\u5f55\u56de\u7b54\u6bd4\u6539\u5199\u56de\u7b54\u66f4\u5bb9\u6613\uff0c\u6a21\u578b\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "conclusion": "\u51fb\u952e\u52a8\u6001\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4b\u4e0d\u540c\u8ba4\u77e5\u9700\u6c42\u548c\u5199\u4f5c\u7b56\u7565\u4e0b\u7684LLM\u8f85\u52a9\u5199\u4f5c\uff0c\u5305\u62ec\u6539\u5199\u548c\u8f6c\u5f55LLM\u751f\u6210\u7684\u56de\u7b54\u3002"}}
{"id": "2507.23381", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.23381", "abs": "https://arxiv.org/abs/2507.23381", "authors": ["Ziang Liu", "Bruno Clerckx"], "title": "A Secure Full-Duplex Wireless Circulator enabled by Non-Reciprocal Beyond-Diagonal RIS", "comment": "Submitted for IEEE journal", "summary": "Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has arisen as a\npromising technology for enhancing wireless communication systems by enabling\nflexible and intelligent wave manipulation. This is achieved through the\ninterconnections among the ports of the impedance network, enabling wave\nreconfiguration when they flow through the surface. Thus, the output wave at\none port depends on waves impinging on neighboring ports, allowing non-local\ncontrol of both phase and magnitude. Non-reciprocal (NR)-BD-RIS further\nenhances this capability by breaking circuit reciprocity and, consequently,\nchannel reciprocity. This feature potentially benefits communication among\nnon-aligned transceivers. This paper introduces a novel application of\nNR-BD-RIS in full-duplex (FD) wireless circulators, where multiple FD devices\ncommunicate via an NR-BD-RIS. This system is particularly beneficial for secure\ntransmission, as it enforces one-way communication among FD devices, suppresses\nsignal from all other users, and thus prevents eavesdropping. In addition, a\nphysics-compliant system model is considered by incorporating structural\nscattering, also known as specular reflection. By accounting for this effect,\nthe advantages of NR-BD-RIS are further validated. Specifically, we formulate\nan all-user sum-rate maximization problem and propose an iterative optimization\nalgorithm that employs block coordinate descent (BCD) and penalty dual\ndecomposition (PDD) methods. Numerical evaluations illustrate that NR-BD-RIS\nconsistently outperforms reciprocal (R)-BD-RIS and conventional diagonal\n(D)-RIS in terms of sum-rate performance, particularly when more than two\nimpinging and reflection directions need to be supported. By analyzing the\npower of signals from all other users and the beampatterns, we show that secure\ntransmission can be achieved.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u4e92\u6613\u6027\u8d85\u8868\u9762\uff08NR-BD-RIS\uff09\u5728\u5168\u53cc\u5de5\u65e0\u7ebf\u5faa\u73af\u5668\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6253\u7834\u7535\u8def\u4e92\u6613\u6027\u5b9e\u73b0\u5355\u5411\u901a\u4fe1\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRIS\u5728\u975e\u5bf9\u9f50\u6536\u53d1\u5668\u901a\u4fe1\u4e2d\u53d7\u9650\uff0cNR-BD-RIS\u901a\u8fc7\u975e\u4e92\u6613\u6027\u589e\u5f3a\u6ce2\u63a7\u5236\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5168\u53cc\u5de5\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eNR-BD-RIS\u7684\u7cfb\u7edf\u6a21\u578b\uff0c\u7ed3\u5408\u7269\u7406\u6563\u5c04\u6548\u5e94\uff0c\u91c7\u7528BCD\u548cPDD\u7b97\u6cd5\u4f18\u5316\u591a\u7528\u6237\u548c\u901f\u7387\u3002", "result": "NR-BD-RIS\u5728\u548c\u901f\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edfRIS\uff0c\u5c24\u5176\u5728\u591a\u65b9\u5411\u6ce2\u675f\u63a7\u5236\u65f6\uff0c\u4e14\u80fd\u5b9e\u73b0\u5b89\u5168\u4f20\u8f93\u3002", "conclusion": "NR-BD-RIS\u4e3a\u5168\u53cc\u5de5\u901a\u4fe1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2507.23736", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23736", "abs": "https://arxiv.org/abs/2507.23736", "authors": ["Kyle Naddeo", "Nikolas Koutsoubis", "Rahul Krish", "Ghulam Rasool", "Nidhal Bouaynaya", "Tony OSullivan", "Raj Krish"], "title": "DICOM De-Identification via Hybrid AI and Rule-Based Framework for Scalable, Uncertainty-Aware Redaction", "comment": "15 pages, 6 figures,", "summary": "Access to medical imaging and associated text data has the potential to drive\nmajor advances in healthcare research and patient outcomes. However, the\npresence of Protected Health Information (PHI) and Personally Identifiable\nInformation (PII) in Digital Imaging and Communications in Medicine (DICOM)\nfiles presents a significant barrier to the ethical and secure sharing of\nimaging datasets. This paper presents a hybrid de-identification framework\ndeveloped by Impact Business Information Solutions (IBIS) that combines\nrule-based and AI-driven techniques, and rigorous uncertainty quantification\nfor comprehensive PHI/PII removal from both metadata and pixel data.\n  Our approach begins with a two-tiered rule-based system targeting explicit\nand inferred metadata elements, further augmented by a large language model\n(LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of\nsynthetic datasets simulating realistic clinical PHI/PII. For pixel data, we\nemploy an uncertainty-aware Faster R-CNN model to localize embedded text,\nextract candidate PHI via Optical Character Recognition (OCR), and apply the\nNER pipeline for final redaction. Crucially, uncertainty quantification\nprovides confidence measures for AI-based detections to enhance automation\nreliability and enable informed human-in-the-loop verification to manage\nresidual risks.\n  This uncertainty-aware deidentification framework achieves robust performance\nacross benchmark datasets and regulatory standards, including DICOM, HIPAA, and\nTCIA compliance metrics. By combining scalable automation, uncertainty\nquantification, and rigorous quality assurance, our solution addresses critical\nchallenges in medical data de-identification and supports the secure, ethical,\nand trustworthy release of imaging data for research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u53bb\u6807\u8bc6\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u89c4\u5219\u548cAI\u6280\u672f\uff0c\u7528\u4e8e\u5b89\u5168\u53bb\u9664\u533b\u5b66\u5f71\u50cf\u6570\u636e\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u6570\u636e\u5171\u4eab\u5bf9\u533b\u7597\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u4e2d\u7684\u654f\u611f\u4fe1\u606f\uff08\u5982PHI/PII\uff09\u963b\u788d\u4e86\u6570\u636e\u7684\u4f26\u7406\u548c\u5b89\u5168\u5171\u4eab\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u89c4\u5219\u7cfb\u7edf\u548cAI\u9a71\u52a8\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\uff0c\u4ece\u5143\u6570\u636e\u548c\u50cf\u7d20\u6570\u636e\u4e2d\u53bb\u9664\u654f\u611f\u4fe1\u606f\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6cd5\u89c4\u6807\u51c6\uff08\u5982DICOM\u3001HIPAA\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53bb\u6807\u8bc6\u5316\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u81ea\u52a8\u5316\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u8d28\u91cf\u4fdd\u8bc1\uff0c\u652f\u6301\u533b\u5b66\u5f71\u50cf\u6570\u636e\u7684\u5b89\u5168\u3001\u4f26\u7406\u5171\u4eab\u3002"}}
{"id": "2507.22959", "categories": ["cs.LG", "cs.CE", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.22959", "abs": "https://arxiv.org/abs/2507.22959", "authors": ["Salah A. Faroughi", "Farinaz Mostajeran", "Amin Hamed Mashhadzadeh", "Shirko Faroughi"], "title": "Scientific Machine Learning with Kolmogorov-Arnold Networks", "comment": null, "summary": "The field of scientific machine learning, which originally utilized\nmultilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold\nNetworks (KANs) for data encoding. This shift is driven by the limitations of\nMLPs, including poor interpretability, fixed activation functions, and\ndifficulty capturing localized or high-frequency features. KANs address these\nissues with enhanced interpretability and flexibility, enabling more efficient\nmodeling of complex nonlinear interactions and effectively overcoming the\nconstraints associated with conventional MLP architectures. This review\ncategorizes recent progress in KAN-based models across three distinct\nperspectives: (i) data-driven learning, (ii) physics-informed modeling, and\n(iii) deep operator learning. Each perspective is examined through the lens of\narchitectural design, training strategies, application efficacy, and\ncomparative evaluation against MLP-based counterparts. By benchmarking KANs\nagainst MLPs, we highlight consistent improvements in accuracy, convergence,\nand spectral representation, clarifying KANs' advantages in capturing complex\ndynamics while learning more effectively. Finally, this review identifies\ncritical challenges and open research questions in KAN development,\nparticularly regarding computational efficiency, theoretical guarantees,\nhyperparameter tuning, and algorithm complexity. We also outline future\nresearch directions aimed at improving the robustness, scalability, and\nphysical consistency of KAN-based frameworks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u4ece\u591a\u5c42\u611f\u77e5\u5668\uff08MLPs\uff09\u8f6c\u5411Kolmogorov-Arnold\u7f51\u7edc\uff08KANs\uff09\u7684\u8d8b\u52bf\uff0c\u5206\u6790\u4e86KANs\u5728\u53ef\u89e3\u91ca\u6027\u3001\u7075\u6d3b\u6027\u53ca\u5efa\u6a21\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u3001\u7269\u7406\u5efa\u6a21\u548c\u6df1\u5ea6\u7b97\u5b50\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "MLPs\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u6fc0\u6d3b\u51fd\u6570\u56fa\u5b9a\u53ca\u96be\u4ee5\u6355\u6349\u5c40\u90e8\u6216\u9ad8\u9891\u7279\u5f81\u7b49\u95ee\u9898\uff0cKANs\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u67b6\u6784\u8bbe\u8ba1\u3001\u8bad\u7ec3\u7b56\u7565\u3001\u5e94\u7528\u6548\u679c\u53ca\u4e0eMLPs\u7684\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u4ece\u4e09\u4e2a\u89c6\u89d2\uff08\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u3001\u7269\u7406\u5efa\u6a21\u3001\u6df1\u5ea6\u7b97\u5b50\u5b66\u4e60\uff09\u5206\u7c7b\u5206\u6790\u4e86KANs\u7684\u8fdb\u5c55\u3002", "result": "KANs\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u6027\u548c\u9891\u8c31\u8868\u793a\u4e0a\u5747\u4f18\u4e8eMLPs\uff0c\u80fd\u66f4\u6709\u6548\u5730\u6355\u6349\u590d\u6742\u52a8\u6001\u3002", "conclusion": "KANs\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u8ba1\u7b97\u6548\u7387\u3001\u7406\u8bba\u4fdd\u8bc1\u7b49\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5176\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.23518", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.23518", "abs": "https://arxiv.org/abs/2507.23518", "authors": ["Joel Poncha Lemayian", "Hachem Bensalem", "Ghyslain Gagnon", "Kaiwen Zhang", "Pascal Giard"], "title": "EVMx: An FPGA-Based Smart Contract Processing Unit", "comment": "6 pages", "summary": "Ethereum blockchain uses smart contracts (SCs) to implement decentralized\napplications (dApps). SCs are executed by the Ethereum virtual machine (EVM)\nrunning within an Ethereum client. Moreover, the EVM has been widely adopted by\nother blockchain platforms, including Solana, Cardano, Avalanche, Polkadot, and\nmore. However, the EVM performance is limited by the constraints of the\ngeneral-purpose computer it operates on. This work proposes offloading SC\nexecution onto a dedicated hardware-based EVM. Specifically, EVMx is an\nFPGA-based SC execution engine that benefits from the inherent parallelism and\nhigh-speed processing capabilities of a hardware architecture. Synthesis\nresults demonstrate a reduction in execution time of 61% to 99% for commonly\nused operation codes compared to CPU-based SC execution environments. Moreover,\nthe execution time of Ethereum blocks on EVMx is up to 6x faster compared to\nanalogous works in the literature. These results highlight the potential of the\nproposed architecture to accelerate SC execution and enhance the performance of\nEVM-compatible blockchains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u786c\u4ef6EVM\uff08EVMx\uff09\uff0c\u7528\u4e8e\u52a0\u901f\u667a\u80fd\u5408\u7ea6\u6267\u884c\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCPU\u73af\u5883\u3002", "motivation": "EVM\u6027\u80fd\u53d7\u9650\u4e8e\u901a\u7528\u8ba1\u7b97\u673a\u7684\u7ea6\u675f\uff0c\u5f71\u54cd\u4e86\u667a\u80fd\u5408\u7ea6\u548c\u533a\u5757\u94fe\u5e73\u53f0\u7684\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86FPGA\u4e3a\u57fa\u7840\u7684\u786c\u4ef6EVM\uff08EVMx\uff09\uff0c\u5229\u7528\u786c\u4ef6\u67b6\u6784\u7684\u5e76\u884c\u6027\u548c\u9ad8\u901f\u5904\u7406\u80fd\u529b\u3002", "result": "EVMx\u5c06\u5e38\u7528\u64cd\u4f5c\u7801\u7684\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u4e8661%\u81f399%\uff0c\u533a\u5757\u6267\u884c\u901f\u5ea6\u6bd4\u6587\u732e\u4e2d\u7684\u7c7b\u4f3c\u5de5\u4f5c\u5feb6\u500d\u3002", "conclusion": "EVMx\u67b6\u6784\u6709\u671b\u663e\u8457\u63d0\u5347\u667a\u80fd\u5408\u7ea6\u6267\u884c\u901f\u5ea6\u548cEVM\u517c\u5bb9\u533a\u5757\u94fe\u7684\u6027\u80fd\u3002"}}
{"id": "2507.23767", "categories": ["stat.ML", "cs.LG", "68T05, 62H30, 62F10, 68Q32", "F.2.2; I.2.6; I.5.2; G.3"], "pdf": "https://arxiv.org/pdf/2507.23767", "abs": "https://arxiv.org/abs/2507.23767", "authors": ["Jonathan R. Landers"], "title": "Scaled Beta Models and Feature Dilution for Dynamic Ticket Pricing", "comment": "27 pages, 11 figures, 3 tables", "summary": "A novel approach is presented for identifying distinct signatures of\nperforming acts in the secondary ticket resale market by analyzing dynamic\npricing distributions. Using a newly curated, time series dataset from the\nSeatGeek API, we model ticket pricing distributions as scaled Beta\ndistributions. This enables accurate parameter estimation from incomplete\nstatistical data using a hybrid of quantile matching and the method of moments.\nIncorporating the estimated $\\alpha$ and $\\beta$ parameters into Random Forest\nclassifiers significantly improves pairwise artist classification accuracy,\ndemonstrating the unique economic signatures in event pricing data.\nAdditionally, we provide theoretical and empirical evidence that incorporating\nzero-variance (constant-value) features into Random Forest models acts as an\nimplicit regularizer, enhancing feature variety and robustness. This\nregularization promotes deeper, more varied trees in the ensemble, improving\nthe bias-variance tradeoff and mitigating overfitting to dominant features.\nThese findings are validated on both the new ticket pricing dataset and the\nstandard UCI ML handwritten digits dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u52a8\u6001\u5b9a\u4ef7\u5206\u5e03\u6765\u8bc6\u522b\u4e8c\u7ea7\u7968\u52a1\u5e02\u573a\u4e2d\u8868\u6f14\u884c\u4e3a\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u4e8c\u7ea7\u7968\u52a1\u5e02\u573a\u4e2d\u8868\u6f14\u884c\u4e3a\u7684\u72ec\u7279\u7ecf\u6d4e\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u5b9a\u4ef7\u5206\u5e03\u5efa\u6a21\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u5206\u7c7b\u3002", "method": "\u4f7f\u7528SeatGeek API\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5c06\u7968\u4ef7\u5206\u5e03\u5efa\u6a21\u4e3a\u7f29\u653eBeta\u5206\u5e03\uff0c\u7ed3\u5408\u5206\u4f4d\u6570\u5339\u914d\u548c\u77e9\u4f30\u8ba1\u65b9\u6cd5\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\uff0c\u5e76\u5c06\u4f30\u8ba1\u53c2\u6570\u7528\u4e8e\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u827a\u672f\u5bb6\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc1\u660e\u96f6\u65b9\u5dee\u7279\u5f81\u5728\u968f\u673a\u68ee\u6797\u4e2d\u5177\u6709\u6b63\u5219\u5316\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u52a8\u6001\u5b9a\u4ef7\u5206\u5e03\u5efa\u6a21\u7684\u6709\u6548\u6027\uff0c\u96f6\u65b9\u5dee\u7279\u5f81\u7684\u6b63\u5219\u5316\u4f5c\u7528\u4e3a\u968f\u673a\u68ee\u6797\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2507.22962", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22962", "abs": "https://arxiv.org/abs/2507.22962", "authors": ["Boyuan Zheng", "Victor W. Chu"], "title": "Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations", "comment": "Pre-print v0.8 2025-07-30", "summary": "Climate extremes present escalating risks to agriculture intensifying the\nneed for reliable multi-hazard early warning systems (EWS). The situation is\nevolving due to climate change and hence such systems should have the\nintelligent to continue to learn from recent climate behaviours. However,\ntraditional single-hazard forecasting methods fall short in capturing complex\ninteractions among concurrent climatic events. To address this deficiency, in\nthis paper, we combine sequential deep learning models and advanced Explainable\nArtificial Intelligence (XAI) techniques to introduce a multi-hazard\nforecasting framework for agriculture. In our experiments, we utilize\nmeteorological data from four prominent agricultural regions in the United\nStates (between 2010 and 2023) to validate the predictive accuracy of our\nframework on multiple severe event types, which are extreme cold, floods,\nfrost, hail, heatwaves, and heavy rainfall, with tailored models for each area.\nThe framework uniquely integrates attention mechanisms with TimeSHAP (a\nrecurrent XAI explainer for time series) to provide comprehensive temporal\nexplanations revealing not only which climatic features are influential but\nprecisely when their impacts occur. Our results demonstrate strong predictive\naccuracy, particularly with the BiLSTM architecture, and highlight the system's\ncapacity to inform nuanced, proactive risk management strategies. This research\nsignificantly advances the explainability and applicability of multi-hazard\nEWS, fostering interdisciplinary trust and effective decision-making process\nfor climate risk management in the agricultural industry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u591a\u707e\u5bb3\u9884\u8b66\u6846\u67b6\uff0c\u7528\u4e8e\u519c\u4e1a\u6c14\u5019\u98ce\u9669\u7ba1\u7406\uff0c\u5177\u6709\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u65f6\u95f4\u89e3\u91ca\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5355\u707e\u5bb3\u9884\u8b66\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u590d\u6742\u6c14\u5019\u4e8b\u4ef6\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u519c\u4e1a\u98ce\u9669\uff0c\u4e9f\u9700\u667a\u80fd\u5316\u7684\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982BiLSTM\uff09\u548cXAI\u6280\u672f\uff08\u5982TimeSHAP\uff09\uff0c\u6784\u5efa\u591a\u707e\u5bb3\u9884\u6d4b\u6846\u67b6\uff0c\u5e76\u9488\u5bf9\u4e0d\u540c\u707e\u5bb3\u7c7b\u578b\u5b9a\u5236\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662fBiLSTM\u67b6\u6784\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u65f6\u95f4\u7279\u5f81\u5f71\u54cd\u7684\u8be6\u7ec6\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86\u591a\u707e\u5bb3\u9884\u8b66\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\uff0c\u652f\u6301\u519c\u4e1a\u6c14\u5019\u98ce\u9669\u7ba1\u7406\u7684\u6709\u6548\u51b3\u7b56\u3002"}}
{"id": "2507.23526", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.23526", "abs": "https://arxiv.org/abs/2507.23526", "authors": ["Wen-Xuan Long", "Shengyu Ye", "Marco Moretti", "Michele Morelli", "Luca Sanguinetti", "Rui Chen", "Cheng-Xiang Wang"], "title": "Channel Estimation for 6G Near-Field Wireless Communications: A Comprehensive Survey", "comment": null, "summary": "The sixth-generation (6G) wireless systems are expected to adopt extremely\nlarge aperture arrays (ELAAs), novel antenna architectures, and operate in\nextremely high-frequency bands to meet growing data demands. ELAAs\nsignificantly increase the number of antennas, enabling finer spatial\nresolution and improved beamforming. At high frequencies, ELAAs shift\ncommunication from the conventional far-field to near-field regime, where\nspherical wavefronts dominate and the channel response depends on both angle\nand distance, increasing channel dimensionality. Conventional far-field channel\nestimation methods, which rely on angular information, struggle in near-field\nscenarios due to increased pilot overhead and computational complexity. This\npaper presents a comprehensive survey of recent advances in near-field channel\nestimation. It first defines the near- and far-field boundary from an\nelectromagnetic perspective and discusses key propagation differences,\nalongside a brief review of ELAA developments. Then, it introduces mainstream\nnear-field channel models and compares them with far-field models. Major\nestimation techniques are reviewed under different configurations\n(single/multi-user, single/multi-carrier), including both direct estimation and\nRIS-assisted cascaded estimation. These techniques reveal trade-offs among\nestimation accuracy, complexity, and overhead. This survey aims to provide\ninsights and foundations for efficient and scalable near-field channel\nestimation in 6G systems, while identifying key challenges and future research\ndirections.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e866G\u7cfb\u7edf\u4e2d\u8fd1\u573a\u4fe1\u9053\u4f30\u8ba1\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63a2\u8ba8\u4e86\u6781\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\uff08ELAA\uff09\u5e26\u6765\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u5e76\u6bd4\u8f83\u4e86\u8fd1\u573a\u4e0e\u8fdc\u573a\u6a21\u578b\u7684\u5dee\u5f02\u3002", "motivation": "\u968f\u77406G\u7cfb\u7edf\u91c7\u7528\u6781\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u548c\u9ad8\u9891\u6bb5\uff0c\u4f20\u7edf\u8fdc\u573a\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u5728\u8fd1\u573a\u573a\u666f\u4e2d\u9762\u4e34\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5bfc\u9891\u5f00\u9500\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6587\u7ae0\u9996\u5148\u4ece\u7535\u78c1\u5b66\u89d2\u5ea6\u5b9a\u4e49\u4e86\u8fd1\u573a\u4e0e\u8fdc\u573a\u7684\u8fb9\u754c\uff0c\u4ecb\u7ecd\u4e86\u4e3b\u6d41\u8fd1\u573a\u4fe1\u9053\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u4f30\u8ba1\u6280\u672f\uff08\u5982\u76f4\u63a5\u4f30\u8ba1\u548cRIS\u8f85\u52a9\u7ea7\u8054\u4f30\u8ba1\uff09\u3002", "result": "\u7efc\u8ff0\u63ed\u793a\u4e86\u8fd1\u573a\u4fe1\u9053\u4f30\u8ba1\u5728\u7cbe\u5ea6\u3001\u590d\u6742\u6027\u548c\u5f00\u9500\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a6G\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u4f30\u8ba1\u65b9\u6cd5\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u4e3a6G\u8fd1\u573a\u4fe1\u9053\u4f30\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u548c\u7814\u7a76\u65b9\u5411\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2507.23768", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23768", "abs": "https://arxiv.org/abs/2507.23768", "authors": ["Nathan Wycoff", "Ali Arab", "Lisa O. Singh"], "title": "Formal Bayesian Transfer Learning via the Total Risk Prior", "comment": null, "summary": "In analyses with severe data-limitations, augmenting the target dataset with\ninformation from ancillary datasets in the application domain, called source\ndatasets, can lead to significantly improved statistical procedures. However,\nexisting methods for this transfer learning struggle to deal with situations\nwhere the source datasets are also limited and not guaranteed to be\nwell-aligned with the target dataset. A typical strategy is to use the\nempirical loss minimizer on the source data as a prior mean for the target\nparameters, which places the estimation of source parameters outside of the\nBayesian formalism. Our key conceptual contribution is to use a risk minimizer\nconditional on source parameters instead. This allows us to construct a single\njoint prior distribution for all parameters from the source datasets as well as\nthe target dataset. As a consequence, we benefit from full Bayesian uncertainty\nquantification and can perform model averaging via Gibbs sampling over\nindicator variables governing the inclusion of each source dataset. We show how\na particular instantiation of our prior leads to a Bayesian Lasso in a\ntransformed coordinate system and discuss computational techniques to scale our\napproach to moderately sized datasets. We also demonstrate that recently\nproposed minimax-frequentist transfer learning techniques may be viewed as an\napproximate Maximum a Posteriori approach to our model. Finally, we demonstrate\nsuperior predictive performance relative to the frequentist baseline on a\ngenetics application, especially when the source data are limited.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u8f6c\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5148\u9a8c\u5206\u5e03\u6574\u5408\u6e90\u6570\u636e\u96c6\u548c\u76ee\u6807\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6e90\u6570\u636e\u6709\u9650\u6216\u4e0d\u5bf9\u9f50\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u73b0\u6709\u8f6c\u79fb\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6e90\u6570\u636e\u6709\u9650\u6216\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6e90\u53c2\u6570\u7684\u98ce\u9669\u6700\u5c0f\u5316\u5668\u6784\u5efa\u8054\u5408\u5148\u9a8c\u5206\u5e03\uff0c\u652f\u6301\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6a21\u578b\u5e73\u5747\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u9057\u4f20\u5b66\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u9891\u7387\u5b66\u57fa\u7ebf\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6e90\u6570\u636e\u6709\u9650\u65f6\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d1d\u53f6\u65af\u6846\u67b6\u4e3a\u6570\u636e\u6709\u9650\u65f6\u7684\u8f6c\u79fb\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.22963", "categories": ["cs.LG", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2507.22963", "abs": "https://arxiv.org/abs/2507.22963", "authors": ["Abdelrhman Gaber", "Hassan Abd-Eltawab", "John Elgallab", "Youssif Abuzied", "Dineo Mpanya", "Turgay Celik", "Swarun Kumar", "Tamer ElBatt"], "title": "FedCVD++: Communication-Efficient Federated Learning for Cardiovascular Risk Prediction with Parametric and Non-Parametric Model Optimization", "comment": null, "summary": "Cardiovascular diseases (CVD) cause over 17 million deaths annually\nworldwide, highlighting the urgent need for privacy-preserving predictive\nsystems. We introduce FedCVD++, an enhanced federated learning (FL) framework\nthat integrates both parametric models (logistic regression, SVM, neural\nnetworks) and non-parametric models (Random Forest, XGBoost) for coronary heart\ndisease risk prediction. To address key FL challenges, we propose: (1)\ntree-subset sampling that reduces Random Forest communication overhead by 70%,\n(2) XGBoost-based feature extraction enabling lightweight federated ensembles,\nand (3) federated SMOTE synchronization for resolving cross-institutional class\nimbalance.\n  Evaluated on the Framingham dataset (4,238 records), FedCVD++ achieves\nstate-of-the-art results: federated XGBoost (F1 = 0.80) surpasses its\ncentralized counterpart (F1 = 0.78), and federated Random Forest (F1 = 0.81)\nmatches non-federated performance. Additionally, our communication-efficient\nstrategies reduce bandwidth consumption by 3.2X while preserving 95% accuracy.\n  Compared to existing FL frameworks, FedCVD++ delivers up to 15% higher\nF1-scores and superior scalability for multi-institutional deployment. This\nwork represents the first practical integration of non-parametric models into\nfederated healthcare systems, providing a privacy-preserving solution validated\nunder real-world clinical constraints.", "AI": {"tldr": "FedCVD++\u662f\u4e00\u79cd\u589e\u5f3a\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u53c2\u6570\u5316\u548c\u975e\u53c2\u6570\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u51a0\u5fc3\u75c5\u98ce\u9669\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u901a\u4fe1\u6548\u7387\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u6bcf\u5e74\u5bfc\u81f4\u5927\u91cf\u6b7b\u4ea1\uff0c\u4e9f\u9700\u9690\u79c1\u4fdd\u62a4\u7684\u9884\u6d4b\u7cfb\u7edf\u3002", "method": "\u63d0\u51faFedCVD++\u6846\u67b6\uff0c\u5305\u62ec\u6811\u5b50\u96c6\u91c7\u6837\u3001XGBoost\u7279\u5f81\u63d0\u53d6\u548c\u8054\u90a6SMOTE\u540c\u6b65\u3002", "result": "\u5728Framingham\u6570\u636e\u96c6\u4e0a\uff0cFedCVD++\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u4fe1\u6548\u7387\u63d0\u53473.2\u500d\uff0cF1\u5206\u6570\u63d0\u9ad815%\u3002", "conclusion": "FedCVD++\u9996\u6b21\u5c06\u975e\u53c2\u6570\u6a21\u578b\u6210\u529f\u6574\u5408\u5230\u8054\u90a6\u533b\u7597\u7cfb\u7edf\u4e2d\uff0c\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23570", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.23570", "abs": "https://arxiv.org/abs/2507.23570", "authors": ["Manjun Cui", "Zhichao Zhang", "Wei Yao"], "title": "Multiple-Parameter Graph Fractional Fourier Transform: Theory and Applications", "comment": null, "summary": "The graph fractional Fourier transform (GFRFT) applies a single global\nfractional order to all graph frequencies, which restricts its adaptability to\ndiverse signal characteristics across the spectral domain. To address this\nlimitation, in this paper, we propose two types of multiple-parameter GFRFTs\n(MPGFRFTs) and establish their corresponding theoretical frameworks. We design\na spectral compression strategy tailored for ultra-low compression ratios,\neffectively preserving essential information even under extreme dimensionality\nreduction. To enhance flexibility, we introduce a learnable order vector scheme\nthat enables adaptive compression and denoising, demonstrating strong\nperformance on both graph signals and images. We explore the application of\nMPGFRFTs to image encryption and decryption. Experimental results validate the\nversatility and superior performance of the proposed MPGFRFT framework across\nvarious graph signal processing tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u591a\u53c2\u6570\u56fe\u5206\u6570\u5085\u91cc\u53f6\u53d8\u6362\uff08MPGFRFT\uff09\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfGFRFT\u5168\u5c40\u5355\u4e00\u5206\u6570\u9636\u9650\u5236\u7684\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8d85\u4f4e\u538b\u7f29\u6bd4\u4e0b\u7684\u8c31\u538b\u7f29\u7b56\u7565\u548c\u53ef\u5b66\u4e60\u9636\u5411\u91cf\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u56fe\u5206\u6570\u5085\u91cc\u53f6\u53d8\u6362\uff08GFRFT\uff09\u5bf9\u6240\u6709\u56fe\u9891\u7387\u4f7f\u7528\u5355\u4e00\u5206\u6570\u9636\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u4e0d\u540c\u4fe1\u53f7\u7279\u6027\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u79cdMPGFRFT\uff0c\u5efa\u7acb\u7406\u8bba\u6846\u67b6\uff0c\u8bbe\u8ba1\u8c31\u538b\u7f29\u7b56\u7565\u548c\u53ef\u5b66\u4e60\u9636\u5411\u91cf\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MPGFRFT\u5728\u591a\u79cd\u56fe\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5305\u62ec\u56fe\u50cf\u52a0\u5bc6\u548c\u89e3\u5bc6\u3002", "conclusion": "MPGFRFT\u6846\u67b6\u5177\u6709\u7075\u6d3b\u6027\u548c\u9ad8\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u4fe1\u53f7\u5904\u7406\u5e94\u7528\u3002"}}
{"id": "2507.23501", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.23501", "abs": "https://arxiv.org/abs/2507.23501", "authors": ["Nicklas Werge", "Yi-Shan Wu", "Bahareh Tasdighi", "Melih Kandemir"], "title": "Directional Ensemble Aggregation for Actor-Critics", "comment": null, "summary": "Off-policy reinforcement learning in continuous control tasks depends\ncritically on accurate $Q$-value estimates. Conservative aggregation over\nensembles, such as taking the minimum, is commonly used to mitigate\noverestimation bias. However, these static rules are coarse, discard valuable\ninformation from the ensemble, and cannot adapt to task-specific needs or\ndifferent learning regimes. We propose Directional Ensemble Aggregation (DEA),\nan aggregation method that adaptively combines $Q$-value estimates in\nactor-critic frameworks. DEA introduces two fully learnable directional\nparameters: one that modulates critic-side conservatism and another that guides\nactor-side policy exploration. Both parameters are learned using ensemble\ndisagreement-weighted Bellman errors, which weight each sample solely by the\ndirection of its Bellman error. This directional learning mechanism allows DEA\nto adjust conservatism and exploration in a data-driven way, adapting\naggregation to both uncertainty levels and the phase of training. We evaluate\nDEA across continuous control benchmarks and learning regimes - from\ninteractive to sample-efficient - and demonstrate its effectiveness over static\nensemble strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u96c6\u6210\u805a\u5408\u65b9\u6cd5\uff08DEA\uff09\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u65b9\u5411\u53c2\u6570\u52a8\u6001\u8c03\u6574Q\u503c\u4f30\u8ba1\uff0c\u4ee5\u4f18\u5316\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u9759\u6001\u96c6\u6210\u805a\u5408\u65b9\u6cd5\uff08\u5982\u53d6\u6700\u5c0f\u503c\uff09\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u65e0\u6cd5\u9002\u5e94\u4efb\u52a1\u9700\u6c42\u6216\u5b66\u4e60\u9636\u6bb5\u7684\u53d8\u5316\uff0c\u5bfc\u81f4\u4fe1\u606f\u6d6a\u8d39\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "DEA\u5f15\u5165\u4e24\u4e2a\u53ef\u5b66\u4e60\u7684\u65b9\u5411\u53c2\u6570\uff1a\u4e00\u4e2a\u8c03\u8282\u6279\u8bc4\u8005\u7684\u4fdd\u5b88\u6027\uff0c\u53e6\u4e00\u4e2a\u6307\u5bfc\u884c\u52a8\u8005\u7684\u7b56\u7565\u63a2\u7d22\u3002\u53c2\u6570\u901a\u8fc7\u57fa\u4e8e\u8d1d\u5c14\u66fc\u8bef\u5dee\u65b9\u5411\u7684\u52a0\u6743\u673a\u5236\u5b66\u4e60\u3002", "result": "DEA\u5728\u591a\u79cd\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u548c\u5b66\u4e60\u6a21\u5f0f\u4e0b\u5747\u4f18\u4e8e\u9759\u6001\u96c6\u6210\u7b56\u7565\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "conclusion": "DEA\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u52a8\u6001\u805a\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2507.23000", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23000", "abs": "https://arxiv.org/abs/2507.23000", "authors": ["Shengao Yi", "Xiaojiang Li", "Wei Tu", "Tianhong Zhao"], "title": "Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation", "comment": null, "summary": "As extreme heat events intensify due to climate change and urbanization,\ncities face increasing challenges in mitigating outdoor heat stress. While\ntraditional physical models such as SOLWEIG and ENVI-met provide detailed\nassessments of human-perceived heat exposure, their computational demands limit\nscalability for city-wide planning. In this study, we propose GSM-UTCI, a\nmultimodal deep learning framework designed to predict daytime average\nUniversal Thermal Climate Index (UTCI) at 1-meter hyperlocal resolution. The\nmodel fuses surface morphology (nDSM), high-resolution land cover data, and\nhourly meteorological conditions using a feature-wise linear modulation (FiLM)\narchitecture that dynamically conditions spatial features on atmospheric\ncontext. Trained on SOLWEIG-derived UTCI maps, GSM-UTCI achieves near-physical\naccuracy, with an R2 of 0.9151 and a mean absolute error (MAE) of 0.41{\\deg}C,\nwhile reducing inference time from hours to under five minutes for an entire\ncity. To demonstrate its planning relevance, we apply GSM-UTCI to simulate\nsystematic landscape transformation scenarios in Philadelphia, replacing bare\nearth, grass, and impervious surfaces with tree canopy. Results show spatially\nheterogeneous but consistently strong cooling effects, with impervious-to-tree\nconversion producing the highest aggregated benefit (-4.18{\\deg}C average\nchange in UTCI across 270.7 km2). Tract-level bivariate analysis further\nreveals strong alignment between thermal reduction potential and land cover\nproportions. These findings underscore the utility of GSM-UTCI as a scalable,\nfine-grained decision support tool for urban climate adaptation, enabling\nscenario-based evaluation of greening strategies across diverse urban\nenvironments.", "AI": {"tldr": "GSM-UTCI\u662f\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u9ad8\u5206\u8fa8\u7387\u7684UTCI\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u6027\uff0c\u9002\u7528\u4e8e\u57ce\u5e02\u6c14\u5019\u9002\u5e94\u89c4\u5212\u3002", "motivation": "\u968f\u7740\u6781\u7aef\u70ed\u4e8b\u4ef6\u7684\u52a0\u5267\uff0c\u57ce\u5e02\u9700\u8981\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u548c\u7f13\u89e3\u6237\u5916\u70ed\u538b\u529b\u3002", "method": "\u63d0\u51faGSM-UTCI\u6846\u67b6\uff0c\u878d\u5408\u5730\u8868\u5f62\u6001\u3001\u571f\u5730\u8986\u76d6\u6570\u636e\u548c\u6c14\u8c61\u6761\u4ef6\uff0c\u4f7f\u7528FiLM\u67b6\u6784\u52a8\u6001\u8c03\u6574\u7a7a\u95f4\u7279\u5f81\u3002", "result": "\u6a21\u578b\u5728R2\u548cMAE\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u65f6\u95f4\u5927\u5e45\u7f29\u77ed\uff0c\u5e76\u5728\u8d39\u57ce\u6a21\u62df\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u964d\u6e29\u6548\u679c\u3002", "conclusion": "GSM-UTCI\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u57ce\u5e02\u6c14\u5019\u9002\u5e94\u7b56\u7565\u7684\u8bc4\u4f30\u3002"}}
{"id": "2507.23695", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.23695", "abs": "https://arxiv.org/abs/2507.23695", "authors": ["Mouli Chakraborty", "Subhash Chandra", "Avishek Nag", "Anshu Mukherjee"], "title": "On the Achievable Rate of Satellite Quantum Communication Channel using Deep Autoencoder Gaussian Mixture Model", "comment": null, "summary": "We present a comparative study of the Gaussian mixture model (GMM) and the\nDeep Autoencoder Gaussian Mixture Model (DAGMM) for estimating satellite\nquantum channel capacity, considering hybrid quantum noise (HQN) and\ntransmission constraints. While GMM is simple and interpretable, DAGMM better\ncaptures non-linear variations and noise distributions. Simulations show that\nDAGMM provides tighter capacity bounds and improved clustering. This introduces\nthe Deep Cluster Gaussian Mixture Model (DCGMM) for high-dimensional quantum\ndata analysis in quantum satellite communication.", "AI": {"tldr": "\u6bd4\u8f83GMM\u548cDAGMM\u5728\u536b\u661f\u91cf\u5b50\u4fe1\u9053\u5bb9\u91cf\u4f30\u8ba1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51faDCGMM\u7528\u4e8e\u9ad8\u7ef4\u91cf\u5b50\u6570\u636e\u5206\u6790\u3002", "motivation": "\u7814\u7a76GMM\u548cDAGMM\u5728\u91cf\u5b50\u536b\u661f\u901a\u4fe1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4ee5\u6539\u8fdb\u5bb9\u91cf\u4f30\u8ba1\u548c\u566a\u58f0\u5efa\u6a21\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u6bd4\u8f83GMM\u548cDAGMM\uff0c\u63d0\u51faDCGMM\u7528\u4e8e\u9ad8\u7ef4\u91cf\u5b50\u6570\u636e\u5206\u6790\u3002", "result": "DAGMM\u5728\u5bb9\u91cf\u8fb9\u754c\u548c\u805a\u7c7b\u6027\u80fd\u4e0a\u4f18\u4e8eGMM\u3002", "conclusion": "DCGMM\u6709\u671b\u6210\u4e3a\u91cf\u5b50\u536b\u661f\u901a\u4fe1\u4e2d\u9ad8\u7ef4\u6570\u636e\u5206\u6790\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.23568", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.23568", "abs": "https://arxiv.org/abs/2507.23568", "authors": ["Fernando Mart\u00ednez-Garc\u00eda", "\u00c1lvaro Rubio-Garc\u00eda", "Samuel Fern\u00e1ndez-Lorenzo", "Juan Jos\u00e9 Garc\u00eda-Ripoll", "Diego Porras"], "title": "Optimised Feature Subset Selection via Simulated Annealing", "comment": "12 pages, 2 figures", "summary": "We introduce SA-FDR, a novel algorithm for $\\ell_0$-norm feature selection\nthat considers this task as a combinatorial optimisation problem and solves it\nby using simulated annealing to perform a global search over the space of\nfeature subsets. The optimisation is guided by the Fisher discriminant ratio,\nwhich we use as a computationally efficient proxy for model quality in\nclassification tasks. Our experiments, conducted on datasets with up to\nhundreds of thousands of samples and hundreds of features, demonstrate that\nSA-FDR consistently selects more compact feature subsets while achieving a high\npredictive accuracy. This ability to recover informative yet minimal sets of\nfeatures stems from its capacity to capture inter-feature dependencies often\nmissed by greedy optimisation approaches. As a result, SA-FDR provides a\nflexible and effective solution for designing interpretable models in\nhigh-dimensional settings, particularly when model sparsity, interpretability,\nand performance are crucial.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.23009", "categories": ["cs.LG", "cs.AI", "91E45", "I.2"], "pdf": "https://arxiv.org/pdf/2507.23009", "abs": "https://arxiv.org/abs/2507.23009", "authors": ["Tom S\u00fchr", "Florian E. Dorner", "Olawale Salaudeen", "Augustin Kelava", "Samira Samadi"], "title": "Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable results on a range of\nstandardized tests originally designed to assess human cognitive and\npsychological traits, such as intelligence and personality. While these results\nare often interpreted as strong evidence of human-like characteristics in LLMs,\nthis paper argues that such interpretations constitute an ontological error.\nHuman psychological and educational tests are theory-driven measurement\ninstruments, calibrated to a specific human population. Applying these tests to\nnon-human subjects without empirical validation, risks mischaracterizing what\nis being measured. Furthermore, a growing trend frames AI performance on\nbenchmarks as measurements of traits such as ``intelligence'', despite known\nissues with validity, data contamination, cultural bias and sensitivity to\nsuperficial prompt changes. We argue that interpreting benchmark performance as\nmeasurements of human-like traits, lacks sufficient theoretical and empirical\njustification. This leads to our position: Stop Evaluating AI with Human Tests,\nDevelop Principled, AI-specific Tests instead. We call for the development of\nprincipled, AI-specific evaluation frameworks tailored to AI systems. Such\nframeworks might build on existing frameworks for constructing and validating\npsychometrics tests, or could be created entirely from scratch to fit the\nunique context of AI.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u505c\u6b62\u7528\u4eba\u7c7b\u6d4b\u8bd5\u8bc4\u4f30AI\uff0c\u547c\u5401\u5f00\u53d1\u9488\u5bf9AI\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u7528\u4eba\u7c7b\u8ba4\u77e5\u548c\u5fc3\u7406\u6d4b\u8bd5\u8bc4\u4f30LLMs\u5b58\u5728\u7406\u8bba\u548c\u65b9\u6cd5\u4e0a\u7684\u9519\u8bef\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4eba\u7c7b\u6d4b\u8bd5\u5728AI\u8bc4\u4f30\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u5f00\u53d1AI\u4e13\u7528\u6d4b\u8bd5\u7684\u7406\u8bba\u4f9d\u636e\u3002", "result": "\u6307\u51fa\u4eba\u7c7b\u6d4b\u8bd5\u7528\u4e8eAI\u8bc4\u4f30\u7f3a\u4e4f\u7406\u8bba\u548c\u5b9e\u8bc1\u652f\u6301\uff0c\u547c\u5401\u8f6c\u5411AI\u4e13\u7528\u6d4b\u8bd5\u3002", "conclusion": "\u5e94\u505c\u6b62\u7528\u4eba\u7c7b\u6d4b\u8bd5\u8bc4\u4f30AI\uff0c\u5f00\u53d1\u9488\u5bf9AI\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2507.23707", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.23707", "abs": "https://arxiv.org/abs/2507.23707", "authors": ["Renato Luis Garrido Cavalcante", "Tomasz Piotrowski", "Slawomir Stanczak"], "title": "Cellular, Cell-less, and Everything in Between: A Unified Framework for Utility Region Analysis in Wireless Networks", "comment": null, "summary": "We introduce a unified framework for analyzing utility regions of wireless\nnetworks, with a focus on the signal-to-interference-noise-ratio (SINR) and\nachievable rate regions. The framework provides valuable insights into\ninterference patterns of modern network architectures, such as cell-less and\nextremely large MIMO networks, and it generalizes existing characterizations of\nthe weak Pareto boundary. A central contribution is the derivation of\nsufficient conditions that guarantee convexity of the utility regions.\nConvexity is an important property because it ensures that time sharing (or\nuser grouping) cannot simultaneously increase the utility of all users when the\nnetwork operates on the weak Pareto boundary. These sufficient conditions also\nhave two key implications. First, they identify a family of (weighted) sum-rate\nmaximization problems that are inherently convex without any variable\ntransformations, thus paving the way for the development of efficient, provably\noptimal solvers for this family. Second, they provide a rigorous justification\nfor formulating sum-rate maximization problems directly in terms of achievable\nrates, rather than SINR levels. Our theoretical insights also motivate an\nalternative to the concept of favorable propagation in the massive MIMO\nliterature -- one that explicitly accounts for self-interference and the\nbeamforming strategy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u5206\u6790\u65e0\u7ebf\u7f51\u7edc\u7684\u6548\u7528\u533a\u57df\uff0c\u91cd\u70b9\u5173\u6ce8SINR\u548c\u53ef\u8fbe\u901f\u7387\u533a\u57df\uff0c\u63ed\u793a\u4e86\u73b0\u4ee3\u7f51\u7edc\u67b6\u6784\u7684\u5e72\u6270\u6a21\u5f0f\uff0c\u5e76\u63a8\u5e7f\u4e86\u5f31\u5e15\u7d2f\u6258\u8fb9\u754c\u7684\u73b0\u6709\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u65e0\u7ebf\u7f51\u7edc\u4e2d\u6548\u7528\u533a\u57df\u7684\u51f8\u6027\u6761\u4ef6\uff0c\u4ee5\u4f18\u5316\u7f51\u7edc\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21MIMO\u548c\u65e0\u8702\u7a9d\u7f51\u7edc\u7b49\u73b0\u4ee3\u67b6\u6784\u4e2d\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u6548\u7528\u533a\u57df\u51f8\u6027\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5206\u6790\u5176\u5bf9\u65f6\u95f4\u5171\u4eab\u548c\u7528\u6237\u5206\u7ec4\u7684\u5f71\u54cd\uff0c\u5e76\u63a8\u5e7f\u5230\u52a0\u6743\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u4e86\u4fdd\u8bc1\u6548\u7528\u533a\u57df\u51f8\u6027\u7684\u5145\u5206\u6761\u4ef6\uff0c\u8bc1\u660e\u4e86\u65e0\u9700\u53d8\u91cf\u8f6c\u6362\u5373\u53ef\u76f4\u63a5\u89e3\u51b3\u52a0\u6743\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\uff0c\u5e76\u91cd\u65b0\u5ba1\u89c6\u4e86\u5927\u89c4\u6a21MIMO\u4e2d\u7684\u6709\u5229\u4f20\u64ad\u6982\u5ff5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e0\u7ebf\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u7279\u522b\u662f\u5728\u51f8\u6027\u6761\u4ef6\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\u4e2d\uff0c\u4e3a\u9ad8\u6548\u6c42\u89e3\u5668\u8bbe\u8ba1\u548c\u7f51\u7edc\u6027\u80fd\u4f18\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.23010", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.23010", "abs": "https://arxiv.org/abs/2507.23010", "authors": ["Siwoo Park"], "title": "Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods", "comment": null, "summary": "This paper investigates the inverse capabilities and broader utility of\nmultimodal latent spaces within task-specific AI (Artificial Intelligence)\nmodels. While these models excel at their designed forward tasks (e.g.,\ntext-to-image generation, audio-to-text transcription), their potential for\ninverse mappings remains largely unexplored. We propose an optimization-based\nframework to infer input characteristics from desired outputs, applying it\nbidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio\n(Whisper-Large-V3, Chatterbox-TTS) modalities.\n  Our central hypothesis posits that while optimization can guide models\ntowards inverse tasks, their multimodal latent spaces will not consistently\nsupport semantically meaningful and perceptually coherent inverse mappings.\nExperimental results consistently validate this hypothesis. We demonstrate that\nwhile optimization can force models to produce outputs that align textually\nwith targets (e.g., a text-to-image model generating an image that an image\ncaptioning model describes correctly, or an ASR model transcribing optimized\naudio accurately), the perceptual quality of these inversions is chaotic and\nincoherent. Furthermore, when attempting to infer the original semantic input\nfrom generative models, the reconstructed latent space embeddings frequently\nlack semantic interpretability, aligning with nonsensical vocabulary tokens.\n  These findings highlight a critical limitation. multimodal latent spaces,\nprimarily optimized for specific forward tasks, do not inherently possess the\nstructure required for robust and interpretable inverse mappings. Our work\nunderscores the need for further research into developing truly semantically\nrich and invertible multimodal latent spaces.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u5728\u4efb\u52a1\u7279\u5b9aAI\u6a21\u578b\u4e2d\u7684\u9006\u5411\u80fd\u529b\u53ca\u5176\u5e7f\u6cdb\u7528\u9014\uff0c\u53d1\u73b0\u5176\u9006\u5411\u6620\u5c04\u5728\u8bed\u4e49\u548c\u611f\u77e5\u4e0a\u7f3a\u4e4f\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u5728\u9006\u5411\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9\u9006\u5411\u6620\u5c04\u80fd\u529b\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4f18\u5316\u7684\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u6587\u672c-\u56fe\u50cf\u548c\u6587\u672c-\u97f3\u9891\u6a21\u6001\u7684\u53cc\u5411\u6620\u5c04\uff0c\u9a8c\u8bc1\u9006\u5411\u4efb\u52a1\u7684\u53ef\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136\u4f18\u5316\u53ef\u4ee5\u5f3a\u5236\u6a21\u578b\u751f\u6210\u4e0e\u76ee\u6807\u5bf9\u9f50\u7684\u8f93\u51fa\uff0c\u4f46\u9006\u5411\u6620\u5c04\u7684\u611f\u77e5\u8d28\u91cf\u548c\u8bed\u4e49\u89e3\u91ca\u6027\u8f83\u5dee\u3002", "conclusion": "\u591a\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u7f3a\u4e4f\u652f\u6301\u7a33\u5065\u9006\u5411\u6620\u5c04\u7684\u7ed3\u6784\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5f00\u53d1\u8bed\u4e49\u4e30\u5bcc\u4e14\u53ef\u9006\u7684\u6f5c\u5728\u7a7a\u95f4\u3002"}}
{"id": "2507.23746", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.23746", "abs": "https://arxiv.org/abs/2507.23746", "authors": ["Hossein Kazemi", "Isaac N. O. Osahon", "Tiankuo Jiao", "David Butler", "Nikolay Ledentsov Jr.", "Ilya Titkov", "Nikolay Ledentsov", "Harald Haas"], "title": "Real-Time Transmission of Uncompressed High-Definition Video Via A VCSEL-Based Optical Wireless Link With Ultra-Low Latency", "comment": "8 pages, 6 figures, 2 tables", "summary": "Real-time transmission of high-resolution video signals in an uncompressed\nand unencrypted format requires an ultra-reliable and low-latency\ncommunications (URLLC) medium with high bandwidth to maintain the quality of\nexperience (QoE) for users. We put forward the design and experimental\ndemonstration of a high-performance laser-based optical wireless communication\n(OWC) system that enables high-definition (HD) video transmission with\nsubmillisecond latencies. The serial digital interface (SDI) output of a camera\nis used to transmit the live video stream over an optical wireless link by\ndirectly modulating the SDI signal on the intensity of a 940 nm vertical cavity\nsurface emitting laser (VCSEL). The proposed SDI over light fidelity (LiFi)\nsystem corroborates error-free transmission of full HD (FHD) and 4K\nultra-high-definition (UHD) resolutions at data rates of 2.97 Gb/s and 5.94\nGb/s, respectively, with a measured end-to-end latency of under 35 ns. Since\nSDI standards support various video formats and VCSELs are high-bandwidth and\nlow-power devices, this presents a scalable and inexpensive solution for\nwireless connectivity between professional broadcast equipment using\noff-the-shelf SDI components.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6fc0\u5149\u7684\u5149\u5b66\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u4f20\u8f93\u9ad8\u6e05\u89c6\u9891\u4fe1\u53f7\uff0c\u5177\u6709\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u5e26\u5bbd\u7279\u6027\u3002", "motivation": "\u5b9e\u65f6\u4f20\u8f93\u672a\u538b\u7f29\u3001\u672a\u52a0\u5bc6\u7684\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u4fe1\u53f7\u9700\u8981\u8d85\u53ef\u9760\u3001\u4f4e\u5ef6\u8fdf\u7684\u901a\u4fe1\u4ecb\u8d28\uff0c\u4ee5\u7ef4\u6301\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u9a8c\u6f14\u793a\u4e86\u4e00\u79cd\u9ad8\u6027\u80fd\u6fc0\u5149\u5149\u5b66\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u76f4\u63a5\u8c03\u5236SDI\u4fe1\u53f7\u5728940 nm VCSEL\u7684\u5f3a\u5ea6\u4e0a\uff0c\u5b9e\u73b0\u9ad8\u6e05\u89c6\u9891\u4f20\u8f93\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e862.97 Gb/s\u548c5.94 Gb/s\u7684\u6570\u636e\u901f\u7387\uff0c\u5206\u522b\u652f\u6301FHD\u548c4K UHD\u5206\u8fa8\u7387\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u4f4e\u4e8e35 ns\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4e13\u4e1a\u5e7f\u64ad\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u4f4e\u6210\u672c\u7684\u65e0\u7ebf\u8fde\u63a5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23035", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.23035", "abs": "https://arxiv.org/abs/2507.23035", "authors": ["Xueying Wu", "Baijun Zhou", "Zhihui Gao", "Yuzhe Fu", "Qilin Zheng", "Yintao He", "Hai Li"], "title": "KLLM: Fast LLM Inference with K-Means Quantization", "comment": null, "summary": "Large language model (LLM) inference poses significant challenges due to its\nintensive memory and computation demands. Weight and activation quantization\n(WAQ) offers a promising solution by reducing both memory footprint and\narithmetic complexity. However, two key challenges remain in the existing WAQ\ndesigns. (1) Traditional WAQ designs rely on uniform integer-based quantization\nfor hardware efficiency, but this often results in significant accuracy\ndegradation at low precision. K-Means-based quantization, a non-uniform\nquantization technique, achieves higher accuracy by matching the Gaussian-like\ndistributions of weights and activations in LLMs. However, its non-uniform\nnature prevents direct execution on low-precision compute units, requiring\ndequantization and floating-point matrix multiplications (MatMuls) during\ninference. (2) Activation outliers further hinder effective low-precision WAQ.\nOffline thresholding methods for outlier detection can lead to significant\nmodel performance degradation, while existing online detection techniques\nintroduce substantial runtime overhead.\n  To address the aforementioned challenges and fully unleash the potential of\nWAQ with K-Means quantization for LLM inference, in this paper, we propose\nKLLM, a hardware-software co-design framework. KLLM features an index-based\ncomputation scheme for efficient execution of MatMuls and nonlinear operations\non K-Means-quantized data, which avoids most of the dequantization and\nfull-precision computations. Moreover, KLLM incorporates a novel outlier\ndetection engine, Orizuru, that efficiently identifies the top-$k$ largest and\nsmallest elements in the activation data stream during online inference.\n  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,\n7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the\nA100 GPU and Atom, respectively.", "AI": {"tldr": "KLLM\u662f\u4e00\u4e2a\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7K-Means\u91cf\u5316\u548c\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u4f18\u5316LLM\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfWAQ\u8bbe\u8ba1\u5728\u4f4e\u7cbe\u5ea6\u4e0b\u7684\u51c6\u786e\u6027\u95ee\u9898\u548c\u6fc0\u6d3b\u5f02\u5e38\u503c\u5bf9\u91cf\u5316\u6548\u679c\u7684\u963b\u788d\u3002", "method": "\u63d0\u51faKLLM\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u7d22\u5f15\u7684\u8ba1\u7b97\u65b9\u6848\u548c\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u5f15\u64ceOrizuru\u3002", "result": "\u5b9e\u9a8c\u663e\u793aKLLM\u5728\u901f\u5ea6\u548c\u80fd\u6548\u4e0a\u663e\u8457\u4f18\u4e8eA100 GPU\u548cAtom\u3002", "conclusion": "KLLM\u6709\u6548\u89e3\u51b3\u4e86WAQ\u7684\u6311\u6218\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23037", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23037", "abs": "https://arxiv.org/abs/2507.23037", "authors": ["Aur\u00e9lie Leribaux", "Rafael Oyamada", "Johannes De Smedt", "Zahra Dasht Bozorgi", "Artem Polyvyanyy", "Jochen De Weerdt"], "title": "Linking Actor Behavior to Process Performance Over Time", "comment": "Accepted for presentation at the 5th Workshop on Change, Drift, and\n  Dynamics of Organizational Processes (ProDy), BPM 2025", "summary": "Understanding how actor behavior influences process outcomes is a critical\naspect of process mining. Traditional approaches often use aggregate and static\nprocess data, overlooking the temporal and causal dynamics that arise from\nindividual actor behavior. This limits the ability to accurately capture the\ncomplexity of real-world processes, where individual actor behavior and\ninteractions between actors significantly shape performance. In this work, we\naddress this gap by integrating actor behavior analysis with Granger causality\nto identify correlating links in time series data. We apply this approach to\nrealworld event logs, constructing time series for actor interactions, i.e.\ncontinuation, interruption, and handovers, and process outcomes. Using Group\nLasso for lag selection, we identify a small but consistently influential set\nof lags that capture the majority of causal influence, revealing that actor\nbehavior has direct and measurable impacts on process performance, particularly\nthroughput time. These findings demonstrate the potential of actor-centric,\ntime series-based methods for uncovering the temporal dependencies that drive\nprocess outcomes, offering a more nuanced understanding of how individual\nbehaviors impact overall process efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u5206\u6790\u548cGranger\u56e0\u679c\u6027\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u63ed\u793a\u4e2a\u4f53\u884c\u4e3a\u5bf9\u6d41\u7a0b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u4e86\u65f6\u95f4\u52a8\u6001\u548c\u4e2a\u4f53\u884c\u4e3a\u5bf9\u6d41\u7a0b\u7ed3\u679c\u7684\u56e0\u679c\u5f71\u54cd\uff0c\u5bfc\u81f4\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u73b0\u5b9e\u6d41\u7a0b\u7684\u590d\u6742\u6027\u3002", "method": "\u6574\u5408\u884c\u4e3a\u5206\u6790\u548cGranger\u56e0\u679c\u6027\uff0c\u5229\u7528Group Lasso\u9009\u62e9\u6ede\u540e\u9879\uff0c\u5206\u6790\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u4e2a\u4f53\u884c\u4e3a\u5bf9\u6d41\u7a0b\u6027\u80fd\uff08\u5982\u541e\u5410\u65f6\u95f4\uff09\u6709\u76f4\u63a5\u4e14\u53ef\u6d4b\u91cf\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "conclusion": "\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u7684\u884c\u4e3a\u5206\u6790\u65b9\u6cd5\u80fd\u66f4\u7ec6\u81f4\u5730\u7406\u89e3\u4e2a\u4f53\u884c\u4e3a\u5bf9\u6574\u4f53\u6d41\u7a0b\u6548\u7387\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.23043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23043", "abs": "https://arxiv.org/abs/2507.23043", "authors": ["Junyi Fan", "Li Sun", "Shuheng Chen", "Yong Si", "Minoo Ahmadi", "Greg Placencia", "Elham Pishgar", "Kamiar Alaei", "Maryam Pishgar"], "title": "Prediction of Significant Creatinine Elevation in First ICU Stays with Vancomycin Use: A retrospective study through Catboost", "comment": null, "summary": "Background: Vancomycin, a key antibiotic for severe Gram-positive infections\nin ICUs, poses a high nephrotoxicity risk. Early prediction of kidney injury in\ncritically ill patients is challenging. This study aimed to develop a machine\nlearning model to predict vancomycin-related creatinine elevation using routine\nICU data.\n  Methods: We analyzed 10,288 ICU patients (aged 18-80) from the MIMIC-IV\ndatabase who received vancomycin. Kidney injury was defined by KDIGO criteria\n(creatinine rise >=0.3 mg/dL within 48h or >=50% within 7d). Features were\nselected via SelectKBest (top 30) and Random Forest ranking (final 15). Six\nalgorithms were tested with 5-fold cross-validation. Interpretability was\nevaluated using SHAP, Accumulated Local Effects (ALE), and Bayesian posterior\nsampling.\n  Results: Of 10,288 patients, 2,903 (28.2%) developed creatinine elevation.\nCatBoost performed best (AUROC 0.818 [95% CI: 0.801-0.834], sensitivity 0.800,\nspecificity 0.681, negative predictive value 0.900). Key predictors were\nphosphate, total bilirubin, magnesium, Charlson index, and APSIII. SHAP\nconfirmed phosphate as a major risk factor. ALE showed dose-response patterns.\nBayesian analysis estimated mean risk 60.5% (95% credible interval: 16.8-89.4%)\nin high-risk cases.\n  Conclusions: This machine learning model predicts vancomycin-associated\ncreatinine elevation from routine ICU data with strong accuracy and\ninterpretability, enabling early risk detection and supporting timely\ninterventions in critical care.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528ICU\u5e38\u89c4\u6570\u636e\u9884\u6d4b\u4e07\u53e4\u9709\u7d20\u76f8\u5173\u808c\u9150\u5347\u9ad8\uff0c\u6027\u80fd\u4f18\u5f02\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\u3002", "motivation": "\u4e07\u53e4\u9709\u7d20\u662fICU\u6cbb\u7597\u4e25\u91cd\u9769\u5170\u6c0f\u9633\u6027\u611f\u67d3\u7684\u5173\u952e\u6297\u751f\u7d20\uff0c\u4f46\u80be\u6bd2\u6027\u98ce\u9669\u9ad8\uff0c\u65e9\u671f\u9884\u6d4b\u80be\u635f\u4f24\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e8610,288\u540dICU\u60a3\u8005\u6570\u636e\uff0c\u4f7f\u7528KDIGO\u6807\u51c6\u5b9a\u4e49\u80be\u635f\u4f24\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u548c\u591a\u79cd\u7b97\u6cd5\uff08\u5982CatBoost\uff09\u5f00\u53d1\u9884\u6d4b\u6a21\u578b\u3002", "result": "CatBoost\u8868\u73b0\u6700\u4f73\uff08AUROC 0.818\uff09\uff0c\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u5305\u62ec\u78f7\u9178\u76d0\u3001\u603b\u80c6\u7ea2\u7d20\u7b49\uff0cSHAP\u548cALE\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u4e07\u53e4\u9709\u7d20\u76f8\u5173\u808c\u9150\u5347\u9ad8\uff0c\u652f\u6301ICU\u65e9\u671f\u98ce\u9669\u68c0\u6d4b\u548c\u5e72\u9884\u3002"}}
{"id": "2507.23073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23073", "abs": "https://arxiv.org/abs/2507.23073", "authors": ["Annalisa Barbara", "Joseph Lazzaro", "Ciara Pike-Burke"], "title": "Locally Differentially Private Thresholding Bandits", "comment": "18th European Workshop on Reinforcement Learning (EWRL 2025)", "summary": "This work investigates the impact of ensuring local differential privacy in\nthe thresholding bandit problem. We consider both the fixed budget and fixed\nconfidence settings. We propose methods that utilize private responses,\nobtained through a Bernoulli-based differentially private mechanism, to\nidentify arms with expected rewards exceeding a predefined threshold. We show\nthat this procedure provides strong privacy guarantees and derive theoretical\nperformance bounds on the proposed algorithms. Additionally, we present general\nlower bounds that characterize the additional loss incurred by any\ndifferentially private mechanism, and show that the presented algorithms match\nthese lower bounds up to poly-logarithmic factors. Our results provide valuable\ninsights into privacy-preserving decision-making frameworks in bandit problems.", "AI": {"tldr": "\u7814\u7a76\u5728\u9608\u503c\u8d4c\u535a\u673a\u95ee\u9898\u4e2d\u5b9e\u73b0\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f2f\u52aa\u5229\u5dee\u5206\u9690\u79c1\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u9690\u79c1\u4fdd\u969c\u548c\u6027\u80fd\u754c\u9650\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u524d\u63d0\u4e0b\uff0c\u89e3\u51b3\u9608\u503c\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u51b3\u7b56\u6846\u67b6\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4f2f\u52aa\u5229\u7684\u5dee\u5206\u9690\u79c1\u673a\u5236\u751f\u6210\u79c1\u6709\u54cd\u5e94\uff0c\u5206\u522b\u5728\u56fa\u5b9a\u9884\u7b97\u548c\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u8bbe\u7f6e\u4e0b\u8bbe\u8ba1\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5177\u6709\u5f3a\u9690\u79c1\u4fdd\u969c\uff0c\u6027\u80fd\u754c\u9650\u4e0e\u7406\u8bba\u4e0b\u754c\u5339\u914d\uff08\u81f3\u591a\u76f8\u5dee\u591a\u5bf9\u6570\u56e0\u5b50\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u8d4c\u535a\u673a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u548c\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2507.23077", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.23077", "abs": "https://arxiv.org/abs/2507.23077", "authors": ["Agnese Marcato", "Aleksandra Pachalieva", "Ryley G. Hill", "Kai Gao", "Xiaoyu Wang", "Esteban Rougier", "Zhou Lei", "Vinamra Agrawal", "Janel Chua", "Qinjun Kang", "Jeffrey D. Hyman", "Abigail Hunter", "Nathan DeBardeleben", "Earl Lawrence", "Hari Viswanathan", "Daniel O'Malley", "Javier E. Santos"], "title": "A Foundation Model for Material Fracture Prediction", "comment": null, "summary": "Accurately predicting when and how materials fail is critical to designing\nsafe, reliable structures, mechanical systems, and engineered components that\noperate under stress. Yet, fracture behavior remains difficult to model across\nthe diversity of materials, geometries, and loading conditions in real-world\napplications. While machine learning (ML) methods show promise, most models are\ntrained on narrow datasets, lack robustness, and struggle to generalize.\nMeanwhile, physics-based simulators offer high-fidelity predictions but are\nfragmented across specialized methods and require substantial high-performance\ncomputing resources to explore the input space. To address these limitations,\nwe present a data-driven foundation model for fracture prediction, a\ntransformer-based architecture that operates across simulators, a wide range of\nmaterials (including plastic-bonded explosives, steel, aluminum, shale, and\ntungsten), and diverse loading conditions. The model supports both structured\nand unstructured meshes, combining them with large language model embeddings of\ntextual input decks specifying material properties, boundary conditions, and\nsolver settings. This multimodal input design enables flexible adaptation\nacross simulation scenarios without changes to the model architecture. The\ntrained model can be fine-tuned with minimal data on diverse downstream tasks,\nincluding time-to-failure estimation, modeling fracture evolution, and adapting\nto combined finite-discrete element method simulations. It also generalizes to\nunseen materials such as titanium and concrete, requiring as few as a single\nsample, dramatically reducing data needs compared to standard ML. Our results\nshow that fracture prediction can be unified under a single model architecture,\noffering a scalable, extensible alternative to simulator-specific workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6570\u636e\u9a71\u52a8\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u8de8\u6750\u6599\u3001\u51e0\u4f55\u548c\u52a0\u8f7d\u6761\u4ef6\u7684\u65ad\u88c2\u9884\u6d4b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u8f93\u5165\u8bbe\u8ba1\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u9700\u6c42\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u6750\u6599\u65ad\u88c2\u5bf9\u8bbe\u8ba1\u5b89\u5168\u53ef\u9760\u7684\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u8ba1\u7b97\u8d44\u6e90\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u7f51\u683c\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u6587\u672c\u8f93\u5165\uff0c\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\u548c\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u4eff\u771f\u573a\u666f\u3002", "result": "\u6a21\u578b\u5728\u591a\u79cd\u6750\u6599\u548c\u52a0\u8f7d\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u53ef\u6cdb\u5316\u5230\u672a\u89c1\u6750\u6599\uff08\u5982\u949b\u548c\u6df7\u51dd\u571f\uff09\uff0c\u4ec5\u9700\u6781\u5c11\u6570\u636e\u5373\u53ef\u5fae\u8c03\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u65ad\u88c2\u9884\u6d4b\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u6a21\u62df\u5668\u4e13\u7528\u65b9\u6cd5\u3002"}}
{"id": "2507.23093", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.23093", "abs": "https://arxiv.org/abs/2507.23093", "authors": ["Ghazal Sobhani", "Md. Monzurul Amin Ifath", "Tushar Sharma", "Israat Haque"], "title": "On the Sustainability of AI Inferences in the Edge", "comment": "14 pages, 8 figures, 6 tables, in preparation for journal submission", "summary": "The proliferation of the Internet of Things (IoT) and its cutting-edge\nAI-enabled applications (e.g., autonomous vehicles and smart industries)\ncombine two paradigms: data-driven systems and their deployment on the edge.\nUsually, edge devices perform inferences to support latency-critical\napplications. In addition to the performance of these resource-constrained edge\ndevices, their energy usage is a critical factor in adopting and deploying edge\napplications. Examples of such devices include Raspberry Pi (RPi), Intel Neural\nCompute Stick (INCS), NVIDIA Jetson nano (NJn), and Google Coral USB (GCU).\nDespite their adoption in edge deployment for AI inferences, there is no study\non their performance and energy usage for informed decision-making on the\ndevice and model selection to meet the demands of applications. This study\nfills the gap by rigorously characterizing the performance of traditional,\nneural networks, and large language models on the above-edge devices.\nSpecifically, we analyze trade-offs among model F1 score, inference time,\ninference power, and memory usage. Hardware and framework optimization, along\nwith external parameter tuning of AI models, can balance between model\nperformance and resource usage to realize practical edge AI deployments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8fb9\u7f18\u8bbe\u5907\uff08\u5982RPi\u3001INCS\u3001NJn\u548cGCU\uff09\u5728AI\u63a8\u7406\u4e2d\u7684\u6027\u80fd\u548c\u80fd\u8017\uff0c\u586b\u8865\u4e86\u8bbe\u5907\u4e0e\u6a21\u578b\u9009\u62e9\u51b3\u7b56\u7684\u7a7a\u767d\u3002", "motivation": "\u7269\u8054\u7f51\u548cAI\u5e94\u7528\u7684\u7ed3\u5408\u9700\u8981\u8fb9\u7f18\u8bbe\u5907\u652f\u6301\u4f4e\u5ef6\u8fdf\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u5176\u6027\u80fd\u548c\u80fd\u8017\u7684\u7814\u7a76\uff0c\u5f71\u54cd\u8bbe\u5907\u4e0e\u6a21\u578b\u9009\u62e9\u7684\u51b3\u7b56\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6027\u80fd\uff0c\u8bc4\u4f30\u6a21\u578bF1\u5206\u6570\u3001\u63a8\u7406\u65f6\u95f4\u3001\u529f\u8017\u548c\u5185\u5b58\u4f7f\u7528\u7684\u6743\u8861\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u786c\u4ef6\u548c\u6846\u67b6\u4f18\u5316\u53caAI\u6a21\u578b\u53c2\u6570\u8c03\u6574\u53ef\u4ee5\u5728\u6a21\u578b\u6027\u80fd\u548c\u8d44\u6e90\u4f7f\u7528\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8fb9\u7f18AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u8bbe\u5907\u4e0e\u6a21\u578b\u9009\u62e9\u7684\u4f9d\u636e\uff0c\u4f18\u5316\u6027\u80fd\u548c\u80fd\u8017\u3002"}}
{"id": "2507.23111", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23111", "abs": "https://arxiv.org/abs/2507.23111", "authors": ["Richard Williams", "Eric Nalisnick", "Andrew Holbrook"], "title": "Scalable Generative Modeling of Weighted Graphs", "comment": "25 pages, 5 figures, included appendix. code at\n  https://github.com/rlwilliams34/BiGG-E", "summary": "Weighted graphs are ubiquitous throughout biology, chemistry, and the social\nsciences, motivating the development of generative models for abstract weighted\ngraph data using deep neural networks. However, most current deep generative\nmodels are either designed for unweighted graphs and are not easily extended to\nweighted topologies or incorporate edge weights without consideration of a\njoint distribution with topology. Furthermore, learning a distribution over\nweighted graphs must account for complex nonlocal dependencies between both the\nedges of the graph and corresponding weights of each edge. We develop an\nautoregressive model BiGG-E, a nontrivial extension of the BiGG model, that\nlearns a joint distribution over weighted graphs while still exploiting\nsparsity to generate a weighted graph with $n$ nodes and $m$ edges in $O((n +\nm)\\log n)$ time. Simulation studies and experiments on a variety of benchmark\ndatasets demonstrate that BiGG-E best captures distributions over weighted\ngraphs while remaining scalable and computationally efficient.", "AI": {"tldr": "BiGG-E\u662f\u4e00\u4e2a\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u6269\u5c55\u81eaBiGG\u6a21\u578b\uff0c\u7528\u4e8e\u5b66\u4e60\u52a0\u6743\u56fe\u7684\u8054\u5408\u5206\u5e03\uff0c\u540c\u65f6\u5229\u7528\u7a00\u758f\u6027\u9ad8\u6548\u751f\u6210\u52a0\u6743\u56fe\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u8981\u4e48\u4e0d\u9002\u7528\u4e8e\u52a0\u6743\u56fe\uff0c\u8981\u4e48\u672a\u8003\u8651\u8fb9\u6743\u91cd\u4e0e\u62d3\u6251\u7684\u8054\u5408\u5206\u5e03\uff0cBiGG-E\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "BiGG-E\u901a\u8fc7\u81ea\u56de\u5f52\u65b9\u5f0f\u5b66\u4e60\u52a0\u6743\u56fe\u7684\u8054\u5408\u5206\u5e03\uff0c\u5229\u7528\u7a00\u758f\u6027\u5b9e\u73b0\u9ad8\u6548\u751f\u6210\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO((n + m)log n)\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBiGG-E\u5728\u591a\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u80fd\u66f4\u597d\u5730\u6355\u6349\u52a0\u6743\u56fe\u5206\u5e03\uff0c\u4e14\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "BiGG-E\u4e3a\u52a0\u6743\u56fe\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23115", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23115", "abs": "https://arxiv.org/abs/2507.23115", "authors": ["David J Goetze", "Dahlia J Felten", "Jeannie R Albrecht", "Rohit Bhattacharya"], "title": "FLOSS: Federated Learning with Opt-Out and Straggler Support", "comment": "5 pages", "summary": "Previous work on data privacy in federated learning systems focuses on\nprivacy-preserving operations for data from users who have agreed to share\ntheir data for training. However, modern data privacy agreements also empower\nusers to use the system while opting out of sharing their data as desired. When\ncombined with stragglers that arise from heterogeneous device capabilities, the\nresult is missing data from a variety of sources that introduces bias and\ndegrades model performance. In this paper, we present FLOSS, a system that\nmitigates the impacts of such missing data on federated learning in the\npresence of stragglers and user opt-out, and empirically demonstrate its\nperformance in simulations.", "AI": {"tldr": "FLOSS\u7cfb\u7edf\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u56e0\u7528\u6237\u9000\u51fa\u548c\u5f02\u6784\u8bbe\u5907\u5bfc\u81f4\u7684\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u9690\u79c1\u534f\u8bae\u5141\u8bb8\u7528\u6237\u9009\u62e9\u4e0d\u5171\u4eab\u6570\u636e\uff0c\u52a0\u4e0a\u8bbe\u5907\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u6570\u636e\u7f3a\u5931\uff0c\u4f1a\u5f15\u5165\u504f\u5dee\u5e76\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faFLOSS\u7cfb\u7edf\uff0c\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u51cf\u5c11\u6570\u636e\u7f3a\u5931\u5bf9\u8054\u90a6\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86FLOSS\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "FLOSS\u80fd\u6709\u6548\u7f13\u89e3\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u7f3a\u5931\u5e26\u6765\u7684\u95ee\u9898\u3002"}}
{"id": "2507.23128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23128", "abs": "https://arxiv.org/abs/2507.23128", "authors": ["Ana\u00efs Baranger", "Lucas Maison"], "title": "Evaluating and Improving the Robustness of Speech Command Recognition Models to Noise and Distribution Shifts", "comment": "Submitted to ICASSP 2026", "summary": "Although prior work in computer vision has shown strong correlations between\nin-distribution (ID) and out-of-distribution (OOD) accuracies, such\nrelationships remain underexplored in audio-based models. In this study, we\ninvestigate how training conditions and input features affect the robustness\nand generalization abilities of spoken keyword classifiers under OOD\nconditions. We benchmark several neural architectures across a variety of\nevaluation sets. To quantify the impact of noise on generalization, we make use\nof two metrics: Fairness (F), which measures overall accuracy gains compared to\na baseline model, and Robustness (R), which assesses the convergence between ID\nand OOD performance. Our results suggest that noise-aware training improves\nrobustness in some configurations. These findings shed new light on the\nbenefits and limitations of noise-based augmentation for generalization in\nspeech models.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8bad\u7ec3\u6761\u4ef6\u548c\u8f93\u5165\u7279\u5f81\u5bf9\u8bed\u97f3\u5173\u952e\u8bcd\u5206\u7c7b\u5668\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u5728\u67d0\u4e9b\u914d\u7f6e\u4e2d\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5df2\u53d1\u73b0\u5206\u5e03\u5185\uff08ID\uff09\u548c\u5206\u5e03\u5916\uff08OOD\uff09\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u4f46\u5728\u97f3\u9891\u6a21\u578b\u4e2d\u8fd9\u79cd\u5173\u7cfb\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u4e0d\u540c\u8bc4\u4f30\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528\u516c\u5e73\u6027\uff08F\uff09\u548c\u9c81\u68d2\u6027\uff08R\uff09\u4e24\u4e2a\u6307\u6807\u91cf\u5316\u566a\u58f0\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "result": "\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u5728\u67d0\u4e9b\u914d\u7f6e\u4e2d\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u566a\u58f0\u589e\u5f3a\u5728\u8bed\u97f3\u6a21\u578b\u6cdb\u5316\u4e2d\u7684\u76ca\u5904\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2507.23136", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23136", "abs": "https://arxiv.org/abs/2507.23136", "authors": ["Erin George", "Deanna Needell", "Berk Ustun"], "title": "Observational Multiplicity", "comment": null, "summary": "Many prediction tasks can admit multiple models that can perform almost\nequally well. This phenomenon can can undermine interpretability and safety\nwhen competing models assign conflicting predictions to individuals. In this\nwork, we study how arbitrariness can arise in probabilistic classification\ntasks as a result of an effect that we call \\emph{observational multiplicity}.\nWe discuss how this effect arises in a broad class of practical applications\nwhere we learn a classifier to predict probabilities $p_i \\in [0,1]$ but are\ngiven a dataset of observations $y_i \\in \\{0,1\\}$. We propose to evaluate the\narbitrariness of individual probability predictions through the lens of\n\\emph{regret}. We introduce a measure of regret for probabilistic\nclassification tasks, which measures how the predictions of a model could\nchange as a result of different training labels change. We present a\ngeneral-purpose method to estimate the regret in a probabilistic classification\ntask. We use our measure to show that regret is higher for certain groups in\nthe dataset and discuss potential applications of regret. We demonstrate how\nestimating regret promote safety in real-world applications by abstention and\ndata collection.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6982\u7387\u5206\u7c7b\u4efb\u52a1\u4e2d\u7531\u4e8e\u89c2\u6d4b\u591a\u91cd\u6027\u5bfc\u81f4\u7684\u9884\u6d4b\u4efb\u610f\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u540e\u6094\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u6982\u7387\u5206\u7c7b\u4efb\u52a1\u4e2d\u6a21\u578b\u9884\u6d4b\u7684\u4efb\u610f\u6027\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u540e\u6094\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u56e0\u8bad\u7ec3\u6807\u7b7e\u53d8\u5316\u800c\u53ef\u80fd\u4ea7\u751f\u7684\u53d8\u5316\uff0c\u5e76\u8bbe\u8ba1\u901a\u7528\u65b9\u6cd5\u4f30\u8ba1\u540e\u6094\u503c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u6570\u636e\u7ec4\u7684\u540e\u6094\u503c\u8f83\u9ad8\uff0c\u540e\u6094\u4f30\u8ba1\u53ef\u7528\u4e8e\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\uff0c\u5982\u901a\u8fc7\u5f03\u6743\u548c\u6570\u636e\u6536\u96c6\u3002", "conclusion": "\u540e\u6094\u5ea6\u91cf\u80fd\u6709\u6548\u8bc6\u522b\u9884\u6d4b\u4efb\u610f\u6027\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.23141", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.23141", "abs": "https://arxiv.org/abs/2507.23141", "authors": ["Xiangshu Gong", "Zhiqiang Xie", "Xiaowei Jin", "Chen Wang", "Yanling Qu", "Wangmeng Zuo", "Hui Li"], "title": "AI paradigm for solving differential equations: first-principles data generation and scale-dilation operator AI solver", "comment": null, "summary": "Many problems are governed by differential equations (DEs). Artificial\nintelligence (AI) is a new path for solving DEs. However, data is very scarce\nand existing AI solvers struggle with approximation of high frequency\ncomponents (AHFC). We propose an AI paradigm for solving diverse DEs, including\nDE-ruled first-principles data generation methodology and scale-dilation\noperator (SDO) AI solver. Using either prior knowledge or random fields, we\ngenerate solutions and then substitute them into the DEs to derive the sources\nand initial/boundary conditions through balancing DEs, thus producing\narbitrarily vast amount of, first-principles-consistent training datasets at\nextremely low computational cost. We introduce a reversible SDO that leverages\nthe Fourier transform of the multiscale solutions to fix AHFC, and design a\nspatiotemporally coupled, attention-based Transformer AI solver of DEs with\nSDO. An upper bound on the Hessian condition number of the loss function is\nproven to be proportional to the squared 2-norm of the solution gradient,\nrevealing that SDO yields a smoother loss landscape, consequently fixing AHFC\nwith efficient training. Extensive tests on diverse DEs demonstrate that our AI\nparadigm achieves consistently superior accuracy over state-of-the-art methods.\nThis work makes AI solver of DEs to be truly usable in broad nature and\nengineering fields.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u65b0\u8303\u5f0f\uff0c\u5305\u62ec\u6570\u636e\u751f\u6210\u65b9\u6cd5\u548c\u5c3a\u5ea6\u6269\u5f20\u7b97\u5b50\uff08SDO\uff09\u6c42\u89e3\u5668\uff0c\u89e3\u51b3\u4e86\u9ad8\u9891\u5206\u91cf\u903c\u8fd1\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u5fae\u5206\u65b9\u7a0b\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfAI\u6c42\u89e3\u5668\u5728\u5904\u7406\u5fae\u5206\u65b9\u7a0b\u65f6\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u9891\u5206\u91cf\u903c\u8fd1\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u6216\u968f\u673a\u573a\u751f\u6210\u89e3\uff0c\u901a\u8fc7\u5e73\u8861\u5fae\u5206\u65b9\u7a0b\u751f\u6210\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff1b\u5f15\u5165\u53ef\u9006SDO\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684Transformer\u6c42\u89e3\u5668\uff0c\u4f18\u5316\u9ad8\u9891\u5206\u91cf\u903c\u8fd1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5fae\u5206\u65b9\u7a0b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u8bad\u7ec3\u6548\u7387\u9ad8\u3002", "conclusion": "\u8be5AI\u8303\u5f0f\u4e3a\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u81ea\u7136\u548c\u5de5\u7a0b\u9886\u57df\u3002"}}
{"id": "2507.23154", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23154", "abs": "https://arxiv.org/abs/2507.23154", "authors": ["Sofiane Bouaziz", "Adel Hafiane", "Raphael Canals", "Rachid Nedjai"], "title": "FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations", "comment": "Accepted in the 2025 International Conference on Machine Intelligence\n  for GeoAnalytics and Remote Sensing (MIGARS)", "summary": "Urban heatwaves, droughts, and land degradation are pressing and growing\nchallenges in the context of climate change. A valuable approach to studying\nthem requires accurate spatio-temporal information on land surface conditions.\nOne of the most important variables for assessing and understanding these\nphenomena is Land Surface Temperature (LST), which is derived from satellites\nand provides essential information about the thermal state of the Earth's\nsurface. However, satellite platforms inherently face a trade-off between\nspatial and temporal resolutions. To bridge this gap, we propose FuseTen, a\nnovel generative framework that produces daily LST observations at a fine 10 m\nspatial resolution by fusing spatio-temporal observations derived from\nSentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative\narchitecture trained using an averaging-based supervision strategy grounded in\nphysical principles. It incorporates attention and normalization modules within\nthe fusion process and uses a PatchGAN discriminator to enforce realism.\nExperiments across multiple dates show that FuseTen outperforms linear\nbaselines, with an average 32.06% improvement in quantitative metrics and\n31.42% in visual fidelity. To the best of our knowledge, this is the first\nnon-linear method to generate daily LST estimates at such fine spatial\nresolution.", "AI": {"tldr": "FuseTen\u662f\u4e00\u79cd\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408Sentinel-2\u3001Landsat 8\u548cTerra MODIS\u7684\u6570\u636e\uff0c\u751f\u6210\u6bcf\u65e510\u7c73\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u5730\u8868\u6e29\u5ea6\uff08LST\uff09\u89c2\u6d4b\u6570\u636e\u3002", "motivation": "\u57ce\u5e02\u70ed\u6d6a\u3001\u5e72\u65f1\u548c\u571f\u5730\u9000\u5316\u662f\u6c14\u5019\u53d8\u5316\u80cc\u666f\u4e0b\u7684\u7d27\u8feb\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u5730\u8868\u6e29\u5ea6\u6570\u636e\u6765\u7814\u7a76\u3002", "method": "FuseTen\u91c7\u7528\u751f\u6210\u67b6\u6784\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u5f52\u4e00\u5316\u6a21\u5757\uff0c\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u539f\u7406\u7684\u76d1\u7763\u7b56\u7565\u548cPatchGAN\u5224\u522b\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFuseTen\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u534732.06%\u548c31.42%\uff0c\u4f18\u4e8e\u7ebf\u6027\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FuseTen\u662f\u9996\u4e2a\u80fd\u4ee510\u7c73\u7a7a\u95f4\u5206\u8fa8\u7387\u751f\u6210\u6bcf\u65e5LST\u6570\u636e\u7684\u975e\u7ebf\u6027\u65b9\u6cd5\u3002"}}
{"id": "2507.23170", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23170", "abs": "https://arxiv.org/abs/2507.23170", "authors": ["Jinan Zhou", "Rajat Ghosh", "Vaishnavi Bhargava", "Debojyoti Dutta", "Aryan Singhal"], "title": "BAR Conjecture: the Feasibility of Inference Budget-Constrained LLM Services with Authenticity and Reasoning", "comment": null, "summary": "When designing LLM services, practitioners care about three key properties:\ninference-time budget, factual authenticity, and reasoning capacity. However,\nour analysis shows that no model can simultaneously optimize for all three. We\nformally prove this trade-off and propose a principled framework named The BAR\nTheorem for LLM-application design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBAR\u5b9a\u7406\uff0c\u8bc1\u660eLLM\u5728\u8bbe\u8ba1\u65f6\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u63a8\u7406\u9884\u7b97\u3001\u4e8b\u5b9e\u771f\u5b9e\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3LLM\u670d\u52a1\u8bbe\u8ba1\u4e2d\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u63a8\u7406\u9884\u7b97\u3001\u4e8b\u5b9e\u771f\u5b9e\u6027\u548c\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u8bc1\u660e\u63d0\u51faBAR\u5b9a\u7406\uff0c\u4e3aLLM\u5e94\u7528\u8bbe\u8ba1\u63d0\u4f9b\u539f\u5219\u6027\u6846\u67b6\u3002", "result": "\u8bc1\u660eLLM\u5728\u8bbe\u8ba1\u65f6\u5b58\u5728\u4e09\u8005\u4e0d\u53ef\u517c\u5f97\u7684\u6743\u8861\u3002", "conclusion": "BAR\u5b9a\u7406\u4e3aLLM\u5e94\u7528\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2507.23186", "categories": ["cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.23186", "abs": "https://arxiv.org/abs/2507.23186", "authors": ["Peter Sharpe"], "title": "NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions", "comment": null, "summary": "Sparsity detection in black-box functions enables significant computational\nspeedups in gradient-based optimization through Jacobian compression, but\nexisting finite-difference methods suffer from false negatives due to\ncoincidental zero gradients. These false negatives can silently corrupt\ngradient calculations, leading to difficult-to-diagnose errors. We introduce\nNaN-propagation, which exploits the universal contamination property of IEEE\n754 Not-a-Number floating-point values to trace input-output dependencies\nthrough floating-point numerical computations. By systematically contaminating\ninputs with NaN and observing which outputs become NaN, the method reconstructs\nconservative sparsity patterns that eliminate false negatives. We demonstrate\nthe approach on an aerospace wing weight model, achieving a 1.52x speedup while\ndetecting dozens of dependencies missed by conventional methods -- a\nsignificant improvement since gradient computation is the bottleneck in many\noptimization workflows. The technique leverages IEEE 754 compliance to work\nacross programming languages and math libraries without modifying existing\nblack-box codes. Advanced strategies including NaN payload encoding enable\nfaster-than-linear time complexity, improving upon existing black-box sparsity\ndetection methods. Practical algorithms are also proposed to mitigate\nchallenges from branching code execution common in engineering applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNaN-propagation\u7684\u65b9\u6cd5\uff0c\u5229\u7528IEEE 754\u6d6e\u70b9\u6570\u7684NaN\u7279\u6027\u68c0\u6d4b\u9ed1\u76d2\u51fd\u6570\u7684\u7a00\u758f\u6027\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6709\u9650\u5dee\u5206\u6cd5\u7684\u5047\u9634\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68af\u5ea6\u8ba1\u7b97\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u6709\u9650\u5dee\u5206\u6cd5\u5728\u68c0\u6d4b\u9ed1\u76d2\u51fd\u6570\u7a00\u758f\u6027\u65f6\u5b58\u5728\u5047\u9634\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u68af\u5ea6\u8ba1\u7b97\u9519\u8bef\u4e14\u96be\u4ee5\u8bca\u65ad\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u7528NaN\u6c61\u67d3\u8f93\u5165\u5e76\u89c2\u5bdf\u8f93\u51fa\u662f\u5426\u53d8\u4e3aNaN\uff0c\u91cd\u5efa\u4fdd\u5b88\u7684\u7a00\u758f\u6a21\u5f0f\uff0c\u6d88\u9664\u5047\u9634\u6027\u3002", "result": "\u5728\u822a\u7a7a\u822a\u5929\u673a\u7ffc\u91cd\u91cf\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u4e861.52\u500d\u7684\u52a0\u901f\uff0c\u5e76\u68c0\u6d4b\u5230\u4f20\u7edf\u65b9\u6cd5\u9057\u6f0f\u7684\u6570\u5341\u4e2a\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "NaN-propagation\u65b9\u6cd5\u5229\u7528IEEE 754\u6807\u51c6\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u9ed1\u76d2\u4ee3\u7801\u5373\u53ef\u8de8\u8bed\u8a00\u548c\u6570\u5b66\u5e93\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u6027\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.23217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23217", "abs": "https://arxiv.org/abs/2507.23217", "authors": ["Hyeon Seong Jeong", "Sangwoo Jo", "Byeong Hyun Yoon", "Yoonseok Heo", "Haedong Jeong", "Taehoon Kim"], "title": "Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation", "comment": null, "summary": "Understanding complex multimodal documents remains challenging due to their\nstructural inconsistencies and limited training data availability. We introduce\n\\textit{DocsRay}, a training-free document understanding system that integrates\npseudo Table of Contents (TOC) generation with hierarchical Retrieval-Augmented\nGeneration (RAG). Our approach leverages multimodal Large Language Models'\n(LLMs) native capabilities to seamlessly process documents containing diverse\nelements such as text, images, charts, and tables without requiring specialized\nmodels or additional training. DocsRay's framework synergistically combines\nthree key techniques: (1) a semantic structuring module using prompt-based LLM\ninteractions to generate a hierarchical pseudo-TOC, (2) zero-shot multimodal\nanalysis that converts diverse document elements into unified, text-centric\nrepresentations using the inherent capabilities of multimodal LLMs, and (3) an\nefficient two-stage hierarchical retrieval system that reduces retrieval\ncomplexity from $O(N)$ to $O(S + k_1 \\cdot N_s)$. Evaluated on documents\naveraging 49.4 pages and 20,971 textual tokens, DocsRay reduced query latency\nfrom 3.89 to 2.12 seconds, achieving a 45% efficiency improvement. On the\nMMLongBench-Doc benchmark, DocsRay-Pro attains an accuracy of 64.7%,\nsubstantially surpassing previous state-of-the-art results.", "AI": {"tldr": "DocsRay\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u6863\u7406\u89e3\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f2a\u76ee\u5f55\u751f\u6210\u548c\u5206\u5c42\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6587\u6863\u5904\u7406\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u6587\u6863\u7ed3\u6784\u4e0d\u4e00\u81f4\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u4f2a\u76ee\u5f55\u751f\u6210\u3001\u96f6\u6837\u672c\u591a\u6a21\u6001\u5206\u6790\u548c\u5206\u5c42\u68c0\u7d22\u7cfb\u7edf\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002", "result": "\u67e5\u8be2\u5ef6\u8fdf\u4ece3.89\u79d2\u964d\u81f32.12\u79d2\uff0c\u6548\u7387\u63d0\u534745%\uff1b\u5728MMLongBench-Doc\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u8fbe64.7%\u3002", "conclusion": "DocsRay\u901a\u8fc7\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u6a21\u6001\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.23221", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23221", "abs": "https://arxiv.org/abs/2507.23221", "authors": ["Charles O'Neill", "Slava Chalnev", "Chi Chi Zhao", "Max Kirkby", "Mudith Jayasekara"], "title": "A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations", "comment": null, "summary": "Contextual hallucinations -- statements unsupported by given context --\nremain a significant challenge in AI. We demonstrate a practical\ninterpretability insight: a generator-agnostic observer model detects\nhallucinations via a single forward pass and a linear probe on its residual\nstream. This probe isolates a single, transferable linear direction separating\nhallucinated from faithful text, outperforming baselines by 5-27 points and\nshowing robust mid-layer performance across Gemma-2 models (2B to 27B).\nGradient-times-activation localises this signal to sparse, late-layer MLP\nactivity. Critically, manipulating this direction causally steers generator\nhallucination rates, proving its actionability. Our results offer novel\nevidence of internal, low-dimensional hallucination tracking linked to specific\nMLP sub-circuits, exploitable for detection and mitigation. We release the\n2000-example ContraTales benchmark for realistic assessment of such solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u89c2\u5bdf\u6a21\u578b\u6b8b\u5dee\u6d41\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\u6765\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\u4e2d\u7684\u5e7b\u89c9\uff08\u65e0\u6839\u636e\u5185\u5bb9\uff09\u7684\u65b9\u6cd5\uff0c\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf5-27\u5206\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u53ef\u64cd\u4f5c\u6027\u3002", "motivation": "\u89e3\u51b3AI\u751f\u6210\u6587\u672c\u4e2d\u5b58\u5728\u7684\u4e0a\u4e0b\u6587\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5668\u65e0\u5173\u7684\u89c2\u5bdf\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u548c\u6b8b\u5dee\u6d41\u4e0a\u7684\u7ebf\u6027\u63a2\u9488\u68c0\u6d4b\u5e7b\u89c9\uff0c\u5e76\u5229\u7528\u68af\u5ea6-\u6fc0\u6d3b\u5b9a\u4f4d\u4fe1\u53f7\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u4e2a\u53ef\u8f6c\u79fb\u7684\u7ebf\u6027\u65b9\u5411\uff0c\u80fd\u6709\u6548\u533a\u5206\u5e7b\u89c9\u4e0e\u5fe0\u5b9e\u6587\u672c\uff0c\u4e14\u53ef\u901a\u8fc7\u64cd\u7eb5\u8be5\u65b9\u5411\u63a7\u5236\u5e7b\u89c9\u7387\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5185\u90e8\u4f4e\u7ef4\u5e7b\u89c9\u4fe1\u53f7\u4e0e\u7279\u5b9aMLP\u5b50\u7535\u8def\u7684\u5173\u8054\uff0c\u4e3a\u68c0\u6d4b\u548c\u7f13\u89e3\u5e7b\u89c9\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u53d1\u5e03\u4e86ContraTales\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2507.23257", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23257", "abs": "https://arxiv.org/abs/2507.23257", "authors": ["Jiawei Liu", "Chenwang Wu", "Defu Lian", "Enhong Chen"], "title": "Efficient Machine Unlearning via Influence Approximation", "comment": "12 pages, 4 figures", "summary": "Due to growing privacy concerns, machine unlearning, which aims at enabling\nmachine learning models to ``forget\" specific training data, has received\nincreasing attention. Among existing methods, influence-based unlearning has\nemerged as a prominent approach due to its ability to estimate the impact of\nindividual training samples on model parameters without retraining. However,\nthis approach suffers from prohibitive computational overhead arising from the\nnecessity to compute the Hessian matrix and its inverse across all training\nsamples and parameters, rendering it impractical for large-scale models and\nscenarios involving frequent data deletion requests. This highlights the\ndifficulty of forgetting. Inspired by cognitive science, which suggests that\nmemorizing is easier than forgetting, this paper establishes a theoretical link\nbetween memorizing (incremental learning) and forgetting (unlearning). This\nconnection allows machine unlearning to be addressed from the perspective of\nincremental learning. Unlike the time-consuming Hessian computations in\nunlearning (forgetting), incremental learning (memorizing) typically relies on\nmore efficient gradient optimization, which supports the aforementioned\ncognitive theory. Based on this connection, we introduce the Influence\nApproximation Unlearning (IAU) algorithm for efficient machine unlearning from\nthe incremental perspective. Extensive empirical evaluations demonstrate that\nIAU achieves a superior balance among removal guarantee, unlearning efficiency,\nand comparable model utility, while outperforming state-of-the-art methods\nacross diverse datasets and model architectures. Our code is available at\nhttps://github.com/Lolo1222/IAU.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u91cf\u5b66\u4e60\u89c6\u89d2\u7684\u9ad8\u6548\u673a\u5668\u9057\u5fd8\u7b97\u6cd5\uff08IAU\uff09\uff0c\u901a\u8fc7\u7406\u8bba\u8fde\u63a5\u8bb0\u5fc6\u4e0e\u9057\u5fd8\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u4e8eHessian\u77e9\u9635\u7684\u9057\u5fd8\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u673a\u5668\u9057\u5fd8\uff08\u8ba9\u6a21\u578b\u201c\u5fd8\u8bb0\u201d\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\uff09\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u5f71\u54cd\u7684\u9057\u5fd8\u65b9\u6cd5\u56e0\u8ba1\u7b97Hessian\u77e9\u9635\u53ca\u5176\u9006\u77e9\u9635\u7684\u5f00\u9500\u8fc7\u5927\u800c\u96be\u4ee5\u5b9e\u7528\u3002", "method": "\u8bba\u6587\u4ece\u8ba4\u77e5\u79d1\u5b66\u4e2d\u201c\u8bb0\u5fc6\u6bd4\u9057\u5fd8\u66f4\u5bb9\u6613\u201d\u7684\u7406\u8bba\u51fa\u53d1\uff0c\u5efa\u7acb\u4e86\u8bb0\u5fc6\uff08\u589e\u91cf\u5b66\u4e60\uff09\u4e0e\u9057\u5fd8\uff08\u673a\u5668\u9057\u5fd8\uff09\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u9ad8\u6548\u7684IAU\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIAU\u5728\u4fdd\u8bc1\u6570\u636e\u5220\u9664\u6548\u679c\u3001\u9057\u5fd8\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u3002", "conclusion": "IAU\u901a\u8fc7\u589e\u91cf\u5b66\u4e60\u89c6\u89d2\u5b9e\u73b0\u4e86\u9ad8\u6548\u673a\u5668\u9057\u5fd8\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u548c\u9891\u7e41\u6570\u636e\u5220\u9664\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23261", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23261", "abs": "https://arxiv.org/abs/2507.23261", "authors": ["Hui Yi Leong", "Yuqing Wu"], "title": "DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System", "comment": null, "summary": "Current multi-agent systems (MAS) frameworks often rely on manually designed\nand static collaboration graph structures, limiting adaptability and\nperformance. To address these limitations, we propose DynaSwarm, a dynamic\nframework that enhances LLM-based MAS through two key innovations: (1) an\nactor-critic reinforcement learning (A2C) mechanism to optimize graph\nstructures with improved stability over prior RL methods, and (2) a dynamic\ngraph selector that adaptively chooses the optimal graph structure for each\ninput sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the\nneed for rigid, one-fits-all graph architectures, instead leveraging\nsample-specific idiosyncrasies to dynamically route queries through specialized\nagent networks. (c) We propose to fine-tune the demonstration retriever to\nfully exploit the power of in-context learning (ICL). Extensive experiments on\nquestion answering, mathematical reasoning, and coding tasks demonstrate that\nDynaSwarm consistently outperforms state-of-the-art single-agent and MAS\nbaselines across multiple LLM backbones. Our findings highlight the importance\nof sample-aware structural flexibility in LLM MAS designs.", "AI": {"tldr": "DynaSwarm\u662f\u4e00\u4e2a\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u6001\u56fe\u9009\u62e9\u5668\u4f18\u5316\u534f\u4f5c\u56fe\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\u4f9d\u8d56\u9759\u6001\u534f\u4f5c\u56fe\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faDynaSwarm\u6846\u67b6\uff0c\u7ed3\u5408A2C\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u548c\u52a8\u6001\u56fe\u9009\u62e9\u5668\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684LLM\u5fae\u8c03\u5b9e\u73b0\u6837\u672c\u611f\u77e5\u7684\u56fe\u7ed3\u6784\u4f18\u5316\u3002", "result": "\u5728\u95ee\u7b54\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0cDynaSwarm\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u3002", "conclusion": "\u6837\u672c\u611f\u77e5\u7684\u7ed3\u6784\u7075\u6d3b\u6027\u5bf9LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.23291", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23291", "abs": "https://arxiv.org/abs/2507.23291", "authors": ["Yuetian Chen", "Zhiqi Wang", "Nathalie Baracaldo", "Swanand Ravindra Kadhe", "Lei Yu"], "title": "Evaluating the Dynamics of Membership Privacy in Deep Learning", "comment": null, "summary": "Membership inference attacks (MIAs) pose a critical threat to the privacy of\ntraining data in deep learning. Despite significant progress in attack\nmethodologies, our understanding of when and how models encode membership\ninformation during training remains limited. This paper presents a dynamic\nanalytical framework for dissecting and quantifying privacy leakage dynamics at\nthe individual sample level. By tracking per-sample vulnerabilities on an\nFPR-TPR plane throughout training, our framework systematically measures how\nfactors such as dataset complexity, model architecture, and optimizer choice\ninfluence the rate and severity at which samples become vulnerable. Crucially,\nwe discover a robust correlation between a sample's intrinsic learning\ndifficulty, and find that the privacy risk of samples highly vulnerable in the\nfinal trained model is largely determined early during training. Our results\nthus provide a deeper understanding of how privacy risks dynamically emerge\nduring training, laying the groundwork for proactive, privacy-aware model\ntraining strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e2a\u4f53\u6837\u672c\u7684\u9690\u79c1\u6cc4\u9732\u52a8\u6001\uff0c\u63ed\u793a\u4e86\u6837\u672c\u5b66\u4e60\u96be\u5ea6\u4e0e\u9690\u79c1\u98ce\u9669\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5982\u4f55\u4ee5\u53ca\u4f55\u65f6\u7f16\u7801\u6210\u5458\u4fe1\u606f\uff0c\u4ee5\u5e94\u5bf9\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIAs\uff09\u5bf9\u9690\u79c1\u7684\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u8ddf\u8e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6bcf\u4e2a\u6837\u672c\u5728FPR-TPR\u5e73\u9762\u4e0a\u7684\u8106\u5f31\u6027\uff0c\u7cfb\u7edf\u6d4b\u91cf\u6570\u636e\u96c6\u590d\u6742\u6027\u3001\u6a21\u578b\u67b6\u6784\u548c\u4f18\u5316\u5668\u9009\u62e9\u7b49\u56e0\u7d20\u5bf9\u6837\u672c\u8106\u5f31\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6837\u672c\u7684\u5185\u5728\u5b66\u4e60\u96be\u5ea6\u4e0e\u9690\u79c1\u98ce\u9669\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u4e14\u6700\u7ec8\u6a21\u578b\u4e2d\u9ad8\u5ea6\u8106\u5f31\u7684\u6837\u672c\u7684\u9690\u79c1\u98ce\u9669\u5728\u8bad\u7ec3\u65e9\u671f\u5c31\u5df2\u786e\u5b9a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u9690\u79c1\u98ce\u9669\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u51fa\u73b0\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u652f\u6301\u5f00\u53d1\u4e3b\u52a8\u7684\u9690\u79c1\u611f\u77e5\u6a21\u578b\u8bad\u7ec3\u7b56\u7565\u3002"}}
{"id": "2507.23292", "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.23292", "abs": "https://arxiv.org/abs/2507.23292", "authors": ["RJ Skerry-Ryan", "Julian Salazar", "Soroosh Mariooryad", "David Kao", "Daisy Stanton", "Eric Battenberg", "Matt Shannon", "Ron J. Weiss", "Robin Scheibler", "Jonas Rothfuss", "Tom Bagby"], "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy", "comment": null, "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u5e8f\u5217\u5efa\u6a21\u7684\u795e\u7ecf\u7f51\u7edc\u5c42API\u548c\u5e93\uff0c\u652f\u6301\u9010\u5c42\u548c\u9010\u6b65\u6267\u884c\uff0c\u786e\u4fdd\u72b6\u6001\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u7ec4\u5408\u7684API\u3002", "motivation": "\u7b80\u5316\u5e8f\u5217\u6a21\u578b\u7684\u521b\u5efa\uff0c\u786e\u4fdd\u5728\u6d41\u5f0f\u5904\u7406\u548c\u5e76\u884c\u5e8f\u5217\u5904\u7406\u4e2d\u7684\u6b63\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u663e\u5f0f\u72b6\u6001\u8868\u793a\u548c\u9010\u6b65\u65b9\u6cd5\uff0c\u786e\u4fdd\u72b6\u6001\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u7ec4\u5408\u7684API\u548c\u5e93\u5b9e\u73b0\u3002", "result": "\u5b9e\u73b0\u4e86\u590d\u6742\u6a21\u578b\u7684\u5373\u65f6\u6d41\u5f0f\u5904\u7406\uff0c\u51cf\u5c11\u4e86\u5e38\u89c1\u9519\u8bef\uff0c\u5e76\u63d0\u4f9b\u4e86JAX\u548cTensorFlow 2\u7684\u5b9e\u73b0\u3002", "conclusion": "SequenceLayers\u4e3a\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u6b63\u786e\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u7ea7\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2507.23303", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23303", "abs": "https://arxiv.org/abs/2507.23303", "authors": ["Luca Corbucci", "Javier Alejandro Borges Legrottaglie", "Francesco Spinnato", "Anna Monreale", "Riccardo Guidotti"], "title": "An Interpretable Data-Driven Unsupervised Approach for the Prevention of Forgotten Items", "comment": null, "summary": "Accurately identifying items forgotten during a supermarket visit and\nproviding clear, interpretable explanations for recommending them remains an\nunderexplored problem within the Next Basket Prediction (NBP) domain. Existing\nNBP approaches typically only focus on forecasting future purchases, without\nexplicitly addressing the detection of unintentionally omitted items. This gap\nis partly due to the scarcity of real-world datasets that allow for the\nreliable estimation of forgotten items. Furthermore, most current NBP methods\nrely on black-box models, which lack transparency and limit the ability to\njustify recommendations to end users. In this paper, we formally introduce the\nforgotten item prediction task and propose two novel interpretable-by-design\nalgorithms. These methods are tailored to identify forgotten items while\noffering intuitive, human-understandable explanations. Experiments on a\nreal-world retail dataset show our algorithms outperform state-of-the-art NBP\nbaselines by 10-15% across multiple evaluation metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u8d85\u5e02\u8d2d\u7269\u4e2d\u9057\u5fd8\u7269\u54c1\u9884\u6d4b\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709NBP\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709NBP\u65b9\u6cd5\u4ec5\u5173\u6ce8\u672a\u6765\u8d2d\u4e70\u9884\u6d4b\uff0c\u672a\u89e3\u51b3\u9057\u5fd8\u7269\u54c1\u68c0\u6d4b\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u900f\u660e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u53ef\u89e3\u91ca\u6027\u8bbe\u8ba1\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u9057\u5fd8\u7269\u54c1\u5e76\u63d0\u4f9b\u76f4\u89c2\u89e3\u91ca\u3002", "result": "\u5728\u771f\u5b9e\u96f6\u552e\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u7b97\u6cd5\u6bd4\u73b0\u6709NBP\u57fa\u7ebf\u6027\u80fd\u63d0\u534710-15%\u3002", "conclusion": "\u8bba\u6587\u586b\u8865\u4e86NBP\u9886\u57df\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9057\u5fd8\u7269\u54c1\u9884\u6d4b\u95ee\u9898\u3002"}}
{"id": "2507.23317", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23317", "abs": "https://arxiv.org/abs/2507.23317", "authors": ["Tao He", "Rongchuan Mu", "Lizi Liao", "Yixin Cao", "Ming Liu", "Bing Qin"], "title": "Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning Model More Efficient Math Learner", "comment": "33 pages, 3 figures, 19 tables", "summary": "Large reasoning models (LRMs) have recently shown promise in solving complex\nmath problems when optimized with Reinforcement Learning (RL). But conventional\napproaches rely on outcome-only rewards that provide sparse feedback, resulting\nin inefficient optimization process. In this work, we investigate the function\nof process reward models (PRMs) to accelerate the RL training for LRMs. We\npropose a novel intrinsic signal-driven generative process evaluation mechanism\noperating at the thought level to address major bottlenecks in RL-based\ntraining. Specifically, instead of requiring PRMs to know how to solve\nproblems, our method uses intrinsic signals in solutions to judge stepwise\ncorrectness and aggregate contiguous correct/incorrect steps into coherent\n'thought' units. This structured, thought-level rewards enable more reliable\ncredit assignment by reducing ambiguity in step segmentation and alleviating\nreward hacking. We further introduce a capability-adaptive reward mechanism\nthat dynamically balances exploration and exploitation based on the LRM's\ncurrent proficiency, guiding learning without stifling creative\ntrial-and-error. These innovations are integrated into a new off-policy RL\nalgorithm, TP-GRPO, which extends grouped proximal optimization with\nprocess-based rewards and improves training efficiency. Experiments on 1.5B and\n7B parameter LRMs demonstrate that our method achieves higher problem-solving\naccuracy with significantly fewer training samples than outcome-only reward\nbaselines. The results validate that well-structured process rewards can\nsubstantially accelerate LRM optimization in math reasoning tasks. Code is\navailable at https://github.com/cs-holder/tp_grpo.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5TP-GRPO\uff0c\u901a\u8fc7\u5185\u5728\u4fe1\u53f7\u9a71\u52a8\u7684\u751f\u6210\u8fc7\u7a0b\u8bc4\u4f30\u673a\u5236\uff0c\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\u53cd\u9988\uff0c\u5bfc\u81f4\u4f18\u5316\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u66f4\u5bc6\u96c6\u7684\u53cd\u9988\uff0c\u52a0\u901fLRMs\u7684\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u5185\u5728\u4fe1\u53f7\u9a71\u52a8\u7684\u751f\u6210\u8fc7\u7a0b\u8bc4\u4f30\u673a\u5236\uff0c\u5c06\u8fde\u7eed\u6b63\u786e/\u9519\u8bef\u7684\u6b65\u9aa4\u805a\u5408\u4e3a\u8fde\u8d2f\u7684\u2018\u601d\u8003\u2019\u5355\u5143\uff0c\u5e76\u5f15\u5165\u80fd\u529b\u81ea\u9002\u5e94\u5956\u52b1\u673a\u5236\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u57281.5B\u548c7B\u53c2\u6570\u7684LRMs\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4ec5\u4f9d\u8d56\u7ed3\u679c\u5956\u52b1\u7684\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u95ee\u9898\u89e3\u51b3\u51c6\u786e\u7387\uff0c\u4e14\u8bad\u7ec3\u6837\u672c\u66f4\u5c11\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u8fc7\u7a0b\u5956\u52b1\u80fd\u663e\u8457\u52a0\u901fLRMs\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4f18\u5316\u3002"}}
{"id": "2507.23335", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23335", "abs": "https://arxiv.org/abs/2507.23335", "authors": ["Qilin Zhou", "Haipeng Wang", "Zhengyuan Wei", "W. K. Chan"], "title": "Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions", "comment": "accepted by QRS 2025", "summary": "Patch robustness certification is an emerging verification approach for\ndefending against adversarial patch attacks with provable guarantees for deep\nlearning systems. Certified recovery techniques guarantee the prediction of the\nsole true label of a certified sample. However, existing techniques, if\napplicable to top-k predictions, commonly conduct pairwise comparisons on those\nvotes between labels, failing to certify the sole true label within the top k\nprediction labels precisely due to the inflation on the number of votes\ncontrolled by the attacker (i.e., attack budget); yet enumerating all\ncombinations of vote allocation suffers from the combinatorial explosion\nproblem. We propose CostCert, a novel, scalable, and precise voting-based\ncertified recovery defender. CostCert verifies the true label of a sample\nwithin the top k predictions without pairwise comparisons and combinatorial\nexplosion through a novel design: whether the attack budget on the sample is\ninfeasible to cover the smallest total additional votes on top of the votes\nuncontrollable by the attacker to exclude the true labels from the top k\nprediction labels. Experiments show that CostCert significantly outperforms the\ncurrent state-of-the-art defender PatchGuard, such as retaining up to 57.3% in\ncertified accuracy when the patch size is 96, whereas PatchGuard has already\ndropped to zero.", "AI": {"tldr": "CostCert\u662f\u4e00\u79cd\u65b0\u9896\u3001\u53ef\u6269\u5c55\u4e14\u7cbe\u786e\u7684\u57fa\u4e8e\u6295\u7968\u7684\u8ba4\u8bc1\u6062\u590d\u9632\u5fa1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u6297\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u5e94\u7528\u4e8etop-k\u9884\u6d4b\u65f6\uff0c\u65e0\u6cd5\u7cbe\u786e\u8ba4\u8bc1\u771f\u5b9e\u6807\u7b7e\uff0c\u4e14\u5b58\u5728\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u3002", "method": "CostCert\u901a\u8fc7\u9a8c\u8bc1\u653b\u51fb\u9884\u7b97\u662f\u5426\u4e0d\u8db3\u4ee5\u8986\u76d6\u6700\u5c0f\u989d\u5916\u6295\u7968\u6570\u6765\u6392\u9664\u771f\u5b9e\u6807\u7b7e\uff0c\u907f\u514d\u4e86\u6210\u5bf9\u6bd4\u8f83\u548c\u7ec4\u5408\u7206\u70b8\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCostCert\u663e\u8457\u4f18\u4e8ePatchGuard\uff0c\u5982\u5728\u8865\u4e01\u5927\u5c0f\u4e3a96\u65f6\u4fdd\u630157.3%\u7684\u8ba4\u8bc1\u51c6\u786e\u7387\uff0c\u800cPatchGuard\u5df2\u964d\u81f3\u96f6\u3002", "conclusion": "CostCert\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u8ba4\u8bc1\u6062\u590d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u9632\u5fa1\u3002"}}
{"id": "2507.23344", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23344", "abs": "https://arxiv.org/abs/2507.23344", "authors": ["Tatsuya Mitomi", "Fumiyasu Makinoshima", "Fumiya Makihara", "Eigo Segawa"], "title": "Designing Dynamic Pricing for Bike-sharing Systems via Differentiable Agent-based Simulation", "comment": null, "summary": "Bike-sharing systems are emerging in various cities as a new ecofriendly\ntransportation system. In these systems, spatiotemporally varying user demands\nlead to imbalanced inventory at bicycle stations, resulting in additional\nrelocation costs. Therefore, it is essential to manage user demand through\noptimal dynamic pricing for the system. However, optimal pricing design for\nsuch a system is challenging because the system involves users with diverse\nbackgrounds and their probabilistic choices. To address this problem, we\ndevelop a differentiable agent-based simulation to rapidly design dynamic\npricing in bike-sharing systems, achieving balanced bicycle inventory despite\nspatiotemporally heterogeneous trips and probabilistic user decisions. We first\nvalidate our approach against conventional methods through numerical\nexperiments involving 25 bicycle stations and five time slots, yielding 100\nparameters. Compared to the conventional methods, our approach obtains a more\naccurate solution with a 73% to 78% reduction in loss while achieving more than\na 100-fold increase in convergence speed. We further validate our approach on a\nlarge-scale urban bike-sharing system scenario involving 289 bicycle stations,\nresulting in a total of 1156 parameters. Through simulations using the obtained\npricing policies, we confirm that these policies can naturally induce balanced\ninventory without any manual relocation. Additionally, we find that the cost of\ndiscounts to induce the balanced inventory can be minimized by setting\nappropriate initial conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u4ee3\u7406\u6a21\u62df\u7684\u52a8\u6001\u5b9a\u4ef7\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\u7684\u5e93\u5b58\u5e73\u8861\uff0c\u663e\u8457\u51cf\u5c11\u635f\u5931\u5e76\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\u4e2d\uff0c\u65f6\u7a7a\u53d8\u5316\u7684\u7528\u6237\u9700\u6c42\u5bfc\u81f4\u5e93\u5b58\u4e0d\u5e73\u8861\uff0c\u589e\u52a0\u642c\u8fc1\u6210\u672c\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u52a8\u6001\u5b9a\u4ef7\u4f18\u5316\u7528\u6237\u9700\u6c42\u7ba1\u7406\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u4ee3\u7406\u6a21\u62df\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u8bbe\u8ba1\u52a8\u6001\u5b9a\u4ef7\u7b56\u7565\uff0c\u8003\u8651\u4e86\u7528\u6237\u591a\u6837\u6027\u548c\u6982\u7387\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51cf\u5c11\u4e8673%\u81f378%\u7684\u635f\u5931\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e86100\u500d\u4ee5\u4e0a\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u52a8\u6001\u5b9a\u4ef7\u7b56\u7565\u80fd\u81ea\u7136\u8bf1\u5bfc\u5e93\u5b58\u5e73\u8861\uff0c\u65e0\u9700\u4eba\u5de5\u642c\u8fc1\uff0c\u4e14\u901a\u8fc7\u8bbe\u7f6e\u5408\u9002\u7684\u521d\u59cb\u6761\u4ef6\u53ef\u6700\u5c0f\u5316\u6298\u6263\u6210\u672c\u3002"}}
{"id": "2507.23389", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23389", "abs": "https://arxiv.org/abs/2507.23389", "authors": ["David Komnick", "Kathrin Lammers", "Barbara Hammer", "Valerie Vaquet", "Fabian Hinder"], "title": "Causal Explanation of Concept Drift -- A Truly Actionable Approach", "comment": "This manuscript is accepted to be presented at the TempXAI workshop\n  at the European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases (ECMLPKDD 2025)", "summary": "In a world that constantly changes, it is crucial to understand how those\nchanges impact different systems, such as industrial manufacturing or critical\ninfrastructure. Explaining critical changes, referred to as concept drift in\nthe field of machine learning, is the first step towards enabling targeted\ninterventions to avoid or correct model failures, as well as malfunctions and\nerrors in the physical world. Therefore, in this work, we extend model-based\ndrift explanations towards causal explanations, which increases the\nactionability of the provided explanations. We evaluate our explanation\nstrategy on a number of use cases, demonstrating the practical usefulness of\nour framework, which isolates the causally relevant features impacted by\nconcept drift and, thus, allows for targeted intervention.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u57fa\u4e8e\u6a21\u578b\u7684\u6f02\u79fb\u89e3\u91ca\u6269\u5c55\u4e3a\u56e0\u679c\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u89e3\u91ca\u7684\u53ef\u64cd\u4f5c\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5feb\u901f\u53d8\u5316\u7684\u4e16\u754c\u4e2d\uff0c\u7406\u89e3\u53d8\u5316\u5bf9\u7cfb\u7edf\u7684\u5f71\u54cd\uff08\u5982\u6982\u5ff5\u6f02\u79fb\uff09\u662f\u907f\u514d\u6a21\u578b\u5931\u8d25\u548c\u7269\u7406\u4e16\u754c\u6545\u969c\u7684\u5173\u952e\u3002", "method": "\u6269\u5c55\u57fa\u4e8e\u6a21\u578b\u7684\u6f02\u79fb\u89e3\u91ca\u4e3a\u56e0\u679c\u89e3\u91ca\uff0c\u901a\u8fc7\u6846\u67b6\u8bc6\u522b\u53d7\u6982\u5ff5\u6f02\u79fb\u5f71\u54cd\u7684\u56e0\u679c\u76f8\u5173\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\u3002", "conclusion": "\u63d0\u51fa\u7684\u56e0\u679c\u89e3\u91ca\u6846\u67b6\u63d0\u9ad8\u4e86\u5bf9\u6982\u5ff5\u6f02\u79fb\u7684\u7406\u89e3\u548c\u5e72\u9884\u80fd\u529b\u3002"}}
{"id": "2507.23391", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23391", "abs": "https://arxiv.org/abs/2507.23391", "authors": ["Tung M. Luu", "Donghoon Lee", "Younghwan Lee", "Chang D. Yoo"], "title": "Policy Learning from Large Vision-Language Model Feedback without Reward Modeling", "comment": "Accepted to IROS 2025", "summary": "Offline reinforcement learning (RL) provides a powerful framework for\ntraining robotic agents using pre-collected, suboptimal datasets, eliminating\nthe need for costly, time-consuming, and potentially hazardous online\ninteractions. This is particularly useful in safety-critical real-world\napplications, where online data collection is expensive and impractical.\nHowever, existing offline RL algorithms typically require reward labeled data,\nwhich introduces an additional bottleneck: reward function design is itself\ncostly, labor-intensive, and requires significant domain expertise. In this\npaper, we introduce PLARE, a novel approach that leverages large\nvision-language models (VLMs) to provide guidance signals for agent training.\nInstead of relying on manually designed reward functions, PLARE queries a VLM\nfor preference labels on pairs of visual trajectory segments based on a\nlanguage task description. The policy is then trained directly from these\npreference labels using a supervised contrastive preference learning objective,\nbypassing the need to learn explicit reward models. Through extensive\nexperiments on robotic manipulation tasks from the MetaWorld, PLARE achieves\nperformance on par with or surpassing existing state-of-the-art VLM-based\nreward generation methods. Furthermore, we demonstrate the effectiveness of\nPLARE in real-world manipulation tasks with a physical robot, further\nvalidating its practical applicability.", "AI": {"tldr": "PLARE\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u6307\u5bfc\u4fe1\u53f7\uff0c\u907f\u514d\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u504f\u597d\u6807\u7b7e\u76f4\u63a5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u907f\u514d\u4e86\u5728\u7ebf\u4ea4\u4e92\u7684\u9ad8\u6210\u672c\u548c\u98ce\u9669\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u589e\u52a0\u4e86\u8d1f\u62c5\u3002PLARE\u65e8\u5728\u901a\u8fc7VLM\u81ea\u52a8\u751f\u6210\u6307\u5bfc\u4fe1\u53f7\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "method": "PLARE\u901a\u8fc7VLM\u5bf9\u89c6\u89c9\u8f68\u8ff9\u7247\u6bb5\u751f\u6210\u504f\u597d\u6807\u7b7e\uff0c\u57fa\u4e8e\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\uff0c\u4f7f\u7528\u76d1\u7763\u5bf9\u6bd4\u504f\u597d\u5b66\u4e60\u76ee\u6807\u76f4\u63a5\u8bad\u7ec3\u7b56\u7565\uff0c\u65e0\u9700\u663e\u5f0f\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728MetaWorld\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cPLARE\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709VLM\u5956\u52b1\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "PLARE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u5bf9\u4eba\u5de5\u5956\u52b1\u8bbe\u8ba1\u7684\u4f9d\u8d56\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.23412", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23412", "abs": "https://arxiv.org/abs/2507.23412", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "A Machine Learning Approach for Honey Adulteration Detection using Mineral Element Profiles", "comment": null, "summary": "This paper aims to develop a Machine Learning (ML)-based system for detecting\nhoney adulteration utilizing honey mineral element profiles. The proposed\nsystem comprises two phases: preprocessing and classification. The\npreprocessing phase involves the treatment of missing-value attributes and\nnormalization. In the classifica-tion phase, we use three supervised ML models:\nlogistic regression, decision tree, and random forest, to dis-criminate between\nauthentic and adulterated honey. To evaluate the performance of the ML models,\nwe use a public dataset comprising measurements of mineral element content of\nauthentic honey, sugar syrups, and adul-terated honey. Experimental findings\nshow that mineral element content in honey provides robust discriminative\ninformation for detecting honey adulteration. Results also demonstrate that the\nrandom forest-based classifier outperforms other classifiers on this dataset,\nachieving the highest cross-validation accuracy of 98.37%.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7cfb\u7edf\uff0c\u5229\u7528\u8702\u871c\u77ff\u7269\u8d28\u5143\u7d20\u8c31\u68c0\u6d4b\u8702\u871c\u63ba\u5047\uff0c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe98.37%\u3002", "motivation": "\u8702\u871c\u63ba\u5047\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u8702\u871c\u7684\u77ff\u7269\u8d28\u5143\u7d20\u8c31\u6765\u68c0\u6d4b\u63ba\u5047\u3002", "method": "\u7cfb\u7edf\u5206\u4e3a\u9884\u5904\u7406\u548c\u5206\u7c7b\u4e24\u9636\u6bb5\uff1a\u9884\u5904\u7406\u5305\u62ec\u7f3a\u5931\u503c\u5904\u7406\u548c\u5f52\u4e00\u5316\uff1b\u5206\u7c7b\u9636\u6bb5\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u3001\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797\u4e09\u79cd\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8702\u871c\u77ff\u7269\u8d28\u5143\u7d20\u80fd\u6709\u6548\u533a\u5206\u771f\u5047\u8702\u871c\uff0c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f18\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe98.37%\u3002", "conclusion": "\u57fa\u4e8e\u77ff\u7269\u8d28\u5143\u7d20\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u9ad8\u6548\u68c0\u6d4b\u8702\u871c\u63ba\u5047\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u662f\u9996\u9009\u5206\u7c7b\u5668\u3002"}}
{"id": "2507.23418", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23418", "abs": "https://arxiv.org/abs/2507.23418", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "Detection of Adulteration in Coconut Milk using Infrared Spectroscopy and Machine Learning", "comment": null, "summary": "In this paper, we propose a system for detecting adulteration in coconut\nmilk, utilizing infrared spectroscopy. The machine learning-based proposed\nsystem comprises three phases: preprocessing, feature extraction, and\nclassification. The first phase involves removing irrelevant data from coconut\nmilk spectral signals. In the second phase, we employ the Linear Discriminant\nAnalysis (LDA) algorithm for extracting the most discriminating features. In\nthe third phase, we use the K-Nearest Neighbor (KNN) model to classify coconut\nmilk samples into authentic or adulterated. We evaluate the performance of the\nproposed system using a public dataset comprising Fourier Transform Infrared\n(FTIR) spectral information of pure and contaminated coconut milk samples.\nFindings show that the proposed method successfully detects adulteration with a\ncross-validation accuracy of 93.33%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea2\u5916\u5149\u8c31\u548c\u673a\u5668\u5b66\u4e60\u7684\u6930\u5b50\u5976\u63ba\u5047\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u5904\u7406\u3001\u7279\u5f81\u63d0\u53d6\u548c\u5206\u7c7b\u4e09\u9636\u6bb5\u5b9e\u73b0\uff0c\u51c6\u786e\u7387\u8fbe93.33%\u3002", "motivation": "\u68c0\u6d4b\u6930\u5b50\u5976\u4e2d\u7684\u63ba\u5047\u95ee\u9898\uff0c\u786e\u4fdd\u98df\u54c1\u5b89\u5168\u548c\u8d28\u91cf\u3002", "method": "\u7cfb\u7edf\u5206\u4e3a\u9884\u5904\u7406\uff08\u53bb\u9664\u65e0\u5173\u6570\u636e\uff09\u3001\u7279\u5f81\u63d0\u53d6\uff08\u4f7f\u7528LDA\u7b97\u6cd5\uff09\u548c\u5206\u7c7b\uff08\u4f7f\u7528KNN\u6a21\u578b\uff09\u4e09\u4e2a\u9636\u6bb5\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a93.33%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u6930\u5b50\u5976\u63ba\u5047\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.23428", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23428", "abs": "https://arxiv.org/abs/2507.23428", "authors": ["Nodens F. Koren", "Samuel Lanthaler"], "title": "Merging Memory and Space: A Spatiotemporal State Space Neural Operator", "comment": null, "summary": "We propose the Spatiotemporal State Space Neural Operator (ST-SSM), a compact\narchitecture for learning solution operators of time-dependent partial\ndifferential equations (PDEs). ST-SSM introduces a novel factorization of the\nspatial and temporal dimensions, using structured state-space models to\nindependently model temporal evolution and spatial interactions. This design\nenables parameter efficiency and flexible modeling of long-range spatiotemporal\ndynamics. A theoretical connection is established between SSMs and neural\noperators, and a unified universality theorem is proved for the resulting class\nof architectures. Empirically, we demonstrate that our factorized formulation\noutperforms alternative schemes such as zigzag scanning and parallel\nindependent processing on several PDE benchmarks, including 1D Burgers'\nequation, 1D Kuramoto-Sivashinsky equation, and 2D Navier-Stokes equations\nunder varying physical conditions. Our model performs competitively with\nexisting baselines while using significantly fewer parameters. In addition, our\nresults reinforce previous findings on the benefits of temporal memory by\nshowing improved performance under partial observability. Our results highlight\nthe advantages of dimensionally factorized operator learning for efficient and\ngeneralizable PDE modeling, and put this approach on a firm theoretical\nfooting.", "AI": {"tldr": "ST-SSM\u662f\u4e00\u79cd\u7d27\u51d1\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u5b66\u4e60\u65f6\u95f4\u4f9d\u8d56\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u89e3\u7b97\u5b50\uff0c\u901a\u8fc7\u65f6\u7a7a\u7ef4\u5ea6\u5206\u89e3\u548c\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u53c2\u6570\u5316\u548c\u957f\u7a0b\u65f6\u7a7a\u52a8\u6001\u5efa\u6a21\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u53c2\u6570\u6548\u7387\u4f4e\u548c\u957f\u7a0b\u52a8\u6001\u5efa\u6a21\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65f6\u7a7a\u7ef4\u5ea6\u5206\u89e3\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5206\u522b\u5efa\u6a21\u65f6\u95f4\u6f14\u5316\u548c\u7a7a\u95f4\u4ea4\u4e92\uff0c\u5e76\u5efa\u7acb\u7406\u8bba\u8fde\u63a5\u3002", "result": "\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6848\uff0c\u53c2\u6570\u66f4\u5c11\u4e14\u6027\u80fd\u7ade\u4e89\u6027\u5f3a\uff0c\u90e8\u5206\u89c2\u6d4b\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u65f6\u7a7a\u5206\u89e3\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u9ad8\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\uff0c\u4e3aPDE\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.23437", "categories": ["cs.LG", "I.2.6; C.1.3; C.3"], "pdf": "https://arxiv.org/pdf/2507.23437", "abs": "https://arxiv.org/abs/2507.23437", "authors": ["Yinhui Ma", "Tomomasa Yamasaki", "Zhehui Wang", "Tao Luo", "Bo Wang"], "title": "Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and Scalable DNN Accelerator Design", "comment": "Accepted to ICCAD 2025 (camera-ready); 9 pages, 5 figures", "summary": "Hardware-Aware Neural Architecture Search (HW-NAS) is an efficient approach\nto automatically co-optimizing neural network performance and hardware energy\nefficiency, making it particularly useful for the development of Deep Neural\nNetwork accelerators on the edge. However, the extensive search space and high\ncomputational cost pose significant challenges to its practical adoption. To\naddress these limitations, we propose Coflex, a novel HW-NAS framework that\nintegrates the Sparse Gaussian Process (SGP) with multi-objective Bayesian\noptimization. By leveraging sparse inducing points, Coflex reduces the GP\nkernel complexity from cubic to near-linear with respect to the number of\ntraining samples, without compromising optimization performance. This enables\nscalable approximation of large-scale search space, substantially decreasing\ncomputational overhead while preserving high predictive accuracy. We evaluate\nthe efficacy of Coflex across various benchmarks, focusing on\naccelerator-specific architecture. Our experi- mental results show that Coflex\noutperforms state-of-the-art methods in terms of network accuracy and\nEnergy-Delay-Product, while achieving a computational speed-up ranging from\n1.9x to 9.5x.", "AI": {"tldr": "Coflex\u662f\u4e00\u79cd\u65b0\u578b\u7684\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u9ad8\u65af\u8fc7\u7a0b\u548c\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u4fdd\u6301\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e2d\u641c\u7d22\u7a7a\u95f4\u5927\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7a00\u758f\u9ad8\u65af\u8fc7\u7a0b\u548c\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u5229\u7528\u7a00\u758f\u8bf1\u5bfc\u70b9\u964d\u4f4e\u6838\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoflex\u5728\u51c6\u786e\u6027\u548c\u80fd\u6548\u5ef6\u8fdf\u79ef\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u53471.9x\u81f39.5x\u3002", "conclusion": "Coflex\u4e3a\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23449", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23449", "abs": "https://arxiv.org/abs/2507.23449", "authors": ["Shervin Rahimzadeh Arashloo"], "title": "Manifold-regularised Signature Kernel Large-Margin $\\ell_p$-SVDD for Multidimensional Time Series Anomaly Detection", "comment": null, "summary": "We generalise the recently introduced large-margin $\\ell_p$-SVDD approach to\nexploit the geometry of data distribution via manifold regularising and a\nsignature kernel representation for time series anomaly detection.\nSpecifically, we formulate a manifold-regularised variant of the $\\ell_p$-SVDD\nmethod to encourage label smoothness on the underlying manifold to capture\nstructural information for improved detection performance. Drawing on an\nexisting Representer theorem, we then provide an effective optimisation\ntechnique for the proposed method and show that it can benefit from the\nsignature kernel to capture time series complexities for anomaly detection.\n  We theoretically study the proposed approach using Rademacher complexities to\nanalyse its generalisation performance and also provide an experimental\nassessment of the proposed method across various data sets to compare its\nperformance against other methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5f62\u6b63\u5219\u5316\u548c\u7b7e\u540d\u6838\u8868\u793a\u7684\u5927\u95f4\u9694\u2113p-SVDD\u65b9\u6cd5\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u6570\u636e\u5206\u5e03\u51e0\u4f55\u7ed3\u6784\u548c\u65f6\u95f4\u5e8f\u5217\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6d41\u5f62\u6b63\u5219\u5316\u2113p-SVDD\u65b9\u6cd5\uff0c\u7ed3\u5408\u7b7e\u540d\u6838\u8868\u793a\uff0c\u4f18\u5316\u6807\u7b7e\u5e73\u6ed1\u6027\u4ee5\u6355\u6349\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u5229\u7528Rademacher\u590d\u6742\u5ea6\u5206\u6790\u6cdb\u5316\u6027\u80fd\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u6570\u636e\u7ed3\u6784\u548c\u590d\u6742\u6027\u3002"}}
{"id": "2507.23491", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23491", "abs": "https://arxiv.org/abs/2507.23491", "authors": ["Olga Vershinina", "Jacopo Sabbatinelli", "Anna Rita Bonfigli", "Dalila Colombaretti", "Angelica Giuliani", "Mikhail Krivonosov", "Arseniy Trukhanov", "Claudio Franceschi", "Mikhail Ivanchenko", "Fabiola Olivieri"], "title": "Explainable artificial intelligence model predicting the risk of all-cause mortality in patients with type 2 diabetes mellitus", "comment": null, "summary": "Objective. Type 2 diabetes mellitus (T2DM) is a highly prevalent\nnon-communicable chronic disease that substantially reduces life expectancy.\nAccurate estimation of all-cause mortality risk in T2DM patients is crucial for\npersonalizing and optimizing treatment strategies. Research Design and Methods.\nThis study analyzed a cohort of 554 patients (aged 40-87 years) with diagnosed\nT2DM over a maximum follow-up period of 16.8 years, during which 202 patients\n(36%) died. Key survival-associated features were identified, and multiple\nmachine learning (ML) models were trained and validated to predict all-cause\nmortality risk. To improve model interpretability, Shapley additive\nexplanations (SHAP) was applied to the best-performing model. Results. The\nextra survival trees (EST) model, incorporating ten key features, demonstrated\nthe best predictive performance. The model achieved a C-statistic of 0.776,\nwith the area under the receiver operating characteristic curve (AUC) values of\n0.86, 0.80, 0.841, and 0.826 for 5-, 10-, 15-, and 16.8-year all-cause\nmortality predictions, respectively. The SHAP approach was employed to\ninterpret the model's individual decision-making processes. Conclusions. The\ndeveloped model exhibited strong predictive performance for mortality risk\nassessment. Its clinically interpretable outputs enable potential bedside\napplication, improving the identification of high-risk patients and supporting\ntimely treatment optimization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6a21\u578b\uff08EST\u6a21\u578b\uff09\uff0c\u7528\u4e8e\u9884\u6d4b2\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u5168\u56e0\u6b7b\u4ea1\u7387\u98ce\u9669\uff0c\u5e76\u901a\u8fc7SHAP\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "2\u578b\u7cd6\u5c3f\u75c5\uff08T2DM\uff09\u663e\u8457\u964d\u4f4e\u60a3\u8005\u9884\u671f\u5bff\u547d\uff0c\u51c6\u786e\u8bc4\u4f30\u5176\u5168\u56e0\u6b7b\u4ea1\u7387\u98ce\u9669\u5bf9\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86554\u540dT2DM\u60a3\u8005\u7684\u6570\u636e\uff0c\u4f7f\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6b7b\u4ea1\u7387\uff0c\u5e76\u901a\u8fc7SHAP\u65b9\u6cd5\u89e3\u91ca\u6700\u4f73\u6a21\u578b\uff08EST\u6a21\u578b\uff09\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "EST\u6a21\u578b\u8868\u73b0\u51fa\u6700\u4f73\u9884\u6d4b\u6027\u80fd\uff08C-statistic\u4e3a0.776\uff09\uff0c\u5e76\u5728\u4e0d\u540c\u65f6\u95f4\u70b9\u7684AUC\u503c\u5747\u8f83\u9ad8\uff08\u59825\u5e74\u4e3a0.86\uff09\u3002SHAP\u65b9\u6cd5\u6210\u529f\u89e3\u91ca\u4e86\u6a21\u578b\u51b3\u7b56\u3002", "conclusion": "\u8be5\u6a21\u578b\u5177\u6709\u5f3a\u9884\u6d4b\u6027\u80fd\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u9ad8\u98ce\u9669\u60a3\u8005\u5e76\u4f18\u5316\u6cbb\u7597\u3002"}}
{"id": "2507.23495", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.23495", "abs": "https://arxiv.org/abs/2507.23495", "authors": ["Maurits Kaptein"], "title": "Incorporating structural uncertainty in causal decision making", "comment": "This work is under review at the Journal of Causal Inference", "summary": "Practitioners making decisions based on causal effects typically ignore\nstructural uncertainty. We analyze when this uncertainty is consequential\nenough to warrant methodological solutions (Bayesian model averaging over\ncompeting causal structures). Focusing on bivariate relationships ($X\n\\rightarrow Y$ vs. $X \\leftarrow Y$), we establish that model averaging is\nbeneficial when: (1) structural uncertainty is moderate to high, (2) causal\neffects differ substantially between structures, and (3) loss functions are\nsufficiently sensitive to the size of the causal effect. We prove optimality\nresults of our suggested methodological solution under regularity conditions\nand demonstrate through simulations that modern causal discovery methods can\nprovide, within limits, the necessary quantification. Our framework complements\nexisting robust causal inference approaches by addressing a distinct source of\nuncertainty typically overlooked in practice.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u56e0\u679c\u6548\u5e94\u51b3\u7b56\u4e2d\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u8d1d\u53f6\u65af\u6a21\u578b\u5e73\u5747\u65b9\u6cd5\uff0c\u5e76\u786e\u5b9a\u4e86\u5176\u9002\u7528\u6761\u4ef6\u3002", "motivation": "\u5b9e\u8df5\u4e2d\u51b3\u7b56\u8005\u5e38\u5ffd\u7565\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\uff0c\u672c\u6587\u65e8\u5728\u5206\u6790\u5176\u5f71\u54cd\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u6a21\u578b\u5e73\u5747\u65b9\u6cd5\uff0c\u5206\u6790\u53cc\u53d8\u91cf\u5173\u7cfb\uff08X\u2192Y vs. X\u2190Y\uff09\uff0c\u5e76\u91cf\u5316\u9002\u7528\u6761\u4ef6\u3002", "result": "\u8bc1\u660e\u5728\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u9ad8\u3001\u56e0\u679c\u6548\u5e94\u5dee\u5f02\u5927\u4e14\u635f\u5931\u51fd\u6570\u654f\u611f\u65f6\uff0c\u6a21\u578b\u5e73\u5747\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "\u6846\u67b6\u8865\u5145\u4e86\u73b0\u6709\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5e38\u88ab\u5ffd\u89c6\u7684\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2507.23504", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23504", "abs": "https://arxiv.org/abs/2507.23504", "authors": ["Maurits Kaptein"], "title": "A Verifier Hierarchy", "comment": "This paper is primarily relevant to cs.CC, but submitted under cs.ML\n  due to lack of endorsement. The paper is under review at \"Information and\n  Communication\"", "summary": "We investigate the trade-off between certificate length and verifier runtime.\nWe prove a Verifier Trade-off Theorem showing that reducing the inherent\nverification time of a language from \\(f(n)\\) to \\(g(n)\\), where \\(f(n) \\ge\ng(n)\\), requires certificates of length at least \\(\\Omega(\\log(f(n) / g(n)))\\).\nThis theorem induces a natural hierarchy based on certificate complexity. We\ndemonstrate its applicability to analyzing conjectured separations between\ncomplexity classes (e.g., \\(\\np\\) and \\(\\exptime\\)) and to studying natural\nproblems such as string periodicity and rotation detection. Additionally, we\nprovide perspectives on the \\(\\p\\) vs. \\(\\np\\) problem by relating it to the\nexistence of sub-linear certificates.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8bc1\u4e66\u957f\u5ea6\u4e0e\u9a8c\u8bc1\u5668\u8fd0\u884c\u65f6\u95f4\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u9a8c\u8bc1\u5668\u6743\u8861\u5b9a\u7406\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u590d\u6742\u6027\u7406\u8bba\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u8bc1\u4e66\u957f\u5ea6\u4e0e\u9a8c\u8bc1\u5668\u8fd0\u884c\u65f6\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u7406\u89e3\u590d\u6742\u6027\u7c7b\u4e4b\u95f4\u7684\u5206\u79bb\u95ee\u9898\uff0c\u5982NP\u4e0eEXPTIME\uff0c\u4ee5\u53ca\u81ea\u7136\u95ee\u9898\uff08\u5982\u5b57\u7b26\u4e32\u5468\u671f\u6027\u548c\u65cb\u8f6c\u68c0\u6d4b\uff09\u7684\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u4e00\u4e2a\u9a8c\u8bc1\u5668\u6743\u8861\u5b9a\u7406\uff0c\u5c55\u793a\u4e86\u9a8c\u8bc1\u65f6\u95f4\u4ecef(n)\u51cf\u5c11\u5230g(n)\u6240\u9700\u7684\u6700\u5c0f\u8bc1\u4e66\u957f\u5ea6\u3002", "result": "\u9a8c\u8bc1\u5668\u6743\u8861\u5b9a\u7406\u8868\u660e\uff0c\u9a8c\u8bc1\u65f6\u95f4\u7684\u51cf\u5c11\u9700\u8981\u81f3\u5c11\u03a9(log(f(n)/g(n)))\u7684\u8bc1\u4e66\u957f\u5ea6\uff0c\u5e76\u5f62\u6210\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bc1\u4e66\u590d\u6742\u6027\u7684\u81ea\u7136\u5c42\u6b21\u7ed3\u6784\u3002", "conclusion": "\u8be5\u5b9a\u7406\u4e3a\u590d\u6742\u6027\u7c7b\u5206\u79bb\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u63a2\u8ba8\u4e86P\u4e0eNP\u95ee\u9898\u7684\u5b50\u7ebf\u6027\u8bc1\u4e66\u5b58\u5728\u6027\u3002"}}
{"id": "2507.23512", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.23512", "abs": "https://arxiv.org/abs/2507.23512", "authors": ["Saleh Vatan Khah", "Savelii Chezhegov", "Shahrokh Farahmand", "Samuel Horv\u00e1th", "Eduard Gorbunov"], "title": "Differentially Private Clipped-SGD: High-Probability Convergence with Arbitrary Clipping Level", "comment": "60 pages", "summary": "Gradient clipping is a fundamental tool in Deep Learning, improving the\nhigh-probability convergence of stochastic first-order methods like SGD,\nAdaGrad, and Adam under heavy-tailed noise, which is common in training large\nlanguage models. It is also a crucial component of Differential Privacy (DP)\nmechanisms. However, existing high-probability convergence analyses typically\nrequire the clipping threshold to increase with the number of optimization\nsteps, which is incompatible with standard DP mechanisms like the Gaussian\nmechanism. In this work, we close this gap by providing the first\nhigh-probability convergence analysis for DP-Clipped-SGD with a fixed clipping\nlevel, applicable to both convex and non-convex smooth optimization under\nheavy-tailed noise, characterized by a bounded central $\\alpha$-th moment\nassumption, $\\alpha \\in (1,2]$. Our results show that, with a fixed clipping\nlevel, the method converges to a neighborhood of the optimal solution with a\nfaster rate than the existing ones. The neighborhood can be balanced against\nthe noise introduced by DP, providing a refined trade-off between convergence\nspeed and privacy guarantees.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u56fa\u5b9a\u88c1\u526a\u9608\u503c\u4e0bDP-Clipped-SGD\u7684\u9ad8\u6982\u7387\u6536\u655b\u6027\uff0c\u9002\u7528\u4e8e\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u51f8\u548c\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9ad8\u6982\u7387\u6536\u655b\u5206\u6790\u901a\u5e38\u8981\u6c42\u88c1\u526a\u9608\u503c\u968f\u4f18\u5316\u6b65\u9aa4\u589e\u52a0\uff0c\u8fd9\u4e0e\u6807\u51c6DP\u673a\u5236\uff08\u5982\u9ad8\u65af\u673a\u5236\uff09\u4e0d\u517c\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56fa\u5b9a\u88c1\u526a\u6c34\u5e73\u7684DP-Clipped-SGD\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u51f8\u548c\u975e\u51f8\u5e73\u6ed1\u4f18\u5316\u95ee\u9898\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u56fa\u5b9a\u88c1\u526a\u6c34\u5e73\u4e0b\uff0c\u65b9\u6cd5\u4ee5\u66f4\u5feb\u901f\u5ea6\u6536\u655b\u5230\u6700\u4f18\u89e3\u90bb\u57df\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u548c\u9690\u79c1\u4fdd\u8bc1\u4e4b\u95f4\u53ef\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u56fa\u5b9a\u88c1\u526a\u6c34\u5e73\u4e0b\u9ad8\u6982\u7387\u6536\u655b\u5206\u6790\u7684\u7a7a\u767d\uff0c\u4e3aDP\u673a\u5236\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u6536\u655b\u901f\u5ea6\u548c\u9690\u79c1\u6743\u8861\u3002"}}
{"id": "2507.23534", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23534", "abs": "https://arxiv.org/abs/2507.23534", "authors": ["Chih-Fan Hsu", "Ming-Ching Chang", "Wei-Chao Chen"], "title": "Continual Learning with Synthetic Boundary Experience Blending", "comment": null, "summary": "Continual learning (CL) aims to address catastrophic forgetting in models\ntrained sequentially on multiple tasks. While experience replay has shown\npromise, its effectiveness is often limited by the sparse distribution of\nstored key samples, leading to overly simplified decision boundaries. We\nhypothesize that introducing synthetic data near the decision boundary\n(Synthetic Boundary Data, or SBD) during training serves as an implicit\nregularizer, improving boundary stability and mitigating forgetting. To\nvalidate this hypothesis, we propose a novel training framework, {\\bf\nExperience Blending}, which integrates knowledge from both stored key samples\nand synthetic, boundary-adjacent data. Experience blending consists of two core\ncomponents: (1) a multivariate Differential Privacy (DP) noise mechanism that\ninjects batch-wise noise into low-dimensional feature representations,\ngenerating SBD; and (2) an end-to-end training strategy that jointly leverages\nboth stored key samples and SBD. Extensive experiments on CIFAR-10, CIFAR-100,\nand Tiny ImageNet demonstrate that our method outperforms nine CL baselines,\nachieving accuracy improvements of 10%, 6%, and 13%, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aExperience Blending\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u8fb9\u754c\u6570\u636e\uff08SBD\uff09\u548c\u5b58\u50a8\u5173\u952e\u6837\u672c\u7684\u7ed3\u5408\uff0c\u7f13\u89e3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u4e25\u91cd\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u7ecf\u9a8c\u56de\u653e\u56e0\u6837\u672c\u5206\u5e03\u7a00\u758f\u5bfc\u81f4\u51b3\u7b56\u8fb9\u754c\u7b80\u5316\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faExperience Blending\u6846\u67b6\uff0c\u5305\u542b\u4e24\u90e8\u5206\uff1a(1) \u591a\u5143\u5dee\u5206\u9690\u79c1\u566a\u58f0\u673a\u5236\u751f\u6210SBD\uff1b(2) \u7ed3\u5408\u5b58\u50a8\u5173\u952e\u6837\u672c\u548cSBD\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cTiny ImageNet\u4e0a\uff0c\u65b9\u6cd5\u4f18\u4e8e9\u79cd\u57fa\u7ebf\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u534710%\u30016%\u548c13%\u3002", "conclusion": "\u5408\u6210\u8fb9\u754c\u6570\u636e\u4f5c\u4e3a\u9690\u5f0f\u6b63\u5219\u5316\u5668\uff0c\u6709\u6548\u63d0\u5347\u51b3\u7b56\u8fb9\u754c\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002"}}
{"id": "2507.23535", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.23535", "abs": "https://arxiv.org/abs/2507.23535", "authors": ["Dhanesh Ramachandram", "Himanshu Joshi", "Judy Zhu", "Dhari Gandhi", "Lucas Hartman", "Ananya Raval"], "title": "Transparent AI: The Case for Interpretability and Explainability", "comment": null, "summary": "As artificial intelligence systems increasingly inform high-stakes decisions\nacross sectors, transparency has become foundational to responsible and\ntrustworthy AI implementation. Leveraging our role as a leading institute in\nadvancing AI research and enabling industry adoption, we present key insights\nand lessons learned from practical interpretability applications across diverse\ndomains. This paper offers actionable strategies and implementation guidance\ntailored to organizations at varying stages of AI maturity, emphasizing the\nintegration of interpretability as a core design principle rather than a\nretrospective add-on.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u900f\u660e\u6027\u5728AI\u7cfb\u7edf\u9ad8\u5f71\u54cd\u51b3\u7b56\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u53ef\u89e3\u91ca\u6027\u5e94\u7528\u7684\u5173\u952e\u89c1\u89e3\u548c\u5b9e\u65bd\u7b56\u7565\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7684\u4f5c\u7528\u65e5\u76ca\u589e\u5f3a\uff0c\u900f\u660e\u6027\u6210\u4e3a\u8d1f\u8d23\u4efb\u548c\u53ef\u4fe1AI\u5b9e\u65bd\u7684\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\uff0c\u603b\u7ed3\u4e86\u53ef\u89e3\u91ca\u6027\u5728\u4e0d\u540c\u9886\u57df\u7684\u5b9e\u8df5\u7ecf\u9a8c\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u4e0d\u540cAI\u6210\u719f\u5ea6\u7ec4\u7ec7\u7684\u53ef\u64cd\u4f5c\u7b56\u7565\u548c\u5b9e\u65bd\u6307\u5357\u3002", "conclusion": "\u5f3a\u8c03\u5e94\u5c06\u53ef\u89e3\u91ca\u6027\u4f5c\u4e3a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff0c\u800c\u975e\u4e8b\u540e\u8865\u5145\u3002"}}
{"id": "2507.23536", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23536", "abs": "https://arxiv.org/abs/2507.23536", "authors": ["Georg Slamanig", "Francesco Corti", "Olga Saukh"], "title": "From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs\nof updating deep learning models by minimizing the number of additional\nparameters used to adapt a model to a down- stream task. While extensively\nresearched in large language models (LLMs), their application to smaller models\nused on edge devices, such as convolutional neural networks, remains\nunderexplored. This paper benchmarks and analyzes popular PEFT methods on\nconvolutional architectures typically deployed in resource-constrained edge\nenvironments. We evaluate LoRA, DoRA, and GaLore for updating standard and\ndepthwise convolutional architectures to handle distribution shifts and\naccommodate unseen classes. We utilize recently proposed PyTorch profilers to\ncompare the updated model performance and computational costs of these PEFT\nmethods with traditional fine-tuning approaches. With resource efficiency in\nmind, we investigate their update behavior across different rank dimensions. We\nfind that the evaluated PEFT methods are only half as memory-efficient when\napplied to depthwise-separable convolution architectures, compared to their\nefficiency with LLMs. Conversely, when targeting convolu- tional architectures\noptimized for edge deployment, adapter-based PEFT methods can reduce floating\npoint operations (FLOPs) during model updates by up to 95%. These insights\noffer valuable guidance for selecting PEFT methods based on hardware\nconstraints, performance requirements, and application needs. Our code is\nonline.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u6548\u7387\u56e0\u67b6\u6784\u4e0d\u540c\u800c\u5f02\uff0c\u5e76\u63d0\u4f9b\u4e86\u9009\u62e9\u65b9\u6cd5\u7684\u6307\u5bfc\u3002", "motivation": "\u63a2\u7d22PEFT\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9002\u7528\u6027\uff0c\u586b\u8865\u5176\u5728\u5c0f\u578b\u6a21\u578b\uff08\u5982\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u4e2d\u5e94\u7528\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5728\u6807\u51c6\u5377\u79ef\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u67b6\u6784\u4e0a\u8bc4\u4f30LoRA\u3001DoRA\u548cGaLore\u7b49PEFT\u65b9\u6cd5\uff0c\u4f7f\u7528PyTorch\u5206\u6790\u5de5\u5177\u6bd4\u8f83\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u3002", "result": "PEFT\u65b9\u6cd5\u5728\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u67b6\u6784\u4e2d\u7684\u5185\u5b58\u6548\u7387\u4ec5\u4e3aLLMs\u7684\u4e00\u534a\uff0c\u4f46\u5728\u8fb9\u7f18\u4f18\u5316\u7684\u5377\u79ef\u67b6\u6784\u4e2d\u53ef\u51cf\u5c1195%\u7684FLOPs\u3002", "conclusion": "PEFT\u65b9\u6cd5\u7684\u9009\u62e9\u9700\u7ed3\u5408\u786c\u4ef6\u9650\u5236\u3001\u6027\u80fd\u9700\u6c42\u548c\u4efb\u52a1\u7279\u70b9\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2507.23539", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.23539", "abs": "https://arxiv.org/abs/2507.23539", "authors": ["Piotr Indyk", "Michael Kapralov", "Kshiteej Sheth", "Tal Wagner"], "title": "Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions", "comment": "Published in ICLR 2025", "summary": "Motivated by the problem of fast processing of attention matrices, we study\nfast algorithms for computing matrix-vector products for asymmetric Gaussian\nKernel matrices $K\\in \\mathbb{R}^{n\\times n}$. $K$'s columns are indexed by a\nset of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$\nqueries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =\ne^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given\na vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to\noutput a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in\ntime subquadratic in $n$ and linear in $d$. Our algorithms rely on the\nfollowing modelling assumption about the matrices $K$: the sum of the entries\nof $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We\nvalidate this assumption experimentally, for Gaussian kernel matrices\nencountered in various settings such as fast attention computation in LLMs. We\nobtain the first subquadratic-time algorithm that works under this assumption,\nfor unrestricted vectors.", "AI": {"tldr": "\u7814\u7a76\u5feb\u901f\u8ba1\u7b97\u975e\u5bf9\u79f0\u9ad8\u65af\u6838\u77e9\u9635\u5411\u91cf\u79ef\u7684\u7b97\u6cd5\uff0c\u63d0\u51fa\u9996\u4e2a\u5728\u77e9\u9635\u548c\u7ebf\u6027\u589e\u957f\u5047\u8bbe\u4e0b\u7684\u6b21\u4e8c\u6b21\u65f6\u95f4\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5feb\u901f\u5904\u7406\u6ce8\u610f\u529b\u77e9\u9635\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u9ad8\u65af\u6838\u77e9\u9635\u7684\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u77e9\u9635\u548c\u7ebf\u6027\u589e\u957f\u7684\u5047\u8bbe\uff0c\u8bbe\u8ba1\u6b21\u4e8c\u6b21\u65f6\u95f4\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u65e0\u9650\u5236\u5411\u91cf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5047\u8bbe\u7684\u5408\u7406\u6027\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4e86\u6b21\u4e8c\u6b21\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u9ad8\u65af\u6838\u77e9\u9635\u7684\u9ad8\u6548\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7b49\u573a\u666f\u3002"}}
{"id": "2507.23562", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.23562", "abs": "https://arxiv.org/abs/2507.23562", "authors": ["Sirine Arfa", "Bernhard Vogginger", "Christian Mayr"], "title": "Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform", "comment": "8 pages, 5 figures, 3 tables", "summary": "Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power\nconsumption and low-latency inference on neuromorphic hardware for a wide range\nof robotic tasks. In this work, we present an energy-efficient implementation\nof a reinforcement learning (RL) algorithm using quantized SNNs to solve two\nclassical control tasks. The network is trained using the Q-learning algorithm,\nthen fine-tuned and quantized to low-bit (8-bit) precision for embedded\ndeployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative\nadvantage of SpiNNaker2 over conventional computing platforms, we analyze\ninference latency, dynamic power consumption, and energy cost per inference for\nour SNN models, comparing performance against a GTX 1650 GPU baseline. Our\nresults demonstrate SpiNNaker2's strong potential for scalable, low-energy\nneuromorphic computing, achieving up to 32x reduction in energy consumption.\nInference latency remains on par with GPU-based execution, with improvements\nobserved in certain task settings, reinforcing SpiNNaker2's viability for\nreal-time neuromorphic control and making the neuromorphic approach a\ncompelling direction for efficient deep Q-learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5316\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\uff0c\u5e76\u5728SpiNNaker2\u795e\u7ecf\u5f62\u6001\u82af\u7247\u4e0a\u5b9e\u73b0\u4f4e\u529f\u8017\u90e8\u7f72\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5229\u7528SNN\u7684\u4f4e\u529f\u8017\u548c\u4f4e\u5ef6\u8fdf\u7279\u6027\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528Q-learning\u7b97\u6cd5\u8bad\u7ec3SNN\uff0c\u7136\u540e\u5bf9\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u548c8\u4f4d\u91cf\u5316\uff0c\u6700\u7ec8\u5728SpiNNaker2\u82af\u7247\u4e0a\u90e8\u7f72\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cSpiNNaker2\u5728\u80fd\u8017\u4e0a\u6bd4\u4f20\u7edfGPU\uff08GTX 1650\uff09\u964d\u4f4e\u4e8632\u500d\uff0c\u540c\u65f6\u63a8\u7406\u5ef6\u8fdf\u4e0eGPU\u76f8\u5f53\uff0c\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660eSpiNNaker2\u5728\u4f4e\u80fd\u8017\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u9002\u5408\u5b9e\u65f6\u63a7\u5236\u4efb\u52a1\uff0c\u4e3a\u9ad8\u6548\u6df1\u5ea6Q\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.23581", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23581", "abs": "https://arxiv.org/abs/2507.23581", "authors": ["Chuanyue Yu", "Kuo Zhao", "Yuhan Li", "Heng Chang", "Mingjian Feng", "Xiangzhe Jiang", "Yufei Sun", "Jia Li", "Yuzhi Zhang", "Jianxin Li", "Ziwei Zhang"], "title": "GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning", "comment": null, "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness\nin enhancing the reasoning abilities of LLMs by leveraging graph structures for\nknowledge representation and modeling complex real-world relationships.\nHowever, existing GraphRAG methods still face significant bottlenecks when\nhandling complex problems that require multi-hop reasoning, as their query and\nretrieval phases are largely based on pre-defined heuristics and do not fully\nutilize the reasoning potentials of LLMs. To address this problem, we propose\nGraphRAG-R1, an adaptive GraphRAG framework by training LLMs with\nprocess-constrained outcome-based reinforcement learning (RL) to enhance the\nmulti-hop reasoning ability. Our method can decompose complex problems,\nautonomously invoke retrieval tools to acquire necessary information, and\nperform effective reasoning. Specifically, we utilize a modified version of\nGroup Relative Policy Optimization (GRPO) that supports rollout-with-thinking\ncapability. Next, we design two process-constrained reward functions. To handle\nthe shallow retrieval problem, we design a Progressive Retrieval Attenuation\n(PRA) reward to encourage essential retrievals. Then, to handle the\nover-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the\nmodel performance with computational costs. We further design a phase-dependent\ntraining strategy, containing three training stages corresponding to cold start\nand these two rewards. Lastly, our method adopts a hybrid graph-textual\nretrieval to improve the reasoning capacity. Extensive experimental results\ndemonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex\nreasoning problems compared to state-of-the-art GraphRAG methods on both\nin-domain and out-of-domain datasets. Furthermore, our framework can be\nflexibly integrated with various existing retrieval methods, consistently\ndelivering performance improvements.", "AI": {"tldr": "GraphRAG-R1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3aLLM\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GraphRAG\u65b9\u6cd5\u5728\u590d\u6742\u95ee\u9898\u4e2d\u7684\u74f6\u9888\u3002", "motivation": "\u73b0\u6709GraphRAG\u65b9\u6cd5\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u56e0\u5176\u68c0\u7d22\u548c\u67e5\u8be2\u9636\u6bb5\u4f9d\u8d56\u9884\u5b9a\u4e49\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u63a8\u7406\u6f5c\u529b\u3002", "method": "\u63d0\u51faGraphRAG-R1\u6846\u67b6\uff0c\u91c7\u7528\u8fc7\u7a0b\u7ea6\u675f\u7684\u5f3a\u5316\u5b66\u4e60\uff08GRPO\u6539\u8fdb\u7248\uff09\u8bad\u7ec3LLM\uff0c\u8bbe\u8ba1\u6e10\u8fdb\u68c0\u7d22\u8870\u51cf\uff08PRA\uff09\u548c\u6210\u672c\u611f\u77e5F1\uff08CAF\uff09\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u5206\u9636\u6bb5\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGraphRAG-R1\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709GraphRAG\u65b9\u6cd5\uff0c\u4e14\u80fd\u7075\u6d3b\u96c6\u6210\u591a\u79cd\u68c0\u7d22\u65b9\u6cd5\u3002", "conclusion": "GraphRAG-R1\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u6027\u80fd\u63d0\u5347\u6f5c\u529b\u3002"}}
{"id": "2507.23600", "categories": ["cs.LG", "cs.CE", "G.1.6; G.3; G.4; I.6.5"], "pdf": "https://arxiv.org/pdf/2507.23600", "abs": "https://arxiv.org/abs/2507.23600", "authors": ["Yu-Tang Chang", "Shih-Fang Chen"], "title": "EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution", "comment": null, "summary": "Signal unmixing analysis decomposes data into basic patterns and is widely\napplied in chemical and biological research. Multivariate curve resolution\n(MCR), a branch of signal unmixing, separates mixed chemical signals into base\npatterns (components) and their concentrations, playing a key role in\nunderstanding composition. Classical MCR is typically framed as matrix\nfactorization (MF) and requires a user-specified component count, usually\nunknown in real data. As dataset size or component count increases, the\nscalability and reliability of MF-based MCR face significant challenges. This\nstudy reformulates MCR as a generative process (gMCR), and introduces an\nenergy-based deep learning solver, EB-gMCR, that automatically discovers the\nsmallest component set able to reconstruct the data faithfully. EB-gMCR starts\nfrom a large candidate pool (e.g., 1024 spectra) and employs a differentiable\ngating network to retain only active components while estimating their\nconcentrations. On noisy synthetic datasets containing up to 256 latent\nsources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count\nwithin 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near\nexact component estimation. Additional chemical priors, such as non-negativity\nor nonlinear mixing, enter as simple plug-in functions, enabling adaptation to\nother instruments or domains without altering the core learning process. By\nuniting high-capacity generative modeling and hard component selection, EB-gMCR\noffers a practical route to large-scale signal unmixing analysis, including\nchemical library-driven scenarios. The source code is available at\nhttps://github.com/b05611038/ebgmcr_solver.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u8fc7\u7a0b\u7684\u4fe1\u53f7\u89e3\u6df7\u65b9\u6cd5\uff08gMCR\uff09\uff0c\u5e76\u5f15\u5165\u80fd\u91cf\u57fa\u6df1\u5ea6\u5b66\u4e60\u6c42\u89e3\u5668EB-gMCR\uff0c\u81ea\u52a8\u53d1\u73b0\u6700\u5c0f\u7ec4\u4ef6\u96c6\u4ee5\u91cd\u5efa\u6570\u636e\u3002", "motivation": "\u4f20\u7edfMCR\u65b9\u6cd5\u4f9d\u8d56\u7528\u6237\u6307\u5b9a\u7ec4\u4ef6\u6570\u91cf\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u6570\u636e\u6216\u7ec4\u4ef6\u6570\u91cf\u589e\u52a0\u65f6\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u5c06MCR\u91cd\u65b0\u5b9a\u4e49\u4e3a\u751f\u6210\u8fc7\u7a0b\uff0c\u4f7f\u7528\u80fd\u91cf\u57fa\u6df1\u5ea6\u5b66\u4e60\u6c42\u89e3\u5668EB-gMCR\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u95e8\u63a7\u7f51\u7edc\u4ece\u5019\u9009\u6c60\u4e2d\u4fdd\u7559\u6d3b\u8dc3\u7ec4\u4ef6\u5e76\u4f30\u8ba1\u5176\u6d53\u5ea6\u3002", "result": "\u5728\u566a\u58f0\u5408\u6210\u6570\u636e\u4e2d\uff0cEB-gMCR\u4fdd\u6301R^2 >= 0.98\uff0c\u7ec4\u4ef6\u6570\u91cf\u6062\u590d\u8bef\u5dee\u57285%\u4ee5\u5185\uff1b\u4f4e\u566a\u58f0\u65f6R^2 >= 0.99\uff0c\u7ec4\u4ef6\u4f30\u8ba1\u63a5\u8fd1\u7cbe\u786e\u3002", "conclusion": "EB-gMCR\u7ed3\u5408\u9ad8\u5bb9\u91cf\u751f\u6210\u5efa\u6a21\u548c\u786c\u7ec4\u4ef6\u9009\u62e9\uff0c\u4e3a\u5927\u89c4\u6a21\u4fe1\u53f7\u89e3\u6df7\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23604", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23604", "abs": "https://arxiv.org/abs/2507.23604", "authors": ["Tommaso Marzi", "Cesare Alippi", "Andrea Cini"], "title": "Hierarchical Message-Passing Policies for Multi-Agent Reinforcement Learning", "comment": null, "summary": "Decentralized Multi-Agent Reinforcement Learning (MARL) methods allow for\nlearning scalable multi-agent policies, but suffer from partial observability\nand induced non-stationarity. These challenges can be addressed by introducing\nmechanisms that facilitate coordination and high-level planning. Specifically,\ncoordination and temporal abstraction can be achieved through communication\n(e.g., message passing) and Hierarchical Reinforcement Learning (HRL)\napproaches to decision-making. However, optimization issues limit the\napplicability of hierarchical policies to multi-agent systems. As such, the\ncombination of these approaches has not been fully explored. To fill this void,\nwe propose a novel and effective methodology for learning multi-agent\nhierarchies of message-passing policies. We adopt the feudal HRL framework and\nrely on a hierarchical graph structure for planning and coordination among\nagents. Agents at lower levels in the hierarchy receive goals from the upper\nlevels and exchange messages with neighboring agents at the same level. To\nlearn hierarchical multi-agent policies, we design a novel reward-assignment\nmethod based on training the lower-level policies to maximize the advantage\nfunction associated with the upper levels. Results on relevant benchmarks show\nthat our method performs favorably compared to the state of the art.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u548c\u6d88\u606f\u4f20\u9012\u7684\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5206\u6563\u5f0fMARL\u4e2d\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u975e\u5e73\u7a33\u6027\u95ee\u9898\u3002", "motivation": "\u5206\u6563\u5f0fMARL\u65b9\u6cd5\u5b58\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u9700\u8981\u5f15\u5165\u534f\u8c03\u548c\u9ad8\u7ea7\u89c4\u5212\u673a\u5236\u3002", "method": "\u91c7\u7528\u5c01\u5efaHRL\u6846\u67b6\u548c\u5206\u5c42\u56fe\u7ed3\u6784\uff0c\u4f4e\u5c42\u667a\u80fd\u4f53\u63a5\u6536\u9ad8\u5c42\u76ee\u6807\u5e76\u4e0e\u540c\u5c42\u90bb\u5c45\u4ea4\u6362\u6d88\u606f\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u4f18\u52bf\u51fd\u6570\u7684\u65b0\u5956\u52b1\u5206\u914d\u65b9\u6cd5\u3002", "result": "\u5728\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u5206\u5c42\u7b56\u7565\u548c\u6d88\u606f\u4f20\u9012\uff0c\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u548c\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2507.23607", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23607", "abs": "https://arxiv.org/abs/2507.23607", "authors": ["Tien Huu Do", "Antoine Masquelier", "Nae Eoun Lee", "Jonathan Crowther"], "title": "Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates", "comment": null, "summary": "Clinical trials are a systematic endeavor to assess the safety and efficacy\nof new drugs or treatments. Conducting such trials typically demands\nsignificant financial investment and meticulous planning, highlighting the need\nfor accurate predictions of trial outcomes. Accurately predicting patient\nenrollment, a key factor in trial success, is one of the primary challenges\nduring the planning phase. In this work, we propose a novel deep learning-based\nmethod to address this critical challenge. Our method, implemented as a neural\nnetwork model, leverages pre-trained language models (PLMs) to capture the\ncomplexities and nuances of clinical documents, transforming them into\nexpressive representations. These representations are then combined with\nencoded tabular features via an attention mechanism. To account for\nuncertainties in enrollment prediction, we enhance the model with a\nprobabilistic layer based on the Gamma distribution, which enables range\nestimation. We apply the proposed model to predict clinical trial duration,\nassuming site-level enrollment follows a Poisson-Gamma process. We carry out\nextensive experiments on real-world clinical trial data, and show that the\nproposed method can effectively predict the number of patients enrolled at a\nnumber of sites for a given clinical trial, outperforming established baseline\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\u9884\u6d4b\u4e34\u5e8a\u8bd5\u9a8c\u60a3\u8005\u62db\u52df\uff0c\u5e76\u901a\u8fc7Gamma\u5206\u5e03\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4e34\u5e8a\u8bd5\u9a8c\u9700\u8981\u5927\u91cf\u8d44\u6e90\u548c\u7cbe\u786e\u9884\u6d4b\uff0c\u60a3\u8005\u62db\u52df\u662f\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u4e34\u5e8a\u6587\u6863\u4e0e\u8868\u683c\u7279\u5f81\uff0c\u5f15\u5165Gamma\u5206\u5e03\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u60a3\u8005\u62db\u52df\u6570\u91cf\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8a\u8bd5\u9a8c\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u5de5\u5177\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.23615", "categories": ["cs.LG", "cs.AI", "68T01", "I.5.1; G.3; H.2.8; I.2.1"], "pdf": "https://arxiv.org/pdf/2507.23615", "abs": "https://arxiv.org/abs/2507.23615", "authors": ["Luis Roque", "Carlos Soares", "Vitor Cerqueira", "Luis Torgo"], "title": "L-GTA: Latent Generative Modeling for Time Series Augmentation", "comment": null, "summary": "Data augmentation is gaining importance across various aspects of time series\nanalysis, from forecasting to classification and anomaly detection tasks. We\nintroduce the Latent Generative Transformer Augmentation (L-GTA) model, a\ngenerative approach using a transformer-based variational recurrent\nautoencoder. This model uses controlled transformations within the latent space\nof the model to generate new time series that preserve the intrinsic properties\nof the original dataset. L-GTA enables the application of diverse\ntransformations, ranging from simple jittering to magnitude warping, and\ncombining these basic transformations to generate more complex synthetic time\nseries datasets. Our evaluation of several real-world datasets demonstrates the\nability of L-GTA to produce more reliable, consistent, and controllable\naugmented data. This translates into significant improvements in predictive\naccuracy and similarity measures compared to direct transformation methods.", "AI": {"tldr": "L-GTA\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u53d8\u5206\u5faa\u73af\u81ea\u7f16\u7801\u5668\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u53d7\u63a7\u53d8\u6362\u751f\u6210\u65b0\u65f6\u95f4\u5e8f\u5217\uff0c\u63d0\u5347\u6570\u636e\u589e\u5f3a\u7684\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u6570\u636e\u589e\u5f3a\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u53ef\u9760\u548c\u53ef\u63a7\u7684\u589e\u5f3a\u6570\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528Transformer-based\u53d8\u5206\u5faa\u73af\u81ea\u7f16\u7801\u5668\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u53d7\u63a7\u53d8\u6362\uff0c\u751f\u6210\u4fdd\u7559\u539f\u59cb\u6570\u636e\u96c6\u7279\u6027\u7684\u65b0\u65f6\u95f4\u5e8f\u5217\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cL-GTA\u751f\u6210\u7684\u589e\u5f3a\u6570\u636e\u66f4\u53ef\u9760\u3001\u4e00\u81f4\u4e14\u53ef\u63a7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u76f8\u4f3c\u6027\u5ea6\u91cf\u3002", "conclusion": "L-GTA\u4e3a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u65b9\u6cd5\uff0c\u4f18\u4e8e\u76f4\u63a5\u53d8\u6362\u65b9\u6cd5\u3002"}}
{"id": "2507.23632", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23632", "abs": "https://arxiv.org/abs/2507.23632", "authors": ["Gabriel Mongaras", "Eric C. Larson"], "title": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective", "comment": null, "summary": "Since its introduction, softmax attention has become the backbone of modern\ntransformer architectures due to its expressiveness and scalability across a\nwide range of tasks. However, the main drawback of softmax attention is the\nquadratic memory requirement and computational complexity with respect to the\nsequence length. By replacing the softmax nonlinearity, linear attention and\nsimilar methods have been introduced to avoid the quadratic bottleneck of\nsoftmax attention. Despite these linear forms of attention being derived from\nthe original softmax formulation, they typically lag in terms of downstream\naccuracy. While strong intuition of the softmax nonlinearity on the query and\nkey inner product suggests that it has desirable properties compared to other\nnonlinearities, the question of why this discrepancy exists still remains\nunanswered. This work demonstrates that linear attention is an approximation of\nsoftmax attention by deriving the recurrent form of softmax attention. Using\nthis form, each part of softmax attention can be described in the language of\nrecurrent neural networks (RNNs). Describing softmax attention as an RNN allows\nfor the ablation of the components of softmax attention to understand the\nimportance of each part and how they interact. In this way, our work helps\nexplain why softmax attention is more expressive than its counterparts.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86softmax\u6ce8\u610f\u529b\u4e0e\u7ebf\u6027\u6ce8\u610f\u529b\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u7684\u5dee\u5f02\uff0c\u901a\u8fc7\u5c06softmax\u6ce8\u610f\u529b\u8f6c\u5316\u4e3aRNN\u5f62\u5f0f\uff0c\u63ed\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u7684\u539f\u56e0\u3002", "motivation": "\u7814\u7a76softmax\u6ce8\u610f\u529b\u4e0e\u7ebf\u6027\u6ce8\u610f\u529b\u5728\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u89e3\u91ca\u4e3a\u4f55softmax\u6ce8\u610f\u529b\u66f4\u5177\u8868\u8fbe\u529b\u3002", "method": "\u5c06softmax\u6ce8\u610f\u529b\u8f6c\u5316\u4e3aRNN\u5f62\u5f0f\uff0c\u5206\u6790\u5176\u5404\u90e8\u5206\u7684\u4f5c\u7528\u53ca\u4ea4\u4e92\u65b9\u5f0f\u3002", "result": "\u63ed\u793a\u4e86softmax\u6ce8\u610f\u529b\u7684\u4f18\u8d8a\u6027\u6e90\u4e8e\u5176RNN\u5f62\u5f0f\u7684\u7279\u5b9a\u7ed3\u6784\u548c\u4ea4\u4e92\u673a\u5236\u3002", "conclusion": "\u901a\u8fc7RNN\u5f62\u5f0f\u7684\u5206\u6790\uff0c\u89e3\u91ca\u4e86softmax\u6ce8\u610f\u529b\u6bd4\u7ebf\u6027\u6ce8\u610f\u529b\u66f4\u5177\u8868\u8fbe\u529b\u7684\u539f\u56e0\u3002"}}
{"id": "2507.23638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23638", "abs": "https://arxiv.org/abs/2507.23638", "authors": ["Mohammad Karami", "Fatemeh Ghassemi", "Hamed Kebriaei", "Hamid Azadegan"], "title": "OptiGradTrust: Byzantine-Robust Federated Learning with Multi-Feature Gradient Analysis and Reinforcement Learning-Based Trust Weighting", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed medical institutions while preserving patient privacy, but remains\nvulnerable to Byzantine attacks and statistical heterogeneity. We present\nOptiGradTrust, a comprehensive defense framework that evaluates gradient\nupdates through a novel six-dimensional fingerprint including VAE\nreconstruction error, cosine similarity metrics, $L_2$ norm, sign-consistency\nratio, and Monte Carlo Shapley value, which drive a hybrid RL-attention module\nfor adaptive trust scoring. To address convergence challenges under data\nheterogeneity, we develop FedBN-Prox (FedBN-P), combining Federated Batch\nNormalization with proximal regularization for optimal accuracy-convergence\ntrade-offs. Extensive evaluation across MNIST, CIFAR-10, and Alzheimer's MRI\ndatasets under various Byzantine attack scenarios demonstrates significant\nimprovements over state-of-the-art defenses, achieving up to +1.6 percentage\npoints over FLGuard under non-IID conditions while maintaining robust\nperformance against diverse attack patterns through our adaptive learning\napproach.", "AI": {"tldr": "OptiGradTrust\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u516d\u7ef4\u6307\u7eb9\u8bc4\u4f30\u68af\u5ea6\u66f4\u65b0\uff0c\u7ed3\u5408FedBN-Prox\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u9632\u5fa1\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\u9762\u4e34\u62dc\u5360\u5ead\u653b\u51fb\u548c\u6570\u636e\u5f02\u6784\u6027\u6311\u6218\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u6846\u67b6\u3002", "method": "\u63d0\u51faOptiGradTrust\u6846\u67b6\uff0c\u5229\u7528\u516d\u7ef4\u6307\u7eb9\u548c\u6df7\u5408RL-\u6ce8\u610f\u529b\u6a21\u5757\u8fdb\u884c\u4fe1\u4efb\u8bc4\u5206\uff1b\u5f00\u53d1FedBN-Prox\u4f18\u5316\u6536\u655b\u3002", "result": "\u5728MNIST\u3001CIFAR-10\u548c\u963f\u5c14\u8328\u6d77\u9ed8MRI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4FLGuard\u63d0\u53471.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "OptiGradTrust\u5728\u9632\u5fa1\u653b\u51fb\u548c\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.23665", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23665", "abs": "https://arxiv.org/abs/2507.23665", "authors": ["Amal Saadallah"], "title": "SHAP-Guided Regularization in Machine Learning Models", "comment": null, "summary": "Feature attribution methods such as SHapley Additive exPlanations (SHAP) have\nbecome instrumental in understanding machine learning models, but their role in\nguiding model optimization remains underexplored. In this paper, we propose a\nSHAP-guided regularization framework that incorporates feature importance\nconstraints into model training to enhance both predictive performance and\ninterpretability. Our approach applies entropy-based penalties to encourage\nsparse, concentrated feature attributions while promoting stability across\nsamples. The framework is applicable to both regression and classification\ntasks. Our first exploration started with investigating a tree-based model\nregularization using TreeSHAP. Through extensive experiments on benchmark\nregression and classification datasets, we demonstrate that our method improves\ngeneralization performance while ensuring robust and interpretable feature\nattributions. The proposed technique offers a novel, explainability-driven\nregularization approach, making machine learning models both more accurate and\nmore reliable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSHAP\u7684\u5f15\u5bfc\u6b63\u5219\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u91cd\u8981\u6027\u7ea6\u675f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63a2\u7d22SHAP\u5728\u6a21\u578b\u4f18\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u71b5\u7684\u60e9\u7f5a\u673a\u5236\uff0c\u9f13\u52b1\u7a00\u758f\u4e14\u7a33\u5b9a\u7684\u7279\u5f81\u5206\u5e03\uff0c\u9002\u7528\u4e8e\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u7279\u5f81\u89e3\u91ca\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u89e3\u91ca\u6027\u9a71\u52a8\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.23674", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23674", "abs": "https://arxiv.org/abs/2507.23674", "authors": ["Muhammad Taha Cheema", "Abeer Aamir", "Khawaja Gul Muhammad", "Naveed Anwar Bhatti", "Ihsan Ayyub Qazi", "Zafar Ayyub Qazi"], "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses", "comment": "13 pages, 9 figures", "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.", "AI": {"tldr": "TweakLLM\u662f\u4e00\u79cd\u65b0\u578b\u8def\u7531\u67b6\u6784\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7LLM\u52a8\u6001\u8c03\u6574\u7f13\u5b58\u54cd\u5e94\uff0c\u663e\u8457\u63d0\u9ad8\u7f13\u5b58\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3LLM\u54cd\u5e94\u7f13\u5b58\u4e2d\u56e0\u4e2a\u6027\u5316\u4ea4\u4e92\u548c\u8bed\u4e49\u641c\u7d22\u51c6\u786e\u6027\u4e0d\u8db3\u5bfc\u81f4\u7684\u96be\u4ee5\u4fdd\u6301\u76f8\u5173\u6027\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7LLM\u52a8\u6001\u8c03\u6574\u7f13\u5b58\u54cd\u5e94\uff0c\u7ed3\u5408\u7528\u6237\u7814\u7a76\u548c\u591a\u4ee3\u7406LLM\u8fa9\u8bba\u8bc4\u4f30\u3002", "result": "\u5728\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u7f13\u5b58\u6548\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u6d41\u91cfLLM\u90e8\u7f72\u3002", "conclusion": "TweakLLM\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u7f13\u5b58\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2507.23675", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23675", "abs": "https://arxiv.org/abs/2507.23675", "authors": ["Tianyi Chen", "Haitong Ma", "Na Li", "Kai Wang", "Bo Dai"], "title": "One-Step Flow Policy Mirror Descent", "comment": null, "summary": "Diffusion policies have achieved great success in online reinforcement\nlearning (RL) due to their strong expressive capacity. However, the inference\nof diffusion policy models relies on a slow iterative sampling process, which\nlimits their responsiveness. To overcome this limitation, we propose Flow\nPolicy Mirror Descent (FPMD), an online RL algorithm that enables 1-step\nsampling during policy inference. Our approach exploits a theoretical\nconnection between the distribution variance and the discretization error of\nsingle-step sampling in straight interpolation flow matching models, and\nrequires no extra distillation or consistency training. We present two\nalgorithm variants based on flow policy and MeanFlow policy parametrizations,\nrespectively. Extensive empirical evaluations on MuJoCo benchmarks demonstrate\nthat our algorithms show strong performance comparable to diffusion policy\nbaselines while requiring hundreds of times fewer function evaluations during\ninference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFPMD\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5355\u6b65\u91c7\u6837\u63d0\u5347\u6269\u6563\u7b56\u7565\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u4f9d\u8d56\u7f13\u6162\u7684\u8fed\u4ee3\u91c7\u6837\uff0c\u9650\u5236\u4e86\u54cd\u5e94\u901f\u5ea6\u3002", "method": "\u5229\u7528\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u7684\u5206\u5e03\u65b9\u5dee\u4e0e\u5355\u6b65\u91c7\u6837\u79bb\u6563\u5316\u8bef\u5dee\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6d41\u7b56\u7565\u548cMeanFlow\u7b56\u7565\u7684\u4e24\u79cd\u7b97\u6cd5\u53d8\u4f53\u3002", "result": "\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4e0e\u6269\u6563\u7b56\u7565\u57fa\u7ebf\u76f8\u5f53\uff0c\u4f46\u63a8\u7406\u65f6\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u51cf\u5c11\u6570\u767e\u500d\u3002", "conclusion": "FPMD\u7b97\u6cd5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2507.23676", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23676", "abs": "https://arxiv.org/abs/2507.23676", "authors": ["Rabeya Tus Sadia", "Qiang Cheng"], "title": "DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data", "comment": null, "summary": "Microbiome data analysis is essential for understanding host health and\ndisease, yet its inherent sparsity and noise pose major challenges for accurate\nimputation, hindering downstream tasks such as biomarker discovery. Existing\nimputation methods, including recent diffusion-based models, often fail to\ncapture the complex interdependencies between microbial taxa and overlook\ncontextual metadata that can inform imputation. We introduce DepMicroDiff, a\nnovel framework that combines diffusion-based generative modeling with a\nDependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise\ndependencies and autoregressive relationships. DepMicroDiff is further enhanced\nby VAE-based pretraining across diverse cancer datasets and conditioning on\npatient metadata encoded via a large language model (LLM). Experiments on TCGA\nmicrobiome datasets show that DepMicroDiff substantially outperforms\nstate-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),\ncosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer\ntypes, demonstrating its robustness and generalizability for microbiome\nimputation.", "AI": {"tldr": "DepMicroDiff\u662f\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u4f9d\u8d56\u611f\u77e5Transformer\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5fae\u751f\u7269\u7ec4\u6570\u636e\u63d2\u8865\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5fae\u751f\u7269\u7ec4\u6570\u636e\u7684\u7a00\u758f\u6027\u548c\u566a\u58f0\u9650\u5236\u4e86\u63d2\u8865\u7684\u51c6\u786e\u6027\uff0c\u5f71\u54cd\u4e86\u4e0b\u6e38\u4efb\u52a1\u5982\u751f\u7269\u6807\u5fd7\u7269\u53d1\u73b0\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u5fae\u751f\u7269\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u548c\u4e0a\u4e0b\u6587\u5143\u6570\u636e\u3002", "method": "DepMicroDiff\u7ed3\u5408\u6269\u6563\u751f\u6210\u6a21\u578b\u548c\u4f9d\u8d56\u611f\u77e5Transformer\uff0c\u5229\u7528VAE\u9884\u8bad\u7ec3\u548c\u57fa\u4e8eLLM\u7684\u5143\u6570\u636e\u7f16\u7801\u3002", "result": "\u5728TCGA\u6570\u636e\u96c6\u4e0a\uff0cDepMicroDiff\u7684Pearson\u76f8\u5173\u7cfb\u6570\u8fbe0.712\uff0c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8fbe0.812\uff0cRMSE\u548cMAE\u66f4\u4f4e\u3002", "conclusion": "DepMicroDiff\u5728\u5fae\u751f\u7269\u7ec4\u6570\u636e\u63d2\u8865\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.23712", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23712", "abs": "https://arxiv.org/abs/2507.23712", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "title": "Anomalous Samples for Few-Shot Anomaly Detection", "comment": null, "summary": "Several anomaly detection and classification methods rely on large amounts of\nnon-anomalous or \"normal\" samples under the assump- tion that anomalous data is\ntypically harder to acquire. This hypothesis becomes questionable in Few-Shot\nsettings, where as little as one anno- tated sample can make a significant\ndifference. In this paper, we tackle the question of utilizing anomalous\nsamples in training a model for bi- nary anomaly classification. We propose a\nmethodology that incorporates anomalous samples in a multi-score anomaly\ndetection score leveraging recent Zero-Shot and memory-based techniques. We\ncompare the utility of anomalous samples to that of regular samples and study\nthe benefits and limitations of each. In addition, we propose an\naugmentation-based validation technique to optimize the aggregation of the\ndifferent anomaly scores and demonstrate its effectiveness on popular\nindustrial anomaly detection datasets.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u5229\u7528\u5f02\u5e38\u6837\u672c\u8fdb\u884c\u4e8c\u5143\u5f02\u5e38\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u96f6\u6837\u672c\u548c\u57fa\u4e8e\u8bb0\u5fc6\u6280\u672f\u7684\u591a\u5206\u6570\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u9a8c\u8bc1\u6280\u672f\u4f18\u5316\u5206\u6570\u805a\u5408\u3002", "motivation": "\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6b63\u5e38\u6837\u672c\uff0c\u4f46\u5f02\u5e38\u6837\u672c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u53ef\u80fd\u66f4\u6709\u4ef7\u503c\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5982\u4f55\u5229\u7528\u5f02\u5e38\u6837\u672c\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u96f6\u6837\u672c\u548c\u57fa\u4e8e\u8bb0\u5fc6\u6280\u672f\u7684\u591a\u5206\u6570\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u9a8c\u8bc1\u6280\u672f\u4f18\u5316\u5206\u6570\u805a\u5408\u3002", "result": "\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5f02\u5e38\u6837\u672c\u7684\u5229\u7528\u4ef7\u503c\u3002", "conclusion": "\u5f02\u5e38\u6837\u672c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u5177\u6709\u663e\u8457\u4ef7\u503c\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2507.23756", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.23756", "abs": "https://arxiv.org/abs/2507.23756", "authors": ["Diana Mortagua"], "title": "Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System", "comment": null, "summary": "This study centers on overcoming the challenge of selecting the best\nannotators for each query in Active Learning (AL), with the objective of\nminimizing misclassifications. AL recognizes the challenges related to cost and\ntime when acquiring labeled data, and decreases the number of labeled data\nneeded. Nevertheless, there is still the necessity to reduce annotation errors,\naiming to be as efficient as possible, to achieve the expected accuracy faster.\nMost strategies for query-annotator pairs do not consider internal factors that\naffect productivity, such as mood, attention, motivation, and fatigue levels.\nThis work addresses this gap in the existing literature, by not only\nconsidering how the internal factors influence annotators (mood and fatigue\nlevels) but also presenting a new query-annotator pair strategy, using a\nKnowledge-Based Recommendation System (RS). The RS ranks the available\nannotators, allowing to choose one or more to label the queried instance using\ntheir past accuracy values, and their mood and fatigue levels, as well as\ninformation about the instance queried. This work bases itself on existing\nliterature on mood and fatigue influence on human performance, simulating\nannotators in a realistic manner, and predicting their performance with the RS.\nThe results show that considering past accuracy values, as well as mood and\nfatigue levels reduces the number of annotation errors made by the annotators,\nand the uncertainty of the model through its training, when compared to not\nusing internal factors. Accuracy and F1-score values were also better in the\nproposed approach, despite not being as substantial as the aforementioned. The\nmethodologies and findings presented in this study begin to explore the open\nchallenge of human cognitive factors affecting AL.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u63a8\u8350\u7cfb\u7edf\u7684\u67e5\u8be2-\u6807\u6ce8\u8005\u914d\u5bf9\u7b56\u7565\uff0c\u901a\u8fc7\u8003\u8651\u6807\u6ce8\u8005\u7684\u60c5\u7eea\u548c\u75b2\u52b3\u6c34\u5e73\u7b49\u5185\u90e8\u56e0\u7d20\uff0c\u51cf\u5c11\u6807\u6ce8\u9519\u8bef\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u4e2d\u6807\u6ce8\u6570\u636e\u7684\u6210\u672c\u548c\u65f6\u95f4\u95ee\u9898\u5df2\u5f97\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u6807\u6ce8\u8005\u7684\u5185\u90e8\u56e0\u7d20\uff08\u5982\u60c5\u7eea\u3001\u75b2\u52b3\uff09\u5bf9\u6807\u6ce8\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u77e5\u8bc6\u63a8\u8350\u7cfb\u7edf\uff08RS\uff09\u5bf9\u6807\u6ce8\u8005\u8fdb\u884c\u6392\u540d\uff0c\u7ed3\u5408\u5176\u5386\u53f2\u51c6\u786e\u6027\u3001\u60c5\u7eea\u548c\u75b2\u52b3\u6c34\u5e73\u4ee5\u53ca\u67e5\u8be2\u5b9e\u4f8b\u4fe1\u606f\uff0c\u9009\u62e9\u6700\u4f73\u6807\u6ce8\u8005\u3002", "result": "\u8003\u8651\u5185\u90e8\u56e0\u7d20\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u9519\u8bef\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63a2\u7d22\u4eba\u7c7b\u8ba4\u77e5\u56e0\u7d20\u5bf9\u4e3b\u52a8\u5b66\u4e60\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u521d\u6b65\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5185\u90e8\u56e0\u7d20\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.23771", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23771", "abs": "https://arxiv.org/abs/2507.23771", "authors": ["Justin Kay", "Grant Van Horn", "Subhransu Maji", "Daniel Sheldon", "Sara Beery"], "title": "Consensus-Driven Active Model Selection", "comment": "ICCV 2025 Highlight. 16 pages, 8 figures", "summary": "The widespread availability of off-the-shelf machine learning models poses a\nchallenge: which model, of the many available candidates, should be chosen for\na given data analysis task? This question of model selection is traditionally\nanswered by collecting and annotating a validation dataset -- a costly and\ntime-intensive process. We propose a method for active model selection, using\npredictions from candidate models to prioritize the labeling of test data\npoints that efficiently differentiate the best candidate. Our method, CODA,\nperforms consensus-driven active model selection by modeling relationships\nbetween classifiers, categories, and data points within a probabilistic\nframework. The framework uses the consensus and disagreement between models in\nthe candidate pool to guide the label acquisition process, and Bayesian\ninference to update beliefs about which model is best as more information is\ncollected. We validate our approach by curating a collection of 26 benchmark\ntasks capturing a range of model selection scenarios. CODA outperforms existing\nmethods for active model selection significantly, reducing the annotation\neffort required to discover the best model by upwards of 70% compared to the\nprevious state-of-the-art. Code and data are available at\nhttps://github.com/justinkay/coda.", "AI": {"tldr": "CODA\u662f\u4e00\u79cd\u4e3b\u52a8\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5019\u9009\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u4f18\u5148\u6807\u6ce8\u80fd\u9ad8\u6548\u533a\u5206\u6700\u4f73\u6a21\u578b\u7684\u6570\u636e\u70b9\uff0c\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u9009\u62e9\u9700\u8981\u5927\u91cf\u6807\u6ce8\u9a8c\u8bc1\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "CODA\u901a\u8fc7\u6982\u7387\u6846\u67b6\u5efa\u6a21\u5206\u7c7b\u5668\u3001\u7c7b\u522b\u548c\u6570\u636e\u70b9\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5229\u7528\u6a21\u578b\u95f4\u7684\u5171\u8bc6\u548c\u5206\u6b67\u6307\u5bfc\u6807\u6ce8\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u66f4\u65b0\u6700\u4f73\u6a21\u578b\u3002", "result": "\u572826\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cCODA\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u51cf\u5c1170%\u4ee5\u4e0a\u7684\u6807\u6ce8\u6210\u672c\u3002", "conclusion": "CODA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e3b\u52a8\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u6807\u6ce8\u9700\u6c42\u3002"}}
