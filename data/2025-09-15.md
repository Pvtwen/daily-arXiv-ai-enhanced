<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [cs.LG](#cs.LG) [Total: 51]
- [stat.ML](#stat.ML) [Total: 4]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Machine-learning competition to grade EEG background patterns in newborns with hypoxic-ischaemic encephalopathy](https://arxiv.org/abs/2509.09695)
*Fabio Magarelli,Geraldine B. Boylan,Saeed Montazeri,Feargal O'Sullivan,Dominic Lightbody,Minoo Ashoori,Tamara Skoric Ceranic,John M. O'Toole*

Main category: eess.SP

TL;DR: 该研究通过机器学习竞赛开发新生儿EEG背景模式严重程度分类模型，发现深度学习模型在验证集上泛化更好，但所有模型在未见数据上性能均显著下降，强调了大规模多样化数据集和保留验证集的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决新生儿脑功能监测中高质量标注数据稀缺的问题，通过机器学习竞赛促进共享学习和多样化专业知识的众包利用，加速临床决策支持工具的开发。

Method: 收集102名新生儿353小时EEG数据，匿名化后分为训练、测试和保留验证集。创建基于网络的竞赛平台，举办机器学习竞赛开发EEG背景模式严重程度分类模型。

Result: 特征模型在测试集排名第一，但深度学习模型在验证集上泛化更好。所有方法在验证集上的性能相比测试集都有显著下降，凸显了模型泛化的挑战。

Conclusion: 研究强调了在大型多样化数据集上训练ML模型以确保稳健泛化的重要性，证明了开放访问数据和协作ML开发在促进合作研究环境和加速新生儿神经监测临床决策支持工具开发方面的潜力。

Abstract: Machine learning (ML) has the potential to support and improve expert
performance in monitoring the brain function of at-risk newborns. Developing
accurate and reliable ML models depends on access to high-quality, annotated
data, a resource in short supply. ML competitions address this need by
providing researchers access to expertly annotated datasets, fostering shared
learning through direct model comparisons, and leveraging the benefits of
crowdsourcing diverse expertise. We compiled a retrospective dataset containing
353 hours of EEG from 102 individual newborns from a multi-centre study. The
data was fully anonymised and divided into training, testing, and held-out
validation datasets. EEGs were graded for the severity of abnormal background
patterns. Next, we created a web-based competition platform and hosted a
machine learning competition to develop ML models for classifying the severity
of EEG background patterns in newborns. After the competition closed, the top 4
performing models were evaluated offline on a separate held-out validation
dataset. Although a feature-based model ranked first on the testing dataset,
deep learning models generalised better on the validation sets. All methods had
a significant decline in validation performance compared to the testing
performance. This highlights the challenges for model generalisation on unseen
data, emphasising the need for held-out validation datasets in ML studies with
neonatal EEG. The study underscores the importance of training ML models on
large and diverse datasets to ensure robust generalisation. The competition's
outcome demonstrates the potential for open-access data and collaborative ML
development to foster a collaborative research environment and expedite the
development of clinical decision-support tools for neonatal neuromonitoring.

</details>


### [2] [Locally Permuted Low Rank Column-wise Sensing](https://arxiv.org/abs/2509.09820)
*Ahmed Ali Abbasi,Namrata Vaswani*

Main category: eess.SP

TL;DR: 本文解决了带有排列/乱序数据的低秩列感知问题，提出了两种算法：PermutedAltGDmin和Permuted-AltMin，实验表明前者收敛速度更快


<details>
  <summary>Details</summary>
Motivation: 解决观测数据中存在排列/乱序/未标记情况下的低秩列感知问题，该问题处于无标记感知和低秩列感知两个研究领域的交叉点

Method: 提出了PermutedAltGDmin算法（交替梯度下降和最小化的推广）和Permuted-AltMin算法（交替最小化）

Result: 通过仿真实验证明两种算法都能收敛，但PermutedAltGDmin比Permuted-AltMin收敛速度快得多

Conclusion: 成功解决了排列低秩列感知问题，提出的PermutedAltGDmin算法在收敛速度方面表现优异

Abstract: We precisely formulate, and provide a solution for, the Low Rank Columnwise
Sensing (LRCS) problem when some of the observed data is
scrambled/permuted/unlabeled. This problem, which we refer to as permuted LRCS,
lies at the intersection of two distinct topics of recent research: unlabeled
sensing and low rank column-wise (matrix) sensing. We introduce a novel
generalization of the recently developed Alternating Gradient Descent and
Minimization (AltGDMin) algorithm to solve this problem. We also develop an
alternating minimization (AltMin) solution. We show, using simulation
experiments, that both converge but PermutedAltGDmin is much faster than
Permuted-AltMin.

</details>


### [3] [Real-Time Remote Tracking with State-Dependent Detection Probability: A POMDP Framework](https://arxiv.org/abs/2509.09837)
*Jiapei Tian,Abolfazl Zakeri,Marian Codreanu,David Gundlegård*

Main category: eess.SP

TL;DR: 提出一种针对异构传感器监控二进制马尔可夫源的实时跟踪系统，通过POMDP建模和信念空间离散化，设计最优调度策略以最小化失真和传输成本加权和。


<details>
  <summary>Details</summary>
Motivation: 解决异构传感器在状态相关检测精度和可能检测失败的情况下，如何优化调度策略来最小化端到端应用失真和传输成本的问题。

Method: 将问题建模为部分可观测马尔可夫决策过程(POMDP)，转化为信念MDP，通过信念空间离散化和相对值迭代算法(RVIA)求解有限状态MDP问题。

Result: 仿真结果表明，所提出的策略显著优于基准策略，并突显了考虑状态相关传感可靠性在传感器调度中的重要性。

Conclusion: 该方法有效处理了传感失败和数据包丢失带来的不确定性，为异构传感器系统的优化调度提供了有效解决方案。

Abstract: We consider a real-time tracking system where a binary Markov source is
monitored by two heterogeneous sensors. Upon command, sensors send their
observations to a remote sink over error-prone channels. We assume each sensor
exhibits state-dependent detection accuracy and may occasionally fail to detect
the source state. At most one sensor is scheduled for sampling at each time
slot. We assess the effectiveness of data communication using a generic
distortion function that captures the end application's objective. We derive
optimal sink-side command policies to minimize the weighted sum of distortion
and transmission costs. To model the uncertainty introduced by sensing failures
(of the sensors) and packet loss, we formulate the problem as a partially
observable Markov decision process (POMDP), which we then cast into a
belief-MDP. Since the belief evolves continuously, the belief space is
discretized into a finite grid and the belief value is quantized to the nearest
grid point after each update. This formulation leads to a finite-state MDP
problem, which is solved using the relative value iteration algorithm (RVIA).
Simulation results demonstrate that the proposed policy significantly
outperforms benchmark strategies and highlights the importance of accounting
for state-dependent sensing reliability in sensor scheduling.

</details>


### [4] [Field evaluation of a wearable instrumented headband designed for measuring head kinematics](https://arxiv.org/abs/2509.09842)
*Anu Tripathi,Yang Wan,Zhiren Zhu,Furkan Camci,Sheila Turcsanyi,Jeneel Pravin Kachhadiya,Mauricio Araiza Canizales,Alison Brooks,Haneesh Kesari,Joseph Andrews,Traci Snedden,Peter Ferrazzano,Christian Franck,Rika Wright Carlsen*

Main category: eess.SP

TL;DR: 本研究评估了用于测量足球头球运动中头部运动学的仪器头带在现场环境中的性能，通过与标准口腔传感器对比，发现头带在角速度和线加速度测量方面表现良好，但在角加速度测量上存在偏差。


<details>
  <summary>Details</summary>
Motivation: 研究足球头球与轻度创伤性脑损伤(mTBI)风险的关系，需要准确测量头部运动学参数。之前开发的仪器头带在实验室环境中表现良好，但需要在真实场景中进行现场验证。

Method: 在典型足球头球场景（掷界外球、球门球和角球）中，将仪器头带与广泛认可的口腔传感器进行对比，比较时间历程和峰值运动学参数的一致性。

Result: 头带与口腔传感器的时间历程一致性从'一般'到'优秀'不等，角速度(0.79±0.08)和线加速度(0.73±0.05)一致性最高，角加速度(0.67±0.06)最低。峰值运动学的平均偏差为：角速度40.9%、线加速度16.6%、角加速度-14.1%。

Conclusion: 仪器头带在现场评估中与口腔传感器在某些运动学测量和冲击条件下表现出合理的一致性，未来工作需要改进头带在所有运动学测量方面的性能。

Abstract: Purpose: To study the relationship between soccer heading and the risk of
mild traumatic brain injury (mTBI), we previously developed an instrumented
headband and data processing scheme to measure the angular head kinematics of
soccer headers. Laboratory evaluation of the headband on an anthropomorphic
test device showed good agreement with a reference sensor for soccer ball
impacts to the front of the head. In this study, we evaluate the headband in
measuring the full head kinematics of soccer headers in the field. Methods: The
headband was evaluated under typical soccer heading scenarios (throw-ins,
goal-kicks, and corner-kicks) on a human subject. The measured time history and
peak kinematics from the headband were compared with those from an instrumented
mouthpiece, which is a widely accepted method for measuring head kinematics in
the field. Results: The time history agreement (CORA scores) between the
headband and the mouthpiece ranged from 'fair' to 'excellent', with the highest
agreement for angular velocities (0.79 \pm 0.08) and translational
accelerations (0.73 \pm 0.05) and lowest for angular accelerations (0.67 \pm
0.06). A Bland-Altman analysis of the peak kinematics from the headband and
mouthpiece found the mean bias to be 40.9% (of the maximum mouthpiece reading)
for the angular velocity, 16.6% for the translational acceleration, and-14.1%
for the angular acceleration. Conclusion: The field evaluation of the
instrumented headband showed reasonable agreement with the mouthpiece for some
kinematic measures and impact conditions. Future work should focus on improving
the headband performance across all kinematic measures.

</details>


### [5] [A General Nonlinear Model for Arbitrary Modulation Formats in the Presence of Inter-Channel Simulated Raman Scattering](https://arxiv.org/abs/2509.10009)
*Zhiwei Liang,Bin Chen,Jiwei Xu,Yi Lei,Qingqing Hu,Fan Zhang,Gabriele Liga*

Main category: eess.SP

TL;DR: 扩展四维非线性模型以包含通道间受激拉曼散射，实现高色散条件下双偏振四维调制格式和概率整形星座的精确预测


<details>
  <summary>Details</summary>
Motivation: 现有模型在高色散条件下对双偏振四维调制格式和概率整形星座的预测精度不足，需要包含通道间受激拉曼散射效应来提高准确性

Method: 扩展四维非线性模型，加入通道间受激拉曼散射效应，并与分步傅里叶方法和增强高斯噪声模型进行比较验证

Result: 提出的模型能够准确预测高色散条件下的双偏振四维调制格式和概率整形星座性能

Conclusion: 包含通道间受激拉曼散射的四维非线性模型显著提高了在高色散条件下对先进调制格式的预测精度

Abstract: The four-dimensional nonlinear model is extended to include the inter-channel
stimulated Raman scattering, enabling accurate prediction of dual-polarization
four-dimensional modulation formats and probabilistically shaped constellations
in high-dispersion regimes. The proposed model is validated via comparisons
with the split-step Fourier method and enhanced Gaussian noise model.

</details>


### [6] [Uplink RSMA for Pinching-Antenna Systems](https://arxiv.org/abs/2509.10076)
*Apostolos A. Tegos,Yue Xiao,Sotiris A. Tegos,George K. Karagiannidis,Panagiotis D. Diamantoulakis*

Main category: eess.SP

TL;DR: 本文研究了一种采用速率分割多址(RSMA)的双用户双pinching天线系统，推导了中断概率的闭式表达式，证明RSMA在室内无线网络中优于NOMA方案。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需要适应变化条件并满足新兴应用对可靠、高容量通信的需求。传统固定天线技术存在局限性，pinching天线系统(PAS)通过激活波导上的天线来减少收发距离，是室内应用的有前景解决方案。

Method: 研究双用户双pinching天线上行链路PAS系统，采用速率分割多址(RSMA)技术构建比非正交多址(NOMA)更具弹性的框架，推导了中断概率的闭式表达式。

Result: 数值结果验证了推导的闭式表达式，证明所提出的RSMA方案在PAS系统中性能优于NOMA方案。

Conclusion: RSMA在pinching天线系统中表现出更好的性能，为室内无线通信提供了有效的多址接入解决方案。

Abstract: One of the key goals of next-generation wireless networks is to adapt to
changing conditions and meet the growing demand for reliable, high-capacity
communications from emerging applications. Overcoming the limitations of
conventional technologies, such as fixed antenna positions, is essential to
achieving this objective because it mitigates the impact of path loss on the
received signal and creates strong line-of-sight links, enhancing system
performance. With this in mind, the newly proposed pinching antenna systems
(PASs) are a promising solution for indoor applications because they can
activate antennas across a waveguide deployed in a room, thus reducing the
distance between the transmitter and receiver. In this paper, we investigate a
two-user, two-pinching-antenna uplink PAS, in which the transmitters use rate
splitting to create a more resilient framework than non-orthogonal multiple
access (NOMA). For this network, we derive novel closed-form expressions for
the outage probability. Numerical results validate these expressions, proving
that the proposed rate-splitting multiple access (RSMA) scheme outperforms NOMA
PAS.

</details>


### [7] [FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification](https://arxiv.org/abs/2509.10082)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Nhi Tran,Sharmony B. Kelly,Gari D. Clifford,Robert Galinsky,Faezeh Marzbanrad*

Main category: eess.SP

TL;DR: FetalSleepNet是首个用于胎儿羊脑电图睡眠状态分类的深度学习框架，通过迁移学习和频谱均衡策略，在胎儿EEG睡眠分期中达到86.6%的准确率


<details>
  <summary>Details</summary>
Motivation: 胎儿脑电图获取复杂且解释困难，但准确的睡眠阶段分类有助于早期检测与妊娠并发症相关的异常脑成熟

Method: 使用轻量级深度神经网络，通过从成人EEG的迁移学习和频谱均衡域适应策略来减少跨域不匹配

Result: 完全微调结合频谱均衡获得最佳性能（准确率86.6%，宏观F1分数62.5），优于基线模型

Conclusion: FetalSleepNet是首个专门为胎儿EEG自动睡眠分期开发的深度学习框架，其轻量设计适合低功耗实时可穿戴胎儿监测系统部署

Abstract: Introduction: This study presents FetalSleepNet, the first published deep
learning approach to classifying sleep states from the ovine
electroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and
laborious to interpret consistently. However, accurate sleep stage
classification may aid in the early detection of abnormal brain maturation
associated with pregnancy complications (e.g. hypoxia or intrauterine growth
restriction).
  Methods: EEG electrodes were secured onto the ovine dura over the parietal
cortices of 24 late gestation fetal sheep. A lightweight deep neural network
originally developed for adult EEG sleep staging was trained on the ovine EEG
using transfer learning from adult EEG. A spectral equalisation-based domain
adaptation strategy was used to reduce cross-domain mismatch.
  Results: We demonstrated that while direct transfer performed poorly, full
fine tuning combined with spectral equalisation achieved the best overall
performance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming
baseline models.
  Conclusions: To the best of our knowledge, FetalSleepNet is the first deep
learning framework specifically developed for automated sleep staging from the
fetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier
functions as a label engine, enabling large scale weak/semi supervised labeling
and distillation to facilitate training on less invasive signals that can be
acquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.
FetalSleepNet's lightweight design makes it well suited for deployment in low
power, real time, and wearable fetal monitoring systems.

</details>


### [8] [Resilient Vital Sign Monitoring Using RIS-Assisted Radar](https://arxiv.org/abs/2509.10088)
*Christian Eckrich,Abdelhak M. Zoubir,Vahid Jamali*

Main category: eess.SP

TL;DR: 提出了一种基于可重构智能表面(RIS)增强的非接触式雷达生命体征监测方法，通过多路径传感提高呼吸监测的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统可穿戴设备在家庭老年护理中存在不适感和用户依从性问题，单雷达视角在不利朝向时无法可靠捕捉呼吸引起的胸部运动

Method: 集成可重构智能表面(RIS)提供额外的传感路径，建立直接雷达路径和RIS反射路径的多路径生命体征传感模型

Result: 提高了呼吸监测的鲁棒性和可靠性，特别是在受试者朝向不利的情况下

Conclusion: 该方法为连续、保护隐私的生命体征监测提供了性能改进和潜在优势

Abstract: Vital sign monitoring plays a critical role in healthcare and well-being, as
parameters such as respiration and heart rate offer valuable insights into an
individual's physiological state. While wearable devices allow for continuous
measurement, their use in settings like in-home elderly care is often hindered
by discomfort or user noncompliance. As a result, contactless solutions based
on radar sensing have garnered increasing attention. This is due to their
unobtrusive design and preservation of privacy advantages compared to
camera-based systems. However, a single radar perspective can fail to capture
breathing-induced chest movements reliably, particularly when the subject's
orientation is unfavorable. To address this limitation, we integrate a
reconfigurable intelligent surface (RIS) that provides an additional sensing
path, thereby enhancing the robustness of respiratory monitoring. We present a
novel model for multi-path vital sign sensing that leverages both the direct
radar path and an RIS-reflected path. We further discuss the potential benefits
and improved performance our approach offers in continuous, privacy-preserving
vital sign monitoring.

</details>


### [9] [Real-time identification and control of influential pandemic regions using graph signal variation](https://arxiv.org/abs/2509.10281)
*Sudeepini Darapu,Subrata Ghosh,Dibakar Ghosh,Chittaranjan Hens,Santosh Nannuru*

Main category: eess.SP

TL;DR: 提出基于图信号变化的度量方法和在线算法，用于识别疫情传播中的关键影响区域，通过隔离这些区域可显著减少累计感染


<details>
  <summary>Details</summary>
Motivation: 全球疫情传播受人口流动影响，需要及时识别加速传播过程的关键区域以控制疫情

Method: 将感染建模为时间演化图信号，提出基于图信号变化的度量方法捕捉时空变化，建模图域和时间域局部性，并设计在线算法识别关键区域

Result: 模拟实验显示该方法能有效识别具有高传播能力的地理区域，隔离这些区域可显著减少累计感染，H1N1和印度COVID-19数据分析验证了方法的实用性

Conclusion: 提出的图信号变化度量和在线算法能增强对感染传播的理解和控制，为疫情防控提供有效工具

Abstract: The global spread of pandemics is facilitated by the mobility of populations,
transforming localized infections into widespread phenomena. To contain it,
timely identification of influential regions that accelerate this process is
necessary. In this work, we model infection as a temporally evolving graph
signal and propose graph signal variation-based metrics to capture
spatio-temporal changes. Both graph domain and time domain locality are
modeled. Based on this metric, we propose an online algorithm to identify
influential regions. Simulations demonstrate that the proposed method
effectively identifies geographical regions with a higher capacity to spread
the infection. Isolating these regions leads to a significant reduction in
cumulative infection. Simulations, along with analyses of hybrid H1N1 data and
real-world Indian COVID-19 data, underscore the utility of proposed metric in
enhancing our understanding and control of infection spread

</details>


### [10] [Low-Complexity Null-Space-Based Simultaneous Wireless Information and Power Transfer Scheme](https://arxiv.org/abs/2509.10296)
*Cheng Luo,Jie Hu,Luping Xiang,Kun Yang,Zhiqin Wang*

Main category: eess.SP

TL;DR: 本文提出了一种基于零空间的多用户SWIPT传输方案，在非线性能量收集模型下重新审视了专用能量波束的作用，发现高斯信号的信息波束通常足以同时支持能量和信息传输，除非使用特殊的能量中心波形。


<details>
  <summary>Details</summary>
Motivation: 同时无线信息和能量传输(SWIPT)技术持续受到关注，但传统方法中专用能量波束的必要性需要重新评估，特别是在实际非线性能量收集模型和多波形选择下。

Method: 提出基于零空间的多用户SWIPT传输方案，考虑非线性能量收集模型，制定能量波束设计优化问题，并开发低复杂度算法忽略信息波束的能量传输贡献。

Result: 数值结果表明，当接收射频功率处于能量收集高效率区域时，确定性正弦波形优于高斯信号，使专用能量波束变得有益。所提算法在M=8,K^I=K^E=2和M=16,K^I=K^E=4情况下分别实现91.43%和98.54%的计算复杂度降低，性能损失可忽略。

Conclusion: 专用能量波束通常不必要，除非使用特殊的能量中心波形。提出的低复杂度算法能显著降低计算复杂度而保持性能，验证了算法的有效性。

Abstract: Simultaneous wireless information and power transfer (SWIPT) has attracted
sustained interest. We propose a null-space-based transmission scheme for
multiuser SWIPT serving both energy users (EUs) and information users (IUs).
Under a practical nonlinear energy-harvesting (EH) model and multiple waveform
options, we revisit the role of dedicated energy beams (EBs). We show that, in
general, dedicated EBs are unnecessary because information beams (IBs) with
Gaussian signaling can simultaneously support wireless energy transfer (WET)
and wireless information transfer (WIT), unless special energy-centric
waveforms (e.g., deterministic sinusoidal waveforms) are employed and provide
sufficient gains. Guided by these insights, we formulate an optimization
problem for EB design to enable dedicated waveform transmission for WET, and we
develop a low-complexity algorithm that reduces computation by ignoring the WET
contribution of IBs during optimization. Numerical results corroborate that
deterministic sinusoidal waveforms outperform Gaussian signaling when the
received RF power lies in the EH high-efficiency region, making dedicated EBs
beneficial. The proposed scheme achieves computational complexity reductions of
91.43\% and 98.54\% for the cases $M=8,,K^I=K^E=2$ and $M=16,,K^I=K^E=4$,
respectively, with negligible performance loss, thereby validating the
efficiency of the low-complexity algorithm.

</details>


### [11] [Realistic UE Antennas for 6G in the 3GPP Channel Model](https://arxiv.org/abs/2509.10357)
*Simon Svendsen,Dimitri Gold,Christian Rom,Volker Pauli,Vuokko Nurmela*

Main category: eess.SP

TL;DR: 3GPP Rel.19对TR 38.901信道模型进行了重要更新，引入了更真实的UE天线建模和用户遮挡效应，支持6G技术评估。


<details>
  <summary>Details</summary>
Motivation: 6G发展需要对信道模型进行更新，特别是手持设备的UE天线和用户引起的遮挡效应建模，以更准确地反映实际设备行为。

Method: 基于高保真仿真和参考智能手机在多频段的测量，引入定向天线模式、实际天线布局、极化效应和单元特定遮挡等更现实的框架。

Result: 新模型使链路级和系统级仿真与实际设备行为保持一致，能够更准确地评估6G技术。

Conclusion: 更新后的3GPP信道模型为行业和研究提供了更一致的性能评估基础，支持6G技术的发展和应用。

Abstract: The transition to 6G has driven significant updates to the 3GPP channel
model, particularly in modeling UE antennas and user-induced blockage for
handheld devices. The 3GPP Rel.19 revision of TR 38.901 introduces a more
realistic framework that captures directive antenna patterns, practical antenna
placements, polarization effects, and element-specific blockage. These updates
are based on high-fidelity simulations and measurements of a reference
smartphone across multiple frequency ranges. By aligning link- and system-level
simulations with real-world device behavior, the new model enables more
accurate evaluation of 6G technologies and supports consistent performance
assessment across industry and research.

</details>


### [12] [Robust Localization in Modern Cellular Networks using Global Map Features](https://arxiv.org/abs/2509.10433)
*Junshi Chen,Xuhong Li,Russ Whiton,Erik Leitinger,Fredrik Tufvesson*

Main category: eess.SP

TL;DR: 本文提出了一种增强型多路径同时定位与建图(MP-SLAM)方法，通过引入全局地图特征(GMF)存储库来提升在复杂环境下的射频信号定位精度。


<details>
  <summary>Details</summary>
Motivation: 在现代蜂窝网络中，由于遮挡视距(OLoS)和多路径传播等挑战性环境，需要开发更鲁棒的定位解决方案。传统方法在这些恶劣信号条件下定位性能有限。

Method: 扩展MP-SLAM方法，集成全局地图特征(GMF)存储库，存储高质量的一致性地图特征。通过概率假设密度(PHD)滤波器将GMF整合到SLAM框架中，随时间传播GMF强度函数。

Result: 通过大量仿真和真实世界实验（在密集城市环境中使用LTE射频信号），证明该方法在严重多路径传播和小区间干扰条件下实现了鲁棒且精确的定位，性能优于传统方法。

Conclusion: 该方法在5G或未来6G网络等现实蜂窝网络中表现出有效性，即使在恶劣信号条件下也能实现可靠定位，为现代蜂窝网络定位提供了有前景的解决方案。

Abstract: Radio frequency (RF) signal-based localization using modern cellular networks
has emerged as a promising solution to accurately locate objects in challenging
environments. One of the most promising solutions for situations involving
obstructed-line-of-sight (OLoS) and multipath propagation is multipathbased
simultaneous localization and mapping (MP-SLAM) that employs map features
(MFs), such as virtual anchors. This paper presents an extended MP-SLAM method
that is augmented with a global map feature (GMF) repository. This repository
stores consistent MFs of high quality that are collected during prior
traversals. We integrate these GMFs back into the MP-SLAM framework via a
probability hypothesis density (PHD) filter, which propagates GMF intensity
functions over time. Extensive simulations, together with a challenging
real-world experiment using LTE RF signals in a dense urban scenario with
severe multipath propagation and inter-cell interference, demonstrate that our
framework achieves robust and accurate localization, thereby showcasing its
effectiveness in realistic modern cellular networks such as 5G or future 6G
networks. It outperforms conventional proprioceptive sensor-based localization
and conventional MP-SLAM methods, and achieves reliable localization even under
adverse signal conditions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: SAM-BG是一个两阶段的自监督学习框架，通过结构语义保护来学习脑图表示，在精神病诊断中优于现有方法，特别是在小样本数据场景下。


<details>
  <summary>Details</summary>
Motivation: 解决脑网络标记数据有限的问题，同时避免现有自监督学习方法中可能破坏脑图关键结构语义的数据增强策略。

Method: 提出两阶段框架：预训练阶段在小型标记子集上训练边缘掩码器捕获结构语义；自监督学习阶段使用提取的结构先验指导结构感知的数据增强过程。

Result: 在两个真实精神病数据集上的实验表明，SAM-BG优于最先进方法，特别是在小标记数据设置下，并发现了具有临床相关性的连接模式。

Conclusion: SAM-BG能够学习更具语义意义和鲁棒性的脑图表示，提高了精神病诊断的准确性和可解释性。

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [14] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: D-CAT是一种解耦跨注意力迁移框架，能够在推理时仅使用单一传感器模态，通过跨模态知识迁移提升多模态分类性能，减少硬件冗余。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态迁移学习方法在训练和推理时都需要配对的多传感器数据，这在资源受限环境中（如经济和技术上无法部署完整传感器套件）部署受限。

Method: 提出D-CAT框架，结合自注意力模块进行特征提取和新型跨注意力对齐损失，在不耦合两种模态分类流水线的情况下对齐传感器特征空间。

Result: 在三个多模态人类活动数据集（IMU、视频和音频）上评估，在分布内场景中，从高性能模态（如视频到IMU）迁移可获得10% F1分数提升；在分布外场景中，即使较弱的源模态也能改善目标性能。

Conclusion: D-CAT通过实现单传感器推理和跨模态知识迁移，在保持精度的同时减少了感知系统的硬件冗余，对成本敏感或自适应部署（如家庭辅助机器人）至关重要。

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [15] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: Meta-RL-Crypto是一个基于Transformer的统一架构，结合元学习和强化学习，创建了一个完全自我改进的交易代理，用于加密货币交易预测，无需人工监督，在多种市场环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 加密货币回报预测极其困难，价格变动受到快速变化的链上活动、新闻流和社交情绪等多种因素驱动，且标记训练数据稀缺昂贵。

Method: 从基础的指令调优LLM开始，代理在闭环架构中迭代交替扮演三个角色（执行者、评判者和元评判者），利用多模态市场输入和内部偏好反馈，持续优化交易策略和评估标准。

Result: 在多样化市场机制下的实验表明，Meta-RL-Crypto在真实市场的技术指标上表现良好，并且优于其他基于LLM的基线方法。

Conclusion: 该研究提出了一个无需人工监督的自我改进交易代理架构，成功解决了加密货币预测的挑战，展示了在复杂市场环境中的优异性能。

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [16] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: LAVa是一个统一的KV缓存压缩框架，通过最小化Transformer残差流中的信息损失来实现动态预算分配，在多种长上下文基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法大多是启发式的，缺乏动态预算分配机制，无法根据任务需求灵活调整不同层和注意力头的缓存预算。

Method: 提出统一框架最小化残差流信息损失，分析层注意力输出损失并推导新度量标准，实现层级压缩和动态头预算分配，通过跨层信息对比实现动态层预算分配。

Result: 在LongBench、Needle-In-A-Haystack、Ruler和InfiniteBench等基准测试中表现优异，发现动态层预算对生成任务关键，动态头预算对抽取任务重要。

Conclusion: LAVa是首个统一的缓存淘汰和动态预算分配策略，无需训练或多策略组合，在各种任务类型中保持顶级性能。

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [17] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: 提出了HACO框架，将风险校准与偏好优化分离，为医疗补助人群的健康管理提供保守、可审计的决策支持


<details>
  <summary>Details</summary>
Motivation: 医疗补助人群的健康管理项目需要协调纵向服务，必须确保安全、公平和可审计性，需要开发能够控制不良事件风险的决策支持系统

Method: 混合自适应符合离线强化学习框架：训练轻量级风险模型、推导符合阈值来屏蔽不安全行为、在安全子集上学习偏好策略，并使用版本无关的FQE进行评估

Result: HACO实现了强大的风险区分能力（AUC约0.81），校准阈值良好，保持高安全覆盖率，亚组分析显示不同人口统计特征间存在系统性价值差异

Conclusion: 符合风险门控与离线强化学习相结合，能够为人口健康管理团队提供保守且可审计的决策支持，强调公平性审计的重要性

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [18] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: 提出基于单头交叉注意力机制的统一路由框架，动态选择最优LLM，在RouterBench基准上实现6.6%的质量提升和2.9%的最大性能提升


<details>
  <summary>Details</summary>
Motivation: 解决不同计算成本和性能的大型语言模型在现实应用中规模化、经济高效部署的挑战

Method: 使用单头交叉注意力机制联合建模查询和模型嵌入，显式捕获细粒度查询-模型交互，预测响应质量和生成成本

Result: 在RouterBench基准测试中，平均质量改进(AIQ)提升6.6%，最大性能提升2.9%，相比现有路由方法效率更高

Conclusion: 该轻量级架构能够有效平衡性能和成本，跨域泛化能力强，为成本感知的LLM路由建立了新标准

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [19] [From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms](https://arxiv.org/abs/2509.09793)
*Vincent Herfeld,Baudouin Denis de Senneville,Arthur Leclaire,Nicolas Papadakis*

Main category: cs.LG

TL;DR: 分析梯度步去噪器及其在即插即用算法中的应用，该去噪器被训练为显式函数的功能下降算子或邻近算子，同时保持最先进的去噪能力


<details>
  <summary>Details</summary>
Motivation: 即插即用优化算法通常使用现成的去噪器来替代图像先验的邻近算子或梯度下降算子，但这些先验通常是隐式的且无法表达

Method: 训练梯度步去噪器，使其能够精确地作为显式函数的功能下降算子或邻近算子

Result: 梯度步去噪器在保持最先进去噪能力的同时，能够作为显式函数的算子

Conclusion: 梯度步去噪器为即插即用算法提供了显式的函数表示，同时保持了优秀的去噪性能

Abstract: In this paper we analyze the Gradient-Step Denoiser and its usage in
Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms
uses off the shelf denoisers to replace a proximity operator or a gradient
descent operator of an image prior. Usually this image prior is implicit and
cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly
the gradient descent operator or the proximity operator of an explicit
functional while preserving state-of-the-art denoising capabilities.

</details>


### [20] [Distinguishing Startle from Surprise Events Based on Physiological Signals](https://arxiv.org/abs/2509.09799)
*Mansi Sharma,Alexandre Duchevet,Florian Daiber,Jean-Paul Imbert,Maurice Rekrut*

Main category: cs.LG

TL;DR: 本研究使用机器学习和多模态融合策略，通过生理信号区分惊吓和惊讶反应，在航空等高危环境中提高安全性。


<details>
  <summary>Details</summary>
Motivation: 意外事件会损害注意力和延迟决策，在航空等高危环境中构成严重安全风险。惊吓和惊讶反应以不同方式影响飞行员表现，但在实践中难以区分。现有研究大多单独研究这些反应，对其组合效应或如何通过生理数据区分它们的研究有限。

Method: 使用机器学习和多模态融合策略，基于生理信号区分惊吓和惊讶事件。采用了SVM和XGBoost等算法，以及Late Fusion融合策略。

Result: 能够可靠预测这些事件，使用SVM和Late Fusion获得最高平均准确率85.7%。在包含基线条件的扩展评估中，使用XGBoost和Late Fusion成功区分惊吓、惊讶和基线状态，最高平均准确率达到74.9%。

Conclusion: 研究表明通过生理信号和机器学习方法可以有效区分惊吓和惊讶反应，为高危环境中的安全风险评估提供了有效工具。

Abstract: Unexpected events can impair attention and delay decision-making, posing
serious safety risks in high-risk environments such as aviation. In particular,
reactions like startle and surprise can impact pilot performance in different
ways, yet are often hard to distinguish in practice. Existing research has
largely studied these reactions separately, with limited focus on their
combined effects or how to differentiate them using physiological data. In this
work, we address this gap by distinguishing between startle and surprise events
based on physiological signals using machine learning and multi-modal fusion
strategies. Our results demonstrate that these events can be reliably
predicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.
To further validate the robustness of our model, we extended the evaluation to
include a baseline condition, successfully differentiating between Startle,
Surprise, and Baseline states with a highest mean accuracy of 74.9% with
XGBoost and Late Fusion.

</details>


### [21] [Flow Straight and Fast in Hilbert Space: Functional Rectified Flow](https://arxiv.org/abs/2509.10384)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: 本文提出了无限维希尔伯特空间中整流流的严格函数形式化方法，建立了基于连续性方程叠加原理的理论框架，并扩展到函数流匹配和概率流ODE，在实验中展现了优于现有函数生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 许多在有限维欧几里得空间中开发的生成模型在无限维设置中都有函数泛化，但整流流向无限维空间的扩展尚未被探索。

Method: 基于无限维空间中连续性方程的叠加原理，建立了整流流的严格函数形式化方法，并将其扩展到函数流匹配和函数概率流ODE。

Result: 实验证明该方法相比现有函数生成模型具有更优越的性能，同时移除了现有函数流匹配理论中的限制性测度理论假设。

Conclusion: 成功建立了无限维希尔伯特空间中整流流的理论框架，为函数生成模型提供了新的非线性泛化方法，并在实践中表现出色。

Abstract: Many generative models originally developed in finite-dimensional Euclidean
space have functional generalizations in infinite-dimensional settings.
However, the extension of rectified flow to infinite-dimensional spaces remains
unexplored. In this work, we establish a rigorous functional formulation of
rectified flow in an infinite-dimensional Hilbert space. Our approach builds
upon the superposition principle for continuity equations in an
infinite-dimensional space. We further show that this framework extends
naturally to functional flow matching and functional probability flow ODEs,
interpreting them as nonlinear generalizations of rectified flow. Notably, our
extension to functional flow matching removes the restrictive measure-theoretic
assumptions in the existing theory of \citet{kerrigan2024functional}.
Furthermore, we demonstrate experimentally that our method achieves superior
performance compared to existing functional generative models.

</details>


### [22] [Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning](https://arxiv.org/abs/2509.09838)
*Reza Asad,Reza Babanezhad,Sharan Vaswani*

Main category: cs.LG

TL;DR: 本文提出了一种解耦actor和critic熵的灵活off-policy actor-critic框架，在离散动作环境中实现了与DQN相当的性能，无需熵正则化或显式探索。


<details>
  <summary>Details</summary>
Motivation: 现有的基于值的方法（如DQN）是离散动作环境中的默认off-policy方法，而常见的基于策略的方法要么无法有效利用off-policy数据（如PPO），要么在离散动作设置中表现不佳（如SAC）。

Method: 从离散SAC（DSAC）出发，发现actor和critic熵的耦合是DSAC性能差的主要原因。通过解耦这两个组件，并引入灵活的off-policy actor-critic框架，使用m步Bellman算子进行critic更新，结合标准策略优化方法和熵正则化。

Result: 理论证明在表格设置中可以收敛到最优正则化值函数。实证表明在标准Atari游戏中可以达到DQN的性能水平，即使没有熵正则化或显式探索。

Conclusion: 解耦actor和critic的熵设计是提升离散动作环境中actor-critic方法性能的关键，提出的框架为off-policy强化学习提供了新的有效解决方案。

Abstract: Value-based approaches such as DQN are the default methods for off-policy
reinforcement learning with discrete-action environments such as Atari. Common
policy-based methods are either on-policy and do not effectively learn from
off-policy data (e.g. PPO), or have poor empirical performance in the
discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC
(DSAC), we revisit the design of actor-critic methods in this setting. First,
we determine that the coupling between the actor and critic entropy is the
primary reason behind the poor performance of DSAC. We demonstrate that by
merely decoupling these components, DSAC can have comparable performance as
DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic
framework that subsumes DSAC as a special case. Our framework allows using an
m-step Bellman operator for the critic update, and enables combining standard
policy optimization methods with entropy regularization to instantiate the
resulting actor objective. Theoretically, we prove that the proposed methods
can guarantee convergence to the optimal regularized value function in the
tabular setting. Empirically, we demonstrate that these methods can approach
the performance of DQN on standard Atari games, and do so even without entropy
regularization or explicit exploration.

</details>


### [23] [Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration](https://arxiv.org/abs/2509.10439)
*Ahmed Khaled,Satyen Kale,Arthur Douillard,Chi Jin,Rob Fergus,Manzil Zaheer*

Main category: cs.LG

TL;DR: 本文研究了Local SGD中外部优化器的作用，证明了新的收敛保证，发现调整外部学习率可以在优化误差和随机梯度噪声方差之间权衡，并弥补内部学习率的不当调整。理论表明外部学习率有时应大于1，且外部优化器中的动量能改善收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在分布式机器学习中，通信是主要瓶颈，Local SGD能有效减少通信开销。虽然已有大量研究关注本地优化过程的超参数影响，但外部优化器及其超参数的选择尚不明确，需要深入研究外部优化器在Local SGD中的作用。

Method: 通过理论分析证明Local SGD的收敛保证，研究外部学习率的调节作用，扩展到使用动量的外部优化器设置，并研究外部优化器中的加速技术。还引入了新的数据依赖性分析，并通过标准语言模型和各种外部优化器进行综合实验验证。

Result: 理论表明调整外部学习率可以权衡优化误差和随机梯度噪声方差，弥补内部学习率的不当调整，外部学习率有时应大于1。外部优化器中的动量能改善收敛速度，超越先前局部应用加速的算法。实验验证了理论结果。

Conclusion: 外部优化器在Local SGD中扮演关键角色，适当调节外部学习率和动量参数能显著提升算法性能，外部加速技术能改善通信轮次相关的收敛速率，为分布式机器学习提供了重要的理论指导和实践启示。

Abstract: Modern machine learning often requires training with large batch size,
distributed data, and massively parallel compute hardware (like mobile and
other edge devices or distributed data centers). Communication becomes a major
bottleneck in such settings but methods like Local Stochastic Gradient Descent
(Local SGD) show great promise in reducing this additional communication
overhead. Local SGD consists of three parts: a local optimization process, an
aggregation mechanism, and an outer optimizer that uses the aggregated updates
from the nodes to produce a new model. While there exists an extensive
literature on understanding the impact of hyperparameters in the local
optimization process, the choice of outer optimizer and its hyperparameters is
less clear. We study the role of the outer optimizer in Local SGD, and prove
new convergence guarantees for the algorithm. In particular, we show that
tuning the outer learning rate allows us to (a) trade off between optimization
error and stochastic gradient noise variance, and (b) make up for ill-tuning of
the inner learning rate. Our theory suggests that the outer learning rate
should sometimes be set to values greater than $1$. We extend our results to
settings where we use momentum in the outer optimizer, and we show a similar
role for the momentum-adjusted outer learning rate. We also study acceleration
in the outer optimizer and show that it improves the convergence rate as a
function of the number of communication rounds, improving upon the convergence
rate of prior algorithms that apply acceleration locally. Finally, we also
introduce a novel data-dependent analysis of Local SGD that yields further
insights on outer learning rate tuning. We conduct comprehensive experiments
with standard language models and various outer optimizers to validate our
theory.

</details>


### [24] [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: HGEN是首个针对异质图的集成学习框架，通过元路径和随机丢弃创建多样化的GNN基学习器，使用残差注意力机制和相关性正则化来提升准确性和多样性，在五个异质网络上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异质图中节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来挑战，需要适应不同的图学习器并提升分类精度。

Method: 使用元路径结合随机丢弃创建等位GNN基学习器，通过残差注意力机制校准不同元路径的GNN以关注更有信息量的图，并采用相关性正则化项增大不同元路径嵌入矩阵的差异来丰富多样性。

Result: 在五个异质网络上的实验验证HGEN始终以显著优势超越最先进的竞争对手，收敛性分析也证实其比简单投票具有更高的正则化强度。

Conclusion: HGEN成功解决了异质图集成学习的挑战，通过创新的元路径优化流程和双重机制（准确性提升和多样性增强）实现了优异的分类性能。

Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous
graphs. We argue that the heterogeneity in node types, nodal features, and
local neighborhood topology poses significant challenges for ensemble learning,
particularly in accommodating diverse graph learners. Our HGEN framework
ensembles multiple learners through a meta-path and transformation-based
optimization pipeline to uplift classification accuracy. Specifically, HGEN
uses meta-path combined with random dropping to create Allele Graph Neural
Networks (GNNs), whereby the base graph learners are trained and aligned for
later ensembling. To ensure effective ensemble learning, HGEN presents two key
components: 1) a residual-attention mechanism to calibrate allele GNNs of
different meta-paths, thereby enforcing node embeddings to focus on more
informative graphs to improve base learner accuracy, and 2) a
correlation-regularization term to enlarge the disparity among embedding
matrices generated from different meta-paths, thereby enriching base learner
diversity. We analyze the convergence of HGEN and attest its higher
regularization magnitude over simple voting. Experiments on five heterogeneous
networks validate that HGEN consistently outperforms its state-of-the-art
competitors by substantial margin.

</details>


### [25] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 论文提出了一个动态计算分配框架，在推理时根据查询需求选择最佳生成策略（如beam search或best-of-N），同时考虑token成本和延迟时间，以优化LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理时扩展方法主要关注并行生成技术（如best-of-N），忽略了增量解码方法（如beam search），且只关注token使用而忽视了延迟时间，这对于用户体验和智能体工作流至关重要。

Method: 将推理时扩展建模为动态计算分配和方法选择问题，系统根据每个查询决定应用哪种策略以及分配多少计算资源，明确考虑token成本和wall-clock延迟。

Result: 在推理基准测试中，该方法始终优于静态策略，实现了良好的准确率-成本权衡，同时保持实际部署的可行性。

Conclusion: 动态计算分配框架能够有效优化LLM推理性能，在保证准确性的同时控制成本和延迟，特别适用于需要高效多查询的智能体工作流。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [26] [Variational Neural Networks for Observable Thermodynamics (V-NOTS)](https://arxiv.org/abs/2509.09899)
*Christopher Eldred,François Gay-Balmaz,Vakhtang Putkaradze*

Main category: cs.LG

TL;DR: 提出了一种基于可观测变量的数据驱动计算框架，通过热力学拉格朗日方法和神经网络来预测耗散动力系统的演化，确保熵不减特性


<details>
  <summary>Details</summary>
Motivation: 现有方法需要相空间变量数据，但实际中动量、熵等变量往往无法直接观测，需要开发仅基于可观测变量的计算方法

Method: 构建基于热力学拉格朗日的新型方法，设计神经网络来保持热力学约束并保证熵不减演化

Result: 网络能够基于有限数据点和较少参数有效描述相空间演化

Conclusion: 该方法为仅使用可观测变量进行耗散系统演化预测提供了有效框架

Abstract: Much attention has recently been devoted to data-based computing of evolution
of physical systems. In such approaches, information about data points from
past trajectories in phase space is used to reconstruct the equations of motion
and to predict future solutions that have not been observed before. However, in
many cases, the available data does not correspond to the variables that define
the system's phase space. We focus our attention on the important example of
dissipative dynamical systems. In that case, the phase space consists of
coordinates, momenta and entropies; however, the momenta and entropies cannot,
in general, be observed directly. To address this difficulty, we develop an
efficient data-based computing framework based exclusively on observable
variables, by constructing a novel approach based on the \emph{thermodynamic
Lagrangian}, and constructing neural networks that respect the thermodynamics
and guarantees the non-decreasing entropy evolution. We show that our network
can provide an efficient description of phase space evolution based on a
limited number of data points and a relatively small number of parameters in
the system.

</details>


### [27] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: 提出了LoFT框架，通过参数高效微调基础模型来解决长尾半监督学习中的过自信和低质量伪标签问题，并在开放世界场景下扩展为LoFT-OW处理OOD样本，仅用1%未标注数据就优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决现有长尾半监督学习方法从头训练模型导致的过自信和低质量伪标签问题，将基础模型微调范式引入长尾半监督学习

Method: 提出LoFT框架，通过参数高效微调基础模型生成更可靠的伪标签；进一步提出LoFT-OW处理开放世界场景下的OOD样本

Result: 在多个基准测试中表现出优越性能，即使仅使用1%的未标注数据也能超越先前方法

Conclusion: 基础模型微调范式能有效提升长尾半监督学习的性能，特别是在处理开放世界OOD样本时表现出色

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [28] [Multi-Play Combinatorial Semi-Bandit Problem](https://arxiv.org/abs/2509.09933)
*Shintaro Nakamura,Yuko Kuroki,Wei Chen*

Main category: cs.LG

TL;DR: 提出了多臂组合半赌博机（MP-CSB）模型，扩展了传统组合半赌博机到非负整数动作空间，并设计了两种高效算法：基于Thompson采样的算法和适应随机与对抗环境的通用算法。


<details>
  <summary>Details</summary>
Motivation: 传统组合半赌博机（CSB）仅限于二元决策空间，无法处理包含非负整数流或分配的重要问题（如最优传输和背包问题），需要扩展模型以支持更广泛的应用场景。

Method: 提出MP-CSB模型，允许选择非负整数动作并从单个臂观察多个反馈。设计了两种算法：1）基于Thompson采样的计算高效算法，适用于动作空间指数大的情况；2）最佳两用算法，在随机和对抗环境下都能取得良好性能。

Result: Thompson采样算法在随机环境下达到O(log T)分布相关遗憾界；最佳两用算法在随机环境下达到O(log T)方差相关遗憾界，在对抗环境下达到Õ(√T)最坏情况遗憾界，且具有数据自适应性。数值实验显示算法优于现有方法。

Conclusion: MP-CSB成功扩展了组合半赌博机框架，提出的两种算法在理论和实验上都表现出色，为解决非负整数动作空间的组合优化问题提供了有效工具。

Abstract: In the combinatorial semi-bandit (CSB) problem, a player selects an action
from a combinatorial action set and observes feedback from the base arms
included in the action. While CSB is widely applicable to combinatorial
optimization problems, its restriction to binary decision spaces excludes
important cases involving non-negative integer flows or allocations, such as
the optimal transport and knapsack problems.To overcome this limitation, we
propose the multi-play combinatorial semi-bandit (MP-CSB), where a player can
select a non-negative integer action and observe multiple feedbacks from a
single arm in each round. We propose two algorithms for the MP-CSB. One is a
Thompson-sampling-based algorithm that is computationally feasible even when
the action space is exponentially large with respect to the number of arms, and
attains $O(\log T)$ distribution-dependent regret in the stochastic regime,
where $T$ is the time horizon. The other is a best-of-both-worlds algorithm,
which achieves $O(\log T)$ variance-dependent regret in the stochastic regime
and the worst-case $\tilde{\mathcal{O}}\left( \sqrt{T} \right)$ regret in the
adversarial regime. Moreover, its regret in adversarial one is data-dependent,
adapting to the cumulative loss of the optimal action, the total quadratic
variation, and the path-length of the loss sequence. Finally, we numerically
show that the proposed algorithms outperform existing methods in the CSB
literature.

</details>


### [29] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: 该论文探索使用LLM作为科学机器学习代理，通过生成代码而非直接预测来解决ODE问题，并引入了包含误导性问题和1000个ODE任务的新基准来评估LLM的科学计算能力。


<details>
  <summary>Details</summary>
Motivation: 传统科学机器学习方法在准确性和鲁棒性方面面临挑战，研究者希望探索LLM通过编写数值算法代码来替代直接函数学习的新途径。

Method: 引入两个新数据集：诊断性误导问题和1000个多样化ODE任务的大规模基准；评估开源和闭源LLM在无引导vs领域知识引导提示、现成vs微调变体下的表现。

Result: 研究发现，在充分上下文和引导提示下，较新的指令跟随模型在可执行性和数值有效性方面都达到高准确率，开源系统表现强劲且无需微调。

Conclusion: 精心设计的提示和微调可以产生能够可靠解决简单ODE问题的专门化LLM代理，为科学计算任务提供了有前景的新方向。

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [30] [DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition](https://arxiv.org/abs/2509.09940)
*Yifei Wang,Wenbin Wang,Yong Luo*

Main category: cs.LG

TL;DR: DyKen-Hyena模型通过将音频视觉线索转换为动态的逐token卷积核来直接调制文本特征提取，而不是简单的特征融合，在多模态意图识别任务上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态意图识别方法通过多头注意力等机制融合多模态特征，但可能会让主要语言特征被噪声或无关的非语言信号污染，无法捕捉细粒度的token级影响。

Method: 将问题从特征融合重新定义为处理调制，将音频视觉线索转换为动态的逐token卷积核，直接调制文本特征提取过程。

Result: 在MIntRec和MIntRec2.0基准测试中达到最先进水平，在超出范围检测中获得+10.46%的F1分数提升。

Conclusion: 该方法创建了更鲁棒的意图表示，验证了细粒度调制方法的有效性。

Abstract: Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential for intent-irrelevant and conflicting information across modalities
may hinder performance from being further improved. Most current models attempt
to fuse modalities by applying mechanisms like multi-head attention to unimodal
feature sequences and then adding the result back to the original
representation. This process risks corrupting the primary linguistic features
with noisy or irrelevant non-verbal signals, as it often fails to capture the
fine-grained, token-level influence where non-verbal cues should modulate, not
just augment, textual meaning. To address this, we introduce DyKen-Hyena, which
reframes the problem from feature fusion to processing modulation. Our model
translates audio-visual cues into dynamic, per-token convolutional kernels that
directly modulate textual feature extraction. This fine-grained approach
achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.
Notably, it yields a +10.46% F1-score improvement in out-of-scope detection,
validating that our method creates a fundamentally more robust intent
representation.

</details>


### [31] [Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms](https://arxiv.org/abs/2509.10369)
*Gul Rukh Khattak,Konstantinos Patlatzoglou,Joseph Barker,Libor Pastika,Boroumand Zeidaabadi,Ahmed El-Medany,Hesham Aggour,Yixiu Liang,Antonio H. Ribeiro,Jeffrey Annis,Antonio Luiz Pinho Ribeiro,Junbo Ge,Daniel B. Kramer,Jonathan W. Waks,Evan Brittain,Nicholas Peters,Fu Siong Ng,Arunashis Sau*

Main category: cs.LG

TL;DR: 对比学习在自监督预训练中广泛应用，但受队列组成影响。研究通过CAPE模型在四大洲不同人群的心电图数据上预训练，发现预训练队列的人口统计和健康状况影响下游性能。多中心多样化队列提高分布内准确性但降低分布外泛化能力，为此提出IDB策略增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习在自监督预训练中对队列组成的依赖性，特别是人口统计学特征、健康状况和人群多样性如何影响下游预测任务的性能表现。

Method: 开发CAPE基础模型，在来自三大洲（北美、南美、亚洲）四个队列（n=5,203,352）的心电图数据上进行预训练，并系统评估预训练队列特征对下游任务的影响，包括在欧洲两个额外队列上的测试。提出In-Distribution Batch (IDB)策略来保持队列内一致性。

Result: 下游性能确实依赖于预训练队列的分布特性，包括人口统计和健康状况。多中心多样化队列虽然提高了分布内准确性，但会编码队列特定伪影，降低对比学习方法的分布外泛化能力。IDB策略有效增强了分布外鲁棒性。

Conclusion: 这项工作为开发临床公平和可泛化的基础模型提供了重要见解，强调了预训练队列组成对模型性能的关键影响，并提出了改进分布外泛化的有效策略。

Abstract: Contrastive learning is a widely adopted self-supervised pretraining
strategy, yet its dependence on cohort composition remains underexplored. We
present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation
model and pretrain on four cohorts (n = 5,203,352), from diverse populations
across three continents (North America, South America, Asia). We systematically
assess how cohort demographics, health status, and population diversity
influence the downstream performance for prediction tasks also including two
additional cohorts from another continent (Europe). We find that downstream
performance depends on the distributional properties of the pretraining cohort,
including demographics and health status. Moreover, while pretraining with a
multi-centre, demographically diverse cohort improves in-distribution accuracy,
it reduces out-of-distribution (OOD) generalisation of our contrastive approach
by encoding cohort-specific artifacts. To address this, we propose the
In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency
during pretraining and enhances OOD robustness. This work provides important
insights for developing clinically fair and generalisable foundation models.

</details>


### [32] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: 提出无需训练的token合并框架，通过自适应合并语义冗余token来压缩transformer表示，在保持精度的同时显著降低计算和通信成本。


<details>
  <summary>Details</summary>
Motivation: 大规模transformer在语义通信中至关重要，但其高计算和通信成本阻碍了在资源受限边缘设备上的部署。

Method: 基于每层相似度阈值选择性合并语义冗余token，将合并策略发现建模为多目标优化问题，利用贝叶斯优化获得帕累托最优权衡。

Result: 在ImageNet分类中，以30%更少的FLOPs和低于20%的原始通信成本匹配未修改transformer的精度；在VQA任务中，以不到1/3计算量和1/10带宽实现与完整LLaVA模型相当的性能。

Conclusion: 该框架为在资源有限的边缘智能场景中部署强大transformer模型提供了实用且通用的解决方案，具有跨信道条件的鲁棒性和隐私保护优势。

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>


### [33] [Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes](https://arxiv.org/abs/2509.09960)
*Mingxuan Jiang,Yongxin Wang,Ziyue Dai,Yicun Liu,Hongyi Nie,Sen Liu,Hongfeng Chai*

Main category: cs.LG

TL;DR: ReFine框架通过从可解释模型提取符号规则嵌入提示词，并结合双粒度过滤策略，有效解决了小样本表格数据生成中的特征依赖性和分布不平衡问题，在回归和分类任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格生成方法（GANs、扩散模型、微调LLMs）需要充足参考数据，在领域特定数据库样本稀缺时效果有限。基于提示的LLMs虽然灵活但难以捕捉数据集特定的特征-标签依赖关系，且容易生成冗余数据导致下游任务性能下降。

Method: 提出ReFine框架：(i)从可解释模型推导符号"if-then"规则并嵌入提示词，显式指导生成过程符合领域特定特征分布；(ii)应用双粒度过滤策略，抑制过采样模式并选择性精炼稀有但信息丰富的样本以减少分布不平衡。

Result: 在各种回归和分类基准测试上的广泛实验表明，ReFine始终优于最先进方法，在回归任务中R平方绝对提升达0.44，在分类任务中F1分数相对提升达10.0%。

Conclusion: ReFine通过结合符号规则引导和双粒度过滤，有效解决了小样本表格数据生成的关键挑战，为领域特定数据库的数据增强提供了有效解决方案。

Abstract: Synthetic tabular data generation is increasingly essential in data
management, supporting downstream applications when real-world and high-quality
tabular data is insufficient. Existing tabular generation approaches, such as
generative adversarial networks (GANs), diffusion models, and fine-tuned Large
Language Models (LLMs), typically require sufficient reference data, limiting
their effectiveness in domain-specific databases with scarce records. While
prompt-based LLMs offer flexibility without parameter tuning, they often fail
to capture dataset-specific feature-label dependencies and generate redundant
data, leading to degradation in downstream task performance. To overcome these
issues, we propose ReFine, a framework that (i) derives symbolic "if-then"
rules from interpretable models and embeds them into prompts to explicitly
guide generation toward domain-specific feature distribution, and (ii) applies
a dual-granularity filtering strategy that suppresses over-sampling patterns
and selectively refines rare but informative samples to reduce distributional
imbalance. Extensive experiments on various regression and classification
benchmarks demonstrate that ReFine consistently outperforms state-of-the-art
methods, achieving up to 0.44 absolute improvement in R-squared for regression
and 10.0 percent relative improvement in F1 score for classification tasks.

</details>


### [34] [Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning](https://arxiv.org/abs/2509.09991)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 提出基于机器学习的方法，仅使用虚拟机资源利用率指标来预测能耗，无需物理功率测量接口或主机特权访问


<details>
  <summary>Details</summary>
Motivation: 解决虚拟化环境（如云平台）中无法直接测量能耗的问题，为能源感知调度和成本优化提供可行方案

Method: 使用梯度提升回归器（Gradient Boosting Regressor），基于客户虚拟机收集的资源利用率指标来预测通过RAPL在主机上测量的能耗

Result: 在多样化工作负载实验中实现了高预测精度（R²在0.90到0.97之间），证明了仅使用客户侧资源进行能耗估计的可行性

Conclusion: 该方法填补了虚拟化环境中无法直接测量能耗的关键空白，能够支持能源感知调度、成本优化和独立于物理主机的能耗估计

Abstract: This paper presents a machine learning-based approach to estimate the energy
consumption of virtual servers without access to physical power measurement
interfaces. Using resource utilization metrics collected from guest virtual
machines, we train a Gradient Boosting Regressor to predict energy consumption
measured via RAPL on the host. We demonstrate, for the first time, guest-only
resource-based energy estimation without privileged host access with
experiments across diverse workloads, achieving high predictive accuracy and
variance explained ($0.90 \leq R^2 \leq 0.97$), indicating the feasibility of
guest-side energy estimation. This approach can enable energy-aware scheduling,
cost optimization and physical host independent energy estimates in virtualized
environments. Our approach addresses a critical gap in virtualized environments
(e.g. cloud) where direct energy measurement is infeasible.

</details>


### [35] [Neural Scaling Laws for Deep Regression](https://arxiv.org/abs/2509.10000)
*Tilen Cadez,Kyoung-Min Kim*

Main category: cs.LG

TL;DR: 论文实证研究了深度回归模型中的神经缩放定律，发现在扭曲范德瓦尔斯磁体参数估计模型中，损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数范围为1到2。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型的成功凸显了神经缩放定律的重要性，但这些定律在深度回归模型中的应用仍未被充分探索。研究旨在填补这一空白，为资源受限条件下的可靠模型开发提供指导。

Method: 使用扭曲范德瓦尔斯磁体的参数估计模型，采用多种架构（包括全连接网络、残差网络和视觉变换器），在不同训练数据集大小和模型容量范围内实证研究损失函数的缩放行为。

Result: 观察到损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数范围在1到2之间，具体值取决于回归参数和模型细节。一致的缩放行为和大缩放指数表明模型性能可随数据量增加而显著提升。

Conclusion: 深度回归模型存在显著的神经缩放定律，大缩放指数意味着增加数据量可以大幅提升模型性能，这为资源分配和模型优化提供了重要指导。

Abstract: Neural scaling laws--power-law relationships between generalization errors
and characteristics of deep learning models--are vital tools for developing
reliable models while managing limited resources. Although the success of large
language models highlights the importance of these laws, their application to
deep regression models remains largely unexplored. Here, we empirically
investigate neural scaling laws in deep regression using a parameter estimation
model for twisted van der Waals magnets. We observe power-law relationships
between the loss and both training dataset size and model capacity across a
wide range of values, employing various architectures--including fully
connected networks, residual networks, and vision transformers. Furthermore,
the scaling exponents governing these relationships range from 1 to 2, with
specific values depending on the regressed parameters and model details. The
consistent scaling behaviors and their large scaling exponents suggest that the
performance of deep regression models can improve substantially with increasing
data size.

</details>


### [36] [Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss](https://arxiv.org/abs/2509.10011)
*Antoine Orioua,Philipp Krah,Julian Koellermeier*

Main category: cs.LG

TL;DR: IDEA是一种能够估计数据集内在维度并重建原始数据的自编码器，通过投影重建损失项和重加权双CancelOut层结构，在理论和实际流体动力学数据上都表现出良好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在估计非线性流形数据集的内在维度方面存在局限性，需要一种既能准确估计内在维度又能有效重建原始数据的方法。

Method: 使用带有重加权双CancelOut层的自编码器结构，引入投影重建损失项来指导训练，通过连续评估去除潜在维度后的重建质量来确定内在维度。

Result: 在理论基准测试中表现出良好的准确性和通用性，在流体动力学数值解数据集上成功估计了内在维度并重建了原始解。

Conclusion: IDEA是一种有效且通用的内在维度估计方法，能够同时处理线性和非线性流形，并在复杂科学计算数据上表现出色。

Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),
which identifies the underlying intrinsic dimension of a wide range of datasets
whose samples lie on either linear or nonlinear manifolds. Beyond estimating
the intrinsic dimension, IDEA is also able to reconstruct the original dataset
after projecting it onto the corresponding latent space, which is structured
using re-weighted double CancelOut layers. Our key contribution is the
introduction of the projected reconstruction loss term, guiding the training of
the model by continuously assessing the reconstruction quality under the
removal of an additional latent dimension. We first assess the performance of
IDEA on a series of theoretical benchmarks to validate its robustness. These
experiments allow us to test its reconstruction ability and compare its
performance with state-of-the-art intrinsic dimension estimators. The
benchmarks show good accuracy and high versatility of our approach.
Subsequently, we apply our model to data generated from the numerical solution
of a vertically resolved one-dimensional free-surface flow, following a
pointwise discretization of the vertical velocity profile in the horizontal
direction, vertical direction, and time. IDEA succeeds in estimating the
dataset's intrinsic dimension and then reconstructs the original solution by
working directly within the projection space identified by the network.

</details>


### [37] [Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts](https://arxiv.org/abs/2509.10025)
*Strahinja Nikolic,Ilker Oguz,Demetri Psaltis*

Main category: cs.LG

TL;DR: 本文提出了一种稀疏专家混合变分自编码器(SMoE-VAE)架构，在QuickDraw数据集上发现无监督专家路由比有监督基准表现更好，能够识别超越人工定义类别边界的有意义子类别结构。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中神经网络内部组织的根本挑战，探索如何更好地理解模型如何发现与目标更一致的基础数据结构。

Method: 使用稀疏专家混合变分自编码器(SMoE-VAE)架构，在QuickDraw数据集上比较无监督专家路由和有监督基准，通过t-SNE可视化和重建分析进行研究。

Result: 无监督路由始终获得更好的重建性能，专家学会了识别有意义且常常超越人工定义类别边界的子类别结构。数据集规模研究揭示了数据量与专家专业化之间的权衡关系。

Conclusion: MoE模型能够发现比预定义标签更符合模型目标的基础数据结构，为设计高效的MoE架构提供了指导。

Abstract: Understanding the internal organization of neural networks remains a
fundamental challenge in deep learning interpretability. We address this
challenge by exploring a novel Sparse Mixture of Experts Variational
Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw
dataset, comparing unsupervised expert routing against a supervised baseline
guided by ground-truth labels. Surprisingly, we find that unsupervised routing
consistently achieves superior reconstruction performance. The experts learn to
identify meaningful sub-categorical structures that often transcend
human-defined class boundaries. Through t-SNE visualizations and reconstruction
analysis, we investigate how MoE models uncover fundamental data structures
that are more aligned with the model's objective than predefined labels.
Furthermore, our study on the impact of dataset size provides insights into the
trade-offs between data quantity and expert specialization, offering guidance
for designing efficient MoE architectures.

</details>


### [38] [Sparse Coding Representation of 2-way Data](https://arxiv.org/abs/2509.10033)
*Boya Ma,Abram Magner,Maxwell McNeil,Petko Bogdanov*

Main category: cs.LG

TL;DR: 提出了一种用于稀疏字典编码的低秩编码模型AODL，通过凸松弛和交替优化方法学习字典，在保证重构质量的同时获得更稀疏的解，并展示了在数据重建和缺失值填补方面的优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决多字典场景下字典学习的数据复杂度问题，传统方法需要学习所有原子组合的编码系数，计算复杂度高且样本需求量大。

Method: 提出低秩编码模型，使用凸松弛方法AODL，通过交替优化稀疏编码矩阵和学习字典来求解，并证明了方法的收敛性。

Result: 在合成和真实数据集上，AODL在固定重构质量下比非低秩和固定字典基线稀疏90%，学习到的字典能揭示训练样本中的可解释模式。

Conclusion: AODL方法有效降低了多字典学习的样本复杂度，提供了更稀疏且可解释的解决方案，在数据重建和缺失值填补任务中表现优异。

Abstract: Sparse dictionary coding represents signals as linear combinations of a few
dictionary atoms. It has been applied to images, time series, graph signals and
multi-way spatio-temporal data by jointly employing temporal and spatial
dictionaries. Data-agnostic analytical dictionaries, such as the discrete
Fourier transform, wavelets and graph Fourier, have seen wide adoption due to
efficient implementations and good practical performance. On the other hand,
dictionaries learned from data offer sparser and more accurate solutions but
require learning of both the dictionaries and the coding coefficients. This
becomes especially challenging for multi-dictionary scenarios since encoding
coefficients correspond to all atom combinations from the dictionaries. To
address this challenge, we propose a low-rank coding model for 2-dictionary
scenarios and study its data complexity. Namely, we establish a bound on the
number of samples needed to learn dictionaries that generalize to unseen
samples from the same distribution. We propose a convex relaxation solution,
called AODL, whose exact solution we show also solves the original problem. We
then solve this relaxation via alternating optimization between the sparse
coding matrices and the learned dictionaries, which we prove to be convergent.
We demonstrate its quality for data reconstruction and missing value imputation
in both synthetic and real-world datasets. For a fixed reconstruction quality,
AODL learns up to 90\% sparser solutions compared to non-low-rank and
analytical (fixed) dictionary baselines. In addition, the learned dictionaries
reveal interpretable insights into patterns present within the samples used for
training.

</details>


### [39] [Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability](https://arxiv.org/abs/2509.10034)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 本文提出了一种使用符号前馈神经网络精确模拟概率有限自动机(PFA)的理论框架，通过矩阵向量乘积实现概率状态传播，证明了PFA与特定神经网络类的等价性，并展示了这些符号模拟器可以通过梯度下降学习得到精确的PFA行为。


<details>
  <summary>Details</summary>
Motivation: 统一概率自动机理论与神经网络架构，弥合符号计算与深度学习之间的差距，提供可并行、可解释且可微分的PFA模拟方法。

Method: 使用符号前馈神经网络架构，将状态分布表示为向量，转移表示为随机矩阵，通过矩阵向量乘积实现概率状态传播，采用分层符号计算进行概率子集构造、ε-闭包和精确模拟。

Result: 证明了PFAs与特定神经网络类的等价性，展示了符号模拟器可以通过标准梯度下降优化在标记序列数据上学习，恢复真实PFA的精确行为。

Conclusion: 该工作为概率自动机理论和神经网络架构建立了严格的代数框架统一，实现了符号计算与深度学习的桥梁连接，核心贡献在于形式化证明了这些符号模拟器的可学习性。

Abstract: We present a formal and constructive theory showing that probabilistic finite
automata (PFAs) can be exactly simulated using symbolic feedforward neural
networks. Our architecture represents state distributions as vectors and
transitions as stochastic matrices, enabling probabilistic state propagation
via matrix-vector products. This yields a parallel, interpretable, and
differentiable simulation of PFA dynamics using soft updates-without
recurrence. We formally characterize probabilistic subset construction,
$\varepsilon$-closure, and exact simulation via layered symbolic computation,
and prove equivalence between PFAs and specific classes of neural networks. We
further show that these symbolic simulators are not only expressive but
learnable: trained with standard gradient descent-based optimization on labeled
sequence data, they recover the exact behavior of ground-truth PFAs. This
learnability, formalized in Proposition 5.1, is the crux of this work. Our
results unify probabilistic automata theory with neural architectures under a
rigorous algebraic framework, bridging the gap between symbolic computation and
deep learning.

</details>


### [40] [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)
*Mohammad Hasan Narimani,Mostafa Tavassolipour*

Main category: cs.LG

TL;DR: FedRP是一种新颖的联邦学习算法，结合随机投影和ADMM优化框架，在保护隐私的同时降低通信成本，并提供差分隐私保证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护用户隐私方面面临挑战，包括潜在的隐私攻击和高通信成本，需要一种既能保护隐私又高效的解决方案。

Method: 提出FedRP算法，集成随机投影技术降低模型参数维度，结合ADMM优化框架，在参数传输前进行降维处理。

Result: 实验结果显示FedRP在保持高模型精度的同时，在隐私保护和通信效率方面优于现有方法，包括传统差分隐私方法和FedADMM。

Conclusion: FedRP算法成功解决了联邦学习中的隐私保护和通信成本问题，为敏感数据领域的协作学习提供了有效的解决方案。

Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model
training across decentralized devices, such as smartphones, balancing enhanced
predictive performance with the protection of user privacy in sensitive areas
like Internet of Things (IoT) and medical data analysis. Despite its
advantages, FL encounters significant challenges related to user privacy
protection against potential attacks and the management of communication costs.
This paper introduces a novel federated learning algorithm called FedRP, which
integrates random projection techniques with the Alternating Direction Method
of Multipliers (ADMM) optimization framework. This approach enhances privacy by
employing random projection to reduce the dimensionality of model parameters
prior to their transmission to a central server, reducing the communication
cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential
privacy guarantee, demonstrating resilience against data reconstruction
attacks. Experimental results reveal that FedRP not only maintains high model
accuracy but also outperforms existing methods, including conventional
differential privacy approaches and FedADMM, in terms of both privacy
preservation and communication efficiency.

</details>


### [41] [Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data](https://arxiv.org/abs/2509.10048)
*Madhushan Ramalingam*

Main category: cs.LG

TL;DR: 这篇论文评估了VBLL与TabPFN集成在表格数据不确定性检验中的表现，发现原生TabPFN在所有数据集上均优于集成版本


<details>
  <summary>Details</summary>
Motivation: 在医疗诊断等安全关键应用中，可靠的不确定性估计至关重要。TabPFN是一种新的表格数据基础模型，本文评估将VBLL与TabPFN集成的不确定性检验性能

Method: 在3个标准医疗表格数据集上对比原生TabPFN和VBLL集成版TabPFN的不确定性检验性能

Result: 与预期相反，原生TabPFN在所有数据集上均一致地超过VBLL集成版本在不确定性检验方面的表现

Conclusion: 虽然VBLL是一种先进的轻量级变分方法，但在与TabPFN集成时并未能提升不确定性检验性能，TabPFN本身已具有良好的不确定性估计能力

Abstract: Predictive models are being increasingly used across a wide range of domains,
including safety-critical applications such as medical diagnosis and criminal
justice. Reliable uncertainty estimation is a crucial task in such settings.
Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine
learning foundation model for tabular dataset, which uses a generative
transformer architecture. Variational Bayesian Last Layers (VBLL) is a
state-of-the-art lightweight variational formulation that effectively improves
uncertainty estimation with minimal computational overhead. In this work we aim
to evaluate the performance of VBLL integrated with the recently proposed
TabPFN in uncertainty calibration. Our experiments, conducted on three
benchmark medical tabular datasets, compare the performance of the original
TabPFN and the VBLL-integrated version. Contrary to expectations, we observed
that original TabPFN consistently outperforms VBLL integrated TabPFN in
uncertainty calibration across all datasets.

</details>


### [42] [KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework](https://arxiv.org/abs/2509.10089)
*Marco Andrea Bühler,Gonzalo Guillén-Gosálbez*

Main category: cs.LG

TL;DR: KAN-SR是一个基于Kolmogorov Arnold Networks的新型符号回归框架，采用分治法，结合深度学习技术和简化策略，能够准确恢复Feynman SRSD数据集的真实方程，并能精确建模生物过程系统的动力学。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归通常使用遗传编程方法，本文旨在利用深度学习技术开发更有效的符号回归框架，提高方程发现的准确性和效率。

Method: 使用Kolmogorov Arnold Networks（KANs）构建分治法框架，结合平移对称性和可分离性等简化策略，并与神经控制微分方程结合进行动态建模。

Result: 成功恢复了Feynman SRSD数据集的真实方程，并精确建模了硅基生物过程系统的动力学，为其他工程系统的动态建模提供了可能。

Conclusion: KAN-SR框架在符号回归任务中表现出色，能够准确发现数学方程并有效建模复杂系统的动力学行为，具有广泛的工程应用前景。

Abstract: We introduce a novel symbolic regression framework, namely KAN-SR, built on
Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.
Symbolic regression searches for mathematical equations that best fit a given
dataset and is commonly solved with genetic programming approaches. We show
that by using deep learning techniques, more specific KANs, and combining them
with simplification strategies such as translational symmetries and
separabilities, we are able to recover ground-truth equations of the Feynman
Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we
show that by combining the proposed framework with neural controlled
differential equations, we are able to model the dynamics of an in-silico
bioprocess system precisely, opening the door for the dynamic modeling of other
engineering systems.

</details>


### [43] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出了一种基于信息几何投影的贝叶斯联邦学习个性化框架，通过将全局模型投影到用户本地模型的邻域来实现全局泛化与本地特化的可调权衡，在异构数据分布下有效平衡全局和本地性能。


<details>
  <summary>Details</summary>
Motivation: 现有的贝叶斯联邦学习方法通常依赖MCMC采样或变分推理，需要个性化机制来适应本地数据分布。本文旨在开发一种能够实现全局泛化和本地特化之间可调权衡的高效个性化方法。

Method: 提出信息几何投影框架，将全局模型投影到用户本地模型的邻域，证明该投影步骤等价于计算统计流形上的重心，从而得到闭式解并实现零成本个性化。应用于使用IVON优化器的变分学习设置，并扩展到BFL中的一般聚合方案。

Result: 在异构数据分布下的实证评估表明，该方法能够有效平衡全局和本地性能，且计算开销最小。

Conclusion: 信息几何投影框架为贝叶斯联邦学习提供了一种高效的个性化方法，能够在保持全局泛化能力的同时实现本地特化，计算成本低且性能优越。

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [44] [BenchECG and xECG: a benchmark and baseline for ECG foundation models](https://arxiv.org/abs/2509.10151)
*Riccardo Lunelli,Angus Nicolson,Samuel Martin Pröll,Sebastian Johannes Reinstadler,Axel Bauer,Clemens Dlaska*

Main category: cs.LG

TL;DR: 提出了BenchECG标准化基准和xECG模型，通过统一的评估框架解决ECG基础模型缺乏公平比较的问题，xECG在多个数据集和任务上表现最佳


<details>
  <summary>Details</summary>
Motivation: 现有ECG基础模型研究缺乏一致的评估标准，使用不同的任务选择和数据集，阻碍了公平比较和进展

Method: 开发BenchECG标准化基准，包含全面的公开ECG数据集和多样化任务；提出基于xLSTM的xECG模型，采用SimDINOv2自监督学习方法

Result: xECG在BenchECG基准上取得了最佳成绩，是唯一在所有数据集和任务上都表现优异的公开可用模型

Conclusion: BenchECG标准化评估促进了ECG表示学习的严谨比较和进展加速，xECG为未来ECG基础模型设立了新的性能基准

Abstract: Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to
deep learning. Recently, interest has grown in developing foundation models for
ECGs - models that generalise across diverse downstream tasks. However,
consistent evaluation has been lacking: prior work often uses narrow task
selections and inconsistent datasets, hindering fair comparison. Here, we
introduce BenchECG, a standardised benchmark comprising a comprehensive suite
of publicly available ECG datasets and versatile tasks. We also propose xECG,
an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,
which achieves the best BenchECG score compared to publicly available
state-of-the-art models. In particular, xECG is the only publicly available
model to perform strongly on all datasets and tasks. By standardising
evaluation, BenchECG enables rigorous comparison and aims to accelerate
progress in ECG representation learning. xECG achieves superior performance
over earlier approaches, defining a new baseline for future ECG foundation
models.

</details>


### [45] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: FedBiF是一种新颖的联邦学习框架，通过在本地训练期间直接学习量化模型参数，每次只更新一个比特位，实现了高效的通信压缩和模型稀疏化，在保持与FedAvg相当精度的同时显著降低了通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但存在严重的通信开销问题。现有方法通常在本地训练后进行量化，这会将量化误差引入训练参数中，可能降低模型精度。

Method: 提出FedBiF框架：服务器先量化模型参数并发送给客户端；每个客户端在本地训练时只更新多比特参数表示中的单个比特，冻结其余比特；采用逐比特更新策略，将每次参数更新减少到1比特。

Result: 在5个常用数据集上进行的IID和非IID设置实验表明，FedBiF不仅实现了优异的通信压缩，还促进了模型稀疏化。仅使用1bpp上行和3bpp下行通信就能达到与FedAvg相当的精度。

Conclusion: FedBiF通过直接在训练过程中学习量化参数，有效解决了联邦学习的通信开销问题，在保持模型精度的同时显著降低了通信成本，并产生了稀疏模型。

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [46] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: 提出了一种联邦多智能体强化学习框架Fed-MARL，用于6G超密集边缘网络中的隐私保护、实时资源管理，通过跨层协同和加密聚合协议实现能效优化和隐私安全。


<details>
  <summary>Details</summary>
Motivation: 6G网络向超密集智能边缘环境发展，需要在严格隐私、移动性和能耗约束下实现高效资源管理，传统集中式方法面临隐私泄露和可扩展性挑战。

Method: 采用联邦多智能体强化学习框架，每个智能体使用深度循环Q网络学习分散化策略，结合椭圆曲线Diffie Hellman密钥交换的安全聚合协议保护隐私，将问题建模为部分可观测多智能体马尔可夫决策过程。

Result: 仿真结果表明Fed-MARL在任务成功率、延迟、能效和公平性方面优于集中式MARL和启发式基线方法，同时确保强大的隐私保护和动态6G边缘网络中的可扩展性。

Conclusion: Fed-MARL框架为6G边缘网络提供了一种有效的隐私保护实时资源管理解决方案，通过跨层协同和联邦学习实现了多目标优化，具有良好的应用前景。

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [47] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: 提出了一种通过神经网络连续函数近似来重新优化表面码解码器的方法，解决了传统方法因非唯一正确预测而只能获取误差概率分布的问题。


<details>
  <summary>Details</summary>
Motivation: 量子纠错码中表面码的解码器传统方法存在只能获取误差概率分布的问题，因为从输入获得的正确预测具有非唯一性，这限制了解码精度。

Method: 使用神经网络数学插值的连续函数来近似综合征测量，重新优化解码器模型，将表面码解码问题重构为深度学习可处理的回归问题。

Result: 在码距为5和7的多层感知机解码器，以及码距为5的卷积神经网络、循环神经网络和Transformer解码器上，重新优化的解码器都显示出比原始模型更高的准确率。

Conclusion: 该方法具有普遍有效性，不受码距或网络架构的影响，表明将表面码解码问题重构为回归问题是有效的策略。

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [48] [The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams](https://arxiv.org/abs/2509.10167)
*Lénaïc Chizat*

Main category: cs.LG

TL;DR: 该研究分析了深度残差网络在梯度训练中的动态特性，证明了当深度趋于无穷时，训练动态收敛到神经平均ODE，并给出了输出与极限之间的误差界限。研究发现不同的残差缩放参数会导致完全特征学习或惰性ODE机制，特别针对两层感知机块的情况进行了详细分析。


<details>
  <summary>Details</summary>
Motivation: 研究深度残差网络的标准随机初始化梯度训练动态，特别是当网络深度趋于无穷时的极限行为，以及不同参数缩放对特征学习能力的影响。

Method: 使用数学分析框架，将残差网络的前向和后向传播视为随机平均ODE的近似，通过混沌传播理论保持训练动态中的这种行为。针对不同残差缩放参数和嵌入维度进行理论分析和实证验证。

Result: 证明了深度残差网络训练动态收敛到神经平均ODE，给出了误差界限O(1/L + α/√(LM))，并验证了该界限的紧致性。发现α=Θ(1)时实现完全特征学习，α→∞时进入惰性ODE机制。对于两层感知机块，唯一能实现完全特征学习的残差缩放是Θ(√D/(LM))。

Conclusion: 该研究为深度残差网络的训练动态提供了新的数学视角，揭示了不同参数缩放对特征学习能力的影响机制，并通过严格的误差分析和实证验证建立了理论框架，对理解深度神经网络的行为具有重要意义。

Abstract: We study the gradient-based training of large-depth residual networks
(ResNets) from standard random initializations. We show that with a diverging
depth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,
the training dynamics converges to a Neural Mean ODE training dynamics.
Remarkably, the limit is independent of the scaling of $M$, covering practical
cases of, say, Transformers, where $M$ (the number of hidden units or attention
heads per layer) is typically of the order of $D$. For a residual scale
$\Theta_D\big(\frac{\alpha}{LM}\big)$, we obtain the error bound
$O_D\big(\frac{1}{L}+ \frac{\alpha}{\sqrt{LM}}\big)$ between the model's output
and its limit after a fixed number gradient of steps, and we verify empirically
that this rate is tight. When $\alpha=\Theta(1)$, the limit exhibits complete
feature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In
contrast, we show that $\alpha \to \infty$ yields a \lazy ODE regime where the
Mean ODE is linearly parameterized. We then focus on the particular case of
ResNets with two-layer perceptron blocks, for which we study how these scalings
depend on the embedding dimension $D$. We show that for this model, the only
residual scale that leads to complete feature learning is
$\Theta\big(\frac{\sqrt{D}}{LM}\big)$. In this regime, we prove the error bound
$O\big(\frac{1}{L}+ \frac{\sqrt{D}}{\sqrt{LM}}\big)$ between the ResNet and its
limit after a fixed number of gradient steps, which is also empirically tight.
Our convergence results rely on a novel mathematical perspective on ResNets :
(i) due to the randomness of the initialization, the forward and backward pass
through the ResNet behave as the stochastic approximation of certain mean ODEs,
and (ii) by propagation of chaos (that is, asymptotic independence of the
units) this behavior is preserved through the training dynamics.

</details>


### [49] [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)
*Benjamin Holzschuh,Georg Kohl,Florian Redinger,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出一个可扩展的框架，用于学习高分辨率3D物理模拟的确定性和概率性神经代理模型，采用混合CNN-Transformer架构，在速度和准确性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决高分辨率3D物理模拟中计算成本高、内存需求大的问题，需要开发能够有效学习物理动力学并支持大规模高分辨率数据训练的神经代理模型。

Method: 引入混合CNN-Transformer主干架构，支持在小块模拟域上进行预训练，然后通过序列到序列模型融合全局解并包含长程依赖关系，还展示了作为扩散模型训练的能力。

Result: 在14种不同类型3D PDE动力学学习任务中显著优于基线方法，能够扩展到512^3空间分辨率的高分辨率各向同性湍流，并能准确捕捉不同雷诺数下高度湍流3D通道流的统计特性。

Conclusion: 该框架为高分辨率3D物理模拟提供了高效且可扩展的神经代理解决方案，在确定性和概率性建模方面都表现出色，具有很好的通用性和实用性。

Abstract: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.

</details>


### [50] [Hadamard-Riemannian Optimization for Margin-Variance Ensemble](https://arxiv.org/abs/2509.10189)
*Zexu Jin*

Main category: cs.LG

TL;DR: 提出了一种新的集成学习框架，通过同时优化负期望边际和边际方差来提升模型鲁棒性和泛化性能，并通过权重重参数化提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于边际的集成方法主要关注最大化期望边际而忽略边际方差的重要性，这限制了模型的泛化能力并增加了过拟合风险，特别是在噪声或不平衡数据集中。同时，传统方法在概率单纯形中优化集成权重存在计算效率低和可扩展性差的问题。

Method: 提出了一种新的集成学习框架，将边际方差明确纳入损失函数，联合优化负期望边际和边际方差。通过将集成权重重新参数化到单位球面上，简化了优化过程并提高了计算效率。

Result: 在多个基准数据集上的广泛实验表明，所提出的方法 consistently 优于传统的基于边际的集成技术。

Conclusion: 该方法通过同时考虑期望边际和边际方差，显著提升了集成学习的鲁棒性和泛化性能，同时通过权重重参数化解决了计算效率问题，具有很好的实用价值。

Abstract: Ensemble learning has been widely recognized as a pivotal technique for
boosting predictive performance by combining multiple base models.
Nevertheless, conventional margin-based ensemble methods predominantly focus on
maximizing the expected margin while neglecting the critical role of margin
variance, which inherently restricts the generalization capability of the model
and heightens its vulnerability to overfitting, particularly in noisy or
imbalanced datasets. Additionally, the conventional approach of optimizing
ensemble weights within the probability simplex often introduces computational
inefficiency and scalability challenges, complicating its application to
large-scale problems. To tackle these limitations, this paper introduces a
novel ensemble learning framework that explicitly incorporates margin variance
into the loss function. Our method jointly optimizes the negative expected
margin and its variance, leading to enhanced robustness and improved
generalization performance. Moreover, by reparameterizing the ensemble weights
onto the unit sphere, we substantially simplify the optimization process and
improve computational efficiency. Extensive experiments conducted on multiple
benchmark datasets demonstrate that the proposed approach consistently
outperforms traditional margin-based ensemble techniques, underscoring its
effectiveness and practical utility.

</details>


### [51] [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](https://arxiv.org/abs/2509.10227)
*Ángel Ladrón,Miguel Sánchez-Domínguez,Javier Rozalén,Fernando R. Sánchez,Javier de Vicente,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的飞机机翼疲劳寿命预测管道，通过飞行参数快速估算疲劳寿命，作为传统有限元模拟方法的补充，减少计算资源和人力需求。


<details>
  <summary>Details</summary>
Motivation: 传统疲劳寿命预测方法虽然可靠但耗时且流程复杂，需要多团队协作和大量有限元模拟。机器学习方法可以提供快速估算，指导决策并补充传统模拟方法。

Method: 开发基于机器学习的管道，根据飞机不同任务的飞行参数来估算机翼各位置的疲劳寿命，并进行统计验证和不确定性量化。

Result: 在真实疲劳寿命估算用例中验证了管道的有效性，获得了准确的预测结果。

Conclusion: 该机器学习管道能够有效补充传统方法，显著减少昂贵的模拟计算量，降低计算和人力资源需求。

Abstract: Fatigue life prediction is essential in both the design and operational
phases of any aircraft, and in this sense safety in the aerospace industry
requires early detection of fatigue cracks to prevent in-flight failures.
Robust and precise fatigue life predictors are thus essential to ensure safety.
Traditional engineering methods, while reliable, are time consuming and involve
complex workflows, including steps such as conducting several Finite Element
Method (FEM) simulations, deriving the expected loading spectrum, and applying
cycle counting techniques like peak-valley or rainflow counting. These steps
often require collaboration between multiple teams and tools, added to the
computational time and effort required to achieve fatigue life predictions.
Machine learning (ML) offers a promising complement to traditional fatigue life
estimation methods, enabling faster iterations and generalization, providing
quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the
fatigue life of different aircraft wing locations given the flight parameters
of the different missions that the aircraft will be operating throughout its
operational life. We validate the pipeline in a realistic use case of fatigue
life estimation, yielding accurate predictions alongside a thorough statistical
validation and uncertainty quantification. Our pipeline constitutes a
complement to traditional methodologies by reducing the amount of costly
simulations and, thereby, lowering the required computational and human
resources.

</details>


### [52] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: 本文通过系统评估发现，简单的提示词注入攻击对LLM同行评审高度有效（可达100%接受率），且LLM评审普遍存在接受偏向（>95%）。


<details>
  <summary>Details</summary>
Motivation: 针对科学同行评审中LLM使用日益增多以及作者使用隐藏提示词注入操纵评审分数的报道，研究此类攻击的可行性和技术成功率。

Method: 使用多种LLM对2024年ICLR论文的1000篇评审进行系统评估，分析提示词注入的有效性和评审偏向性。

Result: 1) 非常简单提示词注入高度有效，最高可达100%接受率；2) LLM评审普遍偏向接受（许多模型>95%接受率）。

Conclusion: 研究结果对LLM在同行评审中使用的持续讨论具有重大影响，揭示了系统的脆弱性和偏向性问题。

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [53] [Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning](https://arxiv.org/abs/2509.10273)
*Sahil Sethi,Kai Sundmacher,Caroline Ganzer*

Main category: cs.LG

TL;DR: 提出了一个基于神经推荐系统的迁移学习框架，利用COSMO-RS模拟数据和稀疏实验数据来准确预测离子液体的五种关键热物理性质。


<details>
  <summary>Details</summary>
Motivation: 离子液体具有可定制的物理化学性质，但由于化学设计空间巨大和实验数据有限，准确预测其热物理性质仍然具有挑战性。

Method: 采用两阶段方法：首先在固定温度压力下使用COSMO-RS模拟数据预训练神经推荐系统模型，学习阳离子和阴离子的性质特异性结构嵌入；然后使用这些嵌入和不同温度压力下的实验数据微调简单前馈神经网络。

Result: 该框架支持性质内和跨性质知识迁移，预训练的密度、粘度和热容模型用于微调所有五种目标性质模型，其中四种性质的性能显著提升。模型能够稳健地外推到未见过的离子液体，并为超过70万种离子液体组合提供性质预测。

Conclusion: 这项工作展示了结合模拟数据和迁移学习来克服实验数据稀疏性的有效性，为离子液体筛选提供了可扩展的解决方案。

Abstract: Ionic liquids (ILs) have emerged as versatile replacements for traditional
solvents because their physicochemical properties can be precisely tailored to
various applications. However, accurately predicting key thermophysical
properties remains challenging due to the vast chemical design space and the
limited availability of experimental data. In this study, we present a
data-driven transfer learning framework that leverages a neural recommender
system (NRS) to enable reliable property prediction for ILs using sparse
experimental datasets. The approach involves a two-stage process: first,
pre-training NRS models on COSMO-RS-based simulated data at fixed temperature
and pressure to learn property-specific structural embeddings for cations and
anions; and second, fine-tuning simple feedforward neural networks using these
embeddings with experimental data at varying temperatures and pressures. In
this work, five essential IL properties are considered: density, viscosity,
surface tension, heat capacity, and melting point. The framework supports both
within-property and cross-property knowledge transfer. Notably, pre-trained
models for density, viscosity, and heat capacity are used to fine-tune models
for all five target properties, achieving improved performance by a substantial
margin for four of them. The model exhibits robust extrapolation to previously
unseen ILs. Moreover, the final trained models enable property prediction for
over 700,000 IL combinations, offering a scalable solution for IL screening in
process design. This work highlights the effectiveness of combining simulated
data and transfer learning to overcome sparsity in the experimental data.

</details>


### [54] [Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case](https://arxiv.org/abs/2509.10291)
*Salih Toprak,Muge Erel-Ozcevik*

Main category: cs.LG

TL;DR: 提出一种基于SDN和AutoML的区块链能源交易架构，利用机器学习回归器生成随机数作为nonce，称为Proof of AutoML，特别适用于灾难场景下的安全能源交易。


<details>
  <summary>Details</summary>
Motivation: 在灾难场景中传统能源基础设施受损时，需要确保太阳能家庭与移动充电单元之间能源交易的安全性和可追溯性，而区块链网络需要强大的随机nonce生成机制。

Method: 采用SDN架构实现灵活的数据流和能源路由控制，使用AutoML选择的五种回归模型（梯度提升、LightGBM、随机森林、额外树和K近邻）生成随机nonce值，通过9000样本数据集评估模型的随机性而非预测精度。

Result: 随机性分析显示随机森林和额外树回归器具有完全的随机依赖性，梯度提升、K近邻和LightGBM分别达到97.6%、98.8%和99.9%的随机性得分，表明树基集成模型适合作为轻量级nonce生成器。

Conclusion: 某些机器学习模型，特别是树基集成方法，可以作为有效的轻量级nonce生成器，用于构建抗灾难的区块链安全SDN能源交易基础设施。

Abstract: In disaster scenarios where conventional energy infrastructure is
compromised, secure and traceable energy trading between solar-powered
households and mobile charging units becomes a necessity. To ensure the
integrity of such transactions over a blockchain network, robust and
unpredictable nonce generation is vital. This study proposes an SDN-enabled
architecture where machine learning regressors are leveraged not for their
accuracy, but for their potential to generate randomized values suitable as
nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN
allows flexible control over data flows and energy routing policies even in
fragmented or degraded networks, ensuring adaptive response during emergencies.
Using a 9000-sample dataset, we evaluate five AutoML-selected regression models
- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest
Neighbors - not by their prediction accuracy, but by their ability to produce
diverse and non-deterministic outputs across shuffled data inputs. Randomness
analysis reveals that Random Forest and Extra Trees regressors exhibit complete
dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and
LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and
99.9%, respectively). These findings highlight that certain machine learning
models, particularly tree-based ensembles, may serve as effective and
lightweight nonce generators within blockchain-secured, SDN-based energy
trading infrastructures resilient to disaster conditions.

</details>


### [55] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: 提出CDQAC离线强化学习算法，直接从历史数据学习作业车间调度策略，无需在线交互，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统在线RL方法需要大量模拟环境交互且样本效率低，无法捕捉真实世界复杂性，需要直接从历史数据学习调度策略的解决方案

Method: CDQAC算法结合分位数critic和延迟策略更新，估计每个机器-操作对的回报分布而非直接选择对

Result: CDQAC显著优于原始数据生成启发式算法和SOTA离线/在线RL基线，仅需10-20个训练实例即可学习高质量策略，且在随机启发式生成数据上表现更好

Conclusion: CDQAC是高效的离线RL方法，能够直接从历史数据学习优秀调度策略，在样本效率和性能方面都表现出色

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [56] [GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction](https://arxiv.org/abs/2509.10308)
*Joshua Dimasaka,Christian Geiß,Robert Muir-Wood,Emily So*

Main category: cs.LG

TL;DR: 提出了GraphCSVAE框架，通过深度学习、图表示和分类概率推理整合卫星时间序列数据与专家知识，用于建模灾害物理脆弱性


<details>
  <summary>Details</summary>
Motivation: 现有灾害风险评估主要关注灾害和暴露度建模，但在物理脆弱性建模方面进展有限，限制了决策者对联合国仙台框架进展的评估能力

Method: 使用图分类结构化变分自编码器(GraphCSVAE)，整合时间序列卫星数据和专家先验知识，引入弱监督一阶转移矩阵来反映物理脆弱性的时空分布变化

Result: 在孟加拉国飓风影响的Khurushkul社区和塞拉利昂泥石流影响的弗里敦市两个灾害频发地区成功揭示了灾后物理脆弱性的区域动态

Conclusion: 该工作为局部时空审计和灾后风险减少的可持续策略提供了有价值的见解，推动了物理脆弱性建模的发展

Abstract: In the aftermath of disasters, many institutions worldwide face challenges in
continually monitoring changes in disaster risk, limiting the ability of key
decision-makers to assess progress towards the UN Sendai Framework for Disaster
Risk Reduction 2015-2030. While numerous efforts have substantially advanced
the large-scale modeling of hazard and exposure through Earth observation and
data-driven methods, progress remains limited in modeling another equally
important yet challenging element of the risk equation: physical vulnerability.
To address this gap, we introduce Graph Categorical Structured Variational
Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for
modeling physical vulnerability by integrating deep learning, graph
representation, and categorical probabilistic inference, using time-series
satellite-derived datasets and prior expert belief systems. We introduce a
weakly supervised first-order transition matrix that reflects the changes in
the spatiotemporal distribution of physical vulnerability in two
disaster-stricken and socioeconomically disadvantaged areas: (1) the
cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the
mudslide-affected city of Freetown in Sierra Leone. Our work reveals
post-disaster regional dynamics in physical vulnerability, offering valuable
insights into localized spatiotemporal auditing and sustainable strategies for
post-disaster risk reduction.

</details>


### [57] [ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting](https://arxiv.org/abs/2509.10324)
*Myung Jin Kim,YeongHyeon Park,Il Dong Yun*

Main category: cs.LG

TL;DR: 基于ARIMA模型灵感的简单卷积模块，直接进行多步时间序列预测，在保持简洁结构的同时具有竞争性的准确性


<details>
  <summary>Details</summary>
Motivation: 传统ARIMA模型需要迭代多步预测，不易扩展到多变量设置，需要一种简单有效的方法来处理长期时间序列预测

Method: 设计了受ARIMA启发的卷积模块，包含两个卷积组件：一个捕捉趋势（自回归），另一个精炼局部变化（移动平均），直接进行多步预测

Result: 在9个广泛使用的标准数据集上识别具有竞争性的准确性，尤其是在显示强烈趋势变化的数据集上

Conclusion: 该模块不仅提供了高效的长期预测能力，而且内在编码了绝对位置信息，有潜力作为轻量级的位置嵌入替代方案用于序列模型

Abstract: This paper proposes a simple yet effective convolutional module for long-term
time series forecasting. The proposed block, inspired by the Auto-Regressive
Integrated Moving Average (ARIMA) model, consists of two convolutional
components: one for capturing the trend (autoregression) and the other for
refining local variations (moving average). Unlike conventional ARIMA, which
requires iterative multi-step forecasting, the block directly performs
multi-step forecasting, making it easily extendable to multivariate settings.
Experiments on nine widely used benchmark datasets demonstrate that our method
ARMA achieves competitive accuracy, particularly on datasets exhibiting strong
trend variations, while maintaining architectural simplicity. Furthermore,
analysis shows that the block inherently encodes absolute positional
information, suggesting its potential as a lightweight replacement for
positional embeddings in sequential models.

</details>


### [58] [Physics-informed sensor coverage through structure preserving machine learning](https://arxiv.org/abs/2509.10363)
*Benjamin David Shaffer,Brooks Kinch,Joseph Klobusicky,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出基于条件神经Whitney形式的数字孪生框架，用于自适应源定位，结合有限元外微积分和Transformer算子学习，保持离散守恒性并实时适应传感器数据。


<details>
  <summary>Details</summary>
Motivation: 解决复杂流体输运系统中源定位问题，传统方法难以在保持物理约束的同时实现实时数据同化和轨迹规划。

Method: 使用条件神经Whitney形式(CNWF)构建数字孪生，结合FEEC数值保证和Transformer算子学习，采用交错方案交替评估数字孪生和应用Lloyd算法指导传感器部署。

Result: 实验显示在复杂几何中比物理无关的Transformer架构精度更高，结构保持为源识别提供了有效的归纳偏置。

Conclusion: 结构保持的数字孪生框架能够有效实现自适应源定位，物理约束的强制执行提高了在复杂环境中的定位准确性。

Abstract: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.

</details>


### [59] [A Discrepancy-Based Perspective on Dataset Condensation](https://arxiv.org/abs/2509.10367)
*Tong Chen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 提出了一个统一的框架来形式化数据集压缩问题，使用差异度量来量化概率分布之间的距离，扩展了传统压缩目标到鲁棒性、隐私性等更多属性


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法缺乏统一的理论框架，且主要关注泛化性能，需要扩展到更广泛的应用场景和性能指标

Method: 建立基于差异度量的统一理论框架，将数据集压缩形式化为概率分布的近似问题，支持多种性能目标的优化

Result: 提出了一个能够涵盖现有数据集压缩方法的统一框架，并将压缩目标从泛化性能扩展到鲁棒性、隐私性等多个维度

Conclusion: 该框架为数据集压缩提供了更严格的形式化定义和理论基础，使其能够适应更广泛的应用需求

Abstract: Given a dataset of finitely many elements $\mathcal{T} = \{\mathbf{x}_i\}_{i
= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic
dataset $\mathcal{S} = \{\tilde{\mathbf{x}}_j\}_{j = 1}^M$ which is
significantly smaller ($M \ll N$) such that a model trained from scratch on
$\mathcal{S}$ achieves comparable or even superior generalization performance
to a model trained on $\mathcal{T}$. Recent advances in DC reveal a close
connection to the problem of approximating the data distribution represented by
$\mathcal{T}$ with a reduced set of points. In this work, we present a unified
framework that encompasses existing DC methods and extend the task-specific
notion of DC to a more general and formal definition using notions of
discrepancy, which quantify the distance between probability distribution in
different regimes. Our framework broadens the objective of DC beyond
generalization, accommodating additional objectives such as robustness,
privacy, and other desirable properties.

</details>


### [60] [Vendi Information Gain for Active Learning and its Application to Ecology](https://arxiv.org/abs/2509.10390)
*Quan Nguyen,Adji Bousso Dieng*

Main category: cs.LG

TL;DR: 提出Vendi信息增益(VIG)主动学习策略，通过考虑数据集整体预测不确定性来选择图像，在Snapshot Serengeti数据集上仅用不到10%的标签就达到了接近全监督的预测精度。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱监测生物多样性时，物种识别因标注资源有限成为主要瓶颈。传统主动学习只关注个体预测不确定性，忽略了整个数据集的不确定性。

Method: 提出Vendi信息增益(VIG)主动学习策略，基于图像对整个数据集预测不确定性的影响来选择样本，同时考虑信息量和多样性。

Result: 在Snapshot Serengeti数据集上，VIG使用不到10%的标签就达到了接近全监督的预测精度，在所有指标和批次大小上都优于标准基线方法，并在特征空间中收集了更多样化的数据。

Conclusion: VIG方法在数据有限的环境中具有广泛适用性，对生物多样性监测具有重要价值，其考虑数据集整体不确定性的思路可推广到其他领域。

Abstract: While monitoring biodiversity through camera traps has become an important
endeavor for ecological research, identifying species in the captured image
data remains a major bottleneck due to limited labeling resources. Active
learning -- a machine learning paradigm that selects the most informative data
to label and train a predictive model -- offers a promising solution, but
typically focuses on uncertainty in the individual predictions without
considering uncertainty across the entire dataset. We introduce a new active
learning policy, Vendi information gain (VIG), that selects images based on
their impact on dataset-wide prediction uncertainty, capturing both
informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG
achieves impressive predictive accuracy close to full supervision using less
than 10% of the labels. It consistently outperforms standard baselines across
metrics and batch sizes, collecting more diverse data in the feature space. VIG
has broad applicability beyond ecology, and our results highlight its value for
biodiversity monitoring in data-limited environments.

</details>


### [61] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: 提出了IGPO（Inpainting Guided Policy Optimization）框架，利用掩码扩散LLM的修复能力指导强化学习探索，在数学推理任务上取得SOTA结果


<details>
  <summary>Details</summary>
Motivation: 解决传统RL在LLM对齐中的探索挑战：稀疏奖励信号和样本浪费问题，利用dLLM独特的修复能力来引导探索

Method: IGPO框架在在线采样时策略性插入部分真实推理轨迹，结合监督微调使用合成重写的简洁轨迹，并采用基于熵的过滤等技术

Result: 在GSM8K、Math500和AMC三个数学基准测试中取得显著提升，为全注意力掩码dLLM实现了新的最先进结果

Conclusion: dLLM的修复能力为RL算法设计提供了新的机会，IGPO框架有效解决了探索效率问题，在数学推理任务上表现出色

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


### [62] [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)
*Rupert Mitchell,Kristian Kersting*

Main category: cs.LG

TL;DR: MuSe是一种高效的softmax注意力近似方法，通过语义聚类和计算物理中的多极展开来降低Transformer的二次计算复杂度，实现3倍加速且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在处理长序列时注意力机制的二次计算复杂度问题，传统方法要么只聚类键值要么使用统一聚类，未能充分考虑注意力机制中查询和键的非对称性。

Method: 在学习的表示空间中分别对查询和键进行语义聚类，采用分层两阶段注意力机制，使用质心近似（单极子）加上捕捉簇内方向方差的偶极子修正，可作为标准注意力的即插即用替代方案。

Result: 在8k上下文长度下比CUDNN Flash Attention快3倍，相对平方误差低于20%；在16k上下文的30M参数模型预训练中实现12.2%运行时间减少，仅0.36%性能损失。

Conclusion: 多极子近似为高效Transformer预训练提供了可行方案，分离聚类和偶极子修正能有效保持训练过程中的丰富信息，复杂度降低为O(NCD)。

Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.

</details>


### [63] [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](https://arxiv.org/abs/2509.10419)
*Francesco Vitale,Tommaso Zoppi,Francesco Flammini,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 使用过程挖掘技术对ERTMS/ETCS L2铁路系统进行运行时控制流异常检测和定位，通过在线一致性检查和无监督机器学习提高系统韧性。


<details>
  <summary>Details</summary>
Motivation: 随着铁路系统复杂性和关键性增加，尽管有严格的验证和认证流程，运行时仍可能出现异常。需要增强系统韧性来应对设计时未知的残余故障、系统环境变化和新兴网络威胁。

Method: 采用过程挖掘技术从执行轨迹中学习系统实际控制流，进行在线一致性检查实现运行时监控，并使用无监督机器学习进行异常定位，将偏差关联到关键系统组件。

Result: 在ERTMS/ETCS L2的RBC/RBC切换参考场景中测试，该方法能够以高准确性、高效性和可解释性检测和定位异常。

Conclusion: 过程挖掘结合无监督机器学习为铁路控制系统提供了一种有效的运行时异常检测和定位方法，能够增强系统韧性应对未知威胁和变化。

Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to
account for uncertainties and changes due to the growing complexity and
criticality of those systems. Although their software relies on strict
verification and validation processes following well-established best-practices
and certification standards, anomalies can still occur at run-time due to
residual faults, system and environmental modifications that were unknown at
design-time, or other emergent cyber-threat scenarios. This paper explores
run-time control-flow anomaly detection using process mining to enhance the
resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European
Train Control System Level 2). Process mining allows learning the actual
control flow of the system from its execution traces, thus enabling run-time
monitoring through online conformance checking. In addition, anomaly
localization is performed through unsupervised machine learning to link
relevant deviations to critical system components. We test our approach on a
reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its
capability to detect and localize anomalies with high accuracy, efficiency, and
explainability.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [64] [An Information-Theoretic Framework for Credit Risk Modeling: Unifying Industry Practice with Statistical Theory for Fair and Interpretable Scorecards](https://arxiv.org/abs/2509.09855)
*Agus Sudjianto,Denis Burakov*

Main category: stat.ML

TL;DR: 本文建立了一个统一的信息论框架，将信用风险建模中的WoE、IV和PSI标准指标与经典信息散度联系起来，并首次推导出统计误差和假设检验方法，为平衡预测性能与公平性提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 信用风险建模广泛使用WoE、IV和PSI等指标，但这些指标的理论基础相互脱节，缺乏统一的统计框架和正式的假设检验方法。

Method: 通过delta方法应用于WoE变换，推导IV和PSI的标准误差；使用深度为1的XGBoost树桩进行自动分箱；比较三种编码策略并采用混合整数规划寻找帕累托最优解。

Result: 所有方法都达到了相当的预测性能（AUC 0.82-0.84），证明了基于信息论的分箱方法比编码选择更重要；首次实现了IV和PSI的统计显著性检验和概率公平约束。

Conclusion: 该框架为广泛使用的信用风险指标提供了首个严格的统计基础，为受监管环境中平衡准确性和公平性提供了原则性工具，连接了理论与实践。

Abstract: Credit risk modeling relies extensively on Weight of Evidence (WoE) and
Information Value (IV) for feature engineering, and Population Stability Index
(PSI) for drift monitoring, yet their theoretical foundations remain
disconnected. We establish a unified information-theoretic framework revealing
these industry-standard metrics as instances of classical information
divergences. Specifically, we prove that IV exactly equals PSI (Jeffreys
divergence) computed between good and bad credit outcomes over identical bins.
Through the delta method applied to WoE transformations, we derive standard
errors for IV and PSI, enabling formal hypothesis testing and probabilistic
fairness constraints for the first time. We formalize credit modeling's
inherent performance-fairness trade-off as maximizing IV for predictive power
while minimizing IV for protected attributes. Using automated binning with
depth-1 XGBoost stumps, we compare three encoding strategies: logistic
regression with one-hot encoding, WoE transformation, and constrained XGBoost.
All methods achieve comparable predictive performance (AUC 0.82-0.84),
demonstrating that principled, information-theoretic binning outweighs encoding
choice. Mixed-integer programming traces Pareto-efficient solutions along the
performance-fairness frontier with uncertainty quantification. This framework
bridges theory and practice, providing the first rigorous statistical
foundation for widely-used credit risk metrics while offering principled tools
for balancing accuracy and fairness in regulated environments.

</details>


### [65] [Repulsive Monte Carlo on the sphere for the sliced Wasserstein distance](https://arxiv.org/abs/2509.10166)
*Vladimir Petrovic,Rémi Bardenet,Agnès Desolneux*

Main category: stat.ML

TL;DR: 本文研究使用蒙特卡洛方法计算高维球面积分的问题，特别关注切片Wasserstein距离的计算。通过分析各种排斥性节点积分方法，推荐在低维使用随机拟蒙特卡洛，高维使用UnifOrtho估计器。


<details>
  <summary>Details</summary>
Motivation: 切片Wasserstein距离作为Wasserstein距离的替代或独立距离度量，在机器学习中日益重要，但需要高效的积分计算方法。现有数值基准主要关注一般积分，而本文专注于使用排斥性节点（负相关）的积分方法，因为这种负相关性可以带来方差减少。

Method: 1) 从确定性点过程和排斥点过程文献中提取和激励积分方法；2) 从切片Wasserstein距离特定文献中获取排斥性积分方法；3) 对这些积分方法进行数值基准测试；4) 分析UnifOrtho估计器的方差特性。

Result: 分析揭示了UnifOrtho估计器在高维切片Wasserstein距离估计中成功的原因，以及文献中反例的解释。DPP基积分方法仅在拟蒙特卡洛有效时表现良好，而排斥性积分方法总体上显示中等程度的方差减少。

Conclusion: 推荐计算切片Wasserstein距离时，低维使用随机拟蒙特卡洛，高维使用UnifOrtho方法。DPP基积分方法适用范围有限，排斥性积分方法需要更多理论工作来提高鲁棒性。

Abstract: In this paper, we consider the problem of computing the integral of a
function on the unit sphere, in any dimension, using Monte Carlo methods.
Although the methods we present are general, our guiding thread is the sliced
Wasserstein distance between two measures on $\mathbb{R}^d$, which is precisely
an integral on the $d$-dimensional sphere. The sliced Wasserstein distance (SW)
has gained momentum in machine learning either as a proxy to the less
computationally tractable Wasserstein distance, or as a distance in its own
right, due in particular to its built-in alleviation of the curse of
dimensionality. There has been recent numerical benchmarks of quadratures for
the sliced Wasserstein, and our viewpoint differs in that we concentrate on
quadratures where the nodes are repulsive, i.e. negatively dependent. Indeed,
negative dependence can bring variance reduction when the quadrature is adapted
to the integration task. Our first contribution is to extract and motivate
quadratures from the recent literature on determinantal point processes (DPPs)
and repelled point processes, as well as repulsive quadratures from the
literature specific to the sliced Wasserstein distance. We then numerically
benchmark these quadratures. Moreover, we analyze the variance of the UnifOrtho
estimator, an orthogonal Monte Carlo estimator. Our analysis sheds light on
UnifOrtho's success for the estimation of the sliced Wasserstein in large
dimensions, as well as counterexamples from the literature. Our final
recommendation for the computation of the sliced Wasserstein distance is to use
randomized quasi-Monte Carlo in low dimensions and \emph{UnifOrtho} in large
dimensions. DPP-based quadratures only shine when quasi-Monte Carlo also does,
while repelled quadratures show moderate variance reduction in general, but
more theoretical effort is needed to make them robust.

</details>


### [66] [Why does your graph neural network fail on some graphs? Insights from exact generalisation error](https://arxiv.org/abs/2509.10337)
*Nil Ayday,Mahalakshmi Sabanayagam,Debarghya Ghoshdastidar*

Main category: stat.ML

TL;DR: 该论文从信号处理角度推导了图神经网络在转导固定设计设置下的精确泛化误差，揭示了只有节点特征与图结构对齐的信息才能促进泛化，并量化了同配性对泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究对GNN成功或失败的理论理解不足，虽然已有工作分析了架构限制如过平滑和过压缩，但无法解释GNN如何提取有意义的表示或为何相似架构性能差异巨大。现有泛化误差边界通常松散且局限于单一架构，缺乏实际指导意义。

Method: 采用信号处理视角，将GNN解释为图滤波器算子，通过图结构作用于节点特征。专注于线性GNN同时允许图滤波器中的非线性，推导了包括卷积、PageRank和注意力模型在内的广泛GNN的精确泛化误差。

Result: 精确泛化误差表征显示只有节点特征与图结构对齐的信息对泛化有贡献。量化了同配性对泛化的影响，提供了理解GNN何时以及为何能有效利用结构和特征信息的框架。

Conclusion: 该工作为GNN模型选择提供了实用指导，解释了GNN利用结构和特征信息的能力条件，建立了理论框架来理解GNN的泛化行为。

Abstract: Graph Neural Networks (GNNs) are widely used in learning on graph-structured
data, yet a principled understanding of why they succeed or fail remains
elusive. While prior works have examined architectural limitations such as
over-smoothing and over-squashing, these do not explain what enables GNNs to
extract meaningful representations or why performance varies drastically
between similar architectures. These questions are related to the role of
generalisation: the ability of a model to make accurate predictions on
unlabelled data. Although several works have derived generalisation error
bounds for GNNs, these are typically loose, restricted to a single
architecture, and offer limited insight into what governs generalisation in
practice. In this work, we take a different approach by deriving the exact
generalisation error for GNNs in a transductive fixed-design setting through
the lens of signal processing. From this viewpoint, GNNs can be interpreted as
graph filter operators that act on node features via the graph structure. By
focusing on linear GNNs while allowing non-linearity in the graph filters, we
derive the first exact generalisation error for a broad range of GNNs,
including convolutional, PageRank-based, and attention-based models. The exact
characterisation of the generalisation error reveals that only the aligned
information between node features and graph structure contributes to
generalisation. Furthermore, we quantify the effect of homophily on
generalisation. Our work provides a framework that explains when and why GNNs
can effectively leverage structural and feature information, offering practical
guidance for model selection.

</details>


### [67] [Differentially Private Decentralized Dataset Synthesis Through Randomized Mixing with Correlated Noise](https://arxiv.org/abs/2509.10385)
*Utsab Saha,Tanvir Muntakim Tonoy,Hafiz Imtiaz*

Main category: stat.ML

TL;DR: 提出CAPE辅助的联邦DP-CDA算法，通过在联邦学习环境中集成CAPE协议来改善差分隐私合成数据生成的隐私-效用权衡，使分布式设置下的效用接近集中式性能。


<details>
  <summary>Details</summary>
Motivation: 在去中心化数据设置中，DP-CDA面临样本量有限导致局部计算敏感性增加的问题，需要注入更多噪声来保证差分隐私，从而导致效用显著下降。

Method: 将CAPE协议集成到联邦DP-CDA框架中，允许客户端生成联合分布的反相关噪声，在聚合时相互抵消，同时在个体层面保持隐私。

Result: 在MNIST和FashionMNIST数据集上的实验表明，该方法在某些参数范围内可以达到与集中式对应方法相当的效用，同时保持严格的差分隐私保证。

Conclusion: CAPE辅助的联邦DP-CDA算法显著改善了联邦设置中的隐私-效用权衡，使分布式差分隐私合成数据生成能够接近集中式性能。

Abstract: In this work, we explore differentially private synthetic data generation in
a decentralized-data setting by building on the recently proposed
Differentially Private Class-Centric Data Aggregation (DP-CDA). DP-CDA
synthesizes data in a centralized setting by mixing multiple randomly-selected
samples from the same class and injecting carefully calibrated Gaussian noise,
ensuring ({\epsilon}, {\delta})-differential privacy. When deployed in a
decentralized or federated setting, where each client holds only a small
partition of the data, DP-CDA faces new challenges. The limited sample size per
client increases the sensitivity of local computations, requiring higher noise
injection to maintain the differential privacy guarantee. This, in turn, leads
to a noticeable degradation in the utility compared to the centralized setting.
To mitigate this issue, we integrate the Correlation-Assisted Private
Estimation (CAPE) protocol into the federated DP-CDA framework and propose CAPE
Assisted Federated DP-CDA algorithm. CAPE enables limited collaboration among
the clients by allowing them to generate jointly distributed (anti-correlated)
noise that cancels out in aggregate, while preserving privacy at the individual
level. This technique significantly improves the privacy-utility trade-off in
the federated setting. Extensive experiments on MNIST and FashionMNIST datasets
demonstrate that the proposed CAPE Assisted Federated DP-CDA approach can
achieve utility comparable to its centralized counterpart under some parameter
regime, while maintaining rigorous differential privacy guarantees.

</details>
