<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 14]
- [cs.LG](#cs.LG) [Total: 87]
- [stat.ML](#stat.ML) [Total: 3]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Compensation of Coarse Quantization Effects on Channel Estimation and BER in Massive MIMO](https://arxiv.org/abs/2512.14893)
*Reza Mohammadkhani,Azad Azizzadeh,Seyed Vahab Al-Din Makki,John Thompson,Maziar Nekovee*

Main category: eess.SP

TL;DR: 论文提出了一种考虑低分辨率量化噪声对信道估计和数据传输影响的MIMO系统性能分析框架，并基于此开发了联合优化量化分辨率、发射功率和导频长度的补偿策略。


<details>
  <summary>Details</summary>
Motivation: 现有大多数研究假设完美信道状态信息，但实际大规模MIMO系统中低分辨率量化会引入噪声，影响信道估计和数据传输性能。需要更真实地评估在非完美CSI条件下的系统性能。

Method: 基于线性最小均方误差信道估计，开发了零迫检测下未编码M-QAM的误码率紧近似表达式。该分析框架考虑了量化噪声对信道估计和数据传输的联合影响。

Result: 提出的框架能够实现量化分辨率、发射功率和导频长度的联合优化。例如在16-QAM系统中，将导频序列延长2.5倍并降低发射功率0.5dB，可使3比特量化系统达到全分辨率情况下的误码率性能。

Conclusion: 该框架为蒙特卡洛仿真提供了快速准确的替代方案，能够在实际量化约束下实现实用的系统优化，提高能量效率，适用于5G/6G大规模MIMO系统设计。

Abstract: Low-resolution quantization is essential to reduce implementation cost and power consumption in massive multiple-input multiple-output (MIMO) systems for 5G and 6G. While most existing studies assume perfect channel state information (CSI), we model the impact of coarse quantization noise on both channel estimation and data transmission, yielding a more realistic assessment of system performance under imperfect CSI conditions in the uplink. We develop a tight approximation for the bit-error ratio (BER) of uncoded M-QAM with zero-forcing detection, based on the linear minimum mean-square error (LMMSE) channel estimate. These analytical results enable compensation strategies that jointly optimize quantization resolution, transmit power, and pilot length across different numbers of users and base station antennas. We further demonstrate the applicability of the proposed framework through several design scenarios that highlight its effectiveness in optimizing system parameters and improving energy efficiency under quantization constraints. For example, in a 16-QAM system, extending the pilot sequence by 2.5 times and lowering transmit power by 0.5 dB enables a 3-bit quantized system to match the BER of the full-resolution case. The proposed framework offers a fast and accurate alternative to Monte Carlo simulations, enabling practical system optimization under realistic quantization constraints.

</details>


### [2] [Janus Metasurface Breaking Polarization Symmetry: Surface-Modulated Electromagnetic Wave Radiation with Coexistent Linear and Circular Polarization](https://arxiv.org/abs/2512.15045)
*Aparna Parameswaran,Hoyoung Kim,Sangkil Kim*

Main category: eess.SP

TL;DR: 提出一种基于Janus超表面的张量阻抗全息天线，能够从单个孔径和单馈源同时辐射线极化和圆极化波束，具有宽带工作特性和低交叉极化。


<details>
  <summary>Details</summary>
Motivation: 为了开发一种能够同时支持线极化和圆极化辐射的单孔径天线，满足先进宽带通信应用中对多功能、紧凑型天线的需求。

Method: 提出改进的张量阻抗方程来显著降低高辐射角度的交叉极化，使用孔径场积分理论验证设计方法，确保阻抗分布产生所需的远场辐射方向图。

Result: 实现了0.5 GHz的宽带工作带宽，同时保持高圆极化纯度，制作了三种全息天线变体的原型验证了性能。

Conclusion: 该天线具有优异的辐射特性，是先进宽带通信应用的有吸引力的选择。

Abstract: In this work, a Janus metasurface based tensor impedance holographic antenna (JHA) is proposed that simultaneously radiates linearly polarized (LP) and circularly polarized (CP) beams from a single aperture excited by a single feed. The proposed design introduces modified tensor impedance equations to significantly reduce cross-polarization at higher radiation angles. It demonstrates broadband operation bandwidth of 0.5 GHz while maintaining high circular polarization purity. The design methodology is verified using aperture field integration theory, ensuring that the impedance distribution produces the desired far-field radiation patterns. Prototypes of three variations of the holographic antenna are fabricated, validating its performance. The radiation characteristics of the proposed antenna make it an attractive choice for advanced broadband communication applications.

</details>


### [3] [Deep Reinforcement Learning for Joint Time and Power Management in SWIPT-EH CIoT](https://arxiv.org/abs/2512.15062)
*Nadia Abdolkhani,Nada Abdel Khalek,Walaa Hamouda,Iyad Dayoub*

Main category: eess.SP

TL;DR: 提出一种基于深度强化学习的认知物联网系统联合时间分配与功率控制方法，采用DDQN增强算法优化能量收集与传输性能


<details>
  <summary>Details</summary>
Motivation: 在认知物联网系统中，需要同时进行无线信息和能量传输，传统方法难以在动态信道条件下有效协调时间分配和功率控制，影响系统吞吐量和寿命

Method: 将联合优化问题建模为马尔可夫决策过程，考虑小尺度衰落、实际能量收集和干扰约束，开发基于上置信界增强的双深度Q网络算法

Result: 仿真结果表明，该方法在性能上优于现有的深度强化学习方法，能够有效提升系统吞吐量和寿命

Conclusion: 提出的深度强化学习方法能够有效解决认知物联网系统中的联合时间分配与功率控制问题，为实际应用提供了可行的解决方案

Abstract: This letter presents a novel deep reinforcement learning (DRL) approach for joint time allocation and power control in a cognitive Internet of Things (CIoT) system with simultaneous wireless information and power transfer (SWIPT). The CIoT transmitter autonomously manages energy harvesting (EH) and transmissions using a learnable time switching factor while optimizing power to enhance throughput and lifetime. The joint optimization is modeled as a Markov decision process under small-scale fading, realistic EH, and interference constraints. We develop a double deep Q-network (DDQN) enhanced with an upper confidence bound. Simulations benchmark our approach, showing superior performance over existing DRL methods.

</details>


### [4] [CF-Net: A Cross-Feature Reconstruction Network for High-Accuracy 1-Bit Target Classification](https://arxiv.org/abs/2512.15105)
*Jundong Qi,Weize Sun,Shaowu Chen,Lei Huang,Qiuchen Liu*

Main category: eess.SP

TL;DR: 提出CF-Net两阶段深度学习框架，通过自监督预训练从1位雷达数据恢复16位图像特征，再微调用于目标分类，在相同采样率下实现与16位方法相当的分类精度。


<details>
  <summary>Details</summary>
Motivation: 1位量化雷达信号具有高频直接采样优势，但极端量化导致信息严重损失。传统过采样补偿方法在高频下不实用，需要在不增加采样率的情况下从1位数据实现高精度分类。

Method: 提出CF-Net两阶段框架：1) 自监督预训练阶段使用双分支U-Net架构，通过交叉特征重建任务学习从1位图像恢复16位图像；2) 微调阶段将预训练的1位编码器重新用于多类目标分类任务。

Result: 在两个雷达目标数据集上的实验表明，CF-Net能够有效从1位图像中提取判别性特征，在不进行过采样的条件下，实现了与某些16位方法相当甚至更优的分类精度。

Conclusion: CF-Net框架成功解决了1位量化雷达数据的信息损失问题，通过自监督特征学习实现了高效的目标分类，为高频雷达系统的简化设计提供了可行方案。

Abstract: Target classification is a fundamental task in radar systems, and its performance critically depends on the quantization precision of the signal. While high-precision quantization (e.g. 16-bit) is well established, 1-bit quantization offers distinct advantages by enabling direct sampling at high frequencies and eliminating complex intermediate stages. However, its extreme quantization leads to significant information loss. Although higher sampling rates can compensate for this loss, such oversampling is impractical at the high frequencies targeted for direct sampling. To achieve high-accuracy classification directly from 1-bit radar data under the same sampling rate, this paper proposes a novel two-stage deep learning framework, CF-Net. First, we introduce a self-supervised pre-training strategy based on a dual-branch U-Net architecture. This network learns to restore high-fidelity 16-bit images from their 1-bit counterparts via a cross-feature reconstruction task, forcing the 1-bit encoder to learn robust features despite extreme quantization. Subsequently, this pre-trained encoder is repurposed and fine-tuned for the downstream multi-class target classification task. Experiments on two radar target datasets demonstrate that CF-Net can effectively extract discriminative features from 1-bit imagery, achieving comparable and even superior accuracy to some 16-bit methods without oversampling.

</details>


### [5] [Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network](https://arxiv.org/abs/2512.15109)
*Zhuoran Li,Zhen Gao,Xinhua Liu,Zheng Wang,Xiaotian Zhou,Lei Liu,Yongpeng Wu,Wei Feng,Yongming Huang*

Main category: eess.SP

TL;DR: 本文提出利用大型人工智能模型将基站转变为智能基站代理，实现感知、通信和计算的融合，为6G系统提供安全关键应用支持。


<details>
  <summary>Details</summary>
Motivation: 6G时代需要将智能置于无线架构核心，融合感知、通信和计算。传统基站功能单一，无法满足未来智能系统的需求，需要将其升级为具备感知、推理和执行能力的智能代理。

Method: 提出智能基站代理架构，结合感知-认知-执行流程与云边端协同，采用参数高效适配。研究两个代表性场景：协同车路感知和低空无人机安全监控。分析关键技术包括LAM设计训练、高效边缘云推理、多模态感知执行、可信安全治理。

Result: 建立了智能基站代理的理论框架和架构设计，提出了包含通信性能、感知精度、决策可靠性、安全性和能效的评估框架，为6G安全关键系统提供了实践路径。

Conclusion: 基于大型AI模型的智能基站代理是实现6G感知-通信-计算融合系统的可行路径，但仍需解决基准测试、持续适配、可信决策和标准化等开放挑战。

Abstract: The advent of sixth-generation (6G) places intelligence at the core of wireless architecture, fusing perception, communication, and computation into a single closed-loop. This paper argues that large artificial intelligence models (LAMs) can endow base stations with perception, reasoning, and acting capabilities, thus transforming them into intelligent base station agents (IBSAs). We first review the historical evolution of BSs from single-functional analog infrastructure to distributed, software-defined, and finally LAM-empowered IBSA, highlighting the accompanying changes in architecture, hardware platforms, and deployment. We then present an IBSA architecture that couples a perception-cognition-execution pipeline with cloud-edge-end collaboration and parameter-efficient adaptation. Subsequently,we study two representative scenarios: (i) cooperative vehicle-road perception for autonomous driving, and (ii) ubiquitous base station support for low-altitude uncrewed aerial vehicle safety monitoring and response against unauthorized drones. On this basis, we analyze key enabling technologies spanning LAM design and training, efficient edge-cloud inference, multi-modal perception and actuation, as well as trustworthy security and governance. We further propose a holistic evaluation framework and benchmark considerations that jointly cover communication performance, perception accuracy, decision-making reliability, safety, and energy efficiency. Finally, we distill open challenges on benchmarks, continual adaptation, trustworthy decision-making, and standardization. Together, this work positions LAM-enabled IBSAs as a practical path toward integrated perception, communication, and computation native, safety-critical 6G systems.

</details>


### [6] [QoS-Aware Hierarchical Reinforcement Learning for Joint Link Selection and Trajectory Optimization in SAGIN-Supported UAV Mobility Management](https://arxiv.org/abs/2512.15119)
*Jiayang Wan,Ke He,Yafei Wang,Fan Liu,Wenjin Wang,Shi Jin*

Main category: eess.SP

TL;DR: 本文提出了一种基于分层深度强化学习的无人机移动管理框架，用于解决空天地一体化网络中异构网络覆盖差异问题，通过离散链路选择和连续轨迹优化的联合优化，显著提升了吞吐量、链路切换频率和服务质量满意度。


<details>
  <summary>Details</summary>
Motivation: 无人机在三维空间中的高度和水平移动变化显著，单一网络难以保证连续可靠的三维覆盖。空天地一体化网络（SAGIN）成为实现无人机无处不在连接的关键架构，但异构网络在覆盖范围和信号特性方面存在显著差异，需要有效的移动管理方案。

Method: 1. 将无人机移动管理建模为约束多目标联合优化问题，耦合离散链路选择和连续轨迹优化
2. 提出两级多智能体分层深度强化学习框架，将问题分解为两个可交替求解的子问题
3. 顶层采用双深度Q网络处理离散链路选择决策
4. 底层结合软演员-评论家算法的最大熵机制，采用基于拉格朗日的约束SAC算法处理连续轨迹动作空间
5. 在集中训练分散执行范式下扩展到多无人机场景

Result: 仿真结果表明，所提方案在吞吐量、链路切换频率和服务质量满意度方面显著优于现有基准方法。算法能够实现稳定高质量的策略学习，动态调整拉格朗日乘子以平衡约束满足和策略优化。

Conclusion: 提出的分层深度强化学习框架有效解决了空天地一体化网络中无人机移动管理的复杂优化问题，通过离散-连续动作空间的协同优化和多智能体扩展，为异构网络环境下的无人机连接提供了高效可靠的解决方案。

Abstract: Due to the significant variations in unmanned aerial vehicle (UAV) altitude and horizontal mobility, it becomes difficult for any single network to ensure continuous and reliable threedimensional coverage. Towards that end, the space-air-ground integrated network (SAGIN) has emerged as an essential architecture for enabling ubiquitous UAV connectivity. To address the pronounced disparities in coverage and signal characteristics across heterogeneous networks, this paper formulates UAV mobility management in SAGIN as a constrained multi-objective joint optimization problem. The formulation couples discrete link selection with continuous trajectory optimization. Building on this, we propose a two-level multi-agent hierarchical deep reinforcement learning (HDRL) framework that decomposes the problem into two alternately solvable subproblems. To map complex link selection decisions into a compact discrete action space, we conceive a double deep Q-network (DDQN) algorithm in the top-level, which achieves stable and high-quality policy learning through double Q-value estimation. To handle the continuous trajectory action space while satisfying quality of service (QoS) constraints, we integrate the maximum-entropy mechanism of the soft actor-critic (SAC) and employ a Lagrangian-based constrained SAC (CSAC) algorithm in the lower-level that dynamically adjusts the Lagrange multipliers to balance constraint satisfaction and policy optimization. Moreover, the proposed algorithm can be extended to multi-UAV scenarios under the centralized training and decentralized execution (CTDE) paradigm, which enables more generalizable policies. Simulation results demonstrate that the proposed scheme substantially outperforms existing benchmarks in throughput, link switching frequency and QoS satisfaction.

</details>


### [7] [Enhancing Alzheimer's Detection through Late Fusion of Multi-Modal EEG Features](https://arxiv.org/abs/2512.15246)
*Nguyen Thanh Vinh,Manoj Vishwanath,Thinh Nguyen-Quang,Nguyen Viet Ha,Bui Thanh Tung,Huy-Dung Han,Nguyen Quang Linh,Nguyen Hai Linh,Hung Cao*

Main category: eess.SP

TL;DR: 本文提出了一种基于脑电图信号的深度学习框架，通过融合多种特征提取技术（α波分析、离散小波变换和马尔可夫转移场）来诊断阿尔茨海默病，实现了87.23%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病是一种进行性神经退行性疾病，早期检测对于及时干预和改善患者预后至关重要。传统诊断方法耗时且需要专家解读，因此自动化方法具有高度需求。

Method: 提出了一种新颖的深度学习框架，整合了多种特征提取技术：α波分析、离散小波变换和马尔可夫转移场。采用后期融合策略将基于这些不同表示的单独神经网络预测结果进行组合，捕捉脑电图数据中的时域和频域模式。通过严格的预处理和针对性的频带选择（特别是与认知相关的α波段）来增强性能。

Result: 在公开数据集上评估，该模型获得了87.23%的分类准确率，精确率为87.95%，召回率为86.91%，F1分数为87.42%。

Conclusion: 这项工作凸显了深度学习在支持医生进行早期阿尔茨海默病诊断方面的潜力，为可靠、可扩展的早期筛查提供了高效且易于使用的工具。

Abstract: Alzheimer s disease (AD) is a progressive neurodegenerative disorder characterized by cognitive decline, where early detection is essential for timely intervention and improved patient outcomes. Traditional diagnostic methods are time-consuming and require expert interpretation, thus, automated approaches are highly desirable. This study presents a novel deep learning framework for AD diagnosis using Electroencephalograph (EEG) signals, integrating multiple feature extraction techniques including alpha-wave analysis, Discrete Wavelet Transform (DWT), and Markov Transition Fields (MTF). A late-fusion strategy is employed to combine predictions from separate neural networks trained on these diverse representations, capturing both temporal and frequency-domain patterns in the EEG data. The proposed model attains a classification accuracy of 87.23%, with a precision of 87.95%, a recall of 86.91%, and an F1 score of 87.42% when evaluated on a publicly available dataset, demonstrating its potential for reliable, scalable, and early AD screening. Rigorous preprocessing and targeted frequency band selection, particularly in the alpha range due to its cognitive relevance, further enhance performance. This work highlights the promise of deep learning in supporting physicians with efficient and accessible tools for early AD diagnosis.

</details>


### [8] [Dataset and UAV Propagation Channel Modeling for LoRa in the 860 MHz ISM Band](https://arxiv.org/abs/2512.15268)
*Joachim Tapparel,Andreas Burg*

Main category: eess.SP

TL;DR: 本文通过SDR测试平台收集并发布了校园环境中的LoRa帧数据集，基于该数据集推导了三种场景下的经验传播信道模型，为LoRa网络性能评估提供了专用信道模型。


<details>
  <summary>Details</summary>
Motivation: 随着物联网网络的快速密集化，需要针对LoRa传输特性（长距离、小带宽）获取定制化的信道模型来评估网络性能。现有蜂窝技术的信道模型研究广泛，但LoRa的特殊传输特性需要专门的测量和建模工作。

Method: 利用基于SDR的测试平台收集校园环境中的LoRa帧数据集，数据集包含多个位置接收帧的IQ样本，支持高时间分辨率的信道变化评估。基于收集的数据推导了三种场景的经验传播信道模型：无人机视距、无人机非视距和行人非视距。

Result: 发布了一个包含LoRa帧IQ样本的数据集，支持高时间分辨率的信道变化评估。推导了三种场景下的经验传播信道模型，包括接收器在距离上的相关性。数据集还包含同步信息，可用于使用实验数据评估接收器算法。

Conclusion: 本研究通过实验测量和数据集发布，为LoRa网络提供了专门的信道模型，填补了现有研究的空白，支持更准确的LoRa网络性能评估和接收器算法开发。

Abstract: LoRa is one of the most widely used low-power wide-area network technology for the Internet of Things. To achieve long-range communication with low power consumption at a low cost, LoRa uses a chirp spread spectrum modulation and transmits in the sub-GHz unlicensed industrial, scientific, and medical (ISM) frequency bands. Due to the rapid densification of IoT networks, it is crucial to obtain tailored channel models to evaluate the performance of LoRa networks. While channel models for cellular technologies have been investigated extensively, specific characteristics of LoRa transmissions operating at long range with a rather small (~ 250kHz) bandwidth require dedicated measurement campaigns and modeling efforts. In this work, we leverage an SDR-based testbed to gather and publish a dataset of LoRa frames transmitted in a campus environment. The dataset includes IQ samples of the received frames at multiple locations and allows for the evaluation of channel variations with high time resolution. Using the gathered data, we derive empirical propagation channel models for LoRa that include receiver correlation over distance for three scenarios: unmanned aerial vehicle (UAV) line-of-sight (LoS), UAV non-LoS, and pedestrian non-LoS. Furthermore, the dataset is annotated with synchronization information, enabling the evaluation of receiver algorithms using experimental data.

</details>


### [9] [Learning-Based Phase Shift Optimization of Liquid Crystal RIS in Dynamic mmWave Networks](https://arxiv.org/abs/2512.15279)
*Le Hao,Robin Neuder,Mohamadreza Delbari,Alejandro Jiménez-Sáez,Vahid Jamali,Arash Asadi,Andrea Ortiz*

Main category: eess.SP

TL;DR: 提出基于强化学习的LC-RIS相位动态优化框架，解决液晶可重构智能表面在动态场景中重配置时间长的限制，通过DDPG算法平衡信噪比与配置时间，实现移动用户数据率最大化。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信中，传统半导体RIS功耗高难以扩展，液晶RIS(LC-RIS)虽能效高但重配置时间长(数十毫秒)，限制了在动态场景中的应用。现有研究多关注硬件设计或静态场景，缺乏动态环境下的优化解决方案。

Method: 提出基于强化学习的优化框架，采用深度确定性策略梯度(DDPG)算法动态控制LC-RIS相位偏移。算法无需完美信道状态信息，能够平衡信噪比(SNR)与配置时间之间的权衡。通过高保真射线追踪仿真验证，利用LC-RIS原型测量数据。

Result: 验证结果表明，所提解决方案能够为动态LC-RIS辅助毫米波系统带来自适应控制能力，有效解决了LC-RIS在动态场景中的适用性问题。

Conclusion: 该研究填补了LC-RIS在动态场景优化方面的空白，通过强化学习框架实现了对LC-RIS相位的自适应控制，平衡了性能与配置时间，为动态毫米波通信系统提供了可行的解决方案。

Abstract: To enhance coverage and signal quality in millimeter-wave (mmWave) frequencies, reconfigurable intelligent surfaces (RISs) have emerged as a game-changing solution to manipulate the wireless environment. Traditional semiconductor-based RISs face scalability issues due to high power consumption. Meanwhile, liquid crystal-based RISs (LC-RISs) offer energy-efficient and cost-effective operation even for large arrays. However, this promise has a caveat. LC-RISs suffer from long reconfiguration times, on the order of tens of milliseconds, which limits their applicability in dynamic scenarios. To date, prior works have focused on hardware design aspects or static scenarios to address this limitation, but little attention has been paid to optimization solutions for dynamic settings. Our paper fills this gap by proposing a reinforcement learning-based optimization framework to dynamically control the phase shifts of LC-RISs and maximize the data rate of a moving user. Specifically, we propose a Deep Deterministic Policy Gradient (DDPG) algorithm that adapts the LC-RIS phase shifts without requiring perfect channel state information and balances the tradeoff between signal-to-noise ratio (SNR) and configuration time. We validate our approach through high-fidelity ray tracing simulations, leveraging measurement data from an LC-RIS prototype. Our results demonstrate the potential of our solution to bring adaptive control to dynamic LC-RIS-assisted mmWave systems.

</details>


### [10] [Moment-Matching Array Processing Technique for diffuse source estimation](https://arxiv.org/abs/2512.15283)
*Colin Cros,Laurent Ferro-Famil*

Main category: eess.SP

TL;DR: MoMET是一种无需先验分布假设的低复杂度DOA估计方法，通过矩匹配技术估计窄带扩散源的平均DOA、扩展和功率。


<details>
  <summary>Details</summary>
Motivation: 传统DOA估计方法对扩散源通常需要先验分布假设，当假设错误时会导致显著估计偏差，需要一种无需先验知识的鲁棒估计方法。

Method: 提出矩匹配估计技术(MoMET)，通过协方差匹配将测量协方差与矩参数化模型拟合，估计源密度的平均DOA和一阶中心矩。

Result: MoMET具有模型假设鲁棒性和数值效率，推导了估计器的渐近偏差和协方差，仿真验证了其性能。

Conclusion: MoMET为无需先验分布知识的扩散源DOA估计提供了一种有效的低复杂度解决方案。

Abstract: Direction of Arrival (DOA) estimation is a fundamental problem in signal processing. Diffuse sources, whose power density cannot be represented with a single angular coordinate, are usually characterized based on prior assumptions, which associate the source angular density with a specific set of functions. However, these assumptions can lead to significant estimation biases when they are incorrect. This paper introduces the Moment-Matching Estimation Technique (MoMET), a low-complexity method for estimating the mean DOA, spread, and power of a narrow diffuse source without requiring prior knowledge on the source distribution. The unknown source density is characterized by its mean DOA and its first central moments, which are estimated through covariance matching techniques which fit the empirical covariance of the measurements to that modeled from the moments. The MoMET parameterization is robust to incorrect model assumptions, and numerically efficient. The asymptotic bias and covariance of the new estimator are derived and its performance is demonstrated through simulations.

</details>


### [11] [On the Asymptotic Performance of Diagonally Loaded Detectors for Large Arrays: To Achieve CFAR and Optimality](https://arxiv.org/abs/2512.15290)
*Jie Zhou,Junhao Xie*

Main category: eess.SP

TL;DR: 提出两种CFAR对角加载自适应匹配滤波器，解决了传统DL-AMF缺乏CFAR特性和加载因子选择问题，通过大维渐近分析推导出渐近最优加载因子。


<details>
  <summary>Details</summary>
Motivation: 解决对角加载自适应匹配滤波器(DL-AMF)的两个关键限制：1) 对任意协方差矩阵缺乏CFAR特性；2) 从最大化检测概率角度缺乏最优加载因子选择标准。

Method: 采用大维渐近分析，在维度N和样本量K趋于无穷且N/K趋于常数c∈(0,1)的条件下，分析DL-AMF的渐近性能。基于分析结果，通过归一化构造两种CFAR DL检测器：CFAR-DL-SCMF和CFAR-DL-AMF，并推导渐近最优加载因子λ_opt。

Result: 提出的CFAR-DL-SCMF和CFAR-DL-AMF对协方差矩阵、目标导向矢量和加载因子均具有CFAR特性。基于λ_opt的最优检测器在满秩和低秩杂波加噪声环境中均优于EL-AMF和persymmetric AMF。

Conclusion: 通过大维渐近分析成功解决了DL-AMF的CFAR特性和加载因子选择问题，提出的最优CFAR检测器在实际应用中具有优越性能。

Abstract: This paper addresses two critical limitations in diagonally loaded (DL) adaptive matched filter (AMF) detector: (1) the lack of CFAR property with respect to arbitrary covariance matrices, and (2) the absence of selection criteria for optimal loading factor from the perspective of maximizing the detection probability (Pd). We provide solutions to both challenges through a comprehensive analysis for the asymptotic performance of DL-AMF under large dimensional regime (LDR) where the dimension N and sample size K tend to infinity whereas their ratio N/K converges to a constant c\in(0,1). The analytical results show that any DL detectors constructed by normalizing the random variable |a|2=|sH(R+λIN)-1y0|2 with a deterministic quantity or a random variable that converges almost surely to a deterministic value will exhibit equivalent performance under LDR. Following this idea, we derive two CFAR DL detectors: CFAR DL semi-clairvoyant matched filter (CFAR-DL-SCMF) detector and CFAR DL adaptive matched filter (CFAR-DL-AMF) detector, by normalizing |a|2 with an appropriate deterministic quantity and its consistent estimate, respectively. The theoretical analysis and simulations show that both CFAR-DL-SCMF and CFAR-DL-AMF achieve CFAR with respect to covariance matrix, target steering vector and loading factor. Furthermore, we derive the asymptotically optimal loading factor λ_opt by maximizing the explicit expression of asymptotic Pd. For practical implementation, we provide a consistent estimator for λ_opt under LDR. Based on λ_opt and its consistent estimate, we establish the optimal CFAR-DL-SCMF (opt-CFAR-DL-SCMF) and the optimal CFAR-DL-AMF (opt-CFAR-DL-AMF). Numerical examples demonstrate that the proposed opt-CFAR-DL-SCMF and opt-CFAR-DL-AMF consistently outperform EL-AMF and persymmetric AMF in both full-rank and low-rank clutter plus noise environments.

</details>


### [12] [Semi-Blind Joint Channel and Symbol Estimation for Beyond Diagonal Reconfigurable Surfaces](https://arxiv.org/abs/2512.15441)
*Gilderlan Tavares de Araújo,André L. F. de Almeida Buno Sokal,Gabor Fodor,Paulo R. B. Gomes*

Main category: eess.SP

TL;DR: 本文提出了一种基于张量分解的半盲信道估计方法，用于BD-RIS系统，无需训练序列即可联合估计信道和数据符号


<details>
  <summary>Details</summary>
Motivation: 传统BD-RIS信道估计面临挑战：复杂的互连结构、大量系数、需要训练序列。在移动性场景下，用户终端-RIS信道时变，传统训练序列方法效率低下

Method: 提出两种半盲接收器：1）两阶段方法，将四阶PARATUCK模型转化为三阶PARAFAC模型；2）单阶段迭代方法，基于四阶TUCKER分解。利用数据符号直接进行张量分解

Result: 推导了可靠联合恢复的可识别性条件，数值结果表明所提方案在性能上优于现有解决方案，并展示了性能权衡

Conclusion: 提出的半盲张量方法有效解决了BD-RIS信道估计挑战，在移动性场景下实现了无需训练序列的联合信道和符号估计，为实际部署提供了实用方案

Abstract: The beyond-diagonal reconfigurable intelligent surface (BD-RIS) is a recent architecture in which scattering elements are interconnected to enhance the degrees of freedom for wave control, yielding performance gains over traditional single-connected RISs. For BD-RIS, channel estimation - well-studied for conventional RIS - becomes more challenging due to the complex connections and a larger number of coefficients. Prior works rely on pilot-assisted estimation followed by data decoding. This paper introduces a semi-blind tensor-based approach for joint channel and symbol estimation that eliminates the need for training sequences by leveraging data symbols directly. A practical scenario with time-varying user terminal-RIS channels under mobility is considered. By reformulating the received signal from a tensor decomposition perspective, we develop two semi-blind receivers: a two-stage method transforming the fourth-order PARATUCK model into a third-order PARAFAC model, and a single-stage iterative process based on fourth-order TUCKER decomposition. Identifiability conditions for reliable joint recovery are derived, and numerical results demonstrate the performance advantages and trade-offs of the proposed schemes over existing solutions.

</details>


### [13] [Optimum Discrete Beamforming via Minkowski Sum of Polygons](https://arxiv.org/abs/2512.15546)
*Heedong Do,Angel Lozano*

Main category: eess.SP

TL;DR: 将最优离散波束成形问题转化为凸多边形Minkowski和计算，证明该问题可高效求解


<details>
  <summary>Details</summary>
Motivation: 传统离散波束成形优化问题计算复杂，需要寻找更直观高效的求解方法

Method: 将离散波束成形问题重新表述为凸多边形的Minkowski和计算问题，利用凸多边形Minkowski和的顶点数最多为原始多边形顶点数之和的特性

Result: 证明了最优波束成形解可以通过高效计算凸多边形的Minkowski和来获得，该方法的计算复杂度可控

Conclusion: 通过将离散波束成形问题转化为几何计算问题，提供了一种直观且高效的最优解计算方法

Abstract: This letter casts the problem of optimum discrete beamforming as the computation of the Minkowski sum of convex polygons, which is itself a convex polygon. The number of vertices of the latter is at most the sum of the number of vertices of the original polygons, enabling its efficient computation. This original and intuitive formulation confirms that the optimum beamforming solution can be found efficiently.

</details>


### [14] [Deep Reinforcement Learning for EH-Enabled Cognitive-IoT Under Jamming Attacks](https://arxiv.org/abs/2512.15558)
*Nadia Abdolkhani,Nada Abdel Khalek,Walaa Hamouda*

Main category: eess.SP

TL;DR: 提出基于深度强化学习的认知物联网抗干扰通信策略，通过DDQN和UCB-IA算法优化吞吐量和网络寿命


<details>
  <summary>Details</summary>
Motivation: 认知物联网面临频谱稀缺和无线通信易受干扰攻击的挑战，需要智能的抗干扰通信策略来保障网络性能

Method: 将CIoT设备通信建模为无模型马尔可夫决策过程，开发双深度Q网络（DDQN）和增强型UCB-IA算法，利用局部可观测信息进行决策

Result: 仿真表明所提DRL算法优于现有基准，实现了更自适应、高能效和安全的频谱共享

Conclusion: 提出的DRL框架为能量受限的CIoT系统在干扰攻击下提供了有效的通信策略优化方案

Abstract: In the evolving landscape of the Internet of Things (IoT), integrating cognitive radio (CR) has become a practical solution to address the challenge of spectrum scarcity, leading to the development of cognitive IoT (CIoT). However, the vulnerability of radio communications makes radio jamming attacks a key concern in CIoT networks. In this paper, we introduce a novel deep reinforcement learning (DRL) approach designed to optimize throughput and extend network lifetime of an energy-constrained CIoT system under jamming attacks. This DRL framework equips a CIoT device with the autonomy to manage energy harvesting (EH) and data transmission, while also regulating its transmit power to respect spectrum-sharing constraints. We formulate the optimization problem under various constraints, and we model the CIoT device's interactions within the channel as a model-free Markov decision process (MDP). The MDP serves as a foundation to develop a double deep Q-network (DDQN), designed to help the CIoT agent learn the optimal communication policy to navigate challenges such as dynamic channel occupancy, jamming attacks, and channel fading while achieving its goal. Additionally, we introduce a variant of the upper confidence bound (UCB) algorithm, named UCB-IA, which enhances the CIoT network's ability to efficiently navigate jamming attacks within the channel. The proposed DRL algorithm does not rely on prior knowledge and uses locally observable information such as channel occupancy, jamming activity, channel gain, and energy arrival to make decisions. Extensive simulations prove that our proposed DRL algorithm that utilizes the UCB-IA strategy surpasses existing benchmarks, allowing for a more adaptive, energy-efficient, and secure spectrum sharing in CIoT networks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts](https://arxiv.org/abs/2512.14706)
*Krunal Jesani,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TL;DR: NN-Caption：基于LLM引导的神经架构搜索管道，通过组合CNN编码器和序列解码器自动生成可运行的图像描述模型


<details>
  <summary>Details</summary>
Motivation: 传统神经架构搜索需要大量人工专业知识或自动化试错，本文旨在利用LLM自动生成图像描述模型架构，降低NAS门槛

Method: 使用DeepSeek-R1-0528-Qwen3-8B作为主要生成器，通过提示模板从LEMUR分类主干中组合CNN编码器与序列解码器（LSTM/GRU/Transformer），在严格Net API下生成可运行模型

Result: LLM生成了数十个描述模型，超过一半成功训练并产生有意义的描述；分析不同输入模型片段数量（5 vs 10）的影响，发现提供更多候选组件时成功率略有下降；报告了训练动态和最高BLEU-4得分

Conclusion: LLM引导的NAS显示出巨大潜力，不仅能提出架构，还能建议超参数和训练实践；通过提示规则和迭代代码修复解决了代码幻觉和API合规性问题；为LEMUR数据集添加了数十个新颖的描述模型，促进可复现基准测试和下游AutoML研究

Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.

</details>


### [16] [EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks](https://arxiv.org/abs/2512.15067)
*Zijiang Yan,Yixiang Huang,Jianhua Pei,Hina Tabassum,Luca Chiaraviglio*

Main category: cs.LG

TL;DR: EMFusion：基于条件扩散的概率多变量电磁场预测框架，整合上下文因素并提供不确定性估计，在频率选择性EMF数据集上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 无线基础设施快速增长需要准确预测电磁场水平以确保合规性、评估健康影响和支持网络规划。现有研究依赖宽带聚合EMF数据的单变量预测，但需要频率选择性多变量预测来捕捉运营商间和频率间的变化，这对主动网络规划至关重要。

Method: 提出EMFusion框架，采用条件多变量扩散概率预测，整合时间、季节、节假日等上下文因素。架构采用残差U-Net骨干网络，通过交叉注意力机制动态整合外部条件，并采用基于插值的采样策略将预测视为结构化修复任务。

Result: 在频率选择性EMF数据集上的实验表明，EMFusion在工作时间上下文信息下优于有或无条件的基线模型：CRPS提升23.85%，归一化均方根误差提升13.93%，预测CRPS误差降低22.47%。

Conclusion: EMFusion能够生成校准的概率预测区间，提供明确的不确定性量化，这对于可信决策至关重要，在电磁场预测方面优于现有方法。

Abstract: The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.

</details>


### [17] [Autonomous Source Knowledge Selection in Multi-Domain Adaptation](https://arxiv.org/abs/2512.14710)
*Keqiuyin Li,Jie Lu,Hua Zuo,Guangquan Zhang*

Main category: cs.LG

TL;DR: AutoS：一种多域自适应方法，通过自主选择源域样本和模型，利用更相关的源信息进行目标预测，同时使用预训练多模态模型增强伪标签以减少噪声。


<details>
  <summary>Details</summary>
Motivation: 在多域自适应中，多个源域通常包含大量冗余或不相关信息，这会损害迁移性能，特别是在大规模源域设置下。需要开发有效策略从海量源域中识别和选择最具可迁移性的知识来解决目标任务。

Method: 提出AutoS方法：1）采用密度驱动的选择策略在训练中选择源样本，并确定哪些源模型应参与目标预测；2）基于预训练多模态模型构建伪标签增强模块，以减轻目标标签噪声并改进自监督。

Result: 在真实世界数据集上的实验表明该方法具有优越性。

Conclusion: AutoS能够自主选择源训练样本和模型，利用更相关和可迁移的源信息进行目标预测，有效解决了多域自适应中的知识选择问题。

Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.

</details>


### [18] [Multi-Modal Semantic Communication](https://arxiv.org/abs/2512.15691)
*Matin Mortaheb,Erciyes Karakaya,Sennur Ulukus*

Main category: cs.LG

TL;DR: 提出一种多模态语义通信框架，通过文本查询引导视觉信息提取，使用跨模态注意力机制融合视觉和语言特征，根据相关度评分和信道带宽自适应传输图像块，实现任务驱动的语义通信。


<details>
  <summary>Details</summary>
Motivation: 传统基于transformer的自注意力方法在复杂多物体场景中缺乏明确的任务指导，难以有效识别关键信息区域。语义通信需要更智能的信息提取机制来适应复杂环境和带宽限制。

Method: 1. 提出多模态语义通信框架，集成文本查询引导信息提取；2. 使用跨模态注意力机制融合视觉特征和语言嵌入，生成软相关度评分；3. 根据评分和瞬时信道带宽，采用算法自适应传输图像块；4. 使用独立训练的编码器-解码器对，总比特率匹配信道容量；5. 接收端重建并组合图像块以保留任务关键信息。

Result: 该框架实现了灵活的目标驱动设计，能够在复杂和带宽受限环境中实现高效的语义通信，相比传统自注意力方法能更好地处理多物体复杂场景。

Conclusion: 通过文本查询引导的跨模态注意力机制和自适应分辨率传输策略，提出的多模态语义通信框架显著提升了复杂场景下的通信效率和任务相关性，为带宽受限环境中的语义通信提供了有效解决方案。

Abstract: Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.

</details>


### [19] [SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI](https://arxiv.org/abs/2512.14712)
*Ryan Cartularo*

Main category: cs.LG

TL;DR: 本文比较了两种多模态融合架构用于脓毒症预测，发现新颖的端到端融合模型在小样本上过拟合，而更精简的上下文感知混合专家模型取得了最佳性能，并开发了可部署的临床决策支持框架。


<details>
  <summary>Details</summary>
Motivation: 脓毒症占ICU入院近20%，但传统预测模型难以有效整合异构数据流，要么模态孤立，要么依赖脆弱的早期融合。需要更有效的多模态融合方法来提高预测准确性。

Method: 1. 比较端到端深度融合与上下文感知堆叠两种架构；2. 提出四模态分层门控注意力网络（SepsisFusionFormer）；3. 针对小样本问题设计更精简的上下文感知混合专家架构（SepsisLateFusion），将模态视为正交专家（静态历史学家、时序监测器、NLP阅读器），通过CatBoost元学习器动态门控；4. 开发SepsisSuite部署框架。

Result: 1. SepsisFusionFormer在小样本抗生素队列（N≈2100）中因"注意力饥饿"而过拟合（AUC 0.66）；2. SepsisLateFusion在临床发作前4小时预测达到SOTA性能（AUC 0.915）；3. 通过校准决策阈值，漏诊病例减少48%；4. 四模态集成在多类抗生素选择任务中取得最高性能（AUC 0.72）。

Conclusion: 对于脓毒症预测任务，上下文感知混合专家架构比复杂的端到端融合更有效，特别是在小样本场景下。该研究提供了可部署的临床决策支持框架，为及时干预开辟了真正的预防窗口。

Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info

</details>


### [20] [A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour](https://arxiv.org/abs/2512.14713)
*Georges Sfeir,Stephane Hess,Thomas O. Hancock,Filipe Rodrigues,Jamal Amani Rad,Michiel Bliemer,Matthew Beck,Fayyaz Khan*

Main category: cs.LG

TL;DR: 提出一个潜在类别强化学习模型，用于捕捉旅行决策中的经验形成和个体异质性，识别出三种不同的偏好适应模式。


<details>
  <summary>Details</summary>
Motivation: 旅行决策涉及经验形成过程，个体随时间学习偏好，同时存在显著的个体异质性，包括基础偏好和偏好演化方式的不同。现有模型难以同时捕捉这两个现象。

Method: 提出潜在类别强化学习模型，应用于驾驶模拟器数据集，通过变分贝叶斯方法估计参数。

Result: 识别出三个明显不同的个体类别：第一类显示情境依赖偏好和情境特定开发倾向；第二类遵循持久开发策略；第三类采用探索策略结合情境特定偏好。

Conclusion: LCRL模型能有效捕捉旅行决策中的经验形成和个体异质性，为理解不同旅行者的偏好适应模式提供了新工具。

Abstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model that allows analysts to capture both of these phenomena. We apply the model to a driving simulator dataset and estimate the parameters through Variational Bayes. We identify three distinct classes of individuals that differ markedly in how they adapt their preferences: the first displays context-dependent preferences with context-specific exploitative tendencies; the second follows a persistent exploitative strategy regardless of context; and the third engages in an exploratory strategy combined with context-specific preferences.

</details>


### [21] [A Regime-Aware Fusion Framework for Time Series Classification](https://arxiv.org/abs/2512.15378)
*Honey Singh Chauhan,Zahraa S. Abdallah*

Main category: cs.LG

TL;DR: Fusion-3 (F3) 自适应融合 Rocket、Sax 和 Sfa 三种时间序列表示，在特定数据集类型上能稳定提升分类性能，尤其在具有结构化变异或丰富频率内容的数据上效果显著。


<details>
  <summary>Details</summary>
Motivation: 尽管基于核的方法（如 Rocket）在单变量时间序列分类中表现优异，但其在不同数据集上的性能并不均衡。作者认为不同表示方法能捕捉互补的结构信息，选择性融合这些表示可以针对特定类型的数据集提供一致性的性能提升。

Method: 提出 Fusion-3 (F3) 轻量级框架，自适应融合 Rocket、Sax 和 Sfa 三种表示。通过元特征（序列长度、频谱结构、粗糙度、类别不平衡）对 UCR 数据集进行聚类，形成六个可解释的数据结构机制。使用三种互补分析：跨数据集的非参数配对统计、隔离各表示作用的消融研究、以及通过 SHAP 识别预测融合增益的数据集属性。

Result: 在 113 个 UCR 数据集上的 5 折交叉验证显示，F3 相比 Rocket 实现了小而一致的平均改进，获得了频率论和贝叶斯证据的支持。融合在具有结构化变异或丰富频率内容的数据机制中表现优异，而在高度不规则或异常值较多的设置中收益递减。

Conclusion: 选择性融合为强大的基于核的方法提供了可靠且可解释的扩展，能够精确地在数据支持的地方纠正其弱点。融合主要通过纠正特定错误来提升性能，自适应增加频率域权重恰好发生在需要修正的地方。

Abstract: Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.

</details>


### [22] [Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms](https://arxiv.org/abs/2512.14714)
*Lucas Cesar Ferreira Domingos,Russell Brinkworth,Paulo Eduardo Santos,Karl Sammut*

Main category: cs.LG

TL;DR: 提出GSE ResNeXt深度学习架构，结合可学习Gabor卷积层与ResNeXt主干网络，通过通道注意力机制提升水下声学目标分类性能，在数据有限场景下优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 水下声学目标远程检测与分类对环境监测和国防至关重要，但复杂的船舶辐射噪声和环境噪声给准确信号处理带来挑战。现有机器学习方法存在数据集有限、实验标准化不足等问题，影响模型的泛化能力和鲁棒性。

Method: 提出GSE ResNeXt架构，将可学习Gabor卷积层与ResNeXt主干网络结合，并加入挤压-激励注意力机制。Gabor滤波器作为二维自适应带通滤波器扩展特征通道表示，与通道注意力结合提升训练稳定性和收敛速度。

Result: 在三个复杂度递增的分类任务中，GSE ResNeXt在分类性能上持续优于Xception、ResNet和MobileNetV2等基线模型。Gabor卷积加入初始层使训练时间减少28%。研究发现船舶与传感器距离显著影响性能。

Conclusion: 信号处理策略对提升模型在不同环境条件下的可靠性和泛化能力至关重要，特别是在数据有限的水下声学分类场景。未来研究应关注减轻环境因素对输入信号的影响。

Abstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues such as limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated on three classification tasks of increasing complexity. In particular, the impact of temporal differences between the training and testing data is explored, revealing that the distance between the vessel and sensor significantly affects performance. Results show that, GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, the addition of Gabor convolutions in the initial layers of the model represents a 28% reduction in training time. These results emphasise the importance of signal processing strategies in improving the reliability and generalisation of models under different environmental conditions, especially in data-limited underwater acoustic classification scenarios. Future developments should focus on mitigating the impact of environmental factors on input signals.

</details>


### [23] [Soft Geometric Inductive Bias for Object Centric Dynamics](https://arxiv.org/abs/2512.15493)
*Hampus Linander,Conor Heins,Alexander Tschantz,Marco Perin,Christopher Buckley*

Main category: cs.LG

TL;DR: 提出基于几何代数神经网络的物体中心世界模型，提供软几何归纳偏置，在2D刚体动力学环境中比非等变基线模型表现更好


<details>
  <summary>Details</summary>
Motivation: 等变性是学习物理动力学的强大先验，但精确的群等变性在对称性被破坏时可能降低性能。需要一种软几何归纳偏置来平衡先验与灵活性

Method: 使用几何代数神经网络构建物体中心世界模型，在2D刚体动力学环境中进行自回归训练，实现多步预测

Result: 在长时程推演中，软归纳偏置模型在物理保真度方面优于非等变基线模型，样本效率更高

Conclusion: 几何代数提供了手工物理与无结构深度网络之间的有效折中，通过简单而精心选择的先验实现鲁棒泛化

Abstract: Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.

</details>


### [24] [How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection](https://arxiv.org/abs/2512.14715)
*Zafaryab Haider,Md Hafizur Rahman,Shane Moeykens,Vijay Devabhaktuni,Prabuddha Chakraborty*

Main category: cs.LG

TL;DR: 本文首次研究如何通过对大语言模型权重进行比特级扰动来影响图像描述生成的语义，同时保持语法结构完整，提出了梯度敏感的BLADE框架来分析语义关键比特。


<details>
  <summary>Details</summary>
Motivation: 先前硬件比特翻转研究主要关注分类器崩溃或精度下降，忽略了生成式系统（如图像描述模型）的语义和语言维度。单个比特翻转可能微妙地改变视觉特征到词语的映射，从而改变AI对世界的叙述方式。

Method: 提出BLADE（Bit-level Fault Analysis via Differentiable Estimation）框架，使用基于梯度的敏感性估计定位语义关键比特，并通过描述级语义-流畅性目标优化比特选择。

Result: 研究发现语义漂移不是随机的，而是可微分估计的。模型自身的梯度可以预测哪些比特被扰动后会最强烈地影响语义，同时保持语法和流畅性不变。

Conclusion: 即使难以察觉的低层比特变化也能引导生成式视觉语言模型的高层语义，这为鲁棒性测试、对抗防御和可解释AI开辟了新途径，揭示了结构化比特级故障如何重塑模型的语义输出。

Abstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.

</details>


### [25] [Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction](https://arxiv.org/abs/2512.15605)
*Mathieu Blondel,Michael E. Sander,Germain Vivier-Ardisson,Tianlin Liu,Vincent Roulet*

Main category: cs.LG

TL;DR: 该论文建立了自回归模型(ARMs)与能量模型(EBMs)之间的显式双射关系，揭示了它们在函数空间中的等价性，并分析了EBMs蒸馏到ARMs的理论误差界限。


<details>
  <summary>Details</summary>
Motivation: 虽然自回归模型在大型语言模型中占主导地位，但能量模型在训练后对齐中自然表征最优策略。论文旨在统一这两种模型类别，理解它们之间的关系，并解释自回归模型为何能在基于下一个token预测的范式下进行前瞻规划。

Method: 以概率链式法则为起点，在函数空间中建立ARMs和EBMs之间的显式双射关系，证明这对应于最大熵强化学习中的软贝尔曼方程特例。基于此双射，推导监督学习中ARMs和EBMs的等价性，并分析EBMs蒸馏到ARMs的理论误差界限。

Result: 建立了ARMs和EBMs之间的函数空间双射关系，证明了监督学习中两者的等价性，提供了EBMs蒸馏到ARMs的理论误差界限，为ARMs的前瞻规划能力提供了理论解释。

Conclusion: 论文通过建立ARMs和EBMs的统一框架，不仅解释了自回归模型的前瞻规划能力，还为两种模型之间的转换提供了理论基础，对理解大型语言模型的本质和优化具有重要意义。

Abstract: Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.

</details>


### [26] [Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox](https://arxiv.org/abs/2512.14717)
*Ziqian Bi,Danyang Zhang,Junhao Song,Chiung-Yi Tseng*

Main category: cs.LG

TL;DR: GPT-OSS-20B模型在金融NLP任务中达到与更大模型相当的准确率(65.1% vs 66.5%)，同时计算效率显著更高，挑战了模型规模直接决定性能的假设。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在金融服务中的快速应用，需要严格的评估框架来评估其性能、效率和实际适用性，特别是在资源受限的生产环境中。

Method: 在10个不同的金融NLP任务上对GPT-OSS模型家族和当代LLMs进行全面评估，使用包括Financial PhraseBank、FiQA-SA和FLARE FINERORD在内的真实金融数据集，并引入新的效率指标来衡量性能与资源利用的权衡。

Result: GPT-OSS-20B模型在保持可比准确率的同时，展现出显著的计算效率优势：198.4 Token Efficiency Score和159.80 tokens/秒的处理速度，甚至超过了Qwen3-235B等更大模型。

Conclusion: GPT-OSS的架构创新和训练策略使较小模型能够以显著降低的计算开销实现竞争性性能，为金融应用中可持续且经济高效的LLM部署提供了途径。

Abstract: The rapid adoption of large language models in financial services necessitates rigorous evaluation frameworks to assess their performance, efficiency, and practical applicability. This paper conducts a comprehensive evaluation of the GPT-OSS model family alongside contemporary LLMs across ten diverse financial NLP tasks. Through extensive experimentation on 120B and 20B parameter variants of GPT-OSS, we reveal a counterintuitive finding: the smaller GPT-OSS-20B model achieves comparable accuracy (65.1% vs 66.5%) while demonstrating superior computational efficiency with 198.4 Token Efficiency Score and 159.80 tokens per second processing speed [1]. Our evaluation encompasses sentiment analysis, question answering, and entity recognition tasks using real-world financial datasets including Financial PhraseBank, FiQA-SA, and FLARE FINERORD. We introduce novel efficiency metrics that capture the trade-off between model performance and resource utilization, providing critical insights for deployment decisions in production environments. The benchmark reveals that GPT-OSS models consistently outperform larger competitors including Qwen3-235B, challenging the prevailing assumption that model scale directly correlates with task performance [2]. Our findings demonstrate that architectural innovations and training strategies in GPT-OSS enable smaller models to achieve competitive performance with significantly reduced computational overhead, offering a pathway toward sustainable and cost-effective deployment of LLMs in financial applications.

</details>


### [27] [SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting](https://arxiv.org/abs/2512.14718)
*Feng Xiong,Zongxia Xie,Yanru Sun,Haoyu Wang,Jianhong Lin*

Main category: cs.LG

TL;DR: SEED是一个基于谱熵引导的时空依赖建模框架，通过动态评估变量依赖关系、保留负相关性、增强时间位置感知，在多变量时间序列预测中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力或图的方法存在三个关键问题：1）强时间自依赖被不相关变量干扰；2）softmax归一化忽略并反转负相关性；3）变量难以感知自身时间位置。需要解决这些问题以更准确建模复杂的变量间依赖关系。

Method: 提出SEED框架，包含四个核心组件：1）依赖评估器：利用谱熵动态评估每个变量的时空依赖，自适应平衡通道独立和通道依赖策略；2）基于谱熵的融合器：进一步细化依赖权重，分离由其他变量影响产生的时序规律；3）符号图构造器：支持带符号边权重，保留负相关性；4）上下文空间提取器：利用局部上下文窗口提取空间特征，帮助变量感知时间位置。

Result: 在12个来自不同应用领域的真实世界数据集上进行广泛实验，SEED实现了最先进的性能，验证了其有效性和通用性。

Conclusion: SEED通过谱熵引导的依赖评估、保留负相关性、增强时间位置感知等创新，有效解决了现有多变量时间序列预测方法的局限性，在复杂变量间依赖建模方面表现出色。

Abstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.

</details>


### [28] [Hybrid Attribution Priors for Explainable and Robust Model Training](https://arxiv.org/abs/2512.14719)
*Zhuoran Zhang,Feng Zhang,Shangyuan Li,Yang Shi,Yuanxing Zhang,Wei Chen,Tengjiao Wang,Kam-Fai Wong*

Main category: cs.LG

TL;DR: 本文提出CAP框架，通过提取类别感知的归因先验，帮助小语言模型学习细粒度类别区分，提升分类任务的解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前解释引导学习框架中，归因先验往往只关注类别相关的通用关键词，缺乏区分相似类别的判别性线索，限制了模型区分能力的提升。

Method: 提出类别感知归因先验（CAP）框架，提取能捕捉细粒度类别区分的归因先验；进一步提出CAP Hybrid，结合CAP先验和现有归因技术，形成更全面的监督信号。

Result: 在完整数据、少样本和对抗场景下的广泛实验表明，该方法能持续提升模型的解释性和鲁棒性。

Conclusion: CAP框架通过提供更具判别性的归因先验，有效解决了现有归因方法在区分相似类别时的局限性，为小语言模型的解释引导学习提供了新思路。

Abstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.

</details>


### [29] [Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example](https://arxiv.org/abs/2512.14721)
*Arno Appenzeller,Nick Terzer,André Hohmeyer,Jan-Philipp Redlich,Sabine Luttmann,Friedrich Feuerhake,Nadine S. Schaadt,Timm Intemann,Sarah Teuber-Hanselmann,Stefan Nikolin,Joachim Weis,Klaus Kraywinkel,Pascal Birnstill*

Main category: cs.LG

TL;DR: 提出一种基于癌症报告统计数据自动生成Synthea规则的方法，并以胶质母细胞瘤为例验证了生成合成数据的可行性。


<details>
  <summary>Details</summary>
Motivation: 合成数据是隐私合规使用医疗数据的重要技术，但Synthea等规则生成方法需要专家知识和真实数据，过程复杂。需要自动化方法来简化规则创建。

Method: 从癌症报告中提取表格数据统计信息，自动生成Synthea规则。以胶质母细胞瘤真实数据集为例，创建Synthea模块并生成合成数据集。

Result: 合成数据成功复现了已知疾病病程，大部分统计特性得以保留。与原始数据集相比，合成数据在统计属性上表现良好。

Conclusion: 合成患者数据在隐私保护研究中具有巨大潜力，可用于假设形成和原型开发，但医疗解释需考虑当前方法的局限性。

Abstract: The generation of synthetic data is a promising technology to make medical data available for secondary use in a privacy-compliant manner. A popular method for creating realistic patient data is the rule-based Synthea data generator. Synthea generates data based on rules describing the lifetime of a synthetic patient. These rules typically express the probability of a condition occurring, such as a disease, depending on factors like age. Since they only contain statistical information, rules usually have no specific data protection requirements. However, creating meaningful rules can be a very complex process that requires expert knowledge and realistic sample data. In this paper, we introduce and evaluate an approach to automatically generate Synthea rules based on statistics from tabular data, which we extracted from cancer reports. As an example use case, we created a Synthea module for glioblastoma from a real-world dataset and used it to generate a synthetic dataset. Compared to the original dataset, the synthetic data reproduced known disease courses and mostly retained the statistical properties. Overall, synthetic patient data holds great potential for privacy-preserving research. The data can be used to formulate hypotheses and to develop prototypes, but medical interpretation should consider the specific limitations as with any currently available approach.

</details>


### [30] [HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers](https://arxiv.org/abs/2512.14722)
*Mohamed Malhou,Ludovic Perret,Kristin Lauter*

Main category: cs.LG

TL;DR: 使用分层注意力变换器（HATs）计算Gröbner基，相比传统平面注意力模型显著节省计算成本，能处理更大规模的多变量多项式方程组


<details>
  <summary>Details</summary>
Motivation: NeurIPS 2024上Kera等人首次将变换器用于计算Gröbner基，这是计算机代数中的核心对象，具有众多实际应用。本文旨在改进这一方法，通过引入分层注意力变换器来更有效地处理多变量多项式方程组

Method: 采用分层注意力变换器（HATs）架构，该架构包含树结构归纳偏置，能够建模数据中的层次关系。方法推广到任意深度，并包含详细的计算成本分析。结合课程学习策略

Result: 相比传统平面注意力模型，HATs实现了显著的计算节省。结合课程学习后，能够解决比Kera等人（2024）工作中大得多的实例

Conclusion: 分层注意力变换器为计算Gröbner基提供了一种更高效的方法，通过利用多项式方程组中的层次结构，能够扩展到更大规模的问题实例

Abstract: At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)

</details>


### [31] [O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization](https://arxiv.org/abs/2512.15229)
*Elio Gruttadauria,Mathieu Fontaine,Jonathan Le Roux,Slim Essid*

Main category: cs.LG

TL;DR: O-EENC-SD：基于EEND-EDA的端到端在线说话人日志系统，采用RNN拼接机制和质心细化解码器，在CallHome数据集上实现竞争性性能，在DER和复杂度间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有说话人日志方法的局限性：无监督聚类方法需要超参数调优，而当前在线端到端方法计算成本过高。需要开发一种既高效又无需超参数调优的在线说话人日志系统。

Method: 基于EEND-EDA架构，引入RNN-based stitching机制处理在线预测，开发新颖的centroid refinement decoder（质心细化解码器），通过消融研究验证其有效性。系统处理独立块且无重叠，实现极高效率。

Result: 在CallHome数据集的双人电话对话语音领域，O-EENC-SD达到与最先进方法竞争的性能。系统在DER（说话人日志错误率）和复杂度之间提供了优秀的平衡，即使处理无重叠的独立块也能保持高效。

Conclusion: O-EENC-SD提供了一个无需超参数调优、计算高效的在线说话人日志解决方案，在性能和效率之间取得了良好折衷，为实时说话人日志应用提供了实用选择。

Abstract: We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.

</details>


### [32] [Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion](https://arxiv.org/abs/2512.14725)
*Francisco Giral,Álvaro Manzano,Ignacio Gómez,Petros Koumoutsakos,Soledad Le Clainche*

Main category: cs.LG

TL;DR: 提出一个基于生成扩散模型的框架，用于在非结构化网格上合成稳态城市风场，仅需几何信息即可生成准确多样的速度场。


<details>
  <summary>Details</summary>
Motivation: 城市风场建模对空气质量评估和可持续城市规划很重要，但现有方法存在局限：低阶模型无法捕捉几何效应，而高保真CFD模拟成本过高，特别是在处理多个几何形状或风况时。

Method: 结合分层图神经网络和基于分数的扩散建模，构建生成扩散框架，在非结构化网格上合成稳态城市风场，无需时间演化或密集测量。

Result: 模型能够泛化到未见过的几何形状，恢复关键流动结构（如尾流和再循环区），提供不确定性感知预测，并通过消融研究验证了对网格变化的鲁棒性和不同推理机制下的性能。

Conclusion: 这是构建建筑环境基础模型的第一步，可帮助城市规划者在城市密集化和气候不确定性下快速评估设计决策。

Abstract: Urban wind flow modeling and simulation play an important role in air quality assessment and sustainable city planning. A key challenge for modeling and simulation is handling the complex geometries of the urban landscape. Low order models are limited in capturing the effects of geometry, while high-fidelity Computational Fluid Dynamics (CFD) simulations are prohibitively expensive, especially across multiple geometries or wind conditions. Here, we propose a generative diffusion framework for synthesizing steady-state urban wind fields over unstructured meshes that requires only geometry information. The framework combines a hierarchical graph neural network with score-based diffusion modeling to generate accurate and diverse velocity fields without requiring temporal rollouts or dense measurements. Trained across multiple mesh slices and wind angles, the model generalizes to unseen geometries, recovers key flow structures such as wakes and recirculation zones, and offers uncertainty-aware predictions. Ablation studies confirm robustness to mesh variation and performance under different inference regimes. This work develops is the first step towards foundation models for the built environment that can help urban planners rapidly evaluate design decisions under densification and climate uncertainty.

</details>


### [33] [Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery](https://arxiv.org/abs/2512.15344)
*Hiroyoshi Nagahama,Katsufumi Inoue,Masayoshi Todorokihara,Michifumi Yoshioka*

Main category: cs.LG

TL;DR: 论文提出两种相位感知预处理策略来处理多轴振动数据中的随机相位变化，通过相位对齐提升旋转机械预测性维护的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的旋转机械预测性维护方法在频谱特征提取时通常丢弃相位信息，或使用原始时间波形而不显式利用相位信息，这限制了性能提升。

Method: 提出两种相位感知预处理策略：1）三轴独立相位调整（独立对齐各轴到零相位）；2）单轴参考相位调整（通过统一时间偏移保持轴间关系）。使用新构建的同步三轴传感器转子数据集，在两级学习框架下评估六种深度学习架构。

Result: 两种方法都带来架构无关的性能提升：三轴独立方法实现一致增益（Transformer提升2.7%），单轴参考方法通过保持空间相位关系达到96.2%准确率（提升5.4%）。

Conclusion: 两种相位对齐策略都是实用且可扩展的预测性维护系统增强方法，相位信息对振动信号分析具有重要价值。

Abstract: Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\% accuracy (+5.4\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.

</details>


### [34] [Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning](https://arxiv.org/abs/2512.14726)
*Abraham Itzhak Weinberg*

Main category: cs.LG

TL;DR: 量子决策变换器(QDT)通过量子启发的注意力机制和前馈网络，在离线强化学习中实现了超过2000%的性能提升，解决了传统决策变换器在长时程信用分配和复杂状态-动作依赖关系上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有决策变换器(DT)架构在离线强化学习中面临长时程信用分配和复杂状态-动作依赖关系的挑战，需要新的架构设计来提升性能。

Method: 提出量子决策变换器(QDT)，包含两个核心组件：1) 量子启发注意力机制，通过纠缠操作捕获非局部特征相关性；2) 量子前馈网络，具有多路径处理和可学习干涉的自适应计算能力。

Result: 在连续控制任务上实现了超过2000%的性能提升，相比标准DT有显著优势，并在不同数据质量下表现出优越的泛化能力。消融研究表明量子启发组件之间存在强协同效应。

Conclusion: 量子启发架构设计需要整体协同设计而非模块化组件采用，为序列决策中的变换器架构提供了有前景的新方向，其计算优势包括增强的信用分配、隐式集成行为和自适应资源分配。

Abstract: Offline reinforcement learning enables policy learning from pre-collected datasets without environment interaction, but existing Decision Transformer (DT) architectures struggle with long-horizon credit assignment and complex state-action dependencies. We introduce the Quantum Decision Transformer (QDT), a novel architecture incorporating quantum-inspired computational mechanisms to address these challenges. Our approach integrates two core components: Quantum-Inspired Attention with entanglement operations that capture non-local feature correlations, and Quantum Feedforward Networks with multi-path processing and learnable interference for adaptive computation. Through comprehensive experiments on continuous control tasks, we demonstrate over 2,000\% performance improvement compared to standard DTs, with superior generalization across varying data qualities. Critically, our ablation studies reveal strong synergistic effects between quantum-inspired components: neither alone achieves competitive performance, yet their combination produces dramatic improvements far exceeding individual contributions. This synergy demonstrates that effective quantum-inspired architecture design requires holistic co-design of interdependent mechanisms rather than modular component adoption. Our analysis identifies three key computational advantages: enhanced credit assignment through non-local correlations, implicit ensemble behavior via parallel processing, and adaptive resource allocation through learnable interference. These findings establish quantum-inspired design principles as a promising direction for advancing transformer architectures in sequential decision-making, with implications extending beyond reinforcement learning to neural architecture design more broadly.

</details>


### [35] [Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection](https://arxiv.org/abs/2512.15385)
*Julian Oelhaf,Mehran Pashaei,Georg Kordowich,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 论文提出一个评估电力系统保护中机器学习模型鲁棒性的统一框架，通过高保真电磁暂态仿真模拟传感器故障、采样率降低等实际退化场景，发现故障分类相对稳定而故障定位对电压损失更敏感。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源和分布式发电的渗透率提高，传统基于固定设置和本地测量的保护方案面临挑战。机器学习为集中式故障分类和定位提供了数据驱动的替代方案，但实际部署需要确保在传感器数据缺失、噪声或退化情况下的鲁棒性。

Method: 引入一个统一框架，使用高保真电磁暂态仿真模拟实际退化场景（包括传感器故障、采样率降低和瞬时通信丢失），为机器学习模型提供一致的基准测试方法，量化有限可观测性的影响，并识别关键测量通道。

Result: 故障分类在大多数退化类型下保持高度稳定，但在单相损失下下降约13%；故障定位整体更敏感，电压损失使定位误差增加超过150%。

Conclusion: 该框架为未来机器学习辅助保护系统的鲁棒性设计提供了可操作的指导，强调了在传感器数据退化情况下确保保护算法可靠性的重要性。

Abstract: The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.
  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems.

</details>


### [36] [A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications](https://arxiv.org/abs/2512.14727)
*Klaus-Rudolf Kladny,Bernhard Schölkopf,Lisa Koch,Christian F. Baumgartner,Michael Muehlebach*

Main category: cs.LG

TL;DR: 论文质疑了保形预测在小校准集下的实际效用，指出虽然统计保证成立，但实际价值高度依赖校准集大小，这在医疗数据稀缺场景中尤为重要。


<details>
  <summary>Details</summary>
Motivation: 机器学习在医疗领域应用需要可靠的不确定性估计，但标准ML模型无法提供。保形预测(CP)可以将启发式不确定性估计转化为具有统计保证的估计。虽然CP理论声称适用于任意大小的校准集，但在医疗领域数据稀缺、难以获得大校准集的情况下，需要验证小校准集下CP的实际效用。

Method: 通过理论分析和实证验证，展示CP的统计保证虽然对任意校准集大小都成立，但实际效用高度依赖校准集大小。在医疗图像分类任务上进行实证演示，验证小校准集下CP的实际局限性。

Result: 研究发现，尽管CP的统计保证在任意校准集大小下都成立，但这些保证的实际效用确实高度依赖校准集的大小。在医疗图像分类任务中，小校准集会导致预测集过大或不稳定，降低了CP的实际应用价值。

Conclusion: 在医疗等数据稀缺领域应用保形预测时需要谨慎，不能仅依赖其理论保证，必须考虑校准集大小对实际效用的影响。研究呼吁对CP在小校准集下的实际性能进行更现实的评估。

Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.

</details>


### [37] [A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems](https://arxiv.org/abs/2512.14728)
*Jie He,Yong Qin,Jianyuan Guo,Xuan Sun,Xuanchuan Zheng*

Main category: cs.LG

TL;DR: 提出基于AFC和AVL数据的城市轨道交通个体出行轨迹推断方法，采用KLEM算法进行数据驱动的参数估计，无需外部调查数据，在高峰时段轨迹推断准确率超过90%。


<details>
  <summary>Details</summary>
Motivation: 城市轨道交通精细化轨迹推断对运营组织具有重要意义，现有方法依赖外部调查数据或合成数据验证，缺乏鲁棒性和适用性。

Method: 基于时空约束建立列车备选集，提出基于KL散度结合EM算法的KLEM方法进行数据驱动的自适应轨迹推断，构建完整的出行轨迹。

Result: 该方法实现了高精度的乘客轨迹推断，在高峰时段城市轨道交通出行轨迹推断准确率超过90%。

Conclusion: 提出的全数据驱动方法能够有效推断个体出行轨迹，消除了对外部调查数据的依赖，提高了模型的鲁棒性和适用性。

Abstract: Refined trajectory inference of urban rail transit is of great significance to the operation organization. In this paper, we develop a fully data-driven approach to inferring individual travel trajectories in urban rail transit systems. It utilizes data from the Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems to infer key trajectory elements, such as selected train, access/egress time, and transfer time. The approach includes establishing train alternative sets based on spatio-temporal constraints, data-driven adaptive trajectory inference, and trave l trajectory construction. To realize data-driven adaptive trajectory inference, a data-driven parameter estimation method based on KL divergence combined with EM algorithm (KLEM) was proposed. This method eliminates the reliance on external or survey data for parameter fitting, enhancing the robustness and applicability of the model. Furthermore, to overcome the limitations of using synthetic data to validate the result, this paper employs real individual travel trajectory data for verification. The results show that the approach developed in this paper can achieve high-precision passenger trajectory inference, with an accuracy rate of over 90% in urban rail transit travel trajectory inference during peak hours.

</details>


### [38] [Semantic Geometry for policy-constrained interpretation](https://arxiv.org/abs/2512.14731)
*Nikit Phadke*

Main category: cs.LG

TL;DR: 提出一种几何框架用于政策约束的语义解释，可证明防止高风险领域中的幻觉承诺，在金融监管数据上实现零幻觉批准


<details>
  <summary>Details</summary>
Motivation: 解决高风险领域中语义解释的幻觉问题，特别是在需要严格遵守政策约束的场景中，防止系统做出不符合政策的承诺或批准

Method: 使用单位球面上的方向表示语义，证据建模为见证向量集合，可接受解释对应球形凸区域，政策约束作为显式先验定义在同一流形上，解释简化为约束区域上的优化问题

Result: 证明框架在信息论上复杂度最优，在大型监管金融数据上实现零幻觉批准，首次在大规模应用中达到此效果

Conclusion: 该几何框架为政策约束的语义解释提供了理论基础和实用方法，能有效防止高风险领域中的幻觉承诺，具有信息论最优性

Abstract: We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.

</details>


### [39] [INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT](https://arxiv.org/abs/2512.14732)
*Idan Tankel,Nir Mazor,Rafi Brada,Christina LeBedis,Guy ben-Yosef*

Main category: cs.LG

TL;DR: 提出一个基于大语言模型和视觉语言模型的规划-执行代理框架，用于自动化腹部CT扫描中偶然发现的检测、分类和报告，提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: CT扫描中的偶然发现虽然通常良性，但具有重要临床意义，需要遵循指南报告。传统放射科医生手动检查耗时且结果不一致，需要更高效、精确的自动化解决方案。

Method: 采用规划-执行代理框架：规划器基于LLM生成Python脚本调用预定义基础函数；执行器运行脚本，通过VLM、分割模型和图像处理子程序执行检测和检查。针对腹部器官医学指南自动化管理偶然发现。

Result: 在腹部CT基准测试中对三个器官进行实验，以完全自动化的端到端方式验证框架有效性。结果显示，该方法在准确性和效率方面优于现有的纯VLM方法。

Conclusion: 提出的LLM和VLM结合的规划-执行代理框架能够有效提升腹部CT扫描中偶然发现的自动化检测、分类和报告性能，为医学影像分析提供了新的高效解决方案。

Abstract: Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.
  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.

</details>


### [40] [Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness](https://arxiv.org/abs/2512.14734)
*Qiang Chen,Venkatesh Ganapati Hegde,Hongfei Li*

Main category: cs.LG

TL;DR: 提出轻量级模型无关的日内个性化方法，通过推理时注入近期观看历史来减少个性化反馈循环，在长视频流媒体中实现显著用户参与度提升


<details>
  <summary>Details</summary>
Motivation: 当前长视频流媒体推荐系统依赖批量训练模型和批量更新特征，用户特征每日更新并在全天静态服务，无法纳入用户最新行为，导致推荐过时

Method: 提出轻量级、模型无关的日内个性化方法，在推理时选择性注入近期观看历史，无需模型重新训练。通过选择性覆盖过时的用户特征，使系统能即时适应不断变化的偏好

Result: 通过将个性化反馈循环从每日减少到日内，观察到关键用户参与度指标统计显著增加0.47%，这是近期实验周期中观察到的最大参与度提升之一

Conclusion: 这是首个证明日内个性化能在长视频流媒体服务中产生有意义影响的公开证据，为需要模型重新训练的完全实时架构提供了有吸引力的替代方案

Abstract: Many recommender systems in long-form video streaming reply on batch-trained models and batch-updated features, where user features are updated daily and served statically throughout the day. While efficient, this approach fails to incorporate a user's most recent actions, often resulting in stale recommendations. In this work, we present a lightweight, model-agnostic approach for intra-day personalization that selectively injects recent watch history at inference time without requiring model retraining. Our approach selectively overrides stale user features at inference time using the recent watch history, allowing the system to adapt instantly to evolving preferences. By reducing the personalization feedback loop from daily to intra-day, we observed a statistically significant 0.47% increase in key user engagement metrics which ranked among the most substantial engagement gains observed in recent experimentation cycles. To our knowledge, this is the first published evidence that intra-day personalization can drive meaningful impact in long-form video streaming service, providing a compelling alternative to full real-time architectures where model retraining is required.

</details>


### [41] [NoveltyRank: Estimating Conceptual Novelty of AI Papers](https://arxiv.org/abs/2512.14738)
*Zhengxu Yan,Han Li,Yuming Feng*

Main category: cs.LG

TL;DR: 开发一个评估AI论文概念新颖性的模型，通过标题、摘要和语义相似度来量化研究原创性，帮助研究人员和审稿人识别真正创新的工作。


<details>
  <summary>Details</summary>
Motivation: 随着学术出版门槛降低，AI领域论文数量激增，真正新颖和有影响力的工作难以脱颖而出。手动评估新颖性不稳定且耗时，需要数据驱动的可扩展评估系统。

Method: 采用两种任务形式：1) 二元分类（预测绝对新颖性），2) 成对新颖性比较（学习相对新颖性）。使用Qwen3-4B-Instruct-2507和SciBERT进行微调，并与GPT-5.1进行基准测试。

Result: 实现了公开可用的新颖性评估系统，分析了任务形式和建模选择对性能的影响。代码已在GitHub开源。

Conclusion: 该研究为AI论文新颖性评估提供了数据驱动的解决方案，有助于识别真正创新的研究，为会议审稿提供定量一致的新颖性信号。

Abstract: With the growing ease of academic publishing, the volume of research papers, especially in AI-related fields, has surged dramatically. This flood of publications makes it difficult for truly novel and impactful work to stand out, and manual novelty assessment is often unstable and time-consuming. Our project aims to develop a model that estimates and ranks the conceptual novelty of AI papers, enabling a data-driven and scalable assessment of research originality. Such a system can help researchers efficiently identify submissions that introduce genuinely innovative ideas rather than minor variants, and provide conference reviewers with a quantitative and consistent signal of novelty. Our approach evaluates novelty primarily through a paper's title, abstract, and semantic similarity to prior literature. Given the motivation of novelty estimation, we explore two task formulations with different modeling objectives, each offering a different perspective: (1) binary classification, which predicts the paper's absolute novelty from learned patterns of prior novel works, and (2) pairwise novelty comparison, which learns to distinguish papers by relative novelty over others. We fine-tune Qwen3-4B-Instruct-2507 and SciBERT on both tasks, benchmarking against GPT-5.1 to analyze how task formulation and modeling choices affect performance. The implementation is publicly available at https://github.com/ZhengxuYan/NoveltyRank.

</details>


### [42] [Guided Discrete Diffusion for Constraint Satisfaction Problems](https://arxiv.org/abs/2512.14765)
*Justin Jung*

Main category: cs.LG

TL;DR: 提出离散扩散引导方法解决约束满足问题，以无监督方式解决数独谜题


<details>
  <summary>Details</summary>
Motivation: 传统约束满足问题求解方法通常需要监督或特定规则，本文旨在开发一种无需监督的通用求解方法

Method: 使用离散扩散模型，通过扩散过程引导约束满足问题的求解，特别针对数独这类离散组合优化问题

Result: 该方法能够有效解决数独谜题，无需任何监督信号或预先定义的求解规则

Conclusion: 离散扩散引导为约束满足问题提供了一种新颖的无监督求解框架，具有扩展到其他组合优化问题的潜力

Abstract: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.

</details>


### [43] [Evaluating Weather Forecasts from a Decision Maker's Perspective](https://arxiv.org/abs/2512.14779)
*Kornelius Raeth,Nicole Ludwig*

Main category: cs.LG

TL;DR: 该论文提出了决策校准框架，从决策者角度评估天气预报价值，发现传统预报评估与下游决策表现不一致


<details>
  <summary>Details</summary>
Motivation: 传统天气预报评估主要关注预报者视角和统计指标，但实际中预报用于决策制定，需要从决策者角度评估预报价值

Method: 提出决策校准框架，在决策层面而非预报层面评估性能，比较机器学习与传统数值天气预报模型在各种天气依赖决策任务中的表现

Result: 模型在预报层面的性能不能可靠地转化为下游决策表现：一些性能差异只在决策层面显现，不同决策任务中模型排名会变化

Conclusion: 传统预报评估不足以为特定决策任务选择最优预报模型，需要决策层面的评估框架

Abstract: Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.

</details>


### [44] [Unreliable Uncertainty Estimates with Monte Carlo Dropout](https://arxiv.org/abs/2512.14851)
*Aslak Djupskås,Alexander Johannes Stasik,Signe Riemer-Sørensen*

Main category: cs.LG

TL;DR: MCD作为贝叶斯推断的高效近似，在不确定性估计方面不如传统贝叶斯方法可靠，特别是在外推和内插区域


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域需要可靠的不确定性估计，但深度神经网络的精确贝叶斯推断计算不可行，MCD被提出作为高效近似方法

Method: 通过实验研究MCD捕捉真实不确定性的能力，并与高斯过程和贝叶斯神经网络进行比较

Result: MCD难以准确反映底层真实不确定性，特别是在外推和内插区域无法捕捉到不确定性增加，不如传统贝叶斯方法可靠

Conclusion: MCD的不确定性估计在捕捉认知和偶然不确定性方面不如传统贝叶斯方法可靠，需要谨慎使用

Abstract: Reliable uncertainty estimation is crucial for machine learning models, especially in safety-critical domains. While exact Bayesian inference offers a principled approach, it is often computationally infeasible for deep neural networks. Monte Carlo dropout (MCD) was proposed as an efficient approximation to Bayesian inference in deep learning by applying neuron dropout at inference time \citep{gal2016dropout}. Hence, the method generates multiple sub-models yielding a distribution of predictions to estimate uncertainty. We empirically investigate its ability to capture true uncertainty and compare to Gaussian Processes (GP) and Bayesian Neural Networks (BNN). We find that MCD struggles to accurately reflect the underlying true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions as observed in Bayesian models. The findings suggest that uncertainty estimates from MCD, as implemented and evaluated in these experiments, is not as reliable as those from traditional Bayesian approaches for capturing epistemic and aleatoric uncertainty.

</details>


### [45] [How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal](https://arxiv.org/abs/2512.14873)
*Sam Jeong,Hae Yong Kim*

Main category: cs.LG

TL;DR: 研究发现FAN网络的改进主要来自正弦激活函数在原点附近的非零导数特性，而非其周期性。这缓解了梯度消失问题，特别是解决了dying-ReLU问题。基于此提出了更高效的Dual-Activation Layer (DAL)。


<details>
  <summary>Details</summary>
Motivation: 虽然Fourier Analysis Network (FAN)被提出能提升神经网络性能，但其改进机制一直不明确。本文旨在揭示FAN真正起作用的原理，并基于此开发更高效的激活层。

Method: 通过分析发现只有正弦激活对性能有正面贡献，而余弦激活反而有害。进一步研究表明改进源于正弦函数在x=0附近的局部行为（非零导数），而非周期性特性。这缓解了梯度消失问题，特别是dying-ReLU问题。基于此提出了Dual-Activation Layer (DAL)。

Result: 在三个任务上评估DAL：噪声正弦信号分类、MNIST数字分类和ECG生物识别。在所有情况下，DAL模型收敛更快，且达到相等或更高的验证准确率。

Conclusion: FAN的改进机制应从频谱解释转向训练动力学分析。正弦激活通过其原点附近的非零导数特性缓解梯度消失问题，特别是dying-ReLU问题。基于此开发的DAL是更高效的收敛加速器。

Abstract: Fourier Analysis Network (FAN) was recently proposed as a simple way to improve neural network performance by replacing part of ReLU activations with sine and cosine functions. Although several studies have reported small but consistent gains across tasks, the underlying mechanism behind these improvements has remained unclear. In this work, we show that only the sine activation contributes positively to performance, whereas the cosine activation tends to be detrimental. Our analysis reveals that the improvement is not a consequence of the sine function's periodic nature; instead, it stems from the function's local behavior near x = 0, where its non-zero derivative mitigates the vanishing-gradient problem. We further show that FAN primarily alleviates the dying-ReLU problem, in which a neuron consistently receives negative inputs, produces zero gradients, and stops learning. Although modern ReLU-like activations, such as Leaky ReLU, GELU, and Swish, reduce ReLU's zero-gradient region, they still contain input domains where gradients remain significantly diminished, contributing to slower optimization and hindering rapid convergence. FAN addresses this limitation by introducing a more stable gradient pathway. This analysis shifts the understanding of FAN's benefits from a spectral interpretation to a concrete analysis of training dynamics, leading to the development of the Dual-Activation Layer (DAL), a more efficient convergence accelerator. We evaluate DAL on three tasks: classification of noisy sinusoidal signals versus pure noise, MNIST digit classification, and ECG-based biometric recognition. In all cases, DAL models converge faster and achieve equal or higher validation accuracy compared to models with conventional activations.

</details>


### [46] [Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse](https://arxiv.org/abs/2512.14879)
*Jingwei Chen*

Main category: cs.LG

TL;DR: 提出ERBP框架，将自指学习中的模型崩溃统一解释为分布空间的Bregman投影序列，通过引入熵库实现稳定，为各种经验性修复方法提供理论依据。


<details>
  <summary>Details</summary>
Motivation: 自指学习（模型在自身生成的数据上训练）虽然具有无限扩展潜力，但长期存在模型崩溃问题。尽管实践中采用各种临时修复方法，但缺乏统一的理论框架来解释失败模式和修复方法的成功原理。

Method: 提出熵库Bregman投影（ERBP）框架，将自指学习建模为分布空间中的随机Bregman投影序列。通过引入熵库（高熵分布）注入可控的熵通量来稳定动力学，并推导出崩溃的必要条件、非平凡熵底限的充分条件以及封闭形式的收敛速率。

Result: 理论分析表明：1）有限样本噪声导致指数熵衰减和最终崩溃；2）熵库注入可稳定动力学；3）收敛速率仅取决于样本量和Bregman生成器的强凸性/Lipschitz常数。在大语言模型自训练、强化学习中的Soft Actor-Critic和GAN优化实验中验证了预测。

Conclusion: ERBP框架将各种经验性修复方法统一为单一量化设计规则：监控和预算熵通量。不同的稳定启发式方法对应于特定的熵库选择和耦合系数，为自指学习提供了理论基础。

Abstract: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.

</details>


### [47] [Task Matrices: Linear Maps for Cross-Model Finetuning Transfer](https://arxiv.org/abs/2512.14880)
*Darrin O' Brien,Dhikshith Gajulapalli,Eric Xia*

Main category: cs.LG

TL;DR: 论文提出"任务矩阵"概念，证明预训练和微调模型之间存在跨层线性编码，通过线性变换提升基础模型性能，接近微调水平。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大型视觉和语言模型在上下文提示偏置下学习隐式线性编码，但更一般的适应机制中是否存在类似线性表示尚未得到验证。

Method: 提出任务矩阵概念——从基础嵌入状态到微调嵌入状态的线性变换，在视觉和文本模型及十个数据集上测试，使用数据驱动的近似方法计算编码。

Result: 基础模型增强任务矩阵后性能超越线性探针，有时接近微调水平，验证了预训练和微调架构间存在跨层线性编码，且数据近似方法高效且可泛化到多领域。

Conclusion: 任务矩阵证实了跨层线性编码的存在，为模型适应提供高效线性方法，代码已公开。

Abstract: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.

</details>


### [48] [OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams](https://arxiv.org/abs/2512.14892)
*Mohammad Abu-Shaira,Alejandro Rodriguez,Greg Speegle,Victor Sheng,Ishfaq Ahmad*

Main category: cs.LG

TL;DR: 提出OLR-WA在线回归模型，通过加权平均处理数据漂移，实现与批量回归相当的性能，并在收敛速度和置信度场景处理上优于现有在线模型。


<details>
  <summary>Details</summary>
Motivation: 在线学习可以避免大规模数据存储和模型重新计算的成本，但现有在线回归模型在处理数据漂移和置信度场景方面存在不足，需要更稳健的解决方案。

Method: 提出OLR-WA（在线回归加权平均）模型，采用保守更新策略，优先考虑置信度较高的旧数据点，有效处理时间漂移和置信度挑战场景。

Result: OLR-WA性能与批量回归相当，收敛速度显著优于其他在线模型（从第1次迭代到最后一次迭代都保持高r²值），即使仅用1%-10%数据初始化也能快速收敛，是唯一能有效处理置信度挑战场景的模型。

Conclusion: OLR-WA在不同场景下表现出多功能性和实用性，是在线线性回归任务的有价值解决方案，特别在处理数据漂移和置信度场景方面具有独特优势。

Abstract: Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce "OLR-WA; OnLine Regression with Weighted Average", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.

</details>


### [49] [Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections](https://arxiv.org/abs/2512.14895)
*Niklas Lauffer,Xiang Deng,Srivatsa Kundurthy,Brad Kenstler,Jeff Da*

Main category: cs.LG

TL;DR: 论文提出OEC方法解决多轮LLM智能体训练中的协变量偏移问题，通过学生模型启动、专家模型中途介入生成部分在线数据，在软件工程任务上相比传统模仿学习提升13-14%


<details>
  <summary>Details</summary>
Motivation: 传统基于模仿学习的LM智能体训练在多轮交互中存在协变量偏移问题：学生策略偏离专家行为时，会遇到训练数据中未见过的新状态，导致微调效果下降

Method: 提出在线专家修正(OEC)数据生成方法：让学生模型启动轨迹，中途切换到专家模型完成剩余部分，生成部分在线数据。结合拒绝采样和监督微调技术进行训练

Result: 在软件工程任务(SWE-bench)上，OEC方法在7b和32b模型上分别相对传统模仿学习提升14%和13%的验证性能

Conclusion: 多轮LM智能体训练需要结合专家演示和在线数据，OEC方法能有效解决协变量偏移问题，提升智能体在复杂多轮任务中的性能

Abstract: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.

</details>


### [50] [ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs](https://arxiv.org/abs/2512.14908)
*Turja Kundu,Sanjukta Bhowmick*

Main category: cs.LG

TL;DR: ATLAS是一种新颖的图学习算法，通过提取多分辨率社区拓扑信息并拼接特征向量，使用MLP而非特征聚合，解决了GNN在异配图上的性能下降和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络的两个关键挑战：1）在异配图（heterophilic graphs）上GNN性能下降；2）迭代特征聚合限制了GNN在大规模图上的可扩展性。

Method: 提取多分辨率级别的图社区拓扑信息，将社区分配结果与特征向量拼接，然后对得到的表示应用多层感知机（MLP），避免了特征聚合过程。

Result: 在多种图上，ATLAS达到与基线方法相当的准确率，在具有负结构偏置的异配图上比GCN提高20个百分点，在同配图上比MLP提高11个百分点。多分辨率社区特征能系统性地调节性能。

Conclusion: ATLAS通过拓扑信息提供节点及其邻域的上下文，避免了特征聚合，具有更好的可扩展性，并为可解释的图学习开辟了原则性路径。

Abstract: We present ATLAS (Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs), a novel graph learning algorithm that addresses two important challenges in graph neural networks (GNNs). First, the accuracy of GNNs degrades when the graph is heterophilic. Second, iterative feature aggregation limits the scalability of GNNs to large graphs. We address these challenges by extracting topological information about graph communities at multiple levels of refinement, concatenating community assignments to the feature vector, and applying multilayer perceptrons (MLPs) to the resulting representation. This provides topological context about nodes and their neighborhoods without invoking aggregation. Because MLPs are typically more scalable than GNNs, our approach applies to large graphs without the need for sampling. Across a wide set of graphs, ATLAS achieves comparable accuracy to baseline methods, with gains as high as 20 percentage points over GCN for heterophilic graphs with negative structural bias and 11 percentage points over MLP for homophilic graphs. Furthermore, we show how multi-resolution community features systematically modulate performance in both homophilic and heterophilic settings, opening a principled path toward explainable graph learning.

</details>


### [51] [Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective](https://arxiv.org/abs/2512.14932)
*Daniel Gomes de Pinho Zanco,Leszek Szczecinski,Jacob Benesty,Eduardo Vinicius Kuhn*

Main category: cs.LG

TL;DR: 提出一种基于Kronecker积表示的低秩MMSE滤波器正则化参数高效选择方法，该方法与秩选择问题相关，实验验证优于常用方法


<details>
  <summary>Details</summary>
Motivation: 低秩MMSE滤波器中正则化参数的选择对性能至关重要，但现有方法效率不高，且参数选择与秩选择问题存在关联需要探索

Method: 基于Kronecker积表示，提出高效的正则化参数选择方法，将参数选择与秩选择问题联系起来

Result: 通过仿真验证，所提方法相比常用方法取得显著性能提升

Conclusion: 正则化参数选择对低秩设置至关重要，所提方法能高效选择参数并获得更好性能

Abstract: In this work, we propose a method to efficiently find the regularization parameter for low-rank MMSE filters based on a Kronecker-product representation. We show that the regularization parameter is surprisingly linked to the problem of rank selection and, thus, properly choosing it, is crucial for low-rank settings. The proposed method is validated through simulations, showing significant gains over commonly used methods.

</details>


### [52] [Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise](https://arxiv.org/abs/2512.14967)
*Felipe J. P. Antunes,Yuri F. Saporito,Sebastian Jaimungal*

Main category: cs.LG

TL;DR: 提出结合可引出性、深度学习和Picard迭代的数值方法，用于求解带共同噪声的McKean-Vlasov前向-后向随机微分方程，避免昂贵的嵌套蒙特卡洛模拟。


<details>
  <summary>Details</summary>
Motivation: 传统方法求解带共同噪声的MV-FBSDEs需要计算条件期望，通常依赖计算昂贵的嵌套蒙特卡洛模拟。需要开发更高效的方法来处理这类复杂的平均场博弈问题。

Method: 结合Picard迭代、可引出性和深度学习：1) 使用可引出性推导路径损失函数；2) 用循环神经网络参数化平均场交互项；3) 用前馈网络近似解耦场表示后向过程；4) 通过最小化可引出分数训练网络。

Result: 在系统性风险银行借贷模型上准确恢复解析解；扩展到分位数介导的交互；成功应用于非平稳Aiyagari-Bewley-Huggett经济增长模型，展示了在复杂平均场博弈问题中的适用性。

Conclusion: 提出的可引出性框架为求解带共同噪声的MV-FBSDEs提供了高效灵活的数值方法，能够处理超越条件均值或矩的复杂交互，适用于缺乏解析解的复杂经济模型。

Abstract: We present a novel numerical method for solving McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs) with common noise, combining Picard iterations, elicitability and deep learning. The key innovation involves elicitability to derive a path-wise loss function, enabling efficient training of neural networks to approximate both the backward process and the conditional expectations arising from common noise - without requiring computationally expensive nested Monte Carlo simulations. The mean-field interaction term is parameterized via a recurrent neural network trained to minimize an elicitable score, while the backward process is approximated through a feedforward network representing the decoupling field. We validate the algorithm on a systemic risk inter-bank borrowing and lending model, where analytical solutions exist, demonstrating accurate recovery of the true solution. We further extend the model to quantile-mediated interactions, showcasing the flexibility of the elicitability framework beyond conditional means or moments. Finally, we apply the method to a non-stationary Aiyagari--Bewley--Huggett economic growth model with endogenous interest rates, illustrating its applicability to complex mean-field games without closed-form solutions.

</details>


### [53] [Softly Constrained Denoisers for Diffusion Models](https://arxiv.org/abs/2512.14980)
*Victor M. Yeom Song,Severi Rissanen,Arno Solin,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出一种在去噪器中集成约束引导的方法，通过软归纳偏置实现约束遵从，同时保持对约束错误指定的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在科学应用中难以生成满足约束的样本，现有方法通过损失正则化或采样引导会偏离真实数据分布，特别是在约束错误指定时问题更严重。

Method: 将引导启发式调整集成到去噪器本身，赋予其软归纳偏置，使去噪器倾向于生成符合约束的样本，同时保持灵活性。

Result: 软约束去噪器能利用约束知识提高样本合规性，同时在约束与观测数据存在错误指定时保持足够的灵活性偏离约束。

Conclusion: 通过将约束引导集成到去噪器架构中，可以在不改变损失函数或采样循环的情况下，平衡约束遵从与数据分布保真度。

Abstract: Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.

</details>


### [54] [Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/abs/2512.14982)
*Yaniv Leviathan,Matan Kalman,Yossi Matias*

Main category: cs.LG

TL;DR: 重复输入提示能提升主流模型性能，无需增加生成token或延迟


<details>
  <summary>Details</summary>
Motivation: 探索在不使用推理的情况下，如何通过简单方法提升大型语言模型的性能表现

Method: 通过重复输入提示（prompt repetition）来测试模型性能变化

Result: 对于Gemini、GPT、Claude和Deepseek等流行模型，重复输入提示能提高性能，且不增加生成token数量或延迟

Conclusion: 简单的提示重复策略是提升模型性能的有效方法，具有实际应用价值

Abstract: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.

</details>


### [55] [Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes](https://arxiv.org/abs/2512.14991)
*Hanqing Jin,Renyuan Xu,Yanzhao Yang*

Main category: cs.LG

TL;DR: 提出一种用于无界连续状态空间扩散过程的自适应分区强化学习算法，通过平衡探索与近似实现高效学习，并建立了包含缩放维度概念的遗憾界理论保证。


<details>
  <summary>Details</summary>
Motivation: 金融、经济和运筹学中常出现具有无界连续状态空间、有界连续动作和多项式增长奖励的扩散过程控制问题，现有方法难以处理这种连续高维域。

Method: 提出基于模型的自适应分区算法，在联合状态-动作空间中进行自适应划分，在每个分区内估计漂移、波动率和奖励，当估计偏差超过统计置信度时细化离散化。

Result: 建立了依赖于问题时域、状态维度、奖励增长阶数和专门为无界扩散过程定义的缩放维度的遗憾界，将现有有界域结果推广到更广泛的扩散类型问题。

Conclusion: 该自适应方案能有效平衡探索与近似，在无界域中实现高效学习，并通过多资产均值-方差投资组合选择等高维问题的数值实验验证了方法的有效性。

Abstract: We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.

</details>


### [56] [DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding](https://arxiv.org/abs/2512.15000)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-Code提出了一种针对编程任务的流程奖励模型，通过将函数视为推理步骤并使用元学习校正机制，在代码生成任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有流程奖励模型在编程任务中效果有限，主要因为代码缺乏有意义的步骤分解，以及蒙特卡洛生成的中间标签存在噪声。

Method: 1. 使用Chain-of-Function提示策略将函数视为推理步骤，实现模块化代码生成；2. 引入基于元学习的校正机制，利用干净的最终解决方案单元测试标签，通过双层优化精炼中间标签。

Result: 在LiveCodeBench上达到80.9%的pass@1率，超越了OpenAI o4-mini，取得了最先进的性能。

Conclusion: DreamPRM-Code通过创新的步骤分解和标签校正方法，显著提升了流程奖励模型在编程任务中的效果，为代码生成提供了有效的测试时扩展方案。

Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.

</details>


### [57] [Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets](https://arxiv.org/abs/2512.15008)
*Sandeep Neela*

Main category: cs.LG

TL;DR: SPA是一个确定性框架，用于从股价数据中提取单调价格走势，关联公开事件，并生成事实性、历史性的解释，旨在提供透明、可复现的历史价格结构分析。


<details>
  <summary>Details</summary>
Motivation: 现有技术指标和预测模型缺乏透明度和可解释性，在需要审计和透明性的场景中存在挑战。需要一种能够提供清晰、可解释价格结构分析的工具。

Method: SPA使用每日OHLCV数据和标准化事件流，通过确定性框架提取单调价格走势，通过对称相关窗口关联公开事件，并生成受约束的事实性解释。

Result: 在AAPL、NVDA、SCHW、PGR四只股票上的评估显示，SPA能够稳定地生成结构性分解和上下文叙事。消融实验表明确定性分割、事件对齐和约束解释都提升了可解释性。

Conclusion: SPA不是预测系统或交易信号生成器，其价值在于提供透明、可复现的历史价格结构视图，可补充分析师工作流程、风险评估和可解释AI管道。

Abstract: Understanding how prices evolve over time often requires peeling back the layers of market noise to identify clear, structural behavior. Many of the tools commonly used for this purpose technical indicators, chart heuristics, or even sophisticated predictive models leave important questions unanswered. Technical indicators depend on platform-specific rules, and predictive systems typically offer little in terms of explanation. In settings that demand transparency or auditability, this poses a significant challenge. We introduce the Stock Pattern Assistant (SPA), a deterministic framework designed to extract monotonic price runs, attach relevant public events through a symmetric correlation window, and generate explanations that are factual, historical, and guardrailed. SPA relies only on daily OHLCV data and a normalized event stream, making the pipeline straight-forward to audit and easy to reproduce. To illustrate SPA's behavior in practice, we evaluate it across four equities-AAPL, NVDA, SCHW, and PGR-chosen to span a range of volatility regimes and sector characteristics. Although the evaluation period is modest, the results demonstrate how SPA consistently produces stable structural decompositions and contextual narratives. Ablation experiments further show how deterministic segmentation, event alignment, and constrained explanation each contribute to interpretability. SPA is not a forecasting system, nor is it intended to produce trading signals. Its value lies in offering a transparent, reproducible view of historical price structure that can complement analyst workflows, risk reviews, and broader explainable-AI pipelines.

</details>


### [58] [Epistemic diversity across language models mitigates knowledge collapse](https://arxiv.org/abs/2512.15011)
*Damian Hodel,Jevin D. West*

Main category: cs.LG

TL;DR: 研究AI生态系统多样性如何缓解知识崩溃，发现适度的认知多样性可以缓解崩溃，但过多或过少都会导致性能下降


<details>
  <summary>Details</summary>
Motivation: AI的广泛使用引发了知识崩溃的担忧，即知识减少到最主导和核心的思想集合。先前研究展示了单一模型崩溃现象，本文受生态学启发，探讨AI生态系统多样性（模型间的多样性）是否能缓解这种崩溃

Method: 在单一模型方法基础上，构建在集体输出上训练的模型生态系统。通过将训练数据在不同语言模型间分割，评估由此产生的生态系统在十次自训练迭代中的表现，研究多样性对模型性能的影响

Result: 增加认知多样性可以缓解崩溃，但只达到一个最优水平。只有少数多样化模型的生态系统无法表达完整真实分布的丰富混合，导致性能快速衰减；而将数据分布在太多模型中会降低每个模型对真实分布的近似能力，导致第一次迭代就表现不佳

Conclusion: 在AI单一文化背景下，需要监控AI系统间的多样性，并制定政策激励更多领域和社区特定的模型发展，以维持健康的AI生态系统

Abstract: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.

</details>


### [59] [Spectral Representation-based Reinforcement Learning](https://arxiv.org/abs/2512.15036)
*Chenxiao Gao,Haotian Sun,Na Li,Dale Schuurmans,Bo Dai*

Main category: cs.LG

TL;DR: 论文提出谱表示视角作为强化学习的新框架，通过谱分解转移算子来抽象系统动态，为策略优化提供理论基础，并在DeepMind Control Suite的20多个任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大状态和动作空间中通常使用函数近似（如神经网络），但这些方法存在理论模糊性、优化不稳定、探索困难和高计算成本等问题。需要一种既能提供清晰理论表征又能有效抽象系统动态的解决方案。

Method: 引入谱表示框架，基于转移算子的谱分解来构建系统动态的有效抽象。针对具有隐变量结构或基于能量结构的转移算子，提出了不同的谱表示学习方法。还将该谱视角扩展到部分可观测MDPs。

Result: 在DeepMind Control Suite的20多个挑战性任务上验证了算法，性能达到或超过了当前最先进的模型无关和基于模型的基线方法。

Conclusion: 谱表示框架为强化学习提供了理论基础清晰、优化稳定且计算高效的解决方案，能够有效处理大状态动作空间问题，并在实际任务中表现出色。

Abstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.

</details>


### [60] [The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems](https://arxiv.org/abs/2512.15068)
*Debu Sinha*

Main category: cs.LG

TL;DR: RAG系统仍会产生幻觉，现有基于语义相似度和NLI的检测方法存在根本缺陷。研究应用保形预测提供有限样本覆盖保证，发现嵌入方法在真实基准上假阳性率过高，而GPT-4作为LLM法官表现更好，揭示了"语义幻觉"问题。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG系统基于检索证据，但仍容易产生幻觉。当前基于语义相似度和自然语言推理的检测方法存在根本局限性，但尚未被严格表征。需要更可靠的检测方法来评估RAG系统的可靠性。

Method: 应用保形预测（conformal prediction）进行幻觉检测，提供有限样本覆盖保证。使用约600个示例的校准集，在合成和真实基准上测试多种方法：包括基于嵌入的方法（OpenAI text-embedding-3-large、交叉编码器模型）和GPT-4作为LLM法官。

Result: 在合成幻觉上达到94%覆盖率且0%假阳性率，但在三个真实基准上嵌入方法假阳性率过高：HaluEval 100%、RAGTruth 88%、WikiBio 50%。GPT-4作为LLM法官仅7%假阳性率，证明任务可通过推理解决。揭示了"语义幻觉"现象。

Conclusion: 嵌入基础的检测方法存在根本缺陷，无法可靠检测语义合理但事实错误的幻觉。GPT-4等LLM法官通过推理能更好解决此问题。嵌入方法不适合生产环境RAG部署，需要更先进的检测方法。

Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.

</details>


### [61] [The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks](https://arxiv.org/abs/2512.15082)
*Wanfu Gao,Zebin He,Jun Gao*

Main category: cs.LG

TL;DR: FEAML是一种基于大语言模型的自动化特征工程方法，专门针对多标签学习任务，通过代码生成和反馈机制优化特征质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的特征工程方法尚未应用于多标签学习任务，缺乏对复杂标签依赖关系的建模能力，且未针对多标签任务特性进行专门适配。

Method: 利用LLMs的代码生成能力，通过元数据和标签共现矩阵引导LLMs理解数据特征与任务目标的关系，生成高质量特征。使用模型准确率评估特征有效性，皮尔逊相关系数检测冗余，并将评估结果作为反馈驱动LLMs持续优化代码生成。

Result: 在多个多标签数据集上的实验结果表明，FEAML优于其他特征工程方法。

Conclusion: 通过将LLMs与反馈机制结合，FEAML实现了一个高效、可解释且自我改进的特征工程范式。

Abstract: Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.

</details>


### [62] [Neural Modular Physics for Elastic Simulation](https://arxiv.org/abs/2512.15083)
*Yifei Li,Haixu Wu,Zeyi Xu,Tuur Stuyck,Wojciech Matusik*

Main category: cs.LG

TL;DR: 提出Neural Modular Physics (NMP)方法，将弹性模拟分解为物理意义明确的神经模块，结合神经网络近似能力和传统模拟器的物理可靠性，提升物理一致性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的物理模拟方法通常使用端到端优化的单一神经网络，虽然有效但可能失去物理可解释性和可靠性等传统数值模拟器的重要特性。受传统模块化模拟器启发，希望结合神经网络近似能力和传统模拟器的物理可靠性。

Method: 提出Neural Modular Physics (NMP)方法，将弹性动力学分解为具有物理意义的神经模块，通过中间物理量连接。采用专门架构和训练策略，将数值计算流程转换为模块化神经模拟器，实现对中间量的直接监督和物理约束。

Result: NMP在实验中表现出：1)对未见初始条件和分辨率的优越泛化能力；2)稳定的长期模拟；3)相比其他神经模拟器更好地保持物理特性；4)相比传统模拟器在未知底层动力学场景中更具可行性。

Conclusion: NMP通过模块化设计成功结合了神经网络近似能力和传统模拟器的物理可靠性，超越了单一学习范式，实现了更好的物理一致性和泛化性能，为物理模拟提供了新的有效方法。

Abstract: Learning-based methods have made significant progress in physics simulation, typically approximating dynamics with a monolithic end-to-end optimized neural network. Although these models offer an effective way to simulation, they may lose essential features compared to traditional numerical simulators, such as physical interpretability and reliability. Drawing inspiration from classical simulators that operate in a modular fashion, this paper presents Neural Modular Physics (NMP) for elastic simulation, which combines the approximation capacity of neural networks with the physical reliability of traditional simulators. Beyond the previous monolithic learning paradigm, NMP enables direct supervision of intermediate quantities and physical constraints by decomposing elastic dynamics into physically meaningful neural modules connected through intermediate physical quantities. With a specialized architecture and training strategy, our method transforms the numerical computation flow into a modular neural simulator, achieving improved physical consistency and generalizability. Experimentally, NMP demonstrates superior generalization to unseen initial conditions and resolutions, stable long-horizon simulation, better preservation of physical properties compared to other neural simulators, and greater feasibility in scenarios with unknown underlying dynamics than traditional simulators.

</details>


### [63] [PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network](https://arxiv.org/abs/2512.15086)
*Hongjin Mi,Huiqiang Lun,Changhong Mou,Yeyu Zhang*

Main category: cs.LG

TL;DR: 提出PIP² Net，一种基于分区惩罚的物理信息深度算子网络，通过改进分区正则化技术提升算子学习的稳定性和表达能力，在多个非线性PDE上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法（如DeepONet和FNO）需要大量训练数据、缺乏显式物理结构、存在主干网络特征不稳定的问题（如模式不平衡或崩溃），影响算子近似精度。受经典分区统一方法的稳定性和局部性启发，研究基于分区统一的正则化技术。

Method: 开发PIP² Net（物理信息分区惩罚深度算子网络），改进现有的POU-PI-DeepONet框架，引入更简化、更有原则的分区惩罚机制，协调主干网络输出，提升表达能力同时保持DeepONet的灵活性。

Result: 在三个非线性PDE（粘性Burgers方程、Allen-Cahn方程、扩散-反应系统）上评估，PIP² Net在预测精度和鲁棒性方面一致优于DeepONet、PI-DeepONet和POU-DeepONet。

Conclusion: 基于分区统一的正则化技术能有效提升算子学习的稳定性和表达能力，PIP² Net为参数化PDE的快速求解提供了更准确、更鲁棒的算子学习框架。

Abstract: Operator learning has become a powerful tool for accelerating the solution of parameterized partial differential equations (PDEs), enabling rapid prediction of full spatiotemporal fields for new initial conditions or forcing functions. Existing architectures such as DeepONet and the Fourier Neural Operator (FNO) show strong empirical performance but often require large training datasets, lack explicit physical structure, and may suffer from instability in their trunk-network features, where mode imbalance or collapse can hinder accurate operator approximation. Motivated by the stability and locality of classical partition-of-unity (PoU) methods, we investigate PoU-based regularization techniques for operator learning and develop a revised formulation of the existing POU--PI--DeepONet framework. The resulting \emph{P}hysics-\emph{i}nformed \emph{P}artition \emph{P}enalty Deep Operator Network (PIP$^{2}$ Net) introduces a simplified and more principled partition penalty that improved the coordinated trunk outputs that leads to more expressiveness without sacrificing the flexibility of DeepONet. We evaluate PIP$^{2}$ Net on three nonlinear PDEs: the viscous Burgers equation, the Allen--Cahn equation, and a diffusion--reaction system. The results show that it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.

</details>


### [64] [SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs](https://arxiv.org/abs/2512.15088)
*Xianglin Wu,Chiheb Ben Hammouda,Cornelis W. Oosterlee*

Main category: cs.LG

TL;DR: SigMA：结合路径签名和多头自注意力的神经网络架构，用于分数布朗运动驱动SDEs的参数估计，在精度、鲁棒性和模型紧凑性方面优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 分数布朗运动驱动的随机微分方程在金融和可靠性工程中应用广泛，但这些过程具有非马尔可夫性和非半鞅结构，传统参数估计方法不适用或计算困难，需要开发新的高效估计方法。

Method: 提出SigMA架构，集成路径签名与多头自注意力机制，包含卷积预处理层和多层感知机进行特征编码。从分数布朗运动、分数Ornstein-Uhlenbeck和粗糙Heston模型生成的路径中学习参数，重点关注Hurst参数估计和联合多参数推断。

Result: 在合成数据和两个真实数据集（股票指数已实现波动率和锂离子电池退化）上的实验表明，SigMA在准确性、鲁棒性和模型紧凑性方面一致优于CNN、LSTM、普通Transformer和深度签名基线方法。

Conclusion: 将签名变换与基于注意力的架构相结合，为具有粗糙或持久时间结构的随机系统中的参数推断提供了一个有效且可扩展的框架。

Abstract: Stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm) are increasingly used to model systems with rough dynamics and long-range dependence, such as those arising in quantitative finance and reliability engineering. However, these processes are non-Markovian and lack a semimartingale structure, rendering many classical parameter estimation techniques inapplicable or computationally intractable beyond very specific cases. This work investigates two central questions: (i) whether integrating path signatures into deep learning architectures can improve the trade-off between estimation accuracy and model complexity, and (ii) what constitutes an effective architecture for leveraging signatures as feature maps. We introduce SigMA (Signature Multi-head Attention), a neural architecture that integrates path signatures with multi-head self-attention, supported by a convolutional preprocessing layer and a multilayer perceptron for effective feature encoding. SigMA learns model parameters from synthetically generated paths of fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, with a particular focus on estimating the Hurst parameter and on joint multi-parameter inference, and it generalizes robustly to unseen trajectories. Extensive experiments on synthetic data and two real-world datasets (i.e., equity-index realized volatility and Li-ion battery degradation) show that SigMA consistently outperforms CNN, LSTM, vanilla Transformer, and Deep Signature baselines in accuracy, robustness, and model compactness. These results demonstrate that combining signature transforms with attention-based architectures provides an effective and scalable framework for parameter inference in stochastic systems with rough or persistent temporal structure.

</details>


### [65] [Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption](https://arxiv.org/abs/2512.15112)
*Sunwoo Kim,Soo Yong Lee,Kyungho Kim,Hyunjin Hwang,Jaemin Yoo,Kijung Shin*

Main category: cs.LG

TL;DR: FUEL：一种无监督节点表示学习方法，通过自适应调整图卷积使用程度，在嵌入空间中增强类内相似性和类间分离性，在多种同质性水平的图上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督节点表示学习方法过度依赖图卷积，特别是在非同质性图中，这可能导致特征或拓扑属性不同的节点产生过于相似的嵌入。虽然监督学习中已探索调整图卷积使用程度的方法，但在无监督场景中此类方法仍未被充分研究。

Method: 提出FUEL方法，自适应学习适当的图卷积使用程度。由于类别未知，FUEL利用节点特征识别节点簇，并将这些簇作为类别的代理，旨在增强嵌入空间中的类内相似性和类间分离性。

Result: 通过15种基线方法和14个基准数据集的广泛实验，证明FUEL在下游任务中的有效性，在具有不同同质性水平的图上实现了最先进的性能。

Conclusion: FUEL通过自适应调整图卷积使用程度，有效解决了无监督节点表示学习中过度依赖图卷积的问题，特别是在非同质性图中，显著提升了嵌入质量。

Abstract: Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.

</details>


### [66] [How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models](https://arxiv.org/abs/2512.15115)
*Ali Ghodsi*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架来分析序列建模架构，揭示了注意力机制与状态空间模型在表达能力与梯度传播之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 序列建模领域存在多种架构（RNN、Transformer、SSM等），但缺乏对表达能力与可训练性权衡的统一理论理解。作者旨在建立一个统一框架来分析这些架构的理论特性。

Method: 引入统一框架，通过输入依赖的有效交互算子W_ij(X)表示广泛的序列映射。识别两种构建模式：统一因子化框架（注意力式混合）和结构化动态（隐式状态空间递归）。在此框架下推导三个理论结果。

Result: 1. 交互秩间隙：统一因子化框架中的模型（如单头注意力）受限于低维算子空间，无法表示某些结构化动态映射。
2. 等价定理：在多头因子化类中，表示线性SSM需要且仅需k个头。
3. 梯度高速公路结果：注意力层允许距离无关的梯度路径，而稳定线性动态则表现出距离相关的梯度衰减。

Conclusion: 该研究形式化了代数表达能力（交互/算子空间）与长程梯度传播之间的基本权衡，为现代序列架构设计提供了理论基础。注意力机制在梯度传播方面更优，而状态空间模型在表达能力方面更强。

Abstract: Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.

</details>


### [67] [FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation](https://arxiv.org/abs/2512.15116)
*Runze Li,Hanchen Wang,Wenjie Zhang,Binghao Li,Yu Zhang,Xuemin Lin,Ying Zhang*

Main category: cs.LG

TL;DR: FADTI：一种基于扩散的多元时间序列插补框架，通过可学习的傅里叶偏置投影模块注入频率感知特征调制，结合自注意力和门控卷积进行时序建模，在结构化缺失模式和分布偏移下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer和扩散的模型缺乏显式归纳偏置和频率感知能力，限制了它们在结构化缺失模式和分布偏移下的泛化性能。多元时间序列插补在医疗、交通预测和生物建模等应用中至关重要，但传感器故障和不规则采样导致普遍缺失值。

Method: 提出FADTI框架，通过可学习的傅里叶偏置投影（FBP）模块注入频率感知特征调制，支持多种谱基函数，能够自适应编码平稳和非平稳模式。结合自注意力和门控卷积进行时序建模，将频域归纳偏置注入生成式插补过程。

Result: 在多个基准测试（包括新引入的生物时间序列数据集）上，FADTI始终优于最先进的方法，特别是在高缺失率下表现突出。

Conclusion: FADTI通过注入频率感知的归纳偏置，显著提升了多元时间序列插补的性能，特别是在结构化缺失模式和分布偏移下的泛化能力，为实际应用提供了有效的解决方案。

Abstract: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF

</details>


### [68] [Automatic Reward Shaping from Multi-Objective Human Heuristics](https://arxiv.org/abs/2512.15120)
*Yuqing Xie,Jiayu Chen,Wenhao Tang,Ya Zhang,Chao Yu,Yu Wang*

Main category: cs.LG

TL;DR: MORSE是一个自动组合多个启发式奖励的多目标奖励塑形框架，通过双层优化和随机探索来避免局部最优，在机器人任务中达到与手动调优相当的性能。


<details>
  <summary>Details</summary>
Motivation: 在多目标环境中设计有效的奖励函数是强化学习中的核心挑战，特别是需要平衡多个目标时。现有方法通常依赖人工设计奖励函数，这既耗时又难以优化。

Method: 提出MORSE框架，将奖励塑形过程建模为双层优化问题：内层训练策略最大化当前塑形奖励，外层更新奖励函数以优化任务性能。引入随机性来鼓励奖励空间的探索，通过任务性能和随机初始化神经网络的预测误差来指导噪声注入。

Result: 在MuJoCo和Isaac Sim环境中的实验结果表明，MORSE能够有效平衡各种机器人任务中的多个目标，达到与手动调优奖励函数相当的任务性能。

Conclusion: MORSE提供了一个通用的多目标奖励塑形框架，能够自动组合多个启发式奖励，通过双层优化和随机探索机制有效解决多目标强化学习中的奖励设计问题。

Abstract: Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.

</details>


### [69] [TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training](https://arxiv.org/abs/2512.15123)
*Mukur Gupta,Niharika Gupta,Saifur Rahman,Shantanu Pal,Chandan Karmakar*

Main category: cs.LG

TL;DR: TrajSyn是一个联邦学习框架，通过从客户端模型更新的轨迹中合成代理数据集，在服务器端实现对抗训练，提高模型鲁棒性且不增加客户端计算负担。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的深度学习模型在安全关键应用中易受对抗性攻击，联邦学习环境中由于客户端数据隐私限制和计算资源有限，难以应用对抗训练。

Method: TrajSyn通过分析客户端模型更新的轨迹来合成代理数据集，在服务器端进行对抗训练，无需访问原始客户端数据。

Result: 在图像分类基准测试中，TrajSyn能持续提高对抗鲁棒性，且不增加客户端设备的额外计算负担。

Conclusion: TrajSyn提供了一种隐私保护的联邦学习对抗训练方法，解决了传统方法在FL环境中的局限性。

Abstract: Deep learning models deployed on edge devices are increasingly used in safety-critical applications. However, their vulnerability to adversarial perturbations poses significant risks, especially in Federated Learning (FL) settings where identical models are distributed across thousands of clients. While adversarial training is a strong defense, it is difficult to apply in FL due to strict client-data privacy constraints and the limited compute available on edge devices. In this work, we introduce TrajSyn, a privacy-preserving framework that enables effective server-side adversarial training by synthesizing a proxy dataset from the trajectories of client model updates, without accessing raw client data. We show that TrajSyn consistently improves adversarial robustness on image classification benchmarks with no extra compute burden on the client device.

</details>


### [70] [From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?](https://arxiv.org/abs/2512.15134)
*Aaron Mueller,Andrew Lee,Shruti Joshi,Ekdeep Singh Lubana,Dhanya Sridhar,Patrik Reizinger*

Main category: cs.LG

TL;DR: 该研究提出多概念评估框架，分析稀疏自编码器和稀疏探针在概念相关性增强时的解耦表示能力，发现特征与概念存在一对多关系，且特征操纵会影响多个概念，表明相关度量不足以评估独立性。


<details>
  <summary>Details</summary>
Motivation: 可解释性研究通常孤立评估概念表示质量，隐含独立性假设可能不成立。需要研究常见特征化方法（稀疏自编码器和稀疏探针）是否能在实践中恢复解耦的概念表示，特别是在概念间存在相关性的情况下。

Method: 提出多概念评估框架：控制文本概念（如情感、领域、时态）间的相关性，分析在相关性增强时的性能。首先评估特征化方法学习解耦表示的能力，然后进行操纵实验，测量每个概念是否可独立操纵。

Result: 发现特征与概念存在一对多关系：特征最多对应一个概念，但概念分布在多个特征上。即使训练时概念分布均匀，稀疏自编码器特征在操纵时通常影响多个概念，表明它们既不具有选择性也不独立，但特征影响不相交的子空间。

Conclusion: 相关度量通常不足以评估操纵时的独立性，影响不相交子空间也不足以保证概念选择性。这些结果强调了可解释性研究中组合评估的重要性。

Abstract: A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.

</details>


### [71] [Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany](https://arxiv.org/abs/2512.15140)
*Roland Baatz*

Main category: cs.LG

TL;DR: 机器学习模型在德国NUTS-3地区作物产量预测中，虽然测试集表现良好，但时间泛化能力显著下降，且解释性方法可能在模型未泛化时仍显示可信特征重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估机器学习模型在农业产量预测中的泛化性能和可解释性，特别关注模型在时间独立验证中的表现以及解释性方法的可靠性。

Method: 使用高质量长期数据集，系统比较集成树模型（XGBoost、随机森林）和深度学习方法（LSTM、TCN）在空间分割测试集和时间独立验证年的表现，并分析SHAP特征重要性。

Result: 所有模型在空间分割测试集上表现良好，但在时间独立验证年性能显著下降；即使模型时间泛化能力弱，SHAP特征重要性仍可能显示可信结果，暴露了后验解释性方法的脆弱性。

Conclusion: 农业环境系统中需要验证感知的ML解释，特征重要性不能盲目接受，除非模型明确展示了对未见时空条件的泛化能力，需要领域感知验证、混合建模策略和更严格的解释性方法审查。

Abstract: This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).
  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.
  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?

</details>


### [72] [An Efficient Gradient-Based Inference Attack for Federated Learning](https://arxiv.org/abs/2512.15143)
*Pablo Montaña-Fernández,Ines Ortega-Fernandez*

Main category: cs.LG

TL;DR: 提出一种新的联邦学习梯度成员推断攻击，利用多层梯度的时间演化模式，无需访问私有数据集，可扩展到属性推断，在CIFAR-100等数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习减少了直接数据暴露，但模型更新的交换仍可能泄露敏感信息。现有攻击方法存在局限，需要探索基于多轮梯度时间演化的更有效攻击方法。

Method: 提出基于梯度的成员推断攻击，利用影子技术学习训练记录的多轮梯度模式，考虑半诚实和恶意敌手，可扩展到离散属性推断，模型无关且适用于分类和回归任务。

Result: 在CIFAR-100和Purchase100数据集上表现出强大的攻击性能，计算和内存开销与现有攻击相当。多轮联邦学习增加攻击脆弱性，聚合器威胁大于数据所有者，高维数据泄露更严重。

Conclusion: 多轮联邦学习增加了推理攻击的脆弱性，聚合器构成更大威胁，攻击性能受数据集特性影响显著，需要更强的隐私保护机制来防御此类基于梯度时间模式的攻击。

Abstract: Federated Learning is a machine learning setting that reduces direct data exposure, improving the privacy guarantees of machine learning models. Yet, the exchange of model updates between the participants and the aggregator can still leak sensitive information. In this work, we present a new gradient-based membership inference attack for federated learning scenarios that exploits the temporal evolution of last-layer gradients across multiple federated rounds. Our method uses the shadow technique to learn round-wise gradient patterns of the training records, requiring no access to the private dataset, and is designed to consider both semi-honest and malicious adversaries (aggregators or data owners). Beyond membership inference, we also provide a natural extension of the proposed attack to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses. The proposed attacks are model-agnostic, and therefore applicable to any gradient-based model and can be applied to both classification and regression settings. We evaluate the attack on CIFAR-100 and Purchase100 datasets for membership inference and on Breast Cancer Wisconsin for attribute inference. Our findings reveal strong attack performance and comparable computational and memory overhead in membership inference when compared to another attack from the literature. The obtained results emphasize that multi-round federated learning can increase the vulnerability to inference attacks, that aggregators pose a more substantial threat than data owners, and that attack performance is strongly influenced by the nature of the training dataset, with richer, high-dimensional data leading to stronger leakage than simpler tabular data.

</details>


### [73] [Understanding NTK Variance in Implicit Neural Representations](https://arxiv.org/abs/2512.15169)
*Chengguang Ou,Yixin Zhuang*

Main category: cs.LG

TL;DR: 该论文提出了一种统一框架，通过分析神经正切核（NTK）的特征值方差来解释不同隐式神经表示（INR）架构如何缓解频谱偏差。研究发现许多INR机制可以通过影响少量成对相似性因子和缩放项来理解，这些因素共同决定了NTK特征值方差。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）通常收敛缓慢且难以恢复高频细节，这归因于频谱偏差。虽然先前工作将此行为与神经正切核（NTK）联系起来，但具体架构选择如何影响NTK条件数仍不清楚。本文旨在揭示不同INR架构如何通过改善NTK条件数来缓解频谱偏差。

Method: 作者提出一个统一框架，将许多INR机制理解为通过影响少量成对相似性因子和缩放项来影响NTK特征值方差。推导了常见INR组件的闭式方差分解，分析了位置编码如何重塑输入相似性，球面归一化如何通过层间缩放减少方差，以及Hadamard调制如何引入严格小于1的额外相似性因子，从而实现乘性方差减少。

Result: 实验验证了预测的方差减少效果，并展示了更快速、更稳定的收敛以及改进的重建质量。该框架解释了为什么标准坐标MLP由于有限的输入特征交互而导致大的特征值分散和不良条件数，而各种INR架构通过改善NTK条件数来缓解频谱偏差。

Conclusion: 该研究提供了一个统一视角来解释不同INR架构如何通过影响NTK特征值方差来缓解频谱偏差。通过分析成对相似性因子和缩放项，可以理解位置编码、球面归一化和Hadamard调制等机制如何改善NTK条件数，从而实现更快的收敛和更好的重建质量。

Abstract: Implicit Neural Representations (INRs) often converge slowly and struggle to recover high-frequency details due to spectral bias. While prior work links this behavior to the Neural Tangent Kernel (NTK), how specific architectural choices affect NTK conditioning remains unclear. We show that many INR mechanisms can be understood through their impact on a small set of pairwise similarity factors and scaling terms that jointly determine NTK eigenvalue variance. For standard coordinate MLPs, limited input-feature interactions induce large eigenvalue dispersion and poor conditioning. We derive closed-form variance decompositions for common INR components and show that positional encoding reshapes input similarity, spherical normalization reduces variance via layerwise scaling, and Hadamard modulation introduces additional similarity factors strictly below one, yielding multiplicative variance reduction. This unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning. Experiments across multiple tasks confirm the predicted variance reductions and demonstrate faster, more stable convergence with improved reconstruction quality.

</details>


### [74] [DEER: Draft with Diffusion, Verify with Autoregressive Models](https://arxiv.org/abs/2512.15176)
*Zicong Cheng,Guo-Wei Yang,Jia Li,Zhijie Deng,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.LG

TL;DR: DEER提出了一种基于扩散大语言模型(dLLM)的推测解码框架，通过并行生成长段草稿来大幅提升推理速度，相比传统自回归草稿模型实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码使用自回归草稿模型存在两个根本问题：1) 逐步不确定性积累导致目标模型与草稿模型之间的信任逐渐崩溃；2) 自回归草稿模型固有的顺序解码特性。这些问题限制了加速效果。

Method: DEER采用扩散大语言模型(dLLM)作为草稿模型，通过两阶段训练管道将dLLM草稿模型与目标自回归模型对齐，并使用单步解码生成长段草稿，然后由自回归模型进行验证。

Result: DEER实现了高达32个token的草稿接受长度，远超EAGLE-3的10个token。在HumanEval测试中，使用Qwen3-30B-A3B模型时，DEER达到5.54倍加速，而EAGLE-3仅为2.41倍。

Conclusion: 扩散大语言模型草稿模型能够克服传统自回归草稿模型的根本限制，通过并行解码生成高质量长段草稿，显著提升推测解码的效率，为LLM驱动的智能系统提供了实用的加速方案。

Abstract: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/

</details>


### [75] [Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT](https://arxiv.org/abs/2512.15206)
*Liyu Zhang,Yejia Liu,Kwun Ho Liu,Runxi Huang,Xiaomin Ouyang*

Main category: cs.LG

TL;DR: Chorus：一种面向物联网的无数据上下文感知模型定制方法，通过跨模态重建学习上下文表示，动态平衡传感器与上下文贡献，在未见上下文条件下实现高效适应。


<details>
  <summary>Details</summary>
Motivation: 物联网应用中传感器数据收集环境多样且动态变化，传统领域适应方法通常忽略上下文信息或使用简单集成策略，难以处理部署后未见过的上下文变化。

Method: 1) 无监督跨模态重建：在未标记传感器数据和基于语言的上下文嵌入之间进行重建，正则化上下文嵌入空间以学习鲁棒表示；2) 轻量级门控头：在有限标记样本上训练，动态平衡传感器和上下文贡献；3) 上下文缓存机制：重用缓存的上下文表示，仅在检测到上下文变化时更新。

Result: 在IMU、语音和WiFi感知任务中，Chorus在未见上下文条件下比最先进基线方法提升达11.3%，同时在智能手机和边缘设备上保持可比的延迟。

Conclusion: Chorus通过有效的上下文表示学习和自适应集成策略，实现了在无目标域数据情况下的高效模型定制，能够处理部署后未见过的上下文变化，具有实际应用价值。

Abstract: In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.

</details>


### [76] [Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures](https://arxiv.org/abs/2512.15228)
*Songze Huo,Xiao-Ming Cao*

Main category: cs.LG

TL;DR: DBCata是一种深度生成模型，通过周期性布朗桥框架和等变图神经网络，直接从非弛豫结构生成DFT弛豫的吸附结构，无需能量或力信息，显著提升催化剂筛选的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习原子间势（MLIP）的训练数据主要来自近平衡结构，分布有限，导致吸附结构和吸附能预测不可靠，限制了大规模催化剂筛选的准确性。

Method: 结合周期性布朗桥框架与等变图神经网络，构建低维过渡流形，直接从非弛豫结构生成DFT弛豫结构；采用混合化学启发式和自监督异常检测方法识别和精炼异常预测。

Result: 在Catalysis-Hub数据集上，DBCata生成的吸附几何结构达到0.035Å的原子间距离平均绝对误差（DMAE），比当前最先进的机器学习势模型提高近三倍；94%的情况下DFT精度可在0.1 eV内改进。

Conclusion: DBCata在氧还原反应高效合金催化剂的高通量计算筛选中表现出色，证明了其作为催化剂设计和优化强大工具的潜力。

Abstract: The adsorption energy serves as a crucial descriptor for the large-scale screening of catalysts. Nevertheless, the limited distribution of training data for the extensively utilised machine learning interatomic potential (MLIP), predominantly sourced from near-equilibrium structures, results in unreliable adsorption structures and consequent adsorption energy predictions. In this context, we present DBCata, a deep generative model that integrates a periodic Brownian-bridge framework with an equivariant graph neural network to establish a low-dimensional transition manifold between unrelaxed and DFT-relaxed structures, without requiring explicit energy or force information. Upon training, DBCata effectively generates high-fidelity adsorption geometries, achieving an interatomic distance mean absolute error (DMAE) of 0.035 \textÅ on the Catalysis-Hub dataset, which is nearly three times superior to that of the current state-of-the-art machine learning potential models. Moreover, the corresponding DFT accuracy can be improved within 0.1 eV in 94\% of instances by identifying and refining anomalous predictions through a hybrid chemical-heuristic and self-supervised outlier detection approach. We demonstrate that the remarkable performance of DBCata facilitates accelerated high-throughput computational screening for efficient alloy catalysts in the oxygen reduction reaction, highlighting the potential of DBCata as a powerful tool for catalyst design and optimisation.

</details>


### [77] [Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis](https://arxiv.org/abs/2512.15250)
*Youssef Ghallab,Omar Iraqy,Mohamed Kandil,Mohamed Ashraf,Saadeldine Eletter,Morougue Ghazal,Ayman Khalafallah,Nagwa El-Makky*

Main category: cs.LG

TL;DR: 提出一种基于自监督预训练的多模态生理信号融合方法，通过专门设计的ECG编码器和预训练的EEG编码器，结合简单嵌入拼接实现有效跨模态学习，在情绪识别任务上达到接近SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）和脑电图（EEG）等生理信号提供了对人体健康和认知的互补洞察，但多模态整合面临挑战：多模态标注数据有限，且不同模态之间存在特异性差异。

Method: 1. 采用CBraMod编码器进行大规模自监督ECG预训练，引入双掩码策略捕捉导联内和导联间依赖关系
2. 使用预训练的CBraMod编码器处理EEG信号
3. 为ECG预训练对称编码器，使每个模态都获得丰富的基础表示
4. 通过简单的嵌入拼接融合这些表示，让分类头学习跨模态交互

Result: 在情绪识别任务上实现了接近最先进的性能，表明精心设计的生理信号编码器即使采用简单的融合方法也能显著提升下游任务性能。

Conclusion: 基础模型方法能够利用生理信号的整体特性，为医疗保健和情感计算提供可扩展、标签高效且可泛化的解决方案。

Abstract: Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.

</details>


### [78] [Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory](https://arxiv.org/abs/2512.15267)
*Huiyan Xue,Xuming Ran,Yaxin Li,Qi Xu,Enhui Li,Yi Xu,Qiang Zhang*

Main category: cs.LG

TL;DR: SSD是一种结构引导的持续学习框架，通过选择性子网络蒸馏在稀疏神经网络中实现跨任务知识重用，提升准确率和表示覆盖度。


<details>
  <summary>Details</summary>
Motivation: 稀疏神经网络在持续学习中具有模块化和低干扰优势，但现有方法如SDMLP的刚性模块化限制了跨任务知识重用，导致高稀疏度下性能下降。

Method: 提出选择性子网络蒸馏(SSD)，将蒸馏视为拓扑对齐的信息通道而非正则化器。通过识别高激活频率神经元，在先前Top-K子网络和输出logits中选择性蒸馏知识，无需重放或任务标签。

Result: 在Split CIFAR-10、CIFAR-100和MNIST上的实验表明，SSD提高了准确率、保留率和表示覆盖度。

Conclusion: SSD为稀疏持续学习提供了结构基础解决方案，实现了结构重新对齐同时保持稀疏模块化。

Abstract: Sparse neural systems are gaining traction for efficient continual learning due to their modularity and low interference. Architectures such as Sparse Distributed Memory Multi-Layer Perceptrons (SDMLP) construct task-specific subnetworks via Top-K activation and have shown resilience against catastrophic forgetting. However, their rigid modularity limits cross-task knowledge reuse and leads to performance degradation under high sparsity. We propose Selective Subnetwork Distillation (SSD), a structurally guided continual learning framework that treats distillation not as a regularizer but as a topology-aligned information conduit. SSD identifies neurons with high activation frequency and selectively distills knowledge within previous Top-K subnetworks and output logits, without requiring replay or task labels. This enables structural realignment while preserving sparse modularity. Experiments on Split CIFAR-10, CIFAR-100, and MNIST demonstrate that SSD improves accuracy, retention, and representation coverage, offering a structurally grounded solution for sparse continual learning.

</details>


### [79] [Topological Metric for Unsupervised Embedding Quality Evaluation](https://arxiv.org/abs/2512.15285)
*Aleksei Shestov,Anton Klenitskiy,Daria Denisova,Amurkhan Dzagkoev,Daniil Petrovich,Andrey Savchenko,Maksim Makarenko*

Main category: cs.LG

TL;DR: 提出Persistence：基于持续同调的无监督度量，用于评估嵌入空间的质量，无需标签即可量化几何结构和拓扑丰富性


<details>
  <summary>Details</summary>
Motivation: 现代表示学习依赖于大规模无标签数据的无监督和自监督方法，但这些方法在无标签情况下评估嵌入质量仍然是一个开放挑战

Method: 提出Persistence度量，基于持续同调（persistent homology），能够捕捉嵌入空间的全局和多尺度组织结构，不依赖线性可分性或协方差结构假设

Result: 在多个领域的实证结果表明，Persistence在下游性能相关性方面始终达到顶级水平，优于现有无监督度量，能够实现可靠的模型和超参数选择

Conclusion: Persistence为无监督表示学习提供了一种有效的嵌入质量评估方法，能够捕捉嵌入空间的几何和拓扑结构，为模型选择和优化提供可靠依据

Abstract: Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.

</details>


### [80] [Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions](https://arxiv.org/abs/2512.15286)
*Siva Sai,Ishika Goyal,Shubham Sharma,Sri Harshita Manuri,Vinay Chamola,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 本文是一篇关于量子机器学习在网络安全领域应用的综述论文，系统梳理了QML技术（如QNNs、QSVMs、VQCs、QGANs）在入侵检测、恶意软件分类等安全任务中的应用，并讨论了当前局限性和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习、规则和基于签名的防御策略已无法应对日益增长的网络安全威胁和大量数据，量子机器学习作为新兴技术，利用量子力学计算在处理高维结构方面具有优势，有望提升网络安全防御能力。

Method: 采用综述研究方法，全面梳理量子机器学习技术在安全领域的应用，包括量子神经网络、量子支持向量机、变分量子电路、量子生成对抗网络等，并将这些方法映射到监督学习、无监督学习和生成学习范式，以及具体的网络安全任务。

Result: 论文系统总结了QML在网络安全各领域的应用现状，包括入侵和异常检测、恶意软件和僵尸网络分类、加密流量分析等，特别讨论了在云计算安全中的应用潜力，同时指出了当前QML在网络安全领域存在的局限性。

Conclusion: 量子机器学习在网络安全领域具有重要应用前景，能够增强安全性和可扩展性，但目前仍存在诸多技术限制，需要进一步研究解决，本文为未来研究方向提供了指导。

Abstract: The increasing number of cyber threats and rapidly evolving tactics, as well as the high volume of data in recent years, have caused classical machine learning, rules, and signature-based defence strategies to fail, rendering them unable to keep up. An alternative, Quantum Machine Learning (QML), has recently emerged, making use of computations based on quantum mechanics. It offers better encoding and processing of high-dimensional structures for certain problems. This survey provides a comprehensive overview of QML techniques relevant to the domain of security, such as Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), and Quantum Generative Adversarial Networks (QGANs), and discusses the contributions of this paper in relation to existing research in the field and how it improves over them. It also maps these methods across supervised, unsupervised, and generative learning paradigms, and to core cybersecurity tasks, including intrusion and anomaly detection, malware and botnet classification, and encrypted-traffic analytics. It also discusses their application in the domain of cloud computing security, where QML can enhance secure and scalable operations. Many limitations of QML in the domain of cybersecurity have also been discussed, along with the directions for addressing them.

</details>


### [81] [Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference](https://arxiv.org/abs/2512.15335)
*Chenxiang Zhang,Tongxi Qu,Zhong Li,Tian Zhang,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 量化技术可降低深度神经网络的隐私泄露风险，低精度模型相比全精度模型在成员推理攻击中的脆弱性可降低一个数量级，但会牺牲部分模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有隐私分析主要关注全精度模型，量化技术通过降低参数数值精度来减少内存和计算成本，但量化如何影响隐私泄露尚未得到系统研究。

Method: 使用成员推理攻击作为评估框架，分析三种流行的后训练量化算法（AdaRound、BRECQ、OBC），在多个精度级别（4位、2位、1.58位）和数据集（CIFAR-10、CIFAR-100、TinyImageNet）上进行系统研究。

Result: 低精度后训练量化可显著减少隐私泄露，相比全精度模型，低精度模型在成员推理攻击中的脆弱性最多可降低一个数量级，但会降低模型效用。在1.58位量化级别上，仅对最后一层进行高精度量化可实现隐私-效用权衡的精细控制。

Conclusion: 量化技术为实际部署中平衡效率、效用和隐私保护提供了可行方案，低精度模型能有效减少隐私泄露，同时通过分层量化策略可实现更精细的隐私-效用权衡。

Abstract: Deep neural networks are widely deployed with quantization techniques to reduce memory and computational costs by lowering the numerical precision of their parameters. While quantization alters model parameters and their outputs, existing privacy analyses primarily focus on full-precision models, leaving a gap in understanding how bit-width reduction can affect privacy leakage. We present the first systematic study of the privacy-utility relationship in post-training quantization (PTQ), a versatile family of methods that can be applied to pretrained models without further training. Using membership inference attacks as our evaluation framework, we analyze three popular PTQ algorithms-AdaRound, BRECQ, and OBC-across multiple precision levels (4-bit, 2-bit, and 1.58-bit) on CIFAR-10, CIFAR-100, and TinyImageNet datasets. Our findings consistently show that low-precision PTQs can reduce privacy leakage. In particular, lower-precision models demonstrate up to an order of magnitude reduction in membership inference vulnerability compared to their full-precision counterparts, albeit at the cost of decreased utility. Additional ablation studies on the 1.58-bit quantization level show that quantizing only the last layer at higher precision enables fine-grained control over the privacy-utility trade-off. These results offer actionable insights for practitioners to balance efficiency, utility, and privacy protection in real-world deployments.

</details>


### [82] [EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning](https://arxiv.org/abs/2512.15405)
*Jianfei Ma,Wee Sun Lee*

Main category: cs.LG

TL;DR: 提出EUBRL算法，利用认知不确定性指导探索，在无限时域折扣MDP中实现接近极小极大最优的遗憾和样本复杂度保证


<details>
  <summary>Details</summary>
Motivation: 智能体在已知与未知边界面临探索-利用困境，认知不确定性反映了知识有限导致的系统性不确定性，需要一种能利用认知指导实现原则性探索的强化学习算法

Method: 提出贝叶斯强化学习算法EUBRL，利用认知不确定性指导探索，自适应减少由估计误差引起的每步遗憾，适用于具有充分表达能力先验的无限时域折扣MDP

Result: 理论证明：在无限时域折扣MDP中，对于一类充分表达的先验，建立了接近极小极大最优的遗憾和样本复杂度保证；实证评估：在稀疏奖励、长时域和随机性任务中，EUBRL表现出优越的样本效率、可扩展性和一致性

Conclusion: EUBRL算法通过认知不确定性指导实现了原则性探索，在理论和实证上都取得了优越性能，为解决强化学习中的探索-利用困境提供了有效方法

Abstract: At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.

</details>


### [83] [FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows](https://arxiv.org/abs/2512.15420)
*Yeonwoo Cha,Semin Kim,Jinhyeon Kwon,Seunghoon Hong*

Main category: cs.LG

TL;DR: FlowBind：一种基于流匹配的高效任意模态转换框架，通过共享潜在空间和模态特定可逆流，大幅减少参数和训练时间


<details>
  <summary>Details</summary>
Motivation: 现有基于流的任意模态转换方法存在效率低下问题：需要大规模数据集且有配对限制、建模联合分布计算成本高、依赖复杂的多阶段训练

Method: 学习一个捕获跨模态信息的共享潜在空间，通过模态特定的可逆流将每个模态桥接到该潜在空间，在单一流匹配目标下联合优化

Result: 在文本、图像和音频模态上，FlowBind达到可比质量的同时，参数减少6倍，训练速度提升10倍

Conclusion: FlowBind通过因子化交互到共享潜在空间，实现了高效、灵活的任意模态转换，显著降低了数据需求和计算成本

Abstract: Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.

</details>


### [84] [Statistics of Min-max Normalized Eigenvalues in Random Matrices](https://arxiv.org/abs/2512.15427)
*Hyakka Nakada,Shu Tanaka*

Main category: cs.LG

TL;DR: 研究随机矩阵中min-max归一化特征值的统计特性，推导其累积分布的标度律和矩阵分解的残差误差，并通过数值实验验证理论预测。


<details>
  <summary>Details</summary>
Motivation: 随机矩阵理论在纯数学、数学物理和机器学习中都很重要。从数据科学实践角度看，输入数据通常需要归一化处理，因此研究随机矩阵中min-max归一化特征值的统计特性具有实际意义。

Method: 应用先前提出的归一化特征值有效分布，推导累积分布的标度律和矩阵分解的残差误差，并通过数值实验验证理论预测。

Result: 推导出了归一化特征值累积分布的标度律和矩阵分解的残差误差，数值实验结果验证了这些理论预测的正确性。

Conclusion: 该研究为随机矩阵中归一化特征值的统计特性提供了理论框架和数值验证，对数据科学中的归一化处理具有实际指导意义。

Abstract: Random matrix theory has played an important role in various areas of pure mathematics, mathematical physics, and machine learning. From a practical perspective of data science, input data are usually normalized prior to processing. Thus, this study investigates the statistical properties of min-max normalized eigenvalues in random matrices. Previously, the effective distribution for such normalized eigenvalues has been proposed. In this study, we apply it to evaluate a scaling law of the cumulative distribution. Furthermore, we derive the residual error that arises during matrix factorization of random matrices. We conducted numerical experiments to verify these theoretical predictions.

</details>


### [85] [FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments](https://arxiv.org/abs/2512.15430)
*Quanxi Zhou,Wencan Mao,Manabu Tsukada,John C. S. Lui,Yusheng Ji*

Main category: cs.LG

TL;DR: 提出FM-EAC算法，结合基于模型和无模型强化学习，通过特征模型和增强的actor-critic框架提升多任务控制的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代强化学习方法在跨任务和场景的有效迁移性方面仍存在困难，需要一种能结合基于模型和无模型强化学习优势的通用算法。

Method: 提出FM-EAC算法，整合规划、行动和学习，采用基于特征的模型和增强的actor-critic框架，支持根据用户需求定制子网络。

Result: 在城市和农业应用的仿真中，FM-EAC持续优于多种最先进的基于模型和无模型强化学习方法。

Conclusion: FM-EAC成功结合了基于模型和无模型强化学习的优势，通过灵活的架构设计提升了多任务控制的泛化能力和适应性。

Abstract: Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.

</details>


### [86] [Double Horizon Model-Based Policy Optimization](https://arxiv.org/abs/2512.15439)
*Akihiro Kubo,Paavo Parmas,Shin Ishii*

Main category: cs.LG

TL;DR: DHMBPO提出双视野模型强化学习方法，通过长分布视野和短训练视野解决模型偏差、分布偏移和梯度不稳定性的平衡问题


<details>
  <summary>Details</summary>
Motivation: 基于模型的强化学习(MBRL)中，视野长度选择面临两难：长视野能更好保持同策略训练但会放大模型偏差；长视野能减少价值估计偏差但会增加策略梯度方差。这两个最优视野可能不同，需要解决这一冲突。

Method: 提出双视野模型策略优化(DHMBPO)，将视野过程分为长"分布视野"(DR)和短"训练视野"(TR)。DR生成同策略状态样本以缓解分布偏移；短TR利用可微分转移提供准确的价值梯度估计，实现稳定梯度更新。

Result: 双视野方法能有效平衡分布偏移、模型偏差和梯度不稳定性，在连续控制基准测试中超越了现有MBRL方法，在样本效率和运行时间方面都有优势。

Conclusion: DHMBPO通过分离分布视野和训练视野，解决了MBRL中视野长度选择的根本冲突，实现了更好的性能平衡，为模型强化学习提供了有效的解决方案。

Abstract: Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long "distribution rollout" (DR) and a short "training rollout" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.

</details>


### [87] [Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting](https://arxiv.org/abs/2512.15442)
*Neeraj Sarna,Yuanyuan Li,Michael von Gablenz*

Main category: cs.LG

TL;DR: 该研究探索使用思维链和任务指令提示，结合负向提示和提示重写，来减少文本到图像生成模型中的版权内容生成。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像生成模型可能会记忆并复制其训练数据中的受版权保护内容，这可能导致版权侵权风险，给AI用户和开发者带来法律和财务损失。

Method: 结合思维链和任务指令提示，加上两种版权缓解策略：负向提示和提示重写，研究生成图像与版权图像的相似度及其与用户输入的相关性。

Result: 在不同复杂度的模型上进行数值实验，提供了关于这些技术在减少版权内容生成方面有效性的见解。

Conclusion: 思维链和任务指令提示结合负向提示和提示重写等技术，有望减少文本到图像生成中的版权内容生成，但效果可能因模型复杂度而异。

Abstract: Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.

</details>


### [88] [From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2512.15460)
*Xiangrui Xu,Zhize Li,Yufei Han,Bin Wang,Jiqiang Liu,Wei Wang*

Main category: cs.LG

TL;DR: 提出Invertibility Loss (InvLoss)量化联邦学习中数据重建攻击的最大风险，推导其可计算上界，并开发InvRE风险估计器和自适应噪声防御方法。


<details>
  <summary>Details</summary>
Motivation: 数据重建攻击对联邦学习系统构成严重威胁，但缺乏理论基础的量化框架来评估风险。现有研究未能解决如何系统表征和评估FL系统中DRA风险的问题。

Method: 引入Invertibility Loss (InvLoss)量化DRA最大有效性，推导其紧致可计算上界。从三个角度探索：1) 证明DRA风险由Jacobian矩阵谱特性决定；2) 开发InvRE风险估计器；3) 提出两种自适应噪声扰动防御方法。

Result: 在真实数据集上的大量实验验证了该框架的有效性，证明其能够系统评估和缓解FL系统中的DRA风险，同时保持分类准确性。

Conclusion: 该工作填补了联邦学习数据重建攻击风险量化框架的空白，为系统评估和缓解DRA风险提供了理论基础和实用工具，有助于增强FL系统的隐私保护。

Abstract: Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.

</details>


### [89] [Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance](https://arxiv.org/abs/2512.15469)
*Ioannis Kalogeropoulos,Giorgos Bouritsas,Yannis Panagakis*

Main category: cs.LG

TL;DR: 提出一种通过图元网络编辑神经网络的方法，可在不牺牲性能的情况下高效满足各种需求（如公平性、数据最小化、权重剪枝等）。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在关键领域部署时需满足多种需求（合规性、公平性、计算约束等），但现有方法（后处理或重新训练）往往效率低下或损害性能，需要一种高效编辑模型而不牺牲效用的方法。

Method: 提出统一框架，使用图元网络作为编辑器，该元网络在神经网络群体上训练，通过最小化目标函数（包含需求满足项和效用保持项）来学习编辑神经网络，编辑过程只需一次推理步骤。

Result: 在多个任务（数据最小化原则、偏差缓解、权重剪枝）上实验，相比传统后处理或重新训练方法，在性能、需求满足和时间效率之间取得了更好的平衡。

Conclusion: 图元网络编辑方法能够高效地修改神经网络以满足各种需求，同时保持模型性能，为解决机器学习模型部署中的多重要求问题提供了有效途径。

Abstract: As machine learning models are increasingly deployed in high-stakes settings, e.g. as decision support systems in various societal sectors or in critical infrastructure, designers and auditors are facing the need to ensure that models satisfy a wider variety of requirements (e.g. compliance with regulations, fairness, computational constraints) beyond performance. Although most of them are the subject of ongoing studies, typical approaches face critical challenges: post-processing methods tend to compromise performance, which is often counteracted by fine-tuning or, worse, training from scratch, an often time-consuming or even unavailable strategy. This raises the following question: "Can we efficiently edit models to satisfy requirements, without sacrificing their utility?" In this work, we approach this with a unifying framework, in a data-driven manner, i.e. we learn to edit neural networks (NNs), where the editor is an NN itself - a graph metanetwork - and editing amounts to a single inference step. In particular, the metanetwork is trained on NN populations to minimise an objective consisting of two terms: the requirement to be enforced and the preservation of the NN's utility. We experiment with diverse tasks (the data minimisation principle, bias mitigation and weight pruning) improving the trade-offs between performance, requirement satisfaction and time efficiency compared to popular post-processing or re-training alternatives.

</details>


### [90] [Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs](https://arxiv.org/abs/2512.15483)
*Luca Torresi,Pascal Friederich*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯优化的扩展方法，能够灵活处理多阶段实验流程并利用中间观测值（代理测量）进行决策，相比传统贝叶斯优化在寻找最优解的速度和质量上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前自驱动实验室广泛使用贝叶斯优化，但传统方法只能处理固定实验流程，无法根据中间测量结果动态调整实验计划。许多真实实验需要简化才能适应现有框架，这限制了自驱动实验室处理复杂实验流程的能力。

Method: 提出贝叶斯优化的扩展方法，支持多阶段工作流的灵活采样，能够基于中间观测值（代理测量）做出最优决策。该方法允许在实验过程中根据中间结果动态调整后续操作。

Result: 在多种场景下，利用代理测量的方法相比传统贝叶斯优化（仅观察最终测量结果）有显著改进：不仅找到良好解决方案的时间更短，而且找到的解决方案整体最优性也更高。

Conclusion: 该方法不仅为自驱动实验室使用更复杂、更真实的实验流程铺平了道路，还能平滑地结合模拟和实验，推动下一代自驱动实验室的发展。

Abstract: Self-driving laboratories (SDLs) are combining recent technological advances in robotics, automation, and machine learning based data analysis and decision-making to perform autonomous experimentation toward human-directed goals without requiring any direct human intervention. SDLs are successfully used in materials science, chemistry, and beyond, to optimise processes, materials, and devices in a systematic and data-efficient way. At present, the most widely used algorithm to identify the most informative next experiment is Bayesian optimisation. While relatively simple to apply to a wide range of optimisation problems, standard Bayesian optimisation relies on a fixed experimental workflow with a clear set of optimisation parameters and one or more measurable objective functions. This excludes the possibility of making on-the-fly decisions about changes in the planned sequence of operations and including intermediate measurements in the decision-making process. Therefore, many real-world experiments need to be adapted and simplified to be converted to the common setting in self-driving labs. In this paper, we introduce an extension to Bayesian optimisation that allows flexible sampling of multi-stage workflows and makes optimal decisions based on intermediate observables, which we call proxy measurements. We systematically compare the advantage of taking into account proxy measurements over conventional Bayesian optimisation, in which only the final measurement is observed. We find that over a wide range of scenarios, proxy measurements yield a substantial improvement, both in the time to find good solutions and in the overall optimality of found solutions. This not only paves the way to use more complex and thus more realistic experimental workflows in autonomous labs but also to smoothly combine simulations and experiments in the next generation of SDLs.

</details>


### [91] [Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier](https://arxiv.org/abs/2512.15492)
*Adrián Detavernier,Jasper De Bock*

Main category: cs.LG

TL;DR: 该论文比较了评估分类器个体预测可靠性的两种方法：鲁棒性量化(RQ)和不确定性量化(UQ)，发现两者互补，结合后的混合方法优于单独使用任一种方法。


<details>
  <summary>Details</summary>
Motivation: 当前评估分类器预测可靠性的方法主要分为鲁棒性量化(RQ)和不确定性量化(UQ)两种概念不同的方法，需要系统比较它们的优劣并探索可能的结合方式。

Method: 在多个基准数据集上系统比较RQ和UQ方法，分析它们的性能差异，并提出将两者结合的混合方法。

Result: 实验表明RQ和UQ没有明确的优劣之分，但两者具有互补性。结合两者的混合方法在评估预测可靠性方面优于单独使用RQ或UQ。此外，研究还能评估每个数据集中不确定性和鲁棒性作为不可靠性来源的相对重要性。

Conclusion: RQ和UQ是评估分类器预测可靠性的互补方法，结合两者的混合方法能提供更优的可靠性评估。研究还提供了评估不同不可靠性来源相对重要性的框架。

Abstract: We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.

</details>


### [92] [Tracking Temporal Dynamics of Vector Sets with Gaussian Process](https://arxiv.org/abs/2512.15538)
*Taichi Aida,Mamoru Komachi,Toshinobu Ogiso,Hiroya Takamura,Daichi Mochihashi*

Main category: cs.LG

TL;DR: 提出一种基于无限维高斯过程和随机傅里叶特征的方法，用于建模和分析随时间变化的向量集分布，能够捕捉跨领域（如犯罪分布和词嵌入）的时序动态。


<details>
  <summary>Details</summary>
Motivation: 许多领域（生态学、犯罪分析、语言学）都需要分析随时间演变的向量集，但这些数据结构复杂且随时间变化，现有方法难以有效建模和比较这些时序动态。

Method: 使用无限维高斯过程建模每个时间点向量集的分布，通过随机傅里叶特征近似高斯过程中的隐函数，获得紧凑且可比较的时序向量表示，从而在低维空间中追踪和可视化向量集的演变。

Result: 方法在犯罪分布数据和词嵌入数据上验证有效，能够捕捉时序动态，提供可解释且稳健的表示，为跨领域时序向量集的结构变化分析提供了强大框架。

Conclusion: 提出的方法为分析随时间变化的向量集提供了一种新颖有效的框架，能够捕捉复杂的时序动态，在多个应用领域展现出良好的解释性和实用性。

Abstract: Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.

</details>


### [93] [Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation](https://arxiv.org/abs/2512.15574)
*Yuxin Cai,Yanyong Huang,Jinyuan Chang,Dongjie Wang,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出JUICE方法，联合学习无监督多视图特征和实例协同选择与跨视图补全，解决不完整多视图数据的特征和实例选择问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不完整多视图数据时，通常先补全缺失数据，然后将所有视图拼接进行协同选择，这种分离策略忽略了补全和选择之间的潜在交互作用，且简单拼接多视图数据无法捕捉视图间的互补信息。

Method: 提出JUICE方法：1）使用可用观测重建不完整多视图数据，将缺失数据恢复与特征实例协同选择统一在一个框架中；2）利用跨视图邻域信息学习样本间关系，在重建过程中进一步优化缺失值补全。

Result: 大量实验表明，JUICE在特征和实例协同选择任务上优于现有最先进方法。

Conclusion: JUICE通过联合学习缺失数据补全和特征实例协同选择，能够选择更具代表性的特征和实例，有效解决了不完整多视图数据的协同选择问题。

Abstract: Feature and instance co-selection, which aims to reduce both feature dimensionality and sample size by identifying the most informative features and instances, has attracted considerable attention in recent years. However, when dealing with unlabeled incomplete multi-view data, where some samples are missing in certain views, existing methods typically first impute the missing data and then concatenate all views into a single dataset for subsequent co-selection. Such a strategy treats co-selection and missing data imputation as two independent processes, overlooking potential interactions between them. The inter-sample relationships gleaned from co-selection can aid imputation, which in turn enhances co-selection performance. Additionally, simply merging multi-view data fails to capture the complementary information among views, ultimately limiting co-selection effectiveness. To address these issues, we propose a novel co-selection method, termed Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE). JUICE first reconstructs incomplete multi-view data using available observations, bringing missing data recovery and feature and instance co-selection together in a unified framework. Then, JUICE leverages cross-view neighborhood information to learn inter-sample relationships and further refine the imputation of missing values during reconstruction. This enables the selection of more representative features and instances. Extensive experiments demonstrate that JUICE outperforms state-of-the-art methods.

</details>


### [94] [Corrective Diffusion Language Models](https://arxiv.org/abs/2512.15596)
*Shuibai Zhang,Fred Zhangzhi Peng,Yiheng Zhang,Jin Pan,Grigorios G. Chrysos*

Main category: cs.LG

TL;DR: 扩散语言模型在迭代错误修正方面具有结构优势，但传统训练方法无法可靠地诱导纠错行为。本文提出了一种面向纠错的后训练方法，显著提升了模型在错误定位和修正方面的能力。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型因其非因果去噪动态特性，理论上适合进行迭代错误修正，但传统的掩码扩散语言模型训练方法无法可靠地诱导这种纠错行为。模型往往无法识别完整输入中的不可靠标记，导致基于置信度的细化方法失效。

Method: 提出了一种面向纠错的后训练原则，明确监督可见的错误标记，使模型能够进行错误感知的置信度评估和针对性修正。同时引入了代码修正基准（CRB），这是一个可控且可执行的基准，用于评估错误定位和原地修正能力。

Result: 实验表明，采用本文方法的模型在代码修正任务和受控设置中，纠错能力显著优于标准掩码扩散语言模型，同时纯完成任务的性能也有所提升。

Conclusion: 扩散语言模型具有迭代修正的结构潜力，但需要专门的训练方法才能实现有效的纠错行为。提出的纠错导向后训练方法能够显著提升模型的错误定位和修正能力，为扩散语言模型的实际应用提供了重要改进。

Abstract: Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.

</details>


### [95] [How Smoothing is N-simplicial Attention?](https://arxiv.org/abs/2512.15600)
*Alexandre Dussolle,Pietro Liò*

Main category: cs.LG

TL;DR: 提出N-单纯形注意力，从成对token相似性扩展到高阶交互，适配RoPE位置编码，并引入成本有效的单纯形选择机制来管理计算复杂度


<details>
  <summary>Details</summary>
Motivation: 从纯MLP到可学习的图消息传递机制（如GATs或Transformers）已经取得了最先进的结果，但存在计算权衡。为了进一步推进，需要从成对token相似性扩展到高阶交互

Method: 引入N-单纯形注意力机制，适配Rotary Position Embeddings（RoPE），并提出成本有效的单纯形选择机制来聚焦计算资源到任务敏感的高阶交互

Result: 通过推导Lipschitz上界证明N-单纯形注意力的平滑性，并发现尽管开启了高阶交互的消息传递，该机制本身也会遭受过平滑问题

Conclusion: N-单纯形注意力将注意力机制从成对交互扩展到高阶交互，通过单纯形选择管理计算复杂度，但需要注意过平滑问题

Abstract: Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.

</details>


### [96] [Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary](https://arxiv.org/abs/2512.15614)
*Xinshun Feng,Mingzhe Liu,Yi Qiao,Tongyu Zhu,Leilei Sun,Shuai Wang*

Main category: cs.LG

TL;DR: BEAT是一个统一的、可迁移的推荐框架，它将用户和物品行为转化为离散、可解释的序列，通过向量量化自编码构建行为词汇表，解耦宏观兴趣和微观意图，并将行为token嵌入冻结语言模型的输入空间，提升零样本推荐性能并生成连贯的解释。


<details>
  <summary>Details</summary>
Motivation: 现有可解释推荐方法通常依赖基于ID的表征，这会掩盖语义含义并对语言模型施加结构约束，限制了在开放场景中的适用性。真实世界交互的复杂性使得多样的用户意图相互纠缠，协作信号很少与语言语义对齐。

Method: 提出BEAT框架：1）通过向量量化自编码过程构建行为词汇表，从图基表征中解耦宏观兴趣和微观意图；2）引入多级语义监督来桥接行为信号和语言空间；3）设计语义对齐正则化机制，将行为token直接嵌入冻结语言模型的输入空间。

Result: 在三个公共数据集上的实验表明，BEAT提高了零样本推荐性能，同时生成连贯且信息丰富的解释。进一步分析显示，行为token能够捕捉细粒度语义，并为将复杂行为模式集成到大型语言模型中提供了即插即用接口。

Conclusion: BEAT通过将行为转化为离散、可解释的token序列，有效解决了现有可解释推荐方法的局限性，实现了语义对齐和可迁移性，为将复杂行为模式集成到大型语言模型中提供了新途径。

Abstract: Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.

</details>


### [97] [SoFlow: Solution Flow Models for One-Step Generative Modeling](https://arxiv.org/abs/2512.15657)
*Tianze Luo,Haotian Yuan,Zhuang Liu*

Main category: cs.LG

TL;DR: SoFlow模型通过分析速度函数与解函数的关系，提出Flow Matching损失和解一致性损失，实现从零开始的一步生成，无需计算Jacobian-vector product，在ImageNet 256×256上优于MeanFlow模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和Flow Matching模型的多步去噪过程存在效率问题，这促使研究者探索少步生成方法。本文旨在解决从零开始的一步生成问题。

Method: 提出Solution Flow Models (SoFlow)框架，通过分析速度ODE中速度函数与解函数的关系，设计Flow Matching损失和解一致性损失来训练模型。Flow Matching损失支持训练中的Classifier-Free Guidance，一致性损失无需计算Jacobian-vector product。

Result: 在ImageNet 256×256数据集上，使用相同DiT架构和训练轮数从头训练时，SoFlow模型比MeanFlow模型获得更好的FID-50K分数。

Conclusion: SoFlow框架成功实现了高效的一步生成，通过创新的损失函数设计避免了计算复杂的Jacobian-vector product，在图像生成质量上超越了现有方法。

Abstract: The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.

</details>


### [98] [A Multivariate Statistical Framework for Detection, Classification and Pre-localization of Anomalies in Water Distribution Networks](https://arxiv.org/abs/2512.15685)
*Oleg Melnikov,Yurii Dorofieiev,Yurii Shakhnovskiy,Huy Truong,Victoria Degeler*

Main category: cs.LG

TL;DR: 提出SICAMS框架，使用多元统计分析检测、分类和初步定位供水管网异常，无需校准水力模型。


<details>
  <summary>Details</summary>
Motivation: 供水管网中的异常（如泄漏、传感器故障）检测和定位对水资源管理至关重要，但传统方法通常需要校准的水力模型，限制了实际应用。

Method: 通过白化变换消除压力流量数据的空间相关性，构建Hotelling's T²统计量进行异常检测，开发启发式算法对异常分类，并提出基于统计贡献度和拉普拉斯插值的粗定位方法。

Result: 在BattLeDIM L-Town基准数据集上验证，显示高灵敏度和可靠性，即使在多泄漏情况下也能保持鲁棒性能，并能通过回归模型近似估计水量损失。

Conclusion: SICAMS框架为供水管网异常管理提供了有效的统计方法，无需校准水力模型即可在实际操作环境中应用，具有实用价值。

Abstract: This paper presents a unified framework, for the detection, classification, and preliminary localization of anomalies in water distribution networks using multivariate statistical analysis. The approach, termed SICAMS (Statistical Identification and Classification of Anomalies in Mahalanobis Space), processes heterogeneous pressure and flow sensor data through a whitening transformation to eliminate spatial correlations among measurements. Based on the transformed data, the Hotelling's $T^2$ statistic is constructed, enabling the formulation of anomaly detection as a statistical hypothesis test of network conformity to normal operating conditions. It is shown that Hotelling's $T^2$ statistic can serve as an integral indicator of the overall "health" of the system, exhibiting correlation with total leakage volume, and thereby enabling approximate estimation of water losses via a regression model. A heuristic algorithm is developed to analyze the $T^2$ time series and classify detected anomalies into abrupt leaks, incipient leaks, and sensor malfunctions. Furthermore, a coarse leak localization method is proposed, which ranks sensors according to their statistical contribution and employs Laplacian interpolation to approximate the affected region within the network. Application of the proposed framework to the BattLeDIM L-Town benchmark dataset demonstrates high sensitivity and reliability in leak detection, maintaining robust performance even under multiple leaks. These capabilities make the method applicable to real-world operational environments without the need for a calibrated hydraulic model.

</details>


### [99] [Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2512.15687)
*Zhenwen Liang,Sidi Lu,Wenhao Yu,Kishan Panaganti,Yujun Zhou,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: G2RL是一种基于梯度引导的强化学习框架，利用模型自身的梯度更新几何来指导探索，而非依赖外部启发式方法，在数学和推理基准测试中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的探索机制（如熵奖励和外部语义比较器）与大型语言模型的实际学习方式存在根本性不匹配，它们鼓励表面变化但不能保证采样轨迹在优化方向上产生差异。

Method: G2RL框架从模型最终层的敏感性构建序列级特征（可通过标准前向传播低成本获得），通过比较采样组内这些特征来衡量每个轨迹如何重塑策略。引入新颖梯度方向的轨迹获得有界乘法奖励缩放，冗余或离流形更新则被弱化。

Result: 在Qwen3基础1.7B和4B模型上，G2RL在MATH500、AMC、AIME24、AIME25、GPQA、MMLUpro等数学和通用推理基准测试中，pass@1、maj@16和pass@k指标均一致优于基于熵的GRPO和外部嵌入方法。

Conclusion: 策略自身的更新空间为大型语言模型强化学习中的探索提供了更忠实有效的基础，G2RL能够扩展探索到更多正交且常为对立的梯度方向，同时保持语义连贯性。

Abstract: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.

</details>


### [100] [FrontierCS: Evolving Challenges for Evolving Intelligence](https://arxiv.org/abs/2512.15699)
*Qiuyang Mang,Wenhao Chai,Zhifei Li,Huanzhi Mao,Shang Zhou,Alexander Du,Hanchen Li,Shu Liu,Edwin Chen,Yichuan Wang,Xieting Chu,Zerui Cheng,Yuan Xu,Tian Xia,Zirui Wang,Tianneng Shi,Jianzhu Yao,Yilong Zhao,Qizheng Zhang,Charlie Ruan,Zeyu Shen,Kaiyuan Liu,Runyuan He,Dong Xing,Zerui Li,Zirong Zeng,Yige Jiang,Lufeng Cheng,Ziyi Zhao,Youran Sun,Wesley Zheng,Meiyuwang Zhang,Ruyi Ji,Xuechang Tu,Zihan Zheng,Zexing Chen,Kangyang Zhou,Zhaozi Wang,Jingbang Chen,Aleksandra Korolova,Peter Henderson,Pramod Viswanath,Vijay Ganesh,Saining Xie,Zhuang Liu,Dawn Song,Sewon Min,Ion Stoica,Joseph E. Gonzalez,Jingbo Shang,Alvin Cheung*

Main category: cs.LG

TL;DR: FrontierCS是一个包含156个开放问题的计算机科学基准测试，专注于未知最优解但可客观评估质量的问题，要求模型通过编写可执行程序而非直接答案来解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注已知最优解的任务，缺乏针对前沿计算机科学问题的评估框架。需要创建能够评估模型在开放性问题上的推理能力，特别是那些最优解未知但可客观评估质量的问题。

Method: 创建了包含156个开放问题的基准测试，涵盖算法问题和研究问题，这些问题通常具有NP-hard特性。每个问题都提供专家参考解决方案和自动评估器。模型需要通过编写可执行程序来解决问题，而不是输出直接答案。

Result: 前沿推理模型在算法和研究两个轨道上都远远落后于人类专家；仅增加推理预算并不能缩小这一差距；模型往往过度优化生成勉强可用的代码，而不是发现高质量的算法和系统设计。

Conclusion: FrontierCS提供了一个评估模型在前沿计算机科学问题上的能力的基准测试，揭示了当前模型与人类专家之间的显著差距，并指出了模型优化方向的问题。

Abstract: We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.

</details>


### [101] [Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data](https://arxiv.org/abs/2512.15706)
*Kayode Olumoyin,Lamees El Naqa,Katarzyna Rejniak*

Main category: cs.LG

TL;DR: 使用物理信息神经网络（PINN）在稀疏数据场景下学习膀胱癌细胞与免疫细胞之间的时变相互作用，预测抗癌联合治疗下的亚群轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统固定参数模型无法捕捉生物有机体随时间变化的动态相互作用，特别是在肿瘤学中，实验数据稀疏且只有少数时间点，需要新的方法来学习时变相互作用和治疗响应。

Method: 采用物理信息神经网络（PINN）方法，在有限数据场景下学习膀胱癌细胞与免疫细胞之间的时变相互作用，预测抗癌联合治疗下亚群在无观测数据时间点的轨迹。

Result: 该方法能够预测亚群轨迹，且与生物学的亚群轨迹解释一致，为学习外部干预下生物有机体间的演化相互作用提供了框架。

Conclusion: PINN方法在稀疏数据场景下有效学习生物系统的时变相互作用，为理解抗癌治疗下细胞间动态相互作用提供了新框架。

Abstract: In a mathematical model of interacting biological organisms, where external interventions may alter behavior over time, traditional models that assume fixed parameters usually do not capture the evolving dynamics. In oncology, this is further exacerbated by the fact that experimental data are often sparse and sometimes are composed of a few time points of tumor volume. In this paper, we propose to learn time-varying interactions between cells, such as those of bladder cancer tumors and immune cells, and their response to a combination of anticancer treatments in a limited data scenario. We employ the physics-informed neural network (PINN) approach to predict possible subpopulation trajectories at time points where no observed data are available. We demonstrate that our approach is consistent with the biological explanation of subpopulation trajectories. Our method provides a framework for learning evolving interactions among biological organisms when external interventions are applied to their environment.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [102] [Online Partitioned Local Depth for semi-supervised applications](https://arxiv.org/abs/2512.15436)
*John D. Foley,Justin T. Lee*

Main category: stat.ML

TL;DR: 在线PaLD算法：一种适用于在线应用的扩展分区局部深度算法，能够在O(n²)时间内将凝聚力网络扩展到新数据点


<details>
  <summary>Details</summary>
Motivation: 需要一种适用于在线应用（如半监督预测）的算法，能够在预计算凝聚力网络的基础上，高效处理新数据点的加入

Method: 扩展分区局部深度(PaLD)算法，构建可查询数据结构后，以O(n²)时间复杂度将凝聚力网络扩展到新数据点

Result: 提出在线PaLD算法，在O(n³)步骤构建数据结构后，能够快速处理新数据点，适用于在线异常检测和半监督分类

Conclusion: 在线PaLD算法为在线应用提供高效解决方案，补充了基于近似和并行化的加速方法，在医疗数据集上展示了应用潜力

Abstract: We introduce an extension of the partitioned local depth (PaLD) algorithm that is adapted to online applications such as semi-supervised prediction. The new algorithm we present, online PaLD, is well-suited to situations where it is a possible to pre-compute a cohesion network from a reference dataset. After $O(n^3)$ steps to construct a queryable data structure, online PaLD can extend the cohesion network to a new data point in $O(n^2)$ time. Our approach complements previous speed up approaches based on approximation and parallelism. For illustrations, we present applications to online anomaly detection and semi-supervised classification for health-care datasets.

</details>


### [103] [A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point](https://arxiv.org/abs/2512.15606)
*Carlos Couto,José Mourão,Mário A. T. Figueiredo,Pedro Ribeiro*

Main category: stat.ML

TL;DR: 该论文研究了神经网络在最优学习点附近的Hessian矩阵特征谱，发现小特征值决定长期学习性能，并分析了线性、多项式和非线性网络的Hessian谱特性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在最优学习点附近时，梯度下降动态的学习性能由损失函数相对于网络参数的Hessian矩阵决定，需要深入理解Hessian特征谱对学习性能的影响。

Method: 使用师生网络问题框架，分析匹配权重情况下的Hessian特征谱。对线性网络进行理论分析，推导大网络下的渐近谱分布；对多项式和非线性网络进行数值分析。

Result: 发现Hessian的小特征值决定长期学习性能；线性网络的谱渐近服从缩放卡方分布与缩放Marchenko-Pastur分布的卷积；多项式网络的Hessian秩可作为有效参数数量；非线性激活函数（如误差函数）的Hessian矩阵通常是满秩的。

Conclusion: Hessian矩阵的特征谱特性对理解神经网络的学习动态至关重要，特别是小特征值对长期学习性能的决定性作用，这为优化算法设计和理论分析提供了重要见解。

Abstract: Near an optimal learning point of a neural network, the learning performance of gradient descent dynamics is dictated by the Hessian matrix of the loss function with respect to the network parameters. We characterize the Hessian eigenspectrum for some classes of teacher-student problems, when the teacher and student networks have matching weights, showing that the smaller eigenvalues of the Hessian determine long-time learning performance. For linear networks, we analytically establish that for large networks the spectrum asymptotically follows a convolution of a scaled chi-square distribution with a scaled Marchenko-Pastur distribution. We numerically analyse the Hessian spectrum for polynomial and other non-linear networks. Furthermore, we show that the rank of the Hessian matrix can be seen as an effective number of parameters for networks using polynomial activation functions. For a generic non-linear activation function, such as the error function, we empirically observe that the Hessian matrix is always full rank.

</details>


### [104] [High-Dimensional Partial Least Squares: Spectral Analysis and Fundamental Limitations](https://arxiv.org/abs/2512.15684)
*Victor Léger,Florent Chatelain*

Main category: stat.ML

TL;DR: 该论文为PLS-SVD方法在高维数据集成中的理论分析，揭示了其检测共同潜在子空间的渐近优越性，同时指出了其反直觉行为和局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管PLS方法在数据集成中应用广泛且实践成功，但其在高维情况下的理论理解仍然有限。需要从理论上精确理解PLS在高维状态下的行为，特别是其检测共同潜在结构的能力和局限性。

Method: 研究一个数据集成模型，其中两个高维数据矩阵共享低秩共同潜在结构，同时包含个体特定成分。使用随机矩阵理论工具分析交叉协方差矩阵的奇异向量，推导估计潜在方向与真实潜在方向对齐的渐近特征。

Result: 为基于奇异值分解的PLS变体（PLS-SVD）的重建性能提供了定量解释，识别了该方法表现出反直觉或限制行为的机制。证明了PLS-SVD在检测共同潜在子空间方面相对于分别对每个数据集应用主成分分析的渐近优越性。

Conclusion: 研究结果为高维PLS-SVD提供了全面的理论理解，阐明了其优势和基本局限性，为该方法在高维数据集成中的应用提供了理论基础。

Abstract: Partial Least Squares (PLS) is a widely used method for data integration, designed to extract latent components shared across paired high-dimensional datasets. Despite decades of practical success, a precise theoretical understanding of its behavior in high-dimensional regimes remains limited. In this paper, we study a data integration model in which two high-dimensional data matrices share a low-rank common latent structure while also containing individual-specific components. We analyze the singular vectors of the associated cross-covariance matrix using tools from random matrix theory and derive asymptotic characterizations of the alignment between estimated and true latent directions. These results provide a quantitative explanation of the reconstruction performance of the PLS variant based on Singular Value Decomposition (PLS-SVD) and identify regimes where the method exhibits counter-intuitive or limiting behavior. Building on this analysis, we compare PLS-SVD with principal component analysis applied separately to each dataset and show its asymptotic superiority in detecting the common latent subspace. Overall, our results offer a comprehensive theoretical understanding of high-dimensional PLS-SVD, clarifying both its advantages and fundamental limitations.

</details>
