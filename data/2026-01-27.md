<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 23]
- [cs.LG](#cs.LG) [Total: 167]
- [stat.ML](#stat.ML) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Inverter-Based Differential Amplifiers With Back-Gate Feedback Linearization](https://arxiv.org/abs/2601.17129)
*Eric Danson,Jeffrey S. Walling*

Main category: eess.SP

TL;DR: 在22nm FD-SOI技术中，通过将共源放大器输出反馈到背栅端，利用负反馈的线性化效应，实现了增益与负载无关、无额外噪声、线性度显著提升的效果。


<details>
  <summary>Details</summary>
Motivation: 在FD-SOI技术中，背栅端提供了额外的控制维度。传统放大器设计中，增益受负载影响，线性度有限。本研究旨在利用背栅反馈来改善这些性能指标。

Method: 采用22nm FD-SOI技术，将共源放大器的输出信号反馈到晶体管的背栅端，形成负反馈结构。通过分析和仿真验证该方法的有效性。

Result: 背栅反馈使整体增益基本与负载无关，不引入额外噪声，并通过背栅电压增益显著改善线性度。在基于反相器或互补共源的差分放大器中，三阶截点(IP3)相比无反馈情况提升至少60倍。

Conclusion: FD-SOI技术中的背栅反馈是一种有效的线性化技术，能够显著提升放大器性能，特别是线性度，为高性能模拟电路设计提供了新途径。

Abstract: Feeding the common-source amplifier output to the back-gate terminal in fully depleted silicon on insulator (FD-SOI) technology exploits the linearizing effect of negative feedback. Analysis and simulation results in 22 nm FD-SOI show that back-gate feedback sets the overall gain approximately independent of the load, contributes no additional noise, and improves linearity by the back-gate voltage gain. Third-order intercept point (IP3) enhancement is at least $60\times$ compared to without feedback in inverter-based, or complementary common-source, differential amplifiers.

</details>


### [2] [Data-Efficient Physics-Informed Learning to Model Synchro-Waveform Dynamics of Grid-Integrated Inverter-Based Resources](https://arxiv.org/abs/2601.17154)
*Shivanshu Tripathi,Hossein Mohsenzadeh Yazdi,Maziar Raissi,Hamed Mohsenian-Rad*

Main category: eess.SP

TL;DR: 提出基于物理信息机器学习(PIML)的同步波形分析框架，利用电路物理知识补偿数据稀缺性，仅需少量网络扰动事件即可准确估计逆变器终端电流响应。


<details>
  <summary>Details</summary>
Motivation: 传统相量和SCADA测量无法准确捕捉逆变器资源(IBRs)的快速暂态动态，而同步波形测量单元(WMUs)虽能提供高分辨率数据，但仍受限于网络扰动事件稀缺和IBRs非线性动态复杂性。

Method: 开发数据高效的物理信息机器学习框架，利用已知电路关系约束学习过程。考虑两种场景：已知电路参数和未知电路参数。后者联合学习IBRs暂态动态和电路参数。

Result: 使用WMU扰动数据进行案例研究，结果显示相比纯数据驱动基线方法，该框架在不同采样率下均能获得更低的电流估计误差，且所需训练事件显著减少。

Conclusion: 提出的PIML框架通过整合物理知识有效解决了同步波形分析中的数据稀缺问题，为基于波形测量的IBRs暂态模型学习提供了高效解决方案。

Abstract: Inverter-based resources (IBRs) exhibit fast transient dynamics during network disturbances, which often cannot be properly captured by phasor and SCADA measurements. This shortcoming has recently been addressed with the advent of waveform measurement units (WMUs), which provide high-resolution, time-synchronized raw voltage and current waveform samples from multiple locations in the power system. However, transient model learning based on synchro-waveform measurements remains constrained by the scarcity of network disturbances and the complexity of the underlying nonlinear dynamics of IBRs. We propose to address these problems by developing a data-efficient physics-informed machine learning (PIML) framework for synchro-waveform analytics that estimates the IBR terminal current response from only a few network disturbance signatures. Here, the physics of the electrical circuits are used to compensate for limited data availability by constraining the learning process through known circuit relationships. Two cases are considered, with known and unknown circuit parameters. In the latter case, the framework jointly learns the transient dynamics of the IBRs and the parameters of the electrical circuit. Case studies using WMU disturbance data across multiple sampling rates shows consistently lower current estimation error with substantially fewer training events than a purely data-driven baseline.

</details>


### [3] [RIS-Enabled Spoofing Against Adversary Sensing: CRB-Maximizing Design and Decoying Analysis](https://arxiv.org/abs/2601.17320)
*Ioannis Gavras,Giuseppe Thadeu Freitas de Abreu,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 研究RIS透明覆盖用户设备时欺骗单基地雷达的能力，提出紧凑RIS核模型分析AoA估计精度，设计欺骗方案在真实反射方向形成深度零陷并在诱饵方向最大化信道增益。


<details>
  <summary>Details</summary>
Motivation: 随着RIS技术的发展，需要研究其在雷达欺骗方面的潜力，保护用户设备免受敌方雷达探测，提高通信安全性。

Method: 提出紧凑RIS核模型，将雷达角度响应与RIS相位配置显式关联；设计RIS欺骗方案，在真实反射AoA形成严格零陷，在诱饵方向最大化信道增益；提出配置无关的放置评分指导诱饵选择。

Result: 数值结果显示在真实反射AoA形成深度零陷，在诱饵方向产生显著峰值，使敌方雷达的最大似然感知变得不可靠。

Conclusion: RIS能够有效欺骗单基地雷达系统，通过在真实方向形成零陷和诱饵方向增强信号，显著降低雷达探测可靠性，为通信安全提供新方法。

Abstract: This paper studies the capability of a Reconfigurable Intelligent Surface (RIS), when transparently covering a User Equipment (UE), to deceive an adversary monostatic radar system. A compact RIS kernel model that explicitly links the radar's angular response to the RIS phase profile is introduced, enabling an analytical investigation of the Angle of Arrival (AoA) estimation accuracy with respect to the kernel's power. This model is also leveraged to formulate an RIS-based spoofing design with the dual objective to enforce strict nulls around the UE's true reflection AoA and maximize the channel gain towards a decoy direction. The RIS's deception capability is quantified using point-wise and angle-range robust criteria, and a configuration-independent placement score guiding decoy selection is proposed. Selected numerical results confirm deep nulls at the true reflection AoA together with a pronounced decoy peak, rendering maximum-likelihood sensing at the adversary radar unreliable.

</details>


### [4] [Semantic-Aware Task Clustering for Federated Cooperative Multi-Task Semantic Communication](https://arxiv.org/abs/2601.17419)
*Ahmad Halimi Razlighi,Pallavi Dhingra,Edgar Beck,Bho Matthiesen,Armin Dekorsy*

Main category: eess.SP

TL;DR: 提出基于联邦学习的分布式多任务语义通信框架，通过语义感知任务聚类解决异构任务间的负向信息传递问题


<details>
  <summary>Details</summary>
Motivation: 现有协作多任务语义通信框架不适用于分布式多用户场景（如卫星网络），且协作多任务可能产生破坏性影响，需要解决异构任务间的负向信息传递问题

Method: 1) 将协作多任务语义通信扩展到分布式设置，提出基于联邦学习的框架；2) 在联邦学习过程中集成语义感知任务聚类方法，基于信息论方法确保建设性协作

Result: 在LEO卫星网络设置下的仿真结果表明，该方法比未聚类的联邦学习和单任务语义通信具有更好的性能

Conclusion: 提出的联邦学习框架和语义感知聚类方法能有效解决分布式多任务语义通信中的负向信息传递问题，在卫星网络等分布式场景中实现建设性协作

Abstract: Task-oriented semantic communication (SemCom) prioritizes task execution over accurate symbol reconstruction and is well-suited to emerging intelligent applications. Cooperative multi-task SemCom (CMT-SemCom) further improves task execution performance. However, [1] demonstrates that cooperative multi-tasking can be either constructive or destructive. Moreover, the existing CMT-SemCom framework is not directly applicable to distributed multi-user scenarios, such as non-terrestrial satellite networks, where each satellite employs an individual semantic encoder. In this paper, we extend our earlier CMT-SemCom framework to distributed settings by proposing a federated learning (FL) based CMT-SemCom that enables cooperative multi-tasking across distributed users. Moreover, to address performance degradation caused by negative information transfer among heterogeneous tasks, we propose a semantic-aware task clustering method integrated in the FL process to ensure constructive cooperation based on an information-theoretic approach. Unlike common clustering methods that rely on high-dimensional data or feature space similarity, our proposed approach operates in the low-dimensional semantic domain to identify meaningful task relationships. Simulation results based on a LEO satellite network setup demonstrate the effectiveness of our approach and performance gain over unclustered FL and individual single-task SemCom.

</details>


### [5] [ME-WARD: A multimodal ergonomic analysis tool for musculoskeletal risk assessment from inertial and video data in working plac](https://arxiv.org/abs/2601.17571)
*Javier González-Alonso,Paula Martín-Tapia,David González-Ortega,Míriam Antón-Rodríguez,Francisco Javier Díaz-Pernas,Mario Martínez-Zarzuela*

Main category: eess.SP

TL;DR: ME-WARD是一个多模态工效学评估系统，通过整合IMU和深度学习姿态跟踪数据实现RULA方法，在工业环境中验证了其可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有工效学评估工具通常依赖专有设备，限制了在资源受限工业环境中的广泛应用。需要一种能够整合多种运动捕捉技术、成本效益高的统一评估方案。

Method: 开发ME-WARD系统，处理来自IMU系统和深度学习人体姿态跟踪模型的关节角度数据，实现RULA方法。系统设计灵活，支持任何能可靠测量关节角度的系统。

Result: 在传送带组装工业环境中验证，ME-WARD产生的RULA分数与IMU系统结果高度一致（特别是屈曲主导动作），与单目3D姿态估计系统性能相当，但在跟踪侧向和旋转运动方面存在局限。

Conclusion: ME-WARD通过支持多种输入源（包括低成本视频系统），提供了一个可扩展、经济高效的多模态工效学评估方案，有助于在资源受限工业环境中推广工效学评估。

Abstract: This study presents ME-WARD (Multimodal Ergonomic Workplace Assessment and Risk from Data), a novel system for ergonomic assessment and musculoskeletal risk evaluation that implements the Rapid Upper Limb Assessment (RULA) method. ME-WARD is designed to process joint angle data from motion capture systems, including inertial measurement unit (IMU)-based setups, and deep learning human body pose tracking models. The tool's flexibility enables ergonomic risk assessment using any system capable of reliably measuring joint angles, extending the applicability of RULA beyond proprietary setups. To validate its performance, the tool was tested in an industrial setting during the assembly of conveyor belts, which involved high-risk tasks such as inserting rods and pushing conveyor belt components. The experiments leveraged gold standard IMU systems alongside a state-of-the-art monocular 3D pose estimation system. The results confirmed that ME-WARD produces reliable RULA scores that closely align with IMU-derived metrics for flexion-dominated movements and comparable performance with the monocular system, despite limitations in tracking lateral and rotational motions. This work highlights the potential of integrating multiple motion capture technologies into a unified and accessible ergonomic assessment pipeline. By supporting diverse input sources, including low-cost video-based systems, the proposed multimodal approach offers a scalable, cost-effective solution for ergonomic assessments, paving the way for broader adoption in resource-constrained industrial environments.

</details>


### [6] [Development of an end-to-end hardware and software pipeline for affordable and feasible ergonomics assessment in the automotive industry](https://arxiv.org/abs/2601.17574)
*Javier González-Alonso,Cristina Simón-Martínez,Míriam Antón-Rodríguez,David González-Ortega,Francisco Javier Díaz-Pernas,Mario Martínez-Zarzuela*

Main category: eess.SP

TL;DR: 提出一个端到端的软硬件管道，用于自动化工业工作场所的人体工程学评估，通过定制IMU传感器、实时动作采集、逆运动学处理和RULA报告生成，提供低成本解决方案。


<details>
  <summary>Details</summary>
Motivation: 工业工作场所中工作相关肌肉骨骼疾病（WMSDs）风险较高，需要自动化、低成本的评估解决方案来减少工人健康问题和相关病假。

Method: 采用模块化解决方案，包括定制IMU传感器、两个实时工人动作采集工具、逆运动学处理和RULA报告生成，基于Unity3D和OpenSim等免费工具构建。

Result: 在汽车工厂实验中，与黄金标准解决方案相比，关节角度测量达到0.95的互相关，肘部RMSE低于10，肩部低于12，全局RULA评分差异小于5%。

Conclusion: 该研究提供了一个低成本的工作场所WMSDs风险评估解决方案，有助于减少肌肉骨骼疾病和工业病假，促进可穿戴系统在人体工程学分析中的普及应用。

Abstract: An end-to-end hardware-software pipeline is introduced to automatize ergonomics assessment in industrial workplaces. The proposed modular solution can interoperate with commercial systems throughout the ergonomics assessment phases involved in the process. The pipeline includes custom-designed Inertial Measurement Unit (IMU) sensors, two real-time worker movement acquisition tools, inverse kinematics processing and Rapid Upper Limb Assessment (RULA) report generation. It is based on free tools such as Unity3D and OpenSim to avoid the problems derived from using proprietary technologies, such as security decisions being made under "black box" conditions. Experiments were conducted in an automotive factory in a workplace with WMSDs risk among workers. The proposed solution obtained comparable results to a gold standard solution, reaching measured joint angles a 0.95 cross-correlation and a Root Mean Square Error (RMSE) lower than 10 for elbows and 12 for shoulders between both systems. In addition, the global RULA score difference is lower than 5% between both systems. This work provides a low-cost solution for WMSDs risk assessment in the workplace to reduce musculoskeletal disorders and associated sick leave in industry, impacting the health of workers in the long term. Our study can ease further research and popularize the use of wearable systems for ergonomics analysis allowing these workplace prevention systems to reach different industrial environments.

</details>


### [7] [Reliable Quasi-Static Post-Fall Floor-Occupancy Detection Using Low-Cost Millimetre-Wave Radar](https://arxiv.org/abs/2601.17710)
*Huy Trinh,Phuong Thai,Elliot Creager,George Shaker*

Main category: eess.SP

TL;DR: 本文提出了一种基于60GHz FMCW雷达的跌倒后地面占用检测方法，采用Capon/MVDR波束成形预处理，相比厂商提供的数字波束成形方法，在长期护理设施环境中显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化加剧，长期护理设施需要可靠的环境监测系统来安全监控居民同时减轻护理人员负担。跌倒是最需要及时响应的关键事件，但频繁的误报或不确定检测会导致警报疲劳。这促使设计从占用和活动感知到跌倒及跌倒后检测的完整端到端环境监测系统。

Method: 使用商用60GHz FMCW雷达在模拟长期护理设施的真实室内环境中进行部署评估。针对跌倒后检测中运动最小且地板和周围物体反射占主导的挑战，比较了厂商提供的数字波束成形（DBF）管道与提出的基于Capon/最小方差无失真响应（MVDR）波束成形的预处理方法。在生成的距离-方位图上应用单元平均恒虚警率（CA-CFAR）检测器，并在7名参与者上进行评估。

Result: 提出的方法将平均帧正率从0.823（DBF）提高到0.916（Proposed），显著改善了跌倒后地面占用检测性能。

Conclusion: 基于Capon/MVDR波束成形的预处理方法在60GHz FMCW雷达跌倒后检测中表现出优越性能，为长期护理设施中的可靠环境监测系统提供了有效解决方案。

Abstract: As the population ages rapidly, long-term care (LTC) facilities across North America face growing pressure to monitor residents safely while keeping staff workload manageable. Falls are among the most critical events to monitor due to their timely response requirement, yet frequent false alarms or uncertain detections can overwhelm caregivers and contribute to alarm fatigue. This motivates the design of reliable, whole end-to-end ambient monitoring systems from occupancy and activity awareness to fall and post-fall detection. In this paper, we focus on robust post-fall floor-occupancy detection using an off-the-shelf 60 GHz FMCW radar and evaluate its deployment in a realistic, furnished indoor environment representative of LTC facilities. Post-fall detection is challenging since motion is minimal, and reflections from the floor and surrounding objects can dominate the radar signal return. We compare a vendor-provided digital beamforming (DBF) pipeline against a proposed preprocessing approach based on Capon or minimum variance distortionless response (MVDR) beamforming. A cell-averaging constant false alarm rate (CA-CFAR) detector is applied and evaluated on the resulting range-azimuth maps across 7 participants. The proposed method improves the mean frame-positive rate from 0.823 (DBF) to 0.916 (Proposed).

</details>


### [8] [Doppler-Domain Respiratory Amplification for Semi-Static Human Occupancy Detection Using Low-Resolution SIMO FMCW Radar](https://arxiv.org/abs/2601.17721)
*Huy Trinh,Elliot Creager,George Shaker*

Main category: eess.SP

TL;DR: RASSO方法通过可逆多普勒域非线性重映射增强低分辨率雷达对半静态人体的检测能力，在护理院环境中显著提升检测性能


<details>
  <summary>Details</summary>
Motivation: 基于雷达的传感是摄像头和可穿戴设备的隐私保护替代方案，但低分辨率SIMO FMCW雷达检测半静态存在（躺、坐、站等微动状态）很困难，因为接近零多普勒能量常被静态杂波淹没

Method: 提出RASSO方法：在自适应Capon波束形成前，对慢时间FFT（多普勒）网格进行可逆多普勒域非线性重映射，在0 m/s附近密集化，生成增强的距离-方位图

Result: 在真实护理院数据集上，RASSO-RA实现AUC=0.981，在FAR=1%/5%时召回率0.920/0.947；CNN达到95-99%准确率，CNN-LSTM达到99.4-99.6%准确率；统计显著提升2.6-3.6个宏F1点

Conclusion: 简单的多普勒域扭曲处理能在真实临床环境中显著改善低分辨率雷达对半静态占用的检测能力，为隐私保护监测提供有效解决方案

Abstract: Radar-based sensing is a promising privacy-preserving alternative to cameras and wearables in settings such as long-term care. Yet detecting quasi-static presence (lying, sitting, or standing with only subtle micro-motions) is difficult for low-resolution SIMO FMCW radar because near-zero Doppler energy is often buried under static clutter. We present Respiratory-Amplification Semi-Static Occupancy (RASSO), an invertible Doppler-domain non-linear remapping that densifies the slow-time FFT (Doppler) grid around 0 m/s before adaptive Capon beamforming. The resulting range-azimuth (RA) maps exhibit higher effective SNR, sharper target peaks, and lower background variance, making thresholding and learning more reliable. On a real nursing-home dataset collected with a short-range 1Tx-3Rx radar, RASSO-RA improves classical detection performance, achieving AUC = 0.981 and recall = 0.920/0.947 at FAR = 1%/5%, outperforming conventional Capon processing and a recent baseline. RASSO-RA also benefits data-driven models: a frame-based CNN reaches 95-99% accuracy and a sequence-based CNN-LSTM reaches 99.4-99.6% accuracy across subjects. A paired session-level bootstrap test confirms statistically significant macro-F1 gains of 2.6-3.6 points (95% confidence intervals above zero) over the non-warped pipeline. These results show that simple Doppler-domain warping before spatial processing can materially improve semi-static occupancy detection with low-resolution radar in real clinical environments.

</details>


### [9] [S-MDMA: Sensitivity-Aware Model Division Multiple Access for Satellite-Ground Semantic Communication](https://arxiv.org/abs/2601.17731)
*Hui Cao,Rui Meng,Shujun Han,Song Gao,Xiaodong Xu,Ping Zhang*

Main category: eess.SP

TL;DR: 提出S-MDMA框架解决卫星-地面语义通信中的带宽限制和用户干扰问题，通过语义提取合并、敏感度排序和正交嵌入实现高效多用户传输。


<details>
  <summary>Details</summary>
Motivation: 卫星-地面语义通信在通信与AI融合中至关重要，但带宽限制和用户干扰严重影响了语义保真度和传输鲁棒性，需要解决这些挑战。

Method: 提出S-MDMA框架：1) 基于MDMA架构进行语义提取和合并以整合冗余信息；2) 语义敏感度排序算法选择性保留关键特征；3) 语义特征正交嵌入和多用户重建损失函数减轻用户间干扰。

Result: 在开源数据集上的实验表明，S-MDMA在不同信噪比和用户配置下均优于现有方法，实现了鲁棒且高保真的重建。

Conclusion: S-MDMA框架有效解决了卫星-地面语义通信中的带宽限制和用户干扰问题，为通信与AI融合提供了高效的多用户传输解决方案。

Abstract: Satellite-ground semantic communication (SemCom) is expected to play a pivotal role in convergence of communication and AI (ComAI), particularly in enabling intelligent and efficient multi-user data transmission. However, the inherent bandwidth constraints and user interference in satellite-ground systems pose significant challenges to semantic fidelity and transmission robustness. To address these issues, we propose a sensitivity-aware model division multiple access (S-MDMA) framework tailored for bandwidth-limited multi-user scenarios. The proposed framework first performs semantic extraction and merging based on the MDMA architecture to consolidate redundant information. To further improve transmission efficiency, a semantic sensitivity sorting algorithm is presented, which can selectively retain key semantic features. In addition, to mitigate inter-user interference, the framework incorporates orthogonal embedding of semantic features and introduces a multi-user reconstruction loss function to guide joint optimization. Experimental results on open-source datasets demonstrate that S-MDMA consistently outperforms existing methods, achieving robust and high-fidelity reconstruction across diverse signal-to-noise ratio (SNR) conditions and user configurations.

</details>


### [10] [Synchronization and Localization in Ad-Hoc ICAS Networks Using a Two-Stage Kuramoto Method](https://arxiv.org/abs/2601.18643)
*Dominik Neudert-Schulz,Thomas Dallmann*

Main category: eess.SP

TL;DR: 提出一种用于车对车网络的联合分布式同步与定位方案，支持通信感知一体化应用，具有信号无关性和抗有限采样频率影响的特性。


<details>
  <summary>Details</summary>
Motivation: 为实现车对车网络中的通信感知一体化，需要通信实体间精确的频率和相位同步，同时自动驾驶汽车需要周围车辆的准确位置估计。

Method: 提出一种联合分布式同步和定位方案，该方案大部分信号无关，可适用于多种通信感知一体化信号，并减轻有限采样频率对性能的影响。

Result: 方案能够实现通信实体间的精确同步和车辆定位，具有广泛的信号适应性，并能有效克服有限采样频率带来的性能下降问题。

Conclusion: 该联合分布式同步与定位方案为车对车网络的通信感知一体化应用提供了有效的技术支撑，具有信号无关性和抗采样频率限制的优势。

Abstract: To enable Integrated Communications and Sensing (ICAS) in a peer-to-peer vehicular network, precise synchronization in frequency and phase among the communicating entities is required. In addition, self-driving cars need accurate position estimates of the surrounding vehicles. In this work, we propose a joint, distributed synchronization and localization scheme for a network of communicating entities. Our proposed scheme is mostly signal-agnostic and therefore can be applied to a wide range of possible ICAS signals. We also mitigate the effect of finite sampling frequencies, which otherwise would degrade the synchronization and localization performance severely.

</details>


### [11] [Over-The-Air Extreme Learning Machines with XL Reception via Nonlinear Cascaded Metasurfaces](https://arxiv.org/abs/2601.17749)
*Kyriakos Stylianopoulos,Mattia Fabiani,Giulia Torcolacci,Davide Dardari,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 论文提出了一种基于可编程超表面的XL-MIMO系统，该系统作为极端学习机，能够完全在无线信道中执行二进制分类任务，并可通过闭式解进行训练。


<details>
  <summary>Details</summary>
Motivation: 面向目标通信范式需要在无线传输数据上应用机器学习推理。现有研究试图在MIMO系统物理层直接实现推理ML模型，但面临显著挑战。本文旨在利用可编程超表面技术，实现完全在无线信道中执行分类任务的系统。

Method: 提出XL-MIMO-ELM系统，采用由密集并行放置的衍射层超表面组成的接收器架构，后接单个射频链。面向MIMO信道的前层采用固定非线性响应的相同单元，其余层使用可调线性响应元件来近似训练好的ELM权重。

Result: 数值研究表明，在超表面元件数量极大的情况下，所提出的XL-MIMO-ELM系统在各种数据集和无线场景下，性能可与数字化和理想化的ML模型相媲美。

Conclusion: 该研究证明了将无线学习能力嵌入未来通信系统的可行性，为物理层机器学习实现提供了新途径。

Abstract: The recently envisioned goal-oriented communications paradigm calls for the application of inference on wirelessly transferred data via Machine Learning (ML) tools. An emerging research direction deals with the realization of inference ML models directly in the physical layer of Multiple-Input Multiple-Output (MIMO) systems, which, however, entails certain significant challenges. In this paper, leveraging the technology of programmable MetaSurfaces (MSs), we present an eXtremely Large (XL) MIMO system that acts as an Extreme Learning Machine (ELM) performing binary classification tasks completely Over-The-Air (OTA), which can be trained in closed form. The proposed system comprises a receiver architecture consisting of densely parallel placed diffractive layers of XL MSs followed by a single reception radio-frequency chain. The front layer facing the MIMO channel consists of identical unit cells of a fixed NonLinear (NL) response, while the remaining layers of elements of tunable linear responses are utilized to approximate OTA the trained ELM weights. Our numerical investigations showcase that, in the XL regime of MS elements, the proposed XL-MIMO-ELM system achieves performance comparable to that of digital and idealized ML models across diverse datasets and wireless scenarios, thereby demonstrating the feasibility of embedding OTA learning capabilities into future communication systems.

</details>


### [12] [Ampli-Flection for 6G: Active-RIS-Aided Aerial Backhaul with Full 3D Coverage](https://arxiv.org/abs/2601.17751)
*Hong-Bae Jeon,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: 提出一种新型空中回程架构，采用空中主动可重构智能表面，为6G网络中的无人机基站和地面用户提供全3D覆盖，实现能量高效的回程连接。


<details>
  <summary>Details</summary>
Motivation: 现有空中RIS方法局限于2D覆盖（仅服务地面用户）或被动操作，无法满足6G网络中无人机基站和地面用户的全面3D覆盖需求。特别是在城市突发流量激增场景下，需要可靠的回程连接来克服信号阻塞问题。

Method: 将主动RIS集成到高空平台，实现可靠视距链路并通过放大克服乘性衰落。在无人机基站处理城市突发流量场景中，空中主动RIS同时反射和放大回程信号。联合优化空中平台位置、阵列分区和RIS相位配置，最大化无人机基站能量效率。

Result: 仿真结果表明，所提方法显著优于基准方案，证明了其在6G网络中提供具有全面3D覆盖的弹性回程连接的强大潜力。

Conclusion: 提出的空中主动RIS架构能够为6G无线网络提供能量高效、具有全3D覆盖的弹性回程连接，特别适用于城市突发流量场景，克服了现有方法的局限性。

Abstract: In this paper, we propose a novel aerial backhaul architecture that employs an aerial active reconfigurable intelligent surface (RIS) to achieve energy-efficient, {full 3D coverage including UAV-BSs and ground users in 6G wireless networks}. Unlike prior aerial-RIS approaches limited to {2D coverage with only servicing ground users} or passive operation, the proposed design integrates an active-RIS onto a high-altitude aerial platform, enabling reliable line-of-sight links and overcoming multiplicative fading through amplification. In a scenario with UAV-BSs deployed to handle sudden traffic surges in urban areas, the aerial-active-RIS both reflects and amplifies backhaul signals to overcome blockage. We jointly optimize the aerial platform placement, array partitioning, and RIS phase configuration to maximize UAV-BS energy-efficiency. Simulation results confirm that the proposed method significantly outperforms benchmarks, demonstrating its strong potential to deliver resilient backhaul connectivity with comprehensive 3D coverage in 6G networks.

</details>


### [13] [AI-Driven Fuzzing for Vulnerability Assessment of 5G Traffic Steering Algorithms](https://arxiv.org/abs/2601.18690)
*Seyed Bagher Hashemi Natanzi,Hossein Mohammadi,Bo Tang,Vuk Marojevic*

Main category: eess.SP

TL;DR: 提出基于NSGA-II的AI驱动模糊测试框架，用于系统性地暴露5G网络流量调度算法的隐藏漏洞，相比传统测试能发现更多漏洞和关键故障。


<details>
  <summary>Details</summary>
Motivation: 5G网络中的流量调度算法在面对干扰尖峰、切换风暴和局部中断等对抗性条件时仍然脆弱，需要更有效的测试方法来提高算法鲁棒性。

Method: 使用基于非支配排序遗传算法II（NSGA-II）的AI驱动模糊测试框架，通过NVIDIA Sionna平台对五种流量调度算法在六种场景下进行评估。

Result: AI驱动模糊测试比传统测试多检测出34.3%的总漏洞和5.8%的关键故障，在多样性和边缘案例发现方面表现更优，但关键故障检测存在方差，反映了罕见漏洞的随机性。

Conclusion: AI驱动模糊测试为改进流量调度算法鲁棒性提供了有效且可扩展的验证方法，有助于构建具有弹性的6G就绪网络。

Abstract: Traffic Steering (TS) dynamically allocates user traffic across cells to enhance Quality of Experience (QoE), load balance, and spectrum efficiency in 5G networks. However, TS algorithms remain vulnerable to adversarial conditions such as interference spikes, handover storms, and localized outages. To address this, an AI-driven fuzz testing framework based on the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) is proposed to systematically expose hidden vulnerabilities. Using NVIDIA Sionna, five TS algorithms are evaluated across six scenarios. Results show that AI-driven fuzzing detects 34.3% more total vulnerabilities and 5.8% more critical failures than traditional testing, achieving superior diversity and edge-case discovery. The observed variance in critical failure detection underscores the stochastic nature of rare vulnerabilities. These findings demonstrate that AI-driven fuzzing offers an effective and scalable validation approach for improving TS algorithm robustness and ensuring resilient 6G-ready networks.

</details>


### [14] [Context-Aware Iterative Token Detection and Masked Transmission for Wireless Token Communication](https://arxiv.org/abs/2601.17770)
*Junyong Shin,Joohyuk Park,Jihong Park,Jinho Choi,Yo-Seb Jeon*

Main category: eess.SP

TL;DR: 提出基于预训练掩码语言模型的上下文感知令牌通信框架，通过联合利用上下文先验和信道观测进行迭代令牌检测，并引入上下文感知掩码策略跳过可预测令牌传输以降低传输速率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型将令牌作为自然语言表示的紧凑且有意义的单元，这启发了无线信道中的令牌通信，将令牌视为无线传输的基本单元。需要开发能够利用上下文信息提高通信效率的框架。

Method: 1. 使用预训练掩码语言模型作为发射端和接收端共享的上下文概率模型；2. 在接收端开发基于贝叶斯视角的迭代令牌检测方法，联合利用MLM引导的上下文先验和信道观测；3. 在发射端引入上下文感知掩码策略，跳过高度可预测的令牌传输以降低传输速率。

Result: 仿真结果表明，所提框架显著提高了重构句子的质量，并在各种信道条件下支持有效的速率自适应。

Conclusion: 该上下文感知令牌通信框架通过利用语言模型的上下文信息，在无线通信中实现了更高效的令牌传输，提高了重构质量并支持速率自适应。

Abstract: The success of large-scale language models has established tokens as compact and meaningful units for natural-language representation, which motivates token communication over wireless channels, where tokens are considered fundamental units for wireless transmission. We propose a context-aware token communication framework that uses a pretrained masked language model (MLM) as a shared contextual probability model between the transmitter (Tx) and receiver (Rx). At Rx, we develop an iterative token detection method that jointly exploits MLM-guided contextual priors and channel observations based on a Bayesian perspective. At Tx, we additionally introduce a context-aware masking strategy which skips highly predictable token transmission to reduce transmission rate. Simulation results demonstrate that the proposed framework substantially improves reconstructed sentence quality and supports effective rate adaptation under various channel conditions.

</details>


### [15] [Comparison of Single Carrier FTN-QAM and PCS-QAM for Amplifier-less Coherent Communication Systems](https://arxiv.org/abs/2601.17803)
*Dongdong Zou,Fan Li,Wei Wang,Zhongxing Tian,Yuheng Liu,Gangxiang Shen,Yi Cai*

Main category: eess.SP

TL;DR: FTN-16QAM在无放大器短距离相干通信系统中比PCS-64QAM具有约0.9dB的功率裕度优势


<details>
  <summary>Details</summary>
Motivation: 比较FTN-QAM和PCS-QAM在无放大器短距离相干通信系统中的性能，为系统设计提供参考

Method: 应用相位跟踪部分响应DFE和turbo均衡策略，对FTN-16QAM和PCS-64QAM进行性能比较

Result: FTN-16QAM比PCS-64QAM具有约0.9dB的功率裕度优势

Conclusion: 在无放大器短距离相干通信系统中，FTN-16QAM相比PCS-64QAM具有显著的性能优势

Abstract: A performance comparison of FTN-QAM and PCS-QAM for amplifier-less short-reach coherent communication systems is provided. With the applications of phase tracking partial response DFE and turbo equalization strategy, FTN-16QAM exhibits about 0.9dB power margin advantage over PCS-64QAM.

</details>


### [16] [Movable Antenna-Enhanced Near-Field Flexible Beamforming: Performance Analysis and Optimization](https://arxiv.org/abs/2601.17825)
*Shun Yang,Xin Wei,Nianbing Su,Weidong Mei,Zhi Chen,Boyu Ning*

Main category: eess.SP

TL;DR: 该论文研究可移动天线在近场波束赋形中的应用，针对波束置零和多波束形成两种场景，提出优化算法并分析位置误差的影响。


<details>
  <summary>Details</summary>
Motivation: 可移动天线相比固定位置天线能更灵活地调整波束赋形，但现有研究对其在近场波束赋形中的应用探索不足，特别是在波束置零和多波束形成这两种典型场景中。

Method: 1. 分析特殊案例，证明通过合理的天线位置调整，可以在特定方向形成波束的同时在其他方向产生零陷或全增益；2. 提出离散采样方法和交替优化算法来求解非凸优化问题；3. 引入泰勒级数近似分析位置误差对近场波束增益的影响。

Result: 数值结果验证了理论发现，并证明了所提算法的有效性。通过可移动天线的位置优化，能够实现更灵活的波束控制，在波束置零和多波束形成场景中均取得良好性能。

Conclusion: 可移动天线在近场波束赋形中具有显著优势，特别是在波束置零和多波束形成场景中。所提出的优化算法能有效解决非凸优化问题，且对位置误差具有鲁棒性，为实际应用提供了理论支持。

Abstract: As an emerging wireless communication technology, movable antennas (MAs) offer the ability to adjust the spatial correlation of steering vectors, enabling more flexible beamforming compared to fixed-position antennas (FPAs). In this paper, we investigate the use of MAs for two typical near-field beamforming scenarios: beam nulling and multi-beam forming. In the first scenario, we aim to jointly optimize the positions of multiple MAs and the beamforming vector to maximize the beam gain toward a desired direction while nulling interference toward multiple undesired directions. In the second scenario, the objective is to maximize the minimum beam gain among all the above directions. However, both problems are non-convex and challenging to solve optimally. To gain insights, we first analyze several special cases and show that, with proper positioning of the MAs, directing the beam toward a specific direction can lead to nulls or full gains in other directions in the two scenarios, respectively. For the general cases, we propose a discrete sampling method and an alternating optimization algorithm to obtain high-quality suboptimal solutions to the two formulated problems. Furthermore, considering the practical limitations in antenna positioning accuracy, we analyze the impact of position errors on the performance of the optimized beamforming and MA positions, by introducing a Taylor series approximation for the near-field beam gain at each target. Numerical results validate our theoretical findings and demonstrate the effectiveness of our proposed algorithms.

</details>


### [17] [A Physics-Informed Digital Twin Framework for Calibrated Sim-to-Real FMCW Radar Occupancy Estimation](https://arxiv.org/abs/2601.17871)
*Huy Trinh,Sebastian Ratto,Elliot Creager,George Shaker*

Main category: eess.SP

TL;DR: 提出轻量级仿真到真实框架，仅需物理几何模拟器和少量未标记真实校准数据，即可实现可靠的FMCW雷达占用检测和人数统计


<details>
  <summary>Details</summary>
Motivation: 直接从真实测量中学习鲁棒的雷达感知模型成本高昂，需要受控实验、重复校准和大量标注。需要一种更高效的训练方法

Method: 提出校准域随机化(CDR)方法，将模拟的距离-多普勒(RD)图的全局噪声底统计与真实环境对齐，同时保留有区分度的微多普勒结构

Result: 在真实世界评估中，仅使用CDR调整模拟训练的ResNet18模型在占用检测上达到97%准确率，人数统计达到72%准确率，优于光线追踪基线模拟和传统随机域随机化基线

Conclusion: 轻量级sim2real框架通过CDR方法有效解决了雷达感知模型训练成本高的问题，仅需少量真实校准数据即可实现高性能的雷达占用检测和人数统计

Abstract: Learning robust radar perception models directly from real measurements is costly due to the need for controlled experiments, repeated calibration, and extensive annotation. This paper proposes a lightweight simulation-to-real (sim2real) framework that enables reliable Frequency Modulated Continuous Wave (FMCW) radar occupancy detection and people counting using only a physics-informed geometric simulator and a small unlabeled real calibration set. We introduce calibrated domain randomization (CDR) to align the global noise-floor statistics of simulated range-Doppler (RD) maps with those observed in real environments while preserving discriminative micro-Doppler structure. Across real-world evaluations, ResNet18 models trained purely on CDR-adjusted simulation achieve 97 percent accuracy for occupancy detection and 72 percent accuracy for people counting, outperforming ray-tracing baseline simulation and conventional random domain randomization baselines.

</details>


### [18] [Integrated Channel Estimation and Sensing for Near-Field ELAA Systems](https://arxiv.org/abs/2601.18333)
*Jionghui Wang,Jun Fang,Hongbin Li,Boyu Ning*

Main category: eess.SP

TL;DR: 提出基于张量分解的近场OFDM系统上行信道估计方法，利用CPD和BTD模型分别处理LoS和NLoS场景，显著降低训练开销并提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 解决ELAA近场OFDM系统中多用户信道估计问题，传统正交导频方案训练开销过大，需要支持更多用户同时减少训练开销。

Method: 将接收信号建模为三阶低秩张量，LoS场景使用CPD分解，NLoS场景使用BTD分解，采用ALS和NLS算法进行分解，从因子矩阵中提取信道参数。

Result: 仿真结果表明，该方法比基于压缩感知的方法获得显著更高的信道估计精度，且导频符号数可远小于用户数，大幅降低训练开销。

Conclusion: 提出的张量分解方法能有效解决ELAA近场OFDM系统的多用户信道估计问题，在减少训练开销的同时提高估计精度，并能实现用户精确定位。

Abstract: In this paper, we study the problem of uplink channel estimation for near-filed orthogonal frequency division multiplexing (OFDM) systems, where a base station (BS), equipped with an extremely large-scale antenna array (ELAA), serves multiple users over the same time-frequency resource block. A non-orthogonal pilot transmission scheme is considered to accommodate a larger number of users that can be supported by ELAA systems without incurring an excessive amount of training overhead. To facilitate efficient multi-user channel estimation, we express the received signal as a third-order low-rank tensor, which admits a canonical polyadic decomposition (CPD) model for line-of-sight (LoS) scenarios and a block term decomposition (BTD) model for non-line-of-sight (NLoS) scenarios. An alternating least squares (ALS) algorithm and a non-linear least squares (NLS) algorithm are employed to perform CPD and BTD, respectively. Channel parameters are then efficiently extracted from the recovered factor matrices. By exploiting the geometry of the propagation paths in the estimated channel, users' positions can be precisely determined in LoS scenarios. Moreover, our uniqueness analysis shows that the proposed tensor-based joint multi-user channel estimation framework is effective even when the number of pilot symbols is much smaller than the number of users, revealing its potential in training overhead reduction. Simulation results demonstrate that the proposed method achieves markedly higher channel estimation accuracy than compressed sensing (CS)-based approaches.

</details>


### [19] [Deep Reinforcement Learning for Hybrid RIS Assisted MIMO Communications](https://arxiv.org/abs/2601.18453)
*Phuong Nam Tran,Nhan Thanh Nguyen,Markku Juntti*

Main category: eess.SP

TL;DR: 提出基于深度强化学习的框架，直接从信道状态信息映射到近最优的发射波束赋形和混合可重构智能表面配置，显著降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 混合可重构智能表面结合了被动反射和主动信号放大，能增强无线系统性能。但联合优化发射波束赋形与HRIS反射和放大系数以最大化频谱效率是一个非凸问题，传统迭代解决方案计算复杂度高

Method: 提出深度强化学习框架，学习从信道状态信息到近最优发射波束赋形和HRIS配置的直接映射。DRL模型离线训练，训练后能以低复杂度和低延迟计算波束赋形和HRIS配置

Result: 仿真结果表明，基于DRL的方法达到了交替优化基准方法95%的频谱效率，同时显著降低了计算复杂度

Conclusion: 深度强化学习框架能有效解决HRIS系统中的联合优化问题，在保持高性能的同时大幅降低计算复杂度，为实际部署提供了可行的解决方案

Abstract: Hybrid reconfigurable intelligent surfaces (HRIS) enhance wireless systems by combining passive reflection with active signal amplification. However, jointly optimizing the transmit beamforming with the HRIS reflection and amplification coefficients to maximize spectral efficiency (SE) is a non-convex problem, and conventional iterative solutions are computationally intensive. To address this, we propose a deep reinforcement learning (DRL) framework that learns a direct mapping from channel state information to the near-optimal transmit beamforming and HRIS configurations. The DRL model is trained offline, after which it can compute the beamforming and HRIS configurations with low complexity and latency. Simulation results demonstrate that our DRL-based method achieves 95% of the SE obtained by the alternating optimization benchmark, while significantly lowering the computational complexity.

</details>


### [20] [Dynamic Channel Charting: An LSTM-AE-based Approach](https://arxiv.org/abs/2601.18473)
*Yuan Gao,Xinyu Guo,Wenjing Xie,Zifan Wang,Hongwen Yu,Gongyang Li,Shugong Xu*

Main category: eess.SP

TL;DR: 提出基于LSTM和自编码器的时间序列信道图表方法，解决传统方法忽略信道动态特性的问题，在6G通信中提升信道图表稳定性和拓扑一致性。


<details>
  <summary>Details</summary>
Motivation: 传统信道图表方法主要学习静态几何结构，忽略了信道随时间变化的动态特性，导致在复杂环境中信道图表不稳定且拓扑一致性差。随着6G通信系统发展，需要更准确的信道状态信息建模。

Method: 提出LSTM-AE-CC方法，将长短期记忆网络与自编码器结合。LSTM用于捕捉CSI中的时间依赖性，自编码器学习连续潜在表示，将时间建模机制融入传统信道图表框架。

Result: 实验结果表明，该方法在各种实际通信场景中优于传统信道图表方法，特别是在信道图表稳定性、轨迹连续性和长期预测能力方面表现更佳。

Conclusion: LSTM-AE-CC方法成功解决了传统信道图表忽略时间动态性的问题，实现了信道几何一致性和时间变化特性的显式建模，为6G通信系统提供了更稳定可靠的信道图表解决方案。

Abstract: With the development of the sixth-generation (6G) communication system, Channel State Information (CSI) plays a crucial role in improving network performance. Traditional Channel Charting (CC) methods map high-dimensional CSI data to low-dimensional spaces to help reveal the geometric structure of wireless channels. However, most existing CC methods focus on learning static geometric structures and ignore the dynamic nature of the channel over time, leading to instability and poor topological consistency of the channel charting in complex environments. To address this issue, this paper proposes a novel time-series channel charting approach based on the integration of Long Short-Term Memory (LSTM) networks and Auto encoders (AE) (LSTM-AE-CC). This method incorporates a temporal modeling mechanism into the traditional CC framework, capturing temporal dependencies in CSI using LSTM and learning continuous latent representations with AE. The proposed method ensures both geometric consistency of the channel and explicit modeling of the time-varying properties. Experimental results demonstrate that the proposed method outperforms traditional CC methods in various real-world communication scenarios, particularly in terms of channel charting stability, trajectory continuity, and long-term predictability.

</details>


### [21] [Dualband OFDM Delay Estimation for Multi-Target Localization](https://arxiv.org/abs/2601.18478)
*Jialun Kou,Achiel Colpaert,Zhuangzhuang Cui,Sofie Pollin*

Main category: eess.SP

TL;DR: 提出PSF中心框架用于双频OFDM时延估计，通过建模观测时延剖面为真实目标响应与PSF的卷积，显式关联频带配置与分辨率/模糊度，并采用RELAX算法抑制PSF伪影，提升碎片化频谱下的ILC性能。


<details>
  <summary>Details</summary>
Motivation: 集成定位与通信系统需要重用通信波形进行同时数据传输和定位，但时延分辨率受限于可用带宽。实际中由于硬件约束和频谱碎片化，难以获得大的连续带宽。聚合非连续窄带可增加有效频率跨度，但非连续频率布局会引入高旁瓣和时延估计模糊等挑战。

Method: 引入点扩散函数中心框架用于双频OFDM时延估计，将观测时延剖面建模为真实目标响应与由双频子载波选择模式决定的PSF的卷积。采用RELAX算法进行双频多目标时延估计以抑制PSF引起的伪影。

Result: 仿真结果表明，在双频场景下提高了鲁棒性和准确性，支持碎片化频谱下的集成定位与通信。

Conclusion: 提出的PSF中心框架和RELAX算法能够有效解决双频OFDM系统中的时延估计问题，为频谱碎片化环境下的集成定位与通信提供了可行的解决方案。

Abstract: Integrated localization and communication systems aim to reuse communication waveforms for simultaneous data transmission and localization, but delay resolution is fundamentally limited by the available bandwidth. In practice, large contiguous bandwidths are difficult to obtain due to hardware constraints and spectrum fragmentation. Aggregating non-contiguous narrow bands can increase the effective frequency span, but a non-contiguous frequency layout introduces challenges such as elevated sidelobes and ambiguity in delay estimation. This paper introduces a point-spread-function (PSF)-centric framework for dual-band OFDM delay estimation. We model the observed delay profile as the convolution of the true target response with a PSF determined by the dual-band subcarrier selection pattern, explicitly linking band configuration to resolution and ambiguity. To suppress PSF-induced artifacts, we adapt the RELAX algorithm for dual-band multi-target delay estimation. Simulations demonstrate improved robustness and accuracy in dual-band scenarios, supporting ILC under fragmented spectrum.

</details>


### [22] [Hybrid Radar Fusion with Quantization: CRB-Rate Trade-offs and ADC Dynamic Range](https://arxiv.org/abs/2601.18539)
*Akhileswar Chowdary,Ahmad Bazzi,Marwa Chafii*

Main category: eess.SP

TL;DR: 论文研究了低分辨率ADC对混合雷达融合系统的影响，发现HRF性能对ADC分辨率敏感，存在感知精度与通信速率的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 虽然低分辨率ADC在ISAC系统中越来越受关注，但其对混合雷达融合系统的具体影响尚未充分研究。HRF系统中上行链路同时携带直接信号和反射信号，反射信号通常弱得多，使得HRF性能对ADC分辨率特别敏感。

Method: 使用量化Cramér-Rao界来测量感知精度，推导了到达角估计的量化CRB上界，并通过两个优化问题探索CRB与速率的权衡关系。

Result: 仿真结果表明：当接收信号动态范围超过ADC支持的动态范围时，HRF变得不可行；通信速率在ADC分辨率超过特定阈值后不再显著提升；存在感知与通信性能的基本权衡。

Conclusion: HRF性能受益于更高ADC分辨率，但通信速率增益会达到平台期，这种权衡关系可以通过仿真得到的CRB-速率边界有效表征。

Abstract: Recent advancements have underscored the relevance of low-resolution analog-to-digital converters (ADCs) in integrated sensing and communication (ISAC) systems. Nevertheless, their specific impact on hybrid radar fusion (HRF) remains largely unexplored. In HRF systems, where uplink (UL) paths carry direct and reflected signals in the same frequency band, the reflected signal is often significantly weaker, making HRF performance particularly sensitive to ADC resolution. To study this effect, we use the quantized Cramér-Rao bound (CRB) to measure sensing accuracy. This work derives an upper bound on the quantized CRB for angle of arrival (AoA) estimation and explores CRB-rate trade-offs through two formulated optimization problems. Simulation results indicate that HRF becomes infeasible when the dynamic range of the received signal exceeds the dynamic range supported by the ADC, which is inherently limited by its resolution. Furthermore, the UL communication rate does not increase significantly when the ADC resolution is raised beyond a certain threshold. These observations highlight a fundamental trade-off between sensing and communication performance: while HRF performance benefits from higher ADC resolutions, the corresponding gains in communication rate plateau. This trade-off is effectively characterized using CRB-rate boundaries derived through simulation.

</details>


### [23] [Low-Bit Quantization of Bandlimited Graph Signals via Iterative Methods](https://arxiv.org/abs/2601.18782)
*Felix Krahmer,He Lyu,Rayan Saab,Jinna Qian,Anna Veselovska,Rongrong Wang*

Main category: eess.SP

TL;DR: 该论文研究图信号的低比特量化，提出基于噪声整形的迭代算法，利用图拉普拉斯谱特性和图不相干性实现高保真近似。


<details>
  <summary>Details</summary>
Motivation: 研究图信号的低比特量化问题，旨在开发高效的量化方法，能够在有限比特率下保持信号质量，这对于图信号处理的实际应用至关重要。

Method: 提出迭代噪声整形算法，包括有顶点替换和无顶点替换的采样方法。利用图拉普拉斯算子的谱特性和图不相干性来设计量化方案。

Result: 为随机采样方法提供了理论保证，在合成和真实世界图数据上的大量数值实验证明了所提方案的高效性和鲁棒性。

Conclusion: 提出的噪声整形量化算法能够有效处理图信号的低比特表示问题，利用图结构特性实现高质量近似，为图信号量化提供了实用解决方案。

Abstract: We study the quantization of real-valued bandlimited signals on graphs, focusing on low-bit representations. We propose iterative noise-shaping algorithms for quantization, including sampling approaches with and without vertex replacement. The methods leverage the spectral properties of the graph Laplacian and exploit graph incoherence to achieve high-fidelity approximations. Theoretical guarantees are provided for the random sampling method, and extensive numerical experiments on synthetic and real-world graphs illustrate the efficiency and robustness of the proposed schemes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation](https://arxiv.org/abs/2601.16984)
*Rahul Ghosh,Chun-Hao Liu,Gaurav Rele,Vidya Sagar Ravipati,Hazar Aouad*

Main category: cs.LG

TL;DR: TelcoAI是一个针对3GPP技术规范的智能文档处理系统，通过多模态RAG架构和智能代理技术，显著提升了复杂技术文档的查询理解和信息检索能力。


<details>
  <summary>Details</summary>
Motivation: 3GPP技术规范具有复杂的层次结构、密集的格式和多模态内容（文本+图表），现有方法难以有效处理复杂查询、视觉信息和文档间的依赖关系，需要专门针对电信技术文档的智能处理系统。

Method: TelcoAI采用代理式多模态检索增强生成系统，包含：1）基于章节感知的分块策略；2）结构化查询规划；3）元数据引导的检索机制；4）文本与图表的跨模态融合技术。

Result: 在多个基准测试（包括专家设计的查询）中，系统达到87%召回率、83%声明召回率和92%忠实度，相比现有最优方法提升了16%。

Conclusion: TelcoAI证明了代理式和多模态推理在技术文档理解中的有效性，为电信研究和工程提供了实用的解决方案，推动了复杂技术文档处理的实际应用。

Abstract: The 3rd Generation Partnership Project (3GPP) produces complex technical specifications essential to global telecommunications, yet their hierarchical structure, dense formatting, and multi-modal content make them difficult to process. While Large Language Models (LLMs) show promise, existing approaches fall short in handling complex queries, visual information, and document interdependencies. We present TelcoAI, an agentic, multi-modal Retrieval-Augmented Generation (RAG) system tailored for 3GPP documentation. TelcoAI introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams. Evaluated on multiple benchmarks-including expert-curated queries-our system achieves $87\%$ recall, $83\%$ claim recall, and $92\%$ faithfulness, representing a $16\%$ improvement over state-of-the-art baselines. These results demonstrate the effectiveness of agentic and multi-modal reasoning in technical document understanding, advancing practical solutions for real-world telecommunications research and engineering.

</details>


### [25] [Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2601.16991)
*Longteng Zhang,Sen Wu,Shuai Hou,Zhengyu Qing,Zhuo Zheng,Danning Ke,Qihong Lin,Qiang Wang,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: SALR是一种新的微调范式，将低秩适应与稀疏剪枝结合，在保持LoRA性能的同时实现50%稀疏度，模型大小减半，推理速度提升1.7倍。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型的微调需要大量参数更新，在资源受限环境中难以应用。LoRA减少了可训练参数，但底层密集权重仍带来高存储和计算成本。传统剪枝方法会损害LoRA性能。

Method: 提出SALR（稀疏感知低秩表示），在均方误差框架下统一低秩适应和稀疏剪枝。静态剪枝冻结的基础权重，通过截断SVD低秩适配器恢复丢弃的残差信息。采用位图编码和两阶段流水线解码+GEMM设计实现硬件效率最大化。

Result: 在多种LLM上实现50%稀疏度，在GSM8K和MMLU任务上性能与LoRA相当，模型大小减少2倍，推理速度提升达1.7倍。

Conclusion: SALR成功将低秩适应与稀疏剪枝结合，在保持性能的同时显著减少模型大小和提升推理速度，为资源受限环境中的模型部署提供了有效解决方案。

Abstract: Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\times$, and delivers up to a $1.7\times$ inference speedup.

</details>


### [26] [A Dataset of Dengue Hospitalizations in Brazil (1999 to 2021) with Weekly Disaggregation from Monthly Counts](https://arxiv.org/abs/2601.16994)
*Lucas M. Morello,Matheus Lima Castro,Pedro Cesar M. G. Camargo,Liliane Moreira Nery,Darllan Collins da Cunha e Silva,Leopoldo Lusquino Filho*

Main category: cs.LG

TL;DR: 该论文发布了一个巴西市级登革热住院时间序列数据集，将原始月度数据通过三次样条插值分解为周度分辨率，并包含多种解释变量，用于流行病学预测的AI模型训练。


<details>
  <summary>Details</summary>
Motivation: 为了提升AI模型在流行病学预测中的训练效果，需要将原始月度数据的时间粒度增加到周度分辨率，从而提供更精细的时间序列数据。

Method: 通过插值协议将市级登革热住院时间序列从月度分解为周度（流行病学周），采用三次样条插值策略，并包含校正步骤以保持月度总量。使用圣保罗州的高分辨率参考数据集评估了三种策略：线性插值、抖动和三次样条。

Result: 三次样条插值在参考数据上表现出最高的贴合度，因此被采用来生成1999-2021期间的周度序列。数据集还包括人口密度、CH4、CO2、NO2排放、贫困和城市化指数、最高温度、月均降水量、最低相对湿度等解释变量。

Conclusion: 该数据集为多变量时间序列分析、环境健康研究和机器学习/深度学习模型的爆发预测开发提供了有价值的资源，并详细记录了数据来源、结构、质量指标和使用建议。

Abstract: This data paper describes and publicly releases this dataset (v1.0.0), published on Zenodo under DOI 10.5281/zenodo.18189192. Motivated by the need to increase the temporal granularity of originally monthly data to enable more effective training of AI models for epidemiological forecasting, the dataset harmonizes municipal-level dengue hospitalization time series across Brazil and disaggregates them to weekly resolution (epidemiological weeks) through an interpolation protocol with a correction step that preserves monthly totals. The statistical and temporal validity of this disaggregation was assessed using a high-resolution reference dataset from the state of Sao Paulo (2024), which simultaneously provides monthly and epidemiological-week counts, enabling a direct comparison of three strategies: linear interpolation, jittering, and cubic spline. Results indicated that cubic spline interpolation achieved the highest adherence to the reference data, and this strategy was therefore adopted to generate weekly series for the 1999 to 2021 period. In addition to hospitalization time series, the dataset includes a comprehensive set of explanatory variables commonly used in epidemiological and environmental modeling, such as demographic density, CH4, CO2, and NO2 emissions, poverty and urbanization indices, maximum temperature, mean monthly precipitation, minimum relative humidity, and municipal latitude and longitude, following the same temporal disaggregation scheme to ensure multivariate compatibility. The paper documents the datasets provenance, structure, formats, licenses, limitations, and quality metrics (MAE, RMSE, R2, KL, JSD, DTW, and the KS test), and provides usage recommendations for multivariate time-series analysis, environmental health studies, and the development of machine learning and deep learning models for outbreak forecasting.

</details>


### [27] [MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning](https://arxiv.org/abs/2601.17006)
*Xuchen Li,Jing Chen,Xuzhao Li,Hao Liang,Xiaohuan Zhou,Taifeng Wang,Wentao Zhang*

Main category: cs.LG

TL;DR: MathMixup：一种通过混合和分解策略系统生成高质量、难度可控数学推理问题的新数据合成范式，结合课程学习显著提升LLM数学推理能力


<details>
  <summary>Details</summary>
Motivation: 现有数学推理训练数据存在多样性有限、难度控制不精确的问题，无法有效支持课程学习等高效训练范式，需要开发能系统控制问题难度的数据合成方法

Method: 提出MathMixup数据合成范式，采用混合和分解策略生成难度可控的数学推理问题，结合自动自检和人工筛选确保语义清晰和难度梯度，构建MathMixupQA数据集并设计课程学习策略

Result: Qwen2.5-7B微调后在7个数学基准测试中平均得分52.6%，超越先前最先进方法，验证了MathMixup在提升LLM数学推理能力和推进数据为中心课程学习方面的有效性

Conclusion: MathMixup通过系统生成难度可控的高质量数学推理数据，结合课程学习策略，能显著提升LLM的数学推理能力，为数据为中心的课程学习提供了有效解决方案

Abstract: In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.

</details>


### [28] [Analysis of voice recordings features for Classification of Parkinson's Disease](https://arxiv.org/abs/2601.17007)
*Beatriz Pérez-Sánchez,Noelia Sánchez-Maroño,Miguel A. Díaz-Freire*

Main category: cs.LG

TL;DR: 该研究提出结合特征选择方法的机器学习模型用于帕金森病早期诊断，通过语音记录分析，发现神经网络模型表现良好，且特征数量可大幅减少而不影响性能。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，因为早期运动症状轻微。虽然语音记录分析有助于诊断，但临床成本高且特征众多，不清楚哪些特征真正相关。需要开发准确高效的诊断方法。

Method: 使用不同类型的机器学习模型结合特征选择方法。特征选择技术确定哪些特征对问题提供最多信息，从而减少分类器使用的特征数量。

Result: 机器学习方法，特别是神经网络，适用于帕金森病分类。特征数量可以显著减少而不影响模型性能，实现了高效准确的诊断。

Conclusion: 机器学习结合特征选择是帕金森病早期诊断的有效方法，能够减少特征维度同时保持诊断准确性，为临床实践提供了实用工具。

Abstract: Parkinson's disease (PD) is a chronic neurodegenerative disease. Early diagnosis is essential to mitigate the progressive deterioration of patients' quality of life. The most characteristic motor symptoms are very mild in the early stages, making diagnosis difficult. Recent studies have shown that the use of patient voice recordings can aid in early diagnosis. Although the analysis of such recordings is costly from a clinical point of view, advances in machine learning techniques are making the processing of this type of data increasingly accurate and efficient. Vocal recordings contain many features, but it is not known whether all of them are relevant for diagnosing the disease.
  This paper proposes the use of different types of machine learning models combined with feature selection methods to detect the disease. The selection techniques allow to reduce the number of features used by the classifiers by determining which ones provide the most information about the problem. The results show that machine learning methods, in particular neural networks, are suitable for PD classification and that the number of features can be significantly reduced without affecting the performance of the models.

</details>


### [29] [Bayesian Robust Financial Trading with Adversarial Synthetic Market Data](https://arxiv.org/abs/2601.17008)
*Haochong Xia,Simin Li,Ruixiao Xu,Zhixia Zhang,Hongxiang Wang,Zhiqian Liu,Teng Yao Long,Molei Qin,Chuqiao Zong,Bo An*

Main category: cs.LG

TL;DR: 提出贝叶斯鲁棒框架，通过宏观条件GAN生成多样化市场数据，结合贝叶斯马尔可夫博弈学习鲁棒交易策略，解决算法交易中模型因市场机制变化而性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 算法交易模型在样本内表现良好，但在实际市场机制变化时性能下降。现有方法存在两个问题：1) 对高级市场波动的鲁棒性不足；2) 缺乏真实多样的模拟环境导致策略过拟合。

Method: 提出贝叶斯鲁棒框架：1) 数据端：使用宏观条件GAN生成器，以宏观经济指标为控制变量，合成具有真实时间、跨工具和宏观相关性的数据；2) 策略端：将交易过程建模为两人零和贝叶斯马尔可夫博弈，对抗代理通过扰动宏观指标模拟市场机制变化，交易代理通过分位数信念网络维护和更新对隐藏市场状态的信念，使用贝叶斯神经虚拟自博弈寻找鲁棒完美贝叶斯均衡。

Result: 在9种金融工具上的广泛实验表明，该框架优于9种最先进的基线方法。在COVID等极端事件中，该方法显示出更好的盈利能力和风险管理能力。

Conclusion: 该框架为不确定和变化的市场动态下的交易提供了可靠解决方案，通过系统整合宏观条件生成模型和鲁棒策略学习，有效解决了算法交易中的鲁棒性问题。

Abstract: Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.

</details>


### [30] [Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study](https://arxiv.org/abs/2601.17010)
*Hudson Golino*

Main category: cs.LG

TL;DR: LLM嵌入作为可搜索的语义景观，通过动态探索图分析优化嵌入维度深度，平衡结构准确性和组织性


<details>
  <summary>Details</summary>
Motivation: 当前LLM嵌入在心理学项目池中作为静态、横截面表示使用，假设所有嵌入坐标贡献均匀，忽略了最优结构信息可能集中在嵌入空间特定区域的可能性

Method: 将嵌入重新定义为可搜索的景观，采用动态探索图分析(DynEGA)系统遍历嵌入坐标，将维度索引视为伪时间顺序。通过大规模蒙特卡洛模拟，使用OpenAI的text-embedding-3-small模型嵌入代表自恋五个维度的项目，在不同项目池大小(每维度3-40项)和嵌入深度(3-1,298维度)下生成网络估计

Result: 总熵拟合指数(TEFI)和归一化互信息(NMI)在嵌入景观中产生竞争性优化轨迹：TEFI在深嵌入范围(900-1200维度)达到最小值，此时基于熵的组织最大化但结构准确性下降；NMI在浅深度达到峰值，此时维度恢复最强但基于熵的拟合仍不理想。单指标优化产生结构不连贯的解，而加权复合标准能识别同时平衡准确性和组织的嵌入维度深度区域。最优嵌入深度随项目池大小系统缩放

Conclusion: 嵌入景观是非均匀的语义空间，需要原则性优化而非默认使用完整向量。最优嵌入深度应根据项目池大小系统选择，通过复合标准平衡结构准确性和组织性

Abstract: Large language model (LLM) embeddings are increasingly used to estimate dimensional structure in psychological item pools prior to data collection, yet current applications treat embeddings as static, cross-sectional representations. This approach implicitly assumes uniform contribution across all embedding coordinates and overlooks the possibility that optimal structural information may be concentrated in specific regions of the embedding space. This study reframes embeddings as searchable landscapes and adapts Dynamic Exploratory Graph Analysis (DynEGA) to systematically traverse embedding coordinates, treating the dimension index as a pseudo-temporal ordering analogous to intensive longitudinal trajectories. A large-scale Monte Carlo simulation embedded items representing five dimensions of grandiose narcissism using OpenAI's text-embedding-3-small model, generating network estimations across systematically varied item pool sizes (3-40 items per dimension) and embedding depths (3-1,298 dimensions). Results reveal that Total Entropy Fit Index (TEFI) and Normalized Mutual Information (NMI) leads to competing optimization trajectories across the embedding landscape. TEFI achieves minima at deep embedding ranges (900--1,200 dimensions) where entropy-based organization is maximal but structural accuracy degrades, whereas NMI peaks at shallow depths where dimensional recovery is strongest but entropy-based fit remains suboptimal. Single-metric optimization produces structurally incoherent solutions, whereas a weighted composite criterion identifies embedding dimensions depth regions that jointly balance accuracy and organization. Optimal embedding depth scales systematically with item pool size. These findings establish embedding landscapes as non-uniform semantic spaces requiring principled optimization rather than default full-vector usage.

</details>


### [31] [ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule](https://arxiv.org/abs/2601.18681)
*Yilie Huang,Wenpin Tang,Xunyu Zhou*

Main category: cs.LG

TL;DR: 提出自适应重参数化时间（ART）方法，通过控制重参数化时间变量的时钟速度来优化扩散模型的时间步长调度，并引入ART-RL作为强化学习框架来学习最优时间调度。


<details>
  <summary>Details</summary>
Motivation: 传统均匀或手动设计的时间网格在有限时间步预算下可能不是最优的，需要一种自适应方法来优化时间步长调度以最小化离散化误差。

Method: 引入自适应重参数化时间（ART）控制重参数化时间变量的时钟速度，保持终止时间不变但产生不均匀的时间步长。进一步提出ART-RL作为强化学习框架，将时间变化建模为连续时间强化学习问题，使用高斯策略并通过actor-critic方法学习最优时间调度。

Result: 基于EDM官方流程，ART-RL在CIFAR-10上显著改善了Fréchet Inception Distance（FID），并且在AFHQv2、FFHQ和ImageNet等数据集上具有良好的迁移性，无需重新训练。

Conclusion: ART-RL提供了一种数据驱动的方法来学习扩散模型的最优时间调度，显著提升采样质量并具有良好的跨数据集迁移能力。

Abstract: We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fréchet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.

</details>


### [32] [FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices](https://arxiv.org/abs/2601.17063)
*Byeongju Kim,Jungwan Lee,Donghyeon Han,Hoi-Jun Yoo,Sangyeob Kim*

Main category: cs.LG

TL;DR: FlashMoE：一个将不活跃专家卸载到SSD的系统，用于在有限RAM下实现高效的MoE推理，通过轻量级ML缓存策略提升缓存命中率


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然通过稀疏激活实现高效推理，但现有DRAM卸载系统不适合内存受限的移动设备环境，且随着模型增长到数百GB，RAM卸载方案变得不切实际

Method: 提出FlashMoE系统，将不活跃专家卸载到SSD，采用轻量级基于机器学习的缓存策略，自适应结合最近使用和频率信号以最大化专家重用，减少存储I/O

Result: 在真实硬件设置上，FlashMoE比LRU和LFU等常见卸载策略提升缓存命中率最高达51%，比现有MoE推理系统加速最高达2.6倍

Conclusion: FlashMoE通过SSD卸载和智能缓存策略，成功解决了内存受限环境下大规模MoE模型推理的挑战，为边缘设备上的高效MoE推理提供了实用解决方案

Abstract: Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.

</details>


### [33] [ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting](https://arxiv.org/abs/2601.17065)
*Haoxuan Li,He Chang,Yunshan Ma,Yi Bin,Yang Yang,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出ThinkTank-ME框架，通过模拟真实智库的多专家协作机制来改进中东事件预测，相比现有单模型方法能更好地捕捉复杂地缘政治细微差别。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的事件预测方法采用单一模型架构，只能生成单一明确轨迹的预测，无法捕捉复杂区域背景下多样化的地缘政治细微差别。中东地区的事件预测特别需要考虑国际关系、区域历史动态和文化背景等多方面因素。

Method: 提出ThinkTank-ME框架，模拟真实世界战略决策中的协作专家分析。构建了POLECAT-FOR-ME基准数据集来促进专家专业化和严格评估，采用多专家协作机制处理复杂的地缘政治预测任务。

Result: 实验结果表明，多专家协作在处理复杂时序地缘政治预测任务方面具有优越性，相比单一模型方法能更好地应对中东地区的复杂预测挑战。

Conclusion: ThinkTank-ME框架通过模拟智库的多专家协作机制，显著提升了中东事件预测的准确性和对复杂地缘政治细微差别的捕捉能力，为解决复杂区域事件预测问题提供了新思路。

Abstract: Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.

</details>


### [34] [Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis](https://arxiv.org/abs/2601.17073)
*Yifei Zhang,Meimei Liu,Zhengwu Zhang*

Main category: cs.LG

TL;DR: CM-JIVNet是一个概率框架，用于从配对的脑结构-功能连接数据中学习因子化潜在表示，通过多头注意力融合模块捕获跨模态依赖并分离模态特定信号，在HCP-YA数据上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 脑组织通过多种成像模态（特别是结构连接SC和功能连接FC）进行表征，整合这些本质上不同但互补的数据源对于揭示驱动行为表型的跨模态模式至关重要。然而，有效整合受到连接组数据的高维度和非线性、复杂的非线性SC-FC耦合以及从模态特定变化中分离共享信息的挑战所阻碍。

Method: 提出跨模态联合-个体变分网络（CM-JIVNet），这是一个统一的概率框架，设计用于从配对的SC-FC数据集中学习因子化潜在表示。模型使用多头注意力融合模块来捕获非线性跨模态依赖，同时分离独立的模态特定信号。

Result: 在人类连接组项目年轻成人（HCP-YA）数据上验证，CM-JIVNet在跨模态重建和行为特征预测方面表现出优越性能。

Conclusion: 通过有效分离联合和个体特征空间，CM-JIVNet为大规模多模态脑分析提供了一个稳健、可解释且可扩展的解决方案。

Abstract: Brain organization is increasingly characterized through multiple imaging modalities, most notably structural connectivity (SC) and functional connectivity (FC). Integrating these inherently distinct yet complementary data sources is essential for uncovering the cross-modal patterns that drive behavioral phenotypes. However, effective integration is hindered by the high dimensionality and non-linearity of connectome data, complex non-linear SC-FC coupling, and the challenge of disentangling shared information from modality-specific variations. To address these issues, we propose the Cross-Modal Joint-Individual Variational Network (CM-JIVNet), a unified probabilistic framework designed to learn factorized latent representations from paired SC-FC datasets. Our model utilizes a multi-head attention fusion module to capture non-linear cross-modal dependencies while isolating independent, modality-specific signals. Validated on Human Connectome Project Young Adult (HCP-YA) data, CM-JIVNet demonstrates superior performance in cross-modal reconstruction and behavioral trait prediction. By effectively disentangling joint and individual feature spaces, CM-JIVNet provides a robust, interpretable, and scalable solution for large-scale multimodal brain analysis.

</details>


### [35] [Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic](https://arxiv.org/abs/2601.18783)
*Deepthi Pathare,Leo Laine,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 提出基于PPO的多目标强化学习框架，为重型卡车学习连续帕累托最优策略集，平衡安全、能耗和时间效率的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 高速公路驾驶中，重型车辆需要在安全、效率和运营成本之间做出复杂权衡。传统的标量奖励函数通过聚合这些竞争目标，往往掩盖了它们之间的权衡结构。

Method: 基于近端策略优化(PPO)的多目标强化学习框架，在卡车战术决策的可扩展仿真平台上学习连续策略集，明确表示安全（碰撞和完成率）、能源效率（能源成本）和时间效率（驾驶员成本）之间的权衡。

Result: 学习到平滑且可解释的连续帕累托最优策略集，能够在不重新训练的情况下在不同驾驶策略之间无缝切换，为自动驾驶卡车应用提供稳健且自适应的决策策略。

Conclusion: 该框架通过明确表示多目标权衡关系，实现了灵活选择不同冲突目标下的驾驶行为，为重型车辆高速公路驾驶提供了有效的决策支持。

Abstract: Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.

</details>


### [36] [Multi-Agent Deep Reinforcement Learning Under Constrained Communications](https://arxiv.org/abs/2601.17069)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: 提出分布式多智能体强化学习框架DG-MAPPO，通过分布式图注意力网络实现多跳通信，完全消除对集中式训练和全局信息的依赖


<details>
  <summary>Details</summary>
Motivation: 集中式训练分散式执行(CTDE)范式存在可扩展性、鲁棒性和泛化性瓶颈，依赖训练时的全局状态信息，在实际场景中(如队友增减、环境动态变化)脆弱且重训练成本高

Method: 开发分布式图注意力网络(D-GAT)通过多跳通信进行全局状态推断，基于此构建分布式图注意力MAPPO(DG-MAPPO)，智能体仅使用局部观测、多跳通信和共享/平均奖励来优化本地策略和价值函数

Result: 在StarCraftII多智能体挑战、Google Research Football和Multi-Agent Mujoco等任务上，DG-MAPPO始终优于强CTDE基线，在同质和异质团队的各种协作任务中实现更优协调

Conclusion: 提出的分布式MARL框架为鲁棒协作提供了原则性和可扩展的解决方案，完全消除了对集中式训练或全局可观测性的需求，是首个仅通过点对点通信实现学习和行动的框架

Abstract: Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.

</details>


### [37] [Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning](https://arxiv.org/abs/2601.18626)
*Yingxiao Huo,Satya Prakash Dash,Radu Stoican,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出一种基于秩1近似的自然策略优化方法，通过近似逆Fisher信息矩阵降低计算复杂度，在多种环境中优于标准actor-critic和信任域基线方法。


<details>
  <summary>Details</summary>
Motivation: 自然梯度在深度强化学习中具有快速收敛特性，但计算自然梯度需要每次迭代都求逆Fisher信息矩阵，这在计算上非常昂贵。因此需要开发高效可扩展的自然策略优化技术。

Method: 提出一种利用秩1近似来逼近完整逆Fisher信息矩阵的自然策略优化方法。该方法通过低秩近似降低计算复杂度，同时保持自然梯度的优势。

Result: 理论证明在特定条件下，秩1近似逆FIM比策略梯度收敛更快，在某些条件下与随机策略梯度方法具有相同的样本复杂度。在多种环境中测试表明，该方法优于标准actor-critic和信任域基线方法。

Conclusion: 提出的秩1近似自然策略优化方法在保持自然梯度优势的同时显著降低了计算复杂度，实现了高效可扩展的自然梯度强化学习算法。

Abstract: Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.

</details>


### [38] [PhysE-Inv: A Physics-Encoded Inverse Modeling approach for Arctic Snow Depth Prediction](https://arxiv.org/abs/2601.17074)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: PhysE-Inv：一种结合物理约束与深度学习的新型框架，用于解决北极雪深反演问题，在数据稀疏和噪声环境下显著提升预测精度和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 北极雪深估计是一个关键的时间变化反演问题，现有基于过程的模型对稀疏数据高度敏感，而数据驱动模型缺乏物理可解释性，无法满足气候关键应用的需求。

Method: 提出PhysE-Inv框架，整合LSTM编码器-解码器多头注意力架构与物理引导对比学习，采用基于静水平衡前向模型的物理约束反演方法，通过潜在空间重建物理正则化从噪声、不完整时间序列中动态发现隐藏物理参数。

Result: 相比最先进基线方法，PhysE-Inv显著提升预测性能，误差降低20%，在物理一致性和数据稀疏性鲁棒性方面优于经验方法。

Conclusion: 该方法为噪声容忍、可解释的反演建模开辟了新路径，在地理空间和冰冻圈领域具有广泛适用性。

Abstract: The accurate estimation of Arctic snow depth ($h_s$) remains a critical time-varying inverse problem due to the extreme scarcity and noise inherent in associated sea ice parameters. Existing process-based and data-driven models are either highly sensitive to sparse data or lack the physical interpretability required for climate-critical applications. To address this gap, we introduce PhysE-Inv, a novel framework that integrates a sophisticated sequential architecture, an LSTM Encoder-Decoder with Multi-head Attention and physics-guided contrastive learning, with physics-guided inference.Our core innovation lies in a surjective, physics-constrained inversion methodology. This methodology first leverages the hydrostatic balance forward model as a target-formulation proxy, enabling effective learning in the absence of direct $h_s$ ground truth; second, it uses reconstruction physics regularization over a latent space to dynamically discover hidden physical parameters from noisy, incomplete time-series input. Evaluated against state-of-the-art baselines, PhysE-Inv significantly improves prediction performance, reducing error by 20\% while demonstrating superior physical consistency and resilience to data sparsity compared to empirical methods. This approach pioneers a path for noise-tolerant, interpretable inverse modeling, with wide applicability in geospatial and cryospheric domains.

</details>


### [39] [E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning](https://arxiv.org/abs/2601.17076)
*Jiajun Chen,Yue Wu,Kai Huang,Wen Xi,Yangyang Wu,Xiaoye Miao,Mengying Zhu,Meng Xi,Guanjie Cheng*

Main category: cs.LG

TL;DR: E2PL框架通过任务定制提示和缺失感知提示解决多视图多标签分类中的视图缺失和类别增量学习问题，使用高效原型张量化将参数复杂度从指数级降至线性级。


<details>
  <summary>Details</summary>
Motivation: 现实网络应用中多视图多标签分类面临视图缺失和类别持续新增的挑战，现有方法无法同时处理这两个问题，要么缺乏对新类别的适应性，要么在处理缺失视图模式时参数呈指数增长，限制了可扩展性。

Method: 提出E2PL框架，包含：1）任务定制提示用于类别增量适应；2）缺失感知提示用于灵活整合任意视图缺失场景；3）高效原型张量化模块通过原子张量分解将提示参数复杂度从指数级降至线性级；4）动态对比学习策略显式建模不同缺失视图模式间的复杂依赖关系。

Result: 在三个基准测试上的广泛实验表明，E2PL在效果和效率方面均优于现有最先进方法。

Conclusion: E2PL框架有效解决了不完整多视图多标签类别增量学习问题，通过创新的提示设计和高效参数化方法，在保持模型性能的同时显著提升了可扩展性。

Abstract: Multi-view multi-label classification (MvMLC) is indispensable for modern web applications aggregating information from diverse sources. However, real-world web-scale settings are rife with missing views and continuously emerging classes, which pose significant obstacles to robust learning. Prevailing methods are ill-equipped for this reality, as they either lack adaptability to new classes or incur exponential parameter growth when handling all possible missing-view patterns, severely limiting their scalability in web environments. To systematically address this gap, we formally introduce a novel task, termed \emph{incomplete multi-view multi-label class incremental learning} (IMvMLCIL), which requires models to simultaneously address heterogeneous missing views and dynamic class expansion. To tackle this task, we propose \textsf{E2PL}, an Effective and Efficient Prompt Learning framework for IMvMLCIL. \textsf{E2PL} unifies two novel prompt designs: \emph{task-tailored prompts} for class-incremental adaptation and \emph{missing-aware prompts} for the flexible integration of arbitrary view-missing scenarios. To fundamentally address the exponential parameter explosion inherent in missing-aware prompts, we devise an \emph{efficient prototype tensorization} module, which leverages atomic tensor decomposition to elegantly reduce the prompt parameter complexity from exponential to linear w.r.t. the number of views. We further incorporate a \emph{dynamic contrastive learning} strategy explicitly model the complex dependencies among diverse missing-view patterns, thus enhancing the model's robustness. Extensive experiments on three benchmarks demonstrate that \textsf{E2PL} consistently outperforms state-of-the-art methods in both effectiveness and efficiency. The codes and datasets are available at https://anonymous.4open.science/r/code-for-E2PL.

</details>


### [40] [SFO: Learning PDE Operators via Spectral Filtering](https://arxiv.org/abs/2601.17090)
*Noam Koren,Rafael Moschopoulos,Kira Radinsky,Elad Hazan*

Main category: cs.LG

TL;DR: SFO是一种新的神经算子，使用通用谱基参数化积分核，能高效捕捉PDE中的长程非局部相互作用，在多个基准测试中达到SOTA精度。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在捕捉PDE解映射中的长程非局部相互作用时效率低下，而离散格林函数具有空间线性动力系统结构，这为高效表示提供了理论基础。

Method: 提出谱滤波算子(SFO)，使用从希尔伯特矩阵特征模态导出的通用谱基参数化积分核，仅学习快速衰减特征值的谱系数，实现高效表示。

Result: 在六个基准测试（包括反应扩散、流体动力学和3D电磁学）中，SFO达到最先进精度，相对于强基线误差减少达40%，同时使用更少参数。

Conclusion: SFO通过谱基表示提供了一种高效捕捉PDE中长程相互作用的神经算子框架，在精度和参数效率方面均有显著提升。

Abstract: Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.

</details>


### [41] [CUROCKET: Optimizing ROCKET for GPU](https://arxiv.org/abs/2601.17091)
*Ole Stüven,Keno Moenck,Thorsten Schüppstuhl*

Main category: cs.LG

TL;DR: CUROCKET：一种在GPU上高效执行ROCKET特征提取的算法，相比CPU版本实现高达11倍的每瓦计算效率提升


<details>
  <summary>Details</summary>
Motivation: ROCKET是2019年提出的时间序列分类特征提取算法，虽然精度与当时最佳算法相当且计算成本显著降低，但现有实现主要局限于CPU执行。卷积操作高度可并行化，适合在GPU上执行以获得显著加速，但ROCKET使用的非均匀卷积核使得标准GPU卷积方法效率低下。

Method: 提出CUROCKET算法，专门针对ROCKET的非均匀卷积核设计，能够在GPU上高效执行ROCKET特征提取。通过优化GPU并行化策略，克服了标准卷积方法在处理非均匀核时的效率问题。

Result: CUROCKET相比CPU上的ROCKET实现，达到高达11倍的每瓦计算效率提升，显著加速了特征提取过程，同时代码已在GitHub上开源。

Conclusion: CUROCKET成功解决了ROCKET在GPU上执行效率低下的问题，通过专门设计的算法实现了显著的性能提升，为时间序列分类任务提供了更高效的计算解决方案。

Abstract: ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository https://github.com/oleeven/CUROCKET on github.

</details>


### [42] [The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations](https://arxiv.org/abs/2601.17093)
*Olha Sirikova,Alvin Chan*

Main category: cs.LG

TL;DR: 提出Triangle of Similarity框架，结合三种互补视角（静态表征相似性、功能相似性、稀疏性相似性）来全面比较神经网络表示，发现架构家族是相似性的主要决定因素，并揭示了剪枝过程中的有趣现象。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络表示比较方法往往提供有限视角，需要更全面的框架来理解模型在科学应用中的内部机制是否收敛到相似状态。

Method: 提出Triangle of Similarity框架，结合三种互补视角：1) 静态表征相似性（CKA/Procrustes），2) 功能相似性（线性模式连接性或预测相似性），3) 稀疏性相似性（剪枝下的鲁棒性）。在CNN、Vision Transformer和视觉语言模型上，使用分布内（ImageNetV2）和分布外（CIFAR-10）测试集进行分析。

Result: 1) 架构家族是表征相似性的主要决定因素，形成明显聚类；2) CKA自相似性与任务准确率在剪枝过程中强相关，但准确率下降更快；3) 对某些模型对，剪枝似乎能正则化表示，暴露出共享的计算核心。

Conclusion: 该框架提供了更全面的方法来评估模型是否收敛到相似的内部机制，为科学研究中的模型选择和分析提供了有用工具。

Abstract: Comparing neural network representations is essential for understanding and validating models in scientific applications. Existing methods, however, often provide a limited view. We propose the Triangle of Similarity, a framework that combines three complementary perspectives: static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyzing a range of CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds, our initial findings suggest that: (1) architectural family is a primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; and (3) for some model pairs, pruning appears to regularize representations, exposing a shared computational core. This framework offers a more holistic approach for assessing whether models have converged on similar internal mechanisms, providing a useful tool for model selection and analysis in scientific research.

</details>


### [43] [Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation](https://arxiv.org/abs/2601.17094)
*Junichiro Niimi*

Main category: cs.LG

TL;DR: 该论文提出"嘴不是大脑"的架构原则，将世界模型与语言模型分离，通过Deep Boltzmann Machine作为世界模型、适配器和冻结GPT-2组成系统，在消费者评论领域验证了该方法能提高生成质量、区分合理配置并实现可控生成。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然能生成流畅文本，但对其是否真正理解世界还是仅产生看似合理的语言存在争议。作者旨在探索如何明确分离世界理解和语言能力，验证通过连接适当世界模型能否使小规模语言模型实现一致、可控的生成。

Method: 提出"嘴不是大脑"架构原则，包含三个组件：1) Deep Boltzmann Machine作为基于能量的世界模型捕获领域结构；2) 适配器将潜在信念状态投影到嵌入空间；3) 冻结的GPT-2提供语言能力但不含领域知识。在亚马逊智能手机评论领域实例化该框架。

Result: 实验表明：1) 通过世界模型调节比仅基于提示的生成显著提高情感相关性、降低困惑度、增加语义相似性；2) DBM能量函数能区分连贯与不连贯的市场配置，对不合理品牌-价格组合分配更高能量；3) 对特定属性的干预能因果传播到生成文本，干预输出与自然样本分布统计一致。

Conclusion: 研究表明，即使小规模语言模型在连接到适当世界模型时也能实现一致、可控的生成，为分离语言能力与世界理解提供了实证支持，验证了"嘴不是大脑"架构原则的有效性。

Abstract: Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.

</details>


### [44] [MambaNet: Mamba-assisted Channel Estimation Neural Network With Attention Mechanism](https://arxiv.org/abs/2601.17108)
*Dianxin Luan,Chengsi Liang,Jie Huang,Zheng Lin,Kaitao Meng,John Thompson,Cheng-Xiang Wang*

Main category: cs.LG

TL;DR: 提出一种结合Mamba架构和自注意力机制的神经网络框架，用于OFDM系统的低复杂度信道估计，特别适用于大规模子载波配置。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的神经网络在信道估计中计算复杂度高，特别是对于大规模子载波配置。需要一种既能捕捉子载波间长距离依赖关系，又能保持低复杂度的解决方案。

Method: 提出Mamba辅助的神经网络框架，集成定制化的Mamba架构，采用双向选择性扫描机制来处理非因果性的子载波信道增益，同时结合自注意力机制。

Result: 在3GPP TS 36.101信道上的仿真结果表明，相比其他基线神经网络方法，该方法在减少可调参数数量的同时，实现了更好的信道估计性能，且空间复杂度低于基于Transformer的网络。

Conclusion: 该框架为大规模子载波OFDM系统提供了一种高效的信道估计解决方案，在性能和复杂度之间取得了良好平衡，特别适合实际通信系统应用。

Abstract: This paper proposes a Mamba-assisted neural network framework incorporating self-attention mechanism to achieve improved channel estimation with low complexity for orthogonal frequency-division multiplexing (OFDM) waveforms, particularly for configurations with a large number of subcarriers. With the integration of customized Mamba architecture, the proposed framework handles large-scale subcarrier channel estimation efficiently while capturing long-distance dependencies among these subcarriers effectively. Unlike conventional Mamba structure, this paper implements a bidirectional selective scan to improve channel estimation performance, because channel gains at different subcarriers are non-causal. Moreover, the proposed framework exhibits relatively lower space complexity than transformer-based neural networks. Simulation results tested on the 3GPP TS 36.101 channel demonstrate that compared to other baseline neural network solutions, the proposed method achieves improved channel estimation performance with a reduced number of tunable parameters.

</details>


### [45] [Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts](https://arxiv.org/abs/2601.17111)
*Xuan-Phi Nguyen,Shrey Pandit,Austin Xu,Caiming Xiong,Shafiq Joty*

Main category: cs.LG

TL;DR: MoE模型在推理时存在路由不平衡问题，导致专家并行计算效率低下。本文提出LLEP算法，通过动态重路由超载token到空闲设备，显著提升推理速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE模型在预训练时使用负载均衡约束，但实际推理时仍会出现显著的路由不平衡。这种不平衡虽然有助于领域知识集中，但在专家并行架构下会导致设备过载，造成计算和内存瓶颈，影响推理效率。

Method: 提出Least-Loaded Expert Parallelism (LLEP)算法：动态监测设备负载，将超载设备上的多余token和相关专家参数重路由到空闲设备，确保所有设备在最小集体延迟内完成工作，同时满足内存约束。

Result: 在不同模型规模下，LLEP相比标准EP实现最高5倍加速和4倍峰值内存使用减少。在gpt-oss-120b模型上实现约1.9倍推理加速。通过理论分析和实证评估验证了方法的有效性。

Conclusion: LLEP算法有效解决了MoE模型在推理时的路由不平衡问题，显著提升了专家并行计算的效率和可扩展性。该方法为硬件特定的超参数调优提供了理论框架，实现了最优性能。

Abstract: Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.

</details>


### [46] [Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization](https://arxiv.org/abs/2601.17112)
*A. El Ichi,K. Jbilou*

Main category: cs.LG

TL;DR: 提出基于cproduct的张量压缩框架，用于降低LLMs的内存占用和计算成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能优异，但存在内存占用大、计算成本高的问题，需要更高效的压缩方法

Method: 利用cproduct的代数结构，在变换域中表示权重张量，通过低秩张量因子联合近似frontal slices，利用多维相关性进行压缩

Result: 该方法能够实现计算高效的压缩，超越传统的SVD方法

Conclusion: 基于cproduct的张量压缩框架为LLMs的高效压缩提供了新途径

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.

</details>


### [47] [How does Graph Structure Modulate Membership-Inference Risk for Graph Neural Networks?](https://arxiv.org/abs/2601.17130)
*Megha Khosla*

Main category: cs.LG

TL;DR: 该论文研究了图神经网络中的成员推理攻击风险，特别关注图结构对节点级别隐私泄露的影响，并分析了差分隐私GNN的可审计性。


<details>
  <summary>Details</summary>
Motivation: GNN在敏感应用中的使用引发了训练数据泄露的担忧。现有隐私泄露研究主要借鉴非图领域（如图像和表格数据）的发现，缺乏针对图结构的专门分析。论文强调需要进行图特定的隐私分析，研究图结构对节点级别成员推理攻击的影响。

Method: 1. 形式化节点-邻居元组的成员推理攻击；2. 研究两个关键维度：训练图构建方法和推理时的边访问权限；3. 实证分析不同采样方法（随机采样vs雪球采样）的影响；4. 研究差分隐私GNN的可审计性，将统计可交换性概念适配到图模型中。

Result: 1. 雪球采样的覆盖偏差通常损害泛化能力；2. 推理时允许训练-测试边访问能提高测试准确率、缩小训练-测试差距，并在大多数模型和数据集上产生最低的成员推理优势；3. 泛化差距（训练和测试节点性能差异）是成员推理风险的不完整代理指标；4. 对于节点级别任务，归纳分割（随机或雪球采样）破坏了可交换性，限制了差分隐私模型成员推理优势标准界限的适用性。

Conclusion: 图结构对GNN隐私泄露有重要影响，需要专门的图特定分析。推理时的边访问权限是成员推理风险的关键因素，而泛化差距不能完全反映隐私风险。差分隐私GNN的可审计性受到图数据分割方式的影响，标准隐私界限在节点级别任务中适用性有限。

Abstract: Graph neural networks (GNNs) have become the standard tool for encoding data and their complex relationships into continuous representations, improving prediction accuracy in several machine learning tasks like node classification and link prediction. However, their use in sensitive applications has raised concerns about the potential leakage of training data. Research on privacy leakage in GNNs has largely been shaped by findings from non-graph domains, such as images and tabular data. We emphasize the need of graph specific analysis and investigate the impact of graph structure on node level membership inference. We formalize MI over node-neighbourhood tuples and investigate two important dimensions: (i) training graph construction and (ii) inference-time edge access. Empirically, snowball's coverage bias often harms generalisation relative to random sampling, while enabling inter-train-test edges at inference improves test accuracy, shrinks the train-test gap, and yields the lowest membership advantage across most of the models and datasets. We further show that the generalisation gap empirically measured as the performance difference between the train and test nodes is an incomplete proxy for MI risk: access to edges dominates-MI can rise or fall independent of gap changes. Finally, we examine the auditability of differentially private GNNs, adapting the definition of statistical exchangeability of train-test data points for graph based models. We show that for node level tasks the inductive splits (random or snowball sampled) break exchangeability, limiting the applicability of standard bounds for membership advantage of differential private models.

</details>


### [48] [RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding](https://arxiv.org/abs/2601.17641)
*Hao Fang,Ryan A. Canfield,Tomohiro Ouchi,Beatrice Macagno,Eli Shlizerman,Amy L. Orsborn*

Main category: cs.LG

TL;DR: 提出RPNT（鲁棒预训练神经Transformer），通过预训练实现跨会话、跨类型、跨被试、跨位点的神经活动解码泛化，在多个任务中超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 脑解码需要模型能够泛化到不同的脑区记录、不同会话、不同行为类型和不同被试。现有模型只能部分解决这些挑战，需要开发能够适应和泛化的预训练神经Transformer模型。

Method: 1) 多维旋转位置嵌入(MRoPE)聚合实验元数据；2) 基于上下文的注意力机制通过卷积核处理全局注意力以学习局部时间结构；3) 鲁棒的自监督学习目标，包含均匀因果掩码策略和对比表示。在两个不同数据集上预训练：多会话多任务多被试微电极基准数据集和Neuropixel 1.0多站点记录数据集。

Result: RPNT在跨会话、跨类型、跨被试、跨站点的下游行为解码任务中，始终达到并超越了现有解码模型的性能。

Conclusion: RPNT通过创新的架构设计和预训练策略，实现了脑解码模型的鲁棒泛化能力，为神经活动解码提供了有效的预训练Transformer解决方案。

Abstract: Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.

</details>


### [49] [Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation](https://arxiv.org/abs/2601.17133)
*Inderjeet Singh,Eleonore Vissol-Gaudin,Andikan Otung,Motoyoshi Sekiya*

Main category: cs.LG

TL;DR: KNEXA-FL：一种用于大语言模型微调的协调去中心化联邦学习框架，通过上下文多臂老虎机算法优化异构代理间的点对点知识交换，解决了数据隐私与协作效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域微调面临数据隐私与协作需求的矛盾：传统联邦学习的中心化聚合存在单点故障和模型反演攻击风险，而去中心化联邦学习的随机点对点配对效率低下且可能导致负迁移。

Method: 提出KNEXA-FL框架，采用非聚合的中心化分析器/匹配器，将点对点协作建模为上下文多臂老虎机问题，使用LinUCB算法基于抽象代理特征学习最优匹配策略，通过安全蒸馏实现异构PEFT-based LLM代理间的直接知识交换。

Result: 在代码生成任务上的实验表明，KNEXA-FL相比随机点对点协作将Pass@1指标提升约50%，且表现出稳定的收敛性，而强大的中心化蒸馏基线则出现灾难性性能崩溃。

Conclusion: 自适应、基于学习的协调机制是构建稳健有效的去中心化AI生态系统的基础原则，KNEXA-FL成功解决了数据隐私保护与高效协作之间的权衡问题。

Abstract: Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.

</details>


### [50] [An Unsupervised Tensor-Based Domain Alignment](https://arxiv.org/abs/2601.18564)
*Chong Hyun Lee,Kibae Lee,Hyun Hee Yim*

Main category: cs.LG

TL;DR: 提出基于张量的域对齐算法，通过对齐矩阵在不变子空间中对齐源和目标张量，使用斜流形约束提供比传统Stiefel流形更大的灵活性，并通过正则化项保持方差，提升分类精度和转换速度。


<details>
  <summary>Details</summary>
Motivation: 现有张量域对齐方法通常使用Stiefel流形约束，灵活性有限。需要更灵活的流形约束来提升域对齐性能，同时保持源和目标张量的方差特性。

Method: 提出基于斜流形的张量域对齐框架，通过迭代优化对齐矩阵和不变子空间，使用正则化项保持源和目标张量的方差，将现有张量域对齐方法作为特例包含。

Result: 实验表明该方法不仅显著提升域对齐转换速度，还大幅提高分类准确率，优于当前最先进技术。

Conclusion: 提出的基于斜流形的张量域对齐方法在灵活性和性能上优于传统方法，是复杂域适应任务的优选方案。

Abstract: We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.

</details>


### [51] [ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning](https://arxiv.org/abs/2601.17135)
*Jakob Karalus,Friedhelm Schwenker*

Main category: cs.LG

TL;DR: ConceptACT：通过概念感知注意力机制改进模仿学习，利用人类提供的语义概念注释（物体属性、空间关系、任务约束）提升学习效率，无需部署时输入语义信息。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习方法仅依赖低级传感器数据，忽略了人类自然拥有的丰富语义知识。人类在演示任务时具有对物体属性、空间关系和任务约束的语义理解，这些知识可以显著提升机器人学习效率。

Method: 扩展Action Chunking with Transformers（ACT），在训练时利用episode级别的语义概念注释。采用改进的transformer架构，在最终编码器层实现概念感知交叉注意力机制，监督其与人类注释对齐。概念仅在演示收集时由人类提供，部署时无需语义输入。

Result: 在两个具有逻辑约束的机器人操作任务上，ConceptACT比标准ACT收敛更快，样本效率更高。注意力机制架构集成显著优于简单的辅助预测损失或语言条件模型。

Conclusion: 适当集成的语义监督为机器人学习提供了强大的归纳偏置，能够实现更高效的学习。概念感知注意力机制是有效利用人类语义知识的关键。

Abstract: Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.

</details>


### [52] [Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging](https://arxiv.org/abs/2601.17180)
*Inés Gonzalez-Pepe,Vinuyan Sivakolunthu,Jacob Fortin,Yohan Chatelain,Tristan Glatard*

Main category: cs.LG

TL;DR: 通过分析CNN数值不确定性，发现许多卷积操作对输出影响可忽略，提出Conservative & Aggressive NaNs方法，利用NaN标记数值不稳定体素以跳过计算，在神经影像任务中实现最高1.67倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在神经影像分析中规模日益增大，计算效率成为持续关注点。研究发现CNN中存在大量对数值噪声主导值的冗余操作，这些操作对模型输出影响可忽略，为计算优化提供了机会。

Method: 提出Conservative & Aggressive NaNs两种变体的最大池化和反池化方法，识别数值不稳定体素并用NaN替换，后续层可跳过对无关数据的计算。方法在PyTorch中实现，无需架构修改。

Result: 在包含至少50% NaN的输入中观察到一致的运行时改进；当NaN超过三分之二时（神经影像常见情况），平均推理加速1.67倍。Conservative NaNs平均减少30%卷积操作，无性能损失，特定层可跳过64.64%卷积；Aggressive NaNs可跳过69.30%卷积，但可能偶尔影响性能。

Conclusion: 数值不确定性可被利用来减少CNN中的冗余计算并提高推理效率，为深度学习模型优化提供了新思路，特别适用于神经影像等数据稀疏领域。

Abstract: Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.

</details>


### [53] [Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data](https://arxiv.org/abs/2601.17183)
*Farzam Asad,Junaid Saif Khan,Maria Tariq,Sundus Munir,Muhammad Adnan Khan*

Main category: cs.LG

TL;DR: 该研究通过模拟四家异构医院客户端，评估了FedProx在心脏病预测中的表现，证明其在非IID数据下优于集中式学习和孤立本地模型，同时保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 医疗数据因隐私法规无法共享，联邦学习提供了解决方案，但临床数据存在非IID特性（人口统计差异、疾病流行度和机构实践不同），需要有效处理异构性的方法。

Method: 基于UCI心脏病数据集（克利夫兰诊所303名患者），通过人口统计分层模拟四家异构医院客户端，创建现实非IID数据分区，使用FedProx算法进行联邦学习，并进行50次独立运行的消融研究。

Result: FedProx（μ=0.05）达到85.00%准确率，优于集中式学习（83.33%）和孤立本地模型（平均78.45%），近端正则化能有效抑制异构环境中的客户端漂移。

Conclusion: FedProx在非IID医疗数据中表现优异，为实际联邦医疗系统提供了算法见解和部署指南，结果可直接应用于医院IT管理员实施隐私保护协作学习。

Abstract: Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.

</details>


### [54] [Rethinking Benchmarks for Differentially Private Image Classification](https://arxiv.org/abs/2601.17189)
*Sabrina Mokhtari,Sara Kodeiri,Shubhankar Mohapatra,Florian Tramer,Gautam Kamath*

Main category: cs.LG

TL;DR: 该论文重新审视了差分隐私图像分类的基准测试，提出了全面的基准集，测试了现有技术，并创建了公开的排行榜来追踪进展。


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私机器学习领域缺乏全面的基准测试，使得不同技术难以在多样化设置下进行公平比较和评估。

Method: 提出一套全面的基准测试集，涵盖不同设置（有无额外数据、凸设置等），在多种不同性质的数据集上测试现有技术，并建立公开的排行榜系统。

Result: 建立了标准化的评估框架，识别了在不同设置下仍然有效的技术，为社区提供了可追踪进展的基准平台。

Conclusion: 该工作为差分隐私机器学习研究提供了重要的基准基础设施，促进了该领域的公平比较和技术进步追踪。

Abstract: We revisit benchmarks for differentially private image classification. We suggest a comprehensive set of benchmarks, allowing researchers to evaluate techniques for differentially private machine learning in a variety of settings, including with and without additional data, in convex settings, and on a variety of qualitatively different datasets. We further test established techniques on these benchmarks in order to see which ideas remain effective in different settings. Finally, we create a publicly available leader board for the community to track progress in differentially private machine learning.

</details>


### [55] [PUNCH: Physics-informed Uncertainty-aware Network for Coronary Hemodynamics](https://arxiv.org/abs/2601.17192)
*Sukirt Thakur,Marcus Roper,Yang Zhou,Reza Akbarian Bafghi,Brahmajee K. Nallamothu,C. Alberto Figueroa,Srinivas Paruchuri,Scott Burger,Maziar Raissi*

Main category: cs.LG

TL;DR: 提出一种非侵入性、不确定性感知的框架，直接从标准血管造影中估计冠状动脉血流储备，无需地面真实血流测量，通过物理信息神经网络和变分推理实现。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉微血管功能障碍影响全球数百万人，但由于金标准的生理测量是侵入性的且可重复性不一，导致诊断不足。需要一种非侵入性、可扩展的评估方法。

Method: 整合物理信息神经网络与变分推理，从造影剂传输的第一性原理模型推断冠状动脉血流，无需地面真实血流测量。使用合成时空强度图进行验证，并在临床患者中与侵入性热稀释法比较。

Result: 框架可靠识别退化数据并输出适当的不确定性估计，预测不确定性与误差强相关。临床验证显示与侵入性热稀释法强一致性，概率CFR估计的置信区间小于重复侵入测量的变异性。

Conclusion: 该方法将常规血管造影转化为定量、不确定性感知的评估，实现可扩展、更安全、更可重复的冠状动脉微血管功能评估，有望扩大CMD诊断的可及性。

Abstract: Coronary microvascular dysfunction (CMD) affects millions worldwide yet remains underdiagnosed because gold-standard physiological measurements are invasive and variably reproducible. We introduce a non-invasive, uncertainty-aware framework for estimating coronary flow reserve (CFR) directly from standard angiography. The system integrates physics-informed neural networks with variational inference to infer coronary blood flow from first-principles models of contrast transport, without requiring ground-truth flow measurements. The pipeline runs in approximately three minutes per patient on a single GPU, with no population-level training.
  Using 1{,}000 synthetic spatiotemporal intensity maps (kymographs) with controlled noise and artifacts, the framework reliably identifies degraded data and outputs appropriately inflated uncertainty estimates, showing strong correspondence between predictive uncertainty and error (Pearson $r = 0.997$, Spearman $ρ= 0.998$). Clinical validation in 12 patients shows strong agreement between PUNCH-derived CFR and invasive bolus thermodilution (Pearson $r = 0.90$, $p = 6.3 \times 10^{-5}$). We focus on the LAD, the artery most commonly assessed in routine CMD testing. Probabilistic CFR estimates have confidence intervals narrower than the variability of repeated invasive measurements.
  By transforming routine angiography into quantitative, uncertainty-aware assessment, this approach enables scalable, safer, and more reproducible evaluation of coronary microvascular function. Because standard angiography is widely available globally, the framework could expand access to CMD diagnosis and establish a new paradigm for physics-informed, patient-specific inference from clinical imaging.

</details>


### [56] [Accelerated Sinkhorn Algorithms for Partial Optimal Transport](https://arxiv.org/abs/2601.17196)
*Nghia Thu Truong,Qui Phu Pham,Quang Nguyen,Dung Luong,Mai Tran*

Main category: cs.LG

TL;DR: ASPOT算法通过结合交替最小化和Nesterov加速技术，将部分最优传输的Sinkhorn算法复杂度提升到O(n^{7/3}ε^{-5/3})，同时通过优化熵参数改进了经典Sinkhorn方法的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 部分最优传输(POT)适用于处理两个分布大小不等或包含异常值的情况，但现有的Sinkhorn方法在POT问题上的复杂度界限不够理想，限制了其可扩展性。

Method: 提出了加速Sinkhorn for POT (ASPOT)算法，将交替最小化与Nesterov风格加速技术相结合，应用于POT设置中。同时展示了通过明智选择熵参数γ可以改进经典Sinkhorn方法的收敛速度。

Result: ASPOT算法达到了O(n^{7/3}ε^{-5/3})的复杂度，优于现有方法。实验验证了理论结果，并展示了所提方法在实际应用中的优越性能。

Conclusion: ASPOT算法通过加速技术显著提升了部分最优传输问题的计算效率，为处理大规模或包含异常值的分布对齐问题提供了更高效的解决方案。

Abstract: Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\mathcal{O}(n^{7/3}\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $γ$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.

</details>


### [57] [SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment](https://arxiv.org/abs/2601.17204)
*Yinkai Wang,Yan Zhou Chen,Xiaohui Chen,Li-Ping Liu,Soha Hassoun*

Main category: cs.LG

TL;DR: SpecBridge提出了一种新颖的隐式对齐框架，通过将质谱编码器直接投影到预训练分子模型（ChemBERTa）的潜在空间中，显著提升了小分子质谱识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前小分子质谱识别面临两个极端：显式生成模型原子级构建分子图，或从零开始学习跨模态子空间的联合对比模型。这些方法在非靶向质谱分析中效果有限，因为谱库不完整，需要更有效的识别方法。

Method: SpecBridge采用隐式对齐框架，将结构识别视为几何对齐问题。它微调自监督质谱编码器（DreaMS），直接投影到冻结的分子基础模型（ChemBERTa）的潜在空间中，然后通过余弦相似度在预计算的分子嵌入库中进行检索。

Result: 在MassSpecGym、Spectraverse和MSnLib基准测试中，SpecBridge相对于强大的神经基线，将top-1检索准确率提高了约20-25%，同时保持可训练参数数量较少。

Conclusion: 与从头设计新架构相比，对齐到冻结的基础模型是一种实用且稳定的替代方案。该方法为小分子质谱识别提供了更有效的解决方案。

Abstract: Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at https://github.com/HassounLab/SpecBridge.

</details>


### [58] [NewPINNs: Physics-Informing Neural Networks Using Conventional Solvers for Partial Differential Equations](https://arxiv.org/abs/2601.17207)
*Maedeh Makki,Satish Chandran,Maziar Raissi,Adrien Grenier,Behzad Mohebbi*

Main category: cs.LG

TL;DR: NewPINNs是一种将神经网络与传统数值求解器耦合的物理信息学习框架，通过求解器一致性而非残差损失来训练网络，避免了传统PINNs的优化难题。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络(PINNs)存在优化病理、损失权重敏感、在刚性或非线性区域表现差等问题，需要设计问题特定的损失函数。

Method: 将数值求解器直接集成到训练循环中，神经网络生成候选解状态，数值求解器推进这些状态，训练最小化网络预测与求解器演化状态之间的差异。

Result: NewPINNs能够有效解决多个前向和反问题，涉及有限体积、有限元和谱求解器，避免了传统PINNs的失败模式。

Conclusion: 通过将物理约束、边界条件和数值稳定性委托给成熟的数值求解器，NewPINNs提供了一种更稳健的物理信息学习框架，无需问题特定的损失工程。

Abstract: We introduce NewPINNs, a physics-informing learning framework that couples neural networks with conventional numerical solvers for solving differential equations. Rather than enforcing governing equations and boundary conditions through residual-based loss terms, NewPINNs integrates the solver directly into the training loop and defines learning objectives through solver-consistency. The neural network produces candidate solution states that are advanced by the numerical solver, and training minimizes the discrepancy between the network prediction and the solver-evolved state. This pull-push interaction enables the network to learn physically admissible solutions through repeated exposure to the solver's action, without requiring problem-specific loss engineering or explicit evaluation of differential equation residuals. By delegating the enforcement of physics, boundary conditions, and numerical stability to established numerical solvers, NewPINNs mitigates several well-known failure modes of standard physics-informed neural networks, including optimization pathologies, sensitivity to loss weighting, and poor performance in stiff or nonlinear regimes. We demonstrate the effectiveness of the proposed approach across multiple forward and inverse problems involving finite volume, finite element, and spectral solvers.

</details>


### [59] [JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers](https://arxiv.org/abs/2601.17215)
*Ruoqing Zheng,Chang Sun,Qibin Liu,Lauri Laatu,Arianna Cox,Benedikt Maier,Alexander Tapper,Jose G. F. Coutinho,Wayne Luk,Zhiqiang Que*

Main category: cs.LG

TL;DR: JetFormer是一个用于LHC粒子喷注标记的Transformer架构，能在离线分析和在线触发等多种场景下高效运行，在保持高性能的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有喷注标记方法通常针对特定部署场景设计，缺乏一个统一的架构来覆盖从高精度离线分析到超低延迟在线触发的全谱系需求。需要一种既能保持高性能又具备硬件部署可行性的通用解决方案。

Method: 提出JetFormer，一种仅编码器的Transformer架构，处理可变长度的粒子特征集而不依赖显式成对相互作用。采用硬件感知优化流程，包括多目标超参数搜索、结构化剪枝和量化，生成适用于FPGA部署的紧凑变体。

Result: 在JetClass数据集上，JetFormer在减少37.4%FLOPs的情况下，性能与ParT模型相当（差异在0.7%以内）。在HLS4ML 150P基准数据集上，比MLPs、Deep Sets和Interaction Networks等现有模型准确率高3-4%。通过压缩技术，JetFormer-tiny变体可满足FPGA触发系统的亚微秒级延迟要求。

Conclusion: JetFormer通过统一的架构框架，将高性能建模和部署可行性相结合，为LHC的离线和在线环境中部署基于Transformer的喷注标记器提供了实用途径，实现了性能与效率的良好平衡。

Abstract: We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at https://github.com/walkieq/JetFormer.

</details>


### [60] [Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning](https://arxiv.org/abs/2601.17224)
*Dmitrii Torbunov,Yihui Ren,Lijun Wu,Yimei Zhu*

Main category: cs.LG

TL;DR: 将条件扩散模型从一维时间信号扩展到二维空间数据，用于材料表征中的CBED参数推断，能提供校准良好的后验分布，揭示参数不确定性。


<details>
  <summary>Details</summary>
Motivation: 科学逆问题中不确定性量化至关重要，需要区分可识别参数与测量数据下仍模糊的参数。先前CDI在一维时间信号上有效，但在高维空间数据的适用性未探索。

Method: 将条件扩散模型逆问题求解器（CDI）扩展到二维空间条件，直接从空间观测进行概率参数推断，在收敛束电子衍射（CBED）参数推断问题上验证。

Result: CDI产生校准良好的后验分布：对确定参数给出紧分布，对模糊参数给出适当宽分布。相比标准回归方法（掩盖不确定性），CDI能提供真实的不确定性信息。

Conclusion: CDI成功从时间域扩展到空间域，为稳健的科学推断提供所需的不确定性信息，在材料表征等复杂逆问题中具有重要应用价值。

Abstract: Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.

</details>


### [61] [A Constrained Optimization Perspective of Unrolled Transformers](https://arxiv.org/abs/2601.17257)
*Javier Porras-Valenzuela,Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一个约束优化框架，训练transformer像优化下降算法一样工作，通过层间下降约束和原始-对偶训练方案，使中间表示在期望上单调降低损失


<details>
  <summary>Details</summary>
Motivation: 希望transformer模型能够像优化算法一样具有可解释性和鲁棒性，使其中间层的表示能够系统性地降低目标函数，从而提高模型的泛化能力和抗扰动性

Method: 采用约束优化框架，在训练transformer时施加层间下降约束，用原始-对偶训练方案替代标准经验风险最小化，确保中间表示在期望上单调降低损失

Result: 在视频去噪和文本分类任务上，约束transformer表现出更强的抗扰动鲁棒性和更高的分布外泛化能力，同时保持分布内性能

Conclusion: 通过约束优化训练transformer使其具有优化下降算法的特性，可以显著提升模型的鲁棒性和泛化能力，为构建更可靠、可解释的深度学习模型提供了新思路

Abstract: We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.

</details>


### [62] [The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment](https://arxiv.org/abs/2601.17260)
*Marco Pollanen*

Main category: cs.LG

TL;DR: DPO的β参数不是简单的"越大越好"，不同架构模型对β的响应模式不同，偏好边界可能与推理能力负相关，高β训练会造成能力损失的滞后效应。


<details>
  <summary>Details</summary>
Motivation: 传统上认为增加DPO的对齐压力（β参数）会持续提升模型表现，但本文质疑这一假设，探索β作为控制参数对模型能力的影响。

Method: 对三个7B开源模型家族（Mistral、Llama、Qwen）在固定DPO配方下密集扫描β参数，分析逻辑探针边界、能力变化和训练路径依赖。

Result: 1) Mistral的能力呈尖锐非单调变化，仅在β≈10⁻²窄带内逻辑探针边界为正；2) 不同架构响应模式不同：Mistral重组、Llama选择性变化、Qwen平滑权衡；3) DPO偏好边界与推理能力负相关（Llama逻辑任务r=-0.91）；4) 高β训练导致能力损失，即使降低β也无法恢复（滞后效应）。

Conclusion: 需要在整个β参数空间进行能力解析评估，而非依赖偏好边界或聚合基准测试，β选择应考虑架构特性和训练历史。

Abstract: Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $β$) yields progressively "better" behavior. We instead treat $β$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $β\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $β$ induces capability losses that persist even after $β$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $β$ landscape rather than reliance on margins or aggregate benchmarks.

</details>


### [63] [AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning](https://arxiv.org/abs/2601.17261)
*Wei Lin,Yining Jiang,Qingyu Song,Qiao Xiang,Hong Xu*

Main category: cs.LG

TL;DR: AGZO是一种激活引导的零阶优化方法，利用前向传播中的激活结构信息，在低秩子空间中进行扰动，显著提升零阶优化的性能，同时保持内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法通常使用各向同性扰动，忽略了前向传播中丰富的结构信息。作者发现线性层的梯度被限制在其输入激活张成的子空间中，这为改进零阶优化提供了关键洞见。

Method: 提出激活引导零阶优化（AGZO），在前向传播过程中动态提取紧凑的激活信息子空间，并将扰动限制在这个低秩子空间中，而不是使用各向同性扰动。

Result: 在Qwen3和Pangu模型上的实验表明，AGZO持续优于最先进的零阶基线方法，显著缩小了与一阶微调的性能差距，同时保持了与其他零阶方法几乎相同的峰值内存占用。

Conclusion: AGZO通过利用激活结构信息改进了零阶优化，在严格内存约束下实现了接近一阶微调的性能，为零阶优化提供了新的理论框架和实践方法。

Abstract: Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.

</details>


### [64] [Unrolled Neural Networks for Constrained Optimization](https://arxiv.org/abs/2601.17274)
*Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一种名为约束对偶展开（CDU）的展开神经网络框架，用于解决约束优化问题，通过两个耦合网络近似拉格朗日函数的鞍点，在混合整数二次规划和无线网络功率分配中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统对偶上升算法在解决约束优化问题时存在计算效率问题，需要开发可学习、加速的替代方案。现有展开神经网络方法通常只关注原始问题，缺乏对偶变量的有效处理。

Method: 提出CDU框架，包含两个耦合神经网络：原始网络模拟迭代优化器寻找拉格朗日函数的驻点，对偶网络生成最优乘子轨迹。通过约束学习施加原始下降和对偶上升约束，采用交替训练策略更新两个网络。

Result: 在混合整数二次规划和无线网络功率分配问题上，CDU能够产生接近最优且接近可行的解，并展现出强大的分布外泛化能力。

Conclusion: CDU框架为约束优化问题提供了有效的可学习解决方案，通过耦合原始和对偶网络实现了对偶上升算法的加速学习版本，在多个应用场景中表现出优越性能。

Abstract: In this paper, we develop unrolled neural networks to solve constrained optimization problems, offering accelerated, learnable counterparts to dual ascent (DA) algorithms. Our framework, termed constrained dual unrolling (CDU), comprises two coupled neural networks that jointly approximate the saddle point of the Lagrangian. The primal network emulates an iterative optimizer that finds a stationary point of the Lagrangian for a given dual multiplier, sampled from an unknown distribution. The dual network generates trajectories towards the optimal multipliers across its layers while querying the primal network at each layer. Departing from standard unrolling, we induce DA dynamics by imposing primal-descent and dual-ascent constraints through constrained learning. We formulate training the two networks as a nested optimization problem and propose an alternating procedure that updates the primal and dual networks in turn, mitigating uncertainty in the multiplier distribution required for primal network training. We numerically evaluate the framework on mixed-integer quadratic programs (MIQPs) and power allocation in wireless networks. In both cases, our approach yields near-optimal near-feasible solutions and exhibits strong out-of-distribution (OOD) generalization.

</details>


### [65] [Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning](https://arxiv.org/abs/2601.17275)
*Lianlei Shan,Han Chen,Yixuan Wang,Zhenjie Liu,Wei Li*

Main category: cs.LG

TL;DR: DLR提出了一种潜在空间双向对比强化学习框架，将推理过程的试错成本从昂贵的token级序列生成转移到连续潜在流形，通过冻结主模型参数避免灾难性遗忘，实现更稳定的训练收敛和更长推理链支持。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂多步推理任务时往往只是"统计拟合"而非系统逻辑推理。传统强化学习虽然引入"先思考后说话"范式，但在高维离散token空间中面临样本效率低、梯度估计方差高和灾难性遗忘三大挑战。

Method: 提出DeepLatent Reasoning框架：1）使用轻量级辅助模型在潜在空间采样K个推理链编码；2）通过基于正确性和格式的双重奖励机制筛选高价值潜在轨迹；3）仅将筛选后的轨迹输入冻结主模型进行单次解码；4）设计对比学习目标实现潜在空间的有向探索。

Result: 在可比的GPU计算预算下，DLR实现了更稳定的训练收敛，支持更长的推理链，促进了推理能力的可持续积累，为LLMs提供了可靠且可扩展的强化学习路径。

Conclusion: DLR通过将强化学习从离散token空间转移到连续潜在空间，从根本上解决了传统方法的三大瓶颈，为LLMs的可靠强化学习提供了可行的技术路径，同时避免了灾难性遗忘问题。

Abstract: While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.

</details>


### [66] [Tabular Foundation Models are Strong Graph Anomaly Detectors](https://arxiv.org/abs/2601.17301)
*Yunhui Liu,Tieke He,Yongchao Liu,Can Yi,Hong Jin,Chuntao Hong*

Main category: cs.LG

TL;DR: TFM4GAD是一个将表格基础模型（TFMs）应用于图异常检测的框架，通过将图结构"扁平化"为增强特征表格，利用TFMs的合成预训练和上下文学习能力，实现了跨图数据集的高性能异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法遵循"一个模型对应一个数据集"的模式，导致计算成本高、数据需求大、泛化能力差。需要一种能够跨多个图数据集检测异常的通用基础模型，但面临图结构和特征异质性的挑战。

Method: 将图"扁平化"为增强特征表格，包含原始节点特征、拉普拉斯嵌入、局部和全局结构特征、异常敏感的邻域聚合。然后使用表格基础模型在完全上下文学习机制下处理这个表格。

Result: 在多个数据集和不同TFM骨干网络上的广泛实验表明，TFM4GAD显著优于从头训练的专业GAD模型，实现了性能提升。

Conclusion: TFM4GAD为利用TFMs作为强大、通用的图异常检测器提供了新视角和实践范式，展示了基础模型在图领域的潜力。

Abstract: Graph anomaly detection (GAD), which aims to identify abnormal nodes that deviate from the majority, has become increasingly important in high-stakes Web domains. However, existing GAD methods follow a "one model per dataset" paradigm, leading to high computational costs, substantial data demands, and poor generalization when transferred to new datasets. This calls for a foundation model that enables a "one-for-all" GAD solution capable of detecting anomalies across diverse graphs without retraining. Yet, achieving this is challenging due to the large structural and feature heterogeneity across domains. In this paper, we propose TFM4GAD, a simple yet effective framework that adapts tabular foundation models (TFMs) for graph anomaly detection. Our key insight is that the core challenges of foundation GAD, handling heterogeneous features, generalizing across domains, and operating with scarce labels, are the exact problems that modern TFMs are designed to solve via synthetic pre-training and powerful in-context learning. The primary challenge thus becomes structural: TFMs are agnostic to graph topology. TFM4GAD bridges this gap by "flattening" the graph, constructing an augmented feature table that enriches raw node features with Laplacian embeddings, local and global structural characteristics, and anomaly-sensitive neighborhood aggregations. This augmented table is processed by a TFM in a fully in-context regime. Extensive experiments on multiple datasets with various TFM backbones reveal that TFM4GAD surprisingly achieves significant performance gains over specialized GAD models trained from scratch. Our work offers a new perspective and a practical paradigm for leveraging TFMs as powerful, generalist graph anomaly detectors.

</details>


### [67] [Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach](https://arxiv.org/abs/2601.17303)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 提出去中心化多智能体群架构，作为IIoT网络的分布式数字免疫系统，实现亚毫秒级响应和97.3%恶意活动检测准确率


<details>
  <summary>Details</summary>
Motivation: 工业物联网环境扩展到数万台设备，集中式安全监控架构存在延迟问题，攻击者可利用此漏洞破坏整个制造生态系统

Method: 提出去中心化多智能体群架构，在每个边缘网关部署自主AI代理，通过轻量级P2P协议协作检测异常行为，无需向云端发送数据；采用基于共识的威胁验证流程，代理对识别威胁的威胁级别进行投票

Result: 在模拟2000台IIoT设备的创新工厂测试环境中，DMAS实现亚毫秒级响应时间（平均0.85ms），高负载下恶意活动检测准确率达97.3%，零日攻击检测准确率87%；相比云端解决方案减少89%网络带宽使用

Conclusion: DMAS架构显著优于集中式和边缘计算基线，能预防工业控制系统的实时级联故障，为IIoT网络提供高效分布式安全解决方案

Abstract: As Industrial Internet of Things (IIoT) environments expand to include tens of thousands of connected devices. The centralization of security monitoring architectures creates serious latency issues that savvy attackers can exploit to compromise an entire manufacturing ecosystem. This paper outlines a new, decentralized multi-agent swarm (DMAS) architecture that includes autonomous artificial intelligence (AI) agents at each edge gateway, functioning as a distributed digital "immune system" for IIoT networks. Instead of using a traditional static firewall approach, the DMAS agents communicate via a lightweight peer-to-peer protocol to cooperatively detect anomalous behavior across the IIoT network without sending data to a cloud infrastructure. The authors also outline a consensus-based threat validation (CVT) process in which agents vote on the threat level of an identified threat, enabling instant quarantine of a compromised node or nodes. The authors conducted experiments on a testbed that simulated an innovative factory environment with 2000 IIoT devices and found that the DMAS demonstrated sub-millisecond response times (average of 0.85ms), 97.3% accuracy in detecting malicious activity under high load, and 87% accuracy in detecting zero-day attacks. All significantly higher than baseline values for both centralized and edge computing. Additionally, the proposed architecture can prevent real-time cascading failures in industrial control systems and reduce network bandwidth use by 89% compared to cloud-based solutions.

</details>


### [68] [Weighted Graph Clustering via Scale Contraction and Graph Structure Learning](https://arxiv.org/abs/2601.17307)
*Haobing Liu,Yinuo Zhang,Tingting Wang,Ruobing Jiang,Yanwei Yu*

Main category: cs.LG

TL;DR: 提出了一种收缩边权重感知图聚类网络，通过图收缩模块减少图规模并保留重要节点，同时使用边权重感知注意力网络识别和削弱噪声连接，从而提升聚类效果。


<details>
  <summary>Details</summary>
Motivation: 现有图聚类方法未能充分利用边权重信息，而利用边权重面临两个关键挑战：1）边权重会显著增加存储空间和训练时间；2）边权重信息可能包含噪声影响聚类效果。需要同时优化聚类和边权重来减轻噪声边的影响。

Method: 提出收缩边权重感知图聚类网络，包含两个核心模块：1）面向聚类的图收缩模块，用于减少图规模同时保留对聚类任务重要的节点；2）边权重感知注意力网络，用于识别和削弱噪声连接。

Result: 在三个真实世界加权图数据集上进行实验，模型性能优于最佳基线方法。图收缩模块能显著减少训练时间和存储空间。

Conclusion: 提出的方法能有效处理加权图中的噪声边问题，通过联合优化聚类和边权重，在减少计算成本的同时提升聚类效果，为加权图聚类提供了有效解决方案。

Abstract: Graph clustering aims to partition nodes into distinct clusters based on their similarity, thereby revealing relationships among nodes. Nevertheless, most existing methods do not fully utilize these edge weights. Leveraging edge weights in graph clustering tasks faces two critical challenges. (1) The introduction of edge weights may significantly increase storage space and training time, making it essential to reduce the graph scale while preserving nodes that are beneficial for the clustering task. (2) Edge weight information may inherently contain noise that negatively impacts clustering results. However, few studies can jointly optimize clustering and edge weights, which is crucial for mitigating the negative impact of noisy edges on clustering task. To address these challenges, we propose a contractile edge-weight-aware graph clustering network. Specifically, a cluster-oriented graph contraction module is designed to reduce the graph scale while preserving important nodes. An edge-weight-aware attention network is designed to identify and weaken noisy connections. In this way, we can more easily identify and mitigate the impact of noisy edges during the clustering process, thus enhancing clustering effectiveness. We conducted extensive experiments on three real-world weighted graph datasets. In particular, our model outperforms the best baseline, demonstrating its superior performance. Furthermore, experiments also show that the proposed graph contraction module can significantly reduce training time and storage space.

</details>


### [69] [PAR: Plausibility-aware Amortized Recourse Generation](https://arxiv.org/abs/2601.17309)
*Anagha Sabu,Vidhya S,Narayanan C Krishnan*

Main category: cs.LG

TL;DR: PAR：基于约束MAP推理的概率性算法补救方法，通过摊销近似推理生成高似然、现实可行的反事实解释


<details>
  <summary>Details</summary>
Motivation: 现有算法补救方法在生成现实可行且高似然的补救建议方面存在不足，需要一种能够同时满足有效性、相似性、稀疏性和高似然性的方法

Method: 将补救问题形式化为约束最大后验概率（MAP）推理问题，提出摊销近似推理程序PAR，使用可精确计算似然的概率模型，结合邻域条件机制为具体事实生成定制化补救

Result: 在广泛使用的算法补救数据集上验证，PAR能高效生成有效、与事实相似、稀疏且高度合理的补救建议，性能优于现有最先进方法

Conclusion: PAR通过概率建模和摊销推理提供了一种高效生成高质量算法补救的方法，在满足多种约束的同时保证了补救建议的现实可行性

Abstract: Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.

</details>


### [70] [Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment](https://arxiv.org/abs/2601.17329)
*Tiejin Chen,Xiaoou Liu,Vishnu Nandam,Kuan-Ru Liou,Hua Wei*

Main category: cs.LG

TL;DR: 提出Conformal Feedback Alignment (CFA)框架，使用Conformal Prediction量化答案可靠性，为DPO和PPO训练提供原则性权重，提升对齐鲁棒性和数据效率


<details>
  <summary>Details</summary>
Motivation: 基于偏好的对齐方法（如RLHF）面临标签噪声和不一致问题，现有不确定性感知方法只关注偏好权重，忽略了被比较答案本身的可靠性这一更基本因素

Method: 提出Conformal Feedback Alignment (CFA)框架：1) 使用Conformal Prediction构建具有可控覆盖率的预测集来量化答案级可靠性；2) 将这些可靠性聚合为原则性权重，适用于DPO和PPO风格的训练

Result: 在不同数据集上的实验表明，CFA提高了对齐的鲁棒性和数据效率，证明建模答案侧不确定性可以补充偏好级加权，实现更鲁棒、数据高效的对齐

Conclusion: CFA通过Conformal Prediction量化答案可靠性，为偏好对齐提供原则性权重，有效解决了标签噪声问题，实现了更鲁棒和高效的对齐方法

Abstract: Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.

</details>


### [71] [Thermodynamically Optimal Regularization under Information-Geometric Constraints](https://arxiv.org/abs/2601.17330)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的理论框架，将热力学最优性、信息几何和正则化联系起来，证明在特定假设下，Fisher-Rao度量是信念空间唯一可容许的几何结构，热力学最优正则化对应于最小化到参考状态的Fisher-Rao距离平方。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习依赖于一系列经验成功但理论上异质的正则化技术（如权重衰减、dropout、指数移动平均），同时训练大模型能耗成本急剧增加，需要探索学习算法是否接近基本效率界限。

Method: 提出统一理论框架连接热力学最优性、信息几何和正则化。在三个明确假设下（A1：最优性需要内在的、参数化不变的信息度量；A2：信念状态由已知约束下的最大熵分布建模；A3：最优过程是准静态的），证明条件最优定理。

Result: 证明Fisher-Rao度量是信念空间唯一可容许的几何结构，热力学最优正则化对应于最小化到参考状态的Fisher-Rao距离平方。推导高斯和圆形信念模型的诱导几何，分别得到双曲和von Mises流形，显示经典正则化方案在结构上无法保证热力学最优性。

Conclusion: 该工作为机器学习中的正则化提供了原则性的几何和热力学基础，引入了学习的热力学效率概念，并提出了可实验验证的预测。

Abstract: Modern machine learning relies on a collection of empirically successful but theoretically heterogeneous regularization techniques, such as weight decay, dropout, and exponential moving averages. At the same time, the rapidly increasing energetic cost of training large models raises the question of whether learning algorithms approach any fundamental efficiency bound. In this work, we propose a unifying theoretical framework connecting thermodynamic optimality, information geometry, and regularization.
  Under three explicit assumptions -- (A1) that optimality requires an intrinsic, parametrization-invariant measure of information, (A2) that belief states are modeled by maximum-entropy distributions under known constraints, and (A3) that optimal processes are quasi-static -- we prove a conditional optimality theorem. Specifically, the Fisher--Rao metric is the unique admissible geometry on belief space, and thermodynamically optimal regularization corresponds to minimizing squared Fisher--Rao distance to a reference state.
  We derive the induced geometries for Gaussian and circular belief models, yielding hyperbolic and von Mises manifolds, respectively, and show that classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality. We introduce a notion of thermodynamic efficiency of learning and propose experimentally testable predictions. This work provides a principled geometric and thermodynamic foundation for regularization in machine learning.

</details>


### [72] [Power-based Partial Attention: Bridging Linear-Complexity and Full Attention](https://arxiv.org/abs/2601.17334)
*Yufeng Huang*

Main category: cs.LG

TL;DR: 论文提出了一种幂基部分注意力机制(PPA)，其复杂度为O(L^{1+p})，其中0≤p≤1，通过调节p值可以探索从线性复杂度到二次复杂度之间的注意力机制性能变化。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer研究普遍认为"注意力就是一切"，但从未系统量化过到底需要多少注意力。需要探究二次复杂度O(L^2)的注意力是否必要，是否存在次二次复杂度的注意力机制能达到可比性能。

Method: 引入幂基部分注意力机制(PPA)，复杂度为O(L^{1+p})，其中p=0对应线性复杂度的滑动窗口注意力，p=1对应完全注意力。通过调节p值来探索Transformer架构性能随注意力缩放行为的变化。

Result: 实验显示性能呈现S曲线行为：在p值的狭窄窗口内，性能从滑动窗口注意力过渡到完全注意力，当p接近1时性能趋于平稳。存在0<p<1使得O(L^{1+p})注意力足以达到与O(L^2)完全注意力相似的结果。

Conclusion: 二次复杂度注意力并非必要，存在次二次复杂度的注意力机制(PPA)能够达到与完全注意力相当的性能，这为设计更高效的Transformer架构提供了理论依据。

Abstract: It is widely accepted from transformer research that "attention is all we need", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \leq p \leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.

</details>


### [73] [Spectral Geometry for Deep Learning: Compression and Hallucination Detection via Random Matrix Theory](https://arxiv.org/abs/2601.17357)
*Davide Ettori*

Main category: cs.LG

TL;DR: 该论文提出基于谱几何和随机矩阵理论的统一框架，通过分析隐藏激活的特征值结构来解决大模型的可靠性问题和计算成本问题。包含两个贡献：EigenTrack用于实时检测幻觉和分布外行为，RMT-KD用于模型压缩。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和深度神经网络虽然性能强大，但存在可靠性问题（如幻觉）和高计算成本问题。需要一种统一的方法来同时解决这两个问题。

Method: 基于谱几何和随机矩阵理论，分析隐藏激活的特征值结构。提出EigenTrack方法，利用谱特征及其时间动态实时检测幻觉和分布外行为；提出RMT-KD方法，通过识别信息性谱分量并应用迭代知识蒸馏进行模型压缩。

Result: 谱统计量为大尺度神经网络的监控不确定性和指导压缩提供了可解释且鲁棒的信号。EigenTrack能有效检测幻觉和分布外行为，RMT-KD能在保持准确性的同时产生紧凑高效的模型。

Conclusion: 谱几何和随机矩阵理论为同时解决大模型的可靠性问题和计算成本问题提供了有效的统一框架，谱统计量是监控不确定性和指导模型压缩的有力工具。

Abstract: Large language models and deep neural networks achieve strong performance but suffer from reliability issues and high computational cost. This thesis proposes a unified framework based on spectral geometry and random matrix theory to address both problems by analyzing the eigenvalue structure of hidden activations. The first contribution, EigenTrack, is a real-time method for detecting hallucinations and out-of-distribution behavior in language and vision-language models using spectral features and their temporal dynamics. The second contribution, RMT-KD, is a principled compression method that identifies informative spectral components and applies iterative knowledge distillation to produce compact and efficient models while preserving accuracy. Together, these results show that spectral statistics provide interpretable and robust signals for monitoring uncertainty and guiding compression in large-scale neural networks.

</details>


### [74] [Robust Privacy: Inference-Time Privacy through Certified Robustness](https://arxiv.org/abs/2601.17360)
*Jiankai Jin,Xiangzheng Zhang,Zhao Liu,Deyue Zhang,Quanchen Zou*

Main category: cs.LG

TL;DR: 论文提出Robust Privacy (RP)概念，通过确保模型在输入邻域内的预测不变性来保护隐私，并开发APE方法将输入级不变性转化为属性级隐私效果。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统可能通过个性化输出泄露敏感属性信息，现有隐私保护方法在推理时存在不足，需要新的推理时隐私保护概念。

Method: 提出Robust Privacy (RP)概念，基于认证鲁棒性思想，确保模型在半径R邻域内的预测不变性；开发Attribute Privacy Enhancement (APE)方法，将输入级不变性转化为属性级隐私保护。

Result: 在推荐任务中，RP扩展了与正面推荐兼容的敏感属性值范围；在模型反演攻击中，即使小噪声水平(σ=0.1)也能将攻击成功率从73%降至4%，部分情况下可降至44%且不影响模型性能。

Conclusion: Robust Privacy提供了一种有效的推理时隐私保护框架，既能保护敏感属性隐私，又能缓解模型反演攻击，在隐私保护与模型性能间取得平衡。

Abstract: Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($σ=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.

</details>


### [75] [Diversified Scaling Inference in Time Series Foundation Models](https://arxiv.org/abs/2601.17376)
*Ruijin Hua,Zichuan Liu,Kun Zhang,Yiyuan Yang*

Main category: cs.LG

TL;DR: 该研究系统探索了时间序列基础模型在推理时的计算潜力，发现标准采样方法因探索不足而无法遵循缩放定律，提出通过多样化采样扰动来扩展生成分布支持，显著提升性能而无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型主要依赖大规模预训练，但推理时的计算潜力尚未充分挖掘。研究者希望探索两个核心问题：TSFMs在标准采样推理下的行为表现如何，以及通过控制采样多样性是否能提升性能。

Method: 首先分析TSFMs在标准采样下的特性，发现其因解空间探索不足而无法遵循缩放定律。然后通过定制化的时间序列扰动实现多样化推理缩放，扩展生成分布的支持范围。理论分析多样性-保真度权衡，推导出多样化采样优于标准采样的临界样本阈值。

Result: 在多种TSFMs和数据集上的广泛实验表明，适当的多样化推理缩放能带来显著的性能提升，且无需参数更新。作为应用，提出了RobustMSE指标来量化固定预算下TSFM的性能上限。

Conclusion: 推理设计是TSFM优化的关键计算高效维度。研究阐明了这些因素的相互作用，使得在不重新训练TSFMs的情况下，通过多样化大规模推理时间序列在并行环境中实现可靠性能成为可能。

Abstract: The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.

</details>


### [76] [GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems](https://arxiv.org/abs/2601.17396)
*Vashista Nobaub*

Main category: cs.LG

TL;DR: GO-OSC是一个几何感知的振荡时间序列表示学习框架，通过强制规范化和可识别的潜在参数化，实现对早期几何退化的稳定检测，相比传统能量基方法具有更高的灵敏度。


<details>
  <summary>Details</summary>
Motivation: 振荡系统的早期退化通常表现为动力学的几何畸变（如相位抖动、频率漂移或相干性损失），这些变化在信号能量变化可检测之前就已出现。传统的能量基诊断和无约束学习表示对此结构不敏感，导致检测延迟或不稳定。

Method: 提出GO-OSC框架：1）强制规范化和可识别的潜在参数化，实现跨短时、无标签窗口的稳定比较和聚合；2）定义一系列不变线性几何探针，针对潜在空间中退化相关的方向；3）提供理论分析，证明在早期仅相位退化情况下，几何探针具有严格正灵敏度。

Result: 理论分析表明，在早期仅相位退化情况下，能量基统计量的检测能力为零阶，而几何探针具有严格正灵敏度。实验在合成基准和真实振动数据集上验证了理论，展示了更早的检测、改进的数据效率和操作条件变化的鲁棒性。

Conclusion: GO-OSC通过几何感知的表示学习和规范化的潜在参数化，解决了振荡系统早期退化检测的挑战，相比传统方法具有更高的灵敏度和鲁棒性，为实际工程应用提供了有效的早期故障检测工具。

Abstract: Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.

</details>


### [77] [Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations](https://arxiv.org/abs/2601.17407)
*Prajwal Chauhan,Salah Eddine Choutri,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: D-SENO：一种轻量级神经算子框架，结合扩张卷积和挤压-激励模块，用于高效求解多种PDE，训练速度比传统方法快约20倍，同时保持或超越其精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的模型和神经算子参数量大，导致训练成本高、部署缓慢，需要开发轻量级且高效的PDE求解替代模型。

Method: 提出D-SENO框架，结合扩张卷积块（捕捉宽感受野和长程物理依赖）与挤压-激励模块（通过通道注意力自适应重新校准特征通道），通过精心选择的扩张率聚焦关键区域。

Result: 模型在多个PDE基准测试中达到或超越标准Transformer模型和神经算子的精度，训练速度提升约20倍；消融研究表明移除SE模块会导致性能轻微下降。

Conclusion: D-SENO是一种高效、准确的轻量级神经算子框架，能够快速求解多种PDE问题，在训练速度和精度方面均优于现有方法。

Abstract: Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.

</details>


### [78] [Active Hypothesis Testing for Correlated Combinatorial Anomaly Detection](https://arxiv.org/abs/2601.17430)
*Zichuan Yang,Yiming Xing*

Main category: cs.LG

TL;DR: 提出ECC-AHT算法，用于相关噪声下的异常流识别，通过主动噪声消除和差分感知实现最优样本复杂度


<details>
  <summary>Details</summary>
Motivation: 监控和网络安全中的异常检测问题，现有组合多臂赌博机和假设检验方法假设独立观测，无法利用相关性进行高效测量设计

Method: ECC-AHT自适应算法，选择连续约束测量以最大化竞争假设间的Chernoff信息，通过差分感知实现主动噪声消除

Result: 达到最优样本复杂度保证，在合成和真实世界相关环境中显著优于最先进基线方法

Conclusion: ECC-AHT算法有效解决了相关噪声下的异常流识别问题，代码已开源

Abstract: We study the problem of identifying an anomalous subset of streams under correlated noise, motivated by monitoring and security in cyber-physical systems. This problem can be viewed as a form of combinatorial pure exploration, where each stream plays the role of an arm and measurements must be allocated sequentially under uncertainty. Existing combinatorial bandit and hypothesis testing methods typically assume independent observations and fail to exploit correlation for efficient measurement design. We propose ECC-AHT, an adaptive algorithm that selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing. ECC-AHT achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments. The code is available on https://github.com/VincentdeCristo/ECC-AHT

</details>


### [79] [Data-driven Clustering and Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2601.17441)
*Ondrej Bohdal,Taha Ceritli,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: 提出D2C方法，通过少量任务示例进行适配器聚类，将多个任务适配器合并为多任务适配器，以在存储受限设备上部署


<details>
  <summary>Details</summary>
Motivation: 移动设备存储容量有限，无法存储所有任务适配器，需要选择具有代表性的适配器来泛化多个任务，但现有文献尚未探索此问题

Method: D2C方法利用少量任务特定示例（如每个任务10个），采用迭代优化过程细化聚类分配，将每个簇内的适配器合并为多任务适配器

Result: 实验结果表明，该方法在考虑存储预算的情况下有效提升了性能

Conclusion: D2C方法通过适配器聚类和合并，为资源受限设备提供了有效的多任务适配器部署方案

Abstract: On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.

</details>


### [80] [DREAM: Dual-Standard Semantic Homogeneity with Dynamic Optimization for Graph Learning with Label Noise](https://arxiv.org/abs/2601.17449)
*Yusheng Zhao,Jiaye Xie,Qixin Zhang,Weizhi Zhang,Xiao Luo,Zhiping Xiao,Philip S. Yu,Ming Zhang*

Main category: cs.LG

TL;DR: DREAM：一种针对带标签噪声的图学习的双重标准语义同质性动态优化方法，通过关系感知的动态优化框架和双重标准锚点选择策略来提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络方法通常假设训练图标签准确，但现实场景中标签可靠性无法保证。现有处理标签噪声的方法存在两个问题：(1)难以区分可靠与不可靠节点，(2)忽略图拓扑中嵌入的关系信息。

Method: 提出DREAM方法，包含关系感知的动态优化框架，在优化过程中迭代重新评估每个标记节点的可靠性。采用双重标准选择策略，基于节点邻近性和图拓扑选择锚点集，计算目标节点与锚点之间的语义同质性作为优化指导。

Result: 在六个不同领域的图数据集上，针对三种类型的图标签噪声进行了广泛实验，与竞争基线相比，结果证明了DREAM的有效性。

Conclusion: DREAM通过关系感知的动态优化和双重标准语义同质性测量，有效解决了图学习中的标签噪声问题，提供了可靠的关系信息优化方法。

Abstract: Graph neural networks (GNNs) have been widely used in various graph machine learning scenarios. Existing literature primarily assumes well-annotated training graphs, while the reliability of labels is not guaranteed in real-world scenarios. Recently, efforts have been made to address the problem of graph learning with label noise. However, existing methods often (i) struggle to distinguish between reliable and unreliable nodes, and (ii) overlook the relational information embedded in the graph topology. To tackle this problem, this paper proposes a novel method, Dual-Standard Semantic Homogeneity with Dynamic Optimization (DREAM), for reliable, relation-informed optimization on graphs with label noise. Specifically, we design a relation-informed dynamic optimization framework that iteratively reevaluates the reliability of each labeled node in the graph during the optimization process according to the relation of the target node and other nodes. To measure this relation comprehensively, we propose a dual-standard selection strategy that selects a set of anchor nodes based on both node proximity and graph topology. Subsequently, we compute the semantic homogeneity between the target node and the anchor nodes, which serves as guidance for optimization. We also provide a rigorous theoretical analysis to justify the design of DREAM. Extensive experiments are performed on six graph datasets across various domains under three types of graph label noise against competing baselines, and the results demonstrate the effectiveness of the proposed DREAM.

</details>


### [81] [Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping](https://arxiv.org/abs/2601.17467)
*Jianxiong Zhang,Bing Guo,Yuming Jiang,Haobo Wang,Bo An,Xuefeng Du*

Main category: cs.LG

TL;DR: 提出ARS方法，通过学习答案一致性表示来检测大型推理模型中的幻觉问题，无需人工标注即可提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型经常生成看似连贯但答案错误的推理轨迹，使得幻觉检测变得困难。现有方法直接使用轨迹文本或隐藏状态进行检测效果不佳，容易过拟合到表面模式而非答案有效性。

Method: 提出答案一致性表示塑形(ARS)方法：通过潜在干预生成反事实答案，扰动轨迹边界嵌入，根据扰动后答案是否与原答案一致来标记。学习将答案一致的表示聚集、答案不一致的表示分离，从而暴露指示幻觉风险的潜在不稳定性。

Result: 实验表明ARS能持续改进检测性能，相比强基线方法取得显著提升。塑形后的嵌入可与现有基于嵌入的检测器即插即用，训练过程中无需人工标注。

Conclusion: ARS通过显式编码答案稳定性来学习检测友好的轨迹条件表示，有效解决了大型推理模型幻觉检测的挑战，提供了一种无需人工标注的实用解决方案。

Abstract: Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.

</details>


### [82] [Identifying and Correcting Label Noise for Robust GNNs via Influence Contradiction](https://arxiv.org/abs/2601.17469)
*Wei Ju,Wei Zhang,Siyu Yi,Zhengyang Mao,Yifan Wang,Jingyang Yuan,Zhiping Xiao,Ziyue Qiao,Ming Zhang*

Main category: cs.LG

TL;DR: 提出ICGNN方法，利用图结构信息处理标签噪声问题，通过影响矛盾分数检测噪声标签，结合高斯混合模型精确识别，并采用软策略校正噪声标签


<details>
  <summary>Details</summary>
Motivation: 现实场景中图数据常存在标签噪声（标注错误或不一致），这会严重影响GNN的性能和鲁棒性，需要有效方法来处理图结构数据中的噪声标签问题

Method: 1. 设计基于图扩散矩阵的影响矛盾分数（ICS）作为噪声指标；2. 使用高斯混合模型精确检测节点标签是否为噪声；3. 开发软策略结合邻居节点预测来校正噪声标签；4. 为大量未标记节点引入伪标签提供辅助监督信号

Result: 在基准数据集上的实验表明，所提出的ICGNN方法在处理图数据标签噪声问题上表现出优越性能

Conclusion: ICGNN通过利用图结构信息有效缓解了噪声标签带来的挑战，为学习鲁棒的图神经网络提供了有效解决方案

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in learning from graph-structured data with various applications such as social analysis and bioinformatics. However, the presence of label noise in real scenarios poses a significant challenge in learning robust GNNs, and their effectiveness can be severely impacted when dealing with noisy labels on graphs, often stemming from annotation errors or inconsistencies. To address this, in this paper we propose a novel approach called ICGNN that harnesses the structure information of the graph to effectively alleviate the challenges posed by noisy labels. Specifically, we first design a novel noise indicator that measures the influence contradiction score (ICS) based on the graph diffusion matrix to quantify the credibility of nodes with clean labels, such that nodes with higher ICS values are more likely to be detected as having noisy labels. Then we leverage the Gaussian mixture model to precisely detect whether the label of a node is noisy or not. Additionally, we develop a soft strategy to combine the predictions from neighboring nodes on the graph to correct the detected noisy labels. At last, pseudo-labeling for abundant unlabeled nodes is incorporated to provide auxiliary supervision signals and guide the model optimization. Experiments on benchmark datasets show the superiority of our proposed approach.

</details>


### [83] [LeanTutor: Towards a Verified AI Mathematical Proof Tutor](https://arxiv.org/abs/2601.17473)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.LG

TL;DR: 开发了一个结合LLM和定理证明器的数学证明辅导系统LeanTutor，包含三个模块：自动形式化/证明检查器、下一步生成器和自然语言反馈生成器，并在PeanoBench数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: LLM虽然支持自然语言交流但容易出错，定理证明器如Lean能保证正确性但学习曲线陡峭。结合两者优势，为数学证明教学提供既易用又可靠的辅导工具。

Method: 提出LeanTutor系统，包含三个核心模块：1) 自动形式化/证明检查器，2) 下一步生成器，3) 自然语言反馈生成器。使用从Natural Numbers Game衍生的PeanoBench数据集（371个皮亚诺算术证明）进行评估。

Result: 开发了概念验证系统LeanTutor，成功结合了LLM的自然语言交互能力和定理证明器的正确性保证，创建了包含人类编写自然语言和形式语言证明的数据集。

Conclusion: 通过结合LLM和定理证明器的互补优势，LeanTutor为数学证明教学提供了既易用又可靠的解决方案，展示了这种混合方法在数学教育中的潜力。

Abstract: This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.

</details>


### [84] [Unintended Memorization of Sensitive Information in Fine-Tuned Language Models](https://arxiv.org/abs/2601.17480)
*Marton Szep,Jorge Marin Ruiz,Georgios Kaissis,Paulina Seidl,Rüdiger von Eisenhart-Rothe,Florian Hinterwimmer,Daniel Rueckert*

Main category: cs.LG

TL;DR: 该研究系统分析了LLM微调中仅出现在输入而非训练目标中的PII泄露风险，通过控制实验量化记忆行为，并比较了四种隐私保护方法的隐私-性能权衡。


<details>
  <summary>Details</summary>
Motivation: LLM在敏感数据集上微调存在意外记忆和泄露PII的重大风险，可能违反隐私法规并危及个人安全。当前对仅出现在模型输入而非训练目标中的PII泄露这一关键且未被充分探索的漏洞缺乏系统研究。

Method: 使用合成和真实数据集设计控制提取探针来量化意外PII记忆，研究语言、PII频率、任务类型和模型规模等因素对记忆行为的影响。进一步对差分隐私、机器遗忘、正则化和偏好对齐四种隐私保护方法进行基准测试。

Result: 后训练方法通常提供更一致的隐私-效用权衡，而差分隐私在特定设置中能显著减少泄露，但可能引入训练不稳定性。研究揭示了微调LLM中记忆问题的持续挑战。

Conclusion: 微调LLM中的记忆问题仍然是一个持续挑战，需要开发强大、可扩展的隐私保护技术来平衡隐私保护和任务性能。

Abstract: Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacy-utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.

</details>


### [85] [Automatic Stability and Recovery for Neural Network Training](https://arxiv.org/abs/2601.17483)
*Barak Or*

Main category: cs.LG

TL;DR: 提出运行时稳定性监控框架，通过隔离创新信号自动检测和恢复训练不稳定，无需修改底层优化器


<details>
  <summary>Details</summary>
Motivation: 现代神经网络训练越来越脆弱，罕见但严重的失稳更新会导致不可逆的发散或性能退化。现有优化方法主要依赖优化器内的预防机制，一旦失稳发生，检测和恢复能力有限。

Method: 引入监督运行时稳定性框架，将优化视为受控随机过程。通过隔离来自次要测量（如验证探针）的创新信号，实现自动检测和恢复失稳更新，无需修改底层优化器。

Result: 提供理论运行时安全保证，形式化有界退化和恢复。实现开销最小，兼容内存受限的训练设置。

Conclusion: 该框架为神经网络训练提供了有效的运行时稳定性监控和恢复机制，解决了现有方法在失稳检测和恢复方面的局限性。

Abstract: Training modern neural networks is increasingly fragile, with rare but severe destabilizing updates often causing irreversible divergence or silent performance degradation. Existing optimization methods primarily rely on preventive mechanisms embedded within the optimizer, offering limited ability to detect and recover from instability once it occurs. We introduce a supervisory runtime stability framework that treats optimization as a controlled stochastic process. By isolating an innovation signal derived from secondary measurements, such as validation probes, the framework enables automatic detection and recovery from destabilizing updates without modifying the underlying optimizer. We provide theoretical runtime safety guarantees that formalize bounded degradation and recovery. Our implementation incurs minimal overhead and is compatible with memory-constrained training settings.

</details>


### [86] [SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving](https://arxiv.org/abs/2601.17489)
*Ashutosh Bajpai,Akshat Bhandari,Akshay Nambi,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: SpatialMath框架通过将空间感知融入符号推理链，显著提升多模态小中型语言模型在视觉密集型数学问题（特别是几何问题）上的表现，相比基线模型提升达10个百分点。


<details>
  <summary>Details</summary>
Motivation: 当前多模态小中型语言模型在视觉理解和数学推理方面存在局限，特别是在几何问题上，难以准确分解复杂视觉输入并将感知与结构化推理相结合，导致性能不佳。

Method: 提出SpatialMath框架，包含专门感知模块从视觉图表中提取空间基础表示，捕获关键几何结构和空间关系，然后将这些表示系统地融入符号推理链中，实现视觉感知的结构化推理。同时创建MATHVERSE-PLUS数据集，包含结构化视觉解释和逐步推理路径。

Result: SpatialMath显著优于强大多模态基线模型，在视觉密集型场景下比监督微调加数据增强提升达10个百分点。鲁棒性分析显示增强的空间表示直接提高了推理准确性。

Conclusion: 该研究强调了在多模态小中型语言模型中构建结构化感知到推理流程的重要性，空间感知与符号推理的集成能有效提升视觉密集型数学问题的解决能力。

Abstract: Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.

</details>


### [87] [PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems](https://arxiv.org/abs/2601.17495)
*Ruiyu Zhang,Lin Nie,Wai-Fung Lam,Qihao Wang,Xin Zhao*

Main category: cs.LG

TL;DR: PEARL是一种标签高效的嵌入对齐方法，通过原型增强表示学习改善局部邻域结构，在标签稀缺条件下显著提升最近邻检索性能。


<details>
  <summary>Details</summary>
Motivation: 现实部署中，基于嵌入的检索系统经常失败，问题不在于语言模型本身，而在于嵌入空间中的最近邻对应错误案例。由于标签稀缺、领域漂移和重新训练成本高，系统性能严重依赖嵌入几何结构，但原始嵌入往往与最近邻检索所需的局部邻域结构不匹配。

Method: PEARL（原型增强对齐表示学习）使用有限监督通过软对齐嵌入到类别原型来重塑局部邻域几何结构。该方法保持维度不变，避免激进投影或坍缩，填补了无监督后处理（增益有限）和全监督投影（需要大量标签数据）之间的空白。

Result: 在标签稀缺条件下，PEARL显著改善局部邻域质量：相比原始嵌入提升25.7%，相比强无监督后处理提升21.1%，在基于相似性的系统最脆弱的场景中表现优异。

Conclusion: PEARL提供了一种标签高效的嵌入对齐方法，能够在标签稀缺的实际部署场景中有效改善检索系统的鲁棒性，填补了无监督和全监督方法之间的空白。

Abstract: In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.

</details>


### [88] [One-Shot Federated Clustering of Non-Independent Completely Distributed Data](https://arxiv.org/abs/2601.17512)
*Yiqun Zhang,Shenghong Cai,Zihua Yang,Sen Feng,Yuzhu Ji,Haijun Zhang*

Main category: cs.LG

TL;DR: 论文提出GOLD框架解决联邦聚类中的Non-IID问题，特别是揭示了更棘手的Non-ICD现象，通过全局导向的局部分布学习提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类在无标签的边缘设备数据中面临Non-IID问题的挑战：如何融合非独立同分布客户端的模式知识、客户端间聚类分布的关系如何、以及这种关系如何与全局知识融合关联。论文发现了一个被忽视但更棘手的现象——不同客户端可能分割同一个聚类（Non-ICD），这制约了现有联邦聚类方法的性能。

Method: 提出GOLD框架：1）精细探索客户端的潜在不完整局部聚类分布；2）将分布摘要上传到服务器进行全局融合；3）在全局分布指导下进行局部聚类增强。

Result: 通过大量实验（包括显著性检验、消融研究、可扩展性评估、定性结果等）证明了GOLD的优越性。

Conclusion: GOLD框架有效解决了联邦聚类中的Non-IID（特别是Non-ICD）挑战，通过全局导向的局部分布学习显著提升了聚类性能。

Abstract: Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.

</details>


### [89] [Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment](https://arxiv.org/abs/2601.17563)
*Nathan Gavenski,Matteo Leonetti,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: 提出UfO方法，通过两阶段无监督学习从观察中模仿，无需动作监督，优于现有ILfO方法并具有更好的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有观察模仿学习方法需要动作监督、假设状态有单一最优动作、不考虑环境状态就应用教师动作，存在局限性

Method: 两阶段无监督学习：第一阶段从观察的状态转移中近似教师真实动作，第二阶段通过调整智能体轨迹使其与教师轨迹对齐来进一步优化策略

Result: 在五个广泛使用的环境中，UfO不仅优于教师和所有其他ILfO方法，而且显示出最小的标准差，表明在未见场景中具有更好的泛化能力

Conclusion: UfO成功解决了现有观察模仿学习方法的局限性，通过无监督方式实现了更好的性能和泛化能力

Abstract: State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.

</details>


### [90] [Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization](https://arxiv.org/abs/2601.17570)
*Hadi Salloum,Ali Jnadi,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.LG

TL;DR: MC强化学习样本效率低，作者提出MC+QUBO方法，将轨迹选择建模为QUBO问题，用量子启发式采样器优化选择，提高收敛速度和策略质量。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛强化学习在稀疏奖励、大状态空间和相关轨迹环境中样本复杂度高，需要更高效的轨迹选择方法。

Method: 将轨迹选择重新表述为二次无约束二进制优化问题，使用模拟量子退火和模拟分岔作为黑盒求解器，从每批轨迹中选择最大化累积奖励同时促进状态空间覆盖的子集。

Result: 在有限视野GridWorld实验中，MC+QUBO在收敛速度和最终策略质量方面优于普通蒙特卡洛方法。

Conclusion: 量子启发式优化作为强化学习中决策子程序具有潜力，MC+QUBO展示了通过组合优化改进蒙特卡洛强化学习的可行性。

Abstract: Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solving it with quantum-inspired samplers. Our method, MC+QUBO, integrates a combinatorial filtering step into standard MC policy evaluation: from each batch of trajectories, we select a subset that maximizes cumulative reward while promoting state-space coverage. This selection is encoded as a QUBO, where linear terms favor high-reward episodes and quadratic terms penalize redundancy. We explore both Simulated Quantum Annealing (SQA) and Simulated Bifurcation (SB) as black-box solvers within this framework. Experiments in a finite-horizon GridWorld demonstrate that MC+QUBO outperforms vanilla MC in convergence speed and final policy quality, highlighting the potential of quantum-inspired optimization as a decision-making subroutine in reinforcement learning.

</details>


### [91] [Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout](https://arxiv.org/abs/2601.17602)
*Xuanzhou Chen*

Main category: cs.LG

TL;DR: 研究Transformer过参数化问题，通过高维编码器-解码器嵌入的角度相似性分析，使用伯努利dropout识别保持Top-1预测的稀疏性阈值。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型的过参数化现象，探索在编码器和解码器之间应用dropout时，模型性能如何随稀疏性变化，以理解嵌入表示对模型鲁棒性的影响。

Method: 在编码器和解码器之间应用伯努利dropout，变化保留概率p来识别稀疏性阈值；理论上证明当嵌入有效稀疏性足够大时，解码器性能在适度坐标dropout下保持稳定；实验上构建带有二进制擦除通道(BEC)的Transformer模型，在英法翻译任务上测试性能。

Result: 实验结果显示验证准确率和BLEU分数在某个阈值处急剧下降，可视化展示了这一趋势，表明存在一个稀疏性阈值，超过该阈值模型性能会显著恶化。

Conclusion: Transformer模型在编码器-解码器嵌入中存在稀疏性阈值，当有效稀疏性足够大时，模型性能在适度dropout下保持稳定，但超过特定阈值后性能会急剧下降，这为理解Transformer过参数化和鲁棒性提供了新视角。

Abstract: We study Transformer overparameterization through the lens of angular similarity in high-dimensional encoder-decoder embeddings. We apply Bernoulli dropout between the encoder and the decoder, varying the keep probability $p$ to identify a sparsity-dependent threshold above which the Top-1 prediction is preserved. Theoretically, we prove that, if the effective sparsity embeddings is sufficiently large, and thus decoder performance, remain stable under moderate coordinate dropout. Empirically, we implement the Bernoulli dropout by constructing a new Transformer model augmented with Binary Erasure Channel (BEC) and test its performance on an English-French translation task. Experimental results visualize the trends for validation accuracies and BLEU scores, both decline sharply at some threshold.

</details>


### [92] [A Thermodynamic Theory of Learning I: Irreversible Ensemble Transport and Epistemic Costs](https://arxiv.org/abs/2601.17607)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 论文提出学习本质上是一个不可逆过程，需要熵产生才能实现认知结构，并推导出认知速度极限(ESL)，为任何学习过程实现给定分布变换所需的最小熵产生提供了下界。


<details>
  <summary>Details</summary>
Motivation: 经典信息论表明确定性变换不会增加信息，这与学习系统能从数据中获得结构化内部表示的现象相矛盾。本文旨在解决这一基本问题：学习如何在不违反信息论限制的情况下产生抽象和洞察？

Method: 将学习建模为模型配置概率分布空间中的传输过程，引入认知自由能框架。在该框架中定义自由能下降作为记录学习轨迹上认知自由能总减少的记账量，并将其分解为可逆部分（潜在改进）和不可逆部分（熵产生）。

Result: 推导出认知速度极限(ESL)，这是一个有限时间不等式，为任何学习过程实现给定分布变换所需的最小熵产生提供了下界。该下界仅取决于初始和最终集合分布之间的Wasserstein距离，与具体学习算法无关。

Conclusion: 学习本质上是一个不可逆过程，认知结构的实现必然伴随熵产生。认知速度极限为学习过程的基本限制提供了理论框架，将热力学概念与机器学习理论联系起来。

Abstract: Learning systems acquire structured internal representations from data, yet classical information-theoretic results state that deterministic transformations do not increase information. This raises a fundamental question: how can learning produce abstraction and insight without violating information-theoretic limits?
  We argue that learning is inherently an irreversible process when performed over finite time, and that the realization of epistemic structure necessarily incurs entropy production. To formalize this perspective, we model learning as a transport process in the space of probability distributions over model configurations and introduce an epistemic free-energy framework.
  Within this framework, we define the free-energy drop as a bookkeeping quantity that records the total reduction of epistemic free energy along a learning trajectory. This reduction decomposes into a reversible component associated with potential improvement and an irreversible component corresponding to entropy production.
  We then derive the Epistemic Speed Limit (ESL), a finite-time inequality that lower-bounds the minimal entropy production required by any learning process to realize a given distributional transformation. This bound depends only on the Wasserstein distance between initial and final ensemble distributions and is independent of the specific learning algorithm.

</details>


### [93] [Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning](https://arxiv.org/abs/2601.17616)
*Fatema Siddika,Md Anwar Hossen,Tanwi Mallick,Ali Jannesari*

Main category: cs.LG

TL;DR: SETA框架通过混合稀疏专家解决LLM持续学习中的可塑性-稳定性困境，将知识分解为任务特定专家和共享专家，使用弹性权重锚定保护关键知识，实现任务无关的持续学习。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型持续学习面临可塑性-稳定性困境：学习新能力会导致灾难性遗忘先前知识。现有方法通常统一处理参数，无法区分任务特定知识和共享能力。

Method: 提出SETA框架，将模型分解为模块化子空间：任务特定专家隔离任务特定模式，共享专家捕获共同特征。通过弹性权重锚定保护关键共享知识，使用统一门控网络在推理时自动检索正确的专家组合。

Result: 在多样化的领域特定和通用基准测试中，SETA始终优于最先进的参数高效微调持续学习方法。

Conclusion: SETA通过混合稀疏专家框架有效解决了LLM持续学习中的可塑性-稳定性冲突，实现了任务无关的持续学习，在多个基准上表现出色。

Abstract: Continual learning in Large Language Models (LLMs) is hindered by the plasticity-stability dilemma, where acquiring new capabilities often leads to catastrophic forgetting of previous knowledge. Existing methods typically treat parameters uniformly, failing to distinguish between specific task knowledge and shared capabilities. We introduce Mixture of Sparse Experts for Task-Agnostic Continual Learning, referred to as SETA, a framework that resolves the plasticity-stability conflict by decomposing the model into modular subspaces. Unlike standard updates, where tasks compete for the same parameters, SETA separates knowledge into unique experts, designed to isolate task-specific patterns, and shared experts, responsible for capturing common features. This structure is maintained through elastic weight anchoring, which protects critical shared knowledge and enables a unified gating network to automatically retrieve the correct expert combination for each task during inference. Extensive experiments across diverse domain-specific and general benchmarks demonstrate that SETA consistently outperforms state-of-the-art parameter-efficient fine-tuning-based continual learning methods.

</details>


### [94] [BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation](https://arxiv.org/abs/2601.17625)
*Yuhan Xie,Jinhan Liu,Xiaoyong Ni,Fei Tan,Icare Sakr,Thibault Collin,Shiqi Sun,Alejandro Rodriguez Guajardo,Demon Fanny,Charles-francois Vincent Latchoumane,Henri Lorach,Jocelyne Bloch,Gregoire Courtine,Mahsa Shoaran*

Main category: cs.LG

TL;DR: BrainDistill：一种用于植入式BCI的新型运动解码管道，通过任务特定知识蒸馏和量化感知训练，在保持高性能的同时大幅降低计算需求


<details>
  <summary>Details</summary>
Motivation: Transformer解码器在BCI任务中表现出色，但参数多、计算需求高，难以部署在功率受限的植入式系统中

Method: 提出BrainDistill管道，包含植入式神经解码器（IND）和任务特定知识蒸馏（TSKD）框架，TSKD通过监督投影优先保留解码关键特征，并采用量化感知训练实现整数推理

Result: IND在多个神经数据集上优于先前解码器，TSKD蒸馏版本在少样本校准设置中超越其他蒸馏方法，量化IND在严格功率约束下部署时性能损失最小

Conclusion: BrainDistill成功解决了植入式BCI系统功率约束与高性能解码需求之间的矛盾，为实际临床应用提供了可行方案

Abstract: Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.

</details>


### [95] [A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization](https://arxiv.org/abs/2601.17646)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: 论文研究了经验风险最小化（ERM）解的稳定性问题，特别关注凸非严格损失函数产生的集值最小化器，提出了Painlevé-Kuratowski上半连续性作为ERM解对应的内在稳定性概念。


<details>
  <summary>Details</summary>
Motivation: 传统ERM稳定性研究通常针对单值输出，但凸非严格损失函数会产生集值最小化器，需要建立更合适的稳定性理论框架来理解解集的稳定性。

Method: 提出使用Painlevé-Kuratowski上半连续性（PK-u.s.c.）作为ERM解对应的内在稳定性概念，并证明在Mosco一致扰动和局部有界最小化器的条件下，可以获得PK-u.s.c.、最小值连续性和消失间隙近最小化器的一致性。

Result: 建立了ERM解集稳定性的完整理论框架：在最小非退化定性机制下，Mosco一致扰动和局部有界最小化器能保证PK-u.s.c.、最小值连续性和近最小化器的一致性；二次增长条件下还能得到显式的定量偏差界。

Conclusion: PK-u.s.c.是ERM解对应的合适稳定性概念，为理解集值最小化器的稳定性提供了理论基础，并建立了从定性到定量的稳定性分析框架。

Abstract: Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlevé-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.

</details>


### [96] [Time-Varying Causal Treatment for Quantifying the Causal Effect of Short-Term Variations on Arctic Sea Ice Dynamics](https://arxiv.org/abs/2601.17647)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出KGCM-VAE模型，通过知识引导的因果建模量化海冰厚度与海面高度之间的因果关系，结合物理约束和分布平衡机制提升因果效应估计精度。


<details>
  <summary>Details</summary>
Motivation: 量化冰融化和淡水分布之间的因果关系对理解极地气候变化和全球海平面上升至关重要，但传统深度学习模型在时空设置中处理未观测混杂因素和缺乏物理约束时存在困难。

Method: 提出知识引导的因果模型变分自编码器(KGCM-VAE)，包含速度调制方案（通过S型函数动态放大平滑速度信号）、最大均值差异(MMD)平衡处理组和对照组协变量分布，以及因果邻接约束解码器确保与物理结构对齐。

Result: 在合成和真实北极数据集上，KGCM-VAE在PEHE指标上优于最先进基准模型。消融研究显示MMD和因果邻接约束联合应用使估计误差降低1.88%。

Conclusion: KGCM-VAE通过整合物理知识和因果约束，有效解决了时空因果推断中的挑战，为量化海冰厚度与海面高度之间的因果关系提供了可靠框架。

Abstract: Quantifying the causal relationship between ice melt and freshwater distribution is critical, as these complex interactions manifest as regional fluctuations in sea surface height (SSH). Leveraging SSH as a proxy for sea ice dynamics enables improved understanding of the feedback mechanisms driving polar climate change and global sea-level rise. However, conventional deep learning models often struggle with reliable treatment effect estimation in spatiotemporal settings due to unobserved confounders and the absence of physical constraints. To address these challenges, we propose the Knowledge-Guided Causal Model Variational Autoencoder (KGCM-VAE) to quantify causal mechanisms between sea ice thickness and SSH. The proposed framework integrates a velocity modulation scheme in which smoothed velocity signals are dynamically amplified via a sigmoid function governed by SSH transitions to generate physically grounded causal treatments. In addition, the model incorporates Maximum Mean Discrepancy (MMD) to balance treated and control covariate distributions in the latent space, along with a causal adjacency-constrained decoder to ensure alignment with established physical structures. Experimental results on both synthetic and real-world Arctic datasets demonstrate that KGCM-VAE achieves superior PEHE compared to state-of-the-art benchmarks. Ablation studies further confirm the effectiveness of the approach, showing that the joint application of MMD and causal adjacency constraints yields a 1.88\% reduction in estimation error.

</details>


### [97] [Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training](https://arxiv.org/abs/2601.17654)
*Ruofan Wu,Jae-Won Chung,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Kareus是一个AI训练系统，通过联合优化细粒度内核调度和频率调节来同时降低动态和静态能耗，在保持相同训练时间下可减少最多28.3%的能耗，或在相同能耗下减少最多27.5%的训练时间。


<details>
  <summary>Details</summary>
Motivation: AI计算需求快速增长，但能源供应跟不上，能源已成为昂贵且竞争激烈的资源。现有工作只关注动态或静态能耗的单一方面，而细粒度内核调度和频率调节会共同影响两种能耗，需要联合优化。

Method: 设计Kareus训练系统，将难以处理的联合优化问题分解为局部的、基于分区的子问题，采用多通道多目标优化算法寻找能够推进时间-能耗权衡前沿的执行调度方案。

Result: 与现有技术相比，Kareus在相同训练时间下可减少最多28.3%的训练能耗，或在相同能耗下减少最多27.5%的训练时间。

Conclusion: 通过联合优化内核调度和频率调节来同时处理动态和静态能耗，Kareus成功推进了时间-能耗权衡前沿，为AI训练提供了更高效的能源管理方案。

Abstract: The computing demand of AI is growing at an unprecedented rate, but energy supply is not keeping pace. As a result, energy has become an expensive, contended resource that requires explicit management and optimization. Although recent works have made significant progress in large model training optimization, they focus only on a single aspect of energy consumption: dynamic or static energy.
  We find that fine-grained kernel scheduling and frequency scaling jointly and interdependently impact both dynamic and static energy consumption. Based on this finding, we design Kareus, a training system that pushes the time--energy tradeoff frontier by optimizing both aspects. Kareus decomposes the intractable joint optimization problem into local, partition-based subproblems. It then uses a multi-pass multi-objective optimization algorithm to find execution schedules that push the time--energy tradeoff frontier. Compared to the state of the art, Kareus reduces training energy by up to 28.3% at the same training time, or reduces training time by up to 27.5% at the same energy consumption.

</details>


### [98] [Entropic Risk-Aware Monte Carlo Tree Search](https://arxiv.org/abs/2601.17667)
*Pedro P. Santos,Jacopo Silvestrin,Alberto Sardinha,Francisco S. Melo*

Main category: cs.LG

TL;DR: 提出一种可证明正确的蒙特卡洛树搜索算法，用于求解具有熵风险度量的风险感知马尔可夫决策过程，并提供非渐近分析证明其正确性和多项式遗憾集中性。


<details>
  <summary>Details</summary>
Motivation: 现有的风险感知MDP求解方法缺乏可证明正确的树搜索算法，特别是在具有熵风险度量（ERM）目标的情况下。需要开发一种既能利用动态规划公式，又能提供理论保证的MCTS算法。

Method: 提出一种基于上置信界（UCB）的蒙特卡洛树搜索算法，该算法利用了先前工作中为ERM目标风险感知MDP引入的动态规划公式。算法在树搜索框架中整合了风险感知的决策制定。

Result: 算法被证明是"正确"的（根节点的经验ERM收敛到最优ERM），并享有"多项式遗憾集中性"。实验表明该风险感知MCTS方法优于相关基线。

Conclusion: 成功开发了一种可证明正确的风险感知MCTS算法，用于求解具有ERM目标的风险感知MDP，填补了理论保证与实用树搜索算法之间的空白。

Abstract: We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving \textit{risk-aware} Markov decision processes (MDPs) with \textit{entropic risk measure} (ERM) objectives. We provide a \textit{non-asymptotic} analysis of our proposed algorithm, showing that the algorithm: (i) is \textit{correct} in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys \textit{polynomial regret concentration}. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.

</details>


### [99] [Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction](https://arxiv.org/abs/2601.17668)
*Jang-Hyun Kim,Dongyoon Han,Sangdoo Yun*

Main category: cs.LG

TL;DR: 提出基于门控的KV缓存淘汰方法，通过轻量级sink-attention门控模块识别关键KV对，实现70%缓存淘汰的同时保持接近无损性能，计算成本可忽略。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩技术通常在性能下降和计算开销之间需要权衡，需要一种既能实现高压缩比又具有可忽略计算成本的方法。

Method: 引入轻量级sink-attention门控模块识别和保留关键KV对，提出仅依赖LLM前向传播的门训练算法，避免昂贵的反向传播，采用任务无关的重建目标实现强任务泛化。

Result: 在Qwen2.5-1M、Qwen3和Gemma3系列模型上实验表明，该方法在淘汰高达70%KV缓存的同时保持接近无损性能，在长上下文理解、代码理解和数学推理等任务上表现一致。

Conclusion: 提出的门控KV缓存淘汰方法实现了高压缩比和可忽略计算成本的平衡，在各种任务上表现出良好的通用性，为LLM实际部署提供了有效的KV缓存管理方案。

Abstract: Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.

</details>


### [100] [$\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts](https://arxiv.org/abs/2601.17680)
*Shota Takashiro,Takeshi Kojima,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: ∞-MoE通过连续空间选择大型FFN的部分参数，实现无限专家数量，在保持计算效率的同时提升性能


<details>
  <summary>Details</summary>
Motivation: 传统MoE将专家视为完全独立且在离散空间组合，当专家数量增加时难以有效训练每个专家。需要一种方法在增加专家数量的同时稳定训练。

Method: 提出∞-MoE，基于每个token采样的连续值选择大型前馈网络的部分参数。通过将专家视为连续空间，可以在保持计算效率的同时实现无限数量的专家。

Result: 基于GPT-2 Small的∞-MoE模型（1.29亿活跃参数，1.86亿总参数）性能与3.5亿参数的密集GPT-2 Medium相当。推理时调整采样专家数量可在准确性和速度之间灵活权衡，比传统MoE准确率提升高达2.5%。

Conclusion: ∞-MoE通过连续空间表示专家，解决了传统MoE在专家数量增加时的训练难题，实现了更好的性能与计算效率平衡。

Abstract: The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.

</details>


### [101] [Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis](https://arxiv.org/abs/2601.17687)
*Hao Li,He Cao,Shenyao Peng,Zijing Liu,Bin Feng,Yu Wang,Zhiyuan Yan,Yonghong Tian,Yu Li,Li Yuan*

Main category: cs.LG

TL;DR: ChemCRAFT：基于智能体强化学习的化学AI框架，通过将化学推理与知识存储解耦，让小模型也能实现高性能化学任务，同时保护隐私并降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前生化领域语言模型面临两难：小模型易产生幻觉且知识有限，大云端模型存在隐私风险和高推理成本。需要一种既能保持高性能又能本地部署的解决方案。

Method: 提出ChemCRAFT框架，采用智能体强化学习将化学推理与知识存储解耦。构建化学智能体沙箱和轨迹构建管道，创建ChemToolDataset数据集，并提出SMILES-GRPO方法建立密集化学奖励函数。

Result: ChemCRAFT在药物设计的多个方面（分子结构分析、分子优化、合成路径预测）均优于当前云端大模型，证明科学推理能力可通过工具编排策略学习获得。

Conclusion: 该工作建立了成本效益高且保护隐私的AI辅助化学范式，为通过本地可部署智能体加速分子发现开辟了新途径。

Abstract: Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model's ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.

</details>


### [102] [REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization](https://arxiv.org/abs/2601.17689)
*Shanu Saklani,Tushar M. Athawale,Nairita Pal,David Pugmire,Christopher R. Johnson,Soumya Dutta*

Main category: cs.LG

TL;DR: REV-INR是一种正则化证据隐式神经表示方法，能够通过单次前向传播同时预测数据值和不确定性，解决传统INR缺乏不确定性估计的问题。


<details>
  <summary>Details</summary>
Motivation: 传统确定性隐式神经表示(INR)只能预测数值，无法提供模型预测不确定性或数据固有噪声的影响，这可能导致不可靠的数据解释和可视化。由于原始数据可能因体积过大而无法获取，从模型预测数据中识别错误结果可能不可行。

Method: 提出REV-INR（正则化证据隐式神经表示），该方法在推理时仅需单次前向传播即可学习准确预测数据值以及相关的坐标级数据不确定性和模型不确定性。

Result: 与现有深度不确定性估计方法相比，REV-INR实现了最佳的体积重建质量，具有鲁棒的数据（偶然性）和模型（认知性）不确定性估计，且推理时间最快。

Conclusion: REV-INR能够评估提取的等值面和体积可视化结果的可靠性和可信度，使得分析可以完全基于模型预测数据驱动。

Abstract: Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.

</details>


### [103] [FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices](https://arxiv.org/abs/2601.17713)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Yinfeng Cao*

Main category: cs.LG

TL;DR: FedCCA是一种面向物联网的联邦学习算法，通过客户端特定编码器和动态选择机制解决数据异构问题，为每个客户端学习独特模型


<details>
  <summary>Details</summary>
Motivation: 物联网中AI模型训练需要保护隐私数据（如人体感知数据），联邦学习虽然能保护隐私，但设备间数据异构问题严重影响了模型性能和收敛速度。现有方法在固定客户端选择和云端聚合方面存在局限，难以在本地训练中提取客户端特定信息。

Method: 提出FedCCA算法，通过选择性适应利用客户端特定知识为每个客户端学习独特模型。具体包括：1）使用动态客户端选择机制；2）基于额外客户端特定编码器的自适应聚合；3）采用基于注意力的全局聚合策略增强多源知识转移。

Result: 在多个数据集上进行广泛实验，结果显示FedCCA在解决数据异构问题上相比基线方法具有显著性能优势。

Conclusion: FedCCA通过客户端中心化适应策略有效缓解了联邦学习中的数据异构问题，为物联网场景下的隐私保护AI训练提供了有效解决方案。

Abstract: With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [104] [Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games](https://arxiv.org/abs/2601.17716)
*Daniel M. Pedrozo,Telma W. de L. Soares,Bryan L. M. de Oliveira*

Main category: cs.LG

TL;DR: 论文提出了一个评估LLM通过是/否问题收集信息能力的多轮对话框架，使用信息增益作为核心指标，在地理猜城市游戏中验证了具有显式推理能力的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在解决用户请求模糊性时提问能力不足，现有基准缺乏基于信息增益的全面评估框架，且很少系统比较使用链式思维推理和不使用该推理的模型表现。

Method: 提出了一个多轮对话框架，使用三个交互的LLM代理（提问者、回答者、假设空间更新者），在分层知识图环境中通过是/否问题收集信息，采用基于香农熵的信息增益作为核心评估指标，在地理猜城市游戏中进行实例化。

Result: 实验表明，在评估的模型中，具有显式推理能力的模型每轮获得更高的信息增益，且用更少的步骤达到解决方案，特别是在部分可观测环境中。小模型通过更积极地探索候选问题来弥补能力限制，而大模型在选择最优查询时表现出更高的自信度。

Conclusion: 该框架为评估LLM的信息收集能力提供了量化方法，证明了显式推理能力对高效提问的重要性，为LLM代理的模糊解决能力评估提供了新基准。

Abstract: Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.

</details>


### [105] [AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation](https://arxiv.org/abs/2601.17761)
*Dongjie Cheng,Ruifeng Yuan,Yongqi Li,Runyang You,Wenjie Wang,Liqiang Nie,Lei Zhang,Wenjie Li*

Main category: cs.LG

TL;DR: AR-Omni是一个统一的任意到任意多模态模型，采用自回归范式，无需专家解码器，支持文本、图像和流式语音生成。


<details>
  <summary>Details</summary>
Motivation: 现实世界的感知和交互本质上是多模态的，涵盖语言、视觉和语音。现有大多数系统依赖额外专家组件实现多模态生成，限制了统一训练和推理的简洁性。自回归建模在文本领域已被证明是优雅且可扩展的基础。

Method: 提出AR-Omni，在自回归范式下构建统一模型：1) 使用单一Transformer解码器支持文本、图像和流式语音生成；2) 通过任务感知损失重加权解决模态不平衡；3) 通过轻量级token级感知对齐损失提升视觉保真度；4) 通过有限状态解码机制平衡稳定性和创造性。

Result: AR-Omni在三个模态上都实现了强大的生成质量，同时保持实时性，语音生成的实时因子达到0.88。

Conclusion: AR-Omni展示了自回归范式作为统一多模态建模基础的潜力，通过单一解码器实现了任意到任意的多模态生成，解决了实际部署中的关键挑战。

Abstract: Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of "Omni" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.

</details>


### [106] [LLM-42: Enabling Determinism in LLM Inference with Verified Speculation](https://arxiv.org/abs/2601.17768)
*Raja Gond,Aditya K Kamath,Arkaprava Basu,Ramachandran Ramjee,Ashish Panwar*

Main category: cs.LG

TL;DR: LLM-42：基于调度的确定性LLM推理方法，通过验证-回滚机制在保持高吞吐量的同时确保输出确定性


<details>
  <summary>Details</summary>
Motivation: LLM推理中的非确定性源于浮点非结合性与动态批处理及GPU内核的交互。现有方法要么牺牲吞吐量（禁用动态批处理），要么需要重新实现内核且产生固定开销。需要一种既能保持高吞吐量又能确保确定性的解决方案。

Method: 受推测解码启发，提出LLM-42调度方法：1）使用非确定性快速路径解码token；2）通过轻量级验证-回滚循环确保确定性；3）验证器在固定形状归约调度下重放候选token，提交保证一致的结果，回滚违反确定性的结果。

Result: LLM-42能够：1）大部分重用现有内核；2）仅对需要确定性的流量产生开销；3）在保持高吞吐量的同时确保输出确定性；4）避免与内核设计的紧耦合。

Conclusion: LLM-42提供了一种实用的调度方法，在LLM推理中实现确定性而无需牺牲吞吐量或重新实现内核，通过验证-回滚机制智能地仅在必要时施加确定性约束。

Abstract: In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.
  Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.

</details>


### [107] [Shortcut Learning in Binary Classifier Black Boxes: Applications to Voice Anti-Spoofing and Biometrics](https://arxiv.org/abs/2601.17782)
*Md Sahidullah,Hye-jin Shim,Rosa Gonzalez Hautamäki,Tomi H. Kinnunen*

Main category: cs.LG

TL;DR: 提出一个分析黑盒分类器中数据集偏见的框架，结合干预和观察视角，使用线性混合效应模型进行后验分析，在音频反欺骗和说话人验证任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在数据驱动应用中的广泛采用引起了人们对数据集和模型偏见潜在风险的关注。被忽视或隐藏的偏见可能导致意外结果，需要解决数据集偏见问题并探索二分类器中的"捷径学习"或"Clever Hans效应"。

Method: 提出一个新颖的框架来分析黑盒分类器，并检查训练和测试数据对分类器分数的影响。该框架结合干预和观察视角，采用线性混合效应模型进行后验分析，超越错误率评估分类器性能。

Result: 通过在音频反欺骗和说话人验证任务中使用统计模型和深度神经网络进行实验，证明了该方法的有效性。研究获得的见解对于解决其他领域的偏见和推进可解释人工智能领域具有更广泛的意义。

Conclusion: 该研究提供了一个全面的框架来分析数据集偏见对分类器行为的影响，为理解偏见的来源和影响提供了新视角，对可解释人工智能的发展有重要贡献。

Abstract: The widespread adoption of deep-learning models in data-driven applications has drawn attention to the potential risks associated with biased datasets and models. Neglected or hidden biases within datasets and models can lead to unexpected results. This study addresses the challenges of dataset bias and explores ``shortcut learning'' or ``Clever Hans effect'' in binary classifiers. We propose a novel framework for analyzing the black-box classifiers and for examining the impact of both training and test data on classifier scores. Our framework incorporates intervention and observational perspectives, employing a linear mixed-effects model for post-hoc analysis. By evaluating classifier performance beyond error rates, we aim to provide insights into biased datasets and offer a comprehensive understanding of their influence on classifier behavior. The effectiveness of our approach is demonstrated through experiments on audio anti-spoofing and speaker verification tasks using both statistical models and deep neural networks. The insights gained from this study have broader implications for tackling biases in other domains and advancing the field of explainable artificial intelligence.

</details>


### [108] [Robust Computational Extraction of Non-Enhancing Hypercellular Tumor Regions from Clinical Imaging Data](https://arxiv.org/abs/2601.17802)
*A. Brawanski,Th. Schaffer,F. Raab,K. -M. Schebesch,M. Schrey,Chr. Doenitz,A. M. Tomé,E. W. Lang*

Main category: cs.LG

TL;DR: 提出一个计算框架，从常规MRI数据生成非增强高细胞性肿瘤区域的概率图，通过多种网络架构解决成像边界不清晰的问题，并验证了其方法学稳健性和生物学相关性。


<details>
  <summary>Details</summary>
Motivation: 准确识别非增强高细胞性肿瘤区域是神经肿瘤成像中尚未满足的需求，对患者管理和治疗规划有重要意义。当前缺乏可靠的成像方法来识别这些区域。

Method: 开发了一个稳健的计算框架，利用多种网络架构从常规MRI数据生成NEH区域的概率图，解决了成像变异性和边界不清晰的问题。

Result: 该框架通过独立临床标志物（相对脑血容量和增强肿瘤复发位置）进行了验证，证明了方法学的稳健性和生物学相关性。

Conclusion: 该框架能够可靠、无创地映射NEH肿瘤区域，支持其作为成像生物标志物整合到临床工作流程中，推进脑肿瘤患者的精准肿瘤学。

Abstract: Accurate identification of non-enhancing hypercellular (NEH) tumor regions is an unmet need in neuro-oncological imaging, with significant implications for patient management and treatment planning. We present a robust computational framework that generates probability maps of NEH regions from routine MRI data, leveraging multiple network architectures to address the inherent variability and lack of clear imaging boundaries. Our approach was validated against independent clinical markers -- relative cerebral blood volume (rCBV) and enhancing tumor recurrence location (ETRL) -- demonstrating both methodological robustness and biological relevance. This framework enables reliable, non-invasive mapping of NEH tumor compartments, supporting their integration as imaging biomarkers in clinical workflows and advancing precision oncology for brain tumor patients.

</details>


### [109] [MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging](https://arxiv.org/abs/2601.17858)
*Jiapeng Wang,Changxin Tian,Kunlong Chen,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: MergeMix利用模型融合权重作为低成本性能代理，高效优化LLM数据混合比例，避免全量训练成本


<details>
  <summary>Details</summary>
Motivation: 当前优化大型语言模型数据混合比例的方法依赖启发式试验或昂贵的代理训练，计算成本过高，需要更高效的自动化解决方案

Method: 通过训练领域专家模型（使用少量tokens），然后优化这些专家模型的融合权重来优化下游任务性能，将模型融合权重作为低成本高保真的性能代理

Result: 在8B和16B参数模型上的实验表明，MergeMix性能达到或超过穷举手动调优，同时大幅降低搜索成本，具有高秩一致性（Spearman ρ>0.9）和强跨尺度可迁移性

Conclusion: MergeMix为数据混合优化提供了一个可扩展的自动化解决方案，能够高效确定最优数据混合比例，显著降低计算成本

Abstract: Optimizing data mixtures is essential for unlocking the full potential of large language models (LLMs), yet identifying the optimal composition remains computationally prohibitive due to reliance on heuristic trials or expensive proxy training. To address this, we introduce \textbf{MergeMix}, a novel approach that efficiently determines optimal data mixing ratios by repurposing model merging weights as a high-fidelity, low-cost performance proxy. By training domain-specific experts on minimal tokens and optimizing their merging weights against downstream benchmarks, MergeMix effectively optimizes the performance of data mixtures without incurring the cost of full-scale training. Extensive experiments on models with 8B and 16B parameters validate that MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs. Furthermore, MergeMix exhibits high rank consistency (Spearman $ρ> 0.9$) and strong cross-scale transferability, offering a scalable, automated solution for data mixture optimization.

</details>


### [110] [EEG Foundation Models: Progresses, Benchmarking, and Open Problems](https://arxiv.org/abs/2601.17883)
*Dingkun Liu,Yuheng Chen,Zhu Chen,Zhenyao Cui,Yaozhi Wen,Jiayu An,Jingwei Luo,Dongrui Wu*

Main category: cs.LG

TL;DR: 该论文对现有EEG基础模型进行了首次全面评估，比较了12个开源模型在13个数据集上的表现，发现线性探测通常不足、专用模型仍具竞争力、模型规模扩大不一定带来更好性能。


<details>
  <summary>Details</summary>
Motivation: EEG基础模型在脑机接口领域发展迅速，但缺乏公平全面的比较，因为存在预训练目标不一致、预处理选择不同、下游评估协议不统一等问题。本文旨在填补这一空白。

Method: 首先回顾50个代表性模型并构建统一分类框架，然后评估12个开源基础模型和竞争性专用基线模型，在13个EEG数据集上涵盖9种BCI范式，考虑跨被试泛化和少样本快速校准两种场景。

Result: 研究发现：1）线性探测通常不足以获得最佳性能；2）从头训练的专用模型在许多任务上仍具竞争力；3）在当前数据规模和训练实践下，更大的基础模型不一定带来更好的泛化性能。

Conclusion: 该研究为EEG基础模型领域提供了首个系统性评估基准，揭示了当前方法的局限性，为未来模型设计提供了重要指导，强调需要更有效的迁移学习策略和模型架构。

Abstract: Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.

</details>


### [111] [Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization](https://arxiv.org/abs/2601.17910)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 提出一个与具体算子无关的公理化框架，用于多教师知识蒸馏中的自适应权重分配，涵盖token、task和context三个互补尺度。


<details>
  <summary>Details</summary>
Motivation: 当前多教师知识蒸馏方法主要依赖启发式或实现特定的权重方案，缺乏理论保证。需要建立一个统一的理论框架来分析自适应权重分配方法。

Method: 开发一个算子无关的公理化框架，形式化自适应权重算子的结构条件，包括良好定义性、多重非等价实现、通过乘积结构归一化进行层次组合。分析算子的存在性、非唯一性、梯度优化收敛性、稳定性和扰动鲁棒性。

Result: 建立了自适应权重算子的理论框架，证明了符合条件算子的存在性和非唯一性，在标准假设下分析了梯度优化的收敛性，提供了安全约束蒸馏的抽象表述。

Conclusion: 该框架将理论保证与具体权重公式解耦，使得能够在异质性、分布偏移和安全约束下对自适应蒸馏方法进行原则性分析。

Abstract: Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.

</details>


### [112] [Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN](https://arxiv.org/abs/2601.17912)
*Qinyi Liu,Mohammad Khalil,Naman Goel*

Main category: cs.LG

TL;DR: TabPFN等表格数据基础模型通过因果预训练获得高预测精度，但其公平性表现中等且不稳定，尤其在MNAR协变量偏移下，因果预训练对公平性提升有限。


<details>
  <summary>Details</summary>
Motivation: 虽然TabPFN等基于因果推理预训练的表格基础模型在预测准确性方面表现优异，但其公平性特性尚未得到充分探索，需要评估这些模型在实际部署中的公平性表现。

Method: 对TabPFN及其微调变体进行全面的实证评估，包括预测性能、公平性和鲁棒性测试，考察不同数据集规模和分布偏移（特别是MNAR协变量偏移）下的表现。

Result: TabPFN相比基线模型具有更强的预测准确性和对虚假相关性的鲁棒性，但在公平性方面的改进中等且不一致，特别是在MNAR协变量偏移情况下表现不佳。

Conclusion: TabPFN的因果预训练对公平性有帮助但不足，实际部署需要额外的公平性干预措施，因果预训练本身不能保证算法公平性。

Abstract: Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, have not yet been explored in sufficient depth. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying such models in practice and the need for further fairness interventions.

</details>


### [113] [UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR](https://arxiv.org/abs/2601.17916)
*Jialu Tang,Tong Xia,Yuan Lu,Aaqib Saeed*

Main category: cs.LG

TL;DR: UniPACT是一个统一的临床预后问答框架，通过结构化提示将数值型EHR数据转换为语义丰富的文本，并与原始ECG波形表示融合，使LLM能够对两种模态进行整体推理，在MDS-ED基准测试中达到89.37%的平均AUROC。


<details>
  <summary>Details</summary>
Motivation: 准确的临床预后需要结合结构化电子健康记录（EHR）和实时生理信号（如心电图ECG）。虽然大型语言模型（LLMs）为此任务提供了强大的推理引擎，但难以原生处理这些异构的非文本数据类型。

Method: 提出UniPACT框架，核心贡献是结构化提示机制，将数值型EHR数据转换为语义丰富的文本。然后将这种文本化的患者上下文与从原始ECG波形直接学习的表示融合，使LLM能够对两种模态进行整体推理。

Result: 在综合MDS-ED基准测试中，UniPACT在包括诊断、恶化、ICU入院和死亡率在内的多样化预后任务上实现了89.37%的平均AUROC，优于专门的基线方法。多模态多任务方法对性能至关重要，并在数据缺失场景中提供了鲁棒性。

Conclusion: UniPACT通过弥合模态差距，使LLM能够有效处理异构的临床数据，为准确的临床预后提供了统一的框架，在多种预后任务上实现了最先进的性能。

Abstract: Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.

</details>


### [114] [treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding](https://arxiv.org/abs/2601.17917)
*Zhongyu Xiao,Zhiwei Hao,Jianyuan Guo,Yong Luo,Jia Liu,Jie Xu,Han Hu*

Main category: cs.LG

TL;DR: Streaming-dLLM是一个训练免费框架，通过空间维度的衰减引导后缀建模和时间维度的动态置信感知策略，显著加速扩散大语言模型的推理速度，最高可达68.2倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型推理加速方法（如KV缓存重用或启发式解码）忽略了块级扩散过程中的内在低效性：空间冗余（对信息稀疏的后缀区域进行统一建模）和时间低效（在整个解码过程中使用固定的去噪调度）。

Method: 提出Streaming-dLLM框架：1）空间维度：通过衰减引导后缀建模，剪枝冗余的掩码标记来近似完整上下文；2）时间维度：采用动态置信感知策略和早期退出机制，允许模型跳过已收敛标记的不必要迭代。

Result: 实验表明Streaming-dLLM在保持生成质量的同时，实现了最高68.2倍的推理加速，证明了其在扩散解码中的有效性。

Conclusion: Streaming-dLLM通过解决扩散大语言模型推理中的空间冗余和时间低效问题，显著提升了推理效率，为扩散模型的实用化提供了有效解决方案。

Abstract: Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.

</details>


### [115] [Dissipative Learning: A Framework for Viable Adaptive Systems](https://arxiv.org/abs/2601.17933)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: 该论文提出BEDS框架，将学习视为内在耗散过程，证明Fisher-Rao正则化是热力学最优策略，统一了现有正则化方法，重新定义学习为在耗散约束下维持可行信念状态。


<details>
  <summary>Details</summary>
Motivation: 传统学习理论将遗忘和正则化视为启发式附加组件，而作者认为它们是自适应系统的结构要求。需要从信息理论、热力学和信息几何角度建立学习作为耗散过程的统一框架。

Method: 提出BEDS（贝叶斯涌现耗散结构）框架，将学习建模为在耗散约束下压缩信念状态的演化。核心贡献是条件最优性定理，证明Fisher-Rao正则化是唯一热力学最优策略。该框架统一了Ridge、SIGReg、EMA、SAC等方法。

Result: 证明Fisher-Rao正则化在热力学意义上是最优的，而欧几里得正则化是结构上次优的。框架区分了BEDS可结晶问题（信念收敛到稳定平衡）和BEDS可维持问题（需要持续适应）。将过拟合解释为过度结晶，灾难性遗忘解释为耗散控制不足。

Conclusion: 学习应被重新定义为在耗散约束下维持可行信念状态的过程。该框架为遗忘、正则化和稳定性提供了原则性视角，将可行性、适应稳定性和有限资源下的稳定性取代渐近最优性作为主要标准。

Abstract: We propose a perspective in which learning is an intrinsically dissipative process. Forgetting and regularization are not heuristic add-ons but structural requirements for adaptive systems. Drawing on information theory, thermodynamics, and information geometry, we introduce the BEDS (Bayesian Emergent Dissipative Structures) framework, modeling learning as the evolution of compressed belief states under dissipation constraints.
  A central contribution is the Conditional Optimality Theorem, showing that Fisher-Rao regularization measuring change via information divergence rather than Euclidean distance is the unique thermodynamically optimal regularization strategy, achieving minimal dissipation. Euclidean regularization is shown to be structurally suboptimal. The framework unifies existing methods (Ridge, SIGReg, EMA, SAC) as special cases of a single governing equation.
  Within this view, overfitting corresponds to over-crystallization, while catastrophic forgetting reflects insufficient dissipation control. The framework distinguishes BEDS-crystallizable problems, where beliefs converge to stable equilibria, from BEDS-maintainable problems, which require continual adaptation. It extends naturally to continual and multi-agent systems, where viability, stability under adaptation and finite resources replaces asymptotic optimality as the primary criterion. Overall, this work reframes learning as maintaining viable belief states under dissipation constraints, providing a principled lens on forgetting, regularization, and stability.

</details>


### [116] [FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering](https://arxiv.org/abs/2601.17935)
*Daniel Commey,Matilda Nkoom,Yousef Alsenani,Sena G. Hounsinou,Garth V. Crosby*

Main category: cs.LG

TL;DR: FedGraph-VASP是一个保护隐私的联邦图学习框架，通过边界嵌入交换协议和量子安全加密，让虚拟资产服务提供商在不暴露原始数据的情况下协作检测洗钱活动。


<details>
  <summary>Details</summary>
Motivation: 虚拟资产服务提供商面临监管合规与用户隐私的冲突，现有方法要么需要共享敏感交易数据，要么孤立运作，无法检测跨机构洗钱模式。

Method: 提出边界嵌入交换协议，只共享压缩的、不可逆的图神经网络边界账户表示，使用NIST标准的Kyber-512密钥封装机制和AES-256-GCM认证加密确保安全。

Result: 在Elliptic比特币数据集上，FedGraph-VASP的F1分数达到0.508，比最先进的生成基线FedSage+（0.453）提升12.1%。在以太坊欺诈检测数据集上，FedGraph-VASP在稀疏连接场景下表现较差（0.635），而FedSage+表现优异（0.855）。

Conclusion: 存在拓扑依赖的权衡：嵌入交换对连接良好的交易图有益，而生成式插补在高度模块化的稀疏图中占优。隐私审计显示嵌入仅部分可逆（R²=0.32），限制了精确特征恢复。

Abstract: Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.

</details>


### [117] [Scaling Effects and Uncertainty Quantification in Neural Actor Critic Algorithms](https://arxiv.org/abs/2601.17954)
*Nikos Georgoudios,Konstantinos Spiliopoulos,Justin Sirignano*

Main category: cs.LG

TL;DR: 研究神经Actor-Critic算法在浅层神经网络下的收敛性和统计特性，分析不同网络宽度缩放方案对算法输出的影响，并提供超参数选择指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注算法收敛速度，但缺乏对神经Actor-Critic方法输出统计特性的全面分析。本文旨在量化算法输出的不确定性，为超参数选择提供理论依据。

Method: 采用一般逆多项式缩放方案，将网络宽度缩放指数作为可调超参数（取值在1/2到1之间）。推导网络输出的渐近展开式，分析其统计结构，并通过数值实验验证理论结果。

Result: 方差随网络宽度以幂律衰减，指数为1/2减去缩放参数。当缩放参数接近1时，统计鲁棒性更好。数值实验支持这一行为，并表明该缩放方案能加速收敛。

Conclusion: 研究为神经Actor-Critic算法提供了统计特性分析框架，给出了学习率和探索率等超参数的具体选择指导，确保算法具有可证明的良好统计行为。

Abstract: We investigate the neural Actor Critic algorithm using shallow neural networks for both the Actor and Critic models. The focus of this work is twofold: first, to compare the convergence properties of the network outputs under various scaling schemes as the network width and the number of training steps tend to infinity; and second, to provide precise control of the approximation error associated with each scaling regime. Previous work has shown convergence to ordinary differential equations with random initial conditions under inverse square root scaling in the network width. In this work, we shift the focus from convergence speed alone to a more comprehensive statistical characterization of the algorithm's output, with the goal of quantifying uncertainty in neural Actor Critic methods. Specifically, we study a general inverse polynomial scaling in the network width, with an exponent treated as a tunable hyperparameter taking values strictly between one half and one. We derive an asymptotic expansion of the network outputs, interpreted as statistical estimators, in order to clarify their structure. To leading order, we show that the variance decays as a power of the network width, with an exponent equal to one half minus the scaling parameter, implying improved statistical robustness as the scaling parameter approaches one. Numerical experiments support this behavior and further suggest faster convergence for this choice of scaling. Finally, our analysis yields concrete guidelines for selecting algorithmic hyperparameters, including learning rates and exploration rates, as functions of the network width and the scaling parameter, ensuring provably favorable statistical behavior.

</details>


### [118] [TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors](https://arxiv.org/abs/2601.17958)
*Ido Andrew Atad,Itamar Zimerman,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: TensorLens：一种将整个Transformer表示为单个输入依赖线性算子的新方法，通过高阶注意力交互张量统一编码注意力、FFN、激活、归一化和残差连接。


<details>
  <summary>Details</summary>
Motivation: 现有注意力分析大多关注单个注意力头或层，无法捕捉模型的全局行为。虽然已有研究通过平均和矩阵乘法扩展多头注意力，或纳入归一化和FFN等组件，但仍缺乏一个统一完整的表示来封装所有Transformer块。

Method: 提出TensorLens方法，将整个Transformer表示为单个输入依赖的线性算子，通过高阶注意力交互张量来联合编码注意力机制、前馈网络、激活函数、归一化层和残差连接。

Result: TensorLens在理论上具有一致性，经验验证表明它比之前的注意力聚合方法产生更丰富的表示。实验证明注意力张量可以作为开发可解释性和模型理解工具的强大基础。

Conclusion: TensorLens提供了一个理论上有依据且表达力强的线性表示，能够统一捕捉Transformer的全局计算行为，为模型理解和可解释性研究提供了新工具。

Abstract: Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.

</details>


### [119] [Federated learning for unpaired multimodal data through a homogeneous transformer model](https://arxiv.org/abs/2601.17986)
*Anders Eklund*

Main category: cs.LG

TL;DR: 提出联邦多模态学习框架，在数据模态分离、无配对样本的分布式环境下训练全局多模态Transformer，通过公共锚点集对齐私有流形，提供数学上优越的隐私保证。


<details>
  <summary>Details</summary>
Motivation: 现实联邦环境中数据通常是未配对且分散在不同节点上的（如图像和文本分别存储），现有联邦学习方法假设本地客户端拥有对齐的数据对或需要共享原始特征嵌入，这违反了数据主权。需要一种能在模态分离的分布式节点上训练全局多模态模型的方法。

Method: 1) 引入小型公共锚点集对齐分离的私有流形；2) 使用基于公共锚点计算的Gram矩阵，通过中心核对齐实现跨模态语义对齐，不传输私有样本；3) 提出子空间稳定微调方法处理大型Transformer模型；4) 提出精度加权平均，利用不确定性估计对不确定节点降权。

Result: 建立了联邦未配对基础模型的数学基础，使全局模型能够从分散、分离和私有的数据孤岛中学习世界的统一表示，无需集中存储或配对样本。

Conclusion: 该框架为联邦多模态学习提供了数学基础，解决了现实联邦环境中数据模态分离、无配对样本的挑战，同时提供了比原型共享更优越的隐私保证，实现了从分散私有数据中学习统一表示的目标。

Abstract: Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.

</details>


### [120] [Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization](https://arxiv.org/abs/2601.17987)
*Ziwei Zheng,Huizhi Liang,Vaclav Snasel,Vito Latora,Panos Pardalos,Giuseppe Nicosia,Varun Ojha*

Main category: cs.LG

TL;DR: 该研究提出了一种系统探索收敛、剪枝和量化关系的计算方法，发现尽管架构多样，但性能基本不变，学习动态呈现三个稳定阶段，并量化了剪枝冗余和量化影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习网络擅长分类，但确定可靠完成任务的最小架构仍然具有挑战性。需要系统探索收敛、剪枝和量化之间的关系，为在剪枝和低精度约束下选择紧凑稳定模型提供指导。

Method: 提出计算方法论：首先对大量架构进行结构化设计扫描，然后在代表性模型上评估收敛行为、剪枝敏感性和量化鲁棒性。研究涵盖深度神经网络、卷积神经网络和视觉变换器，聚焦复杂度递增的图像分类任务。

Result: 尽管架构多样，但性能基本不变，学习动态一致呈现三个阶段：不稳定、学习和过拟合。确定了稳定学习所需的最小可学习参数，揭示了不同的收敛和剪枝阶段，量化了降低数值精度对可训练参数的影响。更深架构比浅层架构对剪枝更具弹性，参数冗余高达60%，量化对可学习参数较少的模型影响更严重，对更难的数据集影响更大。

Conclusion: 这些发现为在图像分类中，在剪枝和低精度约束下选择紧凑、稳定的模型提供了可操作的指导。研究证实了深度架构对剪枝的弹性，并量化了参数冗余和量化影响，有助于模型压缩和部署优化。

Abstract: Deep learning networks excel at classification, yet identifying minimal architectures that reliably solve a task remains challenging. We present a computational methodology for systematically exploring and analyzing the relationships among convergence, pruning, and quantization. The workflow first performs a structured design sweep across a large set of architectures, then evaluates convergence behavior, pruning sensitivity, and quantization robustness on representative models. Focusing on well-known image classification of increasing complexity, and across Deep Neural Networks, Convolutional Neural Networks, and Vision Transformers, our initial results show that, despite architectural diversity, performance is largely invariant and learning dynamics consistently exhibit three regimes: unstable, learning, and overfitting. We further characterize the minimal learnable parameters required for stable learning, uncover distinct convergence and pruning phases, and quantify the effect of reduced numeric precision on trainable parameters. Aligning with intuition, the results confirm that deeper architectures are more resilient to pruning than shallower ones, with parameter redundancy as high as 60%, and quantization impacts models with fewer learnable parameters more severely and has a larger effect on harder image datasets. These findings provide actionable guidance for selecting compact, stable models under pruning and low-precision constraints in image classification.

</details>


### [121] [Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning](https://arxiv.org/abs/2601.17995)
*Shudi Weng,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

TL;DR: 提出H-SecCoGC方案，通过编码策略实现结构化聚合，在不可靠通信下保证模型精度和隐私保护


<details>
  <summary>Details</summary>
Motivation: 分层联邦学习(HFL)虽然改善了客户端与服务器间的链路质量，但在不可靠通信环境下，如何在保证模型精度的同时保护隐私仍然是一个关键挑战，因为隐私噪声的协调可能被随机破坏

Method: 提出鲁棒的分层安全聚合方案H-SecCoGC，集成编码策略来强制执行结构化聚合，避免部分参与问题

Result: 该方案在不同隐私级别下都能确保准确的全局模型构建，显著提高了鲁棒性、隐私保护和学习效率。理论和实验结果都证明了其在不可靠通信和任意强隐私保证下的优越性

Conclusion: H-SecCoGC方案有效解决了分层联邦学习在不可靠通信环境下面临的隐私保护与模型精度平衡问题，通过编码策略实现了鲁棒的结构化聚合

Abstract: Hierarchical federated learning (HFL) has emerged as an effective paradigm to enhance link quality between clients and the server. However, ensuring model accuracy while preserving privacy under unreliable communication remains a key challenge in HFL, as the coordination among privacy noise can be randomly disrupted. To address this limitation, we propose a robust hierarchical secure aggregation scheme, termed H-SecCoGC, which integrates coding strategies to enforce structured aggregation. The proposed scheme not only ensures accurate global model construction under varying levels of privacy, but also avoids the partial participation issue, thereby significantly improving robustness, privacy preservation, and learning efficiency. Both theoretical analyses and experimental results demonstrate the superiority of our scheme under unreliable communication across arbitrarily strong privacy guarantees

</details>


### [122] [Spelling Bee Embeddings for Language Modeling](https://arxiv.org/abs/2601.18030)
*Markus N. Rabe,Judith Clymo,Zheren Dong*

Main category: cs.LG

TL;DR: 通过修改嵌入层，将词汇拼写信息融入词嵌入中，不仅能提升拼写能力，还能在标准基准测试中普遍改进模型性能，相当于节省约8%的计算和数据资源。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的嵌入层主要关注语义信息，忽略了词汇的拼写特征。作者认为将拼写信息融入词嵌入中可能提升模型的语言理解能力，特别是在拼写相关任务上，并可能带来更广泛的性能改进。

Method: 提出了一种简单的嵌入层修改方法：将词汇的拼写信息注入到词嵌入中。具体来说，在训练过程中，让词嵌入不仅包含语义信息，还包含词汇的拼写特征信息。

Result: 在40M到800M参数规模的模型上进行扩展研究，结果显示：使用改进嵌入层的模型不仅在拼写任务上表现更好，在标准基准测试中也有普遍提升。性能改进相当于需要约8%更少的计算资源和数据就能达到相同的测试损失。

Conclusion: 将拼写信息融入词嵌入是一种简单有效的改进方法，能够提升语言模型的整体性能，同时减少训练所需的计算和数据资源，具有实际应用价值。

Abstract: We introduce a simple modification to the embedding layer. The key change is to infuse token embeddings with information about their spelling. Models trained with these embeddings improve not only on spelling, but also across standard benchmarks. We conduct scaling studies for models with 40M to 800M parameters, which suggest that the improvements are equivalent to needing about 8% less compute and data to achieve the same test loss.

</details>


### [123] [Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity](https://arxiv.org/abs/2601.18032)
*Brijesh FNU,Viet Thanh Duy Nguyen,Ashima Sharma,Md Harun Rashid Molla,Chengyi Xu,Truong-Son Hy*

Main category: cs.LG

TL;DR: 开发了一个多模态学习框架，利用预训练的聚合物表示来预测介电弹性体的性能，解决了数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 软可拉伸电子学需要高性能介电弹性体，但现有研究缺乏系统数据集，且同时实现高介电常数和低杨氏模量是重大挑战

Method: 收集了10年文献中的丙烯酸酯基介电弹性体数据集，提出多模态学习框架，利用图基和序列基编码器的预训练聚合物表示进行少样本预测

Result: 成功实现了从分子序列准确预测介电和机械性能，克服了数据稀缺问题，为新范式提供了验证

Conclusion: 该方法可推广到其他聚合物体系，加速数据高效的软高k介电弹性体发现，代码和数据集已开源

Abstract: Dielectric materials are critical building blocks for modern electronics such as sensors, actuators, and transistors. With the rapid recent advance in soft and stretchable electronics for emerging human- and robot-interfacing applications, there is a surging need for high-performance dielectric elastomers. However, it remains a grand challenge to develop soft elastomers that simultaneously possess high dielectric constants (k, related to energy storage capacity) and low Young's moduli (E, related to mechanical flexibility). While some new elastomer designs have been reported in individual (mostly one-off) studies, almost no structured dataset is currently available for dielectric elastomers that systematically encompasses their molecular sequence, dielectric, and mechanical properties. Within this context, we curate a compact, high-quality dataset of acrylate-based dielectric elastomers, one of the most widely explored elastomer backbones due to its versatile chemistry and molecular design flexibility, by screening and aggregating experimental results from the literature over the past 10 years. Building on this dataset, we propose a multimodal learning framework that leverages large-scale pretrained polymer representations from graph- and sequence-based encoders. These pretrained embeddings transfer rich chemical and structural knowledge from vast polymer corpora, enabling accurate few-shot prediction of both dielectric and mechanical properties from molecular sequences. Our results represent a new paradigm for transferring knowledge from pretrained multimodal models to overcome severe data scarcity, which can be readily translated to other polymer backbones (e.g., silicones, urethanes) and thus accelerate data-efficient discovery of soft high-k dielectric elastomers. Our source code and dataset are publicly available at https://github.com/HySonLab/Polymers

</details>


### [124] [Resonant Sparse Geometry Networks](https://arxiv.org/abs/2601.18064)
*Hasi Hays*

Main category: cs.LG

TL;DR: RSGN是一种受大脑启发的稀疏几何网络架构，通过双时间尺度机制实现输入依赖的动态稀疏连接，相比Transformer具有更低的计算复杂度和参数效率。


<details>
  <summary>Details</summary>
Motivation: Transformer架构使用密集注意力机制，具有O(n²)计算复杂度，参数效率低。受大脑稀疏连接和几何组织的启发，需要开发更高效、生物合理的神经网络架构。

Method: RSGN将计算节点嵌入学习到的双曲空间，连接强度随测地距离衰减，实现输入依赖的动态稀疏性。采用双时间尺度：快速可微分激活传播（梯度下降优化）和慢速Hebbian结构学习（局部相关规则）。

Result: RSGN实现O(n*k)计算复杂度（k<<n）。在长程依赖任务上达到96.5%准确率，参数减少约15倍；在20类层次分类任务上达到23.8%准确率（随机基线5%），仅需41,672参数，比需要403,348参数达到30.1%准确率的Transformer少近10倍。

Conclusion: 受大脑启发的稀疏几何计算原则为开发更高效、生物合理的神经架构提供了有前景的方向，Hebbian学习提供了持续改进。

Abstract: We introduce Resonant Sparse Geometry Networks (RSGN), a brain-inspired architecture with self-organizing sparse
  hierarchical input-dependent connectivity. Unlike Transformer architectures that employ dense attention mechanisms with
  O(n^2) computational complexity, RSGN embeds computational nodes in learned hyperbolic space where connection strength
  decays with geodesic distance, achieving dynamic sparsity that adapts to each input. The architecture operates on two
  distinct timescales: fast differentiable activation propagation optimized through gradient descent, and slow
  Hebbian-inspired structural learning for connectivity adaptation through local correlation rules. We provide rigorous
  mathematical analysis demonstrating that RSGN achieves O(n*k) computational complexity, where k << n represents the average
  active neighborhood size. Experimental evaluation on hierarchical classification and long-range dependency tasks
  demonstrates that RSGN achieves 96.5% accuracy on long-range dependency tasks while using approximately 15x fewer
  parameters than standard Transformers. On challenging hierarchical classification with 20 classes, RSGN achieves 23.8%
  accuracy (compared to 5% random baseline) with only 41,672 parameters, nearly 10x fewer than the Transformer baselines
  which require 403,348 parameters to achieve 30.1% accuracy. Our ablation studies confirm the contribution of each architectural
  component, with Hebbian learning providing consistent improvements. These results suggest that brain-inspired principles
  of sparse, geometrically-organized computation offer a promising direction toward more efficient and biologically plausible
  neural architectures.

</details>


### [125] [Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming](https://arxiv.org/abs/2601.18076)
*Alexandra Chouldechova,A. Feder Cooper,Solon Barocas,Abhinav Palia,Dan Vann,Hanna Wallach*

Main category: cs.LG

TL;DR: 论文指出当前AI红队测试中基于攻击成功率(ASR)比较得出的系统安全性或攻击方法有效性结论往往缺乏证据支持，存在苹果与橙子式的无效比较和低效度测量问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全评估中普遍使用攻击成功率(ASR)来比较不同系统的安全性或不同攻击方法的有效性，但这些比较往往缺乏理论基础和统计有效性，可能导致误导性结论。

Method: 采用社会科学测量理论和推断统计学框架，提出攻击成功率有意义的比较条件；以越狱攻击为例，分析苹果与橙子式比较和测量效度挑战。

Result: 研究表明许多ASR比较是无效的，因为它们不满足有意义的比较条件；论文提供了ASR可比较和不可比较的具体条件，并展示了实际评估中的常见问题。

Conclusion: AI红队测试需要更严谨的测量理论和统计基础，攻击成功率的比较必须满足特定条件才有意义；当前许多安全评估结论需要重新审视。

Abstract: We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.

</details>


### [126] [DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal](https://arxiv.org/abs/2601.18081)
*Peixuan Han,Yingjie Yu,Jingjun Xu,Jiaxuan You*

Main category: cs.LG

TL;DR: DRPG是一个用于自动生成学术反驳的智能体框架，通过分解、检索、规划和生成四步流程，显著优于现有方法，使用8B模型即可超越人类平均水平。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在科研工作流中应用日益广泛，但学术反驳这一学术交流和同行评审的关键环节仍缺乏自动化支持。现有方法通常依赖现成大模型或简单流程，难以处理长上下文，且无法生成有针对性和说服力的回应。

Method: 提出DRPG框架，包含四个步骤：1) 将审稿意见分解为原子化问题；2) 从论文中检索相关证据；3) 规划反驳策略；4) 据此生成回应。其中规划器能识别最可行的反驳方向，准确率超过98%。

Result: 在顶级会议数据上的实验表明，DRPG显著优于现有反驳流程，仅使用8B模型就达到了超越人类平均水平的性能。规划器设计有效，能提供多视角和可解释的建议。框架在更复杂的多轮场景中也表现良好。

Conclusion: DRPG框架有效，具有提供高质量反驳内容和支持学术讨论规模化的潜力。代码已开源。

Abstract: Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.

</details>


### [127] [LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts](https://arxiv.org/abs/2601.18089)
*Venmugil Elango,Nidhi Bhatia,Roger Waleffe,Rasoul Shafipour,Tomer Asida,Abhinav Khattar,Nave Assaf,Maximilian Golub,Joey Guman,Tiyasa Mitra,Ritchie Zhao,Ritika Borkar,Ran Zilberstein,Mostofa Patwary,Mohammad Shoeybi,Bita Rouhani*

Main category: cs.LG

TL;DR: LatentMoE是一种通过软硬件协同设计优化的MoE架构，在计算效率和参数效率上优于标准MoE，已被Nemotron-3模型采用。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE已成为许多先进大语言模型的核心组件，但现有MoE架构在推理成本（以每浮点运算精度和每参数精度衡量）方面是否接近最优仍不清楚。需要从软硬件协同设计角度重新审视MoE设计。

Method: 从软硬件协同设计角度，基于经验和理论考虑，分析不同部署场景下的性能瓶颈。通过系统化设计空间探索，在高达950亿参数和超过1万亿token的训练规模下，结合理论分析，开发了LatentMoE架构。

Result: LatentMoE在每FLOP精度和每参数精度方面持续优于标准MoE架构。该架构已被Nemotron-3 Super和Ultra旗舰模型采用，并扩展到更大的规模，包括更长的token序列和更大的模型尺寸。

Conclusion: 通过软硬件协同设计的系统化探索，LatentMoE为MoE架构提供了优化的解决方案，在计算效率和参数效率方面显著优于现有方法，为大语言模型的部署提供了更优的架构选择。

Abstract: Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).

</details>


### [128] [From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models](https://arxiv.org/abs/2601.18091)
*Longwei Ding,Anhao Zhao,Fanghua Ye,Ziyang Chen,Xiaoyu Shen*

Main category: cs.LG

TL;DR: 该研究对比了指令跟随型LLM和推理增强型LLM的剪枝策略，发现不同范式需要不同的剪枝方法，深度剪枝在分类任务表现更好，宽度剪枝在生成和推理任务更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝研究主要关注指令跟随型LLM，但不确定这些策略是否适用于生成长中间推理轨迹的推理增强型模型。需要系统研究不同LLM范式下的剪枝效果差异。

Method: 对指令跟随型(LLM-instruct)和推理增强型(LLM-think)模型进行控制性剪枝研究，对齐剪枝校准和剪枝后恢复数据与原始训练分布，评估静态深度剪枝、静态宽度剪枝和动态剪枝在17个分类、生成和推理任务上的表现。

Result: 发现范式依赖的明显差异：深度剪枝在分类任务上优于宽度剪枝，而宽度剪枝在生成和推理任务上更稳健；静态剪枝能更好地保持推理性能，动态剪枝在分类和生成任务上表现优异但在长链推理上仍有挑战。

Conclusion: 推理增强型LLM需要专门考虑其特点的剪枝策略，不能简单套用指令跟随型模型的剪枝方法。研究强调了为不同LLM范式设计针对性剪枝方案的重要性。

Abstract: Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\textbf{LLM-instruct}$) and reasoning-augmented ($\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.

</details>


### [129] [Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions](https://arxiv.org/abs/2601.18107)
*Pedram Agand,Mo Chen*

Main category: cs.LG

TL;DR: MoReBRAC：基于模型的不确定性感知离线强化学习框架，通过双循环世界模型合成高质量转移数据，采用分层不确定性管道确保合成数据可靠性，在D4RL基准上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在工业机器人等安全关键领域有巨大潜力，但面临静态数据集与学习策略之间的分布偏移问题，通常需要高度保守性，限制了策略改进潜力

Method: 提出MoReBRAC框架，采用双循环世界模型合成高保真转移数据来扩展训练流形，通过分层不确定性管道（VAE流形检测、模型敏感性分析、MC dropout）确保合成数据可靠性

Result: 在D4RL Gym-MuJoCo基准测试中取得显著性能提升，特别是在"随机"和"次优"数据机制下表现优异，并深入分析了VAE作为几何锚点的作用

Conclusion: MoReBRAC通过不确定性感知的潜在合成有效解决了离线强化学习中的分布偏移问题，为从近最优数据集中学习提供了新的见解和解决方案

Abstract: Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.

</details>


### [130] [AttenMIA: LLM Membership Inference Attack through Attention Signals](https://arxiv.org/abs/2601.18110)
*Pedram Zaree,Md Abdullah Al Mamun,Yue Dong,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: AttenMIA：利用Transformer自注意力模式进行成员推理攻击的新框架，相比现有方法显著提升攻击成功率，特别是在低误报率场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据集的巨大规模导致其倾向于记忆训练数据，引发严重的隐私和知识产权担忧。现有的成员推理攻击主要依赖输出置信度或嵌入特征，但这些信号往往脆弱，攻击成功率有限。

Method: 提出AttenMIA框架，利用Transformer模型内部的自注意力模式推断成员身份。通过跨层注意力头的信息，结合基于扰动的发散度量，训练有效的MIA分类器。注意力控制Transformer内部信息流，为记忆化暴露不同模式。

Result: 在LLaMA-2、Pythia和Opt等开源模型上的广泛实验表明，基于注意力的特征始终优于基线方法，特别是在重要的低误报率指标下（在WikiMIA-32基准测试中达到0.996 ROC AUC和87.9% TPR@1%FPR）。注意力信号在不同数据集和架构间具有泛化性。

Conclusion: 注意力机制虽然最初是为了增强可解释性而引入，但可能无意中放大了LLMs中的隐私风险。AttenMIA在数据提取框架中替代其他成员推理攻击时，能实现优于现有技术水平的训练数据提取攻击，突显了开发新防御措施的必要性。

Abstract: Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.

</details>


### [131] [Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting](https://arxiv.org/abs/2601.18111)
*Jean Kossaifi,Nikola Kovachki,Morteza Mardani,Daniel Leibovici,Suman Ravuri,Ira Shokar,Edoardo Calvello,Mohammad Shoaib Abbas,Peter Harrington,Ashay Subramaniam,Noah Brenowitz,Boris Bonev,Wonmin Byeon,Karsten Kreis,Dale Durran,Arash Vahdat,Mike Pritchard,Jan Kautz*

Main category: cs.LG

TL;DR: 本文提出一个可扩展的多尺度大气动力学学习框架，无需复杂架构或专门训练策略，即可实现最先进的概率天气预报技能。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的天气预报方法存在架构复杂、训练策略碎片化的问题，掩盖了预测准确性的根本驱动因素。作者希望证明最先进的概率技能并不需要复杂的架构约束或专门的训练启发式方法。

Method: 引入一个可扩展框架，结合直接下采样的潜在空间和历史条件局部投影器来解析高分辨率物理。该框架设计对概率估计器的选择具有鲁棒性，无缝支持随机插值、扩散模型和基于CRPS的集成训练。

Result: 与集成预报系统和深度学习概率模型GenCast相比，该框架在大多数变量上实现了统计显著的改进。

Conclusion: 扩展通用模型足以实现最先进的中期预测，无需定制训练方案，且在完整的概率框架谱系中都有效。

Abstract: The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.

</details>


### [132] [Robust Learning of a Group DRO Neuron](https://arxiv.org/abs/2601.18115)
*Guyang Cao,Shuyao Li,Sushrut Karmalkar,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 提出一种针对单神经元学习的组分布鲁棒优化方法，通过原对偶算法处理任意标签噪声和组分布偏移，输出与最优参数常数倍竞争的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在存在任意标签噪声和组级分布偏移的情况下，学习单神经元的标准平方损失问题。目标是找到一个在最具挑战性的组重加权下表现良好的"最佳拟合"神经元参数。

Method: 开发了一种计算高效的原对偶算法，处理损失函数的固有非凸性。算法框架采用对偶外推更新，输出与最优参数在常数因子内竞争的向量。

Result: 算法输出向量在最优组加权下与最优参数保持常数倍竞争关系。在LLM预训练基准测试中显示出潜力。

Conclusion: 该研究提供了一种鲁棒学习框架，能够直接处理损失函数的非凸性，在任意标签损坏和组特定分布偏移下提供鲁棒学习保证。

Abstract: We study the problem of learning a single neuron under standard squared loss in the presence of arbitrary label noise and group-level distributional shifts, for a broad family of covariate distributions. Our goal is to identify a ''best-fit'' neuron parameterized by $\mathbf{w}_*$ that performs well under the most challenging reweighting of the groups. Specifically, we address a Group Distributionally Robust Optimization problem: given sample access to $K$ distinct distributions $\mathcal p_{[1]},\dots,\mathcal p_{[K]}$, we seek to approximate $\mathbf{w}_*$ that minimizes the worst-case objective over convex combinations of group distributions $\boldsymbolλ \in Δ_K$, where the objective is $\sum_{i \in [K]}λ_{[i]}\,\mathbb E_{(\mathbf x,y)\sim\mathcal p_{[i]}}(σ(\mathbf w\cdot\mathbf x)-y)^2 - νd_f(\boldsymbolλ,\frac{1}{K}\mathbf1)$ and $d_f$ is an $f$-divergence that imposes (optional) penalty on deviations from uniform group weights, scaled by a parameter $ν\geq 0$. We develop a computationally efficient primal-dual algorithm that outputs a vector $\widehat{\mathbf w}$ that is constant-factor competitive with $\mathbf{w}_*$ under the worst-case group weighting. Our analytical framework directly confronts the inherent nonconvexity of the loss function, providing robust learning guarantees in the face of arbitrary label corruptions and group-specific distributional shifts. The implementation of the dual extrapolation update motivated by our algorithmic framework shows promise on LLM pre-training benchmarks.

</details>


### [133] [Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods](https://arxiv.org/abs/2601.18142)
*Mingxu Zhang,Huicheng Zhang,Jiaming Ji,Yaodong Yang,Ying Sun*

Main category: cs.LG

TL;DR: 提出ADRC-Lagrangian方法，通过主动抗扰控制增强鲁棒性，减少振荡和安全违规，相比传统方法显著提升安全强化学习性能


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习方法（包括PID和经典Lagrangian方法）存在振荡和频繁安全违规问题，主要由于参数敏感性和固有的相位滞后，需要更鲁棒的解决方案

Method: 提出ADRC-Lagrangian方法，将主动抗扰控制（ADRC）集成到Lagrangian框架中，形成统一框架，将经典和PID Lagrangian方法作为特例包含其中

Result: 实验表明该方法减少安全违规达74%，约束违规幅度减少89%，平均成本降低67%，在复杂环境中表现出优越的安全性能

Conclusion: ADRC-Lagrangian方法通过增强鲁棒性和减少振荡，显著提升了安全强化学习的性能，为解决现有方法的局限性提供了有效解决方案

Abstract: Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\%, establishing superior effectiveness for Safe RL in complex environments.

</details>


### [134] [FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning](https://arxiv.org/abs/2601.18150)
*Zhaopeng Qiu,Shuang Yu,Jingqi Zhang,Shuai Zhang,Xue Huang,Jingyi Yang,Junjie Lai*

Main category: cs.LG

TL;DR: 提出一个实用的FP8 rollout堆栈用于LLM强化学习，通过块状FP8量化、KV-cache FP8扩展和重要性采样校正，在保持学习行为的同时实现高达44%的吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 大型语言模型强化学习中的rollout（生成）过程因长输出序列导致注意力机制和KV-cache内存成为端到端时间瓶颈，需要寻找加速方法

Method: 1) 使用块状FP8量化实现W8A8线性层rollout；2) 通过每步QKV尺度重新校准将FP8扩展到KV-cache；3) 使用基于重要性采样的rollout校正（token级TIS/MIS变体）缓解训练-推理不匹配

Result: 在密集和MoE模型上，这些技术实现了高达44%的rollout吞吐量提升，同时保持了与BF16基线相当的学习行为

Conclusion: 提出的FP8 rollout堆栈有效解决了RL中低精度rollout的工程和算法挑战，在veRL生态系统中实现，支持多种训练后端和推理引擎，为大规模LLM RL提供了实用的加速解决方案

Abstract: Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.

</details>


### [135] [Learning Fair Domain Adaptation with Virtual Label Distribution](https://arxiv.org/abs/2601.18171)
*Yuguang Zhang,Lijun Sheng,Jian Liang,Ran He*

Main category: cs.LG

TL;DR: 本文提出VILL框架，通过自适应重加权和KL散度重平衡策略解决无监督域自适应中的类别公平性问题，提升最难分类别的性能同时保持高整体准确率。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应方法虽然提升了整体准确率，但忽视了不同类别间的性能差异（类别公平性问题）。研究发现UDA分类器倾向于偏向容易分类的类别而忽略困难类别。

Method: 提出VILL框架：1）自适应重加权策略，放大难以分类类别的影响；2）基于KL散度的重平衡策略，显式调整决策边界以增强类别公平性。

Result: 在常用数据集上的实验表明，VILL可以作为即插即用模块无缝集成到现有UDA方法中，显著改善类别公平性。

Conclusion: VILL是一个简单有效的框架，能够解决UDA中的类别公平性问题，提升最差情况性能同时保持高整体准确率。

Abstract: Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.

</details>


### [136] [Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients](https://arxiv.org/abs/2601.18189)
*Rui Wu,Yongjun Li*

Main category: cs.LG

TL;DR: 提出AHOC混合阶无环约束和SPG-AHOC平滑近端梯度算法，在有限迭代内精确恢复DAG结构，无需后处理阈值化


<details>
  <summary>Details</summary>
Motivation: 现有连续优化方法（如NOTEARS）只能保证渐近收敛到驻点，通常产生稠密权重矩阵，需要任意的后处理阈值化来恢复DAG。连续优化与离散图结构之间的差距是根本性挑战。

Method: 提出混合阶无环性约束（AHOC），通过平滑近端梯度算法（SPG-AHOC）进行优化。利用近端算法的流形识别特性，实现有限时间精确结构恢复。

Result: 理论证明：在标准可识别性假设下，SPG-AHOC在有限迭代内恢复精确的DAG支持（结构），即使优化的是平滑近似。消除结构模糊性，算法返回具有精确零条目的图而无需启发式截断。

Conclusion: SPG-AHOC通过有限时间oracle性质理论保证，在有限迭代内精确恢复DAG结构，无需后处理阈值化，实现了连续优化与离散图结构之间的桥梁。

Abstract: Continuous optimization has significantly advanced causal discovery, yet existing methods (e.g., NOTEARS) generally guarantee only asymptotic convergence to a stationary point. This often yields dense weighted matrices that require arbitrary post-hoc thresholding to recover a DAG. This gap between continuous optimization and discrete graph structures remains a fundamental challenge. In this paper, we bridge this gap by proposing the Hybrid-Order Acyclicity Constraint (AHOC) and optimizing it via the Smoothed Proximal Gradient (SPG-AHOC). Leveraging the Manifold Identification Property of proximal algorithms, we provide a rigorous theoretical guarantee: the Finite-Time Oracle Property. We prove that under standard identifiability assumptions, SPG-AHOC recovers the exact DAG support (structure) in finite iterations, even when optimizing a smoothed approximation. This result eliminates structural ambiguity, as our algorithm returns graphs with exact zero entries without heuristic truncation. Empirically, SPG-AHOC achieves state-of-the-art accuracy and strongly corroborates the finite-time identification theory.

</details>


### [137] [HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models](https://arxiv.org/abs/2601.18200)
*Chenyu Zhang,Xinchen Lyu,Chenshan Ren,Shuhan Liu,Qimei Cui,Xiaofeng Tao*

Main category: cs.LG

TL;DR: HeterCSI是一个通道自适应预训练框架，通过优化梯度动态和批处理策略，解决无线基础模型在CSI处理中面临的双重异构性挑战，实现跨场景泛化并提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 无线基础模型在处理跨尺度和跨场景的CSI数据时面临双重异构性挑战，现有预训练方法要么限制输入维度，要么按尺度隔离训练，限制了模型的泛化能力和可扩展性。

Method: 提出HeterCSI框架，基于对异构CSI预训练中梯度动态的新理解：尺度异构导致破坏性梯度干扰，而场景多样性在适当管理下促进建设性梯度对齐。采用尺度感知自适应批处理策略对齐相似尺度的CSI样本，设计双重掩码机制隔离有效信号和填充伪影。

Result: 在12个数据集上的实验表明，HeterCSI无需场景特定微调即可建立泛化基础模型，在CSI重建、时域和频域预测方面分别比WiFo基准降低NMSE 7.19 dB、4.08 dB和5.27 dB。训练延迟降低53%，泛化性能平均提升1.53 dB。

Conclusion: HeterCSI通过创新的梯度动态理解和优化的批处理策略，成功解决了无线基础模型在CSI处理中的双重异构性挑战，实现了训练效率与跨场景泛化性能的平衡，为6G网络应用提供了有效的解决方案。

Abstract: Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining approaches either constrain inputs to fixed dimensions or isolate training by scale, limiting the generalization and scalability of wireless foundation models. In this paper, we propose HeterCSI, a channel-adaptive pretraining framework that reconciles training efficiency with robust cross-scenario generalization via a new understanding of gradient dynamics in heterogeneous CSI pretraining. Our key insight reveals that CSI scale heterogeneity primarily causes destructive gradient interference, while scenario diversity actually promotes constructive gradient alignment when properly managed. Specifically, we formulate heterogeneous CSI batch construction as a partitioning optimization problem that minimizes zero-padding overhead while preserving scenario diversity. To solve this, we develop a scale-aware adaptive batching strategy that aligns CSI samples of similar scales, and design a double-masking mechanism to isolate valid signals from padding artifacts. Extensive experiments on 12 datasets demonstrate that HeterCSI establishes a generalized foundation model without scenario-specific finetuning, achieving superior average performance over full-shot baselines. Compared to the state-of-the-art zero-shot benchmark WiFo, it reduces NMSE by 7.19 dB, 4.08 dB, and 5.27 dB for CSI reconstruction, time-domain, and frequency-domain prediction, respectively. The proposed HeterCSI framework also reduces training latency by 53% compared to existing approaches while improving generalization performance by 1.53 dB on average.

</details>


### [138] [PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR](https://arxiv.org/abs/2601.18207)
*James Burgess,Jan N. Hansen,Duo Peng,Yuhui Zhang,Alejandro Lozano,Min Woo Sun,Emma Lundberg,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: 研究人员开发了针对科学文献的搜索智能体，使用强化学习训练模型在1600万篇生物医学论文摘要中搜索并回答技术问题，创建了PaperSearchQA数据集和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR搜索智能体主要针对通用领域QA，缺乏对科学、工程和医学等专业技术领域的支持。需要开发能够搜索和推理科学论文的智能体，这对未来AI科学家系统和实际科学研究至关重要。

Method: 1) 发布包含1600万篇生物医学论文摘要的搜索语料库；2) 构建包含6万个样本的PaperSearchQA事实型QA数据集；3) 使用强化学习训练搜索智能体在该环境中工作；4) 基于流行的Search-R1代码库进行RLVR训练。

Result: 训练的搜索智能体在技术问答任务上超越了非强化学习的检索基线。智能体展现出规划、推理和自我验证等有趣行为。数据创建方法具有可扩展性，可轻松扩展到其他科学领域。

Conclusion: 该工作成功开发了针对科学文献的搜索智能体，提供了可用的语料库、数据集和基准测试，为未来AI科学家系统奠定了基础，且方法可扩展到其他科学领域。

Abstract: Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.

</details>


### [139] [Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting](https://arxiv.org/abs/2601.18231)
*Trong Khiem Tran,Manh Cuong Dao,Phi Le Nguyen,Thao Nguyen Truong,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 提出理论框架分析特征对齐与目标微调之间的交互作用，通过特征标签失真概念建立可证明的泛化界，指导算法设计


<details>
  <summary>Details</summary>
Motivation: 预训练模型适应新特征模态的需求日益增长，但现有方法缺乏对特征对齐与目标微调之间关键交互作用的理论理解，未校准的组合会加剧源-目标特征标签结构的错配并降低目标泛化能力

Method: 开发理论框架，建立目标误差的可证明泛化界，通过特征标签失真概念解释特征对齐与目标拟合的交互作用，为实际算法设计提供可操作的优化指导

Result: 在广泛的基准数据集上，该方法显著优于现有最先进方法

Conclusion: 通过理论框架深入理解特征对齐与目标微调的交互作用，为跨学科知识整合中的预训练模型适应提供更有效的算法设计指导

Abstract: Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.

</details>


### [140] [Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity](https://arxiv.org/abs/2601.18245)
*Santanu Das,Jatin Batra*

Main category: cs.LG

TL;DR: 提出首个多项式时间算法，用于解决带有重尾噪声和对抗性损坏的鲁棒相位恢复问题，样本复杂度接近线性。


<details>
  <summary>Details</summary>
Motivation: 相位恢复在光学、晶体学等领域有广泛应用，但现有算法对测量误差的鲁棒性不足。特别是当测量值和传感向量都可能被对抗性损坏时，需要高效的鲁棒算法。

Method: 通过建立鲁棒谱初始化与鲁棒PCA之间的联系，利用鲁棒PCA的最新算法进展，开发了多项式时间的鲁棒相位恢复算法。

Result: 提出了首个多项式时间算法，能够在重尾噪声和对抗性损坏（常数比例的测量值和传感向量可能被任意损坏）下进行鲁棒相位恢复，样本复杂度接近线性于n。

Conclusion: 通过连接鲁棒谱初始化和鲁棒PCA，解决了先前认为超出已知高效算法技术的鲁棒相位恢复问题，实现了多项式时间算法和接近线性的样本复杂度。

Abstract: Phase retrieval is the classical problem of recovering a signal $x^* \in \mathbb{R}^n$ from its noisy phaseless measurements $y_i = \langle a_i, x^* \rangle^2 + ζ_i$ (where $ζ_i$ denotes noise, and $a_i$ is the sensing vector) for $i \in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.

</details>


### [141] [Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs](https://arxiv.org/abs/2601.18255)
*Fei Meng*

Main category: cs.LG

TL;DR: 本文发现经验回放在大语言模型持续学习中存在关键二分：对非结构化任务产生正向迁移，但对结构化任务（如代码生成）造成负向迁移，并提出正交子空间唤醒方法来解决这一困境。


<details>
  <summary>Details</summary>
Motivation: 大语言模型持续学习面临平衡稳定性（保留旧知识）和可塑性（学习新任务）的挑战。虽然经验回放是应对灾难性遗忘的标准方法，但其对不同能力的影响尚未充分探索。本文旨在揭示经验回放在不同类型任务中的差异化影响，并提出解决方案。

Method: 提出正交子空间唤醒方法：通过简短的"唤醒"阶段识别先前任务的关键参数子空间，并强制新任务进行正交更新，为已建立的知识结构提供数学基础的"安全保证"。

Result: 经验回放存在关键二分：对鲁棒的非结构化任务产生正向向后迁移，但对脆弱的结构化领域（如代码生成）造成严重的负向迁移。OSW方法在四任务序列实验中，在保持脆弱编码能力方面表现优异，同时维持对新任务的高可塑性。

Conclusion: 经验回放以结构完整性换取广泛巩固，需要在LLM持续学习中同时评估结构安全性和平均保留率。OSW方法通过正交更新机制成功解决了这一困境，为保护结构化知识提供了有效解决方案。

Abstract: Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief "wake-up" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded "safety guarantee" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.

</details>


### [142] [FGGM: Fisher-Guided Gradient Masking for Continual Learning](https://arxiv.org/abs/2601.18261)
*Chao-Hong Tan,Qian Chen,Wen Wang,Yukun Ma,Chong Zhang,Chong Deng,Qinglin Zhang,Xiangang Li,Jieping Ye*

Main category: cs.LG

TL;DR: FGGM使用Fisher信息指导梯度掩码，通过自适应阈值选择参数更新，减少大语言模型持续学习中的灾难性遗忘，在TRACE基准上相比SFT提升9.6%，相比MIGU提升4.4%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在持续学习过程中面临灾难性遗忘问题，现有基于参数幅度的掩码方法（如MIGU）缺乏数学原理基础，需要更有效的方法来平衡模型的稳定性和可塑性。

Method: 提出Fisher引导的梯度掩码框架，使用对角Fisher信息矩阵估计参数重要性，动态生成具有自适应阈值的二进制掩码，选择性地更新参数，无需历史数据。

Result: 在TRACE基准上，FGGM相比监督微调在保持通用能力方面相对提升9.6%，相比MIGU在TRACE任务上提升4.4%。代码生成任务分析进一步证实了其优越性能和减少遗忘的效果。

Conclusion: FGGM提供了一种基于数学原理的参数重要性估计方法，有效缓解灾难性遗忘，平衡稳定性和可塑性，是持续学习的有效解决方案。

Abstract: Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.

</details>


### [143] [Neural Network Approximation: A View from Polytope Decomposition](https://arxiv.org/abs/2601.18264)
*ZeYu Li,ShiJun Zhang,TieYong Zeng,FengLei Fan*

Main category: cs.LG

TL;DR: 本文提出了一种基于多面体分解的ReLU网络通用逼近理论，通过考虑目标函数的局部正则性，相比传统的均匀划分方法更加高效灵活，特别在处理奇异点附近时表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有通用逼近理论大多通过均匀划分输入空间来构造神经网络，没有考虑目标函数的局部正则性。作者希望从多面体分解的角度研究ReLU网络的通用逼近能力，提供更现实、任务导向的方法。

Method: 1. 开发显式核多项式方法推导连续函数的通用逼近，特征包括精细的Totik-Ditzian型连续性模和多面体域分解；2. 在每个子域中分别构造ReLU网络来逼近核多项式；3. 将方法扩展到解析函数以获得更高的逼近速率。

Result: 多面体分解使逼近比现有方法更加高效灵活，特别是在目标函数奇异点附近。通过扩展到解析函数，可以达到更高的逼近速率。

Conclusion: 从多面体分解视角研究ReLU网络的通用逼近能力提供了更现实、任务导向的方法，相比传统均匀划分方法在处理局部正则性和奇异点方面具有优势，并能扩展到解析函数获得更高逼近速率。

Abstract: Universal approximation theory offers a foundational framework to verify neural network expressiveness, enabling principled utilization in real-world applications. However, most existing theoretical constructions are established by uniformly dividing the input space into tiny hypercubes without considering the local regularity of the target function. In this work, we investigate the universal approximation capabilities of ReLU networks from a view of polytope decomposition, which offers a more realistic and task-oriented approach compared to current methods. To achieve this, we develop an explicit kernel polynomial method to derive an universal approximation of continuous functions, which is characterized not only by the refined Totik-Ditzian-type modulus of continuity, but also by polytopical domain decomposition. Then, a ReLU network is constructed to approximate the kernel polynomial in each subdomain separately. Furthermore, we find that polytope decomposition makes our approximation more efficient and flexible than existing methods in many cases, especially near singular points of the objective function. Lastly, we extend our approach to analytic functions to reach a higher approximation rate.

</details>


### [144] [What Do Learned Models Measure?](https://arxiv.org/abs/2601.18278)
*Indrė Žliobaitė*

Main category: cs.LG

TL;DR: 论文指出机器学习模型作为测量工具时，标准预测评估标准（如泛化误差、校准、鲁棒性）无法保证测量稳定性，即不同模型可能实现不等价的测量函数，需要新的评估维度。


<details>
  <summary>Details</summary>
Motivation: 在科学和数据驱动应用中，机器学习模型越来越多地被用作测量工具而非仅仅是预测预定义标签。当测量函数从数据中学习时，从观察到数量的映射由训练分布和归纳偏置隐式决定，允许多个不等价的映射满足标准预测评估标准。这引发了对测量函数作为独立评估对象的关注。

Method: 将学习到的测量函数形式化为独立的评估焦点，引入测量稳定性概念，捕捉测量量在学习过程的可接受实现和不同上下文中的不变性。通过真实案例研究，展示具有可比预测性能的模型可以实现系统不等价的测量函数。

Result: 研究表明标准机器学习评估标准（包括泛化误差、校准和鲁棒性）无法保证测量稳定性。分布偏移提供了这种失败的具体例证，模型在预测性能相似的情况下可能实现系统不等价的测量函数。

Conclusion: 现有评估框架在将学习到的模型输出识别为测量的场景中存在局限性，需要额外的评估维度来确保测量稳定性，特别是在科学和数据驱动应用中模型作为测量工具使用时。

Abstract: In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.

</details>


### [145] [TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment](https://arxiv.org/abs/2601.18292)
*Zhewen Tan,Wenhan Yu,Jianfeng Si,Tongxin Liu,Kaiqi Guan,Huiyan Jin,Jiawen Tao,Xiaokun Yuan,Duohe Ma,Xiangzheng Zhang,Tong Yang,Lin Sun*

Main category: cs.LG

TL;DR: TriPlay-RL是一个用于大语言模型安全对齐的闭环强化学习框架，通过攻击者、防御者和评估者三个角色的协同进化，实现无需人工标注的安全性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全风险日益突出，需要减少有毒有害内容的生成。当前主流的安全对齐范式通常采用攻击者、防御者和评估者三个角色的协作框架，但需要大量人工标注。

Method: 提出TriPlay-RL闭环强化学习框架，使三个角色能够在近乎零人工标注的情况下进行迭代式协同改进。攻击者生成对抗性提示，防御者进行安全防御，评估者评估响应质量，三者通过强化学习在统一学习循环中共同进化。

Result: 攻击者在保持高输出多样性的同时，对抗效果提升20%-50%；防御者在安全性能上获得10%-30%的提升，且不损害一般推理能力；评估者通过迭代持续细化判断能力，能准确区分不安全响应、简单拒绝和有用指导。

Conclusion: TriPlay-RL为大语言模型安全对齐建立了一个高效且可扩展的范式，能够在统一学习循环中实现持续的共同进化，为解决LLM安全问题提供了新思路。

Abstract: In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.

</details>


### [146] [A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods](https://arxiv.org/abs/2601.18314)
*Lina Felsner,Sevgi G. Kafali,Hannah Eichhorn,Agnes A. J. Leth,Aidas Batvinskas,Andre Datchev,Fabian Klemm,Jan Aulich,Puntika Leepagorn,Ruben Klinger,Daniel Rueckert,Julia A. Schnabel*

Main category: cs.LG

TL;DR: 学生可重复性黑客松：复现三篇MRI重建论文（MoDL、HUMUS-Net、物理正则化动态MRI方法），探讨可重复代码库构建实践


<details>
  <summary>Details</summary>
Motivation: 促进MRI重建领域的可重复性研究，通过学生黑客松形式验证重要论文结果的可复现性，并建立可重复代码库的最佳实践

Method: 组织学生黑客松，设计复现协议，针对三篇MRI重建论文（MoDL、HUMUS-Net、物理正则化动态MRI方法）进行结果复现，同时进行额外实验验证

Result: 报告了黑客松的复现结果，展示了三篇论文的可复现性程度，并提供了额外的实验数据来验证原始论文的发现

Conclusion: 通过黑客松成功验证了MRI重建论文的可复现性，总结了构建可重复代码库的基本实践，为领域内的可重复研究提供了实用指导

Abstract: We report the design, protocol, and outcomes of a student reproducibility hackathon focused on replicating the results of three influential MRI reconstruction papers: (a) MoDL, an unrolled model-based network with learned denoising; (b) HUMUS-Net, a hybrid unrolled multiscale CNN+Transformer architecture; and (c) an untrained, physics-regularized dynamic MRI method that uses a quantitative MR model for early stopping. We describe the setup of the hackathon and present reproduction outcomes alongside additional experiments, and we detail fundamental practices for building reproducible codebases.

</details>


### [147] [Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals](https://arxiv.org/abs/2601.18326)
*Jie Li,Jing Li,Lu Lv,Zhanyu Ju,Fengkui Gong*

Main category: cs.LG

TL;DR: 提出基于ZC序列和时频图像认知融合的无人机信号OOD检测算法，在RID和OODD指标上分别提升1.7%和7.5%


<details>
  <summary>Details</summary>
Motivation: 无人机远程识别(RID)中需要检测未知或非标准通信协议的无人机信号，现有方法难以有效处理这种分布外检测(OODD)问题

Method: 结合ZC序列和时频图像双模态特征：ZC序列识别DJI无人机协议，时频图像捕获未知协议信号特征；通过特征增强对齐、多模态交互融合、空间通道维度判别分数计算和自适应注意力加权

Result: 在RID和OODD指标上分别提升1.7%和7.5%，在不同飞行条件和无人机类型下表现出强鲁棒性

Conclusion: 提出的双模态认知融合算法能有效检测未知协议无人机信号，显著优于现有方法，具有实际应用价值

Abstract: We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.

</details>


### [148] [Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection](https://arxiv.org/abs/2601.18329)
*Chuhan Feng,Jing Li,Jie Li,Lu Lv,Fengkui Gong*

Main category: cs.LG

TL;DR: 提出基于可区分性驱动的空间-通道选择与梯度范数的无人机信号OOD检测算法，通过自适应加权时频图像特征并融合梯度范数度量，实现优越的OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机信号检测中，传统方法对分布外(OOD)样本识别能力有限，需要开发能够有效区分已知协议与未知信号的检测算法，以应对实际环境中信号多样性和噪声干扰的挑战。

Method: 1) 基于协议特异性时频特征量化类间相似性和方差，自适应加权空间和通道维度的时频图像特征；2) 引入梯度范数度量扰动敏感性，捕捉OOD样本固有不稳定性；3) 将梯度范数与基于能量的分数融合进行联合推理。

Result: 仿真结果表明，该算法在不同信噪比和多种无人机类型下，展现出优越的判别能力和鲁棒性能，显著提升了OOD检测效果。

Conclusion: 提出的基于可区分性驱动的空间-通道选择与梯度范数融合的算法，为无人机信号OOD检测提供了有效解决方案，在复杂实际环境中具有良好的应用前景。

Abstract: We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.

</details>


### [149] [Structural Gender Bias in Credit Scoring: Proxy Leakage](https://arxiv.org/abs/2601.18342)
*Navya SD,Sreekanth D,SS Uma Sankari*

Main category: cs.LG

TL;DR: 研究审计台湾信用违约数据集中的结构性性别偏见，挑战"公平通过盲视"理念，发现即使移除受保护属性和应用标准公平干预，性别预测信号仍深植于非敏感特征中，传统公平审计不足以检测隐性结构性偏见。


<details>
  <summary>Details</summary>
Motivation: 金融机构日益采用机器学习进行信用风险评估，但算法偏见持续存在，阻碍公平金融包容。本研究旨在审计台湾信用违约数据集中的结构性性别偏见，挑战"公平通过盲视"的主流理念。

Method: 使用SHAP（SHapley Additive exPlanations）识别婚姻状况、年龄和信用额度等变量作为性别代理；采用对抗性逆建模框架从纯非敏感金融特征中重建受保护的性别属性。

Result: 研究发现性别预测信号深植于非敏感特征中，传统公平审计无法检测隐性结构性偏见。受保护的性别属性可以从纯非敏感金融特征中以ROC AUC 0.65的分数重建，证明传统公平审计不足。

Conclusion: 研究主张从表面统计平等转向因果感知建模和结构性问责，强调需要更深入的方法来检测和缓解金融AI中的结构性偏见。

Abstract: As financial institutions increasingly adopt machine learning for credit risk assessment, the persistence of algorithmic bias remains a critical barrier to equitable financial inclusion. This study provides a comprehensive audit of structural gender bias within the Taiwan Credit Default dataset, specifically challenging the prevailing doctrine of "fairness through blindness." Despite the removal of explicit protected attributes and the application of industry standard fairness interventions, our results demonstrate that gendered predictive signals remain deeply embedded within non-sensitive features. Utilizing SHAP (SHapley Additive exPlanations), we identify that variables such as Marital Status, Age, and Credit Limit function as potent proxies for gender, allowing models to maintain discriminatory pathways while appearing statistically fair. To mathematically quantify this leakage, we employ an adversarial inverse modeling framework. Our findings reveal that the protected gender attribute can be reconstructed from purely non-sensitive financial features with an ROC AUC score of 0.65, demonstrating that traditional fairness audits are insufficient for detecting implicit structural bias. These results advocate for a shift from surface-level statistical parity toward causal-aware modeling and structural accountability in financial AI.

</details>


### [150] [Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning](https://arxiv.org/abs/2601.18356)
*Weiqin Yang,Haowen Xue,Qingyi Peng,Hexuan Hu,Qian Huang,Tingbo Zhang*

Main category: cs.LG

TL;DR: 提出多模态因果检索增强生成框架，将因果推理与多模态检索结合，提升医学视觉语言模型的准确性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型主要依赖统计相关性而非因果病理机制，导致模型脆弱、易产生幻觉且对数据集偏差敏感。传统检索增强生成依赖语义相似性，会引入新的虚假相关性。

Method: 提出多模态因果检索增强生成框架，从外部源检索临床相关示例和因果图，基于反事实和干预证据而非单纯相关性来调节模型推理。

Result: 应用于放射学报告生成、诊断预测和视觉问答任务，提高了事实准确性、对分布偏移的鲁棒性和可解释性。

Conclusion: 因果检索为医学视觉语言模型提供了一条可扩展的路径，使其超越模式匹配，在高风险临床环境中实现可信的多模态推理。

Abstract: Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.

</details>


### [151] [Estimating Dense-Packed Zone Height in Liquid-Liquid Separation: A Physics-Informed Neural Network Approach](https://arxiv.org/abs/2601.18399)
*Mehmet Velioglu,Song Zhai,Alexander Mitsos,Adel Mhamdi,Andreas Jupke,Manuel Dahmen*

Main category: cs.LG

TL;DR: 使用物理信息神经网络结合扩展卡尔曼滤波器，通过流量测量估计重力沉降器中液-液分散的相高度


<details>
  <summary>Details</summary>
Motivation: 重力沉降器中密集堆积区高度是关键性能和安全指标，但传统光学测量方法昂贵且不实用，需要开发基于低成本流量测量的估计方法

Method: 采用两阶段训练策略：先在合成数据和物理方程上预训练PINN，再用少量实验数据微调；然后将可微PINN集成到扩展卡尔曼滤波器框架中，实现相高度状态估计

Result: 在所有评估中，两阶段训练的PINN相比机理模型、非预训练PINN和纯数据驱动神经网络，都能提供最准确的相高度估计

Conclusion: 提出的两阶段训练PINN结合状态估计框架，能够仅通过流量测量有效估计重力沉降器中的相高度，为工业过程监控提供了实用且经济的解决方案

Abstract: Separating liquid-liquid dispersions in gravity settlers is critical in chemical, pharmaceutical, and recycling processes. The dense-packed zone height is an important performance and safety indicator but it is often expensive and impractical to measure due to optical limitations. We propose to estimate phase heights using only inexpensive volume flow measurements. To this end, a physics-informed neural network (PINN) is first pretrained on synthetic data and physics equations derived from a low-fidelity (approximate) mechanistic model to reduce the need for extensive experimental data. While the mechanistic model is used to generate synthetic training data, only volume balance equations are used in the PINN, since the integration of submodels describing droplet coalescence and sedimentation into the PINN would be computationally prohibitive. The pretrained PINN is then fine-tuned with scarce experimental data to capture the actual dynamics of the separator. We then employ the differentiable PINN as a predictive model in an Extended Kalman Filter inspired state estimation framework, enabling the phase heights to be tracked and updated from flow-rate measurements. We first test the two-stage trained PINN by forward simulation from a known initial state against the mechanistic model and a non-pretrained PINN. We then evaluate phase height estimation performance with the filter, comparing the two-stage trained PINN with a two-stage trained purely data-driven neural network. All model types are trained and evaluated using ensembles to account for model parameter uncertainty. In all evaluations, the two-stage trained PINN yields the most accurate phase-height estimates.

</details>


### [152] [Superlinear Multi-Step Attention](https://arxiv.org/abs/2601.18401)
*Yufeng Huang*

Main category: cs.LG

TL;DR: 提出Superlinear attention架构，通过多步注意力机制实现次二次复杂度，同时保持随机上下文访问能力，在长序列处理中实现高效解码。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制在处理长序列时面临二次复杂度问题，现有方法要么牺牲随机访问能力，要么效率有限。需要一种既能保持随机上下文访问又能实现次二次复杂度的新架构。

Method: 将标准因果自注意力重新表述为N步搜索问题，提出Superlinear attention架构。以N=2为例，第一步进行跨度搜索选择相关序列片段，第二步在选定片段上应用标准注意力，总体复杂度为O(L^{1+1/N})。

Result: 在O(L^{1.54})配置下，在单B200 GPU上实现1M上下文长度时平均解码吞吐量114 tokens/sec，10M上下文时80 tokens/sec。在NIAH任务上达到256K上下文长度的强性能表现。

Conclusion: Superlinear attention架构在保持随机上下文访问的同时实现了次二次复杂度，展示了系统可行性。该工作侧重于架构设计、扩展分析和系统验证，全面质量评估留待未来工作。

Abstract: In this paper, we propose \textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \textbf{random context access} (a.k.a.\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.

</details>


### [153] [Frequency-Based Hyperparameter Selection in Games](https://arxiv.org/abs/2601.18409)
*Aniket Sanyal,Baraah A. M. Sidahmed,Rebekka Burkholz,Tatjana Chavdarova*

Main category: cs.LG

TL;DR: 提出Modal LookAhead (MoLA)方法，通过频率估计自适应选择超参数来优化游戏中的学习过程，解决传统超参数调优在旋转动力学中失效的问题。


<details>
  <summary>Details</summary>
Motivation: 平滑游戏中的学习与标准最小化问题有本质区别，旋转动力学使经典超参数调优策略失效。尽管实践重要，但游戏中有效的调优方法仍未被充分探索。LookAhead方法虽表现良好，但引入了影响性能的关键额外参数。

Method: 通过频率估计分析振荡动力学，包括连续时间轨迹和离散动力学频谱。基于此分析提出Modal LookAhead (MoLA)，扩展LA方法，能够根据给定问题自适应选择超参数。

Result: MoLA在纯旋转游戏和混合机制中都能加速训练，且计算开销极小。提供了收敛性保证，实验验证了其有效性。

Conclusion: MoLA为游戏中的超参数选择提供了原则性方法，通过频率分析自适应调优，解决了旋转动力学带来的挑战，显著提升了训练效率。

Abstract: Learning in smooth games fundamentally differs from standard minimization due to rotational dynamics, which invalidate classical hyperparameter tuning strategies. Despite their practical importance, effective methods for tuning in games remain underexplored. A notable example is LookAhead (LA), which achieves strong empirical performance but introduces additional parameters that critically influence performance. We propose a principled approach to hyperparameter selection in games by leveraging frequency estimation of oscillatory dynamics. Specifically, we analyze oscillations both in continuous-time trajectories and through the spectrum of the discrete dynamics in the associated frequency-based space. Building on this analysis, we introduce \emph{Modal LookAhead (MoLA)}, an extension of LA that selects the hyperparameters adaptively to a given problem. We provide convergence guarantees and demonstrate in experiments that MoLA accelerates training in both purely rotational games and mixed regimes, all with minimal computational overhead.

</details>


### [154] [Gradient Regularized Natural Gradients](https://arxiv.org/abs/2601.18420)
*Satya Prakash Dash,Hossein Abdi,Wei Pan,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出GRNG优化器，将梯度正则化与自然梯度结合，提升优化速度与泛化性能


<details>
  <summary>Details</summary>
Motivation: 梯度正则化能提升模型泛化能力，自然梯度能加速优化初期训练，但二阶优化器的训练动态如何从梯度正则化中受益尚未得到充分研究

Method: 提出梯度正则化自然梯度(GRNG)框架，包含两个算法：1) 频率派变体通过结构化近似避免Fisher信息矩阵显式求逆；2) 贝叶斯变体基于正则化卡尔曼公式完全消除FIM求逆需求

Result: 理论证明梯度正则化提升稳定性并确保收敛到全局最小值；实验显示GRNG在优化速度和泛化性能上优于一阶方法(SGD、AdamW)和二阶基线(K-FAC、Sophia)，在视觉和语言基准上表现强劲

Conclusion: 梯度正则化是解锁自然梯度方法在大规模深度学习中的鲁棒性的原则性和实用工具

Abstract: Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.

</details>


### [155] [GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level](https://arxiv.org/abs/2601.18447)
*Jinlong Hu,Jiacheng Liu*

Main category: cs.LG

TL;DR: GCFX：基于深度图生成的模型级反事实解释方法，通过增强的图生成框架和全局摘要算法，为深度图学习模型提供高质量的反事实解释，提升模型透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 深度图学习模型虽然处理图结构数据能力强，但内部架构复杂且缺乏透明度，导致决策难以解释，用户难以理解和信任这些黑盒模型。

Method: 提出GCFX方法，基于深度图生成技术，采用双编码器、结构感知标记器和消息传递神经网络解码器的架构，准确学习输入数据的真实潜在分布，生成高质量反事实示例，再通过全局反事实摘要算法选择最具代表性的解释。

Result: 在合成数据集和多个真实数据集上的实验表明，GCFX在反事实有效性和覆盖范围方面优于现有方法，同时保持较低的解释成本。

Conclusion: GCFX为深度图学习模型提供了有效的模型级反事实解释，增强了全局反事实解释的实用性和可信度，有助于提高用户对模型决策的理解和信任。

Abstract: Deep graph learning models have demonstrated remarkable capabilities in processing graph-structured data and have been widely applied across various fields. However, their complex internal architectures and lack of transparency make it difficult to explain their decisions, resulting in opaque models that users find hard to understand and trust. In this paper, we explore model-level explanation techniques for deep graph learning models, aiming to provide users with a comprehensive understanding of the models' overall decision-making processes and underlying mechanisms. Specifically, we address the problem of counterfactual explanations for deep graph learning models by introducing a generative model-level counterfactual explanation approach called GCFX, which is based on deep graph generation. This approach generates a set of high-quality counterfactual explanations that reflect the model's global predictive behavior by leveraging an enhanced deep graph generation framework and a global summarization algorithm. GCFX features an architecture that combines dual encoders, structure-aware taggers, and Message Passing Neural Network decoders, enabling it to accurately learn the true latent distribution of input data and generate high-quality, closely related counterfactual examples. Subsequently, a global counterfactual summarization algorithm selects the most representative and comprehensive explanations from numerous candidate counterfactuals, providing broad insights into the model's global predictive patterns. Experiments on a synthetic dataset and several real-world datasets demonstrate that GCFX outperforms existing methods in terms of counterfactual validity and coverage while maintaining low explanation costs, thereby offering crucial support for enhancing the practicality and trustworthiness of global counterfactual explanations.

</details>


### [156] [Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States](https://arxiv.org/abs/2601.18479)
*Kyoleen Kwak,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 提出ASAP方法，通过过渡诱导相似状态和对齐动作来平滑强化学习中的高频振荡，提高控制稳定性


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在控制任务中表现出色，但其固有的高频动作振荡限制了在实际环境中的应用。现有方法通常依赖启发式或合成的状态相似性定义来促进动作一致性，但这些定义往往无法准确反映底层系统动态。

Method: 提出ASAP方法：1）引入过渡诱导相似状态，定义为从前一状态过渡到的下一状态分布；2）通过对齐过渡诱导相似状态中的动作来强制动作平滑性；3）惩罚二阶差异以抑制高频振荡。该方法仅利用环境反馈和实际收集的数据。

Result: 在Gymnasium和Isaac-Lab环境中的实验表明，ASAP相比现有方法能够产生更平滑的控制并提高策略性能。

Conclusion: ASAP通过过渡诱导相似状态有效缓解了深度强化学习中的动作振荡问题，该方法更好地捕捉系统动态，在实际应用中具有更好的控制平滑性和性能表现。

Abstract: Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.

</details>


### [157] [Nearly Optimal Bayesian Inference for Structural Missingness](https://arxiv.org/abs/2601.18500)
*Chen Liang,Donghua Yang,Yutong Wang,Tianle Zhang,Shenghe Zhou,Zhiyu Liang,Hengtong Zhang,Hongzhi Wang,Ziqi Li,Xiyang Zhang,Zheng Liang,Yifei Li*

Main category: cs.LG

TL;DR: 提出贝叶斯框架处理结构性缺失数据，通过后验预测分布集成不确定性而非单点插补，在分类和插补任务上达到SOTA


<details>
  <summary>Details</summary>
Motivation: 结构性缺失数据存在因果循环困境：预测需要缺失特征，但推断它们又依赖于缺失机制；MNAR下缺失部分来自偏移分布；单点插补会锁定不确定性导致过度自信的偏差决策

Method: 采用贝叶斯视角，通过后验预测分布进行预测，集成完整模型后验不确定性。框架解耦为：(1)学习模型内缺失值后验分布；(2)通过优化预测后验分布进行标签预测，实现后验集成

Result: 在43个分类基准和15个插补基准上达到SOTA，在SCM先验下具有有限样本近贝叶斯最优性保证

Conclusion: 贝叶斯框架提供"几乎免费午餐"：一旦学习到后验分布，预测即插即用同时保持不确定性传播，解决了结构性缺失数据的核心挑战

Abstract: Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.

</details>


### [158] [Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark](https://arxiv.org/abs/2601.18509)
*Andro Sabashvili*

Main category: cs.LG

TL;DR: 这篇综述论文探讨了如何在时间序列预测中应用保形预测（Conformal Prediction），主要挑战是时间序列的依赖性违反了保形预测所需的数据可交换性假设，论文系统回顾了解决这一冲突的各类算法方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中的可靠不确定性量化至关重要，但传统方法通常依赖于限制性的分布假设。保形预测作为一种无分布框架，能提供具有严格理论保证的预测区间，但将其应用于序列数据时面临核心挑战：时间依赖性违反了标准保形预测所依赖的数据可交换性假设。

Method: 论文系统回顾了解决时间序列保形预测中可交换性冲突的四大类算法方法：1）放松可交换性假设的方法；2）重新定义数据单元为独立时间序列集合的方法；3）显式建模预测残差动态的方法；4）适应分布漂移以维持长期覆盖的在线学习算法。

Result: 通过综合这些方法，论文强调了计算效率和在实际数据上的性能表现，为时间序列预测中的不确定性量化提供了系统性的解决方案框架。

Conclusion: 保形预测为时间序列预测中的不确定性量化提供了有前景的无分布框架，尽管面临时间依赖性的挑战，但通过多种算法创新可以解决可交换性假设的违反问题，为实际应用提供了有效工具。

Abstract: Reliable uncertainty quantification is of critical importance in time series forecasting, yet traditional methods often rely on restrictive distributional assumptions. Conformal prediction (CP) has emerged as a promising distribution-free framework for generating prediction intervals with rigorous theoretical guarantees. However, applying CP to sequential data presents a primary challenge: the temporal dependencies inherent in time series fundamentally violate the core assumption of data exchangeability, upon which standard CP guarantees are built. This review critically examines the main categories of algorithmic solutions designed to address this conflict. We survey and benchmark methods that relax the exchangeability assumption, those that redefine the data unit to be a collection of independent time series, approaches that explicitly model the dynamics of the prediction residuals, and online learning algorithms that adapt to distribution shifts to maintain long-run coverage. By synthesizing these approaches, we highlight computational efficiency and practical performance on real-world data.

</details>


### [159] [Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates](https://arxiv.org/abs/2601.18510)
*Yibo Li,Zijie Lin,Ailin Deng,Xuan Zhang,Yufei He,Shuo Ji,Tri Cao,Bryan Hooi*

Main category: cs.LG

TL;DR: JitRL是一种无需训练的强化学习框架，通过动态记忆和经验检索实现测试时策略优化，无需梯度更新，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理在部署后权重固定，难以持续适应新任务。传统强化学习（RL）虽然能解决这个问题，但计算成本过高且存在灾难性遗忘风险。

Method: JitRL维护动态非参数化记忆库，检索相关轨迹实时估计动作优势值，然后直接调制LLM的输出logits。该加性更新规则被证明是KL约束策略优化目标的精确闭式解。

Result: 在WebArena和Jericho基准测试中，JitRL在无需训练的方法中达到最先进水平，甚至超越计算昂贵的微调方法（如WebRL），同时将成本降低30倍以上。

Conclusion: JitRL为持续学习代理提供了一条可扩展的路径，能够在测试时优化策略而无需梯度更新，显著降低了计算和金钱成本。

Abstract: While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.

</details>


### [160] [LipNeXt: Scaling up Lipschitz-based Certified Robustness to Billion-parameter Models](https://arxiv.org/abs/2601.18513)
*Kai Hu,Haoqi Hu,Matt Fredrikson*

Main category: cs.LG

TL;DR: LipNeXt：首个无约束、无卷积的1-Lipschitz架构，通过流形优化和空间移位模块实现高效确定性认证，在ImageNet上扩展到10亿参数规模，显著提升认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于Lipschitz的认证方法能提供高效、确定性的鲁棒性保证，但传统方法在模型规模、训练效率和ImageNet性能方面难以扩展。需要开发能够适应现代扩展趋势的Lipschitz架构。

Method: 1. 流形优化：直接在正交流形上更新参数；2. 空间移位模块：无需卷积即可建模空间模式；3. 使用正交投影、空间移位、β-Abs非线性激活和L2空间池化来保持紧密的Lipschitz控制。

Result: 在CIFAR-10/100和Tiny-ImageNet上达到最先进的干净准确率和认证鲁棒准确率；在ImageNet上扩展到1-20亿参数大模型，认证鲁棒准确率比先前Lipschitz模型提升高达8%（ε=1），同时保持高效稳定的低精度训练。

Conclusion: Lipschitz-based认证方法能够受益于现代扩展趋势，而无需牺牲确定性或效率，为大规模确定性鲁棒性认证开辟了新途径。

Abstract: Lipschitz-based certification offers efficient, deterministic robustness guarantees but has struggled to scale in model size, training efficiency, and ImageNet performance. We introduce \emph{LipNeXt}, the first \emph{constraint-free} and \emph{convolution-free} 1-Lipschitz architecture for certified robustness. LipNeXt is built using two techniques: (1) a manifold optimization procedure that updates parameters directly on the orthogonal manifold and (2) a \emph{Spatial Shift Module} to model spatial pattern without convolutions. The full network uses orthogonal projections, spatial shifts, a simple 1-Lipschitz $β$-Abs nonlinearity, and $L_2$ spatial pooling to maintain tight Lipschitz control while enabling expressive feature mixing. Across CIFAR-10/100 and Tiny-ImageNet, LipNeXt achieves state-of-the-art clean and certified robust accuracy (CRA), and on ImageNet it scales to 1-2B large models, improving CRA over prior Lipschitz models (e.g., up to $+8\%$ at $\varepsilon{=}1$) while retaining efficient, stable low-precision training. These results demonstrate that Lipschitz-based certification can benefit from modern scaling trends without sacrificing determinism or efficiency.

</details>


### [161] [Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning](https://arxiv.org/abs/2601.18521)
*Emna Boudabbous,Mohamed Karaa,Lokman Sboui,Julio Montecinos,Omar Alam*

Main category: cs.LG

TL;DR: 提出一个城市级公交延误预测框架，通过多分辨率特征工程、降维和深度学习，实现实时、可扩展的预测系统


<details>
  <summary>Details</summary>
Motivation: 公交机构需要可靠的网络级延误预测来提供准确的到站信息和支持实时运营控制。现有系统通常只处理少数线路，依赖手工特征，缺乏可扩展架构指导

Method: 结合多分辨率特征工程（1,683个时空特征）、自适应PCA降维（保留95%方差压缩至83个组件）、混合H3+拓扑聚类（12个平衡路线簇），采用全局LSTM模型进行预测

Result: 全局LSTM模型在准确性和效率上达到最佳平衡，比Transformer模型性能提升18-52%，参数减少275倍。在蒙特利尔STM网络6个月数据上验证，适合实时城市级部署

Conclusion: 提出的预测管道适用于实时城市级部署，可重用性高，对其他网络只需有限适配。解决了现有系统的可扩展性问题，为公交延误预测提供了实用框架

Abstract: Urban bus transit agencies need reliable, network-wide delay predictions to provide accurate arrival information to passengers and support real-time operational control. Accurate predictions help passengers plan their trips, reduce waiting time, and allow operations staff to adjust headways, dispatch extra vehicles, and manage disruptions. Although real-time feeds such as GTFS-Realtime (GTFS-RT) are now widely available, most existing delay prediction systems handle only a few routes, depend on hand-crafted features, and offer little guidance on how to design a scalable, reusable architecture.
  We present a city-scale prediction pipeline that combines multi-resolution feature engineering, dimensionality reduction, and deep learning. The framework generates 1,683 spatiotemporal features by exploring 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, and compresses them into 83 components using Adaptive PCA while preserving 95% of the variance. To avoid the "giant cluster" problem that occurs when dense urban areas fall into a single H3 region, we introduce a hybrid H3+topology clustering method that yields 12 balanced route clusters (coefficient of variation 0.608) and enables efficient distributed training.
  We compare five model architectures on six months of bus operations from the Société de transport de Montréal (STM) network in Montréal. A global LSTM with cluster-aware features achieves the best trade-off between accuracy and efficiency, outperforming transformer models by 18 to 52% while using 275 times fewer parameters. We also report multi-level evaluation at the elementary segment, segment, and trip level with walk-forward validation and latency analysis, showing that the proposed pipeline is suitable for real-time, city-scale deployment and can be reused for other networks with limited adaptation.

</details>


### [162] [From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale](https://arxiv.org/abs/2601.18524)
*Yongqi Jin,Yecheng Wang,Jun-jie Wang,Rong Zhu,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: 提出半监督框架，利用文献提取的百万级未标记NMR谱图训练化学位移预测模型，无需原子级标注，显著提升预测准确性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有NMR化学位移预测方法依赖有限的人工标注数据集，标注成本高且数据规模受限，需要利用大规模文献中未标注的谱图数据来提升模型性能

Method: 提出半监督框架，将少量标记数据与大规模未标注文献谱图结合；将化学位移预测建模为置换不变集合监督问题，在损失函数满足特定条件下，最优二分匹配简化为基于排序的损失函数，实现稳定的大规模半监督训练

Result: 模型在准确性和鲁棒性上显著超越现有方法，在更大更多样的分子数据集上表现出更强的泛化能力；首次在常见NMR溶剂中捕捉到系统性的溶剂效应

Conclusion: 大规模文献提取的未标注谱图可作为训练NMR位移模型的有效数据源，表明文献衍生的弱结构化数据在科学领域的数据中心AI中具有更广泛的应用潜力

Abstract: Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.

</details>


### [163] [Closing the Modality Gap Aligns Group-Wise Semantics](https://arxiv.org/abs/2601.18525)
*Eleonora Grassucci,Giordano Cicchetti,Emanuele Frasca,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: 论文提出模态间隙（modality gap）对群体级任务（如聚类）影响显著，而传统实例级任务（如检索）影响有限，并提出了减少该间隙的新方法。


<details>
  <summary>Details</summary>
Motivation: CLIP等方法在多模态学习中虽然能有效对齐不同模态的语义，但产生的潜在空间往往只是部分共享，存在结构不匹配的模态间隙。虽然这种间隙对实例级任务影响有限，但作者认为它对群体级任务有重要影响，需要深入研究和解决。

Method: 提出了一种新颖的方法来一致地减少双模态设置中的模态间隙，并可以简单扩展到一般的n模态情况。该方法专门针对减少模态间的结构不匹配问题。

Result: 通过广泛评估发现：减少模态间隙对传统实例级任务仅带来边际或不一致的改进，但对群体级任务（如聚类）有显著提升。这证明了模态间隙在需要语义分组的任务中起着关键作用。

Conclusion: 模态间隙的重要性可能被重新认识，特别是在群体级任务中。这些发现可能重塑我们对模态间隙的理解，强调其在改善需要语义分组的任务性能中的关键作用。

Abstract: In multimodal learning, CLIP has been recognized as the \textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.

</details>


### [164] [Information Hidden in Gradients of Regression with Target Noise](https://arxiv.org/abs/2601.18546)
*Arash Jamshidi,Katsiaryna Haitsiukevich,Kai Puolamäki*

Main category: cs.LG

TL;DR: 提出一种通过梯度方差校准恢复二阶信息的方法：通过注入高斯噪声使目标噪声方差等于批量大小，从而从梯度中准确估计Hessian矩阵。


<details>
  <summary>Details</summary>
Motivation: 在许多现代设置中，只能观察到梯度而无法直接获取二阶信息（如Hessian矩阵或数据协方差），但二阶信息对于优化、诊断和鲁棒性至关重要。

Method: 提出方差校准方法：注入高斯噪声，使总目标噪声方差等于批量大小n，这样经验梯度协方差就能紧密近似Hessian矩阵，即使在远离最优解时也有效。

Result: 在次高斯输入下提供了非渐近算子范数保证；证明没有这种校准，恢复可能失败Ω(1)因子；方法实用（"将目标噪声方差设为n"规则）且鲁棒（方差O(n)足以恢复Σ到比例因子）。

Conclusion: 仅通过梯度就能揭示Hessian矩阵，在线性回归中等于数据协方差Σ；该方法可用于更快的优化预处理、对抗风险估计和分布式系统中的纯梯度训练。

Abstract: Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $Σ$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $Ω(1)$ factor. The proposed method is practical (a "set target-noise variance to $n$" rule) and robust (variance $\mathcal{O}(n)$ suffices to recover $Σ$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.

</details>


### [165] [K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents](https://arxiv.org/abs/2601.18580)
*Vincenzo De Paola,Mirco Mutti,Riccardo Zamboni,Marcello Restelli*

Main category: cs.LG

TL;DR: K-Myriad是一种无监督并行强化学习方法，通过最大化并行策略群体诱导的集体状态熵来促进多样化探索，提供鲁棒的初始化，提高训练效率并发现异构解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统并行强化学习通常使用相同采样分布的多个工作器来加速单个策略训练，这种设计忽视了多样化探索策略的优势，限制了并行化的潜力。

Method: 提出K-Myriad方法，通过最大化并行策略群体诱导的集体状态熵，培养专门的探索策略组合，实现无监督的集体探索。

Result: 在高维连续控制任务和大规模并行化实验中，K-Myriad能够学习到广泛的不同策略，证明了其在集体探索中的有效性。

Conclusion: K-Myriad为强化学习提供了鲁棒的初始化，提高了训练效率并发现了异构解决方案，为新型并行化策略开辟了道路。

Abstract: Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.

</details>


### [166] [Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning](https://arxiv.org/abs/2601.18586)
*Miguel Costa,Arthur Vandervoort,Carolin Schmidt,Morten W. Petersen,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 提出一个结合集成评估模型和强化学习的决策支持框架，用于在不确定性下学习城市交通基础设施的适应性投资路径，以应对气候变化加剧的降雨灾害。


<details>
  <summary>Details</summary>
Motivation: 气候变化预计会加剧降雨等灾害，增加城市交通系统的中断。设计有效的适应策略面临挑战，因为基础设施投资具有长期性、序列性，存在深度不确定性，且涉及复杂的跨部门相互作用。

Method: 提出一个通用的决策支持框架，将集成评估模型（IAM）与强化学习（RL）相结合，在不确定性下学习多十年的适应性投资路径。该框架结合长期气候预测（如IPCC情景路径）和模型，将极端天气驱动因素（如降雨）映射到灾害可能性（如洪水），传播灾害到城市基础设施影响（如交通中断），并评估对服务性能和社会成本的直接和间接后果。嵌入强化学习循环中，学习适应性气候适应政策，权衡投资和维护支出与避免的影响。

Result: 与哥本哈根市政府合作，在2024年至2100年的时间范围内，对内城区的雨洪进行了方法演示。学习到的策略产生了协调的时空路径，并相对于传统优化基线（即不作为和随机行动）提高了鲁棒性。

Conclusion: 该框架展示了可转移到其他灾害和城市的潜力，通过学习适应性气候适应政策，在深度不确定性和复杂相互作用下，为城市基础设施投资提供有效的决策支持。

Abstract: Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities.

</details>


### [167] [LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation](https://arxiv.org/abs/2601.18604)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: LaCoGSEA是一个无监督通路富集分析框架，结合深度表示学习和通路统计，无需表型标签即可进行通路分析


<details>
  <summary>Details</summary>
Motivation: 标准通路富集分析方法（如GSEA）需要预定义表型标签和成对比较，限制了在无监督场景下的应用。现有无监督扩展方法主要捕获线性关系，而深度学习模型虽然能捕获非线性结构，但其解释方法通常依赖通用的可解释AI技术，这些方法并非专为无监督转录组分析中的通路级解释设计。

Method: LaCoGSEA使用自编码器捕获非线性流形，提出全局基因-潜在相关性度量作为差异表达的代理，无需先验标签即可生成密集基因排名。该框架整合了深度表示学习和稳健的通路统计方法。

Result: LaCoGSEA具有三个关键优势：(1)在区分癌症亚型方面比现有无监督基线方法获得更好的聚类性能；(2)与线性降维和基于梯度的XAI方法相比，在更高排名上恢复更广泛的生物学有意义通路；(3)在不同实验方案和数据集大小下保持高稳健性和一致性。

Conclusion: LaCoGSEA在无监督通路富集分析中提供了最先进的性能，填补了无监督转录组分析中通路级解释的空白。

Abstract: Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.
  Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.
  Availability and implementation: https://github.com/willyzzz/LaCoGSEA

</details>


### [168] [Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem](https://arxiv.org/abs/2601.18615)
*Ramiro Valdes Jara,Adam Meyers*

Main category: cs.LG

TL;DR: 提出基于条件扩散模型的数据驱动方法，解决心电图成像逆问题，无需几何建模，实现概率性多重建而非单一确定性估计


<details>
  <summary>Details</summary>
Motivation: 传统心电图成像逆问题方法需要患者特异性几何建模，且通常提供单一确定性解，无法捕捉问题的非唯一性和欠定性本质

Method: 采用条件扩散框架，学习从噪声体表信号到心脏表面电位的概率映射，利用扩散模型的生成特性，实现几何无关的纯数据驱动方法

Result: 在真实心电图成像数据集上评估，相比卷积神经网络、长短时记忆网络和Transformer等确定性基线模型，扩散方法获得更高的重建精度

Conclusion: 扩散模型为无创心脏电生理成像提供了稳健工具，能够更好地处理逆问题的非唯一性，实现概率性多重建

Abstract: This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.

</details>


### [169] [CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling](https://arxiv.org/abs/2601.18620)
*Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Ian Berlot-Attwell,Stéphane Aroca-Ouellette,Kaheer Suleman*

Main category: cs.LG

TL;DR: CASSANDRA：一种神经符号世界建模方法，利用LLM作为知识先验构建轻量级转换模型用于规划，在咖啡店和主题公园模拟器中优于基线方法


<details>
  <summary>Details</summary>
Motivation: 现实世界领域（如商业）具有丰富的语义，可以利用世界知识从有限数据中有效建模复杂的动作效果和因果关系

Method: CASSANDRA整合两个组件：(1) LLM合成的代码建模确定性特征，(2) LLM引导的概率图模型结构学习捕捉随机变量间的因果关系

Result: 在咖啡店模拟器和复杂主题公园商业模拟器中，CASSANDRA在转换预测和规划方面相比基线方法有显著改进

Conclusion: CASSANDRA通过结合LLM的知识先验和轻量级转换模型，能够有效建模复杂世界中的因果关系，提升规划性能

Abstract: Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.

</details>


### [170] [Physics-Informed Uncertainty Enables Reliable AI-driven Design](https://arxiv.org/abs/2601.18638)
*Tingkai Xue,Chin Chun Ooi,Yang Jiang,Luu Trung Pham Duong,Pao-Hsiung Chiu,Weijiang Zhao,Nagarajan Raghavan,My Ha Dao*

Main category: cs.LG

TL;DR: 提出物理信息不确定性作为代理模型预测不确定性的替代方案，通过物理定律违反程度量化不确定性，应用于频率选择表面设计，显著提升优化成功率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的代理辅助优化方法通常缺乏不确定性量化，导致在数据稀疏区域产生错误预测，从而降低优化性能。需要一种能有效量化预测不确定性的方法。

Method: 提出物理信息不确定性范式，将模型预测违反基本物理定律的程度作为预测不确定性的计算廉价且有效的代理。将此方法整合到多保真度不确定性感知优化工作流中，用于设计20-30 GHz范围的复杂频率选择表面。

Result: 将寻找高性能解决方案的成功率从不到10%提高到超过50%，同时与单独使用高保真求解器相比，计算成本降低了一个数量级。

Conclusion: 强调了在高维问题的机器学习驱动逆向设计中纳入不确定性量化的必要性，确立了物理信息不确定性作为物理系统代理模型中量化不确定性的可行替代方案，为能够高效稳健探索和评估候选设计的自主科学发现系统奠定了基础。

Abstract: Inverse design is a central goal in much of science and engineering, including frequency-selective surfaces (FSS) that are critical to microelectronics for telecommunications and optical metamaterials. Traditional surrogate-assisted optimization methods using deep learning can accelerate the design process but do not usually incorporate uncertainty quantification, leading to poorer optimization performance due to erroneous predictions in data-sparse regions. Here, we introduce and validate a fundamentally different paradigm of Physics-Informed Uncertainty, where the degree to which a model's prediction violates fundamental physical laws serves as a computationally-cheap and effective proxy for predictive uncertainty. By integrating physics-informed uncertainty into a multi-fidelity uncertainty-aware optimization workflow to design complex frequency-selective surfaces within the 20 - 30 GHz range, we increase the success rate of finding performant solutions from less than 10% to over 50%, while simultaneously reducing computational cost by an order of magnitude compared to the sole use of a high-fidelity solver. These results highlight the necessity of incorporating uncertainty quantification in machine-learning-driven inverse design for high-dimensional problems, and establish physics-informed uncertainty as a viable alternative to quantifying uncertainty in surrogate models for physical systems, thereby setting the stage for autonomous scientific discovery systems that can efficiently and robustly explore and evaluate candidate designs.

</details>


### [171] [TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning](https://arxiv.org/abs/2601.18640)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: TwinPurify是一个基于Barlow Twins自监督学习的表示学习框架，用于从批量转录组数据中提取肿瘤特异性信号，通过利用同一队列中的相邻正常组织作为背景指导，无需外部参考即可分离肿瘤信号。


<details>
  <summary>Details</summary>
Motivation: 大规模患者队列研究仍依赖批量转录组数据，但肿瘤纯度变异会掩盖肿瘤内在转录信号并限制下游发现。现有的去卷积方法在合成混合物上表现良好，但无法泛化到真实患者队列，因为未建模的生物和技术变异。

Method: 采用Barlow Twins自监督目标，学习连续的高维肿瘤嵌入表示，利用同一队列中的相邻正常组织作为"背景"指导，无需外部参考即可分离肿瘤特异性信号。

Result: 在多个大型癌症队列（RNA-seq和微阵列平台）的基准测试中，TwinPurify在恢复肿瘤内在和免疫信号方面优于传统表示学习方法（如自编码器）。纯化的嵌入改进了分子亚型和分级分类，增强了生存模型一致性，并揭示了更有生物学意义的通路活性。

Conclusion: TwinPurify通过提供可转移的批量转录组去污染框架，扩展了现有临床数据集在分子发现中的实用性，代表了去卷积范式的根本性转变。

Abstract: Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.
  Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as "background" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.
  Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.

</details>


### [172] [FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning](https://arxiv.org/abs/2601.18650)
*Liheng Yu,Zhe Zhao,Yuxuan Wang,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 该论文首次研究长尾分布下的机器遗忘问题，提出FaLW方法通过实例级动态损失重加权解决现有方法在长尾数据上的异构遗忘偏差和倾斜遗忘偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘研究主要评估平衡遗忘集，忽视了现实世界中遗忘数据（如用户活动记录）通常遵循长尾分布的情况。这种研究空白可能导致现有方法在真实场景中表现不佳。

Method: 提出FaLW方法：一种即插即用的实例级动态损失重加权方法。通过比较每个样本的预测概率与同类别未见数据的分布来评估其遗忘状态，使用遗忘感知的重加权方案，通过平衡因子调节每个样本的遗忘强度。

Result: 大量实验证明FaLW在长尾分布设置下实现了优越的性能，有效解决了异构遗忘偏差和倾斜遗忘偏差问题。

Conclusion: 该研究填补了长尾分布下机器遗忘的研究空白，提出的FaLW方法能有效处理现实世界中不平衡的遗忘数据，为数据隐私法规如"被遗忘权"提供了更实用的解决方案。

Abstract: Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \textit{Heterogeneous Unlearning Deviation} and \textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \textbf{Supplementary Material}.

</details>


### [173] [A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.18672)
*Spyros Rigas,Thanasis Papaioannou,Panagiotis Trakadas,Georgios Alexandridis*

Main category: cs.LG

TL;DR: 提出基于曲率的重要性密度函数框架，用于KAN网络网格自适应，相比传统输入密度方法显著提升精度


<details>
  <summary>Details</summary>
Motivation: 现有KAN网格自适应策略仅依赖输入数据密度，未考虑目标函数的几何复杂性或训练过程中的计算指标，限制了性能提升

Method: 提出广义框架，将节点分配视为由重要性密度函数（IDFs）控制的密度估计任务，引入基于曲率的自适应策略，让训练动态决定网格分辨率

Result: 在合成函数拟合、Feynman数据集回归和Helmholtz PDE基准测试中显著优于标准输入基线，平均相对误差分别降低25.3%、9.4%和23.3%

Conclusion: 基于曲率的自适应是KAN训练中稳健且计算高效的替代方案，Wilcoxon符号秩检验证实了统计显著性

Abstract: Kolmogorov-Arnold Networks (KANs) have recently demonstrated promising potential in scientific machine learning, partly due to their capacity for grid adaptation during training. However, existing adaptation strategies rely solely on input data density, failing to account for the geometric complexity of the target function or metrics calculated during network training. In this work, we propose a generalized framework that treats knot allocation as a density estimation task governed by Importance Density Functions (IDFs), allowing training dynamics to determine grid resolution. We introduce a curvature-based adaptation strategy and evaluate it across synthetic function fitting, regression on a subset of the Feynman dataset and different instances of the Helmholtz PDE, demonstrating that it significantly outperforms the standard input-based baseline. Specifically, our method yields average relative error reductions of 25.3% on synthetic functions, 9.4% on the Feynman dataset, and 23.3% on the PDE benchmark. Statistical significance is confirmed via Wilcoxon signed-rank tests, establishing curvature-based adaptation as a robust and computationally efficient alternative for KAN training.

</details>


### [174] [Learning temporal embeddings from electronic health records of chronic kidney disease patients](https://arxiv.org/abs/2601.18675)
*Aditya Kumar,Mario A. Cypko,Oliver Amft*

Main category: cs.LG

TL;DR: 研究验证了在电子健康记录上训练的时序嵌入模型能够学习到具有临床意义的表示，同时保持预测性能，其中时间感知LSTM（T-LSTM）在嵌入质量和下游任务表现上均优于其他架构。


<details>
  <summary>Details</summary>
Motivation: 模型引导的医学需要能够捕捉疾病动态、保持透明且任务无关的表示，而大多数临床预测模型仅针对单一任务优化。表示学习有助于学习能够泛化到下游任务的嵌入，循环架构适合建模临床观察数据中的时序结构。

Method: 使用MIMIC-IV数据集，研究慢性肾病（CKD）患者，比较三种循环架构：普通LSTM、注意力增强LSTM和时间感知LSTM（T-LSTM）。所有模型都训练为嵌入模型和直接端到端预测器。通过CKD阶段聚类和ICU内死亡率预测评估嵌入质量。

Result: T-LSTM产生更具结构化的嵌入，Davies-Bouldin指数更低（DBI=9.91），CKD阶段分类准确率更高（0.74），优于普通LSTM（DBI=15.85，准确率0.63）和注意力增强LSTM（DBI=20.72，准确率0.67）。在ICU死亡率预测中，嵌入模型始终优于端到端预测器，准确率从0.72-0.75提升到0.82-0.83。

Conclusion: 时序嵌入模型能够学习具有临床意义的表示而不损害预测性能，T-LSTM在嵌入质量上表现最佳。学习嵌入作为中间步骤比直接端到端学习更有效，为模型引导医学提供了有前景的方向。

Abstract: We investigate whether temporal embedding models trained on longitudinal electronic health records can learn clinically meaningful representations without compromising predictive performance, and how architectural choices affect embedding quality. Model-guided medicine requires representations that capture disease dynamics while remaining transparent and task agnostic, whereas most clinical prediction models are optimised for a single task. Representation learning facilitates learning embeddings that generalise across downstream tasks, and recurrent architectures are well-suited for modelling temporal structure in observational clinical data. Using the MIMIC-IV dataset, we study patients with chronic kidney disease (CKD) and compare three recurrent architectures: a vanilla LSTM, an attention-augmented LSTM, and a time-aware LSTM (T-LSTM). All models are trained both as embedding models and as direct end-to-end predictors. Embedding quality is evaluated via CKD stage clustering and in-ICU mortality prediction. The T-LSTM produces more structured embeddings, achieving a lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage classification accuracy (0.74) than the vanilla LSTM (DBI = 15.85, accuracy = 0.63) and attention-augmented LSTM (DBI = 20.72, accuracy = 0.67). For in-ICU mortality prediction, embedding models consistently outperform end-to-end predictors, improving accuracy from 0.72-0.75 to 0.82-0.83, which indicates that learning embeddings as an intermediate step is more effective than direct end-to-end learning.

</details>


### [175] [Quasi Monte Carlo methods enable extremely low-dimensional deep generative models](https://arxiv.org/abs/2601.18676)
*Miles Martinez,Alex H. Williams*

Main category: cs.LG

TL;DR: QLVMs是一种专门用于寻找高维数据集极低维可解释嵌入的深度生成模型，通过拟蒙特卡洛积分直接近似边缘似然，在1-3维潜空间中优于传统VAE和IWAE。


<details>
  <summary>Details</summary>
Motivation: 传统变分自编码器(VAE)和重要性加权自编码器(IWAE)虽然能学习潜变量表示，但在极低维情况下难以获得可解释的嵌入，且变分下界可能不够紧致。需要一种专门针对低维可解释嵌入的深度生成模型方法。

Method: 提出拟蒙特卡洛潜变量模型(QLVMs)，不依赖学习编码器和变分下界，而是通过随机拟蒙特卡洛积分直接近似边缘似然。这种方法在1-3维潜空间中特别有效，虽然计算密集但能获得更准确的低维嵌入。

Result: 在多个数据集上的实验表明，QLVMs在匹配潜维度的条件下，一致优于传统VAE和IWAE。生成的嵌入支持透明可视化、非参数密度估计、聚类和测地线路径计算等后验分析。

Conclusion: QLVMs为优先考虑可解释性和潜空间分析的应用提供了有吸引力的解决方案，虽然在复杂数据集上难以生成精细细节且计算密集，但在极低维嵌入任务中表现出色。

Abstract: This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.

</details>


### [176] [Counterfactual Explanations on Robust Perceptual Geodesics](https://arxiv.org/abs/2601.18678)
*Eslam Zaher,Maciej Trzaskowski,Quan Nguyen,Fred Roosta*

Main category: cs.LG

TL;DR: PCG提出了一种基于感知黎曼度量的反事实解释方法，通过追踪测地线生成语义有效的反事实，解决了现有方法中对抗性扰动和语义漂移的问题。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法存在距离度量选择模糊的问题，导致生成的扰动要么是语义上有意义的，要么是对抗性的。现有方法采用平坦或不对齐的几何结构，导致离流形伪影、语义漂移或对抗性崩溃。

Method: 提出了感知反事实测地线(PCG)方法，通过在鲁棒视觉特征诱导的感知黎曼度量下追踪测地线来构建反事实。这种几何结构与人类感知对齐，惩罚脆弱方向，实现平滑、在流形上、语义有效的转换。

Result: 在三个视觉数据集上的实验表明，PCG优于基线方法，并揭示了在标准度量下隐藏的失败模式。

Conclusion: PCG通过引入感知对齐的黎曼几何，解决了反事实解释中的距离度量模糊问题，能够生成语义有效的反事实解释，揭示了传统方法隐藏的模型失败模式。

Abstract: Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.

</details>


### [177] [Explainability Methods for Hardware Trojan Detection: A Systematic Comparison](https://arxiv.org/abs/2601.18696)
*Paul Whitten,Francis Wolff,Chris Papachristou*

Main category: cs.LG

TL;DR: 该研究比较了硬件木马检测中三种可解释性方法：基于属性的分析、基于案例的推理和模型无关特征归因，发现前两种方法在领域对齐和可验证性方面优于通用特征归因方法。


<details>
  <summary>Details</summary>
Motivation: 硬件木马检测不仅需要准确识别，还需要为安全工程师提供可解释的解释，以便验证和采取行动。现有方法缺乏领域特定的可解释性，难以让工程师信任和验证机器学习预测结果。

Method: 比较了三种可解释性方法：1）基于31个电路特定特征的领域感知属性分析（扇入模式、触发器距离、I/O连接性）；2）使用k近邻的基于案例推理；3）模型无关特征归因（LIME、SHAP、梯度方法）。使用XGBoost分类器在Trust-Hub基准上进行评估。

Result: XGBoost在11,392个测试样本上达到46.15%精确率和52.17%召回率，相比先前工作（Hasegawa等：5.13%）精确率提升9倍，误报率从5.6%降至0.25%。基于属性的分析提供电路概念解释，基于案例的推理达到97.4%预测与训练示例对应性，LIME和SHAP特征归因相关性高但缺乏电路级上下文。

Conclusion: 基于属性的分析和基于案例的推理在领域对齐和基于先例的可解释性方面优于通用特征排名方法，对于需要工程师验证机器学习预测的XAI部署具有重要意义。梯度归因比SHAP快481倍但提供类似的领域不透明见解。

Abstract: Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient).
  Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like "high fanin complexity near outputs indicates potential triggers." Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation.
  XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights.
  This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.

</details>


### [178] [Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning](https://arxiv.org/abs/2601.18699)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: 该论文对大型语言模型在连续微调中的灾难性遗忘现象进行了机制性分析，识别了三个主要驱动机制：注意力权重梯度干扰、中间层表征漂移和损失景观平坦化。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型通过预训练和微调在各种任务上表现出色，但在连续任务上进行顺序微调会导致灾难性遗忘，新获得的知识会干扰先前学习的能力。尽管这种现象被广泛观察到，但其机制理解仍然有限。

Method: 通过对多个模型规模（109B到400B总参数）和任务序列进行系统实验，分析灾难性遗忘的机制。识别了三个主要机制：注意力权重梯度干扰、中间层表征漂移和损失景观平坦化。

Result: 遗忘严重程度与任务相似性（Pearson r = 0.87）和梯度对齐指标密切相关。约15-23%的注意力头在微调过程中受到严重干扰，较低层表现出更大的易感性。

Conclusion: 这些发现为持续学习系统中开发有针对性的缓解策略建立了机制基础，提供了对灾难性遗忘现象的深入理解。

Abstract: Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.

</details>


### [179] [From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic](https://arxiv.org/abs/2601.18702)
*Hansheng Ren*

Main category: cs.LG

TL;DR: 论文挑战当前深度学习优先计算吞吐量而非数值精度的范式，提出精确性假说：通用智能需要任意精度算术计算基础，并引入基于有理数算术的Halo架构来减少逻辑不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习范式过于关注计算吞吐量而忽视数值精度，假设智能来自大规模统计相关性。作者认为这种近似计算导致大语言模型出现"幻觉"和逻辑不一致，这些是浮点近似误差在深层组合函数中累积的结果。

Method: 提出精确性假说，认为通用智能需要任意精度算术计算基础。引入Halo架构，采用有理数算术（ℚ）作为计算范式，并设计新型精确推理单元（EIU）来支持该架构。

Result: 在Huginn-0125原型上的实证验证显示，当600B参数的BF16基线模型在混沌系统中崩溃时，Halo架构能够无限期保持零数值发散，证明了精确算术在减少逻辑不确定性方面的有效性。

Conclusion: 精确算术是减少System 2 AGI中逻辑不确定性的先决条件，为通用智能系统提供了更可靠的计算基础，挑战了当前深度学习对近似计算的依赖。

Abstract: Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the "hallucinations" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.

</details>


### [180] [SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model](https://arxiv.org/abs/2601.18707)
*Jan Hagnberger,Mathias Niepert*

Main category: cs.LG

TL;DR: SMART是一种基于神经网络的替代模型，仅使用几何点云表示（无需仿真网格）即可在任意查询位置预测物理量，性能与依赖网格的方法相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于网格的替代模型虽然精度较高，但为新几何生成仿真网格计算成本高昂；而无网格方法通常误差较大。需要一种既高效又准确的替代方案。

Method: 提出SMART模型：将几何和仿真参数编码到共享潜在空间，捕捉结构和参数特征；物理解码器通过跨层交互关注编码器的中间潜在表示，将空间查询映射到物理量。

Result: 大量实验表明，SMART与依赖仿真网格输入的现有方法竞争，且经常超越它们，展示了其在工业级仿真中的能力。

Conclusion: SMART提供了一种无需仿真网格的高效准确替代模型，在保持精度的同时显著降低了计算成本，适用于复杂几何的物理仿真。

Abstract: Machine learning-based surrogate models have emerged as more efficient alternatives to numerical solvers for physical simulations over complex geometries, such as car bodies. Many existing models incorporate the simulation mesh as an additional input, thereby reducing prediction errors. However, generating a simulation mesh for new geometries is computationally costly. In contrast, mesh-free methods, which do not rely on the simulation mesh, typically incur higher errors. Motivated by these considerations, we introduce SMART, a neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry, without requiring access to the simulation mesh. The geometry and simulation parameters are encoded into a shared latent space that captures both structural and parametric characteristics of the physical field. A physics decoder then attends to the encoder's intermediate latent representations to map spatial queries to physical quantities. Through this cross-layer interaction, the model jointly updates latent geometric features and the evolving physical field. Extensive experiments show that SMART is competitive with and often outperforms existing methods that rely on the simulation mesh as input, demonstrating its capabilities for industry-level simulations.

</details>


### [181] [Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data](https://arxiv.org/abs/2601.18728)
*Willem Diepeveen,Oscar Leong*

Main category: cs.LG

TL;DR: Riemannian AmbientFlow：从噪声观测中同时学习概率生成模型和底层非线性数据流形的框架


<details>
  <summary>Details</summary>
Motivation: 在许多科学和成像应用中，无法获得干净样本，只能观测到噪声或线性损坏的测量值。此外，数据中的潜在结构（如流形几何）对于下游科学分析很重要，需要提取。

Method: 基于AmbientFlow的变分推断框架，结合由归一化流诱导的数据驱动黎曼几何，通过回拉度量和黎曼自编码器提取流形结构。

Result: 在适当的几何正则化和测量条件下，学习到的模型能够以可控误差恢复底层数据分布，并获得平滑、双Lipschitz的流形参数化。平滑解码器可作为逆问题的原则性生成先验，具有恢复保证。

Conclusion: 该方法在低维合成流形和MNIST数据集上进行了实证验证，为从损坏观测中同时学习生成模型和底层流形结构提供了理论保证的框架。

Abstract: Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.

</details>


### [182] [Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models](https://arxiv.org/abs/2601.18734)
*Siyan Zhao,Zhihui Xie,Mengchen Liu,Jing Huang,Guan Pang,Feiyu Chen,Aditya Grover*

Main category: cs.LG

TL;DR: 提出On-Policy Self-Distillation (OPSD)框架，让单个LLM同时扮演教师和学生角色，通过在不同上下文条件下进行知识蒸馏，提高数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法存在两个问题：1）在线蒸馏需要单独的大型教师模型，成本高；2）没有充分利用推理数据集中可用的真实解决方案。受启发于"足够强大的LLM能够解释外部特权推理轨迹并教导其较弱版本"的直觉。

Method: OPSD框架中，单个模型同时作为教师和学生：教师策略基于特权信息（如已验证的推理轨迹），学生策略仅看到问题；训练时在学生自己的轨迹上最小化两个分布之间的每令牌差异。

Result: 在多个数学推理基准测试中，相比GRPO等强化学习方法实现了4-8倍的令牌效率，性能优于离线蒸馏方法。

Conclusion: OPSD提供了一种高效的知识蒸馏框架，无需单独教师模型，能充分利用数据集中的真实解决方案，显著提高推理能力训练效率。

Abstract: Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.

</details>


### [183] [Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift](https://arxiv.org/abs/2601.18736)
*Jake Lyon,Ehsan Saeedizade,Shamik Sengupta*

Main category: cs.LG

TL;DR: 本研究评估了四种监督学习模型在IoT恶意软件检测与分类中的表现，发现树模型在数据有限时表现优异，但随时间推移性能会下降。


<details>
  <summary>Details</summary>
Motivation: IoT设备在智能城市、交通和工业系统中快速扩张，但其计算资源有限、物理防护薄弱，且部署在异构动态网络中，容易成为网络攻击和恶意软件的目标。机器学习为自动化恶意软件检测提供了有前景的方法，但实际部署需要既有效又轻量级的模型。

Method: 使用IoT-23数据集，研究四种监督学习模型（随机森林、LightGBM、逻辑回归和多层感知器）在恶意软件检测和分类中的效果。评估模型在二分类和多分类任务中的性能，分析对训练数据量的敏感性，以及时间鲁棒性以模拟不断演变的威胁环境。

Result: 树基模型（随机森林、LightGBM）即使在有限训练数据下也能实现高准确率和良好泛化能力。然而，随着恶意软件多样性增加，所有模型的性能都会随时间推移而下降。

Conclusion: 研究强调了自适应、资源高效的机器学习模型对于在真实环境中保护IoT系统的重要性。树基模型在资源受限的IoT环境中表现出色，但需要持续更新以适应不断变化的威胁环境。

Abstract: The rapid expansion of the Internet of Things (IoT) in domains such as smart cities, transportation, and industrial systems has heightened the urgency of addressing their security vulnerabilities. IoT devices often operate under limited computational resources, lack robust physical safeguards, and are deployed in heterogeneous and dynamic networks, making them prime targets for cyberattacks and malware applications. Machine learning (ML) offers a promising approach to automated malware detection and classification, but practical deployment requires models that are both effective and lightweight. The goal of this study is to investigate the effectiveness of four supervised learning models (Random Forest, LightGBM, Logistic Regression, and a Multi-Layer Perceptron) for malware detection and classification using the IoT-23 dataset. We evaluate model performance in both binary and multiclass classification tasks, assess sensitivity to training data volume, and analyze temporal robustness to simulate deployment in evolving threat landscapes. Our results show that tree-based models achieve high accuracy and generalization, even with limited training data, while performance deteriorates over time as malware diversity increases. These findings underscore the importance of adaptive, resource-efficient ML models for securing IoT systems in real-world environments.

</details>


### [184] [Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback](https://arxiv.org/abs/2601.18751)
*Seyed Amir Hosseini,Maryam Abdolali,Amirhosein Tavakkoli,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.LG

TL;DR: TriTrust-PBRL (TTP) 提出一个统一框架，通过联合学习共享奖励模型和专家特定的信任参数来处理异构偏好数据，能自动识别并反转对抗性偏好，而非简单丢弃。


<details>
  <summary>Details</summary>
Motivation: 现实世界的偏好数据通常来自可靠性各异的异构标注者，包括准确、嘈杂和系统性对抗的标注者。现有PBRL方法要么平等对待所有反馈，要么尝试过滤不可靠来源，但在面对系统性提供错误偏好的对抗性标注者时都会失败。

Method: TriTrust-PBRL (TTP) 框架联合学习共享奖励模型和专家特定的信任参数。关键洞察是信任参数在基于梯度的优化中自然演变为正值（信任）、接近零（忽略）或负值（反转），使模型能自动反转对抗性偏好并恢复有用信号，而非仅仅丢弃损坏的反馈。

Result: 在四个不同领域（MetaWorld操作任务和DM Control运动任务）的各种损坏场景下评估，TTP实现了最先进的鲁棒性，在对抗性损坏下保持接近oracle的性能，而标准PBRL方法则灾难性失败。TTP成功从包含可靠和对抗性标注者的混合专家池中学习，且仅需专家标识索引，无需额外专家特征。

Conclusion: TTP提供了一种统一框架，能有效处理来自异构标注者的偏好数据，包括对抗性标注者，通过自动学习信任参数来反转而非丢弃损坏的反馈，实现了强大的鲁棒性，并与现有PBRL流程无缝集成。

Abstract: Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.

</details>


### [185] [HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs](https://arxiv.org/abs/2601.18753)
*Xinyue Zeng,Junhong Lin,Yujun Yan,Feng Guo,Liang Shi,Jun Wu,Dawei Zhou*

Main category: cs.LG

TL;DR: 该论文提出了Hallucination Risk Bound理论框架，将LLM幻觉风险分解为数据驱动和推理驱动两部分，并基于此开发了HalluGuard检测方法，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗、法律、科学发现等高风险领域的可靠性常因幻觉而受损。现有检测方法通常只针对单一幻觉来源，且依赖任务特定启发式方法，难以泛化到复杂场景。

Method: 1) 提出Hallucination Risk Bound理论框架，将幻觉风险形式化分解为数据驱动（训练时失配）和推理驱动（推理时不稳定性）两部分；2) 基于此开发HalluGuard方法，利用NTK（神经正切核）的诱导几何和捕获表示来联合识别两种幻觉。

Result: 在10个多样化基准测试、11个竞争性基线和9个流行LLM骨干网络上评估HalluGuard，在检测多种形式LLM幻觉方面始终达到最先进的性能。

Conclusion: 该研究提供了一个统一的理论框架来理解LLM幻觉的起源和演化，并开发了有效的检测方法，为提升LLM在高风险应用中的可靠性提供了理论基础和实用工具。

Abstract: The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.

</details>


### [186] [Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values](https://arxiv.org/abs/2601.18760)
*Henry Bell,Lara Neubauer da Costa Schertel,Bochu Ding,Brandon Fain*

Main category: cs.LG

TL;DR: 提出Grounded Constitutional AI (GCAI)框架，通过结合用户对AI的普遍期望和交互时偏好，生成更具代表性、道德基础更扎实的宪法原则。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐框架中，宪法原则的确定缺乏广泛利益相关者的公平参与，需要建立能够代表用户普遍期望和具体交互偏好的宪法生成方法。

Method: 扩展Inverse Constitutional AI (ICAI)方法，利用人类偏好标注数据中的"理由"生成上下文原则，并结合用户关于AI"价值观"的陈述生成普遍原则，形成统一的宪法框架。

Result: 人类评估显示，GCAI生成的宪法相比ICAI生成的宪法更受青睐，被认为更适合个人使用和广泛治理AI行为，且在道德基础、连贯性和多元性方面表现更优。

Conclusion: GCAI框架能够生成更具代表性、道德基础更扎实的宪法原则，为LLM对齐提供了更公平、更全面的宪法制定方法。

Abstract: A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.

</details>


### [187] [PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation](https://arxiv.org/abs/2601.18777)
*Abhishek Divekar,Anirban Majumder*

Main category: cs.LG

TL;DR: 提出PRECISE框架，结合少量人工标注与LLM判断来评估搜索/RAG系统质量，显著减少标注需求并校正LLM偏见


<details>
  <summary>Details</summary>
Motivation: 传统搜索、排序和RAG系统评估需要大量人工相关性标注，而LLM作为自动评估器存在偏见，无法直接用于指标估计

Method: 扩展预测驱动推理(PPI)的统计框架，结合少量人工标注查询(100个)和大量未标注样本(10,000个)，重新制定指标集成空间，将计算复杂度从O(2^|C|)降低到O(2^K)

Result: 在多个检索数据集上实验显示，该方法减少了关键业务指标Precision@K的估计方差，在低资源设置下有效校正LLM偏见

Conclusion: PRECISE框架通过结合少量人工标注与LLM判断，显著降低了评估系统的标注需求，同时提供可靠指标估计并校正LLM偏见

Abstract: Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.

</details>


### [188] [Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability](https://arxiv.org/abs/2601.18778)
*Shobhita Sundaram,John Quan,Ariel Kwiatkowski,Kartik Ahuja,Yann Ollivier,Julia Kempe*

Main category: cs.LG

TL;DR: SOAR：一个通过元强化学习实现LLM自我改进的框架，教师模型生成合成问题训练学生模型，基于学生进步而非内在奖励来构建课程，能在初始成功率极低（0/128）的数学问题上实现突破性学习。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在初始成功率低的困难数据集上微调时会陷入学习停滞，因为缺乏训练信号。研究核心问题是：预训练LLM能否利用潜在知识为自己无法解决的问题生成自动化课程？

Method: SOAR框架：使用教师模型为学生模型生成合成问题，教师根据学生在困难问题子集上的进步获得奖励。关键创新是课程基于实际学生进步而非内在代理奖励，实现双层元强化学习。

Result: 在数学基准最困难子集（初始成功率0/128）上：1）实现了双层元强化学习，解锁了稀疏二元奖励下的学习能力；2）基于进步的奖励优于先前内在奖励方案，避免不稳定性和多样性崩溃；3）问题结构质量和明确性比解决方案正确性对学习进步更重要。

Conclusion: 模型生成有用"垫脚石"的能力不需要预先具备解决困难问题的能力，这为无需额外标注数据就能突破推理停滞提供了原则性路径，展示了预训练模型利用潜在知识自我改进的潜力。

Abstract: Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.

</details>


### [189] [POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration](https://arxiv.org/abs/2601.18779)
*Yuxiao Qu,Amrith Setlur,Virginia Smith,Ruslan Salakhutdinov,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出POPE方法，利用特权信息（如人类解答）引导强化学习在困难问题上的探索，解决传统RL方法在硬问题上探索不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在提升大语言模型推理能力时，在困难问题上常常无法获得任何正确轨迹，导致零奖励和无效学习信号。传统探索方法（如熵奖励、重要性采样裁剪）无法解决此问题，而混合简单和困难问题训练反而会产生"射线干扰"现象，阻碍进步。

Method: POPE（特权在线探索）方法：利用人类或其他oracle解答作为特权信息，在困难问题前添加oracle解答的前缀，引导RL获得非零奖励。关键是通过指令跟随和推理的协同作用，使习得的行为能迁移回原始未引导的问题。

Result: POPE显著扩展了可解问题的范围，在具有挑战性的推理基准测试中大幅提升了性能表现。

Conclusion: 通过利用特权信息引导探索，POPE有效解决了RL在困难推理问题上的探索瓶颈，为提升大语言模型的推理能力提供了新途径。

Abstract: Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.

</details>


### [190] [Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes](https://arxiv.org/abs/2601.18795)
*Amrith Setlur,Zijian Wang,Andrew Cohen,Paria Rashidinejad,Sang Michael Xie*

Main category: cs.LG

TL;DR: PrefixRL：通过重用离策略轨迹的前缀来提升强化学习效率，避免离策略不稳定性，在困难推理问题上实现2倍训练加速和3倍最终奖励提升


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在困难推理问题上效率低下，因为正确策略轨迹稀少、策略梯度消失、学习停滞。需要重用旧采样计算（来自先前推理或RL训练）来提升效率，但标准离策略方法会导致RL优化不稳定。

Method: 提出PrefixRL方法：基于成功离策略轨迹的前缀进行条件化，然后运行在策略RL来完成剩余部分，避免离策略不稳定性。通过调整前缀长度来调节问题难度，提升困难问题的学习信号。

Result: 在困难推理问题上，PrefixRL达到相同训练奖励的速度比最强基线（在离策略数据上进行SFT然后RL）快2倍，最终奖励提升3倍。发现后向泛化现象：仅在带前缀问题上训练能泛化到无前缀性能，且学习策略常与前缀不同。

Conclusion: PrefixRL通过重用离策略轨迹前缀有效提升RL效率，避免离策略不稳定性，在困难推理任务上显著优于现有方法，且具有灵活性和泛化能力。

Abstract: Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [191] [Data-Driven Information-Theoretic Causal Bounds under Unmeasured Confounding](https://arxiv.org/abs/2601.17160)
*Yonghan Jung,Bogyeong Kang*

Main category: stat.ML

TL;DR: 提出信息理论框架，在未测量混杂下实现因果效应的尖锐部分识别，无需外部参数、辅助变量或完整结构模型


<details>
  <summary>Details</summary>
Motivation: 现有方法存在四个主要限制：依赖限制性假设（如结果有界或离散）、需要外部输入（如工具变量或敏感性参数）、需要完整结构因果模型、只关注总体平均而忽略协变量条件效应

Method: 建立信息理论的数据驱动散度界限，证明观测分布与干预分布之间的f-散度仅由倾向得分函数上界，开发满足Neyman正交性的半参数估计器

Result: 能够在无需外部敏感性参数、辅助变量、完整结构规范或结果有界假设的情况下，直接从观测数据中尖锐识别条件因果效应

Conclusion: 该框架克服了现有方法的四个限制，提供紧致有效的因果界限，适用于广泛的数据生成过程，并通过模拟和实际数据应用验证了有效性

Abstract: We develop a data-driven information-theoretic framework for sharp partial identification of causal effects under unmeasured confounding. Existing approaches often rely on restrictive assumptions, such as bounded or discrete outcomes; require external inputs (for example, instrumental variables, proxies, or user-specified sensitivity parameters); necessitate full structural causal model specifications; or focus solely on population-level averages while neglecting covariate-conditional treatment effects. We overcome all four limitations simultaneously by establishing novel information-theoretic, data-driven divergence bounds. Our key theoretical contribution shows that the f-divergence between the observational distribution P(Y | A = a, X = x) and the interventional distribution P(Y | do(A = a), X = x) is upper bounded by a function of the propensity score alone. This result enables sharp partial identification of conditional causal effects directly from observational data, without requiring external sensitivity parameters, auxiliary variables, full structural specifications, or outcome boundedness assumptions. For practical implementation, we develop a semiparametric estimator satisfying Neyman orthogonality (Chernozhukov et al., 2018), which ensures square-root-n consistent inference even when nuisance functions are estimated using flexible machine learning methods. Simulation studies and real-world data applications, implemented in the GitHub repository (https://github.com/yonghanjung/Information-Theretic-Bounds), demonstrate that our framework provides tight and valid causal bounds across a wide range of data-generating processes.

</details>


### [192] [Error Analysis of Bayesian Inverse Problems with Generative Priors](https://arxiv.org/abs/2601.17374)
*Bamdad Hosseini,Ziqi Huang*

Main category: stat.ML

TL;DR: 论文分析了使用生成模型作为先验解决逆问题的方法，给出了最小Wasserstein-2生成模型先验的定量误差界，并展示了后验误差与先验误差在Wasserstein-1距离下的收敛率关系。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法在逆问题求解中日益流行，特别是通过训练生成模型学习特定问题的先验分布。然而，需要对这些方法的理论性能进行分析，以理解生成先验对后验估计的影响。

Method: 使用最小Wasserstein-2生成模型作为先验，分析其在逆问题中的应用。通过理论分析建立定量误差界，证明在某些假设下，由生成先验引起的后验误差会继承先验在Wasserstein-1距离下的收敛率。

Result: 理论分析表明，生成先验引起的后验误差与先验误差具有相同的收敛率。数值实验验证了误差分析的关键方面，包括在椭圆PDE逆问题中，使用生成先验对非平稳场进行建模的效果。

Conclusion: 该研究为使用生成模型作为先验的逆问题求解方法提供了理论保证，证明了后验误差的收敛性质，并通过数值实验验证了理论分析的有效性，特别是在复杂PDE逆问题中的应用前景。

Abstract: Data-driven methods for the solution of inverse problems have become widely popular in recent years thanks to the rise of machine learning techniques. A popular approach concerns the training of a generative model on additional data to learn a bespoke prior for the problem at hand. In this article we present an analysis for such problems by presenting quantitative error bounds for minimum Wasserstein-2 generative models for the prior. We show that under some assumptions, the error in the posterior due to the generative prior will inherit the same rate as the prior with respect to the Wasserstein-1 distance. We further present numerical experiments that verify that aspects of our error analysis manifests in some benchmarks followed by an elliptic PDE inverse problem where a generative prior is used to model a non-stationary field.

</details>


### [193] ["Rebuilding" Statistics in the Age of AI: A Town Hall Discussion on Culture, Infrastructure, and Training](https://arxiv.org/abs/2601.17510)
*David L. Donoho,Jian Kang,Xihong Lin,Bhramar Mukherjee,Dan Nettleton,Rebecca Nugent,Abel Rodriguez,Eric P. Xing,Tian Zheng,Hongtu Zhu*

Main category: stat.ML

TL;DR: 2024 JSM会议圆桌讨论记录，探讨统计学在AI时代的发展方向，涵盖学科文化、数据工作、现代建模、AI应用培训及与AI利益相关者合作等五个核心议题。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能、基础模型、大规模经验建模和数据密集型基础设施的快速发展，统计学领域面临重大变革。本次圆桌会议旨在收集统计学家对学科如何适应AI时代的坦诚观点，促进学科反思和持续对话。

Method: 采用开放式小组讨论和广泛观众问答的形式，而非正式演讲或准备陈述。会议记录以最小编辑干预的方式保存专家和观众的交流内容，围绕五个反复出现的问题组织对话。

Result: 提供了2024年联合统计会议圆桌讨论的完整原始记录，涵盖学科文化与实践、数据管理与"数据工作"、与现代经验建模的互动、大规模AI应用培训、以及与关键AI利益相关者合作等五个核心议题的深入讨论。

Conclusion: 通过保存这次讨论的档案记录，该预印本旨在支持透明度、社区反思和关于统计学在数据和AI为中心的未来中不断演变角色的持续对话，为统计学在AI时代的定位和发展提供重要参考。

Abstract: This article presents the full, original record of the 2024 Joint Statistical Meetings (JSM) town hall, "Statistics in the Age of AI," which convened leading statisticians to discuss how the field is evolving in response to advances in artificial intelligence, foundation models, large-scale empirical modeling, and data-intensive infrastructures. The town hall was structured around open panel discussion and extensive audience Q&A, with the aim of eliciting candid, experience-driven perspectives rather than formal presentations or prepared statements. This document preserves the extended exchanges among panelists and audience members, with minimal editorial intervention, and organizes the conversation around five recurring questions concerning disciplinary culture and practices, data curation and "data work," engagement with modern empirical modeling, training for large-scale AI applications, and partnerships with key AI stakeholders. By providing an archival record of this discussion, the preprint aims to support transparency, community reflection, and ongoing dialogue about the evolving role of statistics in the data- and AI-centric future.

</details>


### [194] [Boosting methods for interval-censored data with regression and classification](https://arxiv.org/abs/2601.17973)
*Yuan Bian,Grace Y. Yi,Wenqing He*

Main category: stat.ML

TL;DR: 提出了针对区间删失数据的非参数提升方法，通过删失无偏变换调整损失函数，在回归和分类任务中提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统提升算法针对完全观测数据设计，在处理区间删失数据时效果不佳。区间删失数据在生存分析、医学研究等领域常见，需要专门方法处理。

Method: 使用删失无偏变换调整损失函数，通过函数梯度下降实现非参数提升方法，适用于回归和分类任务。

Result: 方法具有理论最优性和均方误差权衡性质，在不同有限样本场景下表现出稳健性能，扩展了现有提升技术的适用范围。

Conclusion: 提出的方法为区间删失数据提供了稳健的预测框架，增强了提升算法在实际应用中的适用性，特别是在医学研究、可靠性工程等领域。

Abstract: Boosting has garnered significant interest across both machine learning and statistical communities. Traditional boosting algorithms, designed for fully observed random samples, often struggle with real-world problems, particularly with interval-censored data. This type of data is common in survival analysis and time-to-event studies where exact event times are unobserved but fall within known intervals. Effective handling of such data is crucial in fields like medical research, reliability engineering, and social sciences. In this work, we introduce novel nonparametric boosting methods for regression and classification tasks with interval-censored data. Our approaches leverage censoring unbiased transformations to adjust loss functions and impute transformed responses while maintaining model accuracy. Implemented via functional gradient descent, these methods ensure scalability and adaptability. We rigorously establish their theoretical properties, including optimality and mean squared error trade-offs. Our proposed methods not only offer a robust framework for enhancing predictive accuracy in domains where interval-censored data are common but also complement existing work, expanding the applicability of existing boosting techniques. Empirical studies demonstrate robust performance across various finite-sample scenarios, highlighting the practical utility of our approaches.

</details>


### [195] [A Cherry-Picking Approach to Large Load Shaping for More Effective Carbon Reduction](https://arxiv.org/abs/2601.17990)
*Bokan Chen,Raiden Hasegawa,Adriaan Hilbers,Ross Koningstein,Ana Radovanović,Utkarsh Shah,Gabriela Volpato,Mohamed Ahmed,Tim Cary,Rod Frowd*

Main category: stat.ML

TL;DR: 该研究通过ERCOT电网的DC-OPF模拟分析不同负荷整形策略对碳排放和电力成本的影响，发现基于LMP的策略效果最佳，并提出基于电网信号和历史数据的"择优选择"方法


<details>
  <summary>Details</summary>
Motivation: 数据中心等大型负荷的整形会影响电网调度、碳排放和能源成本，但现有策略（基于平均碳强度、节点边际价格或边际排放）的有效性难以验证，因为缺乏详细的反事实数据

Method: 使用校准的ERCOT日前直流最优潮流模拟进行反事实分析，评估多种负荷整形策略对电网CO2排放和电力成本的影响

Result: 基于LMP的负荷整形在年度电网CO2减排方面优于其他常见策略，但仍有改进空间；提出基于可观测电网信号和历史数据的"择优选择"方法能更有效

Conclusion: "择优选择"的负荷整形方法适用于数据中心、分布式能源和虚拟电厂等大型灵活电力消费者，能更有效地减少电网碳排放和成本

Abstract: Shaping multi-megawatt loads, such as data centers, impacts generator dispatch on the electric grid, which in turn affects system CO2 emissions and energy cost. Substantiating the effectiveness of prevalent load shaping strategies, such as those based on grid-level average carbon intensity, locational marginal price, or marginal emissions, is challenging due to the lack of detailed counterfactual data required for accurate attribution. This study uses a series of calibrated granular ERCOT day-ahead direct current optimal power flow (DC-OPF) simulations for counterfactual analysis of a broad set of load shaping strategies on grid CO2 emissions and cost of electricity. In terms of annual grid level CO2 emissions reductions, LMP-based shaping outperforms other common strategies, but can be significantly improved upon. Examining the performance of practicable strategies under different grid conditions motivates a more effective load shaping approach: one that "cherry-picks" a daily strategy based on observable grid signals and historical data. The cherry-picking approach to power load shaping is applicable to any large flexible consumer on the electricity grid, such as data centers, distributed energy resources and Virtual Power Plants (VPPs).

</details>


### [196] [Nonlinear multi-study factor analysis](https://arxiv.org/abs/2601.18128)
*Gemma E. Moran,Anandi Krishnan*

Main category: stat.ML

TL;DR: 提出了一种多研究稀疏变分自编码器，用于从多组高维数据中识别共享和特定因素，并在血小板基因表达数据中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 高维数据通常包含可由低维因子捕获的变异。对于来自多个研究或环境的高维数据，需要区分哪些潜在因子是所有研究共有的，哪些是特定研究或环境特有的。以血小板基因表达数据为例，需要识别哪些基因簇（生物通路）对所有疾病都活跃，哪些仅对特定疾病活跃。

Method: 提出非线性多研究因子模型，使用多研究稀疏变分自编码器进行拟合。模型具有稀疏性：每个观测特征仅依赖于少量潜在因子。模型隐含地对潜在因子数量施加惩罚，有助于区分共享因子和组特定因子。

Result: 证明了潜在因子的可识别性，并在血小板基因表达数据中展示了该方法能够恢复有意义的因子。

Conclusion: 该方法能够有效从多研究高维数据中识别共享和特定因子，在基因组学等应用中具有实用价值。

Abstract: High-dimensional data often exhibit variation that can be captured by lower dimensional factors. For high-dimensional data from multiple studies or environments, one goal is to understand which underlying factors are common to all studies, and which factors are study or environment-specific. As a particular example, we consider platelet gene expression data from patients in different disease groups. In this data, factors correspond to clusters of genes which are co-expressed; we may expect some clusters (or biological pathways) to be active for all diseases, while some clusters are only active for a specific disease. To learn these factors, we consider a nonlinear multi-study factor model, which allows for both shared and specific factors. To fit this model, we propose a multi-study sparse variational autoencoder. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. In the genomics example, this means each gene is active in only a few biological processes. Further, the model implicitly induces a penalty on the number of latent factors, which helps separate the shared factors from the group-specific factors. We prove that the latent factors are identified, and demonstrate our method recovers meaningful factors in the platelet gene expression data.

</details>


### [197] [Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes](https://arxiv.org/abs/2601.18145)
*Heguang Lin,Binhao Chen,Mengze Li,Daniel Pimentel-Alarcón,Matthew L. Malloy*

Main category: stat.ML

TL;DR: 提出了一种认证算法，用于判断两个观察到的多项分布结果的最小体积置信集(MVCs)是否相交，解决了A/B测试中的核心决策问题。


<details>
  <summary>Details</summary>
Motivation: 最小体积置信集(MVCs)在多项分布参数估计中是最优的，但计算困难且几何形状不规则。实际应用中需要解决一个关键决策问题：给定两个观察结果，能否判断它们的MVCs是否相交？这对于A/B测试和强化学习分析至关重要。

Method: 利用似然排序在对数几率坐标中产生半空间约束，通过自适应几何划分参数空间，计算每个单元上p值的可计算上下界。对于三维情况，开发了高效且可证明正确的算法，能够认证相交、不相交或在规定容差范围内返回不确定结果。

Result: 该方法能够可靠地解决MVCs的相交判断问题，尽管MVCs几何形状不规则，但核心任务仍可进行认证决策。算法扩展到更高维度，为A/B测试提供了实用的认证工具。

Conclusion: 尽管最小体积置信集几何复杂且计算困难，但通过提出的认证方法，能够可靠地解决A/B测试中的核心决策问题，证明了MVCs在实际应用中具有可行的认证决策程序。

Abstract: Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing.

</details>


### [198] [Out-of-Distribution Radar Detection with Complex VAEs: Theory, Whitening, and ANMF Fusion](https://arxiv.org/abs/2601.18677)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: stat.ML

TL;DR: 提出一种基于复数变分自编码器(CVAE)的海杂波中弱复值信号检测方法，通过离群检测和与ANMF融合，在非高斯海杂波中实现更高检测概率。


<details>
  <summary>Details</summary>
Motivation: 解决海杂波等非高斯、距离变化干扰中弱复值信号的检测问题，传统检测器性能受限，需要利用复数域信息处理相位和多普勒结构。

Method: 使用复数变分自编码器(CVAE)仅用杂波加噪声训练进行离群检测，评估两种配置：未处理距离剖面和局部白化后。进一步将CVAE与ANMF通过加权对数概率融合规则在决策级集成。

Result: CVAE在两种配置下都获得比传统检测器更高的检测概率，白化后改进最显著。融合的CVAE-ANMF方案在强非高斯杂波中增强鲁棒性，实现经验校准的虚警率控制。

Conclusion: 统计白化与复数生成建模结合显著改善实际海杂波条件下的检测性能，融合的CVAE-ANMF方案构成模型基检测器的有竞争力替代方案。

Abstract: We investigate the detection of weak complex-valued signals immersed in non-Gaussian, range-varying interference, with emphasis on maritime radar scenarios. The proposed methodology exploits a Complex-valued Variational AutoEncoder (CVAE) trained exclusively on clutter-plus-noise to perform Out-Of-Distribution detection. By operating directly on in-phase / quadrature samples, the CVAE preserves phase and Doppler structure and is assessed in two configurations: (i) using unprocessed range profiles and (ii) after local whitening, where per-range covariance estimates are obtained from neighboring profiles. Using extensive simulations together with real sea-clutter data from the CSIR maritime dataset, we benchmark performance against classical and adaptive detectors (MF, NMF, AMF-SCM, ANMF-SCM, ANMF-Tyler). In both configurations, the CVAE yields a higher detection probability Pd at matched false-alarm rate Pfa, with the most notable improvements observed under whitening. We further integrate the CVAE with the ANMF through a weighted log-p fusion rule at the decision level, attaining enhanced robustness in strongly non-Gaussian clutter and enabling empirically calibrated Pfa control under H0. Overall, the results demonstrate that statistical normalization combined with complex-valued generative modeling substantively improves detection in realistic sea-clutter conditions, and that the fused CVAE-ANMF scheme constitutes a competitive alternative to established model-based detectors.

</details>
